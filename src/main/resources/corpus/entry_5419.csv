2017,TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning,High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work  we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1 0 1}  which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound  we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally  a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. Our source code is available.,TernGrad: Ternary Gradients to Reduce

Communication in Distributed Deep Learning

Wei Wen1  Cong Xu2  Feng Yan3  Chunpeng Wu1  Yandan Wang4  Yiran Chen1  Hai Li1

1Duke University  2Hewlett Packard Labs  3University of Nevada – Reno  4University of Pittsburgh

1{wei.wen  chunpeng.wu  yiran.chen  hai.li}@duke.edu

2cong.xu@hpe.com  3fyan@unr.edu  4yaw46@pitt.edu

Abstract

High network communication cost for synchronizing gradients and parameters
is the well-known bottleneck of distributed training. In this work  we propose
TernGrad that uses ternary gradients to accelerate distributed deep learning in data
parallelism. Our approach requires only three numerical levels {−1  0  1}  which
can aggressively reduce the communication time. We mathematically prove the
convergence of TernGrad under the assumption of a bound on gradients. Guided
by the bound  we propose layer-wise ternarizing and gradient clipping to im-
prove its convergence. Our experiments show that applying TernGrad on AlexNet
doesn’t incur any accuracy loss and can even improve accuracy. The accuracy
loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally  a
performance model is proposed to study the scalability of TernGrad. Experiments
show signiﬁcant speed gains for various deep neural networks. Our source code is
available 1.

Introduction

1
The remarkable advances in deep learning is driven by data explosion and increase of model size.
The training of large-scale models with huge amounts of data are often carried on distributed sys-
tems [1][2][3][4][5][6][7][8][9]  where data parallelism is adopted to exploit the compute capability
empowered by multiple workers [10]. Stochastic Gradient Descent (SGD) is usually selected as the
optimization method because of its high computation efﬁciency. In realizing the data parallelism
of SGD  model copies in computing workers are trained in parallel by applying different subsets of
data. A centralized parameter server performs gradient synchronization by collecting all gradients
and averaging them to update parameters. The updated parameters will be sent back to workers  that
is  parameter synchronization. Increasing the number of workers helps to reduce the computation
time dramatically. However  as the scale of distributed systems grows up  the extensive gradient
and parameter synchronizations prolong the communication time and even amortize the savings
of computation time [4][11][12]. A common approach to overcome such a network bottleneck is
asynchronous SGD [1][4][7][12][13][14]  which continues computation by using stale values without
waiting for the completeness of synchronization. The inconsistency of parameters across computing
workers  however  can degrade training accuracy and incur occasional divergence [15][16]. Moreover 
its workload dynamics make the training nondeterministic and hard to debug.
From the perspective of inference acceleration  sparse and quantized Deep Neural Networks (DNNs)
have been widely studied  such as [17][18][19][20][21][22][23][24][25]. However  these methods
generally aggravate the training effort. Researches such as sparse logistic regression and Lasso
optimization problems [4][12][26] took advantage of the sparsity inherent in models and achieved

1https://github.com/wenwei202/terngrad

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

remarkable speedup for distributed training. A more generic and important topic is how to accelerate
the distributed training of dense models by utilizing sparsity and quantization techniques. For
instance  Aji and Heaﬁeld [27] proposed to heuristically sparsify dense gradients by dropping off
small values in order to reduce gradient communication. For the same purpose  quantizing gradients
to low-precision values with smaller bit width has also been extensively studied [22][28][29][30].
Our work belongs to the category of gradient quantization  which is an orthogonal approach to
sparsity methods. We propose TernGrad that quantizes gradients to ternary levels {−1  0  1} to
reduce the overhead of gradient synchronization. Furthermore  we propose scaler sharing and
parameter localization  which can replace parameter synchronization with a low-precision gradient
pulling. Comparing with previous works  our major contributions include: (1) we use ternary values
for gradients to reduce communication; (2) we mathematically prove the convergence of TernGrad
in general by proposing a statistical bound on gradients; (3) we propose layer-wise ternarizing
and gradient clipping to move this bound closer toward the bound of standard SGD. These simple
techniques successfully improve the convergence; (4) we build a performance model to evaluate the
speed of training methods with compressed gradients  like TernGrad.

2 Related work
Gradient sparsiﬁcation. Aji and Heaﬁeld [27] proposed a heuristic gradient sparsiﬁcation method
that truncated the smallest gradients and transmitted only the remaining large ones. The method
greatly reduced the gradient communication and achieved 22% speed gain on 4 GPUs for a neural
machine translation  without impacting the translation quality. An earlier study by Garg et al. [31]
adopted the similar approach  but targeted at sparsity recovery instead of training acceleration. Our
proposed TernGrad is orthogonal to these sparsity-based methods.
Gradient quantization. DoReFa-Net [22] derived from AlexNet reduced the bit widths of weights 
activations and gradients to 1  2 and 6  respectively. However  DoReFa-Net showed 9.8% accuracy
loss as it targeted at acceleration on single worker. S. Gupta et al. [30] successfully trained neural
networks on MNIST and CIFAR-10 datasets using 16-bit numerical precision for an energy-efﬁcient
hardware accelerator. Our work  instead  tends to speedup the distributed training by decreasing the
communicated gradients to three numerical levels {−1  0  1}. F. Seide et al. [28] applied 1-bit SGD
to accelerate distributed training and empirically veriﬁed its effectiveness in speech applications. As
the gradient quantization is conducted by columns  a ﬂoating-point scaler per column is required. So
it cannot yield speed beneﬁt on convolutional neural networks [29]. Moreover  “cold start” of the
method [28] requires ﬂoating-point gradients to converge to a good initial point for the following 1-bit
SGD. More importantly  it is unknown what conditions can guarantee its convergence. Comparably 
our TernGrad can start the DNN training from scratch and we prove the conditions that promise
the convergence of TernGrad. A. T. Suresh et al. [32] proposed stochastic rotated quantization of
gradients  and reduced gradient precision to 4 bits for MNIST and CIFAR dataset. However  TernGrad
achieves lower precision for larger dataset (e.g. ImageNet)  and has more efﬁcient computation for
quantization in each computing node.
A parallel work by D. Alistarh et al. [29] presented QSGD that explores the trade-off between
accuracy and gradient precision. The effectiveness of gradient quantization was justiﬁed and the
convergence of QSGD was provably guaranteed. Compared to QSGD developed simultaneously 
our TernGrad shares the same concept but advances in the following three aspects: (1) we prove
the convergence from the perspective of statistic bound on gradients. The bound also explains why
multiple quantization buckets are necessary in QSGD; (2) the bound is used to guide practices and
inspires techniques of layer-wise ternarizing and gradient clipping; (3) TernGrad using only 3-level
gradients achieves 0.92% top-1 accuracy improvement for AlexNet  while 1.73% top-1 accuracy loss
is observed in QSGD with 4 levels. The accuracy loss in QSGD can be eliminated by paying the cost
of increasing the precision to 4 bits (16 levels) and beyond.

3 Problem Formulation and Our Approach
3.1 Problem Formulation and TernGrad
Figure 1 formulates the distributed training problem of synchronous SGD using data parallelism. At
iteration t  a mini-batch of training samples are split and fed into multiple workers (i ∈ {1  ...  N}).
Worker i computes the gradients g(i)
. All gradients are

t of parameters w.r.t. its input samples z(i)

t

2

ﬁrst synchronized and averaged at parameter server  and then sent back to update workers. Note
that parameter server in most implementations [1][12] are used to preserve shared parameters  while
here we utilize it in a slightly different way of maintaining shared gradients. In Figure 1  each
worker keeps a copy of parameters locally. We name this technique as parameter localization. The
parameter consistency among workers can be maintained by random initialization with an identical
seed. Parameter localization changes the communication of parameters in ﬂoating-point form to the
transfer of quantized gradients that require much lighter trafﬁc. Note that our proposed TernGrad can
be integrated with many settings like Asynchronous SGD [1][4]  even though the scope of this paper
only focuses on the distributed SGD in Figure 1.
Algorithm 1 formulates the t-th iteration of TernGrad algorithm according to Figure 1. Most steps of
TernGrad remain the same as traditional distributed training  except that gradients shall be quantized
into ternary precision before sending to parameter server. More speciﬁc  ternarize(·) aims to reduce
2 to a ternary vector with
the communication volume of gradients. It randomly quantizes gradient gt
values ∈ {−1  0  +1}. Formally  with a random binary vector bt  gt is ternarized as

˜gt = ternarize(gt) = st · sign (gt) ◦ bt 

(1)

where
(2)
is a scaler  e.g. maximum norm  that can shrink ±1 to a much smaller amplitude. ◦ is the Hadamard
product. sign(·) and abs(·) respectively returns the sign and absolute value of each element. Giving
a gt  each element of bt independently follows the Bernoulli distribution

st (cid:44) max (abs (gt)) (cid:44) ||gt||∞

(cid:26)P (btk = 1 | gt) = |gtk|/st

P (btk = 0 | gt) = 1 − |gtk|/st

 

(3)

where btk and gtk is the k-th element of bt and gt  respectively. This stochastic rounding  instead
of deterministic one  is chosen by both our study and QSGD [29]  as stochastic rounding has an
unbiased expectation and has been successfully studied for low-precision processing [20][30].
Theoretically  ternary gradients can at least reduce the worker-to-server trafﬁc by a factor of
32/log2(3) = 20.18×. Even using 2 bits to encode a ternary gradient  the reduction factor is
still 16×. In this work  we compare TernGrad with 32-bit gradients  considering 32-bit is the default
precision in modern deep learning frameworks. Although a lower-precision (e.g. 16-bit) may be
enough in some scenarios  it will not undervalue TernGrad. As aforementioned  parameter localiza-
tion reduces server-to-worker trafﬁc by pulling quantized gradients from servers. However  summing
t will produce more possible levels and thereby the ﬁnal averaged gradient
gt is no longer ternary as shown in Figure 2(d). It emerges as a critical issue when workers use
different scalers s(i)
t

up ternary values in(cid:80)

. To minimize the number of levels  we propose a shared scaler

i ˜g(i)

st = max({s(i)

t } : i = 1...N )

(4)
across all the workers. We name this technique as scaler sharing. The sharing process has a small
overhead of transferring 2N ﬂoating scalars. By integrating parameter localization and scaler
sharing  the maximum number of levels in gt decreases to 2N + 1. As a result  the server-to-worker
communication reduces by a factor of 32/log2(1 + 2N )  unless N ≥ 230.

Algorithm 1 TernGrad: distributed SGD training
using ternary gradients.
Worker : i = 1  ...  N

1

2

3

4
5
6

7

Figure 1: Distributed SGD with data par-
allelism.

2Here  the superscript of gt is omitted for simplicity.

  a part of a mini-batch of training samples

t

t under z(i)

Input z(i)
zt
Compute gradients g(i)
Ternarize gradients to ˜g(i)
Push ternary ˜g(i)
Pull averaged gradients gt from the server
Update parameters wt+1 ← wt − η · gt

Average ternary gradients gt =(cid:80)

to the server

t

i ˜g(i)

t /N

t = ternarize(g(i)
t )

t

Server :

3

Parameter serverWorker1𝒘"#$←𝒘"−𝒈"Worker2𝒘"#$←𝒘"−𝒈"WorkerN𝒘"#$←𝒘"−𝒈"……𝒈"($)𝒈"(*)𝒈"(+)𝒈"𝒈"𝒈"(5)

(6)

(9)

(10)

(12)

(13)

Convexity is a subset of Assumption 1  and we can easily ﬁnd non-convex functions satisfying it.
Assumption 2. Learning rate γt is positive and constrained as

(cid:26)(cid:80)+∞
(cid:80)+∞
t=0 γ2
t=0 γt = +∞  

t < +∞

3.2 Convergence Analysis and Gradient Bound
We analyze the convergence of TernGrad in the framework of online learning systems. An online
learning system adapts its parameter w to a sequence of observations to maximize performance. Each
observation z is drawn from an unknown distribution  and a loss function Q(z  w) is used to measure
the performance of current system with parameter w and input z. The minimization target then is the
loss expectation

C(w) (cid:44) E{Q(z  w)} .

In General Online Gradient Algorithm (GOGA) [33]  parameter is updated at learning rate γt as

wt+1 = wt − γtgt = wt − γt · ∇wQ(zt  wt) 

g (cid:44) ∇wQ(z  w)

where
(7)
and the subscript t denotes observing step t. In GOGA  E{g} is the gradient of the minimization
target in Eq. (5).
According to Eq. (1)  the parameter in TernGrad is updated  such as
wt+1 = wt − γt (st · sign (gt) ◦ bt)  

(8)
where st (cid:44) ||gt||∞ is a random variable depending on zt and wt. As gt is known for given zt and

wt  Eq. (3) is equivalent to (cid:26)P (btk = 1 | zt  wt) = |gtk|/st

P (btk = 0 | zt  wt) = 1 − |gtk|/st

.

At any given wt  the expectation of ternary gradient satisﬁes

E{st · sign (gt) ◦ bt} = E{st · sign (gt) ◦ E{bt|zt}} = E{gt} = ∇wC(wt) 

which is an unbiased gradient of minimization target in Eq. (5).
The convergence analysis of TernGrad is adapted from the convergence proof of GOGA presented
in [33]. We adopt two assumptions  which were used in analysis of the convergence of standard
GOGA in [33]. Without explicit mention  vectors indicate column vectors here.
Assumption 1. C(w) has a single minimum w∗ and gradient −∇wC(w) always points to w∗  i.e. 
(11)

(w − w∗)T ∇wC(w) > 0.

∀ > 0 

inf

||w−w∗||2>

which ensures γt decreases neither very fast nor very slow respectively.
We deﬁne the square of distance between current parameter wt and the minimum w∗ as

where || · || is (cid:96)2 norm. We also deﬁne the set of all random variables before step t as

ht (cid:44) ||wt − w∗||2  

(14)
Under Assumption 1 and Assumption 2  using Lyapunov process and Quasi-Martingales convergence
theorem  L. Bottou [33] proved
Lemma 1. If ∃A  B > 0 s.t.

Xt (cid:44) (z1...t−1  b1...t−1) .

E(cid:8)(cid:0)ht+1 −(cid:0)1 + γ2

t B(cid:1) ht

(cid:1)|Xt

(cid:9) ≤ −2γt(wt − w∗)T∇wC(wt) + γ2

t A 

(15)

then C(z  w) converges almost surely toward minimum w∗  i.e.  P (limt→+∞ wt = w∗) = 1.

4

We further make an assumption on the gradient as
Assumption 3 (Gradient Bound). The gradient g is bounded as

E{||g||∞ · ||g||1} ≤ A + B ||w − w∗||2  

where A  B > 0 and || · ||1 is (cid:96)1 norm.
With Assumption 3 and Lemma 1  we prove Theorem 1 ( in Supplementary Material):
Theorem 1. When online learning systems update as

(16)

(17)
i.e. 

(18)

wt+1 = wt − γt (st · sign (gt) ◦ bt)

they converge almost surely toward minimum w∗ 

using stochastic ternary gradients 
P (limt→+∞ wt = w∗) = 1.
Comparing with the gradient bound of standard GOGA [33]

E(cid:8)||g||2(cid:9) ≤ A + B ||w − w∗||2  

the bound in Assumption 3 is stronger because

||g||∞ · ||g||1 ≥ ||g||2.

(19)
We propose layer-wise ternarizing and gradient clipping to make two bounds closer  which shall be
explained in Section 3.3. A side beneﬁt of our work is that  by following the similar proof procedure 
we can prove the convergence of GOGA when Gaussian noise N (0  σ2) is added to gradients [34] 
under the gradient bound of

E(cid:8)||g||2(cid:9) ≤ A + B ||w − w∗||2 − σ2.

(20)
Although the bound is also stronger  Gaussian noise encourages active exploration of parameter
space and improves accuracy as was empirically studied in [34]. Similarly  the randomness of ternary
gradients also encourages space exploration and improves accuracy for some models  as shall be
presented in Section 4.

3.3 Feasibility Considerations

The gradient bound of TernGrad in Assumption 3 is stronger than the bound in standard GOGA.
Pushing the two bounds closer can improve the convergence of TernGrad. In Assumption 3  ||g||∞
is the maximum absolute value of all the gradients in the DNN. So  in a large DNN  ||g||∞ could
be relatively much larger than most gradients  implying that the bound in TernGrad becomes much
stronger. Considering the situation  we propose layer-wise ternarizing and gradient clipping to reduce
||g||∞ and therefore shrink the gap between these two bounds.
Layer-wise ternarizing is proposed based on the observation that the range of gradients in each
layer changes as gradients are back propagated. Instead of adopting a large global maximum scaler 

Figure 2: Histograms of (a) original ﬂoating gradients  (b) clipped gradients  (c) ternary gradients
and (d) ﬁnal averaged gradients. Visualization by TensorBoard. The DNN is AlexNet distributed
on two workers  and vertical axis is the training iteration. As examples  top row visualizes the third
convolutional layer and bottom one visualizes the ﬁrst fully-connected layer.

5

(a)original(b)clipped(c)ternary(d)finalIteration#Iteration#convfcwe independently ternarize gradients in each layer using the layer-wise scalers. More speciﬁc  we
separately ternarize the gradients of biases and weights by using Eq. (1)  where gt could be the
gradients of biases or weights in each layer. To approach the standard bound more closely  we can
split gradients to more buckets and ternarize each bucket independently as D. Alistarh et al. [29] does.
However  this will introduce more ﬂoating scalers and increase communication. When the size of
bucket is one  it degenerates to ﬂoating gradients.
Layer-wise ternarizing can shrink the bound gap resulted from the dynamic ranges of the gradients
across layers. However  the dynamic range within a layer still remains as a problem. We propose
gradient clipping  which limits the magnitude of each gradient gi in g as

(cid:26)gi

f (gi) =

|gi| ≤ cσ
sign(gi) · cσ |gi| > cσ

 

(21)

where σ is the standard derivation of gradients in g. In distributed training  gradient clipping is
applied to every worker before ternarizing. c is a hyper-parameter to select  but we cross validate
it only once and use the constant in all our experiments. Speciﬁcally  we used a CNN [35] trained
on CIFAR-10 by momentum SGD with staircase learning rate and obtained the optimal c = 2.5.
Suppose the distribution of gradients is close to Gaussian distribution as shown in Figure 2(a)  very
few gradients can drop out of [−2.5σ  2.5σ]. Clipping these gradients in Figure 2(b) can signiﬁcantly
reduce the scaler but slightly changes the length and direction of original g. Numerical analysis
shows that gradient clipping with c = 2.5 only changes the length of g by 1.0% − 1.5% and its
direction by 2◦ − 3◦. In our experiments  c = 2.5 remains valid across multiple databases (MNIST 
CIFAR-10 and ImageNet)  various network structures (LeNet  CifarNet  AlexNet  GoogLeNet  etc)
and training schemes (momentum  vanilla SGD  adam  etc).
The effectiveness of layer-wise ternarizing and gradient clipping can also be explained as follows.
When the scalar st in Eq. (1) and Eq. (3) is very large  most gradients have a high possibility to be
ternarized to zeros  leaving only a few gradients to large-magnitude values. The scenario raises a
severe parameter update pattern: most parameters keep unchanged while others likely overshoot.
This will introduce large training variance. Our experiments on AlexNet show that by applying both
layer-wise ternarizing and gradient clipping techniques  TernGrad can converge to the same accuracy
as standard SGD. Removing any of the two techniques can result in accuracy degradation  e.g.  3%
top-1 accuracy loss without applying gradient clipping as we shall show in Table 2.

4 Experiments
We ﬁrst investigate the convergence of TernGrad under various training schemes on relatively small
databases and show the results in Section 4.1. Then the scalability of TernGrad to large-scale
distributed deep learning is explored and discussed in Section 4.2. The experiments are performed
by TensorFlow[2]. We maintain the exponential moving average of parameters by employing an
exponential decay of 0.9999 [15]. The accuracy is evaluated by the ﬁnal averaged parameters. This
gives slightly better accuracy in our experiments. For fair comparison  in each pair of comparative
experiments using either ﬂoating or ternary gradients  all the other training hyper-parameters are the
same unless differences are explicitly pointed out. In experiments  when SGD with momentum is
adopted  momentum value of 0.9 is used. When polynomial decay is applied to decay the learning
rate (LR)  the power of 0.5 is used to decay LR from the base LR to zero.

Integrating with Various Training Schemes

4.1
We study the convergence of TernGrad using LeNet on MNIST and a ConvNet [35] (named as
CifarNet) on CIFAR-10. LeNet is trained without data augmentation. While training CifarNet  images

Figure 3: Accuracy vs. worker number for baseline and TernGrad  trained with (a) momentum SGD
or (b) vanilla SGD. In all experiments  total mini-batch size is 64 and maximum iteration is 10K.

6

98.00%98.50%99.00%99.50%100.00%248163264248163264baselineTernGradAccuracyNworkers(a) momentum SGD(b) vanilla SGDTable 1: Results of TernGrad on CifarNet.

SGD

base LR

total mini-batch size

iterations

Adam

0.0002

Adam

0.0002

128

2048

300K

18.75K

gradients
ﬂoating
TernGrad
ﬂoating
TernGrad

workers

2
2
16
16

accuracy
86.56%
85.64% (-0.92%)
83.19%
82.80% (-0.39%)

are randomly cropped to 24 × 24 images and mirrored. Brightness and contrast are also randomly
adjusted. During the testing of CifarNet  only center crop is used. Our experiments cover the scope
of SGD optimizers over vanilla SGD  SGD with momentum [36] and Adam [37].
Figure 3 shows the results of LeNet. All are trained using polynomial LR decay with weight decay of
0.0005. The base learning rates of momentum SGD and vanilla SGD are 0.01 and 0.1  respectively.
Given the total mini-batch size M and the worker number N  the mini-batch size per worker is
M/N. Without explicit mention  mini-batch size refers to the total mini-batch size in this work.
Figure 3 shows that TernGrad can converge to the similar accuracy within the same iterations  using
momentum SGD or vanilla SGD. The maximum accuracy gain is 0.15% and the maximum accuracy
loss is 0.22%. Very importantly  the communication time per iteration can be reduced. The ﬁgure
also shows that TernGrad generalizes well to distributed training with large N. No degradation is
observed even for N = 64  which indicates one training sample per iteration per worker.
Table 1 summarizes the results of CifarNet  where all trainings terminate after the same epochs.
Adam SGD is used for training. Instead of keeping total mini-batch size unchanged  we maintain the
mini-batch size per worker. Therefore  the total mini-batch size linearly increases as the number of
workers grows. Though the base learning rate of 0.0002 seems small  it can achieve better accuracy
than larger ones like 0.001 for baseline. In each pair of experiments  TernGrad can converge to the
accuracy level with less than 1% degradation. The accuracy degrades under a large mini-batch size in
both baseline and TernGrad. This is because parameters are updated less frequently and large-batch
training tends to converge to poorer sharp minima [38]. However  the noise inherent in TernGrad can
help converge to better ﬂat minimizers [38]  which could explain the smaller accuracy gap between
the baseline and TernGrad when the mini-batch size is 2048. In our experiments of AlexNet in
Section 4.2  TernGrad even improves the accuracy in the large-batch scenario. This attribute is
beneﬁcial for distributed training as a large mini-batch size is usually required.

4.2 Scaling to Large-scale Deep Learning
We also evaluate TernGrad by AlexNet and GoogLeNet trained on ImageNet. It is more challenging to
apply TernGrad to large-scale DNNs. It may result in some accuracy loss when simply replacing the
ﬂoating gradients with ternary gradients while keeping other hyper-parameters unchanged. However 
we are able to train large-scale DNNs by TernGrad successfully after making some or all of the
following changes: (1) decreasing dropout ratio to keep more neurons; (2) using smaller weight
decay; and (3) disabling ternarizing in the last classiﬁcation layer. Dropout can regularize DNNs by
adding randomness  while TernGrad also introduces randomness. Thus  dropping fewer neurons helps
avoid over-randomness. Similarly  as the randomness of TernGrad introduces regularization  smaller
weight decay may be adopted. We suggest not to apply ternarizing to the last layer  considering
that the one-hot encoding of labels generates a skew distribution of gradients and the symmetric
ternary encoding {−1  0  1} is not optimal for such a skew distribution. Though asymmetric ternary
levels could be an option  we decide to stick to ﬂoating gradients in the last layer for simplicity. The
overhead of communicating these ﬂoating gradients is small  as the last layer occupies only a small
percentage of total parameters  like 6.7% in AlexNet and 3.99% in ResNet-152 [39].
All DNNs are trained by momentum SGD with Batch Normalization [40] on convolutional layers.
AlexNet is trained by the hyper-parameters and data augmentation depicted in Caffe. GoogLeNet is
trained by polynomial LR decay and data augmentation in [41]. Our implementation of GoogLeNet
does not utilize any auxiliary classiﬁers  that is  the loss from the last softmax layer is the total loss.
More training hyper-parameters are reported in corresponding tables and published source code.
Validation accuracy is evaluated using only the central crops of images.
The results of AlexNet are shown in Table 2. Mini-batch size per worker is ﬁxed to 128. For fast
development  all DNNs are trained through the same epochs of images. In this setting  when there are

7

Table 2: Accuracy comparison for AlexNet.

base LR mini-batch size workers

iterations

0.01

0.02

256

512

2

4

370K

185K

gradients
ﬂoating
TernGrad

TernGrad-noclip ‡

ﬂoating
TernGrad
ﬂoating
TernGrad

weight decay DR†
0.5
0.2
0.2
0.5
0.2
0.5
0.2

0.0005
0.0005
0.0005
0.0005
0.0005
0.0005
0.0005

top-1
top-5
57.33% 80.56%
57.61% 80.47%
54.63% 78.16%
57.32% 80.73%
57.28% 80.23%
56.62% 80.28%
57.54% 80.25%

0.04
† DR: dropout ratio  the ratio of dropped neurons. ‡ TernGrad without gradient clipping.

92.5K

1024

8

Table 3: Accuracy comparison for GoogLeNet.

base LR mini-batch size workers

iterations

0.04

0.08

0.10

128

256

512

2

4

8

600K

300K

300K

gradients weight decay DR
ﬂoating
0.2
TernGrad
0.08
0.2
ﬂoating
TernGrad
0.08
ﬂoating
0.2
TernGrad
0.08

4e-5
1e-5
4e-5
1e-5
4e-5
2e-5

top-5
88.30%
86.77%
87.82%
85.96%
89.00%
86.47%

more workers  the number of iterations becomes smaller and parameters are less frequently updated.
To overcome this problem  we increase the learning rate for large-batch scenario [10]. Using this
scheme  SGD with ﬂoating gradients successfully trains AlexNet to similar accuracy  for mini-batch
size of 256 and 512. However  when mini-batch size is 1024  the top-1 accuracy drops 0.71% for the
same reason as we point out in Section 4.1.
TernGrad converges to approximate accuracy levels regardless of mini-batch size. Notably  it
improves the top-1 accuracy by 0.92% when mini-batch size is 1024  because its inherent randomness
encourages to escape from poorer sharp minima [34][38]. Figure 4 plots training details vs. iteration
when mini-batch size is 512. Figure 4(a) shows that the convergence curve of TernGrad matches
well with the baseline’s  demonstrating the effectiveness of TernGrad. The training efﬁciency can be
further improved by reducing communication time as shall be discussed in Section 5. The training
data loss in Figure 4(b) shows that TernGrad converges to a slightly lower level  which further proves
the capability of TernGrad to minimize the target function even with ternary gradients. A smaller
dropout ratio in TernGrad can be another reason of the lower loss. Figure 4(c) simply illustrate that
on average 71.32% gradients of a fully-connected layer (fc6) are ternarized to zeros.
Finally  we summarize the results of GoogLeNet in Table 3. On average  the accuracy loss is less
than 2%. In TernGrad  we adopted all that hyper-parameters (except dropout ratio and weight decay)
that are well tuned for the baseline [42]. Tuning these hyper-parameters speciﬁcally for TernGrad
could further optimize TernGrad and obtain higher accuracy.

5 Performance Model and Discussion
Our proposed TernGrad requires only three numerical levels {−1  0  1}  which can aggressively
reduce the communication time. Moreover  our experiments in Section 4 demonstrate that within the

Figure 4: AlexNet trained on 4 workers with mini-batch size 512: (a) top-1 validation accuracy  (b)
training data loss and (c) sparsity of gradients in ﬁrst fully-connected layer (fc6) vs. iteration.

8

0%10%20%30%40%50%60%70%050000100000150000baselineterngrad02468050000100000150000baselineterngrad0%20%40%60%80%050000100000150000(c)gradient sparsityofterngradinfc6(b)traininglossvsiteration(a)top-1accuracyvsiterationFigure 5: Training throughput on two different GPUs clusters: (a) 128-node GPU cluster with
1Gbps Ethernet  each node has 4 NVIDIA GTX 1080 GPUs and one PCI switch; (b) 128-node GPU
cluster with 100 Gbps InﬁniBand network connections  each node has 4 NVIDIA Tesla P100 GPUs
connected via NVLink. Mini-batch size per GPU of AlexNet  GoogLeNet and VggNet-A is 128  64
and 32  respectively

same iterations  TernGrad can converge to approximately the same accuracy as its corresponding
baseline. Consequently  a dramatical throughput improvement on the distributed DNN training is
expected. Due to the resource and time constraint  unfortunately  we aren’t able to perform the
training of more DNN models like VggNet-A [43] and distributed training beyond 8 workers. We plan
to continue the experiments in our future work. We opt for using a performance model to conduct
the scalability analysis of DNN models when utilizing up to 512 GPUs  with and without applying
TernGrad. Three neural network models—AlexNet  GoogLeNet and VggNet-A—are investigated.
In discussions of performance model  performance refers to training speed. Here  we extend the
performance model that was initially developed for CPU-based deep learning systems [44] to estimate
the performance of distributed GPUs/machines. The key idea is combining the lightweight proﬁling
on single machine with analytical modeling for accurate performance estimation. In the interest of
space  please refer to Supplementary Material for details of the performance model.
Figure 5 presents the training throughput on two different GPUs clusters. Our results show that
TernGrad effectively increases the training throughput for the three DNNs. The speedup depends on
the communication-to-computation ratio of the DNN  the number of GPUs  and the communication
bandwidth. DNNs with larger communication-to-computation ratios (e.g. AlexNet and VggNet-A)
can beneﬁt more from TernGrad than those with smaller ratios (e.g.  GoogLeNet). Even on a very
high-end HPC system with InﬁniBand and NVLink  TernGrad is still able to double the training
speed of VggNet-A on 128 nodes as shown in Figure 5(b). Moreover  the TernGrad becomes more
efﬁcient when the bandwidth becomes smaller  such as 1Gbps Ethernet and PCI switch in Figure 5(a)
where TernGrad can have 3.04× training speedup for AlexNet on 8 GPUs.

Acknowledgments
This work was supported in part by NSF CCF-1744082 and DOE SC0017030. Any opinions 
ﬁndings  conclusions or recommendations expressed in this material are those of the authors and do
not necessarily reﬂect the views of NSF  DOE  or their contractors. Thanks Ali Taylan Cemgil at
Bogazici University for valuable suggestions on this work.

References

[1] Jeffrey Dean  Greg Corrado  Rajat Monga  Kai Chen  Matthieu Devin  Mark Mao  Marc'aurelio Ranzato 
Andrew Senior  Paul Tucker  Ke Yang  Quoc V. Le  and Andrew Y. Ng. Large scale distributed deep
networks. In Advances in Neural Information Processing Systems  pages 1223–1231. 2012.

[2] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. Tensorﬂow: Large-scale machine learning on

9

(a)(b)020000400006000080000100000Images/sec# of GPUsTraining throughput on GPU cluster with Ethernet and PCI switchAlexNet FP32AlexNet TernGradGoogLeNet FP32GoogLeNet TernGradVggNet-A FP32VggNet-A TernGrad124816326412825651204000080000120000160000200000240000Images/sec# of GPUsTraining throughput on GPU cluster with InfiniBand and NVLinkAlexNet FP32AlexNet TernGradGoogLeNet FP32GoogLeNet TernGradVggNet-A FP32VggNet-A TernGrad1248163264128256512010002000300040000200040006000heterogeneous distributed systems. arXiv preprint:1603.04467  2016.

[3] Adam Coates  Brody Huval  Tao Wang  David Wu  Bryan Catanzaro  and Ng Andrew. Deep learning with

cots hpc systems. In International Conference on Machine Learning  pages 1337–1345  2013.

[4] Benjamin Recht  Christopher Re  Stephen Wright  and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems  pages
693–701  2011.

[5] Trishul M Chilimbi  Yutaka Suzue  Johnson Apacible  and Karthik Kalyanaraman. Project adam: Building

an efﬁcient and scalable deep learning training system. In OSDI  volume 14  pages 571–582  2014.

[6] Eric P Xing  Qirong Ho  Wei Dai  Jin Kyu Kim  Jinliang Wei  Seunghak Lee  Xun Zheng  Pengtao Xie 
Abhimanu Kumar  and Yaoliang Yu. Petuum: A new platform for distributed machine learning on big data.
IEEE Transactions on Big Data  1(2):49–67  2015.

[7] Philipp Moritz  Robert Nishihara  Ion Stoica  and Michael I Jordan. Sparknet: Training deep networks in

spark. arXiv preprint:1511.06051  2015.

[8] Tianqi Chen  Mu Li  Yutian Li  Min Lin  Naiyan Wang  Minjie Wang  Tianjun Xiao  Bing Xu  Chiyuan
Zhang  and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for heterogeneous
distributed systems. arXiv preprint:1512.01274  2015.

[9] Sixin Zhang  Anna E Choromanska  and Yann LeCun. Deep learning with elastic averaging sgd. In

Advances in Neural Information Processing Systems  pages 685–693  2015.

[10] Mu Li. Scaling Distributed Machine Learning with System and Algorithm Co-design. PhD thesis  Carnegie

Mellon University  2017.

[11] Mu Li  David G Andersen  Jun Woo Park  Alexander J Smola  Amr Ahmed  Vanja Josifovski  James Long 
Eugene J Shekita  and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In
OSDI  volume 14  pages 583–598  2014.

[12] Mu Li  David G Andersen  Alexander J Smola  and Kai Yu. Communication efﬁcient distributed machine
learning with the parameter server. In Advances in Neural Information Processing Systems  pages 19–27 
2014.

[13] Qirong Ho  James Cipar  Henggang Cui  Seunghak Lee  Jin Kyu Kim  Phillip B Gibbons  Garth A Gibson 
Greg Ganger  and Eric P Xing. More effective distributed ml via a stale synchronous parallel parameter
server. In Advances in neural information processing systems  pages 1223–1231  2013.

[14] Martin Zinkevich  Markus Weimer  Lihong Li  and Alex J Smola. Parallelized stochastic gradient descent.

In Advances in neural information processing systems  pages 2595–2603  2010.

[15] Xinghao Pan  Jianmin Chen  Rajat Monga  Samy Bengio  and Rafal Jozefowicz. Revisiting distributed

synchronous sgd. arXiv preprint:1702.05800  2017.

[16] Wei Zhang  Suyog Gupta  Xiangru Lian  and Ji Liu. Staleness-aware async-sgd for distributed deep
learning. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence 
IJCAI’16  pages 2350–2356. AAAI Press  2016. ISBN 978-1-57735-770-4. URL http://dl.acm.org/
citation.cfm?id=3060832.3060950.

[17] Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural networks with

pruning  trained quantization and huffman coding. arXiv preprint arXiv:1510.00149  2015.

[18] Wei Wen  Chunpeng Wu  Yandan Wang  Yiran Chen  and Hai Li. Learning structured sparsity in deep

neural networks. In Advances in Neural Information Processing Systems  pages 2074–2082  2016.

[19] J Park  S Li  W Wen  PTP Tang  H Li  Y Chen  and P Dubey. Faster cnns with direct sparse convolutions

and guided pruning. In International Conference on Learning Representations (ICLR)  2017.

[20] Itay Hubara  Matthieu Courbariaux  Daniel Soudry  Ran El-Yaniv  and Yoshua Bengio. Binarized neural

networks. In Advances in Neural Information Processing Systems  pages 4107–4115  2016.

[21] Mohammad Rastegari  Vicente Ordonez  Joseph Redmon  and Ali Farhadi. Xnor-net: Imagenet classiﬁ-
cation using binary convolutional neural networks. In European Conference on Computer Vision  pages
525–542. Springer  2016.

[22] Shuchang Zhou  Yuxin Wu  Zekun Ni  Xinyu Zhou  He Wen  and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 
2016.

[23] Wei Wen  Yuxiong He  Samyam Rajbhandari  Wenhan Wang  Fang Liu  Bin Hu  Yiran Chen  and Hai Li.

Learning intrinsic sparse structures within long short-term memory. arXiv:1709.05027  2017.

[24] Joachim Ott  Zhouhan Lin  Ying Zhang  Shih-Chii Liu  and Yoshua Bengio. Recurrent neural networks

with limited numerical precision. arXiv:1608.06902  2016.

10

[25] Zhouhan Lin  Matthieu Courbariaux  Roland Memisevic  and Yoshua Bengio. Neural networks with few

multiplications. arXiv:1510.03009  2015.

[26] Joseph K Bradley  Aapo Kyrola  Danny Bickson  and Carlos Guestrin. Parallel coordinate descent for

l1-regularized loss minimization. arXiv preprint arXiv:1105.5379  2011.

[27] Alham Fikri Aji and Kenneth Heaﬁeld. Sparse communication for distributed gradient descent. arXiv

preprint:1704.05021  2017.

[28] Frank Seide  Hao Fu  Jasha Droppo  Gang Li  and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech dnns. In Interspeech  pages 1058–1062  2014.

[29] Dan Alistarh  Demjan Grubic  Jerry Li  Ryota Tomioka  and Milan Vojnovic. Qsgd: Communication-
efﬁcient sgd via gradient quantization and encoding. In Advances in Neural Information Processing
Systems  pages 1707–1718  2017.

[30] Suyog Gupta  Ankur Agrawal  Kailash Gopalakrishnan  and Pritish Narayanan. Deep learning with limited

numerical precision. In ICML  pages 1737–1746  2015.

[31] Rahul Garg and Rohit Khandekar. Gradient descent with sparsiﬁcation: an iterative algorithm for sparse
recovery with restricted isometry property. In Proceedings of the 26th Annual International Conference on
Machine Learning  pages 337–344. ACM  2009.

[32] Ananda Theertha Suresh  Felix X Yu  H Brendan McMahan  and Sanjiv Kumar. Distributed mean

estimation with limited communication. arXiv:1611.00429  2016.

[33] Léon Bottou. Online learning and stochastic approximations. On-line learning in neural networks  17(9):

142  1998.

[34] Arvind Neelakantan  Luke Vilnis  Quoc V Le  Ilya Sutskever  Lukasz Kaiser  Karol Kurach  and James
Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint:1511.06807 
2015.

[35] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in Neural Information Processing Systems  pages 1097–1105. 2012.

[36] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks  12(1):

145–151  1999.

[37] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint:1412.6980 

2014.

[38] Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In International
Conference on Learning Representations  2017.

[39] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[40] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint:1502.03167  2015.

[41] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 2818–2826  2016.

[42] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Du-
mitru Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. arXiv
preprint:1409.4842  2015.

[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint:1409.1556  2014.

[44] Feng Yan  Olatunji Ruwase  Yuxiong He  and Trishul M. Chilimbi. Performance modeling and scalability
optimization of distributed deep learning systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  Sydney  NSW  Australia  August 10-13  2015  pages
1355–1364  2015. doi: 10.1145/2783258.2783270. URL http://doi.acm.org/10.1145/2783258.
2783270.

11

,Wei Wen
Cong Xu
Feng Yan
Chunpeng Wu
Yandan Wang
Yiran Chen
Hai Li