2019,Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates,We propose a new algorithm---Stochastic Proximal Langevin Algorithm (SPLA)---for sampling from a log concave distribution. Our method is a generalization of the Langevin algorithm to potentials expressed as the sum of one stochastic smooth term and multiple stochastic nonsmooth terms. In each iteration  our splitting technique only requires access to a stochastic gradient of the smooth term and a stochastic proximal operator  for each of the nonsmooth terms. We establish nonasymptotic  sublinear and linear convergence rates under convexity and strong convexity of the smooth term  respectively  expressed in terms of the KL divergence and Wasserstein distance. We illustrate the efficiency of our sampling technique through numerical simulations on a Bayesian learning task.,Stochastic Proximal Langevin Algorithm:

Potential Splitting and Nonasymptotic Rates

Adil Salim

Dmitry Kovalev

Peter Richtárik∗

King Abdullah University of Science and Technology  Thuwal  Saudi Arabia

Abstract

We propose a new algorithm—Stochastic Proximal Langevin Algorithm
(SPLA)—for sampling from a log concave distribution. Our method is a gen-
eralization of the Langevin algorithm to potentials expressed as the sum of one
stochastic smooth term and multiple stochastic nonsmooth terms. In each itera-
tion  our splitting technique only requires access to a stochastic gradient of the
smooth term and a stochastic proximal operator for each of the nonsmooth terms.
We establish nonasymptotic sublinear and linear convergence rates under convex-
ity and strong convexity of the smooth term  respectively  expressed in terms of
the KL divergence and Wasserstein distance. We illustrate the efﬁciency of our
sampling technique through numerical simulations on a Bayesian learning task.

1

Introduction

Many applications in the ﬁeld of Bayesian machine learning require to sample from a probability
distribution µ⋆ with density µ⋆(x)  x ∈ Rd. Due to their scalability  Monte Carlo Markov Chain
(MCMC) methods such as Langevin Monte Carlo [48] or Hamiltonian Monte Carlo [28] are popular
algorithms to solve such problems. Monte Carlo methods typically generate a sequence of random
variables (xk)k≥0 with the property that the distribution of xk approaches µ⋆ as k grows.
While the theory of MCMC algorithms has remained mainly asymptotic  in recent years the
exploration of non-asymptotic properties of such algorithms has led to a renaissance in the
ﬁeld [14  26  39  15  16  19  22  12  53  10  52]. In particular  if µ⋆(x) ∝ exp(−U (x))  where
U is a smooth convex function  [14  19] provide explicit convergence rates for the Langevin algo-
rithm (LA)

xk+1 = xk − γ∇U (xk) +p2γW k 

where γ > 0 and (W k)k≥0 is a sequence of i.i.d. standard Gaussian random variables. The function
U  also called the potential  enters the algorithm through its gradient.
In optimization  the problem min U where U is composite  i.e. is a sum of nonsmooth terms which
must be handled separately  has many instances  see [17  Section 2]. These optimization problems
can be seen as a Maximum A Posteriori (MAP) computation of some Bayesian model. Sampling
a posterori in these models allows for a better Bayesian inference [20]. In these cases  the task
of sampling a posterori takes the form of sampling from the target distribution µ⋆  where U has a
composite form.
In this work we study the setting where the potential U is the sum of a single smooth and a potentially
large number of nonsmooth convex functions. In particular  we consider the problem

Sample from µ⋆(x) ∝ exp(−U (x))  where U (x) := F (x) +

Gi(x) 

(1)

n

Pi=1

∗Also afﬁliated with Moscow Institute of Physics and Technology  Dolgoprudny  Russia.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Complexity results obtained in Corollaries 2  3 and 4 of our main result (Theorem 1).

F

Stepsize γ

Rate

Theorem

convex

O (ε)

KL(µˆxk | µ⋆) ≤

1

2γ(k+1) W 2(µx0   µ⋆) + O(γ)

Cor 2

α-strongly convex

O (εα)

W 2(µxk   µ⋆) ≤ (1 − γα)kW 2(µx0   µ⋆) + O (cid:0) γ

α (cid:1)

Cor 3

α-strongly convex

O (εα)

KL(µ˜xk | µ⋆) ≤ α(1 − γα)k+1W 2(µx0   µ⋆) + O(γ)

Cor 4

where F : Rd → R is a smooth convex function and G1  . . .   Gn : Rd → R are (possibly nons-
mooth) convex functions. The additive model for U offers ample ﬂexibility as typically there are
multiple decompositions of U in the form (1).

2 Contributions

We now brieﬂy comment some of the key contributions of this work.
⋄ A splitting technique for Langevin algorithm. We propose a new variant of LA for solving (1) 
which we call Stochastic Proximal Langevin Algorithm (SPLA). We assume that F and G can be
written as expectations over some simpler functions f (·  ξ) and gi(·  ξ)

F (x) = Eξ(f (x  ξ)) 

and Gi(x) = Eξ(gi(x  ξ)).

(2)
SPLA (see Algorithm 1 in Section 4) only requires accesses to the gradient of f (·  ξ) and to prox-
imity operators of the functions gi(·  ξ). SPLA can be seen as a Langevin version of the stochastic
Passty algorithm [30  36]. To the best of our knowledge  this is the ﬁrst time a splitting technique
that involves multiple (stochastic) proximity operators is used in a Langevin algorithm.
Remarks: Current forms of LA tackle problem (1) using stochastic subgradients [18]. If n = 1 and
G1 is proximable (i.e.  the learner has access to the full proximity operator of G1)  it has recently
been proposed to use proximity operators instead of (sub)gradients [20  18]  as it is done in the
optimization literature [29  2]. Indeed  in this case  the proximal stochastic gradient method is an
efﬁcient method to minimize U. If n > 1  and the functions Gi are proximable (but not U)  the
minimization of U is usually tackled using the operator splitting framework: the (stochastic) three-
operator splitting [51  17] or (stochastic) primal dual algorithms [13  46  9  35]. These algorithms
involve the computation of (stochastic) gradients and (full) proximity operators and enjoy numerical
stability properties. However  proximity operators are sometimes difﬁcult to implement. In this
case  stochastic proximity operators are cheaper2 than full proximity operators and numerically more
stable than stochastic subgradients to handle nonsmooth terms [32  31  4  5  6] but also smooth [43]
terms. In this paper  we bring together the advantages of operator splitting and stochastic proximity
operators for sampling purposes.
⋄ Theory. We perform a nonasymptotic convergence analysis of SPLA. Our main result  Theo-
rem 1  gives a tractable recursion involving the Kullback-Leibler divergence and Wasserstein dis-
tance (when U is strongly convex) between µ⋆ and the distribution of certain samples generated by
our method. We use this result to show that the KL divergence is lower than ε after O(1/ε2) iterations
if the constant stepsize γ = O(ε) is used (Corollary 2). Assuming F is α-strongly convex  we show
that the Wasserstein distance and (resp. the KL divergence) decrease exponentially  up to an oscil-
lation region of size O(γ/α) (resp. O(γ)) as shown in Corollary 3 (resp. Corollary 4). If we wish to
push the Wasserstein distance below ε (resp. the KL divergence below αε)  this could be achieved
by setting γ = O(εα)  and it would be sufﬁcient to take O(1/ε log 1/ε) iterations. These results are
summarized in Table 1. The obtained convergence rates match the previous known results obtained
in simpler settings [18]. Note that convergence rates of optimization methods involving multiple
stochastic proximity operators haven’t been established yet.

2See www.proximity-operator.net

2

Remarks: Our proof technique is inspired by [38]  which is itself based on [18]. In [38]  the authors
consider the n = 1 case  and assume that the smooth function F is proximable. In [18]  a proximal
stochastic (sub)gradient Langevin algorithm is studied. In this paper  convergence rates are estab-
lished by showing that the probability distributions of the iterates shadow some discretized gradient
ﬂow deﬁned on a measure space. Hence  our work is a contribution to recent efforts to understand
Langevin algorithm as an optimization algorithm in a space of probability measures [26  49  3].
⋄ Online setting. In online settings  U is unknown but revealed across time. Our approach provides
a reasonable algorithm for such situations  especially in cases when the information revealed about
U is stationary in time. In particular  this includes online Bayesian learning with structured priors
or nonsmooth log likelihood [50  23  40  47]. In this context  the learner is required to sample from
some posterior distribution µ⋆ that takes the form (1) where F  G1  . . .   Gn are intractable. However 
these functions can be cheaply sampled  or are revealed across time through i.i.d. streaming data.
⋄ Simulations. We illustrate the promise of our approach numerically by performing experiments
with SPLA. We ﬁrst apply SPLA to a stochastic and nonsmooth toy model with a ground truth. Then 
we consider the problem of sampling from the posterior distribution in the Graph Trend Filtering
context [47]. For this nonsmooth large scale simulation problem  SPLA is performing better than the
state of the art method that uses stochastic subgradients instead of stochastic proximity operators.
Indeed  in the optimization litterature [2]  proximity operators are already known to be more stable
than subgradients.

3 Technical Preliminaries

In this section  we recall certain notions from convex analysis and probability theory  which are keys
to the developments in this paper  state our main assumptions  and introduce needed notations.

3.1 Subdifferential  minimal section and proximity operator

Given a convex function g : Rd → R  its subdifferential at x  ∂g(x)  is the set
∂g(x) := (cid:8)d ∈ Rd : g(x) + hd  y − xi ≤ g(y)(cid:9) .

Since ∂g(x) is a nonempty closed convex set [2]  the projection of 0 onto ∂g(x)—the least norm
element in the set ∂g(x)—is well deﬁned  and we call this element ∇0g(x). The function ∇0g :
Rd → Rd is called the minimal section of ∂g. The proximity operator associated with g is the
mapping proxg : Rd → Rd deﬁned by

Due to its implicit deﬁnition  proxg can be hard to evaluate.

proxg(x) := arg min

y∈Rd (cid:8) 1

2kx − yk2 + g(y)(cid:9) .

3.2 Stochastic structure of F and Gi: integrability  smoothness and convexity

Here we detail the assumptions behind the stochastic structure (2) of the functions F = Eξ(f (x  ξ))
and Gi = Eξ(gi(x  ξ)) deﬁning the potential U. Let (Ω  F   P) be a probability space and denote E
the mathematical expectation and V the variance. Consider ξ a random variable from Ω to another
probability space (Ξ  G ) with distribution µ.
Assumption 1 (Integrability). The functions f : Rd×Ξ → Rd and gi : Rd×Ξ → Rd  i = 1  . . .   n 
are µ-integrable for every x ∈ Rd.
Furthermore  we will make the following convexity and smoothness assumptions.
Assumption 2 (Convexity and differentiability). The function f (·  s) is convex and differentiable
for every s ∈ Ξ. The functions gi(·  s) are convex for every i ∈ {1  2  . . .   n}.
The gradient of f (·  s) is denoted ∇f (·  s)  the subdifferential of gi(·  s) is denoted ∂gi(·  s) and
its minimal section is denoted ∇0gi(·  s). Under Assumption 2  it is known that F is convex and
differentiable and that ∇F (x) = Eξ(∇f (x  ξ)) [34]. Next  we assume that F is smooth and α-
strongly convex. However  we allow α = 0 if F is not strongly convex. We will only assume that
α > 0 in Corollaries 3 and 4.

3

Assumption 3 (Convexity and smoothness of F ). The gradient of F is L-Lipschitz continuous 
where L ≥ 0. Moreover  F is α-strongly convex  where α ≥ 0.
Under Assumption 2  the second part of the above holds for α = 0. Finally  we will introduce two
noise conditions on the stochastic (sub)gradients of f (·  s) and gi(·  s).
Assumption 4 (Bounded variance of ∇f (x ·)). There exists σF ≥ 0  such that Vξ(k∇f (x  ξ)k) ≤
σ2
F for every x ∈ Rd.
Assumption 5 (Bounded second moment of ∇0gi(x ·)). For every i ∈ {1  2  . . .   n}  there exists
LGi ≥ 0 such that Eξ(k∇0gi(x  ξ)k2) ≤ L2

for every x ∈ Rd.

Gi

Note that if gi(·  s) is ℓi(s)-Lipschitz continuous for every s ∈ Ξ  and if ℓi(s) is µ-square integrable 
then Assumption 5 holds.

3.3 KL divergence  entropy and potential energy

Recall from (1) that U := F + Pn
i=1 Gi and assume that R exp(−U (x))dx < ∞. Our goal is to
sample from the unique distribution µ⋆ over Rd with density µ⋆(x) (w.r.t. the Lebesgue measure
denoted L) proportional to exp(−U (x))  for which we write µ⋆(x) ∝ exp(−U (x)). The closeness
between the samples of our algorithm and the target distribution µ⋆ will be evaluated in terms of
information theoretic and optimal transport theoretic quantities.
Let B(Rd) be the Borel σ-ﬁeld of Rd. Given two nonnegative measures µ and ν on (Rd B(Rd))  we
write µ ≪ ν if µ is absolutely continuous w.r.t. ν  and denote dµ
dν its density. The Kullback-Leibler
(KL) divergence between µ and ν  KL(µ | ν)  quantiﬁes the closeness between µ and ν. If µ ≪ ν 
then the KL divergence is deﬁned by

KL(µ | ν) := Z log(cid:18) dµ

dν

(x)(cid:19) dµ(x) 

and otherwise we set KL(µ | ν) = +∞. Up to an additive constant  KL(· | µ⋆) can be seen as the
sum of two terms [37]: the entropy H(µ) and the potential energy EU (µ). The entropy of µ is given
by H(µ) := KL(µ | L)  and the potential energy of µ is deﬁned by EU (µ) := R U dµ(x).

3.4 Wasserstein distance

Although the KL divergence is equal to zero if and only if µ = ν  it is not a mathematical distance
(metric). The Wasserstein distance  deﬁned below  metricizes the space P2(Rd) of probability mea-
sures over Rd with a ﬁnite second moment. Consider µ  ν ∈ P2(Rd). A transference plan of (µ  ν)
is a probability measure υ over (Rd × Rd B(Rd × Rd)) with marginals µ  ν : for every A ∈ B(Rd) 
υ(A × Rd) = µ(A) and υ(Rd × A) = ν(A). In particular  the product measure µ ⊗ ν is a trans-
ference plan. We denote Γ(µ  ν) the set of transference plans. A coupling of (µ  ν) is a random
variable (X  Y ) over some probability space with values in (Rd × Rd B(Rd × Rd)) (i.e.  X and Y
are random variables with values in Rd) such that the distribution of X is µ and the distribution of
Y is ν. In other words  (X  Y ) is a coupling of µ  ν if the distribution of (X  Y ) is a transference
plan of µ  ν. The Wasserstein distance of order 2 between µ and ν is deﬁned by

W 2(µ  ν) := inf (cid:26)ZRd×Rd kx − yk2dυ(x  y) 

υ ∈ Γ(µ  ν)(cid:27) .

One can see that W 2(µ  ν) = inf E(kX − Y k2)  where the inf is taken over all couplings (X  Y ) of
µ  ν deﬁned on some probability space with expectation E.

4

4 The SPLA Algorithm and its Convergence Rates

4.1 The algorithm

To solve the sampling problem (1)  our Stochastic Proximal Langevin Algorithm (SPLA) generates
a sequence of random variables (xk)k≥0 from (Ω  F   P) to (Rd B(Rd)) deﬁned as follows

zk = xk − γ∇f (xk  ξk)
0 = zk +p2γW k
yk
yk
i = proxγgi(· ξk)(yk

i−1)

for

i = 1  . . .   n

xk+1 = yk
n 

where (W k)k≥0 is a sequence of i.i.d. standard Gaussian random variables  (ξk)k≥0 is a sequence of
i.i.d. copies of ξ and γ > 0 is a positive step size. Our SPLA method is formalized as Algorithm 1;
its steps are explained therein.

Algorithm 1 Stochastic Proximal Langevin Algorithm (SPLA)

Initialize: x0 ∈ Rd
for k = 0  1  2  . . . do
Sample random ξk
zk = xk − γ∇f (xk  ξk)
Sample random W k
0 = zk + √2γW k
yk
for i = 1  . . .   n do

yk
i = proxγgi(· ξk)(yk

i−1)

end for
xk+1 = yk
n

end for

4.2 Main theorem

⊲ used for stoch. approximation: F ≈ f (·  ξk) and Gi ≈ gi(·  ξk)
⊲ a stochastic gradient descent step in F
⊲ a standard Gaussian vector in Rd
⊲ a Langevin step w.r.t. F

⊲ prox step to handle the term Gi(·) = Eξgi(·  ξ)
⊲ the ﬁnal SPLA step  accounting for F and G1  G2  . . .   Gn

We now state our main results in terms of Kullback-Leibler divergence and Wasserstein distance.
We denote µx the distribution of every random variable x deﬁned on (Ω  F   P).
Theorem 1. Let Assumptions 1–5 hold and assume that γ ≤ 1/L. There exists C ≥ 0 such that 

2γ KL(µyk

0 | µ⋆) ≤ (1 − γα)W 2(µxk   µ⋆) − W 2(µxk+1   µ⋆) + γ2(2σ2

F + 2Ld + C).

(3)

The constant C can be expressed as a linear combination of L2
with integer coefﬁcients.
G1
Moreover  if n = 2  then C := 2(L2
). More generally  if for every i ∈ {2  . . .   n}  gi(·  ξ)
G1
admits almost surely the representation gi(·  ξ) = ˜gi(·  ξi) where ξ2  . . .   ξn are independent random
variables  then C := n

  . . .   L2

+ L2
G2

Gn

n

.

L2
Gi

Pi=1

Proof. A full proof can be found in the Supplementary material. We only sketch the main steps

here. For every µ-integrable function g : Rd → R  we denote Eg(µ) = R gdµ. Moreover  we denote
F = EU + H. First  using [18  Lemma 1]  µ⋆ ∈ P2(Rd)  EU (µ⋆) H(µ⋆) < ∞ and if µ ∈ P2(Rd) 
then

KL(µ | µ⋆) = EU (µ) + H(µ) − (EU (µ⋆) + H(µ⋆)) = F(µ) − F(µ⋆) 

provided that EU (µ) < ∞. Then  we decompose EU (µ) = EF (µ) + EG(µ) where G = Pn

Using [18] again  we can establish the inequality

i=1 Gi.

2γhH(µyk

0

) − H(µ⋆)i ≤ W 2(µzk   µ⋆) − W 2(µyk

0

  µ⋆).

Then  if γ ≤ 1/L we obtain  for every random variable a with distribution µ⋆ 

Eh(cid:13)(cid:13)zk − a(cid:13)(cid:13)

2i ≤ (1 − γα)Eh(cid:13)(cid:13)xk − a(cid:13)(cid:13)

2i + 2γ [EF (µ⋆) − EF (µzk )] + 2γ2σ2

F  

(4)

(5)

5

using standard computations regarding the stochastic gradient descent algorithm. Using the smooth-
ness of F and the deﬁnition of the Wasserstein distance  this implies

2γhEF (µyk

0

) − EF (µ⋆)i ≤ (1 − γα)W 2(µxk   µ⋆) − W 2(µzk   µ⋆) + γ2(2σ2

F + 2Ld).

It remains to establish 2γhEG(µyk

  µ⋆) − W 2(µxk+1   µ⋆) + γ2C  which
is the main technical challenge of the proof. This is done using the frameworks of Yosida approx-
imation of random subdifferentials and Moreau regularizations of random convex functions [2].
Equation (3) is obtained by summing the obtained inequalities.

) − EG(µ⋆)i ≤ W 2(µyk

0

0

4.3 Link with Wasserstein Gradient Flows

Equation (3) is reminiscent of the fact that the SPLA shadows the gradient ﬂow of KL(· | µ⋆) in
the metric space (P2(Rd)  W ). To see this  ﬁrst consider the gradient ﬂow associated to F . By
deﬁnition  it is the ﬂow of the differential equation [7]

The function x can alternatively be deﬁned as a solution of the variational inequalities

d
dt

x(t) = −∇F (x(t)) 

t > 0.

2 (F (x(t)) − F (a)) ≤ − d

dtkx(t) − ak2 

t > 0 

∀a ∈ Rd.

(6)

(7)

The iterates (uk)k≥0 of the stochastic gradient descent (SGD) algorithm applied to F can be seen
as a (noisy) Euler discretization of (6) with a step size γ > 0. This idea has been used successfully
in the stochastic approximation litterature [33  24]. This analogy goes further since a fundamental
inequality used to analyze SGD applied to F is ([27])

2γE(cid:0)F (uk+1) − F (a)(cid:1) ≤ Ekuk − ak2 − Ekuk+1 − ak2 + γ2K 

k ≥ 0 

where K ≥ 0 is some constant  which can be seen as a discrete counterpart of (7). Note that this
inequality is similar to (5) that is used in the proof of Theorem 1.
In the optimal transport theory  the point of view of (7) is used to deﬁne the gradient ﬂow of a
(geodesically) convex function F deﬁned on P2(Rd) (see [37] or [1  Page 280]). Indeed  the gradient
ﬂow (νt)t≥0 of F in the space (P2(Rd)  W ) satisﬁes for every t > 0  µ ∈ P2(Rd) 

2 (F(νt) − F(µ)) ≤ − d

(8)
which can be seen as a continuous time counterpart of Equation (3) by setting F = KL(· | µ⋆).
Furthermore  Equation (4) in the proof of Theorem 1 is also related to (8). It is obtained by applying
Equation (8) with F = H and ν0 = µzk (see e.g [18  Lemma 5]).
4.4 Explicit convergence rates for convex and strongly convex F

dt W 2(νt  µ) 

Corollaries 2  3 and 4 below are obtained by unrolling the recursion provided by Theorem 1. The
results are summarized in Table 1.
Corollary 2 (Convex F ). Consider a sequence of independent random variables (jk)k≥0 such that
(jk)k≥0 is independent of (W k)k and (ξk)k  and the distribution of jk is uniform over {0  . . .   k}.
Denote ˆxk = yjk

0 . If γ ≤ 1/L  then 
KL(µˆxk | µ⋆) ≤

1

2γ(k + 1)

W 2(µx0   µ⋆) +

γ
2

(2σ2

F + 2Ld + C).

Hence  given any ε > 0  choosing stepsize γ = minn 1
L  

k + 1 ≥ maxn L

ε   2σ2

F +2Ld+C

ε2

ε

2σ2

F +2Ld+Co and a number of iterations
o W 2(µx0   µ⋆) 

implies KL(µˆxk | µ⋆) ≤ ε.
Corollary 3 (Strongly convex F ). If α > 0 and γ ≤ 1/L  then 

W 2(µxk   µ⋆) ≤ (1 − γα)kW 2(µx0   µ⋆) + γ(2σ2

α

F +2Ld+C)

.

6

Hence  given any ε > 0  choosing stepsize γ = minn 1
L  
o log(cid:16) 2W 2(µx0  µ⋆)

k ≥ maxn L

α   2(2σ2

F +2Ld+C)

2(2σ2

εα2

ε

(cid:17)  

εα

F +2Ld+C)o and a number of iterations

implies W 2(µxk   µ⋆) ≤ ε.
Corollary 4 (Strongly convex F ). Consider a sequence of independent random variables (jk)k≥0
such that (jk)k is independent of (W k)k and (ξk)k. Assume that the distribution of jk is geometric
over {0  . . .   k}:
Denote ˜xk = xjk. If α > 0 and γ ≤ 1/L  then 
KL(µ˜xk | µ⋆) ≤ αW 2(µx0  µ⋆)

P(jk = r) ∝ (1 − γα)−r.

1−(1−γα)k+1 + γ(2σ2

F +2Ld+C)

(1−γα)k+1

·

2

2

.

Hence  given any ε > 0  choosing stepsize γ = minn 1
L  

2σ2

εα

F +2Ld+Co and a number of iterations

k ≥ maxn L

α   2σ2

F +2Ld+C

εα2

o log(cid:16)2 maxn1  W 2(µx0  µ⋆)

ε

o(cid:17)  

implies KL(µ˜xk | µ⋆) ≤ αε.
We can compare these bounds with the one of [18]. First  in the particular case n = 1 and
g1(·  s) ≡ G1  SPLA boils down to the algorithm of [18  Section 4.2]  Corollary 2 matches ex-
actly [18  Corollary 18] and Corollary 3 matches [18  Corollary 22]. To our knowledge  Corollary 4
has no counterpart in the litterature. We now focus on the case F ≡ 0 and n = 1 of SPLA  as it con-
centrates the innovations of our paper. In this case  L = 0 and σF = 0. Compared to the Stochastic
Subgradient Langevin Algorithm (SSLA) [18  Section 4.1]  Corollary 2 matches with [18  Corollary
14].

5 Numerical experiments

5.1 Simulations using a ground truth

We ﬁrst concentrate on the case F ≡ 0 and n = 1. Let U = |x| = Eξ(|x|+xξ) (g1(x  s) = |x|+xs) 
where ξ is standard Gaussian. The target µ⋆ ∝ exp(−U ) is a standard Laplace distribution in R.
In this case  L = α = σF = 0 and C = L2
= 2. We shall illustrate the bound on KL(µˆxk|µ⋆)
G1
(Corollary 2 for SPLA and [18  Corollary 14] for SSLA) for both algorithms using histograms. Note
that the distribution µˆxk of ˆxk is a (deterministic) mixture of the µxj : µˆxk = 1
j=1 µxj . Using
Pinsker inequality  we can bound the total variation distance between µˆxk and µ⋆ from the bound
on KL  and illustrate this by histograms. In Figure 1  we take γ = 10 and do 105 iterations of both
algorithms. Note that here the complexity of one iteration of SPLA or SSLA is the same. One can

k Pk

Figure 1: Comparison between histograms of SPLA and SSLA and the true density 0.5 exp(−|x|).
see that SPLA enjoys the well known advantages of stochastic proximal methods [43]: precision 
numerical stability (less outliers)  and robustness to step size.

7

Algorithm 2 SPLA for the Graph Trend Filtering

Initialize: x0 ∈ RV
for k = 0  1  2  . . . do
zk = xk − γ
Sample random W k
0 = zk + √2γW k
yk
for i = 1  . . .   n do

σ2 (xk − Y )

Sample uniform random edges ei
yk
i = proxγgei

i−1)

(yk

end for
xk+1 = yk
n

end for

⊲ standard Gaussian vector in RV

5.2 Application to Trend Filtering on Graphs

In this section we consider the following Bayesian point of view of trend ﬁltering on graphs [42].
Consider a ﬁnite undirected graph G = (V  E)  where V is the set of vertices and E is the set of
edges. Denote d the cardinality of V and |E| the cardinality of E. A realization of a random vector
Y ∈ RV is observed. In a Bayesian framework  the distribution of Y is parametrized by a vector
X ∈ RV which is itself random and whose distribution p is proportional to exp(−λ TV(x  G)) 
where λ > 0 is a scaling parameter and where for every x ∈ RV

TV(x  G) =

Pi j∈V {i j}∈E |x(i) − x(j)| 

is the Total Variation regularization over G. The goal is to learn X after an observation of Y . The
paper [47] consider the case where the distribution of Y given X (a.k.a the likelihood) is proportional
to exp(− 1
2σ2kX − yk2)  where σ ≥ 0 is another scaling parameter. In other words  the distribution
of Y given X is N (X  σ2I)  a normal distribution centered at X with variance σ2I (where I is the
d × d identity matrix). Denoting

π(x | y) ∝ exp(−U (x))  U (x) = 1

2σ2kx − yk2 + λ TV(x  G) 

the posterior distribution of X given Y   the maximum a posteriori estimator in this Bayesian frame-
work is called the Graph Trend Filtering estimate [47]. It can be written

x⋆ = arg max

x∈RV

π(x | Y ) = arg min

x∈RV

1

2σ2kx − Y k2 + λ TV(x  G).

Although maximum a posteriori estimators carry some information  they are not able to capture
uncertainty in the learned parameters. Samples a posteriori provide a better understanding of the
posterior distribution and allow to compute other Bayesian estimates such as conﬁdence intervals.
This allows to avoid overﬁtting among other things. In our context  sampling a posteriori would
require to sample from the target distribution µ⋆(x) = π(x | Y ).
In the case where G is a 2D grid (which can be identiﬁed with an image)  the proximity operator
of TV(·  G) can be computed using a subroutine [8] and the proximal stochastic gradient Langevin
algorithm can be used to sample from π(· | Y ) [20  18]. However  on a large/general graph  the
proximity operator of TV(·  G) is hard to evaluate [41  36]. Since TV(·  G) is written as a sum  we
shall rather select a batch of random edges and compute the proximity operators over these randomly
chosen edges. More precisely  we write the potential U deﬁning π(x | Y ) in the form (1) by setting

n

Pi=1

U (x) = F (x) +

Gi(x)  F (x) = 1

2σ2kx − Y k2  Gi(x) = λ |E|

n

Eei (|x(vi) − x(wi)|)  

where for every i ∈ {1  . . .   n}  ei = {vi  wi} ∈ E is an uniform random edge and the ei
are independent. For every edge e = {v  w} ∈ E  (where v  w are vertices) denote ge(x) =
λ |E|
n |x(v) − x(w)| and note that Gi(x) = Eei (gei (x)). The parameter n can be seen as a
batch parameter: Pn
i=1 gei (x) is an unbiaised approximation of T V (x  G). Also note that we set
f (·  s) ≡ F . The SPLA applied to sample from π(· | Y ) is presented as Algorithm 2. In our
In the
simulations  the SPLA is compared to two different versions of the Langevin algorithm.

8

n

Pi=1

full proximity operator of

Stochastic Subgradient Langevin Algorithm (SSLA) [18]  stochastic subgradients of gei are used
instead of stochastic proximity operators. In the Proximal Langevin Algorithm (ProxLA) [18]  the
Gi is computed using a subroutine. As mentioned in [36  47]  we
use the gradient algorithm for the dual problem. The plots in Figure 2 provide simulations of the
algorithms on our machine (using one thread of a 2 800 MHz CPU and 256GB RAM). Additional
numerical experiments are available in the Appendix. Four real life graphs from the dataset [25]
are considered : the Facebook graph (4 039 nodes and 88 234 edges  extracted from the Facebook
social network)  the Youtube graph (1 134 890 nodes and 2 987 624 edges  extracted from the social
network included in the Youtube website)  the Amazon graph (the 334 863 nodes represent products
linked by and 925 872 edges) and the DBLP graph (a co-authorship network of 317 080 nodes and
1 049 866 edges). On the larger graphs  we only simulate SPLA and SSLA since the computation
of a full proximity operator becomes prohibitive. Numerical experiments over the Amazon and the
DBLP graphs are available in the Supplementary material.

Figure 2: Top row: The functional F = H + EU as a function of CPU time for the three algorithms
over the Facebook graph. Left: Y ∼ N (0  I). Right: Y ∼ N (0  I) and then half of the coordinates
of Y are put to zero. Bottom row: The functional F = H + EU as a function of CPU time for the
two algorithms over the Youtube graph. Left: Y ∼ N (0  I). Right: Y ∼ N (0  I) and then half of
the coordinates of Y are put to zero.

In our simulations  we represent the functional F = H+EU as a function of CPU time while running
the algorithms. The parameters λ and σ are chosen such that the log likelihood term and the Total
Variation regularization term have the same weight. The functionals H and EU are estimated using
ﬁve random realizations of each iterate ˆxk (H is estimated using a kernel density estimator). The
batch parameter n is equal to 400. We consider cases where Y has a standard gaussian distribution
and cases where half of the components of Y are standard gaussians and half are equal to zero (this
correspond to the graph inpainting task [11]). SPLA and SSLA are always simulated with the same
step size.
As expected  the numerical experiments show the advantage of using stochastic proximity operators
instead of stochastic subgradients.
It is a standard fact that proximity operators are better than
subgradients to tackle ℓ1-norm terms [2]. Our ﬁgures show that stochastic proximity operators are
numerically more stable than alternatives [43]. Our ﬁgures also show the advantage of stochastic
methods (SSLA or SPLA) over deterministic ones for large scale problems. The SSLA and the
SPLA provide iterates about one hundred times more frequently than ProxLA  and are faster in the
ﬁrst iterations.

9

References

[1] L. Ambrosio  N. Gigli  and G. Savaré. Gradient Flows: In Metric Spaces and in the Space of

Probability Measures. Springer Science & Business Media  2008.

[2] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in
Hilbert Spaces. CMS Books in Mathematics/Ouvrages de Mathématiques de la SMC. Springer 
New York  2011.

[3] E. Bernton. Langevin Monte Carlo and JKO splitting. arXiv preprint arXiv:1802.08671  2018.

[4] P. Bianchi. Ergodic convergence of a stochastic proximal point algorithm. SIAM Journal on

Optimization  26(4):2235–2260  2016.

[5] P. Bianchi and W. Hachem. Dynamical behavior of a stochastic Forward-Backward algo-
rithm using random monotone operators. Journal of Optimization Theory and Applications 
171(1):90–120  2016.

[6] P. Bianchi  W. Hachem  and A. Salim. A constant step Forward-Backward algorithm involving

random maximal monotone operators. Journal of Convex Analysis  26(2):397–436  2019.

[7] H. Brézis. Opérateurs maximaux monotones et semi-groupes de contractions dans les espaces

de Hilbert. North-Holland mathematics studies. Elsevier Science  Burlington  MA  1973.

[8] A. Chambolle  V. Caselles  D. Cremers  M. Novaga  and T. Pock. An introduction to total vari-
ation for image analysis. Theoretical foundations and numerical methods for sparse recovery 
9:263–340  2010.

[9] A. Chambolle and T. Pock. A ﬁrst-order primal-dual algorithm for convex problems with
applications to imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.

[10] N. S Chatterji  N. Flammarion  Y.-A. Ma  P. L Bartlett  and M. I Jordan. On the theory of
variance reduction for stochastic gradient Monte Carlo. arXiv preprint arXiv:1802.05431 
2018.

[11] S. Chen  A. Sandryhaila  G. Lederman  Z. Wang  J. MF Moura  P. Rizzo  J. Bielak  J. H Garrett 
and J. Kovaˇcevi´c. Signal inpainting on graphs via total variation minimization. In International
Conference on Acoustics  Speech  and Signal Processing  pages 8267–8271  2014.

[12] X. Cheng  N. S Chatterji  P. L Bartlett  and M. I Jordan. Underdamped Langevin MCMC: A

non-asymptotic analysis. arXiv preprint arXiv:1707.03663  2017.

[13] L. Condat. A primal-dual splitting method for convex optimization involving Lipschitzian 
proximable and linear composite terms. Journal of Optimization Theory and Applications 
158(2):460–479  2013.

[14] A. S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-
concave densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
79(3):651–676  2017.

[15] A. S Dalalyan and A. Karagulyan. User-friendly guarantees for the langevin monte carlo with

inaccurate gradient. Stochastic Processes and their Applications  2019.

[16] A. S Dalalyan and L. Riou-Durand. On sampling from a log-concave density using kinetic

Langevin diffusions. arXiv preprint arXiv:1807.09382  2018.

[17] D. Davis and W. Yin. A three-operator splitting scheme and its optimization applications.

Set-Valued and Variational Analysis  25(4):829–858  2017.

[18] A. Durmus  S. Majewski  and B. Miasojedow. Analysis of Langevin Monte Carlo via convex

optimization. Journal of Machine Learning Research  20(73):1–46  2019.

[19] A. Durmus and E. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin

algorithm. The Annals of Applied Probability  27(3):1551–1587  2017.

10

[20] A. Durmus  E. Moulines  and M. Pereyra. Efﬁcient Bayesian computation by proximal Markov
Chain Monte Carlo: when Langevin meets Moreau. SIAM Journal on Imaging Sciences 
11(1):473–506  2018.

[21] E. Hazan  K. Y Levy  and S. Shalev-Shwartz. On graduated optimization for stochastic non-
convex problems. In International conference on machine learning  pages 1833–1841  2016.

[22] Y.-P. Hsieh  A. Kavis  P. Rolland  and V. Cevher. Mirrored Langevin dynamics. In Advances

in Neural Information Processing Systems  pages 2878–2887  2018.

[23] S. Kotz  T. Kozubowski  and K. Podgorski. The Laplace distribution and generalizations: a
revisit with applications to communications  economics  engineering  and ﬁnance. Springer
Science & Business Media  2012.

[24] H. J. Kushner and G. G. Yin. Stochastic approximation and recursive algorithms and appli-
cations  volume 35 of Applications of Mathematics (New York). Springer-Verlag  New York 
second edition  2003. Stochastic Modelling and Applied Probability.

[25] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http:

//snap.stanford.edu/data  June 2014.

[26] Y.-A. Ma  N. Chatterji  X. Cheng  N. Flammarion  P. L Bartlett  and M. I Jordan. Is there an

analog of Nesterov acceleration for MCMC? arXiv preprint arXiv:1902.00996  2019.

[27] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for
In Advances in Neural Information Processing Systems  pages 451–459 

machine learning.
2011.

[28] R. M Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo 

2(11):2  2011.

[29] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends R(cid:13) in Optimization 

1(3):127–239  2014.

[30] G. B. Passty. Ergodic convergence to a zero of the sum of monotone operators in Hilbert space.

Journal of Mathematical Analysis and Applications  72(2):383–390  1979.

[31] A. Patrascu and I. Necoara. Nonasymptotic convergence of stochastic proximal point algo-
rithms for constrained convex optimization. Journal of Machine Learning Research  18:1–42 
2018.

[32] P. Richtárik and M. Takáˇc. Stochastic reformulations of linear systems: algorithms and con-

vergence theory. arXiv:1706.01108  2017.

[33] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical

Statistics  22(3):400–407  1951.

[34] R. T. Rockafellar and R. J.-B. Wets. On the interchange of subdifferentiation and conditional

expectations for convex functionals. Stochastics  7(3):173–182  1982.

[35] L. Rosasco  S. Villa  and B. C. V˜u. Stochastic inertial primal-dual algorithms. arXiv preprint

arXiv:1507.00852  2015.

[36] A. Salim  P. Bianchi  and W. Hachem. Snake: a stochastic proximal gradient algorithm for

regularized problems over large graphs. IEEE Transactions on Automatic Control  2019.

[37] F. Santambrogio. {Euclidean  metric  and Wasserstein} gradient ﬂows: an overview. Bulletin

of Mathematical Sciences  7(1):87–154  2017.

[38] S. Schechtman  A. Salim  and P. Bianchi. Passty Langevin. In CAp  2019.

[39] U. ¸SimŠekli. Fractional Langevin Monte Carlo: Exploring Lévy driven stochastic differential
equations for Markov Chain Monte Carlo. In International Conference on Machine Learning 
pages 3200–3209  2017.

11

[40] P. Sollich. Bayesian methods for support vector machines: Evidence and predictive class

probabilities. Machine learning  46(1-3):21–52  2002.

[41] W. Tansey and J. G Scott. A fast and ﬂexible algorithm for the graph-fused lasso. arXiv

preprint arXiv:1505.06475  2015.

[42] R. J. Tibshirani. Adaptive piecewise polynomial estimation via trend ﬁltering. The Annals of

Statistics  42(1):285–323  2014.

[43] P. Toulis  T. Horel  and E. M Airoldi. Stable Robbins-Monro approximations through stochastic

proximal updates. arXiv preprint arXiv:1510.00967  2015.

[44] T. Van Erven and P. Harremos. Rényi divergence and Kullback-Leibler divergence.

Transactions on Information Theory  60(7):3797–3820  2014.

IEEE

[45] C. Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media 

2008.

[46] B. C. V˜u. A splitting algorithm for dual monotone inclusions involving cocoercive operators.

Advances in Applied Mathematics  38(3):667–681  2013.

[47] Y.-X. Wang  J. Sharpnack  A. J Smola  and R. J Tibshirani. Trend ﬁltering on graphs. The

Journal of Machine Learning Research  17(1):3651–3691  2016.

[48] M. Welling and Y. W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In

International Conference on Machine Learning  pages 681–688  2011.

[49] A. Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as

a composite optimization problem. arXiv preprint arXiv:1802.08089  2018.

[50] X. Xu and M. Ghosh. Bayesian variable selection and estimation for group lasso. Bayesian

Analysis  10(4):909–936  2015.

[51] A. Yurtsever  B. C. Vu  and V. Cevher. Stochastic three-composite convex minimization. In

Advances in Neural Information Processing Systems  pages 4329–4337  2016.

[52] D. Zou  P. Xu  and Q. Gu. Subsampled stochastic variance-reduced gradient langevin dynam-

ics. In International Conference on Uncertainty in Artiﬁcial Intelligence  2018.

[53] D. Zou  P. Xu  and Q. Gu. Sampling from non-log-concave distributions via variance-reduced
gradient langevin dynamics. In The 22nd International Conference on Artiﬁcial Intelligence
and Statistics  pages 2936–2945  2019.

12

,Jiechuan Jiang
Zongqing Lu
Adil SALIM
Dmitry Koralev
Peter Richtarik