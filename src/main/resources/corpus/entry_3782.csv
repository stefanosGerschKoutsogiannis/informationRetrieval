2019,Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs,This paper establishes that optimistic algorithms attain gap-dependent and non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work  our bounds do not suffer a dependence on diameter-like quantities or ergodicity  and smoothly interpolate between the gap dependent logarithmic-regret  and the $\widetilde{\mathcal{O}}(\sqrt{HSAT})$-minimax rate. The key technique in our analysis is a novel ``clipped'' regret decomposition which applies to a broad family of recent optimistic algorithms for episodic MDPs.,Non-Asymptotic Gap-Dependent Regret Bounds for

Tabular MDPs

Max Simchowitz

UC Berkeley

msimchow@berkeley.edu

Kevin Jamieson

University of Washington

jamieson@cs.washington.edu

Abstract

This paper establishes that optimistic algorithms attain gap-dependent and non-
asymptotic logarithmic regret for episodic MDPs.
In contrast to prior work 
our bounds do not suffer a dependence on diameter-like quantities or ergodic-
ity  and smoothly interpolate between the gap dependent logarithmic-regret  and
HSAT )-minimax rate. The key technique in our analysis is a novel
“clipped” regret decomposition which applies to a broad family of recent opti-
mistic algorithms for episodic MDPs.

the (cid:101)O(

√

1

Introduction

Reinforcement learning (RL) is a powerful paradigm for modeling a learning agent’s interactions
with an unknown environment  in an attempt to accumulate as much reward as possible. Because
of its ﬂexibility  RL can encode such a vast array of different problem settings - many of which are
entirely intractable. Therefore  it is crucial to understand what conditions enable an RL agent to
effectively learn about its environment  and to account for the success of RL methods in practice.
In this paper  we consider tabular Markov decision processes (MDPs)  a canonical RL setting where
the agent seeks to learn a policy mapping discrete states x ∈ S to one of ﬁnitely many actions a ∈ A 
in an attempt to maximize cumulative reward over an episode horizon H. We shall study the regret
setting  where the learner plays a policy πk for a sequence of episodes k = 1  . . .   K  and suffers a
regret proportional to the average sub-optimality of the policies π1  . . .   πK.
In recent years  the vast majority of literature has focused on obtaining minimax regret bounds that
match the worst-case dependence on the number states |S|  actions |A|  and horizon length H;

namely  a cumulative regret of(cid:112)H|S||A|T   where T = KH denotes the total number of rounds of

the game [1]. While these bounds are succinct and easy to interpret  they paint an overly pessimistic
account of the complexity of these problems  and do not elucidate the favorable structural properties
of which a learning agent can hope to take advantage.
The earlier literature  on the other hand  establishes a considerable more favorable regret of the
form C log T   where C is an instance-dependent constant given in terms of the sub-optimality gaps
associated with each action at a given state  deﬁned as

(x) − Qπ(cid:63)

gap∞(x  a) = Vπ(cid:63)

(1)
where Vπ(cid:63) and Qπ(cid:63) denote the value and Q functions for an optimal policy π(cid:63)  and the subscript-∞
denotes these bounds hold for a non-episodic  inﬁnite horizon setting. Depending on the constant
C  the regret C log T can yield a major improvement over the
T minimax scaling. Unfortu-
nately  these analyses are asymptotic in nature  and only take effect after a large number of rounds 
depending on other potentially-large  highly-conservative  or difﬁcult-to-verify problem-dependent
quantities such as hitting times or measures of uniform ergodicity [8  13  10].

(x  a) 

√

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

To fully account for the empirical performance of RL algorithms  we seek regret bounds which
take advantage of favorable problem instances  but apply in ﬁnite time and for practically realistic
numbers of rounds T .
Contributions: As a ﬁrst step in this direction  [14] introduced a novel algorithm called EULER 
which enjoys reduced dependence on the episode horizon H for favorable instances  while main-
taining the same worst-case dependence for other parameters in their analysis as in [1].
In this paper  we take the next step by demonstrating that a common class of algorithms for solving
MDPs  based on the optimism principle  attains gap-dependent  problem-speciﬁc bounds similar to
those previously found only in the asymptotic regime. For concreteness  we specialize our analysis
to a minor modiﬁcation of the EULER algorithm we call StrongEuler; as we explain in Section 3 
our analysis extends more broadly to other optimistic algorithms as well. We show that
• For any episodic MDP M  StrongEuler enjoys a high probability regret bound of CM log(1/δ)
for all rounds T ≥ 1  where the constant CM depends on the sub-optimality gaps between actions
at different states  as well as the horizon length  and contains an additive almost-gap-independent
term that scales as AS2poly(H) (Corollary 2.1).

Unlike previous gap-dependent regret bounds 
• The constant CM does not suffer worst-case dependencies on other problem dependent quantities
such as mixing times  hitting times or measures of ergodicity. However  the constant CM does
take advantage of benign problem instances (Deﬁnition 2.2).
• The regret bound of CM log(1/δ) is valid for any total number of rounds T ≥ 1. Selecting
δ = 1/T   this implies a non-asymptotic expected regret bound of CM log T 1.

max regret (cid:101)O((cid:112)H|S||A|T )  the latter of which may be sharper for smaller T (Theorem 2.4).

• The regret of StrongEuler interpolates between instance-dependent regret CM log T and mini-

Following [14]  this dependence on H may also be reﬁned for benign instances.

Lastly  while the StrongEuler algorithm affords sharper regret bounds than past algorithms  our
analysis techniques extend more generally to other optimism based algorithms:
• We introduce a novel “clipped” regret decomposition (Proposition 3.1) which applies to a broad

family of optimistic algorithms  including the algorithms analyzed in [14  6  5  9  1].

• Following our analysis of StrongEuler  the clipped regret decomposition can establish analogous

gap-dependent log T -regret bounds for many of the algorithms mentioned above.

What is CM? In many settings  we show that CM is dominated by an analogue to the sum over
the reciprocals of the gaps deﬁned in (1). This is known to be optimal for non-dynamic MDP
settings like contextual bandits  and we prove a lower bound (Proposition 2.2) which shows that this
is unimprovable for general MDPs as well. Furthermore  building on [14]  we show this adapts to
problems with additional structure  yielding  e.g.  a horizon H-free bound for contextual bandits.
However  our gap-dependent bound also suffers from a certain dependence on the smallest nonzero
gap gapmin (see Deﬁnition 2.1)  which may dominate in some settings. We prove a lower
bound (Theorem 2.3) which shows that optimistic algorithms in the recent literature - including
StrongEuler - necessarily suffer a similar term in their regret. We believe this insight will motivate
new algorithms for which this dependence can be removed  leading to new design principles and
actionable insights for practitioners. Finally  our regret bound incurs an (almost) gap-independent
burn-in term  which is standard for optimistic algorithms  and which we believe is an exciting direc-
tion of research to remove.
Altogether  we believe that the results in our paper serve as a preliminary but signiﬁcant step to
attaining sharp  instance-dependent  and non-asymptotic bounds for tabular MDPs  and hope that
our analysis will guide the design of future algorithms that attain these bounds.

1.1 Related Work

Like the multi-armed bandit setting  regret bounds for MDP algorithms have been characterized both
in gap-independent forms that rely solely on S := |S|  A := |A|  H  T   and in gap-dependent forms
1By this  we mean that for any ﬁxed T ≥ 1  one can attain CM log T regret. Extending the bound to

anytime regret is left to future work

2

√

gap∗

√

√

a (cid:101)O(

HSAT   matching the known lower bound of

D2S2AT ) gap-independent regret  and an (cid:101)O( D2S2A

which take into account the gaps (1)  as well as other instance-speciﬁc properties of the rewards and
transition probabilities.
Finite Sample Bounds  Gap-Independent Bounds: A number of notable recent works give undis-
counted regret bounds for ﬁnite-horizon  tabular MDPs  nearly all of them relying on the principle
of optimism which we describe in Section 3 [4  1  5  9  14]. Many of the more recent works [1  14  6]
attain a regret of
HSAT established in [11  8  4].
As mentioned above  the EULER algorithm of [14] attains the minimax rates and simultaneously en-
joys a reduced dependence on H in benign problem instances  such as the contextual bandits setting
where the transition probabilities do not depend on the current state or learners actions  or when the
total cumulative rewards over any roll-out are bounded by 1 in magnitude.
Diameter Dependent Bounds: In the setting of inﬁnite horizon MDPs with discounted regret 
many previous works have established logarithmic regret bounds of the form C(M) log T   where
C(M) is a constant depending on the underlying MDP. Notably  [8] give an algorithm which attains
log(T )) gap-dependent regret bound 
where gap∗ is the difference between the mean inﬁnite-horizon reward of π∗ and the next-best
stationary policy  and where D denotes the maximum expected traversal time between any two
states x  x(cid:48)  under the policy which attains the minimal traversal time between those two states.
We note that if gap∞(x  a) denotes the sub-optimality of any action a at state x as in (1)  then
gap∗ ≤ minx a gap∞(x  a). The bounds in this work  on the other hand  depend on an average
over inverse gaps  rather than a worst case. Moreover  the diameter D can be quite large when there
exist difﬁcult-to-access states. We stress that the bound due to [8] is non-asympotic  but the bound
in terms of gap∗ dependences other worst-case quantities measuring ergodicity.
Asymptotic Bounds: Prior to [8]  and building on the bounds of [3]  [13] presented bounds in
terms of a diameter-related quantity ¯D ≥ D  which captures the minimal hitting time between
[13] prove that their algorithm enjoys a regret2 of
states when restricted to optimal policies.
gap∞(x a) log(T ) asymptotically in T where CRIT contains those sub-optimal state-
action pairs (x  a) such that a can be made to the the unique  optimal action at x by replacing
p(s(cid:48)|s  a) with some other vector on the S-simplex. Recently  [10] present per-instance lower bounds
for both structured and unstructured MDPs  which apply to any algorithm which enjoys sub-linear
regret on any problem instance  and an algorithm which matches these bounds asymptotically. This
bound replaces ¯D2 with ¯H 2  where ¯H denotes the range of the bias functions  an analogue of H
for the non-episodic setting [2]. We further stress that whereas the logarithmic regret bounds of [8]
hold for ﬁnite time with polynomial dependence on the problem parameters  the number of episodes
needed for the bounds of [3  13  10] to hold may be exponentially large  and depend on additional 
pessimistic problem-dependent quantities (e.g. a uniform hitting time in Proposition 29 in [12]).
Novelty of this work: The major contribution of our work is showing problem-dependent log(T )
regret bounds which i) attain a reﬁned dependence on the gaps  as in [13]  ii) apply in ﬁnite time
after a burn-in time only polynomial in S  A  H and the gaps  iii) depend only on H and not on
the diameter D (and thus  are not adversely affected by difﬁcult to access states)  and iv) smoothly
interpolate between log T regret and the minimax

HSAT rate attained by [1] et seq.

(s a)∈CRIT

(cid:80)

¯D2

√

1.2 Problem Setting  Notation  and Organization
Episodic MDP: A stationary  episodic MDP is a tuple M := (S A  H  r  p  p0  R)  where for
each x ∈ S  a ∈ A we have that R(x  a) ∈ [0  1] is a random reward with expectation r(x  a) 
p : S × A → ∆S denotes transition probabilities  p0 ∈ ∆S is an initial distribution over states 
and H is the horizon  or length of the episode. A policy π is a sequence of mappings πh : S → A.
For our given MDP M  we let Eπ and Pπ denote the expectation and probability operator with
respect to the law of sequence (x1  a1)  . . .   (xH   aH )  where x1 ∼ p0  ah = πh(xh)  xh+1 ∼
and for h ∈ [H] and x ∈ S 
p(xh  ah). We deﬁne the value of π as Vπ
h(cid:48)≥h r(xh(cid:48)  aa(cid:48)) | xh = x
  which we identify with a vector in RS. We deﬁne
so that

Vπ
the associated Q-function Qπ : S × A → R  Qπ

h(x) := Eπ(cid:104)(cid:80)H

0 := Eπ(cid:104)(cid:80)H
(cid:105)

h(x  a) := r(x  a) + p(x  a)(cid:62)Vπ

h=1 r(xh  ah)

h+1 

(cid:105)

2[13] actually presents a bound of the form

the claimed form from the proof.

¯D2SA

min(s a)∈CRIT gap∞(x a) log(T ) but it is straightforward to extract

3

h = Vπ

h = Qπ

h and Q(cid:63)

h and Q(cid:63)

h(x). We denote the set of optimal policies π(cid:63) := arg maxπ Vπ

0   and let
Qπ
h(x  πh(x)) = Vπ
h(x) := {a : πh(x) = a  π ∈ π(cid:63)} denote the set of optimal actions. Lastly  given any optimal
π(cid:63)
π ∈ π(cid:63)  we introduce the shorthand V(cid:63)
h  where we note that even when π is
not unique  V(cid:63)
Episodic Regret: We consider a game that proceeds in rounds k = 1  . . .   K  where at each state
an algorithm Alg selects a policy πk  and observes a roll out (x1  a1)  . . .   (xH   aH ) ∼ Pπk. The

goal is to minimize the cumulative simple regret  deﬁned as RegretK :=(cid:80)K

h do not depend on the choice of optimal policy.

Notation and Organization: For n ∈ N  we deﬁne [n] = {1  . . .   n}. For two expressions f  g that
are functions of any problem-dependent variables of M  we say f (cid:46) g (f (cid:38) g  respectively) if there
exists a universal constant c > 0 independent of M such that f ≤ cg (f ≥ cg  respectively). (cid:47)
will denote an informal  approximate inequality. Section 2 presents our main results  and Section 3
sketches the proof and highlights the novelty of our techniques. All references to the appendix refer
to the appendix of the supplement. All formal proofs  and many rigorous statement of results  are
deferred to the appendix  whose organization and notation are described at length in Appendix A.

0 − Vπk
0 .

k=1 V(cid:63)

√

nk(x a)

1.3 Optimistic Algorithms
Lastly  we introduce optimistic algorithms which select a policy which is optimal for an over-
estimated  or optimistic  estimate of the true Q-function  Q(cid:63).
Deﬁnition 1.1 (Optimistic Algorithm). We say that an algorithm Alg satisifes optimism if  for each
round k ∈ [K] and stage h ∈ [H]  it constructs an optimistic Q-function Qk h(x  a) and policy
πk = (πk h) satisfying ∀x  a : Qk H+1(x  a) = 0  Qk h(x  a) ≥ Q(cid:63)
h(x  a)  and πk h(x) ∈
arg maxa Qk h(x  a). The associated optimistic value function is Vk h(x) := Qk h(x  πk h(x)).
We shall colloquially refer to an algorithm as optimistic if it satsiﬁes optimism with high prob-
ability. Optimism has become the dominant approach for learning ﬁnite-horizon MDPs  and all
recent low-regret algorithms are optimistic [5  6  1  14  9]. In model-based algorithms  the overesti-

mates Qk h are constructed recursively as Qk h(x  a) =(cid:98)rk(x  a) +(cid:98)pk(x  a)(cid:62)Vk h+1 + bk h(x  a) 
where(cid:98)rk(x  a) and (cid:98)pk(x  a) are empirical estimates of the mean rewards and transition probabil-
ities  and bk h(x  a) ≥ 0 is a conﬁdence bonus designed to ensure that Qk h(x  a) ≥ Q(cid:63)(x  a).
bk h(x  a) (cid:104)(cid:113) H log(SAHK/δ)
Letting nk(x  a) denote the total number of times a given state-action pair is visited  a simple bonus
sufﬁces to induce optimism  yielding the UCBVI-CH algorithm of
[1]. This leads to an episodic regret bound of
H greater than the minimax
rate. More reﬁned bonuses based on the “Bernstein trick” achieve the optimal H-dependence [1] 
and the EULER algorithm of [14] adopts further reﬁnements to replace worst-case H dependence
with more adaptive quantities. The StrongEuler algorithm considered in this work applies similarly
adaptive bonuses  but our analysis extends to all aforementioned bonus conﬁgurations. We remark
that there are also model-free optimistic algorithms based on Q-learning (see  e.g. [9]) that construct
overestimates in a slightly different fashion. While our main technical contribution  the clipped re-
gret decomposition (Proposition 3.1)  applies to all optimistic algorithms  our subsequent analysis is
tailored to model-based approaches  and may not extend straightforwardly to Q-learning methods.
2 Main Results
Logarithmic Regret for Optimistic Algorithms: We now state regret bounds that describe the per-
formance of StrongEuler  an instance of the model-based  optimistic algorithms described above.
StrongEuler is based on carefully selected bonuses from [14]  and formally instantiated in Algo-
rithm 1 in Appendix E. We emphasize that other optimistic algorithms enjoy similar regret bounds 
but we restrict our analysis to StrongEuler to attain the sharpest H-dependence.The key quantities
at play are the suboptimality-gaps between the Q-functions:
Deﬁnition 2.1 (Suboptimality Gaps). For h ∈ [H]  deﬁne the stage-dependent suboptimality gap
h(x  a)  as well as the minimal stage-independent gap gap(x  a) :=
gaph(x  a) := V(cid:63)
minh gaph(x  a)  and the minimal gap gapmin := minx a h{gaph(x  a) : gaph(x  a) > 0}.
Note that any optimal a(cid:63) ∈ π(cid:63)
h(x  a(cid:63)) = maxa Q(cid:63)
h(x)  and thus gaph(x  a(cid:63)) = 0 iff a(cid:63) ∈ π(cid:63)
V(cid:63)
benign problem settings which afford an improved dependence on the horizon H:

h(x  a)=
h(x). Following [14]  we consider two illustrative

h(x) satisﬁes the Bellman equation Q(cid:63)

H 2SAT   a factor of

h(x) − Q(cid:63)

√

4

 (cid:88)

RegretK

(cid:46)

H 3

(x a)∈Zsub

gap(x  a)
+ H 4SA(S ∨ H) log

log

M T

δ

M H
gapmin

 +

M T

.

H 3|Zopt|
gapmin

log

M T

δ

Deﬁnition 2.2 (Benign Settings). We say that an MDP M is a contextual bandit instance if
p(x(cid:48)|x  a) does not depend on x or a. An MDP M has G-bounded rewards if  for any policy π 

(cid:80)H
h=1 R(xh  ah) ≤ G holds with probability 1 over trajectories ((xh  ah)) ∼ Pπ.

Lastly  we deﬁne Zopt as the set of pairs (x  a) for which a is optimal at x for some stage h ∈ [H]:
Zopt := {(x  a) : ∃h ∈ [H] with a ∈ π(cid:63)
h(x)} and its complement Zsub := S × A − Zopt. Note
that typically |Zopt|(cid:46) H|S| or even |Zopt|(cid:46) |S| (see Remark B.2 in the appendix). We now state
our ﬁrst result  which gives a gap-dependent regret bound that scales as log(1/δ) with probability
at least 1 − δ. The result is a consequence of a more general result stated as Theorem 2.4  itself a
simpliﬁed version of more precise bounds stated in Appendix B.1.
Corollary 2.1. Fix δ ∈ (0  1/2)  and let A = |A|  S = |S|  M = (SAH)2. Then with probability
at least 1 − δ  StrongEuler run with conﬁdence parameter δ enjoys the following regret bound for
all K ≥ 1:

(2)
Moreover  if M is either a contextual bandits instance  or has G-bounded rewards for G (cid:46) 1  then
the factors of H 3on the ﬁrst line can be sharped to H. In addition  if M is a contextual bandits
instance  the factor of H 3 in the ﬁrst term (summing over (x  a) ∈ Zsub) can be sharped to 1.

log

δ

Setting δ = 1/T and noting that(cid:80)K
[0  1])  we see that the expected regret E[(cid:80)K

k=1 V(cid:63)

0 − Vπk
k=1 V(cid:63)

0 ≤ KH = T with probability 1 (recall R(x  a) ∈
0 ] can be bounded by replacing 1/δ with

0 − Vπk

T in right hand side of the inequality (2); this yields an expected regret that scales as log T .
Three regret terms: The ﬁrst term in Corollary 2.1 reﬂects the sum over sub-optimal state-action
pairs  which a lower bound (Proposition 2.2) shows is unimprovable in general.
In the inﬁnite
horizon setting  [10] gives an algorithm whose regret is asymptotically bounded by an analogue of
this term. The third term characterizes the burn-in time suffered by nearly all model-based ﬁnite-time
analyses and is the number of rounds necessary before standard concentration of measure arguments
kick in. The second term is less familiar and is addressed in Section 2.2 below.
H dependence:Comparing to known results from the inﬁnite-horizon setting  one expects the op-
timal dependence of the ﬁrst term on the horizon to be H 2. However  we cannot rule out that
the optimal dependence is H 3 for the following three reasons: (i) the inﬁnite-horizon analogues
D  ¯D  ¯H (Section 1.1) are not directly comparable to the horizon H; (ii) in the episodic setting  we
h for each h ∈ [H]  whereas the value functions of the
have a potentially different value function V(cid:63)
inﬁnite horizon setting are constant across time; (iii) the H 3 may be unavoidable for non-asymptotic
(in T ) bounds  even if H 2 is the optimal asymptotic dependence after sufﬁcient burn-in (possibly
depending on diameter-like quantities). Resolving the optimal H dependence is left as future work.
We also note that for contextual bandits  we incur no H dependence on the ﬁrst term; and thus the
ﬁrst term coincides with the known asymptotically optimal (in T )  instance-speciﬁc regret [7].
Guarantees for other optimistic algorithms: To make the exposition concrete  we only provide
regret bounds for the StrongEuler algorithm. However  the “gap-clipping” trick (Proposition 3.1)
and subsequent analysis template described in Section 3.1 can be applied to obtain similar bounds
for other recent optimistic algorithms  as in [1  5  14  6].3
2.1 Sub-optimality Gap Lower Bound
Our ﬁrst lower bound shows that when the total number of rounds T = KH is large  the ﬁrst term
of Corollary 2.1 is unavoidable in terms of regret. Speciﬁcally  for every possible choice of gaps 
there exists an instance whose regret scales on the order of the ﬁrst term in (2).
Following standard convention in the literature  the lower bound is stated for algorithms which have
sublinear worst case regret. Namely  we say than an algorithm Alg is α-uniformly good if  for any
MDP instance M  there exists a constant CM > 0 such that EM[RegretK] ≤ CMK α for all K.4
3To achieve logarithmic regret  some of these algorithms require a minor modiﬁcation to their conﬁdence

intervals; otherwise  the gap-dependent regret scales as log2 T . See Appendix E for details.

4We may assume as well that Alg is allowed to take the number of episodes K as a parameter.

5

Proposition 2.2 (Regret Lower Bound). Let S ≥ 2  and A ≥ 2  and let {∆x a}x a∈[S]×[A] ⊂
(0  H/8) denote a set of gaps. Then  for any H ≥ 1  there exists an MDP M with states S = [S +2] 
actions A = [A]  and H stages  such that 

gap1(x  a) = ∆x a 
gaph(x  a) ≥ 1/2 

∀x ∈ [S]  a ∈ A
∀x ∈ {S + 1  S + 2}  a ∈ A − {1} 

and any α-uniformly good algorithm satisﬁes

EM[RegretK]

log T

lim
K→∞

(cid:38) (1 − α)

(cid:88)

H 2

gap1(x  a)

x a:gap1(x a)>0

The above proposition is proven in Appendix H  using a construction based on [4]. For simplicity 
we stated an asymptotic lower bound. We remark that if the constant CM is poly(|S| |A|  H)  then
one can show that the above asymptotic bound holds as soon as K ≥ (|S||A|H/gap∗)O(1/(1−α)) 
where gap∗ := {min gap1(x  a) : gap1(x  a) > 0}. More reﬁned non-asymptotic regret bounds
can be obtained by following [7].
2.2 Why the dependence on gapmin?
ithout the second term  Corollary 2.1 would only suffer one factor of 1/gapmin due to the sum
over state-actions pairs (x  a) ∈ Zsub (when the minimum is achieved by a single pair). However  as
remarked above  |Zopt| typically scales like |S| and therefore the second term scales like |S|/gapmin 
with a dependence on 1/gapmin that is at least a factor of |S| more than we would expect. Here  we
show that |S|/gapmin is unavoidable for the sorts of optimistic algorithms that we typically see in
the literature; a rigorous proof is deferred to Appendix G.
Theorem 2.3 (Informal Lower Bound). Fix δ ∈ (0  1/8). For universal constants c1  c2  c3  c4 
if  ∈ (0  c1)  and S satisﬁes c2 log(−1/δ) ≤ S ≤ c3−1/log(−1/δ)  there exists an MDP
1/2. For this MDP  (cid:80)
with |S|= S  |A|= 2 and horizon H = 2  such that exactly one state has a sub-optimality
gap of gapmin =  and all other states have a minimum sub-optimality gap gaph(x  a) ≥
but all existing optimistic algo-
(cid:80)
log(1/δ) (cid:38)

rithms for ﬁnite-horizon MDPs which are δ-correct suffer a regret of at least

(cid:46) S + 1

with probability at least 1 − c4δ.

h x a:gaph(x a)>0

gaph(x a)

gapmin

gapmin

S

1

log(1/δ)

gaph(x a) + S log(1/δ)

gapmin

h x a:gaph(x a)>0

Interpolating with Minimax Regret for Small T

The particular instance described in Appendix G that witnesses this lower bound is instructive be-
cause it demonstrates a case where optimism results in over-exploration.
2.3
We remark that while the logarithmic regret in Corollary 2.1 is non-asymptotic  the expression can be
loose for a number of rounds T that is small relative to the sum of the inverse gaps. Our more general
result interpolates between the log T gap-dependent and
Theorem 2.4 (Main Regret Bound for StrongEuler). Fix δ ∈ (0  1/2)  and let A = |A|  S = |S| 
M = (SAH)2. Futher  deﬁne for all  > 0 the set Zsub() := {(x  a) ∈ Zsub : gap(x  a) <
}. Then with probability at least 1 − δ  StrongEuler run with conﬁdence parameter δ enjoys the
following regret bound for all K ≥ 2:

T gap-independent regret regimes.

√

where the second inequality follows from the ﬁrst with max{max|Zsub()| |Zopt|} ≤ SA. More-
over  if M is an instance of contextual bandits  then the factors of H under the square roots can be
reﬁned to a 1  and if M has (cid:46) 1-bounded rewards  then these same factors of H can be replaced by
a 1/H. In both settings  logarithmic terms can be reﬁned as in Corollary 2.1.

6

RegretK

(cid:46) min

>0

(cid:88)

(cid:110)(cid:113)|Zsub()|H T (log T ) log M T
(cid:26)(cid:113)|Zopt| H T (log T ) log M T
(cid:110)

+ min
+ H 4SA(S ∨ H) min log M T

δ } +

(x a)∈Zsub\Zsub()

(cid:111)
δ   |Zopt| H 3
gapmin
δ   log M H
gapmin
δ ) + H 4SA(S ∨ H) log2 T M

log M T

δ

δ

 

HSAT log(T ) log( M T

(cid:46)(cid:113)

log(cid:0) M T

δ

(cid:1)(cid:111)

H 3

(cid:1)(cid:27)

gap(x  a)

log(cid:0) M T

δ

√

By the same argument as above  Theorem 2.4 with δ = 1/T implies an expected regret scaling like
gap-dependent log T or worst-case
HSAT . In Appendix B.1  we state a more reﬁned bound given
in terms of the reward bound G  and the maximal variance of any state-action pair (Theorem B.2).
3 Gap-Dependent bounds via ‘clipping’
In this section  we (i) introduce the key properties of optimistic algorithms  (ii) explain existing
approaches to the analysis of such algorithms  and (iii) introduce the “clipping trick”  and sketch
how this technique yields gap-dependent  non-asymptotic bounds.
Deﬁnition 3.1 (Optimistic Surplus). Given an optimistic algorithm Alg  we deﬁne the (optimistic)
surplus Ek h(x  a) := Qk h(x  a) − r(x  a) − p(x  a)(cid:62)Vk h+1.
Alg is strongly optimistic if
Ek h(x  a) ≥ 0 for all k ≥ 1  and (x  a  h) ∈ S ×A× [H]  which implies that Alg is also optimistic.
While the nomenclature “suplus” is unique to our work  surplus-like terms arise in many prior regret
analyses [5  14]. The notion of strong optimism is novel to this work  and facilitates a sharper H-
dependence in contextual bandit setting of Deﬁnition 2.2; intuitively  strong optimism means that
the Q-function Qk h at stage h over-estimates Q(cid:63)
The Regret Decomposition for Optimistic Algorithms: Under optimism alone  we can see that
for any h and any a(cid:63) ∈ π(cid:63)(x) 
Vk h(x) = max

h more than Qk h+1 does Q(cid:63)

k h+1.

a

Qk h(x  a) ≥ Qk h(x  a(cid:63)) ≥ Q(cid:63)
0 − Vπk

h(x  a(cid:63)) = V(cid:63)
h(x) 
0 ≤ Vk 0 − Vπk
0 .

k=1

k=1

h=1

(cid:80)

H(cid:88)

Vk 0 − Vπk

h=1  Vk 0 − Vπk
K(cid:88)

0 = Eπk [(cid:80)H
0 ≤ K(cid:88)

and therefore  we can bound the sub-optimality of πk as V(cid:63)
We can decompose the regret further by introducing the following notation: we let ωk h(x  a) :=
Pπk [(xh  ah) = (x  a)] denote the probability of visiting x and playing a at time h in episode
A standard regret decomposition (see e.g. Lemma E.15 [5]) then shows that for a trajec-
k.
tory (xh  ah)H
x a ωk h(x  a)Ek h(x  a) 
yielding a regret bound of
0 − Vπk
V(cid:63)

h=1 Ek h(xh  ah)] =(cid:80)H
(cid:88)
0 ≤ K(cid:88)
(cid:80)
carefully manipulate the surpluses Ek h(x  a) to show that (cid:80)H
(cid:80)H
(cid:80)
Finally  they replace nk(x  a) with an “idealized analogue”  nk(x  a) :=(cid:80)k
(cid:80)k
j=1 ωj(x  a)  where we introduce ωj(x  a) :=(cid:80)H

Existing Analysis of MDPs: We begin by sketching the ﬂavor of minimax analyses.
Intro-
ducing the notation nk(x  a) := {#times (x  a) is visited before episode k}  existing analyses
x a ωk h(x  a)Ek h(x  a) (cid:46)
+ lower order terms  where typically CM = poly(H  log(T /δ).

h=1 ωj h(x  a) :=
h=1 ωj h(x  a) denote the expected number of
visits of (x  a) at episode j. Letting {Fk} denote the ﬁltration capturing all events up to the end
episode k  we see that E[nk(x  a) − nk−1|Fk−1] = ωk(x  a)  and thus by standard concentration
arguments (see Lemma B.7  or Lemma 6 in [6])  nk(x  a) and nk(x  a) are within a constant factor
of each other for all k such that nk(x  a) is sufﬁciently large. Hence  by replacing nk(x  a) with
nk(x  a)  we have (up to lower order terms)

ωk h(x  a)Ek h(x  a).

x a ωk h(x  a)

(cid:80)H

CM√

nk(x a)

h=1

h=1

h=1

k=1

j=1

x a

0 − Vπk
V(cid:63)

0

x a

k=1

k=1

the

ωk(x  a)

(cid:80)H

+ lower order terms.

A(cid:112)SAK poly(H) bound is typically concluded using a careful application of Cauchy-Schwartz 
(cid:80)

and an integration-type lemma (e.g.  Lemma C.1). An analysis of this ﬂavor is used in Appendix B.4.
On

exactly
=
Then one can achieve a gap dependent bound as soon
as one can show that the algorithm ceases to select suboptimal actions a at (x  h) after sufﬁciently
large T . Crucially  determining if action a is (sub)optimal at (x  h) requires precise knowledge
about the value function at other states in the MDP at future stages h(cid:48) > h. This difﬁculty is
why previous gap-dependent analyses appeal to diameter or ergodicity assumptions  which ensure
sufﬁcient uniform exploration of the MDP to reason about the value function at subsequent stages.

h=1 ωk h(x  a)gaph(x  a).

0 − Vπk

identity V(cid:63)

establish

hand 

other

one

can

the

(3)

x a

0

K(cid:88)

(cid:46) (cid:88)

K(cid:88)

CM(cid:112)nk(x  a)

7

3.1 The Clipping Trick

We now introduce the “clipping trick”  a technique which merges both the minimax analysis in terms
of the surpluses Ek h(x  a)  and the gap-dependent strategy  which attempts to control how many
times a given suboptimal action is selected. Core to our analysis  deﬁne the clipping operator

clip [x| ] = xI{x ≥ } 

0

for all x   > 0. We can now state our ﬁrst main technical result  which states that the sub-optimality
0 − Vπk
can be controlled by a sum over surpluses which have been clipped to zero whenever
V(cid:63)
they are sufﬁciently small.
Proposition 3.1. Let ˇgaph(x  a) := gapmin
algorithm with surpluses Ek h(x  a) 

4H . Then  if πk is induced by an optimistic

2H ∨ gaph(x a)
(cid:88)

H(cid:88)

0 − Vπk
V(cid:63)

0 ≤ 2e

ωk h(x  a) clip [Ek h(x  a)| ˇgaph(x  a)] .

h=1

x a

2H ∨ gaph(x a)

If the algorithm is strongly optimistic  and M is a contextual bandits instance  we can replace
ˇgaph(x  a) with ˇgaph(x  a) := gapmin
The above proposition is a consequence of a more general bound  Theorem B.3  given in Ap-
pendix B. Unlike gap-dependent bounds that appeal to hitting-time arguments  we do not reason
about when a suboptimal action a will cease to be taken. Indeed  an algorithm may still choose a
suboptimal action a even if the surplus Ek h(x  a) is small  because future surpluses may be large.
Instead  we argue in two parts:

.

4

1. A sub-optimal action a /∈ π(cid:63)

h(x) is taken only if Qk h(x  a) ≥ Q(cid:63)

h(x  a(cid:63)) for some
h(x)  or equivalently in terms of the surplus  only if Ek h(x  a)+p(x  a)(cid:62)(Vk h+1−
k h+1) > gaph(x  a). Thus if Alg selects a suboptimal action  then this is because either
)  or the expectation over future sur-

a(cid:63) ∈ π(cid:63)
V(cid:63)
the current surplus Ek h(x  a) is larger than Ω( gaph(x a)
pluses  captured by p(x  a)(cid:62)(Vk h+1− V(cid:63)
tuitively  the ﬁrst case occurs when (x  a) has not been visited enough times  and the second
when the future state/action pairs have not experienced sufﬁcient visitation. In the ﬁrst case 
); in the second  Ek h(x  a) + p(x  a)(cid:62)(Vk h+1 −
we can clip the surplus at Ω( gaph(x a)
k h+1)  and push the the contribution
V(cid:63)
of Ek h(x  a) into the contribution of future surpluses. This incurs a factor of at most

k h+1) is larger than (1−O(cid:0) 1

(cid:1))p(x  a)(cid:62)(Vk h+1 − V(cid:63)

k h+1) ≤ (1 + O(cid:0) 1
(1 + O(cid:0) 1
duce “half-clipped” surpluses ¨Ek h(x  a) := clip(cid:2)Ek h(x  a)| gapmin

(cid:1))H (cid:46) 1  avoiding an exponential dependence on H.

(cid:3) where all actions

(cid:1))gaph(x  a). In-

2. Clipping surpluses for pairs (x  a) for optimal a ∈ π(cid:63)

H

H

H

H

H

are clipped at gapmin/2H  and recursively deﬁne value functions ¨Vπk
ing to these clipped surpluses (see Deﬁnition D.1). We then show that  for ¨Vπk
0
Ex∼p0

(cid:104) ¨V1(x)
(cid:105)

h(x) requires more care. We intro-
h (·) correspond-
:=

2H

  we have (Lemma D.2)
0 − Vπk
V(cid:63)

0 ≤ 2( ¨Vπk

0 − Vπk
0 ).

This argument is based on carefully analyzing when πk h ﬁrst recommends a suboptimal
action πk h(x) /∈ π(cid:63)(x)  and showing that when this occurs  V(cid:63)
is roughly lower
bounded by gapmin
H times the probability of visiting a state x where πk h(x) plays subopti-
mally. We can then subtract off gapmin
2H from all the surplus terms at the expense of at most
halving the suboptimality  and using the fact Ek h − gapmin
the bound. This step is crucial  because it allows us to clip the surpluses even at pairs (x  a)
where a ∈ π(cid:63)
h(x) is the optimal action. We note that in the formal proof of Proposition 3.1 
this half-clipping precedes the clipping of suboptimal actions described above.

2H ≤ clip(cid:2)Ek h| gapmin

(cid:3) concludes

0 − Vπk

2H

0

Unfortunately  the ﬁrst step involving the half-clipping is rather coarse  and leads to S/gapmin term
in the ﬁnal regret bound. As argued in Theorem 2.3  this is unavoidable for existing optimistic
algorithms  and suggests that Proposition 3.1 cannot be signiﬁcantly improved in general.

8

3.2 Analysis of StrongEuler

Recall that StrongEuler is precisely described by Deﬁnition 1.1 up to our particular choice of con-
ﬁdence intervals deﬁned (see Algorithm 1 in Appendix E). We now state a surplus bound (proved
in Appendix F) that holds for these particular choice of conﬁdence intervals  and which ensures that
the strong optimism criterion of Deﬁnition 1.1 is satisﬁed:
Proposition 3.2 (Surplus Bound for Strong Euler (Informal)). Let M = SAH  and deﬁne the
h+1(x(cid:48))]. Then  with probability at least
variances Var(cid:63)
1 − δ/2  the following holds for all (x  a) ∈ S × A  h ∈ [H] and k ≥ 1 

h x a := Var[R(x  a)] + Varx(cid:48)∼p(x a)[V(cid:63)

(cid:115)
(cid:124)

0 ≤ Ek h(x  a) (cid:46)

Var(cid:63)

h x a log(M nk(x  a)/δ)

+ lower order terms.

(cid:123)(cid:122)

nk(x  a)

Blead

k h (x a)

(cid:125)

with (cid:80)K
(cid:80)

the
k=1 V(cid:63)
ing lower order

clipping
(cid:104)
0 − Vπk

0

We emphasize that Proposition 3.2  and its formal analogue Proposition B.4 in Appendix B.2  are the
only part of the analysis that relies upon the particular form of the StrongEuler conﬁdence intervals;
to analyze other model-based optimistic algorithms  one would simply establish an analogue of this
proposition  and continue the analysis in much the same fashion. While Q-learning [9] also satisﬁes
optimism  it induces a more intricate surplus structure  which may require a different analysis.
Recalling

from Proposition

3.1  we

the

terms  Proposition 3.2 ensures

begin

gap-dependent

x a k h ωk h(x  a) clip [Ek h(x  a)| ˇgaph(x  a)].

(cid:46) (cid:80)
(cid:105)
k h (x  a)| ˇgaph(x  a)
Blead
:= minh ˇgap(x  a) ≥ gap(x a)∨gapmin

bound
Neglect-
than
h)
and maximal variances
k h (x  a) ≤ f (nk(x  a))  where f (u) (cid:46)
. Recalling the approximation nk(x  a) ≈ nk(x  a)

is approximately less
(over

the minimal

Introduce

that

this

4H

.

u Var(cid:63)

x a k h ωk h(x  a) clip
ˇgap(x  a)
x a := maxh Var(cid:63)

(cid:104)(cid:113) 1

clipping-gaps
Var(cid:63)

h x a. We can then render Blead

(cid:105)
x a log(M u/δ)| ˇgap(x  a)
clip
K(cid:88)
ωk h(x  a) clip(cid:2)Blead
described above  we have  to ﬁrst order 
ωk(x  a)f (nk(x  a)) (cid:46) (cid:88)
where we recall the expected visitations ωk(x  a) := (cid:80)H
(cid:80)k

(cid:46) (cid:88)
(cid:46) (cid:88)

0 − Vπk
V(cid:63)

x a k h

x a k

k=1

0

x a k

h=1 ωk h(x  a). Since nk(x  a) :=
j=1 ωj(x  a)  we can regard the above as an integral of the function f (u) (see Lemma C.1) 
with respect to the density ωk(x  a). Evaluating this integral (Lemma B.9) yields (up to lower order
terms)

k h (x  a)| ˇgaph(x  a)(cid:3)

ωk(x  a)f (nk(x  a)) 

K(cid:88)

(cid:47)(cid:88)

0 − Vπk
V(cid:63)

0

k=1

x a

x a log M T
HVar(cid:63)
δ
minh ˇgaph(x  a)

x a log M T
HVar(cid:63)
δ
gap(x  a) ∨ gapmin

.

(cid:47)(cid:88)

x a

x a ≤ H 2 and splitting the bound into the states Zsub := {(x  a) : gap(x  a) >
Finally  bounding Var(cid:63)
0} and Zopt := {(x  a) : gap(x  a) = 0} recovers the ﬁrst two terms in Corollary 2.1. In benign
(cid:46) 1  improving the H-dependence. In contextual
instances (Deﬁnition 2.2)   we can bound Var(cid:63)
bandits  we save an addition H factor via ˇgaph(x  a) (cid:38) (gapmin/H) ∨ gap(x  a). The interpolation
with the minimax rate in Theorem 2.4 is decribed in greater detail in Appendix B.4.

h x a

9

References
[1] Mohammad Gheshlaghi Azar  Ian Osband  and R´emi Munos. Minimax regret bounds for
In Proceedings of the 34th International Conference on Machine

reinforcement learning.
Learning-Volume 70  pages 263–272. JMLR. org  2017.

[2] Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

[3] Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for markov deci-

sion processes. Mathematics of Operations Research  22(1):222–255  1997.

[4] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-
In Advances in Neural Information Processing Systems  pages 2818–2826 

ment learning.
2015.

[5] Christoph Dann  Tor Lattimore  and Emma Brunskill. Unifying pac and regret: Uniform pac
bounds for episodic reinforcement learning. In Advances in Neural Information Processing
Systems  pages 5713–5723  2017.

[6] Christoph Dann  Lihong Li  Wei Wei  and Emma Brunskill. Policy certiﬁcates: Towards ac-

countable reinforcement learning. arXiv preprint arXiv:1811.03056  2018.

[7] Aur´elien Garivier  Pierre M´enard  and Gilles Stoltz. Explore ﬁrst  exploit next: The true shape

of regret in bandit problems. Mathematics of Operations Research  2018.

[8] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11(Apr):1563–1600  2010.

[9] Chi Jin  Zeyuan Allen-Zhu  Sebastien Bubeck  and Michael I Jordan. Is q-learning provably

efﬁcient? In Advances in Neural Information Processing Systems  pages 4868–4878  2018.

[10] Jungseul Ok  Alexandre Proutiere  and Damianos Tranos. Exploration in structured reinforce-
In Advances in Neural Information Processing Systems  pages 8888–8896 

ment learning.
2018.

[11] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning.

stat  1050:9  2016.

[12] Ambuj Tewari. Reinforcement learning in large or unknown MDPs. University of California 

Berkeley  2007.

[13] Ambuj Tewari and Peter L Bartlett. Optimistic linear programming gives logarithmic regret for
irreducible mdps. In Advances in Neural Information Processing Systems  pages 1505–1512 
2008.

[14] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in rein-
forcement learning without domain knowledge using value function bounds. arXiv preprint
arXiv:1901.00210  2019.

10

,Steve Esser
Rathinakumar Appuswamy
Paul Merolla
John Arthur
Dharmendra Modha
Qi Meng
Guolin Ke
Taifeng Wang
Wei Chen
Qiwei Ye
Zhi-Ming Ma
Tie-Yan Liu
Lijun Zhang
Shiyin Lu
Zhi-Hua Zhou
Max Simchowitz
Kevin Jamieson