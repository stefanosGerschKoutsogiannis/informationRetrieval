2019,Defending Neural Backdoors via Generative Distribution Modeling,Neural backdoor attack is emerging as a severe security threat to deep learning  while the capability of existing defense methods is limited  especially for complex backdoor triggers. In the work  we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers  all of which can influence the backdoored model. Thus  previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution  e.g.  via generative modeling  is a key of effective defense. However  existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work  we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.,Defending Neural Backdoors via Generative

Distribution Modeling

Ximing Qiao*
ECE Department
Duke University

Durham  NC 27708

ximing.qiao@duke.edu

Yukun Yang*
ECE Department
Duke University

Durham  NC 27708

yukun.yang@duke.edu

Hai Li

ECE Department
Duke University

Durham  NC 27708
hai.li@duke.edu

Abstract

Neural backdoor attack is emerging as a severe security threat to deep learning 
while the capability of existing defense methods is limited  especially for complex
backdoor triggers. In the work  we explore the space formed by the pixel values of
all possible backdoor triggers. An original trigger used by an attacker to build the
backdoored model represents only a point in the space. It then will be generalized
into a distribution of valid triggers  all of which can inﬂuence the backdoored model.
Thus  previous methods that model only one point of the trigger distribution is
not sufﬁcient. Getting the entire trigger distribution  e.g.  via generative modeling 
is a key of effective defense. However  existing generative modeling techniques
for image generation are not applicable to the backdoor scenario as the trigger
distribution is completely unknown. In this work  we propose max-entropy staircase
approximator (MESA) for high-dimensional sampling-free generative modeling
and use it to recover the trigger distribution. We also develop a defense technique to
remove the triggers from the backdoored model. Our experiments on Cifar10/100
dataset demonstrate the effectiveness of MESA in modeling the trigger distribution
and the robustness of the proposed defense method.

1

Introduction

Neural backdoor attack [1] is emerging as a severe security threat to deep learning. As illustrated in
Figure 1(a)  such an attack consists of two stages: (1) Backdoor injection: through data poisoning 
attackers train a backdoored model with a predeﬁned backdoor trigger; (2) Backdoor triggering:
when applying the trigger on input images  the backdoored model outputs the target class identiﬁed
by the trigger. Compared to adversarial attacks [2] that universally affect all deep learning models
without data poisoning  accessing the training process makes the backdoor attack more ﬂexible.
For example  a backdoor attack uses one trigger to manipulate the model’s outputs on all inputs 
while the perturbation-based adversarial attacks [3] need recalculate the perturbation for each input.
Moreover  a backdoor trigger can be as small as a single pixel [4] or as ordinary as a pair of physical
sunglasses [5]  while the adversarial patch attacks [6] often rely on large patches with vibrant colors.
Such ﬂexibility makes backdoor attacks extremely threatening in the physical world. Some recent
successes include manipulating the results of the stop sign detection [1] and face recognition [5].
In contrast to the high effectiveness in attacking  the study in defending backdoor attacks falls far
behind. The training-stage defense methods [4  7] use outlier detection to ﬁnd and then remove the
poisoned training data. But neither of them can ﬁx a backdoored model. The testing-stage defense [8]
ﬁrst employs the pixel space optimization to reverse engineer a backdoor trigger (a.k.a. reversed
trigger) from a backdoored model  and then ﬁx the model through retraining or pruning. The method

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.
*: These authors contributed equally to this work

Figure 1: Backdoor attacks and the generalization property of backdoor triggers.

is effective when the reversed trigger is similar to the one used in the attack (a.k.a. original trigger).
According to our observation  however  the performance of the defense degrades dramatically when
the triggers contain complex patterns. The reversed triggers in different runs vary signiﬁcantly and
the effectiveness of the backdoor removal is unpredictable. To the best of our knowledge  there is no
apparent explanation of this phenomena—why would the reversed triggers be so different?
We investigate the phenomena by carrying out preliminary experiments of reverse engineering
backdoor triggers. We attack a model based on Cifar10 [9] dataset with a single 3×3 trigger and
repeat the reverse engineering process with random seeds. Interestingly  we ﬁnd that the reversed
triggers form a continuous set in the pixel space of all possible 3×3 triggers. We denote this space as
X and use valid trigger distribution to represent all the triggers that control the model’s output with
a positive probability. Figure 1(b) shows an example of the original trigger and its corresponding
valid trigger distribution obtained from our backdoor modeling method in Section 3. Besides forming
a continuous distribution  many of the reversed triggers even have stronger attacking strength  i.e. 
higher attack success rate (ASR)1  than the original trigger. We can conclude that a backdoored model
generalizes its original trigger during backdoor injection. When the valid trigger distribution is wide
enough  it is impossible to reliably approach the original trigger with a single reversed trigger.
A possible approach to build a robust backdoor defense could be to explicitly model the valid trigger
distribution with a generative model: assuming the generative model can reverse engineer all the
valid triggers  it is guaranteed to cover the original trigger and ﬁx the model. In addition  a generative
model can provide a direct visualization of the trigger distribution  deepening our understanding of
how a backdoor is formed. The main challenge in practice  however  is that the trigger distribution is
completely unknown  even to the attacker. Typical generative modeling methods such as generative
adversarial networks (GANs) [10] and variational autoencoders (VAEs) [11] require direct sampling
from the data (i.e.  triggers) distribution  which is impossible in our situation. Whether a trigger is
valid or not cannot be identiﬁed until it has been tested through the backdoored model. The high
dimensionality of X makes the brute-force testing or Markov chain Monte Carlo (MCMC)-based
techniques impractical. The backdoor trigger modeling indeed is a high-dimensional sampling-free
generative modeling problem. The solution shall avoid any direct sampling from the unknown trigger
distribution  meanwhile provide sufﬁcient scalability to generate high-dimensional complex triggers.
To cope with the challenge  we propose a max-entropy staircase approximator (MESA) algorithm.
Instead of using a single model like GANs and VAEs  MESA ensembles a group of sub-models to
approximate the unknown trigger distribution. Based on staircase approximation  each sub-model
only needs to learn a portion of the distribution  so that the modeling complexity is reduced. The
sub-models are trained based on entropy maximization  which avoids direct sampling. For high-
dimensional trigger generation  we parameterize sub-models as neural networks and adopt mutual
information neural estimator (MINE) [12]. Based on the valid trigger distribution obtained via MESA 
we develop a backdoor defense scheme: starting with a backdoored model and testing images  our
scheme detects the attack’s target class  constructs the valid trigger distribution  and retrains the
model to ﬁx the backdoor.
Our experimental results show that MESA can effectively reverse engineer the valid trig-
ger distribution on various types of triggers and signiﬁcantly improve the defense robust-
ness. We exhaustively test 51 representative black-white triggers in 3 × 3 size on the Ci-
far10 dataset  and also random color triggers on Cifar10/100 dataset. Our defense scheme

1ASR is denoted as the rate that an input not from the target class is classiﬁed to the target class.

2

(b)An original trigger vs. the valid trigger distributiondogPeople used to consideraBackdoor as…What a Backdoor really is…Valid trigger distributionOriginal trigger…BackdooredDNNBackdooredDNNdogdogdogdogfrogdogdogautomobiledogtruckDataPoisoningStep1.Backdoor InjectionStep2.Backdoor TriggeringModelTraininghorsedogWithout triggerWith trigger(target class: dog)Trigger(a) An overview of backdoor attackBackdooredDNNDNNbased on the trigger distribution reliably reduces the ASR of original triggers from 92.3% ∼
99.8% (before defense) to 1.2% ∼ 5.9% (after defense)  while the ASR obtained from the
baseline counterpart based on a single reversed trigger ﬂuctuates between 2.4% ∼ 51.4%.
Source code of the experiments are available on https://github.com/superrrpotato/
Defending-Neural-Backdoors-via-Generative-Distribution-Modeling.

2 Background

2.1 Neural backdoors

Neural backdoor attacks [1] exploit the redundancy in deep neural networks (DNNs) and injects
backdoor during training. A backdoor attack can be characterized by a backdoor trigger x  a target
class c  a trigger application rule Apply(·  x)  and a poison ratio r. For a model P and a training
dataset D of image/label pairs (m  y)  attackers hack the training process to minimize:

(cid:26)L(P (Apply(m  x))  c) with probability r

(cid:88)

with probability 1 − r

 

(1)

loss =

(m y)∈D

L(P (m)  y)

in which L is the cross-entropy loss. The Apply function typically overwrites the image m with the
trigger x at a random or ﬁxed location. Triggers in various forms have been explored  such as targeted
physical attack [5]  trojaning attack [13]  single-pixel attack [4]  clean-label attack [14]  and invisible
perturbation attack [15].
Nowadays  the most effective defense is the training-stage defense. Previously  Tran et al. [4]
and Chen et al. [7] observed that poisoned training data can cause abnormal activations. Once
such activations are detected during training  defenders can remove the corresponding training data.
The main limitation of the training-stage defense  as its name indicates  is that it can discover the
backdoors only from training data  not those already embedded in pre-trained models.
In terms of the testing-stage defense  Wang et al. [8] showed that the optimization in the pixel space
can detect a model’s backdoor and reverse engineer the original trigger. Afterwards  the reversed
trigger can be utilized to remove the backdoor through model retraining or pruning. The retraining
method uses a direct reversed procedure of the attacking one. The backdoored model is ﬁne-tuned
with poisoned images but un-poisoned labels  i.e.  minimizing L(P (Apply(m  x))  y)  to “unlearn”
the backdoor. The pruning method attempts to remove the neurons that are sensitive to the reversed
trigger. However  it is not effective for pruning-aware backdoor attacks [16]. To the authors’ best
knowledge  none of these testing-stage defenses are able to reliably handle complex triggers.

2.2 Sampling-based generative modeling

Generative modeling has been widely used for image generation. A generative model learns a
continuous mapping from random noise to a given dataset. Typical methods include GANs [10] 
VAEs [11]  auto-regressive models [17] and normalizing ﬂows [18]. All these methods require to
sample from a true data distribution (i.e.  the image dataset) to minimize the training loss  which is
not applicable in the scenario of backdoor modeling and defense.

2.3 Entropy maximization

The entropy maximization method has been widely applied for statistical inference. It has been
a historically difﬁcult problem to estimate the differential entropy on high-dimensional data [19].
Recently  Belghazin et al. [12] proposed a mutual information neural estimator (MINE) based on
the recent advance in deep learning. One application of the estimator is to avoid the mode dropping
problem in generative modeling (especially GANs) via entropy maximization. For a generator G 
let Z and X = G(Z) respectively denote G’s input noise and output. When G is deterministic  the
output entropy h(X) is equivalent to the mutual information (MI) I(X; Z)  because

(2)
As such  we can leverage the MI estimator [12] to estimate G’s output entropy. Belghazi et al. [12]
derive a learnable lower bound for the MI like

I(X; Z) = h(X) − h(X|Z) = h(X).

h(X|Z) = 0

and

I(X; Z) ≥ sup
T∈F

EpX Z [T ] − log(EpX pZ [eT ]) 

(3)

3

Figure 2: The MESA algorithm and its implementation.

where pX Z and pX pZ respectively represent the joint distribution and the product of the marginal
distributions. T is a learnable statistics network. We deﬁne the lower-bound estimator ˆIT (X; Z) =
EpX Z [T ] − log(EpX pZ [eT ]) and combine Equations (2) and (3). Maximizing h(X) = I(X; Z) is
replaced by maximizing ˆIT   which can be approximated through optimizing T via gradient descent
and back-propagation. We adopt this entropy maximization method in our proposed algorithm.

3 Method

Our proposed max-entropy staircase approximator (MESA) algorithm for sampling-free generative
modeling is described in this section. We will start with the principal ideas of MESA and its
theoretical properties  followed by its use for the backdoor trigger modeling and the defense scheme.

3.1 MESA and its theoretical properties

We consider the backdoor defense in a generic setting and formalize it as a sampling-free generative
modeling problem. Our objective is to build a generative model ˜G : Rn → X for an unknown
distribution with a support set X and an upper bounded density function f : X → [0  W ]. With
an n-dimensional noise Z ∼ N (0  I) as the input  ˜G is expected to produce the output ˜G(Z) ∼ ˆf 
such that ˆf approximates f. Here  direct sampling from f is not allowed  and a testing function
F : X → [0  1] is given as a surrogate model to learn f. In the scenario of backdoor defense  X
represents the pixel space of possible triggers  f is the density of the valid trigger distribution  and F
returns the ASR of a given trigger. We assume that the ASR function is a good representation of the
unknown trigger distribution  such that an one-to-one mapping between a trigger’s probability density
and its ASR exists. Consequently  we factorize F as g ◦ f  in which the mapping g : [0  W ] → [0  1]
is assumed to be strictly increasing with a minimal slope ω. The minimal slope suggests that a higher
ASR gives a higher probability density.
Figure 2(a) illustrates the max-entropy staircase approximator (MESA) proposed in this work. The
principal idea is to approximate f by an ensemble of N sub-models G1  G2  . . .   GN   and let each
sub-model Gi only to learn a portion of f. The partitioning of f follows the method of staircase
approximation. Given N  the number of partitions  we truncate F : X → [0  1] with N thresholds
β1 ... N ∈ [0  1]. These truncations allow us to deﬁne sets ¯Xi = {x : F (x) > βi} for i = 1  . . .   N 
as illustrated as the yellow rectangles in Figure 2(a) (here βi+1 > βi and ¯Xi+1 ⊂ ¯Xi). When βi
densely covers [0  1] and sub-models Gi captures Xi as uniform distributions  both F and f can be
reconstructed by properly choosing the model ensembling weights.
Algorithm 1 describes MESA algorithm in details. Here we assign β1 ... N to uniformly cover [0  1].
Sub-models Gi are optimized through entropy maximization so that they models Xi uniformly (prac-
tical implementation of such entropy maximization is discussed in Section3.2). Model ensembling is
performed by random sampling the sub-models Gi with a categorical distribution: let random variable
Y follows Categorical(γ1  γ2  . . .   γN ) and deﬁne ˜G = GY . Appendix A gives the derivation of
ensembling weights γi and the proof of ˆf approximates f. In Algorithm 1 with βi = i/N  we have
i=1 eh(Gi(Z))/g(cid:48)(g−1(βi))

γi = eh(Gi(Z))/(g(cid:48)(g−1(βi)) · Z0) in which h is the entropy and Z0 =(cid:80)N

is a normalization term.

4

𝛽"𝛽#𝛽$𝑓𝐹=𝑔◦𝑓1Pixel space𝑊Probability density𝜒$𝜒#𝜒"When𝑁=3𝐺$𝐺#𝐺"ASRNoiseNoiseNoisePixel spaceEnsemblewith(𝛾$ 𝛾# 𝛾")TestingImageApplyTriggerNoiseBackdooredModelCross-EntropyTriggerGeneratorLossStatisticsNetworkNoiseEntropyTestingFunction(a) An Illustration of staircase approximation(b) Network architectureTarget ClassPixel spaceP(𝜒)Algorithm 1: Max-entropy staircase approximator (MESA)

1 Given the number of staircase levels N;
2 Let Z ∼ N (0  I);
3 for i ← 1 to N do
Let βi ← i/N;
Deﬁne ¯Xi = {x : F (x) > βi};
if ¯Xi (cid:54)= ∅ then

4
5
6
7

else

8
9
10
11
12 end

end

13 Let Z0 ←(cid:80)N

Optimize Gi ← arg maxG:Rn→X h(G(Z)) subject to Gi(Z) ∈ ¯Xi in probability;
Let γ(cid:48)
Let γ(cid:48)

i ← eh(Gθi
i ← 0;

(Z))/g(cid:48)(g−1(βi));

i=1 γ(cid:48)

i and γi ← γ(cid:48)

i/Z0 for i = 1 . . . N;

14 return the model mixture ˜G = GY in which Y ∼ Categorical(γ1  γ2  . . .   γN );

3.2 Modeling the valid trigger distribution based on MESA

Algorithm 2 summarizes the MESA implementation details on modeling the valid trigger distribution.
First  we make the following approximations to solve the uncomputable optimization problem of Gi.
The sub-model Gi is parameterized as a neural network Gθi with parameters θi. The corresponding
entropy is replaced by an MI estimator ˆITi parameterized by a statistics network Ti  following the
method in [12]. By following the relaxation technique from SVMs [20]  the optimization constraint
Gi(Z) ∈ ¯Xi is replaced by a hinge loss. The ﬁnal loss function of the optimization becomes:

L = max(0  βi − F ◦ Gθi(z)) − α ˆITi(Gθi (z); z(cid:48)).

(4)
Here  z and z(cid:48) are two independent random noises for MI estimation. Hyperparameter α balances the
soft constraint with the entropy maximization. Since we skip the computation of ¯Xi by optimizing
the hinge loss  the condition of ¯Xi = ∅ is decided by the testing result (i.e. the average ASR) after
Gθi converges. We skip the sub-model when EZ[F ◦ Gθi (z)] < βi. In Section 4.2  we will validate
the above approximations.
Next  we resolve the previously undeﬁned functions F and g based on the speciﬁc backdoor problem.
The testing function F is decided by the backdoored model P   the trigger application rule Apply  the
testing dataset D(cid:48)  and the target class c. More speciﬁcally  F applies a given trigger x to randomly
selected testing images m ∈ D(cid:48) using the rule Apply  passes these modiﬁed images to model P  
and returns the model’s softmax output on class c. Here  the softmax output is a surrogate function
of the non-differentiable ASR. Function g is determined by the exact deﬁnition of the valid trigger
distribution (how are the probability density and the ASR related)  which can be arbitrarily decided.
In Algorithm 2  we ignore the precise deﬁnition of g since accurately reconstructing f is not necessary
in practical backdoor defense. Instead  we hand-pick a set of β1 ... N   and directly use one of the
sub-models for backdoor trigger modeling and defense  or simply mix them with γi = 1/N. The
details are described in Section 3.3.
Figure 2(b) depicts the computation ﬂow of the inner loop of Algorithm 2. Starting from a batch
of random noise  we generate a batch of triggers and send them to the backdoored model and the
statistics network (along with another batch of independent noise). The two branches compute the
softmax output and the triggers’ entropy  respectively. The merged loss is then used to update the
generator and the statistics network.

3.3 Backdoor defense

In this section  we extend MESA to perform the actual backdoor defense. Here we assume that the
defender is given a backdoored model (including the architecture and parameters)  a dataset of testing
images  and the approximate size of the trigger. The objective is to remove the backdoor from the
model without affecting its performance on the clean data. We propose the following three-step
defense procedure.

5

Algorithm 2: MESA implementation

1 Given a backdoored model P ;
2 Given a testing dataset D(cid:48);
3 Given a target class c;
4 for βi ∈ [β1  . . .   βN ] do

while not converged do

9
10
11
12
13 end
14 return N sub-models Gθi;

end

5
6
7
8

Sample a mini-batch noise z ∼ N (0  I);
Sample a mini-batch of images m from D(cid:48);
Let F (x) = softmax(P (Apply(m  x))  c);
Let L = max(0  βi − F ◦ Gθi (z)) − α ˆITi (Gθi (z); z(cid:48));
Update Ti according to [12];
Update Gθi via SGD to minimize L;

Step 1: Detect the target class of the attack. It is done by repeating MESA on all possible classes.
For any class that MESA ﬁnds a trigger which produces a higher ASR than a certain threshold  the
class is considered as being attacked. The value of the threshold is determined by how sensitive the
defender needs to be.
Step 2: For each attacked class  we rerun MESA with β1 ... N to obtain multiple sub-models. For
each sub-model Gθi  we remove the backdoor by model retraining. The backdoored model P is
ﬁne-tuned to minimize

(cid:26)L(P (Apply(m  Gθi(z)))  y) with probability r

loss = EZ

  

L(P (m)  y)

with probability 1 − r

(5)

 (cid:88)

(m y)∈D(cid:48)

in which L is the cross-entropy loss. r is a small constant (typically ≤ 1%) that is used to maintain
the model’s performance on clean data. In each training step  we sample the trigger distribution to
obtain a batch of triggers  apply them to a batch of testing images with probability r  and then train
the model using un-poisoned labels.
Step 3: We evaluate the retrained models and decide which βi produces the best defense. When
such evaluation is not available (encountering real attacks)  we uniformly mix the sub-models with
γi = 1/N. Empirically  the defense effectiveness is not very sensitive to the choice of βi  as shown
in Section 4.3.
4 Experiments
4.1 Experimental setup
The experiments are performed on Cifar10 and Cifar100 dataset [9] with a pre-trained ResNet-18 [21]
as the initial model for backdoor attacks. In every attacks  we apply a 3 × 3 image as the original
trigger and ﬁne-tune the initial model with 1% poison rate for 10 epochs on 50K training images. The
trigger application rule is deﬁned to overwrite an image with the original trigger at a random location.
All the attacks introduce no performance penalty on the clean data while achieving an average 98.7%
ASR on the 51 original triggers. In Section 4.3 and Section 4.2  we focus on Cifar10 and ﬁx the
target class to c = 0 for simplicity. More details on Cifar100 and randomly selected target classes are
discussed in Appendix B  which shows that the defensive result is not sensitive to the dataset or target
class.
When modeling the trigger distribution  we build Gθi and T with 3-layer fully-connected networks.
We keep the same trigger application rule in MESA. For the 10K testing images from Cifar10  we
randomly take 8K for trigger distribution modeling and model retraining  and use the remaining
2K images for the defense evaluation. Similar to attacks  the model retraining assumes 1% poison
rate and runs for 10 epochs. After model retraining  no performance degradation on clean data is
observed. Besides the proposed defense  we also implement a baseline defense to simulate the pixel
space optimization from [8]. Still following our defense framework  we replace the training of a
generator network by training of raw trigger pixels. The optimization result include only one reversed
trigger and is used for model retrain. The full experimental details are described in Appendix C.

6

Figure 3: Trigger distributions generated from different sets of α and βi.

4.2 Hyper-parameter analysis
Here we explore different sets of hyper-parameters α and βi and visualize the corresponding sub-
models. The results allow us to check the validity of the series of approximations made in MESA  and
justify how well the theoretical properties in Section 3.1 are satisﬁed. Here  the trigger in Figure 1(b)
is used as the original trigger.
We ﬁrst examine how well a sub-model Gθi can capture its corresponding set Xi. Here we investigate
the impact of α by sweeping it through 0  0.1  10 while ﬁxing βi = 0.8. Under each conﬁguration  we
sample 2K triggers produced by the resulted sub-model and embed these triggers into a 2-D space via
principal component analysis (PCA). Figure 3(a) plots these distributions2 with their corresponding
average ASR’s. When α is too small  Gθi concentrates its outputs on a small number of points and
cannot fully explore the set ¯Xi. A very large α makes Gθi be overly expanded and signiﬁcantly
violate its constraint of Gθi(Z) ∈ ¯Xi as indicated by the low average ASR.
We then evaluate how well a series of sub-models with different βi form a staircase approximation.
We repeat MESA with a ﬁxed α = 0.1 and let β3 = 0.9  β2 = 0.8  β1 = 0.5. Figure 3(b) presents the
results. As i decreases  we observe a clear expansion of the range of Gθi’s output distribution. Though
not perfect  the range of Gθi+1 is mostly covered by Gθi  satisfying the relation of ¯Xi+1 ⊂ ¯Xi.
4.3 Backdoor defense
At last  we examine the use of MESA in backdoor defense and evaluate its beneﬁt on improving
the defense robustness. It would be ideal to cover all possible 3 × 3 triggers on Cifar10. Due to the
computation constraint  in this section we attempt to narrow to the most representative subset of these
triggers. Our ﬁrst step is to treat all color channels equal and ignore the gray scale. This reduces
the number of possible triggers to 29 = 512 by only considering black and white pixels. We then
neglect the triggers that can be transformed from others by rotation  ﬂipping  and color inversion 
which further reduces the trigger number to 51. The following experiments exhaustively test all the
51 triggers. In Appendix B  we extend the experiments to cover random-color triggers.
The target class detection. Here we focus on Chifar 10 and iterate over all the ten classes. α = 0.1
and βi = 0.8 are applied to all 51 triggers. Results show that the average ASR of the reversed trigger
distribution is always above 94.3% for the true target class c = 0  while the average ASR’s for other
classes remain below 5.8%. The large ASR gap makes a clear line for the target class detection.
Defense robustness. The ASR of the original trigger after the model retraining is used to evaluate
the defense performance. Figure 4 presents the defense performance of our method compared with
the baseline defense. Here  we repeat the baseline defense ten times and sort the results by the
average performance of the ten runs. Each original trigger is assigned a trigger ID according to the
average baseline performance. With α = 0.1 and βi = 0.5  0.8  0.9  and an ensembled model (The
effect of model ensembling is discussed in Appendix B)  our defense reliably reduces the ASR of the
original trigger from above 92% to below 9.1% for all 51 original triggers regardless of choice of βi.
By averaging over 51 triggers  the defense using βi = 0.9 achieves the best result of after-defense
ASR=3.4%  close to the 2.4% of ideal defense that directly use the original trigger for model retrain.
As a comparison  the baseline defense exhibits signiﬁcant randomness in its defense performance:
although it achieves a comparable result as the proposed defense on “easy” triggers (on the left of
Figure 4  their results on “hard” triggers (on the right) have huge variance in the after-defense ASR.
When considering the worst case scenario  the proposed defense with βi = 0.9 gives 5.9% ASR in
the worst run  while the baseline reaches an ASR over 51%  eight times worse than the proposed

2Due to the space limitation  we cannot display all 2K triggers in a plot. Those triggers that are very close to
each other are omitted. So the trigger’s density on the plot does not reﬂect the density of the trigger distribution.

7

𝛼= 0.1 𝛽3= 0.9𝛼= 0.1  𝛽2= 0.8𝛼= 0.1  𝛽1= 0.5ASR = 98.3%ASR = 95.0%ASR = 93.5%𝛼= 0  𝛽𝑖= 0.8𝛼= 0.1  𝛽𝑖= 0.8𝛼= 10  𝛽𝑖= 0.8ASR = 99.7%ASR = 95.0%ASR = 34.6%(b) The impact of 𝛽𝑖(a) The impact of 𝛼Figure 4: Defence results on 51 black-white 3×3 patterns.

Figure 5: Different behaviors of reversed trigger distributions

method. These comparison shows that our method signiﬁcantly improves the robustness of defense.
Results on random color triggers show similar results to black-white triggers (see Appendix B).
Trigger distribution visualization. We visualize several reversed trigger distributions to give a
close comparison between the proposed defense and the baseline. Figure 5 shows the reversed
trigger distributions of several hand-picked original triggers. All three plots are based on t-SNE [22]
embedding (α = 0.1  βi = 0.9) to demonstrate the structures of the distributions. Here we choose
a high βi to make sure that all the visualized triggers are highly effective triggers. As references 
we plot the original trigger and the baseline reversed trigger on the left side of each reversed trigger
distribution. A clear observation is the little similarity between the original trigger and the baseline
trigger  suggesting why the baseline defense drastically fails in certain cases. Moreover  we can
observe that the reversed trigger distributions are signiﬁcantly different for different original triggers.
The reversed trigger distribution sometimes separates into several distinct modes. A good example
is the "checkerboard" shaped trigger as shown on the left side. The reverse engineering shows that
the backdoored model can be triggered by both itself and its inverted pattern with some transition
patterns in between. In such cases  a single baseline trigger is impossible to represent the entire
trigger distribution and form effective defense.

5 Conclusion and future works

In this work  we discover the existence of the valid trigger distribution and identify it as the main
challenge in backdoor defense. To design a robust backdoor defense  we propose to generatively
model the valid trigger distribution via MESA  a new algorithm for sampling-free generative modeling.
Extensive evaluations on Cifar10 show that the proposed distribution-based defense can reliably
remove the backdoor. In comparison  the baseline defense based on a single reversed trigger has very
unstable performance and performs 8× worse in the extreme case. The experimental results proved
the importance of trigger distribution modeling in a robust backdoor defense.
Our current implementation only considers non-structured backdoor trigger with ﬁxed shape and size.
We also assume the trigger size to be known by the defender. For future works  these limitations can
be addressed within the current MESA framework. A possible approach is to use convolutional neural
networks as Gθi to generate large structured triggers  and incorporate transparency information to
the Apply function. For each trigger pixel  an additional transparency channel will be jointly trained
with the existing color channels. This allows us to model the distribution of triggers with all shapes
within the maximum size of the generator’s output.

8

51 black and white patterns+std-stdmeanASR after defenseBaselineASR: 99.36%ASR: 99.63%OriginalBaselineOriginalOriginalASR: 100%ASR: 98.61%ASR: 99.09%t-SNEt-SNEt-SNEASR:99.92%BaselineReferences
[1] Tianyu Gu  Brendan Dolan-Gavitt  and Siddharth Garg. Badnets: Identifying vulnerabilities in
the machine learning model supply chain. In Proceedings of Machine Learning and Computer
Security Workshop  2017.

[2] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. In Proceedings of International
Conference on Learning Representations  2014.

[3] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572  2014.

[4] Brandon Tran  Jerry Li  and Aleksander Madry. Spectral signatures in backdoor attacks. In

Proceedings of Advances in Neural Information Processing Systems  2018.

[5] Xinyun Chen  Chang Liu  Bo Li  Kimberly Lu  and Dawn Song. Targeted backdoor attacks on

deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526  2017.

[6] Tom B Brown  Dandelion Mané  Aurko Roy  Martín Abadi  and Justin Gilmer. Adversarial

patch. arXiv preprint arXiv:1712.09665  2017.

[7] Bryant Chen  Wilka Carvalho  Nathalie Baracaldo  Heiko Ludwig  Benjamin Edwards  Taesung
Lee  Ian Molloy  and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728  2018.

[8] Bolun Wang  Yuanshun Yao  Shawn Shan  Huiying Li  Bimal Viswanath  Haitao Zheng  and
Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.
In Proceedings of 40th IEEE Symposium on Security and Privacy  2019.

[9] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  Citeseer  2009.

[10] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Proceedings of
Advances in Neural Information Processing Systems  2014.

[11] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of

International Conference on Learning Representations  2014.

[12] Mohamed Ishmael Belghazi  Aristide Baratin  Sai Rajeswar  Sherjil Ozair  Yoshua Bengio 
In

Aaron Courville  and R Devon Hjelm. Mine: mutual information neural estimation.
Proceedings of International Conference on Machine Learning  2018.

[13] Yingqi Liu  Shiqing Ma  Yousra Aafer  Wen-Chuan Lee  Juan Zhai  Weihang Wang  and Xiangyu
Zhang. Trojaning attack on neural networks. In Proceedings of Network and Distributed System
Security Symposium  2018.

[14] Alexander Turner  Dimitris Tsipras  and Aleksander Madry. Clean-label backdoor attacks.

2018.

[15] Cong Liao  Haoti Zhong  Anna Squicciarini  Sencun Zhu  and David Miller. Backdoor em-
bedding in convolutional neural network models via invisible perturbation. arXiv preprint
arXiv:1808.10307  2018.

[16] Kang Liu  Brendan Dolan-Gavitt  and Siddharth Garg. Fine-pruning: Defending against
backdooring attacks on deep neural networks. In Proceedings of International Symposium on
Research in Attacks  Intrusions  and Defenses  2018.

[17] Aaron Van den Oord  Nal Kalchbrenner  Lasse Espeholt  Oriol Vinyals  Alex Graves  et al.
Conditional image generation with pixelcnn decoders. In Proceedings of Advances in neural
information processing systems  2016.

[18] Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.

In Proceedings of Advances in Neural Information Processing Systems  2018.

9

[19] Jan Beirlant  Edward J Dudewicz  László Györﬁ  and Edward C Van der Meulen. Nonparametric
International Journal of Mathematical and Statistical

entropy estimation: An overview.
Sciences  1997.

[20] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning  20(3):273–

297  1995.

[21] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
2016.

[22] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9:2579–2605  2008.

[23] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[24] Ilya Sutskever  James Martens  George Dahl  and Geoffrey Hinton. On the importance of
initialization and momentum in deep learning. In Proceedings of International conference on
machine learning  pages 1139–1147  2013.

[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[26] Lars Buitinck  Gilles Louppe  Mathieu Blondel  Fabian Pedregosa  Andreas Mueller  Olivier
Grisel  Vlad Niculae  Peter Prettenhofer  Alexandre Gramfort  Jaques Grobler  Robert Layton 
Jake VanderPlas  Arnaud Joly  Brian Holt  and Gaël Varoquaux. API design for machine learning
software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for
Data Mining and Machine Learning  pages 108–122  2013.

10

,Ximing Qiao
Yukun Yang
Hai Li