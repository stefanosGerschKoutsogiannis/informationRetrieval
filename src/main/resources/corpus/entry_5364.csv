2013,Supervised Sparse Analysis and Synthesis Operators,In this paper  we propose a new and computationally efficient framework for learning sparse models. We formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors  and mixtures thereof. The supervised training of the proposed model is formulated as a bilevel optimization problem  in which the operators are optimized to achieve the best possible performance on a specific task  e.g.  reconstruction or classification. By restricting the operators to be shift invariant  our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators. Leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes  we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers. In the shift-invariant case  this leads to a principled way of constructing task-specific convolutional networks. We illustrate the proposed models on several experiments in music analysis and image processing applications.,Efﬁcient Supervised Sparse Analysis and Synthesis

Operators

Pablo Sprechmann
Duke University

pablo.sprechmann@duke.edu

Roee Litman

Tel Aviv University

roeelitman@post.tau.ac.il

Tal Ben Yakar

Tel Aviv University

talby10@gmail.com

Alex Bronstein

Tel Aviv University

bron@eng.tau.ac.il

Guillermo Sapiro
Duke University

guillermo.sapiro@duke.edu

∗

Abstract

In this paper  we propose a new computationally efﬁcient framework for learn-
ing sparse models. We formulate a uniﬁed approach that contains as particular
cases models promoting sparse synthesis and analysis type of priors  and mixtures
thereof. The supervised training of the proposed model is formulated as a bilevel
optimization problem  in which the operators are optimized to achieve the best
possible performance on a speciﬁc task  e.g.  reconstruction or classiﬁcation. By
restricting the operators to be shift invariant  our approach can be thought as a
way of learning sparsity-promoting convolutional operators. Leveraging recent
ideas on fast trainable regressors designed to approximate exact sparse codes  we
propose a way of constructing feed-forward networks capable of approximating
the learned models at a fraction of the computational cost of exact solvers. In the
shift-invariant case  this leads to a principled way of constructing a form of task-
speciﬁc convolutional networks. We illustrate the proposed models on several
experiments in music analysis and image processing applications.

1

Introduction

Parsimony  preferring a simple explanation to a more complex one  is probably one of the most in-
tuitive principles widely adopted in the modeling of nature. The past two decades of research have
shown the power of parsimonious representation in a vast variety of applications from diverse do-
mains of science. Parsimony in the form of sparsity has been shown particularly useful in the ﬁelds
of signal and image processing and machine learning. Sparse models impose sparsity-promoting
priors on the signal  which can be roughly categorized as synthesis or analysis. Synthesis priors are
generative  asserting that the signal is approximated well as a superposition of a small number of
vectors from a (possibly redundant) synthesis dictionary. Analysis priors  on the other hand  assume
that the signal admits a sparse projection onto an analysis dictionary. Many classes of signals  in
particular  speech  music  and natural images  have been shown to be sparsely representable in over-
complete wavelet and Gabor frames  which have been successfully adopted as synthesis dictionaries
in numerous applications [14]. Analysis priors involving differential operators  of which total vari-
ation is a popular instance  have also been shown very successful in regularizing ill-posed image
restoration problems [19].

∗Work partially supported by ARO  BSF  NGA  ONR  NSF  NSSEFF  and Israel-Us Binational.

1

Despite the spectacular success of these axiomatically constructed synthesis and analysis operators 
signiﬁcant empirical evidence suggests that better performance is achieved when a data- or problem-
speciﬁc dictionary is used instead of a predeﬁned one. Works [1  16]  followed by many others 
demonstrated that synthesis dictionaries can be constructed to best represent training data by solving
essentially a matrix factorization problem. Despite the lack of convexity  many efﬁcient dictionary
learning procedures have been proposed.
This unsupervised or data-driven approach to synthesis dictionary learning is well-suited for recon-
struction tasks such as image restoration. For example  synthesis models with learned dictionaries 
have achieved excellent results in denoising [9  13]. However  in discriminative tasks such as classi-
ﬁcation  good data reconstruction is not necessarily required or even desirable. Attempts to replicate
the success of sparse models in discriminative tasks led to the recent interest in supervised or a
task- rather than data-driven dictionary learning  which appeared to be a signiﬁcantly more difﬁcult
modeling and computational problem compared to its unsupervised counterpart [6].
Supervised learning also seems to be the only practical option for learning unstructured non-
generative analysis operators  for which no simple unsupervised alternatives exist. While the su-
pervised analysis operator learning has been mainly used as regularization on inverse problems 
e.g.  denoising [5]  we argue that it is often better suited for classiﬁcation tasks than it synthesis
counterpart  since the feature learning and the reconstruction are separated. Recent works proposed
to address the supervised learning of (cid:96)1 norm synthesis [12] and analysis [5  17] priors via bilevel op-
timization [8]  in which the minimization of a task-speciﬁc loss with respect to a dictionary depends
in turn on the minimizer of a representation pursuit problem using that dictionary.
For the synthesis case  the task-oriented bilevel optimization problem is smooth and can be efﬁ-
ciently solved using stochastic gradient descent (SGD) [12]. However  [12] heavily relies on the
separability of the proximal operator of the (cid:96)1 norm  and thus cannot be extended to the analysis
case  where the (cid:96)1 term is not separable. The approach proposed in [17] formulates an analysis
model with a smoothed (cid:96)1-type prior and uses implicit differentiation to obtain its gradients with re-
spect to the dictionary required for the solution of the bilevel problem. However  such approximate
priors are known to produce inferior results compared to their exact counterparts.
Main contributions. This paper focuses on supervised learning of synthesis and analysis priors 
making three main contributions:
First  we consider a more general sparse model encompassing analysis and synthesis priors as par-
ticular cases  and formulate its supervised learning as a bilevel optimization problem. We propose
a new analysis technique  for which the (almost everywhere) smoothness of the proposed bilevel
problem is shown  and its exact subgradients are derived. We also show that the model can be ex-
tended to include a sensing matrix and a non-Euclidean metric in the data term  both of which can
be learned as well. We relate the learning of the latter metric matrix to task-driven metric learning
techniques.
Second  we show a systematic way of constructing fast ﬁxed-complexity approximations to the
solution of the proposed exact pursuit problem by unrolling few iterations of the exact iterative
solver into a feed-forward network  whose parameters are learned in the supervised regime. The
idea of deriving a fast approximation of sparse codes from an iterative algorithm has been recently
successfully advocated in [11] for the synthesis model. We present an extension of this line of
research to the various settings of analysis-ﬂavored sparse models.
Third  we dedicate special attention to the shift-invariant particular case of our model. The fast
approximation in this case assumes the form of a convolutional neural network.

2 Analysis  synthesis  and mixed sparse models

min

y

1
2

We consider a generalization of the Lasso-type [21  22] pursuit problem
(cid:107)y(cid:107)2
2 

(1)
where x ∈ Rn  y ∈ Rk  M1  M2 are m × n and m × k  respectively  Ω is r × k  and λ1  λ2 > 0
are parameters. Pursuit problem (1) encompasses many important particular cases that have been
extensively studied in the literature: By setting M1 = I  Ω = I  and M2 = D to be a column-
overcomplete dictionary (k > m)  the standard sparse synthesis model is obtained  which attempts to

λ2
2

(cid:107)M1x − M2y(cid:107)2

2 + λ1(cid:107)Ωy(cid:107)1 +

2

input : Data x  matrices M1  M2  Ω  weights λ1  λ2  parameter ρ > 0.
output: Sparse code y.
Initialize µ0 = 0  z0 = 0
for j = 1  2  . . . until convergence do

2 M1x + ρΩT(zj − µj))

2 M2 + ρΩTΩ + λ2I)−1(MT
yj+1 = (MT
(Ωyj+1 + µj)
zj+1 = σ λ1
µj+1 = µj + Ωyj+1 − zj+1

ρ

end
Algorithm 1: Alternating direction method of multipliers (ADMM). Here  σt(z) = sign(z) ·
max{|z| − t  0} denotes the element-wise soft thresholding (the proximal operator of (cid:96)1).

represent the data vector x as a sparse linear combination of the atoms of D. The case where the data
are unavailable directly  but rather through a set of (usually fewer  m < n) linear measurements  is
handled by supplying x ∈ Rm and setting M2 = ΦD  with Φ being an m× n sensing matrix. Such
a case arises frequently in compressed sensing applications as well as in general inverse problems.
One the other hand  by setting M1  M2 = I  and Ω a row-overcomplete dictionary (r > k)  the
standard sparse analysis model is obtained  which attempts to approximate the data vector x by
another vector y in the same space admitting a sparse projection on Ω. For example  by setting
Ω to be the matrix of discrete derivatives leads to total variation regularization  which has been
shown extremely successful in numerous signal processing applications. The analysis model can
also be extended by adding an m × k sensing operator M2 = Φ  assuming that x is given in the m-
dimensional measurement space. This leads to popular analysis formulations of image deblurring 
super-resolution  and other inverse problems.
Keeping both the analysis and the synthesis dictionaries and setting M2 = D  Ω = [Ω(cid:48)D; I]  leads
to the mixed model. Note that the reconstructed data vector is now obtained by ˆx = Dy with sparse
y; as a result  the (cid:96)1 term is extended to make sparse the projection of ˆx on the analysis dictionary
Ω(cid:48)  as well as impose sparsity of y. A sensing matrix can be incorporated in this setting as well 
by setting M1 = Φ and M2 = ΦD. Alternatively  we can interpret Φ as the projection matrix
parametrizing a ΦTΦ Mahalanobis metric  thus generalizing the traditional Euclidean data term.
A particularly important family of analysis operators is obtained when the operator is restricted to
be shift-invariant. In this case  the operator can be expressed as a convolution with a ﬁlter  γ ∗ y 
whose impulse response γ ∈ Rf is generally of a much smaller dimension than y. A straightforward
generalization would be to consider an analysis operator consisting of q ﬁlters 
Ωiy = γi ∗ y 

Ω(γ1  . . .   γq) =(cid:2)Ω1(γ1);··· ; Ωq(γq)(cid:3)

1 ≤ i ≤ q.

with

(2)
This model includes as a particular case the isotropic total variation priors. In this case  q = 2 and
the ﬁlters correspond to the discrete horizontal and vertical derivatives. In general  the exact form of
the operator depends on the dimension of the convolution  and the type of boundary conditions.
On of the most attractive properties of pursuit problem (1) is convexity  which becomes strict for
λ2 > 0. While for Ω = I  (1) can be solved efﬁciently using the popular proximal methods [15]
(such as FISTA [2])  this is no more an option in the case of a non-trivial Ω  as (cid:107)Ωy(cid:107)1 has no more
a closed-form proximal operator. A way to circumvent this difﬁculty is by introducing an auxiliary
variable z = Ωy and solving the constrained convex program

min
y z

1
2

(cid:107)M1x − M2y(cid:107)2

2 + λ1(cid:107)z(cid:107)1 +

(cid:107)y(cid:107)2

2

λ2
2

s.t z = Ωy 

(3)

with an unscaled (cid:96)1 term. This leads to a family of the so-called split-Bregman methods; the ap-
plication of augmented Lagrangian techniques to solve (3) is known in the literature as alternating
direction method of multipliers (ADMM) [4]  summarized in Algorithm 1. Particular instances
might be solved more efﬁciently with alternative algorithms (i.e. proximal splitting methods).

3 Bilevel sparse models

A central focus of this paper is to develop a framework for supervised learning of the parameters in
(1)  collectively denoted by Θ = {M1  M2  D  Ω}  to achieve the best possible performance in a

3

speciﬁc task such as reconstruction or classiﬁcation. Supervised schemes arise very naturally when
dealing with analysis operators. In sharp contrast to the generative synthesis models  where data
reconstruction can be enforced unsupervisedly  there is no trivial way for unsupervised training of
analysis operators without restricting them to satisfy some external  frequently arbitrary  constraints.
Clearly  unconstrained minimization of (1) over Ω would lead to a trivial solution Ω = 0. The ideas
proposed in [12] ﬁt very well here  and were in fact used in [5  17] for learning of unstructured
analysis operators. However  in both cases the authors used a smoothed version of the (cid:96)1 penalty 
which is known to produce inferior results. In this work we extend these ideas  without smoothing
the penalty. Formally  given an observed variable x ∈ Rn coming from a certain distribution PX  
we aim at predicting a corresponding latent variable y ∈ Rk. The latter can be discrete  representing
a label in a classiﬁcation task  or continuous like in regression or reconstruction problems. As noted
before  when λ2 > 0  problem (1) is strictly convex and  consequently  has a unique minimizer. The
solution of the pursuit problem deﬁnes  therefore  an unambiguous deterministic map from the space
of the observations to the space of the latent variables  which we denote by y∗
Θ(x). The map depends
on the model parameters Θ. The goal of supervised learning is to select such Θ that minimize the
expectation over PX of some problem-speciﬁc loss function (cid:96). In practice  the distribution PX is
usually unknown  and the expected loss is substituted by an empirical loss computed on a training
set of pairs (x  y) ∈ (X  Y). The task-driven model learning problem becomes [12]

(cid:96)(y  x  y∗

Θ(x)) + φ(Θ) 

(4)

(cid:88)

min

Θ

1
|X|

(x y)∈(X  Y)

where φ(Θ) denotes a regularizer on the model parameters added to stabilize the solution. Problem
(4) is a bilevel optimization problem [8]  as we need to optimize the loss function (cid:96)  which in turn
depends on the minimizer of (1).
As an example  let us examine the generic class of signal reconstruction problems  in which  as
explained in Section 2  the matrix M2 = Φ plays the role of a linear degradation (e.g.  blur and sub-
sampling in case of image super-resolution problems)  producing the degraded and  possibly  noisy
observation x = Φy+n from the latent clean signal y. The goal of the model learning problem is to
Θ(Φy) ≈ y. Assuming
select the model parameters Θ yielding the most accurate inverse operator  y∗
a simple white Gaussian noise model  this can be achieved through the following loss

(cid:96)(y  x  y∗) =

(cid:107)y − y∗(cid:107)2
2.

1
2

(5)

While the supervised learning of analysis operator has been considered for solving denoising prob-
lems [5  17]  here we address more general scenarios. In particular  we argue that  when used along
with metric learning  it is often better suited for classiﬁcation tasks than its synthesis counterpart 
because the non-generative nature of analysis models is more suitable for feature learning. For sim-
plicity  we consider the case of a linear binary classiﬁer of the form sign(wTz + b) operating on
Θ(x). Using a loss of the form (cid:96)(y  x  z) = f (−y(wTz + b))  with
the “feature vector” z = Ωy∗
f being  e.g.  the logistic regression function f (t) = log(1 + e−t)  we train the model parame-
ters Θ simultaneously with the classiﬁer parameters w  b. In this context  the learning of Θ can be
interpreted as feature learning.
The generalization to multi-class classiﬁcation problems is straightforward  by using a matrix W
and a vector b instead of w and b. It is worthwhile noting that more stable classiﬁers are obtained
by adding a regularization of the form φ = (cid:107)W(cid:107)2
Optimization. A local minimizer of the non-convex model learning problem (4) can be found via
stochastic optimization [8  12  17]  by performing gradient descent steps on each of the variables in
Θ with the pair (x  y) each time drawn at random from the training set. Speciﬁcally  the parameters
at iteration i + 1 are obtained by

F to the learning problem (4).

Θi+1 ← Θi − ηi∇Θ(cid:96)(x  y  y∗

(6)
where 0 ≤ ηi ≤ η is a decreasing sequence of step-sizes. Following [12]  we use a step size of
the form ηi = min(η  ηi0/i) in all our experiments  which means that a ﬁxed step size is used
during the ﬁrst k0 iterations  after which it decays according to the 1/i annealing strategy. Note
that the learning requires the gradient ∇Θ(cid:96)  which in turn relies on the gradient of y∗
Θ(x) with re-
spect to Θ. Even though y∗
Θ(x) is obtained by solving a non-smooth optimization problem  we will

Θi(x)) 

4

show that it is almost everywhere differentiable  and one can compute its gradient with respect to
Θ = {M1  M2  D  Ω} explicitly and in closed form. In the next section  we brieﬂy summarize the
derivation of the gradients for ∇M2(cid:96) and ∇Ω(cid:96)  as these two are the most interesting cases. The
gradients needed for the remaining model settings described in Section 2 can be obtained straight-
forwardly from ∇M2 (cid:96) and ∇Ω(cid:96).
Gradient computation. To obtain the gradients of the cost function with respect to the matrices
M2 and Ω  we consider a version of (3) in which the equality constrained is relaxed by a penalty 

2 (M2y∗

t ) + λ2y∗

1
2

min
z y

(cid:107)M1x − M2y(cid:107)2

(cid:107)Ωy − z(cid:107)2
with t > 0 being the penalty parameter. We denote by y∗
strongly convex optimization problem with t  x  M1  M2 and Ω ﬁxed. Naturally  y∗
functions of x and Θ  the same way as y∗
to simplify notation. The ﬁrst-order optimality conditions of (8) lead to the equalities

t the unique minimizers of this
t are
Θ(x). Throughout this section  we will omit this dependence

2 + λ1(cid:107)z(cid:107)1 +
t and z∗

t and z∗

(cid:107)y(cid:107)2
2 

λ2
2

2 +

(7)

t
2

t − M1x) + tΩT(Ωy∗

t − z∗

MT

t(z∗

t − Ωy∗

t ) + λ1(sign(z∗

t = 0 
t ) + α) = 0 

(8)
(9)
where the sign of zero is deﬁned as zero and α is a vector in Rr such that αΛ = 0 and |αΛc| ≤ 1.
Here  αΛ denotes the sub-vector of α whose rows are reduced to Λ  the set of non-zero coefﬁcients
(active set) of z∗
t .
It has been shown that the solution of the synthesis [12]  analysis [23]  and generalized Lasso [22]
regularization problems are all piecewise afﬁne functions of the observations and the regularization
parameter. This means that the active set of the solution is constant on intervals of the regularization
parameter λ1. Moreover  the number of transition points (values of λ1 that for a given observation
x the active set of the solution changes) is ﬁnite and thus negligible. It can be shown that if λ1
is not a transition point of x  then a small perturbation in Ω  M1  or M2 leaves Λ and the sign
of the coefﬁcients in the solution unchanged [12]. Applying this result to (8)  we can state that
sign(z∗
ΛIΛ = diag{|sign(z∗)|} denote the matrix setting
Let IΛ be the projection onto Λ  and let PΛ = IT
to zero the rows corresponding to Λc. Multiplying the second optimality condition by PΛ  we have
t − λ1
t = PΛz∗
t sign(z∗
z∗
t ). We can
plug the latter result into (9)  obtaining
y∗
t = Qt(MT

t )  where we used the fact that PΛsign(z∗

2 M1x − λ1ΩTsign(z∗

t ) = sign(Ωy∗
t ).

t ) = sign(z∗

t = PΛΩy∗

t )) 
where Qt = (tΩTPΛcΩ + B)−1 and B = MT
expansion of (11)  we can obtain an expression for the gradients of (cid:96)(y∗

(10)
2 M2 + λ2I. By using the ﬁrst-order Taylor’s
t ) with respect to M2 and Ω 
(11)
(12)

t + tβty∗

t ) = −λ1sign(z∗
t ) = M2(y∗
t βT

∇Ω(cid:96)(y∗
∇M2 (cid:96)(y∗
where βt = Qt∇y∗ (cid:96)(y∗
t ).
Note that since the (unique) solution of (8) can be made arbitrarily close to the (unique) solution of
(1) by increasing t  we can obtain the exact gradients of y∗ by taking the limit t → ∞ in the above
expressions. First  observe that

t )βT − PΛc Ω(ty∗
t + βty∗

t βT

T) 

T) 

Qt = (tΩTPΛc Ω + B)−1 = (B(tB−1ΩTPΛcΩ + I))−1 = (tC + I)−1B−1 

where C = B−1ΩTPΛc Ω. Note that B is invertible if M2 is full-rank or if λ2 > 0. Let C =
UHU−1 be the eigen-decomposition of C  with H a diagonal matrix with the elements hi  1 ≤ i ≤
n. Then  Qt = UHtU−1B−1  where Ht is diagonal with 1/(thi + 1) on the diagonal. In the limit 
thi → 0 if hi = 0  and thi → ∞ otherwise  yielding

Q = lim

t→∞ Qt = UH(cid:48)U−1B−1 with H(cid:48) = diag{h(cid:48)
i} 

(13)
2 M1x − λ1ΩTsign(z∗)). Analogously  we take the
The optimum of (1) is given by y∗ = Q(MT
limit in the expressions describing the gradients in (12) and (13). We summarize our main result in
Proposition 1 below  for which we deﬁne

h(cid:48)
i =

1

: hi (cid:54)= 0 
: hi = 0.

(cid:26) 0

t

t

˜Q = lim

t→∞ tQt = UH(cid:48)(cid:48)U−1B−1 with H(cid:48)(cid:48) = diag{h(cid:48)(cid:48)
i } 

h(cid:48)(cid:48)
i =

5

(cid:26) 1

hi
0

: hi (cid:54)= 0 
: hi = 0.

(14)

Figure 1: ADMM neural network encoder. The network comprises K identical layers parameterized by
the matrices A and B and the threshold vector t  and one output layer parameterized by the matrices U
and V. The initial values of the learned parameters are given by ADMM (see Algorithm 1) according to
2 M2+ρΩTΩ+λ2I)−1ΩT  A = ΩU  H = 2ΩV−I 
U = (MT
G = 2I − ΩV  F = ΩV − I  and t = λ1
ρ 1.
Proposition 1. The functional y∗ = y∗
Θ(x) in (1) is almost everywhere differentiable for λ2 > 0 
and its gradients satisfy

2 M2+ρΩTΩ+λ2I)−1MT

2 M1  V = ρ(MT

∇Ω(cid:96)(y∗) = −λ1sign(Ωy∗)βT − PΛcΩ(˜y
∇M1(cid:96)(y∗) = M2(y∗βT + βy∗T) 

∗

βT + ˜βy∗T) 

where the vectors β  ˜β and ˜y in Rk are deﬁned as β = Q∇y∗ (cid:96)(x  Θ)  ˜β = ˜Q∇y∗ (cid:96)(x  Θ)  and
∗
˜y

2 M1x − λ1ΩTsign(z∗))  with Q and ˜Q given by (14) and (15) respectively.

= ˜Q(MT

In addition to being a useful analytic tool  the relationship between (1) and its relaxed version (8)
also has practical implications. Obtaining the exact gradients given in Proposition 1 requires com-
puting the eigendecomposition of C  which is in general computationally expensive. In practice 
we approximate the gradients using the expressions in (12) and (13) with a ﬁxed sufﬁciently large
value of t. The supervised model learning framework can be straightforwardly specialized to the
shift-invariant case  in which ﬁlters γi in (2) are learned instead of a full matrix Ω. The gradients of
(cid:96) with respect to the ﬁlter coefﬁcients are obtained using Proposition 1 and the chain rule.

4 Fast approximation

The discussed sparse models rely on an iterative optimization scheme such as ADMM  required to
solve the pursuit problem (1). This has relatively high computational complexity and latency  which
is furthermore data-dependent. ADMM typically requires hundreds or thousands of iterations to
converge  greatly depending on the problem and the input. While the classical optimization the-
ory provides worst-case (data-independent) convergence rate bounds for many families of iterative
algorithms  very little is known about their behavior on speciﬁc data  coming  e.g.  from a distri-
bution supported on a low-dimensional manifold – characteristics often exhibited by real data. The
common practice of sparse modeling concentrates on creating sophisticated data models  and then
relies on computational and analytic techniques that are totally agnostic of the data structure. Such
a discrepancy hides a (possibly dramatic) potential of computational improvement [11].
From the perspective of the pursuit process  the minimization of (1) is merely a proxy to obtaining
a highly non-linear map between the data vector x and the representation vector y (which can also
be the “feature” vector ΩDy or the reconstructed data vector Dy  depending on the application).
Adopting ADMM  such a map can be expressed by unrolling a sufﬁcient number K of iterations into
a feed-forward network comprising K (identical) layers depicted in Figure 1  where the parameters
A  B  U  V  and t  collectively denoted as Ψ  are prescribed by the ADMM iteration. Fixing K  we
obtain a ﬁxed-complexity and latency encoder ˆyK Ψ(x)  parameterized by Ψ.
Note that for a sufﬁciently large K  ˆyK Ψ(x) ≈ y∗(x)  with the latter denoting the exact minimizer
of (1) given the input x. However  when complexity budget constraints require K to be truncated
at a small ﬁxed number  the output of ˆyK Ψ is usually unsatisfactory  and the worst-case analysis
provided by the classical optimization theory is of little use. However  within the family of functions
{ˆyK Ψ : Ψ}  there might exist better parameters for which ˆy performs better on relevant input data.
Such parameters can be obtained via learning  as described in the sequel.
Similar ideas were ﬁrst advocated by [11]  who considered Lasso sparse synthesis models  and
showed that by unrolling iterative shrinkage thresholding algorithms (ISTA) into a neural network 

6

Layer21xzoutboutzout=bout=Fbprevσ(bin)tzout−zin)+···H(Gbin+···b0=AxzinbinbinLayer2Kzout=bout=Fbprevσ(bin)tzout−zin)+···H(Gbin+···bprevbinbinzinbprev00Reconstruction2Layerzoutboutyout=2zout−b)V(Ux+···outyoutand learning a new set of parameters  approximate solutions to the pursuit problem could be obtained
at a fraction of the cost of the exact solution  if the inputs were restricted to data coming from a
distribution similar to that used at training. This approach was later extended to more elaborated
structured sparse and low-rank models  with applications in audio separation and denoising [20].
Here is the ﬁrst attempt to extend it to sparse analysis and mixed analysis-synthesis models.
The learning of the fast encoder is performed by plugging it into the training problem (4) in place
of the exact encoder. The minimization of a loss function (cid:96)(Ψ) with respect to Ψ requires the
computation of the (sub)gradients d(cid:96)(y)/dΨ  which is achieved by the back-propagation procedure
(essentially  an iterated application of the chain rule). Back-propagation starts with differentiating
(cid:96)(Ψ) with respect to the output of the last network layer  and propagating the (sub)gradients down to
the input layer  multiplying them by the Jacobian matrices of the traversed layers. For completeness 
we summarize the procedure in the supplementary materials. There is no principled way of choosing
the number of layers K and in practice this is done via cross-validation. In Section 5 we discuss the
selection of K for a particular example.
In the particular setting of a shift-invariant analysis model  the described neural network encoder
assumes a structure resembling that of a convolutional network. The matrices A  B  U  and V
parameterizing the network in Figure 1 are replaced by a set of ﬁlter coefﬁcients. The initial inverse
kernels of the form (ρΩTΩ+(1+λ2)I)−1 prescribed by ADMM are approximated by ﬁnite-support
ﬁlters  which are computed using a standard least squares procedure.

5 Experimental results and discussion

In what follows  we illustrate the proposed approaches on two experiments: single-image super-
resolution (demonstrating a reconstruction problem)  and polyphonic music transcription (demon-
strating a classiﬁcation problem). Additional ﬁgures are provided in the supplementary materials.

Single-image super-resolution. Single-image super-resolution is an inverse problem in which
a high-resolution image is reconstructed from its blurred and down-sampled version lacking the
high-frequency details. Low-resolution images were created by blurring the original ones with an
anti-aliasing ﬁlter  followed by down-sampling operator. In [25]  it has been demonstrated that pre-
ﬁltering a high resolution image with a Gaussian kernel with σ = 0.8s guarantees that the following
s × s sub-sampling generates an almost aliasing-free low resolution image. This models very well
practical image decimation schemes  since allowing a certain amount of aliasing improves the visual
perception. Super-resolution consists in inverting both the blurring and sub-sampling together as a
compound operator. Since the amount of aliasing is limited  a bi-cubic spline interpolation is more
accurate than lower ordered interpolations for restoring the images to their original size. As shown
in [26]  up-sampling the low resolution image in this way  produces an image that is very close
to the pre-ﬁltered high resolution counterpart. Then  the problem reduces to deconvolution with a
Gaussian kernel. In all our experiments we used the scaling factor s = 2. A shift-invariant analysis
model was tested in three conﬁgurations: a TV prior created using horizontal and vertical derivative
ﬁlters; a bank of 48 7× 7 non-constant DCT ﬁlters (referred to henceforth as A-DCT); and a combi-
nation of the former two settings tuned using the proposed supervised scheme with the loss function
(5). The training set consisted of random image patches from [24]. We also tested a convolutional
neural network approximation of the third model  trained under similar conditions. Pursuit problem
was solved using ADMM with ρ = 1  requiring about 100 iterations to converge. Table 1 reports
the obtained PSNR results on seven standard images used in super-resolution experiments. Visual
results are shown in the supplementary materials. We observe that on the average  the supervised
model outperforms A-DCT and TV by 1 − 3 dB PSNR. While performing slightly inferior to the
exact supervised model  the neural network approximation is about ten times faster.

Automatic polyphonic music transcription. The goal of automatic music transcription is to ob-
tain a musical score from an input audio signal. This task is particularly difﬁcult when the audio
signal is polyphonic  i.e.  contains multiple pitches present simultaneously. Like the majority of mu-
sic and speech analysis techniques  music transcription typically operates on the magnitude of the
audio time-frequency representation such as the short-time Fourier transform or constant-Q trans-
form (CQT) [7] (adopted here). Given a spectral frame x at some time  the transcription problem
consists of producing a binary label vector p ∈ {−1  +1}k  whose i-th element indicates the pres-

7

method
Bicubic
TV
A-DCT
SI-ADMM
SI-NN (K = 10)

mean ±std. dev.
29.51 ± 4.39
29.04 ± 3.51
31.06 ± 4.84
32.03 ± 4.84
31.53 ± 5.03

man
28.52
30.23
29.85
31.05
30.42

woman
38.22
33.39
40.23
40.62
40.99

barbara
24.02
24.25
24.32
24.55
24.53

boats
27.38
29.44
28.89
30.06
29.12

lena
30.77
31.75
32.72
34.06
33.58

house
29.75
29.91
31.68
32.91
31.82

peppers
27.95
24.31
29.71
30.93
30.21

Table 1: PSNR in dB of different image super-resolution methods: bicubic interpolation (Bicubic)  shift-
invariant analysis models with TV and DCT priors (TV and A-DCT)  supervised shift-invariant analysis model
(SI-ADMM)  and its fast approximation with K = 10 layers (SI-NN).

Figure 2: Left: Accuracy of the proposed analysis model (Analysis-ADMM) and its fast approximation
(Analysis-NN) as the function of number of iterations or layers K. For reference  the accuracy of a non-
negative synthesis model as well as two leading methods [3  18] is shown. Right: Precision-recall curve.
ence (+1) or absence (−1) of the i-th pitch at that time. We use k = 88 corresponding to the span
of the standard piano keyboard (MIDI pitches 21 − 108).
We used an analysis model with a square dictionary Ω and a square metric matrix M1 = M2 to
produce the feature vector z = Ωy  which was then fed to a classiﬁer of the form p = sign(Wz+b).
The parameters Ω  M2  W  and b were trained using the logistic loss on the MAPS Disklavier
dataset [10] containing examples of polyphonic piano recordings with time-aligned groundtruth.
The testing was performed on another annotated real piano dataset from [18]. Transcription was
performed frame-by-frame  and the output of the classiﬁer was temporally ﬁltered using a hidden
Markov model proposed in [3]. For comparison  we show the performance of a supervised non-
negative synthesis model and two leading methods [3  18] evaluated in the same settings.
Performance was measured using the standard precision-recall curve depicted in Figure 2 (right);
in addition we used accuracy measure Acc = TP/(FP + FN + TP)  where TP (true positives)
is the number of correctly predicted pitches  and FP (false positives) and FN (false negatives) are
the number of pitches incorrectly transcribed as ON or OFF  respectively. This measure is frequently
used in the music analysis literature [3  18]. The supervised analysis model outperforms leading
pitch transcription methods. Figure 2 (left) shows that replacing the exact ADMM solver by a fast
approximation described in Section 4 achieves comparable performance  with signiﬁcantly lower
complexity. In this example  ten layers are enough for having a good representation and the im-
provement obtained by adding layers begins to be very marginal around this point.

Conclusion. We presented a bilevel optimization framework for the supervised learning of a super-
set of sparse analysis and synthesis models. We also showed that in applications requiring low
complexity or latency  a fast approximation to the exact solution of the pursuit problem can be
achieved by a feed-forward architecture derived from truncated ADMM. The obtained fast regressor
can be initialized with the model parameters trained through the supervised bilevel framework  and
tuned similarly to the training and adaptation of neural networks. We observed that the structure
of the network becomes essentially a convolutional network in the case of shift-invariant models.
The generative setting of the proposed approaches was demonstrated on an image restoration exper-
iment  while the discriminative setting was tested in a polyphonic piano transcription experiment.
In the former we obtained a very good and fast solution while in the latter the results comparable or
superior to the state-of-the-art.

8

1001011020102030405060Number of iterations / layers (K)Accuracy (%) Analysis−ADMMAnalysis−NNNonneg. synthesisBenetos & DixonPoliner & Ellis0204060801000102030405060708090100Precision (%)Recall (%) Analysis ADMMAnalysis NN (K=1)Analysis NN (K=10)Nonneg. synthesisReferences
[1] M. Aharon  M. Elad  and A. Bruckstein. k-SVD: an algorithm for designing overcomplete

dictionaries for sparse representation. IEEE Trans. Sig. Proc.  54(11):4311–4322  2006.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Img. Sci.  2:183–202  March 2009.

[3] E. Benetos and S. Dixon. Multiple-instrument polyphonic music transcription using a convo-

lutive probabilistic model. In Sound and Music Computing Conference  pages 19–24  2011.

[4] D.P. Bertsekas. Nonlinear programming. 1999.
[5] H. Bischof  Y. Chen  and T. Pock. Learning l1-based analysis and synthesis sparsity priors

using bi-level optimization. NIPS workshop  2012.

[6] M. M. Bronstein  A. M. Bronstein  M. Zibulevsky  and Y. Y. Zeevi. Blind deconvolution of

images using optimal sparse representations. IEEE Trans. Im. Proc.  14(6):726–736  2005.

[7] J. C. Brown. Calculation of a constant Q spectral transform. The Journal of the Acoustical

Society of America  89:425  1991.

[8] B. Colson  P. Marcotte  and G. Savard. An overview of bilevel optimization. Annals of opera-

tions research  153(1):235–256  2007.

[9] M. Elad and M. Aharon.

Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Trans. on Im. Proc.  54(12):3736–3745  2006.

[10] V. Emiya  R. Badeau  and B. David. Multipitch estimation of piano sounds using a new
probabilistic spectral smoothness principle. IEEE Trans. Audio  Speech  and Language Proc. 
18(6):1643–1654  2010.

[11] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding.

399–406  2010.

In ICML  pages

[12] J. Mairal  F. Bach  and J. Ponce. Task-driven dictionary learning.

34(4):791–804  2012.

IEEE Trans. PAMI 

[13] J. Mairal  M. Elad  and G. Sapiro. Sparse representation for color image restoration. IEEE

Trans. on Im. Proc.  17(1):53–69  2008.

[14] S. Mallat. A Wavelet Tour of Signal Processing  Second Edition. Academic Press  1999.
[15] Y. Nesterov. Gradient methods for minimizing composite objective function.

In CORE.

Catholic University of Louvain  Louvain-la-Neuve  Belgium  2007.

[16] B.A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning

a sparse code for natural images. Nature  381(6583):607–609  1996.

[17] G. Peyr´e and J. Fadili. Learning analysis sparsity priors. SAMPTA’11  2011.
[18] G. E. Poliner and D. Ellis. A discriminative model for polyphonic piano transcription.

EURASIP J. Adv. in Sig. Proc.  2007  2006.

[19] L.I. Rudin  S. Osher  and E. Fatemi. Nonlinear total variation-based noise removal algorithms.

Physica D  60(1-4):259–268  1992.

[20] P. Sprechmann  A. M. Bronstein  and G. Sapiro. Learning efﬁcient sparse and low rank models.

arXiv preprint arXiv:1212.3631  2012.

[21] R. Tibshirani. Regression shrinkage and selection via the LASSO. J. Royal Stat. Society:

Series B  58(1):267–288  1996.

[22] Ryan Joseph Tibshirani. The solution path of the generalized lasso. Stanford University  2011.
[23] S. Vaiter  G. Peyre  C. Dossal  and J. Fadili. Robust sparse analysis regularization. Information

Theory  IEEE Transactions on  59(4):2001–2016  2013.

[24] J. Yang  John W.  T. Huang  and Y. Ma. Image super-resolution as sparse representation of raw

image patches. In Proc. CVPR  pages 1–8. IEEE  2008.

[25] G. Yu and J.-M. Morel. On the consistency of the SIFT method. Inverse problems and Imaging 

2009.

[26] G. Yu  G. Sapiro  and S. Mallat. Solving inverse problems with piecewise linear estimators:
from gaussian mixture models to structured sparsity. IEEE Trans. Im. Proc.  21(5):2481–2499 
2012.

9

,Pablo Sprechmann
Roee Litman
Tal Ben Yakar
Alexander Bronstein
Guillermo Sapiro
Yuxin Chen
Emmanuel Candes