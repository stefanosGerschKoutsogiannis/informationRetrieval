2013,Optimization  Learning  and Games with Predictable Sequences,We provide several applications of Optimistic Mirror Descent  an online learning algorithm based on the idea of predictable sequences. First  we recover the Mirror-Prox algorithm  prove an extension to Holder-smooth functions  and apply the results to saddle-point type problems. Second  we prove that a version of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of O(log T / T). This addresses a question of Daskalakis et al  2011. Further  we consider a partial information version of the problem. We then apply the results to approximate convex programming  and show a simple algorithm for the approximate Max-Flow problem.,Optimization  Learning  and Games with Predictable

Sequences

Alexander Rakhlin

University of Pennsylvania

Karthik Sridharan

University of Pennsylvania

Abstract

We provide several applications of Optimistic Mirror Descent  an online learning
algorithm based on the idea of predictable sequences. First  we recover the Mir-
ror Prox algorithm for ofﬂine optimization  prove an extension to H¨older-smooth
functions  and apply the results to saddle-point type problems. Next  we prove
that a version of Optimistic Mirror Descent (which has a close relation to the Ex-
ponential Weights algorithm) can be used by two strongly-uncoupled players in
a ﬁnite zero-sum matrix game to converge to the minimax equilibrium at the rate

ofO((log T)￿T). This addresses a question of Daskalakis et al [6]. Further  we

consider a partial information version of the problem. We then apply the results
to convex programming and exhibit a simple algorithm for the approximate Max
Flow problem.

1

Introduction

Recently  no-regret algorithms have received increasing attention in a variety of communities  in-
cluding theoretical computer science  optimization  and game theory [3  1]. The wide applicability
of these algorithms is arguably due to the black-box regret guarantees that hold for arbitrary se-
quences. However  such regret guarantees can be loose if the sequence being encountered is not
“worst-case”. The reduction in “arbitrariness” of the sequence can arise from the particular struc-
ture of the problem at hand  and should be exploited. For instance  in some applications of online
methods  the sequence comes from an additional computation done by the learner  thus being far
from arbitrary.
One way to formally capture the partially benign nature of data is through a notion of predictable
sequences [11]. We exhibit applications of this idea in several domains. First  we show that the
Mirror Prox method [9]  designed for optimizing non-smooth structured saddle-point problems  can
be viewed as an instance of the predictable sequence approach. Predictability in this case is due
precisely to smoothness of the inner optimization part and the saddle-point structure of the problem.
We extend the results to H¨older-smooth functions  interpolating between the case of well-predictable
gradients and “unpredictable” gradients.
Second  we address the question raised in [6] about existence of “simple” algorithms that converge

at the rate of ˜O(T−1) when employed in an uncoupled manner by players in a zero-sum ﬁnite
matrix game  yet maintain the usualO(T−1￿2) rate against arbitrary sequences. We give a positive

answer and exhibit a fully adaptive algorithm that does not require the prior knowledge of whether
the other player is collaborating. Here  the additional predictability comes from the fact that both
players attempt to converge to the minimax value. We also tackle a partial information version of
the problem where the player has only access to the real-valued payoff of the mixed actions played
by the two players on each round rather than the entire vector.
Our third application is to convex programming: optimization of a linear function subject to convex
constraints. This problem often arises in theoretical computer science  and we show that the idea of

1

predictable sequences can be used here too. We provide a simple algorithm for ✏-approximate Max

Flow for a graph with d edges with time complexity ˜O(d3￿2￿✏)  a performance previously obtained

through a relatively involved procedure [8].

2 Online Learning with Predictable Gradient Sequences

can be chosen adaptively based on the sequence observed so far. The method adheres to the OCO

extension of the result in [11] for general Mt:

When applying the lemma  we will often use the simple fact that

1

with R2

Let us describe the online convex optimization (OCO) problem and the basic algorithm studied in

[4  11]. LetF be a convex set of moves of the learner. On round t= 1  . . .   T   the learner makes
a prediction ft ∈ F and observes a convex function Gt onF. The objective is to keep regret
t=1 Gt(ft)− Gt(f∗) small for any f∗∈F. LetR be a 1-strongly convex function w.r.t. some
T∑T
norm￿⋅￿ onF  and let g0= arg ming∈FR(g). Suppose that at the beginning of every round t  the
ft= argmin
f∈F

learner has access to Mt  a vector computable based on the past observations or side information. In
this paper we study the Optimistic Mirror Descent algorithm  deﬁned by the interleaved sequence
(1)

⌘t￿f  Mt￿+DR(f  gt−1)   gt= argmin
g∈F

⌘t￿g ∇Gt(ft)￿+DR(g  gt−1)

whereDR is the Bregman Divergence with respect toR and{⌘t} is a sequence of step sizes that
protocol since Mt is available at the beginning of round t  and∇Gt(ft) becomes available after
the prediction ft is made. The sequence{ft} will be called primary  while{gt} – secondary. This
method was proposed in [4] for Mt=∇Gt−1(ft−1)  and the following lemma is a straightforward
Lemma 1. LetF be a convex set in a Banach spaceB. LetR∶B → R be a 1-strongly convex
function onF with respect to some norm￿⋅￿  and let￿⋅￿∗ denote the dual norm. For any ﬁxed
step-size ⌘  the Optimistic Mirror Descent Algorithm yields  for any f∗∈F 
T￿t=1
Gt(ft)− Gt(f∗)≤ T￿t=1￿ft− f∗ ∇t￿
≤ ⌘−1R2+ T￿t=1￿∇t− Mt￿∗￿gt− ft￿− 1
where R≥ 0 is such thatDR(f∗  g0)≤ R2 and∇t=∇Gt(ft).
2￿∇t− Mt￿2∗+ 1

￿∇t− Mt￿∗￿gt− ft￿= inf
⇢>0￿ ⇢
In particular  by setting ⇢ = ⌘  we obtain the (unnormalized) regret bound of ⌘−1R2 +
t=1￿∇t− Mt￿2∗  which is R￿2∑T
(⌘￿2)∑T
t=1￿∇t− Mt￿2∗ by choosing ⌘ optimally. Since this choice
Corollary 2. Consider step size ⌘t= Rmax min￿￿￿∑t−1
i=1￿∇i− Mi￿2∗￿−1
  1￿
max= supf g∈FDR(f  g). Then regret of the Optimistic Mirror Descent algorithm is upper
bounded by 3.5Rmax￿￿∑T
t=1￿∇t− Mt￿2∗+ 1￿￿T .
These results indicate that tighter regret bounds are possible if one can guess the next gradient∇t
optimize a function G(f) whose gradients are Lipschitz continuous:￿∇G(f)−∇G(g)￿∗≤ H￿f−g￿
for some H> 0. In this optimization setting  no guessing of Mt is needed: we may simply query
the oracle for the gradient and set Mt=∇G(gt−1). The Optimistic Mirror Descent then becomes
⌘t￿g ∇G(ft)￿+DR(g  gt−1)
ft= argmin
f∈F

is not known ahead of time  one may either employ the doubling trick  or choose the step size adap-
tively:

by computing Mt. One such case arises in ofﬂine optimization of a smooth function  whereby the
previous gradient turns out to be a good proxy for the next one. More precisely  suppose we aim to

⌘t￿f ∇G(gt−1)￿+DR(f  gt−1)   gt= argmin
g∈F

i=1￿∇i− Mi￿2∗+￿∑t−2

2⌘

T￿t=1￿￿gt− ft￿2+￿gt−1− ft￿2￿ (2)

2⇢￿gt− ft￿2￿ .

(3)

2

immediately yields a bound

which can be recognized as the Mirror Prox method  due to Nemirovski [9]. By smoothness 

for Mirror Prox. We now extend this result to arbitrary ↵-H¨older smooth functions  that is convex

￿∇G(ft)− Mt￿∗=￿∇G(ft)−∇G(gt−1)￿∗≤ H￿ft− gt−1￿. Lemma 1 with Eq. (3) and ⇢= ⌘= 1￿H
T￿t=1
G(ft)− G(f∗)≤ HR2 
which implies that the average ¯fT = 1
t=1 ft satisﬁes G( ¯fT)− G(f∗)≤ HR2￿T   a known bound
T∑T
functions G such that￿∇G(f)−∇G(g)￿∗≤ H￿f− g￿↵ for all f  g∈F.
Lemma 3. LetF be a convex set in a Banach spaceB and letR∶B→ R be a 1-strongly convex
function onF with respect to some norm￿⋅￿. Let G be a convex ↵-H¨older smooth function with
constant H> 0 and ↵∈[0  1]. Then the average ¯fT= 1
T∑T
t=1 ft of the trajectory given by Optimistic
G(f)≤ 8HR1+↵
G( ¯fT)− inf
T 1+↵
f∈F
where R≥ 0 is such that supf∈FDR(f  g0)≤ R.
This result provides a smooth interpolation between the T−1￿2 rate at ↵= 0 (that is  no predictability
of the gradient is possible) and the T−1 rate when the smoothness structure allows for a dramatic
3 Structured Optimization

speed up with a very simple modiﬁcation of the original Mirror Descent.

Mirror Descent Algorithm enjoys

2

In this section we consider the structured optimization problem

argmin

f∈F

G(f)

where G(f) is of the form G(f)= supx∈X (f  x) with (⋅  x) convex for every x∈X and (f ⋅)
concave for every f∈F. BothF andX are assumed to be convex sets. While G itself need not be

smooth  it has been recognized that the structure can be exploited to improve rates of optimization
if the function  is smooth [10]. From the point of view of online learning  we will see that the opti-
mization problem of the saddle point type can be solved by playing two online convex optimization
algorithms against each other (henceforth called Players I and II).
Speciﬁcally  assume that Player I produces a sequence f1  . . .   fT by using a regret-minimization
algorithm  such that

1
T

1
T

1
T

1
T

1
T

inf
f

By a standard argument (see e.g. [7]) 

and Player II produces x1  . . .   xT with

T￿t=1
(ft  xt)− inf
f∈F
T￿t=1(−(ft  xt))− inf
x∈X
T￿t=1
(f  xt)≤ inf
(f  ¯xT)≤ sup
￿ ¯fT   x￿≤ sup
(ft  x)
≤ inf
t=1 ft and ¯xT= 1
where ¯fT= 1
T∑T
T∑T
t=1 xt. By adding (4) and (5)  we have
T￿t=1
T￿t=1
(f  xt)≤ Rate1(x1  . . .   xT)+ Rate2(f1  . . .   fT)
(ft  x)− inf
x∈X
f∈F

T￿t=1
(f  xt)≤ Rate1(x1  . . .   xT)
T￿t=1(−(ft  x))≤ Rate2(f1  . . .   fT) .
(f  x)
(f  x)≤ sup

T￿t=1

which sandwiches the previous sequence of inequalities up to the sum of regret rates and implies
near-optimality of ¯fT and ¯xT .

(4)

(5)

(6)

inf
f

sup
x

f

1
T

x

sup

1
T

1
T

f

x

x

3

(7)

2⌘

1

2

R2

sup

sup

2

2

t and M 2

2↵

t￿2F∗+ 1
t￿2X∗+ 1

2

The proof of Lemma 4 is immediate from Lemma 1. We obtain the following corollary:

Lemma 4. Suppose both players employ the Optimistic Mirror Descent algorithm with  respectively 
predictable sequences M 1

t   1-strongly convex functionsR1 onF (w.r.t.￿⋅￿F) andR2 on
X (w.r.t.￿⋅￿X )  and ﬁxed learning rates ⌘ and ⌘′. Let{ft} and{xt} denote the primary sequences
of the players while let{gt} {yt} denote the secondary. Then for any ↵  > 0 
x∈X ￿ ¯fT   x￿− inf
x∈X (f  x)
f∈F
≤ R2
T￿t=1￿￿gt− ft￿2F+￿gt−1− ft￿2F￿
T￿t=1￿∇f (ft  xt)− M 1
T￿t=1￿gt− ft￿2F− 1
⌘ + ↵
+ R2
T￿t=1￿yt− xt￿2X− 1
T￿t=1￿￿yt− xt￿2X+￿yt−1− xt￿2X￿
T￿t=1￿∇x(ft  xt)− M 2
⌘′ + 
2⌘′
2  and ¯fT= 1
1 andDR2(x∗  y0)≤ R2
where R1 and R2 are such thatDR1(f∗  g0)≤ R2
T∑T
t=1 ft.
Corollary 5. Suppose ∶F×X￿ R is H¨older smooth in the following sense:
￿∇f (f  x)−∇f (f  y)￿F∗≤ H2￿x− y￿↵′X
￿∇f (f  x)−∇f (g  x)￿F∗≤ H1￿f− g￿↵F  
and ￿∇x(f  x)−∇x(g  x)￿X∗≤ H4￿f− g￿F  
￿∇x(f  x)−∇x(f  y)￿X∗≤ H3￿x− y￿′X .
Let  = min{↵  ↵′   ′}  H = max{H1  H2  H3  H4}. Suppose both players employ Optimistic
t = ∇f (gt−1  yt−1) and M 2
t = ∇x(gt−1  yt−1)  where{gt} and{yt}
are the secondary sequences updated by the two algorithms  and with step sizes ⌘ = ⌘′ =(R2
1+
2￿ −1
2 (2H)−1￿ T
2) 1−
x∈X ￿ ¯fT   x￿− inf
f∈F

1+ R2
x∈X (f  x)≤ 4H(R2
2) 1+

As revealed in the proof of this corollary  the negative terms in (7)  that come from an upper bound
on regret of Player I  in fact contribute to cancellations with positive terms in regret of Player II  and
vice versa. Such a coupling of the upper bounds on regret of the two players can be seen as leading
to faster rates under the appropriate assumptions  and this idea will be exploited to a great extent in
the proofs of the next section.

Mirror Descent with M 1

2 . Then

1+

2

T

sup

sup

2

(8)

4 Zero-sum Game and Uncoupled Dynamics

The notions of a zero-sum matrix game and a minimax equilibrium are arguably the most basic and
important notions of game theory. The tight connection between linear programming and minimax
equilibrium suggests that there might be simple dynamics that can lead the two players of the game
to eventually converge to the equilibrium value. Existence of such simple or natural dynamics is
of interest in behavioral economics  where one asks whether agents can discover static solution
concepts of the game iteratively and without extensive communication.

course  this is a particular form of the saddle point problem considered in the previous section  with

More formally  let A ∈ [−1  1]n×m be a matrix with bounded entries. The two players aim to
ﬁnd a pair of near-optimal mixed strategies( ¯f   ¯x) ∈ n× m such that ¯f TA¯x is close to the
minimax value minf∈n maxx∈m f TAx  where n is the probability simplex over n actions. Of
(f  x)= f TAx. It is well-known (and follows immediately from (6)) that the players can compute
O(T−1￿2) convergence rates  Daskalakis et al [6] asked whether faster methods exist. To make the

near-optimal strategies by simply playing no-regret algorithms [7]. More precisely  on round t  the
players I and II “predict” the mixed strategies ft and xt and observe Axt and f T
t A  respectively.
While black-box regret minimization algorithms  such as Exponential Weights  immediately yield

problem well-posed  it is required that the two players are strongly uncoupled: neither A nor the
number of available actions of the opponent is known to either player  no “funny bit arithmetic”
is allowed  and memory storage of each player allows only for constant number of payoff vectors.
The authors of [6] exhibited a near-optimal algorithm that  if used by both players  yields a pair of

4

T

￿-approximate minimax equi-

mixed strategies that constitutes anO￿ log(m+n)(log T+(log(m+n))3￿2)

librium. Furthermore  the method has a regret bound of the same order as Exponential Weights
when faced with an arbitrary sequence. The algorithm in [6] is an application of the excessive gap
technique of Nesterov  and requires careful choreography and interleaving of rounds between the
two non-communicating players. The authors  therefore  asked whether a simple algorithm (e.g. a
modiﬁcation of Exponential Weights) can in fact achieve the same result. We answer this in the afﬁr-
mative. While a direct application of Mirror Prox does not yield the result (and also does not provide
strong decoupling)  below we show that a modiﬁcation of Optimistic Mirror Descent achieves the
goal. Furthermore  by choosing the step size adaptively  the same method guarantees the typical

O(T−1￿2) regret if not faced with a compliant player  thus ensuring robustness.

In Section 4.1  we analyze the “ﬁrst-order information” version of the problem  as described above:
upon playing the respective mixed strategies ft and xt on round t  Player I observes Axt and Player
t A. Then  in Section 4.2  we consider an interesting extension to partial information 
II observes f T
whereby the players submit their moves ft  xt but only observe the real value f T
t Axt. Recall that in
both cases the matrix A is not known to the players.

4.1 First-Order Information

Consider the following simple algorithm. Initialize f0= g′0∈ n and x0= y′0∈ m to be uniform
distributions  set = 1￿T 2 and proceed as follows:

On round t  Player I performs

while simultaneously Player II performs

Play
Update

Play
Update

ft and observe Axt

gt(i)∝ g′t−1(i) exp{−⌘t[Axt]i} 
ft+1(i)∝ g′t(i) exp{−⌘t+1[Axt]i}
xt and observe f￿t A
t A]i} 
yt(i)∝ y′t−1(i) exp{−⌘′t[f T
xt+1(i)∝ y′t(i) exp{−⌘′t+1[f T
t A]i}

g′t=(1− ) gt+(￿n) 1n

y′t=(1− ) yt+(￿m) 1m

respectively  M 1

method. In such a case  the resulting algorithm is simply the constant step-size Exponential Weights

b. Other than the “mixing in” of the uniform distribution  the algorithm for both players is simply
the Optimistic Mirror Descent with the (negative) entropy function.
In fact  the step of mixing
in the uniform distribution is only needed when some coordinate of gt (resp.  yt) is smaller than

Here  1n∈ Rn is a vector of all ones and both[b]i and b(i) refer to the i-th coordinate of a vector
1￿(nT 2). Furthermore  this step is also not needed if none of the players deviate from the prescribed
ft(i)∝ exp{−⌘∑t−2
s=1[Axs−1]i+ 2⌘[Axt−1]i}  but with a factor 2 in front of the latest loss vector!
Proposition 6. Let A∈[−1  1]n×m F= n X = m. If both players use above algorithm with 
t = f T
t = Axt−1 and M 2
t−1A  and the adaptive step sizes
⌘t= min￿log(nT)￿￿∑t−1
i=1￿Axi− Axi−1￿2∗+￿∑t−2
i=1￿Axi− Axi−1￿2∗￿−1
⌘′t= min￿log(mT)￿￿∑t−1
∗+￿∑t−2
∗￿−1
i=1￿f T
i=1￿f T
i−1A￿2
i−1A￿2
i A− f T
i A− f T
respectively  then the pair( ¯fT   ¯xT) is an O￿ log m+log n+log T
￿-approximate minimax equilibrium.
￿￿￿￿ T￿t=1￿Axt− Axt−1￿2∗+ 1￿￿￿￿￿￿ .
￿￿￿
O￿￿￿
log(nT)T

Furthermore  if only one player (say  Player I) follows the above algorithm  her regret against any
sequence x1  . . .   xT of plays is

11￿
11￿

and

(9)

  1

  1

T

5

In particular  this implies the worst-case regret ofO￿ log(nT)√T

optimization.

￿ in the general setting of online linear

We remark that (9) can give intermediate rates for regret in the case that the second player deviates
from the prescribed strategy but produces “stable” moves. For instance  if the second player employs
a mirror descent algorithm (or Follow the Regularized Leader / Exponential Weights method) with

Let us ﬁnish with a technical remark. The reason for the extra step of “mixing in” the uniform

regret if the other player deviates from using the algorithm. If one is only interested in the dynamics
when both players cooperate  this step is not necessary  and in this case the extraneous log T factor

step size ⌘  one can typically show stability￿xt− xt−1￿=O(⌘). In this case  (9) yields the rate
O￿ ⌘ log T√T ￿ for the ﬁrst player. A typical setting of ⌘∝ T−1￿2 for the second player still ensures the
O(log T￿T) regret for the ﬁrst player.
distribution stems from the goal of having an adaptive and robust method that still attainsO(T−1￿2)
disappears from the above bound  leading to the O￿ log n+log m
￿ convergence. On the technical side 
max ≥ supgDR1(f∗  g) which is potentially inﬁnite for the negative entropy function
R1. It is possible that the doubling trick or the analysis of Auer et al [2] (who encountered the
preserving the regret minimization property. We also remark that Rmax is small whenR1 is instead

the need for the extra step is the following. The adaptive step size result of Corollary 2 involves
the term R2

the p-norm; hence  the use of this regularizer avoids the extraneous logarithmic in T factor while
still preserving the logarithmic dependence on n and m. However  projection onto the simplex under
the p-norm is not as elegant as the Exponential Weights update.

same problem for the Exponential Weights algorithm) can remove the extra log T factor while still

T

4.2 Partial Information

Interestingly  up to logarithmic factors  the fast rate of the previous section is possible even in this
scenario  but we do require the knowledge of the number of actions of the opposing player (or  an
upper bound on this number). We leave it as an open problem the question of whether one can attain

We now turn to the partial (or  zero-th order) information model. Recall that the matrix A is not
known to the players  yet we are interested in ﬁnding ✏-optimal minimax strategies. On each round 
t Axt.
Now the question is  how many such observations do we need to get to an ✏-optimal minimax
strategy? Can this be done while still ensuring the usual no-regret rate?
The speciﬁc setting we consider below requires that on each round t  the two players play four

the two players choose mixed strategies ft ∈ n and xt ∈ m  respectively  and observe f T
t− f j
times  and that these four plays are -close to each other (that is ￿f i
t￿1≤  for i  j∈{1  . . .   4}).
the 1￿T -type rate with only one play per round.
u1  . . .   un−1 : orthonormal basis of n
Initialize g1  f1= 1
n 1n; Draw i0∼ Unif([n− 1])
At time t= 1 to T
Draw it∼ Unif([n− 1])
r+t =(ft+ uit−1)￿Axt
r−t =(ft− uit−1)￿Axt
¯r+t =(ft+ uit)￿Axt
¯r−t =(ft− uit)￿Axt
ˆat= n
2(r+t − r−t) uit−1
¯at= n
2(¯r+t − ¯r−t) uit
gt(i)∝ g′t−1(i) exp{−⌘tˆat(i)}
g′t=(1− ) gt+(￿n)1
ft+1(i)∝ g′t(i) exp{−⌘t+1¯at(i)}

v1  . . .   vm−1 : orthonormal basis of m
Initialize y1  x1= 1
At time t= 1 to T
Draw jt∼ Unif([m− 1])
s+t =−f￿t A(xt+ vjt−1)
s−t =−f￿t A(xt− vjt−1)
¯s+t =−f￿t A(xt+ vjt)
¯s−t =−f￿t A(xt− vjt)
ˆbt= m
2(s+t− s−t) vjt−1
¯bt= m
2(¯s+t− ¯s−t) vjt
ˆbt(i)}
yt(i)∝ y′t−1(i) exp{−⌘′t
y′t=(1− ) yt+(￿m)1
xt+1(i)∝ y′t(i) exp{−⌘′t+1
¯bt(i)}

m 1m; Draw j0∼ Unif([m−1])

Build estimates :

Build estimates :

Observe :

Player II

Play ft

Observe :

Update :

Play xt

Player I

Update :

End

End

6

 

 

1

1

and

Lemma 7. Let A∈[−1  1]n×m F= n X = m  let  be small enough (e.g. exponentially small
in m  n  T )  and let = 1￿T 2. If both players use above algorithms with the adaptive step sizes

i=1￿ˆai−¯ai−1￿2∗−￿∑t−2
i=1￿ˆai−¯ai−1￿2∗
￿ˆat−1−¯at−2￿2∗
∗−￿∑t−2
i=1￿ˆbi−¯bi−1￿2
i=1￿ˆbi−¯bi−1￿2
∗
￿ˆbt−1−¯bt−2￿2
∗

⌘t= min￿￿log(nT)￿∑t−1
28m￿log(mT)￿
28n￿log(nT)￿￿￿￿￿￿￿
⌘′t= min￿￿￿￿￿￿￿
￿log(mT)￿∑t−1
respectively  then the pair( ¯fT   ¯xT) is an
O￿￿￿￿m log(nT)￿log(mT)+ n log(mT)￿log(nT)￿
￿￿￿
m￿log(mT) log(nT)+ n￿log(nT)∑T
O￿￿￿
t=1￿xt− xt−1￿2
We leave it as an open problem to ﬁnd an algorithm that attains the 1￿T -type rate when both players
i Aej = Ai j upon drawing pure actions i  j from their respective mixed
strategies ft  xt. We hypothesize a rate better than T−1￿2 is not possible in this scenario.
5 Approximate Smooth Convex Programming

-approximate minimax equilibrium. Furthermore  if only one player (say  Player I) follows the above
algorithm  her regret against any sequence x1  . . .   xT of plays is bounded by

only observe the value eT

￿￿￿

T

T

In this section we show how one can use the structured optimization results from Section 3 for ap-
proximately solving convex programming problems. Speciﬁcally consider the optimization problem
(10)

argmax

f∈G

s.t.

c￿f
∀i∈[d]  Gi(f)≤ 1

max

(11)

argmin

sup

x∈X

f∈F

d￿i=1

x(i)Gi(f) .

a mixture of constraints with the aim of violating at least one of them.

i∈[d] Gi(f)= argmin
f∈F

This problem is in the saddle-point form  as studied earlier in the paper. We may think of the ﬁrst

whereG is a convex set and each Gi is an H-smooth convex function. Let the optimal value of the
above optimization problem be given by F∗> 0  and without loss of generality assume F∗ is known
(one typically performs binary search if it is not known). Deﬁne the setsF={f∶ f∈G  c￿f= F∗}
andX= d. The convex programming problem in (10) can now be reformulated as
player as aiming to minimize the above expression overF  while the second player maximizes over
Lemma 8. Fix  ✏ > 0. Assume there exists f0 ∈ G such that c￿f0 ≥ 0 and for every i ∈ [d] 
Gi(f0)≤ 1− . Suppose each Gi is 1-Lipschitz overF. Consider the solution
✏+ and ¯fT= 1
t=1 ft∈F is the average of the trajectory of the procedure in Lemma 4
where ↵= ✏
T∑T
for the optimization problem (11). LetR1(⋅)= 1
2 andR2 be the entropy function. Further let
2￿⋅￿2
B be a known constant such that B≥￿f∗− g0￿2 where g0∈F is some initialization and f∗∈F
is the (unknown) solution to the optimization problem. Set ⌘= argmin
⌘− H 
⌘≤H−1 ￿ B2
t =(G1(gt−1)  . . .   Gd(gt−1)). Let number of iterations T be
t =∑d
i=1 yt−1(i)∇Gi(gt−1) and M 2
⌘≤H−1￿ B2
T> 1

ˆfT=(1− ↵) ¯fT+ ↵f0

⌘ + ⌘ log d
1−⌘H￿  ⌘′= 1

M 1
such that

⌘ + ⌘ log d
1− ⌘H￿

inf

✏

7

Lemma 8 tells us that using the predictable sequences approach for the two players  one can obtain
an ✏
 -approximate solution to the smooth convex programming problem in number of iterations at

most order 1￿✏. If T1 (reps. T2) is the time complexity for single update of the predictable sequence
algorithm of Player I (resp. Player 2)  then time complexity of the overall procedure isO￿ T1+T2
￿

5.1 Application to Max-Flow

✏

We now apply the above result to the problem of ﬁnding Max Flow between a source and a sink
in a network  such that the capacity constraint on each edge is satisﬁed. For simplicity  consider a
network where each edge has capacity 1 (the method can be easily extended to the case of varying
capacity). Suppose the number of edges d in the network is the same order as number of vertices in
the network. The Max Flow problem can be seen as an instance of a convex (linear) programming
problem  and we apply the proposed algorithm for structured optimization to obtain an approximate
solution.

of iterations of the proposed procedure.

Euclidean norm squared as regularizer for the ﬂow player  then projection step can be performed in

For the Max Flow problem  the setsG andF are given by sets of linear equalities. Further  if we use
O(d) time using conjugate gradient method. This is because we are simply minimizing Euclidean
norm squared subject to equality constraints which is well conditioned. Hence T1=O(d). Similarly 
the Exponential Weights update has time complexityO(d) as there are order d constraints  and so
overall time complexity to produce ✏ approximate solution is given byO(nd)  where n is the number
Once again  we shall assume that we know the value of the maximum ﬂow F∗ (for  otherwise  we
Flow problem with f0 = 0∈G the 0 ﬂow  the time complexity to compute an ✏-approximate Max

can use binary search to obtain it).
Corollary 9. Applying the procedure for smooth convex programming from Lemma 8 to the Max

Flow is bounded by

O￿ d3￿2√log d

✏

￿ .

We then have that ˆfT∈G satisﬁes all d constraints and is ✏
￿ F∗ .

c￿ ˆfT≥￿1− ✏

 -approximate  that is

This time complexity matches the known result from [8]  but with a much simpler procedure (gradi-
ent descent for the ﬂow player and Exponential Weights for the constraints). It would be interesting

to see whether the techniques presented here can be used to improve the dependence on d to d4￿3 or
better while maintaining the 1￿✏ dependence. While the result of [5] has the improved d4￿3 depen-

dence  the complexity in terms of ✏ is much worse.

6 Discussion

We close this paper with a discussion. As we showed  the notion of using extra information about the
sequence is a powerful tool with applications in optimization  convex programming  game theory  to
name a few. All the applications considered in this paper  however  used some notion of smoothness
for constructing the predictable process Mt. An interesting direction of further research is to isolate
more general conditions under which the next gradient is predictable  perhaps even when the func-
tions are not smooth in any sense. For instance one could use techniques from bundle methods to
further restrict the set of possible gradients the function being optimized can have at various points
in the feasible set. This could then be used to solve for the right predictable sequence to use so as
to optimize the bounds. Using this notion of selecting predictable sequences one can hope to derive
adaptive optimization procedures that in practice can provide rapid convergence.

Acknowledgements: We thank Vianney Perchet for insightful discussions. We gratefully acknowl-
edge the support of NSF under grants CAREER DMS-0954737 and CCF-1116928  as well as Dean’s
Research Fund.

8

References
[1] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: A meta-algorithm

and applications. Theory of Computing  8(1):121–164  2012.

[2] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algo-

rithms. Journal of Computer and System Sciences  64(1):48–75  2002.

[3] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

[4] C.-K. Chiang  T. Yang  C.-J. Lee  M. Mahdavi  C.-J. Lu  R. Jin  and S. Zhu. Online optimiza-

tion with gradual variations. In COLT  2012.

[5] P. Christiano  J. A Kelner  A. Madry  D. A. Spielman  and S.-H. Teng. Electrical ﬂows  lapla-
cian systems  and faster approximation of maximum ﬂow in undirected graphs. In Proceedings
of the 43rd annual ACM symposium on Theory of computing  pages 273–282. ACM  2011.

[6] C. Daskalakis  A. Deckelbaum  and A. Kim. Near-optimal no-regret algorithms for zero-
sum games. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete
Algorithms  pages 235–254. SIAM  2011.

[7] Y. Freund and R. Schapire. Adaptive game playing using multiplicative weights. Games and

Economic Behavior  29(1):79–103  1999.

[8] A. Goldberg and S. Rao. Beyond the ﬂow decomposition barrier. Journal of the ACM (JACM) 

45(5):783–797  1998.

[9] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization  15(1):229–251  2004.

[10] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming 

103(1):127–152  2005.

[11] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of

the 26th Annual Conference on Learning Theory (COLT)  2013.

9

,Sasha Rakhlin
Karthik Sridharan