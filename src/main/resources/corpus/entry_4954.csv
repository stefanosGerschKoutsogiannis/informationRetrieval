2019,Semi-Implicit Graph Variational Auto-Encoders,Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand the flexibility of variational graph auto-encoders (VGAE) to model graph data. SIG-VAE employs a hierarchical variational framework to enable neighboring node sharing for better generative modeling of graph dependency structure  together with a Bernoulli-Poisson link decoder. Not only does this hierarchical construction provide a more flexible generative graph model to better capture real-world graph properties  but also does SIG-VAE naturally lead to semi-implicit hierarchical variational inference that allows faithful modeling of implicit posteriors of given graph data  which may exhibit heavy tails  multiple modes  skewness  and rich dependency structures. SIG-VAE integrates a carefully designed generative model  well suited to model real-world sparse graphs  and a sophisticated variational inference network  which propagates the graph structural information and distribution uncertainty to capture complex posteriors. SIG-VAE clearly outperforms a simple combination of VGAE with variational inference  including semi-implicit variational inference~(SIVI) or normalizing flow (NF)  which does not propagate uncertainty in its inference network  and provides more interpretable latent representations than VGAE does. Extensive experiments with a variety of graph data show that SIG-VAE significantly outperforms state-of-the-art methods on several different graph analytic tasks.,Semi-Implicit Graph Variational Auto-Encoders

Arman Hasanzadeh†∗  Ehsan Hajiramezanali†∗  Nick Dufﬁeld†  Krishna Narayanan† 

Mingyuan Zhou‡  Xiaoning Qian†

† Department of Electrical and Computer Engineering  Texas A&M University

{armanihm  ehsanr  duffieldng  krn  xqian}@tamu.edu
‡ McCombs School of Business  The University of Texas at Austin

mingyuan.zhou@mccombs.utexas.edu

Abstract

Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand
the ﬂexibility of variational graph auto-encoders (VGAE) to model graph data.
SIG-VAE employs a hierarchical variational framework to enable neighboring node
sharing for better generative modeling of graph dependency structure  together
with a Bernoulli-Poisson link decoder. Not only does this hierarchical construction
provide a more ﬂexible generative graph model to better capture real-world graph
properties  but also does SIG-VAE naturally lead to semi-implicit hierarchical
variational inference that allows faithful modeling of implicit posteriors of given
graph data  which may exhibit heavy tails  multiple modes  skewness  and rich
dependency structures. SIG-VAE integrates a carefully designed generative model 
well suited to model real-world sparse graphs  and a sophisticated variational
inference network  which propagates the graph structural information and distri-
bution uncertainty to capture complex posteriors. SIG-VAE clearly outperforms a
simple combination of VGAE with variational inference  including semi-implicit
variational inference (SIVI) or normalizing ﬂow (NF)  which does not propagate
uncertainty in its inference network  and provides more interpretable latent repre-
sentations than VGAE does. Extensive experiments with a variety of graph data
show that SIG-VAE signiﬁcantly outperforms state-of-the-art methods on several
different graph analytic tasks.

1

Introduction

Analyzing graph data is an important machine learning task with a wide variety of applications.
Transportation networks  social networks  gene co-expression networks  and recommendation systems
are a few example datasets that can be modeled as graphs  where each node represents an agent (e.g. 
road intersection  person  and gene) and the edges manifest the interactions between the agents. The
main challenge for analyzing graph datasets for link prediction  clustering  or node classiﬁcation 
is how to deploy graph structural information in the model. Graph representation learning aims to
summarize the graph structural information by a feature vector in a low-dimensional latent space 
which can be used in downstream analytic tasks.
While the vast majority of existing methods assume that each node is embedded to a deterministic
point in the latent space [5  2  25  30  14  15  10  4]  modeling uncertainty is of crucial importance in
many applications  including physics and biology. For example  when link prediction in Knowledge
Graphs is used for driving expensive pharmaceutical experiments  it would be beneﬁcial to know
what is the conﬁdence level of a model in its prediction. To address this  variational graph auto-
encoder (VGAE) [18] embeds each node to a random variable in the latent space. Despite its

∗Both authors contributed equally.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

popularity  1) the Gaussian assumption imposed on the variational distribution restricts its variational
inference ﬂexibility when the true posterior distribution given a graph clearly violates the Gaussian
assumption; 2) the adopted inner-product decoder restricts its generative model ﬂexibility. While
recent study tries to address the ﬁrst problem by changing the prior distribution but does not show
much practical success [11]  the latter one is not well-studied yet to the best of our knowledge.
Inspired by recently developed semi-implicit variational inference (SIVI) [39] and normalizing
ﬂow (NF) [27  17  23]  which offer the interesting combination of ﬂexible posterior distribution and
effective optimization  we propose a hierarchical variational graph framework for node embedding of
graph structured data  notably increasing the expressiveness of the posterior distribution for each node
in the latent space. SIVI enriches mean-ﬁeld variational inference with a ﬂexible (implicit) mixing
distribution. NF transforms a simple Gaussian random variable through a sequence of invertible
differentiable functions with tractable Jacobians. While NF restricts the mixing distribution in the
hierarchy to have an explicit probability density function  SIVI does not impose such a constraint.
Both SIVI and NF can model complex posterior distributions  which will help when the underlying
true embedded node distribution exhibits heavy tails and/or multiple modes. We further argue that the
graph structure cannot be fully exploited by the posterior distribution from the trivial combination of
SIVI and/or NF with VGAE  if not integrating graph neighborhood information. On the other hand 
it does not address the ﬂexibility of the generative model as stated as the second VGAE problem.
To address the aforementioned issues  instead of explicitly choosing the posterior distribution family
in previous works [18  11]  our hierarchical variational framework adopts a stochastic generative
node embedding model that can learn implicit posteriors while maintaining simple optimization.
Speciﬁcally  we innovate a semi-implicit hierarchical construction to model the posterior distribution
to best ﬁt both the graph topology and node attributes given graphs. With SIVI  even if the posterior
is not tractable  its density can be evaluated with Monte Carlo estimation  enabling efﬁcient model
inference on top of highly enhanced model ﬂexibility/expressive power. Our semi-implicit graph
variational auto-encoder (SIG-VAE) can well model heavy tails  skewness  multimodality  and other
characteristics that are exhibited by the posterior but failed to be captured by existing VGAEs.
Furthermore  a Bernoulli-Poisson link function [41] is adopted in the decoder of SIG-VAE to increase
the ﬂexibility of the generative model and better capture graph properties of real-world networks that
are often sparse. SIG-VAE facilitates end-to-end learning for various graph analytic tasks evaluated in
our experiments. For link prediction  SIG-VAE consistently outperforms state-of-the-art methods by
a large margin. It is also comparable with state-of-the-arts when modiﬁed to perform two additional
tasks  node classiﬁcation and graph clustering  even though node classiﬁcation is more suitable to be
solved by supervised learning methods. We further show that the new decoder is able to generate
sparse random graphs whose statistics closely resemble those of real-world graph data. These results
clearly demonstrate the great practical values of SIG-VAE. The implementation of our proposed
model is accessible at https://github.com/sigvae/SIGraphVAE.

2 Background

Variational graph auto-encoder (VGAE). Many node embedding methods derive deterministic
latent representations [14  15  10]. By expanding the variational auto-encoder (VAE) notion to graphs 
Kipf and Welling [18] propose to solve the following problem by embedding the nodes to Gaussian
random vectors in the latent space.
Problem 1. Given a graph G = (V E) with the adjacency matrix A and M-dimensional node
attributes X ∈ RN×M   ﬁnd the probability distribution of the latent representation of nodes Z ∈
RN×L   i.e.  p(Z| X  A).
Finding the true posterior  p(Z| X  A)  is often difﬁcult and intractable. In Kipf and Welling [18]  it
i=1 qi(zi | ψi) and qi(zi | ψi) = N (zi | ψi)
with ψi = {µi  diag(σ2
i )}. Here  µi and σi are l-dimensional mean and standard deviation vectors
corresponding to node i  respectively. The parameters of q(Z| ψ)  i.e.  ψ = {ψi}N
i=1  are modeled
and learned using two graph convolutional neural networks (GCNs) [19]. More precisely  µ =
GCNµ(X  A)  log σ = GCNσ(X  A) and µ and σ are matrices of µi’s and σi’s  respectively. Given
Z  the decoder in VGAE is a simple inner-product decoder as p(Ai j = 1| zi  zj) = sigmoid(zi zT
j ).

is approximated by a Gaussian distribution  q(Z| ψ) =(cid:81)N

2

The parameters of the model are found by optimizing the well known evidence lower
bound (ELBO) [16  7  8  33]: L = Eq(Z | ψ)[p(A| Z)] − KL[q(Z| ψ) || p(Z)]. Note that q(Z| ψ)
here is equivalent to q(Z| X  A). Despite promising results shown by VGAE  a well-known issue in
variational inference is underestimating the variance of the posterior. The reason behind this is the
mismatch between the representation power of the variational family to which q is restricted and the
complexity of the true posterior  in addition to the use of KL divergence  which is asymmetric  to
measure how different q is from the true posterior.

Semi-implicit variational inference (SIVI). To well characterize the posterior while maintaining
simple optimization  semi-implicit variational inference (SIVI) has been proposed by Yin and Zhou
[39]  which is also related to the hierarchical variational inference [26] and auxiliary deep generative
models [21]; see Yin and Zhou [39] for more details about their connections and differences. It
has been shown that SIVI can capture complex posterior distributions like multimodal or skewed
distributions  which can not be captured by a vanilla VI due to its restricted exponential family
assumption over both the prior and posterior in the latent space. SIVI assumes that ψ  the parameters
of the posterior  are drawn from an implicit distribution rather than being analytic. This hierarchical
construction enables ﬂexible mixture modeling and allows to have more complex posteriors while
maintaining simple optimization for model inference. More speciﬁcally  Z ∼ q(Z| ψ) and ψ ∼ qφ(ψ)
with φ denoting the distribution parameters to be inferred. Marginalizing ψ out leads to the random
variables Z drawn from a distribution family H indexed by variational parameters φ  expressed as

H =

hφ(Z) : hφ(Z) =

q(Z| ψ)qφ(ψ) dψ

.

(1)

(cid:26)

(cid:90)

ψ

(cid:27)

The importance of semi-implicit formulation is that while the original posterior q(Z| ψ) is explicit
and analytic  the marginal distribution  hφ(Z) is often implicit. Note that  if qφ equals a delta function 
then hφ is an explicit distribution. Unlike regular variational inference that assumes independent
latent dimensions  semi-implicit does not impose such a constraint. This enables the semi-implicit
variational distributions to model very complex multivariate distributions.
Since the marginal probability density function hφ(Z) is often intractable  SIVI derives a lower
bound for ELBO  as follows  to optimize the variational parameters.
L = EZ∼hφ(Z)

(cid:104)

(cid:105)

log

p(Y  Z)
hφ(Z)

= −KL(Eψ∼qφ(ψ)[q(Z| ψ)] || p(Z| Y)) + log p(Y)
≥ −Eψ∼qφ(ψ)KL(q(Z| ψ) || p(Z| Y)) + log p(Y)
= Eψ∼qφ(ψ)

(cid:104)EZ∼q(Z | ψ)

(cid:18) p(Y  Z)

(cid:19)(cid:105)(cid:105)

(cid:104)

= L(q(Z| ψ)  qφ(ψ)) 
(2)
where Y is the observations. The inequality EψKL(q(Z| ψ)||p(Z)) ≥ KL(Eψ[q(Z|ψ)]||p(Z)) has
been used to derive L. Optimizing this lower bound  however  could drive the mixing distribution
qφ(ψ) towards a point mass density. To address the degeneracy issue  SIVI adds a nonnegative
regularization term  leading to a surrogate ELBO that is asymptotically exact [39]. We will further
discuss this in the supplementary material.

q(Z| ψ)

log

Normalizing ﬂow (NF). NF [23] also enriches the posterior distribution families. Compared to
SIVI  NF imposes explicit density functions for the mixing distributions in the hierarchy while SIVI
only requires qφ to be reparameterizable. This makes SIVI more ﬂexible  especially when using it
for graph analytics as explained in the next section  since the SIVI posterior can be generated by
transforming random noise using any ﬂexible function  for example a neural network.

3 Baselines: Variational Inference with VGAE

Before presenting our semi-implicit graph variational auto-encoder (SIG-VAE)  we ﬁrst introduce
two baseline methods that directly combines SIVI and NF with VGAE.
SIVI-VGAE. To address Problem 1 while well characterizing the posterior with modeling ﬂexibility
in the VGAE framework  the naive solution is to take the semi-implicit variational distribution in

3

SIVI for modeling latent variables in VGAE  following the hierarchical formulation

Z ∼ q(Z| ψ) 

ψ ∼ qφ(ψ | X  A) 

(3)

by introducing the implicit prior distribution parametrized by ψ  which can be sampled from the
reparametrizable qφ(ψ | X  A). Such a hierarchical semi-implicit construct not only leads to ﬂexible
mixture modeling of the posterior but also enables efﬁcient model inference  for example  with φ
being parameterized by deep neural networks. In this framework  the features from multiple layers
of GNNs can be aggregated and then transformed via multiple fully connected layers after being
concatenated by random noise to derive the posterior distribution for each node separately. More
speciﬁcally  SIVI-VGAE injects random noise at C different stochastic fully connected layers for
each node independently:

hu = GNNu(A  CONCAT(X  hu−1)) 
(cid:96)(i)
t = T t((cid:96)(i)

for u = 1  . . .   L  h0 = 0
L )  where t ∼ qt() for t = 1  . . .   C  (cid:96)(i)

t−1  t  h(i)

0 = 0

L )  Σi(A  X) = gΣ((cid:96)(i)

C   h(i)
L ) 

q(Z| A  X  µ  Σ) =(cid:81)N

µi(A  X) = gµ((cid:96)(i)
C   h(i)
i=1q(zi | A  X  µi  Σi) 

q(zi | A  X  µi  Σi) = N (µi(A  X)  Σi(A  X)) 
where T t  gµ  and gσ are all deterministic neural networks  i is the node index  L is the number
of GNN layers  and t is random noise drawn from the distribution qt. Note that in the equations
above  GNN is any type of existing graph neural networks  such as graph convolutional neural
network (GCN) [19]  GCN with Chebyshev ﬁlters [13]  GraphSAGE [15]  jumping knowledge (JK)
networks [36]  and graph isomorphism network (GIN) [37]. Given the GNNL output hL  µi(A  X)
and Σi(A  X) are now random variables rather than following vanilla VAE to assume deterministic
values. In this way  however  the constructed implicit distributions may not capture the dependency
between neighboring nodes completely. Note that we consider SIVI-VGAE as a naive version of
our proposed SIG-VAE (and call it as Naive SIG-VAE in the rest of the paper)  which is speciﬁcally
designed with neighborhood sharing to capture complex dependency structures in networks  as
detailed in the next section. Please also note that the ﬁrst layer of SIVI can be integrated with NF
rather than simple Gaussian. We leave that for future study.
NF-VGAE. It is also possible to enable VGAE model ﬂexibility by other existing variational inference
methods  for example using NF. However  NF requires deterministic transform functions whose
Jacobians shall be easy to compute  which limits the ﬂexibility when considering complex dependency
structures in graph analytic tasks. We indeed have constructed a non-Gaussian VGAE  i.e. NF-based
variational graph auto-encoder (NF-VGAE) as follows
hu = GNNu(A  CONCAT(X  hu−1)) 

for u = 1  . . .   L  h0 = 0

(4)

q0(Z(0) | A  X) =(cid:81)N
qK(Z(K) | A  X) =(cid:81)N

µ(A  X) = GNNµ(A  CONCAT(X  hL))  Σ(A  X) = GNNΣ(A  CONCAT(X  hL)) 
i )) 

| A  X) = N (µi  diag(σ2

| A  X)  with q0(z(0)

i=1q0(z(0)

i

i

i=1q0(z(K)

i

|A  X) 

ln(qK(z(K)

i

|−)) = ln(q0(z(0)

i

)) −(cid:88)

ln|det ∂fk
∂z(k)

i

| 

k

where the posterior distribution qK(Z(K)|A  X) is obtained by successively transforming a Gaussian
random variable Z(0) with distribution q0 through a chain of K invertible differentiable transforma-
tions fk : Rd → Rd. We will further discuss this in the supplementary material. NF-VGAE is a
two-step inference method that 1) starts with Gaussian random variables and then 2) transforms them
through a series of invertible mappings. We emphasize again that in NF-VGAE  the GNN output
layers are deterministic without neighborhood distribution sharing due to the deterministic nature of
the initial density parameters in q0.

4 Semi-implicit graph variational auto-encoder (SIG-VAE)

While the above two models are able to approximate more ﬂexible and complex posterior  such trivial
combinations may fail to fully exploit graph dependency structure because they are not capable of
propagating uncertainty between neighboring nodes. To enable effective uncertainty propagation 
which is the essential factor to capture complex posteriors with graph data  we develop a carefully

4

designed generative model  SIG-VAE  to better integrate variational inference and VGAE with a
natural neighborhood sharing scheme.
To have tractable posterior inference  we construct SIG-VAE using a hierarchy of multiple stochastic
layers. Speciﬁcally  the ﬁrst stochastic layer q(Z| X  A) is reparameterizable and has an analytic
probability density function. The layers added after are reparameterizable and computationally
efﬁcient to sample from. More speciﬁcally  we adopt a hierarchical encoder in SIG-VAE that injects
random noise at L different stochastic layers:

q(Z| A  X  µ  Σ) =(cid:81)N

i=1q(zi | A  X  µi  Σi) 

hu = GNNu(A  CONCAT(X  u  hu−1))  where u ∼ qu() for u = 1  . . .   L  h0 = 0
µ(A  X) = GNNµ(A  CONCAT(X  hL))  Σ(A  X) = GNNΣ(A  CONCAT(X  hL)) 

(5)
(6)
q(zi | A  X  µi  Σi) = N (µi(A  X)  Σi(A  X)).
Note that in the equations above µ and Σ are random variables and thus q(Z| X  A) is not necessarily
Gaussian after marginalization; u is N-dimensional random noise drawn from a distribution qu;
and qu is chosen such that the samples drawn from it are the same type as X  for example if X is
categorical  Bernoulli is a good choice for qu. By concatenating the random noise and node attributes 
the output of GNNs are random variables rather than deterministic vectors. Their expressive power is
inherited in SIG-VAE to go beyond Gaussian  exponential family  or von Mises-Fisher [11] posterior
distributions for the derived latent representations.
In SIG-VAE  when inferring each node’s latent
posterior  we incorporate the distributions of
the neighboring nodes  better capturing graph
dependency structure than sharing determin-
istic features from GNNs. More speciﬁcally 
the input to our model at stochastic layer u is
CONCAT(X  u) so that the outputs of the sub-
sequent stochastic layers give mixing distribu-
tions by integrating information from neighbor-
ing nodes (Fig. 1). The ﬂexibility of SIG-VAE
directly working on the stochastic distribution
parameters in (5-6) allows neighborhood sharing
to achieve better performance in graph analytic
tasks. We argue that the uncertainty propagation in our carefully designed SIG-VAE  which is the an
outcome of using GNNs and adding noise in the input in equations (5-6)  is the key factor in capturing
more faithful and complex posteriors. Note that (5) is different from the NF-VAE construction (3) 
where the GNN output layers are deterministic. Through experiments  we show that this uncertainty
neighborhood sharing is key for SIG-VAE to achieve superior graph analysis performance.
We further argue that increasing the ﬂexibility of variational inference is not enough to better model
real-world graph data as the optimal solution of the generative model does not change. In SIG-VAE 
the Bernoulli-Poisson link [41] is adopted for the decoder to further increase the expressiveness of
the generative model. Potential extensions with other decoders can be integrated with SIG-VAE if

Figure 1: SIG-VAE diffuses the distributions of the
neighboring nodes  which is more informative than shar-
ing deterministic features  to infer each node’s latent
distribution.

needed. Let Ai j = δ(mij > 0)  mij ∼ Poisson(cid:0) exp((cid:80)l

k=1 rkzik zjk)(cid:1)  and hence
p(Ai j = 1| zi  zj  R) = 1−e− exp ((cid:80)L

N(cid:89)

N(cid:89)

p(A| Z  R) =

p(Ai j | zi  zj  R) 

k=1 rkzik zjk)  (7)

i=1

j=1

where R ∈ RL ×L

+

4.1

Inference

is a diagonal matrix with diagonal elements rk.

To derive the ELBO for model inference in SIG-VAE  we must take into account the fact that ψ has
to be drawn from a distribution. Hence  the ELBO moves beyond the simple VGAE as

L = −KL(Eψ∼qφ(ψ | X A)[q(Z| ψ)] || p(Z)) + Eψ∼qφ(ψ | X A)[EZ∼q(Z | ψ)[log p(A| Z)]] 

(8)
where hφ is deﬁned in (1). The marginal probability density function hφ(Z|X  A) is often intractable 
so the Monte Carlo estimation of the ELBO  L  is prohibited. To address this issue and infer

5

Figure 2: Swiss roll graph (left) and its latent representation using SIG-VAE (middle) and VGAE (right). The
latent representations (middle and right) are heat maps in R3. We expect that the embedding of the Swiss roll
graph with inner-product decoder to be a curved plane in R3  which is clearly captured better by SIG-VAE.

Figure 3: Latent representation distributions of ﬁve example nodes from the Swiss roll graph using SIG-VAE
(blue) and VGAE (red). SIG-VAE clearly infers more complex distributions that can be multi-modal  skewed 
and with sharp and steep changes. This helps SIG-VAE to better represent the nodes in the latent space.

variational parameters of SIG-VAE  we can derive a lower bound for the ELBO as follows (see the
supplementary material for more details)

(cid:2)EZ∼q(Z | ψ)[log p(A| Z)](cid:3) ≤ L.

L = −Eψ∼qφ(ψ | X A)[KL(q(Z| ψ)|| p(Z))] + Eψ∼qφ(ψ | X A)

Further implementation details and the derivation of the surrogate ELBO can be found in the
supplementary material.

5 Experiments

We test the performances of SIG-VAE on different graph
analytic tasks: 1) interpretability of SIG-VAE compared
to VGAE  2) link prediction in various real-world graph
datasets including graphs with node attributes and without
node attributes  3) graph generation  4) node classiﬁcation
in the citation graphs with labels. In all of the experi-
ments  GCN [19] is adopted for all the GNN modules in
SIG-VAE  Naive SIG-VAE  and NF-VGAE  implemented
in Tensorﬂow [1]. The PyGSP package [12] is used to
generate synthetic graphs. Implementation details for all
the experiments  together with graph data statistics  can
be found in the supplementary material.

5.1

Interpretable latent representations

Figure 4: The nodes with multi-modal pos-
teriors (red nodes) reside between different
communities in Swiss Roll graph.

We ﬁrst demonstrate the expressiveness of SIG-VAE by illustrating the approximated variational
distributions of node latent representations. We show that SIG-VAE captures the graph structure
better and has a more interpretable embedding than VGAE on a generated Swiss roll graph with
200 nodes and 1244 edges (Fig. 2). In order to provide a fair comparison  both models share an
identical implementation with the inner-product decoder and same number of parameters. We simply
consider the identity matrix IN as node attributes and choose the latent space dimension to be three
in this experiment. This graph has a simple plane like structure. As the inner-product decoder
assumes that the information is embedded in the angle between latent vectors  we expect that the
node embedding to map nodes of the Swiss roll graph into a curve in the latent space. As we can

6

0.50.00.51.01.50.80.60.40.20.00.20.40.6-2.0-1.00.01.02.01.51.00.50.00.51.01.52.02.01.51.00.50.00.51.01.5-3.0-1.50.01.53.02.52.01.51.00.50.00.51.01.52.52.01.51.00.50.00.51.0321012.52.01.51.00.50.00.51.01012210122.01.51.00.50.00.51.01.52.52.01.51.00.50.00.51.01.52.01.51.00.50.00.51.0210123Table 1: Link prediction performance in networks with node attributes.

Cora

Citeseer

Pubmed

Method

SC [31]
DW [25]
GAE [18]
VGAE [18]
S-VGAE [11]
SEAL [40]
G2G [9]
NF-VGAE
Naive SIG-VAE
SIG-VAE (IP)
SIG-VAE

AUC

84.6 ± 0.01
83.1 ± 0.01
91.0 ± 0.02
91.4 ± 0.01
94.10 ± 0.1
90.09 ± 0.1
92.10 ± 0.9
92.42 ± 0.6
93.97 ± 0.5
94.37 ± 0.1
96.04 ± 0.04

AP

88.5 ± 0.00
85.0 ± 0.00
92.0 ± 0.03
92.6 ± 0.01
94.10 ± 0.3
83.01 ± 0.3
92.58 ± 0.8
93.08 ± 0.5
93.29 ± 0.4
94.41 ± 0.1
95.82 ± 0.06

AUC

80.5 ± 0.01
80.5 ± 0.02
89.5 ± 0.04
90.8 ± 0.02
94.70 ± 0.2
83.56 ± 0.2
95.32 ± 0.7
91.76 ± 0.3
94.25 ± 0.8
95.90 ± 0.1
96.43 ± 0.02

AP

85.0 ± 0.01
83.6 ± 0.01
89.9 ± 0.05
92.0 ± 0.02
95.20 ± 0.2
77.58 ± 0.2
95.57 ± 0.7
93.04 ± 0.8
93.60 ± 0.9
95.46 ± 0.1
96.32 ± 0.02

AUC

84.2 ± 0.02
84.4 ± 0.00
96.4 ± 0.00
94.4 ± 0.02
96.00 ± 0.1
96.71 ± 0.1
94.28 ± 0.3
96.59 ± 0.3
96.53 ± 0.7
96.73 ± 0.1
97.01 ± 0.07

AP

87.8 ± 0.01
84.1 ± 0.00
96.5 ± 0.00
94.7 ± 0.02
96.00 ± 0.1
90.10 ± 0.1
93.38 ± 0.5
96.68 ± 0.4
96.01 ± 0.5
96.67 ± 0.1
97.15 ± 0.04

see in Fig. 2  SIG-VAE derives a clearly more interpretable planar latent structure than VGAE. We
also show the posterior distributions of ﬁve randomly selected nodes from the graph in Fig. 3. As we
can see  SIG-VAE is capable of inferring complex distributions. The inferred distributions can be
multi-modal  skewed  non-symmetric  and with sharp and steep changes. These complex distributions
help the model to get a more realistic embedding capturing the intrinsic graph structure. To explain
why multi-modality may arise  we used Asynchronous Fluid [24] to visualize the Swiss Roll graph
by highlighting detected communities with different colors in Fig. 4. Note that we used a different
layout from the one in Fig. 2(a) to better visualize the communities in the graph. The three red (two
orange) nodes are the nodes with multi-modal (skewed) distributions in Fig. 3. These nodes with
multi-modal posteriors reside between different communities; hence  with a probability  they could
be assigned to multiple communities. The supplementary material contains additional results and
discussions with a torus graph  with similar observations.

5.2 Accurate link prediction

We further conduct extensive experiments for link prediction with various real-world graph datasets.
Our results show that SIG-VAE signiﬁcantly outperforms well-known baselines and state-of-the-art
methods in all benchmark datasets. We consider two types of datasets  i.e.  datasets with node
attributes and datasets without attributes. We preprocess and split the datasets as done in Kipf and
Welling [18] with validation and test sets containing 5% and 10% of network links  respectively. We
learn the model parameters for 3500 epochs with the learning rate 0.0005 and the validation set used
for early stopping. The latent space dimension is set to 16. The hyperparameters of SIG-VAE  Naive
SIG-VAE  and NF-VGAE are the same for all the datasets. For fair comparison  all methods have
the similar number of parameters as the default VGAE. The supplementary material contains further
implementation details. We measure the performance by average precision (AP) and area under the
ROC curve (AUC) based on 10 runs on a test set of previously removed links in these graphs.
With node attributes. We consider three graph datasets with node attribbutes—Citeseer  Cora  and
Pubmed [28]. The number of node attributes for these dataset are 3703  1433  and 500 respectively.
Other statistics of the datasets are summarized in the supplement Table 1. We compare the results
of SIG-VAE  Naive SIG-VAE  and NF-VGAE with six state-of-the-art methods  including spectral
clustering (SC)  DeepWalk (DW) [25]   GAE [18]  VGAE [18]  S-VGAE [11]  and SEAL [40].
The inner-product decoder is also adopted in SIG-VAE to clearly demonstrate the advantages of the
semi-implicit hierarchical variational distribution for the encoder.
We use the same hyperparameters for the competing methods as stated in [40  18  11]. As we can see
in Table 1  SIG-VAE shows signiﬁcant improvement in terms of both AUC and AP over state-of-the-
art methods. Note the standard deviation of SIG-VAE is also smaller compared to other methods 
indicating stable semi-implicit variational inference. Compared to the baseline VGAE  more ﬂexible
posterior in three proposed methods SIGVAE (with both inner-product and Bernoulli-Poisson link
decoders)  Naive SIG-VAE  and NF-VGAE can clearly improve the link prediction accuracy. This
suggests that the Gaussian assumption does not hold for these graph structured data. The performance
improvement of SIG-VAE with inner-product decoder (IP) over Naive SIG-VAE and NF-VGAE
clearly demonstrates the advantages of neighboring node sharing  especially in the smaller graphs.
Even for the large graph Pubmed  on which VGAE performs similar to S-VGAE  our SIG-VAE still
achieves the highest link prediction accuracy  showing the importance of all modeling components

7

Table 2: AUC and AP of link prediction in networks without node attributes. * indicates that the
numbers are reported from Zhang and Chen [40]. The supplementary material contains the complete
result tables with standard deviation values.

Metrics Data

AUC

AP

USAir
NS
Yeast
Power
Router
USAir
NS
Yeast
Power
Router

MF∗
94.08
74.55
90.28
50.63
78.03
94.36
78.41
92.01
53.50
82.59

SBM∗ N2V∗
91.44
94.85
91.52
92.30
91.41
93.67
76.22
66.57
65.46
85.65
89.71
95.08
94.28
92.13
94.90
92.73
81.49
65.48
84.67
68.66

LINE∗
81.47
80.63
87.45
55.637
67.15
79.70
85.17
90.55
56.66
71.92

SC∗
74.22
89.94
93.25
91.78
68.79
78.07
90.83
94.63
91.00
73.53

GAE VGAE∗
89.28
93.09
94.04
93.14
93.74
93.88
71.20
72.21
61.51
55.73
89.27
95.14
95.83
95.26
95.19
95.34
75.91
77.13
67.50
70.36

SEAL∗ G2G NF-VGAE N-SIG-VAE SIG-VAE(IP)
97.09
97.71
97.20
84.18
95.68
95.70
98.12
97.95
86.69
95.66

97.56
98.75
98.11
95.04
95.94
97.50
98.53
97.97
96.50
94.94

94.22
98.00
93.36
93.67
92.66
94.48
97.83
94.24
93.80
92.80

92.17
98.18
97.34
91.35
85.98
90.22
97.43
97.83
92.29
86.28

95.74
98.38
97.86
94.61
93.56
96.27
98.52
98.18
95.76
95.88

SIG-VAE

94.52
99.17
98.32
96.23
96.13
94.95
99.24
98.41
97.28
96.86

in the proposed method including non-Gaussian posterior  using neighborhood distribution  and the
sparse Bernoulli-Poisson link decoder.
Without node attributes. We further consider ﬁve graph datasets without node attributes—USAir 
NS [22]  Router [29]  Power [34] and Yeast [32]. The data statistics are summarized in the supplement
Table 1. We compare the performance of our models with seven competing state-of-the-art methods
including matrix factorization (MF)  stochastic block model (SBM) [3]  node2vec (N2V) [14]  LINE
[30]  spectral clustering (SC)  VGAE [18]  S-VGAE [11]  and SEAL [40].
For baseline methods  we use the same hyperparameters as stated in Zhang et al. [40]. For datasets
without node attributes  we use a two-stage learning process for SIG-VAE. First  the embedding of
each node is learned in the 128-dimensional latent space while injecting 5-dimensional Bernoulli
noise to the system. Then the learned embedding is taken as node features for the second stage to learn
16 dimensional embedding while injecting 64-dimensional noise to SIG-VAE. Through empirical
experiments  we found that this two-stage learning converges faster than end-to-end learning. We
follow the same procedure for Naive SIG-VAE and NF-VGAE.
As we can see in Table 2  SIG-VAE again shows the consistent superior performance compared to the
competing methods  especially over the baseline VGAE  in both AUC and AP. It is interesting to note
that  while the proposed Berhoulli-Poisson decoder works well for sparser graphs  especially NS and
Router datasets  SIG-VAE with inner-product decoder shows superior performance for the USAir
graph which is much denser. Compared to the baseline VGAE  both Naive SIG-VAE and NF-VGAE
improve the results with a large margin in both AUC and AP  showing the beneﬁts of more ﬂexible
posterior. Comparing SIG-VAE with two other ﬂexible inference methods shows not only SIG-VAE
is not restricted to the Gaussian assumption  which is not a good ﬁt for link prediction with the
inner-product decoder [11]  but also it is able to model ﬂexible posterior considering graph topology.
The results for the link prediction of the Power graph clearly magniﬁes this fact as SIG-VAE improves
the accuracy by 34% compared to VGAE. The supplementary material contains the results with
standard deviation values over different runs  showing the stability again.
Ablation studies have also been run to evaluate SIG-VAE with inner-product decoder in link prediction
for citation graphs without using node attributes. The [AUC  AP] are [91.14  90.99] for Cora and
[88.72  88.24] for Citeseer  lower than the values from SIG-VAE with attributes in Table 1 but are still
competitive against existing methods (even with node attributes)  showing the ability of SIG-VAE
of utilizing graph structure. While some of the methods  like SEAL  work well for graphs without
node attributes and some of others  like VGAE  get good performance for graphs with node attributes 
SIG-VAE consistently achieves superior performance in both types of datasets. This is due to the fact
that SIG-VAE can learn implicit distributions for nodes  which are very powerful in capturing graph
structure even without any node attributes.

5.3 Graph generation

To further demonstrate the ﬂexibility of SIG-VAE as a generative model  we have used the inferred
embedding representations to generate new graphs. For example  SIG-VAE infers network parameters
for Cora whose density and average clustering coefﬁcients are 0.00143 and 0.24  respectively. Using
the inferred posterior and learned decoder  a new graph is generated with corresponding rk to see
if its graph statistics are close to the original ones. Please note that we have shrunk inferred rk’s
smaller than 0.01 to 0. The density and average clustering coefﬁcients of this generated graph based
on SIG-VAE are 0.00147 and 0.25  respectively  which are very close to the original graph. We also
generate new graphs based on SIG-VAE with the inner-product decoder and VGAE. The density and

8

average clustering coefﬁcients of the generated graphs based on SIG-VAE (IP) and VGAE are same 
i.e. 0.1178 and 0.49  respectively  showing the inner-product decoder may not be a good choice for
sparse graphs. The supplementary material includes more examples.

5.4 Node classiﬁcation & graph clustering

Table 3: Summary of results in terms of
classiﬁcation accuracy (in percent).

We also have applied SIG-VAE for node classiﬁcation on
citation graphs with labels by modifying the loss func-
tion to include graph reconstruction and semi-supervised
classiﬁcation terms. Results are summarized in Table 3.
Our model exhibits strong generalization properties  high-
lighted by its competitive performance compared to the
state-of-the-art methods  despite not being trained specif-
ically for this task. To show the robustness of SIG-VAE
to missing edges  we randomly removed 10  20  50 and
70 (%) edges while keeping node attributes. The mean
accuracy of 10 run for Cora (2 layers [32 16]) are 79.5 
78.7  75.3 and 60.6  respectively. The supplementary ma-
terial contains additional results and discussion for graph
clustering  again without speciﬁc model tuning.
SIG-VAE has demonstrated state-of-the-art performances in link prediction and comparable results
on other tasks  clearly showing the potential of SIG-VAE on different graph analytic tasks.

Cora Citeseer Pubmed
70.7
59.5
71.1
59.0
63.0
68.0
65.3
67.2
73.9
75.1
77.2
75.7
81.5
79.0
79.3
79.7

Method
ManiReg [6]
SemiEmb [35]
LP [42]
DeepWalk [25]
ICA [20]
Planetoid [38]
GCN [19]
SIG-VAE

60.1
59.6
45.3
43.2
69.1
64.7
70.3
70.4

6 Conclusion

Combining the advantages of semi-implicit hierarchical variational distribution and VGAE with a
Bernoulli-Poisson link decoder  SIG-VAE is developed to enrich the representation power of the
posterior distribution of node embedding given graphs so that both the graph structural and node
attribute information can be best captured in the latent space. By providing a surrogate evidence
lower bound that is asymptotically exact  the optimization problem for SIG-VAE model inference
is amenable via stochastic gradient descent  without compromising the ﬂexbility of its variational
distribution. Our experiments with different graph datasets have shown the promising capability of
SIG-VAE in a range of graph analysis applications with interpretable latent representations  thanks to
the hierarchical construction that diffuses the distributions of neighborhood nodes in given graphs.

7 Acknowledgments

The presented materials are based upon the work supported by the National Science Foundation under
Grants ENG-1839816  IIS-1848596  CCF-1553281  IIS-1812641 and IIS-1812699. We also thank
Texas A&M High Performance Research Computing and Texas Advanced Computing Center for
providing computational resources to perform experiments in this work.

References
[1] Martin Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S. Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow 
Andrew Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser 
Manjunath Kudlur  Josh Levenberg  Dan Mane  Rajat Monga  Sherry Moore  Derek Murray 
Chris Olah  Mike Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal Talwar 
Paul Tucker  Vincent Vanhoucke  Vijay Vasudevan  Fernanda Viegas  Oriol Vinyals  Pete
Warden  Martin Wattenberg  Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems  2015. URL http://tensorflow.org/.
Software available from tensorﬂow.org.

[2] Amr Ahmed  Nino Shervashidze  Shravan Narayanamurthy  Vanja Josifovski  and Alexan-
der J Smola. Distributed large-scale natural graph factorization. In Proceedings of the 22nd
international conference on World Wide Web  pages 37–48. ACM  2013.

9

[3] Edoardo M Airoldi  David M Blei  Stephen E Fienberg  and Eric P Xing. Mixed membership

stochastic blockmodels. Journal of Machine Learning Research  9(Sep):1981–2014  2008.

[4] Mohammadreza Armandpour  Patrick Ding  Jianhua Huang  and Xia Hu. Robust negative sam-
pling for network embedding. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence 
volume 33  pages 3191–3198. AAAI  2019.

[5] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding
and clustering. In Advances in neural information processing systems  pages 585–591  2002.

[6] Mikhail Belkin  Partha Niyogi  and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of machine learning
research  7(Nov):2399–2434  2006.

[7] Christopher M Bishop and Michael E Tipping. Variational relevance vector machines. In
Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence  pages 46–53.
Morgan Kaufmann Publishers Inc.  2000.

[8] David M Blei  Alp Kucukelbir  and Jon D McAuliffe. Variational inference: A review for

statisticians. Journal of the American Statistical Association  112(518):859–877  2017.

[9] Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: Unsuper-
vised inductive learning via ranking. In International Conference on Learning Representations 
2018.

[10] Haochen Chen  Bryan Perozzi  Yifan Hu  and Steven Skiena. Harp: Hierarchical representation

learning for networks. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[11] Tim R Davidson  Luca Falorsi  Nicola De Cao  Thomas Kipf  and Jakub M Tomczak. Hyper-

spherical variational auto-encoders. arXiv preprint arXiv:1804.00891  2018.

[12] Michael Defferrard  Lionel Martin  Rodrigo Pena  and Nathanael Perraudin. Pygsp: Graph

signal processing in python. URL https://github.com/epfl-lts2/pygsp/.

[13] Michael Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing
Systems  pages 3844–3852  2016.

[14] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.

In
Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and
data mining  pages 855–864. ACM  2016.

[15] Will Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on large

graphs. In Advances in Neural Information Processing Systems  pages 1024–1034  2017.

[16] Michael I Jordan  Zoubin Ghahramani  Tommi S Jaakkola  and Lawrence K Saul. An in-
troduction to variational methods for graphical models. Machine learning  37(2):183–233 
1999.

[17] Durk P Kingma  Tim Salimans  Rafal Jozefowicz  Xi Chen  Ilya Sutskever  and Max Welling.
Improved variational inference with inverse autoregressive ﬂow. In Advances in neural informa-
tion processing systems  pages 4743–4751  2016.

[18] Thomas N Kipf and Max Welling. Variational graph auto-encoders.

arXiv:1611.07308  2016.

arXiv preprint

[19] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. In International Conference on Learning Representations  2017.

[20] Qing Lu and Lise Getoor. Link-based classiﬁcation. In Proceedings of the 20th International

Conference on Machine Learning (ICML-03)  pages 496–503  2003.

[21] Lars Maaloe  Casper Kaae Sonderby  Soren Kaae Sonderby  and Ole Winther. Auxiliary deep
generative models. In International Conference on Machine Learning  pages 1445–1453  2016.

10

[22] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices.

Physical review E  74(3):036104  2006.

[23] George Papamakarios  Theo Pavlakou  and Iain Murray. Masked autoregressive ﬂow for density
estimation. In Advances in Neural Information Processing Systems  pages 2338–2347  2017.

[24] Ferran Parés  Dario Garcia Gasulla  Armand Vilalta  Jonatan Moreno  Eduard Ayguadé  Jesús
Labarta  Ulises Cortés  and Toyotaro Suzumura. Fluid communities: a competitive  scalable
and diverse community detection algorithm. In International Conference on Complex Networks
and their Applications  pages 229–240. Springer  2017.

[25] Bryan Perozzi  Rami Al-Rfou  and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 701–710. ACM  2014.

[26] Rajesh Ranganath  Dustin Tran  and David Blei. Hierarchical variational models. In Interna-

tional Conference on Machine Learning  pages 324–333  2016.

[27] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.

arXiv preprint arXiv:1505.05770  2015.

[28] Prithviraj Sen  Galileo Namata  Mustafa Bilgic  Lise Getoor  Brian Galligher  and Tina Eliassi-

Rad. Collective classiﬁcation in network data. AI magazine  29(3):93  2008.

[29] Neil Spring  Ratul Mahajan  and David Wetherall. Measuring isp topologies with rocketfuel.

ACM SIGCOMM Computer Communication Review  32(4):133–145  2002.

[30] Jian Tang  Meng Qu  Mingzhe Wang  Ming Zhang  Jun Yan  and Qiaozhu Mei. Line: Large-
scale information network embedding. In Proceedings of the 24th International Conference
on World Wide Web  pages 1067–1077. International World Wide Web Conferences Steering
Committee  2015.

[31] Lei Tang and Huan Liu. Leveraging social media networks for classiﬁcation. Data Mining and

Knowledge Discovery  23(3):447–478  2011.

[32] Christian Von Mering  Roland Krause  Berend Snel  Michael Cornell  Stephen G Oliver  Stanley
Fields  and Peer Bork. Comparative assessment of large-scale data sets of protein–protein
interactions. Nature  417(6887):399  2002.

[33] Martin J Wainwright  Michael I Jordan  et al. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[34] Duncan J Watts and Steven H Strogatz. Collective dynamics of small-world networks. nature 

393(6684):440  1998.

[35] Jason Weston  Frederic Ratle  Hossein Mobahi  and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade  pages 639–655. Springer 
2012.

[36] Keyulu Xu  Chengtao Li  Yonglong Tian  Tomohiro Sonobe  Ken-ichi Kawarabayashi  and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
International Conference on Machine Learning  pages 5453–5462  2018.

[37] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural

networks? In International Conference on Learning Representations  2019.

[38] Zhilin Yang  William W Cohen  and Ruslan Salakhutdinov. Revisiting semi-supervised learning

with graph embeddings. arXiv preprint arXiv:1603.08861  2016.

[39] Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In International

Conference on Machine Learning  pages 5660–5669  2018.

[40] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. arXiv preprint

arXiv:1802.09691  2018.

11

[41] Mingyuan Zhou. Inﬁnite edge partition models for overlapping community detection and link

prediction. In Artiﬁcial Intelligence and Statistics  pages 1135–1143  2015.

[42] Xiaojin Zhu  Zoubin Ghahramani  and John D Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03)  pages 912–919  2003.

12

,Arman Hasanzadeh
Ehsan Hajiramezanali
Krishna Narayanan
Nick Duffield
Mingyuan Zhou
Xiaoning Qian