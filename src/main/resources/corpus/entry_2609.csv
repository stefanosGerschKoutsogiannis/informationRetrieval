2014,Incremental Clustering: The Case for Extra Clusters,The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods  which process one element at a time and typically store only a small subset of the data. In this paper  we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model  proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore  we show how the limitations of incremental clustering can be overcome by allowing additional clusters.,Incremental Clustering: The Case for Extra Clusters

Margareta Ackerman
Florida State University

600 W College Ave  Tallahassee  FL 32306

mackerman@fsu.edu

Sanjoy Dasgupta

UC San Diego

9500 Gilman Dr  La Jolla  CA 92093

dasgupta@eng.ucsd.edu

Abstract

The explosion in the amount of data available for analysis often necessitates a
transition from batch to incremental clustering methods  which process one ele-
ment at a time and typically store only a small subset of the data. In this paper 
we initiate the formal analysis of incremental clustering methods focusing on the
types of cluster structure that they are able to detect. We ﬁnd that the incremental
setting is strictly weaker than the batch model  proving that a fundamental class of
cluster structures that can readily be detected in the batch setting is impossible to
identify using any incremental method. Furthermore  we show how the limitations
of incremental clustering can be overcome by allowing additional clusters.

1

Introduction

Clustering is a fundamental form of data analysis that is applied in a wide variety of domains  from
astronomy to zoology. With the radical increase in the amount of data collected in recent years 
the use of clustering has expanded even further  to applications such as personalization and targeted
advertising. Clustering is now a core component of interactive systems that collect information on
millions of users on a daily basis. It is becoming impractical to store all relevant information in
memory at the same time  often necessitating the transition to incremental methods.
Incremental methods receive data elements one at a time and typically use much less space than is
needed to store the complete data set. This presents a particularly interesting challenge for unsu-
pervised learning  which unlike its supervised counterpart  also suffers from an absence of a unique
target truth. Observe that not all data possesses a meaningful clustering  and when an inherent
structure exists  it need not be unique (see Figure 1 for an example). As such  different users may
be interested in very different partitions. Consequently  different clustering methods detect distinct
types of structure  often yielding radically different results on the same data. Until now  differences
in the input-output behaviour of clustering methods have only been studied in the batch setting
[13  14  8  4  3  5  2  20]. In this work  we take a ﬁrst look at the types of cluster structures that can
be discovered by incremental clustering methods.
To qualify the type of cluster structure present in data  a number of notions of clusterability have
been proposed (for a detailed discussion  see [1] and [8]). These notions capture the structure of
the target clustering: the clustering desired by the user for a speciﬁc application. As such  notions of
clusterability facilitate the analysis of clustering methods by making it possible to formally ascertain
whether an algorithm correctly recovers the desired partition.
One elegant notion of clusterability  introduced by Balcan et al. [8]  requires that every element be
closer to data in its own cluster than to other points. For simplicity  we will refer to clusterings that
adhere to this requirement as nice. It was shown by [8] that such clusterings are readily detected
ofﬂine by classical batch algorithms. On the other hand  we prove (Theorem 3.8) that no incre-
mental method can discover these partitions. Thus  batch algorithms are signiﬁcantly stronger than
incremental methods in their ability to detect cluster structure.

1

Figure 1: An example of different cluster structures in the same data. The clustering on the left
ﬁnds inherent structure in the data by identifying well-separated partitions  while the clustering on
the right discovers structure in the data by focusing on the dense region. The correct partitioning
depends on the application at hand.

In an effort to identify types of cluster structure that incremental methods can recover  we turn
to stricter notions of clusterability. A notion used by Epter et al. [10] requires that the minimum
separation between clusters be larger than the maximum cluster diameter. We call such clusterings
perfect  and we present an incremental method that is able to recover them (Theorem 4.3).
Yet  this result alone is unsatisfactory. If  indeed  it were necessary to resort to such strict notions
of clusterability  then incremental methods would have limited utility. Is there some other way to
circumvent the limitations of incremental techniques?
It turns out that incremental methods become a lot more powerful when we slightly alter the cluster-
ing problem: if  instead of asking for exactly the target partition  we are satisﬁed with a reﬁnement 
that is  a partition each of whose clusters is contained within some target cluster. Indeed  in many
applications  it is reasonable to allow additional clusters.
Incremental methods beneﬁt from additional clusters in several ways. First  we exhibit an algorithm
that is able to capture nice k-clusterings if it is allowed to return a reﬁnement with 2k−1 clusters
(Theorem 5.3)  which could be reasonable for small k. We also show that this exponential depen-
dence on k is unavoidable in general (Theorem 5.4). As such  allowing additional clusters enables
incremental techniques to overcome their inability to detect nice partitions.
A similar phenomenon is observed in the analysis of the sequential k-means algorithm  one of
the most popular methods of incremental clustering. We show that it is unable to detect perfect
clusterings (Theorem 4.4)  but that if each cluster contains a signiﬁcant fraction of the data  then it
can recover a reﬁnement of (a slight variant of) nice clusterings (Theorem 5.6).
Lastly  we demonstrate the power of additional clusters by relaxing the niceness condition  requiring
only that clusters have a signiﬁcant core (deﬁned in Section 5.3). Under this milder requirement  we
show that a randomized incremental method is able to discover a reﬁnement of the target partition
(Theorem 5.10).
Due to space limitations  many proofs appear in the supplementary material.

2 Deﬁnitions
We consider a space X equipped with a symmetric distance function d : X × X → R+ satisfying
d(x  x) = 0. An example is X = Rp with d(x  x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2. It is assumed that a clustering
algorithm can invoke d(· ·) on any pair x  x(cid:48) ∈ X .
A clustering (or  partition) of X is a set of clusters C = {C1  . . .   Ck} such that Ci ∩ Cj = ∅ for all
i (cid:54)= j  and X = ∪k
Write x ∼C y if x  y are both in some cluster Cj; and x (cid:54)∼C y otherwise. This is an equivalence
relation.

i=1Ci. A k-clustering is a clustering with k clusters.

2

Deﬁnition 2.1. An incremental clustering algorithm has the following structure:

for n = 1  . . .   N:

See data point xn ∈ X
Select model Mn ∈ M

where N might be ∞  and M is a collection of clusterings of X . We require the algorithm to
have bounded memory  typically a function of the number of clusters. As a result  an incremental
algorithm cannot store all data points.

Notice that the ordering of the points is unspeciﬁed. In our results  we consider two types of or-
dering: arbitrary ordering  which is the standard setting in online learning and allows points to be
ordered by an adversary  and random ordering  which is standard in statistical learning theory. In
exemplar-based clustering  M = X k: each model is a list of k “centers” (t1  . . .   tk) that induce
a clustering of X   where every x ∈ X is assigned to the cluster Ci for which d(x  ti) is smallest
(breaking ties by picking the smallest i). All the clusterings we will consider in this paper will be
speciﬁed in this manner.
We also note that the incremental clustering model is closely related to streaming clustering [6  11] 
the primary difference being that in the latter framework multiple passes of the data are allowed.

2.1 Examples of incremental clustering algorithms

The most well-known incremental clustering algorithm is probably sequential k-means  which is
meant for data in Euclidean space. It is an incremental variant of Lloyd’s algorithm [17  18]:
Algorithm 2.2. Sequential k-means.

Set T = (t1  . . .   tk) to the ﬁrst k data points
Initialize the counts n1  n2  ...  nk to 1
Repeat:

Acquire the next example  x
If ti is the closest center to x:

Increment ni
Replace ti by ti + (1/ni)(x − ti)

This method  and many variants of it  have been studied intensively in the literature on self-
organizing maps [16]. It attempts to ﬁnd centers T that optimize the k-means cost function:

(cid:88)

data x

cost(T ) =

(cid:107)x − t(cid:107)2.

min
t∈T

It is not hard to see that the solution obtained by sequential k-means at any given time can have
cost far from optimal; we will see an even stronger lower bound in Theorem 4.4. Nonetheless  we
will also see that if additional centers are allowed  this algorithm is able to correctly capture some
fundamental types of cluster structure.
Another family of clustering algorithms with incremental variants are agglomerative procedures [13]
like single-linkage [12]. Given n data points in batch mode  these algorithms produce a hierarchical
clustering on all n points. But the hierarchy can be truncated at the intermediate k-clustering  yield-
ing a tree with k leaves. Moreover  there is a natural scheme for updating these leaves incrementally:
Algorithm 2.3. Sequential agglomerative clustering.

Set T to the ﬁrst k data points
Repeat:

Get the next point x and add it to T
Select t  t(cid:48) ∈ T for which dist(t  t(cid:48)) is smallest
Replace t  t(cid:48) by the single center merge(t  t(cid:48))

Here the two functions dist and merge can be varied to optimize different clustering criteria 
and often require storing additional sufﬁcient statistics  such as counts of individual clusters. For
instance  Ward’s method of average linkage [19] is geared towards the k-means cost function. We
will consider the variant obtained by setting dist(t  t(cid:48)) = d(t  t(cid:48)) and merge(t  t(cid:48)) to either t or t(cid:48):

3

Algorithm 2.4. Sequential nearest-neighbour clustering.

Set T to the ﬁrst k data points
Repeat:

Get the next point x and add it to T
Let t  t(cid:48) be the two closest points in T
Replace t  t(cid:48) by either of these two points

The above algorithm was proposed by Ben-David and Reyzin [9]. We will see that it is effective at
picking out a large class of cluster structures.

2.2 The target clustering

Unlike supervised learning tasks  which are typically endowed with a unique correct classiﬁcation 
clustering is ambiguous. One approach to disambiguating clustering is identifying an objective
function such as k-means  and then deﬁning the clustering task as ﬁnding the partition with min-
imum cost. Although there are situations to which this approach is well-suited  many clustering
applications do not inherently lend themselves to any speciﬁc objective function. As such  while
objective functions play an essential role in deriving clustering methods  they do not circumvent the
ambiguous nature of clustering.
The term target clustering denotes the partition that a speciﬁc user is looking for in a data set.
This notion was used by Balcan et al. [8] to study what constraints on cluster structure make them
efﬁciently identiﬁable in a batch setting. In this paper  we consider families of target clusterings that
satisfy different properties  and ask whether incremental algorithms can identify such clusterings.
The target clustering C is deﬁned on a possibly inﬁnite space X   from which the learner receives a
sequence of points. At any time n  the learner has seen n data points and has some clustering that
ideally agrees with C on these points. The methods we consider are exemplar-based: they all specify
a list of points T in X that induce a clustering of X (recall the discussion just before Section 2.1).
We consider two requirements:

• (Strong) T induces the target clustering C.
• (Weaker) T induces a reﬁnement of the target clustering C: that is  each cluster induced by

T is part of some cluster of C.

If the learning algorithm is run on a ﬁnite data set  then we require these conditions to hold once
all points have been seen. In our positive results  we will also consider inﬁnite streams of data  and
show that these conditions hold at every time n  taking the target clustering restricted to the points
seen so far.

3 A basic limitation of incremental clustering

We begin by studying limitations of incremental clustering compared with the batch setting.
One of the most fundamental types of cluster structure is what we shall call nice clusterings for the
sake of brevity. Originally introduced by Balcan et al. [8] under the name “strict separation ” this
notion has since been applied in [2]  [1]  and [7]  to name a few.
Deﬁnition 3.1 (Nice clustering). A clustering C of (X   d) is nice if for all x  y  z ∈ X   d(y  x) <
d(z  x) whenever x ∼C y and x (cid:54)∼C z.
See Figure 2 for an example.
Observation 3.2. If we select one point from every cluster of a nice clustering C  the resulting set
induces C. (Moreover  niceness is the minimal property under which this holds.)
A nice k-clustering is not  in general  unique. For example  consider X = {1  2  4  5} on the real
line under the usual distance metric; then both {{1} {2} {4  5}} and {{1  2} {4} {5}} are nice
3-clusterings of X . Thus we start by considering data with a unique nice k-clustering.

4

Figure 2: A nice clustering may include clusters with very different diameters  as long as the distance
between any two clusters scales as the larger diameter of the two.

Since niceness is a strong requirement  we might expect that it is easy to detect. Indeed  in the batch
setting  a unique nice k-clustering can be recovered by single-linkage [8]. However  we show that
nice partitions cannot be detected in the incremental setting  even if they are unique.
We start by formalizing the ordering of the data. An ordering function O takes a ﬁnite set X and
returns an ordering of the points in this set. An ordered distance space is denoted by (O[X ]  d).
Deﬁnition 3.3. An incremental clustering algorithm A is nice-detecting if  given a positive integer
k and (X   d) that has a unique nice k-clustering C  the procedure A(O[X ]  d  k) outputs C for any
ordering function O.

In this section  we show (Theorem 3.8) that no deterministic memory-bounded incremental method
is nice-detecting  even for points in Euclidean space under the (cid:96)2 metric.
We start with the intuition behind the proof. Fix any incremental clustering algorithm and set the
number of clusters to 3. We will specify a data set D with a unique nice 3-clustering that this
algorithm cannot detect. The data set has two subsets  D1 and D2  that are far away from each
other but are otherwise nearly isomorphic. The target 3-clustering is either: (D1  together with a
2-clustering of D2) or (D2  together with a 2-clustering of D1).
The central piece of the construction is the conﬁguration of D1 (and likewise  D2). The ﬁrst point
presented to the learner is xo. This is followed by a clique of points xi that are equidistant from each
other and have the same  slightly larger  distance to xo. For instance  we could set distances within
the clique d(xi  xj) to 1  and distances d(xi  xo) to 2. Finally there is a point x(cid:48) that is either exactly
like one of the xi’s (same distances)  or differs from them in just one speciﬁc distance d(x(cid:48)  xj)
which is set to 2. In the former case  there is a nice 2-clustering of D1  in which one cluster is
xo and the other cluster is everything else. In the latter case  there is no nice 2-clustering  just the
1-clustering consisting of all of D1.
D2 is like D1  but is rigged so that if D1 has a nice 2-clustering  then D2 does not; and vice versa.
The two possibilities for D1 are almost identical  and it would seem that the only way an algorithm
can distinguish between them is by remembering all the points it has seen. A memory-bounded
incremental learner does not have this luxury. Formalizing this argument requires some care; we
cannot  for instance  assume that the learner is using its memory to store individual points.
In order to specify D1  we start with a larger collection of points that we call an M-conﬁguration 
and that is independent of any algorithm. We then pick two possibilities for D1 (one with a nice
2-clustering and one without) from this collection  based on the speciﬁc learner.
Deﬁnition 3.4. In any metric space (X   d)  for any integer M > 0  deﬁne an M-conﬁguration to
be a collection of 2M + 1 points xo  x1  . . .   xM   x(cid:48)

M ∈ X such that

1  . . .   x(cid:48)

• All interpoint distances are in the range [1  2].
• d(xo  xi)  d(xo  x(cid:48)
i) ∈ (3/2  2] for all i ≥ 1.
• d(xi  xj)  d(x(cid:48)
i  x(cid:48)
j)  d(xi  x(cid:48)
• d(xi  x(cid:48)

i) > d(xo  xi).

j) ∈ [1  3/2] for all i (cid:54)= j ≥ 1.

5

1  . . .   x(cid:48)

M be any M-conﬁguration in (X   d). Pick any index
j} ∪ {xi : i ∈ S} has a

The signiﬁcance of this point conﬁguration is as follows.
Lemma 3.5. Let xo  x1  . . .   xM   x(cid:48)
1 ≤ j ≤ M and any subset S ⊂ [M ] with |S| > 1. Then the set A = {xo  x(cid:48)
nice 2-clustering if and only if j (cid:54)∈ S.
Proof. Suppose A has a nice 2-clustering {C1  C2}  where C1 is the cluster that contains xo.
We ﬁrst show that C1 is a singleton cluster. If C1 also contains some x(cid:96)  then it must contain all
the points {xi : i ∈ S} by niceness since d(x(cid:96)  xi) ≤ 3/2 < d(x(cid:96)  xo). Since |S| > 1  these
j) ≤ 3/2 <
points include some xi with i (cid:54)= j. Whereupon C1 must also contain x(cid:48)
d(xi  xo). But this means C2 is empty.
Likewise  if C1 contains x(cid:48)
There is at least one such xi  and we revert to the previous case.
Therefore C1 = {xo} and  as a result  C2 = {xi : i ∈ S} ∪ {x(cid:48)
only if d(xo  x(cid:48)
only if j (cid:54)∈ S.

j) < d(xo  x(cid:48)
j).
j}. This 2-clustering is nice if and
j  xi) for all i ∈ S  which in turn is true if and

j  then it also contains all {xi : i ∈ S  i (cid:54)= j}  since d(xi  x(cid:48)

j) and d(xo  xi) > d(x(cid:48)

j  since d(xi  x(cid:48)

j) > d(xi  x(cid:48)

By putting together two M-conﬁgurations  we obtain:
Theorem 3.6. Let (X   d) be any metric space that contains two M-conﬁgurations separated by
a distance of at least 4. Then  there is no deterministic incremental algorithm with ≤ M/2 bits
of storage that is guaranteed to recover nice 3-clusterings of data sets drawn from X   even when
limited to instances in which such clusterings are unique.

Proof. Suppose the deterministic incremental learner has a memory capacity of b bits. We will refer
to the memory contents of the learner as its state  σ ∈ {0  1}b.
Call the two M-conﬁgurations xo  x1  . . .   xM   x(cid:48)
feed the following points to the learner:

M and zo  z1  . . .   zM   z(cid:48)

1  . . .   x(cid:48)

1  . . .   z(cid:48)

M . We

Batch 1:
Batch 2:
Batch 3:
Batch 4:

xo and zo
b distinct points from x1  . . .   xM
b distinct points from z1  . . .   zM
Two ﬁnal points x(cid:48)

j1 and z(cid:48)

j2

b

(cid:1) > (M/b)b. If M ≥ 2b  this is > 2b  which

The number of distinct sets of b points in batch 2 is(cid:0)M

The learner’s state after seeing batch 2 can be described by a function f : {x1  . . .   xM}b → {0  1}b.
means that two different sets of points must lead to the same state  call it σ ∈ {0  1}b. Let the indices
of these sets be S1  S2 ⊂ [M ] (so |S1| = |S2| = b)  and pick any j1 ∈ S1 \ S2.
Next  suppose the learner is in state σ and is then given batch 3. We can capture its state at the end
of this batch by a function g : {z1  . . .   zM}b → {0  1}b  and once again there must be distinct sets
T1  T2 ⊂ [M ] that yield the same state σ(cid:48). Pick any j2 ∈ T1 \ T2.
It follows that the sequences of inputs xo  zo  (xi : i ∈ S1)  (zi : i ∈ T2)  x(cid:48)
j2 and xo  zo  (xi :
i ∈ S2)  (zi : i ∈ T1)  x(cid:48)
j2 produce the same ﬁnal state and thus the same answer. But in the ﬁrst
case  by Lemma 3.5  the unique nice 3-clustering keeps the x’s together and splits the z’s  whereas
in the second case  it splits the x’s and keeps the z’s together.

  z(cid:48)

  z(cid:48)

j1

j1

An M-conﬁguration can be realized in Euclidean space:
Lemma 3.7. There is an absolute constant co such that for any dimension p  the Euclidean space
Rp  with L2 norm  contains M-conﬁgurations for all M < 2cop.
The overall conclusions are the following.
Theorem 3.8. There is no memory-bounded deterministic nice-detecting incremental clustering
algorithm that works in arbitrary metric spaces. For data in Rp under the (cid:96)2 metric  there is no
deterministic nice-detecting incremental clustering algorithm using less than 2cop−1 bits of memory.

6

4 A more restricted class of clusterings

The discovery that nice clusterings cannot be detected using any incremental method  even though
they are readily detected in a batch setting  speaks to the substantial limitations of incremental
algorithms. We next ask whether there is a well-behaved subclass of nice clusterings that can be
detected using incremental methods. Following [10  2  5  1]  among others  we consider clusterings
in which the maximum cluster diameter is smaller than the minimum inter-cluster separation.
Deﬁnition 4.1 (Perfect clustering). A clustering C of (X   d) is perfect if d(x  y) < d(w  z) whenever
x ∼C y  w (cid:54)∼C z.
Any perfect clustering is nice. But unlike nice clusterings  perfect clusterings are unique:
Lemma 4.2. For any (X   d) and k  there is at most one perfect k-clustering of (X   d).
Whenever an algorithm can detect perfect clusterings  we call it perfect-detecting. Formally  an
incremental clustering algorithm A is perfect-detecting if  given a positive integer k and (X   d) that
has a perfect k-clustering  A(O[X ]  d  k) outputs that clustering for any ordering function O.
We start with an example of a simple perfect-detecting algorithm.
Theorem 4.3. Sequential nearest-neighbour clustering (Algorithm 2.4) is perfect-detecting.

We next turn to sequential k-means (Algorithm 2.2)  one of the most popular methods for incremen-
tal clustering. Interestingly  it is unable to detect perfect clusterings.
It is not hard to see that a perfect k-clustering is a local optimum of k-means. We will now see an
example in which the perfect k-clustering is the global optimum of the k-means cost function  and
yet sequential k-means fails to detect it.
Theorem 4.4. There is a set of four points in R3 with a perfect 2-clustering that is also the global
optimum of the k-means cost function (for k = 2). However  there is no ordering of these points that
will enable this clustering to be detected by sequential k-means.

5

Incremental clustering with extra clusters

Returning to the basic lower bound of Theorem 3.8  it turns out that a slight shift in perspective
greatly improves the capabilities of incremental methods. Instead of aiming to exactly discover the
target partition  it is sufﬁcient in some applications to merely uncover a reﬁnement of it. Formally  a
clustering C of X is a reﬁnement of clustering C(cid:48) of X   if x ∼C y implies x ∼C(cid:48) y for all x  y ∈ X .
We start by showing that although incremental algorithms cannot detect nice k-clusterings  they can
ﬁnd a reﬁnement of such a clustering if allowed 2k−1 centers. We also show that this is tight.
Next  we explore the utility of additional clusters for sequential k-means. We show that for a random
ordering of the data  and with extra centers  this algorithm can recover (a slight variant of) nice
clusterings. We also show that the random ordering is necessary for such a result.
Finally  we prove that additional clusters extend the utility of incremental methods beyond nice
clusterings. We introduce a weaker constraint on cluster structure  requiring only that each cluster
possess a signiﬁcant “core”  and we present a scheme that works under this weaker requirement.

5.1 An incremental algorithm can ﬁnd nice k-clusterings if allowed 2k centers

Earlier work [8] has shown that that any nice clustering corresponds to a pruning of the tree ob-
tained by single linkage on the points. With this insight  we develop an incremental algorithm that
maintains 2k−1 centers that are guaranteed to induce a reﬁnement of any nice k-clustering.
The following subroutine takes any ﬁnite S ⊂ X and returns at most 2k−1 distinct points:
CANDIDATES(S)

Run single linkage on S to get a tree
Assign each leaf node the corresponding data point
Moving bottom-up  assign each internal node the data point in one of its children
Return all points at distance < k from the root

7

for (cid:96) ≤ k. Then the points returned by

Lemma 5.1. Suppose S has a nice (cid:96)-clustering 
CANDIDATES(S) include at least one representative from each of these clusters.
Here’s an incremental algorithm that uses 2k−1 centers to detect a nice k-clustering.
Algorithm 5.2. Incremental clustering with extra centers.
T0 = ∅
For t = 1  2  . . .:

Receive xt and set Tt = Tt−1 ∪ {xt}
If |Tt| > 2k−1: Tt ← CANDIDATES(Tt)

Theorem 5.3. Suppose there is a nice k-clustering C of X . Then for each t  the set Tt has at most
2k−1 points  including at least one representative from each Ci for which Ci ∩ {x1  . . .   xt} (cid:54)= ∅.
It is not possible in general to use fewer centers.
Theorem 5.4. Pick any incremental clustering algorithm that maintains a list of (cid:96) centers that are
guaranteed to be consistent with a target nice k-clustering. Then (cid:96) ≥ 2k−1.

5.2 Sequential k-means with extra clusters

Theorem 4.4 above shows severe limitations of sequential k-means. The good news is that additional
clusters allow this algorithm to ﬁnd a variant of nice partitionings.
The following condition imposes structure on the convex hull of the partitions in the target clustering.
Deﬁnition 5.5. A clustering C = {C1  . . .   Ck} is convex-nice if for any i (cid:54)= j  any points x  y in
the convex hull of Ci  and any point z in the convex hull of Cj  we have d(y  x) < d(z  x).
Theorem 5.6. Fix a data set (X   d) with a convex-nice clustering C = {C1  . . .   Ck} and let β =
mini |Ci|/|X|. If the points are ordered uniformly at random  then for any (cid:96) ≥ k  sequential (cid:96)-means
will return a reﬁnement of C with probability at least 1 − ke−β(cid:96).
The probability of failure is small when the reﬁnement contains (cid:96) = Ω((log k)/β) centers. We can
also show that this positive result no longer holds when data is adversarially ordered.
Theorem 5.7. Pick any k ≥ 3. Consider any data set X in R (under the usual metric) that has
a convex-nice k-clustering C = {C1  . . .   Ck}. Then there exists an ordering of X under which
sequential (cid:96)-means with (cid:96) ≤ mini |Ci| centers fails to return a reﬁnement of C.

5.3 A broader class of clusterings

We conclude by considering a substantial generalization of niceness that can be detected by incre-
mental methods when extra centers are allowed.
Deﬁnition 5.8 (Core). For any clustering C = {C1  . . .   Ck} of (X   d)  the core of cluster Ci is the
maximal subset C o

i ⊂ Ci such that d(x  z) < d(x  y) for all x ∈ Ci  z ∈ C o

i   and y (cid:54)∈ Ci.

In a nice clustering  the core of any cluster is the entire cluster. We now require only that each core
contain a signiﬁcant fraction of points  and we show that the following simple sampling routine will
ﬁnd a reﬁnement of the target clustering  even if the points are ordered adversarially.
Algorithm 5.9. Algorithm subsample.

Set T to the ﬁrst (cid:96) elements
For t = (cid:96) + 1  (cid:96) + 2  . . .:

Get a new point xt
With probability (cid:96)/t:

Remove an element from T uniformly at random and add xt to T

It is well-known (see  for instance  [15]) that at any time t  the set T consists of (cid:96) elements chosen
at random without replacement from {x1  . . .   xt}.
Theorem 5.10. Consider any clustering C = {C1  . . .   Ck} of (X   d)  with core {C o
k}.
1   . . .   C o
Let β = mini |C o
i |/|X|. Fix any (cid:96) ≥ k. Then  given any ordering of X   Algorithm 5.9 detects a
reﬁnement of C with probability 1 − ke−β(cid:96).

8

References
[1] M. Ackerman and S. Ben-David. Clusterability: A theoretical study. Proceedings of AISTATS-

09  JMLR: W&CP  5(1-8):53  2009.

[2] M. Ackerman  S. Ben-David  S. Branzei  and D. Loker. Weighted clustering. Proc. 26th AAAI

Conference on Artiﬁcial Intelligence  2012.

[3] M. Ackerman  S. Ben-David  and D. Loker. Characterization of linkage-based clustering.

COLT  2010.

[4] M. Ackerman  S. Ben-David  and D. Loker. Towards property-based classiﬁcation of clustering

paradigms. NIPS  2010.

[5] M. Ackerman  S. Ben-David  D. Loker  and S. Sabato. Clustering oligarchies. Proceedings of

AISTATS-09  JMLR: W&CP  31(6674)  2013.

[6] Charu C Aggarwal. A survey of stream clustering algorithms.  2013.
[7] M.-F. Balcan and P. Gupta. Robust hierarchical clustering. In COLT  pages 282–294  2010.
[8] M.F. Balcan  A. Blum  and S. Vempala. A discriminative framework for clustering via simi-
larity functions. In Proceedings of the 40th annual ACM symposium on Theory of Computing 
pages 671–680. ACM  2008.

[9] Shalev Ben-David and Lev Reyzin. Data stability in clustering: A closer look. ALT  2012.
[10] S. Epter  M. Krishnamoorthy  and M. Zaki. Clusterability detection and initial seed selection
In The International Conference on Knowledge Discovery in Databases 

in large datasets.
volume 7  1999.

[11] Sudipto Guha  Nina Mishra  Rajeev Motwani  and Liadan O’Callaghan. Clustering data
streams. In Foundations of computer science  2000. proceedings. 41st annual symposium on 
pages 359–366. IEEE  2000.

[12] J.A. Hartigan. Consistency of single linkage for high-density clusters. Journal of the American

Statistical Association  76(374):388–394  1981.

[13] N. Jardine and R. Sibson. Mathematical taxonomy. London  1971.
[14] J. Kleinberg. An impossibility theorem for clustering. Proceedings of International Confer-

ences on Advances in Neural Information Processing Systems  pages 463–470  2003.

[15] D.E. Knuth. The Art of Computer Programming: Seminumerical Algorithms  volume 2. 1981.
[16] T. Kohonen. Self-organizing maps. Springer  2001.
[17] S.P. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory 

28(2):129–137  1982.

[18] J.B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations.
In Proceedings of Fifth Berkeley Symposium on Mathematical Statistics and Probability  vol-
ume 1  pages 281–297. University of California Press  1967.

[19] J.H. Ward. Hierarchical grouping to optimize an objective function. Journal of the American

Statistical Association  58:236–244  1963.

[20] R.B. Zadeh and S. Ben-David. A uniqueness theorem for clustering. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  pages 639–646. AUAI Press 
2009.

9

,Margareta Ackerman
Sanjoy Dasgupta