2013,Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms,We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty  we show that a suitably chosen set of latent variables enables to derive an Expectation-Maximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.,Probabilistic Low-Rank Matrix Completion with

Adaptive Spectral Regularization Algorithms

Adrien Todeschini

INRIA - IMB - Univ. Bordeaux

33405 Talence  France

Adrien.Todeschini@inria.fr

Franc¸ois Caron

Univ. Oxford  Dept. of Statistics

Oxford  OX1 3TG  UK

Caron@stats.ox.ac.uk

Marie Chavent

Univ. Bordeaux - IMB - INRIA

33000 Bordeaux  France

Marie.Chavent@u-bordeaux2.fr

Abstract

We propose a novel class of algorithms for low rank matrix completion. Our ap-
proach builds on novel penalty functions on the singular values of the low rank
matrix. By exploiting a mixture model representation of this penalty  we show
that a suitably chosen set of latent variables enables to derive an Expectation-
Maximization algorithm to obtain a Maximum A Posteriori estimate of the com-
pleted low rank matrix. The resulting algorithm is an iterative soft-thresholded
algorithm which iteratively adapts the shrinkage coefﬁcients associated to the sin-
gular values. The algorithm is simple to implement and can scale to large matrices.
We provide numerical comparisons between our approach and recent alternatives
showing the interest of the proposed approach for low rank matrix completion.

1

Introduction

Matrix completion has attracted a lot of attention over the past few years. The objective is to “com-
plete” a matrix of potentially large dimension based on a small (and potentially noisy) subset of its
entries [1  2  3]. One popular application is to build automatic recommender systems  where the
rows correspond to users  the columns to items and entries may be ratings or binary (like/dislike).
The objective is then to predict user preferences from a subset of the entries.
In many cases  it is reasonable to assume that the unknown m × n matrix Z can be approximated
by a matrix of low rank Z (cid:39) ABT where A and B are respectively of size m × k and n × k  with
k (cid:28) min(m  n). In the recommender system application  the low rank assumption is sensible as it
is commonly believed that only a few factors contribute to user’s preferences. The low rank structure
thus implies some sort of collaboration between the different users/items [4].
We typically observe a noisy version Xij of some entries (i  j) ∈ Ω where Ω ⊂ {1  . . .   m} ×
{1  . . .   n}. For (i  j) ∈ Ω

Xij = Zij + εij  εij

(1)
where σ2 > 0 and N (µ  σ2) is the normal distribution of mean µ and variance σ2. Low rank matrix
completion can be adressed by solving the following optimization problem
(Xij − Zij)2 + λ rank(Z)

(cid:88)

(2)

iid∼ N (0  σ2)

minimize

Z

1
2σ2

(i j)∈Ω

1

(cid:88)

1
2σ2

where λ > 0 is some regularization parameter. For general subsets Ω  the optimization problem (2)
is computationally hard and many authors have advocated the use of a convex relaxation of (2)
[5  6  4]  yielding the following convex optimization problem

(Xij − Zij)2 + λ(cid:107)Z(cid:107)∗

Z

(i j)∈Ω

minimize

(3)
where (cid:107)Z(cid:107)∗ is the nuclear norm of Z  or the sum of the singular values of Z. [4] proposed an
iterative algorithm  called Soft-Impute  for solving the nuclear norm regularized minimization (3).
In this paper  we show that the solution to the objective function (3) can be interpreted as a Maxi-
mum A Posteriori (MAP) estimate when assuming that the singular values of Z are independently
and identically drawn (iid) from an exponential distribution with rate λ. Using this Bayesian in-
terpretation  we propose alternative concave penalties to the nuclear norm  obtained by consider-
ing that the singular values are iid from a mixture of exponential distributions. We show that this
class of penalties bridges the gap between the nuclear norm and the rank penalty  and that a simple
Expectation-Maximization (EM) algorithm can be derived to obtain MAP estimates. The resulting
algorithm iteratively adapts the shrinkage coefﬁcients associated to the singular values. It can be
seen as the equivalent for matrices of reweighted (cid:96)1 algorithms [6] for multivariate linear regression.
Interestingly  we show that the Soft-Impute algorithm of [4] is obtained as a particular case. We also
discuss the extension of our algorithms to binary matrices  building on the same seed of ideas  in the
supplementary material. Finally  we provide some empirical evidence of the interest of the proposed
approach on simulated and real data.

2 Complete matrix X
Consider ﬁrst that we observe the complete matrix X of size m × n. Let r = min(m  n). We
consider the following convex optimization problem

minimize

F + λ(cid:107)Z(cid:107)∗

(4)
where (cid:107)·(cid:107)F is the Frobenius norm. The solution to Eq. (4) in the complete case is a soft-thresholded
singular value decomposition (SVD) of X [7  4]  i.e.
where Sλ(X) = (cid:101)U(cid:101)Dλ(cid:101)V T with (cid:101)Dλ = diag(((cid:101)d1 − λ)+  . . .   ((cid:101)dr − λ)+) and t+ = max(t  0).
X = (cid:101)U(cid:101)D(cid:101)V T is the singular value decomposition of X with (cid:101)D = diag((cid:101)d1  . . .  (cid:101)dr).
The solution (cid:98)Z to the optimization problem (4) can be interpreted as the Maximum A Posteriori

2σ2 (cid:107)X − Z(cid:107)2
(cid:98)Z = Sλσ2(X)

1

Z

estimate under the likelihood (1) and prior

p(Z) ∝ exp (−λ(cid:107)Z(cid:107)∗)

p(Z) = p(U )p(V )p(D)

r(cid:89)

Assuming Z = U DV T   with D = diag(d1  d2  . . .   dr) this can be further decomposed as

where we assume a uniform Haar prior distribution on the unitary matrices U and V   and exponential
priors on the singular values di  hence

Exp (di; λ)

p(d1  . . .   dr) =

(5)
where Exp(x; λ) = λ exp(−λx) is the probability density function (pdf) of the exponential distri-
bution of parameter λ evaluated at x. The exponential distribution has a mode at 0  hence favoring
sparse solutions.
We propose here alternative penalty/prior distributions  that bridge the gap between the rank and the
nuclear norm penalties. Our penalties are based on hierarchical Bayes constructions and the related
optimization problems to obtain MAP estimates can be solved by using an EM algorithm.

i=1

2.1 Hierarchical adaptive spectral penalty

We consider the following hierarchical prior for the low rank matrix Z. We still assume that Z =
U DV T   where the unitary matrices U and V are assigned uniform priors and D = diag(d1  . . .   dr).
We now assume that each singular value di has its own regularization parameter γi.

2

Figure 2: Thresholding rules on the singular

values (cid:101)di of X for the soft thresholding rule

(λ = 1)  and hierarchical adaptive soft thresh-
olding algorithm with a = b = β.

Figure 1: Marginal distribution p(di) with
a = b = β for different values of the param-
eter β. The distribution becomes more con-
centrated around zero with heavier tails as β
decreases. The case β → ∞ corresponds to
an exponential distribution with unit rate.

p(d1  . . .   dr|γ1  . . . γr) =

r(cid:89)

i=1

Exp(di; γi)

r(cid:89)

i=1

p(di|γi) =

r(cid:89)

r(cid:89)

p(γ1  . . .   γr) =

p(γi) =

Gamma(γi; a  b)

i=1

i=1

We further assume that the regularization parameters are themselves iid from a gamma distribution

(cid:90) ∞

0

where Gamma(γi; a  b) is the pdf of the gamma distribution of parameters a > 0 and b > 0 evalu-
ated at γi. The marginal distribution over di is thus a continuous mixture of exponential distributions

p(di) =

Exp(di; γi) Gamma(γi; a  b)dγi =

aba

(di + b)a+1

(6)

It is a Pareto distribution which has heavier tails than the exponential distribution. Figure 1 shows
the marginal distribution p(di) for a = b = β. The lower β  the heavier the tails of the distribution.
When β → ∞  one recovers the exponential distribution of unit rate parameter. Let

pen(Z) = − log p(Z) = − r(cid:88)

log(p(di)) = C1 +

(a + 1) log(b + di)

(7)

r(cid:88)

i=1

i=1

be the penalty induced by the prior p(Z). We call the penalty (7) the Hierarchical Adaptive Spectral
Penalty (HASP). On Figure 3 (top) are represented the balls of constant penalties for a symmetric
2 × 2 matrix  for the HASP  nuclear norm and rank penalties. When the matrix is assumed to
be diagonal  one recovers respectively the lasso  hierarchical adaptive lasso (HAL) [6  8] and (cid:96)0
penalties  as shown on Figure 3 (bottom).
The penalty (7) admits as special cases the nuclear norm penalty λ||Z||∗ when a = λb and b → ∞.
Another closely related penalty is the log-det heuristic [5  9] penalty  deﬁned for a square matrice Z
by log det(Z + δI) where δ is some small regularization constant. Both penalties agree on square
matrices when a = b = 0 and δ = 0.

2.2 EM algorithm for MAP estimation

Using the exponential mixture representation (6)  we now show how to derive an EM algorithm [10]
to obtain a MAP estimate

[log p(X|Z) + log p(Z)]

i.e. to minimize

L(Z) =

(a + 1) log(b + di)

(8)

(cid:98)Z = arg max
2σ2 (cid:107)X − Z(cid:107)2

1

Z

r(cid:88)

F +

i=1

3

01234500.10.20.30.40.50.60.70.80.91dip(di) β=∞β=2β=0.1(a) Nuclear norm

(b) HASP (β = 1)

(c) HASP (β = 0.1)

(d) Rank penalty

(e) (cid:96)1 norm

(f) HAL (β = 1)

(g) HAL (β = 0.1)

(h) (cid:96)0 norm

Figure 3: Top: Manifold of constant penalty  for a symmetric 2 × 2 matrix Z = [x  y; y  z] for
(a) the nuclear norm  (b-c) hierarchical adaptive spectral penalty with a = b = β (b) β = 1 and
(c) β = 0.1  and (d) the rank penalty. Bottom: contour of constant penalty for a diagonal matrix
[x  0; 0  z]  where one recovers the classical (e) lasso  (f-g) hierarchical lasso and (h) (cid:96)0 penalties.

We use the parameters γ = (γ1  . . .   γr) as latent variables in the EM algorithm. The E step is
obtained by

Q(Z  Z∗) = E [log(p(X  Z  γ))|Z∗  X] = C2 − 1

F − r(cid:88)

i=1

E[γi|d∗

i ]di

Hence at each iteration of the EM algorithm  the M step consists in solving the optimization problem

2σ2 (cid:107)X − Z(cid:107)2
r(cid:88)

ωidi

i=1

(9)

1

2σ2 (cid:107)X − Z(cid:107)2

F +

Z

minimize
[− log p(d∗

where ωi = E[γi|d∗
(9) is an adaptive nuclear norm regularized optimization problem  with weights ωi. Without loss of
generality  assume that d∗

i )] = a+1
b+d∗

i ] = ∂
∂d∗

r. It implies that

1 ≥ d∗

2 ≥ . . . ≥ d∗

.

i

i

(10)
The above weights will therefore penalize less heavily higher singular values  hence reducing bias.
As shown by [11  12]  a global optimal solution to Eq. (9) under the order constraint (10) is given
by a weighted soft-thresholded SVD

(cid:98)Z = Sσ2ω(X)
where Sω(X) = (cid:101)U(cid:101)Dω(cid:101)V T with (cid:101)Dω = diag(((cid:101)d1 − ω1)+  . . .   ((cid:101)dr − ωr)+). X = (cid:101)U(cid:101)D(cid:101)V T is the SVD
of X with (cid:101)D = diag((cid:101)d1  . . .  (cid:101)dr) and (cid:101)d1 ≥ (cid:101)d2 . . . ≥ (cid:101)dr.

(11)

0 ≤ ω1 ≤ ω2 ≤ . . . ≤ ωr

Algorithm 1 summarizes the Hierarchical Adaptive Soft Thresholded (HAST) procedure to converge
to a local minimum of the objective (8). This algorithm admits the soft-thresholded SVD operator
as a special case when a = bλ and b = β → ∞. Figure 2 shows the thresholding rule applied to
the singular values of X for the HAST algorithm (a = b = β  with β = 2 and β = 0.1) and the
soft-thresholded SVD for λ = 1. The bias term  which is equal to λ for the nuclear norm  goes to

zero as (cid:101)di goes to inﬁnity.

Setting of the hyperparameters and initialization of the EM algorithm In the experiments  we
have set b = β and a = λβ where λ and β are tuning parameters that can be chosen by cross-
validation. As λ is the mean value of the regularization parameter γi  we initialize the algorithm
with the soft thresholded SVD with parameter σ2λ. It is possible to estimate the hyperparameter σ
within the EM algorithm as described in the supplementary material. In our experiments  we have
found the results not very sensitive to the setting of σ  and set it to 1.

4

Algorithm 1 Hierarchical Adaptive Soft Thresholded (HAST) algorithm for low rank estimation of
complete matrices
Initialize Z (0). At iteration t ≥ 1

• For i = 1  . . .   r  compute the weights ω(t)
• Set Z (t) = Sσ2ω(t) (X)
• If L(Z(t−1))−L(Z(t))

< ε then return (cid:98)Z = Z (t)

L(Z(t−1))

i = a+1

b+d(t−1)

i

3 Matrix completion

We now show how the EM algorithm derived in the previous section can be adapted to the case
where only a subset of the entries is observed. It relies on imputing missing values  similarly to the
EM algorithm for SVD with missing data  see e.g. [10  13].
Consider that only a subset Ω ⊂ {1  . . .   m}×{1  . . .   n} of the entries of the matrix X is observed.
Similarly to [7]  we introduce the operator PΩ(X) and its complementary P ⊥

Ω (X)

PΩ(X)(i  j) =

if (i  j) ∈ Ω
otherwise

and

P ⊥
Ω (X)(i  j) =

if (i  j) ∈ Ω
otherwise

(cid:26) Xij

0

(cid:26) 0

Xij

r(cid:88)

Assuming the same prior (6)  the MAP estimate is obtained by minimizing

1

L(Z) =

F + (a + 1)

We will now derive the EM algorithm  by using latent variables γ and P ⊥
by (details in supplementary material)

2σ2 (cid:107)PΩ(X) − PΩ(Z)(cid:107)2
Q(Z  Z∗) = E(cid:2)log(p(PΩ(X)  P ⊥
(cid:110)(cid:13)(cid:13)PΩ(X) + P ⊥
Ω (Z∗) − Z(cid:13)(cid:13)2
r(cid:88)

Ω (X)  Z  γ))|Z∗  PΩ(X)(cid:3)
(cid:111) − r(cid:88)

Hence at each iteration of the algorithm  one needs to minimize

= C4 − 1
2σ2

i=1

i=1

F

log(b + di)

(12)

Ω (X). The E step is given

E[γi|d∗

i ]di

1

2σ2 (cid:107)X∗ − Z(cid:107)2

(13)
where ωi = E[γi|d∗
Ω (Z∗) is the observed matrix  completed with entries
in Z∗. We now have a complete matrix problem. As mentioned in the previous section  the mini-
mum of (13) is obtained with a weighted soft-thresholded SVD. Algorithm 2 provides the resulting
iterative procedure for matrix completion with the hierarchical adaptive spectral penalty.

i ] and X∗ = PΩ(X) + P ⊥

F +

ωidi

i=1

Algorithm 2 Hierarchical Adaptive Soft Impute (HASI) algorithm for matrix completion
Initialize Z (0). At iteration t ≥ 1

• For i = 1  . . .   r  compute the weights ω(t)
• Set Z (t) = Sσ2ω(t)
• If L(Z(t−1))−L(Z(t))

(cid:0)PΩ(X) + P ⊥
Ω (Z (t−1))(cid:1)
< ε then return (cid:98)Z = Z (t)

L(Z(t−1))

i = a+1

b+d(t−1)

i

Related algorithms Algorithm 2 admits the Soft-Impute algorithm of [4] as a special case when
a = λb and b = β → ∞. In this case  one obtains at each iteration ω(t)
i = λ for all i. On the contrary 
when β < ∞  our algorithm adaptively updates the weights so that to penalize less heavily higher
singular values. Some authors have proposed related one-step adaptive spectral penalty algorithms
[14  11  12]. However  in these procedures  the weights have to be chosen by some procedure
whereas in our case they are iteratively adapted.

Initialization The objective function (12) is in general not convex and different initializations may
lead to different modes. As in the complete case  we suggest to set a = λb and b = β and to initialize
the algorithm with the Soft-Impute algorithm with regularization parameter σ2λ.

5

(cid:0)PΩ(X) + P ⊥

Ω (Z (t−1))(cid:1) which requires calculating a low rank truncated SVD.

Scaling Similarly to the Soft-Impute algorithm  the computationally demanding part of Algo-
rithm 2 is Sσ2ω(t)
For large matrices  one can resort to the PROPACK algorithm [15  16] as described in [4]. This
sophisticated linear algebra algorithm can efﬁciently compute the truncated SVD of the “sparse +
low rank” matrix

PΩ(X) + P ⊥

(cid:125)
Ω (Z (t−1)) = PΩ(X) − PΩ(Z (t−1))

(cid:123)(cid:122)

(cid:124)

sparse

(cid:124) (cid:123)(cid:122) (cid:125)

+ Z (t−1)
low rank

and can thus handle large matrices  as shown in [4].

4 Experiments

4.1 Simulated data

(cid:113) var(Z)

We ﬁrst evaluate the performance of the proposed approach on simulated data. Our simulation
setting is similar to that of [4]. We generate Gaussian matrices A and B respectively of size m × q
and n × q  q ≤ r so that the matrix Z = ABT is of low rank q. A Gaussian noise of variance
σ2 is then added to the entries of Z to obtain the matrix X. The signal to noise ratio is deﬁned as
. We set m = n = 100 and σ = 1. We run all the algorithms with a precision
SNR =
 = 10−9 and a maximum number of tmax = 200 iterations (initialization included for HASI). We

compute err  the relative error between the estimated matrix (cid:98)Z and the true matrix Z in the complete

σ2

case  and errΩ⊥ in the incomplete case  where

||(cid:98)Z − Z||2

F

||Z||2

F

err =

and

errΩ⊥ =

Ω ((cid:98)Z) − P ⊥
||(cid:98)P ⊥
Ω (Z)||2

||P ⊥

F

Ω (Z)||2

F

For the HASP penalty  we set a = λβ and b = β. We compute the solutions over a grid of 50 values
of the regularization parameter λ linearly spaced from λ0 to 0  where λ0 = ||PΩ(X)||2 is the largest
singular value of the input matrix X  padded with zeros. This is done for three different values
β = 1  10  100. We use the same grid to obtain the regularization path for the other algorithms.

Complete case We ﬁrst consider that the observed matrix is complete  with SNR = 1 and q = 10.
The HAST algorithm 1 is compared to the soft thresholded (ST) and hard thresholded (HT) SVD.
Results are reported in Figure 4(a). The HASP penalty provides a bridge/tradeoff between the
nuclear norm and the rank penalty. For example  value of β = 10 show a minimum at the true rank
q = 10 as HT  but with a lower error when the rank is overestimated.

(a) SNR=1; Complete; rank=10

(b) SNR=1; 50% missing; rank=5 (c) SNR=10; 80% missing; rank=5

Figure 4: Test error w.r.t.
the rank obtained by varying the value of the regularization parameter
λ. Results on simulated data are given for (a) complete matrix with SNR=1 (b) 50% missing and
SNR=1 and (c) 80% missing and SNR=10.

Incomplete case Then we consider the matrix completion problem  and remove uniformly a given
percentage of the entries in X. We compare the HASI algorithm to the Soft-Impute  Soft-Impute+
and Hard-Impute algorithms of [4] and to the MMMF algorithm of [17]. Results  averaged over
50 replications  are reported in Figures 4(b-c) for a true rank q = 5  (b) 50% of missing data and

6

01020304050607080901000.10.20.30.40.50.60.70.80.91RankRelative error STHTHASTβ=100HASTβ=10HASTβ=101020304050600.30.40.50.60.70.80.91RankTest error MMMFSoftImpSoftImp+HardImpHASIβ=100HASIβ=10HASIβ=105101520253000.10.20.30.40.50.60.70.80.91RankTest error MMMFSoftImpSoftImp+HardImpHASIβ=100HASIβ=10HASIβ=1(a) SNR=1; 50% miss.

(b) SNR=1; 50% miss. (c) SNR=10; 80% miss. (d) SNR=10; 80% miss.

Figure 5: Boxplots of the test error and ranks obtained over 50 replications on simulated data.

Table 1: Results on the Jester and MovieLens datasets

Jester 1

24983 × 100
27.5% miss.

Jester 2

23500 × 100
27.3% miss.

Jester 3

24938 × 100
75.3% miss.

MovieLens 100k MovieLens 1M
6040 × 3952
95.8% miss.

Method
MMMF
Soft Imp
Soft Imp+
Hard Imp
HASI

NMAE Rank NMAE Rank NMAE Rank
0.161
0.161
0.169
0.158
0.153

0.162
0.162
0.171
0.159
0.153

0.183
0.184
0.184
0.181
0.174

96
100
11
6
100

95
100
14
7
100

58
78
33
4
30

943 × 1682
93.7% miss.
NMAE Rank
0.195
0.197
0.197
0.190
0.187

50
156
108
7
35

NMAE Rank
0.169
0.176
0.189
0.175
0.172

30
30
30
8
27

SNR = 1 and (c) 80% of missing data and SNR = 10. Similar behavior is observed  with the HASI
algorithm attaining a minimum at the true rank q = 5. We then conduct the same experiments 
but remove 20% of the observed entries as a validation set to estimate the regularization parameters
(λ  β) for HASI  and λ for the other methods. We estimate Z on the whole observed matrix  and use
the unobserved entries as a test set. Results on the test error and estimated ranks over 50 replications
are reported in Figure 5. For 50% missing data  HASI is shown to outperform the other methods.
For 80% missing data  HASI and Hard Impute provide the best performances. In both cases  it is
able to recover very accurately the true rank of the matrix.

4.2 Collaborative ﬁltering examples

We now compare the different methods on several benchmark datasets. We ﬁrst consider the Jester
datasets [18]. The three datasets1 contain one hundred jokes  with user ratings between -10 and +10.
We randomly select two ratings per user as a test set  and two other ratings per user as a validation
set to select the parameters λ and β. The results are computed over four values β = 1000  100  10  1.
We compare the results of the different methods with the Normalized Mean Absolute Error (NMAE)

(cid:80)

|Xij − (cid:98)Zij|

NMAE =

1

card(Ωtest)

(i j)∈Ωtest

max(X) − min(X)

where Ωtest is the test set. The mean number of iterations for Soft-Impute  Hard-Impute and HASI
(initialization included) algorithms are respectively 9  76 and 76. Computations for the HASI algo-
rithm take approximately 5 hours on a standard computer. The results  averaged over 10 replications
(with almost no variability observed)  are presented in Table 1. The HASI algorithm provides very
good performance on the different Jester datasets  with lower NMAE than the other methods.
Figure 6 shows the NMAE in function of the rank. Low values of β exhibit a bimodal behavior
with two modes at low rank and full rank. High value β = 1000 is unimodal and outperforms
Soft-Impute at any particular rank.

1Jester datasets can be downloaded from the url http://goldberg.berkeley.edu/jester-data/

7

0.20.30.40.50.60.70.80.91HASIHardImpSoftImp+SoftImpMMMFTest error102030405060708090100Rank00.050.10.150.20.25Test error102030405060708090100Rank(a) Jester 1

(b) Jester 3

Figure 6: NMAE on the test set of the (a) Jester 1 and (b) Jester 3 datasets w.r.t. the rank obtained
by varying the value of the regularization parameter λ. The curves obtained on the Jester 2 dataset
are hardly distinguishable from (a) and hence are not displayed due to space limitation.

Second  we conducted the same comparison on two MovieLens datasets2  which contain ratings of
movies by users. We randomly select 20% of the entries as a test set  and the remaining entries
are split between a training set (80%) and a validation set (20%). For all the methods  we stop
the regularization path as soon as the estimated rank exceeds rmax = 100. This is a practical
consideration: given that the computations for high ranks demand more time and memory  we are
interested in restricting ourselves to low rank solutions. Table 1 presents the results  averaged over 5
replications. For the MovieLens 100k dataset  HASI provides better NMAE than the other methods
with a low rank solution. For the larger MovieLens 1M dataset  the precision  maximum number
of iterations and maximum rank are decreased to  = 10−6  tmax = 100 and rmax = 30. On
this dataset  MMMF provides the best NMAE at maximum rank. HASI provides the second best
performances with a slightly lower rank.

5 Conclusion

The proposed class of methods has shown to provide good results compared to several alternative
low rank matrix completion methods. It provides a bridge between nuclear norm and rank regular-
ization algorithms. Although the related optimization problem is not convex  experiments show that
initializing the algorithm with the Soft-Impute algorithm of [4] provides very satisfactory results.
In this paper  we have focused on a gamma mixture of exponentials  as it leads to a simple and
interpretable expression for the weights. It is however possible to generalize the results presented
here by using a three parameter generalized inverse Gaussian prior distribution (see e.g. [19]) for
the regularization parameters γi  thus offering an additional degree of freedom. Derivations of
the weights are provided in the supplementary material. Additionally  it is possible to derive an
EM algorithm for low rank matrix completion for binary matrices. Details are also provided in
supplementary material.
While we focus on point estimation in this paper  it would be of interest to investigate a fully
Bayesian approach and derive a Gibbs sampler or variational algorithm to approximate the posterior
distribution  and compare to other full Bayesian approaches to matrix completion [20  21].

Acknowledgments

F.C. acknowledges the support of the European Commission under the Marie Curie Intra-European
Fellowship Programme. The contents reﬂect only the authors views and not the views of the Euro-
pean Commission.

2MovieLens datasets can be downloaded from the url http://www.grouplens.org/node/73.

8

01020304050607080901000.160.180.20.220.240.260.280.30.32RankTest error MMMFSoftImpSoftImp+HardImpHASIβ=1000HASIβ=100HASIβ=10HASIβ=101020304050607080900.150.20.250.3RankTest error MMMFSoftImpSoftImp+HardImpHASIβ=1000HASIβ=100HASIβ=10HASIβ=1References
[1] N. Srebro  J.D.M. Rennie  and T. Jaakkola. Maximum-Margin Matrix Factorization. In Ad-
vances in neural information processing systems  volume 17  pages 1329–1336. MIT Press 
2005.

[2] E.J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics  9(6):717–772  2009.

[3] E.J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–

936  2010.

[4] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning
large incomplete matrices. The Journal of Machine Learning Research  11:2287–2322  2010.
[5] M. Fazel. Matrix rank minimization with applications. PhD thesis  Stanford University  2002.
[6] E.J. Cand`es  M.B. Wakin  and S.P. Boyd. Enhancing sparsity by reweighted l1 minimization.

Journal of Fourier Analysis and Applications  14(5):877–905  2008.

[7] J.F. Cai  E.J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix comple-

tion. SIAM Journal on Optimization  20(4):1956–1982  2010.

[8] Anthony Lee  Francois Caron  Arnaud Doucet  and Chris Holmes. A hierarchical Bayesian

framework for constructing sparsity-inducing priors. arXiv preprint arXiv:1009.1914  2010.

[9] M. Fazel  H. Hindi  and S.P. Boyd. Log-det heuristic for matrix rank minimization with ap-
plications to hankel and euclidean distance matrices. In American Control Conference  2003.
Proceedings of the 2003  volume 3  pages 2156–2162. IEEE  2003.

[10] A.P. Dempster  N.M. Laird  and D.B. Rubin. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the Royal Statistical Society. Series B  pages 1–38  1977.

[11] S. Ga¨ıffas and G. Lecu´e. Weighted algorithms for compressed sensing and matrix completion.

arXiv preprint arXiv:1107.1638  2011.

[12] Kun Chen  Hongbo Dong  and Kung-Sik Chan. Reduced rank regression via adaptive nuclear

norm penalization. Biometrika  100(4):901–920  2013.

[13] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In NIPS  volume 20  page 720 

2003.

[14] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research 

9:1019–1048  2008.

[15] R. M. Larsen. Lanczos bidiagonalization with partial reorthogonalization. Technical report 

DAIMI PB-357  1998.

[16] R. M. Larsen. Propack-software for large and sparse svd calculations. Available online. URL

http://sun. stanford. edu/rmunk/PROPACK  2004.

[17] J. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative pre-
In Proceedings of the 22nd international conference on Machine learning  pages

diction.
713–719. ACM  2005.

[18] K. Goldberg  T. Roeder  D. Gupta  and C. Perkins. Eigentaste: A constant time collaborative

ﬁltering algorithm. Information Retrieval  4(2):133–151  2001.

[19] Z. Zhang  S. Wang  D. Liu  and M. I. Jordan. EP-GIG priors and applications in Bayesian

sparse learning. The Journal of Machine Learning Research  98888:2031–2061  2012.

[20] M. Seeger and G. Bouchard. Fast variational Bayesian inference for non-conjugate matrix

factorization models. In Proc. of AISTATS  2012.

[21] S. Nakajima  M. Sugiyama  S. D. Babacan  and R. Tomioka. Global analytic solution of fully-
observed variational Bayesian matrix factorization. Journal of Machine Learning Research 
14:1–37  2013.

9

,Adrien Todeschini
François Caron
Marie Chavent
Yaoqing Yang
Pulkit Grover
Soummya Kar