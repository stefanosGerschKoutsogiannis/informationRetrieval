2019,Fast Decomposable Submodular Function Minimization using Constrained Total Variation,We consider the problem of minimizing the sum of submodular set functions assuming minimization oracles of each summand function. Most existing approaches reformulate the problem as the convex minimization of the sum of the corresponding Lov\'asz extensions and the squared Euclidean norm  leading to algorithms requiring total variation oracles of the summand functions; without further assumptions  these more complex oracles require many calls to the simpler minimization oracles often available in practice. In this paper  we consider a modified convex problem requiring  constrained version of the total variation oracles that can be solved with significantly fewer calls to the simple minimization oracles.  We support our claims by showing results on graph cuts for 2D and 3D graphs.,Fast Decomposable Submodular Function

Minimization using Constrained Total Variation

K S Sesh Kumar

Data Science Institute

Imperial College London  UK
s.karri@imperial.ac.uk

Francis Bach

INRIA and Ecole normale superieure
PSL Research University  Paris France.

francis.bach@inria.fr

Thomas Pock

Institute of Computer Graphics and Vision 

Graz University of Technology  Graz  Austria.

pock@icg.tugraz.at

Abstract

We consider the problem of minimizing the sum of submodular set functions as-
suming minimization oracles of each summand function. Most existing approaches
reformulate the problem as the convex minimization of the sum of the correspond-
ing Lovász extensions and the squared Euclidean norm  leading to algorithms
requiring total variation oracles of the summand functions; without further assump-
tions  these more complex oracles require many calls to the simpler minimization
oracles often available in practice. In this paper  we consider a modiﬁed convex
problem requiring a constrained version of the total variation oracles that can be
solved with signiﬁcantly fewer calls to the simple minimization oracles. We support
our claims by showing results on graph cuts for 2D and 3D graphs.

1

Introduction

A discrete function F deﬁned on a ﬁnite ground set V of n objects is said to be submodular if the
marginal cost of each object reduces with the increase in size of the set it is conditioned on  i.e. 
F : 2V → R is submodular if and only if the marginal cost of an object {x} ∈ V conditioned on the
set A ⊆ V \ {x} denoted by F ({x}|A) = F ({x} ∪ A) − F (A) reduces as the set A becomes bigger.
The diminishing returns property of submodular functions has been central to solving several machine
learning problems such as document summarization [1]  sensor placement [2] and graphcuts [3]
(see [4] for more applications). Without loss of generality  we consider normalized submodular
functions  i.e.  F (∅) = 0.
Submodular function minimization (SFM) can be solved exactly using polynomial algorithms but with
high computational complexity. One of the standard algorithms is the Fujishighe-Wolfe algorithm [5 
6] but most recently  SFM has been tackled using cutting plane methods [7] and geometric scaling [8].
All the above algorithms rely on value function oracles  that is  access to F (A) for arbitrary subsets
A of V   and solve SFM with high running-time complexities  e.g.  O(n4 logO(1) n) and more. These
algorithms are typically not trivial to implement and do not scale to problems with large ground sets
(such as n = 106 in computer vision applications). For scalable practical solutions  it is imperative to
exploit the structural properties of the function to minimize.
Submodularity is closed under addition [5  4]. We make a structural assumption that is practically
useful in many machine learning problems [9  10] (e.g.  when a 2D-grid graph is seen as the
concatenation of vertical and horizontal chains) and consider the problem of minimizing a sum of

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

submodular functions [11]  i.e. 

r(cid:88)

i=1

Fi(A) 

min
A⊂V

F (A) :=

(1)

assuming each summand Fi  i = 1  . . .   r  is “simple”  i.e.  with available efﬁcient oracles which
are more complex than plain function evaluations. The simplest of these oracles is being able to
minimize the submodular function Fi plus some modular function  and we will consider these oracles
in this paper. This is weaker than the usual “total variation” oracles detailed below.
One of the standard approaches to solve the discrete optimization problem in Eq. (1) is to consider
an equivalent continuous optimization problem that minimizes the Lovász extension [12] f of the
submodular function over the n-dimensional unit hypercube (see a deﬁnition in Section 2). This
approach uses a well known result in the submodularity literature that the minimizers of the set
function F can be directly obtained from the minimizers of its Lovász extension f; the continuous
optimization problem is given by

i=1

min

fi(w) 

f (w) :=

w∈[0 1]n

(2)
where fi is the Lovász extensions of submodular function Fi  for each i ∈ [r]. Lovász extension of
submodular functions are convex but non-smooth and piecewise linear functions. Therefore  we can
√
use subgradients as they can be calculated using greedy algorithms in O(n log(n)) time and O(n)
calls to the value function oracle per iteration [13]. However  this is slow with O(1/
t) convergence
rate where t is the number of iterations of the optimization algorithms. Moreover  in signal processing
applications  high precision is needed  hence the need for faster algorithms.
An alternative approach is to consider a continuous optimization problem [4  Chapter 8] of the form

min
w∈Rn

f (w) +

ψ(wj) 

(3)

where ψ : R → R is a convex function whose Fenchel-conjugate [14] is deﬁned everywhere (in order
to have a well-deﬁned dual as later shown in Eq. (9)). This is equivalent to solving all the following
discrete optimization problems parameterized by α ∈ R 

r(cid:88)

n(cid:88)

j=1

ψ(w) =

if |w| (cid:54) ε 
2 w2
+∞ otherwise 

(cid:26) 1

2

F (A) + |A|ψ(cid:48)(α).

min
A⊂V

Given the solutions Aα for all α ∈ R in Eq. (4)  then we may obtain the optimal solution w∗ of
j = sup{α ∈ R  j ∈ Aα}. Conversely  given the optimal solution w∗ of Eq. (3)  we
Eq. (3) using w∗
may obtain the solutions Aα of the discrete optimizaton problems in Eq. (4) by thresholding at α 
i.e.  {w∗ ≥ α}. As a consequence of this we can obtain the solution of Eq. (1)  when α is chosen
so that ψ(cid:48)(α) = 0 (typically α = 0 because ψ is even). Note that this algorithmic scheme seems
wasteful because we take a continuous solution of Eq. (3) and only keep the signs of its solution. One
contribution of the paper is to propose a function ψ that focuses only on values of w close to zero.

Our problem setting and approach. We assume the availability of SFM oracles of the individual
summand functions  which we refer to as SFMDi  for each i ∈ [r] that gives the optimal solution of
(5)
where u ∈ Rn is any n-dimensional vector and 1A ∈ {0  1}n is the indicator function of the set A.
Note that the complexity of the oracle does not typically depend on the vector u. We consider the
following continuous optimization problem  which we refer to as SFMCi for each i ∈ [r] 

Fi(A) − u(cid:62)1A 

SFMDi : argmin

A⊂V

SFMCi : argmin
w∈Rn

fi(w) − t(cid:62)w +

ψ(wj) 

(6)

where t ∈ Rn is any n-dimensional vector. In our setting  we consider ψ as the following convex
function 

n(cid:88)

j=1

(4)

(7)

where ε ∈ R+. In Section 3.1  we show that the continuous optimization problem SFMCi can be
optimized using discrete oracles SFMDi using a modiﬁed divide-and-conquer algorithm [15]. In
Section 3.2  we use various optimization algorithms that use SFMCi as the inner loop to solve the
continuous optimization problem in Eq. (3) consequently solving the SFM problem in Eq. (1).

Related work. Most of the earlier works have considered quadratic functions for the choice of
2 v2. As a result  SFMCi in Eq. (6) is referred to as total
ψ [15  16  17  18  19  20]  i.e.  ψ(v) = 1
2 (t− w)2 [21]. These
variation or TV oracle as they solve the problems of the form minw∈Rn f (w) + 1
oracles are efﬁcient for cut functions deﬁned on chain graphs [22  23] with O(n) complexity. However 
this does not hold for general submodular functions. One way to solve continuous optimization
problems like SFMCi is to use a sequence of at most n discrete minimization oracles like SFMDi
through divide-and-conquer algorithms (see Section 3.1).
Recent work has also focused on using directly discrete minimization oracles of the form SFMDi 
such as [16] that considers a total variation problem with active set methods; [24] used discrete mini-
mization oracles SFMDi but solved a different convex optimization problem. [25] also considers the
total variation problem but uses incidence relations and oblique projections for quicker convergence.
[26] reduces the search space for the SFM problem  i.e.  V   using heuristics. Our choice of ψ results
in a similar reduction of the search space that results in a more efﬁcient solution.

Contributions. Our main contribution is to propose a new convex optimization problem that can be
used to ﬁnd the minimum of a sum of submodular set-functions. For graph cuts  this new problem can
be seen as a constrained total variation problem that is more efﬁcient that the regular total variation
(lesser number of discrete minimization oracle calls). This is beneﬁcial when minimizing the sum
of constrained total variation problems  and consequently beneﬁcial for the corresponding discrete
minimization problem  i.e.  minimizing the sum of submodular functions. For the case of sum of
two functions  we show that recent acceleration techniques from [27] can be highly beneﬁcial in
our case. This is validated using experiments on segmentation of two dimensional images and three
dimensional volumetric surfaces.
Note that we use cuts mainly due to easy access to minimization oracles of cut functions [28]  but our
result applies to all submodular functions.

2 Review of Submodular Function Minimization (SFM)

In this section  we review the relevant concepts from submodular analysis (for more details  see
[4  5]). All possible subsets of the ground set V can be considered as the vertices {0  1}n of the
hypercube in n dimensions (going from A ⊆ V to 1A ∈ {0  1}n). Thus  any set-function may be
seen as a function F deﬁned on the vertices of the hypercube {0  1}n. It turns out that F may be
extended to the full hypercube [0  1]n by piecewise-linear interpolation  and then to the whole vector
space Rn [4].
This extension f is piecewise linear for any set-function F . It turns out that it is convex if and only
if F is submodular [12]. Any piecewise linear convex function may be represented as the support
function of a certain polytope K  i.e.  as f (w) = maxs∈K w(cid:62)s [14]. For the Lovász extension of a
submodular function  there is an explicit description of K  which we now review.

Base polytope. We deﬁne the base polytope as B(F ) = (cid:8)s ∈ Rn  s(V ) = F (V )  ∀A ⊂
V  s(A) (cid:54) F (A)(cid:9)  where we use the classical notation s(A) = s(cid:62)1A. A key result in submodular

analysis is that the Lovász extension is the support function of B(F )  that is  for any w ∈ Rn 

f (w) = sup

s∈B(F )

w(cid:62)s.

(8)

The maximizers above may be computed in closed form from an ordered level-set representation
of w using a “greedy algorithm”  which (a) ﬁrst sorts the elements of w in decreasing order such
that wσ(1) ≥ . . . ≥ wσ(n) where σ represents the order of the elements in V ; and (b) computes
sσ(k) = F ({σ(1)  . . .   σ(k)}) − F ({σ(1)  . . .   σ(k − 1)}). This leads to a closed-form formula
for f (w) and a subgradient.

3

SFM as a convex optimization problem. A key result from submodular analysis [12] is the
equivalence between the SFM problem minA⊆V F (A) and the convex optimization problem
minw∈[0 1]n f (w). One can then obtain an optimal A from level sets of an optimal w. More-
over  this leads to the dual problem maxs∈B(F )
i=1(si)−. Note that for our algorithm to work  we
need oracles SFMDi that output both the primal variable (A or w) and the dual variable s ∈ B(F ).

(cid:80)n

Convex optimization and its dual. We consider the continuous optimization problem in Eq. (3).
Its dual problem derived using Eq. (8) is given by

ψ∗(−sj).

(9)

− n(cid:88)

j=1

max
s∈B(F )

(cid:40) 1

In this paper  we consider the convex function ψ : R → R deﬁned in Eq. (7). Its Fenchel-conjugate
ψ∗ is given by

ψ∗(s) =

2 s2
ε|s| − ε2

2

if |s| (cid:54) ε 
otherwise.

(10)

3 Fast Submodular Function Minimization with Constrained Total

Variation

In this section  we propose an algorithm to optimize the continuous optimization problem in Eq. (3)
using minimization oracles of individual discrete functions SFMDi in Eq. (5). As a ﬁrst step  we
propose a modiﬁed divide-and-conquer algorithm to solve the continuous optimization problem
SFMCi  for each i ∈ [r] in Section 3.1. In Section 3.2  we use the optimization problems SFMCi as
black boxes to solve the continuous optimization problem in Eq. (3). The overview is provided in
Algorithm 1.

Algorithm 1 From SFMDi to SFMC
1: Input Discrete function minimization oracle for Fi : 2V → R and ε ∈ R+.
2: output Optimal primal/dual solutions for Eq. (3) and Eq. (9) respectively (w∗  s∗)
3: for all i ∈ [r] do
4:

Optimize SFMC using SFMCi by applying algorithms in Section 3.3 and Section 3.4. This
requires optimal primal-dual solutions of SFMCi  (w∗
i ) that may be obtained using Algo-
rithm 2 in Section 3.1 assuming oracles SFMDi.

i   s∗

5: end for
6: (w∗(U )  s∗(U )) = (w∗

U   s∗
U )

3.1 Single submodular function

For brevity  we drop the subscript i and consider the following primal optimization problem. Algo-
rithm 2 below is an extension of the classical divide-and-conquer algorithm from [29]. Note that it
requires access to dual certiﬁcates for the SFM problems.

Algorithm 2 From SFMDi to SFMCi
1: Input Discrete function minimization oracle for F : 2V → R and ε ∈ R+.
2: output Optimal primal/dual solutions for Eq. (3) and Eq. (9) respectively (w∗  s∗)
3: A+ = argminA⊂V F (A) + ε|A| with a dual certiﬁcate s+ ∈ B(F ).
4: A− = argminA⊂V F (A) − ε|A| with a dual certiﬁcate s− ∈ B(F ) (we must have A+ ⊆ A−)
5: w∗(A+) = −ε  s∗(A+) = s+  w∗(V \ A−) = ε  s∗(V \ A−) = s−
6: U := A− \ A+ and a discrete function G : 2U s.t. G(B) = F (A+ ∪ B) − F (A+) with Lovász

extension g : {0  1}|U| → R.
algorithm [16] to obtain (w∗
U   s∗
U ).
U   s∗
U )

7: Solve for optimal solutions of minw∈R|U| g(w) + 1
8: (w∗(U )  s∗(U )) = (w∗

2 w2 and its dual using divide-and-conquer

4

Proposition 1 Algorithm 2 gives an optimal primal-dual pair for the optimization problem in Eq. (3)
and Eq. (9) respectively.

See the proof in the supplementary material (Section A). Note that the number of steps is at most
the number of different values that w may take (the solution w is known to have many equal
components [30]). In the worst case  this is still n  but in practice many components are equal to −ε
or ε  thus reducing the number of SFM calls (for ε very close to zero  only two calls are necessary). In
Section 5  we show empirically that this is the case  the number of SFM calls decreases signiﬁcantly
when ε tends to zero.

3.2 Sum of submodular functions

In this section  we consider the optimization problem in Eq. (3) with the function ψ from Eq. (7).
The primal optimization problem is given by

min

w∈[−ε ε]n

fi(w) +

(cid:107)w(cid:107)2
2.

1
2

(11)

r(cid:88)

i=1

In order to derive a dual problem with the appropriate structure  we consider the functions gi deﬁned
as follows: gi(w) = fi(w) if |w| (cid:54) ε  and +∞ otherwise  with the Fenchel conjugate
ti∈B(Fi)

w(cid:62)si − fi(w) = inf

w(cid:62)(si − ti) = ε

g∗
i (si) = sup

(cid:107)si − ti(cid:107)1.

ti∈B(Fi)

sup

inf

w∈[−ε ε]n

w∈[−ε ε]n

Therefore  we can derive the following dual:

min

w∈[−ε ε]n

fi(w) +

1
2

(cid:107)w(cid:107)2

2 = min
w∈Rn

gi(w) +

r(cid:88)

i=1

r(cid:88)
r(cid:88)

i=1

i=1
max

= min
w∈Rn

max
si∈Rn

=

(s1 ... sr)∈Rn×r

(cid:107)w(cid:107)2

1
2

2

(cid:8)w(cid:62)si − g∗
− r(cid:88)

i (si)(cid:9) +
(cid:13)(cid:13) r(cid:88)

i (si) − 1
g∗
2

1
2

i=1

i=1

(cid:107)w(cid:107)2

2

(cid:13)(cid:13)2

2.

si

(12)

We are now faced with the similar optimization problem than previous work [15]  where the primal
problems is equivalent to computing the proximity operator of the sum of functions g1 + g2. The
main difference is that when ε is inﬁnite (i.e.  with no constraints)  then the dual functions g∗
i are
indicator functions of the base polytopes B(Fi)  and the dual problem in Eq. (12) can be seen as
ﬁnding the distance between two polytopes.
This is not the case for our constrained functions. This limits the choice of algorithms. In this paper 
we consider block-coordinate ascent (which was already considered in [15]  leading to alternate
projection algorithms)  and a novel recent accelerated coordinate descent algorithm [27]. We could
also consider (accelerated) proximal gradient descent on the dual problem in Eq. (12)  but it was
shown empirically to be worse than alternating reﬂections [15] (which we compare to in experiments 
but which we cannot readily extend without adding a new hyperparameter).

3.3 Optimization algorithms for all r
All of our algorithms will rely on the computing the proximity operator of the functions g∗
we now consider.
Proximity operator. The key component we will need is the so-called proximal operator of g∗
i  
that is being able to compute efﬁciently  for a certain η 

i   which

g∗
i (si) + 1

2η(cid:107)si − ti(cid:107)2
2.

min
si∈Rn

min
si∈Rn

g∗
i (si) + 1

2η(cid:107)si − ti(cid:107)2

Using the classical Moreau identity [31]  this is equivalent to solving
i si − gi(wi) + 1
w(cid:62)
η ti(cid:107)2
2(cid:107)wi(cid:107)2
2.

max
wi∈Rn
−gi(wi) − η
−gi(wi) + w(cid:62)

2 = min
si∈Rn
= max
wi∈Rn
= max
wi∈Rn

2(cid:107)wi − 1
i ti − η

2η(cid:107)si(cid:107)2
2 + 1

2 + 1
2η(cid:107)ti(cid:107)2

2

2η(cid:107)ti(cid:107)2

2 − 1

η s(cid:62)
i ti

5

This is exactly the oracle SFMCi  for which we presented in Section 3.1 an algorithm using only the
discrete oracles SFMDi.

Block coordinate ascent. We consider the following iteration

∀i ∈ [r]  snew

i = argmin
i ∈Rn
snew

g∗
i (snew

i

) + 1
2

(cid:13)(cid:13)(cid:80)i

j +(cid:80)r

j=1 snew

j=i+1 sj

(cid:13)(cid:13)2

2 

which is exactly block-coordinate ascent on the dual problem in Eq. (12). Since the non-smooth
i (si) is separable  it is globally convergent  with a convergence rate at least equal to

i=1 g∗

O(1/t)  where t is the number of iterations (see  e.g.  Theorem-6.7 of [32]).

function(cid:80)r

3.4 Acceleration for the special case r = 2

When there are only two functions  following [27]  the problem in Eq. (12) can be written as:

−g∗

1(s1) − h1(s1) 

max
s1∈Rn

(13)

with

2(cid:107)s1 + s2(cid:107)2

2(cid:107)s1(cid:107)2
2(cid:107)w2(cid:107)2

2 − 1
2 − w(cid:62)

g∗
2(s2) + 1
−g2(w2) + 1
−g2(w2) − 1

h1(s1) = inf
s2∈Rn
= sup
w2∈Rn
= sup
w2∈Rn

2 s2 − g2(w2) + 1
w(cid:62)
2  with s2 = w2 + s1 
−f2(w2) − w(cid:62)

2(cid:107)s1 + s2(cid:107)2 = sup
inf
s2∈Rn
w2∈Rn
2(cid:107)w2 + s1(cid:107)2
2 s1 =
w2∈[−ε ε]n
1(s1) = s1 + s∗
The function h1 is 1-smooth with gradient equal to h(cid:48)
1(s1) − h1(s1) leads to the iteration
gradient to the problem of maximizing maxs1∈Rn −g∗
2(cid:107)s1 + snew
(cid:107)2
1 − s1(cid:107)2
2(cid:107)snew
2 + h(cid:48)
2(cid:107)snew
1 − s1(cid:107)2
2(cid:107)snew

g∗
2(snew
g∗
1(snew
g∗
1(snew
g∗
1(snew

2 + (s1 + snew
(cid:107)2
2 

= argmin
2 ∈Rn
snew
= argmin
1 ∈Rn
snew
= argmin
1 ∈Rn
snew
= argmin
1 ∈Rn
snew

1 − s1)
)(cid:62)(snew

1(s1)(cid:62)(snew

1 − s1)

1 + snew

snew
1

snew
2

) + 1

) + 1

) + 1

) + 1

sup

2

1

1

1

2

2

2

2 s1 − 1

2(cid:107)w2(cid:107)2
2.

2(s1). Applying proximal

which is exactly block coordinate descent. Each of these steps are exactly using the same oracle as
before. We can now accelerate it using FISTA [33] with the step size from the smoothness constant
which is equal to 1. Starting from a pair of iterate (s1  t1)  this leads to the iteration:

snew
2

snew
1

tnew
1

2

) + 1

= argmin
2 ∈Rn
snew
= argmin
1 ∈Rn
snew
= snew
1 + β(snew

g∗
2(snew
g∗
) + 1
1(snew
1 − s1) 

1

2

(cid:107)2

2(cid:107)t1 + snew
2(cid:107)snew

1 + snew

2

(cid:107)2

2

with β = (t − 1)/(t + 2) at iteration t. This algorithm converges in O(1/t2).
This acceleration can also be used for the case r > 2 by using the product space trick (see  e.g.  [15 
Section 3.2]). However  this requires a correction in the product space that leads to inefﬁciencies of
the algorithm in practice. See [34] for more details.

4 Theoretical Analysis

r(cid:88)

i=1

In this section  we provide a convergence analysis for the methods above. For simplicity of results 
we consider the following primal-dual formulation (where both primal and dual variables live in
bounded sets):

min

w∈[−ε ε]n

fi(w) +

1
2

(cid:107)w(cid:107)2

2 = min
w∈Rn

w(cid:62)ti +

max

ti∈B(Fi)

r(cid:88)

i=1

n(cid:88)
− n(cid:88)

j=1

ψ(wj)

ψ∗(cid:0) r(cid:88)

(cid:1).

tij

(14)

j=1

i=1

=

(t1 ... tr)∈B(F1)×···×B(Fr)

max

6

(a)

(b)

(c)

Figure 1: Comparison to state-of-the-art algorithms for 2D and 3D SFM.

We assume that we have a pair (w  t1  . . .   tr) of approximate primal-dual solutions for Eq. (14)  with
a duality gap ηC. This leads to a pair (w  u) of primal-dual approximate solutions for

min

w∈[−ε ε]n

f (w) +

1
2

(cid:107)w(cid:107)2

2 = max
u∈B(F )

ψ∗(uj) 

(15)

− n(cid:88)

j=1

for which we can get an approximate subset of V (see proof in supplementary material  Section B):

Proposition 2 Given a feasible primal candidate w for Eq. (15) with suboptimality ηC  one of the
suplevel sets {w (cid:62) α} of w is an ηD-optimal minimizer of F   with ηD = ηC

4ε +(cid:112) ηCn

2 .

Since our dual problems are all O(1)-smooth (using the traditional deﬁnitions of smoothness [35]) 
their guarantees will always be of the form ηC = ∆2
tα where ∆ is a notion of diameter of the base
polytopes and α = 2 for accelerated algorithms and α = 1 for plain algorithms. The overall discrete
gap is thus up to constant terms

√

ηD =

∆
n
tα/2

+

∆2
εtα .

We see clearly that the ﬁnal bound on the (discrete) gap is decreasing with ε. This suggest to use ε
proportional to ∆√
ntα to take it as small as possible while only losing a factor of 2 in the convergence
bound.
Guarantees for FISTA applied to the dual of Eq. (14). The function ψ∗ is O(1)-smooth  and the
objective in Eq. (14) is r-smooth. Each B(Fi) has a square diameter less than ∆2
i=1 ∆2
to [36  Cor. 2(b)]  these guarantees extend to the corresponding primal iterate w.

(cid:2)Fi({j})+

i =(cid:80)n

i and α = 2. Owing

j=1

Fi(V \{j}) − Fi(V )(cid:3)2. Thus  in the result above  we have ∆2 = r(cid:80)r
(cid:1) +(cid:80)n
nr(cid:112)(cid:80)r

The primal set has squared diameter nε2; the dual set has squared diameter less than(cid:80)r
a primal-dual algorithm  of the form ∆2 = r(cid:80)r

Guarantees for primal-dual algorithms applied to Eq. (14). We consider the primal-dual for-
mulation

i   the
r. Thus  from [37]  we get a guarantee from
i . We thus get overall a
i + ε

bilinear function has a largest singular value equal to

w(cid:62)(cid:0)(cid:80)r

(t1 ... tr)∈B(F1)×···×B(Fr)

min

w∈[−ε ε]n

j=1 ψ(wj).

i=1 ∆2

i=1 ∆2

√

i=1 ∆2

i=1 ti

√

max

guarantee of the same form as above  with the same dependency in ε.

5 Experiments

In this section  we consider the minimization of cut functions [3] that are an important examples
of submodular functions. In our experiments  we consider the problem of minimizing cuts on 2D
images and 3D volumetric surfaces for segmentation. We consider a two-dimensional image of size
n = 2400 × 2400 = 5.8 × 106 pixels  and a 3D volumetric surface of size n = 102 × 100 × 79 =
8.1 × 105 voxels. The SFM oracles are obtained by using max-ﬂow codes  which is the dual of the
min-cut problem. We compare our results to the standard block coordinate descent (BCD) [15] and

7

#SFM calls0200040006000Discrete Duality Gap10-410-2100102104106BCDaccBCD∆acc∆∆/tacc∆/t∆/√tacc∆/√tAAR#2D SFM calls0200400600Discrete Duality Gap10-2100102104106BCDaccBCD∆acc∆∆/tacc∆/t∆/√tacc∆/√t#SFM calls050010001500Discrete Duality Gap10-410-2100102104106BCD∆∆/t∆/√tAARaveraged alternating reﬂections algorithm (AAR) [15]  which are using full total variation oracles
(which we solve using the usual divide-and-conquer algorithm that is only using SFM calls).
In our approach  we have a parameter ε dependent on ∆√
ntα   where ∆ is the notion of diameter of the
√
base polytope  n is the number of elements in the ground set and t is the number of iterations. In our
experiments  we choose ε proportional to ∆  ∆/t and ∆/
t and respectively represent them by the
same terms in Figure 1. For the case of the sum of two functions  the block coordinate descent can be
√
accelerated [27] as shown in Section 3.4. We refer to their accelerated versions as acc BCD  acc
∆  acc ∆/t and acc ∆/
t respectively. Therefore  BCD  acc BCD  AAR use quadratic ψ and
the rest use ψ as deﬁned in Eq. (7).
Figure 1 shows the performance of various algorithms on different problems which we detail below.
The horizontal axes represents the number of discrete minimization oracles  i.e.  SFMDi required to
solve the SFM and the vertical axes represents the discrete duality gap given by

where A ⊂ V   s ∈ B(F ) are the discrete primal-dual pairs and s−(V ) = (cid:80)n

gap(A  s) = F (A) − s−(V ) 

i=1 min(si  0). We
consider three experiments that may be broadly classiﬁed into sum of two functions and sum of three
functions.

Sum of two functions (r = 2).
In this case  we consider minimization of the submodular function
that can be written as sum of two submodular functions  i.e.  F = F1 + F2. We consider the problem
of mininiming graph cuts on 2D grid that can be written as the sum of horizontal and vertical chain
graphs in Figure 1-(a). In this case  the SFMDi orcale represents the min-cut on a chain graph while
the SFM problem represents min-cut on a 2D grid. We can observe that the constrained total variation
formulation reduces the number of min-cut/ max-ﬂow calls when compared to full total variation.
Here  we explicitly calculate the diameter of the base polytope ∆ for choosing ε.
Figure 2-(a) shows the total number of SFMDi oracle calls required to solve the SFM problem for
different values of ε. Figure 2-(b) shows the total number of constrained TV SFMCi calls required
to solve the SFM problem. Figure 2-(c) shows the average number of SFMDi oracle calls required to
solve a single SFMCi problem. The algorithms considered in these graphs are BCD and accelerated
BCD algorithms using constrained total variation represented by ε and acc ε respectively. We clearly
see the trade-off for the choice of ε: the number of SFM calls per TV calls increases with ε  while the
number of TV calls decreases  leading to intermediate values of ε which lead to signiﬁcant gains in
the total number of SFM calls in Figure 2-(a).
We consider 3D grid that can be decomposed into 2D frames and chains graphs. In Figure 1-(b) 
we show the performance of our algorithm compared to other state-of-the-art algorithms for this
decomposition. In this case we use two different discrete oracles SFMDi  i.e.  min-cut on a chain
and min-cut on a 2D grid to solve SFM  i.e.  min-cut on the 3D grid. We show only the number of
oracle calls to min-cut on 2D grids for analysis as they are more expensive than min-cuts on chains.

Sum of three functions (r = 3).
In this case  we consider minimization of the submodular function
that can be written as sum of three submodular functions  i.e.  F = F1 + F2 + F3. Min-cut on the 3D
grid can also be seen as sum of chain graphs in three directions  thereby using discrete minimization
oracles only of the chain graphs. Figure 1-(c) shows the number of calls to 1D min-cut to solve the
3D min-cut problem using various continuous optimization problems and algorithms. Our approach
considerably reduces the number of calls to 1D min-cut (SFMDi) oracles. Figure 2-(d) shows the
total number of 1D min-cuts (SFMDi) to solve 3D SFM for various values of ε. Figure 2-(e) shows
the total number of constrained total variation SFMCi calls required to solve the SFM problem.
Figure 2-(e) shows the average number of SFMDi oracle calls required to solve SFMCi for this
problem. We observe a similar behavior than for r = 2  with best values of ε not being very small or
very large.

6 Conclusion

In this paper  we have proposed a simple modiﬁcation of state-of-the-art algorithms for decomposable
submodular function minimization. Adding box constraints to the continuous optimization problems
allow for signiﬁcant reduction in the number of individual submodular function minimization calls.

8

(a)

(d)

(b)

(e)

(c)

(f)

Figure 2: BCD and acc BCD for 2D function: F = F1 + F2 and 3D functions: F = F1 + F2 + F3.

The application of accelerated block coordinate ascent techniques makes the speed-up stronger.
Further speed-ups may be achieved by extended the proposed algorithms to [18  25]. These techniques
are easily parallelizable and it would be interesting to compare to dedicated parallel algorithms
for graph cuts [38]. Moreover  these speed-ups could be extended to more general submodular
optimization problems [39].

Acknowledgments. This research was funded by the Leverhulme Centre for the Future of Intelli-
gence  Cambridge and the Data Science Institute  Imperial College London. We acknowledge support
from the European Research Council (SEQUOIA project 724063) and (HOMOVIS project 640156).

References
[1] H. Lin and J. Bilmes. A class of submodular functions for document summarization.

Proceedings of NAACL/HLT  2011.

In

[2] A. Krause and C. Guestrin. Submodularity and its applications in optimized information

gathering. ACM Transactions on Intelligent Systems and Technology  2011.

[3] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts.

IEEE Transactions in Pattern Analysis and Machine Intelligence  23(11):1222–1239  2001.

[4] F. Bach. Learning with Submodular Functions: A Convex Optimization Perspective  volume 6

of Foundations and Trends in Machine Learning. NOW  2013.

[5] S. Fujishige. Submodular Functions and Optimization. Elsevier  2005.

[6] D. Chakrabarty  P. Jain  and P. Kothari. Provable submodular minimization using Wolfe’s

algorithm. In Advances in Neural Information Processing Systems  2014.

[7] Y. T. Lee  A. Sidford  and S. C. Wong. A faster cutting plane method and its implications for
combinatorial and convex optimization. In Annual Symposium on Foundations of Computer
Science  2015.

[8] Daniel Dadush  László A. Végh  and Giaeomo Zambelli. Geometric rescaling algorithms for
submodular function minimization. In Proceedings of the Symposium on Discrete Algorithms
(SODA)  2018.

[9] P. Stobbe. Convex Analysis for Minimizing and Learning Submodular Set functions. PhD thesis 

California Institute of Technology  2013.

9

Epsilon100#SFM calls200030004000500060007000εaccεEpsilon100105#TV calls0100200300400εaccεEpsilon100105Avg. SFM per TV2025303540εaccεEpsilon1001051010#SFM calls500600700800εEpsilon1001051010#TV calls1520253035εEpsilon1001051010Avg. SFM per TV15202530354045ε[10] N. Komodakis  N. Paragios  and G. Tziritas. MRF energy minimization and beyond via dual
decomposition. IEEE Transactions in Pattern Analysis and Machine Intelligence  33(3):531–
552  2011.

[11] V. Kolmogorov. Minimizing a sum of submodular functions. Discrete Applied Mathematics 

2012.

[12] L. Lovász. Submodular functions and convexity. Mathematical Programming: the State of the

Art  Bonn  pages 235–257  1982.

[13] J. Edmonds. Submodular functions  matroids  and certain polyhedra.

optimization - Eureka  you shrink!  pages 11–26. Springer  1970.

In Combinatorial

[14] R. T. Rockafellar. Convex Analysis. Princeton U. P.  1997.

[15] S. Jegelka  F. Bach  and S. Sra. Reﬂection methods for user-friendly submodular optimization.

In Advances in Neural Information Processing Systems  2013.

[16] K. S. Sesh Kumar and Francis Bach. Active-set methods for submodular minimization problems.

Journal of Machine Learning Research  18(1):4809–4839  2017.

[17] Alina Ene and Huy Nguyen. Random coordinate descent methods for minimizing decomposable
submodular functions. In International Conference on Machine Learning  pages 787–795  2015.

[18] Alina Ene  Huy Nguyen  and László A Végh. Decomposable submodular function minimization:

discrete and continuous. In Advances in Neural Information Processing Systems  2017.

[19] R. Nishihara  S. Jegelka  and M. I. Jordan. On the convergence rate of decomposable submodular
function minimization. In Advances in Neural Information Processing Systems 27  pages 640–
648  2014.

[20] Chetan Arora  Subhashis Banerjee  Prem Kalra  and S. N. Maheshwari. Generic cuts: An

efﬁcient algorithm for optimal inference in higher order MRF-MAP. In ECCV  2012.

[21] A. Chambolle. Total variation minimization and a class of binary MRF models. In Energy

Minimization Methods in Computer Vision and Pattern Recognition  2005.

[22] Laurent Condat. A direct algorithm for 1d total variation denoising. Technical report  GREYC

laboratory  CNRS-ENSICAEN-Univ. of Caen  2012.

[23] Álvaro Barbero and Suvrit Sra. Modular proximal optimization for multidimensional total-

variation regularization. Technical Report 1411.0589  ArXiv  2014.

[24] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In

Advances in Neural Information Processing Systems  2010.

[25] P. Li and O. Milenkovic. Revisiting decomposable submodular function minimization with

incidence relations. In Advances in Neural Information Processing Systems  2018.

[26] Weizhong Zhang  Bin Hong  Lin Ma  Wei Liu  and Tong Zhang. Safe element screening for
submodular function minimization. In International Conference on Machine Learning  2018.

[27] Antonin Chambolle and Thomas Pock. A remark on accelerated block coordinate descent for
computing the proximity operators of a sum of convex functions. Journal of computational
mathematics  1:29–54  2015.

[28] V. Kolmogorov and R. Zabih. What energy functions can be minimized by graph cuts? IEEE

TPAMI  26(2):147–159  2004.

[29] H. Groenevelt. Two algorithms for maximizing a separable concave function over a polymatroid

feasible region. European Journal of Operational Research  54(2):227–236  1991.

[30] Francis Bach. Shaping level sets with submodular functions. In Advances in Neural Information

Processing Systems  pages 10–18  2011.

10

[31] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal
processing. In Fixed-point Algorithms for Inverse Problems in Science and Engineering  pages
185–212. Springer  2011.

[32] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and

Trends R(cid:13) in Machine Learning  8(3-4):231–357  2015.

[33] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[34] K. S. Sesh Kumar  A. Barbero  S. Jegelka  S. Sra  and F. Bach. Convex optimization for parallel

energy minimization. Technical Report 01123492  HAL  2015.

[35] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course  volume 87.

Springer Science & Business Media  2013.

[36] Tseng Paul. On accelerated proximal gradient methods for convex-concave optimization.

submitted to SIAM Journal on Optimization  2008.

[37] Antonin Chambolle and Thomas Pock. On the ergodic convergence rates of a ﬁrst-order

primal–dual algorithm. Mathematical Programming  159(1-2):253–287  2016.

[38] A. Shekhovtsov and V. Hlavác. A distributed mincut/maxﬂow algorithm combining path
augmentation and push-relabel. In Energy Minimization Methods in Computer Vision and
Pattern Recognition  2011.

[39] Francis Bach. Submodular functions: from discrete to continuous domains. Mathematical

Programming  pages 1–41  2016.

11

,Senanayak Sesh Kumar Karri
Francis Bach
Thomas Pock