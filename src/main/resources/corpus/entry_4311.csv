2015,Policy Evaluation Using the Ω-Return,We propose the Ω-return as an alternative to the λ-return currently used by the TD(λ) family of algorithms. The benefit of the Ω-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly  we suggest one way of approximating the Ω-return. We provide empirical studies that suggest that it is superior to the λ-return and γ-return for a variety of problems.,Policy Evaluation Using the Ω-Return

Philip S. Thomas

University of Massachusetts Amherst

Carnegie Mellon University

Scott Niekum

University of Texas at Austin

Georgios Theocharous

Adobe Research

George Konidaris
Duke University

Abstract

We propose the Ω-return as an alternative to the λ-return currently used by the
TD(λ) family of algorithms. The beneﬁt of the Ω-return is that it accounts for
the correlation of different length returns. Because it is difﬁcult to compute ex-
actly  we suggest one way of approximating the Ω-return. We provide empirical
studies that suggest that it is superior to the λ-return and γ-return for a variety of
problems.

1

Introduction

Most reinforcement learning (RL) algorithms learn a value function—a function that estimates the
expected return obtained by following a given policy from a given state. Efﬁcient algorithms for esti-
mating the value function have therefore been a primary focus of RL research. The most widely used
family of RL algorithms  the TD(λ) family [1]  forms an estimate of return (called the λ-return) that
blends low-variance but biased temporal difference return estimates with high-variance but unbiased
Monte Carlo return estimates  using a parameter λ ∈ [0  1]. While several different algorithms exist
within the TD(λ) family—the original linear-time algorithm [1]  least-squares formulations [2]  and
methods for adapting λ [3]  among others—the λ-return formulation has remained unchanged since
its introduction in 1988 [1].
Recently Konidaris et al. [4] proposed the γ-return as an alternative to the λ-return  which uses a
more accurate model of how the variance of a return increases with its length. However  both the γ
and λ-returns fail to account for the correlation of returns of different lengths  instead treating them
as statistically independent. We propose the Ω-return  which uses well-studied statistical techniques
to directly account for the correlation of returns of different lengths. However  unlike the λ and
γ-returns  the Ω-return is not simple to compute  and often can only be approximated. We propose a
method for approximating the Ω-return  and show that it outperforms the λ and γ-returns on a range
of off-policy evaluation problems.

2 Complex Backups

Estimates of return lie at the heart of value-function based RL algorithms: an estimate  ˆV π  of the
value function  V π  estimates return from each state  and the learning process aims to reduce the
error between estimated and observed returns. For brevity we suppress the dependencies of V π and
ˆV π on π and write V and ˆV . Temporal difference (TD) algorithms use an estimate of the return
obtained by taking a single transition in the Markov decision process (MDP) [5] and then estimating
the remaining return using the estimate of the value function:

RTD
st

= rt + γ ˆV (st+1) 

1

where RTD
is the return estimate from state st  rt is the reward for going from st to st+1 via action
at  and γ ∈ [0  1] is a discount parameter. Monte Carlo algorithms (for episodic tasks) do not use
st
intermediate estimates but instead use the full return 

for an episode L transitions in length after time t (we assume that L is ﬁnite). These two types of
return estimates can be considered instances of the more general notion of an n-step return 

R(n)
st

=

γirt+i

+ γn ˆV (st+n) 

for n ≥ 1. Here  n transitions are observed from the MDP and the remaining portion of return is
estimated using the estimate of the value function. Since st+L is a state that occurs after the end of
an episode  we assume that ˆV (st+L) = 0  always.
A complex return is a weighted average of the 1  . . .   L step returns:

RMC
st

=

γirt+i 

L−1(cid:88)

i=0

(cid:33)

(cid:32)n−1(cid:88)

i=0

L(cid:88)

n=1

R†

st

=

w†(n  L)R(n)
st

 

(1)

where w†(n  L) are weights and † ∈ {λ  γ  Ω} will be used to specify the weighting schemes of
different approaches. The question that this paper proposes an answer to is: what weighting scheme
will produce the best estimates of the true expected return?
st  is the weighting scheme that is used by the entire family of TD(λ) algorithms
The λ-return  Rλ
[5]. It uses a parameter λ ∈ [0  1] that determines how the weight given to a return decreases as the
length of the return increases:

(cid:40)
1 −(cid:80)n−1

(1 − λ)λn−1

i=1 wλ(i)

if n < L
if n = L.

wλ(n  L) =

= RTD

= RMC

st   which has low variance but high bias. When λ = 1  Rλ
st

st   which
When λ = 0  Rλ
st
has high variance but is unbiased. Intermediate values of λ blend the high-bias but low-variance
estimates from short returns with the low-bias but high-variance estimates from the longer returns.
The success of the λ-return is largely due to its simplicity—TD(λ) using linear function approxi-
mation has per-time-step time complexity linear in the number of features. However  this efﬁciency
comes at a cost: the λ-return is not founded on a principled statistical derivation.1 Konidaris et al. [4]
remedied this recently by showing that the λ-return is the maximum likelihood estimator of V (st)
|V (st) = x) if
given three assumptions. Speciﬁcally  Rλ
st
Assumption 1 (Independence). R(1)
Assumption 2 (Unbiased Normal Estimators). R(n)
st
V (st) for all n.
Assumption 3 (Geometric Variance). Var(R(n)

∈ arg maxx∈R Pr(R(1)
st   . . .   R(L)
st
st are independent random variables 

st   . . .   R(L)

is normally distributed with mean E[R(n)

st ] =

st ) ∝ 1/λn.

st   R(2)

Although this result provides a theoretical foundation for the λ-return  it is based on three typically
false assumptions: the returns are highly correlated  only the Monte Carlo return is unbiased  and the
variance of the n-step returns from each state do not usually increase geometrically. This suggests
three areas where the λ-return might be improved—it could be modiﬁed to better account for the
correlation of returns  the bias of the different returns  and the true form of Var(R(n)
The γ-return uses an approximate formula for the variance of an n-step return in place of Assumption
3. This allows the γ-return to better account for how the variance of returns increases with their

st ).

1To be clear: there is a wealth of theoretical and empirical analyses of algorithms that use the λ-return.
Until recently there was not a derivation of the λ-return as the estimator of V (st) that optimizes some objective
(e.g.  maximizes log likelihood or minimizes expected squared error).

2

length  while simultaneously removing the need for the λ parameter. The γ-return is given by the
weighting scheme:

((cid:80)n
(cid:80)L
ˆn=1((cid:80)ˆn

i=1 γ2(i−1))−1

i=1 γ2(i−1))−1

.

wγ(n  L) =

3 The Ω-Return

st

and R(21)

We propose a new complex return  the Ω-return  that improves upon the λ and γ returns by account-
ing for the correlations of the returns. To emphasize this problem  notice that R(20)
st will
be almost identical (perfectly correlated) for many MDPs (particularly when γ is small). This means
that Assumption 1 is particularly egregious  and suggests that a new complex return might improve
upon the λ and γ-returns by properly accounting for the correlation of returns.
We formulate the problem of how best to combine different length returns to estimate the true ex-
pected return as a linear regression problem. This reformulation allows us to leverage the well-
understood properties of linear regression algorithms. Consider a regression problem with L points 
{(xi  yi)}L
i=1  where the value of yi depends on the value of xi. The goal is to predict yi given
xi. We set xi = 1 and yi = R(i)
st . We can then construct the design matrix (a vector in this case) 
(cid:124). We seek a re-
x = 1 = [1  . . .   1]
gression coefﬁcient  ˆβ ∈ R  such that y ≈ x ˆβ. This ˆβ will be our estimate of the true expected
return.
Generalized least squares (GLS) is a method for selecting ˆβ when the yi are not necessarily in-
dependent and may have different variances. Speciﬁcally  if we use a linear model with (possibly
correlated) mean-zero noise to model the data  i.e.  y = xβ +   where β ∈ R is unknown   is a
random vector  E[] = 0  and Var(|x) = Ω  then the GLS estimator

(cid:124) ∈ RL and the response vector  y = [R(1)

st   . . .   R(L)
st

st   R(2)

]

(cid:124)

Ω−1x)−1x

(cid:124)

Ω−1y 

ˆβ = (x

(2)
is the best linear unbiased estimator (BLUE) for β [6]—the linear unbiased estimator with the
lowest possible variance.
In our setting the assumptions about
that produced the data become that
[R(1)
+   where E[] = 0 (i.e.  the returns are
all unbiased estimates of the true expected return) and Var(|x) = Ω. Since x = 1 in our case 
Var(|x)(i  j) = Cov(R(i)
st )  where Var(|x)(i  j) de-
notes the element of Var(|x) in the ith row and jth column.
So  using only Assumption 2  GLS ((2)  solved for ˆβ) gives us the complex return:

(cid:124)
= [V (st)  V (st)  . . .   V (st)]
st − V (st)  R(j)

st − V (st)) = Cov(R(i)

the true model

st   . . .   R(L)
st

st   R(2)

st   R(j)

(cid:124)

]

[ 1
(cid:124)

ˆβ =

1

. . .

1 ] Ω−1

(cid:123)(cid:122)

(cid:80)L
n m=1 Ω−1(n m)

1

=



R(1)
st
R(2)
st...
R(L)
st



(cid:125)

 

1 ] Ω−1

(cid:123)(cid:122)

n m=1 Ω−1(n m)R(n)

st

 1

1

...

1

. . .

1

−1

[ 1

(cid:125)



(cid:80)L
(cid:80)L
m=1 Ω−1(n  m)
ˆn m=1 Ω−1(ˆn  m)

=(cid:80)L

(cid:124)

which can be written in the form of (1) with weights:

wΩ(n  L) =

 

(3)

where Ω is an L × L matrix with Ω(i  j) = Cov(R(i)
Notice that the Ω-return is a generalization of the λ and γ returns. The λ-return can be obtained
by reintroducing the false assumption that the returns are independent and that their variance grows
geometrically  i.e.  by making Ω a diagonal matrix with Ωn n = λ−n. Similarly  the γ-return can be

obtained by making Ω a diagonal matrix with Ωn n =(cid:80)n

st   R(j)
st ).

i=1 γ2(i−1).

3

st is a BLUE of V (st) if Assumption 2 holds. Since Assumption 2 does not hold  the
Notice that RΩ
Ω-return is not an unbiased estimator of V (s). Still  we expect it to outperform the λ and γ-returns
because it accounts for the correlation of n-step returns and they do not. However  in some cases
it may perform worse because it is still based on the false assumption that all of the returns are
unbiased estimators of V (st). Furthermore  given Assumption 2  there may be biased estimators of
V (st) that have lower expected mean squared error than a BLUE (which must be unbiased).

4 Approximating the Ω-Return

In practice the covariance matrix  Ω  is unknown and must be approximated from data. This ap-
proach  known as feasible generalized least squares (FGLS)  can perform worse than ordinary least
squares given insufﬁcient data to accurately estimate Ω. We must therefore accurately approximate
Ω from small amounts of data.
To study the accuracy of covariance matrix estimates  we estimated Ω using a large number of
trajectories for four different domains: a 5 × 5 gridworld  a variant of the canonical mountain car
domain  a real-world digital marketing problem  and a continuous control problem (DAS1)  all of
which are described in more detail in subsequent experiments. The covariance matrix estimates are
depicted in Figures 1(a)  2(a)  3(a)  and 4(a). We do not specify rows and columns in the ﬁgures
because all covariance matrices and estimates thereof are symmetric. Because they were computed
from a very large number of trajectories  we will treat them as ground truth.
We must estimate the Ω-return when only a few trajectories are available. Figures 1(b)  2(b)  3(b) 
and 4(b) show direct empirical estimates of the covariance matrices using only a few trajectories.
These empirical approximations are poor due to the very limited amount of data  except for the
digital marketing domain  where a “few” trajectories means 10 000. The solid black entries in
Figures 1(f)  2(f)  3(f)  and 4(f) show the weights  wΩ(n  L)  on different length returns when using
different estimates of Ω. The noise in the direct empirical estimate of the covariance matrix using
only a few trajectories leads to poor estimates of the return weights.
When approximating Ω from a small number of trajectories  we must be careful to avoid this over-
ﬁtting of the available data. One way to do this is to assume a compact parametric model for Ω.
Below we describe a parametric model of Ω that has only four parameters  regardless of L (which
determines the size of Ω). We use this parametric model in our experiments as a proof of concept—
we show that the Ω-return using even this simple estimate of Ω can produce improved results over
the other existing complex returns. We do not claim that this scheme for estimating Ω is particularly
principled or noteworthy.

4.1 Estimating Off-Diagonal Entries of Ω

Notice in Figures 1(a)  2(a)  3(a)  and 4(a) that for j > i  Cov(Ri
) =
st
Var(Ri
). This structure would mean that we can ﬁll in Ω given its diagonal values  leaving only L
st
parameters. We now explain why this relationship is reasonable in general  and not just an artifact
of our domains. We can write each entry in Ω as a recurrence relation:

  Ri
st

  Rj
st

st

) ≈ Cov(Ri

Cov[R(i)

st   R(j)

st ] =Cov[R(i)
=Cov[R(i)

st

st   R(j−1)
st   R(j−1)

st

+ γj−1(rt+j + γ ˆV (st+j) − ˆV (st+j−1)]
] + γj−1Cov[R(i)

st   rt+j + γ ˆV (st+j) − ˆV (st+j−1)] 

  Rj
st

when i < j. The term rt+j + γ ˆV (st+j) − ˆV (st+j−1) is the temporal difference error j steps
in the future. The proposed assumption that Cov(Ri
) is equivalent to as-
st
suming that the covariance of this temporal difference error and the i-step return is negligible:
st   rt+j + γ ˆV (st+j) − ˆV (st+j−1)] ≈ 0. The approximate independence of these two
γj−1Cov[R(i)
terms is reasonable in general due to the Markov property  which ensures that at least the conditional
covariance  Cov[R(i)
Because this relationship is not exact  the off-diagonal entries tend to grow as they get farther from
the diagonal. However  especially when some trajectories are padded with absorbing states  this
relationship is quite accurate when j = L  since the temporal difference errors at the absorbing state
st   R(L−1)
are all zero  and Cov[R(i)
]

st   0] = 0. This results in a signiﬁcant difference between Cov[R(i)

st   rt+j + γ ˆV (st+j) − ˆV (st+j−1)|st]  is zero.

) = Var(Ri
st

st

4

(a) Empirical Ω from 1
million trajectories.

(b) Empirical Ω from 5
trajectories.

(c) Approximate Ω from
1 million trajectories.

(d) Approximate Ω from
5 trajectories.

(e) Approximate and empiri-
cal diagonals of Ω.

(f) Approximate and empirical weights
for each return.

(g) Mean squared error from ﬁve trajec-
tories.

Figure 1: Gridworld Results.

(a) Empirical Ω from 1
million trajectories.

(b) Empirical Ω from 2
trajectories.

(c) Approximate Ω from
1 million trajectories.

(d) Approximate Ω from
2 trajectories.

(e) Approximate and empiri-
cal diagonals of Ω.

(f) Approximate and empirical weights
for each return.

(g) Mean squared error from two trajec-
tories.

Figure 2: Mountain Car Results.

5

51015200510152025051015202551015200510152025−10010203051015200510152025051015202551015200510152025010203001530020VarianceReturn LengthEmpirical 1MApprox 1MEmpirical 5Approx 5-0.21120Weight  w(n  20)Return Length  nEmpirical 1MApprox 1MEmpirical 5Approx 5λ=0.8  3.19055App Ω  1.953941101001 000Mean Squared ErrorReturn Type51015202530010203040−2002040608051015202530010203040−10001002005101520253001020304002040608051015202530010203040050100150080160030VarianceReturn LengthEmpirical 1MApprox 1MEmpirical 2Approx 2-0.21130Weight  w(n  11)Return Length  nEmpirical 1MApprox 1MEmpirical 2Approx 2WIS  144.48App Ω  76.39101001 00010 000ISWISλ=0λ=0.1λ=0.2λ=0.3λ=0.4λ=0.5λ=0.6λ=0.7λ=0.8λ=0.9λ=1γEmp ΩApp ΩMean Squared ErrorReturn Type(a) Empirical Ω from 1
million trajectories.

(b) Empirical Ω from
10000 trajectories.

(c) Approximate Ω from
1 million trajectories.

(d) Approximate Ω from
10000 trajectories.

(e) Approximate and empiri-
cal diagonals of Ω.

(f) Approximate and empirical weights
for each return.

(g) Mean squared error from 10000 tra-
jectories.

Figure 3: Digital Marketing Results.

(a) Empirical Ω from
10000 trajectories.

(b) Empirical Ω from 10
trajectories.

(c) Approximate Ω from
10000 trajectories.

(d) Approximate Ω from
10 trajectories.

(e) Approximate and empiri-
cal diagonals of Ω.

(f) Approximate and empirical weights
for each return.

(g) Mean squared error from 10 trajec-
tories.

Figure 4: Functional Electrical Stimulation Results.

6

123456789101234567891000.050.10.150.2123456789101234567891000.050.10.150.2123456789101234567891000.050.10.150.2123456789101234567891000.050.10.150.200.080.16010VarianceReturn LengthEmpirical 1MApprox 1MEmpirical 10kApprox 10k-0.51110Weight  w(n  10)Return Length  nEmpirical 1MApprox 1MEmpirical 10kApprox 10kλ=0  0.0011Emp Ω  0.0007App Ω  0.001100.0020.004ISWISλ=0λ=0.1λ=0.2λ=0.3λ=0.4λ=0.5λ=0.6λ=0.7λ=0.8λ=0.9λ=1γEmp ΩApp ΩMean Squared ErrorReturn Type5101520051015202501020304051015200510152025−100102030510152005101520250102030405101520051015202505101520250204001020VarianceReturn LengthEmpirical 10KApprox 10KEmpirical 10Approx 10-0.51120Weight  w(n  10)Return Length  nEmpirical 10KApprox 10KEmpirical 10Approx 10λ=0  3.2102λ=1  3.47436App Ω  3.1070110100ISWISλ=0λ=0.1λ=0.2λ=0.3λ=0.4λ=0.5λ=0.6λ=0.7λ=0.8λ=0.9λ=1γEmp ΩApp ΩMean Squared ErrorReturn Typest   R(L)
st

and Cov[R(i)
]. Rather than try to model this drop  which can inﬂuence the weights signiﬁ-
cantly  we reintroduce the assumption that the Monte Carlo return is independent of the other returns 
making the off-diagonal elements of the last row and column zero.

4.2 Estimating Diagonal Entries of Ω

The remaining question is how best to approximate the diagonal of Ω from a very small number
of trajectories. Consider the solid and dotted black curves in Figures 1(e)  2(e)  3(e)  and 4(e) 
which depict the diagonals of Ω when estimated from either a large number or small number of
trajectories. When using only a few trajectories  the diagonal includes ﬂuctuations that can have
signiﬁcant impacts on the resulting weights. However  when using many trajectories (which we
treat as giving ground truth)  the diagonal tends to be relatively smooth and monotonically increasing
until it plateaus (ignoring the ﬁnal entry).
This suggests using a smooth parametric form to approximate the diagonal  which we do as follows.
Let vi denote the sample variance of R(i)
st for i = 1 . . . L. Let v+ be the largest sample variance:
v+ = maxi∈{1 ... L} vi. We parameterize the diagonal using four parameters  k1  k2  v+  and vL:

k1

ˆΩk1 k2 v+ vL(i  i) =

vL
min{v+  k1k(1−t)

2

if i = 1
if i = L
otherwise.

}

Ω(1  1) = k1 sets the initial variance  and vL is the variance of the Monte Carlo return. The pa-
rameter v+ enforces a ceiling on the variance of the i-step return  and k2 captures the growth rate of
the variance  much like λ. We select the k1 and k2 that minimize the mean squared error between
ˆΩ(i  i) and vi  and set v+ and vL directly from the data.2
This reduces the problem of estimating Ω  an L × L matrix  to estimating four numbers from return
data. Consider Figures 1(c)  2(c)  3(c)  and 4(c)  which depict ˆΩ as computed from many trajectories.
The differences between these estimates and the ground truth show that this parameterization is not
perfect  as we cannot represent the true Ω exactly. However  the estimate is reasonable and the
resulting weights (solid red) are visually similar to the ground truth weights (solid black) in Figures
1(f)  2(f)  3(f)  and 4(f). We can now get accurate estimates of Ω from very few trajectories. Figures
1(d)  2(d)  3(d)  and 4(d) show ˆΩ when computed from only a few trajectories. Note their similarity
to ˆΩ when using a large number of trajectories  and that the resulting weights (unﬁlled red in Figures
1(f)  2(f)  3(f)  and 4(f)) are similar to the those obtained using many more trajectories (the ﬁlled red
bars).
Pseudocode for approximating the Ω-return is provided in Algorithm 1. Unlike the λ-return  which
can be computed from a single trajectory  the Ω-return requires a set of trajectories in order to
estimate Ω. The pseudocode assumes that every trajectory is of length L  which can be achieved by
padding shorter trajectories with absorbing states.

2We include the constraints that k2 ∈ [0  1] and 0 ≤ k1 ≤ v+.

7

Algorithm 1: Computing the Ω-return.
Require: n trajectories beginning at s and of length L.

for i = 1  . . .   L and for each trajectory.

1. Compute R(i)
s
2. Compute the sample variances  vi = Var(R(i)
3. Set v+ = maxi∈{1 ... L} vi.
4. Search for the k1 and k2 that minimize the mean squared error between vi and

s )  for i = 1  . . .   L.

5. Fill the diagonal of the L × L matrix  Ω  with Ω(i  i) = ˆΩk1 k2 v+ vL(i  i)  using the

6. Fill all of the other entries with Ω(i  j) = Ω(i  i) where j > i. If (i = L or j = L) and

ˆΩk1 k2 v+ vL (i  i) for i = 1  . . .   L.

optimized k1 and k2.
i (cid:54)= j then set Ω(i  j) = 0 instead.

7. Compute the weights for the returns according to (3).
8. Compute the Ω-return for each trajectory according to (1).

5 Experiments

Approximations of the Ω-return could  in principle  replace the λ-return in the whole family of
TD(λ) algorithms. However  using the Ω-return for TD(λ) raises several interesting questions that
are beyond the scope of this initial work (e.g.  is there a linear-time way to estimate the Ω-return?
Since a different Ω is needed for every state  how can the Ω-return be used with function approxi-
mation where most states will never be revisited?). We therefore focus on the speciﬁc problem of
off-policy policy evaluation—estimating the performance of a policy using trajectories generated by
a possibly different policy. This problem is of interest for applications that require the evaluation of
a proposed policy using historical data.
Due to space constraints  we relegate the details of our experiments to the appendix in the supple-
mental documents. However  the results of the experiments are clear—Figures 1(g)  2(g)  3(g)  and
4(g) show the mean squared error (MSE) of value estimates when using various methods.3 Notice
that  for all domains  using the Ω-return (the EMP Ω and APP Ω labels) results in lower MSE than
the γ-return and the λ-return with any setting of λ.

6 Conclusions

Recent work has begun to explore the statistical basis of complex estimates of return  and how we
might reformulate them to be more statistically efﬁcient [4]. We have proposed a return estimator
that improves upon the λ and γ-returns by accounting for the covariance of return estimates. Our
results show that understanding and exploiting the fact that in control settings—unlike in standard
supervised learning—observed samples are typically neither independent nor identically distributed 
can substantially improve data efﬁciency in an algorithm of signiﬁcant practical importance.
Many (largely positive) theoretical properties of the λ-return and TD(λ) have been discovered over
the past few decades. This line of research into other complex returns is still in its infancy  and
so there are many open questions. For example  can the Ω-return be improved upon by removing
Assumption 2 or by keeping Assumption 2 but using a biased estimator (not a BLUE)? Is there
a method for approximating the Ω-return that allows for value function approximation with the
same time complexity as TD(λ)  or which better leverages our knowledge that the environment is
Markovian? Would TD(λ) using the Ω-return be convergent in the same settings as TD(λ)? While
we hope to answer these questions in future work  it is also our hope that this work will inspire other
researchers to revisit the problem of constructing a statistically principled complex return.

3To compute the MSE we used a large number of Monte Carlo rollouts to estimate the true value of each

policy.

8

References
[1] R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning  3(1):9–44 

1988.

[2] S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine

Learning  22(1-3):33–57  March 1996.

[3] C. Downey and S. Sanner. Temporal difference Bayesian model averaging: A Bayesian perspective on
adapting lambda. In Proceedings of the 27th International Conference on Machine Learning  pages 311–
318  2010.

[4] G.D. Konidaris  S. Niekum  and P.S. Thomas. TDγ: Re-evaluating complex backups in temporal differ-

ence learning. In Advances in Neural Information Processing Systems 24  pages 2402–2410  2011.

[5] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  Cambridge  MA 

1998.

[6] T. Kariya and H. Kurata. Generalized Least Squares. Wiley  2004.
[7] D. Precup  R. S. Sutton  and S. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings

of the 17th International Conference on Machine Learning  pages 759–766  2000.

[8] A. R. Mahmood  H. Hasselt  and R. S. Sutton. Weighted importance sampling for off-policy learning with

linear function approximation. In Advances in Neural Information Processing Systems 27  2014.

[9] J. R. Tetreault and D. J. Litman. Comparing the utility of state features in spoken dialogue using rein-
forcement learning. In Proceedings of the Human Language Technology/North American Association for
Computational Linguistics  2006.

[10] G. D. Konidaris  S. Osentoski  and P. S. Thomas. Value function approximation in reinforcement learning
using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Artiﬁcial Intelligence  pages
380–395  2011.

[11] G. Theocharous and A. Hallak. Lifetime value marketing using reinforcement learning.

Multidisciplinary Conference on Reinforcement Learning and Decision Making  2013.

In The 1st

[12] P. S. Thomas  G. Theocharous  and M. Ghavamzadeh. High conﬁdence off-policy evaluation. In Pro-

ceedings of the Twenty-Ninth Conference on Artiﬁcial Intelligence  2015.

[13] D. Blana  R. F. Kirsch  and E. K. Chadwick. Combined feedforward and feedback control of a redundant 
nonlinear  dynamic musculoskeletal system. Medical and Biological Engineering and Computing  47:
533–542  2009.

[14] P. S. Thomas  M. S. Branicky  A. J. van den Bogert  and K. M. Jagodnik. Application of the actor-critic
architecture to functional electrical stimulation control of a human arm. In Proceedings of the Twenty-
First Innovative Applications of Artiﬁcial Intelligence  pages 165–172  2009.

[15] P. M. Pilarski  M. R. Dawson  T. Degris  F. Fahimi  J. P. Carey  and R. S. Sutton. Online human training
of a myoelectric prosthesis controller via actor-critic reinforcement learning. In Proceedings of the 2011
IEEE International Conference on Rehabilitation Robotics  pages 134–140  2011.

[16] K. Jagodnik and A. van den Bogert. A proportional derivative FES controller for planar arm movement.

In 12th Annual Conference International FES Society  Philadelphia  PA  2007.

[17] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolu-

tionary Computation  9(2):159–195  2001.

9

,Tianbao Yang
Rong Jin
Philip Thomas
Scott Niekum
Georgios Theocharous
George Konidaris
Malik Magdon-Ismail
Christos Boutsidis