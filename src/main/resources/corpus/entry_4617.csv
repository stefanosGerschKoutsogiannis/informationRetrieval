2008,Reducing statistical dependencies in natural signals using radial Gaussianization,We consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent. A widely studied linear solution  independent components analysis (ICA)  exists for the case when the signal is generated as a linear transformation of independent non- Gaussian sources. Here  we examine a complementary case  in which the source is non-Gaussian but elliptically symmetric. In this case  no linear transform suffices to properly decompose the signal into independent components  but we show that a simple nonlinear transformation  which we call radial Gaussianization (RG)  is able to remove all dependencies. We then demonstrate this methodology in the context of natural signal statistics. We first show that the joint distributions of bandpass filter responses  for both sound and images  are better described as elliptical than linearly transformed independent sources. Consistent with this  we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by PCA or ICA.,Reducing statistical dependencies in natural signals

using radial Gaussianization

Siwei Lyu

Computer Science Department
University at Albany  SUNY

Albany  NY 12222

lsw@cs.albany.edu

Abstract

Eero P. Simoncelli

Center for Neural Science

New York University
New York  NY 10003
eero@cns.nyu.edu

We consider the problem of transforming a signal to a representation in which
the components are statistically independent. When the signal is generated as a
linear transformation of independent Gaussian or non-Gaussian sources  the solu-
tion may be computed using a linear transformation (PCA or ICA  respectively).
Here  we consider a complementary case  in which the source is non-Gaussian
but elliptically symmetric. Such a source cannot be decomposed into indepen-
dent components using a linear transform  but we show that a simple nonlinear
transformation  which we call radial Gaussianization (RG)  is able to remove all
dependencies. We apply this methodology to natural signals  demonstrating that
the joint distributions of nearby bandpass ﬁlter responses  for both sounds and im-
ages  are closer to being elliptically symmetric than linearly transformed factorial
sources. Consistent with this  we demonstrate that the reduction in dependency
achieved by applying RG to either pairs or blocks of bandpass ﬁlter responses is
signiﬁcantly greater than that achieved by PCA or ICA.

1 Introduction

Signals may be manipulated  transmitted or stored more eﬃciently if they are transformed to a rep-
resentation in which there is no statistical redundancy between the individual components. In the
context of biological sensory systems  the eﬃcient coding hypothesis [1  2] proposes that the princi-
ple of reducing redundancies in natural signals can be used to explain various properties of biological
perceptual systems. Given a source model  the problem of deriving an appropriate transformation
to remove statistical dependencies  based on the statistics of observed samples  has been studied for
more than a century. The most well-known example is principal components analysis (PCA)  a lin-
ear transformation derived from the second-order signal statistics (i.e.  the covariance structure)  that
can fully eliminate dependencies for Gaussian sources. Over the past two decades  a more general
method  known as independent component analysis (ICA)  has been developed to handle the case
when the signal is sampled from a linearly transformed factorial source. ICA and related methods
have shown success in many applications  especially in deriving optimal representations for natural
signals [3  4  5  6].

Although PCA and ICA bases may be computed for nearly any source  they are only guaranteed to
eliminate dependencies when the assumed source model is correct. And even in cases where these
methodologies seems to produce an interesting solution  the components of the resulting represen-
tation may be far from independent. A case in point is that of natural images  for which derived ICA
transformations consist of localized oriented basis functions that appear similar to the receptive ﬁeld
descriptions of neurons in mammalian visual cortex [3  5  4]. Although dependency between the
responses of such linear basis functions is reduced compared to that of the original pixels  this reduc-

1

Linearly  
transformed 
factorial 

Elliptical

Factorial

Gaussian 

Spherical 

Fig. 1. Venn diagram of the relationship between density models. The two circles represent the linearly
transformed factorial densities as assumed by the ICA methods  and elliptically symmetric densities
(ESDs). The intersection of these two classes is the set of all Gaussian densities. The factorial densities
form a subset of the linearly transformed factorial densities and the spherically symmetric densities
form a subset of the ESDs.

tion is only slightly more than that achieved with PCA or other bandpass ﬁlters [7  8]. Furthermore 
the responses of ICA and related ﬁlters still exhibit striking higher-order dependencies [9  10  11].

Here  we consider the dependency elimination problem for the class of source models known as
elliptically symmetric densities (ESDs) [12]. For ESDs  linear transforms have no eﬀect on the
dependencies beyond second-order  and thus ICA decompositions oﬀer no advantage over PCA. We
introduce an alternative nonlinear procedure  which we call radial Gaussianization (RG). In RG 
the norms of whitened signal vectors are nonlinearly adjusted to ensure that the resulting output
density is a spherical Gaussian  whose components are statistically independent. We ﬁrst show that
the joint statistics of proximal bandpass ﬁlter responses for natural signals (sounds and images) are
better described as an ESD than linearly transformed factorial sources. Consistent with this  we
demonstrate that the reduction in dependency achieved by applying RG to such data is signiﬁcantly
greater than that achieved by PCA or ICA. A preliminary version of portions of this work was
described in [13].

2 Elliptically Symmetric Densities

The density of a random vector x ∈ Rd with zero mean is elliptically symmetric if it is of the form:

p(x) =

1

1
2

α|Σ|

f  −

1
2

xT Σ−1x!  

(1)

R ∞

where Σ is a positive deﬁnite matrix 

f (·) is the generating function satisfying f (·) ≥ 0 and
f (−r2/2) rd−1 dr < ∞  and the normalizing constant α is chosen so that the density integrates
0
to one [12]. The deﬁnitive characteristic of an ESD is that the level sets of constant probability are
ellipsoids determined by Σ. In the special case when Σ is a multiple of the identity matrix  the level
sets of p(x) are hyper-spheres and the density is known as a spherically symmetric density (SSD).
Assuming x has ﬁnite second-order statistics  Σ is a multiple of the covariance matrix  which implies
that any ESD can be transformed into an SSD by a PCA/whitening operation.

When the generating function is an exponential  the resulting ESD is a zero-mean multivariate Gaus-
sian with covariance matrix Σ. In this case  x can also be regarded as a linear transformation of a
vector s containing independent unit-variance Gaussian components  as: x = Σ−1/2s. In fact  the
Gaussian is the only density that is both elliptically symmetric and linearly decomposable into inde-
pendent components [14]. In other words  the Gaussian densities correspond to the intersection of
the class of ESDs and the class assumed by the ICA methods. As a special case  a spherical Gaussian
is the only spherically symmetric density that is also factorial (i.e.  has independent components).
These relationships are illustrated in a Venn diagram in Fig. 1.

Apart from the special case of Gaussian densities  a linear transformation such as PCA or ICA cannot
completely eliminate dependencies in the ESDs. In particular  PCA and whitening can transform
an ESD variable to a spherically symmetric variable  xwht  but the resulting density will not be
factorial unless it is Gaussian. And ICA would apply an additional rotation (i.e.  an orthogonal

2

(a)

(b)

rout

g(r)

(c)

pout(r)

(d)

(e)

pin(r)

rin

(f )

Fig. 2. Radial Gaussianization procedure for 2D data. (a e): 2D joint densities of a spherical Gaussian
and a non-Gaussian SSD  respectively. The plots are arranged such that a spherical Gaussian has equal-
spaced contours. (b f): radial marginal densities of the spherical Gaussian in (a) and the SSD in (e) 
respectively. Shaded regions correspond to shaded annuli in (a) and (e). (c): the nonlinear mapping
that transforms the radii of the source to those of the spherical Gaussian. (d): log marginal densities of
the Gaussian in (a) and the SSD in (e)  as red dashed line and green solid line  respectively.

matrix) to transform xwht to a new set of coordinates maximizing a higher-order contrast function
(e.g.  kurtosis). However  for spherically symmetric xwht  p(xwht) is invariant to rotation  and thus
unaﬀected by orthogonal transformations.

3 Radial Gaussianization

Given that linear transforms are ineﬀective in removing dependencies from a spherically symmetric
variable xwht (and hence the original ESD variable x)  we need to consider non-linear mappings. As
described previously  a spherical Gaussian is the only SSD with independent components. Thus  a
natural solution for eliminating the dependencies in a non-Gaussian spherically symmetric xwht is to
transform it to a spherical Gaussian.

Selecting such a non-linear mapping without any further constraint is a highly ill-posed problem.
It is natural to restrict to nonlinear mappings that act radially  preserving the spherical symme-
try. Speciﬁcally  one can show that the generating function of p(xwht) is completely determined
by its radial marginal distribution: pr(r) = rd−1
β f (−r2/2)  where r = kxwhtk  Γ(·) is the standard
Gamma function  and β is the normalizing constant that ensures that the density integrates to one.
In the special case of a spherical Gaussian of unit variance  the radial marginal is a chi-density
rd−1
2d/2−1Γ(d/2) exp(−r2/2). We deﬁne the radial Gaussianization
with d degrees of freedom: pχ(r) =
(RG) transformation as xrg = g(kxwhtk) xwht
kxwhtk   where nonlinear function g(·) is selected to map the
radial marginal density of xwht to the chi-density. Solving for a monotonic g(·) is a standard one-
dimensional density-mapping problem  and the unique solution is the composition of the inverse
cumulative density function (CDF) of pχ with the CDF of pr: g(r) = F −1
χ Fr(r). A illustration of
the procedure is provided in Fig. 2. In practice  we can estimate Fr(r) from a histogram computed
from training data  and use this to construct a numerical approximation (i.e.  a look-up table) of the
continuous function ˆg(r). Note that the accuracy of the estimated RG transformation will depend on
the number of data samples  but is independent of the dimensionality of the data vectors.

In summary  a non-Gaussian ESD signal can be radially Gaussianized by ﬁrst applying PCA and
whitening operations to remove second-order dependency (yielding an SSD)  followed by a nonlin-
ear transformation that maps the radial marginal to a chi-density.

4 Application to Natural Signals

An understanding of the statistical behaviors of source signals is beneﬁcial for many problems in
signal processing  and can also provide insights into the design and functionality of biological sen-
sory systems. Gaussian signal models are widely used  because they are easily characterized and
often lead to clean and eﬃcient solutions. But many naturally occurring signals exhibit striking

3

non-Gaussian statistics  and much recent literature focuses on the problem of characterizing and
exploiting these behaviors. Speciﬁcally  ICA methodologies have been used to derive linear repre-
sentations for natural sound and image signals whose coeﬃcients are maximally sparse or indepen-
dent [3  5  6]. These analyses generally produced basis sets containing bandpass ﬁlters resembling
those used to model the early transformations of biological auditory and visual systems.

Despite the success of ICA methods in providing a fundamental motivation for sensory receptive
ﬁelds  there are a number of simple observations that indicate inconsistencies in this interpreta-
tion. First  the responses of ICA or other bandpass ﬁlters exhibit striking dependencies  in which
the variance of one ﬁlter response can be predicted from the amplitude of another nearby ﬁlter re-
sponse [10  15]. This suggests that although the marginal density of the bandpass ﬁlter responses are
heavy-tailed  their joint density is not consistent with the linearly transformed factorial source model
assumed by ICA. Furthermore  the marginal distributions of a wide variety of bandpass ﬁlters (even
a “ﬁlter” with randomly selected zero-mean weights) are all highly kurtotic [7]. This would not be
expected for the ICA source model: projecting the local data onto a random direction should result
in a density that becomes more Gaussian as the neighborhood size increases  in accordance with a
generalized version of the central limit theorem [16]. A recent quantitative study [8] further showed
that the oriented bandpass ﬁlters obtained through ICA optimization on images lead to a surprisingly
small improvement in reducing dependency relative to decorrelation methods such as PCA. Taken
together  all of these observations suggest that the ﬁlters obtained through ICA optimization repre-
sent a “shallow” optimum  and are perhaps not as uniquely suited for image or sound representation
as initially believed. Consistent with this  recently developed models for local image statistics model
local groups of image bandpass ﬁlter responses with non-Gaussian ESDs [e.g.  17  18  11  19  20].
These all suggest that RG might provide an appropriate means of eliminating dependencies in natu-
ral signals. Below  we test this empirically.

4.1 Dependency Reduction in Natural Sounds

We ﬁrst apply RG to natural sounds. We used sound clips from commercial CDs  which have a
sampling frequency of 44100 Hz and typical length of 15 − 20 seconds  and contents including
animal vocalization and recordings in natural environments. These sound clips were ﬁltered with a
bandpass gammatone ﬁlter  which are commonly used to model the peripheral auditory system [21].
In our experiments  analysis was based on a ﬁlter with center frequency of 3078 Hz.

Shown in the top row of column (a) in Fig.3 are contour plots of the joint histograms obtained
from pairs of coeﬃcients of a bandpass-ﬁltered natural sound  separated with diﬀerent time inter-
vals. Similar to the empirical observations for natural images [17  11]  the joint densities are non-
Gaussian  and have roughly elliptically symmetric contours for temporally proximal pairs. Shown
in the top row of column (b) in Fig.3 are the conditional histograms corresponding to the same pair
of signals. The “bow-tie” shaped conditional distribution  which has been also observed in natural
images [10  11  15]  indicates that the conditional variance of one signal depends on the value of the
other. This is a highly non-Gaussian behavior  since the conditional variances of a jointly Gaussian
density are always constant  independent of the value of the conditioning variable. For pairs that
are distant  both the second-order correlation and the higher-order dependency become weaker. As
a result  the corresponding joint histograms show more resemblance to the factorial product of two
one-dimensional super-Gaussian densities (bottom row of column (a) in Fig.3)  and the shape of the
corresponding conditional histograms (column (b)) is more constant  all as would be expected for
two independent random variables .

As described in previous sections  the statistical dependencies in an elliptically symmetric random
variable can be eﬀectively removed by a linear whitening operation followed by a nonlinear radial
Gaussianization  the latter being implemented as histogram transform of the radial marginal den-
sity of the whitened signal. Shown in columns (c) and (d) in Fig.3 are the joint and conditional
histograms of the transformed data. First  note that when the two signals are nearby  RG is highly
eﬀective  as suggested by the roughly Gaussian joint density (equally spaced circular contours)  and
by the consistent vertical cross-sections of the conditional histogram. However  as the temporal sep-
aration between the two signals increases  the eﬀects of RG become weaker (middle row  Fig. 3).
When the two signals are distant (bottom row  Fig.3)  they are nearly independent  and applying RG
can actually increase dependency  as suggested by the irregular shape of the conditional densities
(bottom row  column (d)).

4

(a)

(b)

(c)

(d)

0.1 msec
(4 samples)

1.5 msec
(63 samples)

3.5 msec
(154 samples)

Fig. 3. Radial Gaussianization of natural sounds.
(a): Contour plots of joint histograms of pairs
of band-pass ﬁlter responses of a natural sound clip. Each row corresponds to pairs with diﬀerent
temporal separation  and levels are chosen so that a spherical Gaussian density will have equally spaced
contours. (c) Joint histograms after whitening and RG transformation. (b d): Conditional histograms
of the same data shown in (a c)  computed by independently normalizing each column of the joint
histogram. Histogram intensities are proportional to probability  except that each column of pixels is
independently rescaled so that the largest probability value is displayed as white.

To quantify more precisely the dependency reduction achieved by RG  we measure the statistical
dependency of our multivariate sources using the multi-information (MI) [22]  which is deﬁned as
the Kulback-Leibler divergence [23] between the joint distribution and the product of its marginals:

I(x) = DKL(cid:0)p(x) kQk p(xk)(cid:1) = Pd

k=1 H(xk) − H(x)  where H(x) = R p(x) log (p(x)) dx is the dif-
ferential entropy of x  and H(xk) denotes the diﬀerential entropy of the kth component of x. As
a measure of statistical dependency among the elements of x  MI is non-negative  and is zero if
and only if the components of x are mutually independent. Furthermore  MI is invariant to any
transformation on individual components of x (e.g.  element-wise rescaling).

To compare the eﬀect of diﬀerent dependency reduction methods  we estimated the MI of pairs of
bandpass ﬁlter responses with diﬀerent temporal separations. This is achieved with a non-parametric
“bin-less” method based on the order statistics [24]  which alleviates the strong bias and variance
intrinsic to the more traditional binning (i.e.  “plug-in”) estimators. It is especially eﬀective in this
case  where the data dimensionality is two. We computed the MI for each pair of raw signals  as well
as pairs of the PCA  ICA and RG transformed signals. The ICA transformation was obtained using
RADICAL [25]  an algorithm that directly optimizes the MI using a smoothed grid search over a
non-parametric estimate of entropy.

The results  averaged over all 10 sounds  are plotted in Fig. 4. First  we note that PCA produces a
relatively modest reduction in MI: roughly 20% for small separations  decreasing gradually as the
separation increase. We also see that ICA oﬀers very little additional reduction over PCA for small
separations. In contrast  the nonlinear RG transformation achieves an impressive reduction (nearly
100%) in MI for pairs separated by less than 0.5 msec. This can be understood by considering the
joint and conditional histograms in Fig. 3. Since the joint density of nearby pairs is approximately
elliptically symmetric  ICA cannot provide much improvement beyond what is obtained with PCA 
while RG is expected to perform well. On the other hand  the joint densities of more distant pairs
(beyond 2.5 msec) are roughly factorial  as seen in the bottom row of Fig. 3. In this case  neither
PCA nor ICA is eﬀective in further reducing dependency  as is seen in the plots of Fig. 4  but RG
makes the pairs more dependent  as indicated by an increase in MI above that of the original pairs
for separation over 2.5 msec. This is a direct result of the fact that the data do not adhere to the
elliptically symmetric source model assumptions underlying the RG procedure. For intermediate
separations (0.2 to 2 msec)  there is a transition of the joint densities from elliptically symmetric
to factorial (second row in Fig. 3)  and ICA is seen to oﬀer a modest improvement over PCA. We

5

 

0.5

raw
pca/ica
rg

)
f
f

e
o
c
/
s
t
i

b
(
 
I

M

0.4

0.3

0.2

0.1

0

 
1

 

raw
pca/ica
rg

2

4

8

16

32

separation (samples)

0.5

0.4

0.3

0.2

0.1

)
f
f

e
o
c
/
s
t
i

b
(
 
I

M

0
 
0.1

0.5

1
separation (msec)

1.5 2 2.5 3.5

Fig. 4. Left: Multi-information (in bits/coeﬃcient) for pairs of bandpass ﬁlter responses of natural
audio signals  as a function of temporal separation. Shown are the MI of the raw ﬁlter response pairs 
as well as the MI of the pairs transformed with PCA  ICA  and RG. Results are averaged over 10
natural sound signals. Right: Same analysis for pairs of bandpass ﬁlter responses averaged over 8
natural images.

blk size = 3x3

0.7

0.6

0.5

0.4

0.3

0.2

blk size = 7x7

1

0.9

0.8

0.7

0.6

0.5

0.4

blk size = 15x15

1.3

1.2

1.1

1

0.9

0.8

0.7

0.6

0.2

0.3

0.4
3 × 3

0.5

0.6

0.4

0.5

0.7

0.8

0.9

0.6

0.7

0.6
7 × 7

0.8
0.9
15 × 15

1

1.1

Fig. 5. Reduction of MI (bits/pixel) achieved with ICA and RG transforms  compared to that achieved
with PCA  for pixel blocks of various sizes. The x-axis corresponds to ∆Ipca. Pluses denotes ∆Irg  and
circles denotes ∆Iica. Each plotted symbol corresponds to the result from one image in our test set.

found qualitatively similar behaviors (right column in Fig. 4) when analyzing pairs of bandpass ﬁlter
responses of natural images using the data sets described in the next section.

4.2 Dependency Reduction in Natural Images

We have also examined the ability of RG to reduce dependencies of image pixel blocks with lo-
cal mean removed. We examined eight images of natural woodland scenes from the van Hateren
database [26]. We extracted the central 1024 × 1024 region from each  computed the log of the in-
tensity values  and then subtracted the local mean [8] by convolving with an isotropic bandpass ﬁlter
that captures an annulus of frequencies in the Fourier domain ranging from π/4 to π radians/pixel.
We denote blocks taken from these bandpass ﬁltered images as xraw. These blocks were then trans-
formed with PCA (denoted xpca)  ICA (denoted xica) and RG (denoted xrg). These block data are
of signiﬁcantly higher dimension than the ﬁlter response pairs examined in the previous section.
For this reason  we switched our ICA computations from RADICAL to the more eﬃcient FastICA
algorithm [27]  with a contrast function g(u) = 1 − exp(−u2) and using the symmetric approach for
optimization.

We would like to compare the dependency reduction performance of each of these methods using
multi-information. However  direct estimation of MI becomes diﬃcult and less accurate with higher
data dimensionality. Instead  as in [8]  we can avoid direct estimation of MI by evaluating and
comparing the diﬀerences in MI of the various transformed blocks relative to xraw. Speciﬁcally  we
use ∆Ipca = I(xraw) − I(xpca) as a reference value  and compare this with ∆Iica = I(xraw) − I(xica) and
∆Irg = I(xraw) − I(xrg). Full details of this computation are described in [13].

6

Shown in Fig.5 are scatter plots of ∆Ipca versus ∆Iica (red circles) and ∆Irg (blue pluses) for various
block sizes. Each point corresponds to MI computation over blocks from one of eight bandpass-
ﬁltered test images. As the ﬁgure shows  RG achieves signiﬁcant reduction in MI for most images 
and this holds over a range of block sizes  whereas ICA shows only a very small improvement
relative to PCA1. We again conclude that ICA does not oﬀer much advantage over second-order
decorrelation algorithms such as PCA  while RG oﬀers signiﬁcant improvements. These results may
be attributed to the fact that the joint density for local pixel blocks tend to be close to be elliptically
symmetric [17  11].

5 Conclusion

We have introduced a new signal transformation known as radial Gaussianization (RG)  which can
eliminate dependencies of sources with elliptically symmetric densities. Empirically  we have shown
that RG transform is highly eﬀective at removing dependencies between pairs of samples in band-
pass ﬁltered sounds and images  and within local blocks of bandpass ﬁltered images.

One important issue underlying our development of this methodology is the intimate relation be-
tween source models and dependency reduction methods. The class of elliptically symmetric densi-
ties represents a generalization of the Gaussian family that is complementary to the class of linearly
transformed factorial densities (see Fig. 1). The three dependency reduction methods we have dis-
cussed (PCA  ICA and RG) are each associated with one of these classes  and are each guaranteed
to produce independent responses when applied to signals drawn from a density belonging to the
corresponding class. But applying one of these methods to a signal with an incompatible source
model may not achieve the expected reduction in dependency (e.g.  applying ICA to an ESD)  and
in some cases can even increase dependencies (e.g.  applying RG to a factorial density).

Several recently published methods are related to RG. An iterative Gaussianization scheme trans-
forms any source model to a spherical Gaussian by alternating between linear ICA transformations
and nonlinear histogram matching to map marginal densities to Gaussians [28]. However  in gen-
eral  the overall transformation of iterative Gaussianization is an alternating concatenation of many
linear/nonlinear transformations  and results in a substantial distortion of the original source space.
For the special case of ESDs  RG provides a simple one-step procedure with minimal distortion.
Another nonlinear transform that has also been shown to be able to reduce higher-order dependen-
cies in natural signals is divisive normalization [15]. In the extended version of this paper [13]  we
show that there is no ESD source model for whose dependencies can be completely eliminated by
divisive normalization. On the other hand  divisive normalization provides a rough approximation
to RG  which suggests that RG might provide a more principled justiﬁcation for normalization-like
nonlinear behaviors seen in biological sensory systems.

There are a number of extensions of RG that are worth considering in the context of signal repre-
sentation. First  we are interested in speciﬁc sub-families of ESD for which the nonlinear mapping
of signal amplitudes in RG may be expressed in closed form. Second  the RG methodology pro-
vides a solution to the eﬃcient coding problem for ESD signals in the noise-free case  and it is
worthwhile to consider how the solution would be aﬀected by the presence of sensor and/or chan-
nel noise. Third  we have shown that RG substantially reduces dependency for nearby samples of
bandpass ﬁltered image/sound  but that performance worsens as the coeﬃcients become more sep-
arated  where their joint densities are closer to factorial. Recent models of natural images [29  30]
have used Markov random ﬁelds based on local elliptically symmetric models  and these are seen to
provide a natural transition of pairwise joint densities from elliptically symmetric to factorial. We
are currently exploring extensions of the RG methodology to such global models. And ﬁnally  we
are currently examining the statistics of signals after local RG transformations  with the expectation
that remaining statistical regularities (e.g.  orientation and phase dependencies in images) can be
studied  modeled and removed with additional transformations.

References
[1] F Attneave. Some informational aspects of visual perception. Psych. Rev.  61:183–193  1954.

1Similar results for the comparison of ICA to PCA were obtained with a slightly diﬀerent method of remov-

ing the mean values of each block [8].

7

[2] H B Barlow. Possible principles underlying the transformation of sensory messages. In W A Rosenblith 

editor  Sensory Communication  pages 217–234. MIT Press  Cambridge  MA  1961.

[3] B A Olshausen and D J Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607–609  1996.

[4] A van der Schaaf and J H van Hateren. Modelling the power spectra of natural images: Statistics and

information. Vision Research  28(17):2759–2770  1996.

[5] A J Bell and T J Sejnowski. The ’independent components’ of natural scenes are edge ﬁlters. Vision

Research  37(23):3327–3338  1997.

[6] M S Lewicki. Eﬃcient coding of natural sounds. Nature Neuroscience  5(4):356–363  2002.
[7] R. Baddeley. Searching for ﬁlters with “interesting” output distributions: an uninteresting direction to

explore. Network  7:409–421  1996.

[8] Matthias Bethge. Factorial coding of natural images: how eﬀective are linear models in removing higher-

order dependencies? J. Opt. Soc. Am. A  23(6):1253–1268  2006.

[9] B Wegmann and C Zetzsche. Statistical dependence between orientation ﬁlter outputs used in an human
vision based image code. In Proc Visual Comm. and Image Processing  volume 1360  pages 909–922 
Lausanne  Switzerland  1990.

[10] E P Simoncelli. Statistical models for images: Compression  restoration and synthesis. In Proc 31st Asilo-
mar Conf on Signals  Systems and Computers  volume 1  pages 673–678  Paciﬁc Grove  CA  November
2-5 1997. IEEE Computer Society.

[11] M J Wainwright and E P Simoncelli. Scale mixtures of Gaussians and the statistics of natural im-
ages. In S. A. Solla  T. K. Leen  and K.-R. M¨uller  editors  Adv. Neural Information Processing Systems
(NIPS*99)  volume 12  pages 855–861  Cambridge  MA  May 2000. MIT Press.

[12] K.T. Fang  S. Kotz  and K.W. Ng. Symmetric Multivariate and Related Distributions. Chapman and Hall 

London  1990.

[13] S. Lyu and E. P. Simoncelli. Nonlinear extraction of “independent components” of elliptically symmet-
ric densities using radial Gaussianization. Technical Report TR2008-911  Computer Science Technical
Report  Courant Inst. of Mathematical Sciences  New York University  April 2008.

[14] D. Nash and M. S. Klamkin. A spherical characterization of the normal distribution. Journal of Multi-

variate Analysis  55:56–158  1976.

[15] O Schwartz and E P Simoncelli. Natural signal statistics and sensory gain control. Nature Neuroscience 

4(8):819–825  August 2001.

[16] William Feller. An Introduction to Probability Theory and Its Applications  volume 1. Wiley  January

1968.

[17] C Zetzsche and G Krieger. The atoms of vision: Cartesian or polar? J. Opt. Soc. Am. A  16(7)  July 1999.
[18] J. Huang and D. Mumford. Statistics of natural images and models. In IEEE International Conference on

Computer Vision and Pattern Recognition (CVPR)  1999.

[19] A Srivastava  X Liu  and U Grenander. Universal analytical forms for modeling image probability. IEEE

Pat. Anal. Mach. Intell.  24(9):1200–1214  Sep 2002.

[20] Y. Teh  M. Welling  and S. Osindero. Energy-based models for sparse overcomplete representations.

Journal of Machine Learning Research  4:1235–1260  2003.

[21] P I M Johannesma. The pre-response stimulus ensemble of neurons in the cochlear nucleus. In Symposium

on Hearing Theory (IPO)  pages 58–69  Eindhoven  Holland  1972.

[22] M. Studeny and J. Vejnarova. The multiinformation function as a tool for measuring stochastic depen-
dence. In M. I. Jordan  editor  Learning in Graphical Models  pages 261–297. Dordrecht: Kluwer.  1998.

[23] T. Cover and J. Thomas. Elements of Information Theory. Wiley-Interscience  2nd edition  2006.
[24] A. Kraskov  H. St¨ogbauer  and P. Grassberger. Estimating mutual information. Phys. Rev. E  69(6):66–82 

Jun 2004.

[25] E. G. Learned-Miller and J. W. Fisher. ICA using spacings estimates of entropy. Journal of Machine

Learning Research  4(1):1271–1295  2000.

[26] J H van Hateren and A van der Schaaf. Independent component ﬁlters of natural images compared with

simple cells in primary visual cortex. Proc. R. Soc. Lond. B  265:359–366  1998.

[27] A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE Trans-

actions on Neural Networks  10(3):626–634  1999.

[28] Scott Saobing Chen and Ramesh A. Gopinath. Gaussianization.

Systems (NIPS)  pages 423–429  2000.

In Advances in Neural Computation

[29] S. Roth and M. Black. Fields of experts: A framework for learning image priors. In IEEE Conference on

Computer Vision and Patten Recognition (CVPR)  volume 2  pages 860–867  2005.

[30] S Lyu and E P Simoncelli. Statistical modeling of images with ﬁelds of Gaussian scale mixtures.

In
B Sch¨olkopf  J Platt  and T Hofmann  editors  Adv. Neural Information Processing Systems 19  volume 19 
Cambridge  MA  May 2007. MIT Press.

8

,Han Liu
Lie Wang
Tuo Zhao