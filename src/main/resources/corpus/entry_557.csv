2016,Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm,In topic modeling  many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e.  words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain  however  and the identification of topics under those models hinges on uncorrelatedness of the topics  which can be unrealistic. This paper revisits topic modeling based on second-order moments  and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption  thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence  similarity count  and clustering accuracy metrics) compared to the prior art.,Anchor-Free Correlated Topic Modeling:

Identiﬁability and Algorithm

Kejun Huang∗

Xiao Fu∗

Department of Electrical and Computer Engineering

Nicholas D. Sidiropoulos

University of Minnesota

Minneapolis  MN 55455  USA

huang663@umn.edu

xfu@umn.edu

nikos@ece.umn.edu

Abstract

In topic modeling  many algorithms that guarantee identiﬁability of the topics have
been developed under the premise that there exist anchor words – i.e.  words that
only appear (with positive probability) in one topic. Follow-up work has resorted
to three or higher-order statistics of the data corpus to relax the anchor word
assumption. Reliable estimates of higher-order statistics are hard to obtain  however 
and the identiﬁcation of topics under those models hinges on uncorrelatedness of
the topics  which can be unrealistic. This paper revisits topic modeling based on
second-order moments  and proposes an anchor-free topic mining framework. The
proposed approach guarantees the identiﬁcation of the topics under a much milder
condition compared to the anchor-word assumption  thereby exhibiting much
better robustness in practice. The associated algorithm only involves one eigen-
decomposition and a few small linear programs. This makes it easy to implement
and scale up to very large problem instances. Experiments using the TDT2 and
Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits
very favorable performance (measured using coherence  similarity count  and
clustering accuracy metrics) compared to the prior art.

Introduction

1
Given a large collection of text data  e.g.  documents  tweets  or Facebook posts  a natural question is
what are the prominent topics in these data. Mining topics from a text corpus is motivated by a number
of applications  from commercial design  news recommendation  document classiﬁcation  content
summarization  and information retrieval  to national security. Topic mining  or topic modeling  has
attracted signiﬁcant attention in the broader machine learning and data mining community [1].
In 2003  Blei et al. proposed a Latent Dirichlet Allocation (LDA) model for topic mining [2]  where
the topics are modeled as probability mass functions (PMFs) over a vocabulary and each document
is a mixture of the PMFs. Therefore  a word-document text data corpus can be viewed as a matrix
factorization model. Under this model  posterior inference-based methods and approximations were
proposed [2  3]  but identiﬁability issues – i.e.  whether the matrix factors are unique – were not
considered. Identiﬁability  however  is essential for topic modeling since it prevents the mixing of
topics that confounds interpretation.
In recent years  considerable effort has been invested in designing identiﬁable models and estimation
criteria as well as polynomial time solvable algorithms for topic modeling [4  5  6  7  8  9  10  11].
Essentially  these algorithms are based on the so-called separable nonnegative matrix factorization
(NMF) model [12]. The key assumption is that every topic has an ‘anchor word’ that only appears
in that particular topic. Based on this assumption  two classes of algorithms are usually employed 
namely linear programming based methods [5  7] and greedy pursuit approaches [11  6  8  10]. The

∗These authors contributed equally.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

former class has a serious complexity issue  as it lifts the number of variables to the square of the size
of vocabulary (or documents); the latter  although computationally very efﬁcient  usually suffers from
error propagation  if at some point one anchor word is incorrectly identiﬁed. Furthermore  since all
the anchor word-based approaches essentially convert topic identiﬁcation to the problem of seeking
the vertices of a simplex  most of the above algorithms require normalizing each data column (or row)
by its (cid:96)1 norm. However  normalization at the factorization stage is usually not desired  since it may
destroy the good conditioning of the data matrix brought by pre-processing and amplify noise [8].
Unlike many NMF-based methods that work directly with the word-document data  the approach
proposed by Arora et al. [9  10] works with the pairwise word-word correlation matrix  which has
the advantage of suppressing sampling noise and also features better scalability. However  [9  10]
did not relax the anchor-word assumption or the need for normalization  and did not explore the
symmetric structure of the co-occurrence matrix – i.e.  the algorithms in [9  10] are essentially the
same asymmetric separable NMF algorithms as in [4  6  8].
The anchor-word assumption is reasonable in some cases  but using models without it is more
appealing in more critical scenarios  e.g.  when some topics are closely related and many key words
overlap. Identiﬁable models without anchor words have been considered in the literature; e.g. 
[13  14  15] make use of third or higher-order statistics of the data corpus to formulate the topic
modeling problem as a tensor factorization problem. There are two major drawbacks with this
approach: i) third- or higher-order statistics require a lot more samples for reliable estimation relative
to their lower-order counterparts (e.g.  second-order word correlation statistics); and ii) identiﬁability
is guaranteed only when the topics are uncorrelated – where a super-symmetric parallel factor analysis
(PARAFAC) model can be obtained [13  14]. Uncorrelatedness is a restrictive assumption [10]. When
the topics are correlated  the model becomes a Tucker model which is not identiﬁable in general;
identiﬁability needs more assumptions  e.g.  sparsity of topic PMFs [15].
Contributions.
In this work  our interest lies in topic mining using word-word correlation matrices
like in [9  10]  because of its potential scalability and noise robustness. We propose an anchor-free
identiﬁable model and a practically implementable companion algorithm. Our contributions are two-
fold: First  we propose an anchor-free topic identiﬁcation criterion. The criterion aims at factoring
the word-word correlation matrix using a word-topic PMF matrix and a topic-topic correlation matrix
via minimizing the determinant of the topic-topic correlation matrix. We show that under a so-called
sufﬁciently scattered condition  which is much milder than the anchor-word assumption  the two
matrices can be uniquely identiﬁed by the proposed criterion. We emphasize that the proposed
approach does not need to resort to higher-order statistics tensors to ensure topic identiﬁability  and
it can naturally deal with correlated topics  unlike what was previously available in topic modeling 
to the best of our knowledge. Second  we propose a simple procedure for handling the proposed
criterion that only involves eigen-decomposition of a large but sparse matrix  plus a few small linear
programs – therefore highly scalable and well-suited for topic mining. Unlike greedy pursuit-based
algorithms  the proposed algorithm does not involve deﬂation and is thus free from error propagation;
it also does not require normalization of the data columns / rows. Carefully designed experiments
using the TDT2 and Reuters text corpora showcase the effectiveness of the proposed approach.
2 Background
Consider a document corpus D ∈ RV ×D  where each column of D corresponds to a document and
D(v  d) denotes a certain measurement of word v in document d  e.g.  the word-frequency of term v
in document d or the term frequency–inverse document frequency (tf-idf) measurement that is often
used in topic mining. A commonly used model is
(1)
where C ∈ RV ×F is the word-topic matrix  whose f-th column C(:  f ) represents the probability
mass function (PMF) of topic f over a vocabulary of words  and W (f  d) denotes the weight of topic
f in document d [2  13  10]. Since matrix C and W are both nonnegative  (1) becomes a nonnegative
matrix factorization (NMF) model – and many early works tried to use NMF and variants to deal with
this problem [16]. However  NMF does not admit a unique solution in general  unless both C and W
satisfy some sparsity-related conditions [17]. In recent years  much effort has been put in devising
polynomial time solvable algorithms for NMF models that admit unique factorization. Such models
and algorithms usually rely on an assumption called “separability” in the NMF literature [12]:
Assumption 1 (Separability / Anchor-Word Assumption) There exists a set of indices Λ =
{v1  . . .   vF} such that C(Λ  :) = Diag(c)  where c ∈ RF .

D ≈ CW  

2

In topic modeling  it turns out that the separability condition has a nice physical interpretation  i.e. 
every topic f for f = 1  . . .   F has a ‘special’ word that has nonzero probability of appearing in topic
f and zero probability of appearing in other topics. These words are called ‘anchor words’ in the
topic modeling literature. Under Assumption 1  the task of matrix factorization boils down to ﬁnding
these anchor words v1  . . .   vF since D(Λ  :) = Diag(c)W — which is already a scaled version of
W — and then C can be estimated via (constrained) least squares.

: D; F .

ˆvf ← arg maxv∈{1 ... V } (cid:107)X(:  v)(cid:107)2;
Λ ← [Λ  ˆvf ];
Θ ← arg minΘ (cid:107)X − X(:  Λ)Θ(cid:107)2
F ;
X ← X − X(:  Λ)Θ;

end
output : Λ

Algorithm 1:
Successive Projection Algorithm [6]
input
Σ = 1T DT
X = DT Σ−1 (normalization);
Λ = ∅;
for f = 1  . . .   F do

Many algorithms have been proposed to tackle this index-
picking problem in the context of separable NMF  hyper-
spectral unmixing  and text mining. The arguably simplest
algorithm is the so-called successive projection algorithm
(SPA) [6] that is presented in Algorithm 1. SPA-like algo-
rithms ﬁrst deﬁne a normalized matrix X = DT Σ−1 where
Σ = Diag(1T DT ) [11]. Note that X = GS where G(:
  f ) = W T(f :)/(cid:107)W (f :)(cid:107)1 and S(f  v) = C(v f )(cid:107)W (f :)(cid:107)1
.
(cid:107)C(v :)(cid:107)1(cid:107)D(v :)(cid:107)1
Consequently  we have 1T S = 1T if W ≥ 0  meaning the
columns of X all lie on the simplex spanned by the columns
of G  and the vertices of the simplex correspond to the anchor
words. Also  the columns of S all live in the unit simplex.
After normalization  SPA sequentially identiﬁes the vertices of the data simplex  in conjunction with
a deﬂation procedure. The algorithms in [8  10  11] can also be considered variants of SPA  with
different deﬂation procedures and pre-/post-processing. In particular  the algorithm in [8] avoids
normalization — for real-word data  normalization at the factorization stage may amplify noise
and damage the good conditioning of the data matrix brought by pre-processing  e.g.  the tf-idf
procedure [8]. To pick out vertices  there are also algorithms using linear programming and sparse
optimization [7  5]  but these have serious scalability issues and thus are less appealing.
In practice D may contain considerable noise  and this has been noted in the literature. In [9  10 
14  15]  the authors proposed to use second and higher-order statistics for topic mining. Particularly 
Arora et al. [9  10] proposed to work with the following matrix:
P = E{DDT} = CEC T  

(2)
where E = E{W W T} can be interpreted as a topic-topic correlation matrix. The matrix P is by
deﬁnition a word-word correlation matrix  but also has a nice interpretation: if D(v  d) denotes the
frequency of word v occurring in document d  P (i  j) is the likelihood that term i and j co-occur
in a document [9  10]. There are two advantages in using P : i) if there is zero-mean white noise  it
will be signiﬁcantly suppressed through the averaging process; and ii) the size of P does not grow
with the size of the data if the vocabulary is ﬁxed. The latter is a desired property when the number
of documents is very large  and we pick a (possibly limited but) manageable vocabulary to work
with. Problems with similar structure to that of P also arise in the context of graph models  where
communities and correlations appear as the underlying factors. The algorithm proposed in [10] also
makes use of Assumption 1 and is conceptually close to Algorithm 1. The work in [13  14  15]
relaxed the anchor-word assumption. The methods there make use of three or higher-order statistics 
e.g.  P ∈ RV ×V ×V whose (i  j  k)th entry represents the co-occurrence of three terms. The work in
[13  14] showed that P is a tensor satisfying the parallel factor analysis (PARAFAC) model and thus
C is uniquely identiﬁable  if the topics are uncorrelated  which is a restrictive assumption (a counter
example would be politics and economy). When the topics are correlated  additional assumptions like
sparsity are needed to restore identiﬁability [15]. Another important concern is that reliable estimates
of higher-order statistics require much larger data sizes  and tensor decomposition is computationally
cumbersome as well.
Remark 1 Among all the aforementioned methods  the deﬂation-based methods are seemingly more
efﬁcient. However  if the deﬂation procedure in Algorithm 1 (the update of Θ) has constraints like
in [8  11]  there is a serious complexity issue: solving a constrained least squares problem with
F V variables is not an easy task. Data sparsity is destroyed after the ﬁrst deﬂation step  and thus
even ﬁrst-order methods or coordinate descent as in [8  11] do not really help. This point will be
exempliﬁed in our experiments.
3 Anchor-Free Identiﬁable Topic Mining
In this work  we are primarily interested in mining topics from the matrix P because of its noise
robustness and scalability. We will formulate topic modeling as an optimization problem  and show

3

that the word-topic matrix C can be identiﬁed under a much more relaxed condition  which includes
the relatively strict anchor-word assumption as a special case.
3.1 Problem Formulation
Let us begin with the model P = CEC T   subject to the constraint that each column of C represents
the PMF of words appearing in a speciﬁc topic  such that CT 1 = 1  C ≥ 0. Such a symmetric
matrix decomposition is in general not identiﬁable  as we can always pick a non-singular matrix
A ∈ RF×F such that AT 1 = 1  A ≥ 0  and deﬁne ˜C = CA  ˜E = A−1CA−1  and then
1 = 1  ˜C ≥ 0. We wish to ﬁnd an identiﬁcation criterion such that under
P = ˜C ˜E ˜C
some mild conditions the corresponding solution can only be the ground-truth E and C up to some
trivial ambiguities such as a common column permutation. To this end  we propose the following
criterion:

T with ˜C

T

| det E| 

subject to P = CEC T   CT 1 = 1  C ≥ 0.

(3)

minimize

E∈RF×F  C∈RV ×F

The ﬁrst observation is that if the anchor-word assumption is satisﬁed  the optimal solutions of the
above identiﬁcation criterion are the ground-truth C and E and their column-permuted versions.
Formally  we show that:
Proposition 1 Let (C(cid:63)  E(cid:63)) be an optimal solution of (3). If the separability / anchor-word assump-
tion (cf. Assumption 1) is satisﬁed and rank(P ) = F   then C(cid:63) = CΠ and E(cid:63) = Π T EΠ  where
Π is a permutation matrix.

The proof of Proposition 1 can be found in the supplementary material. Proposition 1 is merely a
‘sanity check’ of the identiﬁcation criterion in (3): It shows that the criterion is at least a sound one
under the anchor-word assumption. Note that  when the anchor-word assumption is satisﬁed  SPA-
type algorithms are in fact preferable over the identiﬁcation criterion in (3)  due to their simplicity.
The point of the non-convex formulation in (3) is that it can guarantee identiﬁability of C and E
even when the anchor-word assumption is grossly violated. To explain  we will need the following.
Assumption 2 (sufﬁciently scattered) Let cone(CT )∗ denote the polyhedral cone {x : Cx ≥ 0} 
and K denote the second-order cone {x : (cid:107)x(cid:107)2 ≤ 1T x}. Matrix C is called sufﬁciently scattered
if it satisﬁes that: (i) cone(CT )∗ ⊆ K  and (ii) cone(CT )∗ ∩ bdK = {λef : λ ≥ 0  f = 1  . . .   F} 
where bdK denotes the boundary of K  i.e.  bdK = {x : (cid:107)x(cid:107)2 = 1T x}.
Our main result is based on this assumption  whose ﬁrst consequence is as follows:
Lemma 1 If C ∈ RV ×F is sufﬁciently scattered  then rank(C) = F . In addition  given rank(P ) =
F   any feasible solution ˜E ∈ RF×F of Problem (3) has full rank and thus | det ˜E| > 0.
Lemma 1 ensures that any feasible solution pair ( ˜C  ˜E) of Problem (3) has full rank F when the
ground-truth C is sufﬁciently scattered  which is important from the optimization perspective –
otherwise | det ˜E| can always be zero which is a trivial optimal solution of (3). Based on Lemma 1 
we further show that:
Theorem 1 Let (C(cid:63)  E(cid:63)) be an optimal solution of (3). If the ground truth C is sufﬁciently scattered
(cf. Assumption 2) and rank(P ) = F   then C(cid:63) = CΠ and E(cid:63) = Π T EΠ  where Π is a
permutation matrix.

The proof of Theorem 1 is relegated to the supplementary material. In words  for a sufﬁciently
scattered C and an arbitrary square matrix E  given P = CEC T   C and E can be identiﬁed up to
permutation via solving (3). To understand the sufﬁciently scattered condition and Theorem 2  it is
better to look at the dual cones. The notation cone(CT )∗ = {x : Cx ≥ 0} comes from the fact that
it is the dual cone of the conic hull of the row vectors of C  i.e.  cone(CT ) = {CT θ : θ ≥ 0}. A
useful property of dual cone is that for two convex cones  if K1 ⊆ K2  then K∗
1  which means
the ﬁrst requirement of Assumption 2 is equivalent to

2 ⊆ K∗

(4)
Note that the dual cone of K is another second-order cone [12]  i.e.  K∗ = {x|xT 1 ≥ √
F − 1(cid:107)x(cid:107)2} 
which is tangent to and contained in the nonnegative orthant. Eq. (4) and the deﬁnition of K∗ in

K∗ ⊆ cone(CT ).

4

Figure 1: A graphical view of rows of C (blue dots) and various cones in R3  sliced at the plane
1T x = 1. The triangle indicates the non-negative orthant  the enclosing circle is K  and the smaller
circle is K∗. The shaded region is cone(CT )  and the polygon with dashed sides is cone(CT )∗. The
matrix C can be identiﬁed up to column permutation in the left two cases  and clearly separability is
more restrictive than (and a special case of) sufﬁciently scattered.

fact give a straightforward comparison between the proposed sufﬁciently scattered condition and
the existing anchor-word assumption. An illustration of Assumptions 1 and 2 is shown in Fig. 1
(a)-(b) using an F = 3 case  where one can see that sufﬁciently scattered is much more relaxed
compared to the anchor-word assumption: if the rows of the word-topic matrix C are geometrically
scattered enough so that cone(CT ) contains the inner circle (i.e.  the second-order cone K∗)  then
the identiﬁability of the criterion in (3) is guaranteed. However  the anchor-word assumption requires
that cone(CT ) fulﬁlls the entire triangle  i.e.  the nonnegative orthant  which is far more restrictive.
Fig. 1(c) shows a case where rows of C are not “well scattered” in the non-negative orthant  and
indeed such a matrix C cannot be identiﬁed via solving (3).

Remark 2 A salient feature of the criterion in (3) is that it does not need to normalize the data
columns to a simplex — all the arguments in Theorem 1 are cone-based. The upshot is clear: there is
no risk of amplifying noise or changing the conditioning of P at the factorization stage. Furthermore 
matrix E can be any symmetric matrix; it can contain negative values  which may cover more
applications beyond topic modeling where E is always nonnegative and positive semideﬁnite. This
shows the surprising effectiveness of the sufﬁciently scattered condition.

The sufﬁciently scattered assumption appeared in identiﬁability proofs of several matrix factorization
models [17  18  19] with different identiﬁcation criteria. Huang et al. [17] used this condition
to show the identiﬁability of plain NMF  while Fu et al. [19] related the sufﬁciently scattered
condition to the so-called volume-minimization criterion for blind source separation. Note that volume
minimization also minimizes a determinant-related cost function. Like the SPA-type algorithms 
volume minimization works with data that live in a simplex  therefore applying it still requires data
normalization  which is not desired in practice. Theorem 1 can be considered as a more natural
application of the sufﬁciently scattered condition to co-occurrence/correlation based topic modeling 
which explores the symmetry of the model and avoids normalization.
3.2 AnchorFree: A Simple and Scalable Algorithm
The identiﬁcation criterion in (3) imposes an interesting yet challenging optimization problem. One
way to tackle it is to consider the following approximation:

minimize

E C

F + µ| det E|  subject to C ≥ 0  CT 1 = 1 

(5)
where µ ≥ 0 balances the data ﬁdelity and the minimal determinant criterion. The difﬁculty is
that the term CEC T makes the problem tri-linear and not easily decoupled. Plus  tuning a good
µ may also be difﬁcult. In this work  we propose an easier procedure of handling the determinant-
minimization problem in (3)  which is summarized in Algorithm 2  and referred to as AnchorFree.
To explain the procedure  ﬁrst notice that P is symmetric and positive semideﬁnite. Therefore  one
can apply square root decomposition to P = BBT   where B ∈ RV ×F . We can take advantage
of well-established tools for eigen-decomposition of sparse matrices  and there is widely available
software that can compute this very efﬁciently. Now  we have B = CE1/2Q QT Q = QQT = I 
and E = E1/2E1/2; i.e.  the representing coefﬁcients of CE1/2 in the range space of B must be
orthonormal because of the symmetry of P . We also notice that

(cid:13)(cid:13)P − CEC T(cid:13)(cid:13)2

minimize

E C Q

| det E1/2Q|  subject to B = CE1/2Q  CT 1 = 1  C ≥ 0  QT Q = I 

(6)

5

(a) separable / anchor word(b) sufficiently scattered(c) not identifiablehas the same optimal solutions as (3). Since Q is unitary  it does not affect the determinant  so we
further let M = QT E−1/2 and obtain the following optimization problem

maximize

M

| det M|  subject to M T BT 1 = 1  BM ≥ 0.

(7)

with respect to M (:  f )  i.e.  det M = (cid:80)F

By our reformulation  C has been marginalized and we have only F 2 variables left  which is
signiﬁcantly smaller compared to the variable size of the original problem V F + F 2  where V is
the vocabulary size. Problem (7) is still non-convex  but can be handled very efﬁciently. Here  we
propose to employ the solver proposed in [18]  where the same subproblem (7) was used to solve
a dynamical system identiﬁcation problem. The idea is to apply the co-factor expansion to deal
with the determinant objective function  ﬁrst proposed in the context of non-negative blind source
separation [20]: if we ﬁx all the columns of M except the fth one  det M becomes a linear function
k=1(−1)f +kM (k  f ) det ¯M k f = aT M (:  f )  where
a = [a1  . . .   aF ]T   ak = (−1)f +k det ¯M k f   ∀ k = 1  ...  F   and ¯M k f is a matrix obtained by
removing the kth row and fth column of M. Maximizing |aT x| subject to linear constraints is
still a non-convex problem  but we can solve it via maximizing both aT x and −aT x  followed by
picking the solution that gives larger absolute objective. Then  cyclically updating the columns of M
results in an alternating optimization (AO) algorithm. The algorithm is computationally lightweight:
each linear program only involves F variables  leading to a worst-case complexity of O(F 3.5) ﬂops
even when the interior-point method is employed  and empirically it takes 5 or less AO iterations
to converge. In the supplementary material  simulations on synthetic data are given  showing that
Algorithm 2 can indeed recover the ground truth matrix C and E even when matrix C grossly
violates the separability / anchor-word assumption.

: D  F .

Algorithm 2: AnchorFree
input
P ← Co-Occurrence(D);
P = BBT   M ← I;
repeat

for f = 1  . . .   F do

end

until convergence;
C (cid:63) = BM;
E(cid:63) = (C T
output : C (cid:63)  E(cid:63)

(cid:63) C (cid:63))−1C T

ak = (−1)f +k det ¯M k f   ∀ k = 1  ...  F ;
// remove k-th row and f-th column of M to obtain ¯M k f
mmax = arg maxx aT x s.t. Bx ≥ 0  1T Bx = 1;
mmin = arg minx aT x s.t. Bx ≥ 0  1T Bx = 1;
M (:  f ) = arg maxmmax mmin (|aT mmax| |aT mmin|);

(cid:63) P C (cid:63)(C T

(cid:63) C (cid:63))−1;

4 Experiments
Data In this section  we apply the proposed algorithm and the baselines to two popular text mining
datasets  namely  the NIST Topic Detection and Tracking (TDT2) and the Reuters-21578 corpora.
We use a subset of the TDT2 corpus consisting of 9 394 documents which are single-category
articles belonging to the largest 30 categories. The Reuters-21578 corpus is the ModApte version
where 8 293 single-category documents are kept. The original vocabulary sizes of the TDT2 and
the Reuters dataset are 36  771 and 18  933  respectively  and stop words are removed for each trial
of the experiments. We use the standard tf-idf data as the D matrix  and estimate the correlation
matrix using the biased estimator suggested in [9]. A standard pre-processing technique  namely 
normalized-cut weighted (NCW) [21]  is applied to D; NCW is a well-known trick for handling the
unbalanced-cluster-size problem. For each trial of our experiment  we randomly draw F categories
of documents  form the P matrix  and apply the proposed algorithm and the baselines.
Baselines We employ several popular anchor word-based algorithms as baselines. Speciﬁcally 
the successive projection algorithm (SPA) [6]  the successive nonnegative projection algorithm
(SNPA) [11]  the XRAY algorithm [8]  and the fast anchor words (FastAnchor) [10] algorithm.
Since we are interested in word-word correlation/co-occurrence based mining  all the algorithms are

6

Table 1: Experiment results on the TDT2 corpus.

SPA

SNPA XRAY AnchorFree FastAchor SPA SNPA XRAY AnchorFree FastAchor SPA SNPA XRAY AnchorFree

F FastAchor
-612.72
3
-648.20
4
-641.79
5
-654.18
6
-668.92
7
8
-681.35
-688.54
9
-732.39
10
-734.13
15
-756.90
20
25
-792.92

F FastAchor
-652.67
3
-633.69
4
-650.49
5
6
-654.74
-733.73
7
-735.23
8
-761.27
9
-764.18
10
-800.51
15
20
-859.48
-889.55
25

-613.43 -613.43 -597.16
-648.04 -648.04 -657.51
-643.91 -643.91 -665.20
-645.68 -645.68 -674.30
-665.55 -665.55 -664.38
-674.45 -674.45 -657.78
-671.81 -671.81 -690.39
-724.64 -724.64 -698.59
-730.19 -730.19 -773.17
-747.99 -747.99 -819.36
-792.29 -792.29 -876.28

-433.87
-430.07
-405.19
-432.96
-397.77
-450.63
-416.44
-421.25
-445.30
-461.64
-473.95

Coh

Coh

-647.28 -647.28 -574.72
-637.89 -637.89 -586.41
-652.53 -652.53 -581.73
-644.34 -644.34 -586.00
-732.01 -732.01 -612.97
-738.54 -738.54 -616.32
-755.46 -755.46 -640.36
-759.40 -759.40 -656.71
-801.17 -801.17 -585.18
-860.70 -860.70 -615.62
-890.16 -890.16 -633.75

-830.24
-741.35
-762.64
-705.60
-692.12
-726.37
-713.81
-709.48
-688.39
-683.64
-672.44

SimCount

7.98

7.98
8.94
11.18 11.18 13.70
13.36 13.36 22.56
18.10 18.10 31.56
18.84 18.84 39.06
25.14 25.14 40.30
29.10 29.10 53.68
29.86 29.86 53.16
52.62 52.62 59.96
65.00 65.00 82.92
66.00 66.00 101.52

SimCount

11.02
16.92
21.66
39.54
45.24
83.86

3.86
11.02
9.92
16.92
13.06
21.66
27.42
39.54
34.64
45.24
83.86
82.52
118.98 118.98 119.28
121.74 121.74 130.82
309.7
309.7 227.02
538.54 538.54 502.82
650.96

673

673

7.98
10.60
13.06
18.94
20.14
24.82
27.50
31.08
51.62
66.26
69.46

10.98
16.74
21.74
39.9
47.02
85.04
117.48
119.54
307.86
539.58
674.78

1.84
2.88
4.40
7.18
4.48
9.12
9.70
13.02
41.88
79.60
133.42

7.36
12.66
15.48
19.98
35.62
62.02
72.38
86.02
124.6
225.6
335.24

ClustAcc

0.74 0.75
0.69 0.69
0.63 0.62
0.58 0.59
0.60 0.59
0.56 0.58
0.58 0.58
0.55 0.54
0.50 0.50
0.47 0.47
0.47 0.47

0.69 0.69
0.61 0.61
0.55 0.55
0.49 0.50
0.57 0.57
0.53 0.54
0.56 0.56
0.52 0.52
0.40 0.40
0.36 0.36
0.33 0.32

0.73
0.69
0.64
0.60
0.58
0.57
0.53
0.49
0.42
0.38
0.37

0.66
0.60
0.52
0.46
0.54
0.47
0.47
0.42
0.42
0.38
0.37

ClustAc

0.71
0.70
0.63
0.65
0.62
0.57
0.61
0.59
0.51
0.47
0.46

0.66
0.51
0.51
0.47
0.43
0.40
0.37
0.35
0.33
0.31
0.26

0.98
0.94
0.92
0.91
0.90
0.87
0.86
0.85
0.80
0.77
0.74

0.79
0.73
0.65
0.64
0.65
0.61
0.59
0.59
0.53
0.52
0.47

Table 2: Experiment results on the Reuters-21578 corpus.

SPA

SNPA XRAY AnchorFree FastAchor SPA SNPA XRAY AnchorFree FastAchor SPA SNPA XRAY AnchorFree

(cid:80)

combined with the framework provided in [10] and the efﬁcient RecoverL2 process is employed for
estimating the topics after the anchors are identiﬁed.
Evaluation To evaluate the results  we employ several metrics. First  coherence (Coh) is used
to measure the single-topic quality. For a set of words V  the coherence is deﬁned as Coh =
v1 v2∈V log (freq(v1 v2)+/freq(v2))   where v1 and v2 denote the indices of two words in the vocab-
ulary  freq(v2) and freq(v1  v2) denote the numbers of documents in which v1 appears and v1 and v2
co-occur  respectively  and  = 0.01 is used to prevent taking log of zero. Coherence is considered
well-aligned to human judgment when evaluating a single topic — a higher coherence score means
better quality of a mined topic. However  coherence does not evaluate the relationship between
different mined topics; e.g.  if the mined F topics are identical  the coherence score can still be high
but meaningless. To alleviate this  we also use the similarity count (SimCount) that was adopted in
[10] — for each topic  the similarity count is obtained simply by adding up the overlapped words of
the topics within the leading N words  and a smaller SimCount means the mined topics are more
distinguishable. When the topics are very correlated (but different)  the leading words of the topics
may overlap with each other  and thus using SimCount might still not be enough to evaluate the
results. We also include clustering accuracy (ClustAcc)  obtained by using the mined C(cid:63) matrix
to estimate the weights W of the documents  and applying k-means to W . Since the ground-truth
labels of TDT2 and Reuters are known  clustering accuracy can be calculated  and it serves as a good
indicator of topic mining results.
Table 1 shows the experiment results on the TDT2 corpus. From F = 3 to 25  the proposed algorithm
(AnchorFree) gives very promising results: for the three considered metrics  AnchorFree consistently
gives better results compared to the baselines. Particularly  the ClustAcc’s obtained by AnchorFree
are at least 30% higher compared to the baselines for all cases. In addition  the single-topic quality of
the topics mined by AnchorFree is the highest in terms of coherence scores; the overlaps between
topics are the smallest except for F = 20 and 25.
Table 2 shows the results on the Reuters-21578 corpus. In this experiment  we can see that XRAY is
best in terms of single-topic quality  while AnchorFree is second best when F > 6. For SimCount 
AnchorFree gives the lowest values when F > 6. In terms of clustering accuracy  the topics obtained
by AnchorFree again lead to much higher clustering accuracies in all cases.
In terms of the runtime performance  one can see from Fig. 2(a) that FastAnchor  SNPA  XRAY and
AnchorFree perform similarly on the TDT2 dataset. SPA is the fastest algorithm since it has a recursive
update [6]. The SNPA and XRAY both perform nonnegative least squares-based deﬂation  which is
computationally heavy when the vocabulary size is large  as mentioned in Remark 1. AnchorFree
uses AO and small-scale linear programming  which is conceptually more difﬁcult compared to SNPA
and XRAY. However  since the linear programs involved only have F variables and the number of AO
iterations is usually small (smaller than 5 in practice)  the runtime performance is quite satisfactory

7

(a) TDT2

(b) Reuters-21578

Figure 2: Runtime performance of the algorithms under various settings.

Table 3: Twenty leading words of mined topics from an F = 5 case of the TDT2 experiment.

predicts

allegations
lewinsky
clinton

lady
white
hillary
monica

starr
house

husband
dissipate
president

intern
affair

inﬁdelity

grand
jury
sexual
justice

obstruction

slipping

poll

cnnusa
gallup

allegations

clinton

presidents

rating

lewinsky
president
approval

starr
white
monica
house
hurting
slipping
americans

public
sexual
affair

FastAnchor

anchor
cleansing
columbia

strangled

gm

shuttle
space
crew

motors
plants
workers
astronauts michigan

nasa

experiments

ﬂint

strikes
auto
plant
strike
gms
idled

production
walkouts

north
union

assembly

talks
shut

striking

mission
stories

ﬁx

repair
rats
unit

aboard
brain
system
broken
nervous
cleansing
dioxide

tenday
bulls
jazz
nba
utah
ﬁnals
game

chicago
jordan
series
malone
michael

tonight
lakers
win
karl

lewinsky

games

basketball

night

championship

lewinsky
monica

starr
grand
white
jury
house
clinton
counsel
intern

independent

president

investigation

affair

lewinskys
relationship

sexual

ken

former
starrs

gm

motors
plants
ﬂint

workers
michigan

auto
plant
strikes
gms
strike
union
idled

north
shut
talks

AnchorFree

anchor

shuttle
space

columbia
astronauts

nasa
crew

experiments

rats

mission
nervous

brain
aboard
system

earth
mice

animals

ﬁsh

bulls
jazz
nba

chicago

game
utah
ﬁnals
jordan
malone
michael
series

championship

karl

pippen

basketball

win
night
sixth
games

title

jonesboro
arkansas
school
shooting

boys

teacher
students
westside
middle
11year

ﬁre
girls

mitchell
shootings
suspects
funerals
children

killed
13year
johnson

assembly weightlessness
production

autoworkers

walkouts

neurological

seven

and is close to those of SNPA and XRAY which are greedy algorithms. The runtime performance
on the Reuters dataset is shown in Fig. 2(b)  where one can see that the deﬂation-based methods are
faster. The reason is that the vocabulary size of the Reuters corpus is much smaller compared to that
of the TDT2 corpus (18 933 v.s. 36 771).
Table 3 shows the leading words of the mined topics by FastAnchor and AnchorFree from an F = 5
case using the TDT2 corpus. We only present the result of FastAnchor since it gives qualitatively
the best benchmark – the complete result given by all baselines can be found in the supplementary
material. We see that the topics given by AnchorFree show clear diversity: Lewinsky scandal 
General Motors strike  Space Shuttle Columbia  1997 NBA ﬁnals  and a school shooting in Jonesboro 
Arkansas. FastAnchor  on the other hand  exhibit great overlap on the ﬁrst and the second mined
topics. Lewinsky also shows up in the ﬁfth topic mined by FastAnchor  which is mainly about the
1997 NBA ﬁnals. This showcases the clear advantage of our proposed criterion in terms of giving
more meaningful and interpretable results  compared to the anchor-word based approaches.
5 Conclusion
In this paper  we considered identiﬁable anchor-free correlated topic modeling. A topic estimation
criterion based on the word-word co-occurrence/correlation matrix was proposed and its identiﬁability
conditions were proven. The proposed approach features topic identiﬁability guarantee under much
milder conditions compared to the anchor-word assumption  and thus exhibits better robustness to
model mismatch. A simple procedure that only involves one eigen-decomposition and a few small
linear programs was proposed to deal with the formulated criterion. Experiments on real text corpus
data showcased the effectiveness of the proposed approach.
Acknowledgment
This work is supported in part by the National Science Foundation (NSF) under the project numbers
NSF-ECCS 1608961 and NSF IIS-1247632 and in part by the Digital Technology Initiative (DTI)
Seed Grant  University of Minnesota.

8

510152025F100101102103Runtime (sec.)FastAnchorSPASNPAXRAYAnchorFree510152025F100101102103Runtime (sec.)FastAnchorSPASNPAXRAYAnchorFreeReferences
[1] D. M. Blei. Probabilistic topic models. Communications of the ACM  55(4):77–84  2012.

[2] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[3] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of

Sciences  101(suppl 1):5228–5235  2004.

[4] S. Arora  R. Ge  R. Kannan  and A. Moitra. Computing a nonnegative matrix factorization–provably. In

ACM symposium on Theory of Computing  pages 145–162. ACM  2012.

[5] B. Recht  C. Re  J. Tropp  and V. Bittorf. Factoring nonnegative matrices with linear programs. In Proc.

NIPS 2012  pages 1214–1222  2012.

[6] N. Gillis and S.A. Vavasis. Fast and robust recursive algorithms for separable nonnegative matrix factor-

ization. IEEE Trans. Pattern Anal. Mach. Intell.  36(4):698–714  April 2014.

[7] N. Gillis. Robustness analysis of hottopixx  a linear programming model for factoring nonnegative

matrices. SIAM Journal on Matrix Analysis and Applications  34(3):1189–1212  2013.

[8] A. Kumar  V. Sindhwani  and P. Kambadur. Fast conical hull algorithms for near-separable non-negative

matrix factorization. In Proc. ICML-12  2012.

[9] S. Arora  R. Ge  and A. Moitra. Learning topic models–going beyond SVD. In Proc. FOCS 2012  pages

1–10. IEEE  2012.

[10] S. Arora  R. Ge  Y. Halpern  D. Mimno  A. Moitra  D. Sontag  Y. Wu  and M. Zhu. A practical algorithm

for topic modeling with provable guarantees. In Proc. ICML-13  2013.

[11] N. Gillis. Successive nonnegative projection algorithm for robust nonnegative blind source separation.

SIAM Journal on Imaging Sciences  7(2):1420–1450  2014.

[12] D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct decomposition

into parts? In Proc. NIPS 2013  volume 16  2003.

[13] A. Anandkumar  Y.-K. Liu  D. J. Hsu  D. P. Foster  and S. M. Kakade. A spectral algorithm for latent

Dirichlet allocation. In Proc. NIPS 2012  pages 917–925  2012.

[14] A. Anandkumar  S. M. Kakade  D. P. Foster  Y.-K. Liu  and D. Hsu. Two SVDs sufﬁce: Spectral

decompositions for probabilistic topic modeling and latent Dirichlet allocation. Technical report  2012.

[15] A. Anandkumar  D. J. Hsu  M. Janzamin  and S. M. Kakade. When are overcomplete topic models
identiﬁable? uniqueness of tensor Tucker decompositions with structured sparsity. In Proc. NIPS 2013 
pages 1986–1994  2013.

[16] D. Cai  X. He  and J. Han. Locally consistent concept factorization for document clustering. IEEE Trans.

Knowl. Data Eng.  23(6):902–913  2011.

[17] K. Huang  N. Sidiropoulos  and A. Swami. Non-negative matrix factorization revisited: Uniqueness and

algorithm for symmetric decomposition. IEEE Trans. Signal Process.  62(1):211–224  2014.

[18] K. Huang  N. D. Sidiropoulos  E. E. Papalexakis  C. Faloutsos  P. P. Talukdar  and T. M. Mitchell. Principled
neuro-functional connectivity discovery. In Proc. SIAM Conference on Data Mining (SDM)  2015.

[19] X. Fu  W.-K. Ma  K. Huang  and N. D. Sidiropoulos. Blind separation of quasi-stationary sources:
Exploiting convex geometry in covariance domain. IEEE Trans. Signal Process.  63(9):2306–2320 
May 2015.

[20] W.-K. Ma  T.-H. Chan  C.-Y. Chi  and Y. Wang. Convex analysis for non-negative blind source separation
with application in imaging. In D. P. Palomar and Y. Eldar  editors  Convex Optimization in Signal
Processing and Communications  chapter 7  pages 229–265. 2010.

[21] Wei Xu  Xin Liu  and Yihong Gong. Document clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM SIGIR conference on Research and
development in informaion retrieval  pages 267–273. ACM  2003.

9

,Kejun Huang
Xiao Fu
Nikolaos Sidiropoulos