2019,Nearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin,We study the problem of {\em properly} learning large margin halfspaces in the agnostic PAC model. 
In more detail  we study the complexity of properly learning $d$-dimensional halfspaces
on the unit ball within misclassification error $\alpha \cdot \opt_{\gamma} + \eps$  where $\opt_{\gamma}$
is the optimal $\gamma$-margin error rate and $\alpha \geq 1$ is the approximation ratio.
We give learning algorithms and computational hardness results
for this problem  for all values of the approximation ratio $\alpha \geq 1$  
that are nearly-matching for a range of parameters. 
Specifically  for the natural setting that $\alpha$ is any constant bigger 
than one  we provide an essentially tight complexity characterization.
On the positive side  we give an $\alpha = 1.01$-approximate proper learner 
that uses $O(1/(\eps^2\gamma^2))$ samples (which is optimal) and runs in time
$\poly(d/\eps) \cdot 2^{\tilde{O}(1/\gamma^2)}$. On the negative side   
we show that {\em any} constant factor approximate proper learner has runtime 
$\poly(d/\eps) \cdot 2^{(1/\gamma)^{2-o(1)}}$  
assuming the Exponential Time Hypothesis.,Nearly Tight Bounds for Robust Proper Learning

of Halfspaces with a Margin∗

Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

Daniel M. Kane

University of California  San Diego

dakane@cs.ucsd.edu

Pasin Manurangsi†

University of California  Berkeley

pasin@berkeley.edu

Abstract

We study the problem of properly learning large margin halfspaces in the agnostic
PAC model.
In more detail  we study the complexity of properly learning d-
dimensional halfspaces on the unit ball within misclassiﬁcation error α· OPTγ +  
where OPTγ is the optimal γ-margin error rate and α ≥ 1 is the approximation
ratio. We give learning algorithms and computational hardness results for this
problem  for all values of the approximation ratio α ≥ 1  that are nearly-matching
for a range of parameters. Speciﬁcally  for the natural setting that α is any constant
bigger than one  we provide an essentially tight complexity characterization. On
the positive side  we give an α = 1.01-approximate proper learner that uses
O(1/(2γ2)) samples (which is optimal) and runs in time poly(d/) · 2 ˜O(1/γ2).
On the negative side  we show that any constant factor approximate proper learner
has runtime poly(d/) · 2(1/γ)2−o(1)  assuming the Exponential Time Hypothesis.

1

Introduction

1.1 Background and Problem Deﬁnition
Halfspaces are boolean functions hw : Rd → {±1} of the form hw(x) = sign ((cid:104)w  x(cid:105))  where
w ∈ Rd is the associated weight vector. (The function sign : R → {±1} is deﬁned as sign(u) = 1 if
u ≥ 0 and sign(u) = −1 otherwise.) The problem of learning an unknown halfspace with a margin
condition (in the sense that no example is allowed to lie too close to the separating hyperplane) is
as old as the ﬁeld of machine learning — starting with Rosenblatt’s Perceptron algorithm [Ros58]
— and has arguably been one of the most inﬂuential problems in the development of the ﬁeld  with
techniques such as SVMs [Vap98] and AdaBoost [FS97] coming out of its study.
We study the problem of learning γ-margin halfspaces in the agnostic PAC model [Hau92  KSS94].
Speciﬁcally  there is an unknown distribution D on Bd × {±1}  where Bd is the unit ball on Rd  and
the learning algorithm A is given as input a training set S = {(x(i)  y(i))}m
i=1 of i.i.d. samples drawn
from D. The goal of A is to output a hypothesis whose error rate is competitive with the γ-margin error
rate of the optimal halfspace. In more detail  the error rate (misclassiﬁcation error) of a hypothesis
0−1(h) def= Pr(x y)∼D[h(x) (cid:54)= y]. For γ ∈ (0  1)  the
h : Rd → {±1} (with respect to D) is errD
γ-margin error rate of a halfspace hw(x) with (cid:107)w(cid:107)2 ≤ 1 is errD
γ (w) def= Pr(x y)∼D [y(cid:104)w  x(cid:105) ≤ γ].

∗The full version of this paper is available at [DKM19].
†Now at Google Research  Mountain View.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

γ

def= min(cid:107)w(cid:107)2≤1 errD

We denote by OPTD
γ (w) the minimum γ-margin error rate achievable by any
halfspace. We say that A is an α-agnostic learner  α ≥ 1  if it outputs a hypothesis h that with
probability at least 1 − τ satisﬁes errD
0−1(h) ≤ α · OPTD
γ + . (For α = 1  we obtain the standard
notion of agnostic learning.) If the hypothesis h is itself a halfspace  we say that the learning algorithm
is proper. This work focuses on proper learning algorithms.

1.2 Related and Prior Work

γ > 0) is much more challenging computationally.

In this section  we summarize the related work that is directly relavant to the results of this paper. First 
we note that the sample complexity of our learning problem (ignoring computational considerations)
is well-understood. In particular  the ERM that minimizes the number of γ-margin errors over the
training set (subject to a norm constraint) is known to be an agnostic learner (α = 1)  assuming
the sample size is Ω(log(1/τ )/(2γ2)). Speciﬁcally  Θ(log(1/τ )/(2γ2)) samples3 are known to
be sufﬁcient and necessary for this learning problem (see  e.g.  [BM02  McA03]). In the realizable
case (OPTD
γ = 0)  i.e.  if the data is linearly separable with margin γ  the ERM rule above can
be implemented in poly(d  1/  1/γ) time using the Perceptron algorithm. The agnostic setting
(OPTD
The agnostic version of our problem (α = 1) was ﬁrst considered in [BS00]  who gave a proper
learning algorithm with runtime poly(d) · (1/) ˜O(1/γ2). It was also shown in [BS00] that agnostic
proper learning with runtime poly(d  1/  1/γ) is NP-hard. A question left open by their work was
characterizing the computational complexity of proper learning as a function of 1/γ.
Subsequent works focused on improper learning. The α = 1 case was studied in [SSS09  SSS10]
who gave a learning algorithm with sample complexity poly(1/) · 2 ˜O(1/γ) – i.e.  exponential in 1/γ
– and computational complexity poly(d/) · 2 ˜O(1/γ). The increased sample complexity is inherent in
their approach  as their algorithm works by solving a convex program over an expanded feature space.
[BS12] gave an α-agnostic learning algorithm for all α ≥ 1 with sample complexity 2 ˜O(1/(αγ)) and
computational complexity poly(d/)· 2 ˜O(1/(αγ)). (We note that the Perceptron algorithm is known to
achieve α = 1/γ [Ser01]. Prior to [BS12]  [LS11] gave a poly(d  1/  1/γ) time algorithm achieving

α = Θ((1/γ)/(cid:112)log(1/γ)).) [BS12] posed as an open question whether their upper bounds for

improper learning can be also derived for a proper learner.
A related line of work [KLS09  ABL17  DKK+16  LRV16  DKK+17  DKK+18  DKS18  KKM18 
DKS19  DKK+19] has given polynomial time robust estimators for a range of learning tasks. Speciﬁ-
cally  [KLS09  ABL17  DKS18  DKK+19] obtained efﬁcient PAC learning algorithms for halfspaces
with malicious noise [Val85  KL93]  under the assumption that the uncorrupted data comes from
a “tame” distribution  e.g.  Gaussian or isotropic log-concave. It should be noted that the class of
γ-margin distributions considered in this work is signiﬁcantly broader and can be far from satisfying
the structural properties required in the aforementioned works.
A growing body of theoretical work has focused on adversarially robust learning (e.g.  [BLPR19 
MHS19  DNV19  Nak19]). In adversarially robust learning  the learner seeks to output a hypothesis
with small γ-robust misclassiﬁcation error  which for a hypothesis h and a norm (cid:107) · (cid:107) is typically
deﬁned as Pr(x y)∼D[∃x(cid:48) with (cid:107)x(cid:48) − x(cid:107) ≤ γ s.t. h(x(cid:48)) (cid:54)= y]. Notice that when h is a halfspace
and (cid:107) · (cid:107) is the Euclidean norm  the γ-robust misclassiﬁcation error coincides with the γ-margin
error in our context. (It should be noted that most of the literature on adversarially robust learning
focuses on the (cid:96)∞-norm.) However  the objectives of the two learning settings are slightly different:
in adversarially robust learning  the learner would like to output a hypothesis with small γ-robust
misclassiﬁcation error  whereas in our context the learner only has to output a hypothesis with small
zero-one misclassiﬁcation error. Nonetheless  as we point out in Remark 1.3  our algorithms can be
adapted to provide guarantees in line with the adversarially robust setting as well.
Finally  in the distribution-independent agnostic setting without margin assumptions  there is com-
pelling complexity-theoretic evidence that even weak learning of halfspaces is computationally
intractable [GR06  FGKP06  DOSW11  Dan16  BGS18].

3To avoid clutter in the expressions  we will henceforth assume that the failure probability τ = 1/10. Recall

that one can always boost the conﬁdence probability with an O(log(1/τ )) multiplicative overhead.

2

1.3 Our Contributions

We study the complexity of proper α-agnostic learning of γ-margin halfspaces on the unit ball. Our
main result nearly characterizes the complexity of constant factor approximation to this problem:
Theorem 1.1. There is an algorithm that uses O(1/(2γ2)) samples  runs in time poly(d/)·2 ˜O(1/γ2)
and is an α = 1.01-agnostic proper learner for γ-margin halfspaces with conﬁdence probability 9/10.
Moreover  assuming the randomized Exponential Time Hypothesis  any proper learning algorithm
that achieves any constant factor approximation has runtime poly(d/) · Ω(21/γ2−o(1)

).

The reader is referred to Theorem 2.1 for the upper bound and Theorem 3.1 for the lower bound.
First  we note that the approximation ratio of 1.01 in the theorem statement is not inherent. Our
algorithm achieves α = 1 + δ  for any δ > 0  with runtime poly(d/) · 2 ˜O(1/(δγ2)). The runtime of
our algorithm signiﬁcantly improves the runtime of the best known agnostic proper learner [BS00] 
achieving ﬁxed polynomial dependence on 1/  independent of γ. This gain in runtime comes at the
expense of losing a small constant factor in the error guarantee. It is natural to ask whether there
exists an 1-agnostic proper learner matching the runtime of our Theorem 1.1. In Theorem 3.2  we
establish a computational hardness result implying that such an improvement is unlikely.
The runtime dependence of our algorithm scales as 2 ˜O(1/γ2) (which is nearly best possible for
proper learners)  as opposed to 2 ˜O(1/γ) for improper learning [SSS09  BS12]. In addition to the
interpretability of proper learning  the sample complexity of our algorithm is quadratic in 1/γ (which
is optimal)  as opposed to exponential for known improper learners. Moreover  for moderate values
of γ  our algorithm may be faster than known improper learners  as it only uses spectral methods and
ERM  as opposed to convex programming. Finally  we note that the lower bound part of Theorem 1.1
implies a computational separation between proper and improper learning for this problem.
In addition  we explore the complexity of α-agnostic learning for large α > 1. The following theorem
summarizes our results in this setting:
Theorem 1.2. There is an algorithm that uses ˜O(1/(2γ2)) samples  runs in time poly(d) ·
(1/) ˜O(1/(αγ)2) and is a α-agnostic proper learner for γ-margin halfspaces with conﬁdence proba-
bility 9/10. Moreover  assuming NP (cid:54)= RP and the Sliding Scale Conjecture [BGLR94]  no (1/γ)c-
agnostic proper learner runs in poly(d  1/ε  1/γ) time for some (absolute) constant c > 0.

The reader is referred to Theorem 3.3 for a more precise statement of the lower bound; the upper
bound is deferred to the full version [DKM19]. In summary  we give an α-agnostic algorithm with
runtime exponential in 1/(αγ)2  as opposed to 1/γ2  and we show that achieving α = (1/γ)Ω(1) is
computationally hard. (Assuming only NP (cid:54)= RP  we can rule out polynomial time α-agnostic proper
learning for α = (1/γ)
Remark 1.3. While not stated explicitly in the subsequent analysis  our algorithms (with a slight
modiﬁcation to the associated constant factors) not only give a halfspace w∗ with zero-one loss at
most α · OPTD
γ +   but this guarantee holds for the 0.99γ-margin error4 of w∗ as well. Thus  our
learning algorithms also work in the adversarially robust setting (under the Euclidean norm) with a
small loss in the “robustness parameter” (margin) from the one used to compute the optimum (i.e.  γ)
to the one used to measure the error of the output hypothesis (i.e.  0.99γ).

polyloglog(1/γ) .)

1

1.4 Our Techniques

Overview of Algorithms. For the sake of this intuitive explanation  we provide an overview of our
algorithms when the underlying distribution D is explicitly known. The ﬁnite sample analysis of our
algorithms follows from standard generalization bounds (see Section 2).
Our constant factor approximation algorithm relies on the following observation: Let w∗ be the
optimal weight vector. The assumption that |(cid:104)w∗  x(cid:105)| is large for almost all x (by the margin property) 
implies a relatively strong condition on w∗  which will allow us to ﬁnd a relatively small search space
for a near-optimal solution. A ﬁrst idea is to consider the matrix M = E(x y)∼D[xxT ]  and note that

4Here the constant 0.99 can be replaced by any constant less than one  with an appropriate increase to the

algorithm’s running time.

3

w∗T Mw∗ = Ω(γ2). This in turn implies that w∗ has a large component on the subspace spanned by
the largest O(1/(γ2)) eigenvalues of M. This idea suggests a basic algorithm that computes a net
over unit-norm weight vectors on this subspace and outputs the best answer. Unfortunately  this basic
algorithm has runtime poly(d)2 ˜O(1/(γ2)). (Details are deferred to the full version [DKM19].)
To obtain our poly(d/)2 ˜O(1/γ2) time constant factor approximation algorithm (Theorem 1.1)  we
use a reﬁnement of the above idea. Instead of trying to guess the projection of w∗ onto the space of
large eigenvectors all at once  we will do so in stages. In particular  it is not hard to see that w∗ has a
non-trivial projection onto the subspace spanned by the top O(1/γ2) eigenvalues of M. If we guess
this projection  we will have some approximation to w∗  but unfortunately not a sufﬁciently good
one. However  we note that the difference between w∗ and our current hypothesis w will have a large
average squared inner product with the misclassiﬁed points. This suggests an iterative algorithm that
in the i-th iteration considers the second moment matrix M(i) of the points not correctly classiﬁed
by the current hypothesis sign((cid:104)w(i)  x(cid:105))  guesses a vector u in the space spanned by the top few
eigenvalues of M(i)  and sets w(i+1) = u + w(i). This procedure produces a candidate set of weights
with cardinality 2 ˜O(1/γ2) one of which has the desired misclassiﬁcation error. This algorithm and its
analysis are given in Section 2.
Our general α-factor algorithm (Theorem 1.2) relies on approximating the Chow parameters of the
target halfspace f  i.e.  the d numbers E[f (x)xi]  i ∈ [d]. A classical result [Cho61] shows that
the exact values of the Chow parameters of a halfspace (over any distribution) uniquely deﬁne the
halfspace. Although this uniqueness is not very useful in general  the margin assumption implies a
relatively strong approximate identiﬁability result. Combining this with an algorithm of [DDFS14] 
we can efﬁciently compute an approximation to the halfspace f given an approximation to its Chow
parameters. In particular  if we can approximate the Chow parameters to (cid:96)2-error ν · γ  we can
approximate fw∗ within error OPTD
The natural way to approximate the Chow parameters would be by computing the empirical Chow
parameters  namely E(x y)∼D[yx]. In the realizable case  this quantity corresponds exactly to the
vector of Chow parameters. Unfortunately  this does not work in the agnostic case and it can introduce
an error of ω(OPTD
γ ). To overcome this obstacle  we note that in order for a small fraction of errors
to introduce a large error in the empirical Chow parameters  it must be the case that there is some
direction w in which many of these erroneous points introduce a large error. If we can guess some
error that correlates well with w and also guess the correct projection of our Chow parameters
onto this vector  we can correct a decent fraction of the error between the empirical and true Chow
parameters. We show that making the correct guesses ˜O(1/(γα)2) times  we can reduce the empirical
error sufﬁciently so that it can be used to ﬁnd an accurate hypothesis. Once again  we can compute
a hypothesis for each sequence of guesses and return the best one. The formal description of the
algorithm and its analysis can be found in the full version of this paper [DKM19].

γ + ν.

Overview of Computational Lower Bounds. Our hardness results are shown via two reductions.
These reductions take in an instance of a “hard problem” and produce a distribution D on Bd × {±1}.
If the starting instance is a YES instance of the original problem  then OPTD
γ is small for an
appropriate γ. On the other hand  if the starting instance is a NO instance of the original problem 
then OPTD
0−1 is large5. As a result  if there is a “too fast” (α-)agnostic learner for γ-margin halfspaces 
then we would also get a “too fast” algorithm for the starting problem as well  which would violate
the corresponding complexity assumption.
To understand the margin parameter γ we can achieve  we need to ﬁrst understand the problems we
start with. For our reductions  the starting problems can be viewed in the following form: select k
items from v1  . . .   vN that satisfy certain “local constraints”. For instance  in our ﬁrst reduction  the
reduction is from the k-Clique problem where we are given a graph G and an integer k  and the goal
is to determine whether it contains a k-clique as a subgraph. For this problem  v1  . . .   vN are the
vertices of G and the “local” constraints are that every pair of selected vertices induces an edge.
Roughly speaking  our reduction produces D with dimension d = N  with the i-th dimension
corresponding to vi. The “ideal” solution in the YES case is to set wi = 1√
iff vi is selected and set
wi = 0 otherwise. In our reductions  the local constraints are expressed using “sparse” sample vectors

k

5We use OPTD
0−1

def= minw∈Rd errD

0−1(w) to denote the minimum error rate achievable by any halfspace.

4

k

k

1

(cid:16) 1√

ej(cid:17) · w ≤ 1√

2k

k

1√
2k

with say 0.99√

2k

ei + 1√
2

2

γ )poly(d  1

)  where the dimension d is N (and ε =

(with only a constant number of non-zero coordinates all having the same magnitude). For example 
in the case of k-Clique  the constraints can be expressed as: for every non-edge (i  j)  we must have
  where ei and ej denote the i-th and j-th vectors in the standard basis.
A main step in both of our proofs is to show that the reduction still works even when we “shift” the
right hand side by a small multiple of 1√
. For instance  in the case of k-Clique  it is possible to show
  the correctness of the construction remains and we also
that  even if we replace
get the added beneﬁt that now the constraints are satisﬁed by a margin of γ = Θ( 1√
) for our ideal
solution in the YES case.
In the case of k-Clique  the above idea yields a reduction to 1-agnostic learning γ-margin halfspaces
with margin γ = Θ( 1√
poly(N )). As a result  if there
is an f ( 1
ε )-time algorithm for the latter for some function f  then there also exists an
g(k)poly(N )-time algorithm for k-Clique for some function g  which is considered unlikely as it
would break a widely-belived hypothesis in the area of parameterized complexity.
Ruling out α-agnostic learners is slightly more complicated  since we need to produce the “gap” of α
between OPTD
0−1 in the NO case. To create such a gap  we appeal to the
PCP Theorem [AS98  ALM+98] which can be thought of as an NP-hardness proof of the following
“gap version” of 3SAT: given a 3CNF formula as input  distinguish between the case where the
formula is satisﬁable and the case where the formula is not even 0.9-satisﬁable6. Moreover  further
strengthened versions of the PCP Theorem [Din07  MR10] actually implies that this Gap-3SAT
problem cannot even be solved in time O(2n0.999
)  where n denotes the number of variables in the
formula  assuming the exponential time hypothesis (ETH)7. Once again  (Gap-)3SAT can be viewed
in the form of “item selection with local constraint”. However  the number of selected items k is now
equal to n  the number of variables of the formula. With a similar line of reasoning as above  the
margin we get is now γ = Θ( 1√
ε )-time
poly(d  1
α-agnostic learner for γ-margin halfspaces (for an appropriate α)  then there is an O(2n0.995
)-time
algorithm for Gap-3SAT  which would violate ETH.
Unfortunately  the above described idea only gives the “gap” α that is only slightly larger than 1 
because the gap that we start with in the Gap-3SAT problem is already pretty small. To achieve larger
gaps  our actual reduction starts from a generalization of 3SAT called constraint satisfaction problems
(CSPs)  whose gap problems are hard even for very large gap. This concludes the outline of the main
intuitions in our reductions.

n ). As a result  if there is a say 2(1/γ)1.99

γ in the YES case and OPTD

) = Θ( 1√

k

def= ((cid:80)d

1.5 Preliminaries
For n ∈ Z+  we denote [n] def= {1  . . .   n}. We will use small boldface characters for vectors
and capital boldface characters for matrices. For a vector x ∈ Rd  and i ∈ [d]  xi denotes the
i-th coordinate of x  and (cid:107)x(cid:107)2
i )1/2 denotes the (cid:96)2-norm of x. We will use (cid:104)x  y(cid:105)
for the inner product between x  y ∈ Rd. For a matrix M ∈ Rd×d  we will denote by (cid:107)M(cid:107)2
its spectral norm and by tr(M) its trace. Let Bd = {x ∈ Rd : (cid:107)x(cid:107)2 ≤ 1} be the unit ball and
Sd−1 = {x ∈ Rd : (cid:107)x(cid:107)2 = 1} be the unit sphere in Rd. An origin-centered halfspace is a Boolean-
valued function hw : Rd → {±1} of the form hw(x) = sign ((cid:104)w  x(cid:105))  where w ∈ Rd. (Note that
class of all origin-centered halfspaces on Rd. Finally  we use ei to denote the i-th standard basis
vector  i.e.  the vector whose i-th coordinate is one and the remaining coordinates are zeros.

we may assume w.l.o.g. that (cid:107)w(cid:107)2 = 1.) Let Hd =(cid:8)hw(x) = sign ((cid:104)w  x(cid:105))   w ∈ Rd(cid:9) denote the

i=1 x2

2 Algorithm for Proper Agnostic Learning of Halfspaces with a Margin

In this section  we show the following theorem  which gives the upper bound part of Theorem 1.1:

6In other words  for any assignment to the variables  at least 0.1 fraction of the clauses are unsatisﬁed.
7ETH states that the exact version of 3SAT cannot be solved in 2o(n) time.

5

Theorem 2.1. Fix 0 < δ ≤ 1. There is an algorithm that uses O(1/(2γ2)) samples  runs in
time poly(d/) · 2 ˜O(1/(δγ2)) and is a (1 + δ)-agnostic proper learner for γ-margin halfspaces with
conﬁdence probability 9/10.

γ (w∗) ≤ OPTD
γ .

Our algorithm in this section produces a ﬁnite set of candidate weight vectors and outputs the one with
the smallest empirical γ/2-margin error. For the sake of this intuitive description  we will assume
that the algorithm knows the distribution D in question supported on Bd × {±1}. By assumption 
there is a unit vector w∗ so that errD
We note that if a hypothesis hw deﬁned by vector w has γ/2-margin error at least a (1 + δ)OPTD
γ  
then there must be a large number of points correctly classiﬁed with γ-margin by hw∗  but not correctly
classiﬁed with γ/2-margin by hw. For all of these points  we must have that |(cid:104)w∗ − w  x(cid:105)| ≥ γ/2.
This implies that the γ/2-margin-misclassiﬁed points of hw have a large covariance in the w∗ − w
direction. In particular  we have:
Claim 2.2. Let w ∈ Rd be such that errD
γ . Let D(cid:48) be D conditioned on
y(cid:104)w  x(cid:105) ≤ γ/2. Let MD(cid:48)
Proof. We claim that with probability at least δ/2 over (x  y) ∼ D(cid:48) we have that y(cid:104)w  x(cid:105) ≤ γ/2 and
y(cid:104)w∗  x(cid:105) ≥ γ. To see this  we ﬁrst note that Pr(x y)∼D(cid:48)[y(cid:104)w  x(cid:105) > γ/2] = 0 holds by deﬁnition of
D(cid:48). Hence  we have that

γ/2(w) > (1 + δ)OPTD
= E(x y)∼D(cid:48)[xxT ]. Then (w∗ − w)T MD(cid:48)

(w∗ − w) ≥ δγ2/8.

Pr(x y)∼D(cid:48)[y(cid:104)w∗  x(cid:105) ≤ γ] ≤ Pr(x y)∼D[y(cid:104)w∗  x(cid:105) ≤ γ]
Pr(x y)∼D[y(cid:104)w  x(cid:105) ≤ γ/2]

<

OPTD

γ

(1 + δ)OPTD

γ

=

1

(1 + δ)

.

By a union bound  we obtain Pr(x y)∼D(cid:48)[(y(cid:104)w  x(cid:105) > γ/2) ∪ (y(cid:104)w∗  x(cid:105) ≤ γ)] ≤ 1
Therefore  with probability at least δ/(1 + δ) ≥ δ/2 (since δ ≤ 1) over (x  y) ∼ D(cid:48) we have that
y(cid:104)w∗− w  x(cid:105) ≥ γ/2  which implies that (cid:104)w∗− w  x(cid:105)2 ≥ γ2/4. Thus  (w∗− w)T MD(cid:48)
(w∗− w) =
E(x y)∼D(cid:48)[((cid:104)w∗ − w  x(cid:105))2] ≥ δγ2/8  completing the proof.
Claim 2.2 says that w∗ − w has a large component on the large eigenvalues of MD(cid:48)
this claim  we obtain the following result:
Lemma 2.3. Let w∗  w  MD(cid:48)
the top k eigenvectors of MD(cid:48)

be as in Claim 2.2. There exists k ∈ Z+ so that if Vk is the span of
  we have that (cid:107)ProjVk

2 ≥ kδγ2/8.

(w∗ − w)(cid:107)2

. Building on

(1+δ).

is PSD and let 0 > λmax = λ1 ≥ λ2 ≥ . . . ≥ λd ≥ 0 be its set of
Proof. Note that the matrix MD(cid:48)
eigenvalues. We will denote by V≥t the space spanned by the eigenvectors of MD(cid:48)
corresponding to
eigenvalues of magnitude at least t. Let dt = dim(V≥t) be the dimension of V≥t  i.e.  the number
of i ∈ [d] with λi ≥ t. Since x is supported on the unit ball  for (x  y) ∼ D(cid:48)  we have that
tr(MD(cid:48)
i=1 λi and
we can write

) = E(x y)∼D(cid:48)[tr(xxT )] ≤ 1. Since MD(cid:48)

is PSD  we have that tr(MD(cid:48)

) =(cid:80)d

1 ≥ tr(MD(cid:48)

) =

d(cid:80)

d(cid:80)

λi(cid:82)

d(cid:80)

λmax(cid:82)

λi =

1dt =

i=1

i=1

0

i=1

0

λmax(cid:82)

0

1λi≥tdt =

dtdt 

(1)

d(cid:80)

where the last equality follows by changing the order of the summation and the integration. If the
projection of (w∗ − w) onto the i-th eigenvector of MD(cid:48)

i=1

i =

λia2

(w∗−w) =

δγ2/8 ≤ (w∗−w)T MD(cid:48)

(w∗−w)(cid:107)2
2dt 
(2)
where the ﬁrst inequality uses Claim 2.2  the ﬁrst equality follows by the Pythagorean theorem  and
the last equality follows by changing the order of the summation and the integration. Combining (1)
(w∗ − w)(cid:107)2
dtdt. By an averaging argument 
2 ≥ (δγ2/8)dt. Letting k = dt and noting
there exists 0 ≤ t ≤ λmax such that (cid:107)ProjV≥t
that V≥t = Vk completes the proof.

and (2)  we obtain(cid:82) λmax

(w∗ − w)(cid:107)2

(cid:107)ProjV≥t

0

0

λmax(cid:82)

d(cid:80)
2dt ≥ (δγ2/8)(cid:82) λmax

i=1

0

0

has (cid:96)2-norm ai  we have that
(cid:107)ProjV≥t

a2
i 1λi≥tdt =

λmax(cid:82)

6

Lemma 2.3 suggests a method for producing an approximation to w∗  or more precisely a vector
that produces empirical γ/2-margin error at most (1 + δ)OPTD
γ . We start by describing a non-
deterministic procedure  which we will then turn into an actual algorithm.
The method proceeds in a sequence of stages. At stage i  we have a hypothesis weight vector w(i).
(At stage i = 0  we start with w(0) = 0.) At any stage i  if errD
γ   then w(i)
is a sufﬁcient estimator. Otherwise  we consider the matrix M(i) = E(x y)∼D(i) [xxT ]  where D(i) is
D conditioned on y(cid:104)w(i)  x(cid:105) ≤ γ/2. By Lemma 2.3  we know that for some positive integer value
k(i)  we have that the projection of w∗ − w(i) onto Vk(i) has squared norm at least δk(i)γ2/8.
Let p(i) be this projection. We set w(i+1) = w(i) + p(i). Since the projection of w∗ − w(i) and its
complement are orthogonal  we have

γ/2(w(i)) ≤ (1 + δ)OPTD

2

2 =

i=0

s−1(cid:80)

i=0

s−1(cid:80)

(cid:107)w∗ − w(i+1)(cid:107)2

2 − δk(i)γ2/8  

2−(cid:107)w∗−w(s)(cid:107)2

2 ≤ (cid:107)w∗ − w(i)(cid:107)2

2 − (cid:107)w∗ − w(i+1)(cid:107)2

(cid:17) ≥ (δγ2/8)

2 − (cid:107)p(i)(cid:107)2
(3)
2 ≥ k(i)δγ2/8 (as follows from Lemma 2.3). Let s be

2 = (cid:107)w∗ − w(i)(cid:107)2
where the inequality uses the fact that (cid:107)p(i)(cid:107)2
the total number of stages. We can write
1 ≥ (cid:107)w∗−w(0)(cid:107)2
where the ﬁrst inequality uses that (cid:107)w∗ − w(0)(cid:107)2

(cid:16)(cid:107)w∗ − w(i)(cid:107)2
telescoping sum  and the third uses (3). Therefore  s ≤(cid:80)s−1

k(i)  
2 ≥ 0  the second notes the
2 = 1 and (cid:107)w∗ − w(s)(cid:107)2
i=0 k(i) ≤ 8/(δγ2). Therefore  the above
γ/2(w(s)) ≤ (1 + δ)OPTD
procedure terminates after at most 8/(δγ2) stages at some w(s) with errD
γ .
We now describe how to turn the above procedure into an actual algorithm. Our algorithm tries to
simulate the above described procedure by making appropriate guesses. In particular  we start by
guessing a sequence of positive integers k(i) whose sum is at most 8/(δγ2). This can be done in
2O(1/(δγ2)) ways. Next  given this sequence  our algorithm guesses the vectors w(i) over all s stages
in order. In particular  given w(i)  the algorithm computes the matrix M(i) and the subspace Vk(i) 
and guesses the projection p(i) ∈ Vk(i)  which then gives w(i+1). Of course  we cannot expect our
algorithm to guess p(i) exactly (as there are inﬁnitely many points in Vk(i))  but we can guess it
to within (cid:96)2-error poly(γ)  by taking an appropriate net. This involves an additional guess of size
(1/γ)O(k(i)) in each stage. In total  our algorithm makes 2 ˜O(1/(δγ2)) many different guesses.
We note that the sample version of our algorithm is essentially identical to the idealized version
described above  by replacing the distribution D by its empirical version and leveraging the following
statistical bound:
m = Ω(log(1/τ )/(2γ2))  and (cid:98)Dm be the empirical distribution on S. Then with probability at least
Fact 2.4 ([BM02  McA03]). Let S = {(x(i)  y(i))}m
i=1 be a multiset of i.i.d. samples from D  where
1 − τ over S  simultaneously for all unit vectors w and margins γ > 0  if hw(x) = sign((cid:104)w  x(cid:105)) 
we have that errD
The pseudo-code of our algorithm is given below in Algorithm 1.
To show the correctness of the algorithm  we begin by noting that the set C of candidate weight
vectors produced has size 2 ˜O(1/(δγ2)). This is because there are only 2O(1/(δγ2)) many possibil-
ities for the sequence of k(i)  and for each such sequence the product of the sizes of the C (i) is

0−1(hw) ≤ err(cid:98)Dm

(1/(δγ))O((cid:80) k(i)) = 2 ˜O(1/(δγ2)). We note that  by the aforementioned analysis  for any choice of
k(0)  . . .   k(i−1) and w(i)  we either have that err(cid:98)Dm
γ/2(w(i)) ≤ (1 + δ)OPT(cid:98)Dm
of k(i) and p(i) ∈ C (i) such that
2 ≤ (cid:107)w∗ − w(i)(cid:107)2
2 − δk(i)γ2/8 + O(δ2γ6)  
(cid:33)

algorithm  we either ﬁnd some w(i) with err(cid:98)Dm
γ/2(w(i)) ≤ (1 + δ)OPT(cid:98)Dm
(cid:32)i−1(cid:80)
2 ≤ 1 −

where we used (3) and the fact that C (i) is a δγ3-cover of Vk(i). Following the execution path of the

(cid:107)w∗ − w(i) − p(i)(cid:107)2

  or we ﬁnd a w(i) with

(cid:107)w∗ − w(i)(cid:107)2

or there is a choice

δγ2/8 + O(δγ4)  

k(j)

γ

γ

(w) + .

γ

j=0

7

Algorithm 1 Near-Optimal (1 + δ)-Agnostic Proper Learner
1: Draw a multiset S = {(x(i)  y(i))}m
i=1 of i.i.d.

2: Let (cid:98)Dm be the empirical distribution on S.

Ω(log(1/τ )/(2γ2)).

Let w(0) = 0.
for i = 0  1  . . .   s − 1 do

Let D(i) be (cid:98)Dm conditioned on y(cid:104)w(i)  x(cid:105) ≤ γ/2.

3: for all sequences k(0)  k(1)  . . .   k(s−1) of positive integers with sum at most 8/(δγ2) + 2 do
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
13: Let C denote the set of all w(i) generated in the above loop.

Let M(i) = E(x y)∼D(i) [xxT ].
Use SVD on M(i) to ﬁnd a basis for Vk(i)  the span of the top k(i) eigenvectors.
Let C (i) be a δγ3-cover  in (cid:96)2-norm  of Vk(i) ∩ Bd of size (1/(δγ))O(k(i)).
For each p(i) ∈ C (i) repeat the next step of the for loop with w(i+1) = w(i) + p(i).

end for

14: Let v ∈ argminw∈Cerr(cid:98)Dm
15: return Output the hypothesis hv(x) = sign((cid:104)v  x(cid:105)).

γ/2(w).

samples from D  where m =

where the last term is an upper bound for

(cid:16)(cid:80)i−1
j=0 k(j)(cid:17)· O(δ2γ6). Note that this sequence terminates
in at most O(1/(δγ2)) stages  when it becomes impossible that(cid:80) k(j) > 8/(δγ2) + 1. Thus  the
output of our algorithm must contain some weight vector v with err(cid:98)Dm
γ/2(v) ≤ (1 + δ)OPT(cid:98)Dm

proof now follows by an application of Fact 2.4. This completes the proof of Theorem 2.1.

. The

γ

3 Computational Hardness Results

γ   1

γ   1

ε )” to mean that no T (d  1

In this section  we provide several computational lower bounds for agnostic learning of halfspaces
with a margin. To clarify the statements below  we note that we say “there is no algorithm that
ε )-time algorithm works for all combinations of
runs in time T (d  1
parameters d  γ and ε. (Note that we discuss the lower bounds with stronger quatiﬁers in the full
version [DKM19].) Moreover  we also ignore the dependency on τ (the probability that the learner
can be incorrect)  since we only use a ﬁxed τ (say 1/3) in all the bounds below.
First  we show that  for any constant α > 1  α-agnostic learning of γ-margin halfspaces requires
2(1/γ)2−o(1)
poly(d  1/ε) time. Up to the lower order term γo(1) in the exponent  this matches with
our algorithm (in Theorem 2.1). In fact  we show an even stronger result  that if the dependency of
the running time on the margin is say 2(1/γ)1.99  then one has to pay 2d1−o(1) in the running time.
This result holds assuming the so-called (randomized) exponential time hypothesis (ETH) [IP01 
IPZ01]  which postulates that there is no (randomized) algorithm that can solve 3SAT in time 2o(n) 
where n denotes the number of variables. ETH is a standard hypothesis used in proving (tight)
running time lower bounds. We do not discuss ETH further here  but interested readers may refer to a
survey by Lokshtanov et al. [LMS11] for an in-depth discussion and several applications of ETH.
Our ﬁrst lower bound can be stated more precisely as follows:
Theorem 3.1. Assuming the (randomized) ETH  for any universal constant α ≥ 1  there is no proper
α-agnostic learner for γ-margin halfspaces that runs in time O(2(1/γ)2−o(1)
ε ) for any
function f.

2d1−o(1)

)f ( 1

Secondly  we address the question of whether we can achieve α = 1 (standard agnostic learning)
while retaining running time similar to our algorithm. We answer this in the negative (assuming a
ε )-time 1-agnostic learner
standard parameterized complexity assumption): there is no f ( 1
). This demonstrates a stark contrast between what

γ )poly(d  1

γ ) = 2221/γ
for any function f (e.g.  even for f ( 1
we can achieve with and without approximation.

8

Theorem 3.2. Assuming W[1] is not contained in randomized FPT  there is no proper 1-agnostic
learner for γ-margin halfspaces that runs in time f ( 1

ε ) for any function f.

γ )poly(d  1

ε   1

Finally  we explore the other extreme of the trade-off between the running time and approximation
ratio  by asking: what is the best approximation ratio we can achieve if we only consider proper
learners that run in poly(d  1
γ )-time? On this front  it is known [Ser01] that the perceptron
algorithm achieves 1/γ-approximation. We show that a signiﬁcant improvement over this is unlikely 
by showing that (1/γ)
polyloglog(1/γ) -approximation is not possible unless NP = RP. If we additionally
assume the so-called Sliding Scale Conjecture [BGLR94]  this ratio can be improved to (1/γ)c for
some constant c > 0.
Theorem 3.3. Assuming NP (cid:54)= RP  there is no proper (1/γ)1/polyloglog(1/γ)-agnostic learner for γ-
γ ). Furthermore  assuming NP (cid:54)= RP and the Sliding
margin halfspaces that runs in time poly(d  1
Scale Conjecture [BGLR94]  there is no proper (1/γ)c-agnostic learning for γ-margin halfspaces
that runs in time poly(d  1

γ ) for some constant c > 0.

ε   1

ε   1

1

Due to the technical nature of the Sliding Scale Conjecture  we do not state it in full here; please refer
to the full version for a formal statement [DKM19].
We note here that the constant c in Theorem 3.3 is not explicit  i.e.  it depends on the constant from
the Sliding Scale Conjecture (SSC). Moreover  even when assuming the most optimistic parameters
of SSC  the constant c we can get is still very small. For instance  it is still possible that a say

(cid:112)1/γ-agnostic learning algorithm that runs in polynomial time exists  and this remains an interesting

open question. We remark that Daniely et al. [DLS14] have made partial progress in this direction
γ )-time learner that belongs to a “generalized linear family” cannot
by showing that  any poly(d  1
achieve approximation ratio α better than Ω
. We note that the inapproximability ratio
of [DLS14] is close to being tight for a natural  yet restricted  family of improper learners. On the
other hand  our proper hardness result holds against all proper learners under a widely believed
worst-case complexity assumption.
Due to space limitations  the proofs of our hardness results are deferred to the full version of this
work [DKM19].

polylog(1/γ)

(cid:17)

(cid:16)

ε   1

1/γ

4 Conclusions and Open Problems

This work gives nearly tight upper and lower bounds for the problem of α-agnostic proper learning
of halfspaces with a margin  for α = O(1). Our upper and lower bounds for α = ω(1) are far from
tight. Closing this gap is an interesting open problem. Charactering the ﬁne-grained complexity of
the problem for improper learning algorithms remains a challenging open problem.
More broadly  an interesting direction for future work would be to generalize our agnostic learning
results to broader classes of geometric functions. Finally  we believe that ﬁnding further connections
between the problem of agnostic learning with a margin and adversarially robust learning is an
intriguing direction to be explored.

Acknowledgments

Part of this work was performed while Ilias Diakonikolas was at the Simons Institute for the Theory
of Computing during the program on Foundations of Data Science. Ilias Diakonikolas is supported
by Supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. Daniel M.
Kane is supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship.

9

References

[ABL17] P. Awasthi  M. F. Balcan  and P. M. Long. The power of localization for efﬁciently

learning linear separators with noise. J. ACM  63(6):50:1–50:27  2017.

[ALM+98] S. Arora  C. Lund  R. Motwani  M. Sudan  and M. Szegedy. Proof veriﬁcation and the

hardness of approximation problems. J. ACM  45(3):501–555  1998.

[AS98] S. Arora and S. Safra. Probabilistic checking of proofs: A new characterization of NP.

J. ACM  45(1):70–122  1998.

[BGLR94] M. Bellare  S. Goldwasser  C. Lund  and A. Russell. Efﬁcient probabilistic checkable
proofs and applications to approximation. In Proceedings of the Twenty-Sixth Annual
ACM Symposium on Theory of Computing  page 820  1994.

[BGS18] A. Bhattacharyya  S. Ghoshal  and R. Saket. Hardness of learning noisy halfspaces
using polynomial thresholds. In Conference On Learning Theory  COLT 2018  pages
876–917  2018.

[BLPR19] S. Bubeck  Y. T. Lee  E. Price  and I. P. Razenshteyn. Adversarial examples from
computational constraints. In Proceedings of the 36th International Conference on
Machine Learning  ICML 2019  pages 831–840  2019.

[BM02] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3:463–482  2002.

[BS00] S. Ben-David and H. Ulrich Simon. Efﬁcient learning of linear perceptrons. In Advances

in Neural Information Processing Systems (NIPS) 2000  pages 189–195  2000.

[BS12] A. Birnbaum and S. Shalev-Shwartz. Learning halfspaces with the zero-one loss: Time-
accuracy tradeoffs. In Advances in Neural Information Processing Systems 25: NIPS
2012  pages 935–943  2012.

[Cho61] C.K. Chow. On the characterization of threshold functions. In Proceedings of the
Symposium on Switching Circuit Theory and Logical Design (FOCS)  pages 34–38 
1961.

[Dan16] A. Daniely. Complexity theoretic limitations on learning halfspaces. In Proceedings
of the 48th Annual Symposium on Theory of Computing  STOC 2016  pages 105–117 
2016.

[DDFS14] A. De  I. Diakonikolas  V. Feldman  and R. A. Servedio. Nearly optimal solutions for
the chow parameters problem and low-weight approximation of halfspaces. J. ACM 
61(2):11:1–11:36  2014.

[Din07] I. Dinur. The PCP theorem by gap ampliﬁcation. J. ACM  54(3):12  2007.

[DKK+16] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCS’16  pages 655–664  2016.

[DKK+17] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Being
robust (in high dimensions) can be practical. In Proceedings of the 34th International
Conference on Machine Learning  ICML 2017  pages 999–1008  2017.

[DKK+18] I. Diakonikolas  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart. Robustly
learning a gaussian: Getting optimal error  efﬁciently. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms  SODA 2018  pages 2683–2702 
2018.

[DKK+19] I. Diakonikolas  G. Kamath  D. Kane  J. Li  J. Steinhardt  and A. Stewart. Sever: A robust
meta-algorithm for stochastic optimization. In Proceedings of the 36th International
Conference on Machine Learning  ICML 2019  pages 1596–1606  2019.

10

[DKM19] I. Diakonikolas  D. M. Kane  and P. Manurangsi. Nearly tight bounds for robust proper

learning of halfspaces with a margin. CoRR  abs/1908.11335  2019.

[DKS18] I. Diakonikolas  D. M. Kane  and A. Stewart. Learning geometric concepts with nasty
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of

noise.
Computing  STOC 2018  pages 1061–1073  2018.

[DKS19] I. Diakonikolas  W. Kong  and A. Stewart. Efﬁcient algorithms and lower bounds for
robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms  SODA 2019  pages 2745–2754  2019.

[DLS14] A. Daniely  N. Linial  and S. Shalev-Shwartz. The complexity of learning halfspaces
using generalized linear methods. In Proceedings of The 27th Conference on Learning
Theory  COLT 2014  pages 244–286  2014.

[DNV19] Akshay Degwekar  Preetum Nakkiran  and Vinod Vaikuntanathan. Computational
limitations in robust classiﬁcation and win-win results. In Conference on Learning
Theory  COLT 2019  25-28 June 2019  Phoenix  AZ  USA  pages 994–1028  2019.

[DOSW11] I. Diakonikolas  R. O’Donnell  R. Servedio  and Y. Wu. Hardness results for agnostically
learning low-degree polynomial threshold functions. In SODA  pages 1590–1606  2011.

[FGKP06] V. Feldman  P. Gopalan  S. Khot  and A. Ponnuswami. New results for learning noisy

parities and halfspaces. In Proc. FOCS  pages 563–576  2006.

[FS97] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and System Sciences  55(1):119–139 
1997.

[GR06] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In Proc.
47th IEEE Symposium on Foundations of Computer Science (FOCS)  pages 543–552.
IEEE Computer Society  2006.

[Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and

other learning applications. Information and Computation  100:78–150  1992.

[IP01] R. Impagliazzo and R. Paturi. On the complexity of k-sat. J. Comput. Syst. Sci. 

62(2):367–375  2001.

[IPZ01] R. Impagliazzo  R. Paturi  and F. Zane. Which problems have strongly exponential

complexity? J. Comput. Syst. Sci.  63(4):512–530  2001.

[KKM18] A. R. Klivans  P. K. Kothari  and R. Meka. Efﬁcient algorithms for outlier-robust
regression. In Conference On Learning Theory  COLT 2018  pages 1420–1430  2018.

[KL93] M. J. Kearns and M. Li. Learning in the presence of malicious errors. SIAM Journal on

Computing  22(4):807–837  1993.

[KLS09] A. Klivans  P. Long  and R. Servedio. Learning halfspaces with malicious noise. To
appear in Proc. 17th Internat. Colloq. on Algorithms  Languages and Programming
(ICALP)  2009.

[KSS94] M. Kearns  R. Schapire  and L. Sellie. Toward Efﬁcient Agnostic Learning. Machine

Learning  17(2/3):115–141  1994.

[LMS11] D. Lokshtanov  D. Marx  and S. Saurabh. Lower bounds based on the exponential time

hypothesis. Bulletin of the EATCS  105:41–72  2011.

[LRV16] K. A. Lai  A. B. Rao  and S. Vempala. Agnostic estimation of mean and covariance. In

Proceedings of FOCS’16  2016.

[LS11] P. Long and R. Servedio. Learning large-margin halfspaces with more malicious noise.

NIPS  2011.

11

[McA03] D. A. McAllester. Simpliﬁed PAC-bayesian margin bounds. In 16th Annual Conference

on Computational Learning Theory  pages 203–215  2003.

[MHS19] O. Montasser  S. Hanneke  and N. Srebro. VC classes are adversarially robustly learnable 
but only improperly. In Conference on Learning Theory  COLT 2019  pages 2512–2530 
2019.

[MR10] D. Moshkovitz and R. Raz. Two-query PCP with subconstant error. J. ACM  57(5):29:1–

29:29  2010.

[Nak19] P. Nakkiran. Adversarial robustness may be at odds with simplicity. CoRR 

abs/1901.00532  2019.

[Ros58] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and

organization in the brain. Psychological Review  65:386–407  1958.

[Ser01] R. Servedio. Smooth boosting and learning with malicious noise. In Proceedings of
the Fourteenth Annual Conference on Computational Learning Theory  pages 473–489 
2001.

[SSS09] S. Shalev-Shwartz  O. Shamir  and K. Sridharan. Agnostically learning halfspaces with

margin errors. In Technical report  Toyota Technological Institute  2009.

[SSS10] S. Shalev-Shwartz  O. Shamir  and K. Sridharan. Learning kernel-based halfspaces with
the zero-one loss. In The 23rd Conference on Learning Theory  COLT 2010  pages
441–450  2010.

[Val85] L. Valiant. Learning disjunctions of conjunctions. In Proc. 9th IJCAI  pages 560–566 

1985.

[Vap98] V. Vapnik. Statistical Learning Theory. Wiley-Interscience  New York  1998.

12

,Ilias Diakonikolas
Daniel Kane
Pasin Manurangsi