2019,On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective,We consider training over-parameterized two-layer neural networks with Rectified Linear Unit (ReLU) using gradient descent (GD) method. Inspired by a recent line of work  we study the evolutions of network prediction errors across GD iterations  which can be  neatly described in a matrix form. When the network is sufficiently over-parameterized  these matrices individually approximate {\em an} integral operator which is determined by the feature vector distribution $\rho$ only. Consequently  GD method can be viewed as {\em approximately} applying the powers of this integral operator on the underlying/target function $f^*$ that generates the responses/labels. 
 
We show that if $f^*$ admits a low-rank approximation with respect to the eigenspaces of this integral operator  then the empirical risk decreases to this low rank approximation error at a linear rate which is determined by $f^*$ and $\rho$ only  i.e.  the rate is independent of the sample size $n$. Furthermore  if $f^*$ has zero low-rank approximation error  then  as long as the width of the neural network is $\Omega(n\log n)$  the empirical risk decreases to $\Theta(1/\sqrt{n})$. To the best of our knowledge  this is the first result showing the sufficiency of nearly-linear network over-parameterization.  We provide an application of our general results to the setting where $\rho$ is the uniform distribution on the spheres and $f^*$ is a polynomial. Throughout this paper  we consider the scenario where the input dimension $d$ is fixed.,On Learning Over-parameterized Neural Networks:

A Functional Approximation Perspective

Lili Su

CSAIL  MIT
lilisu@mit.edu

Pengkun Yang

Department of Electrical Engineering

Princeton University

pengkuny@princeton.edu

Abstract

We consider training over-parameterized two-layer neural networks with Rectiﬁed
Linear Unit (ReLU) using gradient descent (GD) method. Inspired by a recent
line of work  we study the evolutions of network prediction errors across GD
iterations  which can be neatly described in a matrix form. When the network
is sufﬁciently over-parameterized  these matrices individually approximate an
integral operator which is determined by the feature vector distribution ⇢ only.
Consequently  GD method can be viewed as approximately applying the powers of
this integral operator on the underlying function f⇤ that generates the responses.
We show that if f⇤ admits a low-rank approximation with respect to the eigenspaces
of this integral operator  then the empirical risk decreases to this low-rank approxi-
mation error at a linear rate which is determined by f⇤ and ⇢ only  i.e.  the rate is
independent of the sample size n. Furthermore  if f⇤ has zero low-rank approx-
imation error  then  as long as the width of the neural network is ⌦(n log n)  the
empirical risk decreases to ⇥(1/pn). To the best of our knowledge  this is the ﬁrst
result showing the sufﬁciency of nearly-linear network over-parameterization. We
provide an application of our general results to the setting where ⇢ is the uniform
distribution on the spheres and f⇤ is a polynomial. Throughout this paper  we
consider the scenario where the input dimension d is ﬁxed.

1

Introduction

Neural networks have been successfully applied in many real-world machine learning applications.
However  a thorough understanding of the theory behind their practical success  even for two-layer
neural networks  is still lacking. For example  despite learning optimal neural networks is provably
NP-complete [BG17  BR89]  in practice  even the neural networks found by the simple ﬁrst-order
methods perform well [KSH12]. Additionally  in sharp contrast to traditional learning theory  over-
parameterized neural networks (more parameters than the size of the training dataset) are observed
to enjoy smaller training and even smaller generalization errors [ZBH+16]. In this paper  we focus
on training over-parameterized two-layer neural networks with Rectiﬁed Linear Unit (ReLU) using
gradient descent (GD) method. Our results can be extended to other activation functions that satisfy
some regularity conditions; see [GMMM19  Theorem 2] for an example. The techniques derived and
insights obtained in this paper might be applied to deep neural networks as well  for which similar
matrix representation exists [DZPS18].
Signiﬁcant progress has been made in understanding the role of over-parameterization in training
neural networks with ﬁrst-order methods [AZLL18  DZPS18  ADH+19  OS19  MMN18  LL18 
ZCZG18  DLL+18  AZLS18  CG19]; with proper random network initialization  (stochastic) GD
converges to a (nearly) global minimum provided that the width of the network m is polynomially
large in the size of the training dataset n. However  neural networks seem to interpolate the training

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

data as soon as the number of parameters exceed the size of the training dataset by a constant
factor [ZBH+16  OS19]. To the best of our knowledge  a provable justiﬁcation of why such mild
over-parametrization is sufﬁcient for successful gradient-based training is still lacking. Moreover 
the convergence rates derived in many existing work approach 0 as n ! 1; see Section A in
Supplementary Material for details. In many applications the volumes of the datasets are huge –
the ImageNet dataset [DDS+09] has 14 million images. For those applications  a non-diminishing
(i.e.  constant w. r. t. n) convergence rate is more desirable. In this paper  our goal is to characterize
a constant (w. r. t. n) convergence rate while improving the sufﬁciency guarantee of network over-
parameterization. Throughout this paper  we focus on the setting where the dimension of the feature
vector d is ﬁxed  leaving the high dimensional region as one future direction.
Inspired by a recent line of work [DZPS18  ADH+19]  we focus on characterizing the evolutions of
the neural network prediction errors under GD method. This focus is motivated by the fact that the
neural network representation/approximation of a given function might not be unique [KB18]  and
this focus is also validated by experimental neuroscience [MG06  ASCC18].

Contributions
It turns out that the evolution of the network prediction error can be neatly described
in a matrix form. When the network is sufﬁciently over-parameterized  the matrices involved
individually approximate an integral operator which is determined by the feature vector distribution ⇢
only. Consequently  GD method can be viewed as approximately applying the powers of this integral
operator on the underlying/target function f⇤ that generates the responses/labels. The advantages of
taking such a functional approximation perspective are three-fold:

• We showed in Theorem 2 and Corollary 1 that the existing rate characterizations in the
inﬂuential line of work [DZPS18  ADH+19  DLL+18] approach zero (i.e.  ! 0) as n ! 1.
This is because the spectra of these matrices  as n diverges  concentrate on the spectrum of
the integral operator  in which the unique limit of the eigenvalues is zero.

• We show in Theorem 4 that the training convergence rate is determined by how f⇤ can be
decomposed into the eigenspaces of an integral operator. This observation is also validated
by a couple of empirical observations: (1) The spectrum of the MNIST data concentrates
on the ﬁrst a few eigenspaces [LBB+98]; and (2) the training is slowed down if labels are
partially corrupted [ZBH+16  ADH+19].

• We show in Corollary 2 that if f⇤ can be decomposed into a ﬁnite number of eigenspaces of
the integral operator  then m = ⇥(n log n) is sufﬁcient for the training error to converge to
⇥(1/pn) with a constant convergence rate. To the best of our knowledge  this is the ﬁrst
result showing the sufﬁciency of nearly-linear network over-parameterization.

Notations For any n  m 2 N  let [n] := {1 ···   n} and [m] := {1 ···   m}. For any d 2 N 
denote the unit sphere as S d1 :=x : x 2 Rd  & kxk = 1   where k·k is the standard `2 norm
when it is applied to a vector. We also use k·k for the spectral norm when it is applied to a matrix. The
Frobenius norm of a matrix is denoted by k·kF . Let L2(S d1  ⇢) denote the space of functions with
⇢ are deﬁned as hf  gi⇢ :=RSd1 f (x)g(x)d⇢(x)
ﬁnite norm  where the inner product h· ·i⇢ and k · k2
⇢ :=RSd1 f 2(x)d⇢(x) < 1. We use standard Big-O notations  e.g.  for any sequences
and kfk2
{ar} and {br}  we say ar = O(br) or ar . br if there is an absolute constant c > 0 such that ar
br  c 
we say ar = ⌦(br) or ar & br if br = O(ar) and we say ar = !(br) if limr!1 |ar/br| = 1.

2 Problem Setup and Preliminaries

Statistical learning We are given a training dataset {(xi  yi) : 1  i  n} which consists of n
tuples (xi  yi)  where xi’s are feature vectors that are identically and independently generated from a
common but unknown distribution ⇢ on Rd  and yi = f⇤(xi). We consider the problem of learning
the unknown function f⇤ with respect to the square loss. We refer to f⇤ as a target function. For
simplicity  we assume xi 2 S d1 and yi 2 [1  1]. In this paper  we restrict ourselves to the family
of ⇢ that is absolutely continuous with respect to Lebesgue measure. We are interested in ﬁnding
a neural network to approximate f⇤. In particular  we focus on two-layer fully-connected neural

2

networks with ReLU activation  i.e. 

fW  a(x) =

1
pm

mXj=1

aj [hx  wji]+   8 x 2 S d1 

(1)

where m is the number of hidden neurons and is assumed to be even  W = (w1 ···   wm) 2 Rd⇥m
are the weight vectors in the ﬁrst layer  a = (a1 ···   am) with aj 2 {1  1} are the weights in the
second layer  and [·]+ := max{·  0} is the ReLU activation function.
Many authors assume f⇤ is also a neural network [MMN18  AZLL18  SS96  LY17  Tia16]. Despite
this popularity  a target function f⇤ is not necessarily a neural network. One advantage of working
with f⇤ directly is  as can be seen later  certain properties of f⇤ are closely related to whether f⇤ can
be learned quickly by GD method or not. Throughout this paper  for simplicity  we do not consider
the scaling in d and treat d as a constant.

Empirical risk minimization via gradient descent For each k = 1 ···   m/2:
Initialize
2. Initialize
w2k1 ⇠ N (0  I)  and a2k1 = 1 with probability 1
2  and a2k1 = 1 with probability 1
w2k = w2k1 and a2k = a2k1. All randomnesses in this initialization are independent  and are
independent of the dataset. This initialization is chosen to guarantee zero output at initialization.
Similar initialization is adopted in [CB18  Section 3] and [WGL+19]. 1 We ﬁx the second layer a
and optimize the ﬁrst layer W through GD on the empirical risk w. r. t. square loss 2:

Ln(W ) :=

1
2n

nXi=1h(yi  fW (xi))2i .

For notational convenience  we drop the subscript a in fW  a. The weight matrix W is update as

W t+1 = W t  ⌘

@Ln(W t)

@W t

 

(3)

where ⌘ > 0 is stepsize/learning rate  and W t is the weight matrix at the end of iteration t with W 0
denoting the initial weight matrix. For ease of exposition  let

(2)

(4)

(5)

byi(t) := fW t(xi) =

1
pm

aj⇥⌦wt

j  xi↵⇤+   8 i = 1 ···   n.

Notably byi(0) = 0 for i = 1 ···   n. It can be easily deduced from (3) that wj is updated as

wt+1
j = wt

j +

⌘aj
npm

(yi byi(t)) xi1{hwt

j  xii>0}.

Matrix representation Let y 2 Rn be the vector that stacks the responses of {(xi  yi)}n
by(t) be the vector that stacksbyi(t) for i = 1 ···   n at iteration t. Additionally  let A := {j : aj = 1}
and B := {j : aj = 1} . The evolution of (y by(t)) can be neatly described in a matrix form.
Deﬁne matrices H + fH +  and H fH in Rn ⇥ Rn as: For t  0  and i  i0 2 [n] 
j  xii>0} 
 xii>0} 

j  xi0i>0}1{hwt
j  xi0i>0}1{hwt+1

nm hxi  xi0iXj2A
nm hxi  xi0iXj2A

1{hwt
1{hwt

ii0(t + 1) =

ii0(t + 1) =

i=1. Let

1

1

H +

(6)

(7)

j

and Hii0(t + 1) fHii0(t + 1) are deﬁned similarly by replacing the summation over all the hidden
neurons in A in (6) and (7) by the summation over B. It is easy to see that both H + and H are
1Our analysis might be adapted to other initialization schemes  such as He initialization  with m = ⌦(n2).

Nevertheless  the more stringent requirement on m might only be an artifact of our analysis.

2The simpliﬁcation assumption that the second layer is ﬁxed is also adopted in [DZPS18  ADH+19]. Similar
frozen assumption is adopted in [ZCZG18  AZLS18]. We do agree this assumption might restrict the applicability
of our results. Nevertheless  even this setting is not well-understood despite the recent intensive efforts.

fH +

mXj=1
nXi=1

3

j

positive semi-deﬁnite. The only difference between H +

j  xii>0} is used in the former  whereas 1{hwt+1

ii0(t + 1) (or Hii0(t + 1)) andfH +

ii0(t + 1) (or
fHii0(t+1)) is that 1{hwt
 xii>0} is adopted in the latter.
When a neural network is sufﬁciently over-parameterized (in particular  m = ⌦(poly(n)))  the sign
changes of the hidden neurons are sparse; see [AZLL18  Lemma 5.4] and [ADH+19  Lemma C.2]
for details. The sparsity in sign changes suggests that bothfH +(t) ⇡ H +(t) andfH(t) ⇡ H(t)
are approximately PSD.
Theorem 1. For any iteration t  0 and any stepsize ⌘ > 0  it is true that
⇣I  ⌘⇣fH +(t + 1) + H(t + 1)⌘⌘ (y by(t))
 (y by(t + 1))
⇣I  ⌘⇣H +(t + 1) +fH(t + 1)⌘⌘ (y by(t))  
Theorem 1 says that when the sign changes are sparse  the dynamics of (y by(t)) are governed by a

sequence of PSD matrices. Similar observation is made in [DZPS18  ADH+19].

where the inequalities are entry-wise.

3 Main Results

We ﬁrst show (in Section 3.1) that the existing convergence rates that are derived based on minimum
eigenvalues approach 0 as the sample size n grows. Then  towards a non-diminishing convergence
rate  we characterize (in Section 3.2) how the target function f⇤ affects the convergence rate.

3.1 Convergence rates based on minimum eigenvalues
Let H := H +(1) + H(1). It has been shown in [DZPS18] that when the neural networks are

convergence rates with high probability can be upper bounded as 3

ky by(t)k  (1  ⌘min(H))t ky by(0)k = exp✓t log

sufﬁciently over-parameterized m = ⌦(n6)  the convergence of ky by(t)k and the associated
where min(H) is the smallest eigenvalue of H. Equality (8) holds because ofby(0) = 0. In this
paper  we refer to log
1⌘min(H) as convergence rate. The convergence rate here is quite appealing
at ﬁrst glance as it is independent of the target function f⇤. Essentially (8) says that no matter how
the training data is generated  via GD  we can always ﬁnd an over-parameterized neural network that
perfectly ﬁts/memorizes all the training data tuples exponentially fast! Though the spectrum of the
random matrix H can be proved to concentrate as n grows  we observe that min(H) converges to 0
as n diverges  formally shown in Theorem 2.
Theorem 2. For any data distribution ⇢  there exists a sequence of non-negative real numbers
1  2  . . . (independent of n) satisfying limi!1 i = 0 such that  with probability 1   

1  ⌘min(H)◆kyk  

(8)

1

1

wheree1  ··· en are the spectrum of H. In addition  if m = !(log n)  we have
where P! denotes convergence in probability.
A numerical illustration of the decay of min(H) in n is presented in Fig. 1a. Theorem 2 is proved in
Appendix D. By Theorem 2  the convergence rate in (8) approaches zero as n ! 1.
Corollary 1. For any ⌘ = O(1)  it is true that log

min(H) P! 0 

as n ! 1 

(10)

1

1⌘min(H) ! 0 as n ! 1.

3 Though a reﬁned analysis of that in [DZPS18] is given by [ADH+19  Theorem 4.1]  the analysis crucially

relies on the convergence rate in (8).

4

sup

i

|i ei| r log(4n2/)

m

+r 8 log(4/)

n

.

(9)

(a) The minimum eigenvalues of one realization of H
under different n and d  with network width m = 2n.

(b) The spectrum of K with d = 10  n = 500 concen-
trates around that of LK.

Figure 1: The spectra of H  K  and LK when ⇢ is the uniform distribution over S d1.

In Corollary 1  we restrict our attention to ⌘ = O(1). This is because the general analysis of GD
[Nes18] adopted by [ADH+19  DZPS18] requires that (1  ⌘max(H)) > 0  and by the spectrum
concentration given in Theorem 2  the largest eigenvalue of H concentrates on some strictly positive
value as n diverges  i.e.  max(H) = ⇥(1). Thus  if ⌘ = !(1)  then (1  ⌘max(H)) < 0 for any
sufﬁciently large n  violating the condition assumed in [ADH+19  DZPS18].
Theorem 2 essentially follows from two observations. Let K = E [H]  where the expectation is
taken with respect to the randomness in the network initialization. It is easy to see that by standard
concentration argument  for a given dataset  the spectrum of K and H are close with high probability.
In addition  the spectrum of K  as n increases  concentrates on the spectrum of the following integral
operator LK on L2(S d1  ⇢) 

(LKf )(x) :=ZSd1 K(x  s)f (s)d⇢ 

(11)

with the kernel function:

K(x  s) := hx  si

2⇡

(⇡  arccoshx  si) 8 x  s 2 S d1 

(12)
which is bounded over S d1 ⇥ S d1. In fact  1  2  ··· in Theorem 2 are the eigenvalues
of LK. As supx s2Sd1 K(x  s)  1
2  it is true that i  1 for all i  1. Notably  by deﬁnition 
nK(xi  xi0) is the empirical kernel matrix on the feature vectors of the given
Kii0 = E [Hii0] = 1
dataset {(xi  yi) : 1  i  n}. A numerical illustration of the spectrum concentration of K is given
in Fig. 1b; see  also  [XLS17].
Though a generalization bound is given in [ADH+19  Theorem 5.1 and Corollary 5.2]  it is unclear
how this bound scales in n. In fact  if we do not care about the structure of the target function f⇤ and
allow ypn to be arbitrary  this generalization bound might not decrease to zero as n ! 1. A detailed
argument and a numerical illustration can be found in Appendix B.

3.2 Constant convergence rates
Recall that f⇤ denotes the underlying function that generates output labels/responses (i.e.  y’s) given
input features (i.e.  x’s). For example  f⇤ could be a constant function or a linear function. Clearly 
the difﬁculty in learning f⇤ via training neural networks should crucially depend on the properties
of f⇤ itself. We observe that the training convergence rate might be determined by how f⇤ can
be decomposed into the eigenspaces of the integral operator deﬁned in (11). This observation is
also validated by a couple of existing empirical observations: (1) The spectrum of the MNIST data
[LBB+98] concentrates on the ﬁrst a few eigenspaces; and (2) the training is slowed down if labels
are partially corrupted [ZBH+16  ADH+19]. Compared with [ADH+19]  we use spectral projection
concentration to show how the random eigenvalues and the random projections in [ADH+19  Eq.(8)
in Theorem 4.1] are controlled by f⇤ and ⇢.

We ﬁrst present a sufﬁcient condition for the convergence of ky by(t)k.

5

02004006008001000n105104103102min(H)d=5d=10d=15d=20For any  2 (0  1

4 ) and given T > 0  if

m 

32
c2

then with probability at least 1    the following holds for all t  T :

1
pn


(I  ⌘K)t y  (1  ⌘c0)t + c1  8 t.
1 ✓ 1
+ 2⌘T c1◆4
 ✓ 1
(y by(t))  (1  ⌘c0)t + 2c1.


1
pn

+ 4 log

4n

c0

c0

+ 2⌘T c1◆2!  

(13)

(14)

(15)

Theorem 3 (Sufﬁciency). Let 0 < ⌘ < 1. Suppose there exist c0 2 (0  1) and c1 > 0 such that

i=1 y2

Theorem 3 is proved in Appendix E. Theorem 3 says that if 1pn (I  ⌘K)t y converges to c1
exponentially fast  then 1pn (y by(t)) converges to 2c1 with the same convergence rate guarantee
Roughly speaking  in our setup  yi = ⇥(1) and kyk =pPn

provided that the neural network is sufﬁciently parametrized. Recall that yi 2 [1  1] for each i 2 [n].
i = ⇥(pn). Thus we have the 1pn
scaling in (13) and (14) for normalization purpose.
Similar results were shown in [DZPS18  ADH+19] with ⌘ = min(K)
  c0 = nmin(K) and c1 = 0.
But the obtained convergence rate log
min(K) ! 0 as n ! 1. In contrast  as can be seen later
(in Corollary 2)  if f⇤ lies in the span of a small number of eigenspaces of the integral operator
in (11)  then we can choose ⌘ = ⇥(1)  choose c0 to be a value that is determined by the target
function f⇤ and the distribution ⇢ only  and choose c1 = ⇥( 1pn ). Thus  the resulting convergence
does not approach 0 as n ! 1. The additive term c1 = ⇥(1/pn) arises from the
rate log
fact that only ﬁnitely many data tuples are available. Both the proof of Theorem 3 and the proofs
in [DZPS18  ADH+19  AZLL18] are based on the observation that when the network is sufﬁciently
over-parameterized  the sign changes (activation pattern changes) of the hidden neurons are sparse.
Different from [DZPS18  ADH+19]  our proof does not use min(K); see Appendix E for details.
It remains to show  with high probability  (13) in Theorem 3 holds with properly chosen c0 and c1.
By the spectral theorem [DS63  Theorem 4  Chapter X.3] and [RBV10]  LK has a spectrum with
distinct eigenvalues µ1 > µ2 > ··· 4 such that

1⌘c0

12

n

1

1

LK =Xi1

µiPµi  with Pµi :=

(I  LK)1d 

1

2⇡iZµi

where Pµi : L2(S d1  ⇢) ! L2(S d1  ⇢) is the orthogonal projection operator onto the eigenspace
associated with eigenvalue µi; here (1) i is the imaginary unit  and (2) the integral can be taken over
any closed simple rectiﬁable curve (with positive direction) µi containing µi only and no other
distinct eigenvalue. In other words  Pµif is the function obtained by projecting function f onto the
eigenspaces of the integral operator LK associated with µi.
Given an ` 2 N  let m` be the sum of the multiplicities of the ﬁrst ` nonzero top eigenvalues of LK.
That is  m1 is the multiplicity of µ1 and (m2  m1) is the multiplicity of µ2. By deﬁnition 

m` = µ` 6= µ`+1 = m`+1  8 `.

Theorem 4. For any `  1 such that µi > 0  for i  `  let
f⇤(x)  ( X1i`

✏(f⇤  `) := sup

x2Sd1

Pµif⇤)(x)

be the approximation error of the span of the eigenspaces associated with the ﬁrst ` dis-
(m`m`+1)2 and
tinct eigenvalues.
4 The sequence of distinct eigenvalues can possibly be of ﬁnite length. In addition  the sequences of µi’s and

Then given  2 (0  1

4 ) and T > 0 

if n >

256 log 2


i’s (in Theorem 2) are different  the latter of which consists of repetitions.

6

c0

c0

c2

+ 4 log 4n

+ 2⌘T c1⌘2◆ with c0 = 3
 ⇣ 1
+ 2⌘T c1⌘4
m  32
with probability  (1  3)  for all t  T :
16p2qlog 2
(y by(t)) ✓1 
⌘m`◆t
3
(m`  m`+1)pn
4

1✓⇣ 1


1
pn

+



4 ` and c1 = ✏(f⇤  `)  then

+ 2p2✏(f⇤  `).

1

/ log

1

/ log

c1

/ log

1⌘c0

1⌘c0

1⌘c0

Since m` is determined by f⇤ and ⇢ only  with ⌘ = 1  the convergence rate log
w. r. t. n.
Remark 1 (Early stopping). In Theorems 3 and 4  the derived lower bounds of m grow in
T . To control m  we need to terminate the GD training at some “reasonable” T . Fortunately 
T is typically small. To see this  note that ⌘  c0  and c1 are independent of t. By (13) and
) iterations provided that

(15) we know 1pn (y by(t)) decreases to ⇥(c1) in (log 1

1
1 3
4 m`

is constant

). Similar to us  early stopping is adopted in

log n
 log(1 3
(m`m`+1)2n24

)  T . Thus  to guarantee  1pn (y by(t)) = O(c1)  it is enough to ter-

(log 1
c1
minate GD at iteration T = ⇥(log 1
c1
[AZLL18  LSO19]  and is commonly adopted in practice.
Corollary 2 (zero–approximation error). Suppose there exists ` such that µi > 0  for i  `  and
4 m` ). For a given  2 (0  1
✏(f⇤  `) = 0. Then let ⌘ = 1 and T =
(m`m`+1)2
m`⌘  then with probability  (1  3)  for all t  T :
and m & (n log n)⇣ 1
16p2 log 2/

pn (m`  m`+1)

Corollary 2 says that for ﬁxed f⇤ and ﬁxed distribution ⇢  nearly-linear network over-parameterization
m = ⇥(n log n) is enough for GD method to converge exponentially fast as long as 1
 = O(poly(n)).
Corollary 2 follow immediately from Theorem 4 by specifying the relevant parameters such as ⌘ and
T . To the best of our knowledge  this is the ﬁrst result showing sufﬁciency of nearly-linear network
over-parameterization. Note that (m`  m`+1) > 0 is the eigengap between the `–th and (` + 1)–th
largest distinct eigenvalues of the integral operator  and is irrelevant to n. Thus  for ﬁxed f⇤ and ⇢ 
c1 = ⇥⇣qlog 1

(y by(t))  (1 

 /n⌘.

4 )  if n >

log4 n log2 1


1
pn

256 log 2


3m`

)t +

+

4

m`

1

4

.

4 Application to Uniform Distribution and Polynomials

We illustrate our general results by applying them to the setting where the target functions are
polynomials and the feature vectors are uniformly distributed on the sphere S d1.
Up to now  we implicitly incorporate the bias bj in wj by augmenting the original wj; correspondingly 
the data feature vector is also augmented. In this section  as we are dealing with distribution on the
original feature vector  we explicitly separate out the bias from wj. In particular  let b0
j ⇠ N (0  1).
For ease of exposition  with a little abuse of notation  we use d to denote the dimension of the wj
and x before the above mentioned augmentation. With bias  (1) can be rewritten as fW  b(x) =
j=1 aj [hx  wji + bj]+  where b = (b1 ···   bm) are the bias of the hidden neurons  and the

1pmPm

kernel function in (12) becomes
K(x  s) = hx  si + 1

2⇡

✓⇡  arccos✓ 1

2

(hx  si + 1)◆◆ 8 x  s 2 S d1.

(16)

From Theorem 4 we know the convergence rate is determined by the eigendecomposition of the
target function f⇤ w. r. t. the eigenspaces of LK. When ⇢ is the uniform distribution on S d1  the
eigenspaces of LK are the spaces of homogeneous harmonic polynomials  denoted by H` for `  0.
Speciﬁcally  LK = P`0 `P`  where P` (for `  0) is the orthogonal projector onto H` and
` = ↵`
> 0 is the associated eigenvalue – ↵` is the coefﬁcient of K(x  s) in the expansion into

d2
`+ d2

2

2

7

(a) Plot of ` with ` under different d. Here  the ` is
monotonically decreasing in `.

(b) Training with f⇤ being randomly generated linear
or quadratic functions with n = 1000  m = 2000.

Figure 2: Application to uniform distribution and polynomials.

Gegenbauer polynomials. Note that H` and H`0 are orthogonal when ` 6= `0. See appendix G for
relevant backgrounds on harmonic analysis on spheres.
Explicit expression of eigenvalues ` > 0 is available; see Fig. 2a for an illustration of `. In
fact  there is a line of work on efﬁcient computation of the coefﬁcients of Gegenbauer polynomials
expansion [CI12].
If the target function f⇤ is a standard polynomial of degree `⇤  by [Wan  Theorem 7.4]  we know f⇤
can be perfectly projected onto the direct sum of the spaces of homogeneous harmonic polynomials
up to degree `⇤. The following corollary follows immediately from Corollary 2.
Corollary 3. Suppose f⇤ is a degree `⇤ polynomial  and the feature vector xi’s are i.i.d. generated
from the uniform distribution over S d1. Let ⌘ = 1  and T = ⇥(log n). For a given  2 (0  1
4 )  if
n = ⇥log 1
 )  then with probability at least 1    for all t  T :
+ ⇥(r log 1/


 and m = ⇥(n log n log2 1
(y by(t)) ✓1 
4 ◆t

)  where c0 = min{`⇤  `⇤+1} .

1
pn

3c0

n

For ease of exposition  in the above corollary  ⇥(·) hides dependence on quantities such as eigengaps
– as they do not depend on n  m  and . Corollary 3 and ` in Fig. 2a together suggest that the
convergence rate decays with both the dimension d and the polynomial degree `. This is validated in
Fig. 2a. It might be unfair to compare the absolute values of training errors since f⇤ are different.
Nevertheless  the convergence rates can be read from slope in logarithmic scale. We see that the
convergence slows down as d increases  and learning a quadratic function is slower than learning a
linear function.
Next we present the explicit expression of `. For ease of exposition  let h(u) := K(x  s) where
u = hx  si. By [CI12  Eq. (2.1) and Theorem 2]  we know
2`+2kk! d2

where h` := h(`)(0) is the `–th order derivative of h at zero  and the Pochhammer symbol (a)k is
deﬁned recursively as (a)0 = 1  (a)k = (a + k  1)(a)k1 for k 2 N. By a simple induction  it can
be shown that h0 = h(0)(0) = 1/3  and for k  1 

2 `+k+1

1Xk=0

d  2
2

h`+2k

` =

 

(17)

(18)

hk =

1
2

1{k=1} 

1

⇡2k⇣k (arccos 0.5)(k1) + 0.5 (arccos 0.5)(k)⌘  

where the computation of the higher-order derivative of arccos is standard. It follows from (17) and
(18) that ` > 0  and 2` > 2(`+1) and 2`+1 > 2`+3 for all `  0. However  an analytic order
among ` is unclear  and we would like to explore this in the future.

8

246810degree`109107105103101`d=5d=10d=15d=20References
[ADH+19] Sanjeev Arora  Simon S Du  Wei Hu  Zhiyuan Li  and Ruosong Wang. Fine-grained
analysis of optimization and generalization for overparameterized two-layer neural
networks. arXiv:1901.08584  2019.

[ASCC18] Vivek R Athalye  Fernando J Santos  Jose M Carmena  and Rui M Costa. Evidence for

a neural law of effect. Science  359(6379):1024–1029  2018.

[AZLL18] Zeyuan Allen-Zhu  Yuanzhi Li  and Yingyu Liang. Learning and generalization in
overparameterized neural networks  going beyond two layers. arXiv:1811.04918  2018.

[AZLS18] Zeyuan Allen-Zhu  Yuanzhi Li  and Zhao Song. A convergence theory for deep learning

via over-parameterization. arXiv preprint arXiv:1811.03962  2018.

[BG17] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet
with gaussian inputs. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70  pages 605–614. JMLR. org  2017.

[BR89] Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete.

In Advances in neural information processing systems  pages 494–501  1989.

[CB18] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable

programming. arXiv preprint arXiv:1812.07956  2018.

[CG19] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for

wide and deep neural networks. arXiv preprint arXiv:1905.13210  2019.

[CI12] María José Cantero and Arieh Iserles. On rapid computation of expansions in ultras-

pherical polynomials. SIAM Journal on Numerical Analysis  50(1):307–327  2012.

[DDS+09] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A
large-scale hierarchical image database. In 2009 IEEE conference on computer vision
and pattern recognition  pages 248–255. Ieee  2009.

[DLL+18] Simon S Du  Jason D Lee  Haochuan Li  Liwei Wang  and Xiyu Zhai. Gradient descent

ﬁnds global minima of deep neural networks. arXiv:1811.03804  2018.

[DS63] Nelson Dunford and Jacob T Schwartz. Linear operators: Part II: Spectral Theory:

Self Adjoint Operators in Hilbert Space. Interscience Publishers  1963.

[DX13] Feng Dai and Yuan Xu. Approximation theory and harmonic analysis on spheres and

balls. Springer  2013.

[DZPS18] Simon S Du  Xiyu Zhai  Barnabas Poczos  and Aarti Singh. Gradient descent provably

optimizes over-parameterized neural networks. arXiv:1810.02054  2018.

[GMMM19] Behrooz Ghorbani  Song Mei  Theodor Misiakiewicz  and Andrea Montanari. Lin-

earized two-layers neural networks in high dimension. arXiv:1904.12191  2019.

[JGH18] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Con-
vergence and generalization in neural networks. In Advances in neural information
processing systems  pages 8571–8580  2018.

[KB18] Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu

and squared relu ridge functions with l1 and l0 controls. 2018.

[KSH12] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with
deep convolutional neural networks. In Advances in neural information processing
systems  pages 1097–1105  2012.

[LBB+98] Yann LeCun  Léon Bottou  Yoshua Bengio  Patrick Haffner  et al. Gradient-based
learning applied to document recognition. Proceedings of the IEEE  86(11):2278–2324 
1998.

9

[LL18] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochas-
tic gradient descent on structured data. In Advances in Neural Information Processing
Systems  pages 8157–8166  2018.

[LSO19] Mingchen Li  Mahdi Soltanolkotabi  and Samet Oymak. Gradient descent with early
stopping is provably robust to label noise for overparameterized neural networks.
arXiv:1903.11680  2019.

[LY17] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with
relu activation. In Advances in Neural Information Processing Systems  pages 597–607 
2017.

[MG06] Eve Marder and Jean-Marc Goaillard. Variability  compensation and homeostasis in

neuron and network function. Nature Reviews Neuroscience  7(7):563  2006.

[MMN18] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the

landscape of two-layers neural networks. arXiv:1804.06561  2018.

[Nes18] Yurii Nesterov. Lectures on convex optimization  volume 137. Springer  2018.
[OS19] Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization:
global convergence guarantees for training shallow neural networks. arXiv:1902.04674 
2019.

[RBV10] Lorenzo Rosasco  Mikhail Belkin  and Ernesto De Vito. On learning with integral

operators. Journal of Machine Learning Research  11(Feb):905–934  2010.

[SS96] David Saad and Sara A Solla. Dynamics of on-line gradient descent learning for
multilayer neural networks. In Advances in neural information processing systems 
pages 302–308  1996.

[Sze75] G. Szegö. Orthogonal polynomials. American Mathematical Society  Providence  RI 

4th edition  1975.

[Tia16] Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural

networks with relu nonlinearity. 2016.

[VW18] Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural net-
works: Polynomial convergence and sq lower bounds. arXiv preprint arXiv:1805.02677 
2018.

[Wan] Yi Wang. Harmonic analysis and isoperimetric inequalities. LectureNotes.

[WGL+19] Blake Woodworth  Suriya Gunasekar  Jason Lee  Daniel Soudry  and Nathan Srebro.
Kernel and deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827 
2019.

[XLS17] Bo Xie  Yingyu Liang  and Le Song. Diverse neural network learns true target functions.

In Artiﬁcial Intelligence and Statistics  pages 1216–1224  2017.

[YS19] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for

understanding neural networks. arXiv preprint arXiv:1904.00687  2019.

[ZBH+16] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals.
Understanding deep learning requires rethinking generalization. arXiv:1611.03530 
2016.

[ZCZG18] Difan Zou  Yuan Cao  Dongruo Zhou  and Quanquan Gu. Stochastic gradient descent
optimizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888 
2018.

[ZSJ+17] Kai Zhong  Zhao Song  Prateek Jain  Peter L Bartlett  and Inderjit S Dhillon. Recovery
guarantees for one-hidden-layer neural networks. In Proceedings of the 34th Interna-
tional Conference on Machine Learning-Volume 70  pages 4140–4149. JMLR. org 
2017.

10

,Matthew Staib
Sebastian Claici
Justin Solomon
Stefanie Jegelka
Tobias Sommer Thune
Yevgeny Seldin
Lili Su
Pengkun Yang