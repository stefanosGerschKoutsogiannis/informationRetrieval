2014,Repeated Contextual Auctions with Strategic Buyers,Motivated by real-time advertising exchanges  we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer  where the former makes decisions myopically on every round  and the latter may strategically react to our algorithm  forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer’s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{2/3})) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.,Repeated Contextual Auctions with Strategic Buyers

Kareem Amin

University of Pennsylvania

akareem@cis.upenn.edu

Afshin Rostamizadeh

Google Research

rostami@google.com

Umar Syed

Google Research

usyed@google.com

Abstract

Motivated by real-time advertising exchanges  we analyze the problem of pricing
inventory in a repeated posted-price auction. We consider both the cases of a truth-
ful and surplus-maximizing buyer  where the former makes decisions myopically
on every round  and the latter may strategically react to our algorithm  forgoing
short-term surplus in order to trick the algorithm into setting better prices in the
future. We further assume a buyer’s valuation of a good is a function of a context
vector that describes the good being sold. We give the ﬁrst algorithm attaining

sublinear (eO(T 2/3)) regret in the contextual setting against a surplus-maximizing

buyer. We also extend this result to repeated second-price auctions with multiple
buyers.

1

Introduction

A growing fraction of Internet advertising is sold through automated real-time ad exchanges. In
a real-time ad exchange  after a visitor arrives on a webpage  information about that visitor and
webpage  called the context  is sent to several advertisers. The advertisers then compete in an auction
to win the impression  or the right to deliver an ad to that visitor. One of the great advantages of
online advertising compared to advertising in traditional media is the presence of rich contextual
information about the impression. Advertisers can be particular about whom they spend money
on  and are willing to pay a premium when the right impression comes along  a process known
as targeting. Speciﬁcally  advertisers can use context to specify which auctions they would like to
participate in  as well as how much they would like to bid. These auctions are most often second-
price auctions  wherein the winner is charged either the second highest bid or a prespeciﬁed reserve
price (whichever is larger)  and no sale occurs if the reserve price isn’t cleared by one of the bids.
One side-effect of targeting  which has been studied only recently  is the tendency for such exchanges
to generate many auctions that are rather uncompetitive or thin  in which few advertisers are willing
to participate. Again  this stems from the ability of advertisers to examine information about the
impression before deciding to participate. While this selectivity is clearly beneﬁcial for advertisers 
it comes at a cost to webpage publishers. Many auctions in real-time ad exchanges ultimately involve
just a single bidder  in which case the publisher’s revenue is entirely determined by the selection of
reserve price. Although a lone advertiser may have a high valuation for the impression  a low reserve
price will fail to extract this as revenue for the seller if the advertiser is the only participant in the
auction.
As observed by [2]  if a single buyer is repeatedly interacting with a seller  selecting revenue-
maximizing reserve prices (for the seller) reduces to revenue-maximization in a repeated posted-
price setting: On each round  the seller offers a good to the buyer at a price. The buyer observes her
value for the good  and then either accepts or rejects the offer. The seller’s price-setting algorithm is
known to the buyer  and the buyer behaves to maximize her (time-discounted) cumulative surplus 
i.e.  the total difference between the buyer’s value and the price on rounds where she accepts the
offer. The goal of the seller is to extract nearly as much revenue from the buyer as would have been

1

possible if the process generating the buyer’s valuations for the goods had been known to the seller
before the start of the game. In [2] this goal is called minimizing strategic regret.
Online learning algorithms are typically designed to minimize regret in hindsight  which is deﬁned
as the difference between the loss of the best action and the loss of the algorithm given the observed
sequence of events. Furthermore  it is assumed that the observed sequence of events are generated
adversarially. However  in our setting  the buyer behaves self-interestedly  which is not necessarily
the same as behaving adversarially  because the interaction between the buyer and seller is not
zero-sum. A seller algorithm designed to minimize regret against an adversary can perform very
suboptimally. Consider an example from [2]: a buyer who has a large valuation v for every good.
If the seller announces an algorithm that minimizes (standard) regret  then the buyer should respond
by only accepting prices below some ✏ ⌧ v. In hindsight  posting a price of ✏ in every round would
appear to generate the most revenue for the seller given the observed sequence of buyer actions 
and therefore ✏T cumulative revenue is “no-regret”. However  the seller was tricked by the strategic
buyer; there was (v  ✏)T revenue left on the table. Moreoever  this is a good strategy for the buyer
(it must have won the good for nearly nothing on ⌦(T ) rounds).
The main contribution of this paper is extending the setting described above to one where the buyer’s
valuations in each round are a function of some context observed by both the buyer and seller.
While [2] is motivated by our same application  they imagine an overly simplistic model wherein
the buyer’s value is generated by drawing an independent vt from an unknown distribution D. This
ignores that vt will in reality be a function of contextual information xt  information that is available
to the seller  and the entire reason auctions are thin to begin with (without xt there would be no
targeting). We give the ﬁrst algorithm that attains sublinear regret in the contextual setting  against a
surplus-maximizing buyer. We also note that in the non-contextual setting  regret is measured against
the revenue that could have been made if D were known  and the single ﬁxed optimal price were
selected. Our comparator will be more challenging as we wish to compete with the best function (in
some class) from contexts xt to prices.
The rest of the paper is organized as follows. We ﬁrst introduce a linear model by which values vt are
derived from contexts xt. We then demonstrate an algorithm based on stochastic gradient descent
(SGD) which achieves sublinear regret against an truthful buyer (one that accepts price pt iff pt  vt
on every round t). The analysis for the truthful buyer uses prexisting high probability bounds for
SGD when minimizing strongly convex functions [22]. Our main result requires an extension of
this analysis to cases in which “incorrect” gradients are occasionally observed. This lets us study
a buyer that is allowed to best-respond to our algorithm  possibly rejecting offers that the truthful
buyer would not  in order to receive better offers on future rounds. We also adapt our algorithm
to non-linear settings via a kernelized version of the algorithm. Finally  we extend our results to
second-price auctions with multiple buyers.
Related Work: The pricing of digital good in repeated auctions has been considered by many other
authors  including [2  17  4  3  6  19]. However  most of these papers do not consider a buyer who
behaves strategically across rounds. Buyers either behave randomly [19]  or only participate in a
single round [17  4  3  6]  or participate in multiple rounds but only desire a single good [20  12]
and therefore  in each of these cases  are not incentivized to manipulate the seller’s behavior on
future rounds. In reality buyers repeatedly interact with the same seller. There is empirical evidence
suggesting that buyers are not myopic  and do in fact behave strategically to induce better prices in
the future [9]  as well as literature studying different strategies for strategic buyers [5  15  16].
Repeated posted price actions against the same strategic buyer have been considered in the eco-
nomics literature under the heading of behavior-based price discrimination (BBPD) by [13  23  1 
11]  and more recently by [8]. These works differ from ours in two key ways. First  all these works
imagine that the buyer’s type is drawn from some ﬁxed publicly available distribution. Therefore
learning D is not at issue. In contrast  we argue that access to an accurate prior is particularly prob-
lematic in these settings. After all  the seller cannot expect to reliably estimate D from data when
the buyer is explicitly incentivized to hide its type (as illustrated in the Introduction; see also [2]).
This tension between learning and buyer truthfulness is in many ways central to our study.
Secondly  given a ﬁxed prior  the most common solution concept in the BBPD literature is a perfect
Bayes-Nash equilibrium  in which both the seller and buyer strategies are best responses to each
other. However  in the context of Internet advertising  a seller must ﬁrst deploy an algorithm which

2

automates the pricing strategy  and buyers subsequently react to the observed behavior of the pric-
ing algorithm. Any modiﬁcations the seller wishes to make to the pricing algorithm will typically
require changes to the end-user licensing agreement  which the seller will not want to do too fre-
quently. Therefore  in this paper  we make a commitment assumption on the seller: the seller acts
ﬁrst  announcing its pricing strategy  after which the buyer plays a best response strategy. Such
Stackleberg models of commitment [10] have sparked a great deal of recent interest due to their suc-
cess in security games (see [7] and [18] for an overview)  including practical deployment [21  14].

2 Preliminaries

Throughout this work  we will consider a repeated auction where at every round a single seller
prices an item to sell to a single buyer (extensions to multiple buyers are discussed in Section 5).
The good sold at step t in the repeated auction is represented by a context (feature) vector xt 2X =
{x : kxk2  1} and is drawn according a ﬁxed distribution D  which is unknown to the seller. The
good has a value vt that is a linear function of a parameter vector w⇤  also unknown to the seller 
vt = w⇤>xt (extensions to non-linear functions of the context are considered in Section 5). We
assume that w⇤ 2W = {w : kwk2  1} and also that 0  w⇤>x  1 with probability one with
respect to D.
For rounds t = 1  . . .   T the repeated posted-price auction is deﬁned as follows: (1) The buyer and
seller both observe xt ⇠D . (2) The seller offers a price pt. (3) The buyer selects at 2{ 0  1}. (4)
The seller receives revenue atpt.
Here  at is an indicator variable that represents whether or not the buyer accepted the offered price
(1 indicates yes). The goal of the seller is to select a price pt in each round t such that the expected

regret R(T ) = EhPT

t=1 vt  atpti is o(T ). The choice of at will depend on the buyer’s behavior.

We will analyze two types of buyers in the subsequent sections of the paper: truthful and surplus-
maximizing buyers  and will attempt to minimize regret against the truthful buyer and regret against
the surplus-maximizing buyer. Note  the regret is the difference between the maximum revenue
possible and the amount made by the algorithm that offers prices to the buyer.

3 Truthful Buyer

In this section we introduce the Learn-Exploit Algorithm for Pricing (LEAP)  which we show has

regret of the form O(T 2/3plog(T )) against a truthful buyer. A buyer is truthful if she accepts
any offered price that gives a non-negative surplus  which is deﬁned as the difference between the
buyer’s value for the good minus the price paid: vt  pt. Therefore  for a truthful buyer we deﬁne
at = 1{pt  vt}.
At this point  we note that the loss function vt  1{pt  vt}pt  which we wish to minimize over
all rounds  is not convex  differentiable or even continuous. If the price is even slightly above the
truthful buyers valuation it is rejected and the seller makes zero revenue. To circumvent this our
algorithm will attempt to learn w⇤ directly by minimizing a surrogate loss function for which w⇤
in the minimizer. Our analysis hinges on recent results [22] which give optimal rates for gradient
descent when the function being minimized is strongly convex. Our key trick is to offer prices so
that  in each round  the buyer’s behavior reveals the gradient of the surrogate loss at our current
estimate for w⇤. Below we deﬁne the LEAP algorithm (Algorithm 1)  which we show addresses
these difﬁculties in the online setting.
The algorithm depends on input parameters ↵  ✏ and . The ↵ parameter determines what fraction
of rounds are spent in the learning phase as oppose to the exploit phase. During the learning phase 
uniform random prices are offered and the model parameters are updated as a function of the feed-
back given by the buyer. During the exploit phase  the model parameters are ﬁxed and the offered
price is computed as a linear function of these parameters minus the value of the ✏ parameter. The
✏ parameter can be thought of as inversely proportional to our conﬁdence in the ﬁxed model pa-
rameters and is used to hedge against the possibility of over-estimating the value of a good. The 
parameter is a learning-rate parameter set according to the minimum eigenvalue of the covariance
matrix  and is deﬁned below in Assumption 1. In order to prove a regret bound  we ﬁrst show that

3

Algorithm 1 LEAP algorithm

(Learning phase)

• Let 0  ↵  1  w1 = 0 2W   ✏  0  > 0  T↵ = d↵Te.
• For t = 1  . . .   T↵
– Offer pt ⇠ U  where U is the uniform distribution on the interval [0  1].
– Observe at.
– ˜gt = 2wt · xt  atxt.
– wt+1 =⇧ W (wt  1
– Offer pt = wT↵+1 · xt  ✏.

• For t = T↵ + 1  . . .   T

t ˜gt).

(Exploit phase)

the learning phase of the algorithm is minimizing a strongly convex surrogate loss and then show
that this implies the buyer enjoys near optimal revenue during the exploit phase of the algorithm.

Let gt = 2(w>t xt  1{pt  vt})xt and F (w) = Ex⇠D⇥(w⇤>x  w>x)2⇤. Note that when the
buyer is truthful ˜gt = gt. Against a truthful buyer  gt is an unbiased estimate of the gradient of F .
Proposition 1. The random variable gt satisﬁes E[gt | wt] = rF (wt). Also  kgtk  4 with
probability 1.
Proof. First note that E[gt | wt] = Ext⇥2wt·xtEpt[1{pt  vt}]⇤ = Ext⇥2wt·xtPrpt(pt 
vt)⇤. Since pt is drawn uniformly from [0  1] and vt is guaranteed to lie in [0  1] we have that
Pr(pt  vt) =R 1
0 1{pt  vt}dpt = vt. Plugging this back into gt gives us exactly the expression
for rF (wt). Furthermore  kgtk = 2|w>t xt  1{pt  vt}|kxtk  4 since |w>t xt| k wtkkxtk 
1 and kxtk  1
We now introduce the notion of strong convexity. A twice-differentiable function H(w) is -
strongly convex if and only if the Hessian matrix r2H(w) is full rank and the minimum eigenvalue
of r2H(w) is at least . Note that the function F is strongly convex if and only if the covariance
matrix of the data is full-rank  since r2F (w) = 2Ex[xx>]. We make the following assumption.
Assumption 1. The minimum eigenvalue of 2Ex[xx>] is at least > 0.

Note that if this is not the case then there is redundancy in the features and the data can be pro-
jected (for example using PCA) into a lower dimensional feature space with a full-rank covariance
matrix and without any loss in information. The seller can compute an ofﬂine estimate of both this
projection and  by collecting a dataset of context vectors before starting to offer prices to the buyer.
Thus  in view of Proposition 1 and the strong convexity assumption  we see the learning phase of
the LEAP algorithm is conducting a stochastic gradient descent to minimize the -strongly convex
t ˜gt) and ˜gt = gt is an unbiased
function F   where at each time step we update wt+1 =⇧ W (wt  1
estimate of the gradient. We now make use of an existing bound ([22]) for stochastic gradient
descent on strongly convex functions.
Lemma 1 ([22] Proposition 1). Let  2 (0  1/e)  T↵  4 and suppose F is -strongly convex over
the convex set W. Also suppose E[gt | wt] = rF (w) and kgtk2  G2 with probability 1. Then
with probability at least 1   for any t  T↵ it holds that
(624 log(log(T↵)/) + 1)G2

where w⇤ = argminwF (w) .

kwt  w⇤k2 

2t

This guarantees that  with high probability  the distance between the learned parameter vector wt
and the target weight vector w⇤ is bounded and decreasing as t increases. This allows us to carefully
tune the ✏ parameter that is used in the exploit phase of the algorithm (see Lemma 6 in the appendix).
We are now equipped to prove a bound on the regret of the LEAP algorithm.
Theorem 1. For any T > 4  0 <↵< 1 and assuming a truthful buyer  the LEAP algorithm
  where G = 4  has regret against a truthful buyer at most

with ✏ = q (624 log(pT↵ log(T↵))+1)G2

2T↵

4

R(T )  2↵T + 4q T

↵q (624 log(pT↵ log(T↵))+1)G2

2

R(T )  2T 2/3 + 4T 2/3r (624 log(T 1/3 log(T 2/3)) + 1)G2

2

Proof. We ﬁrst decompose the regret

  which implies for ↵ = T 1/3 a regret at most

= O⇣T 2/3plog(T )⌘ .
TXt=T↵+1

Eh TXt=1

vt  atpti + Eh

vt  atpti = Eh T↵Xt=1

Ehvt  atpti   (1)
where we have used the fact |vtatpt| 1. Let A denote the event that  for all t 2{ T↵ +1  . . .   T} 
at = 1^ vt pt  ✏. Lemma 6 (see Appendix  Section A.1) proves that A occurs with probability at
. For brevity let N =p(624 log(pT↵ log(T↵)) + 1)G2/2  then we can decompose
least 1T 1/2
the expectation in the following way:

vt  atpti  T↵ +

TXt=T↵+1

↵

Ehvt  atpti = Pr[A]E[vt  atpt|A] + (1  Pr[A])E[vt  atpt|¬A]
+r 1

 Pr[A]✏ + (1  Pr[A])  ✏ + T 1/2

T↵

↵

=r N

T↵  2r N
t=T↵+1 E[vt  atpt]  T↵ + d(1↵)Te
pT↵

T↵

 

where the inequalities follow from the deﬁnition of A  Lemma 6  and the fact that |vt  atpt| < 1.
2pN 
Plugging this back into equation (1) gives T↵ +PT
2↵T + 4q T

pN  proving the ﬁrst result of the theorem. ↵ = T 1/3 gives the ﬁnal expression.

In the next section we consider the more challenging setting of a surplus-maximizing buyer  who
may accept/reject prices in a manner meant to lower the prices offered.
4 Surplus-Maximizing Buyer

↵

In the previous section we considered a truthful buyer who myopically accepts every price below

her value  i.e.  she sets at = 1{pt  vt} for every round t. Let S(T ) = EhPT

t=1 tat(vt  pt)i
be the buyer’s cumulative discounted surplus  where {t} is a decreasing discount sequence  with
t 2 (0  1). When prices are offered by the LEAP algorithm  the buyer’s decisions about which
prices to accept during the learning phase have an inﬂuence on the prices that she is offered in the
exploit phase  and so a surplus-maximizing buyer may be able to increase her cumulative discounted
surplus by occasionally behaving untruthfully. In this section we assume that the buyer knows the
pricing algorithm and seeks to maximize S(T ).
Assumption 2. The buyer is surplus-maximizing  i.e.  she behaves so as to maximize S(T )  given
the seller’s pricing algorithm.
We say that a lie occurs in any round t where at 6= 1{pt  vt}. Note that a surplus-maximizing
buyer has no reason to lie during the exploit phase  since the buyer’s behavior during exploit rounds
has no effect on the prices offered. Let L = {t : 1  t  T↵ ^ at 6= 1{pt  vt}} be the set of
learning rounds where the buyer lies  and let L = |L| be the number of lies. Observe that ˜gt 6= gt
in any lie round (recall that E[gt | wt] = rF (wt)  i.e.  gt is the stochastic gradient in round t).
We take a moment to note the necessity of the discount factor t. This essentially models the buyer
as valuing surplus at the current time step more than in the future. Another way of interpreting this 
is that the seller is more “patient” as compared to the buyer. In [2] the authors show a lower bound
on the regret against a surplus-maximizing buyer in the contextless setting of the form O(T)  where
T =PT
i=1 t. Thus  if no decreasing discount factor is used  i.e. t = 1  then sublinear regret is
not possible. Note  the lower bound of the contextless setting applies here as well  since the case of
a distribution D that induces a ﬁxed context x⇤ on every round is a special case of our setting. In
that case the problem reduces to the ﬁxed unknown value setting since on every round vt = w⇤>x⇤.
In the rest of this section we prove an OT 2/3plog(T )(1 + 1/ log(1/)) bound on the seller’s

regret under the assumption that the buyer is surplus-maximizing and that her discount sequence is

5

1
 log( 2
log(1/)

 )+1)

.

t = t1 for some  2 (0  1). The idea of the proof is to show that the buyer incurs a cost for
telling lies  and therefore will not tell very many  and thus the lies she does tell will not signiﬁcantly
affect the seller’s estimate of w⇤.
Bounding the cost of lies: Observe that in any learning round where the surplus-maximizing buyer
tells a lie  she loses surplus in that round relative to the truthful buyer  either by accepting a price
higher than her value (when at = 1 and vt < pt) or by rejecting a price less than her value (when
at = 0 and vt > pt). This observation can be used to show that lies result in a substantial loss of
surplus relative to the truthful buyer  provided that in most of the lie rounds there is a nontrivial gap
between the buyer’s value and the seller’s price. Because prices are chosen uniformly at random
during the learning phase  this is in fact quite likely  and with high probability the surplus lost
relative to the truthful buyer during the learning phase grows exponentially with the number of lies.
The precise quantity is stated in the Lemma below. A full proof appears in the appendix  Section A.3.
Lemma 2. Let the discount sequence be deﬁned as t = t1 for 0 << 1 and assume the buyer
has told L lies. Then for > 0 with probability at least 1   the buyer loses a surplus of at least
L+31
8T↵ log( 1

 )⇣ T↵
1⌘ relative to the truthful buyer during the learning phase.

Bounding the number of lies: Although we argued in the previous lemma that lies during the
learning phase cause the surplus-maximizing buyer to lose surplus relative to the truthful buyer 
those lies may result in lower prices offered during the exploit phase  and thus the overall effect of
lying may be beneﬁcial to the buyer. However  we show that there is a limit on how large that beneﬁt
can be  and thus we have the following high-probability bound on the number of learning phase lies.
Lemma 3. Let the discount sequence be deﬁned as t = t1 for 0 << 1. Then for > 0 with
probability at least 1    the number of lies L  log(32T↵
The full proof is found in the appendix (Section A.4)  and we provide a proof sketch here. The
argument proceeds by comparing the amount of surplus lost (compared to the truthful buyer) due to
telling lies in the learning phase to the amount of surplus that could hope to be gained (compared to
the truthful buyer) in the exploit phase. Due to the discount factor  the surplus lost will eventually
outweigh the surplus gained as the number of lies increases  implying a limit to the number of lies a
surplus maximizing buyer can tell.
Bounding the effect of lies:
In Section 3 we argued that if the buyer is truthful then  in each
learning round t of the LEAP algorithm  ˜gt is a stochastic gradient with expected value rF (wt).
We then use the analysis of stochastic gradient descent in [22] to prove that wT↵+1 converges to w⇤
(Lemma 1). However  if the buyer can lie then ˜gt is not necessarily the gradient and Lemma 1 no
longer applies. Below we extend the analysis in Rakhlin et al. [22] to a setting where the gradient
may be corrupted by lies up to L times.
Lemma 4. Let  2 (0  1/e)  T↵  2. If the buyer tells L lies then with probability at least 1   
kwT↵+1  w⇤k2  1
The proof of the lemma is similar to that of Lemma 1  but with extra steps needed to bound the
additional error introduced due to the erroneous gradients. Due to space constraints  we present
the proof in the appendix  Section A.6. Note that  modulo constants  the bound only differs by the
additive term L/T↵. That is  there is an extra additive error term that depends on the ratio of lies to
number of learning rounds. Thus  if no lies are told  then there is no additive error. While if many
lies are told  e.g. L = T↵  then the bound become vacuous.
Main result: We are now ready to prove an upper bound on the regret of the LEAP algorithm when
the buyer is surplus-maximizing.
Theorem 2. For any 0 <↵< 1 (such that T↵  4)  0 << 1 and assuming a surplus-maximizing
buyer with exponential discounting factor t = t1  then the LEAP algorithm using parame-
ter ✏ =q 1
   where G = 4  has regret
R(T )  2↵T + 4r T
4e2 log(128pT↵ log(4pT↵) + 1)

↵s (624 log(2pT↵ log(T↵)) + e2)G2

T↵ (624 log(2pT↵ log(T↵))+e2)G2

+ 4e2 log(128pT↵ log(4pT↵)+1)

T↵+1⇣ (624 log(log(T↵)/)+e2)G2

2

against a surplus-maximizing buyer at most

+ 4e2L

 ⌘.

2

 log(1/)

+

 log(1/)

 

2

6

1

2

1
 log( 4

 )+1)

 log(1/)

4e2 log(64T↵

which for ↵ = T 1/3 implies R(T )  O⇣T 2/3qlog(T )1 +

log(1/)⌘.
Proof. Taking the high probability statements of Lemma 3 and Lemma 4 with /2 2 [0  1/e]
T↵⇣ (624 log(2 log(T↵)/)+e2)G2
tells us that with probability at least 1    kwT↵  w⇤k2  1
+
Since we assume T↵  4  if we set  = T 1/2
/2  1/e  which is required
for Lemma 4 to hold. Thus  if we set the algorithm parameter ✏ as indicated in the statement of
theorem  we have that with probability at least 1  T 1/2
for all t 2{ T↵ + 1  . . .   T} that at = 1
and vt  pt  ✏  which follows from the same argument used for Lemma 6.
Finally  the same steps as in the proof of Theorem 1 we can be used to show the ﬁrst inequality.
Setting ↵ = T 1/3 shows the second inequality and completes the theorem.

it implies /2 = T 1/2

↵

⌘.

↵

↵

Note that the bound shows that if  ! 1 (i.e. no discounting) the bound becomes vacuous  which
is to be expected since the ⌦(T) lower bound on regret demonstrates the necessity of a discounting
factor. If  ! 0 (i.e. buyer become myopic  thereby truthful)  then we retrieve the truthful bound
modulo constants. Thus for any < 1  we have shown the ﬁrst sublinear bound on the seller’s regret
against a surplus-maximizing buyer in the contextual setting.

5 Extensions

Doubling trick: A drawback of Theorem 2 is that optimally tuning the parameters ✏ and ↵ re-
quires knowledge of the horizon T . The usual way of handling this problem in the standard online
learning setting is to apply the ‘doubling trick’: If a learning algorithm that requires knowledge
of T has regret O(T c) for some constant c  then running independent instances of the algorithm
during consecutive phases of exponentially increasing length (i.e.  the ith phase has length 2i) will
also have regret O(T c). We can also apply the doubling trick to our strategic setting  but we must
exercise caution and argue that running the algorithm in phases does not affect the behavior of a
surplus-maximizing buyer in a way that invalidates the proof of Theorem 2. We formally state and
prove the relevant corollary in Section A.8 of the Appendix.

Kernelized Algorithm:
In some cases  assuming that the value of a buyer is a linear function of
the context may not be accurate. In Section A.7 of the Appendix we describe a kernelized version
of LEAP  which allows for a non-linear model of the buyer value as a function of the context x. At
the same time  the regret guarantees provided in the previous sections still apply since we can view
the model as linear function of the induced features (x)  where (·) is a non-linear map and the
kernel function K is used to compute the inner product in this induced feature space: K(x  x0) =
(x)>(x0).

Multiple Buyers: So far we have assumed that the seller is interacting with a single buyer across
multiple posted price auctions. Recall that the motivation for considering this setting was repeated
second price auctions against a single buyer  a situation that happens often in online advertising
because of targetting. One might nevertheless wonder whether the algorithm can be applied to a
setting where there can be multiple buyers  and whether it remains robust in such a setting. We
describe a way in which the analysis for the posted-price setting can carry over to multiple buyers.
Formally  suppose there are K buyers  and on round t  buyer k receives a valuation of vk t. We let
t = vkval(t) t  and vt = maxk6=kval(t) vk t: the buyer with the highest
kval(t) = arg maxk vk t  v+
valuation  the highest valuation itself  and the second-highest valuation respectively. In a second
t and bt analogously
price auction  each buyer also submits a bid bk t  and we deﬁne kbid(t)  b+
t   vt   corresponding to the highest bidder  the largest bid  and the second-largest bid.
to kval(t)  v+
After the seller announces a reserve price pt  buyers submit their bids {bk t}  and the seller receives
t } max{bt   pt}. The goal of the seller is to minimize R(T ) =
round t revenue of rt = 1{pt  b+
E[PT
t  rt]. We assume that buyers are surplus-maximizing  and select a strategy that maps
previous reserve prices p1  ...  pt1  pt  and vk t to a choice of bid on round t.

t=1 v+

7

We call v+
t the market valuation for good t. The key to extending the LEAP algorithm to the multiple
buyer setting will be to treat market valuations in the same way we treated the individual buyer’s
valuation in the single-buyer setting. In order to do so  we make an analogous modelling assumption
t = w⇤>t xt.1 Note
to that of Section 2. Speciﬁcally  we assume that there is some w⇤ such that v+
that we assume a model on the market price itself.
At ﬁrst glance  this might seem like a strange assumption since v+
is itself the result of a maxi-
t
mization over vk t. However  we argue that it’s actually rather unrestrictive. In fact the individual
valuations vk t can be generated arbitrarily so long as vk t  w⇤>t xt and equality holds for some k.
In other words  we can imagine that nature ﬁrst computes the market valuation v+
t   then arbitrarily
(even adversarialy) selects which buyer gets this valuation  and the other buyer valuations.
Now we can deﬁne at = 1{pt  b+
t }  whether the largest bid was greater than the reserve  and
consider running the LEAP algorithm  but with this choice of at. Notice that for any t  atpt  rt 
thereby giving us the following key fact: R(T )  R0(T )   E[PT
t  atpt]. We also redeﬁne
L to be the number of market lies: rounds t  T↵ where at 6= 1{pt  v+
t }. Note the market tells
a lie if either all valuations were below pt  but somebody bid over pt anyway  or if some valuation
was above pt but no buyer decided to outbid pt. With this choice of L  Lemma 4 holds exactly as
written but in the multiple buyer setting.
It’s well-known [24] that single-shot second price auctions are strategy-proof. Therefore  during the
exploit phase of the algorithm  all buyers are incentivized to bid truthfully. Thus  in order to bound
R0(T ) and therefore R(T )  we need only rederive Lemma 3 to bound the number of market lies. We
begin partitioning the market lies. Let L = {t : t  T↵  1{pt  v+
t }}  while letting
Lk = {t : t  T↵  v+
t < pt  v+
t   kval(t) = k}. In other
words  we attribute a lie to buyer k if (1) the reserve was larger than the market value  but buyer k
won the auction anyway  or (2) buyer k had the largest valuation  but nobody cleared the reserve.
Checking that L = [kLk and letting Lk = |Lk| tells us that L  PK
k=1 Lk. Furthermore  we
can bound Lk using nearly identical arguments to the posted price setting  giving us the subsequent
Corollary for the multiple buyer setting.
Lemma 5. Let the discount sequence be deﬁned as t = t1 for 0 << 1. Then for > 0 with
probability at least 1    Lk  log(32T↵/+1)
Proof. We ﬁrst consider the surplus buyer k loses during learning rounds  compared to if he had
been truthful. Suppose buyer k unilateraly switches to always bidding his value (i.e. bk t = vk t).
For a single-shot second price auction  being truthful is a dominant strategy and so he would only
increase his surplus on learning rounds. Furthermore  on each round in Lk he would increase his
(undiscounted) surplus by at least |vk t  pt|. Now the analysis follows as in Lemmas 2 and 3.
Corollary 1.
In the multiple surplus-maximizing buyers setting the LEAP algorithm with
↵ = T 1/3  ✏ = q 1
T↵ (624 log(2pT↵ log(T↵))+e2)G2
   has regret
R(T )  R0(T )  O⇣T 2/3qlog(T ) + K log(T )
log(1/)⌘

t   kbid(t) = k}[{ t  T↵  b+

+ 4e2K log(128pT↵ log(4pT↵)+1)

t }6 = 1{pt  b+

log(1/)

  and L  KLk.

t < p+

t  b+

t=1 v+

2

 log(1/)

6 Conclusion

In this work  we have introduced the scenario of contextual auctions in the presence of surplus-
maximizing buyers and have presented an algorithm that is able to achieve sublinear regret in this
setting  assuming buyers receive a discounted surplus. Once again  we stress the importance of the
contextual setting  as it contributes to the rise of targeted bids that result in auction with single high-
bidders  essentially reducing the auction to the posted-price scenario studied in this paper. Future
directions for extending this work include considering different surplus discount rates as well as
understanding whether small modiﬁcations to standard contextual online learning algorithms can
lead to no-strategic-regret guarantees.

1Note that we could also apply the kernelized LEAP algorithm in the multiple buyer setting.

8

References
[1] Alessandro Acquisti and Hal R Varian. Conditioning prices on purchase history. Marketing Science  24

(3):367–381  2005.

[2] Kareem Amin  Afshin Rostamizadeh  and Umar Syed. Learning prices for repeated auctions with strategic

buyers. In Advances in Neural Information Processing Systems  pages 1169–1177  2013.

[3] Ziv Bar-Yossef  Kirsten Hildrum  and Felix Wu. Incentive-compatible online auctions for digital goods.

In Proceedings of Symposium on Discrete Algorithms  pages 964–970. SIAM  2002.

[4] Avrim Blum  Vijay Kumar  Atri Rudra  and Felix Wu. Online learning in online auctions. In Proceedings

Symposium on Discrete algorithms  pages 202–204. SIAM  2003.

[5] Matthew Cary  Aparna Das  Ben Edelman  Ioannis Giotis  Kurtis Heimerl  Anna R Karlin  Claire Mathieu 
and Michael Schwarz. Greedy bidding strategies for keyword auctions. In Proceedings of the 8th ACM
conference on Electronic commerce  pages 262–271. ACM  2007.

[6] Nicolo Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Regret minimization for reserve prices in

second-price auctions. In Proceedings of the Symposium on Discrete Algorithms. SIAM  2013.

[7] Vincent Conitzer and Tuomas Sandholm. Computing the optimal strategy to commit to. In Proceedings

of the 7th ACM conference on Electronic commerce  pages 82–90. ACM  2006.

[8] Nikhil R Devanur  Yuval Peres  and Balasubramanian Sivan. Perfect bayesian equilibria in repeated sales.

arXiv preprint arXiv:1409.3062  2014.

[9] Benjamin Edelman and Michael Ostrovsky. Strategic bidder behavior in sponsored search auctions. De-

cision support systems  43(1):192–198  2007.

[10] Drew Fudenberg and Jean Tirole. Game theory. MIT Press Books  1  1991.
[11] Drew Fudenberg and J Miguel Villas-Boas. Behavior-based price discrimination and customer recogni-

tion. Handbook on economics and information systems  1:377–436  2006.

[12] Mohammad Taghi Hajiaghayi  Robert Kleinberg  and David C Parkes. Adaptive limited-supply online
auctions. In Proceedings of the 5th ACM conference on Electronic commerce  pages 71–80. ACM  2004.
[13] Oliver D Hart and Jean Tirole. Contract renegotiation and coasian dynamics. The Review of Economic

Studies  55(4):509–540  1988.

[14] Manish Jain  Jason Tsai  James Pita  Christopher Kiekintveld  Shyamsunder Rathi  Milind Tambe  and
Fernando Ord´o˜nez. Software assistants for randomized patrol planning for the lax airport police and the
federal air marshal service. Interfaces  40(4):267–290  2010.

[15] Brendan Kitts and Benjamin Leblanc. Optimal bidding on keyword auctions. Electronic Markets  14(3):

186–201  2004.

[16] Brendan Kitts  Parameshvyas Laxminarayan  Benjamin Leblanc  and Ryan Meech. A formal analysis
of search auctions including predictions on click fraud and bidding tactics. In Workshop on Sponsored
Search Auctions  2005.

[17] Robert Kleinberg and Tom Leighton. The value of knowing a demand curve: Bounds on regret for online
posted-price auctions. In Symposium on Foundations of Computer Science  pages 594–605. IEEE  2003.
[18] Dmytro Korzhyk  Zhengyu Yin  Christopher Kiekintveld  Vincent Conitzer  and Milind Tambe. Stackel-
berg vs. nash in security games: An extended investigation of interchangeability  equivalence  and unique-
ness. J. Artif. Intell. Res.(JAIR)  41:297–327  2011.

[19] Andres Munoz Medina and Mehryar Mohri. Learning theory and algorithms for revenue optimization
in second price auctions with reserve. In Proceedings of The 31st International Conference on Machine
Learning  pages 262–270  2014.

[20] David C Parkes. Online mechanisms. In Noam Nisan  Tim Roughgarden  Eva Tardos  and Vijay Vazirani 

editors  Algorithmic Game Theory. Cambridge University Press  2007.

[21] James Pita  Manish Jain  Janusz Marecki  Fernando Ord´o˜nez  Christopher Portway  Milind Tambe  Craig
Western  Praveen Paruchuri  and Sarit Kraus. Deployed armor protection: the application of a game
theoretic model for security at the los angeles international airport. In Proceedings of the 7th interna-
tional joint conference on Autonomous agents and multiagent systems: industrial track  pages 125–132.
International Foundation for Autonomous Agents and Multiagent Systems  2008.

[22] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent optimal for strongly

convex stochastic optimization. arXiv preprint arXiv:1109.5647  2011.

[23] Klaus M Schmidt. Commitment through incomplete information in a simple repeated bargaining game.

Journal of Economic Theory  60(1):114–139  1993.

[24] Hal R Varian and Jack Repcheck. Intermediate microeconomics: a modern approach  volume 6. WW

Norton & Company New York  NY  2010.

9

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
Janne Korhonen
Pekka Parviainen
Ashish Kumar
Saurabh Gupta
David Fouhey
Sergey Levine
Jitendra Malik