2016,A state-space model of cross-region dynamic connectivity in MEG/EEG,Cross-region dynamic connectivity  which describes spatio-temporal dependence of neural activity among multiple brain regions of interest (ROIs)  can provide important information for understanding cognition. For estimating such connectivity  magnetoencephalography (MEG) and electroencephalography (EEG) are well-suited tools because of their millisecond temporal resolution. However  localizing source activity in the brain requires solving an under-determined linear problem. In typical two-step approaches  researchers first solve the linear problem with general priors assuming independence across ROIs  and secondly quantify cross-region connectivity. In this work  we propose a one-step state-space model to improve estimation of dynamic connectivity. The model treats the mean activity in individual ROIs as the state variable  and describes non-stationary dynamic dependence across ROIs using time-varying auto-regression. Compared with a two-step method  which first obtains the commonly used minimum-norm estimates of source activity  and then fits the auto-regressive model  our state-space model yielded smaller estimation errors on simulated data where the model assumptions held. When applied on empirical MEG data from one participant in a scene-processing experiment  our state-space model also demonstrated intriguing preliminary results  indicating leading and lagged linear dependence between the early visual cortex and a higher-level scene-sensitive region  which could reflect feed-forward and feedback information flow within the visual cortex during scene processing.,A state-space model of cross-region dynamic

connectivity in MEG/EEG

Ying Yang∗ Elissa M. Aminoff† Michael J. Tarr∗ Robert E. Kass∗

∗Carnegie Mellon University  †Fordham University

ying.yang.cnbc.cmu@gmail.com  {eaminoff@fordham  michaeltarr@cmu  kass@stat.cmu}.edu

Abstract

Cross-region dynamic connectivity  which describes the spatio-temporal depen-
dence of neural activity among multiple brain regions of interest (ROIs)  can
provide important information for understanding cognition. For estimating such
connectivity  magnetoencephalography (MEG) and electroencephalography (EEG)
are well-suited tools because of their millisecond temporal resolution. However 
localizing source activity in the brain requires solving an under-determined linear
problem. In typical two-step approaches  researchers ﬁrst solve the linear problem
with generic priors assuming independence across ROIs  and secondly quantify
cross-region connectivity. In this work  we propose a one-step state-space model to
improve estimation of dynamic connectivity. The model treats the mean activity in
individual ROIs as the state variable and describes non-stationary dynamic depen-
dence across ROIs using time-varying auto-regression. Compared with a two-step
method  which ﬁrst obtains the commonly used minimum-norm estimates of source
activity  and then ﬁts the auto-regressive model  our state-space model yielded
smaller estimation errors on simulated data where the model assumptions held.
When applied on empirical MEG data from one participant in a scene-processing
experiment  our state-space model also demonstrated intriguing preliminary results 
indicating leading and lagged linear dependence between the early visual cortex
and a higher-level scene-sensitive region  which could reﬂect feedforward and
feedback information ﬂow within the visual cortex during scene processing.

1

Introduction

Cortical regions in the brain are anatomically connected  and the joint neural activity in connected
regions are believed to underlie various perceptual and cognitive functions. Besides anatomical
connectivity  researchers are particularly interested in the spatio-temporal statistical dependence
across brain regions  which may vary quickly in different time stages of perceptual and cognitive
processes. Descriptions of such spatio-temporal dependence  which we call dynamic connectivity  not
only help to model the joint neural activity  but also provide insights to understand how information
ﬂows in the brain. To estimate dynamic connectivity in human brains  we need non-invasive
techniques to record neural activity with high temporal resolution. Magnetoencephalography (MEG)
and electroencephalography (EEG) are well-suited tools for such purposes  in that they measure
changes of magnetic ﬁelds or scalp voltages  which are almost instantaneously induced by electric
activity of neurons.
However  spatially localizing the source activity in MEG/EEG is challenging. Assuming the brain
source space is covered by m discrete points  each representing an electric current dipole generated
by the activity of the local population of neurons  then the readings of n MEG/EEG sensors can
be approximated with a linear transformation of the m-dimensional source activity. The linear
transformation  known as the forward model  is computed using Maxwell equations given the relative

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

positions of sensors with respect to the scalp (1). Typically m ≈ 103 ∼ 104 whereas n ≈ 102 (cid:28) m 
so the source localization problem — estimating the source activity from the sensor data— is under-
determined. Previous work has exploited various constraints or priors for regularization  including L2
norm penalty (2; 3)  sparsity-inducing penalty (4)  and priors that encourage local spatial smoothness
or temporal smoothness (5; 6; 7; 8).
When estimating dynamic connectivity from MEG/EEG recordings  especially among several pre-
deﬁned regions of interest (ROIs)  researchers often use a two-step procedure: Step 1  estimating
source activity using one of the common source localization methods  for example  the minimum
norm estimate (MNE) that penalizes squared L2 norm (2); Step 2  extracting the mean activity of
source points within each ROI  and then quantifying the statistical dependence among the ROIs 
using various methods ranging from pairwise correlations of time series to Granger causality and
other extensions (9). However  most of the popular methods in Step 1 do not assume dependence
across ROIs. For example  MNE assumes that all source points have independent and identical priors.
Even in methods that assume auto-regressive structures of source activity (6; 8)  only dependence
on the one-step-back history of a source point itself and its adjacent neighbors is considered  while
long-range dependence across ROIs is ignored. Biases due to these assumptions in Step 1 can not be
adjusted in Step 2 and thus may result in additional errors in the connectivity analysis.
Alternatively  one can combine source localization and connectivity analysis jointly in one step.
Two pioneering methods have explored this direction. The dynamical causal modeling (DCM
(10)) assumes the source activity includes only one single current dipole in each ROI  and the ROI
dipoles are modeled with a nonlinear  neurophysiology-informed dynamical system  where time-
invariant coefﬁcients describe how the current activity in each ROI is dependent on the history of all
ROIs. Another method (11) does not use pre-deﬁned ROIs  but builds a time-invariant multivariate
auto-regressive (AR) model of all m source points  where the AR coefﬁcients are constrained by
structural white-matter connectivity and sparsity-inducing priors. Both methods use static parameters
to quantify connectivity  but complex perceptual or cognitive processes may involve fast changes of
neural activity  and correspondingly require time-varying models of dynamic connectivity.
Here  we propose a new one-step state-space model  designed to estimate dynamic spatio-temporal
dependence across p given ROIs directly from MEG/EEG sensor data. We deﬁne the mean activity
of the source points within each individual ROI as our p-dimensional state variable  and use a time-
varying multivariate auto-regressive model to describe how much the activity in each ROI is predicted
by the one-step-back activity in the p ROIs. More speciﬁcally  we utilize the common multi-trial
structure of MEG/EEG experiments  which gives independent observations at each time point and
facilitates estimating the time-varying auto-regressive coefﬁcients. Given the state variable at each
time point  activities of source points within each ROI are modeled as independent Gaussian variables 
with the ROI activity as the mean and a shared ROI-speciﬁc variance; activities of source points
outside of all ROIs are also modeled as independent Gaussian variables with a zero mean and a shared
variance. Finally  along with the forward model that projects source activity to the sensor space  we
build a direct relationship between the state variables (ROI activities) and the sensor observations 
yielding a tractable Kalman ﬁlter model. Comparing with the previous one-step methods (10; 11) 
the main novelty of our model is the time-varying description of connectivity. We note that the
previous methods and our model all utilize speciﬁc assumptions to regularize the under-determined
source localization problem. These assumptions may not always be satisﬁed universally. However 
we expect our model to serve as a good option in the one-step model toolbox for researchers  when
the assumptions are reasonably met. In this paper  we mainly compare our model with a two-step
procedure using the commonly applied MNE method  on simulated data and in a real-world MEG
experiment.

2 Model

Model formulation In MEG/EEG experiments  researchers typically acquire multiple trials of
the same condition and treat them as independent and identically distributed (i.i.d.) samples. Each
trial includes a ﬁxed time window of (T +1) time points  aligned to the stimulus onset. Assuming
there are n sensors and q trials  we use y(r)
to denote the n-dimensional sensor readings at time
t (t = 0  1  2 ···   T ) in the rth trial (r = 1  2 ···   q). To be more succinct  when alluding to the
sensor readings in a generic trial without ambiguity  we drop the superscript (r) and use yt instead;
the same omission works for source activity and the latent ROI activity described below. We also

t

2

sensor model (forward model): yt = GJ t + et 

assume the mean of sensor data across trials is an n × (T + 1) zero matrix; this assumption can be
easily met by subtracting the n × (T + 1) sample mean across trials from the data.
MEG and EEG are mainly sensitive to electric currents in the pyramidal cells  which are perpendicular
to the folded cortical surfaces (12). Here we deﬁne the source space as a discrete mesh of m source
points distributed on the cortical surfaces  where each source point represents an electric current
dipole along the local normal direction. If we use an m-dimensional vector J t to denote the source
activity at time t in a trial  then the corresponding sensor data yt has the following form
i.i.d∼ N (0  Qe)

(1)
where the n × m matrix G describes the linear projection of the source activity into the sensor
space  and the sensor noise  et  is modeled as temporally independent draws from an n-dimensional
Gaussian distribution N (0  Qe). The noise covariance Qe can be pre-measured using recordings in
the empty room or in a baseline time window before experimental tasks.
Standard source localization methods aim to solve for J t given yt  G and Qe. In contrast  our model
aims to estimate dynamic connectivity among p pre-deﬁned regions of interest (ROIs) in the source
space (see Figure 1 for an illustration). We assume at each time point in each trial  the current dipoles
of the source points within each ROI share a common mean. Given p ROIs  we have a p-dimensional
state variable ut at time t in a trial  where each element represents the mean activity in one ROI. The
state variable ut follows a time-varying auto-regressive model of order 1

et

(2)
where Q0 is a p × p covariance matrix at t = 0  and Ats are the time-varying auto-regressive
coefﬁcients  which describe lagged dependence across ROIs. The p-dimensional Gaussian noise term
t is independent of the past  with a zero mean and a covariance matrix Q.

for t = 1 ···   T.

t ∼ N (0  Q) 

ROI model: u0 ∼ N (0  Q0)

ut = Atut−1 + t 

Figure 1: Illustration of the one-step state-space model

Now we describe how the source activity is distributed given the state variable (i.e.  the ROI means).
Below  we denote the lth element in a vector a by a[l]  and the entry in ith row and jth column of a
matrix L by L[i  j]. Let Ai be the set of indices of source points in the ith ROI (i = 1  2 ···   p);
then for any l ∈ Ai  the activity of the lth source point at time t in a trial (scalar J t[l]) is modeled as
the ROI mean plus noise 

J t[l] = ut[i] + wt[l]  wt[l] i.i.d.∼ N (0  σ2
i ) 

(3)
where wt denotes the m-dimensional noise on the m source points given the ROI means ut  at time t
in the trial. Note that the mean ut[i] is shared by all source points within the ith ROI  and the noise
term wt[l] given the mean is independent and identically distributed as N (0  σ2
i ) for all source points
within the ROI  at any time in any trial. Additionally  we denote the indices of source points outside
of any ROIs by A0 = {l  l /∈ ∪p
i=1Ai}  and similarly  for each such source point  we also assume its
activity at time t in each trial has a Gaussian distribution  but with a zero mean  and a variance σ2
0

∀l ∈ Ai 

J t[l] = 0 + wt[l]  wt[l] i.i.d.∼ N (0  σ2
0) 

∀l ∈ A0.

We can concisely re-write (3) and (4) as

source model: J t = Lut + wt  wt

i.i.d.∼ N (0  QJ )

(4)

(5)

3

 .........sensor spacesource spacestate space (ROI means)sensor modelsource modelROI modelwhere L is a 0/1 m × p matrix indicating whether a source point is in an ROI (i.e.  L[l  i] = 1
if l ∈ Ai and L[l  i] = 0 otherwise). The covariance QJ is an m × m diagonal matrix  where
each diagonal element is one among {σ2
p}  depending on which region the corresponding
1 if l ∈ A1 
source point is in; that is  QJ [l  l] = σ2
QJ [l  l] = σ2
Combining the conditional distributions of (yt|J t) given by (1) and (J t|ut) given by (5)  we can
eliminate J t (by integrating over all values of J t) and obtain the following conditional distribution
for (yt|ut)

0  σ2
0 if l ∈ A0 (outside of any ROIs)  and QJ [l  l] = σ2

2 if l ∈ A2 and so on.

1 ··· σ2

yt = Cut + ηt  ηt

i.i.d.∼ N (0  R) where C = GL  R = Qe + GQJ G(cid:48)

(6)
where G(cid:48) is the transpose of G. Putting (2) and (6) together  we have a time-varying Kalman
t }T q
ﬁlter model  where the observed sensor data from q trials {y(r)
t=0 r=1 and parameters Qe  G
i=0} are to be
t=1  Q0  Q {σ2
and L are given  and the unknown set of parameters θ = {{At}T
i }p
estimated. Among these parameters  we are mainly interested in {At}T
t=1  which describes the
spatio-temporal dependence. Let f (·) denote probability density functions in general. We can add
(cid:80)T
optional priors on θ (denoted by f (θ)) to regularize the parameters. For example  we can use
f (θ) = f ({At}T
t=2 (cid:107)At − At−1(cid:107)2
F ))  which penalizes the
squared Frobenius norm ((cid:107) · (cid:107)F ) of Ats and encourages temporal smoothness.

(cid:80)T
t=1 (cid:107)At(cid:107)2

t=1) ∝ exp(−(λ0

F + λ1

t }T q

t }T q

= E(u(r)
def

Fitting the parameters using the expectation-maximization (EM) algorithm To estimate θ  we
maximize the objective function log f ({y(r)
t=0 r=1; θ) + log f (θ) using the standard expectation-
maximization (EM) algorithm (13). Here log f ({y(r)
t=0 r=1; θ) is the marginal log-likelihood of
the sensor data  and log f (θ) is the logarithm of the prior. We alternate between an E-step and an
M-step.
In the E-step  given an estimate of the parameters (denoted by ˜θ)  we use the forward and
backward steps in the Kalman smoothing algorithm (13) to obtain the posterior mean of ut 
τ }T
|{y(r)
u(r)
τ =0)  and
t|T
τ }T
the posterior cross covariance of ut and ut−1  P (r)
τ =0)  for each t
in each trial r. Here E(·) and cov(·) denote the expectation and the covariance. More details are in
Appendix and (13).
In the M-step  we maximize the expectation of log f ({y(r)
t }T q
t=0 r=1; θ) + log f (θ) 
t }T q
t=0 r=1; ˜θ). Let tr(·) and
with respect to the posterior distribution ˜f
det(·) denote the trace and the determinant of a matrix. Given results in the E-step based on ˜θ  the
M-step is equivalent to minimizing three objectives separately

τ =0)  the posterior covariance of ut  P (r)
t|T

t=0 r=1 {u(r)
t }T q

t }T q
t=0 r=1|{y(r)

def
= cov(u(r)
t−1|{y(r)
  u(r)

= f ({u(r)

def
= cov(u(r)

|{y(r)

(t t−1)|T

τ }T

def

t

t

t

θ

Q0

t=1

i=0

{σ2

t }T q

(−E ˜f (log f ({y(r)
L1 + min
Q {At}T

t }T q
t=0 r=1; θ)) − log f (θ))
L3.
(cid:88)q

t=0 r=1 {u(r)
min
≡ min
L2 + min
i }p
L1(Q0) = q log det(Q0) + tr(Q−1
0 B0) where B0 =
L2(Q {At}T
+ log f ({At}T
where B1t =

t=1) = qT log det(Q) + tr(Q−1(cid:88)T
(cid:88)q
(cid:88)q
(cid:88)q
i=0) = q(T + 1) log det(R) + tr(R−1B4)  where R = Qe + GQJ G(cid:48) 

t|T )(cid:48))  B2t =
(t−1)|T )(cid:48))

(P (r)
(B1t − AtB(cid:48)

t|T (u(r)
(t−1)|T (u(r)

0|T + u(r)
2t − B2tA(cid:48)

B3t =
L3({σ2
i }p

(t t−1)|T + u(r)

(t−1)|T + u(r)

(cid:88)T

(cid:88)q

t|T + u(r)

r=1
(P (r)

0|T (u(r)

(P (r)

(P (r)

t=1)

r=1

r=1

r=1

[(y(r)

t − Cu(r)

t|T )(y(r)

t − Cu(r)

t|T )(cid:48) + CP (r)

t|T C(cid:48))]

and B4 =

t=1

r=1

t=0

0|T )(cid:48))

t + AtB3tA(cid:48)
t))

(9)
(t−1)|T )(cid:48)  )

t|T (u(r)

(7)

(8)

(10)

The optimization for the three separate objectives is relatively easy.

4

• For L1  the analytical solution is Q0 ← (1/q)(B0).
• For L2  optimization for {At}T

Q has the analytical solution Q ← 1/(qT )(cid:80)T

t=1 and Q can be done in alternations. Given {At}T
t=1 
t + AtB3tA(cid:48)
t).
Given Q  we use gradient descent with back-tracking line search (14) to solve for {At}T
t=1 
= 2Q−1(−B2t + AtB3t) + 2Dt  Dt = λ1(2At − At+1 −
where the gradients are ∂L2
At−1) + λ0At for t = 2 ···   T − 1  Dt = λ1(A1 − A2) + λ0A1 for t = 1  and
Dt = λ1(At − AT−1) + λ0At for t = T .

t=1(B1t − AtB(cid:48)

2t − B2tA(cid:48)

∂At

∂σi

∂σi

∂R )(cid:48) ∂R

)  where ∂L3

∂R = R−1−R−1B4R−1 and ∂R

• For L3  we can also use gradient descent to solve for σi  with the gradient ∂L3

=
= 2σiG[:  l ∈ Ai]G[:  l ∈ Ai](cid:48).
tr(( ∂L3
Here G[:  l ∈ Ai] denotes the columns in G corresponding to source points in the ith region.
Because the E-M algorithm only guarantees to ﬁnd a local optimum  we use multiple initializations 
and select the solution that yields the best objective function log f ({y(r)
t=0 r=1) + log f (θ) (see
the appendix on computing log f ({y(r)
t=0 r=1; θ)). The implementation of the model and the
E-M algorithm in Python is available at github.com/YingYang/MEEG_connectivity.

t }T q

t }T q

∂σi

Visualizing the connectivity We visualize the lagged linear dependence between any pair of
ROIs. According to the auto-regressive model in (2)  given {At}T
t=1  we can characterize the linear
dependence of ROI means at time t + h on those at time t by

where ˜At t+h =(cid:81)t+1

τ =t+h Aτ   and in(cid:81)t+1

ut+h = ˜At t+hut + noise independent of ut

τ =t+h  τ decreases from t + h to t + 1. For two ROIs indexed
by i1 and i2  ˜At t+h[i1  i2] indicates the linear dependence of the activity in ROI i1 at time t + h
on the activity in ROI i2 at time t  where the linear dependence on the activity at time t in other
ROIs and ROI i1 itself is accounted for; similarly  ˜At t+h[i2  i1] indicates the linear dependence of
the activity in ROI i2 at time t + h on the activity in ROI i1 at time t. Therefore  we can create a
T × T matrix ∆ for any pair of ROIs (i1 and i2) to describe their linear dependence at any time
lag: ∆[t  t + h] = ˜At t+h[i2  i1] (i1 leading i2) and ∆[t + h  t] = ˜At t+h[i1  i2] (i2 leading i1)  for
t = 1 ···   T and h = 1 ···   T − t − 1.

3 Results

To examine whether our state-space model can improve dynamic connectivity estimation empirically 
compared with the two-step procedure  we applied both approaches on simulated and real MEG data.
We implemented the following two-step method as a baseline for comparison. In Step 1  we applied
the minimum-norm estimate (MNE (2))  one of the most commonly used source localization methods 
to estimate J t for each time point in each trial. This is a Bayesian estimate assuming an L2 prior
on the source activity. Given G  Qe and a prior J t ∼ N (0  (1/λ)I)  λ > 0 and the corresponding
yt  the estimate is J t ← G(cid:48)(GG(cid:48) + λQe)−1yt. We averaged the MNE estimates for source points
within each ROI  at each time point and in each trial respectively  and treated the averages as an
estimate of the ROI means {ut}T q
t=0 r=1. In Step 2  according to the auto-regressive model in (2)  we
estimated Q0 {At}T
t=1 and Q by maximizing the sum of the log-likelihood and the logarithm of the
t=0 r=1) + log f ({At}T
prior (log f ({ut}T q
t=1); the maximization is very similar to the optimization
for L2 in the M-step. Details are deferred to the appendix.

3.1 Simulation

We simulated MEG sensor data according to our model assumptions. The source space was deﬁned
as m ≈ 5000 source points covering the cortical surfaces of a real brain  with 6.2 mm spacing on
average  and n = 306 sensors were used. The sensor noise covariance matrix Qe was estimated
from real data. Two bilaterally merged ROIs were used: the pericalcarine area (ROI 1)  and the
parahippocampal gyri (ROI 2) (see Figure 2a). We selected these two regions  because they were
of interest when we applied the models on the real MEG data (see Section 3.2). We generated the
auto-regressive coefﬁcients for T = 20 time points  where for each At  the diagonal entries were

5

set to 0.5  and the off-diagonal entries were generated as a Morlet function multiplied by a random
scalar drawn uniformly from the interval (−1  1) (see Figure 2b for an example). The covariances
Q0 and Q were random positive deﬁnite matrices  whose diagonal entries were a constant a. The
variances of source space noise {σ2
i }p
i=0 were randomly drawn from a Gamma distribution with the
shape parameter being 2 and the scale parameter being 1. We used two different values  a = 2 and
a = 5  respectively  where the relative strength of the ROI means compared with the source variance
{σ2
i }p
i=0 were different. Each simulation had q = 200 trials  and 5 independent simulations for each
a value were generated. The unit of the source activity was nanoampere meter (nAm).
When running the two-step MNE method for each simulation  a wide range of penalization values (λ)
were used. When ﬁtting the state-space model  multiple initializations were used  including one of
the two-step MNE estimates. In the prior of {At}T
t=1  we set λ0 = 0 and λ1 = 0.1. For the ﬁtted
parameters {At}T
t=1 and Q we deﬁned the relative error as the Frobenius norm of the difference
between the estimate and the true parameter  divided by the Frobenius norm of the true parameter
(e.g.  for the true Q and the estimate ˆQ  the relative error was (cid:107) ˆQ − Q(cid:107)F /(cid:107)Q(cid:107)F ). For different
two-step MNE estimates with different λs  the smallest relative error was selected for comparison.
Figure 2c and 2d show the relative errors and paired differences in errors between the two methods;
in these simulations  the state-space model yielded smaller estimation errors than the two-step MNE
method.

(a)

(b)

(c)

(d)

Figure 2: Simulation results. (a)  Illustration of the two ROIs. (b)  The auto-regressive coefﬁcients
{At}T
t=1 of T = 20 time points in one example simulation (a = 5). Here A[:  i1  i2] indicates the
time-varying coefﬁcient in At[i1  i2]  for i1  i2 = 1  2. (The legends: truth (blue)  true values; ss
(green)  estimates by the state-space model; mne (red)  estimates by the two-step MNE method.) (c)
and (d)  Comparison of the state-space model (ss) with the two-step MNE method (mne) in relative
errors of {At}T
t=1 (c) and Q (d). The error bars show standard errors across individual simulations.

3.2 Real MEG data on scene processing

We also applied our state-space model and the two-step MNE method on real MEG data  to explore
the dynamic connectivity in the visual cortex during scene processing. It is hypothesized that the
ventral visual pathway  which underlies recognition of what we see  is organized in a hierarchical
manner—along the pathway  regions at each level of the hierarchy receive inputs from previous
levels  and perform transformations to extract features that are more and more related to semantics
(e.g.  categories of objects/scenes ) (15). Besides such feedfoward processing  a large number of
top-down anatomical connections along the hypothesized hierarchy also suggest feedback effects

6

 ROI 1ROI 2right hemisphereleft hemisphereA[: 1 1]A[: 2 2]A[: 1 2]A[: 2 1]05101520time index−1.0−0.50.00.51.0A[: 1 1]truthssmne05101520−1.0−0.50.00.51.0A[: 1 2]05101520−1.0−0.50.00.51.0A[: 2 1]05101520−1.0−0.50.00.51.0A[: 2 2]25a−1.0−0.50.00.51.0A errorssmnediff ss-mne25a−1.0−0.50.00.51.0Q error(16). Evidence for both directions has been reported previously (17; 18). However  details of the
dynamic information ﬂow during scene processing  such as when and how signiﬁcant the feedback
effect is  is not well understood. Here  as an exploratory step  we estimate dynamic connectivity
between two regions in the ventral pathway: the early visual cortex (EVC) at the lowest level (in the
pericalcarine areas)  which is hypothesized to process low-level features such as local edges  and
the parahippocampal place area (PPA)  which is a scene-sensitive region on the higher level of the
hierarchy and has been implicated in processing semantic information (19).
The 306-channel MEG data were recorded while a human participant was viewing 362 photos of
various scenes. Each image was presented for 200 ms and repeated 5 times across the session 
and data across the repetitions were averaged  resulting in q = 362 observations. The data was
down-sampled from a sampling rate of 1 kHz to 100 Hz  and cropped within −100 ∼ 700 ms  where
0 ms marked the stimulus onset. Together  we had T + 1 = 80 time points (see the appendix for
more preprocessing details). Given the data  we estimated the dynamic connectivity between the
neural responses to the 362 images in the two ROIs (EVC and PPA)  using our state-space model
and the two-step MNE method. We created a source space including m ≈ 5000 source points for the
participant. In the prior of {At}T
t=1  we set λ0 = 0 and λ1 = 1.0; in the two-step MNE method  we
used the default value of the tuning parameter (λ) for single-trial data in the MNE-python software
(20). After ﬁtting Q0  {At}T
t=1 and Q  we computed the ∆ matrix  as deﬁned in Section 2  to
visualize the lagged linear dependence between the two ROIs (EVC and PPA). We also bootstrapped
the 362 observations 27 times to obtain standard deviations of entries in ∆  and then computed a
z-score for each entry  deﬁned as the ratio between the estimated value and the bootstrapped standard
deviation. Note that the sign of the source activity only indicates the direction of the electric current 
so negative entries in ∆ are as meaningful as positive ones. We ran two-tailed z-tests on the z-scores
(assuming a standard normal null distribution); then we plotted the absolute values of the z-scores
that passed a threshold where the p-value < 0.05/(T 2)  using the Bonferroni correction for T 2
comparisons in all the entries (Figure 3). Larger absolute values indicate more signiﬁcant non-zero
entries of ∆  and more signiﬁcant lagged linear dependence. As illustrated in Figure 3a  the lower
right triangle of ∆ indicates the linear dependence of PPA activity on previous EVC activity (EVC
leading PPA  lower- to higher-level)  whereas the upper left triangle indicates the linear dependence
of EVC activity on previous PPA activity (PPA leading EVC  higher- to lower-level).

(a) Illustration of the ROIs and ∆ (b) Results by the state-space model
Figure 3: Results from real MEG data on scene processing. (a)  illustration of ROIs and the triangular
parts of ∆. (b) and (c)  thresholded z-scores of ∆ by the state-space model (b) and by the two-step
MNE method (c).

(c) Results by the two-step MNE

Figure 3b and 3c show the thresholded absolute values of the z-scores by the state-space model and
the two-step MNE method. In Figure 3b by the state-space model  we observed clusters indicating
signiﬁcant non-zero lagged dependence  in the lower right triangle  spanning roughly from 60 to
280 ms in EVC and from 120 to 300 ms in PPA  which suggests earlier responses in EVC can predict
later responses in PPA in these windows. This pattern could result from feedforward information
ﬂow  which starts when EVC ﬁrst receives the visual input near 60 ms. In the upper left triangle 
we also observed clusters spanning from 100 to 220 ms in PPA and from 140 to 300 ms in EVC 
suggesting earlier responses in PPA can predict later responses in EVC  which could reﬂect feedback
along the top-down direction of the hierarchy. Figure 3c by the two-step MNE method also shows
clusters in similar time windows  yet the earliest cluster in the lower right triangle appeared before
0 ms in EVC  which could be a false positive as visual input is unlikely to reach EVC that early.

7

 PPAright hemisphereleft hemisphereEVCPPAEVCPPA leadingEVCEVC leadingPPA0100200300400500600700PPA time (ms)0100200300400500600700EVC time (ms)0123456789100100200300400500600700PPA time (ms)0100200300400500600700EVC time (ms)012345678910We also observed a small cluster in the top right corner near the diagonal by both methods. This
cluster could indicate late dependence between the two regions  but it was later than the typically
evoked responses before 500 ms. These preliminary results were based on only one participant  and
further analysis for more participants is needed. In addition  the apparent lagged dependence between
the two regions are not necessarily direct or causal interactions; instead  it could be mediated by
other intermediate or higher-level regions  as well as by the stimulus-driven effects. For example 
the disappearance of the stimuli at 200 ms could cause an image-speciﬁc offset-response starting at
260 ms in the EVC  which could make it seem that image-speciﬁc responses in PPA near 120 ms
predicted the responses at EVC after 260 ms. Therefore further analysis including more regions is
needed  and the stimulus-driven effect needs to be considered as well. Nevertheless  the interesting
patterns in Figure 3b suggest that our one-step state-space model can be a promising tool to explore
the timing of feedforward and feedback processing in a data-driven manner  and such analysis can
help to generate speciﬁc hypotheses about information ﬂow for further experimental testing.

4 Discussion

We propose a state-space model to directly estimate the dynamic connectivity across regions of interest
from MEG/EEG data  with the source localization step embedded. In this model  the mean activities
in individual ROIs  (i.e.  the state variable)  are modeled with time-varying auto-regression  which
can ﬂexibly describe the spatio-temporal dependence of non-stationary neural activity. Compared
with a two-step method  which ﬁrst obtains the commonly used minimum-norm estimate of source
activity  and then ﬁts the auto-regressive model  our state-space model yielded smaller estimation
errors than the two-step method in simulated data  where the assumptions in our model held. When
applied on empirical MEG data from one participant in a scene-processing experiment  our state-
space model also demonstrated intriguing preliminary results  indicating leading and lagged linear
dependence between the early visual cortex and a higher-level scene-sensitive region  which could
reﬂect feedforward and feedback information ﬂow within the visual cortex. In sum  these results
shed some light on how to better study dynamic connectivity using MEG/EEG and how to exploit the
estimated connectivity to study information ﬂow in cognition.
One limitation of the work here is that we did not compare with other one-step models (10; 11). In
future work  we plan to do comprehensive empirical evaluations of the available one-step methods.
Another issue is there can be violations of our model assumptions in practice. First  given the
ROI means  the noise on source points could be spatially and temporally correlated  rather than
independently distributed. Secondly  if we fail to include an important ROI  the connectivity estimates
may be inaccurate—the estimates may not even be equivalent to the estimates when this ROI is
marginalized out  due to the under-determined nature of source localization. Thirdly  the assumption
that source points within an ROI share a common mean is typically correct for small ROIs but could
be less accurate for larger ROIs  where the diverse activities of many source points might not be
well-represented by a one-dimensional mean activity. That being said  as long as the activity in
different source points within the ROI is not fully canceled  positive dependence effects of the kind
identiﬁed by our model would still be meaningful in the sense that they reﬂect some cross-region
dependence. To deal with the last two issues  one may divide the entire source space into sufﬁciently
small  non-overlapping ROIs  when applying our state-space model. In such cases  the number of
parameters can be large  and some sparsity-inducing regularization (such as the one in (11)) can
be applied. In ongoing and future work  we plan to explore this idea and also address the effect of
potential assumption violations.

Acknowledgments

This work was supported in part by the National Science Foundation Grant 1439237  the National
Institute of Mental Health Grant RO1 MH64537  as well as the Henry L. Hillman Presidential
Fellowship at Carnegie Mellon University.

References
[1] J. C. Mosher  R. M. Leahy  and P. S. Lewis. EEG and MEG: forward solutions for inverse

methods. Biomedical Engineering  IEEE Transactions on  46(3):245–259  1999.

8

[2] M. Hamalainen and R. Ilmoniemi. Interpreting magnetic ﬁelds of the brain: minimum norm

estimates. Med. Biol. Eng. Comput.  32:35–42  1994.

[3] A. M. Dale  A. K. Liu  B. R. Fischl  R. L. Buckner  J. W. Belliveau  J. D. Lewine  and E. Halgren.
Dynamic statistical parametric mapping: combining fMRI and MEG for high-resolution imaging
of cortical activity. Neuron  26(1):55–67  2000.

[4] A. Gramfort  M. Kowalski  and M. Hamaleinen. Mixed-norm estimates for the m/eeg inverse
problem using accelerated gradient methods. Physics in Medicine and Biology  57:1937–1961 
2012.

[5] R. D. Pascual-Marqui  C. M. Michel  and D. Lehmann. Low resolution electromagnetic
tomography: a new method for localizing electrical activity in the brain. International Journal
of psychophysiology  18(1):49–65  1994.

[6] A. Galka  O. Y. T. Ozaki  R. Biscay  and P. Valdes-Sosa. A solution to the dynamical inverse
problem of eeg generation using spatiotemporal kalman ﬁltering. NeuroImage  23:435–453 
2004.

[7] J. Mattout  C. Phillips  W. D. Penny  M. D. Rugg  and K. J. Friston. MEG source localization
under multiple constraints: an extended bayesian framework. NeuroImage  30(3):753–767 
2006.

[8] C. Lamus  M. S. Hamalainen  S. Temereanca  E. N. Brown  and P. L. Purdon. A spatiotemporal

dynamic distributed solution to the MEG inverse problem. NeuroImage  63:894–909  2012.

[9] V. Sakkalis. Review of advanced techniques for the estimation of brain connectivity measured

with EEG/MEG. Computers in biology and medicine  41(12):1110–1117  2011.

[10] O. David  S. J. Kiebel  L. M. Harrison  J. Mattout  J. M. Kilner  and K. J. Friston. Dynamic
causal modeling of evoked responses in EEG and MEG. NeuroImage  30(4):1255–1272  2006.
[11] M. Fukushima  O. Yamashita  T. R. Knösche  and M.-a. Sato. MEG source reconstruction
based on identiﬁcation of directed source interactions on whole-brain anatomical networks.
NeuroImage  105:408–427  2015.

[12] M. Hamalainen  R. Hari  R. J.

J. Knuutila  and O. V. Lounasmaa.
Magnetoencephalography–theory  instrumentation  to noninvasive studies of the working human
brain. Reviews of Modern Physics  65:414–487  1993.

Ilmoniemi 

[13] R. H. Shumway and D. S. Stoffer. An approach to time series smoothing and forecasting using

the EM algorithm. Journal of time series analysis  3(4):253–264  1982.

[14] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  New York 

NY  USA  2004.

[15] J. J. DiCarlo and D. D. Cox. Untangling invariant object recognition. Trends in cognitive

sciences  11(8):333–341  2007.

[16] D. J. Felleman and D. C. Van Essen. Distributed hierarchical processing in the primate cerebral

cortex. Cerebral cortex  1(1):1–47  1991.

[17] R. M. Cichy  A. Khosla  D. Pantazis  A. Torralba  and A. Oliva. Deep neural networks predict
hierarchical spatio-temporal cortical dynamics of human visual object recognition. arXiv
preprint arXiv:1601.02970  2016.

[18] M. Bar  K. S. Kassam  A. S. Ghuman  J. Boshyan  A. M. Schmid  A. M. Dale  M. S. Hämäläinen 
K. Marinkovic  D. L. Schacter  B. R. Rosen  et al. Top-down facilitation of visual recognition.
Proceedings of the National Academy of Sciences of the United States of America  103(2):449–
454  2006.

[19] R. Epstein  A. Harris  D. Stanley  and N. Kanwisher. The parahippocampal place area: Recogni-

tion  navigation  or encoding? Neuron  23(1):115–125  1999.

[20] A. Gramfort  M. Luessi  E. Larson  D. A. Engemann  D. Strohmeier  C. Brodbeck  R. Goj 
M. Jas  T. Brooks  L. Parkkonen  et al. Meg and eeg data analysis with mne-python. Frontiers
in neuroscience  7:267  2013.

9

,Ying Yang
Elissa Aminoff
Michael Tarr
Kass Robert
Brett Daley
Christopher Amato