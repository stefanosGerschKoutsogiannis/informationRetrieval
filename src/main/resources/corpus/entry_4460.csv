2019,Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters,How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper  we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools  we present an algorithm  Wasserstein Q-Learning (WQL)  starting in the tabular case and then  we show how it can be extended to deal with continuous domains. Furthermore  we prove that  under mild assumptions  a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally  we present an experimental campaign to show the effectiveness of WQL on finite problems  compared to several RL algorithms  some of which are specifically designed for exploration  along with some preliminary results on Atari games.,Propagating Uncertainty in Reinforcement Learning

via Wasserstein Barycenters

Alberto Maria Metelli∗

DEIB

Politecnico di Milano

Milan  Italy

Amarildo Likmeta∗

DEIB

Politecnico di Milano

Milan  Italy

Marcello Restelli

DEIB

Politecnico di Milano

Milan  Italy

albertomaria.metelli@polimi.it

amarildo.likmeta@polimi.it

marcello.restelli@polimi.it

Abstract

How does the uncertainty of the value function propagate when performing tem-
poral difference learning? In this paper  we address this question by proposing a
Bayesian framework in which we employ approximate posterior distributions to
model the uncertainty of the value function and Wasserstein barycenters to propa-
gate it across state-action pairs. Leveraging on these tools  we present an algorithm 
Wasserstein Q-Learning (WQL)  starting in the tabular case and then  we show how
it can be extended to deal with continuous domains. Furthermore  we prove that 
under mild assumptions  a slight variation of WQL enjoys desirable theoretical
properties in the tabular setting. Finally  we present an experimental campaign
to show the effectiveness of WQL on ﬁnite problems  compared to several RL
algorithms  some of which are speciﬁcally designed for exploration  along with
some preliminary results on Atari games.

1

Introduction

Effectively balancing exploration and exploitation is a key challenge in Reinforcement Learning [RL 
43]. When an agent takes decisions under uncertainty  it faces the dilemma between exploiting the
information collected so far to execute what is believed to be the best action or to choose a possibly
suboptimal action to explore new portions of the environment and gather new information  leading
to more proﬁtable behaviors in the future. Traditional exploration strategies  such as -greedy and
Boltzmann exploration [43]  inject random noise into the action-selection process  i.e.  the policy  to
guarantee that each action is tried often enough. Although these methods allow RL algorithms to learn
the optimal value function under mild assumptions [39]  they are not efﬁcient  since exploration is
random and not driven by conﬁdence on the value function estimate. Therefore  they might converge
towards the optimal behavior after an exponential number of steps [24].
The exploration-exploitation dilemma has been extensively analyzed in the RL community  focusing
on the deﬁnition of proper indices for provably-efﬁcient exploration and devising algorithms with
strong theoretical guarantees [25  11  21  30]. Most of these algorithms are inherently model-based 
i.e.  they need to maintain and update estimates of the environment dynamics and the reward function
during the learning process. For this reason  model-based methods are rather unsuited to problems
with large state spaces and inapplicable to continuous environments. Apart from rare exceptions [41] 
the RL community has only recently focused on devising efﬁcient model-free exploration strategies.
Some works have succeeded in obtaining provably-efﬁcient algorithms [35  31  23]; whereas others
are more empirically-oriented [30  29  6].

∗Equal contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

A fundamental step towards efﬁcient exploration is the quantiﬁcation of the uncertainty of the value
function. The notion of uncertainty is formalized in Bayesian statistics by means of a posterior
distribution. Bayesian Reinforcement Learning incorporates the Bayesian inference tools to provide a
principled way to address the exploration-exploitation dilemma [20]. However  these methods rarely
exploit the speciﬁc way in which the uncertainty propagates through the Bellman equation. Recently 
in [28] a partial answer has been provided  proposing an uncertainty Bellman inequality; although no
posterior distribution is explicitly considered.
In this paper  we propose a novel Bayesian framework to address the problem of exploration using
posterior distributions over the value function. Speciﬁcally  we focus on how to model and propagate
uncertainty when performing temporal-difference learning (Section 3). Moreover  we show how
to use this uncertainty information to effectively explore the environment. Finally  we combine
these elements to build our algorithm: Wasserstein Q-Learning (WQL  Section 4). Similarly to
Bayesian Q-Learning [15]  we equip each state-action pair with an approximate posterior distribution
(named Q-posterior)  whose goal is to quantify the uncertainty of the value function. Whenever a
transition occurs  we update our distribution  in a temporal difference [TD  43] fashion  in order to
incorporate all sources of uncertainty: i) the one due to the sample estimate of the reward function
and environment dynamics; ii) the uncertainty injected using the estimate of the next-state value
function. Rather than employing a standard Bayesian update  we resort to a variational approach to
approximate the posterior distribution  based on Wasserstein barycenters [2]. Recently  several works
have embedded into RL algorithms notions coming from Optimal Transport [OT  51]  especially
the Wasserstein metric  to improve the learning abilities of policy search algorithms [34] or in the
ﬁled of robust RL [1]. Furthermore  we prove in Section 5  that a slight modiﬁcation of WQL  in
tabular domains  is PAC-MDP in the average loss setting [42]. After examining the related literature
(Section 6)  we present an experimental evaluation on tabular environments to show the effectiveness
of WQL  compared to the classic RL algorithms  some of which speciﬁcally designed for exploration
(Section 7.1). Finally  we provide some preliminary results on the application of WQL to deep
architectures (Section 7.2). The proofs of all results are reported in Appendix B. The implementation
of the proposed algorithms can be found at https://github.com/albertometelli/wql.

2 Preliminaries

In this section  we provide the notation and the basic notions we will use in the following. Given a
set X   we denote with P(X ) the set of all probability measures over X .
Markov Decision Processes A discrete-time Markov Decision Process [MDP  36] is deﬁned as
a 5-tuple M = (S A P R  γ)  where S is the state space  A is the (ﬁnite) action space  P :
S × A → P(S) is a Markovian transition model  R : S × A → P(R) is a Markovian reward
model  γ ∈ [0  1) is the discount factor. The behavior of an agent is deﬁned by means of a
Markovian policy π : S → P(A). Whenever the environment is in state s ∈ S  the agent performs
action A ∼ π(·|s) and the environment transitions to the next state S(cid:48)
∼ P(·|s  A) providing
the agent with the reward R ∼ R(·|s  A). We assume |R| ≤ rmax < +∞ almost surely. We
indicate with r(s  a) = ER∼R(·|s a)[R] the expected reward obtained by taking action a ∈ A
in state s ∈ S. Given a policy π we deﬁne the state-value function  or V-function  as vπ(s) =
EA∼π(·|s) S(cid:48)∼P(·|s A) [r(s  A) + γvπ(S(cid:48))]. The action-value function  or Q-function  is given by
qπ(s  a) = r(s  a) + γ ES(cid:48)∼P(·|s a) A(cid:48)∼π(·|S(cid:48)) [qπ(S(cid:48)  A(cid:48))]. The optimal action-value function is
deﬁned as q∗(s  a) = supπ∈Π{qπ(s  a)} for all (s  a) ∈ S × A and it satisﬁes the optimal Bellman
equation: q∗(s  a) = r(s  a)+γ ES(cid:48)∼P(·|s a) [maxa(cid:48)∈A{q∗(S(cid:48)  a(cid:48))}]. The boundedness of the reward
function implies that the Q-function is uniformly bounded  i.e.  |q∗(s  a)| ≤ qmax ≤ rmax/(1 − γ).
Then  an optimal policy π∗ is any policy that plays only greedy actions w.r.t. q∗  i.e.  for all s ∈ S we
have π∗(·|s) ∈ P (arg maxa∈A{q∗(s  a)}).
Temporal Difference Learning Temporal-difference methods update the estimate of the optimal
Q-function using the estimates of the next states V-functions [43]. For TD(0)  we have that whenever
a (St  At  St+1  Rt+1) tuple is collected  the temporal difference update rule is executed:

(1)
where qt is the estimated Q-function at time t  αt ≥ 0 is a learning rate  and vt is an estimate of the
V-function at time t. Different choices for vt generate different learning algorithms. If vt(St+1) =

qt+1(St  At) = (1 − αt) qt(St  At) + αt (Rt+1 + γvt(St+1))  

2

qt(St+1  At+1) we get the SARSA update [38]  if vt(St+1) = EA∼πt(·|St+1)[qt(St+1  A)] we get the
Expected SARSA update [50]  being πt the exploration policy played at time t  and if vt(St+1) =
maxa∈A{qt(St+1  a)} we are performing Q-learning [52].
Wasserstein Barycenters Let (X   d) be a complete separable metric (Polish) space and x0 ∈ X
be an arbitrary point. For each p ∈ [1  +∞) we deﬁne Pp(X ) as the set of all probability measures
µ over (X   F ) such that EX∼µ[d(X  x0)p] < +∞. Let µ  ν ∈ Pp(X )  the Lp-Wasserstein distance
between µ and ν is deﬁned as [51]:

(cid:18)

(cid:19)1/p

Wp(µ  ν) =

inf

ρ∈Γ(µ ν)

E

X Y ∼ρ

[d(X  Y )p]

 

(2)

where Γ(µ  ν) is the set of all probability measures on X × X (couplings) with marginals µ and ν.
With little abuse of notation  we will indicate with Wp(X  Y ) = Wp(µ  ν)  whenever clear from
the context. The Wasserstein distance comes from the optimal transport community. Intuitively  it
represents the “cost” to move the probability mass to turn one distribution into the other. Given a set of
probability measures {νi}n
i=1 ξi = 1
and ξi ≥ 0  the L2-Wasserstein barycenter is deﬁned as [2]:

i=1  belonging to the class N   and a set of weights {ξi}n

i=1 (cid:80)n

(cid:41)

ν = arg inf
ν∈N

ξiW2(ν  νi)2

.

(3)

(cid:40) n(cid:88)

i=1

3 How to Model and Propagate Uncertainty?

In this section  we introduce a unifying Bayesian framework for exploration in RL that employs
(approximate) posterior distributions to model uncertainty of value functions (Section 3.1) and Wasser-
stein barycenters to propagate uncertainty when performing TD updates (Section 3.2). Furthermore 
we discuss how to leverage on the Q-posteriors to estimate the action that attains the maximum return
in each state (Section 3.3) and to effectively explore the environment (Section 3.4).

3.1 Modeling Uncertainty via Q-Posteriors
Taking inspiration from Bayesian approaches to RL [15  20]  for each state s ∈ S and action a ∈ A
we maintain a probability distribution Q(s  a)  which we call Q-posterior  representing a (possibly
approximate) posterior distribution of the Q-function estimate. This distribution will depend on
the underlying MDP  in particular  the environment dynamics P and reward model R  and on the
updates of the Q-function estimates performed. As in a model-free scenario we cannot represent
such distribution exactly  we employ a class of approximating probability distributions Q ⊆ P(R).
Similarly to usual value functions  we introduce the V-posterior V(s) which represents the (possibly
approximate) posterior distribution of V-function  that combines the uncertainties modeled by the
Q-posteriors Q(s  a). Furthermore  being the V-function deﬁned  in the usual framework  as the
expectation of the Q-function over the action space  i.e.  vπ(s) = EA∼π[qπ(s  a)]  it is natural to
deﬁne  in our setting  the V-posterior V(s) as the Wasserstein barycenter of the Q-posteriors Q(s  a).2
Deﬁnition 3.1 (V-posterior). Given a policy π and a state s ∈ S  we deﬁne the V-posterior V(s)
induced by the Q-posteriors Q(s  a) with a ∈ A as the Wassertein barycenter of the Q(s  a):

(cid:26)

(cid:104)

W2 (V Q(s  A))2(cid:105)(cid:27)

V(s) ∈ arg inf
V∈Q

E

A∼π(·|s)

.

(4)

When the policy π is known  the expectation over the action space can be computed as we are
assuming that A is ﬁnite. In a prediction problem  policy π is a ﬁxed policy  whereas  in a control
problem  π is a policy aimed at properly selecting the best action in state s accounting for the
uncertainty modeled by the Q-posterior (see Section 3.3). Moreover  when Q(s  a) are deterministic
distributions  V(s) is a deterministic distribution too centered in the mean of the Q(s  a). In this way 
we obtain the usual V-function deﬁnition (see Proposition A.3).

2The Wasserstein barycenter can be regarded as a way of averaging distributions [2].

3

It is important to stress that our approach is rather different from Distributional Reinforcement
Learning [9  13  12  37]. Indeed  we employ a distribution to represent the uncertainty of the Q-
function estimate and not the intrinsic randomness of the return. The two distributions are clearly
related and both depend on the stochasticity of the reward and of the transition model. However  in
our approach the stochasticity refers to the uncertainty on the Q-function estimate which reduces as
the number of updates increases  being a sample mean.3

3.2 Propagating Uncertainty via Wasserstein Barycenters

In this section  we discuss the problem of uncertainty propagation  i.e.  how to deal with the update
of the Q-posteriors when experiencing a transition (St  At  St+1  Rt+1). Whenever a TD update
(Equation (1)) is performed  there are two sources of uncertainty involved. First  we implicitly
estimate the environment dynamics P(·|St  At) and the reward model R(·|St  At) using a set of
sampled transitions (St  At  St+1  Rt+1). Second  when using the V-function estimates of the next
states vt(St+1) we bring into qt+1(St  At) part of the uncertainty of vt(St+1) and they become
correlated. For this reason  the standard Bayesian posterior update  used for instance in Bayesian
Q-learning [15]  becomes rather inappropriate as it assumes that the samples are independent  which
is clearly not true. We argue that  rather than using a Bayesian update  when we have a Q-posterior
Qt(St  At) and a V-posterior Vt(St+1) we can combine them using a notion of barycenter  which
does not require the independence assumption. We formalize this idea in the following update rule.
Deﬁnition 3.2 (Wasserstein Temporal Difference). Let Qt be the current Q-posterior  given a
transition (St  At  St+1  Rt+1)  we deﬁne the TD-target-posterior as Tt = Rt+1 + γVt(St+1). Let
αt ≥ 0 be the learning rate  we deﬁne the Wasserstein Temporal Difference (WTD) update rule as:
(5)

(cid:110)
(1 − αt)W2 (Q Qt(St  At))2 + αtW2 (Q Tt)2(cid:111)

Qt+1(St  At) ∈ arg inf
Q∈Q

.

Therefore  the new Q-posterior Qt+1(St  At) is the Wasserstein barycenter between the current
Q-posterior Qt(St  At) and the TD-target posterior Tt = Rt+1 + γVt(St+1)  which in turn embeds
information of the current transition (i.e.  the reward Rt+1 and the next state St+1) and the next-state
V-posterior Vt(St+1). It is worth noting that the two terms appearing in Equation (5) account for
all sources of uncertainty. Indeed  the ﬁrst term W2 (Q Qt(St  At)) avoids moving too far from the
current estimation Qt(St  At)  as we are performing the update experimenting a single transition 
whereas W2 (Q Tt) allows bringing in the new Q-posterior the V-posterior of the next-state Vt(St+1)
(including its uncertainty). We stress the analogy with the standard TD update in the following result.

Proposition 3.1. If Q is the set of deterministic distributions over R  then the WTD update rule
(Equation (5)) has a unique solution that corresponds to the TD update rule (Equation (1)).

Supporting deterministic distributions  as the Q-posteriors  is fundamental for our method that models
a sample mean  whose variance reduces as the number of samples increases  moving towards a
deterministic distribution. This justiﬁes the choice of the Wasserstein metric over other distributional
distances (e.g.  α-divergences). The choice of the prior for Q0 plays an important role  along with
the learning rate schedule αt. We will show in Section 5 that speciﬁc choices of Q0 and αt  for a
particular class of distributions Q  allow achieving PAC-MDP property in the average loss setting.

3.3 Estimating the Maximum Expected Value
The TD-target-posterior Tt = Rt+1 + γVt(St+1) is deﬁned in terms of the next state V-posterior
Vt(St+1). In a control problem  we aim at learning the optimal Q-function q∗ and  thus  we are
interested in propagating back to Qt+1(St  At) a V-posterior Vt(St+1) related to the optimal action
(cid:1).
to be taken in the next state.4 This can be performed by a suitable choice of the policy π  as
in Deﬁnition 3.1. A straightforward approach consists in propagating the Q-posterior Q(St+1  a)

of the action with the highest estimated mean  i.e.  πM (·|s) ∈ P(cid:0)arg maxa∈A{EQ∼Q(s a)[Q]}

3A notable difference w.r.t. the distributional RL is that the variance of our posterior distribution
4We stress that we are uninterested in modeling the distribution maxa∈A{Q(s  a)}  but rather in exploiting

VarQ∼Q(s a)[Q] vanishes as the number of updates grows to inﬁnity.
the uncertainty modeled by Q(s  a) to properly perform the computation of the optimal action.

4

Table 1: Probability density function (pdf)  Wasserstein Temporal Difference (WTD) update rule and
computation of the V-posterior for Gaussian and Particle posterior distributions.

Q

Gaussian

Particle

exp

σ(s a)

− 1

2

pdf

(cid:17)2(cid:27)

(cid:26)
(cid:16) x−m(s a)
(cid:112)2πσ2(s  a)
(cid:80)M
(cid:80)M
j=1 wjδ(x − xj(s  a))
x1(s  a) ≤ ... ≤ xM (s  a)
i=j wj = 1 and wj ≥ 0

WTD and V-posterior
mt+1(St  At) = αtmt(St  At) + (1 − αt) (Rt+1 + γmt(St+1))
σt+1(St  At) = αtσt(St  At) + (1 − αt)γσt(St+1)
m(s) = EA∼π(·|s) [m(s  A)]
σ(s) = EA∼π(·|s) [σ(s  A)]

xj t+1(St  At) = αtxj t(St  At) + (1− αt) (Rt+1 + γxj t(St+1))
xj(s) = EA∼π(·|s) [xj(s  A)]   j = 1  2  ...  M

We refer to this approach as Mean Estimator (ME) for the maximum. However  when posterior
distributions are available  we can use them to deﬁne a wiser way to estimate the V-posterior of
the next state.5 A ﬁrst method based on Optimism in the Face of Uncertainty [OFU  3] consists
in selecting the action that maximizes a statistical upper bound uδ(s  a) of the Q-posterior  i.e. 

(cid:1). We will refer to this method as Optimistic Estimator

πO(·|s) ∈ P(cid:0)arg maxa∈A{uδ(s  a)}

(OE). However  if we want to make full usage of the Q-posteriors  we can resort to the Posterior
Estimator (PE) of the maximum  based on Posterior Sampling [PS  47]. In this case  each action
contributes to the update rule weighted by the probability of being the optimal action  i.e.  πP (a|s) =
PrQs a∼Q(s a) (a ∈ arg maxa(cid:48)∈A{Qs a(cid:48)}).
3.4 Exploring using the Q-posteriors

In the previous section  we have introduced two approaches that exploit the Q-posterior to properly
deﬁne the V-posterior of the next state  using speciﬁc policies π. These policies can also be used to
implement effective exploration strategies aware of the uncertainty. Using the optimistic policy πO
in each state  we play (deterministically) the action that maximizes the statistical upper bound on
the estimated Q-function uδ(s  a)  we call this strategy Optimistic Exploration (OX). Instead  we
can directly use the posterior policy πP to sample the action from the Q-posterior Q(s  a). Thus  in
Posterior Exploration (PX)  each action is played with the probability of being optimal.

4 Wasserstein Q-Learning

Input: a prior distribution Q0  a step size schedule
(αt)t≥0  an exploration policy schedule (πt)t≥0
1: Initialize Q(s  a) with the prior Q0
2: for t = 1  2  ... do
3:
4:
5:
6:
7: end for

The ideas presented so far can be combined in
an algorithm  Wasserstein Q-Learning (WQL) 
whose pseudocode is reported in Algorithm 1.
We developed our approach for a generic class of
distributions Q  however  in practice  we focus
on two speciﬁc classes: Gaussian posteriors (G-
WQL) and Particle posteriors (P-WQL)  i.e.  a
mixture of M > 1 Dirac deltas. For both classes
the Wasserstein Barycenter is unique and can be
computed in closed form (see Appendix A.3).6 In
Table 1  we summarize the main relevant features
of these distributions classes. WQL simply needs
to store the parameters of the Q-posterior for every state-action pair (m(s  a) and σ(s  a) for G-WQL
and xj(s  a) for P-WQL). Therefore  unlike the majority of provably-efﬁcient algorithms  it can be
extended straightforwardly to continuous state spaces as long as we adopt a function approximator
for the parameters of the posterior. For instance  we could approximate m(s  a) and σ(s  a) or the
particles xj(s  a) using a neural network with multiple heads. For this reason  our method easily

Take action At ∼ πt(·|St)
Observe St+1 and Rt+1
Compute Vt(St+1) using Equation (4)
Update Qt+1(St  At) using Equation (5)

Algorithm 1: Wasserstein Q-Learning.

5This problem was treated in RL  without distributions  proposing several estimators  such as the double

estimator [48] and the weighted estimator [17  16].

6It is worth noting that  even for the Gaussian case  using the standard Bayesian posterior update is inappro-

priate  as the independence of the Q-function estimates across state-action pairs cannot be assumed.

5

applies to deep architecture by adopting a network that directly outputs the posterior parameters 
instead of the value function (see Section 7.2).

5 Theoretical Analysis

In this section  we show that WQL  with some modiﬁcations  enjoys desirable theoretical properties
in the tabular setting. We start providing a modiﬁcation of the WTD update rule that will be used for
the analysis; then we prove that with such modiﬁcation our algorithm  under certain assumptions  is
PAC-MDP in the average loss setting [42].
Deﬁnition 5.1 (Modiﬁed Wasserstein Temporal Difference). Let Qt be the current Q-posterior and
Qb be a zero-mean distribution  given a transition (St  At  St+1  Rt+1)  we deﬁne the TD-target-
posterior as Tt = Rt+1 + γVt(St+1). Let αt   βt ≥ 0 be the learning rates  we deﬁne the Modiﬁed
Wasserstein Temporal Difference (MWTD) update rule as:

(cid:101)Qt+1(St  At) ∈ arg inf

Q∈Q

Qt+1(St  At) ∈ arg inf
Q∈Q

W2

(cid:26)
(cid:26)

(cid:16)

(cid:17)2
Q  (cid:101)Qt(St  At)
(cid:17)2(cid:27)
+ αtW2 (Q Tt)2
Q  (cid:101)Qt+1(St  At) + βtQb

 

(cid:16)

(1 − αt)W2

(cid:27)

 

We will denote the algorithm employing this update rule as Modiﬁed Wasserstein Q-Learning
(MWQL). The reason why we need to change the WTD lies in the fact that the uncertainty on the
Q-function value (the Q-posterior) is  as already mentioned  the contribution of two terms: i) the
uncertainty on the reward and transition model; ii) the uncertainty on the next-state Q-function. These
terms need to be averaged into the Q-posterior at different speeds. If nt(s  a) is the number of times

(s  a) is visited up to time t  (i) has to reduce proportionally to 1/(cid:112)nt(s  a) being a sample mean 

(cid:80)T

(cid:80)T
i=t γi−tRi+1. The quantity LA = 1

while (ii) is averaged with coefﬁcients proportional to 1/nt(s  a). Therefore  we should keep the two
sources of uncertainty separated. To this end  we use an additional distribution Qb to prevent the
uncertainty from reducing too fast.
The notion of PAC-MDP in the average loss setting [42] is a relaxation of the classical PAC-MDP
notion introduced in [24]  in which we consider the actual reward received by the algorithm while
learning  instead of the expected values over future policies. We recall the deﬁnitions given in [42].
Deﬁnition 5.2 (Deﬁnition 4 of [42]). Suppose a learning algorithm A is run for T steps. Consider
partial sequence S0  R1  ...  ST−1  RT   ST visited by A. The instantaneous loss of the agent at time t
is ilA(t) = v∗(St) −
t=1 ilA(t) is called the average loss.
Then  a learning algorithm A is PAC-MDP in the average loss setting if for any  ≥ 0 and δ ∈ [0  1] 
we can choose a value T   polynomial in the relevant quantities (1/  1/δ |S| |A|  1/(1 − γ))  such
that the average loss LA of the agent (following the learning algorithm A) on a trial of T steps is
guaranteed to be less than  with probability at least 1 − δ.
In the following  we will restrict our attention to MWQL with Gaussian posterior  optimistic esti-
mator (OE) and optimistic exploration policy (OX). We leave the analysis of the posterior sampling
exploration (PX) as future work. To prove the main result we need an intermediate result.
Theorem 5.1. Let S0  ...  ST−1  ST be the sequence of states and actions visited by MWQL with
Gaussian posterior  OE and OX. Then  there exists a prior Q0 and a zero-mean distribution Qb and a
learning rate schedule for (αt  βt)t≥0 (whose values are reported in Appendix B.1)  such that for any
δ ∈ [0  1]  with probability at least 1 − δ it holds that:7
qmax
(1 − γ) 3

[v∗(St) − vA(St)] ≤ O

|S||A|T log |S||A|T

T(cid:88)

t=1

where vA is the value function induced by the (non-stationary) policy played by algorithm A.

(cid:114)

(cid:33)

 

(cid:32)

(6)

T

2

δ

From this result  we can exploit an analysis similar to [42] to prove that MWQL with Gaussian
posterior  OE and OX is PAC-MDP in the average loss setting.

7This performance index resembles the regret [21]. However  it is a weaker notion  being deﬁned in terms of

the trajectory generated by algorithm A  instead of the trajectories of an optimal policy.

6

Theorem 5.2. Under the hypothesis of Theorem 5.1  MWQL with Gaussian posterior  OE and OX is
PAC-MDP in the average loss setting  i.e.  for any  ≥ 0 and δ ∈ [0  1]  after

(cid:32)

(cid:33)

T = O

q2
max|S||A|
2(1 − γ)3 log

max|S|2|A|2
q2
δ2(1 − γ)3

steps we have that the average loss LA ≤  with probability at least 1 − δ.
The per-step computational complexity of MWQL is O(log |A|) as we can maintain the upper bounds
of the Q-function as a max-priority queue [40] and the space complexity is O(|S||A|).
Despite the theoretical guarantees  MWQL turns out to be often impractical for two main reasons.
First  MWQL cannot be extended to continuous MDPs  as αt and βt are deﬁned in terms of number
of visits n(s  a) (Equation (20))  which can only be computed for ﬁnite MDPs. Second  as many
provably efﬁcient RL algorithms  MWQL is extremely conservative  leading to very slow convergence.
This is why most provably efﬁcient RL algorithms  when used in practice  are run with non-theoretical
values of hyperparameters. In this sense  WQL can be seen as a “practical” version of MWQL in
which αt is treated as a normal hyper-parameter and βt = 0.

6 Related Works

A variety of approaches has been proposed in the RL literature to tackle the exploration-exploitation
trade-off [44]. We consider only those that do not assume the availability of a simulator of the
environment [26]. A ﬁrst dimension of classiﬁcation is the RL setting they consider: ﬁnite-horizon 
discounted or undiscounted. Finite-horizon MDPs are a convenient framework to devise provably-
efﬁcient exploration algorithms with theoretical guarantees on the regret [32  14  5]. Recently  in
[23] it was shown that Q-learning  in the ﬁnite-horizon setting  can be made efﬁcient by resorting to
suitable exploration bonuses. Similar results have been proposed in the inﬁnite-horizon undiscounted
case. The main challenge of this class of problems is the connection structure of the MDP [7].
Early approaches [25  4  46  21] impose restrictive requirements on either mixing/hitting times or
diameter  which have been progressively relaxed [19]. A signiﬁcant part of the early provably-efﬁcient
algorithms considers the discounted setting [25  11  41  45  27]. However  their theoretical guarantees
are based on the notion of PAC-MDP [24] rather than on regret.
Another relevant dimension is the kind of policy used for exploration. Taking inspiration from the
Multi Armed Bandit [MAB  10] framework  two main approaches have been proposed: Optimism
in the Face of Uncertainty [3] and Thompson Sampling [47]. Most exploration algorithms employ
the optimistic technique  selecting actions from the optimal policy of an optimistic approximation
of the MDP [21] or of the value function directly [41  23]. Some methods  instead  use a posterior
sampling approach in which either the entire MDP or a value function is sampled from a (possibly
approximate) posterior distribution.
Inspired by these methods  numerous practical variants have been devised. Exploration bonuses 
based on pseudo-counts [8  33]  mimicking optimism  have been applied with positive results to
deep architectures. Likewise  with the idea of approximating a posterior distribution  Bootstrapped
DQN [30] and Bayesian DQN [6] succeeded in solving challenging Atari games. Recently  new
results of sample-efﬁciency beyond tabular domains have been derived [22].

7 Experiments

In this section  we provide an experimental evaluation of WQL on tabular domains along with some
preliminary results on Atari games (implementation details are reported in Appendix C).

7.1 Tabular Domains

We evaluate WQL on a set of RL tasks designed to emphasize exploration: the Taxi problem [15]  the
Chain [15]  the River Swim [42]  and the Six Arms [42]. We extensively test several WQL variants that
differ on: i) the Q-posterior model (Gaussian G-WQL vs particle P-WQL); ii) the exploration strategy
(optimistic OX vs posterior sampling PX)  iii) the estimator of the maximum (ME  OE  and PE).

7

Figure 1: Online average return as a function of the number of samples  comparison of P-WQL and
G-WQL with QL  BQL  Delayed-QL  and MBIE-EB. 10 runs  95% c.i.

We compare these combinations with the classic Q-
learning [QL  52] (Boltzmann exploration)  Bootstrapped
Q-learning [BQL  30] both with the double estimator [48] 
Delayed Q-learning [Delayed-QL  41] and MBIE-EB [42].8
Figure 1 shows the online performance on the considered
tabular tasks. While we tried all the WQL variants  due to
space constraints  we show the best combination of explo-
ration strategy and maximum estimator for both Gaussian
and particle models (complete results are reported in Ap-
pendix C). We can see that WQL learns substantially faster
than classical approaches  like QL  in tasks that require sig-
niﬁcant exploration  such as Taxi  Six Arms  or River Swim.
Our algorithm also outperforms BQL in most tasks  except
in the River Swim  where performances are not substan-
tially different. Finally  we can see that across all the tasks
WQL displays a faster learning curve w.r.t. to Delayed-QL.
MBIE-EB outperforms WQL in small domains like Chain
and RiverSwim  but not in SixArms. MBIE-EB was not
tested on the Taxi domain as the number of states (∼ 200) makes the computational time demands
prohibitive. We cross-validate the hyperparameter of Delayed Q-Learning and MBIE-EB.
Among the variants of WQL  we discovered that the choice of the exploration strategy and the
maximum estimator are highly task dependent. However  we can see a general pattern across the
tasks. As intuition suggests  being the exploration strategy and the maximum estimator closely
related  the best combinations are: OX exploration with OE estimator and PX exploration with PE
estimator. We illustrate in Figure 2 all the possible combinations of G-WQL on Six Arms  a domain
in which exploration is essential. We can notice that the “hybrid” combinations  such as OX with PE
and PX with OE are signiﬁcantly outperformed by the more “coherent” ones.

Figure 2: Online average return as a
function of the number of samples for
the different versions of G-WQL algo-
rithm. 10 runs  95% c.i.

7.2 Atari Games

We adapted WQL with the particle model to be used paired with deep architectures. For this purpose 
similarly to Bootstrapped DQN [BDQN  30]  we use a network architecture with a head for each
particle while the convolutional layers are shared among them. We compare the resulting algorithm 
which we call Particle DQN (PDQN)  with Double DQN [DDQN  49]  a classic benchmark in
Deep-RL  and Bootstrapped DQN  speciﬁcally designed for deep exploration using Q-posteriors. To
compare algorithms we consider ofﬂine scores  i.e.  the scores collected using the current greedy
policy. The goal of this experiment  conducted on three Atari games  is to prove that WQL  although
designed to work in ﬁnite environments  can easily be extended to deep networks with potentially
good results. In Figure 3  we can see that PDQN  compared to BDQN and DDQN  manages to

8We are considering a discounted setting  thus  several provably efﬁcient algorithms  like UCRL2 [21] 
PSRL [32]  RLSVI [30]  optimistic Q-learning [23] and UCBVI [5]  cannot be compared as they consider either
average reward or ﬁnite-horizon setting.

8

01234Samples×1050.00.51.01.5AverageReturn×101Taxi0.00.20.40.60.8Samples×1052.02.53.03.5×102Chain0.00.20.40.60.8Samples×1050246×104RiverSwim0.00.20.40.60.8Samples×105012×105SixArmsG-WQLP-WQLBQLQLDelayed-QLMBIE0.00.20.40.60.8Samples×105012AverageReturn×105PX-PEPX-MEPX-OEOX-PEOX-MEOX-OEInput: a prior distribution {xi}M
1: Initialize a Q-function network with M outputs {Qj}M

i=1  a step size schedule (αt)t≥0  an exploration policy schedule (πt)t≥0

j=1 and parameters θ and the target network with

parameters θ− = θ
2: for t = 1  2  ... do
3:
4:
5:
6:

Take action At ∼ πt(·|St; θ)
Store transition (St  At  St+1  Rt+1) in the replay buffer
Sample random a batch of transitions (Sl  Al  Sl+1  Rl+1) from the replay buffer
Compute targets yj(Sl+1) = EA∼π(·|Sl+1)[Qj(Sl+1  A; θ−)] for each output Qj where π ∈
{πM   πO  πP} as in Section 3.3
j=1(yj(St+1)− Qj(Sl  Al; θ))2 and using

Perform a gradient descent step w.r.t. θ on the objective(cid:80)M

7:

8:
9: end for

the step size αt
Periodically update target network θ− = θ

Algorithm 2: Particle DQN.

Figure 3: Ofﬂine average return of the greedy policy as a function of the number of collected frames 
comparing PDQN  DDQN and BDQN on Asterix  Enduro and Breakout games. 5 runs  95% c.i.

achieve higher scores in Asterix and Enduro  where exploration is needed  while achieving similar
scores in Breakout. A relevant feature of PDQN is the particle initialization interval. Indeed  a
narrower initial interval causes faster learning but might lead to premature convergence. In this sense 
the initial interval becomes a hyperparameter of PDQN  which inﬂuences the amount of exploration
and it is likely task-dependent. The pseudocode of PDQN is shown in Algorithm 2.

8 Discussion and Conclusions

In this paper  we presented a novel RL algorithm  Wasserstein Q-Learning (WQL)  which addresses
several issues related to efﬁcient exploration in model-free RL. We discussed how to model uncertainty
of the estimated Q-function by means of approximate posterior distributions (Q-posteriors). Then 
we devised a variational method to propagate uncertainty across state-action pairs when performing
TD learning  based on Wasserstein barycenters. The experimental evaluation allowed us to appreciate
the properties of WQL. In tabular domains  whenever exploration is really necessary  our approach
is able to signiﬁcantly outperform TD methods even if designed speciﬁcally for exploration (e.g. 
Bootstrapped Q-Learning and Delayed Q-Learning). Although preliminary  the results on the Atari
games are promising and need to be further investigated as future work in order to make WQL scale
on complex environments. We believe that our algorithm contributes to bridging the gap between
theory and practice of exploration in RL. WQL is a theoretically grounded method  equipped with
guarantees in the average loss setting  but  at the same time  it is a very simple algorithm  easily
extensible to deal with continuous domains.

References
[1] Mohammed Amin Abdullah  Hang Ren  Haitham Bou Ammar  Vladimir Milenkovic  Rui Luo 
Mingtian Zhang  and Jun Wang. Wasserstein robust reinforcement learning. arXiv preprint
arXiv:1907.13196  2019.

9

0.00.51.0Frames×1080.00.5AverageReturn×104Asterix024Frames×107024×102Breakout012Frames×1070.00.51.0×103EnduroBDQNPDQNDDQN[2] Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on

Mathematical Analysis  43(2):904–924  2011.

[3] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine learning  47(2-3):235–256  2002.

[4] Peter Auer and Ronald Ortner. Logarithmic online regret bounds for undiscounted reinforcement

learning. In Advances in Neural Information Processing Systems  pages 49–56  2007.

[5] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for
In Proceedings of the 34th International Conference on Machine

reinforcement learning.
Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  pages 263–272  2017.

[6] Kamyar Azizzadenesheli  Emma Brunskill  and Animashree Anandkumar. Efﬁcient exploration

through bayesian deep q-networks. arXiv preprint arXiv:1802.04412  2018.

[7] Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

[8] Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi
Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural
Information Processing Systems  pages 1471–1479  2016.

[9] Marc G. Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on rein-
forcement learning. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th
International Conference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11
August 2017  volume 70 of Proceedings of Machine Learning Research  pages 449–458. PMLR 
2017.

[10] Donald A Berry and Bert Fristedt. Bandit problems: sequential allocation of experiments
(monographs on statistics and applied probability). London: Chapman and Hall  5:71–87  1985.

[11] Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research  3:213–231  2002.

[12] Will Dabney  Georg Ostrovski  David Silver  and Rémi Munos. Implicit quantile networks for
distributional reinforcement learning. In Jennifer G. Dy and Andreas Krause  editors  Proceed-
ings of the 35th International Conference on Machine Learning  ICML 2018  Stockholmsmässan 
Stockholm  Sweden  July 10-15  2018  volume 80 of Proceedings of Machine Learning Research 
pages 1104–1113. PMLR  2018.

[13] Will Dabney  Mark Rowland  Marc G. Bellemare  and Rémi Munos. Distributional reinforce-
ment learning with quantile regression. In Sheila A. McIlraith and Kilian Q. Weinberger  editors 
Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence  (AAAI-18)  the
30th innovative Applications of Artiﬁcial Intelligence (IAAI-18)  and the 8th AAAI Symposium
on Educational Advances in Artiﬁcial Intelligence (EAAI-18)  New Orleans  Louisiana  USA 
February 2-7  2018  pages 2892–2901. AAAI Press  2018.

[14] Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforce-
ment learning. In Advances in Neural Information Processing Systems  pages 2818–2826 
2015.

[15] Richard Dearden  Nir Friedman  and Stuart J. Russell. Bayesian q-learning. In Jack Mostow and
Chuck Rich  editors  Proceedings of the Fifteenth National Conference on Artiﬁcial Intelligence
and Tenth Innovative Applications of Artiﬁcial Intelligence Conference  AAAI 98  IAAI 98  July
26-30  1998  Madison  Wisconsin  USA.  pages 761–768. AAAI Press / The MIT Press  1998.

[16] Carlo D’Eramo  Alessandro Nuara  Matteo Pirotta  and Marcello Restelli. Estimating the
maximum expected value in continuous reinforcement learning problems. In Satinder P. Singh
and Shaul Markovitch  editors  Proceedings of the Thirty-First AAAI Conference on Artiﬁcial
Intelligence  February 4-9  2017  San Francisco  California  USA.  pages 1840–1846. AAAI
Press  2017.

10

[17] Carlo D’Eramo  Marcello Restelli  and Alessandro Nuara. Estimating maximum expected value
through gaussian approximation. In Maria-Florina Balcan and Kilian Q. Weinberger  editors 
Proceedings of the 33nd International Conference on Machine Learning  ICML 2016  New York
City  NY  USA  June 19-24  2016  volume 48 of JMLR Workshop and Conference Proceedings 
pages 1032–1040. JMLR.org  2016.

[18] DC Dowson and BV Landau. The fréchet distance between multivariate normal distributions.

Journal of multivariate analysis  12(3):450–455  1982.

[19] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. Efﬁcient bias-span-
constrained exploration-exploitation in reinforcement learning. In Jennifer G. Dy and Andreas
Krause  editors  Proceedings of the 35th International Conference on Machine Learning  ICML
2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  volume 80 of Proceedings of
Machine Learning Research  pages 1573–1581. PMLR  2018.

[20] Mohammad Ghavamzadeh  Shie Mannor  Joelle Pineau  Aviv Tamar  et al. Bayesian reinforce-
ment learning: A survey. Foundations and Trends R(cid:13) in Machine Learning  8(5-6):359–483 
2015.

[21] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[22] Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and Robert E Schapire.
Contextual decision processes with low bellman rank are pac-learnable. In Proceedings of the
34th International Conference on Machine Learning-Volume 70  pages 1704–1713. JMLR. org 
2017.

[23] Chi Jin  Zeyuan Allen-Zhu  Sebastien Bubeck  and Michael I Jordan. Is q-learning provably
efﬁcient?
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and
R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages 4868–4878.
Curran Associates  Inc.  2018.

[24] Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD

thesis  University of London London  England  2003.

[25] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine learning  49(2-3):209–232  2002.

[26] Sven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning
applied to ﬁnding shortest paths in deterministic domains. Technical report  Carnegie Mellon
University Pittsburgh PA School of Computer Science  1992.

[27] Tor Lattimore and Marcus Hutter. Near-optimal PAC bounds for discounted mdps. Theor.

Comput. Sci.  558:125–143  2014.

[28] Brendan O’Donoghue  Ian Osband  Remi Munos  and Vlad Mnih. The uncertainty Bellman
equation and exploration. In Jennifer Dy and Andreas Krause  editors  Proceedings of the
35th International Conference on Machine Learning  volume 80 of Proceedings of Machine
Learning Research  pages 3839–3848  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018.
PMLR.

[29] Ian Osband  John Aslanides  and Albin Cassirer. Randomized prior functions for deep reinforce-
ment learning. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal 
Canada.  pages 8626–8638  2018.

[30] Ian Osband  Charles Blundell  Alexander Pritzel  and Benjamin Van Roy. Deep exploration
via bootstrapped DQN. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016  December 5-10  2016  Barcelona 
Spain  pages 4026–4034  2016.

11

[31] Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via randomized
value functions. In Maria-Florina Balcan and Kilian Q. Weinberger  editors  Proceedings of
the 33nd International Conference on Machine Learning  ICML 2016  New York City  NY 
USA  June 19-24  2016  volume 48 of JMLR Workshop and Conference Proceedings  pages
2377–2386. JMLR.org  2016.

[32] Ian Osband  Daniel Russo  and Benjamin Van Roy. (more) efﬁcient reinforcement learning via
posterior sampling. In Advances in Neural Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held
December 5-8  2013  Lake Tahoe  Nevada  United States.  pages 3003–3011  2013.

[33] Georg Ostrovski  Marc G Bellemare  Aaron van den Oord  and Rémi Munos. Count-based

exploration with neural density models. arXiv preprint arXiv:1703.01310  2017.

[34] Aldo Pacchiano  Jack Parker-Holder  Yunhao Tang  Anna Choromanska  Krzysztof Choro-
manski  and Michael I. Jordan. Behavior-guided reinforcement learning. arXiv preprint
arXiv:1906.04349  2019.

[35] Jason Pazis  Ronald Parr  and Jonathan P. How. Improving PAC exploration using the median
of means. In Advances in Neural Information Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016  December 5-10  2016  Barcelona  Spain  pages
3891–3899  2016.

[36] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  2014.

[37] Mark Rowland  Marc G. Bellemare  Will Dabney  Rémi Munos  and Yee Whye Teh. An analysis
of categorical distributional reinforcement learning. In Amos J. Storkey and Fernando Pérez-
Cruz  editors  International Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2018 
9-11 April 2018  Playa Blanca  Lanzarote  Canary Islands  Spain  volume 84 of Proceedings of
Machine Learning Research  pages 29–37. PMLR  2018.

[38] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems 
volume 37. University of Cambridge  Department of Engineering Cambridge  England  1994.

[39] Satinder P. Singh  Tommi S. Jaakkola  Michael L. Littman  and Csaba Szepesvári. Conver-
gence results for single-step on-policy reinforcement-learning algorithms. Machine Learning 
38(3):287–308  2000.

[40] Alexander L. Strehl  Lihong Li  and Michael L. Littman. Reinforcement learning in ﬁnite mdps:

PAC analysis. Journal of Machine Learning Research  10:2413–2444  2009.

[41] Alexander L. Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L. Littman. PAC
model-free reinforcement learning. In Machine Learning  Proceedings of the Twenty-Third
International Conference (ICML 2006)  Pittsburgh  Pennsylvania  USA  June 25-29  2006 
pages 881–888  2006.

[42] Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation

for markov decision processes. J. Comput. Syst. Sci.  74(8):1309–1331  2008.

[43] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[44] Csaba Szepesvári. Algorithms for reinforcement learning. Synthesis lectures on artiﬁcial

intelligence and machine learning  4(1):1–103  2010.

[45] Istvan Szita and Csaba Szepesvári. Model-based reinforcement learning with nearly tight
In Proceedings of the 27th International Conference on

exploration complexity bounds.
Machine Learning (ICML-10)  June 21-24  2010  Haifa  Israel  pages 1031–1038  2010.

[46] Ambuj Tewari and Peter L Bartlett. Optimistic linear programming gives logarithmic regret for
irreducible mdps. In Advances in Neural Information Processing Systems  pages 1505–1512 
2008.

12

[47] William R Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[48] Hado Van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems 

pages 2613–2621  2010.

[49] Hado van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double
q-learning. In Dale Schuurmans and Michael P. Wellman  editors  Proceedings of the Thirtieth
AAAI Conference on Artiﬁcial Intelligence  February 12-17  2016  Phoenix  Arizona  USA. 
pages 2094–2100. AAAI Press  2016.

[50] Harm Van Seijen  Hado Van Hasselt  Shimon Whiteson  and Marco Wiering. A theoretical and
empirical analysis of expected sarsa. In Adaptive Dynamic Programming and Reinforcement
Learning  2009. ADPRL’09. IEEE Symposium on  pages 177–184. IEEE  2009.

[51] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business

Media  2008.

[52] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis  King’s

College  Cambridge  1989.

13

,Taesup Moon
Seonwoo Min
Byunghan Lee
Sungroh Yoon
Alberto Maria Metelli
Amarildo Likmeta
Marcello Restelli