2015,Smooth Interactive Submodular Set Cover,Interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions  where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few (cost-weighted) actions as possible. It models settings where there is uncertainty regarding which submodular function to optimize. In this paper  we propose a new extension  which we call smooth interactive submodular set cover  that allows the target threshold to vary depending on the plausibility of each hypothesis. We present the first algorithm for this more general setting with theoretical guarantees on optimality. We further show how to extend our approach to deal with real-valued functions  which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings.,Smooth Interactive Submodular Set Cover

Bryan He

Stanford University

bryanhe@stanford.edu

Yisong Yue

California Institute of Technology

yyue@caltech.edu

Abstract

Interactive submodular set cover is an interactive variant of submodular set cover
over a hypothesis class of submodular functions  where the goal is to satisfy
all sufﬁciently plausible submodular functions to a target threshold using as few
(cost-weighted) actions as possible. It models settings where there is uncertainty
regarding which submodular function to optimize. In this paper  we propose a new
extension  which we call smooth interactive submodular set cover  that allows the
target threshold to vary depending on the plausibility of each hypothesis. We
present the ﬁrst algorithm for this more general setting with theoretical guarantees
on optimality. We further show how to extend our approach to deal with real-
valued functions  which yields new theoretical results for real-valued submodular
set cover for both the interactive and non-interactive settings.

1

Introduction

In interactive submodular set cover (ISSC) [10  11  9]  the goal is to interactively satisfy all plausible
submodular functions in as few actions as possible. ISSC is a wide-encompassing framework that
generalizes both submodular set cover [24] by virtue of being interactive  as well as some instances
of active learning by virtue of many active learning criteria being submodular [12  9].
A key characteristic of ISSC is the a priori uncertainty regarding the correct submodular function to
optimize. For example  in personalized recommender systems  the system does not know the user’s
preferences a priori  but can learn them interactively via user feedback. Thus  any algorithm must
choose actions in order to disambiguate between competing hypotheses as well as optimize for the
most plausible ones – this issue is also known as the exploration-exploitation tradeoff.
In this paper  we propose the smooth interactive submodular set cover problem  which addresses
two important limitations of previous work. The ﬁrst limitation is that conventional ISSC [10  11  9]
only allows for a single threshold to satisfy  and this “all or nothing” nature can be inﬂexible for
settings where the covering goal should vary smoothly (e.g.  based on plausibility). In smooth ISSC 
one can smoothly vary the target threshold of the candidate submodular functions according to their
plausibility. In other words  the less plausible a hypothesis is  the less we emphasize maximizing
its associated utility function. We present a simple greedy algorithm for smooth ISSC with prov-
able guarantees on optimality. We also show that our smooth ISSC framework and algorithm fully
generalize previous instances of and algorithms for ISSC by reducing back to just one threshold.
One consequence of smooth ISSC is the need to optimize for real-valued functions  which leads
to the second limitation of previous work. Many natural classes of submodular functions are real-
valued (cf. [25  5  17  21]). However  submodular set cover (both interactive and non-interactive)
has only been rigorously studied for integral or rational functions with ﬁxed denominator  which
highlights a signiﬁcant gap between theory and practice. We propose a relaxed version of smooth
ISSC using an approximation tolerance   such that one needs only to satisfy the set cover criterion to
within . We extend our greedy algorithm to provably optimize for real-valued submodular functions
within this  tolerance. To the best of our knowledge  this yields the ﬁrst theoretically rigorous
algorithm for real-valued submodular set cover (both interactive and non-interactive).

1

Problem 1 Smooth Interactive Submodular Set Cover
1: Given:

3: Goal: Using minimal cost(cid:80)

{(ˆqi  ˆri)}i and S∗ (cid:52)

=(cid:83)
q∈Q r∈q(h∗){(q  r)}.

1. Hypothesis class H (does not necessarily contain h∗)
2. Query set Q and response set R with known q(h) ⊆ R for q ∈ Q  h ∈ H
3. Modular query cost function c deﬁned over Q
4. Monotone submodular objective functions Fh : 2Q×R → R≥0 for h ∈ H
5. Monotone submodular distance functions Gh : 2Q×R → R≥0 for h ∈ H  with Gh(S⊕(q  r))−
6. Threshold function α : R≥0 → R≥0 mapping a distance to required objective function value

Gh(S) = 0 for any S if r ∈ q(h)

2: Protocol: For i = 1  . . .  ∞: ask a question ˆqi ∈ Q and receive a response ˆri ∈ ˆqi(h∗).

i c(ˆqi)  terminate when Fh( ˆS) ≥ α(Gh(S∗)) for all h ∈ H  where ˆS =

2 Background

F (A ⊕ q) ≥ F (A)

Submodular Set Cover. In the basic submodular set cover problem [24]  we are given an action
set Q and a monotone submodular set function F : 2Q → R
≥0 that maps subsets A ⊆ Q to
non-negative scalar values. A set function F is monotone and submodular if and only if:
and F (A ⊕ q) − F (A) ≥ F (B ⊕ q) − F (B) 
∀A ⊆ B ⊆ Q  q ∈ Q :
respectively  where ⊕ denotes set addition (i.e.  A ⊕ q ≡ A ∪ {q}). In other words  monotonicity
implies that adding a set always yields non-negative gain  and submodularity implies that adding to
a smaller set A results in a larger gain than adding to a larger set B. We also assume that F (∅) = 0.
Each q ∈ Q is associated with a modular or additive cost c(q). Given a target threshold α  the goal is
to select a set A that satisﬁes F (A) ≥ α with minimal cost c(A) =(cid:80)q∈A c(q). This problem is NP-
hard; but for integer-valued F   simple greedy forward selection can provably achieve near-optimal
cost of at most (1 + ln(maxa∈Q F ({a}))OP T [24]  and is typically very effective in practice.
One motivating application is content recommendation [5  4  25  11  21]  where Q are items to
recommend  F (A) captures the utility of A ⊆ Q  and α is the satisfaction goal. Monotonicity
of F captures the property that total utility never decreases as one recommends more items  and
submodularity captures the the diminishing returns property when recommending redundant items.
Interactive Submodular Set Cover. In the basic interactive setting [10]  the decision maker must
optimize over a hypothesis class H of submodular functions Fh. The setting is interactive  whereby
the decision maker chooses an action (or query) q ∈ Q  and the environment provides a response r ∈
R. Each query q is now a function mapping hypotheses H to responses R (i.e.  q(h) ∈ R)  and the
environment provides responses according to an unknown true hypothesis h∗ ∈ H (i.e.  r ≡ q(h∗)).
This process iterates until Fh∗ (S) ≥ α  where S denotes the set of observed question/response pairs:
S = {(q  r)} ⊆ Q×R. The goal is to satisfy Fh∗ (S) ≥ α with minimal cost c(S) =(cid:80)(q r)∈S c(q).
For example  when recommending movies to a new user with unknown interests (cf. [10  11])  H
can be a set of user types or movie genres (e.g.  H = {Action  Drama  Horror  . . .}). Then Q would
contain individual movies that can be recommended  and R would be a “yes” or “no” response or
an integer rating representing how interested the user (modeled as h∗) is in a given movie.
The interactive setting is both a learning and covering problem  as opposed to just a covering prob-
lem. The decision maker must balance between disambiguating between hypotheses in H (i.e. 
identifying which is the true h∗) and satisfying the covering goal Fh∗ (S) ≥ α; this issue is also
known as the exploration-exploitation tradeoff. Noisy ISSC [11] extends basic ISSC by no longer
assuming the true h∗ is in H  and uses a distance function Gh and tolerance κ such that the goal is
to satisfy Fh(S) ≥ α for all sufﬁciently plausible h  where plausibility is deﬁned as Gh(S) ≤ κ.
3 Problem Statement

We now present the smooth interactive submodular set cover problem  which generalizes basic
and noisy ISSC [10  11] (described in Section 2). Like basic ISSC  each hypothesis h ∈ H is
associated with a utility function Fh : 2Q×R → R
≥0 that maps sets of query/response pairs to

2

α1

α2

Fh

α3

α1

α2

Fh

α3



Fh

Fh

κ1κ2

Gh
(a)

κ3

κ1κ2

κ3

Gh
(b)

Gh
(c)



Gh
(d)

Figure 1: Examples of (a) multiple thresholds  (b) approximate multiple thresholds  (c) a continuous
convex threshold  and (d) an approximate continuous convex threshold. For the approximate setting 
we essentially allow for satisfying any threshold function that resides in the yellow region.

non-negative scalars. Like noisy ISSC  the hypothesis class H does not necessarily contain the true
h∗ (i.e.  the agnostic setting). Each h ∈ H is associated with a distance or disagreement function
Gh : 2Q×R → R
≥0 which maps sets of question/response pairs to a disagreement score (i.e.  the
larger Gh(S) is  the more h disagrees with S). We further require that Fh(∅) = 0 and Gh(∅) = 0.
Problem 1 describes the general problem setting. Let S∗ (cid:52)=(cid:83)q∈Q r∈q(h∗){(q  r)} denote the set of
all possible question/responses pairs given by h∗. The goal is to construct a question/response set
ˆS with minimal cost such that  for every h ∈ H we have Fh( ˆS) ≥ α(Gh(S∗))  where α(·) maps
disagreement values to desired utilities. In general  α(·) is a non-increasing function  since the goal
is to optimize more the most plausible hypotheses in H. We describe two versions of α(·) below.
Version 1: Step Function (Multiple Thresholds). The ﬁrst version uses a decreasing step function
(see Figure 1(a)). Given a pair of sequences α1 > . . . > αN > 0 and 0 < κ1 < . . . < κN  
the threshold function is α(v) = αnκ(v) where nκ(v) = min{n ∈ {0  . . .   N + 1}|v < κn}  and
α0 (cid:52)= ∞  αN +1 (cid:52)= 0  κ0 (cid:52)= 0  κN +1 (cid:52)= ∞. The goal in Problem 1 is equivalently: “ ∀h ∈ H and
n = 1  . . .   N: satisfy Fh( ˆS) ≥ αn whenever Gh(S∗) < κn.” This version is a strict generalization
of noisy ISSC  which uses only a single α and κ.
Version 2: Convex Threshold Curve. The second version uses a convex α(·) that decreases con-
tinuously as Gh(S∗) increases (see Figure 1(c))  and is not a strict generalization of noisy ISSC.
Approximate Thresholds. Finally  we also consider a relaxed version of smooth ISSC  whereby
we only require that the objectives Fh be satisﬁed to within some tolerance  ≥ 0. More formally 
we say that we approximately solve Problem 1 with tolerance  if its goal is redeﬁned as: “using
minimal cost (cid:80)i c(ˆqi)  guarantee Fh( ˆS) ≥ α(Gh(S∗))−  for all h ∈ H.” See Figure 1(b) & 1(d)

for the approximate versions of the multiple tresholds and convex versions  respectively.
ISSC has only been rigorously studied when the utility functions are Fh are rational-valued with
a ﬁxed denominator. We show in Section 4.3 how to efﬁciently solve the approximate version of
smooth ISSC when Fh are real-valued  which also yields a new approach for approximately solving
the classical non-interactive submodular set cover problem with real-valued objective functions.

4 Algorithm & Main Results

A key question in the study of interactive optimization is how to balance the exploration-exploitation
tradeoff. On the one hand  one should exploit current knowledge to efﬁciently satisfy the plausible
submodular functions. However  hypotheses that seem plausible might actually not be due to imper-
fections in the algorithm’s knowledge. One should thus explore by playing actions that disambiguate
the plausibility of competing hypotheses. Our setting is further complicated due to also solving a
combinatorial optimization problem (submodular set cover)  which is in general intractable.

4.1 Approach Outline

We present a general greedy algorithm  described in Algorithm 1 below  for solving smooth ISSC
with provably near-optimal cost. Algorithm 1 requires as input a submodular meta-objective ¯F

3

Algorithm 1 Worst Case Greedy Algorithm for Smooth Interactive Submodular Set Cover
1: input: ¯F
2: input: ¯Fmax
3: input: Q
4: input: R
5: S ← ∅
6: while ¯F (S) < ¯Fmax do
7:
8:
9:
10: end while

ˆq ← argmaxq∈Q minr∈R(cid:0) ¯F (S ⊕ (q  r)) − ¯F (S)(cid:1) /c(q)

Play ˆq  observe ˆr
S ← S ⊕ (ˆq  ˆr)

// Submodular Meta-Objective
// Termination Threshold for ¯F
// Query or Action Set
// Response Set

Variable Deﬁnition

H
Q
R
Fh
Gh
¯F

¯Fmax
DF
DG
α(·)
αi
κi
N

F (cid:48)
α(cid:48)

h

n

Set of hypotheses
Set of actions or queries
Set of responses
Monotone non-decreasing submodular utility function
Monotone non-decreasing submodular distance function
Monotone non-decreasing submodular function unifying Fh  Gh and the thresholds
Maximum value held by ¯F
Denominator for Fh (when rational)
Denominator for Gh (when rational)
Continuous convex threshold
Thresholds for F (α1 is largest)
Thresholds for G (κ1 is smallest)
Number of thresholds
Approximation tolerance for the real-valued case
Surrogate utility function for the approximate version
Surrogate thresholds for the approximate version

Figure 2: Summary of notation used. The top portion is used in all settings. The middle portion is
used for the multiple thresholds setting. The bottom portion is used for real-valued functions.

that quantiﬁes the exploration-exploitation trade-off  and the speciﬁc instantiation of ¯F depends on
which version of smooth ISSC is being solved. Algorithm 1 greedily optimizes for the worst case
outcome at each iteration (Line 7) until a termination condition ¯F ≥ ¯Fmax has been met (Line 6).
The construction of ¯F is essentially a reduction of smooth ISSC to a simpler submodular set cover
problem  and generalizes the reduction approach in [11]. In particular  we ﬁrst lift the analysis of
[11] to deal with multiple thresholds (Section 4.2). We then show how to deal with approximate
thresholds in the real-valued setting (Section 4.3)  which ﬁnally allows us to address the continuous
threshold setting (Section 4.4). Our cost guarantees are stated relative to the general cover cost
(GCC)  which lower bounds the optimal cost  as stated in Deﬁnition 4.1 and Lemma 4.2 below. Via
this reduction  we can show that our approach achieves cost bounded by (1 + ln ¯Fmax)GCC ≤
(1 + ln ¯Fmax)OP T . For clarity of exposition  all proofs are deferred to the supplementary material.
Deﬁnition 4.1 (General Cover Cost (GCC)). Deﬁne oracles T ∈ RQ to be functions mapping
questions to responses and T ( ˆQ) ∆=(cid:83)ˆqi∈ ˆQ{(ˆqi  T (ˆqi))}. T ( ˆQ) is the set of question-response pairs

given by T for the set of questions ˆQ. Deﬁne the General Cover Cost as:

(cid:18)

(cid:19)

GCC ∆= max
T∈RQ

min

ˆQ: ¯F (T ( ˆQ))≥ ¯Fmax

c( ˆQ)

.

Lemma 4.2 (Lemma 3 from [11]). If there is a question asking strategy for satisfying ¯F ( ˆS) ≥ ¯Fmax
with worst case cost C∗  then GCC ≤ C∗. Thus GCC ≤ OP T .
4.2 Multiple Thresholds Version

We begin with the multiple thresholds version. In this section  we assume that each Fh and Gh
are rational-valued with ﬁxed denominators DF and DG  respectively.1 We ﬁrst deﬁne a doubly

1When each Fh and/or Gh are integer-valued  then DF = 1 and/or DG = 1  respectively.

4

Figure 3: Depicting the relationship between the terms deﬁned in Deﬁnition 4.3. (A) If ¯Fhi n ≥
¯Fhi nmax = (αn−αn+1)(κn−κn−1)  then either Fhi ≥ αn or Ghi ≥ κn; this generates the tradeoff
between satisfying the either of the two thresholds. (B) If ¯Fhi ≥ ¯Fhmax  then ¯Fhi n ≥ ¯Fhi nmax
∀i ∈ {1  . . .   N}; this enforces that all i  at least one of the thresholds αi or κi must be satisﬁed. (C)
If ¯F ≥ ¯Fmax  then ¯Fh ≥ ¯Fhmax ∀h ∈ H; this enforces that all hypotheses must be satisﬁed.
truncated version of each hypothesis submodular utility and distance function:

(1)

Fh αn αj ( ˆS)
Gh κn κj ( ˆS)

(cid:52)
= max(min(Fh( ˆS)  αn)  αj) − αj 
(cid:52)
= max(min(Gh( ˆS)  κn)  κj) − κj.

(2)
In other words  Fh αn αj is truncated from below at αj and from above at αn (it is assumed that
αn > αj)  and is offset by −αj so that Fh αn αj (∅) = 0. Gh κn κj is constructed analogously.
Using (1) and (2)  we can deﬁne the general forms of ¯F and ¯Fmax  which can be instantiated to
address different versions of smooth ISSC.
Deﬁnition 4.3 (General form of ¯F and ¯Fmax).
(κn − κn−1) − Gh κn κn−1 ( ˆS)
N(cid:88)
(cid:88)

Fh αn αn+1 ( ˆS) + Gh κn κn−1 ( ˆS)(αn − αn+1) 

(cid:17)
 ¯Fh n( ˆS)

(cid:89)

(κj − κj−1)

  

¯Fh( ˆS)

(cid:52)
= C ¯F

¯Fh n( ˆS)

(cid:52)
=

n=1

j(cid:54)=n

(cid:16)

¯F ( ˆS)

(cid:52)
=

¯Fh( ˆS)  ¯Fmax

(cid:52)
= |H|CF CG.

h∈H

N(cid:89)

The coefﬁcient C ¯F converts each ¯Fh to be integer-valued  CF is the contribution to ¯Fmax from Fh
and αn  and CG is the contribution to ¯Fmax from Gh and κn.
Deﬁnition 4.4 (Multiple Thresholds Version of ISSC). Given α1  . . .   αN and κ1  . . .   κN   we in-
stantiate ¯F and ¯Fmax in Deﬁnition 4.3 via:

C ¯F = DF DN
G  

CF = DF α1 

CG = DN
G

(κn − κn−1).

n=1

¯F in Deﬁnition 4.4 trades off between exploitation (maximizing the plausible Fh’s) and exploration
(disambiguating plausibility in Fh’s) by allowing each ¯Fh to reach its maximum by either Fh reach-
ing αi or Gh reaching κi. In other words  each ¯Fh can be satisﬁed with either a sufﬁciently large
utility Fh or large distance Gh. Figure 3 shows the logical relationships between these components.
We prove in Appendix A that ¯F is monotone submodular  and that ﬁnding an S such that ¯F (S) ≥
¯Fmax is equivalent to solving Problem 1. For ¯F to be submodular  we also require Condition 4.5 
which is essentially a discrete analogue to the condition that a continuous α(·) should be convex.
Condition 4.5. The sequence (cid:104) αn−αn+1
Theorem 4.6. Given Condition 4.5  Algorithm 1 using Deﬁnition 4.4 solves the multiple thresholds
n=1(κn − κn−1)(cid:17)(cid:17) GCC.
version of Problem 1 using cost at most(cid:16)1 + ln(cid:16)|H|DF DN
If each Gh is integral and κn = κn−1 + 1  then the bound simpliﬁes to (1 + ln (|H|DF α1)) GCC.
We present an alternative formulation in Appendix D.2 that has better bounds when DG is large  but
is less ﬂexible and cannot be easily extended to the real-valued and convex threshold curve settings.

G α1(cid:81)N

n=1 is non-increasing.

κn−κn−1 (cid:105)N

5

216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269¯F¯FmaxC^¯Fh1¯Fhmax···¯Fhi¯Fhmax···¯Fh|H|¯FhmaxB······^¯Fhi 1¯Fh 1max···¯Fhi N¯Fh NmaxA_···_Fhi↵1Ghi1···Fhi↵NGhiNFigure2:ThisﬁgureshowstherelationshipbetweenthetermsdeﬁnedinDeﬁnition4.3.(A)For¯Fhi n¯Fhi nmax=(↵n↵n+1)(nn1) eitherFhi↵norGhin.Thisgeneratesthetradeoffbetweensatisfyingtheeitherofthetwothresholds.(B)For¯Fhi¯Fhmax ¯Fhi n¯Fhi nmaxforalli2{1 ... N}.Thiscreatestherequirementthatallofthethresholdsmustbesatisﬁed.(C)For¯F¯Fmax ¯Fh¯Fhmaxforallh2H.Thiscreatestherequirementthatallofthehypothesesmustbesatisﬁed.Using(1)and(2) wedeﬁnethegeneralformsof¯Fand¯FmaxusedinSections4.2 4.3 and4.4.EachofthesesectionswillapplythisdeﬁnitiontodifferentchoicesofFh Gh N ↵1 ... ↵N and1 ... Ntosolvetheirvariantsoftheproblem.Inthisdeﬁnition Xisaconstanttomake¯Fhtobeinteger-valued YisthecontributiontothemaximumvaluefromFhand↵n andZisthecontributiontothemaximumvaluefromGhandn.Deﬁnition4.3(General¯Fand¯Fmax).¯Fh n(ˆS)4=⇣(nn1)Gh n n1(ˆS)⌘Fh ↵n ↵n+1(ˆS)+Gh n n1(ˆS)(↵n↵n+1) ¯Fh(ˆS)4=XNXn=1240@Yj6=n(jj1)1A¯Fh n(ˆS)35 ¯F(ˆS)4=Xh2H¯Fh(ˆS) ¯Fmax4=|H|YZDeﬁnition4.4(MultipleThresholds).Tosolvethemultiplethresholdsversionoftheproblem Fh Gh N ↵1 ... ↵N and1 ... Nareusedwithoutmodiﬁcation.Theconstantsaresetasthefollowing:X=DFDNG Y=DF↵1 Z=DNGNYn=1(nn1)Thisdeﬁnitionof¯Ftradesoffbetweenexploitation(maximizingthemostplausibleFh)andex-ploration(distinguishingbetweenmoreandlessplausibleFh)byallowingeach¯FitoreachitsmaximumvalueeitherbyhavingFhreach↵iorhavingGhreachi.Inotherwords eachofthethresholdscanbesatisﬁedwitheitherasufﬁcientlylargeutilityFhorasufﬁcientlylargedistanceGh.Figure2showsthelogicalrelationshipsbetweenthesecomponents.WeproveinAppendixAthat¯Fismonotonesubmodular andthatﬁndingaSsuchthat¯F(S)¯FmaxisequivalenttosolvingProblem1.ForDeﬁnition4.4 wealsorequirethat↵nandnthresh-oldssatisfyCondition4.5for¯Ftobesubmodular.Condition4.5.Thesequenceh↵i↵n+1nn1iNi=1isnon-increasing.Theorem4.6.LetFhandGhbemonotonesubmodularandrational-valuedwithﬁxeddenominatorDFandDG respectively.Then ifCondition4.5holds thenapplyingAlgorithm1using¯Fand¯FmaxfromDeﬁnition4.4solvesthemultiplethresholdsversionofProblem1withcostatmost⇣1+ln⇣|H|DFDNG↵1QNn=1(nn1)⌘⌘GCC.54.3 Approximate Thresholds for Real-Valued Functions

Solving even non-interactive submodular set cover is extremely challenging when the utility func-
tions Fh are real-valued. For example  Appendix B.1 describes a setting where the greedy algorithm
performs arbitrarily poorly. We now extend the results from Section 4.2 to real-valued Fh and
α1  . . .   αN .
Rather than trying to solve the problem exactly  we instead solve a relaxed or approximate version 
which will be useful for the convex threshold curve setting. Let  > 0 denote a pre-speciﬁed
approximation tolerance for Fh  (cid:100)·(cid:101)γ denote rounding up to the nearest multiple of γ  and (cid:98)·(cid:99)γ
denote rounding down to the nearest multiple of γ. We deﬁne a surrogate problem:
Deﬁnition 4.7 (Approximate Thresholds for Real-Valued Functions). Deﬁne the following approx-
imations to Fh and αn:


D

| ˆS|(cid:88)
(cid:34)
n(cid:88)

i=1

i=1

D


Fh( ˆS) +
(cid:36)
 |Q|(cid:88)

αn − 
D

D


(|Q| + 1 − i) +

(cid:48)
h( ˆS)

F

(cid:52)
=

(cid:48)
n

α

(cid:52)
=

(cid:52)
=

D

(|Q| + 1 − i)

(cid:35)(cid:37)


D

(2N − 2i)DN−i+1

G

(κj − κj−1)

(cid:34)

N(cid:88)

j=i

(2N − 2i)DN−i+1

G

N(cid:89)

(κj − κj−1)

(cid:35)



+ 2

i=1

i=1

j=i

Instantiate ¯F and ¯Fmax in Deﬁnition 4.3 using F (cid:48)h  α(cid:48)n above  Gh  κn and:

C ¯F = DN

G   CF = α

(cid:48)
1  CG = DN
G

(κn − κn−1).

 

D

 

N(cid:89)

N(cid:89)

n=1

We prove in Appendix B that Deﬁnition 4.7 is an instance of a smooth ISSC problem  and that
solving Deﬁnition 4.7 will approximately solve the original real-valued smooth ISSC problem.
Theorem 4.8. Given Condition 4.5  Algorithm 1 using Deﬁnition 4.7 will approximately solve
the real-valued multiple thresholds version of Problem 1 with tolerance  using cost at most

(cid:16)1 + ln(cid:16)|H|α(cid:48)1DN

G(cid:81)N

n=1(κn − κn−1)(cid:17)(cid:17) GCC.

We show in Appendix B.2 how to apply this result to approximately solve the basic submodular set
cover problem with real-valued objectives. Note that if  is selected as the smallest distinct difference
between values in Fh  then the approximation will be exact.

4.4 Convex Threshold Curve Version
We now address the setting where the threshold curve α(·) is continuous and convex. We again
solve the approximate version  since the threshold curve α(·) is necessarily real-valued. Let  > 0
be the pre-speciﬁed tolerance for F (cid:48)h. Let N be deﬁned so that N DG is the maximal value of Gh.
We convert the continuous version α(·) to a multiple threshold version (with N thresholds) that is
within an -approximation of the former  as shown below.
Deﬁnition 4.9 (Equivalent Multiple Thresholds for Continuous Convex Curve). Instantiate ¯F and
¯Fmax in Deﬁnition 4.3 using Gh without modiﬁcation  and a sequence of thresholds:

Fh( ˆS) +
(cid:36)

α(n) − 
D


D

| ˆS|(cid:88)
(cid:34)
n(cid:88)

i=1

(cid:48)
h( ˆS)

F

(cid:52)
=

(cid:48)
n

α

(cid:52)
=

D


D


(|Q| + 1 − i)

 

 

D

(2N − 2i)DN−i+1

G

i=1

j=i

N(cid:89)

(κj − κj−1)

(cid:35)(cid:37)


D

(cid:52)
= DGn

κn

6

with constants set as:

C ¯F = 1  CF = α

(cid:48)
1  CG = DN
G

N(cid:89)

(κn − κn−1) = DN
G .

n=1

Note that the F (cid:48)h are not too expensive to compute. We prove in Appendix C that satisfying this set of
thresholds is equivalent to satisfying the original curve α(·) within -error. Note also that Deﬁnition
4.9 uses the same form as Deﬁnition 4.7 to handle the approximation of real-valued functions.
Theorem 4.10. Applying Algorithm 1 using Deﬁnition 4.9 approximately solves the convex thresh-
old version of Problem 1 with tolerance  using cost at most:(cid:0)1 + ln(cid:0)|H|α(cid:48)1DN

Note that if  is sufﬁciently large  then N could in principle be smaller  which can lead to less
conservative approximations. There may also be more precise approximations by reducing to other
formulations for the multi-threshold setting (e.g.  Appendix D.2).

G(cid:1)(cid:1) GCC.

5 Simulation Experiments

Comparison of Methods to Solve Multiple Thresholds. We compared our multiple threshold
method against multiple baselines (see Appendix D for more details) in a range of simulation settings
(see Appendix E.1). Figure 4 shows the results. We see that our approach is consistently amongst the
best performing methods. The primary competitor is the circuit of constraints approach from [11]
(see Appendix D.3 for a comparison of the theoretical guarantees). We also note that all approaches
dramatically outperform their worst-case guarantees.

Figure 4: Comparison against baselines in three simulation settings.

Validating Approximation Tolerances. We also validated the efﬁcacy of our approximate thresh-
olds relaxation (see Appendix E.2 for more details of the setup). Figure 5 shows the results. We see
that the actual deviation from the original smooth ISSC problem is much smaller than the speciﬁed
  which suggests that our guarantees are rather conservative. For instance  at  = 15  the algorithm
is allowed to terminate immediately. We also see that the cost to completion steadily decreases as 
increases  which agrees with our theoretical results.

Figure 5: Comparing cost and deviation from the exact function for varying .

6 Summary of Results & Discussion

Figure 6 summarizes the size of ¯Fmax (or ¯F (cid:48)max for real-valued functions) for the various settings.
Recall that our cost guarantees take the form (1 + ln ¯Fmax)OP T . When Fh are real-valued  then
we instead solve the smooth ISSC problem approximately with cost guarantee (1 + ln ¯F (cid:48)max)OP T .
Our results are well developed for many different versions of the utility functions Fh  but are less
ﬂexible for the distance functions Gh. For example  even for rational-valued Gh  ¯Fmax scales as
G   which is not desirable. The restriction of Gh to be rational (or integral) leads to a relatively
DN
straightforward reduction of the continuous convex version of α(·) to a multiple thresholds version.

7

Percentile050100Cost253035404550Cost for Setting APercentile050100Cost20253035Cost for Setting BPercentile050100Cost20253035Cost for Setting CMultiple Threshold (Def 4.4)Alternative (Def D.1)Circuit (Def D.6)Forward (Sec D.1)Backward (Sec D.1)0510152025Cost2628303234Cost vs 00510152025Deviation00.511.52Deviation vs 0In fact  our formulation can be extended to deal with real-valued Gh and κn in the multiple thresh-
olds version; however the resulting ¯F is no longer guaranteed to be submodular. It is possible that a
different assumption than the one imposed in Condition 4.5 is required to prove more general results.

G

F
Rational Rational
Real
Rational

Multiple Thresholds
|H|α1DF DN
|H|α(cid:48)

(cid:81)N
(cid:81)N
i=1(κi − κi−1)
i=1 (κi − κi−1)

1DN
G

G

Convex Threshold Curve
|H|α1DF DN
|H|α(cid:48)

1DN
G

G

Figure 6: Summarizing ¯Fmax. When Fh are real-valued  we show ¯F (cid:48)max instead.

Our analysis appears to be overly conservative for many settings. For instance  all the approaches we
evaluated empirically achieved much better performance than their worst-case guarantees. It would
be interesting to identify ways to constrain the problem and develop tighter theoretical guarantees.

7 Other Related Work
Submodular optimization is an important problem that arises across many settings  including sensor
placements [16  15]  summarization [26  17  23]  inferring latent inﬂuence networks [8]  diversiﬁed
recommender systems [5  4  25  21]  and multiple solution prediction [1  3  22  19]. However  the
majority of previous work has focused on ofﬂine submodular optimization whereby the submodular
function to be optimized is ﬁxed a priori (i.e.  does not vary depending on feedback).
There are two typical ways that a submodular optimization problem can be made interactive. The
ﬁrst is in online submodular optimization  where an unknown submodular function must be re-
optimized repeatedly over many sessions in an online or repeated-games fashion [20  25  21]. In
this setting  feedback is typically provided only at the conclusion of a session  and so adapting from
feedback is performed between sessions. In other words  each session consists of a non-interactive
submodular optimization problem  and the technical challenge stems from the fact that the submod-
ular function is unknown a priori and must be learned from feedback provided post optimization in
each session – this setting is often referred to as inter-session interactive optimization.
The other way to make submodular optimization interactive  which we consider in this paper  is to
make feedback available immediately after each action taken. In this way  one can simultaneously
learn about and optimize for the unknown submodular function within a single optimization session
– this setting is often referred to as intra-session interactive optimization. One can also consider
settings that allow for both intra-session and inter-session interactive optimization.
Perhaps the most well-studied application of intra-session interactive submodular optimization is
active learning [10  7  11  9  2  14  13]  where the goal is to quickly reduce the hypothesis class
to some target residual uncertainty for planning or decision making. Many instances of noisy and
approximate active learning can be formulated as an interactive submodular set cover problem [9].
A related setting is adaptive submodularity [7  2  6  13]  which is a probabilistic setting that essen-
tially requires that the conditional expectation over the hypothesis set of submodular functions is
itself a submodular function. In contrast  we require that the hypothesis class be pointwise submod-
ular (i.e.  each hypothesis corresponds to a different submodular utility function). Although neither
adaptive submodularity nor pointwise submodularity is a strict generalization of the other (cf. [7  9]) 
in practice it can often be easier to model application settings using pointwise submodularity.
The “ﬂipped” problem is to maximize utility with a bounded budget  which is commonly known as
the budgeted submodular maximization problem [18]. Interactive budgeted maximization has been
analyzed rigorously for adaptive submodular problems [7]  but it remains a challenge to develop
provably near-optimal interactive algorithms for pointwise submodular utility functions.
8 Conclusions
We introduced smooth interactive submodular set cover  a smoothed generalization of previous ISSC
frameworks. Smooth ISSC allows for the target threshold to vary based on the plausibility of the
hypothesis. Smooth ISSC also introduces an approximate threshold solution concept that can be
applied to real-valued functions  which also applies to basic submodular set cover with real-valued
objectives. We developed the ﬁrst provably near-optimal algorithm for this setting.

8

References
[1] Dhruv Batra  Payman Yadollahpour  Abner Guzman-Rivera  and Gregory Shakhnarovich. Diverse m-best

solutions in markov random ﬁelds. In European Conference on Computer Vision (ECCV)  2012.

[2] Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular

optimization. In International Conference on Machine Learning (ICML)  2013.

[3] Debadeepta Dey  Tommy Liu  Martial Hebert  and J. Andrew Bagnell. Contextual sequence prediction

via submodular function optimization. In Robotics: Science and Systems Conference (RSS)  2012.

[4] Khalid El-Arini and Carlos Guestrin. Beyond keyword search: discovering relevant scientiﬁc literature.

In ACM Conference on Knowledge Discovery and Data Mining (KDD)  2011.

[5] Khalid El-Arini  Gaurav Veda  Dafna Shahaf  and Carlos Guestrin. Turning down the noise in the blogo-

sphere. In ACM Conference on Knowledge Discovery and Data Mining (KDD)  2009.

[6] Victor Gabillon  Branislav Kveton  Zheng Wen  Brian Eriksson  and S. Muthukrishnan. Adaptive sub-

modular maximization in bandit setting. In Neural Information Processing Systems (NIPS)  2013.

[7] Daniel Golovin and Andreas Krause. Adaptive submodularity: A new approach to active learning and

stochastic optimization. In Conference on Learning Theory (COLT)  2010.

[8] Manuel Gomez Rodriguez  Jure Leskovec  and Andreas Krause.

Inferring networks of diffusion and

inﬂuence. In ACM Conference on Knowledge Discovery and Data Mining (KDD)  2010.

[9] Andrew Guillory. Active Learning and Submodular Functions. PhD thesis  University of Washington 

2012.

[10] Andrew Guillory and Jeff Bilmes.

Machine Learning (ICML)  2010.

Interactive submodular set cover.

In International Conference on

[11] Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise. In Inter-

national Conference on Machine Learning (ICML)  2011.

[12] Steve Hanneke. The complexity of interactive machine learning. Master’s thesis  Carnegie Mellon Uni-

versity  2007.

[13] Shervin Javdani  Yuxin Chen  Amin Karbasi  Andreas Krause  J. Andrew Bagnell  and Siddhartha Srini-
vasa. Near optimal bayesian active learning for decision making. In Conference on Artiﬁcial Intelligence
and Statistics (AISTATS)  2014.

[14] Shervin Javdani  Matthew Klingensmith  J. Andrew Bagnell  Nancy Pollard  and Siddhartha Srinivasa.
Efﬁcient touch based localization through submodularity. In IEEE International Conference on Robotics
and Automation (ICRA)  2013.

[15] Andreas Krause  Ajit Singh  and Carlos Guestrin. Near-optimal sensor placements in gaussian processes.

In International Conference on Machine Learning (ICML)  2005.

[16] Jure Leskovec  Andreas Krause  Carlos Guestrin  Christos Faloutsos  Jeanne VanBriesen  and Natalie
Glance. Cost-effective outbreak detection in networks. In ACM Conference on Knowledge Discovery and
Data Mining (KDD)  2007.

[17] Hui Lin and Jeff Bilmes. Learning mixtures of submodular shells with application to document summa-

rization. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2012.

[18] George Nemhauser  Laurence Wolsey  and Marshall Fisher. An analysis of approximations for maximiz-

ing submodular set functions. Mathematical Programming  14(1):265–294  1978.

[19] Adarsh Prasad  Stefanie Jegelka  and Dhruv Batra. Submodular meets structured: Finding diverse subsets

in exponentially-large structured item sets. In Neural Information Processing Systems (NIPS)  2014.

[20] Filip Radlinski  Robert Kleinberg  and Thorsten Joachims. Learning diverse rankings with multi-armed

bandits. In International Conference on Machine Learning (ICML)  2008.

[21] Karthik Raman  Pannaga Shivaswamy  and Thorsten Joachims. Online learning to diversify from implicit

feedback. In ACM Conference on Knowledge Discovery and Data Mining (KDD)  2012.

[22] Stephane Ross  Jiaji Zhou  Yisong Yue  Debadeepta Dey  and J. Andrew Bagnell. Learning policies for

contextual submodular prediction. In International Conference on Machine Learning (ICML)  2013.

[23] Sebastian Tschiatschek  Rishabh Iyer  Haochen Wei  and Jeff Bilmes. Learning mixtures of submodular
functions for image collection summarization. In Neural Information Processing Systems (NIPS)  2014.
[24] Laurence A Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.

Combinatorica  2(4):385–393  1982.

[25] Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversiﬁed retrieval.

In Neural Information Processing Systems (NIPS)  2011.

[26] Yisong Yue and Thorsten Joachims. Predicting diverse subsets using structural svms. In International

Conference on Machine Learning (ICML)  2008.

9

,Bryan He
Yisong Yue
Saurabh Verma
Zhi-Li Zhang