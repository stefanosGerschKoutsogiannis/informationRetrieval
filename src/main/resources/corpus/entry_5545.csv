2018,Theoretical guarantees for EM under misspecified Gaussian mixture models,Recent years have witnessed substantial progress in understanding
  the behavior of EM for mixture models that are correctly specified.
  Given that model misspecification is common in practice  it is
  important to understand EM in this more general setting.  We provide
  non-asymptotic guarantees for population and sample-based EM for
  parameter estimation under a few specific univariate settings of
  misspecified Gaussian mixture models.  Due to misspecification  the
  EM iterates no longer converge to the true model and instead
  converge to the projection of the true model over the set of models
  being searched over.  We provide two classes of theoretical
  guarantees: first  we characterize the bias introduced due to the
  misspecification; and second  we prove that population EM converges
  at a geometric rate to the model projection under a suitable
  initialization condition.  This geometric convergence rate for
  population EM imply a statistical complexity of order $1/\sqrt{n}$
  when running EM with $n$ samples. We validate our theoretical
  findings in different cases via several numerical examples.,Theoretical guarantees for the EM algorithm when
applied to mis-speciﬁed Gaussian mixture models

Raaz Dwivedi(cid:63) Nhat Ho(cid:63) Koulik Khamaru(cid:63)

UC Berkeley

{raaz.rsk  minhnhat  koulik}@berkeley.edu

Martin J. Wainwright

UC Berkeley
Voleon Group

wainwrig@berkeley.edu

Michael I. Jordan

UC Berkeley

jordan@berkeley.edu

Abstract

Recent years have witnessed substantial progress in understanding the behavior
of EM for mixture models that are correctly speciﬁed. Given that model mis-
speciﬁcation is common in practice  it is important to understand EM in this more
general setting. We provide non-asymptotic guarantees for the population and
sample-based EM algorithms when used to estimate parameters of certain mis-
speciﬁed Gaussian mixture models. Due to mis-speciﬁcation  the EM iterates no
longer converge to the true model and instead converge to the projection of the
true model onto the ﬁtted model class. We provide two classes of theoretical guar-
antees: (a) a characterization of the bias introduced due to the mis-speciﬁcation;
and (b) guarantees of geometric convergence of the population EM to the model
projection given a suitable initialization. This geometric convergence rate for pop-
ulation EM implies that the EM algorithm based on n samples converges to an es-
timate with 1/√n accuracy. We validate our theoretical ﬁndings in different cases
via several numerical examples.

1

Introduction

Mixture models play a central role in statistical applications  where they are used to capture hetero-
geneity of data arising from several underlying subpopulations. However  estimating the parameters
of mixture models is a challenging task  due to the non-convexity of the log likelihood function.
As shown by classical work  the maximum likelihood estimate (MLE) often has good properties for
mixture models  but its computation can be non-trivial. One of the most popular algorithms used to
compute the MLE (approximately) is the expectation maximization (EM) algorithm. Although EM
is widely used in practice  it does not always converge to the MLE  and its convergence rate can vary
as a function of the problem. Classical results provide guarantees about the convergence rates of EM
to local maxima [4  16]. In the speciﬁc setting of Gaussian mixtures  population EM (idealized EM
with inﬁnite samples) was shown to have a range of behavior from super-linear convergence to slow
convergence like a ﬁrst-order method depending on the overlap between the mixtures [9  18]. More
recently  there has been a renewed interest in providing explicit and non-asymptotic guarantees on
the convergence of EM. Notably  Balakrishnan et al. [1] developed a rather general framework for
characterizing the convergence of EM. For well-speciﬁed problems—including the two-component
Gaussian location mixture as a particular example—they provided sufﬁcient conditions for the EM

(cid:63)Raaz Dwivedi  Nhat Ho  and Koulik Khamaru contributed equally to this work.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

algorithm to converge to a small neighborhood of global maximum; in addition  they provided ex-
plicit bounds on the sample complexity of EM  meaning the number of samples n required  as a
function of the tolerance   problem dimension and other parameters  to achieve an -accurate so-
lution. A line of follow-up work has generalized and extended results of this type (e.g.  see the
papers [20  15  7  17  3  19  5  2]).
A shared assumption common to this body of past work is that either the true distribution of each
subpopulations is known  or that the number of components is exactly known; in practice  both of
these conditions are often violated. In such settings  it is well known that the MLE  instead of ap-
proximating the true parameter  approximates a Kullback-Leibler projection of the data-generating
distribution onto the ﬁtted model class. Thus  the MLE exhibits a desirable form of robustness to
model mis-speciﬁcation.
On the other hand  it is not obvious a priori that this robustness need be shared by the solutions
returned by the EM algorithm. Since these solutions are those actually used in practice  it is impor-
tant understand under what conditions the EM algorithm  when applied with mis-speciﬁed models 
converges to an (approximate) KL projection. The main contribution of this paper is to provide
some precise answers to this question  and moreover to quantify the bias that arises from model
mis-speciﬁcation. Our analysis focuses on two classes of mis-speciﬁed mixture models.
• Under-speciﬁed number of components: Suppose that the true model is given by location-shifted
mixture of k ≥ 3 univariate Gaussians  but we use EM to ﬁt a location-shifted Gaussian mix-
ture with k − 1 components. This scenario is very common: it arises naturally when either the
mixture components are very close or some of the mixture weights are very small  so that the
data generating distribution appears to have fewer components. Analysis of the EM algorithm
when the ﬁtting distribution has fewer mixture components than the data-generating distribution
poses new challenges; in particular  it requires an understanding of the model bias  meaning the
Kullback-Leibler discrepancy between the true model from its projection(s) onto the class of
ﬁtted models. In this paper  we provide a detailed analysis of the k = 3 case. First  we charac-
terize the model bias induced by ﬁtting a two-component mixture to a three-component mixture
with unknown means but known variance. We then provide sufﬁcient conditions for the popula-
tion EM updates to converge at a geometric rate to the KL projection of the true model onto the
ﬁtted model class. Finally  using Rademacher-complexity based arguments and the geometric
convergence of population EM  we conclude that with high probability  the EM updates with n
samples converge to a ball of radius 1/√n around the aforementioned KL projection.

• Incorrectly speciﬁed weights or variances: In our second problem class  we assume that the
number of components is correctly speciﬁed  but either the mixture weights or the variances
are mis-speciﬁed. Concretely  suppose that the true model is a two-component location-shifted
Gaussian mixture with weights/variances that differ from those in the ﬁtted model class. Our
analysis reveals a rather surprising phenomenon with respect to EM convergence: despite the
potential non-convexity of the problem  the iterates converge at a geometric rate to a unique
ﬁxed point from an arbitrary initialization. Our results suggest that the projection from the true
model to the ﬁtted model is actually unique. Finally  we prove that the sample-based EM updates
achieve standard minimax convergence rate of order 1/√n.

Table 1 provides a high-level summary of our results  where we use (θ  σ  α) to denote the Gaussian
mixture component with mean θ  variance σ2 and weight α  i.e.  αN (θ  σ2).
The remainder of our paper is organized as follows. In Section 2  we introduce the problem set-up
and provide the background information on the EM algorithm. In Section 3  we present our results
for the ﬁrst framework and provide expressions for the bias and rate of convergence of EM for dif-
ferent 3 component mixture of Gaussians. Section 4 contains results when the mixture weights and
variance are mis-speciﬁed. Numerical experiments illustrating our theoretical results are presented
in Section 5. Finally in Section 6  we conclude the paper with a discussion of our results and a few
possible venues for future work.
Notation: We use c  c(cid:48)  c1  c2 to denote universal constants whose value may vary in different con-
texts. For two distributions P and Q  the Kullback-Leibler divergence between them is denoted by
KL(P  Q). We use the standard big-O notation to depict the scaling with respect to a particular
quantity and hide constants and other problem parameters.

2

True Model

3-component mixture:
(−θ∗(1+ρ)  σ  1/4);
(−θ∗(1−ρ)  σ  1/4);
(θ∗  σ  1/2)
3-component mixture:
(−θ∗  σ  (1−ω)/2);
(θ∗  σ  (1−ω)/2);
(0  σ2  ω)
2-component mixture:
(−θ∗ (cid:112)σ2−θ∗2  1/2);
(θ∗ (cid:112)σ2−θ∗2  1/2);

Best ﬁt with two
components
(−θ  σ  1/2);
(θ  σ  1/2)
σ known

(−θ  σ  1/2);
(θ  σ  1/2);
σ known

(−θ  σ  π);
(θ  σ  1−π)
σ  π(cid:54)= 1/2 known

Bias
min{|θ − θ∗
ρ|θ∗

| + c (ρ|θ∗

|  |θ + θ∗
|}
|/σ)1/4

Statistical error |(cid:98)θn − θ|

of sample EM
n−1/2

cω1/8 |θ∗|1/4
√
1−ωσ1/4

c|θ∗| ((2−4π) + θ∗2)1/2

σ

n−1/2

n−1/2

Table 1. Summary of the main theoretical gurantees of this paper. Here the parameter θ∗ denotes the
true parameter value (in the data-generating distribution)  ¯θ denotes the value of the parameter of the
best ﬁt model  and ˆθn denotes the estimate returned by running the EM algorithm. Recall that the true
model is not in the class of ﬁtted models  and we can only hope to estimate ¯θ; consequently  in the
above table lists the performance of the EM algorithm in estimating ¯θ for different settings. The ﬁrst
column lists the true model  while the second column shows the ﬁtted model. In the third column  we
summarize the bias of the parameter of the best ﬁtted model (2). When using EM with n samples  the

ﬁnal statistical error |(cid:98)θn − θ| has the statistical rate of order n−1/2 in all cases  as depicted in the fourth
column (here(cid:98)θn denotes the ﬁnal sample EM estimate).

2 Problem set-up
Throughout this paper  we assume that data is generated according to some true distribution P∗ 
which admits a continuous density over R. We are interested in the performance of the EM algorithm
when we ﬁt the model below using a two-component mixture of location-shifted Gaussians with
known variance σ2 and known mixture weight π ∈ (0  1):

Pθ = πN (θ  σ2) + (1 − π)N (−θ  σ2)
We consider two distinct settings of the mixture weights in model (1):
• Balanced mixtures: the mixture weights are assumed to be equal  i.e.  π = 1 − π = 1/2.
• Unbalanced mixtures:
In order to estimate the location parameters  we apply the EM algorithm  allowing θ to vary over
some compact set Θ. Since the true distribution P∗ may not belong to the class of ﬁtted models  the
best possible estimator is the projections of P∗ to the ﬁtted model (1). It is given by

the mixture weights are assumed to be unequal π = 1

2 (1 + ) where || ∈ (0  1).

2 (1 − ) and

1 − π = 1

(1)

θ ∈ arg min
θ∈Θ

KL (P∗  Pθ) .

(2)

Our main goal in the paper is to establish the convergence rate of EM updates to θ for various choices
of the data-generating model P∗ and the ﬁtted model (1).

2.1 EM algorithm for two-component location-Gaussian mixtures

Let us now introduce some notation as well as a brief description of the EM algorithm for two-
component Gaussian location mixtures (1). The population version of EM is based on the function

(3)
where the expectation is taken over the true distribution P∗. For any ﬁxed θ  the M-step in the EM
updates for the model (1) is obtained by maximizing the minorization function (3); for a detailed

E(cid:104)wθ(X) (X − θ(cid:48))2 + (1 − wθ(X)) (X + θ(cid:48))2(cid:105)  

Q(θ(cid:48); θ) := −

1
2

3

derivation  see the paper [1]. More precisely  we denote the population EM operator M : R → R as
(4a)

M (θ) := arg max

θ(cid:48) Q(θ(cid:48)  θ) = E [(2wθ(X) − 1)X]  

where the weighting function wθ in the above formulation is given by

wθ(x) :=

π exp(cid:16)− (θ−x)2
2σ2 (cid:17)
2σ2 (cid:17) + (1 − π) exp(cid:16)− (θ+x)2
2σ2 (cid:17) .

π exp(cid:16)− (θ−x)2

Note that the parameter θ  deﬁned in equation (2)  minimizes the KL-distance between the ﬁtted
model and the true model  thereby ensuring that the log-likelihood is maximized at the model in-
dexed by the parameter θ. Consequently  the parameter θ is a ﬁxed point of the population EM
update—that is  M (θ) = θ. The sample version of the EM algorithm—the method actually used
in practice—is obtained by simply replacing the expectations in equations (3) and equation (4a) by
the sample-based counterpart. In particular  given a set of n i.i.d. samples {Xi}n
i=1 from the true
model  the sample EM operator Mn : R (cid:55)→ R takes the form

(4b)

(5)

Mn(θ) :=

1
n

n(cid:88)i=1

(2wθ(Xi) − 1)Xi.

With this notation in place  we are now ready to state our main results.

3 Guarantees for EM algorithm for mis-speciﬁed number of components

In this section  we study the convergence of the EM algorithm in the setting of under-ﬁtted mixtures 
where the number of components in the true model is larger than that in the ﬁtted model. In sharp
contrast to the traditional setting of correctly speciﬁed mixture models  where the number of com-
ponents of the true model is known to the EM algorithm  we analyze the performance of the EM
algorithm in the setting where the true number of the components is not known. Such a scenario
naturally occur in many practical cases  examples include: (1) Some components in the mixture are
very close  and it is hard to distinguish them; (2) Some components have very small mixture weights
and thereby are difﬁcult to detect. Consequently  in the aforementioned situations  the number of
components observed from the data may be much smaller compared to the number of components
present in the true model. In this section  we characterize the bias of the two-component ﬁt and
analyze the convergence properties of EM for such a ﬁt.

3.1 Three-component mixtures with two close components
First  we consider the case  where the true model has distribution P∗ is a mixture of three-component
Gaussian location mixture given by

1

P∗ =

4N (−θ∗(1 + ρ)  σ2) +

(6)
for some θ∗ in a compact subset Θ of the real line  and a small positive scalar ρ that characterizes the
separation between two cluster means −θ∗(1 + ρ) and −θ∗(1− ρ). For ﬁtting the model  we assume
that the variance σ2 is known  and we suspect that the true model is a two-component mixture (since
ρ is small). Consequently  we ﬁt the data with the model

4N (−θ∗(1 − ρ)  σ2) +

1

2N (θ∗  σ2)

1

Pθ =

1
2N (−θ  σ2) +

1
2N (θ  σ2) 

(7)

and we use the EM algorithm to estimate the location parameter θ. Clearly  the performance of
model (7) and consequently the EM algorithm depends on the relationship between the separation
factor ρ and the SNR η := |θ∗
| /σ of the true model (6). Since the true model does not belong in the
family of two components location-Gaussian mixtures in model class (7)  the role of the projection
parameter θ ∈ arg minθ∈Θ KL(P∗  Pθ) becomes crucial. In the next proposition  we provide an
explicit bound for the bias between θ and θ∗ as a function of the problem parameters.

4

Proposition 1. Given the true model (6) and any ρ > 0  we have

 

(8)

− θ(cid:12)(cid:12)  (cid:12)(cid:12)θ∗ + θ(cid:12)(cid:12)(cid:9) ≤ ρ|θ∗

|
where c is a universal positive constant that depends only on the set Θ.

min(cid:8)(cid:12)(cid:12)θ∗
− θ(cid:12)(cid:12)  (cid:12)(cid:12)θ∗ + θ(cid:12)(cid:12)(cid:9) ≤(cid:18)ρ +

| + c(cid:18) ρ|θ∗
σ (cid:19)1/4
In order to simplify our results in the sequel  we assume that η = |θ∗
bound on the bias—viz.:
| ≤(cid:18)ρ +

| /σ ≥ 1 and use a simpler
σ (cid:19)|θ∗
ρ1/4
(9)
| .
The bound above directly implies that(cid:12)(cid:12)θ(cid:12)(cid:12) belongs to the interval [(1 − Cρ)|θ∗
|   (1 + Cρ)|θ∗
|] 
| and(cid:12)(cid:12)θ(cid:12)(cid:12)
assuming that Cρ := ρ + ρ1/4/σ ≤ 1. As ρ → 0  we have Cρ → 0 implying that |θ∗
are almost identical. In the sequel  we utilize this precise control of(cid:12)(cid:12)θ(cid:12)(cid:12) in terms of |θ∗
|  provided
by Proposition 1  to analyze the behavior of the EM algorithm in a neighborhood of θ. Deﬁning
ρ(cid:63) := sup{ρ > 0|Cρ ≤ 1/9}  the following result characterizes the behavior population EM oper-
ator for the three-component Gaussian location mixture described by equation (6).
Theorem 1. There exist universal constants c(cid:48)  c(cid:48)(cid:48) such that the population EM operator for
model (6) with ρ ≤ ρ(cid:63) and η ≥ c(cid:48) satisﬁes

η3/4σ(cid:19)|θ∗

min(cid:8)(cid:12)(cid:12)θ∗

ρ1/4

(cid:12)(cid:12)M (θ) − θ(cid:12)(cid:12) = E(cid:12)(cid:12)2(wθ(X) − wθ(X))X(cid:12)(cid:12) ≤ γ(cid:12)(cid:12)θ − θ(cid:12)(cid:12)  

for any θ ∈ B(θ (cid:12)(cid:12)θ(cid:12)(cid:12) /4).

In words  Theorem 1 establishes that the population EM iterates (in the ideal  inﬁnite data limit)

the Appendix). These results have a direct implication for the sample-based version of EM that
is implemented in practice. In particular  the next result shows that EM updates with n samples
converge in a constant number of steps to a neighborhood of θ.
Corollary 1. Consider any scalar δ ∈ (0  1)  sample size n ≥ c1 log(1/δ) and starting point
θt+1 = Mn (θt) for the model (6) satisﬁes

are γ-contractive with respect to θ over the ball B(θ (cid:12)(cid:12)θ(cid:12)(cid:12) /4)  where γ ≤ e−c(cid:48)(cid:48)η2. Combining that
result with the condition Cρ ≤ 1/9  we can demonstrate that(cid:12)(cid:12)θ(cid:12)(cid:12) is unique (See Section A.1.4 in
θ0 ∈ B(θ (cid:12)(cid:12)θ(cid:12)(cid:12) /4). Then under the assumptions of Theorem 1  the sample-based EM sequence
with probability at least 1 − δ  where γ ≤ e−c(cid:48)η2.
Note that the bound (10) consists of two main terms: the ﬁrst term captures the geometric conver-
gence of the population EM operator from Theorem 1  while the second term characterizes the radius
of convergence in terms of sample complexity  which is O((cid:112)1/n). Therefore  with probability at
least 1 − δ  we have
| (θ∗2 + σ2)))
c|θ∗
(cid:12)(cid:12)θT − θ(cid:12)(cid:12) ≤

| (θ∗2 + σ2)
1 − γ
where c  c(cid:48) are universal constants.

(cid:12)(cid:12)θt − θ(cid:12)(cid:12) ≤ γt(cid:12)(cid:12)θ0 − θ(cid:12)(cid:12) +

|(cid:16)θ∗2 + σ2(cid:17)(cid:114) log(1/δ)

for T ≥ c(cid:48) log(n/(log(1/δ)|θ∗

(cid:114) log(1/δ)

1 − γ |θ∗

log(1/γ)

(10)

c2

n

n

 

3.2 Three-component mixtures with small weight for one component
Next  we consider the case where the true model P∗ is a three-component Gaussian location mixture
model of the form

P∗ =

2 N (−θ∗  σ2) + ωN (0  σ2) +
1 − ω

(11)
In other words  two components are dominant with means −θ∗ and θ∗ respectively  and we have
a small component at the origin. For such a model  it is again conceivable to ﬁt a 2-component
mixture given by equation (7). The primary interest in such a setting is driven by the fact that  when
ω > 0 is sufﬁciently small  recovering the third small component with center at origin is usually
hard; consequently clustering that component with one of the other two may be a good idea. Once
again  the convergence of EM is governed by the properties of θ that we characterize in the next
proposition.

2 N (θ∗  σ2).
1 − ω

5

Proposition 2. For the three components location-Gaussian mixtures in model (11)  we have

 

(12)

min(cid:8)(cid:12)(cid:12)θ∗
− θ(cid:12)(cid:12)  (cid:12)(cid:12)θ∗ + θ(cid:12)(cid:12)(cid:9) ≤ Cω |θ∗

cω1/8 |θ∗
|1/4
σ1/4√1 − ω
where c is a universal positive constant that depends only on the set Θ.
In order to simplify further results  we assume under the condition η := θ∗

− θ(cid:12)(cid:12)  (cid:12)(cid:12)θ∗ + θ(cid:12)(cid:12)(cid:9) ≤
σ ≥ 1. Then we have
min(cid:8)(cid:12)(cid:12)θ∗
|  where Cω := cω1/8/(σ√1 − ω). Such a bound on bias leads
to slightly different conditions for convergence of the EM algorithm for model (11) compared to
the EM convergence for model (6). Note that for any ﬁxed variance σ2  the function Cω increases
with ω and C0 = 0. Let ω(cid:63) = sup{ω > 0|C(ω) ≤ 1/9}. Similar to the model (6)  we analyze the
convergence rate of EM under a strong SNR condition of true model (11). We deﬁne ˜γ := γ(η  ω) =
(1 − ω)e−η2/64 + ω < 1. With the above notations in place  we now establish the contraction of the
population EM operator M (θ) for the three components location-Gaussian mixture (11).
Theorem 2. For SNR η ≥ 1 sufﬁciently large and ω ≤ ω(cid:63)  and for any θ0 ∈ B(θ (cid:12)(cid:12)θ(cid:12)(cid:12) /4)  the

population EM operator for the Gaussian mixture (11) satisﬁes

(13)

|M (θ0) − θ| = E(cid:12)(cid:12)2(wθ0(X) − wθ(X))X(cid:12)(cid:12) ≤ ˜γ(cid:12)(cid:12)θ0 − θ(cid:12)(cid:12) .

Consequently  the population EM sequence θt+1 = M (θt) converges to θ at a linear rate.

The precise expression for the contraction parameter ˜γ provides sufﬁcient conditions for a fast con-
vergence of EM  which involves an interesting trade off between the SNR η and weight ω. More
concretely  if the SNR is large enough  the population EM converges fast towards the projection θ 
which is unique in its absolute value (See Section A.1.4 in the Appendix). This fast convergence of
the population EM again enables us to derive the following convergence rate of sample-based EM:
Corollary 2. Consider the model (11) such that the assumptions of Theorem 2 hold. For any ﬁxed

δ ∈ (0  1)  θ0 ∈ B(θ (cid:12)(cid:12)θ(cid:12)(cid:12) /4)  if n ≥ c1 log(1/δ) then the sample EM iterates θt+1 = Mn(θt) satisfy

c2

1 − ˜γ |θ∗

|(cid:16)θ∗2 + σ2(cid:17)(cid:114) log(1/δ)

n

with probability at least 1 − δ.
Similar to the structure of the convergence result of sample EM updates in Corollary 1  the result in
Corollary 2 also consists of two key terms: the ﬁrst term is the linear rate of convergence from the
population EM operator in Theorem 2 while the second term characterizes the radius of convergence

in terms of sample complexity  which is of O((cid:112)1/n) after T = O(log n/ log(1/˜γ)) iterations.

4 Robustness of EM for mis-speciﬁed variances and weights

(cid:12)(cid:12)θt − θ(cid:12)(cid:12) ≤ ˜γt(cid:12)(cid:12)θ0 − θ(cid:12)(cid:12) +

1

1

P∗ =

In this section  we focus on establishing the convergence rate of EM under different mis-speciﬁed
regime of the ﬁtted model (1). In particular  we assume that the true data distribution P∗ is given by:
(14)
where σ > 0 is a given positive number  and |θ∗
| ∈ (0  σ/2) is a true but unknown parameter. Note
that the assumption that |θ∗
| ∈ (0  σ/2) ensures that the variance σ2 − θ∗2 is bounded away from
zero. We ﬁt the above model by unbalanced two-component Gaussian location mixture model Pθ
given by

2N (−θ∗  σ2 − θ∗2) 

2N (θ∗  σ2 − θ∗2) +

Pθ = πN (−θ  σ2) + (1 − π)N (θ  σ2) 

(15)
where π := 1
2 (1 − ) and || ∈ (0  1) are known apriori and only the parameter θ is to be estimated.
In the ﬁtted model Pθ  we have mis-speciﬁed the variance σ2 and the weight π  and we wish to
understand the rate of convergence of EM to ¯θ  where ¯θ is the parameter of the model P¯θ  and P¯θ is
the projection of the true model P∗ onto the model class Pθ := {Pθ : θ ∈ R}. We emphasize that
the main goal here is to see how the mis-speciﬁcation with variance and weight affects the statistical
inference of EM. We choose variance of the form σ2 − θ∗2 because under this setting  we obtain
interesting behavior of EM without rendering the proof too technical. We begin with the ﬁrst result
establishing the global linear convergence rate of population EM to θ.

6

Theorem 3. For a two-component Gaussian location mixture model (14) and ﬁtted model (15)  the
population EM operator θ (cid:55)→ M (θ) satisﬁes

(cid:12)(cid:12)M (θ) − θ(cid:12)(cid:12) ≤(cid:18)1 −

2

2(cid:19)(cid:12)(cid:12)θ − ¯θ(cid:12)(cid:12) .

Hence  the population EM sequence {θt} converges geometrically to θ from any initialization θ0.
There are two interesting features regarding the geometric convergence of population EM updates to
θ: (1) it does not require an evaluation of bias which was needed for our previous results; (2) it holds
under any initialization θ0. Overall  we have that ¯θ is unique  thereby we conclude that the projection
of P∗ to the model class (15) is unique. Before proceeding to the sample-based convergence of EM 
we establish the following upper bound on the bias of the parameter θ:
Proposition 3. For the two-component Gaussian location mixture model (14)  we have

where c(θ∗  σ) is a positive constant depending only on θ∗  σ  and the set Θ.

min(cid:8)(cid:12)(cid:12)θ − θ∗(cid:12)(cid:12)  (cid:12)(cid:12)θ + θ∗(cid:12)(cid:12)(cid:9) ≤ c(θ∗  σ) ·(cid:113)(cid:2)2 (1 − 2π)(cid:3)θ∗2 + θ∗4 

Given the above bound  we obtain the range of(cid:12)(cid:12)θ(cid:12)(cid:12) as(cid:12)(cid:12)θ(cid:12)(cid:12) ∈ [(1 − Cθ∗ )||θ∗
Cθ∗ := c(θ∗  σ)(cid:113)(cid:2)2(1 − 2π)(cid:3) + θ∗2. Equipped with this bound on(cid:12)(cid:12)θ(cid:12)(cid:12)  we have the following result
Corollary 3. Consider the model (14). Let radius r > 0 and n ≥ c1 log(1/δ) and θ0 ∈ B(cid:0)¯θ  r(cid:1) 

regarding the convergence of sample-based EM:

then the sample-based EM sequence θt+1 = Mn(θt)  satisﬁes
c2 ((1 + Cθ∗ )|θ∗

|   (1 + Cθ∗ )|θ∗

| + r) σ2

|] where

(cid:12)(cid:12)θt − θ(cid:12)(cid:12) ≤(cid:18)1 −

2

2(cid:19)t(cid:12)(cid:12)θ0 − θ(cid:12)(cid:12) +

with probability at least 1 − δ where  := 1 − 2π.
The proof of Corollary 3 is similar to those of Corollary 1 or Corollary 2; therefore  it is omitted. The
last corollary demonstrates that the sample-based EM iterates converge to ball of radius O((cid:112)1/n)
around θ after T = O(log n/ log(1/(1 − 2/2))) iterations.
5 Simulation studies

2

(cid:114) log(1/δ)

n

 

In this section  we illustrate our theoretical results using a few numerical experiments. In particular 
we use the EM algorithm to ﬁt 2-component Gaussian mixtures for the three mis-speciﬁed settings
considered above. For convenience in discussion  we refer to three settings as follows:
• Case 1 refers to the true model (6) from Section 3.1  namely a three component Gaussian mixture
where two of the components very close to each other and the quantity ρ ∈ (0  1) denotes the
extent of weak separation.
• Case 2 refers to the true model (11) from Section 3.2  namely  a three components Gaussian
mixture where one of the components has very small weight at origin and the quantity ω ∈ (0  1)
denotes the small mixture-weight.
• Finally  Case 3 refers to the true model (14) from Section 4  namely where the true model is a
For cases 1 and 2  we ﬁt a symmetric balanced two-Gaussian mixture given by equation (7); while
for the third case we ﬁt the unbalanced two-Gaussian mixture given by equation (15) for different

two-Gaussian mixture.

EM converges to θ (2)  we use the ﬁnal iterate from the population EM sequence to estimate the

values of π. Let(cid:98)θn denote the ﬁnal sample EM estimate. Since our results establish that population
error |(cid:98)θn − θ|. We now summarize our key ﬁndings:
(i) In Figure 1(a)  we observe that for all cases the ﬁnal statistical error |(cid:98)θn−θ| has a parametric

rate n−1/2 which veriﬁes the claims of Corollaries 1  2 and 3.

7

(ii) For all cases  the population EM sequence has a geometric convergence (we omit illustra-
tions for Cases 1 and 2). From Figure 1 (b)  we note that for Case 3  the linear convergence
of the population EM sequence θt+1 = M (θt) is affected by the extent of unbalancedness:
as π → 0.5  the rate of decay of the error of population EM sequence decreases which is
consistent with the contraction result stated in Theorem 3.
(iii) In panels (c) and (d) of Figure 1  we plot the biases for Case 1 and 2  with respect to ρ and
ω respectively. Least squares ﬁt on the log-log scale suggest that the biases stated in Propo-
sition 1 and Proposition 2 are potentially sub-optimal: the numerical scaling of the biases
|θ∗
− θ| is of the order ρ2 and ω for Case 1 and 2 respectively  which is signiﬁcantly smaller
than the corresponding scaling of the order ρ1/4 and ω1/8 stated in Propositions 1 and 2.
In Appendix B  we illustrate the scaling of the bias with θ∗ in these cases via further simu-
lations.

(a)

(c)

(b)

(d)

Figure 1. Plots depicting behavior of EM when ﬁtting two Gaussian mixture (7) for the three mis-
speciﬁed mixtures cases (6)  (11) and (14)  referred to as Case 1  2 and 3 respectively. (a) For all cases 

the statistical error |(cid:98)θn − θ| has the parametric rate n−1/2. (b) For Case 3  convergence of population

EM sequence θt+1 = M (θt) is affected by the mixture weight π. The convergence rate slows down
as π → 0.5. (c) For Case 1  the bias scales quadratically with the extent of weak-separation ρ for
different values of θ∗. (d) For Case 2  the bias scales linearly with the weight ω of the third component 
for different values of θ∗. Refer to the text for more details.

6 Discussion

In this paper  we analyzed the behavior of the EM algorithm for certain classes of mis-speciﬁed
mixture models. Analyzing the behavior of the EM algoirithm under general mis-speciﬁcation is
challenging in general  and we view the results in this paper as a ﬁrst step towards developing a
more general framework for the problem. In this paper  we studied the EM algorithm when it is used
to ﬁt Gaussian location mixture models to data generated by mixture models with larger numbers
of components  and/or differing mixture weights. We considered only univariate mixtures in this
paper  but we believe that several of our results can be extended to multivariate mixtures. It is also
interesting to investigate the behavior of the EM algorithm when it is used to ﬁt models with scale
parameters that vary (in addition to the location parameters). Besides deriving sharper results for the
settings considered in this paper  analyzing the behavior of EM for non-Gaussian and more general
mixture models is an appealing avenue for future work.

8

102103104n→10−210−1100|bθn−θ|StatisticalerrorvsnCase1:θ∗=5 ρ=0.2Case2:θ∗=5 ω=0.2Case3:θ∗=0.5slope=-0.500102030t→10−1110−810−510−2|θt−θ|Case3:ConvergenceofPopulationEMπ=0.10π=0.20π=0.30π=0.4510−310−210−1ρ→10−1210−1010−810−610−410−2|θ∗−θρ|Case1:Scalingofbiasvsρθ∗=1.0slope=2.00θ∗=3.0slope=2.1210−310−210−1ω→10−310−210−1|θ∗−θω|Case2:Scalingofbiasvsωθ∗=1.0θ∗=3.0slope=0.98θ∗=5.0References
[1] S. Balakrishnan  M. J. Wainwright  and B. Yu. Statistical guarantees for the EM algorithm:

From population to sample-based analysis. Annals of Statistics  45:77–120  2017.

[2] T. T. Cai  J. Ma  and L. Zhang. CHIME: Clustering of high-dimensional Gaussian mixtures

with EM algorithm and its optimality. Annals of Statistics  To Appear.

[3] C. Daskalakis  C. Tzamos  and M. Zampetakis. Ten steps of EM sufﬁce for mixtures of two

Gaussians. In Proceedings of the 2017 Conference on Learning Theory  2017.

[4] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 
39:1–38  1977.

[5] B. Hao  W. Sun  Y. Liu  and G. Cheng. Simultaneous clustering and estimation of heteroge-

neous graphical models. Journal of Machine Learning Research  To Appear.

[6] P. Heinrich and J. Kahn. Strong identiﬁability and optimal minimax rates for ﬁnite mixture

estimation. Annals of Statistics  46:2844–2870  2018.

[7] C. Jin  Y. Zhang  S. Balakrishnan  M. J. Wainwright  and M. I. Jordan. Local maxima in the
likelihood of Gaussian mixture models: Structural results and algorithmic consequences. In
Advances in Neural Information Processing Systems 29  2016.

[8] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.

Springer-Verlag  New York  NY  1991.

[9] J. Ma  L. Xu  and M. I. Jordan. Asymptotic convergence rate of the EM algorithm for Gaussian

mixtures. Neural Computation  12:2881–2907  2000.

[10] X. Nguyen. Convergence of latent mixing measures in ﬁnite and inﬁnite mixture models.

Annals of Statistics  4(1):370–400  2013.

[11] H. Teicher. Identiﬁability of ﬁnite mixtures. Ann. Math. Statist.  32:1265–1269  1963.

[12] A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With

Applications to Statistics. Springer-Verlag  New York  NY  2000.

[13] R. Vershynin.

arXiv:1011.3027v7.

Introduction to the non-asymptotic analysis of

random matrices.

[14] C. Villani. Optimal transport: Old and New. Springer  2008.

[15] Z. Wang  Q. Gu  Y. Ning  and H. Liu. High-dimensional expectation-maximization algorithm:
Statistical optimization and asymptotic normality. In Advances in Neural Information Process-
ing Systems 28  2015.

[16] C. F. J. Wu. On the convergence properties of the EM algorithm. Annals of Statistics  11:95–

103  1983.

[17] J. Xu  D. Hsu  and A. Maleki. Global analysis of expectation maximization for mixtures of

two Gaussians. In Advances in Neural Information Processing Systems 29  2016.

[18] L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures.

Neural Computation  8:129–151  1996.

[19] B. Yan  M. Yin  and P. Sarkar. Convergence of gradient EM on multi-component mixture of

Gaussians. In Advances in Neural Information Processing Systems 30  2017.

[20] X. Yi and C. Caramanis. Regularized EM algorithms: A uniﬁed framework and statistical

guarantees. In Advances in Neural Information Processing Systems 28  2015.

9

,Raaz Dwivedi
nhật Hồ
Koulik Khamaru
Martin Wainwright
Michael Jordan