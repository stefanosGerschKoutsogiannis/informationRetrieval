2009,Sparsistent Learning of Varying-coefficient Models with Structural Changes,To estimate the changing structure of a varying-coefficient   varying-structure (VCVS) model remains an important and open problem   in dynamic system modelling  which includes learning trajectories of   stock prices  or uncovering the topology of an evolving gene   network. In this paper  we investigate sparsistent learning of a   sub-family of this model --- piecewise constant VCVS models. We   analyze two main issues in this problem: inferring time points where   structural changes occur and estimating model structure (i.e.  model   selection) on each of the constant segments. We propose a two-stage   adaptive procedure  which first identifies jump points of structural   changes and then identifies relevant covariates to a response on   each of the segments. We provide an asymptotic analysis of the   procedure  showing that with the increasing sample size  number of   structural changes  and number of variables  the true model can be   consistently selected. We demonstrate the performance of the method   on synthetic data and apply it to the brain computer interface   dataset. We also consider how this applies to structure estimation   of time-varying probabilistic graphical models.,Sparsistent Learning of Varying-coefﬁcient Models

with Structural Changes

Mladen Kolar  Le Song and Eric P. Xing ∗

School of Computer Science  Carnegie Mellon University

{mkolar lesong epxing}@cs.cmu.edu

Abstract

To estimate the changing structure of a varying-coefﬁcient varying-structure
(VCVS) model remains an important and open problem in dynamic system mod-
elling  which includes learning trajectories of stock prices  or uncovering the
topology of an evolving gene network. In this paper  we investigate sparsistent
learning of a sub-family of this model — piecewise constant VCVS models. We
analyze two main issues in this problem: inferring time points where structural
changes occur and estimating model structure (i.e.  model selection) on each of
the constant segments. We propose a two-stage adaptive procedure  which ﬁrst
identiﬁes jump points of structural changes and then identiﬁes relevant covariates
to a response on each of the segments. We provide an asymptotic analysis of
the procedure  showing that with the increasing sample size  number of structural
changes  and number of variables  the true model can be consistently selected. We
demonstrate the performance of the method on synthetic data and apply it to the
brain computer interface dataset. We also consider how this applies to structure
estimation of time-varying probabilistic graphical models.

1 Introduction
Consider the following regression model:
Yi = X′

i = 1  . . .   n 

iβ(ti) + ǫi 

(1)
where the design variables Xi ∈ Rp are i.i.d. zero mean random variables sampled at some con-
ditions indexed by i = 1  . . .   n  such as the prices of a set of stocks at time i  or the signals from
some sensors deployed at location i; the noise ǫ1  . . .   ǫn are i.i.d. Gaussian variables with variance
σ2 independent of the design variables; and β(ti) = (β1(ti)  . . .   βp(ti))′ : [0  1] 7→ Rp is a vector
of unknown coefﬁcient functions. Since the coefﬁcient vector is a function of the conditions rather
than a constant  such a model is called a varying-coefﬁcient model [12]. Varying-coefﬁcient models
are a non-parametric extension to the linear regression models  which unlike other non-parametric
models  assume that there is a linear relationship (generalizable to log-linear relationship) between
the feature variables and the output variable  albeit a changing one. The model given in Eq. (1) has
the ﬂexibility of a non-parametric model and the interpretability of an ordinary linear regression.

Varying-coefﬁcient models were popularized in the work of [9] and [16]. Since then  they have been
applied to a variety of domains  including multidimensional regression  longitudinal and functional
data analysis  and modeling problems in econometrics and ﬁnance  to model and predict time- or
space- varying response to multidimensional inputs (see e.g. [12] for an overview.) One can easily
imagine a more general form of such a model applicable to these domains  where both the coefﬁcient
value and the model structure change with values of other variables. We refer to this class of models
as varying-coefﬁcient varying-structure (VCVS) models. The more challenging problem of structure
recovery (or model selection) under VCVS has started to catch attention very recently [1  24].

∗LS is supported by a Ray and Stephenie Lane Research Fellowship. EPX is supported by grant ONR
N000140910758  NSF DBI-0640543  NSF DBI-0546594  NSF IIS-0713379 and an Alfred P. Sloan Research
Fellowship. We also thank Za¨ıd Harchaoui for useful discussions.

1

)
t
(

1

β

)
t
(

2

β

)
t
(

p

β

0.5
0
−0.5
0.5
0
−0.5

0.5
0
−0.5
0

0.2

…

Y

.

.

.

0.4

0.6

Time t (i/n)

0.8

1

)
t
(

1

β

)
t
(

2

β

)
t
(

p

β

0.5
0
−0.5
0.5
0
−0.5

0.5
0
−0.5
0

0.2

β1

β2

βp

)
t
(

1

β

)
t
(

2

β

…

)
t
(

p

β

0.8

1

0.4

0.6

Time t (i/n)

0.5
0
−0.5
0.5
0
−0.5

0.5
0
−0.5
0

0.2

…

0.4

0.6

0.8

1

Time t (i/n)

(a)

(b)

(c)

(d)

Figure 1: (a) Illustration of an VCVS as varying functions of time. The interval [0  1] is partitioned into
{0  0.25  0.4  0.7  1}  which deﬁnes blocks on which the coefﬁcient functions are constant. At different blocks
only covariates with non-zero coefﬁcient affect the response  e.g. on the interval B2 = (0.25  0.4) covariates
X2 and Xp do not affect response. (b) Schematic representation of the covariates affecting the response during
the second block in panel (a)  which is reminiscent of neighborhood selection in graph structure learning. (c)
and (d) Application of VCVS for graph structure estimation (see Section 7) of non-piecewise constant evolving
graphs. Coefﬁcients deﬁning neighborhoods of different nodes can change on different partitions.

In this paper  we analyze VCVS as functions of time  and the main goal is to estimate the dynamic
structure and jump points of the unknown vector function β(t). To be more speciﬁc  we consider the
case where the function β(t) is time-varying  but piecewise constant (see Fig. 1)  i.e.  there exists
a partition T = {T1 = 0 < T2 < . . . < TB = 1}  1 < B ≤ n  of the time interval (scaled to)
[0  1]  such that β(t) = γj  t ∈ [Tj−1  Tj) for some constant vectors γj ∈ Rp  j = 1  . . .   B. We
refer to points T1  . . .   TB as jump points. Furthermore  we assume that at each time point ti only
a few covariates affect the response  i.e.  the vector β(ti) is sparse. A good estimation procedure
would be able to identify the correct partition of the interval [0  1] so that within each segment the
coefﬁcient function is constant. In addition  the procedure can identify active coefﬁcients and their
values within each segment  i.e.  the time-varying structure of the model. This estimation problem
is particularly important in applications where one needs to uncover dynamic relational information
or model structures from time series data. For example  one may want to infer at chosen time points
the (changing) set of stocks that are predictive of a particular stock one has been holding from a
time series of all stock prices; or to understand the evolving circuitry of gene regulation at different
growth stages of an organism that determines the activity of a target gene based on other regulative
genes  based on time series of microarray data. Another important problem is to identify structural
changes in ﬁelds such as signal processing  EEG segmentation and analysis of seismic signals. In
all these problems  the goal is not to estimate the optimum value of β(t) for predicting Y   but to
consistently uncover the zero and non-zero patterns in β(t) at time points of interest that reveal the
changing structure of the model. In this paper  we provide a new algorithm to achieve this goal  and
a theoretical analysis that proves the asymptotic consistency of our algorithm.

Our problem is remotely related to  but very different from  earlier works on linear regression models
with structural changes [4]  and the problem of change-point detection (e.g. [19])  which can also
be analyzed in the framework of varying-coefﬁcient models. A number of existing methods are
available to identify only one structural change in the data; in order to identify multiple changes
these methods can be applied sequentially on smaller intervals that are assumed to harbor only one
change [14]. Another common approach is to assume that there are K changes and use Dynamic
Programming to estimate them [4]. In this paper  we propose and analyze a penalized least squares
approach  which automatically adapts to the unknown number of structural changes present in the
data and performs the variable selection on each of the constant regions.

2 Preliminaries

For a varying-coefﬁcient regression model described in Eq. (1) with structural changes  a reason-
able estimator of the time-varying structure can be obtained by minimizing the so-called TESLA
(temporally smoothed L1-regularized regression) loss proposed in [1]: (for simplicity we suppress
the sample-size notation n in the regularization constants λn = {λn
2}  but it should be clear that
their values depend on n)

1   λn

n

n

p

(Yi − X′

ˆβ(t1; λ)  . . .   ˆβ(tn; λ) = arg min
β

Xk=1
where ||·||1 denotes the ℓ1 norm  and ||·||TV denotes a total variation norm:
||βk||TV =
Pn
i=2 |βk(ti) − βk(ti−1)|. From the analysis of [20]  it is known that each component function

||β(ti)||1 + 2λ2

||βk||TV  

iβ(ti))2 + 2λ1

Xi=1

Xi=1

(2)

2

βk can be chosen as a piecewise constant and right continuous function  i.e.  βk is a spline function 
with potential jump points at observation times ti  i = 1  . . .   n. In this particular case  the total
variation penalty deﬁned above allows us to conceptualize βk as a vector in Rn  whose components
βk i ≡ βk(ti) correspond to function values at ti  i = 1  . . .   n  but not as a function [0  1] 7→ R. We
continue to use the vector representation through the rest of the paper as it will simplify the notation.

The estimation problem deﬁned in Eq. (2) has a few appealing properties. The objective function on
the right-hand-side is convex and there exists a solution ˆβ  which can be found efﬁciently using a
standard convex optimization package. Furthermore  the penalty terms in Eq. (2) are constructed in
a way to perform model selection. Observe that ℓ1 penalty encourages sparsity of the signal at each
time point and enables a selection over the relevant coefﬁcients; whereas the total variation penalty
is used to partition the interval [0  1] so that ˆβk is constant within each segment. However  there are
also some drawbacks of the procedure  as shown in Lemma 1 below.

Let’s start with some notational clariﬁcations. Let X denote the design matrix  input observation
Xi at time i corresponds to the i-th row in X. For simplicity  we assume throughout the paper
that X are normalized to have unit length columns  i.e.  each dimension has unit Euclidean norm.
Let Bj  j = 1  . . .   B  denote the set of time points that fall into the interval [Tj−1  Tj); when the
meaning is clear from the context  we also use Bj as a shorthand of this interval. For example  XBj
and YBj represent the submatrix of X and subvector of Y   respectively  that include elements only
corresponding to time points within interval Bj. For a given solution ˆβ to Eq. (2)  there exists a
block partition ˆT = { ˆT1  . . .   ˆT ˆB} of [0  1] (possibly a trivial one) and unique vectors ˆγj ∈ Rp  j =
1  . . .   ˆB  such that ˆβk i = ˆγj k for ti ∈ ˆBj. The set of relevant covariates during inverval Bj  i.e.  the
support of vector γj  is denoted as SBj = {k | γj k 6= 0}. Likewise we deﬁne ˆS ˆBj
By construction  no consecutive vectors ˆγj and ˆγj+1 are identical. Note that both the number of
partitions ˆB = | ˆT |  and the elements in the partition ˆT   are random quantities. The following
lemma characterizes the vectors ˆγj using the subgradient equation of Eq. (2).
Lemma 1 Let ˆγj and ˆBj  j = 1  . . .   ˆB be vectors and segments obtained from a minimizer of
Eq. (2). Then each ˆγj can be found as a solution to the subgradient equation:

over ˆγj.

where

X′

ˆBj

X ˆBj

j + λ2ˆs(TV)

j

= 0 

ˆBj

Y ˆBj

+ λ1| ˆBj|ˆs(1)

ˆγj − X′
ˆs(1)
j ∈ ∂ ||ˆγj||1 = sign(γj) 

by convention sign(0) ∈ [−1  1]  and ˆs(TV)
if ˆγ2 k − ˆγ1 k > 0
if ˆγ2 k − ˆγ1 k < 0

1 k =(cid:26) −1

ˆs(TV)

1

j

and  for 1 < j < ˆB 

∈ Rp such that

 

ˆs(TV)
ˆB k

=(cid:26) 1 if ˆγ ˆB k − ˆγ ˆB−1 k > 0
−1 if ˆγ ˆB k − ˆγ ˆB−1 k < 0

ˆs(TV)

j k =( 2
−2
0

if ˆγj+1 k − ˆγj k > 0  ˆγj k − ˆγj−1 k < 0
if ˆγj+1 k − ˆγj k < 0  ˆγj k − ˆγj−1 k > 0
if (ˆγj k − ˆγj−1 k)(ˆγj+1 k − ˆγj k) = 1.

(3)

(4)

(5)

(6)

Lemma 1 does not provide a practical way to estimate ˆβTV  but it does characterize a solution.
From Eq. (3) we can see that the coefﬁcients in each of the estimated blocks are biased by two terms
coming from the ℓ1 and ||·||TV penalties. The larger the estimated segments  the smaller the relative
inﬂuence of the bias from the total variation  while the magnitude of the bias introduced by the ℓ1
penalty is uniform across different segments. The additional bias coming from the total variation
penalty was also noted in the problem of signal denoising [23]. In the next section  we introduce a
two step procedure which alleviate this effect.

3 A two-step procedure for estimating time-varying structures

In this section  we propose a new algorithm for estimating the time-varying structure of the varying-
coefﬁcient model in Eq. (1)  which does not suffer from the bias introduced by minimizing the
objective in Eq. (2). The algorithm is a two-step procedure summarized as follows:

3

1. Estimate the block partition ˆT   on which the coefﬁcient vector is constant within each

block. This can be obtained by minimizing the following objective:

n

Xi=1

p

Xk=1

(Yi − X′

iβ(ti))2 + 2λ2

||βk||TV  

(7)

which we refer to as a temporal difference (TD) regression for reasons that will be clear
shortly. We will employ a TD-transformation to Eq. (7) and turn it into an ℓ1-regularized
regression problem  and solve it using the randomized Lasso. Details of the algorithm and
how to extract ˆT from the TD-estimate will be given shortly.
objective within the block:

2. For each block of the partition  ˆBj  1 ≤ j ≤ ˆB  estimate ˆγj by minimizing the Lasso

ˆγj = argmin

γ∈Rp Xti∈ ˆBj

(Yi − X′

iγ)2 + 2λ1 ||γ||1 .

(8)

We name this procedure TDB-Lasso (or TDBL)  after the two steps (TD randomized Lasso  and
Lasso within Blocks) given above. The advantage of the TDB-Lasso compared to a minimizer of
Eq. (2) comes from decoupling the interactions between the ℓ1 and TV penalties (note that the two
procedures result in different estimates). Now we discuss step 1 in detail; step 2 is straightforward
using a standard Lasso toolbox.
To obtain a consistent estimate of ˆT from the TD-regression in Eq. (7)  we can transform Eq. (7)
into an equivalent ℓ1 penalized regression problem  which allows us to cast the ˆT estimation
problem as a feature selection problem. Let β†
k i denote the temporal difference between the re-
gression coefﬁcients corresponding to the same covariate k at successive time points ti−1 and
ti: β†
k i ≡ βk(ti) − βk(ti−1)  k = 1  . . .   p  i = 1  . . .   n with βk(t0) = 0  by conven-
It can be shown that the model in Eq. (1) can be expressed as Y † = X†β† + ǫ†  where
tion.
Y † ∈ Rn is a transformed vector of the TDs of responses  i.e.  each element Y †
i ≡ Yi − Yi−1;
X† = (X†
p) ∈ Rn×np is the transformed design matrix with lower triangular matrices
X†
k ∈ Rn×n corresponding to TD features computed from the covariates; ǫ† ∈ Rn is the trans-
formed TD-error vector; and β† ∈ Rnp is a vector obtained by stacking TD-coefﬁcient vectors β†
k.
(See Appendix for more details of the transformation.) Note that the elements of the vector ǫ† are
not i.i.d. any more. Using the transformation above  the estimation problem deﬁned on objective
Eq. (7) can be expressed in the following matrix form:

1  . . .   X†

(9)

ˆβ† = argmin

2

β∈Rnp (cid:12)(cid:12)(cid:12)(cid:12)Y † − X†β†(cid:12)(cid:12)(cid:12)(cid:12)

2 + 2λ2(cid:12)(cid:12)(cid:12)(cid:12)β†(cid:12)(cid:12)(cid:12)(cid:12)1 .

This transformation was proposed in [8] in the context of one-dimensional signal denoising  how-
ever  we are interested in the estimation of jump points in the context of time-varying coefﬁcient
model.

The estimator deﬁned in Eq. (9) is not robust with respect to small perturbations of data  i.e.  small
changes of variables Xi or Yi would result in a different ˆT . To deal with the problem of robustness 
we employed the stability selection procedure of [22] (see also the bootstrap Lasso [2]  however 
we have decided to use the stability selection because of the weaker assumptions). The stability
selection approach to estimating the jump-points is comprised of two main components: i) simulat-
ing multiple datasets using bootstrap  and ii) using the randomized Lasso outlined in Algorithm 1
(see also Appendix) to solve (9). While the bootstrap step improves the robustness of the estimator 
the randomized Lasso weakens the conditions under which the estimator ˆβ† selects exactly the true
features.
Let { ˆβ†
b=1 represent the set of estimates and their supports (i.e.  index of non-zero elements)
obtained by minimizing (9) for each of the M bootstrapped datasets. We obtain a stable estimate of
the support by selecting variables that appear in multiple supports

b   ˆJ †

b }M

ˆJ τ = {k | PM

b=1 1I{k ∈ ˆJ †
b }

M

≥ τ} 

(10)

which is then used to obtain the block partition estimate ˆT . The parameter τ is a tuning parameter
that controls the number of falsely identiﬁed jump points.

4

i=1 Xi ∈ Rp  penalty parameter λ  weakness parameter α ∈ (0  1]

Algorithm 1 Randomized Lasso
Input: Dataset {Xi  Yi}n
Output: Estimate ˆβ ∈ Rp  support ˆS
1: Choose randomly p weights {Wk}p
2: ˆβ = argminβ∈Rp Pn
3: ˆS = {k | ˆβk 6= 0}

k=1 from interval [α  1]

i=1(Yi − Xiβ)2 + 2λ Pp

k=1

|βk|
Wk

4 Theoretical analysis
We provide a theoretical analysis of TDB-Lasso  and show that under certain conditions both the
jump points and structure of VCVS can be consistently estimated. Proofs are deferred to Appendix.

4.1 Estimating jump points

We ﬁrst address the issue of estimating jump points by analyzing the transformed TD-regression
problem Eq. (9) and its feature selection properties. The feature selection using ℓ1 penalization has
been analyzed intensively over the past few years and we can adapt some of the existing results to
the problem at hand. To prove that all the jump points are included in ˆJ τ   we ﬁrst state a sparse
eigenvalue condition on the design (e.g. [6]). The minimal and maximal sparse eigenvalue  for
matrix X ∈ Rn×p  are deﬁned as

ϕmin(k  X) :=

inf

a∈Rp ||a||0≤k

 

ϕmax(k  X) :=

sup

a∈Rp ||a||0≤k

||Xa||2
||a||2

||Xa||2
||a||2

 

k ≤ p.

(11)

Note that in Eq. (11) eigenvalues are computed over submatrices of size k (i.e.  due to the constraint
on a by the ||·||0 norm). We can now express the sparse eigenvalues condition on the design.
A1: Let J † be the true support of β† and J = |J †|. There exist some C > 1 and κ ≥ 10 such that
(12)

< √C/κ.

ϕmax(CJ 2  X†)
ϕ3/2
min(CJ 2  X†)

This condition guarantees a correlation structure between TD-transformed covariates that allows for
detection of the jump points. Comparing to the irrepresentible condition [30  21  27]  necessary for
the ordinary Lasso to perform feature selection  condition A1 is much weaker [22] and is sufﬁcient
for the randomized Lasso to select the relevant feature with high probability (see also [26]).

Theorem 1 Let A1 be satisﬁed; and let the weakness α be given as α2 = νϕmin(CJ 2  X†)/(CJ 2) 

for any ν ∈ (7/κ  1/√2). If the minimum size of the jump is bounded away from zero as

k∈J † |β†
min
where λmin = 2σ†(√CJ + 1)q log np
i )  for np > 10 and J ≥ 7  there exists
some δ = δJ ∈ (0  1) such that for all τ ≥ 1 − δ  the collection of the estimated jump points ˆJ τ
satisﬁes 
(14)

k| ≥ 0.3(CJ)3/2λmin 
and σ†2

≥ V ar(Y †

(13)

n

P( ˆJ τ = J †) ≥ 1 − 5/np.

Remark: Note that Theorem 1 gives conditions under which we can recover every jump point in
every covariates. In particular  there are no assumptions on the number of covariates that change
values at a jump point. Assuming that multiple covariates change their values at a jump point  we
could further relax the condition on the minimal size of a jump given in Eq. (13). It was also pointed
to us that the framework of [18] may be a more natural way to estimate jump points.

4.2

Identifying correct covariates

Now we address the issue of selecting the relevant features for every estimated segment. Under
the conditions of Theorem 1  correct jump points will be detected with probability arbitrarily close
to 1. That means under the assumption A1  we can run the regular Lasso on each of the estimated
segments to select the relevant features therein. We will assume that the mutual coherence condition
[10] holds for each segment Bj. Let Σj = 1

kl = (Σj)k l.

Xi  with σj

X′
i

|Bj |Pi∈Bj

5

)! = 1.

(15)

d

(cid:12)(cid:12)SBj(cid:12)(cid:12)

A2: We assume there is a constant 0 < d ≤ 1 such that
k∈SBj  l6=k (|σj

P  max

kl| ≤

The assumption A2 is a mild version of the mutual coherence condition used in [7]  which is neces-
sary for identiﬁcation of the relevant covariates in each segment. Let ˆγj  k = 1  . . .   ˆBn denote the
Lasso estimates for each segment obtained by minimizing (8).

Theorem 2 Let A2 be satisﬁed. Also  assume that the conditions of Theorem 1 are satisﬁed. Let
K = max1≤j≤B ||γj||0 be the upper bound on the number of features in segments and let L be
an upper bound on elements of X. Let ρ = min1≤j≤B |Bj| denote the number of samples in the
smallest segment. Then for a sequence δ = δn → 0 

λ1 ≥ 4Lσs ln 2Kp

ρ

δ

ln 4Kp
δ

ρ

∨ 8L

and

min
1≤j≤B

min
k∈SBj |γj k| ≥ 2λ1 

we have

P( ˆB = B) = 1 

lim
n→∞

lim
n→∞

max
1≤j≤B

P(||ˆγj − γj||1 = 0) = 1 

lim
n→∞

min
1≤j≤B

P( ˆSBj = SBj ) = 1.

(16)

(17)

(18)

Theorem 2 states that asymptotically  the two stage procedure estimates the correct model  i.e.  it
selects the correct jump points and for each segment between two jump points it is able to select the
correct covariates. Furthermore  we can conclude that the procedure is consistent.

5 Practical considerations
As in standard Lasso  the regularization parameters in TDB-Lasso need to be tuned appropriately
to attain correct structural recovery. The TD regression procedure requires three parameters: the
penalty parameter λ2  cut-off parameter τ   and weakness parameter α. From our empirical experi-
ence  the recovered set of jump points ˆT vary very little with respect to these parameters in a wide
range. The result of Theorem 1 is valid as long as λ2 is larger than λmin given in the statement
of the theorem. Theorem 1 in [22] gives a way to select the cutoff τ while controlling the number
of falsely included jump points. Note that this relieves users from carefully choosing the range of
parameter λ2  which is challenging. The weakness parameter can be chosen in quite a large interval
(see Appendix on the randomized Lasso) and we report our results using the values α = 0.6.
In the second step of the algorithm  the ordinary Lasso minimizes Eq. (8) on each estimated segment
to select relevant variables  which requires a choice of the penalty parameter λ1. We do so by
minimizing the BIC criterion [25].

In practice  one cannot verify assumptions A1 and A2 on real datasets. In cases where the assump-
tions are violated  the resulting set of estimated jump points is larger than the true set T   e.g.
the
points close to the true jump points get included into the resulting estimate ˆT . We propose to use
an ad hoc heuristic to reﬁne the initially selected set of jump points. A commonly used procedure
for estimation of linear regression models with structural changes [3] is a dynamic programming
method that considers a possible structural change at every location ti  i = 1  . . .   n  with a compu-
tational complexity of O(n2) (see also [15]). We modify this method to consider jump points only
in the estimated set ˆT and thus considerably reducing the computational complexity to O(| ˆT |2) 
since | ˆT | ≪ n. The algorithm effectively chooses a subset ˜T ⊆ ˆT of size ˆB that minimizes the BIC
objective.

6 Experiments on Synthetic Data
We compared the TDB-Lasso on synthetic data with commonly used methods for estimating VCVS
models. The synthetic data was generated as follows. We varied the sample size from n = 100

6

1.5

1

0.5

0

 

Kernel + l
/l
1

2

Kernel + l
1

l
 + TV
1

TDB−Lasso

MREE

 

1

0.5

Corr. zeros

Precision

Recall

1

0.5

1

0.5

F
1

1

0.5

0

0

0

0

200
400
Sample size
Figure 2: Comparison results of different estimation procedures on a synthetic dataset.

200
400
Sample size

200
400
Sample size

200
400
Sample size

200
400
Sample size

to 500 time points  and ﬁxed the number of covariates is ﬁxed to p = 20. The block partition
was generated randomly and consists of ten blocks with minimum length set to 10 time points. In
each of the block  only 5 covariates out of 20 affected the response. Their values were uniformly at
random drawn from [−1 −0.1]∪[0.1  1]. With this conﬁguration  a dataset was created by randomly
drawing Xi ∼ N (0  Ip)  ǫi ∼ N (0  1.52) and computing Yi = Xiβ(ti) + ǫi for i = 1  . . .   n. For
each sample size  we independently generated 100 datasets and report results averaged over them.

A simple local regression method [13]  which is commonly used for estimation in varying coefﬁcient
models  was used as the simplest baseline for comparing the relative performance of estimation. Our
ﬁrst competitor is an extension of the baseline  which uses the following estimator [28]:

n

n

p

(Yi − X′

iβi′ )2Kh(ti′ − ti) +

min

β∈Rp×n

Xi′=1

Xi=1

Xj=1

n

λjvuut
Xi′=1

β2
i′ j 

(19)

where Kh(·) = 1
h K(·/h) is the kernel function. We will call this method “Kernel ℓ1/ℓ2”. Another
competitor uses the ℓ1 penalized local regression independently at each time point  which leads to
the following estimator of β(t) 

min
β∈Rp

Xi=1

n

p

(Yi − X′

iβ)2Kh(ti − t) +

λj|βj|.

(20)

Xj=1

We call this method “Kernel ℓ1”. The difference between the two methods is that “Kernel ℓ1/ℓ2”
biases certain covariates toward zero at every time point  based on global information; whereas
“Kernel ℓ1” biases covariates toward zero only based on local information. The ﬁnal competitor is
chosen to be the minimizer of Eq. (2) [1]  which we call “ℓ1 + TV”. The bandwidth parameter for
“Kernel ℓ1” and “Kernel ℓ1/ℓ2” is chosen using a generalized cross validation of a non-penalized
estimator. The penalty parameters λj are chosen according to the BIC criterion [28]. For the “ℓ1 +
TV” method  we optimize the BIC criterion over a two-dimensional grid of values for λ1 and λ2.

  where ˜β is the baseline
We report the relative estimation error  REE = 100 ×
local linear estimator  as a measure of estimation accuracy. To asses the performance of the model
selection  we report precision  recall and their harmonic mean F1 measure when estimating the
relevant covariates at each time point and the percentage of correctly identiﬁed irrelevant covariates.

i=1 Pp
i=1 Pp

j=1 | ˆβi j −β∗
j=1 | ˜βi j −β∗

Pn
Pn

i j |
i j |

From the experimental results  summarized in Fig. 2  we can see that the TDB-Lasso succeeds in
recovering the true model as the sample size increases. It also estimates the coefﬁcient values with
better accuracy than the other methods. It worth noting that the “Kernel + ℓ1” performs better than
the “Kernel + ℓ1/ℓ2” approach  which is due to the violation of the assumptions made in [28]. The
“ℓ1 + TV” performs better than the local linear regression approaches  however  the method gets
very slow for the larger values of the sample size and it requires selecting two tuning parameters 
which makes it quite difﬁcult to use. We conjecture that the “ℓ1 + TV” and TDB-Lasso have similar
asymptotic properties with respect to model selection  however  from our numerical experiments we
can see that for ﬁnite sample data  the TDB-Lasso performs better.

7 Application to Time-varying Graph Structure Estimation
An interesting application of the TDB-Lasso is in structural estimation of time-varying undirected
graphical models [1  17]. A graph structure estimation can be posed as a neighborhood selection

7

problem  in which neighbors of each node are estimated independently. Neighborhood selection
in the time-varying Gaussian graphical models (GGM) is equivalent to model selection in VCVS 
where value of one node is regressed to the rest of nodes. The regression problem for each node
can be solved using the TDB-Lasso. Graphs estimated in this way will have neighborhoods of each
node that are constant on a partition  but the graph as a whole changes more ﬂexibly (Fig. 1b-d).

t=3.00s

a
a

j

 
t
c
e
b
u
S

t=2.00s

t=1.00s

The graph structure estimation using the TDB-
Lasso is demonstrated on a real dataset of elec-
troencephalogram (EEG) measurements. We use
the brain computer interface (BCI) dataset IVa
from [11] in which the EEG data is collected from
5 subjects  who were given visual cues based on
which they were required to imagine right hand
or right foot for 3.5s. The measurement was per-
formed when the visual cues were presented on the
screen (280 times)  intermitted by periods of random length in which the subject could relax. We
use the down-sampled data at 100Hz. Fig. 3 gives a visualization of the brain interactions over the
time of the experiment for the subject ’aa’ while presented with visual cues for the class 1 (right
hand). Estimated graphs of interactions between different parts of the brain for other subjects and
classes are given in Appendix due to the space limit.

Figure 3: Brain interactions for the subject ’aa’
when presented with visual cues of the class 1

We also want to study whether the estimated time-varying network are discriminative features for
classifying the type of imaginations in the EEG signal. For this purpose  we perform unsupervised
clustering of EEG signals using the time-varying networks and study whether the grouping corre-
spond to the true grouping according to imagination label. We estimate a time-varying GGM using
the TDB-Lasso for each visual cue and cluster the graphs using the spectral K-means clustering [29]
(using a linear kernel on the coefﬁcients to measure similarity). Each cluster is labeled according to
the majority of points it contains. Finally  each cue if classiﬁed based on labels of the time-points
that it contains. Table 1 summarizes the classiﬁcation accuracy for each subject based on K = 4
clusters (K was chosen as a cutoff point  when there was little decrease in K-means objective). We
compare this approach to a case when GGMs with a static structure are estimated [5]. Note that
the supervised classiﬁers with special EEG features are able to achieve much higher classiﬁcation
accuracy  however  our approach does not use any labeled data and can be seen as an exploratory
step. We also used TDB-Lasso for estimating the time-varying gene networks from microarray data
time series data  but due to space limit  results will be reported later in a biological paper.

Table 1: Classiﬁcation accuracies based on learned brain interactions.

Subject

TDB-Lasso

Static

aa
0.69
0.58

al
0.80
0.63

av
0.59
0.54

aw
0.67
0.57

ay
0.83
0.61

8 Discussion

We have developed the TDB-Lasso procedure  a novel approach for model selection and variable es-
timation in the varying-coefﬁcient varying-structure models with piecewise constant functions. The
VCVS models form a ﬂexible nonparametric class of models that retain interpretability of parametric
models. Due to their ﬂexibility  important classical problems  such as linear regression with struc-
tural changes and change point detection  and some more recent problems  like structure estimation
of varying graphical models  can be modeled within this class of models. The TDB-Lasso compares
favorably to other commonly used [28] or latest [1] techniques for estimation in this class of models 
which was demonstrated on the synthetic data. The model selection properties of the TDB-Lasso 
demonstrated on the synthetic data  are also supported by the theoretical analysis. Furthermore  we
demonstrate a way of applying the TDB-Lasso for graph estimation on a real dataset.

Application of the TDB-Lasso procedure goes beyond the linear varying coefﬁcient regression mod-
els. A direct extension is to generalized varying-coefﬁcient models g(m(Xi  ti)) = X′
iβ(ti)  i =
1  . . .   n  where g(·) is a given link function and m(Xi  ti) = E[Y |X = Xi  t = ti] is the con-
ditional mean. Estimation in generalized varying-coefﬁcient models proceeds by changing the
squared loss in Eq. (7) and Eq. (8) to a different appropriate loss function. The generalized varying-
coefﬁcient models can be used to estimate the time-varying structure of discrete Markov Random
Fields  again by performing the neighborhood selection.

8

References
[1] Amr Ahmed and Eric P. Xing. Tesla: Recovering time-varying networks of dependencies in social and

biological studies. Proceeding of the National Academy of Science  2009.

[2] Francis R. Bach. Bolasso: model consistent lasso estimation through the bootstrap. In William W. Cohen 
Andrew McCallum  and Sam T. Roweis  editors  ICML  volume 307 of ACM International Conference
Proceeding Series  pages 33–40. ACM  2008.

[3] J Bai and P Perron. Computation and analysis of multiple structural change models. Journal of Applied

[4] Jushan Bai and Pierre Perron. Estimating and testing linear models with multiple structural changes.

Econometrics  (18):1–22  2003.

Econometrica  66(1):47–78  January 1998.

[5] O. Banerjee  L. El Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood

estimation. J. Mach. Learn. Res.  9:485–516  2008.

[6] P. Bickel  Y. Ritov  and A. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Ann. of Stat.
[7] Florentina Bunea. Honest variable selection in linear and logistic regression models via ℓ1 and ℓ1 + ℓ2

penalization. Electronic Journal of Statistics  2:1153  2008.

[8] Scott S. Chen  David L. Donoho  and Michael A. Saunders. Atomic decomposition by basis pursuit.

SIAM Journal on Scientiﬁc Computing  20(1):33–61  1999.

[9] William S. Cleveland  Eric Grosse  and William M. Shyu. Local regression models. In John M. Chambers

and Trevor J. Hastie  editors  Statistical Models in S  pages 309–376  1991.

[10] David L. Donoho  Michael Elad  and Vladimir N. Temlyakov. Stable recovery of sparse overcomplete

representations in the presence of noise. IEEE Trans. Inform. Theory  52:6–18  2006.

[11] G. Dornhege  B. Blankertz  G. Curio  and K. M¨uller. Boosting bit rates in non-invasive EEG single-trial
classiﬁcations by feature combination and multi-class paradigms. IEEE Trans. Biomed. Eng.  51:993–
1002  2004.

[12] Jianqing Fan and Qiwei Yao. Nonlinear Time Series: Nonparametric and Parametric Methods. (Springer

Series in Statistics). Springer  August 2005.

Statistics  27:1491–1518  2000.

[13] Jianqing Fan and Wenyang Zhang. Statistical estimation in varying-coefﬁcient models. The Annals of

[14] Za¨ıd Harchaoui  Francis Bach  and ´Eric Moulines. Kernel change-point analysis. In D. Koller  D. Schu-
urmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information Processing Systems 21. 2009.
In J.C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20  pages 617–
624. MIT Press  Cambridge  MA  2008.

[15] Za¨ıd Harchaoui and C´eline Levy-Leduc. Catching change-points with lasso.

[16] Trevor Hastie and Robert Tibshirani. Varying-coefﬁcient models. Journal of the Royal Statistical Society.

Series B (Methodological)  55(4):757–796  1993.

[17] Mladen Kolar  Le Song  and Eric Xing. Estimating time-varying networks. In arXiv:0812.5087  2008.
[18] Marc Lavielle and Eric Moulines. Least-squares estimation of an unknown number of shifts in a time

series. Journal of Time Series Analysis  21(1):33–59  2000.

[19] E. Lebarbier. Detecting multiple change-points in the mean of gaussian process by model selection. Signal

[22] Nicolai Meinshausen and Peter B¨uhlmann. Stability selection. Preprint  2008.
[23] Alessandro Rinaldo. Properties and reﬁnements of the fused lasso. Preprint  2008.
[24] Le Song  Mladen Kolar  and Eric P. Xing. Keller: Estimating time-evolving interactions between genes.
In Proceedings of the 16th International Conference on Intelligent Systems for Molecular Biology  2009.
[25] Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and smoothness

via the fused lasso. Journal Of The Royal Statistical Society Series B  67(1):91–108  2005.

[26] S. A. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the lasso  2009.
[27] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity. Preprint  2006.
[28] H. Wang and Y. Xia. Shrinkage estimation of the varying coefﬁcient model. Manuscript  2008.
[29] H Zha  C Ding  M Gu  X He  and H Simon. Spectral relaxation for k-means clustering. pages 1057–1064.

MIT Press  2001.

[30] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res.  7:2541–2563  2006.

[20] E. Mammen and S. van de Geer. Locally adaptive regression splines. Ann. of Stat.  25(1):387–413  1997.
[21] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the lasso. Annals

Process.  85(4):717–736  2005.

of Statistics  34:1436  2006.

9

,Wojciech Samek
Duncan Blythe
Klaus-Robert Müller
Motoaki Kawanabe
Deeparnab Chakrabarty
Prateek Jain
Pravesh Kothari
Tasuku Soma
Yuichi Yoshida