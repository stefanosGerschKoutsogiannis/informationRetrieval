2019,Elliptical Perturbations for Differential Privacy,We study elliptical distributions in locally convex vector spaces  and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfy DP is that the corresponding privacy mechanism must induce equivalent probability measures for all possible input databases. We show that elliptical distributions with the same dispersion operator  $C$  are equivalent if the difference of their means lies in the Cameron-Martin space of $C$. In the case of releasing finite-dimensional summaries using elliptical perturbations  we show that the privacy parameter $\ep$ can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace  $t$  Gaussian  and $K$-norm noise. Surprisingly  we show that the multivariate Laplace noise does not achieve $\ep$-DP in any dimension greater than one. Finally  we show that when the dimension of the space is infinite  no elliptical distribution can be used to give $\ep$-DP; only $(\epsilon \delta)$-DP is possible.,Elliptical Perturbations for Differential Privacy

Matthew Reimherr ∗
Department of Statistics

Pennsylvania State University

University Park  PA 16802

mreimherr@psu.edu

Department of Statistics

Pennsylvania State University

University Park  PA 16802

Jordan Awan †

awan@psu.edu

Abstract

We study elliptical distributions in locally convex vector spaces  and determine
conditions when they can or cannot be used to satisfy differential privacy (DP).
A requisite condition for a sanitized statistical summary to satisfy DP is that the
corresponding privacy mechanism must induce equivalent probability measures
for all possible input databases. We show that elliptical distributions with the
same dispersion operator  C  are equivalent if the difference of their means lies
in the Cameron-Martin space of C. In the case of releasing ﬁnite-dimensional
summaries using elliptical perturbations  we show that the privacy parameter 
can be computed in terms of a one-dimensional maximization problem. We apply
this result to consider multivariate Laplace  t  Gaussian  and K-norm noise. Sur-
prisingly  we show that the multivariate Laplace noise does not achieve -DP in
any dimension greater than one. Finally  we show that when the dimension of the
space is inﬁnite  no elliptical distribution can be used to give -DP; only (  δ)-DP
is possible.

1

Introduction

Inﬁnite dimensional objects and parameters arise commonly in nonparametric statistics  shape anal-
ysis  and functional data analysis. Several recent works have made strides towards extending tools
for differential privacy (DP) to handle such settings. Some of the ﬁrst results in this area were given
in Hall et al. (2013)  with a particular emphasis on Gaussian perturbations and point-wise releases of
statistical summaries represented as univariate functions. This work was extended to more general
Banach and Hilbert space based summaries by Mirshani et al. (2017)  which included protections for
public releases based on path level summaries  nonlinear transformations of functional summaries 
and full function releases as well. However  Gaussian perturbations are not always satisfactory since
they cannot be used to achieve pure DP (-DP)  which requires heavier tailed distributions. Rather 
for pure DP  the most popular distribution is the Laplace mechanism  whose tails are “just right" for
achieving DP in ﬁnite dimensional summaries (Dwork et al.  2006).
When one moves from univariate to multivariate settings  generalizing the Laplace mechanism is
not as simple as generalizing the Gaussian. Often  when the Laplace mechanism is used in mul-
tivariate settings  iid Laplace random variables are used. However  this approach fails to capture
the multivariate dependence structure of the data or parameter of interest. Furthermore  in inﬁnite
dimensional settings  adding iid noise is usually not an option if one wishes to remain in a particular
function space. To address these issues  we study the use of elliptical distributions to satisfy DP 
which allow for a dispersion operator and are closely related to Gaussian distributions. Elliptical
∗Research supported in part by NSF DMS 1712826  NSF SES 1853209  and the Simons Institute for the
†Research supported in part by NSF SES-153443 and NSF SES-1853209.

Theory of Computing at UC Berkeley.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

processes offer a nice option for designing DP mechanisms for multivariate and inﬁnite-dimensional
data as they allow for the customization of both the tail behavior and dependence structure  which
can be tailored to the problem at hand. Recently Bun and Steinke (2019) explored several alterna-
tive univariate distributions for achieving privacy such as Cauchy  Student’s T  Laplace Log-Normal 
Uniform Log-Normal  and Arsinh-Normal  which can be extended to elliptical distributions.
We are interested in releasing a sanitized version of a statistic T : D → X  where D is a metric
space  representing the space of possible input databases  D  and X is a locally convex vector space.

To achieve differential privacy  we will release (cid:101)T = T (D) + σX  where σ is a positive scalar  and X

is a random element of X. In particular  we consider X which are drawn from elliptical distributions 
of which the multivariate Laplace and Gaussian distributions are special cases. Most linear spaces
are locally convex vector spaces  including all Hilbert Spaces  Banach Spaces  Frechet spaces  and
product spaces of normed vector spaces  meaning that our results will hold quite broadly.
We consider the setting where the statistical summary and privacy mechanism are truly inﬁnite
dimensional  meaning that the problem cannot be embedded into a ﬁnite dimensional subspace
where multivariate privacy tools can be used. There are both interesting mathematical and practical
motivations for this perspective. First  our setting can be viewed as a limit of multivariate problems;
if one has privacy over the full inﬁnite dimensional space  then this ensures that the noise is well
behaved when releasing multivariate summaries  regardless of how many are released. Second  one
does not need to ensure that every database uses the same ﬁnite dimensional subspace  allowing
practitioners to use whatever methods and summaries they prefer. And third  our setting is very
convenient when addressing multiple queries. In particular  one does not need to spend a fraction of
the privacy budget for every query. Instead  the amount spent for each subsequent query decreases
dramatically  eventually leveling out to a maximum  or (  δ). To accomplish this  one does not
need to “store” the inﬁnite dimensional noise  instead  we can generate as much of the noise as is
needed for a particular query while conditioning on any noise values generated for prior queries.
We also provide a surprising result showing that -DP can only be achieved for a ﬁnite number of
summaries or point-wise evaluations; in inﬁnite dimensions no elliptical perturbation is capable of
achieving -DP over the full function space  one can only achieve (  δ)-DP. This is in stark contrast
with what is known from the univariate or multivariate literature on DP.
While elliptical distributions are being used more frequently in statistics and machine learning (e.g.
Schmidt  2003; Frahm et al.  2003; Soloveychik and Wiesel  2014; Couillet et al.  2016; Sun et al. 
2016; Goes et al.  2017; Ollila and Raninen  2019)  some fundamental questions regarding elliptical
distributions in function spaces remain underdeveloped. For data privacy  the question of equiva-
lence/orthogonality of elliptical measures is particularly important. In terms of data privacy  if a
perturbation in a dataset produces a private summary that is orthogonal (in a probabilistic sense) to
the old one  then the summaries cannot be differentialy private since they can be distinguished with
probability one. We show that several conditions for making this determination transfer nicely from
the Gaussian setting  but not all. While conditions on the location function remain the same  con-
ditions on the dispersion function change. Furthermore  that all elliptical measures are equivalent
or orthogonal need no longer hold without additional assumptions. Regardless  for the purposes of
privacy  determining equivalence/orthogonality based on the location is the primary requirement.
Related Work: Our general approach of adding noise from a data-independent distribution to a
summary statistic is one of the simplest and most common methods of achieving DP. This approach
was ﬁrst developed using the Laplace mechanism (Dwork et al.  2006)  and has since been expanded
to include a larger variety of distributions. Ghosh et al. (2009) showed that when the data is a count 
then the optimal noise adding distribution is discrete Laplace. Geng and Viswanath (2016) extended
this result to the continuous setting  developing the staircase distribution which is closely related to
discrete Laplace. In the multivariate setting  the most common solution is to add iid Laplace noise
to each coordinate (Dwork and Roth  2014). However  Hardt and Talwar (2010) and Awan and
Slavkovi´c (2019) demonstrate that capturing the covariance structure in the data  via the K-norm
mechanism can substantially reduce the amount of noise required.
After adding noise to summary statistics  researchers have shown that many complex statistical and
machine learning tasks can be produced by post-processing  such as linear regression (Zhang et al. 
2012)  maximum likelihood estimation (Karwa and Slavkovi´c  2016)  hypothesis testing (Vu and
Slavkovi´c  2009; Gaboardi et al.  2016; Awan and Slavkovi´c  2018)  posterior inference (Williams
and Mcsherry  2010; Karwa et al.  2016)  or general asymptotic analysis (Wang et al.  2018).

2

To date  the only additive mechanism in inﬁnite-dimensions is the Gaussian mechanism  developed
by Hall et al. (2013) and Mirshani et al. (2017). However  there has been other work on developing
privacy tools for these spaces. Awan et al. (2019) show that the exponential mechanism (McSherry
and Talwar  2007) can be used in arbitrary Hilbert spaces  by integrating with respect to a ﬁxed
probability measure such as a Gaussian process. An alternative approach proposed by Alda and
Rubinstein (2017) uses Bernstein polynomial approximations to release private functions. Recently 
Smith et al. (2018) utilized the techniques of Hall et al. (2013) to develop private Gaussian process
regression. Similar to the pufferﬁsh approach (Kifer and Machanavajjhala  2014)  they assume that
the predictors are public  and use the known covariance structure to tailor the noise distribution.
Organization: In Section 2  we review the necessary background on locally convex vector spaces 
elliptical distributions  and differential privacy. In Section 3  we study the equivalence and orthog-
onality of elliptical measures  and give a condition that ensures that two elliptical measures are
equivalent. In Section 4  we investigate using elliptical perturbations to achieve DP. First  we con-
sider the ﬁnite dimensional case in Section 4.1  and in Theorem 3 we give a condition for elliptical
perturbations to satisfy -DP as well as a method of computing . In Section 4.2 we show that if the
dimension of the space is inﬁnite  then no elliptical perturbation can satisfy -DP. In fact  we show
that every elliptical distribution can only achieve (  δ)-DP for a positive δ. We give short proof
sketches throughout the document  with detailed proofs left to the Supplementary Material.

2 Elliptical Distributions
Elliptical distributions  whether over Rd or a more general vector space can be deﬁned in a variety
of equivalent ways. Intuitively  an elliptical distribution is one in which its density contours form
hyperellipses. However  this presupposes that the measure is absolutely continuous with respect to
Lebesgue measure. Thus  it is often useful in multivariate settings to use alternative deﬁnitions that
are more easy to generalize  but which are equivalent to the shape of the density contours when they
exist. This is not unique to elliptical measures  such alternative deﬁnitions are often useful when
working with inﬁnite dimensional objects (e.g. Bosq  2000). Throughout  we focus our attention on
an arbitrary  real  locally convex vector space (from here on LCS)  X  but we will restrict ourselves
to simpler spaces (e.g. Banach  Hilbert  or Euclidean spaces) as needed or for illustration. For
ease of reference  recall the following concepts from functional analysis (see Rudin (1991) for an
introduction).

those operations are well deﬁned).

• A set  X  is called a vector space if it is closed under addition and scalar multiplication (and
• A vector space  X  is called a topological vector space  if it is equipped with a topology
under which addition and scalar multiplication are continuous.
• A topological vector space X is called locally convex if its topology is generated by a
separated family of semi-norms  {pα : α ∈ I}  where I is an arbitrary index set and
separated means that for all nonzero x ∈ X there exists α ∈ I such that pα(x) (cid:54)= 0. A base
for the topology is given by sets of the form Aα  = {x ∈ X : pα(x) < }.

• The topological dual  X∗  is the collection of all continuous linear functionals on X.

The assumption that the seminorms are separated is not always included in the deﬁnition  but is
equivalent to assuming that the space is Hausdorff. Recall that a topology deﬁnes the open sets  a
collection of subsets that is closed under uncountable unions  ﬁnite intersections  and contains both
X and ∅. We use this level of generality to include as many settings as possible into our framework.
In particular  all ﬁnite dimensional Euclidean spaces  normed vector spaces  Hilbert Spaces  Banach
Spaces  and Frechet spaces are types of LCS. In addition  uncountable product spaces of normed
spaces  which are often used in the mathematical foundations of stochastic processes  are LCS as
well (when equipped with the product topology). To ﬁnd practical examples of spaces that are not
locally convex spaces  one either has to consider nonlinear spaces  such as manifolds  or equip a
space with an “odd" metric (such as Lp for p < 1).
Example 1 (LCS Examples). The deﬁnition of LCS in terms of seminorms is perhaps unintuitive
at ﬁrst  but can be motivated by product spaces such as R[0 1] = {f : [0  1] → R}. The space R[0 1]
is in a sense “too large” to accommodate a norm  but it is easy to deﬁne a family of semi-norms
that measure the magnitude of the function coordinate-wise. Here α ∈ I := [0  1] and we deﬁne

3

pα(f ) = |f (α)|. Note that for any particular α  pα is not a norm  since pα(f−g) = 0 does not imply
that f = g. However  the entire collection of semi-norms separates points since pα(f − g) = 0 for
all α ∈ [0  1] implies that f = g.
It is easy to see that any normed space ﬁts the deﬁnition of LCS. For C[0  1]  we set I = {1} and

deﬁne p1(f ) = supt∈[0 1] |f (t)|  and for L2[0  1] we set I = {1} and deﬁne p1(f ) =(cid:82) f 2(t)dt.

When working with a LCS one commonly uses one of two σ-algebras. The Borel σ-algebra  B  is the
smallest σ-algebra that contains the open sets. The cylinder σ-algebra  C  is the smallest σ-algebra
that makes all continuous linear functionals measurable. In general we have C ⊆ B  but these two
σ-algebras are not equal unless the space has additional structure  e.g. separable Banach spaces.
This creates complications in inﬁnite dimensional settings. For example  the technical theory for
stochastic processes often starts with product spaces such as R[0 1]. There  the two sigma algebras
are not the same  which is an issue for privacy as one desires privacy over B  not just C. This
is because only the events in the chosen σ-algebra are protected  and a larger σ-algebra offers a
stronger privacy guarantee. More importantly  C does not contain most sets of interest  including
continuous functions  linear functions  polynomials  constant functions  etc.
(Billingsley  1979 
Problem 36.6). To overcome this challenge  Mirshani et al. (2017) used Cameron-Martin theory  to
obtain DP over all of B through careful use of densities in inﬁnite dimensional spaces. This theory
is built upon Gaussian processes; however  we will show that several of their key results  especially
those needed for privacy  extend directly to elliptical distributions. Throughout this paper  we assume
X is equipped with its Borel σ-algebra when discussing measures  measurability  and DP.
Often it is convenient to deﬁne probability measures over abstract spaces in terms of their character-
istic functionals (i.e. Fourier transforms)  which uniquely determine measures in any LCS.
Deﬁnition 1 (Fang  2017). A measure  P   over a locally convex space X is called elliptical if and

only if its characteristic functional  (cid:101)P : X∗ → C  has the form

exp{ig(x)} dP (x) = eig(µ)φ0(C(g  g)) 

(cid:101)P (g) =

(cid:90)

X

where µ ∈ X  C is a symmetric  positive deﬁnite  continuous bilinear form on X∗ × X∗  and φ0 is a
positive deﬁnite function over R  which is continuous at 0 and satisﬁes φ0(0) = 1.
Deﬁnition 1 implies that the distribution of P is uniquely determined by knowing µ  C  and φ0. The
object µ denotes the center of the distribution; we will say a distribution is centered if µ = 0. The
object C is often called the covariance or dispersion operator. In general  C can either be identiﬁed
as an operator or bilinear form (in fact a (0  2) tensor). We avoid introducing extra notation we will
let C(g) denote the operator version and C(f  g) the bilinear form.
We begin by presenting a second characterization of elliptical measures. This is a well known result 
but we are unaware of a reference for this level of generality. We cite Fang (2017)  which covers the
multivariate case  but the proof is the same for general LCS.
Theorem 1 (Fang  2017). Let X ∈ X be an elliptically distributed random variable. Then there
exists a mean zero Gaussian process Z ∈ X with covariance operator C  an element µ ∈ X  and a
strictly positive random variable V ∈ R+  that is independent of Z  and satisﬁes X
This result is often phrased as “every elliptical distribution is a scalar mixture of Gaussian pro-
cesses." While it is  of course  a fascinating result in its own right  it also provides a simple method
of generating and simulating from arbitrary elliptical distributions.
Due to this corollary  we will index every elliptical measure using µ  C  and the mixing distribution
of V   which we will denote as ψ  and use the notation E(µ  C  ψ). Equivalently  we could index
using the φ0 from Deﬁnition 1  but our results in Section 3 are easier to present in terms of ψ.
We conclude by stating a general deﬁnition of DP  which makes sense over any measurable space 
though we state it here for LCS. The concept of differential privacy was ﬁrst introduced in Dwork
et al. (2006) and Dwork (2006). Over time researchers have worked to make the deﬁnition more
precise and ﬂexible  such as Wasserman and Zhou (2010) who state it in terms of conditional distri-
butions. For a general  axiomatic treatment of formal privacy  see Kifer et al. (2012).
Deﬁnition 2. Let (D  d) be a metric space and {PD : D ∈ D} be a family of probability measures
over a locally convex topological vector space X. We say the family achieves (  δ)-differential

L
= µ + V Z.

4

privacy if for any d(D  D(cid:48)) ≤ 1 and any measurable A  we have
PD(A) ≤ ePD(cid:48)(A) + δ.

(1)
Intuitively  D represents the universe of possible input databases. One then refers to {PD : D ∈ D}
as the privacy mechanism. The most common setting when discussing DP is when D is a product
space and the metric is the Hamming distance. However  the Hamming distance (which counts
differences in coordinates) is insensitive to the magnitude of the difference between two inputs D
and D(cid:48)  thus one may wish to consider alternatives and so we take the more general approach. As
discussed earlier  while DP can be deﬁned with any σ-algebra  we assume that X is equipped with
the Borel σ-algebra as it offers more intuitive guarantees. We refer to (  δ)-DP as approximate DP
when δ > 0 and as pure-DP when δ = 0. When using pure DP  we often just write -DP.
Another way of viewing -DP (that is  taking δ = 0) is through the equivalence/orthogonality of
probability measures. As was discussed in Awan et al. (2019)  in an -DP mechanism the individual
measures that make up the mechanism are all equivalent in a probabilistic sense (meaning they agree
on the zero sets). Conversely  if the measures are orthogonal then the mechanism cannot even be
(  δ)-DP. This perspective was used in Mirshani et al. (2017) for the case of Gaussian mechanisms.
However  the corresponding theory for elliptical distributions is less developed. In the next section
we extend several fundamental results of Gaussian processes to elliptical distributions.

3 Equivalence and Orthogonality of Elliptical Measures

A classic result from probability theory is that any two Gaussian processes are either equivalent or
orthogonal (that is  as probability measures they either agree on the zero sets or concentrate their
mass on disjoint sets). Recall that by the Radon-Nikodym theorem  if two measures are equivalent
then there exists a density of one with respect to the other (and vice versa). What we will now show
is that this property  to a degree  extends to any elliptical family. Furthermore  we will show that
the conditions for establishing this equivalence/orthogonality are nearly the same as for Gaussian
processes. We begin with a fairly simple yet surprisingly useful technical lemma.
Lemma 1. Let (Ω F  P ) be a probability space. Let X 1 : Ω → X and X 2 : Ω → X denote two
random elements of X  and let T 1 : Ω → R and T 2 : Ω → R be two random variables. Let P i
denote the probability measure over X induced by X i and let Qi denote the measure over R induced
t denote the conditional measure of X i given T i = t. If Q1 and Q2 are equivalent and
by T i. Let P i
t and P 2
P 1

t are equivalent for almost all t (wrt Q1) then so are P 1 and P 2.

0 and P 1

0 = P 1

1 are orthogonal and set P 2

Implicit in Lemma 1 is that the con-
The proof of Lemma 1 is in the Supplementary Material.
ditional distributions exist. This is not an issue in our setting as the conditional distributions can
be explicitly constructed for elliptical processes  however  for general processes and spaces one
can encounter nontrivial technical problems. We refer the interested reader to Hoffmann-Jørgensen
(1972)  Bogachev (1998  THM A.3.11)  and Kallenberg (2006  Chapter 6) for further discussion.
Interestingly  the reverse statement is not true. That is  even if all of the conditional distributions
are orthogonal  the unconditional measures need not be orthogonal. To see this  suppose that T is
1 and
0 or 1 with equal probability. Now  assume that P 1
0 . Clearly the conditional distributions are orthogonal  but not only are the unconditional
1 = P 1
P 2
measures equivalent  they are actually the same!
Regardless  our goal is more speciﬁc; we want to establish conditions under which E(µ1  C  ψ) and
E(µ2  C  ψ) are orthogonal when they share the same ψ and C. In terms of DP  µ1 and µ2 represent
the private summary from two different databases. If ψ is a point mass  then the two measures are
Gaussian and the conditions are known. The question is  to what degree do such conditions extend
to other mixtures? Theorem 2 shows that the same conditions for Gaussian processes (with the same
covariance  but different means) apply to any elliptical family. Given Corollary 1  this may seem
obvious  but Lemma 1 implies that the matter is surprisingly delicate. For example  two Gaussian
processes with the same mean  but where one has a covariance equal to a scalar c (cid:54)= 1 multiple of
the other  are actually orthogonal (in inﬁnite dimensions). This need not hold for arbitrary elliptical
families as the scalar can be absorbed by the mixing coefﬁcient (and then apply Lemma 1).
Our ﬁrst major result establishes a condition under which DP cannot be achieved  regardless of
the magnitude of the noise. First  let us deﬁne a subspace of X using the bilinear form C (more

5

product (cid:104)· ·(cid:105)K on the dual space X∗ given by (cid:104)f  g(cid:105)K := C(f  g) = (cid:82) f (x)g(x)dP (x)  where P

detail can be found in Bogachev (1998); Mirshani et al. (2017)). In particular  C induces an inner
is a Gaussian measure with mean zero and covariance C. Then  we can view X∗ as a subspace of
L2(X  P )  the space of P -square integrable functions from X → R. By assumption  (cid:104)· ·(cid:105)K is a
continuous  symmetric  and positive deﬁnite bilinear form and thus a valid inner product. However 
X∗ is not complete with respect to this inner product when X is inﬁnite dimensional  so let K denote
its completion. Finally  consider the subset H ⊂ X  such that for h ∈ H the operation Th : K → R
given by Th(g) := g(h) is continuous in the K topology. Then H is called the Cameron-Martin
space of C (or equivalently  of the mean zero Gaussian process with C as its covariance). Intuitively 
the functionals in K are much “rougher" than those in X∗ and thus the elements of H are much more
regular than general elements of X to counter balance this. In fact  C also generates an operator
exactly when it equals h = C(g) for some g ∈ K. The space H is also a Hilbert space (even though
X need not be) equipped with the inner product (cid:104)h1  h2(cid:105)H = (cid:104)g1  g2(cid:105)K where hi = C(gi).
Theorem 2. Let P1 ∼ E(µ1  C  ψ) and P2 ∼ E(µ2  C  ψ) be two elliptical measures over a locally
convex topological vector space  X. Then the two distributions are equivalent if µ1 − µ2 resides in
the Cameron-Martin space of C and orthogonal otherwise.

from K → H denoted as C(g) =(cid:82) xg(x)dP (x). Using this notation  an element h ∈ X lies in H

Proof Sketch. For the ﬁrst direction  if µ1 − µ2 resides in the Cameron-Martin space of C then it
resides in the Cameron-Martin space of vC for v > 0 since they induce equivalent norms. From
Bogachev (1998  Theorem 2.4.5 )  two Gaussian measures with the same covariance  C  are equiv-
alent if the difference of their means resides in the Cameron-Martin space of C. Thus  conditioned
on the mixture V = v  the measures are equivalent for all v. By Lemma 1  they are equivalent.
For the reverse direction we consider  without loss of generality  X1 ∼ E(0  C  ψ) versus X2 ∼
E(µ  C  ψ) where µ is not in the Cameron-Martin space of C. To see that the two measures are
orthogonal  it sufﬁces to show that  for any ﬁxed  ∈ (0  1) we can construct a measurable set A
such that P (X1 ∈ A) ≥ 1 −  while P (X2 ∈ A) ≤ .

To interpret Theorem 2 in the context of privacy  given a database D ∈ D  recall that a private
summary is drawn from the elliptical distribution E(µD  C  ψ). Theorem 2 then says that the mea-
sures are orthogonal (and thus no amount of noise will produce a DP summary) unless all of the
differences µD − µD(cid:48)  for any D  D(cid:48) ∈ D reside in the Cameron-Martin space of C.

4 Achieving DP with Elliptical Perturbations

Now that we have the necessary tools in place and we know when we cannot have DP  we will now
construct a broad class of mechanisms that do achieve DP. Recall that the mechanisms will be of the

form (cid:101)TD = TD + σX  where TD := T (D) is the nonprivate statistical summary  X is a prespeciﬁed

elliptical process and σ > 0 is a ﬁxed scalar. The exact value of σ will be set to achieve some desired
level of privacy. Gaussian perturbations (i.e. taking φ as a point mass) will not achieve -DP even in
ﬁnite dimensions. As is known in the literature  Gaussian perturbations have tails that are too light 
causing the probability inequality of DP to fail for sets in the tails. To ﬁx this  it is common to use
another distribution  often the Laplace distribution  whose tails appear to be just right for achieving
DP. Interestingly  this trick does not carry over to inﬁnite dimensional spaces. We will show that
while some elliptical distributions can achieve -DP for ﬁnite dimensional projections  none can
achieve it over the entire inﬁnite-dimensional space; they can only achieve (  δ)-DP with δ > 0.

4.1 DP in Finite Dimensions

In this subsection  we give a criterion (Theorem 3) that establishes which elliptical distributions
satisfy -DP  when X = Rd. We also provide a related result (Corollary 1) for -DP with d-
dimensional projections of inﬁnite dimensional summaries  which holds uniformly across the choice
of projection  for a ﬁxed d. Elliptical distributions that can achieve -DP (with a ﬁxed d) include (cid:96)2-
mechanism (Chaudhuri and Monteleoni  2009; Chaudhuri et al.  2011; Kifer et al.  2012; Song et al. 
2013; Yu et al.  2014; Awan and Slavkovi´c  2019)  and the multivariate t distribution. Interestingly 
the multivariate Laplace distribution cannot achieve -DP when d ≥ 2.

6

Denote by Σ = {C(ei  ej)} the positive deﬁnite matrix containing the evaluations of C on the

standard basis of Rd. Then the density of(cid:101)TD = TD +σX is proportional to f (σ−2(x−TD)Σ−1(x−
TD))  where f is a decreasing positive function depending only on the dimension d and the elliptical
family for X. The omitted constants depend on Σ  but not on TD. The Cameron-Martin norm can
Theorem 3. Assume that X = Rd and  without loss of generality  assume that that (cid:101)TD has a density
be expressed as (cid:107)g(cid:107)H = g(cid:62)Σ−1g. In fact H = Rd  but equipped with a different norm.
with respect to Lebesgue measure proportional to f(cid:101)TD
f : [0 ∞) → [0 ∞] is a decreasing positive function. Set
(cid:107)TD − TD(cid:48)(cid:107)H = sup
D∼D(cid:48)
f ((c − ∆)2)

(x) ∝ f (σ−2(x−TD)(cid:62)Σ−1(x−TD))  where

If ∆ < ∞  f (0) < ∞  and

(cid:107)Σ−1/2(TD − TD(cid:48))(cid:107)2.

∆ = sup
D∼D(cid:48)

(2)

then (cid:101)TD satisﬁes -DP  where exp() = sup

lim sup
c→∞

< ∞ 

f (c2)

f ((c − σ−2∆)2)

< ∞.

c≥σ−1∆

f (c2)

(cid:17)

i=1 |xi − µi|/σi

(cid:16)−(cid:80)d

this mechanism is proportional

for achieving -DP. The density of

The proof of Theorem 3 is based on the ratio of the densities  and is in the Supplementary Materials.
Next we apply Theorem 3 to several common distributions.
Example 2 (Independent Laplace).
Independent Laplace random variables are a common
to f (x) ∝
tool
. While it is easily proved that this mechanism can be used to sat-
exp
isfy -DP  this distribution is not elliptical  since the density cannot be written as a function of
(x − µ)(cid:62)Σ−1(x − µ) for any µ and Σ.
A natural idea is to use the elliptical multivariate Laplace distribution to try to achieve -DP for
multi-dimensional outputs. Surprisingly  the following example shows that while the tail behavior
of the multivariate Laplace is sufﬁcient to satisfy (2)  the multivariate Laplace distribution cannot be
used to achieve -DP when d ≥ 2  since it has a pole (i.e. goes to inﬁnity) at its center.
Example 3 (Multivariate Laplace). A d-dimensional random variable X ∼ Laplace(µ  Σ) has
density equal to

2(2π)−d/2|Σ|−1/2(cid:0)(x − µ)(cid:62)Σ−1(x − µ)/2(cid:1)ν/2

2(x − µ)(cid:62)Σ−1(x − µ)) 

(cid:113)

where ν = 2−d

tional to f ((x − µ)(cid:62)Σ−1(x − µ))  where f (y) = (y/2)ν/2 Kν((cid:112)2y). The reason this distribution

and Kν is the modiﬁed Bessel function of the second kind. This density is propor-

is called the multivariate Laplace distribution is that it is the only family of distributions such that
every marginal distribution is also distributed as Laplace (iid Laplace does not have this property).
First  let’s check whether (2) is ﬁnite. We use the fact that Kν(z) = c exp(−z)z−1/2(1 + O(1/z))
as z → ∞  where c is a constant (Abramowitz and Stegun  1965  Chapter 9). Then
√
2(c − ∆))
c
c − ∆
2c)

lim
c→∞
We see that the tails of the multivariate Laplace distribution are heavy enough to satisfy -DP. How-
ever  it turns out that there is another problem in this case  which is that f (x) has a pole at x = 0.

We use the fact that for 0 < x (cid:28)(cid:112)|ν| + 1  as x → 0+  Kν(x) is asymptotically similar to

√
2(c − ∆))
√
Kν(
Kν(
2c)

(cid:1)ν
(cid:0) c−∆
(cid:0) c
(cid:1)ν

exp(−√
exp(−√

√
= exp(

f ((c − ∆)2)

= lim
c→∞

= lim
c→∞

f (c2)

√

Kν(

2

2

2

2∆).

From this  we see that the limit is ﬁnite when d = 1  but inﬁnite when d ≥ 2. So  the multivariate
Laplace distribution cannot be used to achieve -DP for d ≥ 2.

7

where γ is a constant (Abramowitz and Stegun  1965  Chapter 9). Then

Kν(x) ∼

(cid:40)− log(x)
Kν((cid:112)2y) ∝ lim

Γ(ν)

2 (2/x)|ν|

y→0+

(cid:16) y

(cid:17)ν/2

if ν = 0
if ν (cid:54)= 0 

exp(−√

y)
− 1
2 log(2y)
(y/2)ν/2( 2√

if d = 1
if d = 2
if d ≥ 3

2y )|ν|

lim
y→0+

f (y) =

2

While we may have supposed that the multivariate Laplace distribution would be well suited for -
DP  in fact it seems that the K-norm mechanism  introduced by Hardt and Talwar (2010)  is a better
generalization of the Laplace mechanism  since it is carefully tuned for privacy.
Example 4 (K-Norm Mechanism). For any norm (cid:107)·(cid:107)K  the K-norm mechanism with mean µ draws
from the density proportional to exp(−(cid:107)x−µ(cid:107)K). For norms of the form (cid:107)x(cid:107) =
x(cid:62)Σ−1x  the K-
norm mechanism is an elliptical distribution  with density is proportional to f ((x−µ)(cid:62)Σ−1(x−µ)) 
where f (y) = exp(−√
For any c ≥ ∆  we have that
exp(−c)
This suggests that this distribution is especially suited for -DP.

y). First note that there is no concern about poles  since f (0) is ﬁnite.

exp(−(cid:112)(c − ∆)2)

= exp(∆)  which is constant.

exp(−(c − ∆))

exp(−

√

√

c2)

=

It is well known in the DP community that Gaussian noise cannot be used to achieve -DP. We show
in the next example how Theorem 3 can be used to easily verify this fact.
Example 5 (Multivariate Normal). The density of a multivariate normal N (µ  Σ) has density pro-
portional to f ((x − µ)(cid:62)Σ−1(x − µ))  where f (y) = exp (−y/2) . If ∆ > 0  then

(cid:16)− (c − ∆)2 /2

(cid:17)

/ exp(cid:0)−c2/2(cid:1) = lim

c→∞ exp

(cid:0)c2 −(cid:2)c2 − 2c∆ + ∆2(cid:3)(cid:1)(cid:19)

= ∞.

(cid:18) 1

2

lim
c→∞ exp

(cid:20)
(cid:0)∆ +(cid:0)√

2

= 1.

(cid:20)

(cid:21)
∆2 + 4ν(cid:1)(cid:1) . Plugging this into

1 + (c − ∆)2/ν

1 + c2/ν

=

The previous result conﬁrms that the tails of the Normal distribution are too light to achieve -DP.
In contrast with the previous example  we show next that the multivariate t-distribution can achieve
-DP  but its tails are maybe “over-kill".
Example 6 (Multivariate t-distribution). A d dimensional t random vector with degrees of freedom
ν > 1  denoted td

ν(µ  Σ) has density proportional to f ((x − µ)(cid:62)Σ−1(x − µ))  where
(cid:21)(ν+d)/2

f (y) = [1 + y/ν]
[1 + (c − ∆)2/ν]−(ν+d)/2

−(ν+d)/2 .

1 + c2/ν

We check the limit:

lim
c→∞

[1 + c2/ν]−(ν+d)/2

= lim
c→∞

1 + (c − ∆)2/ν

Since the limit is ﬁnite  we know that there is a ﬁnite supremum. We solve d
dc
0  and ﬁnd that the unique solution in [∆ ∞) is c = 1

(cid:104)

(cid:105)(ν+d)/2

1+c2/ν

gives us the value of exp().

1+(c−∆)2/ν
We end this subsection with a result for the original inﬁnite dimensional problem: if X is inﬁnite
dimensional  then Theorem 3 can be used to achieve -DP for a set of d linear functionals from K.
Corollary 1. Assume X is an LCS of potentially inﬁnite dimension. Let T : D → X be a summary
with ﬁnite sensitivity ∆ < ∞ with respect to an elliptical noise X ∈ X. Then for any distinct gi ∈ K
µD = {gi(TD)}  Σ = {C(gi  gj)}  and f : [0 ∞) → [0 ∞] is a monotonically decreasing function
depending on d and the elliptical family  but not the speciﬁc gi. If f (0) < ∞  and property (2) of
< ∞.

for i = 1  . . .   d  the density of {gi((cid:101)TD)} is proportional to f (σ−2(x − µD)Σ−1(x − µD))  where
Theorem 3 holds  then {gi((cid:101)TD)} satisﬁes -DP  where exp() = sup
The key point of Corollary 1 is that there is a universal σ such that (cid:101)TD achieves -DP when evaluated

f ((c − σ−2∆)2)

on any d linear functionals. Unfortunately  it does depend on d  and as we will see in the next section 
there is no ﬁnite σ that can guarantee -DP for arbitrary d when using an elliptical perturbation.

c≥σ−1∆

f (c2)

4.2

Impossibility in Inﬁnite Dimensional Spaces

In the previous subsection we gave a condition to check whether an elliptical distribution can be
used to satisfy -DP in ﬁnite dimensional spaces. It is natural to suppose that a similar property
holds in inﬁnite dimensional spaces. However  our main result in this section is that no elliptical
distribution satisﬁes -DP in inﬁnite dimensional spaces. The intuition behind this result is that by

8

Corollary 1  any elliptical process can be expressed as a random mixture of Gaussian processes 
but in inﬁnite dimensional spaces  the mixing variable V is actually measurable with respect to
the inﬁnite dimensional process. That is  if one observes ˜TD = TD + σX  then with probability
one  the mixing random variable V can be computed from ˜TD. This is because one can pool small
amounts of information across an inﬁnite number of dimensions estimate V (even though X still
isn’t observable). So  the noise from any elliptical distribution is equivalent (as far as privacy goes)
to adding noise from a Gaussian process  which Mirshani et al. (2017) show only satisﬁes (  δ)-DP 
a weaker notion of differential privacy than -DP.

Theorem 4. Consider a summary T : D → X and let (cid:101)TD = TD + σX  where X is a centered
singleton  and C does not have ﬁnite rank  then (cid:101)TD will not achieve -DP for any choice of σ.
(cid:80)n
i=1 gi((cid:101)TD)2 converges to V 2 with probability 1 as n → ∞  recovering V from (cid:101)TD.

elliptical distribution and TD := T (D). If X is inﬁnite dimensional  the image T (D) is a not a

Proof Sketch. Consider functionals gi ∈ K such that C(gi  gj) = δij. The estimator Vn =

1
n

Fortunately  elliptical distributions can still achieve (  δ)-DP. However  we run into a bit of an odd
philosophical issue since the mixing coefﬁcient V can be computed from ˜f (D). So  the mechanism
can be viewed as drawing from a mixture of Gaussian processes  but after observing the output the
user knows exactly from which Gaussian distribution the noise came from.
Theorem 5. Let X be a centered elliptical process over X and T : D → X has sensitivity ∆. Then

for any  > 0 and δ > 0 (cid:101)TD = TD + σX 

with

σ2 ≥ 2 log(2/δ(cid:48))

∆2

2

achieves (  δ)-DP  where δ(cid:48) satisﬁes δ = 2MV (log(δ(cid:48)/2)) and MV is the moment generating func-
tion of mixing coefﬁcient V   as deﬁned in Theorem 1.
In Theorem 5  δ(cid:48) represents the DP that would be achieved under the Gaussian mechanism  thus
In addition  for δ(cid:48) ∈ (0  1)  log(δ(cid:48)/2) < 0  so
one will end up with better privacy if δ < δ(cid:48).
MV (log(δ(cid:48)/2)) is ﬁnite and all quantities are well deﬁned. The proof of Theorem 5 is similar to the
proof of Mirshani et al. (2017  Theorem 3.3)  and is in the Supplementary Materials.

5 Discussion

In this work we considered a new class of additive privacy mechanisms based on elliptical distribu-
tions. We also presented a number of foundational results concerning the equivalence/orthogonality
of elliptical distributions. These mechanisms were considered under the general assumption that the
summary resides in a locally convex space  allowing for a wide range of applications from classic
multivariate statistics to nonparametric statistics and functional data analysis. Surprisingly  we show
that while many elliptical distributions may be used for pure DP in ﬁnite dimensions  none are ca-
pable of achieving it in inﬁnite dimensions. This is due to the close connection between Gaussian
processes and elliptical processes  and both can only achieve approximate DP in inﬁnite dimensions.
This work also highlights the need for more tools when the statistical summaries are complex objects
such as functions. Properties that hold in ﬁnite dimensions may not hold in inﬁnite dimensions in
some surprisingly subtle ways. Practically  this can mean that either one does not have the desired
level of protection against privacy disclosures  or that one has to add enormous amounts of noise to
achieve pure DP.
While this paper has focused on the question of whether an elliptical distribution satsﬁes DP  we do
not address the utility of these mechanisms. There is already evidence that elliptical distributions
are useful for different applications (i.e. Awan and Slavkovi´c  2019; Bun and Steinke  2019)  but
further work establishing utility guarantees for elliptical distributions is needed.

References
M. Abramowitz and I. A. Stegun. Handbook of mathematical functions: with formulas  graphs  and

mathematical tables  volume 55. Courier Corporation  1965.

9

F. Alda and B. I. Rubinstein. The Bernstein mechanism: Function release under differential privacy.

In AAAI  pages 1705–1711. 2017.

J. Awan  A. Kenney  M. Reimherr  and A. Slavkovi´c. Beneﬁts and pitfalls of the exponential mech-
anism with applications to hilbert spaces and functional pca. arXiv preprint arXiv:1901.10864 
2019.

J. Awan and A. Slavkovi´c. Differentially private uniformly most powerful tests for binomial data.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors 
Advances in Neural Information Processing Systems 31  pages 4208–4218. Curran Associates 
Inc.  2018.

J. Awan and A. Slavkovi´c. Structure and sensitivity in differential privacy: Comparing k-norm

mechanisms. ArXiv e-prints  2019. Under Review.

P. Billingsley. Probability and measure. New York Wiley  1979.

V. I. Bogachev. Gaussian measures. 62. American Mathematical Soc.  1998.

D. Bosq. Linear Processes in Function Spaces: Theory and Applications. Lecture Notes in Statistics.

Springer New York  2000.

M. Bun and T. Steinke. Average-case averages: Private algorithms for smooth sensitivity and mean

estimation. arXiv preprint arXiv:1906.02830  2019.

K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In D. Koller  D. Schuur-
mans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information Processing Systems 21 
pages 289–296. Curran Associates  Inc.  2009.

K. Chaudhuri  C. Monteleoni  and D. Sarwate. Differentially private empirical risk minimization.

In Journal of Machine Learning Research  volume 12  pages 1069–1109. 2011.

R. Couillet  A. Kammoun  and F. Pascal. Second order statistics of robust estimators of scatter.
application to glrt detection for elliptical signals. Journal of Multivariate Analysis  143:249–274 
2016.

C. Dwork. Differential privacy. In 33rd International Colloquium on Automata  Languages and
Programming  part II (ICALP 2006)  volume 4052  pages 1–12. Springer Verlag  Venice  Italy 
2006.

C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating Noise to Sensitivity in Private Data

Analysis  pages 265–284. Springer Berlin Heidelberg  Berlin  Heidelberg  2006.

C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Found. Trends Theor.

Comput. Sci.  9(3&#8211;4):211–407  2014.

K. W. Fang. Symmetric Multivariate and Related Distributions: 0. Chapman and Hall/CRC  2017.

G. Frahm  M. Junker  and A. Szimayer. Elliptical copulas: applicability and limitations. Statistics

& Probability Letters  63(3):275–286  2003.

M. Gaboardi  H.-W. Lim  R. M. Rogers  and S. P. Vadhan. Differentially private chi-squared hy-
pothesis testing: Goodness of ﬁt and independence testing. In ICML’16 Proceedings of the 33rd
International Conference on International Conference on Machine Learning-Volume 48. JMLR 
2016.

Q. Geng and P. Viswanath. The optimal noise-adding mechanism in differential privacy.

Transactions on Information Theory  62(2):925–951  2016.

IEEE

A. Ghosh  T. Roughgarden  and M. Sundararajan. Universally utility-maximizing privacy mecha-
nisms. In Proceedings of the Forty-ﬁrst Annual ACM Symposium on Theory of Computing  STOC
’09  pages 351–360. ACM  New York  NY  USA  2009.

J. Goes  G. Lerman  and B. Nadler. Robust sparse covariance estimation by thresholding tyler’s

m-estimator. arXiv preprint arXiv:1706.08020  2017.

10

R. Hall  A. Rinaldo  and L. Wasserman. Differential privacy for functions and functional data. The

Journal of Machine Learning Research  14(1):703–727  2013.

M. Hardt and K. Talwar. On the geometry of differential privacy.

In Proceedings of the Forty-
second ACM Symposium on Theory of Computing  STOC ’10  pages 705–714. ACM  New York 
NY  USA  2010.

J. Hoffmann-Jørgensen.
28(2):257–264  1972.

Existence of conditional probabilities. Mathematica Scandinavica 

O. Kallenberg. Foundations of modern probability. Springer Science & Business Media  2006.

V. Karwa  D. Kifer  and A. Slavkovi´c. Private posterior distributions from variational approxima-
tions. NIPS 2015 Workshop on Learning and Privacy with Incomplete Data and Weak Supervi-
sion  2016.

V. Karwa and A. Slavkovi´c.

Inference using noisy degrees: Differentially private β-model and

synthetic graphs. The Annals of Statistics  44(1):87–112  2016.

D. Kifer and A. Machanavajjhala. Pufferﬁsh: A framework for mathematical privacy deﬁnitions.

ACM Transactions on Database Systems (TODS)  39(1):3  2014.

D. Kifer  A. Smith  and A. Thakurta. Private convex empirical risk minimization and high-

dimensional regression. Journal of Machine Learning Research  1:1–41  2012.

F. McSherry and K. Talwar. Mechanism design via differential privacy. In Proceedings of the 48th
Annual IEEE Symposium on Foundations of Computer Science  FOCS ’07  pages 94–103. IEEE
Computer Society  Washington  DC  USA  2007.

A. Mirshani  M. Reimherr  and A. Slavkovic. On the existence of densities for functional data and

their link to statistical privacy. arXiv preprint arXiv:1711.06660  2017.

E. Ollila and E. Raninen. Optimal shrinkage covariance matrix estimation under random sampling

from elliptical distributions. IEEE Transactions on Signal Processing  2019.

W. Rudin. Functional analysis  mcgrawhill. Inc  New York  1991.

R. Schmidt. Credit risk modelling and estimation via elliptical copulae.

267–289. Springer  2003.

In Credit Risk  pages

M. Smith  M. Álvarez  M. Zwiessele  and N. D. Lawrence. Differentially private regression with
gaussian processes. In A. Storkey and F. Perez-Cruz  editors  Proceedings of the Twenty-First
International Conference on Artiﬁcial Intelligence and Statistics  volume 84 of Proceedings of
Machine Learning Research  pages 1195–1203. PMLR  Playa Blanca  Lanzarote  Canary Islands 
2018.

I. Soloveychik and A. Wiesel. Tyler’s covariance matrix estimator in elliptical models with convex

structure. IEEE Transactions on Signal Processing  62(20):5251–5259  2014.

S. Song  K. Chaudhuri  and A. D. Sarwate. Stochastic gradient descent with differentially private
updates. In in Proceedings of the Global Conference on Signal and Information Processing. IEEE 
pages 245–248. 2013.

Y. Sun  P. Babu  and D. P. Palomar. Robust estimation of structured covariance matrix for heavy-
tailed elliptical distributions. IEEE Transactions on Signal Processing  64(14):3576–3590  2016.

D. Vu and A. Slavkovi´c. Differential privacy for clinical trial data: Preliminary evaluations.

In
Proceedings of the 2009 IEEE International Conference on Data Mining Workshops  ICDMW
’09  pages 138–143. IEEE Computer Society  Washington  DC  USA  2009.

Y. Wang  D. Kifer  J. Lee  and V. Karwa. Statistical approximating distributions under differential

privacy. Journal of Privacy and Conﬁdentiality  8(1)  2018.

L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American

Statistical Association  105(489):375–389  2010.

11

O. Williams and F. Mcsherry. Probabilistic inference and differential privacy.

In J. D. Lafferty 
C. K. I. Williams  J. Shawe-Taylor  R. S. Zemel  and A. Culotta  editors  Advances in Neural
Information Processing Systems 23  pages 2451–2459. Curran Associates  Inc.  2010.

F. Yu  M. Rybar  C. Uhler  and S. E. Fienberg. Differentially-private logistic regression for detecting
multiple-snp association in gwas databases. In Privacy in Statistical Databases: UNESCO Chair
in Data Privacy  International Conference  PSD 2014  Ibiza  Spain  September 17-19  2014. Pro-
ceedings  pages 170–184. Springer International Publishing  Cham  2014.

J. Zhang  Z. Zhang  X. Xiao  Y. Yang  and M. Winslett. Functional mechanism: Regression analysis

under differential privacy. Proc. VLDB Endow.  5(11):1364–1375  2012.

12

,Anna Korba
Alexandre Garcia
Florence d'Alché-Buc
Matthew Reimherr
Jordan Awan