2017,Adaptive Clustering through Semidefinite Programming,We analyze the clustering problem through a flexible probabilistic model that aims to identify an optimal partition on the sample X1 ... Xn. We perform exact clustering with high probability using a convex semidefinite estimator that interprets as a corrected  relaxed version of K-means. The estimator is analyzed through a non-asymptotic framework and showed to be optimal or near-optimal in recovering the partition. Furthermore  its performances are shown to be adaptive to the problemâ€™s effective dimension  as well as to K the unknown number of groups in this partition. We illustrate the methodâ€™s performances in comparison to other classical clustering algorithms with numerical experiments on simulated high-dimensional data.,Adaptive Clustering through Semideï¬nite

Programming

Laboratoire de MathÃ©matiques dâ€™Orsay  Univ. Paris-Sud  CNRS 

Martin Royer

UniversitÃ© Paris-Saclay 

91405 Orsay  France

martin.royer@math.u-psud.fr

Abstract

We analyze the clustering problem through a ï¬‚exible probabilistic model that
aims to identify an optimal partition on the sample X1  ...  Xn. We perform
exact clustering with high probability using a convex semideï¬nite estimator that
interprets as a corrected  relaxed version of K-means. The estimator is analyzed
through a non-asymptotic framework and showed to be optimal or near-optimal in
recovering the partition. Furthermore  its performances are shown to be adaptive
to the problemâ€™s effective dimension  as well as to K the unknown number of
groups in this partition. We illustrate the methodâ€™s performances in comparison
to other classical clustering algorithms with numerical experiments on simulated
high-dimensional data.

1

Introduction

Clustering  a form of unsupervised learning  is the classical problem of assembling n observations
X1  ...  Xn from a p-dimensional space into K groups. Applied ï¬elds are craving for robust clus-
tering techniques  such as computational biology with genome classiï¬cation  data mining or image
segmentation from computer vision. But the clustering problem has proven notoriously hard when
the embedding dimension is large compared to the number of observations (see for instance the recent
discussions from [2  21]).
A famous early approach to clustering is to solve for the geometrical estimator K-means [19  13  14].
The intuition behind its objective is that groups are to be determined in a way to minimize the total
intra-group variance. It can be interpreted as an attempt to "best" represent the observations by
K points  a form of vector quantization. Although the method shows great performances when
observations are homoscedastic  K-means is a NP-hard  ad-hoc method. Clustering with probabilistic
frameworks are usually based on maximum likelihood approaches paired with a variant of the EM
algorithm for model estimation  see for instance the works of Fraley & Raftery [11] and Dasgupta
& Schulman [9]. These methods are widespread and popular  but they tend to be very sensitive to
initialization and model misspeciï¬cations.
Several recent developments establish a link between clustering and semideï¬nite programming. Peng
& Wei [17] show that the K-means objective can be relaxed into a convex  semideï¬nite program 
leading Mixon et al. [16] to use this relaxation under a subgaussian mixture model to estimate the
cluster centers. Yan and Sarkar [24] use a similar semideï¬nite program in the context of covariate
clustering  when the network has nodes and covariates. ChrÃ©tien et al. [8] use a slightly different form
of a semideï¬nite program to recover the adjacency matrix of the cluster graph with high probability.
Lastly in the different context of variable clustering  Bunea et al. [6] present a semideï¬nite program
with a correction step to produce non-asymptotic exact recovery results.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this work  we build upon the work and context of [6]  and transpose and adapt their ideas for point
clustering: we introduce a semideï¬nite estimator for point clustering inspired by the ï¬ndings of [17]
with a correction component originally presented in [6]. We show that it produces a very strong
contender for clustering recovery in terms of speed  adaptivity and robustness to model perturbations.
In order to do so we produce a ï¬‚exible probabilistic model inducing an optimal partition of the data
that we aim to recover. Using the same structure of proof in a different context  we establish elements
of stochastic control (see for instance Lemma A.1 on the concentration of random subgaussian Gram
matrices in the supplementary material) to derive conditions of exact clustering recovery with high
probability and show optimal performances â€“ including in high dimensions  improving on [16]  as
well as adaptivity to the effective dimension of the problem. We also show that our results continue
to hold without knowledge of the number of structures given one single positive tuning parameter.
Lastly we provide evidence of our methodâ€™s efï¬ciency and further insight from simulated data.
Notation. Throughout this work we use the convention 0/0 := 0 and [n] = {1  ...  n}. We take
an (cid:46) bn to mean that an is smaller than bn up to an absolute constant factor. Let Sdâˆ’1 denote the
unit sphere in Rd. For q âˆˆ Nâˆ— âˆª {+âˆ}  Î½ âˆˆ Rd  |Î½|q is the lq-norm and for M âˆˆ RdÃ—d(cid:48)
  |M|q 
|M|F and |M|op are respectively the entry-wise lq-norm  the Frobenius norm associated with scalar
product (cid:104).  .(cid:105) and the operator norm. |D|V is the variation semi-norm for a diagonal matrix D  the
difference between its maximum and minimum element. Let A (cid:60) B mean that A âˆ’ B is symmetric 
positive semideï¬nite.

2 Probabilistic modeling of point clustering
Consider X1  ...  Xn and let Î½a = E [Xa]. The variable Xa can be decomposed into

Xa = Î½a + Ea 

a = 1  ...  n 

(1)

with Ea stochastic centered variables in Rp.
Deï¬nition 1. For K > 1  Âµ = (Âµ1  ...  ÂµK) âˆˆ (Rp)K  Î´ (cid:62) 0 and G = {G1  ...  GK} a partition of
[n]  we say X1  ...  Xn are (G  Âµ  Î´)-clustered if âˆ€k âˆˆ [K] âˆ€a âˆˆ Gk |Î½a âˆ’ Âµk|2 (cid:54) Î´. We then call
(2)

|Âµk âˆ’ Âµl|2

âˆ†(Âµ) := min
k<l

the separation between the cluster means  and

Ï(G  Âµ  Î´) := âˆ†(Âµ)/Î´

(3)

the discriminating capacity of (G  Âµ  Î´).
In this work we assume that X1  ...  Xn are (G  Âµ  Î´)-clustered. Notice that this deï¬nition does not
impose any constraint on the data: for any given G  there exists a choice of Âµ  means and radius Î´
important enough so that X1  ...  Xn are (G  Âµ  Î´)-clustered. But we are interested in partitions with
greater discriminating capacity  i.e. that make more sense in terms of group separation. Indeed remark
that if Ï(G  Âµ  Î´) < 2  the population clusters {Î½a}aâˆˆG1  ... {Î½a}aâˆˆGK are not linearly separable 
but a high Ï(G  Âµ  Î´) implies that they are well-separated from each other. Furthermore  we have the
following result.
Proposition 1. Let (Gâˆ—
K  Âµâˆ—  Î´âˆ—) âˆˆ arg max Ï(G  Âµ  Î´) for (G  Âµ  Î´) such that X1  ...  Xn are
(G  Âµ  Î´)-clustered  and |G| = K.
K is the unique maximizer of
Ï(G  Âµ  Î´).
So Gâˆ—
K is the partition maximizing the discriminating capacity over partitions of size K. Therefore
in this work  we will assume that there is a K > 1 such that X1  ...  Xn is (G  Âµ  Î´)-clustered with
|G| = K and Ï(G  Âµ  Î´) > 4. By Proposition 1  G is then identiï¬able. It is the partition we aim to
recover.
We also assume that X1  ...  Xn are independent observations with subgaussian behavior. Instead of
the classical isotropic deï¬nition of a subgaussian random vector (see for example [20])  we use a
more ï¬‚exible deï¬nition that can account for anisotropy.
Deï¬nition 2. Let Y be a random vector in Rd  Y has a subgaussian distribution if there exist
Î£ âˆˆ RdÃ—d such that âˆ€x âˆˆ Rd 

K  Âµâˆ—  Î´âˆ—) > 4 then Gâˆ—

If Ï(Gâˆ—

E(cid:104)
exT (Y âˆ’E Y )(cid:105) (cid:54) exT Î£x/2.

(4)

2

We then call Î£ a variance-bounding matrix of random vector Y   and write shorthand Y âˆ¼ subg(Î£).
Note that Y âˆ¼ subg(Î£) implies Cov(Y ) (cid:52) Î£ in the semideï¬nite sense of the inequality. To sum-up
our modeling assumptions in this work:
Hypothesis 1. Let X1  ...  Xn be independent  subgaussian  (G  Âµ  Î´)-clustered with Ï(G  Âµ  Î´) > 4.
Remark that the modelization of Hypothesis 1 can be connected to another popular probabilistic
model: if we further ask that X1  ...  Xn are identically-distributed within a group (and hence Î´ = 0) 
the model becomes a realization of a mixture model.

3 Exact partition recovery with high probability
Let G = {G1  ...  GK} and m := minkâˆˆ[K] |Gk| denote the minimum cluster size. G can be
represented by its caracteristic matrix Bâˆ— âˆˆ RnÃ—n deï¬ned as âˆ€k  l âˆˆ [K]2 âˆ€(a  b) âˆˆ Gk Ã— Gl 

(cid:26) 1/|Gk|

0

Bâˆ—
ab :=

if k = l
otherwise.

In what follows  we will demonstrate the recovery of G through recovering its caracteristic matrix
Bâˆ—. We introduce the sets of square matrices

: BT = B  tr(B) = K  B1n = 1n  B2 = B}

: BT = B  tr(B) = K  B1n = 1n  B (cid:60) 0}

K

:= {B âˆˆ RnÃ—n

C{0 1}
CK := {B âˆˆ RnÃ—n
C :=

(cid:91)

CK.

+

+

KâˆˆN

(5)
(6)
(7)

We have: C{0 1}
(2007) [17] shows that the K-means estimator Â¯B can be expressed as

âŠ‚ CK âŠ‚ C and CK is convex. Notice that Bâˆ— âˆˆ C{0 1}

K

K . A result by Peng  Wei

for(cid:98)Î› := ((cid:104)Xa  Xb(cid:105))(a b)âˆˆ[n]2 âˆˆ RnÃ—n  the observed Gram matrix. Therefore a natural relaxation is

K

to consider the following estimator:

BâˆˆCK

Notice that E(cid:98)Î› = Î› + Î“ for Î› := ((cid:104)Î½a  Î½b(cid:105))(a b)âˆˆ[n]2 âˆˆ RnÃ—n  and Î“ := E [(cid:104)Ea  Eb(cid:105)](a b)âˆˆ[n]2 =
diag (tr(Var(Ea)))1(cid:54)a(cid:54)n âˆˆ RnÃ—n. The following two results demonstrate that Î› is the signal
structure that lead the optimizations of (8) and (9) to recover Bâˆ—  whereas Î“ is a bias term that can
hurt the process of recovery.
Proposition 2. There exist c0 > 1 absolute constant such that if Ï2(G  Âµ  Î´) > c0(6 +
mâˆ†2(Âµ) > 8|Î“|V   then we have
arg max
BâˆˆC{0 1}

This proposition shows that the (cid:98)B estimator  as well as the K-means estimator  would recover partition

(cid:104)Î› + Î“  B(cid:105) = Bâˆ— = arg max
BâˆˆCK

G on the population Gram matrix if the variation semi-norm of Î“ were sufï¬ciently small compared to
âˆš
the cluster separation. Notice that to recover the partition on the population version  we require the
discriminating capacity to grow as fast as 1 + (
n/m)1/2 instead of simply 1 from Hypothesis 1.
The following proposition demonstrates that if the condition on the variation semi-norm of Î“ is not
met  G may not even be recovered on the population version.
Proposition 3. There exist G  Âµ  Î´ and Î“ such that Ï2(G  Âµ  Î´) = +âˆ but we have mâˆ†2(Âµ) < 2|Î“|V
and

(cid:104)Î› + Î“  B(cid:105).

n/m) and

(10)

âˆš

K

Bâˆ— /âˆˆ arg max
BâˆˆC{0 1}

K

(cid:104)Î› + Î“  B(cid:105)

and Bâˆ— /âˆˆ arg max
BâˆˆCK

(cid:104)Î› + Î“  B(cid:105).

(11)

3

(cid:104)(cid:98)Î›  B(cid:105)

Â¯B = arg max
BâˆˆC{0 1}

(cid:98)B := arg max

(cid:104)(cid:98)Î›  B(cid:105).

(8)

(9)

So Proposition 3 shows that even if the population clusters are perfectly discriminated  there is a
conï¬guration for the variances of the noise that makes it impossible to recover the right clustering by
K-means. This shows that K-means may fail when the random variable homoscedasticity assumption
is violated  and that it is important to correct for Î“ = diag(tr(Var(Ea)))1(cid:54)a(cid:54)n.

Suppose we produce such an estimator(cid:98)Î“corr. Then substracting(cid:98)Î“corr from(cid:98)Î› can be interpreted as a
correcting term  i.e. a way to de-bias(cid:98)Î› as an estimator of Î›. Hence the previous results demonstrate

the interest of studying the following semi-deï¬nite estimator of the projection matrix Bâˆ—  let

(cid:98)Bcorr := arg max

(cid:104)(cid:98)Î› âˆ’(cid:98)Î“corr  B(cid:105).

(12)
In order to demonstrate the recovery of Bâˆ— by this estimator  we introduce different quantitative
measures of the "spread" of our stochastic variables  that affect the quality of the recovery. By
Hypothesis 1 there exist Î£1  ...  Î£n such that âˆ€a âˆˆ [n]  Xa âˆ¼ subg(Î£a). Let

BâˆˆCK

Ïƒ2 := max
aâˆˆ[n]

|Î£a|op  V 2 := max
aâˆˆ[n]

|Î£a|F  

Î³2 := max
aâˆˆ[n]

tr(Î£a)

We now produce(cid:98)Î“corr. Since there is no relation between the variances of the points in our model 
(cid:105)(cid:12)(cid:12) 
(cid:12)(cid:12)(cid:104)Xa âˆ’ Xb  Xcâˆ’Xd
(cid:98)b1 := arg minbâˆˆ[n]\{a} V (a  b) and(cid:98)b2 := arg minbâˆˆ[n]\{a (cid:98)b1} V (a  b). Then for a âˆˆ [n]  let

there is very little hope of estimating Var(Ea). As for our quantity of interest tr(Var(Ea))  a
form of volume  a rough estimation is challenging but possible. The estimator from [6] can be
adapted to our context. For (a  b) âˆˆ [n]2 let V (a  b) := max(c d)âˆˆ([n]\{a b})2

|Xcâˆ’Xd|2

(13)

(14)

(cid:98)Î“corr := diag
|(cid:98)Î“corr âˆ’ Î“|âˆ (cid:54) c7

  Xa âˆ’ X(cid:98)b2

(cid:16)(cid:104)Xa âˆ’ X(cid:98)b1
(cid:17)
(cid:16)
Ïƒ2log n + (Î´ + Ïƒ(cid:112)log n)Î³ + Î´2(cid:17)

(cid:105)aâˆˆ[n]

.

Proposition 4. Assume that m > 2. For c6  c7 > 0 absolute constants  with probability larger than
1 âˆ’ c6/n we have

.

(15)

So apart from the radius Î´ terms  that come from generous model assumptions  a proxy for Î“ is
produced at a Ïƒ2 log n rate that we could not expect to improve on. Nevertheless  this control on Î“ is
key to attain the optimal rates below. It is general and completely independent of the structure of G 
as there is no relation between G and Î“.
We are now ready to introduce this paperâ€™s main result: a condition on the separation between the
cluster means sufï¬cient for ensuring recovery of Bâˆ— with high probability.
Theorem 1. Assume that m > 2. For c1  c2 > 0 absolute constants  if

(cid:0)Ïƒ2(n + m log n) + V 2((cid:112)n + m log n) + Î³(Ïƒ(cid:112)log n + Î´) + Î´2(

n + m)(cid:1) 

mâˆ†2(Âµ) (cid:62) c2

then with probability larger than 1 âˆ’ c1/n we have (cid:98)Bcorr = Bâˆ—  and therefore (cid:98)Gcorr = G.

(16)

âˆš

We call the right hand side of (16) the separating rate. Notice that we can read two kinds of
requirements coming from the separating rate: requirements on the radius Î´  and requirements
on Ïƒ2 V 2  Î³ dependent on the distributions of observations. It appears as if Î´ + Ïƒ
log n can be
interpreted as a geometrical width of our problem. If we ask that Î´ is of the same order as Ïƒ
log n 
a maximum gaussian deviation for n variables  then all conditions on Î´ from (16) can be removed.
Thus for convenience of the following discussion we will now assume Î´ (cid:46) Ïƒ
How optimal is the result from Theorem 1? Notice that our result is adapted to anisotropy in the noise 
but to discuss optimality it is easier to look at the isotropic scenario: V 2 =
pÏƒ2 and Î³2 = pÏƒ2.
Therefore âˆ†2(Âµ)/Ïƒ2 represents a signal-to-noise ratio. For simplicity let us also assume that all
groups have equal size  that is |G1| = ... = |GK| = m so that n = mK and the sufï¬cient condition
(16) becomes

log n.
âˆš

âˆš

âˆš

âˆš

(K + log n)

pK
n

.

(17)

(cid:38)(cid:0)K + log n(cid:1) +

(cid:114)

âˆ†2(Âµ)

Ïƒ2

4

Optimality. To discuss optimality  we distinguish between low and high dimensional setups.
In the low-dimensional setup n âˆ¨ m log n (cid:38) p  we obtain the following condition:

(cid:38)(cid:0)K + log n(cid:1).

âˆ†2(Âµ)

Ïƒ2

(18)

Discriminating with high probability between n observations from two gaussians in dimension 1
would require a separating rate of at least Ïƒ2 log n. This implies that when K (cid:46) log n  our result is
minimax. Otherwise  to our knowledge the best clustering result on approximating mixture center
is from [16]  and on the condition that âˆ†2(Âµ)/Ïƒ2 (cid:38) K 2. Furthermore  the K (cid:38) log n regime is
known in the stochastic-block-model community as a hard regime where a gap is surmised to exist
between the minimal information-theoretic rate and the minimal achievable computational rate (see
for example [7]).
In the high-dimensional setup n âˆ¨ m log n (cid:46) p  condition (17) becomes:

(cid:114)

âˆ†2(Âµ)

(cid:38)

Ïƒ2

(K + log n)

.

(19)

pK
n

There are few information-theoretic bounds for high-dimension clustering. Recently  Banks  Moore 
Vershynin  Verzelen and Xu (2017) [3] proved a lower bound for Gaussian mixture clustering

detection  namely they require a separation of order (cid:112)K(log K)p/n. When K (cid:46) log n  our

condition is only different in that it replaces log(K) by log(n)  a price to pay for going from detecting
the clusters to exactly recovering the clusters. Otherwise when K grows faster than log n there might
exist a gap between the minimal possible rate and the achievable  as discussed previously.
Adaptation to effective dimension. We can analyse further the condition (16) by introducing an
effective dimension râˆ—  measuring the largest volume repartition for our variance-bounding matrices
Î£1  ...  Î£n. We will show that our estimator adapts to this effective dimension. Let

râˆ— :=

Î³2
Ïƒ2 =

maxaâˆˆ[n] tr(Î£a)
maxaâˆˆ[n] |Î£a|op

 

(20)

râˆ— can also be interpreted as a form of global effective rank of matrices Î£a. Indeed  deï¬ne Re(Î£) :=
tr(Î£)/|Î£|op  then we have râˆ— (cid:54) maxaâˆˆ[n] Re(Î£a) (cid:54) maxaâˆˆ[n] rank(Î£a) (cid:54) p.
Now using V 2 (cid:54) âˆš

râˆ—Ïƒ  condition (16) can be written as

râˆ—Ïƒ2 and Î³ =

âˆš

(cid:38)(cid:0)K + log n(cid:1) +

(cid:114)

âˆ†2(Âµ)

Ïƒ2

(K + log n)

râˆ—K
n

.

(21)

By comparing this equation to (17)  notice that râˆ— is in place of p  indeed playing the role of an
effective dimension for the problem. This shows that our estimator adapts to this effective dimension 
without the use of any dimension reduction step. In consequence  equation (21) distinguishes between
an actual high-dimensional setup: nâˆ¨ m log n (cid:46) râˆ— and a "low" dimensional setup râˆ— (cid:46) nâˆ¨ m log n
under which  regardless of the actual value of p  our estimators recovers under the near-minimax
condition of (18).

This informs on the effect of correcting term(cid:98)Î“corr in the theorem above when n + m log n (cid:46) râˆ—.
Ïƒ2râˆ—/m  but with the(cid:98)Î“corr correction on the other hand  (21) has leading separating factor smaller
than Ïƒ2(cid:112)(K + log n)râˆ—/m = Ïƒ2âˆš
setup  our correction enhances the separating rate of at least a factor(cid:112)(n + m log n)/râˆ—.

The un-corrected version of the semi-deï¬nite program (9) has a leading separating rate of Î³2/m =

râˆ—/m. This proves that in a high-dimensional

n + m log n Ã— âˆš

4 Adaptation to the unknown number of group K

It is rarely the case that K is known  but we can proceed without it. We produce an estimator adaptive

to the number of groups K: let(cid:98)Îº âˆˆ R+  we now study the following adaptive estimator:

(cid:101)Bcorr := arg max

(cid:104)(cid:98)Î› âˆ’(cid:98)Î“corr  B(cid:105) âˆ’(cid:98)Îº tr(B).

BâˆˆC

(22)

5

c4

(cid:16)V 2âˆš

Theorem 2. Suppose that m > 2 and (16) is satisï¬ed. For c3  c4  c5 > 0 absolute constants suppose

that the following condition on(cid:98)Îº is satisï¬ed
(cid:17)
n + Ïƒ2n + Î³(Ïƒ(cid:112)log n + Î´) + Î´2âˆš
then we have (cid:101)Bcorr = Bâˆ— with probability larger than 1 âˆ’ c3/n
Notice that condition (23) essentially requires(cid:98)Îº to be seated between mâˆ†2(Âµ) and some components
adaptive estimator (cid:101)Bcorr as well and this shows that it is not necessary to know K in order to
perform well for recovering G. Finding an optimized  data-driven parameter(cid:98)Îº using some form of

of the right-hand side of (16). So under (23)  the results from the previous section apply to the

< c5(cid:98)Îº < mâˆ†2(Âµ) 

n

(23)

cross-validation is outside of the scope of this paper.

5 Numerical experiments

We illustrate our method on simulated Gaussian data in two challenging  high-dimensional setup
experiments for comparing clustering estimators. Our sample of n = 100 points are drawn from
K = 5 identically-sized  perfectly discriminated non-isovolumic clusters of Gaussians - that is we
have âˆ€k âˆˆ [K] âˆ€a âˆˆ Gk  Ea âˆ¼ N (0  Î£k) such that |G1| = ... = |GK| = 20. The distributions are
chosen to be isotropic  and the ratio between the lowest and the highest standard deviation is of 1 to 10.
We draw points of a Rp space in two different scenarii. In (S1)  for a given dimension space p = 500
and a ï¬xed isotropic noise level  we report the algorithmâ€™s performances as the signal-to-noise ratio
âˆ†2(Âµ)/Ïƒ2 is increased from 1 to 15. In (S2) we impose a ï¬xed signal to noise ratio and observe the
algorithmâ€™s decay in performance as the space dimension p is increased from 102 to 105 (logarithmic
scale). All reported points of the simulated space represent a hundred simulations  and indicate a
median value with asymmetric standard deviations in the form of errorbars.

Solving for estimator (cid:98)Bcorr is a hard problem as n grows. For this task we implemented an ADMM
we compare the recovering capacities of (cid:98)Gcorr  labeled â€™pecokâ€™ in Figure 1 with other classical

solver from the work of Boyd et al. [4] with multiple stopping criterions including a ï¬xed number of
iterations of T = 1000. The complexity of the optimization is then roughly O(T n3). For reference 

instance [22]) between the truth and its estimate. In the two experiments  the results of (cid:98)Gcorr are

clustering algorithm. We chose three different but standard clustering procedures: Lloydâ€™s K-means
algorithm [13] with a thousand K-means++ initialization of [1] (although in scenario (S2)  the
algorithm is too slow to converge as p grows so we do not report it)  Wardâ€™s method for Hierarchical
Clustering [23] and the low-rank clustering algorithm applied to the Gram matrix  a spectral method
appearing in McSherry [15]. Lastly we include the CORD algorithm from Bunea et al. [5].
We measure the performances of estimators by computing the adjusted mutual information (see for
markedly better than that of other methods. Scenario (S1) shows it can achieve exact recovery with a
lesser signal to noise ratio than its competitors  whereas scenario (S2) shows its performances start to
decay much later than the other methods as the space dimension is increased exponentially.
Table 1 summarizes the simulations in a different light: for different parameter value on each line  we
count the number of experiments (out of a hundred) that had an adjusted mutual information score
equal to 0.9 or higher. This accounts for exact recoveries  or approximate recoveries that reasonably

reï¬‚ected the underlying truth. In this table it is also evident that (cid:98)Gcorr performs uniformly better  be

it for exact or approximate recovery: it manages to recover the underlying truth much sooner in terms
of signal-to-noise ratio  and for a given signal-to-noise ratio it will represent the truth better as the
embedding dimension increases.
Lastly Table 1 provides the median computing time in seconds for each method over the entire

experiment. (cid:98)Gcorr comes with important computation times because(cid:98)Î“corr is very costly to compute.

Our method is computationally intensive but it is of polynomial order. The solving of a semideï¬nite
program is a vastly developing ï¬eld of Operational Research and even though we used the classical
ADMM method of [4] that proved effective  this instance of the program could certainly have seen a
more proï¬table implementation in the hands of a domain expert. All of the compared methods have a
very hard time reaching high sample sizes n in the high dimensional context.
The PYTHON3 implementation of the method used is found in open access here: martinroyer/pecok
[18]

6

Scenario (S1)

Figure 1: Performance comparison for clustering estimators and (cid:98)Gcorr  labeled â€™pecok4â€™ in reference

to [6]. The adjusted mutual information equals 1 when the clusterings are identical  0 when they are
independent.

Scenario (S2)

hierarchical

kmeans++

lowrank-spectral

pecok4

cord

(S1)

(S2)

90% SNR=4.75
90% SNR=6
90% SNR=7.25
90% SNR=8.5
med time (s)
90% dim=102
90% dim=103
90% dim=5.103
90% dim=104
med time (s)

0
0
18
100
0.01
100
0
0
0

0.14

0
0
0
0

2.76

/
/
/
/
âˆ

0
0
12
100
0.23
100
0
0
0

51
100
100
100

1.84 (+18.92)1

100
100
100
49

0
0
26
76
0.76
94
31
0
0

0.19

0.68
Table 1: Approximate recovery result for experiment (S1) and (S2): number of experiments that had
a score superior to 90%  out of a hundred  and computing times over the experiments
1The median time in parenthesis is the time to compute(cid:98)Î“corr  as opposed to the main time for performing
the SDP. Indeed the(cid:98)Î“corr is very time consuming  its cost is roughly O(n4p). It must be noted that much

faster alternatives  such as the one presented in [6]  perform equally well (there is no signiï¬cant difference in
performance) for the recovery of G  but this is outside the scope of this paper.

1.94 (+68.12)1

6 Conclusion

In this paper we analyzed a new semideï¬nite positive algorithm for point clustering within the context
of a ï¬‚exible probabilistic model and exhibit the key quantities that guarantee non-asymptotic exact
recovery. It implies an essential bias-removing correction that signiï¬canty improves the recovering
rate in the high-dimensional setup. Hence we showed the estimator to be near-minimax  adapted to an
effective dimension of the problem. We also demonstrated that our estimator can be optimally adapted
to a data-driven choice of K  with a single tuning parameter. Lastly we illustrated on high-dimensional
experiments that our approach is empirically stronger than other classical clustering methods. The

(cid:98)Î“corr correction step of the algorithm  it can be interpreted as an independent  denoising step for

the Gram matrix  and we recommend using such a procedure where the probabilistic framework we
developed seems appropriate.

7

$#âˆ†2(Âµ)/Ïƒ2 /*2G cG20 3850.447 3
850.97 07 7.. .47/5 /*2G cG50.447 3
850.97 07 7.. .47/In practice  it is generally more realistic to look at approximate clustering results  but in this work we
chose the point of view of exact clustering for investigating theoretical properties of our estimator.
Our experimental results provide evidence that this choice is not restrictive  i.e. that our ï¬ndings
translate very well to approximate recovery. We expect our results to hold with similar speeds
for approximate clustering  up to some logarithmic terms. One could think of adapting works on
community detection by GuÃ©don and Vershynin [12] based on Grothendieckâ€™s inequality  or work by
Fei and Chen [10] from the stochastic-block-model community on similar semideï¬nite programs. In
fact  referring to a detection bound by Banks  Moore  Vershynin  Verzelen and Xu (2017) [3]  our
âˆš
only margin for improvement on the separation speed is to transform the logarithmic factor
log n
log K when the number of clusters K is of order O(log n) â€“ otherwise the problem is rather
into
open.
As for the robustness of this procedure  a few aspects are to be considered: the algorithm we studied
solves for a convexiï¬ed objective  therefore its performances are empirically more stable than that of
an objective that would prove non-convex  especially in the high-dimensional context. In this work
we also beneï¬t from a permissive probabilistic framework that allows for multiple deviations from the
classical gaussian cluster model  and come at no price in terms of the performance of our estimator.
Points from a same cluster are allowed to have signiï¬cantly different means or ï¬‚uctuations  and the
results for exact recovery with high probability are unchanged  near-minimax and adaptive. Likewise
on simulated data the estimator proves the most efï¬cient in exact as well as approximate recovery.

âˆš

Acknowledgements

This work is supported by a public grant overseen by the French National research Agency (ANR) as
part of the â€œInvestissement dâ€™Avenir" program  through the â€œIDI 2015" project funded by the IDEX
Paris-Saclay  ANR-11-IDEX-0003-02. It is also supported by the CNRS PICS funding HighClust.
We thank Christophe Giraud for a shrewd  unwavering thesis direction.

References
[1] D. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings
of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms  SODA â€™07  pages
1027â€“1035  Philadelphia  PA  USA  2007. Society for Industrial and Applied Mathematics.

[2] M. Azizyan  A. Singh  and L. Wasserman. Minimax theory for high-dimensional gaussian
mixtures with sparse mean separation. In Proceedings of the 26th International Conference
on Neural Information Processing Systems  NIPSâ€™13  pages 2139â€“2147  USA  2013. Curran
Associates Inc.

[3] J. Banks  C. Moore  N. Verzelen  R. Vershynin  and J. Xu.

Information-theoretic bounds
and phase transitions in clustering  sparse PCA  and submatrix localization. arXiv e-prints
arXiv:1607.05222  July 2016.

[4] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Found. Trends Mach. Learn. 
3(1):1â€“122  January 2011.

[5] F. Bunea  C. Giraud  and X. Luo. Minimax optimal variable clustering in g-models via cord.

arXiv preprint arXiv:1508.01939  2015.

[6] F. Bunea  C. Giraud  M. Royer  and N. Verzelen. PECOK: a convex optimization approach to

variable clustering. arXiv e-prints arXiv:1606.05100  June 2016.

[7] Y. Chen and J. Xu. Statistical-computational tradeoffs in planted problems and submatrix
localization with a growing number of clusters and submatrices. Journal of Machine Learning
Research  17(27):1â€“57  2016.

[8] S. ChrÃ©tien  C. Dombry  and A. Faivre. A semi-deï¬nite programming approach to low dimen-

sional embedding for unsupervised clustering. CoRR  abs/1606.09190  2016.

[9] S. Dasgupta and L. Schulman. A probabilistic analysis of em for mixtures of separated  spherical

gaussians. J. Mach. Learn. Res.  8:203â€“226  May 2007.

8

[10] Y. Fei and Y. Chen. Exponential error rates of SDP for block models: Beyond Grothendieckâ€™s

inequality. arXiv e-prints arXiv:1705.08391  May 2017.

[11] C. Fraley and A. E. Raftery. Model-based clustering  discriminant analysis  and density

estimation. Journal of the American Statistical Association  97(458):611â€“631  2002.

[12] O. GuÃ©don and R. Vershynin. Community detection in sparse networks via grothendieckâ€™s

inequality. CoRR  abs/1411.4686  2014.

[13] S. Lloyd. Least squares quantization in pcm. IEEE Trans. Inf. Theor.  28(2):129â€“137  September

1982.

[14] J. MacQueen. Some methods for classiï¬cation and analysis of multivariate observations.
In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability 
Volume 1: Statistics  pages 281â€“297  Berkeley  Calif.  1967. University of California Press.

[15] F. McSherry. Spectral partitioning of random graphs.

In Proceedings of the 42Nd IEEE
Symposium on Foundations of Computer Science  FOCS â€™01  pages 529â€“  Washington  DC 
USA  2001. IEEE Computer Society.

[16] D. G. Mixon  S. Villar  and R. Ward. Clustering subgaussian mixtures with k-means. In 2016

IEEE Information Theory Workshop (ITW)  pages 211â€“215  Sept 2016.

[17] J. Peng and Y. Wei. Approximating k-means-type clustering via semideï¬nite programming.

SIAM J. on Optimization  18(1):186â€“205  February 2007.

[18] Martin Royer. ADMM implementation of PECOK. https://github.com/martinroyer/

pecok  October  2017.

[19] H. Steinhaus. Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci  1:801â€“804 

1956.

[20] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Chapter 5 of:

Compressed Sensing  Theory and Applications. Cambridge University Press  2012.

[21] N. Verzelen and E. Arias-Castro. Detection and Feature Selection in Sparse Mixture Models.

arXiv e-prints arXiv:1405.1478  May 2014.

[22] Nguyen Xuan Vinh  Julien Epps  and James Bailey. Information theoretic measures for cluster-
ings comparison: Variants  properties  normalization and correction for chance. J. Mach. Learn.
Res.  11:2837â€“2854  December 2010.

[23] J. H. Ward. Hierarchical grouping to optimize an objective function. Journal of the American

Statistical Association  58(301):236â€“244  1963.

[24] B. Yan and P. Sarkar. Convex Relaxation for Community Detection with Covariates. arXiv

e-prints arXiv:1607.02675  July 2016.

9

,Martin Royer