2018,Unsupervised Learning of Object Landmarks through Conditional Image Generation,We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image  where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry  we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems  which often use generative adversarial networks  our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous  to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos  all without manual supervision  while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces  people  3D objects  and digits - without any modifications.,Unsupervised Learning of Object Landmarks

through Conditional Image Generation

Tomas Jakab1∗

Ankush Gupta1∗

Hakan Bilen2

Andrea Vedaldi1

1 Visual Geometry Group

University of Oxford

{tomj ankush vedaldi}@robots.ox.ac.uk

2 School of Informatics
University of Edinburgh

hbilen@ed.ac.uk

Abstract

We propose a method for learning landmark detectors for visual objects (such as
the eyes and the nose in a face) without any manual supervision. We cast this as the
problem of generating images that combine the appearance of the object as seen in
a ﬁrst example image with the geometry of the object as seen in a second example
image  where the two examples differ by a viewpoint change and/or an object
deformation. In order to factorize appearance and geometry  we introduce a tight
bottleneck in the geometry-extraction process that selects and distils geometry-
related features. Compared to standard image generation problems  which often
use generative adversarial networks  our generation task is conditioned on both
appearance and geometry and thus is signiﬁcantly less ambiguous  to the point
that adopting a simple perceptual loss formulation is sufﬁcient. We demonstrate
that our approach can learn object landmarks from synthetic image deformations
or videos  all without manual supervision  while outperforming state-of-the-art
unsupervised landmark detectors. We further show that our method is applicable to
a large variety of datasets — faces  people  3D objects  and digits — without any
modiﬁcations.

1

Introduction

There is a growing interest in developing machine learning methods that have little or no dependence
on manual supervision. In this paper  we consider in particular the problem of learning  without
external annotations  detectors for the landmarks of object categories  such as the nose  the eyes  and
the mouth of a face  or the hands  shoulders  and head of a human body.
Our approach learns landmarks by looking at images of deformable objects that differ by acquisition
time and/or viewpoint. Such pairs may be extracted from video sequences or can be generated by
randomly perturbing still images. Videos have been used before for self-supervision  often in the
context of future frame prediction  where the goal is to generate future video frames by observing
one or more past frames. A key difﬁculty in such approaches is the high degree of ambiguity that
exists in predicting the motion of objects from past observations. In order to eliminate this ambiguity 
we propose instead to condition generation on two images  a source (past) image and a target (future)
image. The goal of the learned model is to reproduce the target image  given the source and target
images as input. Clearly  without further constraints  this task is trivial. Thus  we pass the target
through a tight bottleneck meant to distil the geometry of the object (ﬁg. 1). We do so by constraining
the resulting representation to encode spatial locations  as may be obtained by an object landmark
detector. The source image and the encoded target image are then passed to a generator network
which reconstructs the target. Minimising the reconstruction error encourages the model to learn
landmark-like representations because landmarks can be used to encode the geometry of the object 

∗equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Model Architecture. Given a pair of source and target images (x  x(cid:48))  the pose-regressor Φ
extracts K heatmaps from x(cid:48)  which are then marginalized to estimate coordinates of keypoints  to
limit the information ﬂow. 2D Gaussians (y(cid:48)) are rendered from these keypoints and stacked along
with the image features extracted from x  to reconstruct the target as Ψ(x  y(cid:48)) = ˆx(cid:48). By restricting
the information-ﬂow our model learns semantically meaningful keypoints  without any annotations.

which changes between source and target  while the appearance of the object  which is constant  can
be obtained from the source image alone.
The key advantage of our method  compared to other works for unsupervised learning of landmarks 
is the simplicity and generality of the formulation  which allows it to work well on data far more
complex than previously used in unsupervised learning of object landmarks  e.g. landmarks for the
highly-articulated human body. In particular  unlike methods such as [45  44  55]  we show that our
method can learn from synthetically-generated image deformations as well as raw videos as it does
not require access to information about correspondences  optical-ﬂow  or transformation between
images.
Furthermore  while image generation has been used extensively in unsupervised learning  especially
in the context of (variational) auto-encoders [22] and Generative Adversarial Networks (GANs [13];
see section 2)  our approach has a key advantage over such methods. Namely  conditioning on
both source and target images simpliﬁes the generation task considerably  making it much easier
to learn the generator network [18]. The ensuing simpliﬁcation means that we can adopt the direct
approach of minimizing a perceptual loss as in [10]  without resorting to more complex techniques
like GANs. Empirically  we show that this still results in excellent image generation results and
that  more importantly  semantically consistent landmark detectors are learned without manual
supervision (section 4). Project code and details are available at: http://www.robots.ox.ac.uk/
~vgg/research/unsupervised_landmarks/

2 Related work

The recent approaches of [45  44] learn to extract landmarks based on the principles of equivariance
and distinctiveness.
In contrast to our work  these methods are not generative. Further  they
rely on known correspondences between images obtained either through optical ﬂow or synthetic
transformations  and hence  cannot leverage video data directly. Since the principle of equivariance is
orthogonal to our approach  it can be incorporated as an additional cue in our method.
Unsupervised learning of representations has traditionally been achieved using auto-encoders and
restricted Boltzmann machines [14  47  15]. InfoGAN [6] uses GANs to disentangle factors in the
data by imposing a certain structure in the latent space. Our approach also works by imposing a latent
structure  but using a conditional-encoder instead of an auto-encoder.
Learning representations using conditional image generation via a bottleneck was demonstrated
by Xue et al. [52] in variational auto-encoders  and by Whitney et al. [50] using a discrete gating
mechanism to combine representations of successive video frames. Denton et al. [8] factor the pose
and identity in videos through an adversarial loss on the pose embeddings. We instead design our
bottleneck to explicitly shape the features to resemble the output of a landmark detector  without any
adversarial training. Villegas et al. [46] also generate future frames by extracting a representation of
appearance and human pose  but  differently from us  require ground-truth pose annotations. Our
method essentially inverts their analogy network [36] to output landmarks given the source and target
image pairs.

2

-1+1-1Several other generative methods [42  40  37  48  32] focus on video extrapolation. Srivastava et
al. [40] employ Long Short Term Memory (LSTM) [16] networks to encode video sequences into
ﬁxed-length representation and decode it to reconstruct the input sequence. Vondrick et al. [48]
propose a GAN for videos  also with a spatio-temporal convolutional architecture that disentangles
foreground and background to generate realistic frames. Video Pixel Networks [20] estimate the
discrete joint distribution of the pixel values in a video by encoding different modalities such as time 
space and colour information. In contrast  we learn a structured embedding that explicitly encodes
the spatial location of object landmarks.
A series of concurrent works propose similar methods for unsupervised learning of object structure.
Shu et al. [38] learn to factor a single object-category-speciﬁc image into an appearance template in a
canonical coordinate system  and a deformation ﬁeld which warps the template to reconstruct the input 
as in an auto-encoder. They encourage this factorisation by controlling the size of the embeddings.
Similarly  Wiles et al. [51] learn a dense deformation ﬁeld for faces but obtain the template from a
second related image  as in our method. Suwajanakorn et al. [43] learn 3D-keypoints for objects
from two images which differ by a known 3D transformation  by enforcing equivariance [45]. Finally 
the method of Zhang et al. [55] shares several similarities with ours  in that they also use image
generation with the goal of learning landmarks. However  their method is based on generating a
single image from itself using landmark-transported features. This  we show is insufﬁcient to learn
geometry and requires  as they do  to also incorporate the principle of equivariance [45]. This is a key
difference with our method  as ours results in a much simpler system that does not require to know
the optical-ﬂow/correspondences between images  and can learn from raw videos directly.

3 Method
Let x  x(cid:48) ∈ X = RH×W×C be two images of an object  for example extracted as frames in a video
sequence  or synthetically generated by randomly deforming x into x(cid:48). We call x the source image
and x(cid:48) the target image and we use Ω to denote the image domain  namely the H×W lattice.
We are interested in learning a function Φ(x) = y ∈ Y that captures the “structure” of the object in
the image as a set of K object landmarks. As a ﬁrst approximation  assume that y = (u1  . . .   uK) ∈
ΩK = Y are K coordinates uk ∈ Ω  one per landmark.
In order to learn the map Φ in an unsupervised manner  we consider the problem of conditional image
generation. Namely  we wish to learn a generator function

Ψ : X × Y → X  

(x  y(cid:48)) (cid:55)→ x(cid:48)

such that the target image x(cid:48) = Ψ(x  Φ(x(cid:48))) is reconstructed from the source image x and the
representation y(cid:48) = Φ(x(cid:48)) of the target image. In practice  we learn both functions Φ and Ψ jointly
to minimise the expected reconstruction loss minΨ Φ Ex x(cid:48) [L(x(cid:48)  Ψ(x  Φ(x(cid:48))))] . Note that  if we
do not restrict the form of Y  then a trivial solution to this problem is to learn identity mappings
by setting y(cid:48) = Φ(x(cid:48)) = x(cid:48) and Ψ(x  y(cid:48)) = y(cid:48). However  given that y(cid:48) has the “form” of a set of
landmark detections  the model is strongly encouraged to learn those. This is explained next.

3.1 Heatmaps bottleneck

In order for the model Φ(x) to learn to extract keypoint-like structures from the image  we terminate
the network Φ with a layer that forces the output to be akin to a set of K keypoint detections. This
is done in three steps. First  K heatmaps Su(x; k)  u ∈ Ω are generated  one for each keypoint
k = 1  . . .   K. These heatmaps are obtained in parallel as the channels of a RH×W×K tensor using
a standard convolutional neural network architecture. Second  each heatmap is renormalised to a
probability distribution via (spatial) Softmax and condensed to a point by computing the (spatial)
expected value of the latter:

Third  each heatmap is replaced with a Gaussian-like function centred at u∗
standard deviation σ:

k with a small ﬁxed

(cid:19)

(1)

(2)

u∗
k(x) =

u∈Ω ueSu(x;k)
u∈Ω eSu(x;k)

Φu(x; k) = exp

− 1
2σ2(cid:107)u − u∗

k(x)(cid:107)2

(cid:80)
(cid:80)
(cid:18)

3

x

x(cid:48)

Ψ(x  Φ(x(cid:48)))

Φ(x(cid:48))

Figure 2: Unsupervised Landmarks. [left]: CelebA images showing the synthetically transformed
source x and target x(cid:48) images  the reconstructed target Ψ(x  Φ(x(cid:48)))  and the unsupervised landmarks
Φ(x(cid:48)). [middle]: The same for video frames from VoxCeleb. [right]: Two example images with
selected (8 out of 10) landmarks uk overlaid and their corresponding 2D score maps Su(x; k)
(see section 3.1; brighter pixels indicate higher conﬁdence).
The end result is a new tensor y = Φ(x) ∈ RH×W×K that encodes as Gaussian heatmaps the
location of K maxima. Since it is possible to recover the landmark locations exactly from these
heatmaps  this representation is equivalent to the one considered above (2D coordinates); however  it
is more useful as an input to a generator network  as discussed later.
One may wonder whether this construction can be simpliﬁed by removing steps two and three and
simply consider S(x) (possibly after re-normalisation) as the output of the encoder Φ(x). The answer
is that these steps  and especially eq. (1)  ensure that very little information from x is retained  which 
as suggested above  is key to avoid degenerate solutions. Converting back to Gaussian landmarks
in eq. (2)  instead of just retaining 2D coordinates  ensures that the representation is still utilisable by
the generator network.
Separable implementation.
In practice  we consider a separable variant of eq. (1) for computa-
tional efﬁciency. Namely  let u = (u1  u2) be the two components of each pixel coordinate and write
Ω = Ω1 × Ω2. Then we set

 

Sui(x; k) =

S(u1 u2)(x; k) 

(cid:88)

uj∈Ωj

(cid:80)
(cid:80)

u∗
ik(x) =

ui∈Ωi
ui∈Ωi

uieSui (x;k)
eSui (x;k)

where i = 1  2 and j = 2  1 respectively. Figure 2 visualizes the source x  target x(cid:48) and generated
Ψ(x  Φ(x(cid:48))) images  as well as x(cid:48) overlaid with the locations of the unsupervised landmarks Φ(x(cid:48)).
It also shows the heatmaps Su(x; k) and marginalized separable softmax distributions on the top and
left of each heatmap for K = 10 keypoints.

3.2 Generator network using a perceptual loss
The goal of the generator network ˆx(cid:48) = Ψ(x  y(cid:48)) is to map the source image x and the distilled
version y(cid:48) of the target image x(cid:48) to a reconstruction of the latter. Thus the generator network is
optimised to minimise a reconstruction error L(x(cid:48)  ˆx(cid:48)). The design of the reconstruction error is
important for good performance. Nowadays the standard practice is to learn such a loss function
using adversarial techniques  as exempliﬁed in numerous variants of GANs. However  since the goal
here is not generative modelling  but rather to induce a representation y(cid:48) of the object geometry for
reconstructing a speciﬁc target image (as in an auto-encoder)  a simpler method may sufﬁce.
Inspired by the excellent results for photo-realistic image synthesis of [4]  we resort here to use the
“content representation” or “perceptual” loss used successfully for various generative networks [12  1 
9  19  27  30  31]. The perceptual loss compares a set of the activations extracted from multiple layers
of a deep network for both the reference and the generated images  instead of the only raw pixel
2  where Γ(x) is an off-the-shelf
pre-trained neural network  for example VGG-19 [39]  Γl denotes the output of the l-th sub-network
(obtained by chopping Γ at layer l). As our goal is to have a purely-unsupervised learning  we
pre-train the network by using a self-supervised approach  namely colorising grayscale images [25].

values. We deﬁne the loss as L(x(cid:48)  ˆx(cid:48)) =(cid:80)

l αl(cid:107)Γl(x(cid:48)) − Γl(ˆx(cid:48))(cid:107)2

4

n supervised Thewlis [45] Ours selfsup
12.89 ± 3.21
8.16 ± 0.96
7.19 ± 0.45
4.29 ± 0.34
2.83 ± 0.06
2.73 ± 0.03
2.60 ± 0.00
2.58± N/A

1
5
† 10
100
500
1000
5000
All (19 000)

10.82
9.25
8.49
—
—
—
—
7.15

Figure 3: Sample Efﬁciency for Supervised Regression on MAFL. [left]: Supervised linear
regression of 5 keypoints (bottom-row) from 10 unsupervised (top-row) on MAFL test set. Centre
of the white-dots correspond to the ground-truth location  while the dark ones are the predictions.
Both unsupervised and supervised landmarks show a good degree of equivariance with respect to
head rotation (columns 2  4) and invariance to headwear or eyewear (columns 1  3). [right]: MSE
(±σ) (normalised by inter-ocular distance (in %)) on the MAFL test-set for varying number (n) of
supervised samples from MAFL training set used for learning the regressor from 30 unsupervised
landmarks. †: we outperform the previous state-of-the-art [45] with only 10 labelled examples.
We also test using a VGG-19 model pre-trained for image classiﬁcation in ImageNet. All other
networks are trained from scratch. The parameters αl > 0  l = 1  . . .   n are scalars that balance the
terms. We use a linear combination of the reconstruction error for ‘input’  ‘conv1_2’  ‘conv2_2’ 
‘conv3_2’  ‘conv4_2’ and ‘conv5_2’ layers of VGG-19; {αl} are updated online during training to
normalise the expected contribution from each layer as in [4]. However  we use the (cid:96)2 norm instead
of their (cid:96)1  as it worked better for us.

4 Experiments

In section 4.1 we provide the details of the landmark detection and generator networks; a common
architecture is used across all datasets. Next  we evaluate landmark detection accuracy on faces
(section 4.2) and human-body (section 4.3). In section 4.4 we analyse the invariance of the learned
landmarks to various nuisance factors  and ﬁnally in section 4.5 study the factorised representation of
object style and geometry in the generator.

4.1 Model details
Landmark detection network. The landmark detector ingests the image x(cid:48) to produce K landmark
heatmaps y(cid:48). It is composed of sequential blocks consisting of two convolutional layers each. All
the layers use 3×3 ﬁlters  except the ﬁrst one which uses 7×7. Each block doubles the number
of feature channels in the previous block  with 32 channels in the ﬁrst one. The ﬁrst layer in each
block  except the ﬁrst block  downsamples the input tensor using stride 2 convolution. The spatial
size of the ﬁnal output  outputting the heatmaps  is set to 16×16. Thus  due to downsampling  for a
network with n − 3  n ≥ 4 blocks  the resolution of the input image is H×W = 2n×2n  resulting in
16×16×(32 · 2n−3) tensor. A ﬁnal 1×1 convolutional layer maps this tensor to a 16×16×K tensor 
with one layer per landmark. As described in section 3.1  these K feature channels are then used to
render 16×16×K 2D-Gaussian maps y(cid:48) (with σ = 0.1).
Image generation network. The image generator takes as input the image x and the landmarks
y(cid:48) = Φ(x(cid:48)) extracted from the second image in order to reconstruct the latter. This is achieved in
two steps: ﬁrst  the image x is encoded as a feature tensor z ∈ R16×16×C using a convolutional
network with exactly the same architecture as the landmark detection network except for the ﬁnal
1×1 convolutional layer  which is omitted; next  the features z and the landmarks y(cid:48) are stacked
together (along the channel dimension) and fed to a regressor that reconstructs the target frame x(cid:48).
The regressor also comprises of sequential blocks with two convolutional layers each. The input to
each successive block  except the ﬁrst one  is upsampled two times through bilinear interpolation 
while the number of feature channels is halved; the ﬁrst block starts with 256 channels  and a
minimum of 32 channels are maintained till a tensor with the same spatial dimensions as x(cid:48) is
obtained. A ﬁnal convolutional layer regresses the three RGB channels with no non-linearity. All

5

BBC Pose Accuracy (%) at d = 6 pixels

Head Wrsts Elbws Shldrs Avg.
Pﬁster et al. [35] 98.00 88.45 77.10 93.50 88.01
Charles et al. [3] 95.40 72.95 68.70 90.30 79.90
Chen et al. [5]
64.1
Pﬁster et al. [34] 74.90 53.05 46.00 71.40 59.40
Yang et al. [53]
63.40 53.70 49.20 46.10 51.63
81.10 49.05 53.05 70.10 60.79
Ours (selfsup.)
Ours
76.10 56.50 70.70 74.30 68.44

47.9

65.9

66.5

76.8

Figure 4: Learning Human Pose. 50 unsupervised keypoints are learnt on the BBC Pose dataset.
Annotations (empty circles in the images) for 7 keypoints are provided  corresponding to — head 
wrists  elbows and shoulders. Solid circles represent the predicted positions; in [ﬁg-top] these are
raw discovered keypoints which correspond maximally to each annotation; in [ﬁg-bottom] these are
regressed (linearly) from the discovered keypoints. [table]: Comparison against supervised methods;
%-age of points within d= 6-pixels of ground-truth is reported. [top-row]: accuracy-vs-distance d  for
each body-part; [top-row-rightmost]: average accuracy for varying number of supervised samples
used for regression.
layers use 3×3 ﬁlters and each block has two layers similarly to the landmark network.
All the weights are initialised with random Gaussian noise (σ = 0.01)  and optimised using Adam [21]
with a weight decay of 5 · 10−4. The learning rate is set to 10−2  and lowered by a factor of 10 once
the training error stops decreasing; the (cid:96)2-norm of the gradients is bounded to 1.0.

4.2 Learning facial landmarks
Setup. We explore extracting source-target image pairs (x  x(cid:48)) using either (1) synthetic trans-
formations  or (2) videos. In the ﬁrst case  the pairs are obtained as (x  x(cid:48)) = (g1x0  g2x0) by
applying two random thin-plate-spline (TPS) [11  49] warps g1  g2 to a given sample image x0. We
use the 200k CelebA [24] images after resizing them to 128×128 resolution. The dataset provides
annotations for 5 facial landmarks — eyes  nose and mouth corners  which we do not use for training.
Following [45] we exclude the images in MAFL [57] test-set from the training split and generate
synthetically-deformed pairs as in [45  55]  but the transformations themselves are not required for
training. We discount the reconstruction loss in the regions of the warped image which lie outside the
original image to avoid modelling irrelevant boundary artefacts.
In the second case  (x  x(cid:48)) are two frames sampled from a video. We consider VoxCeleb [28]  a
large dataset of face tracks  consisting of 1251 celebrities speaking over 100k English language
utterances. We use the standard training split and remove any overlapping identities which appear in
the test sets of MAFL and AFLW. Pairs of frames from the same video  but possibly belonging to
different utterances are randomly sampled for training. By using video data for training our models
we eliminate the need for engineering synthetic data.

Figure 5: Unsupervised Landmarks on Human3.6M. [left]: an example quadruplet source-target-
reconstruction-keypoint (left to right) from Human3.6M. [right]: learned keypoints on a test video
sequence. The landmarks consistently track the legs  arms  torso and head across frames.

6

051015200102030405060708090100accuracy [%]headCharles (2013)Pfister (2014)Yang (2013)Pfister (2015)oursours selfsup.5101520wrists5101520elbows5101520shoulders5101520average5101520sample efficiency2005001000500010000Method

K MAFL AFLW

Unsupervised / self-supervised

11.60
10.94
8.97
7.65
7.23
6.90

Supervised
–
15.84
9.73
7.95
–
5.39

Thewlis [45]

30 7.15
50 6.67
5.83
Thewlis [44](frames) –
Shu † [38]
–
5.45
10 3.46
Zhang [55]
w/ equiv.
30 3.16
30 8.42
w/o equiv.
Wiles ‡ [51]
–
3.44

RCPR [2]
CFAN [54]
Cascaded CNN [41]
TCDCN [57]
RAR [41]
MTCNN [56]

Qualitative results. Figure 2 shows the learned heatmaps and source-target-reconstruction-
keypoints quadruplets (cid:104)x  x(cid:48)  Ψ (x  Φ(x(cid:48)))   Φ(x(cid:48))(cid:105) for synthetic transformations and videos. We note
that the method extracts keypoints which consistently track facial features across deformation and
identity changes (e.g.  the green circle tracks the lower chin  and the light blue square lies between
the eyes). The regressed semantic keypoints on the MAFL test set are visualised in ﬁg. 3  where they
are localised with high accuracy. Further  the target image x(cid:48) is also reconstructed accurately.
Quantitative results. We follow [45  44] and use un-
supervised keypoints learnt on CelebA and VoxCeleb to
regress manually-annotated keypoints in the MAFL and
AFLW [23] test sets. We freeze the parameters of the
unsupervised detector network (Φ) and learn a linear re-
gressor (without bias) from our unsupervised keypoints
to 5 manually-labelled ones from the respective training
sets. Model selection is done using 10% validation split
of the training data.
We report results in terms of standard MSE normalised
by the inter-ocular distance expressed as a percent-
age [57]  and show a few regressed keypoints in ﬁg. 3.
Before evaluating on AFLW  we ﬁnetune our networks
pre-trained on CelebA or VoxCeleb on the AFLW train-
ing set. We do not use any labels during ﬁnetuning.
Sample efﬁciency. Figure 3 reports the performance of
detectors trained on CelebA as a function of the number
n of supervised examples used to translate from unsuper-
vised to supervised keypoints. We note that n = 10 is
already sufﬁcient for results comparable to the previous
state-of-the-art (SoA) method of Thewlis et al. [45] 
and that performance almost saturates at n = 500
(vs. 19 000 available training samples).
Vs. SoA. Table 1 compares our regression results to the
SoA. We experiment regressing from K={10  30  50}
unsupervised landmarks  using the self-supervised and
the supervised perceptual loss networks; the number of
samples n used for regression is maxed out (= 19000)
to be consistent with previous works. On both MAFL
and AFLW datasets  at 2.58% and 6.31% error respec-
tively (for K = 30)  we signiﬁcantly outperform all
the supervised and unsupervised methods. Notably  we
perform better than the concurrent work of Zhang et
al. [55] (MAFL: 3.16%; AFLW: 6.58%)  while using
a simpler method. When synthetic warps are removed
from [55]  so that the equivariance constraint cannot be
employed  our method is signiﬁcantly better (2.58% vs
8.42% on MAFL). We are also signiﬁcantly better than many SoA supervised detectors [54  41  57]
using only n = 100 supervised training examples  which shows that the approach is very effective at
exploiting the unlabelled data. Finally  training with VoxCeleb video frames degrades the performance
due to domain gap; including a bias in the linear regressor improves the performance.

Table 1: Comparison with state-of-the-
art on MAFL and AFLW. K is the num-
ber of unsupervised landmarks. †: train
a 2-layer MLP instead of a linear regres-
sor. ‡: use the larger VoxCeleb2 [7] dataset
for unsupervised training  and include a
bias term in their regressor (through batch-
normalization). Normalised %-MSE is re-
ported (see ﬁg. 3).

10 3.19
30 2.58
50 2.54
10 3.32
30 2.63
50 2.59

Ours  training set: VoxCeleb

loss-net: selfsup.

w/ bias

loss-net: sup.

6.86
6.31
6.33
6.99
6.39
6.35

6.75

–

7.10

Ours  training set: CelebA

10.53
8.80

7.01
6.58

–

–

–
–

loss-net: selfsup.

loss-net: sup.

30 3.94
30 3.63
30 4.01

fc-layer (d) → 10

20

60

MAFL

20.60 21.94 28.96

ours
K=30
2.58

loss →

(cid:96)1

adv.+ (cid:96)1

(cid:96)2

adv.+ (cid:96)2

MAFL (K=30) 3.64

3.62

2.84

2.80

content
(ours)
2.58

Table 2: Abalation Study. [left]: The keypoint bottleneck when replaced with a low d-dimensional 
d = {10  20  60}  fully-connected (fc) layer leads to signiﬁcantly worse landmark detection perfor-
mance (%-MSE) on the MAFL dataset. [right]: Replacing the content loss with (cid:96)1  (cid:96)2 losses on the
images  optionally paired with an adversarial loss (adv.) also degrades the performance.

7

(a)

(b)

(c)

(d)

Figure 6: Invariant Localisation. Unsupervised keypoints discovered on smallNORB test set for
the car and airplane categories. Out of 20 learned keypoints  we show the most geometrically
stable ones: they are invariant to pose  shape  and illumination. [b–c]: elevation-vs-azimuth; [a  d]:
shape-vs-illumination (y-axis-vs-x-axis).

Ablation study.
In table 2 we present two ablation studies  ﬁrst on the keypoint bottleneck  and
second where we compare against adversarial and other image-reconstruction losses. For both the
settings  we take the best performing model conﬁguration for facial landmark detection on the MAFL
dataset.
Keypoint bottleneck. The keypoint bottleneck has two functions: (1) it provides a differentiable and
distributed representation of the location of landmarks  and (2) it restricts the information from the
target image to spatial locations only. When the bottleneck is replaced with a generic low dimensional
fully-connected layer (as in a conventional auto-encoder) the performance degrades signiﬁcantly This
is because the continuous vector embedding is not encouraged to encode geometry explicitly.
Reconstruction loss. We replace our content/perceptual loss with (cid:96)1 and (cid:96)2 losses on generated pixels;
the losses are also optionally paired with an adversarial term [13] to encourage verisimilitude as
in [18]. All of these alternatives lead to worse landmark detection performance (table 2). While
GANs are useful for aligning image distributions  in our setting we reconstruct a speciﬁc target image
(similar to an auto-encoder). For this task  it is enough to use a simple content/perceptual loss.

4.3 Learning human body landmarks

Setup. Articulated limbs make landmark localisation on human body signiﬁcantly more challeng-
ing than faces. We consider two video datasets  BBC-Pose [3]  and Human3.6M [17]. BBC-Pose
comprises of 20 one-hour long videos of sign-language signers with varied appearance  and dynamic
background; the test set includes 1000 frames. The frames are annotated with 7 keypoints correspond-
ing to head  wrists  elbows  and shoulders which  as for faces  we use only for quantitative evaluation 
not for training. Human3.6M dataset contains videos of 11 actors in various poses  shot from multiple
viewpoints. Image pairs are extracted by randomly sampling frames from the same video sequence 
with the additional constraint of maintaining the time difference within the range 3-30 frames for
Human3.6M. Loose crops around the subjects are extracted using the provided annotations and
resized to 128×128 pixels. Detectors for K = 20 and K = 50 keypoints are trained on Human3.6M
and BBC-Pose respectively.
Qualitative results. Figure 4 shows raw unsupervised keypoints and the regressed semantic ones on
the BBC-Pose dataset. For each annotated keypoint  a maximally matching unsupervised keypoint is
identiﬁed by solving bipartite linear assignment using mean distance as the cost. Regressed keypoints
consistently track the annotated points. Figure 5 shows (cid:104)x  x(cid:48)  Ψ (x  Φ(x(cid:48)))   Φ(x(cid:48))(cid:105) quadruplets  as
for faces  as well as the discovered keypoints. All the keypoints lie on top of the human actors  and
consistently track the body across identities and poses. However  the model cannot discern frontal
and dorsal sides of the human body apart  possibly due to weak cues in the images  and no explicit
constraints enforcing such consistency.
Quantitative results.
Figure 4 compares the accuracy of localising the 7 keypoints on BBC-Pose
against supervised methods  for both self-supervised and supervised perceptual loss networks. The
accuracy is computed as the the %-age of points within a speciﬁed pixel distance d. In this case  the
top two supervised methods are better than our unsupervised approach  but we outperform [33  53]
using 1k training samples (vs. 10k); furthermore  methods such as [35] are specialised for videos and

8

Figure 7: Disentangling Style and Geometry. Image generation conditioned on spatial keypoints
induces disentanglement of representations for style and geometry in the generator. Source image
(x) imparts style (e.g. colour  texture)  while the target image (x(cid:48)) inﬂuences the geometry (e.g.
shape  pose). Here  during inference  x [middle] is sampled to have a different style than x(cid:48) [top] 
although during training  image pairs with consistent style were sampled. The generated images
[bottom] borrow their style from x  and geometry from x(cid:48). (a) SVHN Digits: the foreground and
background colours are swapped. (b) AFLW Faces: pose of the style image x is made consistent
with x(cid:48). (c) Human3.6M: the background  hat  and shoes are retained from x  while the pose is
borrowed from x(cid:48). All images are sampled from respective test sets  never seen during training.

leverage temporal smoothness. Training using the supervised perceptual loss is understandably better
than using the self-supervised one. Performance is particularly good on parts such as the elbow.

4.4 Learning 3D object landmarks: pose  shape  and illumination invariance

We train our unsupervised keypoint detectors on the SmallNORB [26] dataset  comprising 5 object
categories with 10 object instances each  imaged from regularly spaced viewpoints and under different
illumination conditions. We train category-speciﬁc detectors for K = 20 keypoints using image-pairs
from neighbouring viewpoints and show results in ﬁg. 6 for car and airplane (see supplementary
material for visualisation of other object categories). Keypoints most invariant to various factors are
visualised. These landmarks are especially robust to changes in illumination and elevation angle.
They are also invariant to smaller changes in azimuth (±80◦)  but fail to generalise beyond that. Most
interesting  they localise structurally similar regions  even when there is a large change in object shape
(e.g. ﬁg. 6-(d)); such landmarks could thus be leveraged for viewpoint-invariant semantic matching.

4.5 Disentangling appearance and geometry

In ﬁg. 7 we show that our method can be interpreted as disentangling appearance from geometry.
Generator/ keypoint networks are trained on SVHN digits [29]  AFLW faces  and Human3.6M people.
The generator network is capable of retaining the geometry of an image  and substituting the style
with any other image in the dataset  including unrelated image pairs never seen during training. For
example  in the third column we re-render the number 3 by mixing its geometry with the appearance
of the number 5. This generalises signiﬁcantly from the training examples  which only consist of
pairs of digits sampled from the same house number instance  sharing a common style.

5 Conclusions

In this paper we have shown that a simple network trained for conditional image generation can be
utilised to induce  without manual supervision  a object landmark detectors. On faces  our method
outperforms previous unsupervised as well as supervised methods for landmark detection. The
method can also extend to much more challenging data  such as detecting landmarks of people  and
diverse data  such as 3D objects and digits.
Acknowledgements. We are grateful for the support provided by EPSRC AIMS CDT  ERC 638009-
IDIU  and the Clarendon Fund scholarship. We would like to thank James Thewlis for suggestions
and support with code and data  and David Novotný and Triantafyllos Afouras for helpful advice.

9

References
[1] J. Bruna  P. Sprechmann  and Y. LeCun. Super-resolution with deep convolutional sufﬁcient

statistics. In Proc. ICLR  2016.

[2] X. P. Burgos-Artizzu  P. Perona  and P. Dollár. Robust face landmark estimation under occlusion.
In Computer Vision (ICCV)  2013 IEEE International Conference on  pages 1513–1520. IEEE 
2013.

[3] J. Charles  T. Pﬁster  D. Magee  D. Hogg  and A. Zisserman. Domain adaptation for upper body

pose tracking in signed TV broadcasts. In Proc. BMVC  2013.

[4] Q. Chen and V. Koltun. Photographic image synthesis with cascaded reﬁnement networks. In

Proc. ICCV  volume 1  2017.

[5] X. Chen and A. L. Yuille. Articulated pose estimation by a graphical model with image

dependent pairwise relations. In Proc. NIPS  2014.

[6] X. Chen  Y. Duan  R. Houthooft  J. Schulman  I. Sutskever  and P. Abbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In Proc. NIPS 
pages 2172–2180  2016.

[7] J. S. Chung  A. Nagrani  and A. Zisserman. VoxCeleb2: Deep speaker recognition.

INTERSPEECH  2018.

In

[8] E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from

video. In Proc. NIPS. 2017.

[9] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on

deep networks. In Proc. NIPS  2016.

[10] A. Dosovitskiy and T. Brox. Generating images with perceptual similarity metrics based on

deep networks. In Proc. NIPS  pages 658–666  2016.

[11] J. Duchon. Splines minimizing rotation-invariant semi-norms in sobolev spaces. In Constructive

theory of functions of several variables. 1977.

[12] L. A. Gatys  A. S. Ecker  and M. Bethge. Image style transfer using convolutional neural

networks. In Proc. CVPR  2016.

[13] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and

Y. Bengio. Generative adversarial nets. In Proc. NIPS  2014.

[14] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.

Science  313(5786):504–507  2006.

[15] G. E. Hinton  S. Osindero  and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural

computation  18(7):1527–1554  2006.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[17] C. Ionescu  D. Papava  V. Olaru  and C. Sminchisescu. Human3.6m: Large scale datasets and

predictive methods for 3d human sensing in natural environments. PAMI  2014.

[18] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional

adversarial networks. In Proc. CVPR  2017.

[19] J. Johnson  A. Alahi  and F. Li. Perceptual losses for real-time style transfer and super-resolution.

In Proc. ECCV  2016.

[20] N. Kalchbrenner  A. Oord  K. Simonyan  I. Danihelka  O. Vinyals  A. Graves  and

K. Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527  2016.

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

10

[22] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[23] M. Koestinger  P. Wohlhart  P. M. Roth  and H. Bischof. Annotated facial landmarks in the wild:
A large-scale  real-world database for facial landmark localization. In ICCV Workshops  2011.

[24] Z. L.  P. L.  X. W.  and X. T. Deep learning face attributes in the wild. In Proc. ICCV  2015.

[25] G. Larsson  M. Maire  and G. Shakhnarovich. Learning representations for automatic coloriza-

tion. In Proc. ECCV  2016.

[26] Y. LeCun  F. J. Huang  and L. Bottou. Learning methods for generic object recognition with

invariance to pose and lighting. In Proc. CVPR  2004.

[27] C. Ledig  L. Theis  F. Huszár  J. Caballero  A. Cunningham  A. Acosta  A. Aitken  A. Tejani 
J. Totz  Z. Wang  and W. Shi. Photo-realistic single image super-resolution using a generative
adversarial network. In Proc. CVPR  2017.

[28] A. Nagrani  J. S. Chung  and A. Zisserman. Voxceleb: a large-scale speaker identiﬁcation

dataset. In INTERSPEECH  2017.

[29] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural

images with unsupervised feature learning. In NIPS DLW  volume 2011  2011.

[30] A. Nguyen  A. Dosovitskiy  J. Yosinski  T. Brox  and J. Clune. Synthesizing the preferred inputs

for neurons in neural networks via deep generator networks. In Proc. NIPS  2016.

[31] A. Nguyen  J. Yosinski  Y. Bengio  A. Dosovitskiy  and J. Clune. Plug & play generative

networks: Conditional iterative generation of images in latent space. In Proc. CVPR  2017.

[32] V. Patraucean  A. Handa  and R. Cipolla. Spatio-temporal video autoencoder with differentiable

memory. In ICLR Workshop  2015.

[33] T. Pﬁster  J. Charles  and A. Zisserman. Large-scale learning of sign language by watching TV

(using co-occurrences). In Proc. BMVC  2013.

[34] T. Pﬁster  K. Simonyan  J. Charles  and A. Zisserman. Deep convolutional neural networks
for efﬁcient pose estimation in gesture videos. In Proceedings of the Asian Conference on
Computer Vision  2014.

[35] T. Pﬁster  J. Charles  and A. Zisserman. Flowing convnets for human pose estimation in videos.

In Proc. ICCV  2015.

[36] S. E. Reed  Y. Zhang  Y. Zhang  and H. Lee. Deep visual analogy-making. In Proc. NIPS  2015.

[37] S. E. Reed  Z. Akata  S. Mohan  S. Tenka  B. Schiele  and H. Lee. Learning what and where to

draw. In Proc. NIPS  pages 217–225  2016.

[38] Z. Shu  M. Sahasrabudhe  A. Guler  D. Samaras  N. Paragios  and I. Kokkinos. Deforming

autoencoders: Unsupervised disentangling of shape and appearance. In Proc. ECCV  2018.

[39] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. CoRR  abs/1409.1556  2014.

[40] N. Srivastava  E. Mansimov  and R. Salakhudinov. Unsupervised learning of video representa-

tions using lstms. In Proc. ICML  pages 843–852  2015.

[41] Y. Sun  X. Wang  and X. Tang. Deep convolutional network cascade for facial point detection.

In Proc. CVPR  2013.

[42] I. Sutskever  G. E. Hinton  and G. W. Taylor. The recurrent temporal restricted boltzmann

machine. In Proc. NIPS  pages 1601–1608  2009.

[43] S. Suwajanakorn  N. Snavely  J. Tompson  and M. Norouzi. Discovery of latent 3d keypoints

via end-to-end geometric reasoning. In Proc. NIPS  2018.

11

[44] J. Thewlis  H. Bilen  and A. Vedaldi. Unsupervised object learning from dense invariant image

labelling. In Proc. NIPS  2017.

[45] J. Thewlis  H. Bilen  and A. Vedaldi. Unsupervised learning of object landmarks by factorized

spatial embeddings. In Proc. ICCV  2017.

[46] R. Villegas  J. Yang  Y. Zou  S. Sohn  X. Lin  and H. Lee. Learning to generate long-term future

via hierarchical prediction. arXiv preprint arXiv:1704.05831  2017.

[47] P. Vincent  H. Larochelle  Y. Bengio  and P.-A. Manzagol. Extracting and composing robust

features with denoising autoencoders. In Proc. ICML  pages 1096–1103. ACM  2008.

[48] C. Vondrick  H. Pirsiavash  and A. Torralba. Generating videos with scene dynamics. In Proc.

NIPS  pages 613–621  2016.

[49] G. Wahba. Spline models for observational data  volume 59. Siam  1990.

[50] W. F. Whitney  M. Chang  T. Kulkarni  and J. B. Tenenbaum. Understanding visual concepts

with continuation learning. In ICLR Workshop  2016.

[51] O. Wiles  A. S. Koepke  and A. Zisserman. Self-supervised learning of a facial attribute

embedding from video. In Proc. BMVC  2018.

[52] T. Xue  J. Wu  K. L. Bouman  and W. T. Freeman. Visual dynamics: Probabilistic future frame

synthesis via cross convolutional networks. In Proc. NIPS  2016.

[53] Y. Yang and D. Ramanan. Articulated pose estimation with ﬂexible mixtures-of-parts. In Proc.

CVPR  2011.

[54] J. Zhang  S. Shan  M. Kan  and X. Chen. Coarse-to-ﬁne auto-encoder networks (cfan) for

real-time face alignment. In Proc. ECCV  2014.

[55] Y. Zhang  Y. Guo  Y. Jin  Y. Luo  Z. He  and H. Lee. Unsupervised discovery of object landmarks

as structural representations. In Proc. CVPR  2018.

[56] Z. Zhang  P. Luo  C. C. Loy  and X. Tang. Facial landmark detection by deep multi-task learning.

In Proc. ECCV  pages 94–108. Springer  2014.

[57] Z. Zhang  P. Luo  C. C. Loy  and X. Tang. Learning Deep Representation for Face Alignment

with Auxiliary Attributes. PAMI  2016.

12

,Jayadev Acharya
Constantinos Daskalakis
Gautam Kamath
Tomas Jakab
Ankush Gupta
Hakan Bilen
Andrea Vedaldi
Edgar Dobriban
Sifan Liu