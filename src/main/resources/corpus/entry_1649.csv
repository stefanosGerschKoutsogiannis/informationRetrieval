2019,Dying Experts: Efficient Algorithms with Optimal Regret Bounds,We study a variant of decision-theoretic online learning in which the set of experts that are available to Learner can shrink over time. This is a restricted version of the well-studied sleeping experts problem  itself a generalization of the fundamental game of prediction with expert advice. Similar to many works in this direction  our benchmark is the ranking regret. Various results suggest that achieving optimal regret in the fully adversarial sleeping experts problem is computationally hard. This motivates our relaxation where any expert that goes to sleep will never again wake up. We call this setting "dying experts" and study it in two different cases: the case where the learner knows the order in which the experts will die and the case where the learner does not. In both cases  we provide matching upper and lower bounds on the ranking regret in the fully adversarial setting. Furthermore  we present new  computationally efficient algorithms that obtain our optimal upper bounds.,Dying Experts: Efﬁcient Algorithms

with Optimal Regret Bounds

Hamid Shayestehmanesh∗

Department of Computer Science

University of Victoria

Sajjad Azami∗

Department of Computer Science

University of Victoria

Nishant A. Mehta

Department of Computer Science

University of Victoria

{hamidshayestehmanesh  sajjadazami  nmehta}@uvic.ca

Abstract

We study a variant of decision-theoretic online learning in which the set of experts
that are available to Learner can shrink over time. This is a restricted version of the
well-studied sleeping experts problem  itself a generalization of the fundamental
game of prediction with expert advice. Similar to many works in this direction  our
benchmark is the ranking regret. Various results suggest that achieving optimal
regret in the fully adversarial sleeping experts problem is computationally hard.
This motivates our relaxation where any expert that goes to sleep will never again
wake up. We call this setting “dying experts” and study it in two different cases:
the case where the learner knows the order in which the experts will die and the
case where the learner does not. In both cases  we provide matching upper and
lower bounds on the ranking regret in the fully adversarial setting. Furthermore 
we present new  computationally efﬁcient algorithms that obtain our optimal upper
bounds.

1

Introduction

Decision-theoretic online learning (DTOL) [13  20  21  6] is a sequential game between a learning
agent (hereafter called Learner) and Nature. In each round  Learner plays a probability distribution
over a ﬁxed set of experts and suffers loss accordingly. However  in wide range of applications 
this “ﬁxed” set of actions shrinks as the game goes on. One way this can happen is because experts
either get disqualiﬁed or expire over time; a key scenario of contemporary relevance is in contexts
where experts that discriminate are prohibited from being used due to existing (or emerging) anti-
discrimination laws. Two prime examples are college admissions and deciding whether incarcerated
individuals should be granted parole; here the agent may rely on predictions from a set of experts in
order to make decisions  and naturally experts detected to be discriminating against certain groups
should not be played anymore. However  the standard DTOL setting does not directly adapt to this
case  i.e.  for a given round it does not make sense nor may it even be possible to compare Learner’s
performance to an expert or action that is no longer available.
Motivated by cases where the set of experts can change  a reasonable benchmark is the ranking regret
[12  9]  for which Learner competes with the best ordering of the actions (see (1) in Section 2 for
a formal deﬁnition). The situation where the set of available experts can change in each round is
known as the sleeping experts setting  and unfortunately  it appears to be computationally hard to

∗equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

√

obtain a no-regret algorithm in the case of adversarial payoffs (losses in our setting) and adversarial
availability of experts [10]. This motivates the question of whether the optimal regret bounds can
be achieved efﬁciently for the case where the set of experts can only shrink  which we will refer to
as the “dying experts” setting. Applying the results of [12] to the dying experts problem only gives
O(
T K log K) regret  for K experts and T rounds  and their strategy is computationally inefﬁcient.
In more detail  the strategy in [12] is to deﬁne a permutation expert (our terminology) that is identiﬁed
by an ordering of experts  where a permutation expert’s strategy is to play the ﬁrst awake expert in
the ordering. They then run Hedge [6] on the set of all possible permutation experts over K experts.
Although this strategy competes with the best ordering  the per-round computation of running Hedge
on K! experts is O(K K) if naïvely implemented  and the results of [10] suggest that no efﬁcient
algorithm — one that uses computation poly(K) per round — can obtain regret that simultaneously is
o(T ) and poly(K). However  in the dying experts setting  we show that many of these K! orderings
are redundant and only O(2K) of them are “effective”. The notion of effective experts (formally
deﬁned in Section 3) is used to refer to a minimal set of orderings such that each ordering in the set
will behave uniquely in hindsight. The behavior of an ordering is deﬁned as how it uses the initial
experts in its predictions over T rounds. Interestingly  it turns out that this structure also allows for an
efﬁcient implementation of Hedge which  as we show  obtains optimal regret in the dying experts
setting. The key idea that enables an efﬁcient implementation is as follows. Our algorithms group
orderings with identical behavior into one group  where there can be at most K groups at each round.
When an expert dies  the orderings in one of the groups are forced to predict differently and therefore
have to redistribute to the other groups. This splitting and rejoining behavior occurs in a ﬁxed pattern
which enables us to efﬁciently keep track of the weight associated with each group.
In certain scenarios  Learner might be aware of the order in which the experts will become unavailable.
For example  in online advertising  an ad broker has contracts with their providers and these contracts
may expire in an order known to Learner. Therefore  we will study the problem in two different
settings: when Learner is aware of this order and when it is not.

Contributions. Our ﬁrst main result is an upper bound on the number of effective experts (The-
orem 3.1); this result will be used for our regret upper bound in the known order case. Also  in
preparation for our lower bound results  we prove a fully non-asymptotic lower bound on the minimax
regret for DTOL (Theorem 4.1). Our main lower bounds contributions are minimax lower bounds
for both the unknown and known order of dying cases (Theorems 4.2 and 4.4). In addition  we
provide strategies to achieve optimal upper bounds for unknown and known order of dying (Theo-
rems 4.3 and 4.5 respectively)  along with efﬁcient algorithms for each case. This is in particular
interesting since  in the framework of sleeping experts  the results of [10] suggest that no-regret
learning is computationally hard  but we show that it is efﬁciently achievable in the restricted problem.
Finally  in Section 5.3  we show how to generalize our algorithms to other algorithms with adaptive
learning rates  either adapting to unknown T or achieving far greater forms of adaptivity like in
AdaHedge and FlipFlop [5].
All formal proofs not found in the main text can be found in the appendix.

2 Background and related work

The DTOL setting [6] is a variant of prediction with expert advice [13  20  21] in which Learner
receives an example xt in round t and plays a probability distribution pt over K actions. Nature
then reveals a loss vector (cid:96)t that indicates the loss for each expert. Finally  Learner suffers a loss

ˆ(cid:96)t := pt · (cid:96)t =(cid:80)K

i=1 pi t(cid:96)i t.

a ⊆ Et

In the dying experts problem  we assume that the set of experts can only shrink. More formally  for
the set of experts E = {e1  e2  . . . eK}  at each round t  Nature chooses a non-empty set of experts
a for all t ∈ {1  . . .   T − 1}. In other words  in some rounds
a to be available such that Et+1
Et
Nature sets some experts to sleep  and they will never be available again. Similar to [12  11  10]  we
adopt the ranking regret as our notion of regret. Before proceeding to the deﬁnition of ranking regret 
let us deﬁne π to be an ordering over the set of initial experts E. We use the notion of orderings and
permutation experts interchangeably. Learner can now predict using π ∈ Π  where Π is the set of all
the orderings. Also  denote by σt(π) the ﬁrst alive expert of ordering π in round t; expert σt(π) is
the action that will be played by π. The cumulative loss of an ordering π with respect to the available

2

experts Et

a is the sum of the losses of σt(π) at each round. We can now deﬁne the ranking regret:

RΠ(1  T ) =

(cid:96)σt(π) t .

ˆ(cid:96)t − min
π∈Π

(1)

Since we will use the notion of classical regret in our proofs  we also provide its formal deﬁnition:

T(cid:88)
T(cid:88)

t=1

t=1

T(cid:88)
T(cid:88)

t=1

t=1

RE(1  T ) =

(cid:96)i t .

(2)

ˆ(cid:96)t − min
i∈[K]

We use the convention that the subscript of a regret notion R represents the set of experts against which
we compare Learner’s performance. Also  the argument in parentheses represents the set of rounds in
the game. For example  RΠ(1  T ) represents the regret over rounds 1 to T with the comparator set
being all permutation experts Π. Also  we assume that (cid:96)i t ∈ [0  1] for all i ∈ [K]  t ∈ [T ].
Similar to the deﬁnition of Et
a be the set of dead experts at the start of round t. We
refer to a round as a “night” if any expert becomes unavailable on the next round. A “day” is deﬁned
as a continuous subset of rounds where the subset starts with a round after a night and ends with a
night. As an example  if any expert become unavailable at the beginning of round t  we refer to round
t − 1 as a night (and we say the expert dies on that night) and the set of rounds {t  t + 1 . . .   t(cid:48)} as a
day  where t(cid:48) is the next night. We denote by m the number of nights throughout a game of T rounds.

d := E \ Et

a  let Et

Related work. The papers [7] and [1] initiated the line of work on the sleeping experts setting.
These works were followed by [2]  which considered a different notion of regret and a variety of
different assumptions. In [7]  the comparator set is the set of all probability vectors over K experts 
while we compare Learner’s performance to the performance of the best ordering. In particular  the
problem considered in [7] aims to compare Learner’s performance to the best mixture of actions 
which also includes our comparator set (orderings). However  in order to recover an ordering as
we deﬁne  one needs to assign very small probabilities to all experts except for one (the ﬁrst alive
action)  which makes the bound in [7] trivial. As already mentioned  we assume the set Et
a is chosen
adversarially (subject to the restrictions of the dying setting)  while in [11] and [15] the focus is on
the (full) sleeping experts setting with adversarial losses but stochastic generation of Et
a.
For the case of adversarial selection of available actions (which is more relevant to the present paper) 
[12] studies the problem in the cases of stochastic and adversarial rewards with both full information
√
and bandit feedback. Among the four settings  the adversarial full-information setting is most related
to our work. They prove a lower bound of Ω(
T K log K) in this case and a matching upper bound
by creating K! experts and running Hedge on them  which  as mentioned before  requires computation
of order O(K K) per round. They prove an upper bound of O(K
T log K) which is optimal within
a log factor for the bandit setting using a similar transformation of experts. A similar framework in
the bandits setting introduced in [4] is called “mortal bandits”; we do not discuss this work further as
the results are not applicable to our case  given that they do not consider adversarial rewards. There
is also another line of work which considers the contrary direction of the dying experts game. The
setting is usually referred to as “branching” experts  in which the set of experts can only expand. In
particular  part of the inspiration for our algorithms came from [8  14].
The hardness of the sleeping experts setting is well-studied [10  9  12]. First  [12] showed for a
restricted class of algorithms that there is no efﬁcient no-regret algorithm for sleeping experts setting
unless RP = N P . Following this  [10] proved that the existence of a no-regret efﬁcient algorithm
for the sleeping experts setting implies the existence of an efﬁcient algorithm for the problem of
PAC learning DNFs  a long-standing open problem. For the similar but more general case of online
sleeping combinatorial optimization (OSCO) problems  [9] showed that an efﬁcient and optimal
algorithm for “per-action” regret in OSCO problems implies the existence of an efﬁcient algorithm
for PAC learning DNFs. Per-action regret is another natural benchmark for partial availability of
actions for which the regret with respect to an action is only considered in rounds in which that action
was available.

√

3 Number of effective experts in dying experts setting

In this section  we consider the number of effective permutation experts among the set of all possible
orderings of initial experts. The idea behind this is that  given the structure in dying experts  not

3

all the orderings will behave uniquely in hindsight. Formally  the behavior of π is a sequence of
predictions (σ1(π)  σ2(π)  . . .   σT (π)). This means that the behaviors of two permutation experts π
and π(cid:48) are the same if they use the same initial experts in every round. We deﬁne the set of effective
orderings E ⊆ Π to be a set such that  for each unique behavior of orderings  there only exists one
ordering in E.
To clarify the deﬁnition of unique behavior  suppose initial expert e1 is always awake. Then two
orderings π1 = (e1  e2  . . . ) and π2 = (e1  e3  . . . ) will behave the same over all the rounds 
making one of them redundant. Let us clarify that behavior is not deﬁned based on losses  e.g. 
if π1 = (ei  . . . ) and π2 = (ej  . . . ) where i (cid:54)= j both suffer identical losses over all the rounds
(i.e. their performances are equal) while using different original experts  then they are not considered
redundant and hence both of them are said to be effective.
Let di be the number of experts dying on the i th night. Denote by A the number of experts that will

i=1 di. We are now ready to ﬁnd the cardinality of set E.

Theorem 3.1. In the dying experts setting  for K initial experts and m nights  the number of effective

always be awake  so that A = K −(cid:80)m
orderings in Π is f ({d1  d2  . . . dm}  A) = A ·(cid:81)m

s=1(ds + 1).

In the special case where no expert dies (m = 0)  we use the convention that the (empty) product
evaluates to 1 and hence f ({}  A) = A. We mainly care about |E| as we use it to derive our upper
bounds; hence  we should ﬁnd the maximum value of f. We can consider the maximum value of f in
three regimes.

1. In the case of a ﬁxed number of nights m and ﬁxed A  the function f is maximized by
equally spreading the dying experts across the nights. As the number of dying experts might
not be divisible by the number of nights  some of the nights will get one more expert than
D = K − A + m and K − A ≤ m.

the others. Formally  the maximum value is ((cid:6) D

m

(cid:7)D mod m ·(cid:4) D

(cid:5)m−(D mod m) · A)  where

m

2. In the case of a ﬁxed number of dying experts (ﬁxed A)  the maximum value of f is (2K−A ·
A) which occurs when one expert dies on each night. The following is a brief explanation
on how to get this result. Denote by B = (d1  d2  . . .   db) a sequence of numbers of dying
experts where more than one expert dies on some night and B maximizes f (for ﬁxed A) 
so that F = f ({d1  d2  . . .   db}  A). Without loss of generality  assume that d1 > 1. Split
the ﬁrst night into d1 days where one expert dies at the end of each day (and consequently
each of those days becomes a night). Now F (cid:48) = f ({1  1  . . .   1  d2  . . .   db}  A) where 1 is
repeated d1 times. If d1 > 1 then F (cid:48) = F · 2d1 /(d1 + 1) > F . We see that by splitting the
nights we can achieve a larger effective set.

3. In the case of a ﬁxed number of nights m  similar to the previous cases  the maximum value
is obtained when each night has equal impact on the value of f  i.e.  when A = d1 + 1 =
d2 + 1 = ··· = dm + 1; however  it might not be possible to distribute the experts in a way
to get this  in which case we should make the allocation {A  d1 + 1  d2 + 1  . . .   dm + 1} as
uniform as possible.

By looking at cases 2 and 3  we see that by increasing m and the number of dying experts  we
can increase f; thus  the maximum value of f with no restriction is 2K−1 and is achieved when
m = K − 1 and A = 1.

4 Regret bounds for known and unknown order of dying

In this section  we provide lower and upper bounds for the cases of unknown and known order of
dying. In order to prove the lower bounds  we need a non-asymptotic minimax lower bound for the
DTOL framework  i.e.  one which holds for a ﬁnite number of experts K and ﬁnite T . During the
preparation of the ﬁnal version of this work  we were made aware of a result of Orabona and Pál (see
Theorem 8 of [16]) that does give such a bound. However  for completeness  we present a different
fully non-asymptotic result that we independently developed; this result is stated in a simpler form
and admits a short proof (though we admit that it builds upon heavy machinery). We then will prove
matching upper bounds for both cases of unknown and known order of dying.

4

4.1 Fully non-asymptotic minimax lower bound for DTOL

We analyze lower bounds on the minimax regret in the DTOL game with K experts and T rounds.
We assume that all losses are in the interval [0  1]. Let ∆K := ∆([K]) denote the simplex over K
outcomes. The minimax regret is deﬁned as

inf

p1∈∆K

sup

(cid:96)1∈[0 1]K

. . .

inf

pT ∈∆K

pt · (cid:96)t − min
j∈[K]

(cid:96)j t

.

(3)

Theorem 4.1. For a universal constant L  the minimax regret (3) is lower bounded by

sup

(cid:40) T(cid:88)
(cid:110)(cid:112)(T /2) log K  T

(cid:96)T ∈[0 1]K

t=1

(cid:111)

.

1
L

min

(cid:41)

T(cid:88)

t=1

The proof (in the appendix) begins similarly to the proof of the often-cited Theorem 3.7 of [3]  but
it departs at the stage of lower bounding the Rademacher sum; we accomplish this lower bound by
invoking Talagrand’s Sudakov minoration for Bernoulli processes [17  18].

4.2 Unknown order of dying

√

mT log K). Given that we have Et+1

For the case where Learner is not aware of the order in which the experts die  we prove a lower bound
of Ω(
a  the construction for the lower bound of [12]
cannot be applied to our case. In other words  our adversary is much weaker than the one in [12]  but 
surprisingly  we show that the previous lower bound still holds (by setting m = K) even with the
weaker adversary. We then analyze a simple strategy to achieve a matching upper bound.

In this section  we further assume that(cid:112)T /2 log K < T for every T and K so that there is hope to

a ⊆ Et

achieve regret that is sublinear with respect to T . We now present our lower bound on the regret for
the case of unknown order of dying.
Theorem 4.2. When the order of dying is unknown  the minimax regret is Ω(

mT log K).

√

Proof Sketch. We construct a scenario where each day is a game decoupled from the previous ones.
This means that the algorithm will be forced to have no prior information about the experts at the
beginning of each day. First  partition the T rounds into m + 1 days of equal length. The days
are split into two halves. On the ﬁrst half  each expert suffers loss drawn i.i.d. from a Bernoulli
distribution with p = 1/2. At the end of the ﬁrst half of the day  we choose the expert with the lowest
cumulative loss until that round  and that expert will suffer no loss on the second half. For any other
expert ei  we use the loss (cid:96)(1)
i t of ei on the
t th round of the second half; speciﬁcally  we choose the setting (cid:96)(2)
i t . We show that the
ranking regret of the set of orderings over T rounds is obtained by summing the classical regrets of
each day over the set of days.

i t of ei on the t th round of the ﬁrst half to deﬁne the loss (cid:96)(2)

i t := 1 − (cid:96)(1)

A natural strategy in the case of unknown dying order is to run Hedge over the set of initial experts
E and  after each night  reset the algorithm. We will refer to this strategy as “Resetting-Hedge”.
Theorem 4.3 gives an upper bound on regret of Resetting-Hedge.
√
Theorem 4.3. Resetting-Hedge enjoys a regret of RΠ(1  T ) = O(

Proof. Let τs be the set of round indices of day s; hence  we have(cid:80)m+1

s=1 |τs| = T . The overall
ranking regret can be upper bounded by the sum of classical regrets for every interval. Hence  the
analysis is as follows:

mT log K).

RΠ(1  T ) ≤ m+1(cid:88)

(cid:112)|τs| log(K − s) ≤(cid:112)log K

m+1(cid:88)

(cid:112)|τs| ≤(cid:112)(m + 1)T log K;

(4)

the last inequality is essentially from the Cauchy-Schwarz inequality (see Lemma B.2).

s=1

s=1

Although the basic Resetting-Hedge strategy adapts to m  it has many downsides. For example 
resetting can be wasteful in practice. Another natural strategy  simply running Hedge on the set of

5

√
all K! permutation experts  is non-adaptive (obtaining regret O(
T K log K) and computationally
inefﬁcient if implemented naïvely). However  as we show in Section 5.1  this algorithm can be
implemented efﬁciently (with runtime linear in K rather than K!) and also  as we show in Section 5.3 
by running Hedge on top of several copies of Hedge (one per specially chosen learning rate)  we can
obtain a guarantee that is far better than Theorem 4.3. Moreover  our efﬁcient implementation of
Hedge can be extended to adaptive algorithms like AdaHedge and FlipFlop [5].

4.3 Known order of dying

A natural question is whether Learner can leverage information about the order of experts that are
going to die to achieve a better regret. We show that the answer is positive: the bound can be improved
by a logarithmic factor. We also give a matching lower bound for this case (so both bounds are tight).
Similar to the unknown setting  we provide a construction to prove a lower bound on the ranking

regret in this case. We still assume that(cid:112)T /2 log K < T .

√
Theorem 4.4. When Learner knows the order of dying  the minimax regret is Ω(

mT ).

Proof Sketch. Our construction involves ﬁrst partitioning all the rounds to m/2 days of equal length.
On day s  all experts will suffer loss 1 on all the rounds except for experts e2s−1 and e2s  who will
suffer losses drawn i.i.d. from a Bernoulli distribution with success probability p = 1/2. Experts
e2s−1 and e2s will die at the end of day s  and therefore  each “day game” effectively has 2 experts;
our lower bound holds even when Learner knows this fact. Furthermore  Learner will be aware that
these two experts (e2s−1 and e2s) will die at the end of day s. Similar to the proof of Theorem 4.2 
the minimax regret is lower bounded by the sum of the minimax regrets over each day game.

Although the proof is relatively simple  it is at least a little surprising that knowing such rich
information as the order of dying only improves the regret by a logarithmic factor.
To achieve an optimal upper bound  using the results of Theorem 3.1  the strategy is to create
2m(K − m) experts (those that are effective) and run Hedge on this set.
Theorem 4.5. For the case of known order of dying  the strategy as described above achieves a

regret of O(cid:0)(cid:112)T (m + log K)(cid:1).

T log K) for K number of experts. Therefore  running Hedge on

√
Proof. Hedge has regret of O(
2m(K − m) experts yields the desired bound.
Though the order of computation in the above strategy is better than O(K K)  it is still exponential in
K. In the next section  we introduce algorithms that simulate these strategies but in a computationally
efﬁcient way.

5 Efﬁcient algorithms for dying experts

The results of [10] imply computational hardness of achieving no-regret algorithms in sleeping
experts; yet  we are able to provide efﬁcient algorithms for dying experts in the cases of unknown and
known order of dying. For the sake of simplicity  we initially assume that only one expert dies each
night. Later  in Section 5.3  we show how to extend the algorithms for the general case where multiple
experts can die each night. We then show how to extend these algorithms to adaptive algorithms such
as AdaHedge [5]. The algorithms for both cases are given in Algorithms 1 and 2.

5.1 Unknown order of dying

We now show how to efﬁciently implement Hedge over the set of all the orderings. Even though
Resetting-Hedge is already efﬁcient and achieves optimal regret  it has its own disadvantages. The
issue arises when one needs to extend Resetting-Hedge to adaptive algorithms. This is particularly
important in real-world scenarios  where Learner wants to adapt to the environment (such as stochastic
or adversarial losses). We show that Algorithm 1  Hedge-Perm-Unknown (HPU)  can be adapted to
AdaHedge [19] and  therefore  we can simulate FlipFlop [5]. Next  we give the main idea on how the

6

Algorithm 1: Hedge-Perm-Unknown (HPU)
∀i ∈ [K] ci 1 := 1  hi 1 := (K − 1)!
Ea := {e1  e2  . . . eK}
for t = 1  2  . . .   T do

Algorithm 2: Hedge-Perm-Known (HPK)
∀i ∈ [K] ci 1 := 1  hi 1 := (cid:100)2K−i−1(cid:101)
Ea := {e1  e2  . . . eK}
for t = 1  2  . . .   T do

(cid:104)

(cid:105)

(cid:80)k
hi t·ci t
j=1 hj t·cj t

i∈[K]

1 [ei ∈ Ea]·
play pt =
receive ((cid:96)1 t  . . .   (cid:96)K t)
for ei ∈ Ea do

ci t+1 := ci t · e−η(cid:96)i t
hi t+1 := hi t
if expert j dies then
Ea := Ea \ {ej}
for ei ∈ Ea do

hi t+1 := hi t+1 · ci t+1

+(hj t+1 · cj t+1)/|Ea|

ci t+1 := 1

(cid:104)

1 [ei ∈ Ea]·
play pt =
receive ((cid:96)1 t  . . .   (cid:96)K t)
for ei ∈ Ea do

(cid:105)

(cid:80)k
hi t·ci t
j=1 hj t·cj t

i∈[K]

ci t+1 := ci t · e−η(cid:96)i t
hi t+1 := hi t
if expert j dies then
Ea := Ea \ {ej}
for each i = j + 1 to K do
hi t+1 := hi t+1 · ci t+1
+(hj t+1 · cj t+1)((cid:100)2i−2(cid:101)/2K−1−j)

ci t+1 := 1

algorithm works  after which we prove that Algorithm 1 efﬁciently simulates running Hedge over Π.
Before proceeding further  let us recall how Hedge makes predictions in round t. First  it updates the
weights using wi t = wi t−1e−η(cid:96)i t  and it then assigns a probability to expert i as follows:

(cid:80)K

wi t−1
i=1 wi t−1

.

pi t =

Recall that e1  e2  . . .   eK denote the original experts while π1  π2  . . . πK! denote the orderings.
i ⊆ Π to be the set of orderings
Denote by wt
predicting as expert ei in round t. The main ideas behind the algorithm are as follows:

π the weight that Hedge assigns to π in round t. Deﬁne Πt

1. When π and π(cid:48) have the same prediction e in round t (i.e. σt(π) = σt(π(cid:48)) = e)  then we do

π(cid:48); we use wt
e−ηLt−1
π∈Πt

π

j

π(cid:48) instead for the weight of e.
π + wt
  where η is the learning rate and Lt

π is the cumu-

π =(cid:80)t

lative loss of ordering π up until round t  i.e.  Lt

s=1 (cid:96)σs(π) s.

We will discuss how to tune η later. Let J = {j1  . . .   jm} represent the rounds on which any expert
will die. Denote by jt the last night observed so far at the end of round t  formally deﬁned as
jt = maxj∈J j ≤ t. We maintain a tuple (hi t  ci t) for each original expert ei’s in the algorithm
in round t  where hi t is the sum of non-normalized weights of the experts in Πt
i in round jt. We
similarly maintain ci t  except that it only considers the loss suffered from jt + 1 to round t − 1 for
experts in Πt

not need to know wt

2. The algorithm maintains(cid:80)

π and wt

(cid:88)

i. Formally:

e−η((cid:80)jt
It is easy to verify that hj t · cj t =(cid:80)

hi t =

π∈Πt

i

e−ηLt−1

π

.

π∈Πt

j

−η((cid:80)t−1

(cid:88)

π∈Πt

i

s=1 (cid:96)σs(π) s) 

ci t =

e

s=jt+1 (cid:96)σs(π) s) .

The computational cost of the algorithm at each round will be O(K). We claim that HPU will behave
the same as executing Hedge on Π. We use induction on rounds to show the weights are the same
in both algorithms. By “simulating” we mean that the weights over the original experts will be
maintained identically to how Hedge maintains them.
Theorem 5.1. At every round  HPU simulates running Hedge on the set of experts Π.

Proof Sketch. The main idea is to group the permutation experts with similar predictions (the ﬁrst
expert alive in the permutation) in one group. Hence  initially there will be K groups. Then  if expert
ej dies  every ordering in the group associated with ej will be moved to another group and the empty
group will be deleted. We prove that the orderings will distribute to other groups symmetrically after
a night. Using this fact  we show that we do not need to know the elements of a group; we only
maintain the sum of the weights given to all the orderings in each group.

7

5.2 Known order of dying

For the case of known order of dying  we propose Algorithm 2  Hedge-Perm-Known (HPK)  which
is slightly different than HPU. In particular  the weight redistribution (when an expert dies) and
initialization of coefﬁcient hi 1 is different. In the proof of Theorem 5.1  we showed that when the
set of experts includes all the orderings  the weight of the expert j that died will distribute equally
between initial experts (ej ∈ E). But when the set of experts is only the effective experts  this no
longer holds. In this section  we assume without loss of generality that the experts die in the order
e1  e2  . . . and recall that E denotes the set of effective orderings. Based on Theorem 3.1  the number
of experts starting with ei in E is (cid:100)2K−i−1(cid:101); we denote the set of such experts as Eei.
Theorem 5.2. At every round  HPK simulates running Hedge on the set of experts E.

the learning rate for HPU is η =(cid:112)(2 log(K!))/T and for HPK is η =(cid:112)(2 log(2m(K − m)))/T .

Remarks for tuning learning rates. For both algorithms  we assume T is known beforehand. So 

One can use a time-varying learning rate to adapt to T in case it is not known.

5.3 Extensions for algorithms

As we mentioned at the beginning of Section 5  for the sake of simplicity we initially assumed that
only one expert dies each night. First  we discuss how to handle a night with more than one death.
Afterwards  we explain how to extend/modify HPU and HPK to implement the Follow The Leader
(FTL) strategy. We then introduce a new algorithm which simulates FTL efﬁciently and maintains L∗
as well  where L∗
t
t is the cumulative loss of the best permutation expert through the end of round t.
Finally  using L∗
t   we explain how to simulate AdaHedge and FlipFlop [5] by slightly extending HPU
and HPK.

More than one expert dying in a night. We handle nights with more than one death as follows.
We have one of the experts die on that night  and  for each expert j among the other experts that
should have died that night  we create a “dummy round”  give all alive experts (including expert j) a
loss of zero  keep the learning rate the same as the previous round  and have expert j die at the end
of the dummy round (which hence becomes a “dummy night”). Even though the number of rounds
increases with this trick  it is easy to see that the regret is unchanged since in dummy rounds all
experts have the same loss (and also the learning rate after the sequence of dummy rounds is the same
as what it would have been had there been no dummy rounds). Moreover  since now one expert dies
on each night (some of which may be dummy nights)  we may use Theorems 5.1 or 5.2 to conclude
that our algorithm correctly distributes any dying experts’ weights among the alive experts.

Beyond adaptivity to m. Consider the case of unknown order and let the number of nights m be
unknown. As promised  we show that we can improve on the simple Resetting-Hedge strategy.
Theorem 5.3. Consider running Hedge on top of K copies of HPU where  for r ∈ {0  1  . . .   K − 1} 

we set εr =(cid:81)r−1
changes experts at most l times. Then the regret of this algorithm is O(cid:0)(cid:112)T (l + 1) log K)(cid:1).

π∗ be a best permutation expert in hindsight and suppose that the sequence (σ1(π∗)  . . .   σT (π∗)

:=(cid:112)8 log(1/εr)/t. Let

l=0

1

K−l and the r th copy of HPU uses learning rate ηεr

t

Note that this theorem does better than adapt to m  as with m nights we always have l ≤ m but l can
in fact be much smaller than m in practice. Hence  Theorem 5.3 recovers and can improve upon the
regret of Resetting-Hedge and  moreover  wasteful resetting is avoided. Also  while the computation
increases by a factor of K  it is easy to see that one can instead use an exponentially spaced grid of
size log2(K) to achieve regret of the same order.

Follow the Leader. FTL might be the most natural algorithm proposed in online learning. In
round t the algorithm plays the expert with the lowest cumulative loss up to round t  L∗
t−1. By
setting η = ∞ in Hedge and similarly  in HPU and HPK  we recover FTL; hence  our algorithms
can simulate FTL. The motivation for FTL is that it achieves constant regret (with respect to T )
when the losses are i.i.d. stochastic and there is a gap in mean loss between the best and second best
(permutation) experts. Our algorithms do not maintain L∗
t to implement AdaHedge
(which we discuss in the next extension). Here  we propose a simple algorithm to perform FTL on
the set of orderings. The algorithm works as follows:

t   but we need L∗

8

1. Perform as FTL on alive initial experts and keep track of their cumulative losses

(Lt

1  Lt

2  . . .   Lt

K)  while ignoring the dead experts;

2. If expert j dies in round t(cid:48)  then for every alive expert i where Lt(cid:48)

i > Lt(cid:48)
This not only performs the same as FTL but also explicitly keeps track of L∗
implementation to simulate AdaHedge.

:= Lt(cid:48)
j .

j do: Lt(cid:48)
t . We will use this

i

ˆLt =(cid:80)t
and ∆t = ˆLt − Mt where Mt =(cid:80)t

AdaHedge. The following change to the learning rate in HPU/HPK recovers AdaHedge. Let
ˆ(cid:96)r. For round t  AdaHedge on N experts sets the learning rate as ηt = (ln N )/∆t−1
ln(wr · e−ηr(cid:96)r ); here  mr can easily be
computed using the weights from HPU/HPK. As we have the loss of the algorithm at each round 
we can calculate Mt. Also  using the implementation of FTL describe above  we can maintain L∗
t .
Finally  we can compute ∆t and the regret of HPU/HPK.

r=1 mr and mr = − 1

r=1

ηr

FlipFlop. By combining AdaHedge and FTL  [5] proposes FlipFlop which can do as well as either
of AdaHedge (minimax guarantees and more) or FTL (for the stochastic i.i.d. case). We can adapt
HPK and HPU to FlipFlop by implementing AdaHedge and FTL as described above and switching
between the two based on ∆ah
t but the learning rate
associated with mt for FTL is ηftl = ∞ while for AdaHedge it is ηah
Corollary 5.1. By combining FTL and AdaHedge as described above  HPU and HPK simulate
FlipFlop over set of experts A (where A = Π for HPU and A = E for HPK) and achieve regret

is deﬁned similar to ∆ah
.
t = ln K
∆t−1

t   where ∆ftl
t

t and ∆ftl

(cid:40)

(cid:114)

T (T − L∗
L∗
T )

T

ln (|A|) + C3 ln (|A|)

 

(cid:41)

RA(1  T ) < min

C0Rftl

A (1  T ) + C1  C2

where C0  C1  C2  C3 are constants.

The interest in FlipFlop is that in the real-world we may not know if losses are stochastic or adversarial.
This motivates one to use an algorithms that detect and adapt to easier situations.

6 Conclusion

√

In this work  we introduced the dying experts setting. We presented matching upper and lower
bounds on the ranking regret for both the cases of known and unknown order of dying. In the case
of known order  we saw that the reduction in the number of effective orderings allows our bounds
to be reduced by a
log K factor. While it appears to be computationally hard to obtain sublinear
regret in the general sleeping experts problem  in the restricted dying experts setting we provided
efﬁcient algorithms with optimal regret bounds for both cases. Furthermore  we proposed an efﬁcient
implementation of FTL for dying experts which  combined with efﬁciently maintaining mix losses 
enabled us to extend our algorithms to simulate AdaHedge and FlipFlop. It would be interesting to
see if the notion of effective experts can be extended to other settings such as multi-armed bandits.
Furthermore  it might be interesting to study the problem in regimes in between known and unknown
order.

Acknowledgments

This work was supported by the NSERC Discovery Grant RGPIN-2018-03942.

References
[1] Avrim Blum. Empirical support for winnow and weighted-majority algorithms: Results on a

calendar scheduling domain. Machine Learning  26(1):5–23  1997.

[2] Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning

Research  8(Jun):1307–1324  2007.

[3] Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction  Learning  and Games. Cambridge

University Press  2006.

9

[4] Deepayan Chakrabarti  Ravi Kumar  Filip Radlinski  and Eli Upfal. Mortal multi-armed bandits.

In Advances in neural information processing systems  pages 273–280  2009.

[5] Steven de Rooij  Tim van Erven  Peter D Grünwald  and Wouter M Koolen. Follow the leader
if you can  hedge if you must. The Journal of Machine Learning Research  15(1):1281–1316 
2014.

[6] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of computer and system sciences  55(1):119–139  1997.

[7] Yoav Freund  Robert E Schapire  Yoram Singer  and Manfred K Warmuth. Using and combining
predictors that specialize. In In Proceedings of the Twenty-Ninth Annual ACM Symposium on
the Theory of Computing. Citeseer  1997.

[8] Eyal Gofer  Nicolo Cesa-Bianchi  Claudio Gentile  and Yishay Mansour. Regret minimization

for branching experts. In Conference on Learning Theory  pages 618–638  2013.

[9] Satyen Kale  Chansoo Lee  and Dávid Pál. Hardness of online sleeping combinatorial opti-
mization problems. In Advances in Neural Information Processing Systems  pages 2181–2189 
2016.

[10] Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. ACM Transactions

on Computation Theory (TOCT)  6(3):11  2014.

[11] Varun Kanade  H Brendan McMahan  and Brent Bryan. Sleeping experts and bandits with
stochastic action availability and adversarial rewards. In Proceedings of the International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS). JMLR Workshop and Conference
Proceedings Volume 5  2009.

[12] Robert Kleinberg  Alexandru Niculescu-Mizil  and Yogeshwer Sharma. Regret bounds for

sleeping experts and bandits. Machine learning  80(2-3):245–272  2010.

[13] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and

computation  108(2):212–261  1994.

[14] Jaouad Mourtada and Odalric-Ambrym Maillard. Efﬁcient tracking of a growing number of
experts. In International Conference on Algorithmic Learning Theory  pages 517–539  2017.

[15] Gergely Neu and Michal Valko. Online combinatorial optimization with stochastic decision
sets and adversarial losses. In Advances in Neural Information Processing Systems  pages
2780–2788  2014.

[16] Francesco Orabona and Dávid Pál. Optimal non-asymptotic lower bound on the minimax regret

of learning with expert advice. arXiv preprint arXiv:1511.02176  2015.

[17] Michel Talagrand. Regularity of inﬁnitely divisible processes. The Annals of Probability  21(1):

362–432  1993.

[18] Michel Talagrand. The generic chaining  volume 154. Springer  2005.

[19] Tim van Erven  Wouter M Koolen  Steven de Rooij  and Peter Grünwald. Adaptive Hedge. In

Advances in Neural Information Processing Systems  pages 1656–1664  2011.

[20] Vladimir Vovk. Aggregating strategies.

In Proceedings of the third annual workshop on

Computational learning theory  pages 371–386. Morgan Kaufmann Publishers Inc.  1990.

[21] Vladimir Vovk. A game of prediction with expert advice. Journal of Computer and System

Sciences  56(2):153–173  1998.

10

,Hamid Shayestehmanesh
Sajjad Azami
Nishant Mehta