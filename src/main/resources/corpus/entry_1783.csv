2019,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy,We give a simple  computationally efficient  and node-differentially-private algorithm for estimating the parameter of an Erdos-Renyi graph---that is  estimating p in a G(n p)---with near-optimal accuracy.  Our algorithm nearly matches the information-theoretically optimal exponential-time algorithm for the same problem due to Borgs et al. (FOCS 2018).  More generally  we give an optimal  computationally efficient  private algorithm for estimating the edge-density of any graph whose degree distribution is concentrated in a small interval.,Efﬁciently Estimating Erd˝os-Rényi Graphs

with Node Differential Privacy

Adam Sealfon

MIT and UC Berkeley
asealfon@berkeley.edu

Jonathan Ullman

Northeastern University
jullman@ccs.neu.edu

Abstract

We give a simple  computationally efﬁcient  and node-differentially-private algo-
rithm for estimating the parameter of an Erd˝os-Rényi graph—that is  estimating
p in a G(n  p)—with near-optimal accuracy. Our algorithm nearly matches the
information-theoretically optimal exponential-time algorithm for the same problem
due to Borgs et al. (FOCS 2018). More generally  we give an optimal  computa-
tionally efﬁcient  private algorithm for estimating the edge-density of any graph
whose degree distribution is concentrated in a small interval.

1

Introduction

Network data modeling individuals and relationships between individuals are increasingly central in
data science. As some of the most interesting network datasets include sensitive information about
individuals  there is a need for private methods for analysis of these datasets  ideally satisfying strong
mathematical guarantees like differential privacy [9]. However  while there is a highly successful
literature on differentially private statistical estimation for traditional i.i.d. data  the literature on
estimating network statistics is far less developed.
Early work on private network data focused on edge differential privacy  in which the algorithm is
required to “hide” the presence or absence of a single edge in the graph (e.g. [20  14  16  13  1  22  17]
and many more). A more desirable notion of privacy  which is the focus of this work  is node
differential privacy (node-DP)  which requires the algorithm to hide the presence or absence of a
single node and the (arbitrary) set of edges incident to that node.
However  node-DP is often difﬁcult to achieve without compromising accuracy  because even very
simple graph statistics can be highly sensitive to adding or removing a single node. For example 
the count of edges in the graph  |E|  can change by ±n by adding or deleting a single node from an
n-node graph  which means that no node-DP algorithm can count the number of edges with error o(n)
on a worst-case graph. We emphasize that even these simple statistics like the edge count can disclose
sensitive information if no steps are taken to ensure privacy  especially when we release many such
statistics on related graphs. There has been an enormous body of work that has uncovered the privacy
risks of releasing simple statistics like counts in the i.i.d. setting (e.g. [8  10  12  15  19  5  11]) and
the additional graph structure only makes these risks more acute.
Although node-DP is difﬁcult to achieve on worst-case graphs  the beautiful works of Blocki et
al. [2] and Kasiviswanathan et al. [18] showed how to design node-DP estimators that are highly
accurate on “nice” graphs that have additional properties observed in practice—for example  graphs
with small maximum degree—using the technique of Lipschitz extensions. However  many of the
known constructions of Lipschitz extensions require exponential running time  and constructions of
computationally efﬁcient Lipschitz extensions [21  7  6] lag behind. As a result  even for estimating
very simple graph models  there are large gaps in accuracy between the best known computationally
efﬁcient algorithms and the information theoretically optimal algorithms.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this work we focus on arguably the simplest graph statistic  the edge count  |E|  in undirected
unweighted graphs. We give improved estimators for this quantity on concentrated-degree graphs.
Intuitively  a concentrated-degree graph is one in which the degree of every node lies in some small
(but not publicly known) range [ ¯d−k  ¯d+k]  which generalizes the case of graphs with low maximum
degree. We give a simple  polynomial-time node-DP algorithm with optimal accuracy for estimating
the count of edges in concentrated-degree graphs. Our estimator is inspired by Lipschitz extensions 
but avoids directly constructing an efﬁcient Lipschitz extension  and thus our approach may be useful
for computing other graph statistics in settings where efﬁcient Lipschitz extensions are unknown or
unachievable.
The main application of this estimator is to estimate the parameter for the simplest possible network
model  the Erd˝os-Rényi graph. In this model  denoted G(n  p)  we are given a number of nodes n
and a parameter p ∈ [0  1]  and we sample an n-node graph G by independently including each edge
(i  j) for 1 ≤ i < j ≤ n with probability p. The goal is to design a node-DP algorithm that takes as
input a graph G ∼ G(n  p) and outputs an estimate ˆp ≈ p. Surprisingly  until the elegant recent work
of Borgs et al. [3]  the optimal accuracy for estimating the parameter p in a G(n  p) via node-DP
algorithms was unknown. Although that work essentially resolved the optimal accuracy of node-DP
algorithms  their construction is again based on generic Lipschitz extensions  and thus results in an
exponential-time algorithm  and  in our opinion  gives little insight for how to construct an efﬁcient
estimator with similar accuracy. Erd˝os-Rényi graphs automatically satisfy the concentrated-degree
property with high probability  and thus we immediately obtain a computationally efﬁcient  node-DP
estimator for Erd˝os-Rényi graphs. The error of our estimator nearly matches that of Borgs et al.  and
indeed does match it for a wide range of parameters.

1.1 Background: Node-Private Algorithms for Erd˝os-Rényi Graphs

Without privacy  the optimal estimator is simply to output the edge-density pG = |E|/(cid:0)n

(cid:1) of the

2

realized graph G ∼ G(n  p)  which guarantees that

(cid:2)(p − pG)2(cid:3) =

E
G

p(1 − p)

(cid:0)n

(cid:1)

2

.

The simplest way to achieve ε-node-DP is to add zero-mean noise to the edge-density with standard-
deviation calibrated to its global-sensitivity  which is the amount that changing the neighborhood of a
single node in a graph can change its edge-density. The global sensitivity of pG is Θ(1/n)  and thus
the resulting private algorithm Anaïve satisﬁes

(cid:2)(p − Anaïve(G))2(cid:3) = Θ(1/ε2n2).

E
G

Note that this error is on the same order as or larger than the non-private error.
Borgs et al. [3] gave an improved ε-node-DP algorithm such that  when both p and ε are (cid:38) log n
n  

E(cid:2)(p − Abcsz(G))2(cid:3) =

p(1 − p)

(cid:0)n
(cid:1)
(cid:123)(cid:122)

2

(cid:125)

(cid:124)

non-private error

+

(cid:16) p
(cid:123)(cid:122)

˜O

(cid:124)

ε2n3

(cid:17)
(cid:125)

overhead due to privacy

What is remarkable about their algorithm is that  unless ε is quite small (roughly ε (cid:46) n−1/2)  the ﬁrst
term dominates the error  in which case privacy comes essentially for free. That is  the error of the
private algorithm is only larger than that of the optimal non-private algorithm by a 1 + o(1) factor.
However  as we discussed above  this algorithm is not computationally efﬁcient.
The only computationally efﬁcient node-DP algorithms for computing the edge-density apply to
graphs with small maximum degree [2  18  21]  and thus do not give optimal estimators for Erd˝os-
Rényi graphs unless p is very small.

1.2 Our Results

Our main result is a computationally efﬁcient estimator for Erd˝os-Rényi graphs.

2

Theorem 1.1 (Erd˝os-Rényi Graphs  Informal). There is an O(n2)-time ε-node-DP algorithm A such
that for every n and every p (cid:38) 1/n  if G ∼ G(n  p)  then

(cid:2)(p − A(G))2(cid:3) =

E
G A

(cid:18) p

p(1 − p)

(cid:0)n
(cid:1)
(cid:123)(cid:122)

2

(cid:124)

(cid:125)

+ ˜O

(cid:124)

ε2n3 +

1

ε4n4

(cid:123)(cid:122)

non-private error

overhead due to privacy

(cid:19)
(cid:125)

The error of Theorem 1.1 matches that of the exponential-time estimator of Borgs et al. [3] up to the
additive ˜O(1/ε4n4) term  which is often not the dominant term in the overall error. In particular  the
error of our estimator is still within a 1 + o(1) factor of the optimal non-private error unless ε or p is
quite small—for example  when p is a constant and ε (cid:38) n−1/2.
Our estimator actually approximates the edge density for a signiﬁcantly more general class of graphs
than merely Erd˝os-Rényi graphs. Speciﬁcally  Theorem 1.1 follows from a more general result for
the family of concentrated-degree graphs. For k ∈ N  deﬁne Gn k to be the set of n-node graphs such
that the degree of every node is between ¯d − k and ¯d + k  where ¯d = 2|E|/n is the average degree of
the graph.
Theorem 1.2 (Concentrated-Degree Graphs  Informal). For every k ∈ N  there is an O(n2)-time
ε-node-DP algorithm A such that for every n and every G ∈ Gn k 
1

(cid:19)

(cid:104)

(pG − A(G))2(cid:105)

(cid:18) k2
(cid:1) is the empirical edge density of G.

= O

E
A

where pG = |E|/(cid:0)n

2

ε2n4 +

ε4n4

√

Theorem 1.1 follows from Theorem 1.2 by using the fact that for an Erd˝os-Rényi graph  with
overwhelming probability the degree of every node lies in an interval of width ˜O(
pn) around the
average degree.
The main technical ingredient in Theorem 1.2 is to construct a low sensitivity estimator f (G) for
the number of edges. The ﬁrst property we need is that when G satisﬁes the concentrated degree
property  f (G) equals the number of edges in G. The second property of the estimator we construct
is that its smooth sensitivity [20] is low on these graphs G. At a high level  the smooth sensitivity
of f at a graph G is the most that changing the neighborhood of a small number of nodes in G can
change the value of f (G). Once we have this property  it is sufﬁcient to add noise to f (G) calibrated
to its smooth sensitivity. We construct f by carefully reweighting edges that are incident on nodes
that do not satisfy the concentrated-degree condition.
Finally  we are able to show that Theorem 1.2 is optimal for concentrated-degree graphs. In additional
to being a natural class of graphs in its own right  this lower bound demonstrates that in order to
improve Theorem 1.1  we will need techniques that are more specialized to Erd˝os-Rényi graphs.
Theorem 1.3 (Lower Bound  Informal). For every n and k  and every ε-node-DP algorithm A  there
is some G ∈ Gn k such that E
. The same bound applies to
= Ω
(ε  δ)-node-DP algorithms with sufﬁciently small δ (cid:46) ε.

(pG − A(G))2(cid:105)

(cid:16) k2

ε2n4 + 1
ε4n4

(cid:17)

(cid:104)

A

2 Preliminaries
Let Gn be the set of n-node graphs. We say that two graphs G  G(cid:48) ∈ Gn are node-adjacent  denoted
G ∼ G(cid:48)  if G(cid:48) can be obtained by G modifying the neighborhood of a single node i. That is  there
exists a single node i such that for every edge e in the symmetric difference of G and G(cid:48)  e is incident
on i. As is standard in the literature on differential privacy  we treat n as a ﬁxed quantity and deﬁne
adjacency only for graphs with the same number of nodes. We could easily extend our deﬁnition of
adjacency to include adding or deleting a single node itself.
Deﬁnition 2.1 (Differential Privacy [9]). A randomized algorithm A : Gn → R is (ε  δ)-node-
differentially private if for every G ∼ G(cid:48) ∈ Gn and every R ⊆ R 

P[A(G) ∈ R] ≤ eε · P[A(G(cid:48)) ∈ R] + δ.

If δ = 0 we will simply say that A is ε-node-differentially private. As we only consider node
differential privacy in this work  we will frequently simply say that A satisﬁes differential privacy.

3

The next lemma is the basic composition property of differential privacy.
Lemma 2.2 (Composition [9]). If A1 A2 : Gn → R are each (ε  δ)-node-differentially private
algorithms  then the mechanism A(G) = (A1(G) A2(G)) satisﬁes (2ε  2δ)-node-differential privacy.
The same holds if A2 may depend on the output of A1.
We will say that two graphs G  G(cid:48) are at node distance c if there exists a sequence of graphs
G = G0 ∼ G1 ∼ ··· ∼ Gc = G(cid:48). The standard group privacy property of differential privacy yields
the following guarantees for graphs at node distance c > 1.
Lemma 2.3 (Group Privacy [9]). If A : Gn → R is (ε  δ)-node-differentially private and G  G(cid:48) are
at node-distance c  then for every R ⊆ R 

P[A(G) ∈ R] ≤ ecε · P[A(G(cid:48)) ∈ R] + cecεδ.

Sensitivity and Basic DP Mechanisms. The main differentially private primitive we will use is
smooth sensitivity [20]. Let f : Gn → R be a real-valued function. For a graph G ∈ Gn  we can
deﬁne the local sensitivity of f at G and the global sensitivity of f to be

LS f (G) = max

G(cid:48):G(cid:48)∼G

|f (G) − f (G(cid:48))|

and GS f = max
G

LS f (G) = max
G(cid:48)∼G

|f (G) − f (G(cid:48))|.

A basic result in differential privacy says that we can achieve privacy for any real-valued function f
by adding noise calibrated to the global sensitivity of f.
Theorem 2.4 (DP via Global Sensitivity [9]). Let f : Gn → R be any function. Then the algorithm
A(G) = f (G) + GS f
· Z  where Z is sampled from a standard Laplace distribution 1 satisﬁes
(ε  0)-differential privacy. Moreover  this mechanism satisﬁes E
A
A[|A(G) − f (G)| ≥ t · GS f /ε] ≤ exp(−t).
and for every t > 0  P

(cid:2)(A(G) − f (G))2(cid:3) = O(GS f /ε) 

ε

In many cases the global sensitivity of f is too high  and we want to use a more reﬁned mechanism
that adds instance-dependent noise that is more comparable to the local sensitivity. This can be
achieved via the smooth sensitivity framework of Nissim et al. [20].
Deﬁnition 2.5 (Smooth Upper Bound [20]). Let f : Gn → R be a real-valued function and β > 0
be a parameter. A function S : Gn → R is a β-smooth upper bound on LS f if

1. for all G ∈ Gn  S(G) ≥ LSf (G)  and
2. for all neighboring G ∼ G(cid:48) ∈ Gn  S(G) ≤ eβ · S(G(cid:48)).

The key result in smooth sensitivity is that we can achieve differential privacy by adding noise to
f (G) proportional to any smooth upper bound S(G).
Theorem 2.6 (DP via Smooth Sensitivity [20  4]). Let f : Gn → R be any function and S be a
β-smooth upper bound on the local sensitivity of f for any β ≤ ε. Then the algorithm A(G) =
· Z  where Z is sampled from a Student’s t-distribution with 3 degrees of freedom 2
f (G) + S(G)
satisﬁes (O(ε)  0)-differential privacy.
Moreover  for any G ∈ Gn  this algorithm satisﬁes E
A

(cid:2)(A(G) − f (G))2(cid:3) = O(S(G)2/ε2).

ε

3 An Estimator for Concentrated-Degree Graphs

3.1 The Estimator

In order to describe the estimator we introduce some key notation. The input to the estimator is a
graph G = (V  E) and a parameter k∗. Intuitively  k∗ should be an upper bound on the concentration

1The standard Laplace distribution Z has E[Z] = 0  E(cid:2)Z 2(cid:3) = 2  and density µ(z) ∝ e−|z|.
X  Y1  Y2  Y3 ∼ N (0  1) independently from a standard normal and returning Z = X/(cid:112)Y 2
This distribution has E[Z] = 0 and E(cid:2)Z 2(cid:3) = 3  and its density is µ(z) ∝ 1/(1 + z2)2.

2The Student’s t-distribution with 3 degrees of freedom can be efﬁciently sampled by choosing
3 .
2 + Y 2

1 + Y 2

4

k∗).

Let pG = 1
(n
2)

Algorithm 1: Estimating the edge density of a concentrated-degree graph.
Input: A graph G ∈ Gn and parameters ε > 0 and k∗ ≥ 0.
Output: A parameter 0 ≤ ˆp ≤ 1.

(cid:80)
e xe and ¯dG = (n − 1)pG.
√
Let β = min(ε  1/
Let kG > 0 be the smallest positive integer such that at most kG vertices have degree outside
[ ¯dG − k∗ − 3kG  ¯dG + k∗ + 3kG].
For v ∈ V   let tv = min{|t| : degG(v) ± t ∈ [ ¯dG − k∗ − 3kG  ¯dG + k∗ + 3kG]} and let
wtG(v) = max(0  1 − βtv).
For each u  v ∈ V   let wtG({u  v}) = min(wtG(u)  wtG(v)) and let
valG(e) = wtG(e) · xe + (1 − wtG(e))pG.
Let f (G) =

valG({u  v})  where the sum is over unordered pairs of vertices.

(cid:88)

u(cid:54)=v

Let

s = max
(cid:96)∈L

210 · e−β(cid:96) · (kG + (cid:96) + k∗ + β(kG + (cid:96))(kG + (cid:96) + k∗) + 1/β) 

where L = {0 (cid:98)1/β − kG − k∗(cid:99) (cid:100)1/β − kG − k∗(cid:101)}.
Return 1
(n
2)
degrees of freedom.

· (f (G) + (s/ε) · Z)  where Z is sampled from a Student’s t-distribution with three

parameter of the graph  although we obtain more general results when k∗ is not an upper bound  in
case the user does not have an a priori upper bound on this quantity.

(cid:1) be the empirical edge density of G  and let ¯dG =

For a graph G = (V  E)  let pG = |E|/(cid:0)n

(n − 1)pG be the empirical average degree of G. Let kG be the smallest positive integer value such
G := k∗ + 3kG. Deﬁne
that at most kG vertices of G have degree differing from ¯dG by more than k(cid:48)
IG = [ ¯dG − k(cid:48)
G]. For each vertex v ∈ V   let tv = min{|t| : degG(v) ± t ∈ IG} be the
distance between degG(v) and the interval IG  and deﬁne the weight wtG(v) of v as follows. For a
parameter β > 0 to be speciﬁed later  let

G  ¯dG + k(cid:48)

2

1

wtG(v) =

1 − βtv
0

if tv = 0
if tv ∈ (0  1/β]
otherwise.

That is  wtG(v) = max(0  1 − βtv). For each pair of vertices e = {u  v}  deﬁne the weight wtG(e)
and value valG(e) as follows. Let

and

valG(e) = wtG(e) · xe + (1 − wtG(e)) · pG 

wtG(e) = min(wtG(u)  wtG(v))

(cid:80)
where xe denotes the indicator variable on whether e ∈ E. Deﬁne the function f (G) =
u v∈V valG({u  v}) to be the total value of all pairs of vertices in the graph  where the sum

is over unordered pairs of distinct vertices.
Once we construct this function f  we add noise to f proportional to a β-smooth upper bound on the
sensitivity of f  which we derive in this section. Pseudocode for our estimator is given in Algorithm 1.

3.2 Analysis Using Smooth Sensitivity

We begin by bounding the local sensitivity LSf (G) of the function f deﬁned above.
Lemma 3.1. For β = Ω(1/n)  we have that LSf (G) = O((kG + k∗)(1 + βkG) + 1
for β ∈ [1/n  1]  we have LSf (G) < 210((kG + k∗)(1 + βkG) + 1/β).

β ). In particular 

5

n < 2

Proof. Consider any pair of graphs G  G(cid:48) differing in only a single vertex v∗  and note that the
n−1  so ¯dG and ¯dG(cid:48) can differ by
empirical edge densities pG and pG(cid:48) can differ by at most 2
at most 2. Moreover  for any vertex v (cid:54)= v∗  the degree of v can differ by at most 1 between G
and G(cid:48). Consequently  by the Triangle Inequality  for any v (cid:54)= v∗  | ¯dG − degG(v)| can differ from
| ¯dG(cid:48) − degG(cid:48)(v)| by at most 3 and |kG − kG(cid:48)| ≤ 1  so wtG(v) can differ from wtG(cid:48)(v) by at most
6β.
Let FarG denote the set of at most kG vertices whose degree differs from ¯dG by more than k(cid:48)
G =
k∗ + 3kG. For any vertices u  v /∈ FarG ∪ FarG(cid:48) ∪ {v∗}  we have wtG({u  v}) = wtG(cid:48)({u  v}) = 1 
so valG({u  v}) = valG(cid:48)({u  v})  since the edge {u  v} appears in G if and only if it appears in G(cid:48).
Now consider edges {u  v} such that u  v (cid:54)= v∗ but u ∈ FarG ∪ FarG(cid:48) (and v may or may not be as
well). If degG(u) /∈ [ ¯dG − k(cid:48)(cid:48)
G + 1/β + 3  then wtG(u) = wtG(cid:48)(u) = 0 and
so |valG({u  v})− valG(cid:48)({u  v})| = |pG − pG(cid:48)| ≤ 2/n. Otherwise  degG(u) ∈ [ ¯dG − k(cid:48)(cid:48)
G  ¯dG + k(cid:48)(cid:48)
G].
We can break up the sum

G  ¯dG + k(cid:48)(cid:48)

G] for k(cid:48)(cid:48)

G = k(cid:48)

fu(G) :=

valG({u  v}) =

wtG({u  v}) · x{u v} +

(1 − wtG({u  v}))pG.

(cid:88)

v(cid:54)=u

(cid:88)

v(cid:54)=u

Since at most kG other vertices can have weight less than that of u  we can bound the ﬁrst term by

wtG(u)x{u v} ± kGwtG(u) = degG(u)wtG(u) ± kGwtG(u)

v(cid:54)=u

(cid:88)
(cid:88)
(n − 1) −(cid:88)

v(cid:54)=u

v(cid:54)=u

and the second term by

pG ·

wtG({u  v})

 = ¯dG − ¯dGwtG(u) ± pGkGwtG(u)

so the total sum is bounded by fu(G) = ¯dG + (degG(u) − ¯dG)wtG(u) ± 2kGwtG(u). Since
|wtG(u) − wtG(cid:48)(u)| ≤ 6β  it follows that

|fu(G) − fu(G(cid:48))| ≤ 7 + 6β(k(cid:48)(cid:48)

G + 3) + 9β + 6βkG

= 13 + 45β + 6β(k∗ + 4kG)
= O(1 + β(kG + k∗)).

G ≤ 2kG + 1 vertices in u ∈ FarG ∪ FarG(cid:48) \ {v∗}  the total difference
Since there are at most kG + k(cid:48)
in the terms of f (G) and f (G(cid:48)) corresponding to such vertices is at most 2kG + 1 times this  which
is O(kG + βkG(kG + k∗)). However  we are double-counting any edges between two vertices in
u ∈ FarG ∪ FarG(cid:48); the number of such edges is at most 2k2
G)  and for any such
edge e  |valG(e) − valG(cid:48)(e)| ≤ 12β + 2/n = O(β + 1/n). Consequently the error induced by this
double-counting is at most (2k2
G/n)  so the total difference
between the terms of f (G) and f (G(cid:48)) corresponding to such vertices is at most

G + k2
13 + 26kG + 45β + 126βkG + 6βk∗ + 12βk∗kG + 72βk2

G + kG)(12β + 2/n)  which is O(βk2

G + kG = O(k2

G + 6k2

G/n 

(cid:88)

which is still O(kG + βkG(kG + k∗)) for β = Ω(1/n).
Finally  consider the edges {u  v∗} involving vertex v∗. If wtG(v∗) = 0 then
valG({v∗  v}) = (n − 1)pG = ¯dG.

(cid:88)
If wtG(v∗) = 1 then degG(v∗) ∈ [ ¯dG − k(cid:48)

G  ¯dG + k(cid:48)

fv∗ (G) =

G]  so

v(cid:54)=v∗

fv∗ (G) =

valG({v∗  v}) = degG(v∗) ± kG = ¯dG ± k(cid:48)
v(cid:54)=v∗
(cid:88)
G − 1/β  ¯dG + k(cid:48)
Otherwise  degG(v∗) ∈ [ ¯dG − k(cid:48)
valG({v∗  v})
= ¯dG + (degG(v∗) − ¯dG)wtG(v∗) ± kGwtG(v∗)
= ¯dG ± (degG(v∗) − ¯dG) ± kG 

G + 1/β]. Then we have that

fv∗ (G) =

v(cid:54)=v∗

G ± kG.

6

so in either case we have that fv∗ (G) ∈ [ ¯dG−(k(cid:48)
G +kG +1/β)  ¯dG +(k(cid:48)
|fv∗ (G) − fv∗ (G(cid:48))| ≤ 3 + 8kG + 2k∗ + 2/β = O(kG + k∗ + 1/β).
Putting everything together  we have that
LSf (G) ≤ 16 + 34kG + 2k∗ + 45β + 126βkG + 6βk∗ + 12βk∗kG + 72βk2
G/n + 2/β 
which is O((kG + k∗)(1 + βkG) + 1/β) for β = Ω(1/n). In particular  for β ∈ [1/n  1]  we have
that LSf (G) ≤ 210((kG + k∗)(1 + βkG) + 1
β ).

G +kG +1/β)]. Consequently

G + 6k2

We now compute a smooth upper bound on LSf (G). Let

g(kG  k∗  β) = 210((kG + k∗)(1 + βkG) + 1
β )

be the upper bound on LSf (G) from Lemma 3.1  and let

S(G) = max
(cid:96)≥0

e−(cid:96)βg(kG + (cid:96)  k∗  β).

Lemma 3.2. S(G) is a β-smooth upper bound on the local sensitivity of f. Moreover  we have the
bound S(G) = O((kG + k∗)(1 + βkG) + 1
β ).
Proof. For neighboring graphs G  G(cid:48)  we have that

S(G(cid:48)) = max
(cid:96)≥0
≤ max
(cid:96)≥0

e−(cid:96)βg(kG(cid:48) + (cid:96)  k∗  β)
e−(cid:96)βg(kG + (cid:96) + 1  k∗  β)
e−(cid:96)βg(kG + (cid:96)  k∗  β)
e−(cid:96)βg(kG + (cid:96)  k∗  β)

= eβ max
(cid:96)≥1
≤ eβ max
(cid:96)≥0

= eβS(G).

Moreover  for ﬁxed kG  k∗  β  consider the function h((cid:96)) = e−(cid:96)βg(kG + (cid:96)  k∗  β)  and consider the
derivative h(cid:48)((cid:96)). We have that h(cid:48)((cid:96)) = 210 · βe−(cid:96)β(kG + (cid:96))(1 − β(kG + (cid:96) + k∗)). Consequently the
only possible local maximum for (cid:96) > 0 would occur for (cid:96) = 1/β − kG − k∗; note that the function h
decreases as (cid:96) → ∞. Consequently the maximum value of h occurs for some (cid:96) ≤ 1/β  and so we
can show by calculation that S(G) < 630 · ((kG + k∗)(1 + βkG) + 1
Remark. Note that S(G) can be computed efﬁciently  since (cid:96) can be restricted to the nonnegative
integers and so the only candidate values for (cid:96) are 0  (cid:98)1/β − kG − k∗(cid:99)  and (cid:100)1/β − kG − k∗(cid:101).
Theorem 3.3. Algorithm 1 is (O(ε)  0)-differentially private for ε ≥ 1/n. Moreover  for any
k-concentrated n-vertex graph G = (V  E) with k ≥ 1  we have that Algorithm 1 satisﬁes

β ) as desired.

(cid:32)|E|(cid:0)n
(cid:1) − Aε k(G)

(cid:33)2 = O

(cid:18) k2

E
A

2

ε2n4 +

1

ε4n4

(cid:19)

Proof. Algorithm 1 computes function f and releases it with noise proportional to a β-smooth
upper bound on the local sensitivity for β ≤ ε. Consequently (O(ε)  0)-differential privacy follows
immediately from Theorem 2.6.
We now analyze its accuracy on k-concentrated graphs G. If G is k-concentrated and k∗ ≥ k  then
wtG(v) = 1 for all vertices v ∈ V and valG({u  v}) = x{u v} for all u  v ∈ V   and so f (G) = |E|.
Consequently Algorithm 1 computes the edge density of a k-concentrated graph with noise distributed

according to the Student’s t-distribution scaled by a factor of S(G)/(ε(cid:0)n
(cid:19)

Since G is k-concentrated  we also have that kG = 1  and so S(G) = O(k + β(k + 1) + 1/β) ≤
O(k + 1/ε) by Lemma 3.2. The variance of the Student’s t-distribution with three degrees of freedom
is O(1)  so the expected squared error of the algorithm is

(cid:18) (k + 1/ε)2

(cid:18) k2

(cid:1)).

(cid:19)

2

O

as desired.

ε2n4

1

ε2n2 +

ε4n4

= O

7

4 Application to Erd˝os-Rényi Graphs

In this section we show how to apply Algorithm 1 to estimate the parameter of an Erd˝os-Rényi graph.

Algorithm 2: Estimating the parameter of an Erd˝os-Rényi graph.
Input: A graph G ∈ Gn and parameters ε  α > 0.
Output: A parameter 0 ≤ ˆp ≤ 1.
Let ˜p(cid:48) ← 1
(n
2)

(cid:80)
e xe + (2/εn) · Z where Z is a standard Laplace

Let ˜p ← ˜p(cid:48) + 4 log(1/α)/εn and ˜k ←(cid:112)˜pn log(n/α)

Return ˆp ← A˜k ε(G) where A˜k ε is Algorithm 1 with parameters ˜k and ε

It is straightforward to prove that this mechanism satisﬁes differential privacy.
Theorem 4.1. Algorithm 2 satisﬁes (O(ε)  0)-node-differential privacy for ε ≥ 1/n.

Proof. The ﬁrst line computes the empirical edge density of the graph G  which is a function with

(cid:1) = 2/n. Therefore by Theorem 2.4 this step satisﬁes (ε  0)-differential

global sensitivity (n − 1)/(cid:0)n

privacy. The third line runs an algorithm that satisﬁes (O(ε)  0)-differential privacy for every ﬁxed
parameter ˜k. By Lemma 2.2  the composition satisﬁes (O(ε)  0)-differential privacy.

2

Next  we argue that this algorithm satisﬁes the desired accuracy guarantee.
Theorem 4.2. For every n ∈ N and 1
satisﬁes

2 ≥ p ≥ 0  and an appropriate parameter α > 0  Algorithm 2

(cid:2)(p − A(G))2(cid:3) =

p(1 − p)

(cid:0)n

(cid:1) + ˜O

2

(cid:18) max{p  1
n}

ε2n3

(cid:19)

+

1

ε4n4

E

G∼G(n p) A

Proof. We will prove the result in the case where p ≥ log n
n . The case where p is smaller will
follow immediately by using log n
n as an upper bound on p. The ﬁrst term in the bound is simply the
variance of the empirical edge-density ¯p. For the remainder of the proof we will focus on bounding

E(cid:2)(¯p − ˆp)2(cid:3).
2 log(1/α)/n  and (2) the degree of every node i lies in the interval [ ¯d ±(cid:112)pn log(n/α)] where ¯d is

is that with probability at least 1 − 2α: (1) |¯p − p| ≤

A basic fact about G(n  p) for p ≥ log n

the average degree of G. We will assume for the remainder that these events hold.
Using Theorem 2.4  we also have that with probability at least 1 − α  the estimate ˜p(cid:48) satisﬁes
|¯p − ˜p(cid:48)| ≤ 4 log(1/α)/εn. We will also assume for the remainder that this latter event holds.
Therefore  we have p ≤ ˜p and p ≥ ˜p − 8 log(1/α)/εn.
Assuming this condition holds  the graph will have ˜k concentrated degrees for ˜k as speciﬁed on line 2
of the algorithm. Since this assumption holds  we have

n

E(cid:104)
(¯p − A˜k ε(G))2(cid:105)

(cid:32) ˜k2

(cid:33)

(cid:18) pn + 1

(cid:19)

(cid:18) pn

(cid:19)

= ˜O

ε2n4 +

1

ε4n4

= ˜O

εn

ε2n4 +

1

ε4n4

= ˜O

ε2n4 +

1

ε4n4

To complete the proof  we can plug in a suitably small α = 1/poly(n) so that the O(α) probability
of failure will not affect the overall mean-squared error in a signiﬁcant way.

5 Lower Bounds for Concentrated-Degree Graphs

In this section we prove a lower bound for estimating the number of edges in concentrated-degree
graphs. Theorem 5.1  which lower bounds the mean squared error  follows from Jensen’s Inequality.

8

Theorem 5.1. For every n  k ∈ N  every ε ∈ [ 2
A  there exists G ∈ Gn k such that E

A[|pG − A(G)|] = Ω(cid:0) k

4 ] and δ ≤ ε

n   1

εn2 + 1
ε2n2

(cid:1).

32   and every (ε  δ)-node-DP algorithm

ε from
32 )-node-DP algorithm A  there exists b ∈ {0  1} such that

The proof relies only on the following standard fact about differentially private algorithms.
Lemma 5.2. Suppose there are two graphs G0  G1 ∈ Gn k at node distance at most 1
one another. Then for every (ε  ε
A[|pGb − A(Gb)|] = Ω(|pG0 − pG1|).
E
We will construct two simple pairs of graphs to which we can apply Lemma 5.2.
Lemma 5.3 (Lower bound for large k). For every n  k ∈ N and ε ≥ 2/n  there is a pair of graphs
G0  G1 ∈ Gn k at node distance 1/ε such that |pG0 − pG1| = Ω( k
Proof. Let G0 be the empty graph on n nodes. Note that pG0 = 0  ¯dG0 = 0  and G0 is in Gn k.
ε nodes on the left and n − 1
We construct G1 as follows. Start with the empty bipartite graph with 1
ε
nodes on the right. We connect the ﬁrst node on the left to each of the ﬁrst k nodes on the right  then
the second node on the left to each of the next k nodes on the right and so on  wrapping around to

the ﬁrst node on the right when we run out of nodes. By construction  pG1 = k/ε(cid:0)n

(cid:1)  ¯dG1 = 2k/εn.

εn2 ).

ε nodes has degree exactly k and each of the nodes on the right has degree
εn−1 ± 1 Thus  for n larger than some absolute constant  every degree lies in the

Moreover  each of the ﬁrst 1
n−1/ε ± 1 = k
interval [ ¯dG1 ± k] so we have G1 ∈ Gn k.
Lemma 5.4 (Lower bound for small k). For every n ≥ 4 and ε ∈ [2/n  1/4]  there is a pair of
graphs G0  G1 ∈ Gn 1 at node distance 1/ε such that |pG0 − pG1| = Ω( 1
Proof. Let i = (cid:100)nε(cid:101)  and let G0 be the graph consisting of i disjoint cliques each of size (cid:98)n/i(cid:99) or
(cid:100)n/i(cid:101). Let G1 be the graph consisting of i + 1 disjoint cliques each of size (cid:98)n/(i + 1)(cid:99) or (cid:100)n/(i + 1)(cid:101).
We can obtain G0 from G1 by taking one of the cliques and redistributing its vertices among the i
remaining cliques  so G0 and G1 have node distance (cid:96) := (cid:98)n/(i + 1)(cid:99) ≤ 1/ε. For 1/4 ≥ ε ≥ 2/n
we have that (cid:96) ≥ (cid:98)1/2ε(cid:99) > 1/4ε. Transforming G1 into G0 involves removing a clique of size (cid:96) 
at least (cid:96)2 new edges. Consequently G0 contains at least (cid:96)2 − (cid:96)((cid:96) − 1)/2 = (cid:96)((cid:96) + 1)/2 more edges
than G1  so

(cid:1) edges  and then inserting these (cid:96) vertices into cliques that already have size (cid:96)  adding

containing(cid:0)(cid:96)

ε2n2 ).

k/ε

2

2

(cid:0)(cid:96)+1
(cid:1)
(cid:0)n
(cid:1) ≥ (cid:96)2

2

2

n2 ≥ Ω(1/ε2n2) 

|pG1 − pG0| ≥

as desired.

Theorem 5.1 now follows by combining Lemmas 5.2  5.3  and 5.4.

Acknowledgments

Part of this work was done while the authors were visiting the Simons Institute for the Theory of
Computing. AS is supported by NSF MACS CNS-1413920  DARPA/NJIT Palisade 491512803 
Sloan/NJIT 996698  and MIT/IBM W1771646. JU is supported by NSF grants CCF-1718088 
CCF-1750640  and CNS-1816028. The authors are grateful to Adam Smith for helpful discussions.

References
[1] J. Blocki  A. Blum  A. Datta  and O. Sheffet. The johnson-lindenstrauss transform itself
preserves differential privacy. In 53rd IEEE Symposium on Foundations of Computer Science 
FOCS’12  pages 410–419  New Brunswick  NJ  USA  2012.

[2] J. Blocki  A. Blum  A. Datta  and O. Sheffet. Differentially private data analysis of social
In 4th ACM Conference on Innovations in Theoretical

networks via restricted sensitivity.
Computer Science  ITCS ’13  pages 87–96  Berkeley  CA  USA  2013. ACM.

9

[3] C. Borgs  J. T. Chayes  A. D. Smith  and I. Zadik. Revealing network structure  conﬁdentially:
Improved rates for node-private graphon estimation. In 59th Annual IEEE Symposium on
Foundations of Computer Science  FOCS ’18  pages 533–543  Paris  France  2018.

[4] M. Bun and T. Steinke. Smooth sensitivity  revisited. Manuscript  2019.

[5] M. Bun  J. Ullman  and S. Vadhan. Fingerprinting codes and the price of approximate differential
privacy. In 46th Annual ACM Symposium on the Theory of Computing  STOC ’14  pages 1–10 
New York  NY  USA  2014.

[6] C. L. Canonne  G. Kamath  A. McMillan  J. Ullman  and L. Zakynthinou. Private identity

testing for high dimensional distributions. arXiv preprint arXiv:1905.11947  2019.

[7] R. Cummings and D. Durfee. Individual sensitivity preprocessing for data privacy. arXiv

preprint arXiv:1804.08645  2018.

[8] I. Dinur and K. Nissim. Revealing information while preserving privacy. In Proceedings of the
22nd ACM Symposium on Principles of Database Systems  PODS ’03  pages 202–210. ACM 
2003.

[9] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private data
analysis. In Proceedings of the 3rd Conference on Theory of Cryptography  TCC ’06  pages
265–284  Berlin  Heidelberg  2006. Springer.

[10] C. Dwork  F. McSherry  and K. Talwar. The price of privacy and the limits of lp decoding. In
Proceedings of the thirty-ninth annual ACM symposium on Theory of computing  pages 85–94.
ACM  2007.

[11] C. Dwork  A. Smith  T. Steinke  J. Ullman  and S. Vadhan. Robust traceability from trace
amounts. In 56th Annual IEEE Symposium on Foundations of Computer Science  FOCS ’15 
pages 650–669  Berkeley  CA  2015.

[12] C. Dwork and S. Yekhanin. New efﬁcient attacks on statistical disclosure control mechanisms.

In Annual International Cryptology Conference  pages 469–480. Springer  2008.

[13] A. Gupta  A. Roth  and J. Ullman. Iterative constructions and private data release. In 9th IACR
Theory of Cryptography Conference  TCC ’12  pages 339–356  Taormina  Italy  2012. Springer.

[14] M. Hay  C. Li  G. Mikalu  and D. D. Jensen. Accurate estimation of the degree distribution of
private networks. In Proceedings of the 9th IEEE International Confernece on Data Mining 
ICDM’09  pages 169–178  Miami  FL  USA  2009.

[15] N. Homer  S. Szelinger  M. Redman  D. Duggan  W. Tembe  J. Muehling  J. V. Pearson  D. A.
Stephan  S. F. Nelson  and D. W. Craig. Resolving individuals contributing trace amounts
of DNA to highly complex mixtures using high-density SNP genotyping microarrays. PLoS
genetics  4(8):e1000167  2008.

[16] V. Karwa  S. Raskhodnikova  A. D. Smith  and G. Yaroslavtsev. Private analysis of graph

structure. ACM Transactions on Database Systems  39(3):22:1–22:33  2014.

[17] V. Karwa and A. Slavkovi´c. Inference using noisy degrees: Differentially private β-model and

synthetic graphs. Annals of Statistics  44(1):87–112  2016.

[18] S. P. Kasiviswanathan  K. Nissim  S. Raskhodnikova  and A. D. Smith. Analyzing graphs with
node differential privacy. In 10th IACR Theory of Cryptography Conference  TCC ’13  pages
457–476  Tokyo  Japan  2013. Springer.

[19] S. P. Kasiviswanathan  M. Rudelson  A. Smith  and J. Ullman. The price of privately releasing
contingency tables and the spectra of random matrices with correlated rows. In Proceedings of
the 42nd ACM Symposium on Theory of Computing  STOC ’10  pages 775–784. ACM  2010.

[20] K. Nissim  S. Raskhodnikova  and A. Smith. Smooth sensitivity and sampling in private data
analysis. In Proceedings of the 30th annual ACM Symposium on Theory of Computing  STOC 
pages 75–84  2007.

10

[21] S. Raskhodnikova and A. D. Smith. Lipschitz extensions for node-private graph statistics and
the generalized exponential mechanism. In 57th Annual IEEE Symposium on Foundations of
Computer Science  FOCS ’16  pages 495–504  New Brunswick  NJ  USA  2016.

[22] Q. Xiao  R. Chen  and K.-L. Tan. Differentially private network data release via structural
inference. In 20th ACM International Conference on Knowledge Discovery and Data Mining 
KDD’14  pages 911–920  2014.

11

,Eszter Vértes
Maneesh Sahani
Jonathan Ullman
Adam Sealfon