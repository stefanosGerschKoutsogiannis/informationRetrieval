2014,Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models,Sampling from hierarchical Bayesian models is often difficult for MCMC methods  because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting  but are computationally expensive. We introduce a new RMHMC method  which we call semi-separable Hamiltonian Monte Carlo  which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.,Semi-Separable Hamiltonian Monte Carlo

for Inference in Bayesian Hierarchical Models

Yichuan Zhang

School of Informatics
University of Edinburgh

Charles Sutton

School of Informatics
University of Edinburgh

Y.Zhang-60@sms.ed.ac.uk

c.sutton@inf.ed.ac.uk

Abstract

Sampling from hierarchical Bayesian models is often difﬁcult for MCMC meth-
ods  because of the strong correlations between the model parameters and
the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo
(RMHMC) methods have signiﬁcant potential advantages in this setting  but are
computationally expensive. We introduce a new RMHMC method  which we call
semi-separable Hamiltonian Monte Carlo  which uses a specially designed mass
matrix that allows the joint Hamiltonian over model parameters and hyperparam-
eters to decompose into two simpler Hamiltonians. This structure is exploited by
a new integrator which we call the alternating blockwise leapfrog algorithm. The
resulting method can mix faster than simpler Gibbs sampling while being simpler
and more efﬁcient than previous instances of RMHMC.

Introduction

1
Bayesian statistics provides a natural way to manage model complexity and control overﬁtting  with
modern problems involving complicated models with a large number of parameters. One of the
most powerful advantages of the Bayesian approach is hierarchical modeling  which allows partial
pooling across a group of datasets  allowing groups with little data to borrow information from
similar groups with larger amounts of data. However  such models pose problems for Markov chain
Monte Carlo (MCMC) methods  because the joint posterior distribution is often pathological due to
strong correlations between the model parameters and the hyperparameters [3]. For example  one of
the most powerful MCMC methods is Hamiltonian Monte Carlo (HMC). However  for hierarchical
models even the mixing speed of HMC can be unsatisfactory in practice  as has been noted several
times in the literature [3  4  11]. Riemannian manifold Hamiltonian Monte Carlo (RMHMC) [7] is a
recent extension of HMC that aims to efﬁciently sample from challenging posterior distributions by
exploiting local geometric properties of the distribution of interest. However  it is computationally
too expensive to be applicable to large scale problems.
In this work  we propose a simpliﬁed RMHMC method  called Semi-Separable Hamiltonian Monte
Carlo (SSHMC)  in which the joint Hamiltonian over parameters and hyperparameters has special
structure  which we call semi-separability  that allows it to be decomposed into two simpler  separa-
ble Hamiltonians. This condition allows for a new efﬁcient algorithm which we call the alternating
blockwise leapfrog algorithm. Compared to Gibbs sampling  SSHMC can make signiﬁcantly larger
moves in hyperparameter space due to shared terms between the two simple Hamiltonians. Com-
pared to previous RMHMC methods  SSHMC yields simpler and more computationally efﬁcient
samplers for many practical Bayesian models.

2 Hierarchical Bayesian Models
Let D = {Di}N
vations yj = {yji}Ni

i=1 be a collection of data groups where ith data group is a collection of iid obser-
i=1. We assume the data follows a parametric

i=1 and their inputs xj = {xji}Ni

1

N(cid:89)

i=1

distribution p(yi|xi  θi)  where θi is the model parameter for group i. The parameters are assumed
to be drawn from a prior p(θi|φ)  where φ is the hyperparameter with a prior distribution p(φ). The
joint posterior over model parameters θ = (θ1  . . .   θN ) and hyperparameters φ is then

p(θ  φ|D) ∝

p(yi|xi  θi)p(θi|φ)p(φ).

(1)

This hierarchical Bayesian model is popular because the parameters θi for each group are coupled 
allowing the groups to share statistical strength. However  this property causes difﬁculties when
approximating the posterior distribution. In the posterior  the model parameters and hyperparameters
are strongly correlated. In particular  φ usually controls the variance of p(θ|φ) to promote partial
pooling  so the variance of θ|φ D depends strongly on φ. This causes difﬁculties for many MCMC
methods  such as the Gibbs sampler and HMC. An illustrative example of pathological structure
in hierarchical models is the Gaussian funnel distribution [11]. Its density function is deﬁned as
i=1 N (xi|0  e−v)N (v|0  32)  where x is the vector of low-level parameters and v is the
variance hyperparameter. The pathological correlation between x and v is illustrated by Figure 1.

p(x  v) =(cid:81)n

3 Hamiltonian Monte Carlo on Posterior Manifold

Hamiltonian Monte Carlo (HMC) is a gradient-based MCMC method with auxiliary variables. To
generate samples from a target density π(z)  HMC constructs an ergodic Markov chain with the
invariant distribution π(z  r) = π(z)π(r)  where r is an auxiliary variable. The most common
choice of π(r) is a Gaussian distribution N (0  G−1) with precision matrix G. Given the current
sample z  the transition kernel of the HMC chain includes three steps: ﬁrst sample r ∼ π(r) 
second propose a new sample (z(cid:48)  r(cid:48)) by simulating the Hamiltonian dynamics and ﬁnally accept
the proposed sample with probability α = min{1  π(z(cid:48)  r(cid:48))/π(z  r)}  otherwise leave z unchanged.
The last step is a Metropolis-Hastings (MH) correction. Deﬁne H(z  r) := − log π(z  r). The
Hamiltonian dynamics is deﬁned by the differential equations ( ˙z  ˙r) = (∂rH −∂zH)  where z is
called the position and r is called the momentum.
˙H(z  r) = ∂zH ˙z + ∂rH ˙r = 0  which is called the energy preservation property
It is easy to see that
[10  11]. In physics  H(z  r) is known as the Hamiltonian energy  and is decomposed into the sum
of the potential energy U (z) := − log π(z) and the kinetic energy K(r) := − log π(r). The most
used discretized simulation in HMC is the leapfrog algorithm  which is given by the recursion

r(τ + /2) = r(τ ) −


2∇zU (τ )

z(τ + ) = z(τ ) + ∇rK(τ + /2)
r(τ + ) = r(τ + /2) −


2∇θU (τ + ) 

(2a)

(2b)

(2c)

where  is the step size of discretized simulation time. After L steps from the current sample
(z(0)  r(0)) = (z  r)  the new sample is proposed as the last point (z(cid:48)  r(cid:48)) = (z(L)  r(L)). In
Hamiltonian dynamics  the matrix G is called the mass matrix. If G is constant w.r.t. z  then z
and r are independent in π(z  r). In this case we say that H(z  r) is a separable Hamiltonian. In
particular  we use the term standard HMC to refer to HMC using the identity matrix as G. Although
HMC methods often outperform other popular MCMC methods  they may mix slowly if there are
strong correlations between variables in the target distribution. Neal [11] showed that HMC can mix
faster if G is not the identity matrix. Intuitively  such a G acts like a preconditioner. However  if the
curvature of π(z) varies greatly  a global preconditioner can be inadequate.
For this reason  recent work  notably that on Riemannian manifold HMC (RMHMC) [7]  has con-
sidered non-separable Hamiltonian methods  in which G(z) varies with position z  so that z and r
are no longer independent in π(z  r). The resulting Hamiltonian H(z  r) = − log π(z  r) is called
a non-separable Hamiltonian. For example  for Bayesian inference problems  Girolami and Calder-
head [7] proposed using the Fisher Information Matrix (FIM) of π(θ)  which is the metric tensor
of posterior manifold. However  for a non-separable Hamiltonian  the simple leapfrog dynamics
(2a)-(2c) do not yield a valid MCMC method  as they are no longer reversible. Simulation of gen-
eral non-separable systems requires the generalized leapfrog integrator (GLI) [7]  which requires
computing higher order derivatives to solve a system of non-linear differential equations. The com-
putational cost of GLI in general is O(d3) where d is the number of parameters  which is prohibitive
for large d.

2

In hierarchical models  there are two ways to sample the posterior using HMC. One way is to sample
the joint posterior π(θ  φ) directly. The other way is to sample the conditional π(θ|φ) and π(φ|θ) 
simulating from each conditional distribution using HMC. This strategy is called HMC within Gibbs
[11]. In either case  HMC chains tend to mix slowly in hyperparameter space  because the huge vari-
ation of potential energy across different hyperparameter values can easily overwhelm the kinetic
energy in separable HMC [11]. Hierarchical models also pose a challenge to RMHMC  if we want
to sample the model parameters and hyperparameters jointly. In particular  the closed-form FIM
of the joint posterior π(θ  φ) is usually unavailable. Due to this problem  even sampling some toy
models like the Gaussian funnel using RMHMC becomes challenging. Betancourt [2] proposed a
new metric that uses a transformed Hessian matrix of π(θ)  and Betancourt and Girolami [3] demon-
strate the power of this method for efﬁciently sampling hyperparameters of hierarchical models on
some simple benchmarks like Gaussian funnel. However  the transformation requires computing
eigendecomposition of the Hessian matrix  which is infeasible in high dimensions.
Because of these technical difﬁculties  RMHMC for hierarchical models is usually used within a
block Gibbs sampling scheme  alternating between θ and φ. This RMHMC within Gibbs strategy is
useful because the simulation of the non-separable dynamics for the conditional distributions may
have much lower computational cost than that for the joint one. However  as we have discussed  in
hierarchical models these variables tend be very strongly correlated  and it is well-known that Gibbs
samplers mix slowly in such cases [13]. So  the Gibbs scheme limits the true power of RMHMC.

4 Semi-Separable Hamiltonian Monte Carlo

In this section we propose a non-separable HMC method that does not have the limitations of
Gibbs sampling and that scales to relatively high dimensions  based on a novel property that we
will call semi-separability. We introduce new HMC methods that rely on semi-separable Hamilto-
nians  which we call semi-separable Hamiltonian Monte Carlo (SSHMC).

4.1 Semi-Separable Hamiltonian

H(θ  φ  rθ  rφ) = U (θ  φ) + K(rθ  rφ|θ  φ) 

In this section  we deﬁne the semi-separable Hamiltonian system. Our target distribution will be the
posterior π(θ  φ) = log p(θ  φ|D) of a hierarchical model (1)  where θ ∈ Rn and φ ∈ Rm. Let
rθ ∈ Rn and rφ ∈ Rm be the momentum variables corresponding to θ and φ respectively. The
non-separable Hamiltonian is deﬁned as
(3)
where the potential energy is U (θ  φ) = − log π(θ  φ) and the kinetic energy is K(rθ  rφ|θ  φ) =
− log N (rθ  rφ; 0  G(θ  φ)−1)  which includes the normalization term log |G(θ  φ)|. The mass
(cid:16)
matrix G(θ  φ) can be an arbitrary p.d. matrix. For example  previous work on RMHMC [7] has
chosen G(θ  φ) to be FIM of the joint posterior π(θ  φ)  resulting in an HMC method that requires
O
To attack these computational challenges  we introduce restrictions on the mass matrix G(θ  φ) to
enable efﬁcient simulation. In particular  we restrict G(θ  φ) to have the form

time. This limits applications of RMHMC to large scale problems.

(m + n)3(cid:17)

(cid:18) Gθ(φ  x)

0

G(θ  φ) =

(cid:19)

 

0

Gφ(θ)

where Gθ and Gφ are the precision matrices of rθ and rφ  respectively. Importantly  we restrict
Gθ(φ  x) to be independent of θ and Gφ(θ) to be independent of φ. If G has these properties  we
call the resulting Hamiltonian a semi-separable Hamiltonian. A semi-separable Hamiltonian is still
in general non-separable  as the two random vectors (θ  φ) and (rθ  rφ) are not independent.
The semi-separability property has important computational advantages. First  because G is block
diagonal  the cost of matrix operations reduces from O((n + m)k) to O(nk). Second  and more
important  substituting the restricted mass matrix into (3) results in the potential and kinetic energy:
(4)

[log p(yi|θi  xi) + log p(θi|φ)] − log p(φ) 

(cid:88)
(cid:2)rT

i

U (θ  φ) = −
1
2

K(rθ  rφ|φ  θ) =

θ Gθ(x  φ)rθ + rT

φGφ(θ)rφ + log |Gθ(x  φ)| + log |Gφ(θ)|

3

(cid:3) .

(5)

If we ﬁx (θ  rθ) or (φ  rφ)  the non-separable Hamiltonian (3) can be seen as a separable Hamilto-
nian plus some constant terms. In particular  deﬁne the notation
A(rφ|θ) =

rT
θ Gθ(x  φ)rθ 

A(rθ|φ) =

rT
φGφ(θ)rφ.

Then  considering (φ  rφ) as ﬁxed  the non-separable Hamiltonian H in (3) is different from the
following separable Hamiltonian

1
2

1
2

H1(θ  rθ) = U1(θ|φ  rφ) + K1(rθ|φ) 

U1(θ|φ  rφ) = −

[log p(yi|θi  xi) + log p(θi|φ)] + A(rφ|θ) +

1
2

log |Gφ(θ)|  

(6)

(7)

K1(rθ|φ) = A(rθ|φ)

(8)
only by some constant terms that do not depend on (θ  rθ). What this means is that any update to
(θ  rθ) that leaves H1 invariant leaves the joint Hamiltonian H invariant as well. An example is the
leapfrog dynamics on H1  where U1 is considered the potential energy  and K1 the kinetic energy.
Similarly  if (θ  rθ) are ﬁxed  then H differs from the following separable Hamiltonian

(cid:88)

i

(cid:88)

i

H2(φ  rφ) = U2(φ|θ  rθ) + K2(rφ|θ) 
U2(φ|θ  rθ) = −

log p(θi|φ) − log p(φ) + A(rθ|φ) +

1
2

log |Gθ(x  φ)|  

(9)

(10)

(11)

K2(rφ|θ) = A(rφ|θ)

only by terms that are constant with respect to (φ  rφ).
Notice that H1 and H2 are coupled by the terms A(rθ|φ) and A(rφ|θ). Each of these terms appears
in the kinetic energy of one of the separable Hamiltonians  but in the potential energy of the other
one. We call these terms auxiliary potentials because they are potential energy terms introduced by
the auxiliary variables. These auxiliary potentials are key to our method (see Section 4.3).

4.2 Alternating Block-wise Leapfrog Algorithm
Now we introduce an efﬁcient SSHMC method
that exploits the semi-separability property. As
described in the previous section  any update to
(θ  rθ) that leaves H1 invariant also leaves the
joint Hamiltonian H invariant  as does any up-
date to (φ  rφ) that leaves H2 invariant. So a
natural idea is simply to alternate between sim-
ulating the Hamiltonian dynamics for H1 and
that for H2. Crucially  even though the total
Hamiltonian H is not separable in general  both
H1 and H2 are separable. Therefore when sim-
ulating H1 and H2  the simple leapfrog method
can be used  and the more complex GLI method
is not required.
We call this method the alternating block-wise
leapfrog algorithm (ABLA)  shown in Algo-
rithm 1. In this ﬁgure the function “leapfrog”
returns the result of the leapfrog dynamics (2a)-(2c) for the given starting point  Hamiltonian  and
step size. We call each iteration of the loop from 1 . . . L an ABLA step. For simplicity  we have
shown one leapfrog step for H1 and H2 for each ABLA step  but in practice it is useful to use multi-
ple leapfrog steps per ABLA step. ABLA has discretization error due to the leapfrog discretization 
so the MH correction is required. If it is possible to simulate H1 and H2 exactly  then H is preserved
exactly and there is no need for MH correction.
To show that the SSHMC method by ABLA preserves the distribution π(θ  φ)  we also need to
show that the ABLA is a time-reversible and volume-preserving transformation in the joint space of
(θ  rθ  φ  rφ). Let X = Xθ rθ ×Xφ rφ where (θ  rθ) ∈ Xθ rθ and (φ  rφ) ∈ Xφ rφ. Obviously  any
reversible and volume-preserving transformation in a subspace of X is also reversible and volume-
preserving in X .
It is easy to see that each leapfrog step in the ABLA algorithm is reversible
and volume-preserving in either Xθ rθ or Xφ rφ. One more property of integrator of interest is

4

References[1]K.BacheandM.Lichman.UCImachinelearningrepository 2013.URLhttp://archive.ics.uci.edu/ml.[2]M.J.Betancourt.AGeneralMetricforRiemannianManifoldHamiltonianMonteCarlo.ArXive-prints Dec.2012.[3]M.J.BetancourtandM.Girolami.HamiltonianMonteCarloforHierarchicalModels.ArXive-prints Dec.2013.[4]K.Choo.LearninghyperparametersforneuralnetworkmodelsusingHamiltoniandynamics.PhDthesis Citeseer 2000.[5]O.F.Christensen G.O.Roberts andJ.S.Rosenthal.ScalinglimitsforthetransientphaseoflocalMetropolis–Hastingsalgorithms.JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodol-ogy) 67(2):253–268 2005.[6]C.J.Geyer.PracticalMarkovChainMonteCarlo.StatisticalScience pages473–483 1992.[7]M.GirolamiandB.Calderhead.RiemannmanifoldLangevinandHamiltonianMonteCarlomethods.JournaloftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology) 73(2):123–214 2011.ISSN1467-9868.doi:10.1111/j.1467-9868.2010.00765.x.URLhttp://dx.doi.org/10.1111/j.1467-9868.2010.00765.x.[8]M.D.HoffmanandA.Gelman.Theno-U-turnsampler:AdaptivelysettingpathlengthsinHamiltonianMonteCarlo.JournalofMachineLearningResearch Inpress.[9]S.Kim N.Shephard andS.Chib.Stochasticvolatility:likelihoodinferenceandcomparisonwithARCHmodels.TheReviewofEconomicStudies 65(3):361–393 1998.[10]B.LeimkuhlerandS.Reich.SimulatingHamiltoniandynamics volume14.CambridgeUniversityPress 2004.[11]R.Neal.MCMCusingHamiltoniandynamics.HandbookofMarkovChainMonteCarlo pages113–162 2011.[12]A.PakmanandL.Paninski.Auxiliary-variableexacthamiltonianmontecarlosamplersforbinarydistri-butions.InAdvancesinNeuralInformationProcessingSystems26 pages2490–2498.2013.[13]C.P.RobertandG.Casella.MonteCarlostatisticalmethods volume319.Citeseer 2004.[14]Z.Wang S.Mohamed andN.deFreitas.AdaptiveHamiltonianandRiemannmanifoldMonteCarlosamplers.InInternationalConferenceonMachineLearning(ICML) pages1462–1470 2013.URLhttp://jmlr.org/proceedings/papers/v28/wang13e.pdf.JMLRW&CP28(3):1462–1470 2013.[15]Y.Zhang C.Sutton A.Storkey andZ.Ghahramani.ContinuousrelaxationsfordiscreteHamiltonianMonteCarlo.InAdvancesinNeuralInformationProcessingSystems(NIPS) 2012.Algorithm1SSHMCbyABLARequire:(✓ )Sampler✓⇠N(0 G✓( x))andr⇠N(0 G(✓))forlin1 2 ... Ldo(✓(l+✏/2) r(l+✏/2)✓) leapfrog(✓(l) r(l)✓ H1 ✏/2)((l+✏) r(l+✏)) leapfrog((l) r(l) H2 ✏)(✓(l+✏) r(l+✏)✓) leapfrog(✓(l) r(l)✓ H1 ✏/2)endforDrawu⇠U(0 1)ifu<min(1 eH(✓  r✓ r)H(✓(L✏) (L✏) r(L✏) r(L✏)))then(✓0 0 r0✓ r0) (✓(L✏) (L✏) r(L✏)✓ r(L✏))else(✓0 0 r0✓ r0) (✓  r✓ r)endifreturn(✓0 0)9symplecticity. Because each leapfrog integrator is symplectic in a subspace of X [10]  they are also
symplectic in X . Then because ABLA is a composition of symplectic leapfrog integrators  and the
composition of symplectic transformations is symplectic  we know ABLA is symplectic.
We emphasize that ABLA is actually not a discretized simulation of the semi-separable Hamiltonian
system H  that is  if starting at a point (θ  rθ  φ  rφ) in the joint space  we run the exact Hamiltonian
dynamics for H for a length of time L  the resulting point will not be the same as that returned by
ABLA at time L even if the discretized time step is inﬁnitely small. For example  ABLA simulates
H1 with step size 1 and H2 with step size 2 where 1 = 22  when 2 → 0 that preserves H.
4.3 Connection to Other Methods
Although the SSHMC method may seem similar to RMHMC within Gibbs (RMHMCWG)  SSHMC
is actually very different. The difference is in the last two terms of (7) and (10); if these are omit-
ted from SSHMC and the Hamiltonians for π(θ|φ)  then we obtain HMC within Gibbs. Partic-
ularly important among these two terms is the auxiliary potential  because it allows each of the
separable Hamiltonian systems to borrow energy from the other one. For example  if the previous
leapfrog step increases the kinetic energy K1(rθ|φ) in H1(θ  rθ)  then  in the next leapfrog step
for H2(φ  rφ)  we see that φ will have greater potential energy U2(φ|θ  rθ)  because the auxil-
iary potential A(rθ|φ) is shared. That allows the leapfrog step to accommodate a larger change of
log p(φ|θ) using A(rθ|φ). So  the chain will mix faster in Xφ. By the symmetry of θ and φ  the
auxiliary potential will also accelerate the mixing in Xθ.
Another way to see this is that the dynamics in RMHMCWG for (rφ  φ) preserves the distribution
π(θ  rφ  φ) = π(θ  φ)N (rφ; 0  Gφ(φ)−1) but not the joint π(θ  φ  rθ  rφ). That is because the
Gibbs sampler does not take into account the effect of φ on rθ. In other words  the Gibbs step has the
stationary distribution π(φ  rφ|θ) rather than π(φ  rφ|θ  rθ). The difference between the two is the
auxiliary potential. In contrast  the SSHMC methods preserve the Hamiltonian of π(θ  φ  rθ  rφ).

4.4 Choice of Mass Matrix
The choice of Gθ and Gφ in SSHMC is usually similar to RMHMCWG. If the Hessian matrix of
− log p(θ|y  x  φ) is independent of θ and always p.d.  it is natural to deﬁne Gθ as the inverse of the
Hessian matrix. However  for some popular models  e.g.  logistic regression  the Hessian matrix of
the likelihood function depends on the parameters θ. In this case  one can use any approximate Hes-
sian B  like the Hessian at the mode  and deﬁne Gθ := (B + B(φ))−1  where B(φ) is the Hessian
of the prior distribution. Such a rough approximation is usually good enough to improve the mixing
speed  because the main difﬁculty is the correlation between model parameters and hyperparameters.
In general  because the computational bottleneck in HMC and SSHMC is computing the gradient of
the target distribution  both methods have the same computational complexity O(lg)  where g is the
cost of computing the gradient and l is the total number of leapfrog steps per iteration. However  in
practice we ﬁnd it very beneﬁcial to use multiple steps in each blockwise leapfrog update in ABLA;
this can cause SSHMC to require more time than HMC. Also  depending on the mass matrix Gθ  the
cost of leapfrog a step in ABLA may be different from those in standard HMC. For some choices of
Gθ  the leapfrog step in ABLA can be even faster than one leapfrog step of HMC. For example  in
many models the computational bottleneck is the gradient ∇φ log Z(φ)  Z(φ) is the normalization
in prior. Recall that Gθ is a function of φ. If |Gθ| = Z(φ)−1  Z(φ) will be canceled out  avoiding
computation of ∇φ log Z(φ). One example is using Gx = evI in Gaussian funnel distribution
aforementioned in Section 2. A potential problem of such Gθ is that the curvature of the likelihood
function p(D|θ) is ignored. But when the data in each group is sparse and the parameters θ are
strongly correlated  this Gθ can give nearly optimal mixing speed and make SSHMC much faster.
In general  any choice of Gθ and Gφ that would be valid for separable HMC with Gibbs is also valid
for SSHMC.

5 Experimental Results

In this section  we compare the performance of SSHMC with the standard HMC and RMHMC
within Gibbs [7] on four benchmark models.1 The step size of all methods are manually tuned so

1Our use of a Gibbs scheme for RMHMC follows standard practice [7].

5

HMC with diagonal constant mass

SSHMC (semi-separable mass)

Figure 1: The trace of energy over the simulation time and the trajectory of the ﬁrst dimension
of 100 dimensional Gaussian x1 (vertical axis) and hyperparameter v (horizontal axis). The two
simulations start with the same initial point sampled from the Gaussian Funnel.

HMC
RMHMC(Gibbs)
SSHMC

time(s) min ESS(x  v)
(115.35  38.96)
36.63
(1054.33  31.69)
18.92
(3868.79  1541.67)
22.12

min ESS/s (x  v) MSE(E[v]  E[v2])
(3.14  1.06)
(55.15  1.6)
(103.57  41.27)

(0.6  0.18)
(1.58  0.72)
(0.04  0.03)

Table 1: The result of ESS of 5000 samples on 100 + 1 dimensional Gaussian Funnel distribution.
x are model parameters and v is the hyperparameter. The last column is the mean squared error of
the sample estimated mean and variance of the hyperparameter.

HMC
RMHMC(Gibbs)
SSHMC

running time(s) ESS θ (min  med  max) ESS v min ESS/s
378
411
385.82

(2.05  3.68  4.79) ×103
(0.8  4.08  4.99)×103
(2.5  3.42  4.27)×103

815
271
2266

2.15
0.6
5.83

Table 2: The results of ESS of 5000 samples after 1000 burn-in on Hierarchical Bayesian Logistic
Regression. θ are 200 dimensional model parameters and v is the hyperparameter.

HMC
RMHMC(Gibbs)
SSHMC

time (s) ESS x(min  med  max)
162
183
883

(1.6  2.2  5.2)×102
(12.1  18.4  33.5)×102
(78.4  98.9  120.7)×102

ESS(β  σ  φ)
(50  50  128)
(385  163  411)
(4434  1706  1390)

min ESS/s
0.31
0.89
1.57

Table 3: The ESS of 20000 posterior samples of Stochastic Volatility after 10000 burn-in. x are
latent volatilities over 2000 time lags and (β  σ  φ) are hyperparameters. Min ESS/s is the lowest
ESS over all parameters normalized by running time.

that the acceptance rate is around 70-85%. The number of leapfrog steps are tuned for each method
using preliminary runs. The implementation of RMHMC we used is from [7]. The running time
is wall-clock time measured after burn-in. The performance is evaluated by the minimum Effective
Sample Size (ESS) over all dimensions (see [6]). When considering the different computational
complexity of methods  our main efﬁciency metric is time normalized ESS.

v log p(x  v)]−1 = (n + 1

5.1 Demonstration on Gaussian Funnel
We demonstrate SSHMC by sampling the Gaussian Funnel (GF) deﬁned in Section 2. We con-
sider n = 100 dimensional low-level parameters x and 1 hyperparameter v. RMHMC within
v log p(x  v)−1 = evI and
Gibbs on GF has block diagonal mass matrix deﬁned as Gx = −∂2
Gv = −Ex[∂2
9 )−1. We use the same mass matrix in SSHMC  because
it is semi-separable. We use 2 leapfrog steps for low-level parameters and 1 leapfrog step for the
hyperparameter in ABLA and the same leapfrog step size for the two separable Hamiltonians.
We generate 5000 samples from each method after 1000 burn-in iterations. The ESS per second
(ESS/s) and mean squared error (MSE) of the sample estimated mean and variance of the hyper-
parameter are given in Table 1. Notice that RMHMC within Gibbs is much more efﬁcient for the
low-level variables because the mass matrix adapts with the hyperparameter. Figure 1 illustrates a
dramatic difference between HMC and SSHMC. It is clear that HMC suffers from oscillation of the
hyperparameter in a narrow region. That is because the kinetic energy limits the change of hyper-
parameters [3  11]. In contrast  SSHMC has much wider energy variation and the trajectory spans

6

51015202530050100150200250300timeenergy potentialKineticHamltx1v51015202530−300−200−1000100200300timeenergy potentialKineticHamltx1vFigure 2: The normalized histogram of 20000 posterior samples of hyperparameters of the stochas-
tic volatility model (from left to right φ  σ  β) after 10000 burn-in samples. The data is generated by
the hyperparameter (φ = 0.98  σ = 0.15  β = 0.65). All three methods produce accurate estimates 
but SSHMC and RMHMC within Gibbs converge faster than HMC.
a larger range of hyperparameter v. The energy variation of SSHMC is similar to the RMHMC
with Soft-Abs metric (RMHMC-Soft-Abs) reported in [2]  an instance of general RMHMC without
Gibbs. But compared with [2]  each ABLA step is about 100 times faster than each generalized
leapfrog step and SSHMC can generate around 2.5 times more effective samples per second than
RMHMC-Soft-Abs. Although RMHMC within Gibbs has better ESS/s on the low level variables 
its estimation of the mean and variance is biased  indicating that the chain has not yet mixed. More
important  Table 1 shows that the samples generated by SSHMC give nearly unbiased estimates of
the mean and variance of the hyperparameter  which neither of the other methods are able to do.

5.2 Hierarchical Bayesian Logistic Regression
In this experiment  we consider hierarchical Bayesian logistic regression with an exponential prior
for the variance hyperparameter v  that is

(cid:89)

(cid:89)

i

j

p(w  φ|D) ∝

σ(yijwT

i xij)N (wi|0  vI)Exp(v|λ) 

where σ is the logistic function σ(z) = 1/(1+exp(−z)) and (yij  xij) is the jth data point in the ith
group. We use the Statlog (German credit) dataset from [1]. This dataset includes 1000 data points
and each data has 16 categorical features and 4 numeric features. Bayesian logistic regression on
this dataset has been considered as a benchmark for HMC [7  8]  but the previous work uses only
one group in their experiments. To make the problem more interesting  we partition the dataset into
10 groups according to the feature Purpose. The size of group varies from 9 to 285. There are 200
model parameters (20 parameters for each group) and 1 hyperparameter.
We consider the reparameterization of the hyperparameter γ = log v. For RMHMC within Gibbs 
the mass matrix for group i is Gi := I(x  θ)−1  where I(x  θ) is the Fisher Information matrix
for model parameter wi and constant mass Gv. In each iteration of the Gibbs sampler  each wi is
sampled from by RMHMC using 6 generalized leapfrog steps and v is sampled using 6 leapfrog
steps. For SSHMC  Gi := Cov(x) + exp(γ)I and the same constant mass Gv.
The results are shown in Table 2. SSHMC again has much higher ESS/s than the other methods.

5.3 Stochastic Volatility
A stochastic volatility model we consider is studied in [9]  in which the latent volatilities are modeled
by an auto-regressive AR(1) process such that the observations are yt = tβ exp(xt/2) with latent
variable xt+1 = φxt + ηt+1. We consider the distributions x1 ∼ N (0  σ2/(1 − φ2))  t ∼ N (0  1)
and ηt ∼ (0  σ2). The joint probability is deﬁned as
p(yt|xt  β)p(x1)

p(xt|xt−1  φ  σ)π(β)π(φ)φ(σ) 

p(y  x  β  φ  σ) =

T(cid:89)

T(cid:89)

t=2

t=1

where the prior π(β) ∝ 1/β  σ2 ∼ Inv-χ2(10  0.05) and (φ + 1)/2 ∼ beta(20  1.5). The FIM of
p(x|α  β  φ  y) depends on the hyperparameters but not x  but the FIM of p(α  β  φ|x  y) depends
on (α  β  φ). For RMHMC within Gibbs we consider FIM as the metric tensor following [7]. For
SSHMC  we deﬁne Gθ as the inverse Hessian of log p(x|α  β  φ  y)  but Gφ as an identity matrix.
In each ABLA step  we use 5 leapfrog steps for updates of x and 2 leapfrog steps for updates of the
hyperparameters  so that the running time of SSHMC is about 7 times that of standard HMC.

7

0.940.950.960.970.980.99100.010.020.030.040.050.060.070.10.150.20.250.30.3500.010.020.030.040.050.060.070.40.60.811.21.400.020.040.060.080.10.12 RMHMCSSHMCHMC(a)

(b)

(c)

(d)

Figure 3: Sample mean of latent ﬁelds of the LGCPP model from (a) RMHMC and (b) SSHMC.
The normalized histogram of sampled hyperparameter (c) σ and (d) β. We draw 5000 samples from
both methods after 1000 burn-in. The true hyperparameter values are (σ = 1.9  β = 0.03).

SSHMC
RMHMC(Gibbs)

time(h) ESS x(min  med  max) ESS(σ  β)
2.6
2.64

(7.8  30  39)×102
(1  29  38.3)×102

(2101  270)
(200  46)

min ESS/h
103.8
16

Table 4: The ESS of 5000 posterior samples from 32x32 LGCPP after 1000 burn-in samples. x is
the 1024 dimensional vector of latent variables and (σ  β) are the hyperparameters of the Gaussian
Process prior. “min ESS/h” means minimum ESS per hour.
We generate 20000 samples using each method after 10000 burn-in samples. As shown in Figure 2 
the histogram of hyperparameters by all methods converge to the same distribution  so all methods
are mixing well. But from Table 3  we see that SSHMC generates almost two times as many ESS/s
as RMHMC within Gibbs.

i j yi jxi j−m exp(xi j)− 1

log p(y  x|µ  σ  β) =(cid:80)

5.4 Log-Gaussian Cox Point Process
The log-Gaussian Cox Point Process (LGCPP) is another popular testing benchmark [5  7  14]. We
follow the experimental setting of Girolami and Calderhead [7]. The observations Y = {yij} are
counts at the location (i  j)  i  j = 1  . . .   d on a regular spatial grid  which are conditionally inde-
pendent given a latent intensity process Λ = {λ(i  j)} with mean mλ(i  j) = m exp(xi j)  where
m = 1/d2  X = {xi j}  x = Vec(X) and y = Vec(Y). X is assigned a Gaussian process prior  with
mean function m(xi j) = µ1 and covariance function Σ(xi j  xi(cid:48) j(cid:48)) = σ2 exp(−δ(i  i(cid:48)  j  j(cid:48))/βd)
where δ(·) is the Euclidean distance between (i  j) and (i(cid:48)  j(cid:48)). The log joint probability is given by
2 (x−µ1)T Σ−1(x−µ1). We consider a 32×32
grid that has 1024 latent variables. Each latent variable xi j corresponds to a single observation yi j.
We consider RMHMC within Gibbs with FIM of the conditional posteriors. See [7] for the FIM for
this model. The generalized leapfrog steps are required for updating (σ  β)  but only the leapfrog
steps are required for updating x. Each Gibbs iteration takes 20 leapfrog steps for x and 1 general
leapfrog step for (σ  β). In SSHMC  we use Gx = Σ−1 and G(σ β) = I. In each ABLA step  the
update of x takes 2 leapfrog steps and the update of (α  β) takes 1 leapfrog step. Each SSHMC
transition takes 10 ABLA steps. We do not consider HMC on LGCPP  because it mixes extremely
slowly for the hyperparameters.
The results of ESS are given in Table 4. The mean of the sampled latent variables and the histogram
of sampled hyperparameters are given in Figure 3. It is clear that the samples of RMHMC and
SSHMC are consistent  so both methods are mixing well. However  SSHMC generates about six
times as many effective samples per hour as RMHMC within Gibbs.

6 Conclusion
We have presented Semi-Separable Hamiltonian Monte Carlo (SSHMC)  a new version of Rieman-
nian manifold Hamiltonian Monte Carlo (RMHMC) that aims to retain the ﬂexibility of RMHMC for
difﬁcult Bayesian sampling problems  while achieving greater simplicity and lower computational
complexity. We tested SSHMC on several different hierarchical models  and on all the models we
considered  SSHMC outperforms both HMC and RMHMC within Gibbs in terms of number of ef-
fective samples produced in a ﬁxed amount of computation time. Future work could consider other
choices of mass matrix within the semi-separable framework  or the use of SSHMC within discrete
models  following previous work in discrete HMC [12  15].

8

510152025305101520253051015202530510152025300123456700.020.040.060.080.10.12 RMHMCSSHMC00.050.10.150.20.250.30.3500.020.040.060.080.10.120.140.16 RMHMCSSHMCReferences
[1] K. Bache and M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.

uci.edu/ml.

[2] M. J. Betancourt. A general metric for Riemannian manifold Hamiltonian Monte Carlo. ArXiv e-prints 

Dec. 2012.

[3] M. J. Betancourt and M. Girolami. Hamiltonian Monte Carlo for hierarchical models. ArXiv e-prints 

Dec. 2013.

[4] K. Choo. Learning hyperparameters for neural network models using Hamiltonian dynamics. PhD thesis 

Citeseer  2000.

[5] O. F. Christensen  G. O. Roberts  and J. S. Rosenthal. Scaling limits for the transient phase of local
Metropolis–Hastings algorithms. Journal of the Royal Statistical Society: Series B (Statistical Methodol-
ogy)  67(2):253–268  2005.

[6] C. J. Geyer. Practical Markov Chain Monte Carlo. Statistical Science  pages 473–483  1992.
[7] M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  73(2):123–214  2011. doi:
10.1111/j.1467-9868.2010.00765.x.

[8] M. D. Hoffman and A. Gelman. The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian

Monte Carlo. Journal of Machine Learning Research  15:1593–1623  2014.

[9] S. Kim  N. Shephard  and S. Chib. Stochastic volatility: likelihood inference and comparison with ARCH

models. The Review of Economic Studies  65(3):361–393  1998.

[10] B. Leimkuhler and S. Reich. Simulating Hamiltonian dynamics  volume 14. Cambridge University Press 

2004.

[11] R. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo  pages 113–162 

2011.

[12] A. Pakman and L. Paninski. Auxiliary-variable exact Hamiltonian Monte Carlo samplers for binary

distributions. In Advances in Neural Information Processing Systems 26  pages 2490–2498. 2013.

[13] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer  2004.
[14] Z. Wang  S. Mohamed  and N. de Freitas. Adaptive Hamiltonian and Riemann manifold Monte Carlo
samplers.
In International Conference on Machine Learning (ICML)  pages 1462–1470  2013. URL
http://jmlr.org/proceedings/papers/v28/wang13e.pdf. JMLR W&CP 28 (3): 1462–
1470  2013.

[15] Y. Zhang  C. Sutton  A. Storkey  and Z. Ghahramani. Continuous relaxations for discrete Hamiltonian

Monte Carlo. In Advances in Neural Information Processing Systems (NIPS)  2012.

9

,Yichuan Zhang
Charles Sutton