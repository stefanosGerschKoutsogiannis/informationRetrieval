2016,Variational Inference in Mixed Probabilistic Submodular Models,We consider the problem of variational inference in probabilistic models with both log-submodular and log-supermodular higher-order potentials. These models can represent arbitrary distributions over binary variables  and thus generalize the commonly used pairwise Markov random fields and models with log-supermodular potentials only  for which efficient approximate inference algorithms are known. While inference in the considered models is #P-hard in general  we present efficient approximate algorithms exploiting recent advances in the field of discrete optimization. We demonstrate the effectiveness of our approach in a large set of experiments  where our model allows reasoning about preferences over sets of items with complements and substitutes.,Variational Inference in

Mixed Probabilistic Submodular Models

Josip Djolonga

Sebastian Tschiatschek

Department of Computer Science  ETH Z¨urich
{josipd tschiats krausea}@inf.ethz.ch

Andreas Krause

Abstract

We consider the problem of variational inference in probabilistic models with both
log-submodular and log-supermodular higher-order potentials. These models can
represent arbitrary distributions over binary variables  and thus generalize the
commonly used pairwise Markov random ﬁelds and models with log-supermodular
potentials only  for which efﬁcient approximate inference algorithms are known.
While inference in the considered models is #P-hard in general  we present efﬁ-
cient approximate algorithms exploiting recent advances in the ﬁeld of discrete
optimization. We demonstrate the effectiveness of our approach in a large set of
experiments  where our model allows reasoning about preferences over sets of
items with complements and substitutes.

1

Introduction

Probabilistic inference is one of the main building blocks for decision making under uncertainty. In
general  however  this problem is notoriously hard even for deceptively simple-looking models and
approximate inference techniques are necessary. There are essentially two large classes in which
we can categorize approximate inference algorithms — those based on variational inference or on
sampling. However  these methods typically do not scale well to large numbers of variables  or
exhibit an exponential dependence on the model order  rendering them intractable for models with
large factors  which can naturally arise in practice.
In this paper we focus on the problem of inference in point processes  i.e. distributions P (A) over
subsets A ⊆ V of some ﬁnite ground set V . Equivalently  these models can represent arbitrary
distributions over |V | binary variables1. Speciﬁcally  we consider models that arise from submodular
functions. Recently  Djolonga and Krause [1] discussed inference in probabilistic submodular models
(PSMs)  those of the form P (A) ∝ exp(±F (A))  where F is submodular. These models are called
log-submodular (with the plus) and log-supermodular (with the minus) respectively. They generalize
widely used models  e.g.  pairwise purely attractive or repulsive Ising models and determinantal point
processes (DPPs) [2]. Approximate inference in these models via variational techniques [1  3] and
sampling based methods [4  5] has been investigated.
However  many real-world problems have neither purely log-submodular nor log-supermodular
formulations  but can be naturally expressed in the form P (A) ∝ exp(F (A) − G(A))  where both
F (A) and G(A) are submodular functions — we call these types of models mixed PSMs. For instance 
in a probabilistic model for image segmentation there can be both attractive (log-supermodular)
potentials  e.g.  potentials modeling smoothness in the segmentation  and repulsive (log-submodular)
potentials  e.g.  potentials indicating that certain pixels should not be assigned to the same class.
While the sampling based approaches for approximate inference are in general applicable to models
1Distributions over sets A ⊆ V are isomorphic to distributions over |V | binary variables  where each binary

variable corresponds to an indicator whether a certain element is included in A or not.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

with both types of factors  fast mixing is only guaranteed for a subclass of all possible models and
these methods may not scale well to large ground sets. In contrast  the variational inference techniques
were only developed for either log-submodular or log-supermodular models.
In this paper we close this gap and develop variational inference techniques for mixed PSMs. Note
that these models can represent arbitrary positive distributions over sets as any set function can be
represented as the difference of a submodular and a supermodular function [6].2 By exploiting recent
advances in submodular optimization we formulate efﬁcient algorithms for approximate inference
that easily scale to large ground sets and enable the usage of large mixed factors.
Applications/Models. Mixed PSMs are natural models for a variety of applications — modeling of
user preferences  3D stereo reconstruction [7]  and image segmentation [8  9] to name a few. For
instance  user preferences over items can be used for recommending products in an online marketing
application and naturally capture the economic notions of substitutes and complements. Informally 
item a is a substitute for another item b if  given item b  the utility of a diminishes (log-submodular
potentials); on the other hand  an item c is a complement for item d if  given item d  the utility
of c increases (log-supermodular potentials). Probabilistic models that can model substitutes of
items are for example DPPs [2] and the facility location diversity (FLID) model [10]. In §4 we
extend FLID to model both substitutes and complements which results in improved performance on a
real-world product recommendation task. In terms of computer vision problems  non-submodular
binary pairwise MRFs are widely used [8]  e.g.  as discussed above in image segmentation.
Our contributions. We generalize the variational inference procedure proposed in [1] to models
containing both log-submodular and log-supermodular potentials  enabling inference in arbitrary
distributions over binary variables. Furthermore  we provide efﬁcient approximate algorithms for
factor-wise coordinate descent updates enabling faster inference for certain types of models  in
particular for rich scalable diversity models. In a large set of experiments we demonstrate the
effectiveness of mixed higher-order models on a product recommendation task and illustrate the merit
of the proposed variational inference scheme.

2 Background: Variational Inference in PSMs
Submodularity. Let F : 2V → R be a set function  i.e.  a function mapping sets A ⊆ V to real
numbers. We will furthermore w.l.o.g assume that V = {1  2  . . .   n}. Formally  a function F is
called submodular if it satisﬁes the following diminishing returns property for all A ⊆ B ⊆ V \ {i}:

F (A ∪ {i}) − F (A) ≥ F (B ∪ {i}) − F (B).

i∈A mi.

Probabilistic submodular models (PSMs). PSMs are distributions over sets of the form

Informally  this property states that the gain of an item i in the context of a smaller set A is larger than
its gain in the context of a larger set B. A function G is called supermodular if −G is submodular.
A function F is modular  if it is both submodular and supermodular. Modular functions F can be
i∈A mi for some numbers mi ∈ R  and can be thus parameterized by vectors

written as F (A) =(cid:80)
m ∈ Rn. As a shorthand we will frequently use m(A) =(cid:80)
where Z =(cid:80)

Z exp(±F (A)) 

P (A) =

A⊆V exp(±F (A)) ensures that P (A) is normalized  and is often called the partition
function. The distribution P (A) is called log-submodular if the sign in the above deﬁnition is positive
and log-supermodular if the sign is negative. These distributions generalize many well known
classical models and have been effectively used for image segmentation [11]  and for modeling
diversity of item sets in recommender systems [10]. When F (A) = m(A) is a modular function 
P (A) ∝ exp(F (A)) is called log-modular and corresponds to a fully factorized distribution over n
binary random variables X1  . . .   Xn  where we have for each element i ∈ V an associated variable
Xi indicating if this element is included in A or not. The resulting distribution can be written as

1

P (A) =

1
Z exp(m(A)) =

σ(mi)

σ(−mi) 

(cid:89)

i∈A

(cid:89)

i /∈A

2As the authors in [6] note  such a decomposition can be in general hard to ﬁnd.

2

where σ(u) = 1/(1 + e−u) is the sigmoid function.
Variational inference and submodular polyhedra. Djolonga and Krause [1] considered variational
inference for PSMs  whose idea we will present here in a slightly generalized manner. Their
approach starts by bounding F (A) using functions of the form m(A) + t  where m(A) is a modular
function and t ∈ R. Let us ﬁrst analyze the log-supermodular case. If for all A ⊆ V it holds that
m(A) + t ≤ F (A)  then we can bound the partition function Z as

log Z = log

e−F (A) ≤ log

e−m(A)−t =

log(1 + e−mi) − t.

(cid:88)

A⊆V

n(cid:88)

i=1

(cid:88)

A⊆V

n(cid:88)

i=1

Then  the idea is to optimize over the free parameters m and t to ﬁnd the best upper bound  or to
equivalently solve the optimization problem

log(1 + exp(−mi)) − t 

min

(m t)∈L(F )

(1)
where L(F ) is the set of all lower bounds of F   also known as the generalized submodular lower
polyhedron [12]

L(F ) := {(x  t) ∈ Rn+1 | ∀A ⊆ V : x(A) + t ≤ F (A)}.

(2)
Djolonga and Krause [1] show that one obtains the same optimum if we restrict ourselves to t = 0
and one additional constraint  i.e.  if we instead of L(F ) use the base polytope B(F ) deﬁned as

B(F ) := L(F ) ∩ {(x  0) ∈ Rn+1 | x(V ) = F (V )}.

In words  it contains all modular lower bounds of F that are tight at V and ∅. Thanks to the celebrated
result of Edmonds [13]  one can optimize linear functions over B(F ) in time O(n log n). This 
together with the fact that log(1 + e−u) is 1
4-smooth  in turn renders the optimization problem (1)
solvable via the Frank-Wolfe procedure [14  15].
In the log-submodular case  we have to replace in problem (1) the minuses with pluses and use instead
of L(F ) the set of upper bounds. This set  denoted as U(F )  deﬁned by reversing the inequality
sign in Equation (2)  is called the generalized submodular upper polyhedron [12]. Unfortunately  in
contrast to L(F )  one can not easily optimize over U(F ) and asking membership queries is an NP-
hard problem. As discussed by Iyer and Bilmes [12] there are some special cases  like M (cid:92)-concave
functions [16]  where one can describe U(F )  which we will discuss in § 3. Alternatively  which is
the approach taken by [3]  one can select a speciﬁc subfamily of U(F ) and optimize over them.

3

Inference in Mixed PSMs

We consider mixed PSMs  i.e. probability distributions over sets that can be written in the form

P (A) ∝ exp(F (A) − G(A)) 

decompose as F (A) =(cid:80)mF

where F (A) and G(A) are both submodular functions. Furthermore  we assume that F and G
i=1 Gi(A)  where the functions Fi and Gi are
all submodular. Note that this is not a limiting assumption  as submodular functions are closed under
addition and we can always take mF = mG = 1  but such a decomposition will sometimes allow
us to obtain better bounds. The corresponding distribution has the form
exp(−Gj(A)).

i=1 Fi(A)  and G(A) =(cid:80)mG
P (A) ∝ mF(cid:89)
mG(cid:89)

exp(Fj(A))

(3)

j=1

j=1

Similarly to the approach by Djolonga and Krause [1]  we perform variational inference by upper
bounding F (A) − G(A) by a modular function parameterized by m and a constant t such that

F (A) − G(A) ≤ m(A) + t for all A ⊆ V.

(4)
This upper bound induces the log-modular distribution Q(A) ∝ exp(m(A) + t). Ideally  we would
like to select (m  t) such that the partition function of Q(A) is as small as possible (and thus our
approximation of the partition function of P (A) is as tight as possible)  i.e.  we aim to solve

min

(m t)∈U (F−G)

t +

log(1 + exp(mi)).

(5)

|V |(cid:88)

i=1

3

Optimization (and even membership checks) over U(F − G) is in general difﬁcult  mainly because
of the structure of U(F − G)  which is given by 2n inequalities. Thus  we seek to perform a series
of inner approximations of U(F − G) that make the optimization more tractable.
Approximating U(F −G).
In a ﬁrst step we approximate U(F −G) as U(F )−L(G) ⊆ U(F −G) 
where the summation is understood as a Minkowski sum. Then  we can replace L(G) by B(G)
without losing any expressive power  as shown by the following lemma (see [3][Lemma 6]).
Lemma 1. Optimizing problem (5) over U(F ) − L(G) and over U(F ) − B(G) yields the same
optimum value.

This lemma will turn out to be helpful when we shortly describe our strategy for minimizing (5) over
U(F ) − B(G) as it will render some of our subproblems convex optimization problems over B(G)—
these subproblems can then be efﬁciently solved using the Frank-Wolfe algorithm as proposed in [1]
by noting that a greedy algorithm can be used to solve linear optimization problems over B(G) [17].
j=1 Gj 
j=1 B(Gj) (see e.g. [18]). Second  even though it is hard to describe U(F ) 
it might hold that U(Fi) has a tractable description  which leads to the natural inner approximation

By assumption  F (A) and G(A) are composed of simpler functions. First  because G =(cid:80)mG
it holds that B(G) =(cid:80)mG
U(F ) ⊇(cid:80)mF

j=1 U(Fj). To wrap up  we performed the following series of inner approximations

U(F − G) ⊇

U(F )

⊇ (cid:80)mF

j=1 U(Fj) − (cid:80)mG

−

⊇

B(G)

=

j=1 B(Gj)

 

which we then use to approximate U(F − G) in problem (5) before solving it.

Optimization. To solve the resulting problem we use a block coordinate descent procedure. Let us
ﬁrst rewrite the problem in a form that enables us to easily describe the algorithm. Let us write our
resulting approximation as

(m  t) =

(gj  0) 

where we have constrained (fj  tj) ∈ U(Fj) and gj ∈ B(Gj). The resulting problem is then to solve

mF(cid:88)
(fj  tj) − mG(cid:88)
(cid:104)
n(cid:88)

j=1

j=1

i=1

mF(cid:88)
(cid:124)

j=1

mF(cid:88)

fj i − mG(cid:88)

j=1

j=1

(cid:123)(cid:122)

(cid:105)
(cid:125)

=:T ((fj  tj )j=1 ... mF  (gj )j=1 ... mG )

(fj  tj )∈U (Fj ) gj∈B(Gj )

min

tj +

log

1 + exp(

gj i)

.

(6)

Then  until convergence  we pick one of the mG + mF blocks uniformly at random and solve the
resulting optimization problem  which we now show how to do.

Log-supermodular blocks. For a log-supermodular block j  minimizing (6) over gj is a smooth
convex optimization problem and we can either use the Frank-Wolfe procedure as in [1]  or the
divide-and-conquer algorithm (see e.g. [19]). In particular  if we use the Frank-Wolfe procedure we
perform a block coordinate descent step with respect to (6) by iterating the following until we achieve
some desired precision : Given the current gj  we compute ∇gj T and use the greedy algorithm to
solve arg minx∈B(Gj )(cid:104)x ∇gj T(cid:105) in O(n log n) time. We then update gj to (1 − 2
k+2 gj 
where k is the iteration number.

k+2 )x + 2

Log-submodular blocks. As we have already mentioned  this optimization step is much more
challenging. One procedure  which is taken by [1]  is to consider a set of 2n points inside U(Fj)
and optimize over them  which turns out to be a submodular minimization problem. However  for
speciﬁc subfamilies  we can better describe U(Fj). One particularly interesting subfamily is that of
M (cid:92)-concave functions [16]  which have been studied in economics [20].
A set function H is called M (cid:92)-concave if ∀A  B ⊆ V  i ∈ A \ B it satisﬁes
H(A) + H(B) ≤ H(A \ {i}) + H(B ∪ {i}) or

∃j ∈ B \ A : H(A) + H(B) ≤ H((A \ {i}) ∪ {j}) + H((B ∪ {i}) \ {j}).

4

Equivalently  these functions can be deﬁned through the so called gross substitutability property
known in economics. It turns out that M (cid:92)-concave set functions are also submodular. Examples of
these functions include facility location functions  matroid rank functions  monotone concave over
cardinality functions  etc. [16]. For example  H(A) = maxi∈A hi for hi ≥ 0 is M (cid:92)-concave  which
we will exploit in our models in the experimental section.
Returning to our discussion of optimizing (6)  if Fj is an M (cid:92)-concave function  we can minimize (6)
over (fj  tj) ∈ U(Fj) to arbitrary precision in polynomial time. Therefore  we can  similarly
as in [1]  use the Frank-Wolfe algorithm by noting that a polynomial time algorithm for com-
puting arg minx∈U (Fj )(cid:104)x ∇(fj  tj )T(cid:105) exists [20]. Although the minimization can be performed in
polynomial time  it is a very involved algorithm. We therefore consider an inner approximation
ˇU(Fj) := {(m  0) ∈ Rn+1 | ∀A ⊆ V : F (A) ≤ m(A)} ⊆ U(Fj) of U(Fj) over which we
can more efﬁciently approximately minimize (6). As pointed out by Iyer and Bilmes [12]  for M (cid:92)
functions Fj the polyhedron ˇU(Fj) can be characterized by O(n2) inequalities as follows:
ˇU(Fj) := ∪A⊆V {(m  0) ∈ Rn+1 | ∀i ∈ A : mi ≤ Fj(A) − Fj(A \ {i}) 
∀k (cid:54)∈ A : mj ≥ Fj(A ∪ {k}) − Fj(A) 
∀i ∈ A  k (cid:54)∈ A : mi − mk ≤ Fj(A) − Fj((X ∪ {i}) \ {k})}.
We propose to use Algorithm 1 for minimizing over ˇU(Fj). Given a set A where we want our modular
approximation to be exact at  the algorithm iteratively minimizes the partition function of a modular
upper bound on Fj. Clearly  after the ﬁrst iteration of the algorithm (m  0) is an upper bound on
Fj. Furthermore  the partition function corresponding to that bound decreases monotonically over
the iterations of the algorithm. Several heuristics can be used to select A—in the experiments we
determined A as follows: We initialized B = ∅ and then  while 0 < maxi∈V \A F (B ∪ {i}) − F (B) 
added i to B  i.e. B ∪ {arg maxi∈V \B F (B ∪ {i}) − F (B)}. We used the ﬁnal B of this iteration
as our tight set A.

Algorithm 1 Modular upper bound for M (cid:92)-concave functions
Require: M (cid:92) function F   tight set A s.t. m(A) = F (A) for the returned m

Initialize m randomly
for l = 1  2  . . .   max. nr. of iterations do

∀i ∈ A : mi = min{F (A) − F (A \ {i})  mink∈V \A mk + F (A) − F ((A ∪ {i}) \ {k})}
∀k (cid:54)∈ A : mk = max{F (A ∪ {k}) − F (A)  maxi∈A mi − F (A) + F ((A ∪ {i}) \ {k})}

(cid:46) Alt. minimize m over coeff. corresponding to A and V \ A

end for
return Modular upper bound m on F

4 Examples of Mixed PSMs for Modelling Substitutes and Complements

In our experiments we consider probabilistic models that take the following form:

(cid:88)

L(cid:88)

H(A; α  β) =

rl i −(cid:88)
(cid:0) max
(cid:124)
(cid:123)(cid:122)
We would like to point out that even though(cid:80)L

ui + α

i∈A

i∈A

i∈A

Fl(A)

l=1

−β

rl i

(cid:1)
(cid:125)

K(cid:88)

k=1

(cid:0) max
(cid:124)

i∈A

ak i −(cid:88)
(cid:123)(cid:122)

i∈A

Gk(A)

(cid:1)
(cid:125)

 

ak i

(7)

where α  β ∈ {0  1} switch on/off the repulsive and attractive capabilities of the model  respectively.
l=1 Fl(A) is not M (cid:92)-concave  each summand Fl is 
which we will exploit in the next section. The model is parameterized by the vector u ∈ R|V |  and
the weights (rl)l∈[L]  rl ∈ R|V |
≥0   which will be explained shortly. From
the general model (7) we instantiate four different models as explained in the following.
Log-modular model. The log-modular model Pmod(A) is instantiated from (7) by setting α = β =
0  i.e. Fmod(A) := H(A; 0  0) and serves as a baseline model. This model cannot capture any
dependencies between items and corresponds to a fully factorized distribution over the items in V .
Facility location diversity model (FLID). This model is instantiated from (7) by setting α = 1  β = 0 
i.e. FFLID(A) := H(A; 1  0)  and is known as facility location diversity model (FLID) [10]. Note that

≥0 and (ak)k∈[K]  ak ∈ R|V |

5

penalty Fl(A) = maxi∈A rl i −(cid:80)

a· i ∈ RK≥0 of latent properties. In particular  there is a gain of Gk(A) =(cid:80)

this induces a log-submodular distribution. The FLID model parameterizes all items i by an item
quality ui and an L-dimensional vector r· i ∈ RL≥0 of latent properties. The model assigns a negative
i∈A rl i whenever at least two items in A have the same latent
property (the corresponding dimensions of rl are > 0) — thus the model explicitly captures repulsive
dependencies between items.3 Speaking in economic terms  items with similar latent representations
can be considered as substitutes for each other. The FLID model has been shown to perform on par
with DPPs on product recommendation tasks [10].
Facility location complements model (FLIC). This model is instantiated from (7) by setting α =
0  β = 1  i.e. FFLIC(A) := H(A; 0  1) and deﬁnes a log-supermodular probability distribution.
Similar to FLID  the model parameterizes all items i by an item quality ui and a K-dimensional vector
i∈A ak i − maxi∈A ak i
if at least two items in A have the same property k (i.e. for both items the corresponding dimensions
of ak are > 0). In this way  FLIC captures attractive dependencies among items and assigns high
probabilities to sets of items that have similar latent representations — items with similar latent
representations would be considered as complements in economics.
Facility location diversity and complements model (FLDC). This model is instantiated from (7)
via FFLDC(A) := H(A; 1  1). Hence it combines the modelling power of the log-submodular and
log-supermodular models and can explicitly represent attractive and repulsive dependencies. In this
way  FLDC can represent complements and subsitutes for the items in V . The induced probability
distribution is neither log-submodular nor log-supermodular.

5 Experiments

5.1 Experimental Setup

Dataset. We use the Amazon baby registry dataset [21] for evaluating our proposed variational
inference scheme. This dataset is a standard dataset for benchmarking diversity models and consists
of baby registries collected from Amazon. These registries are split into sub-registries according to
13 different product categories  e.g. safety and carseats. Every category contains 32 to 100 different
items and there are ≈ 5.000 to ≈ 13.300 sub-registries per category.

Product recommendation task. We construct a realistic product recommendation task from the
registries of every category as follows. Let D = (S1  . . .   Sn) denote the registries from one category.
From this data  we create a new dataset

ˆD = {(S \ {i}  i) | S ∈ D |S| ≥ 2  i ∈ S} 

(8)
i.e.  ˆD consists of tuples  where the ﬁrst element is a registry from D with one item removed  and
the second element is the removed item. The product recommendation task is to predict i given
S \ {i}. For evaluating the performance of different models on this task we use the following
two metrics: accuracy and mean reciprocal rank. Let us denote the recommendations of a model
(cid:80)
given a partial basket A by σA : V → [n]  where σA(a) = 1 means that product a is recommended
highest  σA(b) = 2 means that product b is recommended second highest  etc. Then  the accuracy
(S(cid:48) i)∈ ˆD[i = σ−1
is computed as Acc = 1
S(cid:48) (1)]. The mean reciprocal rank (MRR) is deﬁned as
| ˆD|
1
MRR = 1
. For our models we consider predictions according to the posterior
−1
| ˆD|
S(cid:48) (i)
(cid:80)
probability of the model given a partial basket A under the constraint that exactly a single item is to
be added  i.e. σA(i) = k if product i achieves the k-th largest value of P (j|A) =
j(cid:48)∈V \A P ({j(cid:48)}∪A)
for j ∈ V \ A (ties are broken arbitrarily).

P ({j}∪A)

(cid:80)

(S(cid:48) i)∈ ˆD

σ

5.2 Mixed Models for Product Recommendation

We learned the models described in the previous section using the training data of the different
categories. In case of the modular model  the parameters u were set according to the item frequencies
in the training data. FLID  FLIC and FLDC were learned using noise contrastive estimation (NCE) [22 
10]. We used stochastic gradient descent for optimizing the NCE objective  created 200.000 noise

3Clearly  also attractive dependencies between items can thereby be modeled implicitly.

6

(a) Accuracy

(b) Mean reciprocal rank

(c) Accuracy of the mixed model
for varying L and K

Figure 1: (a b) Accuracy and MRR on the product recommendation task. For all datasets  the mixed
FLDC model has the best performance. For datasets with small ground set (furniture  carseats  safety)
FLID performs better than FLIC. For most other datasets  FLIC outperforms FLID. (c) Accuracy of
FLDC for different numbers of latent dimensions L and K on the diaper dataset. FLDC (L  K > 0)
performs better than FLID (K = 0) and FLIC (L = 0) for the same value of L + K.

samples from the modular model and made 100 passes through the data and noise samples. We then
used the trained models for the product recommendation task from the previous section and estimated
the performance metrics using 10-fold cross-validation. We used K = 10  L = 10 dimensions for the
weights (if applicable for the corresponding model). The results are shown in Figure 1. For reference 
we also report the performance of DPPs trained with EM [21]. Note that for all categories the mixed
FLDC models achieve the best performance  followed by FLIC and FLID. For categories with more
than 40 items (with the exception of health)  FLIC performs better than FLID. The modular model
performs worst in all cases. As already observed in the literature  the performance of FLID is similar
to that of DPPs [10]. For categories with small ground sets (safety  furniture  carseats)  there is no
advantage of using the higher-order attractive potentials but the repulsive higher-order potentials
improve performance signiﬁcantly. However  in combination with repulsive potentials the attractive
potentials enable the model to improve performance over models with only repulsive higher-order
potentials.

Impact of the Dimension Assignment

5.3
In Figure 1c we show the accuracy of FLDC for different numbers of latent dimensions L and K for
the category diaper  averaged over the 10 cross-validation folds. Similar results can be observed for
the other categories (not shown here because of space constraints). We note that the best performance
is achieved only for models that have both repulsive and attractive components (i.e. L  K > 0). For
instance  if one is constrained to use only 10 latent dimensions in total  i.e. L + K = 10  the best
performance is achieved for the settings L = 3  K = 7 and L = 2  K = 8.

5.4 Quality of the Marginals
In this section we analyze the quality of the marginals obtained by the algorithm proposed in Section 3.
Therefore we repeat the following experiment for all baskets S  |S| ≥ 2 in the the held out test
data. We randomly select a subset S(cid:48) ⊂ S of 1 to |S| − 1 items and a subset S(cid:48)(cid:48) ⊂ V \ S with
|S(cid:48)(cid:48)| = (cid:98)| V \ S|/2(cid:99)  of items not present in the basket. Then we condition our distribution on
the event that the items in S(cid:48) are present and the items S(cid:48)(cid:48) are not present i.e. we consider the
distribution P (A | S(cid:48) ⊆ A  S(cid:48)(cid:48) ∩ A = ∅). This conditioning is supposed to resemble a ﬁctitious
product recommendation task in which we condition on items already selected by a user and exclude
items which are of no interest to the user (for instance  according to the user’s preferences). We then
compute a modular approximation to the posterior distribution using the algorithm from Section 3 

7

0510152025303540furnitureN=32carseatsN=34safetyN=36strollersN=40mediaN=58healthN=62toysN=62bathN=100beddingN=100diaperN=100apparelN=100gearN=100feedingN=100furnitureN=32carseatsN=34safetyN=36strollersN=40mediaN=58healthN=62toysN=62bathN=100beddingN=100diaperN=100apparelN=100gearN=100feedingN=100furnitureN=32carseatsN=34safetyN=36strollersN=40mediaN=58healthN=62toysN=62bathN=100beddingN=100diaperN=100apparelN=100gearN=100feedingN=100furnitureN=32carseatsN=34safetyN=36strollersN=40mediaN=58healthN=62toysN=62bathN=100beddingN=100diaperN=100apparelN=100gearN=100feedingN=100furnitureN=32carseatsN=34safetyN=36strollersN=40mediaN=58healthN=62toysN=62bathN=100beddingN=100diaperN=100apparelN=100gearN=100feedingN=10001020304050modularDPPFLID(L=10)FLIC(K=10)FLDC(L=10 K=10)Table 1: AUC for the considered models on the product recommendation task based on the posterior
marginals. The best result for every dataset is printed in bold. For datasets with at most 62 items 
FLDC has the highest AUC  while for larger datasets FLIC and FLDC have similar AUC. This
indicates a good quality of the marginals computed by the proposed approximate inference procedure.

Dataset Modular
0.731304
0.701840
0.717463
0.727055
0.750271
0.692423
0.666509
0.724763
0.741786
0.700694
0.685051
0.687686
0.686240
0.708698

safety
furniture
carseats
strollers
health
bath
media
toys
bedding
apparel
diaper
gear
feeding
Average

FLID

FLIC

0.756981
0.739646
0.770085
0.794655
0.754185
0.705051
0.667848
0.729089
0.744443
0.696010
0.700543
0.688116
0.686845
0.725653

0.731269
0.702100
0.735472
0.827800
0.756873
0.730443
0.758552
0.765474
0.771159
0.778067
0.787457
0.687501
0.744043
0.752016

FLDC
0.761168
0.759979
0.781642
0.849767
0.758586
0.732407
0.780634
0.777729
0.764595
0.779665
0.787274
0.688885
0.739921
0.766327

bounding(cid:80)L

l=1 Fl(A) and upper bounding(cid:80)K

and recommend items according to these approximate marginals. For evaluation  we compute the
AUC for the product recommendation task and average over the test set data. We found that for
different models different modular upper/lower bounds gave the best results. In particular  for FLID
we used the upper bound given by Algorithm 1 to bound each summand Fl(A) in the facility location
term separately. For FLIC and FLID we optimized the lower bound on the partition function by lower-
k=1 Gk(A)  as suggested in [1]. For approximate
inference in FLIC and FLDC we did not split the facility location terms and bounded them as a whole.
The results are summarized in Table 1. We observe that FLDC has the highest AUC for all datasets
with at most 62 items. For larger datasets  FLDC and FLIC have roughly the same performance and
are superior to FLID and the modular model. These ﬁndings are similar to those from the previous
section and conﬁrm a good quality of the marginals computed from FLDC and FLIC by the proposed
approximate inference procedure.
6 Related Work
Variational inference in general probabilistc log-submodular models has been ﬁrst studied in [1].
The authors propose L-FIELD  an approach for approximate inference in both log-submodular and
log-supermodular models based on super- and sub-differentials of submodular functions. In [3] they
extended their work by relating L-FIELD to the minimum norm problem for submodular minimization 
rendering better scalable algorithms applicable to variational inference in log-submodular models.
The forementioned works can only be applied to models that contain either log-submodular or
log-supermodular potentials and hence do not cover the models considered in this paper.
While the MAP solution in mixed models is known to be NP-hard  there are approximate methods
for its computation based on iterative majorization-minimization (or minorization-maximization)
procedures [23  24]. In [9] the authors consider mixed models in which the supermodular component
is restricted to a tree-structured cut  and provide several algorithms for approximate MAP computation.
In contrast to our work  these methods are non-probabilistic and only provide an approximate MAP
solution without any notion of uncertainty.
7 Conclusion
We proposed efﬁcient algorithms for approximate inference in mixed submodular models based
on inner approximations of the set of modular bounds on the corresponding energy functions. For
many higher-order potentials  optimizing a modular bound over this inner approximation is tractable.
As a consequence  the approximate inference problem can be approached by a block coordinate
descent procedure  tightening a modular upper bound over the individual higher-order potentials in
an iterative manner. Our approximate inference algorithms enable the computation of approximate
marginals and can easily scale to large ground sets. In a large set of experiments  we demonstrated
the effectiveness of our approach.

8

Acknowledgements. The authors acknowledge fruitful discussions with Diego Ballesteros. This research
was supported in part by SNSF grant CRSII2-147633  ERC StG 307036  a Microsoft Research Faculty Fellowship
and a Google European Doctoral Fellowship.

References
[1] Josip Djolonga and Andreas Krause. From MAP to Marginals: Variational Inference in Bayesian Submod-

[2] Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Foundations and

ular Models. In Advances in Neural Information Processing Systems (NIPS)  pages 244–252  2014.
Trends R(cid:13) in Machine Learning  5(2–3):123–286  2012.

[3] Josip Djolonga and Andreas Krause. Scalable Variational Inference in Log-supermodular Models. In

International Conference on Machine Learning (ICML)  2015.

[4] Alkis Gotovos  Hamed Hassani  and Andreas Krause. Sampling from Probabilistic Submodular Models.

In Advances in Neural Information Processing Systems (NIPS)  pages 1936–1944  2015.

[5] Patrick Rebeschini and Amin Karbasi. Fast Mixing for Discrete Point Processes. In Proceedings of the

Conference on Learning Theory (COLT)  pages 1480—-1500  2015.

[6] Mukund Narasimhan and Jeff Bilmes. A Submodular-Supermodular Procedure with Applications to

Discriminative Structure Learning. In Uncertainty in Artiﬁcial Intelligence (UAI)  2005.

[7] Oliver Woodford  Philip Torr  Ian Reid  and Andrew Fitzgibbon. Global stereo reconstruction under
second-order smoothness priors. Pattern Analysis and Machine Intelligence  IEEE Transactions on  31
(12)  2009.

[8] Carsten Rother  Vladimir Kolmogorov  Victor Lempitsky  and Martin Szummer. Optimizing binary mrfs
via extended roof duality. In Computer Vision and Pattern Recognition  2007. CVPR’07. IEEE Conference
on. IEEE  2007.

[9] Yoshinobu Kawahara  Rishabh Iyer  and Jeffrey A. Bilmes. On Approximate Non-submodular Minimization
via Tree-Structured Supermodularity. In Proceedings of the Eighteenth International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS)  2015.

[10] Sebastian Tschiatschek  Josip Djolonga  and Andreas Krause. Learning probabilistic submodular diversity
models via noise contrastive estimation. In Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS)  2016.

[11] Jian Zhang  Josip Djolonga  and Andreas Krause. Higher-order inference for multi-class log-supermodular

models. In International Conference on Computer Vision (ICCV)  2015.

[12] Rishabh Iyer and Jeff Bilmes. Polyhedral aspects of submodularity  convexity and concavity. arXiv preprint

[15] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In Proceedings of the

30th International Conference on Machine Learning (ICML-13)  pages 427–435  2013.

[16] Kazuo Murota. Discrete convex analysis. SIAM  2003.
[17] Jack Edmonds. Submodular functions  matroids  and certain polyhedra. In Combinatorial Optimiza-

tion—Eureka  You Shrink!  pages 11–26. Springer  2003.

[18] S. Fujishige. Submodular Functions and Optimization. Elsevier  2005.
[19] Francis R. Bach. Learning with submodular functions: A convex optimization perspective. Foundations

and Trends in Machine Learning  6(2-3):145–373  2013.

[20] Akiyoshi Shioura. Polynomial-time approximation schemes for maximizing gross substitutes utility under

budget constraints. Mathematics of Operations Research  40(1)  2014.

[21] Jennifer A Gillenwater  Alex Kulesza  Emily Fox  and Ben Taskar. Expectation-maximization for learning

determinantal point processes. In Advances in Neural Information Processing Systems  2014.

[22] Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical models 

with applications to natural image statistics. The Journal of Machine Learning Research  13(1)  2012.

[23] Mukund Narasimhan and Jeff Bilmes. A submodular-supermodular procedure with applications to

discriminative structure learning. arXiv preprint arXiv:1207.1404  2012.

[24] Rishabh Iyer and Jeff Bilmes. Algorithms for approximate minimization of the difference between

submodular functions  with applications. arXiv preprint arXiv:1207.0560  2012.

9

[13] Jack Edmonds. Matroids and the greedy algorithm. Mathematical programming  1(1):127–136  1971.
[14] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics

arXiv:1506.07329  2015.

quarterly  3(1-2):95–110  1956.

,Hanlin Goh
Nicolas Thome
Matthieu Cord
Joo-Hwee Lim
Anastasia Pentina
Christoph Lampert
Josip Djolonga
Sebastian Tschiatschek
Andreas Krause