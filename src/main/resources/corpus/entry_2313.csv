2018,Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex Optimization,We present a unified framework to analyze the global convergence of Langevin dynamics based algorithms for nonconvex finite-sum optimization with $n$ component functions.  At the core of our analysis is a direct analysis of the ergodicity of the numerical approximations to Langevin dynamics  which leads to faster convergence rates. Specifically  we show that gradient Langevin dynamics (GLD) and stochastic gradient Langevin dynamics (SGLD)  converge to the \textit{almost minimizer}\footnote{Following \citet{raginsky2017non}  an almost minimizer is defined to be a point which is within the ball of the global minimizer with radius $O(d\log(\beta+1)/\beta)$  where $d$ is the problem dimension and $\beta$ is the inverse temperature parameter.} within $\tilde O\big(nd/(\lambda\epsilon) \big)$\footnote{$\tilde O(\cdot)$ notation hides polynomials of logarithmic terms and constants.} and $\tilde O\big(d^7/(\lambda^5\epsilon^5) \big)$ stochastic gradient evaluations respectively  where $d$ is the problem dimension  and $\lambda$ is the spectral gap of the Markov chain generated by GLD. Both results improve upon the best known gradient complexity\footnote{Gradient complexity is defined as the total number of stochastic gradient evaluations of an algorithm  which is the number of stochastic gradients calculated per iteration times the total number of iterations.} results \citep{raginsky2017non}. 
Furthermore  for the first time we prove the global convergence guarantee for variance reduced stochastic gradient Langevin dynamics (VR-SGLD) to the almost minimizer within $\tilde O\big(\sqrt{n}d^5/(\lambda^4\epsilon^{5/2})\big)$ stochastic gradient evaluations  which outperforms the gradient complexities of GLD and SGLD in a wide regime.  
Our theoretical analyses shed some light on using Langevin dynamics based algorithms for nonconvex optimization with provable guarantees.,Global Convergence of Langevin Dynamics Based

Algorithms for Nonconvex Optimization

Department of Computer Science

Department of Computer Science

Department of Computer Science

Department of Computer Science

Pan Xu⇤

UCLA

Los Angeles  CA 90095
panxu@cs.ucla.edu

Difan Zou

UCLA

Los Angeles  CA 90095
knowzou@cs.ucla.edu

Jinghui Chen⇤

University of Virginia

Charlottesville  VA 22903
jc4zg@virginia.edu

Quanquan Gu

UCLA

Los Angeles  CA 90095

qgu@cs.ucla.edu

Abstract

We present a uniﬁed framework to analyze the global convergence of Langevin
dynamics based algorithms for nonconvex ﬁnite-sum optimization with n compo-
nent functions. At the core of our analysis is a direct analysis of the ergodicity
of the numerical approximations to Langevin dynamics  which leads to faster
convergence rates. Speciﬁcally  we show that gradient Langevin dynamics (GLD)
and stochastic gradient Langevin dynamics (SGLD) converge to the almost min-

respectively3  where d is the problem dimension  and  is the spectral gap of the
Markov chain generated by GLD. Both results improve upon the best known gradi-
ent complexity4 results [45]. Furthermore  for the ﬁrst time we prove the global
convergence guarantee for variance reduced stochastic gradient Langevin dynamics

imizer2 within eOnd/(✏) and eOd7/(5✏5) stochastic gradient evaluations
(SVRG-LD) to the almost minimizer within eOpnd5/(4✏5/2) stochastic gradi-

ent evaluations  which outperforms the gradient complexities of GLD and SGLD
in a wide regime. Our theoretical analyses shed some light on using Langevin
dynamics based algorithms for nonconvex optimization with provable guarantees.

Introduction

1
We consider the following nonconvex ﬁnite-sum optimization problem

minx Fn(x) := 1/nPn

(1.1)
where fi(x)’s are called component functions  and both Fn(x) and fi(·)’s can be nonconvex. Various
ﬁrst-order optimization algorithms such as gradient descent [42]  stochastic gradient descent [27] and
more recently variance-reduced stochastic gradient descent [46  3] have been proposed and analyzed
for solving (1.1). However  all these algorithms are only guaranteed to converge to a stationary point 
which can be a local minimum  a local maximum  or even a saddle point. This raises an important

i=1 fi(x) 

⇤Equal contribution.
2Following [45]  an almost minimizer is deﬁned to be a point which is within the ball of the global minimizer
with radius O(d log( + 1)/)  where d is the problem dimension and  is the inverse temperature parameter.

3eO(·) notation hides polynomials of logarithmic terms and constants.

is the number of stochastic gradients calculated per iteration times the total number of iterations.

4Gradient complexity is deﬁned as the total number of stochastic gradient evaluations of an algorithm  which

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

question in nonconvex optimization and machine learning: is there an efﬁcient algorithm that is
guaranteed to converge to the global minimum of (1.1)?
Recent studies [17  16] showed that sampling from a distribution which concentrates around the
global minimum of Fn(x) is a similar task as minimizing Fn via certain optimization algorithms.
This justiﬁes the use of Langevin dynamics based algorithms for optimization. In detail  the ﬁrst
order Langevin dynamics is deﬁned by the following stochastic differential equation (SDE)

dX(t) = rFn(X(t))dt +p21dB(t) 

(1.2)
where > 0 is the inverse temperature parameter that is treated as a constant throughout the analysis
of this paper  and {B(t)}t0 is the standard Brownian motion in Rd. Under certain assumptions on
the drift coefﬁcient rFn  it was showed that the distribution of diffusion X(t) in (1.2) converges to its
stationary distribution [14]  a.k.a.  the Gibbs measure ⇡(dx) / exp(Fn(x))  which concentrates
on the global minimum of Fn [29  26  47]. Note that the above convergence result holds even when
Fn(x) is nonconvex. This motivates the use of Langevin dynamics based algorithms for nonconvex
optimization [45  53  50  49]. However  unlike ﬁrst order optimization algorithms [42  27  46  3] 
which have been extensively studied  the non-asymptotic theoretical guarantee of applying Langevin
dynamics based algorithms for nonconvex optimization  is still under studied. In a seminal work 
Raginsky et al. [45] provided a non-asymptotic analysis of stochastic gradient Langevin dynamics
(SGLD) [52] for nonconvex optimization  which is a stochastic gradient based discretization of
(1.2). They proved that SGLD converges to an almost minimizer up to d2/(1/4⇤) log(1/✏) within

eO(d/(⇤✏4)) iterations  where 2 is the variance of stochastic gradient and ⇤ is called the uniform
spectral gap of Langevin diffusion (1.2)  and it is in the order of eeO(d). In a concurrent work  Zhang

et al. [53] analyzed the hitting time of SGLD and proved its convergence to an approximate local
minimum. More recently  Tzen et al. [50] studied the local optimality and generalization performance
of Langevin algorithm for nonconvex functions through the lens of metastability and Simsekli et al.
[49] developed an asynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization
based on variants of SGLD. Erdogdu et al. [23] further developed non-asymptotic analysis of global
optimization based on a broader class of diffusions.
In this paper  we establish the global convergence for a family of Langevin dynamics based algorithms 
including Gradient Langevin Dynamics (GLD) [17  20  16]  Stochastic Gradient Langevin Dynamics
(SGLD) [52] and Stochastic Variance Reduced Gradient Langevin Dynamics (SVRG-LD) [19] for
solving the ﬁnite sum nonconvex optimization problem in (1.1). Our analysis is built upon the direct
analysis of the discrete-time Markov chain rather than the continuous-time Langevin diffusion  and
therefore avoid the discretization error.
1.1 Our Contributions
The major contributions of our work are summarized as follows:

• We provide a uniﬁed analysis for a family of Langevin dynamics based algorithms by a
new decomposition scheme of the optimization error  under which we directly analyze the
ergodicity of numerical approximations for Langevin dynamics (see Figure 1).

• Under our uniﬁed framework  we establish the global convergence of GLD for solving (1.1).
up to precision ✏  where  is the spectral gap of the discrete-time Markov chain generated

In detail  GLD requires eOd/(✏) iterations to converge to the almost minimizer of (1.1)
by GLD and is in the order of eeO(d). This improves the eOd/(⇤✏4)) iteration complexity
of GLD implied by [45]  where ⇤ = eeO(d) is the spectral gap of Langevin diffusion (1.2).
• We establish a faster convergence of SGLD to the almost minimizer of (1.1). In detail 
it converges to the almost minimizer up to ✏ precision within eOd7/(5✏5) stochastic
gradient evaluations. This also improves the eOd9/(⇤5✏8) gradient complexity proved in
eOpnd5/(4✏5/2) stochastic gradient evaluations. It outperforms the gradient complexi-

• We also analyze the SVRG-LD algorithm and investigate its global convergence property.
We show that SVRG-LD is guaranteed to converge to the almost minimizer of (1.1) within

ties of both GLD and SGLD when 1/✏3  n  1/✏5. To the best of our knowledge  this is
the ﬁrst global convergence guarantee of SVRG-LD for nonconvex optimization  while the
original paper [19] only analyzed the posterior sampling property of SVRG-LD.

[45].

2

1.2 Additional Related Work
Stochastic gradient Langevin dynamics (SGLD) [52] and its extensions [2  39  19] have been widely
used in Bayesian learning. A large body of work has focused on analyzing the mean square error of
Langevin dynamics based algorithms. In particular  Vollmer et al. [51] analyzed the non-asymptotic
bias and variance of the SGLD algorithm by using Poisson equations. Chen et al. [12] showed the
non-asymptotic bias and variance of MCMC algorithms with high order integrators. Dubey et al.
[19] proposed variance-reduced algorithms based on stochastic gradient Langevin dynamics  namely
SVRG-LD and SAGA-LD  for Bayesian posterior inference  and proved that their method improves
the mean square error upon SGLD. Li et al. [37] further improved the mean square error by applying
the variance reduction tricks on Hamiltonian Monte Carlo  which is also called the underdamped
Langevin dynamics.
Another line of research [17  21  16  18  22  55] focused on characterizing the distance between
distributions generated by Langevin dynamics based algorithms and (strongly) log-concave target
distributions. In detail  Dalalyan [17] proved that the distribution of the last step in GLD converges to

the stationary distribution in eO(d/✏2) iterations in terms of total variation distance and Wasserstein

distance respectively with a warm start and showed the similarities between posterior sampling and
optimization. Later Durmus and Moulines [20] improved the results by showing this result holds for
any starting point and established similar bounds for the Wasserstein distance. Dalalyan [16] further
improved the existing results in terms of the Wasserstein distance and provide further insights on
the close relation between approximate sampling and gradient descent. Cheng et al. [13] improved
existing 2-Wasserstein results by reducing the discretization error using underdamped Langevin
dynamics. To improve the convergence rates in noisy gradient settings  Chatterji et al. [11]  Zou et al.
[56] presented convergence guarantees in 2-Wasserstein distance for SAGA-LD and SVRG-LD using
variance reduction techniques. Zou et al. [55] proposed the variance reduced Hamilton Monte Carlo
to accelerate the convergence of Langevin dynamics based sampling algorithms. As to sampling from
distribution with compact support  Bubeck et al. [8] analyzed sampling from log-concave distributions
via projected Langevin Monte Carlo  and Brosse et al. [7] proposed a proximal Langevin Monte
Carlo algorithm. This line of research is orthogonal to our work since their analyses are regarding to
the convergence of the distribution of the iterates to the stationary distribution of Langevin diffusion
in total variation distance or 2-Wasserstein distance instead of expected function value gap.
On the other hand  many attempts have been made to escape from saddle points in nonconvex
optimization  such as cubic regularization [43  54]  trust region Newton method [15]  Hessian-vector
product based methods [1  9  10]  noisy gradient descent [24  31  32] and normalized gradient [36].
Yet all these algorithms are only guaranteed to converge to an approximate local minimum rather
than a global minimum. The global convergence for nonconvex optimization remains understudied.
1.3 Notation and Preliminaries
In this section  we present notations used in this paper and some preliminaries for SDE. We use
lower case bold symbol x to denote deterministic vector  and use upper case italicized bold symbol
X to denote random vector. For a vector x 2 Rd  we denote by kxk2 its Euclidean norm. We use
an = O(bn) to denote that an  Cbn for some constant C > 0 independent of n. We also denote
an . bn (an & bn) if an is less than (larger than) bn up to a constant. We also use eO(·) notation to
hide both polynomials of logarithmic terms and constants.
Kolmogorov Operator and Inﬁnitesimal Generator
Suppose X(t) is the solution to the diffusion process represented by the stochastic differential
equation (1.2). For such a continuous time Markov process  let P = {Pt}t>0 be the corresponding
Markov semi-group [4]  and we deﬁne the Kolmogorov operator [4] Ps as follows

Psg(X(t)) = E[g(X(s + t))|X(t)] 

where g is a smooth test function. We have Ps+t = Ps  Pt by Markov property. Further we deﬁne
the inﬁnitesimal generator [4] of the semi-group L to describe the the movement of the process in an
inﬁnitesimal time interval:
=  rFn(X(t)) ·r + 1r2g(X(t)) 

Lg(X(t)) := lim
h!0+
where  is the inverse temperature parameter.

E[g(X(t + h))|X(t)]  g(X(t))

h

3

L = g  ¯g 

Poisson Equation and the Time Average
Poisson equations are widely used in the study of homogenization and ergodic theory to prove the
desired limit of a time-average. Let L be the inﬁnitesimal generator and let be deﬁned as follows
(1.3)
where g is a smooth test function and ¯g is the expectation of g over the Gibbs measure  i.e.  ¯g :=
R g(x)⇡(dx). Smooth function is called the solution of Poisson equation (1.3). Importantly  it has

been shown [23] that the ﬁrst and second order derivatives of the solution of Poisson equation for
Langevin diffusion can be bounded by polynomial growth functions.
2 Review of Langevin Dynamics Based Algorithms
In this section  we brieﬂy review three Langevin dynamics based algorithms proposed recently.
In practice  numerical methods (a.k.a.  numerical integrators) are used to approximate the Langevin
diffusion in (1.2). For example  by Euler-Maruyama scheme [34]  (1.2) can be discretized as follows:
(2.1)
where ✏k 2 Rd is standard Gaussian noise and ⌘> 0 is the step size. The update in (2.1) resembles
gradient descent update except for an additional injected Gaussian noise. The magnitude of the
Gaussian noise is controlled by the inverse temperature parameter . In our paper  we refer this update
as Gradient Langevin Dynamics (GLD) [17  20  16]. The details of GLD are shown in Algorithm 1.
In the case that n is large  the above Euler approximation can be infeasible due to the high computa-
tional cost of the full gradient rFn(Xk) at each iteration. A natural idea is to use stochastic gradient
to approximate the full gradient  which gives rise to Stochastic Gradient Langevin Dynamics (SGLD)
[52] and its variants [2  39  12]. However  the high variance brought by the stochastic gradient can
make the convergence of SGLD slow. To reduce the variance of the stochastic gradient and accelerate
the convergence of SGLD  we use a mini-batch of stochastic gradients in the following update form:
(2.2)

Xk+1 = Xk  ⌘rFn(Xk) +p2⌘1 · ✏k 

Yk+1 = Yk  ⌘/BPi2Ikrfi(Yk) +p2⌘1 · ✏k 

where 1/BPi2Ik rfi(Yk) is the stochastic gradient  which is an unbiased estimator for rFn(Yk)
and Ik is a subset of {1  . . .   n} with |Ik| = B. Algorithm 2 displays the details of SGLD.
Motivated by recent advances in stochastic optimization  in particular  the variance reduction based
techniques [33  46  3]  Dubey et al. [19] proposed the Stochastic Variance Reduced Gradient Langevin
Dynamics (SVRG-LD) for posterior sampling. The key idea is to use semi-stochastic gradient to
reduce the variance of the stochastic gradient. SVRG-LD takes the following update form:

Zk+1 = Zk  ⌘erk +p2⌘1 · ✏k 

where erk = 1/BPik2Ikrfik (Zk)  rfik (eZ(s)) + rFn(eZ(s)) is the semi-stochastic gradient 
eZ(s) is a snapshot of Zk at every L iteration such that k = sL + ` for some ` = 0  1  . . .   L  1  and
Ik is a subset of {1  . . .   n} with |Ik| = B. SVRG-LD is summarized in Algorithm 3.
Note that although all the three algorithms are originally proposed for posterior sampling or more
generally  Bayesian learning  they can be applied for nonconvex optimization  as demonstrated in
many previous studies [2  45  53].
Algorithm 1 Gradient Langevin Dynamics (GLD)

(2.3)

input: step size ⌘> 0; inverse temperature parameter > 0; X0 = 0
for k = 0  1  . . .   K  1 do
randomly draw ✏k ⇠ N (0  Id⇥d)
Xk+1 = Xk  ⌘rFn(Xk) +p2⌘/✏k

end for

3 Main Theory
Before we present our main results  we ﬁrst lay out the following assumptions on the loss function.
Assumption 3.1 (Smoothness). The function fi(x) is M-smooth for M > 0  i = 1  . . .   n  i.e. 

krfi(x)  rfi(y)k2  Mkx  yk2 

for any x  y 2 Rd.

4

Algorithm 2 Stochastic Gradient Langevin Dynamics (SGLD)

input: step size ⌘> 0; batch size B; inverse temperature parameter > 0; Y0 = 0
for k = 0  1  . . .   K  1 do
Yk+1 = Yk  ⌘/BPi2Ik rfi(Yk) +p2⌘/✏k

randomly pick a subset Ik from {1  . . .   n} of size |Ik| = B; randomly draw ✏k ⇠ N (0  Id⇥d)

end for

Algorithm 3 Stochastic Variance Reduced Gradient Langevin Dynamics (SVRG-LD)

input: step size ⌘> 0; batch size B; epoch length L; inverse temperature parameter > 0

initialization: Z0 = 0  eZ(0) = Z0
for s = 0  1  . . .   (K/L)  1 do
fW = rFn(eZ(s))
for ` = 0  . . .   L  1 do
k = sL + `
randomly pick a subset Ik from {1  . . .   n} of size |Ik| = B; draw ✏k ⇠ N (0  Id⇥d)
erk = 1/BPik2Ikrfik (Zk)  rfik (eZ(s)) +fW
Zk+1 = Zk  ⌘erk +p2⌘/✏k
eZ(s) = Z(s+1)L

end for

Assumption 3.1 immediately implies that Fn(x) = 1/nPn

Assumption 3.2 (Dissipative). There exist constants m  b > 0  such that we have

i=1 fi(x) is also M-smooth.

end for

hrFn(x)  xi  mkxk2

2  b 

for all x 2 Rd.

Assumption 3.2 is a typical assumption for the convergence analysis of an SDE and diffusion
approximation [40  45  53]  which can be satisﬁed by enforcing a weight decay regularization [45]. It
says that starting from a position that is sufﬁciently far away from the origin  the Markov process
deﬁned by (1.2) moves towards the origin on average. It can also be noted that all critical points are

within the ball of radius O(pb/m) centered at the origin under this assumption.
Let x⇤ = argminx2Rd Fn(x) be the global minimizer of Fn. Our ultimate goal is to prove the
convergence of the optimization error in expectation  i.e.  E[Fn(Xk)]  Fn(x⇤). In the sequel  we
decompose the optimization error into two parts: (1) E[Fn(Xk)]  E[Fn(X ⇡)]  which characterizes
the gap between the expected function value at the k-th iterate Xk and the expected function value
at X ⇡  where X ⇡ follows the stationary distribution ⇡(dx) of Markov process {X(t)}t0  and (2)
E[Fn(X ⇡)]  Fn(x⇤). Note that the error in part (1) is algorithm dependent  while that in part (2)
only depends on the diffusion itself and hence is identical for all Langevin dynamics based algorithms.
Now we are ready to present our main results regarding to the optimization error of each algorithm
reviewed in Section 2. We ﬁrst show the optimization error bound of GLD (Algorithm 1).
Theorem 3.3 (GLD). Under Assumptions 3.1 and 3.2  consider XK generated by Algorithm 1 with
initial point X0 = 0. The optimization error is bounded by

E[Fn(XK)]  Fn(x⇤)  ⇥eK⌘ +

C ⌘



+

d
2

(3.1)

log✓ eM (b/d + 1)

m

|

RM

{z

 

◆
}

where problem-dependent parameters ⇥ and  are deﬁned as

⇥=

C0M (b + m + d)(m + em⌘M (b + m + d))

m2⇢d/2

  =

2m⇢d

log(2M (b + m + d)/m)

 

and ⇢ 2 (0  1)  C0  C > 0 are constants.
In the optimization error of GLD (3.1)  we denote the upper bound of the error term E[Fn(X ⇡)] 
Fn(x⇤) by RM  which characterizes the distance between the expected function value at X ⇡ and the
global minimum of Fn. The stationary distribution of Langevin diffusion ⇡ / eFn(x) is a Gibbs

5

distribution  which concentrates around the minimizer x⇤ of Fn. Thus a random vector X ⇡ following
the law of ⇡ is called an almost minimizer of Fn within a neighborhood of x⇤ with radius RM [45].
It is worth noting that the ﬁrst term in (3.1) vanishes at a exponential rate due to the ergodicity of
Markov chain {Xk}k=0 1.... Moreover  the exponential convergence rate is controlled by   the
spectral gap of the discrete-time Markov chain generated by GLD  which is in the order of eeO(d).
By setting E[Fn(XK)]  E[Fn(X ⇡)] to be less than a precision ✏  and solving for K  we have the
following corollary on the iteration complexity for GLD to converge to the almost minimizer X ⇡.
Corollary 3.4 (GLD). Under the same conditions as in Theorem 3.3  provided that ⌘ . ✏  GLD
achieves E[Fn(XK)]  E[Fn(X ⇡)]  ✏ with K = Od✏11 · log(1/✏).

Remark 3.5. In a seminal work by [45]  they provided a non-asymptotic analysis of SGLD for non-
convex optimization. By setting the variance of stochastic gradient to 0  their result immediately sug-
gests an O(d/(✏4⇤) log5((1/✏))) iteration complexity for GLD to converge to the almost minimizer
up to precision ✏. Here the quantity ⇤ is the so-called uniform spectral gap for continuous-time
Markov process {Xt}t0 generated by Langevin dynamics. They further proved that ⇤ = eeO(d) 
which is in the same order of our spectral gap  for the discrete-time Markov chain {Xk}k=0 1...
generated by GLD. Both of them match the lower bound for metastable exit times of SDE for
nonconvex functions that have multiple local minima and saddle points [6]. Although for some
speciﬁc function Fn  the spectral gap may be reduced to polynomial in d [25]  in general  the spectral
gap for continuous-time Markov processes is in the same order as the spectral gap for discrete-time
Markov chains. Thus  the iteration complexity of GLD suggested by Corollary 3.4 is better than that
suggested by [45] by a factor of O(1/✏3).

We now present the following theorem  which states the optimization error of SGLD (Algorithm 2).
Theorem 3.6 (SGLD). Under Assumptions 3.1 and 3.2  consider YK generated by Algorithm 2 with
initial point Y0 = 0  the optimization error is bounded by

E[Fn(YK)]  Fn(x⇤)  C1K⌘ (n  B)(Mp+ G)2

B(n  1)

1/2

+⇥ eK⌘ +

C ⌘



+ RM  

(3.2)
where C1 is an absolute constant  C    ⇥ and RM are the same as in Theorem 3.3  B is the
mini-batch size  G = maxi=1 ... n{krfi(x⇤)k2} + bM/m and  = 2(1 + 1/m)(b + 2G2 + d/).
Similar to Corollary 3.4  by setting E[Fn(Yk)]  E[Fn(X ⇡)]  ✏  we obtain the following corollary.
Corollary 3.7 (SGLD). Under the same conditions as in Theorem 3.6  if ⌘ . ✏  SGLD achieves

E[Fn(YK)]  E[Fn(X ⇡)] = Od3/2B1/41 · log(1/✏) + ✏ 
with K = Od✏11 · log(1/✏)  where B is the mini-batch size of Algorithm 2.

Remark 3.8. Corollary 3.7 suggests that if the mini-batch size B is chosen to be large enough to
offset the divergent term log(1/✏)  SGLD is able to converge to the almost minimizer in terms of
expected function value gap. This is also suggested by the result in [45]. More speciﬁcally  the result
in [45] implies that SGLD achieves

(3.3)

E[Fn(YK)]  E[Fn(X ⇡)] = Od21/4⇤1 · log(1/✏) + ✏

with K = O(d/(⇤✏4) · log5(1/✏))  where 2 is the upper bound of stochastic variance in SGLD 
which can be reduced with larger batch size B. Recall that the spectral gap ⇤ in their work scales as
O(eeO(d))  which is in the same order as  in Corollary 3.7. In comparison  our result in Corollary
3.7 indicates that SGLD can actually achieve the same order of error for E[Fn(YK)]  E[Fn(X ⇡)]
with substantially fewer number of iterations  i.e.  O(d/(✏) log(1/✏)) .
Remark 3.9. To ensure SGLD converges in Corollary 3.7  one may set a sufﬁciently large batch size
B to offset the divergent term. For example  if we choose B & d64✏4 log4(1/✏)  SGLD achieves
E[Fn(YK)]  E[Fn(X ⇡)]  ✏ within K = O(d/(✏) log(1/✏)) stochastic gradient evaluations.
In what follows  we proceed to present our result on the optimization error bound of SVRG-LD.

6

Theorem 3.10 (SVRG-LD). Under Assumptions 3.1 and 3.2  consider ZK generated by Algorithm
3 with initial point Z0 = 0. The optimization error is bounded by
E[Fn(ZK)]  Fn(x⇤)
 C1K3/4⌘ LM 2(n  B)
+ RM   (3.4)
where constants C1  C    ⇥    G and RM are the same as in Theorem 3.6  B is the mini-batch
size and L is the length of inner loop of Algorithm 3.

B(n  1) ✓9⌘L(M 2+ G2) +

◆1/4

+⇥ eK⌘ +

C ⌘



d

Similar to Corollaries 3.4 and 3.7  we have the following iteration complexity for SVRG-LD.
Corollary 3.11 (SVRG-LD). Under the same conditions as in Theorem 3.10  if ⌘ . ✏  SVRG-LD
iterations. In addition  if we choose B = pn✏3/2  L = pn✏3/2  the number of stochastic gradient

achieves E[Fn(ZK)]  E[Fn(X ⇡)]  ✏ within K = OLd5B14✏4 · log4(1/✏) + 1/✏ total
evaluations needed for SVRG-LD to achieve ✏ precision is eOpn✏5/2 · eeO(d).

Remark 3.12. In Theorem 3.10 and Corollary 3.11  we establish the global convergence guarantee
for SVRG-LD to an almost minimizer of a nonconvex function Fn. To the best of our knowledge 
this is the ﬁrst iteration/gradient complexity guarantee for SVRG-LD in nonconvex ﬁnite-sum
optimization. Dubey et al. [19] ﬁrst proposed the SVRG-LD algorithm for posterior sampling  but
only proved that the mean square error between averaged sample pass and the stationary distribution

N/A

GLD

SGLD5

This paper

SVRG-LD

eO n
✏4 · eeO(d)
eO n
✏ · eeO(d)

eO 1
✏8 · eeO(d)
eO 1
✏5 · eeO(d)

Table 1: Gradient complexities to converge to the almost minimizer.

converges to ✏ within eO(1/✏3/2) iterations  which has no implication for nonconvex optimization.

In large scale machine learn-
ing problems  the evaluation of
full gradient can be quite ex-
pensive  in which case the iter-
ation complexity is no longer
appropriate to reﬂect the efﬁ-
ciency of different algorithms.
To perform a comprehensive
comparison among the three algorithms  we present their gradient complexities for converging
to the almost minimizer X ⇡ with ✏ precision in Table 1. Recall that gradient complexity is deﬁned as
the total number of stochastic gradient evaluations needed to achieve ✏ precision. It can be seen from
Table 1 that the gradient complexity for GLD has worse dependence on the number of component
functions n and SVRG-LD has worse dependence on the optimization precision ✏. More speciﬁcally 
when the number of component functions satisﬁes n  1/✏5  SVRG-LD achieves better gradient
complexity than SGLD. Additionally  if n  1/✏3  SVRG-LD is better than both GLD and SGLD 
therefore is more favorable.
4 Proof Sketch of the Main Results
In this section  we highlight our high level idea in the analysis of GLD  SGLD and SVRG-LD.
4.1 Roadmap of the Proof
Recall the problem in (1.1) and denote the global minimizer as x⇤ = argminx Fn(x). {X(t)}t0 and
{Xk}k=0 ... K are the continuous-time and discrete-time Markov processes generated by Langevin
diffusion (1.2) and GLD respectively. We propose to decompose the optimization error as follows:

eO⇣ pn
✏5/2⌘ · eeO(d)

[45]

E[Fn(Xk)]  Fn(x⇤)
= E[Fn(Xk)]  E[Fn(X µ)]
}
{z
|

I1

+ E[Fn(X µ)]  E[Fn(X ⇡)]
}
|

{z

I2

+ E[Fn(X ⇡)]  Fn(x⇤)
}
|

{z

I3

 

(4.1)

where X µ follows the stationary distribution µ(dx) of Markov process {Xk}k=0 1 ... K  and X ⇡
follows the stationary distribution ⇡(dx) of Markov process {X(t)}t0  a.k.a.  the Gibbs distribution.
Following existing literature [40  41  12]  here we assume the existence of stationary distributions 
i.e.  the ergodicity  of Langevin diffusion (1.2) and its numerical approximation (2.2). Note that the

5For SGLD in [45]  the result in the table is obtained by choosing the exact batch size suggested by the

authors that could make the stochastic variance small enough to cancel out the divergent term in their paper.

7

X(t)

Xk

X⇡

x⇤

ergodicity property of an SDE is not trivially guaranteed in general and establishing the existence of
the stationary distribution is beyond the scope of our paper. Yet we will discuss the circumstances
when geometric ergodicity holds in the Appendix.
We illustrate the decomposition (4.1) in Figure 1. Unlike exist-
ing optimization analysis of SGLD such as [45]  which measure
the approximation error between Xk and X(t) (blue arrows in
the chart)  we directly analyze the geometric convergence of dis-
cretized Markov chain Xk to its stationary distribution (red arrows
in the chart). Since the distance between Xk and X(t) is a slow-
convergence term in [45]  and the distance between X(t) and X ⇡
depends on the uniform spectral gap  our new roadmap of proof will
bypass both of these two terms  hence leads to a faster convergence
rate.
Bounding I1: Geometric Ergodicity of GLD
To bound the ﬁrst term in (4.1)  we need to analyze the convergence of the Markov chain generated by
Algorithm 1 to its stationary distribution  namely  the ergodic property of the numerical approximation
of Langevin dynamics. In probability theory  ergodicity describes the long time behavior of Markov
processes. For a ﬁnite-state Markov Chain  this is also closely related to the mixing time and has
been thoroughly studied in the literature of Markov processes [28  35  4]. Note that Durmus and
Moulines [21] studied the convergence of the Euler-Maruyama discretization (also referred to as the
unadjusted Langevin algorithm) towards its stationary distribution in total variation. Nevertheless 
they only focus on strongly convex functions which are less challenging than our nonconvex setting.
The following lemma ensures the geometric ergodicity of gradient Langevin dynamics.
Lemma 4.1. Under Assumptions 3.1 and 3.2  the gradient Langevin dynamics (GLD) in Algorithm
1 has a unique invariant measure µ on Rd. It holds that

Figure 1: Illustration of the anal-
ysis framework in our paper.

Xµ

|E[Fn(Xk)]  E[Fn(X µ)]| C⇢d/2(1 + em⌘) exp✓ 

2mk⌘⇢d

log() ◆ 

where ⇢ 2 (0  1) C > 0 are absolute constants  and  = 2M (b + m + d)/b.
Lemma 4.1 establishes the exponential decay of function gap between Fn(Xk) and Fn(X ⇡) using
coupling techniques. Note that the exponential dependence on dimension d is consistent with the
result from [45] using entropy methods.
Bounding I2: Convergence to Stationary Distribution of Langevin Diffusion
Now we are going to bound the distance between two invariant measures µ and ⇡ in terms of their
expectations over the objective function Fn. Our proof is inspired by [51  12]. The key insight here
is that after establishing the geometric ergodicity of GLD  by the stationarity of µ  we have

Z Fn(x)µ(dx) =Z E[Fn(Xk)|X0 = x] · µ(dx).

This property says that after reaching the stationary distribution  any further transition (GLD update)
will not change the distribution. Thus we can bound the difference between two invariant measures.
Lemma 4.2. Under Assumptions 3.1 and 3.2  the invariant measures µ and ⇡ satisfy

where C > 0 is a constant that dominates E[krp (Xk)k] (p = 0  1  2) and is the solution of
Poisson equation (1.3).

E[Fn(X µ)]  E[Fn(X ⇡)]  C ⌘/ 

Lemma 4.2 suggests that the bound on the difference between the two invariant measures depends on
the numerical approximation step size ⌘  the inverse temperature parameter  and the upper bound C .
We emphasize that the dependence on  is reasonable since different  results in different diffusion 
and further leads to different stationary distributions of the SDE and its numerical approximations.
Bounding I3: Gap between Langevin Diffusion and Global Minimum
Most existing studies [52  48  12] on Langevin dynamics based algorithms focus on the convergence
of the averaged sample path to the stationary distribution. The property of Langevin diffusion

8

asymptotically concentrating on the global minimum of Fn is well understood [14  26]   which makes
the convergence to a global minimum possible  even when the function Fn is nonconvex.
We give an explicit bound between the stationary distribution of Langevin diffusion and the global
minimizer of Fn  i.e.  the last term E[Fn(X ⇡)]  Fn(x⇤) in (4.1). For nonconvex objective function 
this has been proved in [45] using the concept of differential entropy and smoothness of Fn. We
formally summarize it as the following lemma:
Lemma 4.3. [45] Under Assumptions 3.1 and 3.2  the model error I3 in (4.1) can be bounded by

E[Fn(X ⇡)]  Fn(x⇤) 

d
2

log✓ eM (m/d + 1)

m

◆ 

where X ⇡ is a random vector following the stationary distribution of Langevin diffusion (1.2).
Lemma 4.3 suggests that Gibbs density concentrates on the global minimizer of objective function.
Therefore  the random vector X ⇡ following the Gibbs distribution ⇡ is also referred to as an almost
minimizer of the nonconvex function Fn in [45].
4.2 Proof of Theorems 3.3  3.6 and 3.10
Now we integrate the previous lemmas to prove our main theorems in Section 3. First  submitting the
results in Lemmas 4.1  4.2 and 4.3 into (4.1)  we immediately obtain the optimization error bound
in (3.1) for GLD  which proves Theorem 3.3. Second  consider the optimization error of SGLD
(Algorithm 2)  we only need to bound the error between E[Fn(YK)] and E[Fn(XK)] and then apply
the results for GLD  which is given by the following lemma.
Lemma 4.4. Under Assumptions 3.1 and 3.2  by choosing mini-batch of size B  the output of SGLD
in Algorithm 2 (YK) and the output of GLD in Algorithm 1 (XK) satisﬁes

|E[Fn(YK)]  E[Fn(XK)]| C1p(Mp+ G)K⌘ n  B

B(n  1)1/4

 

(4.2)

 

1/4

B(n  1)

where C1 is an absolute constant and  = 2(1 + 1/m)(b + 2G2 + d/).
Combining Lemmas 4.1  4.2  4.3 and 4.4 yields the desired result in (3.6) for SGLD  which completes
the proof of Theorem 3.6. Third  similar to the proof of SGLD  we require an additional bound
between Fn(ZK) and Fn(XK) for the proof of SVRG-LD  which is stated by the following lemma.
Lemma 4.5. Under Assumptions 3.1 and 3.2  by choosing mini-batch of size B  the output of
SVRG-LD in Algorithm 3 (ZK) and the output of GLD in Algorithm 1 (XK) satisﬁes

E[Fn(ZK)]  E[Fn(XK)]  C1K3/4⌘ LM 2(n  B)(3L⌘(M 2+ G2) + d/2)

where  = 2(1 + 1/m)(b + 2G2 + d/)  C1 is an absolute constant and L is the number of inner
loops in SVRG-LD.
The optimization error bound in (3.4) for SVRG-LD follows from Lemmas 4.1  4.2  4.3 and 4.5.
5 Conclusions and Future Work
In this work  we present a new framework for analyzing the convergence of Langevin dynamics
based algorithms  and provide non-asymptotic analysis on the convergence for nonconvex ﬁnite-
sum optimization. By comparing the Langevin dynamics based algorithms and standard ﬁrst-order
optimization algorithms  we may see that the counterparts of GLD and SVRG-LD are gradient descent
(GD) and stochastic variance-reduced gradient (SVRG) methods. It has been proved that SVRG
outperforms GD universally for nonconvex ﬁnite-sum optimization [46  3]. This poses a natural
question that whether SVRG-LD can be universally better than GLD for nonconvex optimization?
We will attempt to answer this question in the future.
Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. We thank Maxim
Raginsky for insightful comments and discussion on the ﬁrst version of this paper. We also thank
Tianhao Wang for discussion on this work. This research was sponsored in part by the National
Science Foundation IIS-1652539. The views and conclusions contained in this paper are those of the
authors and should not be interpreted as representing any funding agencies.

9

References
[1] Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding approximate
local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on
Theory of Computing  STOC 2017  pages 1195–1199  2017.

[2] Sungjin Ahn  Anoop Korattikara  and Max Welling. Bayesian posterior sampling via stochastic gradient
ﬁsher scoring. In Proceedings of the 29th International Conference on Machine Learning  pages 1771–1778 
2012.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In International

Conference on Machine Learning  pages 699–707  2016.

[4] Dominique Bakry  Ivan Gentil  and Michel Ledoux. Analysis and geometry of Markov diffusion operators 

volume 348. Springer Science & Business Media  2013.

[5] Francois Bolley and Cedric Villani. Weighted csiszár-kullback-pinsker inequalities and applications to
transportation inequalities. Annales de la Faculté des Sciences de Toulouse. Série VI. Mathématiques  14 
01 2005. doi: 10.5802/afst.1095.

[6] Anton Bovier  Michael Eckhoff  Véronique Gayrard  and Markus Klein. Metastability in reversible diffu-
sion processes i: Sharp asymptotics for capacities and exit times. Journal of the European Mathematical
Society  6(4):399–424  2004.

[7] Nicolas Brosse  Alain Durmus  Éric Moulines  and Marcelo Pereyra. Sampling from a log-concave
distribution with compact support with proximal Langevin Monte Carlo. In Conference on Learning
Theory  pages 319–342  2017.

[8] Sébastien Bubeck  Ronen Eldan  and Joseph Lehec. Sampling from a log-concave distribution with

projected Langevin Monte Carlo. Discrete & Computational Geometry  59(4):757–783  2018.

[9] Yair Carmon and John C Duchi. Gradient descent efﬁciently ﬁnds the Cubic-regularized non-convex

newton step. arXiv preprint arXiv:1612.00547  2016.

[10] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for nonconvex

optimization. SIAM Journal on Optimization  28(2):1751–1772  2018.

[11] Niladri Chatterji  Nicolas Flammarion  Yian Ma  Peter Bartlett  and Michael Jordan. On the theory of
variance reduction for stochastic gradient Monte Carlo. In Proceedings of the 35th International Conference
on Machine Learning  pages 764–773  2018.

[12] Changyou Chen  Nan Ding  and Lawrence Carin. On the convergence of stochastic gradient mcmc
algorithms with high-order integrators. In Advances in Neural Information Processing Systems  pages
2278–2286  2015.

[13] Xiang Cheng  Niladri S. Chatterji  Peter L. Bartlett  and Michael I. Jordan. Underdamped Langevin mcmc:
A non-asymptotic analysis. In Proceedings of the 31st Conference On Learning Theory  volume 75  pages
300–323  2018.

[14] Tzuu-Shuh Chiang  Chii-Ruey Hwang  and Shuenn Jyi Sheu. Diffusion for global optimization in rˆn.

SIAM Journal on Control and Optimization  25(3):737–753  1987.

[15] Frank E Curtis  Daniel P Robinson  and Mohammadreza Samadi. A trust region algorithm with a worst-case
iteration complexity of O(✏3/2) for nonconvex optimization. Mathematical Programming  pages 1–32 
2014.

[16] Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte Carlo

and gradient descent. In Conference on Learning Theory  pages 678–689  2017.

[17] Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  79(3):651–676 
2017.

[18] Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the Langevin Monte Carlo with

inaccurate gradient. arXiv preprint arXiv:1710.00095  2017.

[19] Kumar Avinava Dubey  Sashank J Reddi  Sinead A Williamson  Barnabas Poczos  Alexander J Smola 
and Eric P Xing. Variance reduction in stochastic gradient Langevin dynamics. In Advances in Neural
Information Processing Systems  pages 1154–1162  2016.

10

[20] Alain Durmus and Eric Moulines. Non-asymptotic convergence analysis for the unadjusted Langevin

algorithm. arXiv preprint arXiv:1507.05021  2015.

[21] Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted Langevin

algorithm. arXiv preprint arXiv:1605.01559  2016.

[22] Raaz Dwivedi  Yuansi Chen  Martin J Wainwright  and Bin Yu. Log-concave sampling: Metropolis-
hastings algorithms are fast! In Proceedings of the 31st Conference On Learning Theory  pages 793–797 
2018.

[23] Murat A Erdogdu  Lester Mackey  and Ohad Shamir. Global non-convex optimization with discretized

diffusions. In Advances in Neural Information Processing Systems  pages 9693–9702  2018.

[24] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points-online stochastic gradient

for tensor decomposition. In COLT  pages 797–842  2015.

[25] Rong Ge  Holden Lee  and Andrej Risteski. Beyond log-concavity: Provable guarantees for sampling multi-
modal distributions using simulated tempering Langevin Monte Carlo. arXiv preprint arXiv:1710.02736 
2017.

[26] Saul B Gelfand and Sanjoy K Mitter. Recursive stochastic algorithms for global optimization in Rd. SIAM

Journal on Control and Optimization  29(5):999–1018  1991.

[27] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[28] Martin Hairer and Jonathan C Mattingly. Spectral gaps in wasserstein distances and the 2d stochastic

navier-stokes equations. The Annals of Probability  pages 2050–2091  2008.

[29] Chii-Ruey Hwang. Laplace’s method revisited: weak convergence of probability measures. The Annals of

Probability  pages 1177–1182  1980.

[30] Nobuyuki Ikeda and Shinzo Watanabe. Stochastic differential equations and diffusion processes  volume 24.

Elsevier  2014.

[31] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape saddle

points efﬁciently. In International Conference on Machine Learning  pages 1724–1732  2017.

[32] Chi Jin  Praneeth Netrapalli  and Michael I. Jordan. Accelerated gradient descent escapes saddle points
faster than gradient descent. In Proceedings of the 31st Conference On Learning Theory  pages 1042–1085 
2018.

[33] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In Advances in Neural Information Processing Systems  pages 315–323  2013.

[34] Peter E Kloeden and Eckhard Platen. Higher-order implicit strong numerical schemes for stochastic

differential equations. Journal of statistical physics  66(1):283–314  1992.

[35] David Asher Levin  Yuval Peres  and Elizabeth Lee Wilmer. Markov chains and mixing times. American

Mathematical Soc.  2009.

[36] Kﬁr Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831 

2016.

[37] Zhize Li  Tianyi Zhang  and Jian Li. Stochastic gradient hamiltonian Monte Carlo with variance reduction

for bayesian inference. arXiv preprint arXiv:1803.11159  2018.

[38] Robert S Liptser and Albert N Shiryaev. Statistics of random Processes: I. general Theory  volume 5.

Springer Science & Business Media  2013.

[39] Yi-An Ma  Tianqi Chen  and Emily Fox. A complete recipe for stochastic gradient mcmc. In Advances in

Neural Information Processing Systems  pages 2917–2925  2015.

[40] Jonathan C Mattingly  Andrew M Stuart  and Desmond J Higham. Ergodicity for sdes and approximations:
locally lipschitz vector ﬁelds and degenerate noise. Stochastic processes and their applications  101(2):
185–232  2002.

[41] Jonathan C Mattingly  Andrew M Stuart  and Michael V Tretyakov. Convergence of numerical time-
averaging and stationary measures via poisson equations. SIAM Journal on Numerical Analysis  48(2):
552–577  2010.

11

[42] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science

& Business Media  2013.

[43] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.

Mathematical Programming  108(1):177–205  2006.

[44] Yury Polyanskiy and Yihong Wu. Wasserstein continuity of entropy and outer bounds for interference

channels. IEEE Transactions on Information Theory  62(7):3992–4002  2016.

[45] Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via stochastic gradient
Langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory  pages 1674–1703 
2017.

[46] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International Conference on Machine Learning  pages 314–323 
2016.

[47] Gareth O Roberts and Richard L Tweedie. Exponential convergence of Langevin distributions and their

discrete approximations. Bernoulli  pages 341–363  1996.

[48] Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient Langevin dynamics by
using fokker-planck equation and ito process. In Proceedings of the 31st International Conference on
Machine Learning (ICML-14)  pages 982–990  2014.

[49] Umut Simsekli  Cagatay Yildiz  Than Huy Nguyen  Taylan Cemgil  and Gael Richard. Asynchronous
stochastic quasi-Newton MCMC for non-convex optimization. In Proceedings of the 35th International
Conference on Machine Learning  pages 4674–4683  2018.

[50] Belinda Tzen  Tengyuan Liang  and Maxim Raginsky. Local optimality and generalization guarantees for
the Langevin algorithm via empirical metastability. In Proceedings of the 31st Conference On Learning
Theory  pages 857–875  2018.

[51] Sebastian J Vollmer  Konstantinos C Zygalakis  and Yee Whye Teh. Exploration of the (non-) asymptotic
bias and variance of stochastic gradient Langevin dynamics. Journal of Machine Learning Research  17
(159):1–48  2016.

[52] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings

of the 28th International Conference on Machine Learning  pages 681–688  2011.

[53] Yuchen Zhang  Percy Liang  and Moses Charikar. A hitting time analysis of stochastic gradient Langevin

dynamics. In Conference on Learning Theory  pages 1980–2022  2017.

[54] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic variance-reduced cubic regularized Newton methods.

In Proceedings of the 35th International Conference on Machine Learning  pages 5990–5999  2018.

[55] Difan Zou  Pan Xu  and Quanquan Gu. Stochastic variance-reduced Hamilton Monte Carlo methods. In

Proceedings of the 35th International Conference on Machine Learning  pages 6028–6037  2018.

[56] Difan Zou  Pan Xu  and Quanquan Gu. Subsampled stochastic variance-reduced gradient Langevin

dynamics. In Proceedings of International Conference on Uncertainty in Artiﬁcial Intelligence  2018.

12

,Pan Xu
Jinghui Chen
Difan Zou
Quanquan Gu
Wang-Zhou Dai
Qiuling Xu
Yang Yu
Zhi-Hua Zhou