2011,Selective Prediction of Financial Trends with Hidden Markov Models,Focusing on short term trend prediction in a financial context  we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The first is a rejection in the spirit of Chow‚Äôs well-known ambiguity principle. The second is a specialized mechanism for HMMs that identifies low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective  and that the sHMM  model is superior.,Selective Prediction of Financial Trends with Hidden

Markov Models

Ran El-Yaniv and Dmitry Pidan

Department of Computer Science  Technion

Haifa  32000 Israel

{rani pidan}@cs.technion.ac.il

Abstract

Focusing on short term trend prediction in a Ô¨Ånancial context  we consider the
problem of selective prediction whereby the predictor can abstain from prediction
in order to improve performance. We examine two types of selective mechanisms
for HMM predictors. The Ô¨Årst is a rejection in the spirit of Chow‚Äôs well-known
ambiguity principle. The second is a specialized mechanism for HMMs that iden-
tiÔ¨Åes low quality HMM states and abstain from prediction in those states. We
call this model selective HMM (sHMM). In both approaches we can trade-off pre-
diction coverage to gain better accuracy in a controlled manner. We compare
performance of the ambiguity-based rejection technique with that of the sHMM
approach. Our results indicate that both methods are effective  and that the sHMM
model is superior.

1

Introduction

Selective prediction is the study of predictive models that can automatically qualify their own pre-
dictions and output ‚Äúdon‚Äôt know‚Äù when they are not sufÔ¨Åciently conÔ¨Ådent. Currently  manifestations
of selective prediction within machine learning mainly exist in the realm of inductive classiÔ¨Åcation 
where this notion is often termed ‚ÄòclassiÔ¨Åcation with a reject option.‚Äô In the study of a reject option 
which was initiated more than 40 years ago by Chow [5]  the goal is to enhance accuracy (or reduce
‚Äòrisk‚Äô) by compromising the coverage. For a classiÔ¨Åer or predictor equipped with a rejection mech-
anism we can quantify its performance proÔ¨Åle by evaluating its risk-coverage (RC) curve  giving
the functional relation between error and coverage. The RC curve represents a trade-off: the more
coverage we compromise  the more accurate we can expect to be  up to the point where we reject
everything and (trivially) never err. The essence of selective classiÔ¨Åcation is to construct classiÔ¨Åers
achieving useful (and optimal) RC trade-offs  thus providing the user with control over the choice
of desired risk (with its associated coverage compromise).

Our longer term goal is to study selective prediction models for general sequential prediction tasks.
While this topic has only been sparsely considered in the literature  we believe that it has great po-
tential in dealing with difÔ¨Åcult problems. As a starting point  however  in this paper we focus on the
restricted objective of predicting next-day trends in Ô¨Ånancial sequences. While limited in scope  this
problem serves as a good representative of difÔ¨Åcult sequential data [17]. A very convenient and quite
versatile modeling technique for analyzing sequences is the Hidden Markov Model (HMM). There-
fore  the goal we set had been to introduce selection mechanisms for HMMs  capable of achieving
useful risk-coverage trade-off in predicting next-day trends.

To this end we examined two approaches. The Ô¨Årst is a straightforward application of Chow‚Äôs
ambiguity principle implemented with HMMs. The second is a novel and specialized technique
utilizing the HMM state structure. In this approach we identify latent states whose prediction quality
is systematically inferior  and abstain from predictions while the underlying source is likely to be in

1

those states. We call this model selective HMM (sHMM). While this natural approach can work in
principle  if the HMM does not contain sufÔ¨Åciently many ‚ÄúÔ¨Åne grained‚Äù states  whose probabilistic
volume (or ‚Äúvisit rate‚Äù) is small  the resulting risk-coverage trade-off curve will be a coarse step
function that will prevent Ô¨Åne control and usability. One of our contributions is a solution to this
coarseness problem by introducing algorithms for reÔ¨Åning sHMMs. The resulting reÔ¨Åned sHMMs
give rise to smooth RC trade-off curves.

We present the results of quite extensive empirical study showing the effectiveness of our methods 
which can increase the edge in predicting next-day trends. We also show the advantage of sHMMs
over the classical Chow approach.

2 Preliminaries

2.1 Hidden Markov Models in brief

A Hidden Markov Model (HMM) is a generative probabilistic state machine with latent states  in
which state transitions and observations emissions represent Ô¨Årst-order Markov processes. Given an
observation sequence  O = O1  . . .   OT   hypothesized to be generated by such a model  we would
like to ‚Äúreverse engineer‚Äù the most likely (in a Bayesian sense) state machine giving rise to O  with
associated latent state sequence S = S1  . . .   ST . An HMM is deÔ¨Åned as Œª   hQ  M  œÄ  A  Bi 
where Q is a set of states  M is the number of observations  œÄ is the initial states distribution  œÄi  
P [S1 = qi]  A = (aij) is the transition matrix  aij   P [St+1 = qj | St = qi]  and B = (bj(k)) is
the observation emission matrix  bj(k)   P [Ot = vk | St = qj].
Given an HMM Œª and observation sequence O  an efÔ¨Åcient algorithm for calculating P [O | Œª] is
the forward-backward procedure (see details in  e.g.  Rabiner [16]). The estimation of the HMM
parameters (training) is traditionally performed using a specialized expectation-maximization (EM)
algorithm called the Baum-Welch algorithm [2]. For a large variety of problems it is also essential to
identify the ‚Äúmost likely‚Äù state sequence associated with a given observation sequence. This is com-
monly accomplished using the Viterbi algorithm [22]  which computes arg maxS P [S | O  Œª]. Sim-
ilarly  one can identify the most likely ‚Äúindividual‚Äù state  arg maxq P [St = q | O  Œª]  corresponding
to time t.

2.2 Selective Prediction and the RC Trade-off

To deÔ¨Åne the performance parameters in selective prediction we utilize the following deÔ¨Ånitions for
selective classiÔ¨Åers from [6  7]. A selective (binary) classiÔ¨Åer is represented as a pair of functions
hf  gi  where f is a binary classiÔ¨Åer and g : X ‚Üí {0  1} is a binary qualiÔ¨Åer for f: whenever g(x) =
1  the prediction f (x) is accepted  and otherwise it is ignored. The performance of a selective
classiÔ¨Åer is measured by its coverage and risk. Coverage is the expected volume of non-rejected
data instances  C   E [g(X)]  (where expectation is w.r.t.
the unknown underlying distribution)
and the risk is the error rate over non-rejected instances  R   E [I(f (X) 6= Y )g(X)](cid:14)C  where Y
represents the true classiÔ¨Åcation.
The purpose of a selective prediction model is to provide ‚ÄúsufÔ¨Åciently low‚Äù risk with ‚ÄúsufÔ¨Åciently
high‚Äù coverage. The functional relation between risk and coverage is called the risk coverage (RC)
trade-off. Generally  the user of a selective model would like to bound one measure (either risk or
coverage) and then obtain the best model in terms of the other measure. The RC curve of a given
model characterizes this trade-off on a risk/coverage plane thus describing its full spectrum.

A selective predictor is useful if its RC curve is ‚Äúnon trivial‚Äù in the sense that progressively smaller
risk can be obtained with progressively smaller coverage. Thus  when constructing a selective classi-
Ô¨Åcation or a prediction model it is imperative to examine its RC curve. One can consider theoretical
bounds of the RC curve (as in [6]) or empirical ones as we do here. Interpolated RC curve can be
obtained by selecting a number of coverage bounds at certain grid points of choice  and learning
(and testing) a selective model aiming at achieving the best possible risk for each coverage level.
Obviously  each such model should respect the corresponding coverage bound.

2

3 Selective Prediction with HMMs

3.1 Ambiguity Model

The Ô¨Årst approach we consider is an implementation of the classical ambiguity idea. We construct an
HMM-based classiÔ¨Åer  similar to the one used in [3]  and endow it with a rejection mechanism in the
spirit of Chow [5]. This approach is limited to binary labeled observation sequences. The training
set  consisting of labeled sequences  is partitioned into its positive and negative instances  and two
HMM‚Äôs  Œª+ and Œª‚àí  are trained using those sets  respectively. Thus  Œª+ is trained to identify
positively labeled sequences  and Œª‚àí ‚Äì negatively labeled sequences. Then  each new observation
sequence O is classiÔ¨Åed as sign(P [O | Œª+] ‚àí P [O | Œª‚àí]).
For applying Chow‚Äôs ambiguity idea using the model (Œª+  Œª‚àí)  we need to deÔ¨Åne a measure C(O)
of prediction conÔ¨Ådence for any observation sequence O. A natural choice in this context is to
measure the log-likelihood difference between the positive and negative models  normalized by the
length of the sequence. Thus  we deÔ¨Åne C(O)   | 1
T (log P [O | Œª+] ‚àí log P [O | Œª‚àí])|  where T is
the length of O. The greater C(O) is  the more conÔ¨Ådent are we in the classiÔ¨Åcation of O. Now 
given the classiÔ¨Åcation conÔ¨Ådences of all sequences in the training data set  and given a required
lower bound on the coverage  an empirical threshold can be found such that a designated number of
instances with the smallest conÔ¨Ådence measures will be rejected. If our data is non-stationary (e.g.
Ô¨Ånancial sequences)  this threshold can be re-estimated at the arrival of every new data instance.

3.2 State-Based Selectivity

We propose a different approach for implementing selective prediction with HMMs. The idea is to
designate an appropriate subset of the states as ‚Äúrejective.‚Äù The proposed approach is suitable for
prediction problems whose observation sequences are labeled. SpeciÔ¨Åcally  for each observation 
Ot  we assume that there is a corresponding label lt. The goal is to predict lt at time t ‚àí 1.
Each state is assigned risk and visit rate estimates. For each state q  its risk estimate is used as
a proxy to the probability of making erroneous predictions from q  and its visit rate quantiÔ¨Åes the
probability of outputting any symbol from q. A subset of the highest risk states is selected so that
their total expected visit rate does not exceed the user speciÔ¨Åed rejection bound. These states are
called rejective and predictions from them are ignored. The following two deÔ¨Ånitions formulate
these notions. We associate with each state q a label Lq representing the HMM prediction while at
this state (see Section 3.4). Denote Œ≥t(i)   P [St = qi | O  Œª]  and note that Œ≥t(i) can be efÔ¨Åciently
calculated using the standard forward-backward procedure (see Rabiner [16]).
DeÔ¨Ånition 3.1 (emprirical visit rate). Given an observation sequence  the empirical visit rate  v(i) 
of a state qi  is the fraction of time the HMM spends in state qi  that is v(i)   1
DeÔ¨Ånition 3.2 (empirical state risk). Given an observation sequence  the empirical risk  r(i)  of a
state qi  is the rate of erroneous visits to qi  that is r(i)   1

t=1 Œ≥t(i).

T PT

Œ≥t(i).

v(i)T PT

t=1

Lqi

6=lt

Suppose we are required to meet a user speciÔ¨Åed rejection bound 0 ‚â§ B ‚â§ 1. This means that we
are required to emit predictions (rather than ‚Äúdon‚Äôt know‚Äùs) in at least 1 ‚àí B fraction of the time. To
achieve this we apply the following greedy selection procedure of rejective states whereby highest
risk states are sequentially selected as long as their overall visit rate does not exceed B. We call the
resulting model Naive-sHMM. Formally  let qi1   qi2   . . .   qiN be an ordering of all states  such that
for each j < k  r(ij) ‚â• r(ik). Then  the rejective state subset is 

RS   Ô£±Ô£≤
Ô£≥
3.3 Overcoming Coarseness

qi1   . . .   qiK

K

X

j=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

v(ij) ‚â§ B 

K+1

X

j=1

v(ij) > BÔ£ºÔ£Ω
Ô£æ

.

(3.1)

The above simple approach suffers from the following coarseness problem. If our model does not
include a large number of states  or includes states with very high visit rates (as it is often the case
in applications)  the total visit rate of the rejective states might be far from the requested bound

3

B  entailing that selectivity cannot be fully exploited. For example  consider a model that has
three states such that r(q1) > r(q2) > r(q3)  v(q1) =   and v(q2) = B + . In this case only
the negligibly visited q1 will be rejected. We propose two methods to overcome this coarseness
problem. These methods are presented in the two subsequent sections.

3.3.1 Randomized Linear Interpolation (RLI)

In the randomized linear interpolation (RLI) method  predictions from rejective states are always
rejected  but predictions from the non-rejective state with the highest risk rate are rejected with
appropriate probability  such that the total expected rejection rate equals the rejection bound B. Let
q be the non-rejective state with the highest risk rate. The probability to reject predictions emerging
v(q) (cid:16)B ‚àí Pq0‚ààRS v(q0)(cid:17). Clearly  with pq thus deÔ¨Åned  the
from this state is taken to be pq   1
total expected rejection rate is precisely B  when expectation is taken over random choices.

3.3.2 Recursive ReÔ¨Ånement (RR)

Given an initial HMM model  the idea in the recursive reÔ¨Ånement approach is to construct an ap-
proximate HMM whose states have Ô¨Åner granularity of visit rates. This smaller granularity enables
a selection of rejective states whose total visit rate is closer to the required bound. The reÔ¨Ånement is
achieved by replacing every highly visited state with a complete HMM.
The process starts with a root HMM  Œª0  trained in a standard way using the Baum-Welch algorithm.
In Œª0  states that have visit rate greater than a certain bound are identiÔ¨Åed. For each such state qi
(called a heavy state)  a new HMM Œªi (called a reÔ¨Åning HMM) is trained and combined with Œª0 as
follows: every transition from other states into qi in Œª0 entails a transition into (initial) state in Œªi in
accordance with the initial state distribution of Œªi; every self transition to qi in Œª0 results in a state
transition in Œªi according to its state transition matrix; Ô¨Ånally  every transition from qi to another
state entails transition from a state in Œªi whose probability is the original transition probability from
qi. States of Œªi are assigned the label of qi. This reÔ¨Ånement continues in a recursive manner and
terminates when all the heavy states have reÔ¨Ånements. The non reÔ¨Åned states are called leaf states.



?



?

?

_
1
_ 

?

_
4
_ 



?

?

_
2
_ 

ONML
HIJK5

ONML
HIJK6

ONML
HIJK3

ONML
HIJK7

ONML
HIJK8

Figure 1: Recursively ReÔ¨Åned HMM

Figure 1 depicts a recursively reÔ¨Åned HMM having two reÔ¨Ånement levels. In this model  states 1 2 4
are heavy (and reÔ¨Åned) states  and states 3 5 6 7 8 are leaf (emitting) states. The model consisting
of states 3 and 4 reÔ¨Ånes state 1  the model consisting of states 5 and 6 reÔ¨Ånes state 2  etc.

An aggregate state of the complete hierarchical model corresponds to a set of inner HMM states 
each of which is a state on a path from the root through reÔ¨Åning HMMs  to a leaf state. Only leaf
states actually emit symbols. ReÔ¨Åned states are non-emitting and their role in this construction is to
preserve the structure (and transitions) of the HMMs they reÔ¨Åne.

At every time instance t  the model is at some aggregate state. Transition to the next aggregate state
always starts at Œª0  and recursively progresses to the leaf states  as shown in the following example.
Suppose that the model in Figure 1 is at aggregate state {1 4 7} at time t. The aggregate state at time
t + 1 is calculated as follows. Œª0 is in state 1  so its next state (say 1 again) is chosen according to
the distribution {a11  a12}. We then consider the model that reÔ¨Ånes state 1  which was in state 4 at

4





+
+








k
k






+
+




k
k






+
+


k
k


+
+


k
k
time t. Here again the next state (say 3) is chosen according to the distribution {a43  a44}. State 3
is a leaf state that emits observations  and the aggregate state at time t + 1  is {1 3}. On the other
hand  if state 2 is chosen at the root  a new state (say 6) in its reÔ¨Åning model is chosen according to
the initial distribution {œÄ5  œÄ6} (transition into the heavy state from another state). The chosen state
6 is a leaf state so the new aggregate state becomes {2 6}.

Algorithm 1 TrainReÔ¨ÅningHMM
Input: HMM Œª = h{qj}j=n
1: Draw random HMM  Œªi = h{qj}j=n+N
j k=n+1   {bjm}j=n+N m=M
j=n+1 m=1 i
2: For each 1 ‚â§ j ‚â§ n  j 6= i  replace transition qjqi with qjqn+1 . . . qjqn+N   and qiqj with

j=1   M  œÄ  A  Bi  heavy state qi  O
j=n+1   M  {œÄj}j=n+N

j=n+1   {ajk}j k=n+N

qn+1qj . . . qn+N qj

3: Remove state qi with the corresponding {bim}i=M
i=1

Œªi. Set Lqj = Lqi for each n + 1 ‚â§ j ‚â§ n + N

from Œª  and record it as a state reÔ¨Åned by

For each 1 ‚â§ j ‚â§ n  j 6= i  update aj(n+k) = ajiœÄn+k  and a(n+k)j = aij  1 ‚â§ k ‚â§ N.
For each n + 1 ‚â§ j ‚â§ n + N  update œÄj = œÄiœÄj
For each n + 1 ‚â§ j  k ‚â§ n + N  update ajk = aiiajk
Re-estimate {œÄj}j=n+N

j k=n+1   {bjm}j=n+N m=M

j=n+1 m=1   using Eq.(3.2)

j=n+1   {ajk}j k=n+N

4: while not converged do
5:
6:
7:
8:
9: end while
10: Perform steps 5-7
Output: HMM Œª

Algorithm 1 is a pseudocode of the training algorithm for reÔ¨Åning HMM Œªi  for a heavy state qi.
This algorithm is an extension of the Baum-Welch algorithm [2].
In steps 1-3  a random Œªi is
generated and connected to the HMM Œª instead of qi. Steps 5-8 iteratively update the parameters
of Œªi until the Baum-Welch convergence criterion is met  and in step 10  Œª is updated with the Ô¨Ånal
Œªi parameters. Finally  in step 3 qi is stored as a state reÔ¨Åned by Œªi  to preserve the hierarchical
structure of the resulting model (essential for the selection mechanism). The algorithm is applied on
heavy states until all states in the HMM have visit rates lower than a required bound.

œÄj =

1
Z

Ô£´
Ô£¨Ô£≠

Œ≥1(j) +

T ‚àí1

n

X

t=1

X

k=1
k6=i

Œæt(k  j)

Ô£∂
Ô£∑Ô£∏

  ajk =

Œæt(j  k)

T ‚àí1

Pt=1
Pl=n+1

n+N

  bjm =

Œæt(j  l)

T ‚àí1

Pt=1

T

Pt=1

Ot=m

Œ≥t(j)

T

Pt=1

Œ≥t(j)

(3.2)

In Eq. (3.2)  re-estimation formulas for the parameters of newly added states (Step 8) are presented 
where Œæt(j  k) = P [qt = j  qt+1 = k | O  Œª]. It is easy to see that  similarly to original Baum-Welch
formulas  constraints for the parameters to be valid distributions are preserved (Z is a normalization
factor in the œÄj equation). The main difference from the original formulas is in the re-estimation of
œÄj: in the reÔ¨Ånement process  transitions from other states into heavy state qi also affect the initial
distribution of its reÔ¨Åning states.

The most likely aggregate state at time t  given sequence O  is found in a top-down manner using
the hierarchical structure of the model. Starting with the root model  Œª0  the most likely individual
state in it  say qi  is identiÔ¨Åed. If this state has no reÔ¨Ånement  then we are done. Otherwise  the most
likely individual state in Œªi (HMM that reÔ¨Ånes qi)  say qj  is identiÔ¨Åed  and the aggregate state is
updated to be {qi  qj}. The process continues until the last discovered state has no reÔ¨Ånement.
The above procedure requires calculation of the quantity Œ≥t(i) not only for the leaf states (where it is
calculated using a standard forward-backward procedure)  but also for the reÔ¨Åned states. For those
states  Œ≥t(i) = PN
The rejection subset is found using the Eq. (3.1)  applied to the aggregate states of the reÔ¨Åned model.
Visit and risk estimates for the aggregate state {qi1 . . . qik } are calculated using Œ≥t(ik)  of a leaf state
qik that identiÔ¨Åes this aggregate state.

Œ≥t(j) is calculated recursively over the hierarchical structure.

qj reÔ¨Ånes qi

j=1

5

The outcome of the RR procedure is a tree of HMMs whose main purpose is to redistribute visit rates
among states. This re-distribution is the key element that allows for achieving smooth RC curves.
Various other hierarchical HMM schemes have been proposed in the literature [4  8  10  18  20].
While some of these schemes may appear similar to ours at Ô¨Årst glance  they do not address the visit
rate re-distribution objective. In fact  those models were developed to serve other purposes such as
better modeling of sequences that have special structure (e.g.  sequences hypothesized to be emerged
from a hierarchical generative model).

3.4 State Labeling

It remains to address the assignment of labels to the states in our state-based selection models. Labels
can be assigned to states a-priori  and then a supervised EM method can be used for training (this
model is known as Class HMM)  as in [15]. Alternatively  state labels can be calculated from the
statistics of the states  if an unsupervised training method is used. In our setting  we are following
the latter approach. For a state qi  and given observation label l  we calculate the average number of
visits (at qi) whose corresponding label is l  as E [St = qi | lt = l  O  Œª] = P1‚â§t‚â§T  lt=l Œ≥t(i). Thus 
Lqi is chosen to be an l that maximized this quantity.

4 Experimental Results

We compared empirically the four selection mechanisms presented in Section 3  namely  the ambi-
guity model and the Naive  RLI  and RR sHMMs. All methods were compared on a next-day trend
prediction of the S&P500 index. This problem is known to be very difÔ¨Åcult  and recent experimental
work by Rao and Hong [17] assessed that although HMM succeeds to achieve some positive edge 
the accuracy is near Ô¨Åfty-Ô¨Åfty (51.72%) when a pure price data is used.

For our prediction task  we took as observation sequence directions of the S&P500 price changes.
SpeciÔ¨Åcally  the direction dt  at time t  is dt   sign(pt+1 ‚àí pt)  where pt are close prices. The state-
based models were fed with the series of partial sequences ot   dt‚àí`+1  . . .   dt. For the ambiguity
model  the partial sequences dt‚àí`+1  . . .   dt were used as a pool of observation sequences.
In a preliminary small experiment we observed the advantage of the state-based approach over the
ambiguity model. In order to validate this  we tried to falsify this hypothesis by optimizing the
hyper-parameters of the ambiguity model in hindsight.

For the state-based models we used a 5-state HMM  and predictions were made using the label of
the most likely individual state. Such HMMs are hypothesized to be sufÔ¨Åciently expressive to model
a small number of basic market conditions such as strong/weak trends (up and down) and sideways
markets [17  23]. We have not tried to optimize this basic architecture and better results with more
expressive models can be possibly achieved. For the ambiguity model we constructed two 8-state
HMMs  where the length of a single observation sequence (`) is 5. This architecture was optimized
in hindsight among all possibilities of up to 10 states  and up to length 8  for a single observation
sequence. Every reÔ¨Åning model in the RR procedure had the same structure  and the upper bound
on the visit rate was Ô¨Åxed at 0.1. For sHMMs  the hyper-parameter ` was arbitrarily set to 3 (there
is possibly room for further improvement by optimizing the model w.r.t. this hyper-parameter).

RC curves were computed for each technique by taking the linear grid of rejection rate bounds from
0 to 0.9 in steps of 0.1. For each bound  every model was trained and tested using 30-fold cross-
validation  with each fold consisting of 10 random restarts. Test performance was measured by
mean error rate  taken over the 30 folds  and standard error of the mean (SEM) statistics were also
calculated to monitor statistical signiÔ¨Åcance.

Since the price sequences we deal with are highly non-stationary  we employed a walk-forward
scheme in which the model is trained over the window of past Wp returns and then tested on the
subsequent window of Wf ‚Äúfuture‚Äù returns. Then  we ‚Äúwalk forward‚Äù Wf steps (days) in the return
sequence (so that the next training segment ends where the last test segment ended) and the process
repeats until we consume the entire data sequence.
In the experiments we set Wp = 2000 and
Wf = 50 (that is  in each step we learn to predict the next business quarter  day by day). The data
sequence in this experiment consisted of the 3000 S&P500 returns from 1/27/1999 to 12/31/2010.
With our walk forward procedure  the Ô¨Årst 2000 points were only used for training the Ô¨Årst model.

6

e

t

a
R

 
r
o
r
r

E

0.5

0.48

0.46

0.44

0.42

0.4

 

1.Ambiguity

 

V‚àíSTACKS
Spectral Algorithm

2.Naive

4.RR

3.RLI

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1

Coverage Bound

Bound Amb. Naive
0.999
0.939
0.778
0.719
0.633
0.507
0.385
0.305
0.199

0.889
0.796
0.709
0.616
0.516
0.440
0.337
0.256
0.168

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

RLI
0.899
0.798
0.696
0.593
0.491
0.391
0.291
0.192
0.094

RR
0.942
0.842
0.735
0.628
0.526
0.423
0.324
0.224
0.131

(a) Error rate vs coverage bound

(b) Coverage rate vs coverage bound

Figure 2: S&P500 RC-curves

Figure 2a shows that all four methods exhibited meaningful RC-curves; namely  the error rates
decreased monotonically with decreasing coverage bounds. The RLI and RR models (curves 3 and 4 
respectively) outperformed the Naive one (curve 2)  by better exploiting the allotted coverage bound 
as is evident from Table 2b. In addition  the RR model outperformed the RLI model  and moreover 
its effective coverage is higher for every required coverage bound. This validates the effectiveness
of the RR approach that implements a smarter selection process than the RLI model. SpeciÔ¨Åcally 
when RR reÔ¨Ånes a state and the resulting sub-states have different risk rates  the selection procedure
will tend to reject riskier states Ô¨Årst. Comparing the state-based models (curves 2-4) to the ambiguity
model (curve 1)  we see that all the state-based models outperformed the ambiguity model through
the entire coverage range (despite the advantage we provided to the ambiguity model).

We also compared our models to two alternative HMM learning methods that were recently pro-
posed: the spectral algorithm of Hsu et al. [13]  and the V-STACKS algorithm of Siddiqi et al. [20].
As can be seen in Figure 2a  the selective techniques can also improve the accuracy obtained by
these methods (with full coverage).

Quantitatively very similar results were also obtained in a number of other experiments (not pre-
sented  due to lack of space) with continuous data (without discretization) of the S&P500 index and
of Gold  represented by its GLD exchange traded fund (ETF) replica.

6000

4000

2000

s
e
c
n
a
t
s
n

i
 
f
o
 
r
e
b
m
u
N

2000

1500

1000

500

s
e
c
n
a
t
s
n

i
 
f
o
 
r
e
b
m
u
N

0

‚àí0.2

‚àí0.1

0

Difference
(a) Visit

0.1

0.2

0
‚àí1

‚àí0.5

0.5

1

0

Difference
(b) Risk

Figure 3: Distributions of visit and risk train/test differences

Figure 3a depicts the distribution of differences between empirical visit rates  measured on the train-
ing set  and those rates on the test set. It is evident that this distribution is symmetric and con-
centrated around zero. This means that our empirical visit estimates are quite robust and useful.
Figure 3b depicts a similar distribution  but now for state risks. Unfortunately  here the distribution
is much less concentrated  which means that our naive empirical risk estimates are rather noisy.
While the distribution is symmetric about zero (and underestimates are often compensated by over-
estimates) it indicates that these noisy measurements are a major bottleneck in achieving better error
rates. Therefore  it would be very interesting to consider more sophisticated risk estimation methods.

7

5 Related Work

Selective classiÔ¨Åcation was introduced by Chow [5]  who took a Bayesian route to infer the optimal
rejection rule and analyze the risk-coverage trade-off under complete knowledge of the underlying
probabilistic source. Chow‚Äôs Bayes-optimal policy is to reject instances whenever none of the pos-
teriori probabilities are sufÔ¨Åciently predominant. While this policy cannot be explicitly applied in
agnostic settings  it marked a general ambiguity-based approach for rejection strategies. There is
a substantial volume of research contributions on selective classiÔ¨Åcation where the main theme is
the implementation of reject mechanisms for particular classiÔ¨Åer learning algorithms like support
vector machines  see  e.g.  [21]. Most of these mechanisms can be viewed as variations of the Chow
ambiguity-based policy. The general consensus is that selective classiÔ¨Åcation can often provide
substantial error reductions and therefore rejection techniques have found good use in numerous
applications  see  e.g.  [12]. Rejection mechanisms were also utilized in [14] as a post-processing
output veriÔ¨Åer for HMM-based recognition systems There have been also a few theoretical studies
providing worst case high probability bounds on the risk-coverage trade-off; see  .e.g.  [1  6  7  9].

HMMs have been extensively studied and used both theoretically and in numerous application areas.
In particular  Ô¨Ånancial modeling with HMMs has been considered since their introduction by Baum
et al. While a complete survey is clearly beyond our scope here  we mention a few related results.
Hamilton [11] introduced a regime-switching model  in which the sequence is hypothesized to be
generated by a number of hidden sources  or regimes  whose switching process is modeled by a (Ô¨Årst-
order) Markov chain. Later  in [19] a hidden Markov model of neural network ‚Äúexperts‚Äù was used for
prediction of half-hour and daily price changes of the S&P500 index. Zhang [23] applied this model
for predicting S&P500 next day trends  employing mixture of Gaussians in the states. The latter two
works reported on prominent results in terms of cumulative proÔ¨Åt. The recent experimental work by
Rao and Hong [17] evaluated HMMs for a next-day trend prediction task and measured performance
in terms of accuracy. They reported on a slight but consistent positive prediction edge.

In [3]  an HMM-based classiÔ¨Åer was proposed for ‚Äúreliable trends ‚Äù deÔ¨Åned to be specialized 15 day
return sequences that end with either Ô¨Åve consecutive positive or consecutive negative returns. A
classiÔ¨Åer was constructed using two HMMs  one trained to identify upward (reliable) trends and the
other  for downward (reliable) trends. Non-reliable sequences are always rejected. Therefore  this
technique falls within selective prediction but the selection function has been manually predeÔ¨Åned.

6 Concluding Remarks

The structure and modularity of HMMs make them particularly convenient for incorporating selec-
tive prediction mechanisms. Indeed  the proposed state-based method can result in a smooth and
monotonically decreasing risk-coverage trade-off curve that allows for some control on the desired
level of selectivity. We focused on selective prediction of trends in Ô¨Ånancial sequences. For these
difÔ¨Åcult prediction tasks our models can provide non-trivial prediction improvements. We expect
that the relative advantage of these selective prediction techniques will be higher in easier tasks  or
even in the same task by utilizing more elaborate HMM modeling  perhaps including other sources
of specialized information including prices of other correlated indices.

We believe that a major bottleneck in attaining smaller test errors is the noisy risk estimates we obtain
for the hidden states (see Figure 3b). This noise is partly due to the noisy nature of our prediction
problem  but may also be attributed to the simplistic approach we took in estimating empirical risk.
A challenging problem would be to incorporate more robust estimates in our mechanism  which
is likely to enable better risk-coverage trade-offs. Finally  it will be very interesting to examine
selective prediction mechanisms in the more general context of Bayesian networks and other types
of graphical models.

Acknowledgements

This work was supported in part by the IST Programme of the European Community  under the
PASCAL2 Network of Excellence  IST-2007-216886. This publication only reÔ¨Çects the authors‚Äô
views.

8

References

[1] P. L. Bartlett and M. H. Wegkamp. ClassiÔ¨Åcation with a reject option using a hinge loss.

Journal of Machine Learning Research  9:1823‚Äì1840  2008.

[2] L. E. Baum  T. Petrie  G. Soules  and N. Weiss. A maximization technique occurring in the
statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical
Statistics  41(1):164‚Äì171  1970.

[3] M. Bicego  E. Grosso  and E. Otranto. A Hidden Markov Model approach to classify and

predict the sign of Ô¨Ånancial local trends. SSPR  5342:852‚Äì861  2008.

[4] M. Brand. Coupled Hidden Markov Models for modeling interacting processes. Technical

Report 405  MIT Media Lab  1997.

[5] C. Chow. On optimum recognition error and reject tradeoff. IEEE-IT  16:41‚Äì46  1970.
[6] R. El-Yaniv and Y. Wiener. On the foundations of noise-free selective classiÔ¨Åcation. JMLR 

11:1605‚Äì1641  May 2010.

[7] R. El-Yaniv and Y. Wiener. Agnostic selective classiÔ¨Åcation. In NIPS  2011.
[8] S. Fine  Y. Singer  and N. Tishby. The Hierarchical Hidden Markov Model: Analysis and

Applications. Machine Learning  32(1):41‚Äì62  1998.

[9] Y. Freund  Y. Mansour  and R. E. Schapire. Generalization bounds for averaged classiÔ¨Åers.

Annals of Statistics  32(4):1698‚Äì1722  2004.

[10] Z. Ghahramani and M. I. Jordan. Factorial Hidden Markov Models. Machine Learning  29(2‚Äì

3):245‚Äì273  1997.

[11] J. Hamilton. Analysis of time series subject to changes in regime. Journal of Econometrics 

45(1‚Äì2):39‚Äì70  1990.

[12] B. Hanczar and E. R. Dougherty. ClassiÔ¨Åcation with reject option in gene expression data.

Bioinformatics  24:1889‚Äì1895  2008.

[13] D. Hsu  S. Kakade  and T. Zhang. A spectral algorithm for learning Hidden Markov Models.

In COLT  2009.

[14] A. L. Koerich. Rejection strategies for handwritten word recognition. In IWFHR  2004.
[15] A. Krogh. Hidden Markov Models for labeled sequences. In Proceedings of the 12th IAPR

ICPR‚Äô94  pages 140‚Äì144  1994.

[16] L. R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speech recog-

nition. Proceedings of the IEEE  77(2)  February 1989.

[17] S. Rao and J. Hong. Analysis of Hidden Markov Models and Support Vector Machines in Ô¨Ånan-
cial applications. Technical Report UCB/EECS-2010-63  Electrical Engineering and Computer
Sciences University of California at Berkeley  2010.

[18] L. K. Saul and M. I. Jordan. Mixed memory Markov models: Decomposing complex stochastic

processes as mixtures of simpler ones. Machine Learning  37:75‚Äì87  1999.

[19] S. Shi and A. S. Weigend. Taking time seriously: Hidden Markov Experts applied to Ô¨Ånancial

engineering. In IEEE/IAFE  pages 244‚Äì252. IEEE  1997.

[20] S. Siddiqi  G. Gordon  and A. Moore. Fast State Discovery for HMM Model Selection and

Learning. In AI-STATS  2007.

[21] F. Tortorella. Reducing the classiÔ¨Åcation cost of support vector classiÔ¨Åers through an ROC-

based reject rule. Pattern Anal. Appl.  7:128‚Äì143  2004.

[22] A. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding

algorithm. IEEE-IT  13(2):260‚Äì269  1967.

[23] Y. Zhang. Prediction of Ô¨Ånancial time series with Hidden Markov Models. Master‚Äôs thesis 

The School of Computing Science  Simon Frazer University  Canada  2004.

9

,Pingfan Tang
Jeff Phillips