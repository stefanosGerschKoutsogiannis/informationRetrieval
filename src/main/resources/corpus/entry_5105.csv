2011,Sparse Estimation with Structured Dictionaries,In the vast majority of recent work on sparse estimation algorithms  performance has been evaluated using ideal or quasi-ideal dictionaries (e.g.  random Gaussian or Fourier) characterized by unit $\ell_2$ norm  incoherent columns or features.  But in reality  these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications).  In contrast  herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows.  Sparse penalized regression models are analyzed with the purpose of finding  to the extent possible  regimes of dictionary invariant performance.  In particular  a Type II Bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $\ell_1$ norm  especially in areas where existing theoretical recovery guarantees no longer hold.  This can translate into improved performance in applications such as model selection with correlated features  source localization  and compressive sensing with constrained measurement directions.,Sparse Estimation with Structured Dictionaries

David P. Wipf ∗

Visual Computing Group
Microsoft Research Asia

davidwipf@gmail.com

Abstract

In the vast majority of recent work on sparse estimation algorithms  performance
has been evaluated using ideal or quasi-ideal dictionaries (e.g.  random Gaussian
or Fourier) characterized by unit ℓ2 norm  incoherent columns or features. But in
reality  these types of dictionaries represent only a subset of the dictionaries that
are actually used in practice (largely restricted to idealized compressive sensing
applications). In contrast  herein sparse estimation is considered in the context
of structured dictionaries possibly exhibiting high coherence between arbitrary
groups of columns and/or rows. Sparse penalized regression models are analyzed
with the purpose of ﬁnding  to the extent possible  regimes of dictionary invari-
ant performance. In particular  a Type II Bayesian estimator with a dictionary-
dependent sparsity penalty is shown to have a number of desirable invariance
properties leading to provable advantages over more conventional penalties such
as the ℓ1 norm  especially in areas where existing theoretical recovery guarantees
no longer hold. This can translate into improved performance in applications such
as model selection with correlated features  source localization  and compressive
sensing with constrained measurement directions.

1

Introduction

We begin with the generative model

Y = ΦX0 + E 

(1)
where Φ ∈ Rn×m is a dictionary of basis vectors or features  X0 ∈ Rm×t is a matrix of unknown
coefﬁcients we would like to estimate  Y ∈ Rn×t is an observed signal matrix  and E is a noise
matrix with iid elements distributed as N (0  λ). The objective is to estimate the unknown genera-
tive X0 under the assumption that it is row-sparse  meaning that many rows of X0 have zero norm.
The problem is compounded considerably by the additional assumption that m > n  meaning the
dictionary Φ is overcomplete. When t = 1  this then reduces to the canonical sparse estimation of
a coefﬁcient vector with mostly zero-valued entries or minimal ℓ0 norm [7]. In contrast  estimation
of X0 with t > 1 represents the more general simultaneous sparse approximation problem [6  15]
relevant to numerous applications such as compressive sensing and multi-task learning [9  16]  man-
ifold learning [13]  array processing [10]  and functional brain imaging [1]. We will consider both
scenarios herein but will primarily adopt the more general notation of the t > 1 case.
One possibility for estimating X0 involves solving

X kY − ΦXk2
min

F + λd(X) 

λ > 0 

d(X)  

I [kxi·k > 0]  

(2)

mXi=1

where the indicator function I [kxk > 0] equals one if kxk > 0 and equals zero otherwise (kxk
is an arbitrary vector norm). d(X) penalizes the number of rows in X that are not equal to zero;

∗Draft version for NIPS 2011 pre-proceedings.

1

for nonzero rows there is no additional penalty for large magnitudes. Moreover  it reduces to the ℓ0
norm when t = 1  i.e.  d(x) = kxk0  or a count of the nonzero elements in the vector x. Note that
to facilitate later analysis  we deﬁne x·i as the i-th column of matrix X while xi· represents the i-th
row. For theoretical inquiries or low-noise environments  it is often convenient to consider the limit
as λ → 0  in which case (2) reduces to
min
X

s.t. ΦX0 = ΦX.

d(X) 

(3)

Unfortunately  solving either (2) or (3) involves a combinatorial search and is therefore not tractable
in practice. Instead  a family of more convenient sparse penalized regression cost functions are re-
viewed in Section 2. In particular  we discuss conventional Type I sparsity penalties  such as the ℓ1
norm and the ℓ1 2 mixed norm  and a Type II empirical Bayesian alternative characterized by dictio-
nary dependency. When the dictionary Φ is incoherent  meaning the columns are roughly orthogonal
to one another  then certain Type I selections are well-known to produce good approximations of X0
via efﬁcient implementations. However  as discussed in Section 3  more structured dictionary types
can pose difﬁculties. In Section 4 we analyze the underlying cost functions of Type I and Type II 
and demonstrate that the later maintains several properties that suggest it will be robust to highly
structured dictionaries. Brief empirical comparisons are presented in Section 5.

2 Estimation via Sparse Penalized Regression

Directly solving either (2) or (3) is intractable  so a variety of approximate methods have been
proposed. Many of these can be viewed simply as regression with a sparsity penalty convenient for
optimization purposes. The general regression problem we consider here involves solving

X kY − ΦXk2
min

F + λg(X) 

(4)

where g is some penalty function of the row norms. Type I methods use a separable penalty of the
form

g(I)(X) =Xi

h (kxi·k2)  

(5)

where h is a non-decreasing  typically concave function.1 Common examples include h(z) =
zp  p ∈ (0  1] [11] and h(z) = log(z + α)  α ≥ 0 [4]. The parameters p and α are often heuristically
selected on an application-speciﬁc basis. In contrast  Type II methods  with origins as empirical
Bayesian estimators  implicitly utilize a more complicated penalty function that can only be ex-
pressed in a variational form [18]. Herein we will consider the selection

g(II) (X)   min
Γ(cid:23)0

Tr(cid:2)X T Γ−1X(cid:3) + t log(cid:12)(cid:12)αI + ΦΓΦT(cid:12)(cid:12)   α ≥ 0 

where Γ is a diagonal matrix of non-negative variational parameters [14  18]. While less transparent
than Type I  it has been shown that (6) is a concave non-decreasing function of each row norm of X 
hence it promotes row sparsity as well. Moreover  the dictionary-dependency of this penalty appears
to be the source of some desirable invariance properties as discussed in Section 4. Analogous to (3) 
for analytical purposes all of these methods can be reduced as λ → 0 to solving

(6)

(7)

min
X

g(X)

3 Structured Dictionaries

s.t. ΦX0 = ΦX.

It is now well-established that when the dictionary Φ is constructed with appropriate randomness 
e.g.  iid Gaussian entries  then for certain choices of g  in particular the convex selection g(X) =

Pi kxi·k2 (which represents a generalization of the ℓ1 vector norm to row-sparse matrices)  we

can expect to recover X0 exactly in the noiseless case or to close approximation otherwise. This
assumes that d(X0) is sufﬁciently small relative to some function of the dictionary coherence or a
related measure. However  with highly structured dictionaries these types of performance guarantees
completely break down.

1Other row norms  such as the ℓ∞  have been considered as well but are less prevalent.

2

At the most basic level  one attempt to standardize structured dictionaries is by utilizing some form
of column normalization as a pre-processing step. Most commonly  each column is scaled such that
it has unit ℓ2 norm. This helps ensure that no one column is implicitly favored over another during
the estimation process. However  suppose our observation matrix is generated via Y = ΦX0  where

represents a rank one adjustment. If we apply column normalization to remove the effect of D  the
resulting scale factors will be dominated by the rank one term when σ is large. But if we do not
column normalize  then D can completely bias the estimation results.

Φ =eΦD +σabT  eΦ is some well-behaved  incoherent dictionary  D is a diagonal matrix  and σabT
In general  if our given dictionary is effectively WeΦD  with W an arbitrary invertible matrix that

scales and correlates rows  and D diagonal  the combined effect can be severely disruptive. As
an example from neuroimaging  the MEG/EEG source localization problem involves estimating
sparse neural current sources within the brain using sensors placed near the surface of the scalp.
The effective dictionary or forward model is characterized by highly correlated rows (because the
sensors are physically constrained to be near one another) and columns with drastically different
scales (since deep brain sources produce much weaker signals at the surface than superﬁcial ones).

arbitrary coherence structure between individual or groups of columns in Φ  meaning the structure

More problematic is the situation where Φ = eΦS  since an unrestricted matrix S can introduce
of Φ is now arbitrary regardless of how well-behaved the originaleΦ.

4 Analysis

We will now analyze the properties of both Type I and Type II cost functions when coherent or highly
structured dictionaries are present. Ideally  we would like to arrive at algorithms that are invariant 
to the extent possible  to dictionary transformations that would otherwise disrupt the estimation
efﬁcacy. For simplicity  we will primarily consider the noiseless case  although we surmise that
much of the underlying intuition carries over into the noiseless domain. This strategy mirrors the
progression in the literature of previous sparse estimation theory related to the ℓ1 norm [3  7  8]. All
proofs have been deferred to the Appendix  with some details omitted for brevity.

4.1

Invariance to W and D

g(II)(X)

min
X

We will ﬁrst consider the case where the observation matrix is produced via Y = ΦX0 = WeΦDX0.
Later in Sections 4.2 and 4.3 we will then address the more challenging situation where Φ =eΦS.
Lemma 1. Let W denote an arbitrary full-rank n × n matrix and D an arbitrary full-rank m × m
diagonal matrix. Then with α → 0  the Type II optimization problem
s.t. WeΦDX0 = WeΦDX
(8)

is invariant to W and D in the sense that if X ∗ is a global (or local) minimum to (8)  then D−1X ∗

is a global (or local) minimum when we optimize g(II)(X) subject to the constrainteΦX0 =eΦX.
Therefore  while switching between Φ = WeΦD and Φ =eΦ may inﬂuence the initialization and pos-

sibly the update rules of a particular Type II algorithm  it does not fundamentally alter the underlying
cost function. In contrast  Type I methods do not satisfy this invariance. Invariance is preserved with
a W factor in isolation. Likewise  inclusion of a D factor alone with column normalization leads to
invariance. However  inclusion of both W and D together can be highly disruptive.
Note that for improving Type I performance  it is not sufﬁcient to apply some row decorrelating and
normalizing ˆW −1 to Φ and then column normalize with some ˆD−1. This is because the application
of ˆD−1 will disrupt the effects of ˆW −1. But one possibility to compensate for dictionary structure is
to jointly learn a ˆW −1 and ˆD−1 that produces a Φ satisfying: (i) ΦΦT = CI (meaning rows have a
constant ℓ2 norm of C and are uncorrelated  (ii) kφ·ik2 = 1 for all i. Up to irrelevant scale factors  a
unique such transformation will always exist. In Section 5 we empirically demonstrate that this can
be a highly effective strategy for improving the performance of Type I methods. However  as a ﬁnal
point  we should mention that the invariance Type II exhibits towards W and D (or any corrected
form of Type I) will no longer strictly hold once noise is added.

3

4.2

Invariance to S: The t > 1 Case (Simultaneous Sparse Approximation)

We now turn to the potentially more problematic scenario with Φ = eΦS. We will assume that S
is arbitrary with the only restriction being that the resulting Φ satisﬁes spark[Φ] = n + 1  where
matrix spark quantiﬁes the smallest number of linearly dependent columns [7]. Consequently  the
spark condition is equivalent to saying that each n × n sub-matrix of Φ is full rank. This relatively
weak assumption is adopted for simplicity; in many cases it can be relaxed.

Lemma 2. Let Φ be an arbitrary dictionary with spark [Φ] = n + 1 and X0 a coefﬁcient matrix with
d(X0) < n. Then there exists a constant ρ > 0 such that the optimization problem (7)  with g(X) =
g(II)(X) and α → 0  has no local minima and a unique  global solution at X0 if (x0)T
i· (x0)j· ≤ ρ
for all i 6= j (i.e.  the nonzero rows of X0 are below some correlation threshold). Also  if we enforce
exactly zero row-wise correlations  meaning ρ = 0  then a minimizing solution X ∗ will satisfy
i·k2 = k(x0)i·k2 for all i (i.e.  a matching row-sparsity support)  even for d(X0) ≥ n. This
kx∗
solution will be unique whenever ΦX0X T
0 Φ = ΦΓΦT has a unique solution for some non-negative 
diagonal Γ.2

Corollary 1. There will always exist dictionaries Φ and coefﬁcients X0  consistent with the con-
ditions from Lemma 2  such that the optimization problem (7) with any possible g(X) of the form

g(I)(X) =Pi h (kxi·k2) will have minimizing solutions not equal to X0 (with or without column

normalization).

In general  Lemma 2 suggests that for estimation purposes uncorrelated rows in X0 can potentially
compensate for troublesome dictionary structure  and together with Corollary 1 it also describes a
potential advantage of Type II over Type I. Of course this result only stipulates sufﬁcient conditions
for recovery that are certainly not necessary  i.e.  effective sparse recovery is possible even with
correlated rows (more on this below). We also emphasize that the ﬁnal property of Lemma 2 implies
that the row norms of X0 (and therefore the row-sparsity support) can still be recovered even up
to the extreme case of d(X0) = m > n. While this may seem surprising at ﬁrst  especially since
even brute force minimization of (3) can not achieve a similar feat  it is important to keep in mind
that (3) is blind to the correlation structure of X0. Although Type II does not explicitly require any
such structure  it is able to outperform (3) by implicitly leveraging this structure when the situation
happens to be favorable. While space prevents a full treatment  in the context of MEG/EEG source
estimation  we have successfully localized 500 nonzero sources (rows) using a 100×1000 dictionary.
However  what about the situation where strong correlations do exist between the nonzero rows of
X0? A couple things are worth mentioning in this regard. First  Lemma 2 can be strengthened
considerably via the expanded optimization problem: minX B g(II)(X) s.t. ΦX0 = ΦXB  which
achieves a result similar to Lemma 2 but with a weaker correlation condition (although the row-
norm recovery property is lost). Secondly  in the case of perfect correlation between rows (the
hardest case)  the problem reduces to an equivalent one with t = 1  i.e.  it exactly reduces to the
canonical sparse recovery problem. We address this situation next.

4.3

Invariance to S: The t = 1 Case (Standard Sparse Approximation)

This section considers the t = 1 case  meaning Y = y and X0 = x0 are now vectors. For
convenience  we deﬁne X (S P) as the set of all coefﬁcient vectors in Rm with support (or nonzero
coefﬁcient locations) speciﬁed by the index set S ⊂ {1  . . .   m} and sign pattern given by P ∈
{−1  +1}|S| (here the | · | operator denotes the cardinality of a set).
Lemma 3. Let Φ be an arbitrary dictionary with spark [Φ] = n + 1. Then for any X (S P) with
|S| < n  there exists a non-empty subset ¯X ⊂ X (S P) (with nonzero Lebesgue measure)  such
that if x0 ∈ ¯X   the Type II minimization problem
(9)

g(II)(x)

min

will have a unique minimum and it will be located at x0.

x

s.t. Φx0 = Φx  α → 0

2See Appendix for more details about this condition. In most situations  it will hold if m < n(n + 1)/2 

and likely for many instances with m even greater than this.

4

This Lemma can be obtained with a slight modiﬁcation of results in [18]. In other words  no mat-
ter how poorly structured a particular dictionary is with regard to a given sparsity proﬁle  there
will always be sparse coefﬁcients we are guaranteed to recover (provided we utilize a convergent
algorithm). In contrast  an equivalent claim can not be made for Type I:

Lemma 4. Given an arbitrary Type I penalty g(I)(x) =Pi h(|xi|)  with h a ﬁxed  non-decreasing
function  there will always exist a dictionary Φ (with or without normalized columns) and set
X (S P) such that for any x0 ∈ X (S P)  the problem

min

x

g(I)(x)

s.t. Φx0 = Φx

(10)

will not have a unique minimum located at x0.

This can happen because the global minimum does not equal x0 and/or because of the presence of
local minima. Of course this does not necessarily imply that a particular Type I algorithm will fail.
For example  even with multiple minima  an appropriate optimization strategy could conceivably
still locate an optimum that coincides with x0. While it is difﬁcult to analyze all possible algorithms 
we can address one inﬂuential variety based on iterative reweighted ℓ1 minimization [4  18]. Here
the idea is that if h is concave and differentiable  then a convergent means of minimizing (10) is to
utilize a ﬁrst-order Taylor series approximation of g(I)(x) at some point ˆx. This leads to an iterative
  dh(z)/dz|z=|ˆxi| and then minimize
procedure where at each step we must ﬁrst compute h′
i
i|xi| subject to Φx0 = Φx to update ˆx. This method produces a sparse estimate at each
iteration and is guaranteed to converge to a local minima (or stationary point) of (10). However  this
solution may be suboptimal in the following sense:

Pi h′
Corollary 2. Given an arbitrary g(I)(x) as in Lemma 4  there will always exist a Φ and X (S P) 
such that for any x0 ∈ X (S P)  iterative reweighted ℓ1 minimization will not converge to x0 when
initialized at the minimum ℓ1 norm solution.

Note that this failure does not result from a convergence pathology. Rather  the presence of minima
different from x0 explicitly disrupts the algorithm.
In general  with highly structured dictionaries deviating from the ideal  the global minimum of con-
vex penalties often does not correspond with x0 as theoretical equivalence results break down. This
in turn suggests the use of concave penalty functions to seek possible improvement. However  as
illustrated by the following result  even the simplest of sparse recovery problems  that of estimating
some x0 with only one nonzero element using a dictionary with a 1D null-space  Type I can be
characterized by problematic local minima with (strictly) concave penalties. For this purpose we
deﬁne φ∗ as an arbitrary column of Φ and ¯Φ∗ as all columns of Φ excluding φ∗.

Lemma 5. Let h denote a concave  non-decreasing function with h′
  limz→0 dh(z)/dz and
  limz→∞ dh(z)/dz. Also  let Φ be a dictionary with unit ℓ2 norm columns and spark [Φ] =
h′
m = n + 1 (i.e.  a 1D null-space)  and let x0 satisfy kx0k0 = 1 with associated φ∗. Then the Type
I problem (10) can have multiple local minima if

max

min

h′
max
h′

min

> k ¯Φ−1

∗ φ∗k1.

(11)

This result has a very clear interpretation related to how dictionary coherence can potentially disrupt
even the most rudimentary of estimation tasks. The righthand side of (11) is bounded from below by
1  which is approached whenever one or more columns in some ¯Φ∗ are similar to φ∗ (i.e.  coherent).
Thus  even the slightest amount of curvature (or strict concavity) in h can lead to the inequality being
satisﬁed when highly coherent columns are present. While obviously with h(z) = z this will not be
an issue (consistent with the well-known convexity of the ℓ1 problem)  for many popular non-convex
penalties  this gradient ratio may be large relative to the righthand side  indicating that local minima

5

are always possible. For example  with the h(z) = log(z + α) selection from [4] h′
min → 0 for all α
while h′
max → 1/α. We note that Type II has provably no local minima in this regime (this follows
as a special case of Lemma 3). Of course the point here is not that Type I algorithms are incapable
of solving simple problems with kx0k0 = 1 (and any iterative reweighted ℓ1 scheme will succeed
on the ﬁrst step anyway). Rather  Lemma 5 merely demonstrates how highly structured dictionaries
can begin to have negative effects on Type I  potentially more so than with Type II  even on trivial
tasks. The next section will empirically explore this conjecture.

5 Empirical Results

We now present two simulation examples illustrating the potential beneﬁts of Type II with highly
structured dictionaries. In the ﬁrst experiment  the dictionary represents an MEG leadﬁeld  which at
a high level can be viewed as a mapping from the electromagnetic (EM) activity within m brain vox-
els to n sensors placed near the scalp surface. Computed using Maxwell’s equations and a spherical
shell head model [12]  the resulting Φ is characterized by highly correlated rows  because the small
scalp surface requires that sensors be placed close together  and vastly different column norms  since
the EM ﬁeld strength drops off rapidly for deep brain sources. These effects are well represented by a

dictionary such as Φ = WeΦD as discussed previously. Figure 1 (Left) displays trial-averaged results

comparing Type I algorithms with Type II using such an MEG leadﬁeld dictionary. Data generation
proceeded as follows: We produce Φ by choosing 50 random sensor locations and 100 random vox-
els within the brain volume. We then create a coefﬁcient matrix X0 with t = 5 columns and d(X0)
an experiment-dependent parameter. Nonzero rows of X0 are drawn iid from a unit Gaussian distri-
bution. The observation matrix is then computed as Y = ΦX0. We run each algorithm and attempt
to estimate X0  calculating the probability of success averaged over 200 trials as d(X0) is varied
from 10 to 50. We compared Type II  implemented via a simple iterative reweighted ℓ2 approach 
with two different Type I schemes. The ﬁrst is a homotopy continuation method using the Type I
2 + α)  where α is gradually reduced to zero during the estimation
process [5]. We have often found this to be the near optimal Type I approach on a variety empirical

penalty g(I)(X) =Pi log(kxi·k2
tests. Secondly  we used the standard mixed-norm penalty g(I)(X) = kXk1 2 =Pi kxi·k2  which

leads to a convex minimization problem that generalizes basis pursuit (or the lasso)  to the t > 1
domain [6  10].

While Type II displays invariance to W - and D-like transformations  Type I methods do not. Con-
sequently  we examined two dictionary-standardization methods for Type I. First  we utilized basic
ℓ2 column normalization  without which Type I will have difﬁculty with the vastly different column
scalings of Φ. Secondly  we developed an algorithm to learn a transformed dictionary ˆU Φ ˆΠ  with
ˆU arbitrary  ˆΠ diagonal  such that the combined dictionary has uncorrelated  unit ℓ2 norm rows  and
unit ℓ2 norm columns (as discussed in Section 4.1). Figure 1(left) contains results from all of these
variants  where it is clear that some compensation for the dictionary structure is essential for good
recovery performance. We also note that Type II still outperforms Type I in all cases  suggesting
that even after transformation of the latter  there is still residual structure in the MEG leadﬁeld being
exploited by Type II. This is a very reasonable assumption given that Φ will typically have strong
column-wise correlations as well  which are more effectively modeled by right multiplication by
some S. As a ﬁnal point  the Type II success probability does not go to zero even when d(X0) = 50 
implying that in some cases it is able to ﬁnd a number of nonzeros equal to the number of rows in Φ.
This is possible because even with only t = 5 columns  the nonzero rows of X0 display somewhat
limited sample correlation  and so exact support recovery is still possible. With t > 5 these sample
correlations can be reduced further  allowing consistent support recovery when d(X0) > n (not
shown).

second experiment with explicitly controlled correlations among groups of columns. For each trial

To further test the ability of Type II to handle structure imposed by some eΦS  we performed a
we generated a 50 × 100 Gaussian iid dictionary eΦ. Correlations were then introduced using a
block-diagonal S with 4 × 4 blocks created with iid entries drawn from a uniform distribution
(between 0 and 1). The resulting Φ = eΦS was then scaled to have unit ℓ2 norm columns. We
then generated a random x0 vector (t = 1 case) using iid Gaussian nonzero entries with d(x0)
varied from 10 to 25 (with t = 1  we cannot expect to recover as many nonzeros as when t = 5).
Signal vectors are computed as y = Φx0 or  for purposes of direct comparison with a canonical
iid dictionary  y = eΦx0. We evaluated Type II and the Type I iterative reweighted ℓ1 minimization

6

method from [4]  which is guaranteed to do as well or better than standard ℓ1 norm minimization.

Type II performance is essentially unchanged  Type I performance degrades substantially.

Trial-averaged results using both Φ andeΦ are shown in Figure 1(right)  where it is clear that while

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
s
 
f

o

 
y
t
i
l
i

b
a
b
o
r
p

 

0
10

 

Type II
Type I  homotopy  norm
Type I  homotopy  invariant
Type I  basis pursuit  norm
Type I  basis pursuit  invariant

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

s
s
e
c
c
u
s
 
f

o

 
y
t
i
l
i

b
a
b
o
r
p

Type II  iid Φ
Type II  coherent Φ
Type I  iid Φ
Type I  coherent Φ

 

15

20

25

30

row−sparsity

35

40

45

50

0.1

 

10

15

row−sparsity

20

25

Figure 1: Left: Probability of success recovering coefﬁcient matrices with varying degrees of row-
sparsity using an MEG leadﬁeld as the dictionary. Two Type I methods were compared  a homotopy
continuation method from [5] and a version of basis pursuit extended to the simultaneous sparse
approximation problem by minmizing the ℓ1 2 mixed norm [6  10]. Type I methods were compared
using standard ℓ2 column normalization and a learned invariance transformation. Right: Probability

with clustered columns. The Type I method was the interactive reweighted ℓ1 algorithm from [4].

of success recovering sparse vectors using a Gaussian iid dictionaryeΦ and a coherent dictionary Φ

6 Conclusion

When we are free to choose the basis vectors of an overcomplete signal dictionary  the sparse estima-
tion problem is supported by strong analytical and empirical foundations. However  there are many
applications where physical restrictions or other factors impose rigid constraints on the dictionary
structure such that the assumptions of theoretical recovery guarantees are violated. Examples in-
clude model selection problems with correlated features  source localization  and compressive sens-
ing with constrained measurement directions. This can have signiﬁcant consequences depending
on how the estimated coefﬁcients will ultimately be utilized. For example  in the source localiza-
tion problem  correlated dictionary columns may correspond with drastically different regions (e.g. 
brain areas)  so recovering the exact sparsity proﬁle can be important. Ideally we would like our
recovery algorithms to display invariance  to the extent possible  to the actual structure of the dic-
tionary. With typical Type I sparsity penalties this can be a difﬁcult undertaking; however  with the
natural dictionary dependence of the Type II penalty  to some extent it appears this structure can be
accounted for  leading to more consistent performance across dictionary types.

Appendix

Here we provide brief proofs of several results from the paper. Some details have been omitted for
space considerations.
Proof of Lemma 1: First we address invariance with respect to W . Obviously the equality constraint
is unaltered by a full rank W   so it only remains to check that the dictionary-dependent penalty

purposes  this point is established. With respect to D  we re-parameterize the problem by deﬁning

g(II) is invariant. However  since by standard determinant relationships log |WeΦDΓDeΦT W T| =
log |W||eΦDΓDeΦT||W T| = log |eΦDΓDeΦT|+C  where C is an irrelevant constant for optimization
eX   DX andeΓ   DΓD. It is then readily apparent that the penalty (6) satisﬁes
Tr(cid:2)X T Γ−1X(cid:3) + log |eΦDΓDeΦT| = min
g(II) (X) ≡ min
So we are effectively solving: min eX g(II)(eX) s.t. eΦDX0 =eΦeX.

TrheX TeΓ−1eXi + log |eΦeΓeΦT|.

Proof of Lemma 2 and Corollary 1: Minimizing the Type II cost function can be accomplished
equivalently by minimizing

(12)

(cid:4)

Γ(cid:23)0

eΓ(cid:23)0

7

Φ =" ǫ

ǫ
1 −1
0
0

1
0

1
0

ǫ −ǫ #   X(1) =

1
1
1 −1
0
0
0
0

   X(2) =

0
1
0 −1
0
ǫ
ǫ
0

  

(14)

L(Γ)   TrhΦt−1X0X T

0 ΦT(cid:0)ΦΓΦT(cid:1)−1i + log |ΦΓΦT| 

0 ΦT   meaning the off-diagonal elements of X0X T

(13)
over the non-negative diagonal matrix Γ (this follows from a duality principle in Type II models
0 ΦT and a parameterized model covariance
[18]). L(Γ) includes an observed covariance Φt−1X0X T
0 ] [17]. Moreover  if ΦΓ∗ΦT is sufﬁ-
ΦΓΦT   and is globally minimized with Γ∗ = t−1diag[X0X T
0 are not too large  then
ciently close to t−1ΦX0X T
it can be shown by differentiating along the direction between any arbitrary point Γ′ and Γ∗ that no
local minima exist  leading to the ﬁrst part of Lemma 2.
Regarding the second part  we now allow d(X0) to be arbitrary but require that X0X T
0 be diagonal
(zero correlations). Using similar arguments as above  it is easily shown that any minimizing solu-
tion Γ∗ must satisfy ΦΓ∗ΦT = Φt−1X0X T
0 ΦT . This equality can be viewed as n(n + 1)/2 linear
equations (equal to the number of unique elements in an n× n covariance matrix) and m unknowns 
namely  the diagonal elements of Γ∗. Therefore  if n(n + 1)/2 > m this system of equations will
typically be overdetermined (e.g.  if suitable randomness is present to avoid adversarial conditions)
with a unique solution. Moreover  because of the requirement that Γ be non-negative  it is likely that
a unique solution will exist in many cases where m is even greater than n(n + 1)/2 [2].
Finally  we address Corollary 1. First  consider the case where t = 1  so X0 = x0. To satisfy the
now degenerate correlation condition  we must have d(x0) = 1. Even in this simple regime it can be
demonstrated that a unique minimum at x0 is possible iff h(z) = z based on Lemma 5 (below) and a
complementary result in [17]. So the only Type I possibility is h(z) = z. A simple counterexample
with t = 2 serves to rule this selection out. Consider a dictionary Φ and two coefﬁcient matrices
given by

It is easily veriﬁed that ΦX(1) = ΦX(2) and that X(1) = X0  the maximally row-sparse
solution. Computing the Type I cost for each with h(z) = z gives g(I)(X(1)) = 2√2 and
g(I)(X(2)) = 2(1 + ǫ). Thus  if we allow ǫ to be small  g(I)(X(2)) < g(I)(X(1))  so X(1) = X0
cannot be the minimizing solution. Note that ℓ2 column normalization will not change this
conclusion since all columns of Φ have equal norm already.
(cid:4)

Proof of Lemma 4 and Corollary 2: For brevity  we will assume that h is concave and differentiable 
as is typical of most sparsity penalties used in practice (the more general case follows with some
additional effort). This of course includes h(z) = z  which is both concave and convex  and leads
to the ℓ1 norm penalty. These results will now be demonstrated using a simple counterexample
similar to the one above. Assume we have the dictionary Φ from (14)  and that S = {1  2} and
P = {+1  +1}  which implies that any x0 ∈ X (S P) can be expressed as x0 = [α1  α2  0  0]T   for
some α1  α2 > 0. We will now show that with any member from this set  there will not be a unique
minimum to the Type I cost at x0 for any possible concave  differentiable h.
First assume α1 ≥ α2. Consider the alternative feasible solution x(2) = [(α1 − α2)  0  ǫα2  ǫα2]T .
To check if this is a local minimum  we can evaluate the gradient of the penalty function g(I)(x)
along the feasible region near x(2). Given v = [1  1 −ǫ −ǫ]T ∈ Null(Φ)  this can be accomplished
by computing ∂g(I)(x(2) + βv)/∂β = h′(|α1 − α2 + β|) + h′(|β|) + 2ǫh′(|ǫα2 − ǫβ|). In the limit
as β → 0 (from the right or left)  this expression will always be positive for ǫ < 0.5 based on the
concavity of h. Therefore  x(2) must be a minimum. By symmetry an equivalent argument can be
made when α2 ≥ α1. (In the special case where α1 = α2  there will actually exist two maximally
sparse solutions  the generating x0 and x(2).) It is also straightforward to verify analytically that
iterative reweighted ℓ1 minimization will fail on this example when initialized at the minimum ℓ1
norm solution. It will always become trapped at x(2) after the ﬁrst iteration  assuming α1 ≥ α2  or
a symmetric local minimum otherwise.

(cid:4)

Proof of Lemma 5: This result can be shown by examining properties of various gradients along
the feasible region  not unlike some of the analysis above  and then bounding the resultant quantity.
We defer these details to a later publication.
(cid:4)

8

References

[1] S. Baillet  J.C. Mosher  and R.M. Leahy  “Electromagnetic brain mapping ” IEEE Signal

Processing Magazine  pp. 14–30  Nov. 2001.

[2] A.M. Bruckstein  M. Elad  and M. Zibulevsky  “A non-negative and sparse enough solution of
an underdetermined linear system of equations is unique ” IEEE Trans. Information Theory 
vol. 54  no. 11  pp. 4813–4820  Nov. 2008.

[3] E. Cand`es  J. Romberg  and T. Tao  “Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information ” IEEE Trans. Information Theory  vol. 52  no.
2  pp. 489–509  Feb. 2006.

[4] E. Cand`es  M. Wakin  and S. Boyd  “Enhancing sparsity by reweighted ℓ1 minimization ” J.

Fourier Anal. Appl.  vol. 14  no. 5  pp. 877–905  2008.

[5] R. Chartrand and W. Yin  “Iteratively reweighted algorithms for compressive sensing ” Proc.

Int. Conf. Accoustics  Speech  and Signal Proc.  2008.

[6] S.F. Cotter  B.D. Rao  K. Engan  and K. Kreutz-Delgado  “Sparse solutions to linear inverse
problems with multiple measurement vectors ” IEEE Trans. Signal Processing  vol. 53  no. 7 
pp. 2477–2488  April 2005.

[7] D.L. Donoho and M. Elad  “Optimally sparse representation in general (nonorthogonal) dic-
tionaries via ℓ1 minimization ” Proc. National Academy of Sciences  vol. 100  no. 5  pp. 2197–
2202  March 2003.

[8] J.J. Fuchs  “On sparse representations in arbitrary redundant bases ” IEEE Trans. Information

Theory  vol. 50  no. 6  pp. 1341–1344  June 2004.

[9] S. Ji  D. Dunson  and L. Carin  “Multi-task compressive sensing ” IEEE Trans. Signal Pro-

cessing  vol. 57  no. 1  pp. 92–106  Jan 2009.

[10] D.M. Malioutov  M. C¸ etin  and A.S. Willsky  “Sparse signal reconstruction perspective for
source localization with sensor arrays ” IEEE Transactions on Signal Processing  vol. 53  no.
8  pp. 3010–3022  August 2005.

[11] B.D. Rao  K. Engan  S. F. Cotter  J. Palmer  and K. Kreutz-Delgado  “Subset selection in noise
based on diversity measure minimization ” IEEE Trans. Signal Processing  vol. 51  no. 3  pp.
760–770  March 2003.

[12] J. Sarvas  “Basic methematical and electromagnetic concepts of the biomagnetic inverse prob-

lem ” Phys. Med. Biol.  vol. 32  pp. 11–22  1987.

[13] J.G. Silva  J.S. Marques  and J.M. Lemos  “Selecting landmark points for sparse manifold

learning ” Advances in Neural Information Processing Systems 18  pp. 1241–1248  2006.

[14] M.E. Tipping  “Sparse Bayesian learning and the relevance vector machine ” Journal of

Machine Learning Research  vol. 1  pp. 211–244  2001.

[15] J.A. Tropp  “Algorithms for simultaneous sparse approximation. Part II: Convex relaxation ”

Signal Processing  vol. 86  pp. 589–602  April 2006.

[16] M.B. Wakin  M.F. Duarte  S. Sarvotham  D. Baron  and R.G. Baraniuk  “Recovery of jointly
sparse signals from a few random projections ” Advances in Neural Information Processing
Systems 18  pp. 1433–1440  2006.

[17] D.P. Wipf  Bayesian Methods for Finding Sparse Representations  PhD Thesis  University of

California  San Diego  2006.

[18] D.P. Wipf and S. Nagarajan  “Iterative reweighted ℓ1 and ℓ2 methods for ﬁnding sparse solu-
tions ” J. Selected Topics in Signal Processing (Special Issue on Compressive Sensing)  vol. 4 
no. 2  April 2010.

9

,Ximei Wang
Ying Jin
Mingsheng Long
Jianmin Wang
Michael Jordan