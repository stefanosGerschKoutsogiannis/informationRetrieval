2019,Time-series Generative Adversarial Networks,A good generative model for time-series data should preserve temporal dynamics  in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time  supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives  we encourage the network to adhere to the dynamics of the training data during sampling. Empirically  we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively  we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.,Time-series Generative Adversarial Networks

Jinsung Yoon∗

University of California  Los Angeles  USA

jsyoon0823@g.ucla.edu

Daniel Jarrett∗

University of Cambridge  UK

daniel.jarrett@maths.cam.ac.uk

Mihaela van der Schaar

University of Cambridge  UK

mv472@cam.ac.uk  mihaela@ee.ucla.edu

University of California  Los Angeles  USA

Alan Turing Institute  UK

Abstract

A good generative model for time-series data should preserve temporal dynamics 
in the sense that new sequences respect the original relationships between variables
across time. Existing methods that bring generative adversarial networks (GANs)
into the sequential setting do not adequately attend to the temporal correlations
unique to time-series data. At the same time  supervised models for sequence
prediction—which allow ﬁner control over network dynamics—are inherently
deterministic. We propose a novel framework for generating realistic time-series
data that combines the ﬂexibility of the unsupervised paradigm with the control
afforded by supervised training. Through a learned embedding space jointly
optimized with both supervised and adversarial objectives  we encourage the
network to adhere to the dynamics of the training data during sampling. Empirically 
we evaluate the ability of our method to generate realistic samples using a variety of
real and synthetic time-series datasets. Qualitatively and quantitatively  we ﬁnd that
the proposed framework consistently and signiﬁcantly outperforms state-of-the-art
benchmarks with respect to measures of similarity and predictive ability.

1

Introduction

What is a good generative model for time-series data? The temporal setting poses a unique challenge
to generative modeling. A model is not only tasked with capturing the distributions of features
within each time point  it should also capture the potentially complex dynamics of those variables
across time. Speciﬁcally  in modeling multivariate sequential data x1:T = (x1  ...  xT )  we wish to
accurately capture the conditional distribution p(xt|x1:t−1) of temporal transitions as well.
On the one hand  a great deal of work has focused on improving the temporal dynamics of au-
toregressive models for sequence prediction. These primarily tackle the problem of compounding
errors during multi-step sampling  introducing various training-time modiﬁcations to more accurately
reﬂect testing-time conditions [1  2  3]. Autoregressive models explicitly factor the distribution of

sequences into a product of conditionals(cid:81)t p(xt|x1:t−1). However  while useful in the context of

forecasting  this approach is fundamentally deterministic  and is not truly generative in the sense that
new sequences can be randomly sampled from them without external conditioning. On the other
hand  a separate line of work has focused on directly applying the generative adversarial network
(GAN) framework to sequential data  primarily by instantiating recurrent networks for the roles
of generator and discriminator [4  5  6]. While straightforward  the adversarial objective seeks to
model p(x1:T ) directly  without leveraging the autoregressive prior. Importantly  simply summing

∗ indicates equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the standard GAN loss over sequences of vectors may not be sufﬁcient to ensure that the dynamics of
the network efﬁciently captures stepwise dependencies present in the training data.
In this paper  we propose a novel mechanism to tie together both threads of research  giving rise to a
generative model explicitly trained to preserve temporal dynamics. We present Time-series Generative
Adversarial Networks (TimeGAN)  a natural framework for generating realistic time-series data in
various domains. First  in addition to the unsupervised adversarial loss on both real and synthetic
sequences  we introduce a stepwise supervised loss using the original data as supervision  thereby
explicitly encouraging the model to capture the stepwise conditional distributions in the data. This
takes advantage of the fact that there is more information in the training data than simply whether each
datum is real or synthetic; we can expressly learn from the transition dynamics from real sequences.
Second  we introduce an embedding network to provide a reversible mapping between features and
latent representations  thereby reducing the high-dimensionality of the adversarial learning space.
This capitalizes on the fact the temporal dynamics of even complex systems are often driven by fewer
and lower-dimensional factors of variation. Importantly  the supervised loss is minimized by jointly
training both the embedding and generator networks  such that the latent space not only serves to
promote parameter efﬁciency—it is speciﬁcally conditioned to facilitate the generator in learning
temporal relationships. Finally  we generalize our framework to handle the mixed-data setting  where
both static and time-series data can be generated at the same time.
Our approach is the ﬁrst to combine the ﬂexibility of the unsupervised GAN framework with the
control afforded by supervised training in autoregressive models. We demonstrate the advantages
in a series of experiments on multiple real-world and synthetic datasets. Qualitatively  we conduct
t-SNE [7] and PCA [8] analyses to visualize how well the generated distributions resemble the
original distributions. Quantitatively  we examine how well a post-hoc classiﬁer can distinguish
between real and generated sequences. Furthermore  by applying the "train on synthetic  test on real
(TSTR)" framework [5  9] to the sequence prediction task  we evaluate how well the generated data
preserves the predictive characteristics of the original. We ﬁnd that TimeGAN achieves consistent
and signiﬁcant improvements over state-of-the-art benchmarks in generating realistic time-series.

2 Related Work

TimeGAN is a generative time-series model  trained adversarially and jointly via a learned embedding
space with both supervised and unsupervised losses. As such  our approach straddles the intersection
of multiple strands of research  combining themes from autoregressive models for sequence prediction 
GAN-based methods for sequence generation  and time-series representation learning.
Autoregressive recurrent networks trained via the maximum likelihood principle [10] are prone to
potentially large prediction errors when performing multi-step sampling  due to the discrepancy
between closed-loop training (i.e. conditioned on ground truths) and open-loop inference (i.e.
conditioned on previous guesses). Based on curriculum learning [11]  Scheduled Sampling was
ﬁrst proposed as a remedy  whereby models are trained to generate output conditioned on a mix of
both previous guesses and ground-truth data [1]. Inspired by adversarial domain adaptation [12] 
Professor Forcing involved training an auxiliary discriminator to distinguish between free-running
and teacher-forced hidden states  thus encouraging the network’s training and sampling dynamics to
converge [2]. Actor-critic methods [13] have also been proposed  introducing a critic conditioned
on target outputs  trained to estimate next-token value functions that guide the actor’s free-running
predictions [3]. However  while the motivation for these methods is similar to ours in accounting for
stepwise transition dynamics  they are inherently deterministic  and do not accommodate explicitly
sampling from a learned distribution—central to our goal of synthetic data generation.
On the other hand  multiple studies have straightforwardly inherited the GAN framework within the
temporal setting. The ﬁrst (C-RNN-GAN) [4] directly applied the GAN architecture to sequential
data  using LSTM networks for generator and discriminator. Data is generated recurrently  taking as
inputs a noise vector and the data generated from the previous time step. Recurrent Conditional GAN
(RCGAN) [5] took a similar approach  introducing minor architectural differences such as dropping
the dependence on the previous output while conditioning on additional input [14]. A multitude of
applied studies have since utilized these frameworks to generate synthetic sequences in such diverse
domains as text [15]  ﬁnance [16]  biosignals [17]  sensor [18] and smart grid data [19]  as well as
renewable scenarios [20]. Recent work [6] has proposed conditioning on time stamp information to

2

handle irregularly sampling. However  unlike our proposed technique  these approaches rely only
on the binary adversarial feedback for learning  which by itself may not be sufﬁcient to guarantee
speciﬁcally that the network efﬁciently captures the temporal dynamics in the training data.
Finally  representation learning in the time-series setting primarily deals with the beneﬁts of learning
compact encodings for the beneﬁt of downstream tasks such as prediction [21]  forecasting [22]  and
classiﬁcation [23]. Other works have studied the utility of learning latent representations for purposes
of pre-training [24]  disentanglement [25]  and interpretability [26]. Meanwhile in the static setting 
several works have explored the beneﬁt of combining autoencoders with adversarial training  with
objectives such as learning similarity measures [27]  enabling efﬁcient inference [28]  as well as
improving generative capability [29]—an approach that has subsequently been applied to generating
discrete structures by encoding and generating entire sequences for discrimination [30]. By contrast 
our proposed method generalizes to arbitrary time-series data  incorporates stochasticity at each
time step  as well as employing an embedding network to identify a lower-dimensional space for the
generative model to learn the stepwise distributions and latent dynamics of the data.
Figure 1(a) provides a high-level block diagram of TimeGAN  and Figure 2 gives an illustrative
implementation  with C-RNN-GAN and RCGAN similarly detailed. For purposes of expository and
experimental comparison with existing methods  we employ a standard RNN parameterization. A
table of related works with additional detail can be found in the Supplementary Materials.

3 Problem Formulation

Consider the general data setting where each instance consists of two elements: static features (that
do not change over time  e.g. gender)  and temporal features (that occur over time  e.g. vital signs).
Let S be a vector space of static features  X of temporal features  and let S ∈ S  X ∈ X be random
vectors that can be instantiated with speciﬁc values denoted s and x. We consider tuples of the
form (S  X1:T ) with some joint distribution p. The length T of each sequence is also a random
variable  the distribution of which—for notational convenience—we absorb into p. In the training
data  let individual samples be indexed by n ∈ {1  ...  N}  so we can denote the training dataset
D = {(sn  xn 1:Tn )}N
Our goal is to use training data D to learn a density ˆp(S  X1:T ) that best approximates p(S  X1:T ).
This is a high-level objective  and—depending on the lengths  dimensionality  and distribution of
the data—may be difﬁcult to optimize in the standard GAN framework. Therefore we additionally
make use of the autoregressive decomposition of the joint p(S  X1:T ) = p(S)(cid:81)t p(Xt|S  X1:t−1)
to focus speciﬁcally on the conditionals  yielding the complementary—and simpler—objective of
learning a density ˆp(Xt|S  X1:t−1) that best approximates p(Xt|S  X1:t−1) at any time t.
Two Objectives. Importantly  this breaks down the sequence-level objective (matching the joint
distribution) into a series of stepwise objectives (matching the conditionals). The ﬁrst is global 

n=1. Going forward  subscripts n are omitted unless explicitly required.

ˆp

min

ˆp

min

D(cid:16)p(S  X1:T )(cid:13)(cid:13)ˆp(S  X1:T )(cid:17)
D(cid:16)p(Xt|S  X1:t−1)(cid:13)(cid:13)ˆp(Xt|S  X1:t−1)(cid:17)

(1)

(2)

where D is some appropriate measure of distance between distributions. The second is local 

for any t. Under an ideal discriminator in the GAN framework  the former takes the form of the
Jensen-Shannon divergence. Using the original data for supervision via maximum-likelihood (ML)
training  the latter takes the form of the Kullback-Leibler divergence. Note that minimizing the former
relies on the presence of a perfect adversary (which we may not have access to)  while minimizing
the latter only depends on the presence of ground-truth sequences (which we do have access to). Our
target  then  will be a combination of the GAN objective (proportional to Expression 1) and the ML
objective (proportional to Expression 2). As we shall see  this naturally yields a training procedure
that involves the simple addition of a supervised loss to guide adversarial learning.

4 Proposed Model: Time-series GAN (TimeGAN)

TimeGAN consists of four network components: an embedding function  recovery function  sequence
generator  and sequence discriminator. The key insight is that the autoencoding components (ﬁrst two)

3

are trained jointly with the adversarial components (latter two)  such that TimeGAN simultaneously
learns to encode features  generate representations  and iterate across time. The embedding network
provides the latent space  the adversarial network operates within this space  and the latent dynamics
of both real and synthetic data are synchronized through a supervised loss. We describe each in turn.

4.1 Embedding and Recovery Functions

hS = eS (s) 

ht = eX (hS   ht−1  xt)

The embedding and recovery functions provide mappings between feature and latent space  allowing
the adversarial network to learn the underlying temporal dynamics of the data via lower-dimensional
representations. Let HS  HX denote the latent vector spaces corresponding to feature spaces S X .
Then the embedding function e : S ×(cid:81)t X → HS ×(cid:81)t HX takes static and temporal features to
their latent codes hS   h1:T = e(s  x1:T ). In this paper  we implement e via a recurrent network 
(3)
where eS : S → HS is an embedding network for static features  and eX : HS × HX × X → HX a
recurrent embedding network for temporal features. In the opposite direction  the recovery function
r : HS ×(cid:81)t HX → S ×(cid:81)t X takes static and temporal codes back to their feature representations
(4)
where rS : HS → S and rX : HX → X are recovery networks for static and temporal embeddings.
Note that the embedding and recovery functions can be parameterized by any architecture of choice 
with the only stipulation being that they be autoregressive and obey causal ordering (i.e. output(s) at
each step can only depend on preceding information). For example  it is just as possible to implement
the former with temporal convolutions [31]  or the latter via an attention-based decoder [32]. Here
we choose implementations 3 and 4 as a minimal example to isolate the source of gains.

˜s  ˜x1:T = r(hS   h1:T ). Here we implement r through a feedforward network at each step 

˜s = rS (hs) 

˜xt = rX (ht)

4.2 Sequence Generator and Discriminator

Instead of producing synthetic output directly in feature space  the generator ﬁrst outputs into the
embedding space. Let ZS  ZX denote vector spaces over which known distributions are deﬁned  and
from which random vectors are drawn as input for generating into HS  HX . Then the generating
function g : ZS ×(cid:81)t ZX → HS ×(cid:81)t HX takes a tuple of static and temporal random vectors to
synthetic latent codes ˆhS   ˆh1:T = g(zS   z1:T ). We implement g through a recurrent network 
(5)
where gS : ZS → HS is an generator network for static features  and gX : HS ×HX ×ZX → HX is
a recurrent generator for temporal features. Random vector zS can be sampled from a distribution of
choice  and zt follows a stochastic process; here we use the Gaussian distribution and Wiener process

ˆht = gX (ˆhS   ˆht−1  zt)

ˆhS = gS (zS ) 

Figure 1: (a) Block diagram of component functions and objectives. (b) Training scheme; solid lines
indicate forward propagation of data  and dashed lines indicate backpropagation of gradients.

4

ClassificationsReconstructionsReal SequencesRandom VectorsLatent CodesRecoveryEmbeddingGenerateDiscriminateUnsupervisedLossLearn distributionˆp(S X1:T)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>directlySupervisedLossLearn conditionalsˆp(Xt|S X1:t1)<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ReconstructionLossProvide LatentEmbedding Space2S⇥QtX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>2S⇥QtX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>2[0 1]⇥...<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>2ZS⇥QtZt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>2HS⇥QtHt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>e<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>r<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>d<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>g<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜s ˜x1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>s x1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ˆhS ˆh1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>hS h1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>zS z1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜yS ˜y1:T<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LR@✓e<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LR@✓r<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LS@✓g<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LS@✓e<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LU@✓g<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>@LU@✓d<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>(a) Block Diagram(b) Training Schemerespectively. Finally  the discriminator also operates from the embedding space. The discrimination

function d : HS ×(cid:81)t HX → [0  1] ×(cid:81)t[0  1] receives the static and temporal codes  returning

classiﬁcations ˜yS   ˜y1:T = d(˜hS   ˜h1:T ) . The ˜h∗ notation denotes either real (h∗) or synthetic (ˆh∗)
embeddings; similarly  the ˜y∗ notation denotes classiﬁcations of either real (y∗) or synthetic (ˆy∗) data.
Here we implement d via a bidirectional recurrent network with a feedforward output layer 

˜yS = dS (˜hS )

(6)
where (cid:126)ut = (cid:126)cX (˜hS   ˜ht  (cid:126)ut−1) and (cid:126)ut = (cid:126)cX (˜hS   ˜ht  (cid:126)ut+1) respectively denote the sequences of
forward and backward hidden states  (cid:126)cX   (cid:126)cX are recurrent functions  and dS   dX are output layer
classiﬁcation functions. Similarly  there are no restrictions on architecture beyond the generator being
autoregressive; here we use a standard recurrent formulation for ease of exposition.

˜yt = dX ( (cid:126)ut  (cid:126)ut)

4.3

Jointly Learning to Encode  Generate  and Iterate

First  purely as a reversible mapping between feature and latent spaces  the embedding and recovery
functions should enable accurate reconstructions ˜s  ˜x1:T of the original data s  x1:T from their latent
representations hS   h1:T . Therefore our ﬁrst objective function is the reconstruction loss 

LR = Es x1:T ∼p(cid:2)(cid:107)s − ˜s(cid:107)2 +(cid:80)t (cid:107)xt − ˜xt(cid:107)2(cid:3)

(7)
In TimeGAN  the generator is exposed to two types of inputs during training. First  in pure open-
loop mode  the generator—which is autoregressive—receives synthetic embeddings ˆhS   ˆh1:t−1 (i.e.
its own previous outputs) in order to generate the next synthetic vector ˆht. Gradients are then
computed on the unsupervised loss. This is as one would expect—that is  to allow maximizing (for
the discriminator) or minimizing (for the generator) the likelihood of providing correct classiﬁcations
ˆyS   ˆy1:T for both the training data hS   h1:T as well as for synthetic output ˆhS   ˆh1:T from the generator 
(8)
Relying solely on the discriminator’s binary adversarial feedback may not be sufﬁcient incentive
for the generator to capture the stepwise conditional distributions in the data. To achieve this more
efﬁciently  we introduce an additional loss to further discipline learning. In an alternating fashion  we
also train in closed-loop mode  where the generator receives sequences of embeddings of actual data
h1:t−1 (i.e. computed by the embedding network) to generate the next latent vector. Gradients can
now be computed on a loss that captures the discrepancy between distributions p(Ht|HS   H1:t−1)
and ˆp(Ht|HS   H1:t−1). Applying maximum likelihood yields the familiar supervised loss 
(9)

LU = Es x1:T ∼p(cid:2) log yS +(cid:80)t log yt(cid:3) + Es x1:T ∼ ˆp(cid:2) log(1 − ˆyS ) +(cid:80)t log(1 − ˆyt)(cid:3)

LS = Es x1:T ∼p(cid:2)(cid:80)t (cid:107)ht − gX (hS   ht−1  zt)(cid:107)2(cid:3)

Figure 2: (a) TimeGAN instantiated with RNNs  (b) C-RNN-GAN  and (c) RCGAN. Solid lines de-
note function application  dashed lines denote recurrence  and orange lines indicate loss computation.

5

s<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜s<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜xt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>xt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ht<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>hS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>zS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ˆhS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ˆht<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>zt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>!ut<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit> ut<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜yt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜yS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>eS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>eX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>gS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>gX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>dS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>dX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>rX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>rS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit> cX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>!cX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>LS<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>LU<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>LR<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>EmbeddingDiscriminateRecoveryGenerate!ut<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit> ut<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜yt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>LU<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>Discriminateˆht<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>zt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>Generateˆxt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>˜yt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>LU<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>Discriminateˆht<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>zt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>Generateˆxt<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>s<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>ut<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>s<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>cX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>dX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>dX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit> cX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>!cX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>gX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>gX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>rX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>rX<latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit><latexit sha1_base64="(null)">(null)</latexit>(a) TimeGAN(b) C-RNN-GAN(c) RCGANwhere gX (hS   ht−1  zt) approximates Ezt∼N [ˆp(Ht|HS   H1:t−1  zt)] with one sample zt—as is
standard in stochastic gradient descent. In sum  at any step in a training sequence  we assess the
difference between the actual next-step latent vector (from the embedding function) and synthetic
next-step latent vector (from the generator—conditioned on the actual historical sequence of latents).
While LU pushes the generator to create realistic sequences (evaluated by an imperfect adversary) 
LS further ensures that it produces similar stepwise transitions (evaluated by ground-truth targets).
Optimization. Figure 1(b) illustrates the mechanics of our approach at training. Let θe  θr  θg  θd
respectively denote the parameters of the embedding  recovery  generator  and discriminator networks.
The ﬁrst two components are trained on both the reconstruction and supervised losses 

min
θe θr

(λLS + LR)

(10)

where λ ≥ 0 is a hyperparameter that balances the two losses. Importantly  LS is included such that
the embedding process not only serves to reduce the dimensions of the adversarial learning space—it
is actively conditioned to facilitate the generator in learning temporal relationships from the data.
Next  the generator and discriminator networks are trained adversarially as follows 

min
θg

(ηLS + max

θd LU)

(11)

where η ≥ 0 is another hyperparameter that balances the two losses. That is  in addition to the
unsupervised minimax game played over classiﬁcation accuracy  the generator additionally minimizes
the supervised loss. By combining the objectives in this manner  TimeGAN is simultaneously trained
to encode (feature vectors)  generate (latent representations)  and iterate (across time).
In practice  we ﬁnd that TimeGAN is not sensitive to λ and η; for all experiments in Section 5 
we set λ = 1 and η = 10. Note that while GANs in general are not known for their ease of
training  we do not discover any additional complications in TimeGAN. The embedding task serves
to regularize adversarial learning—which now occurs in a lower-dimensional latent space. Similarly 
the supervised loss has a constraining effect on the stepwise dynamics of the generator. For both
reasons  we do not expect TimeGAN to be more challenging to train  and standard techniques for
improving GAN training are still applicable. Algorithm pseudocode and illustrations with additional
detail can be found in the Supplementary Materials.

5 Experiments

Benchmarks and Evaluation. We compare TimeGAN with RCGAN [5] and C-RNN-GAN [4] 
the two most closely related methods. For purely autoregressive approaches  we compare against
RNNs trained with teacher-forcing (T-Forcing) [33  34] as well as professor-forcing (P-Forcing)
[2]. For additional comparison  we consider the performance of WaveNet [31] as well as its GAN
counterpart WaveGAN [35]. To assess the quality of generated data  we observe three desiderata:
(1) diversity—samples should be distributed to cover the real data; (2) ﬁdelity—samples should be
indistinguishable from the real data; and (3) usefulness—samples should be just as useful as the real
data when used for the same predictive purposes (i.e. train-on-synthetic  test-on-real).
(1) Visualization. We apply t-SNE [7] and PCA [8] analyses on both the original and synthetic
datasets (ﬂattening the temporal dimension). This visualizes how closely the distribution of generated
samples resembles that of the original in 2-dimensional space  giving a qualitative assessment of (1).
(2) Discriminative Score. For a quantitative measure of similarity  we train a post-hoc time-series
classiﬁcation model (by optimizing a 2-layer LSTM) to distinguish between sequences from the
original and generated datasets. First  each original sequence is labeled real  and each generated
sequence is labeled not real. Then  an off-the-shelf (RNN) classiﬁer is trained to distinguish between
the two classes as a standard supervised task. We then report the classiﬁcation error on the held-out
test set  which gives a quantitative assessment of (2).
(3) Predictive Score. In order to be useful  the sampled data should inherit the predictive characteris-
tics of the original. In particular  we expect TimeGAN to excel in capturing conditional distributions
over time. Therefore  using the synthetic dataset  we train a post-hoc sequence-prediction model (by
optimizing a 2-layer LSTM) to predict next-step temporal vectors over each input sequence. Then 
we evaluate the trained model on the original dataset. Performance is measured in terms of the mean

6

absolute error (MAE); for event-based data  the MAE is computed as |1− estimated probability that
the event occurred|. This gives a quantitative assessment of (3).
The Supplementary Materials contains additional information on benchmarks and hyperparame-
ters  as well as further details of visualizations and hyperparameters for the post-hoc evaluation
models. Implementation of TimeGAN can be found at https://bitbucket.org/mvdschaar/
mlforhealthlabpub/src/master/alg/timegan/.

5.1

Illustrative Example: Autoregressive Gaussian Models

Our primary novelties are twofold: a supervised loss to better capture temporal dynamics  and an
embedding network that provides a lower-dimensional adversarial learning space. To highlight
these advantages  we experiment on sequences from autoregressive multivariate Gaussian models as
follows: xt = φxt−1 + n  where n ∼ N (0  σ1 + (1 − σ)I). The coefﬁcient φ ∈ [0  1] allows us to
control the correlation across time steps  and σ ∈ [−1  1] controls the correlation across features.
As shown in Table 1  TimeGAN consistently generates higher-quality synthetic data than benchmarks 
in terms of both discriminative and predictive scores. This is true across the various settings for the
underlying data-generating model. Importantly  observe that the advantage of TimeGAN is greater
for higher settings of temporal correlation φ  lending credence to the motivation and beneﬁt of the
supervised loss mechanism. Likewise  observe that the advantage of TimeGAN is also greater for
higher settings of feature correlation σ  providing conﬁrmation for the beneﬁt of the embedding
network.

Settings

TimeGAN
RCGAN

Temporal Correlations (ﬁxing σ = 0.8)
φ = 0.2

φ = 0.5

φ = 0.8

Table 1: Results on Autoregressive Multivariate Gaussian Data (Bold indicates best performance).
Feature Correlations (ﬁxing φ = 0.8)
σ = 0.8
σ = 0.2
Discriminative Score (Lower the better)
.181±.006
.186±.012
.198±.011
.499±.001
.460±.003
.217±.010
.192±.012

.175±.006 .174±.012
.177±.012 .190±.011
C-RNN-GAN .391±.006 .227±.017
.500±.000 .500±.000
T-Forcing
.498±.002 .472±.008
P-Forcing
WaveNet
.337±.005 .235±.009
WaveGAN
.336±.011 .213±.013

.105±.005
.133±.019
.220±.016
.499±.001
.396±.018
.229±.013
.230±.023

.105±.005
.133±.019
.220±.016
.499±.001
.396±.018
.229±.013
.230±.023

σ = 0.5

.152±.011
.190±.012
.202±.010
.499±.001
.408±.016
.226±.011
.205±.015

Predictive Score (Lower the better)

TimeGAN
RCGAN

.640±.003 .412±.002
.652±.003 .435±.002
C-RNN-GAN .696±.002 .490±.005
.737±.022 .732±.012
T-Forcing
.665±.004 .571±.005
P-Forcing
WaveNet
.718±.002 .508±.003
.712±.003 .489±.001
WaveGAN

.251±.002
.263±.003
.299±.002
.503±.037
.289±.003
.321±.005
.290±.002

.282±.005 .261±0.002 .251±.002
.292±.003
.263±.003
.299±.002
.293±.005
.503±.037
.515±.034
.289±.003
.406±.005
.331±.004
.321±.005
.325±.003
.290±.002

.279±.002
.280±.006
.543±.023
.317±.001
.297±.003
.353±.001

5.2 Experiments on Different Types of Time Series Data

We test the performance of TimeGAN across time-series data with a variety of different characteristics 
including periodicity  discreteness  level of noise  regularity of time steps  and correlation across
time and features. The following datasets are selected on the basis of different combinations of these
properties (detailed statistics of each dataset can be found in the Supplementary Materials).
(1) Sines. We simulate multivariate sinusoidal sequences of different frequencies η and phases θ 
providing continuous-valued  periodic  multivariate data where each feature is independent of others.
For each dimension i ∈ {1  ...  5}  xi(t) = sin(2πηt + θ)  where η ∼ U[0  1] and θ ∼ U[−π  π].

7

(2) Stocks. By contrast  sequences of stock prices are continuous-valued but aperiodic; furthermore 
features are correlated with each other. We use the daily historical Google stocks data from 2004 to
2019  including as features the volume and high  low  opening  closing  and adjusted closing prices.
(3) Energy. Next  we consider a dataset characterized by noisy periodicity  higher dimensionality 
and correlated features. The UCI Appliances energy prediction dataset consists of multivariate 
continuous-valued measurements including numerous temporal features measured at close intervals.
(4) Events. Finally  we consider a dataset characterized by discrete values and irregular time stamps.
We use a large private lung cancer pathways dataset consisting of sequences of events and their times 
and model both the one-hot encoded sequence of event types as well as the event timings.

(a) TimeGAN (b) RCGAN (c) CRNNGAN (d) T-Forcing (e) P-Forcing
Figure 3: t-SNE visualization on Sines (1st row) and Stocks (2nd row). Each column provides the
visualization for each of the 7 benchmarks. Red denotes original data  and blue denotes synthetic.
Additional and larger t-SNE and PCA visualizations can be found in the Supplementary Materials.

(g) WaveGAN

(f) WaveNet

Visualizations with t-SNE and PCA. In Figure 3  we observe that synthetic datasets generated by
TimeGAN show markedly better overlap with the original data than other benchmarks using t-SNE
for visualization (PCA analysis can be found in the Supplementary Materials). In fact  we (in the ﬁrst
column) that the blue (generated) samples and red (original) samples are almost perfectly in sync.
Discriminative and Predictive Scores. As indicated in Table 2  TimeGAN consistently generates
higher-quality synthetic data in comparison to benchmarks on the basis of both discriminative (post-
hoc classiﬁcation error) and predictive (mean absolute error) scores across all datasets. For instance
for Stocks  TimeGAN-generated samples achieve 0.102 which is 48% lower than the next-best
benchmark (RCGAN  at 0.196)—a statistically signiﬁcant improvement. Remarkably  observe that
the predictive scores of TimeGAN are almost on par with those of the original datasets themselves.

Table 2: Results on Multiple Time-Series Datasets (Bold indicates best performance).
Metric

Sines

Discriminative

Score

(Lower the Better)

Predictive

Score

(Lower the Better)

Method
TimeGAN
RCGAN

C-RNN-GAN

T-Forcing
P-Forcing
WaveNet
WaveGAN
TimeGAN
RCGAN

T-Forcing
P-Forcing
WaveNet
WaveGAN
Original

C-RNN-GAN

Stocks
.102±.021
.196±.027
.399±.028
.226±.035
.257±.026
.232±.028
.217±.022
.038±.001
.040±.001
.038±.000
.038±.001
.043±.001
.042±.001
.041±.001
.036±.001

Energy
.236±.012
.336±.017
.499±.001
.483±.004
.412±.006
.397±.010
.363±.012
.273±.004
.292±.005
.483±.005
.315±.005
.303±.006
.311±.005
.307±.007
.250±.003

Events
.161±.018
.380±.021
.462±.011
.387±.012
.489±.001
.385±.025
.357±.017
.303±.006
.345±.010
.360±.010
.310±.003
.320±.008
.333±.004
.324±.006
.293±.000

.011±.008
.022±.008
.229±.040
.495±.001
.430±.027
.158±.011
.277±.013
.093±.019
.097±.001
.127±.004
.150±.022
.116±.004
.117±.008
.134±.013
.094±.001

8

5.3 Sources of Gain

TimeGAN is characterized by (1) the supervised loss  (2) embedding networks  and (3) the joint
training scheme. To analyze the importance of each contribution  we report the discriminative and
predictive scores with the following modiﬁcations to TimeGAN: (1) without the supervised loss  (2)
without the embedding networks  and (3) without jointly training the embedding and adversarial
networks on the supervised loss. (The ﬁrst corresponds to λ = η = 0  and the third to λ = 0).

Table 3: Source-of-Gain Analysis on Multiple Datasets (via Discriminative and Predictive scores).

Metric

Method
TimeGAN

Sines

Discriminative

Score

(Lower the Better)

w/o Supervised Loss
w/o Embedding Net.
w/o Joint Training

Events
.161±.018
.195±.013
.244±.011
.181±.011
.303±.006
.380±.023
.410±.013
.348±.021
We observe in Table 3 that all three elements make important contributions in improving the quality
of the generated time-series data. The supervised loss plays a particularly important role when the
data is characterized by high temporal correlations  such as in the Stocks dataset. In addition  we ﬁnd
that the embedding networks and joint training the with the adversarial networks (thereby aligning
the targets of the two) clearly and consistently improves generative performance across the board.

Stocks
.102±.021
.145±.023
.260±.021
.131±.019
.038±.001
.054±.001
.048±.001
.045±.001

Energy
.236±.012
.298±.010
.286±.006
.268±.012
.273±.004
.277±.005
.286±.002
.276±.004

.011±.008
.193±.013
.197±.025
.048±.011
.093±.019
.116±.010
.124±.002
.107±.008

TimeGAN

w/o Supervised Loss
w/o Embedding Net.
w/o Joint Training

Predictive

Score

(Lower the Better)

6 Conclusion

In this paper we introduce TimeGAN  a novel framework for time-series generation that combines the
versatility of the unsupervised GAN approach with the control over conditional temporal dynamics
afforded by supervised autoregressive models. Leveraging the contributions of the supervised loss and
jointly trained embedding network  TimeGAN demonstrates consistent and signiﬁcant improvements
over state-of-the-art benchmarks in generating realistic time-series data. In the future  further work
may investigate incorporating the differential privacy framework into the TimeGAN approach in
order to generate high-quality time-series data with differential privacy guarantees.

Acknowledgements

The authors would like to thank the reviewers for their helpful comments. This work was supported
by the National Science Foundation (NSF grants 1407712  1462245 and 1533983)  and the US Ofﬁce
of Naval Research (ONR).

References
[1] Samy Bengio  Oriol Vinyals  Navdeep Jaitly  and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Systems  pages
1171–1179  2015.

[2] Alex M Lamb  Anirudh Goyal Alias Parth Goyal  Ying Zhang  Saizheng Zhang  Aaron C Courville  and
Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural
Information Processing Systems  pages 4601–4609  2016.

[3] Dzmitry Bahdanau  Philemon Brakel  Kelvin Xu  Anirudh Goyal  Ryan Lowe  Joelle Pineau  Aaron
Courville  and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086  2016.

[4] Olof Mogren. C-rnn-gan: Continuous recurrent neural networks with adversarial training. arXiv preprint

arXiv:1611.09904  2016.

9

[5] Cristóbal Esteban  Stephanie L Hyland  and Gunnar Rätsch. Real-valued (medical) time series generation

with recurrent conditional gans. arXiv preprint arXiv:1706.02633  2017.

[6] Giorgia Ramponi  Pavlos Protopapas  Marco Brambilla  and Ryan Janssen. T-cgan: Conditional generative
adversarial network for data augmentation in noisy time series with irregular sampling. arXiv preprint
arXiv:1811.08295  2018.

[7] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[8] Fred B Bryant and Paul R Yarnold. Principal-components analysis and exploratory and conﬁrmatory factor

analysis. 1995.

[9] Jinsung Yoon  James Jordon  and Mihaela van der Schaar. PATE-GAN: Generating synthetic data with

differential privacy guarantees. In International Conference on Learning Representations  2019.

[10] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural

networks. Neural computation  1(2):270–280  1989.

[11] Yoshua Bengio  Jérôme Louradour  Ronan Collobert  and Jason Weston. Curriculum learning.

In
Proceedings of the 26th annual international conference on machine learning  pages 41–48. ACM  2009.

[12] Yaroslav Ganin  Evgeniya Ustinova  Hana Ajakan  Pascal Germain  Hugo Larochelle  François Laviolette 
Mario Marchand  and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of
Machine Learning Research  17(1):2096–2030  2016.

[13] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing

systems  pages 1008–1014  2000.

[14] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 

2014.

[15] Yizhe Zhang  Zhe Gan  and Lawrence Carin. Generating text via adversarial training. In NIPS workshop

on Adversarial Training  volume 21  2016.

[16] Luca Simonetto. Generating spiking time series with generative adversarial networks: an application on

banking transactions. 2018.

[17] Shota Haradal  Hideaki Hayashi  and Seiichi Uchida. Biosignal data augmentation based on generative
adversarial networks. In 2018 40th Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC)  pages 368–371. IEEE  2018.

[18] Moustafa Alzantot  Supriyo Chakraborty  and Mani Srivastava. Sensegen: A deep learning architecture for
synthetic sensor data generation. In 2017 IEEE International Conference on Pervasive Computing and
Communications Workshops (PerCom Workshops)  pages 188–193. IEEE  2017.

[19] Chi Zhang  Sanmukh R Kuppannagari  Rajgopal Kannan  and Viktor K Prasanna. Generative adversarial
network for synthetic time series data generation in smart grids. In 2018 IEEE International Conference
on Communications  Control  and Computing Technologies for Smart Grids (SmartGridComm)  pages 1–6.
IEEE  2018.

[20] Yize Chen  Yishen Wang  Daniel Kirschen  and Baosen Zhang. Model-free renewable scenario generation

using generative adversarial networks. IEEE Transactions on Power Systems  33(3):3265–3275  2018.

[21] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information

processing systems  pages 3079–3087  2015.

[22] Xinrui Lyu  Matthias Hueser  Stephanie L Hyland  George Zerveas  and Gunnar Raetsch. Improving clinical
predictions through unsupervised time series representation learning. arXiv preprint arXiv:1812.00490 
2018.

[23] Nitish Srivastava  Elman Mansimov  and Ruslan Salakhudinov. Unsupervised learning of video representa-

tions using lstms. In International conference on machine learning  pages 843–852  2015.

[24] Otto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders.

arXiv:1412.6581  2014.

arXiv preprint

[25] Yingzhen Li and Stephan Mandt. Disentangled sequential autoencoder. arXiv preprint arXiv:1803.02991 

2018.

10

[26] Wei-Ning Hsu  Yu Zhang  and James Glass. Unsupervised learning of disentangled and interpretable
In Advances in neural information processing systems  pages

representations from sequential data.
1878–1889  2017.

[27] Anders Boesen Lindbo Larsen  Søren Kaae Sønderby  Hugo Larochelle  and Ole Winther. Autoencoding

beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300  2015.

[28] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Olivier Mastropietro  Alex Lamb  Martin Arjovsky  and

Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704  2016.

[29] Alireza Makhzani  Jonathon Shlens  Navdeep Jaitly  Ian Goodfellow  and Brendan Frey. Adversarial

autoencoders. arXiv preprint arXiv:1511.05644  2015.

[30] Yoon Kim  Kelly Zhang  Alexander M Rush  Yann LeCun  et al. Adversarially regularized autoencoders.

arXiv preprint arXiv:1706.04223  2017.

[31] Aäron Van Den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex Graves  Nal
Kalchbrenner  Andrew W Senior  and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
SSW  125  2016.

[32] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural machine translation by jointly learning

to align and translate. arXiv preprint arXiv:1409.0473  2014.

[33] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850  2013.

[34] Ilya Sutskever  James Martens  and Geoffrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  pages 1017–1024 
2011.

[35] Chris Donahue  Julian McAuley  and Miller Puckette. Adversarial audio synthesis. arXiv preprint

arXiv:1802.04208  2018.

11

,Jinsung Yoon
Daniel Jarrett
Mihaela van der Schaar