2013,Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation,While several papers have investigated computationally and statistically efficient methods for learning Gaussian mixtures  precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper  we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation  then the sample complexity only depends on the number of relevant dimensions and mean separation  and can be achieved by a simple computationally efficient procedure. Our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.,Minimax Theory for High-dimensional Gaussian

Mixtures with Sparse Mean Separation

Martin Azizyan

Machine Learning Department
Carnegie Mellon University
mazizyan@cs.cmu.edu

Aarti Singh

Machine Learning Department
Carnegie Mellon University

aarti@cs.cmu.edu

Larry Wasserman

Department of Statistics

Carnegie Mellon University
larry@stat.cmu.edu

Abstract

While several papers have investigated computationally and statistically efﬁcient
methods for learning Gaussian mixtures  precise minimax bounds for their statisti-
cal performance as well as fundamental limits in high-dimensional settings are not
well-understood. In this paper  we provide precise information theoretic bounds
on the clustering accuracy and sample complexity of learning a mixture of two
isotropic Gaussians in high dimensions under small mean separation. If there is
a sparse subset of relevant dimensions that determine the mean separation  then
the sample complexity only depends on the number of relevant dimensions and
mean separation  and can be achieved by a simple computationally efﬁcient pro-
cedure. Our results provide the ﬁrst step of a theoretical basis for recent methods
that combine feature selection and clustering.

Introduction

1
Gaussian mixture models provide a simple framework for several machine learning problems in-
cluding clustering  density estimation and classiﬁcation. Mixtures are especially appealing in high
dimensional problems. Perhaps the most common use of Gaussian mixtures is for clustering. Of
course  the statistical (and computational) behavior of these methods can degrade in high dimen-
sions.
Inspired by the success of variable selection methods in regression  several authors have
considered variable selection for clustering. However  there appears to be no theoretical results
justifying the advantage of variable selection in high dimensional setting.
To see why some sort of variable selection might be useful  consider clustering n subjects using a
vector of d genes for each subject. Typically d is much larger than n which suggests that statistical
clustering methods will perform poorly. However  it may be the case that there are only a small
number of relevant genes in which case we might expect better behavior by focusing on this small
set of relevant genes.
The purpose of this paper is to provide precise bounds on clustering error with mixtures of Gaus-
sians. We consider both the general case where all features are relevant  and the special case where
only a subset of features are relevant. Mathematically  we model an irrelevant feature by requiring
the mean of that feature to be the same across clusters  so that the feature does not serve to differ-
entiate the groups. Throughout this paper  we use the probability of misclustering an observation 
relative to the optimal clustering if we had known the true distribution  as our loss function. This is
akin to using excess risk in classiﬁcation.
This paper makes the following contributions:

• We provide information theoretic bounds on the sample complexity of learning a mixture
of two isotropic Gaussians with equal weight in the small mean separation setting that pre-
cisely captures the dimension dependence  and matches known sample complexity require-
ments for some existing algorithms. This also debunks the myth that there is a gap between

1

statistical and computational complexity of learning mixture of two isotropic Gaussians for
small mean separation. Our bounds require non-standard arguments since our loss function
does not satisfy the triangle inequality.
• We consider the high-dimensional setting where only a subset of relevant dimensions deter-
mine the mean separation between mixture components and show that learning is substan-
tially easier as the sample complexity only depends on the sparse set of relevant dimensions.
This provides some theoretical basis for feature selection approaches to clustering.
• We show that a simple computationally feasible procedure nearly achieves the information

theoretic sample complexity even in high-dimensional sparse mean separation settings.

√

Related Work. There is a long and continuing history of research on mixtures of Gaussians. A
complete review is not feasible but we mention some highlights of the work most related to ours.
Perhaps the most popular method for estimating a mixture distribution is maximum likelihood. Un-
fortunately  maximizing the likelihood is NP-Hard. This has led to a stream of work on alternative
methods for estimating mixtures. These new algorithms use pairwise distances  spectral methods or
the method of moments.
Pairwise methods are developed in Dasgupta (1999)  Schulman and Dasgupta (2000) and Arora and
Kannan (2001). These methods require the mean separation to increase with dimension. The ﬁrst
one requires the separation to be
d while the latter two improve it to d1/4. To avoid this problem 
Vempala and Wang (2004) introduced the idea of using spectral methods for estimating mixtures
of spherical Gaussians which makes mean separation independent of dimension. The assumption
that the components are spherical was removed in Brubaker and Vempala (2008). Their method
only requires the components to be separated by a hyperplane and runs in polynomial time  but
requires n = Ω(d4 log d) samples. Other spectral methods include Kannan et al. (2005)  Achlioptas
and McSherry (2005) and Hsu and Kakade (2013). The latter uses clever spectral decompositions
together with the method of moments to derive an effective algorithm.
Kalai et al. (2012) use the method of moments to get estimates without requiring separation between
components of the mixture components. A similar approach is given in Belkin and Sinha (2010).
Chaudhuri et al. (2009) give a modiﬁed k-means algorithm for estimating a mixture of two Gaus-
sians. For the large mean separation setting µ > 1  Chaudhuri et al. (2009) show that n = ˜Ω(d/µ2)
samples are needed. They also provide an information theoretic bound on the necessary sample com-
plexity of any algorithm which matches the sample complexity of their method (up to log factors) in
d and µ. If the mean separation is small µ < 1  they show that n = ˜Ω(d/µ4) samples are sufﬁcient.
Our results for the small mean separation setting give a matching necessary condition. Assuming
the separation between the component means is not too sparse  Chaudhuri and Rao (2008) provide
an algorithm for learning the mixture that has polynomial computational and sample complexity.
Most of these papers are concerned with computational efﬁciency and do not give precise  statistical
minimax upper and lower bounds. None of them deal with the case we are interested in  namely  a
high dimensional mixture with sparse mean separation.
We should also point out that the results in different papers are not necessarily comparable since
different authors use different loss functions. In this paper we use the probability of misclassifying
a future observation  relative to how the correct distribution clusters the observation  as our loss
function. This should not be confused with the probability of attributing a new observation to the
wrong component of the mixture. The latter loss does not typically tend to zero as the sample
size increases. Our loss is similar to the excess risk used in classiﬁcation where we compare the
misclassiﬁcation rate of a classiﬁer to the misclassiﬁcation rate of the Bayes optimal classiﬁer.
Finally  we remind the reader that our motivation for studying sparsely separated mixtures is that
this provides a model for variable selection in clustering problems. There are some relevant recent
papers on this problem in the high-dimensional setting. Pan and Shen (2007) use penalized mixture
models to do variable selection and clustering simultaneously. Witten and Tibshirani (2010) develop
a penalized version of k-means clustering. Related methods include Raftery and Dean (2006); Sun
et al. (2012) and Guo et al. (2010). The applied bioinformatics literature also contains a huge number
of heuristic methods for this problem. None of these papers provide minimax bounds for the clus-
tering error or provide theoretical evidence of the beneﬁt of using variable selection in unsupervised
problems such as clustering.

2

2 Problem Setup

In this paper  we consider the simple setting of learning a mixture of two isotropic Gaussians with
equal mixing weights 1 given n data points X1  . . .   Xn ∈ Rd drawn i.i.d. from a d-dimensional
mixture density function

pθ(x) =

f (x; µ1  σ2I) +

f (x; µ2  σ2I) 

1
2

1
2

where f (·; µ  Σ) is the density of N (µ  Σ)  σ > 0 is a ﬁxed constant  and θ := (µ1  µ2) ∈ Θ. We
consider two classes Θ of parameters:

Θλ = {(µ1  µ2) : (cid:107)µ1 − µ2(cid:107) ≥ λ}

Θλ s = {(µ1  µ2) : (cid:107)µ1 − µ2(cid:107) ≥ λ  (cid:107)µ1 − µ2(cid:107)0 ≤ s} ⊆ Θλ.

The ﬁrst class deﬁnes mixtures where the components have a mean separation of at least λ > 0.
The second class deﬁnes mixtures with mean separation λ > 0 along a sparse set of s ∈ {1  . . .   d}
dimensions. Also  let Pθ denote the probability measure corresponding to pθ.
For a mixture with parameter θ  the Bayes optimal classiﬁcation  that is  assignment of a point
x ∈ Rd to the correct mixture component  is given by the function
f (x; µi  σ2I).

Fθ(x) = argmax
i∈{1 2}

Given any other candidate assignment function F : Rd → {1  2}  we deﬁne the loss incurred by F
as

Lθ(F ) = min

π

Pθ({x : Fθ(x) (cid:54)= π(F (x))})

where the minimum is over all permutations π : {1  2} → {1  2}. This is the probability of misclus-
tering relative to an oracle that uses the true distribution to do optimal clustering.

We denote by (cid:98)Fn any assignment function learned from the data X1  . . .   Xn  also referred to as

estimator. The goal of this paper is to quantify how the minimax expected loss (worst case expected
loss for the best estimator)

EθLθ((cid:98)Fn)

Rn ≡ inf(cid:98)Fn

sup
θ∈Θ

scales with number of samples n  the dimension of the feature space d  the number of relevant di-
mensions s  and the signal-to-noise ratio deﬁned as the ratio of mean separation to standard deviation
λ/σ. We will also demonstrate a speciﬁc estimator that achieves the minimax scaling.
For the purposes of this paper  we say that feature j is irrelevant if µ1(j) = µ2(j). Otherwise we
say that feature j is relevant.

3 Minimax Bounds

3.1 Small mean separation setting without sparsity

(cid:26)

We begin without assuming any sparsity  that is  all features are relevant. In this case  comparing
the projections of the data to the projection of the sample mean onto the ﬁrst principal component
sufﬁces to achieve both minimax optimal sample complexity and clustering loss.
Theorem 1 (Upper bound). Deﬁne

n v1((cid:98)Σn)
if xT v1((cid:98)Σn) ≥(cid:98)µT
i=1 Xi is the sample mean (cid:98)Σn = n−1(cid:80)n
where(cid:98)µn = n−1(cid:80)n
i=1(Xi−(cid:98)µn)(Xi−(cid:98)µn)T is the sample
covariance and v1((cid:98)Σn) denotes the eigenvector corresponding to the largest eigenvalue of (cid:98)Σn. If
(cid:19)(cid:114)
(cid:18) 4σ2

n ≥ max(68  4d)  then

(cid:98)Fn(x) =

otherwise.

d log(nd)

1
2

EθLθ((cid:98)F ) ≤ 600 max

λ2   1

.

n

sup
θ∈Θλ

1We believe our results should also hold in the unequal mixture weight setting without major modiﬁcations.

3

(cid:19)

.

− λ2
80σ2

(cid:18)

32

(cid:17)

+ 9 exp

(cid:16)− n
(cid:40)√
(cid:114)
σ ≤ 0.2. Then

log 2
3

σ2
λ2

d − 1
n

 

1
4

(cid:41)

.

We note that the estimator in Theorem 1 (and that in Theorem 3) does not use knowledge of σ2.
Theorem 2 (Lower bound). Assume that d ≥ 9 and λ

EθLθ((cid:98)Fn) ≥ 1

min

500

inf(cid:98)Fn

sup
θ∈Θλ

We believe that some of the constants (including lower bound on d and exact upper bound on λ/σ)
can be tightened  but the results demonstrate matching scaling behavior of clustering error with d  n
and λ/σ. Thus  we see (ignoring constants and log terms) that

(cid:114)

Rn ≈ σ2
λ2

d
n

 

or equivalently n ≈ d

λ4/σ4 for a constant target value of Rn.

The result is quite intuitive: the dependence on dimension d is as expected. Also we see that the rate
depends in a precise way on the signal-to-noise ratio λ/σ. In particular  the results imply that we
need d ≤ n.
In modern high-dimensional datasets  we often have d > n i.e. large number of features and not
enough samples. However  inference is usually tractable since not all features are relevant to the
learning task at hand. This sparsity of relevant feature set has been successfully exploited in super-
vised learning problems such as regression and classiﬁcation. We show next that the same is true for
clustering under the Gaussian mixture model.

3.2 Sparse and small mean separation setting

features. We begin by constructing an estimator (cid:98)Sn of S as follows. Deﬁne

Now we consider the case where there are s < d relevant features. Let S denote the set of relevant

Furthermore  if λ

√
σ ≥ 2 max(80  14

5d)  then

EθLθ((cid:98)F ) ≤ 17 exp

sup
θ∈Θλ

i∈{1 ... d}(cid:98)Σn(i  i)  where

min

(cid:98)τn =

1 + α
1 − α

(cid:114)

where

α =

6 log(nd)

2 log(nd)

+

.

n

n

Now let

Theorem 3 (Upper bound). Deﬁne

Now we use the same method as before  but using only the features in (cid:98)Sn identiﬁed as relevant.

(cid:98)Sn = {i ∈ {1  . . .   d} :(cid:98)Σn(i  i) >(cid:98)τn}.
(cid:26) 1
v1((cid:98)Σ(cid:98)Sn
(cid:98)Fn(x) =
and (cid:98)Σ(cid:98)Sn

) ≥(cid:98)µT(cid:98)Sn
are the coordinates of x restricted to (cid:98)Sn  and(cid:98)µ(cid:98)Sn
EθLθ((cid:98)F ) ≤ 603 max

v1((cid:98)Σ(cid:98)Sn

(cid:18) 16σ2

where x(cid:98)Sn
covariance of the data restricted to (cid:98)Sn. If n ≥ max(68  4s)  d ≥ 2 and α ≤ 1

4   then

(cid:18) log(nd)

(cid:19)(cid:114)

if xT(cid:98)Sn

otherwise

(cid:19) 1

s log(ns)

+ 220

σ

s

4

.

√

2

)

n

λ

n

λ2   1

sup
θ∈Θλ s

are the sample mean and

Next we ﬁnd the lower bound.
Theorem 4 (Lower bound). Assume that λ

EθLθ((cid:98)Fn) ≥ 1

min

600

(cid:19)
σ ≤ 0.2  d ≥ 17  and that 5 ≤ s ≤ d+3

(cid:18) d − 1

(cid:40)(cid:114) 8

(cid:115)

(cid:41)

4 . Then

σ2
λ2

s − 1
n

log

s − 1

 

1
2

.

inf(cid:98)Fn

sup
θ∈Θλ s

45

4

We remark again that the constants in our bounds can be tightened  but the results suggest that

(cid:18) s2 log d
(cid:19)1/4 (cid:31) Rn (cid:31) σ2
(cid:19)
(cid:18) s2 log d

λ2

n

σ
λ

(cid:114)

s log d

n

 

or n = Ω

λ4/σ4

for a constant target value of Rn.

In this case  we have a gap between the upper and lower bounds for the clustering loss. Also  the
sample complexity can possibly be improved to scale as s (instead of s2) using a different method.
However  notice that the dimension only enters logarithmically. If the number of relevant dimensions
is small then we can expect good rates. This provides some justiﬁcation for feature selection. We
conjecture that the lower bound is tight and that the gap could be closed by using a sparse principal
component method as in Vu and Lei (2012) to ﬁnd the relevant features. However  that method is
combinatorial and so far there is no known computationally efﬁcient method for implementing it
with similar guarantees.
We note that the upper bound is achieved by a two-stage method that ﬁrst ﬁnds the relevant dimen-
sions and then estimates the clusters. This is in contrast to the methods described in the introduction
which do clustering and variable selection simultaneously. This raises an interesting question: is it
always possible to achieve the minimax rate with a two-stage procedure or are there cases where a
simultaneous method outperforms a two-stage procedure? Indeed  it is possible that in the case of
general covariance matrices (non-spherical) two-stage methods might fail. We hope to address this
question in future work.

4 Proofs of the Lower Bounds

The lower bounds for estimation problems rely on a standard reduction from expected error to hy-
pothesis testing that assumes the loss function is a semi-distance  which the clustering loss isn’t.
However  a local triangle inequality-type bound can be shown (Proposition 2). This weaker condi-
tion can then be used to lower-bound the expected loss  as stated in Proposition 1 (which follows
easily from Fano’s inequality).
The proof techniques of the sparse and non-sparse lower bounds are almost identical. The main dif-
ference is that in the non-sparse case  we use the Varshamov–Gilbert bound (Lemma 1) to construct
a set of sufﬁciently dissimilar hypotheses  whereas in the sparse case we use an analogous result for
sparse hypercubes (Lemma 2). See the supplementary material for complete proofs of all results.
In this section and the next  φ and Φ denote the univariate standard normal PDF and CDF.
Lemma 1 (Varshamov–Gilbert bound). Let Ω = {0  1}m for m ≥ 8. There exists a subset
{ω0  ...  ωM} ⊆ Ω such that ω0 = (0  ...  0)  ρ(ωi  ωj) ≥ m
8 for all 0 ≤ i < j ≤ M  and
M ≥ 2m/8  where ρ denotes the Hamming distance between two vectors (Tsybakov (2009)).
Lemma 2. Let Ω = {ω ∈ {0  1}m : (cid:107)ω(cid:107)0 = s} for integers m > s ≥ 1 such that s ≤ m/4. There

(cid:1)s/5 − 1
exist ω0  ...  ωM ∈ Ω such that ρ(ωi  ωj) > s/2 for all 0 ≤ i < j ≤ M  and M ≥ (cid:0) m
(Massart (2007)  Lemma 4.10).
  and if Lθi((cid:98)F ) < γ implies Lθj ((cid:98)F ) ≥ γ for all 0 ≤ i (cid:54)= j ≤ M and
Proposition 1. Let θ0  ...  θM ∈ Θλ (or Θλ s)  M ≥ 2  0 < α < 1/8  and γ > 0. If for all 1 ≤ i ≤
clusterings (cid:98)F   then inf(cid:98)Fn
maxi∈[0..M ] Eθi Lθi ((cid:98)Fn) ≥ 0.07γ.
M  KL(Pθi  Pθ0) ≤ α log M
Proposition 2. For any θ  θ(cid:48) ∈ Θλ  and any clustering (cid:98)F   let τ = Lθ((cid:98)F ) +(cid:112)KL(Pθ  Pθ(cid:48))/2. If
Lθ(Fθ(cid:48)) + τ ≤ 1/2  then Lθ(Fθ(cid:48)) − τ ≤ Lθ(cid:48)((cid:98)F ) ≤ Lθ(Fθ(cid:48)) + τ.
(cid:16)(cid:107)µ(cid:107)
(cid:107)µ(cid:107)
2σ . Then KL(Pθ  Pθ(cid:48)) ≤ ξ4(1 − cos β).

We will also need the following two results. Let θ = (µ0−µ/2  µ0 +µ/2) and θ(cid:48) = (µ0−µ(cid:48)/2  µ0 +
µ(cid:48)/2) for µ0  µ  µ(cid:48) ∈ Rd such that (cid:107)µ(cid:107) = (cid:107)µ(cid:48)(cid:107)  and let cos β =
Proposition 3. Let g(x) = φ(x)(φ(x) − xΦ(−x)). Then 2g
Proposition 4. Let ξ =

sin β cos β ≤ Lθ(Fθ(cid:48)) ≤ tan β
π .

(cid:17)

|µT µ(cid:48)|
(cid:107)µ(cid:107)2

.

2σ

n

s

5

Lθω (Fθν ) ≤ 1
π

tan βω ν ≤ 1
π

Lθω (Fθν ) ≥ 2g(ξ) sin βω ν cos βω ν ≥ g(ξ)(cid:112)1 + cos βω ν

cos βω ν

d − 1
λ
√

  and

(cid:112)ρ(ω  ν)

2g(ξ)

λ

Proof of Theorem 2. Let ξ = λ

1)2. Let Ω = {0  1}d−1. For ω = (ω(1)  ...  ω(d − 1)) ∈ Ω  let µω = λ0ed +(cid:80)d−1

(cid:110)√
i=1 is the standard basis for Rd). Let θω =(cid:0)− µω

(where {ei}d
By Proposition 4  KL(Pθω   Pθν ) ≤ ξ4(1− cos βω ν) where cos βω ν = 1− 2ρ(ω ν)2
ρ is the Hamming distance  so KL(Pθω   Pθν ) ≤ ξ4 2(d−1)2

  ω  ν ∈ Ω  and
. By Proposition 3  since cos βω ν ≥ 1
2 

0 = λ2−(d−
i=1 (2ω(i) − 1)ei

2σ   and deﬁne  = min

. Deﬁne λ2

√
λ
d−1

1√
n  

(cid:111)

σ2
λ

λ2

λ2

(cid:112)1 + cos βω ν

2

4

log 2
3

2   µω

(cid:1) ∈ Θλ.
(cid:112)1 − cos βω ν ≤ 4
(cid:112)1 − cos βω ν ≥

√

π

8

√

where g(x) = φ(x)(φ(x) − xΦ(−x)). By Lemma 1  there exist ω0  ...  ωM ∈ Ω such that M ≥
2(d−1)/8 and ρ(ωi  ωj) ≥ d−1
for all 0 ≤ i < j ≤ M. For simplicity of notation  let θi = θωi for
all i ∈ [0..M ]. Then  for i (cid:54)= j ∈ [0..M ] 
KL(Pθi   Pθj ) ≤ ξ4 2(d − 1)2
λ2
√
(cid:114)
4 (g(ξ) − 2ξ2)
Lθi(Fθj ) + Lθi((cid:98)F ) +
(cid:18) 4

  Lθi (Fθj ) ≤ 4
. Then for any i (cid:54)= j ∈ [0..M ]  and any (cid:98)F such that Lθi((cid:98)F ) < γ 
π

and Lθi (Fθj ) ≥ 1
2
(cid:19) √

2
because  for ξ ≤ 0.1  by deﬁnition of  

(cid:18) 4
(cid:19) √

(g(ξ) − 2ξ2) + ξ2

d − 1
λ

d − 1
λ

d − 1
λ

Deﬁne γ = 1

KL(Pθi  Pθj )

≤ 1
2

d−1
λ

g(ξ)

√

√

1
4

+

<

π

.

So  by Proposition 2  Lθj ((cid:98)F ) ≥ γ. Also  KL(Pθi  Pθ0) ≤ (d− 1)ξ4 22

π

(g(ξ) − 2ξ2) + ξ2

+

1
4

d − 1
λ

≤ 2

d − 1
λ

.

≤ 1
2
λ2 ≤ log M
9n for all 1 ≤ i ≤ M 
(cid:114)

because  by deﬁnition of   ξ4 22

λ2 ≤ log 2

(cid:41)
72n . So by Proposition 1 and the fact that ξ ≤ 0.1 

(cid:40)√

EθiLθi((cid:98)Fn) ≥ 0.07γ ≥ 1

min

d − 1
n

 

1
4

log 2
3

σ2
λ2

inf(cid:98)Fn

max
i∈[0..M ]

500

EθLθ((cid:98)Fn) ≥ maxi∈[0..M ] EθiLθi ((cid:98)Fn) for any (cid:98)Fn. (cid:3)
(cid:27)
(cid:113) 1
n log(cid:0) d−1
(cid:1)  1
i=1 is the standard basis for Rd). Let θω = (cid:0)− µω
(cid:1)s/5 − 1 and ρ(ωi  ωj) ≥ s

(cid:26)(cid:113) 8
{ω ∈ {0  1}d−1 : (cid:107)ω(cid:107)0 = s}. For ω = (ω(1)  ...  ω(d − 1)) ∈ Ω  let µω = λ0ed +(cid:80)d−1
exist ω0  ...  ωM ∈ Ω such that M ≥(cid:0) d−1

and to complete the proof we use supθ∈Θλ
Proof of Theorem 4. For simplicity  we state this construction for Θλ s+1  assuming 4 ≤ s ≤ d−1
4 .
0 = λ2 − s2. Let Ω =
Let ξ = λ
(cid:1) ∈ Θλ s. By Lemma 2  there
i=1 ω(i)ei
2 for all 0 ≤ i < j ≤ M. The
4 (g(ξ) − √
(cid:3)

remainder of the proof is analogous to that of Theorem 2 with γ = 1

2σ   and deﬁne  = min

(where {ei}d

. Deﬁne λ2

√
λ .

2   µω

2ξ2)

λ√
s

σ2
λ

45

s

2

2

s

s

5 Proofs of the Upper Bounds

Propositions 5 and 6 below bound the error in estimating the mean and principal direction  and
can be obtained using standard concentration bounds and a variant of the Davis–Kahan theorem.
Proposition 7 relates these errors to the clustering loss. For the sparse case  Propositions 8 and 9
bound the added error induced by the support estimation procedure. See supplementary material for
proof details.
Proposition 5. Let θ = (µ0 − µ  µ0 + µ) for some µ0  µ ∈ Rd and X1  ...  Xn

i.i.d.∼ Pθ. For any
n with probability at least 1 − 3δ.

δ

δ > 0  we have (cid:107)µ0 −(cid:98)µn(cid:107) ≥ σ

(cid:113) 2 max(d 8 log 1

n

δ )

+ (cid:107)µ(cid:107)

(cid:113) 2 log 1

6

(cid:18) σ2

(cid:115)

(cid:19)√

(cid:18)

(cid:19)

d
δ

Proposition 6. Let θ = (µ0 − µ  µ0 + µ) for some µ0  µ ∈ Rd and X1  ...  Xn

d > 1 and n ≥ 4d. Deﬁne cos β = |v1(σ2I + µµT )T v1((cid:98)Σn)|. For any 0 < δ < d−1√
160   then with probability at least 1 − 12δ − 2 exp(cid:0)− n
(cid:1) 

(cid:17)(cid:113) max(d 8 log 1

i.i.d.∼ Pθ with
e   if

(cid:16) σ2

(cid:107)µ(cid:107)2   σ(cid:107)µ(cid:107)

≤ 1

max

δ )

20

n

sin β ≤ 14 max

(cid:107)µ(cid:107)2  

σ
(cid:107)µ(cid:107)

d

10
n

d
δ

10
n

cos β

2 sin β

(cid:19)

Φ

σ

(cid:18)

log

max

1 

log

.

  then

5

+ 1

.

1
σ

φ

2

−∞

max

0 

− 1
2

+ 2 sin β

21 + 2

− 21

(cid:107)µ(cid:107)
σ

(cid:107)µ(cid:107)
2σ

(cid:107)µ(cid:107)
σ

Proof. Let r =

if xT v ≥ xT
some 1 ≥ 0 and 0 ≤ 2 ≤ 1

Proposition 7. Let θ = (µ0 − µ  µ0 + µ)  and for some x0  v ∈ Rd with (cid:107)v(cid:107) = 1  let (cid:98)F (x) = 1
0 v  and 2 otherwise. Deﬁne cos β = |vT µ|/(cid:107)µ(cid:107). If |(x0 − µ0)T v| ≤ σ1 + (cid:107)µ(cid:107)2 for
(cid:40)
(cid:19)2(cid:41)(cid:20)
(cid:18)
(cid:19)(cid:21)
4   and if sin β ≤ 1√
Lθ((cid:98)F ) ≤ exp
(cid:12)(cid:12)(cid:12). Since the clustering loss is invariant to rotation and translation 
(cid:12)(cid:12)(cid:12) (x0−µ0)T v
(cid:18)(cid:107)µ(cid:107) − |x| tan β − r
(cid:19)(cid:21)
(cid:17)(cid:20)
(cid:90) ∞
(cid:16) x
Lθ((cid:98)F ) ≤ 1
(cid:19)(cid:21)
(cid:20)
(cid:18)(cid:107)µ(cid:107)
(cid:90) ∞
(cid:17) − Φ
(cid:16)(cid:107)µ(cid:107)
−∞
(cid:12)(cid:12)(cid:12) 
(cid:16)
(cid:16)
(cid:17)
(cid:17)(cid:17)
Since tan β ≤ 1
4  we have r ≤ 2σ1 + 2(cid:107)µ(cid:107)2  and Φ
2 and 2 ≤ 1
(cid:18)(cid:107)µ(cid:107) − r
(cid:18)(cid:107)µ(cid:107) − r
(cid:19)
(cid:90) A
(cid:90) ∞
(cid:107)µ(cid:107)
2σ − 21
φ
2
− Φ
(cid:90) A cos β+(u+A sin β) tan β
(cid:18)
(cid:18)(cid:18)

(cid:18)(cid:107)µ(cid:107) + |x| tan β + r
(cid:19)
(cid:18)(cid:107)µ(cid:107) − r

φ(u)φ(v)dudv ≤ 2φ (A) tan β (A sin β + 1)

≤ 2φ
where we used u = x cos β − y sin β and v = x sin β + y cos β in the second step. The bound now
follows easily.

(cid:12)(cid:12)(cid:12)(cid:107)µ(cid:107)−r
(cid:19)(cid:21)

(cid:90) ∞
(cid:18)

(cid:16)(cid:107)µ(cid:107)−r

(cid:16)
(cid:90) ∞

(cid:17) ≤

− |x| tan β

− |x| tan β

. Deﬁning A =

φ(x)φ(y)dydx

(cid:19)(cid:19)

dx ≤ 2

(cid:107)µ(cid:107)
2σ

(cid:107)µ(cid:107)
σ

A−x tan β

− 21

sin β + 1

(cid:107)µ(cid:107)
σ

(cid:20)

−A sin β

φ(x)

Φ

max

0 

max

0 

φ(x)

Φ

1 + 2

− Φ

tan β

2

0

(cid:19)

− Φ

(cid:19)

σ

σ

σ

σ

−∞

= 2

+ 21

A cos β

dx.

≤

dx

σ

σ

σ

σ

σ

Proof of Theorem 1. Using Propositions 5 and 6 with δ = 1√
(C + x) exp(− max(0  x − 4)2/8) ≤ (C + 6) exp(− max(0  x − 4)2/10) for all C  x > 0 

n  Proposition 7  and the fact that

EθLθ((cid:98)F ) ≤ 600 max

(cid:19)(cid:114)

(cid:18) 4σ2
5d) can be shown similarly  using δ = exp(cid:0)− n

λ2   1

d log(nd)

n

(it is easy to verify that the bounds are decreasing with (cid:107)µ(cid:107)  so we use (cid:107)µ(cid:107) = λ
2 to bound the
supremum). In the d = 1 case Proposition 6 need not be applied  since the principal directions agree
trivially. The bound for λ
Proposition 8. Let θ = (µ0 − µ  µ0 + µ) for some µ0  µ ∈ Rd and X1  ...  Xn
0 < δ < 1√

σ ≥ 2 max(80  14
(cid:113) 6 log 1
(cid:115)
2   with probability at least 1 − 6dδ  for all i ∈ [d] 

i.i.d.∼ Pθ. For any

(cid:1). (cid:3)

(cid:115)

√

32

δ

e such that

n ≤ 1
|(cid:98)Σn(i  i) − (σ2 + µ(i)2)| ≤ σ2

6 log 1
δ

n

+ 2σ|µ(i)|

2 log 1
δ

n

Proposition 9. Let θ = (µ0 − µ  µ0 + µ) for some µ0  µ ∈ Rd and X1  ...  Xn

S(θ) = {i ∈ [d] : µ(i) (cid:54)= 0} and (cid:101)S(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ

4 . Then (cid:101)S(θ) ⊆ (cid:98)Sn ⊆ S(θ) with probability at least 1 − 6

α}.

n .

Assume that n ≥ 1  d ≥ 2  and α ≤ 1

δ

+ (σ + |µ(i)|)2 2 log 1
.
i.i.d.∼ Pθ. Deﬁne
√

n

7

(cid:114)

(cid:114)

n

n

n

√

(cid:18)

(cid:19)

2 log(nd)

6 log(nd)

1 − 2 log(nd)

+ (σ + |µ(i)|)2 2 log(nd)

Proof. By Proposition 8  with probability at least 1 − 6
n 
+ 2σ|µ(i)|

|(cid:98)Σn(i  i) − (σ2 + µ(i)2)| ≤ σ2
for all i ∈ [d]. Assume the above event holds. If S(θ) = [d]  then of course (cid:98)Sn ⊆ S(θ). Otherwise 
for i /∈ S(θ)  we have (1 − α)σ2 ≤ (cid:98)Σn(i  i) ≤ (1 + α)σ2  so it is clear that (cid:98)Sn ⊆ S(θ). The
remainder of the proof is trivial if (cid:101)S(θ) = ∅ or S(θ) = ∅. Assume otherwise. For any i ∈ S(θ) 
1−α σ2 ≤(cid:98)Σn(i  i) and i ∈ (cid:98)Sn (we ignore strict

α for all i ∈ (cid:101)S(θ)  so (1+α)2

(cid:98)Σn(i  i) ≥ (1 − α)σ2 +

By deﬁnition  |µ(i)| ≥ 4σ

equality above as a measure 0 event)  i.e. (cid:101)S(θ) ⊆ (cid:98)Sn  which concludes the proof.
Proof of Theorem 3. Deﬁne S(θ) = {i ∈ [d] : µ(i) (cid:54)= 0} and (cid:101)S(θ) = {i ∈ [d] : |µ(i)| ≥ 4σ
Assume (cid:101)S(θ) ⊆ (cid:98)Sn ⊆ S(θ) (by Proposition 9  this holds with probability at least 1 − 6
(cid:101)S(θ) = ∅  then we simply have EθLθ((cid:98)Fn) ≤ 1
Assume (cid:101)S(θ) (cid:54)= ∅. Let cos(cid:98)β = |v1((cid:98)Σ(cid:98)Sn
|v1((cid:98)Σ(cid:98)Sn
and Σ(cid:98)Sn
the same as(cid:98)Σn and Σ in (cid:98)Sn  respectively  and 0 elsewhere. Then sin(cid:98)β ≤ sin(cid:101)β + sin β  and
√

)| where Σ = σ2I + µµT   and for simplicity we deﬁne (cid:98)Σ(cid:98)Sn
(cid:107)µ − µ(cid:98)S(θ)(cid:107)

(cid:107)µ − µ(cid:101)S(θ)(cid:107)

)T v1(Σ)|  and cos β =
to be

µ(i)2 − 2ασ|µ(i)|.

√
n).

α}.
If

n

2.

√

)T v1(Σ)|  cos(cid:101)β = |v1(Σ(cid:98)Sn
(cid:113)|S(θ)| − |(cid:101)S(θ)|
(cid:33)(cid:114)
sα(cid:1)2   1

≤ 4σ

s log(ns)

(cid:107)µ(cid:107)

+ 104

α

n
4 implies log(nd)

σ

≤ 8

sα
λ

.

√

σ

sα
λ

+

3
n

.

)T v1(Σ(cid:98)Sn
sin(cid:101)β =
EθLθ((cid:98)F ) ≤ 600 max

(cid:107)µ(cid:107)

≤

(cid:32)

(cid:107)µ(cid:107)

(cid:0) λ
σ2
√
2 − 4σ

Using the fact Lθ((cid:98)F ) ≤ 1

√
Using the same argument as the proof of Theorem 1  as long as the above bound is smaller than 1
2

 

5

2 always  and that α ≤ 1

n ≤ 1  the bound follows.

(cid:3)

6 Conclusion

We have provided minimax lower and upper bounds for estimating high dimensional mixtures. The
bounds show explicitly how the statistical difﬁculty of the problem depends on dimension d  sample
size n  separation λ and sparsity level s.
For clarity  we focused on the special case where there are two spherical components with equal
mixture weights. In future work  we plan to extend the results to general mixtures of k Gaussians.
One of our motivations for this work is the recent interest in variable selection methods to facilitate
clustering in high dimensional problems. Existing methods such as Pan and Shen (2007); Witten
and Tibshirani (2010); Raftery and Dean (2006); Sun et al. (2012) and Guo et al. (2010) provide
promising numerical evidence that variable selection does improve high dimensional clustering.
Our results provide some theoretical basis for this idea.
However  there is a gap between the results in this paper and the above methodology papers. In-
deed  as of now  there is no rigorous proof that the methods in those papers outperform a two stage
approach where the ﬁrst stage screens for relevant features and the second stage applies standard
clustering methods on the features found in the ﬁrst stage. We conjecture that there are conditions
under which simultaneous feature selection and clustering outperforms a two stage method. Settling
this question will require the aforementioned extension of our results to the general mixture case.

Acknowledgements

This research is supported in part by NSF grants IIS-1116458 and CAREER award IIS-1252412  as
well as NSF Grant DMS-0806009 and Air Force Grant FA95500910373.

8

References
Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions.

Learning Theory  pages 458–469. Springer  2005.

In

Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the

thirty-third annual ACM symposium on Theory of computing  pages 247–257. ACM  2001.

Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of
Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages 103–112. IEEE  2010.
S Charles Brubaker and Santosh S Vempala. Isotropic pca and afﬁne-invariant clustering. In Building

Bridges  pages 241–281. Springer  2008.

Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using correlations

and independence. In COLT  pages 9–20  2008.

Kamalika Chaudhuri  Sanjoy Dasgupta  and Andrea Vattani. Learning mixtures of gaussians using

the k-means algorithm. arXiv preprint arXiv:0912.0086  2009.

Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science  1999. 40th

Annual Symposium on  pages 634–644. IEEE  1999.

Jian Guo  Elizaveta Levina  George Michailidis  and Ji Zhu. Pairwise variable selection for high-

dimensional model-based clustering. Biometrics  66(3):793–804  2010.

Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and
In Proceedings of the 4th conference on Innovations in Theoretical

spectral decompositions.
Computer Science  pages 11–20. ACM  2013.

Adam Tauman Kalai  Ankur Moitra  and Gregory Valiant. Disentangling gaussians. Communica-

tions of the ACM  55(2):113–120  2012.

Ravindran Kannan  Hadi Salmasian  and Santosh Vempala. The spectral method for general mixture

models. In Learning Theory  pages 444–457. Springer  2005.

Pascal Massart. Concentration inequalities and model selection. 2007.
Wei Pan and Xiaotong Shen. Penalized model-based clustering with application to variable selec-

tion. The Journal of Machine Learning Research  8:1145–1164  2007.

Adrian E Raftery and Nema Dean. Variable selection for model-based clustering. Journal of the

American Statistical Association  101(473):168–178  2006.

Leonard J. Schulman and Sanjoy Dasgupta. A two-round variant of em for gaussian mixtures. In

Proc. 16th UAI (Conference on Uncertainty in Artiﬁcial Intelligence)  pages 152–159  2000.

Wei Sun  Junhui Wang  and Yixin Fang. Regularized k-means clustering of high-dimensional data

and its asymptotic consistency. Electronic Journal of Statistics  6:148–167  2012.

Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics.

Springer  2009.

Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of

Computer and System Sciences  68(4):841–860  2004.

Vincent Q Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. arXiv

preprint arXiv:1211.0373  2012.

Daniela M Witten and Robert Tibshirani. A framework for feature selection in clustering. Journal

of the American Statistical Association  105(490)  2010.

9

,Martin Azizyan
Aarti Singh
Larry Wasserman
Quoc Phong Nguyen
Bryan Kian Hsiang Low
Patrick Jaillet