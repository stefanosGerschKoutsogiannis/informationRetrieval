2010,Inductive Regularized Learning of Kernel Functions,In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix  and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore  our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data  we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions  improving the $k$-NN classification accuracy significantly in a variety of domains. Furthermore  our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.,Inductive Regularized Learning of Kernel Functions

Prateek Jain

Microsoft Research Bangalore

Bangalore  India

prajain@microsoft.com

Brian Kulis

UC Berkeley EECS and ICSI

Berkeley  CA  USA

kulis@eecs.berkeley.edu

Inderjit Dhillon

Austin  TX  USA

UT Austin Dept. of Computer Sciences

inderjit@cs.utexas.edu

Abstract

In this paper we consider the problem of semi-supervised kernel function learn-
ing. We ﬁrst propose a general regularized framework for learning a kernel matrix 
and then demonstrate an equivalence between our proposed kernel matrix learn-
ing framework and a general linear transformation learning problem. Our result
shows that the learned kernel matrices parameterize a linear transformation kernel
function and can be applied inductively to new data points. Furthermore  our re-
sult gives a constructive method for kernelizing most existing Mahalanobis metric
learning formulations. To make our results practical for large-scale data  we mod-
ify our framework to limit the number of parameters in the optimization process.
We also consider the problem of kernelized inductive dimensionality reduction in
the semi-supervised setting. To this end  we introduce a novel method for this
problem by considering a special case of our general kernel learning framework
where we select the trace norm function as the regularizer. We empirically demon-
strate that our framework learns useful kernel functions  improving the k-NN clas-
siﬁcation accuracy signiﬁcantly in a variety of domains. Furthermore  our kernel-
ized dimensionality reduction technique signiﬁcantly reduces the dimensionality
of the feature space while achieving competitive classiﬁcation accuracies.

1 Introduction

Learning kernel functions is an ongoing research topic in machine learning that focuses on learning
an appropriate kernel function for a given task. While several methods have been proposed  many
of the existing techniques can only be applied transductively [1–3]; i.e.  they cannot be applied
inductively to new data points. Of the methods that can be applied inductively  several are either too
computationally expensive for large-scale data (e.g. hyperkernels [4]) or are limited to small classes
of possible learned kernels (e.g. multiple kernel learning [5]).
In this paper  we propose and analyze a general kernel matrix learning problem using provided side-
information over the training data. Our learning problem regularizes the desired kernel matrix via
a convex regularizer chosen from a broad class  subject to convex constraints on the kernel. While
the learned kernel matrix should be able to capture the provided side-information well  it is not
clear how the information can be propagated to new data points. Our ﬁrst main result demonstrates
that our kernel matrix learning problem is equivalent to learning a linear transformation (LT) kernel
function (a kernel of the form ϕ(x)T W ϕ(y) for some matrix W ≽ 0) with a speciﬁc regularizer.
With the appropriate representation of W   this result implies that the learned LT kernel function can
be naturally applied to new data. Additionally  we demonstrate that a large class of Mahalanobis
metric learning methods can be seen as learning an LT kernel function and so our result provides a

1

constructive method for kernelizing these methods. Our analysis recovers some recent kernelization
results for metric learning  but also implies several new results.
As our proposed kernel learning formulation learns a kernel matrix over the training points  the
memory requirements scale quadratically in the number of training points  a common issue arising
in kernel methods. To alleviate such issues  we propose an additional constraint to the learning
formulation to reduce the number of parameters. We prove that the equivalence to LT kernel function
learning still holds with the addition of this constraint  and that the resulting formulation can be
scaled to very large data sets.
We then focus on a novel application of our framework to the problem of inductive semi-supervised
kernel dimensionality reduction. Our method is a special case of our kernel function learning
framework with trace-norm as the regularization function. As a result  we learn low-rank linear
transformations  which correspond to low-dimensional embeddings of high- or inﬁnite-dimensional
kernel embeddings; unlike previous kernel dimensionality methods  which are either unsupervised
(kernel-PCA) or cannot easily be applied inductively to new data (spectral kernels [6])  our method
intrinsically possesses both desirable properties. Furthermore  our method can handle a variety of
side-information  e.g.  class labels  click-through rates  etc. Finally  we validate the effectiveness of
our proposed framework. We quantitatively compare several regularizers  including the trace-norm
regularizer for dimensionality reduction  over standard data sets. We also apply the methods to an
object recognition task in computer vision and qualitatively show results of dimensionality reduction
on a handwritten digits data set.
Related Work: Most of the existing kernel learning methods can be classiﬁed into two broad cat-
egories. The ﬁrst category includes parametric approaches  where the learned kernel function is
restricted to be of a speciﬁc form and then the relevant parameters are learned according to the pro-
vided data. Prominent methods include multiple kernel learning [5]  hyperkernels [4]  inﬁnite kernel
learning [7]  and hyper-parameter cross-validation [8]. Most of these methods either lack modeling
ﬂexibility  require non-convex optimization  or are restricted to a supervised learning scenario. The
second category includes non-parametric methods  which explicitly model geometric structure in the
data. Examples include spectral kernel learning [6]  manifold-based kernel learning [9]  and kernel
target alignment [3]. However  most of these approaches are limited to the transductive setting and
cannot be used to naturally generalize to new points. In comparison  our method combines both of
the above approaches. We propose a general non-parametric kernel matrix learning framework  sim-
ilar to methods of the second category. However  we show that our learned kernel matrix corresponds
to a linear transformation kernel function parameterized by a PSD matrix. Hence  our method can
be applied to inductive settings also without sacriﬁcing signiﬁcant modeling power. Furthermore 
our methods can be applied to a variety of domains and with a variety of forms of side-information.
Existing work on learning linear transformations has largely focused on learning Mahalanobis dis-
tances; examples include [10–15]  among others. POLA [13] and ITML [12] provide specialized
kernelization techniques for their respective metric learning formulations. Kernelization of LMNN
was discussed in [16]  though it relied on a convex perturbation based formulation that can lead
to suboptimal solutions. Recently  [17] showed kernelization for a class of metric learning algo-
rithms including LMNN and NCA [15]; as we will see  our result is more general and we can prove
kernelization over a larger class of problems and can also reduce the number of parameters to be
learned. Independent of our work  [18] recently proved a representer type of theorem for spectral
regularization functions. However  the framework they consider is different than ours in that they
are interested in sensing the underlying high-dimensional matrix using given measurements.
Kernel dimensionality reduction methods can generally be divided into two categories: 1) semi-
supervised dimensionality reduction in the transductive setting  2) supervised dimensionality reduc-
tion in the inductive setting. Methods in the ﬁrst category include the incomplete Cholesky de-
composition [19]  colored maximum variance unfolding [20]  manifold preserving semi-supervised
dimensionality reduction [21]. Methods in the second category include the kernel dimensionality re-
duction method [22] and Gaussian Process latent variable models [23]. Kernel PCA [24] reduces the
dimensionality in the inductive unsupervised setting  while various manifold learning methods can
reduce the dimensionality but only in the unsupervised transductive setting. In contrast  our dimen-
sionality reduction method  which is an instantiation of our general kernel learning framework  can
perform kernel dimensionality reduction simultaneously in both the semi-supervised as well as the
inductive setting. Additionally  it can capture the manifold structure using an appropriate baseline
kernel function such as the one proposed by [25].

2

2 Learning Framework
Given an input kernel function κ : Rd × Rd → R  and some side-information over a set of points
X = {x1  x2  . . .   xn} the goal is to learn a new kernel function κW that is regularized against
κ but incorporates the provided side-information (the use of the subscript W will become clear
later). The initial kernel function κ is of the form κ(x  y) = ϕ(x)T ϕ(y) for some mapping ϕ.
Throughout the rest of this paper  we will denote ϕi as shorthand for ϕ(xi)  i.e.  data point xi after
applying the mapping ϕ. We will also assume that the data vectors in X have been mapped via ϕ 
resulting in (cid:8) = {ϕ1  ϕ2  . . .   ϕn}. Learning a kernel function from the provided side-information
is an ill-posed problem since inﬁnitely many such kernels can satisfy the provided supervision. A
common approach is to formulate a transductive learning problem to learn a new kernel matrix over
the training data. Denoting the input kernel matrix K as K = (cid:8)T (cid:8)  we aim to learn a new kernel
matrix KW that is regularized against K while satisfying the available side-information. In this
work  we study the following optimization problem:

−1/2KW K

−1/2)

s.t. gi(KW ) ≤ bi  1 ≤ i ≤ m 

f (K

min
KW ≽0

(1)
where f and gi are functions from Rn×n → R. We call f the regularizer and the gi the constraints.
Note that if f and constraints gi’s are all convex functions  then the above problem can be solved
optimally using standard convex optimization algorithms. Note that our results will also hold for
unconstrained variants of the above problem  as well as variants that incorporate slack variables.
In general  such learning formulations are limited in that the learned kernel cannot readily be applied
to new data points. However  we will show that the above proposed problem is equivalent to learning
linear transformation (LT) kernel functions. Formally  an LT kernel function κW is a kernel function
of the form κW (x  y) = ϕ(x)T W ϕ(y)  where W is a positive semi-deﬁnite (PSD) matrix; we can
think of the LT kernel as describing the linear transformation ϕi → W 1/2ϕi. A natural way to
learn an LT kernel function would be to learn the parameterization matrix W using the provided
side-information. To this end  we consider the following problem:

min
W≽0

f (W )

s.t. gi((cid:8)T W (cid:8)) ≤ bi  1 ≤ i ≤ m 

(2)

where  as before  the function f is the regularizer and the functions gi are the constraints that encode
the side information. The constraints gi are assumed to be a function of the matrix (cid:8)T W (cid:8) of learned
kernel values over the training data. We make two observations about this problem: ﬁrst  for data
mapped to high-dimensional spaces via kernel functions  this problem is seemingly impossible to
optimize since the size of W grows quadratically with the dimensionality. We will show that (2)
need not explicitly be solved for learning an LT kernel function. Second  most Mahalanobis metric
learning methods may be viewed as a special case of the above framework  and we will discuss some
of them throughout the paper.

2.1 Examples of Regularizers and Constraints

∥A − I∥2
−1KW − I∥2

To make the kernel learning optimization problem concrete  we discuss a few examples of possible
regularizers and constraints.
For the regularizer f (A) = 1
F   the resulting kernel learning objective can be equivalently
∥K
2
F . Thus  the goal is to keep the learned kernel close to the
expressed as minimizing 1
input kernel subject to the constraints in gi. Similarly  for f (A) = tr(A − I)  the resulting objective
2
−1KW −I). Another interesting regularizer is f (A) = tr(A)−
can be expressed as minimizing tr(K
log det(A). In this case  the resulting objective is to minimize the LogDet divergence Dℓd(KW   K)
subject to the constraints given by gi. For linear gi  this problem was studied in [12  26].
In terms of constraints  pairwise squared Euclidean distance constraint between a pair of points
(ϕi  ϕj) in feature space can be formulated as KW (i  i) + KW (j  j) − 2KW (i  j) ≥ b or
KW (i  i) + KW (j  j) − 2KW (i  j) ≤ b; this constraint is clearly linear in the entries of KW .
Similarity constraints can be represented as KW (i  j) ≤ b or KW (i  j) ≥ b and are also linear in
KW . Relative distance constraints over a triplet (ϕi  ϕj  ϕk) specify that ϕi should be closer to ϕj
than ϕk  and are often used in metric learning formulations and ranking problems; such constraints
can be easily formulated within our framework. Finally  non-parametric probability estimation con-
straints can be used to constrain the conditional probability of a class c given a data point ϕi 

±p(c|x) = ±

j∈c KW (i  j)

C
t=1

j∈t KW (i  j)

≥ b 

∑

∑

∑

3

where C is the number of classes. This constraint can be written as a linear constraint over KW
after appropriate manipulation.

3 Analysis

We are now ready to analyze the connection between problems (1) and (2). We will show that
the solutions to the two problems are equivalent  in the sense that by optimally solving one of the
problems  the solution to the other can be computed in closed form. More importantly  this result
will yield insight into the type of kernel that is learned by the kernel learning problem.
We begin by deﬁning the class of regularizers considered in our analysis. Note that each of the
example regularizers discussed earlier satisfy the following deﬁnition of spectral functions.
Deﬁnition 3.1. We say that f : Rn×n → R is a spectral function if f (A) =
i fs(λi)  where
λ1  ...  λn are the eigenvalues of A and fs : R → R is a real-valued function over the reals. Note
that if fs is a convex function over the reals  then f is also convex.

∑

3.1 Learning Linear Transformation Kernels
Now we present our main result  i.e.  for a spectral function f  problems (1) and (2) are equivalent.
Theorem 1. Let K ≻ 0 be an invertible matrix  f be a spectral function and denote the global
∗
∗ be an optimal solution to (2) and K
minima of the corresponding scalar function fs as α. Let W
W
be an optimal solution to (1). Then 

where S

∗

−1(K

= K

∗
W

− αK)K

W

= αI + (cid:8)S
∗
−1. Furthermore  K
W = (cid:8)T W

(cid:8)T  

∗

(cid:8).

∗

∗

∗
W to (1)  one can con-
The ﬁrst part of the theorem demonstrates that  given an optimal solution K
∗ to (2)  while the second part shows the reverse (this also
struct the corresponding solution W
demonstrates why W is used in the subscript of the learned kernel). The proof of this theorem
appears in the supplementary material. The main idea behind the proof is to ﬁrst show that the op-
timal solution to (2) is always of the form W = αI + (cid:8)S(cid:8)T   and then we obtain the closed form
expression for S using algebraic manipulations.
As a ﬁrst consequence of this result  we can achieve induction over the learned kernels. Given
that KW = (cid:8)T W (cid:8)  we can see that the learned kernel function is a linear transformation kernel;
that is  κW (ϕi  ϕj) = ϕT
i W ϕj. Given a pairs of new data points ϕn1 and ϕn2  we use the fact
that the learned kernel is a linear transformation kernel  along with the ﬁrst result of the theorem
(W = αI + (cid:8)S(cid:8)T ) to compute the learned kernel as:

n∑

κW (xn1   xn2) = ϕT
n1

W ϕn2 = ακ(xn1   xn2) +

Sijκ(xn1  xi)κ(xj  xn2 ).

i j=1

(3)

As mentioned in Section 2  many Mahalanobis metric learning methods can be viewed as a special
case of (2). Therefore  a corollary of Theorem 1 is that we can constructively apply these metric
learning methods in kernel space by solving their corresponding kernel learning problem  and then
compute the learned metrics via (3). Thus  W need not explicitly be constructed to learn the LT ker-
nel. Kernelization of Mahalanobis metric learning has previously been established for some special
cases; our results generalize and extend previous methods  as well as provide simpler techniques in
some cases. Below  we elaborate with some special cases.
Example 1 [Information Theoretic Metric Learning (ITML)]: [12] proposed the following Ma-
halanobis metric learning problem formulation:
dW (ϕi  ϕj) ≥ bij  (i  j) ∈ D 
min
W≽0
where S and D specify pairs of similar and dissimilar points  respectively  and dW (ϕi  ϕj) =
(ϕi − ϕj)T W (ϕi − ϕj) is the Mahalanobis distance between ϕi and ϕj. ITML is an instantiation
of our framework with regularizer f (A) = tr(A) − log det(A) and pairwise distance constraints
encoded as the gi functions. Furthermore  it is straightforward to show that f is a convex spectral
function with global optima α = 1  so the optimal W can be learned implicitly using (1). The
corresponding kernel learning optimization problem simpliﬁes to:

dW (ϕi  ϕj) ≤ bij  (i  j) ∈ S 

Tr(W ) − log det(W ) 

s.t.

gi(KW ) ≤ bi  1 ≤ i ≤ m 

(4)

min
KW

Dℓd(KW   K) s.t.

4

−1)−log det(KW K

−1)−n is the LogDet divergence [12]  and the
where Dℓd(KW   K) = tr(KW K
positive deﬁniteness of KW is satisﬁed automatically. This recovers the kernelized metric learning
problem analyzed in [12]  where kernelization for this special case was established and an iterative
projection algorithm for optimization was developed. Note that  in the analysis of [12]  the gi were
limited to similarity and dissimilarity constraints; our result is therefore more general than the exist-
ing kernelization result  even for this special case.
Example 2 [Pseudo Online Metric Learning (POLA)]: [13] proposed the following metric learn-
ing formulation:
where yij = 1 if ϕi and ϕj are similar  and yij = −1 if ϕi and ϕj are dissimilar. P is a set
of pairs of points with known distance constraints. POLA is an instantiation of (2) with f (A) =
∥A∥2
F and side-information available in the form of pair-wise distance constraints. Note that the
1
∥A∥2 was also employed in [2  27]  and these methods also fall under our
2
regularizer f (A) = 1
2
general formulation. In this case  f is once again a convex spectral function  and its global minima
is α = 0  so we can use (1) to solve for the learned kernel KW as

s.t. yij(b − dW (ϕi  ϕj)) ≥ 1  ∀(i  j) ∈ P 

∥W∥2
F  

min
W≽0

gi(KW ) ≤ bi  1 ≤ i ≤ m  KW ≽ 0.

(5)

∥KW K

−1∥2

F

s.t.

min
KW

The constraints gi for this problem can be easily constructed by re-writing each of POLA’s con-
straints as a function of (cid:8)T W (cid:8). Note that the above approach for kernelization is much simpler
than the method suggested in [13]  which involves a kernelized Gram-Schmidt procedure at each
step of the algorithm.
Other Examples: The above two examples show that our analysis recovers two well-known ker-
nelization results for Mahalanobis metric learning. However  there are several other metric learning
approaches that fall into our framework as well  including the large margin nearest neighbor met-
ric learning method (LMNN) [11] and maximally collapsing metric learning (MCML) [14]  both
of which can be seen as instantiations of our learning framework with a constant f  as well as rel-
evant component analysis (RCA) [28] and Xing et al.’s Mahalanobis metric learning method for
clustering [10]. Given lack of space  we cannot detail the kernelization of all these methods  but
they follow in the same manner as in the above two examples. In particular  each of these methods
may be run in kernel space  and our analysis yields new insights into these methods; for example 
kernelization of LMNN [11] using Theorem 1 avoids the convex perturbation analysis in [16] that
leads to suboptimal solutions in some cases.
3.2 Parameter Reduction
One of the drawbacks to Theorem 1 is that the size of the matrices KW and S are n × n  and thus
grow quadratically with the number of data points. We would like to have a way to restrict our
optimization over a smaller number of parameters  so we now discuss a generalization of (2) by
introducing an additional constraint to make it possible to reduce the number of parameters to learn 
permitting scalability to data sets with many training points and with very high dimensionality.
In order to
Theorem 1 shows that the optimal K
accommodate fewer parameters to learn  a natural option is to replace the unknown S matrix with a
low-rank matrix JLJ T   where J ∈ Rn×r is a pre-speciﬁed matrix  L ∈ Rr×r is unknown (we use
L instead of S to emphasize that S is of size n× n whereas L is r× r)  and the rank r is a parameter
of the algorithm. Then  we will explicitly enforce that the learned kernel is of this form.
By plugging in KW = αK + KSK into (1) and replacing S with JLJ T   the resulting optimization
problem is given by:

∗
W is of the form (cid:8)T W

(cid:8) = αK + KS

s.t. gi(αK + KJLJ T K) ≤ bi  1 ≤ i ≤ m.

f (αI + K 1/2JLJ T K 1/2)

(6)
While the above problem involves just r × r variables  the functions f and gi’s are applied to n × n
matrices and therefore the problem may still be computationally expensive to optimize. Below  we
show that for any spectral function f and linear constraints gi(KW ) = Tr(CiKW )  (6) reduces to a
problem that applies f and gi’s to r × r matrices only  which provides signiﬁcant scalability.
Theorem 2. Let K = (cid:8)T (cid:8) ≻ 0 and J ∈ Rn×r. Also  let the regularization function f be a spectral
function (see Deﬁnition 3.1) such that the corresponding scalar function fs has a global minima at
α. Then problem (6) is equivalent to the following problem:

min
L≽0

K.

∗

∗

min

L≽−α(KJ )(cid:0)1

f ((K J )

−1/2(αK J + K J LK J )(K J )

−1/2) 

s.t. Tr(LJ T KCiKJ) ≤ bi − Tr(αKCi)  1 ≤ i ≤ m.

(7)

5

Note that (7) is over r × r matrices (after initial pre-processing) and is in fact similar to the kernel
learning problem (1)  but with a kernel K J of smaller size r × r  r ≪ n. A proof of the above
theorem is in the supplementary material  and follows by showing that for spectral functions the
objective functions of the two problems can be shown to differ by a universal constant.
Similar to (1)  we can show that (6) is also equivalent to linear transformation kernel function learn-
ing. This enables us to naturally apply the above kernel learning problem in the inductive setting.
We provide a proof of the following theorem in the supplementary material.
Theorem 3. Consider (6) with a spectral function f so that corresponding scalar function fs has a
global minima at α and let K ≻ 0 be invertible. Then  (6) and (7) are equivalent to the following
linear transformation kernel learning problem (analogous to the connection between (1) and (2)):

min
W≽0 L

f (W )

s.t.

Tr((cid:8)T W (cid:8)) ≤ bi  1 ≤ i ≤ m  W = αI + XJLJX T .

(8)

Note that  in contrast to (2)  where the last constraint over W is achieved automatically  (8) requires
that constraint should be satisﬁed during the optimization process which leads to a reduced number
of parameters for our kernel learning problem. The above theorem shows that our reduced parame-
ters kernel learning method (6) also implicitly learns a linear transformation kernel function  hence
we can generalize the learned kernel to unseen data points using an expression similar to (3).
The parameter reduction approach presented in this section depends critically on the choice of J.
A few simple heuristics for choosing J beyond choosing a subset of the points from (cid:8) include
a randomly sampled coefﬁcient matrix or clustering (cid:8) into r clusters such that J is the cluster
membership indicator function. Also note that using this parameter reduction technique  we can
scale the optimization to kernel learning problems with millions of points of more. For example 
we have applied a special case of this scalable framework to learn kernels over data sets containing
nearly half a million images  as well as the MNIST data set of 60 000 data points [29].

4 Trace-norm based Inductive Semi-supervised Kernel Dimensionality

Reduction (Trace-SSIKDR)

We now consider applying our framework to the scenario of semi-supervised kernel dimensionality
reduction  which provides a novel and practical application of our framework. While there exists a
variety of methods for kernel dimensionality reduction  most of these methods are unsupervised (e.g.
kernel-PCA) or are restricted to the transductive setting. In contrast  we can use our kernel learning
framework to learn a low-rank transformation of the feature vectors implicitly that in turn provides
a low-dimensional embedding of the dataset. Furthermore  our framework permits a variety of side-
information such as pair-wise or relative distance constraints  beyond the class label information
allowed by existing transductive methods.
We describe our method starting from the linear transformation problem. Our goal is to learn a low-
rank linear transformation W whose corresponding low-dimensional mapped embedding of ϕi is
W 1/2ϕi. Even when the dimensionality of ϕi is very large  if the rank of W is low enough  then the
mapped embedding will have small dimensionality. With that in mind  a possible regularizer could
be the rank  i.e.  f (A) = rank(A); one can easily show that this satisﬁes the deﬁnition of a spectral
function. Unfortunately  optimization is intractable in general with the non-convex rank function 
so we use the trace-norm relaxation for the matrix rank function  i.e.  we set f (A) = Tr(A). This
function has been extensively studied as a relaxation for the rank function [30]  and it satisﬁes the
deﬁnition of a spectral function (with α = 0). We also add a small Frobenius norm regularization
for ease of optimization (this does not affect the spectral property of the regularization function).
Then using Theorem 1  the resulting relaxed kernel learning problem is:

τTr(K

−1/2KW K

−1/2) + ∥K

−1/2KW K

−1/2∥2

F

s.t. Tr(CiKW ) ≤ bi  1 ≤ i ≤ m 

(9)

min
KW ≽0

where τ > 0 is a parameter. The above problem can be solved using a method based on Uzawa’s
inexact algorithm  similar to [31].
We brieﬂy describe the steps taken by our method at each iteration. For simplicity  denote ~K =
−1/2; we will optimize with respect to ~K instead of KW . Let ~K t be the t-th iterate.
−1/2KW K
K
i = 0 ∀i. Let δt
Associate variable zt

i   1 ≤ i ≤ m with each constraint at each iteration t  and let z0

6

Table 1: UCI Datasets: accuracy achieved by various methods. The numbers in parentheses show
the rank of the corresponding learned kernels. Trace-SSIKDR achieves accuracy comparable to Frob
(Frobenius norm regularization) and ITML (LogDet regularization) with a signiﬁcantly smaller rank.

Dataset\Method

Iris
Wine

Ionosphere
Soybean
Diabetes

Balance-scale
Breast-cancer
Spectf-heart

Heart-c
Heart-h

Gaussian
0.99(40)
0.80(105)
0.94(337)
0.89(624)
0.75(251)
0.93(156)
0.72(259)
0.74(267)
0.68(228)
0.59(117)

Frob

0.99(27)
0.94(36)
0.98(64)
0.96(96)
0.74(154)
0.96(106)
0.73(61)
0.87(39)
0.78(62)
0.69(71)

ITML
0.99(40)
0.99(105)
0.98(337)
0.96(624)
0.76(251)
0.97(156)
0.78(259)
0.84(267)
0.79(228)
0.70(117)

Frob LR
0.91(4)
0.72(11)
0.98(19)
0.44(40)
0.67(14)
0.97(10)
0.69(21)
0.84(22)
0.73(39)
0.56(31)

ITML LR-pre

0.93(4)
0.85(11)
0.98(19)
0.87(40)
0.62(14)
0.80(10)
0.68(21)
0.89(22)
0.61(39)
0.30(31)

ITML LR-post

0.99(4)
0.46(11)
0.93(19)
0.35(40)
0.73(14)
0.82(10)
0.68(21)
0.89(22)
0.55(39)
0.56(31)

Trace-SSIKDR

0.99(4)
0.94(11)
0.99(19)
0.96(40)
0.74(14)
0.97(10)
0.75(21)
0.84(22)
0.78(39)
0.68(31)

(∑

)

be the step size at iteration t. The algorithm performs the following updates:

U (cid:6)U T ← K 1/2

zt−1
i Ci
← zt−1

i

i
zt
i

~K t ← U max((cid:6) − τ I  0)U T  
K 1/2 
− δ max(Tr(CiK 1/2 ~K tK 1/2) − bi  0) ∀i.

The above updates require computation of K 1/2 which is expensive for large high-rank matrices.
However  using elementary linear algebra we can show that ~K and the learned kernel function
−1/2 from
can be computed efﬁciently without computing K 1/2 by maintaining S = K
step to step. Algorithm 1 details an efﬁcient method for optimizing (9) and returns matrices (cid:6)k 
Dk and Vk all of which are contain only O(nk) parameters  where k is the rank of ~K t  which
changes from iteration to iteration. Note that step 4 of the algorithm computes k singular vectors
and requires O(nk2). Since k is typically signiﬁcantly smaller than n  the computational cost will
be signiﬁcantly smaller than computing the whole SVD. Note that the learned embedding ϕi →
−1/2ki  where ki is a vector of input kernel function values between ϕi and the training
~K 1/2K
data  can be computed efﬁciently as ϕi → (cid:6)1/2
k DkVkki  which does not require K 1/2 explicitly.
We defer the proof of correctness for Algorithm 1 to the supplementary material.

−1/2 ~KK

i = 0  t = 0

Algorithm 1 Trace-SSIKDR
Require: K  (Ci  bi)  1 ≤ i ≤ m  τ  δ
1: Initialize: z0
2: repeat
3:
t = t + 1
Compute Vk and (cid:6)k  the top k eigenvectors and eigenvalues of
4:
argmaxj σj > τ
5: Dk(i  i) ← 1/vT
6:
7: until Convergence
8: Return (cid:6)k  Dk  Vk

− δ max(Tr(CiKVkDk(cid:6)kDkV T

k K) − bi  0) ∀i.

i Kvi  1 ≤ i ≤ k

← zt−1

i

zt
i

(∑
i zt−1
i Ci

)

K  where k =

//St = VkDk(cid:6)kDkV T
k

5 Experimental Results

We now present empirical evaluation of our kernel learning framework and our semi-supervised
kernel dimensionality approach when applied in conjunction with k-nearest neighbor classiﬁcation.
In particular  using different regularization functions  we show that our framework can be used to
obtain signiﬁcantly better kernels than the baseline kernels for k-NN classiﬁcation. Additionally 
we show that our semi-supervised kernel dimensionality reduction approach achieves comparable
accuracy while signiﬁcantly reducing the dimensionality of the linear mapping.
UCI Datasets: First  we evaluate the performance of our kernel learning framework on standard
UCI datasets. We measure accuracy of the learned kernels using 5-NN classiﬁcation with two-fold
cross validation averaged over 10 runs. For training  we use pairwise (dis)similarity constraints as
described in Section 2.1. We select parameters l and u (right-hand side of the pairwise constraints)
using 5th and 95th percentiles of all the pairwise distances between points from the training dataset.

7

(c)

(d)

(a)

(b)

Figure 1: (a): Mean classiﬁcation accuracy on Caltech101 dataset obtained by 1-NN classiﬁcation
with learned kernels obtained by various methods. (b): Rank of the learned kernel functions obtained
by various methods. The rank of the learned kernel function is same as the reduced dimensionality
of the dataset. (c): Two-dimensional embedding of 2000 USPS digits obtained using our method
Trace-SSIKDR for a training set of just 100 USPS digits. Note that we use the inductive setting
here and the embedding is color coded according to the underlying digit. (d): Embedding of the
USPS digits dataset obtained using kernel-PCA.
Table 4 shows the 5-NN classiﬁcation accuracies achieved by our kernel learning framework with
different regularization functions. Gaussian represents the baseline Gaussian kernel  Frob represents
an instantiation of our framework with Frobenius norm (f (A) = ∥A∥2
F ) regularization  while ITML
corresponds to the LogDet regularization (f (A) = Tr(A) − log det(A) ). For the latter case  our
formulation is same as formulation proposed by [12]. Note that for almost all the datasets (except
Iris and Diabetes)  both Frob and ITML improve upon the baseline Gaussian kernel signiﬁcantly.
We also compare our semi-supervised dimensionality reduction method Trace-SSIKDR (see Sec-
tion 4) with baseline kernel dimensionality reduction methods Frob LR  ITML LR-pre  and ITML
LR-post. Frob LR reduces the rank of the learned matrix W (equivalently  it reduces the dimension-
ality) using Frobenius norm regularization by taking the top eigenvectors. Similarly  ITML LR-post
reduces the rank of the learned kernel matrix obtained using ITML by taking its top eigenvectors.
ITML LR-pre reduces the rank of the kernel function by reducing the rank of the training kernel ma-
trix. The learned linear transformation W (or equivalently  the learned kernel function) should have
the same rank as that of training kernel matrix as the LogDet divergence preserves the range space
of the input kernel. We ﬁx the rank of the learned W for Frob LR  ITML LR-pre  ITML LR-post as
the rank of the transformation W obtained by our Trace-SSIKDR method. Note that Trace-SSIKDR
achieves accuracies similar to Frob and ITML  while decreasing the rank signiﬁcantly. Furthermore 
it is signiﬁcantly better than the corresponding baseline dimensionality reduction methods.
Caltech-101: Next  we evaluate our kernel learning framework on the Caltech-101 dataset  a bench-
mark object recognition dataset containing over 3000 images. Here  we compare various methods
using 1-NN classiﬁcation method and the accuracy is measured in terms of the mean recognition
accuracy per class. We use a pool of 30 images per class for our experiments  out of which a vary-
ing number of random images are selected for training and the remaining are used for testing the
learned kernel function. The baseline kernel function is selected to be the sum of four different
kernel functions: PMK [32]  SPMK [33]  Geoblur-1 and Geoblur-2 [34]. Figure 1 (a) shows the
accuracy achieved by various methods (acronyms represent the same methods as described in the
previous section). Clearly  ITML and Frob (which are speciﬁc instances of our framework) are able
to learn signiﬁcantly more accurate kernel functions than the baseline kernel function. Furthermore 
our Trace-SSIKDR method is able to achieve reasonable accuracy while reducing the rank of the
kernel function signiﬁcantly (Figure 1 (b)). Also note that Trace-SSIKDR achieves signiﬁcantly
better accuracy than Frob LR  ITML LR-pre and ITML LR-post  although all of these methods have
the same rank as Trace-SSIKDR.
USPS Digits: Finally  we qualitatively evaluate our dimensionality reduction method on the USPS
digits dataset. Here  we train our method using 100 examples to learn a linear mapping to two
dimensions  i.e.  a rank-2 matrix W . For the baseline kernel  we use the data-dependent kernel func-
tion proposed by [25] that also takes data’s manifold structure into account. We then embed 2000
(unseen) test examples into two dimensions using our learned low-rank transformation. Figure 1 (c)
shows the embedding obtained by our Trace-SSIKDR method  while Figure 1 (d) shows the embed-
ding obtained by the kernel-PCA algorithm. Each point is color coded according to the underlying
digit. Note that our method is able to separate out most of the digits even in 2D  and is signiﬁcantly
better than the embedding obtained using kernel-PCA.
Acknowledgements: This research was supported in part by NSF grant CCF-0728879.

8

51015200.10.20.30.40.50.60.70.8Training examples per classMean recognition accuracyAccuracy vs Training Set Size  Trace−SSIKDRITMLFrobFrob LRITML LR−postITML LR−preBaseline51015200100200300400500Training examples per classDimensionality of the learned mappingDimensionality vs Training Set Size  Trace−SSIKDRFrob−1−0.8−0.6−0.4−0.200.20.40.60.81−0.8−0.6−0.4−0.200.20.40.60.8−0.25−0.2−0.15−0.1−0.0500.05−0.1−0.0500.050.1References
[1] K. Tsuda  G. R¨atsch  and M. K. Warmuth. Matrix exponentiated gradient updates for on-line learning and

Bregman projection. JMLR  6:995–1018  2005.

[2] J. T. Kwok and I. W. Tsang. Learning with idealized kernels. In ICML  2003.
[3] N. Cristianini  J. Shawe-Taylor  A. Elisseeff  and J. Kandola. On kernel-target alignment. In NIPS  2001.
[4] C. S. Ong  A. J. Smola  and R. C. Williamson. Learning the kernel with hyperkernels. JMLR  6:1043–

1071  2005.

[5] G. R. G. Lanckriet  N. Cristianini  P. L. Bartlett  L. El Ghaoui  and M. I. Jordan. Learning the kernel

matrix with semideﬁnite programming. JMLR  5:27–72  2004.

[6] Xiaojin Zhu  Jaz Kandola  Zoubin Ghahramani  and John Lafferty. Nonparametric transforms of graph
kernels for semi-supervised learning. In Lawrence K. Saul  Yair Weiss  and L´eon Bottou  editors  NIPS 
volume 17  pages 1641–1648  2005.

[7] Peter V. Gehler and Sebastian Nowozin. Let the kernel ﬁgure it out; principled learning of pre-processing

for kernel classiﬁers. In CVPR  pages 2836–2843  2009.

[8] Matthias Seeger. Cross-validation optimization for large scale hierarchical classiﬁcation kernel methods.

In NIPS  pages 1233–1240  2006.

[9] Yoshua Bengio  Olivier Delalleau  Nicolas Le Roux  Jean-Francois Paiement  Pascal Vincent  and Marie
Ouimet. Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computation 
16(10):2197–2219  2004.

[10] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. J. Russell. Distance metric learning with application to clustering

with side-information. In NIPS  pages 505–512  2002.

[11] K. Q. Weinberger  J. Blitzer  and L. K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. In NIPS  2005.

[12] J. V. Davis  B. Kulis  P. Jain  S. Sra  and I. S. Dhillon. Information-theoretic metric learning. In ICML 

pages 209–216  2007.

[13] S. Shalev-Shwartz  Y. Singer  and A. Y. Ng. Online and batch learning of pseudo-metrics. In ICML  2004.
[14] A. Globerson and S. T. Roweis. Metric learning by collapsing classes. In NIPS  2005.
[15] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood component analysis. In NIPS 

2004.

[16] B. Kulis  S. Sra  and I. S. Dhillon. Convex perturbations for scalable semideﬁnite programming.

AISTATS  2009.

In

[17] R. Chatpatanasiri  T. Korsrilabutr  P. Tangchanachaianan  and B. Kijsirikul. On kernelization of supervised

Mahalanobis distance learners  2008.

[18] Andreas Argyriou  Charles A. Micchelli  and Massimiliano Pontil. On spectral learning. JMLR  11:935–

953  2010.

[19] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods. In ICML  pages

33–40  2005.

[20] L. Song  A. Smola  K. M. Borgwardt  and A. Gretton. Colored maximum variance unfolding. In NIPS 

pages 1385–1392  2007.

[21] Y. Song  F. Nie  C. Zhang  and S. Xiang. A uniﬁed framework for semi-supervised dimensionality reduc-

tion. Pattern Recognition  41(9):2789–2799  2008.

[22] K. Fukumizu  F. R. Bach  and M. I. Jordan. Kernel dimensionality reduction for supervised learning. In

NIPS  2003.

[23] R. Urtasun and T. Darrell. Discriminative gaussian process latent variable model for classiﬁcation. In

ICML  pages 927–934  2007.

[24] S. Mika  B. Sch¨olkopf  A. J. Smola  K. M¨uller  M. Scholz  and G. R¨atsch. Kernel pca and de-noising in

feature spaces. In NIPS  pages 536–542  1998.

[25] V. Sindhwani  P. Niyogi  and M. Belkin. Beyond the point cloud: from transductive to semi-supervised

learning. In ICML  pages 824–831  2005.

[26] Brian Kulis  M´aty´as Sustik  and Inderjit S. Dhillon. Learning low-rank kernel matrices. In ICML  pages

505–512  2006.

[27] Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative comparisons. In NIPS 

2003.

[28] A. Bar-Hillel  T. Hertz  N. Shental  and D. Weinshall. Learning a mahalanobis metric from equivalence

constraints. JMLR  6:937–965  2005.

[29] P. Jain  B. Kulis  and K. Grauman. Fast image search for learned metrics. In CVPR  2008.
[30] B. Recht  M. Fazel  and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization  2007.

[31] J. Cai  E. J. Candes  and Z. Shen. A singular value thresholding algorithm for matrix completion  2008.
[32] K. Grauman and T. Darrell. The Pyramid Match Kernel: Efﬁcient learning with sets of features. Journal

of Machine Learning Research (JMLR)  8:725–760  April 2007.

[33] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing

natural scene categories. In CVPR  pages 2169–2178  2006.

[34] A. C. Berg and J. Malik. Geometric blur for template matching. In CVPR  pages 607–614  2001.

9

,Yuhong Guo
Isabel Valera
Zoubin Ghahramani
Ting-Chun Wang
Ming-Yu Liu
Jun-Yan Zhu
Guilin Liu
Andrew Tao
Jan Kautz
Bryan Catanzaro