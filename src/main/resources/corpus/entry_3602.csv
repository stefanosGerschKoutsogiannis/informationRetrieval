2008,Multi-stage Convex Relaxation for Learning with Sparse Regularization,We study learning formulations with non-convex regularizaton that are natural for sparse linear models. There are two approaches to this problem: (1) Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. (2) Convex relaxation such as $L_1$-regularization that solves the problem under some conditions. However it often leads to sub-optimal sparsity in reality. This paper tries to remedy the above gap between theory and practice. In particular  we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization. Theoretically  we analyze the behavior of a resulting two-stage relaxation scheme for the capped-$L_1$ regularization. Our performance bound shows that the procedure is superior to the standard $L_1$ convex relaxation for learning sparse targets. Experiments confirm the effectiveness of this method on some simulation and real data.,Multi-stage Convex Relaxation for Learning with

Sparse Regularization

Tong Zhang

Statistics Department
Rutgers University  NJ

tzhang@stat.rutgers.edu

Abstract

We study learning formulations with non-convex regularizaton that are natural for
sparse linear models. There are two approaches to this problem:
• Heuristic methods such as gradient descent that only ﬁnd a local minimum.
A drawback of this approach is the lack of theoretical guarantee showing that
the local minimum gives a good solution.
• Convex relaxation such as L1-regularization that solves the problem under

some conditions. However it often leads to sub-optimal sparsity in reality.

This paper tries to remedy the above gap between theory and practice. In partic-
ular  we investigate a multi-stage convex relaxation scheme for solving problems
with non-convex regularization. Theoretically  we analyze the behavior of a re-
sulting two-stage relaxation scheme for the capped-L1 regularization. Our per-
formance bound shows that the procedure is superior to the standard L1 convex
relaxation for learning sparse targets. Experiments conﬁrm the effectiveness of
this method on some simulation and real data.

1 Introduction
Consider a set of input vectors x1  . . .   xn ∈ Rd  with corresponding desired output variables
y1  . . .   yn. The task of supervised learning is to estimate the functional relationship y ≈ f(x)
between the input x and the output variable y from the training examples {(x1  y1)  . . .   (xn  yn)}.
The quality of prediction is often measured through a loss function φ(f(x)  y). We assume that
φ(f  y) is convex in f throughout the paper.
In this paper  we consider linear prediction model
f(x) = wT x. As in boosting or kernel methods  nonlinearity can be introduced by including non-
linear features in x.
We are mainly interested in the scenario that d (cid:29) n. That is  there are many more features than the
number of samples. In this case  an unconstrained empirical risk minimization is inadequate because
the solution overﬁts the data. The standard remedy for this problem is to impose a constraint on w
to obtain a regularized problem. An important target constraint is sparsity  which corresponds to the
(non-convex) L0 regularization  deﬁned as kwk0 = |{j : wj 6= 0}| = k. If we know the sparsity
parameter k for the target vector  then a good learning method is L0 regularization:

φ(wT xi  yi)

subject to kwk0 ≤ k.

(1)

ˆw = arg min
w∈Rd

1
n

nX

i=1

If k is not known  then one may regard k as a tuning parameter  which can be selected through cross-
validation. This method is often referred to as subset selection in the literature. Sparse learning is
an essential topic in machine learning  which has attracted considerable interests recently. It can be
shown that the solution of the L0 regularization problem in (1) achieves good prediction accuracy

1

if the target function can be approximated by a sparse ¯w. However  a fundamental difﬁculty with
this method is the computational cost  because the number of subsets of {1  . . .   d} of cardinality k
(corresponding to the nonzero components of w) is exponential in k.
Due to the computational difﬁcult  in practice  it is necessary to replace (1) by some easier to solve
formulations below:

nX

i=1

ˆw = arg min
w∈Rd

1
n

φ(wT xi  yi) + λg(w) 

(2)

with g(w) =Pd
as α → 0 P

where λ > 0 is an appropriately chosen regularization condition. We obtain a formulation equiv-
alent to (2) by choosing the regularization function as g(w) = kwk0. However  this function is
discontinuous. For computational reasons  it is helpful to consider a continuous approximation with
g(w) = kwkp  where p > 0. If p ≥ 1  the resulting formulation is convex. In particular  by choos-
ing the closest approximation with p = 1  one obtain Lasso  which is the standard convex relaxation
formulation for sparse learning. With p ∈ (0  1)  the Lp regularization kwkp is non-convex but con-
tinuous. In this paper  we are also interested in the following capped-L1 approximation of kwk0 
j=1 min(|wj|  α)  where for v ∈ R: This is a good approximation to L0 because
j min(|wj|  α)/α → kwk0. Therefore when α → 0  this regularization condition is
equivalent to the sparse L0 regularization upto a rescaling of λ. Note that the capped-L1 regulariza-
tion is also non-convex. It is related to the so-called SCAD regularization in statistics  which is a
smoother version. We use the simpler capped-L1 regularization because the extra smoothness does
not affect our algorithm or theory.
For a non-convex but smooth regularization condition such as capped-L1 or Lp with p ∈ (0  1) 
standard numerical techniques such as gradient descent leads to a local minimum solution. Unfor-
tunately  it is difﬁcult to ﬁnd the global optimum  and it is also difﬁcult to analyze the quality of the
local minimum. Although in practice  such a local minimum solution may outperform the Lasso so-
lution  the lack of theoretical (and practical) performance guarantee prevents the more wide-spread
applications of such algorithms. As a matter of fact  results with non-convex regularization are dif-
ﬁcult to reproduce because different numerical optimization procedures can lead to different local
minima. Therefore the quality of the solution heavily depend on the numerical procedure used.
The situation is very difﬁcult for a convex relaxation formulation such as L1-regularization (Lasso).
The global optimum can be easily computed using standard convex programming techniques. It is
known that in practice  1-norm regularization often leads to sparse solutions (although often sub-
optimal). Moreover  its performance has been theoretically analyzed recently. For example  it is
known from the compressed sensing literature that under certain conditions  the solution of L1 re-
laxation may be equivalent to L0 regularization asymptotically even when noise is present (e.g. [3]
and references therein). If the target is truly sparse  then it was shown in [9] that under some restric-
tive conditions referred to as irrepresentable conditions  1-norm regularization solves the feature
selection problem. The prediction performance of this method has been considered in [4  8  1].
Despite of its success  L1-regularization often leads to suboptimal solutions because it is not a good
approximation to L0 regularization. Statistically  this means that even though it converges to the
true sparse target when n → ∞ (consistency)  the rate of convergence can be suboptimal. The
only way to ﬁx this problem is to employ a non-convex regularization condition that is closer to
L0 regularization  such as the capped-L1 regularization. The superiority of capped-L1 is formally
proved later in this paper.
Because of the above gap between practice and theory  it is important to study direct solutions of
non-convex regularization beyond the standard L1 relaxation. Our goal is to design a numerical pro-
cedure that leads to a reproducible solution with better theoretical behavior than L1-regularization.
This paper shows how this can be done. Speciﬁcally  we consider a general multi-stage convex re-
laxation method for solving learning formulations with non-convex regularization. In this scheme 
concave duality is used to construct a sequence of convex relaxations that give better and better
approximations to the original non-convex problem. Moreover  using the capped-L1 regularization 
we show that after only two stages  the solution gives better statistical performance than standard
Lasso when the target is approximately sparse. In essence  this paper establishes a performance
guarantee for non-convex formulations using a multi-stage convex relaxation approach that is more
sophisticated than the standard one-stage convex relaxation (which is the standard approach com-

2

monly studied in the current literature). Experiments conﬁrm the effectiveness of the multi-stage
approach.

2 Concave Duality

Given a continuous regularization function g(w) in (2) which may be non-convex  we are interested
in rewriting it using concave duality. Let h(w) : Rd → Ω ⊂ Rd be a map with range Ω. It may not
be a one-to-one map. However  we assume that there exists a function ¯gh(u) deﬁned on Ω such that
g(w) = ¯gh(h(w)) holds.
We assume that we can ﬁnd h so that the function ¯gh(u) is a concave function of u on Ω. Under
this assumption  we can rewrite the regularization function g(w) as:

using concave duality [6]. In this case  g∗

g(w) = inf
v∈Rd
h(v) is the concave dual of ¯gh(u) given below

(cid:2)vT h(w) + g∗
h(v)(cid:3)
(cid:2)−vT u + ¯gh(u)(cid:3) .

g∗
h(v) = inf
u∈Ω

Moreover  it is well-known that the minimum of the right hand side of (3) is achieved at

ˆv = ∇u¯gh(u)|u=h(w).

(3)

(4)

j vp/(p−q)
ˆvj = (p/q)|wj|p−q.

j

p ∈ (0  1). Given any q > p  (3) holds with h(w) = [|w1|q  . . .  |wd|q] and g∗

This is a very general framework. For illustration  we include two example non-convex sparse
regularization conditions discussed in the introduction.

Lp regularization We consider the regularization condition g(w) = Pd
c(p  q)P
In this case  ¯gh(u) = Pd
Capped-L1 regularization We consider the regularization condition g(w) =Pd

j=1 |wj|p for some
h(v) =
deﬁned on the domain {v : vj ≥ 0}  where c(p  q) = (q − p)pp/(q−p)qq/(p−q).
on Ω = {u : uj ≥ 0}. The solution in (4) is given by

j=1 min(|wj|  α).
In this case  (2) holds with h(w) = [|w1|  . . .  |wd|] and g∗
j=1 α(1 − vj)I(vj ∈ [0  1])
deﬁned on the domain {v : vj ≥ 0}  where I(·) is the set indicator function. The solution in (4) is
given by ˆvj = I(|wj| ≤ α).

h(v) =Pd

j=1 up/q

j

3 Multi-stage Convex Relaxation

g(w). Let h(w) =P

We consider a general procedure for solving (2) with convex loss and non-convex regularization
j hj(w) be a convex relaxation of g(w) that dominates g(w) (for example 
it can be the smallest convex upperbound (i.e.  the inf over all convex upperbounds) of g(w)). A
simple convex relaxation of (2) becomes

dX

 .

 1

n

nX

"

nX

i=1

1
n

ˆw = arg min
w∈Rd

φ(wT xi  yi) + λ

hj(w)

i=1

j=1

(5)

This simple relaxation can yield a solution that is not close to the solution of (2). However  if h
satisﬁes the condition of Section 2  then it is possible to write g(w) as (3). Now  with this new
representation  we can rewrite (2) as

[ ˆw  ˆv] = arg min
w v∈Rd

φ(wT xi  yi) + λvT h(w) + λg∗

h(v) 

 

(6)

If we can ﬁnd a good approximation of ˆv that
This is clearly equivalent to (2) because of (3).
improves upon the initial value of ˆv = [1  . . .   1]  then the above formulation can lead to a reﬁned
convex problem in w that is a better convex relaxation than (5).

3

#

Our numerical procedure exploits the above fact  which tries to improve the estimation of vj over
the initial choice of vj = 1 in (5) using an iterative algorithm. This can be done using an alternating
optimization procedure  which repeatedly applies the following two steps:

• First we optimize w with v ﬁxed: this is a convex problem in w with appropriately chosen
• Second we optimize v with w ﬁxed: although non-convex  it has a closed form solution

h(w).

that is given by (4).

The general procedure is presented in Figure 1. It can be regarded as a generalization of CCCP
(concave-convex programming) [7]  which takes h(w) = w. By repeatedly reﬁning the parameter
v  we can potentially obtain better and better convex relaxation  leading to a solution superior to that
of the initial convex relaxation. Note that using the Lp and capped-L1 regularization conditions in
Section 2  this procedure lead to more speciﬁc multi-stage convex relaxation algorithms. We skip
the details due to the space limitation.

Tuning parameters: λ
Input: training data (x1  y1)  . . .   (xn  yn)
Output: weight vector ˆw
initialize ˆvj = 1
Repeat the following two steps until convergence:

• Let ˆw = arg minw∈Rd
• Let ˆv = ∇u¯gh(u))|u=h(w)

n

(cid:2) 1

i=1 φ(wT xi  yi) + λˆvT h(w)(cid:3)
Pn

(∗)

Figure 1: Multi-stage Convex Relaxation Method

4 Theory of Two-stage Convex Relaxation for Capped-L1 Regularization

Although the reasoning in Section 3 is appealing  it is only a heuristic argument without any formal
theoretical guarantee. In contrast  the simple one-stage L1 relaxation is known to perform reasonably
well under certain assumptions. Therefore unless we can develop a theory to show the effectiveness
of the multi-stage procedure in Figure 1  our proposal is mere yet another local minimum ﬁnding
scheme that may potentially stuck into a bad local solution.
This section tries to address this issue. Although we have not yet developed a complete theory for
the general procedure  we are able to obtain a learning bound for the capped-L1 regularization. In
particular  if the target function is sparse  then the performance of the solution after merely two-
stages of our procedure is superior to that of Lasso. This demonstrates the effectiveness of the
multi-stage approach. Since the analysis is rather complicated  we focus on the least squares loss
only  and only for the solution after two-stages of the algorithm.
For a complete theory  the following questions are worth asking:

• Under what conditions  the global solution with non-convex penalty is statistically better
than the (one-stage) convex relaxation solution? That is  when does it lead to better predic-
tion accuracy or generalization error?
• Under what conditions  there is only one local minimum solution close to the solution of
the initial convex relaxation  and it is also the global optimum? Moreover  does multi-stage
convex relaxation ﬁnd this solution?

The ﬁrst question answers whether it is beneﬁcial to use a non-convex penalty function. The second
question answers whether we can effectively solve the resulting non-convex problem using multi-
stage convex relaxation. The combination of the two questions leads to a satisfactory theoretical
answer to the effectiveness of the multi-stage procedure.
A general theory along this line will be developed in the full paper. In the following  instead of
trying to answer the above questions separately  we provide a uniﬁed ﬁnite sample analysis for the
procedure that directly addresses the combined effect of the two questions. The result is adopted

4

from [8]  which justiﬁes the multi-stage convex relaxation approach by showing that the two-stage
procedure using capped-L1 regularization can lead to better generalization than the standard one
stage L1 regularization.
The procedure we shall analyze  which is a special case of the multi-stage algorithm in Figure 1 with
capped-L1 regularization and only two stages  is described in Figure 2. It is related to the adaptive
Lasso method [10]. The result is reproducible when the solution of the ﬁrst stage is unique because
it involves two well-deﬁned convex programming problems. Note that it is described with least
squares loss only because our analysis assumes least squares loss: a more general analysis for other
loss functions is possible but would lead to extra complications that are not central to our interests.

Tuning parameters: λ  α
Input: training data (x1  y1)  . . .   (xn  yn)
Output: weight vector ˆw0

Stage 1: Compute ˆw by solving the L1 penalization problem:

ˆw = arg min
w∈Rd

(wT xi − yi)2 + λkwk1

Stage 2: Solving the following selective L1 penalization problem:

"

nX

i=1

1
n

 1

n

nX

i=1

#

.

 .

|wj|

X

j:| ˆwj|≤α

ˆw0 = arg min
w∈Rd

(wT xi − yi)2 + λ

Figure 2: Two-stage capped-L1 Regularization

This particular two-stage procedure also has an intuitive interpretation (besides treating it as a spe-
cial case of multi-stage convex relaxation). We shall refer to the feature components corresponding
to the large weights as relevant features  and the feature components smaller the cut-off threshold α
as irrelevant features. We observe that as an estimation method  L1 regularization has two impor-
tant properties: shrink estimated weights corresponding to irrelevant features toward zero; shrink
estimated weights corresponding to relevant features toward zero. While the ﬁrst effect is desirable 
the second effect is not. In fact  we should avoid shrinking the weights corresponding to the relevant
features if we can identify these features. This is why the standard L1 regularization may have sub-
optimal performance. However  after the ﬁrst stage of L1 regularization  we can identify the relevant
features by picking the components corresponding to the largest weights; in the second stage of L1
regularization  we do not have to penalize the features selected in the ﬁrst stage  as in Figure 2.
A related method  called relaxed Lasso  was proposed recently by Meinshausen [5]  which is similar
to a two-stage Dantzig selector in [2]. Their idea differs from our proposal in that in the second
j are forced to be zero when j /∈ supp0( ˆw). It was pointed out
stage  the weight coefﬁcients w0
in [5] that if supp0( ˆw) can exactly identify all non-zero components of the target vector  then in
the second stage  the relaxed Lasso can asymptotically remove the bias in the ﬁrst stage Lasso.
However  it is not clear what theoretical result can be stated when Lasso cannot exactly identify all
relevant features. In the general case  it is not easy to ensure that relaxed Lasso does not degrade
the performance when some relevant coefﬁcients become zero in the ﬁrst stage. On the contrary  the
two-stage penalization procedure in Figure 2  which is based on the capped-L1 regularization  does
not require that all relevant features are identiﬁed. Consequently  we are able to prove a result for
Figure 2 with no counterpart for relaxed Lasso.
Deﬁnition 4.1 Let w = [w1  . . .   wd] ∈ Rd and α ≥ 0  we deﬁne the set of relevant features with
threshold α as:

suppα(w) = {j : |wj| > α}.

(cid:16)P
j>k |wij|2(cid:17)1/2

Moreover  if |wi1| ≥ ··· ≥ |wid| are in descending order  then deﬁne δk(w) =
as the 2-norm of the largest k components (in absolute value) of w.

For simplicity  we assume sub-Gaussian noise as follows.

5

Assumption 4.1 Assume that {yi}i=1 ... n are independent (but not necessarily identically dis-
tributed) sub-Gaussians: there exists σ ≥ 0 such that ∀i and ∀t ∈ R 

Eyiet(yi−Eyi) ≤ eσ2t2/2.

Both Gaussian and bounded random variables are sub-Gaussian using the above deﬁnition. For
example  if a random variable ξ ∈ [a  b]  then Eξet(ξ−Eξ) ≤ e(b−a)2t2/8. If a random variable is
Gaussian: ξ ∼ N(0  σ2)  then Eξetξ ≤ eσ2t2/2.
i   deﬁne M ˆA = supi6=j | ˆAi j|  and
Theorem 4.1 Let Assumption 4.1 hold. Let ˆA = 1
assume that ˆAj j = 1 for all j. Consider any target vector ¯w such that Ey = ¯wT x  and assume that
¯w contains only s non-zeros where s ≤ d/3 and assume that M ˆAs ≤ 1/6. Let k = |suppλ( ¯w)|.
Consider the two-stage method in Figure 2. Given η ∈ (0  0.5)  with probability larger than 1− 2η:

if α/48 ≥ λ ≥ 12σp2 ln(2d/η)/n  then

Pn

i=1 xixT

n

k ˆw0 − ¯wk2 ≤ 24pk − qλ + 24σ

 

r20q

n

!

1 +

ln(1/η)

+ 168δk( ¯w) 

where q = |supp1.5α( ¯w)|.
The proof of this theorem can be found in [8]. Note that the theorem allows the situation d (cid:29)
n  which is what we are interested in. The condition M ˆAs ≤ 1/6  often referred to as mutual
coherence  is also quite standard in the analysis of L1 regularization  e.g.  in [1  3]. Although the
condition is idealized  the theorem nevertheless yields important insights into the behavior of the
two-stage algorithm. This theorem leads to a bound for Lasso with α = ∞ or q = 0. The bound has
the form

√

k ˆw0 − ¯wk2 = O(δk( ¯w) +

kλ).

√
This bound is tight for Lasso  in the sense that the right hand side cannot be improved except for
the constant. In particular  the factor O(
kλ) cannot be removed using Lasso — this can be easily
veriﬁed with an orthogonal design matrix. It is known that in order for Lasso to be effective  one

has to pick λ no smaller than the order σpln d/n. Therefore  the generalization of standard Lasso
is of the order δk( ¯w) + σpk ln d/n  which cannot be improved. Similar results appear in [1  4].
Lasso result if the sparse target satisﬁes δk( ¯w) (cid:28) √
Now  with a small α  the bound in Theorem 4.1 can be signiﬁcantly better than that of the standard
kλ and k − q (cid:28) k. The latter condition is true
when |supp1.5α( ¯w)| ≈ |suppλ( ¯w)|. These conditions are satisﬁed when most non-zero coefﬁcients
of ¯w in suppλ( ¯w) are relatively large in magnitude and the rest is small in 2-norm. That is  when
components of ¯w are large)  we obtain k ˆw0− ¯wk2 = O(pk ln(1/η)/n) for the two-stage procedure 
the target ¯w can be decompose as a sparse vector with large coefﬁcients plus another (less sparse)
vector with small coefﬁcients. In the extreme case when q = k = |supp0( ¯w)| (that is  all nonzero
which is superior to the standard one-stage Lasso bound k ˆw − ¯wk2 = O(pk ln(d/η)/n). Again 

this bound cannot be improved for Lasso  and the difference can be signiﬁcant when d is large.

5 Experiments

In the following  we show with a synthetic and a real data that our multi-stage approach improves
the standard Lasso in practice. In order to avoid cluttering  we only study results for the two-stage
procedure of Figure 2  which corresponds to the capped-L1 regularization. We shall also compare
it to the two-stage Lp regularization method with p = 0.5  which corresponds to the adaptive Lasso
approach [10]. Note that instead of tuning the α parameter in Figure 2  in these experiments  we
tune the number of features q in ˆw that are larger than the threshold α (i.e.  q = |{j : | ˆwj| > α}|
is the number of features that are not regularized in stage-2). This is clearly more convenient than
tuning α. The standard Lasso corresponds to q = 0.
In the ﬁrst experiment  we generate an n × d random matrix with its column j corresponding to
[x1 j  . . .   xn j]  and each element of the matrix is an independent standard Gaussian N(0  1). We
i j = n. A truly sparse target ¯β  is generated with k

then normalize its columns so thatPn

i=1 x2

6

nonzero elements that are uniformly distributed from [−10  10]. The observation yi = ¯βT xi + i 
where each i ∼ N(0  σ2). In this experiment  we take n = 25  d = 100  k = 5  σ = 1  and repeat
the experiment 100 times. The average training error and 2-norm parameter estimation error are
reported in Figure 3. We compare the performance of the two-stage method with different q versus
the regularization parameter λ. As expected  the training error becomes smaller when q increases.
Compared to the standard Lasso (which corresponds to q = 0)  substantially smaller estimation
error is achieved with q = 3 for Capped-L1 regularization and with p = 0.5 for Lp regularization.
This shows that the multi-stage convex relaxation approach is effective.

Figure 3: Performance of multi-stage convex relaxation on simulation data. Left: average training
squared error versus λ; Right: parameter estimation error versus λ.

In the second experiment  we use real data to illustrate the effectiveness of the multi-stage approach.
Due to the space limitation  we only report the performance on a single data  Boston Housing. This
is the housing data for 506 census tracts of Boston from the 1970 census  available from the UCI
Machine Learning Database Repository: http://archive.ics.uci.edu/ml/. Each census tract is a data-
point  with 13 features (we add a constant offset on e as the 14th feature)  and the desired output is
the housing price. In the experiment  we randomly partition the data into 20 training plus 456 test
points. We perform the experiments 100 times  and report training and test squared error versus the
regularization parameter λ for different q. The results are plotted in Figure 4. In this case  q = 1
achieves the best performance. This means one feature can be reliably identiﬁed in this example.
In comparison  adaptive Lasso is not effective. Note that this dataset contains only a small number
(d = 14) features  which is not the case where we can expect signiﬁcant beneﬁt from the multi-stage
approach (most of other UCI data similarly contain only small number of features). In order to
illustrate the advantage of the two-stage method more clearly  we also consider a modiﬁed Boston
Housing data  where we append 20 random features (similar to the simulation experiments) to the
original Boston Housing data  and rerun the experiments. The results are shown in Figure 5. As
expected from Theorem 4.1 and the discussion thereafter  since d becomes large  the multi-stage
convex relaxation approach with capped-L1 regularization (q > 0) has signiﬁcant advantage over
the standard Lasso (q = 0).

References

[1] Florentina Bunea  Alexandre Tsybakov  and Marten H. Wegkamp. Sparsity oracle inequalities

for the Lasso. Electronic Journal of Statistics  1:169–194  2007.

[2] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is

much larger than n. Annals of Statistics  2007.

[3] David L. Donoho  Michael Elad  and Vladimir N. Temlyakov. Stable recovery of sparse over-
complete representations in the presence of noise. IEEE Trans. Info. Theory  52(1):6–18  2006.

7

lllllllll0.010.020.050.100.200.501.002.000.0050.0200.1000.5002.000lambdatraining errorlllllllllllq=0q=1q=3Lp (p=0.5)lllllllll0.010.020.050.100.200.501.002.00234567lambdaparameter estimation errorlllllllllllq=0q=1q=3Lp (p=0.5)Figure 4: Performance of multi-stage convex relaxation on the original Boston Housing data. Left:
average training squared error versus λ; Right: test squared error versus λ.

Figure 5: Performance of multi-stage convex relaxation on the modiﬁed Boston Housing data. Left:
average training squared error versus λ; Right: test squared error versus λ.

[4] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l’Institut

Henri Poincaré  2008.

[5] Nicolai Meinshausen. Lasso with relaxation. ETH Research Report  2005.
[6] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press  Princeton  NJ  1970.
[7] Alan L. Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation 

15:915–936  2003.

[8] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regulariza-

tion. The Annals of Statistics  2009. to appear.

[9] Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2567  2006.

[10] Hui Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical

Association  101:1418–1429  2006.

8

lllllllllll0.10.20.51.02.0102030405060lambdatraining errorlllllllllllllq=0q=1q=2Lp (p=0.5)lllllllllll0.10.20.51.02.050607080lambdatest errorlllllllllllllq=0q=1q=2Lp (p=0.5)llllllllllllll0.10.20.51.02.05.00.51.02.05.010.050.0200.0lambdatraining errorllllllllllllllllq=0q=1q=2Lp (p=0.5)llllllllllllll0.10.20.51.02.05.0100150200250lambdatest errorllllllllllllllllq=0q=1q=2Lp (p=0.5),Kush Bhatia
Prateek Jain
Purushottam Kar