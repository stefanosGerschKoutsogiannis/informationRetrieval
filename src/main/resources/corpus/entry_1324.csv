2016,Deep ADMM-Net for Compressive Sensing MRI,Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance Imaging (MRI). It aims at reconstructing MR image from a small number of  under-sampled data in k-space  and accelerating the data acquisition in MRI.  To improve the current MRI system in reconstruction accuracy and computational speed   in this paper  we propose a novel deep architecture  dubbed ADMM-Net.  ADMM-Net is defined over a data flow graph  which is derived from the iterative  procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing a CS-based MRI model. In the training phase  all parameters of the net  e.g.  image transforms  shrinkage functions  etc.  are discriminatively trained end-to-end using L-BFGS algorithm. In the testing phase  it has computational overhead similar to ADMM but uses optimized parameters learned from the  training data for CS-based reconstruction task. Experiments on MRI image  reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline ADMM algorithm and achieves high reconstruction  accuracies with fast computational speed.,Deep ADMM-Net for Compressive Sensing MRI

Yan Yang

Xi‚Äôan Jiaotong University

yangyan92@stu.xjtu.edu.cn

Huibin Li

Xi‚Äôan Jiaotong University

huibinli@mail.xjtu.edu.cn

Jian Sun

Xi‚Äôan Jiaotong University

jiansun@mail.xjtu.edu.cn

Zongben Xu

Xi‚Äôan Jiaotong University
zbxu@mail.xjtu.edu.cn

Abstract

Compressive Sensing (CS) is an effective approach for fast Magnetic Resonance
Imaging (MRI). It aims at reconstructing MR image from a small number of under-
sampled data in k-space  and accelerating the data acquisition in MRI. To improve
the current MRI system in reconstruction accuracy and computational speed  in
this paper  we propose a novel deep architecture  dubbed ADMM-Net. ADMM-
Net is deÔ¨Åned over a data Ô¨Çow graph  which is derived from the iterative pro-
cedures in Alternating Direction Method of Multipliers (ADMM) algorithm for
optimizing a CS-based MRI model. In the training phase  all parameters of the
net  e.g.  image transforms  shrinkage functions  etc.  are discriminatively trained
end-to-end using L-BFGS algorithm. In the testing phase  it has computational
overhead similar to ADMM but uses optimized parameters learned from the train-
ing data for CS-based reconstruction task. Experiments on MRI image reconstruc-
tion under different sampling ratios in k-space demonstrate that it signiÔ¨Åcantly
improves the baseline ADMM algorithm and achieves high reconstruction accura-
cies with fast computational speed.

1 Introduction

Magnetic Resonance Imaging (MRI) is a non-invasive imaging technique providing both functional
and anatomical information for clinical diagnosis. Imaging speed is a fundamental challenge. Fast
MRI techniques are essentially demanded for accelerating data acquisition while still reconstructing
high quality image. Compressive sensing MRI (CS-MRI) is an effective approach allowing for data
sampling rate much lower than Nyquist rate without signiÔ¨Åcantly degrading the image quality [1].
CS-MRI methods Ô¨Årst sample data in k-space (i.e.  Fourier space)  then reconstruct image using
compressive sensing theory. Regularization related to the data prior is a key component in a CS-
MRI model to reduce imaging artifacts and improve imaging precision. Sparse regularization can be
explored in speciÔ¨Åc transform domain or general dictionary-based subspace [2]. Total Variation (TV)
regularization in gradient domain has been widely utilized in MRI [3  4]. Although it is easy and
fast to optimize  it introduces staircase artifacts in reconstructed image. Methods in [5  6] leverage
sparse regularization in the wavelet domain. Dictionary learning methods rely on a dictionary of
local patches to improve the reconstruction accuracy [7  8]. The non-local method uses groups of
similar local patches for joint patch-level reconstruction to better preserve image details [9  10  11].
In performance  the basic CS-MRI methods run fast but produce less accurate reconstruction results.
The non-local and dictionary learning-based methods generally output higher quality MR images 
but suffer from slow reconstruction speed.
In a CS-MRI model  it is commonly challenging to
choose an optimal image transform domain / subspace and the corresponding sparse regularization.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

To optimize the CS-MRI models  Alternating Direction Method of Multipliers (ADMM) has proven
to be an efÔ¨Åcient variable splitting algorithm with convergence guarantee [4  12  13]. It considers
the augmented Lagrangian function of a given CS-MRI model  and splits variables into subgroups 
which can be alternatively optimized by solving a few simply subproblems. Although ADMM is
generally efÔ¨Åcient  it is not trivial to determine the optimal parameters (e.g.  update rates  penalty
parameters) inÔ¨Çuencing accuracy in CS-MRI.
In this work  we aim to design a fast yet accurate method to reconstruct high-quality MR images
from under-sampled k-space data. We propose a novel deep architecture  dubbed ADMM-Net  in-
spired by the ADMM iterative procedures for optimizing a general CS-MRI model. This deep archi-
tecture consists of multiple stages  each of which corresponds to an iteration in ADMM algorithm.
More speciÔ¨Åcally  we deÔ¨Åne a deep architecture represented by a data Ô¨Çow graph [14] for ADMM
procedures. The operations in ADMM are represented as graph nodes  and the data Ô¨Çow between
two operations in ADMM is represented by a directed edge. Therefore  the ADMM iterative proce-
dures naturally determine a deep architecture over a data Ô¨Çow graph. Given an under-sampled data
in k-space  it Ô¨Çows over the graph and generates a reconstructed image. All the parameters (e.g. 
transforms  shrinkage functions  penalty parameters  etc.) in the deep architecture can be discrimi-
natively learned from training pairs of under-sampled data in k-space and reconstructed image using
fully sampled data by backpropagation [15] over the data Ô¨Çow graph.
Our experiments demonstrate that the proposed deep ADMM-Net is effective both in reconstruc-
tion accuracy and speed. Compared with the baseline methods using sparse regularization in trans-
form domain  it achieves signiÔ¨Åcantly higher accuracy and takes comparable computational time.
Compared with the state-of-the-art methods using dictionary learning and non-local techniques  it
achieves high accuracy in signiÔ¨Åcantly faster computational speed.
The main contributions of this paper can be summarized as follows. We propose a novel deep
ADMM-Net by reformulating an ADMM algorithm to a deep network for CS-MRI. This is achieved
by designing a data Ô¨Çow graph for ADMM to effectively build and train the ADMM-Net. ADMM-
Net achieves high accuracy in MR image reconstruction with fast computational speed justiÔ¨Åed in
experiments. The discriminative parameter learning approach has been applied to sparse coding
and Markov Random Filed [16  17  18  19]. But  to the best of our knowledge  this is the Ô¨Årst
computational framework that maps an ADMM algorithm to a learnable deep architecture.

2 Deep ADMM-Net for Fast MRI

2.1 Compressive Sensing MRI Model and ADMM Algorithm
General CS-MRI Model: Assume x 2 CN is an MRI image to be reconstructed  y 2 CN
<
N) is the under-sampled k-space data  according to the CS theory  the reconstructed image can be
estimated by solving the following optimization problem:

(N

‚Ä≤

‚Ä≤

}

L‚àë

l=1

{

L‚àë
L‚àë

^x = arg min

x

‚à•Ax (cid:0) y‚à•2

2 +

1
2

(cid:21)lg(Dlx)

;

(1)

‚Ä≤(cid:2)N is a measurement matrix  P 2 RN

where A = P F 2 RN
‚Ä≤(cid:2)N is a under-sampling matrix  and F
is a Fourier transform. Dl denotes a transform matrix for a Ô¨Åltering operation  e.g.  Discrete Wavelet
Transform (DWT)  Discrete Cosine Transform (DCT)  etc. g((cid:1)) is a regularization function derived
from the data prior  e.g.  lq-norm (0 (cid:20) q (cid:20) 1) for a sparse prior. (cid:21)l is a regularization parameter.
ADMM solver: [12] The above optimization problem can be solved efÔ¨Åciently using ADMM algo-
rithm. By introducing auxiliary variables z = fz1; z2;(cid:1)(cid:1)(cid:1) ; zLg  Eqn. (1) is equivalent to:

min
x;z

1
2

‚à•Ax (cid:0) y‚à•2

2 +

l=1
Its augmented Lagrangian function is :

L(cid:26)(x; z; (cid:11)) =

1
2

‚à•Ax (cid:0) y‚à•2

2 +

(cid:21)lg(zl)

s:t: zl = Dlx; 8 l 2 [1; 2;(cid:1)(cid:1)(cid:1) ; L]:

(2)

(cid:21)lg(zl) (cid:0) L‚àë

‚ü®(cid:11)l; zl (cid:0) Dlx‚ü© +

L‚àë

l=1

‚à•zl (cid:0) Dlx‚à•2
2;

(cid:26)l
2

(3)

l=1

l=1

2

2

l

(cid:26)l
2

l

(cid:26)l
2

1
2

L

l

l

l

x

z

(cid:11)

Figure 1: The data Ô¨Çow graph for the ADMM optimization of a general CS-MRI model. This graph
consists of four types of nodes: reconstruction (X)  convolution (C)  non-linear transform (Z)  and
multiplier update (M). An under-sampled data in k-space is successively processed over the graph 
and Ô¨Ånally generates a MR image. Our deep ADMM-Net is deÔ¨Åned over this data Ô¨Çow graph.
where (cid:11) = f(cid:11)lg are Lagrangian multipliers and (cid:26) = f(cid:26)lg are penalty parameters. ADMM alterna-
‚àë
tively optimizes fx; z; (cid:11)g by solving the following three subproblems:
‚àë
; z(n)
; zl (cid:0) Dlx(n+1)‚ü© +
‚ü©;

(cid:0) Dlx‚à•2
2;
‚à•zl (cid:0) Dlx(n+1)‚à•2
2;

(cid:11)(n+1) = arg min

(cid:0) Dlx‚ü© +

z(n+1) = arg min

‚à•z(n)

L
l=1

L
l=1

L
l=1

L
l=1

x(n+1) = arg min

8>>><>>>:
(cid:0)‚àë
‚àë
l=1 (cid:21)lg(zl) (cid:0)‚àë
‚à•Ax (cid:0) y‚à•2
‚àë
8><>:X(n) : x(n) = F T [P T P +

‚ü®(cid:11)(n)
‚ü®(cid:11)(n)
‚ü®(cid:11)l; Dlx(n+1) (cid:0) z(n+1)
‚àë
l = S(Dlx(n) + (cid:12)(n(cid:0)1)
l = (cid:12)(n(cid:0)1)

L
l=1 (cid:26)lF DT
; (cid:21)l=(cid:26)l);
+ (cid:17)l(Dlx(n) (cid:0) z(n)

Z(n) : z(n)
M(n) : (cid:12)(n)

l DlF T ]

L
l=1

(4)
(l 2 [1; 2;(cid:1)(cid:1)(cid:1) ; L])  and
l (z(n(cid:0)1)

(cid:0) (cid:12)(n(cid:0)1)

)];

where n 2 [1; 2;(cid:1)(cid:1)(cid:1) ; Ns] denotes n-th iteration. For simplicity  let (cid:12)l = (cid:11)l
substitute A = P F into Eqn. (4). Then the three subproblems have the following solutions:

(cid:26)l

(5)
where x(n) can be efÔ¨Åciently computed by fast Fourier transform  S((cid:1)) is a nonlinear shrinkage
function. It is usually a soft or hard thresholding function corresponding to the sparse regularization
of l1-norm and l0-norm respectively [20]. The parameter (cid:17)l is an update rate.
In CS-MRI  it commonly needs to run the ADMM algorithm in dozens of iterations to get a satis-
factory reconstruction result. However  it is challenging to choose the transform Dl and shrinkage
function S((cid:1)) for general regularization function g((cid:1)). Moreover  it is also not trivial to tune the
parameters (cid:26)l and (cid:17)l for k-space data with different sampling ratios. To overcome these difÔ¨Åcul-
ties  we will design a data Ô¨Çow graph for the ADMM algorithm  over which we can deÔ¨Åne a deep
ADMM-Net to discriminatively learn all the above transforms  functions  and parameters.

(cid:0)1[P T y +

L
l=1 (cid:26)lF DT

‚àë

l

l

l

l

);

l

2.2 Data Flow Graph for the ADMM Algorithm

To design our deep ADMM-Net  we Ô¨Årst map the ADMM iterative procedures in Eqn. (5) to a
data Ô¨Çow graph [14]. As shown in Fig. 1  this graph comprises of nodes corresponding to different
operations in ADMM  and directed edges corresponding to the data Ô¨Çows between operations. In
this case  the n-th iteration of ADMM algorithm corresponds to the n-th stage of the data Ô¨Çow graph.
In the n-th stage of the graph  there are four types of nodes mapped from four types of operations in
ADMM  i.e.  reconstruction operation (X(n))  convolution operation (C(n)) deÔ¨Åned by fDlx(n)gL
l=1 
nonlinear transform operation (Z(n)) deÔ¨Åned by S((cid:1))  and multiplier update operation (M(n)) in
Eqn. (5). The whole data Ô¨Çow graph is a multiple repetition of the above stages corresponding to
successive iterations in ADMM. Given an under-sampled data in k-space  it Ô¨Çows over the graph
and Ô¨Ånally generates a reconstructed image. In this way  we map the ADMM iterations to a data
Ô¨Çow graph  which is useful to deÔ¨Åne and train our deep ADMM-Net in the following sections.

2.3 Deep ADMM-Net

Our deep ADMM-Net is deÔ¨Åned over the data Ô¨Çow graph. It keeps the graph structure but generalizes
the four types of operations to have learnable parameters as network layers. These operations are
now generalized as reconstruction layer  convolution layer  non-linear transform layer  and multiplier
update layer. We next discuss them in details.

3

Sampling datain k-spaceReconstructedMR image stage n(1)X(n-1)C(n-1)Z(n)X(n-1)M(n)C(n)Z(n+1)X(n)M(n+1)C(n+1)Zs1(N)XÔÄ´(n+1)M(n-1)XL‚àë

l=1

Reconstruction layer (X(n)): This layer reconstructs an MRI image following the reconstruction

operation X(n) in Eqn. (5). Given z(n(cid:0)1)

and (cid:12)(n(cid:0)1)

L‚àë

l

l

  the output of this layer is deÔ¨Åned as:
(cid:0)(cid:12)(n(cid:0)1)

T (z(n(cid:0)1)

(cid:0)1[P T y +

l F T )

(cid:26)(n)
l F H (n)

T H (n)
)]; (6)
is the l-th penalty parameter  l = 1;(cid:1)(cid:1)(cid:1) ; L  and y is the input
are initialized to zeros 

and (cid:12)(0)

l=1

l

l

l

l

x(n) = F T (P T P +

(cid:26)(n)
l F H (n)

l

l

is the l-th Ô¨Ålter  (cid:26)(n)

where H (n)
under-sampled data in k-space. In the Ô¨Årst stage (n = 1)  z(0)
therefore x(1) = F T (P T P +

T H (1)

(cid:0)1(P T y):

l=1 (cid:26)(1)

l F H (1)

l F T )

L

l

l

l

‚àë

Convolution layer (C(n)): It performs convolution operation to transform an image into trans-

form domain. Given an image x(n)  i.e.  a reconstructed image in stage n  the output is

where D(n)
constrain the Ô¨Ålters D(n)

l

l

(7)
is a learnable Ô¨Ålter matrix in stage n. Different from the original ADMM  we do not

l x(n);

c(n)
l = D(n)

and H (n)

l

to be the same to increase the network capacity.

Nonlinear transform layer (Z(n)): This layer performs nonlinear transform inspired by the
shrinkage function S((cid:1)) deÔ¨Åned in Z(n) in Eqn. (5). Instead of setting it to be a shrinkage func-
tion determined by the regularization term g((cid:1)) in Eqn. (1)  we aim to learn more general function
using piecewise linear function. Given c(n)

l

and (cid:12)(n(cid:0)1)
l + (cid:12)(n(cid:0)1)

l

l

z(n)
l = SP LF (c(n)

  the output of this layer is deÔ¨Åned as:
;fpi; q(n)

gNc
i=1);

l;i

where SP LF ((cid:1)) is a piecewise linear function determined by a set of control points fpi; q(n)

(8)
gNc
i=1. i.e.

l;i

‚åã  fpigNc

gNc
where k = ‚åä a(cid:0)p1
p2(cid:0)p1
i=1
are the values at these positions for l-th Ô¨Ålter in n-th stage. Figure 2 gives an illustrative example.
Since a piecewise linear function can approximate any function  we can learn Ô¨Çexible nonlinear
transform function from data beyond the off-the-shelf hard or soft thresholding functions.

i=1 are predeÔ¨Åned positions uniformly located within [-1 1]  and fq(n)

l;i

SP LF (a;fpi; q(n)

l;i

gNc
i=1) =

8>><>>: a + q(n)

l;1
a + q(n)
l;Nc
q(n)
l;k +

(cid:0) p1;
(cid:0) pNc;
(a(cid:0)pk)(q(n)

a < p1 
a > pNc 

(9)

(cid:0)q(n)
l;k )

; p1 (cid:20) a (cid:20) pNc 

l;k+1

pk+1(cid:0)pk

Figure 2: Illustration of a piecewise linear function determined by a set of control points.

Multiplier update layer (M(n)): This layer is deÔ¨Åned by the Lagrangian multiplier updating

procedure M(n) in Eqn. (5). The output of this layer in stage n is deÔ¨Åned as:

l = (cid:12)(n(cid:0)1)
(cid:12)(n)
are learnable parameters.

l

+ (cid:17)(n)

l

(c(n)

l

(cid:0) z(n)

l

where (cid:17)(n)

l

);

(10)

l

in convolution layer  fq(n)

Network Parameters: These layers are organized in a data Ô¨Çow graph shown in Fig. 1. In the
deep architecture  we aim to learn the following parameters: H (n)
in reconstruction layer 
Ô¨Ålters D(n)
in multiplier update
layer  where l 2 [1; 2;(cid:1)(cid:1)(cid:1) ; L] and n 2 [1; 2;(cid:1)(cid:1)(cid:1) ; Ns] are the indexes for the Ô¨Ålters and stages
respectively. All of these parameters are taken as the network parameters to be learned.
Figure 3 shows an example of a deep ADMM-Net with three stages. The under-sampled data in
k-space Ô¨Çows over three stages in a order from circled number 1 to number 12  followed by a
Ô¨Ånal reconstruction layer with circled number 13 and generates a reconstructed image. Immediate
reconstruction result at each stage is shown under each reconstruction layer.

gNc
i=1 in nonlinear transform layer  (cid:17)(n)

and (cid:26)(n)

l;i

l

l

l

4

(ùëùùëñ ùëûùëô ùëñ(ùëõ))‚Ä¶‚Ä¶-11Figure 3: An example of deep ADMM-Net with three stages. The sampled data in k-space is
successively processed by operations in a order from 1 to 12  followed by a reconstruction layer
X (4) to output the Ô¨Ånal reconstructed image. The reconstructed image in each stage is shown under
each reconstruction layer.

3 Network Training

We take the reconstructed MR image using fully sampled data in k-space as the ground-truth MR
image xgt  and under-sampled data y in k-space as the input. Then a training set (cid:0) is constructed
containing pairs of under-sampled data and ground-truth MR image. We choose normalized mean
square error (NMSE) as the loss function in network training. Given pairs of training data  the loss
between the network output and ground truth is deÔ¨Åned as:

‚àë

‚àö
‚àö
‚à•^x(y; (cid:2)) (cid:0) xgt‚à•2
‚à•xgt‚à•2

2

2

E((cid:2)) =

1j(cid:0)j

(y;xgt)2(cid:0)

;

(11)

where ^x(y; (cid:2)) is the network output based on network parameter (cid:2) and under-sampled data y in k-
space. We learn the parameters (cid:2) = f(q(n)
g
; (cid:26)(Ns+1)
(l = 1;(cid:1)(cid:1)(cid:1) ; L) by minimizing the loss w.r.t. them using L-BFGS1. In the following  we Ô¨Årst discuss
the initialization of these parameters and then compute the gradients of the loss function E((cid:2)) w.r.t.
parameters (cid:2) using backpropagation (BP) [21] over the data Ô¨Çow graph.

[ fH (Ns+1)

i=1; D(n)

l;i )Nc

; H (n)

; (cid:17)(n)

; (cid:26)(n)

gNs

l

l

l

l

n=1

l

l

{

}

L‚àë

l=1

3.1

Initialization

We initialize the network parameters (cid:2) according to the ADMM solver of the following baseline
CS-MRI model:

arg min

x

‚à•Ax (cid:0) y‚à•2

2 + (cid:21)

1
2

jjDlxjj1

:

(12)

in convolution layers and H (n)

In this model  we set Dl as a DCT basis and impose l1-norm regularization in the DCT trans-
form space. The function S((cid:1)) in ADMM algorithm (Eqn. (5)) is a soft thresholding function:
S(t; (cid:21)=(cid:26)l) = sgn(t)(jtj (cid:0) (cid:21)=(cid:26)l) when jtj > (cid:21)=(cid:26)l  and 0 otherwise. For each n-th stage of deep
ADMM-Net  Ô¨Ålters D(n)
in reconstruction layers are initialized to be
Dl in Eqn. (12). In the nonlinear transform layer  we uniformly choose 101 positions located within
[-1 1]  and each value q(n)
are initialized to
l;i
be the corresponding values in the ADMM algorithm. In this case  the initialized net is exactly a
realization of ADMM optimizing Eqn. (12)  therefore outputs the same reconstructed image as the
ADMM algorithm. The optimization of the network parameters is expected to produce improved
reconstruction result.

is initialized as S(pi; (cid:21)=(cid:26)l). Parameters (cid:21); (cid:26)(n)

; (cid:17)(n)

l

l

l

l

3.2 Gradient Computation by Backpropagation over Data Flow Graph

It is challenging to compute the gradients of loss w.r.t. parameters using backpropagation over the
deep architecture in Fig. 1  because it is a directed graph. In the forward pass  we process the data
of n-th stage in the order of X(n); C(n); Z(n) and M(n). In the backward pass  the gradients are

1http://users.eecs.northwestern.edu/~nocedal/lbfgsb.html

5

‚ë†‚ë£(1)X‚ë°(1)C‚ë¢(1)Z‚ë§(2)X(1)M‚ëß‚ë•(2)C‚ë¶(2)Z‚ë®(3)X(2)M‚ë´‚ë©(3)C‚ë™(3)Z‚ë¨(4)X(3)MSampling datain k-spaceReconstructedMR image l

l

g. Its output f(cid:12)(n)

f(cid:12)(n(cid:0)1)
x(n+1). The parameters of this layer are (cid:17)(n)
ters can be computed as:
@E
@(cid:17)(n)
l
@E
@(cid:12)(n)
gradients of the output in this layer w.r.t. its inputs:

; where @E
@(cid:12)(n)

@(cid:12)(n)
@(cid:17)(n)

@E
@(cid:12)(n)

@(cid:12)(n+1)

@E

=

=

l

l

l

l

l

l

l

is the summation of gradients along the three dashed blue arrows in Fig. 4(a). We also compute

l

@(cid:12)(n+1)
@(cid:12)(n)

l

+

@E

@z(n+1)

l

@z(n+1)
l
@(cid:12)(n)

l

+

@E

@x(n+1)

@x(n+1)
@(cid:12)(n)

l

:

Figure 4: Illustration of four types of graph nodes (i.e.  layers in network) and their data Ô¨Çows in
stage n. The solid arrow indicates the data Ô¨Çow in forward pass and dashed arrow indicates the
backward pass when computing gradients in backpropagation.

computed in an inverse order. Figure 3 shows an example  where the gradient can be computed
backwardly from the layers with circled number 13 to 1 successively. For a stage n  Fig. 4 shows
four types of nodes (i.e.  network layers) and the data Ô¨Çow over them. Each node has multiple inputs
and (or) outputs. We next brieÔ¨Çy introduce the gradients computation for each layer in a typical
stage n (n < Ns). Please refer to supplementary material for details.

Multiplier update layer (M(n)): As shown in Fig. 4(a)  this layer has three sets of inputs:
g and
; l = 1;(cid:1)(cid:1)(cid:1) ; L. The gradients of loss w.r.t. the parame-

g is the input to compute f(cid:12)(n+1)

g and fz(n)

g;fz(n+1)

g;fc(n)

l

l

l

l

@(cid:12)(n)
@(cid:12)(n(cid:0)1)

l

l

  @(cid:12)(n)
@c(n)

l

l

  and @(cid:12)(n)
@z(n)

l

l

.

g;fc(n)

Nonlinear transform layer (Z(n)): As shown in Fig. 4(b)  this layer has two sets of inputs:
g is the input for computing f(cid:12)(n)
g and x(n+1) in next stage.
i=1; l = 1;(cid:1)(cid:1)(cid:1) ; L. The gradient of loss w.r.t. parameters can
gNc

f(cid:12)(n(cid:0)1)
g  and its output fz(n)
The parameters of this layers are fq(n)
be computed as

l;i

l

l

l

l

@E
@q(n)
l;i

@(cid:12)(n)
@z(n)
We also compute the gradients of layer output to its inputs: @z(n)
@(cid:12)(n)

; where @E
@z(n)

@z(n)
@q(n)
l;i

@E
@(cid:12)(n)

@E
@z(n)

=

=

l

l

l

l

l

l

l

+

@E

@x(n+1)

@x(n+1)
@z(n)

l

:

and @z(n)
@c(n)

l

.

l

l

Convolution layer (C(n)): The parameters of this layer are D(n)

the Ô¨Ålter by D(n)
coefÔ¨Åcients to be learned. The gradients of loss w.r.t. Ô¨Ålter coefÔ¨Åcients are computed as

l;mBm  where Bm is a basis element  and f!(n)

m=1 !(n)

l =

l;m

t

l

(l = 1;(cid:1)(cid:1)(cid:1) ; L). We represent
g is the set of Ô¨Ålter

‚àë

l

‚àë

@E
@!(n)
l;m

@E
@(cid:12)(n)
The gradient of layer output w.r.t. input is computed as @c(n)
@x(n) .

; where @E
@c(n)

@E
@c(n)

=

=

@c(n)
@!(n)
l;m

l

l

l

l

l

@(cid:12)(n)
@c(n)

l

+

@E
@z(n)

l

l

@z(n)
@c(n)

l

:

Reconstruction layer (X(n)): The parameters of this layer are H (n)

l;mBm  where f(cid:13)(n)
to convolution layer  we represent the Ô¨Ålter by H (n)
l;m
Ô¨Ålter coefÔ¨Åcients to be learned. The gradients of loss w.r.t. parameters are computed as

m=1 (cid:13)(n)

l =

; (cid:26)(n)

s

l

l

(l = 1;(cid:1)(cid:1)(cid:1) ; L). Similar
g is the set of

=

@E
@E
@x(n)
@(cid:13)(n)
l;m
; if n (cid:20) Ns;

@c(n)
@x(n)

where @E
@x(n)

=

@E
@c(n)

@x(n)
@(cid:13)(n)
l;m

;

@E
@x(n)

l

@E
@(cid:26)(n)
1j(cid:0)j

=

=

‚àö

@E
@x(n)

l

;

@x(n)
‚àö
@(cid:26)(n)
(x(n) (cid:0) xgt)
‚à•x(n) (cid:0) xgt‚à•2

2

‚à•xgt‚à•2

2

The gradients of layer output w.r.t. inputs are computed as @x(n)
@(cid:12)(n(cid:0)1)

l

and @x(n)
@z(n(cid:0)1)

l

.

6

; if n = Ns + 1:

()nlc(n)Z(1)nxÔÄ´(b) Non-linear transform layer()nlÔÅ¢(c) Convolution layer()nx(n)C()nlz()nlÔÅ¢()nlz(1)nlÔÅ¢ÔÄ≠(n)M(1)nlzÔÄ´(a) Multiplier update layer(1)nlÔÅ¢ÔÄ´(d) Reconstruction layer(n)X()nlc()nlc()nlÔÅ¢()nlÔÅ¢()nlÔÅ¢(1)nlÔÅ¢ÔÄ≠()nlz(1)nlzÔÄ≠()nlc()nlc()nx(1)nlÔÅ¢ÔÄ≠(1)nxÔÄ´()nlz4 Experiments

We train and test ADMM-Net on brain and chest MR images2. For each dataset  we randomly
take 100 images for training and 50 images for testing. ADMM-Net is separately learned for each
sampling ratio. The reconstruction accuracies are reported as the average NMSE and Peak Signal-
to-Noise Ratio (PSNR) over the test images. The sampling pattern in k-space is the commonly used
pseudo radial sampling. All experiments are performed on a desktop with Intel core i7-4790k CPU.

Table 1: Performance comparisons on brain data with different sampling ratios.

Method

Zero-Ô¨Ålling
TV [2]
RecPF [4]
SIDWT
PBDW [6]
PANO [10]
FDLCP [8]
BM3D-MRI [11]
Init-Net13
ADMM-Net13
ADMM-Net14
ADMM-Net15

20%

30%

40%

50%

NMSE
0.1700
0.0929
0.0917
0.0885
0.0814
0.0800
0.0759
0.0674
0.1394
0.0752
0.0742
0.0739

PSNR NMSE
0.1247
29.96
0.0673
35.20
35.32
0.0668
0.0620
35.66
0.0627
36.34
0.0592
36.52
0.0592
36.95
37.98
0.0515
0.1225
31.58
0.0553
37.01
0.0548
37.13
37.17
0.0544

PSNR NMSE
0.0968
32.59
0.0534
37.99
38.06
0.0533
0.0484
38.72
0.0518
38.64
0.0477
39.13
0.0500
39.13
40.33
0.0426
0.1128
32.71
0.0456
39.70
0.0448
39.78
39.84
0.0447

PSNR NMSE
0.0770
34.76
0.0440
40.00
40.03
0.0440
0.0393
40.88
0.0437
40.31
0.0390
41.01
0.0428
40.62
41.99
0.0359
0.1066
33.44
0.0395
41.37
0.0380
41.54
41.56
0.0379

PSNR
36.73
41.69
41.71
42.67
41.81
42.76
42.00
43.47
33.95
42.62
42.99
43.00

Test time

0.0013s
0.7391s
0.3105s
7.8637s
35.3637s
53.4776s
52.2220s
40.9114s
0.6914s
0.6964s
0.7400s
0.7911s

In Tab. 1  we compare our method to conventional compressive sensing MRI methods on brain data.
These methods include Zero-Ô¨Ålling [22]  TV [2]  RecPF [4]  SIDWT 3  and also the state-of-the-art
methods such as PBDW [6]  PANO [10]  FDLCP [8] and BM3D-MRI [11]. For ADMM-Net  we
initialize the Ô¨Ålters in each stage to be eight 3 (cid:2) 3 DCT basis (the average DCT basis is discarded).
Compared with the baseline methods such as Zero-Ô¨Ålling  TV  RecPF and SIDWT  our proposed
method produces the best quality with comparable reconstruction speed. Compared with the state-
of-the-art methods PBDW  PANO and FDLCP  our ADMM-Net has more accurate reconstruction
results with fastest computational speed. For the sampling ratio of 30%  our method (ADMM-
Net15) outperforms the state-of-the-art methods PANO and FDLCP by 0.71 db. Moreover  our
reconstruction speed is around 66 times faster. BM3D-MRI method relies on a well designed BM3D
denoiser  it produces higher accuracy  but runs around 50 times slower in computational time than
ours. The visual comparisons in Fig. 5 show that the proposed network can preserve the Ô¨Åne image
details without obvious artifacts. In Fig. 6(a)  we compare the NMSEs and the average test time for
different methods using scatter plot. It is easy to observe that our method is the best considering
the reconstruction accuracy and running time. Examples of the learned nonlinear functions and the
Ô¨Ålters are shown in Fig. 7.

Table 2: Comparisons of NMSE and PSNR on chest data with 20% sampling ratio.

Method
NMSE
PSNR

TV

0.1019
35.49

RecPF
0.1017
35.51

PANO FDLCP ADMM-Net15-B ADMM-Net15 ADMM-Net17
0.0858
37.01

0.0775
37.77

0.0790
37.68

0.0775
37.84

0.0768
37.92

Network generalization ability: We test the generalization ability of ADMM-Net by applying the
learned net from brain data to chest data. Table 2 shows that our net learned from brain data (ADMM-
Net15-B) still achieves competitive reconstruction accuracy on chest data  resulting in remarkable
a generalization ability. This might be due to that the learned Ô¨Ålters and nonlinear transforms are
performed over local patches  which are repetitive across different organs. Moreover  the ADMM-
Net17 learned from chest data achieves the better reconstruction accuracy on test chest data.
Effectiveness of network training: In Tab. 1  we also present the results of the initialized network
for ADMM-Net13. As discussed in Section 3.1  this initialized network (Init-Net13) is a realization

2CAF Project: https://masi.vuse.vanderbilt.edu/workshop2013/index.php/Segmentation_Challenge_Details
3Rice Wavelet Toolbox: http://dsp.rice.edu/software/rice-wavelet-toolbox

7

Figure 5: Examples of reconstruction results with 20% (the Ô¨Årst row) and 30% (the second row)
sampling ratios. The left four columns show results of ADMM-Net15  RecPF  PANO  BM3D-MRI.

Figure 6: (a) Scatter plot of NMSEs and average test time for different methods; (b) The NMSEs of
ADMM-Net using different number of stages (20% sampling ratio for brain data).

Figure 7: Examples of learned Ô¨Ålters in convolution layer and the corresponding nonlinear trans-
forms (the Ô¨Årst stage of ADMM-Net15 with 20% sampling ratio for brain data).

of the ADMM optimizing Eqn. (12). The network after training produces signiÔ¨Åcantly improved
accuracy  e.g.  PNSR is increased from 32.71 db to 39.84 db with sampling ratio of 30%.
Effect of the number of stages: To test the effect of the number of stages (i.e.  Ns)  we greedily train
deeper network by adding one stage at each time. Fig. 6(b) shows the average testing NMSE values
using different stages in ADMM-Net under the sampling ratio of 20%. The reconstruction error
decreases fast when Ns < 8 and marginally decreases when further increasing the number of stages.
Effect of the Ô¨Ålter sizes: We also train ADMM-Net initialized by two gradient Ô¨Ålters with size of 1(cid:2)3
and 3 (cid:2) 1 respectively for all convolution and reconstruction layers  the corresponding trained net
with 13 stages under 20% sampling ratio achieves NMSE value of 0.0899 and PSNR value of 36.52
db on brain data  compared with 0.0752 and 37.01 db using eight 3 (cid:2) 3 Ô¨Ålters as shown in Tab. 1.
We also learn ADMM-Net13 with 8 Ô¨Ålters sized 5 (cid:2) 5 initialized by DCT basis  the performance is
not signiÔ¨Åcantly improved  but the training and testing time are signiÔ¨Åcantly longer.

5 Conclusions

We proposed a novel deep network for compressive sensing MRI. It is a novel deep architecture de-
Ô¨Åned over a data Ô¨Çow graph determined by an ADMM algorithm. Due to its Ô¨Çexibility in parameter
learning  this deep net achieved high reconstruction accuracy while keeping the computational efÔ¨Å-
ciency of the ADMM algorithm. As a general framework  the idea that models an ADMM algorithm
as a deep network can be potentially applied to other applications in the future work.

8

NMSE:0.0564; PSNR:35.79NMSE:0.0727; PSNR:33.62NMSE:0.0489; PSNR:37.03NMSE:0.0612; PSNR:35.10NMSE:0.0660; PSNR:33.61NMSE:0.0843; PSNR:31.51NMSE:0.0726; PSNR:32.80NMSE:0.0614; PSNR:34.22Ground truth imageGround truth imageTest time in seconds(a)(b)NMSE15Stage numberReferences
[1] Michael Lustig  David L Donoho  Juan M Santos  and John M Pauly. Compressed sensing mri. IEEE

Journal of Signal Processing  25(2):72‚Äì82  2008.

[2] Michael Lustig  David Donoho  and John M Pauly. Sparse mri: The application of compressed sensing

for rapid mr imaging. Magnetic Resonance in Medicine  58(6):1182‚Äì1195  2007.

[3] Kai Tobias Block  Martin Uecker  and Jens Frahm. Undersampled radial mri with multiple coils: Iterative
image reconstruction using a total variation constraint. Magnetic Resonance in Medicine  57(6):1086‚Äì
1098  2007.

[4] Junfeng Yang  Yin Zhang  and Wotao Yin. A fast alternating direction method for tvl1-l2 signal recon-
struction from partial fourier data. IEEE Journal of Selected Topics in Signal Processing  4(2):288‚Äì297 
2010.

[5] Chen Chen and Junzhou Huang. Compressive sensing mri with wavelet tree sparsity. In Advances in

Neural Information Processing Systems  pages 1115‚Äì1123  2012.

[6] Xiaobo Qu  Di Guo  Bende Ning  and et al. Undersampled mri reconstruction with patch-based directional

wavelets. Magnetic resonance imaging  30(7):964‚Äì977  2012.

[7] Saiprasad Ravishankar and Yoram Bresler. Mr image reconstruction from highly undersampled k-space

data by dictionary learning. IEEE Transactions on Medical Imaging  30(5):1028‚Äì1041  2011.

[8] Zhifang Zhan  Jian-Feng Cai  Di Guo  Yunsong Liu  Zhong Chen  and Xiaobo Qu. Fast multi-class
dictionaries learning with geometrical directions in mri reconstruction. IEEE Transactions on Biomedical
Engineering  2016.

[9] Sheng Fang  Kui Ying  Li Zhao  and Jianping Cheng. Coherence regularization for sense reconstruction

with a nonlocal operator (cornol). Magnetic Resonance in Medicine  64(5):1413‚Äì1425  2010.

[10] Xiaobo Qu  Yingkun Hou  Fan Lam  Di Guo  Jianhui Zhong  and Zhong Chen. Magnetic resonance image
reconstruction from undersampled measurements using a patch-based nonlocal operator. Medical Image
Analysis  18(6):843‚Äì856  2014.

[11] Ender M Eksioglu. Decoupled algorithm for mri reconstruction using nonlocal block matching model:

Bm3d-mri. Journal of Mathematical Imaging and Vision  pages 1‚Äì11  2016.

[12] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundation and Trends in Machine
Learning  3(1):1‚Äì122  2011.

[13] Huahua Wang  Arindam Banerjee  and Zhi-Quan Luo. Parallel direction method of multipliers. In Ad-

vances in Neural Information Processing Systems  pages 181‚Äì189  2014.

[14] Krishna M Kavi  Bill P Buckles  and U Narayan Bhat. A formal deÔ¨Ånition of data Ô¨Çow graph models.

IEEE Transactions on Computers  100(11):940‚Äì948  1986.

[15] Yann L√©cun  Leon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to docu-

ment recognition. Proceedings of the IEEE  86(11):2278‚Äì2324  1998.

[16] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the

27th International Conference on Machine Learning  pages 399‚Äì406  2010.

[17] Uwe Schmidt and Stefan Roth. Shrinkage Ô¨Åelds for effective image restoration. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  pages 2774‚Äì2781  2014.

[18] Sun Jian and Xu Zongben. Color image denoising via discriminatively learned iterative shrinkage. IEEE

Transactions on Image Processing  24(11):4148‚Äì4159  2015.

[19] John R Hershey  Jonathan Le Roux  and Felix Weninger. Deep unfolding: Model-based inspiration of

novel deep architectures. arXiv preprint arXiv:1409.2574  2014.

[20] Francis Bach  Rodolphe Jenatton  Julien Mairal  and Guillaume Obozinski. Optimization with sparsity-

inducing penalties. Foundations and Trends in Machine Learning  4(1):1‚Äì106  2012.

[21] David E Rumelhart  Geoffrey E Hinton  and Ronald J Williams. Learning representations by back-

propagating errors. Cognitive modeling  5(3):1  1988.

[22] Matt A Bernstein  Sean B Fain  and Stephen J Riederer. Effect of windowing and zero-Ô¨Ålled reconstruction
of mri data on spatial resolution and acquisition strategy. Magnetic Resonance Imaging  14(3):270‚Äì280 
2001.

9

,yan yang
Jian Sun
Huibin Li