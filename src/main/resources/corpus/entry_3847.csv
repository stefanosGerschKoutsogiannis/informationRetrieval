2012,Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison,Both random Fourier features and the Nyström method have been successfully applied to efficient kernel learning. In this work  we investigate the fundamental difference between these two approaches  and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features  where the basis functions (i.e.  cosine and sine functions) are sampled from a distribution  {\it independent} from the training data  basis functions used by the Nyström method are randomly sampled from the training examples and are therefore {\it data dependent}. By exploring this difference  we show that when there is a large gap in the eigen-spectrum of the kernel matrix  approaches based the Nyström method can yield  impressively  better generalization error bound than random Fourier features based approach. We empirically verify our theoretical findings on a wide range of large data sets.,Nystr¨om Method vs Random Fourier Features:

A Theoretical and Empirical Comparison

Tianbao Yang†  Yu-Feng Li‡  Mehrdad Mahdavi(cid:92)  Rong Jin(cid:92)  Zhi-Hua Zhou‡

†Machine Learning Lab  GE Global Research  San Ramon  CA 94583

‡National Key Laboratory for Novel Software Technology  Nanjing University  210023  China
tyang@ge.com mahdavim rongjin@msu.edu liyf zhouzh@lamda.nju.edu.cn

(cid:92)Michigan State University  East Lansing  MI 48824

Abstract

Both random Fourier features and the Nystr¨om method have been successfully
applied to efﬁcient kernel learning. In this work  we investigate the fundamental
difference between these two approaches  and how the difference could affect
their generalization performances. Unlike approaches based on random Fourier
features where the basis functions (i.e.  cosine and sine functions) are sampled
from a distribution independent from the training data  basis functions used by
the Nystr¨om method are randomly sampled from the training examples and are
therefore data dependent. By exploring this difference  we show that when there
is a large gap in the eigen-spectrum of the kernel matrix  approaches based on
the Nystr¨om method can yield impressively better generalization error bound than
random Fourier features based approach. We empirically verify our theoretical
ﬁndings on a wide range of large data sets.

1

Introduction

Kernel methods [16]  such as support vector machines  are among the most effective learning meth-
ods. These methods project data points into a high-dimensional or even inﬁnite-dimensional feature
space and ﬁnd the optimal hyperplane in that feature space with strong generalization performance.
One limitation of kernel methods is their high computational cost  which is at least quadratic in the
number of training examples  due to the calculation of kernel matrix. Although low rank decom-
position approaches (e.g.  incomplete Cholesky decomposition [3]) have been used to alleviate the
computational challenge of kernel methods  they still require computing the kernel matrix. Other ap-
proaches such as online learning [9] and budget learning [7] have also been developed for large-scale
kernel learning  but they tend to yield performance worse performance than batch learning.
To avoid computing kernel matrix  one common approach is to approximate a kernel learning prob-
lem with a linear prediction problem. It is often achieved by generating a vector representation of
data that approximates the kernel similarity between any two data points. The most well known
approaches in this category are random Fourier features [13  14] and the Nystr¨om method [20  8].
Although both approaches have been found effective  it is not clear what are their essential dif-
ference  and which method is preferable under which situations. The objective of this work is to
understand the difference between these two approaches  both theoretically and empirically
The theoretical foundation for random Fourier transform is that a shift-invariant kernel is the Fourier
transform of a non-negative measure [15]. Using this property  in [13]  the authors proposed to
represent each data point by random Fourier features. Analysis in [14] shows that  the generalization
error bound for kernel learning based on random Fourier features is given by O(N−1/2 + m−1/2) 
where N is the number of training examples and m is the number of sampled Fourier components.

1

An alternative approach for large-scale kernel classiﬁcation is the Nystr¨om method [20  8] that
approximates the kernel matrix by a low rank matrix.
It randomly samples a subset of training

examples and computes a kernel matrix (cid:98)K for the random samples. It then represents each data
(cid:98)K. Most analysis of the Nystr¨om method follows [8] and bounds the error in approximating the

point by a vector based on its kernel similarity to the random samples and the sampled kernel matrix

kernel matrix. According to [8]  the approximation error of the Nystr¨om method  measured in
spectral norm 1  is O(m−1/2)  where m is the number of sampled training examples. Using the
arguments in [6]  we expected an additional error of O(m−1/2) in the generalization performance
caused by the approximation of the Nystr¨om method  similar to random Fourier features.

Contributions
In this work  we ﬁrst establish a uniﬁed framework for both methods from the
viewpoint of functional approximation. This is important because random Fourier features and the
Nystr¨om method address large-scale kernel learning very differently: random Fourier features aim
to approximate the kernel function directly while the Nystr¨om method is designed to approximate
the kernel matrix. The uniﬁed framework allows us to see a fundamental difference between the
two methods: the basis functions used by random Fourier features are randomly sampled from a
distribution independent from the training data  leading to a data independent vector representation;
in contrast  the Nystr¨om method randomly selects a subset of training examples to form its basis
functions  leading to a data dependent vector representation. By exploring this difference  we show
that the additional error caused by the Nystr¨om method in the generalization performance can be
improved to O(1/m) when there is a large gap in the eigen-spectrum of the kernel matrix. Empirical
studies on a synthetic data set and a broad range of real data sets verify our analysis.

2 A Uniﬁed Framework for Approximate Large-Scale Kernel Learning
Let D = {(x1  y1)  . . .   (xN   yN )} be a collection of N training examples  where xi ∈ X ⊆ Rd 
yi ∈ Y. Let κ(· ·) be a kernel function  Hκ denote the endowed Reproducing Kernel Hilbert Space 
and K = [κ(xi  xj)]N×N be the kernel matrix for the samples in D. Without loss of generality 
we assume κ(x  x) ≤ 1 ∀x ∈ X . Let (λi  vi)  i = 1  . . .   N be the eigenvalues and eigenvectors
of K ranked in the descending order of eigenvalues. Let V = [Vij]N×N = (v1  . . .   vN ) denote

the eigenvector matrix. For the Nystr¨om method  let (cid:98)D = {(cid:98)x1  . . .  (cid:98)xm} denote the randomly
sampled examples  (cid:98)K = [κ((cid:98)xi (cid:98)xj)]m×m denote the corresponding kernel matrix. Similarly  let
{((cid:98)λi (cid:98)vi)  i ∈ [m]} denote the eigenpairs of (cid:98)K ranked in the descending order of eigenvalues  and
(cid:98)V = [(cid:98)Vij]m×m = ((cid:98)v1  . . .  (cid:98)vm). We introduce two linear operators induced by examples in D and
(cid:98)D  i.e. 

LN [f ] =

κ(xi ·)f (xi)  Lm[f ] =

(1)

It can be shown that both LN and Lm are self-adjoint operators. According to [18]  the eigenval-

ues of LN and Lm are λi/N  i ∈ [N ] and(cid:98)λi/m  i ∈ [m]  respectively  and their corresponding
normalized eigenfunctions ϕj  j ∈ [N ] and (cid:98)ϕj  j ∈ [m] are given by
m(cid:88)
1(cid:113)(cid:98)λj

(cid:98)Vi jκ((cid:98)xi ·)  j ∈ [m].

Vi jκ(xi ·)  j ∈ [N ] 

(cid:98)ϕj(·) =

1(cid:112)λj

N(cid:88)

i=1

ϕj(·) =

(2)

i=1

To make our discussion concrete  we focus on the RBF kernel 2  i.e.  κ(x  ¯x) = exp(−(cid:107)x −
¯x(cid:107)2
2/[2σ2])  whose inverse Fourier transform is given by a Gaussian distribution p(u) =
N (0  σ−2I) [15]. Our goal is to efﬁciently learn a kernel prediction function by solving the fol-
lowing optimization problem:

N(cid:88)

i=1

1
N

m(cid:88)

i=1

1
m

κ((cid:98)xi ·)f ((cid:98)xi).

N(cid:88)

i=1

min
f∈HD

(cid:107)f(cid:107)2Hκ

+

λ
2

1
N

(cid:96)(f (xi)  yi) 

(3)

1We choose the bound based on spectral norm according to the discussion in [6].
2 The improved bound obtained in the paper for the Nystrom method is valid for any kernel matrix that

satisﬁes the eigengap condition.

2

where HD = span(κ(x1 ·)  . . .   κ(xN  ·)) is a span over all the training examples 3  and (cid:96)(z  y) is
a convex loss function with respect to z. To facilitate our analysis  we assume maxy∈Y (cid:96)(0  y) ≤ 1
and (cid:96)(z  y) has a bounded gradient |∇z(cid:96)(z  y)| ≤ C. The high computational cost of kernel learning
arises from the fact that we have to search for an optimal classiﬁer f (·) in a large space HD.
Given this observation  to alleviate the computational cost of kernel classiﬁcation  we can reduce
space HD to a smaller space Ha  and only search for the solution f (·) ∈ Ha. The main challenge is
how to construct such a space Ha. On the one hand  Ha should be small enough to make it possible
to perform efﬁcient computation; on the other hand  Ha should be rich enough to provide good ap-
proximation for most bounded functions in HD. Below we show that the difference between random
Fourier features and the Nystr¨om method lies in the construction of the approximate space Ha. For
each method  we begin with a description of a vector representation of data  and then connect the
vector representation to the approximate large kernel machine by functional approximation.

Random Fourier Features The random Fourier
features are constructed by ﬁrst sam-
pling Fourier components u1  . . .   um from p(u)  projecting each example x to u1  . . .   um
separately 
i.e.  zf (x) =
(sin(u(cid:62)
mx)). Given the random Fourier features  we then
learn a linear machine f (x) = w(cid:62)zf (x) by solving the following optimization problem:

and then passing them through sine and cosine functions 

1 x)  . . .   sin(u(cid:62)

mx)  cos(u(cid:62)

1 x)  cos(u(cid:62)

min
w∈R2m

(cid:107)w(cid:107)2

2 +

λ
2

1
N

(cid:96)(w(cid:62)zf (xi)  yi).

(4)

To connect the linear machine (4) to the kernel machine in (3) by a functional approximation  we can
construct a functional space Hf
k x)
and ck(x) = cos(u(cid:62)

a = span(s1(·)  c1(·)  . . .   sm(·)  cm(·))  where sk(x) = sin(u(cid:62)

k x). If we approximate HD in (3) by Hf

a  we have

(cid:107)f(cid:107)2Hκ

+

λ
2

1
N

min
f∈Hf
a

(cid:96)(f (xi)  yi).

(5)

The following proposition connects the approximate kernel machine in (5) to the linear machine
in (4). Proofs can be found in supplementary ﬁle.

Proposition 1 The approximate kernel machine in (5) is equivalent to the following linear machine

min
w∈R2m

1
N

w(cid:62)(w ◦ γ) +

λ
2
m)(cid:62) and γs/c

where γ = (γs

1 ···   γs

1  γc

m  γc

i = exp(σ2(cid:107)ui(cid:107)2

2/2).

(cid:96)(w(cid:62)zf (xi)  yi) 

(6)

Comparing (6) to the linear machine based on random Fourier features in (4)  we can see that other
than the weights {γs/c
i=1  random Fourier features can be viewed as to approximate (3) by re-
stricting the solution f (·) to Hf
a.

i }m

The Nystr¨om Method The Nystr¨om method approximates the full kernel matrix K by ﬁrst sam-

pling m examples  denoted by (cid:98)x1 ···  (cid:98)xm  and then constructing a low rank matrix by (cid:98)Kr =
Kb(cid:98)K†K(cid:62)
b   where Kb = [κ(xi (cid:98)xj)]N×m  (cid:98)K = [κ((cid:98)xi (cid:98)xj)]m×m  (cid:98)K† is the pseudo inverse of (cid:98)K 
and r denotes the rank of (cid:98)K. In order to train a linear machine  we can derive a vector representa-
  where (cid:98)Dr = diag((cid:98)λ1  . . .  (cid:98)λr) and
tion of data by zn(x) = (cid:98)D
(cid:98)Vr = ((cid:98)v1  . . .  (cid:98)vr). It is straightforward to verify that zn(xi)(cid:62)zn(xj) = [(cid:98)Kr]ij. Given the vector

(cid:98)V (cid:62)
r (κ(x (cid:98)x1)  . . .   κ(x (cid:98)xm))

representation zn(x)  we then learn a linear machine f (x) = w(cid:62)zn(x) by solving the following
optimization problem:

−1/2
r

(cid:62)

min
w∈Rr

(cid:107)w(cid:107)2

2 +

λ
2

1
N

(cid:96)(w(cid:62)zn(xi)  yi).

(7)

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)

i=1

3We use HD  instead of Hκ in (3)  owing to the representer theorem [16].

3

a = span((cid:98)ϕ1  . . .  (cid:98)ϕr)  where(cid:98)ϕ1  . . .  (cid:98)ϕr are the ﬁrst r normalized eigenfunctions of the operator

In order to see how the Nystr¨om method can be cast into the uniﬁed framework of approximating the
large scale kernel machine by functional approximation  we construct the following functional space
Hn
Lm. The following proposition shows that the linear machine in (7) using the vector representation
of the Nystr¨om method is equivalent to the approximate kernel machine in (3) by restricting the
solution f (·) to an approximate functional space Hn
a.
Proposition 2 The linear machine in (7) is equivalent to the following approximate kernel machine

min
f∈Hn

a

(cid:107)f(cid:107)2Hκ

+

λ
2

1
N

N(cid:88)

i=1

(cid:96)(f (xi)  yi) 

(8)

Although both random Fourier features and the Nystr¨om method can be viewed as variants of the
uniﬁed framework  they differ signiﬁcantly in the construction of the approximate functional space
Ha. In particular  the basis functions used by random Fourier features are sampled from a Gaussian
distribution that is independent from the training examples. In contrast  the basis functions used by
the Nystr¨om method are sampled from the training examples and are therefore data dependent.
This difference  although subtle  can have signiﬁcant impact on the classiﬁcation performance. In
the case of large eigengap  i.e.  the ﬁrst few eigenvalues of the full kernel matrix are much larger than
the remaining eigenvalues  the classiﬁcation performance is mostly determined by the top eigenvec-
tors. Since the Nystr¨om method uses a data dependent sampling method  it is able to discover the
subspace spanned by the top eigenvectors using a small number of samples. In contrast  since ran-
dom Fourier features are drawn from a distribution independent from training data  it may require a
large number of samples before it can discover this subspace. As a result  we expect a signiﬁcantly
lower generalization error for the Nystr¨om method.
To illustrate this point  we generate a synthetic data set consisted of two balanced classes with a
total of N = 10  000 data points generated from uniform distributions in two balls of radius 0.5
centered at (−0.5  0.5) and (0.5  0.5)  respectively. The σ value in the RBF kernel is chosen by
cross-validation and is set to 6 for the synthetic data. To avoid a trivial task  100 redundant features 
each drawn from a uniform distribution on the unit interval  are added to each example. The data
points in the ﬁrst two dimensions are plotted in Figure 1(a) 4  and the eigenvalue distribution is
shown in Figure 1(b). According to the results shown in Figure 1(c)  it is clear that the Nystr¨om
method performs signiﬁcantly better than random Fourier features. By using only 100 samples  the
Nystr¨om method is able to make perfect prediction  while the decision made by random Fourier fea-
tures based method is close to random guess. To evaluate the approximation error of the functional
space  we plot in Figure 1(e) and 1(f)  respectively  the ﬁrst two eigenvectors of the approximate
kernel matrix computed by the Nystr¨om method and random Fourier features using 100 samples.
Compared to the eigenvectors computed from the full kernel matrix (Figure 1(d))  we can see that
the Nystr¨om method achieves a signiﬁcantly better approximation of the ﬁrst two eigenvectors than
random Fourier features.
Finally  we note that although the concept of eigengap has been exploited in many studies of kernel
learning [2  12  1  17]  to the best of our knowledge  this is the ﬁrst time it has been incorporated in
the analysis for approximate large-scale kernel learning.

3 Main Theoretical Result
Let f∗
solution to the full version of kernel learning in (3). Let f∗ be the optimal solution to

m be the optimal solution to the approximate kernel learning problem in (8)  and let f∗

N be the

(cid:18)

(cid:19)

min
f∈Hκ

F (f ) =

+ E [(cid:96)(f (x)  y)]

 

(cid:107)f(cid:107)2Hκ

λ
2

where E[·] takes expectation over the joint distribution P (x  y). Following [10]  we deﬁne the excess
risk of any classiﬁer f ∈ Hκ as

Λ(f ) = F (f ) − F (f∗).
4Note that the scales of the two axes in Figure 1(a) are different.

(9)

4

(a) Synthetic data: the ﬁrst two
dimensions

(b) Eigenvalues (in logarith-
mic scale) vs. rank. N is the
total number of data points.

(c) Classiﬁcation accuracy vs
the number of samples

(d) the ﬁrst two eigenvectors of the
full kernel matrix

(e) the ﬁrst two eigenvectors com-
puted by Nystr¨om method

(f) the ﬁrst two eigenvectors com-
puted by random Fourier features

Figure 1: An Illustration Example

N

(cid:17)1/2

i=1 min(δ2  λi)

Unlike [6]  in this work  we aim to bound the generalization performance of f∗
performance of f∗
In order to obtain a tight bound  we exploit the local Rademacher complexity [10]. Deﬁne ψ(δ) =

N   which better reﬂects the impact of approximating HD by Hn
a.

m by the generalization

(cid:16) 2
(cid:80)N
of(cid:101)ε are determined by the sub-root property of ψ(δ) [4]  and  = max

. Let(cid:101)ε as the solution to(cid:101)ε2 = ψ((cid:101)ε) where the existence and uniqueness

. According
to [10]  we have 2 = O(N−1/2)  and when the eigenvalues of kernel function follow a p-power law 
it is improved to 2 = O(N−p/(p+1)). The following theorem bounds Λ(f∗
N ). Section 4
will be devoted to the proof of this theorem.
Theorem 1 For 162e−2N ≤ λ ≤ 1  λr+1 = O(N/m) and
2 ln(2N 3)

m) by Λ(f∗

(cid:18)(cid:101)ε 

(cid:113) 6 ln N

N

(cid:19)

2 ln(2N 3)

(cid:114)

(cid:32)

(cid:33)

 

(λr − λr+1)/N = Ω(1) ≥ 3

with a probability 1 − 3N−3  we have

m) ≤ 3Λ(f∗
where (cid:101)O(·) suppresses the polynomial term of ln N.

Λ(f∗

N ) +

+

(cid:18)

2 +

m

(cid:101)O

1
λ

m

(cid:19)

 

1
m

√

Theorem 1 shows that the additional error caused by the approximation of the Nystr¨om method is
improved to O(1/m) when there is a large gap between λr and λr+1. Note that the improvement
m) to O(1/m) is very signiﬁcant from the theoretical viewpoint  because it is well
from O(1/
known that the generalization error for kernel learning is O(N−1/2) [4]5. As a result  to achieve
a similar performance as the standard kernel learning  the number of required samples has to be
5It is possible to achieve a better generalization error bound of O(N−p/(p+1)) by assuming the eigenvalues
of kernel matrix follow a p-power law [10]. However  large eigengap doest not immediately indicate power law
distribution for eigenvalues and and consequently a better generalization error.

5

−1−0.500.5100.10.20.30.40.50.60.70.80.911st dimension2nd dimension0.2N0.4N0.6N0.8N  N10−510−410−310−210−1100rankEigenvalues/NSynthetic data  5  10  20  50 100405060708090100# random samplesaccuaracySynthetic data  Nystrom MethodRandom Fourier Features02000400060008000100000.00950.010.0105Eigenvector 10200040006000800010000−0.02−0.0100.010.02Eigenvector 202000400060008000100000.00950.010.0105Eigenvector 10200040006000800010000−0.04−0.0200.020.04Eigenvector 20200040006000800010000−0.04−0.0200.020.04Eigenvector 10200040006000800010000−0.0500.05Eigenvector 2O(N ) if the additional error caused by the kernel approximation is bounded by O(1/
m)  leading
to a high computational cost. On the other hand  with O(1/m) bound for the additional error caused
by the kernel approximation  the number of required samples is reduced to
N  making it more
practical for large-scale kernel learning.
We also note that the improvement made for the Nystr¨om method relies on the property that Hn
a ⊂
HD and therefore requires data dependent basis functions. As a result  it does not carry over to
random Fourier features.

√

√

4 Analysis

In this section  we present the analysis that leads to Theorem 1. Most of the proofs can be found in
the supplementary materials. We ﬁrst present a theorem to show that the excessive risk bound of f∗
m

is related to the matrix approximation error (cid:107)K − (cid:98)Kr(cid:107)2.

Theorem 2 For 162e−2N ≤ λ ≤ 1  with a probability 1 − 2N−3  we have

Λ(f∗

m) ≤ 3Λ(f∗

N ) + C2

(cid:32)

(cid:107)K − (cid:98)Kr(cid:107)2

N λ

2
λ

+

(cid:33)

+ e−N

 

as

where C2 is a numerical constant.

In the sequel  we let Kr be the best rank-r approximation matrix for K. By the triangle inequality 

(cid:107)K − (cid:98)Kr(cid:107)2 ≤ (cid:107)K − Kr(cid:107)2 + (cid:107)Kr − (cid:98)Kr(cid:107)2 ≤ λr+1 + (cid:107)Kr − (cid:98)Kr(cid:107)2  we thus proceed to bound
(cid:107)Kr − (cid:98)Kr(cid:107)2. Using the eigenfunctions of Lm and LN   we deﬁne two linear operators Hr and (cid:98)Hr

r(cid:88)

(cid:98)Hr[f ](·) =

r(cid:88)

(cid:98)ϕi(·)(cid:104)(cid:98)ϕi  f(cid:105)Hκ  

Hr[f ](·) =

ϕi(·)(cid:104)ϕi  f(cid:105)Hκ  

i=1

where f ∈ Hκ. The following theorem shows that (cid:107)Kr − (cid:98)Kr(cid:107)2 is related to the linear operator
∆H = Hr − (cid:98)Hr.
Theorem 3 For(cid:98)λr > 0 and λr > 0  we have

i=1

N (cid:107)2 
N ∆HL1/2
where (cid:107)L(cid:107)2 stands for the spectral norm of a linear operator L.

(cid:107)(cid:98)Kr − Kr(cid:107)2 ≤ N(cid:107)L1/2

(10)

N ∆HL1/2

N (cid:107)2 using matrix perturbation theory [19].

Given the result in Theorem 3  we move to bound the spectral norm of L1/2
N . To this
end  we assume a sufﬁciently large eigengap ∆ = (λr − λr+1)/N. The theorem below bounds
(cid:107)L1/2
Theorem 4 For ∆ = (λr − λr+1)/N > 3(cid:107)LN − Lm(cid:107)HS  we have
4(cid:107)LN − Lm(cid:107)HS
∆ − (cid:107)LN − Lm(cid:107)HS

N ∆HL1/2

(cid:107)L1/2

 

(cid:32)(cid:114)

N (cid:107)2 ≤ η
N ∆HL1/2
2(cid:107)LN − Lm(cid:107)HS
∆ − (cid:107)LN − Lm(cid:107)HS

(cid:33)

.

λr+1

N

 

where η = max

Remark To utilize the result in Theorem 4  we consider the case when λr+1 = O(N/m) and
∆ = Ω(1). We have

(cid:107)L1/2

N ∆HL1/2

N (cid:107)2 ≤ O

max

(cid:107)LN − Lm(cid:107)HS (cid:107)LN − Lm(cid:107)2

HS

Obviously  in order to achieve O(1/m) bound for (cid:107)L1/2
for (cid:107)LN − Lm(cid:107)HS  which is given by the following theorem.

N ∆HL1/2

√
N (cid:107)2  we need an O(1/

m) bound

(cid:18)

(cid:20) 1√

m

(cid:21)(cid:19)

.

6

(cid:107)LN − Lm(cid:107)HS ≤ 2 ln(2N 3)

+

m

(cid:114)

2 ln(2N 3)

.

m

Theorem 5 For κ(x  x) ≤ 1 ∀x ∈ X   with a probability 1 − N−3  we have

Theorem 5 directly follows from Lemma 2 of [18]. Therefore  by assuming the conditions in The-

orem 1 and combining results from Theorems 3  4  and 5  we immediately have (cid:107)K − (cid:98)Kr(cid:107)2 ≤
m + e−N(cid:1). We complete the proof of

O (N/m). Combining this bound with the result in Theorem 2 and using the union bound  we have 
with a probability 1 − 3N−3  Λ(f∗
Theorem 1 by using the fact e−N < 1/N ≤ 1/m.

m) ≤ 3Λ(f∗

N ) + C
λ

(cid:0)2 + 1

5 Empirical Studies

To verify our theoretical ﬁndings  we evaluate the empirical performance of the Nystr¨om method
and random Fourier features for large-scale kernel learning. Table 1 summarizes the statistics of the
six data sets used in our study  including two for regression and four for classiﬁcation. Note that
datasets CPU  CENSUS  ADULT and FOREST were originally used in [13] to verify the effective-
ness of random Fourier features. We evaluate the classiﬁcation performance by accuracy  and the
performance of regression by mean square error of the testing data.
We use uniform sampling in the Nystr¨om method owing to its simplicity. We note that the empirical
performance of the Nystr¨om method may be improved by using a different implementation [21 
11]. We download the codes from the website http://berkeley.intel-research.net/
arahimi/c/random-features for the implementation of random Fourier features. A RBF
kernel is used for both methods and for all the datasets. A ridge regression package from [13] is used
for the two regression tasks  and LIBSVM [5] is used for the classiﬁcation tasks. All parameters
are selected by a 5-fold cross validation. All experiments are repeated ten times  and prediction
performance averaged over ten trials is reported.
Figure 2 shows the performance of both methods with varied number of random samples. Note
that for large datasets (i.e.  COVTYPE and FOREST)  we restrict the maximum number of random
samples to 200 because of the high computational cost. We observed that for all the data sets  the
Nystr¨om method outperforms random Fourier features 6. Moreover  except for COVTYPE with 10
random samples  the Nystr¨om method performs signiﬁcantly better than random Fourier features 
according to t-tests at 95% signiﬁcance level. We ﬁnally evaluate that whether the large eigengap
condition  the key assumption for our main theoretical result  holds for the data sets. Due to the
large size  except for CPU  we compute the eigenvalues of kernel matrix based on 10  000 randomly
selected examples from each dataset. As shown in Figure 3 (eigenvalues are in logarithm scale) 
we observe that the eigenvalues drop very quickly as the rank increases  leading to a signiﬁcant gap
between the top eigenvalues and the remaining eigenvalues.

6 Conclusion and Discussion

We study two methods for large-scale kernel learning  i.e.  the Nystr¨om method and random Fourier
features. One key difference between these two approaches is that the Nystr¨om method uses data

6We note that the classiﬁcation performance of ADULT data set reported in Figure 2 does not match with
the performance reported in [13]. Given the fact that we use the code provided by [13] and follow the same
cross validation procedure  we believe our result is correct. We did not use the KDDCup dataset because of the
problem of oversampling  as pointed out in [13].

Table 1: Statistics of data Sets

TASK
Reg.
Reg.
Class.

DATA # TRAIN # TEST
819
CPU
2 273
16 281

6 554
CENSUS 18 186
ADULT 32 561

#Attr.

TASK
21 Class.
119 Class.
123 Class.

DATA # TRAIN
COD-RNA 59 535
COVTYPE 464 810
FOREST 522 910

# TEST
271 617
116 202
58 102

#Attr.
8
54
54

7

Figure 2: Comparison of the Nymstr¨om method and random Fourier features. For regression tasks 
the mean square error (with std.) is reported  and for classiﬁcation tasks  accuracy (with std.) is
reported.

Figure 3: The eigenvalue distributions of kernel matrices. N is the number of examples used to
compute eigenvalues.

dependent basis functions while random Fourier features introduce data independent basis functions.
This difference leads to an improved analysis for kernel learning approaches based on the Nystr¨om
method. We show that when there is a large eigengap of kernel matrix  the approximation error
of Nystr¨om method can be improved to O(1/m)  leading to a signiﬁcantly better generalization
performance than random Fourier features. We verify the claim by an empirical study.
As implied from our study  it is important to develop data dependent basis functions for large-scale
kernel learning. One direction we plan to explore is to improve random Fourier features by making
the sampling data dependent. This can be achieved by introducing a rejection procedure that rejects
the sample Fourier components when they do not align well with the top eigenfunctions estimated
from the sampled data.

Acknowledgments

This work was partially supported by ONR Award N00014-09-1-0663  NSF IIS-0643494  NSFC
(61073097) and 973 Program (2010CB327903).

8

  10  20  50 100 200500100000.511.522.5# random samplesmean square errorCPU  Nystrom MethodRandom Fourier Features  10  20  50 100 200500100000.511.522.53# random samplesmean square errorCENSUS  Nystrom MethodRandom Fourier Features  10  20  50 100 200500100030405060708090# random samplesaccuracy(%)ADULT  Nystrom MethodRandom Fourier Features  10  20  50 100 200 500405060708090100# random samplesaccuracy(%)COD_RNA  Nystrom MethodRandom Fourier Features  10  20  50 100 200556065707580# random samplesaccuracy(%)COVTYPE  Nystrom MethodRandom Fourier Features  10  20  50 100 200556065707580# random samplesaccuracy(%)FOREST  Nystrom MethodRandom Fourier Features0.2N0.4N0.6N0.8N  N10−810−610−410−2100rankEigenvalues/NCPU0.2N0.4N0.6N0.8N  N10−810−610−410−2100rankEigenvalues/NCENSUS0.2N0.4N0.6N0.8N  N10−1010−810−610−410−2100rankEigenvalues/NADULT0.2N0.4N0.6N0.8N  N10−810−610−410−2100rankEigenvalues/NCOD−RNA0.2N0.4N0.6N0.8N  N10−810−610−410−2100rankEigenvalues/NCOVTYPE0.2N0.4N0.6N0.8N  N10−810−610−410−2100rankEigenvalues/NFORESTReferences
[1] A. Azran and Z. Ghahramani. Spectral methods for automatic multiscale data clustering. In

CVPR  pages 190–197  2006.

[2] F. R. Bach and M. I. Jordan. Learning spectral clustering. Technical Report UCB/CSD-03-

1249  EECS Department  University of California  Berkeley  2003.

[3] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods. In ICML 

pages 33–40  2005.

[4] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local rademacher complexities. Annals of

Statistics  pages 44–58  2002.

[5] C. Chang and C. Lin. Libsvm: a library for support vector machines. TIST  2(3):27  2011.
[6] C. Cortes  M. Mohri  and A. Talwalkar. On the impact of kernel approximation on learning

accuracy. In AISTAT  pages 113–120  2010.

[7] O. Dekel  S. Shalev-Shwartz  and Y. Singer. The forgetron: A kernel-based perceptron on a

ﬁxed budget. In NIPS  2005.

[8] P. Drineas and M. W. Mahoney. On the nystrom method for approximating a gram matrix for

improved kernel-based learning. JMLR  6:2153–2175  2005.

[9] J. Kivinen  A. J. Smola  and R. C. Williamson. Online learning with kernels. IEEE Transac-

tions on Signal Processing  pages 2165–2176  2004.

[10] V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery

Problems. Springer  2011.

[11] S. Kumar  M. Mohri  and A. Talwalkar. Ensemble nystrom method. NIPS  pages 1060–1068 

2009.

[12] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416  2007.
[13] A. Rahimi and B. Recht. Random features for large-scale kernel machines. NIPS  pages 1177–

1184  2007.

[14] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization

with randomization in learning. NIPS  pages 1313–1320  2009.

[15] W. Rudin. Fourier analysis on groups. Wiley-Interscience  1990.
[16] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regulariza-

tion  Optimization  and Beyond. MIT Press  2001.

[17] T. Shi  M. Belkin  and B. Yu. Data spectroscopy: eigenspace of convolution operators and

clustering. The Annals of Statistics  37(6B):3960–3984  2009.

[18] S. Smale and D.-X. Zhou. Geometry on probability spaces. Constructive Approximation 

30(3):311–323  2009.

[19] G. W. Stewart and J. Sun. Matrix Perturbation Theory. Academic Press  1990.
[20] C. Williams and M. Seeger. Using the nystrom method to speed up kernel machines. NIPS 

pages 682–688  2001.

[21] K. Zhang  I. W. Tsang  and J. T. Kwok. Improved nystrom low-rank approximation and error

analysis. In ICML  pages 1232–1239  2008.

9

,Yichao Lu
Paramveer Dhillon
Dean Foster
Lyle Ungar
jean barbier
Mohamad Dia
Nicolas Macris
Florent Krzakala
Thibault Lesieur
Lenka Zdeborová