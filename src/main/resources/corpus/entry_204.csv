2016,A Simple Practical Accelerated Method for Finite Sums,Abstract We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size)  and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth  yielding a method applicable in many areas where operator splitting methods would traditionally be applied.,A Simple Practical Accelerated Method for Finite

Sums

Aaron Defazio

Ambiata  Sydney Australia

Abstract

We describe a novel optimization method for ﬁnite sums (such as empirical risk
minimization problems) building on the recently introduced SAGA method. Our
method achieves an accelerated convergence rate on strongly convex smooth prob-
lems. Our method has only one parameter (a step size)  and is radically simpler
than other accelerated methods for ﬁnite sums. Additionally it can be applied
when the terms are non-smooth  yielding a method applicable in many areas where
operator splitting methods would traditionally be applied.

Introduction

A large body of recent developments in optimization have focused on minimization of convex ﬁnite
sums of the form:

n(cid:88)

i=1

f (x) =

1
n

fi(x) 

a very general class of problems including the empirical risk minimization (ERM) framework as a
special case. Any function h can be written in this form by setting f1(x) = h(x) and fi = 0 for
i (cid:54)= 1  however when each fi is sufﬁciently regular in a way that can be made precise  it is possible to
optimize such sums more efﬁciently than by treating them as black box functions.
In most cases recently developed methods such as SAG [Schmidt et al.  2013] can ﬁnd an -minimum
faster than either stochastic gradient descent or accelerated black-box approaches  both in theory and
in practice. We call this class of methods fast incremental gradient methods (FIG).
FIG methods are randomized methods similar to SGD  however unlike SGD they are able to achieve
linear convergence rates under Lipschitz-smooth and strong convexity conditions [Mairal  2014 
Defazio et al.  2014b  Johnson and Zhang  2013  Koneˇcný and Richtárik  2013]. The linear rate in the
ﬁrst wave of FIG methods directly depended on the condition number L/µ of the problem  whereas
recently several methods have been developed that depend on the square-root of the condition number
[Lan and Zhou  2015  Lin et al.  2015  Shalev-Shwartz and Zhang  2013c  Nitanda  2014]  at least
when n is not too large. Analogous to the black-box case  these methods are known as accelerated
methods.
In this work we develop another accelerated method  which is conceptually simpler and requires
less tuning than existing accelerated methods. The method we give is a primal approach  however it
makes use of a proximal operator oracle for each fi instead of a gradient oracle  unlike other primal
approaches. The proximal operator is also used by dual methods such as some variants of SDCA
[Shalev-Shwartz and Zhang  2013a].

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Algorithm 1
Pick some starting point x0 and step size γ. Initialize each g0
gradient/subgradient at x0.
Then at step k + 1:

1. Pick index j from 1 to n uniformly at random.
2. Update x:

i = f(cid:48)

i (x0)  where f(cid:48)

i (x0) is any

(cid:35)

j − 1
gk
n

(cid:34)
n(cid:88)
(cid:0)zk
(cid:1) .
(cid:0)zk
j − xk+1(cid:1)  and leave the rest of the entries

gk
i

i=1

 

j

zk
j = xk + γ

xk+1 = proxγ
j

3. Update the gradient table: Set gk+1

= 1
γ

unchanged (gk+1

i = gk

j

i for i (cid:54)= j).

1 Algorithm

Our algorithm’s main step makes use of the proximal operator for a randomly chosen fi. For
convenience  we deﬁne:

(cid:26)

(cid:27)

.

proxγ

i (x) = argminy

γfi(y) +

(cid:107)x − y(cid:107)2

1
2

This proximal operator can be computed efﬁciently or in closed form in many cases  see Section 4 for
details. Like SAGA  we also maintain a table of gradients gi  one for each function fi. We denote the
state of gi at the end of step k by gk
i . The iterate (our guess at the solution) at the end of step k is
denoted xk. The starting iterate x0 may be chosen arbitrarily.
The full algorithm is given as Algorithm 1. The sum of gradients 1
i can be cached and
n
updated efﬁciently at each step  and in most cases instead of storing a full vector for each gi  only a
single real value needs to be stored. This is the case for linear regression or binary classiﬁcation with
logistic loss or hinge loss  in precisely the same way as for standard SAGA. A discussion of further
implementation details is given in Section 4.
With step size

(cid:80)n

i=1 gk

the expected convergence rate in terms of squared distance to the solution is given by:

(cid:113)

(cid:18)

γ =

E(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2 ≤

(n − 1)2 + 4n L

2Ln

1 − µγ
1 + µγ

 

µ

n

2L

− 1 − 1
(cid:19)k µ + L
(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2
(cid:19)(cid:33)

(cid:18) 1

(cid:33)

µ

 

(cid:32)(cid:32)(cid:115)

nL
µ

k = O

+ n

log

 



when each fi : Rd → R is L-smooth and µ-strongly convex. See Nesterov [1998] for deﬁnitions of
these conditions. Using big-O notation  the number of steps required to reduce the distance to the
solution by a factor  is:

as  → 0. This rate matches the lower bound known for this problem [Lan and Zhou  2015] under the
gradient oracle. We conjecture that this rate is optimal under the proximal operator oracle as well.
Unlike other accelerated approaches though  we have only a single tunable parameter (the step size
γ)  and the algorithm doesn’t need knowledge of L or µ except for their appearance in the step size.
Compared to the O ((L/µ + n) log (1/)) rate for SAGA and other non-accelerated FIG methods 
accelerated FIG methods are signiﬁcantly faster when n is small compared to L/µ  however for
n ≥ L/µ the performance is essentially the same. All known FIG methods hit a kind of wall at
n ≈ L/µ  where they decrease the error at each step by no more than 1 − 1
n. Indeed  when n ≥ L/µ
the problem is so well conditioned so as to be easy for any FIG method to solve it efﬁciently. This is
sometimes called the big data setting [Defazio et al.  2014b].

2

(cid:16)(cid:16)(cid:112)L/µ
(cid:17)

(cid:17)

Our convergence rate can also be compared to that of optimal ﬁrst-order black box methods  which
have rates of the form k = O
per epoch equivalent. We are able to achieve
√
a
n speedup on a per-epoch basis  for n not too large. Of course  all of the mentioned rates are
signiﬁcantly better than the O ((L/µ) log (1/)) rate of gradient descent.
For non-smooth but strongly convex problems  we prove a 1/-type rate under a standard iterate
averaging scheme. This rate does not require the use of decreasing step sizes  so our algorithm
requires less tuning than other primal approaches on non-smooth problems.

log (1/)

2 Relation to other approaches

Our method is most closely related to the SAGA method. To make the relation clear  we may write
our method’s main step as:

(cid:34)
j(xk+1) − gk
f(cid:48)
(cid:34)

j +

gk
i

 

(cid:35)
(cid:35)

.

1
n

n(cid:88)
n(cid:88)

i=1

i=1

1
n

xk+1 = xk − γ

whereas SAGA has a step of the form:

xk+1 = xk − γ

j(xk) − gk
f(cid:48)

j +

gk
i

The difference is the point at which the gradient of fj is evaluated at. The proximal operator has the
effect of evaluating the gradient at xk+1 instead of xk. While a small difference on the surface  this
change has profound effects. It allows the method to be applied directly to non-smooth problems
using ﬁxed step sizes  a property not shared by SAGA or other primal FIG methods. Additionally  it
allows for much larger step sizes to be used  which is why the method is able to achieve an accelerated
rate.
It is also illustrative to look at how the methods behave at n = 1. SAGA degenerates into regular
gradient descent  whereas our method becomes the proximal-point method [Rockafellar  1976]:

xk+1 = proxγf (xk).

The proximal point method has quite remarkable properties. For strongly convex problems  it
converges for any γ > 0 at a linear rate. The downside being the inherent difﬁculty of evaluating
the proximal operator. For the n = 2 case  if each term is an indicator function for a convex set  our
algorithm matches Dykstra’s projection algorithm if we take γ = 2 and use cyclic instead of random
steps.

Accelerated incremental gradient methods

Several acceleration schemes have been recently developed as extensions of non-accelerated FIG
methods. The earliest approach developed was the ASDCA algorithm [Shalev-Shwartz and Zhang 
2013b c]. The general approach of applying the proximal-point method as the outer-loop of a double-
loop scheme has been dubbed the Catalyst algorithm Lin et al. [2015]. It can be applied to accelerate
any FIG method. Recently a very interesting primal-dual approach has been proposed by Lan and
Zhou [2015]. All of the prior accelerated methods are signiﬁcantly more complex than the approach
we propose  and have more complex proofs.

3 Theory

3.1 Proximal operator bounds

In this section we rehash some simple bounds from proximal operator theory that we will use in
γ (x − pγf (x))  so that
this work. Deﬁne the short-hand pγf (x) = proxγf (x)  and let gγf (x) = 1
pγf (x) = x − γgγf (x). Note that gγf (x) is a subgradient of f at the point pγf (x). This relation is
known as the optimality condition of the proximal operator. Note that proofs for the following two
propositions are in the supplementary material.

3

Notation

xk
x∗
γ

pγf (x)
proxγ
i (x)
gk
i
g∗
i
vi
j
zk
j

Description

Current iterate at step k

Solution
Step size

Short-hand in results for generic f

Proximal operator of γfi at x

A stored subgradient of fi as seen at step k

A subgradient of fi at x∗

j = xk + γ(cid:2)gk

vi = x∗ + γg∗
j − 1

(cid:80)n

i

n

zk

i=1 gk
i

(cid:3)

Chosen component index (random variable)

pγf (x) = proxγf (x)

= argminy

γfi(y) + 1

Additional relation

xk ∈ Rd
x∗ ∈ Rd

(cid:110)
(cid:80)n
i=1 g∗
x∗ = proxγ

i = 0
i (vi)

2 (cid:107)x − y(cid:107)2(cid:111)
(cid:1)
(cid:0)zk

j

j = proxγ
xk+1

j

Table 1: Notation quick reference

Proposition 1. (Strengthening ﬁrm non-expansiveness under strong convexity) For any x  y ∈ Rd 
and any convex function f : Rd → R with strong convexity constant µ ≥ 0 

(cid:104)x − y  pγf (x) − pγf (y)(cid:105) ≥ (1 + µγ)(cid:107)pγf (x) − pγf (y)(cid:107)2 .

In operator theory this property is known as (1 + µγ)-cocoerciveness of pγf .
Proposition 2. (Moreau decomposition) For any x ∈ Rd  and any convex function f : Rd → R
with Fenchel conjugate f∗ :

Recall our deﬁnition of gγf (x) = 1
holds between the proximal operator of the conjugate f∗ and gγf :

pγf (x) = x − γp 1
(1)
γ (x − pγf (x)) also. After combining  the following relation thus

γ f∗ (x/γ).

p 1
γ f∗ (x/γ) =

1
γ

(x − pγf (x)) = gγf (x).

(2)

Theorem 3. For any x  y ∈ Rd  and any convex L-smooth function f : Rd → R:

(cid:18)

(cid:19)

(cid:104)gγf (x) − gγf (y)  x − y(cid:105) ≥ γ

1 +

1
Lγ

(cid:107)gγf (x) − gγf (y)(cid:107)2  

Proof. We will apply cocoerciveness of the proximal operator of f∗ as it appears in the decomposition.
Note that L-smoothness of f implies 1/L-strong convexity of f∗. In particular we apply it to the
points 1

γ x and 1

γ y:

p 1
γ f∗ (

1
γ

x) − p 1

γ f∗ (

1
γ

y) 

1
γ

x − 1
γ

y

≥

1 +

1
Lγ

γ f∗ (

1
γ

x) − p 1

γ f∗ (

1
γ

y)

(cid:29)

(cid:18)

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)p 1

(cid:13)(cid:13)(cid:13)(cid:13)2

.

(cid:28)

Pulling 1

γ from the right side of the inner product out  and plugging in Equation 2  gives the result.

3.2 Notation
Let x∗ be the unique minimizer (due to strong convexity) of f. In addition to the notation used in the
description of the algorithm  we also ﬁx a set of subgradients g∗
j   one for each of fj at x∗  chosen
j . Note that at the solution x∗  we want to

j = 0. We also deﬁne vj = x∗ + γg∗

such that(cid:80)n

j=1 g∗

apply a proximal step for component j of the form:

(cid:0)x∗ + γg∗

(cid:1) = proxγ

j

j (vj) .

x∗ = proxγ

j

4

Lemma 4. (Technical lemma needed by main proof) Under Algorithm 1  taking the expectation over
the random choice of j  conditioning on xk and each gk
i   allows us to bound the following inner
product at step k:

j  (cid:0)xk − x∗(cid:1) + γ

− γg∗

(cid:34)

(cid:35)

n(cid:88)

i=1

j − 1
gk
n

gk
i

− γg∗

j

(cid:43)

(cid:42)

(cid:34)

E

γ

≤ γ2 1
n

n(cid:88)
(cid:13)(cid:13)gk

j − 1
gk
n(cid:88)
n
i − g∗

i=1

i

gk
i

(cid:35)
(cid:13)(cid:13)2

.

i=1

The proof is in the supplementary material.

3.3 Main result

Theorem 5. (single step Lyapunov descent) We deﬁne the Lyapunov function T k of our algorithm
(Point-SAGA) at step k as:

(cid:13)(cid:13)2

+(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

 

i − g∗

i

T k =

c
n

n(cid:88)

i=1

(cid:13)(cid:13)gk
(cid:113)

for c = 1/µL. Then using step size γ =
random choice of j  conditioning on xk and each gk

E(cid:2)T k+1(cid:3) ≤ (1 − κ) T k

2Ln

i   is:

for κ =

µγ

 

1 + µγ

(n−1)2+4n L

µ

− 1− 1

n

2L   the expectation of T k+1  over the

when each fi : Rd → R is L-smooth and µ-strongly convex and 0 < µ < L. This is the same
Lyapunov function as used by Hofmann et al. [2015].

j − 1
gk
n

gk
i

i=1

5

E(cid:13)(cid:13)gk+1

j − g∗

j

(cid:13)(cid:13)2

.

Proof. Term 1 of T k+1 is straight-forward to simplify:

(cid:13)(cid:13)gk+1

i − g∗

i

n(cid:88)

i=1

c
n

E

For term 2 of T k+1 we start by applying cocoerciveness (Theorem 1):

i

n

=

+

i=1

c
n

i − g∗

n(cid:88)

1 − 1
n

(cid:19) c

(cid:13)(cid:13)2
= (1 + µγ)E(cid:13)(cid:13)proxγ
≤ E(cid:10)proxγ
= E(cid:10)xk+1 − x∗   zk

(cid:18)
(cid:13)(cid:13)gk
(1 + µγ)E(cid:13)(cid:13)xk+1 − x∗(cid:13)(cid:13)2
(cid:11) .

j (zk
j ) − proxγ
j − vj

j (zk

j ) − proxγ
j (vj)  zk

(cid:13)(cid:13)2
j (vj)(cid:13)(cid:13)2
(cid:11)

j − vj
(cid:11)

(cid:11)

j − vj

j − vj

j − vj

Now we add and subtract xk :

E(cid:10)xk+1 − xk   zk

(cid:11) + E(cid:10)xk+1 − xk   zk
(cid:11)  

= E(cid:10)xk+1 − xk + xk − x∗   zk
= E(cid:10)xk − x∗   zk
= (cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
+ E(cid:10)xk+1 − xk   zk
j − vj
(cid:11) further:
j − vj] = xk − x∗ (we can take the
where we have pulled out the quadratic term by using E[zk
(cid:11)
E(cid:10)xk+1 − xk   zk
expectation since the left hand side of the inner product doesn’t depend on j). We now expand
j − vj
(cid:11)
= E(cid:10)xk+1 − γg∗
j − vj
(cid:35)
(cid:34)
(cid:42)
n(cid:88)
j − xk   zk
j − vj
j + γg∗
j − 1
(cid:34)
gk
n(cid:88)
(cid:0)xk − x∗(cid:1) + γ
n

j − xk 
j + γg∗
(cid:43)

xk − γgk+1

− γg∗

− γg∗

j + γ

(cid:35)

= E

gk
i

i=1

(3)

.

j

We further split the left side of the inner product to give two separate inner products:

(cid:34)

(cid:42)
(cid:42)

= E

+ E

γ

j − 1
gk
n

γg∗

j − γgk+1

j

(cid:34)

(cid:35)
n(cid:88)
 (cid:0)xk − x∗(cid:1) + γ

− γg∗

j  (cid:0)xk − x∗(cid:1) + γ
(cid:34)
n(cid:88)

gk
i

i=1

j − 1
gk
n

(cid:35)

n(cid:88)
(cid:43)

i=1

gk
i

.

j − 1
(cid:35)
gk
n

gk
i

− γg∗

j

− γg∗

j

(cid:43)

(4)

i

i=1

i=1

(cid:80)n

(cid:13)(cid:13)gk

inner product
i − g∗

in Equation 4 is the quantity we bounded in Lemma 4 by

The ﬁrst
γ2 1
n
rem 3 (note the right side of the inner product is equal to zk

(cid:13)(cid:13)2. The second inner product in Equation 4  can be simpliﬁed using Theo-
−γE(cid:10)gk+1

(cid:13)(cid:13)2
E(cid:13)(cid:13)gk+1
j − g∗
Combing these gives the following bound on (1 + µγ)E(cid:13)(cid:13)xk+1 − x∗(cid:13)(cid:13)2:
(cid:19)
(cid:18)
(1+µγ)E(cid:13)(cid:13)xk+1 − x∗(cid:13)(cid:13)2 ≤(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
E(cid:13)(cid:13)gk+1
(cid:13)(cid:13)2−γ2

(cid:11) ≤ −γ2
n(cid:88)
(cid:13)(cid:13)gk

(cid:19)
j − vj):
1
Lγ

j − g∗

(cid:13)(cid:13)2

.

j − vj

j − g∗

i − g∗

j   zk

(cid:18)

1 +

1 +

.

j

j

i

1
Lγ

+γ2 1
n

i=1

Deﬁne α = 1
and combine with the rest of the Lyapunov function  giving:

1+µγ = 1 − κ  where κ = µγ

1+µγ . Now we multiply the above inequality through by α

We want an α convergence rate  so we pull out the required terms:

(cid:16)

E(cid:2)T k+1(cid:3) ≤ T k +
(cid:16) c

+

n

αγ2 − c
n
− αγ2 − αγ
L

(cid:16)

E(cid:2)T k+1(cid:3) ≤ αT k +
(cid:16) c

+

− αγ2 − αγ
L

n

i

i

j

n

(cid:17) 1
n(cid:88)
(cid:13)(cid:13)gk
(cid:13)(cid:13)2
i − g∗
(cid:17)
E(cid:13)(cid:13)gk+1
(cid:13)(cid:13)2 − κE(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
j − g∗
(cid:17) 1
n(cid:88)
(cid:13)(cid:13)gk
(cid:13)(cid:13)2
E(cid:13)(cid:13)gk+1
(cid:13)(cid:13)2
n
j − g∗
(cid:113)

i − g∗

(cid:17)

αγ2 + κc − c
n

.

j

i

i

(n−1)2+4n L

− 1− 1

.

Now to complete the proof we note that c = 1/µL and γ =
2L ensure that both
terms inside the round brackets are non-positive  giving ET k+1 ≤ αT k. These constants were found
by equating the equations in the brackets to zero  and solving with respect to the two unknowns  γ
and c. It is easy to verify that γ is always positive  as a consequence of the condition number L/µ
always being at least 1.

2Ln

n

µ

Corollary 6. (Smooth case) Chaining Theorem 5 gives a convergence rate for Point-SAGA at step k
under the constants given in Theorem 5 of:

E(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2 ≤ (1 − κ)k µ + L

(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2

 

µ
if each fi : Rd → R is L-smooth and µ-strongly convex.

Theorem 7. (Non-smooth case) Suppose each fi : Rd → R is µ-strongly convex (cid:13)(cid:13)g0
and(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13) ≤ R. Then after k iterations of Point-SAGA with step size γ = R/B

i − g∗
√
n:

i

(cid:13)(cid:13) ≤ B

E(cid:13)(cid:13)¯xk − x∗(cid:13)(cid:13)2 ≤ 2

√

√

n (1 + µ (R/B

µk

n))

RB 

where ¯xk = 1

t=1 xt. The proof of this theorem is included in the supplementary material.

k E(cid:80)k

6

4

Implementation

Care must be taken for efﬁcient implementation  particularly in the sparse gradient case. We discuss
the key points below. A fast Cython implementation is available on the author’s website incorporating
these techniques.
Proximal operators For the most common binary classiﬁcation and regression methods  imple-
menting the proximal operator is straight-forward. We include details of the computation
of the proximal operators for the hinge  square and logistic losses in the supplementary
material. The logistic loss does not have a closed form proximal operator  however it may
be computed very efﬁciently in practice using Newton’s method on a 1D subproblem. For
problems of a non-trivial dimensionality the cost of the dot products in the main step is
much greater than the cost of the proximal operator evaluation. We also detail how to handle
a quadratic regularizer within each term’s prox operator  which has a closed form in terms
of the unregularized prox operator.
i = f(cid:48)

i (x0) before commencing the algorithm  we recommend
i = 0 instead. This avoids the cost of a initial pass over the data. In practical effect

Initialization Instead of setting g0

using g0
this is similar to the SDCA initialization of each dual variable to 0.

5 Experiments
We tested our algorithm which we call Point-SAGA against SAGA [Defazio et al.  2014a]  SDCA
[Shalev-Shwartz and Zhang  2013a]  Pegasos/SGD [Shalev-Shwartz et al.  2011] and the catalyst
acceleration scheme [Lin et al.  2015]. SDCA was chosen as the inner algorithm for the catalyst
scheme as it doesn’t require a step-size  making it the most practical of the variants. Catalyst applied
to SDCA is essentially the same algorithm as proposed in Shalev-Shwartz and Zhang [2013c]. A
single inner epoch was used for each SDCA invocation. Accelerated MISO as well as the primal-dual
FIG method [Lan and Zhou  2015] were excluded as we wanted to test on sparse problems and
they are not designed to take advantage of sparsity. The step-size parameter for each method (κ for
catalyst-SDCA) was chosen using a grid search of powers of 2. The step size that gives the lowest
error at the ﬁnal epoch is used for each method.
We selected a set of commonly used datasets from the LIBSVM repository [Chang and Lin  2011].
The pre-scaled versions were used when available. Logistic regression with L2 regularization was
applied to each problem. The L2 regularization constant for each problem was set by hand to ensure
f was not in the big data regime n ≥ L/µ; as noted above  all the methods perform essentially the
same when n ≥ L/µ. The constant used is noted beneath each plot. Open source code to exactly
replicate the experimental results is available at https://github.com/adefazio/point-saga.
Algorithm scaling with respect to n The key property that distinguishes accelerated FIG methods
from their non-accelerated counterparts is their performance scaling with respect to the dataset size.
For large datasets on well-conditioned problems we expect from the theory to see little difference
between the methods. To this end  we ran experiments including versions of the datasets subsampled
randomly without replacement in 10% and 5% increments  in order to show the scaling with n
empirically. The same amount of regularization was used for each subset.
Figure 1 shows the function value sub-optimality for each dataset-subset combination. We see that
in general accelerated methods dominate the performance of their non-accelerated counter-parts.
Both SDCA and SAGA are much slower on some datasets comparatively than others. For example 
SDCA is very slow on the 5 and 10% COVTYPE datasets  whereas both SAGA and SDCA are much
slower than the accelerated methods on the AUSTRALIAN dataset. These differences reﬂect known
properties of the two methods. SAGA is able to adapt to inherent strong convexity while SDCA can
be faster on very well-conditioned problems.
There is no clear winner between the two accelerated methods  each gives excellent results on each
problem. The Pegasos (stochastic gradient descent) algorithm with its slower than linear rate is a
clear loser on each problem  almost appearing as an almost horizontal line on the log scale of these
plots.

Non-smooth problems We also tested the RCV1 dataset on the hinge loss. In general we did not
expect an accelerated rate for this problem  and indeed we observe that Point-SAGA is roughly as
fast as SDCA across the different dataset sizes.

7

(a) COVTYPE µ = 2 × 10−6 : 5%  10%  100% subsets

(b) AUSTRALIAN µ = 10−4: 5%  10%  100% subsets

(c) MUSHROOMS µ = 10−4: 5%  10%  100% subsets

(d) RCV1 with hinge loss  µ = 5 × 10−5: 5%  10%  100% subsets

Figure 1: Experimental results

8

05101520Epoch10−810−710−610−510−410−310−210−1100FunctionSuboptimality05101520Epoch10−910−810−710−610−510−410−310−210−1100FunctionSuboptimality05101520Epoch10−1210−1110−1010−910−810−710−610−510−410−310−210−1100101FunctionSuboptimality051015202530Epoch10−410−310−210−1100101FunctionSuboptimality051015202530Epoch10−410−310−210−1100FunctionSuboptimality051015202530Epoch10−810−710−610−510−410−310−210−1100FunctionSuboptimality051015202530Epoch10−1010−910−810−710−610−510−410−310−210−1100FunctionSuboptimality051015202530Epoch10−1010−910−810−710−610−510−410−310−210−1100FunctionSuboptimality051015202530Epoch10−1310−1210−1110−1010−910−810−710−610−510−410−310−210−1100FunctionSuboptimality0510152025303540Epoch10−410−310−210−1100FunctionSuboptimality0510152025303540Epoch10−510−410−310−210−1100FunctionSuboptimality0510152025303540Epoch10−510−410−310−210−1100FunctionSuboptimality05101520Epoch106105104103102101100101FunctionSuboptimalityPoint-SAGAPegasosSAGASDCACatalyst-SDCAReferences
Chih-Chung Chang and Chih-Jen Lin. Libsvm : a library for support vector machines. ACM

Transactions on Intelligent Systems and Technology  2:27:1–27:27  2011.

Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. Advances in Neural Information
Processing Systems 27 (NIPS 2014)  2014a.

Aaron Defazio  Tiberio Caetano  and Justin Domke. Finito: A faster  permutable incremental gradient
method for big data problems. Proceedings of the 31st International Conference on Machine
Learning  2014b.

Thomas Hofmann  Aurelien Lucchi  Simon Lacoste-Julien  and Brian McWilliams. Variance reduced
stochastic gradient descent with neighbors. In C. Cortes  N.D. Lawrence  D.D. Lee  M. Sugiyama 
and R. Garnett  editors  Advances in Neural Information Processing Systems 28  pages 2296–2304.
Curran Associates  Inc.  2015.

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. NIPS  2013.

Jakub Koneˇcný and Peter Richtárik. Semi-Stochastic Gradient Descent Methods. ArXiv e-prints 

December 2013.

G. Lan and Y. Zhou. An optimal randomized incremental gradient method. ArXiv e-prints  July 2015.

Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimization.
In C. Cortes  N.D. Lawrence  D.D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural
Information Processing Systems 28  pages 3366–3374. Curran Associates  Inc.  2015.

Julien Mairal. Incremental majorization-minimization optimization with application to large-scale
machine learning. Technical report  INRIA Grenoble Rhône-Alpes / LJK Laboratoire Jean
Kuntzmann  2014.

Yu. Nesterov. Introductory Lectures On Convex Programming. Springer  1998.

Atsushi Nitanda. Stochastic proximal gradient descent with acceleration techniques. In Z. Ghahra-
mani  M. Welling  C. Cortes  N.D. Lawrence  and K.Q. Weinberger  editors  Advances in Neural
Information Processing Systems 27  pages 1574–1582. Curran Associates  Inc.  2014.

R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on

control and optimization  14(5):877–898  1976.

Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Technical report  INRIA  2013.

Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. JMLR  2013a.

Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In
C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q. Weinberger  editors  Advances in
Neural Information Processing Systems 26  pages 378–385. Curran Associates  Inc.  2013b.

Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. Technical report  The Hebrew University  Jerusalem and Rutgers
University  NJ  USA  2013c.

Shai Shalev-Shwartz  Yoram Singer  Nathan Srebro  and Andrew Cotter. Pegasos: Primal estimated

sub-gradient solver for svm. Mathematical programming  127(1):3–30  2011.

9

,Greg Ver Steeg
Aram Galstyan
Dinesh Ramasamy
Upamanyu Madhow
Aaron Defazio