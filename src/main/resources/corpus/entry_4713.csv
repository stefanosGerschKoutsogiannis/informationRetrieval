2018,RetGK: Graph Kernels based on Return Probabilities of Random Walks,Graph-structured data arise in wide applications  such as computer vision  bioinformatics  and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper  we develop a framework for computing graph kernels  based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes  while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform other state-of-the-art approaches in both accuracy and computational efficiency.,RetGK: Graph Kernels based on Return Probabilities

of Random Walks

Zhen Zhang  Mianzhi Wang  Yijian Xiang  Yan Huang  and Arye Nehorai

Department of Electrical and Systems Engineering

Washington University in St. Louis

St. Louis  MO 63130

{zhen.zhang  mianzhi.wang  yijian.xiang  yanhuang640  nehorai}@wustl.edu

Abstract

Graph-structured data arise in wide applications  such as computer vision  bioinfor-
matics  and social networks. Quantifying similarities among graphs is a fundamen-
tal problem. In this paper  we develop a framework for computing graph kernels 
based on return probabilities of random walks. The advantages of our proposed
kernels are that they can effectively exploit various node attributes  while being
scalable to large datasets. We conduct extensive graph classiﬁcation experiments to
evaluate our graph kernels. The experimental results show that our graph kernels
signiﬁcantly outperform existing state-of-the-art approaches in both accuracy and
computational efﬁciency.

1

Introduction

Structured data modeled as graphs arise in many application domains  such as computer vision 
bioinformatics  and social network mining. One interesting problem for graph-type data is quantifying
their similarities based on the connectivity structure and attribute information. Graph kernels  which
are positive deﬁnite functions on graphs  are powerful similarity measures  in the sense that they
make various kernel-based learning algorithms  for example  clustering  classiﬁcation  and regression 
applicable to structured data. For instance  it is possible to classify proteins by predicting whether a
given protein is an enzyme or not.
There are several technical challenges in developing effective graph kernels: (i) When designing graph
kernels  one might come across the graph isomorphism problem  a well-known NP problem. The
kernels should satisfy the isomorphism-invariant property  while being informative on the topological
structure difference. (ii) Graphs are usually coupled with multiple types of node attributes  e.g. 
discrete1 or continuous attributes. For example  a chemical compound may have both discrete and
continuous attributes  which respectively describe the type and position of atoms. A crucial problem
is how to integrate the graph structure and node attribute information in graph kernels. (iii) In some
applications  e.g.  social networks  graphs tend to be very large  with thousands or even millions of
nodes  which requires strongly scalable graph kernels.
In this work  we propose novel methods to overcome these challenges. We revisit the concept
of random walks  introducing a new node structural role descriptor  the return probability feature
(RPF). We rigorously show that the RPF is isomorphism-invariant and encodes very rich connectivity
information. Moreover  RPF allows us to consider attributed and nonattributed graphs in a uniﬁed
framework. With the RPF  we can embed (non-)attributed graphs into a Hilbert space. After that  we
naturally obtain our return probability-based graph kernels ("RetGK" for short). Combining with the
approximate feature maps technique  we represent each graph with a multi-dimensional tensor and
design a family of computationally efﬁcient graphs kernels.

1In the literature  the discrete node attributes are usually called "labels".

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Related work. There are various graph kernels  many of which explore the R-convolutional frame-
work [12]. The key idea is decomposing a whole graph into small substructures and building graph
kernels based on the similarities among these components. Such kernels differ from each other in the
way they decompose graphs. For example  graphlet kernels [26] are based on small subgraphs up to
a ﬁxed size. Weisfeiler-Lehman graph kernels [25] and tree-based kernels [6] are developed with
subtree patterns. Shortest path kernels [1] are derived by comparing the paths between graphs. Still
other graph kernels  such as [30] and [10]  are developed by counting the number of common random
walks on direct product graphs. Recently  subgraph matching kernels [18] and graph invariant kernels
[22] were proposed for handling continuous attributes. However  all the above R-convolution based
graph kernels suffer from a drawback. As pointed out in [32]  increasing the size of substructures
will largely decrease the probability that two graphs contain similar substructures  which usually
results in the "diagonal dominance issue" [14]. Our return probability based kernels are signiﬁcantly
different from the above ones. We measure the similarity between two graphs by directly comparing
their node structural role distributions  avoiding substructures decomposition.
More recently  new methods have been proposed for comparing graphs  which is done by quantifying
the dissimilarity between the distributions of pairwise distances between nodes. [24] uses the shortest
path distance  and [29] uses the diffusion distance. However  these methods can be applied only to
non-attributed (unlabeled) graphs  which largely limits their applications in the real world.
Organization. In Section 2  we introduce the necessary background  including graph concepts and
tensor algebra. In Section 3  we discuss the favorable properties of and computational methods for
RPF. In Section 4  we present the Hilbert space embedding of graphs  and develop the corresponding
graph kernels. In Section 5  we show the tensor representation of graphs  and derive computational
efﬁcient graph kernels. In Section 6  we report the experimental results on 21 benchmark datasets. In
the supplementary material  we provide proofs of all mathematical results in the paper.

2 Background

2.1 Graph concepts
An undirect graph G consists of a set of nodes VG = {v1  v2  ...  vn} and a set of edges EG ⊆ VG×VG.
Each edge (vi  vj) is assigned with a positive value wij describing the connection strength between vi
and vj. For an unweighted graph  all the edge weights are set to be one  i.e.  wij = 1 ∀(vi  vj) ∈ EG.
Two graphs G and H are isomorphic if there exists a permutation map τ : VG → VH  such that

∀(vi  vj) ∈ EG (cid:0)τ (vi)  τ (vj)(cid:1) ∈ EH  and the corresponding edge weights are preserved.
is diagonal matrix whose diagonal terms are DG(i  i) =(cid:80)
summation of all node degrees  i.e.  VolG =(cid:80)n

The adjacent matrix AG is an n × n symmetric matrix with AG(i  j) = wij. The degree matrix DG
wij. The volume of G is the
i=1 DG(i  i). An S-step walk starting from node v0
is a sequence of nodes {v0  v1  v2  ...  vS}  with (vs  vs+1) ∈ EG  0 ≤ s ≤ S − 1. A random walk
on G is a Markov chain (X0  X1  X2  ...)  whose transition probabilities are

(vi vj )∈EG

Pr(Xi+1 = vi+1|Xi = vi  ...  X0 = v0) = Pr(Xi+1 = vi+1|Xi = vi) =

wij

DG(i  i)

 

(1)

which induces the transition probability matrix PG = D−1
G is the s-step
transition matrix  where P s
In our paper  we also consider the case that nodes are associated with multiple attributes. Let A
denote a attribute domain. Typically  A can be a alphabet set or a subset of a Euclidean space  which
corresponds to discrete attributes and continuous attributes  respectively.

G(i  j) is the transition probability in s steps from node vi to vj.

G AG. More generally  P s

2.2 Tensor algebra
A tensor [17] is a multidimensional array  which has multiple indices.2 We use RI1×I2×...×IN to
denote the set of tensors of order N with dimension (I1  I2  ...  IN ). If U ∈ RI1×I2×...×IN   then
Ui1i2 ... iN ∈ R  where 1 ≤ i1 ≤ I1  ...  1 ≤ iN ≤ IN .

2A vector (cid:126)u ∈ RD is a ﬁrst-order tensor  and a matrix A ∈ RD1×D2 is a second-order tensor.

2

The inner product between tensors U  V ∈ RI1×I2×...×IN is deﬁned such that

I1(cid:88)

I2(cid:88)

IN(cid:88)

(cid:104)U  V (cid:105)T = vec(U )T vec(V ) =

...

Ui1i2 ... iN Vi1i2 ... iN .

(2)

A rank-one tensor W ∈ RI1×I2×...×IN is the tensor (outer) product of N vectors  i.e.  W =
(cid:126)w(1) ◦ (cid:126)w(2) ◦ ... ◦ (cid:126)w(N )  Wi1i2 ... iN = (cid:126)w(1)

.

(cid:126)w(2)
i2

... (cid:126)w(N )
iN

i1

i1=1

i2=1

iN =1

3 Return probabilities of random walks

Given a graph G  as we can see from (1)  the transition probability matrix  PG  encodes all the
connectivity information  which leads to a natural intuition: We can compare two graphs by quantify-
ing the difference between their transition probability matrices. However  big technical difﬁculties
exist  since the sizes of two matrices are not necessarily the same  and their rows or columns do not
correspond in most cases.
To tackle the above issues  we make use of the S-step return probabilities of random walks on G. To
do this  we assign each node vi ∈ VG an S-dimensional feature called "return probability feature"
("RPF" for short)  which describes the "structural role" of vi  i.e. 

(3)
G(i  i)  s = 1  2  ...  S  is the return probability of a s-step random walk starting from vi.
G = {(cid:126)p1  (cid:126)p2  ...  (cid:126)pn}. The

where P s
Now each graph is represented by a set of feature vectors in RS: RPFS
RPF has three nice properties: isomorphism-invariance  multi-resolution  and informativeness.

G(i  i)  ...  P S

G(i  i)  P 2

G (i  i)]T  

(cid:126)pi = [P 1

3.1 The properties of RPF

H (τ (i)  τ (i)).

G(i  i) = P s

Isomorphism-invariance. The isomorphism-invariance property of return probability features is
summarized in the following proposition.
Proposition 1. Let G and H be two isomorphic graphs of n nodes  and let τ : {1  2  ...  n} →
{1  2  ...  n} be the corresponding isomorphism. Then 
∀vi ∈ VG  s = 1  2  ... ∞  P s

(4)
H  ∀S = 1  2  ... ∞. Such
G = RPFS
Clearly  isomorphic graphs have the same set of RPF  i.e.  RPFS
a property can be used to check graph isomorphism  i.e.  if ∃S  s.t. RPFS
G (cid:54)= RPFS
H  then G and H
are not isomorphic. Moreover  Proposition 1 allows us to directly compare the structural role of any
two nodes in different graphs  without considering the matching problems.
Multi-resolution. RPF characterizes the "structural role" of nodes with multi-resolutions. Roughly
speaking  P s
G(i  i) reﬂects the interaction between node vi and the subgraph involving vi. With an
increase in s  the subgraph becomes larger. We use a toy example to illustrate our idea. Fig. 1(a)
presents an unweighted graph G  and C1  C2  and C3 are three center nodes in G  which play different
structural roles. In Fig. 1(b)  we plot their s-step return probabilities  s = 1  2  ...  200. C1  C2  and
C3 have the same degree  as do their neighbors. Thus their ﬁrst two return probabilities are the same.
Since C1 and C2 share the similar neighbourhoods at larger scales  their return probability values
are close until the eighth step. Because C3 plays a very different structural role from C1 and C2  its
return probabilities values deviate from those of C1 and C2 in early steps.
In addition  as shown in Fig. 1(b)  when the random walk step s approaches inﬁnity  the return
probability P s
G(i  i) will not change much and will converge to a certain value  which is known as
the stationary probability in Markov chain theory [5]. Therefore  if s is already sufﬁciently large  we
gain very little new information from the RPF by increasing s.
Informativeness. The RPF provides very rich information on the graph structure  in the sense that if
two graphs has the same RPF sets  they share very similar spectral properties.
Theorem 1. Let G and H be two connected graphs of the same size n and volume Vol  and let PG
and PH be the corresponding transition probability matrices. Let {(λk  (cid:126)ψk)}n
k=1 and {(µk  (cid:126)ϕk)}n
be eigenpairs of PG and PH  respectively. Let τ : {1  2  ...  n} → {1  2  ...  n} be a permutation
map. If P s

H (τ (i)  τ (i)) ∀vi ∈ VG ∀s = 1  2  ...  n  i.e.  RPFn

G(i  i) = P s

G = RPFn

H  then 

k=1

3

(a)

(b)

Figure 1: (a) Toy Graph G; (b) The s-step return probability of the nodes C1  C2 and C3 in the toy
graph  s = 1  2  ...  200. The nested ﬁgure is a close-up view of the rectangular region.

H  ∀S = n + 1  n + 2  ... ∞;

G = RPFS

1. RPFS
2. {λ1  λ2  ...  λn} = {µ1  µ2  ...  µn};
3. If the eigenvalues sorted by their magnitudes satisfy: |λ1| > |λ2| > ... > |λm| > 0 
|λm+1| = ... = |λn| = 0  then we have that | (cid:126)ψk(i)| = | (cid:126)ϕk(τ (i))|  ∀vi ∈ VG  ∀k =
1  2  ...  m.

G and RPFS

G  S ≥ n
The ﬁrst conclusion states that the graph structure information contained in RPFn
are the same  coinciding with our previous discussions on RPF with large random walk steps. The
second and third conclusions bridge the RPF with spectral representations of graphs [4]  which
contains almost all graph structure information.
Relation to eigenvector embeddings (EE). One popular way of embedding graph nodes in a Eu-
clidean space uses the eigenvectors of Laplacian or adjacent matrices as the coordinates. In [21]  a
class of graph kernels are developed based on the eigenvector embeddings. From Theorem 1  we
see that both RPF and EE encode the spectral information of graphs. However  our RPF has several
advantages over EE. (i) The eigenvector embeddings reﬂect the closeness among nodes in the same
graph  which makes it difﬁcult to compare node across graphs. (ii) The EE representations  which are
computed up to a change in sign (or more generally  orthonormal transformation in the eigenspace) 
may not be invariant under graph isomorphisms. A counterexample is shown in Fig. 2. G and G’ are
two isomorphic graphs  we visualize their ﬁrst three-dimensional embeddings with RPF and EE 3.
It can be seen that RPFs are invariant while eigenvectors are not. (iii) The eigenvector embeddings
are unstable. The perturbation theory says that two eigenvectors may switch if their eigenvalues are
close.

3.2 The computation of RPF

Given a graph G  the brute-force computation of RPFS
of PG. Therefore  the time complexity is (S − 1)n3  which is quite high when S is large.
Since only the diagonal terms of transition matrices are needed  we have efﬁcient techniques. Write

G requires (S−1)×n×n matrix multiplication

 ∀vi ∈ VG ∀s = 1  2  ...  S.

(6)

Let U = [(cid:126)u1  (cid:126)u2  ...  (cid:126)un]  let V = U (cid:12) U  where (cid:12) denotes Hadamard product  and let (cid:126)Λs =
n]T . Then we can obtain all nodes’ s-step return probabilities in the vector V (cid:126)Λs. The
1  λs
[λs
3Note that since the signs of these eigenvectors are not ﬁxed  we use the absolute value as in [21]

2  ...  λs

k=1

4

− 1
PG = D−1
G AG = D
G )D
− 1
− 1
G is a symmetric matrix. Then P s
G AGD

− 1
G AGD

where BG = D

be the eigenpairs of BG  i.e.  BG =(cid:80)n
n(cid:88)

k=1 λk (cid:126)uk (cid:126)uT

− 1
G (D

2

2

2

2

2

(cid:2)(cid:126)uk(i)(cid:3)2

P s

G(i  i) = Bs

G(i  i) =

λs
k

2

1
2

− 1
G BGD
G = D
− 1
G Bs

GD

G = D

2

1
2

G 
G. Let {(λk  (cid:126)uk)}n

1
2

k=1

(5)

k . Then the return probabilities are

time O(n2). So the total time complexity of the above computational method is O(cid:0)n3 + (S + 1)n2(cid:1).

eigen-decomposition of BG requires time O(n3). Computing V or V (cid:126)Λs  ∀s = 1  2  ...  S  takes

3.2.1 Monte Carlo simulation method
If the graph node number  n  is large  i.e.  n > 105  the eigendecomposition of an n × n matrix is
relatively time-consuming. To make RPF scalable to large graphs  we use the Monte Carlo method to
simulate random walks. Given a graph G  for each node vi ∈ VG  we can simulate a random walk
of length S based on the transition probability matrix PG. We repeat the above procedure M times 
obtaining M sequences of random walks. For each step s = 1  2  ...  S  we use the relative frequency
of returning to the starting point as the estimation of the corresponding s-step return probability. The
random walk simulation is parallelizable and can be implemented efﬁciently  characteristics of which
both contribute to the scalability of RPF.

4 Hilbert space embeddings of graphs

G = {(cid:126)pi}n

In this section  we introduce the Hilbert space embeddings of graphs  based on the RPF. With such
Hilbert space embeddings  we can naturally obtain the corresponding graph kernels.
As discussed in Section 3 
the structural role of each node vi can be characterized by an
S−dimensional return probability vector (cid:126)pi (see 3)  and thus a nonattributed graph can be rep-
resented by the set RPFS
i=1. Since the isomorphism-invariance property allows direct
comparison of nodes’ structural roles across different graphs  we can view the RPF as a special type
of attribute  namely  "the structural role attribute" (whose domain is denoted as A0)  associated with
nodes. Clearly  A0 = RS.
The nodes of attributed graphs usually have other types of attributes  which are obtained by physical
measurements. Let A1 A2  ... AL be their attribute domains. When combined with RPF  an
attributed graph can be represented by the set {((cid:126)pi  a1
i=1 ⊆ A0 × A1 × ... × AL (denoted
as ×L
l=0Al). Such a representation allows us to consider both attributed and nonattributed graphs in
a uniﬁed framework  since if L = 0  the above set just degenerates to the nonattributed case. The
l=0Al  which
set representation forms an empirical distribution µ = 1
n
can be embedded into a reproducing kernel Hilbert space (RKHS) by kernel mean embedding [11].
Let kl  l = 0  1  ...  L be a kernel on Al. Let Hl and φl be the corresponding RKHS and implicit
feature map  respectively. Then we can deﬁne a kernel on A through the tensor product of kernels [28] 
i.e.  k = ⊗L
l=1 kl(al  bl). Its associated
RKHS  H  is the tensor product space generated by Hl  i.e.  H = ⊗L
l=0Hl. Let φ : A → H be the

l=0kl  k(cid:2)((cid:126)p  a1  a2  ...  aL)  ((cid:126)q  b1  b2  ...  bL)(cid:3) = k0((cid:126)p  (cid:126)q)(cid:81)L

i )}n
(cid:80)n

i   ...  aL

i=1 δ((cid:126)pi a1

i  ... aL

i ) on A = ×L

(a)

(b)

(c)

Figure 2: Toy graph G and its adjacent matrix; (b) Toy graph G’ and its adjacent matrix; (c) 3-
D eigenvector and RPF embeddings of nodes in G and G’  respectively. We can see that our RPF
3   V (cid:48)
correctly reﬂects the structural roles. That is  the nodes V3  V4  V5 in graph G and the nodes V (cid:48)
in graph G’ have the same structural role. And the nodes V1  V2 in graph G and the nodes V (cid:48)
5
4 in
graph G’ have the same structural role.

1   V (cid:48)
2   V (cid:48)

5

implicit feature map. Then given a graph G  we can embed it into H in the following procedure 

(cid:90)

n(cid:88)

i=1

φdµG =

1
n

A

φ(pi  a1

i   ...  aL

i ).

(7)

G → µG → mG  and mG =

4.1 Graph kernels (I)

(cid:0)(cid:52)G

An important beneﬁt of Hilbert space embedding of graphs is that it is straightforward to generalize
the positive deﬁnite kernels deﬁned on Euclidean spaces to the set of graphs.
Given two graphs G and H  let {(cid:52)G

(cid:1). Let KGG  KHH  and KGH be the kernel matrices 

j=1 be the respective set representations
i  (cid:52)G
j ) 

i }nG
i ) and likewise (cid:52)H
j )  and (KGH )ij = k((cid:52)G

i=1 and {(cid:52)H

i = ((cid:126)pi  a1

i   ...  aL
i  (cid:52)H

i   a2
induced by the embedding kernel k. That is  they are deﬁned such that (KGG)ij = k((cid:52)G
(KHH )ij = k((cid:52)H
Proposition 2. Let G be the set of graphs with attribute domains A1 A2  ... AL. Let G and H be
two graphs in G. Let mG and mH be the corresponding graph embeddings. Then the following
functions are positive deﬁnite graph kernels deﬁned on G × G.

i  (cid:52)H
j ).

i }nH

j

K1(G  H) = (c + (cid:104)mG  mH(cid:105)H)d = (c +

K2(G  H) = exp(−γ(cid:107)mG − mH(cid:107)pH) = exp(cid:2) − γMMDp(µG  µH )(cid:3)  γ > 0  0 < p ≤ 2 

KGH(cid:126)1nH )d  c ≥ 0  d ∈ N 

nGnH

(cid:126)1T
nG

1

(8a)

(8b)

where MMD(µG  µH ) = ( 1
n2
G
maximum mean discrepancy (MMD) [11].

(cid:126)1T
nG

KGG(cid:126)1nG + 1
n2
H

(cid:126)1T
nH

KHH(cid:126)1nH − 2

nGnH

(cid:126)1T
nG

KGH(cid:126)1nH ) 1

2 is the

Kernel selection. In real applications  such as bioinformatics  graphs may have discrete labels
and (multi-dimensional) real-valued attributes. Hence  three attributes domains are involved in the
computation of our graph kernels: the structural role attribute domain A0  the discrete attribute domain
Ad  and the continuous attribute domain Ac. For Ad  we can use the Delta kernel kd(a  b) = I{a=b}.
For A0 and Ac  which are just the Euclidean spaces  we can use the Gaussian RBF kernel  the
Laplacian RBF kernel  or the polynomial kernel.

5 Approximated Hilbert space embedding of graphs

Based on the above discussions  we see that obtaining a graph kernel value between each pair of
graphs requires calculating the inner product or the L2 distance between two Hilbert embeddings
(see (8a) and (8b))  both of which scale quadratically to the node numbers. Such time complexity
precludes application to large graph datasets. To tackle the above issues  we employ the recently
emerged approximate explicit feature maps [23].
For a kernel kl on the attribute domain Al  l = 0  1  ...  L  we ﬁnd an explicit map ˆφ : Al → RDl  so
that

∀a  b ∈ Al (cid:104) ˆφ(a)  ˆφ(b)(cid:105) = ˆkl(a  b)  and ˆkl(a  b) → kl(a  b) as Dl → ∞.

(9)
The explicit feature maps will be directly used to compute the approximate graph embeddings  by
virtue of tensor algebra (see Section 2.2). The following theorem says that the approximate explicit
graph embeddings can be written as the linear combination of rank-one tensors.
Theorem 2. Let G and H be any two graphs in G.
Let {((cid:126)pi  a1
i=1 and
i   a2
{((cid:126)qj  b1
j=1 be the respective set representations of G and H. Then their approxi-
mate explicit graph embeddings  ˆmG and ˆmH  are tensors in RD0×D1×...×DL  and can be written as

j )}nH

i )}nG

i   ...  aL

j   ...  bL

j   b2

ˆmG =

1
nG

ˆφ0((cid:126)pi)◦ ˆφ1(a1

i )◦ ...◦ ˆφL(aL

i )  ˆmH =

1
nH

ˆφ0((cid:126)qj)◦ ˆφ1(b1

j )◦ ...◦ ˆφL(bL

j ). (10)

That is  as D0  D1  ...  DL → ∞  we have (cid:104) ˆmG  ˆmH(cid:105)T → (cid:104)mG  mH(cid:105)H.

6

nG(cid:88)

i=1

nH(cid:88)

j=1

5.1 Graph Kernels (II)

(cid:80)nG

i=1

With approximate tensor embeddings (10)  we obtain new graph kernels.
Proposition 3. The following functions are positive deﬁnite graph kernels deﬁned on G × G.

ˆK1(G  H) = (c + (cid:104) ˆmG  ˆmH(cid:105)T )d =(cid:2)c + vec( ˆmG)T vec( ˆmH)(cid:3)d

  c ≥ 0  d ∈ N 

ˆK2(G  H) = exp(−γ(cid:107) ˆmG − ˆmH(cid:107)pT ) = exp(−γ(cid:107)vec( ˆmG) − vec( ˆmH )(cid:107)p

(11a)
2)  γ > 0  0 < p ≤ 2..
(11b)
Moreover  as D0  D1  ...  DL → ∞  we have ˆK1(G  H) → K1(G  H) and ˆK2(G  H) → K2(G  H).
The vectorization of ˆmG (or ˆmH) can be easily implemented by the Kronecker product  i.e. 
i ). To obtain above graph kernels  we need only
vec( ˆmG) = 1
nG
to compute the Euclidean inner product or distance between vectors. More notably  the size of the
tensor representation does not depends on node numbers  making it scalable to large graphs.
Approximate explicit feature map selection. For the Delta kernel on the discrete attribute domain 
we directly use the one-hot vector. For shift-invariant kernels  i.e.  k((cid:126)x  (cid:126)y) = k((cid:126)x− (cid:126)y)  on Euclidean
spaces  e.g.  A0 and Ac  we make use of random Fourier feature map [23]  ˆφ : Rd → RD  satisfying
(cid:104) ˆφ((cid:126)x)  ˆφ((cid:126)y)(cid:105) ≈ k((cid:126)x  (cid:126)y). To do this  we ﬁrst draw D i.i.d. samples ω1  ω2  ...  ωD from a proper
distribution p(ω). (Note that in this paper  we use p(ω) =
2σ2 ).) Next  we draw
D i.i.d. samples b1  b2  ...  bD from the uniform distribution on [0  2π]. Finally  we can calculate
ˆφ((cid:126)x) =

D (cid:126)x + bD)(cid:3)T ∈ RD.

2πσ)D exp(−(cid:107)ω(cid:107)2

ˆφ0((cid:126)pi)⊗ ˆφ1(a1

i )⊗ ...⊗ ˆφL(aL

(cid:2) cos(ωT

1 (cid:126)x + b1)  ...  cos(ωT

(cid:113) 2

D

1

√
(

2

6 Experiments

In this section  we conduct extensive experiments to demonstrate the effectiveness of our graph
kernels. We run all the experiments on a laptop with an Intel i7-7820HQ  2.90GHz CPU and 64GB
RAM. We implement our algorithms in Matlab  except for the Monte Carlo based computation of
RPF (see Section 3.2 1)  which is implemented in C++.

6.1 Datasets

We conduct graph classiﬁcation on four types of benchmark datasets [16]. (i) Non-attributed (unla-
beled) graphs datasets: COLLAB  IMDB-BINARY  IMDB-MULTI  REDDIT-BINARY  REDDIT-
MULTI(5K)  and REDDIT-MULTI(12K) [31] are generated from social networks. (ii) Graphs with
discrete attributes (labels): DD [8] are proteins. MUTAG [7]  NCI1 [25]  PTC-FM  PTC-FR  PTC-
MM  and PTC-MR [13] are chemical compounds. (iii) Graphs with continuous attributes: FRANK is
a chemical molecule dataset [15]. SYNTHETIC and Synthie are synthetic datasets based on random
graphs  which were ﬁrst introduced in [9] and [19]  respectively. (iv) Graphs with both discrete
and continuous attributes: ENZYMES and PROTEINS [2] are graph representations of proteins.
BZR  COX2  and DHFR [27] are chemical compounds. Detailed descriptions  including statistical
properties  of these 21 datasets are provided in the supplementary material.

6.2 Experimental setup

We demonstrate both the graph kernels (I) and (II) introduced in Section 4.1 and Section 5.1  which are
denoted by RetGKI and RetGKII  respectively. The Monte Carlo computation of return probability
features  denoted by RetGKII(MC)  is also considered. In our experiments  we repeat 200 Monte
Carlo trials  i.e.  M = 200  for obtaining RPF. For handling the isolated nodes  whose degrees are
zero  we artiﬁcially add a self-loop for each node in graphs.
Parameters. In all experiments  we set the random walk step S = 50. For RetGKI  we use the
Laplacian RBF kernel for both the structural role domain A0  and the continuous attribute domain Ac 
i.e.  k0((cid:126)p  (cid:126)q) = exp(−γ0(cid:107)(cid:126)p − (cid:126)q(cid:107)2) and kc((cid:126)a  (cid:126)b) = exp(−γc(cid:107)(cid:126)a − (cid:126)b(cid:107)2). We set γ0 to be the inverse
of the median of all pairwise distances  and set γc to be the inverse of the square root of the attributes’
dimension  except for the FRANK dataset  whose γc is set to be the recommended value
0.0073

√

7

distp   where dist is the median of all the pairwise graph embedding distances.

in the paper [22] and [19]. For RetGKII  on the ﬁrst three types of graphs  we set the dimensions
of random Fourier feature maps on A0 and Ac both to be 200  i.e.  D0 = Dc = 200  except for the
FRANK dataset  whose Dc is set to be 500 because its attributes lie in a much higher dimensional
space. On the graphs with both discrete and continuous attributes  for the sake of computational
efﬁciency  we set D0 = Dc = 100. For both RetGKI and RetGKII  we make use of the graph
kernels with exponential forms  exp(−γ(cid:107) · (cid:107)p)  (see (8b) and (11b)). We select p from {1  2}  and
set γ = 1
We compare our graph kernels with many state-of-the-art graph classiﬁcation algorithms: (i) the
shortest path kernel (SP) [1]  (ii) the Weisfeiler-Lehman subtree kernel (WL) [25]  (iii) the graphlet
count kernel (GK)[26]  (iv) deep graph kernels (DGK) [31]  (v) PATCHY-SAN convolutional neural
network (PSCN) [20]  (vi) deep graph convolutional neural network (DGCNN) [33]  (vii) graph
invariant kernels (GIK) [22]  (viii) hashing Weisfeiler-Lehman graph kernels (HGK(WL)) [19]  and
(IX) subgraph matching kernels (CSM) [18].
For all kinds of graph kernels  we employ SVM [3] as the ﬁnal classiﬁer. The tradeoff parameter C
is selected from {10−3  10−2  10−1  1  10  102  103}. We perform 10-fold cross-validations  using
9 folds for training and 1 for testing  and repeat the experiments 10 times. We report average
classiﬁcation accuracies and standard errors.

6.3 Experimental Results

The classiﬁcation results4 on four types of datasets are shown in Tables 1  2  3  and 4. The best
results are highlighted in bold. We also report the total time of computing the graph kernels of all the
datasets in each table. It can be seen that graph kernels RetGKI and RetGKII both achieve superior
or comparable performance on all the benchmark datasets. Especially on the datasets COLLAB 
REDDIT-BINARY  REDDIT-MULTI(12K)  Synthie  BZR  COX2  our approaches signiﬁcantly
outperform other state-of-the-art algorithms. The classiﬁcation accuracies of our approaches on
these datasets are at least six percentage points higher than those of the best baseline algorithms.
Moreover  we see that RetGKII and RetGKII(MC) are faster than baseline methods. Their running
times remain perfectly practical. On the large social network datasets (see Table 1)  RetGKII(MC)
is almost one order of magnitude faster than the Weisfeiler-Lehman subtree kernel  which is well
known for its computational efﬁciency.

6.4 Sensitivity analysis

Here  we conduct a parameter sensitivity analysis of RetGKII on the datasets REDDIT-BINARY 
NCI1  SYNTHETIC  Synthie  ENZYMES  and PROTEINS. We test the stability of RetGKII by
varying the values of the random walk steps S  the dimension D0 of the approximate explicit feature
map on A0  and the dimension Dc of the feature map on Ac. We plot the average classiﬁcation
accuracy of ten repetitions of 10-fold cross-validations with respect to S  D0  and Dc in Fig. 3. It can
be concluded that RetGKII performs consistently across a wide range of parameter values.

Table 1: Classiﬁcation results (in %) for non-attributed (unlabeled) graph datasets

Datasets
COLLAB

IMDB-BINARY
IMDB-MULTI

REDDIT-BINARY

REDDIT-MULTI(5K)
REDDIT-MULTI(12K)

Total time

WL

74.8(0.2)
70.8(0.5)
49.8(0.5)
68.2(0.2)
51.2(0.3)
32.6(0.3)

2h3m

GK

72.8(0.3)
65.9(1.0)
43.9(0.4)
77.3(0.2)
41.0(0.2)
31.8(0.1)

–

DGK

73.1(0.3)
67.0(0.6)
44.6(0.5)
78.0(0.4)
41.3(0.2)
32.2(0.1)

–

PSCN
72.6(2.2)
71.0(2.3)
45.2(2.8)
86.3(1.6)
49.1(0.7)
41.3(0.4)

–

RetGKI
81.0(0.3)
71.9(1.0)
47.7(0.3)
92.6(0.3)
56.1(0.5)
48.7(0.2)
48h14m

RetGKII
80.6(0.3)
72.3(0.6)
48.7(0.6)
91.6(0.2)
55.3(0.3)
47.1(0.3)
17m14s

RetGKII(MC)

73.6(0.3)
71.0(0.6)
46.7(0.6)
90.8(0.2)
54.2(0.3)
45.9(0.2)

6m9s

4The accuracies of WL  SP and GK are obtained from our own experiments. For others competing algorithms 

we directly quote the values from their papers.

8

Table 2: Classiﬁcation results (in %) for graph datasets with discrete attributes

Datasets

ENZYMES
PROTEINS
MUTAG

DD
NCI1

PTC-FM
PTC-FR
PTC-MM
PTC-MR
Total time

SP

38.6(1.5)
73.3(0.9)
85.2(2.3)

>24h

74.8(0.4)
60.5(1.7)
61.6(1.0)
62.9(1.4)
57.8(2.1)

>24h

WL

53.4(0.9)
71.2(0.8)
84.4(1.5)
78.6(0.4)
85.4(0.3)
55.2(2.3)
63.9(1.4)
60.6(1.1)
55.4(1.5)
2m27s

GK
–

71.7(0.6)
81.6(2.1)
78.5(0.3)
62.3(0.3)

–
–
–

–

CSM

60.4(1.6)

85.4(1.2)

63.8(1.0)
65.5(1.4)
63.3(1.7)
58.1(1.6)

–

–
–

–

57.3(1.1)

58.6(2.5)

60.1(2.6)

62.3(5.7)

DGCNN

75.5(0.9)
85.8(1.7)
79.4(0.9)
74.4(0.5)

DGK

53.4(0.9)
75.7(0.5)
87.4(2.7)

80.3(0.5)

PSCN

–

75.0(2.5)
89.0(4.4)
76.2(2.6)
76.3(1.7)

–

–
–
–

–

–
–
–

–

–

–
–
–

–

RetGKI
60.4(0.8)
75.8(0.6)
90.3(1.1)
81.6(0.3)
84.5(0.2)
62.3(1.0)
66.7(1.4)
65.6(1.1)
62.5(1.6)
38m4s

RetGKII
59.1(1.1)
75.2(0.3)
90.1(1.0)
81.0(0.5)
83.5(0.2)
63.9(1.3)
67.8(1.1)
67.9(1.4)
62.1(1.5)

49.9s

Figure 3: Parameter sensitivity study for RetGKII on six benchmark datasets

Table 3: Classiﬁcation results (in %) for
graph datasets with continuous attributes

Table 4: Classiﬁcation results (in %) for graph
datasets with both discrete and continuous attributes

Datasets

ENZYMES
PROTEINS

FRANK

SYNTHETIC

Synthie
Total time

HGK(WL)
63.9(1.1)
74.9(0.6)
73.2(0.3)
97.6(0.4)
80.3(1.4)

–

RetGKI
70.0(0.9)
76.2(0.5)
76.4(0.3)
97.9(0.3)
97.1(0.3)
45m30s

RetGKII
70.7(0.9)
75.9(0.4)
76.7(0.4)
98.9(0.4)
96.2(0.3)

40.8s

Datasets

ENZYMES
PROTEINS

BZR
COX2
DHFR

Total time

GIK

71.7(0.8)
76.1(0.3)

–
–
–
–

CSM

69.8(0.7)

79.4(1.2)
74.4(1.7)
79.9(1.1)

–

–

RetGKI
72.2(0.8)
78.0(0.3)
86.4(1.2)
80.1(0.9)
81.5(0.9)
4m17s

RetGKII
70.6(0.7)
77.3(0.5)
87.1(0.7)
81.4(0.6)
82.5(0.8)
2m51s

7 Conclusion

In this paper  we introduced the return probability feature for characterizing and comparing the
structural role of nodes across graphs. Based on the RPF  we embedded graphs in an RKHS
and derived the corresponding graph kernels RetGKI. Then  making use of approximate explicit
feature maps  we represented each graph with a multi-dimensional tensor  and then obtained the
computationally efﬁcient graph kernels RetGKII. We applied RetGKI and RetGKII to classify
graphs  and achieved promising results on many benchmark datasets. Given the prevalence of
structured data  we believe that our work can be potentially useful in many applications.

8 Acknowledgement

This work was supported in part by the AFOSR grant FA9550-16-1-0386.

References
[1] Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Data Mining 

Fifth IEEE International Conference on  pages 8–pp. IEEE  2005.

9

102030405060708090100S6065707580859095100classification accuracy (%)105010020050010002000D06065707580859095100classification accuracy (%)REDDIT-BINARYNCI1SYNTHETICSynthieENZYMESPROTEINS105010020050010002000Dc6065707580859095100classification accuracy (%)[2] Karsten M Borgwardt  Cheng Soon Ong  Stefan Schönauer  SVN Vishwanathan  Alex J
Smola  and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics 
21(suppl_1):i47–i56  2005.

[3] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM

transactions on intelligent systems and technology (TIST)  2(3):27  2011.

[4] Fan RK Chung. Spectral graph theory. Number 92. American Mathematical Soc.  1997.

[5] Erhan Cinlar. Introduction to stochastic processes. Courier Corporation  2013.

[6] Giovanni Da San Martino  Nicolò Navarin  and Alessandro Sperduti. Tree-based kernel for
graphs with continuous attributes. IEEE transactions on neural networks and learning systems 
2017.

[7] Asim Kumar Debnath  Rosa L Lopez de Compadre  Gargi Debnath  Alan J Shusterman  and
Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic
nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of
medicinal chemistry  34(2):786–797  1991.

[8] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes

without alignments. Journal of molecular biology  330(4):771–783  2003.

[9] Aasa Feragen  Niklas Kasenburg  Jens Petersen  Marleen de Bruijne  and Karsten Borgwardt.
Scalable kernels for graphs with continuous attributes. In Advances in Neural Information
Processing Systems  pages 216–224  2013.

[10] Thomas Gärtner  Peter Flach  and Stefan Wrobel. On graph kernels: Hardness results and
efﬁcient alternatives. In Learning Theory and Kernel Machines  pages 129–143. Springer  2003.

[11] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

[12] David Haussler. Convolution kernels on discrete structures. Technical report  Technical report 

Department of Computer Science  University of California at Santa Cruz  1999.

[13] Christoph Helma  Ross D. King  Stefan Kramer  and Ashwin Srinivasan. The predictive

toxicology challenge 2000–2001. Bioinformatics  17(1):107–108  2001.

[14] Jaz Kandola  Thore Graepel  and John Shawe-Taylor. Reducing kernel matrix diagonal dom-
inance using semi-deﬁnite programming. In Learning Theory and Kernel Machines  pages
288–302. Springer  2003.

[15] Jeroen Kazius  Ross McGuire  and Roberta Bursi. Derivation and validation of toxicophores for

mutagenicity prediction. Journal of Medicinal Chemistry  48(1):312–320  2005.

[16] Kristian Kersting  Nils M. Kriege  Christopher Morris  Petra Mutzel  and Marion Neumann.
Benchmark data sets for graph kernels  2016. http://graphkernels.cs.tu-dortmund.de.

[17] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review 

51(3):455–500  2009.

[18] Nils Kriege and Petra Mutzel. Subgraph matching kernels for attributed graphs. In ICML  2012.

[19] Christopher Morris  Nils M Kriege  Kristian Kersting  and Petra Mutzel. Faster kernels for
In Data Mining (ICDM)  2016 IEEE 16th

graphs with continuous attributes via hashing.
International Conference on  pages 1095–1100. IEEE  2016.

[20] Mathias Niepert  Mohamed Ahmed  and Konstantin Kutzkov. Learning convolutional neural
networks for graphs. In International conference on machine learning  pages 2014–2023  2016.

[21] Giannis Nikolentzos  Polykarpos Meladianos  and Michalis Vazirgiannis. Matching node

embeddings for graph similarity. In AAAI  pages 2429–2435  2017.

10

[22] Francesco Orsini  Paolo Frasconi  and Luc De Raedt. Graph invariant kernels. In Proceedings
of the Twenty-fourth International Joint Conference on Artiﬁcial Intelligence  pages 3756–3762 
2015.

[23] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[24] Tiago A Schieber  Laura Carpi  Albert Díaz-Guilera  Panos M Pardalos  Cristina Masoller  and
Martín G Ravetti. Quantiﬁcation of network structural dissimilarities. Nature communications 
8:13928  2017.

[25] Nino Shervashidze  Pascal Schweitzer  Erik Jan van Leeuwen  Kurt Mehlhorn  and Karsten M
Journal of Machine Learning Research 

Borgwardt. Weisfeiler-lehman graph kernels.
12(Sep):2539–2561  2011.

[26] Nino Shervashidze  SVN Vishwanathan  Tobias Petri  Kurt Mehlhorn  and Karsten Borgwardt.
Efﬁcient graphlet kernels for large graph comparison. In Artiﬁcial Intelligence and Statistics 
pages 488–495  2009.

[27] Jeffrey J Sutherland  Lee A O’brien  and Donald F Weaver. Spline-ﬁtting with a genetic
algorithm: A method for developing classiﬁcation structure- activity relationships. Journal of
chemical information and computer sciences  43(6):1906–1915  2003.

[28] Zoltán Szabó and Bharath K Sriperumbudur. Characteristic and universal tensor product kernels.

arXiv preprint arXiv:1708.08157  2017.

[29] Saurabh Verma and Zhi-Li Zhang. Hunt for the unique  stable  sparse and fast feature learning

on graphs. In Advances in Neural Information Processing Systems  pages 87–97  2017.

[30] S Vichy N Vishwanathan  Nicol N Schraudolph  Risi Kondor  and Karsten M Borgwardt. Graph

kernels. Journal of Machine Learning Research  11(Apr):1201–1242  2010.

[31] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
1365–1374. ACM  2015.

[32] Pinar Yanardag and SVN Vishwanathan. A structural smoothing framework for robust graph
comparison. In Advances in Neural Information Processing Systems  pages 2134–2142  2015.

[33] Muhan Zhang  Zhicheng Cui  Marion Neumann  and Yixin Chen. An end-to-end deep learning
architecture for graph classiﬁcation. In Proceedings of AAAI Conference on Artiﬁcial Inteligence 
2018.

11

,Zhen Zhang
Mianzhi Wang
Yijian Xiang
Yan Huang
Arye Nehorai