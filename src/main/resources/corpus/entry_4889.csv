2013,Scalable kernels for graphs with continuous attributes,While graphs with continuous node attributes arise in many applications  state-of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity; for instance  the popular shortest path kernel scales as $\mathcal{O}(n^4)$  where $n$ is the number of nodes. In this paper  we present a class of path kernels with computational complexity $\mathcal{O}(n^2 (m + \delta^2))$  where $\delta$ is the graph diameter and $m$ the number of edges. Due to the sparsity and small diameter of real-world graphs  these kernels scale comfortably to large graphs. In our experiments  the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets.,Scalable kernels for graphs with continuous attributes

Aasa Feragen  Niklas Kasenburg

Machine Learning and Computational Biology Group

Max Planck Institutes T¨ubingen and DIKU  University of Copenhagen

{aasa niklas.kasenburg}@diku.dk

Jens Petersen1 

Marleen de Bruijne1 2

1DIKU  University of Copenhagen
2 Erasmus Medical Center Rotterdam
{phup marleen}@diku.dk

Karsten Borgwardt

Machine Learning and Computational Biology Group

Max Planck Institutes T¨ubingen

Eberhard Karls Universit¨at T¨ubingen

karsten.borgwardt@tuebingen.mpg.de

Abstract

While graphs with continuous node attributes arise in many applications  state-
of-the-art graph kernels for comparing continuous-attributed graphs suffer from
a high runtime complexity. For instance  the popular shortest path kernel scales
as O(n4)  where n is the number of nodes. In this paper  we present a class of
graph kernels with computational complexity O(n2(m + log n + δ2 + d))  where
δ is the graph diameter  m is the number of edges  and d is the dimension of
the node attributes. Due to the sparsity and small diameter of real-world graphs 
these kernels typically scale comfortably to large graphs.
In our experiments 
the presented kernels outperform state-of-the-art kernels in terms of speed and
accuracy on classiﬁcation benchmark datasets.

1

Introduction

Graph-structured data appears in many application domains of machine learning  reaching from
Social Network Analysis to Computational Biology. Comparing graphs to each other is a funda-
mental problem in learning on graphs  and graph kernels have become an efﬁcient and widely-used
method for measuring similarity between graphs. Highly scalable graph kernels have been proposed
for graphs with thousands and millions of nodes  both for graphs without node labels [1] and for
graphs with discrete node labels [2]. Such graphs appear naturally in applications such as natural
language processing  chemoinformatics and bioinformatics. For applications in medical image anal-
ysis  computer vision or even bioinformatics  however  continuous-valued physical measurements
such as shape  relative position or other measured node properties are often important features for
classiﬁcation. An open challenge  which is receiving increased attention  is to develop a scalable
kernel on graphs with continuous-valued node attributes.
We present the GraphHopper kernel between graphs with real-valued edge lengths and any type of
node attribute  including vectors. This kernel is a convolution kernel counting sub-path similarities.
The computational complexity of this kernel is O(n2(m + log n + δ2 + d))  where n and m are the
number of nodes and edges  respectively; δ is the graph diameter; and d is the dimension of the node
attributes. Although δ = n or m = n2 in the worst case  this is rarely the case in real-world graphs 
as is also illustrated by our experiments. We ﬁnd empirically in Section 3.1 that our GraphHopper
kernel tends to scale quadratically with the number of nodes on real data.

1

1.1 Related work

Many popular kernels for structured data are sums of substructure kernels:

k(G  G(cid:48)) =

ksub(s  s(cid:48)).

(cid:88)

(cid:88)

s∈S

s(cid:48)∈S (cid:48)

Here G and G(cid:48) are structured data objects such as strings  trees and graphs with classes S and S (cid:48)
of substructures  and ksub is a substructure kernel. Such k are instances of R-convolution kernels [3].
A large variety of kernels exist for structures such as strings [4  5]  ﬁnite state transducers [6] and
trees [5  7]. For graphs in general  kernels can be sorted into categories based on the types of
attributes they can handle. The graphlet kernel [1] compares unlabeled graphs  whereas several
kernels allow node labels from a ﬁnite alphabet [2  8]. While most kernels have a runtime that
is at least O(n3)  the Weisfeiler-Lehman kernel [2] uses efﬁcient sorting  hashing and counting
algorithms that take advantage of repeated occurrences of node labels from the ﬁnite label alphabet 
and achieves a runtime which is at most quadratic in the number of nodes. Unfortunately  this does
not generalize to graphs with vector-valued node attributes  which are typically all distinct samples
from an inﬁnite alphabet.
The ﬁrst kernel to take advantage of non-discrete node labels was the random walk kernel [9–11].
It incorporates edge probabilities and geometric node attributes [12]  but suffers from tottering [13]
and is empirically slow. Kriege et al. [14] adopt the idea of comparing matched subgraphs  includ-
ing vector-valued attributes on nodes and edges. However  this kernel has a high computational
and memory cost  as we will see in Section 3. Other kernels handling non-discrete attributes use
edit-distance and subtree enumeration [15]. While none of these kernels scale well to large graphs 
the propagation kernel [16] is fast asymptotically and empirically.
It translates the problem of
continuous-valued attributes to a problem of discrete-valued labels by hashing node attributes. Nev-
ertheless  its performance depends strongly on the hashing function and in our experiments it is
outperformed in classiﬁcation accuracy by kernels which do not discretize the attributes.
In problems where continuous-valued node attributes and inter-node distance dG(v  w) along the
graph G are important features  the shortest path kernel [17]  deﬁned as

kSP (G  G(cid:48)) =

kn(v  v(cid:48)) · kl (dG(v  w)  dG(cid:48)(v(cid:48)  w(cid:48))) · kn(w  w(cid:48)) 

(cid:88)

(cid:88)

v w∈V

v(cid:48) w(cid:48)∈V (cid:48)

performs well in classiﬁcation. In particular  kSP allows the user to choose any kernels kn and kl on
nodes and shortest path length. However  the asymptotic runtime of kSP is generally O(n4)  which
makes it unfeasible for many real-world applications.

1.2 Our contribution

In this paper we present a kernel which also compares shortest paths between node pairs from the two
graphs  but with a different path kernel. Instead of comparing paths via products of kernels on their
lengths and endpoints  we compare paths through kernels on the nodes encountered while ”hopping”
along shortest paths. This particular path kernel allows us to decompose the graph kernel as a
weighted sum of node kernels  initially suggesting a potential runtime as low as O(n2d). The graph
structure is encoded in the node kernel weights  and the main algorithmic challenge becomes to
efﬁciently compute these weights. This is a combinatorial problem  which we solve with complexity
O(n2(m + log n + δ2)). Note  moreover  that the GraphHopper kernel is parameter-free except for
the choice of node kernels.
The paper is organized as follows. In Section 2 we give short formal deﬁnitions and proceed to
deﬁning our kernel and investigating its computational properties. Section 3 presents experimental
classiﬁcation results on different datasets in comparison to state-of-the-art kernels as well as empir-
ical runtime studies  before we conclude with a discussion of our ﬁndings in Section 4.

2 Graphs  paths and GraphHoppers
We shall compare undirected graphs G = (V  E) with edge lengths l : E → R+ and node attributes
A : V → X from a set X  which can be any set with a kernel kn; in our data X = Rd. Denote

2

n = |V | and m = |E|. A subtree T ⊂ G is a subgraph of G which is a tree. Such subtrees inherit
node attributes and edge lengths from G by restricting the attribute and length maps A and l to the
new node and edge sets  respectively. For a tree T = (V  E  r) with a root node r  let p(v) and c(v)
denote the parent and the children of any v ∈ V .
Given nodes va  vb ∈ V   a path π from va to vb in G is deﬁned as a sequence of nodes

π = [v1  v2  v3  . . .   vn]  

where v1 = va  vn = vb and [vi  vi+1] ∈ E for all i = 1  . . .   n − 1. Let π(i) = vi denote the ith
node encountered when ”hopping” along the path. Given paths π and π(cid:48) from v to w and from w to
u  respectively  let [π  π(cid:48)] denote their composition  which is a path from v to u. Denote by l(π) the
weighted length of π  given by the sum of lengths l(vi  vi+1) of edges traversed along the path  and
denote by |π| the discrete length of π  deﬁned as the number of nodes in π. The shortest path πab
from va to vb is deﬁned in terms of weighted length; if no edge length function is given  set l(e) = 1
for all e ∈ E as default. The diameter δ(G) of G is the maximal number of nodes in a shortest path
in G  with respect to weighted path length.
In the next few lemmas we shall prove that for a ﬁxed a source node v ∈ V   the directed edges along
shortest paths from v to other nodes of G form a well-deﬁned directed acyclic graph (DAG)  that is 
a directed graph with no cycles.
First of all  subpaths of shortest paths πvw with source node v are shortest paths as well:
Lemma 1.
[18  Lemma 24.1] If π1n = [v1  . . .   vn] is a shortest path from v1 = v to vn  then the
(cid:3)
path π1n(1 : i) consisting of the ﬁrst i nodes of π1n is a shortest path from v1 = v to vi.
Given a source node v ∈ G  construct the directed graph Gv = (Vv  Ev) consisting of all nodes Vv
from the connected component of v in G and the set Ev of all directed edges found in any shortest
path from v to any given node w in Gv. Any directed walk from v in Gv is a shortest path in G:
Lemma 2 If π1n is a shortest path from v1 = v to vn and (vn  vn+1) ∈ Ev  then [π1n  [vn  vn+1]]
is a shortest path from v1 = v to vn+1.
Proof. Since (vn  vn+1) ∈ Ev  there is a shortest path π1(n+1) = [v1  . . .   vn  vn+1] from v1 = v to
vn+1. If this path is shorter than [π1n  [vn  vn+1]]  then π1(n+1)(1 : n) is a shortest path from v1 = v
to vn by Lemma 1  and it must be shorter than π1n. This is impossible  since π1n is a shortest path.(cid:3)
Proposition 3 The shortest path graph Gv is a DAG.
Proof. Assume  on the contrary  that Gv contains a cycle c = [v1  . . .   vn] where (vi  vi+1) ∈ Ev
for each i = 1  . . .   n − 1 and v1 = vn. Let πv1 be the shortest path from v to v1. Using Lemma 2
repeatedly  we see that the path [πv1  c] is a shortest path from v to vn = v1  which is impossible
(cid:3)
since the new path must be longer than the shortest path πv1.

2.1 The GraphHopper kernel
We deﬁne the GraphHopper kernel as a sum of path kernels kp over the families P  P(cid:48) of shortest
paths in G  G(cid:48):

(cid:88)

In this paper  the path kernel kp(π  π(cid:48)) is a sum of node kernels kn on nodes simultaneously encoun-
tered while simultaneously hopping along paths π and π(cid:48) of equal discrete length  that is:

kp(π  π(cid:48)) =

k(G  G(cid:48)) =

kp(π  π(cid:48)) 

π∈P π(cid:48)∈P(cid:48)

(cid:26) (cid:80)|π|
j=1 kn (π(j)  π(cid:48)(j))
(cid:88)

(cid:88)

0

k(G  G(cid:48)) =

w(v  v(cid:48))kn(v  v(cid:48)) 

v∈V

v(cid:48)∈V (cid:48)

if |π| = |π(cid:48)| 
otherwise.

It is clear from the deﬁnition that k(G  G(cid:48)) decomposes as a sum of node kernels:

(4)

(5)

where w(v  v(cid:48)) counts the number of times v and v(cid:48) appear at the same hop  or coordinate  i of
shortest paths π  π(cid:48) of equal discrete length |π| = |π(cid:48)|. We can decompose the weight w(v  v(cid:48)) as

w(v  v(cid:48)) =

(cid:93){(π  π(cid:48))|π(i) = v  π(cid:48)(i) = v(cid:48) |π| = |π(cid:48)| = j} = (cid:104)M (v)  M (v(cid:48))(cid:105) 

δ(cid:88)

δ(cid:88)

j=1

i=1

3

Figure 1: Top: Expansion from the graph G  to the DAG G˜v  to a larger tree S˜v. Bottom left:
Recursive computation of the ov
r in a
rooted tree as in Algorithm 2  and of the dv

˜v. Bottom middle and right: Recursive computation of the dv

˜v on a DAG G˜v as in Algorithm 3.

where M (v) is a δ × δ matrix whose entry [M (v)]ij counts how many times v appears at the ith
coordinate of a shortest path in G of discrete length j  and δ = max{δ(G)  δ(G(cid:48))}. More precisely 
[M (v)]ij = number of times v appears as the ith node on a shortest path of discrete length j

˜v∈V number of times v appears as ith node on a shortest path from ˜v
of discrete length j
˜v∈V D˜v(v  j − i + 1)O˜v(v  i).

=(cid:80)
=(cid:80)

(6)
Here D˜v is a n× δ matrix whose (v  i)-coordinate counts the number of directed walks with i nodes
starting at v in the shortest path DAG G˜v. The O˜v is a n × δ matrix whose (v  i)-coordinate counts
the number of directed walks from ˜v to v in G˜v with i nodes. Given the matrices D˜v and O˜v  we
compute all M (v) by looping through all choices of source node ˜v ∈ V   adding up the contributions
M˜v to M (v) from each ˜v  as detailed in Algorithm 4.
˜v  is computed recursively by message-passing from the root  as
The vth row of O˜v  denoted ov
˜v consists of the nodes v ∈ V for which the shortest
detailed in Figure 1 and Algorithm 1. Here  V j
paths π˜vv of highest discrete length have j nodes. Algorithm 1 sends one message of size at most δ
per edge  thus has complexity O(mδ).
To compute the vth row of D˜v  denoted dv
˜v are
computed easily for trees using a message-passing algorithm as follows. Let T = (V  E  r) be a tree
r counts the number of paths from v in T of
with a designated root node r. The ith coefﬁcient of dv
discrete length i  directed from the root. This is just the number of descendants of v at level i below
v in T . Let ⊕ denote left aligned addition of vectors of possibly different length  e.g.

˜v  we draw inspiration from [19] where the vectors dv

[a  b  c] ⊕ [d  e] = [(a + d)  (b + e)  c].

(7)

Using ⊕  the dv

r can be expressed recursively:
dv
r = [1]

(cid:77)

p(w)=v

[0  dw

r ].

˜v for all v  on G˜v

˜v = [1]; ov

˜v = [0] ∀ v ∈ V \ {˜v}.

Algorithm 1 Message-passing algorithm for computing ov
1: Initialize: o˜v
2: for j = 1 . . . δ do
for v ∈ V j
˜v do
3:
4:
5:
6:
7:
8: end for

for (v  w) ∈ E˜v do
˜v ⊕ [0  ov
˜v]
end for

ow
˜v = ow

end for

4

11213212112132211213222210 10 1 1+ 0 1+ 0 1+ 0 1+ 0 0 10 1 1+ 0 0 1+ 0 0 1+ 0 0 1 1+ 0 0 1 10 0 2 1+ 0 0 10 0 2 1111+ 0 1+ 0 11 2+ 0 1+ 0 1 21 2 2+ 0 1 2 21 1 2 2111+ 0 1+ 0 11 2+ 0 1+ 0 1+ 0 1+ 0 1 2+ 0 1+ 0 1 21 4 2+ 0 1 4 21 3 6 2Algorithm 2 Recursive computation of dv
1: Initialize: dv
2: for e = (v  c(v)) ∈ E do
3:
4: end for

r = [1] ∀ v ∈ V .
r ⊕ [0  dc(v)

dv
r = dv

]

r

r for all v on T = (V  E  r).

Algorithm 3 Recursive computation of dv
˜v = [1] ∀ v ∈ V .
1: Initialize: dv
2: for e = (v  c(v)) ∈ EG do
˜v ⊕ [0  dc(v)
3:
4: end for

dv
˜v = dv

˜v

]

˜v for all v on G˜v

r for all v ∈ V are computed recursively  sending counters along the edges from the leaf nodes
The dv
towards the root  recording the number of descendants of any node at any level  see Algorithm 2 and
r for all v ∈ V are computed in O(nh) time  where h is tree height  since each edge
Figure 1. The dv
passes exactly one message of size ≤ h.
On a DAG  computing dv
˜v is a little more complex. Note that the DAG G˜v generated by all shortest
paths from ˜v ∈ V can be expanded into a rooted tree S˜v by duplicating any node with several
incoming edges  see Figure 1. The tree S˜v contains  as a path from the root ˜v to one of the nodes
labeled v in S˜v  any shortest path from ˜v to v in G. However  the number of nodes in S˜v could  in
theory  be exponential in n  making computation of dv
˜v by message-passing on S˜v intractable. Thus 
we shall compute the dv
˜v in S˜v are given by
˜v ]  where ⊕ is deﬁned in (7). This observation leads to an algorithm in
dv
(w v)∈E˜v
which each edge e ∈ E˜v passes exactly one vector of size ≤ δ + 1 in the direction of the root ˜v 
starting at the leaves of the DAG G˜v and computing updated descendant vectors for each receiving
node. See Algorithm 3 and Figure 1. The complexity of Algorithm 3  which computes dv
˜v for all
v ∈ V   is O(|E˜v|δ) ≤ O(mδ).

˜v on the DAG G˜v rather than on S˜v. As on trees  the dv
[0  dw

˜v = [1] ⊕(cid:76)

2.2 Computational complexity analysis
Given the w(v  v(cid:48)) and the kn(v  v(cid:48)) for all v ∈ V and v(cid:48) ∈ V (cid:48)  the kernel can be computed in
O(n2) time. If we assume that each node kernel kn(v  v(cid:48)) can be computed in O(d) time (as is the
case with many standard kernels including Gaussian and linear kernels)  then all kn(v  v(cid:48)) can be
precomputed in O(n2d) time. Given the matrices M (v) and M (v(cid:48)) for all v ∈ V   v(cid:48) ∈ V (cid:48)  each
w(v  v(cid:48)) requires O(δ2) time  giving O(n2δ2) complexity for computing all weights w(v  v(cid:48)).
Note that Algorithm 4 computes M (v) for all v ∈ G simultaneously. Adding the time complexities
of the lines in each iteration of the algorithm as given on the right hand side of the individual lines
in Algorithm 4  the total complexity of one iteration of Algorithm 4 is

O(cid:0)(mn + n log n) + mδ + mδ + nδ2 + nδ2(cid:1) = O(n(m + log n + δ2)) 

Algorithm 4 Algorithm simultaneously computing all M (v)
1: Initialize: M (v) = 0 ∈ Rδ×δ for each v ∈ V .
2: for all ˜v ∈ V do
3:
4:
5:
6:

compute shortest path DAG G˜v rooted at ˜v using Dijkstra
compute D˜v(v) for each v ∈ V
compute O˜v(v) for each v ∈ V
for each v ∈ V   compute the δ × δ matrix M˜v(v) given by

(cid:26) D˜v(v  j − i + 1)O˜v(v  i) when i ≤ j

otherwise 

[M˜v(v)]ij =

0

update M (v) = M (v) + M˜v(v) for each v ∈ V

7:
8: end for

5

(O(mn + n log n))
(O(mδ))
(O(mδ))

(O(nδ2))
(O(nδ2))

giving total complexity O(n2(m+log n+δ2)) for computing M (v) for all v ∈ V using Algorithm 4.
It follows that the total complexity of computing k(G  G(cid:48)) is

O(n2 + n2d + n2δ2 + n2δ2 + n2(m + log n + δ2)) = O(n2(m + log n + d + δ2)).

When computing the kernel matrix Kij = k(Gi  Gj) for a set {Gi}N
i=1 of graphs with N > m +
n + δ2  note that Algorithm 4 only needs to be run once for every graph Gi. Thus  the average
complexity of computing one kernel value out of all Kij becomes

(cid:0)NO(n2(m + log n + δ2)) + N 2O(n2 + n2d + δ2)(cid:1) ≤ O(n2d).

1
N 2

3 Experiments

Classiﬁcation experiments were made with the proposed GraphHopper kernel and several alterna-
tives: The propagation kernel PROP [16]  the connected subgraph matching kernel CSM [14] and
the shortest path kernel SP [17] all use continuous-valued attributes. In addition  we benchmark
against the Weisfeiler-Lehman kernel WL [2]  which only uses discrete node attributes. All ker-
nels were implemented in Matlab  except for CSM  where a Java implementation was supplied by
N. Kriege. For the WL kernel  the Matlab implementation available from [20] was used. For the
GraphHopper and SP kernels  shortest paths were computed using the BGL package [21] imple-
mented in C++. The PROP kernel was implemented in two different versions  both using the total
variation hash function  as the Hellinger distance is only directly applicable to positive vector-valued
attributes. For PROP-diff  labels were propagated with the diffusion scheme  whereas in PROP-WL
labels were ﬁrst discretised via hashing and then the WL kernel [2] update was used. The bin width
of the hash function was set to 10−5 as suggested in [16]. The PROP-diff  PROP-WL and the WL
kernel were each run with 10 iterations. In the CSM kernel  the clique size parameter was set to
k = 5. Our kernel implementations and datasets (with the exception of AIRWAYS) can be found at
http://image.diku.dk/aasa/software.php.
Classiﬁcation experiments were made on four datasets: ENZYMES  PROTEINS  AIRWAYS and
SYNTHETIC. ENZYMES and PROTEINS are sets of proteins from the BRENDA database [22]
and the dataset of Dobson and Doig [23]  respectively. Proteins are represented by graphs as follows.
Nodes represent secondary structure elements (SSEs)  which are connected whenever they are neigh-
bors either in the amino acid sequence or in 3D space [24]. Each node has a discrete type attribute
(helix  sheet or turn) and an attribute vector containing physical and chemical measurements includ-
ing length of the SSE in ˚Angstrøm ( ˚A)  distance between the Cα atom of its ﬁrst and last residue
in ˚A  its hydrophobicity  van der Waals volume  polarity and polarizability. ENZYMES comes with
the task of classifying the enzymes to one out of 6 EC top-level classes  whereas PROTEINS comes
with the task of classifying into enzymes and non-enzymes. AIRWAYS is a set of airway trees ex-
tracted from CT scans of human lungs [25  26]. Each node represents an airway branch  attributed
with its length. Edges represent adjacencies between airway bronchi. AIRWAYS comes with the
task of classifying airways into healthy individuals and patients suffering from Chronic Obstructive
Pulmonary Disease (COPD). SYNTHETIC is a set of synthetic graphs based on a random graph G
with 100 nodes and 196 edges  whose nodes are endowed with normally distributed scalar attributes
sampled from N (0  1). Two classes A and B each with 150 attributed graphs were generated from
G by randomly rewiring edges and permuting node attributes. Each graph in A was generated by
rewiring 5 edges and permuting 10 node attributes  and each graph in B was generated by rewiring
10 edges and permuting 5 node attributes  after which noise from N (0  0.452) was added to every
node attribute in every graph. Detailed metrics of the datasets are found in Table 1.
Both GraphHopper  SP and CSM depend on freely selected node kernels for continuous attributes 
giving modeling ﬂexibility. For the ENZYMES  AIRWAYS and SYNTHETIC datasets  a Gaussian
node kernel kn(v  v(cid:48)) = e−λ(cid:107)A(v)−A(v(cid:48))(cid:107)2 was used on the continuous-valued attribute  with λ =
1/d. For the PROTEINS dataset  the node kernel was a product of a Gaussian kernel with λ = 1/d
and a Dirac kernel on the continuous- and discrete-valued node attributes  respectively. For the WL
kernel  discrete node labels were used when available (in ENZYMES and PROTEINS); otherwise
node degree was used as node label.
Classiﬁcation was done using a support vector machine (SVM) [27]. The SVM slack parameter was
trained using nested cross validation on 90% of the entire dataset  and the classiﬁer was tested on the

6

Number of nodes
Number of edges
Graph diameter
Node attribute dimension
Dataset size
Class size

ENZYMES

PROTEINS AIRWAYS

SYNTHETIC

32.6
46.7
12.8
18
600
6 × 100

39.1
72.8
11.6

1

1113

221
220
21.1

1

1966

100
196
7
1
300

663/450

980/986

150/150

Table 1: Data statistics: Average node and edge counts and graph diameter  dataset and class sizes.

Kernel
GraphHopper
PROP-diff [16]
PROP-WL [16]
SP [17]
CSM [14]
WL [2]

ENZYMES

69.6 ± 1.3 (12(cid:48)10(cid:48)(cid:48))
37.2 ± 2.2 (13(cid:48)(cid:48))
48.5 ± 1.3 (1(cid:48)9(cid:48)(cid:48))
71.0 ± 1.3 (3 d)
48.0 ± 0.9 (18(cid:48)(cid:48))

69.4 ± 0.8

PROTEINS

AIRWAYS

74.1 ± 0.5 (2.8 h)
66.8 ± 0.5 (1 d 7 h)
73.3 ± 0.4 (26(cid:48)(cid:48))
63.5 ± 0.5 (4(cid:48)12(cid:48)(cid:48))
73.1 ± 0.8 (2(cid:48)40(cid:48)(cid:48))
61.5 ± 0.6 (8(cid:48)17(cid:48)(cid:48))
75.5 ± 0.8 (7.7 d)
OUT OF TIME
OUT OF MEMORY OUT OF MEMORY
75.6 ± 0.5 (2(cid:48)51(cid:48)(cid:48))
62.0 ± 0.6 (7(cid:48)43(cid:48)(cid:48))

SYNTHETIC

86.6 ± 1.0 (12(cid:48)10(cid:48)(cid:48))
46.1 ± 1.9 (1(cid:48)21(cid:48)(cid:48))
44.5 ± 1.2 (1(cid:48)52(cid:48)(cid:48))
85.4 ± 2.1 (3.4 d)
OUT OF TIME
43.3 ± 2.3 (2(cid:48)8(cid:48)(cid:48))

Table 2: Mean classiﬁcation accuracies with standard deviation for all experiments  signiﬁcantly
best accuracies in bold. OUT OF MEMORY means that 100 GB memory was not enough. OUT
OF TIME indicates that the kernel computation did not ﬁnish within 30 days. Runtimes are given in
parentheses; see Section 3.1 for further runtime studies. Above  x(cid:48)y(cid:48)(cid:48) means x minutes  y seconds.

remaining 10%. This experiment was repeated 10 times. Mean accuracies with standard deviations
are reported in Table 2. For each kernel and dataset  runtime is given in parentheses in Table 2.
Runtimes for the CSM kernel are not included  as this implementation was in another language.

3.1 Runtime experiments

An empirical evaluation of the runtime dependence on the parameters n  m and δ is found in Fig-
ure 2. In the top left panel  average kernel evaluation runtime was measured on datasets of 10 random
n(n−1)/2 
graphs with 10  20  30  . . .   500 nodes each  and a density of 0.4. Density is deﬁned as
i.e. the fraction of edges in the graph compared to the number of edges in the complete graph. In the
top right panel  the number of nodes was kept constant n = 100  while datasets of 10 random graphs
were generated with 110  120  . . .   500 edges each. Development of both average kernel evaluation
runtime and graph diameter is shown. In the bottom panels  the relationship between runtime and
graph diameter is shown on subsets of 100 and 200 of the real AIRWAYS and PROTEINS datasets 
respectively  for each diameter.

m

3.2 Results and discussion
Our experiments on ENZYMES and AIRWAYS clearly demonstrate that there are real-world clas-
siﬁcation problems where continuous-valued attributes make a big contribution to classiﬁcation per-
formance. Our experiments on SYNTHETIC demonstrate how the more discrete types of kernels 
PROP and WL  are unable to classify the graphs. Already on SYNTHETIC  which is a modest-sized
set of modest-sized graphs  CSM and SP are too computationally demanding to be practical  and on
AIRWAYS  which is a larger set of larger trees  they cannot ﬁnish in 30 days. The CSM kernel [14]
has asymptotic runtime O(knk+1)  where k is a parameter bounding the size of subgraphs consid-
ered by the kernel  and thus in order to study subgraphs of relevant size  its runtime will be at least
as high as the shortest path kernel. Moreover  the CSM kernel requires the computation of a product
graph which  for graphs with hundreds of nodes  can cause memory problems  which we also ﬁnd in
our experiments. The PROP kernel is fast; however  the reason for the computational efﬁciency of
PROP is that it is not really a kernel for continuous valued features – it is a kernel for discrete fea-
tures combined with a hashing scheme to discretize continuous-valued features. In our experiments 
these hashing schemes do not prove powerful enough to compete in classiﬁcation accuracy with the
kernels that really do use the continuous-valued features.
While ENZYMES and AIRWAYS beneﬁt signiﬁcantly from including continuous attributes  our
experiments on PROTEINS demonstrate that there are also classiﬁcation problems where the most
important information is just as well summarized in a discrete feature: here our combination of

7

Figure 2: Dependence of runtime on n  δ and m on synthetic and real graph datasets.

continuous and discrete node features gives equal classiﬁcation performance as the more efﬁcient
WL kernel using only discrete attributes.
We proved in Section 3.1 that the GraphHopper kernel has asymptotic runtime O(n2(d+m+log n+
δ2))  and that the average runtime for one kernel evaluation in a Gram matrix is O(n2d) when the
number of graphs exceeds m + n + δ2. Our experiments in Section 3.1 empirically demonstrate how
runtime depends on the parameters n  m and δ. As m and δ are dependent parameters  the runtime
dependence on m and δ is not straightforward. An increase in the number of edges m typically
leads to an increased graph diameter δ for small m  but for more densely connected graphs  δ will
decrease with increasing m as seen in the top right panel of Figure 2. A consequence of this is
that graph diameter rarely becomes very large compared to m. The same plot also shows that the
runtime increases slowly with increasing m. Our runtime experiments clearly illustrate that while in
the worst case scenario we could have m = n2 or δ = n  this rarely happens in real-world graphs 
which are often sparse and with small diameter. Our experiments also illustrate an average runtime
quadratic in n on large datasets  as expected based on complexity analysis.

4 Conclusion
We have deﬁned the GraphHopper kernel for graphs with any type of node attributes  presented
an efﬁcient algorithm for computing it  and demonstrated that it outperforms state-of-the-art graph
kernels on real and synthetic data in terms of classiﬁcation accuracy and/or speed. The kernels are
able to take advantage of any kind of node attributes  as they can integrate any user-deﬁned node
kernel. Moreover  the kernel is parameter-free except for the node kernels.
This kernel opens the door to new application domains such as computer vision or medical imaging 
in which kernels that work solely on graphs with discrete attributes were too restrictive so far.

Acknowledgements

The authors wish to thank Nils Kriege for sharing his code for computing the CSM kernel  Nino Shervashidze
and Chlo´e-Agathe Azencott for sharing their preprocessed chemoinformatics data  and Asger Dirksen and
Jesper Pedersen for sharing the AIRWAYS dataset. This work is supported by the Danish Research Council for
Independent Research | Technology and Production  the Knud Høygaard Foundation  AstraZeneca  The Danish
Council for Strategic Research  Netherlands Organisation for Scientic Research  and the DFG project ”Kernels
for Large  Labeled Graphs (LaLa)”. The research of Professor Dr. Karsten Borgwardt was supported by the
Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung.

8

051015202500.0050.010.0150.020.0250.030.035121416182022242600.10.20.30.40.50.60.705010015020025030035040045050000.511.522.533.5410015020025030035040045050000.050.10.150.20.250.302468101214References
[1] N. Shervashidze  S.V.N. Vishwanathan  T. Petri  K. Mehlhorn  and K.M. Borgwardt. Efﬁcient graphlet

kernels for large graph comparison. JMLR  5:488–495  2009.

[2] N. Shervashidze  P. Schweitzer  E.J. van Leeuwen  K. Mehlhorn  and K.M. Borgwardt. Weisfeiler-

Lehman graph kernels. JMLR  12:2539–2561  2011.

[3] D. Haussler. Convolution kernels on discrete structures. Technical report  Department of Computer

Science  University of California at Santa Cruz  1999.

[4] M. Collins and N. Duffy. Convolution kernels for natural language. In NIPS  pages 625–632  2001.
[5] S.V.N. Vishwanathan and A.J. Smola. Fast kernels for string and tree matching. In NIPS  pages 569–576 

2002.

[6] C. Cortes  P. Haffner  and M. Mohri. Rational kernels: Theory and algorithms. JMLR  5:1035–1062 

2004.

[7] D. Kimura and H. Kashima. Fast computation of subpath kernel for trees. In ICML  2012.
[8] P. Mah´e and J.-P. Vert. Graph kernels based on tree patterns for molecules. Machine Learning  75:3–35 

2009.

[9] H. Kashima  K. Tsuda  and A. Inokuchi. Marginalized kernels between labeled graphs. In ICML  pages

321–328  2003.

[10] T. G¨artner  P. Flach  and S. Wrobel. On graph kernels: Hardness results and efﬁcient alternatives. In

Learning Theory and Kernel Machines  volume 2777 of LNCS  pages 129–143  2003.

[11] S.V.N. Vishwanathan  N.N. Schraudolph  R.I. Kondor  and K.M. Borgwardt. Graph kernels. JMLR 

11:1201–1242  2010.

[12] F.R. Bach. Graph kernels between point clouds. In ICML  pages 25–32  2008.
[13] P. Mah´e  N. Ueda  T. Akutsu  J.-L. Perret  and J.-P. Vert. Extensions of marginalized graph kernels. In

ICML  2004.

[14] N. Kriege and P. Mutzel. Subgraph matching kernels for attributed graphs. In ICML  2012.
[15] B. Ga¨uz`ere  L. Brun  and D. Villemin. Two new graphs kernels in chemoinformatics. Pattern Recognition

Letters  15:2038–2047  2012.

[16] M. Neumann  N. Patricia  R. Garnett  and K. Kersting. Efﬁcient graph kernels by randomization.

ECML/PKDD (1)  pages 378–393  2012.

In

[17] K.M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. ICDM  2005.
[18] T.H. Cormen  C.E. Leiserson  R.L. Rivest  and C. Stein. Introduction to Algorithms (3. ed.). MIT Press 

2009.

[19] A. Feragen  J. Petersen  D. Grimm  A. Dirksen  J.H. Pedersen  K. Borgwardt  and M. de Bruijne. Geo-

metric tree kernels: Classiﬁcation of COPD from airway tree geometry. In IPMI 2013  2013.

[20] N. Shervashidze. Graph kernels code  http://mlcb.is.tuebingen.mpg.de/Mitarbeiter/

Nino/Graphkernels/.

[21] D. Gleich. MatlabBGL http://dgleich.github.io/matlab-bgl/.
[22] I. Schomburg  A. Chang  C. Ebeling  M. Gremse  C. Heldt  G. Huhn  and D. Schomburg. Brenda  the

enzyme database: updates and major new developments. Nucleic Acids Research  32:431–433  2004.

[23] P.D. Dobson and A.J. Doig. Distinguishing enzyme structures from non-enzymes without alignments.

Journal of Molecular Biology  330(4):771 – 783  2003.

[24] K.M. Borgwardt  C.S. Ong  S. Sch¨onauer  S.V.N. Vishwanathan  A.J. Smola  and H.-P. Kriegel. Protein

function prediction via graph kernels. Bioinformatics  21(suppl 1):i47–i56  2005.

[25] J. Pedersen  H. Ashraf  A. Dirksen  K. Bach  H. Hansen  P. Toennesen  H. Thorsen  J. Brodersen  B. Skov 
M. Døssing  J. Mortensen  K. Richter  P. Clementsen  and N. Seersholm. The Danish randomized lung
cancer CT screening trial - overall design and results of the prevalence round. J Thorac Oncol  4(5):608–
614  May 2009.

[26] J. Petersen  M. Nielsen  P. Lo  Z. Saghir  A. Dirksen  and M. de Bruijne. Optimal graph based seg-
mentation using ﬂow lines with application to airway wall segmentation. In IPMI  LNCS  pages 49–60 
2011.

[27] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Trans. Int. Syst. and
Tech.  2:27:1–27:27  2011. Software available at http://www.csie.ntu.edu.tw/˜cjlin/
libsvm.

9

,Aasa Feragen
Niklas Kasenburg
Jens Petersen
Marleen de Bruijne
Karsten Borgwardt