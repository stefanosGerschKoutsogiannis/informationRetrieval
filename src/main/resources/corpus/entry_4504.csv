2018,Quadrature-based features for kernel approximation,We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique  we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behavior and conduct an extensive empirical study that supports our hypothesis.,Quadrature-based features for kernel approximation

Marina Munkhoeva†

Yermek Kapushev†

Evgeny Burnaev†

Ivan Oseledets† ‡

†Skolkovo Institute of Science and Technology

Moscow  Russia

‡Institute of Numerical Mathematics of the Russian Academy of Sciences

Moscow  Russia

Abstract

We consider the problem of improving kernel approximation via randomized
feature maps. These maps arise as Monte Carlo approximation to integral
representations of kernel functions and scale up kernel methods for larger datasets.
Based on an efﬁcient numerical integration technique  we propose a unifying
approach that reinterprets the previous random features methods and extends to
better estimates of the kernel approximation. We derive the convergence behaviour
and conduct an extensive empirical study that supports our hypothesis1.

1

Introduction

Kernel methods proved to be an efﬁcient technique in numerous real-world problems. The core idea
of kernel methods is the kernel trick – compute an inner product in a high-dimensional (or even
inﬁnite-dimensional) feature space by means of a kernel function k:

k(x  y) = (cid:104)ψ(x)  ψ(y)(cid:105) 

(1)
where ψ : X → F is a non-linear feature map transporting elements of input space X into a feature
space F. It is a common knowledge that kernel methods incur space and time complexity infeasible
to be used with large-scale datasets directly. For example  kernel regression has O(N 3 + N d2)
training time  O(N 2) memory  O(N d) prediction time complexity for N data points in original
d-dimensional space X .
One of the most successful techniques to handle this problem  known as Random Fourier Features
(RFF) proposed by [29]  introduces a low-dimensional randomized approximation to feature maps:
(2)
This is essentially carried out by using Monte-Carlo sampling to approximate scalar product in (1).
A randomized D-dimensional mapping ˆΨ(·) applied to the original data input allows employing
standard linear methods  i.e. reverting the kernel trick. In doing so one reduces the complexity to
that of linear methods  e.g. D-dimensional approximation admits O(N D2) training time  O(N D)
memory and O(N ) prediction time.
It is well known that as D → ∞  the inner product in (2) converges to the exact kernel k(x  y).
Recent research [35; 14; 9] aims to improve the convergence of approximation so that a smaller D
can be used to obtain the same quality of approximation.
This paper considers kernels that allow the following integral representation
k(x  y) = Ep(w)fxy(w) = I(fxy) 

fxy = φ(w(cid:62)x)φ(w(cid:62)y).

k(x  y) ≈ ˆΨ(x)

(cid:62) ˆΨ(y).

p(w) =

1

(2π)d/2 e−

(cid:107)w(cid:107)2

2

 

(3)

1The code for this paper is available at https://github.com/maremun/quffka.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

For

example 

the

popular Gaussian

kernel

admits

such

representation with

fxy(w) = φ(w(cid:62)x)(cid:62)φ(w(cid:62)y)  where φ(·) = [cos(·)

(cid:62)
sin(·)]

.

The class of kernels admitting the form in (3) covers shift-invariant kernels (e.g. radial basis function
(RBF) kernels) and Pointwise Nonlinear Gaussian (PNG) kernels. They are widely used in practice
and have interesting connections with neural networks [8; 34].
The main challenge for the construction of low-dimensional feature maps is the approximation of
the expectation in (3) which is d-dimensional integral with Gaussian weight. Unlike other research
studies we refrain from using simple Monte Carlo estimate of the integral  instead  we propose to use
speciﬁc quadrature rules. We now list our contributions:

• We propose to use spherical-radial quadrature rules to improve kernel approximation
accuracy. We show that these quadrature rules generalize the RFF-based techniques. We
also provide an analytical estimate of the error for the used quadrature rules that implies
better approximation quality.

• We use structured orthogonal matrices (so-called butterﬂy matrices) when designing
quadrature rule that allow fast matrix by vector multiplications. As a result  we speed
up the approximation of the kernel function and reduce memory requirements.

• We carry out an extensive empirical study comparing our methods with the state-of-the-art
ones on a set of different kernels in terms of both kernel approximation error and downstream
tasks performance. The study supports our hypothesis on the exceeding accuracy of the
method.

2 Quadrature Rules and Random Features

We start with rewriting the expectation in Equation (3) as integral of fxy with respect to p(w):

I(fxy) = (2π)− d

2

e− w(cid:62)w

2 fxy(w)dw.

(cid:90) ∞

(cid:90) ∞

···

−∞

−∞

Integration can be performed by means of quadrature rules. The rules usually take a form of
interpolating function that is easy to integrate. Given such a rule  one may sample points from the
domain of integration and calculate the value of the rule at these points. Then  the sample average of
the rule values would yield the approximation of the integral.
The connection between integral approximation and mapping ψ is straightforward. In what follows
we show a brief derivation of the quadrature rules that allow for an explicit mapping of the form:
ψ(x) = [ a0φ(0) a1φ(w(cid:62)1 x) . . . aDφ(w(cid:62)Dx) ]  where the choice of the weights ai and the points
wi is dictated by the quadrature.
We use the average of sampled quadrature rules developed by [18] to yield unbiased estimates of
I(fxy). A change of coordinates is the ﬁrst step to facilitate stochastic spherical-radial rules. Now 
let w = rz  with z(cid:62)z = 1  so that w(cid:62)w = r2 for r ∈ [0 ∞]  leaving us with (to ease the notation
we substitute fxy with f)

(cid:90)

(cid:90) ∞

(cid:90)

(cid:90) ∞

(2π)− d

2

e− r2

2

0

Ud

2 rd−1f (rz)drdz =

I(f ) = (2π)− d
I(f ) is now a double integral over the unit d-sphere Ud = {z : z(cid:62)z = 1  z ∈ Rd} and over the
radius. To account for both integration regions we apply a combination of spherical (S) and radial
(R) rules known as spherical-radial (SR) rules. To provide an intuition how the rules work  here we
brieﬂy state and discuss their form2.

2 |r|d−1f (rz)drdz 

−∞

(4)

Ud

2

e− r2

Stochastic radial rules of degree 2l + 1 R(h) =

symmetric sums and approximate the inﬁnite range integral T (h) =(cid:82) ∞

h(ρi)+h(−ρi)

ˆwi

i=0

2 |r|d−1h(r)dr. Note
2Please see [18] for detailed derivation of the stochastic radial (section 2)  spherical (section 3) and spherical

−∞

have the form of the weighted

e− r2

2

radial rules (section 4)

l(cid:80)

2

that when h is set to the function f of interest  T (f ) corresponds to the inner integral in (4). To get
an unbiased estimate for T (h)  points ρi are sampled from speciﬁc distributions. The weights ˆwi are
derived so that the rule is exact for polynomials of degree 2l + 1 and give unbiased estimate for other
functions.

(cid:101)wjs(Qzj)  where Q is a random orthogonal matrix 
The weights (cid:101)wj are stochastic with distribution such that the rule is exact for polynomials of degree p

approximate an integral of a function s(z) over the surface of unit d-sphere Ud  where zj are
points on Ud  i.e. z(cid:62)
j zj = 1. Remember that the outer integral in (4) has Ud as its integration region.

Stochastic spherical rules SQ(s) =

and gives unbiased estimate for other functions.
Stochastic spherical-radial rules SR of degree (2l + 1  p) are given by the following expression

p(cid:80)

j=1

p(cid:88)

(cid:101)wj

l(cid:88)

j=1

i=1

SR(2l+2 p)

Q ρ

=

ˆwi

f (ρQzi) + f (−ρQzi)

2

 

where the distributions of weights are such that if degrees of radial rules and spherical rules coincide 
i.e. 2l + 1 = p  then the rule is exact for polynomials of degree 2l + 1 and gives unbiased estimate of
the integral for other functions.

2.1 Spherical-radial rules of degree (1  1) is RFF

2

Q ρ = f (ρQz)+f (−ρQz)

If we take radial rule of degree 1 and spherical rule of degree 1  we obtain the following rule
SR(1 1)
  where ρ ∼ χ(d). It is easy to see that ρQz ∼ N (0  I)  and for shift
invariant kernel f (w) = f (−w)  thus  the rule reduces to SR(1 1)
Q ρ = f (w)  where w ∼ N (0  I).
Now  RFF [29] makes approximation of the RBF kernel in exactly the same way: it generates random
vector from Gaussian distribution and calculates the corresponding feature map.
Proposition 2.1. Random Fourier Features for RBF kernel are SR rules of degree (1  1).

2.2 Spherical-radial rules of degree (1  3) is ORF

Q ρ =(cid:80)d

let’s take radial rule of degree 1 and spherical rule of degree 3.

Now 
we get the following spherical-radial rule SR1 3
ei = (0  . . .   0  1  0  . . .   0)(cid:62) is an i-th column of the identity matrix.
Let us compare SR1 3 rules with Orthogonal Random Features [14] for the RBF kernel. In the
ORF approach  the weight matrix W = SQ is generated  where S is a diagonal matrix with the
entries drawn independently from χ(d) distribution and Q is a random orthogonal matrix. The
i=1 f (wi)  where wi is the i-th row of

approximation of the kernel is then given by kORF(x  y) =(cid:80)d

In this case
  where ρ ∼ χ(d) 

f (ρQei)+f (−ρQei)

the matrix W. As the rows of Q are orthonormal  they can be represented as Qei.
Proposition 2.2. Orthogonal Random Features for RBF kernel are SR rules of degree (1  3).

i=1

2

2.3 Spherical-radial rules of degree (3  3)

We go further and take both spherical and radial rules of degree 3  where we use original and reﬂected
vertices vj of randomly rotated unit vertex regular d-simplex V as the points on the unit sphere

(cid:18)

(cid:19)

SR3 3

Q ρ(f ) =

d
ρ2

1 −

f (0) +

d

d + 1

j=1

where ρ ∼ χ(d + 2). We apply (5) to the approximation of (4) by averaging the samples of SR3 3
Q ρ:

d+1(cid:88)

(cid:21)

(cid:20) f (−ρQvj) + f (ρQvj)
n(cid:88)

2ρ2

SR3 3

Qi ρi(f ) 

1
n

i=1

 

(5)

(6)

I(f ) = EQ ρ[SR3 3

Q ρ(f )] ≈ ˆI(f ) =

3

where n is the number of sampled SR rules. Speaking in terms of the approximate feature maps  the
new feature dimension D in case of the quadrature based approximation equals 2n(d + 1) + 1 as we
sample n rules and evaluate each of them at 2(d + 1) random points and 1 zero point.
In this work we propose to modify the quadrature rule by generating ρj ∼ χ(d + 2) for each vj 
i.e. SR3 3
. It doesn’t affect

(cid:104) f (−ρj Qvj )+f (ρj Qvj )

(cid:80)d+1

d+1(cid:80)

(cid:16)

(cid:17)

(cid:105)

d

Q ρ(f ) =

f (0) + d
d+1

j=1

(d+1)ρ2
j

2ρ2
j

1 −

the quality of approximation while simpliﬁes an analysis of the quadrature-based random features.

j=1

Explicit mapping We ﬁnally arrive at the map ψ(x) = [ a0φ(0) a1φ(w(cid:62)1 x) . . . aDφ(w(cid:62)Dx) ] 

(cid:115)

where a0 =

j=1(cid:80)
(cid:105)
such matrices Wk = ρk(cid:104)

W = ρ ⊗

(QV)
− (QV)

1 −
(cid:62)
(cid:62)

(cid:104)

(cid:105)

(cid:62)
(cid:62)

(cid:113) d

d
ρ2

3  aj = 1
ρj

2(d+1)  wj

is the j-th row in the matrix

d+1
  ρ = [ρ1 . . . ρD](cid:62). To get D features one simply stacks n =

D

2(d+1)+1

(QkV)
− (QkV)

so that W ∈ RD×d  where only Qk ∈ Rd×d and ρk are
. For the 0-order
generated randomly (k = 1  . . .   n). For Gaussian kernel  φ(·) = [cos(·)
arc-cosine kernel  φ(·) = Θ(·)  where Θ(·) is the Heaviside function. For the 1-order arc-cosine
kernel  φ(·) = max(0 ·).
2.4 Generating uniformly random orthogonal matrices

(cid:62)
sin(·)]

The SR rules require a random orthogonal matrix Q. If Q follows Haar distribution  the averaged
samples of SR3 3
Q ρ rules provide an unbiased estimate for (4). Essentially  Haar distribution means
that all orthogonal matrices in the group are equiprobable  i.e. uniformly random. Methods for
sampling such matrices vary in their complexity of generation and multiplication.
We test two algorithms for obtaining Q. The ﬁrst uses a QR decomposition of a random matrix to
obtain a product of a sequence of reﬂectors/rotators Q = H1 . . . Hn−1D  where Hi is a random
Householder/Givens matrix and a diagonal matrix D has entries such that P(dii = ±1) = 1/2.
It implicates no fast matrix multiplication. We test both methods for random orthogonal matrix
generation and  since their performance coincides  we leave this one out for cleaner ﬁgures in the
Experiments section.
The other choice for Q are so-called butterﬂy matrices [17]. For d = 4

c1 −s1

s1
0
0

c1
0
0



c2

0
s2
0

0
0
0
0
c3 −s3
c3
s3

 =

c1c2 −s1c2 −c1s2

s1s2
c1c2 −s1s2 −c1s2
s1c2
c3s2 −s3s2
c3c2 −s3c2
c3c2
s3c2
c3s2
s3s2

 

0 −s2
0
c2
c2
0
s2
0

0
−s2
0
c2

B(4) =

where si  ci is sine and cosine of some angle θi  i = 1  . . .   d − 1. For deﬁnition and discussion
please see Supplementary Materials. The factors of B(d) are structured and allow fast matrix
multiplication. The method using butterﬂy matrices is denoted by B in the Experiments section.

3 Error bounds

Proposition 3.1. Let l be a diameter of the compact set X and p(w) = N (0  σ2
pI) be the probability
density corresponding to the kernel. Let us suppose that |φ(w(cid:62)x)| ≤ κ  |φ(cid:48)(w(cid:62)x)| ≤ µ for all
w ∈ Ω  x ∈ X and
Features approximation ˆk(x  y) of the kernel function k(x  y) and any ε > 0 it holds

(cid:12)(cid:12)(cid:12) ≤ M for all ρ ∈ [0 ∞)  where z(cid:62)z = 1. Then for Quadrature-based

(cid:12)(cid:12)(cid:12) 1−fxy(ρz)

ρ2

(cid:19)

(cid:18) σplκµ

(cid:19) 2d

d+1

(cid:18)

(cid:19)

Dε2

sup

x y∈X |ˆk(x  y) − k(x  y)| ≥ ε
0 ≥ 0  you need to sample ρj two times on average (see Supplementary Materials for details).

8M 2(d + 1)

≤ βd

exp

−

ε

 

3To get a2

(cid:18)

P

4

Method

Table 1: Space and time complexity.
Time
O(Dd)
O(Dd)
O(d log d)
O(d log d)

Space
O(Dd)
O(Dd)
O(d)
O(d)

Quadrature based

ORF
QMC
ROM

(cid:16)

(cid:17)

(cid:16) d

(cid:17) d
(cid:20)

Dataset

#samples

Powerplant
LETTER

Table 2: Experimental settings for the datasets.
#runs
500
500
500
100
50
10

N
9568
20000
9298
70000
60000

d
4
16
256
784
3072
7129

CIFAR100
LEUKEMIA

550
550
550
550
50
10

USPS
MNIST

72

−d
d+1 + d

1

d

d+1

where βd =
more than ε with probability at least 1 − δ as long as
log

8M 2(d + 1)

d+1

2

2

6d+1
d+1

D ≥

ε2

1 + 1
d

(cid:21)

.

σplκµ

ε

+ log

βd
δ

d+1 . Thus we can construct approximation with error no

The proof of this proposition closely follows [33]  details can be found in the Supplementary
Materials.
Term βd depends on dimension d  its maximum is β86 ≈ 64.7 < 65  and limd→∞ βd = 64  though
it is lower for small d. Let us compare this probability bound with the similar result for RFF in [33].
Under the same conditions the required number of samples to achieve error no more than ε with
probability at least 1 − δ for RFF is the following
σpl
ε

8(d + 1)

3d + 3

D ≥

βd
δ

+ log

d + 1

(cid:20)

(cid:21)

log

log

2d

ε2

+

d

2

.

1 + 1
d

For Quadrature-based Features for RBF kernel M = 1

(cid:20)

(cid:21)

2   κ = µ = 1  therefore  we obtain
σpl
ε

βd
δ

+ log

.

log

D ≥

2(d + 1)

ε2

2

1 + 1
d

The asymptotics is the same  however  the constants are smaller for our approach. See Section 4 for
empirical justiﬁcation of the obtained result.
i=1  with xi ∈ Rd and yi ∈ R  let h(x) denote
Proposition 3.2 ([33]). Given a training set {(xi  yi)}n
the result of kernel ridge regression using the positive semi-deﬁnite training kernel matrix K  test
to the training kernel matrix (cid:98)K and test kernel values ˆkx. Further  assume that the training labels
are centered (cid:80)n
kernel values kx and regularization parameter λ. Let ˆh(x) be the same using a PSD approximation

(cid:80)n
y = 1
n
σy√n
λ (cid:107)ˆkx − kx(cid:107)2 +

i=1 y2

i . Also suppose (cid:107)kx(cid:107)∞ ≤ κ. Then

λ2 (cid:107)(cid:98)K − K(cid:107)2.

κσyn

i=1 yi = 0  and let σ2
|ˆh(x) − h(x)| ≤

(cid:107)(cid:98)K − K(cid:107)2 ≤ (cid:107)(cid:98)K − K(cid:107)F ≤ nε. By denoting λ = nλ0 we obtain |ˆh(x) − h(x)| ≤ λ0+1
Suppose that sup|k(x  x(cid:48)) − ˆk(x  x(cid:48))| ≤ ε for all x  x(cid:48) ∈ Rd. Then (cid:107)ˆkx − kx(cid:107)2 ≤ √nε and

σyε.

λ2
0

Therefore 

P(cid:16)

(cid:18)

(cid:17)
≤ P
|ˆh(x) − h(x)| ≥ ε
(cid:107)ˆk(x  x(cid:48)) − k(x  x(cid:48))(cid:107)∞ ≥
(cid:19)2(cid:20)
(cid:18) λ0 + 1

2

λ2
0ε

σy(λ0 + 1)

(cid:19)

.

(cid:21)

D ≥ 8M 2(d + 1)σ2

y

λ2
0ε

1 + 1
d

λ2
0ε

σyσplκµ(λ0 + 1)

log

+ log

βd
δ

.

So  for the quadrature rules we can guarantee |ˆh(x) − h(x)| ≤ ε with probability at least 1 − δ as
long as

4 Experiments

We extensively study the proposed method on several established benchmarking datasets: Powerplant 
LETTER  USPS  MNIST  CIFAR100 [23]  LEUKEMIA [20].
In Section 4.2 we show kernel
approximation error across different kernels and number of features. We also report the quality of
SVM models with approximate kernels on the same data sets in Section 4.3.

5

Figure 1: Kernel approximation error across three kernels and 6 datasets. Lower is better. The x-axis
represents the factor to which we extend the original feature space  n =
2(d+1)+1  where d is the
dimensionality of the original feature space  D is the dimensionality of the new feature space.

D

4.1 Methods

3(cid:81)

We present a comparison of our method (B) with estimators based on a simple Monte Carlo 
quasi-Monte Carlo [35] and Gaussian quadratures [11]. The Monte Carlo approach has a variety of
ways to generate samples: unstructured Gaussian [29]  structured Gaussian [14]  random orthogonal
matrices (ROM) [10].
Monte Carlo integration (G  Gort  ROM). The kernel is estimated as ˆk(x  y) = 1
D φ(Mx)φ(My) 
where M ∈ RD×d is a random weight matrix. For unstructured Gaussian based approximation
M = G  where Gij ∼ N (0  1). Structured Gaussian has M = Gort  where Gort = DQ  Q is
obtained from RQ decomposition of G  D is a diagonal matrix with diagonal elements sampled from
the χ(d) distribution. In compliance with the previous work on ROM we use S-Rademacher with
three blocks: M = √d
SDi  where S is a normalized Hadamard matrix and P(Dii = ±1) = 1/2.
Quasi-Monte Carlo integration (QMC). Quasi-Monte Carlo integration boasts improved rate of
convergence 1/D compared to 1/√D of Monte Carlo  however  as empirical results illustrate its
performance is poorer than that of orthogonal random features [14]. It has larger constant factor
hidden under O notation in computational complexity. For QMC the weight matrix M is generated
as a transformation of quasi-random sequences. We run our experiments with Halton sequences in
compliance with the previous work.
Gaussian quadratures (GQ). We included subsampled dense grid method from [11] into our
comparison as it is the only data-independent approach from the paper that is shown to work well.
We reimplemented code for the paper to the best of our knowledge as it is not open sourced.

i=1

4.2 Kernel approximation

To measure kernel approximation quality we use relative error in Frobenius norm (cid:107)K− ˆK(cid:107)F
  where
(cid:107)K(cid:107)F
K and ˆK denote exact kernel matrix and its approximation. In line with previous work we run
experiments for the kernel approximation on a random subset of a dataset. Table 2 displays the
settings for the experiments across the datasets.
Approximation was constructed for different number of SR samples n =
2(d+1)+1  where d is
an original feature space dimensionality and D is the new one. For the Gaussian kernel we set
hyperparameter γ = 1
d for all the approximants  while the arc-cosine
kernels (see deﬁnition of arc-cosine kernel in the Supplementary Materials) have no hyperparameters.

2σ2 to the default value of 1

D

6

123451.62.43.24.04.8kK−ˆKkkKk×10−1Arc-cosine0Powerplant123450.60.91.21.51.8×10−1LETTER1234523456×10−2USPS123451.21.82.43.03.6×10−2MNIST123450.30.60.91.21.51.8×10−2CIFAR100123450.40.60.81.01.21.4×10−2LEUKEMIA123451.53.04.56.07.5kK−ˆKkkKk×10−1Arc-cosine112345012345×10−1123450.00.20.40.60.81.0×10−1123451.53.04.56.0×10−2123450.00.61.21.82.43.0×10−2123450.61.21.82.43.0×10−212345n1.53.04.56.07.5kK−ˆKkkKk×10−2Gaussian12345n0.000.250.500.751.001.25×10−212345n0.51.01.52.02.53.0×10−212345n012345×10−312345n0.51.01.52.02.5×10−312345n0.00.81.62.43.24.0×10−4GGortROMQMCGQBWe run experiments for each [kernel  dataset  n] tuple and plot 95% conﬁdence interval around the
mean value line. Figure 1 shows the results for kernel approximation error on LETTER  MNIST 
CIFAR100 and LEUKEMIA datasets.
QMC method almost always coincides with RFF except for arc-cosine 0 kernel. It particularly enjoys
Powerplant dataset with d = 4  i.e. small number of features. Possible explanation for such behaviour
can be due to the connection with QMC quadratures. The worst case error for QMC quadratures
scales with n−1(log n)d  where d is the dimensionality and n is the number of sample points [28]. It
is worth mentioning that for large d it is also a problem to construct a proper QMC point set. Thus 
in higher dimensions QMC may bring little practical advantage over MC. While recent randomized
QMC techniques indeed in some cases have no dependence on d  our approach is still computationally
more efﬁcient thanks to the structured matrices. GQ method as well matches the performance of RFF.
We omit both QMC and GQ from experiments on datasets with large d = [3072  7129] (CIFAR100 
LEUKEMIA).
The empirical results in Figure 1 support our hypothesis about the advantages of SR quadratures
applied to kernel approximation compared to SOTA methods. With an exception of a couple of cases:
(Arc-cosine 0  Powerplant) and (Gaussian  USPS)  our method displays clear exceeding performance.

4.3 Classiﬁcation/regression with new features

Figure 2: Accuracy/R2 score using embeddings with three kernels on 3 datasets. Higher is better.
The x-axis represents the factor to which we extend the original feature space  n =

D

2(d+1)+1.

We report accuracy and R2 scores for the classiﬁcation/regression tasks on some of the datasets
(Figure 2). We examine the performance with the same setting as in experiments for kernel
approximation error  except now we map the whole dataset. We use Support Vector Machines
to obtain predictions.
Kernel approximation error does not fully deﬁne the ﬁnal prediction accuracy – the best performing
kernel matrix approximant not necessarily yields the best accuracy or R2 score. However  the
empirical results illustrate that our method delivers comparable and often superior quality on the
downstream tasks.

4.4 Walltime experiment

We measure time spent on explicit mapping of features by running each experiment 50 times and
averaging the measurements. Indeed  Figure 3 demonstrates that the method scales as theoretically
predicted with larger dimensions thanks to the structured nature of the mapping.

7

12345n0.7750.8000.8250.8500.875accuracy/R2Arc-cosine0Powerplant12345n0.30.40.50.60.70.8LETTER12345n0.9450.9480.9510.9540.957USPS12345n0.840.860.880.900.920.94accuracy/R2Arc-cosine112345n0.7000.7250.7500.7750.8000.82512345n0.96550.96700.96850.97000.97150.973012345n0.9240.9270.9300.9330.936accuracy/R2Gaussian12345n0.6450.6600.6750.69012345n0.96000.96080.96160.96240.9632exactGortROMGBFigure 3: Time spent on explicit mapping. The x-axis represents the 5 datasets with increasing input
number of features: LETTER  USPS  MNIST  CIFAR100 and LEUKEMIA.

5 Related work

The most popular methods for scaling up kernel methods are based on a low-rank approximation of
the kernel using either data-dependent or independent basis functions. The ﬁrst one includes Nyström
method [12]  greedy basis selection techniques [31]  incomplete Cholesky decomposition [15].
The construction of basis functions in these techniques utilizes the given training set making them
more attractive for some problems compared to Random Fourier Features approach. In general 
data-dependent approaches perform better than data-independent approaches when there is a gap in
the eigen-spectrum of the kernel matrix. The rigorous study of generalization performance of both
approaches can be found in [36].
In data-independent techniques  the kernel function is approximated directly. Most of the methods
(including the proposed approach) that follow this idea are based on Random Fourier Features [29].
They require so-called weight matrix that can be generated in a number of ways. [24] form the weight
matrix as a product of structured matrices. It enables fast computation of matrix-vector products and
speeds up generation of random features.
Another work [14] orthogonalizes the features by means of orthogonal weight matrix. This leads
to less correlated and more informative features increasing the quality of approximation. They
support this result both analytically and empirically. The authors also introduce matrices with some
special structure for fast computations. [10] propose a generalization of the ideas from [24] and [14] 
delivering an analytical estimate for the mean squared error (MSE) of approximation.
All these works use simple Monte Carlo sampling. However  the convergence can be improved by
changing Monte Carlo sampling to Quasi-Monte Carlo sampling. Following this idea [35] apply
quasi-Monte Carlo to Random Fourier Features. In [37] the authors make attempt to improve quality
of the approximation of Random Fourier Features by optimizing sequences conditioning on a given
dataset.
Among the recent papers there are works that  similar to our approach  use the numerical integration
methods to approximate kernels. While [3] carefully inspects the connection between random
features and quadratures  they did not provide any practically useful explicit mappings for kernels.
Leveraging the connection [11] propose several methods with Gaussian quadratures. Among them
three schemes are data-independent and one is data-dependent. The authors do not compare them with
the approaches for random feature generation other than random Fourier features. The data-dependent
scheme optimizes the weights for the quadrature points to yield better performance. A closely related
work [25] constructs features for kernel approximation by approximating spherical-radial integral
and designs QMC points to speed up approximation and reduce memory.

6 Conclusion

We propose an approach for the random features methods for kernel approximation  revealing a
new interpretation of RFF and ORF. The latter are special cases of the spherical-radial quadrature
rules with degrees (1 1) and (1 3) respectively. We take this further and develop a more accurate
technique for the random features preserving the time and space complexity of the random orthogonal
embeddings.

8

01000200030004000500060007000d datasetinputdimension10−410−310−210−1Time sExplicitmappingtimeGGortGQBOur experimental study conﬁrms that for many kernels on the most datasets the proposed approach
delivers the best kernel approximation. Additionally  the results showed that the quality of the
downstream task (classiﬁcation/regression) is also superior or comparable to the state-of-the-art
baselines.

Acknowledgments

This work was supported by the Ministry of Science and Education of Russian Federation as a part of
Mega Grant Research Project 14.756.31.0001.

References
[1] Theodore W Anderson  Ingram Olkin  and Les G Underhill. Generation of random orthogonal matrices.

SIAM Journal on Scientiﬁc and Statistical Computing  8(4):625–629  1987.

[2] Haim Avron and Vikas Sindhwani. High-performance kernel machines with implicit distributed

optimization and randomization. Technometrics  58(3):341–349  2016.

[3] Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions. Journal

of Machine Learning Research  18(21):1–38  2017. 8

[4] John A Baker. Integration over spheres and the divergence theorem for balls. The American Mathematical

Monthly  104(1):36–47  1997.

[5] James Bergstra  Daniel Yamins  and David Cox. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures. In International Conference on Machine
Learning  pages 115–123  2013.

[6] Salomon Bochner. Monotone funktionen  stieltjessche integrale und harmonische analyse. Mathematische

Annalen  108(1):378–410  1933.

[7] Xixian Chen  Haiqin Yang  Irwin King  and Michael R Lyu. Training-efﬁcient feature map for shift-invariant

kernels. In IJCAI  pages 3395–3401  2015.

[8] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural Information

Processing Systems  pages 342–350  2009. 2

[9] Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear time

kernel expansions. arXiv preprint arXiv:1605.09049  2016. 1

[10] Krzysztof Choromanski  Mark Rowland  and Adrian Weller. The unreasonable effectiveness of random

orthogonal embeddings. arXiv preprint arXiv:1703.00864  2017. 6  8

[11] Tri Dao  Christopher M De Sa  and Christopher Ré. Gaussian quadrature for kernel features. In Advances

in Neural Information Processing Systems  pages 6109–6119  2017. 6  8

[12] Petros Drineas and Michael W Mahoney. On the Nyström method for approximating a Gram matrix for

improved kernel-based learning. Journal of Machine Learning Research  6(Dec):2153–2175  2005. 8

[13] Kai-Tai Fang and Run-Ze Li. Some methods for generating both an NT-net and the uniform distribution on
a Stiefel manifold and their applications. Computational Statistics & Data Analysis  24(1):29–46  1997.

[14] X Yu Felix  Ananda Theertha Suresh  Krzysztof M Choromanski  Daniel N Holtmann-Rice  and Sanjiv
Kumar. Orthogonal Random Features. In Advances in Neural Information Processing Systems  pages
1975–1983  2016. 1  3  6  8

[15] Shai Fine and Katya Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of

Machine Learning Research  2(Dec):243–264  2001. 8

[16] Alexander Forrester  Andy Keane  et al. Engineering design via surrogate modelling: a practical guide.

John Wiley & Sons  2008.

[17] Alan Genz. Methods for generating random orthogonal matrices. Monte Carlo and Quasi-Monte Carlo

Methods  pages 199–213  1998. 4

[18] Alan Genz and John Monahan. Stochastic integration rules for inﬁnite regions. SIAM journal on scientiﬁc

computing  19(2):426–439  1998. 2

9

[19] Alan Genz and John Monahan. A stochastic algorithm for high-dimensional integrals over unbounded
regions with gaussian weight. Journal of Computational and Applied Mathematics  112(1):71–81  1999.

[20] Todd R Golub  Donna K Slonim  Pablo Tamayo  Christine Huard  Michelle Gaasenbeek  Jill P Mesirov 
Hilary Coller  Mignon L Loh  James R Downing  Mark A Caligiuri  et al. Molecular classiﬁcation of
cancer: class discovery and class prediction by gene expression monitoring. Science  286(5439):531–537 
1999. 5

[21] Simon Haykin. Cognitive dynamic systems: perception-action cycle  radar and radio. Cambridge

University Press  2012.

[22] Po-Sen Huang  Haim Avron  Tara N Sainath  Vikas Sindhwani  and Bhuvana Ramabhadran. Kernel
methods match deep neural networks on timit. In Acoustics  Speech and Signal Processing (ICASSP)  2014
IEEE International Conference on  pages 205–209. IEEE  2014.

[23] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 5

[24] Quoc Le  Tamás Sarlós  and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In

Proceedings of the International Conference on Machine Learning  2013. 8

[25] Yueming Lyu. Spherical structured feature maps for kernel approximation. In International Conference on

Machine Learning  pages 2256–2264  2017. 8

[26] Francesco Mezzadri. How to generate random matrices from the classical compact groups. arXiv preprint

math-ph/0609050  2006.

[27] John Monahan and Alan Genz. Spherical-radial integration rules for bayesian computation. Journal of the

American Statistical Association  92(438):664–674  1997.

[28] Art B Owen. Latin supercube sampling for very high-dimensional simulations. ACM Transactions on

Modeling and Computer Simulation (TOMACS)  8(1):71–102  1998. 7

[29] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural

Information Processing Systems  pages 1177–1184  2008. 1  3  6  8

[30] Walter Rudin. Fourier analysis on groups. Courier Dover Publications  2017.

[31] Alex J Smola and Bernhard Schölkopf. Sparse greedy matrix approximation for machine learning. 2000. 8

[32] G. W. Stewart. The efﬁcient generation of random orthogonal matrices with an application to condition
estimators. SIAM Journal on Numerical Analysis  17(3):403–409  1980. ISSN 00361429. URL http:
//www.jstor.org/stable/2156882.

[33] Dougal J Sutherland and Jeff Schneider. On the error of random fourier features. arXiv preprint

arXiv:1506.02785  2015. 5

[34] Christopher KI Williams. Computing with inﬁnite networks. In Advances in Neural Information Processing

Systems  pages 295–301  1997. 2

[35] Jiyan Yang  Vikas Sindhwani  Haim Avron  and Michael Mahoney. Quasi-Monte Carlo feature maps
for shift-invariant kernels. In Proceedings of The 31st International Conference on Machine Learning
(ICML-14)  pages 485–493  2014. 1  6  8

[36] Tianbao Yang  Yu-Feng Li  Mehrdad Mahdavi  Rong Jin  and Zhi-Hua Zhou. Nyström Method vs Random
Fourier Features: A Theoretical and Empirical Comparison. In Advances in Neural Information Processing
Systems  pages 476–484  2012. 8

[37] Felix X Yu  Sanjiv Kumar  Henry Rowley  and Shih-Fu Chang. Compact nonlinear maps and circulant

extensions. arXiv preprint arXiv:1503.03893  2015. 8

10

,Marina Munkhoeva
Yermek Kapushev
Evgeny Burnaev
Ivan Oseledets
Cassidy Laidlaw
Soheil Feizi