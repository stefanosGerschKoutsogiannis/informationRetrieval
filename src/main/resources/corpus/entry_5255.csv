2014,Stochastic Proximal Gradient Descent with Acceleration Techniques,Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper  we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method  and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method  with the appropriate mini-batch size  achieves lower overall complexity than both APG and Prox-SVRG.,Stochastic Proximal Gradient Descent with

Acceleration Techniques

Atsushi Nitanda

NTT DATA Mathematical Systems Inc.

1F Shinanomachi Rengakan  35 

Shinanomachi  Shinjuku-ku  Tokyo 

160-0016  Japan

nitanda@msi.co.jp

Abstract

Proximal gradient descent (PGD) and stochastic proximal gradient descent
(SPGD) are popular methods for solving regularized risk minimization problems
in machine learning and statistics. In this paper  we propose and analyze an ac-
celerated variant of these methods in the mini-batch setting. This method incor-
porates two acceleration techniques: one is Nesterov’s acceleration method  and
the other is a variance reduction for the stochastic gradient. Accelerated proxi-
mal gradient descent (APG) and proximal stochastic variance reduction gradient
(Prox-SVRG) are in a trade-off relationship. We show that our method  with the
appropriate mini-batch size  achieves lower overall complexity than both APG and
Prox-SVRG.

1

Introduction

This paper consider the following optimization problem:

minimize

x∈Rd

f (x) def= g(x) + h(x) 

(1)

where g is the average of the smooth convex functions g1  . . .   gn from Rd to R  i.e.  g(x) =
nPn
1
i=1 gi(x) and h : Rd → R is a relatively simple convex function that can be non-differentiable.
In machine learning  we often encounter optimization problems of this form. For example  given
a sequence of training examples (a1  b1)  . . .   (an  bn)  where ai ∈ Rd and bi ∈ R  if we set
gi(x) = 1
2kxk2 or we obtain
Lasso by setting h(x) = λ|x|. If we set gi(x) = log(1 + exp(−bixT ai))  then we obtain regular-
ized logistic regression.

i x − bi)2  then we obtain ridge regression by setting h(x) = λ

2 (aT

To solve the optimization problem (1)  one popular method is proximal gradient descent (PGD) 
which can be described by the following update rule for k = 1  2  . . .:
xk+1 = proxηkh (xk − ηk∇g(xk))  

where prox is the proximity operator 

proxηh(y) = arg min

x∈Rd (cid:26) 1

2kx − yk2 + ηh(x)(cid:27) .

A stochastic variant of PGD is stochastic proximal gradient descent (SPGD)  where at each iteration
k = 1  2  . . .  we pick ik randomly from {1  2  . . .   n}  and take the following update:

xk+1 = proxηkh (xk − ηk∇gik (xk)) .

1

The advantage of SPGD over PGD is that at each iteration  SPGD only requires the computation
of a single gradient ∇gik (xk). In contrast  each iteration of PGD evaluates the n gradients. Thus
the computational cost of SPGD per iteration is 1/n that of the PGD. However  due to the variance
introduced by random sampling  SPGD obtains a slower convergence rate than PGD. In this paper
we consider problem (1) under the following assumptions.
Assumption 1. Each convex function gi(x) is L-Lipschitz smooth  i.e.  there exist L > 0 such that
for all x  y ∈ Rd 

k∇gi(x) − ∇gi(y)k ≤ Lkx − yk.

From (2)  one can derive the following inequality 

(2)

(3)

(4)

gi(x) ≤ gi(y) + (∇gi(y)  x − y) +

L
2 kx − yk2.

Assumption 2. g(x) is µ-strongly convex; i.e.  there exists µ > 0 such that for all x  y ∈ Rd 

g(x) ≥ g(y) + (∇g(y)  x − y) +

µ
2 kx − yk2.

Note that it is obvious that L ≥ µ.
Assumption 3. The regularization function h(x) is a lower semi-continuous proper convex function;
however  it can be non-differentiable or non-continuous.

Under the Assumptions 1  2  and h(x) ≡ 0  PGD (which is equivalent to gradient descent in this
case) with a constant learning rate ηk = 1
L achieves a linear convergence rate. On the other hand  for
stochastic (proximal) gradient descent  because of the variance introduced by random sampling  we
need to choose diminishing learning rate ηk = O(1/k)  and thus the stochastic (proximal) gradient
descent converges at a sub-linear rate.

To improve the stochastic (proximal) gradient descent  we need a variance reduction technique 
which allows us to take a larger learning rate. Recently  several papers proposed such variance
reduction methods for the various special cases of (1). We refer the reader to [1–13]. In the case
where gi(x) is Lipschitz smooth and h(x) is strongly convex  Shalev-Shwartz and Zhang [1  2]
proposed a proximal stochastic dual coordinate ascent (Prox-SDCA); the same authors developed
accelerated variants of SDCA [3  4]. Le Roux et al. [5] proposed a stochastic average gradient
(SAG) for the case where gi(x) is Lipschitz smooth  g(x) is strongly convex  and h(x) ≡ 0. These
methods achieve a linear convergence rate. However  SDCA and SAG need to store all gradients (or
dual variables)  so that O(nd) storage is required in general problems. Although this can be reduced
to O(n) for linear prediction problems  these methods may be unsuitable for more complex and
large-scale problems. More recently  Johnson and Zhang [6] proposed stochastic variance reduction
gradients (SVRG) for the case where gi(x) is L-Lipschitz smooth  g(x) is µ-strongly convex  and
h(x) ≡ 0. SVRG achieves the following overall complexity (total number of component gradient
evaluations to ﬁnd an ǫ-accurate solution) 

O(cid:18)(n + κ) log(cid:18) 1

ǫ(cid:19)(cid:19)  

(5)

where κ is the condition number L/µ. Furthermore  this method need not store all gradients. Xiao
and Zhang [7] proposed a proximal variant of SVRG  called Prox-SVRG which also achieves the
same complexity.

Another effective method for solving (1) is accelerated proximal gradient descent (APG)  proposed
by Nesterov [14  15]. APG [14] is an accelerated variant of deterministic gradient descent and
achieves the following overall complexity to ﬁnd an ǫ-accurate solution 

O(cid:18)n√κ log(cid:18) 1

ǫ(cid:19)(cid:19) .

(6)

Complexities (5) and (6) are in a trade-off relationship. For example  if κ = n  then the complexity
(5) is less than (6). On the other hand  the complexity of APG has a better dependence on the
condition number κ.

In this paper  we propose and analyze a new method called the Accelerated Mini-Batch Prox-SVRG
(Acc-Prox-SVRG) for solving (1). Acc-Prox-SVRG incorporates two acceleration techniques in

2

the mini-batch setting: (1) Nesterov’s acceleration method of APG and (2) an variance reduction
technique of SVRG. We show that the overall complexity of this method  with an appropriate mini-
batch size  is more efﬁcient than both Prox-SVRG and APG; even when mini-batch size is not
appropriate  our method is still comparable to APG or Prox-SVRG.

2 Accelerated Mini-Batch Prox-SVRG

As mentioned above  to ensure convergence of SPGD  the learning rate ηk has to decay to zero
for reducing the variance effect of the stochastic gradient. This slows down the convergence. As
a remedy to this issue  we use the variance reduction technique of SVRG [6] (see also [7])  which
allows us to take a larger learning rate. Acc-Prox-SVRG is a multi-stage scheme. During each stage 
this method performs m APG-like iterations and employs the following direction with mini-batch
instead of gradient 

vk = ∇gIk (yk) − ∇gIk (˜x) + ∇g(˜x) 

(7)

where Ik = {i1  . . .   ib} is a randomly chosen size b subset of {1  2  . . .   n} and gIk = 1
j=1 gij .
At the beginning of each stage  the initial point x1 is set to be ˜x  and at the end of stage  ˜x is updated.
Conditioned on yk  we can take expectation with respect to Ik and obtain EIk [vk] = ∇g(yk)  so
that vk is an unbiased estimator. As described in the next section  the conditional variance EIkkvk −
∇g(yk)k2 can be much smaller than Eik∇gi(yk)−∇g(yk)k2 near the optimal solution. The pseudo-
code of our Acc-Prox-SVRG is given in Figure 1.

b Pb

Parameters update frequency m  learning rate η  mini-batch size b

and non-negative sequence β1  . . .   βm

Initialize ˜x1
Iterate: for s = 1  2  . . .

i=1 ∇gi(˜x)

˜x = ˜xs
nPn
˜v = 1
x1 = y1 = ˜x
Iterate: for k = 1  2  . . .   m
Randomly pick subset Ik ⊂ {1  2  . . .   n} of size b
vk = ∇gIk (yk) − ∇gIk (˜x) + ˜v
xk+1 = proxηh (yk − ηvk)
yk+1 = xk+1 + βk(xk+1 − xk)
end
set ˜xs+1 = xm+1

end

In our analysis  we focus on a basic variant of the algorithm (Figure 1) with βk = 1−√µη
1+√µη .

Figure 1: Acc-Prox-SVRG

3 Analysis

In this section  we present our analysis of the convergence rates of Acc-Prox-SVRG described in
Figure 1 under Assumptions 1  2 and 3  and provide some notations and deﬁnitions. Note that we
may omit the outer index s for notational simplicity. By the deﬁnition of a proximity operator  there
exists a subgradient ξk ∈ ∂h(xk+1) such that

We deﬁne the estimate sequence Φk(x) (k = 1  2  . . .   m + 1) by

xk+1 = yk − η (vk + ξk) .

Φ1(x) = f (x1) +

µ
2 kx − x1k2 and

Φk+1(x) = (1 − √µη)Φk(x) + √µη(gIk (yk) + (vk  x − yk) +

µ
2 kx − ykk2

+h(xk+1) + (ξk  x − xk+1)) 

f or k ≥ 1.

3

We set

Φ∗k = min
x∈Rd

Φk(x) and zk = arg min

Φk(x).

x∈Rd

Since ∇2Φk(x) = µIn  it follows that for ∀x ∈ Rd 

Φk(x) =

µ
2 kx − zkk2 + Φ∗k.

(8)

The following lemma is the key to the analysis of our method.

Lemma 1. Consider Acc-Prox-SVRG in Figure 1 under Assumptions 1  2  and 3. If η ≤ 1
for k ≥ 1 we have

2L   then

(9)

E [Φk(x)] ≤ f (x) + (1 − √µη)k−1 (Φ1 − f )(x) and
k−1

µ
2

Xl=1

(1 − √µη)k−1−l(cid:26)−

√µη kxl − ylk2 + ηk∇g(yl) − vlk2(cid:27)#   (10)
1 − µη

E [f (xk)] ≤ E"Φ∗k +
where the expectation is taken with respect to the history of random variables I1  . . .   Ik−1.
Note that if the conditional variance of vl is equal to zero  we immediately obtain a linear conver-
gence rate from (9) and (10). Before we can prove Lemma 1  additional lemmas are required  whose
proofs may be found in the Supplementary Material.
Lemma 2. If η < 1

µ   then for k ≥ 1 we have

zk+1 = (1 − √µη)zk + √µηyk −r η
zk − yk =

(yk − xk).

1
√µη

µ

(vk + ξk) and

(11)

(12)

Lemma 3. For k ≥ 1  we have

1

2(cid:0)k∇g(yk) + ξkk2 + kvk + ξkk2 − k∇g(yk) − vkk2(cid:1)   (13)

(14)

(∇g(yk) + ξk  vk + ξk) =
kvk + ξkk2 ≤ 2(cid:0)k∇g(yk) + ξkk2 + k∇g(yk) − vkk2(cid:1)   and
k∇g(yk) + ξkk2 ≤ 2(cid:0)kvk + ξkk2 + k∇g(yk) − vkk2(cid:1) .

(15)

Proof of Lemma 1. Using induction  it is easy to show (9). The proof is in Supplementary Material.
Now we prove (10) by induction. From the deﬁnition of Φ1  Φ∗1 = f (x1). we assume (10) is true
for k. Using Eq. (11)  we have

kyk − zk+1k2 =(cid:13)(cid:13)(cid:13)(cid:13)

(1 − √µη)(yk − zk) +r η
= (1 − √µη)2kyk − zkk2 + 2r η

µ

µ

(vk + ξk)(cid:13)(cid:13)(cid:13)(cid:13)

2

(1 − √µη)(yk − zk  vk + ξk) +

η
µkvk + ξkk2.

From above equation and (8) with x = yk  we get

Φk+1(yk) = Φ∗k+1 +

µ

2 (cid:26)(1 − √µη)2kyk − zkk2 + 2r η

µ

(1 − √µη)(yk − zk  vk + ξk)

+

η

µkvk + ξkk2(cid:27) .

On the other hand  from the deﬁnition of the estimate sequence and (8) 

Φk+1(yk) = (1 − √µη)(cid:16)Φ∗k +

µ

2 kyk − zkk2(cid:17) + √µη(gIk (yk) + h(xk+1) + (ξk  yk − xk+1)).

4

Therefore  from these two equations  we have

Φ∗k+1 = (1 − √µη)Φ∗k +

+(ξk  yk − xk+1)) − (1 − √µη)√µη(yk − zk  vk + ξk) −

(1 − √µη)√µηkyk − zkk2 + √µη(gIk (yk) + h(xk+1)
η
2kvk + ξkk2.

µ
2

Since g is Lipschitz smooth  we bound f (xk+1) as follows:
f (xk+1) ≤ g(yk) + (∇g(yk)  xk+1 − yk) + L

2 kxk+1 − ykk2 + h(xk+1).

Using (16)  (17)  (12)  and xk+1 − yk = −η(vk + ξk) we have
EIk(cid:2)f (xk+1) − Φ∗k+1(cid:3)
≤(16) (17)

L
2 kxk+1 − ykk2 −

EIkh(1 − √µη)(−Φ∗k + g(yk) + h(xk+1)) + (∇g(yk)  xk+1 − yk)
(1 − √µη)√µηkyk − zkk2

+√µη(ξk  xk+1 − yk) +
+(1 − √µη)√µη(yk − zk  vk + ξk) +
EIkh(1 − √µη)(−Φ∗k + g(yk) + h(xk+1) + (xk − yk  vk + ξk)) − η(∇g(yk)  vk + ξk)
−η√µη(ξk  vk + ξk) −

2kvk + ξkk2i

kyk − xkk2 +

1 − √µη
√µη

(19)

µ
2

µ
2

η
2

η

(Lη + 1)kvk + ξkk2i  

=
(12)

where for the ﬁrst inequality we used EIk [gIk (yk)] = g(yk). Here  we give the following

(16)

(17)

(18)

µ

EIk [g(yk) + h(xk+1) + (xk − yk  vk + ξk)]
= EIk [g(yk) + (vk  xk − yk) + h(xk+1) + (ξk  xk − xk+1) + (ξk  xk+1 − yk)]
≤ EIkhg(xk) −
EIk(cid:2)f (xk+1) − Φ∗k+1(cid:3)
≤(19) (20)

2 kxk − ykk2 + h(xk) − η(ξk  vk + ξk)i  

where for the ﬁrst inequality we used EIk [vk] = ∇g(yk) and convexity of g and h. Thus we have

1 − µη
√µη kxk − ykk2

EIkh(1 − √µη)(f (xk) − Φ∗k) −

µ
2

(20)

η
2

(1 + Lη)kvk + ξkk2i

−η(∇g(yk) + ξk  vk + ξk) +
EIk(cid:20)(1 − √µη)(f (xk) − Φ∗k) −
≤(13)
η
2k∇g(yk) + ξkk2 +
EIk(cid:20)(1 − √µη)(f (xk) − Φ∗k) −

−

2L

Lη2
2 kvk + ξkk2 +

µ
2

≤(14) η≤ 1

µ
2

1 − µη
√µη kxk − ykk2
2kvk − ∇g(yk)k2(cid:21)
η
√µη kxk − ykk2 + ηkvk − ∇g(yk)k2(cid:21) .
1 − µη

By taking expectation with respect to the history of random variables I1  . . .   Ik−1  the induction
hypothesis ﬁnishes the proof of (10).

Our bound on the variance of vk is given in the following lemma  whose proof is in the Supplemen-
tary Material.
Lemma 4. Suppose Assumption 1 holds  and let x∗ = arg min
x∈Rd
that

f (x). Conditioned on yk  we have

EIkkvk − ∇g(yk)k2 ≤

1
b

n − b
n − 1(cid:0)2L2kyk − xkk2 + 8L(f (xk) − f (x∗) + f (˜x) − f (x∗))(cid:1) . (21)

5

From (10)  (21)  and (9) with x = x∗  it follows that

If η ≤ min(cid:26) (pb)2

Indeed  using

l=1 (1 − √µη)k−1−l

E [f (xk) − f (x∗)] ≤ (1 − √µη)k−1(Φ1 − f )(x∗) + EhPk−1
·n(cid:16)− µ

1−µη
√µη + n−b
n−1

n−1

2L2η

8Lη

2

µ

L2   1

n−b(cid:17)2
64 (cid:16) n−1
64 (cid:18) n − 1
η ≤

(pb)2

b (f (xl) − f (x∗) + f (˜x) − f (x∗))oi .

b (cid:17)kxl − ylk2 + n−b
2L(cid:27)  then the coefﬁcients of kxl − ylk2 are non-positive for p ≤ 2.
n − b(cid:19)2 µ
L2 ⇒
− µ

1−µη
√µη + n−b
n−1

b ≤ − µ

n − b
n − 1

1−µη
√µη + L

Lη
b ≤

2 √µη

f or p > 0 

√µη 

(22)

2L2η

p
8

2

2

we get

= 1

2√µη (cid:0)−µ + µ2η + µLη(cid:1) ≤µ≤L

1

2√µη (−µ + 2µLη) ≤η≤ 1

2L

0.

Thus  using (22) again with p ≤ 1  we have

E [f (xk) − f (x∗)] ≤ (1 − √µη)k−1(Φ1 − f )(x∗)

(1 − √µη)k−1−lp√µη(f (xl) − f (x∗) + f (˜x) − f (x∗))#

+E"k−1
Xl=1
≤ (1 − √µη)k−1(Φ1 − f )(x∗) + p(f (˜x) − f (x∗))
+E"k−1
(1 − √µη)k−1−lp√µη(f (xl) − f (x∗))#  
Xl=1
l=1 (1 − √µη)k−1−l ≤P∞t=0(1 − √µη)t = 1√µη .

µ

(23)

n−b(cid:17)2
64 (cid:16) n−1

L2   1

2L(cid:27) and 0 < p <

where for the last inequality we usedPk−1
Theorem 1. Suppose Assumption 1  2  and 3. Let η ≤ min(cid:26) (pb)2

1. Then we have

E [f (˜xs+1) − f (x∗)] ≤(cid:18)(1 − (1 − p)√µη)m +

p

1 − p(cid:19) (2 + p)(f (˜xs) − f (x∗)).

Moreover  if m ≥

1

(1−p)√µη log 1−p

p   then it follows that

E [f (˜xs+1) − f (x∗)] ≤

2p(2 + p)

1 − p

(f (˜xs) − f (x∗)).

(24)

(25)

From Theorem 1  we can see that for small 0 < p (e.g. p = 0.1)  the overall complexity of Acc-
Prox-SVRG (total number of component gradient evaluations to ﬁnd an ǫ-accurate solution) is

O(cid:18)(cid:18)n +

b

√µη(cid:19) log

1

ǫ(cid:19) .

Thus  we have the following corollary:
Corollary 1. Suppose Assumption 1  2  and 3. Let p be sufﬁciently small  as stated above  and

η = min(cid:26) (pb)2

n−b(cid:17)2
64 (cid:16) n−1

learning rate η is equal to (pb)2

µ

L2   1

Otherwise  η = 1

2L and the complexity becomes

µ
L2 and the overall complexity is

2L(cid:27). If mini-batch size b is smaller thanl
n−b(cid:17)2
64 (cid:16) n−1
κ(cid:19) log
O(cid:18)(cid:18)n +
O(cid:18)(cid:0)n + b√κ(cid:1) log

n − b
n − 1

ǫ(cid:19) .

ǫ(cid:19) .

1

1

6

8√κn

√2p(n−1)+8√κm  then the

(26)

(27)

Table 1: Comparison of overall complexity. b0 =

8√κn

√2p(n−1)+8√κ

.

ProxSVRG

O(cid:0)(n + κ) log 1

AccProxSVRG b < ⌈b0⌉
n−1 κ(cid:17) log 1
ǫ(cid:17)

ǫ(cid:1) O(cid:16)(cid:16)n + n−b

APG [14]

AccProxSVRG b ≥ ⌈b0⌉

O(cid:0)(n√κ) log 1

ǫ(cid:1) O(cid:0)(n + b√κ) log 1
ǫ(cid:1)

8√κn

√2p(n−1)+8√κ

Table 1 lists the overall complexities of the algorithms that achieve linear convergence. As seen
from Table 1  the complexity of Acc-Prox-SVRG monotonically decreases with respect to b < ⌈b0⌉ 
and monotonically increases when b ≥ ⌈b0⌉. Moreover  if b = 1  then
where b0 =
Acc-Prox-SVRG has the same complexity as that of Prox-SVRG  while if b = n then the complexity
of this method is equal to that of APG. Therefore  with an appropriate mini-batch size  Acc-Prox-
SVRG may outperform both Prox-SVRG and APG; even if the mini-batch is not appropriate  then
Acc-Prox-SVRG is still comparable to Prox-SVRG or APG. The following overall complexity is
the best possible rate of Acc-Prox-SVRG 

O(cid:18)(cid:18)n +

Now we give the proof of Theorem 1.

nκ

n + √κ(cid:19) log(cid:18) 1

ǫ(cid:19)(cid:19) .

Proof of Theorem 1. We denote E[f (xk) − f (x∗)] by Vk  and we use Wk to denote the last expres-
sion in (23). Thus  for k ≥ 1  Vk ≤ Wk. For k ≥ 2  we have
(1 − √µη)k−2−lp√µη Vl)

Wk = (1 − √µη)((1 − √µη)k−2(Φ1 − f )(x∗) + pV1 +

k−2

+p√µη Vk−1 + p√µη V1 ≤ (1 − √µη(1 − p))Wk−1 + p√µη W1.

Xl=1

Since 0 < √µη(1 − p) < 1  the above inequality leads to
Wk ≤(cid:18)(1 − (1 − p)√µη)k−1 +

p

1 − p(cid:19) W1.

(28)

From the strong convexity of g (and f )  we can see

W1 = (1 + p)(f (˜x) − f (x∗)) +

µ
2 k˜x − x∗k2 ≤ (2 + p)(f (˜x) − f (x∗)).

Thus  for k ≥ 2  we have

Vk ≤ Wk ≤(cid:18)(1 − (1 − p)√µη)k−1 +

p

1 − p(cid:19) (2 + p)(f (˜x) − f (x∗)) 

and that is exactly (24). Using log(1 − α) ≤ −α and m ≥

log(1 − (1 − p)√µη)m ≤ −m(1 − p)√µη ≤ − log

 

p

1

(1−p)√µη log 1−p
p   we have
1 − p

so that

(1 − (1 − p)√µη)m ≤

This ﬁnishes the proof of Theorem 1.

4 Numerical Experiments

p

1 − p

.

In this section  we compare Acc-Prox-SVRG with Prox-SVRG and APG on L1-regularized multi-
class logistic regression with the regularization parameter λ. Table 2 provides details of the datasets

7

mnist

covtype.scale

rcv1.binary

Figure 2: Comparison of Acc-Prox-SVRG with Prox-SVRG and APG. Top: Objective gap of L1
regularized multi-class logistic regression. Bottom: Test error rates.

and regularization parameters utilized in our experiments. These datasets can be found at the LIB-
SVM website1. The best choice of mini-batch size is b = ⌈b0⌉  which allows us to take a large
learning rate  η = 1
. When the num-
ber of components n is very large compared with √κ  we see that b0 = O(√κ); for this  we set
m = δb (δ ∈ {0.1  1.0  10}) and βk = b−2
b+2 varying b in the set {100  500  1000}. We ran Acc-
Prox-SVRG using values of η from the range {0.01  0.05  0.1  0.5  1.0  5.0  10.0}  and we chose
the best η in each mini-batch setting.

2L . Therefore  we have m ≥ O(√κ) and βk =

√2κ−1
√2κ+1

Figure 2 compares Acc-Prox-SVRG with Prox-SVRG and APG. The horizontal axis is the number
of single-component gradient evaluations. For Acc-Prox-SVRG  each iteration computes the 2b
gradients  and at the beginning of each stage  the n component gradients are evaluated. For Prox-
SVRG  each iteration computes the two gradients  and at the beginning of each stage  the n gradients
are evaluated. For APG  each iteration evaluates n gradients.

Table 2: Details of data sets and regularization parameters.

Dataset
mnist

covtype.scale
rcv1.binary

classes Training size Testing size
10 000
58 102
677 399

60 000
522 910
20 242

10
7
2

Features
780
54
47 236

λ

10−5
10−6
10−5

As can be seen from Figure 2  Acc-Prox-SVRG with good values of b performs better than or is
comparable to Prox-SVRG and is much better than results for APG. On the other hand  for relatively
large b  Acc-Prox-SVRG may perform worse because of an overestimation of b0  and hence the
worse estimates of m and βk.

5 Conclusion

We have introduced a method incorporating Nesterov’s acceleration method and a variance reduc-
tion technique of SVRG in the mini-batch setting. We prove that the overall complexity of our
method  with an appropriate mini-batch size  is more efﬁcient than both Prox-SVRG and APG; even
when mini-batch size is not appropriate  our method is still comparable to APG or Prox-SVRG. In
addition  the gradient evaluations for each mini-batch can be parallelized [3  16  17] when using our
method; hence  it performs much faster in a distributed framework.

1http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/

8

References

[1] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717 

2012.

[2] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research 14  pages 567-599  2013.

[3] S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. Ad-

vances in Neural Information Processing System 26  pages 378-385  2013.

[4] S. Shalev-Shwartz and T. Zhang. Accelerated Proximal Stochastic Dual Coordinate Ascent for
Regularized Loss Minimization. Proceedings of the 31th International Conference on Machine
Learning  pages 64-72  2014.

[5] N. Le Roux  M. Schmidt  and F. Bach. A Stochastic Gradient Method with an Exponential
Convergence Rate for Finite Training Sets. Advances in Neural Information Processing System
25  pages 2672-2680  2012.

[6] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. Advances in Neural Information Processing System 26  pages 315-323  2013.

[7] L. Xiao and T. Zhang. A Proximal Stochastic Gradient Method with Progressive Variance Re-

duction. arXiv:1403.4699  2014.

[8] J. Koneˇcn´y and P. Richt´arik. S2GD: Semi-Stochastic Gradient Descent Methods.

arXiv:1312.1666  2013.

[9] J. Koneˇcn´y  J. Lu  and P. Richt´arik. mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in
the Proximal Setting. NIPS Workshop on OPT2014: Optimization for Machine Learning  2014.

[10] J. Koneˇcn´y  Z. Qu  and P. Richt´arik. S2CD: Semi-Stochastic Coordinate Descent. NIPS Work-

shop on OPT2014: Optimization for Machine Learning  2014.

[11] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With
Support for Non-Strongly Convex Composite Objectives. Advances in Neural Information Pro-
cessing System 27  pages 1646-1654  2014.

[12] T. Suzuki. Stochastic Dual Coordinate Ascent with Alternating Direction Method of Multipli-
ers. Proceedings of the 31th International Conference on Machine Learning  pages 736-744 
2014.

[13] T. Zhao  M. Yu  Y. Wang  R. Arora  and H. Liu. Accelerated Mini-batch Randomized Block
Coordinate Descent Method. Advances in Neural Information Processing System 27  pages
3329-3337  2014.

[14] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  Boston 

2004.

[15] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discussion

Papers  2007.

[16] O. Dekel  R. Gilad-Bachrach  O. Shamir  and L. Xiao. Optimal distributed online prediction

using mini-batches. Journal of Machine Learning Research 13  pages 165-202  2012.

[17] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. Advances in Neural

Information Processing System 24  pages 873-881  2011.

9

,Daniele Durante
Bruno Scarpa
David Dunson
Atsushi Nitanda
Jiaji Huang
Qiang Qiu
Robert Calderbank
Robert Gower
Filip Hanzely
Peter Richtarik
Sebastian Stich
Thang Vu
Hyunjun Jang
Trung Pham
Chang Yoo