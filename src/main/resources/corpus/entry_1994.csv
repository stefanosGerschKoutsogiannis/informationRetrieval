2009,Fast  smooth and adaptive regression in metric spaces,It was recently shown that certain nonparametric regressors can escape the curse of dimensionality in the sense that their convergence rates adapt to the intrinsic dimension of data (\cite{BL:65  SK:77}). We prove some stronger results in more general settings. In particular  we consider a regressor which  by combining aspects of both tree-based regression and kernel regression  operates on a general metric space  yields a smooth function  and evaluates in time $O(\log n)$. We derive a tight convergence rate of the form $n^{-2/(2+d)}$ where $d$ is the Assouad dimension of the input space.,Fast  smooth and adaptive regression in metric spaces

Samory Kpotufe

UCSD CSE

Abstract

It was recently shown that certain nonparametric regressors can escape the curse
of dimensionality when the intrinsic dimension of data is low ([1  2]). We prove
some stronger results in more general settings. In particular  we consider a regres-
sor which  by combining aspects of both tree-based regression and kernel regres-
sion  adapts to intrinsic dimension  operates on general metrics  yields a smooth
function  and evaluates in time O(log n). We derive a tight convergence rate of
the form n

(cid:0)2=(2+d) where d is the Assouad dimension of the input space.

1 Introduction

Relative to parametric methods  nonparametric regressors require few structural assumptions on the
function being learned. However  their performance tends to deteriorate as the number of features
increases. This so-called curse of dimensionality is quantiﬁed by various lower bounds on the con-
(cid:0)2=(2+D) for data in RD (see e.g. [3  4]). In other words  one might
vergence rates of the form n
require a data size exponential in D in order to attain a low risk.
Fortunately  it is often the case that data in RD has low intrinsic complexity  e.g. the data is near a
manifold or is sparse  and we hope to exploit such situations. One simple approach  termed manifold
learning (e.g. [5  6  7])  is to embed the data into a lower dimensional space where the regressor
might work well. A recent approach with theoretical guarantees for nonparametric regression  is
the study of adaptive procedures  i.e. ones that operate in RD but attain convergence rates that
depend just on the intrinsic dimension of data. An initial result [1] shows that for data on a d-
dimensional manifold  the asymptotic risk at a point x 2 RD depends just on d and on the behavior
of the distribution in a neighborhood of x. Later  [2] showed that a regressor based on the RPtree
of [8] (a hierarchical partitioning procedure) is not only fast to evaluate  but is adaptive to Assouad
dimension  a measure which captures notions such as manifold dimension and data sparsity. The
related notion of box dimension (see e.g. [9]) was shown in an earlier work [10] to control the risk
of nearest neighbor regression  although adaptivity was not a subject of that result.
This work extends the applicability of such adaptivity results to more general uses of nonparametric
regression.
In particular  we present an adaptive regressor which  unlike RPtree  operates on a
general metric space where only distances are provided  and yields a smooth function  an important
property in many domains (see e.g. [11] which considers the smooth control of a robotic tool based
on noisy outside input). In addition  our regressor can be evaluated in time just O(log n)  unlike
(
kernel or nearest neighbor regression. The evaluation time for these two forms of regression is
lower bounded by the number of sample points contributing to the regression estimate. For nearest
neighbor regression  this number is given by a parameter kn whose optimal setting (see [12]) is
(cid:0)1=(2+d) (see [12])  we
O
would expect about nhd (cid:25) n2=(2+d) points in the ball B(x; h) around a query point x.
We note that there exist many heuristics for speeding up kernel regression  which generally combine
fast proximity search procedures with other elaborate methods for approximating the kernel weights
(see e.g. [13  14  15]). There are no rigorous bounds on either the achievable speedup or the risk of
the resulting regressor.

)
. For kernel regression  given an optimal bandwidth h (cid:25) n

n2=(2+d)

1

Figure 1: Left and Middle- Two r-nets at different scales r  each net inducing a partition of the sample X. In
each case  the gray points are the r-net centers. For regression each center contributes the average Y value of
the data points assigned to them (points in the cells). Right- Given an r-net and a bandwidth h  a kernel around
a query point x weights the Y -contribution of each center to the regression estimate for x.

)

(
Our regressor integrates aspects of both tree-based regression and kernel regression. It constructs
partitions of the input dataset X = fXign
1   and uses a kernel to select a few sets within a given
partition  each set contributing its average output Y value to the estimate. We show that such a
(cid:0)2=(2+d)
regressor achieves an excess risk of O
  where d is the Assouad dimension of the input
data space. This is a tighter convergence rate than the O
of RPtree regression
(see [2]). Finally  the evaluation time of O(log n) is arrived at by modifying the cover tree proximity
search procedure of [16]. Unlike in [16]  this guarantee requires no growth assumption on the data
distribution.
We’ll now proceed with a more detailed presentation of the results in the next section  followed by
technical details in sections 3 and 4.

(cid:0)2=(2+O(d log d)

)

(

n

n

2 Detailed overview of results
We’re given i.i.d training data (X; Y) = f(Xi; Yi)gn
1   where the input variable X belongs to a metric
space X where the distance between points is given by the metric (cid:26)  and the output Y belongs to a
subset Y of some Euclidean space. We’ll let (cid:1)X and (cid:1)Y denote the diameters of X and Y.
Assouad dimension: The Assouad or doubling dimension of X is deﬁned as the smallest d such
that any ball can be covered by 2d balls of half its radius.
Examples: A d-dimensional afﬁne subspace of a Euclidean space RD has Assouad dimension O(d)
[9]. A d-dimensional submanifold of a Euclidean space RD has Assouad dimension O(d) subject
to a bound on its curvature [8]. A d-sparse data space in RD  i.e. one where each data point has at
most d non zero coordinates  has Assouad dimension O(d log D) [8  2].
The algorithm has no knowledge of the dimension d  nor of (cid:1)Y  although we assume (cid:1)X is known
(or can be upper-bounded).
Regression function: We assume the regression function f(x) := E [Y jX = x] is Lipschitz  i.e.
there exists (cid:21)   unknown  such that 8x; x
Excess risk: Our performance criteria for a regressor fn(x) is the integrated excess l2 risk:
kf(X) (cid:0) Y k2 :

kfn(X) (cid:0) f(X)k2 = E

kfn(X) (cid:0) Y k2 (cid:0) E

0 2 X ; kf(x) (cid:0) f(x

0)k (cid:20) (cid:21) (cid:1) (cid:26) (x; x
0).

kfn (cid:0) fk2 := E

(1)

X

X;Y

X;Y

2.1 Algorithm overview

We’ll consider a set of partitions of the data induced by a hierarchy of r-nets of X. Here an r-net Qr
is understood to be both an r-cover of X (all points in X are within r of some point in Qr)  and an
r-packing (the points in Qr are at least r apart). The details on how to build the r-nets are covered
{
in section 4. For now  we’ll consider a class of regressors deﬁned over these nets (as illustrated in
}
Figure 1)  and we’ll describe how to select a good regressor out of this class.
Qr; r 2 f(cid:1)X =2igI+2
:= dlog ne  and
Partitions of X: The r-nets are denoted by
induces a partition fX(q); q 2 Qg of X  where
Qr; r 2 f(cid:1)X =2igI+2

Qr (cid:26) X. Each Q 2 {

  where I

}

0

0

2

∑

nq

i:Xi2X(q) Yi.

X(q) designate all those points in X whose closest point in Q is q. We set nq
(cid:22)Yq = 1
Admissible kernels: We assume that K(u) is a non increasing function of u 2 [0;1); K is
positive on u 2 [0; 1)  maximal at u = 0  and vanishes for u (cid:21) 1. To simplify notation  we’ll often
let K(x; q; h) denote K((cid:26) (x; q) =h).

:= jX(q)j  and

Qr; r 2 f(cid:1)X =2igI+2

0

  and given a bandwidth h  we deﬁne the fol-

Regressors: For each Q 2{
∑

lowing regressor:

}
∑
nq(K(x; q; h) + (cid:15))
q02Q nq0(K(x; q0; h) + (cid:15)) :

fn;Q(x) =

wq(x) (cid:22)Yq; where wq =

q2Q

(2)

The positive constant (cid:15) ensures that the estimate remains well deﬁned when K(x; q; h) = 0. We
assume (cid:15) (cid:20) K(1=2)=n2. We can view (K((cid:1)) + (cid:15)) as the effective kernel which never vanishes. It is
clear that the learned function fn;Q inherits any degree of smoothness from the kernel function K 
i.e. if K is of class C k  then so is fn;Q.
g  equation (2) above
Selecting the ﬁnal regressor: For ﬁxed n  K((cid:1))  and fQr; r 2 f(cid:1)X =2igI+2
deﬁnes a class of regressors parameterized by r 2 f(cid:1)X =2igI+2
  and the bandwidth h. We want
to pick a good regressor out of this class. We can reduce the search space right away by noticing
that we need r = (cid:18)(h): if r >> h then B(x; h) \ Qr is empty for most x since the points in Qr
are over r apart  and if r << h then B(x; h) \ Qr might contain a lot of points  thus increasing
evaluation time. So for each choice of h  we will set r = h=4  which will yield good guarantees on
computational and prediction performance. The ﬁnal regressor is selected as follows.
:= dlog ne  and deﬁne H
; Y0) of size n. As before let I
Draw a new sample (X0
0. For
every h 2 H  pick the r-net Qh=4 and test fn;Qh=4 on (X0
; Y0); let the empirical risk be minimized
i) (cid:0) Y
0
0
at ho  i.e. ho
i
Fast evaluation: Each regressor fn;Qh=4(x) can be estimated quickly on points x by traversing
(nested) r-nets as described in detail in section 4.

(cid:13)(cid:13)2. Return fn;Qho =4 as the ﬁnal regressor.

(cid:13)(cid:13)fn;Qh=4(X

:= f(cid:1)X =2igI

:= argminh2H

∑

n
i=1

1
n

0

0

2.2 Computational and prediction performance

The cover property ensures that for some h  Qh=4 is a good summary of local information (for
prediction performance)  while the packing property ensures that few points in Qh=4 fall in B(x; h)
(for fast evaluation). We have the following main result.
Theorem 1. Let d be the Assouad dimension of X and let n (cid:21) max

)2

)2

(

)

(

(

9;

.

;

(cid:21)(cid:1)X
(cid:1)Y

(a) The ﬁnal regressor selected satisﬁes

(cid:13)(cid:13)fn;Qho =4

E

(cid:13)(cid:13)2 (cid:20) C ((cid:21)(cid:1)X )2d=(2+d)

(cid:0) f

)2=(2+d)

(

(cid:1)2Y
n

+ 3(cid:1)2Y

ln(n log n)

n

;

(cid:1)Y
(cid:21)(cid:1)X

√

where C depends on the Assouad dimension d and on K(0)=K(1=2).

(b) fn;Qho =4(x) can be computed in time C
Part (a) of Theorem 1 is given by Corollary 1 of section 3  and does not depend on how the r-nets
are built; part (b) follows from Lemma 4 of section 4 which speciﬁes the nets.

0 log n  where C

0 depends just on d.

3 Risk analysis

Throughout this section we assume 0 < h < (cid:1)X and we let Q = Qh=4. We’ll bound the risk for
fn;Q for any ﬁxed choice of h  and then show that the ﬁnal h0 selected yields a good risk. The results
in this section only require the fact that Q is a cover of data and thus preserves local information 
while the packing property is needed in the next section for fast evaluation.

3

Deﬁne efn;Q(x) := EYjX fn;Q(x)  i.e. the conditional expectation of the estimate  for X ﬁxed. We

have the following standard decomposition of the excess risk into variance and bias terms:

(cid:13)(cid:13)(cid:13)fn;Q(x) (cid:0) efn;Q(x)
(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)(cid:13)efn;Q(x) (cid:0) f(x)

(cid:13)(cid:13)(cid:13)2

+

8x 2 X ; E
YjX

kfn;Q(x) (cid:0) f(x)k2 = E
YjX

:

(3)

We’ll proceed by bounding each term separately in the following two lemmas  and then combining
these bounds in Lemma 3. We’ll let (cid:22) denote the marginal measure over X and (cid:22)n denote the
corresponding empirical measure.
4 -net of X  0 < h < (cid:1)X . Consider x 2 X such
Lemma 1 (Variance at x). Fix X  and let Q be an h
that X \ (B(x; h=4)) 6= ;. We have

2K(0)(cid:1)2Y

E
YjX

K(1=2) (cid:1) n(cid:22)n (B(x; h=4)) :

i vik2 =
E kvik2. We apply this fact twice in the inequalities below  given that  conditioned on X and

Proof. Remember that for independent random vectors vi with expectation 0  E k∑
∑
(cid:13)(cid:13)(cid:13)fn;Q(x) (cid:0) efn;Q(x)
(cid:13)(cid:13)(cid:13)2
Q (cid:26) X  the Yi values are mutually independent and so are the (cid:22)Yq values. We have
∑

∑

wq(x)

(cid:22)Yq

(cid:20)

i

(cid:13)(cid:13)(cid:13)fn;Q(x) (cid:0) efn;Q(x)

(cid:13)(cid:13)(cid:13)2 (cid:20)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

= E
YjX

q2Q

∑
(

q2Q

=

(cid:20)

)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(

(
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:22)Yq (cid:0) E
YjX
∑
})∑

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:22)Yq

(cid:13)(cid:13)(cid:13)(cid:13) (cid:22)Yq (cid:0) E
∑
}

q(x)

q2Q

w2

YjX

(cid:1)2Y
nq

q(x) E
w2
YjX

q2Q

)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

Yi

(cid:20)

1
nq

q(x) E
w2
YjX

i:Xi2X(q)
(cid:1)2Y
nq

{
∑
(K(x; q; h) + (cid:15))(cid:1)2Y
q02Q nq0(K(x; q0; h) + (cid:15))

Yi (cid:0) E
{
YjX
∑
2K(0)(cid:1)2Y
q2Q nqK(x; q; h) :

wq = max
q2Q
(cid:20)

(cid:1)2Y
nq

wq(x)

wq(x)

q2Q

max
q2Q

(4)

= max
q2Q

∑

∑

To bound the fraction in (4)  we lower-bound the denominator as:

nqK(x; q; h) (cid:21)

nqK(x; q; h) (cid:21)

nqK(1=2) (cid:21) K(1=2) (cid:1) n(cid:22)n(B(x; h=4)):

q:(cid:26)(x;q)(cid:20)h=2

q2Q
The last inequality follows by remarking that  since Q is an h
only contain points from [q:(cid:26)(x;q)(cid:20)h=2X(q). Plug this last inequality into (4) and conclude.
Lemma 2 (Bias at x). As before  ﬁx X  and let Q be an h
such that X \ (B(x; h=4)) 6= ;. We have

4 -cover of X  the ball B(x; h=4) can
4 -net of X  0 < h < (cid:1)X . Consider x 2 X

q:(cid:26)(x;q)(cid:20)h=2

E
YjX

∑

where we just applied Jensen’s inequality on the norm square. We bound the r.h.s by breaking the
summation over two subsets of Q as follows.

Proof. We have

(cid:13)(cid:13)(cid:13)efn;Q(x) (cid:0) f(x)
∑
∑

q:(cid:26)(x;q)<h

(cid:20)

(cid:13)(cid:13)(cid:13)2

=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

∑

∑
∑

Xi2X(q)

wq(x)

nq

wq(x)

q:(cid:26)(x;q)<h

nq

Xi2X(q)

(cid:13)(cid:13)(cid:13)efn;Q(x) (cid:0) f(x)
∑

wq(x)

q2Q

nq

Xi2X(q)

(cid:13)(cid:13)(cid:13)2 (cid:20) 2(cid:21)2h2 +
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(f(Xi) (cid:0) f(x))
∑

q:(cid:26)(x;q)<h

kf(Xi) (cid:0) f(x)k2 (cid:20)

(cid:21)2 ((cid:26) (x; q) + (cid:26) (q; Xi))2 (cid:20)

4

(cid:1)2Y
n

:

∑

wq(x)

q2Q

nq

(cid:20)

∑

∑

Xi2X(q)

kf(Xi) (cid:0) f(x)k2 ;

wq(x)

∑

nq

(cid:21)2(cid:26) (Xi; x)2

∑

Xi2X(q)
wq(x)

q:(cid:26)(x;q)<h

nq

Xi2X(q)

25

16 (cid:21)2h2 (cid:20) 2(cid:21)2h2:

∑
∑

∑

Xi2X(q)

Next  we have∑

q:(cid:26)(x;q)(cid:21)h

=

∑
(

wq(x)

nq

(cid:1)2Y
nq(cid:15) +

∑

q:(cid:26)(x;q)(cid:21)h

q:(cid:26)(x;q)<h

kf(Xi) (cid:0) f(x)k2 (cid:20)

∑

q:(cid:26)(x;q)(cid:21)h

nq (K(x; q; h) + (cid:15))

)(cid:0)1 (cid:20) (cid:1)2Y

= (cid:1)2Y

(
1 + K(1=2)

wq(x)(cid:1)2Y

1 +
∑
)(cid:0)1 (cid:20) (cid:1)2Y

∑

nq(cid:15)

q:(cid:26)(x;q)(cid:21)h

q:(cid:26)(x;q)(cid:21)h nq(cid:15)

q:(cid:26)(x;q)<h nq (K(x; q; h) + (cid:15))

(cid:0)1



)
]

1 +

K(1=2)
q:(cid:26)(x;q)(cid:21)h nq(cid:15)

(cid:20) (cid:1)2Y
where the second inequality is due to the fact that  since (cid:22)n(B(x; h=4)) > 0  the set B(x; h=2) \ Q
cannot be empty (remember that Q is an h

4 -cover of X). This concludes the argument.

1 + n

n(cid:15)

;

Lemma 3 (Integrated excess risk). Let Q be an h
kfn;Q (cid:0) fk2 (cid:20) C0

E

(X;Y)

4 -net of X  0 < h < (cid:1)X . We have
n (cid:1) (h=(cid:1)X )d + 2(cid:21)2h2;

(cid:1)2Y

where C0 depends on the Assouad dimension d and on K(0)=K(1=2).
Proof. Applying Fubini’s theorem  the expected excess risk  E(X;Y) kfn;Q (cid:0) fk2  can be written as

(

E
X

E

(X;Y)

kfn;Q(X) (cid:0) f(X)k2

1f(cid:22)n(B(X;h=4))>0g + 1f(cid:22)n(B(X;h=4))=0g

:

By lemmas 1 and 2 we have for X = x ﬁxed 

E

(X;Y)

(
kfn;Q(x) (cid:0) f(x)k2 1f(cid:22)n(B(x;h=4))>0g (cid:20) C1 E
(cid:20) C1

X

[

(cid:1)2Y 1f(cid:22)n(B(x;h=4))>0g

n(cid:22)n(B(x; h=4))
2(cid:1)2Y

n(cid:22)(B(x; h=4))

)

[

+ 2(cid:21)2h2 +

+ 2(cid:21)2h2 +

]

(cid:1)2Y
;
n
(cid:20) 2

(cid:1)2Y
n

(5)

1fb(n;p)>0g

b(n;p)

np (see

where for the last inequality we used the fact that for a binomial b(n; p)  E
lemma 4.1 of [12]).
For the case where B(x; h=4) is empty  we have

E

(X;Y)

kfn;Q(x) (cid:0) f(x)k2 1f(cid:22)n(B(x;h=4))=0g (cid:20) (cid:1)2Y E
(cid:20) (cid:1)2Y e
[

X

Combining (6) and (5)  we can then bound the expected excess risk as

1f(cid:22)n(B(x;h=4))=0g = (cid:1)2Y (1 (cid:0) (cid:22)(B(x; h=4))n
(cid:0)n(cid:22)(B(x;h=4)) (cid:20)

(cid:1)2Y

(6)

n(cid:22)(B(x; h=4)) :

]

1

(cid:22)(B(X; h=4))

+ 2(cid:21)2h2 +

(cid:1)2Y
n

:

(7)

E

(X;Y)

E
X

n

kfn;Q (cid:0) fk2 (cid:20) 3C1(cid:1)2Y
[

]

(cid:20) N∑

1

(cid:22)(B(X; h=4))

[

E
X

The expectation on the r.h.s is bounded using a standard covering argument (see e.g. [12]). Let
8 -cover of X . Notice that for any zi  x 2 B(zi; h=8) implies B(x; h=4) (cid:27) B(zi; h=8).
fzigN
We therefore have

1 be an h

[

]

(cid:20) N∑

i=1

E
X

1fX2B(zi;h=8)g
(cid:22)(B(X; h=8))

]

; where C2 depends just on d:

E
X

1fX2B(zi;h=8)g
(cid:22)(B(X; h=4))

(

)d

i=1

= N (cid:20) C2

(cid:1)X
h

We conclude by combining the above with (7) to obtain
kfn;Q (cid:0) fk2 (cid:20) 3C1C2(cid:1)2Y

E

(X;Y)

n(h=(cid:1)X )d + 2(cid:21)2h2 +

(cid:1)2Y
n

:

5

Corollary 1. Let n (cid:21) max

(cid:13)(cid:13)fn;Qho =4

E

where C depends on the Assouad dimension d and on K(0)=K(1=2).

Proof outline. Let ~h = C3
that such an ~h is in H. We have by Lemma 3 that for ~h 

(cid:1)d=(2+d)

(cid:1)2Y
(cid:21)2n

X

2 H. We note that n is lower bounded so

. The ﬁnal regressor selected satisﬁes

√

+ 3(cid:1)2Y

ln(n log n)

n

;

;

9;

(

(cid:21)(cid:1)X
(cid:1)Y

)

(cid:1)Y
(cid:21)(cid:1)X

(cid:0) f

(cid:1)2Y
n

)2

)2=(2+d)

(
)
)2
(
(
(cid:13)(cid:13)2 (cid:20) C ((cid:21)(cid:1)X )2d=(2+d)
(
(
)1=(2+d)
(cid:13)(cid:13)(cid:13)2 (cid:20) C0 ((cid:21)(cid:1)X )2d=(2+d)
(cid:13)(cid:13)(cid:13)fn;Q~h=4
n∑
(cid:13)(cid:13)fn;Qh=4(X
(cid:13)(cid:13)2 (cid:0) 1
(cid:13)(cid:13)fn;Qh=4(X) (cid:0) Y
i) (cid:0) Y
0
(cid:13)(cid:13)(cid:13)fn;Q~h=4(X) (cid:0) Y
(cid:13)(cid:13)2 (cid:20) E
(cid:13)(cid:13)fn;Qho =4(X) (cid:0) Y
√
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)fn;Q~h=4
(cid:13)(cid:13)2 (cid:20)
(cid:13)(cid:13)fn;Qho =4

+2(cid:1)2Y

E
X;Y

(cid:0) f

(cid:0) f

(cid:0) f

(

X;Y

X;Y

0
i

i=0

n

:

(cid:1)2Y
n

)2=(2+d)
(cid:13)(cid:13)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (cid:20) (cid:1)2Y
√
(cid:13)(cid:13)(cid:13)2

+ 2(cid:1)2Y

ln(jHjp

n)

n)

ln(jHjp
√
ln(jHjp

n

:

n

n)

  which

p
Applying McDiarmid’s to the empirical risk followed by a union bound over H  we have that  with
probability at least 1 (cid:0) 1=

n over the choice of (X0

; Y0)  for all h 2 H

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) E

X;Y

It follows that E

by (1) implies
the randomness in the two samples) over this last inequality and conclude.

n

. Take the expectation (given

4 Fast evaluation

:= f(cid:1)X =2igI

1; I = dlog ne.

In this section we show how to modify the cover-tree procedure of [16] to enable fast evaluation of
fn;Qh=4 for any h 2 H
The cover-tree performs proximity search by navigating a hierarchy of nested r-nets of X. The
navigating-nets of [17] implement the same basic idea. They require additional book-keeping to
enable range queries of the form X \ B(x; h)  for a query point x. Here we need to perform range
searches of the form Qh=4 \ B(x; h) and our book-keeping will therefore be different from [17].
Note that  for each h and Qh=4  one could use a generic range search procedure such as [17] with
the data in Qh=4 as input  but this requires building a separate data structure for each h  which is
expensive. We use a single data structure.

4.1 The hierarchy of nets

Consider an ordering
points in X; inductively for 2 < i < n  X(i) in X is the farthest point from
where the distance to a set is deﬁned as the minimum distance to a point in the set.

1 of the data points obtained as follows: X(1) and X(2) are the farthest
 

X(1); : : : ; X(i(cid:0)1)

X(i)

{
}I+2

}n
{
}) (cid:21) r. Notice that  by construction  Qr is an r-net of X.

X(1); : : : ; X(i)

}

{

where i (cid:21) 1 is the highest index such that

(cid:1)X =2i
X(1); : : : ; X(i(cid:0)1)

0

  deﬁne Qr =

}

For r 2{
(
{

(cid:26)

X(i);

4.2 Data structure

The data structure consists of an acyclic directed graph  and range sets deﬁned below.
Neighborhood graph: The nodes of the graph are the
1   and the edges are given by the
following parent-child relationship: starting at r = (cid:1)X =2  the parent of each node in Qr n Q2r is
the point it is closest to in Q2r. The graph is implemented by maintaining an ordered list of children
for each node  where the order is given by the children’s appearance in the sequence
1 . These
relationships are depicted in Figure 2.

}n

{

X(i)

X(i)

{

}n

6

X(1) X(2) X(3) X(4) X(5) X(6)

X(1) X(2) X(3) X(4) X(5) X(6)

Figure 2: The r-nets (rows of left subﬁgure) are implicit to an ordering of the data. They deﬁne a parent-child
relationship implemented by the neighborhood graph (right)  the structure traversed for fast evaluation.

tively as follows. Given Q (cid:26){

}n
These ordered lists of children are used to implement the operation nextChildren deﬁned itera-
}n
1   let visited children denote any child of q 2 Q that a previous
call to nextChildren has already returned. The call nextChildren (Q) returns children of q 2 Q
that have not yet been visited  starting with the unvisited child with lowest index in
1   say
X(i)  and returning all unvisited children in Qr  the ﬁrst net containing X(i)  i.e. X(i) 2 Qr n Q2r ;
r is also returned. The children returned are then marked off as visited. The time complexity of this
routine is just the number of children returned.

Range sets: For each node X(i) and each r 2{
) (cid:20) 8r

in Qr deﬁned as R(i);r

q 2 Qr : (cid:26)

0   we maintain a set of neighbors of X(i)

}1

(cid:1)X =2i

}

.

X(i); q

{

:=

X(i)

{

X(i)

(

4.3 Evaluation

Procedure evaluate(x  h)

Q Q(cid:1)X ;
repeat
; r nextChildren (Q);
0
Q
00 Q [ Q
0
;
Q
0
if r < h=4 or Q

= ; then // We reached past Qh=4.

00
if (cid:26) (x; Q
Q ;;
Break loop ;
Q fq 2 Q
00

X(i) argminq2Q (cid:26) (x; q); // Closest point to x in Qh=4.
Q R(i);h=4 \ B(x; h); // Search in a range of 2h around X(i).
Break loop ;

) (cid:21) h + 2r then // The set Qh=4 \ B(x; h) is empty.

00
; (cid:26) (x; q) < (cid:26) (x; Q

) + 2rg;
until : : : ;
//At this point Q = Qh=4 \ B(x; h).
return

fn;Qh=4 (x) Pq2Q nq(K(x; q; h) + (cid:15)) (cid:22)Yq + (cid:15)(cid:16)Pq2Qh=4

Pq2Q nq(K(x; q; h) + (cid:15)) + (cid:15)(cid:16)n (cid:0)Pq2Q nq(cid:17)

nq (cid:22)Yq (cid:0)Pq2Q nq (cid:22)Yq(cid:17)

;

)

(

The evaluation procedure consists of quickly identifying the closest point X(i) to x in Qh=4 and
then searching in the range of X(i) for the points in Qh=4 \ B(x; h). The identiﬁcation of X(i) is
done by going down the levels of nested nets  and discarding those points (and their descendants)
that are certain to be farther to x than X(i) (we will argue that “(cid:26) (x; Q00) + 2r” is an upper-bound
on (cid:26)
). Also  if x is far enough from all points at the current level (second if-clause)  we
can safely stop early because B(x; h) is unlikely to contain points from Qh=4 (we’ll see that points
in Qh=4 are all within 2r of their ancestor at the current level).
Lemma 4. The call to procedure evaluate (x h) correctly evaluates fn;Qh=4(x) and has time
complexity C log ((cid:1)X =h) + log n where C is at most 28d.

x; X(i)

7

)

(

(

Qh=4 \ B(x; h)

)
R(i);h=4 \ B(x; h)

Proof. We ﬁrst show that the algorithm correctly returns fn;Q(cid:11)h(x)  and we then argue its run time.
Correctness of evaluate. The procedure works by ﬁrst ﬁnding the closest point to x in Qh=4 
say X(i)  and then identifying all nodes in
(see the ﬁrst
if-clause). We just have to show that this closest point X(i) is correctly identiﬁed.
We’ll argue the following loop invariant I: at the beginning of the loop  X(i) is either in Q00 =
Q [ Q0 or is a descendant of a node in Q0. Let’s consider some iteration where I holds (it certainly
does in the ﬁrst iteration).
If the ﬁrst if-clause is entered  then Q is contained in Qh=4 but Q0 is not  so X(i) must be in Q and
we correctly return.
Suppose the ﬁrst if-clause is not entered. Now let X(j) be the ancestor in Q0 of X(i) or let it be X(i)
itself if it’s in Q00. Let r be as deﬁned in evaluate  we have (cid:26)
k=0 r=2k = 2r by
going down the parent-child relations. It follows that

X(i); X(j)

(

)

=

<

)

(

)

(

∑1
)

(
(
)

00) (cid:20) (cid:26)
(

(

) (cid:20) (cid:26)
)

(cid:26) (x; Q

x; X(j)

x; X(i)

+ (cid:26)

X(i); X(j)

< (cid:26)

x; X(i)

+ 2r:

<

(

> (cid:26)

(

)

)

)

x; X(i)

x; X(i)

(
)

x; X(j)

x; X(k)

X(j); X(k)

> (cid:26) (x; Q00) (cid:0) 2r. Thus  if the second if-clause is entered  we

) (cid:0) 2r (cid:21) (cid:26)

) (cid:21) (cid:26) (x; Q00) + 2r (cid:21) (cid:26)

(
∑1
x; X(j)
so we know X(j) 6= X(i).

x; X(i)
> h  i.e. B(x; h) \ Qh=4 = ; and we correctly return.
(

In other words  we have (cid:26)
necessarily have (cid:26)
x; X(i)
Now assume none of the if-clauses is entered. Let X(j) 2 Q00 be any of the points removed from
Q00 to obtain the next Q. Let X(k) be a child of X(j) that has not yet been visited  or a descendant
of such a child. If neither such X(j) or X(k) is X(i) then  by deﬁnition  I must hold at the next
(
iteration. We sure have X(j) 6= X(i) since (cid:26)
+ 2r. Now
k=0 r=2k = 2r. We thus have
notice that  by the same argument as above  (cid:26)
(cid:26)
Runtime of evaluate. Starting from Q(cid:1)X   a different net Qr is reached at every iteration  and the
loop stops when we reach past Qh=4. Therefore the loop is entered at most log ((cid:1)X =h=4) times. In
each iteration  most of the work is done parsing through Q00  besides time spent for the range search
in the last iteration. So the total runtime is O (log ((cid:1)X =h=4) (cid:1) maxjQ00j) + range search time. We
just need to bound maxjQ00j (cid:20) maxjQj + maxjQ0j and the range search time.
The following fact (see e.g. Lemma 4.1 of [9]) will come in handy: consider r1 and r2 such that
r1=r2 is a power of 2  and let B (cid:26) X be a ball of radius r1; since X has Assouad dimension d 
the smallest r2-cover of B is of size at most (r1=r2)d  and the largest r2-packing of B is of size at
most (r1=r2)2d. This is true for any metric space  and therefore holds for X which is of Assouad
dimension at most d by inclusion in X .
Let Q0 (cid:26) Qr so that Q (cid:26) Q2r at the beginning of some iteration. Let q 2 Q  the children of q in Q0
)
are not in Q2r and therefore are all within 2r of Q; since these children an r-packing of B(q; 2r) 
there are at most 22d of them. Thus  maxjQ0j (cid:20) 22d maxjQj.
Initially Q = Q(cid:1)X so we have jQj (cid:20) 22d since Q(cid:1)X is a (cid:1)X -packing of X (cid:26) B
. At
the end of each iteration we have Q (cid:26) B(x; (cid:26) (x; Q00) + 2r). Now (cid:26) (x; Q00) (cid:20) h + 2r (cid:20) 4r + 2r
since the if-clauses were not entered if we got to the end of the iteration. Thus  Q is an r-packing of
B(x; 8r)  and therefore maxjQj (cid:20) 28d.
To ﬁnish  the range search around X(i) takes time
of B

(cid:12)(cid:12) (cid:20) 28d since R(i);h=4 is an h

(cid:12)(cid:12)R(i);h=4

4 -packing

X(1); 2(cid:1)X

(

(

)

X(i); 2h

.

Acknowledgements

This work was supported by the National Science Foundation (under grants IIS-0347646  IIS-
0713540  and IIS-0812598) and by a fellowship from the Engineering Institute at the Los Alamos
National Laboratory. Many thanks to the anonymous NIPS reviewers for the useful comments  and
thanks to Sanjoy Dasgupta for advice on the presentation.

8

References
[1] P. Bickel and B. Li. Local polynomial regression on unknown manifolds. Tech. Re. Dep. of Stats. UC

Berkley  2006.

[2] S. Kpotufe. Escaping the curse of dimensionality with a tree-based regressor. COLT  2009.
[3] C. J. Stone. Optimal rates of convergence for non-parametric estimators. Ann. Statist.  8:1348–1360 

1980.

[4] C. J. Stone. Optimal global rates of convergence for non-parametric estimators. Ann. Statist.  10:1340–

1353  1982.

[5] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science 

290:2323–2326  2000.

[6] M. Belkin and N. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Computation  15:1373–1396  2003.

[7] J.B. TenenBaum  V. De Silva  and J. Langford. A global geometric framework for non-linear dimension-

ality reduction. Science  290:2319–2323  2000.

[8] S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. STOC  2008.
[9] K. Clarkson. Nearest-neighbor searching and metric space dimensions. Nearest-Neighbor Methods for

Learning and Vision: Theory and Practice  2005.

[10] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under arbitrary sampling.

IEEE Transactions on Information Theory  41  1995.

[11] S. Schaal and C. Atkeson. Robot Juggling: An Implementation of Memory-based Learning. Control

Systems Magazine  IEEE  1994.

[12] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution Free Theory of Nonparametric Regression.

Springer  New York  NY  2002.

[13] D. Lee and A. Grey. Faster gaussian summation: Theory and experiment. UAI  2006.
[14] D. Lee and A. Grey. Fast high-dimensional kernel summations using the monte carlo multipole method.

NIPS  2008.

[15] C. Atkeson  A. Moore  and S. Schaal. Locally weighted learning. AI Review  1997.
[16] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbors. ICML  2006.
[17] R. Krauthgamer and J. Lee. Navigating nets: Simple algorithms for proximity search. SODA  2004.

9

,Anima Anandkumar
Daniel Hsu
Majid Janzamin
Sham Kakade
Mahito Sugiyama
Karsten Borgwardt