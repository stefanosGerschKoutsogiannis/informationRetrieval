2017,An inner-loop free solution to inverse problems using deep neural networks,We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator  a least squares regression that includes a big matrix inversion  and an explicit solution for updating the dual variables. Typically  inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion.  To avoid such drawbacks or limitations  we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically  we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy  leading to so-called amortized inference. For matrix inversion in the second sub-problem  we learn a convolutional neural network to approximate the matrix inversion  i.e.  the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements  i.e.  data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.,An Inner-loop Free Solution to Inverse Problems

using Deep Neural Networks

Kai Fai∗

Duke University

kai.fan@stat.duke.edu

Qi Wei∗

Duke University

qi.wei@duke.edu

Lawrence Carin
Duke University

lcarin@duke.edu

Katherine Heller
Duke University

kheller@stat.duke.edu

Abstract

We propose a new method that uses deep learning techniques to accelerate the
popular alternating direction method of multipliers (ADMM) solution for inverse
problems. The ADMM updates consist of a proximity operator  a least squares
regression that includes a big matrix inversion  and an explicit solution for updating
the dual variables. Typically  inner loops are required to solve the ﬁrst two sub-
minimization problems due to the intractability of the prior and the matrix inversion.
To avoid such drawbacks or limitations  we propose an inner-loop free update rule
with two pre-trained deep convolutional architectures. More speciﬁcally  we learn
a conditional denoising auto-encoder which imposes an implicit data-dependent
prior/regularization on ground-truth in the ﬁrst sub-minimization problem. This
design follows an empirical Bayesian strategy  leading to so-called amortized
inference. For matrix inversion in the second sub-problem  we learn a convolutional
neural network to approximate the matrix inversion  i.e.  the inverse mapping is
learned by feeding the input through the learned forward network. Note that
training this neural network does not require ground-truth or measurements  i.e. 
data-independent. Extensive experiments on both synthetic data and real datasets
demonstrate the efﬁciency and accuracy of the proposed method compared with
the conventional ADMM solution using inner loops for solving inverse problems.

1

Introduction

Most of the inverse problems are formulated directly to the setting of an optimization problem related
to the a forward model [25]. The forward model maps unknown signals  i.e.  the ground-truth  to
acquired information about them  which we call data or measurements. This mapping  or forward
problem  generally depends on a physical theory that links the ground-truth to the measurements.
Solving inverse problems involves learning the inverse mapping from the measurements to the ground-
truth. Speciﬁcally  it recovers a signal from a small number of degraded or noisy measurements. This
is usually ill-posed [26  25]. Recently  deep learning techniques have emerged as excellent models
and gained great popularity for their widespread success in allowing for efﬁcient inference techniques
on applications include pattern analysis (unsupervised)  classiﬁcation (supervised)  computer vision 
image processing  etc [6]. Exploiting deep neural networks to help solve inverse problems has been
explored recently [24  1] and deep learning based methods have achieved state-of-the-art performance
in many challenging inverse problems like super-resolution [3  24]  image reconstruction [20] 

∗The authors contributed equally to this work.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

automatic colorization [13]. More speciﬁcally  massive datasets currently enables learning end-to-end
mappings from the measurement domain to the target image/signal/data domain to help deal with these
challenging problems instead of solving the inverse problem by inference. This mapping function
from degraded data point to ground-truth has recently been characterized by using sophisticated
networks  e.g.  deep neural networks. A strong motivation to use neural networks stems from the
universal approximation theorem [5]  which states that a feed-forward network with a single hidden
layer containing a ﬁnite number of neurons can approximate any continuous function on compact
subsets of Rn  under mild assumptions on the activation function.
More speciﬁcally  in recent work [3  24  13  20]  an end-to-end mapping from measurements y to
ground-truth x was learned from the training data and then applied to the testing data. Thus  the
complicated inference scheme needed in the conventional inverse problem solver was replaced by
feeding a new measurement through the pre-trained network  which is much more efﬁcient. To
improve the scope of deep neural network models  more recently  in [4]  a splitting strategy was
proposed to decompose an inverse problem into two optimization problems  where one sub-problem 
related to regularization  can be solved efﬁciently using trained deep neural networks  leading to
an alternating direction method of multipliers (ADMM) framework [2  17]. This method involves
training a deep convolutional auto-encoder network for low-level image modeling  which explicitly
imposes regularization that spans the subspace that the ground-truth images live in. For the sub-
problem that requires inverting a big matrix  a conventional gradient descent algorithm was used 
leading to an alternating update  iterating between feed-forward propagation through a network and
iterative gradient descent. Thus  an inner loop for gradient descent is still necessary in this framework.
A similar approach to learn approximate ISTA with neural network is illustrated in [11].
In this work  we propose an inner-loop free framework  in the sense that no iterative algorithm
is required to solve sub-problems  using a splitting strategy for inverse problems. The alternating
updates for the two sub-problems were derived by feeding through two pre-trained deep neural
networks  i.e.  one using an amortized inference based denoising convolutional auto-encoder network
for the proximity operation and one using structured convolutional neural networks for the huge
matrix inversion related to the forward model. Thus  the computational complexity of each iteration
in ADMM is linear with respect to the dimensionality of the signals. The network for the proximity
operation imposes an implicit prior learned from the training data  including the measurements as well
as the ground-truth  leading to amortized inference. The network for matrix inversion is independent
from the training data and can be trained from noise  i.e.  a random noise image and its output from
the forward model. To make training the networks for the proximity operation easier  three tricks have
been employed: the ﬁrst one is to use a pixel shufﬂing technique to equalize the dimensionality of the
measurements and ground-truth; the second one is to optionally add an adversarial loss borrowed
from the GAN (Generative Adversarial Nets) framework [10] for sharp image generation; the last one
is to introduce a perceptual measurement loss derived from pre-trained networks  such as AlexNet
[12] or VGG-16 Model [23]. Arguably  the speed of the proposed algorithm  which we term Inf-
ADMM-ADNN (Inner-loop free ADMM with Auxiliary Deep Neural Network)  comes from the fact
that it uses two auxiliary pre-trained networks to accelerate the updates of ADMM.
Contribution The main contribution of this paper is comprised of i) learning an implicit
prior/regularizer using a denoising auto-encoder neural network  based on amortized inference;
ii) learning the inverse of a big matrix using structured convolutional neural networks  without using
training data; iii) each of the above networks can be exploited to accelerate the existing ADMM
solver for inverse problems.

2 Linear Inverse Problem
Notation: trainable networks by calligraphic font  e.g.  A  ﬁxed networks by italic font e.g.  A. As
mentioned in the last section  the low dimensional measurement is denoted as y ∈ Rm  which is
reduced from high dimensional ground truth x ∈ Rn by a linear operator A such that y = Ax. Note
that usually n ≥ m  which makes the number of parameters to estimate no smaller than the number
of data points in hand. This imposes an ill-posed problem for ﬁnding solution x on new observation
y  since A is an underdetermined measurement matrix. For example  in a super-resolution set-up  the
matrix A might not be invertible  such as the strided Gaussian convolution in [21  24]. To overcome
this difﬁculty  several computational strategies  including Markov chain Monte Carlo (MCMC) and
tailored variable splitting under the ADMM framework  have been proposed and applied to different

2

xk+1 = arg min

x

β(cid:107)x − zk + uk/2β(cid:107)2 + λR(x; y)
(cid:107)y − Az(cid:107)2 + β(cid:107)xk+1 − z + uk/2β(cid:107)2

zk+1 = arg min
uk+1 = uk + 2β(xk+1 − zk+1).

z

(3)

(4)

(cid:107)y − Az(cid:107)2 + λR(x) 

s.t. z = x

kinds of priors  e.g.  the empirical Gaussian prior [29  32]  the Total Variation prior [22  30  31]  etc.
In this paper  we focus on the popular ADMM framework due to its low computational complexity
and recent success in solving large scale optimization problems. More speciﬁcally  the optimization
problem is formulated as

ˆx = arg min
x z

(1)
where the introduced auxiliary variable z is constrained to be equal to x  and R(x) captures the
structure promoted by the prior/regularization.
If we design the regularization in an empirical
Bayesian way  by imposing an implicit data dependent prior on x  i.e.  R(x; y) for amortized
inference [24]  the augmented Lagrangian for (1) is

L(x  z  u) = (cid:107)y − Az(cid:107)2 + λR(x; y) + (cid:104)u  x − z(cid:105) + β(cid:107)x − z(cid:107)2

(2)
where u is the Lagrange multiplier  and β > 0 is the penalty parameter. The usual augmented
Lagrange multiplier method is to minimize L w.r.t. x and z simultaneously. This is difﬁcult and does
not exploit the fact that the objective function is separable. To remedy this issue  ADMM decomposes
the minimization into two subproblems that are minimizations w.r.t. x and z  respectively. More
speciﬁcally  the iterations are as follows:

(5)
If the prior R is appropriately chosen  such as (cid:107)x(cid:107)1  a closed-form solution for (3)  i.e.  a soft
thresholding solution is naturally desirable. However  for some more complicated regularizations 
e.g.  a patch based prior [8]  solving (3) is nontrivial  and may require iterative methods. To solve
(4)  a matrix inversion is necessary  for which conjugate gradient descent (CG) is usually applied to
update z [4]. Thus  solving (3) and (4) is in general cumbersome. Inner loops are required to solve
these two sub-minimization problems due to the intractability of the prior and the inversion  resulting
in large computational complexity. To avoid such drawbacks or limitations  we propose an inner
loop-free update rule with two pretrained deep convolutional architectures.

3

Inner-loop free ADMM

1

x = PR(v; y) ⇔ 0 ∈ ∂R(·; y)(x) + x − v ⇔ v − x ∈ ∂R(·; y)(x)

3.1 Amortized inference for x using a conditional proximity operator
Solving sub-problem (3) is equivalent to ﬁnding the solution of the proximity operator PR(v; y) =
2β into R without loss of
2(cid:107)x − v(cid:107)2 + R(x; y)  where we incorporate the constant λ
arg minx
generality. If we impose the ﬁrst order necessary conditions [18]  we have
(6)
where ∂R(·; y) is a partial derivative operator. For notational simplicity  we deﬁne another operator
F =: I + ∂R(·; y). Thus  the last condition in (6) indicates that xk+1 = F−1(v). Note that the
inverse here represents the inverse of an operator  i.e.  the inverse function of F. Thus our objective is
to learn such an inverse operator which projects v into the prior subspace. For simple priors like (cid:107)·(cid:107)1
or (cid:107) · (cid:107)2
2  the projection can be efﬁciently computed. In this work  we propose an implicit example-
based prior  which does not have a truly Bayesian interpretation  but aids in model optimization.
In line with this prior  we deﬁne the implicit proximity operator Gθ(x; v  y) parameterized by θ to
approximate unknown F−1. More speciﬁcally  we propose a neural network architecture referred to
as conditional Pixel Shufﬂing Denoising Auto-Encoders (cPSDAE) as the operator G  where pixel
shufﬂing [21] means periodically reordering the pixels in each channel mapping a high resolution
image to a low resolution image with scale r and increase the number of channels to r2 (see [21] for
more details). This allows us to transform v so that it is the same scale as y  and concatenate it with
y as the input of cPSDAE easily. The architecture of cPSDAE is shown in Fig. 1 (d).

3.2

Inversion-free update of z

While it is straightforward to write down the closed-form solution for sub-problem (4) w.r.t. z as is
shown in (7)  explicitly computing this solution is nontrivial.

zk+1 = K(cid:0)A(cid:62)y + βxk+1 + uk/2(cid:1)   where K =(cid:0)A(cid:62)A + βI(cid:1)−1

(7)

3

(a)

(b)

(c)

(d)

(e)

Figure 1: Network for updating z (in black): (a) loss function (9)  (b) structure of B−1  (c) struture of Cφ.
Note that the input  is random noise independent from the training data. Network for updating z (in blue): (d)
structure of cPSDAE Gθ(x; ˜x  y) (˜x plays the same role as v in training)  (e) adversarial training for R(x; y).
Note again that (a)(b)(c) describes the network for inferring z  which is data-independent and (d)(e) describes
the network for inferring x  which is data-dependent.

In (7)  A(cid:62) is the transpose of the matrix A. As we mentioned  the term K in the right hand side
involves an expensive matrix inversion with computational complexity O(n3) . Under some speciﬁc
assumptions  e.g.  A is a circulant matrix  this matrix inversion can be accelerated with a Fast Fourier
transformation  which has a complexity of order O(n log n). Usually  the gradient based update
has linear complexity in each iteration and thus has an overall complexity of order O(nint log n) 
where nint is the number of iterations. In this work  we will learn this matrix inversion explicitly
by designing a neural network. Note that K is only dependent on A  and thus can be computed in
advance for future use. This problem can be reduced to a smaller scale matrix inversion by applying
the Sherman-Morrison-Woodbury formula:

K = β−1(cid:0)I − A(cid:62)BA(cid:1)   where B =(cid:0)βI + AA(cid:62)(cid:1)−1

.

(8)
Therefore  we only need to solve the matrix inversion in dimension m × m  i.e.  estimating B. We
propose an approach to approximate it by a trainable deep convolutional neural network Cφ ≈ B
parameterized by φ. Note that B−1 = λI + AA(cid:62) can be considered as a two-layer fully-connected
or convolutional network as well  but with a ﬁxed kernel. This inspires us to design two auto-encoders
with shared weights  and minimize the sum of two reconstruction losses to learn the inversion Cφ :
(9)

(cid:2)(cid:107)ε − CφB−1ε(cid:107)2

2 + (cid:107)ε − B−1Cφε(cid:107)2

arg min

Eε

(cid:3)

2

φ

where ε is sampled from a standard Gaussian distribution. The loss in (9) is clearly depicted in Fig. 1
(a) with the structure of B−1 in Fig. 1 (b) and the structure of Cφ in Fig. 1 (c). Since the matrix B is
symmetric  we can reparameterize Cφ as WφW(cid:62)
φ   where Wφ represents a multi-layer convolutional
network and W(cid:62)
φ is a symmetric convolution transpose architecture using shared kernels with Wφ 
as shown in Fig. 1 (c) (the blocks with the same colors share the same network parameters). By

plugging the learned Cφ in (8)   we obtain a reusable deep neural network Kφ = β−1(cid:0)I − A(cid:62)CφA(cid:1)

as a surrogate for the exact inverse matrix K. The update of z at each iteration can be done by
applying the same Kφ as follows:

zk+1 ← β−1(cid:0)I − A(cid:62)CφA(cid:1)(cid:0)A(cid:62)y + βxk+1 + uk/2(cid:1) .

(10)

3.3 Adversarial training of cPSDAE

In this section  we will describe the proposed adversarial training scheme for cPSDAE to update
x. Suppose that we have the paired training dataset (xi  yi)N
i=1  a single cPSDAE with the input
pair (˜x  y) is trying to minimize the reconstruction error Lr(Gθ(˜x  y)  x)  where ˜x is a corrupted
version of x  i.e.  ˜x = x + n where n is random noise. Notice Lr in traditional DAE is commonly

4

deﬁned as (cid:96)2 loss  however  (cid:96)1 loss is an alternative in practice. Additionally  we follow the idea in
[19  7] by introducing a discriminator and a comparator to help train the cPSDAE  and ﬁnd that it can
produce sharper or higher quality images than merely optimizing G. This will wrap our conditional
generative model Gθ into the conditional GAN [10] framework with an extra feature matching
network (comparator). Recent advances in representation learning problems have shown that the
features extracted from well pre-trained neural networks on supervised classiﬁcation problems can
be successfully transferred to others tasks  such as zero-shot learning [15]  style transfer learning
[9]. Thus  we can simply use pre-trained AlexNet [12] or VGG-16 Model [23] on ImageNet as the
comparator without ﬁne-tuning in order to extract features that capture complex and perceptually
important properties. The feature matching loss Lf (C(Gθ(˜x  y))  C(x)) is usually the (cid:96)2 distance of
high level image features  where C represents the pre-trained network. Since C is ﬁxed  the gradient
of this loss can be back-propagated to θ.
For the adversarial training  the discriminator Dψ is a trainable convolutional network. We can keep
the standard discriminator loss as in a traditional GAN  and add the generator loss of the GAN to the
previously deﬁned DAE loss and comparator loss. Thus  we can write down our two objectives 

LD(x  y) = − log Dψ(x) − log (1 − Dψ(Gθ(˜x  y)))
LG(x  y) = λr(cid:107)Gθ(˜x  y) − x(cid:107)2

(11)
(12)
The optimization involves iteratively updating ψ by minimizing LD keeping θ ﬁxed  and then
updating θ by minimizing LG keeping ψ ﬁxed. The proposed method  including training and
inference has been summarized in Algorithm 1. Note that each update of x or z using neural networks
in an ADMM iteration has a complexity of linear order w.r.t. the data dimensionality n.

2 + λf(cid:107)C(Gθ(˜x  y)) − C(x)(cid:107)2

2 − λa log Dψ(Gθ(˜x  y))

3.4 Discussion

Update x cf. xk+1 = F−1(v);
Update z cf. (10);
Update u cf. (5);

R(x; y)
Testing stage:
1: for t = 1  2  . . . do
2:
3:
4:
5: end for

Algorithm 1 Inner-loop free ADMM with Auxil-
iary Deep Neural Nets (Inf-ADMM-ADNN)
Training stage:
1: Train net Kφ for inverting AT A + βI
2: Train net cPSDAE for proximity operator of

A critical point for learning-based methods is
whether the method generalizes to other prob-
lems. More speciﬁcally  how does a method that
is trained on a speciﬁc dataset perform when ap-
plied to another dataset? To what extent can we
reuse the trained network without re-training?
In the proposed method  two deep neural net-
works are trained to infer x and z. For the
network w.r.t. z  the training only requires the
forward model A to generate the training pairs
(  A). The trained network for z can be applied
for any other datasets as long as A remains the
same. Thus  this network can be adapted eas-
ily to accelerate inference for inverse problems
without training data. However  for inverse prob-
lems that depends on a different A  a re-trained network is required. It is worth mentioning that the
forward model A can be easily learned using training dataset (x  y)  leading to a fully blind estimator
associated with the inverse problem. An example of learning ˆA can be found in the supplementary
materials. For the network w.r.t. x  training requires data pairs (xi  yi) because of the amortized
inference. Note that this is different from training a prior for x only using training data xi. Thus 
the trained network for x is conﬁned to the speciﬁc tasks constrained by the pairs (x  y). To extend
the generality of the trained network  the amortized setting can be removed  i.e  y is removed from
the training  leading to a solution to proximity operator PR(v) = arg minx
2(cid:107)x − v(cid:107)2 + R(x).
This proximity operation can be regarded as a denoiser which projects the noisy version v of x into
the subspace imposed by R(x). The trained network (for the proximity operator) can be used as a
plug-and-play prior [27] to regularize other inverse problems for datasets that share similar statistical
characteristics. However  a signiﬁcant change in the training dataset  e.g.  different modalities like
MRI and natural images (e.g.  ImageNet [12])  would require re-training.
Another interesting point to mention is the scalability of the proposed method to data of different
dimensions. The scalability can be adapted using patch-based methods without loss of generality. For
example  a neural network is trained for images of size 64× 64 but the test image is of size 256× 256.
To use this pre-trained network  the full image can be decomposed as four 64 × 64 images and fed to

1

5

the network. To overcome the possible blocking artifacts  eight overlapping patches can be drawn
from the full image and fed to the network. The output of these eight patches are then averaged
(unweighted or weighted) over the overlapping parts. A similar strategy using patch stitching can be
exploited to feed small patches to the network for higher dimensional datasets.

4 Experiments

In this section  we provide experimental results and analysis on the proposed Inf-ADMM-ADNN and
compare the results with a conventional ADMM using inner loops for inverse problems. Experiments
on synthetic data have been implemented to show the fast convergence of our method  which comes
from the efﬁcient feed-forward propagation through pre-trained neural networks. Real applications
using proposed Inf-ADMM-ADNN have been explored  including single image super-resolution 
motion deblurring and joint super-resolution and colorization.

4.1 Synthetic data

1

2β

0

|a| ≤ κ

2 + 1

2σ2(cid:107)y − Ax(cid:107)2

To evaluate the performance of proposed Inf-ADMM-ADNN  we ﬁrst test the neural network
Kφ  approximating the matrix inversion on synthetic data. More speciﬁcally  we assume that the
ground-truth x is drawn from a Laplace distribution Laplace(µ  b)  where µ = 0 is the location
parameter and b is the scale parameter. The forward model A is a sparse matrix representing
convolution with a stride of 4. The architecture of A is available in the supplementary materials
(see Section 2). The noise n is drawn from a standard Gaussian distribution N (0  σ2). Thus  the
observed data is generated as y = Ax + n. Following Bayes theorem  the maximum a posterior
estimate of x given y  i.e.  maximizing p(x|y) ∝ p(y|x)p(x) can be equivalently formulated as
b(cid:107)x(cid:107)1  where b = 1 and σ = 1 in this setting. Following (3)  (4) 
arg minx
(5)  this problem is reduced to the following three sub-problems: i) xk+1 = S 1
(zk − uk/2β);
2 + β(cid:107)xk+1 − z + uk/2β(cid:107)2
2; iii) uk+1 = uk + 2β(xk+1 − zk+1) 
ii) zk+1 = arg minz (cid:107)y − Az(cid:107)2
where the soft thresholding operator S is deﬁned as Sκ(a) =
a − sgn(a)κ |a| > κ and
sgn(a) extracts the sign of a. The update of xk+1 has a closed-form solution  i.e.  soft thresholding
of zk − uk/2β. The update of zk+1 requires the inversion of a big matrix  which is usually solved
using a gradient descent based algorithm. The update of uk+1 is straightforward. Thus  we compare
the gradient descent based update  a closed-form solution for matrix inversion2 and the proposed
inner-free update using a pre-trained neural network. The evolution of the objective function w.r.t.
the number of iterations and the time has been plotted in the left and middle of Figs. 2. While all
three methods perform similarly from iteration to iteration (in the left of Figs. 2)  the proposed inner-
loop free based and closed-form inversion based methods converge much faster than the gradient
based method (in the middle of Figs. 2). Considering the fact that the closed-form solution  i.e.  a
direct matrix inversion  is usually not available in practice  the learned neural network allows us to
approximate the matrix inversion in a very accurate and efﬁcient way.

(cid:26)

Figure 2: Synthetic data: (left) objective v.s. iterations  (middle) objective v.s. time. MNIST dataset: (right)
NMSE v.s. iterations for MNIST image 4× super-resolution.

2Note that this matrix inversion can be explicitly computed due to its small size in this toy experiment. In

practice  this matrix is not built explicitly.

6

051015202530iterations11.21.41.61.8objective104GD-basedClosed-formProposed2224262887508760877002468time/s11.21.41.61.8objective104GD-basedClosed-formProposed0.050.10.150.29000950010000050100150iterations0.10.20.30.40.50.60.7NMSE = 0.0001 = 0.0005 = 0.001 = 0.005 = 0.01 = 0.1Figure 3: Top two rows : (column 1) LR images  (column 2) bicubic interpolation (×4)  (column 3) results
using proposed method (×4)  (column 4) HR image. Bottom row: (column 1) motion blurred images  (column
2) results using Wiener ﬁlter with the best performance by tuning regularization parameter  (column 3) results
using proposed method  (column 4) ground-truth.

4.2

Image super-resolution and motion deblurring

In this section  we apply the proposed Inf-ADMM-ADNN to solve the poplar image super-resolution
problem. We have tested our algorithm on the MNIST dataset [14] and the 11K images of the
Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset [28]. In the ﬁrst two rows of Fig. 3  high
resolution images  as shown in the last column  have been blurred (convolved) using a Gaussian
kernel of size 3 × 3 and downsampled every 4 pixels in both vertical and horizontal directions
to generate the corresponding low resolution images as shown in the ﬁrst column. The bicubic
interpolation of LR images and results using proposed Inf-ADMM-ADNN on a 20% held-out test
set are displayed in column 2 and 3. Visually  the proposed Inf-ADMM-ADNN gives much better
results than the bicubic interpolation  recovering more details including colors and edges. A similar
task to super-resolution is motion deblurring  in which the convolution kernel is a directional kernel
and there is no downsampling. The motion deblurring results using Inf-ADMM-ADNN are displayed
in the bottom of Fig. 3 and are compared with the Wiener ﬁltered deblurring result (the performance
of Wiener ﬁlter has been tuned to the best by adjusting the regularization parameter). Obviously  the
Inf-ADMM-ADNN gives visually much better results than the Wiener ﬁlter. Due to space limitations 
more simulation results are available in supplementary materials (see Section 3.1 and 3.2).
To explore the convergence speed w.r.t. the ADMM regularization parameter β  we have plotted
the normalized mean square error (NMSE) deﬁned as NMSE = (cid:107)ˆx − x(cid:107)2
2  of super-resolved
MNIST images w.r.t. ADMM iterations using different values of β in the right of Fig. 2. It is
interesting to note that when β is large  e.g.  0.1 or 0.01  the NMSE of ADMM updates converges
to a stable value rapidly in a few iterations (less than 10). Reducing the value of β slows down the
decay of NMSE over iterations but reaches a lower stable value. When the value of β is small enough 
e.g.  β = 0.0001  0.0005  0.001  the NMSE converges to the identical value. This ﬁts well with the
claim in Boyd’s book [2] that when β is too large it does not put enough emphasis on minimizing the

2/(cid:107)x(cid:107)2

7

objective function  causing coarser estimation; thus a relatively small β is encouraged in practice.
Note that the selection of this regularization parameter is still an open problem.

4.3

Joint super-resolution and colorization

While image super-resolution tries to enhance spatial resolution from spatially degraded images  a
related application in the spectral domain exists  i.e.  enhancing spectral resolution from a spectrally
degraded image. One interesting example is the so-called automatic colorization  i.e.  hallucinating a
plausible color version of a colorless photograph. To the best knowledge of the authors  this is the
ﬁrst time we can enhance both spectral and spatial resolutions from one single band image. In this
section  we have tested the ability to perform joint super-resolution and colorization from one single
colorless LR image on the celebA-dataset [16]. The LR colorless image  its bicubic interpolation
and ×2 HR image are displayed in the top row of Fig. 4. The ADMM updates in the 1st  4th and
7th iterations (on held-out test set) are displayed in the bottom row  showing that the updated image
evolves towards higher quality. More results are in the supplementary materials (see Section 3.3).

Figure 4: (top left) colorless LR image  (top middle) bicubic interpolation  (top right) HR ground-truth  (bottom
left to right) updated image in 1th  4th and 7th ADMM iteration. Note that the colorless LR images and bicubic
interpolations are visually similar but different in details noticed by zooming out.

5 Conclusion

In this paper we have proposed an accelerated alternating direction method of multipliers  namely 
Inf-ADMM-ADNN to solve inverse problems by using two pre-trained deep neural networks. Each
ADMM update consists of feed-forward propagation through these two networks  with a complexity
of linear order with respect to the data dimensionality. More speciﬁcally  a conditional pixel shufﬂing
denoising auto-encoder has been learned to perform amortized inference for the proximity operator.
This auto-encoder leads to an implicit prior learned from training data. A data-independent structured
convolutional neural network has been learned from noise to explicitly invert the big matrix associated
with the forward model  getting rid of any inner loop in an ADMM update  in contrast to the
conventional gradient based method. This network can also be combined with existing proximity
operators to accelerate existing ADMM solvers. Experiments and analysis on both synthetic and real
dataset demonstrate the efﬁciency and accuracy of the proposed method. In future work we hope to
extend the proposed method to inverse problems related to nonlinear forward models.

8

Appendices

We will address the question proposed by reviewers in this Appendix.

To Reviewer 1 The title has been changed to “An inner-loop free solution to inverse problems
using deep neural networks” according to the reviewer’s suggestion  which is in consistence with our
arxiv submission. The pixel shufﬂing used in our PSDAE architecture is mainly to keep the ﬁlter
size of every layer including input and output as the same  thus trick has been practically proved
to remove the check-board effect. Especially for the super-resolution task with different scales of
input/output  it is basically to use the input to regress the same scale output but with more channels.

Figure 5: Result of super-resolution from SRGAN with different settings.

To Reviewer 2 As we explained in the rebuttal  we have the implementation of SRCNN with or
without adversarial loss in our own but we did not successfully reproduce a reasonable result in our
dataset. Thus  we did not include the visualization in the initial submission  since either blurriness or
check-board effect will appear  but we will further ﬁne-tune the model or use other tricks such as
pixel shufﬂing. [11] has been added to the reference.

To Reviewer 3 Most of the questions have been addressed in the rebuttal.

9

Acknowledgments

The authors would like to thank Siemens Corporate Research for supporting this work and thank
NVIDIA for the GPU donations.

References
[1] Jonas Adler and Ozan Öktem. Solving ill-posed inverse problems using iterative deep neural

networks. arXiv preprint arXiv:1704.04058  2017.

[2] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning  3(1):1–122  2011.

[3] Joan Bruna  Pablo Sprechmann  and Yann LeCun. Super-resolution with deep convolutional

sufﬁcient statistics. arXiv preprint arXiv:1511.05666  2015.

[4] JH Chang  Chun-Liang Li  Barnabas Poczos  BVK Kumar  and Aswin C Sankaranarayanan.
One network to solve them all—solving linear inverse problems using deep projection models.
arXiv preprint arXiv:1703.09912  2017.

[5] Balázs Csanád Csáji. Approximation with artiﬁcial neural networks. Faculty of Sciences  Etvs
[6] Li Deng  Dong Yu  et al. Deep learning: methods and applications. Foundations and Trends R(cid:13)

Lornd University  Hungary  24:48  2001.

in Signal Processing  7(3–4):197–387  2014.

[7] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics
based on deep networks. In Advances in Neural Information Processing Systems  pages 658–666 
2016.

[8] Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations

over learned dictionaries. IEEE Trans. Image Process.  15(12):3736–3745  2006.

[9] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. Image style transfer using convolutional
neural networks. In Proc. IEEE Int. Conf. Comp. Vision and Pattern Recognition (CVPR)  pages
2414–2423  2016.

[10] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems  pages 2672–2680  2014.

[11] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings
of the 27th International Conference on Machine Learning (ICML-10)  pages 399–406  2010.
[12] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems  pages
1097–1105  2012.

[13] Gustav Larsson  Michael Maire  and Gregory Shakhnarovich. Learning representations for au-
tomatic colorization. In Proc. European Conf. Comp. Vision (ECCV)  pages 577–593. Springer 
2016.

[14] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proc. IEEE  86(11):2278–2324  1998.

[15] Jimmy Lei Ba  Kevin Swersky  Sanja Fidler  et al. Predicting deep zero-shot convolutional
neural networks using textual descriptions. In Proc. IEEE Int. Conf. Comp. Vision (ICCV) 
pages 4247–4255  2015.

[16] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proc. IEEE Int. Conf. Comp. Vision (ICCV)  pages 3730–3738  2015.

[17] Songtao Lu  Mingyi Hong  and Zhengdao Wang. A nonconvex splitting method for symmetric
nonnegative matrix factorization: Convergence analysis and optimality. IEEE Transactions on
Signal Processing  65(12):3120–3135  June 2017.

[18] Helmut Maurer and Jochem Zowe. First and second-order necessary and sufﬁcient optimality
conditions for inﬁnite-dimensional programming problems. Math. Progam.  16(1):98–110 
1979.

10

[19] Anh Nguyen  Jason Yosinski  Yoshua Bengio  Alexey Dosovitskiy  and Jeff Clune. Plug & play
generative networks: Conditional iterative generation of images in latent space. arXiv preprint
arXiv:1612.00005  2016.

[20] Jo Schlemper  Jose Caballero  Joseph V Hajnal  Anthony Price  and Daniel Rueckert. A
deep cascade of convolutional neural networks for MR image reconstruction. arXiv preprint
arXiv:1703.00555  2017.

[21] Wenzhe Shi  Jose Caballero  Ferenc Huszár  Johannes Totz  Andrew P Aitken  Rob Bishop 
Daniel Rueckert  and Zehan Wang. Real-time single image and video super-resolution using an
efﬁcient sub-pixel convolutional neural network. In Proc. IEEE Int. Conf. Comp. Vision and
Pattern Recognition (CVPR)  pages 1874–1883  2016.

[22] M. Simoes  J. Bioucas-Dias  L.B. Almeida  and J. Chanussot. A convex formulation for
hyperspectral image superresolution via subspace-based regularization. IEEE Trans. Geosci.
Remote Sens.  53(6):3373–3388  Jun. 2015.

[23] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556  2014.

[24] Casper Kaae Sønderby  Jose Caballero  Lucas Theis  Wenzhe Shi  and Ferenc Huszár. Amortised

MAP inference for image super-resolution. arXiv preprint arXiv:1610.04490  2016.

[25] Albert Tarantola. Inverse problem theory and methods for model parameter estimation. SIAM 

2005.

[26] A.N. Tikhonov and V.I.A. Arsenin. Solutions of ill-posed problems. Scripta series in mathemat-

ics. Winston  1977.

[27] Singanallur V Venkatakrishnan  Charles A Bouman  and Brendt Wohlberg. Plug-and-play priors
for model based reconstruction. In Proc. IEEE Global Conf. Signal and Information Processing
(GlobalSIP)  pages 945–948. IEEE  2013.

[28] Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The

caltech-ucsd birds-200-2011 dataset. 2011.

[29] Q. Wei  N. Dobigeon  and Jean-Yves Tourneret. Bayesian fusion of multi-band images. IEEE J.

Sel. Topics Signal Process.  9(6):1117–1127  Sept. 2015.

[30] Qi Wei  Nicolas Dobigeon  and Jean-Yves Tourneret. Fast fusion of multi-band images based
on solving a Sylvester equation. IEEE Trans. Image Process.  24(11):4109–4121  Nov. 2015.
[31] Qi Wei  Nicolas Dobigeon  Jean-Yves Tourneret  J. M. Bioucas-Dias  and Simon Godsill.
R-FUSE: Robust fast fusion of multi-band images based on solving a Sylvester equation. IEEE
Signal Process. Lett.  23(11):1632–1636  Nov 2016.
[32] N. Zhao  Q. Wei  A. Basarab  N. Dobigeon  D. Kouamé  and J. Y. Tourneret. Fast single image
super-resolution using a new analytical solution for (cid:96)2 − (cid:96)2 problems. IEEE Trans. Image
Process.  25(8):3683–3697  Aug. 2016.

11

,Kai Fan
Qi Wei
Lawrence Carin
Katherine Heller