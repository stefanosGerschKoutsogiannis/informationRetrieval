2015,Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing,The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems  but it involves minimization sub-steps that are often difficult to solve efficiently.   The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler substeps than ADMM  thus producing lower complexity solvers. Despite the flexibility of this method  PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency  or even achieve convergence.  We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence.  We rigorously analyze our methods  and identify convergence rates.  Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.,Adaptive Primal-Dual Splitting Methods for
Statistical Learning and Image Processing

Thomas Goldstein⇤

Department of Computer Science

University of Maryland

College Park  MD

Min Li†

School of Economics and Management

Southeast University

Nanjing  China

Xiaoming Yuan‡

Department of Mathematics
Hong Kong Baptist University
Kowloon Tong  Hong Kong

Abstract

The alternating direction method of multipliers (ADMM) is an important tool for
solving complex optimization problems  but it involves minimization sub-steps
that are often difﬁcult to solve efﬁciently. The Primal-Dual Hybrid Gradient
(PDHG) method is a powerful alternative that often has simpler sub-steps than
ADMM  thus producing lower complexity solvers. Despite the ﬂexibility of this
method  PDHG is often impractical because it requires the careful choice of multi-
ple stepsize parameters. There is often no intuitive way to choose these parameters
to maximize efﬁciency  or even achieve convergence. We propose self-adaptive
stepsize rules that automatically tune PDHG parameters for optimal convergence.
We rigorously analyze our methods  and identify convergence rates. Numerical
experiments show that adaptive PDHG has strong advantages over non-adaptive
methods in terms of both efﬁciency and simplicity for the user.

1

Introduction

Splitting methods such as ADMM [1  2  3] have recently become popular for solving problems
in distributed computing  statistical regression  and image processing. ADMM allows complex
problems to be broken down into sequences of simpler sub-steps  usually involving large-scale least
squares minimizations. However  in many cases these least squares minimizations are difﬁcult to
directly compute.
In such situations  the Primal-Dual Hybrid Gradient method (PDHG) [4  5] 
also called the linearized ADMM [4  6]  enables the solution of complex problems with a simpler
sequence of sub-steps that can often be computed in closed form. This ﬂexibility comes at a cost
– the PDHG method requires the user to choose multiple stepsize parameters that jointly determine
the convergence of the method. Without having extensive analytical knowledge about the problem
being solved (such as eigenvalues of linear operators)  there is no intuitive way to select stepsize
parameters to obtain fast convergence  or even guarantee convergence at all.
In this article we introduce and analyze self-adaptive variants of PDHG – variants that automatically
tune stepsize parameters to attain (and guarantee) fast convergence without user input. Applying
adaptivity to splitting methods is a difﬁcult problem. It is known that naive adaptive variants of

⇤tomg@cs.umd.edu
†limin@seu.edu.cn
‡xmyuan@hkbu.edu.hk

1

ADMM are non-convergent  however recent results prove convergence when speciﬁc mathematical
requirements are enforced on the stepsizes [7]. Despite this progress  the requirements for conver-
gence of adaptive PDHG have been unexplored. This is surprising  given that stepsize selection is a
much bigger issue for PDHG than for ADMM because it requires multiple stepsize parameters.
The contributions of this paper are as follows. First  we describe applications of PDHG and its
advantages over ADMM. We then introduce a new adaptive variant of PDHG. The new algorithm not
only tunes parameters for fast convergence  but contains a line search that guarantees convergence
when stepsize restrictions are unknown to the user. We analyze the convergence of adaptive PDHG 
and rigorously prove convergence rate guarantees. Finally  we use numerical experiments to show
the advantages of adaptivity on both convergence speed and ease of use.

2 The Primal-Dual Hybrid Gradient Method

The PDHG scheme has its roots in the Arrow-Hurwicz method  which was studied by Popov [8].
Research in this direction was reinvigorated by the introduction of PDHG  which converges rapidly
for a wider range of stepsizes than Arrow-Hurwicz. PDHG was ﬁrst presented in [9] and analyzed
for convergence in [4  5]. It was later studied extensively for image segmentation [10]. An extensive
technical study of the method and its variants is given by He and Yuan [11]. Several extensions
of PDHG  including simpliﬁed iterations for the case that f or g is differentiable  are presented by
Condat [12]. Several authors have also derived PDHG as a preconditioned form of ADMM [4  6].
PDHG solves saddle-point problems of the form

min
x2X

max
y2Y

f (x) + yT Ax  g(y).

(1)

for convex f and g. We will see later that an incredibly wide range of problems can be cast as (1).
The steps of PDHG are given by

ˆxk+1 = xk  ⌧kAT yk
xk+1 = arg min
f (x) +

1
2⌧k kx  ˆxk+1k2

ˆyk+1 = yk + kA(2xk+1  xk)
yk+1 = arg min

g(y) +

1
2k ky  ˆyk+1k2

x2X

y2Y

(2)

(3)

(4)

(5)

8>>>>>>><>>>>>>>:

where {⌧k} and {k} are stepsize parameters. Steps (2) and (3) of the method update x  decreasing
the energy (1) by ﬁrst taking a gradient descent step with respect to the inner product term in (1)
and then taking a “backward” or proximal step involving f. In steps (4) and (5)  the energy (1) is
increased by ﬁrst marching up the gradient of the inner product term with respect to y  and then a
backward step is taken with respect to g.
PDHG has been analyzed in the case of constant stepsizes  ⌧k = ⌧ and k = . In particular 
it is known to converge as long as ⌧ < 1/⇢(AT A) [4  5  11]. However  PDHG typically does
not converge when non-constant stepsizes are used  even in the case that k⌧k < 1/⇢(AT A) [13].
Furthermore  it is unclear how to select stepsizes when the spectral properties of A are unknown. In
this article  we identify the speciﬁc stepsize conditions that guarantee convergence in the presence
of adaptivity  and propose a backtracking scheme that can be used when the spectral radius of A is
unknown.

3 Applications

Linear Inverse Problems Many inverse problems and statistical regressions have the form

(6)
where f (the data term) is some convex function  h is a (convex) regularizer (such as the `1-norm) 
A and S are linear operators  and b is a vector of data. Recently  the alternating direction method

h(Sx) + f (Ax  b)

minimize

2

of multipliers (ADMM) has become a popular method for solving such problems. The ADMM
relies on the change of variables y Sx  and generates the following sequence of iterates for some
stepsize ⌧

xk+1 = arg minx f (Ax  b) + (Sx  yk)T k + ⌧
yk+1 = arg miny h(y) + (Sxk+1  y)T k + ⌧
k+1 = k + ⌧ (Sxk+1  yk+1).

2kSx  ykk2
2kSxk+1  yk2

(7)

8<:

The x-update in (7) requires the solution of a (potentially large) least-square problem involving both
A and S. Common formulations such as the consensus ADMM [14] solve these large sub-problems
with direct matrix factorizations  however this is often impractical when either the data matrices are
extremely large or fast transforms (such as FFT  DCT  or Hadamard) cannot be used.
The problem (6) can be put into the form (1) using the Fenchel conjugate of the convex function h 
denoted h⇤  which satisﬁes the important identity

h(z) = max

y

yT z  h⇤(y)

for all z in the domain of h. Replacing h in (6) with this expression involving its conjugate yields

min

x

max

y

f (Ax  b) + yT Sx  h⇤(y)

which is of the form (1). The forward (gradient) steps of PDHG handle the matrix A explicitly 
allowing linear inverse problems to be solved without any difﬁcult least-squares sub-steps. We will
see several examples of this below.

Scaled Lasso The square-root lasso [15] or scaled lasso [16] is a variable selection regression that
obtains sparse solutions to systems of linear equations. Scaled lasso has several advantages over
classical lasso – it is more robust to noise and it enables setting penalty parameters without cross
validation [15  16]. Given a data matrix D and a vector b  the scaled lasso ﬁnds a sparse solution to
the system Dx = b by solving

(8)
for some scaling parameter µ. Note the `2 term in (8) is not squared as in classical lasso. If we write

µkxk1 + kDx  bk2

min

x

µkxk1 = max
ky1k1µ
we can put (8) in the form (1)

yT
1 x 

and

kDx  bk2 = max
ky2k21

yT
2 (Dx  b)

min

x

max

ky1k1µ ky2k21

yT
1 x + yT

2 (Dx  b).

(9)

Unlike ADMM  PDHG does not require the solution of least-squares problems involving D.

1
2kAx  fk2

2

Total-Variation Minimization Total variation [17] is commonly used to solve problems of the
form

(10)
where x is a 2D array (image)  r is the discrete gradient operator  A is a linear operator  and f
contains data. If we add a dual variable y and write µkrxk1 = maxkyk1µ yTrx  we obtain

µkrxk1 +

min

x

max
kyk1µ

min

x

1
2kAx  fk2 + yTrx

(11)

which is clearly of the form (1).
The PDHG solver using formulation (11) avoids the inversion of the gradient operator that is required
by ADMM. This is useful in many applications. For example  in compressive sensing the matrix A
may be a sub-sampled orthogonal Hadamard [18]  wavelet  or Fourier transform [19  20]. In this
case  the proximal sub-steps of PDHG are solvable in closed form using fast transforms because they
do not involve the gradient operator r. The sub-steps of ADMM involve both the gradient operator
and the matrix A simultaneously  and thus require inner loops with expensive iterative solvers.

3

4 Adaptive Formulation

The convergence of PDHG can be measured by the size of the residuals  or gradients of (1) with
respect to the primal and dual variables x and y. These primal and dual gradients are simply

pk+1 = @f (xk+1) + AT yk+1 

and

dk+1 = @g(yk+1) + Axk+1

(12)

where @f and @g denote the sub-differential of f and g. The sub-differential can be directly evalu-
ated from the sequence of PDHG iterates using the optimality condition for (3): 0 2 @f (xk+1) +
(ˆxk+1  xk+1) 2 @f (xk+1). The same method can be
1
⌧k
applied to (5) to obtain @g(yk+1). Applying these results to (12) yields the closed form residuals

(xk+1  ˆxk+1). Rearranging this yields 1

⌧k

pk+1 =

1
⌧k

(xk  xk+1)  AT (yk  yk+1) 

dk+1 =

1
k

(yk  yk+1)  A(xk  xk+1).

(13)

When choosing the stepsize for PDHG  there is a tradeoff between the primal and dual residuals.
Choosing a large ⌧k and a small k drives down the primal residuals at the cost of large dual resid-
uals. Choosing a small ⌧k and large k results in small dual residuals but large primal errors. One
would like to choose stepsizes so that the larger of pk+1 and dk+1 is as small as possible. If we as-
sume the residuals on step k+1 change monotonically with ⌧k  then max{pk+1  dk+1} is minimized
when pk+1 = dk+1. This suggests that we tune ⌧k to “balance” the primal and dual residuals.
To achieve residual balancing  we ﬁrst select a parameter ↵0 < 1 that controls the aggressiveness of
adaptivity. On each iteration  we check whether the primal residual is at least twice the dual. If so 
we increase the primal stepsize to ⌧k+1 = ⌧k/(1 ↵k) and decrease the dual to k+1 = k(1 ↵k).
If the dual residual is at least twice the primal  we do the opposite. When we modify the stepsize  we
shrink the adaptivity level to ↵k+1 = ⌘↵k  for ⌘ 2 (0  1). We will see in Section 5 that this adaptivity
level decay is necessary to guarantee convergence. In our implementation we use ↵0 = ⌘ = .95.
In addition to residual balancing  we check the following backtracking condition after each iteration

c
2⌧k kxk+1  xkk2  2(yk+1  yk)T A(xk+1  xk) +

c
2k kyk+1  ykk2 > 0

(14)

where c 2 (0  1) is a constant (we use c = 0.9) is our experiments. If condition (14) fails  then we
shrink ⌧k and k before the next iteration. We will see in Section 5 that the backtracking condition
(14) is sufﬁcient to guarantee convergence. The complete scheme is listed in Algorithm 1.

Algorithm 1 Adaptive PDHG
1: Choose x0  y0  large ⌧0 and 0  and set ↵0 = ⌘ = 0.95.
2: while kpkk kdkk > tolerance do
3:
4:
5:
6:
7:
8:
9: end while

Compute (xk+1  yk+1) from (xk  yk) using the PDHG updates (2-5)
Check the backtracking condition (14) and if it fails set ⌧k ⌧k/2  k k/2
Compute the residuals (13)  and use them for the following two adaptive updates
If 2kpk+1k < kdk+1k  then set ⌧k+1 = ⌧k(1 ↵k)  k+1 = k/(1 ↵k)  and ↵k+1 = ↵k⌘
If kpk+1k > 2kdk+1k  then set ⌧k+1 = ⌧k/(1 ↵k)  k+1 = k(1 ↵k)  and ↵k+1 = ↵k⌘
If no adaptive updates were triggered  then ⌧k+1 = ⌧k  k+1 = k  and ↵k+1 = ↵k

5 Convergence Theory

In this section  we analyze Algorithm 1 and its rate of convergence. In our analysis  we consider
adaptive variants of PDHG that satisfy the following assumptions. We will see later that these
assumptions guarantee convergence of PDHG with rate O(1/k).

Algorithm 1 trivially satisﬁes Assumption A. The sequence {k} measures the adaptive aggressive-
ness on iteration k  and serves the same role as ↵k in Algorithm 1. The geometric decay of ↵k
ensures that Assumption B holds. The backtracking rule explicitly guarantees Assumption C.

4

Assumptions for Adaptive PDHG
A The sequences {⌧k} and {k} are positive and bounded.
B The sequence {k} is summable  where k = maxn ⌧k⌧k+1
C Either X or Y is bounded  and there is a constant c 2 (0  1) such that for all k > 0
c
c
2k kyk+1  ykk2 > 0.
2⌧k kxk+1  xkk2  2(yk+1  yk)T A(xk+1  xk) +

  0o .

  kk+1

k

⌧k

5.1 Variational Inequality Formulation

For notational simplicity  we deﬁne the composite vector uk = (xk  yk) and the matrices

Mk =✓⌧1

k I AT
A 1

k I◆   Hk =✓⌧1

k I
0

0
1

k I◆   and Q(u) =✓AT y
Ax◆ .

This notation allows us to formulate the optimality conditions for (1) as a variational inequality (VI).
If u? = (x?  y?) is a solution to (1)  then x? is a minimizer of (1). More formally 

Likewise  (1) is maximized by y?  and so

f (x)  f (x?) + (x  x?)T AT y?  0 8 x 2 X.

Subtracting (17) from (16) and letting h(u) = f (x) + g(y) yields the VI formulation

 g(y) + g(y?) + (y  y?)T Ax?  0 8 y 2 Y.

(15)

(16)

(17)

(18)

h(u)  h(u?) + (u  u?)T Q(u?)  0 8u 2 ⌦ 

where ⌦= X ⇥ Y. We say ˜u is an approximate solution to (1) with VI accuracy ✏ if

(19)
where B1(˜u) is a unit ball centered at ˜u. In Theorem 1  we prove O(1/k) ergodic convergence of
adaptive PDHG using the VI notion of convergence.

h(u)  h(˜u) + (u  ˜u)T Q(˜u)  ✏ 8u 2 B1(˜u) \ ⌦ 

5.2 Preliminary Results

We now prove several results about the PDHG iterates that are needed to obtain a convergence rate.
Lemma 1. The iterates generated by PDHG (2-5) satisfy

kuk  u?k2

Mk  kuk+1  ukk2

Mk + kuk+1  u?k2

Mk .

The proof of this lemma follows standard techniques  and is presented in the supplementary material.
This next lemma bounds iterates generated by PDHG.
Lemma 2. Suppose the stepsizes for PDHG satisfy Assumptions A  B and C. Then

for some upper bound CU > 0.

kuk  u?k2

Hk  CU

The proof of this lemma is given in the supplementary material.
Lemma 3. Under Assumptions A  B  and C  we have

nXk=1⇣kuk  uk2

Mk  kuk  uk2

Mk1⌘  2CCU + 2CCHku  u?k2

where C =P1k=0 k and CH is a constant such that ku  u?k2

Hk  CHku  u?k2.

5

Proof. Using the deﬁnition of Mk we obtain

1
k 

1
k1

1

k kyk  yk2◆

)kyk  yk2

(20)

=



Mk  kuk  uk2
1
⌧k 

1
⌧k1
⌧k kxk  xk2 +

Mk1⌘
)kxk  xk2 + (

nXk=1⇣kuk  uk2
nXk=1(
k1✓ 1
nXk=1
nXk=1
k1kuk  uk2
nXk=1
k1kuk  u?k2
nXk=1
k1CU + CHku  u?k2
 2
 2CCU + 2CCHku  u?k2 

 2

Hk

=

Hk
Hk + ku  u?k2

where we have used the bound kuk  u?k2

Hk  CU from Lemma 2 and C =P1k=0 k.

This ﬁnal lemma provides a VI interpretation of the PDHG iteration.
Lemma 4. The iterates uk = (xk  yk) generated by PDHG satisfy

h(u)  h(uk+1) + (u  uk+1)T [Quk+1 + Mk(uk+1  uk)]  0

8u 2 ⌦.

(21)

Proof. Let uk = (xk  yk) be a pair of PDHG iterates. The minimizers in (3) and (5) of PDHG
satisfy the following for all x 2 X

f (x)  f (xk+1) + (x  xk+1)T [AT yk+1  AT (yk+1  yk) +

1
⌧k

(xk+1  xk)]  0 

and also for all y 2 Y

g(y)  g(yk+1) + (y  yk+1)T [Axk+1  A(xk+1  xk) +

1
k

(yk+1  yk)]  0.

(22)

(23)

Adding these two inequalities and using the notation (15) yields the result.

5.3 Convergence Rate

We now combine the above lemmas into our ﬁnal convergence result.
Theorem 1. Suppose that the stepsizes in PDHG satisfy Assumptions A  B  and C. Consider the
sequence deﬁned by

This sequence satisﬁes the convergence bound

˜ut =

uk.

1
t

tXk=1

h(u)  h(˜ut) + (u  ˜ut)T Q(˜ut)  ku  ˜utk2

Mt  ku  u0k2

M0  2CCU  2CCHku  u?k2

.

2t

Thus ˜ut converges to a solution of (1) with rate O(1/k) in the VI sense (19).

6

Because h is convex 

 ku  utk2
 ku  utk2
t1Xk=0

h(uk+1) =

M0 +

Mt  ku  u0k2
Mt  ku  u0k2

tXk=1hku  ukk2

Mki
Mk1  ku  ukk2

M0  2CCU  2CCHku  u?k2.

h(uk)  th 1

t

uk! = th(˜ut).

tXk=1

tXk=1

(26)

(27)

(28)

Proof. We begin with the following identity (a special case of the polar identity for vector spaces):

(u  uk+1)T Mk(uk  uk+1) =

Mk  ku  ukk2
We apply this to the VI formulation of the PDHG iteration (18) to get

(ku  uk+1k2

Mk ) +

1
2

1
2kuk  uk+1k2

Mk .

h(u)  h(uk+1) + (u  uk+1)T Q(uk+1)

Note that



1

2ku  uk+1k2

Mk  ku  ukk2

1
2kuk  uk+1k2

Mk .

(24)

Mk +

(u  uk+1)T Q(u  uk+1) = (x  xk+1)AT (y  yk+1)  (y  yk+1)A(x  xk+1) = 0 

(25)
and so (u  uk+1)T Q(u) = (u  uk+1)T Q(uk+1). Also  Assumption C guarantees that kuk 
uk+1k2

Mk  0. These observations reduce (24) to
h(u)  h(uk+1) + (u  uk+1)T Q(u) 

1

2ku  uk+1k2

Mk .
Mk  ku  ukk2

We now sum (26) for k = 0 to t  1  and invoke Lemma 3 

2

t1Xk=0

[h(u)  h(uk+1) + (u  uk+1)T Q(u)]

The left side of (27) therefore satisﬁes

2th(u)  h(˜ut) + (u  ˜ut)T Q(u)  2
h(u)  h(˜ut) + (u  ˜ut)T Q(u)  ku  utk2

Combining (27) and (28) yields the tasty bound

t1Xk=0⇥h(u)  h(uk+1) + (u  uk+1)T Q(u)⇤ .

Mt  ku  u0k2

M0  2CCU  2CCHku  u?k2

.

2t

Applying (19) proves the theorem.

6 Numerical Results

We apply the original and adaptive PDHG to the test problems described in Section 3. We terminate
the algorithms when both the primal and dual residual norms (i.e. kpkk and kdkk) are smaller
than 0.05. We consider four variants of PDHG. The method “Adapt:Backtrack” denotes adaptive
PDHG with backtracking. The method “Adapt: ⌧  = L” refers to the adaptive method without
backtracking with ⌧0 = 0 = 0.95⇢(AT A) 1
2 .
We also consider the non-adaptive PDHG with two different stepsize choices. The method “Const:
⌧   = pL” refers to the constant-stepsize method with both stepsize parameters equal to pL =
⇢(AT A) 1
2 . The method “Const: ⌧-ﬁnal” refers to the constant-stepsize method  where the stepsizes
are chosen to be the ﬁnal values of the stepsizes used by “Adapt: ⌧  = L.” This ﬁnal method is
meant to demonstrate the performance of PDHG with a stepsize that is customized to the problem
at hand  but still non-adaptive. The speciﬁcs of each test problem are described below:

7

ROF Convergence Curves  µ = 0.05

 

A d a p t: Ba cktra ck
A d a p t: τ σ = L
Co n st: τ = √L
Co n st: τ -ﬁ n a l

Primal Stepsize ( τ k)

 

A d ap t: Backtrack
A d ap t: τ σ = L

p
a
G

y
g
r
e
n
E

107

106

105

104

103

102

101

100

12

10

8

6

4

2

0

k
τ

 
0

50

100

150

200

250

300

Iteration

 
0

50

100

150

200

250

300

Iteration

Figure 1: (left) Convergence curves for the TV denoising experiment with µ = 0.05. The y-axis
displays the difference between the objective (10) at the kth iterate and the optimal objective value.
(right) Stepsize sequences  {⌧k}  for both adaptive schemes.

Table 1: Iteration counts for each problem with runtime (sec) in parenthesis.
Problem

Scaled Lasso (50%)
Scaled Lasso (20%)
Scaled Lasso (10%)

TV  µ = .25
TV  µ = .05
TV  µ = .01

Compressive (20%)
Compressive (10%)
Compressive (5%)

Adapt:Backtrack Adapt: ⌧  = L Const: ⌧   = pL Const: ⌧-ﬁnal
156 (0.27)
197 (0.11)
277 (0.15)
48 (0.121)
97 (0.228)
152 (0.369)
246 (6.03)
437 (9.94)
435 (9.95)

342 (0.60)
437 (0.25)
527 (0.28)
78 (0.184)
281 (0.669)
927 (2.17)
501 (12.54)
908 (20.6)
1505 (34.2)

240 (0.38)
330 (0.21)
322 (0.18)
16 (0.041)
51 (0.122)
122 (0.288)
168 (4.12)
274 (6.21)
438 (10.7)

212 (0.33)
349 (0.22)
360 (0.21)
16 (0.0475)
50 (0.122)
109 (0.262)
163 (4.08)
244 (5.63)
382 (9.54)

Scaled Lasso We test our methods on (8) using the synthetic problem suggested in [21]. The test
problem recovers a 1000 dimensional vector with 10 nonzero components using a Gaussian matrix.
Total Variation Minimization We apply the model (10) with A = I to the “Cameraman” image.
The image is scaled to the range [0  255]  and noise contaminated with standard deviation 10. The
image is denoised with µ = 0.25  0.05  and 0.01. See Table 1 for time trial results. Note the similar
performance of Algorithm 1 with and without backtracking  indicating that there is no advantage to
knowing the constant L = ⇢(AT A)1. We plot convergence curves and show the evolution of ⌧k in
Figure 1. Note that ⌧k is large for the ﬁrst several iterates and then decays over time.
Compressed Sensing We reconstruct a Shepp-Logan phantom from sub-sampled Hadamard mea-
surements. Data is generated by applying the Hadamard transform to a 256 ⇥ 256 discretization of
the Shepp-Logan phantom  and then sampling 5%  10%  and 20% of the coefﬁcients are random.
7 Discussion and Conclusion
Several interesting observations can be made from the results in Table 1. First  both the backtracking
(“Adapt: Backtrack”) and non-backtracking (“Adapt: ⌧  = L”) methods have similar performance
on average for the imaging problems  with neither algorithm showing consistently better perfor-
mance. Thus there is no cost to using backtracking instead of knowing the ideal stepsize ⇢(AT A).
Finally  the method “Const: ⌧-ﬁnal” (using non-adaptive  “optimized” stepsizes) did not always out-
perform the constant  non-optimized stepsizes. This occurs because the true “best” stepsize choice
depends on the active set of the problem and the structure of the remaining error and thus evolves
over time. This is depicted in Figure 1  which shows the time dependence of ⌧k. This show that
adaptive methods can achieve superior performance by evolving the stepsize over time.
8 Acknowledgments
This work was supported by the National Science Foundation ( #1535902)  the Ofﬁce of Naval
Research (#N00014-15-1-2676)  and the Hong Kong Research Grants Council’s General Research
Fund (HKBU 12300515). The second author was supported in part by the Program for New Century
Excellent University Talents under Grant No. NCET-12-0111  and the Qing Lan Project.

8

References
[1] R. Glowinski and A. Marroco. Sur l’approximation  par ´el´ements ﬁnis d’ordre un  et la r´esolution  par
p´enalisation-dualit´e d’une classe de probl`emes de Dirichlet non lin´eaires. Rev. Franc¸aise d’Automat. Inf.
Recherche Op´erationelle  9(2):41–76  1975.

[2] Roland Glowinski and Patrick Le Tallec. Augmented Lagrangian and Operator-Splitting Methods in

Nonlinear Mechanics. Society for Industrial and Applied Mathematics  Philadephia  PA  1989.

[3] Tom Goldstein and Stanley Osher. The Split Bregman method for `1 regularized problems. SIAM J. Img.

Sci.  2(2):323–343  April 2009.

[4] Ernie Esser  Xiaoqun Zhang  and Tony F. Chan. A general framework for a class of ﬁrst order primal-dual
algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences  3(4):1015–
1046  2010.

[5] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems with

applications to imaging. Convergence  40(1):1–49  2010.

[6] Yuyuan Ouyang  Yunmei Chen  Guanghui Lan  and Eduardo Pasiliao Jr. An accelerated linearized alter-

nating direction method of multipliers. arXiv preprint arXiv:1401.6607  2014.

[7] B. He  H. Yang  and S.L. Wang. Alternating direction method with self-adaptive penalty parameters for
monotone variational inequalities. Journal of Optimization Theory and Applications  106(2):337–356 
2000.

[8] L.D. Popov. A modiﬁcation of the arrow-hurwicz method for search of saddle points. Mathematical notes

of the Academy of Sciences of the USSR  28:845–848  1980.

[9] Mingqiang Zhu and Tony Chan. An efﬁcient primal-dual hybrid gradient algorithm for total variation

image restoration. UCLA CAM technical report  08-34  2008.

[10] T. Pock  D. Cremers  H. Bischof  and A. Chambolle. An algorithm for minimizing the mumford-shah

functional. In Computer Vision  2009 IEEE 12th International Conference on  pages 1133–1140  2009.

[11] Bingsheng He and Xiaoming Yuan. Convergence analysis of primal-dual algorithms for a saddle-point

problem: From contraction perspective. SIAM J. Img. Sci.  5(1):119–149  January 2012.

[12] Laurent Condat. A primal-dual splitting method for convex optimization involving lipschitzian  prox-
imable and linear composite terms. Journal of Optimization Theory and Applications  158(2):460–479 
2013.

[13] Silvia Bonettini and Valeria Ruggiero. On the convergence of primal–dual hybrid gradient algorithms for

total variation image restoration. Journal of Mathematical Imaging and Vision  44(3):236–253  2012.

[14] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed Optimization and Statistical Learning
via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning  2010.
[15] A. Belloni  Victor Chernozhukov  and L. Wang. Square-root lasso: pivotal recovery of sparse signals via

conic programming. Biometrika  98(4):791–806  2011.

[16] Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika  99(4):879–898  2012.
[17] L Rudin  S Osher  and E Fatemi. Nonlinear total variation based noise removal algorithms. Physica. D. 

60:259–268  1992.

[18] Tom Goldstein  Lina Xu  Kevin Kelly  and Richard Baraniuk. The STONE transform: Multi-resolution
image enhancement and real-time compressive video. Preprint available at Arxiv.org (arXiv:1311.34056) 
2013.

[19] M. Lustig  D. Donoho  and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR

imaging. Magnetic Resonance in Medicine  58:1182–1195  2007.

[20] Xiaoqun Zhang and J. Froment. Total variation based fourier reconstruction and regularization for com-
In Nuclear Science Symposium Conference Record  2005 IEEE  volume 4  pages

puter tomography.
2332–2336  Oct 2005.

[21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society  Series B  58:267–288  1994.

9

,Sivaraman Balakrishnan
Srivatsan Narayanan
Alessandro Rinaldo
Aarti Singh
Larry Wasserman
Tom Goldstein
Min Li
Xiaoming Yuan