2008,Domain Adaptation with Multiple Sources,This paper presents a theoretical analysis of the problem of adaptation with multiple sources. For each source domain  the distribution over the input points as well as a hypothesis with error at most \epsilon are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular  we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that  instead  combinations weighted by the source distributions benefit from favorable theoretical guarantees. Our main result shows that  remarkably  for any fixed target function  there exists a distribution weighted combining rule that has a loss of at most \epsilon with respect to *any* target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3\epsilon. Finally  we report empirical results for a multiple source adaptation problem with a real-world dataset.,Domain Adaptation with Multiple Sources

Yishay Mansour

Google Research and

Tel Aviv Univ.

Mehryar Mohri

Courant Institute and

Google Research

Afshin Rostamizadeh

Courant Institute

New York University

mansour@tau.ac.il

mohri@cims.nyu.edu

rostami@cs.nyu.edu

Abstract

This paper presents a theoretical analysis of the problem of domain adaptation
with multiple sources. For each source domain  the distribution over the input
points as well as a hypothesis with error at most ǫ are given. The problem con-
sists of combining these hypotheses to derive a hypothesis with small error with
respect to the target domain. We present several theoretical results relating to
this problem. In particular  we prove that standard convex combinations of the
source hypotheses may in fact perform very poorly and that  instead  combinations
weighted by the source distributions beneﬁt from favorable theoretical guarantees.
Our main result shows that  remarkably  for any ﬁxed target function  there exists
a distribution weighted combining rule that has a loss of at most ǫ with respect to
any target mixture of the source distributions. We further generalize the setting
from a single target function to multiple consistent target functions and show the
existence of a combining rule with error at most 3ǫ. Finally  we report empirical
results for a multiple source adaptation problem with a real-world dataset.

1 Introduction

A common assumption in theoretical models of learning such as the standard PAC model [16]  as
well as in the design of learning algorithms  is that training instances are drawn according to the
same distribution as the unseen test examples. In practice  however  there are many cases where this
assumption does not hold. There can be no hope for generalization  of course  when the training and
test distributions vastly differ  but when they are less dissimilar  learning can be more successful.

A typical situation is that of domain adaptation where little or no labeled data is at one’s disposal
for the target domain  but large amounts of labeled data from a source domain somewhat similar to
the target  or hypotheses derived from that source  are available instead. This problem arises in a
variety of applications in natural language processing [4  7  10]  speech processing [8  9  11  13–15] 
computer vision [12]  and many other areas.

This paper studies the problem of domain adaptation with multiple sources  which has also received
considerable attention in many areas such as natural language processing and speech processing.
An example is the problem of sentiment analysis which consists of classifying a text sample such
as a movie review  restaurant rating  or discussion boards  or other web pages. Information about a
relatively small number of domains such as movies or books may be available  but little or none can
be found for more difﬁcult domains such as travel.

We will consider the following problem of multiple source adaptation. For each source i ∈ [1  k] 
the learner receives the distribution Di of the input points corresponding to that source as well
as a hypothesis hi with loss at most ǫ on that source. The learner’s task consists of combining
the k hypotheses hi  i ∈ [1  k]  to derive a hypothesis h with small loss with respect to the target
distribution. The target distribution is assumed to be a mixture of the distributions Di. We will
discuss both the case where the mixture is known to the learner and the case where it is unknown.

1

Note that the distribution Di is deﬁned over the input points and bears no information about the
labels. In practice  Di is estimated from large amounts of unlabeled points typically available from
source i.
An alternative set-up for domain adaptation with multiple sources is one where the learner is not
supplied with a good hypothesis hi for each source but where instead he has access to the labeled
training data for each source domain. A natural solution consists then of combining the raw labeled
data from each source domain to form a new sample more representative of the target distribution
and use that to train a learning algorithm. This set-up and the type of solutions just described
have been in fact explored extensively in applications [8  9  11  13–15]. However  several empirical
observations motivated our study of hypothesis combination  in addition to the theoretical simplicity
and clarity of this framework.

First  in some applications such as very large-vocabulary speech recognition  often the original raw
data used to derive each domain-dependent model is no more available [2  9]. This is because such
models are typically obtained as a result of training based on many hours of speech with ﬁles oc-
cupying hundreds of gigabytes of disk space  while the models derived require orders of magnitude
less space. Thus  combining raw labeled data sets is not possible in such cases. Secondly  a com-
bined data set can be substantially larger than each domain-speciﬁc data set  which can signiﬁcantly
increase the computational cost of training and make it prohibitive for some algorithms. Thirdly 
combining labeled data sets requires the mixture parameters of the target distribution to be known 
but it is not clear how to produce a hypothesis with a low error rate with respect to any mixture
distribution.

Few theoretical studies have been devoted to the problem of adaptation with multiple sources. Ben-
David et al. [1] gave bounds for single source adaptation  then Blitzer et al. [3] extended the work
to give a bound on the error rate of a hypothesis derived from a weighted combination of the source
data sets for the speciﬁc case of empirical risk minimization. Crammer et al. [5  6] also addressed
a problem where multiple sources are present but the nature of the problem differs from adaptation
since the distribution of the input points is the same for all these sources  only the labels change
due to varying amounts of noise. We are not aware of a prior theoretical study of the problem of
adaptation with multiple sources analyzed here.

We present several theoretical results relating to this problem. We examine two types of hypothesis
combination. The ﬁrst type is simply based on convex combinations of the k hypotheses hi. We
show that this natural and widely used hypothesis combination may in fact perform very poorly in
our setting. Namely  we give a simple example of two distributions and two matching hypotheses 
each with zero error for their respective distribution  but such that any convex combination has
expected absolute loss of 1/2 for the equal mixture of the distributions. This points out a potentially
signiﬁcant weakness of a convex combination.

The second type of hypothesis combination  which is the main one we will study in this work 
takes into account the probabilities derived from the distributions. Namely  the weight of hypothesis
hi on an input x is proportional to λiDi(x)  were λ is the set of mixture weights. We will refer
to this method as the distribution weighted hypothesis combination. Our main result shows that 
remarkably  for any ﬁxed target function  there exists a distribution weighted combining rule that
has a loss of at most ǫ with respect to any mixture of the k distributions. We also show that there
exists a distribution weighted combining rule that has loss at most 3ǫ with respect to any consistent
target function (one for which each hi has loss ǫ on Di) and any mixture of the k distributions. In
some sense  our results establish that the distribution weighted hypothesis combination is the “right”
combination rule  and that it also beneﬁts from a well-founded theoretical guarantee.

The remainder of this paper is organized as follows. Section 2 introduces our theoretical model for
multiple source adaptation. In Section 3  we analyze the abstract case where the mixture parameters
of the target distribution are known and show that the distribution weighted hypothesis combination
that uses as weights these mixture coefﬁcients achieves a loss of at most ǫ. In Section 4  we give
a simple method to produce an error of Θ(kǫ) that does not require the prior knowledge of the
mixture parameters of the target distribution. Our main results showing the existence of a combined
hypothesis performing well regardless of the target mixture are given in Section 5 for the case of a
ﬁxed target function  and in Section 6 for the case of multiple target functions. Section 7 reports
empirical results for a multiple source adaptation problem with a real-world dataset.

2

2 Problem Set-Up

Let X be the input space  f : X → R the target function to learn  and L : R × R → R a loss function
penalizing errors with respect to f . The loss of a hypothesis h with respect to a distribution D and
loss function L is denoted by L(D  h  f ) and deﬁned as L(D  h  f ) = Ex∼D[L(h(x)  f (x))] =
Px∈X L(h(x)  f (x))D(x). We will denote by ∆ the simplex ∆ = {λ : λi ≥ 0 ∧ Pk
i=1 λi = 1} of
Rk.

We consider an adaptation problem with k source domains and a single target domain. The input
to the problem is the set of k source distributions D1  . . .   Dk and k corresponding hypotheses
h1  . . .   hk such that for all i ∈ [1  k]  L(Di  hi  f ) ≤ ǫ  for a ﬁxed ǫ ≥ 0. The distribution
of the target domain  DT   is assumed to be a mixture of the k source distributions Dis  that is
DT (x) = Pk
i=1 λiDi(x)  for some unknown mixture weight vector λ ∈ ∆. The adaptation problem
consists of combing the hypotheses his to derive a hypothesis with small loss on the target domain.
Since the target distribution DT is assumed to be a mixture  we will refer to this problem as the
mixture adaptation problem.
A combining rule for the hypotheses takes as an input the his and outputs a single hypothe-
sis h : X → R. We deﬁne two combining rules of particular interest for our purpose: the lin-
ear combining rule which is based on a parameter z ∈ ∆ and which sets the hypothesis to
h(x) = Pk
i=1 zihi(x); and the distribution weighted combining rule also based on a parameter
z ∈ ∆ which sets the hypothesis to h(x) = Pk
j=1 zjDj(x) > 0.
This last condition always holds if Di(x) > 0 for all x ∈ X and some i ∈ [1  k]. We deﬁne H to
be the set of all distribution weighted combining rules. Given the input to the adaptation problem
we have implicit information about the target function f . We deﬁne the set of consistent target
functions  F  as follows 

hi(x) when Pk

i=1

ziDi(x)
j=1 zj Dj (x)

Pk

F = {g : ∀i ∈ [1  k]  L(Di  hi  g) ≤ ǫ} .

By deﬁnition  the target function f is an element of F.
We will assume that the following properties hold for the loss function L: (i) L is non-negative:
L(x  y) ≥ 0 for all x  y ∈ R; (ii) L is convex with respect to the ﬁrst argument: L(Pk
i=1 λixi  y) ≤
Pk
i=1 λiL(xi  y) for all x1  . . .   xk  y ∈ R and λ ∈ ∆; (iii) L is bounded: there exists M ≥ 0
such that L(x  y) ≤ M for all x  y ∈ R; (iv) L(x  y) is continuous in both x and y; and (v) L is
symmetric L(x  y) = L(y  x). The absolute loss deﬁned by L(x  y) = |x − y| will serve as our
primary motivating example.

3 Known Target Mixture Distribution

In this section we assume that the parameters of the target mixture distribution are known. Thus  the
learning algorithm is given λ ∈ ∆ such that DT (x) =Pk
i=1 λiDi(x). A good starting point would be
to study the performance of a linear combining rule. Namely the classiﬁer h(x) = Pk
i=1 λihi(x).
While this seems like a very natural classiﬁer  the following example highlights the problematic
aspects of this approach.
Consider a discrete domain X = {a  b} and two distributions  Da and Db  such that Da(a) = 1
and Db(b) = 1. Namely  each distribution puts all the weight on a single element in X . Consider
the target function f   where f (a) = 1 and f (b) = 0  and let the loss be the absolute loss. Let
h0 = 0 be the function that outputs 0 for all x ∈ X and similarly h1 = 1. The hypotheses h1
and h0 have zero expected absolute loss on the distributions Da and Db  respectively  i.e.  ǫ = 0.
Now consider the target distribution DT with λa = λb = 1/2  thus DT (a) = DT (b) = 1/2. The
hypothesis h(x) = (1/2)h1(x) + (1/2)h0(x) always outputs 1/2  and has an absolute loss of 1/2.
Furthermore  for any other parameter z of the linear combining rule  the expected absolute loss of
h(x) = zh1(x)+ (1 − z)h0(x) with respect to DT is exactly 1/2. We have established the following
theorem.
Theorem 1. There is a mixture adaptation problem with ǫ = 0 for which any linear combination
rule has expected absolute loss of 1/2.

3

Next we show that the distribution weighted combining rule produces a hypothesis with a low ex-
pected loss. Given a mixture DT (x) = Pk
i=1 λiDi(x)  we consider the distribution weighted com-
bining rule with parameter λ  which we denote by hλ. Recall that 

hλ(x) =

k

X

i=1

λiDi(x)
j=1 λjDj(x)

Pk

hi(x) =

k

X

i=1

λiDi(x)
DT (x)

hi(x) .

Using the convexity of L with respect to the ﬁrst argument  the loss of hλ with respect to DT and a
target f ∈ F can be bounded as follows 

L(DT   hλ  f ) = X

L(hλ(x)  f (x))DT (x) ≤ X

X

λiDi(x)L(hi(x)  f (x)) =

x∈X

x∈X

i=1

k

k

X

i=1

λiǫi ≤ ǫ 

where ǫi := L(Di  hi  f ) ≤ ǫ. Thus  we have derived the following theorem.
Theorem 2. For any mixture adaptation problem with target distribution Dλ(x) = Pk
i=1 λiDi(x) 
the expected loss of the hypothesis hλ is at most ǫ with respect to any target function f ∈ F:
L(Dλ  hλ  f ) ≤ ǫ.

4 Simple Adaptation Algorithms

In this section we show how to construct a simple distribution weighted hypothesis that has an
expected loss guarantee with respect to any mixture. Our hypothesis hu is simply based on equal
weights  i.e.  ui = 1/k  for all i ∈ [1  k]. Thus 

hu(x) =

k

X

i=1

(1/k)Di(x)
j=1(1/k)Dj(x)

Pk

hi(x) =

k

X

i=1

Di(x)
j=1 Dj(x)

Pk

hi(x).

We show for hu an expected loss bound of kǫ  with respect to any mixture distribution DT and target
function f ∈ F. (Proof omitted.)
Theorem 3. For any mixture adaptation problem the expected loss of hu is at most kǫ  for any
mixture distribution DT and target function f ∈ F  i.e.  L(DT   hu  f ) ≤ kǫ.

Unfortunately  the hypothesis hu can have an expected absolute loss as large as Ω(kǫ).
omitted.)
Theorem 4. There is a mixture adaptation problem for which the expected absolute loss of hu is
Ω(kǫ). Also  for k = 2 there is an input to the mixture adaptation problem for which the expected
absolute loss of hu is 2ǫ − ǫ2.

(Proof

5 Existence of a Good Hypothesis

In this section  we will show that for any target function f ∈ F there is a distribution weighted
combining rule hz that has a loss of at most ǫ with respect to any mixture DT . We will construct
the proof in two parts. In the ﬁrst part  we will show  using a simple reduction to a zero-sum game 
that one can obtain a mixture of hzs that guarantees a loss bounded by ǫ. In the second part  which
is the more interesting scenario  we will show that for any target function f ∈ F there is a single
distribution weighted combining rule hz that has loss of at most ǫ with respect to any mixture DT .
This later part will require the use of Brouwer ﬁxed point theorem to show the existence of such an
hz.

5.1 Zero-sum game

The adaptation problem can be viewed as a zero-sum game between two players  NATURE and
LEARNER. Let the input to the mixture adaptation problem be D1  . . .   Dk  h1  . . .   hk and ǫ  and
ﬁx a target function f ∈ F. The player NATURE picks a distribution Di while the player LEARNER
selects a distribution weighted combining rule hz ∈ H. The loss when NATURE plays Di and
LEARNER plays hz is L(Di  hz  f ). Let us emphasize that the target function f ∈ F is ﬁxed
beforehand. The objective of NATURE is to maximize the loss and the objective of LEARNER is to
minimize the loss. We start with the following lemma 

4

Lemma 1. Given any mixed strategy of NATURE  i.e.  a distribution µ over Di’s  then the following
action of LEARNER hµ ∈ H has expected loss at most ǫ  i.e.  L(Dµ  hµ  f ) ≤ ǫ.

The proof is identical to that of Theorem 2. This almost establishes that the value of the game is at
most ǫ. The technical part that we need to take care of is the fact that the action space of LEARNER
is inﬁnite. However  by an appropriate discretization of H we can derive the following theorem.
Theorem 5. For any target function f ∈ F and any δ > 0  there exists a function h(x) =
Pm
j=1 αjhzj (x)  where hzi ∈ H  such that L(DT   h  f ) ≤ ǫ + δ for any mixture distribution
DT (x) = Pk
Since we can ﬁx δ > 0 to be arbitrarily small  this implies that a linear mixture of distribution
weighted combining rules can guarantee a loss of almost ǫ with respect to any product distribution.

i=1 λiDi(x).

5.2 Single distribution weighted combining rule

In the previous subsection  we showed that a mixture of hypotheses in H would guarantee a loss of
at most ǫ. Here  we will considerably strengthen the result and show that there is a single hypothesis
in H for which this guarantee holds. Unfortunately our loss is not convex with respect to h ∈ H  so
we need to resort to a more powerful technique  namely the Brouwer ﬁxed point theorem.
For the proof we will need that the distribution weighted combining rule hz be continuous in
In general  this does hold due to the existence of points x ∈ X for which
the parameter z.
Pk
z  as
follows.
Claim 1. Let U denote the uniform distribution over X   then for any η > 0 and z ∈ ∆  let
z : X → R be the function deﬁned by
hη

j=1 zjDj(x) = 0. To avoid this discontinuity  we will modify the deﬁnition of hz to hη

k

hη

z(x) =

X

Then  for any distribution D  L(D  hη

ziDi(x) + ηU (x)/k
j=1 zjDj(x) + ηU (x)

Pk

i=1
z   f ) is continuous in z.1

hi(x).

Let us ﬁrst state Brouwer’s ﬁxed point theorem.
Theorem 6 (Brouwer Fixed Point Theorem). For any compact and convex non-empty set A ⊂ Rn
and any continuous function f : A → A  there is a point x ∈ A such that f (x) = x.

z  f ) are all nearly the same.

We ﬁrst show that there exists a distribution weighted combining rule hη
L(Di  hη
Lemma 2. For any target function f ∈ F and any η  η′ > 0  there exists z ∈ ∆  with zi 6= 0 for all
i ∈ [1  k]  such that the following holds for the distribution weighted combining rule hη

z for which the losses

z ∈ H:

L(Di  hη

z  f ) = γ + η′ −

η′
zik

≤ γ + η′

for any 1 ≤ i ≤ k  where γ = Pk

j=1 zjL(Dj  hη

z  f ).

i = L(Di  hη

z  f ) for all z ∈ ∆ and i ∈ [1  m]. Consider the
Proof. Fix η′ > 0 and let Lz
j + η′) 
mapping φ : ∆ → ∆ deﬁned for all z ∈ ∆ by [φ(z)]i = (ziLz
where [φ(z)]i  is the ith coordinate of φ(x)  i ∈ [1  m]. By Claim 1  φ is continuous. Thus 
by Brouwer’s Fixed Point Theorem  there exists z ∈ ∆ such that φ(z) = z. This implies that
j + η′). Since η′ > 0  we must have zi 6= 0 for any i ∈ [1  m]. Thus 
zi = (ziLz
j=1 zjLz
i +η′/(zik) = (Pk
we can divide by zi and write Lz
i = γ +η′−η′/(zik)
with γ = Pk

j )+η′. Therefore  Lz

i + η′/k)/ (Pk

i + η′/k)/(Pk

j .
j=1 zjLz

j=1 zjLz

j=1 zjLz

1In addition to continuity  the perturbation to hz  hη

z   also helps us ensure that none of the mixture weights

zi is zero in the proof of the Lemma 2 .

5

= Xx∈X
≤ Xx∈X  k
Xi=1
Xi=1

=

k

ziDi(x)L(hi(x)  f (x))! + Xx∈X

ηM U (x)

ziL(Di  hi  f ) + ηM =

ziǫi + ηM ≤ ǫ + ηM .

k

Xi=1

Note that the lemma just presented does not use the structure of the distribution weighted combining
rule  but only the fact that the loss is continuous in the parameter z ∈ ∆. The lemma applies as well
to the linear combination rule and provides the same guarantee. The real crux of the argument is  as
shown in the next lemma  that γ is small for a distribution weighted combining rule (while it can be
very large for a linear combination rule).
Lemma 3. For any target function f ∈ F and any η  η′ > 0  there exists z ∈ ∆ such that
L(Dλ  hη

z  f ) ≤ ǫ + ηM + η′ for any λ ∈ ∆.

Proof. Let z be the parameter guaranteed in Lemma 2. Then L(Di  hη
z   f ) = γ + η′ − η′/(zik) ≤
γ + η′  for 1 ≤ i ≤ k. Consider the mixture Dz  i.e.  set the mixture parameter to be z. Consider the
quantity L(Dz  hη
z  f ) and
thus L(Dz  hη
L(Dz hη
z   f )

z  f ). On the one hand  by deﬁnition  L(Dz  hη

z  f ) = γ. On the other hand 

z  f ) = Pk

i=1 ziL(Di  hη

Dz(x)L(hη

z (x)  f (x)) ≤ Xx∈X

Dz(x)

Dz(x) + ηU (x)  k
Xi=1

(ziDi(x) +

ηU (x)

k

)L(hi(x)  f (x))!

Therefore γ ≤ ǫ + ηM . To complete the proof  note that the following inequality holds for any
mixture Dλ:

L(Dλ  hη

z  f ) =

k

X

i=1

λiL(Di  hη

z  f ) ≤ γ + η′ 

which is at most ǫ + ηM + η′.

By setting η = δ/(2M ) and η′ = δ/2  we can derive the following theorem.
Theorem 7. For any target function f ∈ F and any δ > 0  there exists η > 0 and z ∈ ∆  such that
L(Dλ  hη

z  f ) ≤ ǫ + δ for any mixture parameter λ.

6 Arbitrary target function

The results of the previous section show that for any ﬁxed target function there is a good distribution
weighted combining rule. In this section  we wish to extend these results to the case where the target
function is not ﬁxed in advanced. Thus  we seek a single distribution weighted combining rule that
can perform well for any f ∈ F and any mixture Dλ. Unfortunately  we are not able to prove a
bound of ǫ + o(ǫ) but only a bound of 3ǫ. To show this bound we will show that for any f1  f2 ∈ F
and any hypothesis h the difference of loss is bounded by at most 2ǫ.
Lemma 4. Assume that the loss function L obeys the triangle inequality  i.e.  L(f  h) ≤ L(f  g) +
L(g  h). Then for any f  f ′ ∈ F and any mixture DT   the inequality L(DT   h  f ′) ≤ L(DT   h  f ) +
2ǫ holds for any hypothesis h.

Proof. Since our loss function obeys the triangle inequality  for any functions f  g  h  the following
holds  L(D  f  h) ≤ L(D  f  g) + L(D  g  h). In our case  we observe that replacing g with any
f ′ ∈ F gives  L(Dλ  f  h) ≤ L(Dλ  f ′  h) + L(Dλ  f  f ′). We can bound the term L(Dλ  f  f ′)
with a similar inequality  L(Dλ  f  f ′) ≤ L(Dλ  f  hλ) + L(Dλ  f ′  hλ) ≤ 2ǫ  where hλ is the
distribution weighted combining rule produced by choosing z = λ and using Theorem 2. Therefore 
for any f  f ′ ∈ F we have  L(Dλ  f  h) ≤ L(Dλ  f ′  h) + 2ǫ  which completes the proof.

We derived the following corollary to Theorem 7.
Corollary 1. Assume that the loss function L obeys the triangle inequality. Then  for any δ > 0 
there exists η > 0 and z ∈ ∆  such that for any mixture parameter λ and any f ∈ F 
L(Dλ  hη

z  f ) ≤ 3ǫ + δ.

6

E
S
M

2.1

2

1.9

1.8

1.7

1.6

1.5

 

Uniform Mixture Over 4 Domains

 

In−Domain
Out−Domain

1

2

3

4

5

6

(a)

E
S
M

2.4

2.2

2

1.8

1.6

 

1.4
0

Mixture = α book + (1 − α) kitchen

Mixture = α dvd + (1 − α) electronics

 

 

weighted
linear
book
kitchen

0.2

0.4

α

0.6

0.8

1

(b)

E
S
M

2.4

2.2

2

1.8

1.6

 

1.4
0

weighted
linear
dvd
electronics

0.2

0.4

α

0.6

0.8

1

Figure 1: (a) MSE performance for a target mixture of four domains (1: books  2: dvd  3: electronics 
4: kitchen 5: linear  6: weighted). (b) MSE performance under various mixtures of two source
domains  plot left: book and kitchen  plot right: dvd and electronics.

7 Empirical results

This section reports the results of our experiments with a distribution weighted combining rule using
real-world data. In our experiments  we ﬁxed a mixture target distribution Dλ and considered the
distribution weighted combining rule hz  with z = λ. Since we used real-world data  we did not have
access to the domain distributions. Instead  we modeled each distribution and used large amounts
of unlabeled data available for each source to estimate the model’s parameters. One could have thus
expected potentially signiﬁcantly worse empirical results than the theoretical ones  but this turned
out not to be an issue in our experiments.
We used the sentiment analysis dataset found in [4].2 The data consists of review text and rat-
ing labels  taken from amazon.com product reviews within four different categories (domains).
These four domains consist of book  dvd  electronics and kitchen reviews  where each do-
main contains 2000 data points. 3 In our experiments  we ﬁxed a mixture target distribution Dλ and
considered the distribution weighted combining rule hz  with z = λ.
In our ﬁrst experiment  we considered mixtures of all four domains  where the test set was a uniform
mixture of 600 points  that is the union of 150 points taken uniformly at random from each domain.
The remaining 1 850 points from each domain were used to train the base hypotheses.4 We com-
pared our proposed weighted combining rule to the linear combining rule. The results are shown
in Figure 1(a). They show that the base hypotheses perform poorly on the mixture test set  which
justiﬁes the need for adaptation. Furthermore  the distribution weighted combining rule is shown to
perform at least as well as the worst in-domain performance of a base hypothesis  as expected from
our bounds. Finally  we observe that this real-world data experiment gives an example in which a
linear combining rule performs poorly compared to the distribution weighted combining rule.

In other experiments  we considered the mixture of two domains  where the mixture is varied ac-
cording to the parameter α ∈ {0.1  0.2  . . .   1.0}. For each plot in Figure 1 (b)  the test set consists
of 600α points from the ﬁrst domain and 600(1 − α) points from the second domain  where the
ﬁrst and second domains are made clear in the ﬁgure. The remaining points that were not used for
testing were used to train the base hypotheses. The results show the linear shift from one domain to
the other  as is evident from the performance of the two base hypotheses. The distribution weighted
combining rule outperforms the base hypotheses as well as the linear combining rule.

2http://www.seas.upenn.edu/˜mdredze/datasets/sentiment/.
3The rating label  an integer between 1 and 5  was used as a regression label  and the loss measured by the
mean squared error (MSE). All base hypotheses were generated using Support Vector Regression (SVR) [17]
with the trade-off parameters C = 8  ǫ = 0.1  and a Gaussian kernel with parameter g = 0.00078. The SVR
solutions were obtained using the libSVM software library ( http://www.csie.ntu.edu.tw/˜cjlin/libsvm/).
Our features were deﬁned as the set of unigrams appearing ﬁve times or more in all domains. This deﬁned
about 4000 unigrams. We used a binary feature vector encoding the presence or absence of these frequent
unigrams to deﬁne our instances. To model the domain distributions  we used a unigram statistical language
model trained on the same corpus as the one used to deﬁne the features. The language model was created using
the GRM library (http://www.research.att.com/˜fsmtools/grm/).

4Each experiment was repeated 20 times with random folds. The standard deviation found was far below

what could be legibly displayed in the ﬁgures.

7

Thus  our preliminary experiments suggest that the distribution weighted combining rule performs
well in practice and clearly outperforms a simple linear combining rule. Furthermore  using statis-
tical language models as approximations to the distribution oracles seem to be sufﬁcient in practice
and can help produce a good distribution weighted combining rule.

8 Conclusion

We presented a theoretical analysis of the problem of adaptation with multiple sources. Domain
adaptation is an important problem that arises in a variety of modern applications where limited or
no labeled data is available for a target application and our analysis can be relevant in a variety of
situations. The theoretical guarantees proven for the distribution weight combining rule provide it
with a strong foundation. Its empirical performance with a real-world data set further motivates
its use in applications. Much of the results presented were based on the assumption that the target
distribution is some mixture of the source distributions. A further analysis suggests however that
our main results can be extended to arbitrary target distributions.

Acknowledgments

We thank Jennifer Wortman for helpful comments on an earlier draft of this paper and Ryan McDonald for
discussions and pointers to data sets. The work of M. Mohri and A. Rostamizadeh was partly supported by the
New York State Ofﬁce of Science Technology and Academic Research (NYSTAR).

References
[1] Shai Ben-David  John Blitzer  Koby Crammer  and Fernando Pereira. Analysis of representations for

domain adaptation. In Proceedings of NIPS 2006. MIT Press  2007.

[2] Jacob Benesty  M. Mohan Sondhi  and Yiteng Huang  editors. Springer Handbook of Speech Processing.

Springer  2008.

[3] John Blitzer  Koby Crammer  A. Kulesza  Fernando Pereira  and Jennifer Wortman. Learning bounds for

domain adaptation. In Proceedings of NIPS 2007. MIT Press  2008.

[4] John Blitzer  Mark Dredze  and Fernando Pereira. Biographies  Bollywood  Boom-boxes and Blenders:

Domain Adaptation for Sentiment Classiﬁcation. In ACL 2007  Prague  Czech Republic  2007.

[5] Koby Crammer  Michael Kearns  and Jennifer Wortman. Learning from Data of Variable Quality. In

Proceedings of NIPS 2005  2006.

[6] Koby Crammer  Michael Kearns  and Jennifer Wortman. Learning from multiple sources. In Proceedings

of NIPS 2006  2007.

[7] Mark Dredze  John Blitzer  Pratha Pratim Talukdar  Kuzman Ganchev  Joao Graca  and Fernando Pereira.

Frustratingly Hard Domain Adaptation for Parsing. In CoNLL 2007  Prague  Czech Republic  2007.

[8] Jean-Luc Gauvain and Chin-Hui. Maximum a posteriori estimation for multivariate gaussian mixture
observations of markov chains. IEEE Transactions on Speech and Audio Processing  2(2):291–298  1994.

[9] Frederick Jelinek. Statistical Methods for Speech Recognition. The MIT Press  1998.
[10] Jing Jiang and ChengXiang Zhai. Instance Weighting for Domain Adaptation in NLP. In Proceedings of

ACL 2007  pages 264–271  Prague  Czech Republic  2007. Association for Computational Linguistics.

[11] C. J. Legetter and Phil C. Woodland. Maximum likelihood linear regression for speaker adaptation of

continuous density hidden markov models. Computer Speech and Language  pages 171–185  1995.

[12] Aleix M. Mart´ınez. Recognizing imprecisely localized  partially occluded  and expression variant faces

from a single sample per class. IEEE Trans. Pattern Anal. Mach. Intell.  24(6):748–763  2002.

[13] S. Della Pietra  V. Della Pietra  R. L. Mercer  and S. Roukos. Adaptive language modeling using minimum
discriminant estimation. In HLT ’91: Proceedings of the workshop on Speech and Natural Language 
pages 103–106  Morristown  NJ  USA  1992. Association for Computational Linguistics.

[14] Brian Roark and Michiel Bacchiani. Supervised and unsupervised PCFG adaptation to novel domains. In

Proceedings of HLT-NAACL  2003.

[15] Roni Rosenfeld. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer

Speech and Language  10:187–228  1996.

[16] Leslie G. Valiant. A theory of the learnable. ACM Press New York  NY  USA  1984.
[17] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience  New York  1998.

8

,Tianbao Yang
Rong Jin
Philip Thomas
Scott Niekum
Georgios Theocharous
George Konidaris
Malik Magdon-Ismail
Christos Boutsidis