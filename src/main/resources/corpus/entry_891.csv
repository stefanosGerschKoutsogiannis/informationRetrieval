2012,Calibrated Elastic Regularization in Matrix Completion,This paper concerns the problem of matrix completion  which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels  we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones.,Calibrated Elastic Regularization in Matrix

Completion

Tingni Sun

Cun-Hui Zhang

Statistics Department  The Wharton School

Department of Statistics and Biostatistics

University of Pennsylvania

Philadelphia  Pennsylvania 19104
tingni@wharton.upenn.edu

Rutgers University

Piscataway  New Jersey 08854

czhang@stat.rutgers.edu

Abstract

This paper concerns the problem of matrix completion  which is to estimate a
matrix from observations in a small subset of indices. We propose a calibrated
spectrum elastic net method with a sum of the nuclear and Frobenius penalties and
develop an iterative algorithm to solve the convex minimization problem. The iter-
ative algorithm alternates between imputing the missing entries in the incomplete
matrix by the current guess and estimating the matrix by a scaled soft-thresholding
singular value decomposition of the imputed matrix until the resulting matrix con-
verges. A calibration step follows to correct the bias caused by the Frobenius
penalty. Under proper coherence conditions and for suitable penalties levels  we
prove that the proposed estimator achieves an error bound of nearly optimal order
and in proportion to the noise level. This provides a uniﬁed analysis of the noisy
and noiseless matrix completion problems. Simulation results are presented to
compare our proposal with previous ones.

Introduction

1
Let Θ ∈ IRd1×d2 be a matrix of interest and Ω∗ = {1  . . .   d1} × {1  . . .   d2}. Suppose we observe
vectors (ωi  yi) 

i = 1  . . .   n 

yi = Θωi + εi 

(1)
where ωi ∈ Ω∗ and εi are random errors. We are interested in estimating Θ when n is a small
fraction of d1d2. A well-known application of matrix completion is the Netﬂix problem where yi
is the rating of movie bj by user ai for ω = (ai  bj) ∈ Ω∗ [1]. In such applications  the proportion
of the observed entries is typically very small  so that the estimation or recovery of Θ is impossible
without a structure assumption on Θ. In this paper  we assume that Θ is of low rank.
A focus of recent studies of matrix completion has been on a simpler formulation  also known
as exact recovery  where the observations are assumed to be uncorrupted  i.e. εi = 0. A direct
approach is to minimize rank(M ) subject to Mωi = yi. An iterative algorithm was proposed in [5]
to project a trimmed SVD of the incomplete data matrix to the space of matrices of a ﬁxed rank
r. The nuclear norm was proposed as a surrogate for the rank  leading to the following convex
minimization problem in a linear space [2]:

(cid:98)Θ(CR) = arg min

M

(cid:110)(cid:107)M(cid:107)(N ) : Mωi = yi ∀ i ≤ n
(cid:111)

.

We denote the nuclear norm by (cid:107) · (cid:107)(N ) here and throughout this paper. This procedure  analyzed
in [2  3  4  11] among others  is parallel to the replacement of the (cid:96)0 penalty by the (cid:96)1 penalty in
solving the sparse recovery problem in a linear space.

1

M 2
ωi

of noise  penalized squared error(cid:80)n
In this paper  we focus on the problem of matrix completion with noisy observations (1) and take the
exact recovery as a special case. Since the exact constraint is no longer appropriate in the presence
i=1(Mωi − yi)2 is considered. By reformulating the problem in
(cid:110) n(cid:88)
(cid:98)Θ(MHT) = arg min

Lagrange form  [8] proposed the spectrum Lasso

/2 − n(cid:88)

yiMωi + λ(cid:107)M(cid:107)(N )

(cid:111)

(2)

M

i=1

i=1

sample fraction π0 = n/(d1d2) is small  due to the ill-posedness of the quadratic term(cid:80)n
(cid:96)∞ constraint on M  [7] modiﬁed (2) by replacing the quadratic term(cid:80)n

along with an iterative convex minimization algorithm. However  (2) is difﬁcult to analyze when the
ωi.
i=1 M 2
This has led to two alternatives in [7] and [9]. While [9] proposed to minimize (2) under an additional
ωi with π0(cid:107)M(cid:107)2
(F ).
Both [7  9] provided nearly optimal error bounds when the noise level is of no smaller order than
the (cid:96)∞ norm of the target matrix Θ  but not of smaller order  especially not for exact recovery. In
a different approach  [6] proposed a non-convex recursive algorithm and provided error bounds in
proportion to the noise level. However  the procedure requires the knowledge of the rank r of the
unknown Θ and the error bound is optimal only when d1 and d2 are of the same order.
Our goal is to develop an algorithm for matrix completion that can be as easily computed as the
spectrum Lasso (2) and enjoys a nearly optimal error bound proportional to the noise level to con-
tinuously cover both the noisy and noiseless cases. We propose to use an elastic penalty  a linear
combination of the nuclear and Frobenius norms  which leads to the estimator

i=1 M 2

 

(cid:101)Θ = arg min

M

(cid:110) n(cid:88)

i=1

/2 − n(cid:88)

i=1

M 2
ωi

yiMωi + λ1(cid:107)M(cid:107)(N ) + (λ2/2)(cid:107)M(cid:107)2

(F )

(cid:111)

 

(3)

(4)

where (cid:107) · (cid:107)(N ) and (cid:107) · (cid:107)(F ) are the nuclear and Frobenius norms  respectively. We call (3) spectrum
elastic net (E-net) since it is parallel to the E-net in linear regression  the least squares estimator
with a sum of the (cid:96)1 and (cid:96)2 penalties  introduced in [15]. Here the nuclear penalty provides the
sparsity in the spectrum  while the Frobenius penalty regularizes the inversion of the quadratic term.
Meanwhile  since the Frobenius penalty roughly shrinks the estimator by a factor π0/(π0 + λ2)  we

correct this bias by a calibration step (cid:98)Θ = (1 + λ2/π0)(cid:101)Θ.

We call this estimator calibrated spectrum E-net.
Motivated by [8]  we develop an EM algorithm to solve (3) for matrix completion. The algorithm
iteratively replaces the missing entries with those obtained from a scaled soft-thresholding singular
value decomposition (SVD) until the resulting matrix converges. This EM algorithm is guaranteed
to converge to the solution of (3).
Under proper coherence conditions  we prove that for suitable penalty levels λ1 and λ2  the cali-
brated spectrum E-net (4) achieves a desired error bound in the Frobenius norm. Our error bound
is of nearly optimal order and in proportion to the noise level. This provides a sharper result than
those of [7  9] when the noise level is of smaller order than the (cid:96)∞ norm of Θ  and than that of [6]
when d2/d1 is large. Our simulation results support the use of the calibrated spectrum E-net. They
illustrate that (4) performs comparably to (2) and outperforms the modiﬁed method of [7].
Our analysis of the calibrated spectrum E-net uses an inequality similar to a duel certiﬁcate bound
in [3]. The bound in [3] requires sample size n (cid:16) min{(r log d)2  r(log d)6}d log d  where d =
d1 + d2. We use the method of moments to remove a log d factor in the ﬁrst component of their
sample size requirement. This leads to a sample size requirement of n (cid:16) r2d log d  with an extra r
in comparison to the ideal n (cid:16) rd log d. Since the extra r does not appear in our error bound  its
appearance in the sample size requirement seems to be a technicality.
The rest of the paper is organized as follows. In Section 2  we describe an iterative algorithm for the
computation of the spectrum E-net and study its convergence. In Section 3  we derive error bounds
for the calibrated spectrum E-net. Some simulation results are presented in Section 4. Section 5
provides the proof of our main result.
We use the following notation throughout this paper. For matrices M ∈ Rd1×d2  (cid:107)M(cid:107)(N ) is the
nuclear norm (the sum of all singular values of M)  (cid:107)M(cid:107)(S) is the spectrum norm (the largest

2

singular value)  (cid:107)M(cid:107)(F ) is the Frobenius norm (the (cid:96)2 norm of vectorized M)  and (cid:107)M(cid:107)∞ =
maxjk |Mjk|. Linear mappings from Rd1×d2 to Rd1×d2 are denoted by the calligraphic letters. For
a linear mapping Q  the operator norm is (cid:107)Q(cid:107)(op) = sup(cid:107)M(cid:107)(F )=1 (cid:107)QM(cid:107)(F ). We equip Rd1×d2
with the inner product (cid:104)M1  M2(cid:105) = trace(M(cid:62)
(F ). For projections
P  P⊥ = I − P with I being the identity. We denote by Eω the unit matrix with 1 at ω ∈
{1  . . .   d1} × {1  . . .   d2}  and by Pω the projection to Eω: M → MωEω = (cid:104)Eω  M(cid:105)Eω.

1 M2) so that (cid:104)M  M(cid:105) = (cid:107)M(cid:107)2

2 An algorithm for spectrum elastic regularization

We ﬁrst present a lemma for the M-step of our iterative algorithm.

Lemma 1 Suppose the matrix Z has rank r. The solution to the optimization problem

(cid:110)(cid:107)Z − W(cid:107)2

arg min

Z

(F )/2 + λ1(cid:107)Z(cid:107)(N ) + λ2(cid:107)Z(cid:107)2

(F )/2

(cid:111)

is given by S(W ; λ1  λ2) = U Dλ1 λ2V (cid:48) with Dλ1 λ2 = diag{(d1−λ1)+  . . .   (dr−λ1)+}/(1+λ2) 
where U DV (cid:48) is the SVD of W   D = diag{d1  . . .   dr} and t+ = max(t  0).
The minimization problem in Lemma 1 is solved by a scaled soft-thresholding SVD. This is parallel
to Lemma 1 in [8] and justiﬁed by Remark 1 there. We use Lemma 1 to solve the M-step of the EM
algorithm for the spectrum E-net (3).
We still need an E-step to impute a complete matrix given the observed data {yi  ωi : i = 1  . . .   n}.
Since ωi are allowed to have ties  we need the following notation. Let mω = #{i : ωi = ω  i ≤ n}
be the multiplicity of observations at ω ∈ Ω∗ and m∗ = maxω mω be the maximum multiplicity.
Suppose that the complete data is composed of m∗ observations at each ω for a certain integer m∗.
(com) be the matrix with components
(com)
Let Y
be the sample mean of the complete data at ω and Y
ω
(com)
. If the complete data are available  (3) is equivalent to
Y
ω

(m∗/2)(cid:107)Y

(com) − M(cid:107)2

(F ) + λ1(cid:107)M(cid:107)(N ) + (λ2/2)(cid:107)M(cid:107)2

(F )

(cid:110)

arg min

M

= m−1

(cid:80)

(cid:111)

.

(obs)
Let Y
ω
(obs)
)d1×d2.
ω

(Y
(mω/m∗)Y

ω

ωi=ω yi be the sample mean of the observations at ω and Y
given Y

In the white noise model  the conditional expectation of Y

=
(obs) is

(com)
ω

Y

(obs)

(imp)

= (Y

(imp)
ω

ω + (1 − mω/m∗)Θω for mω ≤ m∗. This leads to a generalized E-step:
ω + (1 − mω/m∗)+Z (old)
We now present the EM-algorithm for the computation of the spectrum E-net(cid:101)Θ in (3).

(5)
where Z (old) is the estimation of Θ in the previous iteration. This is a genuine E-step when m∗ = m∗
but also allows a smaller m∗ to reduce the proportion of missing data.

ω = min{1  (mω/m∗)}Y

)d1×d2  Y

(imp)

(obs)

 

ω

(obs)

Algorithm 1 Initialize with Z (0) and k = 0. Repeat the following steps:

(imp) in (5) with Z (old) = Z (k) and assign k ← k + 1 

• E-step: Compute Y
• M-step: Compute Z (k) = S(Y

until (cid:107)Z (k) − Z (k−1)(cid:107)2

(F )/(cid:107)Z (k)(cid:107)2

(imp)

; λ1/m∗  λ2/m∗) 
(F ) ≤ . Then  return Z (k).

The following theorem states the convergence of Algorithm 1.
Theorem 1 As k → ∞  Z (k) converges to a limit Z (∞) as a function of the data and (λ1  λ2  m∗) 

and Z (∞) = (cid:101)Θ for m∗ ≥ m∗.

3

Theorem 1 is a variation of a parallel result in [8] and follows from the same proof there. As [8]
pointed out  a main advantage of Algorithm 1 is the speed of each iteration. When the maximum
multiplicity m∗ is small  we simply use Z (0) = Y
(obs) and m∗ = m∗; Otherwise  we may ﬁrst run
the EM-algorithm for an m∗ < m∗ and use the output as the initialization Z (0) for a second run of
the EM-algorithm with m∗ = m∗.

3 Analysis of estimation accuracy

In this section  we derive error bounds for the calibrated spectrum E-net. We need the following
notation. Let r = rank(Θ)  U DV (cid:62) be the SVD of Θ  and s1 ≥ . . . ≥ sr be the nonzero singular
values of Θ. Let T be the tangent space with respect to U V (cid:62)  the space of all matrices of the form
U U(cid:62)M1 + M2V V (cid:62). The orthogonal projection to T is given by

Theorem 2 Let ξ = 1 + λ2/π0 and H =(cid:80)n

PT M = U U(cid:62)M + M V V (cid:62) − U U(cid:62)M V V (cid:62).

i=1 Pωi. Deﬁne
R = (H − π0)PT /(π0 + λ2) 
∆ = R(λ2Θ + λ1U V (cid:62)) 
Q = I − H(PTHPT + λ2PT )−1PT .

Let ε =(cid:80)n

i=1 εiEωi. Suppose
(cid:107)PTR(cid:107)(op) ≤ 1/2 
(cid:107)PT ∆(cid:107)(F ) ≤ √
(cid:107)PT ε(cid:107)(F ) ≤ √

rλ1/8 

rλ1/8 

sr ≥ 5λ1/λ2 

(cid:107)∆ − R(PTR + PT )−1PT ∆(cid:13)(cid:13)(S) ≤ λ1/4 

(cid:107)Qε(cid:107)(S) ≤ 3λ1/4 

(cid:107)P⊥

T ε(cid:107)(S) ≤ λ1.

(6)

(7)
(8)

(9)

Then the calibrate spectrum E-net (4) satisﬁes

(cid:107)(cid:98)Θ − Θ(cid:107)(F ) ≤ 2

√

rλ1/π0.

(10)
The proof of Theorem 2 is provided in Section 5. When ωi are random entries in Ω∗  EH = π0I 
so that (8) and the ﬁrst inequality of (7) are expected to hold under proper conditions. Since the
rank of PT ε is no greater than 2r  (9) essentially requires (cid:107)ε(cid:107)(S) (cid:16) λ1. Our analysis allows λ2 to
lie in a certain range [λ∗  λ∗]  and λ∗/λ∗ is large under proper conditions. Still  the choice of λ2 is
constrained by (7) and (8) since ∆ is linear in λ2. When λ2/π0 diverges to inﬁnity  the calibrated
spectrum E-net (4) becomes the modiﬁed spectrum Lasso of [7].
Theorem 2 provides sufﬁcient conditions on the target matrix and the noise for achieving a cer-
tain level of estimation error.
Intuitively  these conditions on the target matrix Θ must imply a
certain level of coherence (or ﬂatness) of the unknown matrix since it is impossible to distinguish
the unknown from zero when the observations are completely outside its support. In [2  3  4  11] 
coherence conditions are imposed on

µ0 = max{(d1/r)(cid:107)U U(cid:62)(cid:107)∞  (d2/r)(cid:107)V V (cid:62)(cid:107)∞}  µ1 =(cid:112)d1d2/r(cid:107)U V (cid:62)(cid:107)∞ 

(11)
where U and V are matrices of singular vectors of Θ. [9] considered a more general notation of
spikiness of a matrix M  deﬁned as the ratio between the (cid:96)∞ and dimension-normalized (cid:96)2 norms 

αsp(M ) = (cid:107)M(cid:107)∞

d1d2/(cid:107)M(cid:107)(F ).

(12)
Suppose in the rest of the section that ωi are iid points uniformly distributed in Ω∗ and εi are iid
N (0  σ2) variables independent of {ωi}. The following theorem asserts that under certain coherence
conditions on the matrices Θ  U U(cid:62)  V V (cid:62) and U V (cid:62)  all conditions of Theorem 2 hold with large
probability when the sample size n is of the order r2d log d.

(cid:112)

Theorem 3 Let d = d1 + d2. Consider λ1 and λ2 satisfying

λ1 = σ(cid:112)8π0d log d 

1 ≤

λ2(cid:107)Θ(cid:107)(F )

λ1{n/(d log d)}1/4

≤ 2.

(13)

4

Then  there exists a constant C such that

(cid:110)

n ≥ C max

implies

µ2
0r2d log d  (µ1 + r)µ1rd log d  (α4/3

(cid:107)(cid:98)Θ − Θ(cid:107)2

(F )/(d1d2) ≤ 32(σ2rd log d)/n

(cid:111)

(14)

sp ∨ κ4∗)r2d log d

with probability at least 1 − 1/d2  where µ0 and µ1 are the coherence constants in (11)  αsp =
αsp(Θ) is the spikiness of Θ and κ∗ = (cid:107)Θ(cid:107)(F )/(r1/2sr).
We require the knowledge of noise level σ to determine the penalty level that is usually con-
sidered as tuning parameter in practice. The Frobenius norm (cid:107)Θ(cid:107)(F ) in (13) can be replaced
In our simulation experiment  we use
by an estimate of the same magnitude in Theorem 3.
i /π0)1/2. The Chebyshev inequality provides

λ2 = λ1{n/(d log d)}1/4/(cid:98)F with (cid:98)F = ((cid:80)n
(cid:98)F /(cid:107)Θ(cid:107)(F ) → 1 when αsp = O(1) and σ2 (cid:28) (cid:107)Θ(cid:107)2∞.

i=1 y2

A key element in our analysis is to ﬁnd a probabilistic bound for the second inequality of (8)  or
equivalently an upper bound for

P(cid:8)(cid:107)R(PTR + PT )−1(λ2Θ + λ1U V (cid:62))(cid:107)(S) > λ1/4(cid:9).

(15)
This guarantees the existence of a primal dual certiﬁcate for the spectrum E-net penalty [14].
For λ2 = 0  a similar inequality was proved in [3]  where the sample size requirement is
n ≥ C0 min{µ2r2(log d)2d  µ2r(log d)6d} for a certain coherence factor µ. We remove a log
factor in the ﬁrst bound  resulting in the sample size requirement in (14)  which is optimal when
r = O(1). For exact recovery in the noiseless case  the sample size n (cid:16) rd(log d)2 is sufﬁcient if
a golﬁng scheme is used to construct an approximate dual certiﬁcate [4  11]. We use the following
lemma to bound (15).

Lemma 2 Let H = (cid:80)n

(cid:112)

Cµ2

0r2dkm/n

(cid:17)2m

.

(cid:111)km(cid:16)

d1d2/r)(cid:107)M(cid:107)∞

ξ2kmE(cid:107)RkM(cid:107)2m

(S) ≤(cid:110)

i=1 Pωi where ωi are iid points uniformly distributed in Ω∗. Let R =
(H − π0)PT /(π0 + λ2) and ξ = 1 + λ2/π0. Let M be a deterministic matrix. Then  there exists a
numerical constant C such that  for all k ≥ 1 and m ≥ 1 
µ−2
0 (

(16)
We use a different graphical approach than those in [3] to bound E trace({(RkM )(cid:62)(RkM )}m) in
the proof of Lemma 2. The rest of the proof of Theorem 3 can be outlined as follows. Assume
that all coherence factors are O(1). Let M = λ2Θ + λ1U V (cid:62) and write R(PTR + PT )−1M =
M +Rem. By (16) with km (cid:16) log d for k ≥ 2 and an even simpler
RM−R2M +···+(−1)k∗−1Rk∗
√
d1d2/r)(cid:107)M(cid:107)∞ (cid:16) λ1η  where η (cid:16) r2d(log d)/n.
bound for k = 1 and Rem  (15) holds when (
Since αsp + µ1 + (cid:107)Θ(cid:107)2
r) = O(1)  this is equivalent to η(srλ2/λ1 + 1) (cid:46) 1. Finally  we use
matrix exponential inequalities [10  12] to verify other conditions of Theorem 2. We omit technical
details of the proof of Lemma 2 and Theorem 3. We would like to point out that if the r2 in (16) can
be replaced by r(log d)γ  e.g. γ = 5 in view of [3]  the rest of the proof of Theorem 3 is intact with
η (cid:16) rd(log d)1+γ/n and a proper adjustment of λ2 in (13).
Compared with [7] and [9]  the main advantage of Theorem 3 is the proportionality of its error
ωi in (2) is replaced by its expectation
π0(cid:107)M(cid:107)2

bound to the noise level. In [7]  the quadratic term(cid:80)n

(F ) and the resulting minimizer is proved to satisfy

(F )/(rs2

i=1 M 2

(cid:107)(cid:98)Θ(KLT) − Θ(cid:107)2

(F )/(d1d2) ≤ C max(σ2 (cid:107)Θ(cid:107)2∞)rd(log d)/n

with large probability  where C is a numerical constant. This error bound achieves the squared error
rate σ2rd(log d)/n as in Theorem 3 when the noise level σ is of no smaller order than (cid:107)Θ(cid:107)∞  but
not of smaller order. In particular  (17) does not imply exact recovery when σ = 0. In Theorem 3 
the error bound converges to zero as the noise level diminishes  implying exact recovery in the
√
noiseless case. In [9]  a constrained spectrum Lasso was proposed that minimizes (2) subject to
(cid:107)M(cid:107)∞ ≤ α∗/

d1d2. For (cid:107)Θ(cid:107)(F ) ≤ 1 and αsp(Θ) ≤ α∗  [9] proved

(cid:107)(cid:98)Θ(NW) − Θ(cid:107)2

(F ) ≤ C max(d1d2σ2  1)(α∗)2rd(log d)/n

(17)

(18)

5

(cid:107)(cid:98)Θ(NW) − Θ(cid:107)2

with large probability. Scale change from the above error bound yields

(F )/(d1d2) ≤ C max{σ2 (cid:107)Θ(cid:107)2

(F )/(d1d2)}(α∗)2rd(log d)/n.

√
Since α∗ ≥ 1 and α∗(cid:107)Θ(cid:107)(F )/
d1d2 ≥ (cid:107)Θ(cid:107)∞  the right-hand side of (18) is of no smaller order
than that of (17). We shall point out that (17) and (18) only require sample size n (cid:16) rd log d. In
addition  [9] allows more practical weighted sampling models.
Compared with [6]  the main advantage of Theorem 3 is the independence of its sample size require-
ment on the aspect ratio d2/d1  where d2 ≥ d1 is assumed without loss of generality by symmetry.
The error bound in [6] implies

(19)
for sample size n ≥ C∗
2} are constants depending on the
same set of coherence factors as in (14) and s1 > ··· > sr are the singular values of Θ. Therefore 

Theorem 3 effectively replaces the root aspect ratio(cid:112)d2/d1 in the sample size requirement of (19)

1 rd log d + C∗

1   C∗

(F )/(d1d2) ≤ C0(s1/sr)4σ2rd(log d)/n

2 r2d(cid:112)d2/d1  where {C∗

(cid:107)(cid:98)Θ(KMO) − Θ(cid:107)2

with a log factor  and removes the coherence factor (s1/sr)4 on the right-hand side of (19). We
note that s1/sr is a larger coherence factor than (cid:107)Θ(cid:107)(F )/(r1/2sr) in the sample size requirement in
Theorem 3. The root aspect ratio can be removed from the sample size requirement for (19) if Θ
can be divided into square blocks uniformly satisfying the coherence conditions.

4 Simulation study

√

r/σ.

deﬁned as SNR =
We compare the calibrated spectrum E-net (4) with the spectrum Lasso (2) and its modiﬁcation

This experiment has the same setting as in Section 9 of [8]. We provide the description of the
simulation settings in our notation as follows: The target matrix is Θ = U V (cid:62)  where Ud1×r and
Vd2×r are random matrices with independent standard normal entries. The sampling points ωi have
Y = PΩ(Θ + ε) with PΩ = H =(cid:80)n
no tie and Ω = {ωi : i = 1  . . .   n} is a uniformly distributed random subset of {1  . . .   d1} ×
{1  . . .   d2}  where n is ﬁxed. The errors ε are iid N (0  σ2) variables. Thus  the observed matrix is
i=1 Pωi being a projection. The signal to noise ratio (SNR) is
(cid:98)Θ(KLT) of [7]. For all methods  we compute a series of estimators with 100 different penalty lev-
λ1{n/(d log d)}1/4/(cid:98)F   where (cid:98)F = ((cid:80)n
els  where the smallest penalty level corresponds to a full-rank solution and the largest penalty
level corresponds to a zero solution. For the calibrated spectrum E-net  we always use λ2 =
i /π0)1/2 is an estimator for (cid:107)Θ(cid:107)(F ). We plot the
training errors and test errors as functions of estimated ranks  where the training and test errors are
(cid:107)PΩ((cid:98)Θ − Y )(cid:107)2
deﬁned as

i=1 y2

(cid:107)P⊥

Ω ((cid:98)Θ − Θ)(cid:107)2
Ω Θ(cid:107)2

(cid:107)P⊥

(F )

(F )

.

Training error =

(cid:107)PΩY (cid:107)2

(F )

(F )

  Test error =

In Figure 1  we report the estimation performance of three methods. The rank of Θ is 10 but
{Θ  Ω  ε} are regenerated in each replication. Different noise levels and proportions of the ob-
served entries are considered. All the results are averaged over 50 replications. In this experiment 
the calibrated spectrum E-net and the spectrum Lasso estimator have very close testing and training
errors  and both of them signiﬁcantly outperform the modiﬁed Lasso. Figure 1 also illustrates that
in most cases  the calibrated spectrum E-net and spectrum Lasso achieve the optimal test error when
the estimated rank is around the true rank.

We note that the constrained spectrum Lasso estimator(cid:98)Θ(NW) would have the same performance as
the spectrum Lasso when the constraint αsp((cid:98)Θ) ≤ α∗ is set with a sufﬁciently high α∗. However 

analytic properties of the spectrum Lasso is unclear without constraint or modiﬁcation.

5 Proof of Theorem 2

The proof of Theorem 2 requires the following proposition that controls the approximation error of
the Taylor expansion of the nuclear norm with subdifferentiation. The result  closely related to those

6

Figure 1: Plots of training and testing errors against the estimated rank: testing error with solid lines;
training error with dashed lines; spectrum Lasso in blue  calibrated spectrum E-net in red; modiﬁed
spectrum Lasso in black; d1 = d2 = 100  rank(Θ) = 10.

in [13]  is used to control the variation of the tangent space of the spectrum E-net estimator. We omit
its proof.
Proposition 1 Let Θ = U DV (cid:62) be the SVD and M be another matrix. Then 
T M(cid:107)(N ) − (cid:104)U V (cid:62)  M − Θ(cid:105)
(F ) + (cid:107)D−1/2U(cid:62)(PT M − Θ)(cid:107)2

0 ≤ (cid:107)M(cid:107)(N ) − (cid:107)Θ(cid:107)(N ) − (cid:107)P⊥
≤ (cid:107)(PT M − Θ)V D−1/2(cid:107)2

(F ).

Proof of Theorem 2. Deﬁne

Since(cid:98)Θ = ξ(cid:101)Θ and ξΘ − Θ = −(λ1/π0)U V (cid:62) 

Θ∗ = (PTHPT + λ2PT )−1(PT ε + PTHΘ − λ1U V (cid:62)) 
Θ = (π0 + λ2)−1(π0Θ − λ1U V (cid:62)) 

∆ = (cid:101)Θ − Θ∗  ∆∗ = Θ∗ − Θ  ∆∗ = (cid:101)Θ − Θ.
(cid:107)(cid:98)Θ − Θ(cid:107)(F ) ≤ ξ(cid:107)∆∗(cid:107)(F ) + (cid:107)ξΘ − Θ(cid:107)(F )

√

= ξ(cid:107)∆∗(cid:107)(F ) +
rλ1/π0
≤ ξ(cid:107)∆(cid:107)(F ) + ξ(cid:107)∆∗(cid:107)(F ) +

√

rλ1/π0.

(20)
(21)

(22)

We consider two cases by comparing λ2 and π0.
Case 1: λ2 ≤ π0. By algebra ξ∆∗ = π−1

ξ(cid:107)∆∗(cid:107)(F ) ≤ π−1

(cid:107)∆(cid:107)(F ). Let Y =(cid:80)n
(cid:101)Θ = arg min

0 (PTR + PT )−1PT (ε + ∆)  so that

0 (cid:107)(PTR + PT )−1(cid:107)(op)(cid:107)PT ∆ + PT ε(cid:107)(F ) ≤ √
(cid:110)(cid:104)HM  M(cid:105)/2 − (cid:104)Y  M(cid:105) + λ1(cid:107)M(cid:107)(N ) + (λ2/2)(cid:107)M(cid:107)2

i=1 yiEωi. We write the spectrum E-net estimator (3) as

(cid:111)

.

(F )

rλ1/(2π0).

The last inequality above follows from the ﬁrst inequalities in (7)  (8) and (9). It remains to bound

M

7

01020304000.51π0=0.2  SNR=1RankError010203000.51π0=0.2  SNR=10RankError020406000.51π0=0.5  SNR=1RankError01020304000.51π0=0.5  SNR=10RankError051015202500.51π0=0.8  SNR=10RankError02040608000.51π0=0.8  SNR=1RankErrorIt follows that for a certain member (cid:98)G in the sub-differential of (cid:107)M(cid:107)(N ) at M = (cid:101)Θ 
0 = ∂Lλ1 λ2((cid:101)Θ) = H(cid:101)Θ − Y + λ2(cid:101)Θ + λ1(cid:98)G = (H + λ2)∆ + (H + λ2)Θ∗ − Y + λ1(cid:98)G.
Let Rem1 = (cid:107)Θ∗(cid:107)(N ) − (cid:104)U V (cid:62)  Θ∗(cid:105). Since (cid:107)Θ∗(cid:107)(N ) − (cid:107)(cid:101)Θ(cid:107)(N ) ≥ −(cid:104)∆ (cid:98)G(cid:105)  we have
(cid:104)(H + λ2)∆  ∆(cid:105) ≤ (cid:104)HΘ + ε − (H + λ2)Θ∗  ∆(cid:105) + λ1(cid:107)Θ∗(cid:107)(N ) − λ1(cid:107)(cid:101)Θ(cid:107)(N )

= (cid:104)H(Θ − Θ∗) + ε − λ2Θ∗  ∆(cid:105) + λ1Rem1 + λ1(cid:104)U V (cid:62)  Θ∗(cid:105) − λ1(cid:107)(cid:101)Θ(cid:107)(N )
T ∆(cid:107)(N )
T (cid:101)Θ = P⊥

≤ λ1Rem1 + (cid:104)ε + H(Θ − Θ∗) − λ2Θ∗ − λ1U V (cid:62)  ∆(cid:105) − λ1(cid:107)P⊥
= λ1Rem1 + (cid:104)ε + H(Θ − Θ∗) P⊥

T ∆(cid:105) − λ1(cid:107)P⊥
T ∆(cid:107)(N ).
T (cid:101)Θ(cid:107)(N ) +(cid:104)U V (cid:62) (cid:101)Θ(cid:105) and P⊥

The second inequality in (23) is due to (cid:107)(cid:101)Θ(cid:107)(N ) ≥ (cid:107)P⊥

T ∆. The
last equality in (23) follows from the deﬁnition of Θ∗ ∈ T   since it gives PT ε + PTH(Θ − Θ∗) −
λ2Θ∗ − λ1U V (cid:62) = −(PTHPT + λ2PT )Θ∗ + PT ε + PTHΘ − λ1U V (cid:62) = 0. By the deﬁnitions
of Q  Θ∗ and ∆  ε + H(Θ − Θ∗) = Qε + H(Θ − Θ) − H(PTHPT + λ2PT )−1PT ∆. Since
P⊥
T HPT = P⊥

T R(π0 + λ2) and (H − π0)(Θ − Θ) = ∆  we ﬁnd

T (H − π0)PT = P⊥

(23)

(cid:104)ε + H(Θ − Θ∗) P⊥

T ∆(cid:105)

= (cid:104)Qε + (H − π0){Θ − Θ − (PTHPT + λ2PT )−1PT ∆} P⊥
= (cid:104)Qε + ∆ − R(PTR + PT )−1PT ∆ P⊥

T ∆(cid:105).

T ∆(cid:105)

Thus  by the second inequalities of (8) and (9) 
(cid:104)ε + H(Θ − Θ∗) P⊥

T ∆(cid:105) ≤ λ1(cid:107)P⊥

T ∆(cid:107)(N ).

(24)
Since Θ∗ = ∆∗ − Θ ∈ T and the singular values of Θ is no smaller than (π0sr − λ1)/(π0 + λ2) ≥
(sr − λ1/λ2)/ξ ≥ 4λ1/(λ2ξ) by the second inequality in (7)  Proposition 1 and (22) imply

Rem1 ≤ 2(cid:107)Θ∗ − Θ(cid:107)2

(F )/{(π0sr − λ1)/(π0 + λ2)} ≤ r(λ1/π0)2/(8ξλ1/λ2).

It follows from (23)  (24) and (25) that

ξ2(cid:107)∆(cid:107)2

(F ) ≤ ξ2(cid:104)(H + λ2)∆  ∆(cid:105)/λ2 ≤ ξ2(λ1/λ2)Rem1 ≤ rλ2

1/(4π2

0).

(25)

(26)

Therefore  the error bound (10) follows from (21)  (22) and (26).
Case 2: λ2 ≥ π0. By applying the derivation of (23) to Θ instead of Θ∗  we ﬁnd

(cid:104)(H + λ2)∆∗  ∆∗(cid:105) + λ1(cid:107)P⊥

(cid:0)(cid:107)Θ(cid:107)(N ) − (cid:104)U V (cid:62)  Θ(cid:105)(cid:1) + (cid:104)ε + H(Θ − Θ) − λ2Θ − λ1U V (cid:62)  ∆∗(cid:105).

T ∆∗(cid:107)(N )

≤ λ1

By the deﬁnitions of ∆  R  and Θ  ∆ = (H − π0)(Θ − Θ) = H(Θ − Θ) − λ2Θ − λ1U V (cid:62). This
and (cid:107)Θ(cid:107)(N ) = (cid:104)U V (cid:62)  Θ(cid:105) gives

(cid:104)(H + λ2)∆∗  ∆∗(cid:105) + λ1(cid:107)P⊥

T ∆∗(cid:107)(N ) ≤ (cid:104)ε + ∆  ∆∗(cid:105).

Since (cid:107)P⊥

T ε(cid:107)(S) ≤ λ1 by the third inequality in (9)  we have
T (ε + ∆)(cid:107)(S) = (cid:107)P⊥
(cid:104)P⊥
T (ε + ∆)  ∆∗(cid:105) ≤ λ1(cid:107)P⊥

T ∆∗(cid:107)(N ).
It follows from (27)  (28) and the ﬁrst inequalities of (8) and (9) that
λ2(cid:107)∆∗(cid:107)2
Thus  due to λ2 ≥ π0 

(F ) ≤ (cid:104)PT (ε + ∆)  ∆∗(cid:105) ≤(cid:110)(cid:107)PT ε(cid:107)(F ) + (cid:107)PT ∆(cid:107)(F )
rλ1/2 ≤ √

ξ(cid:107)∆∗(cid:107)(F ) ≤ (ξ/λ2)

rλ1/π0.

√

(cid:111)(cid:107)∆∗(cid:107)(F ) ≤ √

rλ1(cid:107)∆∗(cid:107)(F )/2.

Therefore  the error bound (10) follows from (20) and (29).

Acknowledgments

(27)

(28)

(29)
(cid:3)

This research is partially supported by the NSF Grants DMS 0906420  DMS-11-06753 and DMS-
12-09014  and NSA Grant H98230-11-1-0205.

8

References

[1] ACM SIGKDD and Netﬂix. Proceedings of KDD Cup and workshop. 2007.
[2] E. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput.

Math.  9:717–772  2009.

[3] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

IEEE Trans. Inform. Theory  56(5):2053–2080  2009.

[4] D. Gross. Recovering low-rank matrices from few coefﬁcients in any basis. CoRR 

abs/0910.1879  2009.

[5] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries.

Transactions on Information Theory  56(6):2980–2998  2010.

IEEE

[6] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. Journal of

Machine Learning Research  11:2057–2078  2010.

[7] V. Koltchinskii  K. Lounici  and A. B. Tsybakov. Nuclear-norm penalization and optimal rates

for noisy low-rank matrix completion. The Annals of Statistics  39:2302–2329  2011.

[8] R. Mazumder  T. Hastie  and R. Tibshirani. Spectral regularization algorithms for learning

large incomplete matrices. Journal of Machine Learning Research  11:2287–2322  2010.

[9] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix comple-

tion: Optimal bounds with noise. 2010.

[10] R. I. Oliveira. Concentration of the adjacency matrix and of the laplacian in random graphs

with independent edges. Technical Report arXiv:0911.0600  arXiv  2010.

[11] B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research 

12:3413–3430  2011.

[12] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Found. Comput. Math.

doi:10.1007/s10208-011-9099-z.  2011.

[13] P.-A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT 

12:99–111  1972.

[14] C.-H. Zhang and T. Zhang. A general framework of dual certiﬁcate analysis for structured

sparse recovery problems. Technical report  arXiv: 1201.3302v1  2012.

[15] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist.

Soc. B  67:301–320  2005.

9

,Mario Lucic
Karol Kurach
Marcin Michalski
Sylvain Gelly
Olivier Bousquet