2012,Clustering Sparse Graphs,We develop a new algorithm to cluster sparse unweighted graphs -- i.e. partition the nodes into disjoint clusters so that there is higher density within clusters  and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small  possibly vanishing in the size of the graph. Sparsity makes the problem noisier  and hence more difficult to solve.   Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case  these must be {\em penalized differently}. We analyze our algorithm's performance on the natural  classical and widely studied ``planted partition'' model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs  and with smaller clusters  than all previous methods. This is seen empirically as well.,Clustering Sparse Graphs

Department of Electrical and Computer Engineering

Yudong Chen

The University of Texas at Austin

Austin  TX 78712

ydchen@utexas.edu

Department of Electrical and Computer Engineering

Sujay Sanghavi

The University of Texas at Austin

Austin  TX 78712

sanghavi@mail.utexas.edu

Huan Xu

Mechanical Engineering Department

National University of Singapore

Singapore 117575  Singapore

mpexuh@nus.edu.sg

Abstract

We develop a new algorithm to cluster sparse unweighted graphs – i.e. partition
the nodes into disjoint clusters so that there is higher density within clusters  and
low across clusters. By sparsity we mean the setting where both the in-cluster and
across cluster edge densities are very small  possibly vanishing in the size of the
graph. Sparsity makes the problem noisier  and hence more difﬁcult to solve.
Any clustering involves a tradeoff between minimizing two kinds of errors: miss-
ing edges within clusters and present edges across clusters. Our insight is that in
the sparse case  these must be penalized differently. We analyze our algorithm’s
performance on the natural  classical and widely studied “planted partition” model
(also called the stochastic block model); we show that our algorithm can cluster
sparser graphs  and with smaller clusters  than all previous methods. This is seen
empirically as well.

1

Introduction

This paper proposes a new algorithm for the following task: given a sparse undirected unweighted
graph  partition the nodes into disjoint clusters so that the density of edges within clusters is higher
than the edges across clusters. In particular  we are interested in settings where even within clus-
ters the edge density is low  and the density across clusters is an additive (or small multiplicative)
constant lower.
Several large modern datasets and graphs are sparse; examples include the web graph  social graphs
of various social networks  etc. Clustering naturally arises in these settings as a means/tool for
community detection  user proﬁling  link prediction  collaborative ﬁltering etc. More generally 
there are several clustering applications where one is given as input a set of similarity relationships 
but this set is quite sparse. Unweighted sparse graph clustering corresponds to a special case in
which all similarities are either “1” or “0”.
As has been well-recognized  sparsity complicates clustering  because it makes the problem noisier.
Just for intuition  imagine a random graph where every edge has a (potentially different) probability
pij (which can be reﬂective of an underlying clustering structure) of appearing in the graph. Consider
setting of small pij → 0  the mean of this variable is pij but its standard deviation is √
now the edge random variable  which is 1 if there is an edge  and 0 else. Then  in the sparse graph
pij  which

1

can be much larger. This problem gets worse as pij gets smaller. Another parameter governing
problem difﬁculty is the size of the clusters; smaller clusters are easier to lose in the noise.
Our contribution: We propose a new algorithm for sparse unweighted graph clustering. Clearly 
there will be two kinds of deviations (i.e. errors) between the given graph and any candidate cluster-
ing: missing edges within clusters  and present edges across clusters. Our key realization is that for
sparse graph clustering  these two types of error should be penalized differently. Doing so gives as
a combinatorial optimization problem; our algorithm is a particular convex relaxation of the same 
based on the fact that the cluster matrix is low-rank (we elaborate below). Our main analytical
result in this paper is theoretical guarantees on its performance for the classical planted partition
model [10]  also called the stochastic block-model [1  22]  for random clustered graphs. While this
model has a rich literature (e.g.  [4  7  10  20])  we show that our algorithm out-performs (upto
at most log factors) every existing method in this setting (i.e. it recovers the true clustering for a
bigger range of sparsity and cluster sizes). Both the level of sparsity and the number and sizes of
the clusters are allowed to be functions of n  the total number of nodes. In fact  we show that in a
sense we are close to the boundary at which “any” spectral algorithm can be expected to work. Our
simulation study conﬁrms our theoretic ﬁnding  that the proposed method is effective in clustering
sparse graphs and outperforms existing methods.
The rest of the paper is organized as follows: Section 1.1 provides an overview of related work;
Section 2 presents both the precise algorithm  and the idea behind it; Section 3 presents the main
results – analytical results on the planted partition / stochastic block model – which are shown to
outperform existing methods; Section 4 provides simulation results; and ﬁnally  the proof of main
theoretic results is outlined in Section 5.

1.1 Related Work

The general ﬁeld of clustering  or even graph clustering  is too vast for a detailed survey here; we
focus on the most related threads  and therein too primarily on work which provides theoretical
“cluster recovery” guarantees on the resulting algorithms.
Correlation clustering: As mentioned above  every candidate clustering will have two kinds of er-
rors; correlation clustering [2] weighs them equally  thus the objective is to ﬁnd the clustering which
minimizes just the total number of errors. This is an NP-hard problem  and [2] develops approxi-
mation algorithms. Subsequently  there has been much work on devising alternative approximation
algorithms for both the weighted and unweighted cases  and for both agreement and disagreement
objectives [12  13  3  9]. Approximations based on LP relaxation [11] and SDP relaxation [25  19] 
followed by rounding  have also been developed. All of this line of work is on worst-case guaran-
tees. We emphasize that while we do convex relaxation as well  we do not do rounding; rather  our
convex program itself yields an optimal clustering.
Planted partition model / Stochastic block model: This is a natural and classic model for studying
graph clustering in the average case  and is also the setting for our performance guarantees. Our
results are directly comparable to work here; we formally deﬁne this setting in section 3 and present
a detailed comparison  after some notation and our theorem  in section 3 below.
Sparse and low-rank matrix decomposition: It has recently been shown [8  6] that  under certain
conditions  it is possible to recover a low-rank matrix from sparse errors of arbitrary magnitude; this
has even been applied to graph clustering [17]. Our algorithm turns out to be a weighted version
of sparse and low-rank matrix decomposition  with different elements of the sparse part penalized
differently  based on the given input. To our knowledge  ours is the ﬁrst paper to study any weighted
version; in that sense  while our weights have a natural motivation in our setting  our results are
likely to have broader implications  for example robust versions of PCA when not all errors are
created equal  but have a corresponding prior.

2 Algorithm

Idea: Our algorithm is a convex relaxation of a natural combinatorial objective for the sparse clus-
tering problem. We now brieﬂy motivate this objective  and then formally describe our algorithm.
Recall that we want to ﬁnd a clustering (i.e. a partition of the nodes) such that in-cluster connectiv-

2

ity is denser than across-cluster connectivity. Said differently  we want a clustering that has a small
number of errors  where an error is either (a) an edge between two nodes in different clusters  or
(b) a missing edge between two nodes in the same cluster. A natural (combinatorial) objective is to
minimize a weighted combination of the two types of errors.
The correlation clustering setup [2] gives equal weights to the two types of errors. However  for
sparse graphs  this will yield clusters with a very small number of nodes. This is because there is
sparsity both within clusters and across clusters; grouping nodes in the same cluster will result in a
lot of errors of type (b) above  without yielding corresponding gains in errors of type (a) – even when
they may actually be in the same cluster. This can be very easily seen: suppose  for example  the
“true” clustering has two clusters with equal size  and the in-cluster and across-cluster edge density
are both less than 1/4. Then  when both errors are weighted equally  the clustering which puts every
node in a separate cluster will have lower cost than the true clustering.
To get more meaningful solutions  we penalize the two types of errors differently. In particular 
sparsity means that we can expect many more errors of type (b) in any solution  and hence we
should give this (potentially much) smaller weight than errors of type (a). Our crucial insight is that
we can know what kind of error will (potentially) occur on any given edge from the given adjacency
matrix itself. In particular  if aij = 1 for some pair i  j  when in any clustering it will either have no
error  or an error of type (a); it will never be an error of type (b). Similarly if aij = 0 then it can only
be an error of type (b)  if at all. Our algorithm is a convex relaxation of the combinatorial problem of
ﬁnding the minimum cost clustering  with the cost for an error on edge i  j determined based on the
value of aij. Perhaps surprisingly  this simple idea yields better results than the extensive literature
already in place for planted partitions.
We proceed by representing the given adjacency matrix A as the sum of two matrices A = Y + S 
where we would like Y to be a cluster matrix  with yij = 1 if and only if i  j are in the same cluster 
and 0 otherwise12. S is the corresponding error matrix as compared to the given A  and has values
of +1  -1 and 0.
We now make a cost matrix C ∈ Rn×n based on the insight above; we choose two values cA and
cAc and set cij = cA if the corresponding aij = 1  and cij = cAc if aij = 0. However  diagonal
cii = 0. With this setup  we have

Combinatorial Objective:

min
Y S
s.t

(cid:107)C ◦ S(cid:107)1
Y + S = A
Y is a cluster matrix

(1)

(3)

Here C ◦ S denotes the matrix obtained via element-wise product between the two matrices C  S 
i.e. (C ◦ S)ij = cijsij. Also (cid:107) · (cid:107)1 denotes the element-wise (cid:96)1 norm (i.e. sum of absolute values of
elements).
Algorithm: Our algorithm involves solving a convex relaxation of this combinatorial objective  by
replacing the “Y is a cluster matrix” constraint with (i) constraints 0 ≤ yij ≤ 1 for all elements i  j 
and (ii) a nuclear norm3 penalty (cid:107)Y (cid:107)∗ in the objective. The latter encourages Y to be low-rank  and
is based on the well-established insight that the cluster matrix (being a block-diagonal collection of
1’s) is low-rank. Thus we have our algorithm:
Sparse Graph Clustering:

(2)

Once (cid:98)Y is obtained  check if it is a cluster matrix (say e.g. via an SVD  which will also reveal

cluster membership if it is).
If it is not  any one of several rounding/aggregration ideas can be
used empirically. Our theoretical results provide sufﬁcient conditions under which the optimum
of the convex program is integral and a clustering  with no rounding required. Section 3 in the
supplementary material provides details on fast implementation for large matrices; this is one reason

min
Y S
s.t.

(cid:107)Y (cid:107)∗ + (cid:107)C ◦ S(cid:107)1
0 ≤ yij ≤ 1 ∀i  j
Y + S = A 

1In this paper we will assume the convention that aii = 1 and yii = 1 for all nodes i.
2In other words  Y is the adjacency matrix of a graph consisting of disjoint cliques.
3The nuclear norm of a matrix is the sum of its singular values.

3

ally  it is not hard to see (proof in the supplementary material) that its performance is “monotone” 

we did not include a semideﬁnite constraint on Y in our algorithm. Our algorithm has two positive
parameters: cA  cAc. We defer discussion on how to choose them until after our main result.

in the following lemma. This shows that  in the terminology of [19  4  14]  our method is robust
under a classical semi-random model where an adversary can add edge within clusters and remove
edges between clusters.

Comments: Based on the given A and these values  the optimal(cid:98)Y may or may not be a cluster ma-
trix. If (cid:98)Y is a cluster matrix  then clearly it minimizes the combinatorial objective above. Addition-
in the sense that adding edges “aligned with”(cid:98)Y cannot result in a different optimum  as summarized
Lemma 1. Suppose(cid:98)Y is the optimum of Formulation (2) for a given A. Suppose now we arbitrarily
change some edges of A to obtain (cid:101)A  by (a) choosing some edges such that(cid:98)yij = 1 but aij = 0 
and making(cid:101)aij = 1  and (b) choosing some edges where(cid:98)yij = 0 but aij = 1  and making(cid:101)aij = 0.
Then  (cid:98)Y is also an optimum of Formulation (2) with (cid:101)A as the input.
Our theoretical guarantees characterize when the optimal (cid:98)Y will be a cluster matrix  and recover

the clustering  in a natural classical problem setting called the planted partition model [10]. These
theoretical guarantees also provide guidance on how one would pick parameter values in practice;
we thus defer discussion on parameter picking until after we present our main theorem.

3 Performance Guarantees

In this section we provide analytical performance guarantees for our algorithm under a natural and
classical graph clustering setting: (a generalization of) the planted partition model [10]. We ﬁrst
describe the model  and then our results.
(Generalized) Planted partition model: Consider a random graph generated as follows: the n
nodes are partitioned into r disjoint clusters  which we will refer to as the “true” clusters. Let K be
the minimum cluster size. For every pair of nodes i  j that belong to the same cluster  edge (i  j) is
present in the graph with probability that is at least ¯p  while for every pair where the nodes are in
different clusters the edge is present with probability at most ¯q. We call this model the “generalized”
planted partition because we allow for clusters to be different sizes  and the edge probabilities also
to be different (but uniformly bounded as mentioned). The objective is to ﬁnd the partition  given
the random graph generated from it.
Recall that A is the given adjacency matrix  and let Y ∗ be the matrix corresponding to the true
clusters as above – i.e. y∗
ij = 1 if and only if i  j are in the same true cluster  and 0 otherwise..
Our result below establishes conditions under which our algorithm  speciﬁcally the convex program
(2)-(3)  yields this Y ∗ as the unique optimum (without any further need for rounding etc.) with high
probability (w.h.p.). Throughout the paper  with high probability means with probability at least
1 − c0n−10 for some absolute constant c0
Theorem 1. Suppose we choose cA =

(cid:26)(cid:113) 1−¯q

  and cAc =
. Then (Y ∗  A − Y ∗) is the unique optimal solution to Formulation (2)

(cid:110)(cid:113) ¯p

(cid:113) n

1
n log n min

(cid:27)

(cid:111)

√

16

¯q  

log4 n

√

16

1
n log n min

w.h.p. provided ¯q ≤ 1

1− ¯p   1
4   and

¯p − ¯q√
where c1 is an absolute positive constant.

¯p

√

n
K

≥ c1

log2 n.

Our theorem quantiﬁes the tradeoff between the two quantities governing the hardness of a planted
partition problem – the difference in edge densities p−q  and the minimum cluster size K – required
for our algorithm to succeed  i.e. to recover the planted partition without any error. Note that here
p  q and K are allowed to scale with n. We now discuss and remark on our result  and then compare
its performance to past approaches and theoretical results in Table 1.

√
n log2 n). This will be achieved only when ¯p − ¯q is a constant
Note that we need K to be Ω(
that does not change with n; indeed in this extreme our theorem becomes a “dense graph” result 

4

¯p decreases with n  corresponding to a sparser regime 

matching e.g. the scaling in [17  19]. If ¯p−¯q√
then the minimum size of K required will increase.
A nice feature of our work is that we only need ¯p − ¯q to be large only as compared to
¯p; several
other existing results (see Table 1) require a lower bound (as a function only of n  or n  K) on
¯p − ¯q itself. This allows us to guarantee recovery for much sparser graphs than all existing results.
For example  when K is Θ(n)  ¯p and ¯p − ¯q can be as small as Θ( log4 n
n ). This scaling is close to
then each cluster will be almost surely disconnected  and if ¯p − ¯q = o( 1
optimal: if ¯p < log n
n ) 
n
then on average a node has equally many neighbours in its own cluster and in another cluster –
both are ill-posed situations in which one can not hope to recover the underlying clustering. When
  while the previous best result for this regime

n log2 n(cid:1)  ¯p and ¯p − ¯q can be Θ

K = Ω(cid:0)√

(cid:16) n log4 n

(cid:17)

√

K2

(cid:17)

(cid:16) n2

K3

requires at least Θ

[20].

Parameters: Our algorithm has two parameters: cA and cAc. The theorem provides a way to choose
their values  assuming we know the values of the bounds ¯p  ¯q. To estimate these from data  we can
use the following rule of thumb; our empirical results are based on this rule. If all the clusters have
equal size K  it is easy to verify that the ﬁrst eigenvalue of E [A − I] is K(p − q) − p + nq with
K −1  and the third eigenvalue
multiplicity 1  the second eigenvalue is K(p−q)−p with multiplicity n
is −p with multiplicities (n − n

K ) [16]. We thus have the following rule of thumb:

1. Compute the eigenvalues of A − I  denoted as λ1  . . .   λn.
2. Let r = arg maxi=1 ... n−1(λi − λi−1). Set K = n/r.

3. Solve for p and q from the equations(cid:26)K(p − q) − p + nq = λ1

K(p − q) − p = λ2

Table 1: Comparison with literature. This table shows the lower-bound requirements on K and
p− q that existing literature needs for exact recovery of the planted partitions/clusters. Note that this
table is under the assumption that every cluster is of size K  and the edge densities are uniformly
p and q (for within and across clusters respectively). As can be seen  our algorithm achieves a
better p − q scaling than every other result. And  we achieve a better K scaling than every other
result except Shamir [23]  Oymak & Hassibi [21] and Giesen & Mitsche[15]; we are off by a at
most log2 n factor from each of these. Perhaps more importantly  we use a completely different
algorithmic approach from all of the others.

Min. cluster size K Density difference p − q

n log2 n)

√

Ω(

pn log2 n

K

)

5

Paper

Boppana [5]

Jerrum & Sorkin [18]
Condon & Karp [10]

Carson & Impaglizzo [7]

Feige & Kilian [14]

Shamir [23]
McSherry [20]

Oymak & Hassibi [21]
Giesen & Mitsche[15]

Bollobas [4]

This paper

n/2
n/2
Ω(n)
n/2
√
n/2
n log n)

Ω(

Ω(n2/3)

√
√

n)

n)

Ω(

Ω(

n

log1/8 n

)

Ω(
√
Ω(

√

1

p log n√
n )
Ω(
1
Ω(
n1/6− )
Ω(
n1/2− )
√
p√
ω(
n log n)
n log n)
Ω( 1
√
n log n
K )
Ω(
K3 )

Ω(

(cid:113) pn2
(cid:113) log n
Ω(max{√
K })
n
Ω(max{(cid:113) q log n
K  
√
n
K )
n })
n   log n

Ω(

(a)

(b)

Figure 1: (a) Comparison of our method with Single-Linkage clustering (SLINK)  spectral cluster-
ing  and low-rank-plus-sparse (L+S) approach. The area above each curve is the values of (p  q) for
which a method successfully recovers the underlying true clustering. (b) More detailed results for
the area in the box in (a). The experiments are conducted on synthetic data with n = 1000 nodes
and r = 5 clusters with equal size K = 200.

4 Empirical Results

We perform experiments on synthetic data  and compare with other methods. We generate a graph
using the planted partition model with n = 1000 nodes  r = 5 clusters with equal size K = 200 
and p  q ∈ [0  1]. We apply our method to the data  where we use the fast solver described in the
supplementary material. We estimate p and q using the heuristic described in Section 3  and choose
the weights cA and cAc according to the main theorem4. Due to numerical accuracy  the output ˆY
of our algorithm may not be integer  so we do the following simple rounding: compute the mean
¯y of the entries of ˆY   and round each entry of ˆY to 1 if it is greater than ¯y  and 0 otherwise. We
measure the error by (cid:107)Y ∗ − round( ˆY )(cid:107)1  which is simply the number of misclassifed pairs. We say
our method succeeds if it misclassiﬁes less than 0.1% of the pairs.
For comparison  we consider three alternative methods: (1) Single-Linkage clustering (SLINK) [24] 
which is a hierarchical clustering method that merge the most similar clusters in each iteration. We
use the difference of neighbours  namely (cid:107)Ai· − Aj·(cid:107)1  as the distance measure of node i and j  and
output when SLINK ﬁnds a clustering with r = 5 clusters. (2) A spectral clustering method [26] 
where we run SLINK on the top r = 5 singular vectors of A. (3) Low-rank-plus-sparse approach
[17  21]  followed by the same rounding scheme. Note the ﬁrst two methods assume knowledge of
r  which is not available to our method. Success is measured in the same way as above.
For each q  we ﬁnd the smallest p for which a method succeeds  and average over 20 trials. The
results are shown in Figure 1(a)  where the area above each curves corresponds to the range of
feasible (p  q) for each method. It can been seen that our method subsumes all others  in that we
succeed for a strictly larger range of (p  q). Figure 1(b) shows more detailed results for sparse graphs
(p ≤ 0.3  q ≤ 0.1)  for which SLINK and trace-norm-plus unweighted (cid:96)1 completely fail  while our
method signiﬁcantly outperforms the spectral method  the only alternative method that works in this
regime.

5 Proof of Theorem 1
Overview: Let S∗ (cid:44) A − Y ∗. The proof consists of two main steps: (a) developing a new approxi-
mate dual certiﬁcate condition  i.e. a set of stipulations which  if satisﬁed by any matrix W   would

4we point out that searching for the best cA and cAc while keeping cA/cAc ﬁxed might lead to better

performance  which we do not pursue here

6

0.10.20.30.40.50.60.70.800.10.20.30.40.50.60.70.80.9qp  Our methodSLINKSpectralL+S00.020.040.060.080.100.050.10.150.20.25qp  Our methodSLINKSpectralL+Sguarantee the optimality of (Y ∗  S∗)  and (b) constructing a W that satisﬁes these stipulations with
high probability. While at a high level these two steps have been employed in several papers on
sparse and low-rank matrix decomposition  our analysis is different because it relies critically on
the speciﬁc clustering setting we are in. Thus  even though we are looking at a potentially more
involved setting with input-dependent weights on the sparse matrix regularizer  our proof is much
simpler than several others in this space. Also  existing proofs do not cover our setting.
Preliminaries: Deﬁne support sets Ω (cid:44) support(S∗)  and R (cid:44) support(Y ∗). Their complements
are Ωc and Rc respectively. Due to the constraints (3) in our convex program  if (Y ∗ + ∆  S∗ − ∆)
is a feasible solution to the convex program (2)  then it has to be that ∆ ∈ D  where

D (cid:44) {M ∈ Rn×n | ∀(i  j) ∈ R : −1 ≤ mij ≤ 0;

∀(i  j) ∈ Rc : 1 ≥ mij ≥ 0}.

Thus we only need to execute steps (a) (b) above for optimality over this restricted set of deviations.
Finally  we deﬁne the (now standard) projection operators: PΩ(M ) is the matrix where the (i  j)th
entry is mij if (i  j) ∈ Ω  and 0 else. Let the SVD of Y ∗ be U0Σ0U(cid:62)
0 (notice that Y ∗ is a symmetric
positive semideﬁnite matrix)  and let PT ⊥ (M ) (cid:44) (I−U0U(cid:62)
0 ) be the projection of M
onto the space of matrices whose columns and rows are orthogonal to those of Y ∗  and PT (M ) (cid:44)
M − PT ⊥ (M ).

0 )M (I−U0U(cid:62)

Step (a) - Dual certiﬁcate condition: The following proposition provides a sufﬁcient condition
for the optimality of (Y ∗  S∗).
Proposition 1 (New Dual Certiﬁcate Conditions for Clustering). If there exists a matrix W ∈
Rn×n and a positive number  obeying the following conditions

1. (cid:107)PT ⊥ W(cid:107) ≤ 1.
2. (cid:107)PT (W )(cid:107)∞ ≤ 

3. (cid:10)PΩ(U0U(cid:62)
4. (cid:10)PΩc(U0U(cid:62)

2 min{cAc  cA}

0 + W )  ∆(cid:11) = (1 + )(cid:107)PΩ(C ◦ ∆)(cid:107)1  ∀∆ ∈ D.
0 + W )  ∆(cid:11) ≥ −(1 − )(cid:107)PΩc (C ◦ ∆)(cid:107)1  ∀∆ ∈ D

then (Y ∗  S∗) is the unique optimal solution to the convex program (2).

The proof is in the supplementary material; it also involves several steps unique to our clustering
setup here.
Step (b) - Dual certiﬁcate constructions: We now construct a W   and show that it satisﬁes the
conditions in Proposition 1 w.h.p. (but not always  and this is key to its simple construction). To keep
the notation light  we consider the standard planted partition model  where the edge probabilities are
uniform; that is  for every pair of nodes in the same cluster  there is an edge between them with
probability p ≥ ¯p  and for every pair where the nodes are in different clusters  the edge is present
with probability q ≤ ¯q. It is straightforward to adapt the proof to the general case with non-uniform
edge probabilities. We deﬁne W (cid:44) W1 + W2 where
1 − p
p

W1 (cid:44) −PΩ(U0U(cid:62)

r(cid:88)

1Rm∩Ωc 

0 ) +

(cid:20)

1
km
cAc (1 − p)

m=1

(cid:21)

.

W2 (cid:44) (1 + )

C ◦ S∗ +

1R∩Ωc − cAq
1 − q

1Rc∩Ωc

p

Intuitively speaking  the idea is that W1 and W2 are zero mean random matrices  so they are likely
to have small norms. To prove Theorem 1  it remains to show that W satisﬁes the desired conditions
w.h.p.; this is done below  with proof in the supplementary  and is much simpler than similar proofs
in the sparse-plus-low-rank literature.
Proposition 2. Under the assumptions of Theorem 1  with high probability  W satisﬁes the condi-
tions in Proposition 1 with  = 2 log2 n

(cid:113) n

K

p .

7

6 Conclusion

We presented a convex optimization formulation  essentially a weighted version of low-rank matrix
decomposition  to address graph clustering where the graph is sparse. We showed that under a wide
range of problem parameters  the proposed method guarantees to recover the correct clustering. In
fact  our theoretic analysis shows that the proposed method outperforms  i.e.  succeeds under less
restrictive conditions  every existing method in this setting. Simulation studies also validates the
efﬁciency and effectiveness of the proposed method.
This work is motivated by analyzing large-scale social network  where inherently  even actors
(nodes) within one cluster are more than likely not having connections. As such  immediate goals
for future work include faster algorithm implementations  as well as developing effective postpro-
cessing schemes (e.g.  rounding) when the obtained solution is not an exact cluster matrix.

Acknowledgments

S. Sanghavi would like to acknowledge NSF grants 0954059 and 1017525  and ARO grant
W911NF1110265. The research of H. Xu is partially supported by the Ministry of Education of
Singapore through NUS startup grant R-265-000-384-133.

References

[1] P. Holland andK.B. Laskey and S. Leinhardt. Stochastic blockmodels: Some ﬁrst steps. Social

Networks  5:109–137  1983.

[2] N. Bansal  A. Blum  and S. Chawla. Correlation clustering. Machine Learning  56(1):89–113 

2004.

[3] H. Becker.

A survey

of

http://www1.cs.columbia.edu/ hila/clustering.pdf  2005.

correlation

clustering.

Available

online

at

[4] B. Bollob´as and AD Scott. Max cut for random graphs with a planted partition. Combinatorics 

Probability and Computing  13(4-5):451–474  2004.

[5] R.B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In Foundations of

Computer Science  1987.  28th Annual Symposium on  pages 280–285. IEEE  1987.

[6] E.J. Candes  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Arxiv preprint

arXiv:0912.3599  2009.

[7] T. Carson and R. Impagliazzo. Hill-climbing ﬁnds random planted bisections. In Proceedings
of the twelfth annual ACM-SIAM symposium on Discrete algorithms  pages 903–909. Society
for Industrial and Applied Mathematics  2001.

[8] V. Chandrasekaran  S. Sanghavi  S. Parrilo  and A. Willsky. Rank-sparsity incoherence for

matrix decomposition. SIAM Journal on Optimization  21(2):572–596  2011.

[9] M. Charikar  V. Guruswami  and A. Wirth. Clustering with qualitative information. In Foun-
dations of Computer Science  2003. Proceedings. 44th Annual IEEE Symposium on  pages
524–533. IEEE  2003.

[10] A. Condon and R.M. Karp. Algorithms for graph partitioning on the planted partition model.

Random Structures and Algorithms  18(2):116–140  2001.

[11] E. Demaine and N. Immorlica. Correlation clustering with partial information. Approximation 
Randomization  and Combinatorial Optimization.. Algorithms and Techniques  pages 71–80 
2003.

[12] E.D. Demaine  D. Emanuel  A. Fiat  and N. Immorlica. Correlation clustering in general

weighted graphs. Theoretical Computer Science  361(2):172–187  2006.

[13] D. Emanuel and A. Fiat. Correlation clustering–minimizing disagreements on arbitrary

weighted graphs. Algorithms-ESA 2003  pages 208–220  2003.

[14] U. Feige and J. Kilian. Heuristics for semirandom graph problems. Journal of Computer and

System Sciences  63(4):639–671  2001.

8

[15] J. Giesen and D. Mitsche. Bounding the misclassiﬁcation error in spectral partitioning in the
planted partition model. In Graph-Theoretic Concepts in Computer Science  pages 409–420.
Springer  2005.

[16] J. Giesen and D. Mitsche. Reconstructing many partitions using spectral techniques. In Fun-

damentals of Computation Theory  pages 433–444. Springer  2005.

[17] A. Jalali  Y. Chen  S. Sanghavi  and H. Xu. Clustering partially observed graphs via convex

optimization. Arxiv preprint arXiv:1104.4803  2011.

[18] M. Jerrum and G.B. Sorkin. The metropolis algorithm for graph bisection. Discrete Applied

Mathematics  82(1-3):155–175  1998.

[19] C. Mathieu and W. Schudy. Correlation clustering with noisy input. In Proceedings of the
Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms  pages 712–728. Society
for Industrial and Applied Mathematics  2010.

[20] F. McSherry. Spectral partitioning of random graphs. In Foundations of Computer Science 

2001. Proceedings. 42nd IEEE Symposium on  pages 529–537. IEEE  2001.

[21] S. Oymak and B. Hassibi. Finding dense clusters via ”low rank+ sparse” decomposition. Arxiv

preprint arXiv:1104.5186  2011.

[22] K. Rohe  S. Chatterjee  and B. Yu. Spectral clustering and the high-dimensional stochastic
block model. Technical report  Technical Report 791  Statistics Department  UC Berkeley 
2010.

[23] R. Shamir and D. Tsur. Improved algorithms for the random cluster graph model. Random

Structures & Algorithms  31(4):418–449  2007.

[24] R. Sibson. Slink: an optimally efﬁcient algorithm for the single-link cluster method. The

Computer Journal  16(1):30–34  1973.

[25] C. Swamy. Correlation clustering: maximizing agreements via semideﬁnite programming.
In Proceedings of the ﬁfteenth annual ACM-SIAM symposium on Discrete algorithms  pages
526–527. Society for Industrial and Applied Mathematics  2004.

[26] U. Von Luxburg. A tutorial on spectral clustering. Statistics and Computing  17(4):395–416 

2007.

9

,Qian Wang
Jiaxing Zhang
Sen Song
Zheng Zhang
Guruprasad Raghavan
Matt Thomson