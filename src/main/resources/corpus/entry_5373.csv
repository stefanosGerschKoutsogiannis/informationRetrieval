2019,Surrogate Objectives for Batch Policy Optimization in One-step Decision Making,We investigate batch policy optimization for cost-sensitive classification and contextual bandits---two related tasks that obviate exploration but require generalizing from observed rewards to action selections in unseen contexts.  When rewards are fully observed  we show that the expected reward objective exhibits suboptimal plateaus and exponentially many local optima in the worst case.  To overcome the poor landscape  we develop a convex surrogate that is calibrated with respect to entropy regularized expected reward.  We then consider the partially observed case  where rewards are recorded for only a subset of actions.  Here we generalize the surrogate to partially observed data  and uncover novel objectives for batch contextual bandit training.  We find that surrogate objectives remain provably sound in this setting and empirically demonstrate state-of-the-art performance.,Surrogate Objectives for Batch Policy Optimization

in One-step Decision Making

Minmin Chen∗ Ramki Gummadi∗ Chris Harris∗ Dale Schuurmans∗†
† University of Alberta

∗Google

Abstract

We investigate batch policy optimization for cost-sensitive classiﬁcation and con-
textual bandits—two related tasks that obviate exploration but require generalizing
from observed rewards to action selections in unseen contexts. When rewards are
fully observed  we show that the expected reward objective exhibits suboptimal
plateaus and exponentially many local optima in the worst case. To overcome
the poor landscape  we develop a convex surrogate that is calibrated with respect
to entropy regularized expected reward. We then consider the partially observed
case  where rewards are recorded for only a subset of actions. Here we generalize
the surrogate to partially observed data  and uncover novel objectives for batch
contextual bandit training. We ﬁnd that surrogate objectives remain provably sound
in this setting and empirically demonstrate state-of-the-art performance.

1

Introduction

Cost-sensitive classiﬁcation [1] and batch contextual bandits [34–36] are two problems that share the
goal of inferring  given a batch of training data  a policy that chooses high reward actions in potentially
unseen contexts. The problems differ in the assumed completeness of the data: in cost-sensitive
classiﬁcation  rewards are given (or inferable [8]) for every action  whereas in batch contextual
bandits  rewards are only observed for a small subset of actions (typically one). The batch contextual
bandit problem is more prevalent in practice  since massive data logs routinely record contexts
encountered  actions taken in response  and the outcomes that resulted [18]. Rarely  if ever  are
counterfactual outcomes recorded for actions that might have been taken instead [3]. Nevertheless  we
ﬁnd it helpful to reconsider cost-sensitive classiﬁcation  since a core learning challenge is orthogonal
to reward incompleteness: both tasks create difﬁcult optimization landscapes.
There is an extensive literature on cost-sensitive classiﬁcation. Problems with two actions have been
particularly well studied [5  8]  and subsequent work has sought to reduce multiple-action learning
to learning binary decisions [1  19]. A reduction strategy has also been used to convert simply
trained stochastic policies to cost-sensitive variants via post-processing [24]. Unfortunately  such
reductions do not compose well with current policy learning methods  which are gradient based and
best formulated as optimizing a single policy model over a well formed optimization objective.
In this paper we investigate cost-sensitive classiﬁcation with stochastic policy representations  to
ensure the developments are compatible with current deep learning methods. Our ﬁrst result is
negative: for natural policy representations  the expected reward objective generates a poor opti-
mization landscape that exhibits plateaus and potentially an exponential number of local maxima.
In response  we develop surrogate objectives for training [28]. Supervised learning research has
observed that solution quality can be ensured by using surrogates that satisfy “calibration” with
respect to a difﬁcult to optimize target loss [32  37  42]. This idea has also recently been applied to
cost-sensitive classiﬁcation [26]. We extend this approach to stochastic policies and deep models by
considering expected reward augmented with entropy regularization. This allows a convex surrogate
to be developed that improves trainability while approximating expected cost to controllable accuracy.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

We then consider batch contextual bandits  where rewards are observed only for a subset (typically
one) of the available actions in each training context. Current work has focused on direct maximization
of expected reward  using importance correction to provide unbiased (or nearly unbiased) estimates of
target gradients [14  16  41  43]. Unfortunately  as we illustrate  such an objective creates an extremely
difﬁcult optimization landscape  even if variance can be reduced to zero [6  9]. Alternatively  we
extend the calibrated surrogate to partially observed rewards  through the introduction of imputed
estimates. We prove soundness of the approach and demonstrate empirical performance beneﬁts.

2 Cost-sensitive Classiﬁcation

We ﬁrst consider cost-sensitive classiﬁcation. For simplicity assume a ﬁnite set of actions A =
{1  ...  K}  and thus we are given training data D = {(xi  ri)}T
i=1  where ri ∈ RK is a vector that
speciﬁes the reward for each action in context xi. The goal is to infer a mapping h : X → A that
speciﬁes a high reward action a ∈ A for a given context x ∈ X. Notation: We let ∆K denote the K
dimensional simplex  1 the vector of all 1s  and 1a the vector of 0s except for 1 in coordinate a.
Much of the literature on cost-sensitive classiﬁcation has focused on deterministic classiﬁers h  but
we consider stochastic policies π : X → ∆K. Any deterministic classiﬁer h can be equivalently
expressed by π(x) = 1h(x). We seek a policy that maximizes expected reward  or equivalently
minimizes expected cost. If we assume the data source is i.i.d. with a joint distribution p(x  r)  the
true risk of a policy π and its empirical risk on data set D can be deﬁned respectively by

ˆR(π D) = − 1

R(π) = −E[π(x) · r]

and

(1)
Since expected cost is the target  one might presume that directly minimizing empirical risk would be
a reasonable approach; unfortunately  this proves problematic [13]. In practice  it is nearly universal
to train an unconstrained model q : X → RK that is converted to a policy via a “softmax” transfer;
that is  policies are normally represented with the composition π(x) = f (q(x)) where the model
output q(x) is converted to a probability vector via

T

(cid:80)
(xi ri)∈D π(xi) · ri.

f (q) = eq−F (q) with F (q) = log(1 · eq).
The true and empirical risk can then be re-expressed in terms of q by
R(f ◦ q) = −E[f (q(x)) · r]
(3)
Unfortunately  the dot product r · f (q(x)) creates signiﬁcant difﬁculty  as this interacts poorly with
the softmax transfer f. A well known consequence is that the expected cost plateaus whenever the
corresponding policy probabilities are nearly deterministic. A potentially greater challenge  however 
is that the softmax transfer can also induce exponentially many local optima.

(cid:80)
(xi ri)∈D f (q(xi)) · ri.

ˆR(f ◦ q D) = − 1

and

(2)

T

Theorem 1 Even for a single context x  a deterministic reward vector r  and a linear model
q(x) = W φ(x)  the function r·f (q(x)) can have a number of local maxima in W that is exponential
in the number of actions K and the number of features in φ. (All proofs given in the appendix.1)

It is therefore unsurprising that empirical risk minimization with stochastic policies is not considered
viable in the cost-sensitive classiﬁcation literature. Nevertheless  it remains the dominant approach
for batch contextual bandits. We seek to bridge the apparent disconnect between these two settings.

2.1 Calibrated Strongly Convex Surrogate

A key idea in cost-sensitive classiﬁcation has been the development of convex surrogate objectives
that exhibit “calibration” with respect to the target risk [2  32]. We require additional deﬁnitions.
Let Q denote the set of measurable functions X → RK  and deﬁne the minimum risk by R∗ =
inf q∈Q R(f ◦ q). Note that the minimum is generally achieved at a deterministic policy  which
cannot be represented by q ∈ Q; however  the inﬁmum can be arbitrarily well approximated within
Q. It will be convenient to expand the risk deﬁnition through a notion of pointwise risk: deﬁne the
local risk as R(π  r  x) = −r · π(x)   which is related to the true risk via R(π) = E[R(π  r  x)] 
with the expectation taken over pairs (x  r) ∼ p(x  r). For each (x  r) deﬁne the minimal risk by

R∗(r  x) = inf π∈P R(π  r  x) = inf q∈Q R(f ◦ q  r  x).

(4)

1Appendix and code available at https://www.cs.ualberta.ca/~dale/neurips19/supplement

2

Consider a surrogate loss function L : (Q  RK  X) → R and let L∗(r  x) = inf q∈Q L(q  r  x). We
say that a surrogate L is calibrated with respect to the target risk R if there exists a calibration
function δ(  x) ≥ 0 such that for all  > 0  all x ∈ X  all r ∈ RK and all q ∈ Q:

L(q  r  x) − L∗(r  x) < δ(  x)

implies R(f ◦ q  r  x) < R∗(r  x) + .

(5)

Although calibrated convex surrogates have been developed for cost-sensitive classiﬁcation [26] 
these do not consider stochastic policies. Rather than extending these constructions to stochastic
policies  which is not straightforward  we develop a new surrogate for the stochastic case. Consider
an entropy regularized version of the target risk [25] which we call the smoothed risk:

and

Sτ (π) = E[Sτ (π  r  x)].

Sτ (π  r  x) = −r · π(x) + τ π(x) · log π(x)

(6)
The smoothed risk approximates the true risk  with a discrepancy that can be made arbitrarily small.
Proposition 2 Let ˜πτ = arg minπ∈P Sτ (π). Then ˜πτ (x) = exp(E[r|x] − F (E[r|x])/τ ) and
R( ˜πτ ) < R∗ + τ log K. Hence for any  > 0 setting τ < / log K ensures R( ˜πτ ) < R∗ + .
Note that the smoothed risk is not convex in q due to the softmax transfer π(x) = f (q(x)).
Nevertheless  it is possible to develop a convex surrogate that is calibrated for the smoothed risk as
follows. First we need a few properties of Bregman divergences in general and the KL divergence in
particular. The Bregman divergence DF   speciﬁed by the convex differentiable potential F   satisﬁes:
(7)
where f = ∇F   F ∗(p) is the convex conjugate of F   p = f (r) and π = f (q) [27]. Clearly  DF is
convex in its ﬁrst argument q  but not necessarily in the second. For the KL divergence in particular
we have F (q) = log 1 · eq  f (q) = eq−F (q)  F ∗(p) = p · log p  hence

DF (q(cid:107)r) = F (q) − F (r) − f (r) · (q − r) = F (q) − q · p + F ∗(p) = DF ∗ (p(cid:107)π) 

DKL(π(cid:107)p) = π · (log π − log p) = DF ∗ (π(cid:107)p) = DF (r(cid:107)q).
This means that the local smoothed risk (6) can be shown to be equivalent to

τ · π(x) − π(x) · log π(x)(cid:1) = −τ F ( r

Sτ (π  r  x) = −τ(cid:0) r

(9)
Later  in Section 3  we will ﬁnd it helpful to consider a shift v of the expected cost; i.e. R(π  r −
v  x) = v − r · π  noting this does not affect the location of the minimizer in q. The above
characterization then allows us to formulate a convex calibrated surrogate by reversing the divergence.

τ ) + τ DF

(cid:0) r
τ (cid:107)q(x)(cid:1) .

(8)

Theorem 3 For an arbitrary baseline v and τ > 0  let

(cid:0)q(x) + v

(cid:13)(cid:13) r

τ

(cid:1) + τ

(cid:13)(cid:13)q(x) − r−v

(cid:13)(cid:13)2

L(q  r  x) = τ DF

(10)
Then  for any ﬁxed v  L is strongly convex in q and calibrated with respect to the smoothed (shifted)
risk Sτ (f ◦ q  r − v  x) = Sτ (f ◦ q  r  x) − v with calibration function δ(  x) =  ∀x.
Therefore  any desired level of accuracy in minimizing empirical smoothed risk can be achieved by
approximately minimizing the surrogate loss L to appropriate accuracy.

4

.

τ

τ

2.2 Experimental Evaluation

To ﬁrst assess the overall approach  we evaluate how well optimizing the surrogate (10) minimizes
true risk  using a separate test set for evaluation. As baselines  we compare to directly minimizing
empirical risk ˆR(π) (1)  and the standard supervised objectives  log-likelihood  −Ep[log π]  and
squared error  (cid:107)q(x) − r−v
τ (cid:107)2. Empirically  we found it beneﬁcial to relax (10) to a tunable combina-
tion between the components and empirical risk. We refer to such a tuned loss as “Composite" in all
experimental results. Since the surrogate objective is a combination of the “reversed KL” objective
DF ∗ (p(cid:107)π) (7) and the squared error  we also evaluate DF ∗ (p(cid:107)π) alone to isolate its effect.
MNIST We ﬁrst consider MNIST data  training a fully connected model with one hidden layer of
512 ReLU units. The original training data was partitioned into the ﬁrst 55K examples for training and
the last 5K examples for validation. We use the validation data to select hyperparameters  including
learning rate  mini-batch size  and combination weights (details in appendix). The policy was trained
by minimizing each objective using SGD with momentum ﬁxed at 0.9 [33] for 100 epochs.

3

)

%

(

2

r
o
r
r
e

1.5

1

0.5

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

0

−
E

(cid:13)(cid:13)

q

−

p[l

o

g

π

]

ˆR
(

π

)

r−

τ (cid:13)(cid:13)2

v

Test
Train

D

F∗

20

15

10

5

0

C

o

m

−
E

(

p(cid:107)

π

)

p

o

site

p[l

o

g

π

]

(cid:13)(cid:13)

q

−

ˆR
(

π

)

r−

τ (cid:13)(cid:13)2

v

Test
Train

D

F∗

C

o

m

(

p(cid:107)

π

)

p

o

site

(a) MNIST

(b) CIFAR10

Figure 1: Training with full reward feedback across all actions (see appendix for additional results).

CIFAR-10 Next we considered the CIFAR-10 data set [15] and trained a Resnet-20 architecture [12] 
using the standard 50K training  10K validation split. We set any unspeciﬁed model hyperparameters
to the defaults for resnet in the open source tensor2tensor library [39] and tuned learning rate and
the composite loss combination weights on validation data. All objectives were trained using the
Momentum optimizer with cosine decay learning rate for 250 epochs (details in appendix).
The results in Figure 1 conﬁrm that directly minimizing ˆR(π) (1)  is not always competitive: it yields
the highest training error on both MNIST and CIFAR10  as well as poor test error. For MNIST  it is
striking that generalization can still be improved with respect to the standard log-likelihood baseline.
For CIFAR-10  as shown in Figure 1b  minimizing the reverse KL DF ∗ (p(cid:107)π) achieved 10.7% test
error  which signiﬁcantly improves directly optimizing empirical risk ˆR(π)  which obtained 19.9%
test error. The reverse KL was competitive even against the baseline log-likelihood  which achieved
10.4% test error. The results for squared error are worse than log-likelihood  while the DF ∗ (p(cid:107)π)
objective performs better in both data sets. This suggests that the generalization improvements are
coming from better minimization of DF ∗ (p(cid:107)π)  while the squared error term is helping improve the
optimization landscape. To further investigate whether ˆR(π) suffers from a difﬁcult optimization
landscape  we ran a much longer training experiment (see appendix)  ﬁnding that every method
except squared loss is eventually able to achieve about 6.5% test error  but at signiﬁcant cost.

3 Batch Contextual Bandits

We now extend these developments to the contextual bandit case. To focus on the most challenging
and practical scenario  we assume a single action has been observed in each context. Therefore  the
training data consists of tuples D = {(xi  ai  ri  βi)}  where xi ∈ X is a context  ai ∈ {1  ...  K}
is an action  ri ∈ R is a reward  and βi is the proposal probability of ai. For simplicity  we
assume a stationary behaviour (logging) policy β : X → ∆K was used to select the actions  hence
βi = β(ai|xi). Although β might not be known [20]  estimating it from D has proved effective
[41  43  4]. We continue to assume contexts and rewards are generated i.i.d. from a joint distribution
p(x  r)  but the distribution of rewards r(x) ∼ p(r|x) and actions a ∼ β(a|x) are conditionally
independent given the context x [43]. Other more elaborate models for missing data are possible  but
require committing to stronger assumptions about the data generation and behavior process [3  21].
As before  the goal is to infer a policy π : X → ∆K that maximizes expected reward. Here we
deﬁne the true risk of a policy π as in (1)  but the empirical risk  also deﬁned in (1)  is no longer
directly observable because it requires rewards for all actions. The standard solution is to formulate
an unbiased estimate of the full empirical risk (which is itself an unbiased estimate of the true risk) 
then use this as a policy optimization objective. In fact  the current literature is dominated by such an
approach  where an unbiased (or nearly unbiased) estimate of the empirical risk (1) is ﬁrst formulated
via importance correction then used as a training objective [14  16  17  29  34–36]. Unfortunately 
importance correction introduces signiﬁcant variance in gradient estimates  even using standard
variance reduction techniques. Also  as identiﬁed in Section 2  even if variance could be completely
eliminated  the underlying optimization landscape presents difﬁculties.

4

3.1 Reward Estimation

Before focusing on policy optimization  we ﬁrst need to address the problem of estimating rewards
from incomplete data. We here adopt a simple approach of imputing missing values with a model
q : X → RK. That is  for a context x  observed action a and observed reward ra  we estimate the
full reward vector r by

ˆr(x) = τ q(x) + 1aλ(x  a)(ra − τ q(x)a) 

(11)
with parameters λ(x  a) and τ. This construction allows the local risk of any policy π to be estimated
by R(π  ˆr  x) = −π(x) · ˆr(x). Although (11) seems simplistic  it is able to express most estimators
in the literature by suitable choices of τ and λ(x  a). For example  choosing τ = 0 and λ(x  a) =
β(a|x)−1 yields importance weighting R(π  ˆr  x) = π(x)a
β(a|x) ra; choosing τ = 1 and λ(x  a) = 0
yields the “direct method” R(π  ˆr  x) = π(x) · q(x); and choosing τ = 1 and λ(x  a) = β(a|x)−1
yields the “doubly robust” estimate R(π  ˆr  x) = π(x)· q(x) + π(x)a
β(a|x) (ra − q(x)a) [6]. The “switch”
estimator [40] can be expressed for a given threshold θ > 0 by setting τ = 1 and λ(x  a) = 0 if
π(x)a/θ > β(a|x)  otherwise τ = 0 and λ(x  a) = β(a|x)−1. The “switch” estimator generalizes
the trimmed importance estimator [3]  and is argued in [40] to be superior to the “magic” estimator
[38]. The “self-normalized” importance estimator [36] can also be expressed by setting τ = 0 and

(xi ai ri)∈D π(xi)aiβ(ai|xi)−1.

For any ﬁxed q(x) and τ it is easy to show that λ(x  a) = β(a|x)−1 implies Ea∼β(·|x)[ˆr(x)] = r(x).
A key question is the provenance of q. The standard approach is to recover q by regressing to the
(xi ai ri)∈D(ri − q(xi)ai)2 for a class of models H. Note that
this is equivalent to conducting a policy evaluation step for the policy β. Stochastic contextual bandits
are a restricted case of reinforcement learning where every policy has the same action value function 
E[r(x)]. Hence  a single policy evaluation  yielding q  can in principle be used to evaluate any policy 
since evaluating π instead of β does not change action values but only introduces covariate shift.

λ(x  a) = β(a|x)−1/(cid:80)
observed rewards q = arg minq∈H(cid:80)

3.2 Policy Optimization

For policy optimization  one could adopt the least squares estimate q and optimize a separate policy 
but if π uses the same architecture the optimum is simply π = f ◦ q (under S). In Section 2  we saw
that least squares estimation of q did not perform well  nor do we expect so here. We would like to
gain the advantages realized in Section 2  but an actor-critic approach obviates policy optimization.
Instead  to couple the value estimator to policy optimization  we consider a uniﬁed approach where
the actor and the critic are the same model. That is  we use the policy transformation π = f ◦ q from
Section 2  but now explicitly treat the logits as action value estimates. A uniﬁed actor-critic model has
been considered previously [22]. In the partially observed case  we propose to replace the observed
reward vector with the estimate ˆr derived from q  allowing any loss to be applied. Although such an
approach seems naive  we ﬁnd that maintaining this form of strict mutual consistency between the
value estimates and policy  combined with the estimator ˆr and surrogate losses  leads to effective
empirical performance. Moreover  we will ﬁnd that this approach is theoretically justiﬁed.

3.3 Calibrated Surrogate

Given (x  a  ra)  deﬁne the optimal imputed local risk and the suboptimality gap respectively by

S∗
τ (ˆr  x) = inf q∈Q Sτ (f ◦ q  ˆr  x)

and

Equality (9) can then be used to show the divergence DF

Proposition 4 For any q  τ > 0 and observation (x  a  ra): τ DF

Gτ (π  ˆr  x) = Sτ (π  ˆr  x) − S∗

τ (cid:107)q(cid:1) characterizes the suboptimality gap:
(cid:0) ˆr
(cid:13)(cid:13)q(x)(cid:1) = Gτ (f ◦ q  ˆr  x).

(cid:0) ˆr(x)

τ (ˆr  x).

(12)

τ

If we consider the imputed form of the surrogate objective L(q  ˆr  x) deﬁned in Theorem 3 we then
ﬁnd that the surrogate remains calibrated for the imputed smoothed risk.

Theorem 5 For any model q  τ > 0  observation (x  a  ra)  and baseline v:

L(q  ˆr  x) ≥ τ DF

= Gτ (f ◦ q  ˆr  x) ≥ 0.

(13)

(cid:16) ˆr(x)

τ

(cid:13)(cid:13)(cid:13)q(x) + v

τ

(cid:17)

5

Moreover  L is calibrated with respect to Sτ (f ◦ q  ˆr−v  x) with calibration function δ(x  ) = .
This result suggests a simple algorithmic approach for policy optimization: given the data D =
{(xi  ai  ri  βi)}  minimize the imputed empirical surrogate objective with respect to the model q:
(14)
That is  we combine the estimate ˆr from Section 3.1  (11)  with the surrogate L from Section 2  (10).

minq∈Q ˆL(q D) where

(xi ai ri βi)∈D L(q  ˆr  xi).

ˆL(q D) = 1

(cid:80)

T

3.4 Analysis

τ = inf q∈Q Sτ (f ◦ q)

The expected smoothed risk quantities we seek to control are deﬁned by:
Sτ (π) = E[Sτ (π  r  x)]  S∗
(15)
For the purposes of analysis  we assume training data consists of tuples drawn from (x  a  ra) ∼
p(x  r)β(a|x)  and that the estimate ˆr is unbiased; i.e.  E[ˆr|x] = E[r|x]  using λ(x  a) = β(a|x)−1.
First  observe that  in expectation  the surrogate objective upper bounds the divergence in Theorem 5 
which  in turn  by Jensen’s inequality  bounds the suboptimality gap in the expected smoothed risk.
Theorem 6 For any model q  any ˆr such that E[ˆr|x] = E[r|x]  and any baseline v:

Gτ (π) = Sτ (π) − S∗
τ .

and

(cid:16) ˆr(x)

τ

(cid:13)(cid:13)(cid:13)q(x) + v

τ

(cid:17)(cid:105) ≥ Gτ (f ◦ q) ≥ 0.

τ DF

(16)

E[L(q  ˆr  x)] ≥ E(cid:104)
(cid:80)

(cid:0) ˆr(xi)

T

i DF

(cid:13)(cid:13)q(xi)(cid:1) also concentrates to its expectation  uniformly over q ∈ H  for

Therefore  minimizing (14)  in expectation  minimizes the true smoothed risk (15).
This result can be made stronger by observing that  under mild assumptions  the empirical divergence
ˆD(q D) = 1
a well behaved model class H. In the appendix  we specify the conditions on H  β  and p(x  r) that 
in addition to ˆr being unbiased (i.e. E[ˆr|x] = E[r|x])  ensure ﬁnite sample concentration. We refer
to a collection H  β  p(x  r) and ˆr that satisﬁes these conditions as “well behaved”.
Lemma 7 Assume H  β  p(x  r) and ˆr are “well behaved”. Then for any τ  δ > 0 there exists a
constant C such that with probability at least 1 − δ:

τ

E(cid:104)

DF

(cid:16) ˆr(x)

τ

(cid:13)(cid:13)q(x)

(cid:17)(cid:105) ≤ ˆDF (q D) + C√

T

∀q ∈ H.

(17)

Combining Theorem 6 with Lemma 7 it can be shown that for ﬁnite sample size T   with high
probability  the empirical surrogate (14) is approximately calibrated with respect to smoothed risk.
Theorem 8 Assume H  β  p(x  r) and ˆr are “well behaved”. Then for any v and τ  δ > 0  there
exists a C such that with probability at least 1−δ: if ˆL(q D) < τ C√
for q ∈ H then Gτ (f ◦q) ≤ 2τ C√
.
That is  if ˆL(q D) can be sufﬁciently minimized within H  the suboptimality gap achieved by q will
be near-optimal with high probability  with bound diminishing to zero for large sample size.

T

T

3.5 Discussion

If we let τ = 0 in the deﬁnition of ˆr  (11)  then ˆr exhibits no dependence on q  making L(q  ˆr  x)
convex in q. However  we have found that empirical results are improved by choosing τ > 0 
since this compels the logits q to also model observed rewards. In addition  even though using an
unbiased ˆr enables the theory above  achieving unbiasedness via importance correction increases
variance  degrades the quality of the reward estimate  and yields inferior results. In our experiments 
we considered τ to be a hyperparameter  and also considered different choices for λ  including
λ(x  a) = β(a|x)−1 and λ(x  a) = 1. We also introduced tunable combination weights between the
Bregman divergence and the squared error terms in (14)  similar to the relaxation in Section 2.2. In
all cases  we chose hyperparameters from validation data only.
Note that the approach developed in this paper differs fundamentally from recent trust-region and
proximal methods in reinforcement learning [30  31]  which still directly optimize expected return 
possibly with entropy regularization [23]. These methods use proximal constraints/regularization to
improve the stability of optimization  but apply a “surrogate” as a local not a global modiﬁcation of
the objective. By contrast  we are changing the entire optimization objective globally  not locally  and
train to maximize a target that is different from expected return.

6

)

%

(

r
o
r
r
e

n
o
i
t
a
c
ﬁ
i
s
s
a
l
C

5

4

3

2

1

0

Test
Train

D

F∗

(

p(cid:107)

π

)

45
40
35
30
25
20
15
10
5
0

C

o

m

p

o

site

(cid:13)(cid:13)

q

−

ˆR
(

π

r−

τ (cid:13)(cid:13)2

v

)

Test
Train

D

F∗

(

p(cid:107)

π

)

C

o

m

p

o

site

(cid:13)(cid:13)

q

−

ˆR
(

π

r−

τ (cid:13)(cid:13)2

v

)

(a) MNIST

(b) CIFAR10

Figure 2: Training with partial (single action) reward feedback (see appendix for additional results).

Unlike previous entropy regularized approaches [11  22]  which generally consider split actor-critic
models  we achieve success with a single model that serves as both.
Another subtlety with optimizing importance corrected objectives  such as R(π  ˆr  x) = π(x)a
β(a|x) ra  is
that this does not account for the policy’s data coverage [20]; that is  a policy might minimize such
an objective by moving mass π(xi)ai away from the training observations (xi  ai  ri)  leading to
a phenomenon known as “propensity overﬁtting” [34–36]. This effect can be countered by adding
coverage-dependent conﬁdence intervals to the estimates [34–36]  or constraining [10] or regularizing
[20] toward the logging policy choices. Although such regularization is helpful  it is orthogonal to
the aim of the current investigation  as any objective can be augmented in this way.

3.6 Experimental Evaluation

As is standard in the ﬁeld [34–36]  we form a partially observed version of a supervised learning task
by sampling actions from a behaviour policy π0  assigning a reward of 1 when the action chosen
matches the correct label and a reward of 0 when it does not. Reward on all counterfactual actions
is therefore missing. For the MNIST and CIFAR-10 experiments  we used the same architecture 
optimizer and model conﬁgurations used in the fully observed label experiments. For the empirical
risk estimator ˆR(π) we used importance correction λ(x  a) = β(a|x)−1; however λ(x  a) = 1
proved to be more effective for DF ∗ (p(cid:107)π)  which is equivalent to replacing the counterfactual
rewards with the model estimate.
MNIST We evaluate the results when data is collected by the uniform behavior policy  π0(x) =
1 · 1
10. The hyperparameters for all objectives were re-optimized on validation data  using the same
optimization algorithm as before. Details are given in the appendix.
CIFAR-10 Here we also evaluate the results when data is collected by the uniform behavior policy 
π0(x) = 1 · 1
10. However  in addition  we also evaluate the proposed objectives using data released
in a recently published benchmark on CIFAR-10 [14]. (Note that the behavior policy itself was not
released in this benchmark; instead different sized training sets of size 50k  100k  150k  and 250k
were generated using this policy.) We used this alternative data to produce each column in Table 1.
For the CIFAR-10 experiments we simply set τ = 1. Additional details are given in the appendix.
Figure 2 shows the results for training on MNIST and CIFAR-10 given data collected by the random
behavior policy. In both cases  the composite objective yields improvements over optimizing ˆR(π)
directly. To investigate whether this difﬁculty is due to plateaus  we again conduct signiﬁcantly longer
training in the appendix  ﬁnding that the Composite and DF ∗ (p(cid:107)π) objectives remain advantageous.
Table 1 then shows results on CIFAR-10 using the alternative behavior data from [14]. This data
appears to be more condusive to optimizing ˆR(π) directly  although even in this scenario the
composite objective is still competitive  signiﬁcantly improving the results reported in [14].
Criteo We also test the proposed surrogate objective on the Criteo data set [18]  a large-scale test-bed
for evaluating batch contextual bandit methods [3  34]. Here again the behavior policy was not

7

τ

(cid:13)(cid:13)2

(cid:13)(cid:13)q − r−v

100k
7.51
18.26
9.85
6.92

50k
8.71
21.85
16.00
8.34

Examples
ˆR(π)
DF ∗ (p(cid:107)π)
Composite
Table 1: CIFAR-10: Test error % for the bandit
feedback data sets from [14] with increasing num-
ber of training examples.

250k
6.66
12.86
8.75
6.36

150k
6.92
14.65
8.68
6.57

Objectives
Random
Behavior

τ

DRO ˆR(π) [7]
POEM [34]

(cid:13)(cid:13)q − r−v

ˆR(π)

(cid:13)(cid:13)2

DF ∗ (p(cid:107)π)
Composite

ˆR(π) × 104
43.68 ± 2.11
53.55
53.07 ± 2.27
51.89 ± 1.73 2
51.72 ± 1.42
52.00 ± 1.28
52.30 ± 0.83
55.09 ± 2.86

Table 2: Criteo: Importance sampling esti-
mated reward on test. Error bars are 99% con-
ﬁdence intervals under normal distribution.

released  but only its generated data. Following [18]  we use only banners with a single slot (i.e. 
where only a single item is chosen) in our learning and evaluation. These banners are randomly split
into training  validation and test sets  each containing 7 million records  using the script provided by
[18]. There are 35 features used to describe the context and candidates actions (2 continuous and the
rest categorical). We encode the discrete features using one-hot encoding  and build linear models
using different learning losses. For evaluation  we report the importance sampling based estimates of
reward (user clicks on banner) ˆR(π) on the test set  as in [18].
We compare the proposed surrogates with several state-of-the-art methods on this data set. Hyperpa-
rameters of the different methods were tuned on validation data  and all objectives optimized by SGD
with momentum and batch size between 1K and 5K; more details regarding the experiment setup and
hyperparameter choices are given in the appendix. All methods use the same input encoding to map
inputs x to φ(x). In particular  we evaluated the following:
Random: Choose a candidate banner (i.e. an action) uniformly at random to display.
Behavior: Simply report observed reward on the test set acting according to the logging policy β.

(cid:13)(cid:13)2: We set v = 0 is effective since expected reward is close to 0.

Squared(cid:13)(cid:13)q − r−v

ˆR(π): Directly optimize ˆR(π) using importance correction; i.e. λ(x  a) = β(a|x)−1  τ = 0 in (11).
DRO ˆR(π): Optimize the doubly robust estimator [7]; i.e.  λ(x  a) = β(a|x)−1 and τ > 0 in (11).
POEM: Combines importance corrected empirical risk estimation  ˆR(π)  with a regularization that
penalizes the variance of the estimated ˆR(π) [34]. We tuned the additional regularization factor λ.
(To keep a fair comparison with the other methods  we did not impose capping on the importance
weight here  which was additionally tuned in [34].)
DF∗ (p(cid:107)π): The imputation strategy for the reward uses λ(x  a) = β(a|x)−1 and we tune τ > 0.
Composite: We combine the ˆR(π) objective with DF∗ (p(cid:107)π). In addition to the scaling factor τ  we
tuned the combination weights.
Table 2 reports the estimated reward obtained by each method on test data. Here we can see that
training with the proposed surrogate performs competitively against previous state-of-the-art-methods.

τ

4 Conclusion

We investigated alternative objectives for policy optimization in cost-sensitive classiﬁcation and
contextual bandits. The formulations developed are directly applicable to deep learning and improve
the underlying optimization landscape. The empirical results in both the cost-sensitive classiﬁcation
and batch contextual bandit scenarios replicate or surpass training with state-of-the-art baseline
objectives  merely through the optimization of the non-standard loss functions. There remain several
opportunities for further development of surrogate training objectives for sequential decision making
tasks (i.e. in planning and reinforcement learning).

2The number reported here is lower than reported in the original paper [34]. One hypothesis is that this is
due to the removal of the cap on importance weights. A similar result to what we obtain was reported in [20].

8

References
[1] Naoki Abe  Bianca Zadrozny  and John Langford. An iterative method for multi-class cost-
sensitive learning. In Proceedings of the International Conference on Knowledge Discovery
and Data Mining (KDD)  pages 3–11  2004.

[2] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  101(473)  2006.

[3] Léon Bottou  Jonas Peters  Joaquin Quiñonero Candela  Denis Xavier Charles  Max Chickering 
Elon Portugaly  Dipankar Ray  Patrice Y. Simard  and Ed Snelson. Counterfactual reasoning
and learning systems: the example of computational advertising. Journal of Machine Learning
Research  14(1):3207–3260  2013.

[4] Minmin Chen  Alex Beutel  Paul Covington  Sagar Jain  Francois Belletti  and Ed H Chi. Top-k
off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining  pages 456–464  2019.

[5] Jacek P. Dmochowski  Paul Sajda  and Lucas C. Parra. Maximum likelihood in cost-sensitive
learning: Model speciﬁcation  approximations  and upper bounds. Journal of Machine Learning
Research  11:3313–3332  2010.

[6] Miroslav Dudík  Dumitru Erhan  John Langford  and Lihong Li. Doubly robust policy evaluation

and optimization. Statistical Science  29(4):458–511  2014.

[7] Miroslav Dudík  John Langford  and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the International Conference on Machine Learning (ICML)  pages 1097–1104 
2011.

[8] Charles Elkan. The foundations of cost-sensitive learning. In Proceedings of the International

Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages 973–978  2001.

[9] Mehrdad Farajtabar  Yinlam Chow  and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 1446–1455  2018.

[10] Scott Fujimoto  David Meger  and Doina Precup. Off-policy deep reinforcement learning
without exploration. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 2052–2062  2019.

[11] Tuomas Haarnoja  Aurick Zhou  Pieter Abbeel  and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the
International Conference on Machine Learning (ICML)  pages 1856–1865  2018.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. arXiv preprint arXiv:1512.03385  2015.

[13] Klaus-Uwe Höffgen  Hans Ulrich Simon  and Kevin S. Van Horn. Robust trainability of single

neurons. Journal of Computer and System Sciences  50(1):114–125  1995.

[14] Thorsten Joachims  Adith Swaminathan  and Maarten de Rijke. Deep learning with logged
bandit feedback. In Proceedings of the International Conference on Learning Representations
(ICLR)  2018.

[15] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report 

University of Toronto  2009.

[16] Carolin Lawrence and Stefan Riezler. Improving a neural semantic parser by counterfactual
learning from human bandit feedback. In Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL)  pages 1820–1830  2018.

[17] Carolin Lawrence  Artem Sokolov  and Stefan Riezler. Counterfactual learning from bandit
feedback under deterministic logging : A case study in statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 
pages 2566–2576  2017.

9

[18] Damien Lefortier  Adith Swaminathan  Xiaotao Gu  Thorsten Joachims  and Maarten de Rijke.
Large-scale validation of counterfactual learning methods: A test-bed. CoRR  abs/1612.00367 
2016.

[19] Hsuan-Tien Lin. Reduction from cost-sensitive multiclass classiﬁcation to one-versus-one
binary classiﬁcation. In Proceedings of the Asian Conference on Machine Learning (ACML) 
2014.

[20] Yifei Ma  Yu-Xiang Wang  and Balakrishnan (Murali) Narayanaswamy. Imitation-regularized
ofﬂine learning. In Proceedings of the International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS)  2019.

[21] Karthika Mohan  Judea Pearl  and Jin Tian. Graphical models for inference with missing data.

In Advances in Neural Information Processing Systems 26  pages 1277–1285  2013.

[22] Oﬁr Nachum  Mohammad Norouzi  and Dale Schuurmans. Bridging the gap between value and
policy based reinforcement learning. In Advances in Neural Information Processing Systems
31  pages 2772–2782  2017.

[23] Oﬁr Nachum  Mohammad Norouzi  Kelvin Xu  and Dale Schuurmans. Trust-PCL: An off-policy
trust region method for continuous control. In Proceedings of the International Conference on
Learning Representations (ICLR)  2018.

[24] Deirdre B. O’Brien  Maya R. Gupta  and Robert M. Gray. Cost-sensitive multi-class classiﬁca-
tion from probability estimates. In Proceedings of the International Conference on Machine
Learning (ICML)  pages 712–719  2008.

[25] Gabriel Pereyra  George Tucker  Jan Chorowski  Lukasz Kaiser  and Geoffrey E. Hinton.
Regularizing neural networks by penalizing conﬁdent output distributions. CoRR  1701.06548 
2017.

[26] Bernardo Ávila Pires  Csaba Szepesvári  and Mohammad Ghavamzadeh. Cost-sensitive multi-
class classiﬁcation risk bounds. In Proceedings of the International Conference on Machine
Learning (ICML)  pages 1391–1399  2013.

[27] Mark D. Reid and Robert C. Williamson. Information  divergence and risk for binary experi-

ments. Journal of Machine Learning Research  12:731–817  2011.

[28] Lorenzo Rosasco  Ernesto De Vito  Andrea Caponnetto  Michele Piana  and Alessandro Verri.

Are loss functions all the same? Neural Computation  16(5):1063–107  2004.

[29] Tobias Schnabel  Adith Swaminathan  Ashudeep Singh  Navin Chandak  and Thorsten Joachims.
Recommendations as treatments: Debiasing learning and evaluation. In Proceedings of the
International Conference on Machine Learning (ICML)  pages 1670–1679  2016.

[30] John Schulman  Sergey Levine  Pieter Abbeel  Michael I. Jordan  and Philipp Moritz. Trust
region policy optimization. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 1889–1897  2015.

[31] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. CoRR  abs/1707.06347  2017.

[32] Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approxi-

mation  26(2):225–287  2007.

[33] Ilya Sutskever  James Martens  George E. Dahl  and Geoffrey E. Hinton. On the importance of
initialization and momentum in deep learning. In Proceedings of the International Conference
on Machine Learning (ICML)  pages 1139–1147  2013.

[34] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. Journal of Machine Learning Research  16:1731–
1755  2015.

10

[35] Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from
logged bandit feedback. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 814–823  2015.

[36] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual
learning. In Advances in Neural Information Processing Systems 28  pages 3231–3239  2015.

[37] Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research  8:1007–1025  2007.

[38] Philip S. Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforce-
ment learning. In Proceedings of the International Conference on Machine Learning (ICML) 
pages 2139–2148  2016.

[39] Ashish Vaswani  Samy Bengio  Eugene Brevdo  Francois Chollet  Aidan N. Gomez  Stephan
Gouws  Llion Jones  Łukasz Kaiser  Nal Kalchbrenner  Niki Parmar  Ryan Sepassi  Noam
Shazeer  and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR 
abs/1803.07416  2018.

[40] Yu-Xiang Wang  Alekh Agarwal  and Miroslav Dudík. Optimal and adaptive off-policy evalua-
tion in contextual bandits. In Proceedings of the International Conference on Machine Learning
(ICML)  pages 3589–3597  2017.

[41] Yuan Xie  Boyi Liu  Qiang Liu  Zhaoran Wang  Yuan Zhou  and Jian Peng. Off-policy evaluation
and learning from logged bandit feedback: Error reduction via surrogate policy. In Proceedings
of the International Conference on Learning Representations (ICLR)  2019.

[42] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5  2004.

[43] Zhengyuan Zhou  Susan Athey  and Stefan Wager. Ofﬂine multi-action policy learning: Gener-

alization and optimization. CoRR  abs/1810.04778  2018.

11

,Minmin Chen
Ramki Gummadi
Chris Harris
Dale Schuurmans