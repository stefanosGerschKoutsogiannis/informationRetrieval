2012,Symmetric Correspondence Topic Models for Multilingual Text Analysis,Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents  such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however  it requires a pivot language to be specified in advance. We propose a new topic model  Symmetric Correspondence LDA (SymCorrLDA)  that incorporates a hidden variable to control a pivot language  in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more effective than some other existing multilingual topic models.,Symmetric Correspondence Topic Models for

Multilingual Text Analysis

†
Kosuke Fukumasu

†
Koji Eguchi

Eric P. Xing

‡

Graduate School of System Informatics  Kobe University  Kobe 657-8501  Japan

†

School of Computer Science  Carnegie Mellon University  Pittsburgh  PA 15213  USA

‡

fukumasu@cs25.scitec.kobe-u.ac.jp  eguchi@port.kobe-u.ac.jp  epxing@cs.cmu.edu

Abstract

Topic modeling is a widely used approach to analyzing large text collections. A
small number of multilingual topic models have recently been explored to dis-
cover latent topics among parallel or comparable documents  such as in Wikipedia.
Other topic models that were originally proposed for structured data are also ap-
plicable to multilingual documents. Correspondence Latent Dirichlet Allocation
(CorrLDA) is one such model; however  it requires a pivot language to be speci-
ﬁed in advance. We propose a new topic model  Symmetric Correspondence LDA
(SymCorrLDA)  that incorporates a hidden variable to control a pivot language 
in an extension of CorrLDA. We experimented with two multilingual compara-
ble datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more
eﬀective than some other existing multilingual topic models.

1 Introduction

Topic models (also known as mixed-membership models) are a useful method for analyzing large
text collections [1  2]. In topic modeling  each document is represented as a mixture of topics  where
each topic is represented as a word distribution. Latent Dirichlet Allocation (LDA) [2] is one of the
well-known topic models. Most topic models assume that texts are monolingual; however  some
can capture statistical dependencies between multiple classes of representations and can be used for
multilingual parallel or comparable documents. Here  a parallel document is a merged document
consisting of multiple language parts that are translations from one language to another  sometimes
including sentence-to-sentence or word-to-word alignments. A comparable document is a merged
document consisting of multiple language parts that are not translations of each other but instead
describe similar concepts and events. Recently published multilingual topic models [3  4]  which
are the equivalent of Conditionally Independent LDA (CI-LDA) [5  6]  can discover latent topics
among parallel or comparable documents. SwitchLDA [6] was modeled by extending CI-LDA. It
can control the proportions of languages in each multilingual topic. However  both CI-LDA and
SwitchLDA preserve dependencies between languages only by sharing per-document multinomial
distributions over latent topics  and accordingly the resulting dependencies are relatively weak.

Correspondence LDA (CorrLDA) [7] is another type of topic model for structured data represented
in multiple classes. It was originally proposed for annotated image data to simultaneously model
words and visual features  and it can also be applied to parallel or comparable documents. In the
modeling  it ﬁrst generates topics for visual features in an annotated image. Then only the topics
associated with the visual features in the image are used to generate words. In this sense  visual
features can be said to be the pivot in modeling annotated image data. However  when CorrLDA
is applied to multilingual documents  a language that plays the role of the pivot (a pivot language1)
1Note that the term ‘pivot language’ does not have exactly the same meaning as that commonly used in the
machine translation community  where it means an intermediary language for translation between more than
three languages.

1

must be speciﬁed in advance. The pivot language selected is sensitive to the quality of the multi-
lingual topics estimated with CorrLDA. For example  a translation of a Japanese book into English
would presumably have a pivot to the Japanese book  but a set of international news stories would
have pivots that diﬀer based on the country an article is about. It is often diﬃcult to appropriately
select the pivot language. To address this problem  which we call the pivot problem  we propose
a new topic model  Symmetric Correspondence LDA (SymCorrLDA)  that incorporates a hidden
variable to control the pivot language  in an extension of CorrLDA. Our SymCorrLDA addresses the
problem of CorrLDA and can select an appropriate pivot language by inference from the data.

We evaluate various multilingual topic models  i.e.  CI-LDA  SwitchLDA  CorrLDA  and our Sym-
CorrLDA  as well as LDA  using comparable articles in diﬀerent languages (English  Japanese  and
Spanish) extracted from Wikipedia. We ﬁrst demonstrate through experiments that CorrLDA outper-
forms the other existing multilingual topic models mentioned  and then show that our SymCorrLDA
works more eﬀectively than CorrLDA in any case of selecting a pivot language.

2 Multilingual Topic Models with Multilingual Comparable Documents

Bilingual topic models for bilingual parallel documents that have word-to-word alignments have
been developed  such as those by [8]. Their models are directed towards machine translation  where
word-to-word alignments are involved in the generative process. In contrast  we focus on analyzing
dependencies among languages by modeling multilingual comparable documents  each of which
consists of multiple language parts that are not translations of each other but instead describe similar
concepts and events. The target documents can be parallel documents  but word-to-word alignments
are not taken into account in the topic modeling. Some other researchers explored diﬀerent types
of multilingual topic models that are based on the premise of using multilingual dictionaries or
WordNet [9  10  11]. In contrast  CI-LDA and SwitchLDA only require multilingual comparable
documents that can be easily obtained  such as from Wikipedia  when we use those models for
multilingual text analysis. This is more similar to the motivation of this paper. Below  we introduce
LDA-style topic models that handle multiple classes and can be applied to multilingual comparable
documents for the above-mentioned purposes.

2.1 Conditionally Independent LDA (CI-LDA)

CI-LDA [5  6] is an extension of the LDA model to handle multiple classes  such as words and
citations in scientiﬁc articles. The CI-LDA framework was used to model multilingual parallel or
comparable documents by [3] and [4]. Figure 1(b) shows a graphical model representation of CI-
LDA for documents in L languages  and Figure 1(a) shows that of LDA for reference. D  T   and
Nd respectively indicate the number of documents  number of topics  and number of word tokens
that appear in a speciﬁc language part in a document d. The superscript ‘(·)’ indicates the variables
corresponding to a speciﬁc language part in a document d. For better understanding  we show below
the process of generating a document according to the graphical model of the CI-LDA model.

For all D documents  sample θd ∼ Dirichlet(α)
For all T topics and for all L languages  sample ϕ(ℓ)
t
For each of the N(ℓ)

d words w(ℓ)

in language ℓ (ℓ ∈ {1 ···   L}) of document d:

∼ Dirichlet(β(ℓ))

1.
2.
3.

a.
b.

Sample a topic z(ℓ)
i
Sample a word w(ℓ)
i

i

∼ Multinomial(θd)
∼ Multinomial(ϕ(ℓ)
z(ℓ)
i

)

For example  when we deal with Japanese and English bilingual data  w(1) and w(2) are a Japanese
and an English word  respectively. CI-LDA preserves dependencies between languages only by
sharing the multinomial distributions with parameters (cid:18)d. Accordingly  there are substantial chances
that some topics are assigned only to a speciﬁc language part in each document  and the resulting
dependencies are relatively weak.

2.2 SwitchLDA

Similarly to CI-LDA  SwitchLDA [6] can be applied to multilingual comparable documents. How-
ever  diﬀerent from CI-LDA  SwitchLDA can adjust the proportions of multiple diﬀerent languages
for each topic  according to a binomial distribution for bilingual data or a multinomial distribu-
tion for data of more than three languages. Figure 1(c) depicts a graphical model representation of
SwitchLDA for documents in L languages. The generative process is described below.

2

(a) LDA

(b) CI-LDA

(c) SwitchLDA

Figure 1: Graphical model representations of (a) LDA  (b) CI-LDA  and (c) SwitchLDA

1.
2.

3.

a.
b.

a.
b.
c.

For all D documents  sample θd ∼ Dirichlet(α)
For all T topics:

For each of the Nd words wi in document d:

For all L languages  sample ϕ(ℓ)
Sample ψt ∼ Dirichlet(η)
t
Sample a topic zi ∼ Multinomial(θd)
Sample a language label si ∼ Multinomial(ψzi)
Sample a word wi ∼ Multinomial(ϕ(si)
zi )

∼ Dirichlet(β(ℓ))

Here   t indicates a multinomial parameter to adjust the proportions of L diﬀerent languages for
topic t. If all components of hyperparameter vector (cid:17) are large enough  SwitchLDA becomes equiv-
alent to CI-LDA. SwitchLDA is an extension of CI-LDA to give emphasis or de-emphasis to speciﬁc
languages for each topic. Therefore  SwitchLDA may represent multilingual topics more ﬂexibly;
however  it still has the drawback that the dependencies between languages are relatively weak.

2.3 Correspondence LDA (CorrLDA)

CorrLDA [7] can also be applied to multilingual comparable documents. In the multilingual setting 
this model ﬁrst generates topics for one language part of a document. We refer to this language as a
pivot language. For the other languages  the model then uses the topics that were already generated
in the pivot language. Figure 2(a) shows a graphical model representation of CorrLDA assuming L
(ℓ ∈ {p  2 ···   L})
languages  when p is the pivot language that is speciﬁed in advance. Here  N(ℓ)
d
denotes the number of words in language ℓ in document d. The generative process is shown below:
∼ Dirichlet(β(ℓ))

For all D documents’ pivot language parts  sample θ(p)
d
For all T topics and for all L languages (including the pivot language)  sample ϕ(ℓ)
t
For each of the N(p)

in the pivot language p of document d:

∼ Dirichlet(α(p))

1.
2.
3.

d words w(p)

i

)

(
in language ℓ (ℓ ∈ {2 ···   L}) of document d:

)

4.

a.
b.

a.

b.

Sample a topic z(p)
i
Sample a word w(p)
i

∼ Multinomial(θ(p)
d )
∼ Multinomial(ϕ(p)
z(p)
i

For each of the N(ℓ)

d words w(ℓ)

i

Sample a topic y(ℓ)
i
Sample a word w(ℓ)
i

z(p)
1

∼ Uni f orm
∼ Multinomial(ϕ(ℓ)
y(ℓ)
i

 ···   z(p)
)

N(p)
d

This model can capture more direct dependencies between languages  due to the constraints that top-
ics have to be selected from the topics selected in the pivot language parts. However  when CorrLDA
is applied to multilingual documents  a pivot language must be speciﬁed in advance. Moreover  the
pivot language selected is sensitive to the quality of the multilingual topics estimated with CorrLDA.

3 Symmetric Correspondence Topic Models

When CorrLDA is applied to parallel or comparable documents  this model ﬁrst generates topics
for one language part of a document  which we refer to this language as a pivot language. For the
other languages  the model then uses the topics that were already generated in the pivot language.
CorrLDA has the great advantage that it can capture more direct dependency between languages;

3

(a) CorrLDA

(b) SymCorrLDA

(c) alternative SymCorrLDA

Figure 2: Graphical model representations of (a) CorrLDA  (b) SymCorrLDA  and (c) its variant

however  it has a disadvantage that it requires a pivot language to be speciﬁed in advance. Since
the pivot language may diﬀer based on the subject  such as the country a document is about  it is
often diﬃcult to appropriately select the pivot language.
To address this problem  we propose
Symmetric Correspondence LDA (SymCorrLDA). This model generates a ﬂag that speciﬁes a pivot
language for each word  adjusting the probability of being pivot languages in each language part
of a document according to a binomial distribution for bilingual data or a multinomial distribution
for data of more than three languages. In other words  SymCorrLDA estimates from the data the
best pivot language at the word level in each document. The pivot language ﬂags may be assigned
to the words in the originally written portions in each language  since the original portions may be
described conﬁdently and with rich vocabulary. Figure 2(b) shows a graphical model representation
of SymCorrLDA. SymCorrLDA’s generative process is shown as follows  assuming L languages:

a.
b.

1.

2.
3.

For all D documents:

For all L languages  sample θ(ℓ)
Sample πd ∼ Dirichlet(γ)
d

∼ Dirichlet(α(ℓ))

For all T topics and for all L languages  sample ϕ(ℓ)
t
For each of the N(ℓ)

d words w(ℓ)

i

a.
b.
c.

d.

Sample a pivot language ﬂag x(ℓ)
i
If (x(ℓ)
i
If (x(ℓ)
i
Sample a word w(ℓ)
i

=ℓ)  sample a topic z(ℓ)
i
=m ℓ)  sample a topic y(ℓ)
i

∼ Multinomial

∼ Dirichlet(β(ℓ))

in language ℓ (ℓ ∈ {1 ···   L}) of document d:
(
∼ Multinomial(θ(ℓ)
d )
 ···   z(m)
z(m)
1
+ (1 − δ

∼ Multinomial(πd)
(
∼ Uni f orm
=ℓϕ(ℓ)
z(ℓ)
i

M(m)
=ℓ)ϕ(ℓ)
y(ℓ)
i

d
x(ℓ)
i

)

)

x(ℓ)
i

δ

is its own language ℓ  and x(ℓ)
i

= m indicates that the pivot language for w(ℓ)
i

The pivot language ﬂag x(ℓ)
= ℓ for an arbitrary language ℓ indicates that the pivot language for the
i
word w(ℓ)
is another
i
language m diﬀerent from its own language ℓ. The indicator function δ takes the value 1 when the
designated event occurs and 0 if otherwise. Unlike CorrLDA  the uniform distribution at Step 3-c is
not based on the topics that are generated for all N(m)
d words with the pivot language ﬂags  but based
only on the topics that are already generated for M(m)
) words with the pivot language
d
ﬂags at each step while in the generative process.2 The full conditional probability for collapsed
Gibbs sampling of this model is given by the following equations  assuming symmetric Dirichlet
priors parameterized by α(ℓ)  β(ℓ) (ℓ ∈ {1 ···   L})  and γ:

≤ N(m)

(M(m)
d

d

P(z(ℓ)
i

P(y(ℓ)
i

= t  x(ℓ)
i

ndℓ −i +

= t  x(ℓ)
i

= w(ℓ)  z(ℓ)−i

∑

·

i

= ℓ|w(ℓ)
∑
ndℓ −i + γ
j ℓ nd j + Lγ
= m|w(ℓ)

i

= w(ℓ)  y(ℓ)−i

+ α(ℓ)

  x−i  α(ℓ)  β(ℓ)  γ) ∝
∑
+ T α(ℓ)
  x−i  β(ℓ)  γ) ∝

  w(ℓ)−i
CT D(ℓ)
td −i
′ CT D(ℓ)
′
d −i
  z(m)  w(ℓ)−i

w(ℓ)

·

t

t

+ β(ℓ)

CW(ℓ)T
′
w(ℓ)

t −i
′ CW(ℓ)T
′
w(ℓ)

t −i

+ W(ℓ)β(ℓ)

(1)

2M(m)

d words may indeed diﬀer in size at the step of generating each word in the generative process. How-
ever  this is not problematic for inference  such as by collapsed Gibbs sampling  where any topic is ﬁrst ran-
domly assigned to every word  and a more appropriate topic is then re-assigned to each word  based on the
topics previously assigned to all N(m)

d words  with the pivot language ﬂags.

d words  not M(m)

4

Table 1: Summary of bilingual data

Table 2: Summary of trilingual data

Japanese

English

No. of documents

229 855

No. of word types (vocab)

No. of word tokens

124 046

61 187 469

173 157

80 096 333

No. of documents

No. of word types (vocab)

70 902

Japanese

English
90 602
98 474

Spanish

96 191

No. of word tokens

25 952 978 33 999 988 25 701 830

ndm −i + γ

∑
j m nd j + Lγ
}  and x(·) = {x(·)

CW(ℓ)T
′
w(ℓ)

t −i
′ CW(ℓ)T
′
w(ℓ)

t −i

·

· CT D(m)
td
N(m)
}. W(·) and N(·)

w(ℓ)

d

∑

ndm −i +
}  z(·) = {z(·)

+ β(ℓ)

+ W(ℓ)β(ℓ)

(2)

i

i

i

= m respectively are allocated to document d. CT D(·)

} in an arbitrary language j ∈ {1 ···   L} of document d  the ﬂags x( j)

where w(·) = {w(·)
d respectively indicate the total number of
vocabulary words (word types) in the speciﬁed language  and the number of word tokens that appear
in the speciﬁed language part of document d. ndℓ and ndm are the number of times  for an arbitrary
word i ∈ {1 ···   N(·)
= ℓ and
indicates the (t  d) element of a T × D
x( j)
i
topic-document count matrix  meaning the number of times topic t is allocated to the document d’s
indicates the (w  t) element of a W(·) × T word-topic
language part speciﬁed in parentheses. CW(·)T
count matrix  meaning the number of times topic t is allocated to word w in the language speciﬁed
in parentheses. The subscript ‘−i’ indicates when wi is removed from the data.
Now we slightly modify SymCorrLDA by replacing Step 3-c in its generative process by:

wt

td

d

i

3-c.

If (x(ℓ)
i

=m ℓ)  sample a topic y(ℓ)
i

∼ Multinomial(θ(m)

)

d

Figure 2(c) shows a graphical model representation of this alternative SymCorrLDA. In this model 
non-pivot topics are dependent on the distribution behind the pivot topics  not dependent directly on
the pivot topics as in the original SymCorrLDA. By this modiﬁcation  the generative process is more
naturally described. Accordingly  Eq. (2) of the full conditional probability is replaced by:

P(y(ℓ)
i

= t  x(ℓ)
i

= w(ℓ)  y(ℓ)−i

= m|w(ℓ)
∑
ndm −i + γ

i

ndm −i +

j m nd j + Lγ

∑

t

  z(m)  w(ℓ)−i
CT D(m)
′ CT D(m)

td

′

t

d

·

  x−i  β(ℓ)  γ) ∝
∑

·

+ α(m)

+ T α(m)

w(ℓ)

+ β(ℓ)

CW(ℓ)T
′
w(ℓ)

t −i
′ CW(ℓ)T
′
w(ℓ)

t −i

+ W(ℓ)β(ℓ)

(3)

As you can see in the second term of the right-hand side above  the constraints are relaxed by this
modiﬁcation so that topics do not always have to be selected from the topics selected for the words
with the pivot language ﬂags  diﬀerently from that of Eq. (2). We will show through experiments
how the modiﬁcation aﬀects the quality of the estimated multilingual topics  in the following section.

4 Experiments

In this section  we demonstrate some examples with SymCorrLDA  and then we compare multi-
lingual topic models using various evaluation methods. For the evaluation  we use held-out log-
likelihood using two datasets  the task of ﬁnding an English article that is on the same topic as that
of a Japanese article  and a task with the languages reversed.

4.1 Settings

The datasets used in this work are two collections of Wikipedia articles: one is in English and
Japanese  the other is in English  Japanese  and Spanish  and articles in each collection are connected
across languages via inter-language links  as of November 2  2009. We extracted text content from
the original Wikipedia articles  removing link information and revision history information. We used
WP2TXT3 for this purpose. For English articles  we removed 418 types of standard stop words [12].
For Spanish articles  we removed 351 types of standard stop words [13]. As for Japanese articles 
we removed function words  such as symbols  conjunctions and particles  using part-of-speech tags
annotated by MeCab4. The statistics of the datasets after preprocessing are shown in Tables 1 and
2. We assumed each set of Wikipedia articles connected via inter-language links between two (or

3http://wp2txt.rubyforge.org/
4http://mecab.sourceforge.net/

5

Figure 3: Change of frequency dis-
tribution of πd 1 according to num-
ber of iterations

(a) Examples with bilingual data

(b) Examples with trilingual data

Figure 4: Document titles and corresponding (cid:25)d

Figure 5: Topic examples and corresponding proportion of pivots assigned to Japanese. An English
translation for each Japanese word follows in parentheses  except for Japanese proper nouns.

three) languages as a comparable document that consists of two (or three) language parts. To carry
out the evaluation in the task of ﬁnding counterpart articles that we will describe later  we randomly
divided the Wikipedia document collection at the document level into 80% training documents and
20% test documents. Furthermore  to compute held-out log-likelihood  we randomly divided each
of the training documents at the word level into 90% training set and 10% held-out set.

We ﬁrst estimated CI-LDA  SwitchLDA  CorrLDA  and SymCorrLDA and its alternative version
(‘SymCorrLDA-alt’) as well as LDA for a baseline  using collapsed Gibbs sampling with the training
set. In addition  we estimated a special implementation of SymCorrLDA  setting (cid:25)d in a simple way
for comparison  where the pivot language ﬂag for each word is randomly selected according to the
proportion of the length of each language part (‘SymCorrLDA-rand’).
For all the models  we assumed symmetric Dirichlet hyperparameters α = 50/T and β = 0.01  which
have often been used in prior work [14]. We imposed the convergence condition of collapsed Gibbs
sampling  such that the percentage change of held-out log-likelihood is less than 0.1%. For Sym-
CorrLDA  we assumed symmetric Dirichlet hyperparameters γ = 1. For SwitchLDA  we assumed
symmetric Dirichlet hyperparameters η = 1. We investigated the eﬀect of γ in SymCorrLDA and
η in SwitchLDA; however  the held-out log-likelihood was almost constant when varying these hy-
perparameters. LDA does not distinguish languages  so for a baseline we assumed all the language
parts connected via inter-language links to be mixed together as a single document.

4.2 Pivot assignments

Figure 3 demonstrates how the frequency distribution of the pivot language-ﬂag (binomial) param-
eter πd 1 for the Japanese language with the bilingual dataset5 in SymCorrLDA changes while in
iterations of collapsed Gibbs sampling. This ﬁgure shows that the pivot language ﬂag is randomly
assigned at the initial state  and then it converges to an appropriate bias for each document as the it-
erations proceed. We next demonstrate how the pivot language ﬂags are assigned to each document.
Figure 4(a) shows the titles of eight documents and the corresponding (cid:25)d when using the bilingual
data (T = 500). If πd 1 is close to 1  the article can be considered to be more related to a subject on
Japanese or Japan. In contrast  if πd 1 is close to 0 and therefore πd 2 = 1 − πd 1 is close to 1  the
article can be considered to be more related to a subject on English or English-speaking countries.
Therefore  a pivot is assigned considering the language biases of the articles. Figure 4(b) shows
the titles of six documents and the corresponding (cid:25)d = (πd 1  πd 2  πd 3) when using the trilingual

5The parameter for English was πd 2 = 1 − πd 1 in this case.

6

0100002000030000400005000060000700008000000.20.40.60.81frequencyπd 10th iteration5th iteration20th iteration50th iteration0.51.00.0Japanese LanguageEuropeAustriaPhysicsHoryu̅-ji(HoryuTemple)PersonalcomputerWestern art historyShogi(Japanese chess)1 dπEuropeSonyMount FujiBullﬁghƟngNFLMobile phone)0 0 1(=dπ)0 1 0(=dπ)1 0 0(=dπ0.51.00.0ProporƟon of Japanese pivotTopic 201irelandirishscotlandscoƫshdublinairurando(Ireland)sukoƩorando(Scotland)nen(year)daburin(Dublin)kitaairurando(Northern Ireland)Topic 13japanosakakyotohughesjapaneseosakakyotoshi(city)nen(year)kobeTopic 251unitedcupmanchestermanagerleaguenen(year)ingurando(England)daihyo̅ (representaƟve)rigu(league)sizun(season)Topic 269speciesinsectseggsbodylarvaerui(species)shu(species)karada(body)konchu̅  (insect)dobutsu(animal)Topic 59castlebaƩleodahideyoshinobunaganobunagashiro(castle)hideyoshishi(surname)odaTopic 426carvehiclevehiclescarstruckkuruma(car)jidosha(automobile)sharyo̅ (vehicle)unten(driving)torakku(truck)Table 3: Per-word held-out log-likelihood with
bilingual data. Boldface indicates the best result
in each column.

T=500

T=1000

Table 4: Per-word held-out log-likelihood with
trilingual data. Boldface indicates the best result
in each column.

T=1000

LDA

CI-LDA

SwitchLDA
CorrLDA1
CorrLDA2

SymCorrLDA

SymCorrLDA-alt
SymCorrLDA-rand

Japanese
-8.127
-8.136
-8.139
-7.463
-7.777
-7.433
-7.476
-7.483

English
-8.633
-8.644
-8.641
-8.403
-8.197
-8.175
-8.206
-8.222

Japanese
-7.992
-8.008
-8.012
-7.345
-7.663
-7.317
-7.358
-7.373

English
-8.530
-8.549
-8.549
-8.346
-8.109
-8.084
-8.116
-8.137

CorrLDA1
CorrLDA2
CorrLDA3

SymCorrLDA

SymCorrLDA-alt

Japanese English Spanish Japanese English Spanish
-8.545
-7.408
-8.401
-7.655
-8.274
-7.794
-8.215
-7.394
-7.440
-8.254

-8.393
-8.122
-8.383
-8.093
-8.120

-8.667
-8.467
-8.338
-8.289
-8.330

-7.305
-7.572
-7.700
-7.287
-7.330

T=500

-8.512
-8.198
-8.460
-8.178
-8.209

data (T = 500). Here  πd 1  πd 2  and πd 3 respectively indicate the pivot language-ﬂag (multinomial)
parameters corresponding to Japanese  English  and Spanish parts in each document. We further
demonstrate the proportions of pivot assignments at the topic level. Figure 5 shows the content of
6 topics through 10 words with the highest probability for each language and for each topic when
using the bilingual data (T = 500)  some of which are biased to Japanese (Topics 13 and 59) or
English (Topics 201 and 251)  while the others have almost no bias. It can be seen that the pivot bias
to speciﬁc languages can be interpreted.

4.3 Held-out log-likelihood

By measuring the held-out log-likelihood  we can evaluate the quality of each topic model. The
higher the held-out log-likelihood  the greater the predictive ability of the model.
In this work 
we estimated multilingual topic models with the training set and computed the log-likelihood of
generating the held-out set that was mentioned in Section 4.1.

Table 3 shows the held-out log-likelihood of each multilingual topic model estimated with the bilin-
gual dataset when T = 500 and 1000. Note that the held-out log-likelihood (i.e.  the micro-average
per-word log-likelihood of the 10% held-out set) is shown for each language in this table  while
the model estimation was performed over the 90% training set in all the languages. Hereafter  Cor-
rLDA1 refers to the CorrLDA model that was estimated when Japanese was the pivot language. As
described in Section 2.3  the CorrLDA model ﬁrst generates topics for the pivot language part of a
document  and for the other language parts of the document  the model then uses the topics that were
already generated in the pivot language. CorrLDA2 refers to the CorrLDA model when English was
the pivot language. As the results in Table 3 show  the held-out log-likelihoods of CorrLDA1 and
CorrLDA2 are much higher than those of the other prior models: CI-LDA  SwitchLDA  and LDA 
in both cases. This is because CorrLDA can capture direct dependencies between languages  due to
the constraints that topics have to be selected from the topics selected in the pivot language parts.
On the other hand  CI-LDA and SwitchLDA are too poorly constrained to eﬀectively capture the
dependencies between languages  as mentioned in Sections 2.1 and 2.2. In particular  CorrLDA1
has the highest held-out log-likelihood among all the prior models for Japanese  while CorrLDA2
is the best among all the prior models for English. This is probably due to the fact that CorrLDA
can estimate topics from the pivot language parts (Japanese in the case of CorrLDA1) without any
speciﬁc constraints; however  great constraints (topics having to be selected from the topics selected
in the pivot language parts) are imposed for the other language parts. In SymCorrLDA  the held-out
log-likelihood for Japanese is larger than that of CorrLDA1 (and the other models)  and the held-out
log-likelihood for English is larger than that of CorrLDA2. This is probably because SymCorrLDA
estimates the pivot language appropriately adjusted for each word in each document. Next  we com-
pare SymCorrLDA and its alternative version (SymCorrLDA-alt). We observed in Table 3 that the
held-out log-likelihood of SymCorrLDA-alt is smaller than that of the original SymCorrLDA  and
comparable to CorrLDA’s best. This is because the constraints in SymCorrLDA-alt are relaxed so
that topics do not always have to be selected from the topics selected for the words with the pivot
language ﬂags.

further consideration 

let us examine the results of

For
the simpliﬁed implementation:
SymCorrLDA-rand  which we deﬁned in Section 4.1. SymCorrLDA-rand’s held-out log-likelihood
lies even below CorrLDA’s best. These results reﬂect the fact that the performance of SymCor-
rLDA in its full form is inherently aﬀected by the nature of the language biases in the multilingual
comparable documents  rather than merely being aﬀected by the language part length.

7

Table 4 shows the held-out log-likelihood with the trilingual data when T = 500 and 1000. Here 
CorrLDA3 refers to the CorrLDA model that was estimated when Spanish was the pivot language.
As you can see in this table  SymCorrLDA’s held-out log-likelihood is larger than CorrLDA’s best.
SymCorrLDA can estimate the pivot language appropriately adjusted for each word in each docu-
ment in the trilingual data  as with the bilingual data. SymCorrLDA-alt behaves similarly as with
the bilingual data.

For both the bilingual and trilingual data  the improvements with SymCorrLDA were statistically
signiﬁcant  compared to each of the other models  according to the Wilcoxon signed-rank test at the
5% level in terms of the word-by-word held-out log-likelihood. As for the scalability  SymCorrLDA
is as scalable as CorrLDA because the time complexity of SymCorrLDA is the same order as that of
CorrLDA: the number of topics times the sum of vocabulary size in each language. On clock time 
SymCorrLDA does pay some extra  such as around 40% of the time for CorrLDA in the case of the
bilingual data  for allocating the pivot language ﬂags.

4.4 Finding counterpart articles

Given an article  we can ﬁnd its unseen counterpart articles in other languages using a multilin-
gual topic model. To evaluate this task  we experimented with the bilingual dataset. We estimated
document-topic distributions of test documents for each language  using the topic-word distributions
that were estimated by each multilingual topic model with training documents. We then evaluated
the performance of ﬁnding English counterpart articles using Japanese articles as queries  and vice
versa. For estimating the document-topic distributions of test documents  we used re-sampling of
LDA using the topic-word distribution estimated beforehand by each multilingual topic model [15].
We then computed the Jensen-Shannon (JS) divergence [16] between a document-topic distribution
of Japanese and that of English for each test document. Each held-out English-Japanese article pair
connected via an inter-language link is considered to be on the same topic; therefore  JS divergence
of such an article pair is expected to be small if the latent topic estimation is accurate. We ﬁrst
assumed each held-out Japanese article to be a query and the corresponding English article to be
relevant  and evaluated the ranking of all the test articles of English in ascending order of the JS
divergence; then we conducted the task with the languages reversed.

Table 5: MRR in counterpart article ﬁnding task.
Boldface indicates the best result in each column.

Table 5 shows the results of mean reciprocal
rank (MRR)  when T = 500 and 1000. The re-
ciprocal rank is deﬁned as the multiplicative in-
verse of the rank of the counterpart article cor-
responding to each query article  and the mean
reciprocal rank is the average of it over all the
query articles. CorrLDA works much more ef-
fectively than the other prior models: CI-LDA 
SwitchLDA  and LDA  and overall  SymCor-
rLDA works the most eﬀectively. We observed that the improvements with SymCorrLDA were
statistically signiﬁcant according to the Wilcoxon signed-rank test at the 5% level  compared with
each of the other models. Therefore  it is clear that SymCorrLDA estimates multilingual topics the
most successfully in this experiment.

Japanese to English
T=1000
T=500
0.1027
0.0743
0.1426
0.1464
0.1347
0.1357
0.3281
0.2987
0.3063
0.2829
SymCorrLDA 0.3256
0.3592

English to Japanese
T=1000
T=500
0.1262
0.0870
0.1697
0.1818
0.1653
0.1668
0.3111
0.2863
0.3464
0.3161
0.3348
0.3685

LDA

CI-LDA

SwitchLDA
CorrLDA1
CorrLDA2

5 Conclusions

In this paper  we compared the performance of various topic models that can be applied to multilin-
gual documents  not using multilingual dictionaries  in terms of held-out log-likelihood and in the
task of cross-lingual link detection. We demonstrated through experiments that CorrLDA works sig-
niﬁcantly more eﬀectively than CI-LDA  which was used in prior work on multilingual topic models.
Furthermore  we proposed a new topic model  SymCorrLDA  that incorporates a hidden variable to
control a pivot language  in an extension of CorrLDA. SymCorrLDA has an advantage in that it does
not require a pivot language to be speciﬁed in advance  while CorrLDA does. We demonstrated that
SymCorrLDA is more eﬀective than CorrLDA and the other topic models  through experiments
with Wikipedia datasets using held-out log-likelihood and in the task of ﬁnding counterpart articles
in other languages. SymCorrLDA can be applied to other kinds of data that have multiple classes of
representations  such as annotated image data. We plan to investigate this in future work.

8

Acknowledgments We thank Sinead Williamson  Manami Matsuura  and the anonymous review-
ers for valuable discussions and comments. This work was supported in part by the Grant-in-Aid for
Scientiﬁc Research (#23300039) from JSPS  Japan.

References

[1] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd Anuual
International ACM SIGIR Conference on Research and Development in Information Retrieval 
pages 50–57  Berkeley  California  USA  1999.

[2] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. Journal of

Machine Learning Research  3:993–1022  2003.

[3] David Mimno  Hanna M. Wallach  Jason Naradowsky  David A. Smith  and Andrew McCal-
lum. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing  pages 880–889  Stroudsburg  Pennsylvania  USA  2009.

[4] Xiaochuan Ni  Jian-Tao Sun  Jian Hu  and Zheng Chen. Mining multilingual topics from
wikipedia. In Proceedings of the 18th International Conference on World Wide Web  pages
1155–1156  Madrid  Spain  2009.

[5] Elena Erosheva  Stephen Fienberg  and John Laﬀerty. Mixed-membership models of scientiﬁc
publications. Proceedings of the National Academy of Sciences of the United States of America 
101:5220–5227  2004.

[6] David Newman  Chaitanya Chemudugunta  Padhraic Smyth  and Mark Steyvers. Statistical
entity-topic models. In Proceedings of the 12th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining  pages 680–686  Philadelphia  Pennsylvania  USA 
2006.

[7] David M. Blei and Michael I. Jordan. Modeling annotated data. In Proceedings of the 26th
Annual International ACM SIGIR Conference on Research and Development in Informaion
Retrieval  pages 127–134  Toronto  Canada  2003.

[8] Bing Zhao and Eric P. Xing. BiTAM: Bilingual topic admixture models for word alignment.
In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics 
pages 969–976  Sydney  Australia  2006.

[9] Jordan Boyd-Graber and David M. Blei. Multilingual topic models for unaligned text.

In
Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence  pages 75–82 
Montreal  Canada  2009.

[10] Jagadeesh Jagarlamudi and Hal Daume. Extracting multilingual topics from unaligned com-
parable corpora. In Advances in Information Retrieval  volume 5993 of Lecture Notes in Com-
puter Science  pages 1–12. Springer  2010.

[11] Duo Zhang  Qiaozhu Mei  and ChengXiang Zhai. Cross-lingual latent topic extraction.

In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 
pages 1128–1137  Uppsala  Sweden  2010.

[12] James P. Callan  W. Bruce Croft  and Stephen M. Harding. The INQUERY retrieval system.
In Proceedings of the 3rd International Conference on Database and Expert Systems Applica-
tions  pages 78–83  Valencia  Spain  1992.

[13] Jacques Savoy. Report on CLEF-2002 experiments: Combining multiple sources of evidence.
In Advances in Cross-Language Information Retrieval  volume 2785 of Lecture Notes in Com-
puter Science  pages 66–90. Springer  2003.

[14] Mark Steyvers and Tom Griﬃths. Handbook of Latent Semantic Analysis  chapter 21: Proba-
bilistic Topic Models. Lawrence Erbaum Associates  Mahwah  New Jersey and London  2007.
[15] Hanna M. Wallach  Iain Murray  Ruslan Salakhutdinov  and David Mimno. Evaluation meth-
ods for topic models. In Proceedings of the 26th International Conference on Machine Learn-
ing  pages 1105–1112  Montreal  Canada  2009.

[16] Jianhua Lin. Divergence measures based on the shannon entropy.

Information Theory  37(1):145–151  1991.

IEEE Transactions on

9

,Sergey Levine
Pieter Abbeel
Chi Zhang
Baoxiong Jia
Feng Gao
Yixin Zhu
HongJing Lu
Song-Chun Zhu