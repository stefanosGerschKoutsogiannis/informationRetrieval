2018,Playing hard exploration games by watching YouTube,Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However  these demonstrations are typically collected under artificial conditions  i.e. with access to the agent’s exact environment setup and the demonstrator’s action and reward trajectories. Here we propose a method that overcomes these limitations in two stages. First  we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second  we embed a single YouTube video in this representation to learn a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma’s Revenge  Pitfall! and Private Eye for the first time  even if the agent is not presented with any environment rewards.,Playing hard exploration games by watching YouTube

Yusuf Aytar∗  Tobias Pfaff∗  David Budden  Tom Le Paine  Ziyu Wang  Nando de Freitas

{yusufaytar tpfaff budden tpaine ziyu nandodefreitas}@google.com

DeepMind  London  UK

Abstract

Deep reinforcement learning methods traditionally struggle with tasks where en-
vironment rewards are particularly sparse. One successful method of guiding
exploration in these domains is to imitate trajectories provided by a human demon-
strator. However  these demonstrations are typically collected under artiﬁcial
conditions  i.e. with access to the agent’s exact environment setup and the demon-
strator’s action and reward trajectories. Here we propose a two-stage method that
overcomes these limitations by relying on noisy  unaligned footage without access
to such data. First  we learn to map unaligned videos from multiple sources to a
common representation using self-supervised objectives constructed over both time
and modality (i.e. vision and sound). Second  we embed a single YouTube video
in this representation to construct a reward function that encourages an agent to
imitate human gameplay. This method of one-shot imitation allows our agent to
convincingly exceed human-level performance on the infamously hard exploration
games MONTEZUMA’S REVENGE  PITFALL! and PRIVATE EYE for the ﬁrst time 
even if the agent is not presented with any environment rewards.

1

Introduction

People learn many tasks  from knitting to dancing to playing games  by watching videos online. They
demonstrate a remarkable ability to transfer knowledge from the online demonstrations to the task
at hand  despite huge gaps in timing  visual appearance  sensing modalities  and body differences.
This rich setup with abundant unlabeled data motivates a research agenda in AI  which could result in
signiﬁcant progress in third-person imitation  self-supervised learning  reinforcement learning (RL)
and related areas. In this paper  we show how this proposed research agenda enables us to make
some initial progress in self-supervised alignment of noisy demonstration sequences for RL agents 
enabling human-level performance on the most complex and previously unsolved Atari 2600 games.
Despite the recent advancements in deep reinforcement learning algorithms [7  9  17  19] and
architectures [22  35]  there are many “hard exploration” challenges  characterized by particularly
sparse environment rewards  that continue to pose a difﬁcult challenge for existing RL agents. One
epitomizing example is Atari’s MONTEZUMA’S REVENGE [10]  which requires a human-like avatar
to navigate a series of platforms and obstacles (the nature of which change substantially room-to-
room) to collect point-scoring items. Such tasks are practically impossible using naive -greedy
exploration methods  as the number of possible action trajectories grows exponentially in the number
of frames separating rewards. For example  reaching the ﬁrst environment reward in MONTEZUMA’S
REVENGE takes approximately 100 environment steps  equivalent to 10018 possible action sequences.
Even if a reward is randomly encountered  γ-discounted RL struggles to learn stably if this signal is
backed-up across particularly long time horizons.

∗denotes equal contribution

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a) ALE frame

(b) Frames from different YouTube videos

Figure 1: Illustration of the domain gap that exists between the Arcade Learning Environment and
YouTube videos from which our agent is able to learn to play MONTEZUMA’S REVENGE. Note
different size  resolution  aspect ratio  color and addition of visual artifacts such as text and avatars.

Successful attempts at overcoming the issue of sparse rewards have fallen broadly into two categories
of guided exploration. First  intrinsic motivation methods provide an auxiliary reward that encourages
the agent to explore states or action trajectories that are “novel” or “informative” with respect to some
measure [8  30  27]. These methods tend to help agents to re-explore discovered parts of state space
that appear novel or uncertain (known-unknowns)  but often fail to provide guidance about where
in the environment such states are to be found in the ﬁrst place (unknown-unknowns). Accordingly 
these methods typically rely on an additional random component to drive the initial exploration
process. The other category is imitation learning [20  41]  whereby a human demonstrator generates
state-action trajectories that are used to guide exploration toward areas considered salient with respect
to their inductive biases. These biases prove to be a very useful constraint in the context of Atari  as
humans can immediately identify e.g. that a skull represents danger  or that a key unlocks a door.
Among existing imitation learning methods  DQfD by Hester et al. [20] has shown the best per-
formance on Atari’s hardest exploration games. Despite these impressive results  there are two
limitations of DQfD [20] and related methods. First  they assume that there is no “domain gap”
between the agent’s and demonstrator’s observation space  e.g. variations in color or resolution  or
the introduction of other visual artifacts. An example of domain gap in MONTEZUMA’S REVENGE
is shown in Figure 1  considering the ﬁrst frame of (a) our environment compared to (b) YouTube
gameplay footage. Second  they assume that the agent has access to the exact action and reward
sequences that led to the demonstrator’s observation trajectory. In both cases  these assumptions
constrain the set of useful demonstrations to those collected under artiﬁcial conditions  typically
requiring a specialized software stack for the sole purpose of RL agent training.
To address these limitations  this paper proposes a method for overcoming domain gaps between the
observation sequences of multiple demonstrations  by using self-supervised classiﬁcation tasks that
are constructed over both time (temporal distance classiﬁcation) and modality (cross-modal temporal
distance classiﬁcation) to learn a common representation (see Figure 2). Unlike previous approaches 
our method requires neither (a) frame-by-frame alignment between demonstrations  or (b) class labels
or other annotations from which an alignment might be indirectly inferred. We additionally propose a
new unsupervised measure (cycle-consistency) for evaluating the quality of such a learnt embedding.
Using our embedding  we propose an auxiliary imitation loss that allows an agent to successfully
play hard exploration games without requiring the knowledge of the demonstrator’s action trajectory.
Speciﬁcally  providing a standard RL agent with an imitation reward learnt from a single YouTube
video  we are the ﬁrst to convincingly exceed human-level performance on three of Atari’s hardest
exploration games: MONTEZUMA’S REVENGE  PITFALL! and PRIVATE EYE. Despite the challenges
of designing reward functions [18  36] or learning them using inverse reinforcement learning [1  49] 
we also achieve human-level performance even in the absence of an environment reward signal.

2 Related Work

Imitation learning methods such as DQfD have yielded promising results for guiding agent exploration
in sparse-reward tasks  both in game-playing [20  32] and robotics domains [41]. However  these
methods have traditionally leveraged observations collected in artiﬁcial conditions  i.e. in the absence
of a domain gap (see Figure 1) and with full visibility over the demonstrator’s action and reward
trajectories. Other approaches include interacting with the environment before introducing the expert

2

(a) An example path

(b) Aligned frames

(c) Our embedding

(d) Pixel embedding

Figure 2: For the path shown in (a)  t-SNE projections [25] of observation sequences using (c)
our embedding  versus (d) raw pixels. Four different domains are compared side-by-side in (b)
for an example frame of MONTEZUMA’S REVENGE: (purple) the Arcade Learning Environment 
(cyan/yellow) two YouTube training videos  and (red) an unobserved YouTube video. It is evident
that all four trajectories are well-aligned in our embedding space  despite (purple) and (red) being
held-aside during training. Using raw pixel values fails to achieve any meaningful alignment.

demonstrations [38  31] and goal conditioned policies for high ﬁdelity imitation [29]  although these
papers typically do not assume domain gap or operate in sparse reward settings.
There are several methods of overcoming the domain gap in the previous literature. In the simple
scenario of demonstrations that are aligned frame-by-frame [24  34]  methods such as CCA [2] 
DCTW [39] or time-contrastive networks (TCN) [34] can be used to learn a common representation
space. However  YouTube videos of Atari gameplay are more complex  as the actions taken by
different demonstrators can lead to very different observation sequences lacking such an alignment.
In this scenario  another common approach for domain alignment involves solving a shared auxiliary
objective across the domains [5  6]. For example  Aytar et al. [5] demonstrated that by solving the
same scene classiﬁcation task bottlenecked by a common decision mechanism (i.e. using the same
network)  several very different domains (i.e. natural images  line drawings and text descriptions)
could be successfully aligned. Similarly  domain adaptive meta-learning [44] uses a shared policy
network for addressing the domain gap for robotic tasks  though they require both ﬁrst person robot
demonstrations and third person human demonstrations. Our work differs from the above approaches
in that we do not make use of any category-guided supervision or ﬁrst person demonstrations. Instead 
we deﬁne our shared tasks using self-supervision over unlabeled data. This idea is motivated by
several recent works in the self-supervised feature learning literature [3  11  12  28  42  45  26].
Other related approaches include single-view TCN [34]  which is another self-supervised task that
does not require paired training data. We differ from this work by using temporal classiﬁcation instead
of triplet-based ranking  which removes the need to empirically tune sensitive hyper parameters (local
neighborhood size  ranking margin  etc). Another approach [33] performs temporal classiﬁcation but
limits its categories to frames close or far away in time. With respect to our use of cross-modal data 
another similar existing method in the feature learning literature is L3-net [3]. This approach learns
to align vision and sound modalities  whereas we learn to align multiple audio-visual sequences (i.e.
demonstrations) using multi-modal alignment as a self-supervised objective. We adapt both TCN
and L3-net for domain alignment and provide an evaluation compared to our proposed method in
Section 6. We also experimented with third-person imitation methods [37] that combine the ideas of
generative adversarial imitation learning (GAIL) [21] and adversarial domain confusion [16  40]  but
were unable to make progress using the very long YouTube demonstration trajectories.
Considering the imitation component of our work  one perspective is that we are learning a reward
function that explains the demonstrator’s behavior  which is closely related to inverse reinforcement
learning [1  49]. There have also been many previous studies that consider supervised [4] and
few-shot methods [13  15] for imitation learning. However  in both cases  our setting is more complex
due to the presence of domain gap and absence of demonstrator action and reward sequences.

3

(a) Temporal and cross-modal pair selection

(b) Embedding networks

(c) Classiﬁcation networks

Figure 3: Illustration of the network architectures and interactions involved in our combined
TDC+CMC self-supervised loss calculation. The ﬁnal layer FC2 of φ is later used to embed the
demonstration trajectory to imitate. Although the Arcade Learning Environment does not expose
an audio signal to our agent at training time  the audio signal present in YouTube footage made a
substantial contribution to the learnt visual embedding function φ.

3 Closing the domain gap

Learning from YouTube videos is made difﬁcult by both the lack of frame-by-frame alignment  and the
presence of domain-speciﬁc variations in color  resolution  screen alignment and other visual artifacts.
We propose that by learning a common representation across multiple demonstrations  our method
will generalize to agent observations without ever being explicitly exposed to the Atari environment.
In the absence of pre-aligned data  we adopt self-supervision in order to learn this embedding. The
rationale of self-supervision is to propose an auxiliary task that we learn to solve simultaneously
across all domains  thus encouraging the network to learn a common representation. This is motivated
by the work of Aytar et al. [5]  but differs in that we do not have access to class labels to establish a
supervised objective. Instead  we propose two novel self-supervised objectives: temporal distance
classiﬁcation (TDC)  described in Section 3.1 and cross-modal temporal distance classiﬁcation
(CMC)  described in Section 3.2. We also propose cycle-consistency in Section 3.3 as a quantitative
measure for evaluating the one-to-one alignment capacity of an embedding.

3.1 Temporal distance classiﬁcation (TDC)

We ﬁrst consider the unsupervised task of predicting the temporal distance ∆t between two frames
of a single video sequence. This task requires an understanding of how visual features move
and transform over time  thus encouraging an embedding that learns meaningful abstractions of
environment dynamics conditioned on agent interactions.
We cast this problem as a classiﬁcation task  with K categories corresponding to temporal distance
intervals  dk ∈ {[0]  [1]  [2]  [3 − 4]  [5 − 20]  [21 − 200]}. Given two frames from the same video 
v  w ∈ I  we learn to predict the interval dk s.t. ∆t ∈ dk. Speciﬁcally  we implement two functions:
an embedding function φ : I → RN   and a classiﬁer τtdc : RN × RN → RK  both implemented as
neural networks (see Section 5 for implementation details). We can then train τtdc(φ(v)  φ(w)) to
predict the distribution over class labels  dk  using the following cross-entropy classiﬁcation loss:

Ltdc(v  w  y) = − K(cid:88)

yj log(ˆyj)

with ˆy = τtdc(φ(v)  φ(w))  

(1)

where y and ˆy are the true and predicted label distributions respectively.

j=1

3.2 Cross-modal temporal distance classiﬁcation (CMC)

In addition to visual observations  our YouTube videos contain audio tracks that can be used to deﬁne
an additional self-supervised task. As the audio of Atari games tends to correspond with salient
events such as jumping  obtaining items or collecting points  a network that learns to correlate audio
and visual observations should learn an abstraction that emphasizes important game events.

4

(a) Cycle-consistency visualization

(b) One shot imitation

Figure 4: (a) Visualization of two embedding spaces with low and high cycle-consistency. Note that
the selected point in sequence V (left) fails and (right) succeeds at cycling back to the original point.
(b) Demonstration of one shot imitation through RL visualized in the embedding space.

We deﬁne the cross-modal classiﬁcation task of predicting the temporal distance between a given
video frame  v ∈ I  and audio snippet  a ∈ A. To achieve this  we introduce an additional
embedding function  ψ : A → RN   which maps from a frequency-decomposed audio snippet to
an N-dimensional embedding vector  ψ(a). The associated classiﬁcation loss  Lcmc(v  a  y)  is
equivalent to Equation (1) using the classiﬁcation function ˆy = τcmc(φ(v)  ψ(a)). Note that by
limiting our categories to the two intervals d0 = [0] and d1 = [l  . . .  ∞] with l being the local
positive neighborhood  this method reduces to L3-Net of Arandjelovic et al. [3]. In our following
experiments  we obtain a ﬁnal embedding by a λ-weighted combination of both cross-modal and
temporal distance classiﬁcation losses  i.e. minimizing L = Ltdc + λLcmc.

3.3 Model selection through cycle-consistency

A challenge of evaluating and meta-optimizing the models presented in Section 3 is deﬁning a measure
of the quality of an embedding φ. Motivated by the success of cyclic relations in CycleGAN [48]
and for matching visual features across images [47]  we propose cycle-consistency for this purpose.
Assume that we have two length-N sequences  V = {v1  v2  ...vn} and W = {w1  w2  . . .   wn}.
We also deﬁne the distance  dφ  as the Euclidean distance in the associated embedding space 
dφ(vi  wj) = ||φ(vi)− φ(wj)||2. To evaluate cycle-consistency  we ﬁrst select vi ∈ V and determine
its nearest neighbor  wj = argminw∈W dφ(vi  w). We then repeat the process to ﬁnd the nearest
neighbor of wj  i.e. vk = argminv∈V dφ(v  wj). We say that vi is cycle-consistent if and only if
|i − k| ≤ 1  and further deﬁne the one-to-one alignment capacity  Pφ  of the embedding space φ as
the percentage of v ∈ V that are cycle-consistent. Figure 4(a) illustrates cycle-consistency in two
example embedding spaces. The same process can be extended to evaluate the 3-cycle-consistency
φ  by requiring that vi remains cycle consistent along both paths V → W → U → V and
of φ  P 3
V → U → W → V   where U is a third sequence.

4 One-shot imitation from YouTube footage

In Section 3  we learned to extract features from unlabeled and unaligned gameplay footage  and
introduced a measure to evaluate the quality of the learnt embedding. In this section  we describe
how these features can be exploited to learn to play games with very sparse rewards  such as the
infamously difﬁcult PITFALL! and MONTEZUMA’S REVENGE. Speciﬁcally  we demonstrate how a
sequence of checkpoints placed along the embedding of a single YouTube video can be presented
as a reward signal to a standard reinforcement learning agent (IMPALA for our experiments [14]) 
allowing successful one-shot imitation even in the complete absence of the environment rewards.
Taking a single YouTube gameplay video  we simply generate a sequence of “checkpoints" every
N = 16 frames along the embedded trajectory. We can then represent the following reward:

(cid:26)0.5

0.0

rimitation =

if ¯φ(vagent) · ¯φ(vcheckpoint) > α
otherwise

(2)

where ¯φ(v) are the zero-centered and l2-normalized embeddings of the agent and checkpoint observa-
tions. We also require that checkpoints be visited in soft-order  i.e. if the last collected checkpoint is at

5

Embedding Method
l2 pixel distance
single-view TCN [34]
TDC (ours)
L3-Net [3]
CMC (ours)
combined (TDC+CMC)

Pφ
30.5
32.2
42.0
27.3
41.7
44.2

P 3
φ
08.4
15.9
23.0
10.9
23.6
27.5

Figure 5: Cycle-consistency evaluation considering different embedding spaces. We compare naive
l2 pixel loss to temporal methods (TDC and single-view TCN) and cross-modal methods (CMC and
L3-Net). Combining TDC and CMC yields the best performance for both 2 and 3-cycle-consistency 
particularly at deeper levels of abstraction (e.g. no performance loss using FC1 or FC2).

v(n)  then vcheckpoint ∈ {v(n+1)  . . .   v(n+1+∆t)}. We set ∆t = 1 and α = 0.5 for our experiments
(except when considering pixel-only embeddings  where α = 0.92 provided the best performance).

5

Implementation Details

The visual embedding function  φ  is composed of three spatial  padded  3x3 convolutional layers
with (32  64  64) channels and 2x2 max-pooling  followed by three residual-connected blocks with
64 channels and no down-sampling. Each layer is ReLU-activated and batch-normalized  and the
output fed into a 2-layer 1024-wide MLP. The network input is a 128x128x3x4 tensor constructed
by random spatial cropping of a stack of four consecutive 140x140 RGB images  sampled from our
dataset. The ﬁnal embedding vector is l2-normalized.
The audio embedding function  ψ  is as per φ except that it has four  width-8  1D convolutional layers
with (32  64  128  256) channels and 2x max-pooling  and a single width-1024 linear layer. The input
is a width-137 (6ms) sample of 256 frequency channels  calculated using STFT. ReLU-activation and
batch-normalization are applied throughout and the embedding vector is l2-normalized.
The same shallow network architecture  τ  is used for both temporal and cross-modal classiﬁcation.
Both input vectors are combined by element-wise multiplication  with the result fed into a 2-layer
MLP with widths (1024  6) and ReLU non-linearity in between. A visualization of these networks and
their interaction is provided in Figure 3. Note that although τtdc and τcmc share the same architecture 
they are operating on two different problems and therefore maintain separate sets of weights.
To generate training data  we sample input pairs (vi  wi) (where vi and wi are sampled from the
same domain) as follows. First  we sample a demonstration sequence from our three training videos.
Next  we sample both an interval  dk ∈ {[0]  [1]  [2]  [3 − 4]  [5 − 20]  [21 − 200]}  and a distance 
∆t ∈ dk. Finally  we randomly select a pair of frames from the sequence with temporal distance ∆t.
The model is trained with Adam using a learning rate of 10−4 and batch size of 32 for 200 000 steps.
As described in Section 4  our imitation loss is constructed by generating checkpoints every N = 16
frames along the φ-embedded observation sequence of a single YouTube video. We train an agent
using the sum of imitation and (optionally) environment rewards. We use the distributed A3C RL
agent IMPALA [14] with 100 actors for our experiments. The only modiﬁcation we make to the
published network is to calculate the distance (as per Equation(2)) between the agent and the next
two checkpoints and concatenate this 2-vector with the ﬂattened output of the last convolutional layer.
We also tried re-starting our agent from checkpoints recorded along its trajectory  similar to Hosu et
al. [23]  but found that it provided minimal improvement given even our very long demonstrations.

6 Analysis and Experiments

In this section we analyze (a) the learnt embedding spaces  and (b) the performance of our RL agent.
We consider three Atari 2600 games that are considered very difﬁcult exploration challenges: MON-
TEZUMA’S REVENGE  PITFALL! and PRIVATE EYE. For each  we select four YouTube videos (three
training and one test) of human gameplay  varying in duration from 3-to-10 minutes. Importantly 

6

Figure 6: For each embedding method  we visualize the t-SNE projection of four observation
sequences traversing the ﬁrst room of MONTEZUMA’S REVENGE. Using pixel space alone fails to
provide any meaningful cross-domain alignment. Purely cross-modal methods perform better  but
produce a very scattered embedding due to missing long-range dependencies. The combination of
temporal and cross-modal objectives yields the best alignment and continuity of trajectories.

none of the YouTube videos were collected using our speciﬁc Arcade Learning Environment [10]  and
the only pre-processing that we apply is keypoint-based (i.e. Harris corners) afﬁne transformation to
spatially align the game screens from the ﬁrst frame only. The dataset used and additional experiments
can be found in the supplemental material to this paper.

6.1 Embedding space evaluation

To usefully close the domain gap between YouTube gameplay footage and our environment observa-
tions  our learnt embedding space should exhibit two desirable properties: (1) one-to-one alignment
capacity and (2) meaningful abstraction. We consider each of these properties in turn.
First  one-to-one alignment is desirable for reliably mapping observations between different sequences.
We evaluate this property using the cycle-consistency measure introduced in Section 3.3. The features
from earlier layers in φ (see Figure 5) are centered and l2-normalized before computing cycle-
consistency. Speciﬁcally  we consider both (a) the 2-way cycle-consistency  Pφ  between the test
video and the ﬁrst training video  and (b) the 3-way cycle-consistency  P 3
φ  between the test video and
the ﬁrst two training videos. These results are presented in Figure 5  comparing the cycle-consistencies
of our TDC  CMC and combined methods to a naive l2-distance in pixel space  single-view time-
contrastive networks (TCN) [34] and L3-Net [3]. Note that we implemented single-view TCN and
φ cycle-consistency. As
L3-Net in our framework and tuned the hyperparameters to achieve the best P 3
expected  pixel loss performs worst in the presence of sequence-to-sequence domain gaps. Our TDC
and CMC methods alone yield improved performance compared to TCN and L3-Net (particularly at
deeper levels of abstraction)  and combining both methods provides the best results overall.
Next  Figure 6 shows the t-SNE projection of observation trajectories taken by different human
demonstrators to traverse the ﬁrst room of MONTEZUMA’S REVENGE. It is again evident that a
pixel-based loss entirely fails to align the sequences. The embeddings learnt using purely cross-modal
alignment (i.e. L3-Net and CMC) perform better but still yield particularly scattered and disjoint
trajectories  which is an undesirable property likely due to the sparsity of salient audio signals. TDC
and our combined TDC+CMC methods provide the more globally consistent trajectories  and are less
likely to produce false-positives with respect to the distance metric described in Section 4.
Finally  a useful embedding should provide a useful abstraction that encodes meaningful  high-level
information of the game while ignoring irrelevant features. To aid in visualizing this property  Figure 7
demonstrates the spatial activation of neurons in the ﬁnal convolutional layer of the embedding
network φ  using the visualization method proposed in [46]. It is compelling that the top activations
are centered on features including the player and enemy positions in addition to the inventory state 
which is informative of the next location that needs to be explored (e.g. if we have collected the key
required to open a door). Important objects such as the key are emphasized more in the cross-modal
and combined embeddings  likely due to the unique sounds that are played when collected (see ﬁgure
7(d) and (e)). Notably absent are activations associated with distractors such as the moving sand
animation  or video-speciﬁc artifacts indicative of the domain gap we wished to close.

7

(a) Neuron #46

(b) Neuron #8

(c) Neuron #39

(d) TDC  Overall

(e) CMC  Overall

Figure 7: (Left) Visualization of select activations in the ﬁnal convolutional layer. Individual neurons
focus on e.g. (a) the player  (b) enemies  and (c) the inventory. Notably absent are activations
associated with distractors or domain-speciﬁc artifacts. (Right) Visualization of activations summed
across all channels in the ﬁnal layer. We observe that use of the audio signal in CMC results in more
emphasis being placed on key items and their location in the inventory.

6.2 Solving hard exploration games with one-shot imitation

Using the method described in Section 4  we train an IMPALA agent to play the hard exploration
Atari games MONTEZUMA’S REVENGE  PITFALL! and PRIVATE EYE using a learned auxiliary
reward to guide exploration. For each game  the embedding network  φ  was trained using just
three YouTube videos  and an additional video was embedded to generate a sequence of exploration
checkpoints. Videos of our agent playing these games can be found here2.
Figure 8 presents our learning curves for each hard exploration Atari game. Without imitation reward 
the pure RL agent is unable to collect any of the sparse rewards in MONTEZUMA’S REVENGE
and PITFALL!  and only reaches the ﬁrst two rewards in PRIVATE EYE (consistent with previous
studies using DQN variants [19  22]). Using pixel-space features  the guided agent is able to obtain
17k points in PRIVATE EYE but still fails to make progress in the other games. Replacing a pixel
embedding with our combined TDC+CMC embedding convincingly yields the best results  even if
the agent is presented only with our TDC+CMC imitation reward (i.e. no environment reward).
To test the impact of the choice of expert trajectory  we generate checkpoints from two additional
videos of MONTEZUMA’S REVENGE from our set  and train agents with those sequences (ﬁgure 8 
left). While all three agents manage to clear the ﬁrst level  expert 1 achieves the highest score. Out of
the three expert sequence considered  expert 1 also has the biggest domain shift. This is in line with
our ﬁndings from section 6.1 that our embedding space can sufﬁciently align our sequences. Domain
shift in the expert trajectories is therefore not a signiﬁcant factor on performance.
Finally  in Table 1 we compare our best policies for each game to the best previously published
results; Rainbow [19] and ApeX DQN [22] without demonstrations  and DQfD [20] using expert
demonstrations. Unlike DQfD our demonstrations are unaligned YouTube footage without access
to action or reward trajectories. Our results are calculated using the standard approach of averaging
over 200 episodes initialized using a random 1-to-30 no-op actions. Importantly  our approach is
the ﬁrst to convincingly exceed human-level performance on all three games – even in the absence
of an environment reward signal. We are the ﬁrst to solve the entire ﬁrst level of MONTEZUMA’S
REVENGE and PRIVATE EYE  and substantially outperform state-of-the-art on PITFALL!.

7 Conclusion

In this paper  we propose a method of guiding agent exploration through hard exploration challenges
by watching YouTube videos. Unlike traditional methods of imitation learning  where demonstrations
are generated under controlled conditions with access to action and reward sequences  YouTube
videos contain only unaligned and often noisy audio-visual sequences. We have proposed novel
self-supervised objectives that allow a domain-invariant representation to be learnt across videos  and
described a one-shot imitation mechanism for guiding agent exploration by embedding checkpoints
throughout this space. Combining these methods with a standard IMPALA agent  we demonstrate

2https://www.youtube.com/playlist?list=PLZuOGGtntKlaOoq_8wk5aKgE_u_Qcpqhu

8

MONTEZUMA’S REVENGE

PITFALL!

PRIVATE EYE

Figure 8: Learning curves of our combined TDC+CMC algorithm with (purple) and without (yellow)
environment reward  versus imitation from pixel-space features (blue) and IMPALA without demon-
strations (green). The red line represents the maximum reward achieved using previously published
methods  and the brown line denotes the score obtained by an average human player.

MONTEZUMA’S REVENGE

PITFALL!

PRIVATE EYE

Rainbow [19]
ApeX [22]
DQfD [20]
Average Human [43]
Ours (rimitation only)
Ours (rimitation + renv)

384.0
2 500.0
4 659.7
4 743.0
37 232.7
58 175.1

0.0
-0.6
57.3

6 464.0
54 912.4
76 812.5

4 234.0

49.8

42 457.2
69 571.0
98 212.5
98 763.2

Table 1: Comparison of our best policy (mean of 200 evaluation episodes) to previously published
results across MONTEZUMA’S REVENGE  PITFALL! and PRIVATE EYE. Our agent is the ﬁrst to
exceed average human-level performance on all three games  even without environment rewards.

the ﬁrst human-level performance in the infamously difﬁcult exploration games MONTEZUMA’S
REVENGE  PITFALL! and PRIVATE EYE.

Acknowledgments We would like to thank the team  especially Serkan Cabi  Bilal Piot and Tobias
Pohlen  for many fruitful discussions. We thank the reviewers for their comments  which helped
in making this a better paper. And ﬁnally  we say ’thank you’ to all the amazing Atari players on
Youtube and Twitch  which inspired this project.

References
[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceed-

ings of the twenty-ﬁrst international conference on Machine learning  page 1. ACM  2004.

[2] Theodore Wilbur Anderson. An introduction to multivariate statistical analysis  volume 2. Wiley New

York  1958.

[3] Relja Arandjelovic and Andrew Zisserman. Look  listen and learn. In 2017 IEEE International Conference

on Computer Vision (ICCV)  pages 609–617. IEEE  2017.

[4] Brenna D Argall  Sonia Chernova  Manuela Veloso  and Brett Browning. A survey of robot learning from

demonstration. Robotics and autonomous systems  57(5):469–483  2009.

[5] Yusuf Aytar  Lluis Castrejon  Carl Vondrick  Hamed Pirsiavash  and Antonio Torralba. Cross-modal scene

networks. IEEE transactions on pattern analysis and machine intelligence  2017.

[6] Yusuf Aytar  Carl Vondrick  and Antonio Torralba. See  hear  and read: Deep aligned representations.

arXiv preprint arXiv:1706.00932  2017.

[7] Gabriel Barth-Maron  Matthew W Hoffman  David Budden  Will Dabney  Dan Horgan  Alistair Muldal 
Nicolas Heess  and Timothy Lillicrap. Distributed distributional deterministic policy gradients. Interna-
tional Conference on Learning Representations (ICLR)  2018.

9

[8] Marc Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing
Systems  pages 1471–1479  2016.

[9] Marc G Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement learning.

International Conference on Machine Learning (ICML)  2017.

[10] Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning environment:
An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47:253–279  2013.

[11] Carl Doersch  Abhinav Gupta  and Alexei A Efros. Unsupervised visual representation learning by context
prediction. In Proceedings of the IEEE International Conference on Computer Vision  pages 1422–1430 
2015.

[12] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In The IEEE International

Conference on Computer Vision (ICCV)  2017.

[13] Yan Duan  Marcin Andrychowicz  Bradly Stadie  OpenAI Jonathan Ho  Jonas Schneider  Ilya Sutskever 
Pieter Abbeel  and Wojciech Zaremba. One-shot imitation learning. In Advances in neural information
processing systems  pages 1087–1098  2017.

[14] Lasse Espeholt  Hubert Soyer  Rémi Munos  Karen Simonyan  Volodymyr Mnih  Tom Ward  Yotam
Doron  Vlad Firoiu  Tim Harley  Iain Dunning  Shane Legg  and Koray Kavukcuoglu. IMPALA: scalable
distributed deep-rl with importance weighted actor-learner architectures. CoRR  abs/1802.01561  2018.

[15] Chelsea Finn  Tianhe Yu  Tianhao Zhang  Pieter Abbeel  and Sergey Levine. One-shot visual imitation

learning via meta-learning. arXiv preprint arXiv:1709.04905  2017.

[16] Yaroslav Ganin  Evgeniya Ustinova  Hana Ajakan  Pascal Germain  Hugo Larochelle  François Laviolette 
Mario Marchand  and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of
Machine Learning Research  17(1):2096–2030  2016.

[17] Audrunas Gruslys  Mohammad Gheshlaghi Azar  Marc G Bellemare  and Remi Munos. The reactor: A
sample-efﬁcient actor-critic architecture. International Conference on Learning Representations (ICLR) 
2017.

[18] Dylan Hadﬁeld-Menell  Smitha Milli  Pieter Abbeel  Stuart J Russell  and Anca Dragan. Inverse reward

design. In Advances in Neural Information Processing Systems  pages 6768–6777  2017.

[19] Matteo Hessel  Joseph Modayil  Hado Van Hasselt  Tom Schaul  Georg Ostrovski  Will Dabney  Dan
Horgan  Bilal Piot  Mohammad Azar  and David Silver. Rainbow: Combining improvements in deep
reinforcement learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence  2017.

[20] Todd Hester  Matej Vecerik  Olivier Pietquin  Marc Lanctot  Tom Schaul  Bilal Piot  Dan Horgan  John
Quan  Andrew Sendonaris  Gabriel Dulac-Arnold  et al. Deep q-learning from demonstrations. Proceedings
of the AAAI Conference on Artiﬁcial Intelligence  2017.

[21] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning.

Information Processing Systems  pages 4565–4573  2016.

In Advances in Neural

[22] Dan Horgan  John Quan  David Budden  Gabriel Barth-Maron  Matteo Hessel  Hado van Hasselt  and David
Silver. Distributed prioritized experience replay. International Conference on Learning Representations
(ICLR)  2018.

[23] Ionel-Alexandru Hosu and Traian Rebedea. Playing atari games with deep reinforcement learning and

human checkpoint replay. CoRR  abs/1607.05077  2016.

[24] Yuxuan Liu  Abhishek Gupta  Pieter Abbeel  and Sergey Levine. Imitation from observation: Learning to

imitate behaviors from raw video via context translation. arXiv preprint arXiv:1707.03374  2017.

[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[26] Aaron van den Oord  Yazhe Li  and Oriol Vinyals. Representation learning with contrastive predictive

coding. arXiv preprint arXiv:1807.03748  2018.

[27] Ian Osband  Daniel Russo  Zheng Wen  and Benjamin Van Roy. Deep exploration via randomized value

functions. arXiv preprint arXiv:1703.07608  2017.

10

[28] Andrew Owens  Jiajun Wu  Josh H McDermott  William T Freeman  and Antonio Torralba. Ambient sound
provides supervision for visual learning. In European Conference on Computer Vision  pages 801–816.
Springer  2016.

[29] Tom Le Paine  Sergio Gómez Colmenarejo  Ziyu Wang  Scott Reed  Yusuf Aytar  Tobias Pfaff  Matt W
Hoffman  Gabriel Barth-Maron  Serkan Cabi  David Budden  et al. One-shot high-ﬁdelity imitation:
Training large-scale deep nets with rl. arXiv preprint arXiv:1810.05017  2018.

[30] Deepak Pathak  Pulkit Agrawal  Alexei A Efros  and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning (ICML)  volume 2017  2017.

[31] Deepak Pathak  Parsa Mahmoudieh  Guanghao Luo  Pulkit Agrawal  Dian Chen  Yide Shentu  Evan
Shelhamer  Jitendra Malik  Alexei A Efros  and Trevor Darrell. Zero-shot visual imitation. arXiv preprint
arXiv:1804.08606  2018.

[32] Tobias Pohlen  Bilal Piot  Todd Hester  Mohammad Gheshlaghi Azar  Dan Horgan  David Budden  Gabriel
Barth-Maron  Hado van Hasselt  John Quan  Mel Veˇcerík  et al. Observe and look further: Achieving
consistent performance on atari. arXiv preprint arXiv:1805.11593  2018.

[33] Nikolay Savinov  Alexey Dosovitskiy  and Vladlen Koltun. Semi-parametric topological memory for

navigation. arXiv preprint arXiv:1803.00653  2018.

[34] Pierre Sermanet  Corey Lynch  Jasmine Hsu  and Sergey Levine. Time-contrastive networks: Self-

supervised learning from multi-view observation. arXiv preprint arXiv:1704.06888  2017.

[35] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van Den Driessche  Julian
Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  et al. Mastering the game of go
with deep neural networks and tree search. Nature  529(7587):484–489  2016.

[36] Satinder Singh  Richard L Lewis  and Andrew G Barto. Where do rewards come from. In Proceedings of

the annual conference of the cognitive science society  pages 2601–2606  2009.

[37] Bradly C Stadie  Pieter Abbeel  and Ilya Sutskever. Third-person imitation learning. arXiv preprint

arXiv:1703.01703  2017.

[38] Faraz Torabi  Garrett Warnell  and Peter Stone. Behavioral cloning from observation. arXiv preprint

arXiv:1805.01954  2018.

[39] George Trigeorgis  Mihalis A Nicolaou  Björn W Schuller  and Stefanos Zafeiriou. Deep canonical time
warping for simultaneous alignment and representation learning of sequences. IEEE transactions on
pattern analysis and machine intelligence  40(5):1128–1138  2018.

[40] Eric Tzeng  Judy Hoffman  Ning Zhang  Kate Saenko  and Trevor Darrell. Deep domain confusion:

Maximizing for domain invariance. arXiv preprint arXiv:1412.3474  2014.

[41] Matej Veˇcerík  Todd Hester  Jonathan Scholz  Fumin Wang  Olivier Pietquin  Bilal Piot  Nicolas Heess 
Thomas Rothörl  Thomas Lampe  and Martin Riedmiller. Leveraging demonstrations for deep reinforce-
ment learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817  2017.

[42] Carl Vondrick  Hamed Pirsiavash  and Antonio Torralba. Anticipating the future by watching unlabeled

video. arXiv preprint arXiv:1504.08023  2015.

[43] Ziyu Wang  Tom Schaul  Matteo Hessel  Hado Van Hasselt  Marc Lanctot  and Nando De Freitas. Dueling
network architectures for deep reinforcement learning. International Conference on Machine Learning
(ICML)  2015.

[44] Tianhe Yu  Chelsea Finn  Annie Xie  Sudeep Dasari  Tianhao Zhang  Pieter Abbeel  and Sergey
Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint
arXiv:1802.01557  2018.

[45] Richard Zhang  Phillip Isola  and Alexei A Efros. Colorful image colorization. In European Conference

on Computer Vision  pages 649–666. Springer  2016.

[46] Bolei Zhou  Aditya Khosla  Agata Lapedriza  Aude Oliva  and Antonio Torralba. Object detectors emerge

in deep scene cnns. arXiv preprint arXiv:1412.6856  2014.

[47] Tinghui Zhou  Philipp Krahenbuhl  Mathieu Aubry  Qixing Huang  and Alexei A Efros. Learning dense
correspondence via 3d-guided cycle consistency. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 117–126  2016.

11

[48] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image translation using

cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593  2017.

[49] Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy inverse

reinforcement learning. In AAAI  volume 8  pages 1433–1438. Chicago  IL  USA  2008.

12

,Yusuf Aytar
Tobias Pfaff
David Budden
Thomas Paine
Ziyu Wang
Nando de Freitas