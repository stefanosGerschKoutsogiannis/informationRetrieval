2012,Learning with Partially Absorbing Random Walks,We propose a novel stochastic process that is with probability $\alpha_i$ being absorbed at current state $i$  and with probability $1-\alpha_i$ follows a random edge out of it. We analyze its properties and show its potential for exploring graph structures. We prove that under proper absorption rates  a random walk starting from a set $\mathcal{S}$ of low conductance will be mostly absorbed in $\mathcal{S}$. Moreover  the absorption probabilities vary slowly inside $\mathcal{S}$  while dropping sharply outside $\mathcal{S}$  thus implementing the desirable cluster assumption for graph-based learning. Remarkably  the partially absorbing process unifies many popular models arising in a variety of contexts  provides new insights into them  and makes it possible for transferring findings from one paradigm to another. Simulation results demonstrate its promising applications in graph-based learning.,Learning with Partially Absorbing Random Walks

Xiao-Ming Wu1  Zhenguo Li1  Anthony Man-Cho So3  John Wright1 and Shih-Fu Chang1 2

1Department of Electrical Engineering  Columbia University

2Department of Computer Science  Columbia University

3Department of SEEM  The Chinese University of Hong Kong

{xmwu  zgli  johnwright  sfchang}@ee.columbia.edu  manchoso@se.cuhk.edu.hk

Abstract

We propose a novel stochastic process that is with probability αi being absorbed
at current state i  and with probability 1 − αi follows a random edge out of it.
We analyze its properties and show its potential for exploring graph structures.
We prove that under proper absorption rates  a random walk starting from a set
S of low conductance will be mostly absorbed in S. Moreover  the absorption
probabilities vary slowly inside S  while dropping sharply outside  thus imple-
menting the desirable cluster assumption for graph-based learning. Remarkably 
the partially absorbing process uniﬁes many popular models arising in a variety
of contexts  provides new insights into them  and makes it possible for transfer-
ring ﬁndings from one paradigm to another. Simulation results demonstrate its
promising applications in retrieval and classiﬁcation.

1 Introduction

Random walks have been widely used for graph-based learning  leading to a variety of models in-
cluding PageRank [14] for web page ranking  hitting and commute times [8] for similarity measure
between vertices  harmonic functions [20] for semi-supervised learning  diffusion maps [7] for di-
mensionality reduction  and normalized cuts [12] for clustering. In graph-based learning one often
adopts the cluster assumption  which states that the semantics usually vary smoothly for vertices
within regions of high density [17]  and suggests to place the prediction boundary in regions of
low density [5]. It is thus interesting to ask how the cluster assumption can be realized in terms of
random walks.

Although a random walk appears to explore the graph globally  it converges to a stationary distribu-
tion determined solely by vertex degrees regardless of the starting points  a phenomenon well known
as the mixing of random walks [11]. This causes some random walk approaches intended to capture
non-local graph structures to fail  especially when the underlying graph is well connected  i.e.  the
random walk has a large mixing rate. For example  it was recently proven in [16] that under some
mild conditions the hitting and commute times on large graphs do not take into account the global
structure of the graph at all  despite the fact that they have integrated all the relevant paths on the
graph. It is also shown in [13] that the “harmonic” walks [20] in high-dimensional spaces converge
to a constant distribution as the data size approaches inﬁnity  which is undesirable for classiﬁcation
and regression. These ﬁndings show that intuitions regarding random walks can sometimes be mis-
leading  and should be taken with caution. A natural question is: can we design a random walk
which implements the cluster assumption with some guarantees?

In this paper  we propose partially absorbing random walks (PARWs)  a novel random walk mod-
el whose properties can be analyzed theoretically. In PARWs  a random walk is with probability
αi being absorbed at current state i  and with probability 1 − αi follows a random edge out of it.
PARWs are guaranteed to implement the cluster assumption in the sense that under proper absorp-

1

kkp

k

(b)

iip

i

jjp

j

'i

i

'k

k

(c)

'j

j

t =

0

1t =
(a)

t =

2

Figure 1: A partially absorbing random walk. (a) A ﬂow perspective (see text). (b) A second-order
Markov chain. (c) An equivalent standard Markov chain with additional sinks.

tion rates  a random walk starting from a set S of low conductance will be mostly absorbed in S.
Furthermore  we show that by setting the absorption rates  the absorption probabilities can vary s-
lowly inside S  while dropping sharply outside S. This approximately piecewise constant property
makes PARWs highly desirable and robust for a variety of learning tasks including ranking  clus-
tering  and classiﬁcation  as demonstrated in Section 4. More interestingly  it turns out that many
existing models including PageRank  hitting and commute times  and label propagation algorithms
in semi-supervised learning  can be uniﬁed or related in PARWs  which brings at least two beneﬁts.
On one hand  our theoretical analysis sheds some light on the understanding of existing models; on
the other hand  it enables transferring ﬁndings among different paradigms. We present our model in
Section 2  analyze a special case of it in Section 3  and show simulation results in Section 4. Section
5 concludes the paper. Most of our proofs are included in supplementary material.

2 Partially Absorbing Random Walks

Let us consider a simple diffusion process illustrated in Fig. 1(a). At the beginning  a unit ﬂow (blue)
is injected to the graph at a selected vertex. After one step  some of the ﬂow (red) is “stored” at the
vertex while the rest (blue) propagates to its neighbors. Whenever the ﬂow passes a vertex  some
fraction of it is retained at that vertex. As this process continues  the amount of ﬂow stored in each
vertex will accumulate and there will be less and less ﬂow left running on the graph. After a certain
number of steps  there will be almost no ﬂow left running and the ﬂow stored will nearly sum up to
1. The above diffusion process can be made precise in terms of random walks  as shown below.

Consider a discrete-time stochastic process X = {Xt : t ≥ 0} on the state space N = {1  2  . . .   n} 
where the initial state X0 is given  say X0 = i  the next state X1 is determined by the transition
probability P(X1 = j|X0 = i) = pij  and the subsequent states are determined by the transition
probabilities

P(Xt+2 = j|Xt+1 = i  Xt = k) =( 1 

i = j  i = k 
i 6= j  i = k 

0 
P(Xt+2 = j|Xt+1 = i) = pij 

(1)

i 6= k 

where t ≥ 0. Note that the process X is time homogeneous  i.e.  the transition probabilities in (1)
are independent of t. In other words  if the previous and current states are the same  the process
will remain in the current state forever. Otherwise  the next state is conditionally independent of the
previous state given the current state  i.e.  the process behaves like a usual random walk.

To illustrate the above construction  consider Fig. 1(b). Starting from state i  there is some probabil-
ity pii that the process will stay at i in the next step; and once it stays  the process will be absorbed
into state i. Hence  we shall call the above process a partially absorbing random walk (PARW) 
where pii is the absorption rate of state i. If 0 < pii < 1  then we say that i is a partially absorbing
state. If pii = 1  then we say that i is a fully absorbing state. Finally  if pii = 0  then we say that i
is a transient state. Note that if pii ∈ {0  1} for every state i ∈ N   then the above process reduces to
a standard Markov chain [9].

A PARW is a second-order Markov chain completely speciﬁed by its ﬁrst order transition probabil-
ities {pij}. One can observe that any PARW can be realized as a standard Markov chain by adding
a sink (fully absorbing state) to each vertex in the graph  as illustrated in Fig. 1(c). The transition

2

probability from i to its sink i′ equals the absorption rate pii in PARWs. One may also notice that
the construction of PARWs can be generalized to the m-th order  i.e.  the process is absorbed at a
state only after it has stayed at that state for m-consecutive steps. However  it can be shown that any
m-th order PARW can be realized by a second-order PARW. We will not elaborate on this due to
space constraints.

2.1 PARWs on Graphs

Let G = (V  W ) be an undirected weighted graph  where V is a set of n vertices and W = [wij ] ∈
Rn×n is a symmetric non-negative matrix of pairwise afﬁnities among vertices. We assume G is

connected. Let D = diag(d1  d2  . . .   dn) with di = Pj wij as the degree of vertex i  and deﬁne
the Laplacian of G by L = D − W [6]. Denote by d(S) :=Pi∈S di the volume of a subset S ⊆ V

of vertices. Let λ1  λ2  . . .   λn ≥ 0 be arbitrary  and set Λ = diag(λ1  λ2  . . .   λn). Suppose that
we deﬁne the ﬁrst order transition probabilities of a PARW by

pij =( λi

wij

λi+di

λi+di

 

 

i = j 

i 6= j.

(2)

Then  we see that state i is an absorbing state (either partially or fully) when λi > 0  and is a transient
state when λi = 0. In particular  the matrix Λ acts like a regularizer that controls the absorption rate
of each state  i.e.  the larger λi  the larger pii. In the sequel  we refer to Λ as the regularizer matrix.
Absorption Probabilities. We are interested in the probability aij that a random walk starting from
state i  is absorbed at state j in any ﬁnite number of steps. Let A = [aij ] ∈ Rn×n be the matrix of
absorption probabilities. The following theorem shows that A has a closed-form.
Theorem 2.1. Suppose λi > 0 for some i. Then A = (Λ + L)−1Λ.

Proof. Since λi > 0 for some i  the matrix Λ + L is positive deﬁnite and hence non-singular.
Moreover  the matrix Λ + D is non-singular  since D is non-singular. Thus  the matrix I − (Λ +
D)−1W = (Λ + D)−1(Λ + L) is also non-singular. Now  observe that the absorbing probabilities
{aij} satisfy the following equations:

aii =

λi

λi + di

aij = Xk6=i

wik

λi + di

wij

λi + di

aji 

× 1 +Xj6=i

akj  i 6= j.

(3)

(4)

Upon writing equations (3) and (4) in matrix form  we have (I − (Λ + D)−1W )A = (Λ + D)−1Λ 
whence A = (I − (Λ + D)−1W )−1(Λ + D)−1Λ = (Λ + D − W )−1Λ = (Λ + L)−1Λ.

The following result conﬁrms that A is indeed a probability matrix.
Proposition 2.1. Suppose λi > 0 for some i. Then A is a non-negative matrix with each row
summing up to 1.

By Proposition 2.1 Pk ajk = 1 for any j. This means that a PARW starting from any vertex will

eventually be absorbed  provided that there is at least one absorbing state in the state space.

2.2 Limits of Absorption Probabilities

By Theorem 2.1  we see that the absorption probabilities (A) are governed by both the structure
of the graph (L) and the regularizer matrix (Λ). It would be interesting to see how A varies with
Λ  particularly when λi’s become small which allows the ﬂow to propagate sufﬁciently (Fig. 1(a)).
The following result shows that as Λ (λi’s) vanishes  each row of A converges to a distribution
proportional to (λ1  λ2  . . .   λn)  regardless of graph structure.
Theorem 2.2. Suppose λi > 0 for all i. Then

⊤
(αΛ + L)−1αΛ = 1¯λ

 

lim
α→0+

(5)

where (¯λ)i = λi/(Pn

j=1 λj). In particular  limα→0+(αI + L)−1αI = 1

n

11⊤.

3

Theorem 2.2 tells us that with Λ = αI and as α → 0 a PARW will converge to the constant
distribution 1/n  regardless of the starting vertex. At ﬁrst glance  this limit seems meaningless.
However  the following lemma will show that it actually has interesting connections with L+  the
pseudo-inverse of the graph Laplacian  a matrix that is widely studied and proven useful for many
learning tasks including recommendation and clustering [8].
Proposition 2.2. Suppose Λ = αI and denote Aα := (Λ + L)−1Λ = (αI + L)−1α. Then 

Aα − 1
n
α

lim
α→0

11⊤

= L+.

(6)

Proposition 2.2 gives a novel probabilistic interpretation of L+. Note that by Theorem 2.2  A0 :=
limα→0 Aα = 1
11⊤. Thus L+ is the derivative of Aα w.r.t. α at α = 0  implying that L+ reﬂects
n
the variation of absorption probabilities when the absorption rate is very small. By (6)  we see that
ranking by L+ is essentially the same as ranking by Aα  when α is sufﬁciently small.

2.3 Relations with Popular Ranking and Classiﬁcation Models

Relations with PageRank Vectors. Suppose λj > 0 for all j. Let a be the absorption probability
vector of a PARW starting from vertex i. Denote by s the indicator vector of i  i.e.  s(i) = 1 and
s(j) = 0 for j 6= i. Then a⊤ = s⊤(Λ + L)−1Λ  which can be rewritten as

a⊤ = s⊤(Λ + D)−1Λ + a⊤Λ−1W (Λ + D)−1Λ.

(7)

By letting Λ = β
1−β D  we have a⊤ = βs⊤ + (1 − β)a⊤D−1W  which is exactly the equilibrium
equation for personalized PageRank [14]. Note that β is often referred to as the “teleportation”
probability in PageRank. This shows that personalized PageRank is a special case of PARWs with
absorption rates pii = λi
Relations with Hitting and Commute Times. The hitting time Hij is the expected time that it
takes a random walk starting from i to ﬁrst arrive at j  and the commute time Cij is the expected
time it takes a random walk starting from i to travel to j and back to i  which can be computed as

= β.

λi+di

Hij = d(G)(L+

jj − L+

ij)  Cij = Hij + Hji = d(G)(L+

ii + L+

jj − 2L+

ij) 

(8)

ii + Aα

jj − Aα

jj − 2Aα

where d(G) := Pi di denotes the volume of the graph. By (6)  when Λ = αI and α is sufﬁciently

ij or
small  ranking with Hij or Cij (say  with respect to i) is the same as ranking by Aα
ij respectively. This appears to be not particularly meaningful because the term Aα
Aα
jj
is the self-absorption probability that does not contain any essential information with the starting
vertex i. Accordingly  it should not be included as part of the ranking function with respect to i.
This argument is also supported in a recent study by [16]  where the hitting and commute times are
shown to be dominated by the inverse of degrees of vertices. In other words  they do not take into
account the graph structure at all. A remedy they propose is to throw away the diagonal terms of
L+ and only use the off-diagonal terms. This happens to suggest using absorption probabilities for
ranking and as similarity measure  because when α is sufﬁciently small  ranking by the off-diagonal
ij  i.e.  the absorption probability of starting
terms of L+ is essentially the same as ranking by Aα
from i and being absorbed at j. Our theoretical analysis in Section 3 and the simulation results in
Section 4 further conﬁrm this argument.
Relations with Semi-supervised Learning. Interestingly  many label propagation algorithms in
semi-supervised learning can be cast in PARWs. The harmonic function method [20] is a PARW
when setting λi = ∞ (absorption rate 1) for the labeled vertices while λi = 0 (absorption rate 0) for
the unlabeled. In [19] the authors have made this interpretation in terms of absorbing random walks 
where a random walk arriving at an absorbing state will stay there forever. PARWs can be viewed
as an extension of absorbing random walks. The regularized harmonic function method [5] is also a
PARW when setting λi = α for the labeled vertices while λi = 0 for the unlabeled. The consistency
method [17]  if using un-normalized Laplacian instead of normalized Laplacian  is a PARW with
Λ = αI. Our analysis in this paper reveals several nice properties of this case (Section 3). A variant
of this method is a PARW with Λ = αD  which is the same as PageRank as shown above. If we
add an additional sink to the graph  a variant of harmonic function method [10] and a variant of the
regularized harmonic function method [3] can all be included as instances of PARWs. We omit the
details here due to space constraints.

4

Beneﬁts of a Unifying View. We have shown that PARWs can unify or relate many models from
different contexts. This brings at least two beneﬁts. First  it sheds some light on existing models. For
instance  hitting and commute times are not suitable for ranking given its interpretation in absorption
probabilities  as discussed above. In the next section  we will show that a special case of PARWs is
better suited for implementing the cluster assumption for graph-based learning. Second  a unifying
view builds bridges between different paradigms thus making it easier to transfer ﬁndings between
them. For example  it has been shown in [2  4] that approximate personalized PageRank vectors can
be computed in O(1/ǫ) iterations  where ǫ is a precision tolerance parameter. We indicate here that
such a technique is also applicable to PARWs due to PARWs’s generalizing nature. Consequently 
most models included in PARWs can be substantially accelerated using the same technique.

3 PARWs with Graph Conductance

In this section  we present results on the properties of the absorption probability vector ai obtained
by a PARW starting from vertex i (i.e.  a⊤
is the row i of A). We show that properties of ai
i
relate closely to the connectivity between i and the rest of graph  which can be captured by the
conductance of the cluster S where i belongs. We also ﬁnd that properties of ai depend on the
setting of absorption rates. Our key results can be summarized as follows. In general  the probability
mass of ai is mostly absorbed by S. Under proper absorption rates  ai can vary slowly within S
while dropping sharply outside S. Such properties are highly desirable for learning tasks such as
ranking  clustering  and classiﬁcation.

The conductance of a subset S ⊂ V of vertices is deﬁned as Φ(S) =

w(S  ¯S)

min(d(S) d( ¯S))

  where

w(S  ¯S) := P(i j)∈e(S  ¯S) wij is the cut between S and its complement ¯S [6]. We denote the

indicator vector of S by χS such that χS (i) = 1 if i ∈ S and χS(i) = 0 otherwise; and denote
the stationary distribution w.r.t. S by πS such that πS(i) = di/d(S) if i ∈ S and πS (i) = 0
otherwise. In terms of the conductance of S  the following theorem gives an upper bound on the
expected probability mass escaped from S if the distribution of the starting vertex is πS.
Theorem 3.1. Let S be any set of vertices satisfying d(S) ≤ 1
γ2 = maxi∈ ¯S

2 d(G). Let γ1 = mini∈S

. Then 

λi
di

and

λi
di

π⊤

S Aχ ¯S ≤

γ2

1 + γ1

1 + γ2

γ2
1

Φ(S).

(9)

Theorem 3.1 shows that most of the probability mass will be absorbed in S  provided that S is of
small conductance and the random walk starts from S according to πS. In other words  a PARW
will be trapped inside the cluster1 from where it starts  as desired. To identify the entire cluster  what
is more desirable would be that the absorption probabilities vary slowly within the cluster while
dropping sharply outside. As such  the cluster can be identiﬁed by detecting the sharp drop. We
show below that such property can be achieved by setting appropriate absorption rates at vertices.

3.1 PARWs with Λ = αI

We will prove that the choice of Λ = αI can fulﬁll the above goal. Before presenting theoretical
analysis  let us discuss the intuition behind it from both ﬂow (Fig. 1(a)) and random walk perspec-
tives. To vary slowly within the cluster  the ﬂow needs to be distributed evenly within it; while to
drop sharply outside  the ﬂow must be prevented from escaping. This means that the absorption
rates should be small in the interior but large near the boundary area of the cluster. Setting Λ = αI
achieves this. It corresponds to the absorption rates pii = λi
  which decrease monoton-
ically with di. Since the degrees of vertices are usually relatively large in the interior of the cluster
due to denser connections  and small near its boundary area (Fig. 2(a))  the absorption rates are
therefore much larger at its boundary than in its interior (Fig. 2(b)). State differently  a random walk
may move freely inside the cluster  but it will get absorbed with high probability when traveling
near the cluster’s boundary. In this way  the absorption rates set up a bounding “wall” around the
cluster to prevent the random walk from escaping  leading to an absorption probability vector that

= α

λi+di

α+di

1A cluster is understood as a subset of vertices of small conductance.

5

4x 10−3

4x 10−3

2

0
0

2

0
0

300

600

900

(c)

300

600

900

(d)

(a)

(b)

Figure 2: Absorption rates and absorption probabilities. (a) A data set of three Gaussians with the
degrees of vertices in the underlying graph shown (see Section 4 for the descriptions of the data
and graph construction). A starting vertex is denoted in black circle. (b–c) Absorption rates and
absorption probabilities for Λ = αI (α = 10−3). (d) Sorted absorption probabilities of (c). For
illustration purpose  in (a–b)  the degrees of vertices and the absorption rates have been properly
scaled  and in (c)  the data are arranged such that points within each Gaussian appear consecutively.

varies slowly within the cluster while dropping sharply outside (Figs. 2(c–d))  thus implementing
the cluster assumption. We make these arguments precise below.

It is worth pointing out that a PARW with Λ = αI is symmetric  i.e.  the absorption probability of
starting from i and absorbed at j is equal to the probability of starting from j and absorbed at i. For
simplicity  we use the abbreviated notation a to denote ai  the absorption probability vector for the
PARW starting from vertex i. By (3) and the symmetry property  we immediately see that a has the
following “harmonic” property:

a(i) =

λi

λi + di

wik

λi + di

a(k) 

+Xk6=i

a(j) =Xk6=j

wjk

λj + dj

a(k)  j 6= i.

(10)

We will use this property to prove some interesting results. Another desirable property one should
notice for this PARW is that the starting vertex always has the largest absorption probability  as
shown by the following lemma.
Lemma 3.2. Given Λ = αI  then aii > aij for any i 6= j.

By Lemma 3.2 and without loss of generality  we assume the vertices are sorted so that a(1) >
a(2) ≥ · · · ≥ a(n)  where vertex 1 is the starting vertex. Let Sk be the set of vertices {1  . . .   k}.
Denote e(Si  Sj ) as the set of edges between Si and Sj.
The following theorem quantiﬁes the drop of the absorption probabilities between Sk and ¯Sk.
Theorem 3.3. For every S ∈ {Sk | k = 1  2  . . .   n} 

X(u v)∈e(S  ¯S)

wuv (a(u) − a(v)) = α 1 −Xk∈S

a(k)! .

(11)

Theorem 3.3 shows that the weighted difference in absorption probabilities between Sk and ¯Sk is

α(cid:16)1 −Pk

j=1

a(j)(cid:17)  implying that it drops slowly when α is small and as k increases  as expected.

Next we show the variation of absorption probabilities with graph conductance. Without loss of
generality  we consider sets Sj where d(Sj) ≤ 1
The following theorem says that a(j +1) will drop little from a(j) if the set Sj has high conductance
or if the vertex j is far away from the starting vertex 1 (i.e.  j ≫ 1).
Lemma 3.4. If Φ(Sj ) = φ  then

2 d(G).

a(j + 1) ≥ a(j) −

α(cid:16)1 −Pj

k=1
φd(Sj )

a(k)(cid:17)

.

(12)

The above result can be extended to describe the drop in a much longer range  as stated in the
following theorem.

6

0.4

0.3

0.2

0.1

0
0

0.8

0.6

0.4

0.2

0
0

0.01

0.005

300

600

900

0
0

300

600

900

(a)

(b)

2x 10−3

1

0
0

1.12x 10−3

1.1

1.08

300

600
(c)

900

1.06
0

300

600

900

(d)

0.03

0.02

0.01

0
0

6x 10−3

3x 10−3

4

2

0
0

300

600

900

(g)

2

1

0
0

300

600
(h)

900

300

600
(i)

900

300

600

900

(f)

1.1114x 10−3

1.1112

1.111

1.1108
0

3x 10−3

2

1

0
0

300

600

900

(e)

300

600

900

(j)

Figure 3: Absorption probabilities on the three Gaussians in Fig. 2(a) with the starting vertex
denoted in black circle.
(a–e) Λ = αI  α = 100  10−2  10−4  10−6  10−8; (f–j) Λ = αD 
α = 100  10−2  10−4  10−6  10−8. For illustration purpose  the data are arranged such that points
within each Gaussian appear consecutively  as in Fig. 2(c).

Table 1: Ranking results (MAP) on USPS

Digits
Λ = αI
PageRank

Manifold Ranking
Euclidean Distance

0

.981
.886
.957
.640

1

.988
.972
.987
.980

2

.876
.608
.827
.318

3

.893
.764
.827
.499

4

.646
.488
.467
.337

5

.778
.568
.630
.294

6

.940
.837
.917
.548

7

.919
.825
.822
.620

8

.746
.626
.675
.368

9

.730
.702
.719
.480

All
.850
.728
.783
.508

Theorem 3.5. If Φ(Sj) ≥ 2φ  then there exists a k > j such that

d(Sk) ≥ (1 + φ)d(Sj )

and

a(k) ≥ a(j) −

α(cid:16)1 −Pj

k=1
φd(Sj )

a(k)(cid:17)

.

Theorem 3.5 tells us that if the set Sj has high conductance  then there will be a set Sk much larger
than Sj where the absorption probability a(k) remains large. In other words  a(k) will not drop
much if Sj is closely connected with the rest of graph. Combining Theorems 3.3  3.5  and 3.1  we
see that the absorption probability vector of the PARW with Λ = αI has the nice property of varying
slowly within the cluster while dropping sharply outside.

We remark that similar analyses have been conducted in [1  2] on personalized PageRank  for the
local clustering problem [15] whose goal is to ﬁnd a local cut of low conductance near a speciﬁed
starting vertex. As shown in Section 2  personalized PageRank is a special case of PARWs with
Λ = αD = β
1−β D  which corresponds to setting the same absorption rate pii = β at each vertex.
This setting does not take advantage of the cluster assumption. Indeed  despite the signiﬁcant cluster
structure in the three Gaussians (Fig. 2)  no clear drop emerges by varying β (Section 4). This
explains the “heuristic” used in [1  2] where the personalized PageRank vector is divided by the
degrees of vertices to generate a sharp drop. In contrast  our choice of Λ = αI appears to be more
justiﬁed  without the need of such post-processing while retaining a probabilistic foundation.

4 Simulation

In this section  we demonstrate our theoretical results on both synthetic and real data. For each data
set  a weighted k-NN graph is constructed with k = 20. The similarity between vertices i and j is
computed as wij = exp(−d2
ij/σ) if i is within j’s k nearest neighbors or vice versa  and wij = 0
otherwise (wii = 0)  where σ = 0.2 × r and r denotes the average square distance between each
point to its 20th nearest neighbor.
The ﬁrst experiment is to examine the absorption probabilities when varying absorption rates. We
use the synthetic three Gaussians in Fig. 2(a)  which consists of 900 points from three Gaussians 
with 300 in each. Fig. 3 compares the cases of Λ = αI and Λ = αD (PageRank). We can

7

Table 2: Classiﬁcation accuracy on USPS

HMN

LGC

Λ = αD

Λ = αI

.782 ± .068

.792 ± .062

.787 ± .048

.881 ± .039

draw several observations. For Λ = αI  when α is large  most probability mass is absorbed in
the cluster of the starting vertex (Fig. 3(a)). As it becomes appropriately small  the probability mass
distributes evenly within the cluster  and a sharp drop emerges (Fig. 3(b)). As α → 0  the probability
mass distributes more evenly within each cluster and also on the entire graph (Figs. 3(c–e))  but the
drops between clusters are still quite signiﬁcant. In contrast  for Λ = αD  no signiﬁcant drops
show for all α’s (Figs. 3(f–j)). This is due to the uniform absorption rates on the graph  which
makes the ﬂow favor vertices with denser connections (i.e.  of large degrees). These observations
support the theoretical arguments in Section 3 for PARWs with Λ = αI and suggest its robustness
in distinguishing between different clusters.

The second experiment is to test the potential of PARWs for information retrieval. We compare
PARWs with Λ = αI to PageRank (i.e.  PARWs with Λ = αD)  Manifold Ranking [18]  and
the baseline using Euclidean distance. For parameter selection  we use α = 10−6 for Λ = αI
and β = 0.15 for PageRank (see Section 2.3) as suggested in [14]. The regularization parameter
in Manifold Ranking is set to 0.99  following [18]. The image benchmark USPS2 is used for this
experiment  which contains 9298 images of handwritten digits from 0 to 9 of size 16 × 16  with
1553  1269  929  824  852  716  834  792  708  and 821 instances of each digit respectively. Each
instance is used as a query and the mean average precision (MAP) is reported. The results are shown
in Table 1. We see that the PARW with Λ = αI consistently gives best results for individual digits
as well as the entire data set.

In the last experiment  we test PARWs on classiﬁcation/semi-supervised learning  also on USPS
with all 9298 images. We randomly sample 20 instances as labeled data and make sure there is
at least one label for each class. For PARWs  we classify each unlabeled instance u to the class
of the labeled vertex v where u is most likely to be absorbed  i.e.  v = arg maxi∈L aui where L
denotes the labeled data and aui is the absorption probability. We compare PARWs with Λ = αI
(α = 10−6) and Λ = αD (β = 0.15) to the harmonic function method (HMN) [20] coupled
with class mass normalization (CMN) and the local and global consistency (LGC) method [17]. No
parameter in HMN is required  and the regularization parameter in LGC is set to 0.99 following [17].
The classiﬁcation accuracy averaged over 1000 runs is shown in Table 2. Again  it conﬁrms the
superior performance of the PARW with Λ = αI.
In the second and third experiments  we also tried other parameter settings for methods where ap-
propriate. We found that the performance of PARWs with Λ = αI is quite stable with small α  and
varying parameters in other methods did not lead to signiﬁcantly better results  which validates our
previous arguments.

5 Conclusions

We have presented partially absorbing random walks (PARWs)  a novel stochastic process generaliz-
ing ordinary random walks. Surprisingly  it has been shown to unify or relate many popular existing
models and provide new insights. Moreover  a new algorithm developed from PARWs has been
theoretically shown to be able to reveal cluster structure under the cluster assumption. Simulation
results have conﬁrmed our theoretical analysis and suggested its potential for a variety of learning
tasks including retrieval  clustering  and classiﬁcation. In future work  we plan to apply our model
to real applications.

Acknowledgements

This work is supported in part by Ofﬁce of Naval Research (ONR) grant #N00014-10-1-0242. The
authors would like to thank the anonymous reviewers for their insightful comments.

2http://www-stat.stanford.edu/ tibs/ElemStatLearn/

8

References

[1] R. Andersen and F. Chung. Detecting sharp drops in pagerank and a simpliﬁed local partition-

ing algorithm. Theory and Applications of Models of Computation  pages 1–12  2007.

[2] R. Andersen  F. Chung  and K. Lang. Local graph partitioning using pagerank vectors. In

FOCS  pages 475–486  2006.

[3] Y. Bengio  O. Delalleau  and N. Le Roux. Label propagation and quadratic criterion. Semi-

supervised learning  pages 193–216  2006.

[4] P. Berkhin. Bookmark-coloring algorithm for personalized pagerank computing.

Mathematics  3(1):41–62  2006.

Internet

[5] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In AIS-

TATS  2005.

[6] F. Chung. Spectral Graph Theory. American Mathematical Society  1997.
[7] R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic Analysis 

21(1):5–30  2006.

[8] F. Fouss  A. Pirotte  J. Renders  and M. Saerens. Random-walk computation of similarities be-
tween nodes of a graph with application to collaborative recommendation. IEEE Transactions
on Knowledge and Data Engineering  19(3):355–369  2007.

[9] J. Kemeny and J. Snell. Finite markov chains. Springer  1976.
[10] B. Kveton  M. Valko  A. Rahimi  and L. Huang. Semisupervised learning with max-margin

graph cuts. In AISTATS  pages 421–428  2010.

[11] L. Lov´asz and M. Simonovits. The mixing rate of markov chains  an isoperimetric inequality 

and computing the volume. In FOCS  pages 346–354  1990.

[12] M. Meila and J. Shi. A random walks view of spectral segmentation. In AISTATS  2001.
[13] B. Nadler  N. Srebro  and X. Zhou. Statistical analysis of semi-supervised learning: The limit

of inﬁnite unlabelled data. In NIPS  pages 1330–1338  2009.

[14] L. Page  S. Brin  R. Motwani  and T. Winograd. The pagerank citation ranking: Bringing order

to the web. 1999.

[15] D. A. Spielman and S.-H. Teng. A local clustering algorithm for massive graphs and its appli-

cation to nearly-linear time graph partitioning. CoRR  abs/0809.3232  2008.

[16] U. Von Luxburg  A. Radl  and M. Hein. Hitting and commute times in large graphs are often

misleading. Arxiv preprint arXiv:1003.1266  2010.

[17] D. Zhou  O. Bousquet  T. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global

consistency. In NIPS  pages 595–602  2004.

[18] D. Zhou  J. Weston  A. Gretton  O. Bousquet  and B. Sch¨olkopf. Ranking on data manifolds.

In NIPS  2004.

[19] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation.

Technical Report CMU-CALD-02-107  Carnegie Mellon University  2002.

[20] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and

harmonic functions. In ICML  2003.

9

,Zhen Xu
Wen Dong
Sargur Srihari
Zhe Wang
Kaiyi Ji
Yi Zhou
Yingbin Liang
Vahid Tarokh