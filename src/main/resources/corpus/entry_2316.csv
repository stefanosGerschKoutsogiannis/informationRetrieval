2018,Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere,Multichannel blind deconvolution is the problem of recovering an unknown signal $f$ and multiple unknown channels $x_i$ from convolutional measurements $y_i=x_i \circledast f$ ($i=1 2 \dots N$). We consider the case where the $x_i$'s are sparse  and convolution with $f$ is invertible. Our nonconvex optimization formulation solves for a filter $h$ on the unit sphere that produces sparse output $y_i\circledast h$. Under some technical assumptions  we show that all local minima of the objective function correspond to the inverse filter of $f$ up to an inherent sign and shift ambiguity  and all saddle points have strictly negative curvatures. This geometric structure allows successful recovery of $f$ and $x_i$ using a simple manifold gradient descent algorithm with random initialization. Our theoretical findings are complemented by numerical experiments  which demonstrate superior performance of the proposed approach over the previous methods.,Global Geometry of Multichannel

Sparse Blind Deconvolution on the Sphere

Yanjun Li

CSL and Department of ECE

University of Illinois
Urbana-Champaign

Yoram Bresler

CSL and Department of ECE

University of Illinois
Urbana-Champaign

yli145@illinois.edu

ybresler@illinois.edu

Abstract

Multichannel blind deconvolution is the problem of recovering an unknown signal
f and multiple unknown channels xi from convolutional measurements yi = xi(cid:126)f
(i = 1  2  . . .   N). We consider the case where the xi’s are sparse  and convolution
with f is invertible. Our nonconvex optimization formulation solves for a ﬁlter
h on the unit sphere that produces sparse output yi (cid:126) h. Under some technical
assumptions  we show that all local minima of the objective function correspond
to the inverse ﬁlter of f up to an inherent sign and shift ambiguity  and all saddle
points have strictly negative curvatures. This geometric structure allows successful
recovery of f and xi using a simple manifold gradient descent algorithm with
random initialization. Our theoretical ﬁndings are complemented by numerical
experiments  which demonstrate superior performance of the proposed approach
over the previous methods.

1

Introduction

Blind deconvolution  which aims to recover unknown vectors x and f from their convolution y =
x (cid:126) f  has been extensively studied  especially in the context of image deblurring [1  2  3]. Recently 
algorithms with theoretical guarantees have been proposed for single channel blind deconvolution
[4  5  6  7  8  9  10]. In order for the problem to be well-posed  these previous methods assume
that both x and f are constrained  to either reside in a known subspace or be sparse over a known
dictionary [11  12]. However  these methods cannot be applied if f (or x) is unconstrained  or does
not have a subspace or sparsity structure.
In many applications in communications [13]  imaging [14]  and computer vision [15]  convolutional
measurements yi = xi (cid:126) f are taken between a single signal (resp. ﬁlter) f and multiple ﬁlters (resp.
signals) {xi}N
i=1. We call such problems multichannel blind deconvolution (MBD). Importantly  in
this multichannel setting  one can assume that only {xi}N
i=1 are structured  and f is unconstrained.
While there has been abundant work on single channel blind deconvolution (with both f and x
constrained)  research on MBD (with f unconstrained) is relatively limited. Traditional MBD works
assumed that the channels xi’s are FIR ﬁlters [16  17  18] or IIR ﬁlters [19]  and proposed to solve
MBD using subspace methods. The problem is generally ill-conditioned  and the recovery using the
subspace methods is highly sensitive to noise [20].
In this paper  while retaining the unconstrained form of f  we consider a different structure of the
multiple channels {xi}N
i=1: sparsity. The resulting problem is termed multichannel sparse blind
deconvolution (MSBD). The sparsity structure arises in many real-world applications.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Opportunistic underwater acoustics: Underwater acoustic channels are sparse in nature [21].
Estimating such sparse channels with an array of receivers using opportunistic sources (e.g.  shipping
noise) involves a blind deconvolution problem with multiple unknown sparse channels [22  23].
Reﬂection seismology: Thanks to the layered earth structure  reﬂectivity in seismic signals is sparse.
It is of great interest to simultaneous recover the ﬁlter (also known as the wavelet)  and seismic
reﬂectivity along the multiple propagation paths between the source and the geophones [24].
Functional MRI: Neural activity signals are composed of brief spikes and are considered sparse.
However  observations via functional magnetic resonance imaging (fMRI) are distorted by convolving
with the hemodynamic response function. A blind deconvolution procedure can reveal the underlying
neural activity [25].
Super-resolution ﬂuorescence microscopy: In super-resolution ﬂuorescence microscopic imaging 
photoswitchable probes are activated stochastically to create multiple sparse images and allow
microscopy of nanoscale cellular structures [26  27]. One can further improve the resolution via a
computational deconvolution approach  which mitigates the effect of the point spread function (PSF)
of the microscope [28]. It is sometimes difﬁcult to obtain the PSF (e.g.  due to unknown aberrations) 
and one needs to jointly estimate the microscopic images and the PSF [29].
Previous approaches to MSBD have provided efﬁcient iterative algorithms to compute maximum
likelihood (ML) estimates of parametric models of the channels {xi}N
i=1 [23]  or maximum a
posteriori (MAP) estimates in various Bayesian frameworks [24  15]. However  these algorithms
usually do not have theoretical guarantees. Recently  guaranteed algorithms for MSBD have been
developed. Wang and Chi [30] proposed a convex formulation of MSBD based on (cid:96)1 minimization.
Li et al. [31] solved a nonconvex formulation using projected gradient descent  and proposed an
initialization algorithm to compute a sufﬁciently good starting point. However  the theoretical
guarantees of these algorithms require restrictive assumptions (e.g.  f has one dominant entry that is
signiﬁcantly larger than other entries [30]  or f has an approximately ﬂat spectrum [31]).
We would like to emphasize that  while earlier papers on MBD [16  17  18  19] consider a linear con-
volution model  more recent guaranteed methods for MSBD [30  31] consider a circular convolution
model. By zero padding the signal and the ﬁlter  one can rewrite a linear convolution as a circular
convolution. In practice  circular convolution is often used to approximate a linear convolution when
the ﬁlter has a compact support or decays fast [32]  and the signal has ﬁnite length or satisﬁes a
circular boundary condition [1]. The accelerated computation of circular convolution via the fast
Fourier transform (FFT) is especially beneﬁcial in 2D or 3D applications [1  29]. Multichannel blind
deconvolution with a circular convolution model is also related to blind gain and phase calibration
with Fourier measurements [33  34  35  36  37].
In this paper  we consider MSBD with circular convolution. In addition to the sparsity prior on the
channels {xi}N
i=1  we impose  without loss of generality  the constraint that f has unit (cid:96)2 norm  i.e.  f
is on the unit sphere. (This eliminates the scaling ambiguity inherent in the MBD problem.) We show
that our sparsity promoting objective function has a nice geometric landscape on the the unit sphere:
(S1) all local minima correspond to signed shifted versions of the desired solution  and (S2) the
objective function is strongly convex in neighborhoods of the local minima  and has strictly negative
curvature directions in neighborhoods of local maxima and saddle points. Similar geometric analysis
has been conducted for dictionary learning [38]  phase retrieval [39]  and single channel sparse blind
deconvolution [10]. Recently  Mei et al. [40] analyzed the geometric structure of the empirical risk
of a class of machine learning problems (e.g.  nonconvex binary classiﬁcation  robust regression  and
Gaussian mixture model). This paper is the ﬁrst such analysis for MSBD.
Although our analysis of global geometry shares a similar roadmap with previous works [10  38  39 
40]  much of our theoretical analysis is tailored for MSBD. For example  our partition of the unit
sphere into three regions (of strong convexity  negative curvature  and large gradient  respectively) is
carefully crafted for our objective function  and is closely related to our error bound. We leverage
tools that are commonly used in related works  such as concentration inequalities and union bounds 
to prove the geometric properties. However  our bounds are derived speciﬁcally for MSBD  under
new assumptions. For example  the single channel sparse blind deconvolution [10] with sparse x 
requires f to have compact support. In contrast  in this work on MSBD  other than invertibility  we
make no assumptions on f.

2

Properties (S1) and (S2) allow simple manifold optimization algorithms to ﬁnd the ground truth in
the nonconvex formulation. Unlike the second order methods in previous works [41  39]  we take
advantage of recent advances in the analysis of ﬁrst-order methods [42  43]  and prove that a simple
manifold gradient descent algorithm  with random initialization and a ﬁxed step size  can accurately
recover a signed shifted version of the ground truth in polynomial time almost surely. This is the ﬁrst
guaranteed algorithm for MSBD that does not rely on restrictive assumptions on f or {xi}N
Recently  many optimization methods have been shown to escape saddle points of objective functions
with benign landscapes  e.g.  gradient descent [44  45]  stochastic gradient descent [46]  perturbed
gradient descent [47]  Natasha [48  49]  and FastCubic [50]. Similarly  optimization methods over
Riemannian manifolds that can escape saddle points include manifold gradient descent [43]  the trust
region method [41  39]  and the negative curvature method [51]. Our main result shows that these
algorithms can be applied to MSBD thanks to the favorable geometric structure of our objective.

i=1.

2 MSBD on the Sphere

i=1 and f from {yi}N

2.1 Problem Statement
In MSBD  the measurements y1  y2  . . .   yN ∈ Rn are the circular convolutions of unknown sparse
vectors x1  x2  . . .   xN ∈ Rn and an unknown vector f ∈ Rn  i.e.  yi = xi (cid:126) f. In this paper  we
solve for {xi}n
i=1. One can rewrite the measurement as Y = Cf X  where
Cf represents the circulant matrix whose ﬁrst column is f  and Y = [y1  y2  . . .   yN ] and X =
[x1  x2  . . .   xN ] are n × N matrices. Without structures  one can solve the problem by choosing any
invertible circulant matrix Cf and compute X = C−1
f Y . The fact that X is sparse narrows down the
search space.
Even with sparsity  the problem suffers from inherent scale and shift ambiguities. Suppose Sj :
Rn → Rn denotes a circular shift by j positions  i.e.  Sj(x)(k) = x(k−j) for j  k ∈ [n]. Here we use
x(j) to denote the j-th entry of x ∈ Rn (treated as modulo n). Note that we have yi = xi (cid:126) f =
(αSj(xi)) (cid:126) (α−1S−j(f )) for every nonzero α ∈ R and j ∈ [n]. Therefore  MSBD has equivalent
solutions generated by scaling and circularly shifting {xi}n
Throughout this paper  we assume that the circular convolution with the signal f is invertible  i.e. 
there exists a ﬁlter g such that f (cid:126) g = e1 (the ﬁrst standard basis vector). Equivalently  Cf is
an invertible matrix  and the discrete Fourier transform (DFT) of f is nonzero everywhere. Since
yi (cid:126) g = xi (cid:126) f (cid:126) g = xi  one can ﬁnd g by solving the following optimization problem:

i=1 and f.

(P0) min
h∈Rn

1
N

(cid:107)Cyih(cid:107)0 

s.t. h (cid:54)= 0.

The constraint eliminates the trivial solution that is 0. If the solution to MSBD is unique up to the
aforementioned ambiguities  then the only minimizers of (P0) are h = αSjg (α (cid:54)= 0  j ∈ [n]).

2.2 Smooth Formulation

Minimizing the non-smooth (cid:96)0 “norm” is usually challeng-
ing. Instead  one can choose a smooth surrogate function for
sparsity. It is well-known that minimizing the (cid:96)1 norm can
lead to sparse solutions [52]. An intuitive explanation is that
the sparse points on the unit (cid:96)2 sphere (which we call unit
sphere from now on) have the smallest (cid:96)1 norm. As demon-
strated in Figure 1  these sparse points also have the largest (cid:96)4
norm. Therefore  maximizing the (cid:96)4 norm  a surrogate for the
“spikiness” [53] of a vector  is akin to minimizing its sparsity.
Here  we make two observations: (1) one can eliminate the
scaling ambiguity by restricting h to the unit sphere Sn−1; (2) sparse recovery can be achieved by
maximizing (cid:107)·(cid:107)4

4. Based on these observations  we adopt the following optimization problem:

Figure 1: Unit (cid:96)1  (cid:96)2  and (cid:96)4 spheres
in 2-D.

(P1) min
h∈Rn

− 1
4N

(cid:107)CyiRh(cid:107)4
4 

s.t. (cid:107)h(cid:107) = 1.

N(cid:88)

i=1

N(cid:88)

i=1

3

(cid:80)N
i=1 C(cid:62)

yi

i=1  we explain how the preconditioner R works.

Cyi)−1/2 ∈ Rn×n is a preconditioner  where θ is a parameter that
i=1. In Section 3  under speciﬁc probabilistic assumptions

The matrix R := ( 1
θnN
is proportional to the sparsity level of {xi}N
on {xi}N
Problem (P1) can be solved using ﬁrst-order or second-order optimization methods over Riemannian
manifolds. The main result of this paper provides a geometric view of the objective function over the
sphere Sn−1 (see Figure 3). We show that some off-the-shelf optimization methods can be used to
obtain a solution ˆh close to a scaled and circularly shifted version of the ground truth. Speciﬁcally  ˆh
satisﬁes Cf Rˆh ≈ ±ej for some j ∈ [n]  i.e.  Rˆh is approximately a signed and shifted version of the
inverse of f. Given solution ˆh to (P1)  one can recover f and xi (i = 1  2  . . .   N) as follows:

ˆf = F−1(cid:2)F(Rˆh)(cid:12)−1(cid:3) 

ˆxi = CyiRˆh.

(1)

Here  we use x(cid:12)−1 to denote the entrywise inverse of x.

3 Global Geometric View
In this paper  we assume that {xi}N
(A1) The channels {xi}N

i=1 are random sparse vectors  and f is invertible:

i=1 follow a Bernoulli-Rademacher model. More precisely  xi(j) = AijBij
are independent random variables  Bij’s follow a Bernoulli distribution Ber(θ)  and Aij’s
follow a Rademacher distribution (taking values 1 and −1  each with probability 1/2).
number of f  which is deﬁned as κ := maxj |(F f )(j)|
and smallest magnitudes of the DFT of f.

(A2) The circular convolution with the signal f is invertible. We use κ to denote the condition
σn(Cf )  i.e.  the ratio of the largest

mink |(F f )(k)| = σ1(Cf )

yi

yi

yi

The Bernoulli-Rademacher model is a special case of the Bernoulli–sub-Gaussian models. The
derivation in this paper can be repeated for other sub-Gaussian nonzero entries  with different tail
bounds. We use the Rademacher distribution for simplicity.
4(cid:107)x(cid:107)4
Let φ(x) = − 1
4. Its gradient and Hessian are deﬁned by
j  and Hφ(x)(jk) = −3x2
∇φ(x)(j) = −x3
j δjk. (We use H(jk)
to denote the entry of H ∈ Rn×n in the j-th row and k-th
(cid:80)N
(cid:80)N
column  and use δjk to denote the Kronecker delta.) Then
(cid:80)N
the objective function in (P1) is L(h) = 1
i=1 φ(CyiRh) 
N
Cyi)−1/2. The gradient and
i=1 C(cid:62)
where R = ( 1
(cid:80)N
θnN
Hessian are ∇L(h) = 1
∇φ(CyiRh)  and
i=1 R(cid:62)C(cid:62)
N
i=1 R(cid:62)C(cid:62)
Hφ(Cyi Rh)Cyi R. Since L(h) is
HL(h) = 1
N
to be minimized over Sn−1  we use optimization methods
over Riemannian manifolds [54]. To this end  we deﬁne the
tangent space at h ∈ Sn−1 as {z ∈ Rn : z ⊥ h} (see Figure
2). We study the Riemannian gradient and Riemannian Hes-
at h ∈ Sn−1): (cid:98)∇L(h) = Ph⊥∇L(h)  and (cid:98)HL(h) = Ph⊥ HL(h)Ph⊥ − (cid:104)∇L(h)  h(cid:105)Ph⊥  where
sian of L(h) (gradient and Hessian along the tangent space
Ph⊥ = I − hh(cid:62) is the projection onto the tangent space at h. We refer the readers to [54] for a more
comprehensive discussion of these concepts.
The toy example in Figure 3 demonstrates the geometric structure of the objective function on Sn−1.
(As shown later  the quantity EL(cid:48)(cid:48)(h) is  up to an unimportant rotation of the coordinate system  a
good approximation to L(h).) The local minima correspond to signed shifted versions of the ground
truth (Figure 3(a)). The Riemannian gradient is zero at stationary points  including local minima 
saddle points  and local maxima of the objective function when restricted to the sphere Sn−1. (Figure
3(b)). The Riemannian Hessian is positive deﬁnite in the neighborhoods of local minima  and has at
least one strictly negative eigenvalue in the neighborhoods of local maxima and saddle points (Figure
3(c)). We say that a stationary point is a “strict saddle point” if the Riemannian Hessian has at least
one strictly negative eigenvalue. Our main result Theorem 3.1 formalizes the observation that L(h)
only has two types of stationary points: (1) local minima  which are close to signed shifted versions

Figure 2: A demonstration of the tan-
gent space of Sn−1 at h  the origin of
which is translated to h. The Rieman-
nian gradient and Riemannian Hes-
sian are deﬁned on tangent spaces.

4

of the ground truth  and (2) strict saddle points. Please refer to the supplementary result for the full
proof.

(a)

Figure 3: Geometric structure of the objective function over the sphere. For n = 3  we plot the follow-

ing quantities on the sphere S2: (a) EL(cid:48)(cid:48)(h)  (b) (cid:107)E(cid:98)∇L(cid:48)(cid:48) (h)(cid:107)  and (c) minz⊥h (cid:107)z(cid:107)=1 z(cid:62)E(cid:98)HL(cid:48)(cid:48)(h)z.

(b)

(c)

ρ4

ρ4

1  c2  c(cid:48)

n ≤ θ < 1

log n}  then with probability at least 1 − n−c(cid:48)

Theorem 3.1. Suppose Assumptions (A1) and (A2) are satisﬁed  and the Bernoulli probability
3 . Let κ be the condition number of f  and let ρ < 10−3 be a small tol-
satisﬁes 1
erance constant. There exist constants c1  c(cid:48)
2 > 0 (depending only on θ)  such that: if
N > max{ c1n9
ρ   c2κ8n8
2  every local
log n
minimum h∗ in (P1) is close to a signed shifted version of the ground truth. I.e.  for some j ∈ [n]:
√
(cid:107)Cf Rh∗ ± ej(cid:107) ≤ 2κ
ρ. Moreover  one can partition Sn−1 into three sets H1  H2  and H3  which 
for some c(n  θ  ρ) > 0  satisfy:
◦ L(h) is strongly convex in H1  i.e.  minz:(cid:107)z(cid:107)=1
z⊥h
◦ L(h) has negative curvature in H2  i.e.  minz:(cid:107)z(cid:107)=1
z⊥h

z(cid:62)(cid:98)HL(h)z ≤ −c(n  θ  ρ) < 0.

z(cid:62)(cid:98)HL(h)z ≥ c(n  θ  ρ) > 0.

◦ L(h) has a descent direction in H3  i.e.  (cid:107)(cid:98)∇L(h)(cid:107) ≥ c(n  θ  ρ) > 0.

1 − n−c(cid:48)

Clearly  all the stationary points of L(h) on Sn−1 belong to H1 or H2. The stationary points in H1
are local minima  and the stationary points in H2 are strict saddle points.

N

N

yi

1
N

f Cf )−1/2h). Since Cf (C(cid:62)

Cyi)−1/2 asymptotically converges to (C(cid:62)

Proof Sketch. Note that R = ( 1
θnN
N increases. Therefore  L(h) can be approximated by L(cid:48)(h) = 1

(cid:80)N
(cid:80)N
i=1 C(cid:62)
i=1 φ(Cyi(C(cid:62)
(cid:80)N
i=1 φ(Cxih(cid:48)) with h(cid:48) = Cf (C(cid:62)

f Cf )−1/2 as
f Cf )−1/2h) =
f Cf )−1/2 is an orthogonal matrix  one can study
f Cf )−1/2h  which is a rotated

the objective function L(cid:48)(cid:48)(h(cid:48)) = 1
version of L(cid:48)(h) on the sphere. Our analysis consists of three parts:

(cid:80)N
i=1 φ(CxiCf (C(cid:62)
(1) Geometric structure of EL(cid:48)(cid:48): We ﬁrst bound minz:(cid:107)z(cid:107)=1  z⊥h z(cid:62)E(cid:98)HL(cid:48)(cid:48)(h)z  which is strictly
(2) Deviation of L(cid:48)(cid:48) (or its rotated version L(cid:48)) from EL(cid:48)(cid:48): We bound (cid:107)(cid:98)∇L(cid:48)(cid:48)(h) − E(cid:98)∇L(cid:48)(cid:48) (h)(cid:107) and
(cid:107)(cid:98)HL(cid:48)(cid:48) (h) − E(cid:98)HL(cid:48)(cid:48) (h)(cid:107) using the matrix Bernstein inequality and union bounds.
(3) Difference between L and L(cid:48): We bound (cid:107)(cid:98)∇L(h) −(cid:98)∇L(cid:48)(h)(cid:107) and (cid:107)(cid:98)HL(h) − (cid:98)HL(cid:48)(h)(cid:107) using the
matrix Bernstein inequality and Lipschitz continuity of (cid:98)∇L(h) and (cid:98)HL(h).

positive near its local minima  and strictly negative near all other stationary points (the strict saddle
points). At the same time  at all other points on Sn−1 (the points further away from stationary points) 
the Riemannian gradient of EL(cid:48)(cid:48) is bounded away from zero.

Theorem 3.1 follows by combining the above results.

5

4 Optimization Method

(cid:0)h(t) − γ(cid:98)∇L(h(t))(cid:1).

Recently  ﬁrst-order methods have been shown to escape strict saddle points with random initialization
[44  45]. In this paper  we use the manifold gradient descent algorithm studied by Lee et al. [43].
One can initialize the algorithm with a random h(0)  and use the following iterative update:

h(t+1) = A(h(t)) := PSn−1

(2)
Each iteration takes a Riemannian gradient descent step in the tangent space  and does a retraction
by normalizing the iterate (projecting onto Sn−1). Using the geometric structure introduced in
Section 3  and some technical results in [42  43]  the following result gives a theoretical guarantee for
manifold gradient descent for our formulation of MSBD: convergence to an accurate estimate (up to
the inherent sign and shift ambiguity) of the true solution.
Theorem 4.1. Suppose that the geometric structure in Theorem 3.1 is satisﬁed. If manifold gradient
descent (2) is initialized with a random h(0) drawn from a uniform distribution on Sn−1  and the step
128n3   then (2) converges to a local minimum of L(h) on Sn−1 almost surely.
size is chosen as γ = 1
It particular  after at most T = 4096n8

θ2(1−3θ)2ρ4 iterations  h(T ) ∈ H1. Moreover  for some j ∈ [n]

√
(cid:107)Cf Rh(T ) ± ej(cid:107) ≤ 2κ

ρ.

Corollary 4.2. If the conditions of Theorem 4.1 are satisﬁed  then the recovered ˆf and ˆxi in (1) 
computed using the output of manifold gradient descent ˆh = h(T )  satisfy (for some j ∈ [n]):

(cid:107)ˆxi ± Sj(xi)(cid:107)

(cid:107)xi(cid:107)

√
≤ 2κ

ρn 

(cid:107) ˆf ± S−j(f )(cid:107)

(cid:107)f(cid:107)

√
≤ 2κ
ρn
√
1 − 2κ

.

ρn

Theorem 4.1 and Corollary 4.2 show that  with a random initialization and a ﬁxed step size  manifold
gradient descent outputs  in polynomial time  a solution that is close to a signed and shifted version
of the ground truth. We prove these results in the supplementary material.

5 Numerical Experiments

5.1 Deconvolution with Synthetic Data

(cid:12)(cid:12)cos ∠(cid:0)Cf Rh(T )  ej

(cid:1)(cid:12)(cid:12) > 0.95. We compute the success rate based on 100 Monte Carlo

In this section  we examine the empirical performance of manifold gradient descent (2) in solving
MSBD (P1). We synthesize {xi}N
i=1 following the Bernoulli-Rademacher model  and synthesize f
following a Gaussian distribution N (0n×1  In). In all experiments  we run manifold gradient descent
for T = 100 iterations  with a ﬁxed step size of γ = 0.1.
Recall that the desired h is a signed shifted version of the ground truth  i.e.  Cf Rh = ±ej
(j ∈ [n]). Therefore  to evaluate the accuracy of the output h(T )  we compute Cf Rh(T ) with
the true f  and declare successful recovery if (cid:107)Cf Rh(T )(cid:107)∞/(cid:107)Cf Rh(T )(cid:107) > 0.95  or equivalently 
if maxj∈[n]
instances. In a typical successful instance  h(t) converges to an accurate estimate of the ground truth
after about 50 iterations (as shown by the error and accuracy plots in Figure 4(d) and 4(h)).
In the ﬁrst experiment  we ﬁx θ = 0.1 (sparsity level  mean of the Bernoulli distribution)  and
run experiments with n = 32  64  . . .   256 and N = 32  64  . . .   256 (see Figure 4(a)).
In the
second experiment  we ﬁx n = 256  and run experiments with θ = 0.02  0.04  . . .   0.16 and N =
32  64  . . .   256 (see Figure 4(b)). The empirical phase transitions suggest that  for sparsity level
relatively small (e.g.  θ < 0.16)  there exist a constant c > 0 such that manifold gradient descent can
recover a signed shifted version of the ground truth with N ≥ cnθ.
In the third experiment  we examine the phase transition with respect to N and the condition number
κ of f  which is the ratio of the largest and smallest magnitudes of its DFT. To synthesize f with
speciﬁc κ  we generate the DFT ˜f of f that is random with the following distribution: (1) The DFT
˜f is symmetric  i.e.  ˜f(j) = ˜f(n+2−j)  so that f is real. (2) The phase of ˜f(j) follows a uniform
distribution on [0  2π)  except for the phases of ˜f(1) and ˜f(n/2+1) (if n is even)  which are always
0 for symmetry. (3) The gains of ˜f follows a uniform distribution on [1  κ]. We ﬁx n = 256 and

6

θ = 0.1  and run experiments with κ = 1  2  4  . . .   128 and N = 32  64  . . .   256 (see Figure 4(c)).
The phase transition suggests that the number N for successful empirical recovery is not sensitive to
the condition number κ.
Manifold gradient descent is robust against noise. We repeat the above experiments with noisy
measurements: yi = xi (cid:126) f + σεi  where εi follows a Gaussian distribution N (0n×1  In). The
nθ (SNR ≈ 20 dB) are shown in Figure 4(e)  4(f)  and 4(g). For a
phase transitions for σ = 0.1
reasonable noise level  the number N of noisy measurements we need to accurately recover a signed
shifted version of the ground truth is roughly the same as with noiseless measurements.

√

256

192

128

64

256

192

128

64

N

N

64

128 192 256

n

(a)

256

192

128

64

N

256

192

128

64

N

0.04 0.08 0.12 0.16

θ

(b)

256

192

128

64

N

256

192

128

64

N

(cid:107)
j
e
−

)
t
(
h
R
f
C
(cid:107)

2

8

κ

32

128

(c)

(cid:107)

∞
(cid:107)

)
t
(
h
R
f
C
(cid:107)

)
t
(
h
R
f
C
(cid:107)

64

128 192 256

n

(e)

0.04 0.08 0.12 0.16

θ

(f)

2

8

κ

32

128

(g)

1

0.5

0

1

0.8

0.6

0.4

0.2

0

0

50
t

100

(d)

50
t

100

(h)

Figure 4: Empirical phase transition (grayscale values represent success rates). The ﬁrst column
shows the phase transitions of N versus n. The second column shows the phase transitions of N
versus θ. The third column shows the phase transitions of N versus κ. (a) - (c) are the results for the
noiseless case. (e) - (g) are the results for SNR ≈ 20 dB. (d) and (h) show the error (cid:107)Cf Rh(t) − ej(cid:107)
and the accuracy (cid:107)Cf Rh(t)(cid:107)∞/(cid:107)Cf Rh(t)(cid:107) as functions of the iteration number t  respectively.

5.2 Blind Gain and Phase Calibration

In this section  we consider the blind calibration problem [31]. Suppose that a sensing system takes
Fourier measurements of unknown signals  with sensors that have unknown gains and phases  i.e. 
˜yi = diag( ˜f )Fxi  where xi are the targeted unknown sparse signals  F is the DFT matrix  and the
entries of ˜f represent the unknown gains and phases. In sensor array processing [55]  the supports
of xi’s are identical  and represent the directions of arrival of incoming sources. The simultaneous
recovery of ˜f and xi’s is equivalent to MSBD in the frequency domain.
Clearly  Assumption (A1) is not satisﬁed in this case. For complex f  xi ∈ Cn  we solve:

N(cid:88)
Cyi)−1/2 ∈ Cn×n  and (·)H represents the Hermitian transpose. If one
where R := ( 1
treats the real and imaginary parts of h separately  then this optimization in Cn can be recast into
θnN
wi(h)(cid:1)  where wi(h) represents wi(h) =
R2n  and the gradient with respect to Re(h) and Im(h) can be used in ﬁrst-order methods. This
is related to Wirtinger gradient descent algorithms (see the discussion in [56]). The Riemannian
√−1∇φ(Im(Cyi Rh))  and P(R·h)⊥ represents the projection onto the tangent
∇φ(Re(CyiRh)) +
space at h in S2n−1 ⊂ R2n: P(R·h)⊥ z = z − Re(hHz) · h. In the complex case  one can initialize
the manifold gradient descent algorithm with a random h(0)  for which [Re(h(0))(cid:62)  Im(h(0))(cid:62)](cid:62)
follows a uniform distribution on S2n−1.

gradient with respect to h is P(R·h)⊥(cid:0) 1

φ(Re(CyiRh)) + φ(Im(Cyi Rh)) 

s.t. (cid:107)h(cid:107) = 1 

min
h∈Cn

(cid:80)N

i=1 RHC H
yi

N

(cid:80)N

1
N

i=1

i=1 C H
yi

7

128

96

64

32

N

128

96

64

32

N

128

96

64

32

N

128

96

64

32

N

4

8

s

12

16

(a)

12

16

4

8

s

(b)

4

8

s

12

16

(c)

12

16

4

8

s

(d)

Figure 5: Empirical phase transition of N versus s  given that n = 128. (a) Manifold gradient
descent. (b) Truncated power iteration [31]. (c) Off-the-grid algebraic method [57]. (d) Off-the-grid
optimization approach [58].

We compare manifold gradient descent (with random initialization) with three blind calibration
algorithms that solve MSBD in the frequency domain: (i) truncated power iteration [31] (initialized
with f (0) = e1 and x(0)
i = 0); (ii) an off-the-grid algebraic method [57] (simpliﬁed from [55]); and
(iii) an off-the-grid optimization approach [58].
We consider Gaussian random ˜f ∼ CN (0n×1  In)  and jointly s-sparse {xi}N
i=1  for which the
support is chosen uniformly at random  and the nonzero entries of {xi}N
i=1 follow a complex
Gaussian distribution CN (0  1). We ﬁx n = 128  and run experiments for N = 16  32  48 ···   128 
and s = 2  4  6  . . .   16. We say that the recovery is successful is the accuracy (cosine of the angle
between the true signal and the recovered signal) is greater than 0.7.
By the phase transitions in Figure 5  manifold gradient descent and truncated power iteration are
both successful when N ≥ 48 and s ≤ 8. However  although truncated power iteration achieves
higher success rates when both N and s are small  it fails for s > 8 even with a large N. In contrast 
manifold gradient descent can recover channels with s = 16 when N ≥ 80.
The off-the-grid methods are designed  hence provide better recovery than the ﬁrst two algorithms 
for the case that the unknown sparse signals do not reside on a discrete grid (i.e.  “off the grid”).
However  the off-the-grid methods rely on the properties of the covariance matrix 1
i   and
N
require a much larger N than the ﬁrst two algorithms to achieve high success rates when the sparse
signals actually lie on a regular grid (see the phase transitions in Figure 5).

(cid:80)N

i=1 yiyH

5.3 Super-Resolution Fluorescence Microscopy

Manifold gradient descent can be applied to deconvolution of time resolved ﬂuorescence microscopy
images. The goal is to recover sharp images xi’s from observations yi’s that are blurred by an
unknown PSF f.
We use a publicly available microtubule dataset [28]  which contains N = 626 images (Figure
6(a)). Since ﬂuorophores are are turned on and off stochastically  the images xi’s are random sparse
samples of the 64 × 64 microtubule image (Figure 6(e)). The observations yi’s (Figure 6(b)  6(f)) are
synthesized by circular convolutions with the PSF in Figure 6(i). The recovered images (Figure 6(c) 
6(g)) and kernel (Figure 6(j)) clearly demonstrate the effectiveness of our approach in this setting.
Blind deconvolution is less sensitive to instrument calibration error than non-blind deconvolution.
If the PSF used in a non-blind deconvolution method fails to account for certain optic aberration 
the resulting images may suffer from spurious artifacts. For example  if we use a miscalibrated PSF
(Figure 6(k)) in non-blind image reconstruction using FISTA [59]  then the recovered images (Figure
6(d)  6(h)) suffer from serious spurious artifacts.

6 Conclusion

In this paper  we study the geometric structure of multichannel sparse blind deconvolution over
the unit sphere. Our theoretical analysis reveals that local minima of a sparsity promoting smooth
objective function correspond to signed shifted version of the ground truth  and saddle points have
strictly negative curvatures. Thanks to the favorable geometric properties of the objective  we can

8

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

Figure 6: Super-resolution ﬂuorescence microscopy experiment using manifold gradient descent. (a)
True images. (b) Observed images. (c) Recovered images using blind deconvolution. (d) Recovered
images using non-blind deconvolution and a miscalibrated PSF. (e)(f)(g)(h) are average images of
(a)(b)(c)(d). (i) True PSF. (j) Recovered PSF using blind deconvolution. (k) Miscalibrated PSF used
in non-blind deconvolution. All images in this ﬁgure are of the same size (64 × 64).

simultaneously recover the unknown signal and unknown channels from convolutional measurements
using manifold gradient descent with a random initialization.
In practice  many convolutional
measurement models are subsampled in the spatial domain (e.g.  image super-resolution) or in the
frequency domain (e.g.  radio astronomy). Studying the effect of subsampling on the geometric
structure of multichannel sparse blind deconvolution is an interesting problem for future work.

Acknowledgments

This work was supported in part by the National Science Foundation (NSF) under Grant IIS 14-47879.
The authors would like to thank Ju Sun for helpful discussions about this paper. The manuscript
beneﬁted from constructive comments by the anonymous reviewers.

References

[1] S. Cho and S. Lee  “Fast motion deblurring ” in ACM Transactions on Graphics (TOG)  vol. 28 

no. 5. ACM  2009  p. 145.

[2] A. Levin  Y. Weiss  F. Durand  and W. T. Freeman  “Understanding blind deconvolution
algorithms ” IEEE Transactions on Pattern Analysis and Machine Intelligence  vol. 33  no. 12 
pp. 2354–2367  Dec 2011.

[3] L. Xu  S. Zheng  and J. Jia  “Unnatural l0 sparse representation for natural image deblurring ”
IEEE  2013 

in Computer Vision and Pattern Recognition (CVPR)  2013 IEEE Conference on.
pp. 1107–1114.

[4] A. Ahmed  B. Recht  and J. Romberg  “Blind deconvolution using convex programming ” IEEE

Transactions on Information Theory  vol. 60  no. 3  pp. 1711–1732  March 2014.

[5] S. Ling and T. Strohmer  “Self-calibration and biconvex compressive sensing ” Inverse Problems 

vol. 31  no. 11  p. 115002  2015.

9

[6] Y. Chi  “Guaranteed blind sparse spikes deconvolution via lifting and convex optimization ”
IEEE Journal of Selected Topics in Signal Processing  vol. 10  no. 4  pp. 782–794  June 2016.
[7] X. Li  S. Ling  T. Strohmer  and K. Wei  “Rapid  robust  and reliable blind deconvolution via

nonconvex optimization ” arXiv preprint arXiv:1606.04933  2016.

[8] K. Lee  Y. Li  M. Junge  and Y. Bresler  “Blind recovery of sparse signals from subsampled
convolution ” IEEE Transactions on Information Theory  vol. 63  no. 2  pp. 802–821  Feb 2017.
[9] W. Huang and P. Hand  “Blind deconvolution by a steepest descent algorithm on a quotient

manifold ” arXiv preprint arXiv:1710.03309  2017.

[10] Y. Zhang  Y. Lau  H.-w. Kuo  S. Cheung  A. Pasupathy  and J. Wright  “On the global geometry
of sphere-constrained sparse blind deconvolution ” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  2017  pp. 4894–4902.

[11] Y. Li  K. Lee  and Y. Bresler  “Identiﬁability in blind deconvolution with subspace or sparsity
constraints ” IEEE Transactions on Information Theory  vol. 62  no. 7  pp. 4266–4275  July
2016.

[12] ——  “Identiﬁability and stability in blind deconvolution under minimal assumptions ” IEEE

Transactions on Information Theory  vol. 63  no. 7  pp. 4619–4633  July 2017.

[13] L. Tong and S. Perreau  “Multichannel blind identiﬁcation: from subspace to maximum likeli-

hood methods ” Proceedings of the IEEE  vol. 86  no. 10  pp. 1951–1968  Oct 1998.

[14] H. She  R.-R. Chen  D. Liang  Y. Chang  and L. Ying  “Image reconstruction from phased-array
data based on multichannel blind deconvolution ” Magnetic resonance imaging  vol. 33  no. 9 
pp. 1106–1113  2015.

[15] H. Zhang  D. Wipf  and Y. Zhang  “Multi-image blind deblurring using a coupled adaptive
sparse prior ” in Computer Vision and Pattern Recognition (CVPR)  2013 IEEE Conference on.
IEEE  2013  pp. 1051–1058.

[16] L. Tong  G. Xu  and T. Kailath  “A new approach to blind identiﬁcation and equalization of
multipath channels ” in [1991] Conference Record of the Twenty-Fifth Asilomar Conference on
Signals  Systems Computers  Nov 1991  pp. 856–860 vol.2.

[17] E. Moulines  P. Duhamel  J. F. Cardoso  and S. Mayrargue  “Subspace methods for the blind
identiﬁcation of multichannel ﬁr ﬁlters ” IEEE Transactions on Signal Processing  vol. 43  no. 2 
pp. 516–525  Feb 1995.

[18] G. Xu  H. Liu  L. Tong  and T. Kailath  “A least-squares approach to blind channel identiﬁcation ”

IEEE Transactions on Signal Processing  vol. 43  no. 12  pp. 2982–2993  Dec 1995.

[19] M. I. Gurelli and C. L. Nikias  “Evam: an eigenvector-based algorithm for multichannel blind
deconvolution of input colored signals ” IEEE Transactions on Signal Processing  vol. 43  no. 1 
pp. 134–149  Jan 1995.

[20] K. Lee  F. Krahmer  and J. Romberg  “Spectral methods for passive imaging: Non-asymptotic

performance and robustness ” arXiv preprint arXiv:1708.04343  2017.

[21] C. R. Berger  S. Zhou  J. C. Preisig  and P. Willett  “Sparse channel estimation for multicarrier
underwater acoustic communication: From subspace methods to compressed sensing ” IEEE
Transactions on Signal Processing  vol. 58  no. 3  pp. 1708–1721  March 2010.

[22] K. G. Sabra and D. R. Dowling  “Blind deconvolution in ocean waveguides using artiﬁcial time
reversal ” The Journal of the Acoustical Society of America  vol. 116  no. 1  pp. 262–271  2004.
[23] N. Tian  S.-H. Byun  K. Sabra  and J. Romberg  “Multichannel myopic deconvolution in
underwater acoustic channels via low-rank recovery ” The Journal of the Acoustical Society of
America  vol. 141  no. 5  pp. 3337–3348  2017.

[24] K. F. Kaaresen and T. Taxt  “Multichannel blind deconvolution of seismic signals ” Geophysics 

vol. 63  no. 6  pp. 2093–2107  1998.

[25] D. R. Gitelman  W. D. Penny  J. Ashburner  and K. J. Friston  “Modeling regional and psy-
chophysiologic interactions in fmri: the importance of hemodynamic deconvolution ” Neuroim-
age  vol. 19  no. 1  pp. 200–207  2003.

[26] M. J. Rust  M. Bates  and X. Zhuang  “Sub-diffraction-limit imaging by stochastic optical

reconstruction microscopy (storm) ” Nature methods  vol. 3  no. 10  p. 793  2006.

10

[27] E. Betzig  G. H. Patterson  R. Sougrat  O. W. Lindwasser  S. Olenych  J. S. Bonifacino  M. W.
Davidson  J. Lippincott-Schwartz  and H. F. Hess  “Imaging intracellular ﬂuorescent proteins at
nanometer resolution ” Science  vol. 313  no. 5793  pp. 1642–1645  2006.

[28] E. A. Mukamel  H. Babcock  and X. Zhuang  “Statistical deconvolution for superresolution

ﬂuorescence microscopy ” Biophysical journal  vol. 102  no. 10  pp. 2391–2400  2012.

[29] P. Sarder and A. Nehorai  “Deconvolution methods for 3-d ﬂuorescence microscopy images ”

IEEE Signal Processing Magazine  vol. 23  no. 3  pp. 32–45  May 2006.

[30] L. Wang and Y. Chi  “Blind deconvolution from multiple sparse inputs ” IEEE Signal Processing

Letters  vol. 23  no. 10  pp. 1384–1388  Oct 2016.

[31] Y. Li  K. Lee  and Y. Bresler  “Blind gain and phase calibration via sparse spectral methods ”

IEEE Transactions on Information Theory  2018.

[32] T. Strohmer  “Four short stories about toeplitz matrix calculations ” Linear Algebra and its

Applications  vol. 343  pp. 321–344  2002.

[33] Y. Li  K. Lee  and Y. Bresler  “Identiﬁability in bilinear inverse problems with applications
to subspace or sparsity-constrained blind gain and phase calibration ” IEEE Transactions on
Information Theory  vol. 63  no. 2  pp. 822–842  Feb 2017.

[34] ——  “Optimal sample complexity for blind gain and phase calibration ” IEEE Transactions on

Signal Processing  vol. 64  no. 21  pp. 5549–5556  Nov 2016.

[35] L. Balzano and R. Nowak  “Blind calibration of sensor networks ” in Proceedings of the 6th
international conference on Information processing in sensor networks. ACM  2007  pp.
79–88.

[36] C. Bilen  G. Puy  R. Gribonval  and L. Daudet  “Convex optimization approaches for blind
sensor calibration using sparsity ” IEEE Transactions on Signal Processing  vol. 62  no. 18  pp.
4847–4856  Sept 2014.

[37] S. Ling and T. Strohmer  “Self-calibration via linear least squares ” arXiv preprint  2016.
[38] J. Sun  Q. Qu  and J. Wright  “Complete dictionary recovery over the sphere i: Overview and
the geometric picture ” IEEE Transactions on Information Theory  vol. 63  no. 2  pp. 853–884 
Feb 2017.

[39] ——  “A geometric analysis of phase retrieval ” Foundations of Computational Mathematics 

Aug 2017. [Online]. Available: https://doi.org/10.1007/s10208-017-9365-9

[40] S. Mei  Y. Bai  and A. Montanari  “The landscape of empirical risk for non-convex losses ”

arXiv preprint arXiv:1607.06534  2016.

[41] J. Sun  Q. Qu  and J. Wright  “Complete dictionary recovery over the sphere ii: Recovery by
riemannian trust-region method ” IEEE Transactions on Information Theory  vol. 63  no. 2  pp.
885–914  Feb 2017.

[42] N. Boumal  P.-A. Absil  and C. Cartis  “Global rates of convergence for nonconvex optimization

on manifolds ” arXiv preprint arXiv:1605.08101  2016.

[43] J. D. Lee  I. Panageas  G. Piliouras  M. Simchowitz  M. I. Jordan  and B. Recht  “First-order

methods almost always avoid saddle points ” arXiv preprint arXiv:1710.07406  2017.

[44] J. D. Lee  M. Simchowitz  M. I. Jordan  and B. Recht  “Gradient descent only converges to

minimizers ” in Conference on Learning Theory  2016  pp. 1246–1257.

[45] I. Panageas and G. Piliouras  “Gradient descent only converges to minimizers: Non-isolated

critical points and invariant regions ” arXiv preprint arXiv:1605.00405  2016.

[46] R. Ge  F. Huang  C. Jin  and Y. Yuan  “Escaping from saddle points—online stochastic gradient

for tensor decomposition ” in Conference on Learning Theory  2015  pp. 797–842.

[47] C. Jin  R. Ge  P. Netrapalli  S. M. Kakade  and M. I. Jordan  “How to escape saddle points

efﬁciently ” in International Conference on Machine Learning  2017  pp. 1724–1732.

[48] Z. Allen-Zhu  “Natasha: Faster stochastic non-convex optimization via strongly non-convex

parameter ” arXiv preprint arXiv:1702.00763  2017.

[49] ——  “Natasha 2: Faster non-convex optimization than sgd ” arXiv preprint arXiv:1708.08694 

2017.

11

[50] N. Agarwal  Z. Allen-Zhu  B. Bullins  E. Hazan  and T. Ma  “Finding approximate local minima
faster than gradient descent ” in Proceedings of the 49th Annual ACM SIGACT Symposium on
Theory of Computing. ACM  2017  pp. 1195–1199.

[51] D. Goldfarb  C. Mu  J. Wright  and C. Zhou  “Using negative curvature in solving nonlinear
programs ” Computational Optimization and Applications  vol. 68  no. 3  pp. 479–502  Dec
2017. [Online]. Available: https://doi.org/10.1007/s10589-017-9925-6

[52] D. L. Donoho and M. Elad  “Optimally sparse representation in general (nonorthogonal)
dictionaries via (cid:96)1 minimization ” Proceedings of the National Academy of Sciences  vol. 100 
no. 5  pp. 2197–2202  feb 2003.

[53] Y. Zhang  H.-W. Kuo  and J. Wright  “Structured local optima in sparse blind deconvolution ”
in Proceedings of the 10th NIPS Workshop on Optimization for Machine Learning (OPTML) 
2017.

[54] P.-A. Absil  R. Mahony  and R. Sepulchre  Optimization algorithms on matrix manifolds.

Princeton University Press  2009.

[55] A. Paulraj and T. Kailath  “Direction of arrival estimation by eigenstructure methods with
unknown sensor gain and phase ” in ICASSP ’85. IEEE International Conference on Acoustics 
Speech  and Signal Processing  vol. 10  Apr 1985  pp. 640–643.

[56] E. J. Candès  X. Li  and M. Soltanolkotabi  “Phase retrieval via wirtinger ﬂow: Theory and
algorithms ” IEEE Transactions on Information Theory  vol. 61  no. 4  pp. 1985–2007  April
2015.

[57] M. P. Wylie  S. Roy  and R. F. Schmitt  “Self-calibration of linear equi-spaced (les) arrays ” in
1993 IEEE International Conference on Acoustics  Speech  and Signal Processing  vol. 1  April
1993  pp. 281–284 vol.1.

[58] Y. C. Eldar  W. Liao  and S. Tang  “Sensor calibration for off-the-grid spectral estimation ” arXiv

preprint arXiv:1707.03378  2017.

[59] A. Beck and M. Teboulle  “A fast iterative shrinkage-thresholding algorithm for linear inverse

problems ” SIAM journal on imaging sciences  vol. 2  no. 1  pp. 183–202  2009.

12

,Remi Munos
Tom Stepleton
Anna Harutyunyan
Marc Bellemare
Yanjun Li
Yoram Bresler