2017,A Learning Error Analysis for Structured Prediction with Approximate Inference,In this work  we try to understand the differences between exact and approximate inference algorithms in structured prediction. We compare the estimation and approximation error of both underestimate and overestimate models. The result shows that  from the perspective of learning errors  performances of approximate inference could be as good as exact inference. The error analyses also suggest a new margin for existing learning algorithms. Empirical evaluations on text classification  sequential labelling and dependency parsing witness the success of approximate inference and the benefit of the proposed margin.,A Learning Error Analysis for Structured Prediction

with Approximate Inference

Yuanbin Wu1  2  Man Lan1  2  Shiliang Sun1  Qi Zhang3  Xuanjing Huang3

1School of Computer Science and Software Engineering  East China Normal University

{ybwu  mlan  slsun}@cs.ecnu.edu.cn  {qz  xjhuang}@fudan.edu.cn

2Shanghai Key Laboratory of Multidimensional Information Processing

3School of Computer Science  Fudan University

Abstract

In this work  we try to understand the differences between exact and approximate
inference algorithms in structured prediction. We compare the estimation and ap-
proximation error of both underestimate (e.g.  greedy search) and overestimate
(e.g.  linear relaxation of integer programming) models. The result shows that 
from the perspective of learning errors  performances of approximate inference
could be as good as exact inference. The error analyses also suggest a new mar-
gin for existing learning algorithms. Empirical evaluations on text classiﬁcation 
sequential labelling and dependency parsing witness the success of approximate
inference and the beneﬁt of the proposed margin.

Introduction

1
Given an input x 2 X   structured prediction is the task of recovering a structure y = h(x) 2 Y 
where Y is a set of combinatorial objects such as sequences (sequential labelling) and trees (syntactic
parsing). Usually  the computation of h(x) needs an inference (decoding) procedure to ﬁnd an
optimal y:

h(x) = arg max

y2Y score(x; y):

Solving the “arg max” operation is essential for training and testing structured prediction models 
and it is also one of the most time-consuming parts due to its combinatorial natural. In practice  the
inference problem often reduces to combinatorial optimization or integer programming problems 
which are intractable in many cases. In order to accelerate models  faster approximate inference
methods are usually applied. Examples include underestimation algorithms which output structures
with suboptimal scores (e.g.  greedy search  max-product belief propagation)  and overestimation al-
gorithms which output structures in a larger output space (e.g.  linear relaxation of integer program-
ming). Understanding the trade-offs between computational efﬁciency and statistical performance is
important for designing effective structured prediction models [Chandrasekaran and Jordan  2013].
Prior work [Kulesza and Pereira  2007] shows that approximate inference may not be sufﬁcient
for learning a good statistical model  even with rigorous approximation guarantees. However  the
successful application of various approximate inference algorithms motivates a deeper exploration
of the topic. For example  the recent work [Globerson et al.  2015] shows that an approximate
inference can achieve optimal results on grid graphs. In this work  instead of focusing on speciﬁc
models and algorithms  we try to analyze general estimation and approximation errors for structured
prediction with approximate inference.
Recall that given a hypothesis space H  a learning algorithm A receives a set of training samples
S = f(xi; yi)gm
i=1 which are i.i.d. according to a distribution D on the space X (cid:2) Y  and returns a

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

hypothesis A(S) 2 H. Let e(h) = EDl(y; h(x)) be the risk of a hypothesis h on X (cid:2) Y (l is a loss
(cid:3)
function)  and h

= arg minh2He(h). Applying algorithm A will suffer two types of error:

|{z}
e(A(S)) = e(h
(cid:3)

}
The estimation error measures how close A(S) is to the best possible h
(cid:3); the approximation error
measures whether H is suitable for D  which only depends on the hypothesis space. Our main
theoretical results are:

approximation

+ e(A(S)) (cid:0) e(h
(cid:3)

)

)

|

{z

estimation

(cid:15) For the estimation error  we show that  comparing with exact inference  overestimate infer-
ence always has larger estimation error  while underestimate inference can probably have
smaller error. The results are based on the PAC-Bayes framework [McAllester  2007] for
structured prediction models.
(cid:15) For the approximation error  we ﬁnd that the errors of underestimate and exact inference
are not comparable. On the other side  overestimate inference algorithms have a smaller
approximation error than exact inference.

The results may explain the success of exact inference: it makes a good balance between the two
errors. They also suggest that the learning performances of approximate inference can still be im-
proved. Our contributions on empirical algorithms are two-fold.
First  following the PAC-Bayes error bounds  we propose to use a new margin (Deﬁnition 3) when
working with approximate algorithms.
It introduces a model parameter which can be tuned for
different inference algorithms. We investigate three widely used structured prediction models with
the new margin (structural SVM  structured perceptron and online passive-aggressive algorithm).
Second  we evaluate the algorithms on three NLP tasks: multi-class text classiﬁcation (a special
case of structured prediction)  sequential labelling (chunking  POS tagging  word segmentation)
and high-order non-projective dependency parsing. Results show that the proposed algorithms can
beneﬁt each structured prediction task.

2 Related Work

The ﬁrst learning error analysis of structured prediction was given in [Collins  2001]. The bounds
depend on the number of candidate outputs of samples  which grow exponentially with the size
of a sample. To tighten the result  Taskar et al. [2003] provided an improved covering number
argument  where the dependency on the output space size is replaced by the l2 norm of feature
vectors  and London et al. [2013] showed that when the data exhibits weak dependence within each
structure (collective stability)  the bound’s dependency on structure size could be improved. A
concise analysis based on the PAC-Bayes framework was given in [McAllester  2007]. It enjoys the
advantages of Taskar et al.’s bound and has a simpler derivation. Besides the structured hinge loss 
the PAC-Bayes framework was also applied to derive generalization bounds (and consistent results)
for ramp and probit surrogate loss functions [McAllester and Keshet  2011]  and loss functions based
on Gibbs decoders [Honorio and Jaakkola  2016]. Recently  Cortes et al. [2016] proposed a new
hypothesis space complexity measurement (factor graph complexity) by extending the Rademacher
complexity  and they can get tighter bounds than [Taskar et al.  2003].
For approximate inference algorithms  theoretical results have been given for different learning sce-
narios  such as the cutting plane algorithm of structured SVMs [Finley and Joachims  2008  Wang
and Shawe-Taylor  2009]  subgradient descent [Martins et al.  2009]  approximate inference via
dual loss [Meshi et al.  2010]  pseudo-max approach [Sontag et al.  2010]  local learning with de-
composed substructures [Samdani and Roth  2012]  perceptron [Huang et al.  2012]  and amortized
inference [Kundu et al.  2013  Chang et al.  2015]. Different from previous works  we try to give
a general analysis of approximate inference algorithms which is independent of speciﬁc learning
algorithms.
The concept of algorithmically separable is deﬁned in [Kulesza and Pereira  2007]  it showed that
without understanding combinations of learning and inference  the learning model could fail. Two
recent works gave theoretical analyses on approximate inference showing that they could also obtain

2

promising performances: Globerson et al. [2015] showed that for a generative 2D grid models  a two-
step approximate inference algorithm achieves optimal learning error. Meshi et al. [2016] showed
that approximation based on LP relaxations are often tight in practice.
The PAC-Bayes approach was initiated by [McAllester  1999]. Variants of the theory include
Seeger’s bound [Seeger  2002]  Catoni’s bound [Catoni  2007] and the works [Langford and Shawe-
Taylor  2002  Germain et al.  2009] on linear classiﬁers.

3 Learning Error Analyses

We will focus on structured prediction with linear discriminant functions. Deﬁne exact inference

h(x; w) = arg max

y2Y w

⊺

(cid:8)(x; y);

where (cid:8)(x; y) 2 Rd is the feature vector  and w is the parameter vector in Rd. We consider two
types of approximate inference algorithms  namely underestimate approximation and overestimate
approximation [Finley and Joachims  2008] 1.
Deﬁnition 1. Given a w  h-(x; w) is an underestimate approximation of h(x; w) on a sample x if

⊺

(cid:3)

) (cid:20) w

⊺

(cid:8)(x; y-) (cid:20) w

⊺

(cid:8)(x; y

(cid:26)w
= h(x; w); y- = h-(x; w) 2 Y. Similarly  h+(x; w) is an overestimate

(cid:8)(x; y

)

(cid:3)

for some (cid:26) > 0  where y
approximation of h(x; w) on sample x if

(cid:3)

(cid:3)

⊺

w

(cid:8)(x; y

(cid:3)

) (cid:20) w

⊺

(cid:8)(x; y+) (cid:20) (cid:26)w

⊺

(cid:8)(x; y

)

for some (cid:26) > 0  where y+ = h+(x; w) 2 (cid:22)Y and Y (cid:18) (cid:22)Y.
Let H;H-;H+ be hypothesis spaces containing h  h- and h+ respectively: H = fh((cid:1); w)jw 2
Rdg  H- = fh-((cid:1); w)j8x 2 X ; h-((cid:1); w) is an underestimationg  and H+ = fh+((cid:1); w)j8x 2
X ; h+((cid:1); w) is an overestimationg. Let l(y; ^y) 2 [0; 1] be a structured loss function on Y (cid:2) Y and
I((cid:1)) be a 0-1 valued function which equals 1 if the argument is true  0 otherwise.

3.1 Estimation Error

Our analysis of the estimation error for approximate inference is based on the PAC-Bayes results
for exact inference [McAllester  2007]. PAC-Bayes is a framework for analyzing hypothesis h((cid:1); w)
′ according to some
with stochastic parameters: given an input x  ﬁrst randomly select a parameter w
distribution Q(w
L(Q;D; h((cid:1); w)) = ED;Q(w′jw)l(y; h(x; w

′jw)  and then make a prediction using h(x; w

)); L(Q; S; h((cid:1); w)) =

EQ(w′jw)l(yi; h(xi; w

m∑

). Deﬁne

′

′

′

)):

1
m

i=1

√

Given some prior distribution P (w) on the model parameter w  the following PAC-Bayes Theorem
[McAllester  2003] gives an estimation error bound of h(x; w).
Lemma 2 (PAC-Bayes Theorem). Given a w  for any distribution D over X (cid:2) Y  loss function
l(y; ^y) 2 [0; 1]  prior distribution P (w) over w  and (cid:14) 2 [0; 1]  we have with probability at least
1 (cid:0) (cid:14) (over the sample set S)  the following holds for all posterior distribution Q(w

′jw):

DKL(Q∥P ) + ln m

L(Q;D; h((cid:1); w)) (cid:20) L(Q; S; h((cid:1); w)) +
where DKL(Q∥P ) is the KL divergence between Q and P .
1Deﬁnition 1 slightly generalizes “undergenerating” and “overgenerating” in [Finley and Joachims  2008].
Instead of requiring (cid:26) > 0  the “undergenerating” there only considers (cid:26) 2 (0; 1)  and “overgenerating” only
considers (cid:26) > 1. Although their deﬁnition is more intuitive (i.e.  the meaning of “over” and “under” is more
(cid:3)
) > 0 for all x and w  which limits the size of hypothesis space. Finally 
clear)  it implicitly assumes w
⊺
by adding a bias term  we could make w
) + b > 0 for all x  and obtain the same deﬁnitions in [Finley
and Joachims  2008].

2(m (cid:0) 1)

(cid:8)(x; y

(cid:8)(x; y

(cid:3)

⊺

;

(cid:14)

3

(cid:3)

; y-; w) (cid:20) 0 for underestimation  and m(cid:26)(x; y

Deﬁnition 3. For (cid:26) > 0  we extend the deﬁnition of margin as m(cid:26)(x; y; ^y; w) ≜ w
where ∆(cid:26)(x; y; ^y) ≜ (cid:26)(cid:8)(x; y) (cid:0) (cid:8)(x; ^y).
Clearly  m(cid:26)(x; y
The following theorem gives an analysis of the estimation error for approximate inference. The
proof (in the supplementary) is based on Theorem 2 of [McAllester  2007]  with emphasis on the
approximation rate (cid:26).
Theorem 4. For a training set S = f(xi; yi)gm
(xi; w) is a (cid:26)i-approximation of
h(xi; w) on xi for all w. Denote (cid:26) = maxi (cid:26)i and Mi = maxy ∥(cid:8)(xi; y)∥1. Then  for any D 
l(y; ^y) 2 [0; 1] and (cid:14) 2 [0; 1]  with probability at least 1 (cid:0) (cid:14)  the following upper bound holds.

; y+; w) (cid:21) 0 for overestimation.

′
i=1  assume h

∆(cid:26)(x; y; ^y) 

(cid:3)

⊺

√

∥w∥2
m

+

(1 + (cid:26))2∥w∥2 ln 2m(cid:21)S
2(m (cid:0) 1)

∥w∥2 + ln m

(cid:14)

;

(1)

L(Q;D; h

L(w; S) =

′

((cid:1); w)) (cid:20) L(w; S) +
m∑
m∑

i=1

8>><>>: 1

m

1
m

maxy l(yi; y)I(m(cid:26)i(xi; y

maxy l(yi; y)I(m(cid:26)i(xi; y

i ; y; w) (cid:20) Mi)
(cid:3)
i ; y; w) (cid:21) (cid:0)Mi)
(cid:3)

′
if h
′
if h

((cid:1); w) 2 H-
((cid:1); w) 2 H+

i=1

2 ln 2m(cid:21)S

where y
(cid:26))

′jw) is Gaussian with identity covariance matrix and mean (1 +
(cid:3)
i = h(xi; w)  Q(w
∥w∥2 w  (cid:21)S is the maximum number of non-zero features among samples in S: (cid:21)S =

√
maxi;y ∥(cid:8)(xi; y)∥0.
We compare the bound in Theorem 4 for two hypotheses h1; h2 with approximation rate (cid:26)1;i; (cid:26)2;i
(cid:3)
⊺
on sample xi. Without loss of generality  we assume w
i ) > 0 and (cid:26)1;i > (cid:26)2;i.
In the case of underestimation  since fyjm(cid:26)1;i(xi; y
i ; y; w) (cid:20)
i ; y; w) (cid:20) Mig (cid:18) fyjm(cid:26)2;i(xi; y
(cid:3)
(cid:3)
Mig  L(w; S) of h1 is smaller than that of h2  but h1 has a larger square root term. Thus  it is
possible that underestimate approximation has a less estimation error than the exact inference. On
the other hand  for overestimation  both L(w; S) and the square root term of h1 are larger than those
of h2. It means that the more overestimation an inference algorithm makes  the larger estimation
error it may suffer.
((cid:1); w) attains approximation rate (cid:26)i on xi for all possible w. This assump-
′
Theorem 4 requires that h
tion could be restrictive for including many approximate inference algorithms. We will try to relax
the requirement of Theorem 4 using the following measurement on stability of inference algorithms.
Deﬁnition 5. h(x; w) is (cid:28)-stable on a sample x with respect to a norm ∥ (cid:1) ∥ if for any w

(cid:8)(xi; y

′

⊺

jw

(cid:8)(x; y) (cid:0) w

′⊺
jw⊺(cid:8)(x; y)j

′

)j

(cid:8)(x; y

(cid:20) (cid:28)

′∥

∥w (cid:0) w
∥w∥

;

′

′

= h(x; w

where y = h(x; w); y
((cid:1); w)
′
Theorem 6. Assume that h
(xi; w) is a (cid:26)i-approximation of h(xi; w) on the sample xi  and h
is (cid:28)-stable on S with respect to ∥ (cid:1) ∥1. Then with the same symbols in Theorem 4  L(Q;D; h
((cid:1); w))
′
is upper bounded by

).

′

√

L(w; S) +

∥w∥2
m

+

(1 + 2(cid:26) + (cid:28) )2∥w∥2 ln 2m(cid:21)S

∥w∥2 + ln m

(cid:14)

2(m (cid:0) 1)

:

′ according to the deﬁnition of (cid:28). However  upper
Note that we still need to consider all possible w
bounds of (cid:28) could be derived for some approximate inference algorithms. As an example  we discuss
the linear programming relaxation (LP-relaxation) of integer linear programming  which covers a
broad range of approximate inference algorithms. The (cid:28)-stability of LP-relaxation can be obtained
from perturbation theory of linear programming [Renegar  1994  1995].
Theorem 7 (Proposition 2.5 of [Renegar  1995]). For a feasible linear programming

⊺

max : w

z

s.t. Az (cid:20) b; z (cid:21) 0;

4

1

′

1

′

1

1

′

2

3

3

′

2

1

2

(a)

1

(b)

′

3

′

1

′

3

2

(c)

2
(a)

′

2

3

1

3

3

2

(d)

′

3

(e)

(f)

2
(d)

′

2

(b)
′

2

(e)

′

1

′

1

3

′

1

′

3

′

3

1

′

3

′

2

2
(c)

(f)

Figure 1: An example of exact inference with
less approximation error than underestimate
inference (i.e.  e(h) < e(h-))

Figure 2: An example of underestimate infer-
ence with less approximation error than exact
inference (i.e.  e(h-) < e(h)).

let ^z  ^z

′ be solutions of the LP w.r.t. w and w

′. Then

⊺

jw

^z (cid:0) w

′⊺

^z

′j (cid:20) max(∥b∥1;jw

⊺

^zj)

∥w (cid:0) w

′∥1;

where d is the l1 distance from A; b to the dual infeasible LP (∥A; b∥1 = maxi;j;kfjAijj;jbkjg):
d = inff(cid:14)j∥∆A; ∆b∥1 < (cid:14) ) the dual problem of the LP with(A + ∆A; b + ∆b) is infeasibleg:

d

3.2 Approximation Error

In this section  we compare the approximation error of models with different inference algorithms.
The discussions are based on the following deﬁnition (Deﬁnition 1.1 of [Daniely et al.  2012]).
Deﬁnition 8. For hypothesis spaces H;H′  we say H essentially contains H′ if for any h
′ 2 H′ 
there is an h 2 H satisfying e(h) (cid:20) e(h
) for all D  where e(h) = EDl(y; h(x)). In other words 
′
for any distribution D  the approximation error of H is at most the error of H′.
Our main result is that there exist cases that approximation errors of exact and underestimate infer-
ence are not comparable  in the sense that neither H contains H-  nor H- contains H. 2
To see that approximation errors could be non-comparable  we consider an approximate inference
algorithm h- which always outputs the second best y for a given w. The two examples in Figure 1
and 2 demonstrate that it is both possible that e(h) < e(h-) and e(h-) < e(h). The following are the
details.
′g. Sample x has three possible
We consider an input space containing two samples X = fx; x
′ also has three possible y 
output structures  which are named with 1; 2; 3 respectively. Sample x
′ be 1 and 1
′. For sample x  feature
′. Let the correct output of x and x
′
which are named with 1
vectors (cid:8)(x; 1); (cid:8)(x; 2); (cid:8)(x; 3) 2 R2 are points on the unit circle and form a equilateral triangle
) are vertices of △(1
△(1; 2; 3). Similarly  feature vectors (cid:8)(x
′
′
). The
; 3
parameter space of w is the unit circle (since inference results only depend on directions of w).
Given a w  the exact inference h(x; w) choose the y whose (cid:8)(x; y) has the largest projection on w
(i.e.  h(x; w) = arg maxy2f1;2;3g w
; y))  and
h-(x; w) choose the y whose (cid:8)(x; y) has the second largest projection on w.

; w) = arg maxy2f1′;2′;3′g w

(cid:8)(x; y) and h(x

); (cid:8)(x

′

′
; 2

′

′
; 3

′

′
; 1

); (cid:8)(x

; 2

; 3

′

; 2

⊺

⊺

′

(cid:8)(x

′

′

2 Note that there exist two paradigms for handling intractability of inference problems. The ﬁrst one is
to develop approximate inference algorithms for the exact problem  which is our focus here. Another one
is to develop approximate problems with tractable exact inference algorithms. For example  in probabilistic
graphical models  one can add conditional independent assumptions to get a simpliﬁed model with efﬁcient
inference algorithms. In the second paradigm  it is clear that approximate models are less expressive than the
exact model  thus the approximation error of them are always larger. Our result  however  shows that it is
possible to have underestimate inference of the original problem with smaller approximation error.

5

′

; w) = 1

; w) = 1

′  which means that approximation error of exact inference H is 0.
′

We ﬁrst show that it is possible e(h) < e(h-). In Figure 1  (a) shows that for sample x  any w in
the gray arc can make the output of exact inference correct (i.e.  h(x; w) = 1). Similarly  in (b) 
′. (c) shows that the two gray arcs in (a) and (b) are
any w in the gray arc guarantees h(x
overlapping on the dark arc. For any w in the dark arc  the exact inference has correct outputs on
both x and x
At the same time  in (d) of Figure 1  gray arcs contain w which makes the underestimate inference
′. (f) shows
correct on sample x (i.e.  h-(x; w) = 1)  gray arcs in (e) are w with h-(x
the gray arcs in (d) and (e) are not overlapping  which means it is impossible to ﬁnd a w such that
′. Thus the approximation error of underestimate inference H- is
h-((cid:1); w) is correct on both x and x
strictly larger than 0  and we have e(h) < e(h-).
Similarly  in Figure 2  (a)  (b)  (c) show that we are able to choose w such that the underestimate
′  which implies the approximation error of underestimation H-
inference is correct both on x and x
equals 0. On the other hand  (d)  (e)  (f) shows that the approximation error of exact inference H is
strictly larger than 0  and we have e(h-) < e(h).
Following the two ﬁgures  we can illustrate that when (cid:8)(x; y) are vertices of convex regular n-gons 
it is both possible that e(h) < e(h-) and e(h-) < e(h)  where h- is an underestimation outputting the
k-th best y. In fact  when we consider the “worst” approximation which outputs y with the smallest
score  its approximation error equals to the exact inference since h(x; w) = h-(x;(cid:0)w). Thus  we
would like to think that the geometry structures of (cid:8)(x; y) could be complex enough to make both
exact and underestimate inference efﬁcient.
To summarize  the examples suggest that underestimation algorithms give us a different family of
predictors. For some data distribution  the underestimation family can have a better predictor than
the exact inference family.
Finally  for the case of overestimate approximation  we can show that H+ contains H using Theorem
1 of [Kulesza and Pereira  2007].
Theorem 9. For (cid:26) > 1  if the loss function l satisﬁes l(y1; y2) (cid:20) l(y1; y3) + l(y3; y2)  then H+
contains H.

4 Training with the New Margin

Theorems 4 and 6 suggest that we could learn the model parameter w by minimizing a non-convex
objective L(w; S) + ∥w∥2. The L(w; S) term is related to the size of the set fyjm(cid:26)(xi; y
i ; y; w) (cid:20)
(cid:3)
Mig  which can be controlled by margin m(cid:26)2(xi; yi; y-
i). Speciﬁcally  for underestimation 

m(cid:26)(xi; y

i ; y; w)(cid:21) (cid:26)w
(cid:3)
(cid:21) (cid:26)w

⊺
⊺

(cid:8)(xi; yi) (cid:0) w
(cid:8)(xi; yi) (cid:0) (cid:26)

(cid:8)(xi; y) (cid:21) (cid:26)w
⊺
(cid:0)1w

(cid:8)(xi; y-

⊺

⊺

i) = (cid:26)

⊺

(cid:8)(xi; yi) (cid:0) w
(cid:0)1m(cid:26)2 (xi; yi; y-

(cid:8)(xi; y
i; w); 8y:

(cid:3)
i )

∑

i) (replacing exact y

i (cid:24)i; s.t. m(cid:26)2(xi; yi; y-

i ; w) > 1 ) m1(xi; yi; y

(cid:3)
i with the approximate y-
jjwjj2 + C

i)  the lower L(w; S). Thus  when working with approxi-
It implies that the larger m(cid:26)2 (xi; yi; y-
mate inference  we can apply m(cid:26)2(xi; yi; y-
i) in existing maximum margin frameworks instead of
m1(xi; yi; y-
i). For example  the structural SVM in
i; w) > 1 (cid:0) (cid:24)i.
[Finley and Joachims  2008] becomes min : 1
2
Intuitively  m(cid:26)2 aims to improve learning process by including more information about inference
algorithms. For overestimation  we don’t have similar lower bounds as underestimation  but since
(cid:0)1  we can apply the margin m(cid:26) as an approxima-
m(cid:26)(xi; yi; y+
tion of m1.
In practice  since it is hard to obtain (cid:26) for inference algorithms (even it is possible  as (cid:26) must consider
the worst case of all possible x  a tight (cid:26) maybe inefﬁcient on individual samples)  we treat it as an
algorithm parameter which can be heuristically determined either by prior knowledge or by tuning
on development data. We leave the study of how to estimate (cid:26) systematically for future work.
For empirical evaluation  we examine structural SVM with cutting plane learning algorithm [Finley
and Joachims  2008]  and we also adapt two wildly used online structured learning algorithms with
m(cid:26): structured perceptron [Collins  2002] (Algorithm 3) and online passive aggressive algorithm
(PA) [Crammer et al.  2006] (Algorithm 4). The mistake bounds of the two algorithms are similar to
bounds with exact inference algorithms (given in the supplementary).

(cid:3)
i ; w) > (cid:26)

6

1: w0 = 0
2: for t = 0 to T do
t = h-(xt; wt)
y-
3:
̸= yt then
if y-
4:
wt+1 = wt +(cid:26)(cid:8)(xt; yt)(cid:0)(cid:8)(xt; y-
t
5:
t)
end if
6:
7: end for
8: return w = wT
Figure 3: Structured perceptron with m(cid:26).

1: w0 = 0
2: for t = 0 to T do
if m(cid:26)(xt; yt; y-
3:
wt+1 = arg minw: ∥w (cid:0) wt∥2
4:
s.t. m(cid:26)(xt; yt; y-
5:
end if
6:
7: end for
8: return w = wT

t; w) < 1 then
t; w) (cid:21) 1

Figure 4: Online PA with m(cid:26).

5 Experiments

We present experiments on three natural language processing tasks: multi-class text classiﬁcation 
sequential labelling and dependency parsing. For text classiﬁcation  we compare with the vanilla
structural SVM. For sequential labelling  we consider three tasks (phrase chunking (chu)  POS
tagging (pos) and Chinese word segmentation (cws)) and the perceptron training. For dependency
parsing  we focus on the second order non-projective parser and the PA algorithm. For each task 
we focus on underestimate inference algorithms.

5.1 Multi-class classiﬁcation

Multi-class classiﬁcation is a special case of structured prediction. It has a limited number of class
labels and a simple exact inference algorithm (i.e.  by enumerating labels). To evaluate the proposed
margin constraints  we consider toy approximate algorithms which output the kth best class label.
We report results on the 20 newsgroups corpus
(18000 documents  20 classes). The meta data
is removed (headers  footers and quotes)  and
feature vectors are simple tf-idf vectors. We
take 20% of the training set as development
set for tuning (cid:26) (grid search in [0  2] with step
size 0.05). The implementation is adapted from
SVMmulticlass 3.
From the results (Figure 5) we ﬁnd that  com-
paring with the vanilla structural SVM  the pro-
posed margin constraints are able to improve
error rates for different inference algorithms.
And  as k becomes larger  the improvement be-
comes more signiﬁcant. This property might be
attractive since algorithms with loose approxi-
mation rates are common in practical use. An-
other observation is that  as k becomes larger 
the best parameter (cid:26) decreases in general.
It
shows that the tuned parameter can reﬂect the
deﬁnition of approximate rate (Defnition 1).

Figure 5: Results on text classiﬁcation. Blue
points are error rates for different k  and red points
are (cid:26) achieving the best error rates on the devel-
opment set. The red dot line is the least square
linear ﬁtting of red points. The model parameter
C = 104.

5.2 Sequential Labelling
In sequential labelling  we predict sequences y = y1y2; : : : ; yK  where yk 2 Y is a label (e.g.  POS
(cid:8)(x; yk; yk(cid:0)1).
⊺
K
tag). We consider the ﬁrst order Markov assumption: h(x) = arg maxy
k=1 w
The inference problem is tractable using O(KY 2) dynamic programming (Viterbi).
We examine a simple and fast greedy iterative decoder (“gid”)  which is also known as the iterative
conditional modes [Besag  1986]. The algorithm ﬂips each label yk of y in a greedy way: for ﬁxed
yk(cid:0)1 and yk+1  it ﬁnds a yk that makes the largest increase of the decoding objective function. The

∑

3http://www.cs.cornell.edu/People/tj/svm_light/svm_multiclass.html

7

2345678k405060708090ErrorratesVanillaSVMSVMwithmρ0.400.450.500.550.600.650.700.75ρFigure 6: Results of sequential labelling tasks with Algorithm 3. The x-axis represents the random
selection parameters u. The y-axis represents label accuracy.

algorithm passes the sequence multiple times and stops when no yk can be changed. It is faster in
practice (speedup of 18x on POS tagging  1:5x on word segmentation)  requires less memory (O(1)
space complexity)  and can obtain a reasonable performance.
We use the CoNLL 2000 dataset [Sang and Buchholz  2000] for chunking and POS tagging 
SIGHAN 2005 bake-off corpus (pku and msr) [Emerson  2005] for word segmentation. We use
Algorithm 3 with 20 iterations and learning step 1. We adopt standard feature sets in all tasks.
To test (cid:26) on more inference algorithms  we will apply a simple random selection strategy to gen-
erate a bunch of in-between inference algorithms: when decoding an example  we select “Viterbi”
with probability u  “gid” with probability 1 (cid:0) u. Heuristically  by varying u  we obtain inference
algorithms with different expected approximation rates.
Figure 6 shows the results of (cid:26) (cid:20) 1 4. We can have following observations:
(cid:15) At u = 0 (i.e.  inference with “gid”)  models with (cid:26) < 1 are signiﬁcantly better than (cid:26) = 1 on
pos and cws (p < 0:01 using z-test for proportions). Furthermore  on pos and cws  the best “gid”
results with parameters (cid:26) < 1 are competitive to the standard perceptron with exact inference (i.e. 
(cid:26) = 1; u = 1). Thus  it is possible for approximate inference to be both fast and good.
(cid:15) For 0 < u < 1  we can ﬁnd that curves of (cid:26) < 1 are above the curve of (cid:26) = 1 in many cases. The
largest gap is 0:2% on chu  0:6% on pos and 2% on cws. Thus  the learning parameter (cid:26) can also
provide performance gains for the combined inference algorithms.
(cid:15) For u = 1 (i.e.  using the “Viterbi”)  it is interesting to see that in pos  (cid:26) < 1 still outperforms
(cid:26) = 1 by a large margin. We suspect that the (cid:26) parameter might also help the structured perceptron
converging to a better solution.

5.3 Dependency Parsing

Our third experiment is high order non-projective dependency parsing  for which the exact inference
is intractable. We follows the approximate inference in MSTParser [McDonald and Pereira  2006]
5. The algorithm ﬁrst ﬁnds the best high order projective tree using a O(n3) dynamic programming
[Eisner  1996]  then heuristically introduces non-projective edges on the projective tree.
We use the online PA in Algorithm 4 with above two-phase approximate inference algorithm. The
parser is trained and tested on 5 languages in the CoNLL-2007 shared task [Nivre et al.  2007] with
non-projective sentences more than 20%. Features are identical to default MSTParser settings 6.
Figure 1 lists the results with different (cid:26). It shows that on all languages  tuning the parameter helps
to improve the parsing accuracy. As a reference  we also include results of the ﬁrst order models.
On Basque and Greek  the performance gains from (cid:26) is comparable to the gains from introducing
second order features  but the improvement on Czech  Hungarian and Turkish are limited. We also
ﬁnd that different with text classiﬁcation and sequential labelling  both (cid:26) > 1 and (cid:26) < 1 can obtain
optimal scores. Thus  with the feature conﬁguration of MSTParser  the value of w
) may
not always be positive during the online learning process  and it reﬂect the fact that feature space of

(cid:8)(x; y

⊺

(cid:3)

4We also test models with (cid:26) > 1  which underperform (cid:26) < 1 in general. Details are in the supplementary.
5http://sourceforge.net/projects/mstparser/
6Features in MSTParser are less powerful than state-of-the-art  but we keep them for an easier implementa-

tion and comparison.

8

0.00.20.40.60.81.0u0.9520.9530.9540.9550.9560.9570.9580.959Accuracychuρ=0.9ρ=0.99ρ=0.999ρ=10.00.20.40.60.81.0u0.9440.9450.9460.9470.9480.9490.9500.9510.952pos0.00.20.40.60.81.0u0.9250.9300.9350.9400.9450.9500.9550.960cws-pku0.00.20.40.60.81.0u0.9450.9500.9550.9600.9650.970cws-msrparsing problems is usually more complex. Finally  setting a global (cid:26) for different training samples
could be coarse (so we only get improvement in a small neighborhood of 1)  and how to estimate (cid:26)
for individual x is an important future work.

Basque Czech Greek Hungarian Turkish

Setting
1st Order
(cid:26) =1
(cid:26) =1(cid:0)10
(cid:26) =1(cid:0)10
(cid:26) =1+10
(cid:26) =1+10

(cid:0)3
(cid:0)4
(cid:0)3
(cid:0)4

79.4
79.8
79.7
80.3
79.4
79.6

82.1
82.8
83.0
82.9
82.3
83.0

81.1
81.7
81.3
82.2
81.5
82.5

79.9
81.7
81.1
81.8
80.7
81.6

85.0
85.5
85.2
85.7
85.6
85.4

Table 1: Results of the second order dependency parsing with parameter (cid:26). We report the unlabelled
attachment score (UAS)  which is the percentage of words with correct parents.

6 Conclusion

We analyzed the learning errors of structured prediction models with approximate inference. For the
estimation error  we gave a PAC-Bayes analysis for underestimation and overestimation inference
algorithms. For the approximation error  we showed the incomparability between exact and underes-
timate inference. The experiments on three NLP tasks with the newly proposed learning algorithms
showed encouraging performances. In future work  we plan to explore more adaptive methods for
estimating approximation rate (cid:26) and combining inference algorithms.

Acknowledgements

The authors wish to thank all reviewers for their helpful comments and suggestions. The corre-
sponding authors are Man Lan and Shiliang Sun. This research is (partially) supported by NSFC
(61402175  61532011)  STCSM (15ZR1410700) and Shanghai Key Laboratory of Trustworthy
Computing (07dz22304201604). Yuanbin Wu is supported by a Microsoft Research Asia Collab-
orative Research Program.

References
Julian Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society B 

48(3):48–259  1986.

Olivier Catoni. PAC-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learn-

ing  volume 56 of Lecture Notes-Monograph Series. IMS  2007.

Venkat Chandrasekaran and Michael I. Jordan. Computational and statistical tradeoffs via convex

relaxation. In Proc. of the National Academy of Sciences  volume 110  2013.

Kai-Wei Chang  Shyam Upadhyay  Gourab Kundu  and Dan Roth. Structural learning with amor-

tized inference. In Proc. of AAAI  pages 2525–2531  2015.

Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of
distribution-free methods. In Proc. of the Seventh International Workshop on Parsing Technolo-
gies  2001.

Michael Collins. Discriminative training methods for hidden Markov models: Theory and experi-

ments with perceptron algorithms. In Proc. of EMNLP  pages 1–8  2002.

Corinna Cortes  Vitaly Kuznetsov  Mehryar Mohri  and Scott Yang. Structured prediction theory

based on factor graph complexity. In NIPS  pages 2514–2522  2016.

Koby Crammer  Ofer Dekel  Joseph Keshet  Shai Shalev-Shwartz  and Yoram Singer. Online

passive-aggressive algorithms. Journal of Machine Learning Research  7:551–585  2006.

9

Amit Daniely  Sivan Sabato  and Shai Shalev-Shwartz. Multiclass learning approaches: A theoreti-

cal comparison with implications. In NIPS  pages 494–502  2012.

Jason M. Eisner. Three new probabilistic models for dependency parsing: An exploration. In Proc.

of COLING  1996.

Thomas Emerson. The second international Chinese word segmentation bakeoff.

SIGHAN Workshop on Chinese Language Processing  pages 123 – 133  2005.

In the Second

Thomas Finley and Thorsten Joachims. Training structural SVMs when exact inference is intractable.

In Proc. of ICML  pages 304–311  2008.

Pascal Germain  Alexandre Lacasse  François Laviolette  and Mario Marchand. PAC-Bayesian learn-

ing of linear classiﬁers. In Proc. of ICML  pages 353–360  2009.

Amir Globerson  Tim Roughgarden  David Sontag  and Cafer Yildirim. How hard is inference for

structured prediction? In Proc. of ICML  pages 2181–2190  2015.

Jean Honorio and Tommi S. Jaakkola. Structured prediction: From gaussian perturbations to linear-

time principled algorithms. In Proc. of UAI  2016.

Liang Huang  Suphan Fayong  and Yang Guo. Structured perceptron with inexact search. In Proc.

of HLT-NAACL  pages 142–151  2012.

Alex Kulesza and Fernando Pereira. Structured learning with approximate inference. In NIPS  pages

785–792  2007.

Gourab Kundu  Vivek Srikumar  and Dan Roth. Margin-based decomposed amortized inference. In

Proc. of ACL  pages 905–913  2013.

John Langford and John Shawe-Taylor. PAC-Bayes & margins. In NIPS  pages 423–430  2002.

Ben London  Bert Huang  Ben Taskar  and Lise Getoor. Collective stability in structured prediction:

Generalization from one example. In Proc. of ICML  pages 828–836  2013.

André F. T. Martins  Noah A. Smith  and Eric P. Xing. Polyhedral outer approximations with appli-

cation to natural language parsing. In Proc. of ICML  pages 713–720  2009.

David McAllester. Generalization Bounds and Consistency for Structured Labeling  chapter Pre-

dicting Structured Data. MIT Press  2007.

David A. McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999.

David A. McAllester. Pac-bayesian stochastic model selection. Machine Learning  51(1):5–21 

2003.

David A. McAllester and Joseph Keshet. Generalization bounds and consistency for latent structural

probit and ramp loss. In NIPS  pages 2205–2212  2011.

Ryan McDonald and Fernando Pereira. Online learning of approximate dependency parsing algo-

rithms. In Proc. of EACL  2006.

Ofer Meshi  David Sontag  Tommi S. Jaakkola  and Amir Globerson. Learning efﬁciently with

approximate inference via dual losses. In Proc. of ICML  pages 783–790  2010.

Ofer Meshi  Mehrdad Mahdavi  Andrian Weller  and David Sontag. Train and test tightness of lp

relaxations in structured prediction. In Proc. of ICML  2016.

Joakim Nivre  Johan Hall  Sandra Kübler  Ryan McDonald  Jens Nilsson  Sebastian Riedel  and
In Proc. of the CoNLL

Deniz Yuret. The CoNLL 2007 shared task on dependency parsing.
Shared Task Session of EMNLP-CoNLL 2007  pages 915–932  2007.

James Renegar. Some perturbation theory for linear programming. Mathematical Programming  65:

73–91  1994.

10

James Renegar. Incorporating condition measures into the complexity theory of linear programming.

SIAM Journal on Optimization  5(3):506–524  1995.

Rajhans Samdani and Dan Roth. Efﬁcient decomposed learning for structured prediction. In Proc.

of ICML  2012.

Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: Chunking.

In Proc. of CoNLL and LLL  2000.

Matthias Seeger. PAC-Bayesian generalisation error bounds for gaussian process classiﬁcation.

JMLR  3:233–269  2002.

David Sontag  Ofer Meshi  Tommi S. Jaakkola  and Amir Globerson. More data means less infer-

ence: A pseudo-max approach to structured learning. In NIPS  pages 2181–2189  2010.

Benjamin Taskar  Carlos Guestrin  and Daphne Koller. Max-margin Markov networks. In NIPS 

pages 25–32  2003.

Zhuoran Wang and John Shawe-Taylor. Large-margin structured prediction via linear programming.

In Proc. of AISTATS  pages 599–606  2009.

11

,Yuanbin Wu
Shiliang Sun
Qi Zhang
Xuanjing Huang