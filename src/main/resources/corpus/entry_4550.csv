2019,Random deep neural networks are biased towards simple functions,We prove that the binary classifiers of bit strings generated by random wide deep neural networks with ReLU activation function are biased towards simple functions. The simplicity is captured by the following two properties. For any given input bit string  the average Hamming distance of the closest input bit string with a different classification is at least sqrt(n / (2π log n))  where n is the length of the string. Moreover  if the bits of the initial string are flipped randomly  the average number of flips required to change the classification grows linearly with n. These results are confirmed by numerical experiments on deep neural networks with two hidden layers  and settle the conjecture stating that random deep neural networks are biased towards simple functions. This conjecture was proposed and numerically explored in [Valle Pérez et al.  ICLR 2019] to explain the unreasonably good generalization properties of deep learning algorithms. The probability distribution of the functions generated by random deep neural networks is a good choice for the prior probability distribution in the PAC-Bayesian generalization bounds. Our results constitute a fundamental step forward in the characterization of this distribution  therefore contributing to the understanding of the generalization properties of deep learning algorithms.,Random deep neural networks are biased towards

simple functions

Bobak T. Kiani
MechE & RLE

MIT

Seth Lloyd

MechE  Physics & RLE

MIT

Giacomo De Palma

MechE & RLE

MIT

Cambridge MA 02139  USA

Cambridge MA 02139  USA

Cambridge MA 02139  USA

gdepalma@mit.edu

bkiani@mit.edu

slloyd@mit.edu

Abstract

We prove that the binary classiﬁers of bit strings generated by random wide deep
neural networks with ReLU activation function are biased towards simple functions.
The simplicity is captured by the following two properties. For any given input
bit string  the average Hamming distance of the closest input bit string with a

different classiﬁcation is at least(cid:112)n/(2π ln n)  where n is the length of the string.

Moreover  if the bits of the initial string are ﬂipped randomly  the average number
of ﬂips required to change the classiﬁcation grows linearly with n. These results
are conﬁrmed by numerical experiments on deep neural networks with two hidden
layers  and settle the conjecture stating that random deep neural networks are biased
towards simple functions. This conjecture was proposed and numerically explored
in [Valle Pérez et al.  ICLR 2019] to explain the unreasonably good generalization
properties of deep learning algorithms. The probability distribution of the functions
generated by random deep neural networks is a good choice for the prior probability
distribution in the PAC-Bayesian generalization bounds. Our results constitute
a fundamental step forward in the characterization of this distribution  therefore
contributing to the understanding of the generalization properties of deep learning
algorithms.

1

Introduction

The ﬁeld of deep learning provides a broad family of algorithms to ﬁt an unknown target function via
a deep neural network and is having an enormous success in the ﬁelds of computer vision  machine
learning and artiﬁcial intelligence [1–5]. The input of a deep learning algorithm is a training set 
which is a set of inputs of the target function together with the corresponding outputs. The goal of the
learning algorithm is to determine the parameters of the deep neural network that best reproduces the
training set.
Deep learning algorithms generalize well when trained on real-world data [6]: the deep neural
networks that they generate usually reproduce the target function even for inputs that are not part of
the training set and do not suffer from over-ﬁtting even if the number of parameters of the network is
larger than the number of elements of the training set [7–10]. A thorough theoretical understanding
of this unreasonable effectiveness is still lacking. The bounds to the generalization error of learning
algorithms are proven in the probably approximately correct (PAC) learning framework [11]. Most of
these bounds depend on complexity measures such as the Vapnik-Chervonenkis dimension [12 13] or
the Rademacher complexity [14  15] which are based on the worst-case analysis and are not sufﬁcient
to explain the observed effectiveness since they become void when the number of parameters is
larger than the number of training samples [10  16–21]. A complementary approach is provided
by the PAC-Bayesian generalization bounds [19  22–25]  which apply to nondeterministic learning

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

algorithms. These bounds depend on the Kullback-Leibler divergence [26] between the probability
distribution of the function generated by the learning algorithm given the training set and an arbitrary
prior probability distribution that is not allowed to depend on the training set: the smaller the
divergence  the better the generalization properties of the algorithm. Making the right choice for the
prior distribution is fundamental to obtain a nontrivial generalization bound.
A good choice for the prior distribution is the probability distribution of the functions generated
by deep neural networks with randomly initialized weights [27]. Understanding this distribution is
therefore necessary to understand the generalization properties of deep learning algorithms. PAC-
Bayesian generalization bounds with this prior distribution led to the proposal that the unreasonable
effectiveness of deep learning algorithms arises from the fact that the functions generated by a random
deep neural network are biased towards simple functions [27–29]. Since real-world functions are
usually simple [30  31]  among all the functions that are compatible with a training set made of
real-world data  the simple ones are more likely to be close to the target function. The conjectured
bias towards simple functions has been numerically explored in [27]  which considered binary
classiﬁcations of bit strings and showed that binary classiﬁers with a small Lempel-Ziv complexity
[32] are more likely to be generated by a random deep neural network than binary classiﬁers with a
large Lempel-Ziv complexity. However  a rigorous proof of this bias is still lacking.

1.1 Our contribution

We prove that random deep neural networks are biased towards simple functions  in the sense that
a typical function generated is insensitive to large changes in the input. We consider random deep
neural networks with Rectiﬁed Linear Unit (ReLU) activation function and weights and biases drawn
from independent Gaussian probability distributions  and we employ such networks to implement
binary classiﬁers of bit strings. Our main results are the following:

• We prove that for n (cid:29) 1  where n is the length of the string  for any given input bit string
the average Hamming distance of the closest bit string with a different classiﬁcation is at

least(cid:112)n/(2π ln n) (Theorem 1)  where the Hamming distance between two bit strings is

the number of different bits.

• We prove that  if the bits of the initial string are randomly ﬂipped  the average number of
bit ﬂips required to change the classiﬁcation grows linearly with n (Theorem 2). From
a heuristic argument  we ﬁnd that the average required number of bit ﬂips is at least n/4
(subsection 3.3)  and simulations on deep neural networks with two hidden layers indicate a
scaling of approximately n/3.

By contrast  for a random binary classiﬁer drawn from the uniform distribution over all the possible
binary classiﬁers of strings of n (cid:29) 1 bits  the average Hamming distance of the closest bit string
with a different classiﬁcation is one  and the average number of random bit ﬂips required to change
the classiﬁcation is two. Therefore  our result identiﬁes a fundamental qualitative difference between
a typical binary classiﬁer generated by a random deep neural network and a uniformly random binary
classiﬁer.
The result proves that the binary classiﬁers generated by random deep neural networks are simple
and identiﬁes the classiﬁers that are likely to be generated as the ones with the property that a large
number of bits need to be ﬂipped in order to change the classiﬁcation. While all the classiﬁers with
this property have a low Kolmogorov complexity1  the converse is not true. For example  the parity
function has a small Kolmogorov complexity  but it is sufﬁcient to ﬂip just one bit of the input to
change the classiﬁcation  hence our result implies that it occurs with a probability exponentially small
in n. Similarly  our results explain why [27] found that the look-up tables for the functions generated
by random deep networks are typically highly compressible using the LZW algorithm [35]  which
identiﬁes statistical regularities  but not all functions with highly compressible look-up tables are
likely to be generated.
The proofs of Theorems 1 and 2 are based on the approximation of random deep neural networks as
Gaussian processes  which becomes exact in the limit of inﬁnite width [36–47]. The crucial property
of random deep neural networks captured by this approximation is that the outputs generated by

1The Kolmogorov complexity of a function is the length of the shortest program that implements the function

on a Turing machine [26  33  34].

2

inputs whose Hamming distance grows sub-linearly with n become perfectly correlated in the limit
n → ∞. These strong correlations are the reason why a large number of input bits need to be ﬂipped
in order to change the classiﬁcation. The proof of Theorem 2 also exploits the theory of stochastic
processes  and in particular the Kolmogorov continuity theorem [48]. We stress that for activation
functions other than the ReLU  the scaling with n of both the Hamming distance of the closest bit
string with a different classiﬁcation and the number of random bit ﬂips necessary to change the
classiﬁcation remain the same. However  the prefactor can change and can be exponentially small in
the number of hidden layers.
We validate all the theoretical results with numerical experiments on deep neural networks with ReLU

activation function and two hidden layers. The experiments conﬁrm the scalings Θ((cid:112)n/ ln n) and

√
Θ(n) for the Hamming distance of the closest string with a different classiﬁcation and for the average
random ﬂips required to change the classiﬁcation  respectively. The theoretical pre-factor 1/
2π
for the closest string with a different classiﬁcation is conﬁrmed within an extremely small error of
1.5%. The heuristic argument that pre-factor for the random ﬂips is greater than 1/4 is conﬁrmed
by numerics which indicate that the pre-factor is approximately 0.33. Moreover  we explore the
Hamming distance to the closest bit string with a different classiﬁcation on deep neural networks
trained on the MNIST database [49] of hand-written digits. The experiments show that the scaling

Θ((cid:112)n/ ln n) survives after the training of the network and that the distance of a training or test

picture from the closest classiﬁcation boundary is strongly correlated with its classiﬁcation accuracy 
i.e.  the correctly classiﬁed pictures are further from the boundary than the incorrectly classiﬁed ones.

1.2 Further related works

The properties of deep neural networks with randomly initialized weights have been the subject of
intensive studies [38–42  50–52]. The relation between generalization and simplicity for Boolean
function was explored in [53]  where the authors provide numerical evidence that the generalization
error is correlated with a complexity measure that they deﬁne. Ref. [10] explores the generalization
properties of deep neural networks trained on partially random data  and ﬁnds that the generalization
error correlates with the amount of randomness in the data. Based on this result  Ref. [28 54] proposed
that the stochastic gradient descent employed to train the network is more likely to ﬁnd the simpler
functions that match the training set rather than the more complex ones. However  further studies [29]
suggested that stochastic gradient descent is not sufﬁcient to justify the observed generalization. The
idea of a bias towards simple patterns has been applied to learning theory through the concepts of
minimum description length [55]  Blumer algorithms [56  57] and universal induction [34]. Ref. [58]
proved that the generalization error grows with the Kolmogorov complexity of the target function if
the learning algorithm returns the function that has the lowest Kolmogorov complexity among all
the functions compatible with the training set. The relation between generalization and complexity
has been further investigated in [30  59]. The complexity of the functions generated by a deep neural
networks has also been studied from the perspective of the number of linear regions [60–62] and of
the curvature of the classiﬁcation boundaries [41]. We note that the results proved here — viz.  that
the functions generated by random deep networks are insensitive to large changes in their inputs —
implies that such functions should be simple with respect to all the measures of complexity above  but
the converse is not true: not all simple functions are likely to be generated by random deep networks.

2 Setup and Gaussian process approximation

We consider a feed-forward deep neural network with L hidden layers  activation function τ  input
in Rn and output in R. The most common choice for τ is the ReLU activation function τ (x) =
max(0  x). We stress that Theorems 1 and 2 do not rely on this assumption and hold for any activation
function. For any x ∈ Rn and l = 2  . . .   L + 1  the network is recursively deﬁned by
+ b(l)  

(1)
where φ(l)(x)  b(l) ∈ Rnl  W (l) is an nl × nl−1 real matrix  n0 = n and nL+1 = 1. We put for
simplicity φ = φ(L+1)  and we deﬁne ψ(x) = sign (φ(x)) for any x ∈ Rn. The function ψ is a
binary classiﬁer on the set of the strings of n bits identiﬁed with the set {−1  1}n ⊂ Rn  where
the classiﬁcation of the string x ∈ {−1  1}n is ψ(x) ∈ {−1  1}. We choose this representation of
the bit strings since any x ∈ {−1  1}n has (cid:107)x(cid:107)2 = n  and the covariance of the Gaussian process

φ(1)(x) = W (1)x + b(1)  

(cid:16)

φ(l)(x) = W (l) τ

φ(l−1)(x)

(cid:17)

3

w/nl−1 and σ2

approximating the deep neural network has a signiﬁcantly simpler expression if all the inputs have the
same norm. Moreover  having the inputs lying on a sphere is a common assumption in the machine
learning literature [63].
We draw each entry of each W (l) and of each b(l) from independent Gaussian distributions with zero
mean and variances σ2
b   respectively. We employ the Gaussian process approximation
of [41  42]  which consists in assuming that for any l and any x  y ∈ Rn  the joint probability
distribution of φ(l)(x) and φ(l)(y) is Gaussian  and φ(l)
j (y) for any
i (cid:54)= j. This approximation is exact for l = 1 and holds for any l in the limit n1  . . .   nL → ∞
[39]. Indeed  φ(l)
  which has a Gaussian distribution  with the nl−1 terms
j=1 which are iid from the inductive hypothesis. Therefore if nl−1 (cid:29) 1  from
{W (l)
the central limit theorem φ(l)
i (x) has a Gaussian distribution. We notice that for ﬁnite width  the
outputs of the intermediate layers have a sub-Weibull distribution [64]. Our experiments in section 4
show agreement with the Gaussian approximation starting from n (cid:38) 100.
In the Gaussian process approximation  for any x  y with (cid:107)x(cid:107)2 = (cid:107)y(cid:107)2 = n  the joint probability
distribution of φ(x) and φ(y) is Gaussian with zero mean and covariance that depends on x  y and n
only through x · y/n:

i (x) is the sum of b(l)
(x))}nl−1

i (x) is independent from φ(l)

ij τ (φ(l−1)

j

i

(cid:107)x(cid:107)2 = (cid:107)y(cid:107)2 = n .

E (φ(x)) = 0  

n

n

(2)
Analogously  φ(x) is a Gaussian process with zero average and covariance given by the kernel

(cid:1). Here Q > 0 is a suitable constant and F : [−1  1] → R is a suitable function

K(x  y) = Q F(cid:0) x·y

that depend on τ  L  σw and σb  but not on n  x nor y. We have introduced the constant Q because it
will be useful to have F satisfy F (1) = 1. We provide the expression of Q and F in terms of τ  L 
σw and σb in the supplementary material  where we also prove that for the ReLU activation function
t ≤ F (t) ≤ 1.
The correlations between outputs of the network generated by close inputs are captured by the
behavior of F (t) for t → 1. If F (t) stays close to 1 as t departs from 1  then the outputs generated by
close inputs are almost perfectly correlated and have the same classiﬁcation with probability close to
one. On the contrary  if F (t) drops quickly  the correlations decay and there is a nonzero probability
that close inputs have different classiﬁcations. In the supplementary material we prove that for the
ReLU activation function we have 0 < F (cid:48)(1) ≤ 1 and for t → 1 

E (φ(x) φ(y)) = Q F(cid:0) x·y

(cid:1)  

(1 − t)

3
2

 

(3)

(cid:16)

(cid:17)

F (t) = 1 − F (cid:48)(1) (1 − t) + O

implying strong short-distance correlations.

3 Theoretical results

3.1 Closest bit string with a different classiﬁcation
Our ﬁrst main result is the following Theorem 1  which states that for n (cid:29) 1  for any given input
bit string of a random deep neural network as in section 2 the average Hamming distance of the

closest input bit string with a different classiﬁcation is(cid:112)n/(2πF (cid:48)(1) ln n). The proof is in the
the output of a random deep neural network as in section 2. Let a > 0 and let hn = (cid:98)a(cid:112)n/ ln n(cid:99) 

supplementary material.
Theorem 1 (closest string with a different classiﬁcation). For any n ∈ N  let φ : {−1  1}n → R be
where (cid:98)t(cid:99) denotes the integer part of t ≥ 0. Let us ﬁx x ∈ {−1  1}n and z > 0  and let Nn(a  z) be
the average number of input bit strings y ∈ {−1  1}n with Hamming distance hn from x and with a
different classiﬁcation from x  conditioned on φ(x) =

(4)
Here h(x  y) is the Hamming distance between x and y and we recall that Q = E(φ(x)2). Then  for
n → ∞

#{y ∈ {−1  1}n : h(x  y) = hn   φ(y) < 0}(cid:12)(cid:12)(cid:12)φ(x) =(cid:112)Q z
(cid:17)(cid:33)

Nn(a  z) = E(cid:16)

(cid:32)

(cid:17)

(cid:16)

Q z:

√

√

.

z2

1 −

4F (cid:48)(1)a2 +

ln ln n
a2
ln n

+ O

4√

1

n ln n

.

(5)

ln Nn(a  z) =

n ln n

a
2

4

In particular 

n→∞ Nn(a  z) = 0 for a <
lim

2(cid:112)F (cid:48)(1)

z

 

n→∞ Nn(a  z) = ∞ for a ≥

lim

2(cid:112)F (cid:48)(1)

z

. (6)

Theorem 1 tells us that  if n (cid:29) 1  for any input bit string x ∈ {−1  1}n  with very high probability
all the input bit strings y ∈ {−1  1}n with Hamming distance from x lower than

|φ(x)|

2(cid:112)Q F (cid:48)(1)

(cid:114) n

ln n

h∗
n(x) =

(7)

(8)

have the same classiﬁcation as x  i.e.  φ(y) has the same sign as φ(x). Moreover  the number of input
bit strings y with Hamming distance from x higher than h∗
n(x) and with a different classiﬁcation than x
is exponentially large in n. Therefore  with very high probability the Hamming distance from x of the
closest bit string with a different classiﬁcation is approximately h∗
the average Hamming distance of the closest string with a different classiﬁcation is

n(x). Since E(|φ(x)|) =(cid:112)2Q/π 
(cid:114) n

(cid:114)

E (h∗

n(x)) =

n

2πF (cid:48)(1) ln n

≥

 

2π ln n

where the last inequality holds for the ReLU activation function and follows since in this case
F (cid:48)(1) ≤ 1.
Remark 1. While Theorem 1 holds for any activation function  the property F (cid:48)(1) ≤ 1 may not hold
for activation functions different from the ReLU. For example  in the case of tanh there are values
of σw and σb such that F (cid:48)(1) grows exponentially with L [41]. In this case  the Hamming distance

of the closest string with a different classiﬁcation still scales as(cid:112)n/ ln n  but the prefactor can be
F (cid:48)(1) may become comparable with(cid:112)n/ ln n and signiﬁcantly affect the Hamming distance.

exponentially small in L. Therefore with the tanh activation function  for ﬁnite values of L and n 

3.2 Random bit ﬂips

Let us now consider the problem of the average number of bits that are needed to ﬂip in order to
change the classiﬁcation of a given bit string. We consider a random sequence of input bit strings
{x(0)  . . .   x(n)} ⊂ {−1  1}n  where at the i-th step x(i) is generated ﬂipping a random bit of x(i−1)
that has not been already ﬂipped in the previous steps. Any sequence as above is geodesic  i.e. 
h(x(i)  x(j)) = |i − j| for any i  j = 0  . . .   n. The following Theorem 2 states that the average
Hamming distance from x(0) of the closest string of the sequence with a different classiﬁcation is
proportional to n. The proof is in the supplementary material.
Theorem 2 (random bit ﬂips). For any n ∈ N  let φ : {−1  1}n → R be the output of a random deep
neural network as in section 2  and let {x(0)  . . .   x(n)} ⊂ {−1  1}n be a geodesic sequence of bit
strings. Let hn be the expected value of the minimum number of steps required to reach a bit string
with a different classiﬁcation from x(0):

1 ≤ i ≤ n : φ(x(0))φ(x(i)) < 0

min

min

(9)
Then  there exists a constant t0 > 0 which depends only on F such that hn ≥ n t0 for any n ∈ N.
Remark 2. Since the entry of the kernel (2) associated to two inputs lying on the sphere is a function
of their squared Euclidean distance  which coincides with the Hamming distance in the case of bit
strings  Theorems 1 and 2 may be generalized to continuous inputs on the sphere by replacing the
Hamming distance with the squared Euclidean distance.

  n

.

hn = E(cid:16)

(cid:110)

(cid:110)

(cid:111)

(cid:111)(cid:17)

3.3 Heuristic argument

√
For a better understanding of Theorems 1 and 2  we provide a simple heuristic argument to their
validity. The crucial observation is that  if one bit of the input is ﬂipped  the change in φ is Θ(1/
n).
Indeed  let x  y ∈ {−1  1}n with h(x  y) = 1. From (2)  φ(y) − φ(x) is a Gaussian random variable
with zero average and variance

E(cid:0)(φ(y) − φ(x))2(cid:1) = 2Q(cid:0)1 − F(cid:0)1 − 2

(cid:1)(cid:1) (cid:39) 4QF (cid:48)(1)/n .

(10)

n

5

(a)

(b)

√

Figure 1: (a) Average Hamming distance to the nearest differently classiﬁed input string versus the
number of input neurons for the neural network. The Hamming distance to the nearest differently

classiﬁed string scales as(cid:112)n/(2π ln n). with respect to the number of input neurons. Left: the

results of the simulations clearly show the importance of the ln n term in the scaling. Right: the
empirically calculated value 0.405 for the pre-factor a is close to the theoretically predicted value
of 1/
2π. Each data point is the average of 1000 different calculations of the Hamming distance
for randomly sampled bit strings. Each calculation was performed on a randomly generated neural
network. Further technical details for the design of the neural networks are given in subsection 4.4.
(b) The linear relationship between |φ(x)| and h∗
n(x) is consistent across neural networks of different
sizes. To calculate the average distance at values of |φ(x)| within an interval  data was averaged
across equally spaced bins of 0.25 for values of |φ(x)|. Averages for each bin are plotted at the
midpoint of the bin. Points are only shown if there are at least 10 samples within the bin.

For any i  at the i-th step of the sequence of bit strings of subsection 3.2  φ changes by the Gaussian
random variable φ(x(i)) − φ(x(i+1))  which from (10) has zero mean and variance 4Q F (cid:48)(1)/n.
Assuming that the changes are independent  after h steps φ changes by a Gaussian random variable
with zero mean and variance 4h Q F (cid:48)(1)/n. Recalling that E(φ(x(0))2) = Q and that F (cid:48)(1) ≤ 1 for
the ReLU activation function  approximately h ≈ n/(4F (cid:48)(1)) ≥ n/4 steps are needed in order to
ﬂip the sign of φ and hence the classiﬁcation.
Let us now consider the problem of the closest bit string with a different classiﬁcation from a given
bit string x. For any bit string y at Hamming distance one from x  φ(y) − φ(x) is a Gaussian
random variable with zero mean and variance 4Q F (cid:48)(1)/n. We assume that these random variables
are independent  and recall that the minimum among n iid normal Gaussian random variables
scales as
2 ln n [65]. There are n bit strings y at Hamming distance one from x  therefore

the minimum over these y of φ(y) − φ(x) is approximately −(cid:112)8Q F (cid:48)(1) ln n/n. This is the
the maximum amount by which we can decrease φ ﬂipping h bits is h(cid:112)8Q F (cid:48)(1) ln n/n. Since
h ≈ (cid:112)n/(8F (cid:48)(1) ln n) ≥ (cid:112)n/(8 ln n)  where the last inequality holds for the ReLU activation

E(φ(x(0))2) = Q  the minimum number of bit ﬂips required to ﬂip the sign of φ is approximately
8 (cid:39) 0.354 obtained with the heuristic proof above is very close to the

maximum amount by which we can decrease φ ﬂipping one bit of the input. Iterating the procedure 

√

√
√
function. The pre-factor 1/
exact pre-factor 1/

2π (cid:39) 0.399 obtained with the formal proof in (8).

4 Experiments

4.1 Closest bit string with a different classiﬁcation

To conﬁrm experimentally the ﬁndings of Theorem 1  Hamming distances to the closest bit string with
a different classiﬁcation were calculated for randomly generated neural networks with parameters
sampled from normal distributions (see subsection 4.4). This distance was calculated using a greedy
search algorithm (Figure 1a). In this algorithm  the search for a differently classiﬁed bit string
progressed in steps  where in each step  the most signiﬁcant bit was ﬂipped. This bit corresponded
to the one that produced the largest change towards zero in the value of the output neuron when

6





:2-074135:9-98;07 0 223/89 3.09430 7089/1107039. 8810/-98973 9 1994a√n/ln(n)a=a√na=



:2-074135:9-98a√n/ln(n)a=a√n/ln(n)a=1/√2π0.00.51.01.52.02.53.03.5Absolute value of output neuron |(cid:8240)(x)|010203040Average Hamming distance todifferently classified bit stringNumber ofinput bits100007000400020001000ﬂipped. To ensure that this algorithm accurately calculated Hamming distances  we compared the
results of the greedy search algorithm to those from an exact search which exhaustively searched all
bit strings at speciﬁed Hamming distances for smaller networks where this exact search method was
computationally feasible. Comparisons between the two algorithms in Table 1 of the supplementary
material show that outcomes from the greedy search algorithm were consistent with those from the

exact search algorithm. The results from the greedy search method conﬁrm the(cid:112)n/ ln n scaling

√
of the average Hamming distance starting from n (cid:38) 100. The value of the pre-factor 1/
2π is also
conﬁrmed with the high precision of 1.5%. Figure 1b empirically validates the linear relationship
between the value of the output neuron |φ(x)| and the Hamming distance to bit strings with different
classiﬁcation h∗
n(x) expressed by (7). This linear relationship was consistent with all neural networks
empirically tested in our analysis. Intuitively  |φ(x)| is an indication of the conﬁdence in classiﬁcation.
The linear relationship shown here implies that as the value of |φ(x)| grows  the conﬁdence of the
classiﬁcation of an input strengthens  increasing the distance from that input to boundaries of different
classiﬁcations.

4.2 Random bit ﬂips

Figure 2 conﬁrms the ﬁndings of Theorem 2  namely that the expected number of random bit ﬂips
required to reach a bit string with a different classiﬁcation scales linearly with the number of input
neurons. The pre-factor found by simulation is 0.33  slightly above the lower bound of 0.25 estimated
from the heuristic argument. Our results show that  though the Hamming distance to the nearest

classiﬁcation boundary scales on average at a rate of(cid:112)n/ ln n  the distance to a random boundary

scales linearly and more rapidly.

Figure 2: The average number of random bit ﬂips
required to reach a bit string with different classi-
ﬁcation scales linearly with the number of input
neurons. Each point is averaged across a sample
of 1000 neural networks  where the Hamming dis-
tances to differently classiﬁed bit strings for each
network are tested at a single random input bit
string.

4.3 Analysis of MNIST data

Our theoretical results hold for random  untrained deep neural networks. It is an interesting question
whether trained deep neural networks exhibit similar properties for the Hamming distances to
classiﬁcation boundaries. Clearly some trained networks will not: a network that has been trained to
return as output the ﬁnal bit of the input string has Hamming distance one to the nearest classiﬁcation
boundary. For networks that are trained to classify noisy data  however  we expect the trained networks
to exhibit relatively large Hamming distances to the nearest classiﬁcation boundary. Moreover  if
a ‘typical’ network can perform the noisy classiﬁcation task  then we expect training to guide the
weights to a nearby typical network that does the job  for the simple reason that networks that exhibit

Θ((cid:112)n/ ln n) distance to the nearest boundary and an average distance of Θ(n) to a boundary under

random bit ﬂips have much higher prior probabilities than atypical networks.
To determine if our results hold for models trained on real-world data  we trained 2-layer fully-
connected neural networks to categorize whether hand-drawn digits taken from the MNIST database
[66] are even or odd. Images of hand drawn digits were converted from their 2-dimensional format
(28 by 28 pixels) into a 1-dimensional vector of 784 binary inputs. The starting 8 bit pixel values were
converted to binary format by determining whether the pixel value was above or below a threshold of
25. Networks were trained to determine whether the hand-drawn digit was odd or even. All networks

7









:2-074135:9-98







;07 0 223/89 3.094/1107039. 8810/-989730891994a*n (a)

(b)

Figure 3: (a) Average Hamming distance to the nearest differently classiﬁed input bit string for
MNIST trained models calculated using the greedy search method. The average distance calculated
for random bits is close to the expected value of approximately 4.33. Further technical details for the
design of the neural networks are given in subsection 4.4.
(b) The linear relationship between |φ(x)| and h∗
n(x) is consistent for networks trained on MNIST
data. To calculate the average distance at values of |φ(x)| within an interval  data was averaged across
equally spaced bins of 2.5 for values of |φ(x)|. Averages for each bin are plotted at the midpoint of
the bin. Points are only shown if there are at least 25 samples within the bin.

followed the design described in subsection 4.4. 400 Networks were trained for 20 epochs using the
Adam optimizer [67]; average test set accuracy of 98.8% was achieved.
For these trained networks  Hamming distances to the nearest bit string with a different classiﬁcation
were calculated using the greedy search method outlined in subsection 4.1. These Hamming distances
were evaluated for three types of bit strings: bit strings taken from the training set  bit strings taken
from the test set  and randomly sampled bit strings where each bit has equal probability of 0 and
1. For the randomly sampled bit strings  the average minimum Hamming distance to a differently

classiﬁed bit string is very close to the expected theoretical value of(cid:112)n/(2π ln n) (Figure 3a). By

contrast  for bit strings taken from the test and training set  the minimum Hamming distances to a
classiﬁcation boundary were on average much higher than that for random bits  as should be expected:
training increases the distance from the data points to the boundary of their respective classiﬁcation
regions and makes the network more robust to errors when classifying real-world data compared with
classifying random bit strings.
Furthermore  even for trained networks  a linear relationship is still observed between the absolute
value of the output neuron (prior to normalization by a sigmoid activation) and the average Hamming
distance to the nearest differently classiﬁed bit string (Figure 3b). Here  the slope of the linear
relationship is larger for test and training set data  consistent with the expectation that training should
extend the Hamming distance to classiﬁcation boundaries for patterns of data found in the training
set.
Finally  we have explored the correlation between the distance of a training or test picture from the
closest classiﬁcation boundary with its classiﬁcation accuracy. Figure 4 shows that the incorrectly
classiﬁed pictures tend to be signiﬁcantly closer to the classiﬁcation boundary than the correctly
classiﬁed ones: the average distances are 1.42 and 10.61  respectively  for the training set  and
2.30 and 10.47  respectively  for the test set. Therefore  our results show that the distance to the
closest classiﬁcation boundary is empirically correlated with the classiﬁcation accuracy and with the
generalization properties of the deep neural network.

4.4 Experimental apparatus and structure of neural networks

Weights for all neural networks are initialized according to a normal distribution with zero mean
and variance equal to 2/nin  where nin is the number of input units in the weight tensor. No bias
term is included in the neural networks. All networks consist of two fully connected hidden layers 

8

Train SetTest SetRandom Bits0246810Average Hamming distance to nearestdifferently classified bit stringExpected Avg. Distance: 4.33Train SetTest SetRandom Bits050|Φ(x)|050|Φ(x)|050|Φ(x)|05101520Average Hamming distance to nearestdifferently classified bit string050|Φ(x)|slope = 0.15slope = 0.20slope = 0.20Figure 4: Histogram counting instances
of correctly and incorrectly classiﬁed
MNIST pictures shows that trained neu-
ral networks are far more likely to mis-
classify points closer to a classiﬁcation
boundary for both the training and test
sets. Results are aggregated across 20
different trained neural networks trained
to classify whether digits are even or odd.
Networks are trained for 10 epochs using
the Adam optimizer.

each with n neurons (equal to number of input neurons) and activation function set to the commonly
used Rectiﬁed Linear Unit (ReLU). All networks contain a single output neuron with no activation
function. In the notation of section 2  this choice corresponds to σ2
b = 0  n0 = n1 = n2 = n
and n3 = 1  and implies F (cid:48)(1) = 1. Simulations were run using the python package Keras with a
backend of TensorFlow [68].

w = 2  σ2

5 Conclusions
We have proved that the binary classiﬁers of strings of n (cid:29) 1 bits generated by wide random deep
neural networks with ReLU activation function are simple. The simplicity is captured by the following
two properties. First  for any given input bit string the average Hamming distance of the closest input

bit string with a different classiﬁcation is at least(cid:112)n/(2π ln n). Second  if the bits of the original

string are randomly ﬂipped  the average number of bit ﬂips needed to change the classiﬁcation is
at least n/4. For activation functions other than the ReLU both scalings remain the same  but the
prefactor can change and can be exponentially small in the number of hidden layers.
The striking consequence of our result is that the binary classiﬁers of strings of n (cid:29) 1 bits generated
by a random deep neural network lie with very high probability in a subset which is an exponentially
small fraction of all the possible binary classiﬁers. Indeed  for a uniformly random binary classiﬁer 
the average Hamming distance of the closest input bit string with a different classiﬁcation is one  and
the average number of bit ﬂips required to change the classiﬁcation is two. Our result constitutes
a fundamental step forward in the characterization of the probability distribution of the functions
generated by random deep neural networks  which is employed as prior distribution in the PAC-
Bayesian generalization bounds. Therefore  our result can contribute to the understanding of the
generalization properties of deep learning algorithms.
Our analysis of the MNIST data suggests that  for certain types of problems  the property that many
bits need to be ﬂipped in order to change the classiﬁcation survives after training the network. Both
our theoretical results and our experiments are completely consistent to the empirical ﬁndings in
the context of adversarial perturbations [69–74]  where the existence of inputs that are close to a
correctly classiﬁed input but have the wrong classiﬁcation is explored. As expected  our results show
that as the size of the input grows  the average number of bits needed to be ﬂipped to change the
classiﬁcation increases in absolute terms but decreases as a percentage of the total number of bits.
An extension of our theoretical results to trained deep neural networks would provide a fundamental
robustness result of deep neural networks with respect to adversarial perturbations  and will be the
subject of future work.
Moreover  our experiments on MNIST show that the distance of a picture to the closest classiﬁcation
boundary is correlated with its classiﬁcation accuracy and thus with the generalization properties of
deep neural networks  and conﬁrm that exploring the properties of this distance is a promising route
towards proving the unreasonably good generalization properties of deep neural networks.
Finally  the simplicity bias proven in this paper might shed new light on the unexpected empirical
property of deep learning algorithms that the optimization over the network parameters does not
suffer from bad local minima  despite the huge number of parameters and the non-convexity of the
function to be optimized [75–79].

9

Test SetTraining Set05101520253035Distance05101520253035Distance0%20%40%60%% of Total0%20%40%60%% of Totalavgavgavgavgcorrectly classifiedincorrectly classifiedAcknowledgements

GdP thanks the Research Laboratory of Electronics of the Massachusetts Institute of Technology for
the kind hospitality in Cambridge  and Dario Trevisan for useful discussions.
GdP acknowledges ﬁnancial support from the European Research Council (ERC Grant Agreements
Nos. 337603 and 321029)  the Danish Council for Independent Research (Sapere Aude)  VILLUM
FONDEN via the QMATH Centre of Excellence (Grant No. 10059) and AFOSR and ARO under
the Blue Sky program. SL and BTK were supported by IARPA  NSF  BMW under the MIT Energy
Initiative  and ARO under the Blue Sky program.

References
[1] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[2] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. nature  521(7553):436 

2015.

[3] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.
[4] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks 

61:85–117  2015.

[5] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. Adaptive computation and machine

learning. MIT Press  2016.

[6] Moritz Hardt  Benjamin Recht  and Yoram Singer. Train faster  generalize better: Stability of

stochastic gradient descent. arXiv preprint arXiv:1509.01240  2015.

[7] Behnam Neyshabur  Ryota Tomioka  and Nathan Srebro. In search of the real inductive bias:
On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614  2014.
[8] Alfredo Canziani  Adam Paszke  and Eugenio Culurciello. An analysis of deep neural network

models for practical applications. arXiv preprint arXiv:1605.07678  2016.

[9] Roman Novak  Yasaman Bahri  Daniel A Abolaﬁa  Jeffrey Pennington  and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. arXiv preprint
arXiv:1802.08760  2018.

[10] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530  2016.

[11] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer New York  2013.
[12] Eric B Baum and David Haussler. What size net gives valid generalization? In Advances in

neural information processing systems  pages 81–90  1989.

[13] Peter L Bartlett  Nick Harvey  Chris Liaw  and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930 
2017.

[14] Shizhao Sun  Wei Chen  Liwei Wang  Xiaoguang Liu  and Tie-Yan Liu. On the depth of deep

neural networks: A theoretical view. In AAAI  pages 2066–2072  2016.

[15] Behnam Neyshabur  Zhiyuan Li  Srinadh Bhojanapalli  Yann LeCun  and Nathan Srebro.
Towards understanding the role of over-parametrization in generalization of neural networks.
arXiv preprint arXiv:1805.12076  2018.

[16] Kenji Kawaguchi  Leslie Pack Kaelbling  and Yoshua Bengio. Generalization in deep learning.

arXiv preprint arXiv:1710.05468  2017.

[17] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nati Srebro. Exploring
generalization in deep learning. In Advances in Neural Information Processing Systems  pages
5947–5956  2017.

[18] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds
for deep (stochastic) neural networks with many more parameters than training data. arXiv
preprint arXiv:1703.11008  2017.

10

[19] Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential

privacy. arXiv preprint arXiv:1802.09583  2018.

[20] Sanjeev Arora  Rong Ge  Behnam Neyshabur  and Yi Zhang. Stronger generalization bounds

for deep nets via a compression approach. arXiv preprint arXiv:1802.05296  2018.

[21] Ari S Morcos  David GT Barrett  Neil C Rabinowitz  and Matthew Botvinick. On the importance

of single directions for generalization. arXiv preprint arXiv:1803.06959  2018.

[22] David A McAllester. Some pac-bayesian theorems. Machine Learning  37(3):355–363  1999.
[23] O. Catoni. Pac-Bayesian Supervised Classiﬁcation: The Thermodynamics of Statistical Learn-
ing. Institute of Mathematical Statistics lecture notes-monograph series. Institute of Mathemati-
cal Statistics  2007.

[24] Guy Lever  François Laviolette  and John Shawe-Taylor. Tighter pac-bayes bounds through

distribution-dependent priors. Theoretical Computer Science  473:4–28  2013.

[25] Behnam Neyshabur  Srinadh Bhojanapalli  David McAllester  and Nathan Srebro. A pac-
bayesian approach to spectrally-normalized margin bounds for neural networks. arXiv preprint
arXiv:1707.09564  2017.

[26] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. A Wiley-Interscience

publication. Wiley  2012.

[27] Guillermo Valle-Perez  Chico Q. Camargo  and Ard A. Louis. Deep learning generalizes because
the parameter-function map is biased towards simple functions. In International Conference on
Learning Representations  2019.

[28] Devansh Arpit  Stanisław Jastrzebski  Nicolas Ballas  David Krueger  Emmanuel Bengio 
Maxinder S Kanwal  Tegan Maharaj  Asja Fischer  Aaron Courville  Yoshua Bengio  et al.
A closer look at memorization in deep networks. In International Conference on Machine
Learning  pages 233–242  2017.

[29] Lei Wu  Zhanxing Zhu  and E Weinan. Towards understanding generalization of deep learning:

Perspective of loss landscapes. arXiv preprint arXiv:1706.10239  2017.

[30] Jürgen Schmidhuber. Discovering neural nets with low kolmogorov complexity and high

generalization capability. Neural Networks  10(5):857–873  1997.

[31] Henry W Lin  Max Tegmark  and David Rolnick. Why does deep and cheap learning work so

well? Journal of Statistical Physics  168(6):1223–1247  2017.

[32] Abraham Lempel and Jacob Ziv. On the complexity of ﬁnite sequences. IEEE Transactions on

Information Theory  22(1):75–81  1976.

[33] Andrei N Kolmogorov. On tables of random numbers. Theoretical Computer Science 

207(2):387–395  1998.

[34] M. Li and P. Vitanyi. An Introduction to Kolmogorov Complexity and Its Applications. Mono-

graphs in Computer Science. Springer New York  2013.

[35] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions

on Information Theory  23(3):337–343  1977.

[36] Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks  pages

29–53. Springer  1996.

[37] Christopher KI Williams. Computing with inﬁnite networks. In Advances in neural information

processing systems  pages 295–301  1997.

[38] Jaehoon Lee  Yasaman Bahri  Roman Novak  Samuel S Schoenholz  Jeffrey Pennington 
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint
arXiv:1711.00165  2017.

[39] Alexander G de G Matthews  Mark Rowland  Jiri Hron  Richard E Turner  and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint
arXiv:1804.11271  2018.

[40] Adrià Garriga-Alonso  Laurence Aitchison  and Carl Edward Rasmussen. Deep convolutional

networks as shallow gaussian processes. arXiv preprint arXiv:1808.05587  2018.

11

[41] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In Advances in neural
information processing systems  pages 3360–3368  2016.

[42] Samuel S Schoenholz  Justin Gilmer  Surya Ganguli  and Jascha Sohl-Dickstein. Deep informa-

tion propagation. arXiv preprint arXiv:1611.01232  2016.

[43] Lechao Xiao  Yasaman Bahri  Jascha Sohl-Dickstein  Samuel S Schoenholz  and Jeffrey Pen-
nington. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10 000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393  2018.

[44] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems  pages
8571–8580  2018.

[45] Roman Novak  Lechao Xiao  Yasaman Bahri  Jaehoon Lee  Greg Yang  Daniel A. Abolaﬁa 
Jeffrey Pennington  and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations 
2019.

[46] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian pro-
cess behavior  gradient independence  and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760  2019.

[47] Jaehoon Lee  Lechao Xiao  Samuel S Schoenholz  Yasaman Bahri  Jascha Sohl-Dickstein  and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. arXiv preprint arXiv:1902.06720  2019.

[48] Daniel W. Stroock and S. R. Srinivasa Varadhan. Multidimensional Diffusion Processes. Classics

in Mathematics. Springer Berlin Heidelberg  2007.

[49] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[50] Maithra Raghu  Ben Poole  Jon Kleinberg  Surya Ganguli  and Jascha Sohl-Dickstein. On the

expressive power of deep neural networks. arXiv preprint arXiv:1606.05336  2016.

[51] Raja Giryes  Guillermo Sapiro  and Alexander M Bronstein. Deep neural networks with
random gaussian weights: A universal classiﬁcation strategy? IEEE Trans. Signal Processing 
64(13):3444–3457  2016.

[52] Jeffrey Pennington  Samuel Schoenholz  and Surya Ganguli. The emergence of spectral
In International Conference on Artiﬁcial Intelligence and

universality in deep networks.
Statistics  pages 1924–1932  2018.

[53] Leonardo Franco. Generalization ability of boolean functions implemented in feedforward

neural networks. Neurocomputing  70(1-3):351–361  2006.

[54] Daniel Soudry  Elad Hoffer  and Nathan Srebro. The implicit bias of gradient descent on

separable data. arXiv preprint arXiv:1710.10345  2017.

[55] Jorma Rissanen. Modeling by shortest data description. Automatica  14(5):465–471  1978.
[56] Anselm Blumer  Andrzej Ehrenfeucht  David Haussler  and Manfred K Warmuth. Occam’s

razor. Information processing letters  24(6):377–380  1987.

[57] David H Wolpert. The relationship between pac  the statistical physics framework  the bayesian
framework  and the vc framework. In The mathematics of generalization  pages 117–214. CRC
Press  2018.

[58] Tor Lattimore and Marcus Hutter. No free lunch versus occam’s razor in supervised learning.
In Algorithmic Probability and Friends. Bayesian Prediction and Artiﬁcial Intelligence  pages
223–235. Springer  2013.

[59] Kamaludin Dingle  Chico Q Camargo  and Ard A Louis. Input–output maps are strongly biased

towards simple outputs. Nature communications  9(1):761  2018.

[60] Razvan Pascanu  Guido Montufar  and Yoshua Bengio. On the number of response regions of
deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098 
2013.

12

[61] Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of
linear regions of deep neural networks. In Advances in neural information processing systems 
pages 2924–2932  2014.

[62] Peter Hinz and Sara van de Geer. A framework for the construction of upper bounds on
the number of afﬁne linear regions of relu feed-forward neural networks. arXiv preprint
arXiv:1806.01918  2018.

[63] Amit Daniely  Roy Frostig  and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
Processing Systems  pages 2253–2261  2016.

[64] Mariia Vladimirova  Jakob Verbeek  Pablo Mesejo  and Julyan Arbel. Understanding priors in
bayesian neural networks at the unit level. In International Conference on Machine Learning 
pages 6458–6467  2019.

[65] Anton Bovier. Extreme values of random processes. Lecture Notes Technische Universität

Berlin  2005.

[66] Yann LeCun  Corinna Cortes  and Christopher J.C. Burges. The mnist database of handwritten

digits  http://yann.lecun.com/exdb/mnist/  1998.

[67] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv:1412.6980 [cs]  2014. arXiv: 1412.6980.

[68] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S. Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow 
Andrew Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser 
Manjunath Kudlur  Josh Levenberg  Dandelion Mané  Rajat Monga  Sherry Moore  Derek
Murray  Chris Olah  Mike Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal
Talwar  Paul Tucker  Vincent Vanhoucke  Vijay Vasudevan  Fernanda Viégas  Oriol Vinyals  Pete
Warden  Martin Wattenberg  Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems  2015. Software available from tensorﬂow.org.
[69] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfel-
low  and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 
2013.

[70] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572  2014.

[71] Jonathan Peck  Joris Roels  Bart Goossens  and Yvan Saeys. Lower bounds on the robustness
to adversarial perturbations. In Advances in Neural Information Processing Systems  pages
804–813  2017.

[72] Aleksander Madry  Aleksandar Makelov  Ludwig Schmidt  Dimitris Tsipras  and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 
2017.

[73] Dimitris Tsipras  Shibani Santurkar  Logan Engstrom  Alexander Turner  and Aleksander Madry.

Robustness may be at odds with accuracy. stat  1050:11  2018.

[74] Preetum Nakkiran. Adversarial robustness may be at odds with simplicity. arXiv preprint

arXiv:1901.00532  2019.

[75] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204 
2015.

[76] Anna Choromanska  Yann LeCun  and Gérard Ben Arous. Open problem: The landscape of the
loss surfaces of multilayer networks. In Conference on Learning Theory  pages 1756–1760 
2015.

[77] Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information

Processing Systems  pages 586–594  2016.

[78] Marco Baity-Jesi  Levent Sagun  Mario Geiger  Stefano Spigler  G Ben Arous  Chiara Cam-
marota  Yann LeCun  Matthieu Wyart  and Giulio Biroli. Comparing dynamics: Deep neural
networks versus glassy systems. arXiv preprint arXiv:1803.06969  2018.

[79] Dhagash Mehta  Xiaojun Zhao  Edgar A Bernal  and David J Wales. Loss surface of xor

artiﬁcial neural networks. Physical Review E  97(5):052307  2018.

13

,Omer Levy
Yoav Goldberg
Kohei Hayashi
Yuichi Yoshida
Giacomo De Palma
Bobak Kiani
Seth Lloyd