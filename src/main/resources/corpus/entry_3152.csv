2019,Learning to Self-Train for Semi-Supervised Few-Shot Classification,Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that leverages unlabeled data and specifically meta-learns how to cherry-pick and label such unsupervised data to further improve performance. To this end  we train the LST model through a large number of semi-supervised few-shot tasks. On each task  we train a few-shot model to predict pseudo labels for unlabeled data  and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization. We evaluate our LST method on two ImageNet benchmarks for semi-supervised few-shot classification and achieve large improvements over the state-of-the-art.,Learning to Self-Train for Semi-Supervised

Few-Shot Classiﬁcation

Xinzhe Li1∗ Qianru Sun2† Yaoyao Liu3∗ Shibao Zheng1†

Qin Zhou4 Tat-Seng Chua5 Bernt Schiele6

1Shanghai Jiao Tong University 2Singapore Management University 3Tianjin University 4Alibaba Group
5National University of Singapore 6Max Planck Institute for Informatics  Saarland Informatics Campus

Abstract

Few-shot classiﬁcation (FSC) is challenging due to the scarcity of labeled training
data (e.g. only one labeled data point per class). Meta-learning has shown to
achieve promising results by learning to initialize a classiﬁcation model for FSC.
In this paper we propose a novel semi-supervised meta-learning method called
learning to self-train (LST) that leverages unlabeled data and speciﬁcally meta-
learns how to cherry-pick and label such unsupervised data to further improve
performance. To this end  we train the LST model through a large number of
semi-supervised few-shot tasks. On each task  we train a few-shot model to
predict pseudo labels for unlabeled data  and then iterate the self-training steps
on labeled and pseudo-labeled data with each step followed by ﬁne-tuning. We
additionally learn a soft weighting network (SWN) to optimize the self-training
weights of pseudo labels so that better ones can contribute more to gradient descent
optimization. We evaluate our LST method on two ImageNet benchmarks for
semi-supervised few-shot classiﬁcation and achieve large improvements over the
state-of-the-art method. Code is at github.com/xinzheli1217/learning-to-self-train.

1

Introduction

Today’s deep neural networks require large amounts of labeled data for supervised training and best
performance [39  8  30]. Their potential applications to the small-data regimes are thus limited.
There has been growing interest in reducing the required amount of data  e.g. to only 1-shot [12].
One of the most powerful methods is meta-learning that transfers the experience learned from
similar tasks to the target task [3]. Among different meta strategies  gradient descent based methods
are particularly promising for today’s neural networks [3  32  27]. Another intriguing idea is to
additionally use unlabeled data. Semi-supervised learning using unlabeled data with a relatively
small set of labeled ones has obtained good performance on standard datasets [21  20]. A classic 
intuitive and simple method is e.g. self-training. It ﬁrst trains a supervised model with labeled data 
and then enlarges the labeled set based on the most conﬁdent predictions (called pseudo labels) on
unlabeled data [40  35  20]. It can outperform regularization based methods [17  6  9]  especially
when labeled data is scarce.
The focus of this paper is thus on the semi-supervised few-shot classiﬁcation (SSFSC) task. Speciﬁ-
cally  there are few labeled data and a much larger amount of unlabeled data for training classiﬁers.
To tackle this problem  we propose a new SSFSC method called learning to self-train (LST) that
successfully embeds a well-performing semi-supervised method  i.e. self-training  into the meta

∗This work was done during their internships mainly supervised by Qianru.
†Corresponding authors: qianrusun@smu.edu.sg; sbzh@sjtu.edu.cn.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

gradient descent paradigm. However  this is non-trivial  as directly applying self-training recursively
may result in gradual drifts and thus adding noisy pseudo-labels [41]. To address this issue  we
propose both to meta-learn a soft weighting network (SWN) to automatically reduce the effect of
noisy labels  as well as to ﬁne-tune the model with only labeled data after every self-training step.
Speciﬁcally  our LST method consists of inner-loop self-training (for one task) and outer-loop
meta-learning (over all tasks). LST meta-learns both to initialize a self-training model and how to
cherry-pick from noisy labels for each task. An inner loop starts from the meta-learned initialization
by which a task-speciﬁc model can be fast adapted with few labeled data. Then  this model is
used to predict pseudo labels  and labels are weighted by the meta-learned soft weighting network
(SWN). Self-training consists of re-training using weighted pseudo-labeled data and ﬁne-tuning on
few labeled data. In the outer loop  the performance of these meta-learners are evaluated via an
independent validation set  and parameters are optimized using the corresponding validation loss.
In summary  our LST method learns to accumulate self-supervising experience from SSFSC tasks
in order to quickly adapt to a new few-shot task. Our contribution is three-fold. (i) A novel self-
training strategy that prevents the model from drifting due to label noise and enables robust recursive
training. (ii) A novel meta-learned cherry-picking method that optimizes the weights of pseudo
labels particularly for fast and efﬁcient self-training. (iii) Extensive experiments on two versions of
ImageNet benchmarks – miniImageNet [36] and tieredImageNet [24]  in which our method achieves
top performance.

2 Related works

Few-shot classiﬁcation (FSC). Most FSC works are based on supervised learning. They can be
roughly divided into four categories: (1) data augmentation based methods [15  29  37  38] generate
data or features in a conditional way for few-shot classes; (2) metric learning methods [36  31 
33] learn a similarity space of image features in which the classiﬁcation should be efﬁcient with
few examples; (3) memory networks [18  28  22  16] design special networks to record training
“experience” from seen tasks  aiming to generalize that to the learning of unseen ones; and (4) gradient
descent based methods [3  4  1  23  11  7  42  32  14] learn a meta-learner in the outer loop to initialize
a base-learner for the inner loop that is then trained on a novel few-shot task. In our LST method 
the outer-inner loop optimization is based on the gradient descent method. Different to previous
works  we propose a novel meta-learner that assigns weights to pseudo-labeled data  particularly for
semi-supervised few-shot learning.
Semi-supervised learning (SSL). SSL methods aim to leverage unlabeled data to obtain decision
boundaries that better ﬁt the underlying data structure [20]. The Π-Model applies a simple consistency
regularization [9]  e.g. by using dropout  adding noise and data augmentation  in which data is
automatically “labeled”. Mean Teacher is more stable version of the Π-Model by making use of a
moving average technique [34]. Visual Adversarial Training (VAT) regularizes the network against
the adversarial perturbation  and it has been shown to be an effective regularization [17]. Another
popular method is Entropy Minimization that uses a loss term to encourage low-entropy (more
conﬁdent) predictions for unlabeled data  regardless of their real classes [6]. Pseudo-labeling is a self
supervised learning method that relies on the predictions of unlabeled data  i.e. pseudo labels [2]. It
can outperform regularization based methods  especially when labeled data is scarce [20] as in our
envisioned setting. We thus use this method in our inner loop training.
Semi-supervised few-shot classiﬁcation (SSFSC). Semi-supervised learning on FSC tasks aims to
improve the classiﬁcation accuracy by adding a large number of unlabeled data in training. Ren et al.
proposed three semi-supervised variants of ProtoNets [31]  basically using Soft k-Means method to
tune clustering centers with unlabeled data. A more recent work used the transductive propagation
network (TPN) [13] to propagate labels from labeled data to unlabeled ones  and meta-learned the
key hyperparameters of TPN. Differently  we build our method based on the simple and classical
self-training [40] and meta gradient descent method [3  32] without requiring a new design of a
semi-supervised network. Rohrbach et al. [25] proposed to further leverage external knowledge 
such as the semantic attributes of categories  to solve not only few-shot but also zero-shot problems.
Similarly  we expect further gains of our approach when using similar external knowledge in our
future work.

2

Figure 1: The pipeline of the proposed LST method on a single (2-class  3-shot) task. The prototype
of a class is the mean feature in the class  and SWN is the soft weighting network whose optimization
procedure is given in Figure 2 and Section 4.2.

3 Problem deﬁnition and denotation

In conventional few-shot classiﬁcation (FSC)  each task has a small set of labeled training data called
support set S  and another set of unseen data for test  called query set Q. Following [24]  we denote
another set of unlabeled data as R to be used for semi-supervised learning (SSL). R may or may not
contain data of distracting classes (not included in S).
Our method follows the uniform episodic formulation of meta-learning [36] that is different to
traditional classiﬁcation in three aspects. (1) Main phases are meta-train and meta-test (instead of
train and test)  each of which includes training (and self-training in our case) and test. (2) Samples in
meta-train and meta-test are not datapoints but episodes (SSFSC tasks in our case). (3) Meta objective
is not to classify unseen datapoints but to fast adapt the classiﬁer on a new task. Let’s detail the
denotations. Given a dataset D for meta-train  we ﬁrst sample SSFSC tasks {T } from a distribution
p(T ) such that each T has few samples from few classes  e.g. 5 classes and 1 sample per class. T
has a support set S plus an unlabeled set R (with a larger number of samples) to train a task-speciﬁc
SSFSC model  and a query set Q to compute a validation loss used to optimize meta-learners. For
meta-test  given an unseen new dataset Dun  we sample a new SSFSC task Tun. “Unseen” means
there is no overlap of image classes (including distracting classes) between meta-test and meta-train
tasks . We ﬁrst initialize a model and weight pseudo labels for this unseen task  then self-train the
model on Sun and Run. We evaluate the self-training performance on a query set Qun. If we have
multiple unseen tasks  we report average accuracy as the ﬁnal evaluation.

4 Learning to self-train (LST)

The computing ﬂow of applying LST to a single task is given in Figure 1. It contains pseudo-labeling
unlabeled samples by a few-shot model pre-trained on the support set; cherry-picking pseudo-labeled
samples by hard selection and soft weighting; re-training on picked “cherries”  followed by a ﬁne-
tuning step; and the ﬁnal test on a query set. On a meta-train task  ﬁnal test acts as a validation to
output a loss for optimizing meta-learned parameters of LST  as shown in Figure 2.

3

(cid:58)(cid:69)(cid:80)(cid:19)(cid:20)(cid:56)(cid:73)(cid:87)(cid:88)(cid:31)(cid:55)(cid:73)(cid:18)(cid:88)(cid:86)(cid:69)(cid:77)(cid:82)(cid:77)(cid:82)(cid:75)(cid:31)(cid:53)(cid:87)(cid:73)(cid:89)(cid:72)(cid:83)(cid:18)(cid:80)(cid:69)(cid:70)(cid:73)(cid:80)(cid:77)(cid:82)(cid:75)(cid:31)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:73)(cid:72)(cid:90)(cid:16)(cid:86)(cid:75)(cid:82)(cid:87)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:81)(cid:74)(cid:11)(cid:81)(cid:82)(cid:76)(cid:86)(cid:92)(cid:12)(cid:75)(cid:68)(cid:85)(cid:71)(cid:3)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:82)(cid:73)(cid:87)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:76)(cid:81)(cid:74)(cid:11)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:314)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:12)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:73)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:89)(cid:68)(cid:79)(cid:17)(cid:3)(cid:83)(cid:86)(cid:3)(cid:87)(cid:72)(cid:86)(cid:87)(cid:79)(cid:82)(cid:86)(cid:86)(cid:83)(cid:86)(cid:3)(cid:68)(cid:70)(cid:70)(cid:88)(cid:85)(cid:68)(cid:70)(cid:92)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:72)(cid:71)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:73)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:72)(cid:71)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:11)(cid:68)(cid:12)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:81)(cid:74)(cid:11)(cid:69)(cid:12)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:11)(cid:71)(cid:12)(cid:73)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:11)(cid:72)(cid:12)(cid:11)(cid:73)(cid:12)(cid:86)(cid:82)(cid:73)(cid:87)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:76)(cid:81)(cid:74)(cid:11)(cid:70)(cid:12)(cid:3)(cid:314)(cid:14)(cid:40)(cid:76)(cid:73)(cid:86)(cid:86)(cid:93)(cid:18)(cid:84)(cid:77)(cid:71)(cid:79)(cid:77)(cid:82)(cid:75)(cid:31)(cid:39)(cid:72)(cid:87)(cid:68)(cid:76)(cid:79)(cid:72)(cid:71)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:71)(cid:76)(cid:73)(cid:86)(cid:86)(cid:93)(cid:18)(cid:84)(cid:77)(cid:71)(cid:79)(cid:77)(cid:82)(cid:75)(cid:31)(cid:83)(cid:85)(cid:82)(cid:87)(cid:82)(cid:87)(cid:92)(cid:83)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:72)(cid:86)(cid:11)(cid:81)(cid:82)(cid:76)(cid:86)(cid:92)(cid:12)(cid:3)(cid:11)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:12)(cid:17)(cid:20)(cid:17)(cid:20)(cid:17)(cid:24)(cid:17)(cid:21)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:26)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:28)(cid:17)(cid:26)(cid:17)(cid:20)(cid:17)(cid:20)(cid:17)(cid:24)(cid:17)(cid:21)(cid:17)(cid:23)(cid:17)(cid:23)(cid:17)(cid:23)(cid:17)(cid:22)(cid:17)(cid:24)(cid:17)(cid:24)(cid:17)(cid:27)(cid:11)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:12)(cid:17)(cid:24)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:26)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:28)(cid:17)(cid:24)(cid:17)(cid:23)(cid:17)(cid:24)(cid:17)(cid:27)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:12)(cid:12)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:3)(cid:87)(cid:82)(cid:83)(cid:16)(cid:61)(cid:3)(cid:83)(cid:72)(cid:85)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:11)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:12)(cid:17)(cid:24)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:26)(cid:17)(cid:22)(cid:17)(cid:23)(cid:17)(cid:28)(cid:17)(cid:24)(cid:17)(cid:23)(cid:17)(cid:24)(cid:17)(cid:27)(cid:54)(cid:58)(cid:49)(cid:86)(cid:82)(cid:73)(cid:87)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86)(cid:11)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:12)(cid:11)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:12)(cid:70)(cid:82)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:81)(cid:68)(cid:87)(cid:72)(cid:3)(cid:11)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:72)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3)(cid:83)(cid:85)(cid:82)(cid:87)(cid:82)(cid:87)(cid:92)(cid:83)(cid:72)(cid:12)(cid:70)(cid:82)(cid:83)(cid:92)(cid:11)(cid:75)(cid:76)(cid:74)(cid:75)(cid:3)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:86)(cid:70)(cid:82)(cid:85)(cid:72)(cid:86)(cid:12)(cid:11)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:72)(cid:71)(cid:12)(cid:39)(cid:72)(cid:87)(cid:68)(cid:76)(cid:79)(cid:72)(cid:71)(cid:3)(cid:86)(cid:87)(cid:72)(cid:83)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:40)(cid:76)(cid:73)(cid:86)(cid:86)(cid:93)(cid:18)(cid:84)(cid:77)(cid:71)(cid:79)(cid:77)(cid:82)(cid:75)(cid:31)4.1 Pseudo-labeling & cherry-picking unlabeled data

Pseudo-labeling. This step deploys a supervised few-shot method to train a task-speciﬁc classiﬁer
θ on the support set S. Pseudo labels of the unlabeled set R are then predicted by θ. Basically 
we can use different methods to learn θ. We choose a top-performing one – meta-transfer learning
(MTL) [32] (for fair comparison we also evaluate this method as a component of other semi-supervised
methods [24  13]) that is based on simple and elegant gradient descent optimization [3]. In the outer
loop meta-learning  MTL learns scaling and shifting parameters Φss to fast adapt a large-scale
pre-trained network Θ (e.g. for 64 classes and 600 images per class on miniImageNet [36]) to a new
learning task. In the inner loop base-learning  MTL takes the last fully-connected layer as classiﬁer θ
and trains it with S.
In the following  we detail the pseudo-labeling process on a task T . Given the support set S  its loss
is used to optimize the task-speciﬁc base-learner (classiﬁer) θ by gradient descent:

θt ← θt−1 − α∇θt−1 L(cid:0)S; [Φss  θt−1](cid:1) 

Y R = f[Φss θT ](R) 

(1)
where t is the iteration index and t ∈ {1  ...  T}. The initialization θ0 is given by θ(cid:48) which is meta-
learned (see Section 4.2). Once trained  we feed θT with unlabeled samples R to get pseudo labels
Y R as follows 

(2)
where f indicates the classiﬁer function with parameters θT and feature extractor with parameters
Φss (the frozen Θ is omitted for simplicity).
Cherry-picking. As directly applying self-training on pseudo labels Y R may result in gradual drifts
due to label noises  we propose two countermeasures in our LST method. The ﬁrst is to meta-learn
the SWN that automatically reweighs the data points to up-weight the more promising ones and
down-weighs the less promising ones  i.e. learns to cherry-pick. Prior to this step we also perform
hard selection to only use the most conﬁdent predictions [35]. The second countermeasure is to
ﬁne-tune the model with only labeled data (in S) after every self-training step (see Section 4.2).
Speciﬁcally  we refer to the conﬁdent scores of Y R to pick-up the top Z samples per class. Therefore 
we have ZC samples from C classes in this pseudo-labeled dataset  namely Rp. Before feeding Rp
to re-training  we compute their soft weights by a meta-learned soft weighting network (SWN)  in
order to reduce the effect of noisy labels. These weights should reﬂect the relations or distances
between pseudo-labeled samples and the representations of C classes. We refer to a supervised
method called RelationNets [33] which makes use of relations between support and query samples
for traditional few-shot classiﬁcation.
First  we compute the prototype feature of each class by averaging the features of all its samples. In
the 1-shot case  we use the unique sample feature as prototype. Then  given a pseudo-labeled sample
(xi  yi) ∈ Rp  we concatenate its feature with C prototype features  then feed them to SWN. The
weight on the c-th class is as follows 

wi c = fΦswn

(3)
where c is the class index and c ∈ [1  ...  C]  k is the sample index in one class and k ∈ [1  ...  K] 
xc k ∈ S  and Φswn denotes the parameters of SWN whose optimization procedure is given in
Section 4.2. Note that {wi c} have been normalized over C classes through a softmax layer in SWN.

fΦss (xi);

K

k fΦss (xc k)

(cid:16)(cid:104)

(cid:80)

(cid:105)(cid:17)

 

4.2 Self-training on cherry-picked data

As shown in Figure 2 (inner loop)  our self-training contains two main stages. The ﬁrst stage contains
a few steps of re-training on the pseudo-labeled data Rp in conjunction with support set S  and the
second are ﬁne-tuning steps with only S.
We ﬁrst initialize the classiﬁer parameters as θ0 ← θ(cid:48)  where θ(cid:48) is meta-optimized by previous tasks
in the outer loop. We then update θ0 by gradient descent on Rp and S. Assuming there are T
iterations  re-training takes the ﬁrst 1 ∼ m iterations and ﬁne-tuning takes the rest m + 1 ∼ T . For
t ∈ {1  ...  m}  we have

θt ← θt−1 − α (cid:53)θt−1 L(cid:0)S ∪ Rp; [Φswn  Φss  θt−1](cid:1) 

(4)

4

Figure 2: Outer-loop and inner-loop training procedures in our LST method. The inner loop in the
red box contains the m steps of re-training (with S and Rp) and T − m steps of ﬁne-tuning (with
only S). In recursive training  the ﬁne-tuned θT replaces the initial MTL learned θT (see Section 4.1)
for the pseudo-labeling at the next stage.

L(cid:0)S ∪ Rp; [Φswn  Φss  θt](cid:1) =

where α is the base learning rate. L denotes the classiﬁcation losses that are different for samples
from different sets  as follows 

(cid:26) Lce

(cid:0)f[Φswn Φss θt](xi)  yi)  if (xi  yi) ∈ S 
(cid:0)wi (cid:12) f[Φswn Φss θt](xi)  yi)  if (xi  yi) ∈ Rp 

(5)
where Lce is the cross-entropy loss. It is computed in a standard way on S. For a pseudo-labeled
sample in Rp  its predictions are weighted by wi = {wi c}C
c=1 before going into the softmax layer.
For t ∈ {m + 1  ...  T}  θt is ﬁne-tuned on S as

Lce

θt ← θt−1 − α (cid:53)θt−1 L(S; [Φswn  Φss  θt−1]).

(6)

Iterating self-training using ﬁne-tuned model. Conventional self-training often follows an iterative
procedure  aiming to obtain a gradually enlarged labeled set [40  35]. Similarly  our method can be
iterated once a ﬁne-tuned model θT is obtained  i.e. to use θT to predict better pseudo labels on R
and re-train θ again. There are two scenarios: (1) the size of R is small  e.g. 10 samples per class 
so that self-training can only be repeated on the same data; and (2) that size is inﬁnite (at least big
enough  e.g. 100 samples per class)  we can split it into multiple subsets (e.g. 10 subsets and each one
has 10 samples) and do the recursive learning each time on a new subset. In this paper  we consider
the second scenario. We also validate in experiments that ﬁrst splitting subsets and then recursive
training is better than using the whole set for one re-training round.
Meta-optimizing Φswn  Φss and θ(cid:48). Gradient descent base methods typically use θT to compute
the validation loss on query set Q used for optimizing meta-learner [32  3]. In this paper  we have
multiple meta-learners with the parameters Φswn  Φss and θ(cid:48). We propose to update them by the
validation losses calculated at different self-training stages  aiming to optimize them particularly
towards speciﬁc purposes. Φss and θ(cid:48) work for feature extraction and ﬁnal classiﬁcation affecting on
the whole self-training. We optimize them by the loss of the ﬁnal model θT . While  Φswn produces
soft weights to reﬁne the re-training steps  and its quality should be evaluated by re-trained classiﬁer
θm. We thus use the loss of θm to optimize it. Two optimization functions are as follows 

Φswn =: Φswn − β1 (cid:53)Φswn L(Q; [Φswn  Φss  θm]) 

[Φss  θ(cid:48)] − β2 (cid:53)[Φss θ(cid:48)] L(Q; [Φswn  Φss  θT ]) 

[Φss  θ(cid:48)] =:

(7)
(8)

where β1 and β2 are meta learning rates that are manually set in experiments.

5 Experiments

We evaluate the proposed LST method in terms of few-shot image classiﬁcation accuracy in semi-
supervised settings. Below we describe the two benchmarks we evaluate on  details of settings 
comparisons to state-of-the-art methods  and an ablation study.

5

(cid:53)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:87)(cid:88)(cid:73)(cid:84)(cid:4)(cid:21)(cid:53)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:87)(cid:88)(cid:73)(cid:84)(cid:4)(cid:81)(cid:135)(cid:3)(cid:135)(cid:3)(cid:135)(cid:3)(cid:3)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:68)(cid:81)(cid:3)(cid:72)(cid:83)(cid:76)(cid:86)(cid:82)(cid:71)(cid:72)(cid:41)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:72)(cid:3)(cid:87)(cid:88)(cid:73)(cid:84)(cid:4)(cid:81)(cid:15)(cid:21)(cid:41)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:72)(cid:3)(cid:87)(cid:88)(cid:73)(cid:84)(cid:4)(cid:56)(cid:80)(cid:72)(cid:87)(cid:68)(cid:3)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72)(cid:80)(cid:72)(cid:87)(cid:68)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72)(cid:71)(cid:72)(cid:83)(cid:79)(cid:82)(cid:92)(cid:135)(cid:3)(cid:135)(cid:3)(cid:135)(cid:44)(cid:81)(cid:81)(cid:72)(cid:85)(cid:3)(cid:79)(cid:82)(cid:82)(cid:83)(cid:50)(cid:88)(cid:87)(cid:72)(cid:85)(cid:3)(cid:79)(cid:82)(cid:82)(cid:83)5.1 Datasets and implementation details

Datasets. We conduct our experiments on two subsets of ImageNet [26]. miniImageNet was ﬁrstly
proposed by Vinyals et al. [36] and has been widely used in supervised FSC works [3  23  32  27  7  5] 
as well as semi-supervised works [13  24]. In total  there are 100 classes with 600 samples of 84× 84
color images per class. In the uniform setting  these classes are divided into 64  16  and 20 respectively
for meta-train  meta-validation  and meta-test. tieredImageNet was proposed by Ren et al. [24].
It includes a larger number of categories  608 classes  than miniImageNet. These classes are from
34 super-classes which are divided into 20 for meta-train (351 classes)  6 for meta-validation (97
classes)  and 8 for meta-test (160 classes). The average image number per class is 1281  which is
much bigger than that on miniImageNet. All images are resized to 84 × 84. On both datasets  we
follow the semi-supervised task splitting method used in previous works [24  13]. We consider the
5-way classiﬁcation  and sample 5-way  1-shot (5-shot) task to contain 1 (5) samples as the support
set S and 15 samples (a uniform number) samples as the query set Q. Then  on the 1-shot (5-shot)
task  we have 30 (50) unlabeled images per class in the unlabeled set R. After hard selection  we
ﬁlter out 10 (20) samples and only use the rest 20 (30) conﬁdent ones to do soft weighting and then
re-training. In the recursive training  we use a larger unlabeled data pool containing 100 samples from
which each iteration we can sample a number of samples  i.e. 30 (50) samples for 1-shot (5-shot).
Network architectures of Θ and Φss are based on ResNet-12 (see details of MTL [32]) which
consist of 4 residual blocks and each block has 3 CONV layers with 3 × 3 kernels. At the end of
each block  a 2 × 2 max-pooling layer is applied. The number of ﬁlters starts from 64 and is doubled
every next block. Following residual blocks  a mean-pooling layer is applied to compress the feature
maps to a 512-dimension embedding. The architecture of SWN consists of 2 CONV layers with
3 × 3 kernels in 64 ﬁlters  followed by 2 FC layers with the dimensionality of 8 and 1  respectively.
Hyperparameters. We follow the settings used in MTL [32]. Base-learning rate α (in Eq. 1  Eq. 4
and Eq. 6) is set to 0.01. Meta-learning rates β1 and β2 (in Eq. 7 and Eq. 8) are set to 0.001 initially
and decay to the half value every 1k meta iterations until a minimum value 0.0001 is reached. We
use a meta-batch size of 2 and run 15k meta iterations. In recursive training  we use 6 (3) recursive
stages for 1-shot (5-shot) tasks. Each recursive stage contains 10 re-training and 30 ﬁne-tuning steps.
Comparing methods. In terms of SSFSC  we have two methods  namely Soft Masked k-Means [24]
and TPN [13] to compare with. Their original models used a shallow  i.e. 4CONV [3] trained from
scratch  for feature extraction. For fair comparison  we implement the MTL as a component of their
models in order to use deeper nets and pre-trained models which have been proved better. In addition 
we run these experiments using the maximum budget of unlabeled data  i.e. 100 samples per class.
We also compare to the state-of-the-art supervised FSC models which are closely related to ours. They
are based on either data augmentation [15  29] or gradient descent [3  23  7  5  42  19  27  32  10].
Ablative settings. In order to show the effectiveness of our LST method  we design following
settings belonging to two groups: with and without meta-training. Following are the detailed ablative
settings. no selection denotes the baseline of once self-training without any selection of pseudo labels.
hard denotes hard selection of pseudo labels. hard with meta-training means meta-learning only
[Φss  θT ]. soft denotes soft weighting on selected pseudo labels by meta-learned SWN. recursive
applies multiple iterations of self-training based on ﬁne-tuned models  see Section 4.2. Note that this
recursive is only for the meta-test task  as the meta-learned SWN can be repeatedly used. We also
have a comparable setting to recursive called mixing in which we mix all unlabeled subsets used in
recursive and run only one re-training round (see the last second paragraph of Section 4.2).

5.2 Results and analyses

We conduct extensive experiments on semi-supervised few-shot classiﬁcation.
In Table 1  we
present our results compared to the state-of-the-art FSC methods  respectively on miniImageNet and
tieredImageNet. In Table 2  we provide experimental results for ablative settings and comparisons
with the state-of-the-art SSFSC methods. In Figure 3  we show the effect of using different numbers
of re-training steps (i.e. varying m in Figure 2).
Overview for two datasets with FSC methods. In the upper part of Table 1  we present SSFSC
results on miniImageNet. We can see that LST achieves the best performance for the 1-shot (70.1%)
setting  compared to all other FSC methods. Besides  it tackles the 5-shot episodes with an accuracy

6

Few-shot Learning Method

Backbone

WRN-40 (pre)
VGG-16 (pre)
4 CONV

Data augmentation Adv. ResNet  [15]
Delta-encoder  [29]
MAML  [3]
Bilevel Programming  [5] ResNet-12(cid:5)
MetaGAN  [42]
ResNet-12
ResNet-12‡
adaResNet  [19]
WRN-28-10 (pre)
LEO  [27]
MTL  [32]
ResNet-12 (pre)
MetaOpt-SVM  [10]†
ResNet-12
recursive  hard  soft
ResNet-12 (pre)

Gradient descent

LST (Ours)

Few-shot Learning Method

Backbone

miniImageNet (test)
5-shot
1-shot
69.6
55.2
58.7
73.6

48.70 ± 1.75
50.54 ± 0.85
52.71 ± 0.64
56.88 ± 0.62
61.76 ± 0.08
61.2 ± 1.8
62.64 ± 0.61
70.1 ± 1.9

63.11 ± 0.92
64.53 ± 0.68
68.63 ± 0.67
71.94 ± 0.57
77.59 ± 0.12
75.5 ± 0.9
78.63 ± 0.46
78.7 ± 0.8

tieredImageNet (test)
1-shot
5-shot

51.67 ± 1.81
66.33 ± 0.05
65.6 ± 1.8
65.99 ± 0.72
77.7 ± 1.6

70.30 ± 0.08
81.44 ± 0.09
78.6 ± 0.9
81.56 ± 0.53
85.2 ± 0.8

Gradient descent

MAML  [3] (by [13])
LEO  [27]
MTL  [32] (by us)
MetaOpt-SVM  [10]†
recursive  hard  soft

ResNet-12
WRN-28-10 (pre)
ResNet-12 (pre)
ResNet-12
ResNet-12 (pre)

LST (Ours)
(cid:5)Additional 2 convolutional layers ‡One additional convolutional layer
†Using 15-shot training samples on every meta-train task.

Table 1: The 5-way  1-shot and 5-shot classiﬁcation accuracy (%) on miniImageNet and tieredIma-
geNet datasets. “pre” means pre-trained for a single classiﬁcation task using all training datapoints.
Note that this is a reference table to show how much we gain by considering unlabeled data.

mini

tiered

fully supervised (upper bound)

no meta

meta

no selection
hard
recursive hard
hard (Φss  θ(cid:48))
soft
hard soft
recursive hard soft
mixing hard soft

Masked Soft k-Means with MTL
TPN with MTL
Masked Soft k-Means [24]
TPN [13]

1(shot)
80.4
59.7
63.0
64.6
64.1
62.8
65.0
70.1
66.2
62.1
62.7
50.4
52.8

5

83.3
75.2
76.3
77.2
76.9
75.9
77.8
78.7
77.9
73.6
74.2
64.4
66.4

1

86.5
67.4
69.8
72.1
74.7
73.1
75.4
77.7
75.6
68.6
72.1
52.4
55.7

5

88.7
81.1
81.5
82.4
83.2
82.8
83.4
85.2
84.6
81.0
83.3
69.9
71.0

mini w/D
5
1
-
-

tiered w/D
5
1
-
-

54.4
61.6
61.2
62.9
61.1
63.7
64.1
64.5
61.0
61.3
49.0
50.4

73.3
75.3
75.7
75.4
74.6
76.2
77.4
76.5
72.0
72.4
63.0
64.9

66.1
68.8
68.3
73.4
72.1
74.1
73.5
73.6
66.9
71.5
51.4
53.5

79.4
81.1
81.1
82.5
81.7
82.9
83.4
83.8
80.2
82.7
69.1
69.9

Table 2: Classiﬁcation accuracy (%) in ablative settings (middle blocks) and related SSFSC works
(bottom block)  on miniImageNet (“mini”) and tieredImageNet (“tiered”). “fully supervised” means
the labels of unlabeled data are used. “w/D” means using unlabeled data from 3 distracting classes
that are excluded in the support set [13  24]. The results of using a small unlabeled set  5 per
class [24]  are given in the supplementary materials.

of 78.7%. This result is slightly better than 78.6% reported by [10]  which uses various regularization
techniques like data augmentation and label smoothing. Compared to the baseline method MTL [32] 
LST improves the accuracies by 8.9% and 3.2% respectively for 1-shot and 5-shot  which proves
the efﬁciency of LST using unlabeled data. In the lower part of Table 1  we present the results on
tieredImageNet. Our LST performs best in both 1-shot (77.7%) and 5-shot (85.2%) and surpasses
the state-of-the-art method [10] by 11.7% and 3.6% respectively for 1-shot and 5-shot. Compared to
MTL [32]  LST improves the results by 12.1% and 6.6% respectively for 1-shot and 5-shot.

7

(a)

(b)

(c)

Figure 3: Classiﬁcation accuracy on 1-shot miniImageNet using different numbers of re-training steps 
e.g. m = 2 means using 2 steps f re-training and 38 steps (40 steps in total) of ﬁne-tuning at every
recursive stage. Each curve shows the results obtained at the ﬁnal stage. Methods are (a) our LST;
(b) recursive  hard (no meta) with MTL [32]; and (c) recursive  hard (no meta) simply initialized by
pre-trained ResNet-12 model [32]. Results on tieredImageNet are given in the supplementary.

Hard selection. In Table 2  we can see that the hard selection strategy often brings improvements.
For example  compared to no selection  hard can boost the accuracies of 1-shot and 5-shot by 3.3%
and 1.1% respectively on miniImageNet  2.4% and 0.4% respectively on tieredImageNet. This is due
to the fact that selecting more reliable samples can relieve the disturbance brought by noisy labels.
Moreover  simply repeating this strategy (recursive hard) brings about 1% average gain.
SWN. The meta-learned SWN is able to reduce the effect of noisy predictions in a soft way  leading
to better performance. When using SWN individually  soft achieves comparable results with two
previous SSFSC methods [24  13]. When using SWN in cooperation with hard selection (hard soft)
achieves 0.9% improvement on miniImageNet for both 1-shot and 5-shot compared to hard(Φss  θ(cid:48)) 
which also shows that SWN and the hard selection strategy are complementary.
Recursive self-training. Comparing the results of recursive hard with hard  we can see that by
doing recursive self-training when updating θ  the performances are improved in both “meta” and “no
meta” scenarios. E.g.  it boosts the results by 5.1% when applying recursive training to hard soft for
miniImageNet 1-shot. However  when using mixing hard soft that learns all unlabeled data without
recursive  the improvement reduces by 3.9%. These observations show that recursive self-training
can successfully leverage unlabeled samples. However  this method sometimes brings undesirable
results in the cases with distractors. E.g.  compared to hard  the recursive hard brings 0.4% and 0.5%
reduction for 1-shot on miniImagenet and tieredImagenet respectively  which might be due to the fact
that disturbances caused by distractors in early recursive stages propagate to later stages.
Comparing with the state-of-the-art SSFSC methods. We can see that Masked Soft k-Means [24]
and TPN [13] improve their performances by a large margin (more than 10% for 1-shot and 7% for 5-
shot) when they are equipped with MTL and use more unlabeled samples (100 per class). Compared
with them  our method (recursive hard soft) achieves more than 7.4% and 4.5% improvements
respectively for 1-shot and 5-shot cases with the same amount of unlabeled samples on miniImagenet.
Similarly  our method also surpasses TPN by 5.6% and 1.9% for 1-shot and 5-shot on tieredImagenet.
Even though our method is slightly more effected when adding distractors to the unlabeled dataset 
we still obtain the best results compared to others.
In order to better understand our method and validate the robustness  we present more in-depth
results regarding the key components  namely re-training steps  distracting classes  pseudo labeling
accuracies  and using different architectures as backbone  in the following texts.
Number of re-training steps. In Figure 3  we present the results for different re-training steps.
Figure 3(a)  (b) and (c) show different settings respectively: LST; recursive hard that uses the off-
the-shelf MTL method; and recursive hard that replaces MTL with pre-trained ResNet-12 model.
All three ﬁgures show that re-training indeed achieves better results  but too many re-training steps
may lead to drifting problems and cause side effects on performance. The ﬁrst two settings reach
best performance at 10 re-training steps while the third one needs 20 re-training steps. That means
MTL-based methods (LST and the recursive hard) achieve faster convergence compared to the one
directly using pre-trained ResNet-12 model.
Quantitative analyses on the number of distracting classes. In Figure 4  we show the effects of
distracting classes on our LST and related methods (improved versions with MTL) [24  34]. More

8

110203040base-learning iterations0.660.680.70meta-test accuracym=2m=5m=10m=20m=40110203040base-learning iterations0.610.630.65110203040base-learning iterations0.520.560.60(a)

(b)

Figure 4: Classiﬁcation accuracy on miniImageNet 1-shot (a) and tieredImageNet 1-shot (b)  using
different numbers of distracting classes.

distracting classes cause more performance deduction for all methods. Our LST achieves the top
performance  especially more than 2% higher than TPN [13] in the hardest case with 7 distracting
classes. Among our different settings  we can see that LST with less re-training steps  i.e.  a smaller
m value  works better for reducing the effect from a larger number of distracting classes.
The performance of pseudo-labeling.. Taking the miniImageNet 1-shot as an example  we record
the accuracy of pseudo labeling for meta-training and meta-test (based on our best model recursive 
hard  soft)  in Table 3 and Table 4  respectively. In meta-training  we can see the accuracy grows
from 47.0% (iter=0) to 71.5% (iter=15k)  and it reaches saturation after 2k iterations. There are 6
recursive stages during meta-test. From stage-2 to stage-6  the average accuracy of 600 meta-test
tasks using our best method increases from 59.8% to 68.8%.

Iteration
Accuracy

0

47.0

0.2k
64.1

0.5k
65.9

1k
70.0

2k
71.2

5k
70.9

10k
71.3

15k
71.5

Table 3: Pseudo-labeling accuracies (%) during the meta-training process  on miniImageNet 1-shot.

Stage

1

2

3

4

5

6

Accuracy

59.8

63.6

65.1

66.9

67.9

68.8

Table 4: Pseudo-labeling accuracies (%) at six recursive stages of meta-test  on miniImageNet 1-shot.
Stage-1 is initialization.

Generalization ability. Our LST approach is in principle able to generalize to other optimization-
based FSC methods. To validate this  we replace MTL with a classical method called MAML [3].
We implement the experiments of MAML-based LST (using recursive hard soft) and compare with
the same 4CONV-arch model TPN [13]. On miniImagenet 1-shot  our method gets the accuracy of
54.8% (52.0% for w/D)  outperforming TPN by 2.0% (1.6% for w/D). On the more challenging
dataset tieredImageNet (1-shot) we achieve even higher superiority  i.e.  2.9% (2.0% for w/D).

6 Conclusions

We propose a novel LST approach for semi-supervised few-shot classiﬁcation. A novel recursive-
learning-based self-training strategy is proposed for robust convergence of the inner loop  while a
cherry-picking network is meta-learned to select and label the unsupervised data optimized in the
outer loop. Our method is general in the sense that any optimization-based few-shot method with
different base-learner architectures can be employed. On two popular few-shot benchmarks  we
found consistent improvements over both state-of-the-art FSC and SSFSC methods.

Acknowledgments

This research is part of NExT research which is supported by the National Research Foundation 
Prime Minister’s Ofﬁce  Singapore under its IRC@SG Funding Initiative. It is also partially supported
by German Research Foundation (DFG CRC 1223)  and National Natural Science Foundation of
China (61772359  61671289  61771301  61521062).

9

1357the number of distracting classes0.600.640.680.72meta-test accuracyLST: m=10  Z=20LST: m=10  Z=5LST: m=2  Z=20Soft k-Means with MTLTPN with MTL1357the number of distracting classes0.640.700.76meta-test accuracyReferences
[1] Antreas Antoniou  Harrison Edwards  and Amos Storkey. How to train your maml. In ICLR 

2019.

[2] Lee Dong-Hyun. Pseudo-label: The simple and efﬁcient semi-supervised learning method for

deep neural networks. In ICML Workshops  2013.

[3] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adapta-

tion of deep networks. In ICML  2017.

[4] Chelsea Finn  Kelvin Xu  and Sergey Levine. Probabilistic model-agnostic meta-learning. In

NeurIPS  2018.

[5] Luca Franceschi  Paolo Frasconi  Saverio Salzo  Riccardo Grazzi  and Massimiliano Pontil.

Bilevel programming for hyperparameter optimization and meta-learning. In ICML  2018.

[6] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In

NIPS  2004.

[7] Erin Grant  Chelsea Finn  Sergey Levine  Trevor Darrell  and Thomas L. Grifﬁths. Recasting

gradient-based meta-learning as hierarchical bayes. In ICLR  2018.

[8] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image

recognition. In CVPR  2016.

[9] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR 

2017.

[10] Kwonjoon Lee  Subhransu Maji  Avinash Ravichandran  and Stefano Soatto. Meta-learning

with differentiable convex optimization. In CVPR  2019.

[11] Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric

and subspace. In ICML  2018.

[12] Fei-Fei Li  Robert Fergus  and Pietro Perona. One-shot learning of object categories. IEEE

Trans. Pattern Anal. Mach. Intell.  28(4):594–611  2006.

[13] Yanbin Liu  Juho Lee  Minseop Park  Saehoon Kim  and Yi Yang. Transductive propagation

network for few-shot learning. In ICLR  2019.

[14] Yaoyao Liu  Yuting Su  An-An Liu  Tat-Seng Chua  Bernt Schiele  and Qianru Sun. Lcc:
Learning to customize and combine neural networks for few-shot learning. arXiv  1904.08479 
2019.

[15] Akshay Mehrotra and Ambedkar Dukkipati. Generative adversarial residual pairwise networks

for one shot learning. arXiv  1703.08033  2017.

[16] Nikhil Mishra  Mostafa Rohaninejad  Xi Chen  and Pieter Abbeel. Snail: A simple neural

attentive meta-learner. In ICLR  2018.

[17] Takeru Miyato  Andrew M. Dai  and Ian J. Goodfellow. Virtual adversarial training for semi-

supervised text classiﬁcation. arXiv  1605.07725  2016.

[18] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In ICML  2017.

[19] Tsendsuren Munkhdalai  Xingdi Yuan  Soroush Mehri  and Adam Trischler. Rapid adaptation

with conditionally shifted neurons. In ICML  2018.

[20] Avital Oliver  Augustus Odena  Colin A. Raffel  Ekin Dogus Cubuk  and Ian J. Goodfellow.

Realistic evaluation of deep semi-supervised learning algorithms. In NeurIPS  2018.

[21] Chapelle Olivier  Schölkopf Bernhard  and Zien Alexander. Semi-supervised learning  volume

ISBN 978-0-262-03358-9. Cambridge  Mass.: MIT Press  2006.

10

[22] Boris N. Oreshkin  Pau Rodríguez  and Alexandre Lacoste. TADAM: task dependent adaptive

metric for improved few-shot learning. In NeurIPS  2018.

[23] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR 

2017.

[24] Mengye Ren  Eleni Triantaﬁllou  Sachin Ravi  Jake Snell  Kevin Swersky  Joshua B. Tenen-
baum  Hugo Larochelle  and Richard S. Zemel. Meta-learning for semi-supervised few-shot
classiﬁcation. In ICLR  2018.

[25] Marcus Rohrbach  Sandra Ebert  and Bernt Schiele. Transfer learning in a transductive setting.

In NIPS  2013.

[26] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng
Huang  Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 
115(3):211–252  2015.

[27] Andrei A. Rusu  Dushyant Rao  Jakub Sygnowski  Oriol Vinyals  Razvan Pascanu  Simon
Osindero  and Raia Hadsell. Meta-learning with latent embedding optimization. In ICLR  2019.
[28] Adam Santoro  Sergey Bartunov  Matthew Botvinick  Daan Wierstra  and Timothy P. Lillicrap.

Meta-learning with memory-augmented neural networks. In ICML  2016.

[29] Eli Schwartz  Leonid Karlinsky  Joseph Shtok  Sivan Harary  Mattias Marder  Rogério Schmidt
Feris  Abhishek Kumar  Raja Giryes  and Alexander M. Bronstein. Delta-encoder: an effective
sample synthesis method for few-shot object recognition. In NeurIPS  2018.

[30] Evan Shelhamer  Jonathan Long  and Trevor Darrell. Fully convolutional networks for semantic

segmentation. IEEE Trans. Pattern Anal. Mach. Intell.  39(4):640–651  2017.

[31] Jake Snell  Kevin Swersky  and Richard S. Zemel. Prototypical networks for few-shot learning.

In NIPS  2017.

[32] Qianru Sun  Yaoyao Liu  Tat-Seng Chua  and Bernt Schiele. Meta-transfer learning for few-shot

learning. In CVPR  2019.

[33] Flood Sung  Yongxin Yang  Li Zhang  Tao Xiang  Philip H. S. Torr  and Timothy M. Hospedales.

Learning to compare: Relation network for few-shot learning. In CVPR  2018.

[34] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged

consistency targets improve semi-supervised deep learning results. In NIPS  2017.

[35] Isaac Triguero  Salvador García  and Francisco Herrera. Self-labeled techniques for semi-
supervised learning: taxonomy  software and empirical study. Knowl. Inf. Syst.  42(2):245–284 
2015.

[36] Oriol Vinyals  Charles Blundell  Tim Lillicrap  Koray Kavukcuoglu  and Daan Wierstra. Match-

ing networks for one shot learning. In NIPS  2016.

[37] Yu-Xiong Wang  Ross B. Girshick  Martial Hebert  and Bharath Hariharan. Low-shot learning

from imaginary data. In CVPR  2018.

[38] Yongqin Xian  Saurabh Sharma  Bernt Schiele  and Zeynep Akata. f-VAEGAN-D2: A feature

generating framework for any-shot learning. In CVPR  2019.

[39] LeCun Yann  Bengio Yoshua  and Hinton Geoffrey. Deep learning. Nature  521(7553):436 

2015.

[40] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In

ACL  1995.

[41] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. In ICLR  2017.

[42] Ruixiang Zhang  Tong Che  Zoubin Grahahramani  Yoshua Bengio  and Yangqiu Song. Metagan:

An adversarial approach to few-shot learning. In NeurIPS  2018.

11

,Xinzhe Li
Qianru Sun
Yaoyao Liu
Qin Zhou
Shibao Zheng
Tat-Seng Chua
Bernt Schiele