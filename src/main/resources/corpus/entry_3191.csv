2013,Least Informative Dimensions,We present a novel non-parametric method for finding a subspace of stimulus features that contains all information about the response of a system. Our method generalizes similar approaches to this problem such as spike triggered average  spike triggered covariance  or maximally informative dimensions. Instead of maximizing the mutual information between features and responses directly  we use integral probability metrics in kernel Hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses. Since estimators of these metrics access the data via kernels  are easy to compute  and exhibit good theoretical convergence properties  our method can easily be generalized to populations of neurons or spike patterns. By using a particular expansion of the mutual information  we can show that the informative features must contain all information if we can make the uninformative features independent of the rest.,Least Informative Dimensions

Fabian H. Sinz

Department for Neuroethology

Eberhard Karls University T¨ubingen

fabee@epagoge.de

Anna St¨ockl

Department for Functional Zoology

Lund University  Sweden

Anna.Stockl@biol.lu.se

Jan Grewe

Department for Neuroethology

Eberhard Karls University T¨ubingen
jan.grewe@uni-tuebingen.de

Jan Benda

Department for Neuroethology

Eberhard Karls University T¨ubingen
jan.benda@uni-tuebingen.de

Abstract

We present a novel non-parametric method for ﬁnding a subspace of stimulus fea-
tures that contains all information about the response of a system. Our method
generalizes similar approaches to this problem such as spike triggered average 
spike triggered covariance  or maximally informative dimensions. Instead of max-
imizing the mutual information between features and responses directly  we use
integral probability metrics in kernel Hilbert spaces to minimize the information
between uninformative features and the combination of informative features and
responses. Since estimators of these metrics access the data via kernels  are easy
to compute  and exhibit good theoretical convergence properties  our method can
easily be generalized to populations of neurons or spike patterns. By using a par-
ticular expansion of the mutual information  we can show that the informative
features must contain all information if we can make the uninformative features
independent of the rest.

1

Introduction

An important aspect of deciphering the neural code is to determine those stimulus features popula-
tions of sensory neurons are most sensitive to. Approaches to that problem include white noise anal-
ysis [2  14]  in particular spike-triggered average [4] or spike-triggered covariance [3  19]  canonical
correlation analysis or population receptive ﬁelds [12]  generalized linear models [18  15]  or max-
imally informative dimensions [22]. All these techniques have in common that they optimize a
statistical dependency measure between stimuli and spike responses over the choice of a linear sub-
space. The particular algorithms differ in the dimensionality of the subspace they extract (one- vs.
multi-dimensional)  the statistical measure they use (correlation  likelihood  relative entropy)  and
whether an extension to population responses is feasible or not. While spike-triggered average uses
correlation and is restricted to a single subspace  spike-triggered covariance and canonical correla-
tion analysis can already extract multi-dimensional subspaces but are still restricted to second-order
statistics. Maximally informative dimensions is the only technique of the above that can extract
multiple dimensions that are informative also with respect to higher-order statistics. However  an
extension to spike patterns or population responses is not straightforward because of the curse of di-
mensionality. Here we approach the problem from a different perspective and propose an algorithm
that can extract a multi-dimensional subspace containing all relevant information about the neural
responses Y in terms of Shannon’s mutual information (if such a subspace exists). Our method
does not commit to a particular parametric model  and can easily be extended to spike patterns or
population responses.

1

I [Y : X] = I [Y : U   V ] = EX Y

(cid:20)

p (U   V   Y )

p (U   V ) p (Y )
p (Y   V | U )

(cid:20)

log

(cid:21)

(cid:21)

In general  the problem of ﬁnding the most informative subspace of the stimuli X about the re-
sponses Y can be described as ﬁnding an orthogonal matrix Q (a basis for Rn) that separates X
into informative and non-informative features (U   V )
= QX. Since Q is orthogonal  the mutual
information I [X : Y ] between X and Y can be decomposed as [5]

(cid:62)

= I [Y : U ] + EY  V
= I [Y : U ] + EU [I [Y | U : V | U ]] .

p (Y | U ) p (V | U )

log

(1)
Since the two terms on the right hand side of equation (1) are always positive and sum up to the
mutual information between Y and X  two ways to obtain maximally informative features U about
Y would be to either maximize I [Y : U ] or to minimize EU [I [Y |U : V |U ]] via the choice of Q.
The ﬁrst possibility is along the lines of maximally informative dimensions [22] and involves direct
estimation of the mutual information. The second possibility which avoids direct estimation has
been proposed by Fukumizu and colleagues [5  6] (we discuss both in Section 3). Here  we explore
a third possibility  which trades practical advantages against a slightly more restrictive objective. The
idea is to obtain maximally informative features U by making V as independent as possible from
the combination of U and Y . For this reason  we name our approach least informative dimensions
(LID). Formally  least informative dimensions tries to minimize the mutual information between the
pair Y   U and V . Using the chain rule for multi information we can write it as (see supplementary
material)

I [Y   U : V ] = I [Y : X] + I [U : V ] − I [Y : U ] .

(2)

This means that minimizing I [Y   U : V ] is equivalent to maximizing I [Y : U ] while simultane-
ously minimizing I [U : V ]. Note that I [Y   U : V ] = 0 implies I [U : V ] = 0. Therefore  if Q
can be chosen such that I [Y   U : V ] = 0 equation (2) reduces to I [Y : X] = I [Y : U ]  pushing
all information about Y into U.
Since each new choice of Q requires the estimation of the mutual information between (potentially
high-dimensional) variables  direct optimization is hard or unfeasible. For this reason  we resort to
another dependency measure which is easier to estimate but shares its minimum with mutual infor-
mation  that is  it is zero if and only if the mutual information is zero. The objective is to choose Q
such that (Y   U ) and V are independent in that dependency measure. If we can ﬁnd such a Q  then
we know that I [Y   U : V ] is zero as well  which means that U are the most informative features in
terms of the Shannon mutual information. This will allow us to obtain maximally informative fea-
tures without ever having to estimate a mutual information. The easier estimation procedure comes
at the cost of only being able to link the alternative dependency measure to the mutual information
if both of them are zero. If there is no Q that achieves this  we will still get informative features in
the alternative measure  but it is not clear how informative they are in terms of mutual information.

2 Least informative dimensions

This section describes how to efﬁciently ﬁnd a Q such that I [Y   U : V ] = 0 (if such a Q exists).
Unless noted otherwise  (U   V )
= QX where U denotes the informative and V the uninforma-
tive features. The mutual information is a special case of the relative entropy

(cid:62)

DKL [p || q] = EX∼p

(cid:20) log p (X)

(cid:21)

log q (X)

between two distribution p and q. While being linked to the rich theoretical background of Shannon
information theory  the relative entropy is known to be hard to estimate [25]. Alternatives to relative
entropy of increasing practical interest are the integral probability metrics (IPM)  deﬁned as [25  17]
(3)

|EX [f (X)] − EZ [f (Z)]| .

γF (X : Z) = sup
f∈F

Intuitively  the metric in equation (3) searches for a function f  which can detect a difference in
the distributions of two random variables X and Z. If no such witness function can be found  the

2

distributions must be equal. If F is chosen to be a sufﬁciently rich reproducing kernel Hilbert space
H [21]  then the supremum in equation (3) can be computed explicitly and the divergence can be
computed in closed form [7]. This particular type of IPM is called maximum mean discrepancy
(MMD) [9  7  10].
A kernel k : X × X → R is a symmetric function such that the matrix Kij = k (xi  xj) is positive
(semi)-deﬁnite for every selection of points x1  ...  xm ∈ X [21]. In that case  the functions k (·  x)
are elements of a reproducing kernel Hilbert space (RKHS) of functions H. This space is endowed
with a dot product (cid:104)· ·(cid:105)H with the so called reproducing property (cid:104)k (·  x)   f(cid:105)H = f (x) for f ∈ H.
In particular  (cid:104)k (·  x)   k (·  x(cid:48))(cid:105)H = k (x  x(cid:48)). When setting F in equation (3) to be the unit ball in
H  then the IPM can be computed in closed form as the norm of the difference between the mean
functions in H [7  10  8  26]:

γH (X : Z) = (cid:107)EX [k (·  X)] − EZ [k (·  Z)](cid:107)H

= (cid:0)EX X(cid:48)(cid:2)k(cid:0)X  X(cid:48)(cid:1)(cid:3) − 2EX Z [k (X  Z)] + EZ Z(cid:48)(cid:2)k(cid:0)Z  Z(cid:48)(cid:1)(cid:3)(cid:1) 1

(4)

2  

(cid:2)exp(cid:0)it(cid:62)X(cid:1)(cid:3) [26  27]. This means that for characteristic kernels MMD is zero

where the ﬁrst equality is derived in [7]  and second equality uses the bi-linearity of the dot product
and the reproducing property of k. Furthermore  (X  X(cid:48)) ∼ PX × PX and (Z  Z(cid:48)) ∼ PZ × PZ are
two independent random variables drawn from the marginal distributions of X and Z  respectively.
The function EX [k (·  X)] is an embedding of the distribution of X into the RKHS H via
X (cid:55)→ EX [k (·  X)]. If this map is injective  that is  if it uniquely represents the probability distribu-
tion of X  then equation (4) is zero if and only if the probability distributions of X and X(cid:48) are the
same. Kernels with that property are called characteristic in analogy to the characteristic function
φX (t) (cid:55)→ EX
exactly if the relative entropy DKL [p(cid:107)q] is zero as well. Since the mutual information is the relative
entropy between the joint distribution and the products of the marginals  we can use MMD to search
for a Q such that γH (PY  U  V : PY  U × PV ) is zero1  which then implies that I [Y   U : V ] = 0
as well. The ﬁnite sample version of (4) is simply given by replacing the expectations with the
empirical mean (and possibly some bias correction) [7  10  8]. The estimation of γH therefore only
involves summation over three kernel matrices and can be done in a few lines of code. Unlike for
√
the relative entropy  the empirical estimation of MMD is therefore much more feasible. Further-
more  the residual error of the empirical estimator can be shown to decrease on the order of 1/
m
where m is the number of data points [25]. Note in particular  that this rate does not depend on the
dimensionality of the data.

Objective function The objective function for our optimization problem now has the following
form: We transform input examples xi into features ui and vi via (ui  vi) = Qxi. Then we use a

kernel k(cid:0)(ui  vi  yi)  (cid:0)uj  vj  yj

Q. In order to do that efﬁciently  a few adaptations are required. First  without loss of generality  we
minimize the squared MMD instead of MMD itself

1

(cid:1)(cid:3) − 2EZ1 Z2 [k (Z1  Z2)] + EZ2 Z(cid:48)

(cid:1)(cid:1) to compute and minimize MMD with respect to the choice of
(cid:2)k(cid:0)Z1  Z(cid:48)
(cid:1)(cid:3)   (5)
(cid:1)(cid:1) · k2 (vi  vj). For this special case  one can
(cid:1)(cid:1) = k1
(cid:0)(ui  yi)  (cid:0)uj  yj
(cid:1)(cid:1)(cid:3) E [k2 (vi  vj)] .
(cid:0)(ui  yi)  (cid:0)uj  yj
(cid:1)(cid:1) · k2 (vi  vj)(cid:3) = E(cid:2)k1

(cid:2)k(cid:0)Z2  Z(cid:48)

2

where Z1 = (Y   U   V ) ∼ PY  U  V and Z2 = (Y   U   V ) ∼ PY  U × PV .
Second  in order to get samples from PY  U × PV   we assume that our kernel takes the form

2

incorporate the independence assumption between U   Y and V directly by using the fact that for
independent random variables  the expectation of the product is equal to the product of expectations 
that is 

γ2H (Z1  Z2) = EZ1 Z(cid:48)

1

k(cid:0)(ui  vi  yi)  (cid:0)uj  vj  yj
(cid:0)(ui  yi)  (cid:0)uj  yj
E(cid:2)k1

This special case of MMD is equivalent to the Hilbert-Schmidt Independence Criterion (HSIC)
[9  23] and can be computed as

(6)
where K1 and K2 denote the matrices of pairwise kernel values between the data sets {(ui  yi)}m
and {vi}m

i=1  respectively  and Hij = δij − m−1.

(m − 1)2 tr (K1HK2H)  

ˆγ2
hs =

i=1

1

1With some abuse of notation  we wrote MMD as a function of the probability measures.

3

Note  however  that one could in principle also optimize (5) for a non-factorizing kernel by simply
shufﬂing the (ui  yi) and vi across examples. We can also use shufﬂing to assess whether the
hs found during the optimization is signiﬁcantly different from zero by comparing
optimal value ˆγ2
the value to a null distribution over ˆγ2
hs obtained from datasets where the (ui  yi) and vi have been
permuted across examples.

hs  then we show how to compute the ∇ui vi ˆγ2

Minimization procedure and gradients For optimizing (6) with respect to Q we use gradient
descent over the orthogonal group SO(n). The optimization can be carried out by computing the
unconstrained gradient ∇Qγ of the objective function with respect to Q (treating Q as an ordinary
matrix)  projecting that gradient onto the tangent space of SO (n)  and performing a line search
along the gradient direction. We now present the necessary formulae to implement the optimization
in a modular fashion. We ﬁrst show how to compute the gradient ∇Qγ in terms of the gradients
∇ui vi ˆγ2
hs in terms of derivatives of kernel functions 
and ﬁnally demonstrate how the formulae change when approximating the kernel matrices with an
incomplete Cholesky decomposition.
Given the unconstrained gradient ∇Qγ the projection onto the tangent space is given by ζ =
Q∇Qγ(cid:62)Q − ∇Qγ [13  eq. (22)]. The function is then minimized by performing a line-search
along π (Q + tζ)  where π is the projection onto SO (n) which can easily be computed via singular
value decomposition of Q + tζ and setting the singular values to one [13  prop. 7].
This means that all we need for the gradient descent on SO(n) is the unconstrained gradient ∇Qγ.
This gradient takes the form of a sum of outer products [16  eq. (20)]

∇Qˆγ2

hs =

∂ˆγ2
hs

· x(cid:62)

i = J(cid:62)Ξ 

J =

∂ (ui  vi)

i=1

(cid:18) ∂ˆγ2

hs

∂ (ui  vi)

(cid:19)

 

i

m(cid:88)

(cid:16)
(cid:17)

where the matrix Ξ contains the stimuli xi in its rows.
The ﬁrst k columns J (u)
J (v) corresponding to the dimension of the features vi are given by
2

2

η

HK2HD(u)(cid:62)

corresponding to the dimension of the features ui and the last n−k columns

(cid:17)

HK1HD(v)(cid:62)

η

 

J (u)
η =

where

(m − 1)2 diag
(cid:16)

D(u)

η

=

ij

and

J (v)
η =

(m − 1)2 diag

k(cid:0)(ui  vi  yi)  (cid:0)uj  vj  yj

(cid:1)(cid:1)(cid:19)

η

(cid:17)
(cid:18) ∂

∂uiη

(cid:16)

ij

contains the partial derivatives of the kernel with respect to the ηth dimension of u (and analogously
for v) in the ﬁrst argument (see supplementary material for the derivation).

Efﬁcient implementation with incomplete Cholesky decomposition of the kernel matrix So
far  the evaluation of HSIC requires the computation of two m× m kernel matrices in each step. For
larger datasets this can quickly become computationally prohibitive. In order to speed up computa-
tion time  we approximate the kernel matrices by an incomplete Cholesky decomposition K = LL(cid:62) 
where L ∈ Rm×(cid:96) is a “tall” matrix [1]. In that case  HSIC can be computed much faster as the trace
of a product of two (cid:96) × (cid:96) matrices because

tr (K1HK2H) = tr(cid:0)L(cid:62)

1 H 2L2L(cid:62)

2 H 2L1

(cid:1)  

where HLk can be efﬁciently computed by centering Lk on its row mean. Also in this case  the
matrix J can be computed efﬁciently in terms of derivatives of sub-matrices of the kernel matrix
(see supplementary material for the exact formulae).

3 Related work

Kernel dimension reduction in regression [5  6] Fukumizu and colleagues ﬁnd maximally in-
formative features U by minimizing EU [I [V | U : Y | U ]] in equation (1) via conditional kernel

4

covariance operators. They show that the covariance operator equals zero if and only if Y is con-
ditionally independent of V given U  that is  Y ⊥⊥V | U. In that case  U carries all information
about Y . Although their approach is closest to ours  it differs in a few key aspects: In contrast to our
approach  their objective involves the inversion of a—potentially large—kernel matrix which needs
additional regularization in order to be invertible. A conceptual difference is that we are optimizing
a slightly more restrictive problem because their objective does not attempt to make U independent
of V as well. However  this will not make a difference in many practical cases  since many stimulus
distributions are Gaussian for which the dependencies between U and V can be removed by pre-
whitening the stimulus data before training LID. In that case I [U : V ] = 0 for every choice of Q
and equation (2) becomes equivalent to maximizing the mutual information between U and Y . The
advantage of our formulation of the problem is that it allows us to detect and quantify independence
by comparing the current ˆγhs to its null distribution obtained by shufﬂing the (yi  ui) against vi
across examples. This is hardly possible in the conditional case. Also note that for spherically sym-
metric data I [U : V ] = const. for every choice of Q. In that case equation (2) becomes equivalent
to maximizing I [Y : U ]. However  a residual redundancy remains which would show up when
comparing ˆγ2
hs to its null distribution. Finally  the use of kernel covariance operators is bound to
kernels that factorize. In principle  our method is also applicable to non-factorizing kernels if we use
γH instead of γhs and obtain the samples from the product distribution of PY  U × PV via shufﬂing.

Maximally informative dimensions [22] Sharpee and colleagues maximize the relative entropy
Ispike = DKL
mative dimensions given a spike  to the marginal distribution of the projection. This relative entropy
is the part of the mutual information which is carried by the arrival of a single spike  since

(cid:2)p(cid:0)v(cid:62)s|spike(cid:1) || p(cid:0)v(cid:62)s(cid:1)(cid:3) between the distribution of stimuli projected onto infor-
I(cid:2)v(cid:62)s : {spike  no spike}(cid:3) = p (spike) · Ispike + p (no spike) Ino spike.

Their method is also completely non-parametric and captures higher order dependencies between
a stimulus and a single spike. However  by focusing on single spikes and the spike triggered den-
sity only  it neglects the dependencies between spikes and the information carried by the silence
of the neuron [28]. Additionally  the generalization to spike patterns or population responses is
non-trivial because the information between the projected stimuli and spike patterns 1  ...  (cid:96) be-
i p (i) · Ii. This requires the estimation of a conditional distribution

(cid:1) for each pattern i which can quickly become prohibitive when the number of patterns

comes I(cid:2)v(cid:62)s : (cid:3) =(cid:80)
p(cid:0)v(cid:62)s|i

grows exponentially.

4 Experiments

In all the experiments below  we demonstrate the validity of our methods on controlled artiﬁcial
examples and on P-unit recordings from electric ﬁsh. We use an RBF kernel on the vi and a tensor
RBF kernel on the (ui  yi):

(cid:18)

−(cid:107)vi − vj(cid:107)2

σ2

(cid:19)

and k(cid:0)(ui  yi)  (cid:0)uj  yj

(cid:1)(cid:1) = exp

(cid:32)

(cid:33)

.

−(cid:107)uiy(cid:62)

i − ujy(cid:62)
j (cid:107)2

σ2

k (vi  vj) = exp

The derivatives of the kernels can be found in the supplementary material. Unless noted otherwise
the σ were chosen to be the median of pairwise Euclidean distances between data points. In all
artiﬁcial experiments  Q was chosen randomly.

nonlinear Poisson (LNP) neuron yi ∼ Poisson(cid:0)(cid:98)(cid:104)w  xi(cid:105) − θ(cid:99)+

Linear Non-Linear Poisson Model (LNP)

(cid:1) with an exponentially decaying

In this experiment  we trained LID on a simple linear

ﬁlter and a rectifying non-linearity (see Figure 1  left). We used m = 5000 data points xi from
a 20-dimensional standard normal distribution N (0  I) as input. The offset was chosen such that
approximately 35% non-zero spike counts in the yi were obtained. We used one informative and 19
non-informative dimensions  and set σ = 1 for the tensor kernel.
After optimization  the ﬁrst dimension q1 of Q converged to the ﬁlter w (Figure 1). We compared
the HSIC values ˆγhs
before and after the optimization to their
null distribution obtained by shufﬂing. Before the optimization  the dependence of (Y   U ) and V

(cid:104){(yi  ui)}i=1 ... m : {vi}i=1 ... m

(cid:105)

5

Figure 1: Left: LNP Model. The informative dimension (gray during optimization  black after op-
timization) converges to the true ﬁlter of an LNP model (blue line). Before optimization (Y   U ) and
V are dependent as shown by the left inset (null distribution obtained via shufﬂing in gray  dashed
line shows actual HSIC value). After the optimization (right inset) the HSIC value is even below
the null distribution. Right: Two state neuron. LID correctly identiﬁes the subspace (blue dashed)
in which the two true ﬁlters (solid black) reside since projections of the ﬁlters on the subspace (red
dashed) closely resemble the original ﬁlters.

is correctly detected (Figure 1  left  insets). After convergence the actual HSIC value lies left to the
null distribution’s domain. Since the appropriate test for independence would be one-sided  the null
hypothesis “(Y   U ) is independent of V ” would not be rejected in this case.

Two state neuron In this experiment  we simulated a neuron with two states that were both at-
tained in 50% of the trials (see Figure 1  right). This time  the output consisted of four “bins”
whose statistics varied depending on the state. In the ﬁrst—steady rate—state  the four bins con-
tained spike counts drawn from an LNP neuron with exponentially decaying ﬁlter as above. In the
second—burst—state  the ﬁrst two bins were drawn from Poisson distribution with a ﬁxed base rate
independent of the stimulus. The second two bins were drawn from an LNP neuron with a modu-
lated exponential ﬁlter and higher gain. We used m = 8000 input stimuli from a 20-dimensional
standard normal distribution. We use two informative dimensions and set σ of the tensor kernel to
two times the median of the pairwise distances. LID correctly identiﬁed the subspace associated
with the two ﬁlters also in this case (Figure 1  right).

Artiﬁcial complex cell
In a second experiment  we estimated the two-dimensional subspace as-
sociated with a artiﬁcial complex cell. We generated a quadrature pair w1 and w2 of two 10-
dimensional ﬁlters (see Figure 2  left). We used m = 8000 input points from a standard nor-
mal distribution. Responses were generated from a Poisson distribution with the rate given by
λi = (cid:104)w1  xi(cid:105)2 + (cid:104)w2  xi(cid:105)2. This led to about 34% non-zero neural responses. When using two
informative subspaces  LID was able to identify the subspace correctly (Figure 2  left). When com-
paring the HSIC value against the null distribution found via shufﬂing  the ﬁnal value indicated no
further dependencies. When only a one-dimensional subspace was used (Figure 2  right)  LID did
not converge to the correct subspace. Importantly  the HSIC value after optimization was clearly
outside the support of the null distribution  thereby correctly indicating residual dependencies.

P-Unit recordings from weakly electric ﬁsh Finally  we applied our method to P-unit recordings
from the weakly electric ﬁsh Eigenmannia virescens. These weakly electric ﬁsh generate a dipole-
like electric ﬁeld which changes polarity with a frequency at about 300Hz. Sensors in the skin of the
ﬁsh are tuned to this carrier frequency and respond to amplitude changes caused by close-by objects
with different conductive properties than water [20]. In the present recordings  the immobilized ﬁsh
was stimulated with 10s of 300 − 600Hz low-pass ﬁltered full ﬁeld frozen Gaussian white noise
amplitude modulations of its own ﬁeld. Neural activity was recorded intra-cellularly from the P-unit
afferents.
Spikes were binned with 1ms precision. We selected m = 8400 random time points in the spike
response and the corresponding preceding 20ms of the input (20 dimensions). We used the same

6

Figure 2: Artiﬁcial Complex Cell. Left: The original ﬁlters are 90° phase shifted Gabor ﬁlters
which form an orthogonal basis for a two-dimensional subspace. After optimization  the two infor-
mative dimensions of LID (ﬁrst two rows of Q) converge to that subspace and also form a pair of
90° phase shifted ﬁlters (note that even if the ﬁlters are not the same  they span the same subspace).
Comparing the HSIC values before and after optimization shows that this subspace contains the
relevant information (left and right inset). Right: If only a one-dimensional informative subspace
is used  the ﬁlter only slightly converges to the subspace. After optimization  a comparison of the
HSIC value to the null distribution obtained via shufﬂing indicates residual dependencies which are
not explained by the one-dimensional subspace (left and right inset).

Figure 3: Most informative feature for a weakly electric ﬁsh P-Unit: A random ﬁlter (blue trace)
exhibits HSIC values that are clearly outside the domain of the null distribution (left inset). Using
the spike triggered average (red trace) moves the HSIC values of the ﬁrst feature of Q already inside
the null distribution (middle inset). Further optimization with LID reﬁnes the feature (black trace)
and brings the HSIC values closer to zero (right inset). After optimization  the informative feature
U is independent of the features V because the ﬁrst row and column of the covariance matrix of the
transformed Gaussian input show no correlations. The fact that one informative feature is sufﬁcient
to bring the HSIC values inside the null distribution indicates that a single subspace captures all
information conveyed by these sensory neurons.

kernels as in the experiment on the LNP model. We initialized the ﬁrst row in Q with the normal-
ized spike triggered average (STA; Figure 3  left  red trace). We neither pre-whitened the data for
computing the STA nor for the optimization of LID. Unlike a random feature (Figure 3  left  blue
trace)  the spike triggered average already achieves HSIC values within the null distribution (Figure
3  left and middle inset). The most informative feature corresponding to U looks very similar to the
STA but shifts the HSIC value deeper into the domain of the null distribution (Figure 3  right inset).

7

This indicates that one single subspace in the input is sufﬁcient to carry all information between the
input and the neural response.

5 Discussion

Here we presented a non-parametric method to estimate a subspace of the stimulus space that con-
tains all information about a response variable Y . Even though our method is completely generic
and applicable to arbitrary input-output pairs of data  we focused on the application in the con-
text of sensory neuroscience. The advantage of the generic approach is that Y can in principle be
anything from spike counts  to spike patterns or population responses. Since our method ﬁnds the
most informative dimensions by making the complement of those dimensions as independent from
the data as possible  we termed it least informative dimensions (LID). We use the Hilbert-Schmidt
independence criterion to minimize the dependencies between the uninformative features and the
combination of informative features and outputs. This measure is easy to implement  avoids the
need to estimate mutual information  and its estimator has good convergence properties independent
of the dimensionality of the data. Even though our approach only estimates the informative features
and not mutual information itself  it can help to estimate mutual information by reducing the number
of dimensions.
it might be that no Q exists such that
As in the approach by Fukumizu and colleagues 
I [Y   U : V ] = 0.
In that situation  the price to pay for an easier measure is that it is hard to
make deﬁnite statements about the informativeness of the features U in terms of the Shannon infor-
mation  since γH = I [Y   U : V ] = 0 is the point that connects γH to the mutual information. As
demonstrated in the experiments  we can detect this case by comparing the actual value of ˆγH to an
empirical null distribution of ˆγH values obtained by shufﬂing the vi against the ui  yi pairs. How-
ever  if γH (cid:54)= 0  theoretical upper bounds on the mutual information are unfortunately not available.
In fact  using results from [25] and Pinsker’s inequality one can show that γ2H bounds the mutual
information from below. One might now be tempted to think that maximizing γH [Y   U ] might be a
better way to ﬁnd informative features. While this might be a way to get some informative features
[24]  it is not possible to link the features to informativeness in terms of Shannon mutual informa-
tion  because the point that builds the bridge between the two dependency measures is where both
of them are zero. Anywhere else the bound may not be tight so the maximally informative features
in terms of γH and in terms of mutual information can be different.
Another problem our approach shares with many algorithms that detect higher-order dependencies
is the non-convexity of the objective function. In practice  we found that the degree to which this
poses a problem very much depends on the problem at hand. For instance  while the subspaces of
the LNP or the two state neuron were detected reliably  the two dimensional subspace of the artiﬁcial
complex cell seems to pose a harder problem. It is likely that the choice of kernel has an inﬂuence
on the landscape of the objective function. We plan to explore this relationship in more detail in the
future. In general  a good initialization of Q helps to get close to the global optimum.
Beyond that  however  integral probability metric approaches to maximally informative dimensions
offer a great chance to avoid many problems associated with direct estimation of mutual information 
and to extend it to much more interesting output structures than single spikes.

Acknowledgements

Fabian Sinz would like to thank Lucas Theis and Sebastian Gerwinn for helpful discussions and comments
on the manuscript. This study is part of the research program of the Bernstein Center for Computational
Neuroscience  T¨ubingen  funded by the German Federal Ministry of Education and Research (BMBF; FKZ:
01GQ1002).

References
[1] F. R. Bach and M. I. Jordan. Predictive low-rank decomposition for kernel methods. In Proceedings of
the 22nd international conference on Machine learning - ICML ’05  pages 33–40  New York  New York 
USA  2005. ACM Press.

[2] E. D. Boer and P. Kuyper. Triggered Correlation  1968.

8

[3] N. Brenner  W. Bialek  and R. De Ruyter Van Steveninck. Adaptive rescaling maximizes information

transmission. Neuron  26(3):695–702  2000.

[4] E. J. Chichilnisky. A simple white noise analysis of neuronal light responses. Network: Comput. Neural

Syst  12:199–213  2001.

[5] K. Fukumizu  F. R. Bach  and M. I. Jordan. Dimensionality Reduction for Supervised Learning with

Reproducing Kernel Hilbert Spaces. Journal of Machine Learning Research  5(1):73–99  2004.

[6] K. Fukumizu  F. R. Bach  and M. I. Jordan. Kernel dimension reduction in regression. Annals of Statistics 

37(4):1871–1905  2009.

[7] A. Gretton  K. M. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel method for the two sample
problem. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information Processing
Systems 19  pages 513—-520  Cambridge  MA  2007. MIT Press.

[8] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Sch¨olkopf  and A. Smola. A Kernel Two-Sample Test.

Journal of Machine Learning Research  13:723–773  2012.

[9] A. Gretton  O. Bousquet  A. Smola  and B. Sch¨olkopf. Measuring Statistical Dependence with Hilbert-
Schmidt Norms. In S. Jain  H. U. Simon  and E. Tomita  editors  Advances in Neural Information Pro-
cessing Systems  pages 63–77. Springer Berlin / Heidelberg  2005.

[10] A. Gretton  K. Fukumizu  Z. Harchaoui  and B. K. Sriperumbudur. A Fast  Consistent Kernel Two-Sample
Test. In Y Bengio  D Schuurmans  J Lafferty  C K I Williams  and A Culotta  editors  Advances in Neural
Information Processing Systems  pages 673–681. Curran  Red Hook  NY  USA  2009.

[11] J. D. Hunter. Matplotlib: A 2D graphics environment. Computing In Science & Engineering  9(3):90–95 

2007.

[12] J. Macke  G. Zeck  and M. Bethge. Receptive Fields without Spike-Triggering. Advances in Neural

Information Processing Systems 20  pages 1–8  2007.

[13] J. H. Manton. Optimization algorithms exploiting unitary constraints. Signal Processing  IEEE Transac-

tions on  50(3):635–650  2002.

[14] P. Z. Marmarelis and K. Naka. White-noise analysis of a neuron chain: an application of the Wiener

theory. Science  175(27):1276–1278  1972.

[15] P McCullagh and J A Nelder. Generalized Linear Models  Second Edition. Chapman and Hall  1989.
[16] T. P. Minka. Old and New Matrix Algebra Useful for Statistics. MIT Media Lab Note  pages 1–19  2000.
[17] A. M¨uller. Integral Probability Metrics and Their Generating Classes of Functions. Advances in Applied

Probability  29(2):429–443  1997.

[18] L. Paninski. Maximum likelihood estimation of cascade point-process neural encoding models. Network:

Computation in Neural Systems  15(4):243–262  2004.

[19] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: an information-theoretic
generalization of spike-triggered average and covariance analysis. Journal of Vision  6(4):414–428  2006.
[20] H. Scheich  T. H. Bullock  and R. H Hamstra. Coding properties of two classes of afferent nerve ﬁbers:
high-frequency electroreceptors in the electric ﬁsh  Eigenmannia. Journal of Neurophysiology  36(1):39–
60  1973.

[21] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Opti-

mization  and Beyond  volume 98 of Adaptive computation and machine learning. MIT Press  2001.

[22] T. Sharpee  N. C. Rust  and W. Bialek. Analyzing neural responses to natural signals: maximally infor-

mative dimensions. Neural Computation  16(2):223–250  2004.

[23] A. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert Space Embedding for Distribu-
tions. In Algorithmic Learning Theory: 18th International Conference  pages 13–31. Springer-Verlag 
Berlin/Heidelberg  2007.

[24] L. Song  A. Smola  A. Gretton  J. Bedo  and K. Borgwardt. Feature selection via dependence maximiza-

tion. Journal of Machine Learning Research  13(May):1393–1434  2012.

[25] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  and G. R. G. Lanckriet. On Integral Probability Metrics 

phi-divergences and binary classiﬁcation. Technical Report 1  arXiv  2009.

[26] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Injective Hilbert Space
Embeddings of Probability Measures. In Proceedings of the 21st Annual Conference on Learning Theory 
number i  pages 111–122. Omnipress  2008.

[27] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and G.R. G. Lanckriet. Hilbert Space
Embeddings and Metrics on Probability Measures. Journal of Machine Learning Research  11(1):48 
2010.

[28] R. S. Williamson  M. Sahani  and J. W. Pillow. Equating information-theoretic and likelihood-based

methods for neural dimensionality reduction. Technical Report 1  arXiv  2013.

9

,Fabian Sinz
Anna Stockl
Jan Grewe
Jan Benda
Shuyang Gao
Greg Ver Steeg
Aram Galstyan