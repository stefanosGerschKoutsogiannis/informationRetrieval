2018,Statistical and Computational Trade-Offs in Kernel K-Means,We investigate the efficiency of k-means  in terms of both statistical and computational requirements.
More precisely   we study  a Nystr\"om approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves  the same accuracy of exact kernel k-means with only a fraction of computations.
Indeed  we prove under basic assumptions  that sampling  $\sqrt{n}$ Nystr\"om  landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result showing in this kind for unsupervised learning.,Statistical and Computational Trade-Offs

in Kernel K-Means

Daniele Calandriello
LCSL – IIT & MIT 

Genoa  Italy

Lorenzo Rosasco
University of Genoa 
LCSL – IIT & MIT

Abstract

We investigate the efﬁciency of k-means in terms of both statistical and compu-
tational requirements. More precisely  we study a Nyström approach to kernel
k-means. We analyze the statistical properties of the proposed method and show
√
that it achieves the same accuracy of exact kernel k-means with only a fraction
of computations. Indeed  we prove under basic assumptions that sampling
n
Nyström landmarks allows to greatly reduce computational costs without incurring
in any loss of accuracy. To the best of our knowledge this is the ﬁrst result of this
kind for unsupervised learning.

1

Introduction

Modern applications require machine learning algorithms to be accurate as well as computationally
efﬁcient  since data-sets are increasing in size and dimensions. Understanding the interplay and
trade-offs between statistical and computational requirements is then crucial [31  30]. In this paper 
we consider this question in the context of clustering  considering a popular nonparametric approach 
namely kernel k-means [33]. K-means is arguably one of most common approaches to clustering and
produces clusters with piece-wise linear boundaries. Its kernel version allows to consider nonlinear
boundaries  greatly improving the ﬂexibility of the approach. Its statistical properties have been
studied [15  24  10] and from a computational point of view it requires manipulating an empirical
kernel matrix. As for other kernel methods  this latter operation becomes unfeasible for large scale
problems and deriving approximate computations is subject of a large body of recent works  see for
example [34  16  29  35  25] and reference therein.
In this paper we are interested into quantifying the statistical effect of computational approximations.
Arguably one could expect the latter to induce some loss of accuracy. In fact  we prove that  perhaps
surprisingly  there are favorable regimes where it is possible maintain optimal statistical accuracy
while signiﬁcantly reducing computational costs. While a similar phenomenon has been recently
shown in supervised learning [31  30  12]  we are not aware of similar results for other learning tasks.
Our approach is based on considering a Nyström approach to kernel k-means based on sampling a
subset of training set points (landmarks) that can be used to approximate the kernel matrix [3  34 
13  14  35  25]. While there is a vast literature on the properties of Nyström methods for kernel
approximations [25  3]  experience from supervised learning show that better results can be derived
focusing on the task of interest  see discussion in [7]. The properties of Nyström approximations for
k-means has been recently studied in [35  25]. Here they focus only on the computational aspect of
the problem  and provide fast methods that achieve an empirical cost only a multiplicative factor
larger than the optimal one.
Our analysis is aimed at combining both statistical and computational results. Towards this end
we derive a novel additive bound on the empirical cost that can be used to bound the true object
of interest: the expected cost. This result can be combined with probabilistic results to show that
optimal statistical accuracy can be obtained considering only O(
n) Nyström landmark points 

√

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

where n is the number of training set of points. Moreover  we show similar bounds not only for the
optimal solution  which is hard to compute in general  but also for approximate solutions that can be
computed efﬁciently using k-means++. From a computational point of view this leads to massive
improvements reducing the memory complexity from O(n2) to O(n
n). Experimental results on
large scale data-sets conﬁrm and illustrate our ﬁndings.
The rest of the paper is organized as follows. We ﬁrst overview kernel k-means  and introduce our
approximate kernel k-means approach based on Nyström embeddings. We then prove our statistical
and computational guarantees and empirically validate them. Finally  we present some limits of our
analysis  and open questions.

√

i=1

from µ  we denote with µn(A) = (1/n)(cid:80)n

2 Background
Notation Given an input space X   a sampling distribution µ  and n samples {xi}n
i=1 drawn i.i.d.
I{xi ∈ A} the empirical distribution. Once the data
has been sampled  we use the feature map ϕ(·) : X → H to maps X into a Reproducing Kernel
Hilbert Space (RKHS) H [32]  that we assume separable  such that for any x ∈ X we have φ = ϕ(x).
Intuitively  in the rest of the paper the reader can assume that φ ∈ RD with D (cid:29) n or even inﬁnite.
Using the kernel trick [2] we also know that φTφ(cid:48) = K(x  x(cid:48))  where K is the kernel function
associated with H and φTφ(cid:48) = (cid:104)φ  φ(cid:48)(cid:105)H is a short-hand for the inner product in H. With a slight
abuse of notation we will also deﬁne the norm (cid:107)φ(cid:107)2 = φTφ  and assume that (cid:107)ϕ(x)(cid:107)2 ≤ κ2 for
any x ∈ X . Using φi = ϕ(xi)  we denote with D = {φi}n
i=1 the input dataset. We also represent
the dataset as the map Φn = [φ1  . . .   φn] : Rn → H with φi as its i-th column. We denote with
nΦn the empirical kernel matrix with entries [Kn]i j = ki j. Finally  given Φn we denote as
Kn = ΦT
n)+ the orthogonal projection matrix on the span Hn = Im(Φn) of the dataset.
Πn = ΦnΦT
k-mean’s objective Given our dataset  we are interested in partitioning it into k disjoint clusters
each characterized by its centroid cj. The Voronoi cell associated with a centroid cj is deﬁned as the
set Cj := {i : j = arg mins=[k] (cid:107)φi − cs(cid:107)2}  or in other words a point φi ∈ D belongs to the j-th
cluster if cj is its closest centroid. Let C = [c1  . . . ck] be a collection of k centroids from H. We
can now formalize the criterion we use to measure clustering quality.
Deﬁnition 1. The empirical and expected squared norm criterion are deﬁned as

n(ΦnΦT

(cid:20)

(cid:21)

W (C  µn) :=

1
n

(cid:107)φi − cj(cid:107)2 

min

j=1 ... k

W (C  µ) := E
φ∼µ

min

j=1 ... k

(cid:107)φ − cj(cid:107)2

.

The empirical risk minimizer (ERM) is deﬁned as Cn := arg minC∈Hk W (C  µn).
The sub-script n in Cn indicates that it minimizes W (C  µn) for the n samples in D. Biau et al. [10]
gives us a bound on the excess risk of the empirical risk minimizer.
Proposition 1 ([10]). The excess risk E(Cn) of the empirical risk minimizer Cn satisﬁes

E(Cn) := ED∼µ [W (Cn  µ)] − W ∗(µ) ≤ O(cid:0)k/

k times larger than a corresponding O((cid:112)k/n)

where W ∗(µ) := inf C∈Hk W (C  µ) is the optimal clustering risk.
From a theoretical perspective  this result is only
lower bound [18]  and therefore shows that the ERM Cn achieve an excess risk optimal in n. From a
computational perspective  Deﬁnition 1 cannot be directly used to compute Cn  since the points φi in
H cannot be directly represented. Nonetheless  due to properties of the squared norm criterion  each
cj ∈ Cn must be the mean of all φi associated with that center  i.e.  Cn belongs to Hn. Therefore  it
can be explicitly represented as a sum of the φi points included in the j-th cluster  i.e.  all the points
in the j-th Voronoi cell Cj. Let V be the space of all possible disjoint partitions [C1  . . .  Cj]. We can
use this fact  together with the kernel trick  to reformulate the objective W (·  µn).
Proposition 2 ([17]). We can rewrite the objective

n(cid:1)

√

√

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)φi − 1|Cj| φCj

with

C∈H W (C  µn) =
min
= ki i − 2|Cj|

(cid:13)(cid:13)(cid:13)2

1|Cj|

minV

1
n

(cid:80)

k(cid:88)

(cid:88)

j=1

i∈Cj

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

φs

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)φi − 1
(cid:80)

|Cj|

s∈Cj

(cid:88)
(cid:80)

s∈Cj

s∈Cj

ki s + 1|Cj|2

s(cid:48)∈Cj

ks s(cid:48)

2

While the combinatorial search over V can now be explicitly computed and optimized using the
kernel matrix Kn  it still remains highly inefﬁcient to do so. In particular  simply constructing and
storing Kn takes O(n2) time and space and does not scale to large datasets.

3 Algorithm

A simple approach to reduce computational cost is to use approximate embeddings  which replace the

map ϕ(·) and points φi = ϕ(xi) ∈ H with a ﬁnite-dimensional approximation(cid:101)φi = (cid:101)ϕ(xi) ∈ Rm.

Nyström kernel k-means Given a dataset D  we denote with I = {φj}m
j=1 a dictionary (i.e. 
subset) of m points φj from D  and with Φm : Rm → H the map with these points as columns.
These points acts as landmarks [36]  inducing a smaller space Hm = Im(Φm) spanned by the
dictionary. As we will see in the next section  I should be chosen so that Hm is close to the whole
span Hn = Im(Φn) of the dataset.
Let Km m ∈ Rm×m be the empirical kernel matrix between all points in I  and denote with

the orthogonal projection on Hm. Then we can deﬁne an approximate ERM over Hm as

Πm = ΦmΦT

m(ΦmΦT

m)+ = ΦmK+

m mΦT

m 

(1)

n(cid:88)

n(cid:88)

i=1

1
n

min
j=[k]

m

i=1

min
j=[k]

C∈Hk

m

(cid:107)Πm(φi − cj)(cid:107)2 

(cid:107)φi − cj(cid:107)2 = arg min
C∈Hk

1
n

Cn m = arg min

(cid:107)Πm(φi − cj)(cid:107)2 = (cid:107)Λ−1/2UTΦT

(2)
since any component outside of Hm is just a constant in the minimization. Note that the centroids
Cn m are still points in Hm ⊂ H  and we cannot directly compute them. Instead  we can use the
eigen-decomposition of Km m = UΛUT to rewrite Πm = ΦmUΛ−1/2Λ−1/2UTΦT
m. Deﬁning

now (cid:101)ϕ(·) = Λ−1/2UTΦT
where (cid:101)φi := Λ−1/2UTΦT
searching over (cid:101)C ∈ Rm×k instead of searching over C ∈ Hk
k(cid:88)
(cid:101)Cn m = arg min
(cid:101)C∈Rm×k

mϕ(·) we have a ﬁnite-rank embedding into Rm. Substituting in Eq. (2)
m(φi − cj)(cid:107)2 = (cid:107)(cid:101)φi − Λ−1/2UTΦT
mcj(cid:107)2 
mφi are the embedded points. Replacing(cid:101)cj := Λ−1/2UTΦT
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:101)φi − 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
n(cid:88)
(cid:107)(cid:101)φi −(cid:101)cj(cid:107)2 =
(cid:101)φs
where we do not need to resort to kernel tricks  but can use the m-dimensional embeddings(cid:101)φi to
explicitly compute the centroid(cid:80)
s∈Cj (cid:101)φs. Eq. (3) can now be solved in multiple ways. The most
straightforward is to run a parametric k-means algorithm to compute (cid:101)Cn m  and then invert the rela-
mUTΛ1/2(cid:101)Cn m =
tionship(cid:101)cj = ΦmUΛ−1/2cj to bring back the solution to Hm  i.e.  Cn m = Φ+
ΦmUΛ−1/2(cid:101)Cn m. This can be done in in O(nm) space and O(nmkt + nm2) time using t steps of
Lloyd’s algorithm [22] for k clusters. More in detail  computing the embeddings(cid:101)φi is a one-off cost
taking nm2 time. Once the m-rank Nyström embeddings(cid:101)φi are computed they can be stored and

mcj and
m  we obtain (similarly to Proposition 2)

manipulated in nm time and space  with an n/m improvement over the n2 time and space required
to construct Kn.

(cid:88)

(cid:88)

j=1

i∈Cj

1
n

minV

 

(3)

|Cj|

s∈Cj

min
j=[k]

i=1

1
n

3.1 Uniform sampling for dictionary construction
Due to its derivation  the computational cost of Algorithm 1 depends on the size m of the dictionary I.
Therefore  for computational reasons we would prefer to select as small a dictionary as possible. As a
approximate W (·  µn) well. Let Π

conﬂicting goal  we also wish to optimize W (·  µn) well  which requires a (cid:101)ϕ(·) and I rich enough to
m be the projection orthogonal to Hm. Then when ci ∈ Hm
⊥

(cid:107)φi − ci(cid:107)2 = (cid:107)(Πm + Π

m)(φi − ci)(cid:107)2 = (cid:107)Πm(φi − ci)(cid:107)2 + (cid:107)Π
⊥

mφi(cid:107)2.
⊥

We will now introduce the concept of a γ-preserving dictionary I to control the quantity (cid:107)Π

mφi(cid:107)2.
⊥

3

Algorithm 1 Nyström Kernel K-Means
Input: dataset D = {φi}n

i=1  dictionary I = {φj}m

j=1 with points from D  number of clusters k

mΦm between all points in I

compute kernel matrix Km m = ΦT
compute eigenvectors U and eigenvalues Λ of Km m

for each point φi  compute embedding(cid:101)φi = Λ−1/2UTΦT
compute optimal centroids (cid:101)Cn m ∈ Rm×k on the embedded dataset (cid:101)D = {(cid:101)φi}n
compute explicit representation of centroids Cn m = ΦmUΛ−1/2(cid:101)Cn m

mφi = Λ−1/2UTKm i ∈ Rm

i=1

Deﬁnition 2. We deﬁne the subspace Hm and dictionary I as γ-preserving w.r.t. space Hn if

−1 .

(ΦnΦT

i (φiφT

m = Πn − Πm (cid:22) γ
⊥
1 − ε
Π

i (ΦnΦT

i (ΦnΦT

i (ΦnΦT

n + γΠn)

n + γΠn)

n + γΠn)

n + γΠn)

mφi(cid:107)2 (cid:46) γφT
⊥

(4)
−1 on the right-hand side of the inequality is crucial to control
Notice that the inverse (ΦnΦT
−1 φi. In particular  since φi ∈ Φn  we have that in the
the error (cid:107)Π
i )+ φi ≤ 1. Conversely 
worst case the error is bounded as φT
n + γΠn)
n) ≤ κ2n we know that in the best case the error can be reduced up to 1/n ≤
since λmax(ΦnΦT
−1 φi. Note that the directions associated with the larger
n) ≤ φT
i φi/λmax(ΦnΦT
φT
eigenvalues are the ones that occur most frequently in the data. As a consequence  Deﬁnition 2
guarantees that the overall error across the whole dataset remains small. In particular  we can control
the residual Π
To construct γ-preserving dictionaries we focus on a uniform random sampling approach[7]. Uniform
sampling is historically the ﬁrst [36]  and usually the simplest approach used to construct I. Leverag-

ing results from the literature [7  14  25] we can show that uniformly sampling (cid:101)O(n/γ) landmarks

mΦn after the projection as (cid:107)Π
⊥

n + γΠn)−1Φn(cid:107) ≤ γ.

mΦn(cid:107)2 ≤ γ(cid:107)ΦT
⊥

−1 φi ≤ φT

generates a γ-preserving dictionary with high probability.
Lemma 1. For a given γ  construct I by uniformly sampling m ≥ 12κ2n/γ log(n/δ)/ε2 landmarks
from D. Then w.p. at least 1 − δ the dictionary I is γ-preserving.
Musco and Musco [25] obtains a similar result  but instead of considering the operator Πn they focus
on the ﬁnite-dimensional eigenvectors of Kn. Moreover  their Πn (cid:22) Πm + εγ
n)+ bound
is weaker and would not be sufﬁcient to satisfy our deﬁnition of γ-accuracy. A result equivalent to
Lemma 1 was obtained by Alaoui and Mahoney [3]  but they also only focus on the ﬁnite-dimensional
eigenvectors of Kn  and did not investigate the implications for H.
Proof sketch of Lemma 1. It is well known [7  14] that uniformly sampling O(n/γε−2 log(n/δ))
points with replacement is sufﬁcient to obtain w.p. 1 − δ the following guarantees on Φm

1−ε (ΦnΦT

n(ΦnΦT

ΦmΦT

m (cid:22) (1 + ε)ΦnΦT

n + εγΠn.

−1 =

1
1 − ε

(ΦnΦT

n + γΠn)

−1

Which implies

(cid:16) n

m

ΦmΦT

m + γΠn

We can now rewrite Πn as

(1 − ε)ΦnΦT

n − εγΠn (cid:22) n
m

(cid:17)−1 (cid:22) ((1 − ε)ΦnΦT
(cid:17)(cid:16) n
(cid:17)+

m + γΠn

ΦmΦT

m

m

(cid:16) n
(cid:16) n
(cid:16) n

m

ΦmΦT
m

ΦmΦT
m

ΦmΦT
m

(cid:16) n

m
n
=
m
(cid:22) n
m

= Πm + γ

ΦmΦT

m + γΠn

m

ΦmΦT

m + γΠn

n − εγΠn + γΠn)
(cid:17)−1
(cid:16) n
(cid:17)−1
(cid:16) n
(cid:17)−1 (cid:22) Πm +

ΦmΦT

+ γ

+ γ

m

m

Πn =

ΦmΦT

(cid:17)−1

m + γΠn

ΦmΦT

m + γΠn

(cid:17)−1

m + γΠn

γ
1 − ε

(ΦnΦT

n + γΠn)

−1

In other words  using uniform sampling we can reduce the size of the search space Hm by a 1/γ factor
(from n to m (cid:39) n/γ) in exchange for a γ additive error  resulting in a computation/approximation
trade-off that is linear in γ.

4

4 Theoretical analysis

Exploiting the error bound for γ-preserving dictionaries we are now ready for the main result of this
paper: showing that we can improve the computational aspect of kernel k-means using Nyström
embedding  while maintaining optimal generalization guarantees.
Theorem 1. Given a γ-preserving dictionary

E(Cn m) = W (Cn m  µ) − W (Cn  µ) ≤ O

k

(cid:18)

(cid:18) 1√

+

γ
n

n

(cid:19)(cid:19)

√

√

√

√

√

√

√

√

n).

from n to

n) + O(k

n/n) ≤ O(k/

n the solution Cn m achieves the O (k/

n  and the total required space from n2 to (cid:101)O(n

From a statistical point of view  Theorem 1 shows that if I is γ-preserving  the ERM in Hm achieves
the same excess risk as the exact ERM from Hn up to an additional γ/n error. Therefore  choosing
n) generalization [10].
γ =
From a computational point of view  Lemma 1 shows that we can construct an
n-preserving
n) points uniformly1  which greatly reduces the embedding size

dictionary simply by sampling (cid:101)O(
Time-wise  the bottleneck becomes the construction of the embeddings(cid:101)φi  which takes nm2 ≤ (cid:101)O(n2)
time  while each iterations of Lloyd’s algorithm only requires nm ≤ (cid:101)O(n

√
n) time. In the full
generality of our setting this is practically optimal  since computing a
n-preserving dictionary
is in general as hard as matrix multiplication [26  9]  which requires Ω(n2) time. In other words 
unlike the case of space complexity  there is no free lunch for time complexity  that in the worst case
must scale as n2 similarly to the exact case. Nonetheless embedding the points is an embarrassingly
parallel problem that can be easily distributed  while in practice it is usually the execution of the
Lloyd’s algorithm that dominates the runtime.
Finally  when the dataset satisﬁes certain regularity conditions  the size of I can be improved  which
reduces both embedding and clustering runtime. Denote with dn

so-called effective dimension [3] of Kn. Since Tr(cid:0)KT
eff(γ) (cid:28) √
(cid:101)O(ndn
would require only (cid:101)O(ndn

n(Kn)+)  we have
eff(γ) can be seen as a soft version of the rank. When
eff(γ) landmarks in
eff(γ)2) time using specialized algorithms [14] (see Section 6). In this case  the embedding step

eff(γ)2) (cid:28) (cid:101)O(n2)  improving both time and space complexity.

eff(γ) ≤ r := Rank(Kn)  and therefore dn

eff(γ) = Tr(cid:0)KT
n(Kn + In)−1(cid:1) ≤ Tr (KT

n(Kn + In)−1(cid:1) the

n it is possible to construct a γ-preserving dictionary with only dn

that dn
dn

√

√
Morever  to the best of our knowledge  this is the ﬁrst example of an unsupervised non-parametric
problem where it is always (i.e.  without assumptions on µ) possible to preserve the optimal O(1/
n)
risk rate while reducing the search from the whole space H to a smaller Hm subspace.
Proof sketch of Theorem 1. We can separate the distance between W (Cn m  µ) − W (Cn  µ) in a
component that depends on how close µ is to µn  bounded using Proposition 1  and a component
W (Cn m  µn) − W (Cn  µn) that depends on the distance between Hn and Hm
Lemma 2. Given a γ-preserving dictionary

W (Cn m  µn) − W (Cn  µn) ≤ min(k  dn
1 − ε

eff(γ))

γ
n

To show this we can rewrite the objective as (see [17])
W (Cn m  µn) = (cid:107)Φn − ΠmΦnSn m(cid:107)2

F = Tr(ΦT

nΦn − SnΦT

nΠmΦnSn) 

where Sn ∈ Rn×n is a k-rank projection matrix associated with the exact clustering Cn. Then using
Deﬁnition 2 we have Πm − Πn (cid:23) − γ
n + γΠn)−1 and we obtain an additive error bound

1−ε (ΦnΦT

(cid:18)
nΦn − SnΦT

Tr(ΦT
≤ Tr

nΠmΦnSn)

Tr(cid:0)SnΦT
1(cid:101)O hides logarithmic dependencies on n and m.

nΦn − SnΦT
ΦT
γ
1 − ε

= W (Cn  µn) +

nΦnSn +

(cid:19)

γ
1 − ε
n(ΦnΦT

n(ΦnΦT

SnΦT
n + γΠn)−1ΦnSn

(cid:1) .

n + γΠn)−1ΦnSn

5

Since (cid:107)ΦT

n + γΠn)−1Φn(cid:107) ≤ 1  Sn is a projection matrix  and Tr(Sn) = k we have

n(ΦnΦT
γ

1−ε Tr(cid:0)SnΦT

1−ε Tr(cid:0)SnΦT

γ

n(ΦnΦT

n + γΠn)−1ΦnSn

1−ε Tr (SnSn) = γk
1−ε .

(cid:1) ≤ γ
1−ε Tr(cid:0)ΦT
(cid:1) ≤ γ

n + γΠn)−1Φn (cid:22) Πn we have
n + Πn)−1Φn

n(ΦnΦT

(cid:1) ≤ γdn

Conversely  if we focus on the matrix ΦT

n(ΦnΦT

eff(γ)
1−ε .
Since both bounds hold simultaneously  we can simply take the minimum to conclude our proof.

n(ΦnΦT

n + Πn)−1ΦnSn

We now compare the theorem with previous work. Many approximate kernel k-means methods have
been proposed over the years  and can be roughly split in two groups.
Low-rank decomposition based methods try to directly simplify the optimization problem from

Proposition 2  replacing the kernel matrix Kn with an approximate (cid:101)Kn that can be stored and

manipulated more efﬁciently. Among these methods we can mention partial decompositions [8] 
Nyström approximations based on uniform [36]  k-means++ [27]  or ridge leverage score (RLS)
sampling[35  25  14]  and random-feature approximations [6]. None of these optimization based
methods focus on the underlying excess risk problem  and their analysis cannot be easily integrated in
existing results  as the approximate minimum found has no clear interpretation as a statistical ERM.
Other works take the same embedding approach that we do  and directly replace the exact ϕ(·)

with an approximate (cid:101)ϕ(·)  such as Nyström embeddings [36]  Gaussian projections [10]  and again
random-feature approximations [29]. Note that these approaches also result in approximate (cid:101)Kn that

ϕ(x) =(cid:80)D
the following m-dimensional approximate feature map (cid:101)ϕ(x) = P[ψ1(x)  . . .   ψD(x)] ∈ Rm. Using

can be manipulated efﬁciently  but are simpler to analyze theoretically. Unfortunately  no existing
embedding based methods can guarantee at the same time optimal excess risk rates and a reduction in
the size of Hm  and therefore a reduction in computational cost.
To the best of our knowledge  the only other result providing excess risk guarantee for approximate
kernel k-means is Biau et al. [10]  where the authors consider the excess risk of the ERM when the
approximate Hm is obtained using Gaussian projections. Biau et al. [10] notes that the feature map
s=1 ψs(x) can be expressed using an expansion of basis functions ψs(x)  with D very large
or inﬁnite. Given a matrix P ∈ Rm×D where each entry is a standard Gaussian r.v.  [10] proposes
Johnson-Lindenstrauss (JL) lemma [19]  they show that if m ≥ log(n)/ν2 then a multiplicative error
bound of the form W (Cn m  µn) ≤ (1 + ν)W (Cn  µn) holds. Reformulating their bound  we obtain
that W (Cn m  µn) − W (Cn  µn) ≤ νW (Cn  µn) ≤ νκ2 and E(Cn m) ≤ O(k/
Note that to obtain a bound comparable to Theorem 1  and if we treat k as a constant  we need to take

ν = γ/n which results in m ≥ (n/γ)2. This is always worse than our (cid:101)O(n/γ) result for uniform

√
√
n risk rate setting Gaussian projections would require
Nyström embedding. In particular  in the 1/
n resulting in m ≥ n log(n) random features  which would not bring any improvement
ν = 1/
over computing Kn. Moreover when D is inﬁnite  as it is usually the case in the non-parametric
setting  the JL projection is not explicitly computable in general and Biau et al. [10] must assume the

existence of a computational oracle capable of constructing (cid:101)ϕ(·). Finally note that  under the hood 
traditional embedding methods such as those based on JL lemma  usually provide only bounds of
the form Πn − Πm (cid:22) γΠn  and an error (cid:107)Π
mφi(cid:107)2 ≤ γ (cid:107)φi(cid:107)2 (see the discussion of Deﬁnition 2).
⊥
Therefore the error can be larger along multiple directions  and the overall error (cid:107)Π
mΦn(cid:107)2 across
⊥
the dictionary can be as large as nγ rather than γ.
Recent work in RLS sampling has also focused on bounding the distance W (Cn m  µn)−W (Cn  µn)
between empirical errors. Wang et al. [35] and Musco and Musco [25] provide multiplicative error
bounds of the form W (Cn m  µn) ≤ (1+ν)W (Cn  µn) for uniform and RLS sampling. Nonetheless 
they only focus on empirical risk and do not investigate the interaction between approximation and
√
generalization  i.e.  statistics and computations. Moreover  as we already remarked for [10]  to achieve
the 1/
n excess risk rate using a multiplicative error bound we would require an unreasonably small
ν  resulting in a large m that brings no computational improvement over the exact solution.
Finally  note that when [31] showed that a favourable trade-off was possible for kernel ridge re-
gression (KRR)  they strongly leveraged the fact that KRR is a γ-regularized problem. Therefore 
all eigenvalues and eigenvectors in the ΦnΦT
n covariance matrix smaller than the γ regularization
do not inﬂuence signiﬁcantly the solution. Here we show the same for kernel k-means  a problem

n + ν).

√

6

without regularization. This hints at a deeper geometric motivation which might be at the root of both
problems  and potentially similar approaches could be leveraged in other domains.

4.1 Further results: beyond ERM
So far we provided guarantees for Cn m  that this the ERM in Hm. Although Hm is much smaller
than Hn  solving the optimization problem to ﬁnd the ERM is still NP-Hard in general [4]. Nonethe-
less  Lloyd’s algorithm [22]  when coupled with a careful k-means++ seeding  can return a good
approximate solution C++
n m.
Proposition 3 ([5]). For any dataset EA[W (C++
is the randomness deriving from the k-means++ initialization.

n m  µn)] ≤ 8(log(k) + 2)W (Cn m  µn)  where A

Note that  similarly to [35  25]  this is a multiplicative error bound on the empirical risk  and as we
discussed we cannot leverage Lemma 2 to bound the excess risk E(C++
n m). Nonetheless we can
still leverage Lemma 2 to bound only the expected risk W (C++
n m  µ)  albeit with an extra error term
appearing that scales with the optimal clustering risk W ∗(µ) (see Proposition 1).
Theorem 2. Given a γ-preserving dictionary

(cid:104)E

E
D∼µ

A[W (C++

n m  µ)]

(cid:18)

(cid:105) ≤ O

log(k)

(cid:18) k√

(cid:19)(cid:19)

γ
n

n

+ k
√

+ W ∗(µ)

.

√
n to obtain a O(k/

√

√

√

√

In particular  each iteration of Lloyd’s algorithm will take only (cid:101)O(n
ﬁnd a solution in (cid:101)O(n

n) rate for the
From a statistical perspective  we can once again  set γ =
ﬁrst part of the bound. Conversely  the optimal clustering risk W ∗(µ) is a µ-dependent quantity that
cannot in general be bounded in n  and captures how well our model  i.e.  the choice of H and how
well the criterion W (·  µ)  matches reality.
From a computational perspective  we can now bound the computational cost of ﬁnding C++
n m.
nk) time. Moreover  when
k-means++ initialization is used  the expected number of iterations required for Lloyd’s algorithm to
converge is only logarithmic [1]. Therefore  ignoring the time required to embed the points  we can
with a strong O(
Finally  if the data distribution satisﬁes some regularity assumption the following result follows [15].
Corollary 1. If we denote by Xµ the support of the distribution µ and assume ϕ(Xµ) to be a d-
dimensional manifold  then W ∗(µ) ≤ dk−2/d  and given a
n-preserving dictionary the expected
cost satisﬁes ED∼µ[EA[W (C++
5 Experiments

nk) time and space instead of the (cid:101)O(n2k) cost required by the exact method 

(cid:16) k√
n + dk−2/d(cid:17)(cid:17)

n m  µ)]] ≤ O(cid:16)

n) improvement.

log(k)

√

We now evaluate experimentally the claims of Theorem 1  namely that sampling (cid:101)O(n/γ) increases the

excess risk by an extra γ/n factor  and that m =
n is sufﬁcient to recover the optimal rate. We use
the Nystroem and MiniBatchKmeans classes from the sklearn python library [28]to implement
kernel k-means with Nyström embedding (Algorithm 1) and we compute the solution C++
n m.
For our experiments we follow the same approach as Wang et al. [35]  and test our algorithm on two
variants of the MNIST digit dataset. In particular  MNIST60K [20] is the original MNIST dataset
containing pictures each with d = 784 pixels. We divide each pixel by 255  bringing each feature
in a [0  1] interval. We split the dataset in two part  n = 60000 samples are used to compute the
n m  µtest)  as a
W (C++
proxy for W (C++
n m  µ). To test the scalability of our approach we also consider the MNIST8M dataset
from the inﬁnite MNIST project [23]  constructed using non-trivial transformations and corruptions
of the original MNIST60K images. Here we compute C++
n m using n = 8000000 images  and compute
n m  µtest) on 100000 unseen images. As in Wang et al. [35] we use Gaussian kernel with
W (C++
bandwidth σ = (1/n2)

n m) centroids  and we leave out unseen 10000 samples to compute W (C++

(cid:113)(cid:80)

i j (cid:107)xi − xj(cid:107)2.

.

MNIST60K: these experiments are small enough to run in less than a minute on a single laptop
with 4 cores and 8GB of RAM. The results are reported in Fig. 1. On the left we report in blue

7

Figure 1: Results for MNIST60K

Figure 2: Results for MNIST8M

n m  µtest)  where the shaded region is a 95% conﬁdence interval for the mean over 10 runs.
W (C++
As predicted  the expected cost decreases as the size of Hm increases  and plateaus once we achieve
√
1/m (cid:39) 1/
n  in line with the statistical error. Note that the normalized mutual information (NMI)
between the true [0 − 9] digit classes y and the computed cluster assignments yn m also plateaus
√
n. While this is not predicted by the theory  it strengthens the intuition that beyond a
around 1/
certain capacity expanding Hm is computationally wasteful.
MNIST8M: to test the scalability of our approach  we run the same experiment on millions of points.
Note that we carry out our MNIST8M experiment on a single 36 core machine with 128GB of RAM 
much less than the setup of [35]  where at minimum a cluster of 8 such nodes are used. The behaviour
of W (C++
n m  µtest) and NMI are similar to MNIST60K  with the increase in dataset size allowing
for stronger concentration and smaller conﬁdence intervals. Finalle  note that around m = 400
uniformly sampled landmarks are sufﬁcient to achieve N M I(yn m  y) = 0.405  matching the 0.406
NMI reported by [35] for a larger m = 1600  although smaller than the 0.423 NMI they report for
m = 1600 when using a slower  PCA based method to compute the embeddings  and RLS sampling
to select the landmarks. Nonetheless  computing C++
n m takes less than 6 minutes on a single machine 
while their best solution required more than 1.5hr on a cluster of 32 machines.

6 Open questions and conclusions

Combining Lemma 1 and Lemma 2  we know that using uniform sampling we can linearly trade-off
√
a 1/γ decrease in sub-space size m with a γ/n increase in excess risk. While this is sufﬁcient to
maintain the O(1/
n) rate  it is easy to see that the same would not hold for a O(1/n) rate  since
we would need to uniformly sample n/1 landmarks losing all computational improvements.
To achieve a better trade-off we must go beyond uniform sampling and use different probabilities for
each sample  to capture their uniqueness and contribution to the approximation error.

8

eff(γ) =(cid:80)n

Deﬁnition 3 ([3]). The γ-ridge leverage score (RLS) of point i ∈ [n] is deﬁned as
i Kn(Kn + γIn)−1ei.

n + γΠn)−1φi = eT

τi(γ) = φT

i (ΦnΦT

(5)

1−ε φT

i (ΦnΦT

mφi(cid:107)2 ≤ γ
⊥

i=1 τi(γ) is the empirical effective dimension of the dataset.

n + γΠn)−1φi. It is easy to see that  up to a factor γ

The sum of the RLSs dn
Ridge leverage scores are closely connected to the residual (cid:107)Π
mφi(cid:107)2 after the projection Πm
⊥
discussed in Deﬁnition 2. In particular  using Lemma 2 we have that the residual can be bounded as
(cid:107)Π
1−ε  high-RLS points
are also high-residual points. Therefore it is not surprising that sampling according to RLSs quickly
selects any high-residual points and covers Hn  generating a γ-preserving dictionary.
Lemma 3. [11] For a given γ  construct I by sampling m ≥ 12κ2dn
from D proportionally to their RLS. Then w.p. at least 1 − δ the dictionary I is γ-preserving.
Note there exist datasets where the RLSs are uniform and therefore in the worst case the two sampling
approaches coincide. Nonetheless  when the data is more structured m (cid:39) dn
eff(γ) can be much smaller
than the n/γ dictionary size required by uniform sampling.
Finally  note that computing RLSs exactly also requires constructing Kn and O(n2) time and space 
but in recent years a number of fast approximate RLSs sampling methods [14] have emerged that can
eff(γ)2) time. Using this result  it

construct γ-preserving dictionaries of size (cid:101)O(dn

eff(γ)) in just (cid:101)O(ndn

eff(γ) log(n/δ)/ε2 landmarks

is trivial to sharpen the computational aspects of Theorem 1 in special cases.
√
n-preserving dictionary with only dn
In particular  we can generate a
n) elements instead of
eff(
the
n required by uniform sampling. Using concentration arguments [31] we also know that w.h.p.
eff(γ) ≤ 3dµ
the empirical effective dimension is at most three times dn
eff(γ) the expected effective
dimension  a µ-dependent quantity that captures the interaction between µ and the RKHS H.
Deﬁnition 4. Given the expected covariance operator Ψ := Ex∼µ [φ(x)φ(x)T]  the expected
effective dimension is deﬁned as dµ
. Moreover  for some
constant c that depends only on ϕ(·) and µ  dµ

(cid:105)
eff(γ) ≤ c (n/γ)η with 0 < η ≤ 1.

eff(γ) = Ex∼µ

φ(x) (Ψ + γΠ)

−1 φ(x)

(cid:104)

√

√

√

√

√

n) ≤ cnη/2.

n) due to the O(k/

eff(γ) ≤ O(n/γ) worst-case upper bound that we saw for dn

eff(γ) ≤ c (n/γ)η  and in our case γ =

√
n we have dµ
eff(
√

Note that η = 1 just gives us the dµ
eff(γ) 
and it is always satisﬁed when the kernel function is bounded. If instead we have a faster spectral
decay  η can be much smaller. For example  if the eigenvalues of Ψ decay polynomially as λi = i−η 
then dµ
We can now better characterize the gap between statistics and computation: using RLSs sampling we
can improve the computational aspect of Theorem 1 from
eff(γ)  but the risk rate remains
O(k/
Assume for a second we could generalize  with additional assumptions  Proposition 1 to a faster
O(1/n) rate. Then applying Lemma 2 with γ = 1 we would obtain a risk E(Cn m) ≤ O(k/n) +
O(k/n). Here we see how the regularity condition on dµ
eff(1) becomes crucial. In particular  if η = 1 
eff(1) ≤ nη. This kind of adaptive
then we have dµ
rates were shown to be possible in supervised learning [31]  but seems to still be out of reach for
approximate kernel k-means.
One possible approach to ﬁll this gap is to look at fast O(1/n) excess risk rates for kernel k-means.
Proposition 4 ([21]  informal). Assume that k ≥ 2  and that µ satisﬁes a margin condition with
radius r0. If Cn is an empirical risk minimizer  then  with probability larger than 1 − e−δ 

eff(1) ∼ n and no gain. If instead η < 1 we obtain dµ

n) component coming from Proposition 1.

n to dµ

E(Cn) ≤ (cid:101)O

(cid:18) 1

r0

(k + log(|M|)) log(1/δ)

n

(cid:19)

 

where |M| is the cardinality of the set of all optimal (up to a relabeling) clustering.
For more details on the margin assumption  we refer the reader to the original paper [21]. Intuitively
the margin condition asks that every labeling (Voronoi grouping) associated with an optimal clustering
is reﬂected by large separation in µ. This margin condition also acts as a counterpart of the usual
margin conditions for supervised learning where µ must have lower density around the neighborhood
of the critical area {x|µ(cid:48)(Y = 1|X = x) = 1/2}. Unfortunately  it is not easy to integrate
Proposition 4 in our analysis  as it is not clear how the margin condition translate from Hn to Hm.

9

Acknowledgments.
This material is based upon work supported by the Center for Brains  Minds and Machines (CBMM)  funded by
NSF STC award CCF-1231216  and the Italian Institute of Technology. We gratefully acknowledge the support
of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research.
L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007
(European Ofﬁce of Aerospace Research and Development)  and the EU H2020-MSCA-RISE project NoMADS
- DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).

References
[1] Nir Ailon  Ragesh Jaiswal  and Claire Monteleoni. Streaming k-means approximation. In

Advances in neural information processing systems  pages 10–18  2009.

[2] M. A. Aizerman  E. A. Braverman  and L. Rozonoer. Theoretical foundations of the potential
function method in pattern recognition learning. In Automation and Remote Control   number 25
in Automation and Remote Control„ pages 821–837  1964.

[3] Ahmed El Alaoui and Michael W. Mahoney. Fast randomized kernel methods with statistical

guarantees. In Neural Information Processing Systems  2015.

[4] Daniel Aloise  Amit Deshpande  Pierre Hansen  and Preyas Popat. Np-hardness of euclidean

sum-of-squares clustering. Machine learning  75(2):245–248  2009.

[5] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  pages
1027–1035. Society for Industrial and Applied Mathematics  2007.

[6] Haim Avron  Michael Kapralov  Cameron Musco  Christopher Musco  Ameya Velingker  and
Amir Zandieh. Random Fourier features for kernel ridge regression: Approximation bounds
and statistical guarantees. In Proceedings of the 34th International Conference on Machine
Learning  2017.

[7] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on

Learning Theory  2013.

[8] Francis R Bach and Michael I Jordan. Predictive low-rank decomposition for kernel methods.
In Proceedings of the 22nd international conference on Machine learning  pages 33–40. ACM 
2005.

[9] Arturs Backurs  Piotr Indyk  and Ludwig Schmidt. On the ﬁne-grained complexity of empirical
risk minimization: Kernel methods and neural networks. In Advances in Neural Information
Processing Systems  2017.

[10] Gérard Biau  Luc Devroye  and Gábor Lugosi. On the performance of clustering in hilbert

spaces. IEEE Transactions on Information Theory  54(2):781–790  2008.

[11] Daniele Calandriello. Efﬁcient Sequential Learning in Structured and Constrained Environments.

PhD thesis  2017.

[12] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Efﬁcient second-order online
In Advances in Neural Information Processing

kernel learning with adaptive embedding.
Systems  pages 6140–6150  2017.

[13] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Second-order kernel online convex
optimization with adaptive sketching. In International Conference on Machine Learning  2017.

[14] Daniele Calandriello  Alessandro Lazaric  and Michal Valko. Distributed adaptive sampling for

kernel matrix approximation. In AISTATS  2017.

[15] Guillermo Canas  Tomaso Poggio  and Lorenzo Rosasco. Learning manifolds with k-means
and k-ﬂats. In Advances in Neural Information Processing Systems  pages 2465–2473  2012.

10

[16] Radha Chitta  Rong Jin  Timothy C. Havens  and Anil K. Jain. Approximate kernel k-means:
Solution to large scale kernel clustering. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining  pages 895–903. ACM  2011.

[17] Inderjit S. Dhillon  Yuqiang Guan  and Brian Kulis. A uniﬁed view of kernel k-means  spectral
clustering and graph cuts. Technical Report No. UTCS TR-04-25  University of Texas at Austin 
2004.

[18] Siegfried Graf and Harald Luschgy. Foundations of Quantization for Probability Distributions.
Lecture Notes in Mathematics. Springer-Verlag  Berlin Heidelberg  2000. ISBN 978-3-540-
67394-1.

[19] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert

space. Contemporary Mathematics  26  1984.

[20] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http:

//yann.lecun.com/exdb/mnist/.

[21] Clément Levrard et al. Nonasymptotic bounds for vector quantization in hilbert spaces. The

Annals of Statistics  43(2):592–619  2015.

[22] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory  28

(2):129–137  1982.

[23] Gaëlle Loosli  Stéphane Canu  and Léon Bottou. Training invariant support vector machines us-
ing selective sampling. In Large Scale Kernel Machines  pages 301–320. MIT Press  Cambridge 
MA.  2007.

[24] Andreas Maurer and Massimiliano Pontil. $ K $-dimensional coding schemes in Hilbert spaces.

IEEE Transactions on Information Theory  56(11):5839–5846  2010.

[25] Cameron Musco and Christopher Musco. Recursive Sampling for the Nyström Method. In

NIPS  2017.

[26] Cameron Musco and David Woodruff. Is input sparsity time possible for kernel low-rank

approximation? In Advances in Neural Information Processing Systems 30. 2017.

[27] Dino Oglic and Thomas Gärtner. Nyström method with kernel k-means++ samples as landmarks.

Journal of Machine Learning Research  2017.

[28] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research  12:2825–2830  2011.

[29] Ali Rahimi and Ben Recht. Random features for large-scale kernel machines.

Information Processing Systems  2007.

In Neural

[30] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random

features. In Advances in Neural Information Processing Systems  pages 3218–3228  2017.

[31] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström computa-
tional regularization. In Advances in Neural Information Processing Systems  pages 1657–1665 
2015.

[32] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2001.

[33] Bernhard Schölkopf  Alexander Smola  and Klaus-Robert Müller. Nonlinear component analysis

as a kernel eigenvalue problem. Neural computation  10(5):1299–1319  1998.

[34] Joel A Tropp  Alp Yurtsever  Madeleine Udell  and Volkan Cevher. Fixed-rank approximation
of a positive-semideﬁnite matrix from streaming data. In Advances in Neural Information
Processing Systems  pages 1225–1234  2017.

11

,Daniele Calandriello
Lorenzo Rosasco