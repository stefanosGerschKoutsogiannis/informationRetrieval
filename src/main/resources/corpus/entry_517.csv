2008,An Homotopy Algorithm for the Lasso with Online Observations,It has been shown that the problem of $\ell_1$-penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper an algorithm to solve the Lasso with online observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and present an application to compressed sensing with sequential observations. Our approach can also be easily extended to compute an homotopy from the current solution to the solution after removing a data point  which leads to an efficient algorithm for leave-one-out cross-validation.,An Homotopy Algorithm for the Lasso with Online

Observations

Redwood Center for Theoretical Neuroscience

Pierre J. Garrigues
Department of EECS

University of California

Berkeley  CA 94720

garrigue@eecs.berkeley.edu

Laurent El Ghaoui
Department of EECS
University of California

Berkeley  CA 94720

elghaoui@eecs.berkeley.edu

Abstract

It has been shown that the problem of ‘1-penalized least-square regression com-
monly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that
are sparse and therefore achieves model selection. We propose in this paper Re-
cLasso  an algorithm to solve the Lasso with online (sequential) observations. We
introduce an optimization problem that allows us to compute an homotopy from
the current solution to the solution after observing a new data point. We com-
pare our method to Lars and Coordinate Descent  and present an application to
compressive sensing with sequential observations. Our approach can easily be
extended to compute an homotopy from the current solution to the solution that
corresponds to removing a data point  which leads to an efﬁcient algorithm for
leave-one-out cross-validation. We also propose an algorithm to automatically
update the regularization parameter after observing a new data point.

1 Introduction

Regularization using the ‘1-norm has attracted a lot of interest in the statistics [1]  signal processing
[2]  and machine learning communities. The ‘1 penalty indeed leads to sparse solutions  which is
a desirable property to achieve model selection  data compression  or for obtaining interpretable
results. In this paper  we focus on the problem of ‘1-penalized least-square regression commonly
referred to as the Lasso [1]. We are given a set of training examples or observations (yi  xi) ∈
R × Rm  i = 1 . . . n. We wish to ﬁt a linear model to predict the response yi as a function of xi
and a feature vector θ ∈ Rm  yi = xT
i θ + νi  where νi represents the noise in the observation. The
Lasso optimization problem is given by

i θ − yi)2 + µnkθk1 
(xT

(1)

nX

i=1

min

θ

1
2

where µn is a regularization parameter. The solution of (1) is typically sparse  i.e. the solution θ has
few entries that are non-zero  and therefore identiﬁes which dimensions in xi are useful to predict
yi.
The ‘1-regularized least-square problem can be formulated as a convex quadratic problem (QP) with
linear equality constraints. The equivalent QP can be solved using standard interior-point methods
(IPM) [3] which can handle medium-sized problems. A specialized IPM for large-scale problems
was recently introduced in [4]. Homotopy methods have also been applied to the Lasso to compute
the full regularization path when λ varies [5] [6][7]. They are particularly efﬁcient when the solution
is very sparse [8]. Other methods to solve (1) include iterative thresholding algorithms [9][10][11] 
feature-sign search [12]  bound optimization methods [13] and gradient projection algorithms [14].

1

We propose an algorithm to compute the solution of the Lasso when the training examples
(yi  xi)i=1...N are obtained sequentially. Let θ(n) be the solution of the Lasso after observing n
training examples and θ(n+1) the solution after observing a new data point (yn+1  xn+1) ∈ R×Rm.
We introduce an optimization problem that allows us to compute an homotopy from θ(n) to θ(n+1).
Hence we use the previously computed solution as a “warm-start”  which makes our method partic-
ularly efﬁcient when the supports of θ(n) and θ(n+1) are close.
In Section 2 we review the optimality conditions of the Lasso  which we use in Section 3 to derive
our algorithm. We test in Section 4 our algorithm numerically  and show applications to compres-
sive sensing with sequential observations and leave-one-out cross-validation. We also propose an
algorithm to automatically select the regularization parameter each time we observe a new data
point.

2 Optimality conditions for the Lasso

The objective function in (1) is convex and non-smooth since the ‘1 norm is not differentiable when
θi = 0 for some i. Hence there is a global minimum at θ if and only if the subdifferential of the
objective function at θ contains the 0-vector. The subdifferential of the ‘1-norm at θ is the following
set

(cid:26)

∂kθk1 =

v ∈ Rm :

(cid:26)vi = sgn(θi) if |θi| > 0

vi ∈ [−1  1] if θi = 0

(cid:27)

.

Let X ∈ Rn×m be the matrix whose ith row is equal to xT
conditions for the Lasso are given by

i   and y = (y1  . . .   yn)T . The optimality

X T (Xθ − y) + µnv = 0  v ∈ ∂kθk1.

We deﬁne as the active set the indices of the elements of θ that are non-zero. To simplify notations we
assume that the active set appears ﬁrst  i.e. θT = (θT
2 )  where v1i = sgn(θ1i)
for all i  and −1 ≤ v2j ≤ 1 for all j. Let X = (X1 X2) be the partitioning of X according to the
active set. If the solution is unique it can be shown that X T
1 X1 is invertible  and we can rewrite the
optimality conditions as

1   0T ) and vT = (vT

1   vT

(cid:26)θ1 = (X T

1 X1)−1(X T

1 y − µnv1)

−µnv2 = X T

2 (X1θ1 − y)

.

Note that if we know the active set and the signs of the coefﬁcients of the solution  then we can
compute it in closed form.

3 Proposed homotopy algorithm

3.1 Outline of the algorithm

Suppose we have computed the solution θ(n) to the Lasso with n observation and that we are given
an additional observation (yn+1  xn+1) ∈ R × Rm. Our goal is to compute the solution θ(n+1) of
the augmented problem. We introduce the following optimization problem

θ(t  µ) = arg min

θ

1
2

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:18) X

txT

n+1

(cid:18) y

tyn+1

θ −

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

+ µkθk1.

(2)

We have θ(n) = θ(0  µn) and θ(n+1) = θ(1  µn+1). We propose an algorithm that computes a path
from θ(n) to θ(n+1) in two steps:

Step 1 Vary the regularization parameter from µn to µn+1 with t = 0. This amounts to comput-
ing the regularization path between µn and µn+1 as done in Lars. The solution path is
piecewise linear and we do not review it in this paper (see [15][7][5]).

Step 2 Vary the parameter t from 0 to 1 with µ = µn+1. We show in Section 3.2 how to compute

this path.

2

3.2 Algorithm derivation

We show in this Section that θ(t  µ) is a piecewise smooth function of t. To make notations lighter
we write θ(t) := θ(t  µ). We saw in Section 2 that the solution to the Lasso can be easily computed
once the active set and signs of the coefﬁcients are known. This information is available at t = 0 
and we show that the active set and signs will remain the same for t in an interval [0  t∗) where the
solution θ(t) is smooth. We denote such a point where the active set changes a “transition point”
and show how to compute it analytically. At t∗ we update the active set and signs which will remain
valid until t reaches the next transition point. This process is iterated until we know the active set
and signs of the solution at t = 1  and therefore can compute the desired solution θ(n+1).
We suppose as in Section 2 and without loss of generality that the solution at t = 0 is such that
θ(0) = (θT
Lemma 1. Suppose θ1i 6= 0 for all i and |v2j| < 1 for all j. There exist t∗ > 0 such that for all
t ∈ [0  t∗)  the solution of (2) has the same support and the same sign as θ(0).
PROOF. The optimality conditions of (2) are given by

2 ) ∈ ∂kθ(0)k1 satisfy the optimality conditions.

1   0T ) and vT = (vT

1   vT

X T (Xθ − y) + t2xn+1

n+1θ − yn+1

(3)
where w ∈ ∂kθk1. We show that there exists a solution θ(t)T = (θ1(t)T   0T ) and w(t)T =
1   w2(t)T ) ∈ ∂kθ(t)k1 satisfying the optimality conditions for t sufﬁciently small. We partition
(vT
n+1 = (xT
xT

n+1 2) according to the active set. We rewrite the optimality conditions as

n+1 1  xT

(cid:1) + µw = 0 

(cid:1) + µv1 = 0
(cid:1) + µw2(t) = 0

T θ1(t) − yn+1
T θ1(t) − yn+1

1 (X1θ1(t) − y) + t2xn+1 1
2 (X1θ1(t) − y) + t2xn+1 2
X T
Solving for θ1(t) using the ﬁrst equation gives

(cid:26)X T
θ1(t) =(cid:0)X T

(cid:0)xT
(cid:0)xn+1 1
(cid:0)xn+1 1
T(cid:1)−1(cid:0)X T

.

(cid:1) .
(cid:1) .

1 such that for t < t∗
2 (X1θ1(t) − y) + t2xn+1 2

1 X1 + t2xn+1 1xn+1 1

(4)
We can see that θ1(t) is a continuous function of t. Since θ1(0) = θ1 and the elements of θ1 are all
strictly positive  there exists t∗
1  all elements of θ1(t) remain positive and do not
change signs. We also have

1 y + t2yn+1xn+1 1 − µv1

−µn+1w2(t) = X T

2 all elements of w2(t) are strictly smaller than 1 in absolute value. By taking t∗ = min(t∗

T θ1(t) − yn+1
Similarly w2(t) is a continuous function of t  and since w2(0) = v2  there exists t∗
t < t∗
we obtain the desired result.
The solution θ(t) will therefore be smooth until t reaches a transition point where either a component
of θ1(t) becomes zero  or one of the component of w2(t) reaches one in absolute value. We now
show how to compute the value of the transition point.

(5)
2 such that for
1  t∗
2)

(cid:0)xn+1 1

(cid:18) X

(cid:19)

(cid:18) y

(cid:19)

(cid:16) ˜X1 ˜X2

(cid:17)

according to the active

Let ˜X =
set. We use the Sherman-Morrison formula and rewrite (4) as

. We partition ˜X =

and ˜y =

xn+1

yn+1

T

where ˜θ1 = ( ˜X T
u = ( ˜X T

1

1

1 + α(t2 − 1) u 
T ˜θ1 − yn+1  α = xn+1 1
˜X1)−1xn+1 1. Let t1i the value of t such that θ1i(t) = 0. We have

1 ˜y − µv1)  ¯e = xn+1 1

θ1(t) = ˜θ1 − (t2 − 1)¯e
(cid:19)−1! 1

˜X1)−1( ˜X T

 

2

t1i =

1 +

− α

 

(cid:18) ¯eui

˜θ1i

T ( ˜X T

1

˜X1)−1xn+1 1 and

We now examine the case where a component of w2(t) reaches one in absolute value. We ﬁrst notice
that

(xn+1 1

T θ1(t) − yn+1 =

˜X1θ1(t) − ˜y = ˜e − (t2−1)¯e
1+α(t2−1)

¯e

1+α(t2−1)
˜X1u

 

3

where ˜e = ˜X1

˜θ1 − ˜y. We can rewrite (5) as

−µw2(t) = ˜X T

2 ˜e +

¯e(t2 − 1)

1 + α(t2 − 1)

(xn+1 2 − ˜X T

2

˜X1u).

Let cj be the jth column of ˜X2  and x(j) the jth element of xn+1 2. The jth component of w2(t)
will become 1 in absolute value as soon as
¯e(t2 − 1)

j ˜e +

1 + α(t2 − 1)

x(j) − cT

j

˜X1u

Let t+

2 j (resp. t−

2 j) be the value such that w2j(t) = 1 (resp. w2j(t) = −1). We have

(cid:12)(cid:12)(cid:12)(cid:12)cT


 
 

2 j =
t+

1 +

t−
2 j =

1 +

(cid:16)
(cid:18) ¯e(x(j)−cT
(cid:18) ¯e(x(j)−cT

j

−µ−cT

j
µ−cT
j ˜e

˜X1u)

j ˜e − α
− α

˜X1u)

(cid:17)(cid:12)(cid:12)(cid:12)(cid:12) = µ.
(cid:19)−1! 1
(cid:19)−1! 1

.

2

2

Hence the transition point will be equal to t0 = min{mini t1i  minj t+
2 j} where we re-
strict ourselves to the real solutions that lie between 0 and 1. We now have the necessary ingredients
to derive the proposed algorithm.

2 j  minj t−

Algorithm 1 RecLasso: homotopy algorithm for online Lasso
1: Compute the path from θ(n) = θ(0  µn) to θ(0  µn+1).
2: Initialize the active set to the non-zero coefﬁcients of θ(0  µn+1) and let v = sign (θ(0  µn+1)).
Let v1 and xn+1 1 be the subvectors of v and xn+1 corresponding to the active set  and ˜X1 the
submatrix of ˜X whose columns correspond to the active set.
Initialize ˜θ1 = ( ˜X T
Initialize the transition point t0 = 0.

3: Compute the next transition point t0. If it is smaller than the previous transition point or greater

1 ˜y − µv1).

˜X1)−1( ˜X T

1

than 1  go to Step 5.
Case 1 The component of θ1(t0) corresponding to the ith coefﬁcient goes to zero:

Remove i from the active set.
Update v by setting vi = 0.

Case 2 The component of w2(t0) corresponding to the jth coefﬁcient reaches one in absolute

value:
Add j to the active set.
If the component reaches 1 (resp. −1)  then set vj = 1 (resp. vj = −1).

4: Update v1  ˜X1 and xn+1 1 according to the updated active set.

˜X1)−1( ˜X T

1 ˜y − µv1) (rank 1 update).

Update ˜θ1 = ( ˜X T
Go to Step 3.

1

5: Compute ﬁnal value at t = 1  where the values of θ(n+1) on the active set are given by ˜θ1.

The initialization amounts to computing the solution of the Lasso when we have only one data point
(y  x) ∈ R × Rm. In this case  the active set has at most one element. Let i0 = arg maxi |x(i)| and
v = sign(yx(i0)). We have

( 1
(x(i0))2 (yx(i0) − µ1v)ei0 if |yx(i0)| > µ1
0 otherwise

.

θ(1) =

We illustrate our algorithm by showing the solution path when the regularization parameter and t
are successively varied with a simple numerical example in Figure 1.

3.3 Complexity
The complexity of our algorithm is dominated by the inversion of the matrix ˜X T
˜X1 at each transition
point. The size of this matrix is bounded by q = min(n  m). As the update to this matrix after a

1

4

Figure 1: Solution path for both steps of our algorithm. We set n = 5  m = 5  µn = .1n. All
the values of X  y  xn+1 and yn+1 are drawn at random. On the left is the homotopy when the
regularization parameter goes from µn = .5 to µn+1 = .6. There is one transition point as θ2
becomes inactive. On the right is the piecewise smooth path of θ(t) when t goes from 0 to 1. We
can see that θ3 becomes zero  θ2 goes from being 0 to being positive  whereas θ1  θ4 and θ5 remain
active with their signs unchanged. The three transition points are shown as black dots.

transition point is rank 1  the cost of computing the inverse is O(q2). Let k be the total number of
transition points after varying the regularization parameter from µn to µn+1 and t from 0 to 1. The
complexity of our algorithm is thus O(kq2). In practice  the size of the active set d is much lower
than q  and if it remains ∼ d throughout the homotopy  the complexity is O(kd2). It is instructive
to compare it with the complexity of recursive least-square  which corresponds to µn = 0 for all n
and n > m. For this problem the solution typically has m non-zero elements  and therefore the cost
of updating the solution after a new observation is O(m2). Hence if the solution is sparse (d small)
and the active set does not change much (k small)  updating the solution of the Lasso will be faster
than updating the solution to the non-penalized least-square problem.
Suppose that we applied Lars directly to the problem with n + 1 observations without using knowl-
edge of θ(n) by varying the regularization parameter from a large value where the size of the active
set is 0 to µn+1. Let k0 be the number of transition points. The complexity of this approach is
O(k0q2)  and we can therefore compare the efﬁciency of these two approaches by comparing the
number of transition points.

4 Applications

4.1 Compressive sensing
Let θ0 ∈ Rm be an unknown vector that we wish to reconstruct. We observe n linear projections
yi = xT
i θ0 + νi  where νi is Gaussian noise of variance σ2. In general one needs m such measure-
ment to reconstruct θ0. However  if θ0 has a sparse representation with k non-zero coefﬁcients  it
has been shown in the noiseless case that it is sufﬁcient to use n ∝ k log m such measurements.
This approach is known as compressive sensing [16][17] and has generated a tremendous amount of
interest in the signal processing community. The reconstruction is given by the solution of the Basis
Pursuit (BP) problem

kθk1 subject to Xθ = y.

min

θ

If measurements are obtained sequentially  it is advantageous to start estimating the unknown sparse
signal as measurements arrive  as opposed to waiting for a speciﬁed number of measurements. Al-
gorithms to solve BP with sequential measurements have been proposed in [18][19]  and it has been
shown that the change in the active set gives a criterion for how many measurements are needed to
recover the underlying signal [19].
In the case where the measurements are noisy (σ > 0)  a standard approach to recover θ0 is to
solve the Basis Pursuit DeNoising problem instead [20]. Hence  our algorithm is well suited for

5

compressive sensing with sequential and noisy measurements. We compare our proposed algorithm
to Lars as applied to the entire dataset each time we receive a new measurement. We also compare
our method to coordinate descent [11] with warm start: when receiving a new measurement  we
initialize coordinate descent (CD) to the actual solution.
We sample measurements of a model where m = 100  the vector θ0 used to sample the data has 25
non- zero elements whose values are Bernoulli ±1  xi ∼ N (0  Im)  σ = 1  and we set µn = .1n.
The reconstruction error decreases as the number of measurements grows (not plotted). The param-
eter that controls the complexity of Lars and RecLasso is the number of transition points. We see
in Figure 2 that this quantity is consistently smaller for RecLasso  and that after 100 measurements
when the support of the solution does not change much there are typically less than 5 transition
points for RecLasso. We also show in Figure 2 timing comparison for the three algorithms that we
have each implemented in Python. We observed that CD requires a lot of iterations to converge to
the optimal solution when n < m  and we found difﬁcult to set a stopping criterion that ensures
convergence. Our algorithm is consistently faster than Lars and CD with warm- start.

Figure 2: Compressive sensing results. On the x- axis of the plots are the iterations of the algorithm 
where at each iteration we receive a new measurement. On the left is the comparison of the number
of transition points for Lars and RecLasso  and on the right is the timing comparison for the three
algorithms. The simulation is repeated 100 times and shaded areas represent one standard deviation.

4.2 Selection of the regularization parameter

We have supposed until now a pre- determined regularization schedule  an assumption that is not
practical. The amount of regularization depends indeed on the variance of the noise present in
the data which is not known a priori. It is therefore not obvious how to determine the amount of
regularization. We write µn = nλn such that λn is the weighting factor between the average mean-
squared error and the ‘1- norm. We propose an algorithm that selects λn in a data- driven manner.
The problem with n observations is given by

θ(λ) = arg min

θ

1
2n

i θ − yi)2 + λkθk1.
(xT

nX

i=1

We have seen previously that θ(λ) is piecewise linear  and we can therefore compute its gradient
n+1θ(λ)−yn+1)2 be the error on the new observation.
unless λ is a transition point. Let err(λ) = (xT
We propose the following update rule to select λn+1

log λn+1 = log λn − η
⇒ λn+1 = λn × exp

n

∂err
∂ log λ
2nηxT

(λn)

n+1 1(X T

1 X1)−1v1(xT

n+1θ1 − yn+1)

o

 

where the solution after n observations corresponding to the regularization parameter λn is given by
1   0T )  and v1 = sign(θ1). We therefore use the new observation as a test set  which allows us
(θT
to update the regularization parameter before introducing the new observation by varying t from 0

6

to 1. We perform the update in the log domain to ensure that λn is always positive. We performed
simulations using the same experimental setup as in Section 4.1 and using η = .01. We show in
Figure 3 a representative example where λ converges. We compared this value to the one we would
obtain if we had a training and a test set with 250 observations each such that we could ﬁt the model
on the training set for various values of λ  and see which one gives the smallest prediction error on
the test set. We obtain a very similar result  and understanding the convergence properties of our
proposed update rule for the regularization parameter is the object of current research.

4.3 Leave-one-out cross-validation

We suppose in this Section that we have access to a dataset (yi  xi)i=1...n and that µn = nλ. The
parameter λ is tied to the amount of noise in the data which we do not know a priori. A standard
approach to select this parameter is leave-one-out cross-validation. For a range of values of λ  we
use n − 1 data points to solve the Lasso with regularization parameter (n − 1)λ and then compute
the prediction error on the data point that was left out. This is repeated n times such that each data
point serves as the test set. Hence the best value for λ is the one that leads to the smallest mean
prediction error.
Our proposed algorithm can be adapted to the case where we wish to update the solution of the Lasso
after a data point is removed. To do so  we compute the ﬁrst homotopy by varying the regularization
parameter from nλ to (n − 1)λ. We then compute the second homotopy by varying t from 1 to 0
which has the effect of removing the data point that will be used for testing. As the algorithm is
very similar to the one we proposed in Section 3.2 we omit the derivation. We sample a model with
n = 32 and m = 32. The vector θ0 used to generate the data has 8 non-zero elements. We add
Gaussian noise of variance 0.2 to the observations  and select λ for a range of 10 values. We show in
Figure 4 the histogram of the number of transition points for our algorithm when solving the Lasso
with n − 1 data points (we solve this problem 10 × n times). Note that in the majority cases there
are very few transition points  which makes our approach very efﬁcient in this setting.

Figure 3: Evolution of the regularization param-
eter when using our proposed update rule.

Figure 4: Histogram of the number of transition
points when removing an observation.

5 Conclusion

We have presented an algorithm to solve ‘1-penalized least-square regression with online observa-
tions. We use the current solution as a “warm-start” and introduce an optimization problem that al-
lows us to compute an homotopy from the current solution to the solution after observing a new data
point. The algorithm is particularly efﬁcient if the active set does not change much  and we show a
computational advantage as compared to Lars and Coordinate Descent with warm-start for applica-
tions such as compressive sensing with sequential observations and leave-one-out cross-validation.
We have also proposed an algorithm to automatically select the regularization parameter where each
new measurement is used as a test set.

7

Acknowledgments

We wish to acknowledge support from NSF grant 0835531  and Guillaume Obozinski and Chris
Rozell for fruitful discussions.

References
[1] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B  58(1):267–288  1996.

[2] S. Chen  D. Donoho  and M. Saunders. Atomic decomposition by basis pursuit. SIAM Review  43(1):129–

159  2001.

[3] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ. Press  2004.
[4] S-J. Kim  K. Koh  M. Lustig  S. Boyd  and D. Gorinevsky. An interior-point method for large-scale
l1-regularized least squares. IEEE Journal of Selected Topics in Signal Processing  1(4):606–617  2007.
[5] B. Efron  T. Hastie  I. Johnstone  and R. Tibshirani. Least angle regression. Annals of Statistics 

32(2):407–499  2004.

[6] M.R. Osborne  B. Presnell  and B.A. Turlach. A new approach to variable selection in least squares

problems. IMA Journal of Numerical Analysis  20:389–404  2000.

[7] D.M. Malioutov  M. Cetin  and A.S. Willsky. Homotopy continuation for sparse signal representation.
In Proceedings of the International Conference on Acoustics  Speech  and Signal Processing (ICASSP) 
Philadelphia  PA  March 2005.

[8] I. Drori and D.L. Donoho. Solution of ‘1 minimization problems by lars/homotopy methods. In Proceed-
ings of the International Conference on Acoustics  Speech  and Signal Processing (ICASSP)  Toulouse 
France  May 2006.

[9] I. Daubechies  M. Defrise  and C. De Mol. An iterative thresholding algorithm for linear inverse problems

with a sparsity constraint. Communications on Pure and Applied Mathematics  57:1413–1541  2004.

[10] C.J. Rozell  D.H. Johnson  R.G. Baraniuk  and B.A. Olshausen. Locally competitive algorithms for sparse
approximation. In Proceedings of the International Conference on Image Processing (ICIP)  San Antonio 
TX  September 2007.

[11] J. Friedman  T. Hastie  H. Hoeﬂing  and R. Tibshirani. Pathwise coordinate optimization. The Annals of

Applied Statistics  1(2):302–332  2007.

[12] H. Lee  A. Battle  R. Raina  and A.Y. Ng. Efﬁcient sparse coding algorithms.

Neural Information Processing Systems (NIPS)  2007.

In Proceedings of the

[13] M. Figueiredo and R. Nowak. A bound optimization approach to wavelet-based image deconvolution.
In Proceedings of the International Conference on Image Processing (ICIP)  Genova  Italy  September
2005.

[14] M. Figueiredo  R. Nowak  and S. Wright. Gradient projection for sparse reconstruction: Application to
compressed sensing and other inverse problems. IEEE Journal of Selected Topics in Signal Processing 
1(4):586–597  2007.

[15] M Osborne. An effective method for computing regression quantiles. IMA Journal of Numerical Analysis 

Jan 1992.

[16] E. Cand`es. Compressive sampling. Proceedings of the International Congress of Mathematicians  2006.
[17] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–1306  2006.
[18] S. Sra and J.A. Tropp. Row-action methods for compressed sensing. In Proceedings of the International

Conference on Acoustics  Speech  and Signal Processing (ICASSP)  Toulouse  France  May 2006.

[19] D. Malioutov  S. Sanghavi  and A. Willsky. Compressed sensing with sequential observations. In Pro-
ceedings of the International Conference on Acoustics  Speech  and Signal Processing (ICASSP)  Las
Vegas  NV  March 2008.

[20] Y. Tsaig and D.L. Donoho. Extensions of compressed sensing. Signal Processing  86(3):549–571  2006.

8

,Sergey Levine
Vladlen Koltun
Trevor Campbell
Julian Straub
John Fisher III