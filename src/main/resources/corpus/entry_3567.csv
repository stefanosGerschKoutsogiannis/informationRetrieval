2010,Multi-Stage Dantzig Selector,We consider the following sparse signal recovery (or feature selection) problem: given a design matrix $X\in \mathbb{R}^{n\times m}$ $(m\gg n)$ and a noisy observation vector $y\in \mathbb{R}^{n}$ satisfying $y=X\beta^*+\epsilon$ where $\epsilon$ is the noise vector following a Gaussian distribution $N(0 \sigma^2I)$  how to recover the signal (or parameter vector) $\beta^*$ when the signal is sparse?  The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper  we propose a multi-stage Dantzig selector method  which iteratively refines the target signal $\beta^*$. We show that if $X$ obeys a certain condition  then with a large probability the difference between the solution $\hat\beta$ estimated by the proposed method and the true solution $\beta^*$ measured in terms of the $l_p$ norm ($p\geq 1$) is bounded as \begin{equation*} \|\hat\beta-\beta^*\|_p\leq \left(C(s-N)^{1/p}\sqrt{\log m}+\Delta\right)\sigma  \end{equation*} $C$ is a constant  $s$ is the number of nonzero entries in $\beta^*$  $\Delta$ is independent of $m$ and is much smaller than the first term  and $N$ is the number of entries of $\beta^*$ larger than a certain value in the order of $\mathcal{O}(\sigma\sqrt{\log m})$. The proposed method improves the estimation bound of the standard Dantzig selector approximately from $Cs^{1/p}\sqrt{\log m}\sigma$ to $C(s-N)^{1/p}\sqrt{\log m}\sigma$ where the value $N$ depends on the number of large entries in $\beta^*$. When $N=s$  the proposed algorithm achieves the oracle solution with a high probability. In addition  with a large probability  the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.,Multi-Stage Dantzig Selector

Ji Liu  Peter Wonka  Jieping Ye

{ji.liu peter.wonka jieping.ye}@asu.edu

Arizona State University

Abstract

We consider the following sparse signal recovery (or feature selection) problem:
given a design matrix X ∈ Rn×m (m (cid:192) n) and a noisy observation vector
y ∈ Rn satisfying y = Xβ∗ +  where  is the noise vector following a Gaussian
distribution N(0  σ2I)  how to recover the signal (or parameter vector) β∗ when
the signal is sparse?
The Dantzig selector has been proposed for sparse signal recovery with strong
theoretical guarantees. In this paper  we propose a multi-stage Dantzig selector
method  which iteratively reﬁnes the target signal β∗. We show that if X obeys a
certain condition  then with a large probability the difference between the solution
ˆβ estimated by the proposed method and the true solution β∗ measured in terms
of the lp norm (p ≥ 1) is bounded as

(cid:112)

(cid:180)

(cid:179)

(cid:107) ˆβ − β∗(cid:107)p ≤

C(s − N)1/p

log m + ∆

σ 

where C is a constant  s is the number of nonzero entries in β∗  ∆ is independent
of m and is much smaller than the ﬁrst term  and N is the number of entries of
β∗ larger than a certain value in the order of O(σ
log m). The proposed method
from Cs1/p√
improves the estimation bound of the standard Dantzig selector approximately
log mσ where the value N depends on the
number of large entries in β∗. When N = s  the proposed algorithm achieves the
oracle solution with a high probability. In addition  with a large probability  the
proposed method can select the same number of correct features under a milder
condition than the Dantzig selector.

log mσ to C(s− N)1/p√

√

1 Introduction

The sparse signal recovery problem has been studied in many areas including machine learning
[18  19  22]  signal processing [8  14  17]  and mathematics/statistics [2  5  7  10  11  12  13  20].
In the sparse signal recovery problem  one is mainly interested in the signal recovery accuracy  i.e. 
the distance between the estimation ˆβ and the original signal or the true solution β∗. If the design
matrix X is considered as a feature matrix  i.e.  each column is a feature vector  and the observation
y as a target object vector  then the sparse signal recovery problem is equivalent to feature selection
(or model selection). In feature selection  one concerns the feature selection accuracy. Typically 
a group of features corresponding to the coefﬁcient values in ˆβ larger than a threshold form the
supporting feature set. The difference between this set and the true supporting set (i.e.  the set of
features corresponding to nonzero coefﬁcients in the original signal) measures the feature selection
accuracy.
Two well-known algorithms for learning sparse signals include LASSO [15] and Dantzig selec-
tor [7]:

LASSO

min
β

:

(cid:107)Xβ − y(cid:107)2

2 + λ||β||1

(1)

1
2

1

Dantzig Selector

: ||β||1

min
β
s.t. : (cid:107)X T (Xβ − y)(cid:107)∞ ≤ λ

(2)

Strong theoretical results concerning LASSO and Dantzig selector have been established in the
literature [4  5  7  17  20  22].

1.1 Contributions

In this paper  we propose a multi-stage procedure based on the Dantzig selector  which estimates
the supporting feature set F0 and the signal ˆβ iteratively. The intuition behind the proposed multi-
stage method is that feature selection and signal recovery are tightly correlated and they can beneﬁt
from each other: a more accurate estimation of the supporting features can lead to a better signal
recovery and a more accurate signal recovery can help identify a better set of supporting features.
In the proposed method  the supporting set F0 starts from an empty set and its size increases by one
after each iteration. At each iteration  we employ the basic framework of Dantzig selector and the
information about the current supporting feature set F0 to estimate the new signal ˆβ. In addition  we
select the supporting feature candidates in F0 among all features in the data at each iteration  thus
allowing to remove incorrect features from the previous supporting feature set.
The main contributions of this paper lie in the theoretical analysis of the proposed method. Speciﬁ-
selector approximately from Cs1/p√
cally  we show: 1) the proposed method can improve the estimation bound of the standard Dantzig
log mσ where the value N depends
on the number of large entries in β∗; 2) when N = s  the proposed algorithm can achieve the oracle
solution with a high probability; 3) with a high probability  the proposed method can select the same
number of correct features under a milder condition than the standard Dantzig selector method. The
numerical experiments validate these theoretical results.

log mσ to C(s − N)1/p√

1.2 Related Work

Sparse signal recovery without the observation noise was studied in [6]. It has been shown that
under certain irrepresentable conditions  the 0-support of the LASSO solution is consistent with the
true solution. It was shown that when the absolute value of each element in the true solution is large
enough  a weaker condition (coherence property) can guarantee the feature selection accuracy [5].
The prediction bound of LASSO  i.e.  (cid:107)X( ˆβ − β∗)(cid:107)2  was also presented. A comprehensive anal-
ysis for LASSO  including the recovery accuracy in an arbitrary lp norm (p ≥ 1)  was presented
in [20]. In [7]  the Dantzig selector was proposed for sparse signal recovery and a bound of recovery
accuracy with the same order as LASSO was presented. An approximate equivalence between the
LASSO estimator and the Dantzig selector was shown in [1]. In [11]  the l∞ convergence rate was
studied simultaneously for LASSO and Dantzig estimators in a high-dimensional linear regression
model under a mutual coherence assumption. In [9]  conditions on the design matrix X under which
the LASSO and Dantzig selector coefﬁcient estimates are identical for certain tuning parameters
were provided.
Many heuristic methods have been proposed in the past  including greedy least squares regression
[16  8  19  21  3]  two stage LASSO [20]  multiple thresholding procedures [23]  and adaptive
LASSO [24]. They have been shown to outperform the standard convex methods in many prac-
tical applications.
It was shown [16] that under an irrepresentable condition the solution of the
greedy least squares regression algorithm (also named OMP or forward greedy algorithm) guaran-
tees the feature selection consistency in the noiseless case. The results in [16] were extended to
the noisy case [19]. Very recently  the results were further improved in [21] by considering arbi-
trary loss functions (not necessarily quadratic). In [3]  the consistency of OMP was shown under
the mutual incoherence conditions. A multiple thresholding procedure was proposed to reﬁne the
solution of LASSO or Dantzig selector [23]. An adaptive forward-backward greedy algorithm was
proposed [18]  and it was shown that under the restricted isometry condition the feature selection
consistency is achieved if the minimal nonzero entry in the true solution is larger than O(σ
log m).
The adaptive LASSO was proposed to adaptively tune the weight value for the L1 penalty  and it
was shown to enjoy the oracle properties [24].

√

2

1.3 Deﬁnitions  Notations  and Basic Assumptions
We use X ∈ Rn×m to denote the design matrix and focus on the case m (cid:192) n  i.e.  the signal
dimension is much larger than the observation dimension. The correlation matrix A is deﬁned as
A = X T X with respect to the design matrix. The noise vector  follows the multivariate normal
distribution  ∼ N(0  σ2I). The observation vector y ∈ Rn satisﬁes y = Xβ∗+  where β∗ denotes
the original signal (or true solution). ˆβ is used to denote the solution of the proposed algorithm. The
α-supporting set (α ≥ 0) for a vector β is deﬁned as

suppα(β) = {j : |βj| > α}.

The “supporting” set of a vector refers to the 0-supporting set. F denotes the supporting set of the
original signal β∗. For any index set S  |S| denotes the size of the set and ¯S denotes the complement
of S in {1  2  3  ...  m}.
In this paper  s is used to denote the size of the supporting set F   i.e. 
s = |F|. We use βS to denote the subvector of β consisting of the entries of β in the index set S.
The lp norm of a vector v is computed by (cid:107)v(cid:107)p = (
i )1/p  where vi denotes the ith entry of v.
i vp
F XF )−1X T
The oracle solution ¯β is deﬁned as ¯βF = (X T
F y  and ¯β ¯F = 0. We employ the following
notation to measure some properties of a PSD matrix M ∈ RK×K [20]:

(cid:80)

µ(p)
M k =

inf

u∈Rk |I|=k

(cid:107)MI I u(cid:107)p

(cid:107)u(cid:107)p

 

θ(p)
M k l =

sup

u∈Rl |I|=k |J|=l I∩J=∅

ρ(p)
M k =
(cid:107)MI J u(cid:107)p

(cid:107)u(cid:107)p

sup

u∈Rk |I|=k

(cid:107)MI I u(cid:107)p

(cid:107)u(cid:107)p

 

 

(3)

(4)

where p ∈ [1 ∞]  I and J are disjoint subsets of {1  2  ...  K}  and MI J ∈ R|I|×|J| is a submatrix
of M with rows from the index set I and columns from the index set J. Additionally  we use the
following notation to denote two probabilities:

1 = η1(π log ((m − s)/η1))−1/2 
η(cid:48)

η(cid:48)
2 = η2(π log(s/η2))−1/2.

(5)

where η1 and η2 are two factors between 0 and 1. In this paper  if we say “large”  “larger” or “the
largest”  it means that the absolute value is large  larger or the largest. For simpler notation in the
computation of sets  we sometimes use “S1 + S2” to indicate the union of two sets S1 and S2  and
use “S1 − S2” to indicate the removal of the intersection of S1 and S2 from the ﬁrst set S1. In this
paper  the following assumption is always admitted.
Assumption 1. We assume that s = |supp0(β∗)| < n  the variable number is much larger than
the feature dimension (i.e. m (cid:192) n)  each column vector is normalized as X T
i Xi = 1 where Xi
indicates the ith column (or feature) of X  and the noise vector  follows the Gaussian distribution
N(0  σ2I).

i Xi = n  which is essentially identical to our assump-
In the literature  it is often assumed that X T
tion. However  this may lead to a slight difference of a factor
n in some conclusions. We have
automatically transformed conclusions from related work according to our assumption when citing
them in our paper.

√

1.4 Organization

The rest of the paper is organized as follows. We present our multi-stage algorithm in Section 2. The
main theoretical results are summarized in Section 3 with detailed proofs given in the supplemental
material. The numerical simulation is reported in Section 4. Finally  we conclude the paper in
Section 5. All proofs can be found in the supplementary ﬁle.

2 The Multi-Stage Dantzig Selector Algorithm

In this section  we introduce the multi-stage Dantzig selector algorithm. In the proposed method 
we update the support set F0 and the estimation ˆβ iteratively; the supporting set F0 starts from an
empty set and its size increases by one after each iteration. At each iteration  we employ the basic

3

framework of Dantzig selector and the information about the current supporting set F0 to estimate
the new signal ˆβ by solving the following linear program:

min (cid:107)β ¯F0(cid:107)1
s.t. (cid:107)X T
¯F0
(cid:107)X T

(Xβ − y)(cid:107)∞ ≤ λ
F0(Xβ − y)(cid:107)∞ = 0.

(6)

Since the features in F0 are considered as the supporting candidates  it is natural to enforce them to
be orthogonal to the residual vector Xβ − y  i.e.  one should make use of them for reconstructing
the overestimation y. This is the rationale behind the constraint: (cid:107)X T
F0(Xβ − y)(cid:107)∞ = 0. The
other advantage is when all correct features are chosen  the proposed algorithm can be shown to
converge to the oracle solution. The detailed procedure is formally described in Algorithm 1 below.
0 = ∅ and N = 0  the proposed method is identical to the standard Dantzig
Apparently  when F (0)
selector.

0

Algorithm 1 Multi-Stage Dantzig Selector
Require: F (0)
  λ  N  X  y 
Ensure: ˆβ(N )  F (N )
1: while i=0; i≤N; i++ do
2:
3:
0
4: end while

0

Obtain ˆβ(i) by solving the problem (6) with F0 = F (i)
Form F (i+1)

as the index set of the i + 1 largest elements of ˆβ(i).

0

3 Main Results

3.1 Motivation

To motivate the proposed multi-stage algorithm  we ﬁrst consider a simple case where some knowl-
edge about the supporting features is known in advance. In standard Dantzig selector  we assume
F0 = ∅. If we assume that the features belonging to a set F0 are known as supporting features  i.e. 
F0 ⊂ F   we have the following result:
Theorem 1. Assume that assumption 1 holds. Take F0 ⊂ F and λ = σ
optimization problem (6). If there exists some l such that

(cid:114)

m−s
η1

in the

2 log

(cid:179)

(cid:180)

1  the lp-norm (1 ≤ p ≤ ∞) of the difference between

(cid:182)1−1/p

l

(cid:181)| ¯F0 − ¯F|
(cid:184)1/p
(cid:179)| ¯F0− ¯F|

A s+l l

1 +

> 0

(cid:183)

(cid:115)

(cid:107) ˆβ − ¯β(cid:107)p ≤

(cid:179)| ¯F0− ¯F|

(| ¯F0 − ¯F| + l2p)1/p

holds  then with a probability larger than 1− η(cid:48)
ˆβ  the solution of the problem (6)  and the oracle solution ¯β is bounded as

A s+l − θ(p)
µ(p)
(cid:180)p−1
A s+l − θ(p)
µ(p)
and with a probability larger than 1 − η(cid:48)
(cid:184)1/p
(cid:180)p−1
ˆβ  the solution of the problem (6) and the true solution β∗ is bounded as
(cid:179)| ¯F0− ¯F|
(cid:112)
A s+l − θ(p)
µ(p)
s1/p

(| ¯F0 − ¯F| + l2p)1/p

(cid:179)| ¯F0− ¯F|

(cid:180)1−1/p

(cid:180)1−1/p

(cid:107) ˆβ − β∗(cid:107)p ≤

(cid:115)

(cid:183)

1 +

A s+l l

A s+l l

l

l

l

l

σ

2 log(s/η2)

µ(p)
(X T

F XF )1/2 s

σ

2 log

m − s
η1

(7)

(cid:181)

(cid:181)

(cid:182)

(cid:182)

σ

2 log

m − s
η1

+

(8)

1 − η(cid:48)

2  the lp-norm (1 ≤ p ≤ ∞) of the difference between

4

It is clear that both bounds (for any 1 ≤ p ≤ ∞) are monotonically increasing with respect to the
value of | ¯F0 − ¯F|. In other words  the larger F0 is  the lower these bounds are. This coincides
with our motivation that more knowledge about the supporting features can lead to a better signal
estimation. Most related literatures directly estimate the bound of (cid:107) ˆβ − β∗(cid:107)p. Since β∗ may not be
a feasible solution of problem (6)  it is not easy to directly estimate the distance between ˆβ and β∗.
The bound in the inequality (8)  which consists of two terms. Since m (cid:192) n ≥ s  we have

(cid:112)

2 log((m − s)/η1) (cid:192)(cid:112)

2 log(s/η2) if η1 ≈ η2. When p = 2  the following holds:

(cid:181)| ¯F0 − ¯F|

(cid:182)1−1/2 ≤ µ(2)

(X T

F XF )1/2 s

F XF  s ≤ µ(2)

(X T

F XF )1/2 s.

A s+l − θ(2)
µ(2)

A s+l l

l
A s ≤ µ(2)

X T

A s+l ≤ µ(2)
µ(2)

since

From the analysis in the next section  we can see that the ﬁrst term is the upper bound of the distance
from the optimizer to the oracle solution (cid:107) ˆβ − ¯β(cid:107)p and the second term is the upper bound of the
distance from the oracle solution to the true solution (cid:107) ¯β − β∗(cid:107)p. Thus  the ﬁrst term might be much
larger than the second term.

3.2 Comparison with Dantzig Selector

We ﬁrst compare our estimation bound with the one in [7] for p = 2. For convenience of comparison 
we rewrite the theorem in [7] equivalently as:
Theorem 2. Suppose β ∈ Rm is any s-sparse vector of parameters obeying δ2s + θ(2)
Setting λp = σ
solution of the standard Dantzig selector ˆβD obeys

A s 2s < 1.
2 log(m/η) (0 < η ≤ 1)  with a probability at least 1 − η(π log m)−1/2  the

(cid:112)

(cid:112)

(cid:107) ˆβD − β∗(cid:107)2 ≤

4

1 − δ2s − θ(2)

A s 2s

s1/2σ

2 log(m/η) 

(9)

A 2s − 1  1 − µ(2)

A 2s).

where δ2s = max(ρ(2)
Theorem 1 also implies a bound estimation result for Dantzig selector by letting F0 = ∅ and p = 2.
Speciﬁcally  we set F0 = ∅  N = 0  and λ = σ
in the multi-stage method  and set
p = 2  l = s  η1 = m−s
m η  and η2 = s
m η for a convenient of comparison with Theorem 1. If follows
that with probability larger than 1 − η(π log m)−1/2  the following bound holds:
√

(cid:114)

m−s
η1

2 log

(cid:179)

(cid:180)
 s1/2σ

(cid:112)

2 log (m/η).

(10)

(cid:107) ˆβ − β∗(cid:107)2 ≤

10
A 2s − θ(2)
µ(2)

A 2s s

+

1

µ(2)
(X T

F XF )1/2 s



It is easy to verify that
F XF )1/2 s ≤ 1.
1−δ2s−θ(2)
A s 2s ≤ µ(2)
Thus  the bound in (10) is comparable to the one in (9). In the following  we compare the perfor-
mance bound of the proposed multi-stage method (N > 0) with the one in (10).

A 2s s ≤ µ(2)

A 2s ≤ µ(2)

A 2s−θ(2)

F XF ) s =

F XF )1/2 s

µ(2)
(X T

(X T

(X T

(cid:179)

(cid:180)2 ≤ µ(2)

3.3 Feature Selection

The estimation bounds in Theorem 1 assume that a set F0 is given. In this section  we show how
the supporting set can be estimated. Similar to previous work [5  19]  |β∗
j | for j ∈ F is required
to be larger than a threshold value. As is clear from the proof in the next section  the threshold
value mainly depends on the value of (cid:107) ˆβ − β∗(cid:107)∞. We essentially employ the result with p = ∞ in
Theorem 1 to estimate the threshold value. In the following  we ﬁrst consider the simple case when
N = 0. We have shown in the last section that the estimation bound in this case is similar to the one
for Dantzig selector.

5

Theorem 3. Under the assumption 1  if there exists an index set J such that |β∗
j ∈ J and there exists a nonempty set

j | > α0 for any

Ω = {l | µ(∞)

> 0}

(cid:180)

(cid:179) s
(cid:182)

l

A s+l l

A s+l − θ(∞)
(cid:115)
(cid:181)
(cid:162) σ
(cid:114)

2 log

(cid:179)

(cid:180)

m − s
η1

m−s
η1

where

α0 = 4 min
l∈Ω

(cid:162)

(cid:161)
1  s
l
µ(∞)
A s+l − θ(∞)

max

A s+l l

(cid:161)

s
l

(cid:112)

2 log(s/η2) 

1

σ

+

µ(∞)

(X T

F XF )1/2 s

√

√

√

(cid:161)

(cid:162)

s
l

A s+l l

2 log

j | > O(σ

A s+l − θ(∞)

into the problem (6) (equivalent to Dantzig
1 −

then taking F0 = ∅  N = 0  λ = σ
selector)  the largest |J| elements of ˆβstd (or ˆβ(0)) belong to F with probability larger than 1− η(cid:48)
η(cid:48)
2.
The theorem above indicates that under the given condition  if minj∈J |β∗
log m) (as-
suming that there exists l ≥ s such that µ(∞)
> 0)  then with high probability
the selected |J| features by Dantzig selector belong to the true supporting set.
In particular  if
|J| = s  then the consistency of feature selection is achieved. The result above is comparable to the
ones for other feature selection algorithms  including LASSO [5  22]  greedy least squares regres-
sion [16  8  19]  two stage LASSO [20]  and adaptive forward-backward greedy algorithm [18]. In
all these algorithms  the condition minj∈F |β∗
log m is required  since the noise level is
O(σ
log m) [18]. Because C is always a coefﬁcient in terms of the covariance matrix XX T (or
the feature matrix X)  it is typically treated as a constant term; see the literature listed above.
Next  we show that the condition |β∗
procedure with N > 0  as summarized in the following theorem:
Theorem 4. Under the assumption 1  if there exists a nonempty set
> 0}

j | > α0 in Theorem 3 can be relaxed by the proposed multi-stage

A s+l − θ(∞)
Ω = {l | µ(∞)
(cid:115)
(cid:181)
J)| > i holds for all i ∈ {0  1  ... |J| − 1}  where
and there exists a set J such that |suppαi(β∗
(cid:162)(cid:111) σ
(cid:180)

(cid:161)
(cid:114)
A s+l − θ(∞)
µ(∞)

αi = 4 min
l∈Ω

m − s
η1

j | ≥ Cσ

2 log(s/η2) 

(cid:180)
(cid:182)

(cid:179) s

F XF )1/2 s

1  s−i

2 log

m−s
η1

1 − η(cid:48)
2.

then taking F (0)

0 = ∅  λ = σ

and N = |J| − 1 into Algorithm 1  the solution after
0 ⊂ F (i.e. |J| correct features are selected) with probability larger than

N iterations satisﬁes F (N )
1 − η(cid:48)
Assume that one aims to select N correct features by the standard Dantzig selector and the multi-
stage method. These two theorems show that the standard Dantzig selector requires that at least N
j |’s with j ∈ F are larger than the threshold value α0  while the proposed multi-stage method
of |β∗
j |’s are larger than the threshold value αi−1  for i = 1 ···   N. Since
requires that at least i of the |β∗
{αj} is a strictly decreasing sequence satisfying for some l ∈ Ω 
(cid:179)

αi−1 − αi >

(cid:115)

(cid:182)

(cid:181)

2 log

(cid:112)

(cid:162)
(cid:161)

µ(∞)

(X T

s−i
l

(cid:110)

2 log

max

(cid:179)

A s+l l

A s+l l

+

1

σ

l

l

4θ(∞)
µ(∞)
A s+l − θ(∞)

A s+l l

A s+l l

l

(cid:162)(cid:180)2 σ

(cid:161)

s−i
l

m − s
η1

 

the proposed multi-stage method requires a strictly weaker condition for selecting N correct features
than the standard Dantzig selector.

3.4 Signal Recovery

In this section  we derive the estimation bound of the proposed multi-stage method by combing
results from Theorems 1  3  and 4.

6

(cid:180)

A s+l l

l

2 log

m−s
η1

1 − η(cid:48)

A 2s − θ(p)

A 2s s > 0 

µ(∞)
A s+l − θ(∞)

Theorem 5. Under the assumption 1  if there exists l such that

(cid:179) s
(cid:180)
> 0 and µ(p)
(cid:114)
(cid:179)
J)| > i holds for all i ∈ {0  1  ... |J| − 1}  where the
and there exists a set J such that |suppαi(β∗
αi’s are deﬁned in Theorem 4  then
(1) taking F0 = ∅  N = 0 and λ = σ
(cid:115)
(cid:181)
than 1 − η(cid:48)
(cid:114)
(cid:179)
(cid:115)

(2) taking F0 = ∅  N = |J| and λ = σ
than 1 − η(cid:48)
(cid:107) ˆβmul − β∗(cid:107)p ≤ (2p+1 + 2)1/p(s − N)1/p

2  the solution of the multi-stage method ˆβmul (i.e.  ˆβ(N )) obeys:

2  the solution of the Dantzig selector ˆβD (i.e  ˆβ(0)) obeys:

(cid:107) ˆβD − β∗(cid:107)p ≤ (2p+1 + 2)1/ps1/p

into Algorithm 1  with probability larger

into Algorithm 1  with probability larger

(cid:182)
(cid:180)
(cid:181)

A 2s − θ(p)
µ(p)

m − s
η1

2 log(s/η2);

1 − η(cid:48)

µ(p)
(X T

F XF )1/2 s

(cid:112)

σ

2 log

m−s
η1

s1/p

+

(cid:112)

σ

2 log

A 2s s

(11)

A 2s−N − θ(p)
µ(p)

A 2s−N s−N

σ

2 log

+

s1/p

µp
(X T

F XF )1/2 s

σ

2 log(s/η2).

(12)

(cid:182)

m − s
η1

Similar to the analysis in Theorem 1  the ﬁrst term (i.e.  the distance from ˆβ to the oracle solution ¯β)
improved the standard Dantzig selector from Cs1/p√
dominates in the estimated bounds. Thus  the performance of the multi-stage method approximately
log mσ. When
p = 2  our estimation has the same order as the greedy least squares regression algorithm [19] and
the adaptive forward-backward greedy algorithm [18].

log mσ to C(s − N)1/p√

3.5 The Oracle Solution

The oracle solution is the minimum-variance unbiased estimator of the true solution given the noisy
observation. We show in the following theorem that the proposed method can obtain the oracle
solution with high probability under certain conditions:
Theorem 6. Under the assumption 1  if there exists l such that µ(∞)
the supporting set F of β∗ satisﬁes |suppαi(β∗
are deﬁned in Theorem 4  then taking F0 = ∅  N = s and λ = σ
the oracle solution can be achieved  i.e. F (N )
1 − η(cid:48)
The theorem above shows that when the nonzero elements of the true coefﬁcients vector β∗ are large
enough  the oracle solution can be achieved with high probability.

> 0  and
F )| > i for all i ∈ {0  1  ...  s − 1}  where the αi’s
into Algorithm 1 
= F and ˆβ(N ) = ¯β with probability larger than

(cid:114)
A s+l − θ(∞)
(cid:180)

1 − η(cid:48)
2.

m−s
η1

s−i
l

2 log

(cid:179)

A s+l l

(cid:162)

(cid:161)

0

4 Simulation Study

We have performed simulation studies to verify our theoretical analysis. Our comparison includes
two aspects: signal recovery accuracy and feature selection accuracy. The signal recovery accuracy
is measured by the relative signal error: SRA = (cid:107) ˆβ − β∗(cid:107)2/(cid:107)β∗(cid:107)2  where ˆβ is the solution of a
speciﬁc algorithm. The feature selection accuracy is measured by the percentage of correct features
selected: F SA = | ˆF ∩ F|/|F|  where ˆF is the estimated feature candidate set.
We generate an n× m random matrix X. Each element of X follows an independent standard Gaus-
sian distribution N(0  1). We then normalize the length of the columns of X to be 1. The s−sparse
original signal β∗ is generated with s nonzero elements independently uniformly distributed from

7

Figure 1: Numerical simulation. We compare the solutions of the standard Dantzig selector method
(N = 0)  the proposed method for different values of N  and the oracle solution. The SRA and
F SA comparisons are reported on the top row and the bottom row  respectively. The starting point
of each curve records the SRA (or F SA) value of the standard Dantzig selector method; the ending
point records the value of the oracle solution; the middle part of each curve records the results by
the proposed method for different values of N.

[−10  10]. We for y by y = Xβ∗ +   where the noise vector  is generated by the Gaussian distri-
bution N(0  σ2I). For a fair comparison  we choose the same λ = σ
2 log m in both algorithms.
The following experiments are repeated 20 times and we report their average performance.

√

0 = ∅ and output the ˆβ(N )’s. Note that the solution of the
We run the proposed algorithm with F (0)
standard Dantzig selector algorithm is equivalent to ˆβ(0) with N = 0. We report the SRA curve of
ˆβ(N ) with respect to N in the top row of Figure 1. Based on ˆβ(N )  we compute the supporting set
ˆF (N ) as the index of the N largest entries in ˆβ(N ). Note that the supporting set we compute here
is different from the supporting set ˆF (N )
0 which only contains the N largest feature indexes. The
bottom row of Figure 1 shows the F SA curve with respect to N. We can observe from Figure 1 that
1) the multi-stage method obtains a solution with a smaller distance to the original signal than the
standard Dantzig selector method; 2) the multi-stage method selects a larger percentage of correct
features than the standard Dantzig selector method; 3) the multi-stage method can achieve the oracle
solution. Overall  the recovery accuracy curve increases with an increasing value of N and the
feature selection accuracy curve is decreasing with an increasing value of N.

5 Conclusion

In this paper  we propose a multi-stage Dantzig selector method which iteratively selects the sup-
porting features and recovers the original signal. The proposed method makes use of the information
of supporting features to estimate the signal and simultaneously makes use of the information of the
estimated signal to select the supporting features. Our theoretical analysis shows that the proposed
method improves upon the standard Dantzig selector in both signal recovery and supporting feature
selection. The ﬁnal numerical simulation validates our theoretical analysis.
Since the multi-stage procedure can improve the Dantzig selector  one natural question is whether
the analysis can be extended to other related techniques such as LASSO. The two-stage LASSO
has been shown to outperform the standard LASSO. We plan to extend our analysis for multi-stage
LASSO in the future. In addition  we plan to improve the proposed algorithm by adopting stopping
rules similar to the ones recently proposed in [3  19  21].

Acknowledgments
This work was supported by NSF IIS-0612069  IIS-0812551  CCF-0811790  IIS-0953662  and
NGA HM1582-08-1-0016.

8

02468101214160.020.040.060.080.10.120.140.160.180.2NSRAn=50 m=200 s=15 σ=0.001 standardoraclemulti−stage02468101214160.050.10.150.20.250.30.350.40.450.50.55NSRAn=50 m=200 s=15 σ=0.1 standardoraclemulti−stage02468100.020.040.060.080.10.120.140.160.18NSRAn=50 m=500 s=10 σ=0.001 standardoraclemulti−stage02468100.020.040.060.080.10.120.140.160.180.2NSRAn=50 m=500 s=10 σ=0.1 standardoraclemulti−stage02468101214160.850.90.951NFSAn=50 m=200 s=15 σ=0.001 standardoraclemulti−stage02468101214160.650.70.750.80.850.90.951NFSAn=50 m=200 s=15 σ=0.1 standardoraclemulti−stage02468100.880.90.920.940.960.981NFSAn=50 m=500 s=10 σ=0.001 standardoraclemulti−stage02468100.840.860.880.90.920.940.960.981NFSAn=50 m=500 s=10 σ=0.1 standardoraclemulti−stageReferences
[1] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

Annals of Statistics  37:1705–1732  2009.

[2] F. Bunea  A. Tsybakov  and M. Wegkamp. Sparsity oracle inequalities for the Lasso. Electronic

Journal of Statistics  2007.

[3] T. Cai and L. Wang. Orthogonal matching pursuit for sparse signal reconvery. Technical

Report  2010.

[4] T. Cai  G. Xu  and J. Zhang. On recovery of sparse signals via l1 minimization. IEEE Trans-

actions on Information Theory  55(7):3388–3397  2009.

[5] E. J. Candes and Y. Plan. Near-ideal model selection by l1 minimization. Annals of Statistics 

37:2145–2177  2006.

[6] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information

Theory  51(12):4203–4215  2005.

[7] E. J. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger

than n. Annals of Statistics  35:2313  2007.

[8] D. L. Donoho  M. Elad  and V. N. Temlyakov. Stable recovery of sparse overcomplete rep-
resentations in the presence of noise. IEEE Transactions on Information Theory  pages 6–18 
2006.

[9] G. M. James  P. Radchenko  and J. Lv. DASSO: connections between the Dantzig selector and

Lasso. Journal of The Royal Statistical Society Series B  71(1):127–142  2009.

[10] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines on-line

learning and bandits. COLT  pages 229–238  2008.

[11] K. Lounici. Sup-norm convergence rate and sign concentration property of Lasso and Dantzig

esti mators. Electronic Journal of Statistics  2:90–102  2008.

[12] N. Meinshausen  P. Bhlmann  and E. Zrich. High dimensional graphs and variable selection

with the Lasso. Annals of Statistics  34:1436–1462  2006.

[13] P. Ravikumar  G. Raskutti  M. J. Wainwright  and B. Yu. Model selection in gaussian graphical

models: High-dimensional consistency of l1-regularized MLE. pages 1329–1336  2008.

[14] J. Romberg. The Dantzig selector and generalized thresholding. CISS  pages 22–25  2008.
[15] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society: Series B  58(1):267–288  1996.

[16] J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions

on Information Theory  50:2231–2242  2004.

[17] M. J. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using
IEEE Transactions on Information Theory 

l1-constrained quadratic programming (Lasso).
pages 2183–2202  2009.

[18] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models.

NIPS  pages 1921–1928  2008.

[19] T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal

of Machine Learning Reserch  10:555–568  2009.

[20] T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization.

Annals of Statistics  37:2109  2009.

[21] T. Zhang. Sparse recovery with orthogonal matching pursuit under RIP. arXiv:1005.2249 

2010.

[22] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning

Reserch  7:2541–2563  2006.

[23] S. Zhou. Thresholding procedures for high dimensional variable selection and statistical esti-

mation. NIPS  pages 2304–2312  2009.

[24] H. Zou. The adaptive Lasso and its oracle properties. Journal of the American Statistical

Association  101:1418–1429  2006.

9

,Yuening Hu
Jordan Ying
Hal Daume III
Z. Irene Ying
Yunbo Wang
Mingsheng Long
Jianmin Wang
Zhifeng Gao
Philip Yu