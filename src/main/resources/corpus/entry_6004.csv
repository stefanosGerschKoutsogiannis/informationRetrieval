2011,t-divergence Based Approximate Inference,Approximate inference is an important technique for dealing with large  intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by defining a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student's t-prior.,t-divergence Based Approximate Inference

Nan Ding2  S.V. N. Vishwanathan1 2  Yuan Qi2 1
Departments of 1Statistics and 2Computer Science

Purdue University

ding10@purdue.edu  vishy@stat.purdue.edu  alanqi@cs.purdue.edu

Abstract

Approximate inference is an important technique for dealing with large  in-
tractable graphical models based on the exponential family of distributions. We
extend the idea of approximate inference to the t-exponential family by deﬁning
a new t-divergence. This divergence measure is obtained via convex duality be-
tween the log-partition function of the t-exponential family and a new t-entropy.
We illustrate our approach on the Bayes Point Machine with a Student’s t-prior.

1

Introduction

The exponential family of distributions is ubiquitous in statistical machine learning. One promi-
nent application is their use in modeling conditional independence between random variables via a
graphical model. However  when the number or random variables is large  and the underlying graph
structure is complex  a number of computational issues need to be tackled in order to make inference
feasible. Therefore  a number of approximate techniques have been brought to bear on the problem.
Two prominent approximate inference techniques include the Monte Carlo Markov Chain (MCMC)
method [1]  and the deterministic method [2  3].
Deterministic methods are gaining signiﬁcant research traction  mostly because of their high efﬁ-
ciency and practical success in many applications. Essentially  these methods are premised on the
search for a proxy in an analytically solvable distribution family that approximates the true under-
lying distribution. To measure the closeness between the true and the approximate distributions 
the relative entropy between these two distributions is used. When working with the exponential
family  one uses the Shannon-Boltzmann-Gibbs (SBG) entropy in which case the relative entropy is
the well known Kullback-Leibler (KL) divergence [2]. Numerous well-known algorithms in expo-
nential family  such as the mean ﬁeld method [2  4] and the expectation propagation [3  5]  are based
on this criterion.
The thin-tailed nature of the exponential family makes it unsuitable for designing algorithms which
are potentially robust against certain kinds of noisy data. Notable work including [6  7] utilizes
mixture/split exponential family based approximate model to improve the robustness. Meanwhile 
effort has also been devoted to develop alternate  generalized distribution families in statistics [e.g.
8  9]  statistical physics [e.g. 10  11]  and most recently in machine learning [e.g. 12]. Of particular
interest to us is the t-exponential family1  which was ﬁrst proposed by Tsallis and co-workers [10 
13  14]. It is a special case of the more general φ-exponential family of Naudts [11  15–17]. Related
work in [18] has applied the t-exponential family to generalize logistic regression and obtain an
algorithm that is robust against certain types of label noise.
In this paper  we attempt to generalize deterministic approximate inference by using the t-
exponential family.
In other words  the approximate distribution used is from the t-exponential
family. To obtain the corresponding divergence measure as in the exponential family  we exploit the

1Sometimes  also called the q-exponential family or the Tsallis distribution.

1

convex duality between the log-partition function of the t-exponential family and a new t-entropy2
to deﬁne the t-divergence. To illustrate the usage of the above procedure  we use it for approximate
inference in the Bayes Point Machine (BPM) [3] but with a Student’s t-prior.
The rest of the paper is organized as follows. Section 2 consists of a brief review of the t-exponential
family. In Section 3 a new t-entropy is deﬁned as the convex dual of the log-partition function of the
t-exponential family. In Section 4  the t-divergence is derived and is used for approximate inference
in Section 5. Section 6 illustrates the inference approach by applying it to the Bayes Point Machine
with a Student’s t-prior  and we conclude the paper with a discussion in Section 7.

2 The t-exponential Family and Related Entropies

The t-exponential family was ﬁrst proposed by Tsallis and co-workers [10  13  14]. It is deﬁned as
(1)

p(x; θ) := expt (�Φ(x)  θ� − gt(θ))   where
expt(x) :=�exp(x)

if t = 1
otherwise.

1
1−t
+

[1 + (1 − t)x]

The inverse of the expt function is called logt. Note that the log-partition function  gt(θ)  in (1)
preserves convexity and satisﬁes

Here q(x) is called the escort distribution of p(x)  and is deﬁned as

∇θgt(θ) = Eq [Φ(x)] .

q(x) :=

.

p(x)t

� p(x)tdx

See the supplementary material for a proof of convexity of gt(θ) based on material from [17]  and a
detailed review of the t-exponential family of distributions.
There are various generalizations of the Shannon-Boltzmann-Gibbs (SBG) entropy which are pro-
posed in statistical physics  and paired with the t-exponential family of distributions. Perhaps the
most well-known among them is the Tsallis entropy [10]:

(2)

(3)

(4)

(5)

(7)

Htsallis(p) := −� p(x)t logt p(x)dx.

Naudts [11  15  16  17] proposed a more general framework  wherein the familiar exp and log
functions are generalized to expφ and logφ functions which are deﬁned via a function φ. These
generalized functions are used to deﬁne a family of distributions  and corresponding to this family
an entropy like measure called the information content Iφ(p) as well as its divergence measure are
deﬁned. The information content is the dual of a function F (θ)  where

∇θF (θ) = Ep [Φ(x)] .

(6)
Setting φ(p) = pt in the Naudts framework recovers the t-exponential family deﬁned in (1). Inter-
estingly when φ(p) = 1
One another well-known non-SBG entropy is the R´enyi entropy [19]. The R´enyi α-entropy (when
α �= 1) of the probability distribution p(x) is deﬁned as:

t p2−t  the information content Iφ is exactly the Tsallis entropy (5).

Hα(p) =

1

1 − α

log�� p(x)αdx� .

Besides these entropies proposed in statistical physics  it is also worth noting efforts that work with
generalized linear models or utilize different divergence measures  such as [5  8  20  21].
It is well known that the negative SBG entropy is the Fenchel dual of the log-partition function of an
exponential family distribution. This fact is crucially used in variational inference [2]. Although all

2Although closely related  our t-entropy deﬁnition is different from either the Tsallis entropy [10] or the
information content in [17]. Nevertheless  it can be regarded as an example of the generalized framework of
the entropy proposed in [8].

2

of the above generalized entropies are useful in their own way  none of them satisfy this important
property for the t-exponential family. In the following sections we attempt to ﬁnd an entropy which
satisﬁes this property  and outline the principles of approximate inference using the t-exponential
family. Note that although our main focus is the t-exponential family  we believe that our results can
also be extended to the more general φ-exponential family of Naudts [15  17].

3 Convex Duality and the t-Entropy

Deﬁnition 1 (Inspired by Wainwright and Jordan [2]) The t-entropy of a distribution p(x; θ) is
deﬁned as

Ht(p(x; θ)) : = −� q(x; θ) logt p(x; θ) dx = − Eq [logt p(x; θ)] .

(8)

where q(x; θ) is the escort distribution of p(x; θ). It is straightforward to verify that the t-entropy is
non-negative. Furthermore  the following theorem establishes the duality between −Ht and gt. The
proof is provided in the supplementary material. This extends Theorem 3.4 of [2] to the t-entropy.

Theorem 2 For any µ  deﬁne θ(µ) (if exists) to be the parameter of the t-exponential family s.t.

µ = Eq(x;θ(µ)) [Φ(x)] =� Φ(x)q(x; θ(µ)) dx.
g∗t (µ) =�−Ht(p(x; θ(µ))) if θ(µ) exists

+∞ otherwise .

Then

where g∗t (µ) denotes the Fenchel dual of gt(θ). By duality it also follows that

gt(θ) = sup

µ {�µ  θ� − g∗t (µ)} .

(9)

(10)

(11)

From Theorem 2  it is obvious that Ht(µ) is a concave function. Below  we derive the t-entropy
function corresponding to two commonly used distributions. See Figure 1 for a graphical illustration.

Example 1 (t-entropy of Bernoulli distribution) Assume the Bernoulli distribution is Bern(p)
with parameter p. The t-entropy is

Ht(p) = −pt logt p − (1 − p)t logt(1 − p)

pt + (1 − p)t

=

1 − (pt + (1 − p)t)−1

t − 1

(12)

Example 2 (t-entropy of Student’s t-distribution) Assume that a k-dim Student’s t-distribution
p(x; µ  Σ  v) is given by (54)  then the t-entropy of p(x; µ  Σ  v) is given by

where K = (v Σ)−1  v = 2

Ψ

Ht(p(x))) = −

t−1 − k  and Ψ =�

1
1 − t

1 − t�1 + v−1� +
(πv)k/2Γ(v/2)| Σ |1/2�−2/(v+k)

Γ((v+k)/2)

(13)

.

3.1 Relation with the Tsallis Entropy

Using (4)  (5)  and (8)  the relation between the t-entropy and Tsallis entropy is obvious. Basically 
the t-entropy is a normalized version of the Tsallis entropy 

Ht(p) = −

1

� p(x)tdx� p(x)t logt p(x)dx =

1

� p(x)tdx

3

Htsallis(p).

(14)

)
p
(
t

H

1

0.8

0.6

0.4

0.2

0

0

t=0.1
t=0.5
t=1.0
t=1.5
t=1.9

0.2

0.4

0.6

0.8

1

p

)
2
σ
(
t

H

15

10

5

0

0

t=1.0
t=1.3
t=1.6
t=1.9

2

4

6

8

10

σ2

Figure 1: t-entropy corresponding to two well known probability distributions. Left: the Bernoulli
distribution Bern(x; p); Right: the Student’s t-distribution St(x; 0  σ2  v)  where v = 2/(t−1)−1.
One can recover the SBG entropy by setting t = 1.0.

3.2 Relation with the R´enyi Entropy

We can equivalently rewrite the R´enyi Entropy as:

.

(15)

Hα(p) =

1

1 − α

log�� p(x)αdx� = − log�� p(x)αdx�−1/(1−α)
= − logt�� p(x)tdx�−1/(1−t)

.

The t-entropy of p(x) (when t �= 1) is equal to
Ht(p) = −� p(x)t logt p(x)dx
� p(x)tdx

Therefore  when α = t 

Ht(p) = − logt(exp(−Hα(p)))

When t and α → 1  both entropies go to the SBG entropy.
4 The t-divergence

(16)

(17)

(18)

(19)

Recall that the Bregman divergence deﬁned by a convex function −H between p and ˜p is [22]:

D(p� ˜p) = −H(p) + H(˜p) +� dH(˜p)

d ˜p

(˜p(x) − p(x))dx.

For the SBG entropy  it is easy to verify that the Bregman divergence leads to the relative SBG-
entropy (also widely known as the Kullback-Leibler (KL) divergence). Analogously  one can deﬁne
the t-divergence3 as the Bregman divergence or relative entropy based on the t-entropy.

Deﬁnition 3 The t-divergence  which is the relative t-entropy between two distribution p(x) and
˜p(x)  is deﬁned as 

Dt(p� ˜p) =� q(x) logt p(x) − q(x) logt ˜p(x)dx.

The following theorem states the relationship between the relative t-entropy and the Bregman diver-
gence. The proof is provided in the supplementary material.
Theorem 4 The t-divergence is the Bregman divergence deﬁned on the negative t-entropy −Ht(p).
3Note that the t-divergence is not a special case of the divergence measure of Naudts [17] because the

entropies are deﬁned differently the derivations are fairly similar in spirit.

4

The t-divergence plays a central role in the variational inference that will be derived shortly. It also
preserves the following properties:

• Dt(p� ˜p) ≥ 0  ∀p  ˜p. The equality holds only for p = ˜p.
• Dt(p� ˜p) �= Dt(˜p�p).

Example 3 (Relative t-entropy between Bernoulli distributions) Assume that two Bernoulli dis-
tributions Bern(p1) and Bern(p2)  then the relative t-entropy Dt(p1�p2) between these two dis-
tributions is:

Dt(p1�p2) =

=

pt
1 logt p1 + (1 − p1)t logt(1 − p1) − pt

1 logt p2 − (1 − p1)t logt(1 − p2)

pt
1 + (1 − p1)t

1 − pt

1p1−t
2 − (1 − p1)t(1 − p2)1−t
(1 − t)(pt

1 + (1 − p1)t)

Example 4 (Relative t-entropy between Student’s t-distributions) Assume that two Student’s t-
distributions p1(x; µ1  Σ1  v) and p2(x; µ2  Σ2  v) are given  then the relative t-entropy Dt(p1�p2)
between these two distributions is:

(20)

(21)

(22)

(23)

Dt(p1�p2) =� q1(x) logt p1(x) − q1(x) logt p2(x)dx

=

−

Ψ1

1 − t�1 + v−1� +

2Ψ2
1 − t

Ψ2
1 − t

T r�K�2 Σ1� −

µ�1 K2 µ2

Ψ2
1 − t

µ�1 K2 µ1 −

Ψ2

1 − t�µ�2 K2 µ2 + 1�

1

0.8

)
2
p
�

0.6

1
p
(
t

D

0.4

0.2

0

0

t=0.1
t=0.5
t=1.0
t=1.5
t=1.9

60

)
2
p
�

40

1
p
(
t

D

20

t=1.0
t=1.3
t=1.6
t=1.9

0.2

0.4

0.6

0.8

1

p1

0

−4

−2

0
µ

2

4

4

3

)
2
p
�

1
p
(
t

D

2

1

0

0

t=1.0
t=1.3
t=1.6
t=1.9

2

4

6

8

10

σ2

Figure 2: The t-divergence between: Left: Bern(p1) and Bern(p2 = 0.5); Middle: St(x; µ  1  v)
and St(x; 0  1  v); Right: St(x; 0  σ2  v) and St(x; 0  1  v)  where v = 2/(t − 1) − 1.

5 Approximate Inference in the t-Exponential Family

In essence  the deterministic approximate inference ﬁnds an approximate distribution from an an-
alytically tractable distribution family which minimizes the relative entropy (e.g. KL-divergence
in exponential family) with the true distribution. Since the relative entropy is not symmetric  the
results of minimizing D(p� ˜p) and D(˜p�p) are different. In the main body of the paper we describe
methods which minimize D(p� ˜p) where ˜p comes from the t-exponential family. Algorithms which
minimize D(˜p�p) are described in the supplementary material.
Given an arbitrary probability distribution p(x)  in order to obtain a good approximation ˜p(x; θ) in
the t-exponential family  we minimize the relative t-relative entropy (19)

˜p = argmin

˜p

Dt(p� ˜p) =� q(x) logt p(x) − q(x) logt ˜p(x; θ)dx.

Here q(x) = 1

Z p(x)t denotes the escort of the original distribution p(x). Since

˜p(x; θ) = expt(�Φ(x)  θ� − gt(θ)) 

5

(24)

(25)

using the fact that ∇θgt(θ) = E˜q[Φ(x)]  one can take the derivative of (24) with respect to θ:

Eq[Φ(x)] = E˜q[Φ(x)].

(26)
In other words  the approximate distribution can be obtained by matching the escort expectation of
Φ(x) between the two distributions.
The escort expectation matching in (26) is reminiscent of the moment matching in the Power-EP [5]
or the Fractional BP [23] algorithm  where the approximate distribution is obtained by

E˜p[Φ(x)] = Epα ˜p1−α /Z[Φ(x)].

(27)
The main reason for using the t-divergence  however  is not to address the computational or conver-
gence issues as is done in the case of power EP/fractional BP. In contrast  we use the generalized
exponential family (t-exponential family) to build our approximate models.
In this context  the
t-divergence plays the same role as KL divergence in the exponential family.
To illustrate our ideas on a non-trivial problem  we apply escort expectation matching to the Bayes
Point Machine (BPM) [3] with a Student’s t-distribution prior.

6 Bayes Point Machine with Student’s t-Prior

Let D = {(x1  y1)  . . .   (xn  yn)} be the training data. Consider a linear model parametrized by the
k-dim weight vector w. For each training data point (xi  yi)  the conditional distribution of the label
yi given xi and w is modeled as [3]:

(28)
where Θ(z) is the step function: Θ(z) = 1 if z > 0 and = 0 otherwise. By making a standard i.i.d.
assumption about the data  the posterior distribution can be written as

ti(w) = p(yi | xi  w) = � + (1 − 2�)Θ(yi �w  xi�) 

p(w | D) ∝ p0(w)�i

ti(w) 

(29)

where p0(w) denotes a prior distribution. Instead of using multivariate Gaussian distribution as a
prior as was done by Minka [3]  we will use a Student’s t-prior  because we want to build robust
models:

p0(w) = St(w; 0  I  v).

(30)

As it turns out  the posterior p(w | D) is infeasible to obtain in practice. Therefore we will ﬁnd a
multivariate Student’s t-distribution to approximate the true posterior.
p(w | D) � ˜p(w) = St(w; ˜µ  ˜Σ  v).

(31)
In order to obtain such a distribution  we implement the Bayesian online learning method [24] 
which is also known as Assumed Density Filter [25]. The extension to the expectation propagation is
similar to [3] and omitted due to space limitation. The main idea is to process data points one by one
and update the posterior by using escort moment matching. Assume the approximate distribution
after processing (x1  y1)  . . .   (xi−1  yi−1) to be ˜pi−1(w) and deﬁne

Then the approximate posterior ˜pi(w) is updated as

˜p0(w) = p0(w)
pi(w) ∝ ˜pi−1(w)ti(w)

(32)
(33)

(35)

(36)

˜pi(w) = St(w; µ(i)  Σ(i)  v) = argmin

µ Σ

Dt(pi(w)�St(w; µ  Σ  v)).

(34)

Because ˜pi(w) is a k-dim Student’s t-distribution with degree of freedom v  for which Φ(w) =
[w  w w�] and t = 1 + 2/(v + k) (see example 5 in Appendix A)  it turns out that we only need

� qi(w) w d w =� ˜qi(w) w d w  and
� qi(w) w w� d w =� ˜qi(w) w w� d w .

6

Here ˜qi(w) ∝ ˜pi(w)t  qi(w) ∝ ˜pi−1(w)t˜ti(w) and

(37)
Denote ˜pi−1(w) = St(w; µ(i−1)  Σ(i−1)  v)  ˜qi−1(w) = St(w; µ(i−1)  v Σ(i−1) /(v + 2)  v + 2)
(also see example 5)  and we make use of the following relations:

˜ti(w) = ti(w)t = �t +�(1 − �)t − �t� Θ(yi �w  xi�).
Z1 =� ˜pi−1(w)˜ti(w)d w
= �t +�(1 − �)t − �t�� z
Z2 =� ˜qi−1(w)˜ti(w)d w
= �t +�(1 − �)t − �t�� z

St(x; 0  1  v)dx

−∞

−∞

St(x; 0  v/(v + 2)  v + 2)dx

g =

G =

1
Z2∇µZ1 = yiα xi
1
Z2∇ΣZ1 = −

1
2

yiα�xi  µ(i−1)�

x�i Σ(i−1) xi

xi x�i

((1 − �)t − �t) St(z; 0  1  v)

Z2�x�i Σ(i−1) xi

and z =

yi�xi  µ(i−1)�
�x�i Σ(i−1) xi

.

where 

α =

(38)

(39)

(40)

(41)

(42)

(43)

(44)

(45)

(46)

(47)

Equations (39) and (41) are analogous to Eq. (5.17) in [3]. By assuming that a regularity condition4

holds � and ∇ can be interchanged in ∇Z1 of (42) and (43). Combining with (38) and (40)  we

obtain the escort expectations of pi(w) from Z1 and Z2 (similar to Eq. (5.12) and (5.13) in [3]) 

Eq[w w�] − Eq[w] Eq[w]� =

Eq[w] =

1

1

Z2� ˜qi−1(w)˜ti(w) w d w = µ(i−1) + Σ(i−1) g
Z2� ˜qi−1(w)˜ti(w) w w� d w − Eq[w] Eq[w]�
= r Σ(i−1) − Σ(i−1)�g g� −2 G� Σ(i−1)

where r = Z1/Z2 and Eq[·] means the expectation with respect to qi(w).
Since the mean and variance of the escort of ˜pi(w) is µ(i) and Σ(i) (again see example 5)  after
combining with (42) and (43) 

µ(i) = Eq[w] = µ(i−1) + αyi Σ(i−1) xi

Σ(i) = Eq[w w�] − Eq[w] Eq[w]� = r Σ(i−1) −(Σ(i−1) xi)� αyi�xi  µ(i)�

x�i Σ(i−1) xi� (Σ(i−1) xi)�.

6.1 Results

In the above Bayesian online learning algorithm  everytime a new data xn coming in 
p(θ | x1  . . .   xn−1) is used as a prior  and the posterior is computed by incorporating the likeli-
hood p(xn | θ). The Student’s t-distribution is a more conservative or non-subjective prior than the
Gaussian distribution because its heavy-tailed nature. More speciﬁcally  it means that the Student’s
t-based BPM can be more strongly inﬂuenced by the newly coming in points.
In many binary classﬁcation problems  it is assumed that the underlying classﬁcation hyperplane
is always ﬁxed. However  in some real situations  this assumption might not hold. Especially  in

4This is a fairly standard technical requirement which is often proved using the Dominated Convergence

Theorem (see e.g. Section 9.2 of Rosenthal [26]).

7

w

f
o

s
n
g
i
S

.
f
f
i

D
#

100

80

60

40

20

0

0

Gauss
v=3
v=10

1 000

2 000

3 000

4 000

# Points

w

f
o

s
n
g
i
S

.
f
f
i

D
#

20

15

10

5

0

0

Gauss
v=3
v=10

1 000

2 000

3 000

4 000

# Points

Figure 3: The number of wrong signs between w. Left: case I; Right: case II

Table 1: The classiﬁcation error of all the data points

Case I
Case II

Gauss
0.337
0.150

v=3
0.242
0.130

v=10
0.254
0.128

an online learning problem  the data sequence coming in is time dependent. It is possible that the
underlying classiﬁer is also time dependent. For a senario like this  we require our learning machine
is able to self-adjust during the time given the data.
In our experiment  we build a synthetic online dataset which mimics the above senario  that is the
underlying classiﬁcation hyperplane is changed during a certain time interval. Our sequence of
data is composed of 4000 data points randomly generated by a 100 dimension isotropic Gaussian
distribution N (0  I). The sequence can be partitioned into 10 sub-sequences of length 400. During
(s) ∈ {−1  +1}100. Each point x(i) of the
each sub-sequence s  there is a base weight vector wb
subsequence is labeled as y(i) = sign�w�(i) x(i)� where w(i) = wb
(s) +n and n is a random noise
from [−0.1  +0.1]100. The base weight vector wb
(s) can be (I) totally randomly generated  or (II)
generated based on the base weight vector wb
(s−1) in the following way:
(s)j =�Rand{−1  +1}

j ∈ [400s − 399  400s]
otherwise.

(48)

wb

wb

(s−1)j

Namely  only 10% of the base weight vector is changed based upon the previous base weight vector.
We compare the Bayes Point Machine with Student’s t-prior (with v = 3 and v = 10) with the
Gaussian prior. For both method  � = 0.01. We report (1) for each point the number of different
signs between the base weight vector and the mean of the posterior (2) the error rate of all the points.
According to the Fig. 3 and Table. 1  we ﬁnd that the Bayes Point Machine with the Student’s-
t prior adjusts itself signiﬁcantly faster than the Gaussian prior and it also ends up with a better
classiﬁcation results. We believe that is mostly resulted from its heavy-tailness.

7 Discussion

In this paper  we investigated the convex duality of the log-partition function of the t-exponential
family  and deﬁned a new t-entropy. By using the t-divergence as a divergence measure  we pro-
posed approximate inference on the t-exponential family by matching the expectation of the escort
distributions. The results in this paper can be extended to the more generalized φ-exponential family
by Naudts [15].
The t-divergence based approximate inference is only applied in a toy example. The focus of our
future work is on utilizing this approach in various graphical models. Especially  it is important to
investigate a new family of graphical models based on heavy-tailed distributions for applications
involving noisy data.

8

References
[1] W. R. Gilks  S. Richardson  and D. J. Spiegelhalter. Markov Chain Monte Carlo in Practice. Chapman &

Hall  1995.

[2] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1 – 2):1 – 305  2008.

[3] T. Minka. Expectation Propagation for approximative Bayesian inference. PhD thesis  MIT Media Labs 

Cambridge  USA  2001.

[4] Y. Weiss. Comparing the mean ﬁeld method and belief propagation for approximate inference in MRFs.

In David Saad and Manfred Opper  editors  Advanced Mean Field Methods. MIT Press  2001.
[5] T. Minka. Divergence measures and message passing. Report 173  Microsoft Research  2005.
[6] C. Bishop  N. Lawrence  T. Jaakkola  and M. Jordan. Approximating posterior distributions in belief

networks using mixtures. In Advances in Neural Information Processing Systems 10  1997.

[7] G. Bouchard and O. Zoeter. Split variational inference. In Proc. Intl. Conf. Machine Learning  2009.
[8] P. Grunwald and A. Dawid. Game theory  maximum entropy  minimum discrepancy  and robust Bayesian

decision theory. Annals of Statistics  32(4):1367–1433  2004.

[9] C. R. Shalizi. Maximum likelihood estimation for q-exponential (tsallis) distributions  2007. URL http:

//arxiv.org/abs/math.ST/0701854.

[10] C. Tsallis. Possible generalization of boltzmann-gibbs statistics. J. Stat. Phys.  52:479–487  1988.
[11] J. Naudts. Deformed exponentials and logarithms in generalized thermostatistics. Physica A  316:323–

334  2002. URL http://arxiv.org/pdf/cond-mat/0203489.

[12] T. D. Sears. Generalized Maximum Entropy  Convexity  and Machine Learning. PhD thesis  Australian

National University  2008.

[13] A. Sousa and C. Tsallis. Student’s t- and r-distributions: Uniﬁed derivation from an entropic variational

principle. Physica A  236:52–57  1994.

[14] C. Tsallis  R. S. Mendes  and A. R. Plastino. The role of constraints within generalized nonextensive

statistics. Physica A: Statistical and Theoretical Physics  261:534–554  1998.

[15] J. Naudts. Generalized thermostatistics based on deformed exponential and logarithmic functions. Phys-

ica A  340:32–40  2004.

[16] J. Naudts. Generalized thermostatistics and mean-ﬁeld theory. Physica A  332:279–300  2004.
[17] J. Naudts. Estimators  escort proabilities  and φ-exponential families in statistical physics. Journal of

Inequalities in Pure and Applied Mathematics  5(4)  2004.

[18] N. Ding and S. V. N. Vishwanathan. t-logistic regression. In Richard Zemel  John Shawe-Taylor  John
Lafferty  Chris Williams  and Alan Culota  editors  Advances in Neural Information Processing Systems
23  2010.

[19] A. R´enyi. On measures of information and entropy. In Proc. 4th Berkeley Symposium on Mathematics 

Statistics and Probability  pages 547–561  1960.

[20] J. D. Lafferty. Additive models  boosting  and inference for generalized divergences. In Proc. Annual

Conf. Computational Learning Theory  volume 12  pages 125–133. ACM Press  New York  NY  1999.

[21] I. Csisz´ar. Information type measures of differences of probability distribution and indirect observations.

Studia Math. Hungarica  2:299–318  1967.

[22] K. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential
family of distributions. Machine Learning  43(3):211–246  2001. Special issue on Theoretical Advances
in On-line Learning  Game Theory and Boosting.

[23] W. Wiegerinck and T. Heskes. Fractional belief propagation. In S. Becker  S. Thrun  and K. Obermayer 

editors  Advances in Neural Information Processing Systems 15  pages 438–445  2003.

[24] M. Opper. A Bayesian approach to online learning.

363–378. Cambridge University Press  1998.

In On-line Learning in Neural Networks  pages

[25] X. Boyen and D. Koller. Tractable inference for complex stochastic processes. In UAI  1998.
[26] J. S. Rosenthal. A First Look at Rigorous Probability Theory. World Scientiﬁc Publishing  2006.

9

,Shouyuan Chen
Michael Lyu
Irwin King
Zenglin Xu
Pan Zhou
Xiaotong Yuan
Huan Xu
Shuicheng Yan
Jiashi Feng