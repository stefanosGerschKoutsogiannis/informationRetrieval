2019,NAT: Neural Architecture Transformer for Accurate and Compact Architectures,Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However  even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g.  convolution or pooling)  which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus  it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately  such a constrained optimization problem is NP-hard. To make the problem feasible  we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g.  skip connection or directly removing the connection). Based on MDP  we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies  we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets  i.e.  CIFAR-10 and ImageNet  demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.,NAT: Neural Architecture Transformer for Accurate

and Compact Architectures

Yong Guo∗  Yin Zheng∗  Mingkui Tan∗†  Qi Chen 

Jian Chen†  Peilin Zhao  Junzhou Huang

South China University of Technology  Weixin Group  Tencent 

Tencent AI Lab  University of Texas at Arlington

{guo.yong  sechenqi}@mail.scut.edu.cn  {mingkuitan  ellachen}@scut.edu.cn 

{yinzheng  masonzhao}@tencent.com  jzhuang@uta.edu

Abstract

Designing effective architectures is one of the key factors behind the success of
deep neural networks. Existing deep architectures are either manually designed
or automatically searched by some Neural Architecture Search (NAS) methods.
However  even a well-searched architecture may still contain many non-signiﬁcant
or redundant modules or operations (e.g.  convolution or pooling)  which may
not only incur substantial memory consumption and computation cost but also
deteriorate the performance. Thus  it is necessary to optimize the operations inside
an architecture to improve the performance without introducing extra computation
cost. Unfortunately  such a constrained optimization problem is NP-hard. To make
the problem feasible  we cast the optimization problem into a Markov decision
process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to
replace the redundant operations with the more computationally efﬁcient ones
(e.g.  skip connection or directly removing the connection). Based on MDP  we
learn NAT by exploiting reinforcement learning to obtain the optimization policies
w.r.t. different architectures. To verify the effectiveness of the proposed strategies 
we apply NAT on both hand-crafted architectures and NAS based architectures.
Extensive experiments on two benchmark datasets  i.e.  CIFAR-10 and ImageNet 
demonstrate that the transformed architecture by NAT signiﬁcantly outperforms
both its original form and those architectures optimized by existing methods.

1

Introduction

Deep neural networks (DNNs) [25] have been producing state-of-the-art results in many challenging
tasks including image classiﬁcation [12  23  42  57  58  18  11  53]  face recognition [38  43  56] 
brain signal processing [33  34]  video analysis [50  49] and many other areas [55  54  24  3  10  9  4].
One of the key factors behind the success lies in the innovation of neural architectures  such as
VGG [40] and ResNet[13]. However  designing effective neural architectures is often labor-intensive
and relies heavily on substantial human expertise. Moreover  the human-designed process cannot
fully explore the whole architecture space and thus the designed architectures may not be optimal.
Hence  there is a growing interest to replace the manual process of architecture design with Neural
Architecture Search (NAS).
Recently  substantial studies [29  35  61] have shown that the automatically discovered architectures
are able to achieve highly competitive performance compared to existing hand-crafted architectures.

∗Authors contributed equally.
†Corresponding author.
2This work is done when Yong Guo works as an intern in Tencent AI Lab.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Comparison between Neural Architecture Optimization (NAO) [31] and our Neural
Architecture Transformer (NAT). Green blocks denote the two input nodes of the cell and blue blocks
denote the intermediate nodes. Red blocks denote the connections that are changed by NAT. The
accuracy and the number of parameters are evaluated on CIFAR-10 models.

However  there are some limitations in NAS based architecture design methods. In fact  since there
is an extremely large search space [35  61] (e.g.  billions of candidate architectures)  these methods
often produce sub-optimal architectures  leading to limited representation performance or substantial
computation cost. Thus  even for a well-designed model  it is necessary yet important to optimize its
architecture (e.g.  removing the redundant operations) to achieve better performance and/or reduce
the computation cost.
To optimize the architectures  Luo et al. recently proposed a neural architecture optimization (NAO)
method [31]. Speciﬁcally  NAO ﬁrst encodes an architecture into an embedding in continuous space
and then conducts gradient descent to obtain a better embedding. After that  it uses a decoder to
map the embedding back to obtain an optimized architecture. However  NAO comes with its own
set of limitations. First  NAO often produces a totally different architecture from the input one
and may introduce extra parameters or additional computation cost. Second  similar to the NAS
based methods  NAO has a huge search space  which  however  may not be necessary for the task
of architecture optimization and may make the optimization problem very expensive to solve. An
illustrative comparison between our method and NAO can be found in Figure 1.
Unlike existing methods that design neural architectures  we seek to design an architecture optimiza-
tion method  called Neural Architecture Transformer (NAT)  to optimize neural architectures. Since
the optimization problem is non-trivial to solve  we cast it into a Markov decision process (MDP).
Thus  the architecture optimization process is reduced to a series of decision making problems. Based
on MDP  we seek to replace the expensive operations or redundant modules in the architecture
with more computationally efﬁcient ones. Speciﬁcally  NAT shall remove the redundant modules
or replace these modules with skip connections. In this way  the search space can be signiﬁcantly
reduced. Thus  the training complexity to learn an architecture optimizer is smaller than those NAS
based methods  e.g.  NAO. Last  it is worth mentioning that our NAT model can be used as a general
architecture optimizer which takes any architecture as the input and output an optimized one. In
experiments  we apply NAT to both hand-crafted and NAS based architectures and demonstrate the
performance on two benchmark datasets  namely CIFAR-10 [22] and ImageNet [8].
The main contributions of this paper are summarized as follows.
• We propose a novel architecture optimization method  called Neural Architecture Transformer
(NAT)  to optimize arbitrary architectures in order to achieve better performance and/or reduce
computation cost. To this end  NAT either removes the redundant paths or replaces the original
operation with skip connection to improve the architecture design.

• We cast the architecture optimization problem into a Markov decision process (MDP)  in which we
seek to solve a series of decision making problems to optimize the operations. We then solve the
MDP problem with policy gradient. To better exploit the adjacency information of operations in
an architecture  we propose to exploit graph convolution network (GCN) to build the architecture
optimization model.

• Extensive experiments demonstrate the effectiveness of our NAT on both hand-crafted and NAS
based architectures. Speciﬁcally  for hand-crafted models (e.g.  VGG)  our NAT automatically
introduces additional skip connections into the plain network and results in 2.75% improvement in
terms of Top-1 accuracy on ImageNet. For NAS based models (e.g.  DARTS [29])  NAT reduces
30% parameters and achieves 1.31% improvement in terms of Top-1 accuracy on ImageNet.

2

(a)

(b)

(c)

Figure 2: An example of the graph representation of a residual block and the diagram of operation
transformations. (a) a residual block [13]; (b) a graph view of residual block; (c) transformations
among three kinds of operations. N denotes a null operation without any computation  S denotes a
skip connection  and O denotes some computational modules other than null and skip connections.

2 Related Work

Hand-crafted architecture design. Many studies focus on architecture design and propose a
series of deep neural architectures  such as Network-in-network [27]  VGG [40] and so on. Unlike
these plain networks that only contain a stack of convolutions  He et al. propose the residual
network (ResNet) [13] by introducing residual shortcuts between different layers. However  the
human-designed process often requires substantial human effort and cannot fully explore the whole
architecture space  making the hand-crafted architectures often not optimal.
Neural architecture search. Recently  neural architecture search (NAS) methods have been proposed
to automate the process of architecture design [61  62  35  1  59  29  2  45  41]. Some researchers
conduct architecture search by modeling the architecture as a graph [51  20]. Unlike these methods 
DSO-NAS [52] ﬁnds the optimal architectures by starting from a fully connected block and then
imposing sparse regularization [17  44] to prune useless connections. Besides  Jin et al. propose a
Bayesian optimization approach [19] to morph the deep architectures by inserting additional layers 
adding more ﬁlters or introducing additional skip connections. More recently  Luo et al. propose the
neural architecture optimization (NAO) [31] method to perform the architecture search on continuous
space by exploiting the encoding-decoding technique. However  NAO is essentially designed for
architecture search and often obtains very different architectures from the input architectures and
may introduce extra parameters. Unlike these methods  our method is able to optimize architectures
without introducing extra computation cost (See the detailed comparison in Figure 1).
Architecture adaptation and model compression. Several methods [48  7  6] have been proposed
to obtain compact architectures by learning the optimal settings of each convolution  including kernel
size  stride and the number of ﬁlters. To obtain compact models  model compression methods [26 
15  30  60] detect and remove the redundant channels from the original models. However  these
methods only change the settings of convolution but ignore the fact that adjusting the connections
in the architecture could be more critical. Recently  Cao et al. propose an automatic architecture
compression method [5]. However  this method has to learn a compressed model for each given
pre-trained model and thus has limited generalization ability to different architectures. Unlike these
methods  we seek to learn a general optimizer for any arbitrary architecture.

3 Neural Architecture Transformer

3.1 Problem Deﬁnition

Given an architecture space Ω  we can represent an architecture α as a directed acyclic graph (DAG) 
i.e.  α = (V E)  where V is a set of nodes that denote the feature maps in DNNs and E is an edge
set [61  35  29]  as shown in Figure 2. Here  the directed edge eij ∈ E denotes some operation (e.g. 
convolution or max pooling) that transforms the feature map from node vi to vj. For convenience 
we divide the edges in E into three categories  namely  S  N  O  as shown in Figure 2(c). Here  S
denotes the skip connection  N denotes the null connection (i.e.  no edge between two nodes)  and
O denotes the operations other than skip connection or null connection (e.g.  convolution or max

3

conv 5x5conv 3x3skip connection201conv 5x5conv 3x3skip connection-1outNOSpooling). Note that different operations have different costs. Speciﬁcally  let c(·) be a function to
evaluate the computation cost. Obviously  we have c(O) > c(S) > c(N ).
In this paper  we seek to design an architecture optimization method  called Neural Architecture
Transformer (NAT)  to optimize any given architecture into a better one with the improved perfor-
mance and/or less computation cost. To achieve this  an intuitive way is to make the original operation
with less computation cost  e.g.  using the skip connection to replace convolution or using the null
connection to replace skip connection. Although the skip connection has slightly higher cost than
the null connection  it often can signiﬁcantly improve the performance [13  14]. Thus  we enable
the transition from null connection to skip connection to increase the representation ability of deep
networks. In summary  we constrain the possible transitions among O  S  and N in Figure 2(c) in
order to reduce the computation cost.
Note that the architecture optimization on an entire network is still very computationally expensive.
Moreover  we hope to learn a general architecture optimizer. Given these two concerns  we consider
learning a computational cell as the building block of the ﬁnal architecture. To build a cell  we follow
the same settings as that in ENAS [35]. Speciﬁcally  each cell has two input nodes  i.e.  v−2 and
v−1  which denote the outputs of the second nearest and the nearest cell in front of the current one 
respectively. Each intermediate node (marked as the blue box in Figure 1) also takes two previous
nodes in this cell as inputs. Last  based on the learned cell  we are able to form any ﬁnal network.

3.2 Markov Decision Process for Architecture Optimization

In this paper  we seek to learn a general architecture optimizer α = NAT(β; θ)  which transforms
any β into an optimized α and is parameterized by θ. Here  we assume β follows some distribution
p(·)  e.g.  multivariate uniformly discrete distribution. Let wα and wβ be the well-learned model
parameters of architectures α and β  respectively. We measure the performance of α and β by some
metric R(α  wα) and R(β  wβ)  e.g.  the accuracy on validation data. For convenience  we deﬁne the
performance improvement between α and β by R(α|β) = R(α  wα) − R(β  wβ).
To learn a good transformer α = NAT(β; θ) to optimize arbitrary β  we can maximize the expectation
of performance improvement R(α|β) over the distribution of β under a constraint of computation
cost c(α) ≤ κ  where c(α) measures the cost of α and κ is an upper bound of the cost. Then  the
optimization problem can be written as

Eβ∼p(·) [R (α|β)]   s.t. c(α) ≤ κ.

max

θ

(1)

(2)

max

θ

Unfortunately  it is non-trivial to directly obtain the optimal α given different β. Nevertheless 
following [61  35]  given any architecture β  we instead sample α from some well learned policy 
denoted by π(·|β; θ)  namely α ∼ π(·|β; θ). In other words  NAT ﬁrst learns the policy and then
conducts sampling from it to obtain the optimized architecture. In this sense  the parameters to be
learned only exist in π(·|β; θ). To learn the policy  we solve the following optimization problem:

(cid:2)Eα∼π(·|β;θ) R (α|β)(cid:3)   s.t. c(α) ≤ κ  α ∼ π(·|β; θ) 

Eβ∼p(·)

(cid:2)Eα∼π(·|β;θ) R (α|β)(cid:3) remains a question.

where Eβ∼p(·) [·] and Eα∼π(·|β;θ) [·] denote the expectation operation over β and α  respectively.
This problem  however  is still very challenging to solve. First  the computation cost of deep
networks can be evaluated by many metrics  such as the number of multiply-adds (MAdds)  latency 
and energy consumption  making it hard to ﬁnd a comprehensive measure to accurately evaluate
the cost. Second  the upper bound of computation cost κ in Eqn. (1) may vary for different cases
and thereby is hard to determine. Even if there already exists a speciﬁc upper bound  dealing with
the constrained optimization problem is still a typical NP-hard problem. Third  how to compute
Eβ∼p(·)
To address the above challenges  we cast the optimization problem into an architecture transformation
problem and reformulate it as a Markov decision process (MDP). Speciﬁcally  we optimize archi-
tectures by making a series of decisions to alternate the types of different operations. Following the
transition graph in Figure 2(c)  as c(O) > c(S) > c(N )  we can naturally obtain more compact archi-
tectures than the given ones. In this sense  we can achieve the goal to optimize arbitrary architecture
without introducing extra cost into the architecture. Thus  for the ﬁrst two challenges  we do not have
to evaluate the cost c(α) or determine the upper bound κ. For the third challenge  we estimate the
expectation value by sampling architectures from p(·) and π(·|β; θ) (See details in Section 3.4).

4

MDP formulation details. A typical MDP [39] is deﬁned by a tuple (S A  P  R  q  γ)  where S is a
ﬁnite set of states  A is a ﬁnite set of actions  P : S × A × S → R is the state transition distribution 
R : S ×A → R is the reward function  q : S → [0  1] is the distribution of initial state  and γ ∈ [0  1]
is a discount factor. Here  we deﬁne an architecture as a state  a transformation mapping β → α as
an action. Here  we use the accuracy improvement on the validation set as the reward. Since the
problem is a one-step MDP  we can omit the discount factor γ. Based on the problem deﬁnition  we
transform any β into an optimized architecture α with the policy π(·|β; θ). Then  the main challenge
becomes how to learn an optimal policy π(·|β; θ). Here  we exploit reinforcement learning [46] to
solve the problem and propose an efﬁcient policy learning algorithm.
Search space of NAT over a cell structure. For a cell structure with B nodes and 3 states for each
edge  there are 2(B−3) edges and the size of the search space w.r.t. a speciﬁc β is |Ωβ| = 32(B−3).
However  NAS methods [35  61] have a large search space with the size of k2(B−3)((B − 2)!)2 
where k is the number of candidate operations (e.g.  k=5 in ENAS [35] and k=8 in DARTS [29]).

3.3 Policy Learning by Graph Convolutional Neural Networks
To learn the optimal policy π(·|β; θ) w.r.t. an arbitrary architecture β  we propose an effective
learning method to optimize the operations inside the architecture. Speciﬁcally  we take an arbitrary
architecture graph β as the input and output the optimization policy w.r.t β. Such a policy is used to
optimize the operations of the given architecture. Since the choice of operation on an edge depends
on the adjacent nodes and edges  we have to consider the attributes of both the current edge and
its neighbors. For this reason  we employ a graph convolution networks (GCN) [21] to exploit
the adjacency information of the operations in the architecture. Here  an architecture graph can be
represented by a data pair (A  X)  where A denotes the adjacency matrix of the graph and X denotes
the attributes of the nodes together with their two input edges3. We consider a two-layer GCN and
formulate the model as:

Z = f (X  A) = Softmax

Aσ

 

(3)

(cid:16)

(cid:16)

AXW(0)(cid:17)

W(1)WFC(cid:17)

where W(0) and W(1) denote the weights of two graph convolution layers  WFC denotes the weight
of the fully-connected layer  σ is a non-linear activation function (e.g.  the Rectiﬁed Linear Unit
(ReLU) [32])  and Z refers to the probability distribution of different candidate operations on the
edges  i.e.  the learned policy π(·|β; θ). For convenience  we denote θ = {W(0)  W(1)  WFC} as the
parameters of the architecture transformer. To cover all possible architectures  we randomly sample
architectures from the whole architecture space and use them to train our model.
Differences with LSTM. The architecture graph can also be processed by the long short-term
memory (LSTM) [16]  which is a common practice in NAS methods [31  61  35]. In these methods 
LSTM ﬁrst treats the graph as a sequence of tokens and then learns the information from the sequence.
However  turning a graph into a sequence of tokens may lose some connectivity information of
the graph  leading to limited performance. On the contrary  our GCN model can better exploit the
information from the graph and yield superior performance (See results in Section 4.4).

3.4 Training and Inference of NAT

We apply the policy gradient [46] to train our model. The overall scheme is shown in Algorithm 1 
which employs an alternating manner. Speciﬁcally  in each training epoch  we ﬁrst train the model
parameters w with ﬁxed transformer parameters θ. Then  we train the transformer parameters θ by
ﬁxing the model parameters w.
Training the model parameters w. Given any θ  we need to update the model parameters w
based on the training data. Here  to accelerate the training process  we adopt the parameter sharing
technique [35]  i.e.  we construct a large computational graph  where each subgraph represents a
neural network architecture  hence forcing all architectures to share the parameters. Thus  we can use
the shared parameters w to represent the parameters for different architectures. For any architecture
β ∼ p(·)  let L(β  w) be the loss function on the training data  e.g.  the cross-entropy loss. Then 
given any m sampled architectures  the updating rule for w with parameter sharing can be given by
w ← w − η 1

(cid:80)m
i=1 ∇wL(βi  w)  where η is the learning rate.

m

3Due to the page limit  we put the detailed representation methods in the supplementary.

5

for each iteration on training data do

// Fix θ and update w.
Sample βi ∼ p(·) to construct a batch {βi}m
Update the model parameters w by descending the gradient:

(cid:80)m
i=1 ∇wL(βi  w).

i=1.

m

end for
for each iteration on validation data do

w ← w − η 1

1: Initiate w and θ.
2: while not convergent do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
end for
15:
16: end while

θ ← θ +η 1

mn

Algorithm 1 Training method for Neural Architecture Transformer (NAT).
Require: The number of sampled input architectures in an iteration m  the number of sampled optimized
architectures for each input architecture n  learning rate η  regularizer parameter λ in Eqn. (4)  input
architecture distribution p(·)  shared model parameters w  transformer parameters θ.

// Fix w and update θ.
Sample βi ∼ p(·) to construct a batch {βi}m
Obtain {αj}n
Update the transformer parameters θ by ascending the gradient:

j=1 according to the policy learned by GCN.

i=1.

(cid:2)∇θ log π(αj|βi; θ)(cid:0)R(αj  w) − R(βi  w)(cid:1)+λ∇θH(cid:0)π (·|βi; θ)(cid:1)(cid:3).

(cid:80)m

(cid:80)n

i=1

j=1

Training the transformer parameters θ. We train the transformer model with policy gradient [46].
To encourage exploration  we introduce an entropy regularization term into the objective to prevent
the transformer from converging to a local optimum too quickly [62]  e.g.  selecting the “original”
option for all the operations. Given the shared parameters w  the objective can be formulated as

(cid:2)Eα∼π(·|β;θ) [R (α  w) − R (β  w)] + λH(cid:0)π(·|β; θ)(cid:1)(cid:3)
(cid:34)(cid:88)
π(α|β; θ)(cid:0)R (α  w) − R (β  w)(cid:1) + λH(cid:0)π(·|β; θ)(cid:1)(cid:35)

J(θ) = Eβ∼p(·)

(4)

.

(cid:88)

β

=

p(β)

α

i=1

j=1

n(cid:88)

m(cid:88)

∇θJ(θ) ≈ 1
mn

where p(β) is the probability to sample some architecture β from the distribution p(·)  π(α|β; θ)
is the probability to sample some architecture α from the distribution π(·|β; θ)  H(·) evaluates the
entropy of the policy  and λ controls the strength of the entropy regularization term. For each input
architecture  we sample n optimized architectures {αj}n
j=1 from the distribution π(·|β; θ) in each
iteration. Thus  the gradient of Eqn. (4) w.r.t. θ becomes4

(cid:2)∇θ log π(αj|βi; θ)(cid:0)R(αj  w) − R(βi  w)(cid:1) + λ∇θH(cid:0)π(·|βi; θ)(cid:1)(cid:3) .

The regularization term H(cid:0)π(·|βi; θ)(cid:1) encourages the distribution π(·|β; θ) to have high entropy 

i.e.  high diversity in the decisions on the edges. Thus  the decisions for some operations would be
encouraged to choose the “identity” or “null” operations during training. As a result  NAT is able to
sufﬁciently explore the whole search space to ﬁnd the optimal architecture.
Inferring the optimized architecture. We do not explicitly obtain the optimized architecture via
α = NAT(β; ). Instead  we conduct sampling according to the learned probability distribution.
Speciﬁcally  we ﬁrst sample several candidate optimized architectures from the learned policy
π(·|β; θ) and then select the architecture with the highest validation accuracy. Note that we can also
obtain the optimized architecture by selecting the operation with the maximum probability  which 
however  tends to reach a local optimum and yields worse results than the sampling based method
(See comparisons in Section 4.4).

(5)

4 Experiments

In this section  we apply NAT on both hand-crafted and NAS based architectures  and conduct
experiments on two image classiﬁcation benchmark datasets  i.e.  CIFAR-10 [22] and ImageNet [8].
All implementations are based on PyTorch.5

4We put the derivations of Eqn. (5) in the supplementary.
5The source code of NAT is available at https://github.com/guoyongcs/NAT.

6

Table 1: Performance comparisons of the optimized architectures obtained by different methods based
on hand-crafted architectures. “/” denotes the original models that are not changed by architecture
optimization methods.

Model

Method

CIFAR-10
#Params (M)

#MAdds (M) Acc. (%)

Model

Method

ImageNet
#Params (M)

#MAdds (M)

VGG16

ResNet20

ResNet56

MobileNetV2

/

/

/

/

NAO[31]

NAT

NAO [31]

NAT

NAO [31]

NAT

NAO [31]

NAT

15.2
19.5
15.2
0.3
0.4
0.3
0.9
1.3
0.9
2.3
2.9
2.3

313
548
315
41
61
42
127
199
129
91
131
92

93.56
95.72
96.04
91.37
92.44
92.95
93.21
95.27
95.40
94.47
94.75
95.17

VGG16

ResNet18

ResNet50

MobileNetV2

/

/

/

/

NAO [31]

NAT

NAO [31]

NAT

NAO [31]

NAT

NAO [31]

NAT

138.4
147.7
138.4
11.7
17.9
11.7
25.6
34.8
25.6
3.4
4.5
3.4

15620
18896
15693
1580
2246
1588
3530
4505
3547
300
513
302

Acc. (%)

Top-1 Top-5
71.6
90.4
91.3
72.9
92.0
74.3
89.1
69.8
89.7
70.8
90.0
71.1
92.9
76.2
77.4
93.2
93.5
77.7
90.3
72.0
90.6
72.2
72.5
91.0

Figure 3: Architecture optimization results of hand-crafted architectures. We provide both the views
of graph (left) and network (right) to show the differences in architecture.

4.1

Implementation Details

We consider two kinds of cells in a deep network  including the normal cell and the reduction cell. The
normal cell preserves the same spatial size as inputs while the reduction cell reduces the spatial size
by 2×. Both the normal and reduction cells contain 2 input nodes and a number of intermediate nodes.
During training  we build the deep network by stacking 8 basic cells and train the transformer for 100
epochs. We set m = 1  n = 1  and λ = 0.003 in the training. We split CIFAR-10 training set into
40% and 60% slices to train the model parameters w and the transformer parameters θ  respectively.
As for the evaluation of the networks with different architectures  we replace the original cell with the
optimized one and train the model from scratch. Please see more details in the supplementary. For all
the considered architectures  we follow the same settings of the original papers. In the experiments 
we only apply cutout to the NAS based architectures on CIFAR-10.

4.2 Results on Hand-crafted Architectures

In this experiment  we apply NAT on three popular hand-crafted models  i.e.  VGG [40]  ResNet [13] 
and MobileNet [37]. To make all architectures share the same graph representation method deﬁned in
Section 3.2  we add null connections into the hand-crafted architectures to ensure that each node has

7

View of GraphView of NetworkVGG CellVGGNAT-VGG-2-1outResNetNAT-ResNetResidual Cell-2-1outArchitectureMobileNetV2 Cell-2-1outMobileNetV2NAT-MobileNetV2Table 2: Comparisons of the optimized architectures obtained by different methods based on NAS
based architectures. “-” denotes that the results are not reported. “/” denotes the original models that
are not changed by architecture optimization methods. † denotes the models trained with cutout.

Model

Method

CIFAR-10
#Params (M)

#MAdds (M) Acc. (%)

Model

Method

ImageNet
#Params (M)

#MAdds (M)

AmoebaNet† [36]

PNAS† [28]
SNAS† [47]
GHN† [51]
ENAS† [35]

DARTS† [29]

NAONet† [31]

/

/

/

/

NAO [31]

NAT

NAO [31]

NAT

NAO [31]

NAT

3.2
3.2
2.9
5.7
4.6
4.5
4.6
3.3
3.5
2.7
128
143
113

-
-
-
-
804
763
804
528
577
424
66016
73705
58326

96.73
96.67
97.08
97.22
97.11
97.05
97.24
97.06
97.09
97.28
97.89
97.91
98.01

AmoebaNet [36]

PNAS [28]
SNAS [47]
GHN [51]

ENAS [35]

DARTS [29]

NAONet [31]

/

/

/

/

NAO [31]

NAT

NAO [31]

NAT

NAO [31]

NAT

5.1
5.1
4.3
6.1
5.6
5.5
5.6
4.7
5.1
4.0
11.35
11.83
8.36

555
588
522
569
607
589
607
574
627
441
1360
1417
1025

Acc. (%)

Top-1 Top-5
74.5
92.0
91.9
74.2
90.8
72.7
91.3
73.0
91.7
73.8
91.7
73.7
91.8
73.9
73.1
91.0
91.1
73.3
91.4
73.7
91.8
74.3
92.0
74.5
74.8
92.3

Figure 4: Architecture optimization results on the architectures of NAS based architectures.

two input nodes (See examples in Figure 3). For a fair comparison  we build deep networks using the
original and optimized architectures while keeping the same depth and number of channels as the
original models. We compare NAT with a strong baseline method Neural Architecture Optimization
(NAO) [31]. We show the results in Table 1 and the corresponding architectures in Figure 3. From
Table 1  although the models with NAO yield better performance than the original ones  they often
have more parameters and higher computation cost. By contrast  our NAT based models consistently
outperform the original models by a large margin with approximately the same computation cost.

8

NAT-ENASENASNAT-DARTSDARTSNormal cellReduction cellArchitectureNAT-NAONetNAONetTable 3: Performance comparisons of the architectures obtained by different methods on CIFAR-10.
The reported accuracy (%) is the average performance of ﬁve runs with different random seeds. “/”
denotes the original models that are not changed by architecture optimization methods. † denotes the
models trained with cutout.

Method

/

Maximum-GCN

Sampling-GCN (Ours)

Random Search

LSTM

VGG16 ResNet20 MobileNetV2
93.56
93.17
94.45
94.37
95.93

94.47
94.38
95.01
94.87
95.13

91.37
91.56
92.19
92.57
92.97

ENAS† DARTS† NAONet†
97.89
97.11
96.31
96.58
97.93
97.05
97.90
96.92
97.21
97.99

97.06
95.17
97.05
97.00
97.26

4.3 Results on NAS Based Architectures

For the automatically searched architectures  we evaluate the proposed NAT on three state-of-the-art
NAS based architectures  i.e.  DARTS [29]  NAONet [31]  and ENAS [35]. Moreover  we also
compare our optimized architectures with other NAS based architectures  including AmoebaNet [36] 
PNAS [28]  SNAS [47] and GHN [51]. From Table 2  all the NAT based architectures yield higher
accuracy than their baseline models and the models optimized by NAO on CIFAR-10 and ImageNet.
Compared with other NAS based architectures  our NAT-DARTS performs the best on CIFAR-10
and achieves the competitive performance compared to the best architecture (i.e.  AmoebaNet)
on ImageNet with less computation cost and fewer number of parameters. We also visualize the
architectures of the original and optimized cell in Figure 4. As for DARTS and NAONet  NAT
replaces several redundant operations with the skip connections or directly removes the connection 
leading to fewer number of parameters. While optimizing ENAS  NAT removes the average pooling
operation and improves the performance without introducing extra computations.

4.4 Comparisons of Different Policy Learners

In this experiment  we compare the performance of different policy learners  including Random
Search  LSTM  and the GCN method. For the Random Search method  we perform random transitions
among O  S  and N on the input architectures. For the GCN method  we consider two variants which
infer the optimized architecture by sampling from the learned policy (denoted by Sampling-GCN)
or by selecting the operation with the maximum probability (denoted by Maximum-GCN). From
Table 3  our Sampling-GCN method outperforms all the considered policies on different architectures.
These results demonstrate the superiority of the proposed GCN method as the policy learner.

4.5 Effect of Different Graph Representations on Hand-crafted Architectures

In this experiment  we investigate the effect of different graph representations on hand-crafted
architectures. Note that an architecture may correspond to many different topological graphs 
especially for the hand-crafted architectures  e.g.  VGG and ResNet  where the number of nodes is
smaller than that of our basic cell. For convenience  we study three different graphs for VGG and
ResNet20  respectively. The average accuracy of NAT-VGG is 95.83% and outperforms the baseline
VGG with the accuracy of 93.56%. Similarly  our NAT-ResNet20 yields the average accuracy of
92.48%  which is also better than the original model. We put the architecture and the performance of
each possible representation in the supplementary. In practice  the graph representation may inﬂuence
the result of NAT and how to alleviate its effect still remains an open question.

5 Conclusion

In this paper  we have proposed a novel Neural Architecture Transformer (NAT) for the task of
architecture optimization. To solve this problem  we cast it into a Markov decision process (MDP)
by making a series of decisions to optimize existing operations with more computationally efﬁcient
operations  including skip connection and null operation. To show the effectiveness of NAT  we
apply it to both hand-crafted architectures and Neural Architecture Search (NAS) based architectures.
Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of the
proposed method in improving the accuracy and the compactness of neural architectures.

9

Acknowledgments

This work was partially supported by Guangdong Provincial Scientiﬁc and Technological Funds
under Grants 2018B010107001  National Natural Science Foundation of China (NSFC) (No.
61602185)  key project of NSFC (No. 61836003)  Fundamental Research Funds for the Central
Universities (No. D2191240)  Program for Guangdong Introducing Innovative and Enterpreneurial
Teams 2017ZT07X183  Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201902) 
Guangdong Special Branch Plans Young Talent with Scientiﬁc and Technological Innovation (No.
2016TQ03X445)  Guangzhou Science and Technology Planning Project (No. 201904010197)  and
Microsoft Research Asia (MSRA Collaborative Research Program). We last thank Tencent AI Lab.

References
[1] B. Baker  O. Gupta  N. Naik  and R. Raskar. Designing neural network architectures using reinforcement

learning. In International Conference on Learning Representations  2017.

[2] H. Cai  L. Zhu  and S. Han. ProxylessNAS: Direct neural architecture search on target task and hardware.

In International Conference on Learning Representations  2019.

[3] J. Cao  Y. Guo  Q. Wu  C. Shen  J. Huang  and M. Tan. Adversarial learning with local coordinate coding.

In International Conference on Machine Learning  pages 706–714  2018.

[4] J. Cao  L. Mo  Y. Zhang  K. Jia  C. Shen  and M. Tan. Multi-marginal wasserstein gan. In Advances in

Neural Information Processing Systems  2019.

[5] S. Cao  X. Wang  and K. M. Kitani. Learnable embedding space for efﬁcient neural architecture compres-

sion. In International Conference on Learning Representations  2019.

[6] T. Chen  I. Goodfellow  and J. Shlens. Net2net: Accelerating learning via knowledge transfer.

International Conference on Learning Representations  2016.

In

[7] X. Dai  P. Zhang  B. Wu  H. Yin  F. Sun  Y. Wang  M. Dukhan  Y. Hu  Y. Wu  Y. Jia  et al. Chamnet:
Towards efﬁcient network design through platform-aware model adaptation. In The IEEE Conference on
Computer Vision and Pattern Recognition  pages 11398–11407  2019.

[8] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In The IEEE Conference on Computer Vision and Pattern Recognition  pages 248–255  2009.

[9] Y. Guo  Q. Chen  J. Chen  J. Huang  Y. Xu  J. Cao  P. Zhao  and M. Tan. Dual reconstruction nets for

image super-resolution with gradient sensitive loss. arXiv preprint arXiv:1809.07099  2018.

[10] Y. Guo  Q. Chen  J. Chen  Q. Wu  Q. Shi  and M. Tan. Auto-embedding generative adversarial networks

for high resolution image synthesis. IEEE Transactions on Multimedia  2019.

[11] Y. Guo  M. Tan  Q. Wu  J. Chen  A. V. D. Hengel  and Q. Shi. The shallow end: Empowering shallower

deep-convolutional networks through auxiliary outputs. arXiv preprint arXiv:1611.01773  2016.

[12] Y. Guo  Q. Wu  C. Deng  J. Chen  and M. Tan. Double forward propagation for memorized batch

normalization. In AAAI Conference on Artiﬁcial Intelligence  pages 3134–3141  2018.

[13] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In The IEEE Conference

on Computer Vision and Pattern Recognition  pages 770–778  2016.

[14] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In The European

Conference on Computer Vision  pages 630–645  2016.

[15] Y. He  X. Zhang  and J. Sun. Channel pruning for accelerating very deep neural networks. In The IEEE

International Conference on Computer Vision  pages 1398–1406  2017.

[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):1735–1780  1997.

[17] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of

the European Conference on Computer Vision (ECCV)  pages 304–320  2018.

[18] Z. Jiang  Y. Zheng  H. Tan  B. Tang  and H. Zhou. Variational deep embedding: An unsupervised and
generative approach to clustering. In International Joint Conference on Artiﬁcial Intelligence  pages
1965–1972  2017.

10

[19] H. Jin  Q. Song  and X. Hu. Auto-keras: Efﬁcient neural architecture search with network morphism. arXiv

preprint arXiv:1806.10282  2018.

[20] W. Jin  K. Yang  R. Barzilay  and T. Jaakkola. Learning multimodal graph-to-graph translation for molecular

optimization. In International Conference on Learning Representations  2019.

[21] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

International Conference on Learning Representations  2016.

In

[22] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report 

Citeseer  2009.

[23] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems  pages 1097–1105  2012.

[24] S. Lauly  Y. Zheng  A. Allauzen  and H. Larochelle. Document neural autoregressive distribution estimation.

The Journal of Machine Learning Research  18(1):4046–4069  2017.

[25] Y. LeCun  B. Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel. Backpropa-

gation Applied to Handwritten zip Code Recognition. Neural Computation  1(4):541–551  1989.

[26] H. Li  A. Kadav  I. Durdanovic  H. Samet  and H. P. Graf. Pruning ﬁlters for efﬁcient convnets. In

International Conference on Learning Representations  2017.

[27] M. Lin  Q. Chen  and S. Yan. Network in network. In International Conference on Learning Representa-

tions  2014.

[28] C. Liu  B. Zoph  M. Neumann  J. Shlens  W. Hua  L.-J. Li  L. Fei-Fei  A. Yuille  J. Huang  and K. Murphy.
Progressive neural architecture search. In The European Conference on Computer Vision  pages 19–34 
2018.

[29] H. Liu  K. Simonyan  and Y. Yang. Darts: Differentiable architecture search. In International Conference

on Learning Representations  2019.

[30] J.-H. Luo  J. Wu  and W. Lin. Thinet: A ﬁlter level pruning method for deep neural network compression.

In The IEEE International Conference on Computer Vision  pages 5058–5066  2017.

[31] R. Luo  F. Tian  T. Qin  E. Chen  and T.-Y. Liu. Neural architecture optimization. In Advances in Neural

Information Processing Systems  pages 7816–7827  2018.

[32] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In International

Conference on Machine Learning  pages 807–814  2010.

[33] C. S. Nam  A. Nijholt  and F. Lotte. Brain–computer interfaces handbook: technological and theoretical

advances. CRC Press  2018.

[34] J. Pan  Y. Li  and J. Wang. An eeg-based brain-computer interface for emotion recognition. In 2016

international joint conference on neural networks (IJCNN)  pages 2063–2067. IEEE  2016.

[35] H. Pham  M. Guan  B. Zoph  Q. Le  and J. Dean. Efﬁcient neural architecture search via parameter sharing.

In International Conference on Machine Learning  pages 4095–4104  2018.

[36] E. Real  A. Aggarwal  Y. Huang  and Q. V. Le. Regularized evolution for image classiﬁer architecture

search. In AAAI Conference on Artiﬁcial Intelligence  volume 33  pages 4780–4789  2019.

[37] M. Sandler  A. Howard  M. Zhu  A. Zhmoginov  and L.-C. Chen. Mobilenetv2: Inverted residuals and linear
bottlenecks. In The IEEE Conference on Computer Vision and Pattern Recognition  pages 4510–4520 
2018.

[38] F. Schroff  D. Kalenichenko  and J. Philbin. Facenet: A Uniﬁed Embedding for Face Recognition and
Clustering. In The IEEE Conference on Computer Vision and Pattern Recognition  pages 815–823  2015.

[39] J. Schulman  S. Levine  P. Abbeel  M. Jordan  and P. Moritz. Trust region policy optimization.

International Conference on Machine Learning  pages 1889–1897  2015.

In

[40] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

International Conference on Learning Representations  2015.

[41] D. R. So  C. Liang  and Q. V. Le. The evolved transformer. In International Conference on Machine

Learning  2019.

11

[42] R. K. Srivastava  K. Greff  and J. Schmidhuber. Training Very Deep Networks. In Advances in Neural

Information Processing Systems  pages 2377–2385  2015.

[43] Y. Sun  X. Wang  and X. Tang. Deeply Learned Face Representations are Sparse  Selective  and Robust. In

The IEEE Conference on Computer Vision and Pattern Recognition  pages 2892–2900  2015.

[44] M. Tan  I. W. Tsang  and L. Wang. Towards ultrahigh dimensional feature selection for big data. The

Journal of Machine Learning Research  15(1):1371–1429  2014.

[45] A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  Ł. Kaiser  and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems  pages 5998–6008  2017.

[46] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

Machine Learning  8(3-4):229–256  1992.

[47] S. Xie  H. Zheng  C. Liu  and L. Lin. Snas: Stochastic neural architecture search. In International

Conference on Learning Representations  2019.

[48] T.-J. Yang  A. Howard  B. Chen  X. Zhang  A. Go  M. Sandler  V. Sze  and H. Adam. Netadapt: Platform-
aware neural network adaptation for mobile applications. In The European Conference on Computer Vision 
pages 285–300  2018.

[49] R. Zeng  C. Gan  P. Chen  W. Huang  Q. Wu  and M. Tan. Breaking winner-takes-all: Iterative-winners-out
networks for weakly supervised temporal action localization. IEEE Transactions on Image Processing 
28(12):5797–5808  2019.

[50] R. Zeng  W. Huang  M. Tan  Y. Rong  P. Zhao  J. Huang  and C. Gan. Graph convolutional networks for

temporal action localization. In The IEEE International Conference on Computer Vision  Oct 2019.

[51] C. Zhang  M. Ren  and R. Urtasun. Graph hypernetworks for neural architecture search. In International

Conference on Learning Representations  2019.

[52] X. Zhang  Z. Huang  and N. Wang. You only search once: Single shot neural architecture search via direct

sparse optimization. arXiv preprint arXiv:1811.01567  2018.

[53] Y. Zhang  H. Chen  Y. Wei  P. Zhao  J. Cao  X. Fan  X. Lou  H. Liu  J. Hou  X. Han  et al. From
whole slide imaging to microscopy: Deep microscopy adaptation network for histopathology cancer
image classiﬁcation. In International Conference on Medical Image Computing and Computer-Assisted
Intervention  pages 360–368. Springer  2019.

[54] Y. Zheng  C. Liu  B. Tang  and H. Zhou. Neural autoregressive collaborative ﬁltering for implicit feedback.
In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems  pages 2–6. ACM  2016.

[55] Y. Zheng  B. Tang  W. Ding  and H. Zhou. A neural autoregressive approach to collaborative ﬁltering. In

International Conference on Machine Learning  pages 764–773  2016.

[56] Y. Zheng  R. S. Zemel  Y.-J. Zhang  and H. Larochelle. A neural autoregressive approach to attention-based

recognition. International Journal of Computer Vision  113(1):67–79  2015.

[57] Y. Zheng  Y.-J. Zhang  and H. Larochelle. Topic modeling of multimodal data: An autoregressive approach.

In The IEEE Conference on Computer Vision and Pattern Recognition  pages 1370–1377  2014.

[58] Y. Zheng  Y.-J. Zhang  and H. Larochelle. A deep and autoregressive approach for topic modeling of
multimodal data. IEEE Transactions on Pattern Analysis and Machine Intelligence  38(6):1056–1069 
2015.

[59] Z. Zhong  J. Yan  W. Wu  J. Shao  and C.-L. Liu. Practical block-wise neural network architecture
generation. In The IEEE Conference on Computer Vision and Pattern Recognition  pages 2423–2432 
2018.

[60] Z. Zhuang  M. Tan  B. Zhuang  J. Liu  Y. Guo  Q. Wu  J. Huang  and J. Zhu. Discrimination-aware channel
pruning for deep neural networks. In Advances in Neural Information Processing Systems  pages 875–886 
2018.

[61] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International Conference

on Learning Representations  2017.

[62] B. Zoph  V. Vasudevan  J. Shlens  and Q. V. Le. Learning transferable architectures for scalable image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition  pages 8697–8710 
2018.

12

,Christof Seiler
Simon Rubinstein-Salzedo
Susan Holmes
Yong Guo
Yin Zheng
Mingkui Tan
Qi Chen
Jian Chen
Peilin Zhao
Junzhou Huang