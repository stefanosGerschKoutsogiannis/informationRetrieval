2010,Multi-View Active Learning in the Non-Realizable Case,The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption  however  rarely holds in practice. In this paper  we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that  with unbounded Tsybakov noise  the sample complexity of multi-view active learning can be $\widetilde{O}(\log \frac{1}{\epsilon})$  contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$  where the order of $1/\epsilon$ is independent of the parameter in Tsybakov noise  contrasting to previous polynomial bounds where the order of $1/\epsilon$ is related to the parameter in Tsybakov noise.,Multi-View Active Learning in

the Non-Realizable Case

Wei Wang and Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology

Nanjing University  Nanjing 210093  China
{wangw zhouzh}@lamda.nju.edu.cn

Abstract

The sample complexity of active learning under the realizability assumption has
been well-studied. The realizability assumption  however  rarely holds in prac-
tice. In this paper  we theoretically characterize the sample complexity of active
learning in the non-realizable case under multi-view setting. We prove that  with
unbounded Tsybakov noise  the sample complexity of multi-view active learning
ǫ )  contrasting to single-view setting where the polynomial improve-
ment is the best possible achievement. We also prove that in general multi-view
setting the sample complexity of active learning with unbounded Tsybakov noise
ǫ )  where the order of 1/ǫ is independent of the parameter in Tsybakov noise 
contrasting to previous polynomial bounds where the order of 1/ǫ is related to the
parameter in Tsybakov noise.

can be eO(log 1
is eO( 1

1

Introduction

In active learning [10  13  16]  the learner draws unlabeled data from the unknown distribution
deﬁned on the learning task and actively queries some labels from an oracle. In this way  the active
learner can achieve good performance with much fewer labels than passive learning. The number
of these queried labels  which is necessary and sufﬁcient for obtaining a good leaner  is well-known
as the sample complexity of active learning.

Many theoretical bounds on the sample complexity of active learning have been derived based on the
realizability assumption (i.e.  there exists a hypothesis perfectly separating the data in the hypothesis
class) [4  5  11  12  14  16]. The realizability assumption  however  rarely holds in practice. Recently 
the sample complexity of active learning in the non-realizable case (i.e.  the data cannot be perfectly
separated by any hypothesis in the hypothesis class because of the noise) has been studied [2  13  17].
It is worth noting that these bounds obtained in the non-realizable case match the lower bound Ω( η2
ǫ2 )
[19]  in the same order as the upper bound O( 1
ǫ2 ) of passive learning (η denotes the generalization
error rate of the optimal classiﬁer in the hypothesis class and ǫ bounds how close to the optimal
classiﬁer in the hypothesis class the active learner has to get). This suggests that perhaps active
learning in the non-realizable case is not as efﬁcient as that in the realizable case. To improve the
sample complexity of active learning in the non-realizable case remarkably  the model of the noise
or some assumptions on the hypothesis class and the data distribution must be considered. Tsybakov
noise model [21] is more and more popular in theoretical analysis on the sample complexity of active
learning. However  existing result [8] shows that obtaining exponential improvement in the sample
complexity of active learning with unbounded Tsybakov noise is hard.

Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity
of active learning in the realizable case remarkably  we have an insight that multi-view setting will
also help active learning in the non-realizable case. In this paper  we present the ﬁrst analysis on the

1

sample complexity of active learning in the non-realizable case under multi-view setting  where the
non-realizability is caused by Tsybakov noise. Speciﬁcally:

-We deﬁne α-expansion  which extends the deﬁnition in [3] and [23] to the non-realizable case 

and β-condition for multi-view setting.

setting can be improved to eO(log 1

-We prove that the sample complexity of active learning with Tsybakov noise under multi-view
ǫ ) when the learner satisﬁes non-degradation condition.1 This
exponential improvement holds no matter whether Tsybakov noise is bounded or not  contrasting to
single-view setting where the polynomial improvement is the best possible achievement for active
learning with unbounded Tsybakov noise.

tive learning with unbounded Tsybakov noise under multi-view setting is eO( 1
1/ǫ is independent of the parameter in Tsybakov noise  i.e.  the sample complexity is always eO( 1

-We also prove that  when non-degradation condition does not hold  the sample complexity of ac-
ǫ )  where the order of
ǫ )
no matter how large the unbounded Tsybakov noise is. While in previous polynomial bounds  the
order of 1/ǫ is related to the parameter in Tsybakov noise and is larger than 1 when unbounded Tsy-
bakov noise is larger than some degree (see Section 2). This discloses that  when non-degradation
condition does not hold  multi-view setting is still able to lead to a faster convergence rate and our
polynomial improvement in the sample complexity is better than previous polynomial bounds when
unbounded Tsybakov noise is large.

The rest of this paper is organized as follows. After introducing related work in Section 2 and
preliminaries in Section 3  we deﬁne α-expansion in the non-realizable case in Section 4. We analyze
the sample complexity of active learning with Tsybakov noise under multi-view setting with and
without the non-degradation condition in Section 5 and Section 6  respectively. Finally we conclude
the paper in Section 7.

2 Related Work

Generally  the non-realizability of learning task is caused by the presence of noise. For learning the
task with arbitrary forms of noise  Balcan et al. [2] proposed the agnostic active learning algorithm
ǫ2 ).2 Hoping to get tighter bound on the sample
complexity of the algorithm A2  Hanneke [17] deﬁned the disagreement coefﬁcient θ  which depends
on the hypothesis class and the data distribution  and proved that the sample complexity of the
ǫ2 ). Later  Dasgupta et al. [13] developed a general agnostic active learning

A2 and proved that its sample complexity is bO( η2
algorithm A2 is bO(θ2 η2
algorithm which extends the scheme in [10] and proved that its sample complexity is bO(θ η2

Recently  the popular Tsybakov noise model [21] was considered in theoretical analysis on ac-
tive learning and there have been some bounds on the sample complexity. For some simple cases 
where Tsybakov noise is bounded  it has been proved that the exponential improvement in the sam-
ple complexity is possible [4  7  18]. As for the situation where Tsybakov noise is unbounded 
only polynomial improvement in the sample complexity has been obtained. Balcan et al. [4] as-
sumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample

ǫ2 ).

µω

Tsybakov noise). This uniform distribution assumption  however  rarely holds in practice. Castro
and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov

on the H¨older smoothness and d is the dimension of the data). This result is also based on the
strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of examples
are generated according to a simple linear noise model and indicated that the sample complexity

1+λ(cid:1) (λ > 0 depends on
(cid:1) (µ > 1 depends on another form of Tsybakov noise  ω ≥ 1 depends

complexity of active learning with unbounded Tsybakov noise is O(cid:0)ǫ− 2
noise is bO(cid:0)ǫ− 2µω+d−2ω−1
of active learning with unbounded Tsybakov noise is O(cid:0)ǫ−
(1+λ)(2+λ)(cid:1). Hanneke [18] proved that
1+λ(cid:1) for active learning with unbounded Tsybakov noise. For active learning with unbounded
bO(cid:0)ǫ− 2
1The eO notation is used to hide the factor log log( 1
2The bO notation is used to hide the factor polylog( 1

Tsybakov noise  Castro and Nowak [8] also proved that at least Ω(ǫ−ρ) labels are requested to learn

the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity

2(3+λ)

ǫ ).
ǫ ).

2

an ǫ-approximation of the optimal classiﬁer (ρ ∈ (0  2) depends on Tsybakov noise). This result
shows that the polynomial improvement is the best possible achievement for active learning with un-
bounded Tsybakov noise in single-view setting. Wang [22] introduced smooth assumption to active
learning with approximate Tsybakov noise and proved that if the classiﬁcation boundary and the
underlying distribution are smooth to ξ-th order and ξ > d  the sample complexity of active learning

is bO(cid:0)ǫ− 2d
active learning is O(cid:0)polylog( 1

ξ+d(cid:1); if the boundary and the distribution are inﬁnitely smooth  the sample complexity of
ǫ )(cid:1). Nevertheless  this result is for approximate Tsybakov noise and

the assumption on large smoothness order (or inﬁnite smoothness order) rarely holds for data with
high dimension d in practice.

3 Preliminaries

In multi-view setting  the instances are described with several different disjoint sets of features. For
the sake of simplicity  we only consider two-view setting in this paper. Suppose that X = X1 × X2
is the instance space  X1 and X2 are the two views  Y = {0  1} is the label space and D is the
distribution over X×Y . Suppose that c = (c1  c2) is the optimal Bayes classiﬁer  where c1 and c2 are
the optimal Bayes classiﬁers in the two views  respectively. Let H1 and H2 be the hypothesis class
in each view and suppose that c1 ∈ H1 and c2 ∈ H2. For any instance x = (x1  x2)  the hypothesis
hv ∈ Hv (v = 1  2) makes that hv(xv) = 1 if xv ∈ Sv and hv(xv) = 0 otherwise  where Sv is a
subset of Xv. In this way  any hypothesis hv ∈ Hv corresponds to a subset Sv of Xv (as for how to
combine the hypotheses in the two views  see Section 5). Considering that x1 and x2 denote the same
instance x in different views  we overload Sv to denote the instance set {x = (x1  x2) : xv ∈ Sv}
without confusion. Let S ∗
v correspond to the optimal Bayes classiﬁer cv. It is well-known [15] that
v = {xv : ϕv(xv) ≥ 1
2 }  where ϕv(xv) = P (y = 1|xv). Here  we also overload S ∗
S ∗
v to denote
the instances set {x = (x1  x2) : xv ∈ S ∗
v }. The error rate of a hypothesis Sv under the distribution
v ) 6= 0 and the excess
v ) is a

D is R(hv) = R(Sv) = P r(x1 x2 y)∈D(cid:0)y 6= I(xv ∈ Sv)(cid:1). In general  R(S ∗

v − Sv) and d(Sv  S ∗

v = (Sv − S ∗

error of Sv can be denoted as follows  where Sv∆S ∗
pseudo-distance between the sets Sv and S ∗
v .

v ) ∪ (S ∗

R(Sv) − R(S ∗

|2ϕv(xv) − 1|pxv dxv

  d(Sv  S ∗
v )

(1)

v ) =ZSv∆S ∗

v

Let ηv denote the error rate of the optimal Bayes classiﬁer cv which is also called as the noise rate
in the non-realizable case. In general  ηv is less than 1
2 . In order to model the noise  we assume that
the data distribution and the Bayes decision boundary in each view satisﬁes the popular Tsybakov
noise condition [21] that P rxv ∈Xv (|ϕv(xv) − 1/2| ≤ t) ≤ C0tλ for some ﬁnite C0 > 0  λ > 0
and all 0 < t ≤ 1/2  where λ = ∞ corresponds to the best learning situation and the noise is called
bounded [8]; while λ = 0 corresponds to the worst situation. When λ < ∞  the noise is called
unbounded [8]. According to Proposition 1 in [21]  it is easy to know that (2) holds.

d(Sv  S ∗

v ) ≥ C1dk

∆(Sv  S ∗
v )

(2)

λ   C1 = 2C −1/λ

Here k = 1+λ
v ) + P r(S ∗
v − Sv) is also
a pseudo-distance between the sets Sv and S ∗
v ) ≤ 1. We will use the
following lamma [1] which gives the standard sample complexity for non-realizable learning task.

v ) = P r(Sv − S ∗
v ) ≤ d∆(Sv  S ∗

λ(λ + 1)−1−1/λ  d∆(Sv  S ∗

v   and d(Sv  S ∗

0

Lemma 1 Suppose that H is a set of functions from X to Y = {0  1} with ﬁnite VC-dimension
V ≥ 1 and D is the ﬁxed but unknown distribution over X × Y . For any ǫ  δ > 0  there is a
positive constant C  such that if the size of sample {(x1  y1)  . . .   (xN   yN )} from D is N (ǫ  δ) =
C

δ )(cid:1)  then with probability at least 1 − δ  for all h ∈ H  the following holds.

ǫ2(cid:0)V + log( 1

|

1

N XN

I(cid:0)h(xi) 6= yi(cid:1) − E(x y)∈D I(cid:0)h(x) 6= y(cid:1)| ≤ ǫ

i=1

4 α-Expansion in the Non-realizable Case

Multi-view active learning ﬁrst described in [20] focuses on the contention points (i.e.  unlabeled
instances on which different views predict different labels) and queries some labels of them. It is
motivated by that querying the labels of contention points may help at least one of the two views
to learn the optimal classiﬁer. Let S1 ⊕ S2 = (S1 − S2) ∪ (S2 − S1) denote the contention points

3

Table 1: Multi-view active learning with the non-degradation condition
Input: Unlabeled data set U = {x1  x2  · · ·   } where each example xj is given as a pair (xj
Process:

Query the labels of m0 instances drawn randomly from U to compose the labeled data set L
iterate: i = 0  1  · · ·   s
Train the classiﬁer hi

v (v = 1  2) by minimizing the empirical risk with L in each view:

1  xj
2)

hi

v = arg minh∈HvP(x1 x2 y)∈L I(h(xv) 6= y);

1 and hi

Apply hi
Query the labels of mi+1 instances drawn randomly from Qi  then add them into L and delete them
from U .

2 to the unlabeled data set U and ﬁnd out the contention point set Qi;

end iterate
Output: hs

+ and hs

−

between S1 and S2  then P r(S1 ⊕ S2) denotes the probability mass on the contentions points. “∆”
and “⊕” mean the same operation rule. In this paper  we use “∆” when referring the excess error
between Sv and S ∗
v and use “⊕” when referring the difference between the two views S1 and S2. In
order to study multi-view active learning  the properties of contention points should be considered.
One basic property is that P r(S1 ⊕ S2) should not be too small  otherwise the two views could be
exactly the same and two-view setting would degenerate into single-view setting.

1 = S ∗

2 = S ∗. As for the situation where S ∗

In multi-view learning  the two views represent the same learning task and generally are consistent
with each other  i.e.  for any instance x = (x1  x2) the labels of x in the two views are the same.
Hence we ﬁrst assume that S ∗
2   we will discuss on it
further in Section 5.2. The instances agreed by the two views can be denoted as (S1 ∩S2)∪(S1 ∩S2).
However  some of these agreed instances may be predicted different label by the optimal classiﬁer
S ∗  i.e.  the instances in (S1 ∩ S2 − S ∗) ∪ (S1 ∩ S2 − S ∗). Intuitively  if the contention points
can convey some information about (S1 ∩ S2 − S ∗) ∪ (S1 ∩ S2 − S ∗)  then querying the labels of
contention points could help to improve S1 and S2. Based on this intuition and that P r(S1 ⊕ S2)
should not be too small  we give our deﬁnition on α-expansion in the non-realizable case.

1 6= S ∗

Deﬁnition 1 D is α-expanding if for some α > 0 and any S1 ⊆ X1  S2 ⊆ X2  (3) holds.

P r(cid:0)S1 ⊕ S2(cid:1) ≥ α(cid:16)P r(cid:0)S1 ∩ S2 − S ∗(cid:1) + P r(cid:0)S1 ∩ S2 − S ∗(cid:1)(cid:17)

(3)

We say that D is α-expanding with respect to hypothesis class H1 × H2 if the above holds for all
S1 ∈ H1 ∩ X1  S2 ∈ H2 ∩ X2 (here we denote by Hv ∩ Xv the set {h ∩ Xv : h ∈ Hv} for v = 1  2).

Balcan et al. [3] also gave a deﬁnition of expansion  P r(T1 ⊕ T2) ≥ α min(cid:2)P r(T1 ∩ T2)  P r(T1 ∩
T2)(cid:3)  for realizable learning task under the assumptions that the learner in each view is never “conﬁ-

dent but wrong” and the learning algorithm is able to learn from positive data only. Here Tv denotes
the instances which are classiﬁed as positive conﬁdently in each view. Generally  in realizable learn-
ing tasks  we aim at studying the asymptotic performance and assume that the performance of initial
classiﬁer is better than guessing randomly  i.e.  P r(Tv) > 1/2. This ensures that P r(T1 ∩ T2) is
larger than P r(T1 ∩ T2). In addition  in [3] the instances which are agreed by the two views but are
predicted different label by the optimal classiﬁer can be denoted as T1 ∩ T2. So  it can be found that
Deﬁnition 1 and the deﬁnition of expansion in [3] are based on the same intuition that the amount of
contention points is no less than a fraction of the amount of instances which are agreed by the two
views but are predicted different label by the optimal classiﬁers.

5 Multi-view Active Learning with Non-degradation Condition

In this section  we ﬁrst consider the multi-view learning in Table 1 and analyze whether multi-
view setting can help improve the sample complexity of active learning in the non-realizable case
remarkably. In multi-view setting  the classiﬁers are often combined to make predictions and many
strategies can be used to combine them. In this paper  we consider the following two combination
schemes  h+ and h−  for binary classiﬁcation:

+(x) =(cid:26) 1 if hi

hi

0 otherwise

1(x1) = hi

2(x2) = 1

−(x) =(cid:26) 0 if hi

hi

1 otherwise

1(x1) = hi

2(x2) = 0

(4)

4

5.1 The Situation Where S ∗

1 = S ∗
2

With (4)  the error rate of the combined classiﬁers hi

+ and hi

− satisfy (5) and (6)  respectively.

R(hi
R(hi

+) − R(S ∗) = R(Si
−) − R(S ∗) = R(Si

1 ∩ Si
1 ∪ Si

2) − R(S ∗) ≤ d∆(Si
2) − R(S ∗) ≤ d∆(Si

1 ∩ Si
1 ∪ Si

2  S ∗)
2  S ∗)

(5)

(6)

v ⊂ Xv (v = 1  2) corresponds to the classiﬁer hi

Here Si
v ∈ Hv in the i-th round. In each round of
multi-view active learning  labels of some contention points are queried to augment the training data
set L and the classiﬁer in each view is then reﬁned. As discussed in [23]  we also assume that the
learner in Table 1 satisﬁes the non-degradation condition as the amount of labeled training examples
increases  i.e.  (7) holds  which implies that the excess error of Si+1
v in
the region of Si

is no larger than that of Si

v

1 ⊕ Si
2.

P r(cid:0)Si+1

v ∆S ∗(cid:12)(cid:12)Si

1 ⊕ Si

2(cid:1) ≤ P r(Si

v∆S ∗(cid:12)(cid:12)Si

1 ⊕ Si
2)

(7)

1   . . .   πv

To illustrate the non-degradation condition  we give the following example: Suppose the data in
Xv (v = 1  2) fall into n different clusters  denoted by πv
n  and every cluster has the same
probability mass for simplicity. The positive class is the union of some clusters while the negative
class is the union of the others. Each positive (negative) cluster πv
ξ in Xv is associated with only
(ξ  ς ∈ {1  . . .   n}) in X3−v (i.e.  given an instance xv in πv
3 positive (negative) clusters π3−v
ξ  
x3−v will only be in one of these π3−v
). Suppose the learning algorithm will predict all instances
in each cluster with the same label  i.e.  the hypothesis class Hv consists of the hypotheses which
do not split any cluster. Thus  the cluster πv
ξ can be classiﬁed according to the posterior probability
P (y = 1|πv
ξ will not inﬂuence the estimation of
the posterior probability for cluster πv
ς (ς 6= ξ). It is evident that the non-degradation condition holds
in this task. Note that the non-degradation assumption may not always hold  and we will discuss on
this in Section 6. Now we give Theorem 1.

ξ ) and querying the labels of instances in cluster πv

ς

ς

C2

1 (cid:0)V + log( 16(s+1)

Theorem 1 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to Deﬁnition 1  when the non-degradation condition holds  if s = ⌈ 2 log 1
⌉ and mi =
log 1
C2
256kC

+
−  at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least

)(cid:1)  the multi-view active learning in Table 1 will generate two classiﬁers hs

and hs
1 − δ.
Here  V = max[V C(H1)  V C(H2)] where V C(H) denotes the VC-dimension of the hypothesis
class H  k = 1+λ

8ǫ

δ

0

λ(λ + 1)−1−1/λ and C2 = 5α+8
6α+8 .

λ   C1 = 2C −1/λ
1 ⊕ Si
v ∩ Qi and τi+1 = P r(T i+1
v = Si+1
P r(T i+1
2 − S ∗) + P r(Si

Proof sketch. Let Qi = Si
Qi) ≤ 1/8. Let T i+1
d∆(Si

1 ∩ Si

⊕T i+1

1

1

2  ﬁrst with Lemma 1 and (2) we have d∆(Si+1

−S ∗)

1 ∩ Si+1

| Qi  S ∗ |
2 . Considering (7) and
2
⊕T i+1
)
1 ∩ Si
2 − S ∗)  then we calculate that

− 1

2

2

As in each round some contention points are queried and added into the training set  the difference
between the two views is decreasing  i.e.  P r(Si+1
2). Let
γi = P r(Si
2   with Deﬁnition 1 and different combinations of τi+1 and γi  we can
 S ∗)

) is no larger than P r(Si

1 ⊕ Si+1

1 ⊕ Si

P r(Si
1

1⊕Si

 S ∗)

2

have either
C2 = 5α+8
Thus  with (5) and (6) we have either R(hs

2 S ∗) ≤ 5α+8

6α+8 or

6α+8 is a constant less than 1  we have either d∆(Ss

d∆(Si+1
d∆(Si
1

1

∪Si+1
2
∪Si

2 S ∗) ≤ 5α+8

+) ≤ R(S ∗) + ǫ or R(hs

6α+8 . When s = ⌈ 2 log 1
log 1
C2
2  S ∗) ≤ ǫ or d∆(Ss
1 ∪ Ss
2  S ∗) ≤ ǫ.
(cid:3)
−) ≤ R(S ∗) + ǫ.

⌉  where

8ǫ

2−S ∗)
2) − 1
⊕Si
∩Si+1
2
∩Si

d∆(Si+1
d∆(Si
1

1

1 ∩ Ss

≤ P r(Si

1 ∩ Si
d∆(Si+1

2

  S ∗)

2|Qi  S ∗|Qi)P r(Qi) = P r(Si
1 ∩ Si+1
1 ∩ Si
1 ∪ Si+1
1 ∩ Si

2 − S ∗) + P r(Si

2 − S ∗) + P r(Si

1 ∩ Si

1 ∩ Si

  S ∗)

2

d∆(Si+1

≤ P r(Si

2 − S ∗) +

P r(Si

1 ⊕ Si

1 ⊕ Si+1

2

2) − τi+1P r(cid:0)(Si+1
2) + τi+1P r(cid:0)(Si+1

) ∩ Qi(cid:1)
) ∩ Qi(cid:1).

2 − S ∗) +

P r(Si

1 ⊕ Si

1 ⊕ Si+1

2

1
8

1
8

5

From Theorem 1 we know that we only need to request Ps

ǫ ) labels to learn hs
+
and hs
−  at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least
1 − δ. If we choose hs
+) ≤ R(S ∗) + ǫ  we can get a classiﬁer whose
error rate is no larger than R(S ∗) + ǫ. Fortunately  there are only two classiﬁers and the probability
of getting the right classiﬁer is no less than 1
−  we give
Deﬁnition 2 at ﬁrst.

i=0 mi = eO(log 1

2 . To study how to choose between hs

+ and it happens to satisfy R(hs

+ and hs

Deﬁnition 2 The multi-view classiﬁers S1 and S2 satisfy β-condition if (8) holds for some β > 0.

P r(cid:0){x : x ∈ S1 ⊕ S2 ∧ y(x) = 1}(cid:1)

P r(S1 ⊕ S2)

P r(cid:0){x : x ∈ S1 ⊕ S2 ∧ y(x) = 0}(cid:1)

P r(S1 ⊕ S2)

−

(cid:12)(cid:12)(cid:12) ≥ β

(8)

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

(8) implies the difference between the examples belonging to positive class and that belonging to
negative class in the contention region of S1 ⊕ S2. Based on Deﬁnition 2  we give Lemma 2 which
provides information for deciding how to choose between h+ and h−. This helps to get Theorem 2.
2 log( 4
δ )

Lemma 2 If the multi-view classiﬁers Ss

2 satisfy β-condition  with the number of

1 and Ss

labels we can decide correctly whether P r(cid:0){x : x ∈ Ss

2 ∧ y(x) = 0}(cid:1)) is smaller with probability at least 1 − δ.

Ss
1 ⊕ Ss

1 ⊕ Ss

2 ∧ y(x) = 1}(cid:1) or P r(cid:0){x : x ∈

β2

classiﬁer whose error rate is no larger than R(S ∗) + ǫ with probability at least 1 − δ.

Theorem 2 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to Deﬁnition 1  when the non-degradation condition holds  if the multi-view classiﬁers satisfy
ǫ ) labels the multi-view active learning in Table 1 will generate a

β-condition  by requesting eO(log 1
From Theorem 2 we know that we only need to request eO(log 1

ǫ ) labels to learn a classiﬁer with
error rate no larger than R(S ∗) + ǫ with probability at least 1 − δ. Thus  we achieve an exponential
improvement in sample complexity of active learning in the non-realizable case under multi-view
setting. Sometimes  the difference between the examples belonging to positive class and that be-
longing to negative class in Ss

2 may be very small  i.e.  (9) holds.

1 ⊕ Ss

P r(cid:0){x : x ∈ Ss

1 ⊕ Ss

P r(Ss

1 ⊕ Ss
2)

2 ∧ y(x) = 1}(cid:1)

P r(cid:0){x : x ∈ Ss

−

1 ⊕ Ss

2 ∧ y(x) = 0}(cid:1)

P r(Ss

1 ⊕ Ss
2)

If so  we need not to estimate whether R(hs
both hs

+ and hs

− are good approximations of the optimal classiﬁer.

+) or R(hs

−) is smaller and Theorem 3 indicates that

(cid:12)(cid:12)(cid:12) = O(ǫ)

(9)

Theorem 3 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to Deﬁnition 1  when the non-degradation condition holds  if (9) is satisﬁed  by request-
+ and
+) ≤ R(S ∗) + ǫ and

ǫ ) labels the multi-view active learning in Table 1 will generate two classiﬁers hs

− which satisfy either (a) or (b) with probability at least 1 − δ. (a) R(hs
hs
R(hs

ing eO(log 1

−) ≤ R(S ∗) + O(ǫ); (b) R(hs

+) ≤ R(S ∗) + O(ǫ) and R(hs

−) ≤ R(S ∗) + ǫ.

The complete proof of Theorem 1  and the proofs of Lemma 2  Theorem 2 and Theorem 3 are given
in the supplementary ﬁle.

5.2 The Situation Where S ∗

1 6= S ∗
2

Although the two views represent the same learning task and generally are consistent with each
other  sometimes S ∗
2 . Therefore  the α-expansion assumption in Deﬁnition
1 should be adjusted to the situation where S ∗
2 . To analyze this theoretically  we replace S ∗
by S ∗

2 in Deﬁnition 1 and get (10). Similarly to Theorem 1  we get Theorem 4.

1 may be not equal to S ∗

1 6= S ∗

1 ∩ S ∗

P r(cid:0)S1 ⊕ S2(cid:1) ≥ α(cid:16)P r(cid:0)S1 ∩ S2 − S ∗

1 ∩ S ∗

2(cid:1) + P r(cid:0)S1 ∩ S2 − S ∗

1 ∩ S ∗

2(cid:1)(cid:17)

(10)

Theorem 4 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to (10)  when the non-degradation condition holds  if s = ⌈ 2 log 1
log 1
C2
log( 16(s+1)
least one of which is with error rate no larger than R(S ∗
(V   k  C1 and C2 are given in Theorem 1.)

)(cid:1)  the multi-view active learning in Table 1 will generate two classiﬁers hs

−  at
2 ) + ǫ with probability at least 1 − δ.

⌉ and mi = 256kC
C2

1 (cid:0)V +

+ and hs

1 ∩ S ∗

8ǫ

δ

6

Input: Unlabeled data set U = {x1  x2  · · ·   } where each example xj is given as a pair (xj
Process:

Table 2: Multi-view active learning without the non-degradation condition
1  xj
2)
Query the labels of m0 instances drawn randomly from U to compose the labeled data set L;
Train the classiﬁer h0

v (v = 1  2) by minimizing the empirical risk with L in each view:

h0

v = arg minh∈HvP(x1 x2 y)∈L I(h(xv) 6= y);

iterate: i = 1  · · ·   s

2

1

and hi−1

to the unlabeled data set U and ﬁnd out the contention point set Qi;

Apply hi−1
Query the labels of mi instances drawn randomly from Qi  then add them into L and delete them
from U ;
Query the labels of (2i − 1)mi instances drawn randomly from U − Qi  then add them into L and
delete them from U ;
Train the classiﬁer hi

v by minimizing the empirical risk with L in each view:

hi

v = arg minh∈HvP(x1 x2 y)∈L I(h(xv) 6= y).

end iterate
Output: hs

+ and hs

−

v is the optimal Bayes classiﬁer in the v-th view  obviously  R(S ∗
v )  (v = 1  2). So  learning a classiﬁer with error rate no larger than R(S ∗

Proof. Since S ∗
than R(S ∗
harder than learning a classiﬁer with error rate no larger than R(S ∗
a classiﬁer with error rate no larger than R(S ∗
R(Si
1 ∩ S ∗
no larger than R(S ∗
error rate is less than R(S ∗
the discussion of Section 5.1  with the proof of Theorem 1 we get Theorem 4 proved.

2 ) is no less
2 ) + ǫ is not
v ) + ǫ. Now we aim at learning
2 ) + ǫ. Without loss of generality  we assume
2 )  we get a classiﬁer with error rate
2 ) + ǫ. Thus  we can neglect the probability mass on the hypothesis whose
2 in
(cid:3)

2 ) for i = 0  1  . . .   s. If R(Si
1 ∩ S ∗

2 as the optimal. Replacing S ∗ by S ∗

2 ) and regard S ∗

1 ∩ S ∗
1 ∩ S ∗

v) ≤ R(S ∗

v) > R(S ∗

1 ∩ S ∗

1 ∩ S ∗

1 ∩ S ∗

1 ∩ S ∗

1 ∩ S ∗

Theorem 4 shows that for the situation where S ∗
two classiﬁers hs
with probability at least 1 − δ. With Lemma 2  we get Theorem 5 from Theorem 4.

2   by requesting eO(log 1

−  at least one of which is with error rate no larger than R(S ∗

+ and hs

ǫ ) labels we can learn
2 ) + ǫ

1 6= S ∗

1 ∩ S ∗

Theorem 5 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to (10)  when the non-degradation condition holds  if the multi-view classiﬁers satisfy β-
ǫ ) labels the multi-view active learning in Table 1 will generate a

condition  by requesting eO(log 1

classiﬁer whose error rate is no larger than R(S ∗

1 ∩ S ∗

2 ) + ǫ with probability at least 1 − δ.

Generally  R(S ∗
2   i.e.  P r(S ∗
S ∗
in the sample complexity of active learning with Tsybakov noise is still possible.

1 is not too much different from
2 ) ≤ ǫ/2  we have Corollary 1 which indicates that the exponential improvement

2 ) is larger than R(S ∗

2 ). When S ∗

1 ) and R(S ∗

1 ∩ S ∗

1 ⊕S ∗

Corollary 1 For data distribution D α-expanding with respect to hypothesis class H1 × H2 ac-
cording to (10)  when the non-degradation condition holds  if the multi-view classiﬁers satisfy β-
condition and P r(S ∗
ǫ ) labels the multi-view active learning in
Table 1 will generate a classiﬁer with error rate no larger than R(S ∗
v )+ǫ (v = 1  2) with probability
at least 1 − δ.

2 ) ≤ ǫ/2  by requesting eO(log 1

1 ⊕ S ∗

The proofs of Theorem 5 and Corollary 1 are given in the supplemental ﬁle.

6 Multi-view Active Learning without Non-degradation Condition

Section 5 considers situations when the non-degradation condition holds  there are cases  however 
the non-degradation condition (7) does not hold. In this section we focus on the multi-view active
learning in Table 2 and give an analysis with the non-degradation condition waived. Firstly  we give
Theorem 6 for the sample complexity of multi-view active learning in Table 2 when S ∗
2 = S ∗.

1 = S ∗

Theorem 6 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to Deﬁnition 1  if s = ⌈ 2 log 1
log 1
C2
learning in Table 2 will generate two classiﬁers hs
−  at least one of which is with error rate
no larger than R(S ∗) + ǫ with probability at least 1 − δ. (V   k  C1 and C2 are given in Theorem 1.)

)(cid:1)  the multi-view active

1 (cid:0)V + log( 16(s+1)

⌉ and mi = 256kC
C2

+ and hs

8ǫ

δ

7

In the (i + 1)-th round  we randomly query (2i+1 − 1)mi labels from Qi and add
Proof sketch.
them into L. So the number of training examples for Si+1
(v = 1  2) is larger than the number of
whole training examples for Si
v. Thus we know that d(Si+1
|Qi  S ∗|Qi) ≤ d(Si
v|Qi  S ∗|Qi) holds
for any ϕv. Setting ϕv ∈ {0  1}  the non-degradation condition (7) stands. Thus  with the proof of
(cid:3)
Theorem 1 we get Theorem 6 proved.

v

v

Theorem 6 shows that we can requestPs

+ and hs
− 
at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least 1 − δ. To
guarantee the non-degradation condition (7)  we only need to query (2i − 1)mi more labels in the
i-th round. With Lemma 2  we get Theorem 7.

i=0 2imi = eO( 1

ǫ ) labels to learn two classiﬁers hs

Theorem 7 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ǫ ) labels the
multi-view active learning in Table 2 will generate a classiﬁer whose error rate is no larger than
R(S ∗) + ǫ with probability at least 1 − δ.

ing to Deﬁnition 1  if the multi-view classiﬁers satisfy β-condition  by requesting eO( 1
Theorem 7 shows that  without the non-degradation condition  we need to request eO( 1

ǫ ) labels to
learn a classiﬁer with error rate no larger than R(S ∗) + ǫ with probability at least 1 − δ. The order of
1/ǫ is independent of the parameter in Tsybakov noise. Similarly to Theorem 3  we get Theorem 8
which indicates that both hs

− are good approximations of the optimal classiﬁer.

+ and hs

Theorem 8 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ǫ ) labels the multi-view active learning in Table
− which satisfy either (a) or (b) with probability at least
−) ≤ R(S ∗) + O(ǫ); (b) R(hs
+) ≤ R(S ∗) + O(ǫ) and

ing to Deﬁnition 1  if (9) holds  by requesting eO( 1

+ and hs
+) ≤ R(S ∗) + ǫ and R(hs

2 will generate two classiﬁers hs
1 − δ. (a) R(hs
R(hs

−) ≤ R(S ∗) + ǫ.

As for the situation where S ∗
and Corollary 2.

1 6= S ∗

2   similarly to Theorem 5 and Corollary 1  we have Theorem 9

Theorem 9 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ǫ ) labels the multi-view
2 )+ǫ

ing to (10)  if the multi-view classiﬁers satisfy β-condition  by requesting eO( 1

active learning in Table 2 will generate a classiﬁer whose error rate is no larger than R(S ∗
with probability at least 1 − δ.

1 ∩S ∗

Corollary 2 For data distribution D α-expanding with respect to hypothesis class H1 × H2 accord-
ing to (10)  if the multi-view classiﬁers satisfy β-condition and P r(S ∗
2 ) ≤ ǫ/2  by requesting
ǫ ) labels the multi-view active learning in Table 2 will generate a classiﬁer with error rate no

1 ⊕ S ∗

eO( 1

larger than R(S ∗

v ) + ǫ (v = 1  2) with probability at least 1 − δ.

The complete proof of Theorem 6  the proofs of Theorem 7 to 9 and Corollary 2 are given in the
supplementary ﬁle.

7 Conclusion

bakov noise can be improved to eO(log 1
prove that the sample complexity of active learning with unbounded Tsybakov noise is eO( 1

We present the ﬁrst study on active learning in the non-realizable case under multi-view setting in
this paper. We prove that the sample complexity of multi-view active learning with unbounded Tsy-
ǫ )  contrasting to single-view setting where only polynomial
improvement is proved possible with the same noise condition. In general multi-view setting  we
ǫ )  where
the order of 1/ǫ is independent of the parameter in Tsybakov noise  contrasting to previous polyno-
mial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. Generally  the
non-realizability of learning task can be caused by many kinds of noise  e.g.  misclassiﬁcation noise
and malicious noise. It would be interesting to extend our work to more general noise model.

Acknowledgments

This work was supported by the NSFC (60635030  60721002)  973 Program (2010CB327903) and
JiangsuSF (BK2008018).

8

References

[1] M. Anthony and P. L. Bartlett  editors. Neural Network Learning: Theoretical Foundations.

Cambridge University Press  Cambridge  UK  1999.

[2] M.-F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. In ICML  pages

65–72  2006.

[3] M.-F. Balcan  A. Blum  and K. Yang. Co-training and expansion: Towards bridging theory and

practice. In NIPS 17  pages 89–96. 2005.

[4] M.-F. Balcan  A. Z. Broder  and T. Zhang. Margin based active learning.

In COLT  pages

35–50  2007.

[5] M.-F. Balcan  S. Hanneke  and J. Wortman. The true sample complexity of active learning. In

COLT  pages 45–56  2008.

[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT 

pages 92–100  1998.

[7] R. M. Castro and R. D. Nowak. Upper and lower error bounds for active learning. In Allerton

Conference  pages 225–234  2006.

[8] R. M. Castro and R. D. Nowak. Minimax bounds for active learning. IEEE Transactions on

Information Theory  54(5):2339–2353  2008.

[9] G. Cavallanti  N. Cesa-Bianchi  and C. Gentile. Linear classiﬁcation and selective sampling

under low noise conditions. In NIPS 21  pages 249–256. 2009.

[10] D. A. Cohn  L. E. Atlas  and R. E. Ladner.

Improving generalization with active learning.

Machine Learning  15(2):201–221  1994.

[11] S. Dasgupta. Analysis of a greedy active learning strategy. In NIPS 17  pages 337–344. 2005.

[12] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS 18  pages 235–

242. 2006.

[13] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In

NIPS 20  pages 353–360. 2008.

[14] S. Dasgupta  A. T. Kalai  and C. Monteleoni. Analysis of perceptron-based active learning. In

COLT  pages 249–263  2005.

[15] L. Devroye  L. Gy¨orﬁ  and G. Lugosi  editors. A Probabilistic Theory of Pattern Recognition.

Springer  New York  1996.

[16] Y. Freund  H. S. Seung  E. Shamir  and N. Tishby. Selective sampling using the query by

committee algorithm. Machine Learning  28(2-3):133–168  1997.

[17] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML  pages

353–360  2007.

[18] S. Hanneke. Adaptive rates of convergence in active learning. In COLT  2009.

[19] M. K¨a¨ari¨ainen. Active learning in the non-realizable case. In ACL  pages 63–77  2006.

[20] I. Muslea  S. Minton  and C. A. Knoblock. Active + semi-supervised learning = robust multi-

view learning. In ICML  pages 435–442  2002.

[21] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics 

32(1):135–166  2004.

[22] L. Wang. Sufﬁcient conditions for agnostic active learnable. In NIPS 22  pages 1999–2007.

2009.

[23] W. Wang and Z.-H. Zhou. On multi-view active learning and the combination with semi-

supervised learning. In ICML  pages 1152–1159  2008.

9

,Shipra Agrawal
Randy Jia