2014,Dimensionality Reduction with Subspace Structure Preservation,Modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. However  dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. Our key contribution is to show that $2K$ projection vectors are sufficient for the independence preservation of any $K$ class data sampled from a union of independent subspaces. It is this non-trivial observation that we use for designing our dimensionality reduction technique. In this paper  we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. We support our theoretical analysis with empirical results on both synthetic and real world data achieving \textit{state-of-the-art} results compared to popular dimensionality reduction techniques.,Dimensionality Reduction with Subspace Structure

Preservation

Devansh Arpit

Ifeoma Nwogu

Department of Computer Science

Department of Computer Science

SUNY Buffalo

Buffalo  NY 14260

devansha@buffalo.edu

SUNY Buffalo

Buffalo  NY 14260

inwogu@buffalo.edu

Venu Govindaraju

Department of Computer Science

SUNY Buffalo

Buffalo  NY 14260

govind@buffalo.edu

Abstract

Modeling data as being sampled from a union of independent subspaces has been
widely applied to a number of real world applications. However  dimensional-
ity reduction approaches that theoretically preserve this independence assumption
have not been well studied. Our key contribution is to show that 2K projection
vectors are sufﬁcient for the independence preservation of any K class data sam-
pled from a union of independent subspaces. It is this non-trivial observation that
we use for designing our dimensionality reduction technique. In summary  we
propose a novel dimensionality reduction algorithm that theoretically preserves
this structure for a given dataset. We support our theoretical analysis with empiri-
cal results on both synthetic and real world data achieving state-of-the-art results
compared to popular dimensionality reduction techniques.

1

Introduction

A number of real world applications model data as being sampled from a union of independent
subspaces. These applications include image representation and compression [6]  systems theory
[12]  image segmentation [15]  motion segmentation [13]  face clustering [7  5] and texture seg-
mentation [8]  to name a few. Dimensionality reduction is generally used prior to applying these
methods because most of these algorithms optimize expensive loss functions like nuclear norm  (cid:96)1
regularization  e.t.c. Most of these applications simply apply off-the-shelf dimensionality reduction
techniques or resize images (in case of image data) as a pre-processing step.
The union of independent subspace model can be thought of as a generalization of the traditional
approach of representing a given set of data points using a single low dimensional subspace (e.g.
Principal Component Analysis). For the application of algorithms that model data at hand with this
independence assumption  the subspace structure of the data needs to be preserved after dimension-
ality reduction. Although a number of existing dimensionality reduction techniques [10  3  1  4] try
to preserve the spacial geometry of any given data  no prior work has tried to explicitly preserve the
independence between subspaces to the best of our knowledge.

This work was partially funded by the National Science Foundation under the grant number CNS-1314803

1

In this paper  we propose a novel dimensionality reduction technique that preserves independence
between multiple subspaces. In order to achieve this  we ﬁrst show that for any two disjoint sub-
spaces with arbitrary dimensionality  there exists a two dimensional subspace such that both the
subspaces collapse to form two lines. We then extend this non-trivial idea to multi-class case and
show that 2K projection vectors are sufﬁcient for preserving the subspace structure of a K class
dataset. Further  we design an efﬁcient algorithm that ﬁnds the projection vectors with the afore-
mentioned properties while being able to handle corrupted data at the same time.

2 Preliminaries

Let S1  S2 . . . SK be K subspaces in Rn. We say that these K subspaces are independent if there
does not exist any non-zero vector in Si which is a linear combination of vectors in the other K − 1
subspaces. Let the columns of the matrix Bi ∈ Rn×d denote the support of the ith subspace of d
dimensions. Then any vector in this subspace can be represented as x = Biw ∀w ∈ Rd. Now we
deﬁne the notion of margin between two subspaces.

Deﬁnition 1 (Subspace Margin) Subspaces Si and Sj are separated by margin γij if

γij = max

u∈Si v∈Sj

(cid:104)u  v(cid:105)
(cid:107)u(cid:107)2(cid:107)v(cid:107)2

(1)

Thus margin between any two subspaces is deﬁned as the maximum dot product between two unit
vectors (u  v)  one from either subspace. Such a vector pair (u  v) is known as the principal vector
pair between the two subspaces while the angle between these vectors is called the principal angle.
With these deﬁnitions of independent subspaces and margin  assume that we are given a dataset
which has been sampled from a union of independent linear subspaces. Speciﬁcally  each class in
this dataset lies along one such independent subspace. Then our goal is to reduce the dimensionality
of this dataset such that after projection  each class continues to lie along a linear subspace and that
each such subspace is independent of all others. Formally  let X = [X1  X2 . . .   XK] be a K class
dataset in Rn such that vectors from class i (x ∈ Xi) lie along subspace Si. Then our goal is to
ﬁnd a projection matrix (P ∈ Rn×m) such that the projected data vectors ¯Xi := {P T x : x ∈ Xi}
(i ∈ {1 . . . K}) are such that data vectors ¯Xi belong to a linear subspace ( ¯Si in Rm). Further  each
subspace ¯Si (i ∈ {1 . . . K}) is independent of all others.

3 Proposed Approach

In this section  we propose a novel subspace learning approach applicable to labeled datasets that
theoretically guarantees independent subspace structure preservation. The number of projection
vectors required by our approach is not only independent of the size of the dataset but is also ﬁxed 
depending only on the number of classes. Speciﬁcally  we show that for any K class labeled dataset
with independent subspace structure  only 2K projection vectors are required for structure preser-
vation.
The entire idea of being able to ﬁnd a ﬁxed number of projection vectors for the structure preserva-
tion of a K class dataset is motivated by theorem 2. This theorem states a useful property of any
pair of disjoint subspaces.

Theorem 2 Let unit vectors v1 and v2 be the ith principal vector pair for any two disjoint subspaces
S1 and S2 in Rn. Let the columns of the matrix P ∈ Rn×2 be any two orthonormal vectors in the
span of v1 and v2. Then for all vectors x ∈ Sj  P T x = αtj (j ∈ {1  2})  where α ∈ R depends on
x and tj ∈ R2 is a ﬁxed vector independent of x. Further 
Proof: We use the notation (M )j to denote the jth column vector of matrix M for any arbitrary
matrix M. We claim that tj = P T vj (j ∈ {1  2}). Also  without any loss of generality  assume that
(P )1 = v1. Then in order to prove theorem 2  it sufﬁces to show that ∀x ∈ S1  (P )T
2 x = 0. By
symmetry  ∀x ∈ S2  P T x will also lie along a line in the subspace spanned by the columns of P .

(cid:107)t1(cid:107)2(cid:107)t2(cid:107)2

tT
1 t2

= vT

1 v2

2

(a) Independent subspaces in 3
dimensions

(b) Subspaces after projection

Figure 1: A three dimensional example of the application of theorem 2. See text in section 3 for
details.
Let the columns of B1 ∈ Rn×d1 and B2 ∈ Rn×d2 be the support of S1 and S2 respectively  where d1
and d2 are the dimensionality of the two subspaces. Then we can represent v1 and v2 as v1 = B1w1
and v2 = B2w2 for some w1 ∈ Rd1 and w2 ∈ Rd2. Let B1w be any arbitrary vector in S1 where
w ∈ Rd1. Then we need to show that T := (B1w)T (P )2 = 0∀w. Notice that 

T = (B1w)T (B2w2 − (wT
1 BT
1 B2w2)B1w1)
1 B2w2 − (wT
1 B2w2)w1) ∀w
1 BT
= wT (BT

(2)

Let U SV T be the svd of BT
1 B2. Then w1 and w2 are the ith columns of U and V respectively  and
1 v2 is the ith diagonal element of S if v1 and v2 are the ith principal vectors of S1 and S2. Thus 
vT

T = wT (U SV T w2 − Sii(U )i)
= wT (Sii(U )i − Sii(U )i) = 0 (cid:3)

(3)

Geometrically  this theorem says that after projection on the plane (P ) deﬁned by any one of the
principal vector pairs between subspaces S1 and S2  both the entire subspaces collapse to just two
lines such that points from S1 lie along one line while points from S2 lie along the second line.
Further  the angle that separates these lines is equal to the angle between the ith principal vector pair
between S1 and S2 if the span of the ith principal vector pair is used as P .
We apply theorem 2 on a three dimensional example as shown in ﬁgure 1. In ﬁgure 1 (a)  the ﬁrst
subspace (y-z plane) is denoted by red color while the second subspace is the black line in x-y axis.
Notice that for this setting  the x-y plane (denoted by blue color) is in the span of the 1st (and only)
principal vector pair between the two subspaces. After projection of both the entire subspaces onto
the x-y plane  we get two lines (ﬁgure 1 (b)) as stated in the theorem.
Finally  we now show that for any K class dataset with independent subspace structure  2K projec-
tion vectors are sufﬁcient for structure preservation.
Theorem 3 Let X = {x}N
i=1 be a K class dataset in Rn with Independent Subspace structure. Let
P = [P1 . . . PK] ∈ Rn×2K be a projection matrix for X such that the columns of the matrix Pk ∈
Rn×2 consists of orthonormal vectors in the span of any principal vector pair between subspaces
j(cid:54)=k Sj. Then the Independent Subspace structure of the dataset X is preserved after

Sk and(cid:80)

projection on the 2K vectors in P .

Before stating the proof of this theorem  we ﬁrst state lemma 4 which we will use later in our proof.
This lemma states that if two vectors are separated by a non-zero angle  then after augmenting these
vectors with any arbitrary vectors  the new vectors remain separated by some non-zero angle as
well. This straightforward idea will help us extend the two subspace case in theorem 2 to multiple
subspaces.

xT

1 y1

Lemma 4 Let x1  y1 be any two ﬁxed vectors of same dimensionality with respect to each other
= γ where γ ∈ [0  1). Let x2  y2 be any two arbitrary vectors of same ﬁnite
such that
dimensionality with respect to each other. Then there exists a constant ¯γ ∈ [0  1) such that vectors
x(cid:48) = [x1; x2] and y(cid:48) = [y1; y2] are also separated such that

(cid:107)x1(cid:107)2(cid:107)y1(cid:107)2

≤ ¯γ.

x(cid:48)T y(cid:48)

(cid:107)x(cid:48)(cid:107)2(cid:107)y(cid:48)(cid:107)2

3

Algorithm 1 Computation of projection matrix P
INPUT: X K λ  itermax

for k=1 to K do

2 ← random vector in R ¯Nk
w∗
while iter < itermax or γ not converged do
1 ← arg minw1(cid:107)Xkw1 − ¯Xkw∗
w∗
(cid:107) ¯Xkw∗
2(cid:107)2
1 ← w∗
w∗
1/norm(w∗
1)
2 ← arg minw2(cid:107) Xkw∗
− ¯Xkw2(cid:107)2 + λ(cid:107)w2(cid:107)2
w∗
(cid:107)Xkw∗
1(cid:107)2
2 ← w∗
2/norm(w∗
w∗
2)
γ ← (Xkw∗
1)T ( ¯Xkw∗
2)
end while
Pk ← [Xkw∗
1  ¯Xkw∗
2]

(cid:107)2 + λ(cid:107)w1(cid:107)2

1

2

end for
P ∗ ← [P1 . . . PK]

OUTPUT: P ∗

k x and P T

Proof of theorem 3:

For the proof of theorem 3  it sufﬁces to show that data vectors from subspaces Sk and(cid:80)
be any vectors in Sk and(cid:80)
pair between Sk and(cid:80)

j(cid:54)=k Sj
(for any k ∈ {1 . . . K}) are separated by margin less than 1 after projection using P . Let x and y
j(cid:54)=k Sj respectively and the columns of the matrix Pk be in the span of
the ith (say) principal vector pair between these subspaces. Using theorem 2  the projected vectors
k y are separated by an angle equal to the the angle between the ith principal vector
k x and P T
P T
j(cid:54)=k Sj. Let the cosine of this angle be γ. Then  using lemma 4  the added
dimensions in the vectors P T
k y to form the vectors P T x and P T y are also separated by
some margin ¯γ < 1. As the same argument holds for vectors from all classes  the Independent
Subspace Structure of the dataset remains preserved after projection. (cid:3)
For any two disjoint subspaces  theorem 2 tells us that there is a two dimensional plane in which
the entire projected subspaces form two lines. It can be argued that after adding arbitrary valued
ﬁnite dimensions to the basis of this plane  the two projected subspaces will also remain disjoint
(see proof of theorem 3). Theorem 3 simply applies this argument to each subspace and the sum of
the remaining subspaces one at a time. Thus for K subspaces  we get 2K projection vectors.
Finally  our approach projects data to 2K dimensions which could be a concern if the original
feature dimension itself is less than 2K. However  since we are only concerned with data that has
underlying independent subspace assumption  notice that the feature dimension must be at least
K. This is because each class must lie on at least 1 dimension which is linearly independent of
others. However  this is too strict an assumption and it is straight forward to see that if we relax this
assumption to 2 dimensions for each class  the feature dimensions are already at 2K.
3.1

Implementation

A naive approach to ﬁnding projection vectors (say for a binary class case) would be to compute
1 X2  where the columns of X1 and X2 contain vectors from class 1 and
the SVD of the matrix X T
class 2 respectively. For large datasets this would not only be computationally expensive but also
be incapable of handling noise. Thus  even though theorem 3 guarantees the structure preservation
of the dataset X after projection using P as speciﬁed  this does not solve the problem of dimen-
sionality reduction. The reason is that given a labeled dataset sampled from a union of independent
subspaces  we do not have any information about the basis or even the dimensionality of the un-
derlying subspaces. Under these circumstances  constructing the projection matrix P as speciﬁed
in theorem 3 itself becomes a problem. To solve this problem  we propose an algorithm that tries
j(cid:54)=k Sj (for k = 1 to K)
given the labeled dataset X. The assumption behind this attempt is that samples from each subspace
(class) are not heavily corrupted and that the underlying subspaces are independent.
Notice that we are not speciﬁcally interested in a particular principal vector pair between any two
subspaces for the computation of the projection matrix. This is because we have assumed indepen-

to ﬁnd the underlying principal vector pair between subspaces Sk and(cid:80)

4

subspaces Sk and(cid:80)

dent subspaces and so each principal vector pair is separated by some margin γ < 1. Hence we
need an algorithm that computes any arbitrary principal vector pair  given data from two indepen-
dent subspaces. These vectors can then be used to form one of the K submatrices in P as speciﬁed
in theorem 3 . For computing the submatrix Pk  we need to ﬁnd a principal vector pair between
j(cid:54)=k Sj. In terms of dataset X  we estimate the vector pair using data in Xk
and ¯Xk where ¯Xk := X \ {Xk}. We repeat this process for each class to ﬁnally form the entire
matrix P ∗. Our approach is stated in algorithm 1. For each class k  the idea is to start with a random
vector in the span of ¯Xk and ﬁnd the vector in Xk closest to this vector. Then ﬁx this vector and
search of the closest vector in ¯Xk. Repeating this process till the convergence of the cosine between
these 2 vectors leads to a principal vector pair. In order to estimate the closest vector from oppo-
site subspace  we have used a quadratic program in 1 that minimizes the reconstruction error of the
ﬁxed vector (of one subspace) using vectors from the opposite subspace. The regularization in the
optimization is to handle noise in data.

3.2

Justiﬁcation

The deﬁnition 1 for margin γ between two subspaces S1 and S2 can be equivalently expressed as

1
2

w1 w2

1 B1 and BT

2 B2 are both identity matrices.

1 − γ = min

(cid:107)B1w1 − B2w2(cid:107)2 s.t. (cid:107)B1w1(cid:107)2 = 1 (cid:107)B2w2(cid:107)2 = 1

(4)
where the columns of B1 ∈ Rn×d1 and B2 ∈ Rn×d2 are the basis of the subspaces S1 and S2
respectively such that BT
Proposition 5 Let B1 ∈ Rn×d1 and B2 ∈ Rn×d2 be the basis of two disjoint subspaces S1 and
S2. Then for any principal vector pair (ui  vi) between the subspaces S1 and S2  the corresponding
vector pair (w1 ∈ Rd1 w2 ∈ Rd2)  s.t. ui = B1w1 and vi = B2w2  is a local minima to the
objective in equation (4).
Proof: The Lagrangian function for the above objective is:
L(w1  w2  η) =

2 B2w2−wT

wT

2 BT

wT

1 BT

1 B1w1+

1 BT

1 B2w2+η1((cid:107)B1w1(cid:107)2−1)+η2((cid:107)B2w2(cid:107)2−1)
(5)

1
2

1
2

Then setting the gradient w.r.t. w1 to zero we get
1 B2w2 = 0

∇w1L = (1 + 2η1)w1 − BT

Let U SV T be the SVD of BT
equation (6) becomes

(6)
1 B2 and w1 and w2 be the ith columns of U and V respectively. Then

∇w1L = (1 + 2η1)w1 − U SV T w2
(7)
= (1 + 2η1)w1 − Siiw1 = 0
2 (Sii−1). Similarly  it can be shown that the gradient
Thus the gradient w.r.t. w1 is zero when η1 = 1
2 (Sii − 1). Thus the gradient of the Lagrangian L is 0 w.r.t. both w1
w.r.t. w2 is zero when η2 = 1
and w2 for every corresponding principal vector pair. Thus vector pair (w1  w2) corresponding to
any of the principal vector pairs between subspaces S1 and S2 is a local minima to the objective 4.
(cid:3)

Since (w1  w2) corresponding to any principal vector pair between two disjoint subspaces form a
local minima to the objective given by equation (4)  one can alternatively minimize equation (4)
w.r.t. w1 and w2 and reach one of the local minima. Thus  by assuming independent subspace
structure for all the K classes in algorithm 1 and setting λ to zero  it is straight forward to see that
the algorithm yields a projection matrix that satisﬁes the criteria speciﬁed by theorem 3.
Finally  real world data do not strictly satisfy the independent subspace assumption in general and
even a slight corruption in data may easily lead to the violation of this independence. In order to
tackle this problem  we add a regularization (λ > 0) term while solving for the principal vector
pair in algorithm 1. If we assume that the corruption is not heavy  reconstructing a sample using
vectors belonging to another subspace would require a large coefﬁcient over those vectors. The
regularization avoids reconstructing data from one class using vectors from another class that are
slightly corrupted by assigning such vectors small coefﬁcients.

5

(a) Data projected
using Pa

(b) Data projected
using Pb

Figure 2: Qualitative comparison between
(a) true projection matrix and (b) projection
matrix from the proposed approach on high
dimensional synthetic two class data.
See
section 4.1.1 for details.

3.3 Complexity

(a)

(c)

(b)

(d)

Figure 3: Four different pairs of classes from
the Extended Yale dataset B projected onto
two dimensional subspaces using proposed
approach. See section 4.1.1 for details.

Solving algorithm 1 requires solving an unconstrained quadratic program within a while-loop. As-
sume that we run this while loop for T iterations and that we use conjugate gradient descent to solve
the quadratic program in each iteration. Also  it is known that for any matrix A ∈ Ra×b and vector
b ∈ Ra  conjugate gradient applied to a problem of the form

arg min

w

(8)

(cid:107)Ax − b(cid:107)2
√K)  where K is the condition number of AT A. Thus it is straight forward to
√K)  where n is the dimensionality of feature space  N is the total number of samples

takes time O(ab
see that the time required to compute the projection matrix for a K class problem in our case is
O(KT nN
and K is the condition number of the matrix (X T

k Xk + λI). Here I is the identity matrix.

4 Empirical Analysis

In this section  we present empirical evidence to support our theoretical analysis of our subspace
learning approach. For real world data  we use the following datasets:
1. Extended Yale dataset B [2]: It consists of ∼ 2414 frontal face images of 38 individuals (K = 38)
with 64 images per person. These images were taken under constrained but varying illumination
conditions.
2. AR dataset [9]: This dataset consists of more than 4000 frontal face images of 126 individuals
with 26 images per person. These images were taken under varying illumination  expression and
facial disguise. For our experiments  similar to [14]  we use images from 100 individuals (K =
100) with 50 males and 50 females. We further use only 14 images per class which correspond to
illumination and expression changes. This corresponds to 7 images from Session 1 and rest 7 from
Session 2.
3. PIE dataset [11]: The pose  illumination  and expression (PIE) database is a subset of CMU PIE
dataset consisting of 11  554 images of 68 people (K = 68).
We crop all the images to 32× 32  and concatenate all the pixel intensity to form our feature vectors.
Further  we normalize all data vectors to have unit (cid:96)2 norm.

6

−0.5−0.4−0.3−0.2−0.100.10.20.30.40.5−0.5−0.4−0.3−0.2−0.100.10.20.30.40.5−0.8−0.6−0.4−0.200.20.40.6−0.8−0.6−0.4−0.200.20.40.600.10.20.30.40.50.60.70.80.91−0.0500.050.10.150.20.250.3−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10−0.25−0.2−0.15−0.1−0.0500.050.1−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10−0.35−0.3−0.25−0.2−0.15−0.1−0.0500.05−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10−0.35−0.3−0.25−0.2−0.15−0.1−0.0500.05(a) Yale dataset B

(b) AR dataset

(c) PIE dataset

Figure 4: Multi-class separation after projection using proposed approach for different
datasets. See section 4.1.2 for details.
4.1 Qualitative Analysis
4.1.1 Two Subspaces-Two Lines

We test both the claim of theorem 2 and the quality of approximation achieved by algorithm 1 in
this section. We perform these tests on both synthetic and real data.
1. Synthetic Data: We generate two random subspaces in R1000 of dimensionality 20 and 30 (notice
that these subspaces will be independent with probability 1). We randomly generate 100 data vectors
from each subspace and normalize them to have unit length. We then compute the 1st principal
vector pair between the two subspaces using their basis vectors by performing SVD of BT
1 B2 
where B1 and B2 are the basis of the two subspaces. We orthonormalize the vector pair to form the
projection matrix Pa. Next  we use the labeled dataset of 200 points generated to form the projection
matrix Pb by applying algorithm 1. The entire dataset of 200 points is then projected onto Pa and Pb
separately and plotted in ﬁgure 2. The green and red points denote data from either subspace. The
results not only substantiate our claim in theorem 2 but also suggest that the proposed algorithm for
estimating the projection matrix is a good approximation.
2. Real Data: Here we use Extended Yale dataset B for analysis. Since we are interested in pro-
jection of two class data in this experimental setup  we randomly choose 4 different pairs of classes
from the dataset and use the labeled data from each pair to generate the two dimensional projection
matrix (for that pair) using algorithm 1. The resulting projected data from the 4 pairs can be seen in
ﬁgure 3. As is evident from the ﬁgure  the projected two class data for each pair approximately lie
along two different lines.
4.1.2 Multi-class separability

We analyze the separation between the K classes of a given K-class dataset after dimensionality
reduction. First we compute the projection matrix for that dataset using our approach and project the
data. Second  we compute the top principal vector for each class separately from the projected data.
This gives us K vectors. Let the columns of the matrix Z ∈ R2K×K contain these vectors. Then
in order to visualize inter-class separability  we simply take the dot product of the matrix Z with
itself  i.e. Z T Z. Figure 4 shows this visualization for the three face datasets. The diagonal elements
represent self-dot product; thus the value is 1 (white). The off-diagonal elements represent inter-
class dot product and these values are consistently small (dark) for all the three datasets reﬂecting
between class separability.
4.2 Quantitative Analysis

In order to evaluate theorem 3  we perform a classiﬁcation experiment on all the three real world
datasets mentioned above after projecting the data vectors using different dimensionality reduction
techniques. We compare our quantitative results against PCA  Linear discriminant analysis (LDA) 
Regularized LDA and Random Projections (RP) 1. We make use of sparse coding [14] for classiﬁ-
cation.

1We also used LPP (Locality Preserving Projections) [3]  NPE (Neighborhood Preserving Embedding) [4] 
and Laplacian Eigenmaps [1] for dimensionality reduction on Extended Yale B dataset. However  because
the best performing of these reduction techniques yielded a result of only 73% compared to the close to 98%
accuracy from our approach  we do not report results from these methods.

7

For Extended Yale dataset B  we use all 38 classes for evaluation with 50% − 50% train-test split 1
and 70% − 30% train-test split 2. Since our method is randomized  we perform 50 runs of comput-
ing the projection matrix using algorithm 1 and report the mean accuracy with standard deviation.
Similarly for RP  we generate 50 different random matrices and then perform classiﬁcation. Since
all other methods are deterministic  there is no need for multiple runs.

Table 1: Classiﬁcation Accuracy on Extended Yale dataset B with 50%-50% train-test split.
See section 4.2 for details.

Method

dim
acc

Ours
76

98.06 ± 0.18

PCA
76

92.54

LDA Reg-LDA
37

37

83.68

95.77

93.78 ± 0.48

RP
76

RP
76

Table 2: Classiﬁcation Accuracy on Extended Yale dataset B with 70%-30% train-test split.
See section 4.2 for details.

Method

dim
acc

Ours
76

99.45 ± 0.20

PCA
76

93.98

LDA Reg-LDA
37

37

93.85

97.47

94.72 ± 0.66

Table 3: Classiﬁcation Accuracy on AR dataset. See section 4.2 for details.

Method

dim
acc

Ours
200

92.18 ± 0.08

PCA LDA Reg-LDA
200
85.00

88.71

99
-

99

RP
200

84.76 ± 1.36

Table 4: Classiﬁcation Accuracy on a subset of CMU PIE dataset. See section 4.2 for details.

Method

dim
acc

Ours
136

93.65 ± 0.08

PCA
136
87.76

LDA Reg-LDA
67

67

86.71

92.59

RP
136

90.46 ± 0.93

Table 5: Classiﬁcation Accuracy on a subset of CMU PIE dataset. See section 4.2 for details.

Method

dim
acc

Ours
20

99.07 ± 0.09

PCA
20

97.06

LDA Reg-LDA

9

95.88

9

97.25

RP
20

95.03 ± 0.41

For AR dataset  we take the 7 images from Session 1 for training and the 7 images from Session 2
for testing. The results are shown in table 3. The result using LDA is not reported because we found
that the summed within class covariance was degenerate and hence LDA was not applicable. It can
be clearly seen that our approach signiﬁcantly outperforms other dimensionality reduction methods.
Finally for PIE dataset  we perform experiments on two different subsets. First  we take all the
68 classes and for each class  we randomly choose 25 images for training and 25 for testing. The
performance for this subset is shown in table 4. Second  we take only the ﬁrst 10 classes of the
dataset and of all the 170 images per class  we randomly split the data into 70%− 30% train-test set.
The performance for this subset is shown in table 5.
Evidently  our approach consistently yields the best performance on all the three datasets compared
to other dimensionality reduction methods.

5 Conclusion

We proposed a theoretical analysis on the preservation of independence between multiple subspaces.
We show that for K independent subspaces  2K projection vectors are sufﬁcient for independence
preservation (theorem 3). This result is motivated from our observation that for any two disjoint
subspaces of arbitrary dimensionality  there exists a two dimensional plane such that after projec-
tion  the entire subspaces collapse to just two lines (theorem 2). Resulting from this analysis  we
proposed an efﬁcient iterative algorithm (1) that tries to exploit these properties for learning a projec-
tion matrix for dimensionality reduction that preserves independence between multiple subspaces.
Our empirical results on three real world datasets yield state-of-the-art results compared to popular
dimensionality reduction methods.

8

References
[1] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data

representation. Neural Comput.  15(6):1373–1396  June 2003.

[2] A.S. Georghiades  P.N. Belhumeur  and D.J. Kriegman. From few to many: Illumination cone
models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal. Mach.
Intelligence  23(6):643–660  2001.

[3] X. He and P. Niyogi. Locality preserving projections (lpp). Proc. of the NIPS  Advances in

Neural Information Processing Systems. Vancouver: MIT Press  103  2004.

[4] Xiaofei He  Deng Cai  Shuicheng Yan  and Hong-Jiang Zhang. Neighborhood preserving
embedding. In Computer Vision  2005. ICCV 2005. Tenth IEEE International Conference on 
volume 2  pages 1208–1213 Vol. 2  Oct 2005.

[5] Jeffrey Ho  Ming-Husang Yang  Jongwoo Lim  Kuang-Chih Lee  and David Kriegman. Clus-
tering appearances of objects under varying illumination conditions. In Computer Vision and
Pattern Recognition  2003. Proceedings. 2003 IEEE Computer Society Conference on  vol-
ume 1  pages I–11–I–18. IEEE  2003.

[6] Wei Hong  John Wright  Kun Huang  and Yi Ma. Multiscale hybrid linear models for lossy

image representation. Image Processing  IEEE Transactions on  15(12):3655–3671  2006.

[7] Guangcan Liu  Zhouchen Lin  and Yong Yu. Robust subspace segmentation by low-rank rep-

resentation. In ICML  2010.

[8] Yi Ma  Harm Derksen  Wei Hong  John Wright  and Student Member. Segmentation of multi-
variate mixed data via lossy coding and compression. IEEE Transactions on Pattern Analysis
and Machine Intelligence  3  2007.

[9] Aleix Mart´ınez and Robert Benavente. AR Face Database  1998.
[10] Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear

embedding. Science  290:2323–2326  December 2000.

[11] Terence Sim  Simon Baker  and Maan Bsat. The cmu pose  illumination  and expression (pie)
database. In Automatic Face and Gesture Recognition  2002. Proceedings. Fifth IEEE Inter-
national Conference on  pages 46–51. IEEE  2002.

[12] Ren´e Vidal  Stefano Soatto  Yi Ma  and Shankar Sastry. An algebraic geometric approach to the
identiﬁcation of a class of linear hybrid systems. In Decision and Control  2003. Proceedings.
42nd IEEE Conference on  volume 1  pages 167–172. IEEE  2003.

[13] Ren´e Vidal  Roberto Tron  and Richard Hartley. Multiframe motion segmentation with missing
data using powerfactorization and gpca. International Journal of Computer Vision  79(1):85–
105  2008.

[14] J. Wright  A.Y. Yang  A. Ganesh  S.S. Sastry  and Yi Ma. Robust face recognition via sparse

representation. IEEEE TPAMI  31(2):210 –227  Feb. 2009.

[15] Allen Y Yang  John Wright  Yi Ma  and S Shankar Sastry. Unsupervised segmentation
of natural images via lossy data compression. Computer Vision and Image Understanding 
110(2):212–225  2008.

9

,Devansh Arpit
Ifeoma Nwogu
Venu Govindaraju