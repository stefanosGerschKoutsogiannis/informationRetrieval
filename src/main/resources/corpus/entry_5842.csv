2018,Variance-Reduced Stochastic Gradient Descent on Streaming Data,We present an algorithm STRSAGA for efficiently maintaining a machine learning model over data points that arrive over time  quickly updating the model as new training data is observed. We present a competitive analysis comparing the sub-optimality of the model maintained by STRSAGA with that of an offline algorithm that is given the entire data beforehand  and analyze the risk-competitiveness of STRSAGA under different arrival patterns. Our theoretical and experimental results show that the risk of STRSAGA is comparable to that of offline algorithms on a variety of input arrival patterns  and its experimental performance is significantly better than prior algorithms suited for streaming data  such as SGD and SSVRG.,Variance-Reduced Stochastic Gradient Descent

on Streaming Data

Ellango Jothimurugesan‚àó‚Ä†
Carnegie Mellon University
ejothimu@cs.cmu.edu

Phillip B. Gibbons‚Ä†

Carnegie Mellon University

gibbons@cs.cmu.edu

Ashraf Tahmasbi‚àó‚Ä°
Iowa State University

tahmasbi@iastate.edu

Srikanta Tirthapura‚Ä°
Iowa State University
snt@iastate.edu

Abstract

We present an algorithm STRSAGA that can efÔ¨Åciently maintain a machine learning
model over data points that arrive over time  and quickly update the model as new
training data are observed. We present a competitive analysis that compares the sub-
optimality of the model maintained by STRSAGA with that of an ofÔ¨Çine algorithm
that is given the entire data beforehand. Our theoretical and experimental results
show that the risk of STRSAGA is comparable to that of an ofÔ¨Çine algorithm on a
variety of input arrival patterns  and its experimental performance is signiÔ¨Åcantly
better than prior algorithms suited for streaming data  such as SGD and SSVRG.

1

Introduction

We consider the maintenance of a model over streaming data that are arriving as an endless sequence
of data points. At any point in time  the goal is to Ô¨Åt the model to the training data points observed
so far  in order to accurately predict/label unobserved test data. Such a model is never ‚Äúcomplete‚Äù
but instead needs to be continuously updated as newer training data points arrive. Methods that
recompute the model from scratch upon the arrival of new data points are infeasible due to their high
computational costs  and hence we need methods that efÔ¨Åciently update the model as more data arrive.
Such efÔ¨Åciency should not come at the expense of accuracy‚Äîthe accuracy of the model maintained
through such updates should be close to that obtained if we were to build a model from scratch  using
all the training data points seen so far.
Fitting a model is usually cast as an optimization problem  where the model parameters are those that
optimize an objective function. In typical cases  the objective function is the empirical or regularized
risk  usually the sum of a Ô¨Ånite number of terms  and often assumed to be convex. Consider a stream
of training data points Si arriving before or at time i consisting of ni data points. Let w denote the
set of parameters characterizing the learned function. The empirical risk function RSi measures the
average loss of w over Si: RSi(w) = 1
j=1 fj(w)  where fj(w) is the loss of w on data point j.
The goal is to Ô¨Ånd the empirical risk minimizer (ERM)  i.e.  the parameters w‚àó that minimize the
empirical risk over all data points observed so far. Typically  some form of gradient descent is used in
pursuit of w‚àó.
There are two common approaches: batch learning and incremental learning (sometimes called
‚Äúonline learning‚Äù) [BL03  Ber16]. Batch learning uses all available data points in the training set to

(cid:80)ni

ni

‚àóEJ and AT contributed equally to this work.
‚Ä†Supported in part by NSF grant 1725663
‚Ä°Supported in part by NSF grants 1527541 and 1725702

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr√©al  Canada.

compute the gradient for each step of gradient descent‚Äîthis method renders a gradient computation
to be expensive  especially for a large dataset. In contrast  an incremental learning algorithm operates
on only a single data point at each step of gradient descent  and hence a single step of an incremental
algorithm is much faster than a corresponding step of a batch algorithm. Incremental algorithms 
e.g.  Stochastic Gradient Descent (SGD) [RM51  BL03] and variance-reduced improvements such
as SVRG [JZ13] and SAGA [DBLJ14]  have been found to be more effective on large datasets than
batch algorithms  and are widely used.
Both batch and incremental algorithms assume that all training data points are available in advance‚Äî
we refer to such algorithms as ofÔ¨Çine algorithms. However  in the setting that we consider  data points
arrive over time according to an unknown arrival distribution  and neither batch nor incremental
algorithms are able to update the model efÔ¨Åciently as more data arrives. Though incremental learning
algorithms use only a single data point in each iteration  they typically select that point from the
entire set of training data points‚Äîthis set of training data points is constantly changing in our setting 
rendering incremental algorithms inapplicable. In the rest of the paper  we refer to an algorithm
that can efÔ¨Åciently update a model upon the arrival of new training data points as a streaming data
algorithm. Note that streaming data algorithms (which are not limited in their memory usage) are
broader than traditional streaming algorithms (which work in a single pass with limited memory).
Streaming data algorithms are relevant in many practical settings given the abundance of memory
these days.
The optimization goal of a streaming data algorithm is to maintain a model using all the data points
that have arrived so far  such that the model‚Äôs empirical risk is close to the ERM over those data
points. The challenges include (i) because the training data is changing at each time step  the ERM
on streaming data is a ‚Äúmoving target‚Äù; (ii) the ERM is an optimal solution that cannot be realized
in limited processing time  while a streaming data algorithm is not only limited in processing time 
but is also presented the data points only sequentially; (iii) with increasing arrival rates  it becomes
increasingly difÔ¨Åcult for the streaming data algorithm to keep up with the ERM; and (iv) data points
may not arrive at a steady rate: the numbers of points arriving at different points in time can be
highly skewed. We present and analyze a streaming data algorithm  STRSAGA  that overcomes these
challenges and achieves an empirical risk close to the ERM in a variety of settings.
Contributions. We present STRSAGA  a streaming data algorithm for maintaining a model. STRSAGA
sees data points in a sequential manner  and can efÔ¨Åciently incorporate newly arrived data points into
the model. Yet  its accuracy at each point in time is comparable to that of an ofÔ¨Çine algorithm that has
access to all training data points in advance. We prove this using a ‚Äúcompetitive analysis‚Äù framework
that compares the accuracy of STRSAGA to a state-of-the-art ofÔ¨Çine algorithm DYNASAGA [DLH16] 
which is based on variance-reduced SGD. We show that given the same computational power  the
accuracy of STRSAGA is competitive to DYNASAGA at each point in time  under certain conditions on
the schedule of input arrivals. Our notion of ‚Äúrisk-competitiveness‚Äù is based on the sub-optimality
of risk with respect to the ERM (‚Äúsub-optimality‚Äù is deÔ¨Åned in Section 3). Our theoretical analysis
relies on a connection between the ‚Äúeffective sample size‚Äù of the algorithm (whether a streaming data
algorithm or an ofÔ¨Çine algorithm) and its sub-optimality of risk with respect to the ERM. We show
that if a streaming data algorithm is ‚Äúsample-competitive‚Äù to an ofÔ¨Çine algorithm  i.e.  its effective
sample size is close to that of an ofÔ¨Çine algorithm  then it is also risk-competitive to the ofÔ¨Çine
algorithm.
A key aspect of our work is that we carefully consider the schedule of arrivals of the data points‚Äîwe
care not only about which training data points have arrived so far  and how many of them  but
also about when they arrived. In our setting where the streaming data algorithm is computationally
bounded  it is not possible to be always risk-competitive with an ofÔ¨Çine algorithm. However  we show
that it is possible to achieve risk-competitiveness  if the schedule of arrivals of training data points
obeys certain conditions that we lay out in Section 5. We show that these conditions are satisÔ¨Åed by
a number of common arrival distributions  including Poisson arrivals and many classes of skewed
arrivals. For all these arrival distributions  we show that STRSAGA is risk-competitive to DYNASAGA.
Our experimental results for two machine learning tasks  logistic regression and matrix factorization 
on two real data sets each  support our analytical Ô¨Åndings: the sub-optimality of STRSAGA on data
points arriving over time (according to a variety of input arrival distributions) is almost always compa-
rable to the ofÔ¨Çine algorithm DYNASAGA that is given all data points in advance  when each algorithm
is given the same computational power. We also show that STRSAGA signiÔ¨Åcantly outperforms natural

2

streaming data versions of both SGD and SSVRG [FGKS15]. Moreover  the update time of STRSAGA
is small  making it practical even for settings when the arrival rate is high.

2 Related work

Stochastic Gradient Descent (SGD) [RM51] and its extensions are used extensively in practice for
learning from large datasets. While an iteration of SGD is cheap relative to an iteration of a full
gradient method  its variance can be high. To control the variance  the learning rate of SGD must
decay over time  resulting in a sublinear convergence rate. Newer variance-reduced versions of SGD 
on the other hand  achieve linear convergence on strongly convex objective functions  generally by
incorporating a correction term in each update step that approximates a full gradient  while still
ensuring each iteration is efÔ¨Åcient like SGD.
SAG [RSB12] was among the Ô¨Årst variance reduction methods proposed and achieves linear con-
vergence rate for smooth and strongly convex problems. SAG requires storage of the last gradient
computed for each data point and uses their average in each update. SAGA [DBLJ14] improves on
SAG by eliminating a bias in the update. Stochastic Variance-Reduced Gradient (SVRG) [JZ13]
is another variance reduction method that does not store the computed gradients  but periodically
computes a full-data gradient  requiring more computation than SAGA. Semi-Stochastic Gradient De-
scent (S2GD) [KR13] is a variant of SVRG where the gaps between full-data gradient computations
are of random length and follow a geometric law. CHEAPSVRG [SAKS16] is another variant of
SVRG. In contrast with SVRG  it estimates the gradient through computing the gradient on a subset
of training data points rather than all the data points. However  all of the above variance-reduced
methods require O(n log n) iterations to guarantee convergence to statistical accuracy (to yield a
good Ô¨Åt to the underlying data) for n data points. DYNASAGA [DLH16] achieves statistical accuracy
in only O(n) iterations by using a gradually increasing sample set and running SAGA on it.
So far  all the algorithms we have discussed are ofÔ¨Çine  and assume the entire dataset is available
beforehand. Streaming SVRG (SSVRG) [FGKS15] is an algorithm that handles streaming data arrivals 
and processes them in a single pass through data  using limited memory. In our experimental study 
we found STRSAGA to be signiÔ¨Åcantly more accurate than SSVRG. Further  our analysis of STRSAGA
shows that it handles arrival distributions which allow for burstiness in the stream  while SSVRG is
not suited for this case. In many practical situations  restricting a streaming data algorithm to use
limited memory is overly restrictive and as our results show  leads to worse accuracy.

3 Model and preliminaries

We consider a data stream setting in which the training data points arrive over time. For i =
1  2  3  . . .   let Xi be the set of zero or more training data points arriving at time step i. We assume
that each training data point is drawn from a Ô¨Åxed distribution P  which is not known to the algorithm.
Dealing with distributions that change over time is beyond the scope of this paper. Let Si = ‚à™i
j=1Xj
denote the set of data points that have arrived in time steps 1 through i (inclusive). Let ni denote the
number of data points in Si.
The model being trained/maintained is drawn from a class of functions F. A function in this class
is parameterized by a vector of weights w ‚àà Rd. For a function w  we deÔ¨Åne its expected risk as
R(w) = E [fx(w)] where fx(w) is the loss of function w on input x and the expectation is taken
over x drawn from distribution P. Let function w‚àó = arg minw‚ààFR(w) denote the optimal function
with respect to R(w). Let R‚àó = R(w‚àó) denote the minimum expected risk possible over distribution
P  within function class F. The function w‚àó is called the distributional risk minimizer. Given a
sample S of training data points drawn from P  the best we can do is minimize the empirical risk
over this sample. We have analogous deÔ¨Ånitions for minimizers of empirical risk over this sample.
The empirical risk of function w over a sample S of n elements is: RS (w) = 1
x‚ààS fx(w). The
S = arg minw‚ààFRS (w). The optimal
optimizer of the empirical risk is denoted as w‚àó
S  deÔ¨Åned as w‚àó
empirical risk is R‚àó
i = w‚àó
.
Si
Similarly  the optimal empirical risk over Si is R‚àó
i = RSi(w‚àó
i ).
Suppose a risk minimization algorithm is given a set of training examples Si  and outputs ap-
proximate solution wi. The statistical error is E [R(w‚àó
i ) ‚àí R(w‚àó)] and the optimization error is

S ). We denote the optimizer of the empirical risk over Si as w‚àó

S = RS (w‚àó

(cid:80)

n

3

i )]  where the expectation is taken over the randomness of Si. The total error

E [R(wi) ‚àí R(w‚àó
(restricting to a Ô¨Åxed function class F) is the sum of the two.
Following Bottou and Bousquet [BB07]  we deÔ¨Åne sub-optimality as follows.
DeÔ¨Ånition 1. The sub-optimality of an algorithm A over training data S is the difference between
A‚Äôs empirical risk and the optimal empirical risk:

SUBOPTS (A) := RS (w) ‚àí RS (w‚àó
S )

S is the empirical risk minimizer over S.

where w is the solution returned by A on S and w‚àó
Let H(n) = cn‚àíŒ±  for a constant c and 1/2 ‚â§ Œ± ‚â§ 1  be an upper bound on the statistical error.
Bottou and Bousquet [BB07] show that if  is a bound on the sub-optimality of w on Si  then the
total error is bounded by H(ni) + . Therefore  in designing an efÔ¨Åcient algorithm for streaming
data  we focus on reducing the sub-optimality to asymptotically balance with H(ni) ‚Äî it does not
pay to reduce the empirical risk even further. Note that although H(ni) is only an upper bound on
the statistical error  Bottou and Bousquet remark ‚Äúit is often accepted that these upper bounds give a
realistic idea of the actual convergence rates‚Äù [BB07]  in which case balancing the sub-optimality
with H(ni) asymptotically minimizes the total error.
We focus on time-efÔ¨Åcient algorithms for maintaining a model over streaming data. We focus on a
basic step used in all SGD-style algorithms (or variants such as SAGA): A random training point x is
chosen from a set of training samples  and the vector w is updated through a gradient computed at
point x. Let œÅ ‚â• 1 denote the number of such basic steps that can be performed in a single time step.

4 STRSAGA: gradient descent over streaming data

We present our algorithm STRSAGA for learning from streaming data. Stochastic gradient descent (or
one of its variants  such as SAGA [DBLJ14]) works by repeatedly sampling a point from a training
set T and using its gradient to determine an update direction. One option to handle streaming data
arrivals is to simply expand the set T from which further sampling is conducted  by adding all the new
arrivals. However  the problem with this approach is that the size of the training set T can change in
an uncontrolled manner  depending on the number of arrivals. As illustrated in prior work [DLH16] 
the optimization error of SAGA increases with the size of the training set T . With an uncontrolled
increase in the size of T   the corresponding sub-optimality of the algorithm over T increases  so that
the function that is Ô¨Ånally computed may have poor accuracy.
To handle this  we use an idea from DYNASAGA [DLH16]  which increases the size of the training set
T in a controlled manner  according to a schedule. Upon increasing the size of T   further increases
are placed on hold until a sufÔ¨Åcient number of SAGA steps have been performed on the current
state of T . By using this idea  DYNASAGA was able to achieve statistical accuracy earlier than SAGA.
However  DYNASAGA is still an ofÔ¨Çine algorithm that assumes that all training data is available in
advance.
STRSAGA deals with streaming arrivals as follows. Arriving points from the next set of points Xi
are added to a buffer Buf. The effective sample set T is expanded in a controlled manner  similar to
DYNASAGA. However  instead of choosing new points from a static training set  such as in DYNASAGA 
STRSAGA chooses new points from the dynamically changing buffer Buf. If Buf is empty  then
available CPU cycles are used to perform further steps of SAGA. After any time step  it is possible
that STRSAGA may have trained over only a subset of the points that are available in Buf  but this is to
ensure that the optimization error on the subset that has been trained is balanced with the statistical
error of the effective sample size. Algorithm 1 depicts the steps taken to process the zero or more
points Xi arriving at time step i. Before any input is seen  the algorithm initializes buffer Buf to
empty  effective sample T0 to empty  and function w0 to random values. STRSAGA as described here
uses the basic framework of DYNASAGA  of adding one training point to Ti every two steps of SAGA
(the linear schedule in [DLH16])  and both algorithms borrow variance-reduction steps from SAGA
(lines 8-9 in Algorithm 1 and using the running average A of all gradients).
Analysis of STRSAGA: Suppose data points Si have been seen till time step i  and ni = |Si|. We Ô¨Årst
note that the time taken to process a set of training points Xi is dominated by the time taken for œÅ
iterations of SAGA. Ideally  the empirical risk of the solution returned by STRSAGA is close to the
empirical risk of the ERM over Si. However  this is not possible in general. Suppose the number

4

Algorithm 1: STRSAGA: Process a set of training points Xi that arrived in time step i  i > 0.
// wi‚àí1 is the current function. Ti‚àí1 the effective sample set.

1 Add Xi to Buf // Buf is the set of training points not added to Ti yet

2 (cid:101)w0 ‚Üê wi‚àí1 and Ti ‚Üê Ti‚àí1

// Do œÅ steps of SAGA at each time step
3 for j ‚Üê 1 to œÅ do

// Every two steps of SAGA  add one training point to Ti  if available
if (Buf is non-empty) AND (j is even) then
Move a single point  z  from Buf to Ti
Œ±(z) ‚Üê 0 // Œ±(z) the prior gradient of z  initialized to 0

A ‚Üê(cid:80)
g ‚Üê ‚àáfp((cid:101)wj‚àí1) // compute the gradient
(cid:101)wj ‚Üê (cid:101)wj‚àí1 ‚àí Œ∑(g ‚àí Œ±(p) + A) // Œ∑ is the learning rate

SAGA  and can be maintained incrementally

Œ±(p)/|Ti| // A the average of all gradients  used by

Sample a point p uniformly from Ti

p‚ààTi

4
5
6
7

8
9
10
11

Œ±(p) ‚Üê g

12 wi ‚Üê (cid:101)wœÅ

// wi is the current function and Ti is the effective sample set.

of points arriving at each time step i were much greater than œÅ  the number of iterations of SAGA
that can be performed at each step. Then not even an ofÔ¨Çine algorithm such as DYNASAGA that has
all points at the beginning of time could be expected to match the empirical risk of the ERM within
the available time. In what follows  we present a competitive analysis  where the performance of
STRSAGA is compared with that of an ofÔ¨Çine algorithm that has all data available to it in advance. We
consider two ofÔ¨Çine algorithms  ERM and DYNASAGA(œÅ)  described below.
Algorithm ERM is the empirical risk minimizer  sees all of Si at the beginning of time  and has
inÔ¨Ånite computational power to process it. A streaming data algorithm has two obstacles if it has to
compete with ERM: (i) Unlike ERM  a streaming data algorithm does not have all data in advance 
and (ii) Unlike ERM  a streaming data algorithm has limited computational power. It is clear that no
streaming data algorithm can do better than ERM. We can practically approach the performance of
ERM through executing DYNASAGA until convergence is achieved.
Algorithm DYNASAGA(œÅ) sees all of Si at the beginning of time  and is given œÅ iterations of gradient
computations in each step. The parenthetical œÅ denotes this algorithm is the extension of the
original DYNASAGA [DLH16]  parameterized by the available amount of processing time. The
algorithm DYNASAGA performs 2ni steps of gradient computations on Si and then terminates  while
DYNASAGA(œÅ) performs œÅi steps  where if œÅi > 2ni  the additional steps are uniformly over Si. The
computational power of DYNASAGA(œÅ) over i time steps matches that of a streaming data algorithm.
However  DYNASAGA(œÅ) is still more powerful than a streaming data algorithm  because it can see
all data in advance. In general  it is not possible for a streaming data algorithm to compete with
DYNASAGA(œÅ) either‚Äîone issue being that streaming arrivals may be very bursty. Consider the
extreme case when all of Si arrives in the ith time step  and there were no arrivals in time steps
1 through i ‚àí 1. An algorithm for streaming data has only œÅ gradient computation steps that it
can perform on ni points  and its earlier œÅ(i ‚àí 1) gradient steps had no data to use. In contrast 
DYNASAGA(œÅ) can perform œÅi gradient steps on Si  and achieve a smaller empirical risk.
Each algorithm STRSAGA  DYNASAGA(œÅ)  and ERM  after seeing Si  has trained its model on a subset
Ti ‚äÜ Si. We call this subset the ‚Äúeffective sample set‚Äù. Let tSTR
denote the sizes of
the effective sample sets of STRSAGA  DYNASAGA(œÅ)  and ERM  respectively  after i time steps. The
following lemma shows that the expected sub-optimality of DYNASAGA(œÅ) over Si is related to tD
i .
Lemma 1 (Lemma 5 in [DLH16]). After i time steps  tD
= ni. The
expected sub-optimality of DYNASAGA(œÅ) over Si after i time steps is O(H(tD

i = min{ni  œÅi/2}  and tERM

i   and tERM

  tD

i

i

i

i )).

Our goal is for a streaming data algorithm to achieve an empirical risk that is close to the risk of an
ofÔ¨Çine algorithm. We present our notion of risk-competitiveness in DeÔ¨Ånition 2.

5

DeÔ¨Ånition 2. For c ‚â• 1  a streaming data algorithm I is said to be c-risk-competitive to
DYNASAGA(œÅ) at time step i if E [SUBOPTSi (I)] ‚â§ cH(tD
i ). Similarly  I is said to be c-risk-
competitive to ERM at time step i if E [SUBOPTSi(I)] ‚â§ cH(ni).
Note that the expected sub-optimality of I is compared
with H(tD
i ) and H(ni)  which are upper bounds on the
statistical errors of DYNASAGA(œÅ) and ERM respectively.
If H() is a tight bound on the statistical error  and hence  a
lower bound on the total error  then c-risk-competitiveness
to DYNASAGA(œÅ) implies that the expected sub-optimality
of the algorithm I is within a factor of c of the total risk
of DYNASAGA(œÅ)  as illustrated in Figure 1. We next show
if a streaming data algorithm is risk-competitive with re-
spect to DYNASAGA(œÅ) then it is also risk-competitive with
respect to ERM  under certain conditions.
Figure 1: The error of each algorithm.
Lemma 2. If a streaming data algorithm I is c-risk-competitive to DYNASAGA(œÅ) at time step i  and
the statistical risk H(n) = n‚àíŒ±  then I is c ¬∑ max
-risk-competitive to ERM at time

(cid:16)(cid:16) 2(cid:101)Œªi

(cid:17)Œ±

(cid:17)

  1

step i  where (cid:101)Œªi =(cid:0) ni

i

(cid:1) and ni is the size of Si.

œÅ

(cid:17)

ni) = c

i = œÅi/2 =

Proof. From DeÔ¨Ånition 2 we have: E [SUBOPTSi(I)] ‚â§ cH(tD
(Lemma 1). First consider the case when ni ‚â§ œÅi/2. We have: E [SUBOPTSi(I)] ‚â§ cH(tD
cH(ni). Therefore  for this case  I is c-risk-competitive to Algorithm ERM.
In the other case  when ni > œÅi/2  we have: tD
cH(tD

ni. Further  E [SUBOPTSi(I)] ‚â§
(cid:3)
Discussion: (cid:101)Œªi = (ni/i) is the average rate of arrivals in a time step. We expect the ratio ((cid:101)Œªi/œÅ) to

i = min(ni  œÅi/2)
i ) =

(cid:17)Œ± H(ni)

2(cid:101)Œªi
i ) = cH( œÅ

i ). We know tD

(cid:16) œÅ
2(cid:101)Œªi

(cid:16) 2(cid:101)Œªi

be a small constant. If this ratio is a large number  much greater than 1  the total number of arrivals
over i time steps far exceeds the number of gradient computations the algorithm can perform over i
time steps. This rate of arrivals is unsustainable  because most practical algorithms such as SGD and
variants  including SVRG and SAGA  require more than one gradient computation for each training
point. Hence  the above lemma implies that if I is O(1)-risk-competitive to DYNASAGA(œÅ)  then it is
also O(1)-risk-competitive to ERM  under reasonable arrival patterns.
Finally  we will bound the expected sub-optimality of STRSAGA over its effective sample set Ti in
Lemma 3. The proof of this lemma is presented in the supplementary material. In Section 5  we will
show how to apply the following result to establish the risk-competitiveness of STRSAGA.
Lemma 3. Suppose all fx are convex and their gradients are L-Lipschitz continuous  and that RTi
is ¬µ-strongly convex. At the end of each time step i  the expected sub-optimality of STRSAGA over Ti
is

œÅ

(cid:18) L

(cid:19)3(cid:18) 1

(cid:19)2

¬µ

tSTR
i

.

E [SUBOPTTi(STRSAGA)] ‚â§ H(tSTR

i

) + 2 (R(w0) ‚àí R(w‚àó))

If we additionally assume that the condition number L/¬µ is bounded by a constant at each time  the
above simpliÔ¨Åes to E [SUBOPTTi(STRSAGA)] ‚â§ (1 + o(1))H(tSTR

).

i

5 Competitive analysis of STRSAGA on speciÔ¨Åc arrival distributions

i

i

)) (note tSTR

Lemma 3 shows that the expected sub-optimality of STRSAGA over its effective sample set Ti is
O(H(tSTR
is not equal to ni the number of points so far). However  our goal is to show that
STRSAGA is risk-competitive to DYNASAGA(œÅ) at each time step i; i.e.  the expected sub-optimality
of STRSAGA over Si is within a factor of H(tD
i ). The connection between the two depends on the
relation between tSTR
i . This relation is captured using sample-competitiveness  which is
introduced in this section. Although not every arrival distribution provides sample-competitiveness 
we will show a number of different patterns of arrival distributions that do provide this property. To
model different arrival patterns  we consider a general arrival model where the number of points
arriving in time step i is a random variable xi which is independently drawn from distribution P with
a Ô¨Ånite mean Œª. We consider arrival distributions of varying degrees of generality  including Poisson

and tD

i

6

ùê∞‚àóERM‚Ñã(ùëõùëñ)‚â•‚Ñã(ùë°ùëñùê∑)DYNASAGA(ùúå)‚â§ùëê‚Ñã(ùë°ùëñùê∑)Algorithm I‚Ñõ‚ÑõùëÜi

i /tD

STR  Ti

ni

STR

ni

STR and Ti

STR

H(ti

STR).

H(ti

STR) ‚â§ (2 + o(1))H(ti

STR).

D)  completing the proof.

. At time i/2  at least kni points have arrived.

STR) ‚â§ (2 + o(1))H(k ¬∑ ti

D) = k‚àíŒ±(2 + o(1))H(ti

2}-sample-competitive to DYNASAGA(œÅ) at time step i.

D denote the effective samples that were used at iteration i for STRSAGA and
D ‚äÜ Si. Using Theorem 3 from [DLH16]  we

arrivals  skewed arrivals  general arrivals with a bounded maximum  and general arrivals with an
unbounded maximum. The proofs of results about speciÔ¨Åc distributions  as well as the full statements
of prior theorems and bounds referenced below  can be found in the supplementary material.
i ‚â• k.
DeÔ¨Ånition 3. At time i  STRSAGA is said to be k-sample-competitive to DYNASAGA(œÅ) if tSTR
Lemma 4. If STRSAGA is k-sample-competitive to DYNASAGA(œÅ) at time step i  then it is c-risk-
competitive to DYNASAGA(œÅ) at time step i with c = k‚àíŒ±(2 + o(1)).
Proof. Let Ti
DYNASAGA(œÅ)  respectively. We know that Ti
have: E [SUBOPTSi(STRSAGA)] ‚â§ E [SUBOPTTi(STRSAGA)] + ni‚àíti
Using Lemma 3  we can rewrite the above inequality as:
E [SUBOPTSi(STRSAGA)] ‚â§ (1 + o(1))H(ti
STR) + ni‚àíti
If STRSAGA is k-sample-competitive to DYNASAGA(œÅ)  then we have: E [SUBOPTSi(STRSAGA)] ‚â§
(2 + o(1))H(ti
(cid:3)
Lemma 5. At time step i  suppose the streaming arrivals satisfy: ni/2 ‚â• kni. Then  STRSAGA is
min{k  1
Proof. We Ô¨Årst bound tSTR
In Algorithm 1  at
time i/2  these points are either in the Buf or already in the effective sample T STR
i/2 . We note that
for every two iterations of SAGA  the algorithm moves one point from Buf (if available) to the
effective sample  thus increasing the size of the effective sample set by 1. In the i/2 time steps from
i/2 + 1  . . .   i  STRSAGA can perform œÅi/2 iterations of SAGA. Within these iterations  it can move
œÅi/4 points to T STR
  if available in the buffer. Hence  the effective sample size for STRSAGA at time i
is: tSTR
We consider four cases.
i = ni
i /2. The other three cases 
and tSTR
(2) œÅi/4 < ni/2 and ni ‚â• œÅi/2  (3) œÅi/4 ‚â• ni/2 and ni < œÅi/2  and (4) œÅi/4 ‚â• ni/2 and ni ‚â• œÅi/2 
(cid:3)
can be handled similarly.
Skewed Arrivals with a Bounded Maximum. We next consider an arrival distribution parameter-
ized by integer M ‚â• Œª  where the number of arrivals per time step can either be high (M) or zero.
More precisely  xi = M with prob. Œª
M . Thus  E[xi] = Œª. For M > Œª 
this models bursty arrival distributions with a number of ‚Äúquiet‚Äù time steps with no arrivals  combined
with an occasional burst of M arrivals. We have the following result for skewed arrivals.
Lemma 6. For a skewed arrival distribution with maximum M and mean Œª  STRSAGA is 6Œ±(2+o(1))-
risk-competitive to DYNASAGA(œÅ)  with probability at least 1 ‚àí   at any time step i > 16M
At a high level  the proof relies on showing sample-competitiveness of STRSAGA. For a time step i
greater than the threshold stated in the lemma  we can prove the concentration of ni and ni/2 using a
Chernoff bound. Using Lemma 5  STRSAGA and DYNASAGA(œÅ) are 1
6-sample-competitive  and the
risk-competitiveness follows from Lemma 4. Note that as M increases  arrivals become more bursty 
and it takes longer for the algorithm to be competitive  with a high conÔ¨Ådence.
General Arrivals with a Bounded Maximum. We next consider a more general arrival distribution
with a maximum of M arrivals  and a mean of Œª. xi = j with probability pj for j = 0  . . .   M  such

In the Ô¨Årst case  (1) if œÅi/4 < ni/2 and ni < œÅi/2  then tD

i ‚â• œÅi/4. In this case  we have tSTR

i ‚â• œÅi/4 > ni/2 = tD

M and xi = 0 with prob. 1 ‚àí Œª

i ‚â• min{œÅi/4  kni}. We know tD

i = min{ni  œÅi/2}.

i

 .
Œª ln 1

that(cid:80)

j pj = 1 and E[xi] = Œª  for an integer M > 0.

3 ) ln 1

Lemma 7. For a general arrival distribution with mean Œª and maximum M  at any time step
   STRSAGA is 8Œ±(2 + o(1))-risk-competitive to DYNASAGA(œÅ)  with probability
Œª + 8
i > ( 16M
at least 1 ‚àí .
The high-level proof sketch for this case is similar to the case of skewed arrivals. The technical aspect
is that in order to prove concentration bounds for ni and ni/2  we use Bernstein‚Äôs inequality [Mas07] 
which lets us bound the sum of independent random variables in a more Ô¨Çexible manner than Chernoff
bounds (for random variables that are not necessarily binary valued)  in conjunction with a bound on
the variance of the distribution. Proof details are in the supplementary material.
General Arrivals with an Unbounded Maximum. More generally  the number of arrivals in a time
step may not have a speciÔ¨Åed maximum. The arrival distribution can have a Ô¨Ånite mean  despite a

7

parameter b: The random variable xi has mean Œª  variance œÉ2  and |E(cid:2)(xi ‚àí Œª)k(cid:3)| ‚â§ 1

small probability of reaching arbitrarily large values. We consider a sub-class of such distributions
where all the polynomial moments are bounded  as in the following Bernstein‚Äôs condition with
2 k!œÉ2bk‚àí2
for all integers k ‚â• 3 [Mas07].
Lemma 8. For any arrival distribution with mean Œª  bounded variance œÉ2 and satisfying Bern-
stein‚Äôs condition with parameter b  STRSAGA is 8Œ±(2 + o(1))-risk-competitive to DYNASAGA(œÅ)  with
probability at least 1 ‚àí   at any time step i > max((16( œÉ
Poisson Arrivals. We next consider the case where the number of points arriving in each time step
follows a Poisson distribution with mean Œª  i.e.  Pr [xi = k] = e‚àíŒªŒªk
Lemma 9. For Poisson arrival distribution with mean Œª  STRSAGA is 8Œ±(2 + o(1))-risk-competitive
to DYNASAGA(œÅ) with probability at least 1 ‚àí   at any time step i > 16
The proof depends on a version of the Chernoff bounds tailored to the Poisson distribution‚Äîfurther
details are in the supplementary material.

for integer k ‚â• 0.

Œª )2 + 8

 .
Œª ln 1

k!

3 ) ln 1

   2(( œÉ

Œª )2 + b

Œª ) ln 1

 ).

6 Experimental results

We empirically conÔ¨Årm the competitiveness of STRSAGA with the ofÔ¨Çine algorithm DYNASAGA(œÅ)
through a set of experiments on real world datasets streamed in under various arrival distributions.
We consider two optimization problems that arise in supervised learning  logistic regression (con-
vex) and matrix factorization (nonconvex). For logistic regression  we use the A9A [DKT17] and
RCV1.binary [LYRL04] datasets  and for matrix factorization  we use two datasets of user-item rat-
ings from Movielens [HK16]. More detail on the datasets are provided in the supplementary material.
These static training data are converted into streams  by ordering them by a random permutation  and
deÔ¨Åning an arrival rate Œª dependent on the dataset size. In our experiments  the training data arrives
over the course of 100 time steps  with skewed arrivals parameterized by M = 8Œª. Experiments on
Poisson arrivals are given in the supplementary material.
At each time step i  a streaming data algorithm has access to œÅ gradient computations to update the
model; we show results for œÅ/Œª = 1 and œÅ/Œª = 5. We compare the sub-optimality of STRSAGA
with the ofÔ¨Çine algorithm DYNASAGA(œÅ)  which is run from scratch at each time i using œÅi steps
on Si. We also compare with two streaming data algorithms  SGD  and for the case œÅ/Œª = 1  the
single-pass algorithm SSVRG.4 In the streaming data setting  in which we are not limited in storage
and the available processing time œÅ may permit revisiting points  our implementation of SGD needs
clariÔ¨Åcation in its sampling procedure. We tried two sampling policies. In the Ô¨Årst  at each time step
i we sample points uniformly from Si  the set of all points received till time step i. In the second 
at each time step i we Ô¨Årst visit points in Si that have not been seen yet  and spend any remaining
processing time to sample uniformly from all of Si. In every case  the second method was better
or indistinguishable from the Ô¨Årst  and so all of our results are based on the second method. For
our implementation of SSVRG  we have relaxed the memory limitation of the original streaming
algorithm by introducing a buffer to store points that have arrived but not yet been processed. With
this additional storage  we allow SSVRG to make progress during time steps even when no new points
arrive  and hence make for a fairer comparison when data points do not arrive at a steady rate.
The main results are summarized in Figure 2  showing the sub-optimality of each algorithm and the
sample-competitive ratio for STRSAGA. Additional plots of the test loss are given in the supplementary
material. The dips in the sample-competitive ratio represent the arrival of a large group of points 
and correspondingly at those times  the sub-optimality spikes  because there are now many new
points added to Si that have yet to be processed. We observe that the sample-competitive ratio
improves over the lifetime of the stream and tends towards 1  outperforming our pessimistic theoretical
analysis. Furthermore  as the sample-competitive ratio increases  the risk-competitiveness of STRSAGA
improves so that the sub-optimality of STRSAGA is comparable to that of the ofÔ¨Çine DYNASAGA(œÅ) 
which is the best we can do given limited computational power. In Figure 2  we also observe that
STRSAGA outperforms both our streaming data version of SGD  due to the faster convergence rate
when using SAGA steps with reduced variance  and also SSVRG  showing the beneÔ¨Åt of revisiting
data points  even when the processing rate is constrained at œÅ = 1Œª.

4We consider SSVRG a œÅ/Œª = 1 algorithm  because for most data points it receives  it uses 1 gradient

computation  and only for an o(1) fraction of the data points does it require 2 gradient computations.

8

Figure 2: Sub-optimality under skewed arrivals with M = 8Œª. Top row is processing rate œÅ = 1Œª 
and bottom row is œÅ = 5Œª. The median is taken over 5 runs.

To better understand the impact of the skewed arrival distribution on the performance of STRSAGA 
we did three experiments in Figure 3  showing the following results. (1) As M/Œª increases  the
arrivals become more bursty and it takes longer for STRSAGA to be sample-competitive  and as a result 
risk-competitive to DYNASAGA(œÅ). Note that the far left endpoint  for skewed arrival parameterized
with M = Œª  is the case of constant arrivals. (2) We observe that there is an intermediate point for
œÅ/Œª where it is more difÔ¨Åcult to be sample-competitive  but at the extremes the ratio tends towards
1. This is because for large œÅ/Œª  whenever a big group of points arrives they can all be processed
quickly. On the other hand  for small œÅ/Œª  at any time i  both STRSAGA and the ofÔ¨Çine algorithm are
still processing points that arrived at some time signiÔ¨Åcantly before i  and so a large variance in the
amount of fresh arrivals at the tail of the stream can be tolerated. (3) The bound on sub-optimality
we showed earlier is dependent on the number of data points processed so far. As we see  as time
passes and STRSAGA sees more data points  its sub-optimality on Si improves. Additionally as œÅ/Œª
increases  STRSAGA has more steps available to incorporate newly arrived data points and becomes
more resilient to bursty arrivals.

Figure 3: Sensitivity analysis. The Ô¨Årst plot varies the skew M/Œª for a Ô¨Åxed processing rate œÅ/Œª 
and the second two plots vary the processing rate for a Ô¨Åxed skew. Results are plotted for time steps
i = 25  50  75  100 over a stream of the RCV dataset of 100 time steps. The median is taken over 9
runs.

7 Conclusion

We considered the ongoing maintenance of a model over data points that are arriving over time 
according to an unknown arrival distribution. We presented STRSAGA  and showed through both
analysis and experiments that  for various arrival distributions  (i) its empirical risk over the data
arriving so far is close to the empirical risk minimizer over the same data  (ii) it is competitive with
a state-of-the-art ofÔ¨Çine algorithm DYNASAGA  and (iii) it signiÔ¨Åcantly outperforms streaming data
versions of both SGD and SSVRG. We conclude that STRSAGA should be the algorithm of choice for
variance-reduced SGD on streaming data in the setting where memory is not limited.

9

02040608010010‚àí210‚àí10.00.20.40.60.81.0a9a  œÅ=1Œª0204060801001002√ó10‚àí13√ó10‚àí14√ó10‚àí16√ó10‚àí10.00.20.40.60.81.0rcv  œÅ=1Œª0204060801001009√ó10‚àí10.00.20.40.60.81.0moviele s1m  œÅ=1Œª02040608010010‚àí210‚àí10.00.20.40.60.81.0a9a  œÅ=5Œª02040608010010‚àí10.00.20.40.60.81.0rcv  œÅ=5Œª02040608010010‚àí10.00.20.40.60.81.0moviele s1m  œÅ=5ŒªTimeSuboptimality o SitSTR/tDSTRSAGASSVRGDYNASAGA(œÅ)SGDtSTR/tD2021222324M/Œª0.60.70.80.91.0tSTR/tDSample-competitive ratio (œÅ=1Œª)i=25i=50i=75i=1002‚àí22‚àí120212223œÅ/Œª0.70.80.91.0tSTR/tDSample-competitive ratio (M=23Œª)i=25i=50i=75i=1002‚àí22‚àí120212223œÅ/Œª0.20.40.6Suboptimality on SiSuboptimality (M=23Œª)i=25i=50i=75i=100References

[BB07] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS  pages 161‚Äì168 

2007.

[BDR15] B. Bercu  B. Delyon  and E. Rio. Concentration inequalities for sums and martingales.

Springer  2015.

[Ber16] D. P. Bertsekas. Nonlinear Programming (3rd Ed.). Athena ScientiÔ¨Åc  2016.

[BL03] L. Bottou and Y. LeCun. Large scale online learning. In NIPS  pages 217‚Äì224  2003.

[DBLJ14] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In NIPS  pages 1646‚Äì1654 
2014.

[DKT17] D. Dua and E. Karra Taniskidou. UCI machine learning repository  2017.

[DLH16] H. Daneshmand  A. Lucchi  and T. Hofmann. Starting small-learning with adaptive

sample sizes. In ICML  pages 1463‚Äì1471  2016.

[FGKS15] R. Frostig  R. Ge  S. M. Kakade  and A. Sidford. Competing with the empirical risk

minimizer in a single pass. In COLT  pages 728‚Äì763  2015.

[HK16] F. M. Harper and J. A. Konstan. The movielens datasets: History and context. ACM TIIS 

5(4):19  2016.

[JZ13] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive

variance reduction. In NIPS  pages 315‚Äì323  2013.

[KR13] J. Konecn`y and P. Richt√°rik. Semi-stochastic gradient descent methods. arXiv preprint

arXiv:1312.1666  2013.

[LYRL04] D. D. Lewis  Y. Yang  T. G. Rose  and F. Li. Rcv1: A new benchmark collection for text

categorization research. JMLR  5(Apr):361‚Äì397  2004.

[Mas07] P. Massart. Concentration inequalities and model selection. Springer  2007.

[MU17] M. Mitzenmacher and E. Upfal. Probability and Computing: Randomization and Proba-
bilistic Techniques in Algorithms and Data Analysis. Cambridge university press  2017.

[RM51] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathemat-

ical Statistics  pages 400‚Äì407  1951.

[RSB12] N. L. Roux  M. Schmidt  and F. R. Bach. A stochastic gradient method with an exponential

convergence rate for Ô¨Ånite training sets. In NIPS  pages 2663‚Äì2671  2012.

[SAKS16] V. Shah  M. Asteris  A. Kyrillidis  and S. Sanghavi. Trading-off variance and complexity

in stochastic gradient descent. arXiv preprint arXiv:1603.06861  2016.

10

,Ellango Jothimurugesan
Ashraf Tahmasbi
Phillip Gibbons
Srikanta Tirthapura