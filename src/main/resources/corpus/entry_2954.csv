2014,Learning Mixed Multinomial Logit Model from Ordinal Data,Motivated by generating personalized recommendations using ordinal (or preference) data  we study the question of learning a mixture of MultiNomial Logit (MNL) model  a parameterized class of distributions over permutations  from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice  operations research and revenue management  little is known about this question. In case of single MNL models (no mixture)  computationally and statistically tractable learning from pair-wise comparisons is feasible. However  even learning mixture of two MNL model is infeasible in general. Given this state of affairs  we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. To that end  we present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular  a mixture of $r$ MNL components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely  $n^3 r^{3.5} \log^4 n$  with $r \ll n^{2/7}$ when the model parameters are sufficiently {\em incoherent}). The algorithm has two phases: first  learn the pair-wise marginals for each component using tensor decomposition; second  learn the model parameters for each component using RankCentrality introduced by Negahban et al. In the process of proving these results  we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.,LearningMixedMultinomialLogitModelfromOrdinalDataSewoongOhDept.ofIndustrialandEnterpriseSystemsEngr.UniversityofIllinoisatUrbana-ChampaignUrbana IL61801swoh@illinois.eduDevavratShahDepartmentofElectricalEngineeringMassachussettsInstituteofTechnologyCambridge MA02139devavrat@mit.eduAbstractMotivatedbygeneratingpersonalizedrecommendationsusingordinal(orpref-erence)data westudythequestionoflearningamixtureofMultiNomialLogit(MNL)model aparameterizedclassofdistributionsoverpermutations frompar-tialordinalorpreferencedata(e.g.pair-wisecomparisons).Despiteitslongstand-ingimportanceacrossdisciplinesincludingsocialchoice operationsresearchandrevenuemanagement littleisknownaboutthisquestion.IncaseofsingleMNLmodels(nomixture) computationallyandstatisticallytractablelearningfrompair-wisecomparisonsisfeasible.However evenlearningmixturewithtwoMNLcomponentsisinfeasibleingeneral.Giventhisstateofaffairs weseekconditionsunderwhichitisfeasibletolearnthemixturemodelinbothcomputationallyandstatisticallyefﬁcientmanner.WepresentasufﬁcientconditionaswellasanefﬁcientalgorithmforlearningmixedMNLmodelsfrompartialpreferences/comparisonsdata.Inparticular amixtureofrMNLcomponentsovernobjectscanbelearntusingsampleswhosesizescalespolynomiallyinnandr(concretely r3.5n3(logn)4 withr(cid:28)n2/7whenthemodelparametersaresufﬁcientlyincoherent).Thealgorithmhastwophases:ﬁrst learnthepair-wisemarginalsforeachcomponentusingtensordecomposi-tion;second learnthemodelparametersforeachcomponentusingRANKCEN-TRALITYintroducedbyNegahbanetal.Intheprocessofprovingtheseresults weobtainageneralizationofexistinganalysisfortensordecompositiontoamorerealisticregimewhereonlypartialinformationabouteachsampleisavailable.1IntroductionBackground.Popularrecommendationsystemssuchascollaborativeﬁlteringarebasedonapar-tiallyobservedratingsmatrix.Theunderlyinghypothesisisthatthetrue/latentscorematrixislow-rankandweobserveitspartial noisyversion.Therefore matrixcompletionalgorithmsareusedforlearning cf.[8 14 15 20].Inreality however observedpreferencedataisnotjustscores.Forexample clickingoneofthemanychoiceswhilebrowsingprovidespartialorderbetweenclickedchoiceversusotherchoices.Further scoresdoconveyordinalinformationaswell e.g.scoreof4forpaperAandscoreof7forpaperBbyareviewersuggestsorderingB>A.SimilarmotivationsledSamuelsontoproposetheAxiomofrevealedpreference[21]asthemodelforrationalbehavior.Inanutshell itstatesthatconsumershavelatentorderofallobjects andtherevealedpreferencesthroughactions/choicesareconsistentwiththisorder.Ifindeedallconsumershadidenticalorder-ing thenlearningpreferencefrompartialpreferencesiseffectivelythequestionofsorting.Inpractice individualshavedifferentorderingsofinterest andfurther eachindividualislikelytomakenoisychoices.Thisnaturallysuggeststhefollowingmodel–eachindividualhasalatentdistributionoverorderingsofobjectsofinterest andtherevealedpartialpreferencesareconsistent1withit i.e.samplesfromthedistribution.Subsequently thepreferenceofthepopulationasawholecanbeassociatedwithadistributionoverpermutations.Recallthatthelow-rankstructureforscorematrices asamodel triestocapturethefactthatthereareonlyafewdifferenttypesofchoiceproﬁle.Inthecontextofmodelingconsumerchoicesasdistributionoverpermutation MultiNomialLogit(MNL)modelwithasmallnumberofmixturecomponentsprovidessuchamodel.MixedMNL.Givennobjectsorchoicesofinterest anMNLmodelisdescribedasaparametricdistributionoverpermutationsofnwithparametersw=[wi]∈Rn:eachobjecti 1≤i≤n hasaparameterwi>0associatedwithit.Thenthepermutationsaregeneratedrandomlyasfollows:chooseoneofthenobjectstoberanked1atrandom whereobjectiischosentoberanked1withprobabilitywi/(Pnj=1wj).Leti1beobjectchosenfortheﬁrstposition.Nowtoselectsecondrankedobject choosefromremainingwithprobabilityproportionaltotheirweight.Werepeatuntilallobjectsforallrankedpositionsarechosen.Itcanbeeasilyseenthat asperthismodel anitemiisrankedhigherthanjwithprobabilitywi/(wi+wj).InthemixedMNLmodelwithr≥2mixturecomponents eachcomponentcorrespondstoadif-ferentMNLmodel:letw(1) ... w(r)bethecorrespondingparametersofthercomponents.Letq=[qa]∈[0 1]rdenotethemixturedistribution i.e.Paqa=1.Togenerateapermutationatrandom ﬁrstchooseacomponenta∈{1 ... r}withprobabilityqa andthendrawrandompermutationasperMNLwithparametersw(a).Briefhistory.TheMNLmodelisaninstanceofaclassofmodelsintroducedbyThurstone[23].ThedescriptionoftheMNLprovidedherewasformallyestablishedbyMcFadden[17].Thesamemodel(informofpair-wisemarginals)wasintroducedbyZermelo[25]aswellasBradleyandTerry[7]independently.In[16] LuceestablishedthatMNListheonlydistributionoverpermutationthatsatisﬁestheaxiomofIndependencefromIrrelevantAlternatives.Onlearningdistributionsoverpermutations thequestionoflearningsingleMNLmodelandmoregenerallyinstancesofThurstone’smodelhavebeenofinterestforquiteawhilenow.Themaximumlikelihoodestimator whichislogisticregressionforMNL hasbeenknowntobeconsistentinlargesamplelimit cf.[13].Recently RANKCENTRALITY[19]wasestablishedtobestatisticalefﬁcient.Forlearningsparsemixturemodel i.e.distributionoverpermutationswitheachmixturebeingdeltadistribution [11]providedsufﬁcientconditionsunderwhichmixturescanbelearntexactlyusingpair-wisemarginals–effectively aslongasthenumberofcomponentsscaledaso(logn)wherecomponentssatisﬁedappropriateincoherencecondition asimpleiterativealgorithmcouldrecoverthemixture.However itisnotrobustwithrespecttonoiseindataorﬁnitesampleerrorinmarginalestimation.Otherapproacheshavebeenproposedtorecovermodelusingconvexoptimizationbasedtechniques cf.[10 18].MNLmodelisaspecialcaseofalargerfamilyofdiscretechoicemodelsknownastheRandomUtilityModel(RUM) andanefﬁcientalgorithmtolearnRUMisintroducedin[22].EfﬁcientalgorithmsforlearningRUMsfrompartialrankingshasbeenintroducedin[3 4].Wenotethattheabovelistofreferencesisverylimited includingonlycloselyrelatedliterature.Giventhenatureofthetopic therearealotofexcitinglinesofresearchdoneoverthepastcenturyandweshallnotbeabletoprovidecomprehensivecoverageduetoaspacelimitation.Problem.GivenobservationsfromthemixedMNL wewishtolearnthemodelparameters themixingdistributionq andparametersofeachcomponentw(1) ... w(r).Theobservationsareinformofpair-wisecomparisons.Formally togenerateanobservation ﬁrstoneofthermixturecomponentsischosen;andthenfor‘ofallpossible(cid:0)n2(cid:1)pairs comparisonoutcomeisobservedasperthisMNLcomponent1.These‘pairsarechosen uniformlyatrandom fromapre-determinedN≤(cid:0)n2(cid:1)pairs:{(ik jk) 1≤k≤N}.WeshallassumethattheselectionofNissuchthattheundirectedgraphG=([n] E) whereE={(ik jk):1≤k≤N} isconnected.Weaskfollowingquestionsofinterest:IsitalwaysfeasibletolearnmixedMNL?Ifnot underwhatconditionsandhowmanysamplesareneeded?Howcomputationallyexpensivearethealgorithms?1Weshallassumethat outcomesofthese‘pairsareindependentofeachother butcomingfromthesameMNLmixturecomponent.ThisiseffectivelytrueeventheyweregeneratedbyﬁrstsamplingapermutationfromthechosenMNLmixturecomponent andthenobservingimplicationofthispermutationforthespeciﬁc‘pairs aslongastheyaredistinctduetotheIrrelevanceofIndependentAlternativehypothesisofLucethatissatisﬁedbyMNL.2Webrieﬂyrecallarecentresult[1]thatsuggeststhatitisimpossibletolearnmixedMNLmodelsingeneral.OnesuchexampleisdescribedinFigure1.Itdepictsanexamplewithn=4andr=2andauniformmixturedistribution.Fortheﬁrstcase inmixturecomponent1 withprobability1theorderingisa>b>c>d(wedenoten=4objectsbya b candd);andinmixturecomponent2 withprobability1theorderingisb>a>d>c.Similarlyforthesecondcase thetwomixturesaremadeupofpermutationsb>a>c>danda>b>d>c.Itiseasytoseethedistributionoverany3-wisecomparisonsgeneratedfromthesetwomixturemodelsisidentical.Therefore itisimpossibletodifferentiatethesetwousing3-wiseorpair-wisecomparisons.Ingeneral [1]establishedthatthereexistmixturedistributionswithr≤n/2overnobjectsthatareimpossibletodistinguishusinglogn-wisecomparisondata.Thatis learningmixedMNLisnotalwayspossible.Mixture Model 1ab>c>d>type 1ba>d>c>type 2ba>c>d>type 1ab>d>c>type 2ab>c>ba>c>ab>d>ba>d>ac>d>ad>c>bc>d>bd>c>Mixture Model 2LatentObservedP( ) = 0.5 P( ) = 0.5 P( ) = 0.5 P( ) = 0.5 P( ) = 0.5 P( ) = 0.5 P( ) = 0.5 P( ) = 0.5 Figure1:Twomixturemodelsthatcannotbedifferentiatedevenwith3-wisepreferencedata.Contributions.ThemaincontributionofthisworkisidentiﬁcationofsufﬁcientconditionsunderwhichmixedMNLmodelcanbelearntefﬁciently bothstatisticallyandcomputationally.Con-cretely weproposeatwo-phaselearningalgorithm:intheﬁrstphase usingatensordecompositionmethodforlearningmixtureofdiscreteproductdistribution weidentifypair-wisemarginalsasso-ciatedwitheachofthemixture;inthesecondphase weusethesepair-wisemarginalsassociatedwitheachmixturetolearntheparametersassociatedwitheachoftheMNLmixturecomponent.ThealgorithmintheﬁrstphasebuildsupontherecentworkbyJainandOh[12].Inparticular Theorem3generalizestheirworkforthesettingwhereforeachsample wehavelimitedinforma-tion-asper[12] wewouldrequirethateachindividualgivestheentirepermutation;instead wehaveextendedtheresulttobeabletocopewiththecurrentsettingwhenweonlyhaveinformationabout‘ potentiallyﬁnite pair-wisecomparisons.ThealgorithminthesecondphaseutilizesRANK-CENTRALITY[19].ItsanalysisinTheorem4worksforsettingwhereobservationsarenolongerindependent asrequiredin[19].Weﬁndthataslongascertainrankandincoherenceconditionsaresatisﬁedbytheparametersofeachofthemixture theabovedescribedtwophasealgorithmisabletolearnmixturedistributionqandparametersassociatedwitheachmixture w(1) ... w(r)faithfullyusingsamplesthatscalepolynomiallyinnandr–concretely thenumberofsamplesrequiredscaleasr3.5n3(logn)4withconstantsdependentontheincoherencebetweenmixturecomponents andaslongasr(cid:28)n2/7aswellasG thegraphofpotentialcomparisons isaspectralexpanderwiththetotalnumberofedgesscalingasN=O(nlogn).Fortheprecisestatement werefertoTheorem1.Thealgorithmsproposedareiterative andprimarilybasedonspectralpropertiesofunderlyingtensors/matriceswithprovable fastconvergenceguarantees.Thatis algorithmsarenotonlypoly-nomialtime theyarepracticalenoughtobescalableforhighdimensionaldatasets.Notations.Weuse[N]={1 ... N}fortheﬁrstNpositiveintegers.Weuse⊗todenotetheouterproductsuchthat(x⊗y⊗z)ijk=xiyjzk.GivenathirdordertensorT∈Rn1×n2×n3andamatrixU∈Rn1×r1 V∈Rn2×r2 W∈Rn3×r3 wedeﬁnealinearmappingT[U V W]∈Rr1×r2×r3asT[U V W]abc=Pi j kTijkUiaVjbWkc.Weletkxk=pPix2ibetheEuclideannormofavector kMk2=maxkxk≤1 kyk≤1xTMybetheoperatornormofamatrix andkMkF=qPi jM2ijbetheFrobeniusnorm.Wesayaneventhappenswithhighprobability(w.h.p)iftheprobabilityislowerboundedby1−f(n)suchthatf(n)=o(1)asnscalesto∞.2MainresultInthissection wedescribethemainresult:sufﬁcientconditionsunderwhichmixedMNLmodelscanbelearntusingtractablealgorithms.Weprovideausefulillustrationoftheresultaswellasdiscussitsimplications.3Deﬁnitions.LetSdenotethecollectionofobservations eachofwhichisdenotedasNdimensional {−1 0 +1}valuedvector.RecallthateachobservationisobtainedbyﬁrstselectingoneofthermixtureMNLcomponent andthenviewingoutcomes asperthechosenMNLmixturecomponent of‘randomlychosenpair-wisecomparisonsfromtheNpre-determinedcomparisons{(ik jk):1≤ik6=jk≤n 1≤k≤N}.Letxt∈{−1 0 +1}Ndenotethetthobservationwithxt k=0ifthekthpair(ik jk)isnotchosenamongstthe‘randomlychosenpairs andxt k=+1(respectively−1)ifik<jk(respectivelyik>jk)asperthechosenMNLmixturecomponent.Bydeﬁnition itiseasytoseethatforanyt∈Sand1≤k≤N E[xt k]=‘NhrXa=1qaPkai wherePka=w(a)jk−w(a)ikw(a)jk+w(a)ik.(1)WeshalldenotePa=[Pka]∈[−1 1]Nfor1≤a≤r.Therefore inavectorformE[xt]=‘NPq whereP=[P1...Pr]∈[−1 1]N×r.(2)Thatis Pisamatrixwithrcolumns eachrepresentingoneofthermixturecomponentsandqisthemixtureprobability.Byindependence foranyt∈S andanytwodifferentpairs1≤k6=m≤N E[xt kxt m]=‘2N2hrXa=1qaPkaPmai.(3)Therefore theN×NmatrixE[xtxTt]orequivalentlytensorE[xt⊗xt]isproportionaltoM2exceptindiagonalentries whereM2=PQPT≡rXa=1qa(Pa⊗Pa) (4)Q=diag(q)beingdiagonalmatrixwithitsentriesbeingmixtureprobabilities q.Inasimilarmanner thetensorE[xt⊗xt⊗xt]isproportionaltoM3(exceptinO(N2)entries) whereM3=rXa=1qa(Pa⊗Pa⊗Pa).(5)Indeed empiricalestimatesˆM2andˆM3 deﬁnedasˆM2=1|S|hXt∈Sxt⊗xti andˆM3=1|S|hXt∈Sxt⊗xt⊗xti (6)providegoodproxyforM2andM3forlargeenoughnumberofsamples;andshallbeutilizedcruciallyforlearningmodelparametersfromobservations.Sufﬁcientconditionsforlearning.Withtheabovediscussion westatesufﬁcientconditionsforlearningthemixedMNLintermsofpropertiesofM2:C1.M2hasrankr;letσ1(M2) σr(M2)>0bethelargestandsmallestsingularvaluesofM2.C2.ForalargeenoughuniversalconstantC0>0 N≥C0r3.5µ6(M2)(cid:16)σ1(M2)σr(M2)(cid:17)4.5.(7)Intheabove µ(M2)representsincoherenceofasymmetricmatrixM2.WerecallthatforasymmetricmatrixM∈RN×NofrankrwithsingularvaluedecompositionM=USUT theincoherenceisdeﬁnedasµ(M)=rNr(cid:16)maxi∈[N]kUik(cid:17).(8)C3.TheundirectedgraphG=([n] E)withE={(ik jk):1≤k≤N}isconnected.LetA∈{0 1}n×nbeadjacencymatrixwithAij=1if(i j)∈Eand0otherwise;letD=diag(di)withdibeingdegreeofvertexi∈[n]andletLG=D−1AbenormalizedLaplacianofG.Letdmax=maxidianddmin=minidi.LettheneigenvaluesofstochasticmatrixLGbe1=λ1(LG)≥...λn(LG)≥−1.DeﬁnespectralgapofG:ξ(G)=1−max{λ2(L) −λn(L)}.(9)4NotethatwechooseagraphG=([n] E)tocollectpairwisedataon andwewanttouseagraphthatisconnected hasalargespectralgap andhasasmallnumberofedges.Incondition(C3) weneedconnectivitysincewecannotestimatetherelativestrengthbetweendisconnectedcomponents(e.g.see[13]).Further itiseasytogenerateagraphwithspectralgapξ(G)boundedbelowbyauniversalconstant(e.g.1/100)andthenumberofedgesN=O(nlogn) forexampleusingtheconﬁgurationmodelforErd¨os-Renyigraphs.Incondition(C2) werequirethematrixM2tobesufﬁcientlyinco-herentwithboundedσ1(M2)/σr(M2).Forexample ifqmax/qmin=O(1)andtheproﬁleofeachtypeinthemixturedistributionissufﬁcientlydifferent i.e.hPa Pbi/(kPakkPbk)<1/(2r) thenwehaveµ(M2)=O(1)andσ1(M2)/σr(M2)=O(1).Wedeﬁneb=maxra=1maxi j∈[n]w(a)i/w(a)j qmax=maxaqa andqmin=minaqa.Thefollowingtheoremprovidesaboundontheerrorandwerefertotheappendixforaproof.Theorem1.ConsideramixedMNLmodelsatisfyingconditions(C1)-(C3).Thenforanyδ∈(0 1) thereexistspositivenumericalconstantsC C0suchthatforanypositiveεsatisfying0<ε<(cid:16)qminξ2(G)d2min16qmaxrσ1(M2)b5d2max(cid:17)0.5 (10)Algorithm1producesestimatesˆq=[ˆqa]andˆw=[ˆw(a)]sothatwithprobabilityatleast1−δ (cid:12)(cid:12)ˆqa−qa(cid:12)(cid:12)≤ε andkˆw(a)−w(a)kkw(a)k≤C(cid:16)rqmaxσ1(M2)b5d2maxqminξ2(G)d2min(cid:17)0.5ε (11)foralla∈[r] aslongas|S|≥C0rN4log(N/δ)qminσ1(M2)2ε2(cid:16)1‘2+σ1(M2)‘N+r4σ1(M2)4σr(M2)5(cid:17).(12)AnillustrationofTheorem1.TounderstandtheapplicabilityofTheorem1 consideraconcreteexamplewithr=2;letthecorrespondingweightsw(1)andw(2)begeneratedbychoosingeachweightuniformlyfrom[1 2].Inparticular therankorderforeachcomponentisauniformlyrandompermutation.Letthemixturedistributionbeuniformaswell i.e.q=[0.50.5].Finally letthegraphG=([n] E)bechosenaspertheErd¨os-R´enyimodelwitheachedgechosentobepartofthegraphwithprobability¯d/n where¯d>logn.Forthisexample itcanbecheckedthatTheorem1guaranteesthatforε≤C/√n¯d |S|≥C0n2¯d2log(n¯d/δ)/(‘ε2) andn¯d≥C0 wehaveforalla∈{1 2} |ˆqa−qa|≤εandkˆw(a)−w(a)k/kw(a)k≤C00√n¯dε.Thatis for‘=Θ(1)andchoosingε=ε0/(√n¯d) weneedsamplesizeof|S|=O(n3¯d3logn)toguaranteeerrorinbothˆqandˆwsmallerthanε0.Instead ifwechoose‘=Θ(n¯d) weonlyneed|S|=O((n¯d)2logn).Limitedsamplesperobservationleadstopenaltyoffactorof(n¯d/‘)insamplecomplexity.Toprovideboundsontheproblemparametersforthisexample weusestandardconcentrationarguments.ItiswellknownforErd¨os-R´enyirandomgraphs(see[6])that withhighprobability thenumberofedgesconcentratesin[(1/2)¯dn (3/2)¯dn]implyingN=Θ(¯dn) andthedegreesalsoconcentratein[(1/2)¯d (3/2)¯d] implyingdmax=dmin=Θ(¯d).Alsousingstandardconcentrationargumentsforspectrumofrandommatrices itfollowsthatthespectralgapofGisboundedbyξ≥1−(C/√¯d)=Θ(1)w.h.p.Sinceweassumetheweightstobein[1 2] thedynamicrangeisboundedbyb≤2.ThefollowingPropositionshowsthatσ1(M2)=Θ(N)=Θ(¯dn) σ2(M2)=Θ(¯dn) andµ(M2)=Θ(1).Proposition2.1.Fortheaboveexample when¯d≥logn σ1(M2)≤0.02N σ2(M2)≥0.017N andµ(M2)≤15withhighprobability.Supposennowforgeneralr weareinterestedinwell-behavedscenariowhereqmax=Θ(1/r)andqmin=Θ(1/r).Toachievearbitrarysmallerrorrateforkˆw(a)−w(a)k/kw(a)k weneed=O(1/√rN) whichisachievedbysamplesize|S|=O(r3.5n3(logn)4)with¯d=logn.3AlgorithmWedescribethealgorithmachievingtheboundinTheorem1.Ourapproachistwo-phased.First learnthemomentsformixturesusingatensordecomposition cf.Algorithm2:foreachtypea∈[r] 5produceestimateˆqa∈RofthemixtureweightqaandestimateˆPa=[ˆP1a...ˆPNa]T∈RNoftheexpectedoutcomePa=[P1a...PNa]Tdeﬁnedasin(1).Secondly foreacha usingtheestimateˆPa applyRANKCENTRALITY cf.Section3.2 toestimateˆw(a)fortheMNLweightsw(a).Algorithm11:Input:Samples{xt}t∈S numberoftypesr numberofiterationsT1 T2 graphG([n] E)2:{(ˆqa ˆPa)}a∈[r]←SPECTRALDIST({xt}t∈S r T1)(seeAlgorithm2)3:fora=1 ... rdo4:set˜Pa←P[−1 1](ˆPa)whereP[−1 1](·)istheprojectiononto[−1 1]N5:ˆw(a)←RANKCENTRALITY(cid:16)G ˜Pa T2(cid:17)(seeSection3.2)6:endfor7:Output:{(ˆq(a) ˆw(a))}a∈[r]ToachieveTheorem1 T1=Θ(cid:0)log(N|S|)(cid:1)andT2=Θ(cid:0)b2dmax(logn+log(1/ε))/(ξdmin)(cid:1)issufﬁcient.Next wedescribethetwophasesofalgorithmsandassociatedtechnicalresults.3.1Phase1:Spectraldecomposition.ToestimatePandqfromthesamples weshallusetensordecompositionofˆM2andˆM3 theempiricalestimationofM2andM3respectively recall(4)-(6).LetM2=UM2ΣM2UTM2betheeigenvaluedecompositionandletH=M3[UM2Σ−1/2M2 UM2Σ−1/2M2 UM2Σ−1/2M2].ThenexttheoremshowsthatM2andM3aresufﬁcienttolearnPandqexactly whenM2hasrankr(throughout weassumethatr(cid:28)n≤N).Theorem2(Theorem3.1[12]).LetM2∈RN×Nhaverankr.ThenthereexistsanorthogonalmatrixVH=[vH1vH2...vHr]∈Rr×randeigenvaluesλHa 1≤a≤r suchthattheorthogonaltensordecompositionofHisH=rXa=1λHa(vHa⊗vHa⊗vHa).LetΛH=diag(λH1 ... λHr).ThentheparametersofthemixturedistributionareP=UM2Σ1/2M2VHΛHandQ=(ΛH)−2.ThemainchallengeinestimatingM2(resp.M3)fromempiricaldataarethediagonalentires.In[12] alternatingminimizationapproachisusedformatrixcompletiontoﬁndthemissingdiagonalentriesofM2 andusedaleastsquaresmethodforestimatingthetensorHdirectlyfromthesamples.LetΩ2denotethesetofoff-diagonalindicesforanN×NmatrixandΩ3denotetheoff-diagonalentriesofanN×N×NtensorsuchthatthecorrespondingprojectionsaredeﬁnedasPΩ2(M)ij≡(cid:26)Mijifi6=j 0otherwise.andPΩ3(T)ijk≡(cid:26)Tijkifi6=j j6=k k6=i 0otherwise.forM∈RN×NandT∈RN×N×N.Inlieuofabovediscussion weshallusePΩ2(cid:0)ˆM2(cid:1)andPΩ3(cid:0)ˆM3(cid:1)toobtainestimationofdiag-onalentriesofM2andM3respectively.Tokeeptechnicalargumentssimple weshalluseﬁrst|S|/2samplesbasedˆM2 denotedasˆM2(cid:0)1 |S|2(cid:1)andsecond|S|/2samplesbasedˆM3 denotedbyˆM3(cid:0)|S|2+1 |S|(cid:1)inAlgorithm2.Next westatecorrectnessofAlgorithm2whenµ(M2)issmall;proofisinAppendix.Theorem3.Thereexistsuniversal strictlypositiveconstantsC C0>0suchthatforallε∈(0 C)andδ∈(0 1) if|S|≥C0rN4log(N/δ)qminσ1(M2)2ε2(cid:16)1‘2+σ1(M2)‘N+r4σ1(M2)4σr(M2)5(cid:17) andN≥C0r3.5µ6(cid:16)σ1(M2)σr(M2)(cid:17)4.5 6Algorithm2SPECTRALDIST:MomentmethodforMixtureofDiscreteDistribution[12]1:Input:Samples{xt}t∈S numberoftypesr numberofiterationsT2:˜M2←MATRIXALTMIN(cid:16)ˆM2(cid:0)1 |S|2(cid:1) r T(cid:17)(seeAlgorithm3)3:Computeeigenvaluedecompositionof˜M2=˜UM2˜ΣM2˜UTM24:˜H←TENSORLS(cid:16)ˆM3(cid:0)|S|2+1 |S|(cid:1) ˜UM2 ˜ΣM2(cid:17)(seeAlgorithm4)5:Computerank-rdecompositionPa∈[r]ˆλ˜Ha(ˆv˜Ha⊗ˆv˜Ha⊗ˆv˜Ha)of˜H usingRTPMof[2]6:Output:ˆP=˜UM2˜Σ1/2M2ˆV˜HˆΛ˜H ˆQ=(ˆΛ˜H)−2 whereˆV˜H=[ˆv˜H1...ˆv˜Hr]andˆΛ˜H=diag(λ˜H1 ... λ˜Hr)thenthereexistsapermutationπover[r]suchthatAlgorithm2achievesthefollowingboundswithachoiceofT=C0log(N|S|)foralli∈[r] withprobabilityatleast1−δ:|ˆqπi−qi|≤ε andkˆPπi−Pik≤εsrqmaxσ1(M2)qmin whereµ=µ(M2)deﬁnedin(8)withrun-timepoly(N r 1/qmin 1/ε log(1/δ) σ1(M2)/σr(M2)).Algorithm3MATRIXALTMIN:AlternatingMinimizationforMatrixCompletion[12]1:Input:ˆM2(cid:0)1 |S|2(cid:1) r T2:InitializeN×rdimensionalmatrixU0←top-reigenvectorsofPΩ2(ˆM2(cid:0)1 |S|2(cid:1))3:forallτ=1toT−1do4:ˆUτ+1=argminUkPΩ2(ˆM2(cid:0)1 |S|2(cid:1))−PΩ2(UUTτ)k2F5:[Uτ+1Rτ+1]=QR(ˆUτ+1)(standardQRdecomposition)6:endfor7:Output:˜M2=(ˆUT)(UT−1)TAlgorithm4TENSORLS:LeastSquaresmethodforTensorEstimation[12]1:Input:ˆM3(cid:0)|S|2+1 |S|(cid:1) ˆUM2 ˆΣM22:Deﬁneoperatorˆν:Rr×r×r→RN×N×Nasfollowsˆνijk(Z)=(PabcZabc(ˆUM2ˆΣ1/2M2)ia(ˆUM2ˆΣ1/2M2)jb(ˆUM2ˆΣ1/2M2)kc ifi6=j6=k6=i 0 otherwise.(13)3:DeﬁneˆA:Rr×r×r→Rr×r×rs.t.ˆA(Z)=ˆν(Z)[ˆUM2ˆΣ−1/2M2 ˆUM2ˆΣ−1/2M2 ˆUM2ˆΣ−1/2M2]4:Output:argminZkˆA(Z)−PΩ3(cid:0)ˆM3(cid:0)|S|2+1 |S|(cid:1)(cid:1)[ˆUM2ˆΣ−1/2M2 ˆUM2ˆΣ−1/2M2 ˆUM2ˆΣ−1/2M2]k2F3.2Phase2:RANKCENTRALITY.RecallthatE={(ik jk):ik6=jk∈[n] 1≤k≤N}representscollectionofN=|E|pairsandG=([n] E)isthecorrespondinggraph.Let˜PadenotetheestimationofPa=[Pka]∈[−1 1]Nforthemixturecomponenta 1≤a≤r;wherePkaisdeﬁnedasper(1).Foreacha usingGand˜Pa weshallusetheRANKCENTRALITY[19]toobtainestimationofw(a).Nextwedescribethealgorithmandguaranteesassociatedwithit.Withoutlossofgenerality wecanassumethatw(a)issuchthatPiw(a)i=1foralla 1≤a≤r.Giventhisnormalization RANKCENTRALITYestimatesw(a)asstationarydistributionofanappropriateMarkovchainonG.Thetransitionprobabilitiesare0forall(i j)/∈E.For(i j)∈E theyarefunctionof˜Pa.Speciﬁcally transitionmatrix˜p(a)=[˜p(a)i j]∈[0 1]n×nwith˜p(a)i j=0if7(i j)/∈E andfor(ik jk)∈Efor1≤k≤N ˜p(a)ik jk=1dmax(1+˜Pka)2and˜p(a)jk ik=1dmax(1−˜Pka)2 (14)Finally ˜p(a)i i=1−Pj6=i˜p(a)i jforalli∈[n].Let˜π(a)=[˜π(a)i]beastationarydistributionoftheMarkovchaindeﬁnedby˜p(a).Thatis ˜π(a)i=Xj˜p(a)ji˜π(a)jforalli∈[n].(15)Computationally wesuggestobtainingestimationof˜πbyusingpower-iterationforTiterations.Asarguedbefore cf.[19] T=Θ(cid:0)b2dmax(logn+log(1/ε))/(ξdmin)(cid:1) issufﬁcienttoobtainreasonablygoodestimationof˜π.Theunderlyingassumptionhereisthatthereisauniquestationarydistribution whichisestablishedbyourresultundertheconditionsofTheorem1.Now˜pisanapproximationoftheidealtransitionprobabilities wherep(a)=[p(a)i j]wherep(a)i j=0if(i j)/∈Eandp(a)i j∝w(a)j/(w(a)i+w(a)j)forall(i j)∈E.SuchanidealMarkovchainisreversibleandaslongasGisconnected(whichis inourcase bychoice) thestationarydistributionofthisidealchainisπ(a)=w(a)(recall wehaveassumedw(a)tobenormalizedsothatallitscomponentsupto1).Now˜p(a)isanapproximationofsuchanidealtransitionmatrixp(a).Inwhatfollows westateresultabouthowthisapproximationerrortranslatesintotheerrorbetween˜π(a)andw(a).Recallthatb≡maxi j∈[n]wi/wj dmaxanddminaremaximumandminimumvertexdegreesofGandξasdeﬁnedin(9).Theorem4.LetG=([n] E)benon-bipartiteandconnected.Letk˜p(a)−p(a)k2≤εforsomepositiveε≤(1/4)ξb−5/2(dmin/dmax).Then forsomepositiveuniversalconstantC k˜π(a)−w(a)kkw(a)k≤Cb5/2ξdmaxdminε.(16)And startingfromanyinitialcondition thepoweriterationmanagestoproduceanestimateof˜π(a)withintwicetheabovestatederrorboundinT=Θ(cid:0)b2dmax(logn+log(1/ε))/(ξdmin)(cid:1)iterations.ProofoftheaboveresultcanbefoundinAppendix.Forspectralexpander(e.g.connectedErdos-Renyigraphwithhighprobability) ξ=Θ(1)andthereforetheboundiseffectivelyO(ε)forboundeddynamicrange i.e.b=O(1).4DiscussionLearningdistributionoverpermutationsofnobjectsfrompartialobservationisfundamentaltomanydomains.Inthiswork wehaveadvancedunderstandingofthisquestionbycharacterizingsufﬁcientconditionsandassociatedalgorithmunderwhichitisfeasibletolearnmixedMNLmodelincomputationallyandstatisticallyefﬁcient(polynomialinproblemsize)mannerfrompartial/pair-wisecomparisons.Theconditionsarenatural–themixturecomponentsshouldbe“identiﬁable”givenpartialpreference/comparisondata–statedintermsoffullrankandincoherenceconditionsofthesecondmomentmatrix.Thealgorithmallowslearningofmixturecomponentsaslongasnumberofmixturecomponentsscaleo(n2/7)fordistributionoverpermutationsofnobjects.Tothebestofourknowledge thisworkprovidesﬁrstsuchsufﬁcientconditionforlearningmixedMNLmodel–aproblemthathasremainedopenineconometricsandstatisticsforawhile andmorerecentlyMachinelearning.Ourworknicelycomplementstheimpossibilityresultsof[1].Analytically ourworkadvancestherecentlypopularizedspectral/tensorapproachforlearningmix-turemodelfromlowerordermoments.Concretely weprovidemeanstolearnthecomponentevenwhenonlypartialinformationaboutthesampleisavailableunlikethepriorworks.Tolearnthemodelparameters onceweidentifythemomentsassociatedwitheachmixture weadvancethere-sultof[19]initsapplicability.Spectralmethodshavealsobeenappliedtorankinginthecontextofassortmentoptimizationin[5].8References[1]A.Ammar S.Oh D.Shah andL.Voloch.What’syourchoice?learningthemixedmulti-nomiallogitmodel.InProceedingsoftheACMSIGMETRICS/internationalconferenceonMeasurementandmodelingofcomputersystems 2014.[2]A.Anandkumar R.Ge D.Hsu S.M.Kakade andM.Telgarsky.Tensordecompositionsforlearninglatentvariablemodels.CoRR abs/1210.7559 2012.[3]H.AzariSouﬁani W.Chen D.CParkes andL.Xia.Generalizedmethod-of-momentsforrankaggrega-tion.InAdvancesinNeuralInformationProcessingSystems26 pages2706–2714.2013.[4]H.AzariSouﬁani D.Parkes andL.Xia.Computingparametricrankingmodelsviarank-breaking.InProceedingsofThe31stInternationalConferenceonMachineLearning pages360–368 2014.[5]J.Blanchet G.Gallego andV.Goyal.Amarkovchainapproximationtochoicemodeling.InEC pages103–104 2013.[6]B.Bollob´as.RandomGraphs.CambridgeUniversityPress January2001.[7]R.A.BradleyandM.E.Terry.Rankanalysisofincompleteblockdesigns:I.themethodofpairedcomparisons.Biometrika 39(3/4):324–345 1955.[8]E.J.Cand`esandB.Recht.Exactmatrixcompletionviaconvexoptimization.FoundationsofComputa-tionalMathematics 9(6):717–772 2009.[9]C.DavisandW.M.Kahan.Therotationofeigenvectorsbyaperturbation.iii.SIAMJournalonNumericalAnalysis 7(1):1–46 1970.[10]J.C.Duchi L.Mackey andM.I.Jordan.Ontheconsistencyofrankingalgorithms.InProceedingsoftheICMLConference Haifa Israel June2010.[11]V.F.Farias S.Jagabathula andD.Shah.Adata-drivenapproachtomodelingchoice.InNIPS pages504–512 2009.[12]P.JainandS.Oh.Learningmixturesofdiscreteproductdistributionsusingspectraldecompositions.arXivpreprintarXiv:1311.2972 2014.[13]L.R.FordJr.Solutionofarankingproblemfrombinarycomparisons.TheAmericanMathematicalMonthly 64(8):28–33 1957.[14]R.H.Keshavan A.Montanari andS.Oh.Matrixcompletionfromafewentries.InformationTheory IEEETransactionson 56(6):2980–2998 2010.[15]R.H.Keshavan A.Montanari andS.Oh.Matrixcompletionfromnoisyentries.TheJournalofMachineLearningResearch 99:2057–2078 2010.[16]D.R.Luce.IndividualChoiceBehavior.Wiley NewYork 1959.[17]D.McFadden.Conditionallogitanalysisofqualitativechoicebehavior.FrontiersinEconometrics pages105–142 1973.[18]I.Mitliagkas A.Gopalan C.Caramanis andS.Vishwanath.Userrankingsfromcomparisons:Learningpermutationsinhighdimensions.InCommunication Control andComputing(Allerton) 201149thAnnualAllertonConferenceon pages1143–1150.IEEE 2011.[19]S.Negahban S.Oh andD.Shah.Iterativerankingfrompair-wisecomparisons.InNIPS pages2483–2491 2012.[20]S.NegahbanandM.J.Wainwright.Restrictedstrongconvexityand(weighted)matrixcompletion:Op-timalboundswithnoise.JournalofMachineLearningResearch 2012.[21]P.Samuelson.Anoteonthepuretheoryofconsumers’behaviour.Economica 5(17):61–71 1938.[22]H.A.Souﬁani D.C.Parkes andL.Xia.Randomutilitytheoryforsocialchoice.InNIPS pages126–134 2012.[23]LouisLThurstone.Alawofcomparativejudgment.Psychologicalreview 34(4):273 1927.[24]J.Tropp.User-friendlytailboundsforsumsofrandommatrices.FoundationsofComputationalMathe-matics 2011.[25]E.Zermelo.Dieberechnungderturnier-ergebnissealseinmaximumproblemderwahrscheinlichkeit-srechnung.MathematischeZeitschrift 29(1):436–460 1929.9,Sewoong Oh
Devavrat Shah
Qizhe Xie
Zihang Dai
Yulun Du
Eduard Hovy
Graham Neubig