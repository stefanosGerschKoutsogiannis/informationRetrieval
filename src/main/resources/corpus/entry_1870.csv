2013,Estimation Bias in Multi-Armed Bandit Algorithms for Search Advertising,In search advertising  the search engine needs to select the most profitable advertisements to display  which can be formulated as an instance of online learning with partial feedback  also known as the stochastic multi-armed bandit (MAB) problem. In this paper  we show that the naive application of MAB algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and “estimation of the largest mean” (ELM) bias that harms the advertisers by increasing game-theoretic player-regret. We then propose simple bias-correction methods with benefits to both the search engine and the advertisers.,Estimation Bias in Multi-Armed Bandit Algorithms

for Search Advertising

Min Xu

Machine Learning Department
Carnegie Mellon University

minx@cs.cmu.edu

Tao Qin

Microsoft Research Asia

taoqin@microsoft.com

Tie-Yan Liu

Microsoft Research Asia

tie-yan.liu@microsoft.com

Abstract

In search advertising  the search engine needs to select the most proﬁtable adver-
tisements to display  which can be formulated as an instance of online learning
with partial feedback  also known as the stochastic multi-armed bandit (MAB)
problem. In this paper  we show that the naive application of MAB algorithms
to search advertising for advertisement selection will produce sample selection
bias that harms the search engine by decreasing expected revenue and “estima-
tion of the largest mean” (ELM) bias that harms the advertisers by increasing
game-theoretic player-regret. We then propose simple bias-correction methods
with beneﬁts to both the search engine and the advertisers.

1

Introduction

Search advertising  also known as sponsored search  has been formulated as a multi-armed bandit
(MAB) problem [11]  in which the search engine needs to choose one ad from a pool of candidate to
maximize some objective (e.g.  its revenue). To select the best ad from the pool  one needs to know
the quality of each ad  which is usually measured by the probability that a random user will click on
the ad. Stochastic MAB algorithms provide an attractive way to select the high quality ads  and the
regret guarantee on MAB algorithms ensures that we do not display the low quality ads too many
times.
When applied to search advertising  a MAB algorithm needs to not only identify the best ad (suppose
there is only one ad slot for simplicity) but also accurately learn the click probabilities of the top two
ads  which will be used by the search engine to charge a fair fee to the winner advertiser according
to the generalized second price auction mechanism [6]. If the probabilities are estimated poorly  the
search engine may charge too low a payment to the advertisers and lose revenue  or it may charge
too high a payment which would encourage the advertisers to engage in strategic behavior. However 
most existing MAB algorithms only focus on the identiﬁcation of the best arm; if naively applied to
search advertising  there is no guarantee to get an accurate estimation for the click probabilities of
the top two ads.
Thus  search advertising  with its special model and goals  merits specialized algorithmic design and
analysis while using MAB algorithms. Our work is a step in this direction. We show in particular that
naive ways of combining click probability estimation and MAB algorithms lead to sample selection
bias that harms the search engine’s revenue. We present a simple modiﬁcation to MAB algorithms
that eliminates such a bias and provably achieves almost the revenue as if an oracle gives us the
actual click probabilities. We also analyze the game theoretic notion of incentive compatibility (IC)

1

and show that low regret MAB algorithms may have worse IC property than high regret uniform
exploration algorithms and that a trade-off may be required.

2 Setting

Each time an user visits a webpage  which we call an impression  the search engine runs a gener-
alized second price (SP) auction [6] to determine which ads to show to the user and how much to
charge advertisers if their ads are clicked. We will in this paper suppose that we have only one ad
slot in which we can display one ad. The multiple slot setting is more realistic but also much more
complicated to analyze; we leave the extension as future work. In the single slot case  generalized
SP auction becomes simply the well known second price auction  which we describe below.
Assume there are n ads. Let bk denote the bid of advertiser k (or the ad k)  which is the maximum
amount of money advertiser k is willing to pay for a click  and ρk denote the click-through-rate
(CTR) of ad k  which is the probability a random user will click on it. SP auction ranks ads according
to the products of the ad CTRs and bids. Assume that advertisers are numbered by the decreasing
order of biρi: b1ρ1 > b2ρ2 > ··· > bnρn. Then advertiser 1 wins the ad slot  and he/she need to pay
b2ρ2/ρ1 for each click on his/her ad. This payment formula is chosen to satisfy the game theoretic
notion of incentive compatibility (see Chapter 9 of [10] for a good introduction). Therefore  the
per-impress expected revenue of SP auction is b2ρ2.

2.1 A Two-Stage Framework

Since the CTRs are unknown to both advertisers and the search engine  the search engine needs to
estimate them through some learning process. We adopt the same two-stage framework as in [12  2] 
which is composed by a CTR learning stage lasting for the ﬁrst T impressions and followed by a SP
auction stage lasting for the second Tend − T impressions.

1. Advertisers 1  ...  n submit bids b1  ...  bn.
2. CTR learning stage:

3. SP auction stage:

For each impression t = 1  ...  T   display ad kt ∈ {1  ...  n} using MAB algorithm M.

Estimate(cid:98)ρi based on the click records from previous stage.
For t = T + 1  ...  Tend  we run SP auction using estimators(cid:98)ρi: display ad that maximizes
bk(cid:98)ρk and charge b(2)(cid:98)ρ(2)
. Here we use (s) to indicate the ad with the s-th largest score bi(cid:98)ρi.
(cid:98)ρ(1)
One can see that in this framework  the estimators (cid:98)ρi’s are computed at the end of the ﬁrst stage

and keep unchanged in the second stage. Recent works [2] suggested one could also run the MAB
algorithm and keep updating the estimators until Tend. However  it is hard to compute a fair payment
when we display ads based using a MAB algorithm rather than the SP auction  and a randomized
payment is proposed in [2]. Their scheme  though theoretically interesting  is impractical because
it is difﬁcult for advertisers to accept a randomized payment rule. We thus adhere to the above

framework and do not update(cid:98)ρi’s in the second stage.
harm the expected revenue: (1) the ranking may be incorrect  i.e. arg maxk bk(cid:98)ρk (cid:54)= arg max bkρk 

It is important to note that in search advertising  we measure the quality of CTR estimators not by
mean-squared error but by criteria important to advertising. One criterion is to the per-impression
expected revenue (deﬁned below) in rounds T + 1  ...  Tend. Two types of estimation errors can

and (2) the estimators may be biased. Another criterion is incentive compatibility  which is a more
complicated concept and we defer its deﬁnition and discussion to Section 4. We do not analyze
the revenue and incentive compatibility properties of the ﬁrst CTR learning stage because of its
complexity and brief duration; we assume that Tend >> T .

Deﬁnition 2.1. Let (1) := arg maxk bk(cid:98)ρk  (2) := arg maxk(cid:54)=(1) bk(cid:98)ρk. We deﬁne the per-
impression empirical revenue as (cid:99)rev := ρ(1)
E[(cid:99)rev] where the expectation is taken over the CTR estimators. We deﬁne then the per-impression
expected revenue loss as b2ρ2 − E[(cid:99)rev]  where b2ρ2 is the oracle revenue we obtain if we know the

and the per-impression expected revenue as

b(2)(cid:98)ρ(2)
(cid:98)ρ(1)

true click probabilities.

2

Choice of Estimator We will analyze the most straightforward estimator (cid:98)ρk = Ck

where Tk is
the number of impression allocated to ad k in the CTR learning stage and Ck is the number of clicks
received by ad k in the CTR learning stage. This estimator is in fact biased and we will later propose
simple improvements.

Tk

2.2 Characterizing MAB Algorithms

We analyze two general classes of MAB algorithms: uniform and adaptive. Because there are many
speciﬁc algorithms for each class  we give our formal deﬁnitions by characterizing Tk  the number
of impressions assigned to each advertiser k at the end of the CTR learning stage.
Deﬁnition 2.2. We say that the learning algorithm M is uniform if  for some constant 0 < c < 1 

for all k  all bid vector b  with probability at least 1 − O(cid:0) n

(cid:1):

T

Tk ≥ c
n

T.

We next describe adaptive algorithm which has low regret because it stops allocating impressions to
ad k if it is certain that bkρk < maxk(cid:48) bk(cid:48)ρk(cid:48).
Deﬁnition 2.3. Let b be a bid vector. We say that a MAB algorithm is adaptive with respect to b 

if  with probability at least 1 − O(cid:0) n

(cid:1)  we have that:
(cid:19)

ln T

≥ Tk ≥ min

(cid:18)

(cid:18)

T

c(cid:48) b2
k
∆2
k

T1 ≥ cTmax

and

(cid:19)

cTmax 

4b2
k
∆2
k

ln T

for all k (cid:54)= 1

where ∆k = b1ρ1 − bkρk and c < 1  c(cid:48) are positive constants and Tmax = maxk Tk. For simplicity 
we assume that c here is the same as c in Deﬁnition 2.2  we can take the minimum of the two if they
are different.

Both the uniform algorithms and the adaptive algorithms have been used in the search advertising
auctions [5  7  12  2  8]. UCB (Uniform Conﬁdence Bound) is a simple example of an adaptive
algorithm.
Example 2.1. UCB Algorithm. The UCB algorithm  at round t  allocate the impression to the ad

with the largest score  which is deﬁned as sk t ≡ bk(cid:98)ρk t + γbk
where Tk(t) is the number of impressions ad k has received before round t and(cid:98)ρk t is the number

of clicks divided by Tk(t) in the history log before round t. γ is a tuning parameter that trades off
exploration and exploitation; the larger γ is  the more UCB resembles uniform algorithms. Some
version of UCB algorithm uses log t instead of log T in the score; this difference is unimportant and
we use the latter form to simplify the proof.

(cid:113) 1

Tk(t) log T .

Under the UCB algorithm  it is well known that the Tk’s satisfy the upper bounds in Deﬁnition 2.3.
That the Tk’s also satisfy the lower bounds is not obvious and has not been previously proved.
Previous analyses of UCB  whose goal is to show low regret  do not need any lower bounds on Tk’s;
our analysis does require a lower bound because we need to control the accuracy of the estimator
Theorem 2.1. Suppose we run the UCB algorithm with γ ≥ 4  then the Tk’s satisfy the bounds
described in Deﬁnition 2.3.

(cid:98)ρk. The following theorem is  to the best of our knowledge  a novel result.

The UCB algorithm in practice satisfy the lower bounds even with a smaller γ. We refer the readers
to Theorem 5.1 and Theorem 5.2 of Section 5.1 of the appendix for the proof.

As described in Section 2.1  we form estimators(cid:98)ρk by dividing the number of clicks by the number
of impressions Tk. The estimator(cid:98)ρk is not an average of Tk i.i.d Bernoulli random variables because
the size Tk is correlated with(cid:98)ρk. This is known as the sample selection bias.
Deﬁnition 2.4. We deﬁne the sample selection bias as E[(cid:98)ρk] − ρk.
We can still make the following concentration of measure statements about(cid:98)ρk  for which we give a

standard proof in Section 5.1 of the appendix.

3

Lemma 2.1. For any MAB learning algorithm  with probability at least 1 − O( n
1  ...  T   for all k = 1  ...  n  the conﬁdence bound holds.

ρk −(cid:112)(1/Tk(t)) log T ≤(cid:98)ρk t ≤ ρk +(cid:112)(1/Tk(t)) log T

T )  for all t =

2.3 Related Work

As mentioned before  how to design incentive compatible payment rules when using MAB algo-
rithms to select the best ads has been studied in [2] and [5]. However  their randomized payment
scheme is very different from the current industry standard and is somewhat impractical. The idea
of using MAB algorithms to simultaneously select ads and estimate click probabilities has proposed
in [11]  [8] and [13] . However  they either do not analyze estimation quality or do not analyze it
beyond a concentration of measure deviation bound. Our work in contrast shows that it is in fact the
estimation bias that is important in the game theoretic setting. [9] studies the effect of CTR learning
on incentive compatibility from the perspective of an advertiser with imperfect information.
This work is only the ﬁrst step towards understanding the effect of estimation bias in MAB algo-
rithms for search advertising auctions  and we only focus on a relative simpliﬁed setting with only
a single ad slot and without budget constraints  which is already difﬁcult to analyze. We leave the
extensions to multiple ad slots and with budget constraints as future work.

3 Revenue and Sample Selection Bias

In this section  we analyze the impact of a MAB algorithm on the search engine’s revenue. We show
that the direct plug-in of the estimators from a MAB algorithm (either unform or adaptive) will cause
the sample selection bias and damage the search engine’s revenue; we then propose a simple de-bias
method which can ensure the revenue guarantee. Throughout the section  we ﬁx a bid vector b. We

deﬁne the notations (1)  (2) as (1) := arg maxk(cid:98)ρkbk and (2) := arg maxk(cid:54)=(1)(cid:98)ρkbk.
(cid:98)ρk > ρk  then the UCB algorithm will select k more often and thus acquire more click data to
gradually correct the overestimation. If (cid:98)ρk < ρk however  the UCB algorithm will select k less

Before we present our main result  we pause to give some intuition about sample selection bias.
Assume b1ρ1 ≥ b2ρ2... ≥ bnρn and suppose we use the UCB algorithm in the learning stage. If

often and the underestimation persists. Therefore  E[ρk] < ρk.

3.1 Revenue Analysis

The following theorem is the main result of this section  which shows that the bias of the CTR
estimators can critically affect the search engine’s revenue.
Theorem 3.1. Let T0 := 4n
ρ2
1
4 nb2
c∆2
2
If T ≥ T0  then  for either adaptive or uniform algorithms 

log T . Let c be the constant introduced in Deﬁnition 2.3 and 2.2.

:= 5c(cid:48)(cid:16)(cid:80)

log T   and T unif
min

log T   T adpt
min

max(b2
∆2
k

(cid:17)

k(cid:54)=1

1 b2
k)

:=

max

ρ1

E[(cid:98)ρ1]

(cid:18)(cid:114) n

(cid:1) − O

(cid:18)(cid:0)b2ρ2 − b2E[(cid:98)ρ2]
(cid:18)
min or if we use uniform algorithms and T ≥ T unif
(b2ρ2 − b2E[(cid:98)ρ2]

(cid:19)
(cid:17)(cid:19)

(cid:17)(cid:19)

(cid:16) n

(cid:16) n

) − O

− O

log T

T

b2ρ2 − E[(cid:99)rev] ≤

.

T

min   then

ρ1

E[(cid:98)ρ1]

T

b2ρ2 − E[(cid:99)rev] ≤

If we use adaptive algorithms and T ≥ T adpt

We leave the full proof to Section 5.2 of the appendix and provide a quick sketch here. In the ﬁrst
case where T is smaller than thresholds T adpt
min   the probability of incorrect ranking  that is 
incorrectly identifying the best ad  is high and we can only use concentration of measure bounds to
control the revenue loss. In the second case  we show that we can almost always identify the best ad

min or T unif

and therefore  the(cid:112) n

T log T error term disappears.

4

The (b2ρ2−b2E[(cid:98)ρ2] ρ1E[(cid:98)ρ1] ) term in the theorem is in general positive because of sample selection bias.
(cid:17)
With bias  the best bound we can get on the expectation E[(cid:98)ρ2] is that |E[(cid:98)ρ2]−ρ2| ≤ O
fore  ρ1E[(cid:98)ρ1] is at most on the order of 1 +(cid:112) n
Combining these derivations  we get that b2ρ2 − E[(cid:99)rev] ≤ O(∆2) + O(cid:0) n

T log T and b2ρ2 − b2E[(cid:98)ρ2] is on the order of O(∆).
(cid:1). This bound suggests

which is through the concentration inequality (Lemma 2.1).
Remark 3.1. With adaptive learning  T1 is at least the order of O( n

that the revenue loss does not converge to 0 as T increases. Simulations in Section 5 show that
our bound is in fact tight: the expected revenue loss for adaptive learning  in presence of sample
selection bias  can be large and persistent.

log T ≥ ∆2
c(cid:48)b2

(cid:16)(cid:113) 1

T ) and 1
T2

. There-

2

2

T

log T

 

T2

For many common uniform learning algorithms (uniformly random selection for instance) sample
selection bias does not exist and so the expected revenue loss is smaller. This seems to suggest that 
because of sample selection bias  adaptive algorithms are  from a revenue optimization perspective 
inferior. The picture is switched however if we use a debiasing technique such as the one we propose
in section 3.2. When sample selection bias is 0  adaptive algorithms yield better revenue because
it is able to correctly identify the best advertisement with fewer rounds. We make this discussion
concrete with the following results in which we assume a post-learning unbiasedness condition.

Deﬁnition 3.1. We say that the post-learning unbiasedness condition holds if for all k  E[(cid:98)ρk] = ρk.

This condition does not hold in general  but we provide a simple debiasing procedure in Section 3.2
to ensure that it always does. The following Corollary follows immediately from Theorem 3.1 with
an application of Jensen’s inequality.
Corollary 3.1. Suppose the post-learning unbiasedness condition holds. Let T0 ≤ T adpt
min ≤ T unif
be deﬁned as in Theorem 3.1.

If we use either adaptive or uniform algorithms and T ≥ T0  then b2ρ2 − E[(cid:99)rev] ≤ O(cid:0)(cid:112) n

T log T(cid:1).

min

If we use adaptive algorithm and T ≥ T adpt

min or if we use uniform algorithm and T ≥ T unif
b2ρ2 − E[(cid:99)rev] ≤ O

min   then

(cid:16) n

(cid:17)

T

The revenue loss guarantee is much stronger with the unbiasedness  which we conﬁrm in our simu-
lations in Section 5.

Corollary 3.1 also shows that the revenue loss drops sharply from(cid:112) n

T once T is larger
than some threshold. Intuitively  this behavior exists because the probability of incorrect ranking be-
comes negligibly small when T is larger than the threshold. Because the adaptive learning threshold
T adpt
min is always smaller and often much smaller than the uniform learning threshold T unif
min   Corol-
lary 3.1 shows that adaptive learning can guarantee much lower revenue loss when T is between
T adpt
min and T unif
min . It is in fact the same adaptiveness that leads to low regret that also leads to the
strong revenue loss guarantees for adaptive learning algorithms.

T log T to n

3.2 Sample Selection Debiasing

Given a MAB algorithm  one simple meta-algorithm to produce an unbiased estimator where the
Tk’s still satisfy Deﬁnition 2.3 and 2.2 is to maintain “held-out” click history logs.
Instead of
keeping one history log for each advertisement  we will keep two; if the original algorithm allocates
one impression to advertiser k  we will actually allocate two impressions at a time and record the
click result of one of the impressions in the ﬁrst history log and the click result of the other in the
heldout history log.

When the MAB algorithm requires estimators(cid:98)ρk’s or click data to make an allocation  we will allow

it access only to the ﬁrst history log. The estimator learned from the ﬁrst history log is biased by
the selection procedure but the heldout history log  since it does not inﬂuence the ad selection  can
be used to output an unbiased estimator of each advertisement’s click probability at the end of the
exploration stage. Although this scheme doubles the learning length  sample selection debiasing can
signiﬁcantly improve the guarantee on expected revenue as shown in both theory and simulations.

5

4 Advertisers’ Utilities and ELM Bias

In this section  we analyze the impact of a MAB algorithm on advertisers’ utilities. The key re-
sult of this section is the adaptive algorithms can exacerbate the “estimation of the largest mean”
(ELM) bias  which arises because expectation of the maximum is larger than the maximum of the
expectation. This ELM bias will damage advertisers’ utilities because of overcharging.
We will assume that the reader is familiar with the concept of incentive compatbility and give only a
brief review. We suppose that there exists a true value vi  which exactly measures how much a click
is worth to advertiser i. The utility per impression of advertiser i in the auction is then ρi(vi − pi)
if the ad i is displayed where pi is the per-click payment charged by the search engine charges. An
auction mechanism is called incentive compatible if the advertisers maximize their own utility by
truthfully bidding: bi = vi. For auctions that are close but not fully incentive compatible  we also
deﬁne player-regret as the utility lost by advertiser i in truthfully bidding vi rather than a bid that
optimizes utility.

4.1 Player-Regret Analysis

(cid:16)

(cid:17)

ρk

(cid:98)ρk(bk)

vkρk − maxk(cid:48)(cid:54)=k bk(cid:48)(cid:98)ρk(cid:48) (bk)

We deﬁne v = (v1  ...  vn) to be the true per-click values of the advertisers. We will for
simplicity assume that the post-learning unbiasedness condition (Deﬁnition 3.1) holds for all
our results in this section. We introduce some formal deﬁnitions before we begin our anal-
ysis. For a ﬁxed vector of competing bids b−k  we deﬁne the player utility as uk(bk) ≡
Ibk(cid:98)ρk(bk)≥bk(cid:48)(cid:98)ρk(cid:48) (bk)∀k(cid:48)
  where Ibk(cid:98)ρk(bk)≥bk(cid:48)(cid:98)ρk(cid:48) (bk)∀k(cid:48) is a 0/1 func-
tion indicating whether the impression is allocated to ad k. We deﬁne the player-regret  with respect
E[uk(vk)]. It is important to note that we are hiding uk(bk)’s and(cid:98)ρk(bk)’s dependency on the com-
E[uk(bk)] −
to a bid vector b  as the player’s optimal gain in utility through false bidding supb
(cid:1). Suppose
Suppose bk∗ ρk∗ − v1ρ1 ≥ ω((cid:112) n
|v1ρ1 − bk∗ ρk∗| ≤ O((cid:112) n
Theorem 4.2. Suppose v1ρ1 − bk∗ ρk∗ ≥ ω(cid:0)(cid:112) n

peting bids b−k in our notation. Without loss of generality  we consider the utility of player 1. We
ﬁx b−1 and we deﬁne k∗ ≡ arg maxk(cid:54)=1 bkρk. We divide our analysis into cases  which cover the
different possible settings of v1 and competing bid b−1.
Theorem 4.1. The following holds for both uniform and adaptive algorithms.

Theorem 4.1 shows that when v1ρ1 is not much larger than bk∗ ρk∗  the player-regret is not too large.
The next Theorem shows that when v1ρ1 is much larger than bk∗ ρk∗ however  the player-regret can
be large.

E[u1(b1)] − E[u1(v1)] ≤ O(cid:0)(cid:112) n
T log T(cid:1)  then  for both uniform and adaptive
(cid:16) n
(cid:17)(cid:17)
0  E[b(2)(v1)(cid:98)ρ(2)(v1)] − E[b(2)(b1)(cid:98)ρ(2)(b1)] + O

E[u1(b1)] − E[u1(v1)] ≤ O(cid:0) n
T log T(cid:1).

algorithms:
∀b1  E[u1(b1  b−1)]−E[u1(v1  b−1)] ≤ max

T log T )  then  supb1

T log T )   then supb1

(cid:16)

T

T

supb1

We give the proofs of both Theorem 4.1 and 4.2 in Section 5.3 of the appendix.

Remark 4.1. In the special case of only two advertisers  it must be that (2) = 2 and therefore

Both expectations E[b(2)(v1)(cid:98)ρ(2)(v1)] and E[b(2)(b1)(cid:98)ρ(2)(b1)] can be larger than b2ρ2 because the
E[maxk(cid:54)=1 bk(cid:98)ρk(v1)] ≥ maxk(cid:54)=1 bkE[(cid:98)ρk(v1)].
E[b(2)(v1)(cid:98)ρ(2)(v1)] = b2ρ2 and E[b(2)(v1)(cid:98)ρ(2)(v1)] = b2ρ2. The player-regret is then very small:
(cid:1).
E[u1(b1  b2)] − E[u1(v1  b2)] ≤ O(cid:0) n
cause the bias E[b(2)(b1)(cid:98)ρ(2)(b1)] − b2ρ2 increases when T2(b1)  ...  Tn(b1) are low–that is  it in-
creases when the variance of (cid:98)ρk(b1)(cid:48)s are high. An omniscient advertiser 1  with the belief that
rithm to allocate more rounds to advertisers 2  ..  n and reduce the variance of (cid:98)ρk(b1)(cid:48)s. Such a

v1ρ1 >> b2ρ2  can thus increase his/her utility by underbidding to manipulate the learning algo-

The incentive can be much larger when there are more than 2 advertisers. Intuitively  this is be-

strategy will give advertiser 1 negative utility in the learning CTR learning stage  but it will yield
positive utility in the longer SP auction stage and thus give an overall increase to the player utility.

T

6

(cid:19)

log T

T

sup
b1

In the case of uniform learning  the advertiser’s manipulation is limited because the learning algo-
rithm is not signiﬁcantly affected by the bid.
Suppose that v1ρ1 − bk∗ ρk∗ ≥
Corollary 4.1. Let the competing bid vector b−1 be ﬁxed.

T log T(cid:1). If uniform learning is used in the ﬁrst stage  we have that
(cid:18)(cid:114) n

ω(cid:0)(cid:112) n
Nevertheless  by contrasting this with(cid:112) n
utility by bidding some b1 smaller than v1 but still large enough to ensure that b1(cid:98)ρ1(b1) still be

T bound we would get in the two
advertiser case  we see the negative impact of ELM bias on incentive compatibility. The negative
effect is even more pronounced in the case of adaptive learning. Advertiser 1 can increase its own

E[u1(b1  b−1)] − E[u1(v1  b−1)] ≤ O

ranked the highest at the end of the learning stage. We explain this intuition with more details in the
following example  which we also simulate in Section 5.
Example 4.1. Suppose we have n advertisers and b2ρ2 = b3ρ3 = ...bnρn. Suppose that v1ρ1 >>
b2ρ2 and we will show that advertiser 1 has the incentive to underbid.
Let ∆k(b1) ≡ b1ρ1 − bkρk  then ∆k(b1)’s are the same for all k and ∆k(v1) >> 0 by previous
supposition. Suppose advertiser 1 bids b1 < v1 but where ∆k(b1) >> 0 still. We assume that
for all k = 2  ...  n  which must hold for large T by deﬁnition of adaptive
Tk(b1) = Θ
learning.
From Lemma 5.4 in the appendix  we know that

T log T bound with the n

(cid:16) log T

∆k(b1)2

(cid:17)

(cid:115)

(cid:115)

E[b(2)(b1)(cid:98)ρ(2)(b1)] − b2ρ2 ≤

log(n − 1)

≤

log(n − 1)

(b1ρ1 − bkρk)

(4.1)

The Eqn. (4.1) is an upper bound but numerical experiments easily show that E[b(2)(b1)(cid:98)ρ(2)(b1)] is
From Eqn. (4.1)  we derive that  for any b1 such that b1ρ1 − b2ρ2 ≥ ω(cid:0)(cid:112) n

in fact on the same order as the RHS of Eqn. (4.1).

log T

Tk

(cid:32)(cid:115)

T log T(cid:1):
(cid:33)

E[u1(b1  b−1)] − E[u1(v1  b−1)] ≤ O

log(n − 1)

log T

(v1ρ1 − bρ1)

Thus  we cannot guarantee that the mechanism is approximately truthful. The bound decreases with
T at a very slow logarithmic rate because with adaptive algorithm  a longer learning period T might

not reduce the variances of many of the estimators(cid:98)ρk’s.

We would like to at this point brieﬂy compare our results with that of [9]  which shows  under an
imperfect information deﬁnition of utility  that advertisers have an incentive to overbid so that the
their CTRs can be better learned by the search engine. Our results are not contradictory since we
show that only the leading advertiser have an incentive to underbid.

4.2 Bias Reduction in Estimation of the Largest Mean

The previous analysis shows that the incentive-incompatibility issue in the case of adaptive learning

is caused by the fact that the estimator b(2)(cid:98)ρ(2) = maxk(cid:54)=1 b2(cid:98)ρ2 is upward biased. E[b(2)(cid:98)ρ(2)] is
much larger than b2ρ2 in general even if the individual estimators(cid:98)ρk’s are unbiased. We can abstract
of the Largest Mean” (ELM): given N probabilities {ρk}k=1 ... N   ﬁnd an estimator(cid:98)ρmax such that
E[(cid:98)ρmax] = maxk ρk. Unfortunately  as proved by [4] and [3]  unbiased estimator for the largest

out the game theoretic setting and distill a problem known in the statistics literature as “Estimation

mean does not exist for many common distributions including the Gaussian  Binomial  and Beta; we
thus survey some methods for reducing the bias.
[3] studies techniques that explicitly estimate and then substract the bias. Their method  though
interesting  is speciﬁc to the case of selecting the larger mean among only two distributions. [1]

7

through history into two sets S  E and get two estimators (cid:98)ρS
k   (cid:98)ρE
k . We then use (cid:98)ρS
and output a weighted average λ(cid:98)ρS

proposes a different approach based on data-splitting. We randomly partition the data in the click-
k for selection
k for estimating the value
because  without conditioning on a speciﬁc selection  it is downwardly biased. We unfortunately
know of no principled way to choose λ. We implement this scheme with λ = 0.5 and show in
simulation studies in Section 5 that it is effective.

k + (1 − λ)(cid:98)ρE

k . We cannot use only(cid:98)ρE

5 Simulations

We simulate our two stage framework for various values of T . Figures 1a and 1b show the effect
of sample selection debiasing (see Section 3  3.2) on the expected revenue where one uses adaptive
learning.
(the UCB algorithm 2.1 in our experiment) One can see that selection bias harms the
revenue but the debiasing method described in Section 3.2  even though it holds out half of the click
data  signiﬁcantly lowers the expected revenue loss  as theoretically shown in Corollary 3.1. We
choose the tuning parameter γ = 1. Figure 1c shows that when there are a large number of poor
quality ads  low regret adaptive algorithms indeed achieve better revenue in much fewer rounds of
learning. Figure 1d show the effect of estimation-of-the-largest-mean (ELM) bias on the utility gain
of the advertiser. We simulate the setting of Example 4.1 and we see that without ELM debiasing 
the advertiser can noticeably increase utility by underbidding. We implement the ELM debiasing
technique described in Section 4.2; it does not completely address the problem since it does not
completely reduce the bias (such a task has been proven impossible)  but it does ameliorate the
problem–the increase in utility from underbidding has decreased.

(a) n = 2  ρ1 = 0.09  ρ2 = 0.1  b1 = 2  b2 = 1 (b) n = 2  ρ1 = .3  ρ2 = 0.1  b1 = 0.7  b2 = 1

(c) n = 42  ρ1 = .2  ρ2 = 0.15  b1 = 0.8 
b2 = 1. All other bk = 1  ρk = 0.01.

(d) n = 5  (cid:126)ρ = {0.15  0.11  0.1  0.05  01} 
(cid:126)b−1 = {0.9  1  2  1}

Figure 1: Simulation studies demonstrating effect of sample selection debiasing and ELM debiasing.
The revenue loss in ﬁgures a to c is relative and is measured by 1 − revenue
; negative loss indicate
revenue improvement over oracle SP. Figure d shows advertiser 1’s utility gain as a function of
possible bids. The vertical dotted black line denote the advertiser’s true value at v = 1. Utility gain
utility(v) − 1; higher utility gain implies that advertiser 1 can beneﬁt more
is relative and deﬁned as utility(b)
from strategic bidding. The expected value is computed over 500 simulated trials.

b2ρ2

8

050001000015000−0.0500.050.1Rounds of ExplorationExpected Revenue Loss no selection debiasingwith selection debiasing050001000015000−0.0500.050.1Rounds of ExplorationExpected Revenue Loss no selection debiasingwith selection debiasing00.511.522.5x 10400.10.20.30.40.5Rounds of ExplorationExpected Revenue Loss uniformadaptive with debiasing0.80.911.11.2−0.1−0.0500.050.1Bid pricePlayer Utility Gain no ELM debiasingwith ELM debiasingReferences
[1] K. Alam. A two-sample estimate of the largest mean. Annals of the Institute of Statistical

Mathematics  19(1):271–283  1967.

[2] M. Babaioff  R.D. Kleinberg  and A. Slivkins. Truthful mechanisms with implicit payment

computation. arXiv preprint arXiv:1004.3630  2010.

[3] S. Blumenthal and A. Cohen. Estimation of the larger of two normal means. Journal of the

American Statistical Association  pages 861–876  1968.

[4] Bhaeiyal Ishwaei D  D. Shabma  and K. Krishnamoorthy. Non-existence of unbiased esti-
mators of ordered parameters. Statistics: A Journal of Theoretical and Applied Statistics 
16(1):89–95  1985.

[5] N.R. Devanur and S.M. Kakade. The price of truthfulness for pay-per-click auctions.
Proceedings of the tenth ACM conference on Electronic commerce  pages 99–106  2009.

In

[6] Benjamin Edelman  Michael Ostrovsky  and Michael Schwarz. Internet advertising and the
generalized second price auction: Selling billions of dollars worth of keywords. Technical
report  National Bureau of Economic Research  2005.

[7] N. Gatti  A. Lazaric  and F. Trov`o. A truthful learning mechanism for contextual multi-slot
sponsored search auctions with externalities. In Proceedings of the 13th ACM Conference on
Electronic Commerce  pages 605–622. ACM  2012.

[8] R. Gonen and E. Pavlov. An incentive-compatible multi-armed bandit mechanism. In Pro-
ceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing 
pages 362–363. ACM  2007.

[9] S.M. Li  M. Mahdian  and R. McAfee. Value of learning in sponsored search auctions. Internet

and Network Economics  pages 294–305  2010.

[10] Noam Nisan  Tim Roughgarden  Eva Tardos  and Vijay V Vazirani. Algorithmic game theory.

Cambridge University Press  2007.

[11] Sandeep Pandey and Christopher Olston. Handling advertisements of unknown quality in

search advertising. Advances in Neural Information Processing Systems  19:1065  2007.

[12] A.D. Sarma  S. Gujar  and Y. Narahari. Multi-armed bandit mechanisms for multi-slot spon-

sored search auctions. arXiv preprint arXiv:1001.1414  2010.

[13] J. Wortman  Y. Vorobeychik  L. Li  and J. Langford. Maintaining equilibria during exploration

in sponsored search auctions. Internet and Network Economics  pages 119–130  2007.

9

,Min Xu
Tao Qin
Tie-Yan Liu
Kihyuk Sohn
Wenling Shang
Honglak Lee