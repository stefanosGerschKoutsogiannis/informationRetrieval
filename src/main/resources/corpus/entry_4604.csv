2018,Recurrent Transformer Networks for Semantic Correspondence,We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair  rather than employing spatial transformer networks to independently normalize each individual image  we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition  a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs  state-of-the-art performance is attained on several benchmarks for semantic correspondence.,Recurrent Transformer Networks for Semantic

Correspondence

Seungryong Kim1  Stephen Lin2  Sangryul Jeon1  Dongbo Min3  and Kwanghoon Sohn1 ∗

1Yonsei University  Seoul  South Korea  2Microsoft Research  Beijing  China 

dbmin@ewha.ac.kr
∗Corresponding author

3Ewha Womans University  Seoul  South Korea

{srkim89 cheonjsr khsohn}@yonsei.ac.kr  stevelin@microsoft.com 

Abstract

We present recurrent transformer networks (RTNs) for obtaining dense correspon-
dences between semantically similar images. Our networks accomplish this through
an iterative process of estimating spatial transformations between the input images
and using these transformations to generate aligned convolutional activations. By
directly estimating the transformations between an image pair  rather than employ-
ing spatial transformer networks to independently normalize each individual image 
we show that greater accuracy can be achieved. This process is conducted in a
recursive manner to reﬁne both the transformation estimates and the feature repre-
sentations. In addition  a technique is presented for weakly-supervised training of
RTNs that is based on a proposed classiﬁcation loss. With RTNs  state-of-the-art
performance is attained on several benchmarks for semantic correspondence.

1

Introduction

Establishing dense correspondences across semantically similar images can facilitate a variety of
computer vision applications including non-parametric scene parsing  semantic segmentation  object
detection  and image editing [25  22  19]. In this semantic correspondence task  the images resemble
each other in content but differ in object appearance and conﬁguration  as exempliﬁed in the images
with different car models in Fig. 1(a-b). Unlike the dense correspondence computed for estimating
depth [34] or optical ﬂow [4]  semantic correspondence poses additional challenges due to intra-class
appearance and shape variations among different instances from the same object or scene category.
To address these challenges  state-of-the-art methods generally extract deep convolutional neural
network (CNN) based descriptors [5  44  18]  which provide some robustness to appearance variations 
and then perform a regularization step to estimate spatially regularized geometric ﬁelds. The most
recent techniques handle geometric deformations in addition to appearance variations within deep
CNNs. These methods can generally be classiﬁed into two categories  namely methods for geometric
invariance in the feature extraction step  e.g.  spatial transformer networks (STNs) [15  5  19]  and
methods for geometric invariance in the regularization step  e.g.  geometric matching networks [30 
31]. The STN-based methods infer geometric deformation ﬁelds within a deep network and transform
the convolutional activations to provide geometric-invariant features [5  41  19]. While this approach
has shown geometric invariance to some extent  we conjecture that directly estimating the geometric
deformations between a pair of input images would be more robust and precise than learning to
transform each individual image to a geometric-invariant feature representation. This direct estimation
approach is used by geometric matching-based techniques [30  31]  which recover a matching model
directly through deep networks. Drawbacks of these methods include that globally-varying geometric
ﬁelds are inferred  and only ﬁxed  untransformed versions of the features are used.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 1: Visualization of results from RTNs: (a) source image; (b) target image; (c)  (d) warped
source and target images using dense correspondences from RTNs; (e)  (f) pseudo ground-truth
transformations in [36]. Our RTNs learn to infer transformations without ground-truth supervision.

In this paper  we present recurrent transformer networks (RTNs) for overcoming the aforementioned
limitations of current semantic correspondence techniques. As illustrated in Fig. 2  the key idea
of RTNs is to directly estimate the geometric transformation ﬁelds between two input images  like
what is done by geometric matching-based approaches [30  31]  but also apply the estimated ﬁeld to
transform the convolutional activations of one of the images  similar to STN-based methods [15  5  19].
We additionally formulate the RTNs to recursively estimate the geometric transformations  which are
used for iterative geometric alignment of feature activations. In this way  regularization is enhanced
through recursive reﬁnement  while feature extraction is likewise iteratively reﬁned according to the
geometric transformations as well as jointly learned with the regularization. Moreover  the networks
are learned in a weakly-supervised manner via a proposed classiﬁcation loss deﬁned between the
source image features and the geometrically-aligned target image features  such that the correct
transformation is identiﬁed by the highest matching score while other transformations are considered
as negative examples.
The presented approach is evaluated on several common benchmarks and examined in an ablation
study. The experimental results show that this model outperforms the latest weakly-supervised and
even supervised methods for semantic correspondence.

2 Related Work

Semantic Correspondence To elevate matching quality  most conventional methods for semantic
correspondence focus on improving regularization techniques while employing handcrafted features
such as SIFT [27]. Liu et al. [25] pioneered the idea of dense correspondence across different scenes 
and proposed SIFT ﬂow. Inspired by this  methods have been presented based on deformable spatial
pyramids (DSP) [17]  object-aware hierarchical graphs [39]  exemplar LDA [3]  joint image set
alignment [45]  and joint co-segmentation [36]. As all of these techniques use handcrafted descriptors
and regularization methods  they lack robustness to geometric deformations.
Recently  deep CNN-based methods have been used in semantic correspondence as their descriptors
provide some degree of invariance to appearance and shape variations. Among them are techniques
that utilize a 3-D CAD model for supervision [44]  employ fully convolutional feature learning [5] 
learn ﬁlters with geometrically consistent responses across different object instances [28]  learn
networks using dense equivariant image labelling [37]  exploit local self-similarity within a fully
convolutional network [18  19]  and estimate correspondences using object proposals [7  8  38].
However  none of these methods is able to handle non-rigid geometric variations  and most of
them are formulated with handcrafted regularization. More recently  Han et al. [9] formulated the
regularization into the CNN but do not deal explicitly with the signiﬁcant geometric variations
encountered in semantic correspondence.

Spatial Invariance Some methods aim to alleviate spatial variation problems in semantic corre-
spondence through extensions of SIFT ﬂow  including scale-less SIFT ﬂow (SLS) [11]  scale-space
SIFT ﬂow (SSF) [29]  and generalized DSP (GDSP) [13]. A generalized PatchMatch algorithm [1]
was proposed for efﬁcient matching that leverages a randomized search scheme. It was utilized by
HaCohen et al. [6] in a non-rigid dense correspondence (NRDC) algorithm. Spatial invariance to scale
and rotation is provided by DAISY ﬁlter ﬂow (DFF) [40]. While these aforementioned techniques
provide some degree of geometric invariance  none of them can deal with afﬁne transformations over
an image. Recently  Kim et al. [20  21] proposed the discrete-continuous transformation matching

2

(a)

(b)

(c)

Figure 2: Intuition of RTNs: (a) methods for geometric invariance in the feature extraction step 
e.g.  STN-based methods [5  19]  (b) methods for geometric invariance in the regularization step 
e.g.  geometric matching-based methods [30  31]  and (c) RTNs  which weave the advantages of
both existing STN-based methods and geometric matching techniques  by recursively estimating
geometric transformation residuals using geometry-aligned feature activations.

(DCTM) framework where dense afﬁne transformation ﬁelds are inferred using a hand-designed
energy function and regularization.
To deal with geometric variations within CNNs  STNs [15] offer a way to provide geometric invari-
ance by warping features through a global transformation. Inspired by STNs  Lin et al. [23] proposed
inverse compositional STNs (IC-STNs) that replaces the feature warping with transformation pa-
rameter propagation. Kanazawa et al. [16] presented WarpNet that predicts a warp for establishing
correspondences. Rocco et al. [30  31] proposed a CNN architecture for estimating a geometric
matching model for semantic correspondence. However  they estimate only globally-varying geomet-
ric ﬁelds  thus leading to limited performance in dealing with locally-varying geometric deformations.
To deal with locally-varying geometric variations  some methods such as UCN-spatial transformer
(UCN-ST) [5] and convolutional afﬁne transformer-FCSS (CAT-FCSS) [19] employ STNs [15] at
the pixel level. Similarly  Yi et al. [41] proposed the learned invariant feature transform (LIFT) to
learn sparsely  locally-varying geometric ﬁelds  inspired by [42]. However  these methods determine
geometric ﬁelds by accounting for the source and target images independently  rather than jointly 
which limits their prediction ability.

3 Background

i and I t

i and Dt

i  from I s

Let us denote semantically similar source and target images as I s and I t  respectively. The objective
is to establish a correspondence ﬁeld fi = [ui  vi]T between the two images that is deﬁned for each
pixel i = [ix  iy]T in I s. Formally  this involves ﬁrst extracting handcrafted or deep features  denoted
i within local receptive ﬁelds  and then estimating the correspondence
by Ds
ﬁeld fi of the source image by maximizing the feature similarity between Ds
over a set of
transformations using handcrafted or deep geometric regularization models. Several approaches [25 
18] assume the transformation to be a 2-D translation with negligible variation within local receptive
ﬁelds. As a result  they often fail to handle complicated deformations caused by scale  rotation  or skew
that may exist among object instances. For greater geometric invariance  recent approaches [20  21]
have modeled the deformations as an afﬁne transformation ﬁeld represented by a 2 × 3 matrix

i and Dt

i+fi

Ti = [Ai  fi]

(1)
i and
i(cid:48)(Ai)  where D(Ai) represents the feature extracted from spatially-varying local receptive

that maps pixel i to i(cid:48) = i + fi. Speciﬁcally  they maximize the similarity between the source Ds
target Dt
ﬁelds transformed by a 2 × 2 matrix Ai [5  19]. For simplicity  we denote Dt(Ti) = Dt
Approaches for geometric invariance in semantic correspondence can generally be classiﬁed into two
categories. The ﬁrst group infers the geometric ﬁelds in the feature extraction step by minimizing
a matching objective function [5  19]  as exempliﬁed in Fig. 2(a). Concretely  Ai is learned
without a ground-truth A∗
i by minimizing the difference between Ds
(Ai) according to a
ground-truth ﬂow ﬁeld f∗
i . This enables explicit feature learning which aims to minimize/maximize
convolutional activation differences between matching/non-matching pixel pairs [5  19]. However 
ground-truth ﬂow ﬁelds f∗
i are still needed for learning the networks  and it predicts the geometric

i and Dt

(Ai).

i+fi

i+fi

3

Feature ExtractionFeature ExtractionFlow EstimationLocalisationifiAsiD()tiDASourceTargetFeature ExtractionFeature ExtractionGeometric MatchingsiDtiDiTSourceTargetFeature ExtractionFeature ExtractionGeometric MatchingsiD()tiDTiTSourceTargetFigure 3: Network conﬁguration of RTNs  consisting of a feature extraction network and a geometric
matching network in a recurrent structure.

ﬁelds Ai based only on the source or target feature  without jointly considering the source and target 
thus limiting performance.
The second group estimates a geometric matching model directly through deep networks by consider-
ing the source and target features simultaneously [30  31]. These methods formulate the geometric
matching networks by mimicking conventional RANSAC-like methods [14] through feature extrac-
tion and geometric matching steps. As illustrated in Fig. 2(b)  the geometric ﬁelds Ti are predicted
in a feed-forward network from extracted source features Ds
i. By learning
to extract source and target features and predict geometric ﬁelds in an end-to-end manner  more
robust geometric ﬁelds can be estimated compared to existing STN-based methods that consider
source or target features independently as shown in [31]. A major limitation of these learning-based
methods is the lack of ground-truth geometric ﬁelds T∗
i between source and target images. To
alleviate this problem  some methods use self-supervision such as synthetic transformations [30] or
weak-supervision such as soft-inlier maximization [31]  but these approaches constrain the global
geometric ﬁeld only. Moreover  these methods utilize feature descriptors extracted from the original
upright images  rather than from geometrically transformed images  which limits their capability to
represent severe geometric variations.

i and target features Dt

4 Recurrent Transformer Networks

4.1 Motivation and Overview

In this section  we describe the formulation of recurrent transformer networks (RTNs). The objective
of our networks is to learn and infer locally-varying afﬁne deformation ﬁelds Ti in an end-to-end
and weakly-supervised fashion using only image pairs without ground-truth transformations T∗
i .
Toward this end  we present an effective and efﬁcient integration of the two existing approaches for
geometric invariance  i.e.  STN-based feature extraction networks [5  19] and geometric matching
networks [30  31]  that includes a novel weakly-supervised loss function tailored to our objective.
Speciﬁcally  the ﬁnal geometric ﬁeld is recursively estimated by deforming the activations of feature
extraction networks according to the intermediate output of the geometric matching networks  in
contrast to existing approaches based on geometric matching which consider only ﬁxed  upright
versions of features [30  31]. At the same time  our method outperforms STN-based approaches [5 
19] by using a deep CNN-based geometric matching network instead of handcrafted matching
criteria. Our recurrent geometric matching approach intelligently weaves the advantages of both
existing STN-based methods and geometric matching techniques  by recursively estimating geometric
transformation residuals using geometry-aligned feature activations.
Concretely  our networks are split into two parts  as shown in Fig. 3: a feature extraction network
i and target Dt(Ti) features  and a geometric matching network to infer the
to extract source Ds
geometric ﬁelds Ti. To learn these networks in a weakly-supervised manner  we formulate a novel
classiﬁcation loss deﬁned without ground-truth T∗
i based on the assumption that the transformation
which maximizes the similarity of the source features Ds
i and transformed target features Dt(Ti) at
a pixel i should be correct  while the matching scores of other transformation candidates should be
minimized.

4

SourceTargetFeature Extraction Net.Geometric Matching NetworkEncoderDecoder1kiT1()tkiDTsiDCorrelation……Skip Connection11(( ())|)kkstkiiiiGDDTTTW1(( ())|)stkiiGDDTW(|)iFIW(a)

(b)

(c)

(d)

(e)

(f)

Figure 4: Visualization of search window Ni in RTNs (e.g.  |Ni| : 5 × 5): Source images with the
search window of (a) stride 4  (c) stride 2   (e) stride 1  and target images with (b)  (d)  (f) transformed
points for (a)  (c)  (e)  respectively. As evolving iterations  the dilate strides are reduced to consider
precise matching details.

4.2 Feature Extraction Network

To extract convolutional features for source Ds and target Dt  the input source and target images (I s 
I t) are ﬁrst passed through fully-convolutional feature extraction networks with shared parameters
WF such that Di = F(Ii|WF )  and the feature for each pixel then undergoes L2 normalization.
In the recurrent formulation  at each iteration the target features Dt can be extracted according to
Ti such that Dt(Ti) = F(I t(Ti)|WF ). However  extracting each feature by transforming local
receptive ﬁelds within the target image I t according to Ti for each pixel i and then passing it through
the networks would be time-consuming when iterating the networks. Instead  we employ a strategy
similar to UCN-ST [5] and CAT-FCSS [19] by ﬁrst extracting the convolutional features of the
entire image I t by passing it through the networks except for the last convolutional layer  and then
computing Dt(Ti) by transforming the resultant convolutional features and ﬁnally passing it through
the last convolution with stride to combine the transformed activations independently [5  19]. It
should be noted that any convolutional features [35  12  19] could be used in this framework. In
experiments  we used CAT-FCSS [19]  and sampled activations after pooling layers such as ‘conv4-4’
for VGGNet [35] and ‘conv4-23’ for ResNet [12].

4.3 Recurrent Geometric Matching Network

Constraint Correlation Volume To predict the geometric ﬁelds from two convolutional features
Ds and Dt  we ﬁrst compute the correlation volume with respect to translational motion only [30  31]
and then pass it to subsequent convolutional layers to determine dense afﬁne transformation ﬁelds. As
shown in [31]  this two-step approach reliably prunes incorrect matches. Speciﬁcally  the similarity
between two extracted features is computed as the cosine similarity with L2 normalization:

(cid:114)(cid:88)

l

C(Ds

i   Dt(Tj)) = < Ds

i   Dt(Tj) >/

< Ds

i   Dt(Tl) >2 

(2)

where j  l ∈ Ni for the search window Ni of pixel i.
Compared to [30  31] that consider all possible samples within an image  the constraint correlation
volume deﬁned within Ni reduces the matching ambiguity and computational times. However  due
to the limited search window range  it may not cover large geometric variations. To alleviate this
limitation  inspired by [43]  we utilize dilation techniques in a manner that the local neighborhood
Ni is enlarged with larger stride than 1 pixel  and this dilation is reduced as the iterations progress  as
exempliﬁed in Fig. 4.

Recurrent Geometry Estimation Based on this matching similarity  the recurrent geometry es-
timation network with parameters WG iteratively estimates the residual between the previous and
current geometric transformation ﬁelds as
i − Tk−1
i = F(C(Ds
Tk
(cid:88)

i denotes the transformation ﬁelds at the k-th iteration. The ﬁnal geometric ﬁelds are then

where Tk
estimated in a recurrent manner as follows:

))|WG) 

i   Dt(Tk−1

i

(3)

F(C(Ds

i   Dt(Tk−1

i

))|WG) 

Ti = T0

i +

(4)

k∈{1 .. Kmax}

5

(a)

(b)

(c)

(d)

(e)

(f)

Figure 5: Convergence of RTNs: (a) source image; (b) target image; Iterative evolution of warped
images (c)  (d)  (e)  and (f) after iteration 1  2  3  and 4. In the recurrent formulation of RTNs  the
predicted transformation ﬁeld becomes progressively more accurate through iterative estimation.

where Kmax denotes the maximum iteration and T0
i is an initial geometric ﬁeld. Unlike [30  31]
which estimate a global afﬁne or thin-plate spline transformation ﬁeld  we formulate the encoder-
decoder networks as in [32] to estimate locally-varying geometric ﬁelds. Moreover  our networks are
formulated in a fully-convolutional manner  thus source and target inputs of any size can be processed 
in contrast to [30  31] which can take inputs of only a ﬁxed size.
Iteratively inferring afﬁne transformation residuals boosts matching precision and facilitates conver-
gence. Moreover  inferring residuals instead of carrying the input information through the network has
been shown to improve network optimization [12]. As shown in Fig. 5  the predicted transformation
ﬁeld becomes progressively more accurate through iterative estimation.

4.4 Weakly-supervised Learning

A major challenge of semantic correspondence with deep CNNs is the lack of ground-truth cor-
respondence maps for training. Obtaining such ground-truth data through manual annotation is
labor-intensive and may be degraded by subjectivity [36  7  8]. To learn the networks using only
weak supervision in the form of image pairs  we formulate the loss function based on the intuition
that the matching score between the source feature Ds
i at each pixel i and the target feature Dt(Ti)
should be maximized while keeping the scores of other transformation candidates low. This can
be treated as a classiﬁcation problem in that the network can learn a geometric ﬁeld as a hidden
variable for maximizing the scores for matchable Ti while minimizing the scores for non-matchable
transformation candidates. The optimal ﬁelds Ti can be learned with a classiﬁcation loss [19] in a
weakly-supervised manner by minimizing the energy function

i   Dt(T)) = − (cid:88)

L(Ds

p∗
j log(p(Ds

i   Dt(Tj))) 

(5)

(6)

where the function p(Ds

i   Dt(Tj)) is a softmax probability deﬁned as

p(Ds

i   Dt(Tj)) =

exp(C(Ds

exp(C(Ds

i   Dt(Tj)))

i   Dt(Tl)))

 

j∈Mi

(cid:80)

l∈Mi

j denoting a class label deﬁned as 1 if j = i  and 0 otherwise for j ∈ Mi for the search
with p∗
window Mi  such that the center point i within Mi becomes a positive sample while the other points
are negative samples.
With this loss function  the derivatives ∂L/∂Ds and ∂L/∂Dt(T) of the loss function L with respect
to the features Ds and Dt(T) can be back-propagated into the feature extraction networks F(·|WF ).
Explicit feature learning in this manner with the classiﬁcation loss has been shown to be reliable [19].
Likewise  the derivatives ∂L/∂Dt(T) and ∂Dt(T)/∂T of the loss function L with respect to
geometric ﬁelds T can be back-propagated into the geometric matching networks F(·|WG) to learn
these networks without ground truth T∗.
It should be noted that our loss function is conceptually similar to [31] in that it is formulated with
source and target features in a weakly-supervised manner. While [31] utilizes only positive samples
in learning feature extraction networks  our method considers both positive and negative samples to
enhance network training.

6

(a)

(b)

(c)

(d)

(e)

(f)

Figure 7: Qualitative results on the TSS benchmark [36]: (a) source image  (b) target image  (c)
DCTM [18]  (d) SCNet [9]  (e) GMat. w/Inl. [31]  and (f) RTNs. The source images are warped to
the target images using correspondences.

5 Experimental Results and Discussion

5.1 Experimental Settings

In the following  we comprehensively evaluated our RTNs through comparisons to state-of-the-art
methods for semantic correspondence  including SF [25]  DSP [17]  Zhou et al. [44]  Taniai et al. [36] 
PF [7]  SCNet [9]  DCTM [18]  geometric matching (GMat.) [30]  and GMat. w/Inl. [31]  as well as
employing the SIFT ﬂow optimizer1 together with UCN-ST [5]  FCSS [18]  and CAT-FCSS [19].
Performance was measured on the TSS dataset [36]  PF-WILLOW dataset [7]  and PF-PASCAL
dataset [8]. In Sec. 5.2  we ﬁrst analyze the effects of the components within RTNs  and then evaluate
matching results with various benchmarks and quantitative measures in Sec. 5.3.

5.2 Ablation Study

To validate the components within RTNs  we evaluated
the matching accuracy for different numbers of itera-
tions  with various window sizes of Ni  for different back-
bone feature extraction networks such as VGGNet [35] 
CAT-FCSS [19]  and ResNet [12]  and with pretrained or
learned backbone networks. For quantitative assessment 
we examined the matching accuracy on the TSS bench-
mark [36]  as described in the following section. As shown
in Fig. 6  RTNs w/ResNet [12] converge in 3−5 iterations.
By enlarging the window size of Ni  the matching accu-
racy improves until 9×9 with longer training and testing
times  but larger window sizes reduce matching accuracy
due to greater matching ambiguity. Note that Mi = Ni.
Table 1 shows that among the many state-of-the-art fea-
ture extraction networks  ResNet [12] exhibits the best
performance for our approach. As shown in comparisons
between pretrained and learned backbone networks  learn-
ing the feature extraction networks jointly with geometric matching networks can boost matching
accuracy  as similarly seen in [31].

Figure 6: Convergence analysis of RTNs
w/ResNet [12] for various numbers of
iterations and search window sizes on
the TSS benchmark [36].

5.3 Matching Results

TSS Benchmark We evaluated RTNs on the TSS benchmark [36]  which consists of 400 image
pairs divided into three groups: FG3DCar [24]  JODS [33]  and PASCAL [10]. As in [18  20] 
ﬂow accuracy was measured by computing the proportion of foreground pixels with an absolute
ﬂow endpoint error that is smaller than a threshold of T = 5  after resizing each image so that

1For these experiments  we utilized the hierarchical dual-layer belief propagation of SIFT ﬂow [25] together

with alternative dense descriptors.

7

123456#Iteration0.40.50.60.70.8Average flow accuracyMethods

Feature

Regular.

Superv.

FG3D.

JODS

PASC. Avg.

SF [25]
DSP [17]
Taniai et al. [36]
PF [7]
DCTM [18]
UCN-ST [5]
FCSS [18  19]

SCNet [9]

GMat. [30]
GMat. w/Inl. [31]
RTNs
RTNs
RTNs
RTNs

CAT-FCSS†
UCN-ST

FCSS

SIFT
SIFT
HOG
HOG

CAT-FCSS
VGGNet
VGGNet
VGGNet
ResNet
ResNet
VGGNet†
VGGNet
CAT-FCSS

ResNet

SF
DSP
TSS
LOM
DCTM

SF
SF
SF
AG
AG+
GMat.
GMat.
GMat.
R-GMat.
R-GMat.
R-GMat.
R-GMat.

-
-
-
-
-

Sup.
Weak.
Weak.
Sup.
Sup.
Self.
Self.
Weak.
Weak.
Weak.
Weak.
Weak.

0.632
0.487
0.830
0.786
0.891
0.853
0.832
0.858
0.764
0.776
0.835
0.886
0.892
0.875
0.893
0.889
0.901

0.509
0.465
0.595
0.653
0.721
0.672
0.662
0.680
0.600
0.608
0.656
0.758
0.758
0.736
0.762
0.775
0.782

0.360
0.382
0.483
0.531
0.610
0.511
0.512
0.522
0.463
0.474
0.527
0.560
0.562
0.586
0.591
0.611
0.633

0.500
0.445
0.636
0.657
0.740
0.679
0.668
0.687
0.609
0.619
0.673
0.735
0.737
0.732
0.749
0.758
0.772

Table 1: Matching accuracy compared to state-of-the-art correspondence techniques (with feature 
regularization  and supervision) on the TSS benchmark [36]. † denotes a pre-trained feature.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 8: Qualitative results on the PF-WILLOW benchmark [7]: (a) source image  (b) target image 
(c) UCN-ST [5]  (d) SCNet [9]  (e) GMat. w/Inl. [31]  and (f) RTNs. The source images are warped
to the target images using correspondences.

its larger dimension is 100 pixels. Table 1 compares the matching accuracy of RTNs to state-of-
the-art correspondence techniques  and Fig. 7 shows qualitative results. Compared to handcrafted
methods [25  17  36  7]  most CNN-based methods have better performance. In particular  methods
that use STN-based feature transformations  namely UCN-ST [5] and CAT-FCSS [19]  show improved
ability to deal with geometric variations. In comparison to the geometric matching-based methods
GMat. [30] and GMat. w/Inl. [30]  RTNs consisting of feature extraction with ResNet and recurrent
geometric matching modules provide higher performance. RTNs additionally outperform existing
CNN-based methods trained with supervision of ﬂow ﬁelds. It should be noted that GMat. w/Inl. [31]
was learned with the initial network parameters set through self-supervised learning as in [30]. RTNs
instead start from fully-randomized parameters in geometric matching networks.

PF-WILLOW Benchmark We also evaluated our method on the PF-WILLOW benchmark [7] 
which includes 10 object sub-classes with 10 keypoint annotations for each image. For the evaluation
metric  we use the probability of correct keypoint (PCK) between ﬂow-warped keypoints and the
ground truth [26  7] as in the experiments of [18  9  19]. Table 2 compares the PCK values of
RTNs to state-of-the-art correspondence techniques  and Fig. 8 shows qualitative results. Our RTNs
exhibit performance competitive to the state-of-the-art correspondence techniques including the
latest weakly-supervised and even supervised methods for semantic correspondence. Since RTNs
estimate locally-varying geometric ﬁelds  it provides more precise localization ability  as shown in the

8

Methods

PF [7]
DCTM [18]
UCN-ST [5]
CAT-FCSS [19]
SCNet [9]
GMat. [30]
GMat. w/Inl. [31]
RTNs w/VGGNet
RTNs w/ResNet

PF-WILLOW [7]

PF-PASCAL [8]

α = 0.05

α = 0.1

α = 0.15

α = 0.05

α = 0.1

α = 0.15

0.284
0.381
0.241
0.362
0.386
0.369
0.370
0.402
0.413

0.568
0.610
0.540
0.546
0.704
0.692
0.702
0.707
0.719

0.682
0.721
0.665
0.692
0.853
0.778
0.799
0.842
0.862

0.314
0.342
0.299
0.336
0.362
0.410
0.490
0.506
0.552

0.625
0.696
0.556
0.689
0.722
0.695
0.748
0.743
0.759

0.795
0.802
0.740
0.792
0.820
0.804
0.840
0.836
0.852

Table 2: Matching accuracy compared to state-of-the-art correspondence techniques on the PF-
WILLOW benchmark [7] and PF-PASCAL benchmark [8].

(a)

(b)

(c)

(d)

(e)

(f)

Figure 9: Qualitative results on the PF-PASCAL benchmark [8]: (a) source image  (b) target image 
(c) CAT-FCSS w/SF [19]  (d) SCNet [9]  (e) GMat. w/Inl. [31]  and (f) RTNs. The source images are
warped to the target images using correspondences.

results of α = 0.05  in comparison to existing geometric matching networks [30  31] which estimate
globally-varying geometric ﬁelds only.

PF-PASCAL Benchmark Lastly  we evaluated our method on the PF-PASCAL benchmark [8] 
which contains 1 351 image pairs over 20 object categories with PASCAL keypoint annotations [2].
Following the split in [9  31]  we used 700 training pairs  300 validation pairs  and 300 testing
pairs. For the evaluation metric  we use the PCK between ﬂow-warped keypoints and the ground
truth as done in the experiments of [9]. Table 2 summarizes the PCK values  and Fig. 9 shows
qualitative results. Similar to the experiments on the PF-WILLOW benchmark [7]  CNN-based
methods [9  30  31] including our RTNs yield better performance  with RTNs providing the highest
matching accuracy.

6 Conclusion

We presented RTNs  which learn to infer locally-varying geometric ﬁelds for semantic correspondence
in an end-to-end and weakly-supervised fashion. The key idea of this approach is to utilize and
iteratively reﬁne the transformations and convolutional activations via geometric matching between
the input image pair. In addition  a technique is presented for weakly-supervised training of RTNs. A
direction for further study is to examine how the semantic correspondence of RTNs could beneﬁt
single-image 3-D reconstruction and instance-level object detection and segmentation.

9

Acknowledgments

This research was supported by Next-Generation Information Computing Development Program
through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and
ICT (NRF-2017M3C4A7069370).

References
[1] C. Barnes  E. Shechtman  D. B. Goldman  and A. Finkelstein. The generalized patchmatch

correspondence algorithm. In: ECCV  2010.

[2] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations .

In: ICCV  2009.

[3] H. Bristow  J. Valmadre  and S. Lucey. Dense semantic correspondence where every pixel is a

classiﬁer. In: ICCV  2015.

[4] D. Butler  J. Wulff  G. Stanley  and M. Black. A naturalistic open source movie for optical ﬂow

evaluation. In: ECCV  2012.

[5] C. B. Choy  Y. Gwak  and S. Savarese. Universal correspondence network. In:NIPS  2016.

[6] Y. HaCohen  E. Shechtman  D. B. Goldman  and D. Lischinski. Non-rigid dense correspondence

with applications for image enhancement. In: SIGGRAPH  2011.

[7] B. Ham  M. Cho  C. Schmid  and J. Ponce. Proposal ﬂow. In: CVPR  2016.

[8] B. Ham  M. Cho  C. Schmid  and J. Ponce. Proposal ﬂow: Semantic correspondences from

object proposals. IEEE Trans. PAMI  2017.

[9] K. Han  R. S. Rezende  B. Ham  K. Wong  M. Cho  C. Schmid  and J. Ponce. Scnet: Learning

semantic correspondence. In: ICCV  2017.

[10] B. Hariharan  P. Arbelaez  L. Bourdev  S. Maji  and J. Malik. Semantic contours from inverse

detectors. In: ICCV  2011.

[11] T. Hassner  V. Mayzels  and L. Zelnik-Manor. On sifts and their scales. In: CVPR  2012.

[12] K. He  X. Zhang  S. Ren  and Sun. J. Deep residual learning for image recognition. In: CVPR 

2016.

[13] J. Hur  H. Lim  C. Park  and S. C. Ahn. Generalized deformable spatial pyramid: Geometry-

preserving dense correspondence estimation. In: CVPR  2015.

[14] Philbin. J.  O. Chum  M. Isard  J. Sivic  and A. Zisserman. Object retrieval with large vocabu-

laries and fast spatial matching. In: CVPR  2007.

[15] M. Jaderberg  K. Simonyan  A. Zisserman  and K. Kavukcuoglu. Spatial transformer networks.

In: NIPS  2015.

[16] A. Kanazawa  D. W. Jacobs  and M. Chandraker. Warpnet: Weakly supervised matching for

single-view reconstruction. In: CVPR  2016.

[17] J. Kim  C. Liu  F. Sha  and K. Grauman. Deformable spatial pyramid matching for fast dense

correspondences. In: CVPR  2013.

[18] S. Kim  D. Min  B. Ham  S. Jeon  S. Lin  and K. Sohn. Fcss: Fully convolutional self-similarity

for dense semantic correspondence. In: CVPR  2017.

[19] S. Kim  D. Min  B. Ham  S. Lin  and K. Sohn. Fcss: Fully convolutional self-similarity for

dense semantic correspondence. TPAMI  2018.

[20] S. Kim  D. Min  S. Lin  and K. Sohn. Dctm: Discrete-continuous transformation matching for

semantic ﬂow. In: ICCV  2017.

10

[21] S. Kim  D. Min  S. Lin  and K. Sohn. Discrete-continuous transformation matching for dense

semantic correspondence. IEEE Trans. PAMI  2018.

[22] J. Liao  Y. Yao  L. Yuan  G. Hua  and S. B. Kang. Visual attribute transfer through deep image

analogy. In: SIGGRAPH  2017.

[23] Chen-Hsuan Lin and Simon Lucey. Inverse compositional spatial transformer networks. CVPR 

2017.

[24] Y. L. Lin  V. I. Morariu  W. Hsu  and L. S. Davis. Jointly optimizing 3d model ﬁtting and

ﬁne-grained classiﬁcation. In: ECCV  2014.

[25] C. Liu  J. Yuen  and A Torralba. Sift ﬂow: Dense correspondence across scenes and its

applications. IEEE Trans. PAMI  33(5):815–830  2011.

[26] J. Long  N. Zhang  and T. Darrell. Do convnets learn correspondence? In: NIPS  2014.
[27] D.G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV  60(2):91–110 

2004.

[28] D. Novotny  D. Larlus  and A. Vedaldi. Anchornet: A weakly supervised network to learn

geometry-sensitive features for semantic matching. In:CVPR  2017.

[29] W. Qiu  X. Wang  X. Bai  A. Yuille  and Z. Tu. Scale-space sift ﬂow. In: WACV  2014.
[30] I. Rocco  R. Arandjelovic  and J. Sivic. Convolutional neural network architecture for geometric

matching. In:CVPR  2017.

[31] I. Rocco  R. Arandjelovi´c  and J. Sivic. End-to-end weakly-supervised semantic alignment. In:

CVPR  2018.

[32] O. Ronneberger  P. Fischer  and T. Brox. U-net: Convolutional networks for biomedical image

segmentation. In: MICCAI  2015.

[33] M. Rubinstein  A. Joulin  J. Kopf  and C. Liu. Unsupervised joint object discovery and

segmentation in internet images. In: CVPR  2013.

[34] D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspon-

dence algorithms. IJCV  47(1):7–42  2002.

[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image

recognition. In: ICLR  2015.

[36] T. Taniai  S. N. Sinha  and Y. Sato. Joint recovery of dense correspondence and cosegmentation

in two images. In: CVPR  2016.

[37] J. Thewlis1  H. Bilen  and A. Vedald. Unsupervised learning of object frames by dense

equivariant image labelling. In: NIPS  2017.

[38] N. Ufer and B. Ommer. Deep semantic feature matching. In:CVPR  2017.
[39] F. Yang  X. Li  H. Cheng  J. Li  and L. Chen. Object-aware dense semantic correspondence.

In:CVPR  2017.

[40] H. Yang  W. Y. Lin  and J. Lu. Daisy ﬁlter ﬂow: A generalized discrete approach to dense

correspondences. In: CVPR  2014.

[41] K. M. Yi  E. Trulls  V. Lepetit  and P. Fua. Lift: Learned invariant feature transform. In: ECCV 

2016.

[42] K. M. Yi  Y. Verdie  P. Fua  and V. Lepetit. Learning to assign orientations to feature points. In:

CVPR  2016.

[43] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In: ICLR  2016.
[44] T. Zhou  P. Krahenbuhl  M. Aubry  Q. Huang  and A. A. Efros. Learning dense correspondence

via 3d-guided cycle consistency. In: CVPR  2016.

[45] T. Zhou  Y. J. Lee  S. X. Yu  and A. A. Efros. Flowweb: Joint image set alignment by weaving

consistent  pixel-wise correspondences. In: CVPR  2015.

11

,Asier Mujika
Florian Meier
Angelika Steger
Seungryong Kim
Stephen Lin
SANG RYUL JEON
Dongbo Min
Kwanghoon Sohn