2017,Convergence rates of a partition based Bayesian multivariate density estimation method,We study a class of non-parametric density estimators under Bayesian settings. The estimators are obtained by adaptively partitioning the sample space. Under a suitable prior  we analyze the concentration rate of the posterior distribution  and demonstrate that the rate does not directly depend on the dimension of the problem in several special cases. Another advantage of this class of Bayesian density estimators is that it can adapt to the unknown smoothness of the true density function  thus achieving the optimal convergence rate without artificial conditions on the density. We also validate the theoretical results on a variety of simulated data sets.,Convergence rates of a partition based Bayesian

multivariate density estimation method

Linxi Liu ∗

Department of Statistics

Columbia University

ll3098@columbia.edu

Dangna Li

ICME

Stanford University

dangna@stanford.edu

Wing Hung Wong

Department of Statistics

Stanford University

whwong@stanford.edu

Abstract

We study a class of non-parametric density estimators under Bayesian settings.
The estimators are obtained by adaptively partitioning the sample space. Under
a suitable prior  we analyze the concentration rate of the posterior distribution 
and demonstrate that the rate does not directly depend on the dimension of the
problem in several special cases. Another advantage of this class of Bayesian
density estimators is that it can adapt to the unknown smoothness of the true
density function  thus achieving the optimal convergence rate without artiﬁcial
conditions on the density. We also validate the theoretical results on a variety of
simulated data sets.

1

Introduction

In this paper  we study the asymptotic behavior of posterior distributions of a class of Bayesian density
estimators based on adaptive partitioning. Density estimation is a building block for many other
statistical methods  such as classiﬁcation  nonparametric testing  clustering  and data compression.
With univariate (or bivariate) data  the most basic non-parametric method for density estimation
is the histogram method. In this method  the sample space is partitioned into regular intervals
(or rectangles)  and the density is estimated by the relative frequency of data points falling into
each interval (rectangle). However  this method is of limited utility in higher dimensional spaces
because the number of cells in a regular partition of a p-dimensional space will grow exponentially
with p  which makes the relative frequency highly variable unless the sample size is extremely
large. In this situation the histogram may be improved by adapting the partition to the data so
that larger rectangles are used in the parts of the sample space where data is sparse. Motivated
by this consideration  researchers have recently developed several multivariate density estimation
methods based on adaptive partitioning [13  12]. For example  by generalizing the classical Pólya
Tree construction [7  22] developed the Optional Pólya Tree (OPT) prior on the space of simple
functions. Computational issues related to OPT density estimates were discussed in [13]  where
efﬁcient algorithms were developed to compute the OPT estimate. The method performs quite well
when the dimension is moderately large (from 10 to 50).
The purpose of the current paper is to address the following questions on such Bayesian density
estimates based on partition-learning. Question 1: what is the class of density functions that can be
“well estimated” by the partition-learning based methods. Question 2: what is the rate at which the
posterior distribution is concentrated around the true density as the sample size increases. Our main
contributions lie in the following aspects:

∗Work was done while the author was at Stanford University.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

• We impose a suitable prior on the space of density functions deﬁned on binary partitions  and
calculate the posterior concentration rate under the Hellinger distance with mild assumptions.
The rate is adaptive to the unknown smoothness of the true density.
• For two dimensional density functions of bounded variation  the posterior contraction rate

of our method is n−1/4(log n)3.

− β

• For Hölder continuous (one-dimensional case) or mixture Hölder continuous (multi-
dimensional case) density functions with regularity parameter β in (0  1]  the posterior
concentration rate is n
2β   whereas the minimax rate for one-dimensional
Hölder continuous functions is (n/ log n)−β/(2β+1).

• When the true density function is sparse in the sense that the Haar wavelet coefﬁcients satisfy
2q−1 .
• We can use a computationally efﬁcient algorithm to sample from the posterior distribution.

a weak-lq (q > 1/2) constraint  the posterior concentration rate is n

2β+p (log n)2+ p

(log n)2+ 1

− q−1/2

2q

We demonstrate the theoretical results on several simulated data sets.

1.1 Related work

An important feature of our method is that it can adapt to the unknown smoothness of the true density
function. The adaptivity of Bayesian approaches has drawn great attention in recent years. In terms of
density estimation  there are mainly two categories of adaptive Bayesian nonparametric approaches.
The ﬁrst category of work relies on basis expansion of the density function and typically imposes a
random series prior [15  17]. When the prior on the coefﬁcients of the expansion is set to be normal
[4]  it is also a Gaussian process prior. In the multivariate case  most existing work [4  17] uses
tensor-product basis. Our improvement over these methods mainly lies in the adaptive structure. In
fact  as the dimension increases the number of tensor-product basis functions can be prohibitively
large  which imposes a great challenge on computation. By introducing adaptive partition  we are
able to handle the multivariate case even when the dimension is 30 (Example 2 in Section 4).
Another line of work considers mixture priors [16  11  18]. Although the mixture distributions have
good approximation properties and naturally lead to adaptivity to very high smoothness levels  they
may fail to detect or characterize the local features. On the other hand  by learning a partition of the
sample space  the partition based approaches can provide an informative summary of the structure 
and allow us to examine the density at different resolutions [14  21].
The paper is organized as follows. In Section 2 we provide more details of the density functions on
binary partitions and deﬁne the prior distribution. Section 3 summarizes the theoretical results on
posterior concentration rates. The results are further validated in Section 4 by several experiments.

2 Bayesian multivariate density estimation
We focus on density estimation problems in p-dimensional Euclidean space. Let (Ω B) be a mea-
surable space and f0 be a compactly supported density function with respect to the Lebesgue
measure µ. Y1  Y2 ···   Yn is a sequence of independent variables distributed according to f0. Af-
ter translation and scaling  we can always assume that the support of f0 is contained in the unit
cube in Rp. Translating this into notations  we assume that Ω = {(y1  y2 ···   yp) : yl ∈ [0  1]}.
Ω f dµ = 1} denotes the collection of all the
density functions on (Ω B  µ). Then F constitutes the parameter space in this problem. Note that F
is an inﬁnite dimensional parameter space.

F = {f is a nonnegative measurable function on Ω :(cid:82)

2.1 Densities on binary partitions
To address the inﬁnite dimensionality of F  we construct a sequence of ﬁnite dimensional approx-
imating spaces Θ1  Θ2 ···   ΘI  ··· based on binary partitions. With growing complexity  these
spaces provide more and more accurate approximations to the initial parameter space F. Here  we
use a recursive procedure to deﬁne a binary partition with I subregions of the unit cube in Rp. Let
Ω = {(y1  y2 ···   yp) : yl ∈ [0  1]} be the unit cube in Rp. In the ﬁrst step  we choose one of
the coordinates yl and cut Ω into two subregions along the midpoint of the range of yl. That is 
0. In this way  we get a partition
Ω = Ωl

0 = {y ∈ Ω : yl ≤ 1/2} and Ωl

1 = Ω\Ωl

1  where Ωl

0 ∪ Ωl

2

with two subregions. Note that the total number of possible partitions after the ﬁrst step is equal to
the dimension p. Suppose after I − 1 steps of the recursion  we have obtained a partition {Ωi}I
i=1
with I subregions. In the I-th step  further partitioning of the region is deﬁned as follows:

1. Choose a region from Ω1 ···   ΩI. Denote it as Ωi0.
2. Choose one coordinate yl and divide Ωi0 into two subregions along the midpoint of the

range of yl.

Such a partition obtained by I − 1 recursive steps is called a binary partition of size I. Figure 1
displays all possible two dimensional binary partitions when I is 1  2 and 3.

Figure 1: Binary partitions

Now  let

ΘI = {f : f =

I(cid:88)

I(cid:88)

θi = 1  {Ωi}I

i=1 is a binary partition of Ω.}

θi
|Ωi| 1Ωi  

i=1

i=1

where |Ωi| is the volume of Ωi. Then  ΘI is the collection of the density functions supported by the
binary partitions of size I. They constitute a sequence of approximating spaces (i.e. a sieve  see
[10  20] for background on sieve theory). Let Θ = ∪∞
I=1ΘI be the space containing all the density
functions supported by the binary partitions. Then Θ is an approximation of the initial parameter
space F to certain approximation error which will be characterized later.
We take the metric on F  Θ and ΘI to be Hellinger distance  which is deﬁned as

((cid:112)f (y) −(cid:112)g(y))2dy)1/2  f  g ∈ F.

ρ(f  g) = (

(cid:90)

Ω

2.2 Prior distribution
An ideal prior Π on Θ = ∪∞
I=1ΘI is supposed to be capable of balancing the approximation error
and the complexity of Θ. The prior in this paper penalizes the size of the partition in the sense
that the probability mass on each ΘI is proportional to exp(−λI log I). Given a sample of size n 
we restrict our attention to Θn = ∪n/ log n
I=1 ΘI  because in practice we need enough samples within
each subregion to get a meaningful estimate of the density. This is to say  when I ≤ n/ log n 
Π(ΘI ) ∝ exp(−λI log I)  otherwise Π(ΘI ) = 0.
If we use TI to denote the total number of possible partitions of size I  then it is not hard to see
that log TI ≤ c∗I log I  where c∗ is a constant. Within each ΘI  the prior is uniform across all
binary partitions. In other words  let {Ωi}I
i=1)
is the collection of piecewise constant density functions on this partition (i.e. F({Ωi}I
i=1) = {f =

i=1 be a binary partition of Ω of size I  and F({Ωi}I

(cid:80)I

i=1

θi|Ωi| 1Ωi :(cid:80)I

i=1 θi = 1 and θi ≥ 0  i = 1  . . .   I})  then

Π(cid:0)F(cid:0){Ωi}I

i=1

(cid:1)(cid:1) ∝ exp(−λI log I)/TI .

(1)

(2)

3

i=1  the weights θi on the subregions follow a Dirichlet distribution with

Let Πn(·|Y1 ···   Yn) to denote the posterior distribution. After integrating out the weights θi  we

Π

θi

f =

I(cid:88)

Given a partition {Ωi}I

parameters all equal to α (α < 1). This is to say  for x1 ···   xI ≥ 0 and(cid:80)I
(cid:32)
(cid:12)(cid:12)(cid:12)(cid:12)F(cid:0){Ωi}I
where D(δ1 ···   δI ) =(cid:81)I
can compute the marginal posterior probability of F(cid:0){Ωi}I
i=1)(cid:12)(cid:12)Y1 ···   Yn

|Ωi| 1Ωi : θ1 ∈ dx1 ···   θI ∈ dxI
i=1 δi).

(cid:1) ∝ Π(cid:0)F({Ωi}I

i=1 Γ(δi)/Γ((cid:80)I

(cid:0)F({Ωi}I

Πn

i=1

i=1

i=1

i=1 xi = 1 

I(cid:89)

i=1

1

=

D(α ···   α)

(cid:1)(cid:33)
(cid:1):
i=1)(cid:1)(cid:90) (cid:32) I(cid:89)
I(cid:89)
dθ1 ··· dθI
· D(α + n1 ···   α + nI )

(θi/|Ωi|)ni

θα−1

(cid:33)

(cid:33)

i=1

i=1

D(α ···   α)

i

1

(cid:32)

×

D(α ···   α)
∝ exp(−λI log I)

TI

I(cid:89)

i=1

1
|Ωi|ni

  (4)

xα−1

i

 

(3)

where ni is the number of observations in Ωi. Under the prior introduced in [13]  the marginal
posterior distribution is:

(cid:0)F({Ωi}I

i=1)(cid:12)(cid:12)Y1 ···   Yn

(cid:1) ∝ exp(−λI)

Π∗

n

D(α + n1 ···   α + nI )

D(α ···   α)

1
|Ωi|ni

 

(5)

while the maximum log-likelihood achieved by histograms on the partition {Ωi}n

I(cid:89)

i=1
i=1 is:

I(cid:88)

i=1

(cid:19)

(cid:18) ni

n|Ωi|

ln(F({Ωi}I

i=1)) :=

max
f∈F ({Ωi}I

i=1)

ln(f ) =

ni log

.

(6)

From a model selection perspective  we may treat the histograms on each binary partition as a model
of the data. When I (cid:28) n  asymptotically 

log(cid:0)Π∗

n

(cid:0)F({Ωi}I

i=1)(cid:12)(cid:12)Y1 ···   Yn

(cid:1)(cid:1) (cid:16) ln(F({Ωi}I

(I − 1) log n.

(7)

i=1)) − 1
2

This is to say  in [13]  selecting the partition which maximizes the marginal posterior distribution is
equivalent to applying the Bayesian information criterion (BIC) to perform model selection. However 
if we allow I to increase with n  (7) will not hold any more. But if we use the prior introduced in this
section  in the case when I

n → ζ ∈ (0  1) as n → ∞  we still have

(cid:0)F({Ωi}I

i=1)(cid:12)(cid:12)Y1 ···   Yn

(cid:1)(cid:1) (cid:16) ln(F({Ωi}I

log(cid:0)Πn

i=1)) − λI log I.

(8)

From a model selection perspective  this is closer to the risk inﬂation criterion (RIC  [8]).

3 Posterior concentration rates

We are interested in how fast the posterior probability measure concentrates around the true the
density f0. Under the prior speciﬁed above  the posterior probability is the random measure given by

(cid:81)n
(cid:81)n

(cid:82)
(cid:82)

B

Θ

Πn(B|Y1 ···   Yn) =

j=1 f (Yj)dΠ(f )
j=1 f (Yj)dΠ(f )

.

A Bayesian estimator is said to be consistent if the posterior distribution concentrates on arbitrarily
small neighborhoods of f0  with probability tending to 1 under P n
0 (P0 is the probability measure
corresponding to the density function f0). The posterior concentration rate refers to the rate at which
these neighborhoods shrink to zero while still possessing most of the posterior mass. More explicitly 
we want to ﬁnd a sequence n → 0  such that for sufﬁciently large M 
Πn ({f : ρ(f  f0) ≥ M n}|Y1 ···   Yn) → 0 in P n

0 − probability.

4

In [6] and [2]  the authors demonstrated that it is impossible to ﬁnd an estimator which works
uniformly well for every f in F. This is the case because for any estimator ˆf  there always exists
f ∈ F for which ˆf is inconsistent. Given the minimaxity of the Bayes estimator  we have to restrict
our attention to a subset of the original parameter space F. Here  we focus on the class of density
functions that can be well approximated by ΘI’s. To be more rigorous  a density function f ∈ F is
said to be well approximated by elements in Θ  if there exits a sequence of fI ∈ ΘI  satisfying that
ρ(fI   f ) = O(I−r)(r > 0). Let F0 be the collection of these density functions. We will ﬁrst derive
posterior concentration rate for the elements in F0 as a function of r. For different function classes 
this approximation rate r can be calculated explicitly. In addition to this  we also assume that f0 has
ﬁnite second moment.
The following theorem gives the posterior concentration rate under the prior introduced in Section
2.2.
Theorem 3.1. Y1 ···   Yn is a sequence of independent random variables distributed according
to f0. P0 is the probability measure corresponding to f0. Θ is the collection of p-dimensional
density functions supported by the binary partitions as deﬁned in Section 2.1. With the modiﬁed prior
distribution  if f0 ∈ F0  then the posterior concentration rate is n = n− r
The strategy to show this theorem is to write the posterior probability of the shrinking ball as

2r+1 (log n)2+ 1
2r .

Π({f : ρ(f  f0) ≥ M n}|Y1 ···   Yn) =

I=1

{f :ρ(f f0)≥M n}∩ΘI

f (Yj )
f0(Yj ) dΠ(f )

j=1

.

(9)

(cid:80)∞

(cid:82)

(cid:81)n

(cid:80)∞

(cid:82)

(cid:81)n

I=1

ΘI

j=1

f (Yj )
f0(Yj ) dΠ(f )

The proof employs the mechanism developed in the landmark works [9] and [19]. We ﬁrst obtain
the upper bounds for the items in the numerator by dividing them into three blocks  each of which
accounts for bias  variance  and rapidly decaying prior respectively  and calculate the upper bound for
each block separately. Then we provide the prior thickness result  i.e.  we bound the prior mass of a
ball around the true density from below. Due to space constraints  the details of the proof will be
provided in the appendix.
This theorem suggests the following two take-away messages: 1. The rate is adaptive to the unknown
smoothness of the true density. 2. The posterior contraction rate is n− r
2r   which does
not directly depend on the dimension p. For some density functions  r may depend on p. But in
several special cases  like the density function is spatially sparse or the density function lies in a low
dimensional subspace  we will show that the rate will not be affected by the full dimension of the
problem.
In the following three subsections  we will calculate the explicit rates for three density classes. Again 
all proofs are given in the appendix.

2r+1 (log n)2+ 1

3.1 Spatial adaptation

First  we assume that the density concentrates spatially. Mathematically  this implies the density
function satisﬁes a type of sparsity. In the past two decades  sparsity has become one of the most
discussed types of structure under which we are able to overcome the curse of dimensionality. A
remarkable example is that it allows us to solve high-dimensional linear models  especially when the
system is underdetermined.
Let f be a p dimensional density function and Ψ the p-dimensional Haar basis. We will work
f ﬁrst. Note that g ∈ L2([0  1]p). Thus we can expand g with respect to Ψ as
with g =
ψ∈Ψ(cid:104)g  ψ(cid:105)ψ  ψ ∈ Ψ. We rearrange this summation by the size of wavelet coefﬁcients. In

g =(cid:80)

√

other words  we order the coefﬁcients as the following

|(cid:104)g  ψ(1)(cid:105)| ≥ |(cid:104)g  ψ(2)(cid:105)| ≥ ··· ≥ |(cid:104)g  ψ(k)(cid:105)| ≥ ···  

then the sparsity condition imposed on the density functions is that the decay of the wavelet coefﬁ-
cients follows a power law 

|(cid:104)g  ψ(k)(cid:105)| ≤ Ck−q for all k ∈ N and q > 1/2 

(10)

where C is a constant.

5

We call such a constraint a weak-lq constraint. The condition has been widely used to characterize
the sparsity of signals and images [1  3]. In particular  in [5]  it was shown that for two-dimensional
cases  when q > 1/2  this condition reasonably captures the sparsity of real world images.
Corollary 3.2. (Application to spatial adaptation) Suppose f0 is a p-dimensional density function
and satisﬁes the condition (10). If we apply our approaches to this type of density functions  the
posterior concentration rate is n

(log n)2+ 1

− q−1/2

2q−1 .

2q

3.2 Density functions of bounded variation
Let Ω = [0  1)2 be a domain in R2. We ﬁrst characterize the space BV (Ω) of functions of bounded
variation on Ω.
For a vector ν ∈ R2  the difference operator ∆ν along the direction ν is deﬁned by

∆ν(f  y) := f (y + ν) − f (y).

For functions f deﬁned on Ω  ∆ν(f  y) is deﬁned whenever y ∈ Ω(ν)  where Ω(ν) := {y :
[y  y + ν] ⊂ Ω} and [y  y + ν] is the line segment connecting y and y + ν. Denote by el  l = 1  2 the
two coordinate vectors in R2. We say that a function f ∈ L1(Ω) is in BV (Ω) if and only if

2(cid:88)

l=1

2(cid:88)

l=1

√

VΩ(f ) := sup
h>0

h−1

(cid:107)∆hel (f ·)(cid:107)L1(Ω(hel)) = lim
h→0

h−1

(cid:107)∆hel (f ·)(cid:107)L1(Ω(hel))

is ﬁnite. The quantity VΩ(f ) is the variation of f over Ω.
Corollary 3.3. Assume that f0 ∈ BV (Ω). If we apply the Bayesian multivariate density estimator
based on adaptive partitioning here to estimate f0  the posterior concentration rate is n−1/4(log n)3.

3.3 Hölder space
In one-dimensional case  the class of Hölder functions H(L  β) with regularity parameter β is deﬁned
as the following: let κ be the largest integer smaller than β  and denote by f (κ) its κth derivative.

H(L  β) = {f : [0  1] → R : |f (κ)(x) − f (κ)(y)| ≤ L|x − y|β−κ}.

In multi-dimensional cases  we introduce the Mixed-Hölder continuity. In order to simplify the
notation  we give the deﬁnition when the dimension is two. It can be easily generalized to high-
dimensional cases. A real-valued function f on R2 is called Mixed-Hölder continuous for some
nonnegative constant C and β ∈ (0  1]  if for any (x1  y1)  (x1  y2) ∈ R2 

|f (x2  y2) − f (x2  y1) − f (x1  y2) + f (x1  y1)| ≤ C|x1 − x2|β|y1 − y2|β.

Corollary 3.4. Let f0 be the p-dimensional density function. If
f0 is Hölder continuous (when
p = 1) or mixed-Hölder continuous (when p ≥ 2) with regularity parameter β ∈ (0  1]  then the
posterior concentration rate of the Bayes estimator is n

2β+p (log n)2+ p
2β .

− β

This result also implies that if f0 only depends on ˜p variable where ˜p < p  but we do not know in
advance which ˜p variables  then the rate of this method is determined by the effective dimension ˜p of
the problem  since the smoothness parameter r is only a function of ˜p. In next section  we will use a
simulated data set to illustrate this point.

4 Simulation

4.1 Sequential importance sampling
Each partition AI = {Ωi}I
i=1 is obtained by recursively partitioning the sample space. We can
use a sequence of partitions A1 A2 ···  AI to keep track of the path leading to AI. Let Πn(·)
denote the posterior distribution Πn(·|Y1 ···   Yn) for simplicity  and ΠI
n be the posterior distribution
conditioning on ΘI. Then ΠI

n(AI ) can be decomposed as
n(A2|A1)··· ΠI

n(A1)ΠI

n(AI ) = ΠI
ΠI

n(AI|AI−1).

6

Figure 2: Heatmap of the density and plots of the 2-dimensional Haar coefﬁcients. For the plot on the
right  the left panel is the plot of the Haar coefﬁcients from low resolution to high resolution up to
level 6. The middle one is the plot of the sorted coefﬁcients according to their absolute values. And
the right one is the same as the middle plot but with the abscissa in log scale.

n(Ai+1|Ai) can be calculated by ΠI

n(Ai). However  the
The conditional distribution ΠI
n(Ai) is sometimes infeasible  especially when both I and
computation of the marginal distribution ΠI
I − i are large  because we need to sum the marginal posterior probability over all binary partitions of
size I for which the ﬁrst i steps in the partition generating path are the same as those of Ai. Therefore 
we adopt the sequential importance algorithm proposed in [13]. In order to build a sequence of binary
n (Ai+1|Ai). The obtained
partitions  at each step  the conditional distribution is approximated by Πi+1
partition is assigned a weight to compensate the approximation  where the weight is

n(Ai+1)/ΠI

wI (AI ) =

n(AI )
ΠI

n(A1)Π2
Π1

n(A2|A1)··· ΠI

n(AI|AI−1)

.

In order to make the data points as uniform as possible  we apply a copula transformation to each
variable in advance whenever the dimension exceeds 3. More speciﬁcally  we estimate the marginal
distribution of each variable Xj by our approach  denoted as ˆfj (we use ˆFj to denote the cdf of
Xj)  and transform each point (y1 ···   yp) to (F1(y1) ···   Fp(yp)). Another advantage of this
transformation is that after the transformation the sample space naturally becomes [0  1]p.

Example 1 Assume that the two-dimensional density function is

(cid:18)Y1

(cid:19)

Y2

N

∼ 2
5

(cid:18)(cid:18)0.25

(cid:19)

0.25

(cid:19)

(cid:18)(cid:18)0.75

0.75

(cid:19)

(cid:19)

.

  0.052I2×2

  0.052I2×2

+

N

3
5

This density function both satisﬁes the spatial sparsity condition and belongs to the space of functions
of bounded variation. Figure 2 shows the heatmap of the density function and its Haar coefﬁcients.
The last panel in the second plot displays the sorted coefﬁcients with the abscissa in log-scale. From
this we can clearly see that the power-law decay deﬁned in Section 3.1 is satisﬁed.
We apply the adaptive partitioning approach to estimate the density  and allow the sample size increase
from 102 to 105. In Figure 3  the left plot is the density estimation result based on a sample with
10000 data points. The right one is the plot of Kullback-Leibler (KL) divergence from the estimated
density to f0 vs. sample size in log-scale. The sample sizes are set to be 100  500  1000  5000  104 
and 105. The linear trend in the plot validates the posterior concentrate rates calculated in Section 3.
The reason why we use KL divergence instead of the Hellinger distance is that for any f0 ∈ F0 and
ˆf ∈ Θ  we can show that the KL divergence and the Hellinger distance are of the same order. But
KL divergence is relatively easier to compute in our setting  since we can show that it is linear in
the logarithm of the posterior marginal probability of a partition. The proof will be provided in the
appendix. For each ﬁxed sample size  we run the experiment 10 times and estimate the standard error 
which is shown by the lighter blue part in the plot.

Example 2 In the second example we work with a density function of moderately high dimension.
Assume that the ﬁrst ﬁve random variables Y1 ··· Y5 are generated from the following location

7

Figure 3: Plot of the estimated density and KL divergence against sample size. We use the posterior
mean as the estimate. The right plot is on log-log scale  while the labels of x and y axes still represent
the sample size and the KL divergence before we take the logarithm.

Figure 4: KL divergence vs. sample size. The blue  purple and red curves correspond to the cases
when p = 5  p = 10 and p = 30 respectively. The slopes of the three lines are almost the same 
implying that the concentration rate only depends on the effective dimension of the problem (which
is 5 in this example).

mixture of the Gaussian distribution:

(cid:32)Y1

(cid:33)

∼ 1
2

N

0.25
Y2
0.25
Y3
Y4  Y5 ∼ N (0.5  0.1) 

(cid:33)

(cid:32)0.25

0.052

0.032

 

 +

(cid:32)(cid:32)0.75
(cid:33)

0.75
0.75

N

1
2

0.032
0.052

0

0
0

0.052

(cid:33)

  0.052I3×3

 

0

the other components Y6 ···   Yp are independently uniformly distributed. We run experiments for
p = 5  10  and 30. For a ﬁxed p  we generate n ∈ {500  1000  5000  104  105} data points. For
each pair of p and n  we repeat the experiment 10 times and calculate the standard error. Figure 4
displays the plot of the KL divergence vs. the sample size on log-log scale. The density function is
continuous differentiable. Therefore  it satisﬁes the mixed-Hölder continuity condition. The effective
dimension of this example is ˜p = 5  and this is reﬂected in the plot: the slopes of the three lines 
which correspond to the concentration rates under different dimensions  almost remain the same as
we increase the full dimension of the problem.

5 Conclusion

In this paper  we study the posterior concentration rate of a class of Bayesian density estimators
based on adaptive partitioning. We obtain explicit rates when the density function is spatially sparse 
belongs to the space of bounded variation  or is Hölder continuous. For the last case  the rate is
minimax up to a logarithmic term. When the density function is sparse or lies in a low-dimensional
subspace  the rate will not be affected by the dimension of the problem. Another advantage of this
method is that it can adapt to the unknown smoothness of the underlying density function.

8

1e25e21e35e31e41e5sample size0.010.200.400.600.801.00KL divergenceBibliography

[1] Felix Abramovich  Yoav Benjamini  David L. Donoho  and Iain M. Johnstone. Adapting to unknown

sparsity by controlling the false discovery rate. The Annals of Statistics  34(2):584–653  04 2006.

[2] Lucien Birgé and Pascal Massart. Minimum contrast estimators on sieves: exponential bounds and rates of

convergence. Bernoulli  4(3):329–375  09 1998.

[3] E.J. Candès and T. Tao. Near-optimal signal recovery from random projections: Universal encoding

strategies? Information Theory  IEEE Transactions on  52(12):5406–5425  Dec 2006.

[4] R. de Jonge and J.H. van Zanten. Adaptive estimation of multivariate functions using conditionally gaussian

tensor-product spline priors. Electron. J. Statist.  6:1984–2001  2012.

[5] R.A. DeVore  B. Jawerth  and B.J. Lucier. Image compression through wavelet transform coding. Informa-

tion Theory  IEEE Transactions on  38(2):719–746  March 1992.

[6] R. H. Farrell. On the lack of a uniformly consistent sequence of estimators of a density function in certain

cases. The Annals of Mathematical Statistics  38(2):471–474  04 1967.

[7] Thomas S. Ferguson. Prior distributions on spaces of probability measures. Ann. Statist.  2:615–629  1974.

[8] Dean P. Foster and Edward I. George. The risk inﬂation criterion for multiple regression. Ann. Statist. 

22(4):1947–1975  12 1994.

[9] Subhashis Ghosal  Jayanta K. Ghosh  and Aad W. van der Vaart. Convergence rates of posterior distributions.

The Annals of Statistics  28(2):500–531  04 2000.

[10] U. Grenander. Abstract Inference. Probability and Statistics Series. John Wiley & Sons  1981.

[11] Willem Kruijer  Judith Rousseau  and Aad van der Vaart. Adaptive bayesian density estimation with

location-scale mixtures. Electron. J. Statist.  4:1225–1257  2010.

[12] Dangna Li  Kun Yang  and Wing Hung Wong. Density estimation via discrepancy based adaptive sequential

partition. 30th Conference on Neural Information Processing Systems (NIPS 2016)  2016.

[13] Luo Lu  Hui Jiang  and Wing H. Wong. Multivariate density estimation by bayesian sequential partitioning.

Journal of the American Statistical Association  108(504):1402–1410  2013.

[14] Li Ma and Wing Hung Wong. Coupling optional pólya trees and the two sample problem. Journal of the

American Statistical Association  106(496):1553–1565  2011.

[15] Vincent Rivoirard and Judith Rousseau. Posterior concentration rates for inﬁnite dimensional exponential

families. Bayesian Anal.  7(2):311–334  06 2012.

[16] Judith Rousseau. Rates of convergence for the posterior distributions of mixtures of betas and adaptive

nonparametric estimation of the density. The Annals of Statistics  38(1):146–180  02 2010.

[17] Weining Shen and Subhashis Ghosal. Adaptive bayesian procedures using random series priors. Scandina-

vian Journal of Statistics  42(4):1194–1213  2015. 10.1111/sjos.12159.

[18] Weining Shen  Surya T. Tokdar  and Subhashis Ghosal. Adaptive bayesian multivariate density estimation

with dirichlet mixtures. Biometrika  100(3):623–640  2013.

[19] Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior distributions. The Annals of

Statistics  29(3):687–714  06 2001.

[20] Xiaotong Shen and Wing Hung Wong. Convergence rate of sieve estimates. The Annals of Statistics 

22(2):pp. 580–615  1994.

[21] Jacopo Soriano and Li Ma. Probabilistic multi-resolution scanning for two-sample differences. Journal of

the Royal Statistical Society: Series B (Statistical Methodology)  79(2):547–572  2017.

[22] Wing H. Wong and Li Ma. Optional pólya tree and bayesian inference. The Annals of Statistics  38(3):1433–

1459  06 2010.

9

,Linxi Liu
Dangna Li
Wing Hung Wong
Dalin Guo
Angela Yu
Jason Altschuler
Francis Bach
Alessandro Rudi
Jonathan Niles-Weed