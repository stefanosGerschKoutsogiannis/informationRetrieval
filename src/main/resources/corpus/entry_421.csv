2016,Adaptive Smoothed Online Multi-Task Learning,This paper addresses the challenge of jointly learning both the per-task model parameters and the inter-task relationships in a multi-task online learning setting. The proposed algorithm features probabilistic interpretation  efficient updating rules and flexible modulation on whether learners focus on their specific task or on jointly address all tasks.  The paper also proves a sub-linear regret bound as compared to the best linear predictor in hindsight. Experiments over three multi-task learning benchmark datasets show advantageous performance of the proposed approach over several state-of-the-art online multi-task learning baselines.,Adaptive Smoothed Online Multi-Task Learning

Keerthiram Murugesan∗
Carnegie Mellon University
kmuruges@cs.cmu.edu

Hanxiao Liu∗

Carnegie Mellon University
hanxiaol@cs.cmu.edu

Jaime Carbonell

Carnegie Mellon University

jgc@cs.cmu.edu

Yiming Yang

Carnegie Mellon University

yiming@cs.cmu.edu

Abstract

This paper addresses the challenge of jointly learning both the per-task model
parameters and the inter-task relationships in a multi-task online learning setting.
The proposed algorithm features probabilistic interpretation  efﬁcient updating
rules and ﬂexible modulation on whether learners focus on their speciﬁc task or
on jointly address all tasks. The paper also proves a sub-linear regret bound as
compared to the best linear predictor in hindsight. Experiments over three multi-
task learning benchmark datasets show advantageous performance of the proposed
approach over several state-of-the-art online multi-task learning baselines.

1

Introduction

The power of joint learning in multiple tasks arises from the transfer of relevant knowledge across
said tasks  especially from information-rich tasks to information-poor ones. Instead of learning
individual models  multi-task methods leverage the relationships between tasks to jointly build
a better model for each task. Most existing work in multi-task learning focuses on how to take
advantage of these task relationships  either to share data directly [1] or to learn model parameters via
cross-task regularization techniques [2  3  4]. In a broad sense  there are two settings to learn these
task relationships 1) batch learning  in which an entire training set is available to the learner 2) online
learning  in which the learner sees the data in a sequential fashion. In recent years  online multi-task
learning has attracted extensive research attention [5  6  7  8  9].
Following the online setting  particularly from [6  7]  at each round t  the learner receives a set of K
observations from K tasks and predicts the output label for each of these observations. Subsequently 
the learner receives the true labels and updates the model(s) as necessary. This sequence is repeated
over the entire data  simulating a data stream. Our approach follows an error-driven update rule in
which the model for a given task is updated only when the prediction for that task is in error. The goal
of an online learner is to minimize errors compared to the full hindsight learner. The key challenge
in online learning with large number of tasks is to adaptively learn the model parameters and the
task relationships  which potentially change over time. Without manageable efﬁcient updates at each
round  learning the task relationship matrix automatically may impose a severe computational burden.
In other words  we need to make predictions and update the models in an efﬁcient real time manner.
We propose an online learning framework that efﬁciently learns multiple related tasks by estimating
the task relationship matrix from the data  along with the model parameters for each task. We learn
the model for each task by sharing data from related task directly. Our model provides a natural
way to specify the trade-off between learning the hypothesis from each task’s own (possibly quite

∗Both student authors contributed equally.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

limited) data and data from multiple related tasks. We propose an iterative algorithm to learn the task
parameters and the task-relationship matrix alternatively. We ﬁrst describe our proposed approach
under a batch setting and then extend it to the online learning paradigm. In addition  we provide a
theoretical analysis for our online algorithm and show that it can achieve a sub-linear regret compared
to the best linear predictor in hindsight. We evaluate our model with several state-of-the-art online
learning algorithms for multiple tasks.
There are many useful application areas for online multi-task learning  including optimizing ﬁnancial
trading  email prioritization  personalized news  and spam ﬁltering. Consider the latter  where some
spam is universal to all users (e.g. ﬁnancial scams)  some messages might be useful to certain afﬁnity
groups  but spam to most others (e.g. announcements of meditation classes or other special interest
activities)  and some may depend on evolving user interests. In spam ﬁltering each user is a task 
and shared interests and dis-interests formulate the inter-task relationship matrix. If we can learn
the matrix as well as improving models from speciﬁc spam/not-spam decisions  we can perform
mass customization of spam ﬁltering  borrowing from spam/not-spam feedback from users with
similar preferences. The primary contribution of this paper is precisely the joint learning of inter-task
relationships and its use in estimating per-task model parameters in an online setting.

1.1 Related Work

While there is considerable literature in online multi-task learning  many crucial aspects remain largely
unexplored. Most existing work in online multi-task learning focuses on how to take advantage of
task relationships. To achieve this  Lugosi et. al [7] imposed a hard constraint on the K simultaneous
actions taken by the learner in the expert setting  Agarwal et. al [10] used matrix regularization  and
Dekel et. al [6] proposed a global loss function  as an absolute norm  to tie together the loss values of
the individual tasks. Different from existing online multi-task learning models  our paper proposes an
intuitive and efﬁcient way to learn the task relationship matrix automatically from the data  and to
explicitly take into account the learned relationships during model updates.
Cavallanti et. al [8] assumes that task relationships are available a priori. Kshirsagar et. al [11]
does the same but in a more adaptive manner. However such task-relation prior knowledge is either
unavailable or infeasible to obtain for many applications especially when the number of tasks K
is large [12] and/or when the manual annotation of task relationships is expensive [13]. Saha et.
al [9] formulated the learning of task relationship matrix as a Bregman-divergence minimization
problem w.r.t. positive deﬁnite matrices. The model suffers from high computational complexity as
semi-deﬁnite programming is required when updating the task relationship matrix at each online
round. We show that with a different formulation  we can obtain a similar but much cheaper updating
rule for learning the inter-task weights.
The most related work to ours is Shared Hypothesis model (SHAMO) from Crammer and Mansour
[1]  where the key idea is to use a K-means-like procedure that simultaneously clusters different
tasks and learns a small pool of m (cid:28) K shared hypotheses. Speciﬁcally  each task is free to choose
a hypothesis from the pool that better classiﬁes its own data  and each hypothesis is learned from
pooling together all the training data that belongs to the same cluster. A similar idea was explored by
Abernathy et. al [5] under expert settings.

2 Smoothed Multi-Task Learning

2.1 Setup

j )(cid:9)Nj

j   y(i)

1 to N. Let(cid:8)(x(i)

Suppose we are given K tasks where the jth task is associated with Nj training examples. For brevity
we consider a binary classiﬁcation problem for each task  but the methods generalize to multi-class
and are also applicable to regression tasks. We denote by [N ] the consecutive integers ranging from
+ be the training set and
j ∈ Rd is the ith instance

batch empirical loss for task j  respectively  where (z)+ = max(0  z)  x(i)
from the jth task and y(i)
j
We start from the motivation of our formulation in Section 2.2  based on which we ﬁrst propose a
batch formulation in Section 2.3. Then  we extend the method to the online setting in Section 2.4.

(cid:0)1 − y(i)

i=1 and Lj(w) = 1

Nj

i∈[Nj ]

j   w(cid:105)(cid:1)

j (cid:104)x(i)

is its corresponding true label.

(cid:80)

2

2.2 Motivation
Lk(wk) ∀k ∈ [K]. However 
Learning tasks may be addressed independently via w∗
when each task has limited training data  it is often beneﬁcial to allow information sharing among the
tasks  which can be achieved via the following optimization:

k = argminwk

w∗
k = argminwk

ηkjLj(wk) ∀k ∈ [K]

(1)

(cid:88)

j∈[K]

tasks have an identical data distribution  optimization (1) amounts to using(cid:80)

k to do well on the remaining K − 1
Beyond each task k  optimization (1) encourages hypothesis w∗
tasks thus allowing tasks to borrow information from each other. In the extreme case where the K
j∈[K] Nj examples for

training as compared to Nk in independent learning.
The weight matrix η is in essence a task relationship matrix  and a prior may be manually speciﬁed
according to domain knowledge about the tasks. For instance  ηkj would typically be set to a large
value if tasks k and j share similar nature. If η = I  (1) reduces to learning tasks independently. It
is clear that manual speciﬁcation of η is feasible only when K is small. Moreover  tasks may be
statistically correlated even if a domain expert is unavailable to identify an explicit relation  or if
the effort required is too great. Hence  it is often desirable to automatically estimate the optimal η
adapted to the inter-task problem structure.
We propose to learn η in a data-driven manner. For the kth task  we optimize

w∗
k  η∗

k = argminwk ηk∈Θ

ηkjLj(wk) + λr(ηk)

(2)

(cid:88)

j∈[K]

where Θ deﬁnes the feasible domain of ηk  and regularizer r prevents degenerate cases  e.g.  where
ηk becomes an all-zero vector. Optimization (2) shares the same underlying insight with Self-Paced
Learning (SPL) [14  15] where the algorithm automatically learns the weights over data points during
training. However  the process and scope in the two methods differ fundamentally: SPL minimizes
the weighted loss over datapoints within a single domain  while optimization (2) minimizes the
weighted loss over multiple tasks across possibly heterogeneous domains.
A common choice of Θ and r(ηk) in SPL is Θ = [0  1]K and r(ηk) = −(cid:107)ηk(cid:107)1. There are several
drawbacks of naively applying this type of settings to the multitask scenario: (i) Lack of focus:
there is no guarantee that the kth learner will put more focus on the kth task itself. When task k is
intrinsically difﬁcult  η∗
k becomes almost independent of the
kth task. (ii) Weak interpretability  the learned η∗
k may not be interpretable as it is not directly tied to
any physical meanings (iii) Lack of worst-case guarantee in the online setting. All those issues will
be addressed by our proposed model in the following.

kk could simply be set near zero and w∗

2.3 Batch Formulation
We parametrize the aforementioned task relationship matrix η ∈ RK×K as follows:

(3)
where I K ∈ RK×K is an identity matrix  P ∈ RK×K is a row-stochastic matrix and α is a scalar in
[0  1]. Task relationship matrix η deﬁned as above has the following interpretations:

η = αI K + (1 − α) P

1. Concentration Factor α quantiﬁes the learners’ “concentration” on their own tasks. Setting
α = 1 amounts to independent learning. We will see from the forthcoming Theorem 1 how
to specify α to ensure the optimality of the online regret bound.
2. Smoothed Attention Matrix P quantiﬁes to which degree the learners are attentive to all tasks.
Speciﬁcally  deﬁne the kth row of P   namely pk ∈ ∆K−1  as a probability distribution over
all tasks where ∆K−1 denotes the probability simplex. Our goal of learning a data-adaptive
η now becomes learning a data-adaptive attention matrix P .

Common choices about η in several existing algorithms are special cases of (3). For instance  domain
adaptation assumes α = 0 and a ﬁxed row-stochastic matrix P ; in multi-task learning  we obtain the

3

effective heuristics of specifying η by Cavallanti et. al. [8] when α = 1
there are m (cid:28) K unique distributions pk  then the problem reduces to SHAMO model [1].
Equation (3) implies the task relationship matrix η is also row-stochastic  where we always reserve
probability α for the kth task itself as ηkk ≥ α. For each learner  the presence of α entails a trade off
between learning from other tasks and concentrating on its own task. Note that we do not require
P to be symmetric due to the asymmetric nature of information transferability—while classiﬁers
trained on a resource-rich task can be well transferred to a resource-scarce task  the inverse is not
usually true. Motivated by the above discussion  our batch formulation instantiates (2) as follows

1+K and P = 1

K 11(cid:62). When

w∗
k  p∗

k = argminwk pk∈∆K−1
= argminwk pk∈∆K−1 Ej∼M ultinomial(ηk(pk))Lj(wk) − λH (pk)

j∈[K]

ηkj(pk)Lj(wk) − λH (pk)

(4)

(5)

(cid:88)

where H(pk) = −(cid:80)

j∈[K] pkj log pkj denotes the entropy of distribution pk. Optimization (4) can be
viewed as to balance between minimizing the cross-task loss with mixture weights ηk and maximizing
the smoothness of cross-task attentions. The max-entropy regularization favours a uniform attention
over all tasks and leads to analytical updating rules for pk (and ηk).
Optimization (4) is biconvex over wk and pk. With p(t)
off-the-shelf solvers. With w(t)

k ﬁxed  solution for wk can be obtained using

k ﬁxed  solution for pk is given in closed-form:

p(t+1)
kj =

λ Lj (w(t)
k )

(cid:80)K
e− 1−α
j(cid:48)=1 e− 1−α

λ Lj(cid:48) (w(t)
k )

∀j ∈ [K]

(6)

The exponential updating rule in (6) has an intuitive interpretation. That is  our algorithm attempts
to use hypothesis w(t)
k obtained from the kth task to classify training examples in all other tasks.
Task j will be treated as related to task k if its training examples can be well classiﬁed by wk. The
intuition is that two tasks are likely to relate to each other if they share similar decision boundaries 
thus combining their associated data should yield to a stronger model  trained over larger data.

2.4 Online Formulation

In this section  we extend our batch formulation to the online setting. We assume that all tasks will be
performed at each round  though the assumption can be relaxed with some added complexity to the
k (cid:105) and
method. At time t  the kth task receives a training instance x(t)
suffers a loss after y(t) is revealed. Our algorithm follows a error-driven update rule in which the
model is updated only when a task makes a mistake.
kj (w) = 1 − y(t)
Let (cid:96)(t)
introduce shorthands (cid:96)(t)

k (cid:105) < 1 and (cid:96)kj(w) = 0 otherwise. For brevity  we
kj = ηkj(p(t)
k ).

k   makes a prediction (cid:104)x(t)

j (cid:104)x(t)
kj = (cid:96)(t)

k   w(t)

j   w(t)

For the kth task we consider the following optimization problem at each time:

j   w(cid:105) if y(t)
j (cid:104)x(t)
kj (w(t)
k ) and η(t)
(cid:88)

w(t+1)

k

  p(t+1)

k

= argmin

C

ηkj(pk)(cid:96)(t)

kj (wk) + (cid:107)wk − w(t)

k (cid:107)2 + λDKL

wk pk∈∆K−1

j∈[K]

j∈[K] ηkj(pk)(cid:96)(t)

where(cid:80)

kj (wk) = Ej∼M ulti(ηk(pk))(cid:96)(t)

(cid:0)pk(cid:107)p(t)
kj (wk) at time t  and negative entropy −H(pk) =(cid:80)

back–Leibler (KL) divergence between current and previous soft-attention distributions. The presence
of last two terms in (7) allows the model parameters to evolve smoothly over time. Optimization
(7) is naturally analogous to the batch optimization (4)  where the batch loss Lj(wk) is replaced
by its noisy version (cid:96)(t)
j pkj log pkj is replaced
by DKL(pk(cid:107)p(t)
k ) also known as the relative entropy. We will show the above formulation leads to
analytical updating rules for both wk and pk  a desirable property particularly as an online algorithm.

kj (wk)  and DKL

k

(cid:1)

(cid:0)pk(cid:107)p(t)
(cid:1) denotes the Kull-

(7)

k

4

Solution for w(t+1)

conditioned on p(t)

k
w(t+1)

k

= prox(w(t)

k ) = argminwk

k is given in closed-form by the proximal operator
kj (wk) + (cid:107)wk − w(t)
k (cid:107)2

ηkj(p(t)

k )(cid:96)(t)

C

(cid:88)
(cid:88)

j∈[K]

(8)

(9)

(11)

= w(t)

k + C

ηkj(p(t)

k )y(t)

j x(t)

j

j:y(t)

j (cid:104)x(t)

j  w(t)

k (cid:105)<1

k is also given in closed-form  analogous to mirror descent [16]
(10)

pkj(cid:96)(t)

kj + λDKL

k

(cid:0)pk(cid:107)p(t)

(cid:1)

(cid:88)

j∈[K]

Solution for p(t+1)

k

conditioned on w(t)

p(t+1)
k

= argminpk∈∆K−1 C(1 − α)
(cid:80)

− C(1−α)

− C(1−α)

p(t)
kj e
j(cid:48) p(t)
kj(cid:48)e

(cid:96)(t)
kj(cid:48)

(cid:96)(t)
kj

λ

λ

j ∈ [K]

=⇒ p(t+1)

kj =

The pseudo-code is in Algorithm 22. Our algorithm is “passive” in the sense that updates are carried
out only when a classiﬁcation error occurs  namely when ˆy(t)
k . An alternative is to perform
k
“aggressive” updates only when the active set {j : y(t)

(cid:54)= y(t)
k (cid:105) < 1} is non-empty.

j (cid:104)x(t)

j   w(t)

Algorithm 1: Batch Algorithm (SMTL-e)
while not converge do

Algorithm 2: Online Algorithm (OSMTL-e)
for t ∈ [T ] do

αLk(wk) + (1 −

for k ∈ [K] do

α)(cid:80)
k ← argminwk
w(t)
j∈[K] p(t)
for j ∈ [K] do
(cid:80)K
kj ← e
p(t+1)
j(cid:48)=1

kj Lj(wk);

− 1−α

λ

Lj (w

(t)
k

)

− 1−α

λ

e

L

j(cid:48) (w

(t)
k

end

end
t ← t + 1;

end

if y(t)

for k ∈ [K] do
k (cid:104)x(t)
k   w(t)
k ← w(t)
w(t+1)

C(1 − α)(cid:80)

k (cid:105) < 1 then
k + Cα1
(t)
(cid:96)
kk >0
p(t)
kj y(t)

j:(cid:96)

(t)
kj >0

for j ∈ [K] do

y(t)
k x(t)
j x(t)

k +
;

j

kj ← p
(cid:80)K
p(t+1)
j(cid:48)=1

(t)
kj e

(t)

kj(cid:48) e
p

− C(1−α)

λ

(cid:96)

(t)
kj

− C(1−α)

λ

(cid:96)

(t)

kj(cid:48)

;

w(t+1)

k

k ← w(t)
  p(t+1)

k   p(t)
k ;

;

)

else

end

end

end

end

k

2.5 Regret Bound

k ∈ Rd  y(t)

where x(t)
and let α be some predeﬁned parameter in [0  1]. Let {w∗
w∗

(cid:1)(cid:9)T
Theorem 1. ∀k ∈ [K]  let Sk =(cid:8)(cid:0)x(t)
k   y(t)
t=1 be a sequence of T examples for the kth task
k (cid:107)2 ≤ R  ∀t ∈ [T ]. Let C be a positive constant
k ∈ {−1  +1} and (cid:107)x(t)
k ∈ Rd and its hinge loss on the examples(cid:0)x(t)
k}k∈[K] be any arbitrary vectors where
kj =(cid:0)1 − y(t)
(cid:0)1 − y(t)
kk =
+ and (cid:96)(t)∗
(cid:1) ≤ 1

+  respectively.
If {Sk}k∈[K] is presented to OSMTL algorithm  then ∀k ∈ [K] we have

(cid:1) and(cid:0)x(t)
(cid:16)

k (cid:104)x(t)
k   w∗
(cid:88)

j(cid:54)=k are given by (cid:96)(t)∗

(1 − α)T

k(cid:105)(cid:1)

k(cid:105)(cid:1)

j (cid:104)x(t)

j   w∗

k   y(t)

j   y(t)

(cid:96)(t)∗

CR2T

(cid:107)w∗

(cid:17)

(cid:96)(t)∗
kk + max

k(cid:107)2 +

kk − (cid:96)(t)∗
((cid:96)(t)

kk

(12)

(cid:1)

+

kj

k

j

j∈[K] j(cid:54)=k

2α

2Cα

α

t∈[T ]

Notice when α → 1  the above reduces to the perceptron mistake bound [17].

2It is recommended to set α ∝ √
√
T
T

1+

√
and C ∝ 1+

T

T

  as suggested by Corollary 2.

5

Corollary 2. Let α =

and C = 1+
T

T

√
√
T
T

1+

kk − (cid:96)(t)∗
((cid:96)(t)

kk

(cid:88)

t∈[T ]

(cid:1) ≤

√

T

√

(cid:18) 1

2

in Theorem 1  we have
k(cid:107)2 + (cid:96)(t)∗

kk + max

(cid:107)w∗

j∈[K] j(cid:54)=k

(cid:19)

(cid:96)(t)∗
kj + 2R2

(13)

Proofs are given in the supplementary. Theorem 1 and Corollary 2 have several implications:

1. Quality of the bound depends on both (cid:96)(t)∗

kk and the maximum of {(cid:96)(t)∗
words  the worst-case regret will be lower if the kth true hypothesis w∗
training examples in both the kth task itself as well as those in all the other tasks.

kj }j∈[K] j(cid:54)=k. In other
k can well distinguish

2. Corollary 2 indicates the difference between the cumulative loss achieved by our algorithm

and by any ﬁxed hypothesis for task k is bounded by a term growing sub-linearly in T .

√
√
T

3. Corollary 2 provides a principled way to set hyperparameters to achieve the sub-linear
regret bound. Speciﬁcally  recall α quantiﬁes the self-concentration of each task. Therefore 
T→∞−→ 1 implies for large horizon it would be less necessary to rely on other tasks
α =
T→∞−→ 0 suggests

as available supervision for the task itself is already plenty; C = 1+
T
diminishing learning rate over the horizon length.

√

T

1+

T

3 Experiments

We evaluate the performance of our algorithm under batch and online settings. All reported results in
this section are averaged over 30 random runs or permutations of the training data. Unless otherwise
speciﬁed  all model parameters are chosen via 5-fold cross validation.

3.1 Benchmark Datasets

We use three datasets for our experiments. Details are given below:
Landmine Detection3 consists of 19 tasks collected from different landmine ﬁelds. Each task is a
binary classiﬁcation problem: landmines (+) or clutter (−) and each example consists of 9 features
extracted from radar images with four moment-based features  three correlation-based features  one
energy ratio feature and a spatial variance feature. Landmine data is collected from two different
terrains: tasks 1-10 are from highly foliated regions and tasks 11-19 are from desert regions  therefore
tasks naturally form two clusters. Any hypothesis learned from a task should be able to utilize the
information available from other tasks belonging to the same cluster.
Spam Detection4 We use the dataset obtained from ECML PAKDD 2006 Discovery challenge for
the spam detection task. We used the task B challenge dataset which consists of labeled training
data from the inboxes of 15 users. We consider each user as a single task and the goal is to build
a personalized spam ﬁlter for each user. Each task is a binary classiﬁcation problem: spam (+)
or non-spam (−) and each example consists of approximately 150K features representing term
frequency of the word occurrences. Since some spam is universal to all users (e.g. ﬁnancial scams) 
some messages might be useful to certain afﬁnity groups  but spam to most others. Such adaptive
behavior of user’s interests and dis-interests can be modeled efﬁciently by utilizing the data from
other users to learn per-user model parameters.
Sentiment Analysis5 We evaluated our algorithm on product reviews from amazon. The dataset
contains product reviews from 24 domains. We consider each domain as a binary classiﬁcation task.
Reviews with rating > 3 were labeled positive (+)  those with rating < 3 were labeled negative (−) 
reviews with rating = 3 are discarded as the sentiments were ambiguous and hard to predict. Similar
to the previous dataset  each example consists of approximately 350K features representing term
frequency of the word occurrences.
We choose 3040 examples (160 training examples per task) for landmine  1500 emails for spam (100
emails per user inbox) and 2400 reviews for sentiment (100 reviews per domain) for our experiments.

3http://www.ee.duke.edu/~lcarin/LandmineData.zip
4http://ecmlpkdd2006.org/challenge.html
5http://www.cs.jhu.edu/~mdredze/datasets/sentiment

6

Figure 1: Average AU C calculated for compared models (left). A visualization of the task relationship
matrix in Landmine learned by SMTL-t (middle) and SMTL-e (right). The probabilistic formulation
of SMTL-e allows it to discover more interesting patterns than SMTL-t.

Note that we intentionally kept the size of the training data small to drive the need for learning from
other tasks  which diminishes as the training sets per task become large. Since all these datasets have
a class-imbalance issue (with few (+) examples as compared to (−) examples)  we use average Area
Under the ROC Curve (AU C) as the performance measure.

3.2 Batch Setting

kj )+

kj ∝ (λ − (cid:96)(t)

Since the main focus of this paper is online learning  we brieﬂy conduct an experiment on landmine
detection dataset for our batch learning to demonstrate the advantages of learning from shared data.
We implement two versions of our proposed algorithm with different updates: SMTL-t (SMTL with
thresholding updates) where p(t+1)
6 and SMTL-e (SMTL with exponential updates)
as in Algorithm 1. We compare our SMTL* with two standard baseline methods for our batch setting:
Independent Task Learning (ITL)—learning a single model for each task and Single Task Learning
(STL)—learning a single classiﬁcation model for pooled data from all the tasks. In addition we
compare our models with SHAMO  which is closest in spirit with our proposed models. We select the
value for λ and α for SMTL* and M for SHAMO using cross validation.
Figure 1 (left) shows the average AU C calculated for different training size on landmine. We can see
that the baseline results are similar to the ones reported by Xue et. al [3]. Our proposed algorithm
(SMTL*) outperforms the other baselines but when we have very few training examples (say 20 per
task)  the performance of STL improves as it has more examples than the others. Since η depends
on the loss incurred on the data from related tasks  this loss-based measure can be unreliable for a
small training sample size. To our surprise  SHAMO performs worse than the other models which
tells us that assuming two tasks are exactly same (in the sense of hypothesis) may be inappropriate in
real-world applications. Figure 1 (middle & left) show the task relationship matrix η for SMTL-t and
SMTL-e on landmine when the number of training instances is 160 per task.

3.3 Online Setting

To evaluate the performance of our algorithm in the online setting  we use all three datasets (landmine 
spam and sentiment) and compare our proposed methods to 5 baselines. We implemented two
variations of Passive-Aggressive algorithm (PA) [18]. PA-ITL learns independent model for each task
and PA-ONE builds a single model for all the tasks. We also implemented the algorithm proposed by
Dekel et. al for online multi-task learning with shared loss (OSGL) [6]. These three baselines do not
exploit the task-relationship or the data from other tasks during model update. Next  we implemented
two online multi-task learning related to our approach: FOML – initializes η with ﬁxed weights [8] 
Online Multi-Task Relationship Learning (OMTRL) [9] – learns a task covariance matrix along with
task parameters. We could not ﬁnd a better way to implement the online version of the SHAMO
algorithm  since the number of shared hypotheses or clusters varies over time.

6Our algorithm and theorem can be easily generalized to other types of updating rules by replacing exp in

(6) with other functions. In latter cases  however  η may no longer have probabilistic interpretations.

7

 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0 50 100 150 200 250 300AUCTraining SizeSTLITLSHAMOSMTL-tSMTL-e12345678910111213141516171819123456789101112131415161718191234567891011121314151617181912345678910111213141516171819Table 1: Average performance on three datasets: means and standard errors over 30 random shufﬂes.

Landmine Detection

Spam Detection

Sentiment Analysis

Time (s)

Time (s)

Time (s)

Models

PA-ONE

PA-ITL

OSGL

FOML

OMTRL

OSMTL-t

OSMTL-e

AUC
0.5473
(0.12)
0.5986
(0.04)
0.6482
(0.03)
0.6322
(0.04)
0.6409
(0.05)
0.6776
(0.03)
0.6404
(0.04)

nSV
2902.9
(4.21)
618.1
(27.31)
740.8
(42.03)
426.5
(36.91)
432.2
(123.81)
333.6
(40.66)

458

(36.79)

0.01

0.01

0.01

0.11

6.9

0.18

0.19

AUC
0.8739
(0.01)
0.8350
(0.01)
0.9551
(0.007)
0.9347
(0.009)
0.9343
(0.008)
0.9509
(0.007)
0.9596
(0.006)

nSV
1455.0
(4.64)
1499.9
(0.37)
1402.6
(13.57)
819.8
(18.57)
840.4
(22.67)
809.5
(19.35)
804.2
(19.05)

0.16

0.16

0.17

1.5

53.6

1.4

1.3

AUC
0.7193
(0.03)
0.7364
(0.02)
0.8375
(0.02)
0.8472
(0.02)
0.7831
(0.02)
0.9354
0.01
0.9465
(0.01)

nSV
2350.7
(6.36)
2399.9
(0.25)
2369.3
(14.63)
1356.0
(78.49)
1346.2
(85.99)
1312.8
(79.15)
1322.2
(80.27)

0.19

0.16

0.17

1.20

128

2.15

2.16

Table 1 summarizes the performance of all the above algorithms on the three datasets. In addition to
the AU C scores  we report the average total number of support vectors (nSV) and the CPU time taken
for learning from one instance (Time). From the table  it is evident that OSMTL* outperforms all the
baselines in terms of both AU C and nSV. This is expected for the two default baselines (PA-ITL and
PA-ONE). We believe that PA-ONE shows better result than PA-ITL in spam because the former learns
the global information (common spam emails) that is quite dominant in spam detection problem. The
update rule for FOML is similar to ours but using ﬁxed weights. The results justify our claim that
making the weights adaptive leads to improved performance.
In addition to better results  our algorithm consumes less or comparable CPU time than the baselines
which take into account inter-task relationships. Compared to the OMTRL algorithm that recomputes
the task covariance matrix every iteration using expensive SVD routines  the adaptive weights in
our are updated independently for each task. As speciﬁed in [9]  we learn the task weight vectors
for OMTRL separately as K independent perceptron for the ﬁrst half of the training data available
(EPOCH=0.5). OMTRL potentially looses half the data without learning task-relationship matrix as
it depends on the quality of the task weight vectors.
It is evident from the table that algorithms which use loss-based update weights η (OSGL  OSMTL*)
considerably outperform the ones that do not use it (FOML OMTRL). We believe that loss incurred
per instance gives us valuable information for the algorithm to learn from that instance  as well as to
evaluate the inter-dependencies among tasks. That said  task relationship information does help by
learning from the related tasks’ data  but we demonstrate that combining both the task relationship
and the loss information can give us a better algorithm  as is evident from our experiments.
We would like to note that our proposed algorithm OSMTL* does exceptionally better in sentiment 
which has been used as a standard benchmark application for domain adaptation experiments in
the existing literature [19]. We believe the advantageous results on sentiment dataset implies that
even with relatively few examples  effectively knowledge transfer among the tasks/domains can be
achieved by adaptively choosing the (probabilistic) inter-task relationships from the data.

4 Conclusion

We proposed a novel online multi-task learning algorithm that jointly learns the per-task hypothesis
and the inter-task relationships. The key idea is based on smoothing the loss function of each task
w.r.t. a probabilistic distribution over all tasks  and adaptively reﬁning such distribution over time. In
addition to closed-form updating rules  we show our method achieves the sub-linear regret bound.
Effectiveness of our algorithm is empirically veriﬁed over several benchmark datasets.

Acknowledgments

This work is supported in part by NSF under grants IIS-1216282 and IIS-1546329.

8

References
[1] Koby Crammer and Yishay Mansour. Learning multiple tasks using shared hypotheses. In

Advances in Neural Information Processing Systems  pages 1475–1483  2012.

[2] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008.

[3] Ya Xue  Xuejun Liao  Lawrence Carin  and Balaji Krishnapuram. Multi-task learning for
classiﬁcation with dirichlet process priors. The Journal of Machine Learning Research  8:35–63 
2007.

[4] Yu Zhang and Dit-Yan Yeung. A regularization approach to learning task relationships in
multitask learning. ACM Transactions on Knowledge Discovery from Data (TKDD)  8(3):12 
2014.

[5] Jacob Abernethy  Peter Bartlett  and Alexander Rakhlin. Multitask learning with expert advice.

In Learning Theory  pages 484–498. Springer  2007.

[6] Ofer Dekel  Philip M Long  and Yoram Singer. Online learning of multiple tasks with a shared

loss. Journal of Machine Learning Research  8(10):2233–2264  2007.

[7] Gábor Lugosi  Omiros Papaspiliopoulos  and Gilles Stoltz. Online multi-task learning with

hard constraints. arXiv preprint arXiv:0902.3526  2009.

[8] Giovanni Cavallanti  Nicolo Cesa-Bianchi  and Claudio Gentile. Linear algorithms for online

multitask classiﬁcation. The Journal of Machine Learning Research  11:2901–2934  2010.

[9] Avishek Saha  Piyush Rai  Suresh Venkatasubramanian  and Hal Daume. Online learning of
multiple tasks and their relationships. In International Conference on Artiﬁcial Intelligence and
Statistics  pages 643–651  2011.

[10] Alekh Agarwal  Alexander Rakhlin  and Peter Bartlett. Matrix regularization techniques for
online multitask learning. EECS Department  University of California  Berkeley  Tech. Rep.
UCB/EECS-2008-138  2008.

[11] Meghana Kshirsagar  Jaime Carbonell  and Judith Klein-Seetharaman. Multisource transfer
learning for host-pathogen protein interaction prediction in unlabeled tasks. In NIPS Workshop
on Machine Learning for Computational Biology  2013.

[12] Kilian Weinberger  Anirban Dasgupta  John Langford  Alex Smola  and Josh Attenberg. Feature
hashing for large scale multitask learning. In Proceedings of the 26th Annual International
Conference on Machine Learning  pages 1113–1120. ACM  2009.

[13] Meghana Kshirsagar  Jaime Carbonell  and Judith Klein-Seetharaman. Multitask learning for

host–pathogen protein interactions. Bioinformatics  29(13):i217–i226  2013.

[14] M Pawan Kumar  Benjamin Packer  and Daphne Koller. Self-paced learning for latent variable

models. In Advances in Neural Information Processing Systems  pages 1189–1197  2010.

[15] Lu Jiang  Deyu Meng  Shoou-I Yu  Zhenzhong Lan  Shiguang Shan  and Alexander Hauptmann.
Self-paced learning with diversity. In Advances in Neural Information Processing Systems 
pages 2078–2086  2014.

[16] A-S Nemirovsky  D-B Yudin  and E-R Dawson. Problem complexity and method efﬁciency in

optimization. 1982.

[17] Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory  algorithms  and applications.

PhD Dissertation  2007.

[18] Koby Crammer  Ofer Dekel  Joseph Keshet  Shai Shalev-Shwartz  and Yoram Singer. Online
passive-aggressive algorithms. The Journal of Machine Learning Research  7:551–585  2006.

[19] John Blitzer  Mark Dredze  Fernando Pereira  et al. Biographies  bollywood  boom-boxes and
blenders: Domain adaptation for sentiment classiﬁcation. In ACL  volume 7  pages 440–447 
2007.

9

,Keerthiram Murugesan
Hanxiao Liu
Jaime Carbonell
Yiming Yang
Dan Xu
Wanli Ouyang
Xavier Alameda-Pineda
Xiaogang Wang
Nicu Sebe