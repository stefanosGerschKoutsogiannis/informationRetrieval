2013,Documents as multiple overlapping windows into grids of counts,In text analysis documents are represented as disorganized bags of words  models of count features are typically based on mixing a small number of topics \cite{lda sam}. Recently  it has been observed that for many text corpora documents evolve into one another in a smooth way  with some features dropping and new ones being introduced. The counting grid \cite{cgUai} models this spatial metaphor literally: it is multidimensional grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper  we overcome to this issue with the \emph{Componential Counting Grid} which brings the componential nature of topic models to the basic counting grid. We also introduce a generative kernel based on the document's grid usage and a visualization strategy useful for understanding large text corpora. We evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.,Documents as multiple overlapping windows into a

grid of counts

Alessandro Perina1

Nebojsa Jojic1

Manuele Bicego2

Andrzej Turski1

1Microsoft Corporation  Redmond  WA

2University of Verona  Italy

Abstract

In text analysis documents are often represented as disorganized bags of words;
models of such count features are typically based on mixing a small number of
topics [1 2]. Recently  it has been observed that for many text corpora documents
evolve into one another in a smooth way  with some features dropping and new
ones being introduced. The counting grid [3] models this spatial metaphor liter-
ally: it is a grid of word distributions learned in such a way that a document’s own
distribution of features can be modeled as the sum of the histograms found in a
window into the grid. The major drawback of this method is that it is essentially
a mixture and all the content must be generated by a single contiguous area on
the grid. This may be problematic especially for lower dimensional grids. In this
paper  we overcome this issue by introducing the Componential Counting Grid
which brings the componential nature of topic models to the basic counting grid.
We evaluated our approach on document classiﬁcation and multimodal retrieval
obtaining state of the art results on standard benchmarks.

Introduction

1
A collection of documents  each consisting of a disorganized bag of words is often modeled
compactly using mixture or admixture models  such as Latent Semantic Analysis (LSA) [4] and
Latent Dirichlet Allocation (LDA) [1]. The data is represented by a small number of semantically
tight topics  and a document is assumed to have a mix of words from an even smaller subset of these
topics. There are no strong constraints in how the topics are mixed [5].
it has been observed that for many text corpora
Recently  an orthogonal approach emerged:
documents evolve into one another in a smooth way  with some words dropping and new ones
being introduced. The counting grid model (CG) [3] takes this spatial metaphor – of moving
through sources of words and dropping and picking new words – literally: it is multidimensional
grid of word distributions  learned in such a way that a document’s own distribution of words can
be modeled as the sum of the distributions found in some window into the grid. By using large
windows to collate many grid distributions from a large grid  CG model can be a very large mixture
without overtraining  as these distributions are highly correlated. LDA model does not have this
beneﬁt  and thus has to deal with a smaller number of topics to avoid overtraining.

In Fig.1a we show an excerpt of a grid learned from cooking recipes from around the world. Each
position in the grid is characterized by a distribution over the words in a vocabulary and for each
position we show the 3 words with higher probability whenever they exceed a threshold. The shaded
positions  are characterized by the presence  with a non-zero probability  of the word “bake”1. On
the grid we also show the windows W of size 4 ⇥ 5 for 5 recipes. Nomi (1)  an Afghan egg-based
bread  is close to the recipe of the usual pugliese bread (2)  as indeed they share most of the ingre-
dients and procedure and their windows largely overlap. Note how moving from (1) to (2) the word

1Which may or may not be in the top three

1

Figure 1: a) A particular of a E = 30 ⇥ 30 componential counting grid ⇡i learned over a corpus
of recipes. In each cell we show the 0-3 most probable words greater than a threshold. The area
in shaded red has ⇡(0bake0) > 0. b) For 6 recipes  we show how their components are mapped
onto this grid. The “mass” of each component (e.g.  ✓ see Sec.2) is represented with the window
thickness. For each component c = j in position j  we show the words generated in each window

cz ·Pj2Wi

⇡j(z)

“egg” is dropped. Moving to the right we encounter the basic pizza (3) whose dough is very simi-
lar to the bread’s. Continuing to the right words often associated to desserts like sugar  almond  etc
emerge. It is not surprising that baked desserts such as cookies (4)  and pastry in general  are mapped
here. Finally further up we encounter other desserts which do not require baking  like tiramisu (5) 
or chocolate crepes. This is an example of a“topical shift”; others appear in different portions of the
full grid which is included in the additional material.
The major drawback of counting grids is that they are essentially a mixture model  assuming only
one source for all features in the bag and the topology of the space highly constrains the document
mappings resulting in local minima or suboptimal grids. For example  more structured recipes like
Grecian Chicken Gyros Pizza or Tex-Mex pizza would have very low likelihood  as words related to
meat  which is abundant in both  are hard to generate in the baking area where the recipes would
naturally goes.
As ﬁrst contribution we extend here the counting grid model so that each document can be rep-
resented by multiple latent windows  rather than just one. In this way  we create a substantially
more ﬂexible admixture model  the componential counting grid (CCG)  which becomes a direct
generalization of LDA as it does allow multiple sources (e.g.  the windows) for each bag  in a math-
ematically identical way as LDA. But  the equivalent of LDA topics are windows in a counting grid 
which allows the model to have a very large number of topics that are highly related  as shift in the
grid only slightly reﬁnes any topic.
Starting from the same grid just described  we recomputed the mapping of each recipe which now
can be described by multiple windows  if needed. Fig. 1b shows mappings for some recipes. Also
the words generated in each component are shown. The three pizzas place most of the mass in the
same area (dough)  but the words related to the topping are borrowed from different areas. Another
example is the Caesar salad which have a component in the salad/vegetable area  and borrows the

2

 grainricecooktypecookerresultantgoodwantdoesntmethodusualhoweverbecausebeingsdonttarkawhitebrandypourchocolatepeakmascarponegradualchocolatefolslowlycleardishrunstartchangesitwayindiantryindianknowgoinggenerationtheykeptonlyexcellencequiteelectricmeringuerumyolkbeatgranulatedcutletpourgentlypickliftbackfullpersianmayreheatvaryusefulgivensectionlookneededcompletionstoretastynormalcontainerairtightelectricextractmixervanillaspeedswirebeatcakeeggspringformspoonfulcarefullypourbottomwoodenprocedurespatulaquicklyroundnonstickspreaddosaleastpancaketexturebiscottigriddlealwayscrepeairtightlongalmondpeachrackcinnamonsugarbutterrindconfectionersalternativepanfullargersideinvertupsideomeletslideslipcylinderspatulasecondlogapartmentgriddlecookiebiscuitpretzelsiftgreasepaperparchmentgoldenfoldpressingmoisteneggbreadcrumbpanfulcrumbbreadtoothpickcrumbcrustyeggbeatenlightlyaltogetheronenaanturnhandfulincorporatehandfulrotidiameterroundinchtogethersheetpastryeggpressingsheetbordersidesaltmixturepoursidebrowndishmixturesheetadditionalbeatstickyshapebrushdividerollrectanglecutsealedgeedgesheetplaceremovablechivespreheatminuteovenproofpreheatmiddlesheetrackbowlturnmixerbulkdoughkneadboarddivideshapesurfacetowelcenterformfoldsealremovableplacedegreeovenovenpreheatbakepreheatbaguetteworkbowlrisesmoothekneadelasticcircledoughclothdamproundcentertogetherformleftsquaretrianglraviolsetasidecentercenterarrangepreheat oildishgreasespraycornmealpizzaloafsurfaceloavesdoublesprinkleballyeastrisewarmballpalmusefulequalbunstartbitworkwrapperlinzestpuddinghalfmixcompletioncoolpourpatterncoolinserttraysharpresemblanceformsurfacelooselongmoistmachinebreadfeelstartersizedesirableamountthoroughlykitchenreadyamountfeedingneatbakeovenNoni Afghan BreadBrown BreadCeasar SaladPizza di NapoliGrecian Chicken Gyros Pizza 'dough' 'roll' 'ball' 'shape' 'yeast' 'knead' 'rise' 'bread' 'egg' 'dough' 'roll' 'yeast' 'knead' 'shape' 'desirable' 'water' 'divide' 'keep' 'water' 'aside' 'add' 'smoothe' 'minute' 'lukewarm' 'remain' 'fry' 'sauce' 'deep' 'oil' 'hot' 'golden' 'mix' 'lettuce' 'salad' 'slice' 'garnish' 'dressings' 'beans' 'mix' 'cheese' 'place' 'melt' 'basil' 'cover' 'bag' 'broil' 'chicken' 'marinade' 'shallow' 'hot' 'coat' 'refrigeration' 'heat' 'crust' 'evenly' 'spread' 'edge' 'pressing' 'center' 'place' 'feta' 'mixture' 'useful'a)b)(cid:83)iWj(1)(2)(3)(4)(5)[...][...][...]croutons from the bread area.
By observing Fig.1b  one can also notice how the embedding produced by CCGs yields to a sim-
ilarity measure based on the grid usage of each sample. For example  words relative to the three
pizzas are generated from windows that overlap  therefore they share words usage and thus they are
“similar”. As second contribution we exploited this fact to deﬁne a novel generative kernel  whose
performance largely outperformed similar classiﬁcation strategies based on LDA’s topic usage [1 2].
We evaluated componential counting grids and in particular the kernel  on the 20-Newsgroup dataset
[6]  on a novel dataset of recipes which we will make available to the community  and on the re-
cent “Wikipedia picture of the day” dataset [7]. In all the experiments  CCGs set a new state of the
art. Finally  for the ﬁrst time we explore visualization through examples and videos available in the
additional material.

2 Counting Grids and Componential Counting Grids
The basic Counting Grid ⇡i is a set of distribu-
tions over the vocabulary on the N-dimensional
discrete grid indexed by i where each id 2
[1 . . . Ed] and E describes the extent of the
counting grid in d dimensions. The index z in-
dexes a particular word in the vocabulary z =
[1 . . . Z] being Z the size of the vocabulary. For
example  ⇡i(0P izza0) is the probability of the
word “Pizza” at the location i. Since ⇡ is a grid

t=1 and each word wt

of distributions Pz ⇡i(z) = 1 everywhere on
the grid. Each bag of words is represented by a
n takes
list of words {wt}T
a value between 1 and Z. In the rest of the pa-
per  we will assume that all the samples have N
words.
Counting Grids assume that each bags follow
a word distribution found somewhere in the
counting grid; in particular  using windows of
dimensions W  a bag can be generated by ﬁrst
averaging all counts in the window Wi starting
at grid location i and extending in each direc-

Figure 2: a) Plate notation representing the CCG
model. b) CCG generative process for one word:
Pick a window from ✓  Pick a position within the
window  Pick a word. c) Illustration of U W and
relative to the particular ✓ shown in plate b).
⇤W
✓

tion d by Wd grid positions to form the histogram hi(z) = 1Qd WdPj2Wi
⇡j(z)  and then generating
a set of features in the bag (see Fig.1a where we used a 3 ⇥ 4 window). In other words  the position
of the window i in the grid is a latent variable given which we can write the probability of the bag
as
⇡j(wn) 

Relaxing the terminology  E and W are referred to as  respectively  the counting grid and the win-
dow size. The ratio of the two volumes    is called the capacity of the model in terms of an
equivalent number of topics  as this is how many non-overlapping windows can be ﬁt onto the grid.
Finally  with Wi we indicate the particular window placed at location i.

Qd Wd · Xj2Wi

1

p({w}|i) =Yn

hi z =Yn 

Componential Counting Grids As seen in the previous section  counting grids generate words
from a distribution in a window W   placed at location i in the grid. Windows close in the grid
generate similar features because they share many cells: As we move the window on the grid 
some new features appear while others are dropped. On the other hand componential models  like
[1]  represent the standard way of modeling of text corpora. In these models each feature can be
generated by a different source or topic  and documents are then seen as admixtures of topics.
Componential counting grids get the best of both worlds: being based on the counting grid geometry
they capture smooth shifts of topics  plus their componential nature  which allows documents to be
generated by several windows (akin to LDA’s topics). The number of windows need not be speciﬁed
a-priori.
Componential Counting Grids assumes the following generative process (also illustrated by Fig.2b.)
for each document in a corpus:

3

 Uw(cid:47)(cid:84)wc)(cid:84)lnknwn(cid:83)(cid:68)NT(cid:68)Z = |Vocabulary|wn = ‘Pizza’(cid:83)knln=(5 3)kn=ln +(0 3)Pick a window W from (cid:84)Pick a location within the window WPick a word from the distribution (cid:83)kb)a)1. Sample the multinomial over the locations ✓ ⇠ Dir(↵)
2. For each of the N words wn

a) Choose a at location ln ⇠ M ultinomial(✓) for a window of size W
b) Choose a location within the window Wln; kn
c) Choose a word wn from ⇡kn

As visible  each word wn is generated from a different window  placed at location ln  but the choice
of the window follows the same prior distributions ✓ for all words. It worth noticing that when
W = 1 ⇥ 1  ln = kn and the model becomes Latent Dirichlet Allocation.
The Bayesian network is shown in Fig.2a) and it deﬁnes the following joint probability distribution
(1)

p(wn|kn ⇡ ) · p(kn|ln) · p(ln|✓) · p(✓|↵)

P =Yt nXln Xkn

where p(wn = z|kn = i ⇡ ) = ⇡i(z) is a multinomial over the vocabulary  p(kn = i|ln = k) =
) in the
U W (i  k) is a distribution over the grid locations  with U W uniform and equal to ( 1
|W|
upper left window of size W and 0 elsewhere (See Fig.2c). Finally p(ln|✓) = ✓(l) is the prior
distribution over the windows location  and p(✓|↵) = Dir(✓; ↵) is a Dirichlet distribution of
parameters ↵.

Since the posterior distribution p(k  l ✓ |w ⇡ ↵ ) is intractable for exact inference  we learned the
model using variational inference [8].
We ﬁrstly introduced the posterior distributions q  approximating the true posterior as qt(k  l ✓ ) =
qt(✓)·Qnqt(kn)· qt(ln) being q(kn) and q(ln) multinomials over the locations  and q(✓) a Dirac
function centered at the optimal value ˆ✓.
Then by bounding (variationally) the non-constant part of log P   we can write the negative free
energy F  and use the iterative variational EM algorithm to optimize it.
qt(kn)· qt(ln)· log ⇡kn(wn)· U W (kn ln)· ✓ln · p(✓|↵)H(qt)⌘
log P  F =Xt ⇣Xn  Xln kn

where H(q) is the entropy of the distribution q.
Minimization of Eq. 2 reduces in the following update rules:

(2)

(3)

(4)

(5)

(6)

qt(ln = j) · log U W (i  j)⌘
qt(kn = j) · log U W (j  i)⌘

qt(kn = i) / ⇡i(wn) · exp⇣Xln=j
qt(ln = i) / ✓t(i) · exp⇣Xkn=j
✓t(i) / ↵i  1 +Xn
⇡i(z) / Xt Xn

qt(ln = i)

qt(kn = i)[wn=z]

where [wn = z] is an indicator function  equal to 1 when wn is equal to z. Finally  the parameters ↵
of the Dirichlet prior can be either kept ﬁxed [9] or learned using standard techniques [10].
The minimization procedure described by Eqs.3-6 can be carried out efﬁciently in O(N log N )
time using FFTs [11].

Some simple mathematical manipulations of Eq.1 can yield to a speed up. In fact  from Eq.1 one
can marginalize the variable ln

P = Yt n Xln=i kn=j
= Yt n Xln=i kn=j
= Yt n Xkn=j

p(wn|kn = j) · p(kn = j|ln = i) · p(ln = i|✓) · p(✓|↵)
⇡j(wn) · U W (j  i) · ✓(i) · p(✓(i)|↵i)

⇡j(wn) ·⇣Xln=i

U W (j  i) · ✓(i)⌘ · p(✓(i)|↵i) =Yt n Xkn=j

⇡j(wn) · ⇤W

✓t · p(✓(i)|↵i)(7)

4

where ⇤W
✓
update for q(k) becomes

is a distribution over the grid locations  equal to the convolution of U W with ✓. The

qt(kn = i) / ⇡i(wn) · ⇤W

✓ (i)

In the same way  we can marginalize the variable kn

P =Yt nXln=i

✓(i)·⇣Xkn=j

U W (j i)· ⇡j(wn)⌘· p(✓(i)|↵i) =Yt nXln=i

(8)

✓(i)· hi(wn)· p(✓(i)|↵i) (9)

to obtain the new update for qt(ln)

qt(ln = i) / hi(wn) · ✓t(i)

(10)
where hi is the feature distribution in a window centered at location i  which can be efﬁciently
computed in linear time using cumulative sums [3]. Eq.10 highlights further relationships between
CCGs and LDA: CCGs can be thought as an LDA model whose topics live on the space deﬁned
by the counting grids geometry. The new updates for the cell distribution q(k) and the window
distribution q(l)  require only a single convolution and  more importantly  they don’t directly depend
on each other. The model becomes more efﬁcient and has a faster convergence. This is very critical
especially when we are analyzing big text corpora.
The most similar generative model to CCG comes from the statistic community. Dunson et al. [12]
worked on sources positioned in a plane at real-valued locations  with the idea that sources within
a radius would be combined to produce topics in an LDA-like model. They used an expensive
sampling algorithm that aimed at moving the sources in the plane and determining the circular
window size. The grid placement of sources of CCG yields much more efﬁcient algorithms and
denser packing.

2.1 A Kernel based on CCG embedding
Hybrid generative discriminative classiﬁcation paradigms have been shown to be a practical and
effective way to get the best of both worlds in approaching classiﬁcation [13–15]. In the context of
topic models a simple but effective kernel is deﬁned as the product of the topic proportions of each
document. This kernel measures the similarity between topic usage of each sample and it proved to
be effective on several tasks [15–17]. Despite CCG’s ✓s  the locations proportions  can be thought
as LDA’s  we propose another kernel  which exploits exactly the same geometric reasoning of the
underlying generative model. We observe in fact that by construction  each point in the grid depends
by its neighborhood  deﬁned by W and this information is not captured using ✓  but using ⇤W
✓
which is deﬁned by spreading ✓ in the appropriate window (Eq.7).
More formally  given two samples t and u  we deﬁne a kernel based on CCG embedding as

K(t  u) =Xi

S(⇤W

✓t (i)  ⇤W

✓u(i)) where ⇤W

✓ (i) =Xj

U W (i  j) · ✓(j)

(11)

where S(· ·) is any similarity measure which deﬁnes a kernel.
In our experiments we considered the simple product  even if other measures  such as histogram
intersection can be used. The ﬁnal kernel turns to be (⇥ is the dot-product)
✓u
✓t ⇥ ⇤W

KLN (t  u) =Xi

✓u(i) = T r⇤W

✓t (i) · ⇤W
⇤W

(12)

3 Experiments
Although our model is fairly simple  it is still has multiple aspects that can be evaluated. As a
generative model  it can be evaluated in left-out likelihood tests. Its latent structure  as in other gen-
erative models  can be evaluated as input to classiﬁcation algorithms. Finally  as both its parameters
and the latent variables live in a compact space of dimensionality and size chosen by the user  our
learning algorithm can be evaluated as an embedding method that yields itself to data visualization
applications. As the latter two have been by far the more important sets of metrics when it comes to
real-world applications  our experiments focus on them.
In all the tests we considered squared grids of size E = [40 ⇥ 40  50 ⇥ 50  . . .   90 ⇥ 90] and win-
dows of size W = [2⇥ 2  4⇥ 4  . . .   8⇥ 8]. A variety of other methods are occasionally compared
to  with slightly different evaluation methods described in individual subsections  when appropriate.

5

a) “Same”-20 NewsGroup Results
90

 

y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C

l

85

80

75

70

65

101

102

Capacity (cid:78)(cid:3)/ No. Topics

b) Mastercook Recipes Results

Componential Counting Grid ((cid:84))
Componential Counting Grid ((cid:47)) 

LDA ((cid:84))
Counting Grid (q(l) )

80

70

60

50

40

30

20

 

y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C

l

101

102

Capacity (cid:78)(cid:3)/ No. Topics

c) Wikipedia Picture of the Day Results

Correspondence LDA
LDA + Discr. Classifier
Multimodal Random Field model
Componential Counting Grid

1

 

0.8

0.6

0.4

0.2

e
t
a
r
 
r
o
r
r
E

0

 
0

0.2

0.4

0.6
Percentage

0.8

1

Figure 3: a-b) Results for the text classiﬁcation tasks. The Mastercook recipes dataset is available
on www.alessandroperina.com. We represented the grid size E using gray levels (see the
text). c) Wikipedia Picture of the day result: average Error rate as a function of the percentage of
the ranked list considered for retrieval. Curves closer to the axes represents better performances.

Document Classiﬁcation We compared componential counting grids (CCGs) with counting grids
[3] (CGs)  latent Dirichlet allocation [1] (LDA) and the spherical admixture model [2] (SAM)  fol-
lowing the validation paradigm previously used in [2  3].
Each data sample consists of a bag of words and a label. The bags were used without labels to train
a model that capture covariation in word occurrences  with CGs mostly modeling thematic shifts 
LDA and SAM modeling topic mixing and CCGs both aspects. Then  the label prediction task is
performed in a 10-folds crossevaluation setting  using the linear kernel presented in Eq.12 which
for LDA reduces in using a linear kernel on the topic proportions. To show the effectiveness of the
spreading in the kernel deﬁnition  we also report results by employing CCG’s ✓s instead of ⇤W
✓ . For
CGs we used the original strategy [3]  Nearest Neighbor in the embedding space  while for SAM
we reported the results from the original paper. To the best of our knowledge the strategies just de-
scribed  based on [3] and [2]  are two of the most effective methods to classify text documents. SAM
is characterized by the same hierarchical nature of LDA  but it represents bags using directional dis-
tributions on a spherical manifold modeling features frequency  presence and absence. The model
captures ﬁne-grained semantic structure and performs better when small semantic distinctions are
important. CCGs map documents on a probabilistic simplex (e.g.  ✓) and for W > [1 ⇥ 1] can be
thought as an LDA model whose topics  hi  are much ﬁner as computed from overlapping windows
(see also Eq.10); a comparison is therefore natural.
As ﬁrst dataset we considered the CMU newsgroup dataset2. Following previous work [2  3  6]
we reduced the dataset into subsets with varying similarities among the news groups; news-
20-different  with posts from rec.sport.baseball  sci.space and alt.atheism 
news-20-similar  with posts from rec.talk.baseball  talk.politics.gun and
talk.politics.misc and news-20-same  with posts from comp.os.ms-windows 
comp.windows.x and comp.graphics. For the news-20-same subset (the hardest)  in Fig.3a
we show the accuracies of CCGs and LDA across the complexities. On the x-axis we have the dif-
ferent model size  in term of capacity   whereas in the y-axis we reported the accuracy. The same
 can be obtained with different choices of E and W therefore we represented the grid size E using
gray levels  the lighter the marker the bigger the grid. The capacity  is roughly equivalent to the
number of LDA topics as it represents the number of independent windows that can be ﬁt in the grid
and we compared the with LDA using this parallelism [18].
Componential counting grids outperform Latent Dirichlet Allocation across all the spectrum and the
accuracy regularly raises with  independently from the Grid size3. The priors helped to prevent
overtraining for big capacities . When using CCG’s ✓s to deﬁne the kernel  as expected the accu-

2http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html
3This happens for “reasonable” window sizes. For small windows (e.g  2 ⇥ 2)  the model doesn’t have

enough overlapping power and performs similarly a mixture model.

6

Table1:Documentclassiﬁcation.TheimprovementonSimilarandSamearestatisticallysig-niﬁcant.TheaccuraciesforSAMaretakenfrom[2]andtheyrepresentthebestresultsobtainedacrossthechoiceofnumberoftopics.BOWstandsforclassiﬁcationwithalinearSVMonthecountsmatrices.DatasetCCG2DCG3DCGLDABOWSAM⇤[3][3][1][2]Different96 49%96 51%96 34%91 8%91 43%94 1%Similar92 81%89 72%90 11%85 7%81 52%88 1%Same83 63%81 28%81 03%75 6%71 81%78 1%etofuwokstirfrycornstarchwokmixcornstarchlightcoatapplydonetawasizzlerstirfrychowshoottbsscalliongingersherrysoyachestnutpiecewhitecutnoodleveinsetasidegarlicsauceasidebitepeppercorndeveinpinkchickenstockremaincarrotceleryremovableaddfryertonguereducebringingboilboilsimmerbringingcoverreturnpotslottedlowermarinationpiecepanfryshallowafghanprovidegrillrefrigerationbrushmuslintwocurdleastclothpaneerputpourleavebreastwingsskinmarinadsauerkrautsavecoalbasteskewercheeseclothtiecharcoaloutsidewoodthreadleavelengthbonepiecedripskimhourovernightmarinadecavitycarcassesturnsecurepreferablesmokercubecubeawayterrinepacklengthroughlyfreshlychopduckcutturnipfatrenderexcessiveporkribharemeathourkabobturnpushcrockinchlongordertightlycrockpotinchshreddingjackrefryremovablepotkidneylargeremovablefatsausagegoosecassouletcasestufftogetherspicemixwidemixgroundlambsixburritobrowncookslightlysaltcookpumpkinmixturespicetavagrinderketchuprestgroundelectricmixownersaintchickenchickenchickenwingkeosnowlettucebedsthinthinbeefcoupleporkribsaucesweetoptionalmsgpoundasianbeefpeanutwokcondimentcuisinerootpastegingerthaiprikaromaticgrasslemongrasstabletomcurrychilicoconutgalangalshrimpdeveinchilisoupeshallotsoupetureenmortarpestlesambalbrothladlefulsoupenectarintendgentlebringingbroccolirouxboila)b)c) Zoom(cid:83)iFigure4:Asimpleinterfacebuiltuponthewordembedding⇡.racydropped(bluedotsinFig.3).Resultsforallthedatasetsandforavarietyofmethods arereportedinTab.1whereweemployed10%ofthetrainingdataasvalidationsettopickacomplexity(adifferentcomplexityhavebeenchosenforeachfold).Asvisible CCGoutperformsothermodels withalargermarginonthemorechallengingsameandsimilardatasets wherewewouldindeedexpectthatquiltingthetopicstocaptureﬁne-grainedsimilaritiesanddifferenceswouldbemosthelpful.Asseconddataset wedownloaded10KMastercookrecipes whicharefreelyavailableonthewebinplaintextformat.Thenweextractedthewordsofeachrecipefromitsingredientsandcookinginstructionsandweusedtheoriginoftherecipe todividethedatasetin15classes4.Theresultingdatasethasavocabularysizeof12538uniquewordsandatotalof⇠1Mtokens.Toclassifytherecipesweused10-foldcrossevaluationwith5repetitions picking80randomrecipesper-classforeachrepetition.ClassiﬁcationresultsareillustratedinFig.3b.Asfortheprevioustest CCGclassiﬁcationaccuraciesgrowsregularlywithindependentlyfromthegridsizeE.Com-ponentialmodels(e.g. LDAandCCGs)performedsigniﬁcantlybetterastocorrectlyclassifytheoriginofarecipe spicepalettes cookingstyleandproceduresmustbeidentiﬁed.ForexamplewhilemostAsiancuisinesusessimilaringredientsandcookingprocedurestheydeﬁnitelyhavedifferentspicepalettes.CountingGrids beingmixtures cannotcapturethatastheymaparecipeinasinglelocationwhichheavilydependsontheingredientsused.Amongcomponentialmodels CCGsworkthebest.MultimodalRetrievalWeconsideredtheWikipediaPictureoftheDaydataset[7] wherethetaskismulti-modalimageretrieval:givenatextquery weaimtoﬁndimagesthataremostrelevanttoit.Toaccomplishthis weﬁrstlylearnedamodelusingthevisualwordsofthetrainingdata{wt V} obtaining✓t ⇡Vi.Then keeping✓tﬁxedanditeratingtheM-step weembeddedthetextualwords{wt T}obtaining⇡Wi.Foreachtestsampleweinferredthevaluesof✓t Vand✓t Wrespectivelyfrom⇡Viand⇡WiandweusedEq.12tocomputetheretrievalscores.Asin[7]wesplitthedatain104Weconsideredthefollowingcuisines:Afghan Cajun Chinese English French German Greek Indian Indonesian Italian Japanese Mexican MiddleEastern SpanishandThai.7foldsandweusedavalidationsettopickacomplexity.ResultsareillustratedinFig.3c.Althoughweusedthissimpleprocedurewithoutdirectlytrainingamultimodalmodel CCGsoutperformLDA CorrLDA[19]andthemultimodaldocumentrandomﬁeldmodelpresentedin[7]andsetsanewstateoftheart.Theareaunderthecurve(AUC)forourmethodis21.92±0.6 whilefor[7]is23.14±1.49(Smallervaluesindicatebetterperformance).CountingGridsandLDAbothfailwithAUCsaround40.VisualizationImportantbeneﬁtsofCCGsarethat1)theylaydownsources⇡iona2-Ddimen-sionalgrid whicharereadyforvisualization and2)theyenforcethatcloselocationsgeneratesimilartopics whichleadstosmooththematicshiftsthatprovideconnectionsamongdistanttopicsonthegrid.Thisisveryusefulforsensemaking[20].Todemonstratethiswedevelopedasimpleinterface.AparticularisshowninFig.4b relativetotheextractofthecountinggridshowninFig.4a.Theinterfaceispannableandzoomableand atanymoment onthescreenonlythetopN=500wordsareshown.Todeﬁnetheimportanceofeachwordineachpositionweweighted⇡i(z)withtheinversedocumentfrequency.Fig.4bshowsthelowestlevelofzoom:onlywordsfromfewcellsarevisibleandthefontsizeresemblestheirweight.Ausercanzoomintoseethecontentofparticularcells/areas untilhereachesthehigh-estlevelofzoomwhenmostofthewordsgeneratedinapositionarevisible Fig.4c.FRYDEEP FRYSTIR FRYFigure5:Searchresultfortheword“fry”.Wealsoproposeasimplesearchstrategy:onceakeywordˆzisselected eachwordzineachpo-sitionj isweightedwithawordandpositiondependentweights.Theﬁrstisequalto1ifzco-occurwithˆzinsomedocument and0other-wise whilethelatteristhesumof⇡i(ˆz)inallthejsgiventhatthereexistsawindowWkthatcon-tainsbothiandj.Otherstrategiesareofcoursepossible.Asresult thisstrategyhighlightssomeareasandwords relatedtoˆzonthegridandineachareaswordsrelated(similartopic)toˆzap-pears.Interestingly ifasearchtermisusedindifferentcontexts fewislandsmayappearonthegrid.ForexampleFig.5showstheresultofthesearchforˆz=“fry”:Thegeneralfryingiswellseparatedfrom“deepfrying”and“stirfrying”whichappearsattheextremesofthesameis-land.Presentingsearchresultsasislandsona2-dimensionalgrid apparentlyimprovesthestandardstrategy alinearlistofhits inwhichrecipesrelativetothethreefryingstyleswouldhavebemixed whiletempurahavelittletodowithpanfriednoodles.4ConclusionInthispaperwepresentedthecomponentialcountinggridmodel–whichbridgesthetopicmodelandcountinggridworlds–togetherwithasimilaritymeasurebasedonit.Wedemonstratedthatthehiddenmappingvariablesassociatedwitheachdocumentcannaturallybeusedinclassiﬁcationtasks leadingtothestateoftheartperformanceonacoupleofdatasets.Bymeansofproposingasimpleinterface wehavealsoshownthegreatpotentialofCCGstovisu-alizeacorpora.AlthoughthesameholdsforCGs thisistheﬁrstpaperthatinvestigatethisaspect.MoreoverCCGssubsumeCGsasthecomponentsareusedonlywhenneeded.Foreveryrestart thegridsqualitativelyalwaysappearedverysimilar andsomeofthemoresalientsimilarityrelation-shipswerecapturedbyalltheruns.ThewordembeddingproducedbyCCGhasalsoadvantagesw.r.t.otherEuclideanembeddingmethodssuchasISOMAP[21] CODE[22]orLLE[23] whichareoftenusedfordatavisualization.InfactCCG’scomputationalcomplexityislinearinthedatasetsize asopposedtothequadraticcomplexityof[21 21–23]whichallarebasedonpairwisedis-tances.Then[21 23]onlyembeddocumentsorwordswhileCG/CCGsprovidebothembeddings.Finallyasopposedtopreviousco-occurrenceembeddingmethodsthatconsiderallpairsofwords ourrepresentationnaturallycapturesthesamewordappearinginmultiplelocationswhereithasadifferentmeaningbasedoncontext.Theword“memory”intheSciencemagazinecorpusisastrikingexample(memoryinneruoscience memoryinelectronicdevices immunologicmemory).8References
[1] Blei  D.  Ng  A.  Jordan  M.: Latent dirichlet allocation. Journal of machine Learning Research 3 (2003)

993–1022

[2] Reisinger  J.  Waters  A.  Silverthorn  B.  Mooney  R.J.: Spherical topic models. In: ICML ’10: Proceed-

ings of the 27th international conference on Machine learning. (2010)

[3] Jojic  N.  Perina  A.: Multidimensional counting grids: Inferring word order from disordered bags of

words. In: Proceedings of conference on Uncertainty in artiﬁcial intelligence (UAI). (2011) 547–556

[4] Hofmann  T.: Unsupervised learning by probabilistic latent semantic analysis. Machine Learning Journal

42 (2001) 177–196

[5] Blei  D.M.  Lafferty  J.D.: Correlated topic models. In: NIPS. (2005)
[6] Banerjee  A.  Basu  S.: Topic models over text streams: a study of batch and online unsupervised learning.

In: In Proc. 7th SIAM Intl. Conf. on Data Mining. (2007)

[7] Jia  Y.  Salzmann  M.  Darrell  T.: Learning cross-modality similarity for multinomial data. In: Proceed-
ings of the 2011 International Conference on Computer Vision. ICCV ’11  Washington  DC  USA  IEEE
Computer Society (2011) 2407–2414

[8] Neal  R.M.  Hinton  G.E.: A view of the em algorithm that justiﬁes incremental  sparse  and other variants.

Learning in graphical models (1999) 355–368

[9] Asuncion  A.  Welling  M.  Smyth  P.  Teh  Y.W.: On smoothing and inference for topic models. In: In

Proceedings of Uncertainty in Artiﬁcial Intelligence. (2009)

[10] Minka  T.P.: Estimating a Dirichlet distribution. Technical report  Microsoft Research (2012)
[11] Frey  B.J.  Jojic  N.: Transformation-invariant clustering using the em algorithm. IEEE Trans. Pattern

Anal. Mach. Intell. 25 (2003) 1–17

[12] Dunson  D.B.  Park  J.H.: Kernel stick-breaking processes. Biometrika 95 (2008) 307–323
[13] Perina  A.  Cristani  M.  Castellani  U.  Murino  V.  Jojic  N.: Free energy score spaces: Using generative

information in discriminative classiﬁers. IEEE Trans. Pattern Anal. Mach. Intell. 34 (2012) 1249–1262

[14] Raina  R.  Shen  Y.  Ng  A.Y.  Mccallum  A.: Classiﬁcation with hybrid generative/discriminative models.

In: In Advances in Neural Information Processing Systems 16  MIT Press (2003)

[15] Jebara  T.  Kondor  R.  Howard  A.: Probability product kernels. J. Mach. Learn. Res. 5 (2004) 819–844
[16] Bosch  A.  Zisserman  A.  Mu˜noz  X.: Scene classiﬁcation using a hybrid generative/discriminative

approach. IEEE Trans. Pattern Anal. Mach. Intell. 30 (2008) 712–727

[17] Bicego  M.  Lovato  P.  Perina  A.  Fasoli  M.  Delledonne  M.  Pezzotti  M.  Polverari  A.  Murino  V.:
Investigating topic models’ capabilities in expression microarray data classiﬁcation. IEEE/ACM Trans.
Comput. Biology Bioinform. 9 (2012) 1831–1836

[18] Perina  A.  Jojic  N.: Image analysis by counting on a grid. In: Proceedings of IEEE Computer Society

Conference on Computer Vision and Pattern Recognition (CVPR). (2011) 1985–1992

[19] Blei  D.M.  Jordan  M.I.: Modeling annotated data. In: Proceedings of the 26th annual international ACM

SIGIR conference on Research and development in informaion retrieval. SIGIR ’03 (2003) 127–134

[20] Thomas  J.  Cook  K.: Illuminating the Path: The Research and Development Agenda for Visual Analyt-

ics. IEEE Press (2005)

[21] Tenenbaum  J.B.  de Silva  V.  Langford  J.C.: A Global Geometric Framework for Nonlinear Dimen-

sionality Reduction. Science 290 (2000) 2319–2323

[22] Globerson  A.  Chechik  G.  Pereira  F.  Tishby  N.: Euclidean embedding of co-occurrence data. Journal

of Machine Learning Research 8 (2007) 2265–2295

[23] Roweis  S.T.  Saul  L.K.: Nonlinear dimensionality reduction by locally linear embedding. SCIENCE

290 (2000) 2323–2326

9

,Alessandro Perina
Nebojsa Jojic
Manuele Bicego
Andrzej Truski