2008,Estimating the Location and Orientation of Complex  Correlated Neural Activity using MEG,The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number  location  and orientation of these sources remains a challenging task  one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity  sensor noise  and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting  the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations  unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efficacy of this approach.,Estimating the Location and Orientation of Complex 

Correlated Neural Activity using MEG

D.P. Wipf  J.P. Owen  H.T. Attias  K. Sekihara  and S.S. Nagarajan

Biomagnetic Imaging Laboratory

University of California  San Francisco

Abstract

The synchronous brain activity measured via MEG (or EEG) can be interpreted
as arising from a collection (possibly large) of current dipoles or sources located
throughout the cortex. Estimating the number  location  and orientation of these
sources remains a challenging task  one that is signi(cid:2)cantly compounded by the
effects of source correlations and the presence of interference from spontaneous
brain activity  sensor noise  and other artifacts. This paper derives an empirical
Bayesian method for addressing each of these issues in a principled fashion. The
resulting algorithm guarantees descent of a cost function uniquely designed to
handle unknown orientations and arbitrary correlations. Robust interference sup-
pression is also easily incorporated. In a restricted setting  the proposed method
is shown to have theoretically zero bias estimating both the location and orien-
tation of multi-component dipoles even in the presence of correlations  unlike a
variety of existing Bayesian localization methods or common signal processing
techniques such as beamforming and sLORETA. Empirical results on both simu-
lated and real data sets verify the ef(cid:2)cacy of this approach.

1 Introduction
Magnetoencephalography (MEG) and related electroencephalography (EEG) use an array of sen-
sors to take electromagnetic (cid:2)eld (or voltage potential) measurements from on or near the scalp
surface with excellent temporal resolution. In both cases  the observed (cid:2)eld is generated by the
same synchronous  compact current sources located within the brain. Although useful for research
and clinical purposes  accurately determining the spatial distribution of these unknown sources is
an open problem. The relevant estimation problem can be posed as follows: The measured electro-
magnetic signal is B 2 Rdb(cid:2)dt  where db equals the number of sensors and dt is the number of time
points at which measurements are made. Each unknown source Si 2 Rdc(cid:2)dt is a dc-dimensional
neural current dipole   at dt timepoints  projecting from the i-th (discretized) voxel or candidate lo-
cation distributed throughout the cortex. These candidate locations can be obtained by segmenting a
structural MR scan of a human subject and tesselating the gray matter surface with a set of vertices.
B and each Si are related by the likelihood model

B =

LiSi + E;

(1)

dsXi=1

where ds is the number of voxels under consideration  Li 2 Rdb(cid:2)dc is the so-called lead-(cid:2)eld
matrix for the i-th voxel. The k-th column of Li represents the signal vector that would be observed
at the scalp given a unit current source/dipole at the i-th vertex with a (cid:2)xed orientation in the k-th
direction. It is common to assume dc = 2 (for MEG) or dc = 3 (for EEG)  which allows (cid:3)exible
source orientations to be estimated in 2D or 3D space. Multiple methods based on the physical
properties of the brain and Maxwell’s equations are available for the computation of each Li [7].
Finally  E is a noise-plus-interference term where we assume  for simplicity  that columns are drawn
independently from N (0; (cid:6)(cid:15)). However  temporal correlations can easily be incorporated if desired
using a simple transformation outlined in [3].

To obtain reasonable spatial resolution  the number of candidate source locations will necessarily
be much larger than the number of sensors (ds (cid:29) db). The salient inverse problem then becomes
the ill-posed estimation of regions with signi(cid:2)cant brain activity  which are re(cid:3)ected by voxels i
such that kSik > 0; we refer to these as active dipoles or sources. Because the inverse model is
severely underdetermined (the mapping from source activity con(cid:2)guration S   [S1; : : : ; Sds ]T to
sensor measurement B is many to one)  all efforts at source reconstruction are heavily dependent
on prior assumptions  which in a Bayesian framework are embedded in the distribution p(S). Such
a prior is often considered to be (cid:2)xed and known  as in the case of minimum current estimation
(MCE) [10]  minimum variance adaptive beamforming (MVAB) [9]  and sLORETA [5]. Alterna-
tively  a number of empirical Bayesian approaches have been proposed that attempt a form of model
selection by using the data  whether implicitly or explicitly  to guide the search for an appropriate
prior. Examples include variational Bayesian methods and hierarchical covariance component mod-
els [3  6  8  12  13]. While advantageous in many respects  all of these methods retain substantial
weaknesses estimating complex  correlated source con(cid:2)gurations with unknown orientation in the
presence of background interference (e.g.  spontaneous brain activity  sensor noise  etc.).
There are two types of correlations that can potentially disrupt the source localization process. First 
there are correlations within dipole components (meaning the individual rows of Si are correlated) 
which always exists to a high degree in real data with unknown orientation (i.e.  dc > 1). Secondly 
there are correlations between different dipoles that are simultaneously active (meaning rows of Si
are correlated with rows of Sj for some voxels i 6= j). These correlations are more application spe-
ci(cid:2)c and may or may not exist. The larger the number of active sources  the greater the chance that
both types or correlation can disrupt the estimation process. This issue can be problematic for two
reasons. First  failure to accurately account for unknown orientations or correlations can severely
disrupt the localization process  leading to a very misleading impression of which brain areas are
active. Secondly  the orientations and correlations themselves may have clinical signi(cid:2)cance.
In this paper  we present an alternative empirical Bayesian scheme that attempts to improve upon
existing methods in terms of source reconstruction accuracy and/or computational robustness and
ef(cid:2)ciency. Section 2 presents the basic generative model which underlies the proposed method and
describes the associated inference problem. Section 3 derives a robust algorithm for estimating the
sources using this model and proves that each iteration is guaranteed to reduce the associated cost
function. It also describes how interference suppression can be naturally incorporated. Section 4
then provides a theoretical analysis of the bias involved in estimating both the location and orien-
tation of active sources  demonstrating that the proposed method has substantial advantages over
existing approaches. Finally  Section 5 contains experimental results using our algorithm on both
simulated and real data  followed by a brief discussion in Section 6.

2 Modeling Assumptions

To begin we invoke the noise model from (1)  which fully de(cid:2)nes the assumed likelihood

p(BjS) / exp0@(cid:0)

1

2(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

B (cid:0)

dsXi=1

2

(cid:6)(cid:0)1

(cid:15)

1A ;

LiSi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(2)

where kXkW denotes the weighted matrix normptrace[X T W X]. The unknown noise covariance

(cid:6)(cid:15) will be estimated from the data using a variational Bayesian factor analysis (VBFA) model as
discussed in Section 3.2 below; for now we will consider that it is (cid:2)xed and known. Next we adopt
the following source prior for S:

p (Sj(cid:0)) / exp (cid:0)

1
2

trace" dsXi=1

i Si#! :

i (cid:0)(cid:0)1
ST

(3)

This is equivalent to applying independently  at each time point  a zero-mean Gaussian distribution
with covariance (cid:0)i to each source Si. We de(cid:2)ne (cid:0) to be the dsdc (cid:2) dsdc block-diagonal matrix
formed by ordering each (cid:0)i along the diagonal of an otherwise zero-valued matrix. This implies 

equivalently  that p (Sj(cid:0)) / exp(cid:0)(cid:0) 1

2trace(cid:2)ST (cid:0)(cid:0)1S(cid:3)(cid:1).

If (cid:0) were somehow known  then the conditional distribution p(SjB; (cid:0)) / p(BjS)p(Sj(cid:0)) is a fully
speci(cid:2)ed Gaussian distribution with mean and covariance given by

Ep(SjB;(cid:0)) [S] = (cid:0)LT(cid:0)(cid:6)(cid:15) + L(cid:0)LT(cid:1)(cid:0)1
Covp(sj jB;(cid:0)) [sj] = (cid:0) (cid:0) (cid:0)LT(cid:0)(cid:6)(cid:15) + L(cid:0)LT(cid:1)(cid:0)1

(4)
(5)
where sj denotes the j-th column of S and individual columns are uncorrelated. However  since (cid:0)
is actually not known  a suitable approximation ^(cid:0) (cid:25) (cid:0) must (cid:2)rst be found. One principled way to
accomplish this is to integrate out the sources S and then maximize

L(cid:0); 8j;

B

p(Bj(cid:0)) =Z p(BjS)p(Sj(cid:0))dS / exp(cid:18)(cid:0)

1
2

This is equivalent to minimizing the cost function

BT (cid:6)(cid:0)1

(6)

b B(cid:19) ; (cid:6)b   (cid:6)(cid:15) + L(cid:0)LT :
b (cid:3) + log j(cid:6)b; j ;

L((cid:0))   (cid:0)2 log p(Bj(cid:0))p((cid:0)) (cid:17) trace(cid:2)Cb(cid:6)(cid:0)1

(7)
where Cb   n(cid:0)1BBT is the empirical covariance  and is sometimes referred to as type-II maximum
likelihood  evidence maximization  or empirical Bayes [1].
The (cid:2)rst term of (7) is a measure of the dissimilarity between the empirical data covariance Cb and
the model data covariance (cid:6)b; in general  this factor encourages (cid:0) to be large. The second term pro-
vides a regularizing or sparsifying effect  penalizing a measure of the volume formed by the model
covariance (cid:6)b.1 Since the volume of any high dimensional space is more effectively reduced by
collapsing individual dimensions as close to zero as possible (as opposed to incrementally reducing
all dimensions isometrically)  this penalty term promotes a model covariance that is maximally de-
generate (or non-spherical)  which pushes elements of (cid:0) to exactly zero. This intuition is supported
theoretically by the results in Section 4.
Given some type-II ML estimate ^(cid:0)  we obtain the attendant empirical prior p(Sj^(cid:0)). To the extent
that this ‘learned’ prior is realistic  the resulting posterior p(SjB; ^(cid:0)) quanti(cid:2)es regions of signi(cid:2)cant
current density and point estimates for the unknown source dipoles Si can be obtained by evaluating
the posterior mean computed using (4). If a given ^(cid:0)i ! 0 as described above  then the associated
^Si computed using (4) also becomes zero. It is this pruning mechanism that naturally chooses the
number of active dipoles.

3 Algorithm Derivation
Given (cid:6)(cid:15) and (cid:0)  computing the posterior on S is trivial. Consequently  determining these unknown
quantities is the primary estimation task. We will (cid:2)rst derive an algorithm for computing (cid:0) assuming
(cid:6)(cid:15) is known. Later in Section 3.2  we will describe a powerful procedure for learning (cid:6)(cid:15).

3.1 Learning the Hyperparameters (cid:0)
The primary objective of this section is to minimize (7) with respect to (cid:0). Of course one option is
to treat the problem as a general nonlinear optimization task and perform gradient descent or some
other generic procedure. Related methods in the MEG literature rely  either directly or indirectly  on
a form of the EM algorithm [3  8]. However  these algorithms are exceedingly slow when ds is large
and they have not been extended to handle (cid:3)exible orientations. Consequently  here we derive an
alternative optimization procedures that expands upon ideas from [8  12]  handles arbitrary/unknown
dipole orientations  and converges quickly.
To begin  we note that L((cid:0)) only depends on the data B through the db(cid:2)db sample correlation matrix

large  without altering that actual cost function. It also implies that  for purposes of computing (cid:0) 
the number of columns of S is reduced to match rank(B). We now re-express the cost function
L((cid:0)) in an alternative form leading to convenient update rules and  by construction  a proof that

Cb. Therefore  to reduce the computational burden  we replace B with a matrix eB 2 Rdb(cid:2)rank(B)
such that eBeBT = Cb. This removes any per-iteration dependency on dt  which can potentially be
L(cid:0)(cid:0)(k+1)(cid:1) (cid:20) L(cid:0)(cid:0)(k)(cid:1) at each iteration.

1The determinant of a matrix is equal to the product of its eigenvalues  a well-known volumetric measure.

First  the data (cid:2)t term can be expressed as

(9)

trace(cid:2)Cb(cid:6)(cid:0)1

b (cid:3) = min
ds(cid:3)T is a matrix of auxiliary variables. Likewise  because the log-

determinant term of L((cid:0)) is concave in (cid:0)  it can be expressed as a minimum over upper-bounding
hyperplanes via

where X   (cid:2)X T

1 ; : : : ; X T

kXik2

(8)

(cid:0)(cid:0)1

(cid:15)

X 24(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)eB (cid:0)
Z " dsXi=1

2

+

(cid:6)(cid:0)1

dsXi=1

dsXi=1

LiXi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
i (cid:0)i(cid:1) (cid:0) h(cid:3)(Z)# ;
trace(cid:0)Z T

i 35 ;

log j(cid:6)bj = min

where Z   (cid:2)Z T

1 ; : : : ; Z T

L((cid:0); X; Z) =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)eB (cid:0)

below  we will never actually have to compute h(cid:3)(Z). Dropping the minimizations and combining
terms from (8) and (9) leads to the modi(cid:2)ed cost function

ds(cid:3)T and h(cid:3)(Z) is the concave conjugate of log j(cid:6)bj. For our purposes
dsXi=1eLiXi(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

i (cid:0)i(cid:1)i (cid:0) h(cid:3)(Z);

+ trace(cid:0)Z T

dsXi=1hkXik2

(10)

(cid:6)(cid:0)1

(cid:0)(cid:0)1

+

2

(cid:15)

i

where by construction L((cid:0)) = minX minZ L((cid:0); X; Z).
It is straightforward to show that if
f^(cid:0); ^X; ^Zg is a local (global) minimum to L((cid:0); X; Z)  then ^(cid:0) is a local (global) minimum to L((cid:0)).
Since direct optimization of L((cid:0)) may be dif(cid:2)cult  we can instead iteratively optimize L((cid:0); X; Z)
via coordinate descent over (cid:0)  X  and Z. In each case  when two are held (cid:2)xed  the third can be
globally minimized in closed form. This ensures that each cycle will reduce L((cid:0); X; Z)  but more
importantly  will reduce L((cid:0)) (or leave it unchanged if a (cid:2)xed-point or limit cycle is reached). The
associated update rules from this process are as follows.
The optimal X (with (cid:0) and Z (cid:2)xed) is just the standard weighted minimum-norm solution given by
(11)

Xnew

i ! (cid:0)iLT

i (cid:6)(cid:0)1

for each i. The minimizing Z equals the slope at the current (cid:0) of log j(cid:6)bj. As such  we have

b eB

Znew

i ! O(cid:0)i log j(cid:6)bj = LT

i (cid:6)(cid:0)1

b Li:

With Z and X (cid:2)xed  computing the minimizing (cid:0) is a bit more dif(cid:2)cult because of the constraint
(cid:0)i 2 H + for all i  where H + is the set of positive-semide(cid:2)nite  symmetric dc (cid:2) dc covariance
matrices. To obtain each (cid:0)i  we must solve

An unconstrained solution will satisfy

(cid:0)new
i ! arg min

(cid:0)i2H +hkXik2

(cid:0)(cid:0)1

i

+ trace(cid:0)Z T

i (cid:0)i(cid:1)i

(12)

(13)

(14)
which  after computing the necessary derivatives and re-arranging terms gives the equivalent condi-
tion

O(cid:0)iL((cid:0)i; Xi; Zi) = 0;

(15)
There are multiple (unconstrained) solutions to this equation; we will choose the unique one that
satis(cid:2)es the constraint (cid:0)i 2 H +. This can be found using

i = (cid:0)iZi(cid:0)i:

XiX T

XiX T

i = Z (cid:0)1=2

i

i XiX T

i Z 1=2

(cid:16)Z 1=2
(cid:16)Z 1=2
(cid:16)Z 1=2

= Z (cid:0)1=2

i

= (cid:20)Z (cid:0)1=2

i

i XiX T

i Z 1=2

i XiX T

i XiX T

i Z 1=2

Z (cid:0)1=2

i

i (cid:17) Z (cid:0)1=2
i (cid:17)1=2(cid:16)Z 1=2
i (cid:17)1=2
(cid:16)Z 1=2

i

Z (cid:0)1=2

i

i

i Z 1=2

i (cid:17)1=2
(cid:21) Zi(cid:20)Z (cid:0)1=2
(cid:16)Z 1=2
i (cid:17)1=2

i Z 1=2

Z (cid:0)1=2

i

;

(cid:0)new
i ! Z (cid:0)1=2

i

i XiX T

This indicates the solution (or update equation)

i XiX T

i Z 1=2

i (cid:17)1=2

(16)

Z (cid:0)1=2

i

(cid:21) :

(17)

which is satis(cid:2)es the constraint. And since we are minimizing a convex function of (cid:0)i (over the
constraint set)  we know that this is indeed a minimizing solution.
In summary then  to estimate (cid:0)  we need simply iterate (11)  (12)  and (17)  and with each pass we
are guaranteed to reduce (or leave unchanged) L((cid:0)). The per-iteration cost is linear in the number
of voxels ds so the computational cost is relatively modest (it is quadratic in db  and cubic in dc 
but these quantities are relatively small). The convergence rate is orders of magnitude faster than
EM-based algorithms such as those in [3  8] (see Figure 1 (right) ).

3.2 Learning the Interference (cid:6)(cid:15)
The learning procedure described in the previous section boils down to (cid:2)tting a structured maximum
likelihood covariance estimate (cid:6)b = (cid:6)(cid:15) + F (cid:0)F T to the data covariance Cb. The idea here is that
F (cid:0)F T will re(cid:3)ect the brain signals of interest while (cid:6)(cid:15) will capture all interfering factors  e.g. 
spontaneous brain activity  sensor noise  muscle artifacts  etc. Since (cid:6)(cid:15) is unknown  it must some-
how be estimated or otherwise accounted for. Given access to pre-stimulus data (i.e.  data assumed
to have no signal/sources of interest)  stimulus evoked factor analysis (SEFA) provides a powerful
means of decomposing a data covariance matrix Cb into signal and interference components. While
details can be found in [4]  SEFA computes the approximation
Cb (cid:25) (cid:3) + EET + AAT ;

(18)
where E represents a matrix of learned interference factors  (cid:3) is a diagonal noise matrix  and A is a
matrix of signal factors. There are two ways to utilize this decomposition (more details can be found
in [11]). First  we can simply set (cid:6)(cid:15) ! (cid:3) + EET and proceed as in Section 3.1. Alternatively 
we can set (cid:6)(cid:15) ! 0 and then substitute AAT for Cb  i.e.  run the same algorithm on a de-noised
signal covariance. For technical reasons beyond the scope of this paper  it appears that algorithm
performance may be superior when the latter paradigm is adopted.

4 Analysis of Theoretical Localization/Orientation Bias
Theoretical support for the proposed algorithm is possible in the context of estimation bias assuming
simpli(cid:2)ed source con(cid:2)gurations. For example  substantial import has been devoted to quantifying
localization bias when estimating a single dipolar source. Recently it has been shown  both empiri-
cally and theoretically [5  9]  that the MVAB and sLORETA algorithms have zero location bias under
this condition at high SNR. This has been extended to include certain empirical Bayesian methods
[8  12]. However  these results assume a single dipole with (cid:2)xed  known orientation (or alternatively 
that dc = 1)  and therefore do not formally handle source correlations or multi-component dipoles.
The methods from [6  13] also purport to address these issues  but no formal analyses are presented.
In contrast  despite being a complex  non-convex function  we now demonstrate that L((cid:0)) has very
attractive bias properties regarding both localization and orientation. We will assume that the full

ds(cid:3)T represents a suf(cid:2)ciently high sampling of the source space such that

any active dipole component aligns with some lead-(cid:2)eld columns. Unbiasedness can also be shown
in the continuous case  but the discrete scenario is more straightforward and of course more relevant
to any practical task.
Some preliminary de(cid:2)nitions are required to proceed. We de(cid:2)ne the empirical intra-dipole corre-
lation matrix at the i-th voxel as Cii   1
i Si; non-zero off-diagonal elements imply that corre-
ST
dt
lations are present. Except in highly contrived situations  this type of correlation will always exist.
The empirical inter-dipole correlation matrix between voxels i and j is Cij   1
i Sj; any non-
ST
dt
zero element implies the existence of a correlation. In practice  this form of correlation may or may
not be present. With regard to the lead-(cid:2)eld L  spark is de(cid:2)ned as the smallest number of linearly
dependent columns [2]. By de(cid:2)nition then  2 (cid:20) spark(L) (cid:20) db + 1. Finally  da denotes the number
of active sources  i.e.  the number of voxels whereby kSik > 0.

lead-(cid:2)eld L  (cid:2)LT

1 ; : : : ; LT

Theorem 1. In the limit as (cid:6)(cid:15) ! 0 (high SNR) and assuming dadc < spark(L) (cid:0) 1  the cost
function L((cid:0)) maintains the following two properties:

1. For arbitrary Cii and Cij  the unique global minimum (cid:0)(cid:3) produces a source estimate S(cid:3) =
Ep(SjB;(cid:0)(cid:3)) [S] computed using (4) that equals the generating source matrix S  i.e.  it is

unbiased in both location and orientation for all active dipoles and correctly zeros out the
inactive ones.

2. If Cij = 0 for all active dipoles (although Cii is still arbitrary)  then there are no local

minima  i.e.  the cost function is unimodal.

The proof has been deferred to [11]. In words  this theorem says that intra-dipole correlations do
not disrupt the estimation process by creating local minima  and that the global minimum is always
unbiased. In contrast  inter-dipole correlations can potentially create local minima  but they do not
affect the global minimum. Empirically  we will demonstrate that the algorithm derived in Section
3 is effective at avoiding these local minima (see Section 5). With added assumptions these results
can be extended somewhat to handle the inclusion of noise.
The cost functions from [8  12] bear the closest resemblance to L((cid:0)); however  neither possesses
the second attribute from Theorem 1. This is a very signi(cid:2)cant failing because  as mentioned previ-
ously  intra-dipole correlations are always present in each active dipole. Consequently  localization
and orientation bias can occur because of convergence to a local minimum. The iterative Bayesian
scheme from [13]  while very different in structure  also directly attempts to estimate (cid:3)exible ori-
entations and handle  to some extent  source correlations. While details are omitted for brevity  we
can prove that the full model upon which this algorithm is based fails to satisfy the (cid:2)rst property
of the theorem  so the corresponding global minimum can be biased.
In contrast  beamformers
and sLORETA are basically linear methods with no issue of global or local minima. However  the
popular sLORETA and MVAB solutions will in general display a bias for multi-component dipoles
(dc > 1) or when multiple dipoles (da > 1) are present  regardless of correlations.

5 Empirical Evaluation
In this section we test the performance of our algorithm on both simulated and real data sets. We
focus here on localization accuracy assuming strong source correlations and unknown orientations.
While orientation estimates themselves are not shown for space considerations  accurate localization
implicitly indicates that this confound has been adequately handled. More comprehensive experi-
ments  including comparisons with additional algorithms  are forthcoming [11].
Simulated Data: We (cid:2)rst conducted tests using simulated data with realistic source con(cid:2)gurations.
The brain volume was segmented into 5mm voxels and a two orientation (dc = 2) forward lead(cid:2)eld
was calculated using a spherical-shell model [7]. The data time course was partitioned into pre- and
post-stimulus periods. In the pre-stimulus period (263 samples) there is only noise and interfering
brain activity  while in the post-stimulus period (437 samples) there is the same (statistically) noise
and interference factors plus source activity of interest. We used two noise conditions - Gaussian-
noise and real-brain noise. In the former case  we seeded voxels with Gaussian noise in each orien-
tation and then projected the activity to the sensors using the lead(cid:2)eld  producing colored Gaussian
noise at the sensors. To this activity  we added additional Gaussian sensor noise. For the real-brain
noise case  we used resting-state data collected from a human subject that is presumed to have on-
going and spontaneous activity and sensor noise. In both the Gaussian and real-brain noise cases 
the pre-stimulus activity was on-going and continued into the post-stimulus period  where the simu-
lated source signals were added. Sources were seeded at locations in the brain as damped-sinusoids
and this voxel activity was projected to the sensors. We could adjust both the signal-to-noise-plus-
interefence ratio (SNIR) and the correlations between the different voxel time-courses to examine
the algorithm performance on correlated sources and unknown dipole orientations.
We ran 100 simulations of three randomly seeded sources at different SNIR levels (-5  0  5  10dB).
The sources in these simulations always had an inter-dipole correlation coef(cid:2)cient of 0.5; intra-
dipole correlations were present as well. We ran the simulation with both Gaussian-noise and real
brain noise using a MVAB and our proposed method. In order to evaluate performance  we used the
following test for a hit or miss. We drew spheres around each seeded source location and obtained the
maximum voxel value in each sphere. Then we calculated the maximum voxel activation outside the
three spheres. If the maximum inside each sphere was greater than the maximum outside all of the
spheres  it was counted as a hit (in this way  we are implicitly accounting somewhat for false alarms).
Each simulation could get a score or 0  1  2   or 3  with 3 being the best. Figure 1 (left) displays
comparative results averaged over 100 trials with standard errors. Our method quite signi(cid:2)cantly
outperforms the MVAB  which is designed to handle unknown orientations but has dif(cid:2)culty with

source correlations. Figure 1 (middle) shows a sample reconstruction on a much more complex
source con(cid:2)guration composed of 10 dipolar sources. Finally  Figure 1 (right) gives an example
of the relative convergence improvement afforded by our method relative to an EM implementation
analogous to [3  8]. We also wanted to test the performance on perfectly correlated sources with
unknown orientations and compare it to other state-of-the-art Bayesian methods. An example using
three such sources and 5 dB SNIR is given in Figure 2.

s
n
o

i
t

a
z

i
l

a
c
o

l
 
l

u

f
s
s
e
c
c
u
s

3

2.5

2

1.5

1

0.5

0

)

m
m

(
 
z

50

40

30

20

10

0

−10

−20

Gaussian Noise
Proposed Method

Real Brain Noise
Proposed Method

Gaussain Noise
MVAB

Real Brain Noise
MVAB

−6

−4

−2

0

2

4

6

8

10

12

−60

−40

−20

SNIR

0

x (mm)

20

40

l

e
u
a
v
 

n
o

i
t
c
n
u

f
 
t
s
o
c

−160

−180

−200

−220

−240

−260

−280
0

EM algorithm
proposed method

10

20

30

40

50

60

70

80

90

100

iteration number

Figure 1: Left: Aggregate localization results for MVAB and the proposed method recovering three
correlated sources with unknown orientations. Middle: Example reconstruction of 10 relatively
shallow sources (green circles) using proposed method (MVAB performs poorly on this task). Right:
Convergence rate of proposed method relative to a conventional EM implementation based on [3  8].

)

m
m

(
 
z

50

40

30

20

10

0

−10

−20

)

m
m

(
 
z

50

40

30

20

10

0

−10

−20

)

m
m

(
 
z

50

40

30

20

10

0

−10

−20

−60

−40

−20

0

x (mm)

20

40

−60

−40

−20

0

x (mm)

20

40

−60

−40

−20

0

x (mm)

20

40

Figure 2: Reconstructions of three perfectly correlated dipoles (green circles) with unknown ori-
entations using  Left: MVAB  Middle: variational Bayesian method from [13]  Right: proposed
method.

Real Data: Two stimulus-evoked data sets were collected from normal  healthy research subjects
on a 275-channel CTF System MEG device. The (cid:2)rst data set was a sensory evoked (cid:2)eld (SEF)
paradigm  where the subject’s right index (cid:2)nger was tapped for a total of 256 trials. A peak is typ-
ically seen 50ms after stimulation in the contralateral (in this case  the left) somatosensory cortical
area for the hand  i.e.  dorsal region of the postcentral gyrus. The proposed algorithm was able to
localize this activation to the correct area of somatosensory cortex as seen in Figure 3 (left) and the
estimated time course shows the typical 50ms peak (data not shown). The second data set analyzed
was an auditory evoked (cid:2)eld (AEF) paradigm. In this paradigm the subject is presented tones binau-
rally for a total of 120 trials. There are two typical peaks seen after the presentation of an auditory
stimulus  one at 50ms and one at 100ms  called the M50 and M100 respectively. The auditory pro-
cessing of tones is bilateral at early auditory areas and the activations are correlated. The algorithm
was able to localize activity in both primary auditory cortices and the time courses for these two
activations reveal the M50 and M100. Figure 3 (middle) and (right) displays these results. The
analysis of simple auditory paradigms is problematic because many source localization algorithms 
such as the MVAB  do not handle the bilateral correlated sources well. We also ran MVAB on the
AEF data and it localized activity to the center of the head between the two auditory cortices (data
not shown).
6 Discussion

This paper derives a novel empirical Bayesian algorithm for MEG source reconstruction that readily
handles multiple correlated sources with unknown orientations  a situation that commonly arises
even with simple imaging tasks. Based on a principled cost function and fast  convergent update

)
0
0
0
1
-
0
(
 
y
t
i
s
n
e
t
n

I
 

d
e
z
i
l
a
m
r
o
N

800

700

600

500

400

300

200

100

0

50

Time (ms)

100

150

Figure 3: Real-world example Left: Somatosensory reconstruction. Middle: Bilateral auditory re-
construction. Right: Recovered timecourse from left auditory cortex (right auditory cortex  not
shown  is similar).
rules  this procedure displays signi(cid:2)cant theoretical and empirical advantages over many existing
methods. We have restricted most of our exposition and analyses to MEG; however  preliminary
work with EEG is also promising. For example  on a real-world passive visual task where subjects
viewed (cid:3)ashing foreground/background textured images  our method correctly localizes activity to
the lateral occipital cortex while two state-of-the-art beamformers fail. This remains an active area
of research.
References
[1] J.O. Berger  Statistical Decision Theory and Bayesian Analysis  Springer-Verlag  New York 

2nd edition  1985.

[2] D.L. Donoho and M. Elad  (cid:147)Optimally sparse representation in general (nonorthogonal) dictio-
naries via ‘1 minimization (cid:148) Proc. National Academy of Sciences  vol. 100  no. 5  pp. 2197(cid:150)
2202  March 2003.

[3] K. Friston  L. Harrison  J. Daunizeau  S. Kiebel  C. Phillips  N. Trujillo-Barreto  R. Henson 
G. Flandin  and J. Mattout  (cid:147)Multiple sparse priors for the MEG/EEG inverse problem (cid:148) Neu-
roImage  2008 (in press).

[4] S.S. Nagarajan  H.T. Attias  K.E. Hild K.E.  K. Sekihara  (cid:147)A probabilistic algorithm for robust
interference suppression in bioelectromagnetic sensor data (cid:148) Stat Med. vol. 26  no. 21  pp.
3886(cid:150)910 Sept. 2007.

[5] R.D. Pascual-Marqui  (cid:147)Standardized low resolution brain electromagnetic tomography (sloreta):
Technical details (cid:148) Methods and Findings in Experimental and Clinical Pharmacology  vol. 24 
no. Suppl D  pp. 5(cid:150)12  2002.

[6] M. Sahani and S.S. Nagarajan  (cid:147)Reconstructing MEG sources with unknown correlations (cid:148)

Advances in Neural Information Processing Systems 16  2004.

[7] J. Sarvas  (cid:147)Basic methematical and electromagnetic concepts of the biomagnetic inverse prob-

lem (cid:148) Phys. Med. Biol.  vol. 32  pp. 11(cid:150)22  1987.

[8] M. Sato  T. Yoshioka  S. Kajihara  K. Toyama  N. Goda  K. Doya  and M. Kawato  (cid:147)Hierarchical

Bayesian estimation for MEG inverse problem (cid:148) NeuroImage  vol. 23  pp. 806(cid:150)826  2004.

[9] K. Sekihara  M. Sahani  and S.S. Nagarajan  (cid:147)Localization bias and spatial resolution of adaptive
and non-adaptive spatial (cid:2)lters for MEG source reconstruction (cid:148) NeuroImage  vol. 25  pp. 1056(cid:150)
1067  2005.

[10] K. Uutela  M. Hamalainen  and E. Somersalo  (cid:147)Visualization of magnetoencephalographic

data using minimum current estimates (cid:148) NeuroImage  vol. 10  pp. 173(cid:150)180  1999.

[11] D.P. Wipf  J.P. Owen  H.T. Attias  K. Sekihara  and S.S. Nagarajan (cid:147)Robust Bayesian Estima-
tion of the Location  Orientation  and Timecourse of Mutliple Correlated Neural Sources using
MEG (cid:148) submitted  2009.

[12] D.P. Wipf  R.R. Ram·(cid:17)rez  J.A. Palmer  S. Makeig  and B.D. Rao  (cid:147)Analysis of empirical
Bayesian methods for neuroelectromagnetic source localization (cid:148) Advances in Neural Informa-
tion Processing Systems 19  2007.

[13] J.M. Zumer  H.T. Attias  K. Sekihara  and S.S. Nagarajan  (cid:147)A probabilistic algorithm for
interference suppression and source reconstruction from MEG/EEG data (cid:148) Advances in Neural
Information Processing System 19  2007.

,Hyun Oh Song
Yong Jae Lee
Stefanie Jegelka
Trevor Darrell