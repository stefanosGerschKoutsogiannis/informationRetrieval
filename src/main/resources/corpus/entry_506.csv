2014,An Autoencoder Approach to Learning Bilingual Word Representations,Cross-language learning allows us to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages  while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences  within and between languages  we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification  where a classifier trained on a given language (e.g.  English) must learn to generalize to a different language (e.g.  German). In experiments on 3 language pairs  we show that our approach achieves state-of-the-art performance  outperforming a method exploiting word alignments and a strong machine translation baseline.,An Autoencoder Approach to Learning

Bilingual Word Representations

Sarath Chandar A P1 ∗  Stanislas Lauly2 ∗  Hugo Larochelle2  Mitesh M Khapra3 

Balaraman Ravindran1  Vikas Raykar3  Amrita Saha3

∗ Both authors contributed equally

1Indian Institute of Technology Madras  2Universit´e de Sherbrooke  3IBM Research India

apsarathchandar@gmail.com  {stanislas.lauly hugo.larochelle}@usherbrooke.ca 

{mikhapra viraykar amrsaha4}@in.ibm.com  ravi@cse.iitm.ac.in

Abstract

Cross-language learning allows one to use training data from one language to
build models for a different language. Many approaches to bilingual learning re-
quire that we have word-level alignment of sentences from parallel corpora. In this
work we explore the use of autoencoder-based methods for cross-language learn-
ing of vectorial word representations that are coherent between two languages 
while not relying on word-level alignments. We show that by simply learning to
reconstruct the bag-of-words representations of aligned sentences  within and be-
tween languages  we can in fact learn high-quality representations and do without
word alignments. We empirically investigate the success of our approach on the
problem of cross-language text classiﬁcation  where a classiﬁer trained on a given
language (e.g.  English) must learn to generalize to a different language (e.g.  Ger-
man). In experiments on 3 language pairs  we show that our approach achieves
state-of-the-art performance  outperforming a method exploiting word alignments
and a strong machine translation baseline.

1

Introduction

The accuracy of Natural Language Processing (NLP) tools for a given language depend heavily on
the availability of annotated resources in that language. For example  high quality POS taggers
[1]  parsers [2]  sentiment analyzers [3] are readily available for English. However  this is not the
case for many other languages such as Hindi  Marathi  Bodo  Farsi  and Urdu  for which annotated
data is scarce. This situation was acceptable in the past when only a few languages dominated the
digital content available online and elsewhere. However  the ever increasing number of languages
on the web today has made it important to accurately process natural language data in such resource-
deprived languages also. An obvious solution to this problem is to improve the annotated inventory
of these languages  but the cost  time and effort required act as a natural deterrent to this.
Another option is to exploit the unlabeled data available in a language. In this context  vectorial text
representations have proven useful for multiple NLP tasks [4  5]. It has been shown that meaning-
ful representations  capturing syntactic and semantic similarity  can be learned from unlabeled data.
While the majority of previous work on vectorial text representations has concentrated on the mono-
lingual case  there has also been considerable interest in learning word and document representations
that are aligned across languages [6  7  8  9  10  11  12]. Such aligned representations allow the use
of resources from a resource-fortunate language to develop NLP capabilities in a resource-deprived
language.
One approach to cross-lingual exploitation of resources is to project parameters learned from the
annotated data of one language to another language [13  14  15  16  17]. These approaches rely on a

1

bilingual resource such as a Machine Translation (MT) system. Recent attempts at learning common
bilingual representations [9  10  11] aim to eliminate the need of such an MT system. A common
property of these approaches is that a word-level alignment of translated sentences is leveraged to
derive a regularization term relating word embeddings across languages. Such methods not only
eliminate the need for an MT system but also outperform MT based projection approaches.
In this paper  we experiment with methods that learn bilingual word representations without word-to-
word alignments of bilingual corpora during training. Unlike previous approaches  we only require
aligned sentences and do not rely on word-level alignments (e.g.  extracted using GIZA++  as is
usual)  simplifying the learning procedure. To do so  we propose and investigate bilingual autoen-
coder models  that learn hidden encoder representations of paired bag-of-words sentences that are
not only informative of the original bag-of-words but also predictive of the other language. Word
representations can then easily be extracted from the encoder and used in the context of a super-
vised NLP task. Speciﬁcally  we demonstrate the quality of these representations for the task of
cross-language document classiﬁcation  where a labeled data set can be available in one language 
but not in another one. As we’ll see  our approach is able to reach state-of-the-art performance 
outperforming a method exploiting word alignments and a strong machine translation baseline.

2 Autoencoder for Bags-of-Words

Let x be the bag-of-words representation of a sentence. Speciﬁcally  each xi is a word index from
a ﬁxed vocabulary of V words. As this is a bag-of-words  the order of the words within x does not
correspond to the word order in the original sentence. We wish to learn a D-dimensional vectorial
representation of our words from a training set of sentence bags-of-words {x(t)}T
We propose to achieve this by using an autoencoder model that encodes an input bag-of-words x with
a sum of the representations (embeddings) of the words present in x  followed by a non-linearity.
Speciﬁcally  let matrix W be the D × V matrix whose columns are the vector representations for
each word. The encoder’s computation will involve summing over the columns of W for each
word in the bag-of-word. We will denote this encoder function φ(x). Then  using a decoder  the
autoencoder will be trained to optimize a loss function that measures how predictive of the original
bag-of-words is the encoder representation φ(x) .
There are different variations we can consider in the design of the encoder/decoder and the choice of
loss function. One must be careful however  as certain choices can be inappropriate for training on
word observations  which are intrinsically sparse and high-dimensional. In this paper  we explore
and compare two different approaches  described in the next two sub-sections.

t=1.

2.1 Binary bag-of-words reconstruction training with merged bags-of-words

In the ﬁrst approach  we start from the conventional autoencoder architecture  which minimizes a
cross-entropy loss that compares a binary vector observation with a decoder reconstruction. We thus
convert the bag-of-words x into a ﬁxed-size but sparse binary vector v(x)  which is such that v(x)xi
is 1 if word xi is present in x and otherwise 0.
From this representation  we obtain an encoder representation by multiplying v(x) with the word
representation matrix W

a(x) = c + Wv(x)  φ(x) = h(a(x))

(1)
where h(·) is an element-wise non-linearity such as the sigmoid or hyperbolic tangent  and c is a
D-dimensional bias vector. Encoding thus involves summing the word representations of the words
present at least once in the bag-of-words.
To produce a reconstruction  we parametrize the decoder using the following non-linear form:

(cid:98)v(x) = sigm(Vφ(x) + b)

(2)
where V = WT   b is the bias vector of the reconstruction layer and sigm(a) = 1/(1 + exp(−a)) is
the sigmoid non-linearity.

2

Then  the reconstruction is compared to the original binary bag-of-words as follows:

(cid:96)(v(x)) = − V(cid:88)

v(x)i log((cid:98)v(x)i) + (1 − v(x)i) log(1 −(cid:98)v(x)i) .

(3)

i=1

Training proceeds by optimizing the sum of reconstruction cross-entropies across the training set 
e.g.  using stochastic or mini-batch gradient descent.
Note that  since the binary bags-of-words are very high-dimensional (the dimensionality corresponds
to the size of the vocabulary  which is typically large)  the above training procedure which aims at
reconstructing the complete binary bag-of-word  will be slow. Since we will later be training on
millions of sentences  training on each individual sentence bag-of-words will be expensive.
Thus  we propose a simple trick  which exploits the bag-of-words structure of the input. Assuming
we are performing mini-batch training (where a mini-batch contains a list of the bags-of-words of
adjacent sentences)  we simply propose to merge the bags-of-words of the mini-batch into a single
bag-of-words and perform an update based on that merged bag-of-words. The resulting effect is that
each update is as efﬁcient as in stochastic gradient descent  but the number of updates per training
epoch is divided by the mini-batch size . As we’ll see in the experimental section  this trick produces
good word representations  while sufﬁciently reducing training time. We note that  additionally  we
could have used the stochastic approach proposed by Dauphin et al. [18] for reconstructing binary
bag-of-words representations of documents  to further improve the efﬁciency of training. They use
importance sampling to avoid reconstructing the whole V -dimensional input vector.

2.2 Tree-based decoder training

The previous autoencoder architecture worked with a binary vectorial representation of the input
bag-of-words. In the second autoencoder architecture we investigate  we consider an architecture
that instead works with the bag (unordered list) representation more directly.
First  the encoder representation will now involve a sum of the representation of all words  reﬂecting
the relative frequency of each word:

a(x) = c +

W· xi  φ(x) = h (a(x)) .

(4)

distribution p((cid:98)x|φ(x)) over any word(cid:98)x observed at the reconstruction output layer. Then  we can

Moreover  decoder training will assume that  from the decoder’s output  we can obtain a probability
treat the input bag-of-words as a |x|-trials multinomial sample from that distribution and use as the
reconstruction loss its negative log-likelihood:

− log p((cid:98)x = xi|φ(x)) .

(5)

(cid:96)(x) =

i=1

cally  we’d like to avoid a procedure scaling linearly with the vocabulary size V   since V will be very

We now must ensure that the decoder can compute p((cid:98)x = xi|φ(x)) efﬁciently from φ(x). Speciﬁ-
large in practice. This precludes any procedure that would compute the numerator of p((cid:98)x = w|φ(x))
Speciﬁcally  we use a probabilistic tree decomposition of p((cid:98)x = xi|φ(x)). Let’s assume each word

for each possible word w separately and normalize it so it sums to one.
We instead opt for an approach borrowed from the work on neural network language models [19  20].

has been placed at the leaf of a binary tree. We can then treat the sampling of a word as a stochastic
path from the root of the tree to one of the leaves.
We denote as l(x) the sequence of internal nodes in the path from the root to a given word x  with
l(x)1 always corresponding to the root. We will denote as π(x) the vector of associated left/right
branching choices on that path  where π(x)k = 0 means the path branches left at internal node l(x)k

and otherwise branches right if π(x)k = 1. Then  the probability p((cid:98)x = x|φ(x)) of reconstructing

a certain word x observed in the bag-of-words is computed as

|x|(cid:88)

i=1

V(cid:88)

p((cid:98)x|φ(x)) =

p(π((cid:98)x)k|φ(x))

|π(ˆx)|(cid:89)

k=1

3

(6)

where p(π((cid:98)x)k|φ(x)) is output by the decoder. By using a full binary tree of words  the number of
different decoder outputs required to compute p((cid:98)x|φ(x)) will be logarithmic in the vocabulary size

V . Since there are |x| words in the bag-of-words  at most O(|x| log V ) outputs are required from
the decoder. This is of course a worst case scenario  since words will share internal nodes between
their paths  for which the decoder output can be computed just once. As for organizing words into a
tree  as in Larochelle and Lauly [21] we used a random assignment of words to the leaves of the full
binary tree  which we have found to work well in practice.
Finally  we need to choose a parametrized form for the decoder. We choose the following form:

p(π((cid:98)x)k = 1|φ(x)) = sigm(bl(ˆxi)k + Vl(ˆxi)k ·φ(x))

(7)
where b is a (V -1)-dimensional bias vector and V is a (V −1)×D matrix. Each left/right branching
probability is thus modeled with a logistic regression model applied on the encoder representation
of the input bag-of-words φ(x).

3 Bilingual autoencoders
Let’s now assume that for each sentence bag-of-words x in some source language X   we have an
associated bag-of-words y for this sentence translated in some target language Y by a human expert.
Assuming we have a training set of such (x  y) pairs  we’d like to use it to learn representations in
both languages that are aligned  such that pairs of translated words have similar representations.
To achieve this  we propose to augment the regular autoencoder proposed in Section 2 so that  from
the sentence representation in a given language  a reconstruction can be attempted of the original
sentence in the other language. Speciﬁcally  we now deﬁne language speciﬁc word representation
matrices Wx and Wy  corresponding to the languages of the words in x and y respectively. Let
V X and V Y also be the number of words in the vocabulary of both languages  which can be dif-
ferent. The word representations however are of the same size D in both languages. For the binary
reconstruction autoencoder  the bag-of-words representations extracted by the encoder become

φ(x) = h(cid:0)c + WX v(x)(cid:1)   φ(y) = h(cid:0)c + WY v(y)(cid:1)

and are similarly extended for the tree-based autoencoder. Notice that we share the bias c before the
non-linearity across encoders  to encourage the encoders in both languages to produce representa-
tions on the same scale.
From the sentence in either languages  we want to be able to perform a reconstruction of the original
sentence in both the languages. In particular  given a representation in any language  we’d like a
decoder that can perform a reconstruction in language X and another decoder that can reconstruct in
language Y. Again  we use decoders of the form proposed in either Section 2.1 or 2.2 (see Figure 1) 
but let the decoders of each language have their own parameters (bX   VX ) and (bY   VY ).
This encoder/decoder decomposition structure allows us to learn a mapping within each language
and across the languages. Speciﬁcally  for a given pair (x  y)  we can train the model to (1) construct
y from x (loss (cid:96)(x  y))  (2) construct x from y (loss (cid:96)(y  x))  (3) reconstruct x from itself (loss
(cid:96)(x)) and (4) reconstruct y from itself (loss (cid:96)(y)). We follow this approach in our experiments and
optimize the sum of the corresponding 4 losses during training.

3.1

Joint reconstruction and cross-lingual correlation

We also considered incorporating two additional terms to the loss function  in an attempt to favour
even more meaningful bilingual representations:

(cid:96)(x  y) + (cid:96)(y  x) + (cid:96)(x) + (cid:96)(y) + β(cid:96)([x  y]  [x  y]) − λ · cor(a(x)  a(y))

(8)
The term (cid:96)([x  y]  [x  y]) is simply a joint reconstruction term  where both languages are simul-
tanouesly presented as input and reconstructed. The second term cor(a(x)  a(y)) encourages corre-
lation between the representation of each language. It is the sum of the scalar correlations between
each pair a(x)k  a(y)k  across all dimensions k of the vectors a(x)  a(y)1. To obtain a stochastic
estimate of the correlation  during training  small mini-batches are used.

1While we could have applied the correlation term on φ(x)  φ(y) directly  applying it to the pre-activation

function vectors was found to be more numerically stable.

4

Figure 1: Left: Bilingual autoencoder based on the binary reconstruction error. Right: Tree-based
bilingual autoencoder. In this example  they both reconstruct the bag-of-words for the English sen-
tence “the dog barked” from its French translation “le chien a japp´e”.
3.2 Document representations

Once we learn the language speciﬁc word representation matrices Wx and Wy as described above 
we can use them to construct document representations  by using their columns as word vector
representations. Given a document d written in language Z ∈ {X  Y} and containing m words 
z1  z2  . . .   zm  we represent it as the tf-idf weighted sum of its words’ representations ψ(d) =
. zi. We use the document representations thus obtained to train our document

(cid:80)m
i=1 tf-idf(zi) · WZ

classiﬁers  in the cross-lingual document classiﬁcation task described in Section 5.

4 Related Work

Recent work that has considered the problem of learning bilingual representations of words usually
has relied on word-level alignments. Klementiev et al. [9] propose to train simultaneously two neural
network languages models  along with a regularization term that encourages pairs of frequently
aligned words to have similar word embeddings. Thus  the use of this regularization term requires
to ﬁrst obtain word-level alignments from parallel corpora. Zou et al. [10] use a similar approach 
with a different form for the regularizer and neural network language models as in [5]. In our work 
we speciﬁcally investigate whether a method that does not rely on word-level alignments can learn
comparably useful multilingual embeddings in the context of document classiﬁcation.
Looking more generally at neural networks that learn multilingual representations of words or
phrases  we mention the work of Gao et al. [22] which showed that a useful linear mapping between
separately trained monolingual skip-gram language models could be learned. They too however
rely on the speciﬁcation of pairs of words in the two languages to align. Mikolov et al. [11] also pro-
pose a method for training a neural network to learn useful representations of phrases  in the context
of a phrase-based translation model. In this case  phrase-level alignments (usually extracted from
word-level alignments) are required. Recently  Hermann and Blunsom [23]  [24] proposed neural
network architectures and a margin-based training objective that  as in this work  does not rely on
word alignments. We will brieﬂy discuss this work in the experiments section.

5 Experiments

The techniques proposed in this paper enable us to learn bilingual embeddings which capture cross-
language similarity between words. We propose to evaluate the quality of these embeddings by using
them for the task of cross-language document classiﬁcation. We followed closely the setup used by
Klementiev et al. [9] and compare with their method  for which word representations are publicly
available2. The set up is as follows. A labeled data set of documents in some language X is available
to train a classiﬁer  however we are interested in classifying documents in a different language Y
at test time. To achieve this  we leverage some bilingual corpora  which is not labeled with any

2http://people.mmci.uni-saarland.de/˜aklement/data/distrib/

5

document-level categories. This bilingual corpora is used to learn document representations that are
coherent between languages X and Y. The hope is thus that we can successfully apply the classiﬁer
trained on document representations for language X directly to the document representations for
language Y. Following this setup  we performed experiments on 3 data sets of language pairs:
English/German (EN/DE)  English/French (EN/FR) and English/Spanish (EN/ES).

5.1 Data

For learning the bilingual embeddings  we used sections of the Europarl corpus [25] which contains
roughly 2 million parallel sentences. We considered 3 language pairs. We used the same pre-
processing as used by Klementiev et al. [9]. We tokenized the sentences using NLTK [26]  removed
punctuations and lowercased all words. We did not remove stopwords.
As for the labeled document classiﬁcation data sets  they were extracted from sections of the Reuters
RCV1/RCV2 corpora  again for the 3 pairs considered in our experiments. Following Klementiev
et al. [9]  we consider only documents which were assigned exactly one of the 4 top level categories
in the topic hierarchy (CCAT  ECAT  GCAT and MCAT). These documents are also pre-processed
using a similar procedure as that used for the Europarl corpus. We used the same vocabularies as
those used by Klementiev et al. [9] (varying in size between 35  000 and 50  000).
For each pair of languages  our overall procedure for cross-language classiﬁcation can be summa-
rized as follows:
Train representation: Train bilingual word representations Wx and Wy on sentence pairs ex-
tracted from Europarl for languages X and Y. Optionally  we also use the monolingual documents
from RCV1/RCV2 to reinforce the monolingual embeddings (this choice is cross-validated). These
non-parallel documents can be used through the losses (cid:96)(x) and (cid:96)(y) (i.e. by reconstructing x from x
or y from y). Note that Klementiev et al. [9] also used this data when training word representations.
Train classiﬁer: Train document classiﬁer on the Reuters training set for language X   where docu-
ments are represented using the word representations Wx (see Section 3.2). As in Klementiev et al.
[9] we used an averaged perceptron trained for 10 epochs  for all the experiments.
Test-time classiﬁcation: Use the classiﬁer trained in the previous step on the Reuters test set for
language Y  using the word representations Wy to represent the documents.
We trained the following autoencoders3: BAE-cr which uses reconstruction error based decoder
training (see Section 2.1) and BAE-tr which uses tree-based decoder training (see Section 2.2).
Models were trained for up to 20 epochs using the same data as described earlier. BAE-cr used
mini-batch (of size 20) stochastic gradient descent  while BAE-tr used regular stochastic gradient.
All results are for word embeddings of size D = 40  as in Klementiev et al. [9]. Further  to speed
up the training for BAE-cr we merged each 5 adjacent sentence pairs into a single training instance 
as described in Section 2.1. For all language pairs  the joint reconstruction β was ﬁxed to 1 and
the cross-lingual correlation factor λ to 4 for BAE-cr. For BAE-tr  none of these additional terms
were found to be particularly beneﬁcial  so we set their weights to 0 for all tasks. The other hyper-
parameters were tuned to each task using a training/validation set split of 80% and 20% and using
the performance on the validation set of an averaged perceptron trained on the smaller training set
portion (notice that this corresponds to a monolingual classiﬁcation experiment  since the general
assumption is that no labeled data is available in the test set language).

5.2 Comparison of the performance of different models

We now present the cross language classiﬁcation results obtained by using the embeddings produced
by our two autoencoders. We also compare our models with the following approaches:
Klementiev et al.: This model uses word embeddings learned by a multitask neural network lan-
guage model with a regularization term that encourages pairs of frequently aligned words to have
similar word embeddings. From these embeddings  document representations are computed as de-
scribed in Section 3.2.

3Our word representations and code are available at http://www.sarathchandar.in/crl.html

6

Table 1: Cross-lingual classiﬁcation accuracy for 3 language pairs  with 1000 labeled examples.
EN → DE DE → EN EN → FR FR → EN EN → ES ES → EN
81.8
91.8
77.6
68.1
46.8

BAE-tr
BAE-cr
Klementiev et al.
MT
Majority Class

59.4
49.0
31.3
52.0
15.3

60.4
64.4
63.0
58.4
22.2

60.1
74.2
71.1
67.4
46.8

70.4
84.6
74.5
76.3
22.5

61.8
74.2
61.9
71.1
25.0

MT: Here  test documents are translated to the language of the training documents using a standard
phrase-based MT system  MOSES4 which was trained using default parameters and a 5-gram lan-
guage model on the Europarl corpus (same as the one used for inducing our bilingual embeddings).
Majority Class: Test documents are simply assigned the most frequent class in the training set.
For the EN/DE language pairs  we directly report the results from Klementiev et al. [9]. For the other
pairs (not reported in Klementiev et al. [9])  we used the embeddings available online and performed
the classiﬁcation experiment ourselves. Similarly  we generated the MT baseline ourselves.
Table 1 summarizes the results. They were obtained using 1000 RCV training examples. We report
results in both directions  i.e. language X to Y and vice versa. The best performing method is always
either BAE-cr or BAE-tr  with BAE-cr having the best performance overall. In particular  BAE-cr
often outperforms the approach of Klementiev et al. [9] by a large margin.
We also mention the recent work of Hermann and Blunsom [23]  who proposed two neural network
architectures for learning word and document representations using sentence-aligned data only. In-
stead of an autoencoder paradigm  they propose a margin-based objective that aims to make the
representation of aligned sentences closer than non-aligned sentences. While their trained embed-
dings are not publicly available  they report results for the EN/DE classiﬁcation experiments  with
representations of the same size as here (D = 40) and trained on 500K EN/DE sentence pairs. Their
best model in that setting reaches accuracies of 83.7% and 71.4% respectively for the EN → DE and
DE → EN tasks. One clear advantage of our model is that unlike their model  it can use additional
monolingual data. Indeed  when we train BAE-cr with 500k EN/DE sentence pairs  plus monolin-
gual RCV documents (which come at no additional cost)  we get accuracies of 87.9% (EN → DE)
and 76.7% (DE → EN)  still improving on their best model.
If we do not use the monolingual
data  BAE-cr’s performance is worse but still competitive at 86.1% for EN → DE and 68.8% for
DE → EN. Finally  without constraining D to 40 (they use 128) and by using additional French data 
the best results of Hermann and Blunsom [23] are 88.1% (EN → DE) and 79.1% (DE → EN)  the
later being  to our knowledge  the current state-of-the-art.
We also evaluate the effect of varying the amount of supervised training data for training the classi-
ﬁer. For brevity  we report only the results for the EN/DE pair  which are summarized in Figure 2.
We observe that BAE-cr clearly outperforms the other models at almost all data sizes. More impor-
tantly  it performs remarkably well at very low data sizes (100)  suggesting it learns very meaningful
embeddings  though the method can still beneﬁt from more labeled data (as in the DE → EN case).
Table 2 also illustrates the properties captured within and across languages  for the EN/DE pair5.
For a few English words  the words with closest word representations (in Euclidean distance) are
shown  for both English and German. We observe that words that form a translation pair are close 
but also that close words within a language are syntactically/semantically similar as well.
The excellent performance of BAE-cr suggests that merging several sentences into single bags-of-
words can still yield good word embeddings.
In other words  not only we do not need to rely
on word-level alignments  but exact sentence-level alignment is also not essential to reach good
performances. We experimented with the merging of 5  25 and 50 adjacent sentences (see the
supplementary material). Generally speaking  these experiments also conﬁrm that even coarser
merges can sometimes not be detrimental. However  for certain language pairs  there can be an
important decrease in performance. On the other hand  when comparing the performance of BAE-tr
with the use of 5-sentences merges  no substantial impact is observed.

4http://www.statmt.org/moses/
5See also the supplementary material for a t-SNE visualization of the word representations.

7

Table 2: Example English words along with the closest words both in English (EN) and German
(DE)  using the Euclidean distance between the embeddings learned by BAE-cr.

Word

oil

Lang Nearest neighbors
EN
DE
EN
DE
EN
DE

january  march  october
januar  m¨arz  oktober
president  i  mr  presidents microsoft
pr¨asident  pr¨asidentin
said  told  say  believe
gesagt  sagte  sehr  heute

market

Word

january

president

said

Lang Nearest neighbors
EN
DE
EN
DE
EN
DE

oil  supply  supplies  gas
¨ol  boden  beﬁndet  ger¨at
microsoft  cds  insider
microsoft  cds  warner
market  markets  single
markt  marktes  m¨arkte

Figure 2: Cross-lingual classiﬁcation accuracy results  from EN → DE (left)  and DE → EN (right).

6 Conclusion and Future Work

We presented evidence that meaningful bilingual word representations could be learned without
relying on word-level alignments or using fairly coarse sentence-level alignments. In particular  we
showed that even though our model does not use word level alignments  it is able to reach state-of-
the-art performance  even compared to a method that exploits word-level alignments. In addition  it
also outperforms a strong machine translation baseline. For future work  we would like to investigate
extensions of our bag-of-words bilingual autoencoder to bags-of-n-grams  where the model would
also have to learn representations for short phrases. Such a model should be particularly useful in the
context of a machine translation system. We would also like to explore the possibility of converting
our bilingual model to a multilingual model which can learn common representations for multiple
languages given different amounts of parallel data between these languages.

Acknowledgement

We would like to thank Alexander Klementiev and Ivan Titov for providing the code for the classiﬁer
and data indices. This work was supported in part by Google.

References
[1] Kristina Toutanova  Dan Klein  Christopher D. Manning  and Yoram Singer. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics  NAACL ’03  pages 173–180  2003.

[2] Richard Socher  John Bauer  Christopher D. Manning  and Ng Andrew Y. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers)  pages 455–465  Soﬁa  Bulgaria  August 2013.

[3] Bing Liu. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.

Morgan & Claypool Publishers  2012.

8

[4] Joseph Turian  Lev Ratinov  and Yoshua Bengio. Word representations: A simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2010)  pages 384–394  2010.

[5] Ronan Collobert  Jason Weston  L´eon Bottou  Michael Karlen  Koray Kavukcuoglu  and Pavel Kuksa.
Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research  12  2011.
[6] Susan T Dumais  Todd A Letsche  Michael L Littman  and Thomas K Landauer. Automatic cross-
language retrieval using latent semantic indexing. AAAI spring symposium on cross-language text and
speech retrieval  15:21  1997.

[7] John C. Platt  Kristina Toutanova  and Wen-tau Yih. Translingual document representations from discrim-
inative projections. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing  EMNLP ’10  pages 251–261  Stroudsburg  PA  USA  2010.

[8] Wen-tau Yih  Kristina Toutanova  John C. Platt  and Christopher Meek. Learning discriminative projec-
tions for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural
Language Learning  CoNLL ’11  pages 247–256  Stroudsburg  PA  USA  2011.

[9] Alexandre Klementiev  Ivan Titov  and Binod Bhattarai. Inducing Crosslingual Distributed Representa-

tions of Words. In Proceedings of the International Conference on Computational Linguistics  2012.

[10] Will Y. Zou  Richard Socher  Daniel Cer  and Christopher D. Manning. Bilingual Word Embeddings for

Phrase-Based Machine Translation. In Empirical Methods in Natural Language Processing  2013.

[11] Tomas Mikolov  Quoc Le  and Ilya Sutskever. Exploiting Similarities among Languages for Machine

Translation. Technical report  arXiv  2013.

[12] Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual correla-
tion. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics  pages 462–471  Gothenburg  Sweden  April 2014.

[13] David Yarowsky and Grace Ngai. Inducing multilingual pos taggers and np bracketers via robust projec-
tion across aligned corpora. In Proceedings of the second meeting of the North American Chapter of the
Association for Computational Linguistics on Language technologies  pages 1–8  Pennsylvania  2001.

[14] Dipanjan Das and Slav Petrov. Unsupervised part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies  pages 600–609  Portland  Oregon  USA  June 2011.

[15] Rada Mihalcea  Carmen Banea  and Janyce Wiebe. Learning multilingual subjective language via cross-
In Proceedings of the 45th Annual Meeting of the Association of Computational

lingual projections.
Linguistics  pages 976–983  Prague  Czech Republic  June 2007.

[16] Xiaojun Wan. Co-training for cross-lingual sentiment classiﬁcation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP  pages 235–243  Suntec  Singapore  August 2009.

[17] Sebastian Pad´o and Mirella Lapata. Cross-lingual annotation projection for semantic roles. Journal of

Artiﬁcial Intelligence Research (JAIR)  36:307–340  2009.

[18] Yann Dauphin  Xavier Glorot  and Yoshua Bengio. Large-Scale Learning of Embeddings with Recon-
struction Sampling. In Proceedings of the 28th International Conference on Machine Learning (ICML
2011)  pages 945–952. Omnipress  2011.

[19] Frederic Morin and Yoshua Bengio. Hierarchical Probabilistic Neural Network Language Model.

In
Proceedings of the 10th International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS 2005) 
pages 246–252. Society for Artiﬁcial Intelligence and Statistics  2005.

[20] Andriy Mnih and Geoffrey E Hinton. A Scalable Hierarchical Distributed Language Model. In Advances

in Neural Information Processing Systems 21 (NIPS 2008)  pages 1081–1088  2009.

[21] Hugo Larochelle and Stanislas Lauly. A Neural Autoregressive Topic Model.

Information Processing Systems 25 (NIPS 25)  2012.

In Advances in Neural

[22] Jianfeng Gao  Xiaodong He  Wen-tau Yih  and Li Deng. Learning continuous phrase representations for
translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers)  pages 699–709  Baltimore  Maryland  June 2014.

[23] Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributed semantics.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics  ACL 2014 
June 22-27  2014  Baltimore  MD  USA  Volume 1: Long Papers  pages 58–68  2014.

[24] Karl Moritz Hermann and Phil Blunsom. Multilingual Distributed Representations without Word Align-

ment. In Proceedings of International Conference on Learning Representations (ICLR)  2014.

[25] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT Summit  2005.
[26] Edward Loper Bird Steven and Ewan Klein. Natural Language Processing with Python. OReilly Media

Inc.  2009.

9

,Agnieszka Grabska-Barwinska
Jeff Beck
Alexandre Pouget
Peter Latham
Sarath Chandar A P
Stanislas Lauly
Hugo Larochelle
Mitesh Khapra
Balaraman Ravindran
Vikas Raykar
Amrita Saha