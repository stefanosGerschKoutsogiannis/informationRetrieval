2019,Localized Structured Prediction,Key to structured prediction is exploiting the problem's structure to simplify the learning process. A major challenge arises when data exhibit a local structure (i.e.  are made ``by parts'') that can be leveraged to better approximate the relation between (parts of) the input and (parts of) the output. Recent literature on signal processing  and in particular computer vision  shows that capturing these aspects is indeed essential to achieve state-of-the-art performance. However  in this context algorithms are typically derived on a case-by-case basis. In this work we propose the first theoretical framework to deal with part-based data from a general perspective and study a novel method within the setting of statistical learning theory. Our analysis is novel in that it explicitly quantifies the benefits of leveraging the part-based structure of a problem on the learning rates of the proposed estimator.,Localized Structured Prediction

Carlo Ciliberto 1

c.ciliberto@imperial.ac.uk

Francis Bach 2

francis.bach@inria.fr

Alessandro Rudi 2

alessandro.rudi@inria.fr

1 Department of Electrical and Electronic Engineering  Imperial College  London  UK.

2 INRIA - Département d’informatique  École Normale Supérieure - PSL Research University  Paris  France.

Abstract

Key to structured prediction is exploiting the problem’s structure to simplify the
learning process. A major challenge arises when data exhibit a local structure
(i.e.  are made “by parts”) that can be leveraged to better approximate the relation
between (parts of) the input and (parts of) the output. Recent literature on signal
processing  and in particular computer vision  shows that capturing these aspects is
indeed essential to achieve state-of-the-art performance. However  in this context
algorithms are typically derived on a case-by-case basis. In this work we propose
the ﬁrst theoretical framework to deal with part-based data from a general per-
spective and study a novel method within the setting of statistical learning theory.
Our analysis is novel in that it explicitly quantiﬁes the beneﬁts of leveraging the
part-based structure of a problem on the learning rates of the proposed estimator.

1

Introduction

Structured prediction deals with supervised learning problems where the output space is not endowed
with a canonical linear metric but has a rich semantic or geometric structure [5  29]. Typical
examples are settings in which the outputs correspond to strings (e.g.  captioning [19])  images (e.g. 
segmentation [1])  rankings [16  20]  points on a manifold [33]  probability distributions [24] or
protein foldings [18]. While the lack of linearity poses several modeling and computational challenges 
this additional complexity comes with a potentially signiﬁcant advantage: when suitably incorporated
within the learning model  knowledge about the structure allows to capture key properties of the data.
This could potentially lower the sample complexity of the problem  attaining better generalization
performance with less training examples. A natural scenario in this sense is the case where both
input and output data are organized into “parts” that can interact with one another according to a
speciﬁc structure. Examples can be found in computer vision (e.g.  segmentation [1]  localization
[6  22]  pixel-wise classiﬁcation [41])  speech recognition [4  40]  natural language processing [43] 
trajectory planing [31] or hierarchical classiﬁcation [44].
Recent literature on the topic has empirically shown that the local structure in the data can indeed
lead to signiﬁcantly better predictions than global approaches [17  45]. However in practice  these
ideas are typically investigated on a case-by-case basis  leading to ad-hoc algorithms that cannot
be easily adapted to new settings. On the theoretical side  few works have considered less speciﬁc
part-based factorizations [12] and a comprehensive theory analyzing the effect of local interactions
between parts within the context of learning theory is still missing.
In this paper  we propose: 1) a novel theoretical framework that can be applied to a wide family of
structured prediction settings able to capture potential local structure in the data  and 2) a structured
prediction algorithm  based on this framework for which we prove universal consistency and general-
ization rates. The proposed approach builds on recent results from the structured prediction literature
that leverage the concept of implicit embeddings [8  9  28  15  25]  also related to [30  39]. A key
contribution of our analysis is to quantify the impact of the part-based structure of the problem on the
learning rates of the proposed estimator. In particular  we prove that under natural assumptions on

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

20

40

60

80

100

120

140

160

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Figure 1: (Left) Between-locality in a sequence-to-sequence setting: each window (part) yp of the output
sequence y is fully determined by the part xp of the input sequence x  for every p 2 P . (Right) Empirical
within-locality Cp q of 100 images sampled from ImageNet between a 20 ⇥ 20 patch q and the central patch p.

20

40

60

80

100

120

140

160

the local behavior of the data  our algorithm beneﬁts adaptively from this underlying structure. We
support our theoretical ﬁndings with experiments on the task of detecting local orientation of ridges
in images depicting human ﬁngerprints.

2 Learning with Between- & Within-locality

To formalize the concept of locality within a learning problem  in this work we assume that the data is
structured in terms of “parts”. Practical examples of this setting often arise in image/audio or language
processing  where the signal has a natural factorization into patches or sub-sequences. Following
these guiding examples  we assume every input x 2 X and output y 2 Y to be interpretable as a
collection of (possibly overlapping) parts  and denote xp (respectively yp) its p-th part  with p 2 P a
set of part identiﬁers (e.g.  the position and size of a patch in an image). We assume input and output
to share same part structure with respect to P . To formalize the intuition that the learning problem
should interact well with this structure of parts  we introduce two key assumptions: between-locality
and within-locality. They characterize respectively the interplay between corresponding input-output
parts and the correlation of parts within the same input.
Assumption 1 (Between-locality). yp is conditionally independent from x  given xp  moreover the
probability of yp given xp is the same as yq given xq  for any p  q 2 P .
Between-locality (BL) assumes that the p-th part of the output y 2 Y depends only on the p-th part of
the input x 2 X  see Fig. 1 (Left) for an intuition in the case of sequence-to-sequence prediction.
This is often veriﬁed in pixel-wise classiﬁcation settings  where the class yp of a pixel p is determined
only by the sub-image in the corresponding patch xp. BL essentially corresponds to assuming a joint
graphical model on the parts of x and y  where each yp is only connected to xp but not to other parts.
BL motivates us to focus on a local level by directly learning the relation between input-output parts.
This is often an effective strategy in computer vision [22  45  17] but intuitively  one that provides
signiﬁcant advantages only when the input parts are not highly correlated with each other: in the
extreme case where all parts are identical  there is no advantage in solving the learning problem
locally. In this sense it can be useful to measure the amount of “covariance”

Cp q = Ex S(xp  xq)  Ex x0 S(xp  x0q)

(1)
between two parts p and q of an input x  for S(xp  xq) a suitable measure of similarity between parts
(if S(xp  xq) = xpxq  with xp and xq scalars random variables  then Cp q is the p  q-th entry of the
covariance matrix of the vector (x1  . . .   x|P|) ). Here ExS(xp  xq) and Ex x0S(xp  x0q) measure the
similarity between the p-th and the q-th part of  respectively  the same input  and two independent
ones (in particular Cp q = 0 when the p-th and q-th part of x are independent). In many applications 
it is reasonable to assume that Cp q decays according to the distance between p and q.
Assumption 2 (Within-locality). There exists a distance d : P ⇥ P ! R and   0  such that

(2)

|Cp q| 6 r2 ed(p q)

with

r2 = sup

x x0 |S(x  x0)|.

Within-locality (WL) is always satisﬁed for  = 0. However  when xp is independent of xq  it holds
with  = 1 and d(p  q) = p q the Dirac’s delta. Exponential decays of correlation are typically

2

Cp q decreases extremely fast as a function of the distancep  q  suggesting that Assumption 2

observed when the distribution of the parts of x factorizes in a graphical model that connects parts
which are close in terms of the distance d: although all parts depend on each other  the long-range
dependence typically goes to zero exponentially fast in the distance (see  e.g.  [26] for mixing
properties of Markov chains). Fig. 1 (Right) reports the empirical WL measured on 100 images
randomly sampled from ImageNet [13]: each pixel (i  j) reports the value of Cp q of the central
patch p with respect to a 20 ⇥ 20 patch q centered in (i  j). Here S(xp  xq) = x>p xq. We note that
holds for a large value of .
Contributions. In this work we present a novel structured prediction algorithm that adaptively
leverages locality in the learning problem  when present (Sec. 4). We study the generalization
properties of the proposed estimator (Sec. 5)  showing that it is equivalent to the state of the art in the
worst case scenario. More importantly  if the locality Assumptions 1 and 2 are satisﬁed  we prove that
our learning rates improve proportionally to the number |P| of parts in the problem. Here we give an
informal version of this main result  reported in more detail in Thm. 4 (Sec. 5). Below we denote by bf
the proposed estimator  by E(f ) the expected risk of a function f : X ! Y and f⇤ = argminf E(f ).
Theorem 1 (Informal - Learning Rates & Locality). Under mild assumptions on the loss and the
data distribution  if the learning problem is local (Assumptions 1 and 2)  there exists c0 > 0 such that

E hE(bf ) E (f⇤)i 6 c0✓ s

n|P|◆1/4

 

s =

r2
|P|

|P|Xp q=1

ed(p q) 

(3)

where the expectation is taken with respect to the sample of n input-output points used to train bf.
In the worst-case scenario  = 0 (no exponential decay of the covariance between parts)  the bound
in (3) scales as 1/n1/4 (since s = r2|P|) recovering [8]  where no structure is assumed on the parts.
However  as soon as > 0  s can be upper bounded by a constant independent of |P| and thus the
rate scales as 1/(|P|n)1/4  accelerating proportionally to the number of parts. In this sense  Thm. 1
shows the signiﬁcant beneﬁt of making use of locality. The following example focuses on the special
case of sequence-to-sequence prediction.
Example 1 (Locality on Sequences). As depicted in Fig. 1  for discrete sequences we can consider
parts (e.g.  windows) indexed by P = {1  . . .  |P|}  with d(p  q) = |p  q| for p  q 2 P (see
Appendix K.1 for more details). In this case  Assumption 2 leads to
(4)
which for > 0 is bounded by a constant not depending on the number of parts. Hence  Thm. 1
guarantees a learning rate of order 1/(n|P|)1/4  which is signiﬁcanlty faster than the rate 1/n1/4 of
methods that do not leverage locality such as [8]. See Sec. 6 for empirical support to this observation.

s 6 2r2(1  e)1 

3 Problem Formulation

We denote by X  Y and Z respectively the input space  label space and output space of a learning
problem. Let ⇢ be a probability measure on X ⇥ Y and 4 : Z ⇥ Y ⇥ X ! R a loss measuring
prediction errors between a label y 2 Y and a output z 2 Z  possibly parametrized by an input x 2 X.
To stress this interpretation we adopt the notation 4(z  y|x). Given a ﬁnite number of (xi  yi)n
independently sampled from ⇢  our goal is to approximate the minimizer f⇤ of the expected risk

i=1

f :X!Z E(f )  with E(f ) =Z 4(f (x)  y|x) d⇢(x  y).

min

(5)

Loss Made by Parts. We formalize the intuition introduced in Sec. 2 that data are decomposable
into parts: we denote the sets of parts of X  Y and Z by respectively [X]  [Y ] and [Z]. These are
abstract sets that depend on the problem at hand (see examples below). We assume P to be a set of
part “indices” equipped with a selection operator X ⇥ P ! [X] denoted (x  p) 7! [x]p (analogously
for Y and Z). When clear from context  we will use the shorthand xp = [x]p. For simplicity  in the
following we will assume P be ﬁnite  however our analysis generalizes also to the inﬁnite case (see

3

supplementary material). Let ⇡(·|x) be a probability distribution over the set of parts P   conditioned
with respect to an input x 2 X. We study loss functions 4 that can be represented as
(6)

⇡(p|x) Lp(zp  yp| xp).

4(z  y|x) =Xp2P

The collection of (Lp)p2P is a family of loss functions Lp : [Z] ⇥ [Y ] ⇥ [X] ! R  each comparing
the p-th part of a label y and output z. For instance  in an image processing scenario  Lp could
measure the similarity between the two images at different locations and scales  indexed by p. In this
sense  the distribution ⇡(p|x) allows to weigh each Lp differently depending on the application (e.g. 
mistakes at large scales could be more relevant than at lower scales). Various examples of parts and
concrete cases are illustrated in the supplementary material  here we report an extract.
Example 2 (Sequence to Sequence Prediction). Let X = Ak  Y = Z = Bk for two sets A  B
and k 2 N a ﬁxed length. We consider in this example parts that are windows of length l 6 k.
Then P = {1  . . .   k  l + 1} where p 2 P indexes the window xp = (x(p)  . . .   x(p+l1))  with
x 2 X  where we have denoted x(s) the s-th entry of the sequence x 2 X  analogous deﬁnition
for yp  zp. Finally  we choose the loss Lp to be the 0-1 distance between two strings of same length
Lp(zp  yp|x) = 1(zp 6= yp). Finally  we can choose ⇡(p|x) = 1/|P|  leading to a loss function
|P|Pp2P 1(zp 6= yp)  which is common in the context of Conditional Random Fields
4(z  y|x) = 1
(CRFs) [21].

The example above  highlights a tight connection between the framework considerd in this work
and the literature of CRFs. However  we care to stress that the two approaches differ by the way
they interpret the concepts of loss (used to evaluate ﬁtting errors at training time) and the score
functions (used to estimate predictions at inference time). Speciﬁcally  while such functions are two
separate entities in CRF settings  they essentially coincide in our framework (i.e. the score is a linear
combination of loss functions). However  as shown in Example 2  the resulting score functions for
both CRFs and our approach have essentially the same structure. Hence they ultimately lead to the
same inference problem [40]. We conclude this section by providing additional examples of loss
functons decomposable into parts.
Remark 1 (Examples of Loss Functions by Parts). Several loss functions used in machine learning
have a natural formulation in terms of (6). Notable examples are the Hamming distance [10  42  11] 
used in settings such as hierarchical classiﬁcation [44]  computer vision [29  45  41] or trajectory
planning [31] to name a few. Also  loss functions used in natural language processing  such as
the precision/recall and F1 score can be written in this form. Finally  we point out that multi-task
learning settings [27] can be seen as problem by parts  with the loss corresponding to the sum of
standard regression/classiﬁcation loss functions (least-squares  logistic  etc.) over the tasks/parts.

4 Algorithm

In this section we introduce our estimator for structured prediction problems with parts. Our approach
starts with an auxiliary step for dataset generation that explicitly extracts the parts from the data.
Auxiliary Dataset Generation. The locality assumptions introduced in Sec. 2 motivate us to learn the
local relations between individual parts p 2 P of each input-output pair. In this sense  given a training
dataset D = (xi  yi)n
i=1 a ﬁrst step would be to extract a new  part-based dataset {(xp  p  yp) | (x  y) 2
D  p 2 P}. However in most applications the cardinality |P| of the set of parts can be very large
(possibly inﬁnite as we discuss in the Appendix) making this process impractical. Instead  we
generate an auxiliary dataset by randomly sub-sampling m 2 N elements from the part-based dataset.
Concretely  for j 2{ 1  . . .   m}  we ﬁrst sample ij according to the uniform distribution Un on
{1  . . .   n}  set j = xij   sample pj ⇠ ⇡(· | j) and ﬁnally set ⌘j = [yij ]pj . This leads to the
auxiliary dataset D0 = (j  pj ⌘ j)m
Estimator. Given the auxiliary dataset  we propose the estimator bf : X ! Z  such that 8x 2 X
z2Z Xp2P
bf (x) = argmin
The functions ↵j : X ⇥ P ! R are learned from the auxiliary dataset and are the fundamental
components allowing our estimator to capture the part-based structure of the learning problem. Indeed 

↵j(x  p)h⇡(p|x) Lp(zp ⌘ j|xp)i.

j=1  as summarized in the GENERATE routine of Alg. 1.

mXj=1

(7)

4

Output

z<latexit sha1_base64="kAAWtYBn7/0aoVBmFqz2tm+nEUw=">AAAB6HicbZC7SwNBEMbnfMbzFbW0WQyCVbiz0UYM2lgmYB6QhLC3mUvW7O0du3tCPAL2NhaK2PrP2Nv537h5FJr4wcKP75thZyZIBNfG876dpeWV1bX13Ia7ubW9s5vf26/pOFUMqywWsWoEVKPgEquGG4GNRCGNAoH1YHA9zuv3qDSP5a0ZJtiOaE/ykDNqrFV56OQLXtGbiCyCP4PC5ad78QgA5U7+q9WNWRqhNExQrZu+l5h2RpXhTODIbaUaE8oGtIdNi5JGqNvZZNARObZOl4Sxsk8aMnF/d2Q00noYBbYyoqav57Ox+V/WTE143s64TFKDkk0/ClNBTEzGW5MuV8iMGFqgTHE7K2F9qigz9jauPYI/v/Ii1E6Lvlf0K16hdAVT5eAQjuAEfDiDEtxAGarAAOEJXuDVuXOenTfnfVq65Mx6DuCPnI8fV0iOxw==</latexit>

<latexit sha1_base64="tAmXynKJdtPm35TZtik9m1rvKrM=">AAAB6HicbZC7SgNBFIbPxltcb1FLm8EgWIVdG23EoI1lAuYCyRJmJ2eTMbMXZmaFuOQJbCwUsdWHsbcR38bJpdDEHwY+/v8c5pzjJ4Ir7TjfVm5peWV1Lb9ub2xube8UdvfqKk4lwxqLRSybPlUoeIQ1zbXAZiKRhr7Ahj+4GueNO5SKx9GNHibohbQX8YAzqo1Vve8Uik7JmYgsgjuD4sWHfZ68f9mVTuGz3Y1ZGmKkmaBKtVwn0V5GpeZM4MhupwoTyga0hy2DEQ1Redlk0BE5Mk6XBLE0L9Jk4v7uyGio1DD0TWVIdV/NZ2Pzv6yV6uDMy3iUpBojNv0oSAXRMRlvTbpcItNiaIAyyc2shPWppEyb29jmCO78yotQPym5TsmtOsXyJUyVhwM4hGNw4RTKcA0VqAEDhAd4gmfr1nq0XqzXaWnOmvXswx9Zbz9I15A7</latexit>
<latexit sha1_base64="tAmXynKJdtPm35TZtik9m1rvKrM=">AAAB6HicbZC7SgNBFIbPxltcb1FLm8EgWIVdG23EoI1lAuYCyRJmJ2eTMbMXZmaFuOQJbCwUsdWHsbcR38bJpdDEHwY+/v8c5pzjJ4Ir7TjfVm5peWV1Lb9ub2xube8UdvfqKk4lwxqLRSybPlUoeIQ1zbXAZiKRhr7Ahj+4GueNO5SKx9GNHibohbQX8YAzqo1Vve8Uik7JmYgsgjuD4sWHfZ68f9mVTuGz3Y1ZGmKkmaBKtVwn0V5GpeZM4MhupwoTyga0hy2DEQ1Redlk0BE5Mk6XBLE0L9Jk4v7uyGio1DD0TWVIdV/NZ2Pzv6yV6uDMy3iUpBojNv0oSAXRMRlvTbpcItNiaIAyyc2shPWppEyb29jmCO78yotQPym5TsmtOsXyJUyVhwM4hGNw4RTKcA0VqAEDhAd4gmfr1nq0XqzXaWnOmvXswx9Zbz9I15A7</latexit>
<latexit sha1_base64="/VpXkFonZ7oqnzGnYgpVrbAMIVo=">AAAB6HicbVA9TwJBEJ3DL8Qv1NJmIzGxInc2UhJtLCGRjwQuZG+Zg5W9vcvungle+AU2Fhpj60+y89+4wBUKvmSSl/dmMjMvSATXxnW/ncLG5tb2TnG3tLd/cHhUPj5p6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWByO/c7j6g0j+W9mSboR3QkecgZNVZqPg3KFbfqLkDWiZeTCuRoDMpf/WHM0gilYYJq3fPcxPgZVYYzgbNSP9WYUDahI+xZKmmE2s8Wh87IhVWGJIyVLWnIQv09kdFI62kU2M6ImrFe9ebif14vNWHNz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/WabqV+k8dRhDM4h0vw4BrqcAcNaAEDhGd4hTfnwXlx3p2PZWvByWdO4Q+czx/oAYz6</latexit>

Algorithm 1

p<latexit sha1_base64="bo9dss+6DWUHdvNWkVZVuywmJiw=">AAAB6HicbVDLSgNBEOyNrxhfUY96GAyCp7DrJR6DXjwmYB6QLGF20puMmZ1dZmaFsOQLvHhQxKtf4Xd48+anOHkcNLGgoajqprsrSATXxnW/nNza+sbmVn67sLO7t39QPDxq6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbD6g0j+WdGSfoR3QgecgZNVaqJ71iyS27M5BV4i1IqXr6Uf8GgFqv+NntxyyNUBomqNYdz02Mn1FlOBM4KXRTjQllIzrAjqWSRqj9bHbohJxbpU/CWNmShszU3xMZjbQeR4HtjKgZ6mVvKv7ndVITXvkZl0lqULL5ojAVxMRk+jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZl2XPLXt2mcQ1z5OEEzuACPKhAFW6hBg1ggPAIz/Di3DtPzqvzNm/NOYuZY/gD5/0H6NOPNQ==</latexit>

<latexit sha1_base64="rEhUZs3qal4B2jyV9+tPIlj3nL8=">AAAB6HicbVDLSgNBEOyNrxhfUY+KDAbBU9j1osegF48JmAckS5iddJIxs7PLzKwQlhw9efGgiFe/It/hzW/wJ5w8DppY0FBUddPdFcSCa+O6X05mZXVtfSO7mdva3tndy+8f1HSUKIZVFolINQKqUXCJVcONwEaskIaBwHowuJn49QdUmkfyzgxj9EPak7zLGTVWqsTtfMEtulOQZeLNSaF0PK58P56My+38Z6sTsSREaZigWjc9NzZ+SpXhTOAo10o0xpQNaA+blkoaovbT6aEjcmaVDulGypY0ZKr+nkhpqPUwDGxnSE1fL3oT8T+vmZjulZ9yGScGJZst6iaCmIhMviYdrpAZMbSEMsXtrYT1qaLM2GxyNgRv8eVlUrsoem7Rq9g0rmGGLBzBKZyDB5dQglsoQxUYIDzBC7w6986z8+a8z1ozznzmEP7A+fgBxxiQmw==</latexit>
<latexit sha1_base64="rEhUZs3qal4B2jyV9+tPIlj3nL8=">AAAB6HicbVDLSgNBEOyNrxhfUY+KDAbBU9j1osegF48JmAckS5iddJIxs7PLzKwQlhw9efGgiFe/It/hzW/wJ5w8DppY0FBUddPdFcSCa+O6X05mZXVtfSO7mdva3tndy+8f1HSUKIZVFolINQKqUXCJVcONwEaskIaBwHowuJn49QdUmkfyzgxj9EPak7zLGTVWqsTtfMEtulOQZeLNSaF0PK58P56My+38Z6sTsSREaZigWjc9NzZ+SpXhTOAo10o0xpQNaA+blkoaovbT6aEjcmaVDulGypY0ZKr+nkhpqPUwDGxnSE1fL3oT8T+vmZjulZ9yGScGJZst6iaCmIhMviYdrpAZMbSEMsXtrYT1qaLM2GxyNgRv8eVlUrsoem7Rq9g0rmGGLBzBKZyDB5dQglsoQxUYIDzBC7w6986z8+a8z1ozznzmEP7A+fgBxxiQmw==</latexit>
<latexit sha1_base64="NJcRm2LSCR31unftaQYs8y2SaBY=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9Ej04hESCyTQkO0yhZXtttndmpCGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczf3OEyrNE/lgpikGMR1JHnFGjZVa6aBac+vuAmSdeAWpQYHmoPrVHyYsi1EaJqjWPc9NTZBTZTgTOKv0M40pZRM6wp6lksaog3xx6IxcWGVIokTZkoYs1N8TOY21nsah7YypGetVby7+5/UyE90EOZdpZlCy5aIoE8QkZP41GXKFzIipJZQpbm8lbEwVZcZmU7EheKsvr5P2Vd1z617LrTVuizjKcAbncAkeXEMD7qEJPjBAeIZXeHMenRfn3flYtpacYuYU/sD5/AHY2Yzw</latexit>

Test

Input
x<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>

<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>
<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>
<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>

x0

<latexit sha1_base64="fzQOM3XmCMsoIkkF0vRHMJe6ZRc=">AAAB6XicbZC7SgNBFIbPeo3rLWppMxhEq7Bro40YtLGMYi6QLGF2cjYZMju7zMyKIQR8ABsLRWx9GHs738bJpdDEHwY+/v8c5pwTpoJr43nfzsLi0vLKam7NXd/Y3NrO7+xWdZIphhWWiETVQ6pRcIkVw43AeqqQxqHAWti7GuW1e1SaJ/LO9FMMYtqRPOKMGmvdPhy18gWv6I1F5sGfQuHi0z1/BIByK//VbCcsi1EaJqjWDd9LTTCgynAmcOg2M40pZT3awYZFSWPUwWA86ZAcWqdNokTZJw0Zu787BjTWuh+HtjKmpqtns5H5X9bITHQWDLhMM4OSTT6KMkFMQkZrkzZXyIzoW6BMcTsrYV2qKDP2OK49gj+78jxUT4q+V/RvvELpEibKwT4cwDH4cAoluIYyVIBBBE/wAq9Oz3l23pz3SemCM+3Zgz9yPn4AtLiO9g==</latexit>
<latexit sha1_base64="O2Bd58WbeH8pIuR8c3raJLFsFWs=">AAAB6XicbZDLSgMxFIbP1Fsdb1WXboJFdFVm3OhGLLpxWcVeoB1KJs20oZlkSDJiGfoGblwo4rYP496N+Daml4W2/hD4+P9zyDknTDjTxvO+ndzS8srqWn7d3djc2t4p7O7VtEwVoVUiuVSNEGvKmaBVwwynjURRHIec1sP+9TivP1ClmRT3ZpDQIMZdwSJGsLHW3eNxu1D0St5EaBH8GRQvP9yLZPTlVtqFz1ZHkjSmwhCOtW76XmKCDCvDCKdDt5VqmmDSx13atChwTHWQTSYdoiPrdFAklX3CoIn7uyPDsdaDOLSVMTY9PZ+Nzf+yZmqi8yBjIkkNFWT6UZRyZCQar406TFFi+MACJorZWRHpYYWJscdx7RH8+ZUXoXZa8r2Sf+sVy1cwVR4O4BBOwIczKMMNVKAKBCJ4ghd4dfrOs/PmvE9Lc86sZx/+yBn9AKZHkGo=</latexit>
<latexit sha1_base64="O2Bd58WbeH8pIuR8c3raJLFsFWs=">AAAB6XicbZDLSgMxFIbP1Fsdb1WXboJFdFVm3OhGLLpxWcVeoB1KJs20oZlkSDJiGfoGblwo4rYP496N+Daml4W2/hD4+P9zyDknTDjTxvO+ndzS8srqWn7d3djc2t4p7O7VtEwVoVUiuVSNEGvKmaBVwwynjURRHIec1sP+9TivP1ClmRT3ZpDQIMZdwSJGsLHW3eNxu1D0St5EaBH8GRQvP9yLZPTlVtqFz1ZHkjSmwhCOtW76XmKCDCvDCKdDt5VqmmDSx13atChwTHWQTSYdoiPrdFAklX3CoIn7uyPDsdaDOLSVMTY9PZ+Nzf+yZmqi8yBjIkkNFWT6UZRyZCQar406TFFi+MACJorZWRHpYYWJscdx7RH8+ZUXoXZa8r2Sf+sVy1cwVR4O4BBOwIczKMMNVKAKBCJ4ghd4dfrOs/PmvE9Lc86sZx/+yBn9AKZHkGo=</latexit>
<latexit sha1_base64="kOgXll9xUeMgz+G4CFAzvcFEdEI=">AAAB6XicbVA9TwJBEJ3DL8Qv1NJmIzFakTsbLIk2lmjkI4EL2VvmYMPe3mV3z0gu/AMbC42x9R/Z+W9c4AoFXzLJy3szmZkXJIJr47rfTmFtfWNzq7hd2tnd2z8oHx61dJwqhk0Wi1h1AqpRcIlNw43ATqKQRoHAdjC+mfntR1Sax/LBTBL0IzqUPOSMGivdP533yxW36s5BVomXkwrkaPTLX71BzNIIpWGCat313MT4GVWGM4HTUi/VmFA2pkPsWipphNrP5pdOyZlVBiSMlS1pyFz9PZHRSOtJFNjOiJqRXvZm4n9eNzXhlZ9xmaQGJVssClNBTExmb5MBV8iMmFhCmeL2VsJGVFFmbDglG4K3/PIqaV1WPbfq3bmV+nUeRxFO4BQuwIMa1OEWGtAEBiE8wyu8OWPnxXl3PhatBSefOYY/cD5/AEWAjSk=</latexit>

Train

p0

<latexit sha1_base64="4WpupHrjNcbUQWsFG+Vu/KmZ/yU=">AAAB6XicbVC7TsNAEFyHVwivACUUJyIEVWTTQBlBQ5kg8pASKzpf1skp57N1d0aKrPwBDQUI0fITfAcdHZ/C5VFAwkgrjWZ2tbsTJIJr47pfTm5ldW19I79Z2Nre2d0r7h80dJwqhnUWi1i1AqpRcIl1w43AVqKQRoHAZjC8mfjNB1Sax/LejBL0I9qXPOSMGivdJWfdYsktu1OQZeLNSaly/FH7BoBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7alkkao/Wx66ZicWqVHwljZkoZM1d8TGY20HkWB7YyoGehFbyL+57VTE175GZdJalCy2aIwFcTEZPI26XGFzIiRJZQpbm8lbEAVZcaGU7AheIsvL5PGRdlzy17NpnENM+ThCE7gHDy4hArcQhXqwCCER3iGF2foPDmvztusNefMZw7hD5z3H0lSj2Y=</latexit>
<latexit sha1_base64="7/yf2jvoONEmwc0y4gAKPPtHSMs=">AAAB6XicbVDLSgNBEOz1GeMr6lGRwSB6Crte9Bj04jER84AkhNnJbDJkdnaZ6RXCkqM3Lx4U8epP5Du8+Q3+hJPHQRMLGoqqbrq7/FgKg6775Swtr6yurWc2sptb2zu7ub39qokSzXiFRTLSdZ8aLoXiFRQoeT3WnIa+5DW/fzP2aw9cGxGpexzEvBXSrhKBYBStdBeftXN5t+BOQBaJNyP54tGo/P14PCq1c5/NTsSSkCtkkhrT8NwYWynVKJjkw2wzMTymrE+7vGGpoiE3rXRy6ZCcWqVDgkjbUkgm6u+JlIbGDELfdoYUe2beG4v/eY0Eg6tWKlScIFdsuihIJMGIjN8mHaE5QzmwhDIt7K2E9aimDG04WRuCN//yIqleFDy34JVtGtcwRQYO4QTOwYNLKMItlKACDAJ4ghd4dfrOs/PmvE9bl5zZzAH8gfPxAyeXkMw=</latexit>
<latexit sha1_base64="7/yf2jvoONEmwc0y4gAKPPtHSMs=">AAAB6XicbVDLSgNBEOz1GeMr6lGRwSB6Crte9Bj04jER84AkhNnJbDJkdnaZ6RXCkqM3Lx4U8epP5Du8+Q3+hJPHQRMLGoqqbrq7/FgKg6775Swtr6yurWc2sptb2zu7ub39qokSzXiFRTLSdZ8aLoXiFRQoeT3WnIa+5DW/fzP2aw9cGxGpexzEvBXSrhKBYBStdBeftXN5t+BOQBaJNyP54tGo/P14PCq1c5/NTsSSkCtkkhrT8NwYWynVKJjkw2wzMTymrE+7vGGpoiE3rXRy6ZCcWqVDgkjbUkgm6u+JlIbGDELfdoYUe2beG4v/eY0Eg6tWKlScIFdsuihIJMGIjN8mHaE5QzmwhDIt7K2E9aimDG04WRuCN//yIqleFDy34JVtGtcwRQYO4QTOwYNLKMItlKACDAJ4ghd4dfrOs/PmvE9bl5zZzAH8gfPxAyeXkMw=</latexit>
<latexit sha1_base64="kS16dWTX2TN5GUpptjWVDVXB36g=">AAAB6XicbVBNS8NAEJ3Ur1q/oh69LBbRU0m86LHoxWMV+wFtKJvtpl262YTdiVBC/4EXD4p49R9589+4bXPQ1gcDj/dmmJkXplIY9Lxvp7S2vrG5Vd6u7Ozu7R+4h0ctk2Sa8SZLZKI7ITVcCsWbKFDyTqo5jUPJ2+H4dua3n7g2IlGPOEl5ENOhEpFgFK30kJ733apX8+Ygq8QvSBUKNPruV2+QsCzmCpmkxnR9L8UgpxoFk3xa6WWGp5SN6ZB3LVU05ibI55dOyZlVBiRKtC2FZK7+nshpbMwkDm1nTHFklr2Z+J/XzTC6DnKh0gy5YotFUSYJJmT2NhkIzRnKiSWUaWFvJWxENWVow6nYEPzll1dJ67LmezX/3qvWb4o4ynACp3ABPlxBHe6gAU1gEMEzvMKbM3ZenHfnY9FacoqZY/gD5/MHOViNIQ==</latexit>

<latexit sha1_base64="v4j8uaegtE+C0iCEy+6RU2kGmcM=">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabSbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco0hxZXUuluwAxIkUALBUrophpYHEjoBOPbmd95BG2ESh5wkoIfs2EiIsEZWqnXfxIhjBjm0XRQrbl1dw66SryC1EiB5qD61Q8Vz2JIkEtmTM9zU/RzplFwCdNKPzOQMj5mQ+hZmrAYjJ/PT57SM6uENFLaVoJ0rv6eyFlszCQObGfMcGSWvZn4n9fLMLr2c5GkGULCF4uiTFJUdPY/DYUGjnJiCeNa2FspHzHNONqUKjYEb/nlVdK+qHtu3bu/rDVuijjK5IScknPikSvSIHekSVqEE0WeySt5c9B5cd6dj0VrySlmjskfOJ8/uwCRiA==</latexit>
<latexit sha1_base64="v4j8uaegtE+C0iCEy+6RU2kGmcM=">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabSbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco0hxZXUuluwAxIkUALBUrophpYHEjoBOPbmd95BG2ESh5wkoIfs2EiIsEZWqnXfxIhjBjm0XRQrbl1dw66SryC1EiB5qD61Q8Vz2JIkEtmTM9zU/RzplFwCdNKPzOQMj5mQ+hZmrAYjJ/PT57SM6uENFLaVoJ0rv6eyFlszCQObGfMcGSWvZn4n9fLMLr2c5GkGULCF4uiTFJUdPY/DYUGjnJiCeNa2FspHzHNONqUKjYEb/nlVdK+qHtu3bu/rDVuijjK5IScknPikSvSIHekSVqEE0WeySt5c9B5cd6dj0VrySlmjskfOJ8/uwCRiA==</latexit>
<latexit sha1_base64="v4j8uaegtE+C0iCEy+6RU2kGmcM=">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabSbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco0hxZXUuluwAxIkUALBUrophpYHEjoBOPbmd95BG2ESh5wkoIfs2EiIsEZWqnXfxIhjBjm0XRQrbl1dw66SryC1EiB5qD61Q8Vz2JIkEtmTM9zU/RzplFwCdNKPzOQMj5mQ+hZmrAYjJ/PT57SM6uENFLaVoJ0rv6eyFlszCQObGfMcGSWvZn4n9fLMLr2c5GkGULCF4uiTFJUdPY/DYUGjnJiCeNa2FspHzHNONqUKjYEb/nlVdK+qHtu3bu/rDVuijjK5IScknPikSvSIHekSVqEE0WeySt5c9B5cd6dj0VrySlmjskfOJ8/uwCRiA==</latexit>

bf<latexit sha1_base64="v4j8uaegtE+C0iCEy+6RU2kGmcM=">AAAB8nicbVBNS8NAEN3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabSbt0kw27E6WE/gwvHhTx6q/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco0hxZXUuluwAxIkUALBUrophpYHEjoBOPbmd95BG2ESh5wkoIfs2EiIsEZWqnXfxIhjBjm0XRQrbl1dw66SryC1EiB5qD61Q8Vz2JIkEtmTM9zU/RzplFwCdNKPzOQMj5mQ+hZmrAYjJ/PT57SM6uENFLaVoJ0rv6eyFlszCQObGfMcGSWvZn4n9fLMLr2c5GkGULCF4uiTFJUdPY/DYUGjnJiCeNa2FspHzHNONqUKjYEb/nlVdK+qHtu3bu/rDVuijjK5IScknPikSvSIHekSVqEE0WeySt5c9B5cd6dj0VrySlmjskfOJ8/uwCRiA==</latexit>

p<latexit sha1_base64="bo9dss+6DWUHdvNWkVZVuywmJiw=">AAAB6HicbVDLSgNBEOyNrxhfUY96GAyCp7DrJR6DXjwmYB6QLGF20puMmZ1dZmaFsOQLvHhQxKtf4Xd48+anOHkcNLGgoajqprsrSATXxnW/nNza+sbmVn67sLO7t39QPDxq6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbD6g0j+WdGSfoR3QgecgZNVaqJ71iyS27M5BV4i1IqXr6Uf8GgFqv+NntxyyNUBomqNYdz02Mn1FlOBM4KXRTjQllIzrAjqWSRqj9bHbohJxbpU/CWNmShszU3xMZjbQeR4HtjKgZ6mVvKv7ndVITXvkZl0lqULL5ojAVxMRk+jXpc4XMiLEllClubyVsSBVlxmZTsCF4yy+vkuZl2XPLXt2mcQ1z5OEEzuACPKhAFW6hBg1ggPAIz/Di3DtPzqvzNm/NOYuZY/gD5/0H6NOPNQ==</latexit>

<latexit sha1_base64="rEhUZs3qal4B2jyV9+tPIlj3nL8=">AAAB6HicbVDLSgNBEOyNrxhfUY+KDAbBU9j1osegF48JmAckS5iddJIxs7PLzKwQlhw9efGgiFe/It/hzW/wJ5w8DppY0FBUddPdFcSCa+O6X05mZXVtfSO7mdva3tndy+8f1HSUKIZVFolINQKqUXCJVcONwEaskIaBwHowuJn49QdUmkfyzgxj9EPak7zLGTVWqsTtfMEtulOQZeLNSaF0PK58P56My+38Z6sTsSREaZigWjc9NzZ+SpXhTOAo10o0xpQNaA+blkoaovbT6aEjcmaVDulGypY0ZKr+nkhpqPUwDGxnSE1fL3oT8T+vmZjulZ9yGScGJZst6iaCmIhMviYdrpAZMbSEMsXtrYT1qaLM2GxyNgRv8eVlUrsoem7Rq9g0rmGGLBzBKZyDB5dQglsoQxUYIDzBC7w6986z8+a8z1ozznzmEP7A+fgBxxiQmw==</latexit>
<latexit sha1_base64="rEhUZs3qal4B2jyV9+tPIlj3nL8=">AAAB6HicbVDLSgNBEOyNrxhfUY+KDAbBU9j1osegF48JmAckS5iddJIxs7PLzKwQlhw9efGgiFe/It/hzW/wJ5w8DppY0FBUddPdFcSCa+O6X05mZXVtfSO7mdva3tndy+8f1HSUKIZVFolINQKqUXCJVcONwEaskIaBwHowuJn49QdUmkfyzgxj9EPak7zLGTVWqsTtfMEtulOQZeLNSaF0PK58P56My+38Z6sTsSREaZigWjc9NzZ+SpXhTOAo10o0xpQNaA+blkoaovbT6aEjcmaVDulGypY0ZKr+nkhpqPUwDGxnSE1fL3oT8T+vmZjulZ9yGScGJZst6iaCmIhMviYdrpAZMbSEMsXtrYT1qaLM2GxyNgRv8eVlUrsoem7Rq9g0rmGGLBzBKZyDB5dQglsoQxUYIDzBC7w6986z8+a8z1ozznzmEP7A+fgBxxiQmw==</latexit>
<latexit sha1_base64="NJcRm2LSCR31unftaQYs8y2SaBY=">AAAB6HicbVBNT8JAEJ3iF+IX6tHLRmLiibRe9Ej04hESCyTQkO0yhZXtttndmpCGX+DFg8Z49Sd589+4QA8KvmSSl/dmMjMvTAXXxnW/ndLG5tb2Tnm3srd/cHhUPT5p6yRTDH2WiER1Q6pRcIm+4UZgN1VI41BgJ5zczf3OEyrNE/lgpikGMR1JHnFGjZVa6aBac+vuAmSdeAWpQYHmoPrVHyYsi1EaJqjWPc9NTZBTZTgTOKv0M40pZRM6wp6lksaog3xx6IxcWGVIokTZkoYs1N8TOY21nsah7YypGetVby7+5/UyE90EOZdpZlCy5aIoE8QkZP41GXKFzIipJZQpbm8lbEwVZcZmU7EheKsvr5P2Vd1z617LrTVuizjKcAbncAkeXEMD7qEJPjBAeIZXeHMenRfn3flYtpacYuYU/sD5/AHY2Yzw</latexit>

implied 
similarity
`(zp  y0p0)

<latexit sha1_base64="gb/0RFMA+wfeilWXrvTgmuyC2tA=">AAAB+3icbVDLSsNAFJ3UV62vWJduBou0gpREBF0W3bisYB/QhjCZTtqhk8kwMxFjyK+4caGIW3/EnX/jtM1CqwcuHM65l3vvCQSjSjvOl1VaWV1b3yhvVra2d3b37P1qV8WJxKSDYxbLfoAUYZSTjqaakb6QBEUBI71gej3ze/dEKhrzO50K4kVozGlIMdJG8u3qkDDWePTFKUzrfibq+Ylv15ymMwf8S9yC1ECBtm9/DkcxTiLCNWZIqYHrCO1lSGqKGckrw0QRgfAUjcnAUI4iorxsfnsOj40ygmEsTXEN5+rPiQxFSqVRYDojpCdq2ZuJ/3mDRIeXXka5SDTheLEoTBjUMZwFAUdUEqxZagjCkppbIZ4gibA2cVVMCO7yy39J96zpOk339rzWuiriKINDcAQawAUXoAVuQBt0AAYP4Am8gFcrt56tN+t90VqyipkD8AvWxzd+i5Ns</latexit>
<latexit sha1_base64="gb/0RFMA+wfeilWXrvTgmuyC2tA=">AAAB+3icbVDLSsNAFJ3UV62vWJduBou0gpREBF0W3bisYB/QhjCZTtqhk8kwMxFjyK+4caGIW3/EnX/jtM1CqwcuHM65l3vvCQSjSjvOl1VaWV1b3yhvVra2d3b37P1qV8WJxKSDYxbLfoAUYZSTjqaakb6QBEUBI71gej3ze/dEKhrzO50K4kVozGlIMdJG8u3qkDDWePTFKUzrfibq+Ylv15ymMwf8S9yC1ECBtm9/DkcxTiLCNWZIqYHrCO1lSGqKGckrw0QRgfAUjcnAUI4iorxsfnsOj40ygmEsTXEN5+rPiQxFSqVRYDojpCdq2ZuJ/3mDRIeXXka5SDTheLEoTBjUMZwFAUdUEqxZagjCkppbIZ4gibA2cVVMCO7yy39J96zpOk339rzWuiriKINDcAQawAUXoAVuQBt0AAYP4Am8gFcrt56tN+t90VqyipkD8AvWxzd+i5Ns</latexit>
<latexit sha1_base64="gb/0RFMA+wfeilWXrvTgmuyC2tA=">AAAB+3icbVDLSsNAFJ3UV62vWJduBou0gpREBF0W3bisYB/QhjCZTtqhk8kwMxFjyK+4caGIW3/EnX/jtM1CqwcuHM65l3vvCQSjSjvOl1VaWV1b3yhvVra2d3b37P1qV8WJxKSDYxbLfoAUYZSTjqaakb6QBEUBI71gej3ze/dEKhrzO50K4kVozGlIMdJG8u3qkDDWePTFKUzrfibq+Ylv15ymMwf8S9yC1ECBtm9/DkcxTiLCNWZIqYHrCO1lSGqKGckrw0QRgfAUjcnAUI4iorxsfnsOj40ygmEsTXEN5+rPiQxFSqVRYDojpCdq2ZuJ/3mDRIeXXka5SDTheLEoTBjUMZwFAUdUEqxZagjCkppbIZ4gibA2cVVMCO7yy39J96zpOk339rzWuiriKINDcAQawAUXoAVuQBt0AAYP4Am8gFcrt56tN+t90VqyipkD8AvWxzd+i5Ns</latexit>
<latexit sha1_base64="gb/0RFMA+wfeilWXrvTgmuyC2tA=">AAAB+3icbVDLSsNAFJ3UV62vWJduBou0gpREBF0W3bisYB/QhjCZTtqhk8kwMxFjyK+4caGIW3/EnX/jtM1CqwcuHM65l3vvCQSjSjvOl1VaWV1b3yhvVra2d3b37P1qV8WJxKSDYxbLfoAUYZSTjqaakb6QBEUBI71gej3ze/dEKhrzO50K4kVozGlIMdJG8u3qkDDWePTFKUzrfibq+Ylv15ymMwf8S9yC1ECBtm9/DkcxTiLCNWZIqYHrCO1lSGqKGckrw0QRgfAUjcnAUI4iorxsfnsOj40ygmEsTXEN5+rPiQxFSqVRYDojpCdq2ZuJ/3mDRIeXXka5SDTheLEoTBjUMZwFAUdUEqxZagjCkppbIZ4gibA2cVVMCO7yy39J96zpOk339rzWuiriKINDcAQawAUXoAVuQBt0AAYP4Am8gFcrt56tN+t90VqyipkD8AvWxzd+i5Ns</latexit>

observed 
similarity
k(xp  x0p0)

<latexit sha1_base64="f9BEIqmjgdd2hZNBgCGqBtl1jjw=">AAAB+HicbVBNS8NAEJ3Ur1o/GvXoZbFIK0hJRNBj0YvHCvYD2hA22227dLMJuxtpDf0lXjwo4tWf4s1/47bNQVsfDDzem2FmXhBzprTjfFu5tfWNza38dmFnd2+/aB8cNlWUSEIbJOKRbAdYUc4EbWimOW3HkuIw4LQVjG5nfuuRSsUi8aAnMfVCPBCszwjWRvLt4qgy9uNzNC77aVyenvl2yak6c6BV4makBBnqvv3V7UUkCanQhGOlOq4Tay/FUjPC6bTQTRSNMRnhAe0YKnBIlZfOD5+iU6P0UD+SpoRGc/X3RIpDpSZhYDpDrIdq2ZuJ/3mdRPevvZSJONFUkMWifsKRjtAsBdRjkhLNJ4ZgIpm5FZEhlphok1XBhOAuv7xKmhdV16m695el2k0WRx6O4QQq4MIV1OAO6tAAAgk8wyu8WU/Wi/VufSxac1Y2cwR/YH3+AC3Zkh0=</latexit>
<latexit sha1_base64="f9BEIqmjgdd2hZNBgCGqBtl1jjw=">AAAB+HicbVBNS8NAEJ3Ur1o/GvXoZbFIK0hJRNBj0YvHCvYD2hA22227dLMJuxtpDf0lXjwo4tWf4s1/47bNQVsfDDzem2FmXhBzprTjfFu5tfWNza38dmFnd2+/aB8cNlWUSEIbJOKRbAdYUc4EbWimOW3HkuIw4LQVjG5nfuuRSsUi8aAnMfVCPBCszwjWRvLt4qgy9uNzNC77aVyenvl2yak6c6BV4makBBnqvv3V7UUkCanQhGOlOq4Tay/FUjPC6bTQTRSNMRnhAe0YKnBIlZfOD5+iU6P0UD+SpoRGc/X3RIpDpSZhYDpDrIdq2ZuJ/3mdRPevvZSJONFUkMWifsKRjtAsBdRjkhLNJ4ZgIpm5FZEhlphok1XBhOAuv7xKmhdV16m695el2k0WRx6O4QQq4MIV1OAO6tAAAgk8wyu8WU/Wi/VufSxac1Y2cwR/YH3+AC3Zkh0=</latexit>
<latexit sha1_base64="f9BEIqmjgdd2hZNBgCGqBtl1jjw=">AAAB+HicbVBNS8NAEJ3Ur1o/GvXoZbFIK0hJRNBj0YvHCvYD2hA22227dLMJuxtpDf0lXjwo4tWf4s1/47bNQVsfDDzem2FmXhBzprTjfFu5tfWNza38dmFnd2+/aB8cNlWUSEIbJOKRbAdYUc4EbWimOW3HkuIw4LQVjG5nfuuRSsUi8aAnMfVCPBCszwjWRvLt4qgy9uNzNC77aVyenvl2yak6c6BV4makBBnqvv3V7UUkCanQhGOlOq4Tay/FUjPC6bTQTRSNMRnhAe0YKnBIlZfOD5+iU6P0UD+SpoRGc/X3RIpDpSZhYDpDrIdq2ZuJ/3mdRPevvZSJONFUkMWifsKRjtAsBdRjkhLNJ4ZgIpm5FZEhlphok1XBhOAuv7xKmhdV16m695el2k0WRx6O4QQq4MIV1OAO6tAAAgk8wyu8WU/Wi/VufSxac1Y2cwR/YH3+AC3Zkh0=</latexit>
<latexit sha1_base64="f9BEIqmjgdd2hZNBgCGqBtl1jjw=">AAAB+HicbVBNS8NAEJ3Ur1o/GvXoZbFIK0hJRNBj0YvHCvYD2hA22227dLMJuxtpDf0lXjwo4tWf4s1/47bNQVsfDDzem2FmXhBzprTjfFu5tfWNza38dmFnd2+/aB8cNlWUSEIbJOKRbAdYUc4EbWimOW3HkuIw4LQVjG5nfuuRSsUi8aAnMfVCPBCszwjWRvLt4qgy9uNzNC77aVyenvl2yak6c6BV4makBBnqvv3V7UUkCanQhGOlOq4Tay/FUjPC6bTQTRSNMRnhAe0YKnBIlZfOD5+iU6P0UD+SpoRGc/X3RIpDpSZhYDpDrIdq2ZuJ/3mdRPevvZSJONFUkMWifsKRjtAsBdRjkhLNJ4ZgIpm5FZEhlphok1XBhOAuv7xKmhdV16m695el2k0WRx6O4QQq4MIV1OAO6tAAAgk8wyu8WU/Wi/VufSxac1Y2cwR/YH3+AC3Zkh0=</latexit>

y0

<latexit sha1_base64="DedVXuzqohYwGt26TRQfrqg/Pf4=">AAAB6XicbVC7TsNAEFyHVwivACUUJyIEVWTTQBlBQ5kg8pASKzpf1skp57N1d0aKrPwBDQUI0fITfAcdHZ/C5VFAwkgrjWZ2tbsTJIJr47pfTm5ldW19I79Z2Nre2d0r7h80dJwqhnUWi1i1AqpRcIl1w43AVqKQRoHAZjC8mfjNB1Sax/LejBL0I9qXPOSMGivdjc66xZJbdqcgy8Sbk1Ll+KP2DQDVbvGz04tZGqE0TFCt256bGD+jynAmcFzopBoTyoa0j21LJY1Q+9n00jE5tUqPhLGyJQ2Zqr8nMhppPYoC2xlRM9CL3kT8z2unJrzyMy6T1KBks0VhKoiJyeRt0uMKmREjSyhT3N5K2IAqyowNp2BD8BZfXiaNi7Lnlr2aTeMaZsjDEZzAOXhwCRW4hSrUgUEIj/AML87QeXJenbdZa86ZzxzCHzjvP1b/j28=</latexit>
<latexit sha1_base64="QSLzEHFJDnMA9st08y9SebZEOV8=">AAAB6XicbVC7SgNBFL3rM8ZX1FKRwSBahV0bLYM2lomYByRLmJ3MJkNmZpeZWWFZUtrZWChi60/kO+z8Bn/CyaPQxAMXDufcy733BDFn2rjul7O0vLK6tp7byG9ube/sFvb26zpKFKE1EvFINQOsKWeS1gwznDZjRbEIOG0Eg5ux33igSrNI3ps0pr7APclCRrCx0l161ikU3ZI7AVok3owUy0ej6vfj8ajSKXy2uxFJBJWGcKx1y3Nj42dYGUY4HebbiaYxJgPcoy1LJRZU+9nk0iE6tUoXhZGyJQ2aqL8nMiy0TkVgOwU2fT3vjcX/vFZiwis/YzJODJVkuihMODIRGr+NukxRYnhqCSaK2VsR6WOFibHh5G0I3vzLi6R+UfLckle1aVzDFDk4hBM4Bw8uoQy3UIEaEAjhCV7g1Rk4z86b8z5tXXJmMwfwB87HDzVEkNU=</latexit>
<latexit sha1_base64="QSLzEHFJDnMA9st08y9SebZEOV8=">AAAB6XicbVC7SgNBFL3rM8ZX1FKRwSBahV0bLYM2lomYByRLmJ3MJkNmZpeZWWFZUtrZWChi60/kO+z8Bn/CyaPQxAMXDufcy733BDFn2rjul7O0vLK6tp7byG9ube/sFvb26zpKFKE1EvFINQOsKWeS1gwznDZjRbEIOG0Eg5ux33igSrNI3ps0pr7APclCRrCx0l161ikU3ZI7AVok3owUy0ej6vfj8ajSKXy2uxFJBJWGcKx1y3Nj42dYGUY4HebbiaYxJgPcoy1LJRZU+9nk0iE6tUoXhZGyJQ2aqL8nMiy0TkVgOwU2fT3vjcX/vFZiwis/YzJODJVkuihMODIRGr+NukxRYnhqCSaK2VsR6WOFibHh5G0I3vzLi6R+UfLckle1aVzDFDk4hBM4Bw8uoQy3UIEaEAjhCV7g1Rk4z86b8z5tXXJmMwfwB87HDzVEkNU=</latexit>
<latexit sha1_base64="vc8g6+JR5IwOpGHOELKqDXqKSPk=">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbRU0m86LHoxWMV+wFtKJvtpF262YTdjRBC/4EXD4p49R9589+4bXPQ1gcDj/dmmJkXJIJr47rfTmltfWNzq7xd2dnd2z+oHh61dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5nfmdJ1Sax/LRZAn6ER1JHnJGjZUesvNBtebW3TnIKvEKUoMCzUH1qz+MWRqhNExQrXuemxg/p8pwJnBa6acaE8omdIQ9SyWNUPv5/NIpObPKkISxsiUNmau/J3IaaZ1Fge2MqBnrZW8m/uf1UhNe+zmXSWpQssWiMBXExGT2NhlyhcyIzBLKFLe3EjamijJjw6nYELzll1dJ+7LuuXXv3q01boo4ynACp3ABHlxBA+6gCS1gEMIzvMKbM3FenHfnY9FacoqZY/gD5/MHRwWNKg==</latexit>

p0

<latexit sha1_base64="4WpupHrjNcbUQWsFG+Vu/KmZ/yU=">AAAB6XicbVC7TsNAEFyHVwivACUUJyIEVWTTQBlBQ5kg8pASKzpf1skp57N1d0aKrPwBDQUI0fITfAcdHZ/C5VFAwkgrjWZ2tbsTJIJr47pfTm5ldW19I79Z2Nre2d0r7h80dJwqhnUWi1i1AqpRcIl1w43AVqKQRoHAZjC8mfjNB1Sax/LejBL0I9qXPOSMGivdJWfdYsktu1OQZeLNSaly/FH7BoBqt/jZ6cUsjVAaJqjWbc9NjJ9RZTgTOC50Uo0JZUPax7alkkao/Wx66ZicWqVHwljZkoZM1d8TGY20HkWB7YyoGehFbyL+57VTE175GZdJalCy2aIwFcTEZPI26XGFzIiRJZQpbm8lbEAVZcaGU7AheIsvL5PGRdlzy17NpnENM+ThCE7gHDy4hArcQhXqwCCER3iGF2foPDmvztusNefMZw7hD5z3H0lSj2Y=</latexit>
<latexit sha1_base64="7/yf2jvoONEmwc0y4gAKPPtHSMs=">AAAB6XicbVDLSgNBEOz1GeMr6lGRwSB6Crte9Bj04jER84AkhNnJbDJkdnaZ6RXCkqM3Lx4U8epP5Du8+Q3+hJPHQRMLGoqqbrq7/FgKg6775Swtr6yurWc2sptb2zu7ub39qokSzXiFRTLSdZ8aLoXiFRQoeT3WnIa+5DW/fzP2aw9cGxGpexzEvBXSrhKBYBStdBeftXN5t+BOQBaJNyP54tGo/P14PCq1c5/NTsSSkCtkkhrT8NwYWynVKJjkw2wzMTymrE+7vGGpoiE3rXRy6ZCcWqVDgkjbUkgm6u+JlIbGDELfdoYUe2beG4v/eY0Eg6tWKlScIFdsuihIJMGIjN8mHaE5QzmwhDIt7K2E9aimDG04WRuCN//yIqleFDy34JVtGtcwRQYO4QTOwYNLKMItlKACDAJ4ghd4dfrOs/PmvE9bl5zZzAH8gfPxAyeXkMw=</latexit>
<latexit sha1_base64="7/yf2jvoONEmwc0y4gAKPPtHSMs=">AAAB6XicbVDLSgNBEOz1GeMr6lGRwSB6Crte9Bj04jER84AkhNnJbDJkdnaZ6RXCkqM3Lx4U8epP5Du8+Q3+hJPHQRMLGoqqbrq7/FgKg6775Swtr6yurWc2sptb2zu7ub39qokSzXiFRTLSdZ8aLoXiFRQoeT3WnIa+5DW/fzP2aw9cGxGpexzEvBXSrhKBYBStdBeftXN5t+BOQBaJNyP54tGo/P14PCq1c5/NTsSSkCtkkhrT8NwYWynVKJjkw2wzMTymrE+7vGGpoiE3rXRy6ZCcWqVDgkjbUkgm6u+JlIbGDELfdoYUe2beG4v/eY0Eg6tWKlScIFdsuihIJMGIjN8mHaE5QzmwhDIt7K2E9aimDG04WRuCN//yIqleFDy34JVtGtcwRQYO4QTOwYNLKMItlKACDAJ4ghd4dfrOs/PmvE9bl5zZzAH8gfPxAyeXkMw=</latexit>
<latexit sha1_base64="kS16dWTX2TN5GUpptjWVDVXB36g=">AAAB6XicbVBNS8NAEJ3Ur1q/oh69LBbRU0m86LHoxWMV+wFtKJvtpl262YTdiVBC/4EXD4p49R9589+4bXPQ1gcDj/dmmJkXplIY9Lxvp7S2vrG5Vd6u7Ozu7R+4h0ctk2Sa8SZLZKI7ITVcCsWbKFDyTqo5jUPJ2+H4dua3n7g2IlGPOEl5ENOhEpFgFK30kJ733apX8+Ygq8QvSBUKNPruV2+QsCzmCpmkxnR9L8UgpxoFk3xa6WWGp5SN6ZB3LVU05ibI55dOyZlVBiRKtC2FZK7+nshpbMwkDm1nTHFklr2Z+J/XzTC6DnKh0gy5YotFUSYJJmT2NhkIzRnKiSWUaWFvJWxENWVow6nYEPzll1dJ67LmezX/3qvWb4o4ynACp3ABPlxBHe6gAU1gEMEzvMKbM3ZenHfnY9FacoqZY/gD5/MHOViNIQ==</latexit>

Figure 2: Illustration of the prediction process for the
Localized Structured Prediction Estimator (7) for a
hypothetical computer vision application.

training set (xi  yi)n

Input:
i=1  distributions
⇡(·|x) a reproducing kernel k on X ⇥ P   hyper-
parameter > 0  auxiliary dataset size m 2 N.
GENERATE the auxiliary set (⌘j  j  pj)m
j=1:
Sample ij 2 Un(·). Set j = xij .
Sample pj ⇠ ⇡(·|j). Set ⌘j = [yij ]pj .

LEARN the coefﬁcients for the map ↵:

Set K with Kjj0 = k((j  pj)  (j0   pj0 )).
A = (K + mI)1.

Return the map ↵ : (x  p) 7! A v(x  p) 2 Rm
with v(x  p)j = k(j  pj)  (x  p).

for any test point x 2 X and part p 2 P   the value ↵j(x  p) can be interpreted as a measure of how
similar xp is to the pj-th part of the auxiliary training point j. For instance  assume ↵j(x  p) to be
an approximation of the delta function that is 1 when xp = [j]pj

and 0 otherwise. Then 

↵j(x  p) Lp(zp ⌘ j|xp) ⇡ (xp  [j]pj ) Lp(zp ⌘ j|xp) 

which implies essentially that

(8)

xp ⇡ [j]pj =) zp ⇡ ⌘j.

(9)
In other words  if the p-th part of test input x and the pj-th part of the auxiliary training input j (i.e. 
the pj-th part of the training input xij ) are deemed similar  then the estimator will encourage the
p-th part of the test output z to be similar to the auxiliary part ⌘j. This process is illustrated in Fig. 2
for an ideal computer vision application: for a given test image x  the ↵ scores detect a similarity
between the p-th patch of x and the pj-th patch of the training input xij . Hence  the estimator will
enforce the p-th patch of the output z to be similar to the pj-th patch of the training label yij .
Learning ↵. In line with previous work on structured prediction [8]  we learn each ↵j by solving a
linear system for a problem akin to kernel ridge regression (see Sec. 5 for the theoretical motivation).
In particular  let k : (X ⇥ P ) ⇥ (X ⇥ P ) ! R be a positive deﬁnite kernel  we deﬁne

(↵1(x  p)  . . .  ↵ m(x  p))> = (K + mI)1v(x  p) 

(10)
where K 2 Rm⇥m is the empircal kernel matrix with entries Kjh = k((j  pj)  (h  ph)) and
v(x  p) 2 Rm is the vector with entries v(x  p)j = k((j  pj)  (x  p)). Training the proposed
algorithm  consists in precomputing A = (K + mI)1 to evaluate the coefﬁcients ↵ as detailed by
the LEARN routine in Alg. 1. While computing A amounts to solving a linear system  which requires
O(m3) operations  we note that it is possible to achieve the same statistical accuracy with reduced
complexity O(mpm) by means of low rank approximations (see [14  32]).

Remark 2 (Evaluating bf). According to (7)  evaluating bf on a test point x 2 X consists in solving
an optimization problem over the output space Z. This is a standard strategy in structured prediction 
where an optimization protocol is derived on a case-by-case basis depending on both 4 and Z
(see  e.g.  [29]). Hence  from a computational viewpoint  the inference step in this work is not
more demanding than previous methods (while also enjoying strong theoretical guarantees on the
prediction perfomance  as discussed in Sec. 5). Moreover  the speciﬁc form of our estimator suggests
a general stochastic meta-algorithm to address the inference problem in special settings. In particular 
we can reformulate (7) as

with p sampled according to ⇡  j 2{ 1  . . .   m} sampled according to the weights ↵j and hj p suitably
deﬁned in terms of Lp. When the hj p are (sub)differentiable  (11) can be effectively addressed by
stochastic gradient methods (SGM). In Alg. 3 in Appendix J we give an example of this strategy.

Ej p hj p(z|x) 

(11)

bf (x) = argmin

z2Z

5

5 Generalization Properties of Structured Prediction with Parts

In this section we study the statistical properties for the proposed algorithm  with particular attention
to the impact of locality on learning rates  see Thm. 4 (for a complete analysis of univeral consistency
and learning rates without locality assumptions  see Appendices F and H). Our analysis leverages the
assumption that the loss function 4 is a Structure Encoding Loss Function (SELF) by Parts.
Deﬁnition 1 (SELF by Parts). A function 4 : Z⇥Y ⇥X ! R is a Structure Encoding Loss Function
(SELF) by Parts if it admits a factorization in the form of (6) with functions Lp : [Z]⇥[Y ]⇥[X] ! R 
and there exists a separable Hilbert space H and two bounded maps : [Z] ⇥ [X] ⇥ P !H  
' : [Y ] !H such that for any ⇣ 2 [Z]  ⌘ 2 [Y ]  ⇠ 2 [X]  p 2 P
(12)

Lp(⇣ ⌘ |⇠) = h (⇣ ⇠  p ) ' (⌘)iH .

The deﬁnition of “SELF by Parts” specializes the deﬁnition of SELF in [9] and in the following
we will always assume 4 to satisfy it. Indeed  Def. 1 is satisﬁed when the spaces of parts involved
are discrete sets and it is rather mild in the general case (see [8] for an exhaustive list of examples).
Note that when 4 is SELF  the solution of (5) is completely characterized in terms of the conditional
expectation (related to the conditional mean embedding [7  23  36  34]) of '(yp) given x  denoted by
g⇤ : X ⇥ P !H   as follows.
Lemma 2. Let 4 be SELF and Z compact. Then  the minimizer of (5) is ⇢X-a.e. characterized by
f⇤(x) = argmin
(13)

 

⇡(p|x)h (zp  xp  p)  g⇤(x  p)iH

g⇤(x  p) =ZY

'(yp)d⇢(y|x).

z2Z Xp2P

Lemma 2 (proved in Appendix C) shows that f⇤ is completely characterized in terms of the conditional

expectation g⇤  which indeed plays a key role in controlling the learning rates of bf. In particular 

we investigate the learning rates in light of the two assumptions of between- and within-locality
introduced in Sec. 2. To this end  we ﬁrst study the direct effects of these two assumptions on the
learning framework introduced in this work.
The effect of Between-locality. We start by observing that the between-locality between parts of the
inputs and parts of the output allows for a reﬁned characterization of the conditional mean g⇤.
Lemma 3. Let g⇤ be deﬁned as in (13). Under Assumption 1  there exists ¯g⇤ : [X] !H such that
(14)

g⇤(x  p) = ¯g⇤(xp)

8x 2 X  p 2 P.

Lemma 3 above shows that we can learn g⇤ by focusing on a “simpler” problem  identiﬁed by
the function ¯g⇤ acting only the parts [X] of X rather than on the whole input directly (for a proof
see Lemma 21 in Appendix G). This motivates the adoption of the restriction kernel [6]  namely a
function k : (X ⇥ P ) ⇥ (X ⇥ P ) ! R such that

k((x  p)  (x0  q)) = ¯k(xp  xq) 

(15)
which  for any pair of inputs x  x0 2 X and parts p  q 2 P   measures the similarity between the p-part
of x and the q-th part of q via a kernel ¯k : [X] ⇥ [X] ! R on the parts of X. The restriction kernel is
a well-established tool in structured prediction settings [6] and it has been observed to be remarkably
effective in computer vision applications [22  45  17].
The effect of Within-locality. We recall that within-locality characterizes the statistical correlation
between two different parts of the input (see Assumption 2). To this end we consider the simpliﬁed
for any
scenario where the parts are sampled from the uniform distribution on P   i.e.  ⇡(p|x) = 1
|P|
x 2 X and p 2 P . While more general situations can be considered  this setting is useful to illustrate
the effect we are interested in this work. We now deﬁne some important quantities that characterize
the learning rates under locality 

Cp q = Ex x0⇥ ¯k(xp  xq)2  ¯k(xp  x0q)2⇤  

r = sup

x2X p2P

¯k(xp  xp).

(16)

It is clear that the terms Cp q and r above correspond respectively to the correlations introduced in (1)

and the scale parameter introduced in (2)  with similarity function S = ¯k2. Let bf be the structured
prediction estimator in (7) learned using the restriction kernel in (15) based on ¯k and denote by ¯G the

6

0.12

0.1

0.08

0.06

0.04

0.02

 Local-

Local-LS

Global-

KRLS

Figure 3: Learning the direction of ridges in ﬁngerprint images. (Left) Examples of ground truths and predictions
with pixels’ color corresponding to the local direction of ridges. (Right) Test error according to 4 in (18).

space of functions ¯G = H⌦ ¯F with ¯F the reproducing kernel Hilbert space [3] associated to ¯k. In
particular  in the following we will consider the standard assumption in the context of non-parametric
estimation [7] on the regularity of the target function  which in our context reads as ¯g⇤ 2 ¯G. Finally
we introduce c2
to measure the “complexity” of the loss 4
w.r.t. the representation induced by SELF decomposition (Def. 1) analogously to Thm. 2 of [8].
Theorem 4 (Learning Rates & Locality). Under Assumptions 1 and 2 with S = ¯k2  let ¯g⇤ satisfying
Lemma 3  with ¯g = k¯g⇤k ¯G < 1. Let s be as in (3). When  = (r2/m + s/(|P|n))1/2  then

4 = supz2Z x2X

H

1

|P|Pp2P k (z  x  p)k2
E E(bf ) E (f⇤) 6 12 c4 ¯g ✓ r2

m

The proof of the result above can be found in Appendix G.1. We can see that between- and within-
locality allow to reﬁne (and potentially improve) the bound of n1/4 from structured prediction
without locality [8] (see also Thm. 5 in Appendix F). In particular  we observe that the adoption of the
restriction kernel in Thm. 4 allows the structured prediction estimator to leverage the within-locality 
gaining a beneﬁt proportional to the magnitude of the parameter . Indeed r2 6 s 6 r2|P| by
deﬁnition. More precisely  if  = 0 (e.g.  all parts are identical copies) then s = r2|P| and we recover
the rate of O(n1/4) of [8]  while if  is large (the parts are almost not correlated) then s = r2 and we
can take m / n|P| achieving a rate of the order of O(n|P|)1/4. We clearly see that depending

on the amount of within-locality in the learning problem  the proposed estimator is able to gain
signiﬁcantly in terms of ﬁnite sample bounds.

+

r2
|P|n

+

s

|P|n◆1/4

.

(17)

6 Empirical Evaluation

We evaluate the proposed estimator on simulated as well as real data. We highlight how locality leads
to improved generalization performance  in particular when only few training examples are available.
Learning the Direction of Ridges for Fingerprint. Similarly to [37]  we considered the problem of
detecting the pointwise direction of ridges in a ﬁngerprint image on the FVC04 dataset1 comprising 80
grayscale 640 ⇥ 480 input images depicting ﬁngerprints and corresponding output images encoding
in each pixel the local direction of the ridges of the input ﬁngerprint as an angle ✓ 2 [⇡  ⇡ ]. A
natural loss function is the average pixel-wise error sin(✓  ✓0)2 between a ground-truth angle ✓ and
the predicted ✓0 according to the geodesic distance on the sphere. To apply the proposed algorithm 
we consider the following representation of the loss in term of parts: let P be the collection of patches
of dimension 20 ⇥ 20 and equispaced each 5 ⇥ 5 pixels2 so that each pixel belongs exactly to 16
patches. For all z  y 2 R640⇥480  the average pixel-wise error is
1

L(zp  yp) 

with

L(⇣ ⌘ ) =

sin([⇣]ij  [⌘]ij)2 

(18)

4(z  y) =

16

|P|Xp2P

20Xi j=1

20 ⇥ 20

1http://bias.csr.unibo.it/fvc2004  DB1_B. The output is obtained by applying 7⇥ 7 Sobel ﬁltering.
2For simplicity we assume “circular images”  namely [x]i j = [x](i mod 640) (j mod 480).

7

103

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

 = 0.0  Global-LS
 = 0.1  Global-LS
 = 0.2  Global-LS
 = 0.6  Global-LS
 = 1.6  Global-LS
 = 4.0  Global-LS
 = 10.0  Global-LS

Independent-Parts-LS

 = 0.1  Local-LS
 = 0.2  Local-LS
 = 0.6  Local-LS
 = 1.6  Local-LS
 = 4.0  Local-LS
 = 10.0  Local-LS

102

Figure 4: Empirical estimation of within-locality
for the central patch of the ﬁngerprints dataset.

Figure 5: Effect of within-locality w.r.t.  and |P|:
Global-LS vs. IndependentParts-LS vs. Local-LS (ours).

1

50

100

# of Parts

150

200

where ⇣ = zp ⌘ = yp 2 [⇡  ⇡ ]20⇥20 are the extracted patches and [·]ij their value at pixel (i  j).
We compared our approach using 4 (Local-4) or least-squares (Local-LS) with competitors that do
not take into account the local structure of the problem  namely standard vector-valued kernel ridge
regression (KRLS) [7] and the structured prediction algorithm in [8] with 4 loss (4-Global). We used
a Gaussian kernel on the input (for the local estimators the restriction kernel in (15) with ¯k Gaussian).
We randomly sampled 50/30 images for training/testing  performing 5-fold cross-validation on  in
[106  10] (log spaced) and the kernel bandwidth in [103  1]. For Local-4 and Local-LS we built
an auxiliary set with m = 30000 random patches (see Sec. 4)  sampled from the 50 training images.
Results. Fig. 3 (Left) reports the average prediction error across 10 random train-test splits. We make
two observations: ﬁrst  methods that leverage the locality in the data are consistently superior to their
“global” counterparts  supporting our theoretical results in Sec. 5 that the proposed estimator can lead
to signiﬁcantly better performance  in particular when few training points are available. Second  the
experiment suggests that choosing the right loss is critical  since exploiting locality without the right
loss (i.e.  Local-LS in the ﬁgure) generally leads to worse performance. The three sample predictions
in Fig. 3 (Right) provide more qualitative insights on the models tested. In particular while both
locality-aware methods are able to recover the correct structure of the ﬁngerprints  only combining
this information with the loss 4 leads to accurate recovery of the ridge orientation.
Within-locality. In Fig. 4 we visualize the (empirical) within-locality of the central patch p for the
ﬁngerprint dataset. The ﬁgure depicts Cp q (deﬁned in (16)) for q 2 P   with the (i  j)-th pixel in
the image corresponding to Cp q with q the 20 ⇥ 20 patch centered in (i  j). The fast decay of these
values as the distance from the central patch p increase  suggests that within-locality holds for a large
value of   possibly justifying the good performance exhibited by (Local-4) in light of Thm. 4.
Simulation: Within-Locality. We complement our analysis with synthetic experiments where we
control the “amount” of within-locality . We considered a setting where input points are vectors
x 2 Rk|P| comprising |P| parts of dimension k = 1000. Inputs are sampled according to a normal
distribution with zero mean and covariance ⌃() = M () ⌦ I  where M () 2 R|P|⇥|P| has entries
M ()pq = ed(p q) and d(p  q) = |p  q|/|P|. By design  as  grows C varies from being rank-one
(all parts are identical copies) to diagonal (all parts are independently sampled).
To isolate the effect of within-locality on learning  we tested our estimator on a linear multitask (actu-
ally vector-valued) regression problem with least-squares loss 4. We generated datasets (xi  yi)n
i=1
of size n = 100 for training and n = 1000 for testing  with xi sampled as described above and
yi = w>xi + ✏ with noise ✏ 2 Rk|P| sampled from an isotropic Gaussian with standard deviation
0.5. To guarantee between-locality to hold  we generated the target vector w = [ ¯w  . . .   ¯w] 2 Rk|P|
by concatenating copies of a ¯w 2 Rk sampled uniformly on the radius-one ball. We performed
regression with linear restriction kernel on the parts/subvectors (Local-LS) on the “full” auxiliary
dataset ([xi]p  [yi]p) with 1 6 i 6 n and 1 6 p 6 |P|  and compared it with standard linear regres-
sion (Global-LS) on the original dataset (xi  yi)n
i=1 and linear regression performed independently
for each (local) subdataset ([xi]p  [yi]p)n
i=1 (IndependentParts - LS). The parameter  was chosen by
hold-out cross-validation in [106  10] (log spaced).

8

Fig. 5 reports the (log scale) mean square error (MSE) across 100 runs of the two estimators for
increasing values of  and |P|. In line with Thm. 4  when  and |P| are large  Local-LS signiﬁcantly
outperforms both i) Global-LS  which solves one single problem jointly and does not beneﬁt within-
locality  and ii) IndependentParts-LS  which is insensitive to the between-locality across parts and
solves each local prediction problem in isolation. For a smaller   such advantage becomes less
prominent even when the number of parts is large. This is expected since for  = 0 the input parts
are extremely correlated and there is no within locality that can be exploited.

7 Conclusion

We proposed a novel approach for structured prediction in presence of locality in the data. Our
method builds on [8] by incorporating knowledge of the parts directly within the learning model. We
proved the beneﬁts of locality by showing that  under a low-correlation assumption on the parts of the
input (within locality)  the learning rates of our estimator can improve proportionally to the number
of parts in the data. To obtain this result we additionally introduced a natural assumption on the
conditional independence between input-output parts (between locality)  which provides also a formal
justiﬁcation for adoption of the so-called “restriction kernel”  previously proposed in the literature 
as a mean to lower the sample complexity of the problem. Empirical evaluation on synthetic as
well as real data shows that our approach offers signiﬁcant advantages when few training points are
available and leveraging structural information such as locality is crucial to achieve good prediction
performance. We identify two main directions for future work: 1) consider settings where the parts
are unknown (or “latent”) and need to be discovered/learned from data; 2) Consider more general
locality assumptions. In particular  we argue that Assumption 2 (WL) might be weakened to account
for different (but related) local input-output relations across adjacent parts.

References
[1] Karteek Alahari  Pushmeet Kohli  and Philip H. S. Torr. Reduce  reuse & recycle: Efﬁciently
solving multi-label MRFs. In Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 1–8  2008.

[2] Charalambos D. Aliprantis and Kim Border. Inﬁnite Dimensional Analysis: a Hitchhiker’s

Huide. Springer Science & Business Media  2006.

[3] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathemati-

cal Society  68(3):337–404  1950.

[4] Lalit Bahl  Peter Brown  Peter De Souza  and Robert Mercer. Maximum mutual information esti-
mation of hidden markov model parameters for speech recognition. In International Conference
on Acoustics  Speech  and Signal Processing (ICASSP)  volume 11  pages 49–52  1986.

[5] G. H. Bakir  T. Hofmann  B. Schölkopf  A. J. Smola  B. Taskar  and S. V. N. Vishwanathan.

Predicting Structured Data. MIT Press  2007.

[6] Matthew B. Blaschko and Christoph H. Lampert. Learning to localize objects with structured
output regression. In European Conference on Computer Vision  pages 2–15. Springer  2008.
[7] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[8] Carlo Ciliberto  Lorenzo Rosasco  and Alessandro Rudi. A consistent regularization approach
for structured prediction. Advances in Neural Information Processing Systems 29 (NIPS)  pages
4412–4420  2016.

[9] Carlo Ciliberto  Alessandro Rudi  Lorenzo Rosasco  and Massimiliano Pontil. Consistent multi-
task learning with nonlinear output relations. In Advances in Neural Information Processing
Systems  pages 1983–1993  2017.

[10] Michael Collins. Parameter estimation for statistical parsing models: Theory and practice of
distribution-free methods. In New Developments in Parsing Technology  pages 19–55. Springer 
2004.

9

[11] Corinna Cortes  Vitaly Kuznetsov  and Mehryar Mohri. Ensemble methods for structured

prediction. In International Conference on Machine Learning  pages 1134–1142  2014.

[12] Corinna Cortes  Vitaly Kuznetsov  Mehryar Mohri  and Scott Yang. Structured prediction theory
based on factor graph complexity. In Advances in Neural Information Processing Systems 
pages 2514–2522  2016.

[13] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition  pages 248–255. Ieee  2009.

[14] Aymeric Dieuleveut  Nicolas Flammarion  and Francis Bach. Harder  better  faster  stronger con-
vergence rates for least-squares regression. Journal of Machine Learning Research  18(1):3520–
3570  2017.

[15] Moussab Djerrab  Alexandre Garcia  Maxime Sangnier  and Florence d’Alché Buc. Output

ﬁsher embedding regression. Machine Learning  107(8-10):1229–1256  2018.

[16] John C. Duchi  Lester W. Mackey  and Michael I. Jordan. On the consistency of ranking
algorithms. In Proceedings of the International Conference on Machine Learning (ICML) 
pages 327–334  2010.

[17] Pedro F. Felzenszwalb  Ross B. Girshick  David McAllester  and Deva Ramanan. Object
IEEE Transactions on Pattern

detection with discriminatively trained part-based models.
Analysis and Machine Intelligence  32(9):1627–1645  2010.

[18] Thorsten Joachims  Thomas Hofmann  Yisong Yue  and Chun-Nam Yu. Predicting structured

objects with support vector machines. Communications of the ACM  52(11):97–104  2009.

[19] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image
descriptions. In Proceedings of the Conference on Computer Vision and Pattern Recognition 
pages 3128–3137  2015.

[20] Anna Korba  Alexandre Garcia  and Florence d’Alché Buc. A structured prediction approach
for label ranking. In Advances in Neural Information Processing Systems  pages 8994–9004 
2018.

[21] John Lafferty  Andrew McCallum  and Fernando C. N. Pereira. Conditional random ﬁelds:

Probabilistic models for segmenting and labeling sequence data. 2001.

[22] Christoph H. Lampert  Matthew B. Blaschko  and Thomas Hofmann. Efﬁcient subwindow
search: A branch and bound framework for object localization. IEEE Transactions on Pattern
Analysis and Machine Intelligence  31(12):2129–2142  2009.

[23] Guy Lever  Luca Baldassarre  Sam Patterson  Arthur Gretton  Massimiliano Pontil  and Steffen
Grünewälder. Conditional mean embeddings as regressors. In International Conference on
Machine Learing (ICML)  volume 5  2012.

[24] Giulia Luise  Alessandro Rudi  Massimiliano Pontil  and Carlo Ciliberto. Differential properties
In Advances in Neural

of sinkhorn approximation for learning with wasserstein distance.
Information Processing Systems  pages 5859–5870  2018.

[25] Giulia Luise  Dimitris Stamos  Massimiliano Pontil  and Carlo Ciliberto. Leveraging low-rank
relations between surrogate tasks in structured prediction. International Conference on Machine
Learning (ICML)  2019.

[26] Sean P. Meyn and Richard L. Tweedie. Markov Chains and Stochastic Stability. Springer

Science & Business Media  2012.

[27] Charles A. Micchelli and Massimiliano Pontil. Kernels for multi–task learning. In Advances in

Neural Information Processing Systems  pages 921–928  2004.

[28] Alex Nowak-Vila  Francis Bach  and Alessandro Rudi. Sharp analysis of learning with discrete

losses. AISTATS  2018.

10

[29] Sebastian Nowozin  Christoph H Lampert  et al. Structured learning and prediction in computer

vision. Foundations and Trends in Computer Graphics and Vision  2011.

[30] Anton Osokin  Francis Bach  and Simon Lacoste-Julien. On structured prediction theory with
calibrated convex surrogate losses. In Advances in Neural Information Processing Systems 
pages 302–313  2017.

[31] Nathan D. Ratliff  J. Andrew Bagnell  and Martin A. Zinkevich. Maximum margin planning.
In Proceedings of the International Conference on Machine Learning  pages 729–736. ACM 
2006.

[32] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. Falkon: An optimal large scale kernel

method. In Advances in Neural Information Processing Systems  pages 3891–3901  2017.

[33] Alessandro Rudi  Carlo Ciliberto  GianMaria Marconi  and Lorenzo Rosasco. Manifold
structured prediction. In Advances in Neural Information Processing Systems  pages 5610–5621 
2018.

[34] Rahul Singh  Maneesh Sahani  and Arthur Gretton. Kernel instrumental variable regression.

Advances in Neural Information Processing Systems  2019.

[35] Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their

approximations. Constructive Approximation  26(2):153–172  2007.

[36] Le Song  Kenji Fukumizu  and Arthur Gretton. Kernel embeddings of conditional distributions:
A uniﬁed kernel framework for nonparametric inference in graphical models. IEEE Signal
Processing Magazine  30(4):98–111  2013.

[37] Florian Steinke  Matthias Hein  and Bernhard Schölkopf. Nonparametric regression between

general riemannian manifolds. SIAM Journal on Imaging Sciences  3(3):527–563  2010.

[38] Ingo Steinwart and Andreas Christmann. Support Vector Machines. Information Science and

Statistics. Springer New York  2008.

[39] Kirill Struminsky  Simon Lacoste-Julien  and Anton Osokin. Quantifying learning guarantees
for convex but inconsistent surrogates. In Advances in Neural Information Processing Systems 
pages 669–677  2018.

[40] Charles Sutton and Andrew McCallum. An introduction to conditional random ﬁelds. Founda-

tions and Trends R in Machine Learning  4(4):267–373  2012.

[41] Martin Szummer  Pushmeet Kohli  and Derek Hoiem. Learning CRFs using graph cuts. In

European Conference on Computer Vision  pages 582–595. Springer  2008.

[42] Ben Taskar  Carlos Guestrin  and Daphne Koller. Max-margin Markov networks. In Advances

in Neural Information Processing Systems  pages 25–32  2004.

[43] Ioannis Tsochantaridis  Thorsten Joachims  Thomas Hofmann  and Yasemin Altun. Large
margin methods for structured and interdependent output variables. volume 6  pages 1453–1484 
2005.

[44] Devis Tuia  Jordi Munoz-Mari  Mikhail Kanevski  and Gustavo Camps-Valls. Structured
output svm for remote sensing image classiﬁcation. Journal of Signal Processing Systems 
65(3):301–310  2011.

[45] Andrea Vedaldi and Andrew Zisserman. Structured output regression for detection with partial
truncation. In Advances in Neural Information Processing Systems  pages 1928–1936  2009.

11

,Carlo Ciliberto
Francis Bach
Alessandro Rudi