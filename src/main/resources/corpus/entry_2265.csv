2019,Generalized Matrix Means for Semi-Supervised Learning with Multilayer Graphs,We study the task of semi-supervised learning on multilayer graphs by taking into account both labeled and unlabeled observations together with the information encoded by each individual graph layer. We propose a regularizer based on the generalized matrix mean  which is a one-parameter family of matrix means that includes the arithmetic  geometric and harmonic means as particular cases. We analyze it in expectation under a Multilayer Stochastic Block Model and verify numerically that it outperforms state of the art methods. Moreover  we introduce a matrix-free numerical scheme based on contour integral quadratures and Krylov subspace solvers that scales to large sparse multilayer graphs.,Generalized Matrix Means for Semi-Supervised

Learning with Multilayer Graphs

Pedro Mercado1  Francesco Tudisco2 and Matthias Hein1

1University of Tübingen  Germany
2Gran Sasso Science Institute  Italy

Abstract

We study the task of semi-supervised learning on multilayer graphs by taking into
account both labeled and unlabeled observations together with the information
encoded by each individual graph layer. We propose a regularizer based on the
generalized matrix mean  which is a one-parameter family of matrix means that
includes the arithmetic  geometric and harmonic means as particular cases. We
analyze it in expectation under a Multilayer Stochastic Block Model and verify
numerically that it outperforms state of the art methods. Moreover  we introduce a
matrix-free numerical scheme based on contour integral quadratures and Krylov
subspace solvers that scales to large sparse multilayer graphs.

1

Introduction

The task of graph-based Semi-Supervised Learning (SSL) is to build a classiﬁer that takes into
account both labeled and unlabeled observations  together with the information encoded by a given
graph[4  27]. A common and successful approach is to take a suitable loss function on the labeled
nodes and a regularizer which provides information encoded by the graph [2  15  30  32  35]. Whereas
this task is well studied  traditionally these methods assume that the graph is composed by interactions
of one single kind  i.e. only one graph is available.
For the case where multiple graphs  or equivalently  multiple layers are available  the challenge is to
boost the classiﬁcation performance by merging the information encoded in each graph. The arguably
most popular approach for this task consists of ﬁnding some form of convex combination of graph
matrices  where more informative graphs receive a larger weight [1  13  14  23  28  29  31  33].
Note that a convex combination of graph matrices can be seen as a weighted arithmetic mean of
graph matrices. In the context of multilayer graph clustering  previous studies [19–21] have shown
that weighted arithmetic means are suboptimal under certain benchmark generative graph models 
whereas other matrix means  such as the geometric [20] and harmonic means [19]  are able to discover
clustering structures that the arithmetic means overlook.
In this paper we study the task of semi-supervised learning with multilayer graphs with a novel
regularizer based on the power mean Laplacian. The power mean Laplacian is a one-parameter family
of Laplacian matrix means that includes as special cases the arithmetic  geometric and harmonic mean
of Laplacian matrices.We show that in expectation under a Multilayer Stochastic Block Model  our
approach provably correctly classiﬁes unlabeled nodes in settings where state of the art approaches fail.
In particular  a limit case of our method is provably robust against noise  yielding good classiﬁcation
performance as long as one layer is informative and remaining layers are potentially just noise. We
verify the analysis in expectation with extensive experiments with random graphs  showing that our
approach compares favorably with state of the art methods  yielding a good classiﬁcation performance
on several relevant settings where state of the art approaches fail.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

minimum harmonic mean
name
p → −∞
mp(a  b) min{a  b}

p = −1
a + 1

p

b

2(cid:0) 1

(cid:1)−1

geometric mean

p → 0
√
ab

arithmetic mean maximum
p → ∞
max{a  b}

(a + b)/2

p = 1

Table 1: Particular cases of scalar power means

Moreover  our approach scales to large datasets: even though the computation of the power mean
Laplacian is in general prohibitive for large graphs  we present a matrix-free numerical scheme based
on integral quadratures methods and Krylov subspace solvers which allows us to apply the power
mean Laplacian regularizer to large sparse graphs. Finally  we perform numerical experiments on
real world datasets and verify that our approach is competitive to state of the art approaches.

2 The Power Mean Laplacian

In this section we introduce our multilayer graph regularizer based on the power mean Laplacian.
We deﬁne a multilayer graph G with T layers as the set G = {G(1)  . . .   G(T )}  with each graph
layer deﬁned as G(t) = (V  W (t))  where V = {v1  . . .   vn} is the node set and W (t) ∈ Rn×n
is the
corresponding adjacency matrix  which we assume symmetric and nonnegative. We further denote
sym = I − (D(t))−1/2W (D(t))−1/2  where D(t) is the degree
the layers’ normalized Laplacians as L(t)

diagonal matrix with (D(t))ii =(cid:80)n

+

j=1 W (t)
ij .

The scalar power mean is a one-parameter family of scalar means deﬁned as

(cid:80)T

mp(x1  . . .   xT ) = ( 1
T

i=1 xp

i )1/p

where x1  . . .   xT are nonnegative scalars and p is a real parameter. Particular choices of p yield
speciﬁc means such as the arithmetic  geometric and harmonic means  as illustrated in Table 1.
The Power Mean Laplacian  introduced in [19]  is a matrix extension of the scalar power mean
applied to the Laplacians of a multilayer graph and proposed as a more robust way to blend the
information encoded across the layers. It is deﬁned as

(cid:16) 1

T

(cid:80)T

sym)p(cid:17)1/p

Lp =

i=1(L(i)

where A1/p is the unique positive deﬁnite solution of the matrix equation X p = A. For the case
p ≤ 0 a small diagonal shift ε > 0 is added to each Laplacian  i.e. we replace L(i)
sym + ε  to
ensure that Lp is well deﬁned as suggested in [3]. In what follows all the proofs hold for an arbitrary
shift. Following [19]  we set ε = log10(1 + |p|) + 10−6 for p ≤ 0 in the numerical experiments.

sym with L(i)

3 Multilayer Semi-Supervised Learning with the Power Mean Laplacian

In this paper we consider the following optimization problem for the task of semi-supervised learning
in multilayer graphs: Given k classes r = 1  . . .   k and membership vectors Y (r) ∈ Rn deﬁned by
Y (r)
i = 1 if node vi belongs to class r and Y (r)

i = 0 otherwise  we let
(cid:107)f − Y (r)(cid:107)2 + λf T Lpf .

f (r) = arg min

f∈Rn

(1)

The ﬁnal class assignment for an unlabeled node vi is yi = arg max{f (1)
i }. Note that the
solution f of (1)  for a particular class r  is such that (I + λLp)f = Y (r). Equation (1) has two terms:
the ﬁrst term is a loss function based on the labeled nodes whereas the second term is a regularization
term based on the power mean Laplacian Lp  which accounts for the multilayer graph structure. It is
worth noting that the Local-Global approach of [32] is a particular case of our approach when only
one layer (T = 1) is considered. Moreover  not that when p = 1 we obtain a regularizer term based
on the arithmetic mean of Laplacians L1 = 1
sym. In the following section we analyze our
T
proposed approach (1) under the Multilayer Stochastic Block Model.

(cid:80)T

  . . .   f (k)

i=1 L(i)

i

2

4 Multilayer Stochastic Block Model

in and p(t)

In this section we provide an analysis of semi-supervised learning for multilayer graphs with the
power mean Laplacian as a regularizer under the Multilayer Stochastic Block Model (MSBM). The
MSBM is a generative model for graphs showing certain prescribed clusters/classes structures via
a set of membership parameters p(t)
out  t = 1  . . .   T . These parameters designate the edge
probabilities: given nodes vi and vj the probability of observing an edge between them on layer t is
p(t)
in (resp. p(t)
out)  if vi and vj belong to the same (resp. different) cluster/class. Note that  unlike the
Labeled Stochastic Block Model [11]  the MSBM allows multiple edges between the same pairs of
nodes across the layers. For SSL with one layer under the SBM we refer the reader to [12  22  26].
We present an analysis in expectation. We consider k clusters/classes C1  . . .  Ck of equal size
|C| = n/k. We denote with calligraphic letters the layers of a multilayer graph in expectation
E(G) = {E(G(1)  . . .   E(G(T ))}  i.e. W (t) is the expected adjacency matrix of the tth-layer. We
assume that our multilayer graphs are non-weighted  i.e. edges are zero or one  and hence we have
W (t)
out) for nodes vi  vj belonging to the same (resp. different) cluster/class.
In order to grasp how different methods classify the nodes in multilayer graphs following the MSBM
we analyze two different settings. In the ﬁrst setting (Section 4.1) all layers have the same class
structure and we study the conditions for different regularizers Lp to correctly predict class labels.
We further show that our approach is robust against the presence of noise layers  in the sense that it
achieves a small classiﬁcation error when at least one layer is informative and the remaining layers
are potentially just noise. In this setting we distinguish the case where each class has the same
amount of initial labels and the case where different classes have different number of labels. In the
second setting (Section 4.2) we consider the case where each layer taken alone would lead to a large
classiﬁcation error whereas considering all the layers together can lead to a small classiﬁcation error.

in   (resp. W (t)

ij = p(t)

ij = p(t)

4.1 Complementary Information Layers

A common assumption in multilayer semi-supervised learning is that at least one layer encodes
relevant information in the label prediction task. The next theorem discusses the classiﬁcation error
of the expected power mean Laplacian regularizer in this setting.
Theorem 1. Let E(G) be the expected multilayer graph with T layers following the multilayer SBM
with k classes C1  . . .  Ck of equal size and parameters
. Assume the same number of
labeled nodes are available per class. Then  the solution of (1) yields zero test error if and only if

p(t)
in   p(t)

(cid:17)T

(cid:16)

out

t=1

mp(ρ) < 1 +   

(2)

out)/(p(t)

out < p(t)

in − p(t)

in + (k − 1)p(t)

out) +   and t = 1  . . .   T .

where (ρ)t = 1 − (p(t)
This theorem shows that the power mean Laplacian regularizer allows to correctly classify the nodes
if p is such that condition (2) holds. In order to better understand how this condition changes when p
varies  we analyze in the next corollary the limit cases p → ±∞.
Corollary 1. Let E(G) be an expected multilayer graph as in Theorem 1. Then 
• For p → ∞  the test error is zero if and only if p(t)
in for all t = 1  . . .   T .
• For p→−∞  the test error is zero if and only there exists a t∈{1  . . .   T} such that p(t)
out < p(t)
in .
This corollary implies that the limit case p → ∞ requires that all layers convey information regarding
the clustering/class structure of the multilayer graph  whereas the case p → −∞ requires that at
least one layer encodes clustering/class information  and hence it is clear that conditions for the limit
p → −∞ are less restrictive than the conditions for the limit case p → ∞. The next Corollary shows
that the smaller the power parameter p is  the less restrictive are the conditions to yield a zero test
error.
Corollary 2. Let E(G) be an expected multilayer graph as in Theorem 1. Let p ≤ q. If Lq yields
zero test error  then Lp yields a zero test error.
The previous results show the effectivity of the power mean Laplacian regularizer in expectation.
We now present a numerical evaluation based on Theorem 1 and Corollaries 1 and 2 on random

3

Classiﬁcation Error

(a) L−10

(b) L−1

(c) L0

(d) L1

(e) L10

(f) SMACD

(g) AGML

(h) TLMV

(i) SGMI

(j) TSS

in   p(1)

out  p(2)

in   p(2)

in > p(2)

in − p(1)

in < p(2)
in + p(t)

Figure 1: Average classiﬁcation error under the Stochastic Block Model computed from 100 runs.
Top Row: Particular cases with the power mean Laplacian. Bottom Row: State of the art models.
graphs sampled from the SBM. The corresponding results are presented in Fig. 1 for classiﬁcation
with regularizers L−10  L−1  L0  L1  L10 and λ = 1. We ﬁrst describe the setting we consider: we
generate random multilayer graphs with two layers (T = 2) and two classes (k = 2) each composed
by 100 nodes (|C| = 100). For each parameter conﬁguration (p(1)
out) we generate 10
random multilayer graphs and 10 random samples of labeled nodes  yielding a total of 100 runs per
parameter conﬁguration  and report the average test error. Our goal is to evaluate the classiﬁcation
performance under different SBM parameters and different amounts of labeled nodes. To this end 
we ﬁx the ﬁrst layer G(1) to be informative of the class structure (p(1)
out = 0.08)  i.e. one can
achieve a low classiﬁcation error by taking this layer alone  provided sufﬁciently many labeled nodes
are given. The second layer will go from non-informative (noisy) conﬁgurations (p(2)
out  left
half of x-axis) to informative conﬁgurations (p(2)
out  right half of x-axis)  with p(t)
out = 0.1
for both layers. Moreover  we consider different amounts of labeled nodes: going from 1% to 50%
(y-axis). The corresponding results are presented in Figs. 1a 1b 1c 1d  and 1e.
In general one can expect a low classiﬁcation error when both layers G(1) and G(2) are informative
(right half of x-axis). We can see that this is the case for all power mean Laplacian regularizers here
considered (see top row of Fig. 1). In particular  we can see in Fig. 1e that L10 performs well only
when both layers are informative and completely fails when the second layer is not informative 
regardless of the amount of labeled nodes. On the other side we can see in Fig. 1a that L−10 achieves
in general a low classiﬁcation error  regardless of the conﬁguration of the second layer G(2)  i.e. when
G(1) or G(2) are informative. Moreover  we can see that overall the areas with low classiﬁcation
error (dark blue) increase when the parameter p decreases  verifying the result from Corollary 2. In
the bottom row of Fig. 1 we present the performance of state of the art methods. We can observe
that most of them present a classiﬁcation performance that resembles the one of the power mean
Laplacian regularizer L1. In general their classiﬁcation performance drops when the level of noise
increases  i.e. for non-informative conﬁgurations of the second layer G(2)  and they are outperformed
by the power mean Laplacian regularizer for small values of p.
Unbalanced Class Proportion on Labeled Datasets. In the previous analysis we assumed that
we had the same amount of labeled nodes per class. We consider now the case where the number
of labeled nodes per class is different. This setting was considered in [35]  where the goal was to
overcome unbalanced class proportions in labeled nodes. To this end  they propose a Class Mass
Normalization (CMN) strategy  whose performance was also tested in [34]. In the following result
we show that  provided the ground truth classes have the same size  different amounts of labeled
nodes per class affect the conditions in expectation for zero classiﬁcation error of (1). For simplicity 
we consider here only the case of two classes.

4

 0 0.5-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550-0.100.102550Figure 2: Different
class weighted loss
strategies. Left to right:
uniform loss  weighted
loss  and Class Mass
Normalization.

(a) Uniform loss

(b) Weighted loss

(c) CMN

Theorem 2. Let E(G) be the expected multilayer graph with T layers following the multilayer SBM
with two classes C1 C2 of equal size and parameters
. Assume n1  n2 nodes from
C1 C2 are labeled  respectively. Let λ = 1. Then (1) yields zero test error if

p(t)
in   p(t)

t=1

out

(cid:17)T

(cid:16)
(cid:26) n1

n2

(cid:27)

 

n2
n1

mp(ρ) < min
in + (k − 1)p(t)

out)/(p(t)

in − p(t)

where (ρ)t = 1 − (p(t)
Observe that Theorem 2 provides only a sufﬁcient condition. A necessary and sufﬁcient condition for
zero test error in terms of p  n1 and n2 is given in the supplementary material.
A different objective function can be employed for the case of classes with different number of labels
per class. Let C be the diagonal matrix deﬁned by Cii = n/nr  if node vi has been labeled to belong
to class Cr. Consider the following modiﬁcation of (1)

out) +   and t = 1  . . .   T .

(3)

(4)

(cid:107)f − CY (cid:107)2 + λf T Lpf

arg min

f∈Rn

The next Theorem shows that using (4) in place of (1) allows us to retrieve the same condition of
Theorem 1 for zero test error in expectation in the setting where the number of labeled nodes per
class are not equal.
Theorem 3. Let E(G) be the expected multilayer graph with T layers following the multilayer SBM
k classes C1  . . .  Ck of equal size and parameters
. Let n1  . . .   nk be the number of
labeled nodes per class. Let C ∈ Rn×n be a diagonal matrix with Cii = n/nr for vi ∈ Cr. The
solution to (4) yields a zero test classiﬁcation error if and only if

p(t)
in   p(t)

(cid:17)T

(cid:16)

t=1

out

mp(ρ) < 1 +   

(5)

out)/(p(t)

in − p(t)

in − p(2)

out = 0  with p(t)

in + (k − 1)p(t)

out) +   and t = 1  . . .   T .

where (ρ)t = 1 − (p(t)
In Figs. 2a  2b  and 2c. we present a numerical experiment with random graphs of our analysis
in expectation. We consider the following setting: we generate multilayer graphs with two layers
(T = 2) and two classes (k = 2) each composed by 100 nodes (|C| = 100). We ﬁx p(1)
out = 0.08
and p(2)
out = 0.1 for both layers. We ﬁx the total amount of labeled nodes
to be n1 + n2 = 50 and let n1  n2 = 1  . . . 49. For each setting we generate 10 multilayer graphs
and 10 sets of labeled nodes  yielding a total of 100 runs per setting  and report the average test
classiﬁcation error. In Fig. 2a we can see the performance of the power mean Laplacian regularizer
without modiﬁcations. We can observe how different proportions of labeled nodes per class affect the
performance. In Fig. 2b  we present the performance of the modiﬁed approach (4) and observe that it
yields a better performance against different class label proportions. Finally in Fig. 2c we present
the performance based on Class Mass Normalization 1  where we can see that its effect is slightly
skewed to one class and its overall performance is larger than the proposed approach.

in −p(1)

in + p(t)

4.2

Information-Independent Layers

In the previous section we considered the case where at least one layer had enough information to
correctly estimate node class labels. In this section we now consider the case where single layers

1We follow the authors’ implementation: http://pages.cs.wisc.edu/~jerryzhu/pub/harmonic_function.m

5

0.021490.10.20.30.40.50.60.021490.10.20.30.40.50.021490.40.50.60.70.8Classiﬁcation Error

(a) L−10

(b) L−1

(c) L0

(d) L1

(e) L10

(f) SMACD

(g) AGML

(h) TLMV

(i) SGMI

(j) TSS

Figure 4: Average test error under the SBM.Multilayer graph with 3 layers and 3 classes.Top Row:
Particular cases with the power mean Laplacian. Bottom Row: State of the art models.

taken alone obtain a large classiﬁcation error  whereas when all the layers are taken together it is
possible to obtain a good classiﬁcation performance. For this setting we consider multilayer graphs
with 3 layers (T = 3) and three classes (k = 3) C1 C2 C3  each composed by 100 nodes (|C| = 100)
with the following expected adjacency matrix per layer:

(cid:26)pin 

pout 

W (t)

i j =

vi  vj ∈ Ct or vi  vj ∈ Ct
else

(6)

for t = 1  2  3  i.e. layer G(t) is informative of class Ct but not of the remaining classes  and hence any
classiﬁcation method using one single layer will provide a poor classiﬁcation performance. In Fig. 4
we present numerical experiments: for each parameter setting (pin  pout) we generate 5 multilayer
graphs together with 5 samples of labeled nodes yielding a total of 25 runs per setting  and report
the average test classiﬁcation error. Also in this case we observe that the power mean Laplacian
regularizer does identify the global class structure and that it leverages the information provided by
labeled nodes  particularly for smaller values of p. On the other hand  this is not the case for all other
state of the art methods. In fact  we can see that SGMI and TSS performs similarly to L10 which has
the largest classiﬁcation error. Moreover  we can see that AGML and TLMV perform similarly to
the arithmetic mean of Laplacians L1  which in turn is outperformed by the power mean Laplacian
regularizer L−10. Please see the supplementary material for a more detailed comparison.

5 A Scalable Matrix-free Numerical Method for the System (I + λLp)f = Y

In this section we introduce a matrix-free method for the solution of the system (I + λLp)f = Y
based on contour integrals and Krylov subspace methods. The method exploits the sparsity of
the Laplacians of each layer and is matrix-free  in the sense that it requires only to compute the
sym × vector  without requiring to store the matrices. Thus  when the layers
matrix-vector product L(i)
are sparse  the method scales to large datasets. Observe that this is a critical requirement as Lp is
in general a dense matrix  even for very sparse layers  and thus computing and storing Lp is very
prohibitive for large multilayer graphs. We present a method for negative integer values p < 0 
leaving aside the limit case p → 0 as it requires a particular treatment. The following is a brief
overview of the proposed approach. Further details are available in the supplementary material.
Let A1  . . .   AT be symmetric positive deﬁnite matrices  ϕ : C → C deﬁned by ϕ(z) = z1/p and
Lp = T −1/pϕ(Sp)  where Sp = Ap
T . The proposed method consists of three main steps:

1 + ··· + Ap

6

 0 0.700.050.10255000.050.10255000.050.10255000.050.10255000.050.10255000.050.10255000.050.10255000.050.10255000.050.10255000.050.102550Figure 5: Mean execution time of 10 runs for different meth-
ods. L−1(ours) stands for the power mean Laplacian reg-
ularizer together with our proposed matrix-free contour in-
tegral based method. We generate multilayer graphs with
two layers  each with two classes of same size with param-
eters pin = 0.05 and pin = 0.025 and graphs of of sizes
[0.5  1  2  4  8] × 104. Observe that our matrix free approach
for L−1 (solid blue curve) is competitive to state of the art ap-
proaches as TSS[28]  outperforming AGML[23]  SGMI[13]
and SMACD[9]. For TLMV[33] and SGMI we use our own
implementation.

(cid:110)(cid:80)N

i=1 βi(z2

i I − Sp)−1Y

(cid:111)

 

1. We solve the system (I + λLp)−1Y via a Krylov method (e.g. PCG or GMRES) with convergence
rate O(( κ2−1
κ2 )h/2) [25]  where κ = λmax(Lp)/λmin(Lp). At iteration h  this method projects
the problem onto the Krylov subspace spanned by {Y  λLpY  (λLp)2Y  . . .   (λLp)hY }  and
efﬁciently solve the projected problem.
2. The previous step requires the matrix-vector product LpY = T −1/pϕ(Sp)Y which we compute
by approximating the Cauchy integral form of the function ϕ with the trapezoidal rule in the
complex plane [10]. Taking N suitable contour points and coefﬁcients β0  . . .   βN   we have

ϕN (Sp)Y = β0Sp Im

p Y }. Since Sp =(cid:80)T

(7)
which has geometric convergence [10]: (cid:107)ϕ(Sp)Y − ϕN (Sp)Y (cid:107) = O(e−2π2N/(ln(M/m)+6)) 
where m  M are such that M ≥ λmax(Sp) and m ≤ λmin(Sp).
3. The previous step requires to solve linear systems of the form (zI − Sp)−1Y . We solve each of
these systems via a Krylov subspace method  projecting  at each iteration h  onto the subspace
spanned by {Y  SpY  S2
this problem reduces to comput-
ing |p| linear systems with Ai as coefﬁcient matrix  for i = 1 . . .   T . Provided that A1  . . .   AT
are sparse matrices  this is done efﬁciently using pcg with incomplete Cholesky preconditioners.
Notice that the method allows a high level of parallelism. In fact  the N (resp. p) linear systems
solvers at step 2 (resp. 3) are independent and can be run in parallel. Moreover  note that the main
task of the method is solving linear systems with Laplacian matrices  which can be solved linearly in
the number of edges in the corresponding adjacency matrix. Hence  the proposed approach scales to
large sparse graphs and is highly parallelizable. A time execution analysis is provided in Fig 5  where
we can see that the time execution of our approach is competitive to the state of the art as TSS[28] 
outperforming AGML[23]  SGMI[13] and SMACD[9].

pY  . . .   Sh

−|p|
i

i=1 A

6 Experiments on Real Datasets

In this section we compare the performance of the proposed approach with state of the art methods
on real world datasets. We consider the following datasets: 3-sources [16]  which consists of news
articles that were covered by news sources BBC  Reuters and Guardian; BBC[7] and BBC Sports[8]
news articles  a dataset of Wikipedia articles with ten different classes [24]  the hand written UCI
digits dataset with six different set of features  and citations datasets CiteSeer[17]  Cora[18] and
WebKB(Texas)[5]. For each dataset we build the corresponding layer adjacency matrices by taking the
symmetric k-nearest neighbour graph using as similarity measure the Pearson linear correlation  (i.e.
we take the k neighbours with highest correlation)  and take the unweighted version of it. Datasets
CiteSeer  Cora and WebKB have only two layers  where the ﬁrst one is a ﬁxed precomputed citation
layer  and the second one is the corresponding k-nearest neighbour graph built from document
features.
As baseline methods we consider: TSS [28] which identiﬁes an optimal linear combination of graph
Laplacians  SGMI [13] which performs label propagation by sparse integration  TLMV [33] which is
a weighted arithmetic mean of adjacency matrices  CGL [1] which is a convex combination of the
pseudo inverse Laplacian kernel  AGML [23] which is a parameter-free method for optimal graph
layer weights  ZooBP [6] which is a fast approximation of Belief Propagation  and SMACD [9] which
is a tensor factorization method designed for semi-supervised learning. Finally we set parameters
for TSS to (c = 10  c0 = 0.4)  SMACD (λ = 0.01)2  TLMV (λ = 1)  SGMI (λ1 = 1  λ2 = 10−3)

2this is the default value in the code released by the authors: https://github.com/egujr001/SMACD

7

0.51248104100101102103104Mean time (sec.)3sources

BBC

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

1%
29.8
50.2
91.5
23.9
31.0
29.8
34.4
33.5
28.4
40.9

1%
25.6
79.2
77.8
34.6
33.8
23.9
31.9
29.9
23.8
48.7

1%
28.9
81.8
73.6
25.3
30.8
24.0
36.0
31.3
30.5
57.0

1%
46.0
85.5
75.6
54.7
54.7
38.8
57.3
50.7
43.2
62.0

5% 10% 15% 20% 25%
16.5
21.5
19.8
45.5
91.3
91.1
22.0
26.3
15.3
21.9
23.9
35.0
17.9
26.6
14.6
23.9
20.0
17.9
29.1
14.7

20.8
36.4
91.2
33.9
21.3
33.1
25.4
23.4
21.8
21.9

15.5
23.8
90.7
26.1
15.0
34.8
19.1
15.6
17.2
14.8

20.3
30.6
90.9
33.3
19.8
34.6
24.4
20.1
22.0
19.3

BBCS

5% 10% 15% 20% 25%
5.4
12.6
12.7
51.6
98.3
80.6
5.4
17.4
13.9
6.2
12.2
13.2
12.1
19.6
7.2
15.0
5.1
11.6
22.5
6.1

10.5
34.9
82.4
12.1
11.3
14.1
16.6
13.5
8.7
14.2

7.5
23.4
96.4
7.0
8.8
12.3
15.5
10.6
6.3
9.1

6.4
16.5
98.4
6.0
7.6
13.1
14.8
8.7
5.8
7.8

UCI

5% 10% 15% 20% 25%
12.7
20.4
46.7
64.0
81.9
81.0
17.2
12.0
13.0
21.7
15.6
17.6
48.8
44.4
13.2
23.8
11.9
17.1
33.8
13.4

16.3
54.6
90.0
15.2
17.6
16.6
50.9
18.7
13.8
23.7

13.7
46.7
86.2
12.5
14.1
15.8
50.2
14.4
12.3
15.3

14.4
49.1
90.0
13.2
15.1
15.9
50.4
15.6
12.6
17.6

Cora

5% 10% 15% 20% 25%
20.6
34.1
40.0
70.1
76.7
87.1
16.5
36.0
26.2
38.0
27.7
19.1
38.5
47.7
25.6
38.2
31.8
17.2
22.3
46.3

28.8
56.5
78.7
25.4
32.9
24.1
43.0
33.4
24.5
35.4

22.5
44.2
81.0
18.1
27.6
20.0
40.1
28.2
18.8
25.2

25.8
49.1
78.7
20.7
30.2
21.5
41.8
31.2
21.1
29.4

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

TLMV
CGL

SMACD
AGML
ZooBP
TSS
SGMI

L1
L-1
L-10

1%
29.0
72.5
74.4
60.0
31.1
40.4
37.6
31.3
31.0
51.6

1%
65.7
87.3
85.4
71.3
67.6
87.7
69.3
68.2
59.1
66.9

1%
51.5
89.3
90.7
47.3
63.6
58.5
59.4
56.3
52.4
68.6

1%
58.6
80.4
87.3
56.5
52.0
60.9
44.9
58.5
49.9
52.3

5% 10% 15% 20% 25%
8.8
19.3
17.1
52.3
72.4
73.5
9.5
34.2
9.1
20.1
26.1
19.7
19.3
28.9
8.9
22.8
8.7
17.0
26.9
9.5

9.3
22.0
72.5
11.0
10.0
19.8
20.7
10.2
9.2
10.3

13.2
36.1
72.8
18.6
15.0
20.9
24.9
17.4
11.5
16.6

11.1
27.4
72.6
13.1
12.2
20.1
22.8
13.5
10.5
12.8

Wikipedia

5% 10% 15% 20% 25%
39.2
56.8
83.0
83.0
90.0
85.6
37.3
66.6
58.0
39.8
81.4
84.7
82.8
84.8
42.3
61.1
34.1
52.3
57.2
34.9

40.8
83.0
86.8
38.4
41.2
82.3
83.2
44.1
35.1
36.3

46.4
82.5
85.4
48.1
47.0
83.3
84.5
53.6
40.2
43.2

43.1
82.2
85.3
42.1
43.8
81.9
83.8
48.3
36.3
38.7

Citeseer

5% 10% 15% 20% 25%
30.3
39.4
40.9
71.8
68.9
90.4
32.3
27.0
32.2
41.9
38.4
49.5
39.2
46.8
34.7
44.1
29.5
39.0
54.6
37.2

31.6
44.5
66.8
27.5
33.8
39.8
40.5
36.1
30.9
39.7

36.5
58.0
67.0
29.6
38.7
45.9
44.0
41.2
35.6
48.5

33.7
49.8
65.5
28.2
35.8
42.1
42.3
38.5
32.6
43.0

WebKB

5% 10% 15% 20% 25%
48.2
49.4
89.2
82.4
87.2
87.8
46.8
50.3
33.5
45.0
48.7
51.0
39.7
52.5
44.4
49.0
45.5
40.3
39.5
41.9

47.6
82.7
87.8
47.6
36.4
49.2
40.3
44.5
39.9
36.8

47.2
86.9
87.4
44.7
38.5
47.3
34.9
44.3
39.5
38.1

45.6
84.4
87.2
46.8
38.7
50.5
41.9
44.8
40.7
38.0

Table 2: Experiments in real datasets. Notation: best performances are marked with bold fonts and
gray background and second best performances with only gray background.

and λ = 0.1 for L1 and λ = 10 for L−1 and L−10. We do not perform cross validation in our
experimental setting due to the large execution time in some of the methods here considered. Hence
we ﬁx the parameters for each method in all experiments.
We ﬁx nearest neighbourhood size to k = 10 and generate 10 samples of labeled nodes  where the
percentage of labeled nodes per class is in the range {1%  5%  10%  15%  20%  25%}. The average
test errors are presented in table 2  where the best (resp. second best ) performances are marked
with bold fonts and gray background (resp. with only gray background). We can see that the ﬁrst and
second best positions are in general taken by the power mean Laplacian regularizers L1  L−1  L−10 
being clear for all datasets except with 3-sources. Moreover we can see that in 77% of all cases L−1
presents either the best or the second best performance  further verifying that our proposed approach
based on the power mean Laplacian for semi-supervised learning in multilayer graph is a competitive
alternative to state of the art methods3.

3Communications with the authors of [9] could not clarify the bad performance of SMACD.

8

Acknowledgement P.M and M.H are supported by the DFG Cluster of Excellence “Machine Learning
– New Perspectives for Science”  EXC 2064/1  project number 390727645

References
[1] A. Argyriou  M. Herbster  and M. Pontil. Combining graph Laplacians for semi–supervised

learning. In NeurIPS  2006.

[2] M. Belkin  I. Matveeva  and P. Niyogi. Regularization and semi-supervised learning on large

graphs. In COLT  2004.

[3] K. V. Bhagwat and R. Subramanian. Inequalities between means of positive operators. Mathe-

matical Proceedings of the Cambridge Philosophical Society  83(3):393–401  1978.

[4] O. Chapelle  B. Schölkopf  and A. Zien. Semi-Supervised Learning. The MIT Press  2010.

[5] M. Craven  D. DiPasquo  D. Freitag  A. McCallum  T. Mitchell  K. Nigam  and S. Slattery.

Learning to extract symbolic knowledge from the world wide web. In AAAI  2011.

[6] D. Eswaran  S. Günnemann  C. Faloutsos  D. Makhija  and M. Kumar. Zoobp: Belief propaga-

tion for heterogeneous networks. In VLDB  2017.

[7] D. Greene and P. Cunningham. Producing accurate interpretable clusters from high-dimensional

data. In PKDD  2005.

[8] D. Greene and P. Cunningham. A matrix factorization approach for integrating multiple data

views. In ECML PKDD  2009.

[9] E. Gujral and E. E. Papalexakis. SMACD: Semi-supervised multi-aspect community detection.

In SDM  2018.

[10] N. Hale  N. J. Higham  and L. N. Trefethen. Computing Aα  log(A)  and related matrix
functions by contour integrals. SIAM Journal on Numerical Analysis  46(5):2505–2523  2008.

[11] S. Heimlicher  M. Lelarge  and L. Massoulié. Community detection in the labelled stochastic

block model. arXiv:1209.2910  2012.

[12] V. Kanade  E. Mossel  and T. Schramm. Global and local information in clustering labeled

block models. IEEE Transactions on Information Theory  62(10):5906–5917  2016.

[13] M. Karasuyama and H. Mamitsuka. Multiple graph label propagation by sparse integration.

IEEE Transactions on Neural Networks and Learning Systems  24(12):1999–2012  2013.

[14] T. Kato  H. Kashima  and M. Sugiyama. Robust label propagation on multiple networks.

Transactions on Neural Networks  20(1):35–44  Jan. 2009.

[15] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

In ICLR  2017.

[16] J. Liu  C. Wang  J. Gao  and J. Han. Multi-view clustering via joint nonnegative matrix

factorization. In SDM  2013.

[17] Q. Lu and L. Getoor. Link-based classiﬁcation. In ICML  2003.

[18] A. K. McCallum  K. Nigam  J. Rennie  and K. Seymore. Automating the construction of internet

portals with machine learning. Information Retrieval  3(2):127–163  2000.

[19] P. Mercado  A. Gautier  F. Tudisco  and M. Hein. The power mean Laplacian for multilayer

graph clustering. In AISTATS  2018.

[20] P. Mercado  F. Tudisco  and M. Hein. Clustering signed networks with the geometric mean of

Laplacians. In NeurIPS. 2016.

[21] P. Mercado  F. Tudisco  and M. Hein. Spectral clustering of signed graphs via matrix power

means. In ICML  2019.

9

[22] E. Mossel and J. Xu. Local algorithms for block models with side information. In ITCS  2016.

[23] F. Nie  J. Li  and X. Li. Parameter-free auto-weighted multiple graph learning: A framework

for multiview clustering and semi-supervised classiﬁcation. In IJCAI  2016.

[24] N. Rasiwasia  J. Costa Pereira  E. Coviello  G. Doyle  G. R. Lanckriet  R. Levy  and N. Vascon-

celos. A new approach to cross-modal multimedia retrieval. In ACM Multimedia  2010.

[25] Y. Saad and M. H. Schultz. GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientiﬁc and Statistical Computing  7(3):856–
869  1986.

[26] A. Saade  F. Krzakala  M. Lelarge  and L. Zdeborová. Fast randomized semi-supervised

clustering. Journal of Physics: Conference Series  1036:012015  2018.

[27] A. Subramanya and P. P. Talukdar. Graph-Based Semi-Supervised Learning. Morgan &

Claypool Publishers  2014.

[28] K. Tsuda  H. Shin  and B. Schölkopf. Fast protein classiﬁcation with multiple networks.

Bioinformatics  21(2):59–65  2005.

[29] K. Viswanathan  S. Sachdeva  A. Tomkins  and S. Ravi. Improved semi-supervised learning

with multiple graphs. In AISTATS  2019.

[30] Z. Yang  W. W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with graph

embeddings. In ICML  2016.

[31] J. Ye and L. Akoglu. Robust semi-supervised learning on multiple networks with noise. In

PKDD  2018.

[32] D. Zhou  O. Bousquet  T. N. Lal  J. Weston  and B. Schölkopf. Learning with local and global

consistency. In NeurIPS  2003.

[33] D. Zhou and C. J. Burges. Spectral clustering and transductive learning with multiple views. In

ICML  2007.

[34] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation.

Technical report  2002.

[35] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and

harmonic functions. In ICML  2003.

10

,Pedro Mercado
Francesco Tudisco
Matthias Hein