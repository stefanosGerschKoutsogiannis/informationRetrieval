2013,B-test: A Non-parametric  Low Variance Kernel Two-sample Test,We propose a family of maximum mean discrepancy (MMD) kernel two-sample tests that have low sample complexity and are consistent. The test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time. Our family of tests  which we denote as B-tests  is both computationally and statistically efficient  combining favorable properties of previously proposed MMD two-sample tests.  It does so by better leveraging samples to produce low variance estimates in the finite sample case  while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample U-statistics. The B-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on Type I error. Finally  recent results of combining multiple kernels transfer seamlessly to our hypothesis test  allowing a further increase in discriminative power and decrease in sample complexity.,B-tests: Low Variance Kernel Two-Sample Tests

Wojciech Zaremba

Center for Visual Computing

´Ecole Centrale Paris

Chˆatenay-Malabry  France

Matthew Blaschko

´Equipe GALEN

Inria Saclay

Chˆatenay-Malabry  France

Arthur Gretton

Gatsby Unit

University College London

United Kingdom

{woj.zaremba arthur.gretton}@gmail.com  matthew.blaschko@inria.fr

Abstract

A family of maximum mean discrepancy (MMD) kernel two-sample tests is intro-
duced. Members of the test family are called Block-tests or B-tests  since the test
statistic is an average over MMDs computed on subsets of the samples. The choice
of block size allows control over the tradeoff between test power and computation
time. In this respect  the B-test family combines favorable properties of previ-
ously proposed MMD two-sample tests: B-tests are more powerful than a linear
time test where blocks are just pairs of samples  yet they are more computation-
ally efﬁcient than a quadratic time test where a single large block incorporating all
the samples is used to compute a U-statistic. A further important advantage of the
B-tests is their asymptotically Normal null distribution: this is by contrast with
the U-statistic  which is degenerate under the null hypothesis  and for which esti-
mates of the null distribution are computationally demanding. Recent results on
kernel selection for hypothesis testing transfer seamlessly to the B-tests  yielding
a means to optimize test power via kernel choice.

Introduction

i=1 where xi ∼ P i.i.d.  and {yi}n

1
Given two samples {xi}n
i=1  where yi ∼ Q i.i.d  the two sample
problem consists in testing whether to accept or reject the null hypothesis H0 that P = Q  vs the
alternative hypothesis HA that P and Q are different. This problem has recently been addressed
using measures of similarity computed in a reproducing kernel Hilbert space (RKHS)  which apply
in very general settings where P and Q might be distributions over high dimensional data or struc-
tured objects. Kernel test statistics include the maximum mean discrepancy [10  6] (of which the
energy distance is an example [18  2  22])  which is the distance between expected features of P and
Q in the RKHS; the kernel Fisher discriminant [12]  which is the distance between expected feature
maps normalized by the feature space covariance; and density ratio estimates [24]. When used in
testing  it is necessary to determine whether the empirical estimate of the relevant similarity mea-
sure is sufﬁciently large as to give the hypothesis P = Q low probability; i.e.  below a user-deﬁned
threshold α  denoted the test level. The test power denotes the probability of correctly rejecting the
null hypothesis  given that P (cid:54)= Q.
The minimum variance unbiased estimator MMDu of the maximum mean discrepancy  on the basis
of n samples observed from each of P and Q  is a U-statistic  costing O(n2) to compute. Unfor-
tunately  this statistic is degenerate under the null hypothesis H0 that P = Q  and its asymptotic
distribution takes the form of an inﬁnite weighted sum of independent χ2 variables (it is asymptot-
ically Gaussian under the alternative hypothesis HA that P (cid:54)= Q). Two methods for empirically
estimating the null distribution in a consistent way have been proposed: the bootstrap [10]  and a
method requiring an eigendecomposition of the kernel matrices computed on the merged samples
from P and Q [7]. Unfortunately  both procedures are computationally demanding: the former costs
O(n2)  with a large constant (the MMD must be computed repeatedly over random assignments
of the pooled data); the latter costs O(n3)  but with a smaller constant  hence can in practice be

1

faster than the bootstrap. Another approach is to approximate the null distribution by a member
of a simpler parametric family (for instance  a Pearson curve approximation)  however this has no
consistency guarantees.
More recently  an O(n) unbiased estimate MMDl of the maximum mean discrepancy has been pro-
posed [10  Section 6]  which is simply a running average over independent pairs of samples from P
and Q. While this has much greater variance than the U-statistic  it also has a simpler null distribu-
tion: being an average over i.i.d. terms  the central limit theorem gives an asymptotically Normal
distribution  under both H0 and HA. It is shown in [9] that this simple asymptotic distribution makes
it easy to optimize the Hodges and Lehmann asymptotic relative efﬁciency [19] over the family of
kernels that deﬁne the statistic: in other words  to choose the kernel which gives the lowest Type II
error (probability of wrongly accepting H0) for a given Type I error (probability of wrongly reject-
ing H0). Kernel selection for the U-statistic is a much harder question due to the complex form of
the null distribution  and remains an open problem.
It appears that MMDu and MMDl fall at two extremes of a spectrum: the former has the lowest
variance of any n-sample estimator  and should be used in limited data regimes; the latter is the
estimator requiring the least computation while still looking at each of the samples  and usually
achieves better Type II error than MMDu at a given computational cost  albeit by looking at much
more data (the “limited time  unlimited data” scenario). A major reason MMDl is faster is that its
null distribution is straightforward to compute  since it is Gaussian and its variance can be calculated
at the same cost as the test statistic. A reasonable next step would be to ﬁnd a compromise between
these two extremes: to construct a statistic with a lower variance than MMDl  while retaining an
asymptotically Gaussian null distribution (hence remaining faster than tests based on MMDu). We
study a family of such test statistics  where we split the data into blocks of size B  compute the
quadratic-time MMDu on each block  and then average the resulting statistics. We call the resulting
tests B-tests. As long as we choose the size B of blocks such that n/B → ∞  we are still guaranteed
asymptotic Normality by the central limit theorem  and the null distribution can be computed at the
same cost as the test statistic. For a given sample size n  however  the power of the test can increase
dramatically over the MMDl test  even for moderate block sizes B  making much better use of the
available data with only a small increase in computation.
The block averaging scheme was originally proposed in [13]  as an instance of a two-stage U-
statistic  to be applied when the degree of degeneracy of the U-statistic is indeterminate. Differences
with respect to our method are that Ho and Shieh compute the block statistics by sampling with
replacement [13  (b) p. 863]  and propose to obtain the variance of the test statistic via Monte
Carlo  jackknife  or bootstrap techniques  whereas we use closed form expressions. Ho and Shieh
further suggest an alternative two-stage U-statistic in the event that the degree of degeneracy is
known; we return to this point in the discussion. While we conﬁne ourselves to the MMD in this
paper  we emphasize that the block approach applies to a much broader variety of test situations
where the null distribution cannot easily be computed  including the energy distance and distance
covariance [18  2  22] and Fisher statistic [12] in the case of two-sample testing  and the Hilbert-
Schmidt Independence Criterion [8] and distance covariance [23] for independence testing. Finally 
the kernel learning approach of [9] applies straightforwardly  allowing us to maximize test power
over a given kernel family. Code is available at http://github.com/wojzaremba/btest.
2 Theory
In this section we describe the mathematical foundations of the B-test. We begin with a brief review
of kernel methods  and of the maximum mean discrepancy. We then present our block-based average
MMD statistic  and derive its distribution under the H0 (P = Q) and HA (P (cid:54)= Q) hypotheses. The
central idea employed in the construction of the B-test is to generate a low variance MMD estimate
by averaging multiple low variance kernel statistics computed over blocks of samples. We show
simple sufﬁcient conditions on the block size for consistency of the estimator. Furthermore  we
analyze the properties of the ﬁnite sample estimate  and propose a consistent strategy for setting the
block size as a function of the number of samples.

2.1 Deﬁnition and asymptotics of the block-MMD
Let Fk be an RKHS deﬁned on a topological space X with reproducing kernel k  and P a Borel
probability measure on X . The mean embedding of P in Fk  written µk(p) ∈ Fk is deﬁned such

2

(b) B = 250

(a) B = 2. This setting corresponds to the MMDl
statistic [10].
Figure 1: Empirical distributions under H0 and HA for different regimes of B for the music experiment
(Section 3.2). In both plots  the number of samples is ﬁxed at 500. As we vary B  we trade off the quality of the
ﬁnite sample Gaussian approximation to the null distribution  as in Theorem 2.3  with the variances of the H0
and HA distributions  as outlined in Section 2.1. In (b) the distribution under H0 does not resemble a Gaussian
(it does not pass a level 0.05 Kolmogorov-Smirnov (KS) normality test [16  20])  and a Gaussian approximation
results in a conservative test threshold (vertical green line). The remaining empirical distributions all pass a KS
normality test.

for all f ∈ Fk  and exists for all Borel probability measures when
that Ex∼pf (x) = (cid:104)f  µk(p)(cid:105)Fk
k is bounded and continuous [3  10]. The maximum mean discrepancy (MMD) between a Borel
probability measure P and a second Borel probability measure Q is the squared RKHS distance
between their respective mean embeddings 
ηk(P  Q) = (cid:107)µk(P ) − µk(Q)(cid:107)2Fk

= Exx(cid:48)k(x  x(cid:48)) + Eyy(cid:48)k(y  y(cid:48)) − 2Exyk(x  y) 

where x(cid:48) denotes an independent copy of x [11]. Introducing the notation z = (x  y)  we write

(1)

ηk(P  Q) = Ezz(cid:48)hk(z  z(cid:48)) 

h(z  z(cid:48)) = k(x  x(cid:48)) + k(y  y(cid:48)) − k(x  y(cid:48)) − k(x(cid:48)  y).

(2)
When the kernel k is characteristic  then ηk (P  Q) = 0 iff P = Q [21]. Clearly  the minimum
variance unbiased estimate MMDu of ηk(P  Q) is a U-statistic.
By analogy with MMDu  we make use of averages of h(x  y  x(cid:48)  y(cid:48)) to construct our two-sample
test. We denote by ˆηk(i) the ith empirical estimate MMDu based on a subsample of size B  where
1 ≤ i ≤ n
B (for notational purposes  we will index samples as though they are presented in a random
ﬁxed order). More precisely 

ˆηk(i) =

1

B(B − 1)

h(za  zb).

(3)

The B-test statistic is an MMD estimate obtained by averaging the ˆηk(i). Each ˆηk(i) under H0
converges to an inﬁnite sum of weighted χ2 variables [7]. Although setting B = n would lead to the
lowest variance estimate of the MMD  computing sound thresholds for a given p-value is expensive 
involving repeated bootstrap sampling [5  14]  or computing the eigenvalues of a Gram matrix [7].
are i.i.d. variables  and averaging them allows us to apply
In contrast  we note that ˆηk(i)i=1 ...  n
the central limit theorem in order to estimate p-values from a normal distribution. We denote the
average of the ˆηk(i) by ˆηk 

B

iB(cid:88)

iB(cid:88)

a=(i−1)B+1

b=(i−1)B+1 b(cid:54)=a

B(cid:88)

n

i=1

ˆηk =

B
n

ˆηk(i).

(4)

We would like to apply the central limit theorem to variables ˆηk(i)i=1 ...  n
. It remains for us to
derive the distribution of ˆηk under H0 and under HA. We rely on the result from [11  Theorem 8]
for HA. According to our notation  for every i 

B

3

−0.05−0.04−0.03−0.02−0.0100.010.020.030.040.05050100150200250 HA histogramH0 histogramapproximated 5% quantile of H0−4−20246810x 10−3050100150200250 HA histogramH0 histogramapproximated 5% quantile of H0Theorem 2.1 Assume 0 < E(h2) < ∞  then under HA  ˆηk converges in distribution to a Gaussian
according to

u = 4(cid:0)Ez[(Ez(cid:48)h(z  z(cid:48)))2 − Ez z(cid:48)(h(z  z(cid:48)))]2(cid:1).

2 (ˆηk(i) − MMD2) D→ N (0  σ2
u) 

B

1

where σ2

This in turn implies that

For an average of {ˆηk(i)}i=1 ...  n

B

D→ N(cid:16)

ˆηk(i) D→ N (MMD2  σ2
  the central limit theorem implies that under HA 

uB−1).

= N(cid:0)MMD2  σ2

un−1(cid:1) .

−1(cid:17)

ˆηk

MMD2  σ2

(7)
This result shows that the distribution of HA is asymptotically independent of the block size  B.
Turning to the null hypothesis  [11  Theorem 8] additionally implies that under H0 for every i 
Theorem 2.2

u (Bn/B)

(5)

(6)

∞(cid:88)

l=1

B ˆηk(i) D→

λl[z2

l − 2] 

(8)

(cid:90)

where zl ∼ N (0  2)2 i.i.d  λl are the solutions to the eigenvalue equation

(9)
and ¯k(xi  xj) := k(xi  xj)− Exk(xi  x)− Exk(x  xj) + Ex x(cid:48)k(x  x(cid:48)) is the centered RKHS kernel.

¯k(x  x(cid:48))ψl(x)dp(x) = λlψl(x(cid:48)) 

X

As a consequence  under H0  ˆηk(i) has expected variance 2B−2(cid:80)∞
= N(cid:0)0  C(nB)−1(cid:1)

variance by CB−2. The central limit theorem implies that under H0 

0  C(cid:0)B2n/B(cid:1)−1(cid:17)

D→ N(cid:16)

ˆηk

l=1 λ2. We will denote this

(10)

The asymptotic distributions for ˆηk under H0 and HA are Gaussian  and consequently it is easy
to calculate the distribution quantiles and test thresholds. Asymptotically  it is always beneﬁcial to
increase B  as the distributions for η under H0 and HA will be better separated. For consistency  it
is sufﬁcient to ensure that n/B → ∞.
A related strategy of averaging over data blocks to deal with large sample sizes has recently been
developed in [15]  with the goal of efﬁciently computing bootstrapped estimates of statistics of
interest (e.g. quantiles or biases). Brieﬂy  the approach splits the data (of size n) into s subsamples
each of size B  computes an estimate of the n-fold bootstrap on each block  and averages these
estimates. The difference with respect to our approach is that we use the asymptotic distribution
of the average over block statistics to determine a threshold for a hypothesis test  whereas [15] is
concerned with proving the consistency of a statistic obtained by averaging over bootstrap estimates
on blocks.

2.2 Convergence of Moments

In this section  we analyze the convergence of the moments of the B-test statistic  and comment on
potential sources of bias.
The central limit theorem implies that the empirical mean of {ˆηk(i)}i=1 ...  n
converges to E(ˆηk(i)).
converges to E(ˆηk(i))2−E(ˆηk(i)2). Finally  all
Moreover it states that the variance {ˆηk(i)}i=1 ...  n
remaining moments tend to zero  where the rate of convergence for the jth moment is of the order
[1]. This indicates that the skewness dominates the difference of the distribution from a

(cid:1) j+1

(cid:0) n

B

B

2

B
Gaussian.

4

Under both H0 and HA  thresholds computed from normal distribution tables are asymptotically un-
biased. For ﬁnite samples sizes  however  the bias under H0 can be more severe. From Equation (8)
we have that under H0  the summands  ˆηk(i)  converge in distribution to inﬁnite weighted sums of
χ2 distributions. Every unweighted term of this inﬁnite sum has distribution N (0  2)2  which has
ﬁnite skewness equal to 8. The skewness for the entire sum is ﬁnite and positive 

C =

(11)
as λl ≥ 0 for all l due to the positive deﬁniteness of the kernel k. The skew for the mean of the
ˆηk(i) converges to 0 and is positively biased. At smaller sample sizes  test thresholds obtained from
the standard Normal table may therefore be inaccurate  as they do not account for this skew. In our
experiments  this bias caused the tests to be overly conservative  with lower Type I error than the
design level required (Figures 2 and 5).

8λ3
l  

l=1

∞(cid:88)

2.3 Finite Sample Case

In the ﬁnite sample case  we apply the Berry-Ess´een theorem  which gives conservative bounds on
the (cid:96)∞ convergence of a series of ﬁnite sample random variables to a Gaussian distribution [4].
Theorem 2.3 Let X1  X2  . . .   Xn be i.i.d. variables. E(X1) = 0  E(X 2
E(|X1|3) = ρ < ∞. Let Fn be a cumulative distribution of
normal distribution. Then for every x 

1 ) = σ2 > 0  and
√
i=1 Xi
nσ   and let Φ denote the standard

(cid:80)n

|Fn(x) − Φ(x)| ≤ Cρσ−3n−1/2 

(12)

where C < 1.

O(1)
O(B−1)
3
2
n ).

n
B

This result allows us to ensure fast point-wise convergence of the B-test. We have that ρ(ˆηk) =
O(1)  i.e.  it is dependent only on the underlying distributions of the samples and not on the sample
size. The number of i.i.d. samples is nB−1. Based on Theorem 2.3  the point-wise error can be
n ) under HA. Under H0  the error can be bounded by
upper bounded by

= O( B2√

√

√

= O( B3.5√

n
B

O(1)
O(B−2)
3
2
While the asymptotic results indicate that convergence to an optimal predictor is fastest for larger
B  the ﬁnite sample results support decreasing the size of B in order to have a sufﬁcient number
of samples for application of the central limit theorem. As long as B → ∞ and n
B → ∞  the
assumptions of the B-test are fulﬁlled.
By varying B  we make a fundamental tradeoff in the construction of our two sample test. When B
is small  we have many samples  hence the null distribution is close to the asymptotic limit provided
by the central limit theorem  and the Type I error is estimated accurately. The disadvantage of a
small B is a lower test power for a given sample size. Conversely  if we increase B  we will have
a lower variance empirical distribution for H0  hence higher test power  but we may have a poor
estimate of the number of Type I errors (Figure 1). A sensible family of heuristics therefore is to set
(13)
for some 0 < γ < 1  where we round to the nearest integer. In this setting the number of samples
available for application of the central limit theorem will be [n(1−γ)]. For given γ computational

complexity of the B-test is O(cid:0)n1+γ(cid:1). We note that any value of γ ∈ (0  1) yields a consistent
O(cid:0)n1.5(cid:1): we emphasize that this is a heuristic  and just one choice that fulﬁls our assumptions.

2 in the experimental results section  with resulting complexity

estimator. We have chosen γ = 1

B = [nγ]

3 Experiments

We have conducted experiments on challenging synthetic and real datasets in order to empirically
measure (i) sample complexity  (ii) computation time  and (iii) Type I / Type II errors. We evaluate
B-test performance in comparison to the MMDl and MMDu estimators  where for the latter we
compare across different strategies for null distribution quantile estimation.

5

Method

Kernel parameters

Additional
parameters

Minimum number

of samples

B-test

Pearson curves

Gamma approximation
Gram matrix spectrum

Bootstrap

Pearson curves

Gamma approximation
Gram matrix spectrum

Bootstrap

B = 2
√
B = 8
B =
n
any B
B = 2
B = 8

B =(cid:112) n

2

B = n

σ = 1

σ = median

multiple kernels

σ = 1

σ = median

26400
3850
886

> 60000
37000
5400
1700
186
183
186
190

> 60000  or 2h

per iteration

timeout

Computation

time (s)
0.0012
0.0039
0.0572

0.0700
0.1295
0.8332
387.4649
0.2667
407.3447
129.4094

Consistent

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
×
×
(cid:88)
(cid:88)
×
×
(cid:88)
(cid:88)

Table 1: Sample complexity for tests on the distributions described in Figure 3. The fourth column indicates
the minimum number of samples necessary to achieve Type I and Type II errors of 5%. The ﬁfth column is the
computation time required for 2000 samples  and is not presented for settings that have unsatisfactory sample
complexity.

(a)

(b)

(c)

Figure 2: Type I errors on the distributions shown in Figure 3 for α = 5%: (a) MMD  single kernel  σ = 1  (b)
MMD  single kernel  σ set to the median pairwise distance  and (c) MMD  non-negative linear combination of
multiple kernels. The experiment was repeated 30000 times. Error bars are not visible at this scale.
3.1 Synthetic data
Following previous work on kernel hypothesis testing [9]  our synthetic distributions are 5 × 5 grids
of 2D Gaussians. We specify two distributions  P and Q. For distribution P each Gaussian has
identity covariance matrix  while for distribution Q the covariance is non-spherical. Samples drawn
from P and Q are presented in Figure 3. These distributions have proved to be very challenging for
existing non-parametric two-sample tests [9].
We employed three different kernel selection strategies
in the hypothesis test. First  we used a Gaussian kernel
with σ = 1  which approximately matches the scale of
the variance of each Gaussian in mixture P . While this
is a somewhat arbitrary default choice  we selected it as
it performs well in practice (given the lengthscale of the
data)  and we treat it as a baseline. Next  we set σ equal
to the median pairwise distance over the training data 
which is a standard way to choose the Gaussian kernel
bandwidth [17]  although it is likewise arbitrary in this
context. Finally  we applied a kernel learning strategy  in
which the kernel was optimized to maximize the test power for the alternative P (cid:54)= Q [9]. This
approach returned a non-negative linear combination combination of base kernels  where half the
data were used in learning the kernel weights (these data were excluded from the testing phase).
The base kernels in our experiments were chosen to be Gaussian  with bandwidths in the set σ ∈
{2−15  2−14  . . .   210}. Testing was conducted using the remaining half of the data.

Figure 3: Synthetic data distributions P and
Q. Samples belonging to these classes are
difﬁcult to distinguish.

(b) Distribution Q

(a) Distribution P

6

24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=√n24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=√n24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=pn2Figure 4: Synthetic experiment: number of Type II er-
rors vs B  given a ﬁxed probability α of Type I er-
rors. As B grows  the Type II error drops quickly when
the kernel is appropriately chosen. The kernel selec-
tion method is described in [9]  and closely approx-
imates the baseline performance of the well-informed
user choice of σ = 1.

For comparison with the quadratic time U-
statistic MMDu [7  10]  we evaluated four
null distribution estimates: (i) Pearson curves 
(ii) gamma approximation  (iii) Gram matrix
spectrum  and (iv) bootstrap. For methods us-
ing Pearson curves and the Gram matrix spec-
trum  we drew 500 samples from the null distri-
bution estimates to obtain the 1 − α quantiles 
for a test of level α. For the bootstrap  we ﬁxed
the number of shufﬂes to 1000. We note that
Pearson curves and the gamma approximation
are not statistically consistent. We considered
only the setting with σ = 1 and σ set to the
median pairwise distance  as kernel selection is
not yet solved for tests using MMDu [9].
In the ﬁrst experiment we set the Type I error to
be 5%  and we recorded the Type II error. We
conducted these experiments on 2000 samples
over 1000 repetitions  with varying block size 
B. Figure 4 presents results for different kernel
choice strategies  as a function of B. The me-
dian heuristic performs extremely poorly in this
experiment. As discussed in [9  Section 5]  the reason for this failure is that the lengthscale of the
difference between the distributions P and Q differs from the lengthscale of the main data variation
as captured by the median  which gives too broad a kernel for the data.
In the second experiment  our aim was to compare the empirical sample complexity of the various
methods. We again ﬁxed the same Type I error for all methods  but this time we also ﬁxed a Type
II error of 5%  increasing the number of samples until the latter error rate was achieved. Column
four of Table 1 shows the number of samples required in each setting to achieve these error rates.
We additionally compared the computational efﬁciency of the various methods. The computation
time for each method with a ﬁxed sample size of 2000 is presented in column ﬁve of Table 1. All
experiments were run on a single 2.4 GHz core.
Finally  we evaluated the empirical Type I error for α = 5% and increasing B. Figure 2 displays the
empirical Type I error  where we note the location of the γ = 0.5 heuristic in Equation (13). For the
user-chosen kernel (σ = 1  Figure 2(a))  the number of Type I errors closely matches the targeted
test level. When median heuristic is used  however  the test is overly conservative  and makes fewer
Type I errors than required (Figure 2(b)). This indicates that for this choice of σ  we are not in the
asymptotic regime  and our Gaussian null distribution approximation is inaccurate. Kernel selection
via the strategy of [9] alleviates this problem (Figure 2(c)). This setting coincides with a block size
substantially larger than 2 (MMDl)  and therefore achieves lower Type II errors while retaining the
targeted Type I error.
3.2 Musical experiments
In this set of experiments  two amplitude modulated Rammstein songs were compared (Sehnsucht
vs. Engel  from the album Sehnsucht). Following the experimental setting in [9  Section 5]  samples
from P and Q were extracts from AM signals of time duration 8.3 × 10−3 seconds in the original
audio. Feature extraction was identical to [9]  except that the amplitude scaling parameter was set

32. Table 2 summarizes the empirical Type I and Type II errors over 1000 repetitions  and the
average computation times. Figure 5 shows the average number of Type I errors as a function of
B: in this case  all kernel selection strategies result in conservative tests (lower Type I error than
required)  indicating that more samples are needed to reach the asymptotic regime. Figure 1 shows
the empirical H0 and HA distributions for different B.
4 Discussion
We have presented experimental results both on a difﬁcult synthetic problem  and on real-world data
from amplitude modulated audio recordings. The results show that the B-test has a much better

to 0.3 instead of 0.5. As the feature vector had size 1000 we set the block size B = (cid:6)√

1000(cid:7) =

7

10110210300.20.40.60.81Size of inner blockEmprical number of Type II errors  B−test  a single kernel  σ = 1B−test  a single kernel  σ = medianB−test kernel selectionTests estimating MMDu with σ=1Tests estimating MMDu with σ=medianMethod

B-test

Kernel

parameters

σ = 1

σ = median

multiple kernels

Additional
parameters

√
B = 2
B =
n
√
B = 2
n
B =
B = 2

B =(cid:112) n

2

Gram matrix spectrum

Bootstrap

Gram matrix spectrum

Bootstrap

σ = 1

σ = median

B = 2000

Type I error Type II error Computational

time (s)
0.039
1.276
0.047
1.259
0.607
18.285
160.1356
121.2570
286.8649
122.8297

0.038
0.006
0.043
0.026
0.0481
0.025

0

0.01

0

0.01

0.927
0.597
0.786

0.867
0.012

0

0
0
0
0

Table 2: A comparison of consistent tests on the music experiment described in Section 3.2. Here computation
time is reported for the test achieving the stated error rates.

(a)

(b)

(c)

Figure 5: Empirical Type I error rate for α = 5% on the music data (Section 3.2). (a) A single kernel test with
σ = 1  (b) A single kernel test with σ = median  and (c) for multiple kernels. Error bars are not visible at this
scale. The results broadly follow the trend visible from the synthetic experiments.

sample complexity than MMDl over all tested kernel selection strategies. Moreover  it is an order
of magnitude faster than any test that consistently estimates the null distribution for MMDu (i.e. 
the Gram matrix eigenspectrum and bootstrap estimates): these estimates are impractical at large
sample sizes  due to their computational complexity. Additionally  the B-test remains statistically
consistent  with the best convergence rates achieved for large B. The B-test combines the best
features of MMDl and MMDu based two-sample tests: consistency  high statistical efﬁciency  and
high computational efﬁciency.
A number of further interesting experimental trends may be seen in these results. First  we have
observed that the empirical Type I error rate is often conservative  and is less than the 5% targeted
by the threshold based on a Gaussian null distribution assumption (Figures 2 and 5). In spite of this
conservatism  the Type II performance remains strong (Tables 1 and 2)  as the gains in statistical
power of the B-tests improve the testing performance (cf. Figure 1). Equation (7) implies that the
size of B does not inﬂuence the asymptotic variance under HA  however we observe in Figure 1 that
the empirical variance of HA drops with larger B. This is because  for these P and Q and small B 
the null and alternative distributions have considerable overlap. Hence  given the distributions are
effectively indistinguishable at these sample sizes n  the variance of the alternative distribution as a
function of B behaves more like that of H0 (cf. Equation (10)). This effect will vanish as n grows.
Finally  [13] propose an alternative approach for U-statistic based testing when the degree of de-
generacy is known: a new U-statistic (the TU-statistic) is written in terms of products of centred
U-statistics computed on the individual blocks  and a test is formulated using this TU-statistic. Ho
and Shieh show that a TU-statistic based test can be asymptotically more powerful than a test using
a single U-statistic on the whole sample  when the latter is degenerate under H0  and nondegenerate
under HA. It is of interest to apply this technique to MMD-based two-sample testing.
Acknowledgments We thank Mladen Kolar for helpful discussions. This work is partially funded by ERC
Grant 259112  and by the Royal Academy of Engineering through the Newton Alumni Scheme.

8

24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=√n24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=√n24816326412800.010.020.030.040.050.060.070.08Size of inner blockType I error  EmpiricalTypeIerrorExpectedTypeIerrorB=pn2References
[1] Bengt Von Bahr. On the convergence of moments in the central limit theorem. The Annals of

Mathematical Statistics  36(3):pp. 808–818  1965.

[2] L. Baringhaus and C. Franz. On a new multivariate two-sample test. J. Multivariate Anal. 

88:190–206  2004.

[3] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Kluwer  2004.

[4] Andrew C Berry. The accuracy of the gaussian approximation to the sum of independent

variates. Transactions of the American Mathematical Society  49(1):122–136  1941.

[5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall  1993.
[6] M. Fromont  B. Laurent  M. Lerasle  and P. Reynaud-Bouret. Kernels based tests with non-

asymptotic bootstrap approaches for two-sample problems. In COLT  2012.

[7] A Gretton  K Fukumizu  Z Harchaoui  and BK Sriperumbudur. A fast  consistent kernel two-
sample test. In Advances in Neural Information Processing Systems 22  pages 673–681  2009.
[8] A. Gretton  K. Fukumizu  C.-H. Teo  L. Song  B. Sch¨olkopf  and A. J. Smola. A kernel
statistical test of independence. In Advances in Neural Information Processing Systems 20 
pages 585–592  Cambridge  MA  2008. MIT Press.

[9] A Gretton  B Sriperumbudur  D Sejdinovic  H Strathmann  S Balakrishnan  M Pontil  and
K Fukumizu. Optimal kernel choice for large-scale two-sample tests. In Advances in Neural
Information Processing Systems 25  pages 1214–1222  2012.

[10] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Sch¨olkopf  and Alexander

Smola. A kernel two-sample test. J. Mach. Learn. Res.  13:723–773  March 2012.

[11] Arthur Gretton  Karsten M. Borgwardt  Malte J. Rasch  Bernhard Sch¨olkopf  and Alexander J.

Smola. A kernel method for the two-sample-problem. In NIPS  pages 513–520  2006.

[12] Z. Harchaoui  F. Bach  and E. Moulines. Testing for homogeneity with kernel Fisher discrimi-

nant analysis. In NIPS  pages 609–616. MIT Press  Cambridge  MA  2008.

[13] H.-C. Ho and G. Shieh. Two-stage U-statistics for hypothesis testing. Scandinavian Journal

of Statistics  33(4):861–873  2006.

[14] Norman Lloyd Johnson  Samuel Kotz  and Narayanaswamy Balakrishnan. Continuous uni-

variate distributions. Distributions in statistics. Wiley  2nd edition  1994.

[15] A. Kleiner  A. Talwalkar  P. Sarkar  and M. I. Jordan. A scalable bootstrap for massive data.

Journal of the Royal Statistical Society  Series B  In Press.

[16] Andrey N Kolmogorov. Sulla determinazione empirica di una legge di distribuzione. Giornale

dellIstituto Italiano degli Attuari  4(1):83–91  1933.

[17] B Sch¨olkopf. Support vector learning. Oldenbourg  M¨unchen  Germany  1997.
[18] D. Sejdinovic  A. Gretton  B. Sriperumbudur  and K. Fukumizu. Hypothesis testing using

pairwise distances and associated kernels. In ICML  2012.

[19] R. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley  New York  1980.
[20] Nickolay Smirnov. Table for estimating the goodness of ﬁt of empirical distributions. The

Annals of Mathematical Statistics  19(2):279–281  1948.

[21] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research 
11:1517–1561  2010.

[22] G. Sz´ekely and M. Rizzo. Testing for equal distributions in high dimension. InterStat  (5) 

November 2004.

[23] G. Sz´ekely  M. Rizzo  and N. Bakirov. Measuring and testing dependence by correlation of

distances. Ann. Stat.  35(6):2769–2794  2007.

[24] M. Yamada  T. Suzuki  T. Kanamori  H. Hachiya  and M. Sugiyama. Relative density-ratio
estimation for robust distribution comparison. Neural Computation  25(5):1324–1370  2013.

9

,Wojciech Zaremba
Arthur Gretton
Matthew Blaschko
Dan Garber