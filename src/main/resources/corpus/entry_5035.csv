2016,Single Pass PCA of Matrix Products,In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually  and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets.,Single Pass PCA of Matrix Products

Shanshan Wu

The University of Texas at Austin

shanshan@utexas.edu

Srinadh Bhojanapalli

Toyota Technological Institute at Chicago

srinadh@ttic.edu

Sujay Sanghavi

The University of Texas at Austin
sanghavi@mail.utexas.edu

Alexandros G. Dimakis

The University of Texas at Austin

dimakis@austin.utexas.edu

Abstract

In this paper we present a new algorithm for computing a low rank approximation
of the product AT B by taking only a single pass of the two matrices A and B. The
straightforward way to do this is to (a) ﬁrst sketch A and B individually  and then
(b) ﬁnd the top components using PCA on the sketch. Our algorithm in contrast
retains additional summary information about A  B (e.g. row and column norms
etc.) and uses this additional information to obtain an improved approximation from
the sketches. Our main analytical result establishes a comparable spectral norm
guarantee to existing two-pass methods; in addition we also provide results from
an Apache Spark implementation1 that shows better computational and statistical
performance on real-world and synthetic evaluation datasets.

1

Introduction

Given two large matrices A and B we study the problem of ﬁnding a low rank approximation of their
product AT B  using only one pass over the matrix elements. This problem has many applications in
machine learning and statistics. For example  if A = B  then this general problem reduces to Principal
Component Analysis (PCA). Another example is a low rank approximation of a co-occurrence matrix
from large logs  e.g.  A may be a user-by-query matrix and B may be a user-by-ad matrix  so AT B
computes the joint counts for each query-ad pair. The matrices A and B can also be two large bag-of-
word matrices. For this case  each entry of AT B is the number of times a pair of words co-occurred
together. As a fourth example  AT B can be a cross-covariance matrix between two sets of variables 
e.g.  A and B may be genotype and phenotype data collected on the same set of observations. A low
rank approximation of the product matrix is useful for Canonical Correlation Analysis (CCA) [3].
For all these examples  AT B captures pairwise variable interactions and a low rank approximation is
a way to efﬁciently represent the signiﬁcant pairwise interactions in sub-quadratic space.
Let A and B be matrices of size d ⇥ n (d  n) assumed too large to ﬁt in main memory. To obtain
a rank-r approximation of AT B  a naive way is to compute AT B ﬁrst  and then perform truncated
singular value decomposition (SVD) of AT B. This algorithm needs O(n2d) time and O(n2) memory
to compute the product  followed by an SVD of the n ⇥ n matrix. An alternative option is to directly
run power method on AT B without explicitly computing the product. Such an algorithm will need to
access the data matrices A and B multiple times and the disk IO overhead for loading the matrices to
memory multiple times will be the major performance bottleneck.
For this reason  a number of recent papers introduce randomized algorithms that require only a few
passes over the data  approximately linear memory  and also provide spectral norm guarantees. The

1The code can be found at https://github.com/wushanshan/MatrixProductPCA

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

key step in these algorithms is to compute a smaller representation of data. This can be achieved by
two different methods: (1) dimensionality reduction  i.e.  matrix sketching [15  5  14  6]; (2) random
sampling [7  1]. The recent results of Cohen et al. [6] provide the strongest spectral norm guarantee

⌘4

of the former. They show that a sketch size of O(˜r/✏2) sufﬁces for the sketched matrices eATeB to
achieve a spectral error of ✏  where ˜r is the maximum stable rank of A and B. Note that eATeB is

not the desired rank-r approximation of AT B. On the other hand  [1] is a recent sampling method
with very good performance guarantees. The authors consider entrywise sampling based on column
norms  followed by a matrix completion step to compute low rank approximations. There is also a lot
of interesting work on streaming PCA  but none can be directly applied to the general case when A is
different from B (see Figure 4(c)). Please refer to Appendix D for more discussions on related work.
Despite the signiﬁcant volume of prior work  there is no method that computes a rank-r approximation
of AT B when the entries of A and B are streaming in a single pass 2. Bhojanapalli et al. [1] consider
a two-pass algorithm which computes column norms in the ﬁrst pass and uses them to sample in
a second pass over the matrix elements. In this paper  we combine ideas from the sketching and
sampling literature to obtain the ﬁrst algorithm that requires only a single pass over the data.
Contributions: We propose a one-pass algorithm SMP-PCA (which stands for Streaming Matrix
Product PCA) that computes a rank-r approximation of AT B in time O((nnz(A) + nnz(B)) ⇢2r3 ˜r
⌘2 +
nr6⇢4 ˜r3
). Here nnz(·) is the number of non-zero entries  ⇢ is the condition number  ˜r is the maximum
stable rank  and ⌘ measures the spectral norm error. Existing two-pass algorithms such as [1] typically
have longer runtime than our algorithm (see Figure 3(a)). We also compare our algorithm with the
simple idea that ﬁrst sketches A and B separately and then performs SVD on the product of their
sketches. We show that our algorithm always achieves better accuracy and can perform arbitrarily
better if the column vectors of A and B come from a cone (see Figures 2  4(b)  3(b)).
The central idea of our algorithm is a novel rescaled JL embedding that combines information from
matrix sketches and vector norms. This allows us to get better estimates of dot products of high
dimensional vectors compared to previous sketching approaches. We explain the beneﬁt compared to
a naive JL embedding in Figure 2 and the related discussion; we believe it may be of more general
interest beyond low rank matrix approximations.
We prove that our algorithm recovers a low rank approximation of AT B up to an error that depends
on kAT B  (AT B)rk and kAT Bk  decaying with increasing sketch size and number of samples
(Theorem 3.1). The ﬁrst term is a consequence of low rank approximation and vanishes if AT B is
exactly rank-r. The second term results from matrix sketching and subsampling; the bounds have
similar dependencies as in [6].
We implement SMP-PCA in Apache Spark and perform several distributed experiments on synthetic
and real datasets. Our distributed implementation uses several design innovations described in
Section 4 and Appendix C.5 and it is the only Spark implementation that we are aware of that
can handle matrices that are large in both dimensions. Our experiments show that we improve by
approximately a factor of 2⇥ in running time compared to the previous state of the art and scale
gracefully as the cluster size increases. The source code is available at [18].
In addition to better performance  our algorithm offers another advantage: It is possible to compute
low-rank approximations to AT B even when the entries of the two matrices arrive in some arbitrary
order (as would be the case in streaming logs). We can therefore discover signiﬁcant correlations
even when the original datasets cannot be stored  for example due to storage or privacy limitations.

2 Problem setting and algorithms
Consider the following problem: given two matrices A 2 Rd⇥n1 and B 2 Rd⇥n2 that are stored in
disk  ﬁnd a rank-r approximation of their product AT B. In particular  we are interested in the setting
where both A  B and AT B are too large to ﬁt into memory. This is common for modern large scale
machine learning applications. For this setting  we develop a single-pass algorithm SMP-PCA that
computes the rank-r approximation without explicitly forming the entire matrix AT B.

2One straightforward idea is to sketch each matrix individually and perform SVD on the product of the
sketches. We compare against that scheme and show that we can perform arbitrarily better using our rescaled JL
embedding.

2

Notations. Throughout the paper  we use A(i  j) or Aij to denote (i  j) entry for any matrix A. Let
Ai and Aj be the i-th column vector and j-th row vector. We use kAkF for Frobenius norm  and
kAk for spectral (or operator) norm. The optimal rank-r approximation of matrix A is Ar  which
can be found by SVD. For any positive integer n  let [n] denote the set {1  2 ···   n}. Given a set
⌦ ⇢ [n1] ⇥ [n2] and a matrix A 2 Rn1⇥n2  we deﬁne P⌦(A) 2 Rn1⇥n2 as the projection of A on ⌦ 
i.e.  P⌦(A)(i  j) = A(i  j) if (i  j) 2 ⌦ and 0 otherwise.
2.1 SMP-PCA

Our algorithm SMP-PCA (Streaming Matrix Product PCA) takes four parameters as input: the
desired rank r  number of samples m  sketch size k  and the number of iterations T . Performance
guarantee involving these parameters is provided in Theorem 3.1. As illustrated in Figure 1  our
algorithm has three main steps: 1) compute sketches and side information in one pass over A and B;
2) given partial information of A and B  estimate important entries of AT B; 3) compute low rank
approximation given estimates of a few entries of AT B. Now we explain each step in detail.

Figure 1: An overview of our algorithm. A single pass is performed over the data to produce the

of iterations: T

sketched matrices eA  eB and the column norms kAik  kBjk  for i 2 [n1] and j 2 [n2]. We then
compute the sampled matrix P⌦(fM ) through a biased sampling process  where P⌦(fM ) = fM (i  j) if
(i  j) 2 ⌦ and zero otherwise. Here ⌦ represents the set of sampled entries. The (i  j)-th entry offM
is given in Eq. (2). Performing matrix completion on P⌦(fM ) gives the desired rank-r approximation.
Algorithm 1 SMP-PCA: Streaming Matrix Product PCA
1: Input: A 2 Rd⇥n1  B 2 Rd⇥n2  desired rank: r  sketch size: k  number of samples: m  number
2: Construct a random matrix ⇧ 2 Rk⇥d  where ⇧(i  j) ⇠N (0  1/k)  8(i  j) 2 [k] ⇥ [d]. Perform
a single pass over A and B to obtain: eA =⇧ A  eB =⇧ B  and kAik  kBjk  for i 2 [n1] and
j 2 [n2].
3: Sample each entry (i  j) 2 [n1] ⇥ [n2] independently with probability ˆqij = min{1  qij}  where
qij is deﬁned in Eq.(1); maintain a set ⌦ ⇢ [n1] ⇥ [n2] which stores all the sampled pairs (i  j).
4: Deﬁne fM 2 Rn1⇥n2  where fM (i  j) is given in Eq. (2). Calculate P⌦(fM ) 2 Rn1⇥n2  where
P⌦(fM ) = fM (i  j) if (i  j) 2 ⌦ and zero otherwise.
5: Run WAltMin(P⌦(fM )  ⌦  r  ˆq  T )  see Appendix A for more details.
6: Output: bU 2 Rn1⇥r andbV 2 Rn2⇥r.
compute sketches eA := ⇧A and eB := ⇧B  where ⇧ 2 Rk⇥d is a random matrix with entries being
i.i.d. N (0  1/k) random variables. It is known that ⇧ satisﬁes an "oblivious Johnson-Lindenstrauss
(JL) guarantee" [15][17] and it helps preserving the top row spaces of A and B [5]. Note that any
sketching matrix ⇧ that is an oblivious subspace embedding can be considered here  e.g.  sparse JL
transform and randomized Hadamard transform (see [6] for more discussion).

Step 1: Compute sketches and side information in one pass over A and B. In this step we

Besides eA and eB  we also compute the L2 norms for all column vectors  i.e.  kAik and kBjk  for
i 2 [n1] and j 2 [n2]. We use this additional information to design better estimates of AT B in the

3

qij = m · ( kAik2
2n2kAk2

F

+ kBjk2
2n1kBk2

F

).

(1)

that needs one pass over data.
Step 2: Estimate important entries of AT B by rescaled JL embedding. In this step we use partial

next step  and also to determine important entries of eATeB to sample. Note that this is the only step
information obtained from the previous step to compute a few important entries of eATeB. We ﬁrst
determine what entries of eATeB to sample  and then propose a novel rescaled JL embedding for
estimating those entries.
We sample entry (i  j) of AT B independently with probability ˆqij = min{1  qij}  where

Let ⌦ ⇢ [n1]⇥ [n2] be the set of sampled entries (i  j). Since E(Pi j qij) = m  the expected number

of sampled entries is roughly m. The special form of qij ensures that we can draw m samples in
O(n1 + m log(n2)) time; we show how to do this in Appendix C.5.
Note that qij intuitively captures important entries of AT B by giving higher weight to heavy rows
and columns. We show in Section 3 that this sampling actually generates good approximation to the
matrix AT B.
The biased sampling distribution of Eq. (1) is ﬁrst proposed by Bhojanapalli et al. [1]. However  their
algorithm [1] needs a second pass to compute the sampled entries  while we propose a novel way of
estimating dot products  using information obtained in the ﬁrst step.

DeﬁnefM 2 Rn1⇥n2 as
Note that we will not compute and storefM  instead  we only calculatefM (i  j) for (i  j) 2 ⌦. This
matrix is denoted as P⌦(fM )  where P⌦(fM )(i  j) = fM (i  j) if (i  j) 2 ⌦ and 0 otherwise.

fM (i  j) = kAik·k Bjk·

eAT
i eBj
keAik·k eBjk

(2)

.

2

JL embedding
Rescaled JL embedding

t
c
u
d
o
r
p
 
t
o
d
 
d
e
t
a
m

i
t
s
E

1

0

-1

-2

-1

-0.5

0

0.5

1

True dot product

(a)

(b)

Figure 2: (a) Rescaled JL embedding (red dots) captures the dot products with smaller variance
compared to JL embedding (blue triangles). Mean squared error: 0.053 versus 0.129. (b) Lower
ﬁgure illustrates how to construct unit-norm vectors from a cone with angle ✓. Let x be a ﬁxed
unit-norm vector  and let t be a random Gaussian vector with expected norm tan(✓/2)  we set y as
either x + t or (x + t) with probability half  and then normalize it. Upper ﬁgure plots the ratio of
spectral norm errors kAT B  eATeBk/kAT B fMk  when the column vectors of A and B are unit
vectors drawn from a cone with angle ✓. Clearly fM has better accuracy than eATeB for all possible
We now explain the intuition of Eq. (2)  and whyfM is a better estimator than eATeB. To estimate the
i eBj = keAik·k eBjk· cose✓ij  wheree✓ij is the
(i  j) entry of AT B  a straightforward way is to use eAT

values of ✓  especially when ✓ is small.

4

distorted column norms3.

angle between vectors eAi and eBj. Since we already know the actual column norms  a potentially
better estimator would be kAik·k Bjk· cose✓ij. This removes the uncertainty that comes from
i eBj (JL embedding) andfM (i  j) (rescaled JL embedding)
Figure 2(a) compares the two estimators eAT
for dot products. We plot simulation results on pairs of unit-norm vectors with different angles. The
vectors have dimension 1 000 and the sketching matrix has dimension 10-by-1 000. Clearly rescaling
by the actual norms help reduce the estimation uncertainty. This phenomenon is more prominent
when the true dot products are close to ±1  which makes sense because cos ✓ has a small slope
when cos ✓ approaches ±1  and hence the uncertainty from angles may produce smaller distortion
compared to that from norms. In the extreme case when cos ✓ = ±1  rescaled JL embedding can
perfectly recover the true dot product4.
In the lower part of Figure 2(b) we illustrate how to construct unit-norm vectors from a cone with
angle ✓. Given a ﬁxed unit-norm vector x  and a random Gaussian vector t with expected norm
tan(✓/2)  we construct new vector y by randomly picking one from the two possible choices x+t and
(x + t)  and then renormalize it. Suppose the columns of A and B are unit vectors randomly drawn
from a cone with angle ✓  we plot the ratio of spectral norm errors kAT B  eATeBk/kAT B fMk in
Figure 2(b). We observe thatfM always outperforms eATeB and can be much better when ✓ approaches

zero  which agrees with the trend indicated in Figure 2(a).
Step 3: Compute low rank approximation given estimates of few entries of AT B. Finally we
compute the low rank approximation of AT B from the samples using alternating least squares:

min

U V 2Rn⇥r X(i j)2⌦

wij(eT

i U V T ej fM (i  j))2 

(3)

where wij = 1/ˆqij denotes the weights  and ei  ej are standard base vectors. This is a popular
technique for low rank recovery and matrix completion (see [1] and the references therein). After T

subroutine is quite standard  so we defer the details to Appendix A.

iterations  we will get a rank-r approximation offM presented in the convenient factored form. This

3 Analysis

Now we present the main theoretical result. Theorem 3.1 characterizes the interaction between
the sketch size k  the sampling complexity m  the number of iterations T   and the spectral error
k(AT B)r  [AT Brk  where [AT Br is the output of SMP-PCA  and (AT B)r is the optimal rank-r
approximation of AT B. Note that the following theorem assumes that A and B have the same size.
For the general case of n1 6= n2  Theorem 3.1 is still valid by setting n = max{n1  n2}.
Theorem 3.1. Given matrices A 2 Rd⇥n and B 2 Rd⇥n  let (AT B)r be the optimal rank-r
approximation of AT B. Deﬁne ˜r = max{kAk2
as
the condition number of (AT B)r  where ⇤i is the i-th singular values of AT B.
Let [AT Br be the output of Algorithm SMP-PCA. If the input parameters k  m  and T satisfy

kBk2 } as the maximum stable rank  and ⇢ = ⇤1

kAk2   kBk2

⇤r

F

F

C1kAk2kBk2⇢2r3

max{˜r  2 log(n)} + log (3/)

k 

kAT Bk2
C2˜r2

F

m 



·

F

F + kBk2

·✓kAk2
kAT BkF ◆2
·
T  log(kAkF + kBkF

⇣

⌘2

nr3⇢2 log(n)T 2

⌘2

 

(4)

(5)

 

(6)
where C1 and C2 are some global constants independent of A and B. Then with probability at least
1    we have

(7)
3We also tried using the cosine rule for computing the dot product  and another sketching method speciﬁcally

k(AT B)r  [AT Brk  ⌘kAT B  (AT B)rkF + ⇣ + ⌘⇤r .

designed for preserving angles [2]  but empirically those methods perform worse than our current estimator.

4See http://wushanshan.github.io/files/RescaledJL_project.pdf for more results.

) 

5

F

F +kBk2
kAT BkF

Remark 1. Compared to the two-pass algorithm proposed by [1]  we notice that Eq. (7) contains
an additional error term ⌘⇤r. This extra term captures the cost incurred when we are approximating
entries of AT B by Eq. (2) instead of using the actual values. The exact tradeoff between ⌘ and k is
given by Eq. (4). On one hand  we want to have a small k so that the sketched matrices can ﬁt into
memory. On the other hand  the parameter k controls how much information is lost during sketching 
and a larger k gives a more accurate estimation of the inner products.
Remark 2. The dependence on kAk2
captures one difﬁcult situation for our algorithm. If
kAT BkF is much smaller than kAkF or kBkF   which could happen  e.g.  when many column
vectors of A are orthogonal to those of B  then SMP-PCA requires many samples to work. This is
reasonable. Imagine that AT B is close to an identity matrix  then it may be hard to tell it from an
all-zero matrix without enough samples. Nevertheless  removing this dependence is an interesting
direction for future research.
Remark 3. For the special case of A = B  SMP-PCA computes a rank-r approximation of matrix
AT A in a single pass. Theorem 3.1 provides an error bound in spectral norm for the residual matrix
(AT A)r  [AT Ar. Most results in the online PCA literature use Frobenius norm as performance
measure. Recently  [10] provides an online PCA algorithm with spectral norm guarantee. They
achieves a spectral norm bound of ✏⇤1 + ⇤r+1  which is stronger than ours. However  their algorithm
requires a target dimension of O(r log n/✏2)  i.e.  the output is a matrix of size n-by-O(r log n/✏2) 
while the output of SMP-PCA is simply n-by-r.
Remark 4. We defer our proofs to Appendix C. The proof proceeds in three steps. In Appendix C.2 
we show that the sampled matrix provides a good approximation of the actual matrix AT B. In
Appendix C.3  we show that there is a geometric decrease in the distance between the computed

subspaces bU bV and the optimal ones U⇤  V ⇤ at each iteration of WAltMin algorithm. The spectral
norm bound in Theorem 3.1 is then proved using results from the previous two steps.
Computation Complexity. We now analyze the computation complexity of SMP-PCA. In Step
1  we compute the sketched matrices of A and B  which requires O(nnz(A)k + nnz(B)k) ﬂops.
Here nnz(·) denotes the number of non-zero entries. The main job of Step 2 is to sample a set ⌦
and calculate the corresponding inner products  which takes O(m log(n) + mk) ﬂops. Here we
deﬁne n as max{n1  n2} for simplicity. According to Eq. (4)  we have log(n) = O(k)  so Step 2
takes O(mk) ﬂops. In Step 3  we run alternating least squares on the sampled matrix  which can
be completed in O((mr2 + nr3)T ) ﬂops. Since Eq. (5) indicates nr = O(m)  the computation
complexity of Step 3 is O(mr2T ). Therefore  SMP-PCA has a total computation complexity
O(nnz(A)k + nnz(B)k + mk + mr2T ).

4 Numerical Experiments

Spark implementation. We implement our SMP-PCA in Apache Spark 1.6.2 [19]. For the purpose
of comparison  we also implement a two-pass algorithm LELA [1] in Spark5. The matrices A
and B are stored as a resilient distributed dataset (RDD) in disk (by setting its StorageLevel
as DISK_ONLY). We implement the two passes of LELA using the treeAggregate method.
For SMP-PCA  we choose the subsampled randomized Hadamard transform (SRHT) [16] as the
sketching matrix. The biased sampling procedure is performed using binary search (see Appendix C.5
for how to sample m elements in O(m log n) time). After obtaining the sampled matrix  we run ALS
(alternating least squares) to get the desired low-rank matrices. More details can be found at [18].
Description of datasets. We test our algorithm on synthetic datasets and three real datasets:
SIFT10K [9]  NIPS-BW [11]  and URL-reputation [12]. For synthetic data  we generate matri-
ces A and B as GD  where G has entries independently drawn from standard Gaussian distribution 
and D is a diagonal matrix with Dii = 1/i. SIFT10K is a dataset of 10 000 images. Each image is
represented by 128 features. We set A as the image-by-feature matrix. The task here is to compute
a low rank approximation of AT A  which is a standard PCA task. The NIPS-BW dataset contains
bag-of-words features extracted from 1 500 NIPS papers. We divide the papers into two subsets 
and let A and B be the corresponding word-by-paper matrices  so AT B computes the counts of
co-occurred words between two sets of papers. The original URL-reputation dataset has 2.4 million

5To our best knowledge  this the ﬁrst distributed implementation of LELA.

6

Runtime (sec) vs Cluster size

LELA

SMC-PCA

3000

2000

1000

0.2

0.15

0.1

0.05

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

SVD( !AT !B)
SMP-PCA
LELA
Optimal

0.3

0.25

0.2

0.15

0.1

0.05

SVD( !AT !B)
SMP-PCA
LELA
Optimal

0

2

5
(a)

10

Sketch size (k)

Sketch size (k)

1000

2000

1000

2000

(b)

Figure 3: (a) Spark-1.6.2 running time on a 150GB dataset. All nodes are m3.2xlarge EC2 instances.
See [18] for more details. (b) Spectral norm error achieved by three algorithms over two datasets:

SIFT10K and 1.1 for NIPS-BW. The error of SMP-PCA keeps decreasing as the sketch size k grows.

SIFT10K (left) and NIPS-BW (right). SMP-PCA outperforms SVD(eATeB) by a factor of 1.8 for

URLs. Each URL is represented by 3.2 million features  and is indicated as malicious or benign. This
dataset has been used previously for CCA [13]. Here we extract two subsets of features  and let A and
B be the corresponding URL-by-feature matrices. The goal is to compute a low rank approximation
of AT B  the cross-covariance matrix between two subsets of features.
Sample complexity. In Figure 4(a) we present simulation results on a small synthetic data with
n = d = 5  000 and r = 5. We observe that a phase transition occurs when the sample complexity
m =⇥( nr log n). This agrees with the experimental results shown in previous papers  see  e.g.  [4  1].
For all rest experiments  unless otherwise speciﬁed  we set r = 5  T = 10  and m as 4nr log n.

Table 1: A comparison of spectral norm error over three datasets

Dataset

d

n

Synthetic

100 000

100 000

Algorithm Sketch size k
Optimal
LELA

-
-

SMP-PCA

2 000

URL-
malicious

URL-
benign

792 145

10 000

1 603 985

10 000

Optimal
LELA

SMP-PCA

Optimal
LELA

SMP-PCA

-
-

2 000

-
-

2 000

Error
0.0271
0.0274
0.0280
0.0163
0.0182
0.0188
0.0103
0.0105
0.0117

Comparison of SMP-PCA and LELA. We now compare the statistical performance of SMP-PCA
and LELA [1] on three real datasets and one synthetic dataset. As shown in Figure 3(b) and Table 1 
LELA always achieves a smaller spectral norm error than SMP-PCA  which makes sense because
LELA takes two passes and hence has more chances exploring the data. Besides  we observe that as
the sketch size increases  the error of SMP-PCA keeps decreasing and gets closer to that of LELA.
In Figure 3(a) we compare the runtime of SMP-PCA and LELA using a 150GB synthetic dataset on
m3.2xlarge Amazon EC2 instances6. The matrices A and B have dimension n = d = 100  000. The
sketch dimension is set as k = 2  000. We observe that the speedup achieved by SMP-PCA is more
prominent for small clusters (e.g.  56 mins versus 34 mins on a cluster of size two). This is possibly
due to the increasing spark overheads at larger clusters  see [8] for more related discussion.

Comparison of SMP-PCA and SVD(eATeB). In Figure 4(b) we repeat the experiment in Section 2
by generating column vectors of A and B from a cone with angle ✓. Here SVD(eATeB) refers to

6Each machine has 8 cores  30GB memory  and 2⇥80GB SSD.

7

Ratio of errors vs theta

105

k = 400
k = 800

0.5

0.4

0.3

0.2

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

r Br

AT
SMP-PCA

 

r
o
r
r
e
m
r
o
n

 
l

a
r
t
c
e
p
S

1

0.8

0.6

0.4

0.2

2

1
# Samples / nrlogn

3

(a)

4

100

0

π/4

π/2 3π/4
(b)

π

200 400 600 800 1000

Sketch size (k)

(c)

Figure 4: (a) A phase transition occurs when the sample complexity m =⇥( nr log n). (b) This

ﬁgure plots the ratio of spectral norm error of SVD(eATeB) over that of SMP-PCA. The columns of

A and B are unit vectors drawn from a cone with angle ✓. We see that the ratio of errors scales to
inﬁnity as the cone angle shrinks. (c) If the top r left singular vectors of A are orthogonal to those of
B  the product AT

r Br is a very poor low rank approximation of AT B.

over that of SMP-PCA  as a function of ✓. Note that this is different from Figure 2(b)  as now we
take the effect of random sampling and SVD into account. However  the trend in both ﬁgures are the

computing SVD on the sketched matrices7. We plot the ratio of the spectral norm error of SVD(eATeB)
same: SMP-PCA always outperforms SVD(eATeB) and can be arbitrarily better as ✓ goes to zero.
In Figure 3(b) we compare SMP-PCA and SVD(eATeB) on two real datasets SIFK10K and NIPS-BW.
The y-axis represents spectral norm error  deﬁned as ||AT B  [AT Br||/||AT B||  where [AT Br is
the rank-r approximation found by a speciﬁc algorithm. We observe that SMP-PCA outperforms
SVD(eATeB) by a factor of 1.8 for SIFT10K and 1.1 for NIPS-BW.
Now we explain why SMP-PCA produces a more accurate result than SVD(eATeB). The reasons are
twofold. First  our rescaled JL embedding fM is a better estimator for AT B than eATeB (Figure 2).
Second  the noise due to sampling is relatively small compared to the beneﬁt obtained fromfM  and
hence the ﬁnal result computed using P⌦(fM ) still outperforms SVD(eATeB).

Comparison of SMP-PCA and AT
r Br. Let Ar and Br be the optimal rank-r approximation of A
and B  we show that even if one could use existing methods (e.g.  algorithms for streaming PCA)
to estimate Ar and Br  their product AT
r Br can be a very poor low rank approximation of AT B.
This is demonstrated in Figure 4(c)  where we intentionally make the top r left singular vectors of A
orthogonal to those of B.

5 Conclusion

We develop a novel one-pass algorithm SMP-PCA that directly computes a low rank approximation
of matrix product  using ideas of matrix sketching and entrywise sampling. As a subroutine of our
algorithm  we propose rescaled JL for estimating entries of AT B  which has smaller error compared
to the standard estimator ˜AT ˜B. This we believe can be extended to other applications. Moreover 
SMP-PCA allows the non-zero entries of A and B to be presented in any arbitrary order  and hence
can be used for steaming applications. We design a distributed implementation for SMP-PCA. Our

experimental results show that SMP-PCA can perform arbitrarily better than SVD(eATeB)  and is

signiﬁcantly faster compared to algorithms that require two or more passes over the data.
Acknowledgements We thank the anonymous reviewers for their valuable comments. This research
has been supported by NSF Grants CCF 1344179  1344364  1407278  1422549  1302435  1564000 
and ARO YIP W911NF-14-1-0258.

7This can be done by standard power iteration based method  without explicitly forming the product matrix

eATeB  whose size is too big to ﬁt into memory according to our assumption.

8

References
[1] S. Bhojanapalli  P. Jain  and S. Sanghavi. Tighter low-rank approximation via sampling the leveraged
element. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA)  pages 902–920. SIAM  2015.

[2] P. T. Boufounos. Angle-preserving quantized phase embeddings. In SPIE Optical Engineering+ Applica-

tions. International Society for Optics and Photonics  2013.

[3] X. Chen  H. Liu  and J. G. Carbonell. Structured sparse canonical correlation analysis. In International

Conference on Artiﬁcial Intelligence and Statistics  pages 199–207  2012.

[4] Y. Chen  S. Bhojanapalli  S. Sanghavi  and R. Ward. Completing any low-rank matrix  provably. arXiv

preprint arXiv:1306.2979  2013.

[5] K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th annual ACM symposium on Symposium on theory of computing  pages 81–90.
ACM  2013.

[6] M. B. Cohen  J. Nelson  and D. P. Woodruff. Optimal approximate matrix product in terms of stable rank.

arXiv preprint arXiv:1507.02268  2015.

[7] P. Drineas  R. Kannan  and M. W. Mahoney. Fast monte carlo algorithms for matrices ii: Computing a

low-rank approximation to a matrix. SIAM Journal on Computing  36(1):158–183  2006.

[8] A. Gittens  A. Devarakonda  E. Racah  M. F. Ringenburg  L. Gerhardt  J. Kottalam  J. Liu  K. J. Maschhoff 
S. Canon  J. Chhugani  P. Sharma  J. Yang  J. Demmel  J. Harrell  V. Krishnamurthy  M. W. Mahoney  and
Prabhat. Matrix factorization at scale: a comparison of scientiﬁc data analytics in spark and C+MPI using
three case studies. arXiv preprint arXiv:1607.01335  2016.

[9] H. Jegou  M. Douze  and C. Schmid. Product quantization for nearest neighbor search. Pattern Analysis

and Machine Intelligence  IEEE Transactions on  33(1):117–128  2011.

[10] Z. Karnin and E. Liberty. Online pca with spectral bounds. In Proceedings of The 28th Conference on

Learning Theory (COLT)  volume 40  pages 1129–1140  2015.

[11] M. Lichman. UCI machine learning repository. http://archive.ics.uci.edu/ml  2013.

[12] J. Ma  L. K. Saul  S. Savage  and G. M. Voelker. Identifying suspicious urls: an application of large-scale
online learning. In Proceedings of the 26th annual international conference on machine learning  pages
681–688. ACM  2009.

[13] Z. Ma  Y. Lu  and D. Foster. Finding linear structure in large datasets with scalable canonical correlation

analysis. arXiv preprint arXiv:1506.08170  2015.

[14] A. Magen and A. Zouzias. Low rank matrix-valued chernoff bounds and approximate matrix multiplication.
In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms  pages 1422–
1436. SIAM  2011.

[15] T. Sarlos. Improved approximation algorithms for large matrices via random projections. In Foundations

of Computer Science  2006. FOCS’06. 47th Annual IEEE Symposium on  pages 143–152. IEEE  2006.

[16] J. A. Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive

Data Analysis  pages 115–126  2011.

[17] D. P. Woodruff. Sketching as a tool for numerical linear algebra. arXiv preprint arXiv:1411.4357  2014.

[18] S. Wu  S. Bhojanapalli  S. Sanghavi  and A. Dimakis. Github repository for "single-pass pca of matrix

products". https://github.com/wushanshan/MatrixProductPCA  2016.

[19] M. Zaharia  M. Chowdhury  T. Das  A. Dave  J. Ma  M. McCauley  M. J. Franklin  S. Shenker  and I. Stoica.
Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In Proceedings
of the 9th USENIX conference on Networked Systems Design and Implementation  2012.

9

,Shanshan Wu
Srinadh Bhojanapalli
Sujay Sanghavi
Alexandros Dimakis
Fan Yang
Zhilin Yang
William Cohen