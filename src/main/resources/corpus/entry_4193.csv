2010,PAC-Bayesian Model Selection for Reinforcement Learning,This paper introduces the first set of PAC-Bayesian bounds for the batch reinforcement learning problem in finite state spaces. These bounds hold regardless of the correctness of the prior distribution. We demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment  or on the value of actions. Our empirical results confirm that PAC-Bayesian model-selection is able to leverage prior distributions when they are informative and  unlike standard Bayesian RL approaches  ignores them when they are misleading.,PAC-Bayesian Model Selection
for Reinforcement Learning

Mahdi Milani Fard

School of Computer Science

McGill University
Montreal  Canada

Joelle Pineau

School of Computer Science

McGill University
Montreal  Canada

mmilan1@cs.mcgill.ca

jpineau@cs.mcgill.ca

Abstract

This paper introduces the ﬁrst set of PAC-Bayesian bounds for the batch rein-
forcement learning problem in ﬁnite state spaces. These bounds hold regardless
of the correctness of the prior distribution. We demonstrate how such bounds can
be used for model-selection in control problems where prior information is avail-
able either on the dynamics of the environment  or on the value of actions. Our
empirical results conﬁrm that PAC-Bayesian model-selection is able to leverage
prior distributions when they are informative and  unlike standard Bayesian RL
approaches  ignores them when they are misleading.

1

Introduction

Bayesian methods in machine learning  although elegant and concrete  have often been criticized
not only for their computational cost  but also for their strong assumptions on the correctness of the
prior distribution. There are usually no theoretical guarantees when performing Bayesian inference
with priors that do not admit the correct posterior. Probably Approximately Correct (PAC) learning
techniques  on the other hand  provide distribution-free convergence guarantees with polynomially-
bounded sample sizes [1]. These bounds  however  are notoriously loose and impractical. One can
argue that such loose bounds are to be expected  as they reﬂect the inherent difﬁculty of the problem
when no assumptions are made on the distribution of the data.
Both PAC and Bayesian methods have been proposed for reinforcement learning (RL) [2  3  4  5 
6  7  8]  where an agent is learning to interact with an environment to maximize some objective
function. Many of these methods aim to solve the so-called exploration–exploitation problem by
balancing the amount of time spent on gathering information about the dynamics of the environment
and the time spent acting optimally according to the currently built model. PAC methods are much
more conservative than Bayesian methods as they tend to spend more time exploring the system
and collecting information [9]. Bayesian methods  on the other hand  are greedier and only solve
the problem over a limited planning horizon. As a result of this greediness  Bayesian methods can
converge to suboptimal solutions. It has been shown that Bayesian RL is not PAC [9]. We argue here
that a more adaptive method can be PAC and at the same time more data efﬁcient if an informative
prior is taken into account. Such adaptive techniques have been studied within the PAC-Bayesian
literature for supervised learning.
The PAC-Bayesian approach  ﬁrst introduced by McAllester [10] (extending the work of Shawe-
Taylor et al. [11])  combines the distribution-free correctness of PAC theorems with the data-
efﬁciency of Bayesian inference. This is achieved by removing the assumption of the correctness of
the prior and  instead  measuring the consistency of the prior over the training data. The empirical
results of model selection algorithms for classiﬁcation tasks using these bounds are comparable to
some of the most popular learning algorithms such as AdaBoost and Support Vector Machines [12].
PAC-Bayesian bounds have also been linked to margins in classiﬁcation tasks [13].

1

This paper introduces the ﬁrst results of the application of PAC-Bayesian techniques to the batch
RL problem. We derive two PAC-Bayesian bounds on the approximation error in the value function
of stochastic policies for reinforcement learning on observable and discrete state spaces. One is a
bound on model-based RL where a prior distribution is given on the space of possible models. The
second one is for the case of model-free RL  where a prior is given on the space of value functions.
In both cases  the bound depends both on an empirical estimate and a measure of distance between
the stochastic policy and the one imposed by the prior distribution. We present empirical results
where model-selection is performed based on these bounds  and show that PAC-Bayesian bounds
follow Bayesian policies when the prior is informative and mimic the PAC policies when the prior
is not consistent with the data. This allows us to adaptively balance between the distribution-free
correctness of PAC and the data-efﬁciency of Bayesian inference.

2 Background and Notation

In this section  we introduce the notations and deﬁnitions used in the paper.
A Markov Decision Process (MDP) M = (S  A  T  R) is deﬁned by a set of states S  a set of
actions A  a transition function T (s  a  s(cid:48)) deﬁned as:

T (s  a  s(cid:48)) = p(st+1 = s(cid:48)|st = s  at = a) ∀s  s(cid:48) ∈ S  a ∈ A 

(1)
and a (possibly stochastic) reward function R(s  a) : S × A → [Rmin  Rmax]. Throughout the paper
we assume ﬁnite-state  ﬁnite-action  discounted-reward MDPs  with the discount factor denoted by
γ. A reinforcement learning agent chooses an action and receives a reward. The environment will
then change to a new state according to the transition probabilities.
A policy is a (possibly stochastic) function from states to actions. The value of a state–action pair
t γtrt) if the
agent acts according to that policy after taking action a in the ﬁrst step. The value function satisﬁes
the Bellman equation [14]:

(s  a) for policy π  denoted by Qπ(s  a)  is the expected discounted sum of rewards ((cid:80)

Qπ(s  a) = R(s  a) + γ

(T (s  a  s(cid:48))Qπ(s(cid:48)  π(s(cid:48)))) .

(2)

The optimal policy is the policy that maximizes the value function. The optimal value of a state–
action pair  denoted by Q∗(s  a)  satisﬁes the Bellman optimality equation [14]:

Q∗(s  a) = R(s  a) + γ

T (s  a  s(cid:48)) max
a(cid:48)∈A

Q∗(s(cid:48)  a(cid:48))

.

(3)

(cid:88)

s(cid:48)∈S

(cid:18)

(cid:88)

s(cid:48)∈S

(cid:18)

(cid:88)

s(cid:48)∈S

(cid:19)

(cid:19)

There are many methods developed to ﬁnd the optimal policy for a given MDP when transition and
reward functions are known. Value iteration [14] is a simple dynamic programming method in which
one iteratively applies the Bellman optimality operator  denoted by B  to an initial guess of the
optimal value function:

BQ(s  a) = R(s  a) + γ

T (s  a  s(cid:48)) max
a(cid:48)∈A

Q(s(cid:48)  a(cid:48))

.

(4)

For simplicity we write BQ when B is applied to the value of all state–action pairs. Since B is a
contraction with respect to the inﬁnity norm [15] (i.e. (cid:107)BQ − BQ(cid:48)(cid:107)∞ ≤ γ(cid:107)Q − Q(cid:48)(cid:107)∞)  the value
iteration algorithm will converge to the ﬁxed point of the Bellman optimality operator  which is the
optimal value function (BQ∗ = Q∗).

3 Model-Based PAC-Bayesian Bound

In model-based RL  one aims to estimate the transition and reward functions and then act optimally
according to the estimated models. PAC methods use the empirical average for their estimated model
along with frequentist bounds. Bayesian methods use the Bayesian posterior to estimate the model.
This section provides a bound that suggests an adaptive method to choose a stochastic estimate
between these two extremes  which is both data-efﬁcient and has guaranteed performance.

2

Assuming that the reward model is known (we make this assumption throughout this section)  one
can build empirical models of the transition dynamics by gathering sample transitions  denoted by
U  and taking the empirical average. Let this empirical average model be ˆT (s  a  s(cid:48)) = ns a s(cid:48)/ns a 
where ns a s(cid:48) and ns a are the number of corresponding transitions and samples. Trivially  E ˆT = T .
The empirical value function  denoted by ˆQ  is deﬁned to be the value function on an MDP with
the empirical transition model. As one observes more and more sample trajectories on the MDP  the
empirical model gets increasingly more accurate  and so will the empirical value function. Different
forms of the following lemma  connecting the error rates on ˆT and ˆQ  are used in many of the
PAC-MDP results [4]:
Lemma 1. There is a constant k ≥ (1 − γ)2/γ such that:
⇒

∀s  a : (cid:107) ˆT (s  a  .) − T (s  a  .)(cid:107)1 ≤ k

∀π : (cid:107) ˆQπ − Qπ(cid:107)∞ ≤ .

(5)

As a consequence of the above lemma  one can act near-optimally in the part of the MDP for which
we have gathered enough samples to have a good empirical estimate of the transition model. PAC-
MDP methods explicitly [2] or implicitly [3] use that fact to exploit the knowledge on the model
as long as they are in the “known” part of the state space. The downside of these methods is that
without further assumptions on the model  it will take a large number of sample transitions to get a
good empirical estimate of the transition model.
The Bayesian approach to modeling the transition dynamics  on the other hand  starts with a prior
distribution over the transition probability and then marginalizes this prior over the data to get a
posterior distribution. This is usually done by assuming independent Dirichlet distributions over the
transition probabilities  with some initial count vector α  and then adding up the observed counts to
this initial vector to get the conjugate posterior [6]. The initial α-vector encodes the prior knowledge
on the transition probabilities  and larger initial values further bias the empirical observation towards
the initial belief.
If a strong prior is close to the true values  the Bayesian posterior will be more accurate than the
empirical point estimate. However  a strong prior peaked on the wrong values will bias the Bayesian
model away from the correct probabilities. Therefore  the Bayesian posterior might not provide
the optimal estimate of the model parameters. A good posterior distribution might be somewhere
between the empirical point estimate and the Bayesian posterior.
The following theorem is the ﬁrst PAC-Bayesian bound on the estimation error in the value function
when we build a stochastic policy1 based on some arbitrary posterior distribution Mq.
Theorem 2. Let π∗
T (cid:48) be the optimal policy with respect to the MDP with transition model T (cid:48) and
∆T (cid:48) = (cid:107) ˆQπ∗
T (cid:48) − Qπ∗
T (cid:48)(cid:107)∞. For any prior distribution Mp on the transition model  any posterior Mq 
any i.i.d. sampling distribution U  with probability no less than 1 − δ over the sampling of U ∼ U:

∀Mq : ET (cid:48)∼Mq ∆T (cid:48) ≤

D(Mq(cid:107)Mp) − ln δ + |S| ln 2 + ln|S| + ln nmin

(nmin − 1)k2/2

 

(6)

where nmin = mins a ns a and D(.(cid:107).) is the Kullback–Leibler (KL) divergence.
The above theorem (proved in the Appendix) provides a lower bound on the expectation of the true
value function when the policy is taken to be optimal according to the sampled model from the
posterior:

(cid:115)

(cid:18)(cid:113)

(cid:19)

EQπ∗

T (cid:48) ≥ E ˆQπ∗

T(cid:48) − ˜O

D(Mq(cid:107)Mp)/nmin

.

(7)

This lower bound suggests a stochastic model-selection method in which one searches in the space
of posteriors to maximize the bound. Notice that there are two elements to the above bound. One
is the PAC part of the bound that suggests the selection of models with high empirical value func-
tions for their optimal policy. There is also a penalty term (or a regularization term) that penalizes
distributions that are far from the prior (the Bayesian side of the bound).

1This is a more general form of stochastic policy than is usually seen in the RL literature. A complete policy

is sampled from an imposed distribution  correlating the selection of actions on different states.

3

Margin for Deterministic Policies

One could apply Theorem 2 with any choice Mq. Generally  this will result in a bound on the value
of a stochastic policy. However  if the optimal policy is the same for all of the possible samples from
the posterior  then we will get a bound for that particular deterministic policy.
We deﬁne the support of policy π  denoted by Tπ  to be the set of transition models for which the
optimal policy is π. Putting all the posterior probability on Tπ will result in a tighter bound for the
value of the policy π. The tightest bound occurs when Mq is a scaled version of Mp summing to 1
over Tπ  that is when we have:

(cid:40) Mp(T (cid:48))
Mp(Tπ) T (cid:48) ∈ Tπ
T (cid:48) /∈ Tπ
(cid:18)(cid:113)− ln Mp(Tπ)/nmin

0

(cid:19)

.

Mq(T (cid:48)) =

EQπ∗

T(cid:48) ≥ E ˆQπ∗

T (cid:48) − ˜O

In that case  the KL divergence is D(Mq(cid:107)Mp) = − ln Mp(Tπ)  and the bound will be:

(8)

(9)

Intuitively  we will get tighter bounds for policies that have larger empirical values and higher prior
probabilities supporting them.
Finding Mp(Tπ) might not be computationally tractable. Therefore  we deﬁne a notion of margin
for transition functions and policies and use it to get tractable bounds. The margin of a transition
function T (cid:48)  denoted by θT (cid:48)  is the maximum distance we can move away from T (cid:48) such that the
optimal policy does not change:

(cid:107)T (cid:48)(cid:48) − T (cid:48)(cid:107)1 ≤ θT (cid:48) ⇒ π∗

T (cid:48)(cid:48) = π∗
T (cid:48).

(10)
The margin deﬁnes a hypercube around T (cid:48) for which the optimal policy does not change. In cases
where the support set of a policy is difﬁcult to ﬁnd  one can use this hypercube to get a reasonable
bound for the true value function of the corresponding policy. In that case  we would deﬁne the
posterior to be the scaled prior deﬁned only on the margin hypercube. The idea behind this method
is similar to that of the Luckiness framework [11] and large-margin classiﬁers [16  13]. This shows
that the idea of maximizing margins can be applied to control problems as well as classiﬁcation and
regression tasks.
To ﬁnd the margin of any given T (cid:48)  if we know the value of the second best policy  we can calculate
its regret according to T (cid:48) (it will be the smallest regret ηmin). Using Lemma 1  we can conclude
that if (cid:107)T (cid:48)(cid:48) − T (cid:48)(cid:107)1 ≤ kηmin/2  then the value of the best and second best policies can change by at
most ηmin/2  and thus the optimal policy will not change. Therefore  θT (cid:48) ≥ kηmin/2. One can then
deﬁne the posterior on the transitions inside the margin to get a bound for the value function.

4 Model-Free PAC-Bayes Bound

In this section we introduce a PAC-Bayesian bound for model-free reinforcement learning on dis-
crete state spaces. This time we assume that we are given a prior distribution on the space of value
functions  rather than on transition models. This prior encodes an initial belief about the optimal
value function for a given RL domain. This could be useful  for example  in the context of transfer
learning  where one has learned a value function in one environment and then uses that as the prior
belief on a similar domain.
We start by deﬁning the TD error of a given value function Q to be (cid:107)Q − BQ(cid:107)∞. In most cases  we
do not have access to the Bellman optimality operator. When we only have access to a sample set U
collected on the RL domain  we can deﬁne the empirical Bellman optimality operator ˆB to be:

r + γ max

a(cid:48) Q(s(cid:48)  a(cid:48))

 

(11)

(cid:88)

(cid:16)

ˆBQ(s  a) =

1

ns a

(s a s(cid:48) r)∈U

(cid:17)

Note that E[ ˆBQ] = BQ. We further make an assumption that all the BQ values we could observe
are bounded in the range [cmin  cmax]  with c = cmax − cmin. Using this assumption  one can use
Hoeffding’s inequality to bound the difference between the empirical and true Bellman operators:

Pr{| ˆBQ(s  a) − BQ(s  a)| > } ≤ e−2ns a2/c2

.

(12)

4

When the true Bellman operator is not known  one makes use of the empirical TD error  similarly
deﬁned to be (cid:107)Q − ˆBQ(cid:107)∞. Q-learning [14] and its derivations with function approximation [17] 
and also batch methods such as LSTD [18]  often aim to minimize the empirical (projected) TD
error. We argue that it might be better to choose a function that is not a ﬁxed point of the empirical
Bellman operator. Instead  we aim to minimize the upper bound on the approximation error (which
might be referred to as loss) of the Q function  as compared to the true optimal value.
The following theorem (proved in the Appendix) is the ﬁrst PAC-Bayesian bound for model-free
batch RL on discrete state spaces:
Theorem 3. Let ∆Q = (cid:107)Q − Q∗(cid:107)∞ − (cid:107)Q− ˆBQ(cid:107)∞
. For all prior distributions Jp and posteriors Jq
over the space of value functions  with probability no less than 1 − δ over the sampling of U ∼ U:

1−γ

∀Jq : EQ∼Jq ∆Q ≤

D(Jq(cid:107)Jp) − ln δ + ln|S| + ln|A| + ln nmin

2(nmin − 1)(1 − γ)2/c2

.

(13)

(cid:115)

This time we have an upper bound on the expected approximation error:

E(cid:107)Q − Q∗(cid:107)∞ ≤ E(cid:107)Q − ˆBQ(cid:107)∞

1 − γ

+ ˜O

D(Jq(cid:107)Jp)/nmin

(cid:18)(cid:113)

(cid:19)

.

(14)

This suggests a model-selection method in which one would search for a posterior Jq to minimize the
above bound. The PAC side of the bound guides this model-selection method to look for posteriors
with smaller empirical TD error. The Bayesian part  on the other hand  penalizes the selection of
posteriors that are far from the prior distribution.
One can use general forms of priors that would impose smoothness or sparsity for this model-
selection technique. In that sense  this method would act as a regularization technique that penalizes
complex and irregular functions. The idea of regularization in RL with function approximation is
not new to this work [19]. This bound  however  is more general  as it could incorporate not only
smoothness constraints  but also other forms of prior knowledge into the learning process.

5 Empirical Results

To illustrate the model-selection techniques based on the bounds in the paper  we consider one
model-based RL domain and one model-free problem. The model-based domain is a chain model
in which states are ordered by their index. The last state has a reward of 1 and all other states have
reward 0. There are two types of actions. One is a stochastic “forward” operation which moves us to
the next state in the chain with probability 0.5 and otherwise makes a random transition. The second
type is a stochastic “reset” which moves the system to the ﬁrst state in the chain with probability 0.5
and makes a random transition otherwise. In this domain  we have at each state two actions that do
stochastic reset and one action that is a stochastic forward. There are 10 states and γ = 0.9.
When there are only a few number of sample transitions for each state–action pair  there is a high
chance that the frequentist estimate confuses a reset action with a forward. Therefore  we expect a
good model-based prior to be useful in this case. We use independent Dirichlets as our prior. We
experiment with priors for which the Dirichlet α-vector sums up to 10. We deﬁne our good prior to
have α-vectors proportional to the true transition probabilities. A misleading prior is one for which
the vector is proportional to a transition model when the actions are switched between forward and
reset. A weighted sum between the good and bad priors creates a range of priors that gradually
change from being informative to misleading.
We compare the expected regret of three different methods. The empirical method uses the optimal
policy with respect to the empirical models. The Bayesian method samples a transition model from
the Bayesian Dirichlet posteriors (when the observed counts are added to the prior α-vectors) and
then uses the optimal policy with respect to the sampled model. The PAC-Bayesian method uses
counts + λαprior as the α-vector of the posterior and ﬁnds the value of λ ∈ [0  1]  using linear
search within values with distance 0.1  that maximizes the lower bound of Theorem 2 (with a more
optimistic value for k and δ = 0.05). It then samples from that distribution and uses the optimal
policy with respect to the sampled model. The running time for a single run is a few seconds.

5

Figure 1 (left) shows the comparison between the maximum regret in these methods for different
sample sizes when the prior is informative. This is averaged over 50 runs for the Bayesian and PAC-
Bayesian methods and 10000 runs for the empirical method. The number of sampled transitions is
the same for all state–action pairs. As expected  the Bayesian method outperforms the empirical
one for small sample sizes. We can see that the PAC-Bayesian method is closely following the
Bayesian one in this case. With a misleading prior  however  as we can see in Figure 1 (center) 
the empirical method outperforms the Bayesian one. This time  the regret rate of the PAC-Bayesian
method follows that of the empirical method. Figure 1 (right) shows how the PAC-Bayesian method
switches between following the empirical estimate and the Bayesian posterior as the prior gradually
changes from being misleading to informative (four sample transitions per state action pair). This
shows that the bound of Theorem 2 is helpful as a model selection technique.

Figure 1: Average regrets of different methods. Error bars are 1 standard deviation of the mean.

The next experiment is to test the model-free bound of Theorem 3. The domain is a “puddle world”.
An agent moves around in a grid world of size 5×9 containing puddles with reward −1  an absorbing
goal state with reward +1  and reward 0 for the remaining states. There are stochastic actions along
each of the four cardinal directions that move in the correct direction with probability 0.7 and move
in a random direction otherwise. If the agent moves towards the boundary then it stays in its current
position.

Figure 2: Maps of puddle world RL domain. Shaded boxes are puddles.

We ﬁrst learn the true value function of a known prior map of the world (Figure 2  left). We then use
that value function as the prior for our model-selection technique on two other environments. One of
them is a similar environment where the shape of the puddle is slightly changed (Figure 2  center).
We expect the prior to be informative and useful in this case. The other environment is  however 
largely different from the ﬁrst map (Figure 2  right). We thus expect the prior to be misleading.

Table 1: Performance of different model-selection methods.
PAC-Bayesian Regret

Empirical Regret Bayesian Regret
0.10 ± 0.01
1.16 ± 0.09

0.21 ± 0.03
0.19 ± 0.03

0.12 ± 0.01
0.22 ± 0.04

Average λ
0.58 ± 0.01
0.03 ± 0.03

(cid:17)−1

(cid:16) λ

σ2
0

Similar Map
Different Map

(cid:17)

(cid:16) λµ0

σ2
0

(cid:17)(cid:46)(cid:16) λ

σ2
0

ˆσ2

+ n
ˆσ2

and variance

We start with independent Gaussians (one for each state–action pair) as the prior  with the initial
0 = 0.01 for the variance. The posterior is chosen to be the
map’s Q-values for the mean µ0  and σ2
+ n ˆQ(. .)
product of Gaussians with mean
  where
ˆσ2 is the empirical variance. We sample from this posterior and act according to its greedy policy.
For λ = 1  this is the Bayesian posterior for the mean of a Gaussian with known variance. For
λ = 0  the prior is completely ignored. We will  however  ﬁnd the λ ∈ [0  1] that minimizes the
PAC-Bayesian bound of Theorem 3 (with an optimistic choice of c and δ = 0.05) and compare it
with the performance of the empirical policy and a semi-Bayesian policy that acts according to a
sampled value from the Bayesian posterior.
Table 1 shows the average over 100 runs of the maximum regret for these methods and the average
of the selected λ  with equal sample size of 20 per state–action pair. Again  it can be seen that the
PAC-Bayesian method makes use of the prior (with higher values of λ) when the prior is informative 
and otherwise follows the empirical estimate (smaller values of λ). It adaptively balances the usage
of the prior based on its consistency over the observed data.

+ n
ˆσ2

6

51015200.040.060.080.10.120.140.16sample size for each state−action pairregret EmpiricalBayesianPAC−Bayesian 51015200.050.10.150.20.250.30.350.4sample size for each state−action pairregret EmpiricalBayesianPAC−Bayesian 00.20.40.60.810.050.10.150.20.250.30.350.4weight on the good priorregret EmpiricalBayesianPAC−Bayesian GGG6 Discussion

This paper introduces the ﬁrst set of PAC-Bayesian bounds for the batch RL problem in ﬁnite state
spaces. We demonstrate how such bounds can be used for both model-based and model-free RL
methods. Our empirical results show that PAC-Bayesian model-selection uses prior distributions
when they are informative and useful  and ignores them when they are misleading.
For the model-based bound  we expect the running time of searching in the space of parametrized
posteriors to increase rapidly with the size of the state space. A more scalable version would sample
models around the posteriors  solve each model  and then use importance sampling to estimate the
value of the bound for each possible posterior. This problem does not exist with the model-free
approach  as we do not need to solve the MDP for each sampled model.
A natural extension to this work would be on domains with continuous state spaces  where one would
use different forms of function approximation for the value function. There is also the possibility
of future work in applications of PAC-Bayesian theorems in online reinforcement learning  where
one targets the exploration–exploitation problem. Online PAC RL with Bayesian priors has recently
been addressed with the BOSS algorithm [20]. PAC-Bayesian bounds could help derive similar
model-free algorithms with theoretical guarantees.
Acknowledgements: Funding for this work was provided by the National Institutes of Health (grant
R21 DA019800) and the NSERC Discovery Grant program.

Appendix

The following lemma  due to McAllester [21]  forms the basis of the proofs for both bounds:

Lemma 4. For β > 0  K > 0  and Q P  ∆ ∈ Rn satisfying Pi Qi  ∆i ≥ 0 and(cid:80)n
Qi∆i ≤(cid:112)(D(Q(cid:107)P) + ln K)/β.

Pieβ∆2

n(cid:88)

n(cid:88)

i ≤ K

⇒

i=1 Qi = 1:

(15)

i=1

i=1

Note that even when we have arbitrary probability measures Q and P on a continuous space of
∆’s  it might still be possible to deﬁne a sequence of vectors Q(1) Q(2)  . . .   P (1) P (2)  . . . and
∆(1)  ∆(2)  . . . such that Q(n) P (n) and ∆(n) satisfy the condition of the lemma and

EQ∆ = lim
n→∞

Q(n)
i ∆(n)

i

 

D(Q(cid:107)P) = lim
n→∞

n(cid:88)

i=1

n(cid:88)

i=1

Q(n)

i

ln

Q(n)
P (n)

i

i

.

(16)

We will then take the limit of the conclusion of the lemma to get a bound for the continuous case [21].

Proof of Theorem 2 (Model-Based Bound)
Lemma 5. Let ∆T (cid:48) = (cid:107) ˆQπ∗

T (cid:48) − Qπ∗

T (cid:48)(cid:107)∞. With probability no less than 1 − δ over the sampling:
2 (nmin−1)k2∆2

T (cid:48) ] ≤ |S|2|S|nmin

.

1

(17)

δ

ET (cid:48)∼Mp [e

Before proving Lemma 5  note that Lemma 5 and Lemma 4 together imply Therorem 2. We only
need to apply the method described for arbitrary probability measures. To prove Lemma 5  it sufﬁces
to prove the following  swap the expectations and apply Markov’s inequality:

T (cid:48) ] ≤ |S|2|S|nmin.
Therefore  we only need to show that for any choice of T (cid:48)  EU∼U [e 1
bound. Let as = π∗

2 (nmin−1)k2∆2

ET (cid:48)∼MP

EU∼U [e

1

(18)

2 (nmin−1)k2∆2

T (cid:48) ] follows the

T (cid:48)(s). We have:

Pr{∆T (cid:48) ≥ } ≤ (cid:88)
≤ (cid:88)

s

(cid:16)

Pr{(cid:107) ˆT (s  as  .) − T (s  as  .)(cid:107)1 > k}

2 ns as (k)2(cid:17) ≤ |S|2|S|e− 1

2|S|e− 1

2 nmin(k)2

(19)

(20)

.

s

7

The ﬁrst line is by Lemma 1. The second line is a concentration inequality for multinomials [22].
2 nmin(k)2.
We choose to maximize EU∼U [e 1
The maximum occurs when the inequality is tight and the p.d.f. for ∆T (cid:48) is:

T (cid:48) ]  subject to Pr{∆T (cid:48) ≥ } ≤ |S|2|S|e− 1

2 (nmin−1)k2∆2

f (∆) = |S|2|S|k2nmin∆e− 1

2 nmink2∆2

.

We thus get:

EU∼U [e

1

2 (nmin−1)k2∆2

T (cid:48) ] ≤

1

2 (nmin−1)k2∆2

e

f (∆)d∆

|S|2|S|k2nmin∆e− 1
This concludes the proof of Lemma 5 and consequently Theorem 2.

=

0

2 k2∆2

(cid:90) ∞
(cid:90) ∞

0

(21)

(22)

(23)

d∆ ≤ |S|2|S|nmin.

Proof of Theorem 3 (Model-Free Bound)
Since B is a contraction with respect to the inﬁnity norm and Q∗ is its ﬁxed point  we have:

(cid:107)Q − Q∗(cid:107)∞ = (cid:107)Q − BQ + BQ − BQ∗(cid:107)∞ ≤ (cid:107)Q − BQ(cid:107)∞ + (cid:107)BQ − BQ∗(cid:107)∞ (24)
(25)

≤ (cid:107)Q − BQ(cid:107)∞ + γ(cid:107)Q − Q∗(cid:107)∞

And thus (cid:107)Q − Q∗(cid:107)∞ ≤ 1
Lemma 6. Let ∆Q = max(0 (cid:107)Q − Q∗(cid:107)∞ − (cid:107)Q− ˆBQ(cid:107)∞

1−γ(cid:107)Q − BQ(cid:107)∞.

1−γ

). With probability no less than 1 − δ:

EQ∼Jp [e2(nmin−1)(1−γ)2∆2

Q/c2

] ≤ |S||A|nmin

δ

.

(26)

Q/c2

≤ Pr

] follows the bound. We have that:

Similar to the previous section  Lemma 6 and Lemma 4 together imply Theorem 3.
To prove Lemma 6  similar to the previous proof  we only need to show that for any choice of Q 
EU∼U [e2(nmin−1)(1−γ)2∆2
Pr{∆Q≥ } = Pr

(cid:110)(cid:107)Q − Q∗(cid:107)∞ ≥  + (cid:107)Q − ˆBQ(cid:107)∞/(1 − γ)
(cid:111)
(cid:17)(cid:111)
(cid:110)(cid:107)Q − BQ(cid:107)∞ ≥ (1 − γ)
(cid:110)|Q(s  a) − BQ(s  a)| ≥ (1 − γ) + (cid:107)Q − ˆBQ(cid:107)∞
(cid:111)
(cid:110)|Q(s  a) − ˆBQ(s  a)| + | ˆBQ(s  a) − BQ(s  a)| ≥ (1 − γ) + (cid:107)Q − ˆBQ(cid:107)∞
(cid:110)| ˆBQ(s  a) − BQ(s  a)| ≥ (1 − γ)

 + (cid:107)Q − ˆBQ(cid:107)∞/(1 − γ)

(cid:111)

(cid:111)

(cid:16)

(29)

(28)

(30)

(27)

Pr

Pr

Pr

e−2ns a(1−γ)22/c2 ≤ |S||A|e−2nmin(1−γ)22/c2

s a

Eqn (28) follows from the derivations at the beginning of this section. Eqn (29) is by the union
bound. Eqn (31) is by the deﬁnition of inﬁnity norm. Last derivation is by Hoeffding inequality of
Equation (12). Now again  similar to the model-based case  when the inequality is tight the p.d.f. is:

f (∆) = 4|S||A|nmin(1 − γ)2c−2∆e−2nmin(1−γ)2∆2/c2

.

s a

≤ (cid:88)
≤ (cid:88)
≤ (cid:88)
≤ (cid:88)

s a

s a

(31)

(32)

We thus get:

EU∼U [e2(nmin−1)(1−γ)2∆2

Q/c2

] ≤

(cid:90) ∞
(cid:90) ∞

0

=
≤ |S||A|nmin.

0

e2(nmin−1)(1−γ)2∆2/c2

f (∆)d∆

4|S||A|nmin(1 − γ)2c−2∆e−2(1−γ)2∆2/c2

d∆

This concludes the proof of Lemma 6 and consequently Theorem 3.

8

References
[1] L. G. Valiant. A theory of the learnable. Commun. ACM  27(11):1134–1142  1984.
[2] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning 

49(2-3):209–232  2002.

[3] R. I. Brafman and M. Tennenholtz. R-max – A general polynomial time algorithm for near-optimal

reinforcement learning. The Journal of Machine Learning Research  3:213–231  2003.

[4] A. L. Strehl and M. L. Littman. A theoretical analysis of model-based interval estimation. In Proceedings

of the 22nd International Conference on Machine Learning  pages 856–863  2005.

[5] S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis  University College

London  2003.

[6] M. O. G. Duff. Optimal learning: Computational procedures for Bayes-adaptive Markov decision pro-

cesses. PhD thesis  University of Massachusetts Amherst  2002.

[7] M. J. A. Strens. A Bayesian Framework for Reinforcement Learning. In Proceedings of the 17th Inter-

national Conference on Machine Learning  pages 943–950  2000.

[8] T. Wang  D. Lizotte  M. Bowling  and D. Schuurmans. Bayesian sparse sampling for on-line reward
In Proceedings of the 22nd International Conference on Machine Learning  page 963 

optimization.
2005.

[9] J. Z. Kolter and A. Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th

International Conference on Machine Learning  pages 513–520  2009.

[10] D. A. McAllester. Some PAC-Bayesian theorems. Machine Learning  37(3):355–363  1999.
[11] J. Shawe-Taylor and R. C. Williamson. A PAC analysis of a Bayesian estimator. In Proceedings of the

10th Annual Conference on Computational Learning Theory  pages 2–9  1997.

[12] P. Germain  A. Lacasse  F. Laviolette  and M. Marchand. PAC-Bayesian learning of linear classiﬁers. In

Proceedings of the 26th International Conference on Machine Learning  pages 353–360  2009.

[13] J. Langford and J. Shawe-Taylor. PAC-Bayes and margins. In Proceedings of Advances in Neural Infor-

mation Processing Systems  pages 439–446  2002.

[14] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press  Cambridge  MA 

1998.

[15] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming (Optimization and Neural Computation

Series  3). Athena Scientiﬁc  1996.

[16] R. Herbrich and T. Graepel. A PAC-Bayesian margin bound for linear classiﬁers. IEEE Transactions on

Information Theory  48(12):3140–3150  2002.

[17] R. S. Sutton  D. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement learn-
ing with function approximation. Proceedings of Advances in Neural Information Processing Systems 
12:1057–1063  2000.

[18] J. A. Boyan. Technical update: Least-squares temporal difference learning. Machine Learning 

49(2):233–246  2002.

[19] A. Farahmand  M. Ghavamzadeh  C. Szepesv´ari  and S. Mannor. Regularized ﬁtted Q-iteration: Applica-

tion to planning. Recent Advances in Reinforcement Learning  pages 55–68  2008.

[20] J. Asmuth  L. Li  M. L. Littman  A. Nouri  and D. Wingate. A Bayesian sampling approach to exploration

in reinforcement learning. The 25th Conference on Uncertainty in Artiﬁcial Intelligence  2009.

[21] D. A. McAllester. PAC-Bayesian model averaging. In Proceedings of the 12th Annual Conference on

Computational Learning Theory  pages 164–170  1999.

[22] T. Weissman  E. Ordentlich  G. Seroussi  S. Verdu  and M. J. Weinberger. Inequalities for the L1 deviation
of the empirical distribution. Technical report  Information Theory Research Group  HP Laboratories 
2003.

9

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
Janne Korhonen
Pekka Parviainen
Ashish Kumar
Saurabh Gupta
David Fouhey
Sergey Levine
Jitendra Malik