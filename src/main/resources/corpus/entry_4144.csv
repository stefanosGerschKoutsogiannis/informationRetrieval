2007,Optimistic Linear Programming gives Logarithmic Regret for Irreducible MDPs,We present an algorithm called Optimistic Linear Programming (OLP) for learning to optimize average reward in an irreducible but otherwise unknown Markov decision process (MDP). OLP uses its experience so far to estimate the MDP. It chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates: a computation that corresponds to solving linear programs. We show that the total expected reward obtained by OLP up to time $T$ is within $C(P)\log T$ of the reward obtained by the optimal policy  where $C(P)$ is an explicit  MDP-dependent constant. OLP is closely related to an algorithm proposed by Burnetas and Katehakis with four key differences: OLP is simpler  it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler  but our regret bound is a constant factor larger than the regret of their algorithm. OLP is also similar in flavor to an algorithm recently proposed by Auer and Ortner. But OLP is simpler and its regret bound has a better dependence on the size of the MDP.,Optimistic Linear Programming gives Logarithmic

Regret for Irreducible MDPs

Ambuj Tewari

Computer Science Division

Univeristy of California  Berkeley

Berkeley  CA 94720  USA

ambuj@cs.berkeley.edu

Computer Science Division and Department of Statistics

Peter L. Bartlett

University of California  Berkeley

Berkeley  CA 94720  USA

bartlett@cs.berkeley.edu

Abstract

We present an algorithm called Optimistic Linear Programming (OLP) for learn-
ing to optimize average reward in an irreducible but otherwise unknown Markov
decision process (MDP). OLP uses its experience so far to estimate the MDP. It
chooses actions by optimistically maximizing estimated future rewards over a set
of next-state transition probabilities that are close to the estimates  a computation
that corresponds to solving linear programs. We show that the total expected re-
ward obtained by OLP up to time T is within C(P ) log T of the reward obtained
by the optimal policy  where C(P ) is an explicit  MDP-dependent constant. OLP
is closely related to an algorithm proposed by Burnetas and Katehakis with four
key differences: OLP is simpler  it does not require knowledge of the supports
of transition probabilities  the proof of the regret bound is simpler  but our regret
bound is a constant factor larger than the regret of their algorithm. OLP is also
similar in ﬂavor to an algorithm recently proposed by Auer and Ortner. But OLP
is simpler and its regret bound has a better dependence on the size of the MDP.

1 Introduction

Decision making under uncertainty is one of the principal concerns of Artiﬁcial Intelligence and
Machine Learning. Assuming that the decision maker or agent is able to perfectly observe its own
state  uncertain systems are often modeled as Markov decision processes (MDPs). Given complete
knowledge of the parameters of an MDP  there are standard algorithms to compute optimal policies 
i.e.  rules of behavior such that some performance criterion is maximized. A frequent criticism of
these algorithms is that they assume an explicit description of the MDP which is seldom available.
The parameters constituting the description are themselves estimated by simulation or experiment
and are thus not known with complete reliability. Taking this into account brings us to the well
known exploration vs. exploitation trade-off. On one hand  we would like to explore the system as
well as we can to obtain reliable knowledge about the system parameters. On the other hand  if we
keep exploring and never exploit the knowledge accumulated  we will not behave optimally.
Given a policy π  how do we measure its ability to handle this trade-off? Suppose the agent gets a
numerical reward at each time step and we measure performance by the accumulated reward over
time. Then  a meaningful quantity to evaluate the policy π is its regret over time. To understand
what regret means  consider an omniscient agent who knows all parameters of the MDP accurately
and behaves optimally. Let VT be the expected reward obtained by this agent up to time T . Let V π
T
denote the corresponding quantity for π. Then the regret Rπ
T measures how much π is
hurt due to its incomplete knowledge of the MDP up to time T . If we can show that the regret Rπ
T
grows slowly with time T   for all MDPs in a sufﬁciently big class  then we can safely conclude that
π is making a judicious trade-off between exploration and exploitation. It is rather remarkable that

T = VT − V π

1

T = O(log T ). Thus the per-step regret Rπ

for this notion of regret  logarithmic bounds have been proved in the literature [1 2]. This means that
there are policies π with Rπ
T /T goes to zero very quickly.
Burnetas and Katehakis [1] proved that for any policy π (satisfying certain reasonable assumptions)
T ≥ CB(P ) log T where they identiﬁed the constant CB(P ). This constant depends on the tran-
Rπ
sition function P of the MDP1. They also gave an algorithm (we call it BKA) that achieves this rate
and is therefore optimal in a very strong sense. However  besides assuming that the MDP is irre-
ducible (see Assumption 1 below) they assumed that the support sets of the transition distributions
pi(a) are known for all state-action pairs. In this paper  we not only get rid of this assumption but
our optimistic linear programming (OLP) algorithm is also computationally simpler. At each step 
OLP considers certain parameters in the vicinity of the estimates. Like BKA  OLP makes optimistic
choices among these. But now  making these choices only involves solving linear programs (LPs)
to maximize linear functions over L1 balls. BKA instead required solving non-linear (though con-
vex) programs due to the use of KL-divergence. Another beneﬁt of using the L1 distance is that
it greatly simpliﬁes a signiﬁcant part of the proof. The price we pay for these advantages is that
the regret of OLP is C(P ) log T asymptotically  for a constant C(P ) ≥ CB(P ). We should note
here that a number of algorithms in the literature have been inspired by the “optimism in the face of
uncertainty” principle [3]–[7].
The algorithm of Auer and Ortner (we refer to it as AOA) is another logarithmic regret algorithm for
irreducible2 MDPs. AOA does not solve an optimization problem at every time step but only when
a conﬁdence interval is halved. But then the optimization problem they solve is more complicated
because they ﬁnd a policy to use in the next few time steps by optimizing over a set of MDPs. The
regret of AOA is CA(P ) log T where

|S|5|A|Tw(P )κ(P )2

 

CA(P ) = c

(1)
for some universal constant c. Here |S| |A| denote the state and action space size  Tw(P ) is the worst
case hitting time over deterministic policies (see Eqn. (12)) and ∆∗(P ) is the difference between
the long term average return of the best policy and that of the next best policy. The constant κ(P ) is
also deﬁned in terms of hitting times. Under Auer and Ortner’s assumption of bounded rewards  we
can show that the constant for OLP satisﬁes

∆∗(P )2

C(P ) ≤ 2|S||A|T (P )2

Φ∗(P )

.

(2)

Here T (P ) is the hitting time of an optimal policy is therefore necessarily smaller than Tw(P ). We
get rid of the dependence on κ(P ) while replacing Tw(P ) with T (P )2. Most importantly  we signif-
icantly improve the dependence on the state space size. The constant Φ∗(P ) can roughly be thought
of as the minimum (over states) difference between the quality of the best and the second best ac-
tion (see Eqn. (9)). The constants ∆∗(P ) and Φ∗(P ) are similar though not directly comparable.
Nevertheless  note that C(P ) depends inversely on Φ∗(P ) not Φ∗(P )2.

2 Preliminaries
Consider an MDP (S A  R  P ) where S is the set of states  A = ∪i∈SA(i) is the set of actions
(A(i) being the actions available in state i)  R = {r(i  a)}i∈S a∈A(i) are the rewards and P =
{pi j(a)}i j∈S a∈A(i) are the transition probabilities. For simplicity of analysis  we assume that
the rewards are known to us beforehand. We do not assume that we know the support sets of the
distributions pi(a).
The history σt up to time t is a sequence i0  k0  . . .   it−1  kt−1  it such that ks ∈ A(is) for all s < t.
A policy π is a sequence {πt} of probability distributions on A given σt such that πt(A(st)|σt) = 1
where st denotes the random variable representing the state at time t. The set of all policies is
denoted by Π. A deterministic policy is simply a function µ : S → A such that µ(i) ∈ A(i).
Denote the set of deterministic policies by ΠD. If D is a subset of A  let Π(D) denote the set of

1Notation for MDP parameters is deﬁned in Section 2 below.
2Auer & Ortner prove claims for unichain MDPs but their usage seems non-standard. The MDPs they call

unichain are called irreducible in standard textbooks (for example  see [9  p. 348])

2

policies that take actions in D. Probability and expectation under a policy π  transition function P
and starting state i0 will be denoted by Pπ P
respectively. Given history σt  let Nt(i) 
Nt(i  a) and Nt(i  a  j) denote the number of occurrences of the state i  the pair (i  a) and the triplet
(i  a  j) respectively in σt.
We make the following irreducibility assumption regarding the MDP.
Assumption 1. For all µ ∈ ΠD  the transition matrix P µ = (pi j(µ(i)))i j∈S is irreducible (i.e. it
is possible to reach any state from any other state).

and Eπ P

i0

i0

Consider the rewards accumulated by the policy π before time T  

T−1X

T (i0  P ) := Eπ P
V π

i0

[

r(st  at)]  

where at is the random variable representing the action taken by π at time t. Let VT (i0  P ) be the
maximum possible sum of expected rewards before time T  

t=0

VT (i0  P ) := sup
π∈Π

T (i0  P ) .
V π

The regret of a policy π at time T is a measure of how well the expected rewards of π compare with
the above quantity 

T (i0  P ) := VT (i0  P ) − V π
Rπ

T (i0  P ) .

Deﬁne the long term average reward of a policy π as

Under assumption 1  the above limit exists and is independent of the starting state i0. Given a
restricted set D ⊆ A of actions  the gain or the best long term average performance is

λπ(i0  P ) := lim inf
T→∞

T (i0  P )
V π

.

T

λ(P D) := sup
π∈Π(D)

λπ(i0  P ) .

As a shorthand  deﬁne λ∗(P ) := λ(P A).

2.1 Optimality Equations
A restricted problem (P D) is obtained from the original MDP by choosing subsets D(i) ⊆ A(i)
and setting D = ∪i∈SD(i). The transition and reward functions of the restricted problems are
simply the restrictions of P and r to D. Assumption 1 implies that there is a bias vector h(P D) =
{h(i; P D)}i∈S such that the gain λ(P D) and bias h(P D) are the unique solutions to the average
reward optimality equations:

[r(i  a) + hpi(a)  h(P D)i] .

∀i ∈ S  λ(P D) + h(i; P  D) = max
a∈D(i)

(3)
We will use h∗(P ) to denote h(P A). Also  denote the inﬁnity norm kh∗(P )k∞ by H∗(P ). Note
that if h∗(P ) is a solution to the optimality equations and e is the vector of ones  then h∗(P ) + ce
is also a solution for any scalar c. We can therefore assume ∃i∗ ∈ S  h∗(i∗; P ) = 0 without any loss
of generality.
It will be convenient to have a way to denote the quantity inside the ‘max’ that appears in the
optimality equations. Accordingly  deﬁne

L(i  a  p  h) := r(i  a) + hp  hi  

L∗(i; P D) := max
a∈D(i)

L(i  a  pi(a)  h(P  D)) .

To measure the degree of suboptimality of actions available at a state  deﬁne
φ∗(i  a; P ) = L∗(i; P A) − L(i  a  pi(a)  h∗(P )) .

Note that the optimal actions are precisely those for which the above quantity is zero.

Any policy in O(P D) is an optimal policy  i.e. 

O(i; P D) := {a ∈ D(i) : φ∗(i  a; P ) = 0}  

O(P D) := Πi∈SO(i; P D) .

∀µ ∈ O(P D)  λµ(P ) = λ(P D) .

3

2.2 Critical pairs

From now on  ∆+ will denote the probability simplex of dimension determined by context. For a
suboptimal action a /∈ O(i; P A)  the following set contains probability distributions q such that if
pi(a) is changed to q  the quality of action a comes within  of an optimal action. Thus  q makes a
look almost optimal:

MakeOpt(i  a; P  ) := {q ∈ ∆+ : L(i  a  q  h∗(P )) ≥ L∗(i; P A) − } .

(4)
Those suboptimal state-action pairs for which MakeOpt is never empty  no matter how small  is 
play a crucial role in determining the regret. We call these critical state-action pairs 

Crit(P ) := {(i  a) : a /∈ O(i; P A) ∧ (∀ > 0  MakeOpt(i  a; P  ) 6= ∅)} .

(5)

Deﬁne the function 

Ji a(p; P  ) := inf{kp − qk2

1 : q ∈ MakeOpt(i  a; P  )} .

(6)
To make sense of this deﬁnition  consider p = pi(a). The above inﬁmum is then the least distance
(in the L1 sense) one has to move away from pi(a) to make the suboptimal action a look -optimal.
Taking the limit of this as  decreases gives us a quantity that also plays a crucial role in determining
the regret 

(7)
Intuitively  if K(i  a; P ) is small  it is easy to confuse a suboptimal action with an optimal one and
so it should be difﬁcult to achieve small regret. The constant that multiplies log T in the regret bound
of our algorithm OLP (see Algorithm 1 and Theorem 4 below) is the following:

K(i  a; P ) := lim
→0

Ji a(pi(a); P  ) .

C(P ) := X

(i a)∈Crit(P )

2φ∗(i  a; P )
K(i  a; P ) .

(8)

This deﬁnition might look a bit hard to interpret  so we give an upper bound on C(P ) just in terms
of the inﬁnity norm H∗(P ) of the bias and Φ∗(P ). This latter quantity is deﬁned below to be the
minimum degree of suboptimality of a critical action.
Proposition 2. Suppose A(i) = A for all i ∈ S. Deﬁne
(i a)∈Crit(P )

φ∗(i  a; P ) .

Φ∗(P ) :=

min

(9)

Then  for any P  

See the appendix for a proof.

2.3 Hitting times

C(P ) ≤ 2|S||A|H∗(P )2

Φ∗(P )

.

It turns out that we can bound the inﬁnity norm of the bias in terms of the hitting time of an optimal
policy. For any policy µ deﬁne its hitting time to be the worst case expected time to reach one state
from another:

Tµ(P ) := max
i6=j

Eµ P
j

[min{t > 0 : st = i}] .

The following constant is the minimum hitting time among optimal policies:

T (P ) := min

µ∈O(P D)

Tµ(P ) .

(11)

The following constant is deﬁned just for comparison with results in [2]. It is the worst case hitting
time over all policies:

(10)

(12)

We can now bound C(P ) just in terms of the hitting time T (P ) and φ∗(P ).
Proposition 3. Suppose A(i) = A for all i ∈ S and that r(i  a) ∈ [0  1] for all i ∈ S  a ∈ A. Then
for any P  

Tw(P ) := max
µ∈ΠD

Tµ(P ) .

C(P ) ≤ 2|S||A|T (P )2

.

Φ∗(P )

See the appendix for a proof.

4

3 The optimistic LP algorithm and its regret bound

Algorithm 1 Optimistic Linear Programming
1: for t = 0  1  2  . . . do
st ← current state
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for

end if

else

t

t

i j(a) ← 1+Nt(i a j)
|A(i)|+Nt(i a)

. Compute solution for “empirical MDP” excluding “undersampled” actions
∀i  j ∈ S  a ∈ A(i)  ˆpt
∀i ∈ S  Dt(i) ← {a ∈ A(i) : Nt(i  a) ≥ log2 Nt(i)}
ˆht  ˆλt ← solution of the optimality equations (3) with P = ˆP t D = Dt
. Compute indices of all actions for the current state
∀a ∈ A(st)  Ut(st  a) ← supq∈∆+{r(st  a) + hq  ˆhti : kˆpt

Nt(st a)}
. Optimal actions (for the current problem) that are about to become “undersampled”
t ← {a ∈ O(st; ˆP t Dt) : Nt(st  a) < log2(Nt(st) + 1)}
Γ1
. The index maximizing actions
t ← arg maxa∈A(st) Ut(st  a)
Γ2
t = O(st; ˆP t Dt) then
if Γ1
at ← any action in Γ1
at ← any action in Γ2

(a) − qk1 ≤q 2 log t

st

Algorithm 1 is the Optimistic Linear Programming algorithm. It is inspired by the algorithm of
Burnetas and Katehakis [1] but uses L1 distance instead of KL-divergence. At each time step t 
the algorithm computes the empirical estimates for transition probabilities. It then forms a restricted
problem ignoring relatively undersampled actions. An action a ∈ A(i) is considered “undersam-
pled” if Nt(i  a) < log2 Nt(i). The solutions ˆht  ˆλt might be misleading due to estimation errors.
To avoid being misled by empirical samples we compute optimistic “indices” Ut(st  a) for all legal
actions a ∈ A(st) where st is the current state. The index for action a is computed by looking at
(a) and choosing a probability distribution q that max-
an L1-ball around the empirical estimate ˆpt
st
imizes L(i  a  q  ˆht). Note that if the estimates were perfect  we would take an action maximizing
L(i  a  ˆpt
(a)  ˆht). Instead  we take an action that maximizes the index. There is one case where we
are forced not to take an index-maximizing action. It is when all the optimal actions of the current
problem are about to become undersampled at the next time step. In that case  we take one of these
actions (steps 18–22). Note that both steps 7 and 10 can be done by solving LPs. The LP for solving
optimality equations can be found in several textbooks (see  for example  [9  p. 391]). The LP in step
10 is even simpler: the L1 ball has only 2|S| vertices and so we can maximize over them efﬁciently.
Like the original Burnetas-Katehakis algorithm  the modiﬁed one also satisﬁes a logarithmic regret
bound as stated in the following theorem. Unlike the original algorithm  OLP does not need to know
the support sets of the transition distributions.
Theorem 4. Let β denote the policy implemented by Algorithm 1. Then we have  for all i0 ∈ S and
for all P satisfying Assumption 1 

st

lim sup
T→∞

Rβ

T (i0  P )
log T

≤ C(P )  

where C(P ) is the MDP-dependent constant deﬁned in (8).

Proof. From Proposition 1 in [1]  it follows that

[NT (i  a)]φ∗(i  a; P ) + O(1) .

(13)

T (i0  P ) =X

Rβ

X

i∈S

a /∈O(i;P A)

Eβ P
i0

5

Deﬁne the event

Deﬁne 

At := {kˆht − h∗(P )k∞ ≤  ∧ O( ˆP t Dt) ⊆ O(P )} .

(14)

N 1

T (i  a; ) :=

N 2

T (i  a; ) :=

N 3

T () :=

1 [(st  at) = (i  a) ∧ At ∧ Ut(i  a) ≥ L∗(i; P A) − 2]  

1 [(st  at) = (i  a) ∧ At ∧ Ut(i  a) < L∗(i; P A) − 2]  

1(cid:2) ¯At

(cid:3)  

t=0

T−1X
T−1X
T−1X

t=0

t=0

where ¯At denotes the complement of At. For all  > 0 
T (i  a; ) + N 2

NT (i  a) ≤ N 1

(15)
The result then follows by combining (13) and (15) with the following three propositions and then
letting  → 0 sufﬁciently slowly.
Proposition 5. For all P and i0 ∈ S  we have
Eβ P
i0

T (i  a; ) + N 3

T (i  a; )]

X

X

T () .

φ∗(i  a; P ) ≤ C(P ) .

lim
→0

lim sup
T→∞

i∈S

a /∈O(i;P A)

[N 1
log T

Proposition 6. For all P   i0  i ∈ S  a /∈ O(i; P A) and  sufﬁciently small  we have

Proposition 7. For all P satisfying Assumption 1  i0 ∈ S and  > 0  we have

Eβ P
i0

[N 2

T (i  a; )] = o(log T ) .

Eβ P
i0

[N 3

T ()] = o(log T ) .

4 Proofs of auxiliary propositions

We prove Propositions 5 and 6. The proof of Proposition 7 is almost the same as that of Proposition 5
in [1] and therefore omitted (for details  see Chapter 6 in the ﬁrst author’s thesis [8]). The proof of
Proposition 6 is considerably simpler (because of the use of L1 distance rather than KL-divergence)
than the analogous Proposition 4 in [1].
Proof of Proposition 5. There are two cases depending on whether (i  a) ∈ Crit(P ) or not.
If
(i  a) /∈ Crit(P )  there is an 0 > 0 such that MakeOpt(i  a; P  0) = ∅. On the event At (recall the
deﬁnition given in (14))  we have |hq  ˆhti − hq  h∗(P )i| ≤  for any q ∈ ∆+. Therefore 

{r(i  a) + hq  ˆhti}
{r(i  a) + hq  h∗(P )i} + 

Ut(i  a) ≤ sup
q∈∆+
≤ sup
q∈∆+
< L∗(i; P A) − 0 + 
< L∗(i; P A) − 2 provided that 3 < 0

[∵ MakeOpt(i  a; P  0) = ∅]

Therefore for  < 0/3  N 1
Now suppose (i  a) ∈ Crit(P ). The event Ut(i  a) ≥ L∗(i; P A) − 2 is equivalent to

T (i  a; ) = 0.

∃q ∈ ∆+ s.t.

kˆpt

i(a) − qk2

r(i  a) + hq  ˆhti ≥ L∗(i; P A) − 2

.

(cid:17)

On the event At  we have |hq  ˆhti − hq  h∗(P )i| ≤  and thus the above implies

∃q ∈ ∆+ s.t.

kˆpt

i(a) − qk2

1 ≤ 2 log t
Nt(i  a)

∧ (r(i  a) + hq  h∗(P )i ≥ L∗(i; P A) − 3) .

(cid:18)
(cid:18)

∧(cid:16)

(cid:19)
1 ≤ 2 log t
Nt(i  a)
(cid:19)

6

Recalling the deﬁnition (6) of Ji a(p; P  )  we see that this implies

Ji a(ˆpt

i(a); P  3) ≤ 2 log t
Nt(i  a) .

We therefore have 

N 1

1

T (i  a; ) ≤ T−1X
≤ T−1X
T−1X

t=0

t=0

1

+

(cid:21)

(st  at) = (i  a) ∧ Ji a(ˆpt

(cid:20)
i(a); P  3) ≤ 2 log t
(cid:20)
Nt(i  a)
(st  at) = (i  a) ∧ Ji a(pi(a); P  3) ≤ 2 log t
1(cid:2)(st  at) = (i  a) ∧ Ji a(pi(a); P  3) > Ji a(ˆpt
Nt(i  a)

+ δ

(cid:21)
i(a); P  3) + δ(cid:3)

t=0

(16)

where δ > 0 is arbitrary. Each time the pair (i  a) occurs Nt(i  a) increases by 1  so the ﬁrst count
is no more than

2 log T

Ji a(pi(a); P  3) − δ

.

(17)

To control the expectation of the second sum  note that continuity of Ji a in its ﬁrst argument implies
that there is a function f such that f(δ) > 0 for δ > 0  f(δ) → 0 as δ → 0 and Ji a(pi(a); P  3) >
i(a)k1 > f(δ). By a Chernoff-type bound  we have 
Ji a(ˆpt
for some constant C1 

i(a); P  3) + δ implies that kpi(a) − ˆpt

Pβ P
i0

[kpi(a) − ˆpt

i(a)k1 > f(δ)| Nt(i  a) = m] ≤ C1 exp(−mf(δ)2) .

and so the expectation of the second sum is no more than

Eβ P
i0

[

C1 exp(−Nt(i  a)f(δ)2)] ≤

t=0

m=1

C1 exp(−mf(δ)2) =

C1

1 − exp(−f(δ)2) .

(18)

Combining the bounds (17) and (18) and plugging them into (16)  we get

Eβ P
i0

[N 1

T (i  a; )] ≤

Ji a(pi(a); P  3) − δ
Letting δ → 0 sufﬁciently slowly  we get that for all  > 0 
2 log T

2 log T

Eβ P
i0

[N 1

T (i  a; )] ≤

Ji a(pi(a); P  3)

+ o(log T ) .

+

C1

1 − exp(−f(δ)2) .

Therefore 

lim
→0

lim sup
T→∞

Eβ P
i0

T (i  a; )]

[N 1
log T

≤ lim
→0

2

Ji a(pi(a); P  3)

=

2

K(i  a; P )  

where the last equality follows from the deﬁnition (7) of K(i  a; P ). The result now follows by
summing over (i  a) pairs in Crit(P ).

T−1X

∞X

Proof of Proposition 6. Deﬁne the event

t(i  a; ) := {(st  at) = (i  a) ∧ At ∧ Ut(i  a) < L∗(i; P A) − 2}  
A0

so that we can write

Note that on A0
taken at time t  so it must have been in Γ2
optimal actions a∗ ∈ O(i; P A)  we have  on the event A0

t(i  a; )  we have Γ1

t(i  a; ) 

t(i  a; )] .

T (i  a; ) =
(19)
N 2
t ⊆ O(i; ˆP t Dt) ⊆ O(i; P A). So  a /∈ O(i; P A). But a was
t which means it maximized the index. Therefore  for all

t=0

Ut(i  a∗) ≤ Ut(i  a) < L∗(i; P A) − 2 .

T−1X

1 [A0

7

Since L∗(i; P A) = r(i  a∗) + hpi(a∗)  h∗(P )i  this implies

s

∀q ∈ ∆+  kq − ˆpt

i(a∗)k1 ≤

⇒ hq  ˆhti < hpi(a∗)  h∗(P )i − 2 .

2 log t
Nt(i  a∗)

Moreover  on the event At  |hq  ˆhti − hq  h∗(P )i| ≤ . We therefore have  for any a∗ ∈ O(i; P A) 

t(i  a; ) ⊆
A0

∀q ∈ ∆+  kq − ˆpt

i(a)k1 ≤

⇒ hq  h∗(P )i < hpi(a)  h∗(P )i − 

)

)

⊆

(
(
(
⊆ t[

⊆

m=1

s
s

2 log t
Nt(i  a)

2 log t
Nt(i  a)

s

)

∀q ∈ ∆+  kq − ˆpt

i(a)k1 ≤

⇒ kq − pi(a)k1 >



kh∗(P )k∞

kˆpt

i(a) − pi(a)k1 >
(



h∗(P )

+

2 log t
Nt(i  a)

Nt(i  a) = m ∧ kˆpt

i(a) − pi(a)k1 >



kh∗(P )k∞

+

s

)

2 log t
Nt(i  a)

Using a Chernoff-type bound  we have  for some constant C1 

Pβ P
i0

[kˆpt

i(a) − pi(a)k1 > δ | Nt(i  a) = m] ≤ C1 exp(−mδ2/2) .

Using a union bound  we therefore have 

t(i  a; )] ≤ tX

Pβ P
i0

[A0

 

− m
(cid:18)

2



kh∗(P )k∞

C1 exp

m=1

≤ C1
t

∞X

m=1

exp

− m2
2kh∗(P )k2∞

!2
r2 log t
(cid:19)

m

+
√
2m log t
− 
kh∗(P )k∞

= o

(cid:18)1

(cid:19)

t

.

Combining this with (19) proves the result.

References

[1] Burnetas  A.N. & Katehakis  M.N. (1997) Optimal adaptive policies for Markov decision processes. Math-
ematics of Operations Research 22(1):222–255
[2] Auer  P. & Ortner  R. (2007) Logarithmic online regret bounds for undiscounted reinforcement learning.
Advances in Neural Information Processing Systems 19. Cambridge  MA: MIT Press.
[3] Lai  T.L. & Robbins  H. (1985) Asymptotically efﬁcient adaptive allocation rules. Advances in Applied
Mathematics 6(1):4–22.
[4] Brafman  R.I. & Tennenholtz  M. (2002) R-MAX - a general polynomial time algorithm for near-optimal
reinforcement learning. Journal of Machine Learning Research 3:213–231.
[5] Auer  P. (2002) Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learn-
ing Research 3:397–422.
[6] Auer  P.  Cesa-Bianchi  N. & and Fischer  P. (2002) Finite-time analysis of the multiarmed bandit problem.
Machine Learning 47(2-3):235-256.
[7] Strehl  A.L. & Littman  M. (2005) A theoretical analysis of model-based interval estimation. In Proceedings
of the Twenty-Second International Conference on Machine Learning  pp. 857-864. ACM Press.
[8] Tewari  A. (2007) Reinforcement Learning in Large or Unknown MDPs. PhD thesis  Department of Elec-
trical Engineering and Computer Sciences  University of California at Berkeley.
[9] Puterman  M.L. (1994) Markov Decision Processes: Discrete Stochastic Dynamic Programming. New
York: John Wiley and Sons.

8

,Ying Yang
Elissa Aminoff
Michael Tarr
Kass Robert
Brett Daley
Christopher Amato