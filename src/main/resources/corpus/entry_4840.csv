2019,Sample Complexity of Learning Mixture of Sparse Linear Regressions,In the problem of learning mixtures of linear regressions  the goal is to learn a col-lection of signal vectors from a sequence of (possibly noisy) linear measurements where each measurement is evaluated on an unknown signal drawn uniformly fromthis collection. This setting is quite expressive and has been studied both in termsof practical applications and for the sake of establishing theoretical guarantees. Inthis paper  we consider the case where the signal vectors aresparse; this generalizesthe popular compressed sensing paradigm. We improve upon the state-of-the-artresults as follows: In the noisy case  we resolve an open question of Yin et al. (IEEETransactions on Information Theory  2019) by showing how to handle collectionsof more than two vectors and present the first robust reconstruction algorithm  i.e. if the signals are not perfectly sparse  we still learn a good sparse approximationof the signals. In the noiseless case  as well as in the noisy case  we show how tocircumvent the need for a restrictive assumption required in the previous work. Ourtechniques are quite different from those in the previous work: for the noiselesscase  we rely on a property of sparse polynomials and for the noisy case  we providenew connections to learning Gaussian mixtures and use ideas from the theory of,Sample Complexity of Learning Mixtures of

Sparse Linear Regressions

Akshay Krishnamurthy
Microsoft Research  NYC
akshay@cs.umass.edu

Andrew McGregor

UMass Amherst

mcgregor@cs.umass.edu

Arya Mazumdar
UMass Amherst

arya@cs.umass.edu

Soumyabrata Pal
UMass Amherst

spal@cs.umass.edu

Abstract

In the problem of learning mixtures of linear regressions  the goal is to learn a col-
lection of signal vectors from a sequence of (possibly noisy) linear measurements 
where each measurement is evaluated on an unknown signal drawn uniformly from
this collection. This setting is quite expressive and has been studied both in terms
of practical applications and for the sake of establishing theoretical guarantees. In
this paper  we consider the case where the signal vectors are sparse; this generalizes
the popular compressed sensing paradigm. We improve upon the state-of-the-art
results as follows: In the noisy case  we resolve an open question of Yin et al. (IEEE
Transactions on Information Theory  2019) by showing how to handle collections
of more than two vectors and present the ﬁrst robust reconstruction algorithm  i.e. 
if the signals are not perfectly sparse  we still learn a good sparse approximation
of the signals. In the noiseless case  as well as in the noisy case  we show how to
circumvent the need for a restrictive assumption required in the previous work. Our
techniques are quite different from those in the previous work: for the noiseless
case  we rely on a property of sparse polynomials and for the noisy case  we provide
new connections to learning Gaussian mixtures and use ideas from the theory of
error correcting codes.

1

Introduction

Learning mixtures of linear regressions is a natural generalization of the basic linear regression
problem. In the basic problem  the goal is to learn the best linear relationship between the scalar
responses (i.e.  labels) and the explanatory variables (i.e.  features). In the generalization  each scalar
response is stochastically generated by picking a function uniformly from a set of L unknown linear
functions  evaluating this function on the explanatory variables and possibly adding noise; the goal
is to learn the set of L unknown linear functions. The problem was introduced by De Veaux [11]
over thirty years ago and has recently attracted growing interest [8  14  22  24  25  27]. Recent
work focuses on a query-based scenario in which the input to the randomly chosen linear function
can be speciﬁed by the learner. The sparse setting  in which each linear function depends on only
a small number of variables  was recently considered by Yin et al. [27]  and can be viewed as a
generalization of the well-studied compressed sensing problem [7  13]. The problem has numerous
applications in modelling heterogeneous data arising in medical applications  behavioral health  and
music perception [27].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Formal Problem Statement. There are L unknown distinct vectors 1  2  . . .   L 2 Rn and
each is k-sparse  i.e.  the number of non-zero entries in each i is at most k where k is some known
parameter. We deﬁne an oracle O which  when queried with a vector x 2 Rn  returns the noisy
output y 2 R:

y = hx  i + ⌘

(1)
where ⌘ is a random variable with E⌘ = 0 that represents the measurement noise and  is chosen
uniformly1 from the set B = {1  2  . . .   L}. The goal is to recover all vectors in B by making
a set of queries x1  x2  . . .   xm to the oracle. We refer to the values returned by the oracle given
these queries as samples. Note that the case of L = 1 corresponds to the problem of compressed
sensing. Our primary focus is on the sample complexity of the problem  i.e.  minimizing the number
of queries that sufﬁces to recover the sparse vectors up to some tolerable error.

Related Work. The most relevant previous work is by Yin et al. [27]. For the noiseless case  i.e. 
⌘ = 0  they show that O(kL log(kL)) queries are sufﬁcient to recover all vectors in B with high
probability. However  their result requires a restrictive assumption on the set of vectors and do not
hold for an arbitrary set of sparse vectors. Speciﬁcally  they require that for any    0 2B  

for each

j 6= 0j

j 2 supp() \ supp(0) .

(2)
Their approach depends crucially on this assumption and this limits the applicability of their approach.
Note that our results will not depend on such an assumption. For the noisy case  the approach taken
by Yin et al. only handles the L = 2 case and they state the case of L > 2 as an important open
problem. Resolving this open problem will be another one of our contributions.
More generally  both compressed sensing [7  13] and learning mixtures of distributions [10  23]
are immensely popular topics across statistics  signal processing and machine learning with a large
body of prior work. Mixture of linear regressions is a natural synthesis of mixture models and
linear regression  a very basic machine learning primitive [11]. Most of the work on the problem
has considered learning generic vectors  i.e.  not necessary sparse  and they propose a variety of
algorithmic techniques to obtain polynomial sample complexity [8  14  19  24  26]. To the best of our
knowledge  Städler et al. [22] were the ﬁrst to impose sparsity on the solutions. However  many of
the earlier papers on mixtures of linear regression  essentially consider the queries to be ﬁxed  i.e. 
part of the input  whereas in this paper  and in Yin et al. [27]  we are interested in designing queries
in such a way to minimize the number of queries.

Our Results and Techniques. We present results for both the noiseless and noisy cases. The latter
is signiﬁcantly more involved and is the main technical contribution of this paper.
Noiseless Case: In the case where there is no noise and the L unknown vectors are k-sparse  we show
that O(kL log(kL)) queries sufﬁce and that ⌦(kL) queries are necessary. The upper bound matches
the query complexity of the result by Yin et al. but our result applies for all k-sparse vectors rather
than just those satisfying the assumption in Eq. 2. The approach we take is as follows: In compressed
sensing  exact recovery of k-sparse vectors is possible by taking samples with an m ⇥ n matrix with
any 2k columns linearly independent. Such matrices exists with m = 2k (such as Vandermonde
matrices) and are called MDS matrices. We use rows of such a matrix repeatedly to generate samples.
Since there are L different vectors in the mixture  with O(L log L) measurements with a row we will
be able to see the samples corresponding to each of the L vectors with that row. However  even if this
is true for measurements with each rows  we will still not be able to align measurements across the
rows. For example  even though we will obtain hx  `i for all ` 2 [L] and for all x that are rows of an
MDS matrix  we will be unable to identify the samples corresponding to 1. To tackle this problem 
we propose using a special type of MDS matrix that allows us to align measurements corresponding
to the same s. After that  we just use the sparse recovery property of the MDS matrix to individually
recover each of the vectors.
Noisy Case: We assume that the noise ⌘ is a Gaussian random variable with zero mean. Going forward 
we write N (µ  2) to denote a Gaussian distribution with mean µ and variance 2. Furthermore  we
will no longer assume vectors in B are necessarily sparse. From the noisy samples  our objective is to
1Many of our results can be generalized to non-uniform distributions but we will assume a uniform distribution

throughout for the sake of clarity.

2

recover an estimate ˆ for each  2B such that

k  ˆk  ck  ⇤k 

(3)
where c is an absolute constant and ⇤ is the best k-sparse approximation of   i.e.  all except the
largest (by absolute value) k coordinates set to 0. The norms in the above equation can be arbitrary
deﬁning the strength of the guarantee  e.g.  when we refer to an `1/`1 guarantee both norms are k·k 1.
Our results should be contrasted with [27]  where results not only hold for only L = 2 and under
assumption (2)  but the vectors are also strictly k-sparse. However  like [27]  we assume ✏-precision
of the unknown vectors  i.e.  the value in each coordinate of each  2B is an integer multiple of ✏.2
Notice that in this model the noise is additive and not multiplicative. Hence  it is possible to increase
the `2 norm of the queries arbitrarily so that the noise becomes inconsequential. However  in a real
setting  this cannot be allowed since increasing the strength (norm) of the queries has a cost and it
is in our interest to minimize the cost. Suppose the algorithm designs the ith query vector by ﬁrst
choosing a distribution Qi and subsequently sampling a query vector xi ⇠ Qi. Let us now deﬁne the
signal to noise ratio as follows:

Exi⇠Qi|hxi  `i|2

.

i

min

`

E⌘2

SNR = max

(4)
Our objective in the noisy setting is to recover the unknown vectors 1  2  . . .   L 2 Rn while
minimizing the number of queries and the SNR at the same time. In this setting  assuming that
all the unknown vectors have unit norm  we show that O(k log3 n exp((/✏)2/3)) queries with
SNR = O(1/2) sufﬁce to reconstruct the L = O(1) vectors in B with the approximation guarantees
given in Eq. (3) with high probability if the noise ⌘ is a zero mean gaussian with a variance of 2.
This is equivalent to stating that O(k log3 n exp(1/(✏pSNR)2/3)) queries sufﬁce to recover the L
unknown vectors with high probability.
Note that in the previous work ✏pSNR is assumed to be at least constant and  if this is the case 
our result is optimal up to polynomial factors since ⌦(k) queries are required even if L = 1. More
generally  the dependence upon ✏pSNR in our result improves upon the dependence in the result by
Yin et al. Note that we assumed L = O(1) in our result because the dependence of sample complexity
on L is complicated as it is implicit in the signal-to-noise ratio.
As in noiseless case  our approach is to use a compressed sensing matrix and use its rows multiple
time as queries to the oracle. At the ﬁrst step  we would like to separate out the different s from
their samples with the same rows. Unlike the noiseless case  even this turns out to be a difﬁcult task.
Under the assumption of Gaussian noise  however  we are able to show that this is equivalent to
learning a mixture of Gaussians with different means. In this case  the means of the Gaussians belong
to an “✏-grid"  because of the assumption on the precision of s. This is not a standard setting in the
literature of learning Gaussian mixtures  e.g.  [1  16  20]. Note that  this is possible if the vector that
we are sampling with has integer entries. As we will see a binary-valued compressed sensing matrix
will do the job for us. We will rely on a novel complex-analytic technique to exactly learn the means
of a mixture of Gaussians  with means belonging to an ✏-grid. This technique is paralleled by the
recent developments in trace reconstructions where similar methods were used for learning a mixture
of binomials [18  21].
Once for each query  the samples are separated  we are still tasked with aligning them so that we
know the samples produced by the same  across different queries. The method for the noiseless case
fails to work here. Instead  we use a new method motivated by error-correcting codes. In particular 
we perform several redundant queries  that help us to do this alignment. For example  in addition to
the pair of queries xi  xj  we also perform the queries deﬁned by xi + xj and xi  xj.
After the alignment  we use the compressed sensing recovery to estimate the unknown vectors. For
this  we must start with a matrix that with minimal number of rows  will allow us to recover any
vector with a guarantee such as (3). On top of this  we also need the matrix to have integer entries so
that we can use our method of learning a mixture of Gaussians with means on an ✏-grid. Fortunately 
a random binary ±1 matrix satisﬁes all the requirements [3]. Putting now these three steps of learning
mixtures  aligning and compressed sensing  lets us arrive at our results.
While we concentrate on sample complexity in this paper  our algorithm for the noiseless case is
computationally efﬁcient  and the only computationally inefﬁcient step in the general noisy case is

2Note that we do not assume ✏-precision in the noiseless case.

3

Algorithm 1 Noiseless Recovery The algorithm for extracting recovering vectors via queries to
oracle in noiseless setting.
Require: Number of unknown sparse vectors L  dimension n  sparsity k.
1: Let t 2R {0  1  2  . . .   k2L2  1} and deﬁne ↵1 ↵ 2  . . .  ↵ 2k where ↵j = 2kt+j
2k3L2 .
2: for i = 1  2  . . .   2k do
3: Make L log(Lk2) oracle queries with vector [1 ↵i ↵2
i
4: end for
5: for i = 1  2  . . .   2k do
6:

For each batch of query responses corresponding to the same query vector  retain unique values
and sort them in ascending order. Refer to this as the processed batch.

]. Refer to these as a batch.

. . . ↵ n1

i

j

the query vector [1 ↵j ↵2

7: end for
8: Set matrix Q of dimension 2k ⇥ L such that its jth row is the processed batch corresponding to
9: for i = 1  2  . . .   L do
10:
11: end for
12: Return 1  2  . . .   L.

Decode the ith column of the matrix Q to recover i.

j . . . ↵ n1

]

that of learning Gaussian mixtures. However  in practice one can perform a simple clustering (such
as Lloyd’s algorithm) to learn the means of the mixture.

Organization and Notation.
In Section 2  we present our results for the noiseless case. In Section
3.1 we consider the case with noise when L = 2 and then consider noise and general L in Section
3.2. Most proofs are deferred to the appendix in the supplementary material. Throughout  we write
x 2R X to denote taking an element x from a ﬁnite set X uniformly at random. For n 2 N  let
[n] := {1  2  . . .   n}.
2 Exact sparse vectors and noiseless samples

To begin  we deal with the case of uniform mixture of exact sparse vectors with the oracle returning
noiseless answers when queried with a vector. For this case  our scheme is provided in Algorithm 1.
The main result for this section is the following.
Theorem 1. For a collection of L vectors 1  2  . . .   L 2 Rn such that kik0  k 8i 2 [L]  one
can recover all of them exactly with probability at least 1  3/k with a total of 2kL log Lk2 oracle
queries. See Algorithm 1.

A Vandermonde matrix is a matrix such that the entries in each row of the matrix are in geometric
progression i.e.  for an m ⇥ n dimensional Vandermonde matrix the entry in the (i  j)th entry is
↵j
i where ↵1 ↵ 2  . . .  ↵ m 2 R are distinct values. We will use the following useful property of the
Vandermonde matrices; see  e.g.  [15  Section XIII.8] for the proof.
Lemma 1. The rank of any m ⇥ m square submatrix of a Vandermonde matrix is m assuming
↵1 ↵ 2  . . .  ↵ m are distinct and positive.
This implies that  with the samples from a 2k ⇥ n Vandermonde matrix  a k-sparse vector can be
exactly recovered. This is because for any two unknown vectors  and ˆ  the same set of responses
for all the 2k rows of the Vandermonde matrix implies that a 2k ⇥ 2k square submatrix of the
Vandermonde matrix is not full rank which is a contradiction to Lemma 1.
We are now ready to prove Theorem 1.

Proof. For the case of L = 1  note that the setting is the same as the well-known compressed sensing
problem. Furthermore  suppose a 2k ⇥ n matrix has the property that any 2k ⇥ 2k submatrix is full
rank  then using the rows of this matrix as queries is sufﬁcient to recover any k-sparse vector. By
Lemma 1  any 2k ⇥ n Vandemonde matrix has the necessary property.
Let 1  2  . . .   L be the set of unknown k-sparse vectors. Notice that a particular row of the
Vandermonde matrix looks like [1 z z2 z3 . . . zn1] for some value of z 2 R. Therefore  for

4

j=0 i

some vector i and a particular row of the Vandermonde matrix  the inner product of the two can be
interpreted as a degree n polynomial evaluated at z such that the coefﬁcients of the polynomial form
the vector i. More formally  the inner product can be written as f i(z) =Pn1
jzj where f i is
the polynomial corresponding to the vector i. For any value z 2 Rn  we can deﬁne an ordering over
the L polynomials f 1  f 2  . . .   f L such that f i > f j iff f i(z) > f j(z).
For two distinct indices i  j 2 [L]  we will call the polynomial f i  f j a difference polynomial.
Each difference polynomial has at most 2k non-zero coefﬁcients and therefore has at most 2k
positive roots by Descartes’ Rule of Signs [9]. Since there are at most L(L  1)/2 distinct difference
polynomials  the total number of distinct values that are roots of at least one difference polynomial
is less than kL2. Note that if an interval does not include any of these roots  then the ordering of
f 1  . . .   f L remains consistent for any point in that interval. In particular  consider the intervals
(0  ]  (  2]  . . .   (1    1] where  = 1/(k2L2). At most kL2 of these intervals include a root
of a difference polynomial and hence if we pick a random interval then with probability at least
1  1/k  the ordering of f 1  . . .   f L are consistent throughout the interval. If the interval chosen is
(t  (t + 1)] then set ↵j = t + j/ (2k) for j = 1  . . .   2k.
Now for each value of ↵i  deﬁne the vector xi ⌘ [1 ↵i ↵2
]. For each i 2 [2k]  the
vector xi will be used as query to the oracle repeatedly for L log Lk2 times. We will call the set of
query responses from the oracle for a ﬁxed query vector xi a batch. For a ﬁxed batch and j 
Pr(j is not sampled by the oracle in the batch) ⇣1 
1
Lk2 .
Taking a union bound over all the vectors (L of them) and all the batches (2k of them)  we get that in
every batch every vector j for j 2 [L] is sampled with probability at least 1  2/k. Now  for each
batch  we will retain the unique values (there should be exactly L of them with high probability) and
sort the values in each batch. Since the ordering of the polynomial remains same  after sorting  all
the values in a particular position in each batch correspond to the same vector j for some unknown
index j 2 [L]. We can aggregate the query responses of all the batches in each position and since
there are 2k linear measurements corresponding to the same vector  we can recover all the unknown
vectors j using Lemma 1. The failure probability of this algorithm is at most 3/k.

L⌘L log Lk2

 e log Lk2

=

i ↵3
i

. . . ↵ n1

i

1

The following theorem establishes that our method is almost optimal in terms of sample complexity.
Theorem 2. At least 2Lk oracle queries are necessary to recover an arbitrary set of L vectors that
are k-sparse.

3 Noisy Samples and Sparse Approximation

We now consider the more general setting where the oracle is noisy and the vectors 1  . . .   L are
not necessarily sparse. We assume L is an arbitrary constant  i.e.  it does not grow with n or k and
that the unknown vectors have ✏ precision  i.e.  each entries is an integer multiple of ✏. The noise will
be Gaussian with zero mean and variance 2  i.e.  ⌘ ⇠N (0  2). Our main result of this section is
the following.
Theorem 3. It is possible to recover approximations with the `1/`1 guarantee in Eq. (3) with
probability at least 1  2/n of all the unknown vectors ` 2{ 0 ±✏ ±2✏ ±3✏  . . .}n ` = 1  . . .   L
with O(k(log3 n) exp((/✏)2/3) oracle queries where SNR = O(1/2).

Before we proceed with the ideas of proof  it would be useful to recall the restricted isometry property
(RIP) of matrices in the context of recovery guarantees of (3). A matrix  2 IRm⇥n satisﬁes the
(k  )-RIP if for any vector z 2 IRn with kzk0  k 
(5)
2  kzk2
It is known that if a matrix is (2k  )-RIP with < p2  1  then the guarantee of (3) (in particular 
`1/`1-guarantee and also an `2/`1-guarantee) is possible [6] with the the basis pursuit algorithm  an
efﬁcient algorithm based on linear programming. It is also known that a random ±1 matrix (with
normalized columns) satisﬁes the property with csk log n rows  where cs is an absolute constant [3].
There are several key ideas of the proof. Since the case of L = 2 is simpler to handle  we start with
that and then provide the extra steps necessary for the general case subsequently.

2  (1 + )kzk2
2.

(1  )kzk2

5

Algorithm 2 Noisy Recovery for L = 2 The algorithm for recovering best k-sparse approxima-
tion of vectors via queries to oracle in noisy setting.
Require: SNR = 1/2  Precision of unknown vectors ✏  and the constant cs where csk log n rows

are sufﬁcient for RIP in binary matrices.

2

Call SampleAndRecover(vi) where vi 2R {+1 1}n.

Call SampleAndRecover((vi + vj)/2) and SampleAndRecover((vi  vj)/2)

1: for i = 1  2  . . .   csk log(n/k) do
2:
3: end for
4: for i 2 [log n] and j 2 [csk log(n/k)] with j 6= i do
5:
6: end for
7: Choose vector v from {v1  v2  . . .   vlog n} such that hv  1i 6= hv  2i.
8: for i = 1  2  . . .   k log(n/k) and vi 6= v do
Label one of hvi  1i hvi  2i to be hv  1i if their sum is in the pair h vi+v
9:
and their difference is in the pair h vvi
10: end for
11: Aggregate all (query  denoised query response pairs) labelled hv  1i and hv  2i separately and
multiply all denoised query responses by a factor of 1/(pcsk log(n/k)).
12: Return best k-sparse approximation of 1 and 2 by using Basis Pursuit algorithm on each
Issue T = c2 exp(/✏)2/3 queries to oracle with v.
Return hv  1i  hv  2i via min-distance estimator (Gaussian mixture learning  lemma 2).

13: function SampleAndRecover (v)
14:
15:
16: end function

aggregated cluster of (query  denoised query response) pairs.

  2i. Label the other hv   2i.

  1i h vvi

2

2

  1i h vi+v

2

  2i

3.1 Gaussian Noise: Two vectors
Algorithm 2 addresses the setting with only two unknown vectors. We will assume k1k2 = k2k2 =
1  so that we can subsequently show that the SNR is simply 1/2. This assumption is not necessary
but we make this for the ease of presentation. The assumption of ✏-precision for  was made in
Yin et al. [27]  and we stick to the same assumption. On the other hand  Yin et al. requires further
assumptions that we do not need to make. Furthermore  the result of Yin et al. is restricted to exactly
sparse vectors  whereas our result holds for general sparse approximation.
For the two-vector case the result we aim to show is following.
Theorem 4. Algorithm 2 uses O(k log3 n exp((/✏)2/3)) queries to recover both the vectors 1 and
2 with an `1/`1 guarantee in Eq. (3) with probability at least 1  2/n.
This result is directly comparable with [27]. On the statistical side  we improve their result in
several ways: (1) we improve the dependence on /✏ in the sample complexity from exp(/✏) to
exp((/✏)2/3) 3 (2) our result applies for dense vectors  recovering the best k-sparse approximations 
and (3) we do not need the overlap assumption (eq. (2)) used in their work.
Once we show SNR = 1/2  Theorem 4 trivially implies Theorem 3 in the case L = 2. Indeed 
from Algorithm 2  notice that we have used vectors v sampled uniformly at random from {+1 1}n
and use them as query vectors. We must have Ev|hv  `i|2/E⌘2 = k`k2
2/2 = 1/2 for ` = 1  2.
Further  we have used the sum and difference query vectors which have the form (v1 + v2)/2 and
(v1  v2)/2 respectively where v1  v2 are sampled uniformly and independently from {+1 1}n.
Therefore  we must have for ` = 1  2  Ev1 v2|h(v1 ± v2)/2  `i|2/E⌘2 = 1/22. According to our
deﬁnition of SNR  we have that SNR = 1/2.
A description of Algorithm 2 that lead to proof of Theorem 4 can be found in Appendix B. We
provide a short sketch here and state an important lemma that we will use in the more general case.
The main insight is that for a ﬁxed sensing vector v  if we repeatedly query with v  we obtain samples
2N (hv  2i  2). If we can exactly recover the
from a mixture of Gaussians 1
3Note that [27] treat /✏ as constant in their theorem statement  but the dependence can be extracted from

2N (hv  1i  2) + 1

their proof.

6

means of these Gaussians  we essentially reduce to the noiseless case from the previous section. The
ﬁrst key step upper bounds the sample complexity for exactly learning the parameters of a mixture of
Gaussians.
LPL
Lemma 2 (Learning Gaussian mixtures). Let M = 1
i=1 N (µi  2) be a uniform mixture of L
univariate Gaussians  with known shared variance 2 and with means µi 2 ✏Z. Then  for some
constant c > 0 and some t = !(L)  there exists an algorithm that requires ctL2 exp((/✏)2/3)
samples from M and exactly identiﬁes the parameters {µi}L
i=1 with probability at least 1  2e2t.
If we sense with v 2 {1  +1}n then hv  1i hv  2i 2 ✏Z  so appealing to the above lemma  we
can proceed assuming we know these two values exactly. Unfortunately  the sensing vectors here are
more restricted — we must maintain bounded SNR and our technique of mixture learning requires
that the means have ﬁnite precision — so we cannot simply appeal to our noiseless results for the
alignment step. Instead we design a new alignment strategy  inspired by error correcting codes. Given
two query vectors v1  v2 and the exact means hvi  ji  i  j 2{ 1  2}  we must identify which values
correspond to 1 and 2. In addition to sensing with any pair v1 and v2 we sense with v1±v2
  and
we use these two additional measurements to identify which recovered means correspond to 1 and
which correspond to 2. Intuitively  we can check if our alignment is correct via these reference
measurements.
Therefore  we can obtain aligned  denoised inner products with each of the two parameter vectors. At
this point we can apply a standard compressed sensing result as mentioned at the start of this section
to obtain the sparse approximations of vectors.

2

3.2 General value of L
In this setting  we will have L > 2 unknown vectors 1  2  . . .   L 2 Rn of unit norm each from
which the oracle can sample from with equal probability. We assume that L does not grow with n or
k and as before  all the elements in the unknown vectors lie on a ✏-grid. Here  we will build on the
ideas for the special case of L = 2.
The main result of this section is the following.

✏ )2/3⌘⌘ queries with SNR = O(1/2) to recover

Theorem 5. Algorithm 3 uses O⇣k(log n)3 exp⇣( 
all the vectors 1  . . .   L with `1/`1 guarantees in Eq. (3) with probability at least 1  2/n.
Theorem 3 follows as a corollary of this result.
The analysis of Algorithm 3 and the proofs of Theorems 3 and 5 are provided in detail in Appendix D.
Below we sketch some of the main points of the proof.
There are two main hurdles in extending the steps explained for L = 2. For a query vector v  we deﬁne
the denoised query means to be the set of elements {hv  ii}L
i=1. Recall that a query vector v is de-
ﬁned to be good if all the elements in the set of denoised query means {hv  1i hv  2i  . . .  hv  Li}
are distinct. For L = 2  the probability of a query vector v being good for L = 2 is at least 1/2 but
for a value of L larger than 2  it is not possible to obtain such guarantees without further assumptions.
For a more concrete example  consider L  4 and the unknown vectors 1  2  . . .   L to be such
that i has 1 in the ith position and zero everywhere else. If v is sampled from {+1 1}n as before 
then hv  ii can take values only in {1  0  +1} and therefore it is not possible that all the values
hv  ii are distinct. Secondly  even if we have a good query vector  it is no longer trivial to extend
the clustering or alignment step. Hence a number of new ideas are necessary to solve the problem for
any general value of L.

We need to deﬁne a few constants which are used in the algorithm. Let < p2  1 be a constant

(we need a  that allow k-sparse approximation given a (2k  )-RIP matrix). Let c0 be a large positive
constant such that

Secondly  let ↵? be another positive constant that satisﬁes the following for a given value of c0 

↵? = maxn↵ :

↵↵

(↵  1)↵1 < exp⇣ 2

16 

3
48 

1

c0⌘o.

2
16 

3
48 

1
c0

> 0.

7

(A)

(B)

and precision of unknown vectors as ✏.

Algorithm 3 Noisy Recovery for any constant L The algorithm for recovering best k-sparse
approximation of vectors via queries to oracle in noisy setting.
Require: c0 ↵ ?  z? as deﬁned in equations (A)  (B) and (C) respectively  Variance of noise E⌘2 = 2
1: for i = 1  2  . . .  p↵? log n + c0↵?k log(n/k) do
2:
3: Make c2 exp((/✏)2/3) queries to the oracle using each of the vectors (qi  1)ri  vi + qiri
t=1 by using min-distance
4:

Let vi 2R {+1 1}n  ri 2R {2z? 2z? + 1  . . .   2z?}n  qi 2R {1  2  . . .   4z? + 1}
and vi + ri.
t=1 {hvi + ri  ti}L
Recover h{(qi  1)ri  ti}L
estimator (Gaussian mixture learning  lemma 2).

t=1 {hvi + qiri  ti}L

5: end for
6: for i 2 [p↵? log n] and j 2 [↵?k log(nk)] do
7: Make c2 exp((/✏)2/3) queries to the oracle using the vector ri+j + ri.
8:

Recover {hri+j + ri  ti}L
Lemma 2).

t=1  by using the min-distance estimator (Gaussian mixture learning 

9: end for
10: Choose vector (v?  r?  q?) from {(vt  rt  qt)}

such that (v? + r?  (q  1)r?  v? + q?r?)
is good. Call a triplet (v + r  (q  1)r  v + qr) to be good if no element in {hv + qr  ii}L
can be written in two possible ways as sum of two elements  one each from {hv + r  ii}L
and {h(q  1)r  ii}L

p↵? log n
t=1

i=1.

i=1

i=1

11: Initialize Sj =  for j = 1  . . .   L
12: for i = p↵? log n + 1  2  . . .  p↵? log n + c0↵?k log n
13:

k do

if (vi + ri  (qi  1)ri  vi + qri) is matching good with respect to (v? + r?  (q  1)r?  v? +
q?r?) (Call a triplet (v0 + r0  (q0  1)r0  v0 + q0r0) to be matching good w.r.t a good triplet
(v? + r?  (q?  1)r?  v? + q?r?) if (v0 + r0  (q0  1)r0  v0 + q0r0) and (r0  r?  r0 + r?) are
good. ) then

Label the elements in {hvi  ti}L
for j = 1  2  . . .   L do

t=1 as described in Lemma 18

Sj = Sj [ {hvi  ti} if label of hvi  ti is hr?  ji

end for

14:
15:
16:
17:
end if
18:
19: end for
20: for j = 1  2  . . .   L do
21:
22:
23: end for
24: Return 1  2  . . .   L.

Aggregate the elements of Sj and scale them by a factor of 1/c0k log(n/k).
Recover the vector j by using basis pursuit algorithms (compressed sensing decoding).

Finally  for a given value of ↵? and L  let z? be the smallest integer that satisﬁes the following:

z? = minnz 2 Z : 1  L3⇣

3

4z + 1 

1

4z2 + 1⌘ 

1

p↵?o.

(C)

The Denoising Step.
In each step of the algorithm  we sample a vector v uniformly at random from
{+1 1}n  another vector r uniformly at random from G ⌘ {2z? 2z? + 1  . . .   2z?  1  2z?}n
and a number q uniformly at random from {1  2  . . .   4z? + 1}. Now  we will use a batch of queries
corresponding to the vectors v + r  (q  1)r and v + qr. We deﬁne a triplet of query vectors
(v1  v2  v3) to be good if for all triplets of indices i  j  k 2 [L] such that i  j  k are not identical 

hv1  ii + hv2  ji 6= hv3  ki.

We show that the query vector triplet (v + r  (q  1)r  v + qr) is good with at least some probability.
This implies if we choose O(log n) triplets of such query vectors  then at least one of the triplets are
good with probability 1 1/n. It turns out that  for a good triplet of vectors (v + r  (q  1)r  v + qr) 
we can obtain hv  ii for all i 2 [L].
Furthermore  it follows from Lemma 2 that for a query vector v with integral entries  a batch size of
T > c3 log n exp((/✏)2/3)   for some constant c3 > 0  is sufﬁcient to recover the denoised query
responses hv  1i hv  2i  . . .  hv  Li for all the queries with probability at least 1  1/poly(n).

8

The Alignment Step. Let a particular good query vector triplet be (v? + r?  (q?1)r?  v? +q?r?).
From now  we will consider the L elements {hr?  ii}L
i=1 to be labels and for a vector u  we will
associate a label with every element in {hu  ii}L
i=1. The labelling is correct if  for all i 2 [L]  the
element labelled as hr?  ii also corresponds to the same unknown vector i. Notice that we can
label the elements {hv?  ii}L
i=1 correctly because the triplet (v? + r?  (q?  1)r?  v? + q?r?) is
good. Consider another good query vector triplet (v0 + r0  (q0  1)r0  v0 + q0r0). This matches with
the earlier query triplet if additionally  the vector triplet (r0  r?  r0 + r?) is also good.
Such matching pair of good triplets exists  and can be found by random choice with some probability.
We show that  the matching good triplets allow us to do the alignment in the case of general L > 2.
At this point we would again like to appeal to the standard compressed sensing results. However
we need to show that the matching good vectors themselves form a matrix that has the required RIP
property. As our ﬁnal step  we establish this fact.
Remark 3 (Reﬁnement and adaptive queries). It is possible to have a sample complexity of

O⇣k(log n)2 log k exp⇣(✏pSNR)2/3⌘⌘ in Theorem 3  but with a probability of 1  poly(k1).

Also it is possible to shave-off another log n factor from sample complexity if we can make the queries
adaptive.

Acknowledgements: This research is supported in part by NSF Grants CCF 1642658  1618512 
1909046  1908849 and 1934846.

References
[1] S. Arora and R. Kannan. Learning mixtures of arbitrary gaussians. In Symposium on Theory of

Computing  2001.

[2] R. Baraniuk  M. Davenport  R. DeVore  and M. Wakin. The johnson-lindenstrauss lemma meets

compressed sensing. preprint  100(1):0  2006.

[3] R. Baraniuk  M. Davenport  R. DeVore  and M. Wakin. A simple proof of the restricted isometry

property for random matrices. Constructive Approximation  28(3):253–263  2008.

[4] P. Borwein and T. Erdélyi. Littlewood-type problems on subarcs of the unit circle. Indiana

University Mathematics Journal  1997.

[5] S. Boucheron  G. Lugosi  and P. Massart. Concentration inequalities: A nonasymptotic theory

of independence. Oxford university press  2013.

[6] E. J. Candes. The restricted isometry property and its implications for compressed sensing.

Comptes rendus mathematique  346(9-10):589–592  2008.

[7] E. J. Candès  J. Romberg  and T. Tao. Robust uncertainty principles: exact signal reconstruction
from highly incomplete frequency information. IEEE Transactions on Information Theory 
52(2):489–509  2006.

[8] A. T. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. In

International Conference on Machine Learning  pages 1040–1048  2013.

[9] D. Curtiss. Recent extentions of descartes’ rule of signs. Annals of Mathematics  pages 251–278 

1918.

[10] S. Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science  pages

634–644  1999.

[11] R. D. De Veaux. Mixtures of linear regressions. Computational Statistics & Data Analysis 

8(3):227–245  1989.

[12] L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Science &

Business Media  2012.

9

[13] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory  52(4):1289–1306 

2006.

[14] S. Faria and G. Soromenho. Fitting mixtures of linear regressions. Journal of Statistical

Computation and Simulation  80(2):201–225  2010.

[15] F. R. Gantmakher. The theory of matrices  volume 131. American Mathematical Soc.  1959.
[16] M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In Symposium on

Theory of Computing  2015.

[17] A. Kalai  A. Moitra  and G. Valiant. Disentangling Gaussians. Communications of the ACM 

55(2):113–120  2012.

[18] A. Krishnamurthy  A. Mazumdar  A. McGregor  and S. Pal. Trace reconstruction: Generalized
and parameterized. In 27th Annual European Symposium on Algorithms  ESA 2019  September
9-11  2019  Munich/Garching  Germany.  pages 68:1–68:25  2019.

[19] J. Kwon and C. Caramanis. Global convergence of em algorithm for mixtures of two component

linear regression. arXiv preprint arXiv:1810.05752  2018.

[20] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In

Foundations of Computer Science  2010.

[21] F. Nazarov and Y. Peres. Trace reconstruction with exp(O(n1/3) samples. In Symposium on

Theory of Computing  2017.

[22] N. Städler  P. Bühlmann  and S. Van De Geer. l1-penalization for mixture regression models.

Test  19(2):209–256  2010.

[23] D. M. Titterington  A. F. Smith  and U. E. Makov. Statistical analysis of ﬁnite mixture distribu-

tions. Wiley  1985.

[24] K. Viele and B. Tong. Modeling with mixtures of linear regressions. Statistics and Computing 

12(4):315–330  2002.

[25] X. Yi  C. Caramanis  and S. Sanghavi. Alternating minimization for mixed linear regression. In

International Conference on Machine Learning  pages 613–621  2014.

[26] X. Yi  C. Caramanis  and S. Sanghavi. Solving a mixture of many random linear equations by
tensor decomposition and alternating minimization. arXiv preprint arXiv:1608.05749  2016.
[27] D. Yin  R. Pedarsani  Y. Chen  and K. Ramchandran. Learning mixtures of sparse linear
regressions using sparse graph codes. IEEE Transactions on Information Theory  65(3):1430–
1451  2019.

10

,Akshay Krishnamurthy
Arya Mazumdar
Andrew McGregor
Soumyabrata Pal