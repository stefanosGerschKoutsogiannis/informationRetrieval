2018,Contour location via entropy reduction leveraging multiple information sources,We introduce an algorithm to locate contours of functions that are expensive to evaluate. The problem of locating contours arises in many applications  including classification  constrained optimization  and  performance analysis of mechanical and dynamical systems (reliability  probability of failure  stability  etc.). Our algorithm locates contours using information from multiple sources  which are available in the form of relatively inexpensive  biased  and possibly noisy
 approximations to the original function. Considering multiple information sources can lead to significant cost savings. We also introduce the concept of contour entropy  a formal measure of uncertainty about the location of the zero contour of a function approximated by a statistical surrogate model. Our algorithm locates contours efficiently by maximizing the reduction of contour entropy per unit cost.,Contour location via entropy reduction
leveraging multiple information sources

Alexandre N. Marques

Department of Aeronautics and Astronautics

Massachusetts Institute of Technology

Cambridge  MA 02139

Remi R. Lam

Center for Computational Engineering
Massachusetts Institute of Technology

Cambridge  MA 02139

noll@mit.edu

rlam@mit.edu

Institute for Computational Engineering and Sciences

Karen E. Willcox

University of Texas at Austin

Austin  TX 78712

kwillcox@ices.utexas.edu

Abstract

We introduce an algorithm to locate contours of functions that are expensive to
evaluate. The problem of locating contours arises in many applications  including
classiﬁcation  constrained optimization  and performance analysis of mechanical
and dynamical systems (reliability  probability of failure  stability  etc.). Our algo-
rithm locates contours using information from multiple sources  which are available
in the form of relatively inexpensive  biased  and possibly noisy approximations
to the original function. Considering multiple information sources can lead to
signiﬁcant cost savings. We also introduce the concept of contour entropy  a formal
measure of uncertainty about the location of the zero contour of a function approxi-
mated by a statistical surrogate model. Our algorithm locates contours efﬁciently
by maximizing the reduction of contour entropy per unit cost.

1

Introduction

In this paper we address the problem of locating contours of functions that are expensive to evaluate.
This problem arises in several areas of science and engineering. For instance  in classiﬁcation
problems the contour represents the boundary that divides objects of different classes. Another
example is constrained optimization  where the contour separates feasible and infeasible designs.
This problem also arises when analyzing the performance of mechanical and dynamical systems 
where contours divide different behaviors such as stable/unstable  safe/fail  etc. In many of these
applications  function evaluations involve costly computational simulations  or testing expensive
physical samples. We consider the case when multiple information sources are available  in the form
of relatively inexpensive  biased  and possibly noisy approximations to the original function. Our
goal is to use information from all available sources to produce the best estimate of a contour under a
ﬁxed budget.
We address this problem by introducing the CLoVER (Contour Location Via Entropy Reduction)
algorithm. CLoVER is based on a combination of principles from Bayesian multi-information source
optimization [1–3] and information theory [4]. Our new contributions are:
• The concept of contour entropy  a measure of uncertainty about the location of the zero contour

of a function approximated by a statistical surrogate model.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

• An acquisition function that maximizes the reduction of contour entropy per unit cost.
• An algorithm that locates contours of functions using multiple information sources via reduction

of contour entropy.

This work is related to the topic of Bayesian multi-information source optimization (MISO) [1–
3  5  6]. Speciﬁcally  we use a statistical surrogate model to ﬁt the available data and estimate the
correlation between different information sources  and we choose the location for new evaluations
as the maximizer of an acquisition function. However  we solve a different problem than Bayesian
optimization algorithms. In the case of Bayesian optimization  the objective is to locate the global
maximum of an expensive-to-evaluate function. In contrast  we are interested in the entire set of
points that deﬁne a contour of the function. This difference is reﬂected in our deﬁnition of an
acquisition function  which is fundamentally distinct from Bayesian optimization algorithms.
Other algorithms address the problem of locating the contour of expensive-to-evaluate functions 
and are based on two main techniques: Support Vector Machine (SVM) and Gaussian process (GP)
surrogate. CLoVER lies in the second category.
SVM [7] is a commonly adopted classiﬁcation technique  and can be used to locate contours by
deﬁning the regions separated by them as different classes. Adaptive SVM [8–10] and active learning
with SVM [11–13] improve the original SVM framework by adaptively selecting new samples in
ways that produce better classiﬁers with a smaller number of observations. Consequently  these
variations are well suited for situations involving expensive-to-evaluate functions. Furthermore 
Dribusch et al. [14] propose an adaptive SVM construction that leverages multiple information
sources  as long as there is a predeﬁned ﬁdelity hierarchy between the information sources.
Algorithms based on GP surrogates [15–20] use the uncertainty encoded in the surrogate to make
informed decisions about new evaluations  reducing the overall number of function evaluations
needed to locate contours. These algorithms differ mainly in the acquisition functions that are
optimized to select new evaluations. Bichon et al. [15]  Ranjan et al. [16]  and Picheny et al. [17]
deﬁne acquisition functions based on greedy reduction of heuristic measures of uncertainty about the
location of the contour  whereas Bect et al. [18] and Chevalier et al. [19] deﬁne acquisition functions
based on one-step look ahead reduction of quadratic loss functions of the probability of an excursion
set. In addition  Stroh et al. [21] use a GP surrogate based on multiple information sources  under the
assumption that there is a predeﬁned ﬁdelity hierarchy between the information sources. Opposite to
the algorithms discussed above  Stroh et al. [21] do not use the surrogate to select samples. Instead 
a pre-determined nested LHS design allocates the computational budget throughout the different
information sources.
CLoVER has two fundamental distinctions with respect to the algorithms described above. First  the
acquisition function used in CLoVER is based on one-step look ahead reduction of contour entropy  a
formal measure of uncertainty about the location of the contour. Second  the multi-information source
GP surrogate used in CLoVER does not require any hierarchy between the information sources. We
show that CLoVER outperforms the algorithms of Refs. [15–20] when applied to two problems
involving a single information source. One of these problems is discussed in Sect. 4  while the other
is discussed in the supplementary material.
The remainder of this paper is organized as follows. In Sect. 2 we present a formal problem statement
and introduce notation. Then  in Sect. 3 we introduce the details of the CLoVER algorithm  including
the deﬁnition of the concept of contour entropy. Finally  in Sect. 4 we present examples that illustrate
the performance of CLoVER.

2 Problem statement and notation1
Let g : D (cid:55)→ R denote a continuous function on the compact set D ∈ Rd  and g(cid:96) : D (cid:55)→ R  (cid:96) ∈ [M ] 
denote a collection of the M information sources (IS) that provide possibly biased estimates of g.
(For M ∈ Z+  we use the notation [M ] = {1  . . .   M} and [M ]0 = {0  1  . . .   M}). In general 
we assume that observations of g(cid:96) may be noisy  such that they correspond to samples from the
normal distribution N (g(cid:96)(x)  λ(cid:96)(x)). We further assume that  for each IS (cid:96)  the query cost function 
1The statistical model used in the present algorithm is the same introduced in [3]  and we attempt to use a

notation as similar as possible to this reference for the sake of consistency.

2

c(cid:96) : D (cid:55)→ R+  and the variance function λ(cid:96) are known and continuously differentiable over D.
Finally  we assume that g can also be observed directly without bias (but possibly with noise)  and
refer to it as information source 0 (IS0)  with query cost c0 and variance λ0.
Our goal is to ﬁnd the best approximation  within a ﬁxed budget  to a speciﬁc contour of g by using a
combination of observations of g(cid:96). In the remainder of this paper we assume  without loss of generality 
that we are interested in locating the zero contour of g  deﬁned as the set Z = {z ∈ D | g(z) = 0}.

3 The CLoVER algorithm

In this section we present the details of the CLoVER (Contour Location Via Entropy Reduction)
algorithm. CLoVER has three main components: (i) a statistical surrogate model that combines
information from all M + 1 information sources  presented in Sect. 3.1  (ii) a measure of the en-
tropy associated with the zero contour of g that can be computed from the surrogate  presented in
Sect. 3.2  and (iii) an acquisition function that allows selecting evaluations that reduce this entropy
measure  presented in Sect. 3.3. In Sect. 3.4 we discuss the estimation of the hyperparameters
of the surrogate model  and in Sect. 3.5 we show how these components are combined to form
an algorithm to locate the zero contour of g. We discuss the computational cost of CLoVER
in the supplementary material. An implementation of CLoVER in Python 2.7 is available at
https://github.com/anmarques/CLoVER.

3.1 Statistical surrogate model

CLoVER uses the statistical surrogate model introduced by Poloczek et al. [3] in the context of multi-
information source optimization. This model constructs a single Gaussian process (GP) surrogate
that approximates all information sources g(cid:96) simultaneously  encoding the correlations between
them. Using a GP surrogate allows data assimilation using standard tools of Gaussian process
regression [22].
We denote the surrogate model by f  with f ((cid:96)  x) being the normal distribution that represents the
belief about IS (cid:96)  (cid:96) ∈ [M ]0  at location x. The construction of the surrogate follows from two
modeling choices: (i) a GP approximation to g denoted by f (0  x)  i.e.  f (0  x) ∼ GP (µ0  Σ0) 
and (ii) independent GP approximations to the biases δ(cid:96)(x) = g(cid:96)(x) − g(x)  δ(cid:96) ∼ GP (µ(cid:96)  Σ(cid:96)).
Similarly to [3]  we assume that µ(cid:96) and Σ(cid:96)  (cid:96) ∈ [M ]0  belong to one of the standard parameterized
classes of mean functions and covariance kernels. Finally  we construct the surrogate of g(cid:96) as
f ((cid:96)  x) = f (0  x) + δ(cid:96)(x). As a consequence  the surrogate model f is a GP  f ∼ GP (µ  Σ)  with
(1)
(2)

Σ(cid:0)((cid:96)  x)  (m  x(cid:48))(cid:1) = Cov(cid:0)f ((cid:96)  x)  f (m  x(cid:48))(cid:1) = Σ0(x  x(cid:48)) + 1(cid:96) mΣ(cid:96)(x  x(cid:48)) 

µ((cid:96)  x) = E[f ((cid:96)  x)] = µ0(x) + µ(cid:96)(x) 

where 1(cid:96) m denotes the Kronecker’s delta.

3.2 Contour entropy

In information theory [4]  the concept of entropy is a measure of the uncertainty in the outcome of
a random process. In the case of a discrete random variable W with k distinct possible values wi 
i ∈ [k]  entropy is deﬁned by

H(W ) = − k(cid:88)

P (wi) ln P (wi) 

(3)

i=1

where P (wi) denotes the probability mass of value wi. It follows from this deﬁnition that lower
values of entropy are associated to processes with little uncertainty (P (wi) ≈ 1 for one of the possible
outcomes).
We introduce the concept of contour entropy as the entropy of a discrete random variable associated
with the uncertainty about the location of the zero contour of g  as follows. For any given x ∈ D 
the posterior distribution of f (0  x) (surrogate model of g(x))  conditioned on all the available
evaluations  is a normal random variable with known mean µ(0  x) and variance σ2(0  x). Given
(x) ∈ R+  an observation y of this random variable can be classiﬁed as one of the following three

3

Figure 1: Left: GP surrogate  distribution f (0  x(cid:48)) and probability mass of events L  C  and U  which
deﬁne the random variable Wx(cid:48). Right: Entropy H(Wx) as a function of the probability masses. The
black dot corresponds to H(Wx(cid:48)).

events: y < −(x) (denoted as event L)  |y| < (x) (denoted as event C)  or y > (x) (denoted
as event U). These three events deﬁne a discrete random variable  Wx  with probability mass
P (L) = Φ((−µ(0  x)− (x))/σ(0  x))  P (C) = Φ((−µ(0  x) + (x))/σ(0  x))− Φ((−µ(0  x)−
(x))/σ(0  x))  P (U ) = Φ((µ(0  x) − (x))/σ(0  x))  where Φ is the unit normal cumulative
distribution function. Figure 1 illustrates events L  C  and U  and the probability mass associated
with each of them. In particular  P (C) measures the probability of g(x) being within a band of width
2(x) surrounding the zero contour  as estimated by the GP surrogate. The parameter (x) represents
a tolerance in our deﬁnition of a zero contour. As the algorithm gains conﬁdence in its predictions 
it is natural to reduce (x) to tighten the bounds on the location of the zero contour. As discussed
in the supplementary material  numerical experiments indicate that (x) = 2σ(x) results in a good
balance between exploration and exploitation.
The entropy of Wx measures the uncertainty in whether g(x) lies below  within  or above the tolerance
(x)  and is given by

H(Wx; f ) = −P (L) log P (L) − P (C) log P (C) − P (U ) log P (U ).

(4)

This entropy measures uncertainty at parameter value x only. To characterize the uncertainty of the
location of the zero contour  we deﬁne the contour entropy as

H(Wx; f ) dx 

(5)

H (f ) =

1

V (D)

where V (D) denotes the volume of D.

3.3 Acquisition function

CLoVER locates the zero contour by selecting samples that are likely reduce the contour entropy at
each new iteration. In general  samples from IS0 are the most informative about the zero contour
of g  and thus are more likely to reduce the contour entropy  but they are also the most expensive to
evaluate. Hence  to take advantage of the other M IS available  the algorithm performs observations
that maximize the expected reduction in contour entropy  normalized by the query cost.
Consider the algorithm after n samples evaluated at Xn = {((cid:96)i  xi)}n
i=1  which result in observations
i=1. We denote the posterior GP of f  conditioned on {Xn  Yn}  as f n  with mean µn and
Yn = {yi}n
covariance matrix Σn. Then  the algorithm selects a new parameter value x ∈ D  and IS (cid:96) ∈ [M ]0
that satisfy the following optimization problem.

(cid:90)

D

4

(cid:96)∈[M ]0  x∈D u((cid:96)  x; f n) 
maximize

where

Ey[H (f n) − H (f n+1) | (cid:96)n+1 = (cid:96)  xn+1 = x]

 

u((cid:96)  x; f n) =

yn+1 ∼ N(cid:0)µn((cid:96)  x)  Σn(((cid:96)  x)  ((cid:96)  x))(cid:1). To make the optimization problem tractable 

c(cid:96)(x)
distribution

expectation

of

possible

and

the

observations 
the

is

taken

over

the

(6)

(7)

IS0IS1Figure 2: Comparison between functions involving products of Φ and ln Φ and approximations (8–9).

search domain is replaced by a discrete set of points A ⊂ D  e.g.  a Latin Hypercube design. We
discuss how to evaluate the acquisition function u next.
Given that f n is known  H (f n) is a deterministic quantity that can be evaluated from (4–5). Namely 
H(Wx; f n) follows directly from (4)  and the integration over D is computed via a Monte Carlo-based
approach (or regular quadrature if the dimension of D is relatively small).
Evaluating Ey[H (f n+1)] requires a few additional steps. First  the expectation operator com-
mutes with the integration over D. Second  for any x(cid:48) ∈ D  the entropy H(Wx(cid:48); f n+1) de-
pends on yn+1 through its effect on the mean µn+1(0  x(cid:48)) (the covariance matrix Σn+1 de-
pends only on the location of the samples). The mean is afﬁne with respect to the observa-
tion yn+1 and thus is distributed normally: µn+1(0  x(cid:48)) ∼ N (µn(0  x(cid:48))  ¯σ2(x(cid:48); (cid:96)  x))  where

/(cid:0)λ(cid:96)(x) + Σn(((cid:96)  x)  ((cid:96)  x))(cid:1). Hence  after commuting with

¯σ2(x(cid:48); (cid:96)  x) =(cid:0)Σn((0  x(cid:48))  ((cid:96)  x))(cid:1)2

the integration over D  the expectation with respect to the distribution of yn+1 can be equivalently
replaced by the expectation with respect to the distribution of µn+1(0  x(cid:48))  denoted by Eµ[(.)].
Third  in order to compute the expectation operator analytically  we introduce the following approxi-
mations.

Φ(x) ln Φ(x) ≈
(Φ(x + d) − Φ(x − d)) ln(Φ(x + d) − Φ(x − d)) ≈

2π cϕ(x − ¯x) 

2π c(cid:0)ϕ(x − d + ¯x) + ϕ(x + d − ¯x)(cid:1) 

√

(8)

√

(9)
where ϕ is the normal probability density function  ¯x = Φ−1(e−1)  and c = Φ(¯x) ln Φ(¯x). Figure 2
shows the quality of these approximations. Then  we can ﬁnally write
Ey[H (f n+1) | (cid:96)n+1 = (cid:96)  xn+1 = x]

(cid:90)

D

(cid:90)

1

=

V (D)
≈ − c

Eµ[H(Wx(cid:48); f n+1)|(cid:96)n+1 = (cid:96)  xn+1 = x] dx(cid:48)

1(cid:88)

1(cid:88)

i=0

j=0

(cid:32)

− 1
2

(cid:18) µn(0  x(cid:48)) + (−1)i

ˆσ(x(cid:48); (cid:96)  x)

V (D)

D

rσ(x; (cid:96)  x)

exp

+ (−1)j ¯xrσ(x(cid:48); (cid:96)  x)

(cid:19)2(cid:33)

dx(cid:48) 

(10)

where

ˆσ2(x(cid:48); (cid:96)  x) = Σn+1((0  x(cid:48))  (0  x(cid:48))) + ¯σ2(x(cid:48); (cid:96)  x) 

σ(x(cid:48); (cid:96)  x) =
r2

Σn+1((0  x(cid:48))  (0  x(cid:48)))

ˆσ2(x(cid:48); (cid:96)  x)

.

3.4 Estimating hyperparameters

Our experience indicates that the most suitable approach to estimate the hyperparameters depends on
the problem. Maximum a posteriori (MAP) estimates normally perform well if reasonable guesses
are available for the priors of hyperparameters. On the other hand  maximum likelihood estimates
(MLE) may be sensitive to the randomness of the initial data  and normally require a larger number
of evaluations to yield appropriate results.
Given the challenge of estimating hyperparameters with small amounts of data  we recommend
updating these estimates throughout the evolution of the algorithm. We adopt the strategy of

5

-10-50510-0.4-0.20estimating the hyperparameters whenever the algorithm makes a new evaluation of IS0. The data
obtained by evaluating IS0 is used directly to estimate the hyperparameters of µ0 and Σ0. To estimate
the hyperparameters of µ(cid:96) and Σ(cid:96)  (cid:96) ∈ [M ]  we evaluate all other M information sources at the same
location and compute the biases δ(cid:96) = y(cid:96) − y0  where y(cid:96) denotes data obtained by evaluating IS (cid:96).
The biases are then used to estimate the hyperparameters of µ(cid:96) and Σ(cid:96).

3.5 Summary of algorithm
1. Compute an initial set of samples by evaluating all M + 1 IS at the same values of x ∈ D. Use
2. Prescribe a set of points A ⊂ D which will be used as possible candidates for sampling.
3. Until budget is exhausted  do:

samples to compute hyperparameters and the posterior of f.

(a) Determine the next sample by solving the optimization problem (6).
(b) Evaluate the next sample at location xn+1 using IS (cid:96)n+1.
(c) Update hyperparameters and posterior of f.

4. Return the zero contour of E[f (0  x)].

4 Numerical results

In this section we present three examples that demonstrate the performance of CLoVER. The ﬁrst
two examples involve multiple information sources  and illustrate the reduction in computational cost
that can be achieved by combining information from multiple sources in a principled way. The last
example compares the performance of CLoVER to that of competing GP-based algorithms  showing
that CLoVER can outperform existing alternatives even in the case of a single information source.

4.1 Multimodal function

In this example we locate the zero contour of the following function within the domain
D = [−4  7] × [−3  8].

g(x) =

1 + 4)(x2 − 1)
(x2

20

− sin

− 2.

(11)

(cid:18) 5x1

(cid:19)

2

This example was introduced in Ref. [15] in the context of reliability analysis  where the zero
contour represents a failure boundary. We explore this example further in the supplementary material 
where we compare CLoVER to competing algorithms in the case of a single information source.
To demonstrate the performance of CLoVER in the presence of multiple information sources  we
introduce the following biased estimates of g:

g1(x) = g(x) + sin

 

g2(x) = g(x) + 3 sin

(x1 + x2 + 7)

.

We assume that the query cost of each information source is constant: c0 = 1  c1 = 0.01  c2 = 0.001.
We further assume that all information sources can be observed without noise.
Figure 3 shows predictions made by CLoVER at several iterations of the algorithm. CLoVER starts
with evaluations of all three IS at the same 10 random locations. These evaluations are used to
compute the hyperparameters using MLE  and to construct the surrogate model. The surrogate model
is based on zero mean functions and squared exponential covariance kernels [22]. The contour
entropy of the initial setup is H = 0.315  which indicates that there is considerable uncertainty in the
estimate of the zero contour. CLoVER proceeds by exploring the parameter space using mostly IS2 
which is the model with the lowest query cost. The algorithm stops after 123 iterations  achieving
a contour entropy of H = 4 × 10−9. Considering the samples used in the initial setup  CLoVER
makes a total of 17 evaluations of IS0  68 evaluations of IS1  and 68 evaluations of IS2. The total
query cost is 17.8. We repeat the calculations 100 times using different values for the initial 10
random evaluations  and the median query cost is 18.1. In contrast  the median query cost using a
single information source (IS0) is 38.0  as shown in the supplementary material. Furthermore  at
query cost 18.0  the median contour entropy using a single information source is H = 0.19.

6

(cid:18) 5

(cid:16)

22

(cid:17)

(cid:19)

x1 +

x2
2

+

5
4

(cid:18) 5

11

(cid:19)

Figure 3: Locating the zero contour of the multimodal function (11). Upper left: Zero contour of
IS0  IS1  and IS2. Other frames: Samples and predictions made by CLoVER at several iterations.
Dashed black line: Zero contour predicted by the surrogate model. Colors: Mean of the surrogate
model f (0  x). CLoVER obtains a good approximation to the zero contour with only 17 evaluations
of expensive IS0.

Figure 4: Left: Relative error in the estimate of the area of the set S. Right: Contour entropy. Median 
25  and 75 percentiles.

We assess the accuracy of the zero contour estimate produced by CLoVER by measuring the area
of the set S = {x ∈ D | g(x) > 0} (shaded region shown on the top left frame of Figure 3). We
estimate the area using Monte Carlo integration with 106 samples in the region [−4  7] × [1.4  8].
We compute a reference value by averaging 20 Monte Carlo estimates based on evaluations of
g: area(S) = 36.5541. Figure 4 shows the relative error in the area estimate obtained with 100
evaluations of CLoVER. This ﬁgure also shows the evolution of the contour entropy.

4.2 Stability of tubular reactor

We use CLoVER to locate the stability boundary of a nonadiabatic tubular reactor with a mixture of
two chemical species. This problem is representative of the operation of industrial chemical reactors 

7

truthIS1IS2-10-50510and has been the subject of several investigations  e.g. [23]. The reaction between the species releases
heat  increasing the temperature of the mixture. In turn  higher temperature leads to a nonlinear
increase in the reaction rate. These effects  combined with heat diffusion and convection  result in
complex dynamical behavior that can lead to self-excited instabilities. We use the dynamical model
described in Refs. [24  25]. This model undergoes a Höpf bifurcation  when the response of the
system transitions from decaying oscillations to limit cycle oscillations. This transition is controlled
by the Damköhler number D  and here we consider variations in the range D ∈ [0.16  0.17] (the
bifurcation occurs at the critical Damköhler number Dcr = 0.165). To characterize the bifurcation 
we measure the temperature at the end of the tubular reactor (θ)  and introduce the following indicator
of stability.

(cid:26) α(D) 

g(D) =

(γr(D))2 

for decaying oscillations 
for limit cycle oscillations.

α is the growth rate  estimated by ﬁtting the temperature in the last two cycles of oscillation to the
approximation θ ≈ θ0 + ¯θeαt  where t denotes time. Furthermore  r is the amplitude of limit cycle
oscillations  and γ = 25 is a parameter that controls the intensity of the chemical reaction.
Our goal is to locate the critical Damköhler number using two numerical models of the tubular reactor
dynamics. The ﬁrst model results from a centered ﬁnite-difference discretization of the governing
equations and boundary conditions  and corresponds to IS0. The second model is a reduced-order
model based on the combination of proper orthogonal decomposition and the discrete empirical
interpolation method  and corresponds to IS1. Both models are described in details by Zhou [24].
Figure 5 shows the samples selected by CLoVER  and the uncertainty predicted by the GP surrogate at
several iterations. The algorithm starts with two random evaluations of both models. This information
is used to compute a MAP estimate of the hyperparameters of the GP surrogate  using the procedure
recommended by Poloczek et al. [3]2 and to provide an initial estimate of the surrogate. In this
example we use covariance kernels of the Matérn class [22] with ν = 5/2  and zero mean functions.

2For the length scales of the covariance kernels  Poloczek et al. [3] recommend using normal distribution
priors with mean values given by the range of D in each coordinate direction. We found this heuristics to be
only appropriate for functions that are very smooth over D. In the present example we adopt d0 = 0.002 and
d1 = 0.0005 as the mean values for the length scales of Σ0 and Σ1  respectively.

Figure 5: Locating the Höpf bifurcation of a tubular reactor (zero contour of stability indicator).
Shaded area: ±3σ around the mean of the GP surrogate. CLoVER locates the bifurcation after 22
iterations  using only 4 evaluations of IS0.

8

0.160.1620.1640.1660.1680.17-3-2-101230.160.1620.1640.1660.1680.17-3-2-101230.160.1620.1640.1660.1680.17-3-2-101230.160.1620.1640.1660.1680.17-3-2-10123Figure 6: Left: Contour entropy and query cost during the iterations of the CLoVER algorithm. Right:
Reduction in contour entropy per unit query cost at every iteration. CLoVER explores IS1 to decrease
the uncertainty about the location of the bifurcation before using evaluations of expensive IS0.

After these two initial evaluations  CLoVER explores the parameter space using 11 evaluations of IS1.
This behavior is expected  since the query cost of IS0 is 500-3000 times the query cost of IS1. Figure 6
shows the evolution of the contour entropy and query cost along the iterations. After an exploration
phase  CLoVER starts exploiting near D = 0.165. Two evaluations of IS0  at iterations 12 and 14 
allow CLoVER to gain conﬁdence in predicting the critical Damköhler number at Dcr = 0.165.
After eight additional evaluations of IS1  CLoVER determines that other bifurcations are not likely in
the parameter range under consideration. CLoVER concludes after a total of 22 iterations  achieving
H = 6 × 10−9.

4.3 Comparison between CLoVER and existing algorithms for single information source

Here we compare the performance of CLoVER with a single information source to those of algorithms
EGRA [15]  Ranjan [16]  TMSE [17]  TIMSE [18]  and SUR [18]. This comparison is based on
locating the contour g = 80 of the two-dimensional Branin-Hoo function [26] within the domain
D = [−5  10] × [0  15]. We discuss a similar comparison  based on a different problem  in the
supplementary material.
The algorithms considered here are implemented in the R package KrigInv [19]. Our goal is to
elucidate the effects of the distinct acquisition functions  and hence we execute KrigInv using the
same GP prior and schemes for optimization and integration as the ones used in CLoVER. Namely 
the GP prior is based on a constant mean function and a squared exponential covariance kernel 
and the hyperparameters are computed using MLE. The integration over D is performed with the
trapezoidal rule on a 50 × 50 uniform grid  and the optimization set A is composed of a 30 × 30
uniform grid. All algorithms start with the same set of 12 random evaluations of g  and we repeat the
computations 100 times using different random sets of evaluations for initialization.
We compare performance by computing the area of the set S = {x ∈ D | g(x) > 80}. We compute
the area using Monte Carlo integration with 106 samples  and compare the results to a reference value
computed by averaging 20 Monte Carlo estimates based on evaluations of g: area(S) = 57.8137.
Figure 7 compares the relative error in the area estimate computed with the different algorithms. All
algorithms perform similarly  with CLoVER achieving a smaller error on average.

Acknowledgments

This work was supported in part by the U.S. Air Force Center of Excellence on Multi-Fidelity
Modeling of Rocket Combustor Dynamics  Award FA9550-17-1-0195  and by the AFOSR MURI on
managing multiple information sources of multi-physics systems  Awards FA9550-15-1-0038 and
FA9550-18-1-0023.

References
[1] R. Lam  D. L. Allaire  and K. Willcox  “Multiﬁdelity optimization using statistical surrogate
modeling for non-hierarchical information sources ” in 56th AIAA/ASCE/AHS/ASC Structures 
Structural Dynamics  and Materials Conference  AIAA  2015.

9

051015202510-1010-5100200400600800100012000510152025-1012310-4Figure 7: Relative error in the estimate of the area of the set S (median  25th  and 75th percentiles).
Left: comparison between CLoVER and greedy algorithms EGRA  Ranjan  and TMSE. Right:
comparison between CLoVER and one-step look ahead algorithms TIMSE and SUR.

[2] R. Lam  K. Willcox  and D. H. Wolpert  “Bayesian optimization with a ﬁnite budget: An
approximate dynamic programming approach ” in Advances in Neural Information Processing
Systems 29  pp. 883–891  Curran Associates  Inc.  2016.

[3] M. Poloczek  J. Wang  and P. Frazier  “Multi-information source optimization ” in Advances in

Neural Information Processing Systems 30  pp. 4291–4301  Curran Associates  Inc.  2017.

[4] T. M. Cover and J. A. Thomas  Elements of Information Theory (Wiley Series in Telecommuni-

cations and Signal Processing). Wiley-Interscience  2006.

[5] A. I. Forrester  A. Sóbester  and A. J. Keane  “Multi-ﬁdelity optimization via surrogate mod-
elling ” Proceedings of the Royal Society of London A: Mathematical  Physical and Engineering
Sciences  vol. 463  no. 2088  pp. 3251–3269  2007.

[6] K. Swersky  J. Snoek  and R. P. Adams  “Multi-task Bayesian optimization ” in Advances in

Neural Information Processing Systems 26  pp. 2004–2012  Curran Associates  Inc.  2013.

[7] C. Cortes and V. Vapnik  “Support-vector networks ” Machine Learning  vol. 20  no. 3  pp. 273–

297  1995.

[8] A. Basudhar  S. Missoum  and A. H. Sanchez  “Limit state function identiﬁcation using sup-
port vector machines for discontinuous responses and disjoint failure domains ” Probabilistic
Engineering Mechanics  vol. 23  no. 1  pp. 1 – 11  2008.

[9] A. Basudhar and S. Missoum  “An improved adaptive sampling scheme for the construction of
explicit boundaries ” Structural and Multidisciplinary Optimization  vol. 42  no. 4  pp. 517–529 
2010.

[10] M. Lecerf  D. Allaire  and K. Willcox  “Methodology for dynamic data-driven online ﬂight

capability estimation ” AIAA Journal  vol. 53  no. 10  pp. 3073–3087  2015.

[11] G. Schohn and D. Cohn  “Less is more: Active learning with support vector machines ” in
Proceedings of the 17th International Conference on Machine Learning (ICML 2000)  (Stanford 
CA)  pp. 839–846  Morgan Kaufmann  2000.

[12] S. Tong and D. Koller  “Support vector machine active learning with applications to text

classiﬁcation ” Journal of Machine Learning Research  vol. 2  no. Nov  pp. 45–66  2001.

[13] M. K. Warmuth  G. Rätsch  M. Mathieson  J. Liao  and C. Lemmen  “Active learning in the drug
discovery process ” in Advances in Neural Information Processing Systems 14  pp. 1449–1456 
MIT Press  2002.

[14] C. Dribusch  S. Missoum  and P. Beran  “A multiﬁdelity approach for the construction of
explicit decision boundaries: application to aeroelasticity ” Structural and Multidisciplinary
Optimization  vol. 42  pp. 693–705  Nov 2010.

10

EGRARanjanTMSECLoVERTIMSESURCLoVER[15] B. J. Bichon  M. S. Eldred  L. P. Swiler  S. Mahadevan  and J. M. McFarland  “Efﬁcient global
reliability analysis for nonlinear implicit performance functions ” AIAA Journal  vol. 46  no. 10 
pp. 2459–2468  2008.

[16] P. Ranjan  D. Bingham  and G. Michailidis  “Sequential experiment design for contour estima-

tion from complex computer codes ” Technometrics  vol. 50  no. 4  pp. 527–541  2008.

[17] V. Picheny  D. Ginsbourger  O. Roustant  R. T. Haftka  and N.-H. Kim  “Adaptive designs of
experiments for accurate approximation of a target region ” Journal of Mechanical Design 
vol. 132  Jun 2010.

[18] J. Bect  D. Ginsbourger  L. Li  V. Picheny  and E. Vazquez  “Sequential design of computer
experiments for the estimation of a probability of failure ” Statistics and Computing  vol. 22 
no. 3  pp. 773–793  2012.

[19] C. Chevalier  J. Bect  D. Ginsbourger  E. Vazquez  V. Picheny  and Y. Richet  “Fast paral-
lel Kriging-based stepwise uncertainty reduction with application to the identiﬁcation of an
excursion set ” Technometrics  vol. 56  no. 4  pp. 455–465  2014.

[20] H. Wang  G. Lin  and J. Li  “Gaussian process surrogates for failure detection: A bayesian
experimental design approach ” Journal of Computational Physics  vol. 313  pp. 247 – 259 
2016.

[21] R. Stroh  J. Bect  S. Demeyer  N. Fischer  D. Marquis  and E. Vazquez  “Assessing ﬁre safety
using complex numerical models with a Bayesian multi-ﬁdelity approach ” Fire Safety Journal 
vol. 91  pp. 1016 – 1025  2017. Fire Safety Science: Proceedings of the 12th International
Symposium.

[22] C. E. Rasmussen and C. K. I. Williams  Gaussian Processes for Machine Learning (Adaptive

Computation and Machine Learning). The MIT Press  2005.

[23] R. F. Heinemann and A. B. Poore  “Multiplicity  stability  and oscillatory dynamics of the

tubular reactor ” Chemical Engineering Science  vol. 36  no. 8  pp. 1411 – 1419  1981.

[24] Y. B. Zhou  “Model reduction for nonlinear dynamical systems with parametric uncertainties ”

Master’s thesis  Massachusetts Institute of Technology  Cambridge  MA  2010.

[25] B. Peherstorfer  K. Willcox  and M. Gunzburger  “Optimal model management for multiﬁdelity
Monte Carlo estimation ” SIAM Journal on Scientiﬁc Computing  vol. 38  no. 5  pp. A3163–
A3194  2016.

[26] S. Surjanovic and D. Bingham  “Virtual Library of Simulation Experiments: Test Functions and
Datasets  Optimization Test Problems  Emulation/Prediction Test Problems  Branin Function.”
Available at https://www.sfu.ca/~ssurjano/branin.html  last visited 2018-7-31.

11

,Omar El Housni
Vineet Goyal
Alexandre Marques
Remi Lam
Karen Willcox