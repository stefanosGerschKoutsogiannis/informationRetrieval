2012,Ensemble weighted kernel estimators for multivariate entropy estimation,The problem of estimation of entropy functionals of probability densities has received much attention in the information theory  machine learning and statistics communities. Kernel density plug-in estimators are simple  easy to implement and widely used for estimation of entropy. However  kernel plug-in estimators suffer from the curse of dimensionality  wherein the MSE rate of convergence is glacially slow - of order  $O(T^{-{\gamma}/{d}})$  where $T$ is the number of samples  and $\gamma>0$ is a rate parameter. In this paper  it is shown that for sufficiently smooth densities  an ensemble of kernel plug-in estimators can be combined via a weighted convex combination  such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order $O(T^{-1})$. Furthermore  it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density  and therefore can be performed offline. This novel result is remarkable in that  while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality  by appropriate ensemble averaging we can achieve parametric convergence rates.,Ensemble weighted kernel estimators

for multivariate entropy estimation

Kumar Sricharan  Alfred O. Hero III

Department of EECS
University of Michigan
Ann Arbor  MI 48104

{kksreddy hero}@umich.edu

Abstract

The problem of estimation of entropy functionals of probability densities
has received much attention in the information theory  machine learning
and statistics communities. Kernel density plug-in estimators are simple 
easy to implement and widely used for estimation of entropy. However  for
large feature dimension d  kernel plug-in estimators suﬀer from the curse
of dimensionality: the MSE rate of convergence is glacially slow - of order
O(T −γ/d)  where T is the number of samples  and γ > 0 is a rate para-
meter. In this paper  it is shown that for suﬃciently smooth densities  an
ensemble of kernel plug-in estimators can be combined via a weighted con-
vex combination  such that the resulting weighted estimator has a superior
parametric MSE rate of convergence of order O(T −1). Furthermore  it is
shown that these optimal weights can be determined by solving a convex
optimization problem which does not require training data or knowledge of
the underlying density  and therefore can be performed oﬄine. This novel
result is remarkable in that  while each of the individual kernel plug-in es-
timators belonging to the ensemble suﬀer from the curse of dimensionality 
by appropriate ensemble averaging we can achieve parametric convergence
rates.

1

Introduction

Non-linear entropy functionals of a multivariate density f of the form R g(f (x)  x)f (x)dx

arise in applications including machine learning  signal processing  mathematical statistics 
and statistical communication theory. Important examples of such functionals include Shan-
non and R´enyi entropy. Entropy based applications include image registration and texture
classiﬁcation  ICA  anomaly detection  data and image compression  testing of statistical
models and parameter estimation. For details and other applications  see  for example  Beir-
lant et al. [2] and Leonenko et al. [18]. In these applications  the functional of interest must
be estimated empirically from sample realizations of the underlying densities. Several estim-
ators of entropy measures have been proposed for general multivariate densities f . These
include consistent estimators based on histograms [10  2]  kernel density plug-in estimators 
entropic graphs [5  20]  gap estimators [24] and nearest neighbor distances [8  18  19].

Kernel density plug-in estimators [1  6  11  15  12] are simple  easy to implement  computa-
tionally fast and therefore widely used for estimation of entropy [2  23  14  4  13]. However 
these estimators suﬀer from mean squared error (MSE) rates which typically grow with
feature dimension d as O(T −γ/d)  where T is the number of samples and γ is a positive rate
parameter.

1

In this paper  we propose a novel weighted ensemble kernel density plug-in estimator
of entropy ˆGw  that achieves parametric MSE rates of O(T −1) when the feature dens-
ity is smooth. The estimator is constructed as a weighted convex combination ˆGw =
Pl∈¯l w(l) ˆGk(l) of
individual kernel density plug-in estimators ˆGk(l) wrt the weights
{w(l); l ∈ ¯l}. Here  ¯l is a vector of indices {l1  ..  lL} and k(l) = lpT /2 is proportional
to the the volume of the kernel bins used in evaluating ˆGk(l). The individual kernel estim-
ators ˆGk(l) are similar to the data-split kernel estimator of Gy¨orﬁ and van der Muelen [11] 
and have slow MSE rates of convergence of order O(T −1/1+d). Please refer to Section 2 for
the exact deﬁnition of ˆGk(l).
The principal result presented in this paper is as follows.
It is shown that the weights
{w(l); l ∈ ¯l} can be chosen so as to signiﬁcantly improve the rate of MSE convergence
of the weighted estimator ˆGw. In fact our ensemble averaging method can improve MSE
convergence of ˆGw to the parametric rate O(T −1). These optimal weights can be determined
by solving a convex optimization problem. Furthermore  this optimization problem does not
involve any density-dependent parameters and can therefore be performed oﬄine.

1.1 Related work

Ensemble based methods have been previously proposed in the context of classiﬁcation. For
example  in both boosting [21] and multiple kernel learning [16] algorithms  lower complexity
weak learners are combined to produce classiﬁers with higher accuracy. Our work diﬀers
from these methods in several ways. First and foremost  our proposed method performs
estimation rather than classiﬁcation. An important consequence of this is that the weights
we use are data independent   while the weights in boosting and multiple kernel learning
must be estimated from training data since they depend on the unknown distribution.

Birge and Massart [3] show that for density f in a Holder smoothness class with s de-
rivatives  the minimax MSE rate for estimation of a smooth functional is T −2γ  where
γ = min{1/2  4s/(4s + d)}. This means that for s > 4/d  parametric rates are achievable.
The kernel estimators proposed in this paper require higher order smoothness conditions
on the density  i. e. the density must be s > d times diﬀerentiable. While there exist other
estimators [17  7] that achieve parametric MSE rates of O(1/T ) when s > 4/d  these es-
timators are more diﬃcult to implement than kernel density estimators  which are a staple
of many toolboxes in machine learning  pattern recognition  and statistics. The proposed
ensemble weighted estimator is a simple weighted combination of oﬀ-the-shelf kernel density
estimators.

1.2 Organization

The reminder of the paper is organized as follows. We formally describe the kernel plug-in
entropy estimators for entropy estimation in Section 2 and discuss the MSE convergence
properties of these estimators. In particular  we establish that these estimators have MSE
rate which decays as O(T −1/1+d). Next  we propose the weighted ensemble of kernel en-
tropy estimators in Section 3. Subsequently  we provide an MSE-optimal set of weights as
the solution to a convex optimization(3.4) and show that the resultant optimally weighted
estimator has a MSE of O(T −1). We present simulation results in Section 4 that illustrate
the superior performance of this ensemble entropy estimator in the context of (i) estimation
of the Panter-Dite distortion-rate factor [9] and (ii) testing the probability distribution of a
random sample. We conclude the paper in Section 5.

Notation

We will use bold face type to indicate random variables and random vectors and regular
type face for constants. We denote the expectation operator by the symbol E  the variance
operator as V[X] = E[(X − E[X])2]  and the bias of an estimator by B.

2

2 Entropy estimation

This paper focuses on the estimation of general non-linear functionals G(f ) of d-dimensional
multivariate densities f with known support S = [a  b]d  where G(f ) has the form

G(f ) = Z g(f (x)  x)f (x)dµ(x) 

(2.1)

for some smooth function g(f  x). Let B denote the boundary of S. Here  µ denotes the
Lebesgue measure and E denotes statistical expectation with respect to the density f . As-
sume that T = N + M i.i.d realizations of feature vectors {X1  . . .   XN   XN +1  . . .   XN +M}
are available from the density f . In the sequel f will be called the feature density.

2.1 Plug-in estimators of entropy

A truncated kernel density estimator with uniform kernel is deﬁned below. Our proposed
weighted ensemble method applies to other types of kernels as well but we specialize to
uniform kernels as it makes the derivations clearer. For integer 1 ≤ k ≤ M   deﬁne the
distance dk to be: dk = (k/M )1/d. Deﬁne the truncated kernel bin region for each X ∈ S
to be Sk(X) = {Y ∈ S : ||X − Y ||1 ≤ dk/2}  and the volume of the truncated kernel bins
to be Vk(X) = RSk(X) dz. Note that when the smallest distance from X to S is greater
than dk  Vk(X) = dd
k = k/M . Let lk(X) denotes the number of points falling in Sk(X):
lk(X) = PM

i=1 1{Xi∈Sk(X)}. The truncated kernel density estimator is deﬁned as

ˆfk(X) =

.

(2.2)

lk(X)

M Vk(X)

The plug-in estimator of the density functional is constructed using a data splitting ap-
proach as follows. The data is randomly subdivided into two parts {X1  . . .   XN} and
{XN +1  . . .   XN +M} of N and M points respectively.
In the ﬁrst stage  we estimate
the kernel density estimate ˆfk at the N points {X1  . . .   XN} using the M realizations
{XN +1  . . .   XN +M}. Subsequently  we use the N samples {X1  . . .   XN} to approximate
the functional G(f ) and obtain the plug-in estimator:

ˆGk =

1
N

N

Xi=1

g(ˆf k(Xi)  Xi).

(2.3)

Also deﬁne a standard kernel density estimator with uniform kernel ˜fk(X)  which is identical
to ˆfk(X) except that the volume Vk(X) is always set to be Vk(X) = k/M . Deﬁne

˜Gk =

1
N

N

Xi=1

g(˜f k(Xi)  Xi).

(2.4)

The estimator ˜Gk is identical to the estimator of Gy¨orﬁ and van der Muelen [11]. Observe
that the implementation of ˜Gk  unlike ˆGk  does not require knowledge about the support
of the density.

2.1.1 Assumptions

We make a number of technical assumptions that will allow us to obtain tight MSE con-
vergence rates for the kernel density estimators deﬁned in above. These assumptions are
comparable to other rigorous treatments of entropy estimation. Please refer to Section
II  [2] for details. (A.0) : Assume that the kernel bandwidth satisﬁes k = k0M β for any
rate constant 0 < β < 1  and assume that M   N and T are linearly related through the
proportionality constant αf rac with: 0 < αf rac < 1  M = αf racT and N = (1 − αf rac)T .
(A.1) : Let the feature density f be uniformly bounded away from 0 and upper bounded
on the set S  i.e.  there exist constants ǫ0  ǫ∞ such that 0 < ǫ0 ≤ f (x) ≤ ǫ∞ < ∞ ∀x ∈ S.
(A.2): Assume that the density f has continuous partial derivatives of order d in the in-
terior of the set S  and that these derivatives are upper bounded. (A.3): Assume that the

3

function g(f  x) has max{λ  d} partial derivatives w.r.t. the argument f   where λ satisﬁes
the conditions λβ > 1. Denote the n-th partial derivative of g(f  x) wrt x by g(n)(f  x).
Also  let g′(f  x) := g(1)(f  x) and g′′(f  x) := g(2)(f  x). (A.4): Assume that the absolute
value of the functional g(f  x) and its partial derivatives are strictly bounded away from
∞ in the range ǫ0 < x < ǫ∞ for all y. (A.5): Let ǫ ∈ (0  1) and δ ∈ (2/3  1). Let C(M )
be a positive function satisfying the condition C(M ) = O(exp(−M β(1−δ))). For some ﬁxed
0 < ǫ < 1  deﬁne pl = (1 − ǫ)ǫ0 and pu = (1 + ǫ)ǫ∞. Assume that the following four condi-
tions are satisﬁed by h(f  x) = g(f  x)  g(3)(f  x) and g(λ)(f  x) : (i) supx |h(0  x)| = G1 < ∞ 
(ii) supf ∈(pl pu) x |h(f  x)| = G2/4 < ∞  (iii) supf ∈(1/k pu) x |h(f  x)|C(M ) = G3 < ∞  and
(iv)E[supf ∈(pl 2dM/k) x |h(f  x)|]C(M ) = G4 < ∞.
2.1.2 Analysis of MSE

Under these assumptions  we have shown the following (please see [22] for the proof) :
Theorem 1. The bias of the plug-in estimators ˆGk  ˜Gk is given by
M(cid:19)
k

M(cid:19)i/d

+ o(cid:18) 1

c2
k

+

+

B( ˆGk) = Xi∈I
B( ˜Gk) = c1(cid:18) k

c1 i(cid:18) k
M(cid:19)1/d

k

+

c2
k

+ o(cid:18) 1

k

+

k

M(cid:19) .

Theorem 2. The variance of the plug-in estimators ˆGk  ˜Gk is given by

V( ˆGk) = c4(cid:18) 1
V( ˜Gk) = c4(cid:18) 1

N(cid:19) + c5(cid:18) 1
N(cid:19) + c5(cid:18) 1

M(cid:19) + o(cid:18) 1
M(cid:19) + o(cid:18) 1

M

M

+

+

1

N(cid:19)
N(cid:19) .

1

In the above expressions  c1 i  c1  c2  c4 and c5 are constants that depend only on g  f and
their partial derivatives  and I = {1  . . .   d}. In particular  the constants c1 i  c1  c2  c4 and
c5 are independent of k  N and M .

2.1.3 Optimal MSE rate

From Theorem 1  k → ∞ and k/M → 0 for the estimators ˆGk and ˜Gk to be unbiased.
Likewise from Theorem 2 N → ∞ and M → ∞ for the variance of the estimator to
converge to 0. We can optimize the choice of bandwidth k  and the data splitting proportions
N/(N + M )  M/(N + M ) for minimum M.S.E.

Minimizing the MSE over k is equivalent to minimizing the bias over k. The optimal choice
of k is given by kopt = O(M 1/(1+d))  and the bias evaluated at kopt is O(M −1/(1+d)). Also
observe that the MSE of ˆGk and ˜Gk is dominated by the squared bias (O(M −2/(1+d))) as
contrasted to the variance (O(1/N + 1/M )). This implies that the asymptotic MSE rate of
convergence is invariant to selected proportionality constant αf rac. The optimal MSE for
the estimators ˆGk and ˜Gk is therefore achieved for the choice of k = O(M 1/(1+d))  and is
given by O(T −2/(1+d)). In particular  observe that both ˆGk and ˜Gk have identical optimal
rates of MSE convergence. Our goal is to reduce the estimator MSE to O(T −1). We do so
by applying the method of weighted ensembles described next in section 3.

3 Ensemble estimators

For a positive integer L > d  choose ¯l = {l1  . . .   lL} to be a vector of distinct positive real
numbers. Deﬁne the mapping k(l) = l√M and let ¯k = {k(l); l ∈ ¯l}. Observe that any k ∈ ¯k
corresponds to the rate constant β = 0.5  and that N = Θ(T ) and M = Θ(T ). Deﬁne the
weighted ensemble estimator

ˆGw = Xl∈¯l

w(l) ˆGk(l).

4

(3.1)

Theorem 3. There exists a weight vector w∗ such that

E[( ˆGw∗ − G(f ))2] = O(1/T ).

This weight vector can be found by solving a convex optimization. Furthermore  this op-
timal weight vector does not depend on the unknown feature density f or the samples
{X1  ..  XN +M}  and hence can be solved oﬀ-line.
Proof. For each i ∈ I  deﬁne γw(i) = Pl∈¯l w(l)li/d. The bias of the ensemble estimator

follows from Theorem 1 and is given by

B[ ˆGw] = Xi∈I

c1 iγw(i)M −i/2d + O(cid:18) 1

√T (cid:19) .

(3.2)

Denote the covariance matrix of { ˆGk(l); l ∈ ¯l} by ΣL. Let ¯ΣL = ΣLT . Observe that by
(2.5) and the Cauchy-Schwarz inequality  the entries of ¯ΣL are O(1). The variance of the
weighted estimator ˆGw can then be bound as follows:

V[ ˆGw] = V
Xl∈¯l

wl ˆGk(l)

 = w′ΣLw =

w′ ¯ΣLw

T

≤

λmax( ¯ΣL)||w||2

2

T

.

(3.3)

We seek a weight vector w that (i) ensures that the bias of the weighted estimator is
O(T −1/2) and (ii) has low ℓ2 norm ||w||2 in order to limit the contribution of the variance
of the weighted estimator. To this end  let w∗ be the solution to the convex optimization
problem

w

minimize

||w||2
subject to Xl∈¯l

w(l) = 1 

|γw(i)| = 0  i ∈ I.

(3.4)

This problem is equivalent to minimizing ||w||2 subject to A0w = b  where A0 and b are
deﬁned below. Let fIN : I → {1  ..  I} be a bijective mapping. Let a0 be the vector of
ones:
L ]. Deﬁne
A0 = [a′
I ] and b = [1; 0; 0; ..; 0](I+1)×1. Observe that the entries
of A0 and b are O(1)  and therefore the entries of the solution w∗ are O(1). Consequently 

[1  1...  1]1×L; and let afIN (i)  for i ∈ I be given by afIN (i) = [li/d
0  a′

by (3.2)  the bias B[ ˆGw∗] = O(1/√T ). Furthermore  the optimal minimum η(d) := ||w∗||2
0). By (6.4)  the estimator variance V[ ˆGw∗] is of
is given by η(d) = pdet(A1A′

1)/det(A0A′
order O(η(d)/T ). This concludes the proof.

I ]′  A1 = [a′

1   ..  li/d

1  ...  a′

1  ...  a′

While we have illustrated the weighted ensemble method only in the context of kernel
estimators  this method can be applied to any general ensemble of estimators that satisfy
bias and variance conditions C .1 and C .2 in [22].

4 Experiments

We illustrate the superior performance of the proposed weighted ensemble estimator for two
applications: (i) estimation of the Panter-Dite rate distortion factor  and (ii) estimation of
entropy to test for randomness of a random sample.

4.1 Panter-Dite factor estimation

For a d-dimensional source with underlying density f   the Panter-Dite distortion-rate
distortion-rate function [9] for a q-dimensional vector quantizer with n levels of quantiz-

ation is given by δ(n) = n−2/qR f q/(q+2)(x)dx. The Panter-Dite factor corresponds to the

5

100

 

r
o
r
r
e
d
e
r
a
u
q
s
 
n
a
e
M

10−1

10−2

10−3

10−4

 

103

 

104

Standard kernel plug−in estimator [12]
Truncated kernel plug−in estimator (2.3)
Histogram plug−in estimator [11]
k−nearest neighbor estimator [19]
Entropic graph estimator [6 21]
Weighted kernel estimator (3.1)

Sample size T

(a) Variation of MSE of Panter-Dite factor estimates as a function of
sample size T . From the ﬁgure  we see that the proposed weighted es-
timator has the fastest MSE rate of convergence wrt sample size T .

100

r
o
r
r
e

 

d
e
r
a
u
q
s
 

n
a
e
M

10−1

10−2

10−3

10−4
1

 

2

3

4

 

10

Standard kernel plug−in estimator [12]
Truncated kernel plug−in estimator (2.3)
Histogram plug−in estimator [11]
k−nearest neighbor estimator [19]
Entropic graph estimator [6 21]
Weighted kernel estimator (3.1)
6
5
9
dimension d

7

8

(b) Variation of MSE of Panter-Dite factor estimates as a function of di-
mension d. From the ﬁgure  we see that the MSE of the proposed weighted
estimator has the slowest rate of growth with increasing dimension d.

Figure 1: Variation of MSE of Panter-Dite factor estimates using standard kernel plug-in es-
timator [12]  truncated kernel plug-in estimator (2.3)  histogram plug-in estimator[11]  k-NN
estimator [19]  entropic graph estimator [6 21] and the weighted ensemble estimator (3.1).

functional G(f ) with g(f  x) = n−2/qf −2/(q+2)I(f > 0) + I(f = 0)  where I(.) is the indic-
ator function. The Panter-Dite factor is directly related to the R´enyi α-entropy  for which
several other estimators have been proposed.

In our simulations we compare six diﬀerent choices of functional estimators - the three
estimators previously introduced: (i) the standard kernel plug-in estimator ˆGk  (ii) the
boundary truncated plug-in estimator ˆGk and (iii) the weighted estimator ˆGw with optimal
weight w = w∗ given by (3.4)  and in addition the following popular entropy estimators: (iv)
histogram plug-in estimator [10]  (v) k-nearest neighbor (k-NN) entropy estimator [18] and
(vi) entropic k-NN graph estimator [5  20]. For both ˜Gk and ˆGk  we select the bandwidth
parameter k as a function of M according to the optimal proportionality k = M 1/(1+d) and
N = M = T /2. To illustrate the weighted estimator of the Panter-Dite factor we assume
that f is the d = 6 dimensional mixture density f (a  b  p  d) = pfβ(a  b  d) + (1 − p)fu(d);
where fβ(a  b  d) is a d-dimensional Beta density with parameters a = 6  b = 6  fu(d) is a
d-dimensional uniform density and the mixing ratio p is 0.8.

4.1.1 Variation of MSE with sample size T

The MSE results of these diﬀerent estimators are shown in Fig. 1(a) as a function of sample
size T . It is clear from the ﬁgure that the proposed ensemble estimator ˆGw has signiﬁcantly

6

1

0.5

0

Hypothesis index
True entropy
Standard kernel plug−in estimate
Truncated kernel plug−in estimate
Weighted plug−in estimate

−0.5

Standard kernel plug−in estimate

 

−1

−1.5

−2

 

−2.5
0

Truncated kernel plug−in estimate

   Weighted plug−in estimate

100

200

300

400

500

600

700

800

900

1000

100

50

0
−1

100

50

0
−1.4

100

50

0
−2.3

−0.9

−1.35

−1.3

−0.8

−0.7

−0.6

Standard kernel plug−in estimate

−0.5

−1.25
Truncated kernel plug−in estimate

−1.15

−1.2

−1.1

−0.4

−0.3

−1.05

−1

−0.95

−2.2

−2.1

−2

−1.9

−1.8

−1.7
Weighted estimate

−1.6

−1.5

−1.4

−1.3

(a) Entropy estimates for random samples cor-
responding to hypothesis H0 and H1.

(b) Histogram envelopes of entropy estimates
for random samples corresponding to hypo-
thesis H0 (blue) and H1 (red).

Figure 2: Entropy estimates using standard kernel plug-in estimator  truncated kernel plug-
in estimator and the weighted estimator  for random samples corresponding to hypothesis
H0 and H1. The weighted estimator provided better discrimination ability by suppressing
the bias  at the cost of some additional variance.

faster rate of convergence while the MSE of the rest of the estimators  including the truncated
kernel plug-in estimator  have similar  slow rates of convergence. It is therefore clear that the
proposed optimal ensemble averaging signiﬁcantly accelerates the MSE convergence rate.

4.1.2 Variation of MSE with dimension d

The MSE results of these diﬀerent estimators are shown in Fig. 1(b) as a function of di-
mension d  for ﬁxed sample size T = 3000. For the standard kernel plug-in estimator and
truncated kernel plug-in estimator  the MSE varies exponentially with d as expected. The
MSE of the histogram and k-NN estimators increase at a similar rate  indicating that these
estimators suﬀer from the curse of dimensionality as well. The MSE of the weighted estim-
ator on the other hand increases at a slower rate  which is in agreement with our theory that
the MSE is O(η(d)/T ) and observing that η(d) is an increasing function of d. Also observe
that the MSE of the weighted estimator is signiﬁcantly smaller than the MSE of the other
estimators for all dimensions d > 3.

4.2 Distribution testing

In this section  Shannon diﬀerential entropy is estimated using the function g(f  x) =
− log(f )I(f > 0) + I(f = 0) and used as a test statistic to test for the underlying probab-
ility distribution of a random sample. In particular  we draw 500 instances each of random
samples of size 103 from the probability distribution f (a  b  p  d)  described in Sec. 4. 1  with
ﬁxed d = 6  p = 0.75 for two sets of values of a  b under the null and alternate hypothesis 
H0 : a = a0  b = b0 versus H1 : a = a1  b = b1.

First  we ﬁx a0 = b0 = 6 and a1 = b1 = 5. We note that the underlying density under the
null hypothesis f (6  6  0.75  6) has greater curvature relative to f (5  5  0.75  6) and therefore
has smaller entropy (randomness). The true entropy  and entropy estimates using ˜Gk  ˆGk
and ˆGw for the cases corresponding to each of the 500 instances of hypothesis H0 and H1
are shown in Fig. 2(a). From this ﬁgure  it is apparent that the weighted estimator provides
better discrimination ability by suppressing the bias  at the cost of some additional variance.

To demonstrate that the weighted estimator provides better discrimination  we plot the
histogram envelope of the entropy estimates using standard kernel plug-in estimator  trun-
cated kernel plug-in estimator and the weighted estimator for the cases corresponding to
the hypothesis H0 (color coded blue) and H1 (color coded red) in Fig. 2(b). Furthermore 
we quantitatively measure the discriminative ability of the diﬀerent estimators using the
1  where µ0 and σ0 (respectively µ1 and σ1) are

0 + σ2

deﬂection statistic ds = |µ1 − µ0|/pσ2

7

e

t

a
r
 

e
v
i
t

 

a
g
e
N
e
s
a
F

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

 

 

1

e
v
r
u
c
 
C
O
R

 
r
e
d
n
u
a
e
r
A

 

0.9

0.8

0.7

0.6

Standard kernel plug−in estimator
Truncated kernel plug−in estimator
Weighted estimator

0.05

0.1

0.15

0.2
0.3
False Positive rate

0.25

0.35

0.4

0.45

0.5

0.5

 

0.2

0.4

 

Neyman−Pearson test
Standard kernel plug−in estimate
Truncated kernel plug−in estimate
Weighted estimate
0.6

0.8

1

δ

(a) ROC curves corresponding to entropy estim-
ates obtained using standard and truncated ker-
nel plug-in estimator and the weighted estimator.
The corresponding AUC are given by 0.9271 
0.9459 and 0.9619.

(b) Variation of AUC curves vs δ(= a0 − a1  b0 −
b1) corresponding to Neyman-Pearson omni-
scient test  entropy estimates using the standard
and truncated kernel plug-in estimator and the
weighted estimator.

Figure 3: Comparison of performance in terms of ROC for the distribution testing problem.
The weighted estimator uniformly outperforms the individual plug-in estimators.

the sample mean and standard deviation of the entropy estimates. The deﬂection statistic
was found to be 1.49  1.60 and 1.89 for the standard kernel plug-in estimator  truncated
kernel plug-in estimator and the weighted estimator respectively. The receiver operating
curves (ROC) for this test using these three diﬀerent estimators is shown in Fig. 3(a). The
corresponding area under the ROC curves (AUC) are given by 0.9271  0.9459 and 0.9619.
In our ﬁnal experiment  we ﬁx a0 = b0 = 10 and set a1 = b1 = 10 − δ  draw 500 instances
each of random samples of size 5 × 103 under the null and alternate hypothesis  and plot
the AUC as δ varies from 0 to 1 in Fig. 3(b). For comparison  we also plot the AUC for the
Neyman-Pearson likelihood ratio test. The Neyman-Pearson likelihood ratio test  unlike
the Shannon entropy based tests  is an omniscient test that assumes knowledge of both
the underlying beta-uniform mixture parametric model of the density and the parameter
values a0  b0 and a1  b1 under the null and alternate hypothesis respectively. Figure 4 shows
that the weighted estimator uniformly and signiﬁcantly outperforms the individual plug-in
estimators and is closest to the performance of the omniscient Neyman-Pearson likelihood
test. The relatively superior performance of the Neyman-Pearson likelihood test is due to
the fact that the weighted estimator is a nonparametric estimator that has marginally higher
variance (proportional to ||w∗||2
2) compared to the underlying parametric model for which
the Neyman-Pearson test statistic provides the most powerful test.

5 Conclusions

A novel method of weighted ensemble estimation was proposed in this paper. This method
combines slowly converging individual estimators to produce a new estimator with faster
MSE rate of convergence.
In this paper  we applied weighted ensembles to improve the
MSE of a set of uniform kernel density estimators with diﬀerent kernel width parameters.
We showed by theory and in simulation that that the improved ensemble estimator achieves
parametric MSE convergence rate O(T −1). The optimal weights are determined by solving
a convex optimization problem which does not require training data and can be performed
oﬄine. The superior performance of the weighted ensemble entropy estimator was veriﬁed
in the context of two important problems: (i) estimation of the Panter-Dite factor and (ii)
non-parametric hypothesis testing.

Acknowledgments

This work was partially supported by ARO grant W911NF-12-1-0443.

8

References

[1] I. Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous

distributions (corresp.). Information Theory  IEEE Trans. on  22(3):372 – 375  May 1976.

[2] J. Beirlant  EJ Dudewicz  L. Gy¨orﬁ  and EC Van der Meulen. Nonparametric entropy estim-

ation: An overview. Intl. Journal of Mathematical and Statistical Sciences  6:17–40  1997.

[3] L. Birge and P. Massart. Estimation of integral functions of a density. The Annals of Statistics 

23(1):11–29  1995.

[4] D. Chauveau and P. Vandekerkhove. Selection of a MCMC simulation strategy via an entropy

convergence criterion. ArXiv Mathematics e-prints  May 2006.

[5] J.A. Costa and A.O. Hero. Geodesic entropic graphs for dimension and entropy estimation in

manifold learning. Signal Processing  IEEE Transactions on  52(8):2210–2221  2004.

[6] P. B. Eggermont and V. N. LaRiccia. Best asymptotic normality of the kernel density entropy
estimator for smooth densities. Information Theory  IEEE Trans. on  45(4):1321 –1326  May
1999.

[7] E. Gin´e and D.M. Mason. Uniform in bandwidth estimation of integral functionals of the

density function. Scandinavian Journal of Statistics  35:739761  2008.

[8] M. Goria  N. Leonenko  V. Mergel  and P. L. Novi Inverardi. A new class of random vec-
tor entropy estimators and its applications in testing statistical hypotheses. Nonparametric
Statistics  2004.

[9] R. Gupta. Quantization Strategies for Low-Power Communications. PhD thesis  University of

Michigan  Ann Arbor  2001.

[10] L. Gy¨orﬁ and E. C. van der Meulen. Density-free convergence properties of various estimators

of entropy. Comput. Statist. Data Anal.  pages 425–436  1987.

[11] L. Gy¨orﬁ and E. C. van der Meulen. An entropy estimate based on a kernel density estimation.

Limit Theorems in Probability and Statistics  pages 229–240  1989.

[12] P. Hall and S. C. Morton. On the estimation of the entropy. Ann. Inst. Statist. Meth.  45:69–88 

1993.

[13] K. Hlav´aˇckov´a-Schindler  M. Paluˇs  M. Vejmelka  and J. Bhattacharya. Causality detection
based on information-theoretic approaches in time series analysis. Physics Reports  441(1):1–
46  2007.

[14] A.T. Ihler  J.W. Fisher III  and A.S. Willsky. Nonparametric estimators for online signature
authentication. In Acoustics  Speech  and Signal Processing  2001. Proceedings.(ICASSP’01).
2001 IEEE International Conference on  volume 6  pages 3473–3476. IEEE  2001.

[15] H. Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the

Institute of Statistical Mathematics  41(4):683–697  1989.

[16] G. Lanckriet  N. Cristianini  P. Bartlett  and L. El Ghaoui. Learning the kernel matrix with

semi-deﬁnite programming. Journal of Machine Learning Research  5:2004  2002.

[17] B. Laurent. Eﬃcient estimation of integral functionals of a density. The Annals of Statistics 

24(2):659–681  1996.

[18] N. Leonenko  L. Prozanto  and V. Savani. A class of R´enyi information estimators for multi-

dimensional densities. Annals of Statistics  36:2153–2182  2008.

[19] E. Liiti¨ainen  A. Lendasse  and F. Corona. On the statistical estimation of r´enyi entropies.
In Proceedings of IEEE/MLSP 2009 International Workshop on Machine Learning for Signal
Processing  Grenoble (France)  September 2-4 2009.

[20] D. Pal  B. Poczos  and C. Szepesvari. Estimation of R´enyi entropy and mutual information
based on generalized nearest-neighbor graphs. In Proc. Advances in Neural Information Pro-
cessing Systems (NIPS). MIT Press  2010.

[21] Robert E. Schapire. The strength of weak learnability. Machine Learning  5(2):197–227–227 

June 1990.

[22] K. Sricharan and A. O. Hero  III. Ensemble estimators for multivariate entropy estimation.

ArXiv e-prints  March 2012.

[23] C. Studholme  C. Drapaca  B. Iordanova  and V. Cardenas. Deformation-based mapping of
volume change from serial brain mri in the presence of local tissue contrast change. Medical
Imaging  IEEE Transactions on  25(5):626–639  2006.

[24] B. van Es. Estimating functionals related to a density by class of statistics based on spacing.

Scandinavian Journal of Statistics  1992.

9

,Mikhail Figurnov
Shakir Mohamed
Andriy Mnih