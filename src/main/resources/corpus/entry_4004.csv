2011,On the accuracy of l1-filtering of signals with block-sparse structure,We discuss new methods for the recovery of signals with block-sparse structure  based on l1-minimization. Our emphasis is on the efficiently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance.,On the accuracy of (cid:96)1-ﬁltering of signals with

block-sparse structure

Anatoli Juditsky∗

Fatma Kılınc¸ Karzan†

Arkadi Nemirovski‡

Boris Polyak§

Abstract

We discuss new methods for the recovery of signals with block-sparse structure 
based on (cid:96)1-minimization. Our emphasis is on the efﬁciently computable error
bounds for the recovery routines. We optimize these bounds with respect to the
method parameters to construct the estimators with improved statistical proper-
ties. We justify the proposed approach with an oracle inequality which links the
properties of the recovery algorithms and the best estimation performance.

Introduction

1
Suppose an observation y ∈ Rm is available where

y = Ax + u + Dξ.

(1)
Here A is a given m × n sensing matrix  x ∈ Rn is an unknown vector  u is an unknown (determin-
istic) nuisance parameter  known to belong to a certain set U ⊂ Rm  D ∈ Rm×m is known noise
intensity matrix  and ξ ∈ Rm is random noise with standard normal distribution.
We aim to recover a linear transformation w = Bx of the signal x  where B is a given N × n matrix 
under the assumption that w is block-sparse. Namely  the space W(= RN ) where w “lives” is
represented as W = Rn1 × ...×RnK   so that w = Bx ∈ RN is a block vector: w = [w[1]; ...; w[K]]
with blocks w[k] = B[k]x ∈ Rnk  1 ≤ k ≤ K  where B[k]  1 ≤ k ≤ K are nk × n matrices.
The s-block-sparsity of w means that at most a given number s of the blocks u[k]  1 ≤ k ≤ K  are
nonzero.
To motivate the interest for the presented model  let us consider two examples.

Tracking of a singularly perturbed linear system Consider a discrete-time linear system

z[i] = Gz[i − 1] + w[i] + F η[i] 

i = 1  2  ...  z[0] ∈ Rd 

where F η[i] are random perturbations with η[i] being i.i.d. standard normal vectors η[i] ∼ N (0  Id) 
and G  F ∈ Rd×d are known matrices. We assume that the perturbation vectors w[i] ∈ Rd i =
1  2  ... are mostly zeros  but a small proportion of w[i] are nonvanishing unknown vectors in Rd.
Suppose that we are given the linear noisy observation y ∈ Rm  such that y = A[z[0]; ...z[K]] + σξ 
where the matrix A ∈ Rm×d(K+1) and the noise intensity σ > 0 are known  and ξ ∼ N (0  Im).
k=1  w[k] ∈ Rd  and
Given y  our objective is to recover the sequence of perturbations w = [w[k]]K
the trajectory z of the system.

∗LJK  Universit´e J. Fourier  B.P. 53  38041 Grenoble Cedex 9  France  Anatoli.Juditsky@imag.fr
†Carnegie Mellon University  Pittsburgh  Pennsylvania 15213  USA  fkilinc@andrew.cmu.edu
‡Georgia Institute of Technology  Atlanta  Georgia 30332  USA  nemirovs@isye.gatech.edu
Research of the second and the third authors was supported by the Ofﬁce of Naval Research grant #240667V.
§Institute of Control Sciences of Russian Academy of Sciences  Moscow 117997  Russia 

boris@ipu.rssi.ru

1

To ﬁt the tracking problem into the basic framework let us decompose z = x + ζ  where x =
[x[0]; ...; x[K]] with x[i] = Gx[i − 1] + w[i]  x[0] = z[1]  and ζ = [ζ[0]; ...; ζ[K]] with ζ[i] =
Gζ[i − 1] + F η[i]  ζ[0] = 0. Then

y = Ax + [Aζ + σξ] 

where the distribution of Aζ + σξ is normal with zero mean and the covariance matrix D2 =
AV AT + σ2I. Here the Kd × Kd covariance matrix V of ζ has the block structure with blocks

(cid:96)∧k(cid:88)

i=1

V k (cid:96) = Cov(ζ[k]  ζ[(cid:96)]) =

G(cid:96)−iF F T (GT )k−i 

with(cid:80)0

1 = 0  by convention.

Image reconstruction with regularization by Total Variation (TV)
[21  7] Here one looks to
recover an image Z ∈ Rn1×n2 from a blurred noisy observation y: y = Az + σξ  y ∈ Rm  where
z = Col(Z) ∈ Rn  n = n1n2  A ∈ Rm×n is the matrix of discrete convolution  σ > 0 is known 
and ξ ∼ N (0  Im). We assume that the image z may be decomposed as z = x + v  where v is a
“regular component”  which is modeled by restricting v to belong to the set V of “smooth images”;
let w = Bx ∈ Rn×2  be the (discretized) gradient of the x-component at the points of the grid.
In this example Bx naturally splits into 2-dimensional blocks and TV is nothing but the sum of (cid:96)2
norms of these blocks. We suppose that w is (nearly) sparse. When denoting u = Av we come to
the observation model y = Ax + u + Dξ  with u ∈ U = AV  D = σI  and ξ ∼ N (0  Im).

The recovery routines we consider are based on the block-(cid:96)1 minimization  i.e.  the estimate (cid:98)w(y)
of w = Bx is (cid:98)w = B(cid:98)x(y)  where(cid:98)x(y) is obtained by minimizing the norm(cid:80)K
k=1 (cid:107)B[k]z(cid:107)(k) over
signals z ∈ Rn with Az “ﬁtting ” in certain precise sense  the observations y. Above  (cid:107) · (cid:107)(k) are
given in advance norms on the spaces Rnk where the blocks of w take their values.
In the sequel we refer to the given in advance collection B = (B  n1  ...  nK (cid:107) · (cid:107)(1)  ... (cid:107) · (cid:107)(K))
as to the representation structure. Given such a structure B and matrix A  our goal is to understand
how well one can recover the s-block-sparse transform Bx by appropriately implemented block-(cid:96)1
minimization.

Related Compressed Sensing research Our situation and goal form a straightforward extension
of the usual block sparsity Compressed Sensing framework. Indeed  the standard representation
structure B = In  nk = 1  (cid:107)·(cid:107)(k) = |·|  1 ≤ k ≤ K = n  leads to the standard Compressed Sensing
setting – recovering a sparse signal x ∈ Rn from its noisy observations (1) via (cid:96)1 minimization. With
the same B = In and nontrivial block structure {nk (cid:107) · (cid:107)(k)}K
k=1  we arrive at block-sparsity and
related block-(cid:96)1 minimization routines considered in numerous recent papers. There is a number
of applications where block-sparsity seem to arise naturally (see  e.g.  [10] and references therein).
Several methods of estimation and selection extending the plain (cid:96)1-minimization to block sparsity
were proposed and investigated recently. Most of the related research is focused so far on block
regularization schemes – Lasso-type algorithms

(cid:8)(cid:107)Az − y(cid:107)2

2 + λL[1 2](z)(cid:9)   L[1 2](z) =

K(cid:88)

k=1

(cid:107)z[k](cid:107)2 

(cid:98)x(y) ∈

Argmin

z=[z[1];...;z[K]]∈Rn=Rn1×...×RnK

(cid:107) · (cid:107)2 being the “usual” (cid:96)2-norm on Rnk. In particular  the huge literature on plain Lasso has a
signiﬁcant counterpart on group Lasso  see  e.g.  [1  2  8  9  10  11  16  19  20  22  23]  and references
therein. Another classical Compressed Sensing estimator  Dantzig Selector  is studied in the block-
sparse case in [12  17]. The available theoretical results allows to bound the errors of recovery in
terms of magnitude of the observation noise and “s-concentration” of the true signal x (that is  its
L[1 2] distance from the space of signals with at most s nonzero blocks). Typically  these results deal
with the quadratic risks of estimation and rely on a natural block analogy (“Block RIP ” see  e.g. 
[10]) of the celebrated Restricted Isometry property for the sensing matrix A  introduced by Cand´es
and Tao [5]  or on a block analogy [18] of the Restricted Eigenvalue property from [3].

Contributions of this work To the best of our knowledge  the conditions used when studying
theoretical properties of block-sparse recovery (with a notable exception of the Mutual Block In-
coherence condition of [9]) are unveriﬁable. The latter means that given the matrix A  one cannot

2

answer in any reasonable time if the (block-) RI or RE property holds with given parameters. While
the efﬁcient veriﬁability of a condition is by no means necessary for a condition to be meaningful
and useful  we believe also that veriﬁability has its value and is worthy of being investigated. In par-
ticular  the veriﬁability property allows us to design new recovery routines with explicit conﬁdence
bounds for the recovery error and optimize these bounds with respect to the method parameters.
Thus  the major novelty in what follows is the emphasis on veriﬁable conditions on A and the repre-
sentation structure which guarantee good recovery of Bx from noisy observations of Ax  provided
that Bx is nearly s-block-sparse  and the observation noise is low. In this respect  this work extends
the results of [15  13  14]  where (cid:96)1-recovery of the “usual” sparse vectors was considered (in the
ﬁrst two papers – in the case of uncertain-but-bounded observation errors  and in the third – in the
case of Gaussian observation noise). We propose new routines of block-sparse recovery which ex-
plicitly utilize the veriﬁability certiﬁcate – the contrast matrix  and show how these routines may be
tuned to attain the best performance bounds.
The rest of the manuscript is organized as follows.
In Section 2 we give the detailed problem
statement and introduce the family Qs q  1 ≤ q ≤ ∞  of conditions which underly the subsequent
developments. Then in Section 2.3 we introduce the recovery routines and provide the bounds for
their risks. We discuss the properties of conditions Qs q in Section 3. In particular  in Section 3.1 we
show how one can efﬁciently verify (the strongest from the family Qs q) condition Qs ∞. Then in
Section 3.2 we provide an oracle inequality which shows that the condition Qs ∞ is also necessary
for recovery of block-sparse signals in (cid:96)∞-norm.

2 Accuracy bounds for (cid:96)1-recovery routines

i=1 |w[k]i|r)

1

1 ≤ r ≤ ∞: (cid:107)w[k](cid:107)r = ((cid:80)nk

2.1 Problem statement and notations
Let w = Bx ∈ W = Rn1 × ... × RnK . To streamline the presentation  we restrict ourselves to
the case where all the norms (cid:107) · (cid:107)(k) on the factors of the representations are the usual (cid:96)r-norms 
r   i.e.  the representation structures we consider are
B = (B  n1  ...  nK  (cid:107) · (cid:107)r). Let r∗ = r
r−1  so that (cid:107) · (cid:107)r∗ is the norm conjugate to (cid:107) · (cid:107)r. A vector
w = [w[1]; ...; w[K]] from W is called s-block-sparse  if the number of nonzero blocks w[k] ∈ Rnk
in w is at most s.
For w ∈ W  we call the number (cid:107)w[k](cid:107)r the magnitude of k-th block in w  and denote by ws the
representation vector obtained from w by zeroing out all but the s largest in magnitude blocks in
w (with the ties resolved arbitrarily). For w ∈ W and 1 ≤ p ≤ ∞  we denote by L[p r](w) the
(cid:107)·(cid:107)p-norm of the vector [(cid:107)w[1](cid:107)r; ...;(cid:107)w[K](cid:107)r]  so that L[p r](·) is a norm on W with the conjugate
p−1. Given a positive integer s ≤ K  we
norm L∗
set Ls [p r](w) = L[p r](ws); note that Ls [p r](·) is a norm on W. When the representation structure
B of x (and thus the norm (cid:107) · (cid:107)r) is ﬁxed  we use the notation Lp  L∗
p  and Ls p  instead of L[p r] 
L∗
[p r]  and Ls [p r].
The recovery problem we are interested in is as follows: suppose we are given an indirect observation
(cf (1))

[p r](w) = (cid:107)[(cid:107)w[1](cid:107)(r∗); ...;(cid:107)w[K](cid:107)(r∗)](cid:107)p∗  p∗ = p

y = Ax + u + Dξ

of unknown signal x ∈ Rn. Here A ∈ Rm×n  u + Dξ is the observation error; in this error  u is
an unknown nuisance known to belong to a given compact convex set U ⊂ Rm symmetric w.r.t. the
origin  D ∈ Rm×m is known  and ξ ∼ N (0  Im).
We want to recover x and the representation w = Bx of x  knowing in advance that this representa-
tion is nearly s-block-sparse  for some given s. Speciﬁcally  we consider the set

X(s  υ) = {x ∈ Rn : L1(Bx − [Bx]s) ≤ υ}.

A recovery routine is a Borel function(cid:98)x(y) : Rm → Rn and we characterize the performance of
such a routine by its Lp-risk of recovery (cid:98)w(y) = B(cid:98)x(y) of w = Bx:

= inf {δ : Probξ {Lp ((cid:98)w(y) − w) ≤ δ ∀(u ∈ U  x ∈ X(s  υ))} ≥ 1 − } .

Riskp((cid:98)w(·)|s  D  υ  )

3

here 0 ≤  ≤ 1 and 1 ≤ p ≤ ∞. In other words  Riskp((cid:98)w(·)|s  D  υ  ) ≤ δ if and only if there
norm of B[(cid:98)x(y) − x] is ≤ δ whenever ξ ∈ Ξ  u ∈ U  and whenever x ∈ Rn is such that Bx

exists a set Ξ ∈ Rm of “good realizations of ξ” such that Prob{ξ ∈ Ξ} ≥ 1 −  and the L[p r]-

can be approximated by s-block-sparse representation vector within accuracy υ (measured in the
L[1 r]-norm).

2.2 Condition Qs q(κ)
Let a sensing matrix A and a representation structure B = (B  n1  ...  nK (cid:107) · (cid:107)r) be given  and let
s ≤ K be a positive integer  q ∈ [1 ∞] and κ > 0. We say that a pair (H (cid:107)·(cid:107))  where H ∈ Rm×M
and (cid:107) · (cid:107) is a norm on RM   satisﬁes the condition Qs q(κ) associated with the matrix A and B  if

∀x ∈ Rn : Ls q(Bx) ≤ s

1

q (cid:107)H T Ax(cid:107) + κs

1

q −1L1(Bx).

(2)

The following is an evident observation.
Observation 2.1 Given A and a representation structure B  let (H (cid:107) · (cid:107)) satisfy Qs q(κ). Then
(H (cid:107) · (cid:107)) satisﬁes Qs q(cid:48)(κ(cid:48)) for all q(cid:48) ∈ (1  q) and κ(cid:48) ≥ κ. Besides this  if s(cid:48) ≤ s is a positive
integer  ((s/s(cid:48))
Whenever (B  n1  ...  nK (cid:107) · (cid:107)r) is the standard representation structure  meaning that B is the
identity matrix  n1 = 1  and (cid:107) · (cid:107)r = | · |  the condition Qs q(κ) reduces to the condition Hs q(κ)
introduced in [14].

q H (cid:107) · (cid:107)) satisﬁes Qs(cid:48) q((s(cid:48)/s)1− 1

q κ).

1

2.3

(cid:96)1-Recovery Routines

We consider two block-sparse recovery routines.

Regular (cid:96)1 recovery

is given by

(cid:98)xreg(y) ∈ Argmin

z

(cid:8)L1(Bz) : (cid:107)H T (Az − y)(cid:107) ≤ ρ(cid:9)  

where H ∈ Rm×M   (cid:107) · (cid:107) and ρ > 0 are parameters of the construction.
Theorem 2.1 Let s be a positive integer  q ∈ [1 ∞] and κ ∈ (0  1/2). Let also  ∈ (0  1). Assume
that the parameters H  (cid:107) · (cid:107)  ρ of the regular (cid:96)1-recovery are such that

A. (H (cid:107)·(cid:107)) satisﬁes the condition Qs q(κ) associated with the matrix A and the representation

structure B;

B. There exists a set Ξ ⊂ Rm such that Prob(ξ ∈ Ξ) ≥ 1 −  and

(cid:107)H T (u + Dξ)(cid:107) ≤ ρ ∀(u ∈ U  ξ ∈ Ξ).

Then for all 1 ≤ p ≤ q and υ > 0 

Riskp(B(cid:98)xreg(y)|s  D  υ  ) ≤ (4s)

1
p

Penalized (cid:96)1 recovery is(cid:98)xpen(y) ∈ Argmin

z

2ρ + s−1υ
1 − 2κ

  1 ≤ p ≤ q.

(cid:8)L1(Bz) + 2s(cid:107)H T (Az − y)(cid:107)(cid:9)  

where H ∈ Rm×M   (cid:107) · (cid:107) and a positive integer s are parameters of the construction. The accuracy
of the penalized recovery is given by the following analogue of Theorem 2.1:
Theorem 2.2 Let s be a positive integer  q ∈ [1 ∞] and κ ∈ (0  1/2). Let also  ∈ (0  1). Assume
that the parameters H  (cid:107) · (cid:107)  s of the penalized recovery and a ρ ≥ 0 satisfy conditions A  B from
Theorem 2.1. Then for all 1 ≤ p ≤ q and υ > 0 we have

Riskp(B(cid:98)xpen(y)|s  D  υ  ) ≤ 2(2s)

1
p

2ρ + s−1υ
1 − 2κ

  1 ≤ p ≤ q 

(5)

(3)

(4)

cf. (4).

4

3 Evaluating Condition Qs ∞(κ)

The condition Qs q(κ)  of Section 2.2  is closely related to known conditions  introduced to study
the properties of recovery routines in the context of block-sparsity. Let us consider the representation
structure with B = In. If the norm (cid:107) · (cid:107) in (2) is chosen to be the (cid:96)∞-norm  we have the following
obvious observation:

(!) Let H satisfy Qs q(κ) and let(cid:98)λ be the maximum of the Euclidean norms of

columns in H. Then

∀x ∈ Rn : Ls q(x) ≤(cid:98)λs

1

q (cid:107)Ax(cid:107)2 + κs

1

q −1L1(x).

Note that conditions of this kind with κ < 1/2 and (cid:107)·(cid:107)r = (cid:107)·(cid:107)2 play a crucial role in the performance
analysis of group-Lasso and Dantzig Selector. For example  the error bounds for Lasso recovery 
obtained in [18] rely upon the Restricted Eigenvalue assumption RE(s  κ) which is as follows: there
is κ > 0 such that

Hence  Ls 1(x) ≤ √

L2(xs) ≤ 1
sLs 2(x) ≤ √

κ(cid:107)Ax(cid:107)2 whenever 3L1(xs) ≥ L1(x − xs).
sκ (cid:107)Ax(cid:107)2 whenever 4Ls 1(x) ≥ L1(x)  so that

∀x ∈ Rn : Ls 1(x) ≤ s1/2

κ (cid:107)Ax(cid:107)2 +

1
4

L1(x)

(6)

(observe that (6) is nothing but the “block version” of the Compatibility condition from [4]).
The bad news is that  in general  condition Qs q(κ)  as well as RE and Compatibility conditions 
cannot be veriﬁed. Speciﬁcally  given a sensing matrix A and a representation structure B  it seems
to be difﬁcult even to verify that a pair (H (cid:107)·(cid:107)) satisﬁes condition Qs q(κ) associated with A  B  let
alone to synthesize such H which satisﬁes this condition and results in the best possible error bounds
(4)  (5) for the regular and the penalized (cid:96)1-recoveries. The good news is that when (cid:107) · (cid:107)r is the
uniform norm (cid:107)·(cid:107)∞ and  in addition  q = ∞ the condition Qs q(κ) becomes “fully computationally
tractable”.1 We intend to demonstrate also that this condition Qs ∞(κ) is in fact necessary for the
bounds of the form (4)  (5) to be valid when p = ∞.
3.1 Condition Qs ∞(κ)  case r = ∞: tractability
Consider the case of the representation structure B∞ = (B  n1  ...nK (cid:107)·(cid:107)∞). We have the following
result.
Proposition 3.1 Let (cid:107) · (cid:107)(k) = (cid:107) · (cid:107)∞ for all k ≤ K  and let a positive integer s and reals κ > 0 
 ∈ (0  1) be given.
(i) Assume that a triple (H (cid:107) · (cid:107)  ρ)  where H ∈ RM×m  (cid:107) · (cid:107) is a norm on RM   and ρ ≥ 0  is such
that

(!) (H (cid:107)·(cid:107)) satisﬁes Qs ∞(κ) and the set Ξ = {ξ : (cid:107)H T [u+Dξ](cid:107) ≤ ρ ∀u ∈ U}
is such that Prob(ξ ∈ Ξ) ≥ 1 − .

Then there exist N = n1+...+nK vectors h1  ...  hN in Rm and N×N block matrix V = [V k(cid:96)]K
(the blocks V k(cid:96) of V are nk × n(cid:96) matrices) such that

k (cid:96)=1

(cid:18)

(cid:107)V k(cid:96)(cid:107)∞ ∞ ≤ s−1κ ∀k  (cid:96) ≤ K

(a) B = V B + [h1  ...  hN ]T A 
(b)
(here (cid:107)V k(cid:96)(cid:107)∞ ∞ = max1≤j≤n(cid:96) (cid:107)Rowj(V k(cid:96))(cid:107)1  Rowj(M ) being the j-th row of M) 
(c) Prob

(cid:19)
u∈U uT hi + |(Dξ)T hi| ≤ ρ  1 ≤ i ≤ N}
(cid:98)H = [h1  ...  hN ]  the norm (cid:107) · (cid:107)∞ on RN and ρ form a triple satisfying (!).

(ii) Whenever vectors h1  ...  hN ∈ Rm and a matrix V = [V k(cid:96)]K

k (cid:96)=1 satisfy (7)  the m × N matrix

Ξ+ := {ξ : max

≥ 1 − .

(7)

1Recall that by Observation 2.1  q = ∞ corresponds to the strongest among the conditions Qs q(κ) asso-
ciated with A and a given representation structure B and ensures the validity of the bounds (4) and (5) in the
largest possible range  1 ≤ p ≤ ∞  of values of p.

5

Discussion. Let a sensing matrix A ∈ Rm×n and a representation structure B∞ be given  along
with a positive integer s  an uncertainty set U  and quantities D and . Recall that Theorems 2.1  2.2
say that if a triple (H (cid:107) · (cid:107)  ρ) is such that (H (cid:107) · (cid:107)) satisﬁes Qs ∞(κ) with κ < 1/2 and H  ρ are
such that for the set

Ξ = {ξ : (cid:107)H T [u + Dξ](cid:107) ≤ ρ ∀u ∈ U}

it holds P (Ξ) ≥ 1 −   then for all υ ≥ 0  for the regular (cid:96)1 recovery associated with (H (cid:107) · (cid:107)  ρ)
and for the penalized (cid:96)1 recovery associated with (H (cid:107) · (cid:107)  s) one has:

Riskp(B(cid:98)x|s  D  υ  ) ≤ 2(2s)

1
p

2ρ + s−1υ
1 − 2κ

  1 ≤ p ≤ ∞.

(8)

Proposition 3.1 states that when applying this result  we lose nothing by restricting ourselves with
triples H = [h1  ...  hN ] ∈ Rm×N   N = n1 + ... + nK  (cid:107) · (cid:107) = (cid:107) · (cid:107)∞ on RN   ρ ≥ 0 which can be
augmented by an appropriately chosen N × N matrix V to satisfy relations (7). In the rest of this
discussion  it is assumed that we are speaking about triples (H (cid:107) · (cid:107)  ρ) satisfying the just deﬁned
restrictions.
Now  as far as bounds (8) are concerned  they are completely determined by two parameters — κ
(which should be < 1/2) and ρ; the smaller are these parameters  the better are the bounds. In what
follows we address the issue of efﬁcient synthesis of matrices H with “as good as possible” values
of κ and ρ.
Observe  ﬁrst  that H = [h1  ...  hN ] and κ should admit an extension by a matrix V to a solution of
the system of constraints (7). Let µU (h) = max

Prob(cid:0)Ξ+ =(cid:8)ξ : µU (hi) + |(Dξ)T hi| ≤ ρ  1 ≤ i ≤ N(cid:9)(cid:1) ≥ 1 −  

u∈U uT h. Note that the restriction

(cid:16) 
(cid:17)(cid:107)DT hi(cid:107)2
Ignoring the “gap” between erﬁnv(/2) and erﬁnv(cid:0) 

where erﬁnv(·) is the inverse error function2  and it is implied by

(cid:17)(cid:107)DT hi(cid:107)2
(cid:105)
(cid:1)  we can safely model the restriction (9) by

ρ ≥(cid:104)

ρ ≥ max
1≤i≤N

  1 ≤ i ≤ N.

µU (hi) + erﬁnv

µU (hi) + erﬁnv

(cid:16) 

implies that

the system of convex constraints (10). Thus  the set Gs of admissible κ  ρ can be safely approxi-
mated by the computationally tractable convex set

(cid:105)

 

2N

2N

(cid:104)

(9)

(10)

2

(cid:26)

G∗
s =

(κ  ρ) : ∃H = [h1  ...  hN ] ∈ Rm×N   V = [V k(cid:96) ∈ Rnk×n(cid:96)]K
  1 ≤ k  (cid:96) ≤ K

B = BV + H T A  (cid:107)V k(cid:96)(cid:107)∞ ∞ ≤ κ
s

(cid:17)(cid:107)DT hi(cid:107)2 ≤ ρ  1 ≤ i ≤ N

(cid:16) 

u∈U uT hi + erﬁnv
max

2N

(cid:27)

k (cid:96)=1 s.t.

3.2 Condition Qs ∞(κ)  case r = ∞: necessity
Let the representation structure B∞ = (B  n1  ...  nK (cid:107) · (cid:107)∞) be ﬁxed. From the above discussion
we know that if  for some κ < 1/2 and ρ > 0  there exist H = [h1  ...  hN ] ∈ Rm×N and
V = [V k(cid:96) ∈ Rnk×n(cid:96)]K
k (cid:96)=1 satisfying (7)  then regular (cid:96)1-recovery with appropriate choice of
parameters ensures that

Risk∞(B(cid:98)xreg|s  D  υ  ) ≤ 2ρ + s−1υ

1 − 2κ

.

(11)

We are about to demonstrate that this implication can be “nearly inverted:”

2i.e.  u = erﬁnv(δ) means that

1√
2π

(cid:82) ∞
u e−t2/2dt = δ.

6

Proposition 3.2 Let a sensing matrix A  an uncertainty set U  and reals κ > 0   ∈ (0  1) be
given. Assume also that the observation error “is present ” speciﬁcally  that for every r > 0  the set
{u + De : u ∈ U (cid:107)e(cid:107)2 ≤ r} contains a neighborhood of the origin.

Given a positive integer S  assume that there exists a recovering routine(cid:98)x satisfying an error bound

∀(x ∈ Rn  u ∈ U) : Probξ{(cid:107)B[(cid:98)x(y) − x](cid:107)∞ ≤ α + S−1L1(Bx − [Bx]S)} ≥ 1 − .

for some α > 0. Then there exist H = [h1  ...  hN ] ∈ Rm×N and V = [V k(cid:96) ∈ Rnk×n(cid:96)]K
satisfying

of the form (11)  namely 

(12)
k (cid:96)=1 

with

(c)

and such that

Prob

(cid:18)

(a)

(b)

(cid:20)

B = V B + H T A 
(cid:107)V k(cid:96)(cid:107)∞ ∞ ≤ 2S−1 ∀k  (cid:96) ≤ K 

u∈U uT hi + erﬁnv(
max

erﬁnv(cid:0) 
erﬁnv(cid:0) 
(cid:19)
u∈U uT hi + |(Dξ)T hi| ≤ ρ  1 ≤ i ≤ N}

)(cid:107)DT hi(cid:107)2

≥ 1 − .

≤ 2α


2N

(cid:21)

2N

2

(cid:1)
(cid:1)  

ρ := max
1≤i≤N

Ξ+ := {ξ : max

The latter exactly means that the exhibited H satisﬁes the condition Qs ∞(κ) (see Proposition 3.1)
for s “nearly as large as S ” namely  s ≤ κS
2 . Further  H = [h1  ...  hk]  ρ satisfy conditions (10)
(and thus – condition B of Theorem 2.1)  with ρ being “nearly α”  namely  ρ ≤ 2α
. As a
consequence  under the premise of the proposition  we have for s ≤ S
8 (cf (11)):

erﬁnv( 
2N )
erﬁnv( 
2 )

Risk∞(B(cid:98)xreg|s  D  υ  ) ≤ 8α

erﬁnv( 
2N )
erﬁnv( 
2 )

+ 2s−1υ.

3.3 Condition Qs ∞(κ)  case r = 2: a veriﬁable sufﬁcient condition
In this section we consider the case of the representation structure B2 = (B  n1  ...  nK (cid:107) · (cid:107)2). A
veriﬁable sufﬁcient condition for Qs ∞(κ) is given by the following
Proposition 3.3 Let a sensing matrix A  a representation structure B2 be given. Let N = n1 + ... +
nK  and let N × N matrix V = [V k(cid:96)]K
k (cid:96)=1 (V k(cid:96) are nk × n(cid:96)) and m × N matrix H satisfy the
relation
(13)
B = V B + H T A.

Let

ν∗(V ) = max
1≤k (cid:96)≤K

σmax(V k(cid:96)) 

(14)

where σmax stands for the maximal singular value. Then for all s ≤ K we have:

Ls ∞(Bx) ≤ L∞(H T Ax) + ν∗(V )L1(Bx)  ∀x.

(15)
Suppose that the matrix A  the representation structure B2  the uncertainty set U  and the parameters
D   are given. Let us assume that the triple H  (cid:107) · (cid:107) = L∞(·)  and ρ can be augmented by an
appropriately chosen block N × N matrix V to satisfy the system of convex constraints
(13) 
(14). Our objective now is to synthesize the matrix H = [H k ∈ Rm×nk ]K
k=1 which satisﬁes the
relationship (3) with “as good as possible” value of ρ.
Let us compute a bound on the probability of deviations of the variable (cid:107)(H k)T Dξ(cid:107)2. Note that the
distribution of (cid:107)(H k)T Dξ(cid:107)2
i   where
η  ...  ηnk are i.i.d N (0  1) and v[k] = [σ2
[k]]  σi[k] being the principal singular values
of (H k)T D. To bound the deviation probabilities for ζk we use the bound of [6] for the deviation of
the weighted χ2:

2 coincides with that of the random variable ζk =(cid:80)nk

1[k]  ...  σ2
nk

k=1 vi[k]η2

Prob

vi[k]η2

i ≥ (cid:107)v[k](cid:107)1 +

≤ 2 exp

−

τ 2

4(cid:107)v[k](cid:107)2

2 + 4τ(cid:107)v[k](cid:107)∞

(cid:19)

.

(cid:40) nk(cid:88)

i=1

(cid:41)

(cid:18)

√

2(cid:107)v[k](cid:107)2τ

7

When substituting (cid:107)v[k](cid:107)∞ = σ2
2  where
σmax[k] is the maximal singular value and (cid:107)σ[k](cid:107)2 is the Frobenius norm of H T D  after a simple
algebra we come to

nk  and (cid:107)v[k](cid:107)1 = (cid:107)σ[k](cid:107)2

max[k]

max[k]  (cid:107)v[k](cid:107)2 ≤ σ2
(cid:113)

(cid:27)
4 ln(2K−1) + 2(cid:112)nk ln(2K−1)

Prob

(cid:107)(Dξ)T H[k](cid:107)2 ≥ (cid:107)σ[k](cid:107)2 + σmax[k]

(cid:26)

≤ 
K

.

√

(cid:20)

Let µU (Hk) = max

u∈U (cid:107)uT H[k](cid:107)2. Then the chance constraint
Prob(cid:8)ξ : µU (H k) + (cid:107)(Dξ)T H[k](cid:107)2 ≤ ρ  1 ≤ k ≤ K(cid:9) ≥ 1 −  
(cid:21)
(cid:113)
4 ln(2K−1) + 2(cid:112)nk ln(2K−1)

is satisﬁed for
ρ ≥ max
(here (cid:107)·(cid:107)F stands for the Frobenius norm). In particular  in the case U = {0} (there is no nuisance) 
the set Gs of admissible κ  ρ can be safely approximated by the computationally tractable convex
set

µU (H k) + (cid:107)DT H[k](cid:107)F + σmax(DT H[k])

k

(cid:26)

G∗
s =

(κ  ρ) : ∃H = [H k ∈ Rm×nk ]K

k=1  V = [V k(cid:96) ∈ Rnk×n(cid:96)]K
  1 ≤ k  (cid:96) ≤ K

k (cid:96)=1

B = BV + H T A  σmax(V k(cid:96)) ≤ κ
s

4 ln(2K−1) + 2(cid:112)nk ln(2K−1)  1 ≤ k ≤ K

(cid:113)

(cid:27)

.

ρ ≥ (cid:107)DT H[k](cid:107)F + σmax(DT H[k])

We have mentioned in Introduction that  to the best of our knowledge  the only previously proposed
veriﬁable sufﬁcient condition for the validity of block (cid:96)1 recovery is the Mutual Block Incoherence
condition [9]. We aim now to demonstrate that this condition is covered by Proposition 3.3.
The Mutual Block Incoherence condition deals with the case where B = I and all block norms are
(cid:107) · (cid:107)2-norms. Let the sensing matrix A in question be partitioned as A = [A[1]  ...  A[K]]  where
A[k]  k = 1  ...  K  has nk columns. Let us deﬁne the mutual block-incoherence µ of A w.r.t. the
representation structure in question as follows:

(cid:0)C−1
k AT [k]A[(cid:96)](cid:1)  

µ = max
k(cid:54)=(cid:96)

1≤k (cid:96)≤K 

σmax

[Ck := AT [k]A[k]]

(16)

provided that all matrices Ck  1 ≤ k ≤ K  are nonsingular  otherwise µ = ∞. Note that in the case
of the standard representation structure  the just deﬁned quantity is nothing but the standard mutual
incoherence known from the Compressed Sensing literature.
We have the following observation.
Proposition 3.4 Given m × n sensing matrix A and a representation structure B2 with B = I 
1 ≤ k ≤ K  let A = [A[1]  ...  A[K]] be the corresponding partition of A.
Let µ be the mutual block-incoherence of A with respect to B2. Assuming µ < ∞  we set

1

[A[1]C−1

1   A[2]C−1

2   ...  A[K]C−1

H =

1 + µ

(17)
Then the contrast matrix H along with the matrix V = I − H T A satisﬁes condition (13) (where
B = I) and condition (14) with ν∗(V ) ≤ µs
1+µ . As a result  applying Proposition 3.3  we conclude
that whenever

K ]  Ck = AT [k]A[k].

2µ
the pair (H  L∞(·)) satisﬁes Qs ∞(κ) with κ = µs
1+µ < 1/2.

1 + µ

 

s <

(18)

Note that Proposition 3.4 essentially covers the results of [9] where the authors prove  under a
condition which is marginally stronger than that of
(18)  that an appropriate version of block-(cid:96)1
recovery allows to recover exactly every block-sparse signal from the noiseless observation y = Ax.

8

References
[1] F. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res. 

9:1179–1225  2008.

[2] Z. Ben-Haim and Y. C. Eldar. Near-oracle performance of greedy block-sparse estimation
techniques from noisy measurements. Technical report  2010. http://arxiv.org/abs/
1009.0906.

[3] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

Annals of Stat.  37(4):1705–1732  2008.

[4] P. B¨uhlmann and S. van de Geer. On the conditions used to prove oracle results for the Lasso.

Electron. J. Statist.  3:1360–1392  2009.

[5] E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Inf. Theory  51:4203–

4215  2006.

[6] L. Cavalier  G. K. Golubev  D. Picard  and A. B. Tsybakov. Oracle inequalities for inverse

problems. Ann. Statist.  30(3):843–874  2002.

[7] A. Chambolle. An algorithm for total variation minimization and applications. Journal of

Mathematical Imaging and Vision  20(1-2):89–97  2004.

[8] C. Chesneau and M. Hebiri. Some theoretical results on the grouped variables Lasso. Mathe-

matical Methods of Statistics  27(4):317–326  2008.

[9] Y. C. Eldar  P. Kuppinger  and H. B¨olcskei. Block-sparse signals: Uncertainty relations and

efﬁcient recovery. IEEE Trans. on Signal Processing  58(6):3042–3054  2010.

[10] Y. C. Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces.

IEEE Trans. Inf. Theory  55(11):5302–5316  2009.

[11] J. Huang and T. Zhang. The beneﬁt of group sparsity. Annals of Stat.  38(4):1978–2004  2010.
[12] G. M. James  P. Radchenko  and J. Lv. Dasso: connections between the Dantzig selector and

Lasso. J. Roy. Statist. Soc. Ser. B  71(1):127–142  2009.

[13] A. B. Juditsky  F. Kılınc¸-Karzan  and A. S. Nemirovski. Veriﬁable conditions of (cid:96)1 recovery
for sparse signals with sign restrictions. Math. Progr.  127(1):89–122  2010. http://www.
optimization-online.org/DB_HTML/2009/03/2272.html.

[14] A. B. Juditsky and A. S. Nemirovski. Accuracy guarantees for (cid:96)1-recovery. Techni-
cal report  2010. http://www.optimization-online.org/DB_HTML/2010/10/
2778.html.

[15] A. B. Juditsky and A. S. Nemirovski. On veriﬁable sufﬁcient conditions for sparse signal
recovery via (cid:96)1 minimization. Math. Progr.  127(1):57–88  2010. Special issue on machine
learning.

[16] H. Liu and J. Zhang. Estimation consistency of the group Lasso and its applications. Journal

of Machine Learning Research - Proceedings Track  5:376–383  2009.

[17] H. Liu  J. Zhang  X. Jiang  and J. Liu. The group Dantzig selector. Journal of Machine

Learning Research - Proceedings Track  9:461–468  2010.

[18] K. Lounici  M. Pontil  A. Tsybakov  and S. van de Geer. Oracle inequalities and optimal
inference under group sparsity. Technical report  2010. http://arxiv.org/pdf/1007.
1771.

[19] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group Lasso estimator for linear

models. Electron. J. Statist.  2:605–633  2008.

[20] G. Obozinski  M. J. Wainwright  and M. I. Jordan. Support union recovery in high-dimensional

multivariate regression. Annals of Stat.  39(1):1–47  2011.

[21] L. Rudin  S. Osher  and E. Fatemi. Nonlinear total variation based noise removal algorithms.

Physica D  60(1-4):259–268  1992.

[22] M. Stojnic  F. Parvaresh  and B. Hassibi. On the reconstruction of block-sparse signals with an
optimal number of measurements. IEEE Trans. on Signal Processing  57(8):3075–3085  2009.
[23] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J.

Roy. Stat. Soc. Ser. B  68(1):49–67  2006.

9

,Samy Bengio
Oriol Vinyals
Navdeep Jaitly
Noam Shazeer