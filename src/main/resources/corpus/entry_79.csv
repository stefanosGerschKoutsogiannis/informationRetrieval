2009,A Game-Theoretic Approach to Hypergraph Clustering,Hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order (rather than pairwise) similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes  thereby obtaining the clusters as a by-product of the partitioning process. In this paper  we provide a radically different perspective to the problem. In contrast to the classical approach  we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose. Specifically  we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player ``clustering game  whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept. From the computational viewpoint  we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex  and we provide a discrete-time dynamics to perform this optimization. Experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques.,A Game-Theoretic Approach to

Hypergraph Clustering

Samuel Rota Bul`o

Marcello Pelillo

University of Venice  Italy

{srotabul pelillo}@dsi.unive.it

Abstract

Hypergraph clustering refers to the process of extracting maximally coherent
groups from a set of objects using high-order (rather than pairwise) similarities.
Traditional approaches to this problem are based on the idea of partitioning the
input data into a user-deﬁned number of classes  thereby obtaining the clusters as
a by-product of the partitioning process. In this paper  we provide a radically dif-
ferent perspective to the problem. In contrast to the classical approach  we attempt
to provide a meaningful formalization of the very notion of a cluster and we show
that game theory offers an attractive and unexplored perspective that serves well
our purpose. Speciﬁcally  we show that the hypergraph clustering problem can
be naturally cast into a non-cooperative multi-player “clustering game”  whereby
the notion of a cluster is equivalent to a classical game-theoretic equilibrium con-
cept. From the computational viewpoint  we show that the problem of ﬁnding the
equilibria of our clustering game is equivalent to locally optimizing a polynomial
function over the standard simplex  and we provide a discrete-time dynamics to
perform this optimization. Experiments are presented which show the superiority
of our approach over state-of-the-art hypergraph clustering techniques.

1 Introduction
Clustering is the problem of organizing a set of objects into groups  or clusters  in a way as to have
similar objects grouped together and dissimilar ones assigned to different groups  according to some
similarity measure. Unfortunately  there is no universally accepted formal deﬁnition of the notion
of a cluster  but it is generally agreed that  informally  a cluster should correspond to a set of objects
satisfying two conditions: an internal coherency condition  which asks that the objects belonging to
the cluster have high mutual similarities  and an external incoherency condition  which states that
the overall cluster internal coherency decreases by adding to it any external object.

Objects similarities are typically expressed as pairwise relations  but in some applications higher-
order relations are more appropriate  and approximating them in terms of pairwise interactions can
lead to substantial loss of information. Consider for instance the problem of clustering a given set of
d-dimensional Euclidean points into lines. As every pair of data points trivially deﬁnes a line  there
does not exist a meaningful pairwise measure of similarity for this problem. However  it makes
perfect sense to deﬁne similarity measures over triplets of points that indicate how close they are
to being collinear. Clearly  this example can be generalized to any problem of model-based point
pattern clustering  where the deviation of a set of points from the model provides a measure of their
dissimilarity. The problem of clustering objects using high-order similarities is usually referred to
as the hypergraph clustering problem.

In the machine learning community  there has been increasing interest around this problem. Zien
and co-authors [24] propose two approaches called “clique expansion” and “star expansion”  respec-
tively. Both approaches transform the similarity hypergraph into an edge-weighted graph  whose
edge-weights are a function of the hypergraph’s original weights. This way they are able to tackle

1

the problem with standard pairwise clustering algorithms. Bolla [6] deﬁnes a Laplacian matrix for
an unweighted hypergraph and establishes a link between the spectral properties of this matrix and
the hypergraph’s minimum cut. Rodr`ıguez [16] achieves similar results by transforming the hyper-
graph into a graph according to “clique expansion” and shows a relationship between the spectral
properties of a Laplacian of the resulting matrix and the cost of minimum partitions of the hy-
pergraph. Zhou and co-authors [23] generalize their earlier work on regularization on graphs and
deﬁne a hypergraph normalized cut criterion for a k-partition of the vertices  which can be achieved
by ﬁnding the second smallest eigenvector of a normalized Laplacian. This approach generalizes
the well-known “Normalized cut” pairwise clustering algorithm [19]. Finally  in [2] we ﬁnd another
work based on the idea of applying a spectral graph partitioning algorithm on an edge-weighted
graph  which approximates the original (edge-weighted) hypergraph. It is worth noting that the ap-
proaches mentioned above are devised for dealing with higher-order relations  but can all be reduced
to standard pairwise clustering approaches [1]. A different formulation is introduced in [18]  where
the clustering problem with higher-order (super-symmetric) similarities is cast into a nonnegative
factorization of the closest hyper-stochastic version of the input afﬁnity tensor.

All the afore-mentioned approaches to hypergraph clustering are partition-based. Indeed  clusters
are not modeled and sought directly  but they are obtained as a by-product of the partition of the input
data into a ﬁxed number of classes. This renders these approaches vulnerable to applications where
the number of classes is not known in advance  or where data is affected by clutter elements which
do not belong to any cluster (as in ﬁgure/ground separation problems). Additionally  by partitioning 
clusters are necessarily disjoint sets  although it is in many cases natural to have overlapping clusters 
e.g.  two intersecting lines have the point in the intersection belonging to both lines.

In this paper  following [14  20] we offer a radically different perspective to the hypergraph cluster-
ing problem. Instead of insisting on the idea of determining a partition of the input data  and hence
obtaining the clusters as a by-product of the partitioning process  we reverse the terms of the prob-
lem and attempt instead to derive a rigorous formulation of the very notion of a cluster. This allows
one  in principle  to deal with more general problems where clusters may overlap and/or outliers
may get unassigned. We found that game theory offers a very elegant and general mathematical
framework that serves well our purposes. The basic idea behind our approach is that the hypergraph
clustering problem can be considered as a multi-player non-cooperative “clustering game”. Within
this context  the notion of a cluster turns out to be equivalent to a classical equilibrium concept from
(evolutionary) game theory  as the latter reﬂects both the internal and external cluster conditions
alluded to before. We also show that there exists a correspondence between these equilibria and
the local solutions of a polynomial  linearly-constrained  optimization problem  and provide an al-
gorithm for ﬁnding them. Experiments on two standard hypergraph clustering problems show the
superiority of the proposed approach over state-of-the-art hypergraph clustering techniques.

2 Basic notions from evolutionary game theory

Evolutionary game theory studies models of strategic interactions (called games) among large
numbers of anonymous agents. A game can be formalized as a triplet Γ = (P  S  π)  where
P = {1  . . .   k} is the set of players involved in the game  S = {1  . . .   n} is the set of pure
strategies (in the terminology of game-theory) available to each player and π : Sk → R is the payoff
function  which assigns a payoff to each strategy proﬁle  i.e.  the (ordered) set of pure strategies
played by the individuals. The payoff function π is assumed to be invariant to permutations of the
strategy proﬁle. It is worth noting that in general games  each player may have its own set of strate-
gies and own payoff function. For a comprehensive introduction to evolutionary game theory we
refer to [22].

By undertaking an evolutionary setting we assume to have a large population of non-rational agents 
which are randomly matched to play a game Γ = (P  S  π). Agents are considered non-rational  be-
cause each of them initially chooses a strategy from S  which will be always played when selected
for the game. An agent  who selected strategy i ∈ S  is called i-strategist. Evolution in the popula-
tion takes place  because we assume that there exists a selection mechanism  which  by analogy with
a Darwinian process  spreads the ﬁttest strategies in the population to the detriment of the weakest
one  which will in turn be driven to extinction. We will see later in this work a formalization of such
a selection mechanism.

2

The state of the population at a given time t can be represented as a n-dimensional vector x(t) 
where xi(t) represents the fraction of i-strategists in the population at time t. The set of all possible
states describing a population is given by

∆ =(x ∈ Rn : Xi∈S

xi = 1 and xi ≥ 0 for all i ∈ S)  

which is called standard simplex. In the sequel we will drop the time reference from the population
state  where not necessary. Moreover  we denote with σ(x) the support of x ∈ ∆  i.e.  the set of
strategies still alive in population x ∈ ∆: σ(x) = {i ∈ S : xi > 0}.

(i) ∈ ∆ is the probability distribution identifying which strategy the ith player will adopt if

If y
drawn to play the game Γ  then the average payoff obtained by the agents can be computed as

k

u(y

(1)  . . .   y

(k)) = X(s1 ... sk)∈Sk

Yj=1

π(s1  . . .   sk)

y(j)
sj .

(1)

Note that (1) is invariant to any permutation of the input probability vectors.

i  x

k)  where x

k−1)  where e

i ∈ ∆ is a vector with xi = 1 and zero elsewhere.

Assuming that the agents are randomly and independently drawn from a population x ∈ ∆ to play
the game Γ  the population average payoff is given by u(x
k is a shortcut for x  . . .   x
repeated k times. Furthermore  the average payoff that an i-strategist obtains in a population x ∈ ∆
is given by u(e
An important notion in game theory is that of equilibrium [22]. A population x ∈ ∆ is in equilibrium
when the distribution of strategies will not change anymore  which intuitively happens when every
individual in the population obtains the same average payoff and no strategy can thus prevail on the
other ones. Formally  x ∈ ∆ is a Nash equilibrium if
k)  

(2)
In other words  every agent in the population performs at most as well as the population average
payoff. Due to the multi-linearity of u  a consequence of (2) is that

for all i ∈ S .

k−1) ≤ u(x

i  x

u(e

(3)
i.e.  all the agents that survived the evolution obtain the same average payoff  which coincides with
the population average payoff.

for all i ∈ σ(x)  

k−1) = u(x

i  x

k)  

u(e

A key concept pertaining to evolutionary game theory is that of an evolutionary stable strategy
[7  22]. Such a strategy is robust to evolutionary pressure in an exact sense. Assume that in a
population x ∈ ∆  a small share ǫ of mutant agents appears  whose distribution of strategies is
y ∈ ∆. The resulting postentry population is given by wǫ = (1 − ǫ)x + ǫy. Biological intuition
suggests that evolutionary forces select against mutant individuals if and only if the average payoff
of a mutant agent in the postentry population is lower than that of an individual from the original
population  i.e. 

k−1
ǫ

u(y  w

) < u(x  w

(4)
A population x ∈ ∆ is evolutionary stable (or an ESS) if inequality (4) holds for any distribution of
mutant agents y ∈ ∆ \ {x}  granted the population share of mutants ǫ is sufﬁciently small (see  [22]
for pairwise contests and [7] for n-wise contests).
An alternative  but equivalent  characterization of ESSs involves a leveled notion of evolutionary
stable strategies [7]. We say that x ∈ ∆ is an ESS of level j against y ∈ ∆  if there exists j ∈
{0  . . .   k − 1} such that both conditions

) .

k−1
ǫ

u(y

u(y

j  x
i  x

j+1  x
i+1  x

k−j−1) < u(y
k−i−1) = u(y

(5)
(6)
are satisﬁed. Clearly  x ∈ ∆ is an ESS if it satisﬁes a condition of this form for every y ∈ ∆ \ {x}.
It is straightforward to see that any ESS is a Nash equilibrium [22  7]. An ESS  which satisﬁes
conditions (6) with j never more than J  will be called an ESS of level J. Note that for the generic
case most of the preceding conditions will be superﬂuous  i.e.  only ESSs of level 0 or 1 are required
[7]. Hence  in the sequel  we will consider only ESSs of level 1. It is not difﬁcult to verify that any
ESS (of level 1) x ∈ ∆ satisﬁes

for all 0 ≤ i < j  

k−j )  
k−i)  

u(w

k
ǫ ) < u(x

k)  

(7)

for all y ∈ ∆ \ {x} and small enough values of ǫ.

3

3 The hypergraph clustering game
The hypergraph clustering problem can be described by an edge-weighted hypergraph. Formally 
an edge-weighted hypergraph is a triplet H = (V  E  s)  where V = {1  . . .   n} is a ﬁnite set
of vertices  E ⊆ P(V ) \ {∅} is the set of (hyper-)edges (here  P(V ) is the power set of V ) and
s : E → R is a weight function which associates a real value with each edge. Note that negative
weights are allowed too. Although hypergraphs may have edges of varying cardinality  we will focus
on a particular class of hypergraphs  called k-graphs  whose edges have all ﬁxed cardinality k ≥ 2.
In this paper  we cast the hypergraph clustering problem into a game  called (hypergraph) clustering
game  which will be played in an evolutionary setting. Clusters are then derived from the analy-
sis of the ESSs of the clustering game. Speciﬁcally  given a k-graph H = (V  E  s) modeling a
hypergraph clustering problem  where V = {1  . . .   n} is the set of objects to cluster and s is the
similarity function over the set of objects in E  we can build a game involving k players  each of
them having the same set of (pure) strategies  namely the set of objects to cluster V . Under this
setting  a population x ∈ ∆ of agents playing a clustering game represents in fact a cluster  where
xi is the probability for object i to be part of it. Indeed  any cluster can be modeled as a probability
distribution over the set of objects to cluster. The payoff function of the clustering game is deﬁned
in a way as to favour the evolution of agents supporting highly coherent objects. Intuitively  this
is accomplished by rewarding the k players in proportion to the similarity that the k played objects
have. Hence  assuming (v1  . . .   vk) ∈ V k to be the tuple of objects selected by k players  the payoff
function can be simply deﬁned as

π(v1  . . .   vk) =(cid:26) 1

if {v1  . . .   vk} ∈ E  
else  
where the term 1/k! has been introduced for technical reasons.
Given a population x ∈ ∆ playing the clustering game  we have that the average population payoff
k) measures the cluster’s internal coherency as the average similarity of the objects forming the
u(x
cluster  whereas the average payoff u(e
k−1) of an agent supporting object i ∈ V in population
x  measures the average similarity of object i with respect to the cluster.
An ESS of a clustering game incorporates the properties of internal coherency and external inco-
herency of a cluster:

(8)

k! s ({v1  . . .   vk})
0

i  x

internal coherency: since ESSs are Nash equilibria  from (3)  it follows that every object contribut-
ing to the cluster  i.e.  every object in σ(x)  has the same average similarity with respect to
the cluster  which in turn corresponds to the cluster’s overall average similarity. Hence  the
cluster is internally coherent;

external incoherency: from (2)  every object external to the cluster  i.e.  every object in V \ σ(x) 
has an average similarity which does not exceed the cluster’s overall average similarity.
There may still be cases where the average similarity of an external object is the same as
that of an internal object  mining the cluster’s external incoherency. However  since x is
an ESS  from (7) we see that whenever we try to extend a cluster with small shares of
external objects  the cluster’s overall average similarity drops. This guarantees the external
incoherency property of a cluster to be also satisﬁed.

Finally  it is worth noting that this theory generalizes the dominant-sets clustering framework which
has recently been introduced in [14]. Indeed  ESSs of pairwise clustering games  i.e. clustering
games deﬁned on graphs  correspond to the dominant-set clusters [20  17]. This is an additional
evidence that ESSs are meaningful notions of cluster.

4 Evolution towards a cluster
In this section we will show that the ESSs of a clustering game are in one-to-one correspondence
with (strict) local solution of a non-linear optimization program. In order to ﬁnd ESSs  we will also
provide a dynamics due to Baum and Eagon  which generalizes the replicator dynamics [22].

Let H = (V  E  s) be a hypergraph clustering problem and Γ = (P  V  π) be the corresponding
clustering game. Consider the following non-linear optimization problem:

maximize

f (x) = Xe∈E

s(e)Yi∈e

4

xi  

subject to

x ∈ ∆ .

(9)

It is simple to see that any ﬁrst-order Karush-Kuhn-Tucker (KKT) point x ∈ ∆ of program (9) [13]
is a Nash equilibrium of Γ. Indeed  by the KKT conditions there exist µi ≥ 0  i ∈ S  and λ ∈ R
such that for all i ∈ S 

∇f (x)i + µi − λ =

1
k

u(e

i  x

k−1) + µi − λ = 0

and

µixi = 0  

i  x

where ∇ is the gradient operator. From this it follows straightforwardly that u(e
k)
for all i ∈ S. Moreover  it turns out that any strict local maximizer x ∈ ∆ of (9) is an ESS of Γ.
Indeed  by deﬁnition  a strict local maximizer of this program satisﬁes u(z
k) = f (z) < f (x) =
k)  for any z ∈ ∆ \ {x} close enough to x  which is in turn equivalent to (7) for sufﬁciently
u(x
small values of ǫ.
The problem of extracting ESSs of our hypergraph clustering game can thus be cast into the problem
of ﬁnding strict local solutions of (9). We will address this optimization task using a result due to
Baum and Eagon [3]  who introduced a class of nonlinear transformations in probability domain.
Theorem 1 (Baum-Eagon). Let P (x) be a homogeneous polynomial in the variables xi with non-
negative coefﬁcients  and let x ∈ ∆. Deﬁne the mapping z = M(x) as follows:

k−1) ≤ u(x

zi = xi∂iP (x).

n

Xj=1

xj∂j P (x) 

i = 1  . . .   n.

(10)

Then P (M(x)) > P (x)  unless M(x) = x. In other words M is a growth transformation for the
polynomial P .

The Baum-Eagon inequality provides an effective iterative means for maximizing polynomial func-
tions in probability domains  and in fact it has served as the basis for various statistical estimation
techniques developed within the theory of probabilistic functions of Markov chains [4]. It was also
employed for the solution of relaxation labelling processes [15].
Since f (x) is a homogeneous polynomial in the variables xi  we can use the transformation of
Theorem 1 in order to ﬁnd a local solution x ∈ ∆ of (9)  which in turn provides us with an ESS of the
hypergraph clustering game. By taking the support of x  we have a cluster under our framework. The
complexity of ﬁnding a cluster is thus O(ρ|E|)  where |E| is the number of edges of the hypergraph
describing the clustering problem and ρ is the average number of iteration needed to converge. Note
that ρ never exceeded 100 in our experiments.
In order to obtain the clustering  in principle  we have to ﬁnd the ESSs of the clustering game.
This is a non-trivial  although still feasible  task [21]  which we leave as a future extension of this
work. By now  we adopt a naive peeling-off strategy for our cluster extraction procedure. Namely 
we iteratively ﬁnd a cluster and remove it from the set of objects  and we repeat this process on
the remaining objects until a desired number of clusters have been extracted. The set of extracted
ESSs with this procedure does not technically correspond to the ESSs of the original game  but to
ESSs of sub-games of it. The cost of this approximation is that we unfortunately loose (by now) the
possibility of having overlapping clusters.

5 Experiments

In this section we present two types of experiments. The ﬁrst one addresses the problem of line
clustering  while the second one addresses the problem of illuminant-invariant face clustering. We
tested our approach against Clique Averaging algorithm (CAVERAGE)  since it was the best per-
forming approach in [2] on the same type of experiments. Speciﬁcally  CAVERAGE outperformed
Clique Expansion [10] combined with Normalized cuts  Gibson’s Algorithm under sum and product
model [9]  kHMeTiS [11] and Cascading RANSAC [2]. We also compare against Super-symmetric
Non-negative Tensor Factorization (SNTF) [18]  because it is the only approach  other than ours 
which does not approximate the hypergraph to a graph.

Since both CAVERAGE and SNTF  as opposed to our method  require the number of classes K to be
speciﬁed  we run them with values of K ∈ {K ∗ − 1  K ∗  K ∗ + 1} among which the optimal one
(K ∗) is present. This allows us to verify the robustness of the approaches under wrong values of K 
which may occur in general as the optimal number of clusters is not known in advance.

5

1.5

1

0.5

0

−0.5

−1

−1.5

−1.5

−1

−0.5

0

0.5

1

1.5

2

(a) Example of three lines with σ = 0.04.

HoCluGame

Cav. K=3

Cav. K=4

Cav. K=5

Sntf K=3

Sntf K=4

Sntf K=5

e
r
u
s
a
e
m
-
F

1.2

1

0.8

0.6

0.4

0.2

0

HoCluGame

Cav. K=2

Cav. K=3

Cav. K=4

Sntf K=2

Sntf K=3

Sntf K=4

0

0.01 0.02

0.04

0.08

σ

(b) Three lines.

HoCluGame

Cav. K=4

Cav. K=5

Cav. K=6

Sntf K=4

Sntf K=5

Snft K=6

e
r
u
s
a
e
m
-
F

e
r
u
s
a
e
m
-
F

1.2

1

0.8

0.6

0.4

0.2

0

1.2

1

0.8

0.6

0.4

0.2

0

0

0.01 0.02

0.04

0.08

σ

(c) Four lines.

0

0.01 0.02

0.04

0.08

σ

(d) Five lines.

Figure 1: Results on clustering 3  4 and 5 lines perturbed with increasing levels of Gaussian noise
(σ = 0  0.01  0.02  0.04  0.08).

We executed the experiments on a AMD Sempron 3Ghz computer with 1Gb RAM. Moreover  we
evaluated the quality of a clustering by computing the average F-measure of each cluster in the
ground-truth with the most compatible one in the obtained solution (according to a one-to-one cor-
respondence).

5.1 Line clustering
We consider the problem of clustering lines in spaces of dimension greater than two  i.e.  given a
set of points in Rd  the task is to ﬁnd sets of collinear points. Pairwise measures of similarity are
useless and at least three points are needed. The dissimilarity measure on triplets of points is given
by their mean distance to the best ﬁtting line. If d(i  j  k) is the dissimilarity of points {i  j  k}  the
similarity function is given by s({i  j  k}) = exp(−d(i  j  k)2/σ2)  where σ is a scaling parameter 
which has been optimally selected for all the approaches according to a small test set.

We conducted two experiments  in order to assess the robustness of the approaches to both local
and global noise. Local noise refers to a Gaussian perturbation applied to the points of a line  while
global noise consists of random outlier points.

A ﬁrst experiment consists in clustering 3  4 and 5 lines generated in the 5-dimensional space
[−2  2]5. Each line consists of 20 points  which have been perturbed according to 5 increasing
levels of Gaussian noise  namely σ = 0  0.01  0.02  0.04  0.08. With this setting there are no outliers
and every point should be assigned to a line (e.g.  see Figure 1(a)). Figure 1(b) shows the results
obtained with three lines. We reported  for each noise level  the mean and the standard deviation
of the average F-measures obtained by the algorithms on 30 randomly generated instances. Note
that  if the optimal K is used  CAVERAGE and SNTF perform well and the inﬂuence of local noise
is minimal. This behavior intuitively makes sense under moderate perturbations  because if the ap-
proaches correctly partitioned the data without noise  it is unlikely that the result will change by
slightly perturbing them. Our approach however achieves good performances as well  although we
can notice that with the highest noise level  the performance slightly drops. This is due to the fact
that points deviating too much from the overall cluster average collinearity will be excluded as they
undermine the cluster’s internal coherency. Hence  some perturbed points will be considered out-
liers. Nevertheless  it is worth noting that by underestimating the optimal number of classes both
CAVERAGE and SNTF exhibit a drastic performance drop  whereas the inﬂuence of overestimations

6

1.5

1

0.5

0

−0.5

−1

−1.5

−2

 

 

First line
Second line
Outliers

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

(a) Example of two lines with 40 outliers.

e
r
u
s
a
e
m
-
F

1.2

1

0.8

0.6

0.4

0.2

0

0

HoCluGame

Cav. K=2

Cav. K=3

Cav. K=4

Sntf K=2

Sntf K=3

Sntf K=4

10

20

σ

40

(c) Three lines.

e
r
u
s
a
e
m
-
F

e
r
u
s
a
e
m
-
F

1.2

1

0.8

0.6

0.4

0.2

0

0

1.2

1

0.8

0.6

0.4

0.2

0

0

HoCluGame

Cav. K=2

Cav. K=3

Cav. K=4

Sntf K=2

Sntf K=3

Sntf K=4

10

20

σ

40

(b) Two lines.

HoCluGame

Cav. K=3

Cav. K=4

Cav. K=5

Sntf K=3

Sntf K=4

Sntf K=5

10

20

σ

40

(d) Four lines.

Figure 2: Results on clustering 2  3 and 4 lines with an increasing number of outliers (0  10  20  40).

has a lower impact on the two partition-based algorithms. By increasing the number of lines involved
in the experiment from three to four (Figure 1(c)) and to ﬁve (Figure 1(d)) the scenario remains al-
most the same for our approach and SNTF  while we can notice a slight decrease of CAVERAGE’s
performance.

The second experiment consists in clustering 2  3 and 4 slightly perturbed lines (with ﬁxed local
noise σ = 0.01) generated in the 5-dimensional space [−2  2]5. Again  each line consists of 20
points. This time however we added also global noise  i.e.  0  10  20 and 40 random points as outliers
(e.g.  see Figure 2(a)). Figure 2(b) shows the results obtained with two lines. Here  the supremacy
of our approach over partition-based ones is clear. Indeed  our method is not inﬂuenced by outliers
and therefore it performs almost perfectly  whereas CAVERAGE and SNTF perform well only without
outliers and with the optimal K. It is interesting to notice that  as outliers are introduced  CAVERAGE
and SNTF perform better with K > 2. Indeed  the only way to get rid of outliers is to group them in
additional clusters. However  since outliers are not mutually similar and intuitively they do not form
a cluster  we have that the performance of CAVERAGE and SNTF drop drastically as the number of
outliers increases. Finally  by increasing the number of lines from two to three (Figure 2(c)) and
to four (Figure 2(d))  the performance of CAVERAGE and SNTF get worse  while our approach still
achieves good results.

4/(s2

1 + · · · + s2

5.2 Illuminant-invariant face clustering
In [5] it has been shown that images of a Lambertian object illuminated by a point light source lie in
a three dimensional subspace. According to this result  if we assume that four images of a face form
the columns of a matrix then d = s2
4) provides us with a measure of dissimilarity 
where si is the ith singular value of this matrix [2]. We use this dissimilarity measure for the face
clustering problem and we consider as dataset the Yale Face Database B and its extended version
[8  12]. In total we have faces of 38 individuals  each under 64 different illumination conditions. We
compared our approach against CAVERAGE and SNTF on subsets of this face dataset. Speciﬁcally 
we considered cases where we have faces from 4 and 5 random individuals (10 faces per individual) 
and with and without outliers. The case with outliers consists in 10 additional faces each from a
different individual. For each of those combinations  we created 10 random subsets. Similarly to the
case of line clustering  we run CAVERAGE and SNTF with values of K ∈ {K ∗ − 1  K ∗  K ∗ + 1} 
where K ∗ is the optimal one.

7

n. of classes:
n. of outliers:
CAVERAGE K=3
CAVERAGE K=4
CAVERAGE K=5
CAVERAGE K=6
SNTF K=3
SNTF K=4
SNTF K=5
SNTF K=6
HoCluGame

0

0.63±0.11
0.96±0.06
0.91±0.06

-

0.62±0.12
0.87±0.07
0.82±0.09

-

0.95±0.03

4

10

0.59±0.07
0.84±0.07
0.79±0.05

-

0.58±0.10
0.81±0.08
0.76±0.09

-

0.94±0.02

5

10
-

-

0
-

-

0.56±0.14
0.85±0.12
0.84±0.09

0.58±0.07
0.83±0.06
0.82±0.06

0.61±0.13
0.86±0.12
0.85±0.08
0.95±0.05

0.59±0.09
0.80±0.07
0.79±0.11
0.94±0.02

Table 1: Experiments on illuminant-invariant face clustering.

In Table 1 we report the average F-measures (mean and standard deviation) obtained by the three
approaches. The results are consistent with those obtained in the case of line clustering with the
exception of SNTF  which performs worse than the other approaches on this real-world application.
CAVERAGE and our algorithm perform comparably well when clustering 4 individuals without out-
liers. However  our approach turns out to be more robust in every other tested case  i.e.  when the
number of classes increases and when outliers are introduced. Indeed  CAVERAGE’s performance
decreases  while our approach yields the same good results.

In both the experiments of line and face clustering the execution times of our approach were higher
than those of CAVERAGE  but considerably lower than SNTF. The main reason why CAVERAGE
run faster is that our approach and SNTF work directly on the hypergraph without resorting to pair-
wise relations  which is indeed what CAVERAGE does. Further  we mention that our code was not
optimized to improve speed and all the approaches were run without any sampling policy.

6 Discussion
In this paper  we offered a game-theoretic perspective to the hypergraph clustering problem. Within
our framework the clustering problem is viewed as a multi-player non-cooperative game  and clas-
sical equilibrium notions from evolutionary game theory turn out to provide a natural formalization
of the notion of a cluster. We showed that the problem of ﬁnding these equilibria (clusters) is equiv-
alent to solving a polynomial optimization problem with linear constraints  which we solve using an
algorithm based on the Baum-Eagon inequality. An advantage of our approach over traditional tech-
niques is the independence from the number of clusters  which is indeed an intrinsic characteristic
of the input data  and the robustness against outliers  which is especially useful when solving ﬁgure-
ground-like grouping problems. We also mention  as a potential positive feature of the proposed
approach  the possibility of ﬁnding overlapping clusters (e.g.  along the lines presented in [21])  al-
though in this paper we have not explicitly dealt with this problem. The experimental results show
the superiority of our approach with respect to the state of the art in terms of quality of solution. We
are currently studying alternatives to the plain Baum-Eagon dynamics in order to improve efﬁciency.
Acknowledgments. We acknowledge ﬁnancial support from the FET programme within EU FP7 
under the SIMBAD project (contract 213250). We also thank Sameer Agarwal and Ron Zass for
providing us with the code of their algorithms.

References

[1] S. Agarwal  K. Branson  and S. Belongie. Higher order learning with graphs. In Int. Conf. on

Mach. Learning  volume 148  pages 17–24  2006.

[2] S. Agarwal  J. Lim  L. Zelnik-Manor  P. Perona  D. Kriegman  and S. Belongie. Beyond
pairwise clustering. In IEEE Conf. Computer Vision and Patt. Recogn.  volume 2  pages 838–
845  2005.

[3] L. E. Baum and J. A. Eagon. An inequality with applications to statistical estimation for
probabilistic functions of Markov processes and to a model for ecology. Bull. Amer. Math.
Soc.  73:360–363  1967.

8

[4] L. E. Baum  T. Petrie  G. Soules  and N. Weiss. A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains. Ann. Math. Statistics  41:164–
171  1970.

[5] P. Belhumeur and D. Kriegman. What is the set of images of an object under all possible

lighting conditions. Int. J. Comput. Vision  28(3):245–260  1998.

[6] M. Bolla. Spectral  euclidean representations and clusterings of hypergraphs. Discr. Math. 

117:19–39  1993.

[7] M. Broom.  C. Cannings  and G. T. Vickers. Multi-player matrix games. Bull. Math. Biology 

59(5):931–952  1997.

[8] A. S. Georghiades.  P. N. Belhumeur  and D. J. Kriegman. From few to many: illumination
cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal.
Machine Intell.  23(6):643–660  2001.

[9] D. Gibson  J. M. Kleinberg  and P. Raghavan. VLDB  chapter Clustering categoral data: An
approach based on dynamical systems.  pages 311–322. Morgan Kaufmann Publishers Inc. 
1998.

[10] T. Hu and K. Moerder. Multiterminal ﬂows in hypergraphs. In T. Hu and E. S. Kuh  editors 

VLSI circuit layout: theory and design  pages 87–93. 1985.

[11] G. Karypis and V. Kumar. Multilevel k-way hypergraph partitioning. VLSI Design  11(3):285–

300  2000.

[12] K. C. Lee  J. Ho  and D. Kriegman. Acquiring linear subspaces for face recognition under

variable lighting. IEEE Trans. Pattern Anal. Machine Intell.  27(5):684–698  2005.

[13] D. G. Luenberger. Linear and nonlinear programming. Addison Wesley  1984.
[14] M. Pavan and M. Pelillo. Dominant sets and pairwise clustering. IEEE Trans. Pattern Anal.

Machine Intell.  29(1):167–172  2007.

[15] M. Pelillo. The dynamics of nonlinear relaxation labeling processes. J. Math. Imag. and Vision 

7(4):309–323  1997.

[16] J. Rodr`ıguez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilin-

ear Algebra  51:285–297  2003.

[17] S. Rota Bul`o. A game-theoretic framework for similarity-based data clustering. PhD thesis 

University of Venice  2009.

[18] A. Shashua  R. Zass  and T. Hazan. Multi-way clustering using super-symmetric non-negative

tensor factorization. In Europ. Conf. on Comp. Vision  volume 3954  pages 595–608  2006.

[19] J. Shi and J. Malik. Normalized cuts and image segmentation.

Machine Intell.  22:888–905  2000.

IEEE Trans. Pattern Anal.

[20] A. Torsello  S. Rota Bul`o  and M. Pelillo. Grouping with asymmetric afﬁnities: a game-
In IEEE Conf. Computer Vision and Patt. Recogn.  pages 292–299 

theoretic perspective.
2006.

[21] A. Torsello  S. Rota Bul`o  and M. Pelillo. Beyond partitions: allowing overlapping groups in

pairwise clustering. In Int. Conf. Patt. Recogn.  2008.

[22] J. W. Weibull. Evolutionary game theory. Cambridge University Press  1995.
[23] D. Zhou  J. Huang  and B. Sch¨olkopf. Learning with hypergraphs: clustering  classiﬁcation 

embedding. In Adv. in Neur. Inf. Processing Systems  volume 19  pages 1601–1608  2006.

[24] J. Y. Zien  M. D. F. Schlag  and P. K. Chan. Multilevel spectral hypergraph partitioning
with arbitrary vertex sizes. IEEE Trans. on Comp.-Aided Design of Integr. Circ. and Systems 
18:1389–1399  1999.

9

,Giulia Fanti
Pramod Viswanath