2019,Learning Disentangled Representations for Recommendation,User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users’ decision making processes. The factors are highly entangled  and may range from high-level ones that govern user intentions  to low-level ones that characterize a user’s preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness  interpretability  and controllability. However  learning such disentangled representations from user behavior is challenging  and remains largely neglected by the existing literature. In this paper  we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g.  to buy a shirt or a cellphone)  while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer  stemming from an information-theoretic interpretation of VAEs  then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g.  the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable  which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.,Learning Disentangled Representations for

Recommendation

Jianxin Ma1 2∗  Chang Zhou1∗  Peng Cui2  Hongxia Yang1  Wenwu Zhu2

1Alibaba Group  2Tsinghua University

majx13fromthu@gmail.com  ericzhou.zc@alibaba-inc.com 

cuip@tsinghua.edu.cn  yang.yhx@alibaba-inc.com  wwzhu@tsinghua.edu.cn

Abstract

User behavior data in recommender systems are driven by the complex interactions
of many latent factors behind the users’ decision making processes. The factors are
highly entangled  and may range from high-level ones that govern user intentions 
to low-level ones that characterize a user’s preference when executing an intention.
Learning representations that uncover and disentangle these latent factors can bring
enhanced robustness  interpretability  and controllability. However  learning such
disentangled representations from user behavior is challenging  and remains largely
neglected by the existing literature. In this paper  we present the MACRo-mIcro
Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled
representations from user behavior. Our approach achieves macro disentanglement
by inferring the high-level concepts associated with user intentions (e.g.  to buy
a shirt or a cellphone)  while capturing the preference of a user regarding the
different concepts separately. A micro-disentanglement regularizer  stemming
from an information-theoretic interpretation of VAEs  then forces each dimension
of the representations to independently reﬂect an isolated low-level factor (e.g. 
the size or the color of a shirt). Empirical results show that our approach can
achieve substantial improvement over the state-of-the-art baselines. We further
demonstrate that the learned representations are interpretable and controllable 
which can potentially lead to a new paradigm for recommendation where users are
given ﬁne-grained control over targeted aspects of the recommendation lists.

1

Introduction

Learning representations that reﬂect users’ preference  based chieﬂy on user behavior  has been a
central theme of research on recommender systems. Despite their notable success  the existing user
behavior-based representation learning methods  such as the recent deep approaches [49  32  31  52 
11  18]  generally neglect the complex interaction among the latent factors behind the users’ decision
making processes. In particular  the latent factors can be highly entangled  and range from macro
ones that govern the intention of a user during a session  to micro ones that describe at a granular level
a user’s preference when implementing a speciﬁc intention. The existing methods fail to disentangle
the latent factors  and the learned representations are consequently prone to mistakenly preserve the
confounding of the factors  leading to non-robustness and low interpretability.
Disentangled representation learning  which aims to learn factorized representations that uncover
and disentangle the latent explanatory factors hidden in the observed data [3]  has recently gained
much attention. Not only can disentangled representations be more robust  i.e.  less sensitive to the
misleading correlations presented in the limited training data  the enhanced interpretability also ﬁnds

∗Equal contribution. Work done at Alibaba.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Our framework. Macro disentanglement is achieved by learning a set of prototypes  based
on which the user intention related with each item is inferred  and then capturing the preference of a
user about the different intentions separately. Micro disentanglement is achieved by magnifying the
KL divergence  from which a term that penalizes total correlation can be separated  with a factor of β.

direct application in recommendation-related tasks  such as transparent advertising [33]  customer-
relationship management  and explainable recommendation [51  17]. Moreover  the controllability
exhibited by many disentangled representations [19  14  10  8  9  25] can potentially bring a new
paradigm for recommendation  by giving users explicit control over the recommendation results and
providing a more interactive experience. However  the existing efforts on disentangled representation
learning are mainly from the ﬁeld of computer vision [28  15  20  30  53  14  10  39  19].
Learning disentangled representations based on user behavior data  a kind of discrete relational
data that is fundamentally different from the well-researched image data  is challenging and largely
unexplored. Speciﬁcally  it poses two challenges. First  the co-existence of macro and micro factors
requires us to to separate the two levels when performing disentanglement  in a way that preserves
the hierarchical relationships between an intention and the preference about the intention. Second 
the observed user behavior data  e.g.  user-item interactions  are discrete and sparse in nature  while
the learned representations are continuous. This implies that the majority of the points in the high-
dimensional representation space will not be associated with any behavior  which is especially
problematic when one attempts to investigate the interpretability of an isolated dimension by varying
the value of the dimension while keeping the other dimensions ﬁxed.
In this paper  we propose the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE)
for learning disentangled representations based on user behavior. Our approach explicitly models
the separation of macro and micro factors  and performs disentanglement at each level. Macro
disentanglement is achieved by identifying the high-level concepts associated with user intentions 
and separately learning the preference of a user regarding the different concepts. A regularizer
for micro disentanglement  derived by interpreting VAEs [27  44] from an information-theoretic
perspective  is then strengthened so as to force each individual dimension to reﬂect an independent
micro factor. A beam-search strategy  which handles the conﬂict between sparse discrete observations
and dense continuous representations by ﬁnding a smooth trajectory  is then proposed for investi-
gating the interpretability of each isolated dimension. Empirical results show that our approach can
achieve substantial improvement over the state-of-the-art baselines. And the learned disentangled
representations are demonstrated to be interpretable and controllable.

2 Method

In this section  we present our approach for learning disentangled representations from user behaivor.

2.1 Notations and Problem Formulation
A user behavior dataset D consists of the interactions between N users and M items. The interaction
between the uth user and the ith item is denoted by xu i ∈ {0  1}  where xu i = 1 indicates that user
u explicitly adopts item i  whereas xu i = 0 means there is no recorded interaction between the two.
For convenience  we use xu = {xu i : xu i = 1} to represent the items adopted by user u. The goal
is to learn user representations {zu}N
u=1 that achieves both macro and micro disentanglement. We
use θ to denote the set that contains all the trainable parameters of our model.

2

Encoder621477325172643535461µ(3)µ(2)(1)(2)(3)z(1)µ(1)z(3)N(µ(1) (1))N(µ(3) (3))z(2)N(µ(2) (2))N(0 20I)N(0 20I)N(0 20I)DKLDKLDKLReconstruction LossEncoderEncoderDecoderDecoderDecoderPrototypeyy Positive ItemNegative ItemNot EncodedMacro disentanglement Users may have very diverse interests  and interact with items that belong
to many high-level concepts  e.g.  product categories. We aim to achieve macro disentanglement 
u ] ∈ Rd(cid:48)
u ; . . . ; z(K)
by learning a factorized representation of user u  namely zu = [z(1)
  where
d(cid:48) = Kd  assuming that there are K high-level concepts. The kth component z(k)
u ∈ Rd is for
capturing the user’s preference regarding the kth concept. Additionally  we infer a set of one-hot
vectors C = {ci}M
i=1 for the items  where ci = [ci 1; ci 2; . . . ; ci K]. If item i belongs to concept k 
then ci k = 1 and ci k(cid:48) = 0 for any k(cid:48)

u=1 and C unsupervisedly.

u ; z(2)

(cid:54)= k. We jointly infer {zu}N

Micro disentanglement High-level concepts correspond to the intentions of a user  e.g.  to buy
clothes or a cellphone. We are also interested in disentangling a user’s preference at a more granular
level regarding the various aspects of an item. For example  we would like the different dimensions
of z(k)

u to individually capture the user’s preferred sizes  colors  etc.  if concept k is clothing.

2.2 Model

We start by proposing a generative model that encourages macro disentanglement. For a user u  our
generative model assumes that the observed data are generated from the following distribution:

(cid:21)

 

(1)

(2)

(cid:20)(cid:90)

(cid:89)

pθ(xu) = Epθ (C)

pθ (xu | zu  C) pθ(zu) dzu

pθ (xu | zu  C) =

pθ(xu i | zu  C).

xu i∈xu

The meanings of xu  zu  C are described in the previous subsection. We have assumed pθ(zu) =
pθ(zu | C) in the ﬁrst equation  i.e.  zu and C are generated by two independent sources. Note that
Zu =(cid:80)M
ci = [ci 1; ci 2; . . . ; ci K] is one-hot  since we assume that item i belongs to exactly one concept. And
pθ(xu i | zu  C) = Z−1
θ (z(k)
u ) is a categorical distribution over the M items  where
u ·
θ : Rd → R+ is a shallow neural network that estimates
how much a user with a given preference is interested in item i. We use sampeld softmax [23] to
estimate Zu based on a few sampled items when M is very large.

(cid:80)K
(cid:80)K
k=1 ci k · g(i)
u ) and g(i)
θ (z(k)
k=1 ci k · g(i)

i=1

Macro disentanglement We assume above that the user representation zu is sufﬁcient for predict-
ing how the user will interact with the items. And we further assume that using the kth component
z(k)
u alone is already sufﬁcient if the prediction is about an item from concept k. This design explicitly
encourages z(k)
u to capture preference regarding only the kth concept  as long as the inferred concept
assignment matrix C is meaningful. We will describe later the implementation details of pθ(C) 
pθ(zu) and g(i)
u ). Nevertheless  we note that pθ(C) requires careful design to prevent mode
collapse  i.e.  the degenerate case where almost all items are assigned to a single concept.

θ (z(k)

optimize θ by maximizing a lower bound of(cid:80)
ln pθ(xu) ≥ Epθ (C)

(cid:2)Eqθ (zu|xu C)[ln pθ(xu | zu  C)] − DKL(qθ(zu | xu  C)(cid:107)pθ(zu))(cid:3) .

Variational inference We follow the variational auto-encoder (VAE) paradigm [27  44]  and
u ln pθ(xu)  where ln pθ(xu) is bounded as follows:
(3)
See the supplementary material for the derivation of the lower bound. Here we have introduced
a variational distribution qθ(zu | xu  C)  whose implementation also encourages macro disentan-
glement and will be presented later. The two expectations  i.e.  Epθ (C)[·] and Eqθ (zu|xu C)[·]  are
intractable  and are therefore estimated using the Gumbel-Softmax trick [22  41] and the Gaussian
re-parameterization trick [27]  respectively. Once the training procedure is ﬁnished  we use the mode
of pθ(C) as C  and the mode of qθ(zu | xu  C) as the representation of user u.
(cid:81)d
Micro disentanglement A natural strategy to encourage micro disentanglement is to force statisti-
cal independence between the dimensions  i.e.  to force qθ(z(k)
u j | C)  so that
Fortunately  the Kullback–Leibler (KL) divergence term in the lower bound above does provide a
way to encourage independence. Speciﬁcally  the KL term of our model can be rewritten as:
Epdata(xu) [DKL(qθ(zu | xu  C)(cid:107)pθ(zu))] = Iq(xu; zu) + DKL(qθ(zu | C)(cid:107)pθ(zu)).

each dimension describes an isolated factor. Here qθ(zu | C) =(cid:82) qθ(zu | xu  C)pdata(xu) dxu.

u | C) ≈

j=1 qθ(z(k)

(4)

3

independence between the dimensions  if we choose a prior that satisﬁes pθ(zu) =(cid:81)d(cid:48)

See the supplementary material for the proof. Similar decomposition of the KL term has been
noted for the original VAEs previously [1  25  9]. Penalizing the latter KL term would encourage
j=1 pθ(zu j).
On the other hand  the former term Iq(xu; zu) is the mutual information between xu and zu under
qθ(zu | xu  C)·pdata(xu). Penalizing Iq(xu; zu) is equivalent to applying the information bottleneck
principle [47  2]  which encourages zu to ignore as much noise in the input as it can and to focus
on merely the essential information. We therefore follow β-VAE [19]  and strengthen these two
regularization terms by a factor of β (cid:29) 1  which brings us to the following training objective:

(cid:2)Eqθ (zu|xu C)[ln pθ(xu | zu  C)] − β · DKL(qθ(zu | xu  C)(cid:107)pθ(zu))(cid:3) .

Epθ (C)

(5)

Implementation

i=1 ∈ RM×d used by the decoder  M context representations {ti}M

2.3
In this section  we describe the implementation of pθ(C)  pθ(xu i | zu  C) (the decoder)  pθ(zu)
(the prior)  qθ(zu | xu  C) (the encoder)  and propose an efﬁcient strategy to combat mode collapse.
k=1 ∈ RK×d  M item
The parameters θ of our implementation include: K concept prototypes {mk}K
i=1 ∈ RM×d
representations {hi}M
used by the encoder  and the parameters of a neural network fnn : Rd → R2d. We optimize θ to
maximize the training objective (see Equation 5) using Adam [26].
(cid:81)M
Prototype-based concept assignment A straightforward approach would be to assume pθ(C) =
i=1 p(ci) and parameterize each categorical distribution p(ci) with its own set of K − 1 parameters.
This approach  however  would result in over-parameterization and low sample efﬁciency. We instead
propose a prototype-based implementation. To be speciﬁc  we introduce K concept prototypes
i=1 from the decoder. We then assume ci is a
{mk}K
one-hot vector drawn from the following categorical distribution pθ(ci):

k=1 and reuse the item representations {hi}M

τ   1

si k = COSINE(hi  mk)/τ 

ci ∼ CATEGORICAL (SOFTMAX([si 1; si 2; . . . ; si K]))  

(6)
where COSINE(a  b) = a(cid:62)b/((cid:107)a(cid:107)2 (cid:107)b(cid:107)2) is the cosine similarity  and τ is a hyper-parameter that
τ ]. We set τ = 0.1 to obtain a more skewed distribution.
scales the similarity from [−1  1] to [− 1
Preventing mode collapse We use cosine similarity  instead of the inner product similarity adopted
by most existing deep learning methods [32  31  18]. This choice is crucial for preventing mode
collapse. In fact  with inner product  the majority of the items are highly likely to be assigned
to a single concept mk(cid:48) that has an extremely large norm  i.e.  (cid:107)mk(cid:48)(cid:107)2 → ∞  even when the
items {hi}M
i=1 correctly form K clusters in the high-dimensional Euclidean space. And we observe
empirically that this phenomenon does occur frequently with inner product (see Figure 2e). In
contrast  cosine similarity avoids this degenerate case due to the normalization. Moreover  cosine
similarity is related with the Euclidean distance on the unit hypersphere  and the Euclidean distance
is a proper metric that is more suitable for inferring the cluster structure  compared to inner product.

u ; z(2)

(cid:80)K
u ; . . . ; z(K)
k=1 ci k · g(i)

u ) = exp(COSINE(z(k)
u }N

Decoder The decoder predicts which item out of the M ones is mostly likely to be clicked by a user 
when given the user’s representation zu = [z(1)
u ] and the one-hot concept assignments
θ (z(k)
u ) is a categorical distribution over
i=1. We assume that pθ(xu i | zu  C) ∝
{ci}M
θ (z(k)
the M items  and deﬁne g(i)
u   hi)/τ ). This design implies that {hi}M
i=1
will be micro-disentangled if {z(k)
u=1 is micro-disentangled  as the two’s dimensions are aligned.
Prior & Encoder The prior pθ(zu) needs to be factorized in order to achieve micro disentan-
0I). The encoder qθ(zu | xu  C) is for comput-
glement. We therefore set pθ(zu) to N (0  σ2
ing the representation of a user when given the user’s behavior data xu. The encoder main-
tains an additional set of context representations {ti}M
i=1  rather than reusing the item represen-
tations {hi}M
i=1 from the decoder  which is a common practice in the literature [32]. We assume
| xu  C) as a multivariate
normal distribution with a diagonal covariance matrix N (µ(k)
u )]2)  where the mean and

qθ(zu | xu  C) = (cid:81)K

| xu  C)  and represent each qθ(z(k)
u   [diag(σ(k)

k=1 qθ(z(k)

u

u

4

(cid:18)

1
2

−

b(k)
u

(cid:19)

. (7)

a(k)
u
(cid:107)a(k)
u (cid:107)2

the standard deviation are parameterized by a neural network fnn : Rd → R2d:
(a(k)

  σ(k)

u ) = fnn

u   b(k)

u =

u ← σ0 · exp

   µ(k)

(cid:80)
(cid:113)(cid:80)

i:xu i=+1 ci k · ti
i:xu i=+1 c2
i k

The neural network fnn(·) captures nonlinearity  and is shared across the K components. We
normalize the mean  so as to be consistent with the use of cosine similarity which projects the
representations onto a unit hypersphere. Note that σ0 should be set to a small value  e.g.  around 0.1 
since the learned representations are now normalized.

2.4 User-Controllable Recommendation

The controllability enabled by the disentangled representations can bring a new paradigm for recom-
mendation. It allows a user to interactively search for items that are similar to an initial item except for
some controlled aspects  or to explicitly adjust the disentangeld representation of his/her preference 
learned by the system from his/her past behaviors  to actually match the current preference. Here we
formalize the task of user-controllable recommendation  and illustrate a possible solution.
Task deﬁnition Let h∗ ∈ Rd be the representation to be altered  which can be initialized as either
an item representation or a component of a user representation. The task is to gradually alter its jth
dimension h∗ j  while retrieving items whose representations are similar to the altered representation.
This task is nontrivial  since usually no item will have exactly the same representation as the altered
one  especially when we want the transition to be smooth  monotonic  and thus human-understandable.

Solution Here we illustrate our approach to this task. We ﬁrst probe the suitable range (a  b) for
h∗ j. Let us assume that prototype k∗ is the prototype closest to h∗. The range (a  b) is decided such
that: prototype k∗ remains the prototype closest to h∗ if and only if h∗ j ∈ (a  b). We can decide each
endpoint of the range using binary search. We then divide the range (a  b) into B subranges  a = a0 <
a1 < a2 . . . < aB = b. We ensure that the subranges contain roughly the same number of items from
decide the B items by maximizing(cid:80)
concept k∗ when dividing (a  b) . Finally  we aim to retrieve B items {it}B
t=1 ∈ {1  2  . . .   M}B
that belong to concept k∗  each from one of the B subranges  i.e.  hit j ∈ (at−1  at]. We thus
t(cid:48)  −j )
where hi −j = [hi 1; hi 2; . . . ; hi j−1; hi j+1; . . . ; hi d] ∈ Rd−1 and γ is a hyper-parameter. We
approximately solve this maximization problem sequentially using beam search [36].
Intuitively  selecting items from the B subranges ensures that the items change monotonously in
terms of the jth dimension. On the other hand  the ﬁrst term in the maximization problem forces the
retrieved items to be similar with the initial item in terms of the dimensions other than j  while the
second term encourages any two retrieved items to be similar in terms of the dimensions other than j.

COSINE(hit −j  h∗ −j )

1≤t<t(cid:48)≤B e

COSINE(hit −j  hi

1≤t≤B e

(cid:80)

+γ·

τ

τ

 

3 Empirical Results

3.1 Experimental Setup

Datasets We conduct our experiments on ﬁve real-world datasets. Speciﬁcally  we use the large-
scale Netﬂix Prize dataset [4]  and three MovieLens datasets of different scales (i.e.  ML-100k 
ML-1M  and ML-20M) [16]. We follow MultVAE [32]  and binarize these four datasets by keeping
ratings of four or higher while only keeping users who have watched at least ﬁve movies. We
additionally collect a dataset  named AliShop-7C 2  from Alibaba’s e-commerce platform Taobao.
AliShop-7C contains user-item interactions associated with items from seven categories  as well as
item attributes such as titles and images. Every user in this dataset clicks items from at least two
categories. The category labels are used for evaluation only  and not for training.

Baselines We compare our approach with MultDAE [32] and β-MultVAE [32]  the two state-of-
the-art methods for collaborative ﬁltering. In particular  β-MultVAE is similar to β-VAE [19]  and
has a hyper-parameter β that controls the strength of disentanglement. However  β-MultVAE does
not learn disentangled representations  because it requires β (cid:28) 1 to perform well.

2The dataset and our code are at https://jianxinma.github.io/disentangle-recsys.html.

5

Table 1: Collaborative ﬁltering. All methods are constrained to have around 2M d parameters  where
M is the number of items and d is the dimension of each item representation. We set d = 100.

Dataset

Method
AliShop-7C MultDAE

ML-100k

ML-1M

ML-20M

Netﬂix

Ours

Ours

Ours

NDCG@100

Ours

Ours

0.23923 (±0.00380)
β-MultVAE 0.23875 (±0.00379)
0.29148 (±0.00380)
MultDAE
0.24487 (±0.02738)
β-MultVAE 0.27484 (±0.02883)
0.28895 (±0.02739)
MultDAE
0.40453 (±0.00799)
β-MultVAE 0.40555 (±0.00809)
0.42740 (±0.00789)
MultDAE
0.41900 (±0.00209)
β-MultVAE 0.41113 (±0.00212)
0.42496 (±0.00212)
0.37450 (±0.00095)
MultDAE
β-MultVAE 0.36291 (±0.00094)
0.37987 (±0.00096)

Metrics

Recall@20

0.15242 (±0.00305)
0.15040 (±0.00302)
0.18616 (±0.00317)
0.23794 (±0.03605)
0.24838 (±0.03294)
0.30951 (±0.03808)
0.34382 (±0.00961)
0.33960 (±0.00919)
0.36046 (±0.00947)
0.39169 (±0.00271)
0.38263 (±0.00273)
0.39649 (±0.00271)
0.33982 (±0.00123)
0.32792 (±0.00122)
0.34587 (±0.00124)

Recall@50

0.24892 (±0.00391)
0.24589 (±0.00387)
0.30256 (±0.00397)
0.32279 (±0.04070)
0.35270 (±0.03927)
0.41309 (±0.04503)
0.46781 (±0.01032)
0.45825 (±0.01039)
0.49039 (±0.01029)
0.53054 (±0.00285)
0.51975 (±0.00289)
0.52901 (±0.00284)
0.43247 (±0.00126)
0.41960 (±0.00125)
0.43478 (±0.00125)

Hyper-parameters We constrain the number of learnable parameters to be around 2M d for each
method so as to ensure fair comparison  which is equivalent to using d-dimensional representations
for the M items. Note that all the methods under investigation use two sets of item representations 
and we do not constrain the dimension of user representations since they are not parameters. We set
d = 100 unless otherwise speciﬁed. We ﬁx τ to 0.1. We tune the other hyper-parameters of both our
approach’s and our baselines’ automatically using the TPE method [6] implemented by Hyepropt [5].

3.2 Recommendation Performance

We evaluate the performance of our approach on the task of collaborative ﬁltering for implicit feedback
datasets [21]  one of the most common settings for recommendation. We follow the experiment
protocol established by the previous work [32] strictly  and use the same preprocessing procedure as
well as evaluation metrics. The results on the ﬁve datasets are listed in Table 1.
We observe that our approach outperforms the baselines signiﬁcantly  especially on small  sparse
datasets. The improvement is likely due to two desirable properties of our approach. Firstly  macro
disentanglement not only allows us to accurately represent the diverse interests of a user using
the different components  but also alleviates data sparsity by allowing a rarely visited item to
borrow information from other items of the same category  which is the motivation behind many
hierarchical methods [50  38]. Secondly  as we will show in Section 3.4  the dimensions of the
representations learned by our approach are highly disentangled  i.e.  independent  thanks to the
micro disentanglement regularizer  which leads to more robust performance.

3.3 Macro Disentanglement

We visualize the high-dimensional representations learned by our approach on AliShop-7C in order to
qualitatively examine to which degree our approach can achieve macro disentanglement. Speciﬁcally 
we set K to seven  i.e.  the number of ground-truth categories  when training our model. We visualize
the item representations and the user representations together using t-SNE [40]  where we treat the
K components of a user as K individual points and keep only the two components that have the
i:xu i>0 ci k  where ci k is

highest conﬁdence levels. The conﬁdence of component k is deﬁned as(cid:80)

the value inferred by our model  rather than the ground-truth. The results are shown in Figure 2.

Interpretability Figure 2c  which shows the clusters inferred based on the prototypes  is rather
similar to Figure 2d that shows the ground-truth categories  despite the fact that our model is trained

6

(a) Items and users. Item i is colored according to arg maxk ci k 
i.e.  the inferred category. Each component of a user is treated as an
individual point  and the kth component is colored according to k.

(b) Users only  colored in the
same way as Figure 2a.

(c) Items only  colored in the same
way as Figure 2a.

(d) Items only  colored according
to their ground-truth categories.

(e) Items  obtained by training a
new model that uses inner product
instead of cosine  colored accord-
ing to the value of arg maxk ci k.
Figure 2: The discovered clusters of items (see Figure 2c)  learned unsupervisedly  align well with
the ground-truth categories (see Figure 2d  where the color order is chosen such that the connections
between the ground-truth categories and the learned clusters are easy to verify). Figure 2e highlights
the importance of using cosine similarity  rather than inner product  to combat mode collapse.

(a) Bag size.

(b) Bag color.

(c) Styles of phone cases.

(d) Bag size. The same dimension
as Figure 3a.
Figure 3: Starting from an item representation  we gradually alter the value of a target dimension 
and list the items that have representations similar to the altered representations (see Subsection 2.4).

(e) Bag color. The same dimension
as Figure 3b.

(f) Chicken → beef → mutton →
seafood.

without the ground-truth category labels. This demonstrates that our approach is able to discover and
disentangle the macro structures underlying the user behavior data in an interpretable way. Moreover 
the components of the user representations are near the correct cluster centers (see Figure 2a and
Figure 2b)  and are hence likely capturing the users’ separate preferences for different categories.

Cosine vs. inner product To highlight the necessity of using cosine similarity instead of the more
commonly used inner product similarity  we additionally train a new model that uses inner product
in place of cosine  and visualize the learned item representations in Figure 2e. With inner product 
the majority of the items are assigned to the same prototype (see Figure 2e). In comparison  all
seven prototypes learned by the cosine-based model are assigned a signiﬁcant number of items (see
Figure 2c). This ﬁnding supports our claim that a proper metric space  such as the one implied by the
cosine similarity  is important for preventing mode collapse.

3.4 Micro Disentanglement
Independence We vary the hyper-parameters related with micro disentanglement (β and σ0 for our
approach  β for β-MultVAE)  and plot in Figure 4 the relationship between the level of independence
achieved and the recommendation performance. Each method is evaluated with 2 000 randomly

7

Figure 4: Micro disentanglement vs. recommendation performance. (d  d(cid:48)) indicates d-dimensional
item representations and d(cid:48)-dimensional user representations. Note that d(cid:48) = Kd. We observe that
(1) our approach outperforms the baselines in terms of both performance and micro disentanglement 
and (2) macro disentanglement beneﬁts micro disentanglement  as K = 7 is better than K = 1.

(cid:80)
sampled conﬁgurations on ML-100k. We quantify the level of independence achieved by a set of
d-dimensional representations using 1 − 2
1≤i<j≤d |corri j|  where corri j is the correlation
between dimension i and j. Figure 4 suggests that high performance is in general associated with a
relatively high level of independence. And our approach achieves a much higher level of independence
than β-MultVAE. In addition  the improvement brought by using K = 7 instead of K = 1 reveals
that macro disentanglement can possibly help improve micro disentanglement.

d(d−1)

Interpretability We train our model with K = 7  d = 10  β = 50 and σ0 = 0.3  on AliShop-7C 
and investigate the interpretability of the dimensions using the approach illustrated in Subsection 2.4.
In Figure 3  we list some representative dimensions that have human-understandable semantics.
These examples suggest that our approach has the potential to give users ﬁne-grained control over
targeted aspects of the recommendation lists. However  we note that not all dimensions are human-
understandable. In addition  as pointed out by Locatello et al. [34]  well-trained interpretable models
can only be reliably identiﬁed with the help of external knowledge  e.g.  item attributes. We thus
encourage future efforts to focus more on (semi-)supervised methods [35].

4 Related Work

Learning representations from user behavior Learning from user behavior has been a central
task of recommender systems since the advent of collaborative ﬁltering [43  42  46  12  21]. Early
attempts apply matrix factorization [29  45]  while the more recent deep learning methods [49  32 
31  52  11  18] achieve massive improvement by learning highly informative representations. The
entanglement of the latent factors behind user behavior  however  is mostly neglected by the black-box
representation learning process adopted by the majority of the existing methods. To the extent of our
knowledge  we are the ﬁrst to study disentangled representation learning on user behavior data.

Disentangled representation learning Disentangled representation learning aims to identify and
disentangle the underlying explanatory factors [3]. β-VAE [19] demonstrates that disentanglement
can emerge once the KL divergence term in the VAE [27] objective is aggressively penalized. Later
approaches separate the information bottleneck term [48  47] and the total correlation term  and
achieve a greater level of disentanglement [9  25  8]. Though a few existing approaches [14  10  7 
13  24] do notice that a dataset can contain samples from different concepts  i.e.  follow a mixture
distribution  their settings are fundamentally different from ours. To be speciﬁc  these existing
approaches assume that each instance is from a concept  while we assume that each instance interacts
with objects from different concepts. The majority of the existing efforts are from the ﬁeld of computer
vision [28  15  20  30  53  14  10  39  19]. Disentangled representation learning on relational data 
such as graph-structured data  was not explored until recently [37]. This work focus on disentangling
user behavior  another kind of relational data commonly seen in recommender systems.

5 Conclusions

In this paper  we studied the problem of learning disentangled representations from user behavior 
and presented our approach that performs disentanglement at both a macro and a micro level. An
interesting direction for future research is to explore novel applications that can be enabled by the
interpretability and controllability brought by the disentangled representations.

8

0.140.160.180.200.220.240.26Performance (Recall@20)0.20.30.40.50.60.70.80.9Disentanglement (Uncorrelatedness)Representations = Items'0.140.160.180.200.220.240.26Performance (Recall@20)Representations = Users'MethodOurs(100 700)Ours(100 100)beta-MultVAE(100 700)beta-MultVAE(100 100)Acknowledgments

The authors from Tsinghua University are supported in part by National Program on Key Basic
Research Project (No. 2015CB352300)  National Key Research and Development Project (No.
2018AAA0102004)  National Natural Science Foundation of China (No. 61772304  No. 61521002 
No. 61531006  No. U1611461)  Beijing Academy of Artiﬁcial Intelligence (BAAI)  and the Young
Elite Scientist Sponsorship Program by CAST. All opinions  ﬁndings  and conclusions in this paper
are those of the authors and do not necessarily reﬂect the views of the funding agencies.

References
[1] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. IEEE transactions on pattern analysis and machine intelligence 
40(12):2897–2905  2018.

[2] Alexander A Alemi  Ian Fischer  Joshua V Dillon  and Kevin Murphy. Deep variational

information bottleneck. In International Conference for Learning Representations  2015.

[3] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and
new perspectives. IEEE transactions on pattern analysis and machine intelligence  35(8):1798–
1828  2013.

[4] James Bennett and Stan Lanning. The netﬂix prize. In Proceedings of KDD Cup and Workshop

2007  2007.

[5] James Bergstra  Dan Yamins  and David D Cox. Hyperopt: A python library for optimizing the
hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science
conference  pages 13–20. Citeseer  2013.

[6] James S Bergstra  Rémi Bardenet  Yoshua Bengio  and Balázs Kégl. Algorithms for hyper-
parameter optimization. In Advances in neural information processing systems  pages 2546–
2554  2011.

[7] Diane Bouchacourt  Ryota Tomioka  and Sebastian Nowozin. Multi-level variational autoen-
coder: Learning disentangled representations from grouped observations. In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence  2018.

[8] Christopher P Burgess  Irina Higgins  Arka Pal  Loic Matthey  Nick Watters  Guillaume Des-
jardins  and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv preprint
arXiv:1804.03599  2018.

[9] Tian Qi Chen  Xuechen Li  Roger B Grosse  and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In Advances in Neural Information Processing
Systems  pages 2610–2620  2018.

[10] Xi Chen  Yan Duan  Rein Houthooft  John Schulman  Ilya Sutskever  and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems  pages 2172–2180  2016.

[11] Paul Covington  Jay Adams  and Emre Sargin. Deep neural networks for youtube recommenda-
tions. In Proceedings of the 10th ACM Conference on Recommender Systems  pages 191–198.
ACM  2016.

[12] Mukund Deshpande and George Karypis. Item-based top-n recommendation algorithms. ACM

Transactions on Information Systems (TOIS)  22(1):143–177  2004.

[13] Nat Dilokthanakul  Pedro AM Mediano  Marta Garnelo  Matthew CH Lee  Hugh Salimbeni 
Kai Arulkumaran  and Murray Shanahan. Deep unsupervised clustering with gaussian mixture
variational autoencoders. arXiv preprint arXiv:1611.02648  2016.

[14] Emilien Dupont. Learning disentangled joint continuous and discrete representations.

Advances in Neural Information Processing Systems  pages 710–720  2018.

In

9

[15] SM Ali Eslami  Danilo Jimenez Rezende  Frederic Besse  Fabio Viola  Ari S Morcos  Marta
Garnelo  Avraham Ruderman  Andrei A Rusu  Ivo Danihelka  Karol Gregor  et al. Neural scene
representation and rendering. Science  360(6394):1204–1210  2018.

[16] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm

transactions on interactive intelligent systems (tiis)  5(4):19  2016.

[17] Xiangnan He  Tao Chen  Min-Yen Kan  and Xiao Chen. Trirank: Review-aware explainable
In Proceedings of the 24th ACM International on

recommendation by modeling aspects.
Conference on Information and Knowledge Management  pages 1661–1670. ACM  2015.

[18] Xiangnan He  Lizi Liao  Hanwang Zhang  Liqiang Nie  Xia Hu  and Tat-Seng Chua. Neural
collaborative ﬁltering. In Proceedings of the 26th International Conference on World Wide Web 
pages 173–182  2017.

[19] Irina Higgins  Loic Matthey  Arka Pal  Christopher Burgess  Xavier Glorot  Matthew Botvinick 
Shakir Mohamed  and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations 
volume 3  2017.

[20] Jun-Ting Hsieh  Bingbin Liu  De-An Huang  Li F Fei-Fei  and Juan Carlos Niebles. Learning
to decompose and disentangle representations for video prediction. In Advances in Neural
Information Processing Systems  pages 517–526  2018.

[21] Yifan Hu  Yehuda Koren  and Chris Volinsky. Collaborative ﬁltering for implicit feedback

datasets. In ICDM  volume 8  pages 263–272. Citeseer  2008.

[22] Eric Jang  Shixiang Gu  and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144  2016.

[23] Sébastien Jean  Kyunghyun Cho  Roland Memisevic  and Yoshua Bengio. On using very large
target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics (ACL 2015)  2015.

[24] Zhuxi Jiang  Yin Zheng  Huachun Tan  Bangsheng Tang  and Hanning Zhou. Variational deep
embedding: an unsupervised and generative approach to clustering. In Proceedings of the 26th
International Joint Conference on Artiﬁcial Intelligence  pages 1965–1972. AAAI Press  2017.

[25] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on

Machine Learning  pages 2654–2663  2018.

[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference for Learning Representations  2014.

[27] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[28] Nikos Komodakis and Spyros Gidaris. Unsupervised representation learning by predicting

image rotations. In International Conference on Learning Representations (ICLR)  2018.

[29] Yehuda Koren  Robert Bell  Chris Volinsky  et al. Matrix factorization techniques for recom-

mender systems. Computer  42(8):30–37  2009.

[30] Adam Kosiorek  Hyunjik Kim  Yee Whye Teh  and Ingmar Posner. Sequential attend  infer 
repeat: Generative modelling of moving objects. In Advances in Neural Information Processing
Systems  pages 8606–8616  2018.

[31] Xiaopeng Li and James She. Collaborative variational autoencoder for recommender systems.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  pages 305–314. ACM  2017.

[32] Dawen Liang  Rahul G. Krishnan  Matthew D. Hoffman  and Tony Jebara. Variational autoen-
coders for collaborative ﬁltering. In Proceedings of the 2018 World Wide Web Conference 
WWW ’18  pages 689–698  2018.

10

[33] Bin Liu  Anmol Sheth  Udi Weinsberg  Jaideep Chandrashekar  and Ramesh Govindan. Adre-
veal: Improving transparency into online targeted advertising. In Proceedings of the Twelfth
ACM Workshop on Hot Topics in Networks  2013.

[34] Francesco Locatello  Stefan Bauer  Mario Lucic  Sylvain Gelly  Bernhard Schölkopf  and
Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled
representations. In Proceedings of the 36th International Conference on Machine Learning
(ICML 2019)  2019.

[35] Francesco Locatello  Michael Tschannen  Stefan Bauer  Gunnar Rätsch  Bernhard Schölkopf 
and Olivier Bachem. Disentangling factors of variation using few labels. arXiv preprint
arXiv:1905.01258  2019.

[36] B LOWERE. The harpy speech recognition system. PhD thesis  Carnegie Mellon University 

1976.

[37] Jianxin Ma  Peng Cui  Kun Kuang  Xin Wang  and Wenwu Zhu. Disentangled graph convolu-
tional networks. In Proceedings of the 36th International Conference on Machine Learning
(ICML 2019)  2019.

[38] Jianxin Ma  Peng Cui  Xiao Wang  and Wenwu Zhu. Hierarchical taxonomy aware network
embedding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  2018.

[39] Liqian Ma  Qianru Sun  Stamatios Georgoulis  Luc Van Gool  Bernt Schiele  and Mario Fritz.
Disentangled person image generation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 99–108  2018.

[40] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research  9(Nov):2579–2605  2008.

[41] Chris J Maddison  Andriy Mnih  and Yee Whye Teh. The concrete distribution: A continuous

relaxation of discrete random variables. arXiv preprint arXiv:1611.00712  2016.

[42] Steffen Rendle  Christoph Freudenthaler  Zeno Gantner  and Lars Schmidt-Thieme. Bpr:
In Proceedings of the twenty-ﬁfth

Bayesian personalized ranking from implicit feedback.
conference on uncertainty in artiﬁcial intelligence  pages 452–461. AUAI Press  2009.

[43] Paul Resnick  Neophytos Iacovou  Mitesh Suchak  Peter Bergstrom  and John Riedl. Grouplens:
an open architecture for collaborative ﬁltering of netnews. In Proceedings of the 1994 ACM
conference on Computer supported cooperative work  pages 175–186. ACM  1994.

[44] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In Proceedings of the 31st International
Conference on International Conference on Machine Learning-Volume 32  pages II–1278.
JMLR. org  2014.

[45] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In NIPS  volume 20 

pages 1–8  2011.

[46] Badrul Sarwar  George Karypis  Joseph Konstan  and John Riedl. Item-based collaborative
ﬁltering recommendation algorithms. In Proceedings of the 10th international conference on
World Wide Web  pages 285–295. ACM  2001.

[47] Naftali Tishby  Fernando C Pereira  and William Bialek. The information bottleneck method.

arXiv preprint physics/0004057  2000.

[48] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In

2015 IEEE Information Theory Workshop (ITW)  pages 1–5. IEEE  2015.

[49] Hao Wang  Naiyan Wang  and Dit-Yan Yeung. Collaborative deep learning for recommender
systems. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  pages 1235–1244. ACM  2015.

11

[50] Yi Zhang and Jonathan Koren. Efﬁcient bayesian hierarchical user modeling for recommen-
dation system. In Proceedings of the 30th annual international ACM SIGIR conference on
Research and development in information retrieval  2007.

[51] Yongfeng Zhang  Guokun Lai  Min Zhang  Yi Zhang  Yiqun Liu  and Shaoping Ma. Explicit
factor models for explainable recommendation based on phrase-level sentiment analysis. In
Proceedings of the 37th international ACM SIGIR conference on Research & development in
information retrieval  pages 83–92. ACM  2014.

[52] Chang Zhou  Jinze Bai  Junshuai Song  Xiaofei Liu  Zhengchao Zhao  Xiusi Chen  and Jun
Gao. Atrank: An attention-based user behavior modeling framework for recommendation. In
Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[53] Jun-Yan Zhu  Zhoutong Zhang  Chengkai Zhang  Jiajun Wu  Antonio Torralba  Josh Tenen-
baum  and Bill Freeman. Visual object networks: Image generation with disentangled 3d
representations. In Advances in Neural Information Processing Systems  pages 118–129  2018.

12

,Dylan Foster
Satyen Kale
Mehryar Mohri
Karthik Sridharan
Jianxin Ma
Chang Zhou
Peng Cui
Hongxia Yang
Wenwu Zhu