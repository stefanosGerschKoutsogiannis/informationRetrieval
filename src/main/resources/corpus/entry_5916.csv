2018,Weakly Supervised Dense Event Captioning in Videos,Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area  existing methods tackle this task by making use of dense temporal annotations  which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning  which does not require temporal segment annotations for model training.  Our solution is based on the one-to-one correspondence assumption  each caption describes one temporal segment  and each temporal segment has one caption  which holds in current benchmark datasets and  most real world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to  demonstrate the ability of our model  on both dense event captioning and sentence localization in videos.,Weakly Supervised Dense Event Captioning in Videos

Xuguang Duan∗ 1  Wenbing Huang∗2  Chuang Gan3  Jingdong Wang4 

Wenwu Zhu1  Junzhou Huang2

1 Tsinghua University  Beijing  China; 2 Tencent AI Lab. ;

3 MIT-IBM Watson AI Lab; 4 Microsoft Research Asia  Beijing  China;

duan_xg@outlook.com  hwenbing@126.com  ganchuang1990@gmail.com 

jingdw@microsoft.com  wwzhu@tsinghua.edu.cn joehhuang@tencent.com

Abstract

Dense event captioning aims to detect and describe all events of interest contained
in a video. Despite the advanced development in this area  existing methods tackle
this task by making use of dense temporal annotations  which is dramatically
source-consuming. This paper formulates a new problem: weakly supervised dense
event captioning  which does not require temporal segment annotations for model
training. Our solution is based on the one-to-one correspondence assumption 
each caption describes one temporal segment  and each temporal segment has one
caption  which holds in current benchmark datasets and most real-world cases. We
decompose the problem into a pair of dual problems: event captioning and sentence
localization and present a cycle system to train our model. Extensive experimental
results are provided to demonstrate the ability of our model on both dense event
captioning and sentence localization in videos.

1

Introduction

Dramatic improvements have been made on video understanding due to the development of deep
neural networks and large-scale video datasets [1  2  3]. Among the wide variety of applications
on video understanding  the video captioning task is attracting more and more interests in recent
years [4  5  6  7  8  9  10  11]. In video captioning  the machine is required to describe the video
content in the natural language form  which makes it more meticulous and thus challenging compared
to other tasks describing the video content using a few tags or labels  such as video classiﬁcation and
action detection [12  13].
The current trend on video captioning is to perform Dense Event Captioning (DEC  also called
Dense-Captioning Event in videos in [10]). As one video usually contains more than one event of
interest  the goal of DEC is to locate all events in the video and perform captioning for each of
them. Clearly  such dense captioning enriches the information we obtained and is beneﬁcial for more
in-depth video analysis. Nevertheless  to achieve this goal  we need to collect the caption annotation
for each event along with its temporal segment coordinate (i.e.  the start and end times) for network
training  which is source-consuming and impractical.
In this paper  we introduce a new problem  Weakly Supervised Dense Event Captioning (WS-DEC)2 
which aims at dense event captioning only using the caption annotations for training. In the training
∗denotes equal contributions. This paper was done when Xuguang Duan was served as a research intern in

Tencent AI Lab. Wenwu Zhu is the corresponding author.

2More speciﬁcally  the term “weakly” in our paper refers to the incompleteness of the supervision rather than

the amount of information.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

dataset  only a paragraph or a set of sentences is available to describe each video  but the temporal
segment coordinate of each event and its correspondence to the captioning sentence is not given. For
testing  the model is able to detect all events of interest and provides the caption for each event. One
obvious advantage of the weak supervision is the signiﬁcant reduction of the annotation cost. This
beneﬁt becomes more demanded if we attempt to make use of the videos in the wild (e.g. the videos
on the web) to enlarge the training set.
We solve the problem by unitizing the one-to-one correspondence assumption: each caption describes
one temporal segment  and each temporal segment has one caption. We decompose the problem
into a cycle of dual problems: caption generation and sentence localization. During the training
phase  we perform sentence localization from the given caption annotation  to obtain the associated
segment that is then fed to the caption generator to reconstruct the caption back. The objective is
to minimize the reconstruction error. Our cycle process repeatedly optimizes caption generator and
sentence localizer without any ground-truth segment. During the testing phase  it is infeasible to
apply the cycle process in the same way as the training phase  as the caption is unknown. Instead  we
ﬁrst perform caption generation on a bunch of randomly initialized candidate segments and then map
the resulting captions back to the segment space. The output segments by this cycle process will get
closer to the ground-truths if certain properties are satisﬁed. We thus formulate an extra loss for the
training to enforce our model to meet these properties. Based on the detected segment  we are able to
perform event captioning on it  and thus achieve the goal of dense event captioning.
We summarize our contributions as follow. I. We propose to solve the DEC task without the need of
temporal segments annotation  thus introduce a new problem WS-DEC  aiming at making use of the
huge amount of data in the web and thus reducing the cost of annotation. II. We develop a ﬂexible
and efﬁcient method to address WS-DEC by exploring the one-to-one correspondence between
the temporal segment and event caption. III. We evaluate the performance of our approach on the
widely-used benchmark ActivityNet Captions [10]. The experimental results verify the effectiveness
of our method regarding the dense event captioning ability and sentence localization accuracy.

2 Related Work

We brieﬂy review recent advances on video captioning  dense even captioning and sentence localiza-
tion in videos in the next few paragraphs.
Video captioning. Early researchers simply aggregate frame-level features by mean pooling and
then use similar pipelines as image captioning [4] to generate caption sentences. This mean-pooling
strategy works well for short video clips  but will easily crash with the increase of video length.
Recurrent Neural Networks (RNNs) along with attention mechanisms are thus employed [5  6  7  8] 
among which S2VT[7] exhibits more desirable efﬁciency and ﬂexibility. Since a single sentence is
far from enough to describe the dynamics of untrimmed real-world video  some researchers attempt
to generate multiple sentences or a paragraph to describe the given video [9  14  15]. Among them 
the work by [15] aims at providing diverse captions corresponded to different spatial regions in a
weakly supervised manner. Despite the similar weakly-supervised setting to this work  our paper
differently is to localize different events temporally and perform captioning for each detected event 
which generates descriptions based on meaningful events instead of bewildering visual features.
Dense Event Captioning. Recent attention have been paid on dense event captioning in videos [10 
11]. Current works all follow the "detection and description" framework. The model proposed by [10]
resorts to the DAP method[16] for event detection and enhance the caption ability by applying the
context-aware S2VT[7]. Meanwhile  [11] employs a grouping schema based on their previous video
highlight detector[17] to perform event detection  and the attribute-augmented LSTM (LSTM-A)[18]
for caption generation. Most recently  [19  20] try to boost the event proposal with generated sentence 
while [21] tries to leverage bidirectional SST[22] instead of DAP[16]. Also  [21] proposes to use
bidirectional attention for dense captioning. In contrast to these fully-supervised works  we address
the task without the guidance of temporal segments during training. Speciﬁcally  instead of detecting
all event using the one-to-many-mapping event detector[23  22]  we try to localize them one by one
using our sentence localizer and caption generator.
Sentence localization in videos. Localizing sentence in videos is constrained to certain visual
domains (e.g.  kitchen environment) in the early stage[24  25  26]. Due to the development of
deep learning  several models have been proposed to work on real-world videos [27  28  29]. The

2

approaches by [27  28] are categorized into the typical two-stage framework as “scan and localize”.
To elaborate a bit  the work by [27] employs a Moment Context Network(MCN) for matching
candidate video clip and sentence query  while the model in [28] proposes a Cross-modal Temporal
Regression Localizer (CTRL) to make use of coarsely sampled clips for computation reduction. In
contrast  [29] opens up a different direction by regressing the temporal coordinate given learned video
representation and sentence representation. In our framework  the sentence localization is originally
formulated as an intermediate task to enable weakly supervised training for dense event captioning.
Actually  our model also provides an unsupervised solution to sentence localization.

3 The Proposed Method

We start this section by presenting the fundamental formulation of our method and follow it up with
providing the details on model architecture.
Notations. Prior to further introduction  we ﬁrst provide the key notations used in this work. We
denote the given video by V = (v1  v2 ···   vT ) with vt indexing the image frame at time t. We
deﬁne the event of interest as a temporally-continues segment of V and denote all the events by their
temporal coordinate as {Si = (mi  wi)}N
i   where N is the number of events  mi and wi denote the
temporal center and width  respectively. The temporal coordinates for all events are normalized to be
within [0  1] throughout this paper. Let the caption for the segment Si be the sentence Ci = {cij}Tc
where cij denotes the j-th word  and Tc is the length of caption sentence.

j=0

3.1 Formulation
Formally  the conventional event captioning models [10  11] ﬁrst locate the temporal segments {Si}N
i
of the events by the event proposal module  and then generate the caption Ci for each segment Si
through the caption generator. Here  for our weak supervision  the segment labels are unprovided and
only the caption sentences (could be multiple for a single video) are available.
The biggest difﬁculty of our task lies in that it’s impossible to perform weakly supervised event
proposal which in nature is a one-to-many mapping problem and is too noisy for weakly learning.
Instead  we try a novel new direction that makes use the bidirectional one-to-one mapping between
caption sentence and temporal segment. Formally  we formulate a pair of dual tasks of sentence
localization and event captioning. Conditioned on a target video V   the dual tasks are deﬁned as:
• Sentence localization: this task is to localize segment Si corresponded to the given caption
• Event Captioning:

Ci  i.e.  learning the mapping lθ1 : (V   Ci) → Si  associated with parameter θ1;
the event captioning inversely generates caption Ci for the given
segment Si  i.e.  learning the function gθ2 : (V   Si) → Ci  associated with parameter θ2.
The dual problems exist simultaneously once the correspondence between Si and Ci is one-to-one 
which is the case in our problem for that Si and Ci are tied together by their corresponding event.
If we nest these dual functions together  then any valid caption and segment pair (Ci  Si) becomes a
ﬁxed-point solution of the following functions:

Ci = gθ2 (V   lθ1(V   Ci)) 
Si = lθ1 (V   gθ2(V   Si)).

(1)
(2)

More interestingly  Eq. (1) derives an auto-encoder for Ci where the segment Si gets vanished. This
gives us a solution to train the parameters of both functions of l and g  by formulating the loss as

Lc = dist(Ci  gθ2(V   lθ1(V   Ci))) 

(3)

where dist(· ·) is a loss distance.
A remaining issue that it is still infeasible to perform dense event captioning in the testing phase by
applying lθ1 or gθ2 since both the temporal segment and caption sentence are unknown. To tackle the
testing issue  we introduce the concept of the ﬁxed-point iteration [30] as follow.
Proposition 1 (Fixed-Point-Iteration). We deﬁne the iteration as

S(t + 1) = lθ1 (V   gθ2(V   S(t))) 

(4)

3

Figure 1: Model structure and training connections. Our model is composed of a sentence localizer
and a caption generator. For training  the video and all event descriptions are available. We feed
the video and one of its event descriptions to the sentence localizer to obtain a temporal segment
prediction  and then the temporal segment is used to regenerate the caption sentence  and to relocate
the temporal segment. The trained dual system is used to generate dense event caption with random
temporal segments in the test phase.

where S(t) will converge to the ﬁxed-point solution i.e. S∗ = lθ1(V   gθ2 (V   S∗))  if there exists a
sufﬁciently small  > 0 satisfying (cid:107)S(0) − S∗(cid:107) ≤  and the function lθ1 (V   gθ2(V   S)) is locally
Lipschitz continuous around S∗ with Lipschitz constant L < 1.

Note that the proof has already been derived previously. For better readability  we include them in the
supplementary material.
With the application of the ﬁxed-point-iteration  we can solve the event captioning task without any
caption or segment during testing. We sample a random bunch of candidate segments {S(r)
for
the target video as initial guesses  and then perform the iteration in Eq. (4) on these candidates. After
sufﬁcient iterations  the outputs will converge to the ﬁxed-point solutions (i.e. the valid segments)
S∗. In our experiments  we only use one-round iteration by S(cid:48)
)) and ﬁnd it
i}N(cid:48)
sufﬁcient to deliver promising results. With the reﬁned segments {S(cid:48)
i=0 at hand  we are able to
generate the captions as {Ci = gθ2(V   S(cid:48)
As introduced afterward  both lθ1 and gθ2 are stacked by multiple neural layers which not naturally
satisfy the local-Lipschitz-continuity in Proposition 1. We thus apply the idea of denoising auto-
encoder in [31]  where we generate noisy data by adding a Gaussian noise to the training data
and minimize the reconstruction of the noisy data to the true ones. Explicitly  we enforce the
temporal segments around the true data to converge to the ﬁxed-point solutions by one-round iteration.
Recalling that for weakly supervised constraint  we do not have the ground-truth segments during
training  we thus apply lθ1(V   Ci) as the approximated segment  and minimize the following loss:

i=0 and thus solve the dense event captioning task.

i = lθ1 (V   gθ2(V   S(r)

i)}N(cid:48)

i }Nr

i

i

Ls = dist(lθ1(V   Ci)  lθ1 (V   gθ2(V   εi + lθ1 (V   Ci)))) 

(5)
where εi ∼ N(0  σ) is a Gaussian noise. The Gaussian smooth (Eq. (5)) does not theoretically hold
the Lipschitz continuity  but it practically enforces the random proposals to converge to the positive
segments as veriﬁed by our experiments.
By combining Eq.(3) and Eq.(5)  we obtain the hybrid loss as
L = Lc + λsLs 

(6)

where λs is the trade-off parameter.

4

vcs1l2g1lcvsfcsvc..'c's( ')( ')slossdistccdistssSMFfeaturefusionlocationregressoranchorpredictorGRUGRUGRUAmanisCaptionFeatureSoftMaskVideoFeatureattentionInitHiddenContextGRUGRUVideoFeatureattentionattention1( ):lvcssentencelocalization2( ):gvsccaptiongenerationGRU3.2 Network Design
The core of our framework as illustrated in Fig. 1 consists of a Sentence Localizer (i.e. lθ1 (V   C))
and a Caption Generator (i.e. gθ2(V   S)). Any differential model can be applied to formulate the
sentence localizer and caption generator. Here  we introduce the ones that we use. Besides  we omit
the RNN-based video and sentence feature extractors  leaving the details of them in the supplementary
material. In the following several paragraph  suppose we have obtained the features V = {vt ∈
Rk}Tv
t=0  hidden
states {h(c)
t=0 for each caption sentence. Tv and Tc are the lengths of the video and caption.
Sentence Localizer. Performing localization requires to model the correspondence between the video
and caption. We absorb the ideas from [29  28]  and propose a cross-attention multi-model feature
fusion framework. Here  we develop a novel attention mechanism named as Crossing Attention 
which contains two sub-attention computations. The ﬁrst one computes the attention between the
ﬁnal hidden state of the video and the caption feature at each time step  namely 

t=0 for each video  and the features C = {ct ∈ Rk}Tc

t=0  hidden states {h(v)

t ∈ Rk}Tv

t ∈ Rk}Tc

(7)
where ()T denotes the matrix transposition and Ac ∈ Rk×k is the learnable matrix. The other one is
to calculate the attention between the ﬁnal hidden state of the caption and the video features  i.e. 

f c = softmax((h(v)
Tv

)TAcC)CT

f v = softmax((h(c)
Tc

)TAvV )V T

(8)

where Av ∈ Rk×k is the learnable matrix.
Then  we apply the multi-model feature fusion layer in [28] to fuse two sub-attentions as

f cv = (f c + f v)(cid:107)(f c · f v)(cid:107)FC(f s(cid:107)f v) 

(9)
where · is the element-wise multiplication  FC(·) is a Fully-Connected (FC) layer  and (cid:107) denotes the
column-wise concatenation.
One can regress the temporal segment directly by adding an FC layer on the mixed feature f cv 
which however is easy to get suck in local minimums if the initial output is far away from the valid
segment. To allow our prediction to move between two distant locations efﬁciently  we ﬁrst relax the
regression problem to a classiﬁcation task. Particularly  we evenly divide the input video into multiple
anchor segments under multiple scales  and train a FC layer on thef cv to predict the best anchor
that produces the highest Meteor score [32] of the generated caption sentence. We then conduct
regression around the best anchor that gives the highest score. Formally  we attain

S =

S(a) + ∆S 

(10)

where S(a) is the best anchor segment and ∆S = (∆m  ∆w) are the regression output by performing
a FC layer on f cv.
Caption Generator. Given the temporal segment  we can perform captioning on the frames clipped
from the video. However  such clipping operation is non-differential  making it intractable for
end-to-end training. Here  we perform a soft clipping by deﬁning a continues mask function with
respect to the time t. This mask is deﬁned by

M (t  S) = Sig(−K(t − m + w/2)) − Sig(−K(t − m − w/2)) 

(11)
where S = (m  w) is the temporal segment  K is the scaling factor  and Sig(·) is the sigmoid function.
When K is large enough  the mask function becomes a step function whose value is zero exact for
the region [m − w/2  m + w/2]. The conventional mean-pooling feature of clipped frames are then
equal to the weighted sum of the video features by the mask after normalization  i.e. 

(cid:88)Tv

t=1

(cid:88)Tv

t=1

v(cid:48) =

M (t  S) · vt/

M (t  S).

(12)

Regarding v(cid:48) as context  and h(v)

m+w/2 as initial hidden state  RNN is applied to generate the caption:
{¯ct}Tc

t=1 = RN N (v(cid:48)  h(v)

m+w/2).

(13)

5

Loss Function The loss function in Eq. (6) contains two terms  Lc and Ls.
Lc is used to minimize the distance between the ground-truth C = {ci}Tc
¯C = {¯ci}Tc
i=0. We apply cross-enctropy loss as follow(or say  perplexity loss):

i=0 and our prediction

ct · log(¯ct|c0 : ct−1).

(14)

Lc = −(cid:88)Tc

t=1

Ls is applied to compare the difference between S = (m  w) and S(cid:48) = (m(cid:48)  w(cid:48)) as illustrated in
Fig. 1  which is implemented by the (cid:96)2 norm as

Ls = (m − m(cid:48))2 + (w − w(cid:48))2.

(15)

As metioned in Eq. (10)  we further train the sentence localizer to predict the best anchor segment
by adding a soft-max layer on the mixed feature f cv in Eq. (9). We deﬁne the one-hot label as
y = [y1 ···   yNa ] where yj = 1 if the j-th anchor segment is the best one  otherwise yj = 0.
Suppose our prediction output is p = [p1 ···   pNa ] by the soft-max layer. The classiﬁcation loss is
formulated as

La = −(cid:88)Na

yi log pi.

i=0

(16)

(17)

Taking all losses together  we have

L = Lc + λsLs + λaLa 

where λs and λa are constant parameters.

4 Experiments

We conduct experiments on the ActivityNet Captions[10] dataset that has been applied as the
benchmark for dense video captioning. This dataset contains 20 000 videos in total  covering a wide
range of complex human activities. For each video  the temporal segment and caption sentence
of each human event is annotated. On average  there are 3.65 events annotated among each video 
resulting in a total of 100 000 events. We follow the suggested protocol by [10  11] to use 50% of the
videos for training  25% for validation  and 25% for testing.
The vocabulary size for all text sentence is set to be 6000. As detailed in the supplementary material 
both the video and sentence encoders apply the GRU models[33] for feature extraction  where the
dimensions of hidden and output layers are 512. The trade-off parameters in our loss  i.e.  λs and
λa are both set to 0.1. We train our model by using the stochastic gradient descent with the initial
learning rate as 0.01 and momentum factor as 0.9. Our code is implemented by Pytorch-0.3.
Training. Under the weak supervision constraint  the ground truth temporal segments are unused
for training. The video itself is regarded as a special segment that is given by S(f ) = (0.5  1). We
ﬁrst pre-train the caption generator by using the entire video as input and each event caption among
it as output. Such a pretraining process allows us to learn a well-initialized caption generator since
the whole video content is related to the event caption  even the correlation is not precise. After the
pretraining  we train our model in 2 stages. In the ﬁrst stage  we minimize the captioning loss Lc
and reconstruction loss Ls. Then we minimize La in the second stage. Details about training are
provided in the Supplementary materials and our Github repository.
Testing. For testing  only input videos are available. As already discussed in § 3.1  We starts from a
random bunch of segments {S(r)
i=0 for initial guesses(Nr = 15 in our reported result). After the
one-round ﬁxed-point iteration  we obtain the reﬁned segments as {Si}Nr
i=0. We further ﬁlter them
based on the IoU between S(r)
and Si(More details are given in the supplemental material)  and keep
those having high IoU as valid proposals. We then input the ﬁltered segments to the caption generator
to obtain event captioning sentences. It’s nothing to mention that we do not choose using pretrained
temporal segment proposal model(e.g. SST][22]) for the initial temporal segment generation  which 
as a matter of fact  uses external temporal segment data  and is in contradiction with our motivation.

i }Nr

i

6

4.1 Evaluation of dense event captioning

Evaluation metric. The performance is measured with the commonly-used evaluation metrics:
METEOR [32]  CIDEr[23]  Rouge-L[34]  and Bleu@N[35]. We compute above metrics on the
proposals if their overlapping with the ground-truth segments is larger than a given tIoU3 threshold 
and set the score to be 0 otherwise. All scores are averaged with tIoU thresholds of 0.3  0.5  0.7 and
0.9 in our experiments. We use the ofﬁcial scripts4 for the score computation.
Baselines. Not any previous method is proposed for dense event captioning under the weak supervi-
sion. For Oracle comparisons  we still report the results by two fully-supervised methods [10  11]. As
for our method  we implement various variants to analysis the impact of each component. The ﬁrst
variant is the pretrained model where we randomly sample an event segment from each video and
feed it into the pretrained caption generator for captioning in the testing phase. Another variant is the
method by removing the anchor classiﬁcation in Eq. 10  and thus regressing the temporal coordinate
globally as in [29]. As a compliment  we also carry out the version by preserving the classiﬁcation
term but removing the regression component ∆S from Eq. 10.
Results. The event captioning results
are summarized in Table 1. In general 
the Meteor and Cider metrics are con-
sidered to be more convictive than other
scores: the Meteor score is highly cor-
related with human judgment  and has
been used as the ﬁnal ranking in the
ActivityNet challenge; while Cider is a
newly proposed metric where the repe-
tition of sentences is taken into account.
Our method reaches comparable perfor-
mance with the fully-supervised methods
regarding the Meteor score and obtains
the best score in terms of the Cider met-
ric. Such results are encouraging as our
method is weak supervised and not any
ground-truth segment is used. For the comparisons between different variants of our method  it is
observed that removing the anchor classiﬁcation or regression does decrease the accuracy  which
veriﬁes the necessity for each component in our model.
As we use a bunch of randomly selected temporal segments to generate the caption results  the
robustness of the model towards such random strategy should also be evaluated. We use a different
number of temporal segments and different random seeds to generate event caption sentences  and the
evaluation results are summarized in Table 3. From the table  we can see that the variance is small on
different random seeds. Besides  we can see a slight increase of performance along with the increase
of the number of temporal segments. We choose Nr = 15 as a trade-off between complexity and
performance in our ﬁnal experiment.
Moreover  we display the recalls of the detected events by various methods with respect to the testing
segments in Figure 2. To compute the recall  we assign the predicted segment as a positive sample
if its overlap with the testing segment is larger than the tIOU threshold. From Fig. 2  we can ﬁnd
that our model is much better than the random proposal model  which veriﬁed the power of our
weakly-supervised methods. Also  our ﬁnal model is better than the two baselines in general.
Illustrations. Figure. 3 illustrates event captioning results of two videos. It presents the ground-truth
descriptions  the captioning sentences by the pretrained model and our method. Compared with the
pretrained model which generates a single description for each video  our model is capable to generate
more accurate and detailed description. Compared to the ground truths  some of the descriptions are
comparable in consideration of the generated sentence and event temporal segment. However  two
issues still remain. One is that our model sometimes cannot capture the beginning of an event  which 
in our opinion  is due to the fact that we use the ﬁnal hidden state of a temporal segment to generate
description which does not rely much on the starting coordinate. Another is that our model tris to

Figure 2: Evaluation of the event detection.

3temporal Intersection over Union
4https://github.com/ranjaykrishna/densevid_eval

7

Table 1: Evaluation results of captioning. The term ws denotes "weak supervision" for short.

Model
Krishna’s[10]
Yao’s[11]
Pretrained
Ours (no classiﬁcation)
Ours (no regression)
Ours

ws
False
False
True
True
True
True

M
4.82
7.71
4.58
6.08
6.11
6.30

C

17.29
16.08
10.45
15.1
17.66
18.77

R
–

13.27
9.27
12.25
12.40
12.55

B@1 B@2 B@3 B@4
2.20
17.95
17.50
3.38
0.69
8.7
0.80
11.85
1.44
11.98
12.41
1.27

3.86
5.54
1.50
1.90
2.69
2.62

7.69
9.62
3.39
4.67
5.45
5.50

Table 2: Evaluation results of sentence localization. The term us denotes “unsupervised” for short.

Model
CTRL[28]
ABLR[29]
Full-supervised
Our Final

us
False
False
False
True

R@1  IoU 0.1 R@1  IoU 0.3 R@1  IoU 0.5

49.09
73.30
70.01
62.71

28.70
55.67
52.89
41.98

14.00
36.79
37.61
23.34

mIoU
20.54
36.99
40.36
28.23

generate 2 to 3 three descriptions most of the time  which means that it’s not good at capture all the
event in a video  especially those ones with many weeny events.

4.2 Evaluation of sentence localization

Using the learned caption localizer  our model can also be applied to the sentence localization task in
an unsupervised way. In this section  we provide experimental results to demonstrate the effectiveness
of our model on this task.
Evaluation metric. Following the works of [29  28]  we compute the "R@1  IoU=σ" and “mIoU”
scores to measure the model’s sentence localization ability. In details  for a given sentence and video
pair  the "R@1  IoU=σ" score indicates the percentage of sentences who’s top-1 predicted temporal
segment has a higher IoU with the ground truth temporal segment than the given threshold σ  while
the "mIoU" is the average tIoU between all top-1 prediction and ground truth temporal segment. In
our experiment  σ is set to 0.1  0.3 and 0.5 following the setting in [29].
Baselines. We compare our model’s sentence localization ability with Cross-modal Temporal
Regression Localizer (CTRL) [28] and Attention Based Location Regression (ABLR) [29]. Such two
models achieve the state-of-the-art performance for now. Besides the unsupervised model  we also
implement a fully-supervised version by using ground-truth segments.
Results Table. 2 shows the results of all compared methods. First  our supervised implementation
reaches similar performance as ABLR( the state-of-the-art) compared with another fully-supervised
baseline  thus indicating the effectiveness of our model. As for the unsupervised scenario  we can
see that our unsupervised model outperforms CTRL by a considerable margin  which shows that our
model can really learn to locate meaningful temporal segment from the indirect losses.

Table 3: Evaluation of model rebustness towards random temporal segments during testing(see
Sec. 4). We report the captioning evaluations on varying Nr. For each value of Nr  we run the
experiments over 5 trials  and obtain the results in the form of mean±std.

Nr
Nr = 10
Nr = 15
Nr = 20

M

6.13±0.03
6.29±0.01
6.34±0.01

C

17.75±0.12
18.65±0.14
19.14±0.05

B@1

12.10±0.06
12.39±0.04
12.52±0.02

B@2

5.33±0.05
5.49±0.02
5.57±0.02

B@3

2.50±0.03
2.58±0.03
2.61±0.02

B@4

1.20±0.01
1.23±0.02
1.25±0.02

8

5 Conclusion and Future Work

We raise a new task termed Weakly Supervised Dense Event Caption(WS-DEC) and propose an
efﬁcient method to tackle it. The weak supervision is of great importance as it eliminates the source-
consuming annotation of accurate temporal coordinates and encourages us to explore the huge amount
of videos in the wild. The proposed solution not only solves the task efﬁciently but also provides
an unsupervised method for sentence localization. Extensive experiments on both tasks verify the
effectiveness of our model. For future research  one potential direction is to verify our model by
performing experiments directly on Web videos. Meanwhile  since weakly supervised learning is
becoming an important research vein in the domain  our proposed method by using the cycle process
and ﬁxed-point iteration could be applied to more other tasks  e.g.  weakly-supervised detection.

Figure 3: Illustration of the generated dense event captions. Left is the ground truth  middle is the
generation of our pretrained caption model  and right is by our weakly-supervised training approach.
Different colors indicate different temporal segments and sentence descriptions.

Acknowledgements
This work was supported in part by National Program on Key Basic Research Project (No.
2015CB352300)  and National Natural Science Foundation of China Major Project (No. U1611461).

9

References
[1] Bernard Ghanem Fabian Caba Heilbron  Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-
scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 961–970  2015.

[2] Andrej Karpathy  George Toderici  Sanketh Shetty  Thomas Leung  Rahul Sukthankar  and Li Fei-Fei.

Large-scale video classiﬁcation with convolutional neural networks. In CVPR  2014.

[3] Will Kay  Joao Carreira  Karen Simonyan  Brian Zhang  Chloe Hillier  Sudheendra Vijayanarasimhan 
Fabio Viola  Tim Green  Trevor Back  Paul Natsev  et al. The kinetics human action video dataset. arXiv
preprint arXiv:1705.06950  2017.

[4] Subhashini Venugopalan  Huijuan Xu  Jeff Donahue  Marcus Rohrbach  Raymond Mooney  and Kate
Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint
arXiv:1412.4729  2014.

[5] Chiori Hori  Takaaki Hori  Teng-Yok Lee  Ziming Zhang  Bret Harsham  John R Hershey  Tim K Marks 
and Kazuhiko Sumi. Attention-based multimodal fusion for video description. In Computer Vision (ICCV) 
2017 IEEE International Conference on  pages 4203–4212. IEEE  2017.

[6] Jeffrey Donahue  Lisa Anne Hendricks  Sergio Guadarrama  Marcus Rohrbach  Subhashini Venugopalan 
Kate Saenko  and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and
description. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
2625–2634  2015.

[7] Subhashini Venugopalan  Marcus Rohrbach  Jeffrey Donahue  Raymond Mooney  Trevor Darrell  and Kate
Saenko. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on
computer vision  pages 4534–4542  2015.

[8] Huijuan Xu  Subhashini Venugopalan  Vasili Ramanishka  Marcus Rohrbach  and Kate Saenko. A

multi-scale multiple instance video description network. arXiv preprint arXiv:1505.05914  2015.

[9] Haonan Yu  Jiang Wang  Zhiheng Huang  Yi Yang  and Wei Xu. Video paragraph captioning using
hierarchical recurrent neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 4584–4593  2016.

[10] Ranjay Krishna  Kenji Hata  Frederic Ren  Li Fei-Fei  and Juan Carlos Niebles. Dense-captioning events
in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
706–715  2017.

[11] Ting Yao  Yehao Li  Zhaofan Qiu  Fuchen Long  Yingwei Pan  Dong Li  and Tao Mei. Msr asia msm at
activitynet challenge 2017: Trimmed action recognition  temporal action proposals and dense-captioning
events in videos.

[12] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in
videos. In Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors  Advances
in Neural Information Processing Systems 27  pages 568–576. Curran Associates  Inc.  2014.

[13] Limin Wang  Yuanjun Xiong  Zhe Wang  Yu Qiao  Dahua Lin  Xiaoou Tang  and Luc Van Gool. Temporal
segment networks: Towards good practices for deep action recognition. In European Conference on
Computer Vision  pages 20–36. Springer  2016.

[14] Tomáš Mikolov  Martin Karaﬁát  Lukáš Burget  Jan ˇCernock`y  and Sanjeev Khudanpur. Recurrent neural
network based language model. In Eleventh Annual Conference of the International Speech Communication
Association  2010.

[15] Zhiqiang Shen  Jianguo Li  Zhou Su  Minjun Li  Yurong Chen  Yu-Gang Jiang  and Xiangyang Xue.
Weakly supervised dense video captioning. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  volume 2  page 10  2017.

[16] Victor Escorcia  Fabian Caba Heilbron  Juan Carlos Niebles  and Bernard Ghanem. Daps: Deep action
proposals for action understanding. In European Conference on Computer Vision  pages 768–784. Springer 
2016.

[17] Ting Yao  Tao Mei  and Yong Rui. Highlight detection with pairwise deep ranking for ﬁrst-person video
summarization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 982–990  2016.

10

[18] Ting Yao  Yingwei Pan  Yehao Li  Zhaofan Qiu  and Tao Mei. Boosting image captioning with attributes.

OpenReview  2(5):8  2016.

[19] Yehao Li  Ting Yao  Yingwei Pan  Hongyang Chao  and Tao Mei. Jointly localizing and describing events

for dense video captioning. arXiv preprint arXiv:1804.08274  2018.

[20] Luowei Zhou  Yingbo Zhou  Jason J Corso  Richard Socher  and Caiming Xiong. End-to-end dense video

captioning with masked transformer. arXiv preprint arXiv:1804.00819  2018.

[21] Jingwen Wang  Wenhao Jiang  Lin Ma  Wei Liu  and Yong Xu. Bidirectional attentive fusion with context

gating for dense video captioning. arXiv preprint arXiv:1804.00100  2018.

[22] Shyamal Buch  Victor Escorcia  Chuanqi Shen  Bernard Ghanem  and Juan Carlos Niebles. Sst: Single-
stream temporal action proposals. In Computer Vision and Pattern Recognition (CVPR)  2017 IEEE
Conference on  pages 6373–6382. IEEE  2017.

[23] Ramakrishna Vedantam  C Lawrence Zitnick  and Devi Parikh. Cider: Consensus-based image description
evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
4566–4575  2015.

[24] Iftekhar Naim  Young Chol Song  Qiguang Liu  Henry A Kautz  Jiebo Luo  and Daniel Gildea. Unsu-
pervised alignment of natural language instructions with video segments. In AAAI  pages 1558–1564 
2014.

[25] Young Chol Song  Iftekhar Naim  Abdullah Al Mamun  Kaustubh Kulkarni  Parag Singla  Jiebo Luo 
Daniel Gildea  and Henry A Kautz. Unsupervised alignment of actions in video with text descriptions. In
IJCAI  pages 2025–2031  2016.

[26] Piotr Bojanowski  Rémi Lajugie  Edouard Grave  Francis Bach  Ivan Laptev  Jean Ponce  and Cordelia
In Computer Vision (ICCV)  2015 IEEE

Schmid. Weakly-supervised alignment of video with text.
International Conference on  pages 4462–4470. IEEE  2015.

[27] Jiyang Gao  Zhenheng Yang  Chen Sun  Kan Chen  and Ram Nevatia. Turn tap: Temporal unit regression

network for temporal action proposals. arXiv preprint arXiv:1703.06189  2017.

[28] Jiyang Gao  Chen Sun  Zhenheng Yang  and Ram Nevatia. Tall: Temporal activity localization via
language query. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 5267–5275  2017.

[29] Yitian Yuan  Tao Mei  and Wenwu Zhu. To ﬁnd where you talk: Temporal sentence localization in video

with attention based location regression. arXiv preprint arXiv:1804.07014  2018.

[30] CE Chidume. Iterative approximation of ﬁxed points of lipschitzian strictly pseudocontractive mappings.

Proceedings of the American Mathematical Society  99(2):283–288  1987.

[31] Pascal Vincent  Hugo Larochelle  Yoshua Bengio  and Pierre-Antoine Manzagol. Extracting and composing
robust features with denoising autoencoders. In Proceedings of the 25th international conference on
Machine learning  pages 1096–1103. ACM  2008.

[32] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation
measures for machine translation and/or summarization  pages 65–72  2005.

[33] Kyunghyun Cho  Bart Van Merriënboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares  Holger
Schwenk  and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078  2014.

[34] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics  page 605. Association for Computational Linguistics  2004.

[35] Kishore Papineni  Salim Roukos  Todd Ward  and Wei-Jing Zhu. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting on association for computational
linguistics  pages 311–318. Association for Computational Linguistics  2002.

11

,Xuguang Duan
Wenbing Huang
Chuang Gan
Jingdong Wang
Wenwu Zhu
Junzhou Huang
Enrique Fita Sanmartin
Sebastian Damrich
Fred Hamprecht