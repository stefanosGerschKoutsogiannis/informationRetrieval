2014,SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives,In this work we introduce a new fast incremental gradient method SAGA  in the spirit of SAG  SDCA  MISO and SVRG. SAGA improves on the theory behind SAG and SVRG  with better theoretical convergence rates  and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA  SAGA supports non-strongly convex problems directly  and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.,SAGA: A Fast Incremental Gradient Method With

Support for Non-Strongly Convex Composite

Objectives

Aaron Defazio

Ambiata ∗

Australian National University  Canberra

Francis Bach

INRIA - Sierra Project-Team

´Ecole Normale Sup´erieure  Paris  France

Simon Lacoste-Julien

INRIA - Sierra Project-Team

´Ecole Normale Sup´erieure  Paris  France

Abstract

In this work we introduce a new optimisation method called SAGA in the spirit of
SAG  SDCA  MISO and SVRG  a set of recently proposed incremental gradient
algorithms with fast linear convergence rates. SAGA improves on the theory be-
hind SAG and SVRG  with better theoretical convergence rates  and has support
for composite objectives where a proximal operator is used on the regulariser. Un-
like SDCA  SAGA supports non-strongly convex problems directly  and is adap-
tive to any inherent strong convexity of the problem. We give experimental results
showing the effectiveness of our method.

Introduction

1
Remarkably  recent advances [1  2] have shown that it is possible to minimise strongly convex
ﬁnite sums provably faster in expectation than is possible without the ﬁnite sum structure. This is
signiﬁcant for machine learning problems as a ﬁnite sum structure is common in the empirical risk
minimisation setting. The requirement of strong convexity is likewise satisﬁed in machine learning
problems in the typical case where a quadratic regulariser is used.
In particular  we are interested in minimising functions of the form

n(cid:88)

i=1

f (x) =

1
n

fi(x) 

where x ∈ Rd  each fi is convex and has Lipschitz continuous derivatives with constant L. We will
also consider the case where each fi is strongly convex with constant µ  and the “composite” (or
proximal) case where an additional regularisation function is added:

F (x) = f (x) + h(x) 

where h : Rd → Rd is convex but potentially non-differentiable  and where the proximal operation
of h is easy to compute — few incremental gradient methods are applicable in this setting [3][4].
Our contributions are as follows. In Section 2 we describe the SAGA algorithm  a novel incremental
gradient method. In Section 5 we prove theoretical convergence rates for SAGA in the strongly
convex case better than those for SAG [1] and SVRG [5]  and a factor of 2 from the SDCA [2]
convergence rates. These rates also hold in the composite setting. Additionally  we show that
∗The ﬁrst author completed this work while under funding from NICTA. This work was partially supported

by the MSR-Inria Joint Centre and a grant by the European Research Council (SIERRA project 239993).

1

like SAG but unlike SDCA  our method is applicable to non-strongly convex problems without
modiﬁcation. We establish theoretical convergence rates for this case also. In Section 3 we discuss
the relation between each of the fast incremental gradient methods  showing that each stems from a
very small modiﬁcation of another.
2 SAGA Algorithm
We start with some known initial vector x0 ∈ Rd and known derivatives f(cid:48)
i = x0
for each i. These derivatives are stored in a table data-structure of length n  or alternatively a n × d
matrix. For many problems of interest  such as binary classiﬁcation and least-squares  only a single
ﬂoating point value instead of a full gradient vector needs to be stored (see Section 4). SAGA is
inspired both from SAG [1] and SVRG [5] (as we will discuss in Section 3). SAGA uses a step size
of γ and makes the following updates  starting with k = 0:
SAGA Algorithm: Given the value of xk and of each f(cid:48)
for iteration k + 1 is as follows:

i ) at the end of iteration k  the updates

i ) ∈ Rd with φ0

i (φk

i (φ0

) in the table. All other entries in the table remain

1. Pick a j uniformly at random.
j = xk  and store f(cid:48)
2. Take φk+1
unchanged. The quantity φk+1
)  f(cid:48)

3. Update x using f(cid:48)

j(φk+1

j

j

j

j(φk+1
is not explicitly stored.
j ) and the table average:

j(φk

(cid:34)

wk+1 = xk − γ

j

j(φk

f(cid:48)
j(φk+1

) − f(cid:48)
(cid:0)wk+1(cid:1) .
xk+1 = proxh
γ

j ) +

(cid:35)

 

f(cid:48)
i (φk
i )

n(cid:88)

i=1

1
n

(1)

(2)

(3)

(cid:26)

(cid:27)

.

1
2γ (cid:107)x − y(cid:107)2
(cid:2)f (x0) −(cid:10)f

The proximal operator we use above is deﬁned as

proxh

γ (y) := argmin

x∈Rd

h(x) +

In the strongly convex case  when a step size of γ = 1/(2(µn+L)) is chosen  we have the following
convergence rate in the composite and hence also the non-composite case:
(cid:48)

n

µ

∗

(cid:18)

(cid:19)k(cid:20)(cid:13)(cid:13)x0 − x
∗(cid:13)(cid:13)2

E(cid:13)(cid:13)(cid:13)xk − x

∗(cid:13)(cid:13)(cid:13)2 ≤

∗(cid:11) − f (x

∗
(x

)  x0 − x

)(cid:3)(cid:21)

.

1 −

+

µn + L

2(µn + L)

µ

We prove this result in Section 5. The requirement of strong convexity can be relaxed from needing
to hold for each fi to just holding on average  but at the expense of a worse geometric rate (1 −
6(µn+L) )  requiring a step size of γ = 1/(3(µn + L)).
In the non-strongly convex case  we have established the convergence rate in terms of the average
iterate  excluding step 0: ¯xk = 1
k
4n
k

t=1 xt. Using a step size of γ = 1/(3L) we have

(cid:80)k
(cid:20) 2L

+ f (x0) −

(cid:21)

n

This result is proved in the supplementary material. Importantly  when this step size γ = 1/(3L) is
used  our algorithm automatically adapts to the level of strong convexity µ > 0 naturally present 
giving a convergence rate of (see the comment at the end of the proof of Theorem 1):
)  x0 − x

)(cid:3)(cid:21)

E(cid:13)(cid:13)(cid:13)xk − x

+

∗

 

.

E(cid:2)F (¯xk)(cid:3)
− F (x∗) ≤
(cid:26) 1
(cid:18)
∗(cid:13)(cid:13)(cid:13)2 ≤

1 − min

(cid:10)f(cid:48)(x∗)  x0 − x∗(cid:11)
(cid:2)f (x0) −(cid:10)f

(x

∗

(cid:48)

(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)2
(cid:27)(cid:19)k(cid:20)(cid:13)(cid:13)x0 − x
∗(cid:13)(cid:13)2

.

− f (x∗)
∗(cid:11) − f (x

µ
3L

4n

2n
3L

Although any incremental gradient method can be applied to non-strongly convex problems via the
addition of a small quadratic regularisation  the amount of regularisation is an additional tunable
parameter which our method avoids.
3 Related Work
We explore the relationship between SAGA and the other fast incremental gradient methods in this
section. By using SAGA as a midpoint  we are able to provide a more uniﬁed view than is available
in the existing literature. A brief summary of the properties of each method considered in this section
is given in Figure 1. The method from [3]  which handles the non-composite setting  is not listed as
its rate is of the slow type and can be up to n times smaller than the one for SAGA or SVRG [5].

2

Strongly Convex (SC)

Convex  Non-SC*

Prox Reg.
Non-smooth

Low Storage Cost
Simple(-ish) Proof

Adaptive to SC

SAGA









SAG


?





SDCA




[6]





SVRG


?




?

FINITO


?




?

Figure 1: Basic summary of method properties. Question marks denote unproven  but not experimentally
ruled out cases. (*) Note that any method can be applied to non-strongly convex problems by adding a small
amount of L2 regularisation  this row describes methods that do not require this trick.

SAGA: midpoint between SAG and SVRG/S2GD

In [5]  the authors make the observation that the variance of the standard stochastic gradient (SGD)
update direction can only go to zero if decreasing step sizes are used  thus preventing a linear conver-
gence rate unlike for batch gradient descent. They thus propose to use a variance reduction approach
(see [7] and references therein for example) on the SGD update in order to be able to use constant
step sizes and get a linear convergence rate. We present the updates of their method called SVRG
(Stochastic Variance Reduced Gradient) in (6) below  comparing it with the non-composite form
of SAGA rewritten in (5). They also mention that SAG (Stochastic Average Gradient) [1] can be
interpreted as reducing the variance  though they do not provide the speciﬁcs. Here  we make this
connection clearer and relate it to SAGA.
We ﬁrst review a slightly more generalized version of the variance reduction approach (we allow the
updates to be biased). Suppose that we want to use Monte Carlo samples to estimate EX and that
we can compute efﬁciently EY for another random variable Y that is highly correlated with X. One
variance reduction approach is to use the following estimator θα as an approximation to EX: θα :=
α(X−Y )+EY   for a step size α ∈ [0  1]. We have that Eθα is a convex combination of EX and EY :
Eθα = αEX + (1 − α)EY . The standard variance reduction approach uses α = 1 and the estimate
is unbiased Eθ1 = EX. The variance of θα is: Var(θα) = α2[Var(X) + Var(Y ) − 2 Cov(X  Y )] 
and so if Cov(X  Y ) is big enough  the variance of θα is reduced compared to X  giving the method
its name. By varying α from 0 to 1  we increase the variance of θα towards its maximum value
(which usually is still smaller than the one for X) while decreasing its bias towards zero.
Both SAGA and SAG can be derived from such a variance reduction viewpoint: here X is the SGD
direction sample f(cid:48)
j ). SAG is obtained by using
α = 1/n (update rewritten in our notation in (4))  whereas SAGA is the unbiased version with α = 1
(see (5) below). For the same φ’s  the variance of the SAG update is 1/n2 times the one of SAGA 
but at the expense of having a non-zero bias. This non-zero bias might explain the complexity of
the convergence proof of SAG and why the theory has not yet been extended to proximal operators.
By using an unbiased update in SAGA  we are able to obtain a simple and tight theory  with better
constants than SAG  as well as theoretical rates for the use of proximal operators.

j(xk)  whereas Y is a past stored gradient f(cid:48)

j(φk

(cid:34)
(cid:34)
(cid:34)

f(cid:48)
j(xk) − f(cid:48)

j(φk
j )

n

+

j(φk

j ) +

j(xk) − f(cid:48)
f(cid:48)
f(cid:48)
j(xk) − f(cid:48)

(cid:35)
(cid:35)

f(cid:48)
i (φk
i )

 

f(cid:48)
i (φk
i )

 

(cid:35)

1
n

i=1

n(cid:88)
n(cid:88)
n(cid:88)

i=1

1
n

i=1

j(˜x) +

1
n

f(cid:48)
i (˜x)

.

(SAG)

xk+1 = xk − γ

(SAGA)

xk+1 = xk − γ

(SVRG)

xk+1 = xk − γ

(4)

(5)

(6)

The SVRG update (6) is obtained by using Y = f(cid:48)
j(˜x) with α = 1 (and is thus unbiased – we note
that SAG is the only method that we present in the related work that has a biased update direction).
The vector ˜x is not updated every step  but rather the loop over k appears inside an outer loop  where
˜x is updated at the start of each outer iteration. Essentially SAGA is at the midpoint between SVRG
and SAG; it updates the φj value each time index j is picked  whereas SVRG updates all of φ’s as
a batch. The S2GD method [8] has the same update as SVRG  just differing in how the number of
inner loop iterations is chosen. We use SVRG henceforth to refer to both methods.

3

SVRG makes a trade-off between time and space. For the equivalent practical convergence rate it
makes 2x-3x more gradient evaluations  but in doing so it does not need to store a table of gradients 
but a single average gradient. The usage of SAG vs. SVRG is problem dependent. For example for
linear predictors where gradients can be stored as a reduced vector of dimension p − 1 for p classes 
SAGA is preferred over SVRG both theoretically and in practice. For neural networks  where no
theory is available for either method  the storage of gradients is generally more expensive than the
additional backpropagations  but this is computer architecture dependent.
SVRG also has an additional parameter besides step size that needs to be set  namely the number of
iterations per inner loop (m). This parameter can be set via the theory  or conservatively as m = n 
however doing so does not give anywhere near the best practical performance. Having to tune one
parameter instead of two is a practical advantage for SAGA.

Finito/MISOµ

u0 := x0 + γ(cid:80)n

i=1 f(cid:48)

To make the relationship with other prior methods more apparent  we can rewrite the SAGA
algorithm (in the non-composite case) in term of an additional intermediate quantity uk  with

i (x0)  in addition to the usual xk iterate as described previously:

SAGA: Equivalent reformulation for non-composite case: Given the value of uk and of each
f(cid:48)
i (φk

i ) at the end of iteration k  the updates for iteration k + 1  is as follows:
1. Calculate xk:

(7)

n(cid:88)

i=1

f(cid:48)
i (φk

i ).

xk = uk − γ
n (xk − uk).

2. Update u with uk+1 = uk + 1
3. Pick a j uniformly at random.
j = xk  and store f(cid:48)
4. Take φk+1

j(φk+1

j

) in the table replacing f(cid:48)

j(φk

j ). All other entries in

the table remain unchanged. The quantity φk+1

j

is not explicitly stored.

Eliminating uk recovers the update (5) for xk. We now describe how the Finito [9] and MISOµ [10]
methods are closely related to SAGA. Both Finito and MISOµ use updates of the following form 
for a step length γ:

(cid:88)

i

1
n

n(cid:88)

i=1

xk+1 =

φk
i − γ

f(cid:48)
i (φk

i ).

(8)

(cid:80)
E(cid:2) ¯φk+1(cid:3) = E

(cid:20)

The step size used is of the order of 1/µn. To simplify the discussion of this algorithm we will
introduce the notation ¯φ = 1
n
SAGA can be interpreted as Finito  but with the quantity ¯φ replaced with u  which is updated in the
same way as ¯φ  but in expectation. To see this  consider how ¯φ changes in expectation:

i φk
i .

(cid:1)(cid:21)

(cid:0)xk − φk

j

1
n

¯φk +

(cid:0)xk − ¯φk(cid:1) .

= ¯φk +

1
n

The update is identical in expectation to the update for u  uk+1 = uk + 1
n (xk − uk). There are
three advantages of SAGA over Finito/MISOµ. SAGA does not require strong convexity to work 
it has support for proximal operators  and it does not require storing the φi values. MISO has
proven support for proximal operators only in the case where impractically small step sizes are
used [10]. The big advantage of Finito/MISOµ is that when using a per-pass re-permuted access
ordering  empirical speed-ups of up-to a factor of 2x has been observed. This access order can also
be used with the other methods discussed  but with smaller empirical speed-ups. Finito/MISOµ is
particularly useful when fi is computationally expensive to compute compared to the extra storage
costs required over the other methods.

SDCA

The Stochastic Dual Coordinate Descent (SDCA) [2] method on the surface appears quite different
from the other methods considered. It works with the convex conjugates of the fi functions. How-
ever  in this section we show a novel transformation of SDCA into an equivalent method that only
works with primal quantities  and is closely related to the MISOµ method.

4

Consider the following algorithm:

SDCA algorithm in the primal
Step k + 1:

γ (z)  where γ = 1

1. Pick an index j uniformly at random.
2. Compute φk+1
3. Store the gradient f(cid:48)

j = proxfj

j(φk+1

) = 1
γ
table entries are unchanged (f(cid:48)
i f(cid:48)

At completion  return xk = −γ(cid:80)n

i (φk+1

i (φk

i ) .

j

i

(cid:0)z − φk+1

) = f(cid:48)

j

µn and z = −γ(cid:80)n
(cid:1) in the table at location j. For i (cid:54)= j  the

i(cid:54)=j f(cid:48)

i (φk

i ).

i (φk

i )).

j

.

αi

µn

i=1

i=1

1
n

−

µ
2

(cid:1)

(cid:0)

D(α) =

n(cid:88)

n(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

  

αk+1
j = αk

f∗
i (−αi)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

j + argmax
∆aj∈Rd

(cid:13)(cid:13)(cid:13)(cid:13)xk +

We claim that this algorithm is equivalent to the version of SDCA where exact block-coordinate
maximisation is used on the dual.1 Firstly  note that while SDCA was originally described for one-
dimensional outputs (binary classiﬁcation or regression)  it has been expanded to cover the multi-
class predictor case [11] (called Prox-SDCA there). In this case  the primal objective has a separate
strongly convex regulariser  and the functions fi are restricted to the form fi(x) := ψi(X T
i x)  where
Xi is a d×p feature matrix  and ψi is the loss function that takes a p dimensional input  for p classes.
To stay in the same general setting as the other incremental gradient methods  we work directly with
the fi(x) functions rather than the more structured ψi(X T
i x). The dual objective to maximise then
becomes

where αi’s are d-dimensional dual variables. Generalising the exact block-coordinate maximisation
update that SDCA performs to this form  we get the dual update for block j (with xk the current
primal iterate):

−
(cid:40)
−f∗
In the special case where fi(x) = ψi(X T
i x)  we can see that (9) gives exactly the same update as
Option I of Prox-SDCA in [11  Figure 1]  which operates instead on the equivalent p-dimensional
dual variables ˜αi with the relationship that αi = Xi ˜αi.2 As noted by Shalev-Shwartz & Zhang [11] 
the update (9) is actually an instance of the proximal operator of the convex conjugate of fj. Our
primal formulation exploits this fact by using a relation between the proximal operator of a function
and its convex conjugate known as the Moreau decomposition:
(v) = v − proxf (v).

This decomposition allows us to compute the proximal operator of the conjugate via the primal
proximal operator. As this is the only use in the basic SDCA method of the conjugate function 
applying this decomposition allows us to completely eliminate the “dual” aspect of the algorithm 
then xk = γ(cid:80)
yielding the above primal form of SDCA. The dual variables are related to the primal representa-
tives φi’s through αi = −f(cid:48)
i (φi). The KKT conditions ensure that if the αi values are dual optimal
i αi as deﬁned above is primal optimal. The same trick is commonly used to in-
terpret Dijkstra’s set intersection as a primal algorithm instead of a dual block coordinate descent
algorithm [12].
The primal form of SDCA differs from the other incremental gradient methods described in this
section in that it assumes strong convexity is induced by a separate strongly convex regulariser 
rather than each fi being strongly convex. In fact  SDCA can be modiﬁed to work without a separate
regulariser  giving a method that is at the midpoint between Finito and SDCA. We detail such a
method in the supplementary material.

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

j − ∆αj

proxf∗

−αk

µn
2

1
µn

∆αj

(9)

−

method as “SDCA” in this paper for brevity.

1More precisely  to Option I of Prox-SDCA as described in [11  Figure 1]. We will simply refer to this
2This is because f∗

ψ∗
i ( ˜αi).

i (αi) =

inf

˜αi s.t. αi=Xi ˜αi

5

SDCA variants

j

µn

i (φk

i f(cid:48)

(cid:80)

is chosen so that: f(cid:48)

j )+βf(cid:48)
i ). The variants differ in how β ∈ [0  1] is chosen. Note that φk+1

The SDCA theory has been expanded to cover a number of other methods of performing the coor-
dinate step [11]. These variants replace the proximal operation in our primal interpretation in the
previous section with an update where φk+1
j(xk) 
where xk = − 1
does
not actually have to be explicitly known  just the gradient f(cid:48)
)  which is the result of the above
interpolation. Variant 5 by Shalev-Shwartz & Zhang [11] does not require operations on the conju-
gate function  it simply uses β = µn
L+µn. The most practical variant performs a line search involving
the convex conjugate to determine β. As far as we are aware  there is no simple primal equivalent
of this line search. So in cases where we can not compute the proximal operator from the standard
SDCA variant  we can either introduce a tuneable parameter into the algorithm (β)  or use a dual
line search  which requires an efﬁcient way to evaluate the convex conjugates of each fi.

) = (1−β)f(cid:48)

j(φk+1

j(φk+1

j(φk

j

j

j

Implementation

4
We brieﬂy discuss some implementation concerns:
• For many problems each derivative f(cid:48)
i is just a simple weighting of the ith data vector.
Logistic regression and least squares have this property. In such cases  instead of storing
the full derivative f(cid:48)
i for each i  we need only to store the weighting constants. This reduces
the storage requirements to be the same as the SDCA method in practice. A similar trick
can be applied to multi-class classiﬁers with p classes by storing p − 1 values for each i.
• Our algorithm assumes that initial gradients are known for each fi at the starting point x0.
Instead  a heuristic may be used where during the ﬁrst pass  data-points are introduced one-
by-one  in a non-randomized order  with averages computed in terms of those data-points
processed so far. This procedure has been successfully used with SAG [1].

• The SAGA update as stated is slower than necessary when derivatives are sparse. A just-in-
time updating of u or x may be performed just as is suggested for SAG [1]  which ensures
that only sparse updates are done at each iteration.

• We give the form of SAGA for the case where each fi is strongly convex. However in
practice we usually have only convex fi  with strong convexity in f induced by the addition
of a quadratic regulariser. This quadratic regulariser may be split amongst the fi functions
evenly  to satisfy our assumptions. It is perhaps easier to use a variant of SAGA where the
regulariser µ

2||x||2 is explicit  such as the following modiﬁcation of Equation (5):
xk+1 = (1 − γµ) xk − γ

f(cid:48)
j(xk) − f(cid:48)

f(cid:48)
i (φk
i )

(cid:88)

j(φk

j ) +

(cid:34)

(cid:35)

1
n

.

i

For sparse implementations instead of scaling xk at each step  a separate scaling constant
βk may be scaled instead  with βkxk being used in place of xk. This is a standard trick
used with stochastic gradient methods.

For sparse problems with a quadratic regulariser the just-in-time updating can be a little intricate. In
the supplementary material we provide example python code showing a correct implementation that
uses each of the above tricks.
5 Theory
In this section  all expectations are taken with respect to the choice of j at iteration k + 1 and
conditioned on xk and each f(cid:48)
We start with two basic lemmas that just state properties of convex functions  followed by Lemma 1 
which is speciﬁc to our algorithm. The proofs of each of these lemmas is in the supplementary
material.
Lemma 1. Let f (x) = 1
continuous gradients with constant L. Then for all x and x∗:
n
2 (cid:107)x∗
µ

i=1 fi(x). Suppose each fi is µ-strongly convex and has Lipschitz

[f (x∗) − f (x)] −

i ) unless stated otherwise.

(cid:104)f(cid:48)(x)  x∗

(cid:80)n

− x(cid:105) ≤

− x(cid:107)2

L − µ

i (φk

L

6

(cid:88)

Lemma 2. We have that for all φi and x∗:
1
n

1
n

i

(cid:107)f(cid:48)
i (φi) − f(cid:48)
E(cid:13)(cid:13)(cid:13)wk+1 − xk − γf

(x

∗

)

(cid:48)

Lemma 3. It holds that for any φk

i

i

1

µ

−

1
n

(cid:105) .

2Ln

(cid:35)

i (x)(cid:107)2 −

L (cid:104)f(cid:48)(x∗)  x − x∗
(cid:88)
(cid:104)f(cid:48)
i (x∗)  φi − x∗
(cid:105)
+ γ2(1 + β)E(cid:13)(cid:13)(cid:13)f
i   x∗  xk and β > 0  with wk+1 as deﬁned in Equation 1:
j(xk) − f
(cid:48)
(cid:48)
∗
j(x

(cid:88)
(cid:107)f(cid:48)
i (x∗) − f(cid:48)
(cid:34)
(cid:88)
i (x∗)(cid:107)2 ≤ 2L
(cid:13)(cid:13)(cid:13)2 ≤ γ2(1 + β
−1)E(cid:13)(cid:13)(cid:13)f
(cid:13)(cid:13)(cid:13)f
(cid:88)
i ) − f (x∗) −
2γ(1−γµ)n   and κ = 1

fi(φi) − f (x∗) −
(cid:13)(cid:13)(cid:13)2
(cid:10)f(cid:48)

(cid:13)(cid:13)(cid:13)2
i − x∗(cid:11) + c(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2

γµ   we have the following expected change in the

j ) − f
∗
(x

i (x∗)  φk

(cid:48)
j(φk
(cid:48)

(cid:88)

(xk) − f

(cid:13)(cid:13)(cid:13)2

− γ2β

(cid:48)
∗
j(x

1
n

1
n

)

)

)

1

.

.

.

(cid:48)

i

Theorem 1. With x∗ the optimal solution  deﬁne the Lyapunov function T as:
T k := T (xk {φk
Then with γ =
Lyapunov function between steps of the SAGA algorithm (conditional on T k):

2(µn+L)   c =

i=1) :=

i }n

fi(φk

1

i

i

1
κ

)T k.

E[T k+1] ≤ (1 −
(cid:18)

i

i

i

i

i

)

n

n

=

E

E

−

−

i ).

1
n

1
n

1
n

1
n

1
n

1
n

(cid:34)

(cid:34)

1−

1 −

fi(φk

= −

f (xk) +

fi(φk+1

(cid:10)f(cid:48)

(cid:88)

i (x∗)  φk

i (x∗)  φk+1

i − x∗(cid:11) .

Proof. The ﬁrst three terms in T k+1 are straight-forward to simplify:

For the change in the last term of T k+1  we apply the non-expansiveness of the proximal operator3:

We expand the quadratic and apply E[wk+1] = xk − γf(cid:48)(xk) to simplify the inner product term:

(cid:35)
(cid:19) 1
(cid:88)
(cid:88)
i − x∗(cid:11)(cid:35)
(cid:19) 1
(cid:18)
(cid:88)
(cid:10)f(cid:48)
(cid:10)f(cid:48)(x∗)  xk − x∗(cid:11)
c(cid:13)(cid:13)xk+1 − x∗(cid:13)(cid:13)2
− γf(cid:48)(x∗))(cid:13)(cid:13)2
= c(cid:13)(cid:13)proxγ(wk+1) − proxγ(x∗
≤ c(cid:13)(cid:13)wk+1 − x∗ + γf(cid:48)(x∗)(cid:13)(cid:13)2
cE(cid:13)(cid:13)wk+1 − x∗ + γf(cid:48)(x∗)(cid:13)(cid:13)2
= cE(cid:13)(cid:13)xk − x∗ + wk+1 − xk + γf(cid:48)(x∗)(cid:13)(cid:13)2
= c(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
+ 2cE(cid:2)(cid:10)wk+1 − xk + γf(cid:48)(x∗)  xk − x∗(cid:11)(cid:3) + cE(cid:13)(cid:13)wk+1 − xk + γf(cid:48)(x∗)(cid:13)(cid:13)2
− 2cγ(cid:10)f(cid:48)(xk) − f(cid:48)(x∗)  xk − x∗(cid:11) + cE(cid:13)(cid:13)wk+1 − xk + γf(cid:48)(x∗)(cid:13)(cid:13)2
= c(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
≤ c(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
− cγ2β(cid:13)(cid:13)f(cid:48)(xk) − f(cid:48)(x∗)(cid:13)(cid:13)2
− 2cγ(cid:10)f(cid:48)(xk)  xk − x∗(cid:11) + 2cγ(cid:10)f(cid:48)(x∗)  xk − x∗(cid:11)
+(cid:0)1 + β−1(cid:1) cγ2E(cid:13)(cid:13)f(cid:48)
j(x∗)(cid:13)(cid:13)2
+ (1 + β) cγ2E(cid:13)(cid:13)f(cid:48)
j(x∗)(cid:13)(cid:13)2
The value of β shall be ﬁxed later. Now we apply Lemma 1 to bound −2cγ(cid:10)f(cid:48)(xk)  xk − x∗(cid:11) and
j ) − f(cid:48)
j(xk) − f(cid:48)
Lemma 2 to bound E(cid:13)(cid:13)f(cid:48)
j(x∗)(cid:13)(cid:13)2:
(cid:17) E(cid:13)(cid:13)f(cid:48)
(cid:16)
cE(cid:13)(cid:13)xk+1 − x∗(cid:13)(cid:13)2
≤ (c − cγµ)(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
j ) − f(cid:48)
j(xk) − f(cid:48)
− cγ2β(cid:13)(cid:13)f(cid:48)(xk) − f(cid:48)(x∗)(cid:13)(cid:13)2
(cid:10)f(cid:48)(x∗)  xk − x∗(cid:11)(cid:3)
(cid:2)f (xk) − f (x∗) −
(cid:34)
(cid:88)
(cid:88)
(cid:10)f(cid:48)
+ 2(cid:0)1 + β−1(cid:1) cγ2L

j(x∗)(cid:13)(cid:13)2
i − x∗(cid:11)(cid:35)

i ) − f (x∗) −

(1 + β)cγ2 −

2cγ(L − µ)

i (x∗)  φk

(Lemma 3)

fi(φk

j(φk

j(φk

cγ
L

1
n

1
n

−

+

L

.

.

.

i

i

3Note that the ﬁrst equality below is the only place in the proof where we use the fact that x∗ is an optimality

point.

7

y
t
i
l
a
m

i
t
p
o
-
b
u
s

n
o
i
t
c
n
u
F

Gradient evaluations / n

Figure 2: From left to right we have the MNIST  COVTYPE  IJCNN1 and MILLIONSONG datasets. Top
row is the L2 regularised case  bottom row the L1 regularised case.

−2µ(cid:2)f (xk) − f (x∗) −

We can now combine the bounds that we have derived for each term in T   and pull out a frac-
tion 1
≤

(cid:13)(cid:13)f(cid:48)(xk) − f(cid:48)(x∗)(cid:13)(cid:13)2
(cid:10)f(cid:48)(x∗)  xk − x∗(cid:11)(cid:3) [13  Thm. 2.1.10]  that yields:
κ of T k (for any κ at this point). Together with the inequality −
(cid:18) 1
∗(cid:69)(cid:105)
) −(cid:68)
∗(cid:69)(cid:35)
(cid:68)
(cid:88)
(cid:13)(cid:13)(cid:13)2

− 2cγ(L − µ)
(cid:19)(cid:34)
(cid:88)
(cid:18)

E[T k+1] − T k ≤ − 1
κ

(cid:13)(cid:13)(cid:13)xk − x

cγE(cid:13)(cid:13)(cid:13)f

−1)cγ2L − 1
n

(cid:18) 1
(cid:18) 1

f (xk) − f (x
∗

j(xk) − f
(cid:48)

− 2cγ2µβ

∗(cid:13)(cid:13)(cid:13)2

i ) − f (x

)  xk − x

) − 1
n

(cid:19)(cid:104)

(1 + β)γ − 1
L

i − x

+ 2(1 + β

− γµ

(cid:48)
∗
i (x

fi(φk

(cid:19)

(cid:19)

∗

(cid:48)
j(x

T k +

)  φk

(10)

(cid:48)

f

(x

∗

)

.

κ

κ

+

+

1
n

i

n

L

f

i

∗

c

+

Note that each of the terms in square brackets are positive  and it can be readily veriﬁed that our
γµ)  together with
assumed values for the constants (γ =
β = 2µn+L
ensure that each of the quantities in round brackets are non-positive (the constants were
determined by setting all the round brackets to zero except the second one — see [14] for the details).
Adaptivity to strong convexity result: Note that when using the γ = 1
3L step size  the same c as
above can be used with β = 2 and 1

(cid:9) to ensure non-positive terms.

2γ(1−γµ)n  and κ = 1

2(µn+L)  c =

L

1

1

4n   µ

3L

κ = min(cid:8) 1
(cid:19)k(cid:20)(cid:13)(cid:13)x0 − x
∗(cid:13)(cid:13)2

Corollary 1. Note that c(cid:13)(cid:13)xk − x∗(cid:13)(cid:13)2
(cid:20)(cid:13)(cid:13)(cid:13)xk − x
∗(cid:13)(cid:13)(cid:13)2(cid:21)

(cid:18)

1 −

≤

2(µn + L)

µ

in the constants explicitly and using µ(n − 0.5) ≤ µn to simplify the expression  we get:
E

(cid:2)f (x0) −(cid:10)f

)  x0 − x

∗(cid:11) − f (x

≤ T k  and therefore by chaining the expectations  plugging

)(cid:3)(cid:21)

(x

+

n

∗

∗

(cid:48)

.

µn + L

Here the expectation is over all choices of index jk up to step k.

6 Experiments
We performed a series of experiments to validate the effectiveness of SAGA. We tested a binary
classiﬁer on MNIST  COVTYPE  IJCNN1 and a least squares predictor on MILLIONSONG. Details
of these datasets can be found in [9]. We used the same code base for each method  just changing the
main update rule. SVRG was tested with the recalibration pass used every n iterations  as suggested
in [8]. Each method had its step size parameter chosen so as to give the fastest convergence.
We tested with a L2 regulariser  which all methods support  and with a L1 regulariser on a subset
of the methods. The results are shown in Figure 2. We can see that Finito (perm) performs the
best on a per epoch equivalent basis  but it can be the most expensive method per step. SVRG is
similarly fast on a per epoch basis  but when considering the number of gradient evaluations per
epoch is double that of the other methods for this problem  it is middle of the pack. SAGA can be
seen to perform similar to the non-permuted Finito case  and to SDCA. Note that SAG is slower
than the other methods at the beginning. To get the optimal results for SAG  an adaptive step size
rule needs to be used rather than the constant step size we used. In general  these tests conﬁrm that
the choice of methods should be done based on their properties as discussed in Section 3  rather than
their convergence rate.

8

510152010−410−810−12510152010−410−810−12510152010−410−810−12510152010010−410−810−12510152010−110−251015203×10−22×10−2510152010210110010−110−2510152010010−1510152010010410810121016FinitopermFinitoSAGASVRGSAGSDCALBFGSReferences
[1] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Technical report  INRIA  hal-0086005  2013.

[2] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regular-

ized loss minimization. JMLR  14:567–599  2013.

[3] Paul Tseng and Sangwoon Yun. Incrementally updated gradient methods for constrained and
regularized optimization. Journal of Optimization Theory and Applications  160:832:853 
2014.

[4] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance re-
duction. Technical report  Microsoft Research  Redmond and Rutgers University  Piscataway 
NJ  2014.

[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-

ance reduction. NIPS  2013.

[6] Taiji Suzuki. Stochastic dual coordinate ascent with alternating direction method of multipliers.

Proceedings of The 31st International Conference on Machine Learning  2014.

[7] Evan Greensmith  Peter L. Bartlett  and Jonathan Baxter. Variance reduction techniques for

gradient estimates in reinforcement learning. JMLR  5:1471–1530  2004.

[8] Jakub Koneˇcn´y and Peter Richt´arik. Semi-stochastic gradient descent methods. ArXiv e-prints 

arXiv:1312.1666  December 2013.

[9] Aaron Defazio  Tiberio Caetano  and Justin Domke. Finito: A faster  permutable incremental
gradient method for big data problems. Proceedings of the 31st International Conference on
Machine Learning  2014.

[10] Julien Mairal. Incremental majorization-minimization optimization with application to large-
scale machine learning. Technical report  INRIA Grenoble Rhˆone-Alpes / LJK Laboratoire
Jean Kuntzmann  2014.

[11] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent
for regularized loss minimization. Technical report  The Hebrew University  Jerusalem and
Rutgers University  NJ  USA  2013.

[12] Patrick Combettes and Jean-Christophe Pesquet. Proximal Splitting Methods in Signal Pro-
cessing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer 
2011.

[13] Yu. Nesterov. Introductory Lectures On Convex Programming. Springer  1998.
[14] Aaron Defazio. New Optimization Methods for Machine Learning. PhD thesis  (draft under
examination) Australian National University  2014. http://www.aarondefazio.com/pubs.html.

9

,Aaron Defazio
Francis Bach
Simon Lacoste-Julien
Siddhartha Banerjee
Peter Lofgren
Soheil Feizi
Hamid Javadi
David Tse
Jingwei Xu
Bingbing Ni
Xiaokang Yang