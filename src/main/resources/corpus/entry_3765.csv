2012,Newton-Like Methods for Sparse Inverse Covariance Estimation,We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The first approach  which we call the Newton-LASSO method  minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding method (FISTA) to solve this subproblem. The second approach  which we call the Orthant-Based Newton method  is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.  These methods exploit the structure of the Hessian to efficiently compute the search direction and to avoid explicitly storing the Hessian.  We show that quasi-Newton methods are also effective in this context  and describe a limited memory BFGS variant of the orthant-based Newton method.  We present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem. Comparisons with the method implemented in the QUIC software package are presented.,Newton-Like Methods for Sparse Inverse Covariance

Estimation

Peder A. Olsen

IBM  T. J. Watson Research Center

pederao@us.ibm.com

Jorge Nocedal

Northwestern University

nocedal@eecs.northwestern.edu

Figen Oztoprak
Sabanci University

figen@sabanciuniv.edu

Steven J. Rennie

IBM  T. J. Watson Research Center

sjrennie@us.ibm.com

Abstract

We propose two classes of second-order optimization methods for solving the
sparse inverse covariance estimation problem. The ﬁrst approach  which we call
the Newton-LASSO method  minimizes a piecewise quadratic model of the objec-
tive function at every iteration to generate a step. We employ the fast iterative
shrinkage thresholding algorithm (FISTA) to solve this subproblem. The second
approach  which we call the Orthant-Based Newton method  is a two-phase algo-
rithm that ﬁrst identiﬁes an orthant face and then minimizes a smooth quadratic ap-
proximation of the objective function using the conjugate gradient method. These
methods exploit the structure of the Hessian to efﬁciently compute the search di-
rection and to avoid explicitly storing the Hessian. We also propose a limited
memory BFGS variant of the orthant-based Newton method. Numerical results 
including comparisons with the method implemented in the QUIC software [1] 
suggest that all the techniques described in this paper constitute useful tools for
the solution of the sparse inverse covariance estimation problem.

1

Introduction

Covariance selection  ﬁrst described in [2]  has come to refer to the problem of estimating a nor-
mal distribution that has a sparse inverse covariance matrix P  whose non-zero entries correspond
to edges in an associated Gaussian Markov Random Field  [3]. A popular approach to covariance
selection is to maximize an (cid:96)1 penalized log likelihood objective  [4]. This approach has also been
applied to related problems  such as sparse multivariate regression with covariance estimation  [5] 
and covariance selection under a Kronecker product structure  [6]. In this paper  we consider the
same objective function as in these papers  and present several Newton-like algorithms for minimiz-
ing it.
Following [4  7  8]  we state the problem as

P∗ = arg max

log det(P) − trace(SP) − λ(cid:107)vec(P)(cid:107)1 

(1)

where λ is a (ﬁxed) regularization parameter 

P(cid:31)0

(cid:80)N
i=1(xi − µ)(xi − µ)T

(2)
is the empirical sample covariance  µ is known  the xi ∈ Rn are assumed to be independent 
identically distributed samples  and vec(P) deﬁnes a vector in Rn2 obtained by stacking the columns
of P. We recast (1) as the minimization problem

S = 1
N

F (P) def= L(P) + λ(cid:107)vec(P)(cid:107)1 

min
P(cid:31)0

(3)

1

where L is the negative log likelihood function

L(P) = −log det(P) + trace(SP).

The convex problem (3) has a unique solution P∗ that satisﬁes the optimality conditions [7]

S − [P∗]−1 + λZ∗ = 0 

 1

−1
α ∈ [−1  1]

if P ∗
if P ∗
if P ∗

ij > 0
ij < 0
ij = 0.

where

Z∗
ij =

We note that Z∗ solves the dual problem
Z∗ = arg max(cid:107)vec(Z)(cid:107)∞≤1
S+λZ(cid:31)0

U (Z) 

U (Z) = −log det(S + λZ) + n.

The main contribution of this paper is to propose two classes of second-order methods for solving
problem (3). The ﬁrst class employs a piecewise quadratic model in the step computation  and can be
seen as a generalization of the sequential quadratic programming method for nonlinear programming
[9]; the second class minimizes a smooth quadratic model of F over a chosen orthant face in Rn2.
We argue that both types of methods constitute useful tools for solving the sparse inverse covariance
matrix estimation problem.
An overview of recent work on the sparse inverse covariance estimation problem is given in [10  11].
First-order methods proposed include block-coordinate descent approaches  such as COVSEL  [4  8]
and GLASSO [12]  greedy coordinate descent  known as SINCO [13]  projected subgradient methods
PSM [14]  ﬁrst order optimal gradient ascent [15]  and the alternating linearization method ALM
[16]. Second-order methods include the inexact interior point method IPM proposed in [17]  and the
coordinate relaxation method described in [1] and implemented in the QUIC software. It is reported
in [1] that QUIC is signiﬁcantly faster than the ALM  GLASSO  PSM  SINCO and IPM methods. We
compare the algorithms presented in this paper to the method implemented in QUIC.

2 Newton Methods

We can deﬁne a Newton iteration for problem (1) by constructing a quadratic  or piecewise
quadratic  model of F using ﬁrst and second derivative information. It is well known [4] that the
derivatives of the log likelihood function (4) are given by

g def= L(cid:48)(P) = vec(S − P−1)

(7)
where ⊗ denotes the Kronecker product. There are various ways of using these quantities to deﬁne
a model of F   and each gives rise to a different Newton-like iteration.
In the Newton-LASSO Method  we approximate the objective function F at the current iterate Pk
by the piecewise quadratic model

and

H def= L(cid:48)(cid:48)(P) = (P−1 ⊗ P−1) 

k vec(P − Pk) + 1

qk(P) = L(Pk) + g(cid:62)

2 vec(cid:62)(P − Pk)Hkvec(P − Pk) + λ(cid:107)vec(P)(cid:107)1 

(8)
where gk = L(cid:48)(Pk)  and similarly for Hk. The trial step of the algorithm is computed as a mini-
mizer of this model  and a backtracking line search ensures that the new iterate lies in the positive
deﬁnite cone and decreases the objective function F . We note that the minimization of qk is often
called the LASSO problem [18] in the case when the unknown is a vector.
It is advantageous to perform the minimization of (8) in a reduced space; see e.g. [11] and the
references therein. Speciﬁcally  at the beginning of the k-th iteration we deﬁne the set Fk of (free)
variables that are allowed to move  and the active set Ak. To do so  we compute the steepest descent
for the function F   which is given by

−(gk + λZk) 

(4)

(5)

(6)

where

(Zk)ij =



1
−1
−1
1
λ [gk]ij

− 1

if (Pk)ij > 0
if (Pk)ij < 0
if (Pk)ij = 0 and [gk]ij > λ
if (Pk)ij = 0 and [gk]ij < −λ
if (Pk)ij = 0 and | [gk]ij| ≤ λ.

(9)

2

The sets Fk Ak are obtained by considering a small step along this steepest descent direction  as this
guarantees descent in qk(P). For variables satisfying the last condition in (9)  a small perturbation
of Pij will not decrease the model qk. This suggests deﬁning the active and free sets of variables at
iteration k as
Ak = {(i  j)|(Pk)ij = 0 and |[gk]ij| ≤ λ} 
(10)
The algorithm minimizes the model qk over the set of free variables. Let us deﬁne pF = vec(P)F  
to be the free variables  and let pkF = vecF (Pk) denote their value at the current iterate – and
similarly for other quantities. Let us also deﬁne HkF to be the matrix obtained by removing from
Hk the columns and rows corresponding to the active variables (with indices in Ak). The reduced
model is given by

Fk = {(i  j)|(Pk)ij (cid:54)= 0 or |[gk]ij| > λ}.

qF (P) = L(Pk) + g(cid:62)

The search direction d is deﬁned by

kF (pF − pkF ) + 1
2 (pF − pkF )(cid:62)HkF (pF − pkF ) + λ(cid:107)pF(cid:107)1.
(cid:21)

(cid:20) ˆpF − pkF

(cid:20) dF

(cid:21)

d =

=

dA

0

 

where ˆpF is the minimizer of (11). The algorithm performs a line search along the direction D =
mat(d)  where the operator mat(d) satisﬁes mat(vec(D)) = D. The line search starts with the
unit steplength and backtracks  if necessary  to obtain a new iterate Pk+1 that satisﬁes the sufﬁcient
decrease condition and positive deﬁniteness (checked using a Cholesky factorization):

F (Pk+1) − F (Pk) < σ (qF (Pk+1) − qF (Pk))

and Pk+1 (cid:31) 0 

where σ ∈ (0  1).
It is suggested in [1] that coordinate descent is the most effective iteration for solving the LASSO
problem (11). We claim  however  that other techniques merit careful investigation. These include
gradient projection [19] and iterative shrinkage thresholding algorithms  such as ISTA [20] and FISTA
[21]. In section 3 we describe a Newton-LASSO method that employs the FISTA iteration.
Convergence properties of the Newton-LASSO method that rely on the exact solution of the LASSO
problem (8) are given in [22]. In practice  it is more efﬁcient to solve problem (8) inexactly  as
discussed in section 6. The convergence properties of inexact Newton-LASSO methods will be the
subject of a future study.
The Orthant-Based Newton method computes steps by solving a smooth quadratic approximation
of F over an appropriate orthant – or more precisely  over an orthant face in Rn2. The choice
of this orthant face is done  as before  by computing the steepest descent direction of F   and is
characterized by the matrix Zk in (9). Speciﬁcally the ﬁrst four conditions in (9) identify an orthant
in Rn2 where variables are allowed to move  while the last condition in (9) determines the variables
to be held at zero. Therefore  the sets of free and active variables are deﬁned as in (10). If we deﬁne
zF = vecF (Z)  then the quadratic model of F over the current orthant face is given by
(pF − pkF )(cid:62)HF (pF − pkF ) + λz(cid:62)

QF (P) = L(Pk) + g(cid:62)

F pF .
F = pkF − H−1F (gF + λzF )  and the step of the algorithm is given by

The minimizer is p∗

F (pF − pkF ) +

(14)

1
2

(cid:21)

(cid:20) dF

dA

(cid:20) p∗

F − pkF

0

(cid:21)

(11)

(12)

(13)

d =

=

.

(15)

If pkF +d lies outside the current orthant  we project it onto this orthant and perform a backtracking
line search to obtain the new iterate Pk+1  as discussed in section 4.
The orthant-based Newton method therefore moves from one orthant face to another  taking advan-
tage of the fact that F is smooth in every orthant in Rn2. In Figure 1 we compare the two methods
discussed so far.

The optimality conditions (5) show that P∗ is diagonal when λ ≥ |Sij| for all i (cid:54)= j  and given by
(diag(S) + λI)−1. This suggests that a good choice for the initial value (for any value of λ > 0) is
(16)

P0 = (diag(S) + λI)−1.

3

Method NL (Newton-LASSO)
Repeat:

1. Phase I: Determine the sets of ﬁxed and

free indices Ak and Fk  using (10).

2. Phase II: Compute the Newton step D
given by (12)  by minimizing the piece-
wise quadratic model (11) for the free
variables Fk.

3. Globalization: Choose Pk+1 by per-
forming an Armijo backtracking line
search starting from Pk + D.

4. k ← k + 1.

Method OBN (Orthant-Based Newton)
Repeat:

1. Phase I: Determine the active orthant
face through the matrix Zk given in (9).
2. Phase II: Compute the Newton direc-
tion D given by (15)  by minimizing
the smooth quadratic model (14) for the
free variables Fk.

3. Globalization: Choose Pk+1 in the cur-
rent orthant by a projected backtracking
line search starting from Pk + D.

4. k ← k + 1.

Figure 1: Two classes of Newton methods for the inverse covariance estimation problem (3).

Numerical experiments indicate that this choice is advantageous for all methods considered.
A popular orthant based method for the case when the unknown is a vector is OWL [23]; see also
[11]. Rather than using the Hessian (7)  OWL employs a quasi-Newton approximation to minimize
the reduced quadratic  and applies an alignment procedure to ensure descent. However  for reasons
that are difﬁcult to justify the OWL step employs the reduced inverse Hessian (as apposed to the
inverse of the reduced Hessian)  and this can give steps of poor quality. We have dispensed with
the alignment as it is not needed in our experiments. The convergence properties of OBM methods
are the subject of a future study (we note in passing that the convergence proof given in [23] is not
correct).

3 A Newton-LASSO Method with FISTA Iteration

Let us consider a particular instance of the Newton-LASSO method that employs the Fast Iterative
Shrinkage Thresholding Algorithm FISTA [21] to solve the reduced subproblem (11). We recall that
for the problem

where f is a smooth convex quadratic function  the ISTA iteration [20] is given by

where c is a constant such that cI − f(cid:48)(cid:48)(x) (cid:31) 0  and the FISTA acceleration is given by

where ˆx1 = x0  t1 = 1  ti+1 =
operator given by

/2. Here Sλ/c denotes the soft thresholding

We can apply the ISTA iteration (18) to the reduced quadratic in (11) starting from x0 = vecFk (X0)
(which is not necessarily equal to pk = vecFk (Pk)). Substituting in the expressions for the ﬁrst and
second derivative in (7) gives

(cid:18)
(cid:18)

xi = Sλ/c

= Sλ/c

vecFk ( ˆXi) − 1
c
vecFk ( ˆXi) − 1
c

(cid:16)
gkFk + HkFk vecFk ( ˆXi − Pk)
vecFk (S − 2P−1
ˆXiP−1
k )

k + P−1

k

(cid:17)(cid:19)
(cid:19)

 

4

f (x) + λ(cid:107)x(cid:107)1 

min
x∈Rn2

(cid:18)

(cid:19)

xi = Sλ/c

ˆxi − 1
c

∇f (ˆxi)

 

(xi − xi−1) 

ˆxi+1 = xi +

(cid:16)

ti − 1
ti+1

(cid:17)

1 +(cid:112)1 + 4t2
(cid:26)

i

(Sσ(y))i =

0

yi − σsign(yi)

if |yi| ≤ σ 
otherwise.

(17)

(18)

(19)

where the constant c should satisfy c > 1/(eigminPk)2. The FISTA acceleration step is given by
(19). Let ¯x denote the free variables part of the (approximate) solution of (11) obtained by the
FISTA iteration. Phase I of the Newton-LASSO-FISTA method selects the free and active sets  Fk Ak 
as indicated by (10). Phase II  applies the FISTA iteration to the reduced problem (11)  and sets
Pk+1 ← mat

. The computational cost of K iterations of the FISTA algorithm is O(Kn3).

(cid:18)¯x

(cid:19)

0

4 An Orthant-Based Newton-CG Method

We now consider an orthant-based Newton method in which a quadratic model of F is minimized
approximately using the conjugate gradient (CG) method. This approach is attractive since  in addi-
tion to the usual advantages of CG (optimal Krylov iteration  ﬂexibility)  each CG iteration can be
efﬁciently computed by exploiting the structure of the Hessian matrix in (7).
Phase I of the orthant-based Newton-CG method computes the matrix Zk given in (9)  which is used
to identify an orthant face in Rn2. Variables satisfying the last condition in (9) are held at zero and
their indices are assigned to the set Ak  while the rest of the variables are assigned to Fk and are
allowed to move according to the signs of Zk: variables with (Zk)ij = 1 must remain non-negative 
and variables with (Zk)ij = −1 must remain non-positive.
Having identiﬁed the current orthant face  phase II of the method constructs the quadratic model
QF in the free variables  and computes an approximate solution by means of the conjugate gradient
method  as described in Algorithm 1.

Conjugate Gradient Method for Problem (14)
input : Gradient g  orthant indicator z  current iterate P0  maximum steps K  residual tolerance

r  and the regularization parameter λ.

output: Approximate Newton direction d = cg(P0  g  z  λ  K)
n = size(P0  1)   G = mat(g)   Z = mat(z);
A = {(i  j) : [P0]ij = 0 & |Gij| ≤ λ};
B = P−1
0   X0 = 0n×n  x0 = vec(X0);
R0 = −(G + λZ)  [R0]A ← 0;
k = 0  q0 = r0 = vec(R0);
while k ≤ min(n2  K) and (cid:107)rk(cid:107) > r do

( ∴ [r0]F = vF )

;

Qk = reshape(qk  n  n);
Yk = BQkB  [Yk]A ← 0  yk = vec(Yk);
αk = r(cid:62)
k rk
q(cid:62)
k yk
xk+1 = xk + αkqk;
rk+1 = rk − αkyk;
r(cid:62)
k+1rk+1
βk =
r(cid:62)
k rk
qk+1 = rk+1 + βkqk;
k ← k + 1;

;

end
return d = xk+1

Algorithm 1: CG Method for Minimizing the Reduced Model QF .

The search direction of the method is given by D = mat(d)  where d denotes the output of Al-
gorithm 1. If the trial step Pk + D lies in the current orthant  it is the optimal solution of (14).
Otherwise  there is at least one index such that
(i  j) ∈ Ak and [L(cid:48)(Pk + D)]ij /∈ [−λ  λ] 

(i  j) ∈ Fk and (Pk + D)ijZij < 0.

or

In this case  we perform a projected line search to ﬁnd a point in the current orthant that yields a
decrease in F . Let Π(·) denote the orthogonal projection onto the orthant face deﬁned by Zk  i.e. 

(cid:26)Pij

0

Π(Pij) =

if sign(Pij) = sign(Zk)ij
otherwise.

(20)

5

The line search computes a steplength αk to be the largest member of
{1  1/2  . . .   1/2i  . . .} such that

F (Π(Pk + αkD)) ≤ F (Pk) + σ(cid:101)∇F (Pk)T (Π(Pk + αkD) − Pk)  

where σ ∈ (0  1) is a given constant and (cid:101)∇F denotes the minimum norm subgradient of F . The

new iterate is deﬁned as Pk+1 = Π(Pk + αkD).
The conjugate gradient method requires computing matrix-vector products involving the reduced
Hessian  HkF . For our problem  we have

the sequence

(21)

HkF (pF − pkF ) = (cid:2)Hk
= (cid:2)P−1

(cid:0) pF−pkF
(cid:1)(cid:3)
k mat(cid:0) pF−pkF

F

0

(cid:1) P−1

(cid:3)

(22)
The second line follows from the identity (A ⊗ B)vec(C) = vec(BCA(cid:62)). The cost of performing
K steps of the CG algorithm is O(Kn3) operations  and K = n2 steps is needed to guarantee an
exact solution. Our practical implementation computes a small number of CG steps relative to n 
K = O(1)  and as a result the search direction is not an accurate approximation of the true Newton
step. However  such inexact Newton steps achieve a good balance between the computational cost
and the quality of the direction.

F .

0

k

5 Quasi-Newton Methods

The methods considered so far employ the exact Hessian of the likelihood function L  but one can
also approximate it using (limited memory) quasi-Newton updating. At ﬁrst glance it may not seem
promising to approximate a complicated Hessian like (7) in this manner  but we will see that quasi-
Newton updating is indeed effective  provided that we store matrices using the compact limited
memory representations [9].
Let us consider an orthant-based method that minimizes the quadratic model (14)  where HF is
replaced by a limited memory BFGS matrix  which we denote by BF . This matrix is not formed
explicitly  but is deﬁned in terms of the difference pairs

yk = gk+1 − gk 

sk = vec(Pk+1 − Pk).

(23)

It is shown in [24  eq(5.11)] that the minimizer of the model QF is given by

F = pF + B−1F (λzF − gF )
p∗
= 1

θ (λzF − gF ) + 1

θ2 RTF W(I − 1

−1

MWT RF (λzF − gF ).

θ MWT RF RTF W)

(24)
Here RF is a matrix consisting of the set of unit vectors that span the subspace of free variables  θ
is a scalar  W is an n2 × 2t matrix containing the t most recent correction pairs (23)  and M is a
2t × 2t matrix formed by inner products between the correction pairs. The total cost of computing
F is 2t2|F| + 4t|F| operations  where |F| is the cardinality of F. Since the memory
the minimizer p∗
parameter t in the quasi-Newton updating scheme is chosen to be a small number  say between 5
and 20  the cost of computing the subspace minimizer (24) is quite affordable. A similar approach
was taken in [25] for the related constrained inverse covariance sparsity problem.
We have noted above that OWL  which is an orthant based quasi-Newton method does not correctly
approximate the minimizer (24). We note also that quasi-Newton updating can be employed in
Newton-LASSO methods  but we do not discuss this approach here for the sake of brevity.

6 Numerical Experiments
We generated test problems by ﬁrst creating a random sparse inverse covariance matrix1  Σ−1  and
then sampling data to compute a corresponding non-sparse empirical covariance matrix S. The di-
mensions  sparsity  and conditioning of the test problems are given along with the results in Table 2.
For each data set  we solved problem (3) with λ values in the range [0.01  0.5]. The number of
samples used to compute the sample covariance matrix was 10n.

1http://www.cmap.polytechnique.fr/˜aspremon/CovSelCode.html  [7]

6

The algorithms we tested are listed in Table 1. With the exception of C:QUIC  all of these algorithms
were implemented in MATLAB. Here NL and OBN are abbreviations for the methods in Figure 1.
NL-Coord is a MATLAB implementation of the QUIC algorithm that follows the C-version [1]

Description
Newton-LASSO-FISTA method
Newton-LASSO method using coordinate descent
Orthant-based Newton-CG method with a limit of K CG iterations
OBN-CG-K with K=5 initially and increased by 1 every 3 iterations.

Algorithm
NL-FISTA
NL-Coord
OBN-CG-K
OBN-CG-D
OBN-LBFGS Orthant-based quasi-Newton method (see section 5)
ALM∗
C:QUIC

Alternating linearization method [26].
The C implementation of QUIC given in [1].

Table 1: Algorithms tested. ∗For ALM  the termination criteria was changed to the (cid:96)∞ norm and the value
of ABSTOL was set to 10−6 to match the stopping criteria of the other algorithms.

faithfully. We have also used the original C-implementation of QUIC and refer to it as C:QUIC. For
the Alternating Linearization Method (ALM) we utilized the MATLAB software available at [26] 
which implements the ﬁrst-order method described in [16]. The NL-FISTA algorithm terminated
the FISTA iteration when the minimum norm subgradient of the LASSO subproblem qF became
less than 1/10 of the minimum norm subgradient of F at the previous step.
Let us compare the computational cost of the inner iteration techniques used in the Newton-like
methods discussed in this paper.
(i) Applying K steps of the FISTA iteration requires O(Kn3)
operations. (ii) Coordinate descent  as implemented in [1]  requires O(Kn|F|) operations for K
coordinate descent sweeps through the set of free variables; (iii) Applying KCG iterations of the CG
methods costs O(KCGn3) operations.
The algorithms were terminated when either 10n iterations were executed or the minimum norm
subgradient of F was sufﬁciently small  i.e. when (cid:107) ˜∇F (P)(cid:107)∞ ≤ 10−6. The time limit of each run
was set to 5000 seconds.
The results presented in Table 2 show that the ALM method was never the fastest algorithm  but
nonetheless outperformed some second-order methods when the solution was less sparse. The num-
bers in bold indicate the fastest MATLAB implementation for each problem. As for the other meth-
ods  no algorithm appears to be consistently superior to the others  and the best choice may depend
on problem characteristics. The Newton-LASSO method with coordinate descent (NL-Coord) is
the most efﬁcient when the sparsity level is below 1%  but the methods introduced in this paper 
NL-FISTA  OBN-CG and OBN-LBFGS  seem more robust and efﬁcient for problems that are less
sparse. Based on these results  OBN-LBFGS appears to be the best choice as a universal solver for
the covariance selection problem. The C implementation of the QUIC algorithm is roughly ﬁve times
faster than its Matlab counterpart (OBN-Coord). C:QUIC was best in the two sparsest conditions 
but not in the two densest conditions. We expect that optimized C implementations of the presented
algorithms will also be signiﬁcantly faster. Note also that the crude strategy for dynamically in-
creasing the number of CG-steps in OBN-CG-D was effective  and we expect it could be further
improved. Our focus in this paper has been on exploring optimization methods and ideas rather
than implementation efﬁciency. However  we believe the observed trends will hold even for highly
optimized versions of all tested algorithms.

7

0.5

time

iter

0.1

time

iter

0.05

time

iter

0.01

time

7.27%
27.38

22.01
100.63
26.24
15.41
21.92
152.76
15.62

14.86%
16.11

13.12
19.69
7.36
5.22
11.42
32.98
3.79

6.65%
18.23

106.79
225.59
87.73
51.99
80.02
639.49
46.14

8.18%
11.75

72.21
79.71
35.85
26.87
40.31
255.79
17.42

1.75%
23.71

1039.08
1178.07
896.24
532.15
497.31
>5000
183.53

1.49%
4.72

153.18
71.55
71.54
75.82
78.34
1262.83
24.65

10
49
97
34
178
387
41

19
14
27
15
82
78
13

9
24
51
21
111
252
22

7
12
20
12
67
99
11

10
34
78
27
155
-
17

7
7
6
6
26
76
8

11.83%
51.01

37.04
279.69
70.91
43.29
38.23
115.11
35.64

25.66%
32.27

34.53
71.51
28.40
14.14
23.04
61.35
11.91

13.19%
39.59

203.07
951.23
198.17
132.38
111.49
462.34
186.87

18.38%
26.75

156.46
408.62
83.42
78.98
82.51
267.02
90.62

4.33%
46.54

1490.37
>5000
2394.95
1038.26
785.36
>5000
818.54

10.51%
17.02

694.93
1152.86
250.11
188.93
232.23
1800.67
256.90

11
66
257
65
293
284
58

15
21
101
31
155
149
21

12
36
108
39
178
186
34

10
19
47
27
124
106
19

10
-
203
43
254
-
40

9
13
21
13
71
106
13

12
103
1221
189
519
574
100

13
55
795
176
455
720
56

12
-
1103
171
548
734
72

22
49
681
148
397
577
52

32.48%
118.56

47.33%
99.49

106.27
1885.89
373.63
275.29
84.13
219.80
206.42

100.90
791.84
240.90
243.55
78.33
292.43
103.58

25.03%
132.13

36.34%
106.34

801.79
>5000
2026.26
1584.14
384.30
1826.29
1445.17

554.08
4837.46
1778.88
2055.44
297.90
1448.83
1100.72

14.68%
134.54
-
-
-
-
610
-
-
31.68%
79.61

>5000
>5000
>5000
>5000
2163.12
>5000
>5000

12
-
397
110
318
-
33

2852.86
>5000
4766.69
5007.83
1125.67
>5000
3899.68

λ

= 3.5%

= 2.4%

problem

algorithm
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 500
Card(Σ−1) OBN-CG-5
OBN-CG-D
OBN-LBFGS
ALM
C:QUIC
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 500
Card(Σ−1) OBN-CG-5
OBN-CG-D
= 20.1%
OBN-LBFGS
ALM
C:QUIC
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 1000
Card(Σ−1) OBN-CG-5
OBN-CG-D
OBN-LBFGS
ALM
C:QUIC
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 1000
Card(Σ−1) OBN-CG-5
OBN-CG-D
OBN-LBFGS
ALM
C:QUIC
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 2000
Card(Σ−1) OBN-CG-5
OBN-CG-D
OBN-LBFGS
ALM
C:QUIC
card(P∗)
cond(P∗)
NL-FISTA
NL-Coord
n = 2000
Card(Σ−1) OBN-CG-5
OBN-CG-D
= 18.7%
OBN-LBFGS
ALM
C:QUIC

= 11%

= 1%

iter

8
21
15
12
47
445
16

4
4
3
3
9
93
6

7
9
9
8
34
247
10

4
4
3
3
8
113
6

0.74%
8.24

5.71
3.86
4.07
3.88
5.37
162.96
0.74

0.21%
3.39

1.25
0.42
0.83
0.84
1.00
35.75
0.19

0.18%
6.22

28.20
5.23
15.34
15.47
18.27
617.63
2.38

0.10%
4.20

9.03
2.23
4.70
4.61
4.29
283.99
1.18

0.13%
7.41

8
14
13
9
41

264.94
54.33
187.41
127.11
115.13
- >5000
18.07

11

0.05%
2.32
P∗ = P0
P∗ = P0
P∗ = P0
P∗ = P0
P∗ = P0
52
8

874.22
10.35

Table 2: Results for 5 Newton-like methods and the QUIC  ALM method.

8

References
[1] C. J. Hsieh  M. A. Sustik  P. Ravikumar  and I. S. Dhillon. Sparse inverse covariance matrix estimation

using quadratic approximation. Advances in Neural Information Processing Systems (NIPS)  24  2011.

[2] A. P. Dempster. Covariance selection. Biometrics  28:157–75  1972.
[3] J. D. Picka. Gaussian Markov random ﬁelds: theory and applications. Technometrics  48(1):146–147 

2006.

[4] O. Banerjee  L. El Ghaoui  A. d’Aspremont  and G. Natsoulis. Convex optimization techniques for ﬁtting

sparse Gaussian graphical models. In ICML  pages 89–96. ACM  2006.

[5] A.J. Rothman  E. Levina  and J. Zhu. Sparse multivariate regression with covariance estimation. Journal

of Computational and Graphical Statistics  19(4):947–962  2010.

[6] T. Tsiligkaridis and A. O. Hero III. Sparse covariance estimation under Kronecker product structure. In

ICASSP 2006 Proceedings  pages 3633–3636  Kyoto  Japan  2012.

[7] O. Banerjee  L. El Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research  9:485–
516  2008.

[8] A. d’Aspremont  O. Banerjee  and L. El Ghaoui. First-order methods for sparse covariance selection.

SIAM Journal on Matrix Analysis and Applications  30(1):56–66  2008.

[9] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research. 1999.
[10] I. Rish and G. Grabarnik. ELEN E6898 Sparse Signal Modeling (Spring 2011): Lecture 7  Beyond

LASSO: Other Losses (Likelihoods). https://sites.google.com/site/eecs6898sparse2011/  2011.

[11] S. Sra  S. Nowozin  and S. J. Wright. Optimization for Machine Learning. MIT Press  2011.
[12] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical LASSO.

Biostatistics  9(3):432  2008.

[13] K. Scheinberg and I. Rish. SINCO-a greedy coordinate ascent method for sparse inverse covariance

selection problem. Technical report  IBM RC24837  2009.

[14] J. Duchi  S. Gould  and D. Koller. Projected subgradient methods for learning sparse Gaussians. In Proc.

of the Conf. on Uncertainty in AI. Citeseer  2008.

[15] Z. Lu. Smooth optimization approach for sparse covariance selection. Arxiv preprint arXiv:0904.0687 

2009.

[16] K. Scheinberg  S. Ma  and D. Goldfarb. Sparse inverse covariance selection via alternating linearization

methods. Arxiv preprint arXiv:1011.0097  2010.

[17] L. Li and K. C. Toh. An inexact interior point method for L1-regularized sparse covariance selection.

Mathematical Programming Computation  2(3):291–315  2010.

[18] R. Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society

B  58(1):267–288  1996.

[19] B. T. Polyak. The conjugate gradient method in extremal problems. U.S.S.R. Computational Mathematics

and Mathematical Physics  9:94–112  1969.

[20] I. Daubechies  M. Defrise  and C. De Mol. An iterative thresholding algorithm for linear inverse problems
with a sparsity constraint. Communications on pure and applied mathematics  57(11):1413–1457  2004.
[21] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[22] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable minimization.

Mathematical Programming  117(1):387–423  2009.

[23] G. Andrew and J. Gao. Scalable training of L1-regularized log-linear models. In ICML  pages 33–40.

ACM  2007.

[24] R. H. Byrd  P. Lu  J. Nocedal  and C. Zhu. A limited memory algorithm for bound constrained optimiza-

tion. SIAM Journal on Scientiﬁc Computing  16(5):1190–1208  1995.

[25] J. Dahl  V. Roychowdhury  and L. Vandenberghe. Maximum likelihood estimation of gaussian graphical

models: numerical implementation and topology selection. UCLA Preprint  2005.

[26] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Matlab scripts for alternating direction method

of multipliers. Technical report  http://www.stanford.edu/ boyd/papers/admm/  2012.

9

,Po-Hsuan (Cameron) Chen
Uri Hasson
Peter Ramadge
Max Vladymyrov