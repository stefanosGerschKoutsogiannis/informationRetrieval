2011,Approximating Semidefinite Programs in Sublinear Time,In recent years semidefinite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the first  sublinear time approximation algorithm for semidefinite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric.,Approximating Semideﬁnite Programs in Sublinear

Time

Dan Garber

Haifa 32000 Israel

Elad Hazan

Haifa 32000 Israel

Technion - Israel Institute of Technology

Technion - Israel Institute of Technology

dangar@cs.technion.ac.il

ehazan@ie.technion.ac.il

Abstract

In recent years semideﬁnite optimization has become a tool of major importance
in various optimization and machine learning problems. In many of these prob-
lems the amount of data in practice is so large that there is a constant need for
faster algorithms. In this work we present the ﬁrst sublinear time approximation
algorithm for semideﬁnite programs which we believe may be useful for such
problems in which the size of data may cause even linear time algorithms to have
prohibitive running times in practice. We present the algorithm and its analysis
alongside with some theoretical lower bounds and an improved algorithm for the
special problem of supervised learning of a distance metric.

1

Introduction

Semideﬁnite programming (SDP) has become a tool of great importance in optimization in the past
years. In the ﬁeld of combinatorial optimization for example  numerous approximation algorithms
have been discovered starting with Goemans and Williamson [1] and [2  3  4]. In the ﬁeld of machine
learning solving semideﬁnite programs is at the heart of many learning tasks such as learning a
distance metric [5]  sparse PCA [6]  multiple kernel learning [7]  matrix completion [8]  and more. It
is often the case in machine learning that the data is assumed no be noisy and thus when considering
the underlying optimization problem  one can settle for an approximated solution rather then an
exact one. Moreover it is also common in such problems that the amounts of data are so large that
fast approximation algorithms are preferable to exact generic solvers  such as interior-point methods 
which have impractical running times and memory demands and are not scalable.
In the problem of learning a distance metric [5] one is given a set of points in Rn and similarity
information in the form of pairs of points and a label indicating weather the two points are in the
same class or not. The goal is to learn a distance metric over Rn which respects this similarity
information. That is it assigns small distances to points in the same class and bigger distances to
points in different classes. Learning such a metric is important for other learning tasks which rely on
having a good metric over the input space  such as K-means  nearest-neighbours and kernel-based
algorithms.
In this work we present the ﬁrst approximation algorithm for general semideﬁnite programming
which runs in time that is sublinear in the size of the input. For the special case of learning a
pseudo-distance metric  we present an even faster sublinear time algorithm. Our algorithms are the
fastest possible in terms of the number of constraints and the dimensionality  although slower than
other methods in terms of the approximation guarantee.

1.1 Related Work

Semideﬁnite programming is a notoriously difﬁcult optimization formulation  and has attracted a
host of attempts at fast approximation methods. Klein and Lu [9] gave a fast approximate solver for

1

the MAX-CUT semideﬁnite relaxation of [1]. Various faster and more sophisticated approximate
solvers followed [10  11  12]  which feature near-linear running time albeit polynomial dependence
on the approximation accuracy. For the special case of covering an packing SDP problems  [13]
and [14] respectively give approximation algorithms with a smaller dependency on the approxima-
tion parameter . Our algorithms are based on the recent work of [15] which described sublinear
algorithms for various machine learning optimization problems such has linear classiﬁcation and
minimum enclosing ball. We describe here how such methods  coupled with techniques  may be
used for semideﬁnite optimization.

(cid:113)(cid:80)

The notation C ◦ X is just the dot product between matrices  that is C ◦ X =(cid:80)
We denote by ∆m the m-dimensional simplex  that is ∆m = {p|(cid:80)m

2 Preliminaries
In this paper we denote vectors in Rn by a lower case letter (e.g. v) and matrices in Rn×n by
upper case letters (e.g. A). We denote by (cid:107)v(cid:107) the standard euclidean norm of the vector v and by
(cid:107)A(cid:107) the frobenius norm norm of the matrix A  that is (cid:107)A(cid:107) =
i j A(i  j)2. We denote by (cid:107)v(cid:107)1
the l1-norm of v. The notation X (cid:23) 0 states that the matrix X is positive semi deﬁnite  i.e. it is
symmetric and all of its eigenvalues are non negative. The notation X (cid:23) B states that X − B (cid:23) 0.
i j C(i  j)X(i  j).
i=1 pi = 1 ∀i : pi ≥ 0}.
We denote by 1n the all ones n-dimensional vector and by 0n×n the all zeros n × n matrix. We
denote by I the identity matrix when its size is obvious from context. Throughout the paper we will
use the complexity notation ˜O(·) which is the same as the notation O(·) with the difference that it
suppresses poly-logarithmic factors that depend on n  m  −1.
We consider the following general SDP problem
Maximise C ◦ X
subject to Ai ◦ X ≥ 0

i = 1  ...  m

X (cid:23) 0

(1)

Where C  A1  ...  Am ∈ Rn×n. For reasons that will be made clearer in the analysis  we will assume
that for all i ∈ [m]  (cid:107)Ai(cid:107) ≤ 1
The optimization problem (1) can be reduced to a feasibility problem by a standard reduction of
performing a binary search over the value of the objective C◦X and adding an appropriate constraint.
Thus we will only consider the feasibility problem of ﬁnding a solution that satisﬁes all constraints.
The feasibility problem can be rewritten using the following min-max formulation

max
X(cid:23)0

min
i∈[m]

Ai ◦ X

(2)

Clearly if the optimum value of (2) is non-negative  then a feasible solution exists and vice versa.
Denoting the optimum of (2) by σ  an  additive approximation algorithm to (2) is an algorithm that
produces a solution X such that X (cid:23) 0 and for all i ∈ [m]  Ai ◦ X ≥ σ − .
For the simplicity of the presentation we will only consider constraints of the form A ◦ X ≥ 0 but
we mention in passing that SDPs with other linear constraints can be easily rewritten in the form of
(1).
We will be interested in a solution to (2) which lies in the bounded semideﬁnite cone K =
{X|X (cid:23) 0  Tr(X) ≤ 1}. The demand on a solution to (2) to have bounded trace is due to the
observation that in case σ > 0  any solution needs to be bounded or else the products Ai ◦ X could
be made to be arbitrarily large.

Learning distance pseudo metrics
In the problem of learning a distance metric from examples 
i  yi}}m
i ∈ Rn and yi ∈ {−1  1}. A value
we are given a set triplets S = {{xi  x(cid:48)
i are in the same class and a value yi = −1 indicates that they
yi = 1 indicates that the vectors xi  x(cid:48)
are from different classes. Our goal is to learn a pseudo-metric over Rn which respects the example
set. A pseudo-metric is a function d : R× R → R  which satisﬁes three conditions: (i) d(x  x(cid:48)) ≥ 0 
(ii) d(x  x(cid:48)) = d(x(cid:48)  x)   and (iii) d(x1  x2) + d(x2  x3) ≥ d(x1  x3). We consider pseudo-metrics

of the form dA(x  x(cid:48)) ≡(cid:112)(x − x(cid:48))(cid:62)A(x − x(cid:48)). Its easily veriﬁed that if A (cid:23) 0 then dA is indeed a

i=1 such that xi  x(cid:48)

pseudo-metric. A reasonable demand from a ”good” pseudo metric is that it separates the examples

2

(assuming such a separation exists). That is we would like to have a matrix A (cid:23) 0 and a threshold
value b ∈ R such that for all {xi  x(cid:48)

i  yi} ∈ S it will hold that 

(dA(xi − x(cid:48)
(dA(xi − x(cid:48)

i))2 = (xi − x(cid:48)
i))2 = (xi − x(cid:48)

i)(cid:62)A(xi − x(cid:48)
i)(cid:62)A(xi − x(cid:48)

i) ≤ b − σ/2
i) ≥ b + σ/2

yi = 1
yi = −1

where σ is the margin of separation which we would like to maximize. Denoting by vi = (xi − x(cid:48)
i)
for all i ∈ [m]  (3) can be summarized into the following formalism:

Without loss of generality we can assume that b = 1 and derive the following optimization problem

(3)

(4)

(cid:0)b − v(cid:62)

yi

i Avi

(cid:1) ≥ σ
(cid:0)1 − v(cid:62)

i Avi

(cid:1)

max
A(cid:23)0

min
i∈[m]

yi

3 Algorithm for General SDP

Our algorithm for general SDPs is based on the generic framework for constrained optimization
problems that ﬁt a max-min formulation  such as (2)  presented in [15]. Noticing that mini∈[m] Ai ◦
X = minp∈∆m

(cid:80)
i∈[m] p(i)Ai ◦ X  we can rewrite (2) in the following way

p(i)A(cid:62)
i x

(5)

x∈K min
max
p∈∆m

two online algorithms: one that wishes to maximize(cid:80)
other that wishes to minimize(cid:80)
solution a vector in the direction of the gradient(cid:80)

Building on [15]  we use an iterative primal-dual algorithm that simulates a repeated game between
i∈[m] p(i)Ai ◦ X as a function of X and the
i∈[m] p(i)Ai ◦ X as a function of p. If both algorithms achieve
sublinear regret  then this framework is known to approximate max-min problems such as (5)  in
case a feasible solution exists [16].
The primal algorithm which controls X is a gradient ascent algorithm that given p adds to the current
i∈[m] p(i)Ai. Instead of adding the exact gradient
we actually only sample from it by adding Ai with probability p(i) (lines 5-6). The dual algorithm
which controls p is a variant of the well known multiplicative (or exponential) update rule for online
optimization over the simplex which updates the weight p(i) according to the product Ai ◦ X (line
11). Here we replace the exact computation of Ai ◦ X by employing the l2-sampling technique
used in [15] in order to estimate this quantity by viewing only a single entry of the matrix Ai (line
9). An important property of this sampling procedure is that if (cid:107)Ai(cid:107) ≤ 1  then E[˜vt(i)2] ≤ 1.
Thus  we can estimate the product Ai ◦ X with constant variance  which is important for our anal-
ysis. A problem that arises with this estimation procedure is that it might yield unbounded values
which do not ﬁt well with the multiplicative weights analysis. Thus we use a clipping procedure
clip(z  V ) ≡ min{V  max{−V  Z}} to bound these estimations in a certain range (line 10). Clip-
ping the samples yields unbiased estimators of the products Ai ◦ X but the analysis shows that this
bias is not harmful.
The algorithm is required to generate a solution X ∈ K. This constraint is enforced by performing
a projection step onto the convex set K after each gradient improvement step of the primal online
algorithm. A projection of a matrix Y ∈ Rn×n onto K is given by Yp = arg minX∈K (cid:107)Y − X(cid:107).
Unlike the algorithms in [15] that perform optimization over simple sets such as the euclidean unit
ball which is trivial to project onto  projecting onto the bounded semideﬁnite cone is more compli-
cated and usually requires to diagonalize the projected matrix (assuming it is symmetric). Instead 
we show that one can settle for an approximated projection which is faster to compute (line 4). Such
approximated projections could be computed by Hazan’s algorithm for ofﬂine optimization over the
bounded semideﬁnite cone  presented in [12]. Hazan’s algorithm gives the following guarantee
Lemma 3.1. Given a matrix Y ∈ Rn×n   > 0  let f (X) = −(cid:107)Y − X(cid:107)2 and denote X∗ =
arg maxX∈K f (X). Then Hazan’s algorithm produces a solution ˜X ∈ K of rank at most −1 such
that (cid:107)Y − ˜X(cid:107)2 − (cid:107)Y − X∗(cid:107)2 ≤  in O

(cid:16) n2

time.

(cid:17)

1.5

We can now state the running time of our algorithm.
Lemma 3.2. Algorithm SublinearSDP has running time ˜O

3

(cid:16) m

2 + n2

5

(cid:17)

.

Algorithm 1 SublinearSDP
1: Input:  > 0  Ai ∈ Rn×n for i ∈ [m].

2: Let T ← 602−2 log m  Y1 ← 0n×n  w1 ← 1m  η ←(cid:113) log m

T   P ← /2.

  Xt ←ApproxProject(Yt  2
P ).

pt ← wt(cid:107)wt(cid:107)1
Choose it ∈ [m] by it ← i w.p. pt(i).
Yt+1 ← Yt + 1√
Choose (jt  lt) ∈ [n] × [n] by (jt  lt) ← (j  l) w.p. Xt(j  l)2/(cid:107)Xt(cid:107)2.
for i ∈ [m] do

Ait

2T

˜vt ← Ai(jt  lt)(cid:107)Xt(cid:107)2/Xt(jt  lt)
vt(i) ←clip(˜vt(i)  1/η)
wt+1(i) ← wt(i)(1 − ηvt(i) + η2vt(i)2)

3: for t = 1 to T do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for
14: return ¯X = 1
T

end for

(cid:80)

t Xt

We also have the following lower bound.
Theorem 3.3. Any algorithm which computes an -approximation with probability at least 2
has running time Ω

(cid:16) m

(cid:17)

.

2 + n2

2

3 to (2)

We note that while the dependency of our algorithm on the number of constraints m is close to
optimal (up to poly-logarithmic factors)  there is a gap of ˜O(−3) between the dependency of our
algorithm on the size of the constraint matrices n2 and the above lower bound. Here it is important to
note that our lower bound does not reﬂect the computational effort in computing a general solution
that is positive semideﬁnite which is in fact the computational bottleneck of our algorithm (due to
the use of the projection procedure).

4 Analysis

We begin with the presentation of the Multiplicative Weights algorithm used in our algorithm.
Deﬁnition 4.1. Consider a sequence of vectors q1  ...  qT ∈ Rm. The Multiplicative Weights (MW)
algorithm is as follows. Let 0 < η ∈ R  w1 ← 1m  and for t ≥ 1 

pt ← wt/(cid:107)wt(cid:107)1  wt+1 ← wt(i)(1 − ηqt(i) + η2qt(i)2)

The following lemma gives a bound on the regret of the MW algorithm  suitable for the case in
which the losses are random variables with bounded variance.
Lemma 4.2. The MW algorithm satisﬁes

max{qt(i) − 1
η

} +

log m

η

+ η

p(cid:62)
t q2
t

(cid:88)

t∈[t]

(cid:88)

(cid:88)
Lemma 4.3. For 1/4 ≥ η ≥(cid:113) log m

t qt ≤ min
p(cid:62)
i∈[m]

t∈[T ]

t∈[T ]

The following lemma gives concentration bounds on our random variables from their expectations.

(cid:80)
t∈[T ][vt(i) − Ai ◦ Xt] ≤ 4ηT (ii)

(i) maxi∈[m]

T   with probability at least 1 − O(1/m)  it holds that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)

t∈[T ]

Ait ◦ Xt − (cid:88)

t∈[T ]

p(cid:62)
t vt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 8ηT

The following Lemma gives a regret bound on the lazy gradient ascent algorithm used in our algo-
rithm (line 6). For a proof see Lemma A.2 in [17].

4

Lemma 4.4. Consider matrices A1  ...  AT ∈ Rn×n such that for all i ∈ [m] (cid:107)Ai(cid:107) ≤ 1. Let
X0 = 0n×n and for all t ≥ 1 let Xt+1 = arg minX∈K

(cid:13)(cid:13)(cid:13) 1√

(cid:80)t
τ =1 Aτ − X
√
At ◦ Xt ≤ 2

2T

2T

(cid:13)(cid:13)(cid:13) Then

(cid:88)

t∈[T ]

At ◦ X − (cid:88)

t∈[T ]

max
X∈K

We are now ready to state the main theorem and prove it.
Theorem 4.5 (Main Theorem). With probability 1/2  the SublinearSDP algorithm returns an -
additive approximation to (5).
Proof. At ﬁrst assume that the projection onto the set K in line 4 is an exact projection and not an
approximation and denote by ˜Xt the exact projection of Yt. In this case  by lemma 4.4 we have

(cid:88)

t∈[T ]

Ait ◦ X − (cid:88)

t∈[T ]

max
x∈K

√
Ait ◦ ˜Xt ≤ 2

2T

By the law of cosines and lemma 3.1 we have for every t ∈ [T ]

(cid:107)Xt − ˜Xt(cid:107)2 ≤ (cid:107)Yt − Xt(cid:107)2 − (cid:107)Yt − ˜Xt(cid:107)2 ≤ 2

P

Rewriting (6) we have

Ait ◦ X − (cid:88)

t∈[T ]

max
x∈K

(cid:88)
Ait ◦ X − (cid:88)

t∈[T ]

Ait ◦ Xt − (cid:88)
(cid:88)

t∈[T ]

2T +

Using the Cauchy-Schwarz inequality  (cid:107)Ait(cid:107) ≤ 1 and (7) we get

(cid:88)

t∈[T ]

max
x∈K

Rearranging and plugging maxx∈K mini∈[m] Ai ◦ X = σ we get
2T − T P

√
Ait ◦ ( ˜Xt − Xt) ≤ 2

2T

(cid:107)Ait(cid:107)(cid:107) ˜Xt − Xt(cid:107) ≤ 2

√

2T + T P

(6)

(7)

(8)

Turning to the MW part of the algorithm  by the MW Regret Lemma 4.2  and using the clipping of

vt(i) we have (cid:88)

t∈[T ]

By Lemma 4.3  with high probability and for any i ∈ [n] 

t∈[T ]

t∈[T ]

√
Ait ◦ Xt ≤ 2
(cid:88)

t∈[T ]

√
Ait ◦ Xt ≥ T σ − 2
(cid:88)
t vt ≤ min
p(cid:62)
i∈[i]
(cid:88)
vt(i) ≤ (cid:88)
(cid:88)

t∈[T ]

t∈[T ]

t∈[t]

vt(i) + (log m)/η + η

Ai ◦ Xt + 4ηT

Ai ◦ Xt + (log m)/η + η

p(cid:62)
t v2

t + 4ηT

(9)

Combining (8) and (9) we get

Ai ◦ Xt ≥ − (log m)/η − η

t − 4ηT + T σ

Thus with high probability it holds that

(cid:88)

t∈[T ]

t vt ≤ min
p(cid:62)
i∈[i]
(cid:88)

min
i∈[i]

t∈[t]
√

− 2

2T −

t∈[t]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)

t∈[T ]

By a simple Markov inequality argument it holds that w.p. at least 3/4 

(cid:88)

t∈[T ]

p(cid:62)
t v2
t

t∈[T ]

(cid:88)
(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − T P

p(cid:62)
t v2

t∈[T ]

Ait ◦ Xt

p(cid:62)

t vt − (cid:88)
(cid:88)

t∈[T ]

p(cid:62)
t v2

t ≤ 8T

t∈[T ]

5

Combined with lemma 4.3  we have w.p. at least 3

4 − O( 1

n ) ≥ 1

2

√
Ai ◦ Xt ≥ −(log m)/η − 8ηT − 4ηT + T σ − 2

2T − 8ηT − T P

(cid:88)

t∈[t]

min
i∈[i]

≥ T σ − log m

η

− 20ηT − 2

√

2T − T P

Dividing through by T and plugging in our choice for η and P   we have mini∈[m] Ai ◦ ¯X ≥ σ − 
w.p. at least 1/2.

5 Application to Learning Pseudo-Metrics

(cid:0)1 − v(cid:62)

i Avi

(cid:1)
(cid:26)(cid:18) A 0

As in the problem of general SDP  we can also rewrite (4) by replacing the mini∈[m] objective with
minp∈∆m and arrive at the following formalism 

max
A(cid:23)0

min
p∈∆m

yi

(10)
As we demanded a solution to general SDP to have bounded trace  here we demand that (cid:107)A(cid:107) ≤ 1.
Letting v(cid:48)
  we
can rewrite (10) in the following form.

and deﬁning the set of matrices P =

|A (cid:23) 0 (cid:107)A(cid:107) ≤ 1

(cid:18) vi

0 −1

(cid:27)

(cid:19)

(cid:19)

i =

1

A∈P min
max
p∈∆m

−yiv(cid:48)
i ◦ A
iv(cid:48)(cid:62)
In what comes next  we use the notation Ai = −yiv(cid:48)
iv(cid:48)
i.
Since projecting a matrix onto the set P is as easy as projecting a matrix onto the set
{A (cid:23) 0 (cid:107)A(cid:107) ≤ 1}  we assume for the simplicity of the presentation that the set on which we opti-
mize is indeed P = {A (cid:23) 0 (cid:107)A(cid:107) ≤ 1}.
We proceed with presenting a simpler algorithm for this problem than the one given for general SDP.
i ◦ A with respect to A is a symmetric rank one matrix and here we have the
The gradient of yiv(cid:48)
iv(cid:48)(cid:62)
following useful fact that was previously stated in [18].
Theorem 5.1. If A ∈ Rn×n is positive semi deﬁnite  v ∈ Rn and α ∈ R then the matrix B =
A + αvv(cid:62) has at most one negative eigenvalue.

(11)

The proof is due to the eigenvalue Interlacing Theorem (see [19] pp. 94-97 and [20] page 412).
Thus after performing a gradient step improvement of the form Yt+1 = Xt + ηyiviv(cid:62)
i   projecting
Yt+1 onto to the feasible set P comes down to the removal of at most one eigenvalue in case we
subtracted a rank one matrix (yit = −1) or normalizing the l2 norm in case we added a rank
one matrix (yit = 1). Since in practice computing eigenvalues fast  using the Power or Lanczos
methods  can be done only up to a desired approximation  in fact the resulting projection Xt+1
might not be positive semideﬁnite. Nevertheless  we show by care-full analysis that we can still
settle for a single eigenvector computation in order to compute an approximated projection with the
price that Xt+1 (cid:23) −3I. That is Xt+1 might be slightly outside of the positive semideﬁnite cone.
The beneﬁt is an algorithm with improved performance over the general SDP algorithm since far
less eigenvalue computations are required than in Hazan’s algorithm.
The projection to the set P is carried out in lines 7-11. In line 7 we check if Yt+1 has a negative
eigenvalue and if so  we compute the corresponding eigenvector in line 8 and remove it in line 9. In
line 11 we normalize the l2 norm of the solution. The procedure Sample(Ai  Xt) will be detailed
later on when we discuss the running time.

The following Lemma is a variant of Zinkevich’s Online Gradient Ascent algorithm [21] suitable
for the use of approximated projections when Xt is not necessarily inside the set P.
Lemma 5.2. Consider a set of matrices A1  ...  AT ∈ Rn×n such that (cid:107)Ai(cid:107) ≤ 1. Let X0 = 0n×n
and for all t ≥ 0 let

Yt+1 = Xt + ηAt 

˜Xt+1 = arg min

X∈P (cid:107)Yt+1 − X(cid:107)

6

Algorithm 2 SublinearPseudoMetric
1: Input:  > 0  Ai = yiviv(cid:62)

2: Let T ← 602−2 log m  X1 =← 0n×n  w1 ← 1m  η ←(cid:113) log m

i ∈ Rn×n for i ∈ [m].

T .

pt ← wt(cid:107)wt(cid:107)1
Choose it ∈ [m] by it ← i w.p. pt(i).
Yt+1 ← Xt +
if yi < 0 and λmin(Yt+1) < 0 then

T yitvitv(cid:62)

(cid:113) 2

u ← arg minz:(cid:107)z(cid:107)=1 z(cid:62)Yt+1z
Yt+1 = Yt+1 − λuu(cid:62)

it

.

3: for t = 1 to T do
4:
5:
6:
7:
8:
9:
10:
11: Xt+1 ←
12:
13:
14:
15:
16: end for
17: return ¯X = 1
T

end if
for i ∈ [m] do

end for

and let Xt+1 be such that

Yt+1

max {1 (cid:107)Yt+1(cid:107)}

vt(i) ← clip(Sample(Ai  Xt)  1/η)
wt+1(i) ← wt(i)(1 − ηvt(i) + η2vt(i)2)

(cid:80)

t Xt

(cid:13)(cid:13)(cid:13) ˜Xt+1 − Xt+1
(cid:88)
At ◦ X − (cid:88)

t∈[T ]

max
X∈P

t∈[T ]

(cid:13)(cid:13)(cid:13) ≤ d. Then  for a proper choice of η it holds that 

√

At ◦ Xt ≤

2T +

3
2

dT 3/2

The following lemma states the connection between the precision used in eigenvalues approximation
in lines 7-8  and the quality of the approximated projection.
Lemma 5.3. Assume that on each iteration t of the algorithm  the eigenvalue computation in
4T 1.5 additive approximation of the smallest eigenvalue of Yt+1 and let ˜Xt =
line 7 is a δ = d
arg minX∈P (cid:107)Yt − X(cid:107). It holds that

(cid:107) ˜Xt − Xt(cid:107) ≤ d

Theorem 5.4. Algorithm SublinearPseudoMetric computes an  additive approximation to (11) w.p.
1/2.

Proof. Combining lemmas 5.2  5.3 we have 

At ◦ Xt ≤

√

2T +

3
2

dT 3/2

√
Setting d = 2P
T
3

where P is the same as in theorem 4.5 yields 
√

At ◦ Xt ≤

2T + P T

(cid:88)

t∈[T ]

max
X∈P

At ◦ X − (cid:88)
At ◦ X − (cid:88)
(cid:88)

t∈[T ]

t∈[T ]

arg max
X∈P

t∈[T ]

The rest of the proof follows the same lines as theorem 4.5.

matrices. That is Xt is of the form Xt = (cid:80)

We move on to discus the time complexity of the algorithm. It is easily observed from the algorithm
that for all t ∈ [T ]  the matrix Xt can be represented as the sum of kt ≤ 2T symmetric rank-one
i   (cid:107)zi(cid:107) = 1 for all i. Thus instead of
computing Xt explicitly  we may represent it by the vectors zi and scalars αi. Denote by α the
vector of length kt in which the ith entry is just αi  for some iteration t ∈ [T ]. Since (cid:107)Xt(cid:107) ≤
1 it holds that (cid:107)α(cid:107) ≤ 1. The sampling procedure Sample(Ai  Xt) in line 13  returns the value
Ai(j l)(cid:107)α(cid:107)2
k(cid:107)α(cid:107)2 · (zk(j)zk(l))2. That is we ﬁrst sample a vector zi according to
zk(j)zk(l)αk

with probability α2

i∈[kt] αiziz(cid:62)

7

α and then we sample an entry (j  l) according to the chosen vector zi. It is easily observed that
˜vt(i) = Sample(Ai  Xt) is an unbiased estimator of Ai ◦ Xt. It also holds that:

(cid:88)

(cid:18) α2
k(cid:107)α(cid:107)2 (zk(j)zk(l))2 · Ai(j  l)2(cid:107)α(cid:107)4

(zk(j)zk(l))2α2
k

(cid:19)

E[˜vt(i)2] =

j∈[n] l∈[n] k∈[kt]

= kt(cid:107)α(cid:107)2(cid:107)Ai(cid:107)2 = ˜O(−2)

Thus taking ˜vt(i) to be the average of ˜O(−2) i.i.d samples as described above yields an unbiased
estimator of Ai · Xt with variance at most 1 as required for the analysis of our algorithm.
We can now state the running time of the algorithm.

Lemma 5.5. Algorithm SublinearPseudoMetric can be implemented to run in time ˜O(cid:0) m

(cid:1).

4 + n
6.5

Proof. According the lemmas 5.3  5.4  the required precision in eigenvalue approximation is
O(1)T 2 .
Using the Lanczos method for eigenvalue approximation and the sparse representation of Xt de-
scribed above  a single eigenvalue computation takes ˜O(n−4.5) time per iteration. Estimating the
products Ai ◦ Xt on each iteration takes by the discussion above ˜O(m−2). Overall the running
time on all iteration is as stated in the lemma.



6 Conclusions

We have presented the ﬁrst sublinear time algorithm for approximate semi-deﬁnite programming  a
widely used optimization framework in machine learning. The algorithm’s running time is optimal
up to poly-logarithmic factors and its dependence on ε - the approximation guarantee. The algorithm
is based on the primal-dual approach of [15]  and incorporates methods from previous SDP solvers
[12].
For the problem of learning peudo-metrics  we have presented further improvements to the basic
method which entail an algorithm that performs O( log n
ε2 ) iterations  each encompassing at most one
approximate eigenvector computation.

Acknowledgements

This work was supported in part by the IST Programme of the European Community  under the
PASCAL2 Network of Excellence  IST-2007-216886. This publication only reﬂects the authors’
views.

References

[1] Michel. X. Goemans and David P. Williamson. Improved approximation algorithms for maxi-
mum cut and satisﬁability problems using semideﬁnite programming. In Journal of the ACM 
volume 42  pages 1115–1145  1995.

[2] Sanjeev Arora  Satish Rao  and Umesh Vazirani. Expander ﬂows  geometric embeddings and
graph partitioning. In Proceedings of the thirty-sixth annual ACM symposium on Theory of
computing  STOC ’04  pages 222–231  2004.

[3] Amit Agarwal  Moses Charikar  Konstantin Makarychev  and Yury Makarychev. O(sqrt(log
n)) approximation algorithms for min uncut  min 2cnf deletion  and directed cut problems. In
Proceedings of the thirty-seventh annual ACM symposium on Theory of computing  STOC ’05 
pages 573–581  2005.

[4] Sanjeev Arora  James R. Lee  and Assaf Naor. Euclidean distortion and the sparsest cut. In
Proceedings of the thirty-seventh annual ACM symposium on Theory of computing  STOC ’05 
pages 553–562  2005.

[5] Eric P. Xing  Andrew Y. Ng  Michael I. Jordan  and Stuart Russell. Distance metric learn-
ing  with application to clustering with side-information. In Advances in Neural Information
Processing Systems 15  pages 505–512  2002.

8

[6] Alexandre d’Aspremont  Laurent El Ghaoui  Michael I. Jordan  and Gert R. G. Lanckriet. A di-
rect formulation of sparse PCA using semideﬁnite programming. In SIAM Review  volume 49 
pages 41–48  2004.

[7] Gert R. G. Lanckriet  Nello Cristianini  Laurent El Ghaoui  Peter Bartlett  and Michael I.
Jordan. Learning the kernel matrix with semi-deﬁnite programming. In Journal of Machine
Learning Research  pages 27–72  2004.

[8] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press 

2004.

[9] Philip Klein and Hsueh-I Lu. Efﬁcient approximation algorithms for semideﬁnite programs
arising from max cut and coloring. In Proceedings of the twenty-eighth annual ACM sympo-
sium on Theory of computing  STOC ’96  pages 338–347  1996.

[10] Sanjeev Arora  Elad Hazan  and Satyen Kale. Fast algorithms for approximate semide.nite
In Proceedings of the 46th
programming using the multiplicative weights update method.
Annual IEEE Symposium on Foundations of Computer Science  FOCS ’05  pages 339–348 
2005.

[11] Sanjeev Arora and Satyen Kale. A combinatorial  primal-dual approach to semideﬁnite pro-
grams. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing 
STOC ’07  pages 227–236  2007.

[12] Elad Hazan. Sparse approximate solutions to semideﬁnite programs. In Proceedings of the 8th

Latin American conference on Theoretical informatics  LATIN’08  pages 306–316  2008.

[13] Garud Iyengar  David J. Phillips  and Clifford Stein. Feasible and accurate algorithms for

covering semideﬁnite programs. In SWAT  pages 150–162  2010.

[14] Garud Iyengar  David J. Phillips  and Clifford Stein. Approximating semideﬁnite packing

programs. In SIAM Journal on Optimization  volume 21  pages 231–268  2011.

[15] Kenneth L. Clarkson  Elad Hazan  and David P. Woodruff. Sublinear optimization for ma-
chine learning. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of
Computer Science  FOCS ’10  pages 449–457  2010.

[16] Elad Hazan.

Approximate convex optimization by online game playing.

abs/cs/0610119  2006.

CoRR 

[17] Kenneth L. Clarkson  Elad Hazan  and David P. Woodruff. Sublinear optimization for machine

learning. CoRR  abs/1010.4408  2010.

[18] Shai Shalev-shwartz  Yoram Singer  and Andrew Y. Ng. Online and batch learning of pseudo-

metrics. In ICML  pages 743–750  2004.

[19] James Hardy Wilkinson. The algebric eigenvalue problem. Claderon Press  Oxford  1965.
[20] Gene H. Golub and Charles F. Van Loan. Matrix computations. John Hopkins University

Press  1989.

[21] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In ICML  pages 928–936  2003.

9

,Sébastien Gerchinovitz
Tor Lattimore
Haw-Shiuan Chang
Erik Learned-Miller
Andrew McCallum
Junnan Li
Yongkang Wong
Qi Zhao
Mohan Kankanhalli