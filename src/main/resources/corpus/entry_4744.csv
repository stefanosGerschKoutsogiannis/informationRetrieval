2014,Robust Bayesian Max-Margin Clustering,We present max-margin Bayesian clustering (BMC)  a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models  as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints  and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model  which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets  and the results indicate superior clustering performance of our methods compared to related baselines.,Robust Bayesian Max-Margin Clustering

Changyou Chen†

Jun Zhu‡

†Dept. of Electrical and Computer Engineering  Duke University  Durham  NC  USA
‡State Key Lab of Intelligent Technology & Systems; Tsinghua National TNList Lab;
‡Dept. of Computer Science & Tech.  Tsinghua University  Beijing 100084  China

Xinhua Zhang(cid:93)

(cid:93)Australian National University (ANU) and National ICT Australia (NICTA)  Canberra  Australia
cchangyou@gmail.com; dcszj@tsinghua.edu.cn; xinhua.zhang@anu.edu.au

Abstract

We present max-margin Bayesian clustering (BMC)  a general and robust frame-
work that incorporates the max-margin criterion into Bayesian clustering models 
as well as two concrete models of BMC to demonstrate its ﬂexibility and effective-
ness in dealing with different clustering tasks. The Dirichlet process max-margin
Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the
underlying Gaussian assumption of Dirichlet process Gaussian mixtures by in-
corporating max-margin posterior constraints  and is able to infer the number of
clusters from data. We further extend the ideas to present max-margin cluster-
ing topic model  which can learn the latent topic representation of each document
while at the same time cluster documents in the max-margin fashion. Extensive
experiments are performed on a number of real datasets  and the results indicate
superior clustering performance of our methods compared to related baselines.

1

Introduction

Existing clustering methods fall roughly into two categories. Deterministic clustering directly op-
timises some loss functions  while Bayesian clustering models the data generating process and in-
fers the clustering structure via Bayes rule. Typical deterministic methods include the well known
kmeans [1]  nCut [2]  support vector clustering [3]  Bregman divergence clustering [4  5]  and the
methods built on the very effective max-margin principle [6–9]. Although these methods can ﬂexi-
bly incorporate constraints for better performance  it is challenging for them to ﬁnely capture hidden
regularities in the data  e.g.  automated inference of the number of clusters and the hierarchies un-
derlying the clusters. In contrast  Bayesian clustering provides favourable convenience in modelling
latent structures  and their posterior distributions can be inferred in a principled fashion. For ex-
ample  by deﬁning a Dirichlet process (DP) prior on the mixing probability of Gaussian mixtures 
Dirichlet process Gaussian mixture models [10] (DPGMM) can infer the number of clusters in the
dataset. Other priors on latent structures include the hierarchical cluster structure [11–13]  co-
clustering structure [14]  etc. However  Bayesian clustering is typically difﬁcult to accommodate
external constraints such as max-margin. This is because under the standard Bayesian inference
designing some informative priors (if any) that satisfy these constraints is highly challenging.
To address this issue  we propose Bayesian max-margin clustering (BMC)  which allows max-
margin constraints to be ﬂexibly incorporated into a Bayesian clustering model. Distinct from the
traditional max-margin clustering  BMC is fully Bayesian and enables probabilistic inference of the
number of clusters or the latent feature representations of data. Technically  BMC leverages the
regularized Bayesian inference (RegBayes) principle [15]  which has shown promise on supervised
learning tasks  such as classiﬁcation [16  17]  link prediction [18]  and matrix factorisation [19] 
where max-margin constraints are introduced to improve the discriminative power of a Bayesian

1

model. However  little exploration has been devoted to the unsupervised setting  due in part to the
absence of true labels that makes it technically challenging to enforce max-margin constraints. BMC
constitutes a ﬁrst extension of RegBayes to the unsupervised clustering task. Note that distinct from
the clustering models using maximum entropy principle [20  21] or posterior regularisation [22] 
BMC is more general due to the intrinsic generality of RegBayes [15].
We demonstrate the ﬂexibility and effectiveness of BMC by two concrete instantiations. The ﬁrst is
Dirichlet process max-margin Gaussian mixture (DPMMGM)  a nonparametric Bayesian clustering
model that relaxes the Gaussian assumption underlying DPGMM by incorporating max-margin con-
straints  and is able to infer the number of clusters in the raw input space. To further discover latent
feature representations  we propose the max-margin clustering topic model (MMCTM). As a topic
model  it performs max-margin clustering of documents  while at the same time learns the latent
topic representation for each document. For both DPMMGM and MMCTM  we develop efﬁcient
MCMC algorithms by exploiting data augmentation techniques. This avoids imposing restrictive
assumptions such as in variational Bayes  thereby facilitating the inference of the true posterior. Ex-
tensive experiments demonstrate superior clustering performance of BMC over various competitors.

2 Regularized Bayesian Inference
We ﬁrst brieﬂy overview the principle of regularised Bayesian inference (RegBayes) [15].
The motivation of RegBayes is to enrich the posterior of a probabilistic model by incorporating ad-
ditional constraints  under an information-theoretical optimisation formulation. Formally  suppose a
probabilistic model has latent variables Θ  endowed with a prior p(Θ) (examples of Θ will be clear
soon later). We also have observations X := {x1 ···   xn}  with xi ∈ Rp. Let p(X|Θ) be the
likelihood. Then  posterior inference via the Bayes’ theorem is equivalent to solving the following
optimisation problem [15]:

KL(q(Θ) || p(Θ)) − EΘ∼q(Θ) [log p(X|Θ)]

(1)
where P is the space of probability distribution1  q(Θ) is the required posterior (here and afterwards
we will drop the dependency on X for notation simplicity). In other words  the Bayesian posterior
p(Θ|X) is identical to the optimal solution to (1). The power of RegBayes stems in part from
the ﬂexibility of engineering P  which typically encodes constraints imposed on q(Θ)  e.g.  via
expectations of some feature functions of Θ (and possibly the data X). Furthermore  the constraints
can be parameterised by some auxiliary variable ξ. For example  ξ may quantify the extent to which
the constraints are violated  then it is penalised in the objective through a function U. To summarise 
RegBayes can be generally formulated as

inf

q(Θ)∈P

KL(q(Θ) || p(Θ))− EΘ∼q(Θ) [log p(X|Θ)]+ U (ξ)

ξ q(Θ)

inf
s.t. q(Θ) ∈ P(ξ).

(2)
To distinguish from the standard Bayesian posterior  the optimal q(Θ) is called post-data posterior.
Under mild regularity conditions  RegBayes admits a generic representation theorem to characterise
the solution q(Θ) [15]. It is also shown to be more general than the conventional Bayesian methods 
including those methods that introduce constraints on a prior. Such generality is essential for us
to develop a Bayesian framework of max-margin clustering. Note that like many sophisticated
Bayesian models  posterior inference remains as a key challenge of developing novel RegBayes
models. Therefore  one of our key technical contributions is on developing efﬁcient and accurate
algorithms for BMC  as detailed below.

3 Robust Bayesian Max-margin Clustering
For clustering  one key assumption of our model is that X forms a latent cluster structure. In partic-
ular  let each cluster be associated with a latent projector ηk ∈ Rp  which is included in Θ and has
prior distribution subsumed in p(Θ). Given any distribution q on Θ  we then deﬁne the compatibility
score of xi with respect to cluster k by using the marginal distribution on ηk (as ηk ∈ Θ):

(cid:2)ηT

Fk(xi) = Eq(ηk)

k xi

(cid:3) = Eq(Θ)

(cid:2)ηT

k xi

(cid:3) .

(3)

1In theory  we also require that q is absolutely continuous with respect to p to make the KL-divergence well

deﬁned. The present paper treats this constraint as an implicit assumption for clarity.

2

For each example xi  we introduce a random variable yi valued in Z+  which denotes its cluster
assignment and is also included in Θ. Inspired by conventional multiclass SVM [7  23]  we utilize
P(ξ) in RegBayes (2) to encode the max-margin constraints based on Fk(xi)  with the slack variable
ξ penalised via their sum in U (ξ). This amounts to our Bayesian max-margin clustering (BMC):

inf

L(q(Θ)) + 2c

ξi≥0 q(Θ)
s.t. Fyi(xi) − Fk(xi) ≥ (cid:96) I(yi (cid:54)= k) − ξi 

i

∀i  k

(4)

(cid:88)

ξi

(cid:16)

(cid:20)

where L(q(Θ)) = KL(q(Θ)||p(Θ)) − EΘ∼q(Θ)[log p(X|Θ)] measures the KL divergence between
q and the original Bayesian posterior p(Θ|X) (up to a constant); I(·) = 1 if · holds true  and
0 otherwise; (cid:96) > 0 is a constant scalar of margin. Note we found that the commonly adopted
balance constraints in max-margin clustering models [6] either are unnecessary or do not help in our
framework. We will address this issue in speciﬁc models.
Clearly by absorbing the slack variables ξ  the optimisation problem (4) is equivalent to

L(q(Θ)) + 2c

EΘ∼q(Θ)[ζik]

i

max

inf
q(Θ)

0  max
k:k(cid:54)=yi

(5)
where ζik := (cid:96) I(yi (cid:54)= k) − (ηyi − ηk)T xi. Exact solution to (5) is hard to compute. An alternative
approach is to approximate the posterior by assuming independence between random variables  e.g.
variational inference. However  this is usually slow and susceptible to local optimal. In order to
obtain an analytic optimal distribution q that facilitates efﬁcient Bayesian inference  we resort to the
technique of Gibbs classiﬁer [17] which approximates (in fact  upper bounds due to the convexity
of max function) the second term in (5) by an expected hinge loss  i.e.  moving the expectation out
of the max. This leads to our ﬁnal formulation of BMC:

(cid:88)

(cid:17)

(cid:16)

(cid:17)(cid:21)

L(q(Θ)) + 2c

inf
q(Θ)

EΘ∼q(Θ)

max

0  max
k:k(cid:54)=yi

ζik

.

(6)

(cid:88)

i

Problem (6) is still much more challenging than existing RegBayes models [17]  which are restricted
to supervised learning with two classes only. Speciﬁcally  BMC allows multiple clusters/classes in
an unsupervised setting  and the latent cluster membership yi needs to be inferred. This complicates
the model and brings challenges for posterior inference  as addressed below.
In a nutshell  our
inference algorithms rely on two key steps by exploring data augmentation techniques. First  in order
to tackle the multi-class case  we introduce auxiliary variables si := arg maxk:k(cid:54)=yi ζik. Applying
standard derivations in calculus of variation [24] and augmenting the model with {si}  we obtain an
analytic form of the optimal solution to (6) by augmenting Θ (refer to Appendix A for details):

q(Θ {si}) ∝ p(Θ|X)

exp(−2c max(0  ζisi )) .

(7)

(cid:89)

i

(cid:89)

Second  since the max term in (7) obfuscates efﬁcient sampling  we apply the augmentation tech-
nique introduced by [17]  which showed that q(Θ {si}) is identical to the marginal distribution of
the augmented post-data posterior

˜φi(λi|Θ) 

q(Θ {si} {λi}) ∝ p(Θ|X)

2

− 1
i

exp(cid:0) −1

(λi + cζisi)2(cid:1). Here λi is an augmented variable for xi that has

where ˜φi(λi|Θ) := λ
an generalised inverse Gaussian distribution [25] given Θ and xi.
Note that our two steps of data augmentation are exact and incur no approximation. With the aug-
mented variables ({si} {λi})  we can develop efﬁcient sampling algorithms for the augmented
posterior q(Θ {si} {λi}) without restrictive assumptions  thereby allowing us to approach the true
target posterior q(Θ) by dropping the augmented variables. The details will become clear soon in
our subsequent clustering models.

2λi

i

(8)

4 Dirichlet Process Max-margin Gaussian Mixture Models
In (4)  we have left unspeciﬁed the prior p(Θ) and the likelihood p(X|Θ). This section presents an
instantiation of Bayesian nonparametric clustering for non-Gaussian data. We will present another
instantiation of max-margin document clustering based on topic models in next section.

3

ηk

Λk

µk
∞

w

α

yi

xi

N

ω

γ

v

v

ν  S

r  m

α0

α1

α

µ0

µk

K

yi

µi

ηk

K

β

φt

T

zil

wil

Ni

D

Figure 1: Left: Graphical model of DPMMGM. The part excluding ηk and v corresponds to
DPGMM. Right: Graphical model of MMCTM. The one excluding {ηk} and the arrow between
yi and wil corresponds to CTM.

Here a convenient model of p(X  Θ) is mixture of Gaussian. Let the mean and variance of the k-th
cluster component be µk and Λk. In a nonparametric setting  the number of clusters is allowed to
be inﬁnite  and the cluster yi that each data point belongs to is drawn from a Dirichlet process [10].
To summarize  the latent variables are Θ = {µk  Λk  ηk}∞
k=1 ∪{yi}n
i=1. The prior p(Θ) is speciﬁed
as: µk and Λk employ a standard Normal-inverse Wishart prior [26]:

µk ∼ N (µk; m  (rΛk)−1)  and Λk ∼ IW(Λk; S  ν).

(9)
yi ∈ Z+ has a Dirichlet process prior with parameter α. ηk follows a normal prior with mean 0 and
variance vI  where I is the identity matrix. The likelihood p(xi|Θ) is N (xi; µyi  (rΛyi)−1)  i.e.
independent of ηk. The max-margin constraints take effects in the model via ˜φi’s in (8). Note this
model of p(Θ  X)  apart from ηk  is effectively the Dirichlet process Gaussian mixture model [10]
(DPGMM). Therefore  we call our post-data posterior q(Θ {si} {λi}) in (8) as Dirichlet process
max-margin Gaussian mixture model (DPMMGM). The hyperparameters include m  r  S  ν  α  v.
Interpretation as a generalised DP mixture The formula of the augmented post-data posterior
in (8) reveals that  compared with DPGMM  each data point is associated with an additional factor
˜φi(λi|Θ). Thus we can interpret DPMMGM as a generalised DP mixture with Normal-inversed
Wishart-Normal as the base distribution  and a generalised pseudo likelihood that is proportional to
(10)
To summarise  DPMMGM employs the following generative process with the graphical model
shown in Fig. 1 (left):

(µk  Λk  ηk) ∼ N(cid:0)µk; m  (rΛk)−1(cid:1) × IW (Λk; S  ν) × N (ηk; 0  vI)   k = 1  2 ···

f (xi  λi|yi  µyi  Λyi {ηk}) := N (xi; µyi  (rΛyi )−1) ˜φi (λi|Θ) .

w ∼ Stick-Breaking(α) 

yi|w ∼ Discrete(w) 

i ∈ [n]
(xi  λi)|yi {µk  Λk  ηk} (cid:39) f (xi  λi|yi  µyi  Λyi  {ηk}).
i ∈ [n]
Here [n] := {1 ···   n} is the set of integers up to n and (cid:39) means that (xi  λi) is generative from a
distribution that is proportional to f (·). Since this normalisation constant is shared by all samples
xi  there is no need to deal with it by posterior inference. Another beneﬁt of this interpretation
is that it allows us to use existing techniques for non-conjugate DP mixtures to sample the cluster
indicators yi efﬁciently  and to infer the number of clusters in the data. This approach is different
from previous work on RegBayes nonparametric models where truncated approximation is used to
deal with the inﬁnite dimensional model space [15  18]. In contrast  our method does not rely on any
approximation. Note that DPMMGM does not need the complicated class balance constraints [6]
because the Gaussians in the pseudo likelihood would balance the clusters to some extent.
Posterior inference Posterior inference for DPMMGM can be done by efﬁcient Gibbs sam-
pling. We integrate out the inﬁnite dimension vector w  so the variables needed to be sampled
are {µk  Λk  ηk}k ∪{yi  si  λi}i. Conditional distributions are derived in Appendix B. Note that we
use an extension of the Reused Algorithm [27] to jointly sample (yi  si)  which allows it to allocate
to empty clusters in Bayesian nonparametric setting. The time complexity is almost the same as
DPGMM except for the additional step to sample ηk  with cost O(p3). So it would be necessary to
put the constraints on a subspace (e.g.  by projection) of the original feature space when p is high.

4

Dir(αµyi).

(cid:80)Ni

Ni

l=1

i=1 ∪ {zil}D Ni

i=1 l=1.

5 Max-margin Clustering Topic Model
Although many applications exhibit clustering structures in the raw observed data which can be
effectively captured by DPMMGM  it is common that such regularities are more salient in terms
of some high-level but latent features. For example  topic distributions are often more useful than
word frequency in the task of document clustering. Therefore  we develop a max-margin clustering
topic model (MMCTM) in the framework of BMC  which allows topic discovery to co-occur with
document clustering in a Bayesian and max-margin fashion. To this end  the latent Dirichlet alloca-
tion (LDA) [28] needs to be extended by introducing a cluster label into the model  and deﬁne each
cluster as a mixture of topic distributions. This cluster-based topic model [29] (CTM) can then be
used in concert with BMC to enforce large margin between clusters in the posterior q(Θ).
Let V be the size of the word vocabulary  T be the number of topics  and K be the number of clusters 
1N be a N-dimensional one vector. Then the generative process of CTM for the documents goes as:
1. For each topic t  generate its word distribution φt: φt|β ∼ Dir(β1V ).
2. Draw a base topic distribution µ0: µ0|α0 ∼ Dir(α01T ). Then for each cluster k  generate its
topic distribution mixture µk: µk|α1  µ0 ∼ Dir(α1µ0).
3. Draw a base cluster distribution γ: γ|ω ∼ Dir(ω1K). Then for each document i ∈ [D]:

i=1 in the posterior  thus Θ = {yi}D

• Generate a cluster label yi and a topic distribution µi: yi|γ ∼ Discrete(γ)  µi|α  µyi ∼
• Generate the observed words wil: zil ∼ Discrete(µi)  wil ∼ Discrete(φzil )  ∀ l ∈ [Ni].
Fig. 1 (right) shows the structure. We then augment CTM with max-margin constraints  and get
the same posterior as in Eq. (7)  with the variables Θ corresponding to {φt}T
k=1 ∪
{µ0  γ} ∪ {µi  yi}D
Compared with the raw word space which is normally extremely high-dimensional and sparse  it
is more reasonable to characterise the clustering structure in the latent feature space–the empirical
latent topic distributions as in the MedLDA [16]. Speciﬁcally  we summarise the topic distribution
of document i by xi ∈ RT   whose t-th element is 1
I(zil = t). Then the compatibility score
for document i with respect to cluster k is deﬁned similar to (3) as Fk(xi) = Eq(Θ)
however  the expectation is also taken over xi since it is not observed.
k=1 ∪
Posterior inference To achieve fast mixing  we integrate out {φt}T
{µi}D
i=1 l=1. The integration is straightfor-
ward by the Dirichlet-Multinomial conjugacy. The detailed form of the posterior and the conditional
distributions are derived in Appendix C. By extending CTM with max-margin  we note that many
of the the sampling formulas are extension of those in CTM [29]  with additional sampling for ηk 
thus the sampling can be done fairly efﬁciently.
Dealing with vacuous solutions Different from DPMMGM  the max-margin constraints in MM-
CTM do not interact with the observed words wil  but with the latent topic representations xi (or zil)
that are also inferred from the model. This easily makes the latent representation zi’s collapse into a
single cluster  a vacuous solution plaguing many other unsupervised learning methods as well. One
remedy is to incorporate the cluster balance constraints into the model [7]. However  this does not
help in our Bayesian setting because apart from signiﬁcant increase in computational cost  MCMC
often fails to converge in practice2. Another solution is to morph the problem into a weakly semi-
supervised setting  where we assign to each cluster a few documents according to their true label
(we will refer to these documents as landmarks)  and sample the rest as in the above unsupervised
setting. These “labeled examples” can be considered as introducing constraints that are alternative
to the balance constraints. Usually only a very small number of labeled documents are needed  thus
barely increasing the cost in training and labelling. We will focus on this setting in experiment.
6 Experiments
6.1 Dirichlet Process Max-margin Gaussian Mixture

t=1 ∪ {µ0  γ} ∪ {µk}K

i=1∪{ηk}K

k=1∪{zil}D Ni

t=1 ∪ {ηk  µk}K

(cid:2)ηT

k xi

(cid:3). Note 

2We observed the cluster sizes kept bouncing with sampling iterations. This is probably due to the highly
nonlinear mapping from observed word space to the feature space (topic distribution)  making the problem
multi-modal  i.e.  there are multiple optimal topic assignments in the post-data posterior (8). Also the balance
constraints might weaken the max-margin constraints too much.

5

We ﬁrst show the distinction between our DPMMGM and DPGMM
by running both models on the non-Gaussian half-rings data set [30].
There are a number of hyperparameters to be determined  e.g. 
(α  r  S  ν  v  c  (cid:96)); see Section 4. It turns out the cluster structure is
insensitive to (α  r  S  ν)  and so we use a standard sampling method
to update α [31]  while r  ν  S are sampled by employing Gamma 
truncated Poisson  inverse Wishart priors respectively  as is done
in [32]. We set v = 0.01  c = 0.1  (cid:96) = 5 in this experiment. Note that
the clustering structure is sensitive to the values of c and (cid:96)  which will
be studied below. Empirically we ﬁnd that DPMMGM converges
much faster than DPGMM  both converging well within 200 iter-
ations (see Appendix D.4 for examples).
In Fig. 2  the clustering
structures demonstrate clearly that DPMMGM relaxes the Gaussian
assumption of the data distribution  and correctly ﬁnds the number of
clusters based on the margin boundary  whereas DPGMM produces
a too fragmented partition of the data for the clustering task.
Parameter sensitivity We next study the sensitivity of hyperparameters c and (cid:96)  with other hyperpa-
rameters sampled during inference as above. Intuitively the impact of these parameters is as follows.
c controls the weight that the max-margin constraint places on the posterior. If there were no other
constraint  the max-margin constraint would drive the data points to collapse into a single cluster.
As a result  we expect that a larger value of the weight c will result in fewer clusters. Similarly 
increasing the value of (cid:96) will lead to a higher loss for any violation of the constraints  thus driving
the data points to collapse as well. To test these implications  we run DPMMGM on a 2-dimensional
synthetic dataset with 15 clusters [33]. We vary c and (cid:96) to study how the cluster structures change
with respect to these parameter settings. As can be observed from Fig. 3  the results indeed follow
our intuition  providing a mean to control the cluster structure in applications.

Figure 2: An illustration
of DPGMM (up) and DPM-
MGM (bottom).

(a) c :5e-6  (cid:96) :5e-1

(b) c :5e-4  (cid:96) :5e-1

(c) c :5e-3  (cid:96) :5e-1

(d) c :5e-2  (cid:96) :5e-1

(e) c :5e-1  (cid:96) :5e-1

(f) c :5e-3  (cid:96) :5e-4

(g) c :5e-3  (cid:96) :5e-2

(h) c :5e-3  (cid:96) :5e-1

(i) c :5e-3  (cid:96) :2

(j) c :5e-3  (cid:96) :5

Figure 3: Clustering structures with varied (cid:96) and c: (ﬁrst row) ﬁxed (cid:96) and increasing c; (second row)
ﬁxed c and increasing (cid:96). Lines are η’s. Clearly the number cluster decreases with growing c and (cid:96).

Real Datasets. As other clustering models  we test DPMMGM on ten real datasets (small to mod-
erate sizes) from the UCI repository [34]. Scaling up to large dataset is an interesting future. The
ﬁrst three columns of Table 1 list some of the statistics of these datasets (we used random subsets of
the three large datasets – Letter  MNIST  and Segmentation).
A heuristic approach for model selection. Model selection is generally hard for unsupervised
clustering. Most existing algorithms simply ﬁx the hyperparameters without examining their impacts
on model performance [10  35]. In DPMMGM  the hyperparameters c and (cid:96) are critical to clustering
quality since they control the number of clusters. Without training data in our setting they can not
be set using cross validation. Moreover  they are not feasible to be estimated use Bayesian sampling
as well because they are not parameters from a proper Bayesian model. we thus introduce a time-
efﬁcient heuristic approach to selecting appropriate values. Suppose the dataset is known to have
K clusters. Our heuristic goes as follows. First initialise c and (cid:96) to 0.1. Then at each iteration 
we compare the inferred number of clusters with K. If it is larger than K (otherwise we do the
n  where u is a uniform random
converse)  we choose c or (cid:96) randomly  and increase its value by u
variable in [0  1] and n is the number of iterations so far. According to the parameter sensitivity
studied above  increasing c or (cid:96) tends to decrease the number of clusters  and the model eventually

6

50505050505050505050246802468246802468246802468246802468246802468246802468246802468246802468246802468246802468Dataset

Glass

Half circle

Iris
Letter
MNIST
Satimage
Segment’n

Vehicle
Vowel
Wine

Data property
n
214
300
150
1000
1000
4435
1000
846
990
178

p
10
2
4
16
784
36
19
18
10
13

K kmeans
0.37±0.04
7
0.43±0.00
2
0.72±0.08
3
0.33±0.01
10
0.50±0.01
10
0.57±0.06
6
0.52±0.03
7
0.10±0.00
4
0.42±0.01
11
0.84±0.01
3

nCut

0.22±0.00
1.00±0.00
0.61±0.00
0.04±0.00
0.38±0.00
0.55±0.00
0.34±0.00
0.14±0.00
0.44±0.00
0.46±0.00

NMI

DPGMM DPMMGM DPMMGM∗
0.37±0.05
0.49±0.02
0.73±0.00
0.19±0.09
0.55±0.03
0.21±0.05
0.23±0.09
0.02±0.02
0.28±0.03
0.56±0.02

0.46±0.01
0.67±0.02
0.73±0.00
0.38±0.04
0.56±0.01
0.51±0.01
0.61±0.05
0.14±0.00
0.39±0.02
0.90±0.02

0.45±0.01
0.51±0.07
0.73±0.00
0.23±0.04
0.55±0.02
0.30±0.00
0.52±0.10
0.05±0.00
0.41±0.02
0.59±0.01

Table 1: Comparison for different methods on NMI scores. K: true number of clusters.

stabilises due to the stochastic decrement by u
n. We denote the model learned from this heuristic
as DPMMGM. In the case where the true number of clusters is unknown  we can still apply this
strategy  except that the number of clusters K needs to be ﬁrst inferred from DPGMM. This method
is denoted as DPMMGM∗.
Comparison. We measure the quality of clustering results by using the standard normalised mutual
information (NMI) criterion [36]. We compare our DPMMGM with the well established KMeans 
nCut and DPGMM clustering methods3. All experiments are repeated for ﬁve times with random
initialisation. The results are shown in Table 1. Clearly DPMMGM signiﬁcantly outperforms other
models  achieving the best NMI scores. DPMMGM∗  which is not informed of the true number of
clusters  still obtains reasonably high NMI scores  and outperforms the DPGMM model.
6.2 Max-margin Clustering Topic Model
Datasets. We test the MMCTM model on two document datasets: 20NEWS and Reuters-R8 . For
the 20NEWS dataset  we combine the training and test datasets used in [16]  which ends up with 20
categories/clusters with roughly balanced cluster sizes. It contains 18 772 documents in total with a
vocabulary size of 61 188. The Reuters-R8 dataset is a subset of the Reuters-21578 dataset4  with of
8 categories and 7 674 documents in total. The size of different categories is biased  with the lowest
number of documents in a category being 51 while the highest being 2 292.
Comparison We choose L ∈ {5  10  15  20  25} documents randomly from each category as the
landmarks  use 80% documents for training and the rest for testing. We set the number of topics
(i.e.  T ) to 50  and set the Dirichlet prior in Section 5 to ω = 0.1  β = 0.01  α = α0 = α1
= 10  as clustering quality is not sensitive to them. For the other hyperparameters related to the
max-margin constraints  e.g.  v in the Gaussian prior for η  the balance parameter c  and the cost
parameter (cid:96)  instead of doing cross validation which is computationally expensive and not helpful
for our scenario with few labeled data  we simply set v = 0.1  c = 9  (cid:96) = 0.1. This is found to
be a good setting and denoted as MMCTM. To test the robustness of this setting  we vary c over
{0.1  0.2  0.5  0.7  1  3  5  7  9  15  30  50} and keep v = (cid:96) = 0.1 ((cid:96) and c play similar roles and so
varying one is enough). We choose the best performance out of these parameter settings  denoted
as MMCTM∗  which can be roughly deemed as the setting for the optimal performance. We com-
pared MMCTM with state-of-the-art SVM and semi-supervised SVM (S3VM) models. They are
efﬁciently implemented in [37]  and the related parameters are chosen by 5-fold cross validation.
As in [16]  raw word frequencies are used as input features. We also compare MMCTM with a
Bayesian baseline–cluster based topic model (CTM) [29]  the building block of MMCTM without
the max-margin constraints. Note we did not compare with the standard MedLDA [16] because it
is supervised. We measure the performance by cluster accuracy  which is the proportion of cor-
rectly clustered documents. To accelerate MMCTM  we simply initialise it with CTM  and ﬁnd it
converges surprisingly fast in term of accuracy  e.g.  usually within 30 iterations (refer to Appendix

3We additionally show some comparison with some existing max-margin clustering models in Appendix D.2

on two-cluster data because their code only deals with the case of two clusters. Our method performs best.

4Downloaded from csmining.org/index.php/r52-and-r8-of-reuters-21578.html.

7

L

5
10
15
20
25

5
10
15
20
25

CTM

SVM

S3VM
20NEWS

17.22± 4.2
24.50± 4.5
22.76± 4.2
26.07± 7.2
27.20± 1.5

41.27± 16.7
42.63± 7.4
39.67± 9.9
58.24± 8.3
51.93± 5.9

37.13± 2.9
46.99± 2.4
52.80± 1.2
56.10± 1.5
59.15± 1.4

39.36± 3.2
47.91± 2.8
52.49± 1.4
54.44± 2.1
57.45± 1.7

Reuters-R8

78.12± 1.1
80.69± 1.2
83.25± 1.7
85.66± 1.0
84.95± 0.1

78.51± 2.3
79.15± 1.2
81.87± 0.8
73.95± 2.0
82.39± 1.8

MMCTM MMCTM∗

56.70± 1.9
54.92± 1.6
55.06± 2.7
56.62± 2.2
55.70± 2.4

79.18± 4.1
80.04± 5.3
85.48± 2.1
82.92± 1.7
86.56± 2.5

57.86± 0.9
56.56± 1.3
57.80± 2.2
59.70± 1.4
61.92± 3.0

80.86± 2.9
83.48± 1.0
86.86± 2.5
83.82± 1.6
88.12± 0.5

(a) 20NEWS dataset

(b) Reuters-R8 dataset

Table 2: Clustering acc. (in %). Bold means signiﬁcantly different.

Figure 4: Accuracy vs. #topic aaaaaaaaaaaaaaaa

Figure 5: 2-D tSNE embedding on 20NEWS for MMCTM (left) and CTM (right). Best viewed in
color. See Appendix D.3 for the results on Reuters-R8 datasets.

D.5). The accuracies are shown in Table 2  and we can see that MMCTM outperforms other models
(also see Appendix D.4)  except for SVM when L = 20 on the Reuters-R8 dataset. In addition 
MMCTM performs almost as well as using the optimal parameter setting (MMCTM∗).
Sensitivity to the number of topics (i.e.  T ). Note the above experiments simply set T = 50. To
validate the affect of T   we varied T from 10 to 100  and the corresponding accuracies are plotted
In Fig. 4 for the two datasets. In both cases  T = 50 seems to be a good parameter value.
Cluster embedding. We ﬁnally plot the clustering results by embedding them into the 2-
dimensional plane using tSNE [38]. In Fig. 5  it can be observed that compared to CTM  MMCTM
generates well separated clusters with much larger margin between clusters.
7 Conclusions
We propose a robust Bayesian max-margin clustering framework to bridge the gap between max-
margin learning and Bayesian clustering  allowing many Bayesian clustering algorithms to be di-
rectly equipped with the max-margin criterion. Posterior inference is done via two data augmenta-
tion techniques. Two models from the framework are proposed for Bayesian nonparametric max-
margin clustering and topic model based document clustering. Experimental results show our mod-
els signiﬁcantly outperform existing methods with competitive clustering accuracy.
Acknowledgments
This work was supported by an Australia China Science and Research Fund grant (ACSRF-06283)
from the Department of Industry  Innovation  Climate Change  Science  Research and Tertiary Ed-
ucation of the Australian Government  the National Key Project for Basic Research of China (No.
2013CB329403)  and NSF of China (Nos. 61322308  61332007). NICTA is funded by the Aus-
tralian Government as represented by the Department of Broadband  Communications and the Dig-
ital Economy and the Australian Research Council through the ICT Centre of Excellence program.

8

10203050701000204060Number of topics (#topic)Accuracy (%) trainingtest1020305070100020406080Number of topicsAccuracy (%) −50−40−30−20−1001020304050−60−40−200204060−50−40−30−20−1001020304050−60−40−200204060References
[1] J. MacQueen. Some methods of classiﬁcation and analysis of multivariate observations. In Proc. 5th

Berkeley Symposium on Math.  Stat.  and Prob.  page 281  1967.

[2] J. Shi and J. Malik. Normalized cuts and image segmentation. TPAMI  22(8):705–767  2000.
[3] A. Ben-Hur  D. Horn  H. Siegelmann  and V. Vapnik. Support vector clustering. JMLR  2:125–137  2001.
[4] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh. Clustering with Bregman divergences. JMLR  6:

[5] H. Cheng  X. Zhang  and D. Schuurmans. Convex relaxations of Bregman divergence clustering. In UAI 

[6] L. Xu  J. Neufeld  B. Larson  and D. Schuurmans. Max-margin clustering. In NIPS  2005.
[7] B. Zhao  F. Wang  and C. Zhang. Efﬁcient multiclass maximum margin clustering. In ICML  2008.
[8] Y. F. Li  I. W. Tsang  J. T. Kwok  and Z. H. Zhou. Tighter and convex maximum margin clustering. In

1705–1749  2005.

2013.

AISTATS  2009.

[9] G. T. Zhou  T. Lan  A. Vahdat  and G. Mori. Latent maximum margin clustering. In NIPS  2013.
[10] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In NIPS  2000.
[11] K. A. Heller and Z. Ghahramani. Bayesian hierarchical clustering. In ICML  2005.
[12] Y. W. Teh  H. Dau ´me III  and D. Roy. Bayesian agglomerative clustering with coalescents. In NIPS  2008.
[13] Y. Hu  J. Boyd-Graber  H. Dau ´me  and Z. I. Ying. Binary to bushy: Bayesian hierarchical clustering with

the Beta coalescent. In NIPS  2013.

bles. In SDM  2011.

inﬁnite latent SVMs. JMLR  2014.

[14] P. Wang  K. B. Laskey  C. Domeniconi  and M. I. Jordan. Nonparametric Bayesian co-clustering ensem-

[15] J. Zhu  N. Chen  and E. P. Xing. Bayesian inference with posterior regularization and applications to

[16] J. Zhu  A. Ahmed  and E. P. Xing. MedLDA: Maximum margin supervised topic models. JMLR  13(8):

[17] J. Zhu  N. Chen  H. Perkins  and B. Zhang. Gibbs max-margin topic models with fast sampling algorithms.

[18] J. Zhu. Max-margin nonparametric latent feature models for link prediction. In ICML  2012.
[19] M. Xu  J. Zhu  and B. Zhang. Fast max-margin matrix factorization with data augmentation. In ICML 

[20] L. Wang  X. Li  Z. Tu  and J. Jia. Discriminative cllustering via generative feature mapping. In AAAI 

2237–2278  2012.

In ICML  2013.

2013.

2012.

In NIPS  2010.

[21] R. Gomes  A. Krause  and P. Perona. Discriminative clustering by regularized information maximization.

[22] K.‘Ganchev  J. Graa  J. Gillenwater  and B. Taskar. Posterior regularization for structured latent variable

models. JMLR  11:2001–2049  2010.

chines. JMLR  2:265–292  2001.

[23] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector ma-

[24] C. Fox. An introduction to the calculus of variations. Courier Dover Publications  1987.
[25] B. Jorgensen. Statistical properties of the generalized inverse Gaussian distribution. Lecture Notes in

Statistics  1982.

[26] K. P. Murphy. Conjugate Bayesian analysis of the Gaussian distribution. Technical report  UCB  2007.
[27] S. Favaro and Y. W. Teh. MCMC for normalized random measure mixture models. Stat. Sci.  2013.
[28] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. JMLR  3:993–1022  2003.
[29] H. M. Wallach. Structured topic models for language. PhD thesis  University of Cambridge  2008.
[30] A. Jain and M. Law. Data clustering: A user’s dilemma. Lecture Notes in Comp. Sci.  3776:1–10  2005.
[31] Y.W. Teh  M.I. Jordan  M.J. Beal  and D.M. Blei. Hierarchical Dirichlet processes. J. Amer. Statist.

[32] M. Davy and J. Y. Tourneret. Generative supervised classiﬁcation using Dirichlet process priors. TPAMI 

Assoc.  101(476):1566–1581  2006.

32(10):1781–1794  2010.

[33] P. Franti and O. Virmajoki. Iterative shrinking method for clustering problems. PR  39(5):761–765  2006.
[34] K. Bache and M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.

uci.edu/ml.

[35] A. Shah and Z. Ghahramani. Determinantal clustering process – a nonparametric bayesian approach to

kernel based semi-supervised clustering. In UAI  2013.

[36] N. X. Vinh  J. Epps  and J. Bailey. Information theoretic measures for clusterings comparison: variants 

properties  normalization and correction for chance. JMLR  (11):2837–2854  2010.

[37] V. Sindhwani  P. Niyogi  and M. Belkin. SVMlin: Fast linear SVM solvers for supervised and semi-
In NIPS Workshop on Machine Learning Open Source Software  2006. http:

supervised learning.
//vikas.sindhwani.org/svmlin.html.

[38] L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-SNE. JMLR  9(11):

2579–2605  2008.

9

,Vibhav Vineet
Carsten Rother
Philip Torr
Changyou Chen
Jun Zhu
Xinhua Zhang
Sida Wang
Arun Tejasvi Chaganty
Percy Liang
Gamaleldin Elsayed
Simon Kornblith
Quoc Le