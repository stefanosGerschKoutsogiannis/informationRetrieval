2018,Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks,The performance of neural networks on high-dimensional data
  distributions suggests that it may be possible to parameterize a
  representation of a given high-dimensional function with
  controllably small errors  potentially outperforming standard
  interpolation methods.  We demonstrate  both theoretically and
  numerically  that this is indeed the case.  We map the parameters of
  a neural network to a system of particles relaxing with an
  interaction potential determined by the loss function.  We show that
  in the limit that the number of parameters $n$ is large  the
  landscape of the mean-squared error becomes convex and the
  representation error in the function scales as $O(n^{-1})$.
  In this limit  we prove a dynamical variant of the universal
  approximation theorem showing that the optimal
  representation can be attained by stochastic gradient
  descent  the algorithm ubiquitously used for parameter optimization
  in machine learning.  In the asymptotic regime  we study the
  fluctuations around the optimal representation and show that they
  arise at a scale $O(n^{-1})$.  These fluctuations in the landscape
  identify the natural scale for the noise in stochastic gradient
  descent.  Our results apply to both single and multi-layer neural
  networks  as well as standard kernel methods like radial basis
  functions.,Parameters as interacting particles: long time

convergence and asymptotic error scaling of neural

networks

Courant Institute of Mathematical Sciences

Courant Institute of Mathematical Sciences

Eric Vanden-Eijnden

New York University
eve2@cims.nyu.edu

Grant M. Rotskoff

New York University

rotskoff@cims.nyu.edu

Abstract

The performance of neural networks on high-dimensional data distributions sug-
gests that it may be possible to parameterize a representation of a given high-
dimensional function with controllably small errors  potentially outperforming
standard interpolation methods. We demonstrate  both theoretically and numer-
ically  that this is indeed the case. We map the parameters of a neural network
to a system of particles relaxing with an interaction potential determined by the
loss function. We show that in the limit that the number of parameters n is large 
the landscape of the mean-squared error becomes convex and the representation
error in the function scales as O(n−1). In this limit  we prove a dynamical variant
of the universal approximation theorem showing that the optimal representation
can be attained by stochastic gradient descent  the algorithm ubiquitously used
for parameter optimization in machine learning. In the asymptotic regime  we
study the ﬂuctuations around the optimal representation and show that they arise
at a scale O(n−1). These ﬂuctuations in the landscape identify the natural scale
for the noise in stochastic gradient descent. Our results apply to both single and
multi-layer neural networks  as well as standard kernel methods like radial basis
functions.

1

Introduction

The methods and models of machine learning are rapidly becoming de facto tools for the analysis and
interpretation of large data sets. The ability to synthesize and simplify high-dimensional data raises
the possibility that neural networks may also ﬁnd applications as efﬁcient representations of known
high-dimensional functions. In fact  these techniques have already been explored in the context of
free energy calculations [1]  partial differential equations [2  3]  and forceﬁeld parameterization [4].
Yet determining the optimal set of parameters or “training” a given neural network remains one of
the central challenges in applications due to the slow dynamics of training [5] and the complexity
of the objective function [6  7]. Parameter optimization in machine learning typically relies on the
stochastic gradient descent algorithm (SGD)  which makes an empirical estimate of the gradient of
the objective function over a small number of sample points [5]. SGD has been analyzed in some
cases—for example  when the problem is known to be convex  as in the over-parameterized limit or
other idealized settings [8  9  10  11]  there are rigorous guarantees of convergence and estimates of
convergence rates [12].
While ﬁnding the best set of parameters is computationally challenging  we have strong theoretical
guarantees that neural networks can represent a large class of functions. The universal approximation
theorems [13  14  15] ensure the existence of a (possibly large) set of parameters that bring a neural
network arbitrarily close to a given function over a compact domain. A similar statement has been

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

proved for radial basis functions [16]. However  the proofs of the universal approximation theorems
do not ensure that any particular optimization technique can locate the ideal set of parameters.
Parameters as particles—In order to study the properties of stochastic gradient descent for neural
network optimization  we recast the standard training procedure in terms of a system of interacting
particles [17]. In doing so  we give an exact rewriting of stochastic gradient descent as a stochastic
differential equation with multiplicative noise  which has been studied previously [18  19]. We
interpret the limiting behavior of the parameter optimization via a nonlinear Liouville equation for the
time evolution of a parameter distribution [20]. This framework provides analytical tools to determine
a Law of Large Numbers for the convergence of the optimization and to derive scaling results for the
error term as time and the number of parameters grow large. A similar perspective has been adopted
concurrently by Mei et al. [21]  Chizat and Bach [22]  and Sirignano and Spiliopoulos [23]  which
study the “mean ﬁeld limit”  similar to our Law of Large Numbers  but not asymptotic ﬂuctuations or
error scaling.
Convergence and asymptotic dynamics of stochastic gradient descent—We demonstrate that the
optimization problem becomes convex in the limit n → ∞ and we show that both gradient descent
and SGD convergence to the global minimum [24  21]. This argument shows that the universal
approximation theorem can be obtained as the limit of a stochastic gradient based optimization
procedure under an appropriate choice of hyper-parameters. In the scaling limit  our analysis gives
bounds on the error of a representation and characterizes the asymptotic ﬂuctuations in that error.
Convergence to the optimum to ﬁrst order occurs rapidly  i.e. on O(1) timescales. Diminishing the
error at next order requires quenching the noise in the dynamics on O(log n) time scales.
Implications of noise in descent dynamics—Our results give an explicit theoretical explanation for
the observation that additional noise in can lead to better generalization for neural networks [25  26];
local minima of depth O(n−1) are washed out by the noise of SGD.
Numerical experiments—We verify the scaling predicted by our asymptotic arguments for single
layer neural networks. Because it is impossible to determine the exact interaction potential in general 
we carry out numerical experiments using stochastic gradient descent for ReLU neural networks. We
use the p-spin energy function [27  28] as the target function due to its complexity as the dimension
grows large.
Key assumptions—In order to derive the stochastic partial differential equation for SGD  we effec-
tively assume the large data limit. Because we are focusing on function approximation we can always
generate new training data by sampling random points in the domain of the function and evaluating
the target function at those points. The partial differential equation for gradient descent represents
the evolution of the parameters on the true loss landscape  i.e.  the large data limit. In this limit  the
dynamics is similar to online algorithms for stochastic gradient descent [5].

2 Parameters as particles
Given a function f : Ω → R deﬁned on a compact set Ω ⊂ Rd  consider its approximation by

(1)
where n ∈ N  ϕ : Ω × D → R is some kernel and (ci  yi) ∈ R × D with D ⊂ RN . The ci and yi
are parameters to be learned for i = 1  . . .   n. We place the following assumption on the kernel: for
any test function h 

ciϕ(x  yi)

fn(x) =

n(cid:88)

i=1

1
n

(cid:90)

∀y ∈ D :

Ω

h(x)ϕ(x  y)dµ(x) = 0 ⇒ h(x) = 0 

∀x ∈ Ω 

(2)

where µ is some positive measure on Ω (for example the Lebesgue measure  dµ(x) = dx). This
condition is satisﬁed for nonlinearities typically encountered in machine learning; a neural network
with any number of layers using a positive nonlinear activation function (e.g.  ReLU  sigmoid) will
clearly satisfy this property if the linear coefﬁcients are non-zero. The property above is similar to the
discriminatory kernel condition in Cybenko [13]. Our results apply to radial basis functions  single
layer neural networks  and multilayer neural networks in which the ﬁnal layer is scaled with n. In
particular  the statements we make require a “wide” ﬁnal layer but are still applicable to networks
with multiple layers.

2

By “training” the representation  we mean that we seek to optimize the parameters so as to minimize
the mean-squared error loss function 

(cid:90)
Ω |f (x) − fn(x)|2 dµ(x).

(cid:96)(f  fn) = 1
2

(3)

n(cid:88)

i=1

n(cid:88)

1
2n

In this case we have chosen to employ the mean-squared error and we can view (cid:96)(f  fn) as an “energy”
function for the parameters {(ci  yi)}n

i=1 

E(c1  y1  . . .   cn  yn) := n ((cid:96)(f  fn) − Cf ) =

(cid:82)
Ω |f (x)|2 dµ(x) is a constant unaffected by the optimization and we have deﬁned
(5)

ϕ(x  y)ϕ(x  z)dµ(x).

f (x)ϕ(x  y)dµ(x)

cicjK(yi  yj)

ciF (yi) +

K(y  z) =

F (y) =

(cid:90)

(cid:90)

i j=1

(4)

where Cf = 1
2

Ω

Ω

Directly optimizing the coefﬁcients to minimize the loss function (cid:96) is challenging in general because
we do not have any guarantee of convexity. However  these difﬁculties can be conceptually alleviated
by instead writing the objective function in terms of a weighted distribution

Gn : D → R  Gn(y) =

1
n

ciδ(y − yi)

(6)

which converges weakly to some G(y) as n → ∞  a fact which we describe in detail below. Convo-
lution with this weighted distribution provides a convenient expression for the function representation

fn(x) =

ciϕ(x  y)δ(y − yi)dy ≡ ϕ (cid:63) Gn.

(7)

Interestingly  in the limit that n → ∞ the objective function for the optimization becomes convex in
terms of the signed distribution 

(cid:90)
Ω |f (x) − (ϕ (cid:63) G)(x)|2 dµ(x).

(cid:96)(f  ϕ (cid:63) G) = 1
2

(8)

meaning that a unique minimum value of the loss function can be attained for a not necessarily unique
minimizer G∗ for which (cid:96)(f  ϕ (cid:63) G∗) = 0. This observation formalizes the statements made by
Bengio et al. in Ref. [24]. While the objective function is convex  it is by no means trivial to optimize
the weighted distribution. Writing the loss function in this language gives us a perspective that can
be exploited to derive the scaling of the error in arbitrary neural networks trained with stochastic
gradient descent.

3 Gradient descent

We ﬁrst discuss the case of gradient descent for which we provide derivations of a law of large
numbers (LLN) and central limit theorem (CLT) for the optimization dynamics. These statements
reveal the scaling in the representation error and the analysis has synergies which are useful in
deriving LLN and CLT for stochastic gradient descent. Detailed arguments for the propositions stated
here are provided in the supplementary material.
The gradient descent dynamics is given by coupled ordinary differential equations for the weight and

n(cid:88)

i=1

(cid:90)

1
n

D

n(cid:88)

i=1

the parameters of the kernel 

1
n

˙Y i = Ci∇F (Yi) −
n(cid:88)

˙Ci = F (Yi) −

1
n

n(cid:88)

j=1

CjK(Yi  Yj)

j=1

3

CiCj∇K(Y i  Yj) 

(9)

with initial conditions sampled independently from a probability distribution ρin(y  c) with full
support in the domain D × R. We analyze the evolution of the parameters by studying the “particle”
distribution
(10)

n(cid:88)

ρn(t  y  c) =

δ(c − Ci(t))δ(y − Y i(t))

1
n

i=1

the ﬁrst moment of which is the weighted distribution (6) 

Gn(t  y) =

cρn(t  y  c)dc =

Ci(t)δ(y − Y i(t)).

(cid:82) ϕ(x  y)Gn(t  y)dy. Taking the limit n → ∞  we see that the zeroth order term of the distri-

We can express the function representation in terms of the distribution as fn(t  x) =

bution has smooth initial data ρ0(0) = ρin by the Law of Large Numbers. In Sec S1.1 we derive a
nonlinear partial differential equation satisﬁed by ρ0  essentially by applying the chain rule:

i=1

(cid:90)

n(cid:88)

1
n

where

∂tρ0 = ∇ · (c∇U ([ρ0]  y)ρ0) + ∂c (U ([ρ0]  y)ρ0)  

(cid:90)

U ([ρ]  y) = −F (y) +

D×R

c(cid:48)K(y  y(cid:48))ρ(y(cid:48)  c(cid:48))dy(cid:48)dc(cid:48)

The PDE (12) is gradient descent in Wasserstein metric on a convex energy functional of the density
(cf. Sec. S1.2.1); we refer to this type of equation as a nonlinear Liouville equation.

3.1 Law of large numbers

The limiting equation (12) is a well-posed and deterministic nonlinear partial integro-differential
equation. We can express it in terms of the target function f (x) by denoting

(cid:90)

D×R

(cid:90)

(11)

(12)

(13)

(14)

(15)

(16)

f0(t  x) =

cϕ(x  y)ρ0(t  y  c)dydc

and we see that

where the symmetric kernel function M is given by

∂tf0(t  x) = −

Ω

M ([ρ0(t)]  x  x(cid:48)) (f0(t  x) − f (x)) dµ(x(cid:48))

(cid:0)c2∇yϕ(x  y) · ∇yϕ(x(cid:48)  y) + ϕ(x  y)ϕ(x(cid:48)  y)(cid:1) ρ(y  c) dydc.

(cid:90)

D×R

M ([ρ]  x  x(cid:48)) =

This kernel is positive deﬁnite and symmetric implying that the only stable ﬁxed point is f0 = f if
ρ0(t = 0) = ρin > 0  as discussed in Sec S1.2. Fixed points of the gradient ﬂow that are not energy
minimizers exist  but they are not dynamically accessible from the initial density that we use (cf. [22]
and Sec S1.2).

Proposition 3.1 (LLN for gradient descent) Let fn(t) = fn(t  x) = (cid:80)n

i=1 Ci(t)ϕ(x  Yi(t))
(9) for the initial condition where each pair

where {Yi(t)  Ci(t)}n
(Y i(0)  Ci(0)) is sampled independently from ρin > 0. Then

i=1 are the solution of

fn(t) = f0(t)

lim
n→∞

Pin-almost surely

where f0(t) solves (15) and satisﬁes

f0(t) = f

a.e. in Ω.

lim
t→∞

(17)

(18)

In addition  the limits in n and t commute  i.e. we also have limn→∞ limt→∞ fn(t) = f.
A detailed derivation of the LLN for gradient descent can be found in Sec. S1.2. The LLN should
be understood as a guarantee that gradient descent reaches the optimal representation for initial
conditions sampled iid from a smooth distribution with full support on D × R.

4

3.2 Central Limit Theorem and asymptotic ﬂuctuations and error

To study the ﬂuctuations around the optimal representation we look at the discrepancy between
fn(t  x) and f0(t  x). These ﬂuctuations are on the scale O(n−1/2) initially and diminish as the
optimization progresses to reach scale O(n−1) or below  as summarized in the next two propositions.
Proposition 3.2 (CLT for GD) Let fn(t) be as in Proposition 3.1. Then for any t < ∞ as n → ∞ 
we have
(19)

in distribution

n−1/2 (fn(t) − f0(t)) = f1/2(t)

where f0(t) solves (15) and f1/2(t) is a Gaussian process with mean zero and some given covariance
that satisﬁes f1/2(t) → 0 almost surely as t → ∞.
This result is derived in Sec. S2  where the covariance of f1/2(t) is also given (S46). Since f1/2(t)
converges to zero as t → ∞  it is useful to quantify the scale at which the ﬂuctuations settle on long
time scales:

lim
n→∞

Proposition 3.3 (Asymptotic error for GD) Under the same conditions as those in Proposition 3.2 
on any sequence an > 0 such that an/ log n → ∞ as n → ∞  we have

lim
n→∞

n−ξ (fn(an) − f ) = 0

almost surely for any ξ < 1

(20)

This proposition characterizes the asymptotic error of the neural network  showing that it goes as
fn = f + Cn−1 for some constant C ≥ 0. This scaling is more favorable than might be expected
from the initial condition because the order of the error “heals” from 1/2 to 1 in the long time limit.
That is  the error from the initial  non-optimal parameter selection decays during the optimization
dynamics  becoming much more favorable at late times.

4 Stochastic gradient descent

We cannot typically evaluate the integrals required to compute F (y) and K(y  y(cid:48)). Instead  at each
i=1 which we refer to
time step we estimate these functions using a small set of sample points {xi}P
as a batch of size P . Consequently  we introduce noise by sampling random data to make imperfect
estimates of the gradient of the objective function. To estimate the gradient of the loss we use an
unbiased estimator which is simply the sample mean over a collection or “batch” of P points

EP (z) =

n
2P

|fn(xi  z) − f (xi)|2

(21)

P(cid:88)

i=1

(cid:90)

where  for simplicity  we write the parameters as a single vector z = (c1  y1  . . . cn  yn) ∈ (D ×
R)n. Note that we have scaled the loss function by n so that ∇EP is O(1) because our function
representation is scaled by n−1. The evolution equation of the corresponding dynamical variable
Z(t) is

Z(t + ∆t) = Z(t) − ∆t∇EP (Z(t)).

(22)

The dynamics can be analyzed as a stochastic differential equation with a multiplicative noise term
arising from the approximate evaluation of the gradient of the loss function. To derive this dynamical
equation  we ﬁrst need the covariance which we can write explicitly:

n2

Ω

(fn − f )2 ∇fn ⊗ ∇fndµ − n2∇(cid:96)(f  fn) ⊗ ∇(cid:96)(f  fn) ≡

1
P

R(z).

(23)

where fn = fn(x  z) and f = f (x). The discretized dynamics (22) is statistically equivalent to the
stochastic differential equation

(24)
where E(z) is the energy (4) based on the exact loss  θ = ∆t/P   and the quadratic variation of the
noise is (cid:104)dB(t  z)  dB(t  z)(cid:105) = R(z)dt. The SDE (24) is not Langevin dynamics in the classical
sense because the noise has spatiotemporal correlations. In our case  because new data is sampled at

dZ = −∇zE(Z)dt + √θdB(t  Z)

5

every time step  there are no temporal correlations  which are a consequence of revisiting samples in
a training set. Written in terms of F and K  the parameters satisfy a collection of coupled SDEs that
we can use to study the evolution of ρn 



dY i = Ci(t)∇F (Yi(t))∆t −

Ci(t)Cj(t)∇K(Y i(t)  Yj(t))∆t + dBi 

(25)

dCi = F (Yi(t))∆t −

1
n

Cj(t)K(Y i(t)  Y j(t))∆t + dB(cid:48)i

n(cid:88)

j=1

1
n

n(cid:88)

j=1

where ∆t > 0 is the time step. The time evolution of the parameter distribution can be derived by
using the Itô formula  which in turn gives rise to a stochastic partial differential equation for the
time-evolution of ρn(t  c  y). This SPDE is

∂tρn = ∇ · (c∇U ([ρn]  y)ρn) + ∂c (U ([ρn]  y)ρn)
 

+ θD[ρn  y  y] + √θ (η(t  y) + η(t  c))

(26)

where D is a diffusive term given explicitly in Sec. S4.1 and which we do not reproduce here because
it does not contribute in the subsequent scaling. This equation can be viewed as an extension of
Dean’s equation [20] to a setting with multiplicative noise. The noise terms η and η (deﬁned in Eq.
S69) have a quadratic variation that diminishes as fn becomes close to f.

4.1 Law of large numbers

At ﬁrst  it may appear that we could choose an arbitrary expansion in powers of n−α for some
α > 0. However  as explained in Sec. S5  the expansion of ρnρ(cid:48)n contains terms of order n−1  which
constrains the choice of α. To perform an expansion  we take θ ∝ n−2α so that  in the limit n → ∞ 
ρ0 satisﬁes the same deterministic equation as in the case of gradient descent. This means that an
analogous statement to Proposition 3.1 holds:

Proposition 4.1 (LLN for SGD) Let fn(t) = fn(t  x) = (cid:80)n

{Yi(t)  Ci(t)}n
each pair (Y i(0)  Ci(0)) is sampled independently from ρin > 0. Then

i=1 Ci(t)ϕ(x  Yi(t)) with
i=1 solution to (24) with θ = an−2α  a > 0 α ∈ (0  1] and initial condition where

almost surely  where f0(t) solves (15). Furthermore 

fn(t) = f0(t)

lim
n→∞

f0(t) = f

a.e. in Ω.

lim
t→∞

(27)

(28)

In addition the limits commute  i.e. limn→∞ limt→∞ fn(t) = f.
The Law of Large Numbers implies the universal approximation theorem  but notable additional
information has emerged from our analysis. First  we emphasize that here we have obtained the
representation as the limit of a stochastic gradient descent optimization procedure. Secondly  the
PDE describing the time evolution of f0 is independent of n  meaning the rate of convergence in time
of fn does not depend on the number of parameters to leading order.

4.2 Asymptotic ﬂuctuations and error

A remarkable feature of stochastic gradient descent is that the scale of ﬂuctuations is controlled by
the accuracy of the representation. Roughly  the closer fn is to f  the smaller the discrepancy in their
gradients meaning that the variance of the noise term is also small. We make use of this property to
assess the asymptotic error for stochastic gradient descent:

Proposition 4.2 (Asymptotic error for SGD) Let fn(t) = fn(t  x) be as in Proposition 4.1. Then
for any an > 0 such that an/ log n → ∞ as n → ∞  we have

lim
n→∞

nα (fn(an) − f ) = 0

almost surely.

(29)

6

The discrepancy converges to zero almost surely with respect to the initial data as well as the statistics
of the noise terms in (24). In terms of the loss function  we have

(cid:96)(f  fn(an)) = 1
so that the following proposition holds:

2(cid:107)f − f0(an)(cid:107)2 − n−α (cid:104)f − f0(an)  fα(an)(cid:105) + 1

2 n−2α(cid:107)fα(an)(cid:107)2 + o(n−α) (30)

Proposition 4.3 Under the same conditions as those in Proposition 4.2  the loss function satisﬁes
(31)

nα(cid:96)(f  fn(an)) = 0

almost surely.

lim
n→∞

This means that the error at order n−1 can be quenched by increasing the batch size or decreasing the
time step as a function of the optimization time  e.g.  setting α = 1 by taking a batch of size n2.

5 Numerical experiments

To test our results  we will use a function known for its complex features in high-dimensions:

the spherical 3-spin model  which is a map from the d − 1 sphere of radius √d to the reals f :
Sd−1(√d) → R  given by

f (x) =

ap q rxpxqxr 

x ∈ Sd−1(√d) ⊂ Rd

(32)

d(cid:88)

1
d

p q r=1
p q r=1 are independent Gaussian random variables with mean zero and
where the coefﬁcients {ap q r}d
variance one. The function (32) is known to have a number of critical points that grows exponentially
with the dimensionality d [27  6  28]. We note that previous works have sought to draw a parallel
between the glassy 3-spin function and generic loss functions [7]  but we are not exploring such an
analogy here. Rather  we simply use the function (32) as a difﬁcult target for approximation by neural
networks. That is  throughout this section  we train networks to learn f with a particular realization of
ap q r and study the accuracy of that representation as a function of the number of particles n. In Fig. 1
we show the representation error by computing the loss as well as the discrepancy between the target
function and the neural network representation averaged over points at which the function is positive

(or negative)  i.e.  1/P(cid:80)P

i=1 (fn(xi) − f (xi)) Θ(f (xi)) where Θ is the Heaviside function.

Single layer sigmoid / ReLU neural network We consider the case that the nonlinear function h(x)
is max(0  x)  the restricted linear unit or ReLU activation function frequently used in large scale
applications of machine learning. In these experiments  we test the scaling in d = 50  prohibitively
high dimensional for any grid based method. We trained the networks with batch size P = 50 using
stochastic gradient descent with n = i × 104 for i = 1  . . .   6. For the two smallest networks  we
ran for 2 × 106 time steps with ∆t = 10−3 and then quenched with P = 2500 for 2 × 105 steps.
For the largest networks  we used ∆t = 5 × 10−4 to ensure stability and therefore doubled the
number of steps so that the total training time remained ﬁxed. Scaling data for the loss and the signed
discrepancy are shown in Fig. 1. We also looked at sigmoid nonlinearities in d = 10  25. These
networks were trained as above but with P = (cid:98)n/5(cid:99) with a quench of P 2.
6 Conclusions and outlook

We have introduced a perspective based on particle distribution functions that enables asymptotic
analysis of the optimization dynamics of neural networks. We have focused on the limit where the
number of parameters n → ∞  in which the objective function becomes convex and a stochastic
partial differential equation describes the time evolution of the parameters. Our results emphasize that
the optimal parameters in this limit are accessible via stochastic gradient descent (Proposition 4.1) and
that ﬂuctuations around the optimum can be controlled by modulating the batch size (Proposition 4.2).
Surprisingly  the dynamical evolution does not depend on n  suggesting that the rate of convergence
should be asymptotically independent of the number of parameters.
Our results do not address many features of neural network parameterization that merit further study
exploiting the mathematical tools that have been developed for particle systems. In particular  the
statements we have derived are insensitive to the details of network architecture  which is among the

7

Figure 1: Large ReLU networks in high dimension (d = 50)  and sigmoid neural networks in
intermediate dimensions (bottom two rows). In all cases  we see linear scaling of the empirical loss
averaged with P = 106. For the sigmoid neural networks  we also plot a measure of the discrepancy
between the functions  which also scales as O(n−1). In each plot  the error scaling as a function of
the width of the network is plotted for 10 distinct random realizations of the function deﬁned in (32)
with different colored stars for each realization.

most important considerations when designing or using a neural network. It would also be beneﬁcial
to explore the ways in which regularizing processes  drop-out  for example  affect the convergence of
the PDE. Developing a rigorous understanding of which kernels and which architectures are optimal
for different types of target functions remains a compelling goal that appears within reach using the
tools outlined here.

8

10210310−210−1‘P(f fn)(d=10)10210310−410−310−2h(f−fn)Θ(f)i10210310−310−2h(fn−f)Θ(−f)i102103100101‘P(f fn)(d=25)10210310−310−210−1100h(f−fn)Θ(f)i10210310−210−1100h(fn−f)Θ(−f)i1042×1043×1044×104n3×10−14×10−16×10−1‘P(f fn)(d=50)ReLUAcknowledgments

We would like to thank Andrea Montanari and Matthieu Wyart for useful discussions regarding
the ﬁxed points of gradient ﬂows in the Wasserstein metric. GMR was supported by the James
S. McDonnell Foundation. EVE was supported by National Science Foundation (NSF) Materials
Research Science and Engineering Center Program Award DMR-1420073; and by NSF Award
DMS-1522767.

References
[1] Elia Schneider  Luke Dai  Robert Q Topper  Christof Drechsel-Grau  and Mark E Tuckerman.
Stochastic Neural Network Approach for Learning High-Dimensional Free Energy Surfaces.
Physical Review Letters  119(15):150601  October 2017.

[2] Yuehaw Khoo  Jianfeng Lu  and Lexing Ying. Solving for high dimensional committor functions

using artiﬁcial neural networks. arXiv:1802.10275  February 2018.

[3] Jens Berg and Kaj Nyström. A uniﬁed deep artiﬁcial neural network approach to partial

differential equations in complex geometries. arXiv:1711.06464  November 2017.

[4] Jörg Behler and Michele Parrinello. Generalized Neural-Network Representation of High-

Dimensional Potential-Energy Surfaces. Physical Review Letters  98(14):583  April 2007.

[5] Léon Bottou and Yann L. Cun. Large Scale Online Learning. In S. Thrun  L. K. Saul  and
B. Schölkopf  editors  Advances in Neural Information Processing Systems 16  pages 217–224.
MIT Press  2004.

[6] Levent Sagun  V Ugur Guney  Gérard Ben Arous  and Yann LeCun. Explorations on high

dimensional landscapes. arXiv:1412.6615  December 2014.

[7] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.

The Loss Surfaces of Multilayer Networks. arXiv:1412.0233  November 2014.

[8] C Daniel Freeman and Joan Bruna. Topology and Geometry of Half-Rectiﬁed Network Opti-

mization. arXiv:1611.01540  November 2016.

[9] Luca Venturi  Afonso S Bandeira  and Joan Bruna. Neural Networks with Finite Intrinsic

Dimension have no Spurious Valleys. arXiv:1802.06384  February 2018.

[10] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error

guarantees for multilayer neural networks. arXiv:1605.08361  May 2016.

[11] K Fukumizu and S Amari. Local minima and plateaus in hierarchical structures of multilayer

perceptrons. Neural Networks  13(3):317–327  2000.

[12] Léon Bottou  Frank E Curtis  and Jorge Nocedal. Optimization Methods for Large-Scale

Machine Learning. arXiv:1606.04838  June 2016.

[13] G Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control 

Signals and Systems  2(4):303–314  December 1989.

[14] A R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE

Transactions on Information Theory  39(3):930–945  May 1993.

[15] Francis Bach. Breaking the Curse of Dimensionality with Convex Neural Networks. Journal of

Machine Learning Research  18(19):1–53  2017.

[16] J Park and I W Sandberg. Universal Approximation Using Radial-Basis-Function Networks.

Neural Computation  3(2):246–257  June 1991.

[17] Sylvia Serfaty. Systems of Points with Coulomb Interactions. arXiv:1712.04095  December

2017.

9

[18] Wenqing Hu  Chris Junchi Li  Lei Li  and Jian-Guo Liu. On the diffusion approximation of

nonconvex stochastic gradient descent. arXiv:1705.07562  May 2017.

[19] Qianxiao Li  Cheng Tai  and Weinan E. Stochastic modiﬁed equations and adaptive stochastic
gradient algorithms. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th
International Conference on Machine Learning  volume 70 of Proceedings of Machine Learning
Research  pages 2101–2110  International Convention Centre  Sydney  Australia  06–11 Aug
2017. PMLR.

[20] David S Dean. Langevin equation for the density of a system of interacting Langevin processes.

Journal of Physics A: Mathematical and Theoretical  29(24):L613–L617  January 1999.

[21] Song Mei  Andrea Montanari  and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of
two-layer neural networks. Proceedings of the National Academy of Sciences  115(33):E7665–
E7671  August 2018.

[22] Lénaïc Chizat and Francis Bach. On the Global Convergence of Gradient Descent for Over-

parameterized Models using Optimal Transport. arXiv:1805.09545  May 2018.

[23] Justin Sirignano and Konstantinos Spiliopoulos. Mean Field Analysis of Neural Networks.

arXiv:1805.01053  May 2018.

[24] Yoshua Bengio  Nicolas L. Roux  Pascal Vincent  Olivier Delalleau  and Patrice Marcotte.
Convex neural networks. In Y. Weiss  B. Schölkopf  and J. C. Platt  editors  Advances in Neural
Information Processing Systems 18  pages 123–130. MIT Press  2006.

[25] Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping
Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
Minima. arXiv:1609.04836  September 2016.

[26] Elad Hoffer  Itay Hubara  and Daniel Soudry. Train longer  generalize better: closing the

generalization gap in large batch training of neural networks. arXiv:1705.08741  May 2017.

[27] Antonio Aufﬁnger and Gérard Ben Arous. Complexity of random smooth functions on the

high-dimensional sphere. The Annals of Probability  41(6):4214–4247  November 2013.

[28] Antonio Aufﬁnger  Gérard Ben Arous  and Jiˇrí ˇCerný. Random Matrices and Complexity of

Spin Glasses. Communications on Pure and Applied Mathematics  66(2):165–201  2012.

10

,Grant Rotskoff
Eric Vanden-Eijnden