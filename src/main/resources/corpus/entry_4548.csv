2019,Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond,Several important families of computational and statistical results in machine learning and randomized algorithms rely on uniform bounds on quadratic forms of random vectors or matrices. Such results include the Johnson-Lindenstrauss (J-L) Lemma  the Restricted Isometry Property (RIP)  randomized sketching algorithms  and approximate linear algebra. The existing results critically depend on statistical independence  e.g.  independent entries for random vectors  independent rows for random matrices  etc.  which prevent their usage in dependent or adaptive modeling settings. In this paper  we show that such independence is in fact not needed for such results which continue to hold under fairly general dependence structures. In particular  we present uniform bounds on random quadratic forms of stochastic processes which are conditionally independent and sub-Gaussian given another (latent) process. Our setup allows general dependencies of the stochastic process on the history of the latent process and the latent process to be influenced by realizations of the stochastic process. The results are thus applicable to adaptive modeling settings and also allows for sequential design of random vectors and matrices. We also discuss stochastic process based forms of  J-L  RIP  and sketching  to illustrate the generality of the results.,Random Quadratic Forms with Dependence:
Applications to Restricted Isometry and Beyond

Arindam Banerjee

Qilong Gu

Vidyashankar Sivakumar

Zhiwei Steven Wu

Department of Computer Science & Engineering  University of Minnesota  Twin Cities

Minneapolis  MN 55455  USA

Abstract

Several important families of computational and statistical results in machine
learning and randomized algorithms rely on uniform bounds on quadratic forms
of random vectors or matrices. Such results include the Johnson-Lindenstrauss
(J-L) Lemma  the Restricted Isometry Property (RIP)  randomized sketching al-
gorithms  and approximate linear algebra. The existing results critically depend
on statistical independence  e.g.  independent entries for random vectors  inde-
pendent rows for random matrices  etc.  which prevent their usage in dependent
or adaptive modeling settings.
In this paper  we show that such independence
is in fact not needed for such results which continue to hold under fairly gen-
eral dependence structures. In particular  we present uniform bounds on random
quadratic forms of stochastic processes which are conditionally independent and
sub-Gaussian given another (latent) process. Our setup allows general dependen-
cies of the stochastic process on the history of the latent process and the latent
process to be inﬂuenced by realizations of the stochastic process. The results are
thus applicable to adaptive modeling settings and also allows for sequential design
of random vectors and matrices. We also discuss stochastic process based forms
of J-L  RIP  and sketching  to illustrate the generality of the results. 1

1

Introduction

Over the past few decades  a set of key developments in machine learning and randomized algo-
rithms have been relying on uniform large deviation bounds on quadratic forms involving random
vectors or matrices. The Restricted Isometry Property (RIP) is a well known and widely studied
result of this type  which has had a major impact in high-dimensional statistics [35  5  45  46]. The
Johnson-Lindenstrauss (J-L) Lemma is another well known result of this type  which has led to ma-
jor statistical and algorithmic advances in the context of random projections [25  2  23]. Similar
substantial developments have been made in several other contexts  including sketching algorithms
based on random matrices [49  26]  advances in approximate linear algebra [32  20]  among others.
Such existing developments in one way or another rely on uniform bounds on quadratic forms of
random vectors or matrices. Let A be a set of (m × n) matrices and ξ ∈ Rn be a sub-Gaussian
random vector [45  46]. The existing results stem from large deviation bounds of the following
random variable [28]:

CA(ξ) = sup
A∈A

2 − E(cid:107)Aξ(cid:107)2

2

Results such as RIP and J-L can then be obtained in a straightforward manner from such bounds by
converting the matrix A into a vector θ = vec(A) and converting ξ into a suitable random matrix X
to get bounds on

CΘ(X) = sup
θ∈Θ

2 − E(cid:107)Xθ(cid:107)2

2

1The full version of this paper is available at https://arxiv.org/abs/1910.04930 [6].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(cid:12)(cid:12)(cid:107)Aξ(cid:107)2
(cid:12)(cid:12)(cid:107)Xθ(cid:107)2

(cid:12)(cid:12) .
(cid:12)(cid:12)  

(1)

(2)

where Θ = {vec(A)|A ∈ A}. Results on other domains such as sketching [49  26] and approximate
linear algebra [32  20] can be similarly obtained. Further  note that such bounds are considerably
more general than the popular Hanson-Wright inequality [39  22] for quadratic forms of random
vectors  which focus on a ﬁxed matrix A instead of a uniform bound over a set A.
The key assumption in all existing results is that the entries ξj of ξ need to be statistically inde-
pendent. Such independence assumption shows up as element-wise independence of the random
vector ξ in quadratic forms like CA(ξ) and row-wise or element-wise independence of the random
matrix X in quadratic forms like CΘ(X). Existing analysis techniques  typically based on advanced
tools from empirical processes [45  30]  rely on such independence to get the large deviation bound.
In this paper  we consider a generalization of such existing results by allowing for statistical de-
pendence in ξ. In particular  we assume ξ = {ξj} to be a stochastic process where the marginal
random variables ξj are conditionally independent and sub-Gaussian given some other (latent) pro-
cess F = {Fj}. While hidden Markov models (HMMs) [7] are a simple example of such a setup 
with F being the latent variable sequence and ξ being the observations  our setup described in de-
tail in Section 2 allows for far more complex dependencies  and allows for many different types of
graphical models connecting ξ and F .
In Section 2 we discuss two key conditions such graphical
models need to satisfy and give a set of concrete examples of graphical models which satisfy the
conditions illustrating the ﬂexibility of the setup. Our main result is to establish a uniform large
deviation bound for CA(ξ) in (1) where ξ is any stochastic process following the setup outlined in
Section 2.
There are two broad implications of our results allowing for dependence in random quadratic forms.
First  there are several emerging domains where data collection  modeling and estimation take place
adaptively  including bandits learning  active learning  and time-series analysis [4  40  31]. The de-
pendence in such adaptive settings is hard to handle  and existing analysis for speciﬁc cases goes
to great lengths to work with or around such dependence [36  18  34]. The general tool we pro-
vide for such settings has the potential to simplify and generalize results in adaptive data collection 
e.g.  our results are applicable to the smoothed analysis of contextual linear bandits considered in
[27]. Second  since our results allow for sequential construction of random vectors and matrices
by considering what has happened so far  algorithmic approaches such as J-L and sketching would
arguably be able to take advantage of such extra ﬂexibility possibly leading to adaptive and more
computationally efﬁcient algorithms. In Section 4  we illustrate how such basic results on adap-
tive regression  RIP  and J-L would look like by allowing for dependence in the random vectors or
matrices.
The technical analysis for our main result is a signiﬁcant generalization of prior analysis on tail be-
havior of chaos processes [3  28  43] for random vectors with i.i.d. elements. To construct a uniform
bound on CA(ξ) in (1) for a stochastic process ξ with statistically dependent entries  we decom-
pose the analysis into two parts: 1) bounding the off-diagonal terms of AT A  and 2) bounding the
diagonal terms of AT A. Our analysis for the off-diagonal terms is based on two key tools: decou-
pling [38] and generic chaining [43]  both with suitable generalizations from i.i.d. counter-parts to
stochastic processes ξ. For decoupling  we present a new result on decoupling of quadratic forms
of sub-Gaussian stochastic processes ξ satisfying the conditions of our setup. Our result general-
izes the classical decoupling result for vectors with i.i.d. entries [38  28]. For generic chaining 
we develop new results of interest in our context as well as generalize certain existing results for
i.i.d. random vectors to stochastic processes. While generic chaining  as a technique  does not rely
on statistical independence [43]  an execution of the chaining argument does rely on an atomic large
deviation bound such as the Hoeffding bound for independent elements [28]. In our setting  the
atomic deviation bound in generic chaining carefully utilizes conditional independence satisﬁed by
the stochastic process ξ. Our analysis for the diagonal terms is based on suitable use of symmetriza-
tion  de-symmetrization  and contraction inequalities [8  29]. However  we cannot use the standard
form for symmetrization and de-symmetrization which are based on i.i.d. elements. We generalize
the classical symmetrization and de-symmetrization results [8] to stochastic processes ξ in our setup 
and subsequently utilize these inequalities to bound the diagonal terms. We present a gentle exposi-
tion to the analysis in Section 3 and the technical proofs are all in [6  Appendix]. We have tried to
make the exposition self-contained beyond certain key deﬁnitions and concepts such as Talagrand’s
γ-function and admissible sequence in generic chaining [43].
Notation. Our results are for stochastic processes ξ = {ξj} adapted to another stochastic process
F = {Fi} with both moment and conditional independence assumptions outlined in detail in Sec-
tion 2. We will consider conditional probabilities Xj = ξj|f1:j  where f1:j is a realization of F1:j 

2

Figure 1: Graphical Model 1 (GM1) structure for stochastic process {ξi} adapted to {Fi} satisﬁes
(SP-2) by construction. While we show arrows only from one random variable  e.g.  Fi−1 → ξi 
the conditional random variable ξi|F1:(i−1) can have dependence on the entire history F1:(i−1). All
these arrows are not depicted in this and other ﬁgures to avoid clutter.

(cid:107)A(cid:107)F = (cid:112)Tr(AT A) and the operator norm (cid:107)A(cid:107)2→2 = sup(cid:107)x(cid:107)2≤1 (cid:107)Ax(cid:107)2. For the set A  we

and assume Xj to be zero-mean L-sub-Gaussian  i.e.  P(|Xj| > τ ) ≤ 2 exp(−τ 2/L2) for some
constant L > 0 and all τ ≥ τ0  a constant [45  46]. For the exposition  we will call a random
variable sub-Gaussian without explicitly referring to the constant L. With n denoting the length
of the stochastic process  we will abuse notation and consider a random vector ξ = [ξj] ∈ Rn
corresponding to the stochastic process ξ = {ξj}  where the usage will be clear from the con-
text. Our results are based on two classes of complexity measures of a set of (m × n) matrices
A. The ﬁrst class  denoted by dF (A) and d2→2(A)  are the radius of A in the Frobenius norm
have dF (A) = supA∈A (cid:107)A(cid:107)F   and d2→2(A) = supA∈A (cid:107)A(cid:107)2→2. The second class is Talagrand’s
γ2(A (cid:107) · (cid:107)2→2) functional  deﬁned in Section 3 [43  42]. Recent literature have used the notion
of Gaussian width: w(A) = E supA∈A | Tr(GT A)| where G = [gi j] ∈ Rm×n have i.i.d. normal
entries  i.e.  gi j ∼ N (0  1). It can be shown [43] that γ2(A (cid:107) · (cid:107)2→2) can be bounded by the Gaus-
sian width w(A)  i.e.  γ2(A (cid:107) · (cid:107)2→2) ≤ cw(A)  for some constant c > 0. Our analysis will be
based on bounding Lp-norms of suitable random variables. For a random variable X  its Lp-norm
is (cid:107)X(cid:107)Lp = (E|X|p)1/p.
2 Setup

We describe the formal set up of stochastic processes for which we provide large deviation bounds.
Let ξ = {ξi} = {ξ1  . . .   ξn} be a sub-Gaussian stochastic process which is decoupled when condi-
tioned on another stochastic process F = {Fi} = {F1  . . .   Fn}. In particular  we assume:
(SP-1) for each i = 1  . . .   n  ξi|f1:i is a zero mean sub-Gaussian random variable [46] for all
(SP-2) for each i = 1  . . .   n  there exists an index (i) ≤ i which is non-decreasing  i.e.  (j) ≤

realizations f1:i of F1:i; and
(i) for j < i  such that ξi ⊥ ξj|F1:(i)  j < i and ξi ⊥ Fk|F1:(i)  k > (i).

where ⊥ denotes (conditional) independence. The stochastic process ξ = {ξi} is said to be adapted
to the process F = {Fi} satisfying (SP-1) and (SP-2).
(SP-1) is an assumption on the moments of the distributions ξi|f1:i. Note that the assumption allows
the speciﬁcs of the distribution to depend on the history. (SP-2) is an assumption on the conditional
independence structure of ξ. The assumption allows ξi to depend on the history F1:(i). Further  we
can have Fi−1 depending on ξi−1 and ξi depending on Fi−1. Graphical models GM1 (Figure 1) 
GM2 (Figure 2) and GM3 (Figure 3) are examples of graphical models satisfying (SP-2). For GM1 
(i) = i − 1 and Fi depends on F1:(i−1)  but not on ξi. Further  ξi can depend on the entire history
F1:(i−1). GM2 is a variant of GM1 and structurally resembles a HMM (hidden Markov model)
with (i) = i  Fi depending on Fi−1 (or the entire history F1:(i−1))  and ξi depends on Fi (or
the entire history F1:i). GM3 is a more complex model with (i) = i and Fi depends both on
F1:(i−1) and ξi. For GM1 and GM3  we consider an additional ‘prior’ F0  and the properties (SP-1)
and (SP-2) can be naturally extended to include such a prior. We also give concrete examples of
potential interest in the context of machine learning in Section 4. For certain graphical models  it
may be at times more natural to ﬁrst construct a stochastic process {Zi} respecting the graphical
model structure governed by (SP-2)  and then construct the sequence {ξi} by conditional centering 
i.e.  ξi|F1:i = Zi|F1:i − E[Zi|F1:i] so that E[ξi|F1:i] = 0 as required by (SP-1). Such a centered
construction is inspired by how one can construct martingale difference sequences (MDS) from
martingales [48].

3

F1F2F3…Fn-1Fnξ1ξ2ξ3…ξn-1ξnF0Figure 2: Graphical Model 2 (GM2) structure for stochastic process {ξi} adapted to {Fi} satisﬁes
(SP-2) by construction. While we show arrows only from one random variable  e.g.  Fi → ξi  the
conditional random variable ξi|F1:i can have dependence on the entire history F1:i.

Figure 3: Graphical Model 3 (GM3) structure for stochastic process {ξi} adapted to {Fi} sat-
isﬁes (SP-2) by construction. Note that there is no restriction on the conditional distribution
Fi | (F1:(i−1)  ξi)  so that Fi can have arbitrary dependence on F1:(i−1) and Zi. While we show
arrows only to one random variable  e.g.  Fi−1 → ξi  the conditional random variable ξi|F1:(i−1)
can have dependence on the entire history F1:(i−1). Similarly  Fi|F1:(i−1)  Zi is illustrated only with
arrows from Fi−1  Zi to Fi to avoid clutter.

3 Main Results
Let A be a set of (m × n) matrices and let ξ be a L-sub-Gaussian random vector. The random
variable of interest for the current analysis is:
CA(ξ) (cid:44) sup
A∈A

(cid:12)(cid:12)(cid:107)Aξ(cid:107)2

2 − E(cid:107)Aξ(cid:107)2

(cid:12)(cid:12) .

(3)

2

Based on the literature on empirical processes and generic chaining [43  30]  the random variable
CA(ξ) can be referred to as an order-2 sub-Gaussian chaos [43  28]. While widely used results like
the restricted isometry property (RIP) [10  19] and Johnson-Lindenstrauss (J-L) lemma [25  49] do
not explicitly appear in the above form  getting such results from a large deviation bound on CA(ξ)
is straightforward [28  33]. For ease of exposition  we will refer to such converted but otherwise
equivalent form as the random matrix form of CA(ξ).

3.1 The Main Result: Warm-up
The main technical result in the paper is a large deviation bound on CA(ξ) for the setting when ξ is
a stochastic process adapted to F satisfying (SP-1) and (SP-2)  as deﬁned in Section 2. To develop
large deviation bounds on CA(ξ)  we decompose the quadratic form into terms depending on the
off-diagonal and the diagonal elements of AT A respectively. First note that the contributions from
the off-diagonal terms of AT A to E(cid:107)Aξ(cid:107)2
2 is 0. To see this  with Aj denoting the jth column of A 
by linearity of expectation we have

n(cid:88)

(cid:2)Eξj  ξk|F1:n [ξjξk](cid:3)(cid:104)Aj  Ak(cid:105)

 n(cid:88)

j k=1
j(cid:54)=k

 =

n(cid:88)
n(cid:88)

j k=1
j(cid:54)=k

Eξ

ξjξk(cid:104)Aj  Ak(cid:105)

Eξj  ξk [ξjξk](cid:104)Aj  Ak(cid:105) =

EF1:n

(cid:2)Eξj|F1:n [ξj]Eξk|F1:n [ξk](cid:3)(cid:104)Aj  Ak(cid:105) (b)

j k=1
j(cid:54)=k

= 0  

(a)
=

EF1:n

j k=1
j(cid:54)=k

where (a) follows since ξj ⊥ ξk|F1:n by (SP-2)   and (b) follows since Eξj|F1:n [ξj] = Eξk|F1:n [ξk] =
0 by (SP-1).

4

F1F2F3…Fn-1Fnξ1ξ2ξ3…ξn-1ξnF1F2F3…Fn-1Fnξ1ξ2ξ3…ξn-1ξnF0Now  by deﬁnition and Jensen’s inequality  we have

(cid:12)(cid:12)

2 − E(cid:107)Aξ(cid:107)2

2

CA(ξ) = sup
A∈A

= sup
A∈A

≤ sup
A∈A

(cid:12)(cid:12)(cid:107)Aξ(cid:107)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

j k=1
j(cid:54)=k

j k=1
j(cid:54)=k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ξjξk(cid:104)Aj  Ak(cid:105) +

(|ξj|2 − E|ξj|2)(cid:107)Aj(cid:107)2

2

ξjξk(cid:104)Aj  Ak(cid:105)

(|ξj|2 − E|ξj|2)(cid:107)Aj(cid:107)2

2

n(cid:88)

j=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) + sup

A∈A

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

j=1

= BA(ξ) + DA(ξ)
Therefore  for any p ∈ [1 ∞)  we have

(cid:107)CA(ξ)(cid:107)Lp ≤ (cid:107)BA(ξ)(cid:107)Lp + (cid:107)DA(ξ)(cid:107)Lp .

(4)
Our approach to getting a large deviation bound for CA(ξ) is based on bounding (cid:107)CA(ξ)(cid:107)Lp 
which in turn is based on bounding (cid:107)BA(ξ)(cid:107)Lp and (cid:107)DA(ξ)(cid:107)Lp. For convenience  we will re-
fer to BA(ξ) as the off-diagonal term and DA(ξ) as the diagonal term. Such bounds lead to a bound
on (cid:107)CA(ξ)(cid:107)Lp of the form

(5)
where a  b  c are constants which do not depend on p. Note that by using the moment-generating
function and Markov’s inequality [48  44]  these Lp-norm bounds imply  for all u > 0

(cid:107)CA(ξ)(cid:107)Lp ≤ a +

p · b + p · c  

∀p ≥ 1  

√

P (|CA(ξ)| ≥ a + b · √
(cid:26)

P (|CA(ξ)| ≥ a + u) ≤ exp

− min

u + c · u) ≤ e−u  

(cid:19)(cid:27)

(cid:18) u2

4b2  

u
2c

(6)

(7)

 

or  equivalently

which yields the desired large deviation bound.

3.2 Upper Bounding BA(ξ) and DA(ξ)
The bound on (cid:107)BA(ξ)(cid:107)Lp is based on two techniques: decoupling [38] and generic chaining [43].
Our main result in decoupling extends the classical result for ξ with i.i.d. entries to stochastic pro-
cesses ξ satisfying (SP-1) and (SP-2). The second part of the analysis uses generic chaining [43  42]
which is arguably one of the most powerful tools for such analysis. Since we use generic chaining 
the results are in terms of Talagrand’s γ-functions [43] deﬁned below.

Deﬁnition 1 For a metric space (T  d)  an admissible sequence of T is a collection of subsets of T  
{Tr : r ≥ 0}  with |T0| = 1 and |Tr| ≤ 22r for all r ≥ 1. For β ≥ 1  the γβ functional is deﬁned by

γβ(T  d) = inf sup
t∈T

2r/βd(t  Tr)  

(8)

where the inﬁmum is over all admissible sequences of T .
In particular  our results are in terms of γ2(A (cid:107) · (cid:107)2→2)  which is related to the Gaussian width of
the set by the majorizing measure theorem [42  Theorem 2.1.1][43  Theorem 2.4.1]. Recent years
have seen major advances in using Gaussian width for both statistical and computational analysis in
the context of high-dimensional statistics and related areas [11  5  37  13]. Hence  recent tools for
bounding Gaussian width [11  13] can be applied to our setting to get concrete bounds for cases of

interest. For example  if A is a set of s-sparse (m × n) matrices  γ2(A (cid:107) · (cid:107)2→2) ≤ c(cid:112)s log(mn) 

for some constant c [30  45] (also see Section 4).
While the diagonal term DA(ξ) does not have any interaction terms of the form ξjξk  the term
depends on centered random variables |ξj|2 − E|ξj|2. Our analysis relies on three key results: sym-
metrization  de-symmetrization  and contraction [8  29]. Our overall approach reduces to showing
that upper bounds on DA(ξ) can be derived from upper bounds on DA(g)  where g has i.i.d. normal
entries  and additional terms which can be bounded using generic chaining [43]. Bounds on DA(g)
can be obtained using existing results [28].

5

∞(cid:88)

r=0

3.3 The Main Result
Based on the analysis above  we have our main result as stated below
Theorem 1 Let A be a set of (m × n) matrices and let ξ be a stochastic process adapted to F
satisfying (SP-1) and (SP-2). Let

(cid:18)

(cid:18)

M = γ2(A (cid:107) · (cid:107)2→2) ·

γ2(A (cid:107) · (cid:107)2→2) + dF (A)

V = d2→2(A) ·
2→2(A) .

U = d2

γ2(A (cid:107) · (cid:107)2→2) + dF (A)

(cid:19)

(cid:19)

(9)

(10)

(11)

 

(12)

Then  for any ε > 0 

(cid:18)

P

sup
A∈A

(cid:12)(cid:12)(cid:107)Aξ(cid:107)2

2 − E(cid:107)Aξ(cid:107)2

2

(cid:19)
(cid:12)(cid:12) ≥ c1M + ε

(cid:18)

≤ 2 exp

−c2 min

(cid:27)(cid:19)

(cid:26) ε2

V 2  

ε
U

where c1  c2 are constants which depend on the support.

(cid:18)

(cid:18)

(cid:19)

(cid:19)

It is instructive to compare our bounds for stochastic processes ξ satisfying (SP-1) and (SP-2) to the
sharpest existing bound on CA(ξ) for the special case when ξ has i.i.d. sub-Gaussian entries [28].
For this i.i.d. sub-Gaussian case  [28] showed a large deviation bound based on

M(cid:48) = γ2(A (cid:107) · (cid:107)2→2) ·

γ2(A (cid:107) · (cid:107)2→2) + dF (A)

V (cid:48) = d2→2(A) ·
2→2(A) .
U(cid:48) = d2

γ2(A (cid:107) · (cid:107)2→2) + dF (A)

+ dF (A) · d2→2(A)

(13)

(14)

(15)
By comparing the terms with those in Theorem 1  we note that U = U(cid:48) and V = V (cid:48) and while
M(cid:48) has an extra additional term dF (A) · d2→2(A)  for symmetric sets A with A = −A we have
d2→2(A) ≤ γ2(A (cid:107) · (cid:107)2→2)  so the terms are of the same order. Thus  the generalization to the
stochastic process ξ yields the same order bound as the i.i.d. case which allows seamless extension
of applications of the result to random vectors/matrices with statistical dependence.
Finally  our results can be extended to the case of non-zero mean stochastic processes. In particular
with x = ξ + µ  where ξ is the stochastic process satisfying (SP-1) and (SP-2) and µ is the mean
vector  i.e.  E[x] = µ  we have (cid:107)Ax(cid:107)2 − E(cid:107)Ax(cid:107)2
2) + (cid:104)ξ  2AT Aµ(cid:105)  where
the ﬁrst term is what we analyze and bound in Theorem 1  and the second term is a linear form of ξ.
For the uniform bound  the two terms can be separated using Jensen’s inequality  the ﬁrst term can
be bounded using Theorem 1 and the second term can be bounded using a standard application of
generic chaining using (SP-1) and (SP-2). Thus  mean shifted versions of our results also hold.

2 − E(cid:107)Aξ(cid:107)2

2 = ((cid:107)Aξ(cid:107)2

4

Implications of the Main Results

We show several applications of our results  including the Johnson Lindenstrauss (J-L)  Restricted
Isometry Property (RIP)  and sketching. All proofs can be found in [6  Section 4].

Johnson-Lindenstrauss with Stochastic Processes

4.1
Let X ∈ Rn×p   n < p and let A be any set of N vectors in Rp. X is a Johnson-Lindenstrauss
transform (JLT) [25  2] if for any ε > 0 

(1 − ε)(cid:107)u(cid:107)2

2 ≤ (cid:107)Xu(cid:107)2

2 ≤ (1 + ε)(cid:107)u(cid:107)2

2

for all u ∈ A .

(16)

JLT is a random projection which embeds high-dimensional data into lower-dimensional space while
approximately preserving all pairwise distances [49  32  24]. JLT has found numerous applications
that include searching for an aproximate nearest neighbor in high-dimensional Euclidean space [23] 
dimension reduction in data bases [1]  learning mixture of Gaussians [15] and sketching [49]. It is

6

˜X  where ˜X contains standard i.i.d. normal elements  is a JLT with high

well known that X = 1√
probability when n = Ω(log N ) [25]. .
Now denote the element in the i-th row and j-th column of ˜X as ˜Xi j  and the i-th row as ˜Xi :. Let
the entries of ˜Xi j being sequentially generated as follows:

n

1. Initially  draw the ﬁrst element ˜X1 1 from a zero-mean sub-Gaussian distribution.
2. ˜Xi j is a conditionally 1-sub-Gaussian random variable satisfying E[ ˜Xi j|fi j] = 0. The
fi j are realizations of a stochastic process which can possibly depend on the entries
{{ ˜Xi(cid:48) :}i(cid:48)<i { ˜Xi j(cid:48)}j(cid:48)<j}.

3. ˜Xi j ⊥ {{ ˜Xi(cid:48) :}i(cid:48)<i { ˜Xi j(cid:48)}j(cid:48)<j} | fi j and ˜Xi j ⊥ {{fi j(cid:48)}j(cid:48)>j {fi(cid:48) :}i(cid:48)>i} | fi j

The following result is an immediate consequence of Theorem 1
Corollary 1 (JL) Let X ∈ Rn×p be a matrix constructed as X = 1√
Ω(−2 log N )  X is a JLT with probability at least 1 − 1

˜X.
N c for a constant c > 0.

n

If we choose n =

4.2 Restricted Isometry Property (RIP) with Stochastic Processes
Matrices satisfying Restricted Isometry Property (RIP) are approximately orthonormal on sparse
vectors [10  9]. Let X ∈ Rn×p and let A be the set of all s-sparse vectors in Rp. We deﬁne matrix
X to satisfy RIP with the restricted isometry constant δs ∈ (0  1) if for all u ∈ A 

(1 − δs)(cid:107)u(cid:107)2

2 ≤ 1
n

(cid:107)Xu(cid:107)2

2 ≤ (1 + δs)(cid:107)u(cid:107)2
2 .

(17)

Matrices satisfying RIP are of interest in high-dimensional statistics and compressed sensing prob-
lems where the goal is to recover a sparse signal θ∗ ∈ Rp from limited noisy linear measure-
ments [47  46]. Sub-Gaussian random matrices with i.i.d.
rows  e.g.  rows sampled from a
N (0  σ2Ip×p) satisﬁes RIP [10  9  35  5] when n = Ω(s log p). But the i.i.d. rows assumption is vi-
olated in many practical settings when data is generated adaptively/sequentially. Examples include
times-series regression and bandits problems [31  27]  active learning [40  21] or volume sampling
[16  17]. An application of our new results shows that the i.i.d. assumption is not necessary and
design matrices generated from dependent elements also satisfy RIP when n = Ω(s log p). For
example  the following result holds for matrices X generated similar to matrix ˜X in Section 4.1.
Corollary 2 (RIP) Let X ∈ Rn×p be a matrix generated from the process outlined earlier. Then

for any ε > 0  if we choose n = Ω(ε−2s log(2p/s))  then δs ≤ ε with probability at least 1−(cid:16) s

(cid:17)cs

2p

for a constant c > 0.

RIP for adaptively generated rows. Sequential learning problems like linear contextual bandits
involve estimating a parameter with a design matrix whose rows are adverserially generated based
on previously observed rows and rewards which are linear functions of the rows. An example is
the linear contextual bandit problem considered  e.g.  in [27  41]. The data in any time step t is
generated as follows [27  41]: .

t   . . .   µk

adversary At−1 maps the histories to k contexts µ1
At−1 : Ht−1 → (Bp
2 )k where Bp
the contexts with random Gaussian noise  i.e.  xi
in the context of GM3  Ht−1 ∪ {x1

1. Let Ht−1 denote historical data observed until time t − 1. In time step t − 1 an adaptive
t(cid:107)2 ≤ 1  i.e. 
2 represents the unit ball in p dimensions. Nature perturbs
t ∼ N (0  σ2Ip×p). Now 
t } based on historical
data Ht−1. Let xit
t denote the corresponding Gaussian
t −
perturbation. In the context of GM3  we denote the centered Gaussian perturbation git
t   θ∗(cid:105) + ωt where ωt is an
E[git
unknown sub-Gaussian noise. History at time step t is now augmented with the new data 
i.e.  Ht = Ht−1 ∪ {{x1

t ] by ξt. The learner receives the noisy reward yt = (cid:104)xit

2. In time step t  a learner chooses one among k contexts {x1

t in Rp with (cid:107)µ1
t with gi

t denote the selected context and git

t } represents F1:t−1.

t }  xit

t   yt}.

t = µi

t + gi

t   . . .   xk

t   . . .   xk

t   . . .   xk

7

The data generation process mirrors GM3 with Ft being a sub-Gaussian process which is inﬂuenced
by Ft−1 and ξt but generated adaptively by an adversary. ξt is a sub-Gaussian random vector cho-
sen by the learner using historical data Ht−1 satisfying (SP-2). The algorithm proposed in [41  27]
involves a parameter estimation step in each time step t using the observed contexts and the corre-
sponding rewards {xit(cid:48)
t(cid:48)   yt(cid:48)}  1 ≤ t(cid:48) ≤ t. With Xt the matrix which has the centered Gaussian per-
turbations ξ1  . . .   ξt−1 as rows  [41  27] show that
2 for some constant κ
depending on the problem parameters and require the following lower bound on the non-asymptotic
RIP condition for efﬁcient parameter estimation:
2] − 

2] ≥ tκ(cid:107)u(cid:107)2

E[(cid:107)Xtu(cid:107)2

E[(cid:107)Xtu(cid:107)2

(cid:107)u(cid:107)2

2 ≤ inf
u∈Rp

(cid:107)Xtu(cid:107)2
2 .

(cid:18)

inf
u∈Rp

inf
u∈Rp

(cid:19)

(18)

Since the data generation follows graphical model GM3  the following Corollary 3 is a direct con-
sequence of Theorem 1

Corollary 3 Let Xt be a design matrix generated from the process described above. Then for any
 > 0  if we choose t = Ω(−2κ−2p)  then with probability atleast 1− exp(−cp) for constant c > 0 
the following condition is satisﬁed 

(cid:107)X tu(cid:107)2

2 ≥ tκ(1 − )(cid:107)u(cid:107)2
2 .

inf
u∈Rp

(19)

ηi j satisfy(cid:80)n

4.3 CountSketch
CountSketch or sparse JL transform is used in real world applications like data streaming and dimen-
sionality reduction [12  49]. Every column of a (n × p) CountSketch matrix X has only d(d (cid:28) n)
non-zero elements  therefore for any vector u ∈ Rp  computing Xu takes only O(dp) instead of
d  where δi j is an
O(np). Each entry of a CountSketch matrix X is given by Xi j = ηi jδi j/
independent Rademacher random variable  and ηi j is a random variable sampled adaptively. The
i=1 ηi j = d  ηi j ∈ {0  1}  that is each column has exactly d non zero elements. For
every column j of X  the ηi j can be generated by sampling d indices from {1  2  . . .   n} adaptively
given previous columns  then set corresponding Xi j to be a Rademacher random variable  so that
Xi j depends on X1 j  X2 j . . .   Xi−1 j. The data generation process of countSketch matrix follows
graphical model GM1. The variance of Xi j is 1
n and since all the entries of X are bounded by 1  X
is a JLT over N points when the number of rows satisﬁes n = Ω(−2 log N ). Unlike [14  26]  our
bound does not depend on the choice of d. Our bound also matches the state of the art [26].

√

5 Conclusions

Several existing results in machine learning and randomized algorithms  e.g.  RIP  J-L  sketching 
etc.  rely on uniform large deviation bounds of random quadratic forms based on random vectors or
matrices. Such results are uniform over suitable sets of matrices or vectors  and have found wide
ranging applications over the past few decades. Growing interest in adaptive data collection  mod-
eling  and estimation in modern machine learning is revealing a key limitation of such results: the
need for statistical independence  e.g.  elementwise independence of random vectors  row-wise in-
dependence of random matrices  etc. In this paper  we have presented a generalization of such results
that allows for statistical dependence on the history. We have also given examples for certain cases
of interest  including RIP  J-L  and sketching  illustrating that in spite of allowing for dependence 
our bounds are of the same order as that for the case of independent random vectors. We anticipate
our results to simplify and help make advances in analyzing learning settings based on adaptive
data collection. Further  the added ﬂexibility of designing random matrices sequentially may lead
to computationally and/or statistically efﬁcient random projection based algorithms. In future work 
we plan to investigate applications of these results in adaptive data collection and modeling settings.
Acknowledgements: The research was supported by NSF grants OAC-1934634  IIS-1908104 
IIS-1563950  IIS-1447566  IIS-1447574  IIS-1422557  CCF-1451986  a Google Faculty Research
Award  a J.P. Morgan Faculty Award  and a Mozilla research grant. Part of this work completed
while ZSW was visiting the Simons Institute for the Theory of Computing at UC Berkeley.

8

References
[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary
coins. Journal of Computer and System Sciences  66(4):671 – 687  2003. Special Issue on
PODS 2001.

[2] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnson-

lindenstrauss transform. In STOC  2006.

[3] M. Arcones and E. Gine. On decoupling  series expansions  and tail behavior of chaos pro-

cesses. Journal of Theoretical Probability  6(1):101–122  1993.

[4] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. J. Mach. Learn.

Res.  3:397–422  mar 2003.

[5] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In

Advances in Neural Information Processing Systems (NIPS)  2014.

[6] Arindam Banerjee  Qilong Gu  Vidyashankar Sivakumar  and Zhiwei Steven Wu. Ran-
dom quadratic forms with dependence: Applications to restricted isometry and beyond.
arXiv:1910.04930  2019.

[7] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press  2012.

[8] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic Theory

of Independence. Oxford University Press  2013.

[9] E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than

n. The Annals of Statistics  35(6):2313–2351  2007.

[10] E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information

Theory  51:4203–4215  2005.

[11] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[12] Moses Charikar  Kevin Chen  and Martin Farach-Colton. Finding frequent items in data

streams. In ICALP  2002.

[13] Sheng Chen and Arindam Banerjee. Structured estimation with atomic norms: General bounds

and applications. In Advances in Neural Information Processing Systems 28  2015.

[14] Anirban Dasgupta  Ravi Kumar  and Tam´as Sarlos. A sparse johnson lindenstrauss transform.

In STOC  2010.

[15] Sanjoy Dasgupta. Learning mixtures of gaussians. In FOCS  pages 634–  1999.

[16] Michal Derezinski and Manfred Warmuth. Subsampling for ridge regression via regularized

volume sampling. In AISTATS  pages 716–725  2018.

[17] Michat Derezi´nski  Manfred K. Warmuth  and Daniel Hsu. Leveraged volume sampling for

linear regression. In NIPS  pages 2510–2519  2018.

[18] Yash Deshpande  Lester W. Mackey  Vasilis Syrgkanis  and Matt Taddy. Accurate inference
for adaptive linear models. In Proceedings of the 35th International Conference on Machine
Learning  ICML 2018  Stockholmsm¨assan  Stockholm  Sweden  July 10-15  2018  2018.

[19] D. Donoho. Compressed Sensing. IEEE Transactions on Information Theory  52(4):1289–

1306  2006.

[20] P. Drineas  R. Kannan  and M. Mahoney. Fast monte carlo algorithms for matrices i: Approx-

imating matrix multiplication. SIAM Journal on Computing  36(1):132–157  2006.

[21] Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in

Machine Learning  7(2-3):131–309  2014.

9

[22] D. L. Hanson and F. T. Wright. A bound on tail probabilities for quadratic forms in independent

random variables. Ann. Math. Statist.  42(3):1079–1083  06 1971.

[23] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse

of dimensionality. In STOC  pages 604–613  1998.

[24] Piotr Indyk  Rajeev Motwani  Prabhakar Raghavan  and Santosh Vempala. Locality-preserving

hashing in multidimensional spaces. In STOC  1997.

[25] William Johnson and Joram Lindenstrauss. Extensions of lipschitz maps into a hilbert space.

Contemporary Mathematics  26:189–206  01 1984.

[26] Daniel M. Kane and Jelani Nelson. Sparser johnson-lindenstrauss transforms.

61(1):4:1–4:23  January 2014.

J. ACM 

[27] Sampath Kannan  Jamie Morgenstern  Aaron Roth  Bo Waggoner  and Zhiwei Steven Wu. A
smoothed analysis of the greedy algorithm for the linear contextual bandit problem. CoRR
arXiv:1801.04323  2018.

[28] F. Krahmer  S. Mendelson  and H. Rauhut. Suprema of chaos processes and the restricted
isometry property. Communications on Pure and Applied Mathematics  67(11):1877–1904 
2014.

[29] M. Ledaux and M. Talagrand. Probability in Banach Spaces:

Springer  1991.

Isometry and Processes.

[30] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.

Springer  2013.

[31] Helmut Ltkepohl. New Introduction to Multiple Time Series Analysis. Springer  2005.

[32] Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends

in Machine Learning  3(2):123–224  2011.

[33] Shahar Mendelson  Holger Rauhut  and Rachel Ward. Improved bounds for sparse recovery

from subsampled random convolutions. Ann. Appl. Probab.  28(6):3491–3527  12 2018.

[34] Seth Neel and Aaron Roth. Mitigating bias in adaptive data gathering via differential pri-
vacy. In Proceedings of the 35th International Conference on Machine Learning  ICML 2018 
Stockholmsm¨assan  Stockholm  Sweden  July 10-15  2018  2018.

[35] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis

of regularized M-estimators. Statistical Science  27(4):538–557  2012.

[36] Xinkun Nie  Xiaoying Tian  Jonathan Taylor  and James Zou. Why adaptively collected data
In International Conference on Artiﬁcial In-
have negative bias and how to correct for it.
telligence and Statistics  AISTATS 2018  9-11 April 2018  Playa Blanca  Lanzarote  Canary
Islands  Spain  2018.

[37] S. Oymak  B. Recht  and M. Soltanolkotabi. Sharp timedata tradeoffs for linear inverse prob-

lems. IEEE Transactions on Information Theory  64(6):4129–4158  June 2018.

[38] V. H. Pena and E. Gine. Decoupling: From Dependence to Independence. Springer  1999.

[39] Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentra-

tion. Electron. Commun. Probab.  18:9 pp.  2013.

[40] Burr Settles. Active learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learn-

ing  6(1):1–114  2012.

[41] Vidyashankar Sivakumar  Zhiwei Steven Wu  and Arindam Banerjee. Structured linear con-

textual bandits: A sharp and geometric smoothed analysis. In preparation  2019.

[42] M. Talagrand. The Generic Chaining. Springer  2005.

10

[43] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer  2014.

[44] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and
G. Kutyniok  editors  Compressed Sensing  chapter 5  pages 210–268. Cambridge University
Press  2012.

[45] R. Vershynin. Estimation in high dimensions: A geometric perspective  pages 3–66. Springer

International Publishing  Cham  2014.

[46] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press  2018.

[47] Martin Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge

University Press (To appear)  2019.

[48] David Williams. Probability with Martingales. Cambridge University Press  1991.

[49] David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor.

Comput. Sci.  10(1&#8211;2):1–157  October 2014.

11

,Arindam Banerjee
Qilong Gu
Vidyashankar Sivakumar
Steven Wu