2016,Efficient Second Order Online Learning by Sketching,We propose Sketched Online Newton (SON)  an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step  which  via sketching techniques enjoys a running time linear in the dimension and sketch size.  We further develop sparse forms of the sketching methods (such as Oja's rule)  making the computation linear in the sparsity of features. Together  the algorithm eliminates all computational obstacles in previous second order online learning approaches.,Efﬁcient Second Order Online Learning by Sketching

Haipeng Luo

Alekh Agarwal

Princeton University  Princeton  NJ USA

Microsoft Research  New York  NY USA

haipengl@cs.princeton.edu

alekha@microsoft.com

Nicolò Cesa-Bianchi

Università degli Studi di Milano  Italy

nicolo.cesa-bianchi@unimi.it

John Langford

Microsoft Research  New York  NY USA

jcl@microsoft.com

Abstract

We propose Sketched Online Newton (SON)  an online second order learning
algorithm that enjoys substantially improved regret guarantees for ill-conditioned
data. SON is an enhanced version of the Online Newton Step  which  via sketching
techniques enjoys a running time linear in the dimension and sketch size. We
further develop sparse forms of the sketching methods (such as Oja’s rule)  making
the computation linear in the sparsity of features. Together  the algorithm eliminates
all computational obstacles in previous second order online learning approaches.

1

Introduction

Online learning methods are highly successful at rapidly reducing the test error on large  high-
dimensional datasets. First order methods are particularly attractive in such problems as they typically
enjoy computational complexity linear in the input size. However  the convergence of these methods
crucially depends on the geometry of the data; for instance  running the same algorithm on a rotated
set of examples can return vastly inferior results. See Fig. 1 for an illustration.
Second order algorithms such as Online Newton Step [18] have the attractive property of being
invariant to linear transformations of the data  but typically require space and update time quadratic
in the number of dimensions. Furthermore  the dependence on dimension is not improved even
if the examples are sparse. These issues lead to the key question in our work: Can we develop
(approximately) second order online learning algorithms with efﬁcient updates? We show that
the answer is “yes” by developing efﬁcient sketched second order methods with regret guarantees.
Speciﬁcally  the three main contributions of this work are:

1. Invariant learning setting and optimal algorithms (Section 2). The typical online regret
minimization setting evaluates against a benchmark that is bounded in some ﬁxed norm (such as the
(cid:96)2-norm)  implicitly putting the problem in a nice geometry. However  if all the features are scaled
down  it is desirable to compare with accordingly larger weights  which is precluded by an apriori
ﬁxed norm bound. We study an invariant learning setting similar to the paper [33] which compares
the learner to a benchmark only constrained to generate bounded predictions on the sequence of
examples. We show that a variant of the Online Newton Step [18]  while quadratic in computation 
stays regret-optimal with a nearly matching lower bound in this more general setting.

2. Improved efﬁciency via sketching (Section 3). To overcome the quadratic running time  we
next develop sketched variants of the Newton update  approximating the second order information
using a small number of carefully chosen directions  called a sketch. While the idea of data sketching
is widely studied [36]  as far as we know our work is the ﬁrst one to apply it to a general adversarial

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

online learning setting and provide rigorous regret guarantees. Three different sketching methods are
considered: Random Projections [1  19]  Frequent Directions [12  23]  and Oja’s algorithm [28  29] 
all of which allow linear running time per round. For the ﬁrst two methods  we prove regret bounds
similar to the full second order update whenever the sketch-size is large enough. Our analysis makes
it easy to plug in other sketching and online PCA methods (e.g. [11]).

3. Sparse updates (Section 4). For practical
implementation  we further develop sparse ver-
sions of these updates with a running time linear
in the sparsity of the examples. The main chal-
lenge here is that even if examples are sparse 
the sketch matrix still quickly becomes dense.
These are the ﬁrst known sparse implementa-
tions of the Frequent Directions1 and Oja’s algo-
rithm  and require new sparse eigen computation
routines that may be of independent interest.
Empirically  we evaluate our algorithm using
the sparse Oja sketch (called Oja-SON) against
ﬁrst order methods such as diagonalized ADA-
GRAD [6  25] on both ill-conditioned synthetic
and a suite of real-world datasets. As Fig. 1
shows for a synthetic problem  we observe sub-
stantial performance gains as data conditioning
worsens. On the real-world datasets  we ﬁnd
improvements in some instances  while observing no substantial second-order signal in the others.

Figure 1: Error rate of SON using Oja’s sketch  and
ADAGRAD on a synthetic ill-conditioned problem.
m is the sketch size (m = 0 is Online Gradient 
m = d resembles Online Newton). SON is nearly
invariant to condition number for m = 10.

Related work Our online learning setting is closest to the one proposed in [33]  which studies
scale-invariant algorithms  a special case of the invariance property considered here (see also [31 
Section 5]). Computational efﬁciency  a main concern in this work  is not a problem there since each
coordinate is scaled independently. Orabona and Pál [30] study unrelated notions of invariance. Gao
et al. [9] study a speciﬁc randomized sketching method for a special online learning setting.
The L-BFGS algorithm [24] has recently been studied in the stochastic setting2 [3  26  27  34  35]  but
has strong assumptions with pessimistic rates in theory and reliance on the use of large mini-batches
empirically. Recent works [7  15  14  32] employ sketching in stochastic optimization  but do not
provide sparse implementations or extend in an obvious manner to the online setting. The Frank-
Wolfe algorithm [8  20] is also invariant to linear transformations  but with worse regret bounds [17]
without further assumptions and modiﬁcations [10].

Notation Vectors are represented by bold letters (e.g.  x  w  . . . ) and matrices by capital letters
(e.g.  M  A  . . . ). Mi j denotes the (i  j) entry of matrix M. I d represents the d × d identity matrix 
0m×d represents the m × d matrix of zeroes  and diag{x} represents a diagonal matrix with x on
the diagonal. λi(A) denotes the i-th largest eigenvalue of A  (cid:107)w(cid:107)A denotes
w(cid:62)Aw  |A| is the
i j Ai jBi j  and A (cid:22) B means that
B − A is positive semideﬁnite. The sign function SGN(a) is 1 if a ≥ 0 and −1 otherwise.

determinant of A  TR(A) is the trace of A  (cid:104)A  B(cid:105) denotes(cid:80)

√

2 Setup and an Optimal Algorithm

We consider the following setting. On each round t = 1  2 . . .   T : (1) the adversary ﬁrst presents an
example xt ∈ Rd  (2) the learner chooses wt ∈ Rd and predicts w(cid:62)
t xt  (3) the adversary reveals a
loss function ft(w) = (cid:96)t(w(cid:62)xt) for some convex  differentiable (cid:96)t : R → R+  and (4) the learner
suffers loss ft(wt) for this round.

The learner’s regret to a comparator w is deﬁned as RT (w) =(cid:80)T

t=1 ft(w). Typical
results study RT (w) against all w with a bounded norm in some geometry. For an invariant update 
1Recent work by [13] also studies sparse updates for a more complicated variant of Frequent Directions

t=1 ft(wt) −(cid:80)T

which is randomized and incurs extra approximation error.

2Stochastic setting assumes that the examples are drawn i.i.d. from a distribution.

2

0501001502000.060.080.10.120.140.16condition numbererror rate AdaGradOja−SON (m=0)Oja−SON (m=5)Oja−SON (m=10)we relax this requirement and only put bounds on the predictions w(cid:62)xt. Speciﬁcally  for some
pre-chosen constant C we deﬁne Kt
comparators that generate bounded predictions on every data point  that is:

def= (cid:8)w : |w(cid:62)xt| ≤ C(cid:9) . We seek to minimize regret to all
T(cid:92)

Kt =(cid:8)w : ∀t = 1  2  . . . T 

|w(cid:62)xt| ≤ C(cid:9) .

RT (w) where K def=

RT = sup
w∈K

t=1

(cid:48)

Under this setup  if the data are transformed to M xt for all t and some invertible matrix M ∈ Rd×d 
the optimal w∗ simply moves to (M−1)(cid:62)w∗  which still has bounded predictions but might have
signiﬁcantly larger norm. This relaxation is similar to the comparator set considered in [33].
We make two structural assumptions on the loss functions.
Assumption 1. (Scalar Lipschitz) The loss function (cid:96)t satisﬁes |(cid:96)
Assumption 2. (Curvature) There exists σt ≥ 0 such that for all u  w ∈ K  ft(w) is lower bounded
by ft(u) + ∇ft(u)(cid:62)(w − u) + σt
Note that when σt = 0  Assumption 2 merely imposes convexity. More generally  it is satisﬁed by
squared loss ft(w) = (w(cid:62)xt − yt)2 with σt = 1
8C2 whenever |w(cid:62)xt| and |yt| are bounded by C 
as well as for all exp-concave functions (see [18  Lemma 3]).
Enlarging the comparator set might result in worse regret. We next show matching upper and lower
bounds qualitatively similar to the standard setting  but with an extra unavoidable
Theorem 1. For any online algorithm generating wt ∈ Rd and all T ≥ d  there exists a sequence of
T examples xt ∈ Rd and loss functions (cid:96)t satisfying Assumptions 1 and 2 (with σt = 0) such that the

(cid:0)∇ft(u)(cid:62)(u − w)(cid:1)2

t(z)| ≤ L whenever |z| ≤ C.

regret RT is at least CL(cid:112)dT /2.

d factor. 3

√

2

.

We now give an algorithm that matches the lower bound up to logarithmic constants in the worst case
but enjoys much smaller regret when σt (cid:54)= 0. At round t + 1 with some invertible matrix At speciﬁed
later and gradient gt = ∇ft(wt)  the algorithm performs the following update before making the
prediction on the example xt+1:

ut+1 = wt − A−1

t gt 

and wt+1 = argmin
w∈Kt+1

(cid:107)w − ut+1(cid:107)At

.

(1)

The projection onto the set Kt+1 differs from typical norm-based projections as it only enforces
boundedness on xt+1 at round t + 1. Moreover  this projection step can be performed in closed form.
Lemma 1. For any x (cid:54)= 0  u ∈ Rd and positive deﬁnite matrix A ∈ Rd×d  we have

argmin

w : |w(cid:62)x|≤C

(cid:107)w − u(cid:107)A = u − τC(u(cid:62)x)
x(cid:62)A−1x

A−1x  where τC(y) = SGN(y) max{|y| − C  0}.

If At is a diagonal matrix  updates similar to those of Ross et al. [33] are recovered. We study a
choice of At that is similar to the Online Newton Step (ONS) [18] (though with different projections):

t(cid:88)
(σs + ηs)gsg(cid:62)

s

s=1

At = αI d +

(2)

for some parameters α > 0 and ηt ≥ 0. The regret guarantee of this algorithm is shown below:
Theorem 2. Under Assumptions 1 and 2  suppose that σt ≥ σ ≥ 0 for all t  and ηt is non-increasing.
Then using the matrices (2) in the updates (1) yields for all w ∈ K 

(cid:32)

(σ + ηT )(cid:80)T

dα

t=1 (cid:107)gt(cid:107)2

2

(cid:33)

.

RT (w) ≤ α
2

(cid:107)w(cid:107)2

2 + 2(CL)2

ηt +

d

2(σ + ηT )

ln

1 +

3In the standard setting where wt and xt are restricted such that (cid:107)wt(cid:107) ≤ D and (cid:107)xt(cid:107) ≤ X  the minimax

√
regret is O(DXL
T ). This is clearly a special case of our setting with C = DX.

T(cid:88)

t=1

3

Receive example xt.

Algorithm 1 Sketched Online Newton (SON)
Input: Parameters C  α and m.
1: Initialize u1 = 0d×1.
2: Initialize sketch (S  H) ← SketchInit(α  m).
3: for t = 1 to T do
4:
5:
6:
7:
8:
9:
10: end for

Projection step: compute(cid:98)x = Sxt  γ = τC (u(cid:62)
(S  H) ← SketchUpdate((cid:98)g).

Predict label yt = w(cid:62)
Compute gradient gt = (cid:96)(cid:48)
Update weight: ut+1 = wt − 1

t xt and suffer loss (cid:96)t(yt).

α (gt − S(cid:62)HSgt).

x(cid:62)

t xt−(cid:98)x(cid:62)H(cid:98)x and set wt = ut − γ(xt − S(cid:62)H(cid:98)x).

t xt)

t(yt)xt and the to-sketch vector(cid:98)g =

√

σt + ηtgt.

The dependence on (cid:107)w(cid:107)2
2 implies that the method is not completely invariant to transformations of
the data. This is due to the part αI d in At. However  this is not critical since α is ﬁxed and small
while the other part of the bound grows to eventually become the dominating term. Moreover  we
can even set α = 0 and replace the inverse with the Moore-Penrose pseudoinverse to obtain a truly
invariant algorithm  as discussed in Appendix D. We use α > 0 in the remainder for simplicity.
The implication of this regret bound is the following: in the worst case where σ = 0  we set

ηt =(cid:112)d/C 2L2t and the bound simpliﬁes to

RT (w) ≤ α
2

(cid:107)w(cid:107)2

2 +

CL
2

√

T d ln

1 +

(cid:32)

(cid:33)

(cid:80)T
t=1 (cid:107)gt(cid:107)2
2
αCL
T d

√

√
+ 4CL

T d  

essentially only losing a logarithmic factor compared to the lower bound in Theorem 1. On the other
hand  if σt ≥ σ > 0 for all t  then we set ηt = 0 and the regret simpliﬁes to

(cid:32)

σ(cid:80)T

(cid:33)

t=1 (cid:107)gt(cid:107)2
dα

2

RT (w) ≤ α
2

(cid:107)w(cid:107)2

2 +

d
2σ

ln

1 +

 

(3)

extending the O(d ln T ) results in [18] to the weaker Assumption 2 and a larger comparator set K.

3 Efﬁciency via Sketching

Our algorithm so far requires Ω(d2) time and space just as ONS. In this section we show how to
achieve regret guarantees nearly as good as the above bounds  while keeping computation within a
constant factor of ﬁrst order methods.

Let Gt ∈ Rt×d be a matrix such that the t-th row is(cid:98)g

t where we deﬁne(cid:98)gt =

σt + ηtgt to be
the to-sketch vector. Our previous choice of At (Eq. (2)) can be written as αI d + G(cid:62)
t Gt. The idea
of sketching is to maintain an approximation of Gt  denoted by St ∈ Rm×d where m (cid:28) d is a
small constant called the sketch size. If m is chosen so that S(cid:62)
t Gt well  we can
redeﬁne At as αI d + S(cid:62)
To see why this admits an efﬁcient algorithm  notice that by the Woodbury formula one has A−1
τC(u(cid:62)

(cid:1). With the notation Ht = (αI m + StS(cid:62)

t =
t )−1 ∈ Rm×m and γt =

(cid:0)I d − S(cid:62)

t St approximates G(cid:62)

t HtStxt+1)  update (1) becomes:

t )−1St
t+1xt+1 − x(cid:62)

t St for the algorithm.

t (αI m + StS(cid:62)

√

(cid:62)

1
α

t+1xt+1)/(x(cid:62)
ut+1 = wt − 1

α

(cid:0)gt − S(cid:62)

t+1S(cid:62)
t HtStgt

(cid:1) 

The operations involving Stgt or Stxt+1 require only O(md) time  while matrix vector products
with Ht require only O(m2). Altogether  these updates are at most m times more expensive than ﬁrst
order algorithms as long as St and Ht can be maintained efﬁciently. We call this algorithm Sketched
Online Newton (SON) and summarize it in Algorithm 1.
We now discuss three sketching techniques to maintain the matrices St and Ht efﬁciently  each
requiring O(md) storage and time linear in d.

and wt+1 = ut+1 − γt

(cid:0)xt+1 − S(cid:62)

t HtStxt+1

(cid:1) .

4

Algorithm 2 FD-Sketch for FD-SON
Internal State: S and H.
SketchInit(α  m)
1: Set S = 0m×d and H = 1
2: Return (S  H).

SketchUpdate((cid:98)g)
1: Insert(cid:98)g into the last row of S.

α I m.

2: Compute eigendecomposition: V (cid:62)ΣV =

S(cid:62)S and set S = (Σ − Σm mI m) 1
2 V .
 ···   1

α+Σ1 1−Σm m

α

1

(cid:110)

(cid:111)

.

3: Set H = diag
4: Return (S  H).

Algorithm 3 Oja’s Sketch for Oja-SON
Internal State: t  Λ  V and H.
SketchInit(α  m)
1: Set t = 0  Λ = 0m×m  H = 1

α I m and V
to any m× d matrix with orthonormal rows.

2: Return (0m×d  H).

SketchUpdate((cid:98)g)

1: Update t ← t + 1  Λ and V as Eqn. 4.
2: Set S = (tΛ) 1
3: Set H = diag
4: Return (S  H).

 ···  

(cid:110)

α+tΛ1 1

2 V .

α+tΛm m

1

1

(cid:111)

.

t

can be realized by two rank-one updates: H−1

Here we consider Gaussian Random Projection sketch: St = St−1 + rt(cid:98)g
Random Projection (RP). Random projections are classical methods for sketching [19  1  21].
(cid:62)
√
t   where each entry of
rt ∈ Rm is an independent random Gaussian variable drawn from N (0  1/
m). One can verify that
qt = St(cid:98)gt − (cid:107)(cid:98)gt(cid:107)2
t = H−1
the update of H−1
t−1 + qtr(cid:62)
t where
2 rt. Using Woodbury formula  this results in O(md) update of S and H (see
Algorithm 6 in Appendix E). We call this combination of SON with RP-sketch RP-SON. When α = 0
this algorithm is invariant to linear transformations for each ﬁxed realization of the randomness.
Using the existing guarantees for RP-sketch  in Appendix E we show a similar regret bound as
Theorem 2 up to constants  provided m = ˜Ω(r) where r is the rank of GT . Therefore RP-SON is
near invariant  and gives substantial computational gains when r (cid:28) d with small regret overhead.

t + rtq(cid:62)

2

method. FD maintains the invariant that the last row of St is always 0. On each round  the vector(cid:98)g

Frequent Directions (FD). When GT is near full-rank  however  RP-SON may not perform well.
To address this  we consider Frequent Directions (FD) sketch [12  23]  a deterministic sketching
(cid:62)
t
is inserted into the last row of St−1  then the covariance of the resulting matrix is eigendecomposed
into V (cid:62)
2 Vt where ρt is the smallest eigenvalue. Since the rows
of St are orthogonal to each other  Ht is a diagonal matrix and can be maintained efﬁciently (see
Algorithm 2). The sketch update works in O(md) time (see [12] and Appendix G.2) so the total
running time is O(md) per round. We call this combination FD-SON and prove the following regret

t ΣtVt and St is set to (Σt − ρtI m) 1

bound with notation Ωk =(cid:80)d
i=k+1 λi(G(cid:62)
T(cid:88)

Theorem 3. Under Assumptions 1 and 2  suppose that σt ≥ σ ≥ 0 for all t and ηt is non-increasing.
FD-SON ensures that for any w ∈ K and k = 0  . . .   m − 1  we have
TR(S(cid:62)
RT (w) ≤ α
mα
2

T GT ) for any k = 0  . . .   m − 1.
(cid:19)

2(m − k)(σ + ηT )α

2 + 2(CL)2

2(σ + ηT )

(cid:107)w(cid:107)2

T ST )

(cid:18)

mΩk

ηt +

1 +

m

ln

+

.

t=1

2 (cid:107)w(cid:107)2

Instead of the rank  the bound depends on the spectral decay Ωk  which essentially is the only extra
term compared to the bound in Theorem 2. Similarly to previous discussion  if σt ≥ σ  we get the
2(m−k)σα . With α tuned well  we pay logarithmic regret
bound α
for the top m eigenvectors  but a square root regret O(
Ωk) for remaining directions not controlled
by our sketch. This is expected for deterministic sketching which focuses on the dominant part of the
spectrum. When α is not tuned we still get sublinear regret as long as Ωk is sublinear.

1 + TR(S(cid:62)

+ mΩk

T ST )
mα

2 + m

2σ ln

√

(cid:16)

(cid:17)

eigenvalues of data in a streaming fashion  with the to-sketch vector(cid:98)gt’s as the input. Speciﬁcally 

Oja’s Algorithm. Oja’s algorithm [28  29] is not usually considered as a sketching algorithm
but seems very natural here. This algorithm uses online gradient descent to ﬁnd eigenvectors and
let Vt ∈ Rm×d denote the estimated eigenvectors and the diagonal matrix Λt ∈ Rm×m contain the
estimated eigenvalues at the end of round t. Oja’s algorithm updates as:

Λt = (I m − Γt)Λt−1 + Γt diag{Vt−1(cid:98)gt}2  

orth←−− Vt−1 + ΓtVt−1(cid:98)gt(cid:98)g

Vt

(cid:62)
t

(4)

5

where Γt ∈ Rm×m is a diagonal matrix with (possibly different) learning rates of order Θ(1/t)
on the diagonal  and the “ orth←−−” operator represents an orthonormalizing step.4 The sketch is then
St = (tΛt) 1
2 Vt. The rows of St are orthogonal and thus Ht is an efﬁciently maintainable diagonal
matrix (see Algorithm 3). We call this combination Oja-SON.
The time complexity of Oja’s algorithm is O(m2d) per round due to the orthonormalizing step. To
improve the running time to O(md)  one can only update the sketch every m rounds (similar to
the block power method [16  22]). The regret guarantee of this algorithm is unclear since existing
analysis for Oja’s algorithm is only for the stochastic setting (see e.g. [2  22]). However  Oja-SON
provides good performance experimentally.

t gt (or sketched versions) are typically dense even if gt is sparse.

4 Sparse Implementation
In many applications  examples (and hence gradients) are sparse in the sense that (cid:107)xt(cid:107)0 ≤ s for all t
and some small constant s (cid:28) d. Most online ﬁrst order methods enjoy a per-example running time
depending on s instead of d in such settings. Achieving the same for second order methods is more
difﬁcult since A−1
We show how to implement our algorithms in sparsity-dependent time  speciﬁcally  in O(m2 +
ms) for RP-SON and FD-SON and in O(m3 + ms) for Oja-SON. We emphasize that since the
sketch would still quickly become a dense matrix even if the examples are sparse  achieving purely
sparsity-dependent time is highly non-trivial (especially for FD-SON and Oja-SON)  and may be of
independent interest. Due to space limit  below we only brieﬂy mention how to do it for Oja-SON.
Similar discussion for the other two sketches can be found in Appendix G. Note that mathematically
these updates are equivalent to the non-sparse counterparts and regret guarantees are thus unchanged.
There are two ingredients to doing this for Oja-SON: (1) The eigenvectors Vt are represented as
Vt = FtZt  where Zt ∈ Rm×d is a sparsely updatable direction (Step 3 in Algorithm 5) and
Ft ∈ Rm×m is a matrix such that FtZt is orthonormal. (2) The weights wt are split as ¯wt + Z(cid:62)
t−1bt 
where bt ∈ Rm maintains the weights on the subspace captured by Vt−1 (same as Zt−1)  and ¯wt
captures the weights on the complementary subspace which are again updated sparsely.
We describe the sparse updates for ¯wt and bt below with the details for Ft and Zt deferred to
Appendix H. Since St = (tΛt) 1

2 Vt = (tΛt) 1

(cid:1) gt
α = ¯wt − gt

(cid:124)

2 FtZt and wt = ¯wt + Z(cid:62)
t (bt + 1

(cid:125)
α − (Zt − Zt−1)(cid:62)bt

+Z(cid:62)

(cid:123)(cid:122)

(cid:124)

t−1bt  we know ut+1 is
α F (cid:62)
t (tΛtHt)FtZtgt
) .

(5)

(cid:123)(cid:122)

wt −(cid:0)I d − S(cid:62)

t HtSt

def= b(cid:48)
Since Zt − Zt−1 is sparse by construction and the matrix operations deﬁning b
(cid:48)
t+1 scale with m 
overall the update can be done in O(m2 + ms). Using the update for wt+1 in terms of ut+1  wt+1
is equal to

def= ¯ut+1

t+1

ut+1 − γt(I d − S(cid:62)

(cid:125)
t HtSt)xt+1 = ¯ut+1 − γtxt+1

(cid:123)(cid:122)

(cid:124)

def= ¯wt+1

(cid:124)

(cid:123)(cid:122)

def= bt+1

+Z(cid:62)

t (b

(cid:48)
t+1 + γtF (cid:62)

t (tΛtHt)FtZtxt+1

) .

(6)

(cid:125)

(cid:125)

Again  it is clear that all the computations scale with s and not d  so both ¯wt+1 and bt+1 require only
(cid:62)
O(m2 + ms) time to maintain. Furthermore  the prediction w(cid:62)
t Zt−1xt can also
be computed in O(ms) time. The O(m3) in the overall complexity comes from a Gram-Schmidt
step in maintaining Ft (details in Appendix H).
The pseudocode is presented in Algorithms 4 and 5 with some details deferred to Appendix H. This
is the ﬁrst sparse implementation of online eigenvector computation to the best of our knowledge.

t xt = ¯w(cid:62)

t xt + b

5 Experiments

Preliminary experiments revealed that out of our three sketching options  Oja’s sketch generally has
better performance (see Appendix I). For more thorough evaluation  we implemented the sparse

4For simplicity  we assume that Vt−1 + ΓtVt−1(cid:98)gt(cid:98)g

does not reduce the dimension of Vt.

(cid:62)
t is always of full rank so that the orthonormalizing step

6

(Algorithm 5).

Receive example xt.

Algorithm 4 Sparse Sketched Online Newton with Oja’s Algorithm
Input: Parameters C  α and m.
1: Initialize ¯u = 0d×1 and b = 0m×1.
2: (Λ  F  Z  H) ← SketchInit(α  m)
3: for t = 1 to T do
4:
5:

Projection step: compute(cid:98)x = F Zxt and γ = τC ( ¯u(cid:62)xt+b(cid:62)Zxt)
t xt−(t−1)(cid:98)x(cid:62)ΛH(cid:98)x.
Obtain ¯w = ¯u − γxt and b ← b + γ(t − 1)F (cid:62)ΛH(cid:98)x (Equation 6).
t(yt)xt and the to-sketch vector(cid:98)g =
√
(Λ  F   Z  H  δ) ← SketchUpdate((cid:98)g)
b)(cid:98)g and b ← b + 1
α gt − (δ

Predict label yt = ¯w(cid:62)xt + b
Compute gradient gt = (cid:96)(cid:48)
Update weight: ¯u = ¯w − 1

Zxt and suffer loss (cid:96)t(yt).

(Algorithm 5).

σt + ηtgt.
α tF (cid:62)ΛHF Zgt

6:
7:
8:
9:
10: end for

x(cid:62)

(cid:62)

(cid:62)

(Equation 5).

Algorithm 5 Sparse Oja’s Sketch
Internal State: t  Λ  F   Z  H and K.
SketchInit(α  m)
1: Set t = 0  Λ = 0m×m  F = K = αH = I m and Z to any m × d matrix with orthonormal rows.
2: Return (Λ  F   Z  H).

SketchUpdate((cid:98)g)
1: Update t ← t+1. Pick a diagonal stepsize matrix Γt to update Λ ← (I−Γt)Λ+Γt diag{F Z(cid:98)g}2.
2: Set δ = A−1ΓtF Z(cid:98)g and update K ← K + δ(cid:98)g
3: Update Z ← Z + δ(cid:98)g

4: (L  Q) ← Decompose(F  K) (Algorithm 13)  so that LQZ = F Z and QZ is orthogonal. Set
5: Set H ← diag
α+tΛ1 1
6: Return (Λ  F   Z  H  δ).

Z(cid:62) + Z(cid:98)gδ

(cid:62)(cid:98)g)δδ

+ ((cid:98)g

 ···  

F = Q.

α+tΛm m

(cid:110)

(cid:111)

(cid:62).

(cid:62).

1

.

(cid:62)

(cid:62)

1

version of Oja-SON in Vowpal Wabbit.5 We compare it with ADAGRAD [6  25] on both synthetic and
real-world datasets. Each algorithm takes a stepsize parameter: 1
α serves as a stepsize for Oja-SON
and a scaling constant on the gradient matrix for ADAGRAD. We try both methods with the parameter
set to 2j for j = −3 −2  . . .   6 and report the best results. We keep the stepsize matrix in Oja-SON
t I m throughout. All methods make one online pass over data minimizing square loss.
ﬁxed as Γt = 1

5.1 Synthetic Datasets

To investigate Oja-SON’s performance in the setting it is really designed for  we generated a range
of synthetic ill-conditioned datasets as follows. We picked a random Gaussian matrix Z ∼ RT×d
(T = 10 000 and d = 100) and a random orthonormal basis V ∈ Rd×d. We chose a speciﬁc spectrum
λ ∈ Rd where the ﬁrst d − 10 coordinates are 1 and the rest increase linearly to some ﬁxed condition
number parameter κ. We let X = Zdiag{λ} 1
2 V (cid:62) be our example matrix  and created a binary
x)  where θ ∈ Rd is a random vector. We generated
(cid:62)
classiﬁcation problem with labels y = sign(θ
20 such datasets with the same Z  V and labels y but different values of κ ∈ {10  20  . . .   200}. Note
that if the algorithm is truly invariant  it would have the same behavior on these 20 datasets.
Fig. 1 (in Section 1) shows the ﬁnal progressive error (i.e. fraction of misclassiﬁed examples after one
pass over data) for ADAGRAD and Oja-SON (with sketch size m = 0  5  10) as the condition number
increases. As expected  the plot conﬁrms the performance of ﬁrst order methods such as ADAGRAD
degrades when the data is ill-conditioned. The plot also shows that as the sketch size increases 
Oja-SON becomes more accurate: when m = 0 (no sketch at all)  Oja-SON is vanilla gradient
descent and is worse than ADAGRAD as expected; when m = 5  the accuracy greatly improves; and
ﬁnally when m = 10  the accuracy of Oja-SON is substantially better and hardly worsens with κ.

5An open source machine learning toolkit available at http://hunch.net/~vw

7

Figure 2: Oja’s algorithm’s
eigenvalue recovery error.

Figure 3: (a) Comparison of two sketch sizes on real data 
and (b) Comparison against ADAGRAD on real data.

To further explain the effectiveness of Oja’s algorithm in identifying top eigenvalues and eigenvec-
tors  the plot in Fig. 2 shows the largest relative difference between the true and estimated top 10
eigenvalues as Oja’s algorithm sees more data. This gap drops quickly after seeing just 500 examples.

5.2 Real-world Datasets

the matrix(cid:80)t−1

Next we evaluated Oja-SON on 23 benchmark datasets from the UCI and LIBSVM repository (see
Appendix I for description of these datasets). Note that some datasets are very high dimensional but
very sparse (e.g. for 20news  d ≈ 102  000 and s ≈ 94)  and consequently methods with running
time quadratic (such as ONS) or even linear in dimension rather than sparsity are prohibitive.
In Fig. 3(a)  we show the effect of using sketched second order information  by comparing sketch
size m = 0 and m = 10 for Oja-SON (concrete error rates in Appendix I). We observe signiﬁcant
improvements in 5 datasets (acoustic  census  heart  ionosphere  letter)  demonstrating the advantage
of using second order information. However  we found that Oja-SON was outperformed by ADA-
GRAD on most datasets  mostly because the diagonal adaptation of ADAGRAD greatly reduces the
condition number on these datasets. Moreover  one disadvantage of SON is that for the directions not
in the sketch  it is essentially doing vanilla gradient descent. We expect better results using diagonal
adaptation as in ADAGRAD in off-sketch directions.
To incorporate this high level idea  we performed a simple modiﬁcation to Oja-SON: upon seeing
− 1
t xt to our algorithm instead of xt  where Dt ∈ Rd×d is the diagonal part of
example xt  we feed D
τ =1 gτ g(cid:62)
τ .6 The intuition is that this diagonal rescaling ﬁrst homogenizes the scales of
all dimensions. Any remaining ill-conditioning is further addressed by the sketching to some degree 
while the complementary subspace is no worse-off than with ADAGRAD. We believe this ﬂexibility
in picking the right vectors to sketch is an attractive aspect of our sketching-based approach.
With this modiﬁcation  Oja-SON outperforms ADAGRAD on most of the datasets even for m = 0 
as shown in Fig. 3(b) (concrete error rates in Appendix I). The improvement on ADAGRAD at
m = 0 is surprising but not impossible as the updates are not identical–our update is scale invariant
like Ross et al. [33]. However  the diagonal adaptation already greatly reduces the condition number
on all datasets except splice (see Fig. 4 in Appendix I for detailed results on this dataset)  so little
improvement is seen for sketch size m = 10 over m = 0. For several datasets  we veriﬁed the
accuracy of Oja’s method in computing the top-few eigenvalues (Appendix I)  so the lack of difference
between sketch sizes is due to the lack of second order information after the diagonal correction.
The average running time of our algorithm when m = 10 is about 11 times slower than ADAGRAD 
matching expectations. Overall  SON can signiﬁcantly outperform baselines on ill-conditioned data 
while maintaining a practical computational complexity.

2

Acknowledgements This work was done when Haipeng Luo and Nicolò Cesa-Bianchi were at
Microsoft Research  New York.

6D1 is deﬁned as 0.1 × I d to avoid division by zero.

8

020004000600080001000000.20.40.60.81number of examplesrelative eigenvalue difference κ=50κ=100κ=150κ=20000.10.20.30.400.10.20.30.4error rate of Oja−SON (m=0)error rate of Oja−SON (m=10) m = 0 vs m=1000.10.20.30.400.050.10.150.20.250.30.350.4error rate of AdaGraderror rate of Oja−SON AdaGrad vs Oja−SON (m=0)AdaGrad vs Oja−SON (m=10)References
[1] D. Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary coins. Journal of

Computer and System Sciences  66(4):671–687  2003.

[2] A. Balsubramani  S. Dasgupta  and Y. Freund. The fast convergence of incremental pca. In NIPS  2013.
[3] R. H. Byrd  S. Hansen  J. Nocedal  and Y. Singer. A stochastic quasi-newton method for large-scale

optimization. SIAM Journal on Optimization  26:1008–1031  2016.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press  2006.
[5] N. Cesa-Bianchi  A. Conconi  and C. Gentile. A second-order perceptron algorithm. SIAM Journal on

Computing  34(3):640–668  2005.

[6] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. JMLR  12:2121–2159  2011.

[7] M. A. Erdogdu and A. Montanari. Convergence rates of sub-sampled newton methods. In NIPS  2015.
[8] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly  3

(1-2):95–110  1956.

[9] W. Gao  R. Jin  S. Zhu  and Z.-H. Zhou. One-pass auc optimization. In ICML  2013.
[10] D. Garber and E. Hazan. A linearly convergent conditional gradient algorithm with applications to online

and stochastic optimization. SIAM Journal on Optimization  26:1493–1528  2016.
[11] D. Garber  E. Hazan  and T. Ma. Online learning of eigenvectors. In ICML  2015.
[12] M. Ghashami  E. Liberty  J. M. Phillips  and D. P. Woodruff. Frequent directions: Simple and deterministic

matrix sketching. SIAM Journal on Computing  45:1762–1792  2015.

[13] M. Ghashami  E. Liberty  and J. M. Phillips. Efﬁcient frequent directions algorithm for sparse matrices. In

KDD  2016.

[14] A. Gonen and S. Shalev-Shwartz. Faster sgd using sketched conditioning. arXiv:1506.02649  2015.
[15] A. Gonen  F. Orabona  and S. Shalev-Shwartz. Solving ridge regression using sketched preconditioned

svrg. In ICML  2016.

[16] M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In NIPS  2014.
[17] E. Hazan and S. Kale. Projection-free online learning. In ICML  2012.
[18] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine

Learning  69(2-3):169–192  2007.

[19] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.

In STOC  1998.

[20] M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML  2013.
[21] D. M. Kane and J. Nelson. Sparser johnson-lindenstrauss transforms. Journal of the ACM  61(1):4  2014.
[22] C.-L. Li  H.-T. Lin  and C.-J. Lu. Rivalry of two families of algorithms for memory-restricted streaming

pca. arXiv:1506.01490  2015.

[23] E. Liberty. Simple and deterministic matrix sketching. In KDD  2013.
[24] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical

programming  45(1-3):503–528  1989.

[25] H. B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In COLT 

2010.

[26] A. Mokhtari and A. Ribeiro. Global convergence of online limited memory bfgs. JMLR  16:3151–3181 

2015.

[27] P. Moritz  R. Nishihara  and M. I. Jordan. A linearly-convergent stochastic l-bfgs algorithm. In AISTATS 

2016.

[28] E. Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical biology  15

(3):267–273  1982.

[29] E. Oja and J. Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the expectation

of a random matrix. Journal of mathematical analysis and applications  106(1):69–84  1985.

[30] F. Orabona and D. Pál. Scale-free algorithms for online linear optimization. In ALT  2015.
[31] F. Orabona  K. Crammer  and N. Cesa-Bianchi. A generalized online mirror descent with applications to

classiﬁcation and regression. Machine Learning  99(3):411–435  2015.

[32] M. Pilanci and M. J. Wainwright. Newton sketch: A linear-time optimization algorithm with linear-

quadratic convergence. arXiv:1505.02250  2015.

[33] S. Ross  P. Mineiro  and J. Langford. Normalized online learning. In UAI  2013.
[34] N. N. Schraudolph  J. Yu  and S. Günter. A stochastic quasi-newton method for online convex optimization.

In AISTATS  2007.

[35] J. Sohl-Dickstein  B. Poole  and S. Ganguli. Fast large-scale optimization by unifying stochastic gradient

and quasi-newton methods. In ICML  2014.

[36] D. P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Machine

Learning  10(1-2):1–157  2014.

9

,Justin Domke
Haipeng Luo
Alekh Agarwal
Nicolò Cesa-Bianchi
John Langford