2013,Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model,In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are in two folds. First  in low dimensions and under a double asymptotic framework where both the dimension $d$ and sample size $n$ can increase  by borrowing the strength from recent development in minimax optimal principal component estimation  we first time sharply characterize the potential advantage of classical principal component regression over least square estimation under the Gaussian model. Secondly  we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian  including many well known distributions such as multivariate Gaussian  rank-deficient Gaussian  $t$  Cauchy  and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra flexibilities make it very suitable for modeling finance and biomedical imaging data. Under the elliptical model  we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method.,Robust Sparse Principal Component Regression

under the High Dimensional Elliptical Model

Fang Han

Department of Biostatistics
Johns Hopkins University

Baltimore  MD 21210
fhan@jhsph.edu

Han Liu

Department of Operations Research

and Financial Engineering

Princeton University
Princeton  NJ 08544

hanliu@princeton.edu

Abstract

In this paper we focus on the principal component regression and its application to
high dimension non-Gaussian data. The major contributions are two folds. First 
in low dimensions and under the Gaussian model  by borrowing the strength from
recent development in minimax optimal principal component estimation  we ﬁrst
time sharply characterize the potential advantage of classical principal component
regression over least square estimation. Secondly  we propose and analyze a new
robust sparse principal component regression on high dimensional elliptically dis-
tributed data. The elliptical distribution is a semiparametric generalization of the
Gaussian  including many well known distributions such as multivariate Gaus-
sian  rank-deﬁcient Gaussian  t  Cauchy  and logistic. It allows the random vector
to be heavy tailed and have tail dependence. These extra ﬂexibilities make it very
suitable for modeling ﬁnance and biomedical imaging data. Under the elliptical
model  we prove that our method can estimate the regression coefﬁcients in the
optimal parametric rate and therefore is a good alternative to the Gaussian based
methods. Experiments on synthetic and real world data are conducted to illustrate
the empirical usefulness of the proposed method.

Introduction

1
Principal component regression (PCR) has been widely used in statistics for years (Kendall  1968).
Take the classical linear regression with random design for example. Let x1  . . .   xn ∈ Rd be n
independent realizations of a random vector X ∈ Rd with mean 0 and covariance matrix Σ. The
classical linear regression model and simple principal component regression model can be elaborated
as follows:

(Classical linear regression model)
(Principal Component Regression Model)

Y = Xβ + ;

(1.1)
where X = (x1  . . .   xn)T ∈ Rn×d  Y ∈ Rn  ui is the i-th leading eigenvector of Σ  and  ∈
Nn(0  σ2Id) is independent of X  β ∈ Rd and α ∈ R. Here Id ∈ Rd×d is the identity matrix. The

principal component regression then can be conducted in two steps: First we obtain an estimator(cid:98)u1
of u1; Secondly we project the data in the direction of (cid:98)u1 and solve a simple linear regression in

Y = αXu1 +  

estimating α.
By checking Equation (1.1)  it is easy to observe that the principal component regression model is a
subset of the general linear regression (LR) model with the constraint that the regression coefﬁcient
β is proportional to u1. There has been a lot of discussions on the advantage of principal component
regression over classical linear regression. In low dimensional settings  Massy (1965) pointed out
that principal component regression can be much more efﬁcient in handling collinearity among pre-
dictors compared to the linear regression. More recently  Cook (2007) and Artemiou and Li (2009)
argued that principal component regression has potential to play a more important role. In partic-

ular  letting (cid:98)uj be the j-th leading eigenvector of the sample covariance matrix (cid:98)Σ of x1  . . .   xn 

1

Artemiou and Li (2009) show that under mild conditions with high probability the correlation be-

tween the response Y and X(cid:98)ui is higher than or equal to the correlation between Y and X(cid:98)uj when

i < j. This indicates  although not rigorous  there is possibility that principal component regression
can borrow strength from the low rank structure of Σ  which motivates our work.
Even though the statistical performance of principal component regression in low dimensions is not
fully understood  there is even less analysis on principal component regression in high dimensions
where the dimension d can be even exponentially larger than the sample size n. This is partially
due to the fact that estimating the leading eigenvectors of Σ itself has been difﬁcult enough. For
example  Johnstone and Lu (2009) show that  even under the Gaussian model  when d/n → γ

for some γ > 0  there exist multiple settings under which (cid:98)u1 can be an inconsistent estimator of

u1. To attack this “curse of dimensionality”  one solution is adding a sparsity assumption on u1 
leading to various versions of the sparse PCA. See  Zou et al. (2006); d’Aspremont et al. (2007);
Moghaddam et al. (2006)  among others. Under the (sub)Gaussian settings  minimax optimal rates
are being established in estimating u1  . . .   um (Vu and Lei  2012; Ma  2013; Cai et al.  2013).
Very recently  Han and Liu (2013b) relax the Gaussian assumption in conducting a scale invariant
version of the sparse PCA (i.e.  estimating the leading eigenvector of the correlation instead of the
covariance matrix). However  it can not be easily applied to estimate u1 and the rate of convergence
they proved is not the parametric rate.
This paper improves upon the aforementioned results in two directions. First  with regard to the
classical principal component regression  under a double asymptotic framework in which d is al-
lowed to increase with n  by borrowing the very recent development in principal component anal-
ysis (Vershynin  2010; Lounici  2012; Bunea and Xiao  2012)  we for the ﬁrst time explicitly show
the advantage of principal component regression over the classical linear regression. We explicitly
conﬁrm the following two advantages of principal component regression: (i) Principal component
regression is insensitive to collinearity  while linear regression is very sensitive to; (ii) Principal
component regression can utilize the low rank structure of the covariance matrix Σ  while linear
regression cannot.
Secondly  in high dimensions where d can increase much faster  even exponentially faster  than n 
we propose a robust method in conducting (sparse) principal component regression under a non-
Gaussian elliptical model. The elliptical distribution is a semiparametric generalization to the Gaus-
sian  relaxing the light tail and zero tail dependence constraints  but preserving the symmetry prop-
erty. We refer to Kl¨uppelberg et al. (2007) for more details. This distribution family includes many
well known distributions such as multivariate Gaussian  rank deﬁcient Gaussian  t  logistic  and
many others. Under the elliptical model  we exploit the result in Han and Liu (2013a)  who showed
that by utilizing a robust covariance matrix estimator  the multivariate Kendall’s tau  we can obtain

an estimator(cid:101)u1  which can recover u1 in the optimal parametric rate as shown in Vu and Lei (2012).
We then exploit (cid:101)u1 in conducting principal component regression and show that the obtained esti-
mator ˇβ can estimate β in the optimal(cid:112)s log d/n rate. The optimal rate in estimating u1 and

β  combined with the discussion in the classical principal component regression  indicates that the
proposed method has potential to handle high dimensional complex data and has its advantage over
high dimensional linear regression methods  such as ridge regression and lasso. These theoretical
results are also backed up by numerical experiments on both synthetic and real world equity data.
2 Classical Principal Component Regression
This section is devoted to the discussion on the advantage of classical principal component re-
gression over the classical linear regression. We start with a brief introduction of notations. Let
M = [Mij] ∈ Rd×d and v = (v1  ...  vd)T ∈ Rd. We denote vI to be the subvector of v whose
entries are indexed by a set I. We also denote MI J to be the submatrix of M whose rows are
indexed by I and columns are indexed by J. Let MI∗ and M∗J be the submatrix of M with rows
indexed by I  and the submatrix of M with columns indexed by J. Let supp(v) := {j : vj (cid:54)= 0}.
For 0 < q < ∞  we deﬁne the (cid:96)0  (cid:96)q and (cid:96)∞ vector norms as

d(cid:88)

i=1

(cid:107)v(cid:107)0 := card(supp(v))  (cid:107)v(cid:107)q := (

|vi|q)1/q and (cid:107)v(cid:107)∞ := max
1≤i≤d

|vi|.

Let Tr(M) be the trace of M. Let λj(M) be the j-th largest eigenvalue of M and Θj(M) be the
corresponding leading eigenvector. In particular  we let λmax(M) := λ1(M) and λmin(M) :=

2

λd(M). We deﬁne Sd−1 := {v ∈ Rd : (cid:107)v(cid:107)2 = 1} to be the d-dimensional unit sphere. We deﬁne
the matrix (cid:96)max norm and (cid:96)2 norm as (cid:107)M(cid:107)max := max{|Mij|} and (cid:107)M(cid:107)2 := supv∈Sd−1 (cid:107)Mv(cid:107)2.
We deﬁne diag(M) to be a diagonal matrix with [diag(M)]jj = Mjj for j = 1  . . .   d. We denote
vec(M) := (MT∗1  . . .   MT∗d)T . For any two sequence {an} and {bn}  we denote an
c C(cid:16) bn if there
exist two ﬁxed constants c  C such that c ≤ an/bn ≤ C.
Let x1  . . .   xn ∈ Rd be n independent observations of a d-dimensional random vector X ∼
Nd(0  Σ)  u1 := Θ1(Σ) and 1  . . .   n ∼ N1(0  σ2) are independent from each other and
{Xi}n

i=1. We suppose that the following principal component regression model holds:

Y = αXu1 +  

(2.1)
where Y = (Y1  . . .   Yn)T ∈ Rn  X = [x1  . . .   xn]T ∈ Rn×d and  = (1  . . .   n)T ∈ Rn. We
are interested in estimating the regression coefﬁcient β := αu1.

Let (cid:98)β represent the solution of the classical least square estimator without taking the information
that β is proportional to u1 into account. (cid:98)β can be expressed as follows:
We then have the following proposition  which shows that the mean square error of (cid:98)β − β is highly

(cid:98)β := (XT X)−1XT Y .

(2.2)

related to the scale of λmin(Σ).
Proposition 2.1. Under the principal component regression model shown in (2.1)  we have

E(cid:107)(cid:98)β − β(cid:107)2

σ2

2 =

n − d − 1

+ ··· +

1

λd(Σ)

λ1(Σ)

(cid:18) 1

(cid:19)

.

Proposition 2.1 reﬂects the vulnerability of least square estimator on the collinearity. More speciﬁ-

estimator even when d is ﬁxed. On the other hand  using the Markov inequality  when λd(Σ) is

Motivated from Equation (2.1)  the classical principal component regression estimator can be elab-
orated as follows.

cally  when λd(Σ) is extremely small  going to zero in the scale of O(1/n)  (cid:98)β can be an inconsistent
lower bounded by a ﬁxed constant and d = o(n)  the rate of convergence of (cid:98)β is well known to be
OP ((cid:112)d/n).
(cid:80) xixT
(1) We ﬁrst estimate u1 using the leading eigenvector(cid:98)u1 of the sample covariance(cid:98)Σ := 1
data (cid:98)Z := X(cid:98)u1 ∈ Rn:
The ﬁnal principal component regression estimator (cid:101)β is then obtained as (cid:101)β = (cid:101)α(cid:98)u1. We then have
the following important theorem  which provides a rate of convergence for (cid:101)β to approximate β.

i .
(2) We then estimate α ∈ R in Equation (2.1) by the standard least square estimation on the projected

(cid:101)α := ((cid:98)ZT (cid:98)Z)−1(cid:98)ZT Y  

Theorem 2.2. Let r∗(Σ) := Tr(Σ)/λmax(Σ) represent the effective rank of Σ (Vershynin  2010).
Suppose that

n

(cid:114)
(cid:32)

(cid:107)Σ(cid:107)2 ·

(cid:40)(cid:114) 1

n

+

α +

r∗(Σ) log d

n

= o(1).

(cid:33)

(cid:114)

·

1(cid:112)λmax(Σ)

(cid:107)(cid:101)β − β(cid:107)2 = OP

Under the Model (2.1)  when λmax(Σ) > c1 and λ2(Σ)/λ1(Σ) < C1 < 1 for some ﬁxed constants
C1 and c1  we have

(cid:41)

r∗(Σ) log d

n

.

(2.3)

Theorem 2.2  compared to Proposition 2.1  provides several important messages on the performance

of principal component regression. First  compared to the least square estimator (cid:98)β  (cid:101)β is insensitive
to collinearity in the sense that λmin(Σ) plays no role in the rate of convergence of (cid:101)β. Secondly 
the rate of convergence for (cid:98)β is OP ((cid:112)d/n) and for (cid:101)β is OP ((cid:112)r∗(Σ) log d/n)  while r∗(Σ) :=

when λmin(Σ) is lower bounded by a ﬁxed constant and α is upper bounded by a ﬁxed constant 

3

Tr(Σ)/λmax(Σ) ≤ d and is of order o(d) when there exists a low rank structure for Σ. These
two observations  combined together  illustrate the advantages of the classical principal component
regression over least square estimation. These advantages justify the use of principal component

regression. There is one more thing to be noted: the performance of (cid:101)β  unlike (cid:98)β  depends on α.
When α is small  (cid:101)β can predict β more accurately.

These three observations are veriﬁed in Figure 1. Here the data are generated according to Equation
(2.1) and we set n = 100  d = 10  Σ to be a diagonal matrix with descending diagonal values
Σii = λi and σ2 = 1. In Figure 1(A)  we set α = 1  λ1 = 10  λj = 1 for j = 2  . . .   d − 1  and
changing λd from 1 to 1/100; In Figure 1(B)  we set α = 1  λj = 1 for j = 2  . . .   d and changing
λ1 from 1 to 100; In Figure 1(C)  we set λ1 = 10  λj = 1 for j = 2  . . .   d  and changing α from
0.1 to 10. In the three ﬁgures  the empirical mean square error is plotted against 1/λd  λ1  and α. It
can be observed that the results  each by each  matches the theory.

A

B

C

Figure 1: Justiﬁcation of Proposition 2.1 and Theorem 2.2. The empirical mean square errors are
plotted against 1/λd  λ1  and α separately in (A)  (B)  and (C). Here the results of classical linear
regression and principal component regression are marked in black solid line and red dotted line.
3 Robust Sparse Principal Component Regression under Elliptical Model
In this section  we propose a new principal component regression method. We generalize the settings
in classical principal component regression discussed in the last section in two directions: (i) We
consider the high dimensional settings where the dimension d can be much larger than the sample
size n; (ii) In modeling the predictors x1  . . .   xn  we consider a more general elliptical  instead of
the Gaussian distribution family. The elliptical family can capture characteristics such as heavy tails
and tail dependence  making it more suitable for analyzing complex datasets in ﬁnance  genomics 
and biomedical imaging.
3.1 Elliptical Distribution
In this section we deﬁne the elliptical distribution and introduce the basic property of the elliptical
distribution. We denote by X d= Y if random vectors X and Y have the same distribution.
Here we only consider the continuous random vectors with density existing. To our knowledge 
there are essentially four ways to deﬁne the continuous elliptical distribution with density. The most
intuitive way is as follows: A random vector X ∈ Rd is said to follow an elliptical distribution
ECd(µ  Σ  ξ) if and only there exists a random variable ξ > 0 (a.s.) and a Gaussian distribution
Z ∼ Nd(0  Σ) such that

X d= µ + ξZ.

(3.1)

Note that here ξ is not necessarily independent of Z. Accordingly  elliptical distribution can be
regarded as a semiparametric generalization to the Gaussian distribution  with the nonparametric
part ξ. Because ξ can be very heavy tailed  X can also be very heavy tailed. Moreover  when Eξ2
exists  we have

Cov(X) = Eξ2Σ and Θj(Cov(X)) = Θj(Σ) for j = 1  . . .   d.

This implies that  when Eξ2 exists  to recover u1 := Θ1(Cov(X))  we only need to recover Θ1(Σ).
Here Σ is conventionally called the scatter matrix.

4

0204060801000.20.40.60.81.01/lambda_minMean Square ErrorLRPCR0204060801000.00.20.40.60.8lambda_maxMean Square ErrorLRPCR02468100.00.20.40.60.81.0alphaMean Square ErrorLRPCRWe would like to point out that the elliptical family is signiﬁcantly larger than the Gaussian. In
fact  Gaussian is fully parameterized by ﬁnite dimensional parameters (mean and variance).
In
contrast  the elliptical is a semiparametric family (since the elliptical density can be represented as
g((x−µ)T Σ−1(x−µ)) where the function g(·) function is completely unspeciﬁed.). If we consider
the “volumes” of the family of the elliptical family and the Gaussian family with respect to the
Lebesgue reference measure  the volume of Gaussian family is zero (like a line in a 3-dimensional
space)  while the volume of the elliptical family is positive (like a ball in a 3-dimensional space).
3.2 Multivariate Kendall’s tau
As a important step in conducting the principal component regression  we need to estimate u1 =
Θ1(Cov(X)) = Θ1(Σ) as accurately as possible. Since the random variable ξ in Equation (3.1)
can be very heavy tailed  the according elliptical distributed random vector can be heavy tailed.
Therefore  as has been pointed out by various authors (Tyler  1987; Croux et al.  2002; Han and Liu 

2013b)  the leading eigenvector of the sample covariance matrix (cid:98)Σ can be a bad estimator in esti-
of this estimator. Let X ∼ ECd(µ  Σ  ξ) and (cid:102)X be an independent copy of X. The population

mating u1 = Θ1(Σ) under the elliptical distribution. This motivates developing robust estimator.
In particular  in this paper we consider using the multivariate Kendall’s tau (Choi and Marden  1998)
and recently deeply studied by Han and Liu (2013a). In the following we give a brief description
multivariate Kendall’s tau matrix  denoted by K ∈ Rd×d  is deﬁned as:

.

(3.2)

(cid:32)

K := E

(cid:33)

(X −(cid:102)X)(X −(cid:102)X)T

(cid:107)X −(cid:102)X(cid:107)2
(cid:88)

2

i(cid:54)=j

(cid:32)

(cid:33)

1

n(n − 1)

Let x1  . . .   xn be n independent observations of X. The sample version of multivariate Kendall’s

tau is accordingly deﬁned as(cid:98)K =
(cid:98)K is a matrix version U statistic and it is easy to see that
and we have that E((cid:98)K) = K.
maxjk |Kjk| ≤ 1  maxjk |(cid:98)Kjk| ≤ 1. Therefore  (cid:98)K is a bounded matrix and hence can be a nicer

(xi − xj)(xi − xj)T

statistic than the sample covariance matrix. Moreover  we have the following important proposition 
coming from Oja (2010)  showing that K has the same eigenspace as Σ and Cov(X).
Proposition 3.1 (Oja (2010)). Let X ∼ ECd(µ  Σ  ξ) be a continuous distribution and K be the
population multivariate Kendall’s tau statistic. Then if λj(Σ) (cid:54)= λk(Σ) for any k (cid:54)= j  we have

(cid:107)xi − xj(cid:107)2

(3.3)

 

2

Θj(Σ) = Θj(K) and λj(K) = E

λj(Σ)U 2
j
1 + . . . + λd(Σ)U 2
d

λ1(Σ)U 2

 

(3.4)

where U := (U1  . . .   Ud)T follows a uniform distribution in Sd−1. In particular  when Eξ2 exists 
Θj(Cov(X)) = Θj(K).

3.3 Model and Method
In this section we discuss the model we build and the accordingly proposed method in conducting
high dimensional (sparse) principal component regression on non-Gaussian data.
Similar as in Section 2  we consider the classical simple principal component regression model:

Y = αXu1 +  = α[x1  . . .   xn]T u1 + .

To relax the Gaussian assumption  we assume that both x1  . . .   xn ∈ Rd and 1  . . .   n ∈ R be
elliptically distributed. We assume that xi ∈ ECd(0  Σ  ξ). To allow the dimension d increasing
much faster than n  we impose a sparsity structure on u1 = Θ1(Σ). Moreover  to make u1 iden-
tiﬁable  we assume that λ1(Σ) (cid:54)= λ2(Σ). Thusly  the formal model of the robust sparse principal
component regression considered in this paper is as follows:
Md(Y   ; Σ  ξ  s) :

(cid:26) Y = αXu1 +  

x1  . . .   xn ∼ ECd(0  Σ  ξ)  (cid:107)Θ1(Σ)(cid:107)0 ≤ s  λ1(Σ) (cid:54)= λ2(Σ).

(3.5)

5

(cid:101)u1 = arg max

vT(cid:98)Kv 

Then the robust sparse principal component regression can be elaborated as a two step procedure:
(i) Inspired by the model Md(Y   ; Σ  ξ  s) and Proposition 3.1  we consider the following opti-
mization problem to estimate u1 := Θ1(Σ):

subject to v ∈ Sd−1 ∩ B0(s) 

(3.6)

v∈Rd

of Θ1(Cov(X))  whenever the covariance matrix exists.
(ii) We then estimate α ∈ R in Equation (3.5) by the standard least square estimation on the projected

where B0(s) := {v ∈ Rd : (cid:107)v(cid:107)0 ≤ s} and (cid:98)K is the estimated multivariate Kendall’s tau matrix.
The corresponding global optimum is denoted by(cid:101)u1. Using Proposition 3.1 (cid:101)u1 is also an estimator
data (cid:101)Z := X(cid:101)u1 ∈ Rn:
The ﬁnal principal component regression estimator ˇβ is then obtained as ˇβ = ˇα(cid:101)u1.

ˇα := ((cid:101)ZT (cid:101)Z)−1(cid:101)ZT Y  

3.4 Theoretical Property
In Theorem 2.2  we show that how to estimate u1 accurately plays an important role in conducting
the principal component regression. Following this discussion and the very recent results in Han and
Liu (2013a)  the following “easiest” and “hardest” conditions are considered. Here κL  κU are two
constants larger than 1.

1 κU(cid:16) dλj(Σ) for any j ∈ {2  . . .   d} and λ2(Σ)

Condition 1 (“Easiest”): λ1(Σ)
j ∈ {3  . . .   d};
Condition 2 (“Hardest”): λ1(Σ)
In the sequel  we say that the model Md(Y   ; Σ  ξ  s) holds if the data (Y   X) are generated using
the model Md(Y   ; Σ  ξ  s).
Under Conditions 1 and 2  we then have the following theorem  which shows that under certain

conditions  (cid:107) ˇβ − β(cid:107)2 = OP ((cid:112)s log d/n)  which is the optimal parametric rate in estimating the

κL κU(cid:16) λj(Σ) for any j ∈ {2  . . .   d}.

1 κU(cid:16) λj(Σ) for any

regression coefﬁcient (Ravikumar et al.  2008).
Theorem 3.2. Let the model Md(Y   ; Σ  ξ  s) hold and |α| in Equation (3.5) are upper bounded
by a constant and (cid:107)Σ(cid:107)2 is lower bounded by a constant. Then under Condition 1 or Condition 2
and for all random vector X such that

we have the robust principal component regression estimator ˇβ satisﬁes that

max

v∈Sd−1 (cid:107)v(cid:107)0≤2s

|vT ((cid:98)Σ − Σ)v| = oP (1) 

(cid:32)(cid:114)

(cid:33)

(cid:107) ˇβ − β(cid:107)2 = OP

s log d

n

.

Normal

multivariate-t

EC1

EC2

Figure 2: Curves of averaged estimation errors between the estimates and true parameters for differ-
ent distributions (normal  multivariate-t  EC1  and EC2  from left to right) using the truncated power
method. Here n = 100  d = 200  and we are interested in estimating the regression coefﬁcient β.
The horizontal-axis represents the cardinalities of the estimates’ support sets and the vertical-axis
represents the empirical mean square error. Here from the left to the right  the minimum mean
square errors for lasso are 0.53  0.55  1  and 1.

6

0204060800.00.20.40.60.81.01.2number of selected featuresaveraged errorPCRRPCR0204060800.00.20.40.60.81.01.2number of selected featuresaveraged errorPCRRPCR0204060800.00.51.01.5number of selected featuresaveraged errorPCRRPCR0204060800.00.20.40.60.81.01.21.4number of selected featuresaveraged errorPCRRPCR4 Experiments

In this section we conduct study on both synthetic and real-world data to investigate the empirical
performance of the robust sparse principal component regression proposed in this paper. We use the
truncated power algorithm proposed in Yuan and Zhang (2013) to approximate the global optimums

(cid:101)u1 to (3.6). Here the cardinalities of the support sets of the leading eigenvectors are treated as tuning

parameters. The following three methods are considered:
lasso: the classical L1 penalized regression;
PCR: The sparse principal component regression using the sample covariance matrix as the sufﬁ-
cient statistic and exploiting the truncated power algorithm in estimating u1;
RPCR: The robust sparse principal component regression proposed in this paper  using the mul-
tivariate Kendall’s tau as the sufﬁcient statistic and exploiting the truncated power algorithm to
estimate u1.

4.1 Simulation Study

d=

κξ∗

1 /ξ∗

2. Here ξ∗

1

d= χd and ξ∗

2

√

1 + . . . + Y 2
d

j=1(ωj−ωd)ujuT

i=1 si (cid:80)j

as Σ =(cid:80)2

i.i.d.∼ N (0  1) (cid:112)Y 2

sj for k ∈ [1 +(cid:80)j−1

d= χd. Here χd is the chi-distribution with degree of freedom
d= χd. In this setting  X follows the Gaussian
√

In this section  we conduct simulation study to back up the theoretical results and further investigate
the empirical performance of the proposed robust sparse principal component regression method.
To illustrate the empirical usefulness of the proposed method  we ﬁrst consider generating the data
matrix X. To generate X  we need to consider how to generate Σ and ξ.
In detail  let ω1 >
ω2 > ω3 = . . . = ωd be the eigenvalues and u1  . . .   ud be the eigenvectors of Σ with uj :=
(uj1  . . .   ujd)T . The top 2 leading eigenvectors u1  u2 of Σ are speciﬁed to be sparse with sj :=
(cid:107)uj(cid:107)0 and ujk = 1/
i=1 si] and zero for all the others. Σ is generated
j +ωdId. Across all settings  we let s1 = s2 = 10  ω1 = 5.5  ω2 = 2.5 
and ωj = 0.5 for all j = 3  . . .   d. With Σ  we then consider the following four different elliptical
distributions:
(Normal) X ∼ ECd(0  Σ  ζ1) with ζ1
d. For Y1  . . .   Yd
distribution (Fang et al.  1990).
(Multivariate-t) X ∼ ECd(0  Σ  ζ2) with ζ2
d= χκ with
κ ∈ Z+. In this setting  X follows a multivariate-t distribution with degree of freedom κ (Fang
et al.  1990). Here we consider κ = 3.
(EC1) X ∼ ECd(0  Σ  ζ3) with ζ3 ∼ F (d  1)  an F distribution.
(EC2) X ∼ ECd(0  Σ  ζ4) with ζ4 ∼ Exp(1)  an exponential distribution.
We then simulate x1  . . .   xn from X. This forms a data matrix X. Secondly  we let Y = Xu1 +  
where  ∼ Nn(0  In). This produces the data (Y   X). We repeatedly generate n data according
to the four distributions discussed above for 1 000 times. To show the estimation accuracy  Figure
2 plots the empirical mean square error between the estimate ˇu1 and true regression coefﬁcient β
against the numbers of estimated nonzero entries (deﬁned as (cid:107) ˇu1(cid:107)0)  for PCR and RPCR  under
different schemes of (n  d)  Σ and different distributions. Here we considered n = 100 and d = 200.
It can be seen that we do not plot the results of lasso in Figure 2. As discussed in Section 2 
especially as shown in Figure 1  linear regression and principal component regression have their
own advantages in different settings. More speciﬁcally  we do not plot the results of lasso here
simply because it performs so bad under our simulation settings. For example  under the Gaussian
settings with n = 100 and d = 200  the lowest mean square error for lasso is 0.53 and the errors
are averagely above 1.5  while for RPCR is 0.13 and is averagely below 1.
Figure 2 shows when the data are non-Gaussian but follow an elliptically distribution  RPCR out-
performs PCR constantly in terms of estimation accuracy. Moreover  when the data are indeed nor-
mally distributed  there is no obvious difference between RPCR and PCR  indicating that RPCR
is a safe alternative to the classical sparse principal component regression.

7

A

B

Figure 3: (A) Quantile vs. quantile plot of the log-return values for one stock ”Goldman Sachs”.
(B) Prediction error against the number of features selected. The scale of the prediction errors is
enlarged by 100 times for better visualization.

4.2 Application to Equity Data
In this section we apply the proposed robust sparse principal component regression and the other
two methods to the stock price data from Yahoo! Finance (finance.yahoo.com). We collect
the daily closing prices for 452 stocks that are consistently in the S&P 500 index between January 1 
2003 through January 1  2008. This gives us altogether T=1 257 data points  each data point corre-
sponds to the vector of closing prices on a trading day. Let St = [Stt j] denote by the closing price of
stock j on day t. We are interested in the log return data X = [Xtj] with Xtj = log(Stt j/Stt−1 j).
We ﬁrst show that this data set is non-Gaussian and heavy tailed. This is done ﬁrst by conducting
marginal normality tests (Kolmogorove-Smirnov  Shapiro-Wilk  and Lillifors) on the data. We ﬁnd
that at most 24 out of 452 stocks would pass any of three normality test. With Bonferroni correction
there are still over half stocks that fail to pass any normality tests. Moreover  to illustrate the heavy
tailed issue  we plot the quantile vs. quantile plot for one stock  “Goldman Sachs”  in Figure 3(A).
It can be observed that the log return values for this stock is heavy tailed compared to the Gaussian.
To illustrate the power of the proposed method  we pick a subset of the data ﬁrst. The stocks can
be summarized into 10 Global Industry Classiﬁcation Standard (GICS) sectors and we are focusing
on the subcategory “Financial”. This leave us 74 stocks and we denote the resulting data to be
F ∈ R1257×74. We are interested in predicting the log return value in day t for each stock indexed
by k (i.e.  treating Ft k as the response) using the log return values for all the stocks in day t − 1
to day t − 7 (i.e.  treating vec(Ft−7≤t(cid:48)≤t−1 ·) as the predictor). The dimension for the regressor is
accordingly 7 × 74 = 518. For each stock indexed by k  to learn the regression coefﬁcient βk  we
use Ft(cid:48)∈{1 ... 1256} · as the training data and applying the three different methods on this dataset. For

each method  after obtaining an estimator (cid:98)βk  we use vec(Ft(cid:48)∈{1250 ... 1256} ·)(cid:98)β to estimate F1257 k.
number of features selected (i.e.  (cid:107)(cid:98)β(cid:107)0) in Figure 3(B). To visualize the difference more clearly  in

This procedure is repeated for each k and the averaged prediction errors are plotted against the

the ﬁgures we enlarge the scale of the prediction errors by 100 times. It can be observed that RPCR
has the universally lowest prediction error with regard to different number of features.

Acknowledgement

Han’s research is supported by a Google fellowship. Liu is supported by NSF Grants III-1116730
and NSF III-1332109  an NIH sub-award and a FDA sub-award from Johns Hopkins University.

8

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll−3−2−10123−6−4−2024Theoretical QuantilesSample Quantiles0501001500.100.150.200.250.300.350.400.45number of selected featuresaveraged prediction errorlassoPCRRPCRReferences
Artemiou  A. and Li  B. (2009). On principal components and regression: a statistical explanation of a natural

phenomenon. Statistica Sinica  19(4):1557.

Bunea  F. and Xiao  L. (2012). On the sample covariance matrix estimator of reduced effective rank population

matrices  with applications to fPCA. arXiv preprint arXiv:1212.5321.

Cai  T. T.  Ma  Z.  and Wu  Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. The Annals of

Statistics (to appear).

Choi  K. and Marden  J. (1998). A multivariate version of kendall’s τ. Journal of Nonparametric Statistics 

9(3):261–293.

Cook  R. D. (2007). Fisher lecture: Dimension reduction in regression. Statistical Science  22(1):1–26.
Croux  C.  Ollila  E.  and Oja  H. (2002). Sign and rank covariance matrices: statistical properties and ap-
plication to principal components analysis. In Statistical data analysis based on the L1-norm and related
methods  pages 257–269. Springer.

d’Aspremont  A.  El Ghaoui  L.  Jordan  M. I.  and Lanckriet  G. R. (2007). A direct formulation for sparse

PCA using semideﬁnite programming. SIAM review  49(3):434–448.

Fang  K.  Kotz  S.  and Ng  K. (1990). Symmetric multivariate and related distributions. Chapman&Hall 

London.

Han  F. and Liu  H. (2013a). Optimal sparse principal component analysis in high dimensional elliptical model.

arXiv preprint arXiv:1310.3561.

Han  F. and Liu  H. (2013b). Scale-invariant sparse PCA on high dimensional meta-elliptical data. Journal of

the American Statistical Association (in press).

Johnstone  I. M. and Lu  A. Y. (2009). On consistency and sparsity for principal components analysis in high

dimensions. Journal of the American Statistical Association  104(486).

Kendall  M. G. (1968). A course in multivariate analysis.
Kl¨uppelberg  C.  Kuhn  G.  and Peng  L. (2007). Estimating the tail dependence function of an elliptical

distribution. Bernoulli  13(1):229–251.

Lounici  K. (2012).
arXiv:1205.7060.

Sparse principal component analysis with missing observations.

arXiv preprint

Ma  Z. (2013). Sparse principal component analysis and iterative thresholding. to appear Annals of Statistics.
Massy  W. F. (1965). Principal components regression in exploratory statistical research. Journal of the Amer-

ican Statistical Association  60(309):234–256.

Moghaddam  B.  Weiss  Y.  and Avidan  S. (2006). Spectral bounds for sparse PCA: Exact and greedy algo-

rithms. Advances in neural information processing systems  18:915.

Oja  H. (2010). Multivariate Nonparametric Methods with R: An approach based on spatial signs and ranks 

volume 199. Springer.

Ravikumar  P.  Raskutti  G.  Wainwright  M.  and Yu  B. (2008). Model selection in gaussian graphical models:
High-dimensional consistency of l1-regularized mle. Advances in Neural Information Processing Systems
(NIPS)  21.

Tyler  D. E. (1987). A distribution-free m-estimator of multivariate scatter. The Annals of Statistics  15(1):234–

251.

Vershynin  R. (2010).

arXiv:1011.3027.

Introduction to the non-asymptotic analysis of random matrices.

arXiv preprint

Vu  V. Q. and Lei  J. (2012). Minimax rates of estimation for sparse pca in high dimensions. Journal of Machine

Learning Research (AIStats Track).

Yuan  X. and Zhang  T. (2013). Truncated power method for sparse eigenvalue problems. Journal of Machine

Learning Research  14:899–925.

Zou  H.  Hastie  T.  and Tibshirani  R. (2006). Sparse principal component analysis. Journal of computational

and graphical statistics  15(2):265–286.

9

,Fang Han
Han Liu
Robert Wang
Xiang Li
Charles Ling