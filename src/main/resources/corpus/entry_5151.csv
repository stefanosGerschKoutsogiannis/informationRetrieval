2019,Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering,State-of-the-art approaches to causal discovery usually assume a fixed underlying causal model. However  it is often the case that causal models vary across domains or subjects  due to possibly omitted factors that affect the quantitative causal effects. As a typical example  causal connectivity in the brain network has been reported to vary across individuals  with significant differences across groups of people  such as autistics and typical controls. In this paper  we develop a unified framework for causal discovery and mechanism-based group identification. In particular  we propose a specific and shared causal model (SSCM)  which takes into account the variabilities of causal relations across individuals/groups and leverages their commonalities to achieve statistically reliable estimation. The learned SSCM gives the specific causal knowledge for each individual as well as the general trend over the population. In addition  the estimated model directly provides the group information of each individual. Experimental results on synthetic and real-world data demonstrate the efficacy of the proposed method.,Speciﬁc and Shared Causal Relation Modeling and

Mechanism-Based Clustering

Biwei Huang 1 ∗  Kun Zhang1  Pengtao Xie2  Mingming Gong3  Eric Xing2 4  Clark Glymour1

1Department of Philosophy  Carnegie Mellon University  Pittsburgh  PA  USA.

3School of Mathematics and Statistics  University of Melbourne  Melbourne  Australia.
4Department of Machine Learning  Carnegie Mellon University  Pittsburgh  PA  USA.

2Petuum Inc.  USA.

Abstract

State-of-the-art approaches to causal discovery usually assume a ﬁxed underly-
ing causal model. However  it is often the case that causal models vary across
domains or subjects  due to possibly omitted factors that affect the quantitative
causal effects. As a typical example  causal connectivity in the brain network
has been reported to vary across individuals  with signiﬁcant differences across
groups of people  such as autistics and typical controls. In this paper  we develop
a uniﬁed framework for causal discovery and mechanism-based group identiﬁca-
tion. In particular  we propose a speciﬁc and shared causal model (SSCM)  which
takes into account the variabilities of causal relations across individuals/groups
and leverages their commonalities to achieve statistically reliable estimation. The
learned SSCM gives the speciﬁc causal knowledge for each individual as well as
the general trend over the population. In addition  the estimated model directly
provides the group information of each individual. Experimental results on syn-
thetic and real-world data demonstrate the efﬁcacy of the proposed method.

1

Introduction

Learning causal relations from observational data automatically  known as causal discovery  has
shown its increasing importance and efﬁcacy. State-of-the-art approaches to causal discovery usu-
ally assume a ﬁxed causal model [33  2  13  31  14  39  16]; that is  causal mechanisms are invariant
across instances in the data set. Under this assumption  causal relations can be identiﬁed by lever-
aging the conditional independence between observed variables [33] or the asymmetrical indepen-
dence between estimated noise term and hypothetical causes  implied by suitable functional causal
models [31  37  14  39].
In real-world scenarios  it is often the case that causal relations over the considered set of variables
may vary across individuals or individual groups  and meanwhile they also share many commonal-
ities. For example  in healthcare  individuals may show different responses to the same treatment.
The varying responses may be due to some (unmeasured) factors  such as nutrition and health status.
At the same time  although the effect may be different for different individuals  a large proportion
may still show a similar trend  while others may show very distinct effects. This suggests that to
understand causal effects  it is helpful to properly divide these subjects into different groups: within
groups  the variation of the treatment effect should be small  while it may be large across groups.
When examining whether a treatment is effective and should be adopted as standard practice  one
should not only care about its effect in the general population  but also account for the response to
the treatment of each individual or each properly divided group. The brain network is another ex-
ample. There is ample evidence that heterogeneity in brain processes exists across individuals [34].

∗Correspondence to: Biwei Huang  email: biweih@andrew.cmu.edu.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

More speciﬁcally  there exist signiﬁcant differences of brain information ﬂows in different groups
of people. For example  it has been shown that cases of autism are associated with atypical brain
connectivities [5  17]  and that the differences provide a good criterion for autism diagnosis and help
to localize neuropathology biomarkers.
To ﬁnd the difference across individuals  a typical solution is to analyze data from each subject
separately and then make a comparison. However  this approach may suffer from low statistical
reliability  because the size of samples from one subject may be small while the data dimension
may be high. For example  in healthcare data  each patient may have only a few records due to
resource and time constraints  while many clinical variables are measured. Fortunately  although
individuals may not have the same causal model  they usually share many commonalities  which
can be leveraged to achieve more reliable estimation results. This reasoning is motivated by multi-
task learning [1]  where multiple learning tasks are solved jointly in a principled way  thus exploiting
their commonalities while at the same time preserving useful information for each individual task.
On the other hand  if we ignore the differences and concatenate the data to estimate a causal graph 
spurious edge or incorrect causal directions may be introduced [38].
Recently  some approaches have been developed for causal discovery in the case where causal rela-
tions over the observed variables change across domains. For example  causal discovery from het-
erogeneous/nonstationary data (CD-NOD) [38  19] concatenates data from different domains and
considers the domain index as a surrogate to characterize the variability of causal mechanisms  and
ﬁnally recovers ﬁxed as well as varying causal relations. In the linear case  invariant causal rela-
tions are found based on invariant predictions [25]  and some other methods can directly estimate
varying causal relations [18  20  11  36  15]. Despite their success on the considered problem  these
approaches do not explicitly provide the group-level information regarding which ones are similar
to each other and can be grouped together. In addition  they do not allow opposite causal directions
in different domains. The Group Iterative Multiple Model Estimation (GIMME) approach [22  9]
tries to recover causal structure at both group and individual levels in a heuristic way. It ﬁrst learns a
group model by selecting adjacencies which will improve the majority of individuals’ maps in an it-
erative forward selection procedure  and then selects individual-level adjacencies that will optimally
improve that model. As a heuristic method  GIMME does not have theoretical guarantees of the
identiﬁability of the learned causal structure.
Motivated by the above real-world scenarios in healthcare and neuroscience  we propose a Speciﬁc
and Shared Causal Model (SSCM)  to achieve the following goals: (1) Discover a general trend
of causal relations over the population. (2) Identify speciﬁc causal relations for each individual or
each automatically determined group. (3) Exploit variations and commonalities of causal relations
to cluster individuals into different groups. In particular  for each individual  the causal model is
formalized with a linear non-Gaussian model. The learned speciﬁc and shared causal model gives
the information of the speciﬁc causal knowledge for each individual  as well as the general trend
over the population. Each individual can be grouped by directly using the learned causal model.
Moreover  the proposed causal model is theoretically identiﬁable under mild conditions.

2 Speciﬁc and Shared Causal Models
Suppose there are n individuals  which can be divided into q groups; we do not know which group
each individual belongs to. All individuals have the same m observed variables under investigation 
but their causal relations may be different. For the s-th individual (s = 1 ···   n)  we observe ls data
points for the m variables; the ls data points can be either independent and identically distributed
(i.i.d.) or from a stationary time series. Consider the brain connectivity problem. We have n
subjects  which are expected to be from q groups; for the s-th subject we record ls fMRI data points
over m variables. We aim to learn a shared causal model over the m variables  which is shared
across the population  and also a speciﬁc causal model for each individual. Moreover  we cluster
these n individuals into q groups by leveraging the learned causal model.
Suppose the m observed variables from the s-th individual  X s(t) = (xs
the following generating process

1(t) ···   xs

m(t))T  satisfy

xs
i (t) =

bs
ijxs

j(t)

+

+es

i (t) 

(1)

(cid:88)
(cid:124)

j∈P s

i

(cid:123)(cid:122)

(cid:88)

j∈Ls

i

pl(cid:88)
(cid:124)

p=1

(cid:125)

as
ij pxs

(cid:123)(cid:122)

j(t − p)
(cid:125)

instantaneous

time-lagged

2

pl(cid:88)

ij p p-lagged causal inﬂuences  P s
i the set of indices of lagged direct causes of xs
ij and as

for i = 1 ···   m  where bs
ij represents instantaneous causal inﬂuences from variable j to i in the
s-th subject  as
i the set of indices of instantaneous direct causes
i   and Ls
i . Each individual has ﬁxed causal
of xs
coefﬁcients bs
i (t) is non-
j(t − p)  for all
Gaussian  representing some unmeasured factors. It is independent of xs
j  p ∈ N +. Note that for i.i.d. samples  we only consider instantaneous causal relations  while for
stationary time series  we allow both instantaneous and time-lagged causal relations. Eq. (1) can be
represented in the matrix form:

ij p  while they may change across individuals. The noise term es

j(t) and xs

p=1

ij  As

1(t) ···   es

X s(t) = BsX s(t) +

ij p  and Es(t) = (es

(2)
p the m×m lagged

pX s(t − p) + Es(t) 
As
where Bs is the m×m instantaneous causal adjacency matrix with entries bs
m(t))T.
causal adjacency matrix with entries as
We allow that both instantaneous causal relations bs
ij p change across groups
ij and lagged relations as
or individuals. More speciﬁcally  they vary across different groups  while there are also slight dif-
ferences across individuals within the group. Intuitively  one may estimate corresponding causal
relationships for each individual separately. However  the limited sample size from each individual
limits statistical efﬁciency or even makes causal discovery impossible  especially when the data di-
mension is high and the causal graph is dense. Although individuals may not have the same causal
model  they usually share many commonalities  which can be leveraged to achieve more reliable
estimation results.
To exploit both variations and commonalities across groups  as well
as across individuals  and meanwhile perform mechanism-based clus-
tering in a principled way  we propose speciﬁc and shared causal re-
lation modeling. Speciﬁcally  we take the instantaneous causal inﬂu-
ence as a random variable bij  where bs
ij can be seen as an instance of
bij. To encode the variation across groups  as well as that within each
group  we assume that in each group  bij follows a Gaussian distribu-
tion  while in different groups the Gaussian distributions are different.
Therefore  we impose a mixture of Gaussians (MoG) prior on bij. Let
a q-dimensional binary vector Z denote the index of the Gaussian com-
ponent in the MoG for bij (i.e.  which group the individual belongs to)
where a particular element zk = 1 and all other elements are zero.
Then the distribution of bij can be represented as

ZE

b12

x1

e1

e2

x2

Z

P (bij) =

P (zk = 1)P (bij|µk ij  σk ij) 

where P (bij|µk ij  σk ij) = N (bij|µk ij  σ2

Gaussian distribution  P (zk = 1) = πk  and(cid:80)q

k=1

for lagged causal inﬂuences  we have

(3)
k ij) and N (·) denotes a
k=1 πk = 1. Similarly 

P (aij p) =

P (zk = 1)P (aij p|νk ij p  ωk ij p) 

(4)

q(cid:88)

q(cid:88)

k=1

Figure 1: Graphical repre-
sentation of a two-variable
case: x1 and x2 are two
observed variables  b12 is
the instantaneous causal
strength from x1 to x2  e1
and e2 denote the noise
terms w.r.t x1 and x2  re-
spectively  Z is the group
indicator  and ZE is the in-
dicator of the MoG of e1
and e2.

where P (aij p|νk ij p  ωk ij p) = N (aij p|νk ij p  ω2
bij  for all i  j  p) in the same group share the same P (Z).
We also allow the noise distribution varies across groups but remains the same within the group.
More speciﬁcally  we model the non-Gaussian noise in each group with an MoG  and in different
groups  the noise may have different MoG distributions. Denote by Z E the indicator of the MoG of
E  with Z E = (zE

k ij p). Different causal coefﬁcients (aij p and

q(cid:48) )  and thus in group k with zk = 1  the distribution of E is:

1  ···   zE
P (E|zk = 1) =

q(cid:48)(cid:88)
k(cid:48)=1 πk k(cid:48) = 1. Thus  P (E) =(cid:80)q
(cid:80)q(cid:48)

where P (E|zE

k(cid:48)=1

k(cid:48) = 1  zk = 1) = N (E|(cid:126)µE

P (zE

k(cid:48) = 1|zk = 1)P (E|zE

k k(cid:48)  ΣE

k k(cid:48))  P (zE
k=1 P (E|zk = 1)P (zk = 1).

k(cid:48) = 1  zk = 1) 
k(cid:48) = 1|zk = 1) = πE

k k(cid:48)  and

(5)

3

Figure 1 shows the graphical representation of the entire model in a two-variable case  with only
instantaneous causal relations. Because in our model we consider bij as a random variable  there is
a causal edge from b12 to x2. Therefore  the speciﬁc and shared causal model is represented as

X(t) = BX(t) +

ApX(t − p) + E(t) 

pl(cid:88)

p=1

(6)

(7)

with

P (bij) =(cid:80)q
P (aij p) =(cid:80)q
p(E) =(cid:80)q

k=1 πkN (bij|µk ij  σ2
k ij) 
(cid:80)q(cid:48)
k=1 πkN (aij p|νk ij p  ω2
k k(cid:48)N (E|(cid:126)µE

k(cid:48)=1 πE

k=1 πk

k ij p) 
k k(cid:48)  ΣE

k k(cid:48)).

In the next section  we will discuss its identiﬁability; the identiﬁability applies to both the causal
structure and model parameters.

3 Model Identiﬁability

Theorem 1 shows the identiﬁability in a speciﬁc case  where there are different groups  and causal
relations are different across groups but identical within each group.
Theorem 1. The proposed causal model in (6) and (7)  including the causal structure and model
parameters  is identiﬁable  as n → ∞  under the following conditions:

1. The parameters σk ij = 0 and ωk ij p = 0  for all i  j  k  p ∈ N +.
2. The sample size of each individual ls > 2q − 1  where q is the number of groups.
3. The instantaneous causal structure for each individual is acyclic.

Note that although the instantaneous causal structure for each individual is acyclic  we allow that
across different groups  some causal directions are reversed. For instance  in the brain network 
different directions may be activated across subjects or states  which is hard to handle with traditional
methods. In addition  if there are cycles in the instantaneous causal structure  the identiﬁability
requires two more conditions [23]: (1) the cycles are disjoint  and (2) the causal model is stable  i.e. 
k→∞ Bk = 0. As an unsupervised method  the order of the group index is not identiﬁable; i.e.  it can
lim
be arbitrarily permuted. We are aware that in the above result it is assumed that there is no variation
within groups. For the general case  the proof of the identiﬁability results does not seem immediate 
but our empirical results suggest that the causal model is also identiﬁable. In the following  we give
a proof sketch of Theorem 1; for detailed proofs  please refer to the supplementary material.

Proof Sketch. Condition 1 means that bij and aij p take a degenerate Gaussian distribution in each
group; their distributions can be represented as follows:

P (bij) =(cid:80)q

k=1 πkδµk ij (bij)  P (aij p) =(cid:80)q

k=1 πkδνk ij p(aij p) 

where δµk ij (bij) = 1  if bij = µk ij  and 0 if otherwise; δνk ij p(aij p) = 1  if aij = νk ij p 
and 0 if otherwise. With condition 1  the identiﬁability of the proposed causal model can be seen
from the view of ﬁnite mixture models with grouped samples. The “grouped samples” means that
for each individual there are several samples  and it is known in advance that they are identically
distributed samples from the same component. Note that the identiﬁability of ﬁnite mixture models
with grouped samples is easier to achieve (see [35])  compared to the case where the observations
are drawn i.i.d. from a mixture model. Imagine an extreme case: if there are enough samples for
each individual  then the corresponding components can be identiﬁed from each individual directly.
We ﬁrst show that under condition 1 and 2  where ls > 2q − 1 [35]  the cumulative distribu-
tion function of each mixture component  as well as the mixture proportion  is identiﬁable; that is 
P (X|zk = 1) is identiﬁable  for k = 1 ···   q  and P (Z) is identiﬁable. Next  thanks to the identi-
ﬁability of independent component analysis-based models [7  31]  we can show that in each group 
instantaneous causal relations bij and lagged causal relations aij p are identiﬁable [21].

4

4 Model Identiﬁcation

The speciﬁc and shared causal model deﬁned above can be regarded as a latent variable

model  with U = (cid:8){bij}m
(cid:8){πk} {µk ij} {σk ij} {νk ij p} {ωk ij p} {πE

i j=1 {aij p}(cid:9) as latent variables that we are interested in  and θ =
k k(cid:48)}(cid:9) as free parameters that need

k k(cid:48)} {ΣE

to be estimated.
In particular  we exploit a stochastic approximation expectation maximization
(SAEM) algorithm [4]  combined with Gibbs sampling in the E step and EM algorithm in the M
step  for model estimation.

k k(cid:48)} {(cid:126)µE

4.1 Parameter Estimation with SAEM
For a traditional EM algorithm  the procedure is initialized at some θ0 ∈ Θ and then iterates between
two steps  expectation (E) and maximization (M):

(E) Compute Pθr−1(U|X) and the lower bound of the log-likelihood  Q(θ  θr−1)  with

(cid:90)

Q(θ  θr−1) =

Pθr−1(U|X) log Pθ(X  U) dU.

(M) Compute θr = arg maxθ∈Θ Q(θ  θr−1).

M(cid:88)

j=1

In the E step  we need to compute the expectation under the posterior Pθr−1(U|X)  which is in-
tractable in our case  since P (X  U) is not Gaussian. To address this issue  SAEM computes the E
step by Monte Carlo integration and uses a stochastic approximation update of the quantity Q at the
r-th iteration:

˜Qr(θ) = (1 − λr) ˜Qr−1(θ) + λr

1
M

log Pθ(X1:n  ˚U (1:n r j)) 

(8)

s=1

r λ2

r λr = ∞ and(cid:80)

of positive step size  with(cid:80)

where ˚U indicates sampled particles of U  M is the generated number of particles  X1:n = {Xs}n
s=1  and {λr}r≥1 is a decreasing sequence
and Xs = (X s(1) ···   X s(ls))  ˚U (1:n r j) = {˚U (s r j)}n
r < ∞. More speciﬁcally  given the learned
parameters in the current iteration  the values of latent variables are ﬁrst sampled under the posteriori
density. Then these sampled data are used to update the value of the conditional expectation of the
complete log-likelihood with stochastic approximation. The E-step is thus replaced by the following:
(E(cid:48)) At each iteration  generate M particles of ˚U (1:n r j) from Pθr−1 (U|X) and compute ˜Qr(θ)
according to (8). A method for sampling from Pθr−1 (U|X) is introduced in the following.
Gibbs Sampling in E-step Since the dimension of latent variables U may be high  especially
when m is large  we use Gibbs sampling to sample particles ˚U from the posterior distribution 
and within Gibbs sampling  we use independent doubly adaptive rejection metropolis sampling
(IA2RMS) [24].
The idea in Gibbs sampling is to generate posterior samples by sweeping through each variable to
sample from its conditional distribution with the remaining variables ﬁxed to their current values.
At each iteration  perform

bij ∼ P (bij|X1:n  U\bij) 

aij p ∼ P (aij p|X1:n  U\aij p) 

(9)
for all i  j  p ∈ N+  where U\bij and U\aij p denote all variables in U except bij and aij p  respec-
tively. In each sampling  we use IA2RMS. It differs from adaptive rejection metropolis sampling 
with an additional adaptive step to improve the proposal probability density function.
EM Algorithm in M-step In the M step  we compute θr = arg maxθ∈Θ Q(θ  θr−1). It is achieved
by an inner EM algorithm. See supplementary materials for detailed derivations.
The computational complexity of SAEM in each iteration is O(m2nM T0)  where m is the number
of variables  n the number of subjects  M the number of sampled particles (we used M = 30) 
and T0 the number of iterations needed in the Gibbs sampling for each variable  depending on the
number of rejection times and the number of supporting points that need to be calculated in the
adaptive rejection sampling.

5

4.2 Speciﬁc and Shared Causal Relation Determination
After estimating the parameters  we can derive the posterior distribution of {Ap}pl
i j p P (aij p)P (bij) 

P ({Ap}  B|X1:n) ∝ P (X1:n|{Ap}  B)(cid:81)
s=1 ls · PE((I − B) ˇX0 −(cid:80)
k ij)  P (aij p) =(cid:80)q

P (X1:n|{Ap}  B) = | det(I − B)|(cid:80)n
P (bij) =(cid:80)q
p Ap ˇXp) =(cid:80)q
PE((I − B) ˇX0 −(cid:80)
k=1 πkN (bij|µk ij  σ2

k k(cid:48)N(cid:0)(I − B) ˇX0 −(cid:80)

k=1 πkN (aij p|νk ij p  ωk ij p) 
p Ap ˇXp|µE

(cid:80)q(cid:48)

p Ap ˇXp) 

k=1 πk

where

ˇX0 = (X1

p+1:l1

 ···   Xn

p+1:ln

) 

and ˇXp = (X1

k(cid:48)=1 πE
1:l1−p ···   Xn

1:ln−p).

p=1 and B  with

(10)

k k(cid:48)(cid:1) 

k k(cid:48)  ΣE

Then the estimated speciﬁc causal relationships are implied by the posterior distribution of Ap and
B given the data from the s-th individual. More speciﬁcally  one may take the maximum a posterior
(MAP) as a point estimator of Ap and B:

{{ ˆAs

p}  ˆBs} = arg max
{Ap} B

P ({Ap}  B|Xs).

(11)

The estimated shared causal relationships are implied by the posterior distribution of Ap and B
given the data from all individuals  and its point estimator is

{{ ˆAp}  ˆB} = arg max
{Ap} B

P ({Ap}  B|X1:n).

(12)

Recall that the linear non-Gaussian acyclic model (LiNGAM [31]) ﬁrst estimates W = (I − B)−1
and then recovers the underlying adjacency matrix B by performing extra permutation and rescal-
ing  since W is only identiﬁed up to permutation and scale. In our model  we directly model the
causal process B  with the following advantages: (1) It is easy to add prior knowledge of causal
connections. In practice  experts may have domain knowledge about some causal edges. (2) One
can directly enforce sparsity constraints on causal adjacencies. (3) The estimation procedure directly
outputs the causal adjacency matrix  without additional steps of permutation and rescaling  which
are usually expensive.

5 Mechanism-based Clustering with Speciﬁc and Shared Causal Model

After estimating the speciﬁc and shared causal model  we can immediately cluster individuals into
q groups  by estimating P (zk = 1|Xs)  for k = 1 ···   q  where

P (zk = 1|Xs) ∝ P (Xs|zk = 1)P (zk = 1) 

(13)

(14)

and

P (Xs|zk = 1) =

(cid:90) (cid:90)

P (Xs|{Ap}  B  zk = 1)P ({Ap}  B|zk = 1) d{Ap} dB.

The above integration does not have a closed form  and thus we use Monte Carlo integration. We
sample M values of {Ap} and B from P ({Ap}  B|zk = 1)  and thus

= 1
M

where A(i)

P (Xs|zk = 1)

(cid:80)M
i=1 | det(I − B(i))|ls ·(cid:80)q(cid:48)
1:ls−p; (cid:126)µE
p and B(i) denote the sampled i-th value from P ({Ap}  B|zk = 1). Therefore 

k(cid:48)=1 πk k(cid:48)N(cid:0)(I − B(i))Xs
πk k(cid:48)N(cid:0)(I−B(i))Xs

p+1:ls −(cid:80)
p+1:ls −(cid:88)

| det(I−B(i))|ls · q(cid:48)(cid:88)

1:ls−p|µE

M(cid:88)

p ApXs

ApXs

P (zk = 1|Xs) ∝ πk
M

i=1

k k(cid:48)   ΣE

k k(cid:48)(cid:1) 
k k(cid:48)(cid:1).

k k(cid:48)   ΣE

p

Denote by cs the group that individual s belongs to. The estimated group index for individual s is
ﬁnally given by:

ˆcs = arg max

P (zk = 1|Xs).

(15)

k(cid:48)=1

k

6

6 Experimental Results

To show the efﬁcacy of the proposed approach to speciﬁc and shared causal relation discovery and
its performance in mechanism-based clustering  we apply it to both synthetic and real-world data.

k ij ∼ U(0.01  0.1)  ω2

k=1 πk = 1  (cid:80)q(cid:48)

k(cid:48)=1 πE

k k(cid:48) ∼ U(0.2  0.5)  πk ∼ U(0.3  0.6)  πE

U(0.3  0.6)  and(cid:80)q

Other parameters were set as follows: σ2
k k(cid:48) ∼ U(−0.6 −0.4)∪U(0.4  0.6)  each entry of ΣE
(cid:126)µE

Synthetic Data We randomly generated acyclic causal structures according to the Erdos-Renyi
model [6] with parameter 0.3. We denote by G the graph structure. Each generated graph has 5
variables. To show the generality of the proposed method  we varied the number of groups with
q = 2  3  the number of samples for each individual with ls = 20  40  60  and the number of
individuals with n = 60  80  100. Motivated from the real-world scenario that brain connectivities
may be enhanced or inhibited in individuals with mental disorders  such as autism and schizophrenia 
compared to typical controls  the parameters were set in the following way:
• In the 2-group case (q = 2)  when the group index k = 1 (e.g.  typical control group)  we set
µk ij ∼ U(0.8  1) for all i  j where Gij = 1; when k = 2 (e.g.  autism group)  we randomly
sampled pairs of i(cid:48)  j(cid:48) where Gi(cid:48)j(cid:48) = 1  and set µk i(cid:48)j(cid:48) ∼ U(0  0.2) to model the situation that
some causal edges are inhibited  and for the remaining i  j where Gij = 1  µk ij ∼ U(0.8  1).
• In the 3-group case (q = 3)  when k = 1 or k = 2  µk ij was the same as above; when k = 3
(e.g.  schizophrenia group)  we randomly sampled pairs of i(cid:48)  j(cid:48) where Gi(cid:48)j(cid:48) = 1  and set µ1 i(cid:48)j(cid:48) ∼
U(1.8  2) to model the situation that some causal edges are enhanced.
k ij p ∼ U(0.01  0.1)  each entry of
k k(cid:48) ∼
k k(cid:48) = 1  where U(l  u) denotes a uniform distribution
between l and u. For each setting (a particular group size q  a particular sample size for each
individual ls  and the number of individuals n)  we generated 30 realizations.
For causal discovery  we identiﬁed speciﬁc causal relations by the proposed approach. We compared
it with other well-known approaches in causal discovery  including LiNGAM [31]  the minimal
change method (MC) [11]  the identical boundaries method (IB) [11]  and GIMME [22  9].
In
particular  we applied LiNGAM on each individual separately because it assumes a ﬁxed causal
model. Both MC and IB leverage the minimal change principle to identify the causal structure.
GIMME is a heuristic method  which is designed to learn both speciﬁc and averaged causal relations.
Since the state-of-the-art baselines  such as LiNGAM  MC  and IB  only consider instantaneous
causal relations  we report the identiﬁcation results of instantaneous causal relations. For time-
lagged causal relations  the causal direction is ﬁxed (from past to future)  and thus  it reduces to a
parameter identiﬁcation problem.
In our method  we initialized the parameters in the following way: we ﬁrst estimated the correlation
matrix for each individual and clustered the estimated correlation matrices with K-means clustering 
and then we used the estimated centroids of each group as the initial value of µk ij. Other parameters
were initialized randomly. In our experiments  the number of groups was given. If there is a large
number of groups  one may use some information criteria  such as the Minimal Message Length
[8] to determine it. We denote by ˆGs the estimated causal graph for the s-th individual. It was
determined as follows: ˆGs
ij = 0 if otherwise. Alternatively  one may use
Wald test to examine signiﬁcance of edges  as in [31]. The simulation was conducted on a 2.9GHZ
Intel Core i5  and each trial costs about 5 minutes.
In Figure 2(Upper)  we reported the F1 score to measure the accuracy of learned causal graphs.
Speciﬁcally  sub-ﬁgure (a) shows the F1 score (y-axis) for the number of groups q = 2  the sample
size of each individual ls = 20  and the number of individuals n = 60  80  100 (x-axis)  (b) for
q = 2  n = 100  and ls = 20  40  60 (x-axis)  (c) for q = 3  ls = 20  and n = 60  80  100 (x-axis) 
and (d) for q = 3  n = 100  and ls = 20  40  60 (x-axis). We can see that our proposed method
SSCM has the best performance (the highest F1) in all cases  and the accuracy slightly increases
along with the number of individuals or the sample size per individual. MC  IB  and LiNGAM show
similar performance and are less accurate than SSCM. MC and IB perform less well because they
only take into account the ﬁrst two orders of noise distributions. The performance of LiNGAM may
be affected by the small sample size. GIMME does not perform well  possibly because it uses a
greedy  heuristic strategy without theoretical guarantees.

ij = 1 if |ˆbs

ij| > 0.1  and ˆGs

7

Figure 2: (Upper) F1 score of the recovered causal structure; (Lower) L2 distance of the estimated
causal strength.

Besides the causal structure  which only takes into account the existence of an edge  we also com-
pared the accuracy of the estimated causal strength quantitatively. It is important to compare the
estimated causal strength  because in different groups the causal strength may be enhanced or inhib-
ited while the causal structure remains the same. In particular  we compared the L2 distance between
the true causal strength and the estimation for each individual  i.e.  (cid:107)Bs − ˆBs(cid:107)2. Figure 2(Lower)
reports the estimated L2 distance with the proposed method in different settings  compared to that
with LiNGAM  IB  MC  and GIMME. Our SSCM gives the most accurate estimation of the causal
strength (the smallest L2 distance). MC and IB have the second-best accuracy  while LiNGAM and
GIMME perform less well. LiNGAM fails to estimate the quantitative causal strength accurately 
although the estimated qualitative causal graph has a similar accuracy with MC and IB.

Figure 3: Adjusted Rand Index.

Next  we performed mechanism-based clustering by directly leveraging the estimated speciﬁc and
shared causal model. Figure 3 gives the clustering performance in different settings  measured by
Adjusted Rand Index (ARI [27]). It measures the similarity between the estimated groups and the
ground truth (the higher  the more accurate). We compared our method with GIMME  LiNGAM-K-
Means  MC-K-Means  and IB-K-Means. In particular  GIMME originally estimates the group index
for each individual. LiNGAM-K-Means  MC-K-Means  and IB-K-Means use K-means to cluster
the causal relations estimated by LiNGAM  MC  and IB  respectively. We also performed paired 
one-sided Wilcoxon signed rank test on the estimated ARI between our method and each of the
remaining ones [12]  across different settings. Our method signiﬁcantly outperforms LiNGAM and
GIMME  with p-values less than 0.002  and achieves performance that is comparable to MC and IB.

8

6080100# of subjects (a) 0.20.40.60.81.0F1 score2 groups204060# of samples (b) 2 groups6080100# of subjects (c) 3 groups204060# of samples (d) 3 groupsSSCMLiNGAMIBMCGIMME6080100# of subjects (a) 0.81.21.62.02.4L2 distance2 groups204060# of samples (b) 2 groups6080100# of subjects (c) 3 groups204060# of samples (d) 3 groups6080100# of subjects (a) 0.20.61ARI2 groupsSSCMLiNGAMIBMCGIMME204060# of samples (b) 2 groups6080100# of subjects (c) 3 groups204060# of samples (d) 3 groupsfMRI Hippocampus We applied our methods to the fMRI hippocampus data [26]  which contains
signals from six separate brain regions: perirhinal cortex (PRC)  parahippocampal cortex (PHC) 
entorhinal cortex (EC)  subiculum (Sub)  CA1  and CA3/Dentate Gyrus (CA3) in resting states on
the same person on 84 successive days. We used anatomical connections [3  38] as a reference. Some
biological evidence has shown that in resting-states  the effective pathways in the hippocampus may
change  depending on unmeasured intrinsic states [10]. We assume that the causal relations are ﬁxed
on the same day  but may change across different days. With the proposed method  we found that
the causal relations between these six regions can be divided into two groups: in one group  the edge
Sub → EC is inhibited; in the other group  EC → CA3 and CA1 → Sub are inhibited. This result is
consistent with the ﬁnding that EC → CA3 and CA1 → Sub are usually involved in consolidation
of a long-term memory [28]  while Sub → EC is usually implicated in working memory [29]. The
edge CA3 → CA1 is robust  existing in both groups  which coincides with the current ﬁndings in
neuroscience [32].

Methods
ARI

MC GIMME Plain K-Means
0.25

0.10

0.87

Table 1: Clustering performance on ﬂow cytometry data

SSCM LiNGAM IB
0.92

0.21

0.78

Cellular Signaling Networks We applied the proposed method to multivariate ﬂow cytometry
data  which were measured from 11 phosphorylated proteins and phospholipids [30]. A series of
stimulatory cues and inhibitory interventions were performed  leading to different conditions. With
different interventions in different conditions  the causal relations over the 11 variables may change
across them. The data from each condition mimic a group  and in each condition  we segmented
the data into subsets  with 30 samples in each subset  mimicking an individual. Table 1 reports
the clustering performance  measured by ARI  on the data from condition phorbol myristate
acetate and condition anti-CD3 + anti-CD28 + LY294002. Besides those comparisons in the
simulation  we also compared the clustering performance with plain K-means  that is  directly ap-
plying K-means to the original data. Our method achieves the best performance  with ARI 0.92.
Compared to the former condition  the causal strength of the following edges in the latter condition
are inhibited: PIP2 → PIP3  Erk → Pka  Jnk → Pkc  and the following edges are enhanced: Raf
→ Mek  Mek → Raf  Akt → Pka  Pkc → P38. For the estimated cellular signaling networks under
each condition  please see the supplementary material.

7 Conclusions and Future Work

In this paper  we proposed a uniﬁed framework for causal relations discovery and mechanism-based
clustering. In particular  we developed a speciﬁc and shared causal model  which takes into ac-
count the variabilities of causal relations across individuals/groups and leverages commonalities to
achieve statistically reliable estimation. Experimental results on synthetic and real-world data show
that the learned SSCM gives the speciﬁc causal knowledge for each individual as well as the gen-
eral trend over the population  and the estimated model directly provides the group information of
each individual. Our current implementation relies on maximum likelihood estimation with SAEM 
which does not generally scale well: currently we can handle 10 variables with 200 subjects within
1 hour. For the purpose of improving scalability  one line of our future work is to use likelihood-free
frameworks for parameter estimation with  e.g.  adversarial learning. Moreover  we will extend our
methods to cover nonlinear causal relationships  to partially observable processes  and to data with
selection bias [40].

Acknowledgements

We thank Petar Stojanov for helping to revise the paper. We would like to acknowledge the support
by National Institutes of Health under Contract No. NIH-1R01EB022858-01  FAIN-R01EB022858 
NIH-1R01LM012087  NIH-5U54HG008540-02  and FAIN-U54HG008540  by the United States
Air Force under Contract No. FA8650-17-C-7715  and by National Science Foundation EAGER
Grant No. IIS-1829681. The National Institutes of Health  the U.S. Air Force  and the National
Science Foundation are not responsible for the views reported in this article. KZ also beneﬁted from
funding from Living Analytics Research Center and Singapore Management University.

9

References
[1] R. Caruana. Multi-task learning. Machine Learning  28:41–75  1997.

[2] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal of Machine

Learning Research  3:507–554  2003.

[3] M. B. Chris and B. Neil. The hippocampus and memory: insights from spatial processing.

Nature Reviews Neuroscience  9(3):182–194  2008.

[4] B. Delyon  M. Lavielle  and E. Moulines. Convergence of a stochastic approximation version

of the EM algorithm. In The Annals of Statistics  volume 27(1)  pages 94–128  1999.

[5] S. J. H. Ebisch  V. Gallese  R. M. Willems  D. Mantini  W. B. Groen  G. L. Romani  and
et al. Altered intrinsic functional connectivity of anterior and posterior insula regions in high-
functioning participants with autism spectrum disorder. Hum. Brain Mapp  32(7):1013–1028 
2011.

[6] P. Erd˝os and A. R´enyi. On random graphs I. Publicationes Mathematicae  6:290–297  1959.

[7] J. Eriksson and V. Koivunen. Identiﬁability  separability  and uniqueness of linear ICA models.

IEEE Signal Processing Letters  11(7):601–604  2004.

[8] M. A. T. Figueiredo and A. K. Jain. Unsupervised learning on ﬁnite mixture models. IEEE

transactions of pattern analysis and machine intelligence  24(3):381–396  2002.

[9] K. M. Gates  P. C. Molenaar  F. G. Hillary  N. Ram  and M. J. Rovine. Automatic search for
fMRI connectivity mapping: An alternative to Granger causality testing using formal equiv-
alences among SEM path modeling  VAR  and uniﬁed SEM. NeuroImage  50:1118–1125 
2010.

[10] M. S. Gazzaniga  R. B. Ivry  and G. R. Mangun. Cognitive Neuroscience: The Biology of the

Mind. W. W. Norton Company  New York  2014.

[11] A. E. Ghassami  N. Kiyavash  B. Huang  and K. Zhang. Multi-domain causal structure learning

in linear systems. In Advances in neural information processing systems (NeurIPS)  2018.

[12] J. D. Gibbons and S. Chakraborti. Chapman Hall/CRC Press  2011.

[13] D. Heckerman  D. Geiger  and D. M. Chickering. Learning bayesian networks: The combina-

tion of knowledge and statistical data. Machine Learning  20:197–243  1995.

[14] P. O. Hoyer  D. Janzing  J. Mooji  J. Peters  and B. Sch¨olkopf. Nonlinear causal discovery with
additive noise models. In Advances in Neural Information Processing Systems 21  Vancouver 
B.C.  Canada  2009.

[15] B. Huang  K. Zhang  M. Gong  and C. Glymour. Causal discovery and forecasting in non-
In International Conference on Machine

stationary environments with state-space models.
Learning (ICML)  2019.

[16] B. Huang  K. Zhang  Y. Lin  B. Sch¨olkopf  and C. Glymour. Generalized score functions
for causal discovery. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery Data Mining (KDD)  pages 1551–1560  2018.

[17] B. Huang  K. Zhang  R. Sanchez-Romero  J. Ramsey  M. Glymour  and C. Glymour. Diagnosis
of autism spectrum disorder by causal inﬂuence strength learned from resting-state fMRI data.
In arXiv preprint arXiv:1902.10073  2019.

[18] B. Huang  K. Zhang  and B. Sch¨olkopf. Identiﬁcation of time-dependent causal model: A gaus-
sian process treatment. In the 24th International Joint Conference on Artiﬁcial Intelligence 
pages 3561–3568  2015.

[19] B. Huang  K. Zhang  J. Zhang  J. Ramsey  R. Sanchez-Romero  C. Glymour  and B. Sch¨olkopf.
Causal discovery from heterogeneous/nonstationary data. In arXiv preprint arXiv:1902.10073 
2019.

10

[20] B. Huang  K. Zhang  J. Zhang  Sanchez-Romero  C. R.  Glymour  and B. Sch¨olkopf. Behind
distribution shift: Mining driving forces of changes and causal arrows. In IEEE International
Conference on Data Mining (ICDM)  pages 913–918  2017.

[21] A. Hyv¨arinen  K. Zhang  S. Shimizu  and P. Hoyer. Estimation of a structural vector autoregres-
sion model using non-gaussianity. Journal of Machine Learning Research  pages 1709–1731 
2010.

[22] J. Kim  W. Zhu  L. Chang  P. Bentler  and T. Ernst. Uniﬁed structural equation modeling
approach for the analysis of multisubject  multivariate functional mri data. Human Brain Map-
ping  28:85–93  2007.

[23] G. Lacerda  P. Spirtes  J. Ramsey  and P. O. Hoyer. Discovering cyclic causal models by
independent components analysis. In Proceedings of the 24th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI)  Helsinki  Finland  2008.

[24] L. Martino  J. Read  and D. Luengo. Independent doubly adaptive rejection metropolis sam-
IEEE Transactions on Signal Processing  63(12):3123–3138 

pling within gibbs sampling.
2015.

[25] J. Peters  P. B¨uhlmann  and N. Meinshausen. Causal inference using invariant prediction:
identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society: Series B 
2016.

[26] Poldrack and Laumann. https://openfmri.org/dataset/ds000031/  2015.

[27] W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the Amer-

ican Statistical Association  66(336):846–850  1971.

[28] M. Remondes and E M. Schuman. Role for a cortical input to hippocampal area ca1 in the

consolidation of a long-term memory. In Nature  volume 431(7009)  pages 699–703  2004.

[29] C. Riegert  R. Galani  S. Heilig  C. Lazarus  B. Cosquer  and J. C. Cassel. Electrolytic lesions
of the ventral subiculum weakly alter spatial memory but potentiate amphetamine-induced
locomotion. In Behav. Brain Res.  volume 152(1)  pages 23–34  2004.

[30] K. Sachs  O. Perez  D. Pe’er  D. A. Lauffenburger  and G. P. Nolan. Causal protein signaling
networks derived from multiparameter single-cell data. In Science  volume 308  pages 523–
529  2005.

[31] S. Shimizu  P.O. Hoyer  A. Hyv¨arinen  and A.J. Kerminen. A linear non-Gaussian acyclic

model for causal discovery. Journal of Machine Learning Research  7:2003–2030  2006.

[32] D. Song  M. C. Hsiao  I. Opris  R. E. Hampson  V. Z. Marmarelis  G. A. Gerhardt  S. A. Dead-
wyler  and TW. Berger. Hippocampal microcircuits  functional connectivity  and prostheses.
Recent Advances On the Modular Organization of the Cortex  pages 385–405  2015.

[33] P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. Spring-Verlag

Lectures in Statistics  1993.

[34] L. Q. Uddin  K. Supekar  and V. Menon. Typical and atypical development of functional human
brain networks: insights from resting-state fMRI. In Frontiers in systems neuroscience  volume
4(21)  2010.

[35] R. A. Vandermeulen and C. D. Scott. On the identiﬁability of mixture models from grouped

samples. In arXiv preprint arXiv:1502.06644  2015.

[36] Y. Wang  C. Squires  A. Belyaeva  and C. Uhler. Direct estimation of differences in causal

graphs. In NIPS 2018.

[37] K. Zhang and L. Chan. Extensions of ICA for causality discovery in the hong kong stock
market. In Proc. 13th International Conference on Neural Information Processing (ICONIP
2006)  2006.

11

[38] K. Zhang  B. Huang  J. Zhang  C. Glymour  and B. Sch¨olkopf. Causal discovery from non-
stationary/heterogeneous data: Skeleton estimation and orientation determination. In IJCAI 
2017.

[39] K. Zhang and A. Hyv¨arinen. On the identiﬁability of the post-nonlinear causal model.

In
Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  Montreal 
Canada  2009.

[40] K. Zhang  J. Zhang  B. Huang  B. Sch¨olkopf  and C. Glymour. On the identiﬁability and
In

estimation of functional causal models in the presence of outcome-dependent selection.
Proceedings of the 32rd Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2016.

12

,Biwei Huang
Kun Zhang
Pengtao Xie
Mingming Gong
Eric Xing
Clark Glymour