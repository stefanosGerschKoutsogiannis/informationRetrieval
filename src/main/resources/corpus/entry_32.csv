2014,A Block-Coordinate Descent Approach for Large-scale Sparse Inverse Covariance Estimation,The sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing. In this problem  the inverse of a covariance matrix of a multivariate normal distribution is estimated  assuming that it is sparse. An $\ell_1$ regularized log-determinant optimization problem is typically solved to approximate such matrices. Because of memory limitations  most existing algorithms are unable to handle large scale instances of this problem. In this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets. Our method treats the sought matrix block-by-block using quadratic approximations  and we show that this approach has advantages over existing methods in several aspects. Numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods  especially for large-scale problems.,A Block-Coordinate Descent Approach for

Large-scale Sparse Inverse Covariance Estimation

Eran Treister∗†

Computer Science  Technion  Israel
and Earth and Ocean Sciences  UBC
Vancouver  BC  V6T 1Z2  Canada
eran@cs.technion.ac.il

Javier Turek∗

Department of Computer Science

Technion  Israel Institute of Technology

Technion City  Haifa 32000  Israel
javiert@cs.technion.ac.il

Abstract

The sparse inverse covariance estimation problem arises in many statistical appli-
cations in machine learning and signal processing. In this problem  the inverse of a
covariance matrix of a multivariate normal distribution is estimated  assuming that
it is sparse. An (cid:96)1 regularized log-determinant optimization problem is typically
solved to approximate such matrices. Because of memory limitations  most exist-
ing algorithms are unable to handle large scale instances of this problem. In this
paper we present a new block-coordinate descent approach for solving the prob-
lem for large-scale data sets. Our method treats the sought matrix block-by-block
using quadratic approximations  and we show that this approach has advantages
over existing methods in several aspects. Numerical experiments on both syn-
thetic and real gene expression data demonstrate that our approach outperforms
the existing state of the art methods  especially for large-scale problems.

Introduction

1
The multivariate Gaussian (Normal) distribution is ubiquitous in statistical applications in machine
learning  signal processing  computational biology  and others. Usually  normally distributed ran-
dom vectors are denoted by x ∼ N (µ  Σ) ∈ Rn  where µ∈ Rn is the mean  and Σ∈ Rn×n is the
covariance matrix. Given a set of realizations {xi}m
i=1  many such applications require estimating
the mean µ  and either the covariance Σ or its inverse Σ−1  which is also called the precision matrix.
Estimating the inverse of the covariance matrix is useful in many applications [2] as it represents the
underlying graph of a Gaussian Markov Random Field (GMRF). Given the samples {xi}m
i=1  both
the mean vector µ and the covariance matrix Σ are often approximated using the standard maximum
likelihood estimator (MLE)  which leads to ˆµ = 1
m

i=0 xi and1

(cid:80)m

m(cid:88)

i=0

(cid:52)
= ˆΣMLE =

S

1
m

(xi − ˆµ)(xi − ˆµ)T  

(1)

which is also called the empirical covariance matrix. Speciﬁcally  according to the MLE  Σ−1 is
estimated by solving the optimization problem

(cid:52)
= min
A(cid:31)0
∗The authors contributed equally to this work.
†Eran Treister is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship.
1Equation (1) is the standard MLE estimator. However  sometimes the unbiased MLE estimation is pre-

− log(det A) + tr(SA) 

min
A(cid:31)0

f (A)

(2)

ferred  where m − 1 replaces m in the denominator.

1

which is obtained by applying − log to the probability density function of the Normal distribution.
However  if the number of samples is lower than the dimension of the vectors  i.e.  m < n  then
S in (1) is rank deﬁcient and not invertible  whereas the true Σ is assumed to be positive deﬁnite 
hence full-rank. Still  when m < n one can estimate the matrix by adding further assumptions. It is
well-known [5] that if (Σ−1)ij = 0 then the random scalar variables in the i-th and j-th entries in x
are conditionally independent. Therefore  in this work we adopt the notion of estimating the inverse
of the covariance  Σ−1  assuming that it is sparse. (Note that in most cases Σ is dense.) For this
purpose  we follow [2  3  4]  and minimize (2) with a sparsity-promoting (cid:96)1 prior:

(cid:52)
= min
A(cid:31)0

f (A) + λ(cid:107)A(cid:107)1.

(3)

min
A(cid:31)0

F (A)

Here  f (A) is the MLE functional deﬁned in (2)  (cid:107)A(cid:107)1 ≡(cid:80)

i j |aij|  and λ > 0 is a regularization
parameter that balances between the sparsity of the solution and the ﬁdelity to the data. The spar-
sity assumption corresponds to a small number of statistical dependencies between the variables.
Problem (3) is also called Covariance Selection [5]  and is non-smooth and convex.
Many methods were recently developed for solving (3)—see [3  4  7  8  10  11  12  15  16] and ref-
erences therein. The current state-of-the-art methods  [10  11  12  16]  involve a “proximal Newton”
approach [20]  where a quadratic approximation is applied on the smooth part f (A) in (3)  leaving
the non-smooth (cid:96)1 term intact  in order to obtain the Newton descent direction. To obtain this  the
gradient and Hessian of f (A) are needed and are given by

∇f (A) = S − A−1 

∇2f (A) = A−1 ⊗ A−1 

(4)

where ⊗ is the Kronecker product. The gradient in (4) already shows the main difﬁculty in solving
this problem: it contains A−1  the inverse of the sparse matrix A  which may be dense and expensive
to compute. The advantage of the proximal Newton approach for this problem is the low overhead:
by calculating the A−1 in ∇f (A)  we also get the Hessian at the same cost [11  12  16].
In this work we aim at solving large scale instances of (3)  where n is large  such that O(n2) variables
cannot ﬁt in memory. Such problem sizes are required in fMRI [11] and gene expression analysis
[9] applications  for example. Large values of n introduce limitations: (a) They preclude storing
the full matrix S in (1)  and allow us to use only the vectors {xi}m
i=1  which are assumed to ﬁt in
memory. (b) While the sparse matrix A in (3) ﬁts in memory  its dense inverse does not. Because
of this limitation  most of the methods mentioned above cannot be used to solve (3)  as they require
computing the full gradient of f (A)  which is a dense n × n symmetric matrix. The same applies
for the blocking strategies of [2  7]  which target the dense covariance matrix itself rather than
its inverse  using the dual formulation of (3). One exception is the proximal Newton approach in
[11]  which was made suitable for large-scale matrices by treating the Newton direction problem in
blocks.
In this paper  we introduce an iterative Block-Coordinate Descent [20] method for solving large-
scale instances of (3). We treat the problem in blocks deﬁned as subsets of columns of A. Each
block sub-problem is solved by a quadratic approximation  resulting in a descent direction that
corresponds only to the variables in the block. Since we consider one sub-problem at a time  we can
fully store the gradient and Hessian for the block. In contrast  [11] applies a blocking approach to
the full Newton problem  which results in a sparse n× n descent direction. There  all the columns of
A−1 are calculated for the gradient and Hessian of the problem for each inner iteration when solving
the full Newton problem. Therefore  our method requires less calculations of A−1 than [11]  which
is the most computationally expensive task in both algorithms. Furthermore  our blocking strategy
allows an efﬁcient linesearch procedure  while [11] requires computing a determinant of a sparse
n × n matrix. Although our method is of linear order of convergence  it converges in less iterations
than [11] in our experiments. Note that the asymptotic convergence of [11] is quadratic only if the
exact Newton direction is found at each iteration  which is very costly for large-scale problems.

2

1.1 Newton’s Method for Covariance Selection

The proximal Newton approach mentioned earlier is iterative  and at each iteration k  the smooth part
of the objective in (3) is approximated by a second order Taylor expansion around the k-th iterate
A(k). Then  the Newton direction ∆∗ is the solution of an (cid:96)1 penalized quadratic minimization
problem 

min
∆

˜F (A(k) + ∆) = min
∆

where W =(cid:0)A(k)(cid:1)−1 is the inverse of the k-th iterate. Note that the gradient and Hessian of f (A)

tr(∆W∆W) + λ(cid:107)A(k) + ∆(cid:107)1 

f (A(k)) + tr(∆(S − W)) +

(5)

1
2

in (4) are featured in the second and third terms in (5)  respectively  while the ﬁrst term of (5) is
constant and can be ignored. Problem (5) corresponds to the well-known LASSO problem [18] 
which is popular in machine learning and signal/image processing applications [6]. The methods of
[12  16  11] apply known LASSO-solvers for treating the Newton direction minimization (5).
Once the direction ∆∗ is computed  it is added to A(k) employing a linesearch procedure to suf-
ﬁciently reduce the objective in (3) while ensuring positive deﬁniteness. To this end  the updated
iterate is A(k+1) = A(k) + α∗∆∗  and the parameter α∗ is obtained using Armijo’s rule [1  12]. That
is  we choose an initial value of α0  and a step size 0 < β < 1  and accordingly deﬁne αi = βiα0.
We then look for the smallest i ∈ N that satisﬁes the constraint A(k) + αi∆∗ (cid:31) 0  and the condition

F (A(k) + αi∆∗) ≤ F (A(k)) + αiσ

tr(∆∗(S − W)) + λ(cid:107)A(k) + ∆∗(cid:107)1 − λ(cid:107)A(k)(cid:107)1

.

(6)

(cid:105)

(cid:104)

The parameters α0  β  and σ are usually chosen as 1 0.5  and 10−4 respectively.

1.2 Restricting the Updates to Active Sets

An additional signiﬁcant idea of [12] is to restrict the minimization of (5) at each iteration to an
“active set” of variables and keep the rest as zeros. The active set of a matrix A is deﬁned as

Active(A) =(cid:8)(i  j) : Aij (cid:54)= 0 ∨ |(S − A−1)ij| > λ(cid:9) .

This set comes from the deﬁnition of the sub-gradient of (3). In particular  as A(k) approaches

the solution A∗  Active(A(k)) approaches(cid:8)(i  j) : A∗
ij (cid:54)= 0(cid:9). As noted in [12  16]  restricting
(5) to the variables in Active(cid:0)A(k)(cid:1) reduces the computational complexity: given the matrix W 
K = |Active(cid:0)A(k)(cid:1)|. Hence  any method for solving the LASSO problem can be utilized to
solve (5) effectively while saving computations by restricting its solution to Active(cid:0)A(k)(cid:1). Our
experiments have veriﬁed that restricting the minimization of (5) only to Active(cid:0)A(k)(cid:1) does not

the Hessian (third) term in (5) can be calculated in O(Kn) operations instead of O(n3)  where

(7)

signiﬁcantly increase the number of iterations needed for convergence.

2 Block-Coordinate-Descent for Inverse Covariance (BCD-IC) Estimation

In this Section we describe our contribution. To solve problem (3)  we apply an iterative Block-
Coordinate-Descent approach [20]. At each iteration  we divide the column set {1  ...  n} into
blocks. Then we iterate over all blocks  and in turn minimize (3) restricted to the “active” vari-
ables of each block  which are determined according to (7). The other matrix entries remain ﬁxed
during each update. The matrix A is updated after each block-minimization.
We choose our blocks as sets of columns because the portion of the gradient (4) that corresponds
to such blocks can be computed as solutions of linear systems. Because the matrix is symmetric 
the corresponding rows are updated simultaneously. Figure 1 shows an example of a BCD iteration
where the blocks of columns are chosen in sequential order. In practice  the sets of columns can
be non-contiguous and vary between the BCD iterations. We elaborate later on how to partition

3

Figure 1: Example of a BCD iteration. The blocks are treated successively.

the columns  and on some advantages of this block-partitioning. Partitioning the matrix into small
blocks enables our method to solve (3) in high dimensions (up to millions of variables)  requiring
O(n2/p) additional memory  where p is the number of blocks (that is in addition to the memory
needed for storing the iterated solution A(k) itself).
2.1 Block Coordinate Descent Iteration
Assume that the set of columns {1  ...  n} is divided into p blocks {Ij}p
j=1  where Ij is the set of
indices that corresponds to the columns and rows in the j-th block. As mentioned before  in the
BCD-IC algorithm we traverse all blocks and update the iterated solution matrix block by block.
We denote the updated matrix after treating the j-th block at iteration k by A(k)
and the next iterate
A(k+1) is deﬁned once the last block is treated  i.e.  A(k+1) = A(k)
p .
To treat each block of (3)  we adopt both of the ideas described earlier: we use a quadratic approxi-
mation to solve each block  while also restricting the updated entries to the active set. For simplicity
of notation in this section  let us denote the updated matrix A(k)
j−1  before treating block j at iteration
k  by ˜A. To update block j  we change only the entries in the rows/columns in Ij. First  we form
and minimize a quadratic approximation of problem (3)  restricted to the rows/columns in Ij:

j

˜F ( ˜A + ∆j) 

min
∆j

(8)

where ˜F (·) is the quadratic approximation of (3) around ˜A  similarly to (5)  and ∆j has non-zero
entries only in the rows/columns in Ij. In addition  the non-zeros of ∆j are restricted to Active( ˜A)
deﬁned in (7). That is  we restrict the minimization (8) to

ActiveIj ( ˜A) = Active( ˜A) ∩ {(i  k) : i ∈ Ij ∨ k ∈ Ij}  

(9)

while all other elements are set to zero for the entire treatment of the j-th block. To calculate this
set  we check the condition in (7) only in the columns and rows of Ij. To deﬁne this active set  and
to calculate the gradient (4) for block Ij  we ﬁrst calculate the columns Ij of ˜A−1  which is the
main computational task of our algorithm. To achieve that  we solve |Ij| linear systems  with the
canonical vectors el as right-hand-sides for each l ∈ Ij  i.e.  ( ˜A−1)Ij = ˜A−1EIj . The solution
of these linear systems can be achieved in various ways. Direct methods may be applied using
the Cholesky factorization  which requires up to O(n3) operations. For large dimensions  iterative
methods such as Conjugate Gradients (CG) are usually preferred  because the cost of each iteration
is proportional to the number of non-zeros in the sparse matrix. See Section A.4 in the Appendix
for details about the computational cost of this part of the algorithm.

2.1.1 Treating a Block-subproblem by Newton’s Method
To get the Newton direction for the j-th block  we solve the LASSO problem (8)  for which there are
many available solvers [22]. We choose the Polak-Ribiere non-linear Conjugate Gradients (NLCG)
method of [19] which  together with a diagonal preconditioner  was used to solve this problem in
[22  19]. We describe the NLCG algorithm in Apendix A.1. To use this method  we need to calculate
the objective of (8) and its gradient efﬁciently.
The calculation of the objective in (8) is much simpler than the full version in (5)  because only
blocks of rows/columns are considered. Denoting W = ˜A−1  to compute the objective in (8) and
its gradient we need to calculate the matrices W∆jW and S − W only at the entries where ∆j is

4

non-zero (in the rows/columns in Ij). These matrices are symmetric  and hence  only their columns
are necessary. This idea applies for the (cid:96)1 term of the objective in (8) as well.
In each iteration of the NLCG method  the main computational task involves calculating W∆jW in
the columns of Ij. For that  we reuse the Ij columns of ˜A−1 calculated for obtaining (9)  which we
denote by WIj . Since we only need the result in the columns Ij  we ﬁrst notice that (W∆jW)Ij
=
W∆jWIj   and the product ∆jWIj can be computed efﬁciently because ∆j is sparse.
Computing W(∆jWIj ) is another relatively expensive part of our algorithm  and here we exploit
the restriction to the Active Set. That is  we only need to compute the entries in (9). For this  we
follow the idea of [11] and use the rows (or columns) of W that are represented in (9). Besides the
columns Ij of W we also need the “neighborhood” of Ij deﬁned as

Nj =(cid:8)i : ∃k /∈ Ij : (i  k) ∈ ActiveIj (A)(cid:9) .

(10)

The size of this set will determine the amount of additional columns of W that we need  and there-
fore we want it to be as small as possible. To achieve that  we deﬁne the blocks {Ij} using clustering
methods  following [11]. We use METIS [13]  but other methods may be used instead. The aim of
these methods is to partition the indices of the matrix columns/rows into disjoint subsets of rela-
tively small size  such that there are as few as possible non-zero entries outside the diagonal blocks
of the matrix that correspond to each subset. In our notation  we aim that the size of Nj will be as
small as possible for every block Ij  and that the size of Ij will be small enough. Note that after
we compute WNj   we need to actually store and use only |Nj| × |Nj| numbers out of WNj . How-
ever  there might be situations where the matrix has a few dense columns  resulting in some sets Nj
of size O(n). Computing WNj for those sets is not possible because of memory limitations. We
treat this case separately—see Section A.2 in the Appendix for details. For a discussion about the
computational cost of this part—see Section A.4 in the Appendix.

j = A(k)

j−1 + α∗∆∗

j is the Newton direction obtained by solving problem (8). Now we seek to update
j  where α∗ > 0 is obtained by a linesearch procedure

2.1.2 Optimizing the Solution in the Newton Direction with Line-search
Assume that ∆∗
the iterated matrix A(k)
similarly to Equation (6).
For a general Newton direction matrix ∆∗ as in (6)  this procedure requires calculating the determi-
nant of an n×n matrix. In [11]  this is done by solving n−1 linear systems of decreasing sizes from
n − 1 to 1. However  since our direction ∆∗
j has a special block structure  we obtain a signiﬁcantly
cheaper linesearch procedure compared to [11]  assuming that the blocks Ij are relatively small.
First  the trace and (cid:96)1 terms that are involved in the objective of (3) can be calculated with respect
only to the entries in the columns Ij (the rows are taken into account by symmetry). The log det
term  however  needs more special care  and is eventually reduced to calculating the determinant of
an |Ij| × |Ij| matrix  which becomes cheaper as the block size decreases. Let us introduce a parti-
tioning of any matrix A into blocks  according to a set of indices Ij ⊆ {1  ...  n}. Assume without
loss of generality that the rows and columns of A have been permuted such that the columns/rows
with indices in Ij appear ﬁrst  and let

 A11

A21

A =



A12

A22

(11)

be a partitioning of A into four blocks. The sub-matrix A11 corresponds to the elements in rows
Ij and in columns Ij in ˜A. According to the Schur complement [17]  for any invertible matrix and
block-partitioning as above  the following holds:

log det(A) = log det(A22) + log det(A11 − A12A−1

22 A21).

(12)

5

In addition  for any symmetric matrix A the following applies:

22 A21 (cid:31) 0.
Using the above notation for ˜A and the corresponding partitioning for ∆∗

A (cid:31) 0 ⇔ A22 (cid:31) 0 and A11 − A12A−1

(13)

j  we write using (12):

log det ( ˜A + α∆j) = log det ( ˜A22) + log det(B0 + αB1 + α2B2)

(14)

22

22

22 ∆21  and

˜A21  B1 = ∆11 − ∆12 ˜A−1

22 ∆21. (Note that here we replaced ∆∗

˜A21 − ˜A12 ˜A−1
j by ∆ to ease notation.)

where B0 = ˜A11 − ˜A12 ˜A−1
B2 = −∆12 ˜A−1
j (cid:31) 0 involved in the linesearch (6) is equivalent
Finally  the positive deﬁniteness condition ˜A+α∗∆∗
to B0 + αB1 + α2B2 (cid:31) 0  assuming that ˜A22 (cid:31) 0  following (13). Throughout the iterations  we
always guarantee that our iterated solution matrix ˜A remains positive deﬁnite by linesearch in every
update. This requires that the initialization of the algorithm  A(0)  be positive deﬁnite. If the set
Ij is relatively small  then the matrices Bi in (14) are also small (|Ij| × |Ij|)  and we can easily
compute the objective F (·)  and apply the Armijo rule (6) for ∆∗
j. Calculating the matrices Bi
in (14) seems expensive  however  as we show in Appendix A.3  they can be obtained from the
previously computed matrices WIj and WNj mentioned earlier. Therefore  computing (14) can be
achieved in O(|Ij|3) time complexity.

Algorithm: BCD-IC(A(0) {xi}m
for k = 0  1  2  ... do

i=1 λ)
Calculate clusters of elements {Ij}p
% Denote: A(k)
for j = 1  ...  p do

0 = A(k)

j=1 based on A(k).

(cid:16)
(cid:16)
(cid:16)

j−1)−1(cid:17)
(cid:17)
j−1)−1(cid:17)

Ij

Nj

Compute WIj =

(A(k)

. % solve |Ij| linear systems

A(k)
j−1
(A(k)

Deﬁne ActiveIj
Compute WNj =
Find the Newton direction ∆∗
Update the solution: A(k)

as in (9)  and deﬁne the set Nj in (10).

. % solve |Nj| linear systems

j by solving the LASSO problem (8).
j−1 + α∗∆∗

j by linesearch.

j = A(k)

end
% Denote: A(k+1) = A(k)

p

end

Algorithm 1: Block Coordinate Descent for Inverse Covariance Estimation

3 Convergence Analysis
In this Section  we elaborate on the convergence of the BCD-IC algorithm to the global optimum
of (3). We base our analysis on [20  12].
In [20]  a general block-coordinate-descent approach
is analyzed to solve minimization problems of the form F (A) = f (A) + λh(A) composed of
the sum of a smooth function f (·) and a separable convex function h(·)  which in our case are
− log det(A) + tr(SA) and (cid:107)A(cid:107)1  respectively. Although this setup ﬁts the functional F (A) in (3) 
[20] treats the problem in the Rn×n domain  while the minimization in (3) is being constrained over
Sn
++—the symmetric positive deﬁnite matrices domain. To overcome this limitation  the authors in
[12] extended the analysis in [20] to treat the speciﬁc constrained problem (3).
In particular  [20  12] consider block-coordinate-descent methods where in each step t a subset Jt
of variables is updated. Then  a Gauss-Seidel condition is necessary to ensure that all variables are
updated every T steps:

Jl+t ⊇ N ∀t = 1  2  . . .  

(15)

(cid:91)

l=0 ... T−1

6

where N is the set of all variables  and T is a ﬁxed number. Similarly to [12]  treating each block
of columns Ij in the BCD-IC algorithm is equivalent to updating the elements outside the active set
ActiveIj (A)  followed by an update of the elements in ActiveIj (A). Therefore  in (15)  we set

J2t = {(i  l) : i ∈ Ij ∨ l ∈ Ij} \ ActiveIj ( ˜A) 

J2t+1 = ActiveIj ( ˜A) 

where the step index t corresponds to the block j at the iteration k of BCD-IC. In [12  Lemma
1]  it is shown that setting the elements outside the active set for block j to zero satisﬁes the opti-
mality condition of that step. Therefore  in our algorithm we only need to update the elements in
ActiveIj (A). Now  if we were using p ﬁxed blocks containing all the coordinates of A in Algo-
rithm (1) (no clustering is applied)  then the Gauss-Seidel condition (15) would be satisﬁed every
T = 2p blocks. When clustering is applied  the block-partitioning {Ij} can change at every acti-
vation of the clustering method. Therefore  condition (15) is satisﬁed at most after T = 4˜p  where
˜p is the maximum number of blocks obtained from all the activations of the clustering algorithm.
For completeness  we include in Appendix A.5 the lemmas in [12] and the proof of the following
theorem:
Theorem 1. In Algorithm 1  the sequence

converges to the global optimum of (3).

(cid:110)

(cid:111)

A(k)

j

4 Numerical Results
In this section we demonstrate the efﬁciency of the BCD-IC method  and compare it with other
methods for both small and large scales. For small-scale problems we include QUIC [12]  BIG-
QUIC [11] and G-ISTA [8]  which are the state-of-the-art methods at this scale. For large-scale
problems  we compare our method only with BIG-QUIC as it is the only feasible method known
to us at this scale. For all methods  we use the original code which was provided by the authors—
all implemented in C and parallelized (except QUIC which is partially parallelized). Our code for
BCD-IC is MATLAB based with several routines in C. All the experiments were run on a machine
with 2 Intel Xeon E-2650 2.0GHz processors with 16 cores and 64GB RAM  using Windows 7 OS.
As a stopping criterion for BCD-IC  we use the rule as in [11]: (cid:107)gradSF (A(k))(cid:107)1 < (cid:107)A(k)(cid:107)1 
where gradSF (·) is the minimal norm subgradient  deﬁned in Equation (25) in Appendix A.5. For
 = 10−2 as we choose  this results in the entries in A(k) being about two digits accurate compared
to the true solution Σ−1∗
. As in [11]  we approximate WIj and WNj by using CG  which we
stop once the residual drops below 10−5 and 10−4  respectively. For stopping NLCG (Algorithm 2)
we use nlcg = 10−4 (see details at the end of Section A.1). We note that for the large-scale test
problems  BCD-IC with optimal block size requires less memory than BIG-QUIC.

(cid:105)

(cid:104)

4.1 Synthetic Experiments
We use two synthetic experiments to compare the performance of the methods. First  the random
matrix from [14]  which is generated to have about 10 non-zeros per row  and to be well-conditioned.
We generate matrices of sizes n varying from 5 000 to 160 000  and generate 200 samples for each
(m = 200). The values of λ are chosen so that the solution Σ−1∗
has approximately 10n non-zeros.
BCD-IC is run with block sizes of 64  96  128  256  and 256 for each of the random tests in Table
1  respectively. The second problem is a 2D version of the chain example in [14]  which can be
  applied on a square lattice. λ is chosen such that Σ−1∗
represented as the 2D stencil 1
4
has about 5n non-zeros. For these tests  BCD-IC is run with block size of 1024.
Table 1 summarizes the results for this test case. The results show that for small-scale problems 
G-ISTA is the fastest method and BCD-IC is just behind it. However  from size 20 000 and higher 
BCD-IC is the fastest. We could not run QUIC and G-ISTA on problems larger than 20 000 because
of memory limitations. The time gap between G-ISTA and both BCD-IC and BIG-QUIC in small-
scales can be reduced if their programs receive the matrix S as input instead of the {xi}m
i=1.
4.2 Gene Expression Analysis Experiments
For the large-scale real-world experiments  we use gene expression datasets that are available at the
Gene Expression Omnibus (http://www.ncbi.nlm.nih.gov/geo/). We use several of the

−1 5 −1

−1
−1

7

test  n

(cid:107)Σ−1(cid:107)0
59 138
random 5K
118 794
random 10K
237 898
random 20K
475 406
random 40K
random 80K
950 950
random 160K 1 901 404
1 248 000
2 503 488
4 996 000

G-ISTA
13.6s(7)
60.2s(7)
491s(8)
*
*
*
*
*
*
Table 1: Results for the random and 2D synthetic experiments. (cid:107)Σ−1(cid:107)0 and (cid:107)Σ−1∗(cid:107)0 denote the number of
non-zeros in the true and estimated inverse covariance matrices  respectively. For each run  timings are reported
in seconds and number of iterations in parentheses. ‘*’ means that the algorithm ran out of memory.

BIG-QUIC
19.6s(5)
73.8s(5)
673s(5)
2 671s(5)
16 764s(5)
25 584s(4)
40 530s(4)
203 370s(4)
1 220 213s(4)

BCD-IC
15.3s(3)
61.8s(3)
265s(3)
729s(4)
4 102s(4)
21 296s(4)
24 235s(4)
130 636s(4)
777 947s(4)

(cid:107)Σ−1∗(cid:107)0
63 164
139 708
311 932
423 696
891 268
1 852 198
1 553 698
3 002 338
5 684 306

QUIC
28.7s(5)
114s(5)
823s(5)
*
*
*
*
*
*

λ
0.22
0.23
0.24
0.26
0.27
0.28
0.30
0.31
0.32

2D 5002
2D 7082
2D 10002

tests reported in [9]. The data is preprocessed to have zero mean and unit variance for each variable
(i.e.  diag(S) = I). Table 2 shows the datasets as well as the numbers of variables (n) and samples
(m) on each. In particular  these datasets have many variables and very few samples (m (cid:28) n).
Because of the size of the problems  we ran only BCD-IC and BIG-QUIC for these test cases.
For the ﬁrst three tests in Table 2  λ was chosen so that the solution matrix has about 10n non-zeros.
For the fourth test  we choose a relatively high λ = 0.9 since the low number of samples causes the
solutions with smaller λ’s to be quite dense. BCD-IC is run with block size of 256 for all the tests
in Table 2. We found these datasets to be more challenging than the synthetic experiments above.
Still  both algorithms BCD-IC and BIG-QUIC manage to estimate the inverse covariance matrix in
reasonable time. As in the synthetic case  BCD-IC outperforms BIG-QUIC in all test cases. BCD-IC
requires a smaller number of iterations to converge  which translates into shorter timings. Moreover 
the average time of each BCD-IC iteration is faster than that of BIG-QUIC.

n

m

Description
Liver cancer
Breast cancer
Prostate cancer
Liver cancer

BIG-QUIC
code name
GSE1898
5 079.5s (12)
GSE20194
2 810.6s (10)
GSE17951
8 229.7s
(9)
GSE14322
127 199s (14)
Table 2: Gene expression results. (cid:107)Σ−1∗(cid:107)0 denotes the number of non-zeros in the estimated covariance
matrix. For each run  timings are reported in seconds and number of iterations in parentheses.

BCD-IC
788.3s (7)
452.9s (8)
1 621.9s (6)
55 314.8s (9)

21  794
22  283
54  675
104  702

λ
0.7
0.7
0.78
0.9

182
278
154
76

(cid:107)Σ−1∗(cid:107)0
293 845
197 953
558 929
4 973 476

5 Conclusions

In this work we introduced a Block-Coordinate Descent method for solving the sparse inverse co-
variance problem. Our method has a relatively low memory footprint  and therefore it is especially
attractive for solving large-scale instances of the problem. It solves the problem by iterating and up-
dating the matrix block by block  where each block is chosen as a subset of columns and respective
rows. For each block sub-problem  a proximal Newton method is applied  requiring a solution of a
LASSO problem to ﬁnd the descent direction. Because the update is limited to a subset of columns
and rows  we are able to store the gradient and Hessian for each block  and enjoy an efﬁcient line-
search procedure. Numerical results show that for medium-to-large scale experiments our algorithm
is faster than the state-of-the-art methods  especially when the problem is relatively hard.
Acknowledgement: The authors would like to thank Prof. Irad Yavneh for his valuable comments
and guidance throughout this work. The research leading to these results has received funding from
the European Union’s - Seventh Framework Programme (FP7/2007-2013) under grant agreement no
623212 MC Multiscale Inversion.

8

References

[1] L. Armijo. Minimization of functions having lipschitz continuous ﬁrst partial derivatives. Paciﬁc Journal

of Mathematics  16(1):1–3  1966.

[2] O. Banerjee  L. El Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood
estimation for multivariate gaussian or binary data. J. of Machine Learning Research  9:485–516  2008.

[3] O. Banerjee  L. El Ghaoui  A. d’Aspremont  and G. Natsoulis. Convex optimization techniques for ﬁtting

sparse gaussian graphical models. In Proceedings of the 23rd ICML  pages 89–96. ACM  2006.

[4] A. d’Aspremont  O. Banerjee  and L. El Ghaoui. First-order methods for sparse covariance selection.

SIAM Journal on Matrix Analysis and App.  30(1):56–66  2008.

[5] A. P. Dempster. Covariance selection. Biometrics  pages 157–175  1972.

[6] M. Elad. Sparse and redundant representations: from theory to applications in signal and image process-

ing. Springer  2010.

[7] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estimation with the

graphical lasso. Biostatistics  9(3):432–441  2008.

[8] D. Guillot  B. Rajaratnam  B. T. Rolfs  A. Maleki  and I. Wong. Iterative thresholding algorithm for sparse

inverse covariance estimation. NIPS  Lake Tahoe CA  2012.

[9] J. Honorio and T. S. Jaakkola. Inverse covariance estimation for high-dimensional data in linear time and

space: Spectral methods for riccati and sparse models. In Proc. of the 29th Conference on UAI  2013.

[10] Cho-Jui Hsieh  Inderjit Dhillon  Pradeep Ravikumar  and Arindam Banerjee. A divide-and-conquer

method for sparse inverse covariance estimation. In NIPS 25  pages 2339–2347  2012.

[11] Cho-Jui Hsieh  Matyas A Sustik  Inderjit Dhillon  Pradeep Ravikumar  and Russell Poldrack. Big & Quic:

Sparse inverse covariance estimation for a million variables. In NIPS 26  pages 3165–3173  2013.

[12] Cho-Jui Hsieh  Matyas A Sustik  Inderjit S Dhillon  and Pradeep D Ravikumar. Sparse inverse covariance

matrix estimation using quadratic approximation. In NIPS 24  pages 2330–2338  2011.

[13] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular

graphs. SIAM Journal on Scientiﬁc Computing  20(1):359–392  1998.

[14] Lu Li and Kim-Chuan Toh. An inexact interior point method for l-1 regularized sparse covariance selec-

tion. Mathematical Programming Computation  2(3-4):291–315  2010.

[15] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale

graphical lasso. The Journal of Machine Learning Research  13:781–794  2012.

[16] Peder A Olsen  Figen ¨oztoprak  Jorge Nocedal  and Steven J Rennie. Newton-like methods for sparse

inverse covariance estimation. In NIPS 25  pages 764–772  2012.

[17] Y. Saad. Iterative methods for sparse linear systems  2nd edition. SIAM  2003.

[18] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  pages 267–288  1996.

[19] E. Treister and I. Yavneh. A multilevel iterated-shrinkage approach to l1 penalized least-squares mini-

mization. Signal Processing  IEEE Transactions on  60(12):6319–6329  2012.

[20] Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for nonsmooth separable mini-

mization. Mathematical Programming  117(1-2):387–423  2009.

[21] Z. Wen  W. Yin  D. Goldfarb  and Y. Zhang. A fast algorithm for sparse reconstruction based on shrinkage 

subspace optimization and continuation. SIAM Sci. Comp.  32(4):1832–1857  2010.

[22] M. Zibulevsky and M. Elad. L1-l2 optimization in signal and image processing. Signal Processing

Magazine  IEEE  27(3):76–88  May 2010.

9

,Yanshuai Cao
Marcus Brubaker
David Fleet
Aaron Hertzmann
Eran Treister
Javier Turek
Xinan Wang
Sanjoy Dasgupta
Bo Dai
Dahua Lin
Tao Sun
Yuejiao Sun
Dongsheng Li
Qing Liao