2013,Learning Adaptive Value of Information for Structured Prediction,Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations.  However  the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications  often dominating the cost of inference in the models.  Significant efforts have been devoted to sparsity-based model selection to decrease this cost.  Such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time.  We address the key challenge of learning to control fine-grained feature extraction adaptively  exploiting non-homogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efficient value-function approximation  which adaptively determines the value of information of features at the level of individual variables for each input.  We demonstrate significant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video  we achieve a more accurate state-of-the-art model that is simultaneously 4$\times$ faster while using only a small fraction of possible features  with similar results on an OCR task.,Learning Adaptive Value of Information

for Structured Prediction

David Weiss

University of Pennsylvania

Philadelphia  PA

djweiss@cis.upenn.edu

taskar@cs.washington.edu

University of Washington

Ben Taskar

Seattle  WA

Abstract

Discriminative methods for learning structured models have enabled wide-spread
use of very rich feature representations. However  the computational cost of fea-
ture extraction is prohibitive for large-scale or time-sensitive applications  often
dominating the cost of inference in the models. Signiﬁcant efforts have been de-
voted to sparsity-based model selection to decrease this cost. Such feature se-
lection methods control computation statically and miss the opportunity to ﬁne-
tune feature extraction to each input at run-time. We address the key challenge
of learning to control ﬁne-grained feature extraction adaptively  exploiting non-
homogeneity of the data. We propose an architecture that uses a rich feedback
loop between extraction and prediction. The run-time control policy is learned us-
ing efﬁcient value-function approximation  which adaptively determines the value
of information of features at the level of individual variables for each input. We
demonstrate signiﬁcant speedups over state-of-the-art methods on two challeng-
ing datasets. For articulated pose estimation in video  we achieve a more accurate
state-of-the-art model that is also faster  with similar results on an OCR task.

1

Introduction

Effective models in complex computer vision and natural language problems try to strike a favorable
balance between accuracy and speed of prediction. One source of computational cost is inference in
the model  which can be addressed with a variety of approximate inference methods. However  in
many applications  computing the scores of the constituent parts of the structured model–i.e. feature
computation–is the primary bottleneck. For example  when tracking articulated objects in video 
optical ﬂow is a very informative feature that often requires many seconds of computation time per
frame  whereas inference for an entire sequence typically requires only fractions of a second [16];
in natural language parsing  feature computation may take up to 80% of the computation time [7].
In this work  we show that large gains in the speed/accuracy trade-off can be obtained by departing
from the traditional method of “one-size-ﬁts-all” model and feature selection  in which a static set
of features are computed for all inputs uniformly. Instead  we employ an adaptive approach: the
parts of the structured model are constructed speciﬁcally at test-time for each particular instance  for
example  at the level of individual video frames. There are several key distinctions of our approach:
• No generative model. One approach is to assume a joint probabilistic model of the input
and output variables and a utility function measuring payoffs. The expected value of infor-
mation measures the increase in expected utility after observing a given variable [12  8].
Unfortunately  the problem of computing optimal conditional observation plans is compu-
tationally intractable even for simple graphical models like Naive Bayes [9]. Moreover 
joint models of input and output are typically quite inferior in accuracy to discriminative
models of output given input [10  3  19  1].

1

• Richly parametrized  conditional value function. The central component of our method
is an approximate value function that utilizes a set of meta-features to estimate future
changes in value of information given a predictive model and a proposed feature set as in-
put. The critical advantage here is that the meta-features can incorporate valuable properties
beyond conﬁdence scores from the predictive model  such as long-range input-dependent
cues that convey information about the self-consistency of a proposed output.
• Non-myopic reinforcement learning. We frame the control problem in terms of ﬁnd-
ing a feature extraction policy that sequentially adds features to the models until a budget
limit is reached  and we show how to learn approximate policies that result in accurate
structured models that are dramatically more efﬁcient. Speciﬁcally  we learn to weigh the
meta-features for the value function using linear function approximation techniques from
reinforcement learning  where we utilize a deterministic model that can be approximately
solved with a simple and effective sampling scheme.

In summary  we provide a discriminative  practical architecture that solves the value of information
problem for structured prediction problems. We ﬁrst learn a prediction model that is trained to use
subsets of features computed sparsely across the structure of the input. These feature combinations
factorize over the graph structure  and we allow for sparsely computed features such that different
vertices and edges may utilize different features of the input. We then use reinforcement learning to
estimate a value function that adaptively computes an approximately optimal set of features given a
budget constraint. Because of the particular structure of our problem  we can apply value function
estimation in a batch setting using standard least-squares solvers. Finally  we apply our method to
two sequential prediction domains: articulated human pose estimation and handwriting recognition.
In both domains  we achieve more accurate prediction models that utilize less features than the
traditional monolithic approach.

2 Related Work

There is a signiﬁcant amount of prior work on the issue of controlling test-time complexity. How-
ever  much of this work has focused on the issue of feature extraction for standard classiﬁcation
problems  e.g. through cascades or ensembles of classiﬁers that use different subsets of features at
different stages of processing. More recently  feature computation cost has been explicitly incorpo-
rated speciﬁcally into the learning procedure (e.g.  [6  14  2  5].) The most related recent work of this
type is [20]  who deﬁne a reward function for multi-class classiﬁcation with a series of increasingly
complex models  or [6]  who deﬁne a feature acquisition model similar in spirit to ours  but with
a different reward function and modeling a variable trade-off rather than a ﬁxed budget. We also
note that [4] propose explicitly modeling the value of evaluating a classiﬁer  but their approach uses
ensembles of pre-trained models (rather than the adaptive model we propose). And while the goals
of these works are similar to ours–explicitly controlling feature computation at test time–none of the
classiﬁer cascade literature addresses the structured prediction nor the batch setting.
Most work that addresses learning the accuracy/efﬁciency trade-off in a structured setting applies
primarily to inference  not feature extraction. E.g.  [23] extend the idea of a classiﬁer cascade to
the structured prediction setting  with the objective deﬁned in terms of obtaining accurate inference
in models with large state spaces after coarse-to-ﬁne pruning. More similar to this work  [7] incre-
mentally prune the edge space of a parsing model using a meta-features based classiﬁer  reducing
the total the number of features that need to be extracted. However  both of these prior efforts rely
entirely on the marginal scores of the predictive model in order to make their pruning decisions  and
do not allow future feature computations to rectify past mistakes  as in the case of our work.
Most related is the prior work of [22]  in which one of an ensemble of structured models is selected
on a per-example basis. This idea is essentially a coarse sub-case of the framework presented in this
work  without the adaptive predictive model that allows for composite features that vary across the
input  without any reinforcement learning to model the future value of taking a decision (which is
critical to the success of our method)  and without the local inference method proposed in Section 4.
In our experiments (Section 5)  the “Greedy (Example)” baseline is representative of the limitations
of this earlier approach.

2

Algorithm 1: Inference for x and budget B.
deﬁne an action a as a pair (cid:104)α ∈ G  t ∈ {1  . . .   T}(cid:105) ;
initialize B(cid:48) ← 0  z ← 0  y ← h(x  z) ;
initialize action space (ﬁrst tier) A = {(α  1) | α ∈ G};
while B(cid:48) < B and |A| > 0 do

a ← argmaxa∈A β(cid:62)φ(x  z  a);
A ← A \ a;
if ca ≤ (B − B(cid:48)) then
z ← z + a  B(cid:48) ← B(cid:48) + ca  y ← h(x  z);
A ← A ∪ (α  t + 1);

end

end

Figure 1: Overview of our approach. (Left) A high level summary of the processing pipeline: as in standard
structured prediction  features are extracted and inference is run to produce an output. However  information
may optionally feedback in the form of extracted meta-features that are used by a control policy to determine
another set of features to be extracted. Note that we use stochastic subgradient to learn the inference model w
ﬁrst and reinforcement learning to learn the control model β given w. (Right) Detailed algorithm for factor-
wise inference for an example x given a graph structure G and budget B. The policy repeatedly selects the
highest valued action from an action space A that represents extracting features for each constituent part of the
graph structure G.

3 Learning Adaptive Value of Information for Structured Prediction

Setup. We consider the setting of structured prediction  in which our goal is to learn a hypothesis
mapping inputs x ∈ X to outputs y ∈ Y(x)  where |x| = L and y is a L-vector of K-valued
variables  i.e. Y(x) = Y1×···×Y(cid:96) and each Yi = {1  . . .   K}. We follow the standard max-margin
structured learning approach [18] and consider linear predictive models of the form w(cid:62)f (x  y).
However  we introduce an additional explicit feature extraction state vector z:

h(x  z) = argmax
y∈Y(x)

w(cid:62)f (x  y  z).

(1)

Above  f (x  y  z) is a sparse vector of D features that takes time c(cid:62)z to compute for a non-negative
cost vector c and binary indicator vector z of length |z| = F . Intuitively  z indicates which of F sets
of features are extracted when computing f; z = 1 means every possible feature is extracted  while
z = 0 means that only a minimum set of features is extracted.
Note that by incorporating z into the feature function  the predictor h can learn to use different linear
weights for the same underlying feature value by conditioning the feature on the value of z. As we
discuss in Section 5  adapting the weights in this way is crucial to building a predictor h that works
well for any subset of features. We will discuss how to construct such features in more detail in
Section 4.
Suppose we have learned such a model h. At test time  our goal is to make the most accurate
predictions possible for an example under a ﬁxed budget B. Speciﬁcally  given h and a loss function
(cid:96) : Y × Y (cid:55)→ R+  we wish to ﬁnd the following:
H(x  B) = argmin

Ey|x[(cid:96)(y  h(x  z))]

(2)

z

In practice  there are three primary difﬁculties in optimizing equation (2). First  the distribution
P (Y |X) is unknown. Second  there are exponentially many zs to explore. Most important  how-
ever  is the fact that we do not have free access to the objective function. Instead  given x  we are
optimizing over z using a function oracle since we cannot compute f (x  y  z) without paying c(cid:62)z 
and the total cost of all the calls to the oracle must not exceed B. Our approach to solving these
problems is outlined in Figure 1; we learn a control model (i.e. a policy) by posing the optimization
problem as an MDP and using reinforcement learning techniques.
Adaptive extraction MDP. We model the budgeted prediction optimization as the following Markov
Decision Process. The state of the MDP s is the tuple (x  z) for an input x and feature extraction

3

INPUTEXTRACTFEATURESINFERENCEPOLICYEXTRACTMETA-FEATURESOUTPUTstate z (for brevity we will simply write s). The start state is s0 = (x  0)  with x ∼ P (X)  and
z = 0 indicating only a minimal set of features have been extracted. The action space A(s) is
{i | zi = 0}∪{0}  where zi is the i’the element of z; given a state-action pair (s  a)  the next state is
deterministically s(cid:48) = (x  z + ea)  where ea is the indicator vector with a 1 in the a’th component or
the zero vector if a = 0. Thus  at each state we can choose to extract one additional set of features 
or none at all (at which point the process terminates.) Finally  for ﬁxed h  we deﬁne the shorthand
η(s) = Ey|x(cid:96)(y  h(x  z)) to be the expected error of the predictor h given state z and input x.
We now deﬁne the expected reward to be the adaptive value of information of extracting the a’th set
of features given the system state and budget B:

(cid:26)η(s) − η(s(cid:48))

R(s  a  s(cid:48)) =

0

if c(cid:62)z(s(cid:48)) ≤ B
otherwise

(3)

Intuitively  (3) says that each time we add additional features to the computation  we gain reward
equal to the decrease in error achieved with the new features (or pay a penalty if the error increases.)
However  if we ever exceed the budget  then any further decrease does not count; no more reward
can be gained. Furthermore  assuming f (x  y  z) can be cached appropriately  it is clear that we pay
only the additional computational cost ca for each action a  so the entire cumulative computational
burden of reaching some state s is exactly c(cid:62)z for the corresponding z vector.
Given a trajectory of states s0  s1  . . .   sT   computed by some deterministic policy π  it is clear that
the ﬁnal cumulative reward Rπ(s0) is the difference between the starting error rate and the rate of
the last state satisfying the budget:

Rπ(s0) = η(s0) − η(s1) + η(s1) − ··· = η(s0) − η(st(cid:63) ) 

(4)
where t(cid:63) is the index of the ﬁnal state within the budget constraint. Therefore  the optimal policy
π(cid:63) that maximizes expected reward will compute z(cid:63) minimizing (2) while satisfying the budget
constraint.
Learning an approximate policy with long-range meta-features. In this work  we focus on a
straightforward method for learning an approximate policy: a batch version of least-squares policy
iteration [11] based on Q-learning [21]. We parametrize the policy using a linear function of meta-
features φ computed from the current state s = (x  z): πβ(s) = argmaxa β(cid:62)φ(x  z  a). The meta-
features (which we abbreviate as simply φ(s  a) henceforth) need to be rich enough to represent
the value of choosing to expand feature a for a given partially-computed example (x  z). Note that
we already have computed f (x  h(x  z)  z)  which may be useful in estimating the conﬁdence of
the model on a given example. However  we have much more freedom in choosing φ(s  a) than
we had in choosing f; while f is restricted to ensure that inference is tractable  we have no such
restriction for φ. We therefore compute functions of h(x  z) that take into account large sets of
output variables  and since we need only compute them for the particular output h(x  z)  we can
do so very efﬁciently. We describe the speciﬁc φ we use in our experiments in Section 5  typically
measuring the self-consistency of the output as a surrogate for the expected accuracy.
One-step off-policy Q-learning with least-squares. To simplify the notation  we will assume given
current state s  taking action a deterministically yields state s(cid:48). Given a policy π  the value of a policy
is recursively deﬁned as the immediate expected reward plus the discounted value of the next state:
(5)
The goal of Q-learning is to learn the Q for the optimal policy π(cid:63) with maximal Qπ(cid:63); however  it is
clear that we can increase Q by simply stopping early when Qπ(s  a) < 0 (the future reward in this
case is simply zero.) Therefore  we deﬁne the off-policy optimized value Q(cid:63)

Qπ(s  a) = R(s  a  s(cid:48)) + γQπ(s(cid:48)  π(s(cid:48))).

π as follows:

Q(cid:63)

π(st+1  π(st+1))]+ .

π(st  π(st)) = R(st  π(st)  st+1) + γ [Q(cid:63)

(6)
We propose the following one-step algorithm for learning Q from data. Suppose we have a ﬁnite
trajectory s0  . . .   sT . Because both π and the state transitions are deterministic  we can unroll the
recursion in (6) and compute Q(cid:63)
π(st  π(st)) for each sample using simple dynamic programming.
π(si  π(si)) = η(si) −
For example  if γ = 1 (there is no discount for future reward)  we obtain Q(cid:63)
η(st(cid:63) )  where t(cid:63) is the optimal stopping time that satisﬁes the given budget.
We therefore learn parameters β(cid:63) for an approximate Q as follows. Given an initial policy π  we
execute π for each example (xj  yj) to obtain trajectories sj
T . We then solve the following

0  . . .   sj

4

least-squares optimization 

β(cid:63) = argmin

λ||β||2 +

β

(cid:88)

(cid:16)

j t

1
nT

β(cid:62)φ(sj

t   π(sj

t )) − Q(cid:63)

(cid:17)2

π(sj

t   π(sj

t ))

 

(7)

using cross validation to determine the regularization parameter λ.
We perform a simple form of policy iteration as follows. We ﬁrst initialize β by estimating the
expected reward function (this can be estimated by randomly sampling pairs (s  s(cid:48))  which are more
efﬁcient to compute than Q-functions on trajectories). We then compute trajectories under πβ and
use these trajectories to compute β(cid:63) that approximates Q(cid:63)
π. We found that additional iterations of
policy iteration did not noticeably change the results.
Learning for multiple budgets. One potential drawback of our approach just described is that we
must learn a different policy for every desired budget. A more attractive alternative is to learn a
single policy that is tuned to a range of possible budgets. One solution is to set γ = 1 and learn
with B = ∞  so that the value Q(cid:63)
π represents the best improvement possible using some optimal
budget B(cid:63); however  at test time  it may be that B(cid:63) is greater than the available budget B and Q(cid:63)
π is
an over-estimate. By choosing γ < 1  we can trade-off between valuing reward for short-term gain
with smaller budgets B < B(cid:63) and longer-term gain with the unknown optimal budget B(cid:63).
In fact  we can further encourage our learned policy to be useful for smaller budgets by adjusting
the reward function. Note that two trajectories that start at s0 and end at st(cid:63) will have the same
reward  yet one trajectory might maintain much lower error rate than the other during the process
and therefore be more useful for smaller budgets. We therefore add a shaping component to the
expected reward in order to favor the more useful trajectory as follows:

Rα(s  a  s(cid:48)) = η(s) − η(s(cid:48)) − α [η(s(cid:48)) − η(s)]+ .

(8)
This modiﬁcation introduces a term that does not cancel when transitioning from one state to the
next  if the next state has higher error than our current state. Thus  we can only achieve optimal
reward η(s0) − η(st(cid:63) ) when there is a sequence of feature extractions that never increases the error
rate1; if such a sequence does not exist  then the parameter α controls the trade-off between the
importance of reaching st(cid:63) and minimizing any errors along the way. Note that we can still use the
procedure described above to learn β when using Rα instead of R. We use a development set to
tune α as well as γ to ﬁnd the most useful policy when sweeping B across a range of budgets.
Batch mode inference. At test time  we are typically given a test set of m examples  rather than
a single example. In this setting the budget applies to the entire inference process  and it may be
useful to spend more of the budget on difﬁcult examples rather than allocate the budget evenly across
all examples. In this case  we extend our framework to concatenate the states of all m examples
s = (x1  . . .   xm  z1  . . .   zm). The action consists of choosing an example and then choosing
an action within that example’s sub-state; our policy searches over the space of all actions for all
examples simultaneously. Because of this  we impose additional constraints on the action space 
speciﬁcally:

∀a(cid:48) < a.

z(a  . . . ) = 1 =⇒ z(a(cid:48)  . . . ) = 1 

(9)
Equation (9) states that there is an inherent ordering of feature extractions  such that we cannot
compute the a’th feature set without ﬁrst computing feature sets 1  . . .   a− 1. This greatly simpliﬁes
the search space in the batch setting while at the same time preserving enough ﬂexibility to yield
signiﬁcant improvements in efﬁciency.
Baselines. We compare to two baselines: a simply entropy-based approach and a more complex
imitation learning scheme (inspired by [7]) in which we learn a classiﬁer to reproduce a target
policy given by an oracle. The entropy-based approach simply computes probabilistic marginals
and extracts features for whichever portion of the output space has highest entropy in the predicted
distribution. For the imitation learning model  we use the same trajectories used to learn Q(cid:63)
π  but
instead we create a classiﬁcation dataset of positive and negative examples given a budget B by
assigning all state/action pairs along a trajectory within the budget as positive examples and all
budget violations as negative examples. We tune the budget B using a development set to optimize
the overall trade-off when the policy is evaluated with multiple budgets.

1While adding features decreases training error on average  even on the training set additional features may

lead to increased error for any particular example.

5

Feature
Tier (T )

4
3
2
1

Best

Error (%)

44.07
46.17
46.98
51.49
43.45

Fixed
16.20s
12.00s
5.50s
2.75s
—

Time (s)
Entropy Q-Learn
16.20s
8.10s
6.80s
—
—

8.91s
5.51s
4.86s
—

13.45s

Table 1: Trade-off between average elbow and wrist error rate and total runtime time achieved by our method
on the pose dataset; each row ﬁxes an error rate and determines the amount of time required by each method
to achieve the error. Unlike using entropy-based conﬁdence scores  our Q-learning approach always improves
runtime over a priori selection and even yields a faster model that is also more accurate (ﬁnal row).

4 Design of the information-adaptive predictor h
Learning. We now address the problem of learning h(x  z) from n labeled data points {(xj  yj}n
j=1.
Since we do not necessarily know the test-time budget during training (nor would we want to repeat
the training process for every possible budget)  we formulate the problem of minimizing the expected
training loss according to a uniform distribution over budgets:

w(cid:63) = argmin

λ||w||2 +

w

1
n

Ez[(cid:96)(yj  h(xj  z)].

(10)

n(cid:88)

j=1

Note that if (cid:96) is convex  then (10) is a weighted sum of convex functions and is also convex. Our
choice of distribution for z will determine how the predictor h is calibrated. In our experiments  we
sampled z’s uniformly at random. To learn w  we use Pegasos-style [17] stochastic sub-gradient de-
scent; we approximate the expectation in (10) by resampling z every time we pick up a new example
(xj  yj). We set λ and a stopping-time criterion through cross-validation onto a development set.
Feature design. We now turn to the question of designing f (x  y  z). In the standard pair-wise
graphical model setting (before considering z)  we decompose a feature function f (x  y) into unary
and pairwise features. We consider several different schemes of incorporating z of varying com-
plexity. The simplest scheme is to use several different feature functions f 1  . . .   f F . Then |z| = F  
and za = 1 indicates that f a is computed. Thus  we have the following expression  where we use
z(a) to indicate the a’th element of z:

F(cid:88)

a=1

(cid:88)

i∈V

(cid:88)

(i j)∈E



f (x  y  z) =

z(a)

f a
u (x  yi) +

f a
e (x  yi  yj)

(11)

Note that in practice we can choose each f a to be a sparse vector such that f a· f a(cid:48)
that is  each feature function f a “ﬁlls out” a complementary section of the feature vector f.
A much more powerful approach is to create a feature vector as the composite of different extracted
features for each vertex and edge in the model. In this setting  we set z = [zu ze]  where |z| =
(|V| + |E|)F   and we have

= 0 for all a(cid:48) (cid:54)= a;

(cid:88)

F(cid:88)

i∈V

a=1

(cid:88)

F(cid:88)

(i j)∈E

a=1

f (x  y  z) =

zu(a  i)f a

u (x  yi) +

ze(a  ij)f a

e (x  yi  yj).

(12)

We refer to this latter feature extraction method a factor-level feature extraction  and the former as
example-level.2
Reducing inference overhead. Feature computation time is only one component of the computa-
tional cost in making predictions; computing the argmax (1) given f can also be expensive. Note

2The restriction (9) also allows us to increase the complexity of the feature function f as follows; when
using the a’th extraction  we allow the model to re-weight the features from extractions 1 through a. In other
words  we condition the value of the feature on the current set of features that have been computed; since
there are only F sets in the restricted setting (and not 2F )  this is a feasible option. We simply deﬁne ˆf a =
[0 . . . f 1 . . . f a . . . 0]  where we add duplicates of features f 1 through f a for each feature block a. Thus 
the model can learn different weights for the same underlying features based on the current level of feature
extraction; we found that this was crucial for optimal performance.

6

Figure 2: Trade-off performance on the pose dataset for wrists (left) and elbows (right). The curve shows
the increase in accuracy over the minimal-feature model as a function of total runtime per frame (including
all overhead). We compare to two baselines that involve no learning: forward selection and extracting factor-
wise features based on the entropy of marginals at each position (“Entropy”). The learned policy results are
either greedy (“Greedy” example-level and factor-level) or non-myopic (either our “Q-learning” or the baseline
“Imitation”). Note that the example-wise method is far less effective than the factor-wise extraction strategy.
Furthermore  Q-learning in particular achieves higher accuracy models at a fraction of the computational cost
of using all features  and is more effective than imitation learning.

that for reasons of simplicity  we only consider low tree-width models in this work for which (1) can
be efﬁciently solved via a standard max-sum message-passing algorithm. Nonetheless  since φ(s  a)
requires access to h(x  z) then we must run message-passing every time we compute a new state s
in order to compute the next action. Therefore  we run message passing once and then perform less
expensive local updates using saved messages from the previous iteration. We deﬁne an simple al-
gorithm for such quiescent inference (given in the Supplemental material); we refer to this inference
scheme as q-inference. The intuition is that we stop propagating messages once the magnitude of
the update to the max-marginal decreases below a certain threshold q; we deﬁne q in terms of the
margin of the current MAP decoding at the given position  since that margin must be surpassed if
the MAP decoding will change as a result of inference.

5 Experiments

5.1 Tracking of human pose in video

Setup. For this problem  our goal is to predict the joint locations of human limbs in video clips
extracted from Hollywood movies. Our testbed is the MODEC+S model proposed in [22]; the
MODEC+S model uses the MODEC model of [15] to generate 32 proposed poses per frame of a
video sequence  and then combines the predictions using a linear-chain structured sequential pre-
diction model. There are four types of features used by MODEC+S  the ﬁnal and most expensive
of which is a coarse-to-ﬁne optical ﬂow [13]; we incrementally compute poses and features to mini-
mize the total runtime. For more details on the dataset/features  see [22]. We present cross validation
results averaged over 40 80/20 train/test splits of the dataset. We measure localization performance
or elbow and wrists in terms of percentage of times the predicted locations fall within 20 pixels of
the ground truth.
Meta-features. We deﬁne the meta-features φ(s  a) in terms of the targeted position in the sequence
i and the current predictions y(cid:63) = h(x  z). Speciﬁcally  we concatenate the already computed unary
and edge features of y(cid:63)
i and its neighbors (conditioned on the value of z at i)  the margin of the
current MAP decoding at position i  and a measure of self-consistency computed on y(cid:63) as follows.
For all sets of m frames overlapping with frame i  we extract color histograms for the predicted
arm segments and compute the maximum χ2-distance from the ﬁrst frame to any other frame; we
then also add an indicator feature each of these maximum distances exceeds 0.5  and repeat for
m = 2  . . .   5. We also add several bias terms for which sets of features have been extracted around
position i.

7

246810121416012345678Total runtime (s)∆ Accuracy (Wrist)Accuracy gained per Computation (Wrist)  Forward SelectionEntropy (Factor)Greedy (Example)Greedy (Factor)ImitationQ−Learning246810121416012345678Total runtime (s)∆ Accuracy (Elbow)Accuracy gained per Computation (Elbow)  Forward SelectionEntropy (Factor)Greedy (Example)Greedy (Factor)ImitationQ−LearningFigure 3: Controlling overhead on the OCR dataset. While our approach is is extremely efﬁcient in terms of
how many features are extracted (Left)  the additional overhead of inference is prohibitively expensive for the
OCR task without applying q-inference (Right) with a large threshold. Furthermore  although the example-wise
strategy is less efﬁcient in terms of features extracted  it is more efﬁcient in terms of overhead.
Discussion. We present a short summary of our pose results in Table 1  and compare to various
baselines in Figure 2. We found that our Q-learning approach is consistently more effective than all
baselines; Q-learning yields a model that is both more accurate and faster than the baseline model
trained with all features. Furthermore  while the feature extraction decisions of the Q-learning model
are signiﬁcantly correlated with the error of the starting predictions (ρ = 0.23)  the entropy-based
are not (ρ = 0.02)  indicating that our learned reward signal is much more informative.
5.2 Handwriting recognition

Setup. For this problem  we use the OCR dataset from [19]  which is pre-divided into 10 folds that
we use for cross validation. We use three sets of features: the original pixels (free)  and two sets
of Histogram-of-Gradient (HoG) features computed on the images for different bin sizes. Unlike
the pose setting  the features are very fast to compute compared to inference. Thus  we evaluate the
effectiveness of q-inference with various thresholds to minimize inference time. For meta-features 
we use the same construction as for pose  but instead of inter-frame χ2-distance we use a binary
indicator as to whether or not the speciﬁc m-gram occurred in the training set. The results are
summarized in Figure 3; see caption for details.
Discussion. Our method is extremely efﬁcient in terms of the features computed for h; however 
unlike the pose setting  the overhead of inference is on par with the feature computation. Thus  we
obtain a more accurate model with q = 0.5 that is 1.5× faster  even though it uses only 1/5 of the
features; if the implementation of inference were improved  we would expect a speedup much closer
to 5×.
6 Conclusion
We have introduced a framework for learning feature extraction policies and predictive models that
adaptively select features for extraction in a factor-wise  on-line fashion. On two tasks our approach
yields models that both more accurate and far more efﬁcient; our work is a signiﬁcant step towards
eliminating the feature extraction bottleneck in structured prediction. In the future  we intend to
extend this approach to apply to loopy model structures where inference is intractable  and more
importantly  to allow for features that change the structure of the underlying graph  so that the graph
structure can adapt to both the complexity of the input and the test-time computational budget.

Acknowledgements. The authors were partially supported by ONR MURI N000141010934  NSF
CAREER 1054215  and by STARnet  a Semiconductor Research Corporation program sponsored
by MARCO and DARPA.

8

00.20.40.60.811.21.41.61.80123456789Improvement (%)Additional Feature Cost (s)OCR Trade−off (Feature Only)  Single TierGreedy (Example)Q−learning (q = 0)Q−learning (q = 0.1)Q−learning (q = 0.5)00.511.522.533.544.550123456789Improvement (%)Additional Total Cost (s)OCR Trade−off (Feature+Overhead)  Single TierGreedy (Example)Q−learning (q = 0)Q−learning (q = 0.1)Q−learning (q = 0.5)References
[1] Y. Altun  I. Tsochantaridis  and T. Hofmann. Hidden Markov support vector machines. In Proc. ICML 

2003.

[2] M. Chen  Z. Xu  K.Q. Weinberg  O. Chapelle  and D. Kedem. Classiﬁer cascade for minimizing feature

evaluation cost. In AISATATS  2012.

[3] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with

perceptron algorithms. In Proc. EMNLP  2002.

[4] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In NIPS  2011.
[5] A. Grubb and D. Bagnell. Speedboost: Anytime prediction with uniform near-optimality. In AISTATS 

2012.

[6] H. He  H. Daum´e III  and J. Eisner. Imitation learning by coaching. In NIPS  2012.
[7] H. He  H. Daum´e III  and J. Eisner. Dynamic feature selection for dependency parsing. In EMNLP  2013.
Information value theory. Systems Science and Cybernetics  IEEE Transactions on 
[8] R. A Howard.

2(1):22–26  1966.

[9] Andreas Krause and Carlos Guestrin. Optimal value of information in graphical models. Journal of

Artiﬁcial Intelligence Research (JAIR)  35:557–591  2009.

[10] J.D. Lafferty  A. McCallum  and F.C.N. Pereira. Conditional random ﬁelds: Probabilistic models for

segmenting and labeling sequence data. In Proc. ICML  2001.

[11] M. Lagoudakis and R. Parr. Least-squares policy iteration. JMLR  2003.
[12] Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathe-

matical Statistics  pages 986–1005  1956.

[13] C. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. PhD thesis 

MIT  2009.

[14] V.C. Raykar  B. Krishnapuram  and S. Yu. Designing efﬁcient cascaded classiﬁers: tradeoff between

accuracy and cost. In SIGKDD  2010.

[15] B. Sapp and B. Taskar. MODEC: Multimodal decomposable models for human pose estimation.

CVPR  2013.

In

[16] B. Sapp  D. Weiss  and B. Taskar. Parsing human motion with stretchable models. In CVPR  2011.
[17] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In

ICML  2007.

[18] B. Taskar  V. Chatalbashev  D. Koller  and C. Guestrin. Learning structured prediction models: A large

margin approach. In ICML  2005.

[19] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In NIPS  2003.
[20] K. Trapeznikov and V. Saligrama. Supervised sequential classiﬁcation under budget constraints.

AISTATS  2013.

In

[21] C. Watkins and P. Dayan. Q-learning. Machine learning  1992.
[22] D. Weiss  B. Sapp  and B. Taskar. Dynamic structured model selection. In ICCV  2013.
[23] D. Weiss and B. Taskar. Structured prediction cascades. In AISTATS  2010.

9

,David Weiss
Ben Taskar