2009,Robust Nonparametric Regression with Metric-Space Valued Output,Motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including a robust median type estimator and the classical Frechet mean. Depending on the choice of the output space and the chosen metric the estimator reduces to partially well-known procedures for multi-class classification  multivariate regression in Euclidean space  regression with manifold-valued output and even some cases of structured output learning. In this paper we focus on the case of regression with manifold-valued input and output. We show pointwise and Bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimator with experiments.,Robust Nonparametric Regression with Metric-Space

valued Output

Department of Computer Science  Saarland University

Campus E1 1  66123 Saarbr¨ucken  Germany

Matthias Hein

hein@cs.uni-sb.de

Abstract

Motivated by recent developments in manifold-valued regression we propose a
family of nonparametric kernel-smoothing estimators with metric-space valued
output including several robust versions. Depending on the choice of the output
space and the metric the estimator reduces to partially well-known procedures for
multi-class classiﬁcation  multivariate regression in Euclidean space  regression
with manifold-valued output and even some cases of structured output learning.
In this paper we focus on the case of regression with manifold-valued input and
output. We show pointwise and Bayes consistency for all estimators in the family
for the case of manifold-valued output and illustrate the robustness properties of
the estimators with experiments.

1

Introduction

In recent years there has been an increasing interest in learning with output which differs from
the case of standard classiﬁcation and regression. The need for such approaches arises in several
applications which possess more structure than the standard scenarios can model.
In structured
output learning  see [1  2  3] and references therein  one generalizes multiclass classiﬁcation to
more general discrete output spaces  in particular incooperating structure of the joint input and
output space. These methods have been successfully applied in areas like computational biology 
natural language processing and information retrieval. On the other hand there has been a recent
series of work which generalizes regression with multivariate output to the case where the output
space is a Riemannian manifold  see [4  5  6  7]  with applications in signal processing  computer
vision  computer graphics and robotics. One can also see this branch as structured output learning
if one thinks of a Riemannian manifold as isometrically embedded in a Euclidean space. Then the
restriction that the output has to lie on the manifold can be interpreted as constrained regression in
Euclidean space  where the constraints couple several output features together.
In this paper we propose a family of kernel estimators for regression with metric-space valued input
and output motivated by estimators proposed in [6  8] for manifold-valued regression. We discuss
loss functions and the corresponding Bayesian decision theory for this general regression problem.
Moreover  we show that this family of estimators has several well known estimators as special
cases for certain choices of the output space and its metric. However  our main emphasis lies on
the problem of regression with manifold-valued input and output which includes the multivariate
Euclidean case. In particular  we show for all our proposed estimators their pointwise and Bayes
consistency  that is in the limit as the sample size goes to inﬁnity the estimated mapping converges
to the Bayes optimal mapping. This includes estimators implementing several robust loss functions
like the L1-loss  Huber loss or the ε-insensitive loss. This generality is possible since our proof
considers directly the functional which is minimized instead of its minimizer as it is usually done in
consistency proofs of the Nadaraya-Watson estimator. Finally  we conclude with a toy experiment
illustrating the robustness properties and difference of the estimators.

1

2 Bayesian decision theory and loss functions for metric-space valued output
We consider the structured output learning problem where the task is to learn a mapping φ : M → N
between two metric spaces M and N  where dM denotes the metric of M and dN the metric of N.
We assume that both metric spaces M and N are separable1. In general  we are in a statistical
setting where the given input/output pairs (Xi  Yi) are i.i.d. samples from a probability measure P
on M × N.
In order to prove later on consistency of our metric-space valued estimator we ﬁrst have to deﬁne the
Bayes optimal mapping φ∗ : M → N in the case where M and N are general metric spaces which
depends on the employed loss function. In multivariate regression the most common loss function
is  L(y  f(x)) = (cid:107)y − f(x)(cid:107)2
2. However  it is well known that this loss is sensitive to outliers. In
univariate regression one therefore uses the L1-loss or other robust loss functions like the Huber or ε-
insensitive loss. For the L1-loss the Bayes optimal function f∗ is given as f∗(x) = Med[Y |X = x] 
where Med denotes the median of P(Y |X = x) which is a robust location measure. Several general-
izations of the median for multivariate output have been proposed  see e.g. [9]. In this paper we refer
to the minimizer of the loss function L(y  f(x)) = (cid:107)y − f(x)(cid:107)Rn resp. L(y  f(x)) = dN (y  f(x))
as the (generalized) median  since this seems to be the only generalization of the univariate me-
dian which has a straightforward extension to metric spaces. In analogy to Euclidean case  we will
therefore use loss functions penalizing the distance between predicted output and desired output:

L(y  φ(x)) = Γ(cid:0)dN (y  φ(x))(cid:1) 

where Γ : R+ → R+. We will later on restrict Γ to a certain family of functions. The associated
Γ : M → N
risk (or expected loss) is: RΓ(φ) = E[L(Y  φ(X))] and its Bayes optimal mapping φ∗
can then be determined by

y ∈ N  x ∈ M 

E[Γ(cid:0)dN (Y  φ(X))(cid:1)]

φ∗
Γ :=

arg min

φ:M→N  φ measurable

=

arg min

φ:M→N  φ measurable

RΓ(φ) =

EX[EY |X[Γ(cid:0)dN (Y  φ(X))(cid:1)| X].

φ:M→N  φ measurable

arg min

(1)

φ : M → N so that E[Γ(cid:0)dN (Y  φ(X))(cid:1)] < ∞. This holds always once N has bounded diameter.

In the second step we used a result of [10] which states that a joint probability measure on the product
of two separable metric spaces can always be factorized into a conditional probability measure and
the marginal. In order that the risk is well-deﬁned  we assume that there exists a measurable mapping
Apart from the global risk RΓ(φ) we analyze for each x ∈ M the pointwise risk R(cid:48)

Γ(x  φ(x)) = EY |X[Γ(cid:0)dN (Y  φ(X))(cid:1)| X = x] 
E[Γ(cid:0)dN (Y  p)(cid:1)| X = x] = arg min

which measures the loss suffered by predicting φ(x) for the input x ∈ M. The total loss RΓ(φ) of
the mapping φ is then RΓ(φ) = E[R(cid:48)
Γ(X  φ(X))]. As in standard regression the factorization allows
to ﬁnd the Bayes optimal mapping φ∗ pointwise 
φ∗
Γ(x) = arg min

Γ(cid:0)dN (y  p)(cid:1) dµx(y) 

R(cid:48)
Γ(x  p) = arg min

Γ(x  φ(x)) 

(cid:90)

R(cid:48)

p∈N

p∈N

p∈N

N

where dµx is the conditional probability of Y conditioned on X = x. Later on we prove consistency
for a set of kernel estimators each using a different loss function Γ from the following class of
functions.
Deﬁnition 1 A convex function Γ : R+ → R+ is said to be (α  s)-bounded if

• Γ : R+ → R+ is continuously differentiable  monotonically increasing and Γ(0) = 0 
• Γ(2x) ≤ α Γ(x) for x ≥ s and Γ(s) > 0 and Γ(cid:48)(s) > 0.

Several functions Γ corresponding to standard loss functions in regression are (α  s)-bounded:

• Lp-type loss: Γ(x) = xγ for γ ≥ 1 is (2γ  1)-bounded 
• Huber-loss: Γ(x) = 2x2
2 and Γ(x) = 2x − ε
1A metric space is separable if it contains a countable dense subset.

for x ≤ ε

ε

2 for x > ε

2 is (3  ε

2)-bounded.

2

• ε-insensitive loss: Γ(x) = 0 for x ≤ ε and Γ(x) = x − ε if x > ε is (3  2ε)-bounded.

Γ(x ·) cannot be guaranteed
While uniqueness of the minimizer of the pointwise loss functional R(cid:48)
Γ(x ·) has
anymore in the case of metric space valued output  the following lemma shows that R(cid:48)
reasonable properties (all longer proofs can be found in Section 7 or in the supplementary material).
It generalizes a result provided in [11] for Γ(x) = x2 to all (α  s)-bounded losses.
Lemma 1 Let N be a complete and separable metric space such that d(x  y) < ∞ for all x  y ∈ N
Γ(x  q) < ∞ for some
and every closed and bounded set is compact. If Γ is (α  s)-bounded and R(cid:48)
q ∈ N  then
• R(cid:48)
• R(cid:48)
• The set of minimizers Q∗ = arg min

Γ(x  p) < ∞ for all p ∈ N 
Γ(x ·) is continuous on N 

R(cid:48)
Γ(x  q) exists and is compact.

q∈N

It is interesting to have a look at one special loss  the case Γ(x) = x2. The minimizer of the
pointwise risk 

(cid:90)

F (p) = arg min

p∈N

N

N (y  p) dµx(y) 
d2

is called the Frech´et mean2 or Karcher mean in the case where N is a manifold. It is the generaliza-
tion of a mean in Euclidean space to a general metric space. Unfortunately  it needs to be no longer
unique as in the Euclidean case. A simple example is the sphere as the output space together with
a uniform probability measure on it. In this case every point p on the sphere attains the same value
F (p) and thus the global minimum is non-unique. We refer to [12  13  11] for more information
under which conditions one can prove uniqueness of the global minimizer if N is a Riemannian
manifold. The generalization of the median to Riemannian manifolds  that is Γ(x) = x  is discussed
in [9  4  8]. For a discussion of the computation of the median in general metric spaces see [14].

3 A family of kernel estimators with metric-space valued input and output

In the following we provide the deﬁnition of the kernel estimator with metric-space valued out-
put motivated by the two estimators proposed in [6  8] for manifold-valued output. We use in the
following the notation kh(x) = 1

hm k(x/h).

i=1 be the sample with Xi ∈ M and Yi ∈ N. The metric-space-valued
Deﬁnition 2 Let (Xi  Yi)l
kernel estimator φl : M → N from metric space M to metric space N is deﬁned for all x ∈ M as

Γ(cid:0)dN (q  Yi)(cid:1) kh

(cid:0)dM (x  Xi)(cid:1) 

l(cid:88)

i=1

1
l

φl(x) = arg min

q∈N

(cid:0)dM (x  Xi)(cid:1) is to measure the similarity between x and Xi in M which should decrease as the

where Γ : R+ → R+ is (α  s)-bounded and k : R+ → R+.
If the data contains a large fraction of outliers one should use a robust loss function Γ  see Sec-
tion 6. Usually the kernel function should be monotonically decreasing since the interpretation of
kh
distance increases. The computational complexity to determine φl(x) is quite high as for each test
point one has to solve an optimization problem but comparable to structured output learning (see
discussion below) where one maximizes for each test point the score function over the output space.
For manifold-valued output we will describe in the next section a simple gradient-descent type opti-
mization scheme in order to determine φl(x).
It is interesting to see that several well-known nonparametric estimators for classiﬁcation and re-
gression can be seen as special cases of this estimator (or a slightly more general form) for different
choices of the output space  its metric and the loss function. In particular  the approach shows a cer-
tain analogy of a generalization of regression into a continuous space (manifold-valued regression)
and regression into a discrete space (structured output learning).

2In some cases the set of all local minimizers is denoted as the Frech´et mean set and the Frech´et mean is

called unique if there exists only one global minimizer.

3

(2)

Multiclass classiﬁcation: Let N = {1  . . .   K} where K denotes the number of classes K. If
there is no special class-structure  then we use the discrete metric on N  dN (q  q(cid:48)) = 1 if q (cid:54)= q(cid:48) and
0 else leads for any Γ to the standard multiclass classiﬁcation scheme using a majority vote. Cost-
sensitive multiclass classiﬁcation can be done by using dN (q  q(cid:48)) to model the cost of misclassifying
class q by class q(cid:48). Since general costs can generally not be modeled by a metric  it should be noted
that the estimator can be modiﬁed using a similarity function  s : N × N → R 

φl(x) = arg max

q∈N

1
l

s(cid:0)q  Yi

(cid:1) kh

(cid:0)dM (x  Xi)(cid:1) 

l(cid:88)

i=1

The consistency result below can be generalized to this case given that N has ﬁnite cardinality.

Multivariate regression: Let N = Rn and M be a metric space. Then for Γ(x) = x2  one gets

1
l

(cid:107)q − Yi(cid:107)2 kh

(cid:0)dM (x  Xi)(cid:1) 

l(cid:88)
(cid:0)dM (x Xi)(cid:1)Yi
(cid:0)dM (x Xi)(cid:1) . This is the well-known Nadaraya-Watson

i=1

q∈N

φl(x) = arg min
(cid:80)l
(cid:80)l

1
l

which has the solution  φl(x) =
estimator  see [15  16]  on a metric space. In [17] a related estimator is discussed when M is a closed
Riemannian manifold and [18] discusses the Nadaraya-Watson estimator when M is a metric space.

1
l

i=1 kh
i=1 kh

Manifold-valued regression:
In [6] the estimator φl(x) has been proposed for the case where N is
a Riemannian manifold and Γ(x) = x2  in particular with the emphasis on N being the manifold of
shapes. The discussion of a robust median-type estimator  that is Γ(x) = x  has been done recently
in [8]. While it has been shown in [7] that an approach using a global smoothness regularizer
outperforms the estimator φl(x)  it is a well working baseline with a simple implementation  see
Section 4.

lated using kernels k(cid:0)(x1  q1)  (x2  q2)(cid:1) on the product M × N of input and output space  which are

Structured output: Structured output learning  see [1  2  3] and references therein  can be formu-

supposed to measure jointly the similarity and thus can capture non-trivial dependencies between
input and output. Using such kernels [1  2  3] learn a score function s : M × N → R  with

Ψ(x) = arg max

s(x  q).

being the ﬁnal prediction for x ∈ M. The similarity to our estimator φl(x) in (2) becomes more
obvious when we use that in the framework of [1] the learned score function can be written as

Ψl(x) = arg max

q∈N

1
l

αi k(cid:0)(x  q)  (Xi  Yi)(cid:1) 

where α ∈ Rl is the learned coefﬁcient vector. Apart from the coefﬁcient vector α this has almost
the form of the previously discussed estimator in Equation (3)  using a joint similarity function on
input and output space. Clearly  a structured output method where the coefﬁcients α have been
optimized  should perform better than αi = const. In cases where training time is prohibitive the
estimator without α is an alternative  at least it provides a useful baseline for structured output

learning. Moreover  if the joint kernel factorizes  k(cid:0)(x1  q1)  (x2  q2)(cid:1) = kM (x1  x2) kN (q1  q2) on

M and N  and kN (q  q) = const.  then one can rewrite the problem in (4) as 

(3)

(4)

q∈N

l(cid:88)

i=1

l(cid:88)

i=1

Ψl(x) = arg min

q∈N

1
l

αi kM (x  Xi)d2

N (q  Yi) 

where dN is the induced (semi)-metric3 of kN . Apart from the learned coefﬁcients this is basically
equivalent to φl(x) in (2) for Γ(x) = x2.
In the following we restrict ourselves to the case where M and N are Riemannian manifolds. In this
case the optimization to obtain φl(x) can still be done very efﬁciently as the next section shows.
N (p  q) = kN (p  p) + kN (q  q) − 2kN (p  q).

3The kernel kN induces a (semi)-metric dN on N via: d2

4

l(cid:88)

wi Γ(cid:0)dN (q  Yi)(cid:1).

Implementation of the kernel estimator for manifold-valued output

4
For ﬁxed x ∈ M  the functional F (q) for q ∈ N which is optimized in the kernel estimator φl(x)
can be rewritten with wi = kh(dM (x  Xi)) as 

The covariant gradient of F (q) is given as  ∇F(cid:12)(cid:12)q =(cid:80)l

F (q) =

i=1

i=1 wiΓ(cid:48)(cid:0)dN (p  Yi)(cid:1) vi  where vi ∈ TqN is

a tangent vector at q with (cid:107)vi(cid:107)TqN = 1 given by the tangent vector at q of the minimizing4 geodesic
from Yi to q (pointing “away” from Yi). Denoting by expq : TqN → N the exponential map at q 
the simple gradient descent based optimization scheme can be written as

• choose a random point q0 from N 
• while stopping criteria not fulﬁlled 

1. compute gradient ∇F at qk
2. one has: qk+1 = expqk
3. determine stepsize α by Armijo rule [19].

(cid:0) − α∇F|qk

(cid:1)

As stopping criterion we use either the norm of the gradient or a threshold on the change of F . For
the experiments in Section 6 we get convergence in 5 to 40 steps.

5 Consistency of the kernel estimator for manifold-valued input and output

In this section we show the pointwise and Bayes consistency of the kernel estimator φl in the case
where M and N are Riemannian manifolds. This case already subsumes several of the interesting
applications discussed in [6  8]. The proof of consistency of the general metric-space valued kernel
estimator (for a restricted class of metric spaces including all Riemannian manifolds) requires high
technical overload which is interesting in itself but which would make the paper hard accessible.
The consistency of φl will be proven under the following assumptions:

Assumptions (A1):

i=1 is an i.i.d. sample of P on M × N 

1. The loss Γ : R+ → R+ is (α  s)-bounded.
2. (Xi  Yi)l
3. M and N are compact m-and n-dimensional manifolds 
4. The data-generating measure P on M × N is absolutely continuous with respect to the
5. The marginal density on M fulﬁlls: p(x) ≥ pmin  ∀ x ∈ M 
6. The density p(·  y) is continuous on M for all y ∈ N 

7. The kernel fulﬁlls: a 1s≤r1 ≤ k(s) ≤ b e−γ s2 and(cid:82)

Rm (cid:107)x(cid:107) k((cid:107)x(cid:107)) dx < ∞ 

natural volume element 

Note  that existence of a density is not necessary for consistency. However  in order to keep the
det g dx denotes the
proofs simple  we restrict ourselves to this setting. In the following dV =
natural volume element of a Riemannian manifold with metric g  vol(S) and diam(N) are the
volume and diameter of the set S. For the proof of our main theorem we need the following two
propositions. The ﬁrst one summarizes two results from [20].
Proposition 1 Let M be a compact m-dimensional Riemannian manifold. Then  there exists r0 > 0
and S1  S2 > 0 such that for all x ∈ M the volume of the balls B(x  r) with radius r ≤ r0 satisﬁes 

S1 rm ≤ vol(cid:0)B(x  r)(cid:1) ≤ S2 rm.

√

Moreover  the cardinality K of a δ-covering of M is upper bounded as  K ≤ vol(N )

S1

(cid:17)m

(cid:16) 2

δ

.

4The set of points where there the minimizing geodesic is not unique  the so called cut locus  has measure

zero and therefore plays no role in the optimization.

5

Moreover  we need a result about convolutions on manifolds.
Proposition 2 Let the assumptions A1 hold  then if f is continuous we get for any x ∈ M\∂M 

kh(dM (x  z))f(z) dV (z) = Cxf(x) 

(cid:90)

lim
h→0

M

(cid:82)

(cid:90)

where Cx = limh→0
Lipschitz constant L  then there exists a h0 > 0 such that for all h < h0(x) 

M kh(dM (x  z)) dV (z) > 0.

If moreover f is Lipschitz continuous with

kh(dM (x  z))f(z) dV (z) = Cx f(x) + O(h).

M

The following main theorem proves the almost sure pointwise convergence of the manifold-valued
kernel estimator for all (α  s)-bounded loss functions Γ.

Theorem 1 Suppose the assumptions in A1 hold. Let φl(x) be the estimate of the kernel estimator
for sample size l. If h → 0 and lhm/ log l → ∞  then for any x ∈ M\∂M 

Γ(x  q)| = 0 
R(cid:48)
If additionally p(·  y) is Lipschitz-continuous for any y ∈ N  then

Γ(x  φl(x)) − arg min
q∈N

l→∞|R(cid:48)
lim

Γ(x  q)| = O(h) + O(cid:0)(cid:112)log l/(l hm)(cid:1) 

R(cid:48)

l→∞|R(cid:48)
lim

Γ(x  φl(x)) − arg min
q∈N

The optimal rate is given by h = O(cid:0)(log l/l) 1

2+m(cid:1) so that

almost surely.

almost surely.

(cid:16)(cid:0) log l/l(cid:1) 1
2+m(cid:17)

 

almost surely.

l→∞ R(cid:48)
lim

Γ(x  φl(x)) − arg min
q∈N

R(cid:48)
Γ(x  q) = O

Note  that the condition l hm/ log l → ∞ for convergence is the same as for the Nadaraya-Watson
estimator on a m-dimensional Euclidean space. This had to be expected as this condition still holds
if one considers multivariate output  see [15  16]. Thus  doing regression with manifold-valued
output is not more “difﬁcult” than standard regression with multivariate output.
Next  we show Bayes consistency of the manifold-valued kernel estimator.
Theorem 2 Let the assumptions A1 hold. If h → 0 and lhm/ log l → ∞  then

l→∞ RΓ(φl) − RΓ(φ∗) = 0 
lim
Proof: We have  RΓ(φl) − RΓ(φ∗) ≤ E[|R(cid:48)
we have almost everywhere 
E[R(cid:48)
theorem proven by Glick  see [21]  provides the result.

Γ(X  φ(X))] < ∞ and E[R(cid:48)

Γ(X  φl(X)) − R(cid:48)

Γ(X  φ∗(X))|]. Moreover 
liml→∞ R(cid:48)
Since
Γ(X  φ∗(X))] < ∞  an extension of the dominated convergence
(cid:3)

Γ(x  φ∗(x)) almost surely.

Γ(x  φl(x)) = R(cid:48)

almost surely.

6 Experiments

k(cid:0)|x − y|/h(cid:1) = 1 − |x − y|/h. The parameter h was found by 5-fold cross validation from the set

We illustrate the differences of median and mean type estimator on a synthetic dataset with the task
of estimating a curve on the sphere  that is M = [0  1] and N = S1. The kernel used had the form 
[5  10  20  40] ∗ 10−3. The results are summarized for different levels of outliers and different levels
of van-Mises noise (note that the parameter k is inverse to the variance of the distribution) in Table
1. As expected the the L1-loss and the Huber loss as robust loss functions outperform the L2-loss
in the presence of outliers  whereas the L2-loss outperforms the robust versions when no outliers
are present. Note  that the Huber loss as a hybrid version between L1- and L2-loss is even slightly
better than the L1-loss in the presence of outliers as well as in the outlier free case. Thus for a given
dataset it makes sense not only to do cross-validation of the parameter h of the kernel function but
also over different loss functions in order to adapt to possible outliers in the data.

6

Figure 1: Regression problem on the sphere with 1000 training points (black points). The blue
points are the ground truth disturbed by van Mises noise with parameter k = 100 and 20% (outliers)
with k = 3. The estimated curves are shown in green. Left: Result of L1-loss  mean error (ME)
0.256  mean squared error (MSE) 0.165. Middle: Result of L2-loss: ME = 0.265  MSE = 0.169.
Right: Result of Huber loss with ε = 0.1: ME = 0.255  MSE = 0.165. In particular  the curves
found using L1 and Huber loss are very close to the ground truth.

Table 1: Mean squared error (unit 10−1) for regression on the sphere - for different noise levels k 
number of labeled points  without and with outliers. Results are averaged over 10 runs.
20% outliers
500

Number of samples
2.1 ± 0.2 1.57 ± 0.05
0.63 ± 0.11
k = 100
k = 1000 0.43 ± 0.12
2.1 ± 0.5
1.45 ± 0.03
k = 100 0.43 ± 0.10 0.230 ± 0.007 0.208 ± 0.001 2.0 ± 0.2 1.59 ± 0.02
k = 1000 0.28 ± 0.16 0.032 ± 0.003 0.025 ± 0.001 2.0 ± 0.4 1.51 ± 0.03
k = 100

L1-Loss
Γ(x) = x
L2-Loss
Γ(x) = x2
0.61 ± 0.11
Huber-Loss
with ε = 0.1 k = 1000 0.42 ± 0.12

1.521 ± 0.015
1.400 ± 0.008
1.549 ± 0.021
1.447 ± 0.015
2.1 ± 0.2 1.57 ± 0.05 1.520 ± 0.021
2.1 ± 0.5 1.44 ± 0.02 1.397 ± 0.008

no outliers
500
0.260 ± 0.027
0.043 ± 0.005

0.257 ± 0.026
0.040 ± 0.005

0.219 ± 0.003
0.030 ± 0.001

0.218 ± 0.003
0.028 ± 0.001

1000

1000

100

100

7 Proofs
Lemma 2 Let φ : R+ → R be convex  differentiable and monotonically increasing. Then
min{φ(cid:48)(x)  φ(cid:48)(y)}|y − x| ≤ |φ(y) − φ(x)| ≤ max{φ(cid:48)(x)  φ(cid:48)(y)}|y − x|.

Proof of Theorem 1 We deﬁne R(cid:48)
arg min

. Note that φl(x) =
R(cid:48)
Γ l(x  q) as we have only divided by a constant factor. We use the standard technique for

i=1 Γ(dN (q Yi)) kh(dM (x Xi))

E[kh(dM (x X))]

Γ l(x  q) =

1
l

q∈N
the pointwise estimate 
Γ(x  φl(x)) − min
Γ(x  q) ≤ R(cid:48)
R(cid:48)
R(cid:48)
q∈N

Γ(x  φl(x)) − R(cid:48)

Γ l(x  φl(x)) + R(cid:48)

Γ l(x  φl(x)) − min
q∈N

R(cid:48)
Γ(x  q)

≤ 2 sup
q∈N

|R(cid:48)

Γ l(x  q) − R(cid:48)

Γ(x  q)|.

l

(cid:12)(cid:12) 1

In order

i=1 kh(dM (x Xi))

E[kh(dM (x X))] − 1(cid:12)(cid:12) < 1
(cid:80)l
(cid:16) 2
(cid:17)n
Moreover  we assume to have a δ-covering of N with centers Nδ = {qα}K
1 we have K ≤ vol(N )
Introducing RE

to bound the supremum  we will work on the event E  where we assume 
2  which holds with probability 1 − 2 e−C l hm for some constant C.
α=1 where using Lemma
. Thus for each q ∈ N there exists qα ∈ Nδ such that dN (q  qα) ≤ δ.
E[kh(dM (x X))]

S1

δ

we have to control four terms 

Γ (x  q) = E[Γ(dN (q Y ))kh(dM (x X))]
Γ l(x  q) − R(cid:48)
Γ l(x  q) − R(cid:48)
Γ(x  q) =R(cid:48)
R(cid:48)
+ RE
(cid:80)l

Γ l(x  qα)(cid:12)(cid:12) =
≤ 2 dN (q  qα) Γ(cid:48)(cid:0) diam(N)(cid:1) 1

(cid:12)(cid:12)(cid:12)(cid:12) 1

i=1

l

l

Γ l(x  q)−R(cid:48)

Γ (x  qα) − RE

and using the decomposition 
Γ l(x  qα) − RE
Γ l(x  qα) + R(cid:48)
Γ (x  q) − R(cid:48)
Γ (x  q) + RE

Γ (x  qα)
Γ(x  q) 

(cid:0)Γ(cid:0)dN (q  Yi)(cid:1) − Γ(cid:0)dN (qα  Yi)(cid:1)(cid:1)kh(dM (x  Xi))
(cid:12)(cid:12)(cid:12)(cid:12)
≤ 3 Γ(cid:48)(cid:0) diam(N)(cid:1) δ.

E[kh(dM (x  X))]
i=1 kh(dM (x  Xi))
E[kh(dM (x  X))]

(cid:80)l

(cid:12)(cid:12)R(cid:48)

(cid:80)l

7

where we have used Lemma 2 and the fact that E holds. Then  there exists a constant C such that

(cid:16)

P

max
1≤α≤K

|R(cid:48)

vol(N)

(cid:17)n

Γ (x  qα)| > ε

(cid:17) ≤ 2

Γ l(x  qα) − RE

(cid:16)2
(cid:80)l
S1
i=1 Wi − E[Wi] where Wi =
together with a union bound over the elements in the covering Nδ using
  Var Wi ≤ Γ(diam(N))2E[k2

h(dM (x  X))]

e−C l hmε2

δ

 

 

≤ b
a

Γ(diam(N))2
hmS1rm
1 pmin

(E[kh(dM (x  X))])2

which can be shown using Bernstein’s inequality for 1
l
Γ(dN (qα Yi))kh(dM (x Xi))

E[kh(dM (x X))]
|Wi| ≤ b
a

Γ(diam(N))
hmS1rm
1 pmin

where we used Proposition 1 to lower bound vol(B(x  h r1)) for small enough h. Third  we get for
the third term using again Lemma 2 

|RE

Γ (x  qα) − RE

Γ (x  q)| ≤ 2Γ(cid:48)(diam(N))dN (q  qα) ≤ 2Γ(cid:48)(diam(N))δ.

Last  we have to bound the approximation error RE
tion on the joint density p(x  y) we can use Proposition 2. For every x ∈ M\∂M we get 

Γ(x  q)  Under the continuity assump-

Γ (x  q)− R(cid:48)
(cid:90)
(cid:90)

lim
h→0

M

kh(dM (x  z))p(z)dV (z) = Cxp(x) 

kh(dM (x  z))p(z)dV (z) 

kh(dM (x  z))p(z  y)dV (z) = Cx p(x  y) 

lim
h→0
where Cx > 0. Thus with

M

(cid:90)

(cid:90)

M

fh =

kh(dM (x  z))p(z  y)dV (z) 

gh =

we get for every x ∈ M\∂M 

(cid:12)(cid:12)(cid:12) fh

gh

(cid:12)(cid:12)(cid:12) ≤ lim

h→0

− f
g

lim
h→0

M

+ lim
h→0

f

|fh − f|

gh

|gh − g|
g gh

= 0 

RE

lim
h→0

where we have used gh ≥ aS1r1pmin > 0 and g = Cxp(x) > 0. Moreover  using results from
the proof of Proposition 2 one can show fh < C for some constant C. Thus fh/gh < C for some
constant and fh/gh → f /g as h → 0. Using the dominated convergence theorem we thus get
p(x) dy = R(cid:48)
For the case where the joint density is Lipschitz continuous one gets using Proposition 2  RE
R(cid:48)
Γ(x  q) + O(h).
In total  there exist constants A  B  C  D1  D2  such that for sufﬁciently small h one has with prob-
ability 1 − AeB n log( 1

Γ(cid:0)dN (q  y)(cid:1) p(x  y)

E[Γ(dN (q  Y ))kh(dM (x  X))]

E[kh(dM (x  X))]

Γ (x  q) = lim
h→0

Γ (x  q) =

Γ(x  q).

(cid:90)

=

N

δ )−Clhmε2 
sup
q∈N

|R(cid:48)

Γ l(x  q) − RE

Γ (x  q)| ≤ 2D1δ + ε.

Γ (x  q) = R(cid:48)

Γ(x  q).
Γ (x  q) = R(cid:48)

log l → ∞ together with
With δ = l−s for some s > 0 one gets convergence if
For the case where p(·  y) is Lipschitz continuous for all
limh→0 RE
y ∈ N we have RE
Γ(x  q) + O(h) and can choose s large enough so that the bound
log l → ∞ the
from the approximation error dominates the one of the covering. Under the condition lhm
probabilistic bound is summable in l which yields almost sure convergence by the Borel-Cantelli-
Lemma. The optimal rate in the Lipschitz continuous case is then determined by ﬁxing h such that
(cid:3)
both terms of the bound are of the same order.

lhm

Acknowledgments

We thank Florian Steinke for helpful discussions about relations between generalized kernel esti-
mators and structured output learning. This work has been partially supported by the Cluster of
Excellence MMCI at Saarland University.

8

References
[1] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured

and interdependent output variables. JMLR  6:1453–1484  2005.

[2] J. Weston  G. BakIr  O. Bousquet  B. Sch¨olkopf  T. Mann  and W. S. Noble. Joint kernel maps.

In Predicting Structured Data  pages 67–84. MIT Press  2007.

[3] E. Ricci  T. De Bie  and N. Cristianini. Magic moments for structured output prediction. JMLR 

9:2803–2846  2008.

[4] K.V. Mardia and P.E. Jupp. Directional statistics. Wiley New York  2000.
[5] Inam Ur Rahman  Iddo Drori  Victoria C. Stodden  David L. Donoho  and Peter Schroder.
Multiscale representations for manifold-valued data. Multiscale Modeling and Simulation 
4(4):1201–1232  2005.

[6] B. C. Davis  P. T. Fletcher  E. Bullitt  and S. Joshi. Population shape regression from random
design data. Computer Vision  2007. ICCV 2007. IEEE 11th International Conference on 
pages 1–7  2007.

[7] F. Steinke and M. Hein. Non-parametric regression between Riemannian manifolds. In Ad-

vances in Neural Information Processing Systems (NIPS) 21  pages 1561 – 1568  2009.

[8] P. T. Fletcher  S. Venkatasubramanian  and S. Joshi. The geometric median on Riemannian

manifolds with application to robust atlas estimation. NeuroImage  45:143 – 152  2009.

[9] C. G. Small. A survey of multidimensional medians. International Statistical Review  58:263–

277  1990.

[10] D. Blackwell and M. Maitra. Factorization of probability measures and absolutely measurable

sets. Proc. Amer. Math. Soc.  92(2):251–254  1984.

[11] R. Bhattacharya and V. Patrangenaru. Large sample theory of intrinsic and extrinsic sample

means on manifolds I. Ann. Stat.  31(1):1–29  2003.

[12] H. Karcher. Riemannian center of mass and molliﬁer smoothing. Communications on Pure

and Applied Mathematics  30:509–541  1977.

[13] W. Kendall. Probability  convexity  and harmonic maps with small image. I. Uniqueness and

ﬁne existence. Proc. London Math. Soc.  61(2):371–406  1990.

[14] P. Indyk. Sublinear time algorithms for metric space problems. In Proceedings of the 31st

Symposium on Theory of computing (STOC)  pages 428 – 434  1999.

[15] L. Gy¨orﬁ  M. Kohler  A. Krzy˙zak  and H. Walk. A Distribution-Free Theory of Nonparametric

Regression. Springer  New York  2004.

[16] W. Greblicki and M. Pawlak. Nonparametric System Identiﬁcation. Cambridge University

Press  Cambrige  2008.

[17] B. Pelletier. Nonparametric regression estimation on closed Riemannian manifolds. J. of

Nonparametric Stat.  18:57–67  2006.

[18] S. Dabo-Niang and N. Rhomari. Estimation non parametrique de la regression avec variable

explicative dans un espace metrique. C. R. Math. Acad. Sci. Paris  1:75–80  2003.

[19] D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc  Belmont  Mass.  1999.
[20] M. Hein. Uniform convergence of adaptive graph-based regularization.

In G. Lugosi and
H. Simon  editors  Proc. of the 19th Conf. on Learning Theory (COLT)  pages 50–64  Berlin 
2006. Springer.

[21] N. Glick. Consistency conditions for probability estimators and integrals of density estimators.

Utilitas Math.  6:61–74  1974.

9

,Xinyang Yi
Constantine Caramanis