2016,Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis,A spectral analysis of the Koopman operator  which is an infinite dimensional linear operator on an observable  gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper  we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes  and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore  we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data.,Dynamic Mode Decomposition with Reproducing

Kernels for Koopman Spectral Analysis

Yoshinobu Kawaharaab

a The Institute of Scientiﬁc and Industrial Research  Osaka University

b Center for Advanced Integrated Intelligence Research  RIKEN

ykawahara@sanken.osaka-u.ac.jp

Abstract

A spectral analysis of the Koopman operator  which is an inﬁnite dimensional lin-
ear operator on an observable  gives a (modal) description of the global behavior
of a nonlinear dynamical system without any explicit prior knowledge of its gov-
erning equations. In this paper  we consider a spectral analysis of the Koopman
operator in a reproducing kernel Hilbert space (RKHS). We propose a modal de-
composition algorithm to perform the analysis using ﬁnite-length data sequences
generated from a nonlinear system. The algorithm is in essence reduced to the
calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the
eigendecomposition of the projection of the Koopman operator onto the subspace
spanned by the bases. The algorithm returns a decomposition of the dynamics
into a ﬁnite number of modes  and thus it can be thought of as a feature extraction
procedure for a nonlinear dynamical system. Therefore  we further consider appli-
cations in machine learning using extracted features with the presented analysis.
We illustrate the method on the applications using synthetic and real-world data.

1 Introduction

Modeling nonlinear dynamical systems using data is fundamental in a variety of engineering and
scientiﬁc ﬁelds. In machine learning  the problem of learning dynamical systems has been actively
discussed  and several Bayesian approaches have been proposed [11  34]. In the ﬁelds of physics 
one popular approach for this purpose is the decomposition methods that factorize the dynamics into
modes based on some criterion from the data. For example  proper orthogonal decomposition (POD)
(see  for example  [12])  which generates orthogonal modes that optimally capture the vector energy
of a given dataset  has been extensively applied to complex phenomena in physics [5  22] even
though this method is currently known to have several drawbacks. The so-called spectral method
for dynamical systems [15  31  17]  which is often discussed in machine learning  is closely related
to this type of technique  where one aims to estimate a prediction model rather than understand the
dynamics by examining the obtained modes.
Among the decomposition techniques  dynamic mode decomposition (DMD) [25  26] has recently
attracted attention in the ﬁeld of physics  such as ﬂow mechanics  and in engineering  and has been
applied to data obtained from complex phenomena [2  4  6  10  21  25  27  32]. DMD approximates
the spectra of the Koopman operator [16]  which is an inﬁnite-dimensional linear operator that rep-
resents nonlinear and ﬁnite-dimensional dynamics without linearization. While POD just ﬁnds the
principal directions in a dataset  DMD can yield direct information concerning the dynamics such
as growth rates and the frequencies of the dynamics.
In this paper  we consider a spectral analysis of the Koopman operator in reproducing kernel Hilbert
spaces (RKHSs) for a nonlinear dynamical system
(1)
where x2M is the state vector on a ﬁnite-dimensional manifold M(cid:18) Rd  and f is a (possibly 
nonlinear) state-transition function. We present a modal decomposition algorithm to perform this 

xt+1 = f (xt);

1

which is in principle reduced to the calculation of a set of orthogonal bases for the Krylov matrix
in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace
spanned by the bases. Although existing DMD algorithms can conceptually be thought of as produc-
ing an approximation of the eigenfunctions of the Koopman operator using a set of linear monomials
of observables (or the pre-determined functional maps of observables) as basis functions  which is
analogous to a one-term Taylor expansion at each point  our algorithm gives an approximation with
a set of nonlinear basis functions due to the expressiveness of kernel functions. The proposed algo-
rithm provides a modal decomposition of the dynamics into a ﬁnite number of modes  and thus it
could be considered as a feature extraction procedure for a nonlinear dynamical system. Therefore 
we consider applications using extracted features from our analysis such as state prediction  sequen-
tial change-point detection  and dynamics recognition. We illustrate our method on the applications
using synthetic and real-world data.
The remainder of this paper is organized as follows. In Section 2  we brieﬂy review the spectral anal-
ysis of nonlinear dynamical systems with the Koopman operator and DMD. In Section 3  we extend
the analysis with reproducing kernels  and provide a modal decomposition algorithm to perform this
analysis based on the equivalent principle of DMD. Although this method is mathematically correct 
a practical implementation could yield an ill-conditioned algorithm. Therefore  in Section 4  we
describe a way to robustly it by projecting data onto the POD directions. In Section 5  we describe
related works. In Section 6  we show some empirical examples by the proposed algorithm and  in
Section 7  we describe several applications using extracted features with empirical results. Finally 
we conclude the paper in Section 8.

2 The Koopman Operator and Dynamic Mode Decomposition

(Kgi)(x) = gi ◦ f (x);

Consider a discrete-time nonlinear dynamical system (1). The Koopman operator [16]  which we
denote here by K  is an inﬁnite-dimensional linear operator that acts on a scalar function gi : M! C 
mapping gi to a new function Kgi given as follows:
(2)
where ◦ denotes the composition of gi with f. We see that K acts linearly on the function gi  even
though the dynamics deﬁned by f may be nonlinear. Since K is a linear operator  it has  in general 
an eigendecomposition
(3)
where (cid:21)j 2 C is the j-th eigenvalue (called the Koopman eigenvalue) and φj is the corresponding
eigenfunction (called the Koopman eigenfunction). We denote the concatenation of gi as g :=
⊤. If each gi lies within the span of the eigenfunctions φj  we can expand the vector-
[g1; : : : ; gp]
valued g in terms of these eigenfunctions as
g(x) =

(4)
where uj is a set of vector coefﬁcients called Koopman modes. Then  by the iterative applications
of Eqs. (2) and (3)  we obtain

Kφj(x) = (cid:21)jφj(x);

j=1φj(x)uj;

j=1(cid:21)l

jφj(x)uj;

(5)
where f l is the l-time compositions of f. Therefore  (cid:21)j characterizes the temporal behavior of the
corresponding Koopman mode uj  i.e.  the phase of (cid:21)j determines its frequency  and the magnitude
determines the growth rate of the dynamics. Note that  for a system evolving on an attractor  the
Koopman eigenvalues always lie on a unit circle [20].
DMD [25  26] (and its variants) is a popular approach for estimating the approximations of (cid:21)j and uj
from a ﬁnite-length data sequence y0; y1; : : : ; y(cid:28) (2 Rp)  where we denote yt := g(xt). DMD can
fundamentally be considered as a special use of the Arnoldi method [1]. That is  using the empirical
Ritz values ~(cid:21)j and vectors vj obtained by the Arnoldi method when regarding the subspace spanned
by y0; : : : ; y(cid:28)(cid:0)1 as the Krylov subspace for y0 (and implicitly for some matrix A 2 Rp(cid:2)p)  it is
shown that the observables are expressed as

∑1
∑1

g ◦ f l(x) =

∑
∑

yt =
y(cid:28) =

jvj (t = 0; : : : ; (cid:28) (cid:0) 1); and
~(cid:21)t
j vj + r where r ? spanfy0; : : : ; y(cid:28)(cid:0)1
~(cid:21)(cid:28)

(cid:28)
j=1
(cid:28)
j=1

g:

(6a)
(6b)

Comparing Eq. (6a) with Eq. (5) infers that the empirical Ritz values ~(cid:21)j and vectors vj behave in
precisely the same manner as the Koopman eigenvalues (cid:21)j and modes uj (φj(x0)uj)  but for the

2

ﬁnite sum in Eq. (6a) instead of the inﬁnite sum in Eq. (5). Note that  for r = 0 in Eq. (6b) (which
could happen when the data are sufﬁciently large)  the approximate modes are indistinguishable
from the true Koopman eigenvalues and modes (as far as the data points are concerned)  with the
expansion (5) comprising only a ﬁnite number of terms.

3 Dynamic Mode Decomposition with Reproducing Kernels

As described above  the estimation of the Koopman mode by DMD (and its variants) can capture
the nonlinear dynamics from ﬁnite-length data sequences generated from a dynamical system. Con-
ceptually  DMD can be considered as producing an approximation of the Koopman eigenfunctions
using a set of linear monomials of observables as basis functions  which is analogous to a one-term
Taylor expansion at each point. In situations where eigenfunctions can be accurately approximated
using linear monomials (e.g.  in a small neighborhood of a stable ﬁxed point)  DMD will produce
an accurate local approximation of the Koopman eigenfunctions. However  this is certainly not ap-
plicable to all systems (in particular  beyond the region of validity for local linearization). Here  we
extend the Koopman spectral analysis with reproducing kernels to approximate the Koopman eigen-
functions with richer basis functions. We provide a modal decomposition algorithm to perform this
analysis based on the equivalent principle with DMD.
Let H be the RKHS embedded with the dot product ⟨(cid:1);(cid:1)⟩H (we abbreviate ⟨(cid:1);(cid:1)⟩H as ⟨(cid:1);(cid:1)⟩ for simplic-
ity) and a positive deﬁnite kernel k. Additionally  let ϕ : M ! H. Then  we deﬁne the Koopman
operator on the feature map ϕ by

(KHϕ)(x) = ϕ ◦ f (x):

(7)
Thus  the Koopman operator KH is a linear operator in H. Note that almost of the theoretical claims
in this and the next sections do not necessarily require ϕ to be in RKHS (it is sufﬁcient that ϕ stays in
a Hilbert space). However  this assumption should perform the calculation in practice (as described
in the last parts of this and the next sections). Therefore  we proceed with this assumption in the
following parts. We denote by φj the j-th eigenfunction of KH with the corresponding eigenvalue
(cid:21)j. Also  we deﬁne (cid:8) := spanfϕ(x) : x 2 Mg.
We ﬁrst expand the notions  such as the Ritz values and vectors  that appear in DMD with reproduc-
ing kernels. Suppose we have a sequence x0; x1; : : : ; x(cid:28) . The Krylov subspace for ϕ(x0) is deﬁned
as the subspace spanned by ϕ(x0); (KHϕ)(x0); : : : ; (K(cid:28)(cid:0)1H ϕ)(x0). Note that this is identical to the
one spanned by ϕ(x0); : : : ; ϕ(x(cid:28)(cid:0)1)  whose corresponding Krylov matrix is given by
(8)
Therefore  if we denote a set of (cid:28) orthogonal bases of the Krylov subspace by q1; : : : ; q(cid:28) (2 H)
(obtained from the Gram-Schmidt orthogonalization described below)  then the orthogonal projec-
tion of KH onto M(cid:28) is given by P(cid:28) = Q(cid:3)
(cid:28) indicates the
Hermitian transpose of Q(cid:28) . Consequently  the empirical Ritz values and vectors are deﬁned as the
eigenvalues and vectors of P(cid:28)   respectively. Now  we have the following theorem:
Theorem 1. Consider a sequence ϕ(x0); ϕ(x1); : : : ; ϕ(x(cid:28) )  and let ~(cid:21)j and ~φj be the empirical Ritz
values and vectors for this sequence. Assume that ~(cid:21)j’s are distinct. Then  we have

KHQ(cid:28)   where Q(cid:28) = [q1 (cid:1)(cid:1)(cid:1) q(cid:28) ] and Q(cid:3)

M(cid:28) = [ϕ(x0) (cid:1)(cid:1)(cid:1) ϕ(x(cid:28)(cid:0)1)]:

(cid:28)
j=1
(cid:28)
j=1

j ~φj (t = 0; : : : ; (cid:28) (cid:0) 1); and
~(cid:21)t
j ~φj + where ? spanfϕ(x0); : : : ; ϕ(x(cid:28)(cid:0)1)g:
~(cid:21)(cid:28)

(9a)
(9b)
Proof. Let M(cid:28) = Q(cid:28) R (R 2 C(cid:28)(cid:2)(cid:28) ) be the Gram-Schmidt QR decomposition of M(cid:28) . Then  the
companion matrix (rational canonical form) of P(cid:28) is given as F := R
(cid:0)1P(cid:28) R. Note that the sets of
eigenvalues of P(cid:28) and F are equivalent. Since F is a companion matrix and ~(cid:21)j’s are distinct  F can
(cid:0)1 ~(cid:3)T   where ~(cid:3) is a diagonal matrix with ~(cid:21)1; : : : ; ~(cid:21)(cid:28) and T is a
be diagonalized in the form F = T
Vandermonde matrix deﬁned by Tij = ~(cid:21)j(cid:0)1
. Therefore  the empirical Ritz vectors ~φj are obtained
as the columns of V = M(cid:28) T
(cid:0)1. This proves Eq. (9a). Suppose a linear expansion of ϕ(x(cid:28) ) is
represented as

∑
∑

(cid:0)1P(cid:28) R = M(cid:0)1
Since F = R
last column of M(cid:28) F = M(cid:28) T

ϕ(x(cid:28) ) = M(cid:28) c + where ? spanfϕ(x0); : : : ; ϕ(x(cid:28)(cid:0)1)g:

(10)
KHM(cid:28) (therefore  M(cid:28) F = KHM(cid:28) )  the ﬁrst term is given by the
(cid:0)1 ~(cid:3)T = V ~(cid:3)T . This proves Eq. (9b).

(cid:28)

ϕ(xt) =

ϕ(x(cid:28) ) =

(cid:28)

i

3

This theorem gives an extension of DMD via the Gram-Schmidt QR decomposition in the feature
space. Although in Step (2)  the Gram-Schmidt QR orthogonalization is performed in RKHS  this
calculation can be reduced to operations on a Gram matrix due to the reproducing property of kernel
functions.
(1) Deﬁne M(cid:28) by Eq. (8) and M+ := [ϕ(x1); : : : ; ϕ(x(cid:28) )].
(2) Calculate the Gram-Schmidt QR decomposition M(cid:28) = Q(cid:28) R (e.g.  refer to Section 5.2 of [29]).
(cid:0)1 ~(cid:3)T   where each diagonal ele-
(3) Calculate the eigendecomposition of R
(4) Deﬁne ~φj to be the columns of M(cid:28) T
The original DMD algorithm (and its variants) produce an approximation of the eigenfunctions of
the Koopman operator in Eq. (2) using the set of linear monomials of observables as basis functions.
In contrast  because the above algorithm works with operations directly in the functional space 
the Koopman operator deﬁned in Eq. (7) is identical to the transition operator on an observable.
Therefore  the eigenfunctions of the Koopman operator are fully recovered if the Krylov subspace
is sufﬁciently large  i.e.  ϕ(x(cid:28) ) is also in spanfϕ(x0); : : : ; ϕ(x(cid:28)(cid:0)1)g (or = 0).

M+(=F ) = T

ment of ~(cid:3) gives ~(cid:21)j.

(cid:0)1Q(cid:3)

(cid:0)1.

(cid:28)

4 Robustifying with POD Bases

(cid:28)

Although the above decomposition based on the Gram-Schmidt orthogonalization is mathematically
correct  a practical implementation could yield an ill-conditioned algorithm that is often incapable
of extracting multiple modes. A similar issue has been well known for DMD [26]  where one needs
to adopt a way to robustify DMD by projecting data onto the (truncated) POD directions [8  33].
Here  we discuss a similar modiﬁcation of our principle with the POD basis.
(cid:3) be the eigen-decomposition
First  consider kernel PCA [28] on x0; x1; : : : ; x(cid:28)(cid:0)1: Let (cid:22)G = BSB
of the centered Gram matrix (cid:22)G = HGH = G (cid:0) 1(cid:28) G (cid:0) G1(cid:28) + 1(cid:28) G1(cid:28)   where G = M(cid:3)
M(cid:28) is
the Gram matrix for the data  H = I (cid:0) 1(cid:28) and 1(cid:28) is a (cid:28)-by-(cid:28) matrix for which each element takes
∑
the value 1=(cid:28). Suppose the eigenvalues and eigenvectors can be truncated accordingly based on the
magnitudes of the eigenvalues  which results in (cid:22)G (cid:25) (cid:22)B (cid:22)S (cid:22)B
(cid:3) where p ((cid:20)(cid:28) ) eigenvalues are adopted.
∑
Denote the j-th column of (cid:22)B by (cid:12)j and let (cid:22)ϕ(xi)=ϕ(xi)(cid:0)ϕc  where ϕc=
(cid:28)(cid:0)1
j=0 ϕ(xj). A principal
(cid:22)ϕ(xi) = M(cid:28) H(cid:11)j (j =
(cid:28)(cid:0)1
orthogonal direction in the feature space is then given by (cid:23)j =
i=0 (cid:11)j;i
(cid:0)1=2). Since M+ = KHM(cid:28)  
jj (cid:12)j. Let U = [(cid:23)1; : : : ; (cid:23)p] (= M(cid:28) H (cid:22)B (cid:22)S
(cid:0)1=2
1; : : : ; p)  where (cid:11)j = (cid:22)S
the projection of KH onto the space spanned by (cid:23)j is given as
M+)H (cid:22)B (cid:22)S
^F := U(cid:3)KHU = (cid:22)S
(cid:0)1=2:
(cid:0)1=2 (cid:22)B
(cid:3)
Note that the (i; j)-the element of the matrix (M(cid:3)
(cid:0)1 ^(cid:3) ^T be the eigendecomposition of ^F   then
^F = ^T

H(M(cid:3)
(11)
M+) is given by k(xi(cid:0)1; xj). Then  if we let

(cid:22)φj = Ubj = M(cid:28) H (cid:22)B (cid:22)S
(cid:0)1  can be used as an alternative to the empirical Ritz vector ~φj.
where bj is the j-th column of ^T
That is  we have the following theorem:
Theorem 2. Assume that φj 2 (cid:8)  so that φj(x) = ⟨ϕ(x); (cid:20)j⟩ for some (cid:20)j 2 H and 8x 2 M. If
(cid:20)j is in the subspace spanned by the columns of U  so that (cid:20)j = Uaj for some aj 2 Cp  then aj is
a left eigenvector of ^F with eigenvalue (cid:21)j  and also we have
p
j=1φj(x) (cid:22)φj:

(12)
Proof. Since KHφj = (cid:21)jφj  we have ⟨ϕ(f (x)); (cid:20)j⟩ = (cid:21)j ⟨ϕ(x); (cid:20)j⟩. Thus  from the assumption 

(cid:0)1=2bj;

∑

ϕ(x) =

(cid:28)

(cid:28)

⟨ϕ(f (x));Uaj⟩ = (cid:21)j ⟨ϕ(x);Uaj⟩ :

By evaluating at x0; x1; : : : ; x(cid:28)(cid:0)1 and then stacking into matrices  we have

(cid:0)1HM(cid:3)
If we multiply H (cid:22)G
U(cid:3)M+H (cid:22)G
(cid:3)
j

a

(cid:28)

(cid:28)

(Uaj)

(cid:3)M+ = (cid:21)j(Uaj)

(cid:3)M(cid:28) :
U from the righthand side  this gives
U(cid:3)M(cid:28) H (cid:22)G

U = (cid:21)ja

(cid:0)1HM(cid:3)

(cid:3)
j

(cid:0)1HM(cid:3)

(cid:28)

U = (cid:21)ja
(cid:3)
j :

4

(cid:28)

p
j=1(a

∑

Since U(cid:3)M+H (cid:22)G
U = U(cid:3)KHU(= ^F )  this means aj is a left eigenvector of ^F with eigen-
(cid:0)1HM(cid:3)
∑
value (cid:21)j. Let bj be a (right) eigenvector of ^F with eigenvalue (cid:21)j and the corresponding left eigen-
j bj = (cid:14)ij  then any vector h 2 Cp can be
(cid:3)
∑
vector aj.Assuming these have been normalized so that a
written as h =

j h)bj. Applying this to U(cid:3)
(cid:3)
U(cid:3)
Since bj = (U(cid:3)U)bj = U(cid:3)
This theorem clearly gives the connection between the eigenvalues/eigenvectors found by the above
procedure and the Koopman eigenvalues/eigenfunctions. The assumptions in the theorem means
that the data are sufﬁciently rich and thus a set of the kernel principal components gives a good
approximation of the representation with the Koopman eigenfunctions. As in the case of Eq. (5)  by
the iterative applications of Eq. (3)  we obtain

ϕ(x) =
(cid:22)φj  this proves Eq. (12).

p
j=1φj(x)bj

ϕ(x) gives

ϕ(x))bj: =

p
j=1(a

U(cid:3)

(cid:3)
j

∑

ϕ(xt) =

p

j=1(cid:21)t

jφj(x0) (cid:22)φj:

(13)

The procedure for the robustiﬁed variant of the DMD is summarized as follows.1
(1) Deﬁne M(cid:28) and calculate the centered Gram matrix (cid:22)G = HM(cid:3)
(2) Calculate the eigendecomposition (cid:22)G (cid:25) (cid:22)B (cid:22)S (cid:22)B
(3) Calculate ^F as in Eq. (11) and its eigendecomposition ^F = ^T

(cid:3)  which gives the kernel principal directions U.
(cid:0)1 ^(cid:3) ^T   where each diagonal

M(cid:28) H.

(cid:28)

element of ^(cid:3) gives (cid:21)j.

(cid:0)1.

(cid:0)1=2 ^T

(4) Deﬁne (cid:22)φj to be the columns of M(cid:28) H (cid:22)B (cid:22)S
Unlike the procedure described in Section 3  the above procedure can perform the truncation of
eigenvectors corresponding to small singular values. As well as DMD  this step becomes beneﬁcial
in practice when the Gram matrix G  in our case  is rank-deﬁcient or nearly so.
Remark: Although we assumed that data is a consecutive sequence for demonstrating the correct-
ness of the algorithm  as evident from the above steps  the estimation procedure itself does not neces-
sarily require a sequence but rather a collection of pairs of consecutive observables f(x(i)
i=1 
1 )  with the appropriate deﬁnitions of M(cid:28) and M+.
where each pair is supposed to be x(i)

2 = f (x(i)

2 )g(cid:28)

1 ; x(i)

5 Related Works

Spectral analysis (or  referred as the decomposition technique) for dynamical systems is a popular
approach aimed at extracting information concerning (low-dimensional) dynamics from data. Com-
mon techniques include global eigenmodes for linearized dynamics (see  e.g.  [3])  discrete Fourier
transforms  POD for nonlinear dynamics [30  12]  and balancing modes for linear systems [24] as
well as multiple variants of these techniques  such as those using shift modes [22] in conjunction
with POD modes. In particular  POD  which is in principle equivalent to principal component analy-
sis  has been extensively applied to the analysis of physical phenomena [5  22] even though it suffers
from numerous known issues  including the possibility of principal directions in a set of data may
not necessarily correspond to the dynamically important ones.
DMD has recently attracted considerable attention in physics such as ﬂuid mechanics [2  10  21  25 
27] and in engineering ﬁelds [4  6  32]. Unlike POD (and its variants)  DMD yields direct infor-
mation about the dynamics such as growth rates and frequencies associated with each mode  which
can be obtained from the magnitude and phase of each corresponding eigenvalue of the Koopman
operator. However  the original DMD has several numerical disadvantages related to the accuracy of
the approximate expressions of the Koopman eigenfunctions from data. Therefore  several variants
of DMD have been proposed to rectify this point  including exact DMD [33] and optimized DMD
[8]. Jovanovi´c et al. proposed sparsity-promoting DMD [13]  which provides a framework for the
approximation of the Koopman eigenfunctions with fewer bases. Williams et al. proposed extended
DMD [35]  which works on pre-determined basis functions instead of the monomials of observables.
Although in extended DMD the Koopman mode is deﬁned as the eigenvector of the corresponding
operator of coefﬁcients on basis functions  the resulting procedure is similar to the robust-version of
our algorithm.

1The Matlab code is available at http://en.44nobu.net/codes/kdmd.zip

5

Figure 1: Estimated eigenvalues with the data from the
toy system (left) and the H´enon map (right).

Figure 2: Examples of the true versus (1-step) pre-
dicted values via the proposed method for the toy
system (left) and the H´enon map (right).

In system control  subspace identiﬁcation [23  14]  or called the eigensystem realization method 
has been a popular approach to modeling of dynamical systems. This method basically identiﬁes
low-dimensional (hidden) states as canonical vectors determined by canonical correlation analysis 
and estimates parameters in the governing system using the state estimates. This type of method
is known as a spectral method for dynamical systems in the machine learning community and has
recently been applied to several types of systems such as variants of hidden Markov models [31  19] 
nonlinear dynamical systems [15]  and predictive state-representation [17]. The relation between
DMD and other methods  particularly the eigensystem realization method  is an interesting open
problem. This is brieﬂy mentioned in [33] but it would require further investigation in future studies.

6 Empirical Example

To illustrate how our algorithm works  we here consider two examples: a toy nonlinear system given
by xt+1= 0:9xt  yt+1= 0:5yt+(0:92(cid:0)0:5)x2
t   and one of the well-known chaotic maps  called
the H´enon map (xt+1 = 1 (cid:0) ax2
t + yt  yt+1 = bxt)  which was originally presented by H´enon
as a simpliﬁed model of the Poincar´e section of the Lorenz attractor. As for the toy one  the two
eigenvalues are 0.5 and 0.9 with the corresponding eigenfunctions φ0:9 = xt and φ0:5 = yt (cid:0) x2
t  
respectively. And as for the H´enon map  we set the parameters as a = 1:4  b = 0:3. It is known
that this map has two equilibrium points ((cid:0)1:13135;(cid:0)0:339406) and (0:631354; 0:189406)  whose
corresponding eigenvalues are 2:25982 and (cid:0)1:09203  and (cid:0)2:92374 and (cid:0)0:844054.
We generated samples according to these systems with several initial conditions and then applied
the presented procedure to estimate the Koopman modes. We used the polynomial kernel of degree
three for the toy system  and the Gaussian kernel with width 1 for the H´enon map  respectively.
The graphs in Fig. 1 show the estimated eigenvalues for two cases. As seen from the left graph  the
eigenvalues for the toy system were precisely estimated. Meanwhile  from the right graph  the part
of the eigenvalues of the equilibrium points seem to be approximately estimated by the algorithm.

7 Applications

The above algorithm provides a decomposition of the dynamics into a ﬁnite number of modes  and
therefore  could be considered as a feature extraction procedure for a nonlinear dynamical system.
This would be useful to directly understand dominant characteristics of the dynamics  as done in
scientiﬁc ﬁelds with DMD [2  10  21  25  27]. However  here we consider some examples of appli-
cations using extracted features with the proposed analysis; prediction  sequential change detection 
and the recognition of dynamic patterns  with some empirical examples.
Prediction via Preimage: As is known in physics (nonlinear science)  long-term predictions in a
nonlinear dynamical system are  in principle  impossible if at least one of its Lyapunov exponents
is positive  which would be typically the case of interests. This is true even if the dimension of the
system is low because uncertainty involved in the evolution of the system exponentially increases
over time. However  it may be possible to predict an observable in the near future (i.e.  short-
term prediction) if we could formulate a precise predictive model. Therefore  we here consider a
prediction based on estimated Koopman spectra as in Eq. (13). Since Eq. (13) is represented as the
linear combination of ϕ(xi) (i = 0; : : : ; (cid:28) (cid:0) 1)  a prediction can be obtained by considering the
pre-image of the predicted observables in the feature space. Even though any method for ﬁnding a
pre-image of a vector in the feature space can be used for this purpose  here we describe an approach

6

123456700.20.40.60.81kernel DMDTrueIndexEigenvalue8-3-2-10-10.500.51kernel DMDEquilibriumDMD123RealImage050100150200-4-2024x1TruePredicted value(cid:85)(cid:74)(cid:78)(cid:70)020406080100-1.5-1-0.500.511.5TruePredicted valuex1(cid:85)(cid:74)(cid:78)(cid:70)Figure 3: MDS embedding with the distance ma-
trix from kernel principal angle between subspaces of
the estimated Koopman eigenfunctions for locomotion
data. Each point is colored according to its assigned
motion (jump  walk  run  and varied).

Figure 4: Sample sequence (top) and change
scores by our method (green) and the kernel
change detection method (blue).

based on a similar idea with multidimensional scaling (MDS)  as describe in [18]  where a pre-image
is recovered to preserve the distance between it and other data points in the input space as well as the
feature space. The basic steps are (i) ﬁnd n-neighbors of a new point ^ϕ(x(cid:28) +l) in the feature space 
(ii) calculate the corresponding distance between the preimage ^x(cid:28) +l and each data point xt based on
the relation between the feature- and input-space distances  and (iii) calculate the pre-image in order
to preserve the input distances. For step (i)  we need the distance between the estimated feature and
each data point in the feature space  which is calculated as

∥ ^ϕ(x(cid:28) +l) (cid:0) ϕ(xt)∥2 = ∥ ^ϕ(x(cid:28) +l)∥2 + ∥ϕ(xt)∥2 (cid:0) 2 ^ϕ(x(cid:28) +l)
(cid:3)
(M(cid:3)

M(cid:28) )c + k(xt; xt) (cid:0) 2c

(M(cid:3)

= c

(cid:3)

(cid:3)

(cid:28)

ϕ(xt)
(cid:28) ϕ(xt));

where c is from Eq. (10). Note that the ﬁrst and third terms in the above equation can be calculated
using the values in the Gram matrix for the data. Once we obtain n-neighbors based on the feature
distances  we can construct the corresponding local coordinate by calculating a set of orthogonal
bases (via  for example  singular value decomposition of the data matrix for the neighbors) based
on the distances in the input spaces  which are analytically obtained from the feature distances [18].
The graphs in Fig. 2 show empirical examples of the true versus predicted values as described above
for the toy nonlinear system and the H´enon map. The setups for the data generation and the kernels
etc. are same with the previous section.
Embedding and Recognition of Dynamics: A direct but important application of the presented
analysis is the embedding and recognition of dynamics with the extracted features. Like (kernel)
PCA  a set of Koopman eigenfunctions estimated via the analysis can be used as the bases of a
low dimensional subspace that represents the dynamics. For example  the recognition of dynamics
based on this representation can be performed as follows. Suppose we are given m collection of
data sequences fxtg(cid:28)i
t=0 (i=1;: : : ;m) each of which is generated from some known dynamics C
(e.g.  walks  runs  jumps etc.). Then  a set of estimated Koopman eigenfunctions for each known
dynamics  which we denote by Ac = M(cid:28) wc for the corresponding complex vector wc  can be
regarded as the bases of a low-dimensional embedding of the sequences. Hence  if we let A be a
set of the estimated Koopman eigenfunctions for a new sequence  its category of dynamics can be
estimated as
where dist(A;Ac) is a distance between two subspaces spanned by A and Ac. For example  such a
distance can be given via the kernel principal angles between two subspaces in the feature space [36].
Fig. 3 shows an empirical example of this application using the locomotion data from CMU Graphics
Lab Motion Capture Database.2 We used the RBF Gaussian kernel  where the kernel width was set
as the median of the distances from a data matrix. The ﬁgure shows an embedding of the sequences
via MDS with the distance matrix  which was calculated with kernel principal angles [36] between
subspaces spanned by the Koopman eigenfunctions. Each point is colored according to its motion
(jump  walk  run  and varied).

dist(A;Ac);

^i = argmin

c2C

2Available at http://mocap.cs.cmu.edu.

7

-1-0.500.511.5-0.400.40.81.2jumpwalkrunvariedwalk  then turnrun  then stoprun  then turnslow walk  then stop0500100015002000-20-1001020304050x1x2x310kDMD101-SVMSequential Change-Point Detection: Another possible application is the sequential detection of
change-points in a nonlinear dynamical system based on the prediction via the presented analy-
sis. Here  we give a criterion for this problem based on the so-called cumulative-sum (CUSUM)
of likelihood-ratios (see  for example  [9]). Let x0; x1; x2; : : : be a sequence of random vec-
tors distributed according to some distribution ph (h = 0; 1). Then  change-point detection is
deﬁned as the sequential decision between hypotheses; H0 : p(xi) = p0(xi) for i = 1; : : : ; T  
and H1 : p(xi) = p0(xi) for i = 1; : : : ; (cid:28) and p(xi) = p1(xi) for i = (cid:28) + 1; : : : ; T   where
1 (cid:20) (cid:28) (cid:20) T ((cid:20) 1). In CUSUM  the stopping rule is given as

∑

}
t=(cid:28) +1 log (p1(xt)=p0(xt)) (cid:21) c

T

T : max1(cid:20)(cid:28) <T

= inf

T
(cid:3) is the stopping time). Although the Koopman operator is  in general  deﬁned for
where c > 0 (T
a deterministic system  it is known to be extended to a stochastic system xt+1 = f (xt; vt)  where
vt is a stochastic disturbance [20]. In that case  the operator works on the expectation. Hence  let us
deﬁne the distribution of xt as a nonparametric exponential family [7]  given by

p(xt) = exp (⟨(cid:18)((cid:1)); (xt)⟩H (cid:0) g((cid:18))) = exp (⟨ϕ ◦ f (xt(cid:0)1); ϕ(xt)⟩H (cid:0) g(ϕ ◦ f (xt(cid:0)1))) ;

;

{

(cid:3)

where g is the log-partition function. Then  the log-likelihood ratio score is given as

i=(cid:28) +1 log (p1(xt)=p0(xt)) / (cid:0)∑
∑

T

(∑

i k(xj; xi) (cid:0)∑

log (cid:3)(cid:28) (x1:T ) :=

T
i=(cid:28) +1

(cid:28)

j=1(cid:11)(0)

(cid:28)

j=1(cid:11)(1)

i k(xj; xi)

;

)

i

and (cid:11)(1)

where (cid:11)(0)
are the coefﬁcients obtained by the proposed algorithm with the data for i =
1; : : : ; (cid:28) and i = (cid:28) + 1; : : : ; T   respectively. Here  since the variation of the second term is much
smaller than the ﬁrst one (cf. [7])  the decision rule  log (cid:3)(cid:3) (cid:21) c  can be simpliﬁed by ignoring the
second term. As a result  we have the following decision rule with some critical value ~c (cid:20) 0:

i

(cid:0) log (cid:3)(cid:28) (x1:T ) (cid:25)∑

∑

T
i=(cid:28) +1

(cid:28)

j=1(cid:11)(0)

i k(xj; xi) (cid:20) ~c;

A change-point is detected if the above rule is satisﬁed. Otherwise  the procedure will be repeated
until a change-point is detected by updating the coefﬁcients using new samples. Fig. 4 shows an
empirical example of the (normalized) change score calculated with the proposed algorithm  with
comparison with the one by the kernel change detection method (cf. [7])  for the shown data gener-
ated from the Lorenz map. We used the RBF Gaussian kernel as in the same way. In the simulation 
the parameter of the map changes at 800 and 1200 although the ranges of the data values dramatically
change in other areas (where the score by the comparative method has changed correspondingly).

8 Conclusions

We presented a spectral analysis method with the Koopman operator in RKHSs  and developed
algorithms to perform the analysis using a ﬁnite-length data sequence from a nonlinear dynamical
system  that is essentially reduced to the calculation of a set of orthogonal bases of the Krylov matrix
in RKHSs and the eigendecomposition of the projection of the Koopman operator onto the subspace
spanned by the bases. We further considered applications using estimated Koopman spectra with
the proposed analysis  which were empirically illustrated using synthetic and real-world data.

Acknowledgments

This work was supported by JSPS KAKENHI Grant Number JP16H01548.

References
[1] W.E. Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem.

Quarterly of Applied Mathematics  9:17–29  1951.

[2] S. Bagheri. Koopman-mode decomposition of the cylinder wake. Journal of Fluid Mechanics  726:596–

623  2013.

[3] S. Bagheri  P. Schlatter  P.J. Schmid  and D.S. Henningson. Global stability of a jet in cross ﬂow. Journal

of Fluid Mechanics  624:33–44  2009.

[4] E. Berger  M. Satsuma  D. Vogt  B. Jung  and H. Ben Amor. Dynamic mode decomposition for perturba-
tion estimation in human robot interaction. In Proc. of the 23rd IEEE Int’l Symp. on Robot and Human
Interactive Communication  pages 593–600  2014.

8

[5] J.-P. Bonnet  C.R. Cole  J. Delville  M.N. Glauser  and L.S. Ukeiley. Stochastic estimation and proper
orthogonal decomposition: Complementary techniques for identifying structure. Experiments in Fluids 
17:307–314  1994.

[6] B. Brunton  L. Aohnson  J. Ojemann  and J. Nathan Kutz. Extracting spatial-temporal coherent patterns
in large-scale neural recordings using dynamic mode decomposition. Journal of Neuroscience Methods 
258:1–15  2016.

[7] S. Canu and A. Smola. Kernel methods and the exponential family. Neurocomputing  69:714–720  2006.
[8] K.K. Chen  J.H. Tu  and C.W. Rowley. Variants of dynamic mode decomposition: Boundary condition 

Koopman  and Fourier analyses. Journal of Nonlinear Science  22(6):887–915  2012.

[9] M. Cs¨org¨o and L. Horv´ath. Limit Theorems in Change-Point Analysis. Wiley  1988.
[10] D. Duke  D. Honnery  and J. Soria. Experimental investigation of nonlinear instabilities in annular liquid

sheets. Journal of Fluid Mechanics  691:594–604  2012.

[11] Z. Ghahramani and S.T. Roweis. Learning nonlinear dynamical systems using an EM algorithm. In Proc.

of the 1998 Conf. on Advances in Neural Information Processing Systems II  pages 431–437.

[12] P. Holmes  J.L. Lumley  and G. Berkooz. Turbulence  Coherent Structures  Dynamical Systems and

[13] M.R. Jovanovi´c  P.J. Schmid  and J.W. Nichols. Sparsity-promoting dynamic mode decomposition.

Symmetry. Cambridge University Press  1996.

Physics of Fluids  26:024103  2014.

[14] T. Katayama. Subspace Methods for System Identiﬁcation. Springer  2005.
[15] Y. Kawahara  T. Yairi  and K. Machida. A kernel subspace method by stochastic realization for learning

nonlinear dynamical systems. In Adv. in Neural Infor. Processing Systems 19  pages 665–672. 2007.

[16] B.O. Koopman. Hamiltonian systems and transformation in Hilbert space. Proc. of the National Academy

of Sciences of the United States of America  17(5):315–318  1931.

[17] A. Kulesza  N. Jiang  and S. Singh. Spectral learning of predictive state representations with insufﬁcient

statistics. In Proc. of the 29th AAAI Conf. on Artiﬁcial Intelligence (AAAI’15)  pages 2715–2721.

[18] James Tin-Yau Kwok and Ivor Wai-Hung Tsang. The pre-image problem in kernel methods. IEEE Trans.

on Neural Networks  15(6):1517–1525  2004.

[19] I. Melnyk and A. Banerjee. A spectral algorithm for inference in hidden semi-markov models. In Proc.

of the 18th Int’l Conf. on Artiﬁcial Intelligence and Statistics (AISTATS’15)  pages 690–698  2015.

[20] I. Mezi´c. Spectral properties of dynamical systems  model reduction and decompositions. Nonlinear

Dynamics  41:309–325  2005.

[21] T.W. Muld  G. Efraimsson  and D.S. Henningson. Flow structures around a high-speed train extracted
using proper orthogonal decomposition and dynamic mode decomposition. Computers and Fluids  57:87–
97  2012.

[22] B.R. Noack  K. Afanasiev  M. Morzynski  G. Tadmor  and F. Thiele. A hierarchy of low-dimensional

models for the transient and post-transient cylinder wake. J. of Fluid Mechanics  497:335–363  2003.

[23] P. Van Overschee and B. De Moor. Subspace Identiﬁcation for Linear Systems: Theory  Implementation 

Applications. Kluwer Academic Publishers  1996.

[24] C.W. Rowley. Model reduction for ﬂuids using balanced proper orthogonal decomposition. International

Journal of Bifurcation Chaos  15(3):997–1013  2005.

[25] C.W. Rowley  I. Mezi´c  S. Bagheri  P. Schlatter  and D.S. Henningson. Spectral analysis of nonlinear

ﬂows. Journal of Fluid Mechanics  641:115–127  2009.

[26] P.J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Me-

chanics  656:5–28  2010.

[27] P.J. Schmid and J. Sesterhenn. Dynamic mode decomposition of turbulent cavity ﬂows for self-sustained

oscillations. Int’l J. of Heat and Fluid Flow  32(6):1098–1110  2010.

[28] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue problem.

Neural Computation  10:1299–1319  1998.

[29] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press  2004.
[30] L. Sirovich. Turbulence and the dynamics of coherent structures. Quarterly of applied mathematics 

45:561–590  1987.

[31] L. Song  B. Boots  S.M. Siddiqi  G. Gordon  and A. Smola. Hilbert space embeddings of hidden markov

models. In Proc. of the 27th Int’l Conf. on Machine Learning (ICML’10)  pages 991–998.

[32] Y. Suzuki and I. Mezi´c. Nonlinear koopman modes and power system stability assessment without mod-

els. IEEE Trans. on Power Systems  29:899–907  2013.

[33] J.H. Tu  C.W. Rowley  D.M. Luchtenburg  S.L. Brunton  and J.N. Kutz. On dynamic mode decomposition:

Theory and applications. Journal of Computational Dynamics  1(2):391–421  2014.

[34] J. Wang  A. Hertzmann  and D.M. Blei. Gaussian process dynamical models. In Advances in Neural

Information Processing Systems 18  pages 1441–1448. 2006.

[35] M.O. Williams  I.G. Kevrekidis  and C.W. Rowley. A data-driven approximation of the Koopman opera-

tor: Extending dynamic mode decomposition. Journal of Nonlinear Science  25:1307–1346  2015.

[36] L. Wolf and A. Shashua. Learning over sets using kernel principal angles. Journal of Machine Learning

Research  4:913–931  2003.

9

,Mario Marchand
Hongyu Su
Emilie Morvant
Juho Rousu
John Shawe-Taylor
Yoshinobu Kawahara