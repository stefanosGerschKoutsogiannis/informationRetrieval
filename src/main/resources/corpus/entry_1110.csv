2014,Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings,State of the art statistical estimators for high-dimensional problems take the form of regularized  and hence non-smooth  convex programs. A key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however  proximal optimization methods attain only a sublinear rate. In this paper  we investigate a novel variant of strong convexity  which we call Constant Nullspace Strong Convexity (CNSC)  where we require that the objective function be strongly convex only over a constant subspace. As we show  the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method  when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments  and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.,Constant Nullspace Strong Convexity and

Fast Convergence of Proximal Methods under

High-Dimensional Settings

Ian E.H. Yen

Cho-Jui Hsieh

Pradeep Ravikumar

Inderjit Dhillon

Department of Computer Science

{ianyen cjhsieh pradeepr inderjit}@cs.utexas.edu

University of Texas at Austin

Abstract

State of the art statistical estimators for high-dimensional problems take the form
of regularized  and hence non-smooth  convex programs. A key facet of these
statistical estimation problems is that these are typically not strongly convex un-
der a high-dimensional sampling regime when the Hessian matrix becomes rank-
deﬁcient. Under vanilla convexity however  proximal optimization methods attain
only a sublinear rate. In this paper  we investigate a novel variant of strong con-
vexity  which we call Constant Nullspace Strong Convexity (CNSC)  where we re-
quire that the objective function be strongly convex only over a constant subspace.
As we show  the CNSC condition is naturally satisﬁed by high-dimensional sta-
tistical estimators. We then analyze the behavior of proximal methods under this
CNSC condition: we show global linear convergence of Proximal Gradient and lo-
cal quadratic convergence of Proximal Newton Method  when the regularization
function comprising the statistical estimator is decomposable. We corroborate our
theory via numerical experiments  and show a qualitative difference in the con-
vergence rates of the proximal algorithms when the loss function does satisfy the
CNSC condition.

1 Introduction

There has been a growing interest in high-dimensional statistical problems  where the number of
parameters d is comparable to or even larger than the sample size n  spurred in part by many modern
science and engineering applications. It is now well understood that in order to guarantee statistical
consistency it is key to impose low-dimensional structure  such as sparsity  or low-rank structure 
on the high-dimensional statistical model parameters. A strong line of research has thus developed
classes of regularized M-estimators that leverage such structural constraints  and come with strong
statistical guarantees even under high-dimensional settings [13]. These state of the art regularized
M-estimators typically take the form of convex non-smooth programs.
A facet of computational consequence with these high-dimensional sampling regimes is that these
M-estimation problems  even when convex  are typically not strongly convex. For instance  for
the ℓ1-regularized least squares estimator (LASSO)  the Hessian is rank deﬁcient when n < d. In
the absence of additional assumptions however  optimization methods to solve general non-smooth
non-strongly convex programs can only achieve a sublinear convergence rate [19  21]; faster rates
typically require strong convexity [1  20]. In the past few years  an effort has thus been made to
impose additional assumptions that are stronger than mere convexity  and yet weaker than strong
convexity; and proving faster rates of convergence of optimization methods under these assump-
tions. Typically these assumptions take the form of a restricted variant of strong convexity  which
incidentally mirror those assumed for statistical guarantees as well  such as the Restricted Isometry

1

Property or Restricted Eigenvalue property. A caveat with these results however is that these statisti-
cally motivated assumptions need not hold in general  or require sufﬁciently large number of samples
to hold with high probability. Moreover  the standard optimization methods have to be modiﬁed in
some manner to leverage these assumptions [5  7  17]. Another line of research exploits a local error
bound to establish asymptotic linear rate of convergence for a special form of non-strongly convex
functions [16  8  6]. However  these do not provide ﬁnite-iteration convergence bounds  due to the
potentially large number of iterations spent on early stage.
In this paper  we consider a novel simple condition  which we term Constant Nullspace Strong
Convexity (CNSC). This assumption is motivated not from statistical considerations  but from the
algebraic form of standard M-estimators; indeed as we show  standard M-estimation problems even
under high-dimensional settings naturally satisfy the CNSC condition. Under this CNSC condition 
we then investigate the convergence rates of the class of proximal optimization methods; speciﬁcally
the Proximal Gradient method (Prox-GD) [14  15  18] and the Proximal Newton method (Prox-
Newton) [1  2  9]. These proximal methods are very amenable to regularized M-estimation prob-
lems: they do not treat the M-estimation problem as a black-box convex non-smooth problem  but
instead leverage the composite nature of the objective of the form F (x) = h(x)+f (x)  where h(x)
is a possibly non-smooth convex function while f (x) is a convex smooth function with Lipschitz-
continuous gradient. We show that under our CNSC condition  Proximal Gradient achieves global
linear convergence when the non-smooth component is a decomposable norm. We also show that
Proximal Newton  under the CNSC condition  achieves local quadratic convergence as long as the
non-smooth component is Lipschitz-continuous. Note that in the absence of strong convexity  but
under no additional assumptions beyond convexity  the proximal methods can only achieve sublin-
ear convergence as noted earlier. We have thus identiﬁed an algebraic facet of the M-estimators
that explains the strong computational performance of standard proximal optimization methods in
practical settings in solving high-dimensional statistical estimation problems.
The paper is organized as follows.
In Section 2  we deﬁne the CNSC condition and introduce
the Proximal Gradient and Proximal Newton methods. Then we prove global linear convergence
of Prox-GD and local quadratic convergence of Prox-Newton in Section 3 and 4 respectively. In
Section 5  we corroborate our theory via experiments on real high-dimensional data set. We will
leave all the proof of lemmas to the appendix.

2 Preliminaries

We are interested in composite optimization problems of the form

min
x∈Rd

F (x) = h(x) + f (x) 

(1)

where h(x) is a possibly non-smooth convex function and f (x) is twice differentiable convex func-
tion with its Hessian matrix H(x) = ∇2f (x) satisfying

mI ≼ H(x) ≼ M I  ∀x ∈ Rd 

(2)

where for strongly convex f (x) we have m > 0; otherwise  for convex but not strongly convex f (x)
we have m = 0.

2.1 Constant Nullspace Strong Convexity (CNSC)

Before deﬁning our strong convexity variant of Constant Nullspace Strong Convexity (CNSC)  we
ﬁrst provide some intuition by considering the following large class of statistical estimation problems
in high-dimensional machine learning  where f (x) takes the form

n∑

f (x) =

i=1

L(aT

i x  yi) 

(3)

where L(u  y) is a non-negative loss function that is convex in its ﬁrst argument  ai is the observed
feature vector and yi is the observed response of the i-th sample. The Hessian matrix of (3) takes
the form

H(x) = AT D(Ax)A 

(4)

2

(aT

i and D(Ax) is a diagonal matrix with
where A is a n by d design (data) matrix with Ai : = aT
′′
′′
i x  yi)  where the double-derivative in L
(u  y) is with respect to the ﬁrst argu-
Dii(x) = L
ment. It is easy to see that in high-dimensional problems with d > n  (4) is not positive deﬁnite so
that strong convexity would not hold. However  for strictly convex loss function L(·  y)  we have
′′
L

(u  y) > 0 and

iff Av = 0.

vT H(x)v = 0

(5)
As a consequence vT H(x)v > 0 as long as v does not lie in the Nullspace of A; that is  the Hessian
H(x) might satisfy the strong convexity bound in the above restricted sense. We generalize this
concept as follows. We ﬁrst deﬁne the following notation: given a subspace T   we let ΠT (·) denote
the orthogonal projection onto T   and let T ⊥ denote the orthogonal subspace to T .
Assumption 1 ( Constant Nullspace Strong Convexity ). A twice-differentiable f (x) satisﬁes Con-
stant Nullspace Strong Convexity (CNSC) with respect to T (CNSC-T ) iff there is a constant vector
space T s.t. f (x) depends only on z = ΠT (x) and its Hessian matrix satisﬁes

(6)

(7)

for some m > 0  and ∀z ∈ T  

vT H(z)v ≥ m∥v∥2  ∀v ∈ T

H(z)v = 0  ∀v ∈ T ⊥

.

∫

0

From the motivating section above  the above condition can be seen to hold for a wide range of loss
functions  such as those arising from linear regression models  as well as generalized linear models
(u  y) ≥ mL >
′′
(e.g. logistic regression  poisson regression  multinomial regression etc.) 1. For L
0  we have m = mLλmin(AT A) > 0 as the constant in (6)  where λmin(AT A) is the minimum
positive eigenvalue of AT A.
Then by the assumption  any point x can be decomposed as x = z + y  where z = ΠT (x) 
y = ΠT ⊥(x)  so that the difference between gradient of two points can be written as

g(x1) − g(x2) =

1

H(s∆x + x2)∆xds =

H(s∆z + z2)∆zds = ˜H(z1  z2)∆z 

(8)

where ∆x = x1 − x2  ∆z = z1 − z2  and ˜H(z1  z2) =
0 H(s∆z + z2)ds is the average Hessian
matrix along the path from z2 to z1. It is easy to verify that ˜H(z1  z2) satisﬁes inequalities (2) 
(6) and equality (7) for all z1  z2 ∈ T by just applying inequalities (equality) to each individual
Hessian matrix being integrated. Then we have following theorem that shows the uniqueness of ¯z
at optimal.
Theorem 1 (Optimality Condition). For f (x) satisfying CNSC-T  

1

∫

1

0

∫

1. ¯x is an optimal solution of (1) iff −g(¯x) = ¯(cid:26) for some ¯(cid:26) ∈ ∂h(¯x).
2. The optimal ¯(cid:26) and ¯z = ΠT (¯x) are unique.

Proof. The ﬁrst statement is true since ¯x is an optimal solution iff 0 ∈ ∂h(¯x) + ∇f (¯x). To prove
the second statement  suppose ¯x1 = ¯z1 + ¯y1 and ¯x2 = ¯z2 + ¯y2 are both optimal. Let ∆x = ¯x1− ¯x2
and ∆z = ¯z1 − ¯z2. Since h(x) is convex  −g(¯x1) ∈ ∂h(¯x1) and −g(¯x2) ∈ ∂h(¯x2) should satisfy

However  since f (x) satisﬁes CNSC-T   by (8) 

⟨−g(¯x1) + g(¯x2)  ∆x⟩ ≥ 0.

⟨−g(¯x1) + g(¯x2)  ∆x⟩ = ⟨− ˜H(¯z1  ¯z2)∆z  ∆x⟩ = −∆z ˜H(¯z1  ¯z2)∆z ≤ −m∥∆z∥2

2

for some m > 0. The two inequalities can simultaneously hold only if ∆¯z = 0. Therefore  ¯z is
unique at optimum  and thus g(¯x) = g(0) + ˜H(¯z  0)¯z and ¯(cid:26) = −g(¯x) are also unique.

In next two sections  we review the Proximal Gradient Method (Prox-GD) and Proximal Newton
Method (Prox-Newton)  and introduce some tools that will be used in our analysis.

′′
1 Note for many generalized linear models  the second derivative L

(u  y) of loss function approaches 0 if |u| → ∞. However  this
could not happen as long as there is a penalty term h(x) which goes to inﬁnity if x diverges  which then serves as a ﬁnite constraint bound on
x.

3

2.2 Proximal Gradient Method

The Prox-GD algorithm comprises a gradient descent step

xt+ 1

2

= xt − 1

M

g(xt)

followed by a proximal step

xt+1 = proxh

M (xt+ 1

2

) = arg
x

min h(x) +

∥x − xt+ 1

2

∥2
2 

(9)

M
2

where ∥ · ∥2 means the Frobinius norm if x is a matrix. For simplicity  we will denote proxh
M (.) as
prox(.) in the following discussion when it is clear from the context. In Prox-GD algorithm  it is
assumed that (9) can be computed efﬁciently  which is true for most of decomposable regularizers.
Here we introduce some properties of proximal operator that can facilitate our analysis.
Lemma 1. Deﬁne ∆P x = x − prox(x)  the following properties hold for proximal operation (9).

1. M ∆P x ∈ ∂h(prox(x)).
2. ∥prox(x1) − prox(x2)∥2

2

2.3 Proximal Newton Method

≤ ∥x1 − x2∥2

2

− ∥∆P x1 − ∆P x2∥2
2.

In this section  we introduce the Proximal Newton method  which has been shown to be consider-
ably more efﬁcient than ﬁrst-order methods in many applications [1]  including Sparse Inverse Co-
variance Estimation [2] and ℓ1-regularized Logistic-Regression [9  10]. Each step of Prox-Newton
solves a local quadratic approximation

x+

t = arg
x

min h(x) +

1
2

(x − xt)T Ht(x − xt) + gT

t (x − xt)

(10)

to ﬁnd a search direction x+ − xt  and then conduct a line search procedure to ﬁnd t such that

f (xt+1) = f (xt + t(x+
t

− xt))

meets a sufﬁcient decrease condition. Note unlike Prox-GD update (9)  in most of cases (10) requires
an iterative procedure to solve. For example if h(x) is ℓ1-norm  then a coordinate descent algorithm
is usually employed to solve (10) as an LASSO subproblem [1  2  9  10].
The convergence of Newton-type method comprises two phases [1  3]. In the ﬁrst phase  it is possible
that step size t < 1 is chosen  while in the second phase  which occurs when xt is close enough
to optimum  step size t = 1 is always chosen and each step leads to quadratic convergence. In this
paper  we focus on the quadratic convergence phase  while refer readers to [21] for a global analysis
of Prox-Newton without strong convexity assumption. In the quadratic convergence phase  we have
xt+1 = x+

t and the update can be written as

(

)

xt+1 = proxHt

xt + ∆xnt
t

  Ht∆xnt

t = −gt 

(11)

where ∆xnt
t
for any PSD matrix H as

is the Newton step when h(x) is absent  and the proximal operator proxH (.) is deﬁned

proxH (x) = arg
v

min h(v) +

∥v − x∥2
H .

(12)

1
2

Note while we use ∥x∥2
H to denote xT Hx  we only require H to be PSD instead of PD. Therefore 
∥x∥H is not a true norm  and (12) might have multiple solutions  where proxH (x) refers to any
one of them.
In the following  we show proxH (.) has similar properties as that of prox(.) in
previous section.
Lemma 2. Deﬁne ∆P x = x− proxH (x)  the following properties hold for the proximal operator:

1. H∆P x ∈ ∂h(proxH (x)).
2. ∥proxH (x1) − proxH (x2)∥2

H

≤ ∥x1 − x2∥2
H.

4

3 Linear Convergence of Proximal Gradient Method
In this section  we analyze convergence of Proximal Gradient Method for h(x) = λ∥x∥  where ∥·∥
is a decomposable norm deﬁned as follows.
Deﬁnition 1 (Decomposable Norm). ∥ · ∥ is a decomposable norm if there are orthogonal sub-
Mi such that for any point x ∈ Rd that can be written as
i=1 with Rd = ∪J
spaces {Mi}J
∑
j∈E cjaj  where cj > 0 and aj ∈ Mj  ∥aj∥∗ = 1  we have
cj  and ∂∥x∥ = {(cid:26) | ΠMj ((cid:26)) = aj ∀j ∈ E; ∥ΠMj ((cid:26))∥∗ ≤ 1 ∀j /∈ E} 

∑
∥x∥ =

x =

i=1

(13)

j∈E

where ∥ · ∥∗ is the dual norm of ∥ · ∥.
The above deﬁnition includes several well-known examples such as ℓ1-norm ∥x∥1 and group-ℓ1
norm ∥X∥1 2. For ℓ1-norm  Mj corresponds to vectors with only j-th coordinate not equal to 0 
and E is the set of non-zero coordinates of x. For group-ℓ1 norm  Mj corresponds to vectors with
only j-th group not equal to 0T and E are the set of non-zero groups of X. Under the deﬁnition  we
can proﬁle the set of optimal solutions as follows.
Lemma 3 (Optimal Set). Let ¯E be the active set at optimal and ¯E + = {j| ∥ ΠMj (¯(cid:26))∥∗ = λ} be its
augmented set (which is unique since ¯(cid:26) is unique) such that ΠMj (¯(cid:26)) = λ¯aj  j ∈ ¯E +. The optimal
solutions of (1) form a polyhedral set
¯X =

{
x | ΠT (x) = ¯z and x ∈ ¯O}

(14)

 

{
x | x =

∑
j∈ (cid:22)E + cj ¯aj  cj ≥ 0  j ∈ ¯E +

}

is the set of x with ¯(cid:26) ∈ ∂h(x).

where ¯O =

Given the optimal set is a polyhedron  we can then employ the following lemma to bound the dis-
tance of an iterate xt to the optimal set ¯X .
Lemma 4 (Hoffman’s bound). Consider a polyhedral set S = {x|Ax ≤ b  Ex = c}. For any point
x ∈ Rd  there is a ¯x ∈ S such that

∥x − ¯x∥2 ≤ θ(S)

 

(15)

(cid:13)(cid:13)(cid:13)(cid:13) [Ax − b]+

Ex − c

(cid:13)(cid:13)(cid:13)(cid:13)

2

where θ(S) is a positive constant that depends only on A and E.
The above bound ﬁrst appears in [11]  and was employed in [4] to prove linear convergence of
Feasible Descent method for a class of convex smooth function. A proof of the ℓ2-norm version (15)
can be found in [4  lemma 4.3]. By applying (15) to the set ¯X   the distance of a point x to ¯X can be
bounded by infeasible amounts to the two constraints ΠT (x) = z and x ∈ ¯O  where the latter can
be bounded according the following lemma when cj = ⟨x  ¯aj⟩ ≥ 0 ∀j ∈ ¯E +.
Lemma 5. Let ¯A = span(¯a1  ¯a2 . . .   ¯a| (cid:22)E +|). Suppose ∥x∥ ≤ R and ΠMj (x) = 0 for j /∈ ¯E +.
Then
λ2∥x − Π (cid:22)A(x)∥2
where (cid:26) ∈ ∂h(x) and ¯(cid:26) is as deﬁned in Theorem 1.
Now we are ready to prove the main theorem of this section.
Theorem 2 (Linear Convergence of Prox-GD). Let ¯X be the set of optimal solutions for problem
(1)  and ¯x = Π (cid:22)X (x) be the solution closest to x. Denote dλ = minj /∈ (cid:22)E +
> 0.
For the sequence {xt}∞

(
λ − ∥ΠMj (¯(cid:26))∥∗

t=0 produced by Proximal Gradient Method  we have:

≤ R2∥(cid:26) − ¯(cid:26)∥2
2 

)

2

(a) If xt+1 satisﬁes the condition that

∃j /∈ ¯E + : ΠMj (xt+1) ̸= 0 or ∃j ∈ ¯E + : ⟨xt+1  ¯aj⟩ < 0 

we then have:

∥xt+1 − ¯xt+1∥2

2

≤ (1 − α)∥xt − ¯xt∥2

2  α =

5

d2
λ

M 2∥x0 − ¯x0∥2

2

(16)

(17)

2

∥xt − ¯xt∥2
= ∥∆xt∥2
≥ ∥∆xt∥2

2

2

≥ ∥xt − ¯xt∥2

− ∥xt+1 − ¯xt+1∥2
− ∥prox(xt − g(xt)/M ) − prox(¯xt − g(¯xt)/M )∥2
− ¯(cid:26)∥2
− ∥(xt − g(xt)/M ) − (¯xt − g(¯xt)/M )∥2

− ∥xt+1 − ¯xt∥2

2 + ∥(cid:26)t

2

2

2

2

2/M 2.

∥xt − ¯xt∥2

Since g(xt) − g(¯xt) = ˜H∆x from (8)  we have
(
≥ ∥∆xt∥2
≥ ∆xT
≥ m∥∆zt∥2

− ∥xt+1 − ¯xt+1∥2

)
− ∥∆xt − ˜H∆xt/M∥2
2
− ¯(cid:26)∥2
˜H/M
2/M + ∥(cid:26)t
2/M 2.

∆xt + ∥(cid:26)t
− ¯(cid:26)∥2

2

2

t

2 + ∥(cid:26)t
2/M 2

− ¯(cid:26)∥2

2/M 2

(20)

(21)

(b) If xt+1 does not satisfy the condition in (16) but xt does  then

∥xt+1 − ¯xt+1∥2

2

≤ (1 − α)∥xt−1 − ¯xt−1∥2

2  α =

(c) If neither xt+1  xt satisfy the condition in (16)  then

d2
λ

M 2∥x0 − ¯x0∥2

2

(18)

∥xt+2 − ¯xt+2∥2

≤ 1

∥xt − ¯xt∥2

(19)
where we recall that θ( ¯X ) is the constant determined by polyhedron ¯X from Hoffman’s
Bound (15).

2  β =

M θ( ¯X )2

1 + β

2

 

m

Proof. Since ¯xt is an optimal solution  we have ¯xt = prox(¯xt − g(¯xt)/M ). Let ∆xt = xt − ¯xt 
− xt+1) ∈ ∂h(xt+1) and ˜H = ˜H(zt  ¯zt). by Lemma 1  each iterate of Prox-GD
(cid:26)t = M (xt+ 1
has

2

The second inequality holds since 2 ˜H/M − ˜H 2/M 2 = ( ˜H/M )(2I − ˜H/M ) ≽ ˜H/M. The
inequality tells us ∥xt − ¯xt∥2 − ∥xt+1 − ¯xt+1∥2 ≥ 0  that is  the distance to the optimal set
∥xt − ¯xt∥ is monotonically non-increasing. To get a tighter bound  we consider two cases.
Case 1: ΠMj (xt) ̸= 0 for some j /∈ ¯E + or ⟨xt  ¯aj⟩ < 0 for some j ∈ ¯E +.
In this case  suppose there is j /∈ E +

t with ΠMj (xt) ̸= 0  then 2

≥ ∥ΠMj ((cid:26)t) − ΠMj (¯(cid:26))∥2∗ ≥ (∥ΠMj ((cid:26)t)∥∗ − ∥ΠMj (¯(cid:26))∥∗)2 ≥ d2
λ.

(22)
On the other hand  if ⟨xt  ¯aj⟩ < 0 for some j ∈ ¯E +  then we have ⟨aj  ¯aj⟩ < 0 for ΠMj ((cid:26)t) =
λaj. Therefore
− ¯(cid:26)∥2

2 = λ2(2 − 2⟨aj  ¯aj⟩) > 2λ2.

− ¯(cid:26)∥2

∥(cid:26)t

∥(cid:26)t

2

2

Either cases we have
∥xt − ¯xt∥2

2

≥ ∥ΠMj ((cid:26)t) − ΠMj (¯(cid:26))∥2
≥ ∥(cid:26)t

− ∥xt+1 − ¯xt+1∥2

2

2

≥ λ2∥aj − ¯aj∥2
(

− ¯(cid:26)∥2
M 2

2

≥

d2
λ

M 2∥x0 − ¯x0∥2

2

∥xt − ¯xt∥2
2.

(23)

)

Case 2: Both xt  xt+1 do not fall in Case 1
Given ⟨xt  ¯aj⟩ ≥ 0 ∀j ∈ ¯E + and ΠMj (xt) = 0 ∀j /∈ ¯E +  then x belongs to the set ¯O deﬁned in
Lemma 3 iff ∥x − Π (cid:22)A(x)∥2
2 = 0 
where R is a bound on ∥xt∥ holds for ∀t  which must exist as long as the regularization parameter
λ > 0 in h(x) = λ∥x∥.
By Lemma 4  the distance of point xt to the polyhedral set ¯X is bounded by its infeasible amount

2 = 0. The condition can be also scaled as

mM R2∥x − Π (cid:22)A(x)∥2

λ2

)

∥xt − ¯xt∥2

2

≤ θ( ¯X )2

2 +

λ2

mM R2

∥xt − Π (cid:22)A(xt)∥2

2

 

(24)

(
∥zt − ¯z∥2

2From our deﬁnition of decomposable norm  if a vector v belongs to single subspace Mj  then ∥v∥ = ∥v∥∗ = ∥v∥2. The reason
is: By the deﬁnition  if v ∈ Mj  then v = cj aj for some cj > 0  aj ∈ Mj   ∥aj∥∗ = 1  and it has decomposable norm ∥v∥ = cj.
However  we also have ∥v∥∗ = ∥cj aj∥∗ = cj∥aj∥∗ = cj = ∥v∥. The norm equals to its dual norm only if it is ℓ2-norm.

6

where zt = ΠT (xt). Applying (24) to (21) for iteration t + 1  we have

∥xt+1 − ¯xt+1∥2 − ∥xt+2 − ¯xt+2∥2
≥

∥∆xt+1∥2 − λ2
M 2R2

m

M θ( ¯X )2
For iteration t  we have

∥xt+1 − Π (cid:22)A(xt+1)∥2

2 +

∥(cid:26)t+1

− ¯(cid:26)∥2
M 2

.

∥xt − ¯xt∥2 − ∥xt+1 − ¯xt+1∥2 ≥ m

M

∥∆zt∥2

2 +

∥(cid:26)t

− ¯(cid:26)∥2
M 2

. By Lemma 5  adding the two inequalities gives
m

∥xt − ¯xt∥2 − ∥xt+2 − ¯xt+2∥2 ≥
≥

M θ( ¯X )2
M θ( ¯X )2
which yields desired result (18) after arrangement.

m

∥∆xt+1∥2 +
∥∆xt+1∥2 ≥

∥(cid:26)t+1

− ¯(cid:26)∥2
2 +
M 2
∥∆xt+2∥2 

∥∆zt∥2

m
M
M θ( ¯X )2

m

We note that the descent in the ﬁrst two cases is actually even stronger than stated above: from the
proofs  that the distance can be seen to reduce by a ﬁxed constant. This is faster than superlinear
convergence since the ﬁnal solution could then be obtained in a ﬁnite number of steps.

4 Quadratic Convergence of Proximal Newton Method

The key idea of the proof is to re-formulate Prox-Newton update (10) as

zt+1 = arg

z∈T min h(z + ˆy(z)) + gT

t (z − zt) +

∥z − zt∥2

Ht

1
2

where

ˆy(z) = arg
y∈T ⊥

min h(z + y) 

so that we can focus our convergence analysis on z = ΠT (x) as follows.
Lemma 6 (Optimality Condition). For any matrix H satisfying CNSC-T   the update

∆x = arg
d

min h(x + d) + g(x)T d +

∥d∥2

H

1
2

has

F (x + t∆x) − F (x) ≤ −t∥∆z∥2

H + O(t2) 

where ∆z = ΠT (∆x). Furthermore  if x is an optimal solution  ∆x = 0 satisﬁes (27).

(25)

(26)

(27)

(28)

(29)

The following lemma then states that  for Prox-Newton  the function suboptimality is bounded by
only distance in the T space.
Lemma 7. Suppose h(x) and f (x) are Lipschitz-continuous with Lipschitz constants Lh and Lf .
In quadratic convergence phase (deﬁned in Theorem 3)  Proximal Newton Method has

F (xt) − F (¯x) ≤ L∥zt − ¯z∥ 

where L = max{Lh  Lf} and zt = ΠT (xt)  ¯z = ΠT (¯x).
By the above lemma  we have F (xt) − F (¯x) ≤ Lϵ as long as ∥zt − ¯z∥ ≤ ϵ. Therefore  it sufﬁces
to show quadratic convergence of ∥zt − ¯z∥ to guarantee F (xt) − F (¯x) double its precision after
each iteration.
Theorem 3 (Quadratic Convergence of Prox-Newton). For f (x) satisfying CNSC-T with Lipschitz-
continuous second derivative ∇2f (x)  the Proximal Newton update (10) has

where ¯z = ΠT (¯x)  zt = ΠT (xt)  and LH is the Lipschitz constant for ∇2f (x).

∥zt+1 − ¯z∥ ≤ LH

∥zt − ¯z∥2 

2m

7

Proof. Let ¯x be an optimal solution of (1). By Lemma 6  for any PSD matrix H the update ∆¯x = 0
satisﬁes (27)  which means

¯x = proxHt (¯x + ∆¯xnt)  Ht∆¯xnt = −g(¯x).
Then by non-expansiveness of proximal operation (Lemma 2)  we have
t ) − proxHt (¯x + ∆¯xnt)∥Ht
− ∆¯znt

∥xt+1 − ¯x∥Ht = ∥proxHt(xt + ∆xnt
≤ ∥(xt + ∆xnt
= ∥(zt − ¯z) + (∆znt

t

t

t ) − (¯x + ∆¯xnt)∥Ht = ∥(xt − ¯x) + (∆xnt
t )∥Ht .
m∥z∥Ht  (31) leads to
≤ 1√
m
1√
m

∥Ht(zt − ¯z) − Ht(∆znt
∥Ht(zt − ¯z) − (gt

m

=

∥zt − ¯z∥2
2 

Since for z ∈ T   ∥Htz∥2 ≥ √
∥xt+1 − ¯x∥Ht

(30)

− ∆¯xnt)∥Ht

(31)

(32)

(33)

t

− ∆¯znt)∥2
√
− ¯g)∥2 ≤ LH
2
≥ √

m∥zt+1 − ¯z∥2.
√
∥zt − ¯z∥2
2 

where last inequality follows from Lipschitz-continuity of ∇2f (x). Since zt+1  ¯z ∈ T   we have

∥xt+1 − ¯x∥Ht = ∥zt+1 − ¯z∥Ht

Finally  combining (33) with (32) 

where quadratic convergence phase occurs when ∥zt − ¯z∥ <

2m
LH

.

∥zt+1 − ¯z∥2 ≤ LH

2m

5 Numerical Experiments

In this section  we study the convergence behavior of Proximal Gradient method and Proxi-
mal Newton method on high-dimensional real data set with and without the CNSC condition.
In particular  two loss functions — logistic loss L(u  y)=log(1 + exp(−yu)) and ℓ2-hinge loss
L(u  y)=max(1 − yu  0)2 — are used in (3) with ℓ1-regularization h(x) = λ∥x∥1  where both
losses are smooth but only logistic loss has strict convexity that implies the CNSC condition. For
Proximal Newton method we employ an randomized coordinate descent algorithm to solve sub-
problem (10) as in [9]. Figure 5 shows their convergence results of objective value relative to the
optimum on rcv1.1k  subset of a document classiﬁcation data set with dimension d = 10  192 and
number of samples n = 1000. From the ﬁgure one can clearly observe the linear convergence of
Prox-GD and quadratic convergence of Prox-Newton on problem satisfying CNSC  contrasted to
the qualitatively different behavior on problem without CNSC.

Figure 1: objective value (relative to optimum) of Proximal Gradient method (left) and Proximal
Newton method (right) with logistic loss and ℓ2-hinge loss.

Acknowledgement
This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H acknowledges
support from an IBM PhD fellowship. P.R. acknowledges the support of ARO via W911NF-12-1-
0390 and NSF via IIS-1149803  IIS-1320894  IIS-1447574  and DMS-1264033.

8

0.511.522.53x 10610−810−610−410−2iterobjProx−GD logisticL2hinge5101520253010−810−610−410−2100iterobjProx−Newton logisticL2hingeReferences
[1] J. D. Lee  Y. Sun  and M. A. Saunders. Proximal newton-type methods for minimizing compos-

ite functions. In NIPS  2012.

[2] C.-J. Hsieh  M. A. Sustik  I. S. Dhillon  and P. Ravikumar. Sparse inverse covariance estimation

using quadratic approximation. In NIPS 2011.

[3] S. Boyd and L. Vandenberghe  Convex Optimization  Cambridge Univ. Press  Cambridge  U.K. 

2003.

[4] P.-W. Wang and C.-J. Lin. Iteration Complexity of Feasible Descent Methods for Convex Op-
timization. Technical report  Department of Computer Science  National Taiwan University 
Taipei  Taiwan  2013.

[5] A. Agarwal  S. Negahban  and M. Wainwright. Fast Global Convergence Rates of Gradient

Methods for High-Dimensional Statistical Recovery. In NIPS 2010.

[6] K. Hou  Z. Zhou  A. M.-S. So  and Z.-Q. Luo  On the linear convergence of the proximal gra-
dient method for trace norm regularization  in Neural Information Processing Systems (NIPS) 
2013.

[7] L. Xiao and T. Zhang  A proximal-gradient homotopy method for the l1-regularized least-

squares problem  in ICML  2012.

[8] P. Tseng and S. Yun  A coordinate gradient descent method for nonsmooth separable minimiza-

tion  Math. Prog. B. 117 (2009).

[9] G.-X. Yuan  C.-H. Ho  and C.-J. Lin  An improved GLMNET for l1-regularized logistic regres-

sion  Journal of Machine Learning Research  vol. 13  pp. 19992030  2012

[10] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin  LIBLINEAR: A library for
large linear classiﬁcation  Journal of Machine Learning Research  vol. 9  pp. 1871-1874  2008.
[11] Alan J Hoffman. On approximate solutions of systems of linear inequalities. Journal of Re-

search of the National Bureau of Standards  1952.

[12] Tewari  A  Ravikumar  P  and Dhillon  I S. Greedy Algorithms for Structurally Constrained

High Dimensional Problems. In NIPS  2011.

[13] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-

dimensional analysis of M-estimators with decomposable regularizers. In NIPS  2009.

[14] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[15] S. Becker  J. Bobin  and E.J.Candes. Nesta: a fast and accurate ﬁrst-order method for sparse

recovery. SIAM Journal on Imaging Sciences  2011.

[16] Z. Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a

general approach. Annals of Operations Research  46-47:157–178  1993.

[17] Rahul Garg and Rohit Khandekar. Gradient Descent with Sparsiﬁcation: an iterative algorithm

for sparse recovery with restricted isometry property. In ICML 2009.

[18] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In ICML  2009.
[19] Y. E. Nesterov  Gradient Methods for Minimizing Composite Objective Function  CORE re-

port  2007.

[20] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers 

New York  2004

[21] K. Scheinberg  X. Tang. Practical Inexact Proximal Quasi-Newton Method with Global Com-

plexity Analysis. COR@L Technical Report at Lehigh University. arXiv:1311.6547  2013.

9

,Ian En-Hsu Yen
Cho-Jui Hsieh
Pradeep Ravikumar
Inderjit Dhillon
Pan Xu
Quanquan Gu