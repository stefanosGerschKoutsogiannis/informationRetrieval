2018,Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions,An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases  these different distributions can be modeled as different contexts of a single underlying system  in which each distribution corresponds to a different perturbation of the system  or in causal terms  an intervention. We focus on a class of such causal domain adaptation problems  where data for one or more source domains are given  and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph  the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.,Domain Adaptation by Using Causal Inference to

Predict Invariant Conditional Distributions

Sara Magliacane

MIT-IBM Watson AI Lab  IBM Research∗

sara.magliacane@gmail.com

Thijs van Ommen

University of Amsterdam

thijsvanommen@gmail.com

Tom Claassen

Radboud University Nijmegen

tomc@cs.ru.nl

Stephan Bongers

University of Amsterdam
srbongers@gmail.com

Philip Versteeg

University of Amsterdam

p.j.j.p.versteeg@uva.nl

Joris M. Mooij

University of Amsterdam

j.m.mooij@uva.nl

Abstract

An important goal common to domain adaptation and causal inference is to make
accurate predictions when the distributions for the source (or training) domain(s)
and target (or test) domain(s) differ. In many cases  these different distributions
can be modeled as different contexts of a single underlying system  in which each
distribution corresponds to a different perturbation of the system  or in causal terms 
an intervention. We focus on a class of such causal domain adaptation problems 
where data for one or more source domains are given  and the task is to predict the
distribution of a certain target variable from measurements of other variables in one
or more target domains. We propose an approach for solving these problems that
exploits causal inference and does not rely on prior knowledge of the causal graph 
the type of interventions or the intervention targets. We demonstrate our approach
by evaluating a possible implementation on simulated and real world data.

1

Introduction

Predicting unknown values based on observed data is a problem central to many sciences  and well
studied in statistics and machine learning. This problem becomes signiﬁcantly harder if the training
and test data do not have the same distribution  for example because they come from different
domains. Such a distribution shift can happen whenever the circumstances under which the training
data were gathered are different from those for which the predictions are to be made. A rich literature
exists on this problem of domain adaptation  a particular task in the ﬁeld of transfer learning; see
e.g. Quiñonero-Candela et al. [2009]  Pan and Yang [2010] for overviews.
When the domain changes  so may the relations between the different variables under consideration.
While for some sets of variables A  a function f : A → Y learned in one domain may continue
to offer good predictions for Y ∈ Y in a different domain  this may not be true of other sets A(cid:48) of
variables. Causal graphs [e.g.  Pearl  2009  Spirtes et al.  2000] allow us to reason about this in a
principled way when the domains correspond to different external interventions on the system  or
more generally  to different contexts in which a system has been measured. Knowledge of the causal
graph that describes the data generating mechanism  and of which parts of the model are invariant

∗Most of the work was performed while at the University of Amsterdam.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

C1

X1

X2

X3

(a) Causal graph

for {X1}:
(b) No distribution shift
P(Y | X1  C1 = 0) = P(Y | X1  C1 = 1)

(c) Strong distribution shift for {X3}:
P(Y | X3  C1 = 0) (cid:54)= P(Y | X3  C1 = 1)
Figure 1: In this scenario  an intervention C1 leads to a shift of distribution between source domain
and target domain (see also Example 1). Green crosses show source domain data (C1 = 0)  blue
circles show target domain data (C1 = 1). A standard feature selection method that does not take into
account the causal structure  but would use X3 to predict Y := X2 (because X3 is a good predictor
of Y in the source domain)  would obtain extremely biased predictions in the target domain. Using
X1 instead yields less accurate predictions in the source domain  but much more accurate ones in the
target domain.

across the different domains  allows one to transfer knowledge from one domain to the other in order
to address the problem of domain adaptation [Spirtes et al.  2000  Storkey  2009  Schölkopf et al. 
2012  Bareinboim and Pearl  2016].
Over the last years  various methods have been proposed to exploit the causal structure of the data
generating process in order to address certain domain adaptation problems  each relying on different
assumptions. For example  Bareinboim and Pearl [2016] provide theory for identiﬁability under
transfer (“transportability”) assuming that the causal graph is known  that interventions are perfect 
and that the intervention targets are known. Hyttinen et al. [2015] also assume perfect interventions
with known targets but do not rely on complete knowledge of the causal graph  instead inferring
the relevant aspects of it from the data. Rojas-Carulla et al. [2018] make the assumption that if the
conditional distribution of the target given some subset of covariates is invariant across different
source domains  then this conditional distribution must also be the same in the target domain. The
methods proposed in [Schölkopf et al.  2012  Zhang et al.  2013  2015  Gong et al.  2016] all address
challenging settings in which conditional independences that follow from the usual Markov and
faithfulness assumptions alone do not sufﬁce to solve the problem  but additional assumptions on the
data generating process have to be made.
In this work  we will make no such additional assumptions  and address the setting in which both the
causal graph and the intervention types and targets may be (partially) unknown. Our contributions are
the following. We consider a set of relatively weak assumptions that make the problem well-posed.
We propose an approach to solve this class of causal domain adaptation problems that can deal with
the presence of latent confounders. The main idea is to select the subset of features A that leads to
the best predictions of Y in the source domains  while satisfying invariance (i.e.  P(Y | A) is the
same in the source and target domains). To test whether the invariance condition is satisﬁed  we
apply the recently proposed Joint Causal Inference (JCI) framework [Mooij et al.  2018] to exploit
the information provided by multiple domains corresponding to different interventions. The basic
idea is as follows. First  a standard feature selection method is applied to source domains data to ﬁnd
sets of features that are predictive of a target variable  trading off bias and variance  but unaware of
changes in the distribution across domains. A causal inference method then draws conclusions from
all given data about the possible causal graphs  avoiding sets of features for which the predictions
would not transfer to the target domains. We propose a proof-of-concept implementation of our
approach building on a causal discovery algorithm by Hyttinen et al. [2014]. We evaluate the method
on synthetic data and a real-world example.

2 Theory

Before giving a precise deﬁnition of the class of domain adaptation problems that we consider in this
work  we begin with a motivating example.

2

X1X2X3X2Example 1. We are given three variables X1  X2  X3 describing different aspects of a system (for
example  certain blood cell phenotypes in mice). We have observational measurements of these three
variables (the source domain  designated with C1 = 0)  and in addition  measurements of X1 and
X3 under an intervention (the target domain  designated with C1 = 1)  e.g.  in which the mice have
been exposed to a certain drug. The domain adaptation task is to predict the values of Y := X2
in the interventional target domain (i.e.  when C1 = 1). Let us assume for this example that the
causal graph in Figure 1a applies  i.e.  we assume that X2 is affected by X1 and affects X3  while
C1 affects both X1 and X3 (i.e.  the intervention targets the variables X1 and X3). This causal
graph implies P(Y | X1  C1 = 0) = P(Y | X1  C1 = 1). Suppose further that the relation between
X1 and X2 is about equally strong as the relation between X2 and X3  but considerably more noisy.
Then a feature selection method using only available source domain data  and aiming to select the
best subset of features to use for prediction of Y will prefer both {X3} and {X1  X3} over {X1}
(because predicting Y from X1 leads to larger variance than predicting Y from X3  and to a larger
bias than predicting Y from both X1 and X3). However  under the intervention (C1 = 1)  P(Y | X3)
and P(Y | X1  X3) both change 2 so that using those features to predict Y in the target domain could
lead to extreme bias  as illustrated in Figure 1c. Because the conditional distribution of Y given X1
is invariant across domains  as illustrated in Figure 1b  predictions of Y based only on X1 can be
safely transferred to the target domain.

This example provides an instance of a domain adaptation problem where feature selection methods
that do not take into account the causal structure would pick a set of features that does not generalize
to the target domain  and may lead to arbitrarily bad predictions (even asymptotically  as the number
of data points tends to inﬁnity). On the other hand  correctly taking into account the causal structure
and the possible distribution shift from source to target domain allows to upper bound the prediction
error in the target domain  as we will see in Section 2.3.

2.1 Problem Setting

We now formalize the domain adaptation problems that we address in this paper. We will make use of
the terminology of the recently proposed Joint Causal Inference (JCI) framework [Mooij et al.  2018].
Let us consider a system of interest described by a set of system variables {Xj}j∈J . In addition  we
model the domain in which the system has been measured by context variables {Ci}i∈I (we will use
“context” as a synonym for “domain”). We will denote the tuple of all system and context variables
as V = ((Xj)j∈J   (Ci)i∈I). System and context variables can be discrete or continuous. As a
concrete example  the system of interest could be a mouse. The system variables could be blood cell
phenotypes such as the concentration of red blood cells  the concentration of white blood cells  and
the mean red blood cell volume. The context variables could indicate for example whether a certain
gene has been knocked out  the dosage of a certain drug administered to the mice  the age and gender
of the mice  or the lab in which the measurements were done. The important underlying assumption
is that context variables are exogenous to the system  whereas system variables are endogenous. The
interventions are not limited to the perfect (“surgical”) interventions modeled by the do-operator of
Pearl [2009]  but can also be other types of interventions such as mechanism changes [Tian and Pearl 
2001]  soft interventions [Markowetz et al.  2005]  fat-hand interventions [Eaton and Murphy  2007] 
activity interventions [Mooij and Heskes  2013]  and stochastic versions of all these. Knowledge
of the intervention targets is not necessary (but is certainly helpful). For example  administering
a drug to the mice may have a direct causal effect on an unknown subset of the system variables 
but we can simply model it as a binary exogenous variable (indicating whether or not the drug was
administered) or a continuous exogenous variable (describing the dosage of the administered drug)
without specifying in advance on which variables it has a direct effect. We can now formally state the
domain adaptation task that we address in this work:
Task 1 (Domain Adaptation Task). We are given data for a single or for multiple source domains 
in each of which C1 = 0  and for a single or for multiple target domains  in each of which C1 = 1.
Assume the source domains data is complete (i.e.  no missing values)  and the target domains data is
complete with the exception of all values of a certain target variable Y = Xj. The task is to predict
these missing values of the target variable Y given the available source and target domains data.

2More precisely  we should say that P(Y | X3  C1 = 1) may differ from P(Y | X3  C1 = 0)  and similarly

when conditioning on {X1  X3}.

3

Context variables
C1
0
0
0
0
0
1
1
1
1
1

C2
0.1
0.2
0.4
1.5
1.7
0.2
0.1
1.6
1.8
1.7

s
n
i
a
m
o
d

e
c
r
u
o
s

s
n
i
a
m
o
d

t
e
g
r
a
t

System variables
X3
X1
0.1
0.5
0.49
0.13
0.51
0.23
0.52
0.5
0.6
0.51
0.92
0.2
0.99
0.23
0.95
0.53
0.90
0.61
0.55
0.97

X2
0.2
0.21
0.21
0.19
0.18
?
?
?
?
?

Context
variables

System
variables

C1

C2

X1

X2

X3

Figure 2: Example of a causal domain adaptation problem. The causal graph is depicted on the right 
the corresponding data on the left. The task is to predict the missing values of Y = X2 in the target
domains (C1 = 1)  based on the observed data from the source domains and the target domains 
without knowledge of the causal graph. See also Example 2.

An example is provided in Figure 2. In the next subsection  we will formalize our assumptions to
turn this task into a well-posed problem.

2.2 Assumptions

Our ﬁrst main assumption is that the data generating process (on both system and context variables)
can be represented as a Structural Causal Model (SCM) (see e.g.  [Pearl  2009]):

Ci
p(E) =(cid:81)

Xj

M :

= gi(EPA(i)∩K) 
= fj(XPA(j)∩J   CPA(j)∩I  EPA(j)∩K) 

i ∈ I

k∈K p(Ek).

j ∈ J

(1)

Here  we introduced exogenous latent independent “noise” variables (Ek)k∈K that model latent
causes of the context and system variables. The parents of each variable are denoted by PA(·). Each
context and system variable is related to its parent variables by a structural equation. In addition 
we assume a factorizing probability distribution on the exogenous variables. There could be cyclic
dependencies  for example due to feedback loops  but for simplicity of exposition we will discuss
only the acyclic case here  noting that the extension to the cyclic case is straightforward given recent
theoretical advances on cyclic SCMs [Bongers et al.  2018]. This SCM provides a causal model for
the distributions of the various domains  and in particular  it induces a joint distribution P(V ) on
the context and system variables. Note that we will assume that the data generating process can be
modeled by some model of this form  but we do not rely on knowing the precise model.
The SCM M can be represented graphically by its causal graph G(M)  a graph with nodes I ∪ J
(i.e.  the labels of both system and context variables)  directed edges l1 → l2 for l1  l2 ∈ I ∪ J iff
l1 ∈ PA(l2)  and bidirected edges l1 ↔ l2 for l1  l2 ∈ I ∪J iff there exists a k ∈ PA(l1)∩ PA(l2)∩K.
In the acyclic case  this causal graph is an Acyclic Directed Mixed Graph (ADMG)  and M is also
known as a Semi-Markov Causal Model (see e.g.  [Pearl  2009]). The directed edges represent direct
causal relationships  and the bidirected edges may represent hidden confounders (both relative to
the set of variables in the ADMG). The (causal) Markov assumption holds [Richardson  2003]  i.e. 
any d-separation A ⊥ B | S [G(M)] between sets of random variables A  B  S ⊆ V in the ADMG
G(M) implies a conditional independence A⊥⊥ B | S [P(V )] in the distribution P(V ) induced by
the SCM M. A standard assumption in causal discovery is that the joint distribution P(V ) is faithful
with respect to the ADMG G(M)  i.e.  that there are no other conditional independences in the joint
distribution than those implied by d-separation.
We will make the following assumptions on the causal structure (where henceforth we will simply
write G instead of G(M))  which are discussed in detail by Mooij et al. [2018]:
Assumption 1 (JCI Assumptions). Let G be a causal graph with variables V (consisting of system
variables {Xj}j∈J and context variables {Ci}i∈I).
(i) No system variable directly causes any context variable (“exogeneity”)

(ii) No system variable is confounded with a context variable (“randomization”)

(∀j ∈ J  ∀i ∈ I : Xj → Ci /∈ G);
(∀j ∈ J  ∀i ∈ I : Xj ↔ Ci /∈ G);

4

(iii) Every pair of context variables is purely confounded (“genericity”)

(∀i  i(cid:48) ∈ I : Ci ↔ Ci(cid:48) ∈ G ∧ Ci → Ci(cid:48) /∈ G).

The ﬁrst assumption is the most crucial one that captures what we mean by “context”. The other
two assumptions are less crucial and could be omitted  depending on the application. For a more
in-depth discussion of these modeling assumptions and on how they compare with other possible
causal modeling approaches  we refer the reader to [Mooij et al.  2018]. Any causal discovery method
can in principle be used in the JCI setting  but identiﬁability greatly beneﬁts from taking into account
the background knowledge on the causal graph from Assumption 1.
In addition  in order to be able to address the causal domain adaptation task  we will assume:
Assumption 2. Let G be a causal graph with variables V (consisting of system variables {Xj}j∈J
and context variables {Ci}i∈I)  and P(V ) be the corresponding distribution on V . Let C1 be the
source/target domains indicator and Y = Xj the target variable.
(i) The distribution P(V ) is Markov and faithful w.r.t. G;
(ii) Any conditional independence involving Y in the source domains also holds in the target

domains  i.e.  if A ∪ B ∪ S contains Y but not C1 then:3

A⊥⊥ B | S [C1 = 0] =⇒ A⊥⊥ B | S [C1 = 1];

(iii) C1 has no direct effect on Y w.r.t. V   i.e.  C1 → Y /∈ G.

The Markov and faithfulness assumptions are standard in constraint-based causal discovery on a
single domain; we apply them here on the “meta-system” composed of system and context.
Assumption 2(ii) may seem non-intuitive  but as we show in the Supplementary Material  it follows
from more intuitive (but stronger) assumptions  for example if both the pooled source domains
distribution P(V | C1 = 0) and the pooled target domains distribution P(V | C1 = 1) are Markov
and faithful to the subgraph of G which excludes C1. These stronger assumptions imply that the
causal structure (i.e.  presence or absence of causal relationships and confounders) of the other
variables is invariant when going from source to target domains. Assumption 2(ii) is a weakened
version of these more natural assumptions  allowing additional independences to hold in the target
domains compared to the source domains  e.g.  when C1 models a perfect surgical intervention.
Assumption 2(iii) is strong  yet some assumption of that type seems necessary to make the task
well-deﬁned. Without any information at all about the target(s) of C1  or the causal mechanism
that determines the values of Y in the target domains  predicting the values of Y for the target
domains seems generally impossible. Note that the assumption is more likely to be satisﬁed if the
interventions are believed to be precisely targeted  and gets weaker the more relevant system variables
are observed.4
As one example of a real-world setting in which these assumptions are reasonable  consider a
genomics experiment  in which gene expression levels of many different genes are measured in
response to knockouts of single genes. Given our present-day understanding of the biology of gene
expression  it is very reasonable to assume that the knockout of gene Xi only has a direct effect on the
expression level of gene Xi itself. As long as we do not ask to predict the expression level of Xi under
a knockout of Xi  but only the expression level of other genes Y = Xj with j (cid:54)= i  Assumption 2(iii)
seems justiﬁed. It is also reasonable (based on present-day understanding of biology) to expect that a
single gene knockout does not change the causal mechanisms in the rest of the system. This justiﬁes
Assumption 2(ii) in this setting if one is willing to assume faithfulness.
In the next subsections  we will discuss how these assumptions enable us to address the domain
adaptation task.

3Here  with A⊥⊥ B | S [C1 = 0] we mean A⊥⊥ B | S [P(V | C1 = 0)]  i.e.  the conditional independence
of A from B given S in the mixture of the source domains P(V | C1 = 0)  and similarly for the target domains.
4This assumption can be weakened further: in some circumstances one can infer from the data and the other
assumptions that C1 cannot have a direct effect on Y . For example: if there exists a descendant D ∈ DE(Y ) 
and if there exists a set S ⊆ V \ ({C1  Y } ∪ DE(Y ))  such that C1 ⊥⊥ D | S  then C1 is not a direct cause of Y
w.r.t. V . For some proposals on alternative assumptions that can be made when this assumption is violated  see
e.g.  [Schölkopf et al.  2012  Zhang et al.  2013  2015  Gong et al.  2016].

5

risk E(cid:0)(Y − ˆY (A))2 | C1 = 1(cid:1)  and is given by the conditional expectation (regression function)

2.3 Separating Sets of Features
Our approach to addressing Task 1 is based on ﬁnding a separating set A ⊆ V \ {C1  Y } of (context
and system) variables that satisﬁes C1 ⊥ Y | A [G]. If such a separating set A can be found  then the
distribution of Y conditional on A is invariant under transferring from the source domains to the
target domains  i.e.  P(Y | A  C1 = 0) = P(Y | A  C1 = 1). As the former conditional distribution
can be estimated from the source domains data  we directly obtain a prediction for the latter  which
then enables us to predict the values of Y from the observed values of A in the target domains.5
We will now discuss the effect of the choice of A on the quality of the predictions. For simplicity of
the exposition  we make use of the squared loss function and look at the asymptotic case  ignoring
ﬁnite-sample issues. When predicting Y from a subset of features A ⊆ V \ {Y  C1} (that may or
may not be separating)  the optimal predictor is deﬁned as the function ˆY mapping from the range
of possible values of A to the range of possible values of Y that minimizes the target domains
A(a) := E(Y | A = a  C1 = 1). Since Y is not observed in the target domains  we cannot directly
ˆY 1
estimate this regression function from the data.
One approach that is often used in practice is to ignore the difference in distribution between source
A(a) := E(Y | A = a  C1 = 0)  which minimizes
and target domains  and use instead the predictor ˆY 0
A that
we will refer to as the transfer bias (when predicting Y from A). When ignoring that source domains
and target domains have different distributions  any standard machine learning method can be used to
predict Y from A. As the transfer bias can become arbitrarily large (as we have seen in Example 1) 
the prediction accuracy of this solution strategy may be arbitrarily bad (even in the inﬁnite-sample
limit).
Instead  we propose to only predict Y from A when the set A of features satisﬁes the following
separating set property:
(2)
i.e.  it d-separates C1 from Y in G. By the Markov assumption  this implies C1 ⊥⊥ Y | A [P(V )]. In
other words (as already mentioned above)  for separating sets  the distribution of Y conditional on A
is invariant under transferring from the source domains to the target domains  i.e.  P(Y | A  C1 =
0) = P(Y | A  C1 = 1). By virtue of this invariance  regression functions are identical for the source
domains and target domains  i.e.  ˆY 0
A  and hence also the source domains and target domains
risks are identical when using the predictor ˆY 0
A:

the source domains risk E(cid:0)(Y − ˆY )2 | C1 = 0(cid:1). This approximation introduces a bias ˆY 1

C1 ⊥ Y | A [G] 

A − ˆY 0

A = ˆY 1

C1 ⊥ Y | A [G] =⇒ E(cid:0)(Y − ˆY 0

A)2 | C1 = 1(cid:1) = E(cid:0)(Y − ˆY 0

A)2 | C1 = 0(cid:1).

(3)
The r.h.s. can be estimated from the source domains data  and the l.h.s. equals the generalization
error to the target domains when using the predictor ˆY 0
A trained on the source domains (which equals
the predictor ˆY 1
A that one could obtain if all target domains data  including the values of Y   were
observed).6 Although this approach leads to zero transfer bias  it introduces another bias: by using
only a subset of the features A  rather than all available features V \ {C1  Y }  we may miss relevant
V \{Y C1} − ˆY 1
information to predict Y . We refer to this bias as the incomplete information bias  ˆY 1
A.
A to predict Y is the sum of the transfer bias and the incomplete

The total bias when using ˆY 0
information bias:

+ ( ˆY 1

V \{Y C1} − ˆY 1
A)

.

incomplete information bias

(cid:123)(cid:122)

(cid:124)

(cid:125)
V \{Y C1} − ˆY 0
ˆY 1

(cid:123)(cid:122)

A

total bias

= ( ˆY 1

(cid:124)

(cid:123)(cid:122)
A − ˆY 0
A)
transfer bias

(cid:125)

(cid:124)

(cid:125)

For some problems  one may be better off by simply ignoring the transfer bias and minimizing the
incomplete information bias  while for other problems  it is crucial to take the transfer into account to

5This trivial observation is not novel; see e.g. [Ch. 7  p. 164  Spirtes et al.  2000]. It also follows as a special
case of [Theorem 2  Pearl and Bareinboim  2011]. The main novelty of this work is the proposed strategy to
identify such separating sets.

6Note that this equation only holds asymptotically; for ﬁnite samples  in addition to the transfer from source
domains to target domains  we have to deal with the generalization from empirical to population distributions
and from the covariate shift if P(A| C1 = 1) (cid:54)= P(A| C1 = 0) [see e.g. Mansour et al.  2009].

6

obtain small generalization errors. In that situation  we could use any subset A for prediction that
satisﬁes the separating set property (2)  implying zero transfer bias; obviously  the best predictions
are then obtained by selecting a separating subset that also minimizes the source domains risk (i.e. 
minimizes the incomplete information bias). We conclude that this strategy of selecting a subset
A to predict Y may yield an asymptotic guarantee on the prediction error by (3)  whereas simply
ignoring the shift in distribution may lead to unbounded prediction error  since the transfer bias could
be arbitrarily large in the worst case scenario.

2.4

Identiﬁability of Separating Feature Sets

For the strategy of selecting the best separating sets of features as discussed in Section 2.3  we
need to ﬁnd one or more sets A ⊆ V \ {C1  Y } that satisfy (2). Of course  the problem is that we
cannot directly test this in the data  because the values of Y are missing for C1 = 1. Note that also
Assumption 2(ii) cannot be directly used here  because it only applies when C1 is not in A∪B. When
the causal graph G is known  it is easy to verify whether (2) holds directly using d-separation. Here
we address the more challenging setting in which the causal graph and the targets of the interventions
are (partially) unknown.7 Conceptually  one could estimate a set of possible causal graphs by using a
causal discovery algorithm (for example  extending any standard method to deal with the missing
conditional independence tests in C1 = 1)  and then read off separating sets from these graphs. In
practice  it is not necessary to estimate completely these causal graphs: we only need to know enough
about them to verify or falsify whether a given set of features separates C1 from Y . The following
example (with details in the Supplementary Material) illustrates a case where such reasoning allows
us to identify a separating set.
Example 2. Assume that Assumptions 1 and 2 hold for two context variables C1  C2 and three system
variables X1  X2  X3 with Y := X2. If the following conditional (in)dependences all hold in the
source domains:

C2 ⊥⊥ X2 | X1 [C1 = 0] 

C2 (cid:54)⊥⊥ X2 |∅ [C1 = 0] 

C2 ⊥⊥ X3 | X2 [C1 = 0] 

(4)
then C1 ⊥ X2 | X1 [G]  i.e.  {X1} is a separating set for C1 and X2. One possible causal graph
leading to those (in)dependences is provided in Figure 2 (the others are shown in Figure 1c in the
Supplementary Material). For that ADMG  and given enough data  feature selection applied to the
source domains data will generically select {X1  X3} as the optimal set of features for predicting
Y := X2  which can lead to an arbitrarily large prediction error. On the other hand  the set {X1}
is separating in any ADMG satisfying (4)  so using it to predict Y leads to zero transfer bias  and
therefore provides a guarantee on the target domains risk (i.e.  it provides an upper bound on the
optimal target domains risk  which can be estimated from the source domains data).

Rather than characterizing by hand all possible situations in which a separating set can be identiﬁed
(like in Example 2)  in this work we delegate the causal inference to an automatic theorem prover.
Intuitively  the idea is to provide the automatic theorem prover with the conditional (in)dependences
that hold in the data  in combination with an encoding of Assumptions 1 and 2 into logical rules 
and ask the theorem prover whether it can prove that C1 ⊥ Y | A holds for a candidate set A from
the assumptions and provided conditional (in)dependences. There are three possibilities: either it
can prove the query (and then we can proceed to predict Y from A and get an estimate of the target
domains risk)  or it can disprove the query (and then we know A will generically give predictions that
suffer from an arbitrarily large transfer bias)  or it can do neither (in which case hopefully another
subset A can be found that does provably satisfy (2)).

2.5 Algorithm

A simple (brute-force) algorithm that ﬁnds the best separating set as described in Section 2.3 is
the following. By using a standard feature selection method  produce a ranked list of subsets
A ⊆ V \ {Y  C1}  ordered ascendingly with respect to the empirical source domains risks. Going
through this list of subsets (starting with the one with the smallest empirical source domains risk) 
7Another option  proposed by Rojas-Carulla et al. [2018]  is to assume that if p(Y | A) is invariant across all
source domains (i.e.  p(Y | A  C1 = 0  C\1 = c) = p(Y | A  C1 = 0) for all c)  then the same holds across all
source and target domains (i.e.  p(Y | A  C1 = 1) = p(Y | A  C1 = 0  C\1 = c) for all c). This assumption
can be violated in some simple cases  e.g. see Example 2.

7

test whether the separating set property can be inferred from the data by querying the automated
theorem prover. If (2) can be shown to hold  use that subset A for prediction of Y and stop; if not 
continue with the next candidate subset A in the list. If no subset satisﬁes (2)  abstain from making a
prediction.8
An important consequence of Assumption 2(ii) is that it enables us to transfer conditional indepen-
dence involving the target variable from the source domains to the target domains (proof provided in
the Supplementary Material):
Proposition 1. Under Assumption 2 

A⊥⊥ B | S [C1 = 0] ⇐⇒ A⊥⊥ B | S ∪ {C1} ⇐⇒ A ⊥ B | S ∪ {C1} [G]

for subsets A  B  S ⊆ V such that their union contains Y but not C1.
To test the separating set condition (2)  we use the approach proposed by Hyttinen et al. [2014]  where
we simply add the JCI assumptions (Assumption 1) as constraints on the optimization problem  in
addition to the domain-adaptation speciﬁc assumption that C1 → Y /∈ G (Assumption 2(iii)). As
inputs we use all directly testable conditional independence test p-values pA ⊥⊥ B | S in the pooled
data (when Y (cid:54)∈ A ∪ B ∪ S) and all those resulting from Proposition 1 from the source domains
data only (if Y ∈ A ∪ B ∪ S). If background knowledge on intervention targets or the causal graph
is available  it can easily be added as well. We use the method proposed by Magliacane et al. [2016]
to query for the conﬁdence of whether some statement (e.g.  Y ⊥⊥ C1 | A) is true or false. The results
of Magliacane et al. [2016] show that this approach is sound under oracle inputs  and asymptotically
consistent whenever the statistical conditional independence tests used are asymptotically consistent.
In other words  in this way the probability of wrongly deciding whether a subset A is a separating
set converges to zero as the sample size increases. We chose this approach because it is simple to
implement on top of existing open source code.9 Note that the computational cost quickly increases
with the number of variables  limiting the number of variables that can be considered simultaneously.
One remaining issue is how to predict Y when an optimal separating set A has been found. As the
distribution of A may shift when transferring from source domains to target domains  this means that
there is a covariate shift to be taken into account when predicting Y . Any method (e.g.  least-squares
regression) could in principle be used to predict Y from a given set of covariates  but it is advisable
to use a prediction method that works well under covariate shift  e.g.  [Sugiyama et al.  2008].

3 Evaluation

We perform an evaluation on both synthetic data and a real-world dataset based on a causal inference
challenge.10 The latter dataset consists of hematology-related measurements from the International
Mouse Phenotyping Consortium (IMPC)  which collects measurements of phenotypes of mice with
different single-gene knockouts.
In both evaluations we compare a standard feature selection method (which uses Random Forests)
with our method that builds on top of it and selects from its output the best separating set. First  we
score all possible subsets of features by their out-of-bag score using the implementation of Random
Forest Regressor from scikit-learn [Pedregosa et al.  2011] with default parameters. For the
baseline we then select the best performing subset and predict Y . Instead  for our proposed method
we try to ﬁnd a subset of features A that is also a separating set  starting from the subsets with the best
scores. To test whether A is a separating set  we use the method described in Section 2.5  using the
ASP solver clingo 4.5.4 [Gebser et al.  2014]. We provide as inputs the independence test results
from a partial correlation test with signiﬁcance level α = 0.05 and combine it with the weighting
scheme from Magliacane et al. [2016]. We then use the ﬁrst subset A in the ranked list of predictive
sets of features found by the Random Forest method for which the conﬁdence that C1 ⊥ Y | A holds
is positive. If there is no set A that satisﬁes this criterion  then we abstain from making a prediction.

8Abstaining from predictions can be advantageous when trading off recall and precision. If a prediction has
to be made  we can fall back on some other method or simply accept the risk that the transfer bias may be large.
9We build on the source code provided by Magliacane et al. [2016] which in turn extends the source code
provided by Hyttinen et al. [2014]. The full source code of our implementation and the experiments is available
online at https://github.com/caus-am/dom_adapt.

10Part of the CRM workshop on Statistical Causal Inference and Applications to Genetics  Montreal  Canada

(2016). See also http://www.crm.umontreal.ca/2016/Genetics16/competition_e.php

8

(a) Synthetic data with N = 1000 samples
and a large perturbation
Figure 3: Evaluation results (see main text and Supplementary Material for details).

(b) Real-world data

For the synthetic data  we generate randomly 200 linear acyclic models with latent variables and
Gaussian noise  each with three system variables  and sample N data points each for the observational
and two experimental domains  where we simulate soft interventions on randomly selected targets 
with different sizes of perturbations. We randomly select which of the two context variables will be
C1 and which of the three system variables will be Y . We disallow direct effects of C1 on Y   and
enforce that no intervention can directly affect all variables simultaneously. More details on how
the data were simulated are provided in the Supplementary Material. Figure 3a shows a boxplot
of the L2 loss of the predicted Y values with respect to the true values for both the baseline and
our method  considering the 121 cases out of 200 in which our method does produce an answer. In
particular  Figure 3a considers the case of N = 1000 samples per regime and interventions that all
produce a large perturbation. In the Supplementary Material we show that results improve with more
samples  both for the baseline  but even more so for our method  since the quality of the conditional
independence tests improves. We also show that  according to expectations  if the target distribution
is very similar to the source distributions  i.e.  the transfer bias is small  our method does not provide
any beneﬁt and seems to perform worse than the baseline. Conversely  the larger the intervention
effect  the bigger the advantage of using our method.
For the real-world dataset  we select a subset of the variables considered in the CRM Causal Inference
Challenge. Speciﬁcally  for simplicity we focus on 16 phenotypes that are not deterministically
related to each other. The dataset contains measurements for 441 “wild type” mice and for about
10 “mutant” mice for each of 13 different single gene knockouts. We then generate 1000 datasets
by randomly selecting subsets of 3 variables and 2 gene knockout contexts  and always include also
“wild type” mice. For each dataset we randomly choose Y and C1  and leave out the observed values
of Y for C1 = 1. Figure 3b shows a boxplot of the L2 loss of the predicted Y values with respect to
the real values for the baseline and our method. Given the small size of the datasets  this is a very
challenging problem. In this case  our method abstains from making a prediction for 170 cases out of
1000 but performs similarly to the baseline on the remaining cases.

4 Discussion and Conclusion

We have deﬁned a general class of causal domain adaptation problems and proposed a method that
can identify sets of features that lead to transferable predictions. Our assumptions are quite general
and in particular do not require the causal graph or the intervention targets to be known. The method
gives promising results on simulated data. It is straightforward to extend our method to the cyclic
case by making use of the results by Forré and Mooij [2018]. More work remains to be done on
the implementation side  for example  scaling up to more variables. Currently  our approach can
handle about seven variables on a laptop computer  and with recent advances in exact causal discovery
algorithms [e.g.  Rantanen et al.  2018]  a few more variables would be feasible. For scaling up to
dozens of variables  we plan to adapt constraint-based causal discovery algorithms like FCI [Spirtes
et al.  2000] to deal with the missing-data aspect of the domain adaptation task. We hope that this
work will also inspire further research on the interplay between bias  variance and causality from a
statistical learning theory perspective.

9

0.00.10.20.30.40.5Feature selectionOur methodMethodL2 loss0246Feature selectionOur methodMethodL2 lossAcknowledgments

We thank Patrick Forré for proofreading a draft of this work. We thank Renée van Amerongen and
Lucas van Eijk for sharing their domain knowledge about the hematology-related measurements from
the International Mouse Phenotyping Consortium (IMPC). SM  TC  SB  and PV were supported
by NWO  the Netherlands Organization for Scientiﬁc Research (VIDI grant 639.072.410). SM was
also supported by the Dutch programme COMMIT/ under the Data2Semantics project. TC was
also supported by NWO grant 612.001.202 (MoCoCaDi)  and EU-FP7 grant agreement n.603016
(MATRICS). TvO and JMM were supported by the European Research Council (ERC) under the
European Union’s Horizon 2020 research and innovation programme (grant agreement 639466).

References
E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy

of Sciences  113(27):7345–7352  2016.

S. Bongers  J. Peters  B. Schölkopf  and J. M. Mooij. Theoretical aspects of cyclic structural causal models.
arXiv.org preprint  arXiv:1611.06221v2 [stat.ME]  Aug. 2018. URL https://arxiv.org/abs/1611.
06221v2.

D. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In Proceedings of
the Eleventh International Conference on Artiﬁcial Intelligence and Statistics  (AISTATS-07)  volume 2 of
Proceedings of Machine Learning Research  pages 107–114  2007.

P. Forré and J. M. Mooij. Constraint-based causal discovery for non-linear structural causal models with cycles
and latent confounders. In Proceedings of the 34th Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-18)  2018.

M. Gebser  R. Kaminski  B. Kaufmann  and T. Schaub. Clingo = ASP + control: Extended report. Tech-
nical report  University of Potsdam  2014. URL http://www.cs.uni-potsdam.de/wv/pdfformat/
gekakasc14a.pdf.

M. Gong  K. Zhang  T. Liu  D. Tao  C. Glymour  and B. Schölkopf. Domain adaptation with conditional
transferable components. In Proceedings of the 33rd International Conference on Machine Learning (ICML
2016)  volume 48 of JMLR Workshop and Conference Proceedings  pages 2839–2848  2016.

A. Hyttinen  F. Eberhardt  and M. Järvisalo. Constraint-based causal discovery: Conﬂict resolution with
answer set programming. In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial Intelligence 
(UAI-14)  pages 340–349  2014.

A. Hyttinen  F. Eberhardt  and M. Järvisalo. Do-calculus when the true graph is unknown. In Proceedings of the

Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2015)  pages 395–404  2015.

S. Magliacane  T. Claassen  and J. M. Mooij. Ancestral causal inference. In In Proceedings of Advances in

Neural Information Processing Systems  (NIPS-16)  pages 4466–4474  2016.

Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms.

Proceedings of the Twenty-Second Annual Conference on Learning Theory (COLT 2009)  2009.

In

F. Markowetz  S. Grossmann  and R. Spang. Probabilistic soft interventions in conditional Gaussian networks.
In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics  (AISTATS-05) 
pages 214–221  2005.

J. M. Mooij and T. Heskes. Cyclic causal discovery from continuous equilibrium data. In Proceedings of the

29th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-13)  pages 431–439  2013.

J. M. Mooij  S. Magliacane  and T. Claassen. Joint causal inference from multiple contexts. arXiv.org preprint 
https://arxiv.org/abs/1611.10351v3 [cs.LG]  Mar. 2018. URL https://arxiv.org/abs/1611.10351v3.

S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering 

22(10):1345–1359  Oct. 2010.

J. Pearl. Causality: models  reasoning and inference. Cambridge University Press  2009.

J. Pearl and E. Bareinboim. Transportability of causal and statistical relations: A formal approach. In Proceedings

of the Twenty-Fifth AAAI Conference on Artiﬁcial Intelligence  pages 247–254  2011.

10

F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Prettenhofer  R. Weiss 
V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and E. Duchesnay. Scikit-learn:
Machine learning in Python. Journal of Machine Learning Research  12:2825–2830  2011.

J. Quiñonero-Candela  M. Suyiyama  A. Schwaighofer  and N. D. Lawrence  editors. Dataset Shift in Machine

Learning. MIT Press  2009.

K. Rantanen  A. Hyttinen  and M. Järvisalo. Learning optimal causal graphs with exact search. In Proceedings of
the 9th International Conference on Probabilistic Graphical Models (PGM 2018)  volume 72 of Proceedings
of Machine Learning Research  pages 344–355  2018.

T. Richardson. Markov properties for acyclic directed mixed graphs. Scandinavian Journal of Statistics  30:

145–157  2003.

M. Rojas-Carulla  B. Schölkopf  R. Turner  and J. Peters. Invariant models for causal transfer learning. Journal

of Machine Learning Research  19(36):1–34  2018.

B. Schölkopf  D. Janzing  J. Peters  E. Sgouritsa  K. Zhang  and J. M. Mooij. On causal and anticausal learning.
In Proceedings of the 29th International Conference on Machine Learning (ICML 2012)  pages 1255–1262 
2012.

P. Spirtes  C. Glymour  and R. Scheines. Causation  Prediction  and Search. MIT press  2nd edition  2000.

A. Storkey. When training and test sets are different: Characterizing learning transfer. In Dataset Shift in

Machine Learning  chapter 1  pages 3–28. MIT Press  2009.

M. Sugiyama  S. Nakajima  H. Kashima  P. V. Buenau  and M. Kawanabe. Direct importance estimation with
model selection and its application to covariate shift adaptation. In In Proceedings of Advances in Neural
Information Processing Systems (NIPS-08)  pages 1433–1440  2008.

J. Tian and J. Pearl. Causal discovery from changes. In Proceedings of the 17th Conference in Uncertainty in

Artiﬁcial Intelligence  (UAI-01)  2001.

K. Zhang  B. Schölkopf  K. Muandet  and Z. Wang. Domain adaptation under target and conditional shift.
In Proceedings of the 30th International Conference on Machine Learning  volume 28 of Proceedings of
Machine Learning Research  pages 819–827  2013.

K. Zhang  M. Gong  and B. Schölkopf. Multi-source domain adaptation: A causal view. In Proceedings of the

Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence  pages 3150–3157  2015.

11

,Sara Magliacane
Thijs van Ommen
Tom Claassen
Stephan Bongers
Philip Versteeg
Joris Mooij