2016,Interpretable Nonlinear Dynamic Modeling of Neural Trajectories,A central challenge in neuroscience is understanding how neural system implements computation through its dynamics. We propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories. Our model assumes low-dimensional continuous dynamics in a finite volume. It incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories. We show that our model can recover qualitative features of the phase portrait such as attractors  slow points  and bifurcations  while also producing reliable long-term future predictions in a variety of dynamical models and in real neural data.,Interpretable Nonlinear Dynamic Modeling

of Neural Trajectories

Yuan Zhao and Il Memming Park

Department of Neurobiology and Behavior

{yuan.zhao  memming.park}@stonybrook.edu

Department of Applied Mathematics and Statistics

Institute for Advanced Computational Science

Stony Brook University  NY 11794

Abstract

A central challenge in neuroscience is understanding how neural system imple-
ments computation through its dynamics. We propose a nonlinear time series
model aimed at characterizing interpretable dynamics from neural trajectories.
Our model assumes low-dimensional continuous dynamics in a ﬁnite volume. It
incorporates a prior assumption about globally contractional dynamics to avoid
overly enthusiastic extrapolation outside of the support of observed trajectories.
We show that our model can recover qualitative features of the phase portrait such
as attractors  slow points  and bifurcations  while also producing reliable long-
term future predictions in a variety of dynamical models and in real neural data.

1

Introduction

Continuous dynamical systems theory lends itself as a framework for both qualitative and quanti-
tative understanding of neural models [1  2  3  4]. For example  models of neural computation are
often implemented as attractor dynamics where the convergence to one of the attractors represents
the result of computation. Despite the wide adoption of dynamical systems theory in theoretical
neuroscience  solving the inverse problem  that is  reconstructing meaningful dynamics from neural
time series  has been challenging. Popular neural trajectory inference algorithms often assume lin-
ear dynamical systems [5  6] which lack nonlinear features ubiquitous in neural computation  and
typical approaches of using nonlinear autoregressive models [7  8] sometimes produce wild extrap-
olations which are not suitable for scientiﬁc study aimed at conﬁdently recovering features of the
dynamics that reﬂects the nature of the underlying computation.
In this paper  we aim to build an interpretable dynamics model to reverse-engineer the neural imple-
mentation of computation. We assume slow continuous dynamics such that the sampled nonlinear
trajectory is locally linear  thus  allowing us to propose a ﬂexible nonlinear time series model that
directly learns the velocity ﬁeld. Our particular parameterization yields to better interpretations:
identifying ﬁxed points and ghost points are easy  and so is the linearization of the dynamics around
those points for stability and manifold analyses. We further parameterize the velocity ﬁeld using a
ﬁnite number of basis functions  in addition to a global contractional component. These features en-
courage the model to focus on interpolating dynamics within the support of the training trajectories.

2 Model

Consider a general d-dimensional continuous nonlinear dynamical system driven by external input 
(1)

˙x = F (x  u)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

where x ∈ Rd represent the dynamic trajectory  and F : Rd × Rdi → Rd fully deﬁnes the dynamics
in the presence of input drive u ∈ Rdi. We aim to learn the essential part of the dynamics F from a
collection of trajectories sampled at frequency 1/∆.
Our work builds on extensive literature in nonlinear time series modeling. Assuming a separable  lin-
ear input interaction  F (x  u) = Fx(x)+Fu(x)u  a natural nonlinear extension of an autoregressive
model is to use a locally linear expansion of (1) [7  9]:

xt+1 = xt + A(xt)xt + b(xt) + B(xt)ut + ϵt

(2)
where b(x) = Fx(x)∆  A(x) : Rd → Rd×d is the Jacobian matrix of Fx at x scaled by time step
∆  B(x) : Rd → Rd×di is the linearization of Fu around x  and ϵt denotes model mismatch noise of
order O(∆2). For example  {A  B} are parametrized with a radial basis function (RBF) network in
the multivariate RBF-ARX model of [10  7]  and {A  b  B} are parametrized with sigmoid neural
networks in [9]. Note that A(·) is not guaranteed to be the Jacobian of the dynamical system (1)
since A and b also change with x. In fact  the functional form for A(·) is not unique  and a powerful
function approximator for b(·) makes A(·) redundant and over parameterizes the dynamics.
Note that (2) is a subclass of a general nonlinear model:

xt+1 = f (xt) + B(xt)ut + ϵt 

(3)
where f   B are the discrete time solution of Fx  Fu. This form is widely used  and called nonlinear
autoregressive with eXogenous inputs (NARX) model where f assumes various function forms (e.g.
neural network  RBF network [11]  or Volterra series [8]).
We propose to use a speciﬁc parameterization 

xt+1 = xt + g(xt) + B(xt)ut + ϵt
g(xt) = Wgφ(xt) − e−τ 2

xt

(4)

vec(B(xt)) = WBφ(xt)
where φ(·) is a vector of r continuous basis functions 

φ(·) = (φ1(·)  . . .  φ r(·))⊤.

(5)
Note the inclusion of a global leak towards the origin whose rate is controlled by τ 2. The further
away from the origin (and as τ → 0)  the larger the effect of the global contraction. This encodes our
prior knowledge that the neural dynamics are limited to a ﬁnite volume of phase space  and prevents
solutions with nonsensical runaway trajectories.
The function g(x) directly represents the velocity ﬁeld of an underlying smooth dynamics (1)  unlike
f (x) in (3) which can have convoluted jumps. We can even run the dynamics backwards in time 
since the time evolution for small ∆ is reversible (by taking g(xt) ≈ g(xt+1))  which is not possible
for (3)  since f (x) is not necessarily an invertible function.
Fixed points x∗ satisfy g(x∗) + B(x∗)u = 0 for a constant input u. Far away from the ﬁxed points 
dynamics are locally just a ﬂow (rectiﬁcation theorem) and largely uninteresting. The Jacobian in
the absence of input  J = ∂g(x)
∂x provides linearization of the dynamics around the ﬁxed points (via
the Hartman-Grobman theorem)  and the corresponding ﬁxed point is stable if all eigenvalues of J
are negative.
We can further identify ﬁxed points  and ghost points (resulting from disappearance of ﬁxed points
due to bifurcation) from local minima of ∥g∥ with small magnitude. The ﬂow around the ghost
points can be extremely slow [4]  and can exhibit signatures of computation through meta-stable
dynamics [12]. Continuous attractors (such as limit cycles) are also important features of neural dy-
namics which exhibit spontaneous oscillatory modes. We can easily identify attractors by simulating
the model.

3 Estimation

We deﬁne the mean squared error as the loss function

L(Wg  WB  c1...r σ 1...r) =

1
T

T−1!t=0

∥g(xt) + B(xt)ut + xt − xt+1∥2
2 

(6)

2

where we use normalized squared exponential radial basis functions

2

2σ2

exp"−∥z−ci∥2
i #
i=1 exp"−∥z−ci∥2
i #  

2

2σ2

φi(z) =

ϵ +$r

(7)

with centers ci and corresponding kernel width σi. The small constant ϵ = 10−7 is to avoid numeri-
cal 0 in the denominator.
We estimate the parameters {Wg  WB τ  c  σ} by minimizing the loss function through gradient
descent (Adam [13]) implemented within TensorFlow [14]. We initialize the matrices Wg and
WB by truncated standard normal distribution  the centers {ci} by the centroids of the K-means
clustering on the training set  and the kernel width σ by the average euclidean distance between the
centers.

4

Inferring Theoretical Models of Neural Computation

We apply the proposed method to a variety of low-dimensional neural models in theoretical neuro-
science. Each theoretical model is chosen to represent a different mode of computation.

4.1 Fixed point attractor and bifurcation for binary decision-making
Perceptual decision-making and working memory tasks are widely used behavioral tasks where the
tasks typically involve a low-dimensional decision variable  and subjects are close to optimal in their
performance. To understand how the brain implements such neural computation  many competing
theories have been proposed [15  16  17  18  19  20  21]. We implemented the two dimensional
dynamical system from [20] where the ﬁnal decision is represented by two stable ﬁxed points corre-
sponding to each choice. The stimulus strength (coherence) nonlinearly interacts with the dynamics
(see appendix for details)  and biases the choice by increasing the basin of attraction (Fig. 1). We
encode the stimulus strength as a single variable held constant throughout each trajectory as in [20].
The model with 10 basis functions learned the dynamics from 90 training trajectories (30 per coher-
ence c = 0  0.5 −0.5). We visualize the log-speed as colored contours  and the direction component
of the velocity ﬁeld as arrows in Fig. 1. The ﬁxed/ghost points are shown as red dots  which ideally
should be at the crossing of the model nullclines given by solid lines. For each coherence  two novel
starting points were simulated from the true model and the estimated model in Fig. 1. Although the
model was trained with only low or moderate coherence levels where there are 2 stable and 1 unsta-
ble ﬁxed points  it predicts bifurcation at higher coherence and it identiﬁes the ghost point (lower
right panel).
We compare the model (4) to the following “locally linear” (LL) model 

xt+1 =A(xt)xt + B(xt)ut + xt

vec(A(xt)) =WAφ(xt)
vec(B(xt)) =WBφ(xt)

(8)

in terms of training and prediction errors in Table 1. Note that there is no contractional term. We
train both models on the same trajectories described above. Then we simulate 30 trajectories from
the true system and trained models for coherence c = 1 with the same random initial states within the
unit square and calculate the mean squared error between the true trajectories and model-simulated
ones as prediction error. The other parameters are set to the same value as training. The LL model

Table 1: Model errors

Model Training error

(4)
(8)

4.06E-08
2.04E-08

Prediction error: mean (std)
0.002 (0.008)
0.244 (0.816)

has poor prediction on the test set. This is due to unbounded ﬂow out of the phase space where the
training data lies (see Fig. 6 in the supplement).

3

Figure 1: Wong and Wang’s 2D dynamics model for perceptual decision-making [20]. We train the
model with 90 trajectories (uniformly random initial points within the unit square  0.5 s duration 
1 ms time step) with different input coherence levels c = {0  0.5 −0.5} (30 trajectories per coher-
ence). The yellow and green lines are the true nullclines. The black arrows represent the true velocity
ﬁelds (direction only) and the red arrows are model-predicted ones. The black and gray circles are
the true stable and unstable ﬁxed points  while the red ones are local minima of model-prediction
(includes ﬁxed points and slow points). The background contours are model-predicted log∥ d s
d t∥2.
We simulated two 1 s trajectories each for true and learned model dynamics. The trajectories start
from the cyan circles. The blue lines are from the true model and the cyan ones are simulated from
trained models. Note that we do not train our model on trajectories from the bottom right condition
(c = 1).

4

(a)

(c)

(b)

(d)

Figure 2: FitzHugh-Nagumo model. (a) Direction (black arrow) and log-speed (contour) of true ve-
locity ﬁeld. Two blue trajectories starting at the blue circles are simulated from the true system. The
yellow and green lines are nullclines of v and w. The diamond is a spiral point. (b) 2-dimensional
embedding of v model-predicted velocity ﬁeld (red arrow and background contour). The black ar-
rows are true velocity ﬁeld. There are a few model-predicted slow points in light red. The blue
lines are the same trajectories as the ones in (a). The cyan ones are simulated from trained model
withe the same initial states of the blue ones. (c) 100-step prediction every 100 steps using a test
trajectory generated with the same setting as training. (d) 200-step prediction every 200 steps using
a test trajectory driven by sinusoid input with 0.5 standard deviation white Gaussian noise.

4.2 Nonlinear oscillator model

One of the most successful application of dynamical systems in neuroscience is in the biophysical
model of a single neuron. We study the FitzHugh-Nagumo (FHN) model which is a 2-dimensional
reduction of the Hodgkin-Huxley model [3]:

v3
3 − w + I 

˙v = v −
˙w = 0.08(v + 0.7 − 0.8w) 

(9)
(10)

where v is the membrane potential  w is a recovery variable and I is the magnitude of stimulus
current. The FHN has been used to model the up-down states observed in the neural time series of
anesthetized auditory cortex [22].
We train the model with 50 basis functions on 100 simulated trajectories with uniformly random
initial states within the unit square [0  1] × [0  1] and driven by injected current generated from a 0.3
mean and 0.2 standard deviation white Gaussian noise. The duration is 200 and the time step is 0.1.

5

(a)

(b)

Figure 3: (a) Velocity ﬁeld (true: black arrows  model-predicted: red arrows) for both direction and
log-speed; model-predicted ﬁxed points (red circles  solid: stable  transparent: unstable). (b) One
trajectory from the true model (x  y)  and one trajectory from the ﬁtted model (ˆx  ˆy). The trajectory
remains on the circle for both. Both are driven by the same input  and starts at same initial state.

In electrophysiological experiments  we only have access to v(t)  and do not observe the slow re-
covery variable w. Delay embedding allows reconstruction of the phase space under mild condi-
tions [23]. We build a 2D model by embedding v(t) as (v(t)  v(t − 10))  and ﬁt the dynamical
model (Fig. 2b). The phase space is distorted  but the overall prediction of the model is good given
a ﬁxed current (Fig. 2b). Furthermore  the temporal simulation of v(t) for white noise injection
shows reliable long-term prediction (Fig. 2c). We also test the model in a regime far from the train-
ing trajectories  and the dynamics does not diverge away from reasonable region of the phase space
(Fig. 2d).

4.3 Ring attractor dynamics for head direction network

Continuous attractors such as line and ring attractors are often used as models for neural represen-
tation of continuous variables [17  4]. For example  the head direction neurons are tuned for the
angle of the animal’s head direction  and a bump attractor network with ring topology is proposed
as the dynamics underlying the persistently active set of neurons [24]. Here we use the following 2
variable reduction of the ring attractor system:

τr ˙r = r0 − r 
τθ ˙θ = I(t) 

(11)
(12)

where θ represents the head direction driven by input I(t)  and r is the radial component representing
the overall activity in the bump. The computational role of this ring attractor is to be insensitive to the
noise in the r direction  while integrating the differential input in the θ direction. In the absence of
input  the head direction θ does a random walk around the ring attractor. The ring attractor consists
of a continuum of stable ﬁxed points with a center manifold.
We train the model with 50 basis functions on 150 trajectories. The duration is 5 and the time step
is 0.01. The parameters are set as r0 = 2  τr = 1 and τθ = 1. The initial states are uniformly
random within (x  y) ∈ [−3  3] × [−3  3]. The inputs are constant angles evenly spaced in [−π  π ]
with Gaussian noises (µ = 0 σ = 5) added (see Fig. 7 in online supplement).
From the trained model  we can identify a number of ﬁxed points arranged around the ring attractor
(Fig. 3a). The true ring dynamics model has one negative eigenvalue  and one zero-eigenvalue in the
Jacobian. Most of the model-predicted ﬁxed points are stable (two negative real parts of eigenvalues)
and the rest are unstable (two positive real parts of eigenvalues).

6

Figure 4: (a) Vector plot of 1-step-ahead prediction on one Lorenz trajectory (test). (b) 50-step
prediction every 50 steps on one Lorenz trajectory (test). (c) A 200-step window of (b) (100-300).
The dashed lines are the true trajectory  the solid lines are the prediction and the circles are the start
points of prediction.

4.4 Chaotic dynamics
Chaotic dynamics (or near chaos) has been postulated to support asynchronous states in the cor-
tex [1]  and neural computation over time by generating rich temporal patterns [2  25]. We consider
the 3D Lorenz attractor as an example chaotic system. We simulate 20 trajectories from 

˙x = 10(y − x) 
˙y = x(28 − z) − y 
˙z = xy −

8
3

z.

(13)

The initial state of each trajectory is standard normal. The duration is 200 and the time step is 0.04.
The ﬁrst 300 transient states of each trajectory are discarded. We use 19 trajectories for training and
the last one for testing. We train a model with 10 basis functions. Figure 4a shows the direction
of prediction. The vectors represented by the arrows start from current states and point at the next
future state. The predicted vectors (red) overlap the true vectors (blue) implying the one-step-ahead
predictions are close to the true values in both speed and direction. Panel (b) gives an overview that
the prediction resembles the true trajectory. Panel (c) shows that the prediction is close to the true
value up to 200 steps.

5 Learning V1 neural dynamics

To test the model on data obtained from cortex  we use a set of trajectories obtained from the
variational Gaussian latent process (vLGP) model [26]. The latent trajectory model infers a 5-
dimensional trajectory that describes a large scale V1 population recording (see [26] for details). The
recording was from an anesthetized monkey where 72 different equally spaced directional drifting
gratings were presented for 50 trials each. We used 63 well tuned neurons out of 148 simultaneously
recorded single units. Each trial lasts for 2.56 s and the stimulus was presented only during the ﬁrst
half.
We train our model with 50 basis functions on the trial-averaged trajectories for 71 directions  and
use 1 direction for testing. The input was 3 dimensional: two boxcars indicating the stimulus direc-
tion (sin θ  cos θ)  and one corresponding to a low-pass ﬁltered stimulus onset indicator. Figure 5
shows the prediction of the best linear dynamical system (LDS) for the 71 directions  and the non-
linear prediction from our model. LDS is given as xt+1 = Axt + But + xt with parameters A and
B found by least squares. Although the LDS is widely used for smoothing the latent trajectories  it
clearly is not a good predictor for the nonlinear trajectory of V1 (Fig. 5a). In comparison  our model
does a better job at capturing the oscillations much better  however  it fails to capture the ﬁne details
of the oscillation and the stimulus-off period dynamics.

7

(a) LDS prediction

(b) Proposed model prediction

Figure 5: V1 latent dynamics prediction. Models trained on 71 average trajectories for each direc-
tional motion are tested on the 1 unseen direction. We divide the average trajectory at 0◦ into 200
ms segments and predict each whole segment from the starting point of the segment. Note the poor
predictive performance of linear dynamical system (LDS) model.

6 Discussion

To connect dynamical theories of neural computation with neural time series data  we need to be
able to ﬁt an expressive model to the data that robustly predicts well. The model then needs to
be interpretable such that signatures of neural computation from the theories can be identiﬁed by
its qualitative features. We show that our method successfully learns low-dimensional dynamics in
contrast to ﬁtting a high-dimensional recurrent neural network models in previous approaches [17 
4  25]. We demonstrated that our proposed model works well for well known dynamical models
of neural computation with various features: chaotic attractor  ﬁxed point dynamics  bifurcation 
line/ring attractor  and a nonlinear oscillator. In addition  we also showed that it can model nonlinear
latent trajectories extracted from high-dimensional neural time series.
Critically  we assumed that the dynamics consists of a continuous and slow ﬂow. This allowed us
to parameterize the velocity ﬁeld directly  reducing the complexity of the nonlinear function approx-
imation  and making it easy to identify the ﬁxed/slow points. An additional structural assumption
was the existence of a global contractional dynamics. This regularizes and encourages the dynamics
to occupy a ﬁnite phase volume around the origin.
Previous strategies of visualizing arbitrary trajectories from a nonlinear system such as recurrence
plots were often difﬁcult to understand. We visualized the dynamics using the velocity ﬁeld decom-
posed into speed and direction  and overlaid ﬁxed/slow points found numerically as local minima
of the speed. This is obviously more difﬁcult for higher-dimensional dynamics  and dimensionality
reduction and visualization that preserves essential dynamic features are left for future directions.
The current method is a two-step procedure for analyzing neural dynamics: ﬁrst infer the latent
trajectories  and then infer the dynamic laws. This is clearly not an inefﬁcient inference  and the next
step would be to combine vLGP observation model and inference algorithm with the interpretable
dynamic model and develop a uniﬁed inference system.
In summary  we present a novel complementary approach to studying the neural dynamics of neural
computation. Applications of the proposed method are not limited to neuroscience  but should
be useful for studying other slow low-dimensional nonlinear dynamical systems from observa-
tions [27].

Acknowledgment

We thank the reviewers for their constructive feedback. This work was partially supported by the
Thomas Hartman Foundation for Parkinson’s Research.

8

References
[1] D. Hansel and H. Sompolinsky. Synchronization and computation in a chaotic neural network. Physical

Review Letters  68(5):718–721  Feb 1992.

[2] W. Maass  T. Natschläger  and H. Markram. Real-time computing without stable states: A new framework

for neural computation based on perturbations. Neural Computation  14:2531–2560  2002.

[3] E. M. Izhikevich. Dynamical systems in neuroscience : the geometry of excitability and bursting. Com-

putational neuroscience. MIT Press  2007.

[4] D. Sussillo and O. Barak. Opening the black box: Low-Dimensional dynamics in High-Dimensional

recurrent neural networks. Neural Computation  25(3):626–649  December 2012.

[5] L. Paninski  Y. Ahmadian  D. G. G. Ferreira  et al. A new look at state-space models for neural data.

Journal of computational neuroscience  29(1-2):107–126  August 2010.

[6] J. P. Cunningham and B. M. Yu. Dimensionality reduction for large-scale neural recordings. Nat Neurosci 

17(11):1500–1509  November 2014.

[7] T. Ozaki. Time Series Modeling of Neuroscience Data. CRC Press  January 2012.
[8] S. Eikenberry and V. Marmarelis. A nonlinear autoregressive volterra model of the HodgkinHuxley equa-

tions. Journal of Computational Neuroscience  34(1):163–183  August 2013.

[9] M. Watter  J. Springenberg  J. Boedecker  and M. Riedmiller. Embed to control: A locally linear latent
dynamics model for control from raw images. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama 
and R. Garnett  editors  Advances in Neural Information Processing Systems 28  pages 2746–2754. Curran
Associates  Inc.  2015.

[10] M. Gan  H. Peng  X. Peng  X. Chen  and G. Inoussa. A locally linear RBF network-based state-dependent
Information Sciences  180(22):4370–4383  November

AR model for nonlinear time series modeling.
2010.

[11] S. Chen  S. A. Billings  C. F. N. Cowan  and P. M. Grant. Practical identiﬁcation of NARMAX models

using radial basis functions. International Journal of Control  52(6):1327–1350  December 1990.

[12] M. I. Rabinovich  R. Huerta  P. Varona  and V. S. Afraimovich. Transient cognitive dynamics  metastabil-

ity  and decision making. PLoS Computational Biology  4(5):e1000072+  May 2008.

[13] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980  2014.
[14] M. Abadi  A. Agarwal  P. Barham  et al. TensorFlow: Large-scale machine learning on heterogeneous

systems  2015. Software available from tensorﬂow.org.

[15] O. Barak  D. Sussillo  R. Romo  M. Tsodyks  and L. F. Abbott. From ﬁxed points to chaos: three models

of delayed discrimination. Progress in neurobiology  103:214–222  April 2013.

[16] C. K. Machens  R. Romo  and C. D. Brody. Flexible control of mutual inhibition: A neural model of

Two-Interval discrimination. Science  307(5712):1121–1124  February 2005.

[17] V. Mante  D. Sussillo  K. V. Shenoy  and W. T. Newsome. Context-dependent computation by recurrent

dynamics in prefrontal cortex. Nature  503(7474):78–84  November 2013.

[18] S. Ganguli  J. W. Bisley  J. D. Roitman  et al. One-dimensional dynamics of attention and decision making

in LIP. Neuron  58(1):15–25  April 2008.

[19] M. E. Mazurek  J. D. Roitman  J. Ditterich  and M. N. Shadlen. A role for neural integrators in perceptual

decision making. Cerebral Cortex  13(11):1257–1269  November 2003.

[20] K.-F. Wong and X.-J. Wang. A recurrent network mechanism of time integration in perceptual decisions.

The Journal of Neuroscience  26(4):1314–1328  January 2006.

[21] M. S. Goldman. Memory without feedback in a neural network. Neuron  61(4):621–634  February 2009.
[22] C. Curto  S. Sakata  S. Marguet  V. Itskov  and K. D. Harris. A simple model of cortical dynamics
explains variability and state dependence of sensory responses in Urethane-Anesthetized auditory cortex.
The Journal of Neuroscience  29(34):10600–10612  August 2009.

[23] H. Kantz and T. Schreiber. Nonlinear Time Series Analysis. Cambridge University Press  2003.
[24] A. Peyrache  M. M. Lacroix  P. C. Petersen  and G. Buzsaki. Internally organized mechanisms of the head

direction sense. Nature Neuroscience  18(4):569–575  March 2015.

[25] R. Laje and D. V. Buonomano. Robust timing and motor patterns by taming chaos in recurrent neural

networks. Nat Neurosci  16(7):925–933  July 2013.

[26] Y. Zhao and I. M. Park. Variational latent Gaussian process for recovering single-trial dynamics from

population spike trains. ArXiv e-prints  April 2016.

[27] B. C. Daniels and I. Nemenman. Automated adaptive inference of phenomenological dynamical models.

Nature Communications  6:8133+  August 2015.

9

,Yuan Zhao
Il Memming Park
Seunghyun Park
Seonwoo Min
Hyun-Soo Choi
Sungroh Yoon