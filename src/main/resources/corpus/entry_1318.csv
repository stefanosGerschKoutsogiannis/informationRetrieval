2017,Deconvolutional Paragraph Representation Learning,Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However  the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence  purely convolutional and deconvolutional autoencoding framework that is free of the above issue  while also being computationally efficient. The proposed method is simple  easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs  our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.,Deconvolutional Paragraph Representation Learning

Yizhe Zhang

Dinghan Shen

Guoyin Wang

Zhe Gan

Ricardo Henao

Department of Electrical & Computer Engineering  Duke University

Lawrence Carin

Abstract

Learning latent representations from long text sequences is an important ﬁrst step
in many natural language processing applications. Recurrent Neural Networks
(RNNs) have become a cornerstone for this challenging task. However  the qual-
ity of sentences during RNN-based decoding (reconstruction) decreases with the
length of the text. We propose a sequence-to-sequence  purely convolutional and
deconvolutional autoencoding framework that is free of the above issue  while
also being computationally efﬁcient. The proposed method is simple  easy to
implement and can be leveraged as a building block for many applications. We
show empirically that compared to RNNs  our framework is better at reconstruct-
ing and correcting long paragraphs. Quantitative evaluation on semi-supervised
text classiﬁcation and summarization tasks demonstrate the potential for better
utilization of long unlabeled text data.

1

Introduction

A central task in natural language processing is to learn representations (features) for sentences or
multi-sentence paragraphs. These representations are typically a required ﬁrst step toward more
applied tasks  such as sentiment analysis [1  2  3  4]  machine translation [5  6  7]  dialogue systems
[8  9  10] and text summarization [11  12  13]. An approach for learning sentence representations
from data is to leverage an encoder-decoder framework [14]. In a standard autoencoding setup  a
vector representation is ﬁrst encoded from an embedding of an input sequence  then decoded to the
original domain to reconstruct the input sequence. Recent advances in Recurrent Neural Networks
(RNNs) [15]  especially Long Short-Term Memory (LSTM) [16] and variants [17]  have achieved
great success in numerous tasks that heavily rely on sentence-representation learning.
RNN-based methods typically model sentences recursively as a generative Markov process with
hidden units  where the one-step-ahead word from an input sentence is generated by conditioning on
previous words and hidden units  via emission and transition operators modeled as neural networks.
In principle  the neural representations of input sequences aim to encapsulate sufﬁcient information
about their structure  to subsequently recover the original sentences via decoding. However  due to the
recursive nature of the RNN  challenges exist for RNN-based strategies to fully encode a sentence into
a vector representation. Typically  during training  the RNN generates words in sequence conditioning
on previous ground-truth words  i.e.  teacher forcing training [18]  rather than decoding the whole
sentence solely from the encoded representation vector. This teacher forcing strategy has proven
important because it forces the output sequence of the RNN to stay close to the ground-truth sequence.
However  allowing the decoder to access ground truth information when reconstructing the sequence
weakens the encoder’s ability to produce self-contained representations  that carry enough information
to steer the decoder through the decoding process without additional guidance. Aiming to solve
this problem  [19] proposed a scheduled sampling approach during training  which gradually shifts
from learning via both latent representation and ground-truth signals to solely use the encoded latent
representation. Unfortunately  [20] showed that scheduled sampling is a fundamentally inconsistent

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

training strategy  in that it produces largely unstable results in practice. As a result  training may fail
to converge on occasion.
During inference  for which ground-truth sentences are not available  words ahead can only be gener-
ated by conditioning on previously generated words through the representation vector. Consequently 
decoding error compounds proportional to the length of the sequence. This means that generated
sentences quickly deviate from the ground-truth once an error has been made  and as the sentence
progresses. This phenomenon was coined exposure bias in [19].
We propose a simple yet powerful purely convolutional framework for learning sentence representa-
tions. Conveniently  without RNNs in our framework  issues connected to teacher forcing training
and exposure bias are not relevant. The proposed approach uses a Convolutional Neural Network
(CNN) [21  22  23] as encoder and a deconvolutional (i.e.  transposed convolutional) neural network
[24  25] as decoder. To the best of our knowledge  the proposed framework is the ﬁrst to force
the encoded latent representation to capture information from the entire sentence via a multi-layer
CNN speciﬁcation  to achieve high reconstruction quality without leveraging RNN-based decoders.
Our multi-layer CNN allows representation vectors to abstract information from the entire sentence 
irrespective of order or length  making it an appealing choice for tasks involving long sentences or
paragraphs. Further  since our framework does not involve recursive encoding or decoding  it can
be very efﬁciently parallelized using convolution-speciﬁc Graphical Process Unit (GPU) primitives 
yielding signiﬁcant computational savings compared to RNN-based models.

2 Convolutional Auto-encoding for Text Modeling

2.1 Convolutional encoder

c

Let wt denote the t-th word in a given sentence. Each word wt is embedded into a k-dimensional
word vector xt = We[wt]  where We ∈ Rk×V is a (learned) word embedding matrix  V is the
vocabulary size  and We[v] denotes the v-th column of We. All columns of We are normalized
to have unit (cid:96)2-norm  i.e.  ||We[v]||2 = 1 ∀v  by dividing each column with its (cid:96)2-norm. After
embedding  a sentence of length T (padded where necessary) is represented as X ∈ Rk×T   by
concatenating its word embeddings  i.e.  xt is the t-th column of X.
For sentence encoding  we use a CNN architecture similar to [26]  though originally proposed for
image data. The CNN consists of L layers (L − 1 convolutional  and the Lth fully-connected) that
ultimately summarize an input sentence into a (ﬁxed-length) latent representation vector  h. Layer
l ∈ {1  . . .   L} consists of pl ﬁlters  learned from data. For the i-th ﬁlter in layer 1  a convolutional
∈ Rk×h to X  where h is the convolution ﬁlter
operation with stride length r(1) applies ﬁlter W(i 1)
c + b(i 1)) ∈ R(T−h)/r(1)+1  where γ(·) is
size. This yields latent feature map  c(i 1) = γ(X ∗ W(i 1)
a nonlinear activation function  b(i 1) ∈ R(T−h)/r(1)+1  and ∗ denotes the convolutional operator. In
our experiments  γ(·) is represented by a Rectiﬁed Linear Unit (ReLU) [27]. Note that the original
embedding dimension  k  changes after the ﬁrst convolutional layer  as c(i 1) ∈ R(T−h)/r(1)+1 
for i = 1  . . .   p1. Concatenating the results from p1 ﬁlters (for layer 1)  results in feature map 
C(1) = [c(1 1) . . . c(p1 1)] ∈ Rp1×[(T−h)/r(1)+1].
After this ﬁrst convolutional layer  we apply the convolution operation to the feature map  C(1)  using
the same ﬁlter size  h  with this repeated in sequence for L − 1 layers. Each time  the length along
the spatial coordinate is reduced to T (l+1) = (cid:98)(T (l) − h)/r(l) + 1(cid:99)  where r(l) is the stride length 
T (l) is the spatial length  l denotes the l-th layer and (cid:98)·(cid:99) is the ﬂoor function. For the ﬁnal layer  L 
the feature map C(L−1) is fed into a fully-connected layer  to produce the latent representation h.
Implementation-wise  we use a convolutional layer with ﬁlter size equals to T (L−1) (regardless of
h)  which is equivalent to a fully-connected layer; this implementation trick has been also utilized in
[26]. This last layer summarizes all remaining spatial coordinates  T (L−1)  into scalar features that
encapsulate sentence sub-structures throughout the entire sentence characterized by ﬁlters  {W(i l)
}
for i = 1  . . .   pl and l = 1  . . .   L  where W(i l)
denotes ﬁlter i for layer l. This also implies that
the extracted feature is of ﬁxed-dimensionality  independent of the length of the input sentence.

c

c

2

Figure 1: Convolutional auto-encoding architecture. Encoder: the input sequence is ﬁrst expanded to an
embedding matrix  X  then fully compressed to a representation vector h  through a multi-layer convolutional
encoder with stride. In the last layer  the spatial dimension is collapsed to remove the spatial dependency.
Decoder: the latent vector h is fed through a multi-layer deconvolutional decoder with stride to reconstruct X as
ˆX  via cosine-similarity cross-entropy loss.

Having pL ﬁlters on the last layer  results in pL-dimensional representation vector  h = C(L)  for the
input sentence. For example  in Figure 1  the encoder consists of L = 3 layers  which for a sentence
of length T = 60  embedding dimension k = 300  stride lengths {r(1)  r(2)  r(3)} = {2  2  1}  ﬁlter
sizes h = {5  5  12} and number of ﬁlters {p1  p2  p3} = {300  600  500}  results in intermediate
feature maps  C(1) and C(2) of sizes {28 × 300  12 × 600}  respectively. The last feature map of
size 1 × 500  corresponds to latent representation vector  h.
Conceptually  ﬁlters from the lower layers capture primitive sentence information (h-grams  analo-
gous to edges in images)  while higher level ﬁlters capture more sophisticated linguistic features  such
as semantic and syntactic structures (analogous to image elements). Such a bottom-up architecture
models sentences by hierarchically stacking text segments (h-grams) as building blocks for repre-
sentation vector  h. This is similar in spirit to modeling linguistic grammar formalisms via concrete
syntax trees [28]  however  we do not pre-specify a tree structure based on some syntactic structure
(i.e.  English language)  but rather abstract it from data via a multi-layer convolutional network.
2.2 Deconvolutional decoder
We apply the deconvolution with stride (i.e.  convolutional transpose)  as the conjugate operation of
convolution  to decode the latent representation  h  back to the source (discrete) text domain. As
the deconvolution operation proceeds  the spatial resolution gradually increases  by mirroring the
convolutional steps described above  as illustrated in Figure 1. The spatial dimension is ﬁrst expanded
to match the spatial dimension of the (L − 1)-th layer of convolution  then progressively expanded as
T (l+1) = (T (l) − 1) ∗ r(l) + h  for l = 1 ··· up to L-th deconvolutional layer (which corresponds to
the input layer of the convolutional encoder). The output of the L-layer deconvolution operation aims
to reconstruct the word embedding matrix  which we denote as ˆX. In line with word embedding
matrix We  columns of ˆX are normalized to have unit (cid:96)2-norm.
Denoting ˆwt as the t-th word in reconstructed sentence ˆs  the probability of ˆwt to be word v is
speciﬁed as

p( ˆwt = v) =

 

(1)

(cid:80)

exp[τ−1Dcos(ˆxt  We[v])]
v(cid:48)∈V exp[τ−1Dcos(ˆxt  We[v(cid:48)])]

(cid:104)x y(cid:105)
where Dcos(x  y) is the cosine similarity deﬁned as 
||x||||y||  We[v] is the v-th column of We 
ˆxt is the t-th column of ˆX  τ is a positive number we denote as temperature parameter [29]. This
parameter is akin to the concentration parameter of a Dirichlet distribution  in that it controls the
spread of probability vector [p( ˆwt = 1) . . . p( ˆwt = V )]  thus a large τ encourages uniformly
distributed probabilities  whereas a small τ encourages sparse  concentrated probability values. In
the experiments we set τ = 0.01. Note that in our setting  the cosine similarity can be obtained
as an inner product  provided that columns of We and ˆX have unit (cid:96)2-norm by speciﬁcation. This
deconvolutional module can also be leveraged as building block in VAE[30  31] or GAN[32  33]

3

300x	60Deconvolution	LayersConvolution	Layers(k h p1 r(1))(300 5 300 2)28x30012x600(kxT)300600500600300(T(1)xp1)(T(2)xp2)(T(2)xp2)12x60028x300(T(1)xp1)300x	60(kxT)(k h p2 r(2))(1 5 600 2)(k T(2) p3 r(3))(1 12 500 1)C(1)C(2)2.3 Model learning

The objective of the convolutional autoencoder described above can be written as the word-wise
log-likelihood for all sentences s ∈ D  i.e. 

Lae =(cid:80)

d∈D(cid:80)

d = wt

d)  

t log p( ˆwt

(2)
where D denotes the set of observed sentences. The simple  maximum-likelihood objective in (2)
is optimized via stochastic gradient descent. Details of the implementation are provided in the
experiments. Note that (2) differs from prior related work in two ways: i) [22  34] use pooling and
un-pooling operators  while we use convolution/deconvolution with stride; and ii) more importantly 
[22  34] do not use a cosine similarity reconstruction as in (1)  but a RNN-based decoder. A further
discussion of related work is provided in Section 3. We could use pooling and un-pooling instead
of striding (a particular case of deterministic pooling/un-pooling)  however  in early experiments
(not shown) we did not observe signiﬁcant performance gains  while convolution/deconvolution
operations with stride are considerably more efﬁcient in terms of memory footprint. Compared to a
standard LSTM-based RNN sequence autoencoders with roughly the same number of parameters 
computations in our case are considerably faster (see experiments) using single NVIDIA TITAN X
GPU. This is due to the high parallelization efﬁciency of CNNs via cuDNN primitives [35].

Comparison between deconvolutional and RNN Decoders The proposed framework can be seen
as a complementary building block for natural language modeling. Contrary to the standard LSTM-
based decoder  the deconvolutional decoder imposes in general a less strict sequence dependency
compared to RNN architectures. Speciﬁcally  generating a word from an RNN requires a vector of
hidden units that recursively accumulate information from the entire sentence in an order-preserving
manner (long-term dependencies are heavily down-weighted)  while for a deconvolutional decoder 
the generation only depends on a representation vector that encapsulates information from throughout
the sentence without a pre-speciﬁed ordering structure. As a result  for language generation tasks  a
RNN decoder will usually generate more coherent text  when compared to a deconvolutional decoder.
On the contrary  a deconvolutional decoder is better at accounting for distant dependencies in long
sentences  which can be very beneﬁcial in feature extraction for classiﬁcation and text summarization
tasks.

2.4 Semi-supervised classiﬁcation and summarization

Identifying related topics or sentiments  and abstracting (short) summaries from user generated content
such as blogs or product reviews  has recently received signiﬁcant interest [1  3  4  36  37  13  11]. In
many practical scenarios  unlabeled data are abundant  however  there are not many practical cases
where the potential of such unlabeled data is fully realized. Motivated by this opportunity  here we
seek to complement scarcer but more valuable labeled data  to improve the generalization ability of
supervised models. By ingesting unlabeled data  the model can learn to abstract latent representations
that capture the semantic meaning of all available sentences irrespective of whether or not they are
labeled. This can be done prior to the supervised model training  as a two-step process. Recently 
RNN-based methods exploiting this idea have been widely utilized and have achieved state-of-the-art
performance in many tasks [1  3  4  36  37]. Alternatively  one can learn the autoencoder and classiﬁer
jointly  by specifying a classiﬁcation model whose input is the latent representation  h; see for
instance [38  31].
In the case of product reviews  for example  each review may contain hundreds of words. This poses
challenges when training RNN-based sequence encoders  in the sense that the RNN has to abstract
information on-the-ﬂy as it moves through the sentence  which often leads to loss of information 
particularly in long sentences [39]. Furthermore  the decoding process uses ground-truth information
during training  thus the learned representation may not necessarily keep all information from the
input text that is necessary for proper reconstruction  summarization or classiﬁcation.
We consider applying our convolutional autoencoding framework to semi-supervised learning from
long-sentences and paragraphs. Instead of pre-training a fully unsupervised model as in [1  3]  we cast
the semi-supervised task as a multi-task learning problem similar to [40]  i.e.  we simultaneously train
a sequence autoencoder and a supervised model. In principle  by using this joint training strategy 
the learned paragraph embedding vector will preserve both reconstruction and classiﬁcation ability.

4

Speciﬁcally  we consider the following objective:
t log p( ˆwt

d∈{Dl+Du}(cid:80)

Lsemi = α(cid:80)

d) +(cid:80)

Lsup(f (hd)  yd)  

d∈Dl

d = wt

(3)
where α > 0 is an annealing parameter balancing the relative importance of supervised and unsu-
pervised loss; Dl and Du denote the set of labeled and unlabeled data  respectively. The ﬁrst term
in (3) is the sequence autoencoder loss in (2) for the d-th sequence. Lsup(·) is the supervision loss
for the d-th sequence (labeled only). The classiﬁer function  f (·)  that attempts to reconstruct yd
from hd can be either a Multi-Layer Perceptron (MLP) in classiﬁcation tasks  or a CNN/RNN in text
summarization tasks. For the latter  we are interested in a purely convolutional speciﬁcation  however 
we also consider an RNN for comparison. For classiﬁcation  we use a standard cross-entropy loss 
and for text summarization we use either (2) for the CNN or the standard LSTM loss for the RNN.
In practice  we adopt a scheduled annealing strategy for α as in [41  42]  rather than ﬁxing it a
priori as in [1]. During training  (3) gradually transits from focusing solely on the unsupervised
sequence autoencoder to the supervised task  by annealing α from 1 to a small positive value αmin.
We set αmin = 0.01 in the experiments. The motivation for this annealing strategy is to ﬁrst focus on
abstracting paragraph features  then to selectively reﬁne learned features that are most informative to
the supervised task.

3 Related Work

Previous work has considered leveraging CNNs as encoders for various natural language processing
tasks [22  34  21  43  44]. Typically  CNN-based encoder architectures apply a single convolution
layer followed by a pooling layer  which essentially acts as a detector of speciﬁc classes of h-grams 
given a convolution ﬁlter window of size h. The deep architecture in our framework will  in principle 
enable the high-level layers to capture more sophisticated language features. We use convolutions
with stride rather than pooling operators  e.g.  max-pooling  for spatial downsampling following
[26  45]  where it is argued that fully convolutional architectures are able to learn their own spatial
downsampling. Further  [46] uses a 29-layer CNN for text classiﬁcation. Our CNN encoder is
considerably simpler in structure (convolutions with stride and no more than 4 layers) while still
achieving good performance.
Language decoders other than RNNs are less well studied. Recently  [47] proposed a hybrid model
by coupling a convolutional-deconvolutional network with an RNN  where the RNN acts as decoder
and the deconvolutional model as a bridge between the encoder (convolutional network) and decoder.
Additionally  [42  48  49  50] considered CNN variants  such as pixelCNN [51]  for text generation.
Nevertheless  to achieve good empirical results  these methods still require the sentences to be
generated sequentially  conditioning on the ground truth historical information  akin to RNN-based
decoders  thus still suffering from the exposure bias.
Other efforts have been made to improve embeddings from long paragraphs using unsupervised
approaches [2  52]. The paragraph vector [2] learns a ﬁxed length vector by concatenating it with
a word2vec [53] embedding of history sequence to predict future words. The hierarchical neural
autoencoder [52] builds a hierarchical attentive RNN  then it uses paragraph-level hidden units of
that RNN as embedding. Our work differs from these approaches in that we force the sequence to be
fully restored from the latent representation  without aid from any history information.
Previous methods have considered leveraging unlabeled data for semi-supervised sequence classiﬁca-
tion tasks. Typically  RNN-based methods consider either i) training a sequence-to-sequence RNN
autoencoder  or a RNN classiﬁer that is robust to adversarial perturbation  as initialization for the en-
coder in the supervised model [1  4]; or  ii) learning latent representation via a sequence-to-sequence
RNN autoencoder  and then using them as inputs to a classiﬁer that also takes features extracted from
a CNN as inputs [3]. For summarization tasks  [54] has considered a semi-supervised approach based
on support vector machines  however  so far  research on semi-supervised text summarization using
deep models is scarce.

4 Experiments

Experimental setup For all the experiments  we use a 3-layer convolutional encoder followed by a
3-layer deconvolutional decoder (recall implementation details for the top layer). Filter size  stride

5

Ground-truth:

Hier. LSTM [52]

on every visit to nyc   the hotel beacon is the place we love to stay . so conveniently located to central park   lincoln
center and great local restaurants . the rooms are lovely . beds so comfortable   a great little kitchen and new wizz
bang coffee maker . the staff are so accommodating and just love walking across the street to the fairway supermarket
with every imaginable goodies to eat .
every time in new york   lighthouse hotel is our favorite place to stay . very convenient   central park   lincoln center
  and great restaurants . the room is wonderful   very comfortable bed   a kitchenette and a large explosion of coffee
maker . the staff is so inclusive   just across the street to walk to the supermarket channel love with all kinds of what
to eat .

Our LSTM-LSTM on every visit to nyc   the hotel beacon is the place to relax and wanting to become conveniently located . hotel   in
the evenings out good budget accommodations . the views are great and we were more than two couples . manny the
doorman has a great big guy come and will deﬁnitly want to leave during my stay and enjoy a wonderfully relaxing
wind break in having for 24 hour early rick’s cafe . oh perfect ! easy easy walking distance to everything imaginable
groceries . if you may want to watch yours !
on every visit to nyc   the hotel beacon is the place we love to stay . so closely located to central park   lincoln center
and great local restaurants . biggest rooms are lovely . beds so comfortable   a great little kitchen and new UNK
suggestion coffee maker . the staff turned so accommodating and just love walking across the street to former fairway
supermarket with every food taxes to eat .

Our CNN-DCNN

Table 1: Reconstructed paragraph of the Hotel Reviews example  used in [52].

and word embedding are set to h = 5  rl = 2  for l = 1  . . .   3 and k = 300  respectively. The
dimension of the latent representation vector varies for each experiment  thus is reported separately.
For notational convenience  we denote our convolutional-deconvolutional autoencoder as CNN-
DCNN. In most comparisons  we also considered two standard autoencoders as baselines: a) CNN-
LSTM: CNN encoder coupled with LSTM decoder; and b) LSTM-LSTM: LSTM encoder with
LSTM decoder. An LSTM-DCNN conﬁguration is not included because it yields similar performance
to CNN-DCNN while being more computationally expensive. The complete experimental setup and
baseline details is provided in the Supplementary Material (SM). CNN-DCNN has the least number
of parameters. For example  using 500 as the dimension of h results in about 9  13  15 million total
trainable parameters for CNN-DCNN  CNN-LSTM and LSTM-LSTM  respectively.

Model

LSTM-LSTM [52]

Hier. LSTM-LSTM [52]

Hier. + att. LSTM-LSTM [52]

CNN-LSTM
CNN-DCNN

BLEU ROUGE-1 ROUGE-2
24.1
26.7
28.5
18.3
94.2

30.2
33.0
35.5
28.2
94.2

57.1
59.0
62.4
56.6
97.0

Table 2: Reconstruction evaluation results on the Hotel Reviews
Dataset.

Figure 2: BLEU score vs. sentence
length for Hotel Review data.

Paragraph reconstruction We ﬁrst investigate the performance of the proposed autoencoder in
terms of learning representations that can preserve paragraph information. We adopt evaluation
criteria from [52]  i.e.  ROUGE score [55] and BLEU score [56]  to measure the closeness of the
reconstructed paragraph (model output) to the input paragraph. Brieﬂy  ROUGE and BLEU scores
measures the n-gram recall and precision between the model outputs and the (ground-truth) references.
We use BLEU-4  ROUGE-1  2 in our evaluation  in alignment with [52]. In addition to the CNN-
LSTM and LSTM-LSTM autoencoder  we also compared with the hierarchical LSTM autoencoder
[52]. The comparison is performed on the Hotel Reviews datasets  following the experimental setup
from [52]  i.e.  we only keep reviews with sentence length ranging from 50 to 250 words  resulting
in 348 544 training data samples and 39 023 testing data samples. For all comparisons  we set the
dimension of the latent representation to h = 500.
From Table 1  we see that for long paragraphs  the LSTM decoder in CNN-LSTM and LSTM-LSTM
suffers from heavy exposure bias issues. We further evaluate the performance of each model with
different paragraph lengths. As shown in Figure 2 and Table 2  on this task CNN-DCNN demonstrates
a clear advantage  meanwhile  as the length of the sentence increases  the comparative advantage
becomes more substantial. For LSTM-based methods  the quality of the reconstruction deteriorates
quickly as sequences get longer. In constrast  the reconstruction quality of CNN-DCNN is stable and
consistent regardless of sentence length. Furthermore  the computational cost  evaluated as wall-clock 
is signiﬁcantly lower in CNN-DCNN. Roughly  CNN-LSTM is 3 times slower than CNN-DCNN 
and LSTM-LSTM is 5 times slower on a single GPU. Details are reported in the SM.

Character-level and word-level correction This task seeks to evaluate whether the deconvolu-
tional decoder can overcome exposure bias  which severely limits LSTM-based decoders. We consider

6

6080100120140160180200Sentence length020406080100Bleu scoreCNN-DCNNCNN-LSTMLSTM-LSTMa denoising autoencoder where the input is tweaked slightly with certain modiﬁcations  while the
model attempts to denoise (correct) the unknown modiﬁcation  thus recover the original sentence.
For character-level correction  we consider the Yahoo! Answer dataset [57]. The dataset description
and setup for word-level correction is provided in the SM. We follow the experimental setup in
[58] for word-level and character-level spelling correction (see details in the SM). We considered
substituting each word/character with a different one at random with probability η  with η = 0.30.
For character-level analysis  we ﬁrst map all characters into a 40 dimensional embedding vector  with
the network structure for word- and character-level models kept the same.

Model

Actor-critic[58]
LSTM-LSTM
CNN-LSTM
CNN-DCNN

Model

LSTM-LSTM
CNN-LSTM
CNN-DCNN

Yahoo(CER)

0.2284
0.2621
0.2035
0.1323

ArXiv(WER)

0.7250
0.3819
0.3067

Table 3: CER and WER com-
parison on Yahoo and ArXiv
data.

Figure 4: Spelling error denoising compar-
ison. Darker colors indicate higher uncer-
tainty. Trained on modiﬁed sentences.

Figure 3: CER comparison.
Black triangles indicate the end
of an epoch.
We employ Character Error Rate (CER) [58] and Word Error Rate (WER) [59] for evaluation. The
WER/CER measure the ratio of Levenshtein distance (a.k.a.  edit distance) between model predictions
and the ground-truth  and the total length of sequence. Conceptually  lower WER/CER indicates
better performance. We use LSTM-LSTM and CNN-LSTM denoising autoencoders for comparison.
The architecture for the word-level baseline models is the same as in the previous experiment. For
character-level correction  we set dimension of h to 900. We also compare to actor-critic training
[58]  following their experimental guidelines (see details in the SM).
As shown in Figure 3 and Table 3  we observed CNN-DCNN achieves both lower CER and faster
convergence. Further  CNN-DCNN delivers stable denoising performance irrespective of the noise
location within the sentence  as seen in Figure 4. For CNN-DCNN  even when an error is detected
but not exactly corrected (darker colors in Figure 4 indicate higher uncertainty)  denoising with future
words is not effected  while for CNN-LSTM and LSTM-LSTM the error gradually accumulates with
longer sequences  as expected.
For word-level correction  we consider word substitutions only  and mixed perturbations from three
kinds: substitution  deletion and insertion. Generally  CNN-DCNN outperforms CNN-LSTM and
LSTM-LSTM  and is faster. We provide experimental details and comparative results in the SM.

Semi-supervised sequence classiﬁcation & summarization We investigate whether our CNN-
DCNN framework can improve upon supervised natural language tasks that leverage features learned
from paragraphs. In principle  a good unsupervised feature extractor will improve the general-
ization ability in a semi-supervised learning setting. We evaluate our approach on three popular
natural language tasks: sentiment analysis  paragraph topic prediction and text summarization. The
ﬁrst two tasks are essentially sequence classiﬁcation  while summarization involves both language
comprehension and language generation.
We consider three large-scale document classiﬁcation datasets: DBPedia  Yahoo! Answers and
Yelp Review Polarity [57]. The partition of training  validation and test sets for all datasets follows
the settings from [57]. The detailed summary statistics of all datasets are shown in the SM. To
demonstrate the advantage of incorporating the reconstruction objective into the training of text
classiﬁers  we further evaluate our model with different amounts of labeled data (0.1%  0.15%  0.25% 
1%  10% and 100%  respectively)  and the whole training set as unlabeled data.
For our purely supervised baseline model (supervised CNN)  we use the same convolutional encoder
architecture described above  with a 500-dimensional latent representation dimension  followed by
a MLP classiﬁer with one hidden layer of 300 hidden units. The dropout rate is set to 50%. Word
embeddings are initialized at random.
As shown in Table 4  the joint training strategy consistently and signiﬁcantly outperforms the purely
supervised strategy across datasets  even when all labels are available. We hypothesize that during the
early phase of training  when reconstruction is emphasized  features from text fragments can be readily

7

010203040506070Time (hour)0.00.20.40.60.81.0Character Error Rate (CER)CNN-DCNNCNN-LSTMLSTM-LSTMOriginalcOriginalaOriginalnOriginal OriginalaOriginalnOriginalyOriginaloOriginalnOriginaleOriginal OriginalsOriginaluOriginalgOriginalgOriginaleOriginalsOriginaltOriginal OriginalsOriginaloOriginalmOriginaleOriginal OriginalgOriginaloOriginaloOriginaldOriginal OriginalbOriginaloOriginaloOriginalkOriginalsOriginal Original?ModifiedcModifiedaModifiedpModified ModifiedaModifiednModifiedyModifiedoModifiednModifiedkModified ModifiedwModifieduModifiedgModifiedgModifiedeModifiedsModifiedtModified ModifiedxModifiedoModifiedhModifiedeModified ModifiediModifiedoModifiedrModifieddModified ModifiedyModifiedoModifiedoModifiedkModifieduModified Modified?ActorCriticcActorCriticaActorCriticnActorCritic ActorCriticaActorCriticnActorCriticyActorCriticoActorCriticnActorCriticeActorCritic ActorCriticwActorCriticiActorCritictActorCritichActorCriticeActorCriticsActorCritictActorCritic ActorCritictActorCriticoActorCritic ActorCriticeActorCritic ActorCriticfActorCriticoActorCriticrActorCriticdActorCritic ActorCriticyActorCriticoActorCriticuActorCritic ActorCriticuActorCritic ActorCritic?LSTM-LSTMcLSTM-LSTMaLSTM-LSTMnLSTM-LSTM LSTM-LSTMaLSTM-LSTMnLSTM-LSTMyLSTM-LSTMoLSTM-LSTMnLSTM-LSTMeLSTM-LSTM LSTM-LSTMsLSTM-LSTMuLSTM-LSTMgLSTM-LSTMgLSTM-LSTMeLSTM-LSTMsLSTM-LSTMtLSTM-LSTM LSTM-LSTMjLSTM-LSTMoLSTM-LSTMkLSTM-LSTMeLSTM-LSTM LSTM-LSTMfLSTM-LSTMoLSTM-LSTMoLSTM-LSTMdLSTM-LSTM LSTM-LSTMyLSTM-LSTMoLSTM-LSTMuLSTM-LSTMnLSTM-LSTMgLSTM-LSTM LSTM-LSTM?CNN-LSTMcCNN-LSTMaCNN-LSTMnCNN-LSTM CNN-LSTMaCNN-LSTMnCNN-LSTMyCNN-LSTMoCNN-LSTMnCNN-LSTMeCNN-LSTM CNN-LSTMgCNN-LSTMuCNN-LSTMiCNN-LSTMtCNN-LSTMeCNN-LSTMsCNN-LSTM CNN-LSTMsCNN-LSTMoCNN-LSTMmCNN-LSTMeCNN-LSTM CNN-LSTMoCNN-LSTMwCNN-LSTMeCNN-LSTM CNN-LSTMpCNN-LSTMoCNN-LSTMoCNN-LSTMkCNN-LSTMsCNN-LSTM CNN-LSTM?CNN-LSTM CNN-LSTM CNN-DCNNcCNN-DCNNaCNN-DCNNnCNN-DCNN CNN-DCNNaCNN-DCNNnCNN-DCNNyCNN-DCNNoCNN-DCNNnCNN-DCNNeCNN-DCNN CNN-DCNNsCNN-DCNNuCNN-DCNNgCNN-DCNNgCNN-DCNNeCNN-DCNNsCNN-DCNNtCNN-DCNN CNN-DCNNsCNN-DCNNoCNN-DCNNmCNN-DCNNeCNN-DCNN CNN-DCNNwCNN-DCNNoCNN-DCNNoCNN-DCNNdCNN-DCNN CNN-DCNNbCNN-DCNNoCNN-DCNNoCNN-DCNNkCNN-DCNNsCNN-DCNN CNN-DCNN?OriginalwOriginalhOriginalaOriginaltOriginal OriginalsOriginal OriginalyOriginaloOriginaluOriginalrOriginal OriginaliOriginaldOriginaleOriginalaOriginal OriginaloOriginalfOriginal OriginalaOriginal OriginalsOriginaltOriginaleOriginalpOriginalpOriginaliOriginalnOriginalgOriginal OriginalsOriginaltOriginaloOriginalnOriginaleOriginal OriginaltOriginaloOriginal OriginalbOriginaleOriginaltOriginaltOriginaleOriginalrOriginal OriginaltOriginalhOriginaliOriginalnOriginalgOriginalsOriginal OriginaltOriginaloOriginal OriginalcOriginaloOriginalmOriginaleOriginal Original?ModifiedwModifieduModifiedaModifiedtModified ModifiedsModified ModifiedyModifiedoModifiedgModifiedrModified ModifiediModifieddModifiedeModifiedmModified ModifiedoModifiedfModified ModifiedtModified ModifiedsModifiedtModifiedeModifiedpModifieduModifiedkModifiednModifiedgModified ModifiedjModifiedtModifiedzModifiednModifiedeModified ModifiedtModifiediModified ModifiedbModifiedeModifiedtModifiedtModifiedeModifiedrModified ModifiedtModifiedhModifiediModifiednModifiedgModifiedzModified ModifiedtModifiedtModified ModifiedcModifiedoModifiedeModifiedeModified Modified?ActorCriticwActorCritichActorCriticaActorCritictActorCritic ActorCriticsActorCritic ActorCriticyActorCriticoActorCriticuActorCriticrActorCritic ActorCriticiActorCriticdActorCriticeActorCriticmActorCritic ActorCriticoActorCriticfActorCritic ActorCritictActorCritic ActorCriticsActorCritictActorCriticeActorCriticpActorCriticuActorCriticaActorCriticnActorCriticgActorCritic ActorCriticjActorCriticoActorCritickActorCriticnActorCriticeActorCritic ActorCritictActorCriticiActorCritic ActorCriticbActorCriticeActorCritictActorCritictActorCriticeActorCriticrActorCritic ActorCritictActorCritichActorCriticiActorCriticnActorCriticgActorCritic ActorCriticiActorCritictActorCritictActorCritic ActorCriticcActorCriticoActorCriticmActorCriticeActorCritic ActorCritic?LSTM-LSTMwLSTM-LSTMhLSTM-LSTMaLSTM-LSTMtLSTM-LSTM LSTM-LSTMsLSTM-LSTM LSTM-LSTMyLSTM-LSTMoLSTM-LSTMuLSTM-LSTMrLSTM-LSTM LSTM-LSTMiLSTM-LSTMdLSTM-LSTMeLSTM-LSTMaLSTM-LSTM LSTM-LSTMoLSTM-LSTMfLSTM-LSTM LSTM-LSTMaLSTM-LSTM LSTM-LSTMsLSTM-LSTMpLSTM-LSTMeLSTM-LSTMaLSTM-LSTMkLSTM-LSTMiLSTM-LSTMnLSTM-LSTMgLSTM-LSTM LSTM-LSTMsLSTM-LSTMtLSTM-LSTMaLSTM-LSTMnLSTM-LSTMdLSTM-LSTM LSTM-LSTMtLSTM-LSTMoLSTM-LSTM LSTM-LSTMbLSTM-LSTMeLSTM-LSTMtLSTM-LSTMtLSTM-LSTMeLSTM-LSTMrLSTM-LSTM LSTM-LSTMtLSTM-LSTMhLSTM-LSTMiLSTM-LSTMnLSTM-LSTMgLSTM-LSTMsLSTM-LSTM LSTM-LSTMtLSTM-LSTMoLSTM-LSTM LSTM-LSTMcLSTM-LSTMoLSTM-LSTMmLSTM-LSTMeLSTM-LSTM LSTM-LSTM?CNN-LSTMwCNN-LSTMhCNN-LSTMaCNN-LSTMtCNN-LSTM CNN-LSTMsCNN-LSTM CNN-LSTMyCNN-LSTMoCNN-LSTMuCNN-LSTMrCNN-LSTM CNN-LSTMiCNN-LSTMdCNN-LSTMeCNN-LSTMmCNN-LSTM CNN-LSTMoCNN-LSTMfCNN-LSTM CNN-LSTMaCNN-LSTM CNN-LSTMsCNN-LSTMtCNN-LSTMeCNN-LSTMpCNN-LSTMpCNN-LSTMiCNN-LSTMnCNN-LSTMgCNN-LSTM CNN-LSTMsCNN-LSTMtCNN-LSTMaCNN-LSTMrCNN-LSTMtCNN-LSTM CNN-LSTMtCNN-LSTMoCNN-LSTM CNN-LSTMbCNN-LSTMeCNN-LSTMtCNN-LSTMtCNN-LSTMeCNN-LSTMrCNN-LSTM CNN-LSTMtCNN-LSTMhCNN-LSTMiCNN-LSTMnCNN-LSTMgCNN-LSTM CNN-LSTMtCNN-LSTMoCNN-LSTM CNN-LSTMcCNN-LSTMoCNN-LSTMmCNN-LSTMeCNN-LSTM CNN-LSTM?CNN-LSTM CNN-DCNNwCNN-DCNNhCNN-DCNNaCNN-DCNNtCNN-DCNN CNN-DCNNsCNN-DCNN CNN-DCNNyCNN-DCNNoCNN-DCNNuCNN-DCNNrCNN-DCNN CNN-DCNNiCNN-DCNNdCNN-DCNNeCNN-DCNNaCNN-DCNN CNN-DCNNoCNN-DCNNfCNN-DCNN CNN-DCNNaCNN-DCNN CNN-DCNNsCNN-DCNNtCNN-DCNNeCNN-DCNNpCNN-DCNNpCNN-DCNNiCNN-DCNNnCNN-DCNNgCNN-DCNN CNN-DCNNsCNN-DCNNtCNN-DCNNoCNN-DCNNnCNN-DCNNeCNN-DCNN CNN-DCNNtCNN-DCNNoCNN-DCNN CNN-DCNNbCNN-DCNNeCNN-DCNNtCNN-DCNNtCNN-DCNNeCNN-DCNNrCNN-DCNN CNN-DCNNtCNN-DCNNhCNN-DCNNiCNN-DCNNnCNN-DCNNgCNN-DCNNsCNN-DCNN CNN-DCNNtCNN-DCNNoCNN-DCNN CNN-DCNNcCNN-DCNNoCNN-DCNNmCNN-DCNNeCNN-DCNN CNN-DCNN?learned. As the training proceeds  the most discriminative text fragment features are selected. Further 
the subset of features that are responsible for both reconstruction and discrimination presumably
encapsulate longer dependency structure  compared to the features using a purely supervised strategy.
Figure 5 demonstrates the behavior of our model in a semi-supervised setting on Yelp Review dataset.
The results for Yahoo! Answer and DBpedia are provided in the SM.

Model

ngrams TFIDF

Large Word ConvNet
Small Word ConvNet
Large Char ConvNet
Small Char ConvNet
SA-LSTM (word-level)

Deep ConvNet

Ours (Purely supervised)

Ours (joint training with CNN-LSTM)
Ours (joint training with CNN-DCNN)

DBpedia

1.31
1.72
1.85
1.73
1.98
1.40
1.29
1.76
1.36
1.17

Yelp P.
4.56
4.89
5.54
5.89
6.53

-

4.28
4.62
4.21
3.96

Yahoo
31.49
29.06
30.02
29.55
29.84

-

26.57
27.42
26.32
25.82

5%
12.40
16.04

Obs. proportion σ

Supervised
Semi-sup.

Figure 5: Semi-supervised classiﬁca-
tion accuracy on Yelp review data.

Table 4: Test error rates of document classiﬁcation (%). Results
from other methods were obtained from [57].
For summarization  we used a dataset composed of 58 000 abstract-title pairs  from arXiv. Abstract-
title pairs are selected if the length of the title and abstract do not exceed 50 and 500 words 
respectively. We partitioned the training  validation and test sets into 55000  2000  1000 pairs each.
We train a sequence-to-sequence model to generate the title given the abstract  using a randomly
selected subset of paired data with proportion σ = (5%  10%  50%  100%). For every value of
σ  we considered both purely supervised summarization using just abstract-title pairs  and semi-
supervised summarization  by leveraging additional abstracts without titles. We compared LSTM and
deconvolutional network as the decoder for generating titles for σ = 100%.
Table 5 summarizes quantitative results
using ROUGE-L (longest common sub-
sequence) [55]. In general  the additional
abstracts without titles improve the gen-
eralization ability on the test set. Inter-
estingly  even when σ = 100% (all titles
are observed)  the joint training objective
still yields a better performance than using Lsup alone. Presumably  since the joint training objective
requires the latent representation to be capable of reconstructing the input paragraph  in addition
to generating a title  the learned representation may better capture the entire structure (meaning) of
the paragraph. We also empirically observed that titles generated under the joint training objective
are more likely to use the words appearing in the corresponding paragraph (i.e.  more extractive) 
while the the titles generated using the purely supervised objective Lsup  tend to use wording more
freely  thus more abstractive. One possible explanation is that  for the joint training strategy  since the
reconstructed paragraph and title are all generated from latent representation h  the text fragments
that are used for reconstructing the input paragraph are more likely to be leveraged when “building”
the title  thus the title bears more resemblance to the input paragraph.
As expected  the titles produced by a deconvolutional decoder are less coherent than an LSTM
decoder. Presumably  since each paragraph can be summarized with multiple plausible titles  the
deconvolutional decoder may have trouble when positioning text segments. We provide discussions
and titles generated under different setups in the SM. Designing a framework which takes the best of
these two worlds  LSTM for generation and CNN for decoding  will be an interesting future direction.

Table 5: Summarization task on arXiv data  using ROUGE-L
metric. First 4 columns are for the LSTM decoder  and the last
column is for the deconvolutional decoder (100% observed).

50% 100% DCNN dec.
15.87
17.64

14.75
16.83

16.37
18.14

10%
13.07
16.62

5 Conclusion

We proposed a general framework for text modeling using purely convolutional and deconvolutional
operations. The proposed method is free of sequential conditional generation  avoiding issues
associated with exposure bias and teacher forcing training. Our approach enables the model to
fully encapsulate a paragraph into a latent representation vector  which can be decompressed to
reconstruct the original input sequence. Empirically  the proposed approach achieved excellent long
paragraph reconstruction quality and outperforms existing algorithms on spelling correction  and
semi-supervised sequence classiﬁcation and summarization  with largely reduced computational cost.

8

0.1110100Proportion (%) of labeled data556065707580859095100Accuracy (%)SupervisedSemi (CNN-DCNN)Semi (CNN-LSTM)Acknowledgements This research was supported in part by ARO  DARPA  DOE  NGA and ONR.

References
[1] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NIPS  2015.

[2] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML  2014.

[3] Rie Johnson and Tong Zhang. Supervised and Semi-Supervised Text Categorization using LSTM for

Region Embeddings. arXiv  February 2016.

[4] Takeru Miyato  Andrew M Dai  and Ian Goodfellow. Adversarial Training Methods for Semi-Supervised

Text Classiﬁcation. In ICLR  May 2017.

[5] Dzmitry Bahdanau  Kyunghyun Cho  and Yoshua Bengio. Neural Machine Translation by Jointly Learning

to Align and Translate. In ICLR  2015.

[6] Kyunghyun Cho  Bart Van Merriënboer  Caglar Gulcehre  Dzmitry Bahdanau  Fethi Bougares  Holger
Schwenk  and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. In EMNLP  2014.

[7] Fandong Meng  Zhengdong Lu  Mingxuan Wang  Hang Li  Wenbin Jiang  and Qun Liu. Encoding source

language with convolutional neural network for machine translation. In ACL  2015.

[8] Tsung-Hsien Wen  Milica Gasic  Nikola Mrksic  Pei-Hao Su  David Vandyke  and Steve Young. Se-
mantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv 
2015.

[9] Jiwei Li  Will Monroe  Alan Ritter  Michel Galley  Jianfeng Gao  and Dan Jurafsky. Deep reinforcement

learning for dialogue generation. arXiv  2016.

[10] Jiwei Li  Will Monroe  Tianlin Shi  Alan Ritter  and Dan Jurafsky. Adversarial learning for neural dialogue

generation. arXiv:1701.06547  2017.

[11] Ramesh Nallapati  Bowen Zhou  Cicero Nogueira dos santos  Caglar Gulcehre  and Bing Xiang. Abstractive

Text Summarization Using Sequence-to-Sequence RNNs and Beyond. In CoNLL  2016.

[12] Shashi Narayan  Nikos Papasarantopoulos  Mirella Lapata  and Shay B Cohen. Neural Extractive Summa-

rization with Side Information. arXiv  April 2017.

[13] Alexander M Rush  Sumit Chopra  and Jason Weston. A Neural Attention Model for Abstractive Sentence

Summarization. In EMNLP  2015.

[14] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural networks. In

NIPS  2014.

[15] Tomas Mikolov  Martin Karaﬁát  Lukas Burget  Jan Cernock`y  and Sanjeev Khudanpur. Recurrent neural

network based language model. In INTERSPEECH  2010.

[16] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. In Neural computation  1997.

[17] Junyoung Chung  Caglar Gulcehre  KyungHyun Cho  and Yoshua Bengio. Empirical evaluation of gated

recurrent neural networks on sequence modeling. arXiv  2014.

[18] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural

networks. Neural computation  1(2):270–280  1989.

[19] Samy Bengio  Oriol Vinyals  Navdeep Jaitly  and Noam Shazeer. Scheduled sampling for sequence

prediction with recurrent neural networks. In NIPS  2015.

[20] Ferenc Huszár. How (not) to train your generative model: Scheduled sampling  likelihood  adversary?

arXiv  2015.

[21] Nal Kalchbrenner  Edward Grefenstette  and Phil Blunsom. A convolutional neural network for modelling

sentences. In ACL  2014.

[22] Yoon Kim. Convolutional neural networks for sentence classiﬁcation. In EMNLP  2014.

[23] Zhe Gan  Yunchen Pu  Henao Ricardo  Chunyuan Li  Xiaodong He  and Lawrence Carin. Learning generic

sentence representations using convolutional neural networks. In EMNLP  2017.

9

[24] Ishaan Gulrajani  Kundan Kumar  Faruk Ahmed  Adrien Ali Taiga  Francesco Visin  David Vazquez  and

Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv  2016.

[25] Yunchen Pu  Win Yuan  Andrew Stevens  Chunyuan Li  and Lawrence Carin. A deep generative deconvo-

lutional image model. In Artiﬁcial Intelligence and Statistics  pages 741–750  2016.

[26] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv  2015.

[27] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In ICML 

pages 807–814  2010.

[28] Ian Chiswell and Wilfrid Hodges. Mathematical logic  volume 3. OUP Oxford  2007.

[29] Emil Julius Gumbel and Julius Lieblein. Statistical theory of extreme values and some practical applications:

a series of lectures. 1954.

[30] Yunchen Pu  Xin Yuan  and Lawrence Carin. A generative model for deep convolutional learning. arXiv

preprint arXiv:1504.04054  2015.

[31] Yunchen Pu  Zhe Gan  Ricardo Henao  Xin Yuan  Chunyuan Li  Andrew Stevens  and Lawrence Carin.

Variational autoencoder for deep learning of images  labels and captions. In NIPS  2016.

[32] Yizhe Zhang  Zhe Gan  Kai Fan  Zhi Chen  Ricardo Henao  Dinghan Shen  and Lawrence Carin. Adversarial

feature matching for text generation. In ICML  2017.

[33] Zhe Gan  Liqun Chen  Weiyao Wang  Yunchen Pu  Yizhe Zhang  Hao Liu  Chunyuan Li  and Lawrence

Carin. Triangle generative adversarial networks. arXiv preprint arXiv:1709.06548  2017.

[34] Ronan Collobert  Jason Weston  Léon Bottou  Michael Karlen  Koray Kavukcuoglu  and Pavel Kuksa.

Natural language processing (almost) from scratch. In JMLR  2011.

[35] Sharan Chetlur  Cliff Woolley  Philippe Vandermersch  Jonathan Cohen  John Tran  Bryan Catanzaro  and

Evan Shelhamer. cudnn: Efﬁcient primitives for deep learning. arXiv  2014.

[36] Zichao Yang  Diyi Yang  Chris Dyer  Xiaodong He  Alex Smola  and Eduard Hovy. Hierarchical attention

networks for document classiﬁcation. In NAACL  2016.

[37] Adji B Dieng  Chong Wang  Jianfeng Gao  and John Paisley. TopicRNN: A Recurrent Neural Network

with Long-Range Semantic Dependency. In ICLR  2016.

[38] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-supervised

learning with deep generative models. In NIPS  2014.

[39] Sepp Hochreiter  Yoshua Bengio  Paolo Frasconi  and Jürgen Schmidhuber. Gradient ﬂow in recurrent

nets: the difﬁculty of learning long-term dependencies  2001.

[40] Richard Socher  Jeffrey Pennington  Eric H Huang  Andrew Y Ng  and Christopher D Manning. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In EMNLP. Association for
Computational Linguistics  2011.

[41] Samuel R Bowman  Luke Vilnis  Oriol Vinyals  Andrew M Dai  Rafal Jozefowicz  and Samy Bengio.

Generating sentences from a continuous space. arXiv  2015.

[42] Zichao Yang  Zhiting Hu  Ruslan Salakhutdinov  and Taylor Berg-Kirkpatrick. Improved Variational

Autoencoders for Text Modeling using Dilated Convolutions. arXiv  February 2017.

[43] Baotian Hu  Zhengdong Lu  Hang Li  and Qingcai Chen. Convolutional neural network architectures for

matching natural language sentences. In NIPS  2014.

[44] Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural

networks. In NAACL HLT  2015.

[45] Jost Tobias Springenberg  Alexey Dosovitskiy  Thomas Brox  and Martin Riedmiller. Striving for simplicity:

The all convolutional net. arXiv  2014.

[46] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In ICLR  2015.

10

[47] Stanislau Semeniuta  Aliaksei Severyn  and Erhardt Barth. A Hybrid Convolutional Variational Autoen-

coder for Text Generation. arXiv  February 2017.

[48] Nal Kalchbrenner  Lasse Espeholt  Karen Simonyan  Aaron van den Oord  Alex Graves  and Koray

Kavukcuoglu. Neural machine translation in linear time. arXiv  2016.

[49] Yann N Dauphin  Angela Fan  Michael Auli  and David Grangier. Language Modeling with Gated

Convolutional Networks. arXiv  December 2016.

[50] J. Gehring  M. Auli  D. Grangier  D. Yarats  and Y. N. Dauphin. Convolutional Sequence to Sequence

Learning. arXiv  May 2017.

[51] Aaron van den Oord  Nal Kalchbrenner  Lasse Espeholt  Oriol Vinyals  Alex Graves  et al. Conditional

image generation with pixelcnn decoders. In NIPS  pages 4790–4798  2016.

[52] Jiwei Li  Minh-Thang Luong  and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs and

documents. In ACL  2015.

[53] Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg S Corrado  and Jeff Dean. Distributed representations of

words and phrases and their compositionality. In NIPS  2013.

[54] Kam-Fai Wong  Mingli Wu  and Wenjie Li. Extractive summarization using supervised and semi-supervised

learning. In ICCL. Association for Computational Linguistics  2008.

[55] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In ACL workshop  2004.

[56] Kishore Papineni  Salim Roukos  Todd Ward  and Wei-Jing Zhu. Bleu: a method for automatic evaluation

of machine translation. In ACL. Association for Computational Linguistics  2002.

[57] Xiang Zhang  Junbo Zhao  and Yann LeCun. Character-level convolutional networks for text classiﬁcation.

In NIPS  pages 649–657  2015.

[58] Dzmitry Bahdanau  Philemon Brakel  Kelvin Xu  Anirudh Goyal  Ryan Lowe  Joelle Pineau  Aaron

Courville  and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv  2016.

[59] JP Woodard and JT Nelson. An information theoretic measure of speech recognition performance. In

Workshop on standardisation for speech I/O  1982.

11

,Yizhe Zhang
Dinghan Shen
Guoyin Wang
Zhe Gan
Ricardo Henao
Lawrence Carin
Wenbo Guo
Sui Huang
Yunzhe Tao
Xinyu Xing
Lin Lin