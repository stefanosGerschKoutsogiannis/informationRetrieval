2015,Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization,We propose a new first-order optimization algorithm to solve high-dimensional  non-smooth composite minimization problems. Typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty. The proposed algorithm  called Semi-Proximal Mirror-Prox  leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain. The algorithm stands in contrast with more classical proximal gradient algorithms with smoothing  which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-Prox  which exhibits the optimal complexity bounds for the number of calls to linear minimization oracle. We present promising experimental results showing the interest of the approach in comparison to competing methods.,Semi-Proximal Mirror-Prox

for Nonsmooth Composite Minimization

Niao He

Georgia Institute of Technology

Zaid Harchaoui

NYU  Inria

nhe6@gatech.edu

firstname.lastname@nyu.edu

Abstract

We propose a new ﬁrst-order optimization algorithm to solve high-dimensional
non-smooth composite minimization problems. Typical examples of such prob-
lems have an objective that decomposes into a non-smooth empirical risk part
and a non-smooth regularization penalty. The proposed algorithm  called Semi-
Proximal Mirror-Prox  leverages the saddle point representation of one part of the
objective while handling the other part of the objective via linear minimization
over the domain. The algorithm stands in contrast with more classical proximal
gradient algorithms with smoothing  which require the computation of proximal
operators at each iteration and can therefore be impractical for high-dimensional
problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-
Prox  which exhibits the optimal complexity bounds  i.e. O(1/ǫ2)  for the number
of calls to linear minimization oracle. We present promising experimental results
showing the interest of the approach in comparison to competing methods.

1

Introduction

A wide range of machine learning and signal processing problems can be formulated as the mini-
mization of a composite objective:

min
x∈X

F (x) := f (x) + kBxk

(1)

where X is closed and convex  f is convex and can be either smooth  or nonsmooth yet enjoys
a particular structure. The term kBxk deﬁnes a regularization penalty through a norm k · k  and
x 7→ Bx a linear mapping on a closed convex set X.
In many situations  the objective function F of interest enjoys a favorable structure  namely a so-
called saddle point representation [6  11  13]:

f (x) = max

z∈Z {hx  Azi − ψ(z)}

(2)

where Z is convex compact subset of a Euclidean space  and ψ(·) is a convex function. Sec. 4 will
give several examples of such situations. Saddle point representations can then be leveraged to use
ﬁrst-order optimization algorithms.

The simple ﬁrst option to minimize F is using the so-called Nesterov smoothing technique [19]
along with a proximal gradient algorithm [23]  assuming that the proximal operator associated with
X is computationally tractable and cheap to compute. However  this is certainly not the case when
considering problems with norms acting in the spectral domain of high-dimensional matrices  such
as the matrix nuclear-norm [12] and structured extensions thereof [5  2].
In the latter situation 
another option is to use a smoothing technique now with a conditional gradient or Frank-Wolfe
algorithm to minimize F   assuming that a a linear minimization oracle associated with X is cheaper
to compute than the proximal operator [6  14  24]. Neither option takes advantage of the composite
structure of the objective (1) or handles the case when the linear mapping B is nontrivial.

1

Contributions Our goal is to propose a new ﬁrst-order optimization algorithm  called Semi-
Proximal Mirror-Prox  designed to solve the difﬁcult non-smooth composite optimization prob-
lem (1)  which does not require the exact computation of proximal operators. Instead  the Semi-
Proximal Mirror-Prox relies upon i) Saddle point representability of f (a less restricted role than
Fenchel-type representation); ii) Linear minimization oracle associated with k · k in the domain
X. While the saddle point representability of f allows to cure the non-smoothness of f   the linear
minimization over the domain X allows to tackle the non-smooth regularization penalty k · k. We
establish the theoretical convergence rate of Semi-Proximal Mirror-Prox  which exhibits the optimal
complexity bounds  i.e. O(1/ǫ2)  for the number of calls to linear minimization oracle. Furthermore 
Semi-Proximal Mirror-Prox generalizes previously proposed approaches and improves upon them
in special cases:

1. Case B ≡ 0: Semi-Proximal Mirror-Prox does not require assumptions on favorable geom-

etry of dual domain Z or simplicity of ψ(·) in (2).

2. Case B = I: Semi-Proximal Mirror-Prox is competitive with previously proposed ap-

proaches [15  24] based on smoothing techniques.

3. Case of non-trivial B: Semi-Proximal Mirror-Prox is the ﬁrst proximal-free or conditional-

gradient-type optimization algorithm for (1).

Related work The Semi-Proximal Mirror-Prox algorithm belongs to the family of conditional
gradient algorithms  whose most basic instance is the Frank-Wolfe algorithm for constrained smooth
optimization using a linear minimization oracle; see [12  1  4]. Recently  in [6  13]  the authors
consider constrained non-smooth optimization when the domain Z has a “favorable geometry” 
the domain is amenable to proximal setups (favorable geometry)  and establish a complexity
i.e.
bound with O(1/ǫ2) calls to the linear minimization oracle. Recently  in [15]  a method called
conditional gradient sliding is proposed to solve similar problems  using a smoothing technique 
with a complexity bound in O(1/ǫ2) for the calls to the linear minimization oracle (LMO) and
additionally a O(1/ǫ) bound for the linear operator evaluations. Actually  this O(1/ǫ2) bound for
the LMO complexity can be shown to be indeed optimal for conditional-gradient-type or LMO-
based algorithms  when solving general1 non-smooth convex problems [14].

However  these previous approaches are appropriate for objective with a non-composite structure.
When applied to our problem (1)  the smoothing would be applied to the objective taken as a whole 
ignoring its composite structure. Conditional-gradient-type algorithms were recently proposed for
composite objectives [7  9  26  24  16]  but cannot be applied for our problem. In [9]  f is smooth
and B is identity matrix  whereas in [24]  f is non-smooth and B is also the identity matrix. The
proposed Semi-Proximal Mirror-Prox can be seen as a blend of the successful components resp. of
the Composite Conditional Gradient algorithm [9] and the Composite Mirror-Prox [11]  that enjoys
the optimal complexity bound O(1/ǫ2) on the total number of LMO calls  yet solves a broader class
of convex problems than previously considered.

2 Framework and assumptions

We present here our theoretical framework  which hinges upon a smooth convex-concave sad-
dle point reformulation of the norm-regularized non-smooth minimization (3). We shall use the
following notations throughout the paper. For a given norm k · k  we deﬁne the dual norm as
ksk∗ = maxkxk≤1hs  xi. For any x ∈ Rm×n  kxk2 = kxkF = (Pm

j=1 |xij|2)1/2.

i=1Pn

Problem We consider the composite minimization problem
f (x) + kBxk

Opt = min
x∈X

(3)

where X is a closed convex set in the Euclidean space Ex; x 7→ Bx is a linear mapping from X
to Y (⊃ BX)  where Y is a closed convex set in the Euclidean space Ey. We make two important
assumptions on the function f and the norm k·k deﬁning the regularization penalty  explained below.
1Related research extended such approaches to stochastic or online settings [10  8  15]; such settings are

beyond the scope of this work.

2

Saddle Point Representation The non-smoothness of f can be challenging to tackle. However 
in many cases of interest  the function f enjoys a favorable structure that allows to tackle it with
smoothing techniques. We assume that f (x) is a non-smooth convex function given by

f (x) = max
z∈Z

Φ(x  z)

(4)

where Φ(x  z) is a smooth convex-concave function and Z is a convex and compact set in the Eu-
clidean space Ez. Such representation was introduced and developed in [6  11  13]  for the purpose
of non-smooth optimization. Saddle point representability can be interpreted as a general form of
the smoothing-favorable structure of non-smooth functions used in the Nesterov smoothing tech-
nique [19]. Representations of this type are readily available for a wide family of “well-structured”
nonsmooth functions f (see Sec. 4 for examples )  and actually for all empirical risk functions with
convex loss in machine learning  up to our knowledge.

Composite Linear Minimization Oracle Proximal-gradient-type algorithms require the compu-

tation of a proximal operator at each iteration  i.e. miny∈Y (cid:8) 1

cases of interest  described below  the computation of the proximal operator can be expensive or
intractable. A classical example is the nuclear norm  whose proximal operator boils down to sin-
gular value thresholding  therefore requiring a full singular value decomposition. In contrast to the
proximal operator  the linear minimization oracle can be much cheaper. The linear minimization
oracle (LMO) is a routine which  given an input α > 0 and η ∈ Ey  returns a point

2kyk2

2 + hη  yi + αkyk(cid:9). For several

LMO(η  α) := argmin

{hη  yi + αkyk}

y∈Y

(5)

In the case of nuclear-norm  the LMO only requires the computation of the leading pair of singular
vectors  which is an order of magnitude faster in time-complexity.

Saddle Point Reformulation. The crux of our approach is a smooth convex-concave saddle point
reformulation of (3). After massaging the saddle-point reformulation  we consider the associated
variational inequality  which provides the sufﬁcient and necessary condition for an optimal solution
to the saddle point problem [3  4]. For any optimization problem with convex structure (including
convex minimization  convex-concave saddle point problem  convex Nash equilibrium)  the corre-
sponding variational inequality is directly related to the accuracy certiﬁcate used to guarantee the
accuracy of a solution to the optimization problem; see Sec. 2.1 in [11] and [18]. We shall present
then an algorithm to solve the variational inequality established below  that exploits its particular
structure.

Assuming that f admits a saddle point representation (4)  we write (3) in epigraph form

Opt =

min

x∈X y∈Y τ ≥kyk

z∈Z {Φ(x  z) + τ : y = Bx} .
max

where Y (⊃ BX) is a convex set. We can approximate Opt by

dOpt =

min

x∈X y∈Y τ ≥kyk

max

z∈Z kwk2≤1 {Φ(x  z) + τ + ρhy − Bx  wi} .

(6)

For properly selected ρ > 0  one hasdOpt = Opt (see details in [11]). By introducing the variables

u := [x  y; z  w] and v := τ   the variational inequality associated with the above saddle point
problem is fully described by the domain

X+ = {x+ = [u; v] : x ∈ X  y ∈ Y  z ∈ Z kwk2 ≤ 1  τ ≥ kyk}

and the monotone vector ﬁeld

F (x+ = [u; v]) = [Fu(u); Fv]  

where

u =
Fu


x
y
z
w




 =



∇xΦ(x  z) − ρBT w

ρw

−∇zΦ(x  z)
ρ(Bx − y)


  

Fv(v = τ ) = 1.

In the next section  we present an efﬁcient algorithm to solve this type of variational inequality 
which enjoys a particular structure; we call such an inequality semi-structured.

3

3 Semi-Proximal Mirror-Prox for Semi-structured Variational Inequalities

Semi-structured variational inequalities (Semi-VI) enjoy a particular mixed structure  that allows to
get the best of two worlds  namely the proximal setup (where the proximal operator can be com-
puted) and the LMO setup (where the linear minimization oracle can be computed). Basically  the
domain X is decomposed as a Cartesian product over two sets X = X1 × X2  such that X1 admits
a proximal-mapping while X2 admits a linear minimization oracle. We now describe the main the-
oretical and algorithmic components of the Semi-Proximal Mirror-Prox algorithm  resp. in Sec. 3.1
and in Sec. 3.2  and ﬁnally describe the overall algorithm in Sec. 3.3.

3.1 Composite Mirror-Prox with Inexact Prox-mappings

We ﬁrst present a new algorithm  which can be seen as an extension of the Composite Mirror Prox al-
gorithm  denoted CMP for brevity  that allows inexact computation of prox-mappings and can solve
a broad class of variational inequalites. The original Mirror Prox algorithm was introduced in [17]
and was extended to composite settings in [11] assuming exact computations of prox-mappings.

Structured Variational Inequalities. We consider the variational inequality VI(X  F ):

with domain X and operator F that satisfy the assumptions (A.1)–(A.4) below.

Find x∗ ∈ X : hF (x)  x − x∗i ≥ 0 ∀x ∈ X

where U is convex and closed  Eu  Ev are Euclidean spaces;

(A.1) Set X ⊂ Eu × Ev is closed convex and its projection P X = {u : x = [u; v] ∈ X} ⊂ U  
(A.2) The function ω(·) : U → R is continuously differentiable and also 1-strongly convex w.r.t.
some norm2 k · k. This deﬁnes the Bregman distance Vu(u′) = ω(u′) − ω(u) − hω′(u)  u′ −
ui ≥ 1
(A.3) The operator F (x = [u  v]) : X → Eu × Ev is monotone and of form F (u  v) = [Fu(u); Fv]
with Fv ∈ Ev being a constant and Fu(u) ∈ Eu satisfying the condition
∀u  u′ ∈ U : kFu(u) − Fu(u′)k∗ ≤ Lku − u′k + M

2ku′ − uk2.

(A.4) The linear form hFv  vi of [u; v] ∈ Eu × Ev is bounded from below on X and is coercive on
t=1 is bounded

for some L < ∞  M < ∞;
X w.r.t. v: whenever [ut; vt] ∈ X  t = 1  2  ... is a sequence such that {ut}∞
and kvtk2 → ∞ as t → ∞  we have hFv  vti → ∞  t → ∞.

The quality of an iterate  in the course of the algorithm  is measured through the so-called dual gap
function

ǫVI(x(cid:12)(cid:12)X  F ) = sup

y∈X hF (y)  x − yi .

We give in Appendix A a refresher on dual gap functions  for the reader’s convenience. We shall
establish the complexity bounds in terms of this dual gap function for our algorithm  which directly
provides an accuracy certiﬁcate along the iterations. However  we ﬁrst need to deﬁne what we mean
by an inexact prox-mapping.

ǫ-Prox-mapping Inexact proximal mappings were recently considered in the context of acceler-
ated proximal gradient algorithms [25]. The deﬁnition we give below is more general  allowing for
non-Euclidean proximal-mappings.
We introduce here the notion of ǫ-prox-mapping for ǫ ≥ 0. For ξ = [η; ζ] ∈ Eu × Ev and x =
[u; v] ∈ X  let us deﬁne the subset P ǫ

x(ξ) of X as

P ǫ

x(ξ) = {bx = [bu;bv] ∈ X : hη + ω′(bu) − ω′(u) bu − si + hζ bv − wi ≤ ǫ ∀[s; w] ∈ X}.

When ǫ = 0  this reduces to the exact prox-mapping  in the usual setting  that is

Px(ξ) = Argmin

[s;w]∈X {hη  si + hζ  wi + Vu(s)} .

2There is a slight abuse of notation here. The norm here is not the same as the one in problem (3)

4

When ǫ > 0  this yields our deﬁnition of an inexact prox-mapping  with inexactness parameter ǫ.
Note that for any ǫ ≥ 0  the set P ǫ
x(ξ = [η; γFv]) is well deﬁned whenever γ > 0. The Composite

Mirror Prox with inexact prox-mappings is outlined in Algorithm 1.

Algorithm 1 Composite Mirror Prox Algorithm (CMP) for VI(X  F )

Input: stepsizes γt > 0  inexactness ǫt ≥ 0  t = 1  2  . . .
Initialize x1 = [u1; v1] ∈ X
for t = 1  2  . . .   T do

end for

yt := [but;bvt] ∈ P ǫt
xt+1 := [ut+1; vt+1] ∈ P ǫt
−1PT

t=1 γt)

Output: xT := [¯uT ; ¯vT ] = (PT

t=1 γtyt

xt (γtF (xt)) = P ǫt
xt (γtF (yt)) = P ǫt

xt (γt[Fu(ut); Fv])

xt (γt[Fu(but); Fv])

(7)

The proposed algorithm is a non-trivial extension of the Composite Mirror Prox with exact prox-
mappings  both from a theoretical and algorithmic point of views. We establish below the theoretical
convergence rate; see Appendix B for the proof.
Theorem 3.1. Assume that the sequence of step-sizes (γt) in the CMP algorithm satisfy

t M 2  

σt := γthFu(but) − Fu(ut) but − ut+1i − Vbut (ut+1) − Vut (but) ≤ γ2
t = 1  2  . . .   T . (8)
Then  denoting Θ[X] = sup[u;v]∈X Vu1 (u)  for a sequence of inexact prox-mappings with inexact-
ness ǫt ≥ 0  we have
Θ[X] + M 2PT
PT

ǫVI(¯xT(cid:12)(cid:12)X  F ) := sup

x∈X hF (x)  ¯xT − xi ≤

t + 2PT

t=1γ2
t=1 γt

Remarks. Note that the assumption on the sequence of step-sizes (γt) is clearly satisﬁed when

γt ≤ (√2L)−1. When M = 0 (which is essentially the case for the problem described in Section 2) 
it sufﬁces as long as γt ≤ L−1. When (ǫt) is summable  we achieve the same O(1/T ) convergence
rate as when there is no error. If (ǫt) decays with a rate of O(1/t)  then the overall convergence
is only affected by a log(T ) factor. Convergence results on the sequence of projections of (¯xT )
onto X1 when F stems from saddle point problem minx1∈X1 supx2∈X2 Φ(x1  x2) is established in
Appendix B.

t=1ǫt

(9)

.

The theoretical convergence rate established in Theorem 3.1 and Corollary B.1 generalizes the pre-
vious result established in Corollary 3.1 in [11] for CMP with exact prox-mappings. Indeed  when
exact prox-mappings are used  we recover the result of [11]. When inexact prox-mappings are used 
the errors due to the inexactness of the prox-mappings accumulate and is reﬂected in (9) and (37).

3.2 Composite Conditional Gradient

We now turn to a variant of the composite conditional gradient algorithm  denoted CCG  tailored
for a particular class of problems  which we call smooth semi-linear problems. The composite
conditional gradient algorithm was ﬁrst introduced in [9] and also developed in [21]. We present an
extension here which turns to be well-suited for sub-problems that will be solved in Sec. 3.3.

Minimizing Smooth Semi-linear Functions. We consider the smooth semi-linear problem

min

x=[u;v]∈X(cid:8)φ+(u  v) = φ(u) + hθ  vi(cid:9)

(10)

represented by the pair (X; φ+) such that the following assumptions are satisﬁed. We assume that

and compact;

i) X ⊂ Eu × Ev is closed convex and its projection P X on Eu belongs to U   where U is convex
ii) φ(u) : U → R is a convex continuously differentiable function  and there exist 1 < κ ≤ 2 and

L0 < ∞ such that

L0
κ ku′ − ukκ ∀u  u′ ∈ U ;

(11)

φ(u′) ≤ φ(u) + h∇φ(u)  u′ − ui +

5

iii) θ ∈ Ev is such that every linear function on Eu × Ev of the form

(12)
with η ∈ Eu attains its minimum on X at some point x[η] = [u[η]; v[η]]; we have at our disposal
a Composite Linear Minimization Oracle (LMO) which  given on input η ∈ Eu  returns x[η].

[u; v] 7→ hη  ui + hθ  vi

Algorithm 2 Composite Conditional Gradient Algorithm CCG(X  φ(·)  θ; ǫ)

Input: accuracy ǫ > 0 and γt = 2/(t + 1)  t = 1  2  . . .
Initialize x1 = [u1; v1] ∈ X
for t = 1  2  . . . do

Compute δt = hgt  ut − ut[gt]i + hθ  vt − vt[gt]i  where gt = ∇φ(ut);
if δt ≤ ǫ then
else

Return xt = [ut; vt]

Find xt+1 = [ut+1; vt+1] ∈ X such that φ+(xt+1) ≤ φ+ (xt + γt(xt[gt] − xt))

end if
end for

The algorithm is outlined in Algorithm 2. Note that CCG works essentially as if there were no v-
component at all. The CCG algorithm enjoys a convergence rate in O(t−(κ−1)) in the evaluations
of the function φ+  and the accuracy certiﬁcates (δt) enjoy the same rate O(t−(κ−1)) as well.
Proposition 3.1. Denote D the k·k-diameter of U . When solving problems of type (10)  the sequence
of iterates (xt) of CCG satisﬁes

φ+(xt) − min

φ+(x) ≤
In addition  the accuracy certiﬁcates (δt) satisfy
δs ≤ O(1)L0Dκ(cid:18) 2

min
1≤s≤t

2L0Dκ

κ(3 − κ)(cid:18) 2
t + 1(cid:19)κ−1

x∈X

t + 1(cid:19)κ−1

  t ≥ 2

  t ≥ 2

(13)

(14)

3.3 Semi-Proximal Mirror-Prox for Semi-structured Variational Inequality

We now give the full description of a special class of variational inequalities  called semi-structured
variational inequalities. This family of problems encompasses both cases that we discussed so far
in Section 3.1 and 3.2. But most importantly  it also covers many other problems that do not fall into
these two regimes and in particular  our essential problem of interest (3).

Semi-structured Variational Inequalities. The class of semi-structured variational inequalities
allows to go beyond Assumptions (A.1)− (A.4)  by assuming more structure. This structure is con-
sistent with what we call a semi-proximal setup  which encompasses both the regular proximal setup
and the regular linear minimization setup as special cases. Indeed  we consider variational inequality
VI(X  F ) that satisﬁes  in addition to Assumptions (A.1) − (A.4)  the following assumptions:
(S.1) Proximal setup for X: we assume that Eu = Eu1 × Eu2   Ev = Ev1 × Ev2   and U ⊂
U1 × U2  X = X1 × X2 with Xi ∈ Eui × Evi and PiX = {ui : [ui; vi] ∈ Xi} ⊂ Ui
for i = 1  2  where U1 is convex and closed  U2 is convex and compact. We also assume that
  with ω2(·) : U2 → R continuously
ω(u) = ω1(u1) + ω2(u2) and kuk = ku1kEu1 +ku2kEu2
differentiable such that

ω2(u′

2) ≤ ω2(u2) + h∇ω2(u2)  u′

2 ∈ U2;
for a particular 1 < κ ≤ 2 and L0 < ∞. Furthermore  we assume that the k · kEu2
of U2 is bounded by some D > 0.

2 − u2i +

2 − u2kκ

 ∀u2  u′

Eu2

L0
κ ku′

-diameter

(S.2) Partition of F : the operator F induced by the above partition of X1 and X2 can be written as

F (x) = [Fu(u); Fv] with Fu(u) = [Fu1 (u1  u2); Fu2 (u1  u2)]  Fv = [Fv1 ; Fv2 ].

6

(S.3) Proximal mapping on X1: we assume that for any η1 ∈ Eu1 and α > 0  we have at our

disposal easy-to-compute prox-mappings of the form 

Proxω1 (η1  α) :=

argmin

x1=[u1;v1]∈X1 {ω1(u1) + hη1  u1i + αhFv1   v1i} .

(S.4) Linear minimization oracle for X2: we assume that we we have at our disposal Composite
Linear Minimization Oracle (LMO)  which given any input η2 ∈ Eu2 and α > 0  returns an
optimal solution to the minimization problem with linear form  that is 

LMO(η2  α) :=

argmin

x2=[u2;v2]∈X2 {hη2  u2i + αhFv2   v2i} .

Semi-proximal setup We denote such problems as Semi-VI(X  F ). On the one hand  when U2 is
a singleton  we get the full-proximal setup. On the other hand  when U1 is a singleton  we get the full
linear-minimization-oracle setup (full LMO setup). The semi-proximal setup allows to cover both
setups and all the ones in between as well.

The Semi-Proximal Mirror-Prox algorithm. We ﬁnally present here our main contribution  the
Semi-Proximal Mirror-Prox algorithm  which solves the semi-structured variational inequality under
(A.1) − (A.4) and (S.1) − (S.4). The Semi-Proximal Mirror-Prox algorithm blends both CMP and
CCG. Basically  for sub-domain X2 given by LMO  instead of computing exactly the prox-mapping 
we mimick inexactly the prox-mapping via a conditional gradient algorithm in the Composite Mirror
Prox algorithm. For the sub-domain X1  we compute the prox-mapping as it is.

Algorithm 3 Semi-Proximal Mirror-Prox Algorithm for Semi-VI(X  F )

1; x1

1 = [u1

[2] Compute yt = [yt

Input: stepsizes γt > 0  accuracies ǫt ≥ 0  t = 1  2  . . .
[1] Initialize x1 = [x1
1]; x1
for t = 1  2  . . .   T do

2] ∈ X  where x1
1; yt
2] that
1 := [but
1;bvt
1] = Proxω1 (γtFu1 (ut
2) − ω′
1(ut
2 := [but
2;bvt
2] = CCG(X2  ω2(·) + hγtFu2 (ut

[3] Compute xt+1 = [xt+1

; xt+1

1; v1

1  ut

] that

yt
yt

2

2 = [u1

2  ; v1
2].

1)  γt)
2) − ω′

1  ut

2(ut

2) ·i  γtFv2 ; ǫt)

xt+1
1
xt+1
2

:= [ut+1
:= [ut+1

1

2

1
; vt+1
; vt+1

1

2

end for

Output: xT := [¯uT ; ¯vT ] = (PT
1 = [but
1;bvt
2) = maxy2∈X2h∇φ+(yt

2) − ω′
] = Proxω1 (γtFu1 (but
1 but
1)  γt)
2) − ω′
] = CCG(X2  ω2(·) + hγtFu2 (but
1 but

1(ut

2(ut

2) ·i  γtFv2 ; ǫt)

t=1 γt)

t=1 γtyt

−1PT

2]

2 = [but
2;bvt

1] by computing the exact prox-mapping and build yt
At step t  we ﬁrst update yt
by running the composite conditional gradient algorithm to problem (10) speciﬁcally with

X = X2  φ(·) = ω2(·) + hγtFu2 (ut

1  ut

2) − ω′

2(ut

2) ·i  and θ = γtFv2  
; vt+1

2

2

; vt+1

2)  yt

2 − y2i ≤ ǫt. We then build xt+1

2 =
] similarly except this time taking the value of the operator at point yt. Combining the

until δ(yt
[ut+1
results in Theorem 3.1 and Proposition 3.1  we arrive at the following complexity bound.
Proposition 3.2. Under the assumption (A.1)− (A.4) and (S.1)− (S.4) with M = 0  and choice of
stepsize γt = L−1  t = 1  . . .   T   for the outlined algorithm to return an ǫ-solution to the variational
inequality V I(X  F )  the total number of Mirror Prox steps required does not exceed

1 = [ut+1

] and xt+1

1

1

and the total number of calls to the Linear Minimization Oracle does not exceed

Total number of steps = O(1)

LΘ[X]

ǫ

N = O(1)(cid:18) L0LκDκ

ǫκ

κ−1

(cid:19) 1

Θ[X].

In particular  if we use Euclidean proximal setup on U2 with ω2(·) = 1
2kx2k2  which leads to κ = 2
and L0 = 1  then the number of LMO calls does not exceed N = O(1)(cid:0)L2D2(Θ[X1] + D2)(cid:1) /ǫ2.

7

l

l

e
u
a
v
 
e
v
i
t
c
e
b
O

j

100

10−1

10−2

 
0

Semi−MP(eps=10/t)
Semi−MP(fixed=96)
Smooth−CG(g =0.01)
Semi−SPG(eps=5/t)
Semi−SPG(fixed=96)

 

100

l

l

e
u
a
v
 
e
v
i
t
c
e
b
O

j

 

Semi−MP(eps=5/t)
Semi−MP(fixed=24)
Smooth−CG(g =1)
Semi−SPG(eps=10/t)
Semi−SPG(fixed=24)

1000

2000

3000

Elapsed time (sec)

10−1

 
0

4000

500

1000
2000
Elapsed time (sec)

1500

2500

3000

l

e
u
a
v
 

e
v
i
t
c
e
b
o

j

0.9

0.8

0.7

0.6

0.5

0.4

0.3
 
0

 

Semi−MP(eps=1e2/t)
Semi−MP(eps=1e1/t)
Semi−MP(eps=1e0/t)
Semi−LPADMM(eps=1e−3/t)
Semi−LPADMM(eps=1e−4/t)
Semi−LPADMM(eps=1e−5/t)

1000

2000

3000

4000

number of LMO calls

5000

l

e
u
a
v
 

e
v
i
t
c
e
b
o

j

0.9

0.8

0.7

0.6

0.5

0.4

0.3
 
0

Semi−MP(eps=1e1/t)
Semi−LPADMM(eps=1e−5/t)

 

400

200
800
number of LMO calls

600

1000

Figure 1: Robust collaborative ﬁltering and link prediction: objective function vs elapsed time.
From left to right: (a) MovieLens100K; (b) MovieLens1M; (c) Wikivote (1024); (d) Wikivote (full)

Discussion The proposed Semi-Proximal Mirror-Prox algorithm enjoys the optimal complexity
bounds  i.e. O(1/ǫ2)  in the number of calls to LMO; see [14] for the optimal complexity bounds
for general non-smooth optimization with LMO. Consequently  when applying the algorithm to the
variational reformulation of the problem of interest (3)  we are able to get an ǫ-optimal solution
within at most O(1/ǫ2) LMO calls. Thus  Semi-Proximal Mirror-Prox generalizes previously
proposed approaches and improves upon them in special cases of problem (3); see Appendix D.2.

4 Experiments

We report the experimental results obtained with the proposed Semi-Proximal Mirror-Prox  denoted
Semi-MP here  and competing algorithms. We consider two different applications: i) robust col-
laborative ﬁltering for movie recommendation; ii) link prediction for social network analysis. For
i)  we compare to two competing approaches: a) smoothing conditional gradient proposed in [24]
(denoted Smooth-CG); b) smoothing proximal gradient [20  5] equipped with semi-proximal setup
(Semi-SPG). For ii)  we compare to Semi-LPADMM  using [22] equipped with semi-proximal
setup. Additional experiments and implementation details are given in Appendix E.

Robust collaborative ﬁltering We consider the collaborative ﬁltering problem  with a nuclear-
norm regularization penalty and an ℓ1-loss function. We run the above three algorithms on the
the small and medium MovieLens datasets. The small-size dataset consists of 943 users and 1682
movies with about 100K ratings  while the medium-size dataset consists of 3952 users and 6040
movies with about 1M ratings. We follow [24] to set the regularization parameters. In Fig. 1  we
can see that Semi-MP clearly outperforms Smooth-CG  while it is competitive with Semi-SPG.

Link prediction We consider now the link prediction problem  where the objective consists a
hinge-loss for the empirical risk part and multiple regularization penalties  namely the ℓ1-norm and
the nuclear-norm. For this example  applying the Smooth-CG or Semi-SPG would require two
smooth approximations  one for hinge loss term and one for ℓ1 norm term. Therefore  we consider
an alternative approach  Semi-LPADMM  where we apply the linearized preconditioned ADMM al-
gorithm [22] by solving proximal mapping through conditional gradient routines. Up to our knowl-
edge  ADMM with early stopping is not fully theoretically analyzed in literature. However  intu-
itively  as long as the error is controlled sufﬁciently  such variant of ADMM should converge.

We conduct experiments on a binary social graph data set called Wikivote  which consists of 7118
nodes and 103747 edges. Since the computation cost of these two algorithms mainly come from the
LMO calls  we present in below the performance in terms of number of LMO calls. For the ﬁrst set
of experiments  we select top 1024 highest degree users from Wikivote and run the two algorithms
on this small dataset with different strategies for the inner LMO calls.

In Fig. 1  we observe that the Semi-MP is less sensitive to the inner accuracies of prox-mappings
compared to the ADMM variant  which sometimes stops progressing if the prox-mappings of early
iterations are not solved with sufﬁcient accuracy. The results on the full dataset corroborate the fact
that Semi-MP outperforms the semi-proximal variant of the ADMM algorithm.

Acknowledgments

The authors would like to thank A. Juditsky and A. Nemirovski for fruitful discussions. This work
was supported by NSF Grant CMMI-1232623  LabEx Persyval-Lab (ANR-11-LABX-0025)  project
“Titan” (CNRS-Mastodons)  project “Macaron” (ANR-14-CE23-0003-01)  the MSR-Inria joint cen-
tre  and the Moore-Sloan Data Science Environment at NYU.

8

References

[1] Francis Bach. Duality between subgradient and conditional gradient methods. SIAM Journal on Opti-

mization  2015.

[2] Francis Bach  Rodolphe Jenatton  Julien Mairal  and Guillaume Obozinski. Optimization with sparsity-

inducing penalties. Found. Trends Mach. Learn.  4(1):1–106  2012.

[3] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert

Spaces. Springer  2011.

[4] D. P. Bertsekas. Convex Optimization Algorithms. Athena Scientiﬁc  2015.

[5] Xi Chen  Qihang Lin  Seyoung Kim  Jaime G Carbonell  and Eric P Xing. Smoothing proximal gradient

method for general structured sparse regression. The Annals of Applied Statistics  6(2):719–752  2012.

[6] Bruce Cox  Anatoli Juditsky  and Arkadi Nemirovski. Dual subgradient algorithms for large-scale nons-

mooth learning problems. Mathematical Programming  pages 1–38  2013.

[7] M. Dudik  Z. Harchaoui  and J. Malick. Lifted coordinate descent for learning with trace-norm regulariza-
tion. Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 
2012.

[8] Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications to

online and stochastic optimization. arXiv preprint arXiv:1301.4666  2013.

[9] Zaid Harchaoui  Anatoli Juditsky  and Arkadi Nemirovski. Conditional gradient algorithms for norm-

regularized smooth convex optimization. Mathematical Programming  pages 1–38  2013.

[10] E. Hazan and S. Kale. Projection-free online learning. In ICML  2012.

[11] Niao He  Anatoli Juditsky  and Arkadi Nemirovski. Mirror prox algorithm for multi-term composite

minimization and semi-separable problems. arXiv preprint arXiv:1311.1098  2013.

[12] Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML  pages 427–

435  2013.

[13] Anatoli Juditsky and Arkadi Nemirovski. Solving variational inequalities with monotone operators on

domains given by linear minimization oracles. arXiv preprint arXiv:1312.107  2013.

[14] Guanghui Lan. The complexity of large-scale convex programming under a linear optimization oracle.

arXiv  2013.

[15] Guanghui Lan and Yi Zhou. Conditional gradient sliding for convex optimization. arXiv  2014.

[16] Cun Mu  Yuqian Zhang  John Wright  and Donald Goldfarb. Scalable robust matrix recovery: Frank-

wolfe meets proximal methods. arXiv preprint arXiv:1403.7588  2014.

[17] Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lips-
chitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal
on Optimization  15(1):229–251  2004.

[18] Arkadi Nemirovski  Shmuel Onn  and Uriel G Rothblum. Accuracy certiﬁcates for computational prob-

lems with convex structure. Mathematics of Operations Research  35(1):52–78  2010.

[19] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical programming  103(1):127–

152  2005.

[20] Yurii Nesterov. Smoothing technique and its applications in semideﬁnite optimization. Math. Program. 

110(2):245–259  2007.

[21] Yurii Nesterov. Complexity bounds for primal-dual methods minimizing the model of objective function.
Technical report  Universit´e catholique de Louvain  Center for Operations Research and Econometrics
(CORE)  2015.

[22] Yuyuan Ouyang  Yunmei Chen  Guanghui Lan  and Eduardo Pasiliao Jr. An accelerated linearized alter-

nating direction method of multipliers  2014. http://arxiv.org/abs/1401.6607.

[23] Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization  pages

1–96  2013.

[24] Federico Pierucci  Zaid Harchaoui  and J´erˆome Malick. A smoothing approach for composite conditional

gradient with nonsmooth loss. In Conf´erence dApprentissage Automatique–Actes CAP14  2014.

[25] Mark Schmidt  Nicolas L. Roux  and Francis R. Bach. Convergence rates of inexact proximal-gradient

methods for convex optimization. In Adv. NIPS. 2011.

[26] X. Zhang  Y. Yu  and D. Schuurmans. Accelerated training for matrix-norm regularization: A boosting

approach. In NIPS  2012.

9

,Niao He
Zaid Harchaoui
Taiji Suzuki
Heishiro Kanagawa
Hayato Kobayashi
Nobuyuki Shimizu
Yukihiro Tagami
Yi Zhou
Zhe Wang
Yingbin Liang