2014,Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection,Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end  various useful approaches have been proposed in the context of $\ell_1$ penalized estimation in the Gaussian framework. Though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization  they still depend on the Gaussian functional form. To address this gap  a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood  and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however  this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper  we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing $\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\ell_1$-penalized partial correlation graph estimation outside the Gaussian setting  thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence  and also derive rates thereof.,Optimization Methods for Sparse Pseudo-Likelihood

Graphical Model Selection

Sang-Yun Oh

Computational Research Division
Lawrence Berkeley National Lab

syoh@lbl.gov

Kshitij Khare

Department of Statistics

University of Florida

kdkhare@stat.ufl.edu

Onkar Dalal

Stanford University

onkar@alumni.stanford.edu

Bala Rajaratnam

Department of Statistics

Stanford University

brajarat@stanford.edu

Abstract

Sparse high dimensional graphical model selection is a popular topic in contempo-
rary machine learning. To this end  various useful approaches have been proposed
in the context of (cid:96)1-penalized estimation in the Gaussian framework. Though
many of these inverse covariance estimation approaches are demonstrably scal-
able and have leveraged recent advances in convex optimization  they still depend
on the Gaussian functional form. To address this gap  a convex pseudo-likelihood
based partial correlation graph estimation method (CONCORD) has been recently
proposed. This method uses coordinate-wise minimization of a regression based
pseudo-likelihood  and has been shown to have robust model selection proper-
ties in comparison with the Gaussian approach. In direct contrast to the parallel
work in the Gaussian setting however  this new convex pseudo-likelihood frame-
work has not leveraged the extensive array of methods that have been proposed
in the machine learning literature for convex optimization. In this paper  we ad-
dress this crucial gap by proposing two proximal gradient methods (CONCORD-
ISTA and CONCORD-FISTA) for performing (cid:96)1-regularized inverse covariance
matrix estimation in the pseudo-likelihood framework. We present timing com-
parisons with coordinate-wise minimization and demonstrate that our approach
yields tremendous payoffs for (cid:96)1-penalized partial correlation graph estimation
outside the Gaussian setting  thus yielding the fastest and most scalable approach
for such problems. We undertake a theoretical analysis of our approach and rigor-
ously demonstrate convergence  and also derive rates thereof.

1

Introduction

Sparse inverse covariance estimation has received tremendous attention in the machine learning 
statistics and optimization communities. These sparse models  popularly known as graphical mod-
els  have widespread use in various applications  especially in high dimensional settings. The most
popular inverse covariance estimation framework is arguably the (cid:96)1-penalized Gaussian likelihood
optimization framework as given by
minimize
Ω∈Sp

− log det Ω + tr(SΩ) + λ(cid:107)Ω(cid:107)1

on the elements of Ω = (ωij)1≤i≤j≤p by the term (cid:107)Ω(cid:107)1 =(cid:80)

++ denotes the space of p-dimensional positive deﬁnite matrices  and (cid:96)1-penalty is imposed
i j |ωij| along with the scaling factor

where Sp

++

1

λ > 0. The matrix S denotes the sample covariance matrix of the data Y ∈ IRn×p. As the (cid:96)1-
penalized log likelihood is convex  the problem becomes more tractable and has beneﬁted from
advances in convex optimization. Recent efforts in the literature on Gaussian graphical models
therefore have focused on developing principled methods which are increasingly more and more
scalable. The literature on this topic is simply enormous and for the sake of brevity  space constraints
and the topic of this paper  we avoid an extensive literature review by referring to the references in
the seminal work of [1] and the very recent work of [2]. These two papers contain references to
recent work  including past NIPS conference proceedings.

1.1 The CONCORD method

Despite their tremendous contributions  one shortcoming of the traditional approaches to (cid:96)1-
penalized likelihood maximization is the restriction to the Gaussian assumption. To address this
gap  a number of (cid:96)1-penalized pseudo-likelihood approaches have been proposed: SPACE [3] and
SPLICE [4]  SYMLASSO [5]. These approaches are either not convex  and/or convergence of
corresponding maximization algorithms are not established.
In this sense  non-Gaussian partial
correlation graph estimation methods have lagged severely behind  despite the tremendous need to
move beyond the Gaussian framework for obvious practical reasons. In very recent work  a con-
vex pseudo-likelihood approach with good model selection properties called CONCORD [6] was
proposed. The CONCORD algorithm minimizes

Qcon(Ω) = − p(cid:88)

n log ωii +

1
2

i=1

p(cid:88)

i=1

(cid:88)

j(cid:54)=i

(cid:107)ωiiYi +

ωijYj(cid:107)2

2 + nλ

|ωij|

(1)

(cid:88)

1≤i<j≤p

(cid:17)(cid:17)

(2)

(3)

(4)

via cyclic coordinate-wise descent that alternates between updating off-diagonal elements and diag-
onal elements. It is straightforward to show that operators Tij for updating (ωij)1≤i<j≤p (holding
(ωii)1≤i≤p constant) and Tii for updating (ωii)1≤i≤p (holding (ωij)1≤i<j≤p constant) are given by

Sλ

(cid:16)−(cid:16)(cid:80)
−(cid:80)

j(cid:48)(cid:54)=j ωij(cid:48)sjj(cid:48) +(cid:80)
(cid:114)(cid:16)(cid:80)

sii + sjj

j(cid:54)=i ωijsij +

j(cid:54)=i ωijsij

(cid:17)2

i(cid:48)(cid:54)=i ωi(cid:48)jsii(cid:48)

+ 4sii

.

2sii

(Tij(Ω))ij =

(Tii(Ω))ii =

This coordinate-wise algorithm is shown to converge to a global minima though no rate is given
[6]. Note that the equivalent problem assuming a Gaussian likelihood has seen much development
in the last ten years  but a parallel development for the recently introduced CONCORD framework
is lacking for obvious reasons. We address this important gap by proposing state-of-the-art proxi-
mal gradient techniques to minimize Qcon. A rigorous theoretical analysis of the pseudo-likelihood
framework and the associated proximal gradient methods which are proposed is undertaken. We
establish rates of convergence and also demonstrate that our approach can lead to massive computa-
tional speed-ups  thus yielding extremely fast and principled solvers for the sparse inverse covariance
estimation problem outside the Gaussian setting.

2 CONCORD using proximal gradient methods

The penalized matrix version the CONCORD objective function in (1) is given by

(cid:2)− log det Ω2

Qcon(Ω) =

n
2

D + tr(SΩ2) + λ(cid:107)ΩX(cid:107)1

(cid:3) .

where ΩD and ΩX denote the diagonal and off-diagonal elements of Ω. We will use the notation
A = AD + AX to split any matrix A into its diagonal and off-diagonal terms.
This section proposes a scalable and thorough approach to solving the CONCORD objective func-
tion using recent advances in convex optimization and derives rates of convergence for such algo-
rithms. In particular  we use proximal gradient-based methods to achieve this goal and demonstrate
the efﬁcacy of such methods for the non-Gaussian graphical modeling problem. First  we propose
CONCORD-ISTA and CONCORD-FISTA in section 2.1: methods which are inspired by the itera-
tive soft-thresholding algorithms in [7]. We undertake a comprehensive treatment of the CONCORD

2

optimization problem by also investigating the dual of the CONCORD problem. Other popular
methods in the literature  including the potential use of alternating minimization algorithm and the
second order proximal Newtons method are considered in Supplemental section A.8.

2.1

Iterative Soft Thresholding Algorithms: CONCORD-ISTA  CONCORD-FISTA

The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal
paper by Beck and Teboulle [7]. The ISTA methods are based on the Forward-Backward Splitting
method from [8] and Nesterov’s accelerated gradient methods [9] using soft-thresholding as the
proximal operator for the (cid:96)1-norm. The essence of the proximal gradient algorithms is to divide
the objective function into a smooth part and a non-smooth part  then take a proximal step (w.r.t.
the non-smooth part) in the negative gradient direction of the smooth part. Nesterov’s accelerated
gradient extension [9] uses a combination of gradient and momentum steps to achieve accelerated
rates of convergence. In this section  we apply these methods in the context of CONCORD which
also has a composite objective function.
The matrix CONCORD objective function (4) can be split into a smooth part h1(Ω) and a non-
smooth part h2(Ω):

h1(Ω) = − log det ΩD +

tr(ΩSΩ)  h2(Ω) = λ(cid:107)ΩX(cid:107)1.

The gradient and hessian of the smooth function h1 are given by

(cid:0)SΩT + ΩS(cid:1)  
T(cid:3) +
(cid:2)eiei

T ⊗ eiei

1
2

∇h1(Ω) = −ΩD

−1 +

∇2h1(Ω) =

ω−2

ii

i=p(cid:88)

i=1

(5)

(6)

1
2

(cid:26)

(S ⊗ I + I ⊗ S)  

1
2

(cid:27)

where ei is a column vector of zeros except for a one in the i-th position.
The proximal operator for h2 is given by element-wise soft-thresholding operator Sλ as

proxh2

(Ω) = arg min

h2(Θ) +

Θ

(cid:107)Ω − Θ(cid:107)2

F

1
2

(7)
where Λ is a matrix with 0 diagonal and λ for each off-diagonal entry. The details of the proximal
gradient algorithm CONCORD-ISTA are given in Algorithm 1  and the details of the accelerated
proximal gradient algorithm CONCORD-FISTA are given in Algorithm 2.

= SΛ(Ω) = sign(Ω) max{|Ω| − Λ  0} 

2.2 Choice of step size

In the absence of a good estimate of the Lipschitz constant L  the step size for each iteration of
CONCORD-ISTA and CONCORD-FISTA is chosen using backtracking line search. The line search
for iteration k starts with an initial step size τ(k 0) and reduces the step with a constant factor c until
the new iterate satisﬁes the sufﬁcient descent condition:

h1(Ω(k+1)) ≤ Q(Ω(k+1)  Ω(k))

Q(Ω  Θ) = h1(Θ) + tr(cid:0)(Ω − Θ)T∇h1(Θ)(cid:1) +

(8)

(cid:13)(cid:13)Ω − Θ(cid:13)(cid:13)2

F .

1
2τ

where 

In section 4  we have implemented algorithms choosing the initial step size in three different ways:
(a) a constant starting step size (=1)  (b) the feasible step size from the previous iteration τk−1  (c)
the step size heuristic of Barzilai-Borwein. The Barzilai-Borwein heuristic step size is given by

tr(cid:0)(Ω(k+1) − Ω(k))T (Ω(k+1) − Ω(k))(cid:1)
tr(cid:0)(Ω(k+1) − Ω(k))T (G(k+1) − G(k))(cid:1) .

τk+1 0 =

(9)

This is an approximation of the secant equation which works as a proxy for second order information
using successive gradients (see [10] for details).

3

Algorithm 1 CONCORD-ISTA

Algorithm 2 CONCORD-FISTA

Input: sample covariance matrix S  penalty Λ
Set: Ω(0) ∈ Sp
+  τ(0 0) ≤ 1  c < 1  ∆subg = 1
while ∆subg > subg do

G(k) = −(cid:16)

Ω(k)
D

(cid:17)−1
(cid:0)S Ω(k) + Ω(k)S(cid:1)
Ω(k) − τkG(k)(cid:17) (cid:96) (8).
(cid:16)

+ 1
2

Take largest τk ∈ {cjτ(k 0)}j=0 1 ... s.t.

Ω(k+1) = SτkΛ
Compute: τ(k+1 0)
1
Compute: ∆subg

end while

1: ∆subg =

(cid:107)∇h1(Ω(k)) + ∂h2(Ω(k))(cid:107)

(cid:107)Ω(k)(cid:107)

2.3 Computational complexity

Θ(k)
D

G(k) = −(cid:16)

Take largest τk ∈ {cjτ(k 0)}j=0 1 ... s.t.

Input: sample covariance matrix S  penalty Λ
+  α1 = 1  τ(0 0) ≤ 1 
Set: (Θ(1) =)Ω(0) ∈ Sp
c < 1  ∆subg = 1.
(cid:17)−1
(cid:0)SΘ(k) + Θ(k)S(cid:1)
while ∆subg > subg do
Θ(k) − τkG(k)(cid:17) (cid:96) (8)
(cid:16)
(cid:112)
Ω(k) = SτkΛ
(cid:17)(cid:0)Ω(k) − Ω(k−1)(cid:1)
(cid:16) αk−1
αk+1 = (1 +
Θ(k+1) = Ω(k) +
Compute: τ(k+1 0)
1
Compute: ∆subg

1 + 4αk

2)/2

+ 1
2

αk+1

end while

tr(Ω(SΩ)) can be computed efﬁciently using tr(ΩW ) =(cid:80) ωijwij over the set of non-zero ωij’s.

After the one time calculation of S 
the most signiﬁcant computation for each iteration in
CONCORD-ISTA and CONCORD-FISTA algorithms is the matrix-matrix multiplication W = SΩ
in the gradient term. If s is the number of non-zeros in Ω  then W can be computed using O(sp) op-
erations if we exploit the extreme sparsity in Ω. The second matrix-matrix multiplication for the term
This computation only requires O(s) operations. The remaining computations are all at the element
level which can be completed in O(p2) operations. Therefore  the overall computational complexity
for each iteration reduces to max(O(sp) O(p2)). On the other hand  the proximal gradient algo-
rithms for the Gaussian framework require inversion of a full p×p matrix which is non-parallelizable
and requires O(p3) operations. The coordinate-wise method for optimizing CONCORD in [6] also
requires cycling through the p2 entries of Ω in speciﬁed order and thus does not allow parallelization.
In contrast  CONCORD-ISTA and CONCORD-FISTA can use ‘perfectly parallel’ implementations
to distribute the above matrix-matrix multiplications. At no step do we need to keep all of the dense
matrices S  SΩ ∇h1 on a single machine. Therefore  CONCORD-ISTA and CONCORD-FISTA
are scalable to any high dimensions restricted only by the number of machines.

3 Convergence Analysis

In this section  we prove convergence of CONCORD-ISTA and CONCORD-FISTA methods along
with their respective convergence rates of O(1/k) and O(1/k2). We would like to point out that 
although the authors in [6] provide a proof of convergence for their coordinate-wise minimization
algorithm for CONCORD  they do not provide any rates of convergence. The arguments for con-
vergence leverage the results in [7] but require some essential ingredients. We begin with proving
lower and upper bounds on the diagonal entries ωkk for Ω belonging to a level set of Qcon(Ω). The
lower bound on the diagonal entries of Ω establishes Lipschitz continuity of the gradient ∇h1(Ω)
based on the hessian of the smooth function as stated in (6). The proof for the lower bound uses the
existence of an upper bound on the diagonal entries. Hence  we prove both bounds on the diagonal
entries. We begin by deﬁning a level set C0 of the objective function starting with an arbitrary initial
point Ω(0) with a ﬁnite function value as

C0 =

Ω | Qcon(Ω) ≤ Qcon(Ω(0)) = M

.

(10)

(cid:110)

(cid:111)

For the positive semideﬁnite matrix S  let U denote 1√
times the upper triangular matrix from the
LU decomposition of S  such that S = 2U T U (the factor 2 simpliﬁes further arithmetic). Assuming

2

4

the diagonal entries of S to be strictly nonzero (if skk = 0  then the kth component can be ignored
upfront since it has zero variance and is equal to a constant for every data point)  we have at least
one k such that uki (cid:54)= 0 for every i. Using this  we prove the following theorem.
Theorem 3.1. For any symmetric matrix Ω satisfying Ω ∈ C0  the diagonal elements of Ω are
bounded above and below by constants which depend only on M  λ and S. In other words 

0 < a ≤ |ωkk| ≤ b  ∀ k = 1  2  . . .   p 

for some constants a and b.
Proof. (a) Upper bound: Suppose |ωii| = max{|ωkk|  for k = 1  2  . . .   p}. Then  we have

Considering kith entry in the Frobenious norm and the ith column in the third term we get

M = Qcon(Ω(0)) ≥ Qcon(Ω) = h1(Ω) + h2(Ω)

≥ − log det ΩD + tr(cid:0)(U Ω)T (U Ω)(cid:1) + λ(cid:107)ΩX(cid:107)1
F + λ(cid:107)ΩX(cid:107)1.
j=p(cid:88)

= − log det ΩD + (cid:107)U Ω(cid:107)2

ukjωji

+ λ

2

j=p(cid:88)

j=k

|ωji|.

j=k j(cid:54)=i

M ≥ −p log |ωii| +

Now  suppose |ukiωii| = z and(cid:80)j=p
|x| ≤ j=p(cid:88)

j=k j(cid:54)=i

j=k j(cid:54)=i ukjωji = x. Then

|ukj||ωji| ≤ ¯u

|ωji| 

j=p(cid:88)

j=k j(cid:54)=i

(11)

(12)

where ¯u = max{|ukj|}  for j = k  . . .   p  j (cid:54)= i. Substituting in (12)  for ¯λ = λ

¯M = M + ¯λ2 − p log |uki| ≥ −p log z + (z + x)2 + 2¯λ|x| + ¯λ2

= −p log z +(cid:0)z + x + ¯λsign(x)(cid:1)2 − 2¯λz sign(x)

(13)
(14)
Here  if x ≥ 0  then ¯M ≥ −p log z + z2 using the ﬁrst inequality (13)  and if x < 0  then ¯M ≥
−p log z + 2¯λz using the second inequality (14). In either cases  the functions −p log z + z2 and
−p log z + 2¯λz are unbounded as z → ∞. Hence  the upper bound of ¯M on these functions
guarantee an upper bound b such that |ωii| ≤ b. Therefore  |ωkk| ≤ b for all k = 1  2  . . .   p.
(b) Lower bound: By positivity of the trace term and the (cid:96)1 term (for off-diagonals)  we have

2¯u  we have

(15)
The negative log function g(z) = − log(z) is a convex function with a lower bound at z∗ = b with
g(z∗) = − log b. Therefore  for any k = 1  2  . . .   p  we have

i=1

M ≥ − log det ΩD =

− log |ωii|.

i=p(cid:88)

− log |ωii| ≥ −(p − 1) log b − log |ωkk|.

(16)

M ≥ i=p(cid:88)

i=1

Simplifying the above equation  we get

log |ωkk| ≥ −M − (p − 1) log b.

Therefore  |ωkk| ≥ a = e−M−(p−1) log b > 0 serves as a lower bound for all k = 1  2  . . .   p.

Given that the function values are non-increasing along the iterates of Algorithms 1  2 and 3  the
sequence of Ω(k) satisfy Ω(k) ∈ C0 for k = 1  2  ..... The lower bounds on the diagonal elements of
Ω(k) provides the Lipschitz continuity using

∇2h1(Ω(k)) (cid:22)(cid:0)a−2 + (cid:107)S(cid:107)2

(cid:1) (I ⊗ I) .

Therefore  using the mean-value theorem  the gradient ∇h1 satisﬁes
(cid:107)∇h1(Ω) − ∇h1(Θ)(cid:107)F ≤ L(cid:107)Ω − Θ(cid:107)F  

(18)
with the Lipschitz continuity constant L = a−2 + (cid:107)S(cid:107)2. The remaining argument for convergence
follows from the theorems in [7].

(17)

5

Theorem 3.2. ([7  Theorem 3.1]). Let {Ω(k)} be the sequence generated by either Algorithm 1 with
constant step size or with backtracking line-search. Then  for the solution Ω∗  for any k ≥ 1 

Qcon(Ω(k)) − Qcon(Ω∗) ≤ αL(cid:107)Ω(0) − Ω∗(cid:107)2

F

2k

 

(19)

where α = 1 for the constant step size setting and α = c for the backtracking step size setting.
Theorem 3.3. ([7  Theorem 4.4]). Let {Ω(k)} {Θ(k)} be the sequences generated by Algorithm 2
with either constant step size or backtracking line-search. Then  for the solution Ω∗  for any k ≥ 1 

Qcon(Ω(k)) − Qcon(Ω∗) ≤ 2αL(cid:107)Ω(0) − Ω∗(cid:107)2

F

(k + 1)2

 

(20)

where α = 1 for the constant step size setting and α = c for the backtracking step size setting.
Hence  CONCORD-ISTA and CONCORD-FISTA converge at the rates of O(1/k) and O(1/k2) for
the kth iteration.

4

Implementation & Numerical Experiments

In this section  we outline algorithm implementation details and present results of our comprehen-
sive numerical evaluation. Section 4.1 gives performance comparisons from using synthetic multi-
variate Gaussian datasets. These datasets are generated from a wide range of sample sizes (n) and
dimensionality (p). Additionally  convergence of CONCORD-ISTA and CONCORD-FISTA will be
illustrated. Section 4.2 has timing results from analyzing a real breast cancer dataset with outliers.
Comparisons are made to the coordinate-wise CONCORD implementation in gconcord package
for R available at http://cran.r-project.org/web/packages/gconcord/.
For implementing the proposed algorithms  we can take advantage of existing linear algebra li-
braries. Most of the numerical computations in Algorithms 1 and 2 are linear algebra opera-
tions  and  unlike the sequential coordinate-wise CONCORD algorithm  CONCORD-ISTA and
CONCORD-FISTA implementations can solve increasingly larger problems as more and more scal-
able and efﬁcient linear algebra libraries are made available. For this work  we opted to using Eigen
library [11] for its sparse linear algebra routines written in C++. Algorithms 1 and 2 were also writ-
ten in C++ then interfaced to R for testing. Table 1 gives names for various CONCORD-ISTA and
CONCORD-FISTA versions using different initial step size choices.

4.1 Synthetic Datasets

Synthetic datasets were generated from true sparse positive random Ω matrices of three sizes:
p = {1000  3000  5000}. Instances of random matrices used here consist of 4995  14985 and
24975 non-zeros  corresponding to 1%  0.33% and 0.20% edge densities  respectively. For each
p  Gaussian and t-distributed datasets of sizes n = {0.25p  0.75p  1.25p} were used as inputs.
The initial guess  Ω(0)  and the convergence criteria was matched to those of coordinate-wise CON-
CORD implementation. Highlights of the results are summarized below  and the complete set of
comparisons are given in Supplementary materials Section A.
For normally distributed synthetic datasets  our experiments indicate that two variations of the
CONCORD-ISTA method show little performance difference. However  ccista 0 was marginally
faster in our tests. On the other hand  ccfista 1 variation of CONCORD-FISTA that uses
τ(k+1 0) = τk as initial step size was signiﬁcantly faster than ccfista 0. Table 2 gives actual
running times for the two best performing algorithms  ccista 0 and ccfista 1  against the
coordinate-wise concord. As p and n increase ccista 0 performs very well. For smaller n
and λ  coordinate-wise concord performs well (more in Supplemental section A). This can be
attributed to min(O(np2) O(p3)) computational complexity of coordinate-wise CONCORD [6] 
and the sparse linear algebra routines used in CONCORD-ISTA and CONCORD-FISTA implemen-
tations slowing down as the number of non-zero elements in Ω increases. On the other hand  for
large n fraction (n = 1.25p)  the proposed methods ccista 0 and ccfista 1 are signiﬁcantly
faster than coordinate-wise concord. In particular  when p = 5000 and n = 6250  the speed-up
of ccista 0 can be as much as 150 times over coordinate-wise concord. Also  for t-distributed
synthetic datasets  ccista 0 is generally fastest  especially when n and p are both large.

6

Figure 1: Convergence of CONCORD-ISTA and CONCORD-FISTA for threshold ∆subg < 10−5

When a good initial guess Ω(0) is available  warm-starting cc ista 0 and cc fista 0 algorithms
substantially shortens the running times. Simulations with Gaussian datasets indicate the running
times can be shortened by  on average  as much as 60%. Complete simulation results are given in
the Supplemental Section A.6.
Convergence behavior of CONCORD-ISTA and CONCORD-FISTA methods is shown in Figure
1. The best performing algorithms ccista 0 and ccfista 1 are shown. The vertical axis is
the subgradient ∆subg (See Algorithms 1  2). Plots show that ccista 0 seems to converge at a
constant rate much faster than ccfista 1 that appears to slow down after a few initial iterations.
While the theoretical convergence results from section 3 prove convergence rates of O(1/k) and
O(1/k2) for CONCORD-ISTA and CONCORD-FISTA  in practice  ccista 0 with constant step
size performed the fastest for the tests in this section.

4.2 Real Data

Real datasets arising from various physical and biological sciences often are not multivariate Gaus-
sian and can have outliers. Hence  convergence characteristic may be different on such datasets. In
this section  the performance of proposed methods are assessed on a breast cancer dataset [12]. This
dataset contains expression levels of 24481 genes on 266 patients with breast cancer. Following the
approach in Khare et al. [6]  the number of genes are reduced by utilizing clinical information that is
provided together with the microarray expression dataset. In particular  survival analysis via univari-
ate Cox regression with patient survival times is used to select a subset of genes closely associated
with breast cancer. A choice of p-value < 0.03 yields a reduced dataset with p = 4433 genes.
Often times  graphical model selection algorithms are applied in a non-Gaussian and n (cid:28) p setting
such as the case here. In this n (cid:28) p setting  coordinate-wise CONCORD algorithm is especially
fast due to its computational complexity O(np2). However  even in this setting  the newly proposed
methods ccista 0  ccista 1  and ccfista 1 perform competitively to  or often better than 
concord as illustrated in Table 3. On this real dataset  ccista 1 performed the fastest whereas
ccista 0 was the fastest on synthetic datasets.

5 Conclusion

The Gaussian graphical model estimation or inverse covariance estimation has seen tremendous ad-
vances in the past few years. In this paper we propose using proximal gradient methods to solve
the general non-Gaussian sparse inverse covariance estimation problem. Rates of convergence
were established for the CONCORD-ISTA and CONCORD-FISTA algorithms. Coordinate-wise
minimization has been the standard approach to this problem thus far  and we provide numer-
ical results comparing CONCORD-ISTA/FISTA and coordinate-wise minimization. We demon-
strate that CONCORD-ISTA outperforms coordinate-wise in general  and in high dimensional set-
tings CONCORD-ISTA can outperform coordinate-wise optimization by orders of magnitude. The
methodology is also tested on real data sets. We undertake a comprehensive treatment of the prob-
lem by also examining the dual formulation and consider methods to maximize the dual objective.
We note that efforts similar to ours for the Gaussian case has appeared in not one  but several NIPS
and other publications. Our approach on the other hand gives a complete and thorough treatment of
the non-Gaussian partial correlation graph estimation problem  all in this one self-contained paper.

7

ccista_0ccfista_1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1e−011e−021e−031e−041e−050204002040iterDsubgmethodllccista_0ccfista_1lambdal0.050.10.20.40.5ccista_0ccfista_1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1e−011e−021e−031e−041e−050204002040iterDsubgmethodllccista_0ccfista_1lambdal0.050.10.20.40.5ccista_0ccfista_1lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll1e−011e−021e−031e−041e−050204002040iterDsubgmethodllccista_0ccfista_1lambdal0.050.10.20.40.5Table 1: Naming convention for step size variations

Variation
Method
Initial step

concord

Coordinatewise

-

ccista 0

ISTA

Constant

ccista 1

ccfista 0

ccfista 1

ISTA

Barzilai-Borwein

FISTA
Constant

FISTA

τk

Table 2: Timing comparison of concord and proposed methods: ccista 0 and ccfista 1.

concord

ccista 0

p

n

250

1000

750

1250

750

3000

2250

3750

1250

5000

3750

6250

λ

0.150
0.163
0.300
0.090
0.103
0.163
0.071
0.077
0.163
0.090
0.103
0.163
0.053
0.059
0.090
0.040
0.053
0.163
0.066
0.077
0.103
0.039
0.049
0.077
0.039
0.077
0.163

NZ%
1.52
0.99
0.05
1.50
0.76
0.23
1.41
0.97
0.23
1.10
0.47
0.08
1.07
0.56
0.16
1.28
0.28
0.07
1.42
0.53
0.10
1.36
0.31
0.10
0.27
0.10
0.04

iter
9
9
9
9
9
9
9
9
9
17
17
16
16
16
16
16
16
15
17
17
17
17
17
17
17
17
16

seconds
3.2
2.6
2.6
8.9
8.4
8.0
41.3
40.5
43.8
147.4
182.4
160.1
388.3
435.0
379.4
2854.2
2921.5
2780.5
832.7
674.7
667.6
2102.8
1826.6
2094.7
15629.3
15671.1
14787.8

iter
13
18
15
11
15
15
10
15
13
20
28
28
17
28
16
17
15
25
32
30
27
18
16
29
17
27
26

seconds
1.8
2.0
1.2
1.4
1.6
1.6
1.4
1.7
1.2
32.4
36.0
28.3
28.5
38.5
19.9
33.0
23.5
35.1
193.9
121.4
81.2
113.0
73.4
95.8
93.9
101.0
97.3

ccfista 1
seconds
iter
3.3
20
3.3
26
2.7
23
17
2.5
3.3
24
2.8
24
2.9
17
24
3.3
2.8
23
53.2
25
60.1
35
39.9
26
17
39.6
61.9
26
23.6
15
47.3
17
16
31.4
56.1
32
379.2
37
265.8
35
163.0
33
17
176.3
107.4
17
178.1
33
130.0
17
123.9
25
34
173.7

Table 3: Running time comparison on breast cancer dataset

λ

0.450
0.451
0.454
0.462
0.478
0.515
0.602
0.800

NZ% concord
sec
724.5
0.110
664.2
0.109
0.106
690.3
671.6
0.101
663.3
0.088
600.6
0.063
383.5
0.027
0.002
193.6

iter
80
80
80
79
77
63
46
24

ccista 0
sec
iter
686.7
132
669.2
129
130
686.2
640.4
125
558.6
117
466.0
104
308.0
80
45
133.8

ccista 1
sec
iter
504.0
123
457.0
112
352.9
81
447.1
109
337.9
87
282.4
75
229.7
66
92.2
32

ccfista 0
sec
iter
10870.3
250
7867.2
216
213
7704.2
7978.4
214
6913.1
202
9706.9
276
4685.2
172
74
1077.2

ccfista 1
sec
iter
672.6
201
662.9
199
198
677.8
646.3
196
609.0
197
542.0
184
409.1
152
70
169.8

Acknowledgments: S.O.  O.D. and B.R. were supported in part by the National Science Foun-
dation under grants DMS-0906392  DMS-CMG 1025465  AGS-1003823  DMS-1106642  DMS
CAREER-1352656 and grants DARPA-YFAN66001-111-4131 and SMC-DBNKY. K.K was par-
tially supported by NSF grant DMS-1106084. S.O. was supported also in part by the Laboratory
Directed Research and Development Program of Lawrence Berkeley National Laboratory under
U.S. Department of Energy Contract No. DE-AC02-05CH11231.

8

References
[1] Onureena Banerjee  Laurent El Ghaoui  and Alexandre DAspremont. Model Selection
Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data.
JMLR  9:485–516  2008.

[2] Onkar Anant Dalal and Bala Rajaratnam. G-ama: Sparse gaussian graphical model estimation

via alternating minimization. arXiv preprint arXiv:1405.3034  2014.

[3] Jie Peng  Pei Wang  Nengfeng Zhou  and Ji Zhu. Partial Correlation Estimation by Joint Sparse
Regression Models. Journal of the American Statistical Association  104(486):735–746  June
2009.

[4] Guilherme V Rocha  Peng Zhao  and Bin Yu. A path following algorithm for Sparse Pseudo-

Likelihood Inverse Covariance Estimation (SPLICE). Technical Report 60628102  2008.

[5] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Applications of the lasso and grouped

lasso to the estimation of sparse graphical models. Technical report  2010.

[6] Kshitij Khare  Sang-Yun Oh  and Bala Rajaratnam. A convex pseudo-likelihood framework
for high dimensional partial correlation estimation with convergence guarantees. Journal of
the Royal Statistical Society: Series B (to appear)  2014.

[7] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[8] R.T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on

[9] Yurii Nesterov. A method of solving a convex programming problem with convergence rate

Control and Optimization  14(5):877–898  1976.
O(1/k2). In Soviet Mathematics Doklady  volume 27  pages 372–376  1983.

[10] J. Barzilai and J.M. Borwein. Two-point step size gradient methods. IMA Journal of Numerical

Analysis  8(1):141–148  1988.

[11] Ga¨el Guennebaud  Benoˆıt Jacob  et al. Eigen v3. http://eigen.tuxfamily.org  2010.
[12] Howard Y Chang  Dimitry S A Nuyten  Julie B Sneddon  Trevor Hastie  Robert Tibshirani 
Therese Sørlie  Hongyue Dai  Yudong D He  Laura J van’t Veer  Harry Bartelink  Matt van de
Rijn  Patrick O Brown  and Marc J van de Vijver. Robustness  scalability  and integration of
a wound-response gene expression signature in predicting breast cancer survival. Proceedings
of the National Academy of Sciences of the United States of America  102(10):3738–43  March
2005.

9

,Sang Oh
Kshitij Khare
Bala Rajaratnam