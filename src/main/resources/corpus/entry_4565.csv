2014,Rates of Convergence for Nearest Neighbor Classification,We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample  distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds  and enable us  as a by-product  to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find  for instance  that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.,Rates of convergence for nearest neighbor

classiﬁcation

Kamalika Chaudhuri

Computer Science and Engineering
University of California  San Diego

kamalika@cs.ucsd.edu

Sanjoy Dasgupta

Computer Science and Engineering
University of California  San Diego

dasgupta@cs.ucsd.edu

Abstract

We analyze the behavior of nearest neighbor classiﬁcation in metric spaces and
provide ﬁnite-sample  distribution-dependent rates of convergence under minimal
assumptions. These are more general than existing bounds  and enable us  as a
by-product  to establish the universal consistency of nearest neighbor in a broader
range of data spaces than was previously known. We illustrate our upper and lower
bounds by introducing a new smoothness class customized for nearest neighbor
classiﬁcation. We ﬁnd  for instance  that under the Tsybakov margin condition the
convergence rate of nearest neighbor matches recently established lower bounds
for nonparametric classiﬁcation.

1

Introduction

In this paper  we deal with binary prediction in metric spaces. A classiﬁcation problem is deﬁned
by a metric space (X   ρ) from which instances are drawn  a space of possible labels Y = {0  1} 
and a distribution P over X × Y. The goal is to ﬁnd a function h : X → Y that minimizes the
probability of error on pairs (X  Y ) drawn from P; this error rate is the risk R(h) = P(h(X) (cid:54)= Y ).
The best such function is easy to specify: if we let µ denote the marginal distribution of X and η
the conditional probability η(x) = P(Y = 1|X = x)  then the predictor 1(η(x) ≥ 1/2) achieves
the minimum possible risk  R∗ = EX [min(η(X)  1− η(X))]. The trouble is that P is unknown and
thus a prediction rule must instead be based only on a ﬁnite sample of points (X1  Y1)  . . .   (Xn  Yn)
drawn independently at random from P.
Nearest neighbor (NN) classiﬁers are among the simplest prediction rules. The 1-NN classiﬁer
assigns each point x ∈ X the label Yi of the closest point in X1  . . .   Xn (breaking ties arbitrarily 
say). For a positive integer k  the k-NN classiﬁer assigns x the majority label of the k closest points
in X1  . . .   Xn. In the latter case  it is common to let k grow with n  in which case the sequence
(kn : n ≥ 1) deﬁnes a kn-NN classiﬁer.
The asymptotic consistency of nearest neighbor classiﬁcation has been studied in detail  starting
with the work of Fix and Hodges [7]. The risk of the NN classiﬁer  henceforth denoted Rn  is a
random variable that depends on the data set (X1  Y1)  . . .   (Xn  Yn); the usual order of business is
to ﬁrst determine the limiting behavior of the expected value ERn and to then study stronger modes
of convergence of Rn. Cover and Hart [2] studied the asymptotics of ERn in general metric spaces 
under the assumption that every x in the support of µ is either a continuity point of η or has µ({x}) >
0. For the 1-NN classiﬁer  they found that ERn → EX [2η(X)(1 − η(X))] ≤ 2R∗(1 − R∗); for
kn-NN with kn ↑ ∞ and kn/n ↓ 0  they found ERn → R∗. For points in Euclidean space  a series
of results starting with Stone [15] established consistency without any distributional assumptions.
For kn-NN in particular  Rn → R∗ almost surely [5].
These consistency results place nearest neighbor methods in a favored category of nonparametric
estimators. But for a fuller understanding it is important to also have rates of convergence. For

1

instance  part of the beauty of nearest neighbor is that it appears to adapt automatically to different
distance scales in different regions of space. It would be helpful to have bounds that encapsulate this
property.
Rates of convergence are also important in extending nearest neighbor classiﬁcation to settings such
as active learning  semisupervised learning  and domain adaptation  in which the training data is not
a fully-labeled data set obtained by i.i.d. sampling from the future test distribution. For instance  in
active learning  the starting point is a set of unlabeled points X1  . . .   Xn  and the learner requests
the labels of just a few of these  chosen adaptively to be as informative as possible about η. There
are many natural schemes for deciding which points to label: for instance  one could repeatedly
pick the point furthest away from the labeled points so far  or one could pick the point whose k
nearest labeled neighbors have the largest disagreement among their labels. The asymptotics of such
selective sampling schemes have been considered in earlier work [4]  but ultimately the choice of
scheme must depend upon ﬁnite-sample behavior. The starting point for understanding this behavior
is to ﬁrst obtain a characterization in the non-active setting.

1.1 Previous work on rates of convergence

There is a large body of work on convergence rates of nearest neighbor estimators. Here we outline
some of the types of results that have been obtained  and give representative sources for each.
The earliest rates of convergence for nearest neighbor were distribution-free. Cover [3] studied the 1-
NN classiﬁer in the case X = R  under the assumption of class-conditional densities with uniformly-
bounded third derivatives. He showed that ERn converges at a rate of O(1/n2). Wagner [18]
and later Fritz [8] also looked at 1-NN  but in higher dimension X = Rd. The latter obtained an
asymptotic rate of convergence for Rn under the milder assumption of non-atomic µ and lower
semi-continuous class-conditional densities.
Distribution-free results are valuable  but do not characterize which properties of a distribution most
inﬂuence the performance of nearest neighbor classiﬁcation. More recent work has investigated
different approaches to obtaining distribution-dependent bounds  in terms of the smoothness of the
distribution.
A simple and popular smoothness parameter is the Holder constant. Kulkarni and Posner [12] ob-
tained a fairly general result of this kind for 1-NN and kn-NN. They assumed that for some constants
K and α  and for all x1  x2 ∈ X  

|η(x1) − η(x2)| ≤ Kρ(x1  x2)2α.

They then gave bounds in terms of the Holder parameter α as well as covering numbers for the
marginal distribution µ. Gyorﬁ [9] looked at the case X = Rd  under the weaker assumption that
for some function K : Rd → R and some α  and for all z ∈ Rd and all r > 0 

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)η(z) −

(cid:90)

1

µ(B(z  r))

B(z r)

η(x)µ(dx)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ K(z)rα.

The integral denotes the average η value in a ball of radius r centered at z; hence  this α is similar
in spirit to the earlier Holder parameter  but does not require η to be continuous. Gyorﬁ obtained
asymptotic rates in terms of α. Another generalization of standard smoothness conditions was pro-
posed recently [17] in a “probabilistic Lipschitz” assumption  and in this setting rates were obtained
for NN classiﬁcation in bounded spaces X ⊂ Rd.
The literature leaves open several basic questions that have motivated the present paper.
(1) Is
it possible to give tight ﬁnite-sample bounds for NN classiﬁcation in metric spaces  without any
smoothness assumptions? What aspects of the distribution must be captured in such bounds? (2)
Are there simple notions of smoothness that are especially well-suited to nearest neighbor? Roughly
speaking  we consider a notion suitable if it is possible to sharply characterize the convergence rate
of nearest neighbor for all distributions satisfying this notion. As we discuss further below  the
Holder constant is lacking in this regard.
(3) A recent trend in nonparametric classiﬁcation has
been to study rates of convergence under “margin conditions” such as that of Tsybakov. The best
achievable rates under these conditions are now known: does nearest neighbor achieve these rates?

2

Figure 1: One-dimensional distributions. In each case  the class-conditional densities are shown.

1.2 Some illustrative examples

We now look at a couple of examples to get a sense of what properties of a distribution most critically
affect the convergence rate of nearest neighbor. In each case  we study the k-NN classiﬁer.
To start with  consider a distribution over X = R in which the two classes (Y = 0  1) have class-
conditional densities µ0 and µ1. Assume that these two distributions have disjoint support  as on the
left side of Figure 1. The k-NN classiﬁer will make a mistake on a speciﬁc query x only if x is near
the boundary between the two classes. To be precise  consider an interval around x of probability
mass k/n  that is  an interval B = [x−r  x+r] with µ(B) = k/n. Then the k nearest neighbors will
lie roughly in this interval  and there will likely be an error only if the interval contains a substantial
portion of the wrong class. Whether or not η is smooth  or the µi are smooth  is irrelevant.
In a general metric space  the k nearest neighbors of any query point x are likely to lie in a ball
centered at x of probability mass roughly k/n. Thus the central objects in analyzing k-NN are balls
of mass ≈ k/n near the decision boundary  and it should be possible to give rates of convergence
solely in terms of these.
Now let’s turn to notions of smoothness. Figure 1  right  shows a variant of the previous example
in which it is no longer the case that η ∈ {0  1}. Although one of the class-conditional densities in
the ﬁgure is highly non-smooth  this erratic behavior occurs far from the decision boundary and thus
does not affect nearest neighbor performance. And in the vicinity of the boundary  what matters is
not how much η varies within intervals of any given radius r  but rather within intervals of probability
mass k/n. Smoothness notions such as Lipschitz and Holder constants  which measure changes in
η with respect to x  are therefore not entirely suitable: what we need to measure are changes in η
with respect to the underlying marginal µ on X .

1.3 Results of this paper
Let us return to our earlier setting of pairs (X  Y )  where X takes values in a metric space (X   ρ) and
has distribution µ  while Y ∈ {0  1} has conditional probability function η(x) = Pr(Y = 1|X =
x). We obtain rates of convergence for k-NN by attempting to make precise the intuitions discussed
above. This leads to a somewhat different style of analysis than has been used in earlier work.
Our main result is an upper bound on the misclassiﬁcation rate of k-NN that holds for any sample
size n and for any metric space  with no distributional assumptions. The bound depends on a novel
notion of the effective boundary for k-NN: for the moment  denote this set by An k ⊂ X .

• We show that with high probability over the training data  the misclassiﬁcation rate of the
k-NN classiﬁer (with respect to the Bayes-optimal classifer) is bounded above by µ(An k)
plus a small additional term that can be made arbitrarily small (Theorem 5).

• We lower-bound the misclassiﬁcation rate using a related notion of effective boundary

(Theorem 6).

• We identify a general condition under which  as n and k grow  An k approaches the ac-
tual decision boundary {x | η(x) = 1/2}. This yields universal consistency in a wider
range of metric spaces than just Rd (Theorem 1)  thus broadening our understanding of the
asymptotics of nearest neighbor.

3

Class 0Class 1Class 0Class 1We then specialize our generalization bounds to smooth distributions.

our upper and lower bounds under this kind of smoothness (Theorem 3).

• We introduce a novel smoothness condition that is tailored to nearest neighbor. We compare
• We obtain risk bounds under the margin condition of Tsybakov that match the best known
• We look at additional speciﬁc cases of interest: when η is bounded away from 1/2  and the

results for nonparametric classiﬁcation (Theorem 4).
even more extreme scenario where η ∈ {0  1} (zero Bayes risk).

2 Deﬁnitions and results
Let (X   ρ) be any separable metric space. For any x ∈ X   let

Bo(x  r) = {x(cid:48) ∈ X | ρ(x  x(cid:48)) < r} and B(x  r) = {x(cid:48) ∈ X | ρ(x  x(cid:48)) ≤ r}

denote the open and closed balls  respectively  of radius r centered at x.
Let µ be a Borel regular probability measure on this space (that is  open sets are measurable  and
every set is contained in a Borel set of the same measure) from which instances X are drawn. The
label of an instance X = x is Y ∈ {0  1} and is distributed according to the measurable conditional
probability function η : X → [0  1] as follows: Pr(Y = 1|X = x) = η(x).
Given a data set S = ((X1  Y1)  . . .   (Xn  Yn)) and a query point x ∈ X   we use the notation
X (i)(x) to denote the i-th nearest neighbor of x in the data set  and Y (i)(x) to denote its label.
Distances are calculated with respect to the given metric ρ  and ties are broken by preferring points
earlier in the sequence. The k-NN classiﬁer is deﬁned by

(cid:26) 1

0

gn k(x) =

if Y (1)(x) + ··· + Y (k)(x) ≥ k/2
otherwise

We analyze the performance of gn k by comparing it with g(x) = 1(η(x) ≥ 1/2)  the omniscient
Bayes-optimal classiﬁer. Speciﬁcally  we obtain bounds on PrX (gn k(X) (cid:54)= g(X)) that hold with
high probability over the choice of data S  for any n. It is worth noting that convergence results
for nearest neighbor have traditionally studied the excess risk Rn k − R∗  where Rn k = Pr(Y (cid:54)=
gn k(X)). If we deﬁne the pointwise quantities

Rn k(x) − R∗(x) = |1 − 2η(x)|1(gn k(x) (cid:54)= g(x)).

(1)
Taking expectation over X  we then have Rn k − R∗ ≤ PrX (gn k(X) (cid:54)= g(X))  and so we also
obtain upper bounds on the excess risk.
The technical core of this paper is the ﬁnite-sample generalization bound of Theorem 5. We begin 
however  by discussing some of its implications since these relate directly to common lines of inquiry
in the statistical literature. All proofs appear in the appendix.

2.1 Universal consistency
A series of results  starting with [15]  has shown that kn-NN is strongly consistent (Rn = Rn kn →
R∗ almost surely) when X is a ﬁnite-dimensional Euclidean space and µ is a Borel measure. A
consequence of the bounds we obtain in Theorem 5 is that this phenomenon holds quite a bit more
generally. In fact  strong consistency holds in any metric measure space (X   ρ  µ) for which the
Lebesgue differentiation theorem is true: that is  spaces in which  for any bounded measurable f 

1

lim
r↓0

µ(B(x  r))

B(x r)

f dµ = f (x)

(2)

for almost all (µ-a.e.) x ∈ X .
For more details on this differentiation property  see [6  2.9.8] and [10  1.13]. It holds  for instance:

(cid:90)

4

Rn k(x) = Pr(Y (cid:54)= gn k(x)|X = x)
R∗(x) = min(η(x)  1 − η(x)) 

for all x ∈ X   we see that

• When (X   ρ) is a ﬁnite-dimensional normed space [10  1.15(a)].
• When (X   ρ  µ) is doubling [10  1.8]  that is  when there exists a constant C(µ) such that
• When µ is an atomic measure on X .

µ(B(x  2r)) ≤ C(µ)µ(B(x  r)) for every ball B(x  r).

For the following theorem  recall that the risk of the kn-NN classiﬁer  Rn = Rn kn  is a function of
the data set (X1  Y1)  . . .   (Xn  Yn).
Theorem 1. Suppose metric measure space (X   ρ  µ) satisﬁes differentiation condition (2). Pick
a sequence of positive integers (kn)  and for each n  let Rn = Rn kn be the risk of the kn-NN
classiﬁer gn kn.

1. If kn → ∞ and kn/n → 0  then for all  > 0 

n→∞ Prn(Rn − R∗ > ) = 0.

lim

Here Prn denotes probability over the data set (X1  Y1)  . . .   (Xn  Yn).

2. If in addition kn/(log n) → ∞  then Rn → R∗ almost surely.

2.2 Smooth measures

Before stating our ﬁnite-sample bounds in full generality  we provide a glimpse of them under
smooth probability distributions. We begin with a few deﬁnitions.

The support of µ. The support of distribution µ is deﬁned as

supp(µ) = {x ∈ X | µ(B(x  r)) > 0 for all r > 0}.

It was shown by [2] that in separable metric spaces  µ(supp(µ)) = 1. For the interested reader  we
reproduce their brief proof in the appendix (Lemma 24).

The conditional probability function for a set. The conditional probability function η is deﬁned
for points x ∈ X   and can be extended to measurable sets A ⊂ X with µ(A) > 0 as follows:

(cid:90)

1

µ(A)

A

η(A) =

η dµ.

(3)

This is the probability that Y = 1 for a point X chosen at random from the distribution µ restricted
to set A. We exclusively consider sets A of the form B(x  r)  in which case η is deﬁned whenever
x ∈ supp(µ).

2.2.1 Smoothness with respect to the marginal distribution

For the purposes of nearest neighbor  it makes sense to deﬁne a notion of smoothness with respect
to the marginal distribution on instances. For α  L > 0  we say the conditional probability function
η is (α  L)-smooth in metric measure space (X   ρ  µ) if for all x ∈ supp(µ) and all r > 0 

|η(B(x  r)) − η(x)| ≤ L µ(Bo(x  r))α.

(As might be expected  we only need to apply this condition locally  so it is enough to restrict
attention to balls of probability mass upto some constant po.) One feature of this notion is that it is
scale-invariant: multiplying all distances by a ﬁxed amount leaves α and L unchanged. Likewise  if
the distribution has several well-separated clusters  smoothness is unaffected by the distance-scales
of the individual clusters.
It is common to analyze nonparametric classiﬁers under the assumption that X = Rd and that η is
αH-Holder continuous for some α > 0  that is 

|η(x) − η(x(cid:48))| ≤ L(cid:107)x − x(cid:48)(cid:107)αH

for some constant L. These bounds typically also require µ to have a density that is uniformly
bounded (above and/or below). We now relate these standard assumptions to our notion of smooth-
ness.

5

Lemma 2. Suppose that X ⊂ Rd  and η is αH-Holder continuous  and µ has a density with respect
to Lebesgue measure that is ≥ µmin on X . Then there is a constant L such that for any x ∈ supp(µ)
and r > 0 with B(x  r) ⊂ X   we have |η(x) − η(B(x  r))| ≤ Lµ(Bo(x  r))αH /d.
(To remove the requirement that B(x  r) ⊂ X   we would need the boundary of X to be well-
behaved  for instance by requiring that X contains a constant fraction of every ball centered in it.
This is a familiar assumption in nonparametric classiﬁcation  including the seminal work of [1] that
we discuss shortly.)
Our smoothness condition for nearest neighbor problems can thus be seen as a generalization of the
usual Holder conditions. It applies in broader range of settings  for example for discrete µ.

2.2.2 Generalization bounds for smooth measures

Under smoothness  our general ﬁnite-sample convergence rates (Theorems 5 and 6) take on an eas-
ily interpretable form. Recall that gn k(x) is the k-NN classiﬁer  while g(x) is the Bayes-optimal
prediction.
Theorem 3. Suppose η is (α  L)-smooth in (X   ρ  µ). The following hold for any n and k.
(Upper bound on misclassiﬁcation rate.) Pick any δ > 0 and suppose that k ≥ 16 ln(2/δ). Then

(cid:32)(cid:26)
(cid:18)(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)η(x) − 1
(cid:12)(cid:12)(cid:12)(cid:12) η(x) (cid:54)=

1
2

2

(cid:114) 1

(cid:12)(cid:12) ≤

ln

2
δ

+ L

k

(cid:19)α(cid:27)(cid:33)
(cid:18) k
(cid:19)α(cid:27)(cid:19)
(cid:18) 2k

2n

.

.

(gn k(X) (cid:54)= g(X)) ≤ δ + µ

x ∈ X

Pr
X

X

(Lower bound on misclassiﬁcation rate.) Conversely  there is an absolute constant co such that

En Pr

(gn k(X) (cid:54)= g(X)) ≥ coµ

x ∈ X

  |η(x) − 1
2

| ≤ 1√
k

− L

n

Here En is expectation over the data set.
The optimal choice of k is ∼ n2α/(2α+1)  and with this setting the upper and lower bounds are
directly comparable: they are both of the form µ({x : |η(x) − 1/2| ≤ ˜O(k−1/2)})  the probability
mass of a band of points around the decision boundary η = 1/2.
It is noteworthy that these upper and lower bounds have a pleasing resemblance for every distribution
in the smoothness class. This is in contrast to the usual minimax style of analysis  in which a bound
on an estimator’s risk is described as “optimal” for a class of distributions if there exists even a single
distribution in that class for which it is tight.

2.2.3 Margin bounds

An achievement of statistical theory in the past two decades has been margin bounds  which give
fast rates of convergence for many classiﬁers when the underlying data distribution P (given by µ
and η) satisﬁes a large margin condition stipulating  roughly  that η moves gracefully away from
1/2 near the decision boundary.
Following [13  16  1]  for any β ≥ 0  we say P satisﬁes the β-margin condition if there exists a
constant C > 0 such that

(cid:18)(cid:26)

µ

x

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)η(x) − 1

2

(cid:12)(cid:12) ≤ t

(cid:27)(cid:19)

≤ Ctβ.

Larger β implies a larger margin. We now obtain bounds for the misclassiﬁcation rate and the excess
risk of k-NN under smoothness and margin conditions.
Theorem 4. Suppose η is (α  L)-smooth in (X   ρ  µ) and satisﬁes the β-margin condition (with
constant C)  for some α  β  L  C ≥ 0. In each of the two following statements  ko and Co are
constants depending on α  β  L  C.

(a) For any 0 < δ < 1  set k = kon2α/(2α+1)(log(1/δ))1/(2α+1). With probability at least

1 − δ over the choice of training data 

(cid:18) log(1/δ)

(cid:19)αβ/(2α+1)

.

n

PrX (gn k(X) (cid:54)= g(X)) ≤ δ + Co

6

(b) Set k = kon2α/(2α+1). Then EnRn k − R∗ ≤ Con−α(β+1)/(2α+1).

It is instructive to compare these bounds with the best known rates for nonparametric classiﬁcation
under the margin assumption. The work of Audibert and Tsybakov [1] (Theorems 3.3 and 3.5) shows
that when (X   ρ) = (Rd (cid:107)·(cid:107))  and η is αH-Holder continuous  and µ lies in the range [µmin  µmax]
for some µmax > µmin > 0  and the β-margin condition holds (along with some other assumptions) 
an excess risk of n−αH (β+1)/(2αH +d) is achievable and is also the best possible. This is exactly the
rate we obtain for nearest neighbor classiﬁcation  once we translate between the different notions of
smoothness as per Lemma 2.
We discuss other interesting scenarios in Section C.4 in the appendix.

2.3 A general upper bound on the misclassiﬁcation error

We now get to our most general ﬁnite-sample bound. It requires no assumptions beyond the basic
measurability conditions stated at the beginning of Section 2  and it is the basis of the all the results
described so far. We begin with some key deﬁnitions.

The radius and probability-radius of a ball. When dealing with balls  we will primarily be
interested in their probability mass. To this end  for any x ∈ X and any 0 ≤ p ≤ 1  deﬁne

Thus µ(B(x  rp(x))) ≥ p (Lemma 23)  and rp(x) is the smallest radius for which this holds.

rp(x) = inf{r | µ(B(x  r)) ≥ p}.

The effective interiors of the two classes  and the effective boundary. When asked to make a
prediction at point x  the k-NN classiﬁer ﬁnds the k nearest neighbors  which can be expected to
lie in B(x  rp(x)) for p ≈ k/n. It then takes an average over these k labels  which has a standard
deviation of ∆ ≈ 1/
k. With this in mind  there is a natural deﬁnition for the effective interior of
the Y = 1 region: the points x with η(x) > 1/2 on which the k-NN classiﬁer is likely to be correct:

√

X +
p ∆ = {x ∈ supp(µ) | η(x) >

1
2

  η(B(x  r)) ≥ 1
2

+ ∆ for all r ≤ rp(x)}.

The corresponding deﬁnition for the Y = 0 region is

X −
p ∆ = {x ∈ supp(µ) | η(x) <

1
2
The remainder of X is the effective boundary 

  η(B(x  r)) ≤ 1
2

− ∆ for all r ≤ rp(x)}.

∂p ∆ = X \ (X +

p ∆ ∪ X −

p ∆).

Observe that ∂p(cid:48) ∆(cid:48) ⊂ ∂p ∆ whenever p(cid:48) ≤ p and ∆(cid:48) ≤ ∆. Under mild conditions  as p and ∆ tend
to zero  the effective boundary tends to the actual decision boundary {x | η(x) = 1/2} (Lemma 14) 
which we shall denote ∂o.
The misclassiﬁcation rate of the k-NN classiﬁer can be bounded by the probability mass of the
effective boundary:
Theorem 5. Pick any 0 < δ < 1 and positive integers k < n. Let gn k denote the k-NN classiﬁer
based on n training points  and g(x) the Bayes-optimal classiﬁer. With probability at least 1 − δ
over the choice of training data 

PrX (gn k(X) (cid:54)= g(X)) ≤ δ + µ(cid:0)∂p ∆
(cid:32)
1 −(cid:112)(4/k) ln(2/δ)

  and ∆ = min

1

(cid:1) 

(cid:114) 1

k

ln

2
δ

(cid:33)

.

1
2

 

where

p =

·

k
n

7

2.4 A general lower bound on the misclassiﬁcation error

n k  where

Finally  we give a counterpart to Theorem 5 that lower-bounds the expected probability of error
of gn k. For any positive integers k < n  we identify a region close to the decision boundary
in which a k-NN classiﬁer has a constant probability of making a mistake. This high-error set is
En k = E +
E +
n k =
E−
n k =

(cid:26)
n k ∪ E−
(cid:26)
x ∈ supp(µ) | η(x) >

for all rk/n(x) ≤ r ≤ r(k+
√
for all rk/n(x) ≤ r ≤ r(k+
√

  η(B(x  r)) ≤ 1
2
  η(B(x  r)) ≥ 1
2

x ∈ supp(µ) | η(x) <

+

1√
k
− 1√
k

1
2
1
2

(cid:27)
(cid:27)

k+1)/n(x)

k+1)/n(x)

.

(Recall the deﬁnition (3) of η(A) for sets A.) For smooth η this region turns out to be comparable
k. Meanwhile  here is a lower bound that applies to any
to the effective decision boundary ∂k/n 1/
(X   ρ  µ).
Theorem 6. For any positive integers k < n  let gn k denote the k-NN classiﬁer based on n training
points. There is an absolute constant co such that the expected misclassiﬁcation rate satisﬁes

√

EnPrX (gn k(X) (cid:54)= g(X)) ≥ co µ(En k) 

where En is expectation over the choice of training set.

Acknowledgements

The authors are grateful to the National Science Foundation for support under grant IIS-1162581.

8

References
[1] J.-Y. Audibert and A.B. Tsybakov. Fast learning rates for plug-in classiﬁers. Annals of Statis-

tics  35(2):608–633  2007.

[2] T. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Infor-

mation Theory  13:21–27  1967.

[3] T.M. Cover. Rates of convergence for nearest neighbor procedures. In Proceedings of The

Hawaii International Conference on System Sciences  1968.

[4] S. Dasgupta. Consistency of nearest neighbor classiﬁcation under selective sampling.

Twenty-Fifth Conference on Learning Theory  2012.

In

[5] L. Devroye  L. Gyorﬁ  A. Krzyzak  and G. Lugosi. On the strong universal consistency of

nearest neighbor regression function estimates. Annals of Statistics  22:1371–1385  1994.

[6] H. Federer. Geometric Measure Theory. Springer  1969.
[7] E. Fix and J. Hodges. Discriminatory analysis  nonparametric discrimination. USAF School of
Aviation Medicine  Randolph Field  Texas  Project 21-49-004  Report 4  Contract AD41(128)-
31  1951.

[8] J. Fritz. Distribution-free exponential error bound for nearest neighbor pattern classiﬁcation.

IEEE Transactions on Information Theory  21(5):552–557  1975.

[9] L. Gyorﬁ. The rate of convergence of kn-nn regression estimates and classiﬁcation rules. IEEE

Transactions on Information Theory  27(3):362–364  1981.

[10] J. Heinonen. Lectures on Analysis on Metric Spaces. Springer  2001.
[11] R. Kaas and J.M. Buhrman. Mean  median and mode in binomial distributions. Statistica

Neerlandica  34(1):13–18  1980.

[12] S. Kulkarni and S. Posner. Rates of convergence of nearest neighbor estimation under arbitrary

sampling. IEEE Transactions on Information Theory  41(4):1028–1039  1995.

[13] E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics 

27(6):1808–1829  1999.

[14] E. Slud. Distribution inequalities for the binomial law. Annals of Probability  5:404–412 

1977.

[15] C. Stone. Consistent nonparametric regression. Annals of Statistics  5:595–645  1977.
[16] A.B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statis-

tics  32(1):135–166  2004.

[17] R. Urner  S. Ben-David  and S. Shalev-Shwartz. Access to unlabeled data can speed up pre-

diction time. In International Conference on Machine Learning  2011.

[18] T.J. Wagner. Convergence of the nearest neighbor rule. IEEE Transactions on Information

Theory  17(5):566–571  1971.

9

,Kamalika Chaudhuri
Sanjoy Dasgupta