2011,Learning to Learn with Compound HD Models,We introduce HD (or ``Hierarchical-Deep'') models  a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM).  This compound HDP-DBM model learns to learn novel concepts from very few training examples  by learning low-level generic features  high-level features that capture correlations among low-level features  and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts.  We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition  handwritten character recognition  and human motion capture datasets.,Learning to Learn with Compound HD Models

Ruslan Salakhutdinov

Department of Statistics  University of Toronto

rsalakhu@utstat.toronto.edu

Joshua B. Tenenbaum

Brain and Cognitive Sciences  MIT

jbt@mit.edu

Antonio Torralba

CSAIL  MIT

torralba@mit.edu

Abstract

We introduce HD (or “Hierarchical-Deep”) models  a new compositional learn-
ing architecture that integrates deep learning models with structured hierarchical
Bayesian models. Speciﬁcally we show how we can learn a hierarchical Dirichlet
process (HDP) prior over the activities of the top-level features in a Deep Boltz-
mann Machine (DBM). This compound HDP-DBM model learns to learn novel
concepts from very few training examples  by learning low-level generic features 
high-level features that capture correlations among low-level features  and a cat-
egory hierarchy for sharing priors over the high-level features that are typical of
different kinds of concepts. We present efﬁcient learning and inference algorithms
for the HDP-DBM model and show that it is able to learn new concepts from very
few examples on CIFAR-100 object recognition  handwritten character recogni-
tion  and human motion capture datasets.

Introduction

1
“Learning to learn”  or the ability to learn abstract representations that support transfer to novel
but related tasks  lies at the core of many problems in computer vision  natural language processing 
cognitive science  and machine learning. In typical applications of machine classiﬁcation algorithms
today  learning curves are measured in tens  hundreds or thousands of training examples. For humans
learners  however  just one or a few examples are often sufﬁcient to grasp a new category and make
meaningful generalizations to novel instances [25  16]. The architecture we describe here takes a
step towards this “one-shot learning” ability by learning several forms of abstract knowledge that
support transfer of useful representations from previously learned concepts to novel ones.
We call our architectures compound HD models  where “HD” stands for “Hierarchical-Deep”  be-
cause they are derived by composing hierarchical nonparametric Bayesian models with deep net-
works  two inﬂuential approaches from the recent unsupervised learning literature with comple-
mentary strengths. Recently introduced deep learning models  including Deep Belief Networks [5] 
Deep Boltzmann Machines [14]  deep autoencoders [10]  and others [12  11]  have been shown to
learn useful distributed feature representations for many high-dimensional datasets. The ability to
automatically learn in multiple layers allows deep models to construct sophisticated domain-speciﬁc
features without the need to rely on precise human-crafted input representations  increasingly im-
portant with the proliferation of data sets and application domains.
While the features learned by deep models can enable more rapid and accurate classiﬁcation learn-
ing  deep networks themselves are not well suited to one-shot learning of novel classes. All units
and parameters at all levels of the network are engaged in representing any given input and are ad-
justed together during learning. In contrast  we argue that one-shot learning of new classes will be
easier in architectures that can explicitly identify only a small number of degrees of freedom (latent
variables and parameters) that are relevant to the new concept being learned  and thereby achieve
more appropriate and ﬂexible transfer of learned representations to new tasks. This ability is the

1

hallmark of hierarchical Bayesian (HB) models  recently proposed in computer vision  statistics 
and cognitive science [7  25  4  13] for learning to learn from few examples. Unlike deep networks 
these HB models explicitly represent category hierarchies that admit sharing the appropriate ab-
stract knowledge about the new class’s parameters via a prior abstracted from related classes. HB
approaches  however  have complementary weaknesses relative to deep networks. They typically
rely on domain-speciﬁc hand-crafted features [4  1] (e.g. GIST  SIFT features in computer vision 
MFCC features in speech perception domains). Committing to the a-priori deﬁned feature repre-
sentations  instead of learning them from data  can be detrimental. Moreover  many HB approaches
often assume a ﬁxed hierarchy for sharing parameters [17  3] instead of learning the hierarchy in an
unsupervised fashion.
In this work we investigate compound HD (hierarchical-deep) architectures that integrate these deep
models with structured hierarchical Bayesian models. In particular  we show how we can learn a hi-
erarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltz-
mann Machine (DBM)  coming to represent both a layered hierarchy of increasingly abstract fea-
tures  and a tree-structured hierarchy of classes. Our model depends minimally on domain-speciﬁc
representations and achieves state-of-the-art one-shot learning performance by unsupervised discov-
ery of three components: (a) low-level features that abstract from the raw high-dimensional sensory
input (e.g. pixels  or 3D joint angles); (b) high-level part-like features that express the distinctive
perceptual structure of a speciﬁc class  in terms of class-speciﬁc correlations over low-level fea-
tures; and (c) a hierarchy of super-classes for sharing abstract knowledge among related classes. We
evaluate the compound HDP-DBM model on three different perceptual domains. We also illustrate
the advantages of having a full generative model  extending from highly abstract concepts all the
way down to sensory inputs: we can not only generalize class labels but also synthesize new exam-
ples in novel classes that look reasonably natural  and we can signiﬁcantly improve classiﬁcation
performance by learning parameters at all levels jointly by maximizing a joint log-probability score.

(cid:88)

h

(cid:18)(cid:88)

ij

(cid:88)

jl

(cid:19)

(cid:88)

lm

2 Deep Boltzmann Machines (DBMs)
A Deep Boltzmann Machine is a network of symmetrically coupled stochastic binary units. It con-
tains a set of visible units v ∈ {0  1}D  and a sequence of layers of hidden units h1 ∈ {0  1}F1 
h2 ∈ {0  1}F2 ...  hL ∈ {0  1}FL. There are connections only between hidden units in adjacent
layers  as well as between visible and hidden units in the ﬁrst hidden layer. Consider a DBM with
three hidden layers1 (i.e. L = 3). The probability of a visible input v is:

P (v; ψ) =

1

Z(ψ)

exp

W(1)

ij vih1

j +

W(2)

jl h1

j h2

l +

W(2)

lm h2

l h3
m

 

(1)

(cid:81)F3

(cid:81)F2

units: Q(h|v; µ) = (cid:81)F1

where h = {h1  h2  h3} are the set of hidden units  and ψ = {W(1)  W(2)  W(3)} are the model
parameters  representing visible-to-hidden and hidden-to-hidden symmetric interaction terms.
Approximate Learning: Exact maximum likelihood learning in this model is intractable  but efﬁ-
cient approximate learning of DBMs can be carried out by using a mean-ﬁeld inference to estimate
data-dependent expectations  and an MCMC based stochastic approximation procedure to approx-
imate the model’s expected sufﬁcient statistics [14]. In particular  consider approximating the true
posterior P (h|v; ψ) with a fully factorized approximating distribution over the three sets of hidden
j|v)q(h2
m|v) where µ = {µ1  µ2  µ3} are the
k=1
i for l = 1  2  3. In this case  we can write down the
mean-ﬁeld parameters with q(hl
variational lower bound on the log-probability of the data  which takes a particularly simple form:
W(2)µ2 + µ2(cid:62)

m=1 q(h1
i = 1) = µl
log P (v; ψ) ≥ v(cid:62)W(1)µ1 + µ1(cid:62)

(2)
where H(·) is the entropy functional. Learning proceeds by ﬁnding the value of µ that maximizes
this lower bound for the current value of model parameters ψ  which results in a set of the mean-ﬁeld
ﬁxed-point equations. Given the variational parameters µ  the model parameters ψ are then updated
to maximize the variational bound using stochastic approximation (for details see [14  22  26]).
Multinomial DBMs: To allow DBMs to express more information and introduce more structured
hierarchical priors  we will use a conditional multinomial distribution to model activities of the top-
level units. Speciﬁcally  we will use M softmax units  each with “1-of-K” encoding (so that each

W(3)µ2 − log Z(ψ) + H(Q) 

k|v)q(h3

j=1

1For clarity  we use three hidden layers. Extensions to models with more than three layers is trivial.

2

Deep Boltzmann Machine

M replicated softmax units

with tied weights

Multinomial unit
sampled M times

HDP prior
over activities of
the top-level units

“Animal”

Learned Hierarchy
of super-classes

“Vehicle”

Figure 1: Left: Multinomial DBM model: the top layer represents M softmax hidden units h3  which share the
same set of weights. Middle: A different interpretation: M softmax units are replaced by a single multinomial
unit which is sampled M times. Right: Hierarchical Dirichlet Process prior over the states of h3.
unit contains a set of K weights). All M separate softmax units will share the same set of weights 
connecting them to binary hidden units at the lower-level (Fig. 1). A key observation is that M
separate copies of softmax units that all share the same set of weights can be viewed as a single
multinomial unit that is samples M times [15  19]. A pleasing property of using softmax units is that
the mathematics underlying the learning algorithm for binary-binary DBMs remains the same.

3 Compound HDP-DBM model
After a DBM model has been learned  we have an undirected model that deﬁnes the joint dis-
tribution P (v  h1  h2  h3). One way to express what has been learned is the conditional model
P (v  h1  h2|h3) and a prior term P (h3). We can therefore rewrite the variational bound as:

Q(h|v; µ) log P (v  h1  h2|h3) + H(Q) +

Q(h3|v; µ) log P (h3). (3)

h1 h2 h3

h3

log P (v) ≥ (cid:88)

(cid:88)

This particular decomposition lies at the core of the greedy recursive pretraining algorithm: we keep
the learned conditional model P (v  h1  h2|h3)  but maximize the variational lower-bound of Eq. 3
with respect to the last term [5]. Instead of adding an additional undirected layer  (e.g. a restricted
Boltzmann machine)  to model P (h3)  we can place a hierarchical Dirichlet process prior over
h3  that will allow us to learn category hierarchies  and more importantly  useful representations
of classes that contain few training examples. The part we keep  P (v  h1  h2|h3)  represents a
conditional DBM model  which can be viewed as a two-layer DBM but with bias terms given by the
states of h3:
P (v  h1  h2|h3) =

(cid:18)(cid:88)

(cid:88)

(cid:88)

(cid:19)

exp

(4)

1

.

W(1)

ij vih1

j +

W(2)

jl h1

j h2

l +

W(3)

lm h2

l h3
m

Z(ψ  h3)

ij

jl

lm

3.1 A Hierarchical Bayesian Topic Prior
In a typical hierarchical topic model  we observe a set of N documents  each of which is modeled
as a mixture over topics  that are shared among documents. Let there be K words in the vocabulary.
A topic t is a discrete distribution over K words with probability vector φt. Each document n has
its own distribution over topics given by probabilities θn.
In our compound HDP-DBM model  we will use a hierarchical topic model as a prior over the
activities of the DBM’s top-level features. Speciﬁcally  the term “document” will refer to the top-
level multinomial unit h3  and M “words” in the document will represent the M samples  or active
DBM’s top-level features  generated by this multinomial unit. Words in each document are drawn
by choosing a topic t with probability θnt  and then choosing a word w with probability φtw. We
will often refer to topics as our learned higher-level features  each of which deﬁnes a topic speciﬁc
distribution over DBM’s h3 features. Let h3

in be the ith word in document n  and xin be its topic:

θn|π ∼ Dir(απ)  φt|τ ∼ Dir(βτ )  xin|θn ∼ Mult(θn)  h3

(5)
where π is the global distribution over topics  τ is the global distribution over K words  and α and
β are concentration parameters.

in|xin  φxin ∼ Mult(φxin) 

3

h3h2h1vW2W1W3h3h2h1vW2W1W3α(2)α(3)γHG(1)cG(1)cG(1)cG(1)cG(1)cG(2)kG(2)kG(3)α(3)GnGnGnGnGnφinφinφinφinφinh3inh3inh3inh3inh3incNcowhorsecarvantruckMMLet us further assume that we are presented with a ﬁxed two-level category hierarchy. Suppose that
N documents  or objects  are partitioned into C basic level categories (e.g. cow  sheep  car). We
n ∈ {1  ...  C}. We
represent such partition by a vector zb of length N  each entry of which is zb
also assume that our C basic-level categories are partitioned into S super-categories (e.g. animal 
c ∈ {1  ...  S}. These partitions deﬁne a
vehicle)  represented by by a vector zs of length C  with zs
ﬁxed two-level tree hierarchy (see Fig. 1). We will relax this assumption later.
The hierarchical topic model can be readily extended to modeling the above hierarchy. For each
document n that belong to the basic category c  we place a common Dirichlet prior over θn with
parameters π(1)
. The Dirichlet parameters π(1) are themselves drawn from a Dirichlet prior with
parameters π(2)  and so on (see Fig. 1). Speciﬁcally  we deﬁne the following prior over h3:

c

g

zs
c

∼ Dir(α(3)π3
g) 
∼ Dir(α(2)π(2)
∼ Dir(α(1)π(1)

s |π(3)
π(2)
|π(2)
π(1)
c
θn|π(1)
xin|θn ∼ Mult(θn) 
φt|τ ∼ Dir(βτ ) 

zb
n

zb
n

zs
c

for each super-category s=1 .. S

(6)

for each basic-category c = 1  ..  C

) 

) 

for each document n = 1  ..  N
for each word i = 1  ..  M
in|xin  φxin ∼ Mult(φxin) 
h3

g

is the global distribution over topics  π(2)

where π(3)
is the
class speciﬁc distribution over topics  or higher-level features. These high-level features  in turn 
deﬁne topic-speciﬁc distribution over h3 features  or “words” in a DBM model.
For a ﬁxed number of topics T   the above model represents a hierarchical extension of LDA. We
typically do not know the number of topics a-priori. It is therefore natural to consider a nonparamet-
ric extension based on the HDP model [21]  which allows for a countably inﬁnite number of topics.
In the standard hierarchical Dirichlet process notation  we have

is the super-category speciﬁc and π(1)

s

c

g ∼ DP(γ  Dir(βτ ))  G(2)
G(3)

s ∼ DP(α(3)  G(3)

Gn ∼ DP(α(1)  G(1)

)  φ∗

zb
n

g )  G(1)
in|Gn ∼ Gn  h3

c ∼ DP(α(2)  G(2)
in|φ∗
in ∼ Mult(φ∗
in) 

zs
c

) 

(7)

c (φ) =(cid:80)∞

g (φ) = (cid:80)∞

ct δφt  and Gn(φ) =(cid:80)∞

in. Making use of topic index variables xin  we denote φ∗
t=1 π(3)

where Dir(βτ ) is the base-distribution  and each φ∗ is a factor associated with a single observa-
tion h3
in = φxin (see Eq. 6). Using a
stick-breaking representation we can write: G(3)
gt δφt  G(2)
st δφt 
G(3)
t=1 θntδφt that represent sums of point masses. We also
place Gamma priors over concentration parameters as in [21].
The overall generative model is shown in Fig. 1. To generate a sample we ﬁrst draw M words  or
activations of the top-level features  from the HDP prior over h3 given by Eq. 7. Conditioned on h3 
we sample the states of v from the conditional DBM model given by Eq. 4.

s (φ) = (cid:80)∞

t=1 π(2)

t=1 π(1)

3.2 Modeling the number of super-categories
So far we have assumed that our model is presented with a two-level partition z = {zs  zb}. If 
however  we are not given any level-1 or level-2 category labels  we need to infer the distribution
over the possible category structures. We place a nonparametric two-level nested Chinese Restaurant
Prior (CRP) [2] over z  which deﬁnes a prior over tree structures and is ﬂexible enough to learn
arbitrary hierarchies. The main building block of the nested CRP is the Chinese restaurant process 
a distribution on partition of integers. Imagine a process by which customers enter a restaurant with
an unbounded number of tables  where the nth customer occupies a table k drawn from:
  if k is new} 

P (zn = k|z1  ...  zn−1) = {

  if nk > 0;

(8)

nk

η

n − 1 + η

n − 1 + η

where nk is the number of previous customers at table k and η is the concentration parameter. The
nested CRP  nCRP(η)  extends CRP to nested sequence of partitions  one for each level of the tree.
In this case each observation n is ﬁrst assigned to the super-category zs
n using Eq. 8. Its assignment
to the basic-level category zb
n  that is placed under a super-category zs
n  is again recursively drawn
from Eq. 8. We also place a Gamma prior Γ(1  1) over η. The proposed model allows for both: a
nonparametric prior over potentially unbounded number of global topics  or higher-level features 
as well as a nonparametric prior that allow learning an arbitrary tree taxonomy.

4

Inference

4
Inferences about model parameters at all levels of hierarchy can be performed by MCMC. When the
tree structure z of the model is not given  the inference process will alternate between ﬁxing z while
sampling the space of model parameters  and vice versa.
Sampling HDP parameters: Given category assignment vectors z  and the states of the top-level
DBM features h3  we use posterior representation sampler of [20]. In particular  the HDP sampler
maintains the stick-breaking weights {θ}N
g }; and topic indicator variables x
(parameters φ can be integrated out). The sampler alternatives between: (a) sampling cluster indices
xin using Gibbs updates in the Chinese restaurant franchise (CRF) representation of the HDP; (b)
sampling the weights at all three levels conditioned on x using the usual posterior of a DP2.
Sampling category assignments z: Given current instantiation of the stick-breaking weights  using
a deﬁning property of a DP  for each input n  we have:

n=1  and {π(1)

s   π(3)

  π(2)

c

(θ1 n  ...  θT n  θnew n) ∼ Dir(α(1)π(1)

zn 1  ...  α(1)π(1)

zn T   α(1)π(1)

zn new)

(9)

Combining the above likelihood term with the CRP prior (Eq. 8)  the posterior over the category
assignment can be calculated as follows:

p(zn|θn  z−n  π(1)) ∝ p(θn|π(1)  zn)p(zn|z−n) 

(10)

where z−n denotes variables z for all observations other than n. When computing the probability of
placing θn under a newly created category  its parameters are sampled from the prior.
Sampling DBM’s hidden units: Given the states of the DBM’s top-level multinomial unit h3  con-
n|h3
n  vn) can be obtained by running a Gibbs sampler that alternates
ditional samples from P (h1
n  and vice versa. Conditioned on topic
n independently given h2
between sampling the states of h1
n for each input n are sampled using
assignments xin and h2
Gibbs conditionals:
in|xin) 

n  the states of the multinomial unit h3

n  h3−in  xn) ∝ P (h2

n)P (h3

in|h2

n|h3

n  h2

P (h3

(11)

where the ﬁrst term is given by the product of logistic functions (see Eq. 4):

P (h2|h3) =

P (h2

l |h3)  with P (h2

l = 1|h3) =

(cid:89)

l

1 + exp(cid:0) −(cid:80)

1

m W(3)

lm h3
m

(cid:1)  

(12)

in) is given by the multinomial: Mult(φxin ) (see Eq. 7  in our conjugate

and the second term P (h3
setting  parameters φ can be further integrated out).
Fine-tuning DBM: More importantly  conditioned on h3  we can further ﬁne-tune low-level DBM
parameters ψ = {W(1)  W(2)  W(3)} by applying approximate maximum likelihood learning (see
section 2) to the conditional DBM model of Eq. 4. For the stochastic approximation algorithm  as
the partition function depends on the states of h3  we maintain one “persistent” Markov chain per
data point (for details see [22  14]).
Making predictions: Given a test input vt  we can quickly infer the approximate posterior over h3
t
using the mean-ﬁeld of Eq. 2  followed by running the full Gibbs sampler to get approximate samples
from the posterior over the category assignments. In practice  for faster inference  we ﬁx learned
t belongs to category zt by assuming that
topics φt and approximate the marginal likelihood that h3
document speciﬁc DP can be well approximated by the class-speciﬁc DP3 Gt ≈ G(1)
zt (see Fig. 1):
(13)
) 

t|φ  Gt)P (Gt|G(1)

t|zt  G(1)  φ) =

)dGt ≈ P (h3

t|φ  G(1)

P (h3

P (h3

(cid:90)

zt

zt

Gt

Combining this likelihood term with nCRP prior P (zt|z−t) (Eq. 8) allows us to efﬁciently infer
approximate posterior over category assignments4.

2Conditioned on the draw of the super-class DP G(2)

s

and the state of the CRF  the posteriors over G(1)

c

become independent. We can easily speed up inference by sampling from these conditionals in parallel.

3We note that G(1)
4In all of our experimental results  computing this approximate posterior takes a fraction of a second.

zt = E[Gt|G(1)
zt ]

5

DBM features

1st layer

2nd layer

HDP high-level features

1.

2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12
13
14
15
16
17
18
20
22

bed  chair  clock  couch  dinosaur  lawn mower  table 
telephone  television  wardrobe
bus  house  pickup truck  streetcar  tank  tractor  train
crocodile  kangaroo  lizard  snake  spider  squirrel
hamster  mouse  rabbit  raccoon  possum  bear
apple  orange  pear  sunﬂower  sweet pepper
baby  boy  girl  man  woman
dolphin  ray  shark  turtle  whale
otter  porcupine  shrew  skunk
beaver  camel  cattle  chimpanzee  elephant
fox  leopard  lion  tiger  wolf
maple tree  oak tree  pine tree  willow tree
ﬂatﬁsh  seal  trout  worm
butterﬂy  caterpillar  snail
bee  crab  lobster
bridge  castle  road  skyscraper
bicycle  keyboard  motorcycle  orchid  palm tree
bottle  bowl  can  cup  lamp
cloud  plate  rocket 19. mountain  plain  sea
poppy  rose  tulip
beetle  cockroach

21. aquarium ﬁsh  mushroom
23. forest

Figure 2: A random subset of the 1st  2nd layer DBM features 
and higher-level class-sensitive HDP features/topics.

Figure 3: A typical partition of the 100
basic-level categories

5 Experiments
We present experimental results on the CIFAR-100 [8]  handwritten character [9]  and human motion
capture recognition datasets. For all datasets  we ﬁrst pretrain a DBM model in unsupervised fashion
on raw sensory input (e.g. pixels  or 3D joint angles)  followed by ﬁtting an HDP prior  which is run
for 200 Gibbs sweeps. We further run 200 additional Gibbs steps in order to ﬁne-tune parameters of
the entire compound HDP-DBM model. This was sufﬁcient to reach convergence and obtain good
performance. Across all datasets  we also assume that the basic-level category labels are given 
but no super-category labels are available. The training set includes many examples of familiar
categories but only a few examples of a novel class. Our goal is to generalize well on a novel class.
In all experiments we compare performance of HDP-DBM to the following alternative models:
stand-alone Deep Boltzmann Machines  Deep Belief Networks [5]  “Flat HDP-DBM” model  that
always uses a single super-category  SVMs  and k-NN. The Flat HDP-DBM approach could po-
tentially identify a set of useful high-level features common to all categories. Finally  to evaluate
performance of DBMs (and DBNs)  we follow [14]. Note that using HDPs on top of raw sensory in-
put (i.e. pixels  or even image-speciﬁc GIST features) performs far worse compared to HDP-DBM.

5.1 CIFAR-100 dataset
The CIFAR-100 image dataset [8] contains 50 000 training and 10 000 test images of 100 object
categories (100 per class)  with 32 × 32 × 3 RGB pixels. Extreme variability in scale  viewpoint 
illumination  and cluttered background makes object recognition task for this dataset quite difﬁcult.
Similar to [8]  in order to learn good generic low-level features  we ﬁrst train a two-layer DBM in
completely unsupervised fashion using 4 million tiny images5 [23]. We use a conditional Gaussian
distribution to model observed pixel values [8  6]. The ﬁrst DBM layer contained 10 000 binary
hidden units  and the second layer contained M=1000 softmax units  each deﬁning a distribution
over 10  000 second layer features6. We then ﬁt an HDP prior over h2 to the 100 object classes.
Fig. 2 displays a random subset of the 1st and 2nd layer DBM features  as well as higher-level class-
sensitive features  or topics  learned by the HDP model. To visualize a particular higher-level feature 
we ﬁrst sample M words from a ﬁxed topic φt  followed by sampling RGB pixel values from the
conditional DBM model. While DBM features capture mostly low-level structure  including edges
and corners  the HDP features tend to capture higher-level structure  including contours  shapes 
color components  and surface boundaries. More importantly  features at all levels of the hierarchy
evolve without incorporating any image-speciﬁc priors. Fig. 3 shows a typical partition over 100
classes that our model learns with many super-categories containing semantically similar classes.
We next illustrate the ability of the HDP-DBM to generalize from a single training example of a
“pear” class. We trained the model on 99 classes containing 500 training images each  but only one
training example of a “pear” class. Fig. 4 shows the kind of transfer our model is performing: the
model discovers that pears are like apples and oranges  and not like other classes of images  such as
dolphins  that reside in very different parts of the hierarchy. Hence the novel category can inherit

5The dataset contains random images of natural scenes downloaded from the web
6We also experimented with a 3-layer DBM model  as well as various softmax parameters: M = 500 and

M = 2000. The difference in performance was not signiﬁcant.

6

Shared HDP high-level features

Shape

Color

Learning with 3 examples

CIFAR Dataset

Characters Dataset

Figure 4: Left: Training examples along with eight most probable topics φt  ordered by hand. Right: Perfor-
mance of HDP-DBM  DBM  and SVMs for all object classes when learning with 3 examples. Object categories
are sorted by their performance.

CIFAR Dataset
Number of examples

Handwritten Characters

Number of examples

Motion Capture
Number of examples

Model
1
Tuned HDP-DBM 0.36
0.34
HDP-DBM
0.27
Flat HDP-DBM
0.26
DBM
0.25
DBN
SVM
0.18
0.17
1-NN
GIST
0.27

3
0.41
0.39
0.37
0.36
0.33
0.27
0.18
0.31

5
0.46
0.45
0.42
0.41
0.37
0.31
0.19
0.33

10
0.53
0.52
0.50
0.48
0.45
0.38
0.20
0.39

50
0.62
0.61
0.61
0.61
0.60
0.61
0.32
0.58

1
0.67
0.65
0.58
0.57
0.51
0.41
0.43

-

3
0.78
0.76
0.73
0.72
0.72
0.66
0.65

-

5
0.87
0.85
0.82
0.81
0.81
0.77
0.73

-

10
0.93
0.92
0.89
0.89
0.89
0.86
0.81

-

1
0.67
0.66
0.63
0.61
0.61
0.54
0.58

-

3
0.84
0.82
0.79
0.79
0.79
0.78
0.75

-

5
0.90
0.88
0.86
0.85
0.84
0.84
0.81

-

10
0.93
0.93
0.91
0.91
0.92
0.91
0.88

-

50
0.96
0.96
0.96
0.95
0.96
0.96
0.93

Table 1: Classiﬁcation performance on the test set using 2*AUROC-1. The results in bold correspond to ROCs
that are statistically indistinguishable from the best (the difference is not statistically signiﬁcant).

the prior distribution over similar high-level shape and color features  allowing the HDP-DBM to
generalize considerably better to new instances of the “pear” class.
Table 1 quantiﬁes performance using the area under the ROC curve (AUROC) for classifying 10 000
test images as belonging to the novel vs. all other 99 classes (we report 2*AUROC-1  so zero cor-
responds to the classiﬁer that makes random predictions). The results are averaged over 100 classes
using “leave-one-out” test format. Based on a single example  the HDP-DBM model achieves an
AUROC of 0.36  signiﬁcantly outperforming DBMs  DBNs  SVMs  as well as 1-NN using standard
image-speciﬁc GIST features [24] that achieve an AUROC of 0.26  0.25  0.18 and 0.27 respectively.
Table 1 also shows that ﬁne-tuning parameters of all layers jointly as well as learning super-category
hierarchy signiﬁcantly improves model performance. As the number of training examples increases 
the HDP-DBM model still consistently outperforms alternative methods. Fig. 4 further displays per-
formance of HDP-DBM  DBM  and SVM models for all object categories when learning with only
three examples. Observe that over 40 classes beneﬁt in various degrees from learning a hierarchy.

5.2 Handwritten Characters
The handwritten characters dataset [9] can be viewed as the “transpose” of MNIST. Instead of con-
taining 60 000 images of 10 digit classes  the dataset contains 30 000 images of 1500 characters (20
examples each) with 28 × 28 pixels. These characters are from 50 alphabets from around the world 
including Bengali  Cyrillic  Arabic  Sanskrit  Tagalog (see Fig. 5). We split the dataset into 15 000
training and 15 000 test images (10 examples of each class). Similar to the CIFAR dataset  we pre-
train a two-layer DBM model  with the ﬁrst layer containing 1000 hidden units  and the second layer
containing M=100 softmax units  each deﬁning a distribution over 1000 second layer features.
Fig. 2 displays a random subset of training images  along with the 1st and 2nd layer DBM features 
as well as higher-level class-sensitive HDP features. The HDP features tend to capture higher-level
parts  many of which resemble pen “strokes”. Table 1 further shows results for classifying 15 000
test images as belonging to the novel vs. all other 1 499 character classes. The HDP-DBM model
signiﬁcantly outperforms other methods  particularly when learning characters with few training
examples. Fig. 6 further displays learned super-classes along with examples of entirely novel char-
acters that have been generated by the model for the same super-class  as well as conditional samples

7

020040060080010001200140000.10.20.30.40.50.60.70.80.91Sorted Class Index2*AUROC−1  HDP−DBMDBMSVM010203040506070809010000.10.20.30.40.50.60.70.80.9Sorted Class Index2*AUROC−1  HDP−DBMDBMSVMTraining samples

DBM features

1st layer

2nd layer

HDP high-level features

Figure 5: A random subset of the training images along with 1st and 2nd layer DBM features  as well as
higher-level class-sensitive HDP features/topics.

Learning with 3 examples

Learned Super-Classes (by row) Sampled Novel Characters

Training Examples

Conditional Samples

Figure 6: Left: Learned super-classes along with examples of novel characters  generated by the model for
the same super-class. Right: Three training examples along with 8 conditional samples.
when learning only with three training examples. (we note that using Deep Belief Networks instead
of DBMs produced far inferior generative samples). Remarkably  many samples look realistic  con-
taining coherent  long-range structure  while at the same time being different from existing training
images (see Supplementary Materials for a much richer class of generated samples).

5.3 Motion capture
We next applied our model to human motion capture data consisting of sequences of 3D joint angles
plus body orientation and translation [18]. The dataset contains 10 walking styles  including normal 
drunk  graceful  gangly  sexy  dinosaur  chicken  old person  cat  and strong. There are 2500 frames
of each style at 60fps  where each time step was represented by a vector of 58 real-valued numbers.
The dataset was split at random into 1500 training and 1000 test frames of each style. We further
preprocessed the data by treating each window of 10 consecutive frames as a single 58 ∗ 10 = 580-
d data vector. For the two-layer DBM model  the ﬁrst layer contained 500 hidden units  with the
second layer containing M=50 softmax units  each deﬁning a distribution over 500 second layer
features. As expected  Table 1 shows that the HDP-DBM model performs much better compared
to other models when discriminating between existing nine walking styles vs. novel walking style.
The difference is particularly large in the regime when we observe only a handful number of training
examples of a novel walking style.

6 Conclusions
We developed a compositional architecture that learns an HDP prior over the activities of top-level
features of the DBM model. The resulting compound HDP-DBM model is able to learn low-level
features from raw sensory input  high-level features  as well as a category hierarchy for parameter
sharing. Our experimental results show that the proposed model can acquire new concepts from
very few examples in a diverse set of application domains. The compositional model considered in
this paper was directly inspired by the architecture of the DBM and HDP  but it need not be. Indeed 
any other deep learning module  including Deep Belief Networks  sparse auto-encoders  or any
other hierarchical Bayesian model can be adapted. This perspective opens a space of compositional
models that may be more suitable for capturing the human-like ability to learn from few examples.
Acknowledgments: This research was supported by NSERC  ONR (MURI Grant 1015GNA126) 
ONR N00014-07-1-0937  ARO W911NF-08-1-0242  and Qualcomm.

8

References
[1] E. Bart  I. Porteous  P. Perona  and M. Welling. Unsupervised learning of visual taxonomies.

In CVPR  pages 1–8  2008.

[2] David M. Blei  Thomas L. Grifﬁths  and Michael I. Jordan. The nested chinese restaurant

process and bayesian nonparametric inference of topic hierarchies. J. ACM  57(2)  2010.

[3] Kevin R. Canini and Thomas L. Grifﬁths. Modeling human transfer learning with the hierar-

chical dirichlet process. In NIPS 2009 workshop: Nonparametric Bayes  2009.
[4] Li Fei-Fei  R. Fergus  and P. Perona. One-shot learning of object categories.

Pattern Analysis and Machine Intelligence  28(4):594–611  April 2006.

IEEE Trans.

[5] G. E. Hinton  S. Osindero  and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural

[6] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural net-

Computation  18(7):1527–1554  2006.

works. Science  313(5786):504 – 507  2006.

[7] C. Kemp  A. Perfors  and J. Tenenbaum. Learning overhypotheses with hierarchical Bayesian

models. Developmental Science  10(3):307–321  2006.

[8] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report 

Dept. of Computer Science  University of Toronto  2009.

[9] Brenden Lake  Ruslan Salakhutdinov  Jason Gross  and Josh Tenenbaum. One-shot learning
In Proceedings of the 33rd Annual Conference of the Cognitive

of simple visual concepts.
Science Society  2011.

[10] H. Larochelle  Y. Bengio  J. Louradour  and P. Lamblin. Exploring strategies for training deep

neural networks. Journal of Machine Learning Research  10:1–40  2009.

[11] Honglak Lee  Roger Grosse  Rajesh Ranganath  and Andrew Y. Ng. Convolutional deep belief
networks for scalable unsupervised learning of hierarchical representations. In Proceedings of
the 26th International Conference on Machine Learning  pages 609–616  2009.

[12] M. A. Ranzato  Y. Boureau  and Y. LeCun. Sparse feature learning for deep belief networks.

Advances in Neural Information Processing Systems  2008.

[13] A. Rodriguez  D. Dunson  and A. Gelfand. The nested Dirichlet process. Journal of the

American Statistical Association  103:11311144  2008.

[14] R. R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines.

In Proceedings of the

International Conference on Artiﬁcial Intelligence and Statistics  volume 12  2009.

[15] R. R. Salakhutdinov and G. E. Hinton. Replicated softmax: an undirected topic model. In

Advances in Neural Information Processing Systems  volume 22  2010.

[16] L.B. Smith  S.S. Jones  B. Landau  L. Gershkoff-Stowe  and L. Samuelson. Object name
learning provides on-the-job training for attention. Psychological Science  pages 13–19  2002.
[17] E. B. Sudderth  A. Torralba  W. T. Freeman  and A. S. Willsky. Describing visual scenes using
transformed objects and parts. International Journal of Computer Vision  77(1-3):291–330 
2008.

[18] G. Taylor  G. E. Hinton  and S. T. Roweis. Modeling human motion using binary latent vari-

ables. In Advances in Neural Information Processing Systems. MIT Press  2006.

[19] Y. W. Teh and G. E. Hinton. Rate-coded restricted Boltzmann machines for face recognition.

In Advances in Neural Information Processing Systems  volume 13  2001.

[20] Y. W. Teh and M. I. Jordan. Hierarchical Bayesian nonparametric models with applications. In

Bayesian Nonparametrics: Principles and Practice. Cambridge University Press  2010.

[21] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical dirichlet processes. Journal

of the American Statistical Association  101(476):1566–1581  2006.

[22] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood

gradient. In ICML. ACM  2008.

[23] A. Torralba  R. Fergus  and W. T. Freeman. 80 million tiny images: a large dataset for non-
parametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence  30(11):1958–1970  2008.

[24] A. Torralba  R. Fergus  and Y. Weiss. Small codes and large image databases for recognition.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  2008.

[25] Fei Xu and Joshua B. Tenenbaum. Word learning as bayesian inference. Psychological Review 

[26] L. Younes. On the convergence of Markovian stochastic algorithms with rapidly decreasing

114(2)  2007.

ergodicity rates  March 17 2000.

9

,Il Memming Park
Evan Archer
Kenneth Latimer
Jonathan Pillow
Jose Alvarez
Mathieu Salzmann