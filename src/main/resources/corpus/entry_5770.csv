2017,Unsupervised Image-to-Image Translation Networks,Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions  one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem  we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks  including street scene image translation  animal image translation  and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.,Unsupervised Image-to-Image Translation Networks

Ming-Yu Liu  Thomas Breuel 

Jan Kautz

{mingyul tbreuel jkautz}@nvidia.com

NVIDIA

Abstract

Unsupervised image-to-image translation aims at learning a joint distribution of
images in different domains by using images from the marginal distributions in
individual domains. Since there exists an inﬁnite set of joint distributions that
can arrive the given marginal distributions  one could infer nothing about the joint
distribution from the marginal distributions without additional assumptions. To
address the problem  we make a shared-latent space assumption and propose an
unsupervised image-to-image translation framework based on Coupled GANs.
We compare the proposed framework with competing approaches and present
high quality image translation results on various challenging unsupervised image
translation tasks  including street scene image translation  animal image translation 
and face image translation. We also apply the proposed framework to domain
adaptation and achieve state-of-the-art performance on benchmark datasets. Code
and additional results are available in https://github.com/mingyuliutw/unit.

1

Introduction

Many computer visions problems can be posed as an image-to-image translation problem  mapping
an image in one domain to a corresponding image in another domain. For example  super-resolution
can be considered as a problem of mapping a low-resolution image to a corresponding high-resolution
image; colorization can be considered as a problem of mapping a gray-scale image to a corresponding
color image. The problem can be studied in supervised and unsupervised learning settings. In the
supervised setting  paired of corresponding images in different domains are available [8  15]. In the
unsupervised setting  we only have two independent sets of images where one consists of images
in one domain and the other consists of images in another domain—there exist no paired examples
showing how an image could be translated to a corresponding image in another domain. Due to
lack of corresponding images  the UNsupervised Image-to-image Translation (UNIT) problem is
considered harder  but it is more applicable since training data collection is easier.
When analyzing the image translation problem from a probabilistic modeling perspective  the key
challenge is to learn a joint distribution of images in different domains. In the unsupervised setting 
the two sets consist of images from two marginal distributions in two different domains  and the task is
to infer the joint distribution using these images. The coupling theory [16] states there exist an inﬁnite
set of joint distributions that can arrive the given marginal distributions in general. Hence  inferring
the joint distribution from the marginal distributions is a highly ill-posed problem. To address the
ill-posed problem  we need additional assumptions on the structure of the joint distribution.
To this end we make a shared-latent space assumption  which assumes a pair of corresponding images
in different domains can be mapped to a same latent representation in a shared-latent space. Based on
the assumption  we propose a UNIT framework that are based on generative adversarial networks
(GANs) and variational autoencoders (VAEs). We model each image domain using a VAE-GAN. The
adversarial training objective interacts with a weight-sharing constraint  which enforces a shared-
latent space  to generate corresponding images in two domains  while the variational autoencoders
relate translated images with input images in the respective domains. We applied the proposed

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Z : shared latent space

z

E2

E1

G1 G2

x1

X1

x2

X2

x1

x2

(a)

E1

G1

z

E2

G2

˜x1!1
1
˜x2!1
2

˜x1!2
1
˜x2!2
2

D1

D2

T/F

T/F

(b)

Figure 1: (a) The shared latent space assumption. We assume a pair of corresponding images (x1  x2) in
two different domains X1 and X2 can be mapped to a same latent code z in a shared-latent space Z. E1 and
E2 are two encoding functions  mapping images to latent codes. G1 and G2 are two generation functions 
mapping latent codes to images. (b) The proposed UNIT framework. We represent E1 E2 G1 and G2 using
CNNs and implement the shared-latent space assumption using a weight sharing constraint where the connection
weights of the last few layers (high-level layers) in E1 and E2 are tied (illustrated using dashed lines) and the
connection weights of the ﬁrst few layers (high-level layers) in G1 and G2 are tied. Here  ˜x1!1
and ˜x2!2
are self-reconstructed images  and ˜x1!2
are domain-translated images. D1 and D2 are adversarial
discriminators for the respective domains  in charge of evaluating whether the translated images are realistic.

and ˜x2!1

1

2

1

2

Networks

Roles

Table 1: Interpretation of the roles of the subnetworks in the proposed framework.
{E1  G1}
VAE for X1

{E1  G1  D1}
Image Translator X1 !X 2 GAN for X1 VAE-GAN [14]

{G1  D1}

{E1  G2}

{G1  G2  D1  D2}

CoGAN [17]

framework to various unsupervised image-to-image translation problems and achieved high quality
image translation results. We also applied it to the domain adaptation problem and achieved state-of-
the-art accuracies on benchmark datasets. The shared-latent space assumption was used in Coupled
GAN [17] for joint distribution learning. Here  we extend the Coupled GAN work for the UNIT
problem. We also note that several contemporary works propose the cycle-consistency constraint
assumption [29  10]  which hypothesizes the existence of a cycle-consistency mapping so that an
image in the source domain can be mapped to an image in the target domain and this translated image
in the target domain can be mapped back to the original image in the source domain. In the paper  we
show that the shared-latent space constraint implies the cycle-consistency constraint.

2 Assumptions

Let X1 and X2 be two image domains. In supervised image-to-image translation  we are given
samples (x1  x2) drawn from a joint distribution PX1 X2(x1  x2). In unsupervised image-to-image
translation  we are given samples drawn from the marginal distributions PX1(x1) and PX2(x2). Since
an inﬁnite set of possible joint distributions can yield the given marginal distributions  we could infer
nothing about the joint distribution from the marginal samples without additional assumptions.
We make the shared-latent space assumption. As shown Figure 1  we assume for any given pair
of images x1 and x2  there exists a shared latent code z in a shared-latent space  such that we
can recover both images from this code  and we can compute this code from each of the two
images. That is  we postulate there exist functions E⇤1  E⇤2  G⇤1  and G⇤2 such that  given a pair of
corresponding images (x1  x2) from the joint distribution  we have z = E⇤1 (x1) = E⇤2 (x2) and
conversely x1 = G⇤1(z) and x2 = G⇤2(z). Within this model  the function x2 = F ⇤1!2(x1) that
maps from X1 to X2 can be represented by the composition F ⇤1!2(x1) = G⇤2(E⇤1 (x1)). Similarly 
x1 = F ⇤2!1(x2) = G⇤1(E⇤2 (x2)). The UNIT problem then becomes a problem of learning F ⇤1!2
and F ⇤2!1. We note that a necessary condition for F ⇤1!2 and F ⇤2!1 to exist is the cycle-consistency
constraint [29  10]: x1 = F ⇤2!1(F ⇤1!2(x1)) and x2 = F ⇤1!2(F ⇤2!1(x2)). We can reconstruct
the input image from translating back the translated input image. In other words  the proposed
shared-latent space assumption implies the cycle-consistency assumption (but not vice versa).
To implement the shared-latent space assumption  we further assume a shared intermediate repre-
sentation h such that the process of generating a pair of corresponding images admits a form of

2

x1
& x2

.

z ! h %

(1)
Consequently  we have G⇤1 ⌘ G⇤L 1  G⇤H and G⇤2 ⌘ G⇤L 2  G⇤H where G⇤H is a common high-level
generation function that maps z to h and G⇤L 1 and G⇤L 2 are low-level generation functions that map
h to x1 and x2  respectively. In the case of multi-domain image translation (e.g.  sunny and rainy
image translation)  z can be regarded as the compact  high-level representation of a scene ("car in
front  trees in back")  and h can be considered a particular realization of z through G⇤H ("car/tree
occupy the following pixels")  and G⇤L 1 and G⇤L 2 would be the actual image formation functions
in each modality ("tree is lush green in the sunny domain  but dark green in the rainy domain").
Assuming h also allow us to represent E⇤1 and E⇤2 by E⇤1 ⌘ E⇤H  E⇤L 1 and E⇤2 ⌘ E⇤H  E⇤L 2.
In the next section  we discuss how we realize the above ideas in the proposed UNIT framework.

3 Framework

1

2 = G2(z2 ⇠ q2(z2|x2)).

Our framework  as illustrated in Figure 1  is based on variational autoencoders (VAEs) [13  22  14]
and generative adversarial networks (GANs) [6  17]. It consists of 6 subnetworks: including two
domain image encoders E1 and E2  two domain image generators G1 and G2  and two domain
adversarial discriminators D1 and D2. Several ways exist to interpret the roles of the subnetworks 
which we summarize in Table 1. Our framework learns translation in both directions in one shot.
VAE. The encoder–generator pair {E1  G1} constitutes a VAE for the X1 domain  termed VAE1. For
an input image x1 2X 1  the VAE1 ﬁrst maps x1 to a code in a latent space Z via the encoder E1 and
then decodes a random-perturbed version of the code to reconstruct the input image via the generator
G1. We assume the components in the latent space Z are conditionally independent and Gaussian with
unit variance. In our formulation  the encoder outputs a mean vector Eµ 1(x1) and the distribution
of the latent code z1 is given by q1(z1|x1) ⌘N (z1|Eµ 1(x1)  I) where I is an identity matrix. The
reconstructed image is ˜x1!1
= G1(z1 ⇠ q1(z1|x1)). Note that here we abused the notation since
we treated the distribution of q1(z1|x1) as a random vector of N (Eµ 1(x1)  I) and sampled from it.
Similarly  {E2  G2} constitutes a VAE for X2: VAE2 where the encoder E2 outputs a mean vector
Eµ 2(x2) and the distribution of the latent code z2 is given by q2(z2|x2) ⌘N (z2|Eµ 2(x2)  I). The
reconstructed image is ˜x2!2
Utilizing the reparameterization trick [13]  the non-differentiable sampling operation can be reparam-
eterized as a differentiable operation using auxiliary random variables. This reparameterization trick
allows us to train VAEs using back-prop. Let ⌘ be a random vector with a multi-variate Gaussian
distribution: ⌘ ⇠N (⌘|0  I). The sampling operations of z1 ⇠ q1(z1|x1) and z2 ⇠ q2(z2|x2) can be
implemented via z1 = Eµ 1(x1) + ⌘ and z2 = Eµ 2(x2) + ⌘  respectively.
Weight-sharing. Based on the shared-latent space assumption discussed in Section 2  we enforce a
weight-sharing constraint to relate the two VAEs. Speciﬁcally  we share the weights of the last few
layers of E1 and E2 that are responsible for extracting high-level representations of the input images
in the two domains. Similarly  we share the weights of the ﬁrst few layers of G1 and G2 responsible
for decoding high-level representations for reconstructing the input images.
Note that the weight-sharing constraint alone does not guarantee that corresponding images in two
domains will have the same latent code. In the unsupervised setting  no pair of corresponding images
in the two domains exists to train the network to output a same latent code. The extracted latent
codes for a pair of corresponding images are different in general. Even if they are the same  the same
latent component may have different semantic meanings in different domains. Hence  the same latent
code could still be decoded to output two unrelated images. However  we will show that through
adversarial training  a pair of corresponding images in the two domains can be mapped to a common
latent code by E1 and E2  respectively  and a latent code will be mapped to a pair of corresponding
images in the two domains by G1 and G2  respectively.
The shared-latent space assumption allows us to perform image-to-image translation. We can
translate an image x1 in X1 to an image in X2 through applying G2(z1 ⇠ q1(z1|x1)). We term such
an information processing stream as the image translation stream. Two image translation streams exist
in the proposed framework: X1 !X 2 and X2 !X 1. The two streams are trained jointly with the
two image reconstruction streams from the VAEs. Once we could ensure that a pair of corresponding

3

images are mapped to a same latent code and a same latent code is decoded to a pair of corresponding
images  (x1  G2(z1 ⇠ q1(z1|x1))) would form a pair of corresponding images. In other words  the
composition of E1 and G2 functions approximates F ⇤1!2 for unsupervised image-to-image translation
discussed in Section 2  and the composition of E2 and G1 function approximates F ⇤2!1.
GANs. Our framework has two generative adversarial networks: GAN1 = {D1  G1} and GAN2 =
{D2  G2}. In GAN1  for real images sampled from the ﬁrst domain  D1 should output true  while
for images generated by G1  it should output false. G1 can generate two types of images: 1) images
from the reconstruction stream ˜x1!1
= G1(z1 ⇠ q1(z1|x1)) and 2) images from the translation
stream ˜x2!1
2 = G1(z2 ⇠ q2(z2|x2)). Since the reconstruction stream can be supervisedly trained  it
. We
is sufﬁce that we only apply adversarial training to images from the translation stream  ˜x2!1
apply a similar processing to GAN2 where D2 is trained to output true for real images sampled from
the second domain dataset and false for images generated from G2.
Cycle-consistency (CC). Since the shared-latent space assumption implies the cycle-consistency
constraint (See Section 2)  we could also enforce the cycle-consistency constraint in the proposed
framework to further regularize the ill-posed unsupervised image-to-image translation problem. The
resulting information processing stream is called the cycle-reconstruction stream.
Learning. We jointly solve the learning problems of the VAE1  VAE2  GAN1 and GAN2 for the
image reconstruction streams  the image translation streams  and the cycle-reconstruction streams:

1

2

min

E1 E2 G1 G2

D1 D2LVAE1(E1  G1) + LGAN1(E1  G1  D1) + LCC1(E1  G1  E2  G2)
max
LVAE2(E2  G2) + LGAN2(E2  G2  D2) + LCC2(E2  G2  E1  G1).

(2)

VAE training aims for minimizing a variational upper bound In (2)  the VAE objects are
LVAE1(E1  G1) =1KL(q1(z1|x1)||p⌘(z))  2Ez1⇠q1(z1|x1)[log pG1(x1|z1)]
LVAE2(E2  G2) =1KL(q2(z2|x2)||p⌘(z))  2Ez2⇠q2(z2|x2)[log pG2(x2|z2)].

(3)
(4)
where the hyper-parameters 1 and 2 control the weights of the objective terms and the KL
divergence terms penalize deviation of the distribution of the latent code from the prior distribution.
The regularization allows an easy way to sample from the latent space [13]. We model pG1 and pG2
using Laplacian distributions  respectively. Hence  minimizing the negative log-likelihood term is
equivalent to minimizing the absolute distance between the image and the reconstructed image. The
prior distribution is a zero mean Gaussian p⌘(z) = N (z|0  I).
In (2)  the GAN objective functions are given by

LGAN1(E1  G1  D1) = 0Ex1⇠PX1
LGAN2(E2  G2  D2) = 0Ex2⇠PX2

(5)
(6)
The objective functions in (5) and (6) are conditional GAN objective functions. They are used to
ensure the translated images resembling images in the target domains  respectively. The hyper-
parameter 0 controls the impact of the GAN objective functions.
We use a VAE-like objective function to model the cycle-consistency constraint  which is given by

[log D1(x1)] + 0Ez2⇠q2(z2|x2)[log(1  D1(G1(z2)))]
[log D2(x2)] + 0Ez1⇠q1(z1|x1)[log(1  D2(G2(z1)))].

LCC1(E1  G1  E2  G2) =3KL(q1(z1|x1)||p⌘(z)) + 3KL(q2(z2|x1!2

1

4Ez2⇠q2(z2|x1!2

1

)[log pG1(x1|z2)]

))||p⌘(z))

(7)

LCC2(E2  G2  E1  G1) =3KL(q2(z2|x2)||p⌘(z)) + 3KL(q1(z1|x2!1

2

))||p⌘(z))

2

)[log pG2(x2|z1)].

4Ez1⇠q1(z1|x2!1

(8)
where the negative log-likelihood objective term ensures a twice translated image resembles the
input one and the KL terms penalize the latent codes deviating from the prior distribution in the
cycle-reconstruction stream (Therefore  there are two KL terms). The hyper-parameters 3 and 4
control the weights of the two different objective terms.
Inheriting from GAN  training of the proposed framework results in solving a mini-max problem
where the optimization aims to ﬁnd a saddle point. It can be seen as a two player zero-sum game.
The ﬁrst player is a team consisting of the encoders and generators. The second player is a team
consisting of the adversarial discriminators. In addition to defeating the second player  the ﬁrst player
has to minimize the VAE losses and the cycle-consistency losses. We apply an alternating gradient

4

0.7

0.56

0.42

0.28

0.14

y
c
a
r
u
c
c
A

0

1

0.7

0.56

0.42

0.28

0.14

0
1000

1=10
1 = 10
1=0.1
1 = 0.1
100

1=1
1 = 1
1=0.01
1 = 0.01
10

1

2
2
(c)

6 Dis
4 Dis

5 Dis
3 Dis

2

3

4

# of shared layers in gen.

(b)

(a)

Accuracy

0.569±0.029

Method
Weight 
Sharing
Cycle 

0.568±0.010

Consistenc
Proposed 0.600±0.015

(d)

Figure 2: (a) Illustration of the Map dataset. Left: satellite image. Right: map. We translate holdout satellite
images to maps and measure the accuracy achieved by various conﬁgurations of the proposed framework.
(b) Translation accuracy versus different network architectures.
(c) Translation accuracy versus different
hyper-parameter values. (d) Impact of weight-sharing and cycle-consistency constraints on translation accuracy.

update scheme similar to the one described in [6] to solve (2). Speciﬁcally  we ﬁrst apply a gradient
ascent step to update D1 and D2 with E1  E2  G1  and G2 ﬁxed. We then apply a gradient descent
step to update E1  E2  G1  and G2 with D1 and D2 ﬁxed.
Translation: After learning  we obtain two image translation functions by assembling a subset of the
subnetworks. We have F1!2(x1) = G2(z1 ⇠ q1(z1|x1)) for translating images from X1 to X2 and
F2!1(x2) = G1(z2 ⇠ q2(z2|x2)) for translating images from X2 to X1.

4 Experiments

We ﬁrst analyze various components of the proposed framework. We then present visual results on
challenging translation tasks. Finally  we apply our framework to the domain adaptation tasks.
Performance Analysis. We used ADAM [11] for training where the learning rate was set to 0.0001
and momentums were set to 0.5 and 0.999. Each mini-batch consisted of one image from the ﬁrst
domain and one image from the second domain. Our framework had several hyper-parameters. The
default values were 0 = 10  3 = 1 = 0.1 and 4 = 2 = 100. For the network architecture 
our encoders consisted of 3 convolutional layers as the front-end and 4 basic residual blocks [7] as
the back-end. The generators consisted of 4 basic residual blocks as the front-end and 3 transposed
convolutional layers as the back-end. The discriminators consisted of stacks of convolutional layers.
We used LeakyReLU for nonlinearity. The details of the networks are given in Appendix A.
We used the map dataset [8] (visualized in Figure 2)  which contained corresponding pairs of images
in two domains (satellite image and map) useful for quantitative evaluation. Here  the goal was to
learn to translate between satellite images and maps. We operated in an unsupervised setting where
we used the 1096 satellite images from the training set as the ﬁrst domain and 1098 maps from the
validation set as the second domain. We trained for 100K iterations and used the ﬁnal model to
translate 1098 satellite images in the test set. We then compared the difference between a translated
satellite image (supposed to be maps) and the corresponding ground truth maps pixel-wisely. A pixel
translation was counted correct if the color difference was within 16 of the ground truth color value.
We used the average pixel accuracy over the images in the test set as the performance metric. We
could use color difference for measuring translation accuracy since the target translation function
was unimodal. We did not evaluate the translation from maps to images since the translation was
multi-modal  which was difﬁcult to construct a proper evaluation metric.
In one experiment  we varied the number of weight-sharing layers in the VAEs and paired each
conﬁguration with discriminator architectures of different depths during training. We changed the
number of weight-sharing layers from 1 to 4. (Sharing 1 layer in VAEs means sharing 1 layer for
E1 and E2 and  at the same time  sharing 1 layer for G1 and G2.) The results were reported in
Figure 2(b). Each curve corresponded to a discriminator architecture of a different depth. The x-axis
denoted the number of weigh-sharing layers in the VAEs. We found that the shallowest discriminator
architecture led to the worst performance. We also found that the number of weight-sharing layer
had little impact. This was due to the use of the residual blocks. As tying the weight of one layer  it
effectively constrained the other layers since the residual blocks only updated the residual information.
In the rest of the experiments  we used VAEs with 1 sharing layer and discriminators of 5 layers.

5

We analyzed impact of the hyper-parameter values to the translation accuracy. For different weight
values on the negative log likelihood terms (i.e.  2  4)  we computed the achieved translation
accuracy over different weight values on the KL terms (i.e.  1  3). The results were reported in
Figure 2(c). We found that  in general  a larger weight value on the negative log likelihood terms
yielded a better translation accuracy. We also found setting the weights of the KL terms to 0.1 resulted
in consistently good performance. We hence set 1 = 3 = 0.1 and 2 = 4 = 100.
We performed an ablation study measuring impact of the weight-sharing and cycle-consistency
constraints to the translation performance and showed the results in Figure 2(d). We reported average
accuracy over 5 trials (trained with different initialized weights.). We note that when we removed
the weight-sharing constraint (as a consequence  we also removed the reconstruction streams in the
framework)  the framework was reduced to the CycleGAN architecture [29  10]. We found the model
achieved an average pixel accuracy of 0.569. When we removed the cycle-consistency constraint
and only used the weight-sharing constraint1  it achieved 0.568 average pixel accuracy. But when we
used the full model  it achieved the best performance of 0.600 average pixel accuracy. This echoed
our point that for the ill-posed joint distribution recovery problem  more constraints are beneﬁcial.
Qualitative results. Figure 3 to 6 showed results of the proposed framework on various UNIT tasks.
Street images. We applied the proposed framework to several unsupervised street scene image
translation tasks including sunny to rainy  day to night  summery to snowy  and vice versa. For each
task  we used a set of images extracted from driving videos recorded at different days and cities. The
numbers of the images in the sunny/day  rainy  night  summery  and snowy sets are 86165  28915 
36280  6838  and 6044. We trained the network to translate street scene image of size 640⇥480. In
Figure 3  we showed several example translation results . We found that our method could generate
realistic translated images. We also found that one translation was usually harder than the other.
Speciﬁcally  the translation that required adding more details to the image was usually harder (e.g.
night to day). Additional results are available in https://github.com/mingyuliutw/unit.
Synthetic to real. In Figure 3  we showed several example results achieved by applying the proposed
framework to translate images between the synthetic images in the SYNTHIA dataset [23] and the
real images in the Cityscape dataset [2]. For the real to synthetic translation  we found our method
made the cityscape images cartoon like. For the synthetic to real translation  our method achieved
better results in the building  sky  road  and car regions than in the human regions.
Dog breed conversion. We used the images of Husky  German Shepherd  Corgi  Samoyed  and Old
English Sheep dogs in the ImageNet dataset to learn to translate dog images between different breeds.
We only used the head regions  which were extracted by a template matching algorithm. Several
example results were shown in Figure 4. We found our method translated a dog to a different breed.
Cat species conversion. We also used the images of house cat  tiger  lion  cougar  leopard  jaguar 
and cheetah in the ImageNet dataset to learn to translate cat images between different species. We
only used the head regions  which again were extracted by a template matching algorithm. Several
example results were shown in Figure 5. We found our method translated a cat to a different specie.
Face attribute. We used the CelebA dataset [18] for attribute-based face images translation. Each face
image in the dataset had several attributes  including blond hair  smiling  goatee  and eyeglasses. The
face images with an attribute constituted the 1st domain  while those without the attribute constituted
the 2nd domain. In Figure 6  we visualized the results where we translated several images that do not
have blond hair  eye glasses  goatee  and smiling to corresponding images with each of the individual
attributes. We found that the translated face images were realistic.
Domain Adaptation. We applied the proposed framework to the problem for adapting a classiﬁer
trained using labeled samples in one domain (source domain) to classify samples in a new domain
(target domain) where labeled samples in the new domain are unavailable during training. Early
works have explored ideas from subspace learning [4] to deep feature learning [5  17  26].
We performed multi-task learning where we trained the framework to 1) translate images between
the source and target domains and 2) classify samples in the source domain using the features
extracted by the discriminator in the source domain. Here  we tied the weights of the high-level
layers of D1 and D2. This allows us to adapt a classiﬁer trained in the source domain to the target
domain. Also  for a pair of generated images in different domains  we minimized the L1 distance

1We used this architecture in an earlier version of the paper.

6

Figure 3: Street scene image translation results. For each pair  left is input and right is the translated image.

Old Eng.
Sheep Dog

Husky

German
Shepherd

Input

Corgi

Input

Husky

Corgi

Input

Cougar

Cheetah

Leopard

Lion

Tiger

Input

Leopard

Figure 4: Dog breed translation results.

Input

+Blond Hair +Eyeglasses

Figure 5: Cat species translation results.
+Goatee

+Smiling

Input

+Blond Hair +Eyeglasses

+Goatee

+Smiling

Figure 6: Attribute-based face translation results.

7

Table 2: Unsupervised domain adaptation performance. The reported numbers are classiﬁcation accuracies.

SA [4]
0.5932

-
-

DANN [5]

0.7385

-
-

DTN [26]
0.8488

-
-

Method

SVHN! MNIST
MNIST! USPS
USPS! MNIST

CoGAN

UNIT (proposed)

-

0.9565
0.9315

0.9053
0.9597
0.9358

between the features extracted by the highest layer of the discriminators  which further encouraged
D1 and D2 to interpret a pair of corresponding images in the same way. We applied the approach to
several tasks including adapting from the Street View House Number (SVHN) dataset [20] to the
MNIST dataset and adapting between the MNIST and USPS datasets. Table 2 reported the achieved
performance with comparison to the competing approaches. We found that our method achieved a
0.9053 accuracy for the SVHN!MNIST task  which was much better than 0.8488 achieved by the
previous state-of-the-art method [26]. We also achieved better performance for the MNIST$SVHN
task than the Coupled GAN approach  which was the state-of-the-art. The digit images had a small
resolution. Hence  we used a small network. We also found that the cycle-consistency constraint was
not necessary for this task. More details about the experiments are available in Appendix B.

5 Related Work

Several deep generative models were recently proposed for image generation including GANs [6] 
VAEs [13  22]  and PixelCNN [27]. The proposed framework was based on GANs and VAEs but it
was designed for the unsupervised image-to-image translation task  which could be considered as a
conditional image generation model. In the following  we ﬁrst review several recent GAN and VAE
works and then discuss related image translation works.
GAN learning is via staging a zero-sum game between the generator and discriminator. The quality
of GAN-generated images had improved dramatically since the introduction. LapGAN [3] proposed
a Laplacian pyramid implementation of GANs. DCGAN [21] used a deeper convolutional network.
Several GAN training tricks were proposed in [24]. WGAN [1] used the Wasserstein distance.
VAEs optimize a variational bound. By improving the variational approximation  better image
generation results were achieved [19  12]. In [14]  a VAE-GAN architecture was proposed to improve
image generation quality of VAEs. VAEs were applied to translate face image attribute in [28].
Conditional generative model is a popular approach for mapping an image from one domain to
another. Most of the existing works were based on supervised learning [15  8  9]. Our work differed
to the previous works in that we do not need corresponding images. Recently  [26] proposed the
domain transformation network (DTN) and achieved promising results on translating small resolution
face and digit images. In addition to faces and digits  we demonstrated that the proposed framework
can translate large resolution natural images. It also achieved a better performance in the unsupervised
domain adaptation task. In [25]  a conditional generative adversarial network-based approach was
proposed to translate a rendering images to a real image for gaze estimation. In order to ensure
the generated real image was similar to the original rendering image  the L1 distance between
the generated and original image was minimized. We note that two contemporary papers [29  10]
independently introduced the cycle-consistency constraint for the unsupervised image translation.
We showed that that the cycle-consistency constraint is a natural consequence of the proposed
shared-latent space assumption. From our experiment  we found that cycle-consistency and the
weight-sharing (a realization of the shared-latent space assumption) constraints rendered comparable
performance. When the two constraints were jointed used  the best performance was achieved.

6 Conclusion and Future Work

We presented a general framework for unsupervised image-to-image translation. We showed it
learned to translate an image from one domain to another without any corresponding images in two
domains in the training dataset. The current framework has two limitations. First  the translation
model is unimodal due to the Gaussian latent space assumption. Second  training could be unstable
due to the saddle point searching problem. We plan to address these issues in the future work.

8

References
[1] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875  2017.
[2] M. Cordts  M. Omran  S. Ramos  T. Scharwächter  M. Enzweiler  R. Benenson  U. Franke  S. Roth  and
B. Schiele. The cityscapes dataset. Conference on Computer Vision and Pattern Recognition Workshop 
2015.

[3] E. L. Denton  S. Chintala  R. Fergus  et al. Deep generative image models using a laplacian pyramid of

adversarial networks. Advances in Neural Information Processing Systems  2015.

[4] B. Fernando  A. Habrard  M. Sebban  and T. Tuytelaars. Unsupervised visual domain adaptation using

subspace alignment. International Conference on Computer Vision  2013.

[5] Y. Ganin  E. Ustinova  H. Ajakan  P. Germain  H. Larochelle  F. Laviolette  M. Marchand  and V. Lempitsky.

Domain-adversarial training of neural networks. Journal of Machine Learning Research  2016.

[6] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. Advances in Neural Information Processing Systems  2014.

[7] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. Computer Vision and

Pattern Recognition  2016.

[8] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional adversarial

networks. Conference on Computer Vision and Pattern Recognition  2017.

[9] J. Johnson  A. Alahi  and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution.

European Conference in Computer Vision  2016.

[10] T. Kim  M. Cha  H. Kim  J. Lee  and J. Kim. Learning to discover cross-domain relations with generative

adversarial networks. International Conference on Machine Learning  2017.

[11] D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning

[12] D. P. Kingma  T. Salimans  and M. Welling. Improving variational inference with inverse autoregressive

ﬂow. Advances in Neural Information Processing Systems  2016.

[13] D. P. Kingma and M. Welling. Auto-encoding variational bayes. International Conference on Learning

Representations  2015.

Representations  2014.

[14] A. B. L. Larsen  S. K. Sønderby  H. Larochelle  and O. Winther. Autoencoding beyond pixels using a

learned similarity metric. International Conference on Machine Learning  2016.

[15] C. Ledig  L. Theis  F. Huszár  J. Caballero  A. Cunningham  A. Acosta  A. Aitken  A. Tejani  J. Totz 
Z. Wang  et al. Photo-realistic single image super-resolution using a generative adversarial network.
Conference on Computer Vision and Pattern Recognition  2017.

[16] T. Lindvall. Lectures on the coupling method. Courier Corporation  2002.
[17] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. Advances in Neural Information

Processing Systems  2016.

on Computer Vision  2015.

[18] Z. Liu  P. Luo  X. Wang  and X. Tang. Deep learning face attributes in the wild. International Conference

[19] L. Maaløe  C. K. Sønderby  S. K. Sønderby  and O. Winther. Auxiliary deep generative models. Interna-

tional Conference on Machine Learning  2016.

[20] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural images with

unsupervised feature learning. Advances in Neural Information Processing Systems workshop  2011.

[21] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. International Conference on Learning Representations  2016.

[22] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and variational inference in

deep latent gaussian models. International Conference on Machine Learning  2014.

[23] G. Ros  L. Sellart  J. Materzynska  D. Vazquez  and A. Lopez. The SYNTHIA Dataset: A large collection
of synthetic images for semantic segmentation of urban scenes. Conference on Computer Vision and
Pattern Recognition  2016.

[24] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved techniques for

training gans. Advances in Neural Information Processing Systems  2016.

[25] A. Shrivastava  T. Pﬁster  O. Tuzel  J. Susskind  W. Wang  and R. Webb. Learning from simulated and
unsupervised images through adversarial training. Conference on Computer Vision and Pattern Recognition 
2017.

[26] Y. Taigman  A. Polyak  and L. Wolf. Unsupervised cross-domain image generation.

International

Conference on Learning Representations  2017.

[27] A. van den Oord  N. Kalchbrenner  L. Espeholt  O. Vinyals  A. Graves  et al. Conditional image generation

with pixelcnn decoders. Advances in Neural Information Processing Systems  2016.

[28] X. Yan  J. Yang  K. Sohn  and H. Lee. Attribute2image: Conditional image generation from visual

attributes. European Conference in Computer Vision  2016.

[29] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using cycle-consistent

adversarial networks. International Conference on Computer Vision  2017.

9

,Naiyan Wang
Dit-Yan Yeung
Ming-Yu Liu
Jan Kautz