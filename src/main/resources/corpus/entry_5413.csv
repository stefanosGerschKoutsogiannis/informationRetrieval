2016,Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction,Time series prediction problems are becoming increasingly high-dimensional in modern applications  such as climatology and demand forecasting. For example  in the latter problem  the number of items for which demand needs to be forecast might be as large as 50 000. In addition  the data is generally noisy and full of missing values. Thus  modern applications require methods that are  highly scalable  and can deal with noisy data in terms of corruptions or missing values. However  classical time series methods usually fall short of handling these issues.  In this paper  we present a temporal regularized matrix factorization  (TRMF) framework which supports data-driven temporal learning and  forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values.  Our proposed TRMF is highly general  and subsumes many existing approaches for time series analysis.  We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular   TRMF is two orders of magnitude  faster than other methods on a problem of dimension 50 000  and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.,Temporal Regularized Matrix Factorization for

High-dimensional Time Series Prediction

Hsiang-Fu Yu

University of Texas at Austin
rofuyu@cs.utexas.edu

Nikhil Rao

Technicolor Research

nikhilrao86@gmail.com

Inderjit S. Dhillon

University of Texas at Austin

inderjit@cs.utexas.edu

Abstract

Time series prediction problems are becoming increasingly high-dimensional in
modern applications  such as climatology and demand forecasting. For example 
in the latter problem  the number of items for which demand needs to be forecast
might be as large as 50 000. In addition  the data is generally noisy and full of
missing values. Thus  modern applications require methods that are highly scalable 
and can deal with noisy data in terms of corruptions or missing values. However 
classical time series methods usually fall short of handling these issues. In this
paper  we present a temporal regularized matrix factorization (TRMF) framework
which supports data-driven temporal learning and forecasting. We develop novel
regularization schemes and use scalable matrix factorization methods that are
eminently suited for high-dimensional time series data that has many missing values.
Our proposed TRMF is highly general  and subsumes many existing approaches
for time series analysis. We make interesting connections to graph regularization
methods in the context of learning the dependencies in an autoregressive framework.
Experimental results show the superiority of TRMF in terms of scalability and
prediction quality. In particular  TRMF is two orders of magnitude faster than
other methods on a problem of dimension 50 000  and generates better forecasts on
real-world datasets such as Wal-mart E-commerce datasets.

Introduction

1
Time series analysis is a central problem in many applications such as demand forecasting and
climatology. Often  such applications require methods that are highly scalable to handle a very large
number (n) of possibly inter-dependent one-dimensional time series and/or have a large time frame
(T ). For example  climatology applications involve data collected from possibly thousands of sensors 
every hour (or less) over several years. Similarly  a store tracking its inventory would track thousands
of items every day for multiple years. Not only is the scale of such problems huge  but they might
also involve missing values  due to sensor malfunctions  occlusions or simple human errors. Thus 
modern time series applications present two challenges to practitioners: scalability to handle large n
and T and the ﬂexibility to handle missing values.
Most approaches in the traditional time series literature such as autoregressive (AR) models or
dynamic linear models (DLM)[7  21] focus on low-dimensional time-series data and fall short of
handling the two aforementioned issues. For example  an AR model of order L requires O(T L2n4 +
L3n6) time to estimate O(Ln2) parameters  which is prohibitive even for moderate values of n.
Similarly  Kalman ﬁlter based DLM approaches need O(kn2T + k3T ) computation cost to update
parameters  where k is the latent dimensionality  which is usually chosen to be larger than n in many
situations [13]. As a speciﬁc example  the maximum likelihood estimator implementation in the
widely used R-DLM package [12]  which relies on a general optimization solver  cannot scale beyond

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

n in the tens. (See Appendix D for details). On the other hand  for models such as AR  the ﬂexibility
to handle missing values can also be very challenging even for one-dimensional time series [1]  let
alone the difﬁculty to handle high dimensional time series.
A natural way to model high-dimensional time series data is in the form of a matrix  with rows
corresponding to each one-dimensional time series and columns corresponding to time points. In light
of the observation that n time series are usually highly correlated with each other  there have been
some attempts to apply low-rank matrix factorization (MF) or matrix completion (MC) techniques
to analyze high-dimensional time series [2  14  16  23  26]. Unlike the AR and DLM models above 
state-of-the-art MF methods scale linearly in n  and hence can handle large datasets. Let Y 2 Rn⇥T
be the matrix for the observed n-dimensional time series with Yit being the observation at the t-th
time point of the i-th time series. Under the standard MF approach  Yit is estimated by the inner
product f>i xt  where fi 2 Rk is a k-dimensional latent embedding for the i-th time series  and
xt 2 Rk is a k-dimensional latent temporal embedding for the t-th time point. We can stack the xts
into the columns into a matrix X 2 Rk⇥T and f>i
into the rows of F 2 Rn⇥k (Figure 1) to get
Y ⇡ F X. We can then solve:
(1)

min

+ fRf (F ) + xRx(X) 

F X X(i t)2⌦Yit  f>i xt2

Y

Ynew

⇡

Xnew

F

f>i

X

xt

s

m
e
t
I

Time

Time- dependent variables

Figure 1: Matrix Factorization model for mul-
tiple time series. F captures features for each
time series in the matrix Y   and X captures
the latent and time-varying variables.

where ⌦ is the set of the observed entries. Rf (F ) 
Rx(X) are regularizers for F and X  which usu-
ally play a role to avoid overﬁtting and/or to en-
courage some speciﬁc temporal structures among
the embeddings. It is clear that the common choice
of the regularizer Rx(X) = kXkF is no longer
appropriate for time series applications  as it does
not take into account the ordering among the tem-
poral embeddings {xt}. Most existing MF ap-
proaches [2  14  16  23  26] adapt graph-based ap-
proaches to handle temporal dependencies. Specif-
ically  the dependencies are described by a weighted similarity graph and incorporated through
a Laplacian regularizer [18]. However  graph-based regularization fails in cases where there are
negative correlations between two time points. Furthermore  unlike scenarios where explicit graph
information is available with the data (such as a social network or product co-purchasing graph
for recommender systems)  explicit temporal dependency structure is usually unavailable and has
to be inferred or approximated  which causes practitioners to either perform a separate procedure
to estimate the dependencies or consider very short-term dependencies with simple ﬁxed weights.
Moreover  existing MF approaches  while yielding good estimations for missing values in past points 
are poor in terms of forecasting future values  which is the problem of interest in time series analysis.
In this paper  we propose a novel temporal regularized matrix factorization framework (TRMF) for
high-dimensional time series analysis. In TRMF  we consider a principled approach to describe the
structure of temporal dependencies among latent temporal embeddings {xt} and design a temporal
regularizer to incorporate this temporal structure into the standard MF formulation. Unlike most
existing MF approaches  our TRMF method supports data-driven temporal dependency learning
and also brings the ability to forecast future values to a matrix factorization approach. In addition 
inherited from the property of MF approaches  TRMF can easily handle high-dimensional time series
data even in the presence of many missing values. As a speciﬁc example  we demonstrate a novel
autoregressive temporal regularizer which encourages AR structure among temporal embeddings
{xt}. We also make connections between the proposed regularization framework and graph-based
approaches [18]  where even negative correlations can be accounted for. This connection not only
leads to better understanding about the dependency structure incorporated by our framework but also
brings the beneﬁt of using off-the-shelf efﬁcient solvers such as GRALS [15] directly to solve TRMF.
Paper Organization. In Section 2  we review the existing approaches and their limitations on data
with temporal dependencies. We present the proposed TRMF framework in Section 3  and show that
the method is highly general and can be used for a variety of time series applications. We introduce
a novel AR temporal regularizer in Section 4  and make connections to graph-based regularization
approaches. We demonstrate the superiority of the proposed approach via extensive experimental
results in Section 5 and conclude the paper in Section 6.

2

2 Motivations: Existing Approaches and Limitations
2.1 Classical Time-Series Models
Models such as AR and DLM are not suitable for modern multiple high-dimensional time series data
(i.e.  both n and T are large) due to their inherent computational inefﬁciency (see Section 1). To avoid
overﬁtting in AR models  there have been studies with various structured transition matrices such
as low rank and sparse matrices [5  10  11]. The focus of this research has been on obtaining better
statistical guarantees. The scalability issue of AR models remains open. On the other hand  it is also
challenging for many classic time-series models to deal with data that has many missing values [1].
In many situations where the model parameters are either given or designed by practitioners  the
Kalman ﬁlter approach is used to perform forecasting  while the Kalman smoothing approach is
used to impute missing entries. When model parameters are unknown  EM algorithms are applied to
estimate both the model parameters and latent embeddings for DLM [3  8  9  17  19]. As most EM
approaches for DLM contain the Kalman ﬁlter as a building block  they cannot scale to very high
dimensional time series data. Indeed  as shown in Section 5  the popular R package for DLM’s does
not scale beyond data with tens of dimensions.
2.2 Existing Matrix Factorization Approaches for Data with Temporal Dependencies
t=1kxtk2 is usually the
In standard MF (1)  the squared Frobenius norm Rx(X) = kXk2
regularizer of choice for X. Because squared Frobenius norm assumes no dependencies among {xt} 
standard MF formulation is invariant to column permutation and not applicable to data with temporal
dependencies. Hence most existing temporal MF approaches turn to the framework of graph-based
regularization [18] for temporally dependent {xt}  with a graph encoding the temporal dependencies.
An exception is the work in [22]  where the authors use specially designed regularizers to encourage
a log-normal structure on the temporal coefﬁcients.
Graph regularization for temporal dependencies:The framework of graph-based regularization is
an approach to describe and incorporate general dependencies among variables. Let G be a graph
over {xt} and Gts be the edge weight between the t-th node and s-th node. A popular regularizer to
include as part of an objective function is the following:

F = PT

1

⌘

w4

(2)

kxtk2 

2Xt

Gtskxt  xsk2 +

Rx(X) = G(X | G  ⌘) :=

2Xt⇠s
where t ⇠ s denotes an edge between t-th node and
s-th node  and the second summation term is used to
guarantee strong convexity. A large Gts will ensure
that xt and xs are close to each other in Euclidean
distance  when (2) is minimized. Note that to guaran-
tee the convexity of G(X | G  ⌘)  we need Gts  0.
To apply graph-based regularizers to temporal dependencies  we need to specify the (repeating)
dependency pattern by a lag set L and a weight vector w such that all the edges t ⇠ s of distance
l (i.e.  |s  t| = l) share the same weight Gts = wl. See Figure 2 for an example with L = {1  4}.
Given L and w  the corresponding graph regularizer becomes
wl(xt  xtl)2 +

···
···
Figure 2: Graph-based regularization for tem-
poral dependencies.

G(X | G  ⌘) =

kxtk2.

(3)

t  3

t  1

t  4

w1

w1

t  2

w1

w1

t

w1

t + 1

w4

1

2Xl2LXt:t>l

⌘

2Xt

This direct use of graph-based approach  while intuitive  has two issues: a) there might be negatively
correlated dependencies between two time points; b) unlike many applications where such regularizers
are used  the explicit temporal dependency structure is usually not available and has to be inferred.
As a result  most existing approaches consider only very simple temporal dependencies such as a
small size of L (e.g.  L = {1}) and/or uniform weights (e.g.  wl = 1  8l 2L ). For example  a
simple chain graph is considered to design the smoothing regularizer in TCF [23]. This leads to poor
forecasting abilities of existing MF methods for large-scale time series applications.
2.3 Challenges to Learn Temporal Dependencies
One could try to learn the weights wl automatically  by using the same regularizer as in (3) but with
the weights unknown. This would lead to the following optimization problem:
wl(xt  xtl)2+

+fRf (F )+

x⌘

min

F X w0 X(i t)2⌦Yit  f>i xt2

2 Xt kxtk2  (4)

where 0 is the zero vector  and w  0 is the constraint imposed by graph regularization.

2 Xl2L Xt:tl>0

x

3

It is not hard to see that the above optimization yields the trivial all-zero solution for w⇤  meaning
the objective function is minimized when no temporal dependencies exist! To avoid the all zero

is not hard to see that this will result in w⇤ being a 1-sparse vector  with wl⇤ being 1  where

solution  one might want to impose a simplex constraint on w (i.e.  Pl2L wl = 1). Again  it
l⇤ = arg minl2LPt:t>l kxt  xtlk2. Thus  looking to learn the weights automatically by simply

plugging in the regularizer in the MF formulation is not a viable option.
3 Temporal Regularized Matrix Factorization
In order to resolve the limitations mentioned in Sections 2.2 and 2.3  we propose the Temporal
Regularized Matrix Factorization (TRMF) framework  which is a novel approach to incorporate
temporal dependencies into matrix factorization models. Unlike the aforementioned graph-based
approaches  we propose to use well-studied time series models to describe temporal dependencies
among {xt} explicitly. Such models take the form:
(5)
where ✏t is a Gaussian noise vector  and M⇥ is the time-series model parameterized by L and ⇥. L
is a set containing the lag indices l  denoting a dependency between t-th and (t  l)-th time points 
while ⇥ captures the weighting information of temporal dependencies (such as the transition matrix
in AR models). To incorporate the temporal dependency into the standard MF formulation (1)  we
propose to design a new regularizer TM(X | ⇥) which encourages the structure induced by M⇥.
Taking a standard approach to model time series  we set TM(X | ⇥) be the negative log likelihood of
observing a particular realization of the {xt} for a given model M⇥:
(6)
TM(X | ⇥) =  log P(x1  . . .   xT | ⇥).
When ⇥ is given  we can use Rx(X) = TM(X | ⇥) in the MF formulation (1) to encourage {xt} to
follow the temporal dependency induced by M⇥. When the ⇥ is unknown  we can treat ⇥ as another
set of variables and include another regularizer R✓(⇥) into (1):

xt = M⇥({xtl : l 2L} ) + ✏t 

min

F X ⇥ X(i t)2⌦Yit  f>i xt2

+ fRf (F ) + xTM(X | ⇥) + ✓R✓(⇥) 

(7)

min
⇥

xTM(X | ⇥) + ✓R✓(⇥) 

which be solved by an alternating minimization procedure over F   X  and ⇥.
Data-driven Temporal Dependency Learning in TRMF:Recall that in Section 2.3  we showed
that directly using graph based regularizers to incorporate temporal dependencies leads to trivial
solutions for the weights. TRMF circumvents this issue. When F and X are ﬁxed  (7) is reduced to:
(8)
which is a maximum-a-posterior (MAP) estimation problem (in the Bayesian sense) to estimate the
best ⇥ for a given {xt} under the M⇥ model. There are well-developed algorithms to solve (8) and
obtain non-trivial ⇥. Thus  unlike most existing temporal matrix factorization approaches where the
strength of dependencies is ﬁxed  ⇥ in TRMF can be learned automatically from data.
Time Series Analysis with TRMF:We can see that TRMF (7) lends itself seamlessly to handle a
variety of commonly encountered tasks in analyzing data with temporal dependency:
• Time-series Forecasting: Once we have M⇥ for latent embeddings {xt : 1  . . .   T}  we can
use it to predict future latent embeddings {xt : t > T} and have the ability to obtain non-trivial
forecasting results for yt = F xt for t > T .
• Missing-value Imputation: In some time-series applications  some entries in Y might be unob-
served  for example  due to faulty sensors in electricity usage monitoring or occlusions in the
case of motion recognition in video. We can use f>i xt to impute these missing entries  much like
standard matrix completion  and is useful in recommender systems [23] and sensor networks [26].
Extensions to Incorporate Extra Information:Like matrix factorization  TRMF (7) can be ex-
tended to incorporate additional information. For example  pairwise relationships between the time
series can be incorporated using structural regularizers on F . Furthermore  when features are known
for the time series  we can make use of interaction models such as those in [6  24  25]. Also  TRMF
can be extended to tensors. More details on these extensions can be found in Appendix B.
4 A Novel Autoregressive Temporal Regularizer
In Section 3  we described the TRMF framework in a very general sense  with the regularizer
TM(X | ⇥) incorporating dependencies speciﬁed by the time series model M⇥. In this section 
we specialize this to the case of AR models  which are parameterized by a lag set L and weights
W = W (l) 2 Rk⇥k : l 2L . Assume that xt is a noisy linear combination of some previous

4

1
2

1
2

1
2

(9)

+

(10)

2

⌘

+

⌘
2k ¯xk2 

TXt=m

W (l)xtl

points; that is  xt =Pl2L W (l)xtl + ✏t  where ✏t is a Gaussian noise vector. For simplicity  we
assume that the ✏t ⇠N (0  2Ik)  where Ik is the k ⇥ k identity matrix1. The temporal regularizer
TM(X | ⇥) corresponding to this AR model can be written as:

TAR( ¯x|L  ¯w ⌘ ) =

xt Xl2L

TAR(X |L W ⌘ ) :=

wlxtl!2

TXt=m xt Xl2L

r=1 TAR( ¯xr |L  ¯wr ⌘ )  where we deﬁne

2Xt kxtk2 
where m := 1 + L  L := max(L)  and ⌘> 0 to guarantee the strong convexity of (9).
TRMF allows us to learn the weightsW (l) when they are unknown. Since each W (l) 2 Rk⇥k 
there will be |L|k2 variables to learn  which may lead to overﬁtting. To prevent this and to yield
more interpretable results  we consider diagonal W (l)  reducing the number of parameters to |L|k.
To simplify notation  we use W to denote the k ⇥ L matrix where the l-th column constitutes
the diagonal elements of W (l). Note that for l /2L   the l-th column of W is a zero vector. Let
¯x>r = [···   Xrt ··· ] be the r-th row of X and ¯w>r = [···  Wrl ··· ] be the r-th row of W. Then
(9) can be written as TAR(X |L W ⌘ ) =Pk
with xt being the t-th element of ¯x  and wl being the l-th element of ¯w.
Correlations among Multiple Time Series. Even whenW l is diagonal  TRMF retains the power
to capture the correlations among time series via the factors {fi}  since it has an effect only on the
structure of latent embeddings {xt}. Indeed  as the i-th dimension of {yt} is modeled by f>i X
in (7)  the low rank F is a k dimensional latent embedding of multiple time series. This embedding
captures correlations among multiple time series. Furthermore  {fi} acts as time series features 
which can be used to perform classiﬁcation/clustering even in the presence of missing values.
Choice of Lag Index Set L. Unlike most approaches mentioned in Section 2.2  the choice of L in
TRMF is more ﬂexible. Thus  TRMF can provide important advantages: First  because there is no
need to specify the weight parameters W  L can be chosen to be larger to account for long range
dependencies  which also yields more accurate and robust forecasts. Second  the indices in L can be
discontinuous so that one can easily embed domain knowledge about periodicity or seasonality. For
example  one might consider L = {1  2  3  51  52  53} for weekly data with a one year seasonality.
Connections to Graph Regularization. We now establish connections between TAR( ¯x|L  ¯w ⌘ )
and graph regularization (2) for matrix factorization. Let ¯L := L[{ 0}  w0 = 1 so that (10) is

⌘
2k ¯xk2 
and let (d) :=l 2 ¯L : l  d 2 ¯L . We then have the following result:
Theorem 1. Given a lag index set L  weight vector ¯w 2 RL  and ¯x 2 RT   there is a weighted
signed graph GAR with T nodes and a diagonal matrix D 2 RT⇥T such that
(11)
where G ¯x | GAR ⌘ is the graph regularization (2) with G = GAR. Furthermore  8t and d
t t+d =8<:

wlxtl1A
TAR( ¯x|L  ¯w ⌘ ) = G ¯x | GAR ⌘ +
and Dtt =0@Xl2 ¯L
wlwld if (d) 6=  
otherwise 

wl1A0@Xl2 ¯L

Xl2(d) Xmt+lT

TAR( ¯x|L  ¯w ⌘ ) =

0@Xl2 ¯L

wl[m  t + l  T ]1A

TXt=m

See Appendix C.1 for a detailed proof. From
Theorem 1  we see that (d) is non-empty if and
only if there are edges between time points sep-
arated by d in GAR. Thus  we can construct the
dependency graph for TAR( ¯x|L  ¯w ⌘ ) by check-
ing whether (d) is empty. Figure 3 demon-
strates an example with L = {1  4}. We can see
that besides edges of distance d = 1 and d = 4  there are also edges of distance d = 3 (dotted edges
in Figure 3) because 4  3 2 ¯L and (3) = {4}.

Figure 3: The graph structure induced by the AR
temporal regularizer (10) with L = {1  4}.

w1w4

w1w4

w1w4

t  3

t  2

t  4

t  1

···

···

w1

w1

w1

w1

w1

t + 1

t

¯x>D ¯x 

w4

w4

2

+

GAR

0

1
2

1If the (known) covariance matrix is not identity  we can suitably modify the regularizer.

5

Table 1: Data statistics.

synthetic electricity trafﬁc walmart-1 walmart-2
1 582
187
49.3%

963
10 560
0%

370
26 304
0%

1 350
187
55.3%

16
128
0%

n
T

missing ratio

Although Theorem 1 shows that AR-based regularizers are similar to the graph-based regularization
framework  we note the following key differences:
• The graph GAR in Theorem 1 contains both positive and negative edges. This implies that the
AR temporal regularizer is able to support negative correlations  which the standard graph-based
regularizer cannot. This can make G ¯x | GAR ⌘ non-convex. The addition of the second term in
(11)  however  still leads to a convex regularizer TAR( ¯x|L  ¯w ⌘ ).
• Unlike (3) where there is freedom to specify a weight for each distance  in the graph GAR  the
weight values for the edges are more structured (e.g.  the weight for d = 3 in Figure 3 is w1w4).
Hence  minimization w.r.t. w0s is not trivial  and neither are the obtained solutions.
Plugging TM(X | ⇥) = TAR(X |L W ⌘ ) into (7)  we obtain the following problem:

min

F X W X(i t)2⌦Yit  f>i xt2

kXr=1

+ fRf (F ) +

xTAR( ¯xr |L  ¯wr ⌘ ) + wRw(W) 

(12)

where Rw(W) is a regularizer for W. We will refer to (12) as TRMF-AR. We can apply alternating
minimization to solve (12). In fact  solving for each variable reduces to well known methods  for
which highly efﬁcient algorithms exist:
Updates for F . When X and W are ﬁxed  the subproblem of updating F is the same as updating F
while X ﬁxed in (1). Thus  fast algorithms such as alternating least squares or coordinate descent can
be applied directly to ﬁnd F   which costs O(|⌦|k2) time.
Updates for X. We solve arg minXP(i t)2⌦Yit  f>i xt2 + xPk
r=1 TAR( ¯xr |L  ¯wr ⌘ ). From
Theorem 1  we see that TAR( ¯x|L  ¯w ⌘ ) shares the same form as the graph regularizer  and we can
apply GRALS [15] to ﬁnd X  which costs O(|L|T k2) time.
Updates for W. How to update W while F and X ﬁxed depends on the choice of Rw(W). There
are many parameter estimation techniques developed for AR with various regularizers [11  20]. For
simplicity  we consider the squared Frobenius norm: Rw(W) = kWk2
F . As a result  each row of ¯wr
of W can be updated by solving the following one-dimensional autoregressive problem.
w
x k ¯wk2 

xTAR( ¯xr |L  ¯w ⌘ ) + wk ¯wk2 ⌘ arg min

wlxtl!2

arg min
¯w

+

¯w

TXt=m xt Xl2L

which is a simple |L| dimensional ridge regression problem with T  m + 1 instances  which can be
solved efﬁciently by Cholesky factorization in O(|L|3 + T|L|2) time
Note that since our method is highly modular  one can resort to any method to solve the optimization
subproblems that arise for each module. Moreover  as mentioned in Section 3  TRMF can also be
used with different regularization structures  making it highly adaptable.
4.1 Connections to Existing MF Approaches
TRMF-AR is a generalization of many existing MF approaches to handle data with temporal depen-
dencies. Speciﬁcally  Temporal Collaborative Filtering [23] corresponds to W (1) = Ik on {xt}. The
NMF method of [2] is an AR(L) model with W (l) = ↵l1(1  ↵)Ik  8l  where ↵ is pre-deﬁned.
The AR(1) model of [16  26] has W (1) = In on {F xt}. Finally the DLM [7] is a latent AR(1)
model with a general W (1)  which can be estimated by EM algorithms.
4.2 Connections to Learning Gaussian Markov Random Fields
The Gaussian Markov Random Field (GMRF) is a general way to model multivariate data with
dependencies. GMRF assumes that data are generated from a multivariate Gaussian distribution
with a covariance matrix ⌃ which describes the dependencies among T dimensional variables i.e. 
¯x ⇠N (0  ⌃). If the unknown ¯x is assumed to be generated from this model  The negative log
likelihood of the data can be written as ¯x>⌃1 ¯x  ignoring the constants and where ⌃1 is the inverse
covariance matrix of the Gaussian distribution. This prior can be incorporated into an empirical risk
minimization framework as a regularizer. Furthermore  it is known that if⌃1st = 0  xt and xs

are conditionally independent  given the other variables. In Theorem 1 we established connections

6

Table 2: Forecasting results: ND/ NRMSE for each approach. Lower values are better. “-” indicates
an unavailability due to scalability or an inability to handle missing values.

Forecasting with Full Observation

Matrix Factorization Models

TRMF-AR
0.373/ 0.487
0.255/ 1.397
0.187/ 0.423

SVD-AR(1)
0.444/ 0.872
0.257/ 1.865
0.555/ 1.194

TCF

1.000/ 1.424
0.349/ 1.838
0.624/ 0.931

AR(1)

0.928/ 1.401
0.219/ 1.439
0.275/ 0.536

Time Series Models
DLM

R-DLM

0.936/ 1.391
0.435/ 2.753
0.639/ 0.951

0.996/ 1.420

-/ -
-/ -

Mean

1.000/ 1.424
1.410/ 4.528
0.560/ 0.826

synthetic
electricity
trafﬁc

Forecasting with Missing Values

-/ -
-/ -

-/ -
-/ -

-/ -
-/ -

0.540/2.231
0.446/1.124

0.602/ 2.293
0.453/ 1.110

0.533/ 1.958
0.432/ 1.065

walmart-1
1.239/3.103
walmart-2
1.097/2.088
to graph based regularizers  and that such methods can be seen as regularizing with the inverse
covariance matrix for Gaussians [27]. We thus have the following result:
Corollary 1. For any lag set L  ¯w  and ⌘> 0  the inverse covariance matrix ⌃1
AR of the GMRF model
corresponding to the quadratic regularizer Rx( ¯x) := TAR( ¯x|L  ¯w ⌘ ) shares the same off-diagonal
non-zero pattern as GAR deﬁned in Theorem 1. Moreover  we have TAR( ¯x|L  ¯w ⌘ ) = ¯x>⌃1
AR ¯x.
A detailed proof is in Appendix C.2. As a result  our proposed AR-based regularizer is equivalent to
imposing a Gaussian prior on ¯x with a structured inverse covariance described by the matrix GAR
deﬁned in Theorem 1. Moreover  the step to learn W has a natural interpretation: the lag set L
imposes the non-zero pattern of the graphical model on the data  and then we solve a simple least
squares problem to learn the weights corresponding to the edges. As an application of Theorem 1
from [15] and Corollary 1  when Rf (F ) = kFk2
F X:Z=F XkFk2

F  we can relate TAR to a weighted nuclear norm:
F +Xr TAR( ¯xr |L  ¯w ⌘ ) 
(13)

AR = U SU> is the eigen-decomposition of ⌃1

where B = U S1/2 and ⌃1
AR. (13) enables us to apply
the results from [15] to obtain guarantees for the use of AR temporal regularizer when W is given. For
simplicity  we assume ¯wr = ¯w  8r and consider a relaxed convex formulation for (12) as follows:
(14)

kZBk⇤ =

1
2

inf

(Yij  Zij)2 + zkZBk⇤ 

ˆZ = arg min
Z2C

N

N

+ O(↵2/N ) 

where N = |⌦|  and C is a set of matrices with low spikiness. Full details are provided in Ap-
pendix C.3. As an application of Theorem 2 from [15]  we have the following corollary.
Corollary 2. Let Z? = F X be the ground truth n ⇥ T time series matrix of rank k. Let Y be
the matrix with N = |⌦| randomly observed entries corrupted with additive Gaussian noise with
variance 2. Then if z  C1q (n+T ) log(n+T )
  with high probability for the ˆZ obtained by (14) 
Z?  ˆZF  C2↵2 max(1  2)
k(n + T ) log(n + T )

where C1 C2 are positive constants  and ↵ depends on the product Z?B.
See Appendix C.3 for details. From the results in Table 3  we observe superior performance of
TRMF-AR over standard MF  indicating that ¯w learnt from our data-driven approach (12) does aid
in recovering the missing entries for time series. We would like to point out that establishing a
theoretical guarantee for TRMF with W is unknown remains a challenging research direction.
5 Experimental Results
We consider ﬁve datasets (Table 1). For synthetic  we ﬁrst
randomly generate F 2 R16⇥4 and generate {xt} follow-
ing an AR process with L = {1  8}. Then Y is obtained
by yt = F xt + ✏t where ✏t ⇠N (0  0.1I). The data sets
electricity and trafﬁc are obtained from the UCI reposi-
tory  while walmart-1 and walmart-2 are two propriety
datasets from Walmart E-commerce containing weekly
sale information. Due to reasons such as out-of-stock 
55.3% and 49.3% of entries are missing respectively. To
evaluate the prediction performance  we consider the nor-
malized deviation (ND) and normalized RMSE (NRMSE).
See details for the description for each dataset and the
formal deﬁnition for each criterion in Appendix A.

Figure 4: Scalability: T = 512. n 2
{500  1000  . . .   50000}. AR({1  . . .   8})
cannot ﬁnish in 1 day.

1

N X(i j)2⌦

7

|⌦|n⇥T
TRMF-AR
20% 0.467/ 0.661
30% 0.336/ 0.455
40% 0.231/ 0.306
50% 0.201/ 0.270
20% 0.245/ 2.395
30% 0.235/ 2.415
40% 0.231/ 2.429
50% 0.223/ 2.434
20% 0.190/ 0.427
30% 0.186/ 0.419
40% 0.185/ 0.416
50% 0.184/ 0.415

TCF

0.713/ 1.030
0.629/ 0.961
0.495/ 0.771
0.289/ 0.464
0.255/ 2.427
0.245/ 2.436
0.242/ 2.457
0.233/ 2.459
0.208/ 0.448
0.199/ 0.432
0.198/ 0.428
0.193/ 0.422

MF

0.688/ 1.064
0.595/ 0.926
0.374/ 0.548
0.317/ 0.477
0.362/ 2.903
0.355/ 2.766
0.348/ 2.697
0.319/ 2.623
0.310/ 0.604
0.299/ 0.581
0.292/ 0.568
0.251/ 0.510

Time Series Models
DLM
Mean

0.933/ 1.382
0.913/ 1.324
0.834/ 1.259
0.772/ 1.186
0.462/ 4.777
0.410/ 6.605
0.196/ 2.151
0.158/ 1.590
0.353/ 0.603
0.286/ 0.518
0.251/ 0.476
0.224/ 0.447

1.002/ 1.474
1.004/ 1.445
1.002/ 1.479
1.001/ 1.498
1.333/ 6.031
1.320/ 6.050
1.322/ 6.030
1.320/ 6.109
0.578/ 0.857
0.578/ 0.856
0.578/ 0.857
0.578/ 0.857

synthetic

electricity

trafﬁc

Table 3: Missing value imputation results: ND/ NRMSE for each approach. Note that TRMF
outperforms all competing methods in almost all cases.

Matrix Factorization Models

Methods/Implementations Compared:
• TRMF-AR: The proposed formulation (12) with Rw(W) = kWk2
F . For L  we use {1  2  . . .   8}
for synthetic  {1  . . .   24}[{7 ⇥ 24  . . .   8 ⇥ 24  1} for electricity and trafﬁc  and {1  . . .   10}[
{50  . . .   56} for walmart-1 and walmart-2 to capture seasonality.
• SVD-AR(1): The rank-k approximation of Y = U SV > is ﬁrst obtained by SVD. After setting
F = U S and X = V >  a k-dimensional AR(1) is learned on X for forecasting.
• TCF: Matrix factorization with the simple temporal regularizer proposed in [23].
• AR(1): n-dimensional AR(1) model.2
• DLM: two implementations: the widely used R-DLM package [12] and the code provided in [8].
• Mean: The baseline  which predicts everything to be the mean of the observed portion of Y .
For each method and data set  we perform a grid search over various parameters (such as k   values)
following a rolling validation approach described in [11].
Scalability: Figure 4 shows that traditional time-series approaches such as AR or DLM suffer
from the scalability issue for large n  while TRMF-AR scales much better with n. Speciﬁcally  for
n = 50  000  TRMF is 2 orders of magnitude faster than competing AR/DLM methods. Note that
the results for R-DLM are not available because the R package cannot scale beyond n in the tens
(See Appendix D for more details.). Furthermore  the dlmMLE routine in R-DLM uses a general
optimization solver  which is orders of magnitude slower than the implementation provided in [8].
5.1 Forecasting
Forecasting with Full Observations. We ﬁrst compare various methods on the task of forecasting
values in the test set  given fully observed training data. For synthetic  we consider one-point ahead
forecasting task and use the last ten time points as the test periods. For electricity and trafﬁc  we
consider the 24-hour ahead forecasting task and use last seven days as the test periods. From Table 2 
we can see that TRMF-AR outperforms all the other methods on both metrics considered.
Forecasting with Missing Values. We next compare the methods on the task of forecasting in the
presence of missing values in the data. We use the Walmart datasets here  and consider 6-week ahead
forecasting and use last 54 weeks as the test periods. Note that SVD-AR(1) and AR(1) cannot handle
missing values. The second part of Table 2 shows that we again outperform other methods.
5.2 Missing Value Imputation
We next consider the case of imputing missing values in the data. As in [9]  we assume that blocks of
data are missing  corresponding to sensor malfunctions for example  over a length of time. To create
data with missing entries  we ﬁrst ﬁxed the percentage of data that we were interested in observing 
and then uniformly at random occluded blocks of a predetermined length (2 for synthetic data and
5 for the real datasets). The goal was to predict the occluded values. Table 3 shows that TRMF
outperforms the methods we compared to on almost all cases.
6 Conclusions
We propose a novel temporal regularized matrix factorization framework (TRMF) for high-
dimensional time series problems with missing values. TRMF not only models temporal dependency
among the data points  but also supports data-driven dependency learning. TRMF generalizes sev-
eral well-known methods and yields superior performance when compared to other state-of-the-art
methods on real-world datasets.
Acknowledgements: This research was supported by NSF grants (CCF-1320746  IIS-1546459  and CCF-
1564000) and gifts from Walmart Labs and Adobe. We thank Abhay Jha for the help on Walmart experiments.

2In Appendix A  we also show a baseline which applies an independent AR model to each dimension.

8

References
[1] O. Anava  E. Hazan  and A. Zeevi. Online time series prediction with missing data. In Proceedings of the

International Conference on Machine Learning  pages 2191–2199  2015.

[2] Z. Chen and A. Cichocki. Nonnegative matrix factorization with temporal smoothness and/or spatial
decorrelation constraints. Laboratory for Advanced Brain Signal Processing  RIKEN  Tech. Rep  68  2005.
[3] Z. Ghahramani and G. E. Hinton. Parameter estimation for linear dynamical systems. Technical report 

Technical Report CRG-TR-96-2  University of Totronto  Dept. of Computer Science  1996.

[4] R. L. Graham  D. E. Knuth  and O. Patashnik. Concrete Mathematics: A Foundation for Computer Science.

Addison-Wesley Longman Publishing Co.  Inc.  2nd edition  1994.

[5] F. Han and H. Liu. Transition matrix estimation in high dimensional time series. In Proceedings of the

International Conference on Machine Learning  pages 172–180  2013.

[6] P. Jain and I. S. Dhillon. Provable inductive matrix completion. arXiv preprint arXiv:1306.0626  2013.
[7] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Journal of Fluids Engineering 

82(1):35–45  1960.

[8] L. Li and B. A. Prakash. Time series clustering: Complex is simpler! In Proceedings of the International

Conference on Machine Learning  pages 185–192  2011.

[9] L. Li  J. McCann  N. S. Pollard  and C. Faloutsos. DynaMMo: Mining and summarization of coevolving
sequences with missing values. In ACM SIGKDD International Conference on Knowledge discovery and
data mining  pages 507–516. ACM  2009.

[10] I. Melnyk and A. Banerjee. Estimating structured vector autoregressive model. In Proceedings of the

Thirty Third International Conference on Machine Learning (ICML)  2016.

[11] W. B. Nicholson  D. S. Matteson  and J. Bien. Structured regularization for large vector autoregressions.

Technical report  Technical Report  University of Cornell  2014.

[12] G. Petris. An r package for dynamic linear models. Journal of Statistical Software  36(12):1–16  2010.
[13] G. Petris  S. Petrone  and P. Campagnoli. Dynamic Linear Models with R. Use R! Springer  2009.
[14] S. Rallapalli  L. Qiu  Y. Zhang  and Y.-C. Chen. Exploiting temporal stability and low-rank structure
for localization in mobile networks. In International Conference on Mobile Computing and Networking 
MobiCom ’10  pages 161–172. ACM  2010.

[15] N. Rao  H.-F. Yu  P. K. Ravikumar  and I. S. Dhillon. Collaborative ﬁltering with graph information:

Consistency and scalable methods. In Advances in Neural Information Processing Systems 27  2015.

[16] M. Roughan  Y. Zhang  W. Willinger  and L. Qiu. Spatio-temporal compressive sensing and internet trafﬁc

matrices (extended version). IEEE/ACM Transactions on Networking  20(3):662–676  June 2012.

[17] R. H. Shumway and D. S. Stoffer. An approach to time series smoothing and forecasting using the EM

algorithm. Journal of time series analysis  3(4):253–264  1982.

[18] A. J. Smola and R. Kondor. Kernels and regularization on graphs. In Learning theory and kernel machines 

pages 144–158. Springer  2003.

[19] J. Z. Sun  K. R. Varshney  and K. Subbian. Dynamic matrix factorization: A state space approach. In
Proceedings of International Conference on Acoustics  Speech and Signal Processing  pages 1897–1900.
IEEE  2012.

[20] H. Wang  G. Li  and C.-L. Tsai. Regression coefﬁcient and autoregressive order shrinkage and selection via
the lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  69(1):63–78  2007.
[21] M. West and J. Harrison. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer 

2013.

[22] K. Wilson  B. Raj  and P. Smaragdis. Regularized non-negative matrix factorization with temporal

dependencies for speech denoising. In Interspeech  pages 411–414  2008.

[23] L. Xiong  X. Chen  T.-K. Huang  J. G. Schneider  and J. G. Carbonell. Temporal collaborative ﬁltering
with Bayesian probabilistic tensor factorization. In SIAM International Conference on Data Mining  pages
223–234  2010.

[24] M. Xu  R. Jin  and Z.-H. Zhou. Speedup matrix completion with side information: Application to multi-
label learning. In C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Weinberger  editors  Advances
in Neural Information Processing Systems 26  pages 2301–2309  2013.

[25] H.-F. Yu  P. Jain  P. Kar  and I. S. Dhillon. Large-scale multi-label learning with missing labels. In

Proceedings of the International Conference on Machine Learning  pages 593–601  2014.

[26] Y. Zhang  M. Roughan  W. Willinger  and L. Qiu. Spatio-temporal compressive sensing and internet trafﬁc

matrices. SIGCOMM Comput. Commun. Rev.  39(4):267–278  Aug. 2009. ISSN 0146-4833.

[27] T. Zhou  H. Shan  A. Banerjee  and G. Sapiro. Kernelized probabilistic matrix factorization: Exploiting

graphs and side information. In SDM  volume 12  pages 403–414. SIAM  2012.

9

,Hsiang-Fu Yu
Nikhil Rao
Inderjit Dhillon
YAN ZHENG
Zhaopeng Meng
Jianye Hao
Zongzhang Zhang
Tianpei Yang
Changjie Fan
Chris Wendler
Markus Püschel
Dan Alistarh