2018,Legendre Decomposition for Tensors,We present a novel nonnegative tensor decomposition method  called Legendre decomposition  which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry  the reconstructed tensor is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods.,Legendre Decomposition for Tensors

Mahito Sugiyama

National Institute of Informatics

JST  PRESTO

Hiroyuki Nakahara

RIKEN Center for Brain Science

hiro@brain.riken.jp

Koji Tsuda

The University of Tokyo

NIMS; RIKEN AIP

mahito@nii.ac.jp

tsuda@k.u-tokyo.ac.jp

Abstract

We present a novel nonnegative tensor decomposition method  called Legendre
decomposition  which factorizes an input tensor into a multiplicative combination
of parameters. Thanks to the well-developed theory of information geometry  the
reconstructed tensor is unique and always minimizes the KL divergence from an in-
put tensor. We empirically show that Legendre decomposition can more accurately
reconstruct tensors than other nonnegative tensor decomposition methods.

1

Introduction

Matrix and tensor decomposition is a fundamental technique in machine learning; it is used to analyze
data represented in the form of multi-dimensional arrays  and is used in a wide range of applications
such as computer vision (Vasilescu and Terzopoulos  2002  2007)  recommender systems (Symeoni-
dis  2016)  signal processing (Cichocki et al.  2015)  and neuroscience (Beckmann and Smith  2005).
The current standard approaches include nonnegative matrix factorization (NMF; Lee and Seung 
1999  2001) for matrices and CANDECOMP/PARAFAC (CP) decomposition (Harshman  1970) or
Tucker decomposition (Tucker  1966) for tensors. CP decomposition compresses an input tensor
into a sum of rank-one components  and Tucker decomposition approximates an input tensor by a
core tensor multiplied by matrices. To date  matrix and tensor decomposition has been extensively
analyzed  and there are a number of variations of such decomposition (Kolda and Bader  2009) 
where the common goal is to approximate a given tensor by a smaller number of components  or
parameters  in an eﬃcient manner.
However  despite the recent advances of decomposition techniques  a learning theory that can
systematically deﬁne decomposition for any order tensors including vectors and matrices is still
under development. Moreover  it is well known that CP and Tucker tensor decomposition include
non-convex optimization and that the global convergence is not guaranteed. Although there are a
number of extensions to transform the problem into a convex problem (Liu et al.  2013; Tomioka and
Suzuki  2013)  one needs additional assumptions on data  such as a bounded variance.
Here we present a new paradigm of matrix and tensor decomposition  called Legendre decomposition 
based on information geometry (Amari  2016) to solve the above open problems of matrix and tensor
decomposition.
In our formulation  an arbitrary order tensor is treated as a discrete probability
distribution in a statistical manifold as long as it is nonnegative  and Legendre decomposition is
realized as a projection of the input tensor onto a submanifold composed of reconstructable tensors.
The key to introducing the formulation is to use the partial order (Davey and Priestley  2002; Gierz
et al.  2003) of indices  which allows us to treat any order tensors as a probability distribution in the
information geometric framework.
Legendre decomposition has the following remarkable properties: It always ﬁnds the unique tensor
that minimizes the Kullback–Leibler (KL) divergence from an input tensor. This is because Legendre
decomposition is formulated as convex optimization  and hence we can directly use gradient descent 
which always guarantees the global convergence  and the optimization can be further speeded up by

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: (a) Overview of Legendre decomposition. (b) Illustration of (cid:18) and (cid:17) for second-order
tensor (matrix) when B = [I1] (cid:2) [I2].

using a natural gradient (Amari  1998) as demonstrated in our experiments. Moreover  Legendre
decomposition is ﬂexible as it can decompose sparse tensors by removing arbitrary entries beforehand 
for examples zeros or missing entries.
Furthermore  our formulation has a close relationship with statistical models  and can be interpreted
as an extension of the learning of Boltzmann machines (Ackley et al.  1985). This interpretation
gives new insight into the relationship between tensor decomposition and graphical models (Chen
et al.  2018; Yılmaz et al.  2011; Yılmaz and Cemgil  2012) as well as the relationship between tensor
decomposition and energy-based models (LeCun et al.  2007). In addition  we show that the proposed
formulation belongs to the exponential family  where the parameter (cid:18) used in our decomposition is
the natural parameter  and (cid:17)  used to obtain the gradient of (cid:18)  is the expectation of the exponential
family.
The remainder of this paper is organized as follows. We introduce Legendre decomposition in
Section 2. We deﬁne the decomposition in Section 2.1  formulate it as convex optimization in
Section 2.2  describe algorithms in Section 2.3  and discuss the relationship with other statistical
models in Section 2.4. We empirically examine performance of our method in Section 3  and
summarize our contribution in Section 4.

2 The Legendre Decomposition

(cid:21)0

We introduce Legendre decomposition for tensors. We begin with a nonnegative Nth-order tensor
X 2 RI1(cid:2)I2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)IN
. To simplify the notation  we write the entry xi1i2:::iN by xv with the index
vector v = (i1; i2; : : : ; iN ) 2 [I1] (cid:2) [I2] (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) [IN ]  where each [Ik] = f1; 2; : : : ; Ikg. To treat X
as a discrete probability mass function in our formulation  we normalize X by dividing each entry by
the sum of all entries  yielding P = X =
v xv. In the following  we always work with a normalized
tensor P.

∑

2.1 Deﬁnition
Legendre decomposition always ﬁnds the best approximation of a given tensor P. Our strategy is
to choose an index set B (cid:18) [I1] (cid:2) [I2] (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) [IN ] as a decomposition basis  where we assume
(1; 1; : : : ; 1) ̸2 B for a technical reason  and approximate the normalized tensor P by a multiplicative
combination of parameters associated with B.
First we introduce the relation “(cid:20)” between index vectors u = (j1; : : : ; jN ) and v = (i1; : : : ; iN ) as
u (cid:20) v if j1 (cid:20) i1  j2 (cid:20) i2  : : :   jN (cid:20) iN. It is clear that this relation gives a partial order (Gierz et al. 
2003); that is  (cid:20) satisﬁes the following three properties for all u; v; w: (1) v (cid:20) v (reﬂexivity)  (2)
u (cid:20) v  v (cid:20) u ) u = v (antisymmetry)  and (3) u (cid:20) v  v (cid:20) w ) u (cid:20) w (transitivity). Each tensor
is treated as a discrete probability mass function with the sample space Ω (cid:18) [I1](cid:2)(cid:1)(cid:1)(cid:1)(cid:2)[IN ]. While it
is natural to set Ω = [I1](cid:2)(cid:1)(cid:1)(cid:1)(cid:2)[IN ]  our formulation allows us to use any subset Ω (cid:18) [I1](cid:2)(cid:1)(cid:1)(cid:1)(cid:2)[IN ].
Hence  for example  we can remove unnecessary indices such as missing or zero entries of an input
tensor P from Ω. We deﬁne Ω+ = Ω n f(1; 1; : : : ; 1)g.

2

ηv = ࢣ qu89࢑v࢏vuࢠ࢏vInput tensor Decomposable tensor Reconstruction(one-to-one)KL divergenceis minimizedParameters (θv)vࢠBLegendredecompositionabqv = Ü exp(θu)uࢠ࢑vexp(ψ(θ))1E = (8  9)(cid:21)0

We deﬁne a tensor Q 2 RI1(cid:2)I2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)IN
as fully decomposable with B (cid:18) Ω+ if each entry qv 2 Ω is
∏
represented in the form of

(1)
using jBj parameters ((cid:18)v)v2B with (cid:18)v 2 R and the normalizer ((cid:18)) 2 R  which is always uniquely
determined from the parameters ((cid:18)v)v2B as

#v = f u 2 B j u (cid:20) v g ;
∏

exp( ((cid:18)))

∑

exp((cid:18)u);

u2#v

qv =

1

 ((cid:18)) = log

exp((cid:18)u):

v2Ω

u2#v

(cid:21)0

(cid:21)0

∑

This normalization does not have any eﬀect on the decomposition performance  but rather it is
needed to formulate our decomposition as an information geometric projection  as shown in the next
subsection. There are two extreme cases for a choice of a basis B: If B = ∅  a fully decomposable
Q is always uniform; that is  qv = 1=jΩj for all v 2 Ω. In contrast  if B = Ω+  any input P itself
becomes decomposable.
We now deﬁne Legendre decomposition as follows: Given an input tensor P 2 RI1(cid:2)I2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)IN
  the
sample space Ω (cid:18) [I1](cid:2) [I2](cid:2)(cid:1)(cid:1)(cid:1)(cid:2) [IN ]  and a parameter basis B (cid:18) Ω+  Legendre decomposition
ﬁnds the fully decomposable tensor Q 2 RI1(cid:2)I2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)IN
with a B that minimizes the Kullback–
Leibler (KL) divergence DKL(P;Q) =
v2Ω pv log(pv=qv) (Figure 1[a]). In the next subsection 
we introduce an additional parameter ((cid:17)v)v2B and show that this decomposition is always possible
via the dual parameters ( ((cid:18)v)v2B; ((cid:17)v)v2B ) with information geometric analysis. Since (cid:18) and (cid:17) are
connected via Legendre transformation  we call our method Legendre decomposition.
Legendre decomposition for second-order tensors (that is  matrices) can be viewed as a low rank
approximation not of an input matrix P but of its entry-wise logarithm logP. This is why the rank
∑
of logQ with the fully decomposable matrix Q coincides with the parameter matrix T 2 RI1(cid:2)I2
such that tv = (cid:18)v if v 2 B  t(1;1) = 1= exp( ((cid:18)))  and tv = 0 otherwise. Thus we ﬁll zeros into
entries in Ω+ n B. Then we have log qv =
u2#v tu  meaning that the rank of logQ coincides with
the rank of T . Therefore if we use a decomposition basis B that includes only l rows (or columns) 
rank(logQ) (cid:20) l always holds.
2.2 Optimization
We solve the Legendre decomposition by formulating it as a convex optimization problem. Let us
assume that B = Ω+ = Ω n f(1; 1; : : : ; 1)g  which means that any tensor is fully decomposable.
Our deﬁnition in Equation (1) can be re-written as

{

∑

∑

if u (cid:20) v;
otherwise

1
0

(cid:16)(u; v)(cid:18)u (cid:0) ((cid:18)) =

∑

u2Ω

u2Ω+

(cid:16)(u; v) =

(cid:16)(u; v)(cid:18)u;

log qv =

(2)
with (cid:0) ((cid:18)) = (cid:18)(1;1;:::;1)  and the sample space Ω is a poset (partially ordered set) with respect to
the partial order “(cid:20)” with the least element ? = (1; 1; : : : ; 1). Therefore our model belongs to
the log-linear model on posets introduced by Sugiyama et al. (2016  2017)  which is an extension
of the information geometric hierarchical log-linear model (Amari  2001; Nakahara and Amari 
2002). Each entry qv and the parameters ((cid:18)v)v2Ω+ in Equation (2) directly correspond to those in
Equation (8) in Sugiyama et al. (2017). According to Theorem 2 in Sugiyama et al. (2017)  if we
introduce ((cid:17)v)v2Ω+ such that

"v = f u 2 Ω j u (cid:21) v g ;

u2Ω

qu =

(cid:17)v =

u2"v

(cid:16)(v; u)qu;

(3)
∑
for each v 2 Ω+ (see Figure 1[b])  the pair ( ((cid:18)v)v2Ω+; ((cid:17)v)v2Ω+ ) is always a dual coordinate
system of the set of normalized tensors S = fP j 0 < pv < 1 and
v2Ω pv = 1g with respect to
the sample space Ω  as they are connected via Legendre transformation. Hence S becomes a dually
ﬂat manifold (Amari  2009).
}
Here we formulate Legendre decomposition as a projection of a tensor onto a submanifold. Suppose
that B (cid:18) Ω+ and let S B be the submanifold such that

{ Q 2 S(cid:12)(cid:12) (cid:18)v = 0 for all v 2 Ω+ n B

S B =

;

∑

3

Figure 2: Projection in statistical manifold.

which is the set of fully decomposable tensors with B and is an e-ﬂat submanifold as it has constraints
on the (cid:18) coordinate (Amari  2016  Chapter 2.4). Furthermore  we introduce another submanifold
SP for a tensor P 2 S and A (cid:18) Ω+ such that

SP = f Q 2 S j (cid:17)v = ^(cid:17)v for all v 2 A g ;

where ^(cid:17)v is given by Equation (3) by replacing qu with pu  which is an m-ﬂat submanifold as it has
constraints on the (cid:17) coordinate.
The dually ﬂat structure of S with the dual coordinate systems ( ((cid:18)v)v2Ω+; ((cid:17)v)v2Ω+ ) leads to the
following strong property: If A = B  that is  (Ω+ n B) [ A = Ω+ and (Ω+ n B) \ A = ∅  the
intersection S B \ SP is always a singleton; that is  the tensor Q such that Q 2 S B \ SP always
uniquely exists  and Q is the minimizer of the KL divergence from P (Amari  2009  Theorem 3):

Q = argmin
R2SB

DKL(P;R):

∑

∑

(4)
The transition from P to Q is called the m-projection of P onto S B  and Legendre decomposition
coincides with m-projection (Figure 2). In contrast  if some fully decomposable tensor R 2 S B is
given  ﬁnding the intersection Q 2 S B \ SP is called the e-projection of R onto SP. In practice 
we use e-projection because the number of parameters to be optimized is jBj in e-projection while
it is jΩ n Bj in m-projection  and jBj (cid:20) jΩ n Bj usually holds.
The e-projection is always convex optimization as the e-ﬂat submanifold S B is convex with respect
to ((cid:18)v)v2Ω+. More precisely 

pv log pv
qv

DKL(P;Q) =
pv log qv = (cid:0)
v2Ω
∑
∑
where H(P) is the entropy of P and independent of ((cid:18)v)v2Ω+. Since we have
(cid:0)

(∑

∑

v2Ω

v2Ω

v2Ω

=

∑

pv log pv (cid:0)

∑
)
(cid:16)(u; v)((cid:0)(cid:18)u) + ((cid:18))

pv

exp

v2Ω

v2Ω

u2B

(cid:16)(u; v)(cid:18)(u)

pv log qv =

; ((cid:18)) = log
v2Ω
 ((cid:18)) is convex and DKL(P;Q) is also convex with respect to ((cid:18)v)v2Ω+.
2.3 Algorithm
Here we present our two gradient-based optimization algorithms to solve the KL divergence mini-
mization problem in Equation (4). Since the KL divergence DKL(P;Q) is convex with respect to
each (cid:18)v  the standard gradient descent shown in Algorithm 1 can always ﬁnd the global optimum 
where " > 0 is a learning rate. Starting with some initial parameter set ((cid:18)v)v2B  the algorithm
iteratively updates the set until convergence. The gradient of (cid:18)w for each w 2 B is obtained as

u2B

;

pv log qv (cid:0) H(P);

(∑

)

∑

(cid:16)(u; v)(cid:18)u +

@
@(cid:18)w

pv ((cid:18))

v2Ω

∑

v2Ω

∑

∑

u2B

@
@(cid:18)w

DKL(P;Q) = (cid:0) @
∑
@(cid:18)w

= (cid:0)

v2Ω

pv log qv = (cid:0) @
@(cid:18)w
= (cid:17)w (cid:0) ^(cid:17)w;

v2Ω

pv

@ ((cid:18))
@(cid:18)w

pv(cid:16)(w; v) +

4

B(fully decomposable tensors)(tensors with the same ηfor basis B)(input tensor)(solution)m-projectione-projectionˆÓInitialize ((cid:18)v)v2B;
repeat

Algorithm 1: Legendre decomposition by gradient descent
1 GradientDescent(P  B)
2
3
4
5
6
7

Compute Q using the current parameter ((cid:18)v)v2B;
Compute ((cid:17)v)v2B from Q;
(cid:18)v (cid:18)v (cid:0) "((cid:17)v (cid:0) ^(cid:17)v);
until convergence of ((cid:18)v)v2B;

foreach v 2 B do

8

Initialize ((cid:18)v)v2B;
repeat

Algorithm 2: Legendre decomposition by natural gradient
1 NaturalGradient(P  B)
2
3
4
5
6
7
8

Compute Q using the current parameter ((cid:18)v)v2B;
Compute ((cid:17)v)v2B from Q and ∆(cid:17) (cid:17) (cid:0) ^(cid:17);
Compute the inverse G
(cid:18) (cid:18) (cid:0) G

until convergence of ((cid:18)v)v2B;

(cid:0)1∆(cid:17)

// e.g. (cid:18)v = 0 for all v 2 B

// e.g. (cid:18)v = 0 for all v 2 B

(cid:0)1 of the Fisher information matrix G using Equation (5);

where the last equation uses the fact that @ ((cid:18))=@(cid:18)w = (cid:17)w in Theorem 2 in Sugiyama et al. (2017).
This equation also shows that the KL divergence DKL(P;Q) is minimized if and only if (cid:17)v = ^(cid:17)v for
all v 2 B. The time complexity of each iteration is O(jΩjjBj)  as that of computing Q from ((cid:18)v)v2B
(line 5 in Algorithm 1) is O(jΩjjBj) and computing ((cid:17)v)v2B from Q (line 6 in Algorithm 1) is
O(jΩj). Thus the total complexity is O(hjΩjjBj2) with the number of iterations h until convergence.
Although gradient descent is an eﬃcient approach  in Legendre decomposition  we need to repeat
“decoding” from ((cid:18)v)v2B and “encoding” to ((cid:17)v)v2B in each iteration  which may lead to a loss of
eﬃciency if the number of iterations is large. To reduce the number of iterations to gain eﬃciency 
we propose to use a natural gradient (Amari  1998)  which is the second-order optimization method 
shown in Algorithm 2. Again  since the KL divergence DKL(P;Q) is convex with respect to
((cid:18)v)v2B  a natural gradient can always ﬁnd the global optimum. More precisely  our natural gradient
algorithm is an instance of the Bregman algorithm applied to a convex region  which is well known
to always converge to the global solution (Censor and Lent  1981). Let B = fv1; v2; : : : ; vjBjg 
(cid:18) = ((cid:18)v1; : : : ; (cid:18)vjBj )T   and (cid:17) = ((cid:17)v1; : : : ; (cid:17)vjBj)T . In each update of the current (cid:18) to (cid:18)next  the natural
gradient method uses the relationship 

∆(cid:17) = (cid:0)G∆(cid:18); ∆(cid:17) = (cid:17) (cid:0) ^(cid:17) and ∆(cid:18) = (cid:18)next (cid:0) (cid:18);

which leads to the update formula

(cid:18)next = (cid:18) (cid:0) G
]

[

(cid:0)1∆(cid:17);

∑

=

w2Ω

where G = (guv) 2 RjBj(cid:2)jBj is the Fisher information matrix such that

guv((cid:18)) =

@(cid:17)u
@(cid:18)v

= E

@ log pw

@ log pw

@(cid:18)u

@(cid:18)v

(cid:16)(u; w)(cid:16)(v; w)pw (cid:0) (cid:17)u(cid:17)v

(5)

as given in Theorem 3 in Sugiyama et al. (2017). Note that natural gradient coincides with Newton’s
method in our case as the Fisher information matrix G corresponds to the (negative) Hessian matrix:

@2

@(cid:18)u@(cid:18)v

DKL(P;Q) = (cid:0) @(cid:17)u
@(cid:18)v

= (cid:0)guv:

The time complexity of each iteration is O(jΩjjBj + jBj3)  where O(jΩjjBj) is needed to compute
Q from (cid:18) and O(jBj3) to compute the inverse of G  resulting in the total complexity O(h
′jΩjjBj +
′jBj3) with the number of iterations h
h

′ until convergence.

5

2.4 Relationship to Statistical Models
We demonstrate interesting relationships between Legendre decomposition and statistical models 
including the exponential family and the Boltzmann (Gibbs) distributions  and show that our decom-
position method can be viewed as a generalization of Boltzmann machine learning (Ackley et al. 
1985). Although the connection between tensor decomposition and graphical models has been ana-
lyzed by Chen et al. (2018); Yılmaz et al. (2011); Yılmaz and Cemgil (2012)  our analysis adds a new
insight as we focus on not the graphical model itself but the sample space of distributions generated
by the model.

2.4.1 Exponential family
We show that the set of normalized tensors S = fP 2 RI1(cid:2)I2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)IN
in the exponential family. The exponential family is deﬁned as

>0

j∑
)
(cid:18)iki(x) + r(x) (cid:0) C((cid:18))

(∑

p(x; (cid:18)) = exp

for natural parameters (cid:18). Since our model in Equation (1) can be written as

pv =

1

exp( ((cid:18)))

exp((cid:18)u) = exp

(cid:18)u(cid:16)(u; v) (cid:0) ((cid:18))

(∑

u2Ω+

)

v2Ω pv = 1g is included

∏

u2#v

[

with (cid:18)u = 0 for u 2 Ω+ n B  it is clearly in the exponential family  where (cid:16) and ((cid:18)) correspond
to k and C((cid:18))  respectively  and r(x) = 0. Thus  the ((cid:18)v)v2B used in Legendre decomposition are
interpreted as natural parameters of the exponential family. Moreover  we can obtain ((cid:17)v)v2B by
taking the expectation of (cid:16)(u; v):

]

∑

v2Ω

E

(cid:16)(u; v)

=

(cid:16)(u; v)pv = (cid:17)u:

Thus Legendre decomposition of P is understood to ﬁnd a fully decomposable Q that has the same
expectation with P with respect to a basis B.
2.4.2 Boltzmann Machines
A Boltzmann machine is represented as an undirected graph G = (V; E) with a vertex set V and an
edge set E (cid:18) V (cid:2) V   where we assume that V = [N ] = f1; 2; : : : ; Ng without loss of generality.
This V is the set of indices of N binary variables. A Boltzmann machine G deﬁnes a probability
distribution P   where each probability of an N-dimensional binary vector x 2 f0; 1gN is given as

∏

(

(

)∏

fi;jg2E

p(x) =

1

Z((cid:18))

i2V

exp

(cid:18)ixi

exp

(cid:18)ijxixj

;

where (cid:18)i is a bias  (cid:18)ij is a weight  and Z((cid:18)) is a partition function.
To translate a Boltzmann machine into our formulation  let Ω = f1; 2gN and suppose that

{
B(V ) = f (ia

B(E) =

1; : : : ; ia

(iab

1 ; : : : ; iab

N ) 2 Ω j a 2 V g ;
N ) 2 Ω

(cid:12)(cid:12) fa; bg 2 E

}

;

ia
l =

iab
l =

2
1

2
1

if l = a;
otherwise;
if l 2 fa; bg;
otherwise:

Then it is clear that the set of probability distributions  or Gibbs distributions  that can be represented
by the Boltzmann machine G is exactly the same as S B with B = B(V ) [ B(E) and exp( ((cid:18))) =
Z((cid:18)); that is  the set of fully decomposable Nth-order tensors deﬁned by Equation (1) with the
basis B(V ) [ B(E) (Figure 3). Moreover  let a given Nth-order tensor P 2 R2(cid:2)2(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)2
be
an empirical distribution estimated from data  where each pv is the probability for a binary vector
v(cid:0)(1; : : : ; 1) 2 f0; 1gN. The tensorQ obtained by Legendre decomposition with B = B(V )[B(E)
coincides with the distribution learned by the Boltzmann machine G = (V; E). The condition
(cid:17)v = ^(cid:17)v in the optimization of the Legendre decomposition corresponds to the well-known learning

(cid:21)0

6

)

{
{

Figure 3: Boltzmann machine with V = f1; 2; 3g and E = ff1; 2g;f2; 3gg (left) and its sample
space (center)  which corresponds to a tensor (right). Grayed circles are the domain of parameters (cid:18).

equation of Boltzmann machines  where ^(cid:17) and (cid:17) correspond to the expectation of the data distribution
and that of the model distribution  respectively.
Therefore our Legendre decomposition is a generalization of Boltzmann machine learning in the
following three aspects:
1. The domain is not limited to binary but can be ordinal; that is  f0; 1gN is extended to [I1](cid:2) [I2](cid:2)
(cid:1)(cid:1)(cid:1) (cid:2) [IN ] for any I1; I2; : : : ; IN 2 N.
2. The basis B with which parameters (cid:18) are associated is not limited to B(V ) [ B(E) but can be
any subset of [I1] (cid:2) (cid:1)(cid:1)(cid:1) (cid:2) [IN ]  meaning that higher-order interactions (Sejnowski  1986) can be
included.
3. The sample space of probability distributions is not limited to f0; 1gN but can be any subset of
[I1](cid:2)[I2](cid:2)(cid:1)(cid:1)(cid:1)(cid:2)[IN ]  which enables us to perform eﬃcient computation by removing unnecessary
entries such as missing values.

Hidden variables are often used in Boltzmann machines to increase the representation power  such
as in restricted Boltzmann machines (RBMs; Smolensky  1986; Hinton  2002) and deep Boltzmann
machines (DBMs; Salakhutdinov and Hinton  2009  2012). In Legendre decomposition  including
a hidden variable corresponds to including an additional dimension. Hence if we include H hidden
variables  the fully decomposable tensor Q has the order of N + H. This is an interesting extension
to our method and an ongoing research topic  but it is not a focus of this paper since our main aim is
to ﬁnd a lower dimensional representation of a given tensor P.
In the learning process of Boltzmann machines  approximation techniques of the partition function
Z((cid:18)) are usually required  such as annealed importance sampling (AIS; Salakhutdinov and Murray 
2008) or variational techniques (Salakhutdinov  2008). This requirement is because the exact
computation of the partition function requires the summation over all probabilities of the sample
space Ω  which is always ﬁxed to 2V with the set V of variables in the learning process of Boltzmann
machines  and which is not tractable. Our method does not require such techniques as Ω is a subset
of indices of an input tensor and the partition function can always be directly computed.

3 Experiments

We empirically examine the eﬃciency and the eﬀectiveness of Legendre decomposition using syn-
thetic and real-world datasets. We used Amazon Linux AMI release 2018.03 and ran all experiments
on 2.3 GHz Intel Xeon CPU E5-2686 v4 with 256 GB of memory. The Legendre decomposition
was implemented in C++ and compiled with icpc 18.0.0 1.
Throughout the experiments  we focused on the decomposition of third-order tensors and used
three types of decomposition bases in the form of B1 = fv j i1 = i2 = 1g [ fv j i2 = i3 =
1g [ fv j i1 = i3 = 1g  B2(l) = fv j i1 = 1; i2 2 C2(l)g [ fv j i1 2 C1(l); i2 = 1g with
Ck(l) = fc⌊Ik=l⌋ j c 2 [l]g  and B3(l) = fv j (i1; i2) 2 Hi3(l)g with Hi3 (l) being the set indices
for the top l elements of the i3th frontal slice in terms of probability.
In these bases  B1 works
as a normalizer for each mode  B2 works as a normalizer for rows and columns of each slice  and
B3 highlights entries with high probabilities. We always assume that (1; : : : ; 1) is not included in

1Implementation is available at: https://github.com/mahito-sugiyama/Legendre-decomposition

7

(0  0  0)(1  0  0)(0  0  1)(1  1  0)(0  1  0)(1  0  1)(0  1  1)(1  1  1)(1  1  1)(2  1  1)(1  2  1)(1  2  2)(1  1  2)(2  2  1)(2  2  2)(2  1  2)Boltzmann machineHasse diagram of sample spaceTensor(2  1  2)(0  1  0)(1  0  1)(1  1  1)123Figure 4: Experimental results on synthetic data. (a  b) Comparison of natural gradient (Algorithm 2)
and gradient descent (Algorithm 1)  where both algorithms produce exactly the same results. (c)
Comparison of Legendre decomposition (natural gradient) and other tensor decomposition methods.

the above bases. The cardinality of a basis corresponds to the number of parameters used in the
decomposition. We used l to vary the number of parameters for decomposition in our experiments.
To examine the eﬃciency and the eﬀectiveness of tensor decomposition  we compared Legendre
decomposition with two standard nonnegative tensor decomposition techniques  nonnegative Tucker
decomposition (Kim and Choi  2007) and nonnegative CANDECOMP/PARAFAC (CP) decomposi-
tion (Shashua and Hazan  2005). Since both of these methods are based on least square objective
functions (Lee and Seung  1999)  we also included a variant of CP decomposition  CP-Alternating
Poisson Regression (CP-APR; Chi and Kolda  2012)  which uses the KL-divergence for its objective
function. We used the TensorLy implementation (Kossaiﬁ et al.  2016) for the nonnegative Tucker
and CP decompositions and the tensor toolbox (Bader et al.  2017; Bader and Kolda  2007) for
CP-APR. In the nonnegative Tucker decomposition  we always employed rank-(m; m; m) Tucker
decomposition with the single number m  and we use rank-n decomposition in the nonnegative CP
decomposition and CP-APR. Thus rank-(m; m; m) Tucker decomposition has (I1 + I2 + I3)m + m3
parameters  and rank-n CP decomposition and CP-APR have (I1 + I2 + I3)n parameters.

Results on Synthetic Data First we compared our two algorithms  the gradient descent in Algo-
rithm 1 and the natural gradient in Algorithm 2  to evaluate the eﬃciency of these optimization
algorithms. We randomly generated a third-order tensor with the size 20(cid:2) 20(cid:2) 20 from the uniform
distribution and obtained the running time and the number of iterations. We set B = B3(l) and
varied the number of parameters jBj with increasing l. In Algorithm 2  we used the outer loop (from
line 3 to 8) as one iteration for fair comparison and ﬁxed the learning rate " = 0:1.
Results are plotted in Figure 4(a  b) that clearly show that the natural gradient is dramatically faster
than gradient descent. When the number of parameters is around 400  the natural gradient is more
than six orders of magnitude faster than gradient descent. The increased speed comes from the
reduction of iterations. The natural gradient requires only two or three iterations until convergence
in all cases  while gradient descent requires more than 105 iterations to get the same result. In the
following  we consistently use the natural gradient for Legendre decomposition.
Next we examined the scalability compared to other tensor decomposition methods. We used the
same synthetic datasets and increased the tensor size from 20(cid:2) 20(cid:2) 20 to 500(cid:2) 500(cid:2) 500. Results
are plotted in Figure 4(c). Legendre decomposition is slower than Tucker and CP decompositions
if the tensors get larger  while the plots show that the running time of Legendre decomposition is
linear with the tensor size (Figure 5). Moreover  Legendre decomposition is faster than CP-APR if
the tensor size is not large.

Results on Real Data Next we demonstrate the eﬀectiveness of Legendre decomposition on real-
world datasets of third-order tensors. We evaluated the quality of decomposition by the root mean
squared error (RMSE) between the input and the reconstructed tensors. We also examined the
scalability of our method in terms of the number of parameters.
First we examine Legendre decomposition and three competing methods on the face image dataset2.
We picked up the ﬁrst entry from the fourth mode (corresponds to lighting) from the dataset and the
2This dataset is originally distributed at http://www.cl.cam.ac.uk/research/dtg/attarchive/
facedatabase.html and also available from the R rTensor package (https://CRAN.R-project.org/
package=rTensor).

8

#D<14A>5?0A0<4C4AB#D<14A>58C4A0C8>=B								
	x
	z
	|)4=B>AB8I4'D==8=6C8<4(B42)	{	{
		{		{		{
	–y
	x
	y
	z
	{#D<14A>5?0A0<4C4AB'D==8=6C8<4(B42)								
	z
	|
	x
	–z#0CDA0;6A03A0334B24=C!464=3A4#>==460C8E4)D2:4A#>==460C8E4%%%'012Figure 5: Experimental results on the face image dataset (a  b) and MNIST (c  d).

ﬁrst 20 faces from the third mode  resulting in a single third-order tensor with a size of 92(cid:2) 112(cid:2) 20 
where the ﬁrst two modes correspond to image pixels and the third mode to individuals. We set
decomposition bases B as B = B1 [ B2(l)[ B3(l). For every decomposition method  we gradually
increased l  m  and n and checked the performance in terms of RMSE and running time.
Results are plotted in Figure 5(a  b). In terms of RMSE  Legendre decomposition is superior to
the other methods if the number of parameters is small (up to 2 000)  and it is competitive with
nonnegative CP decomposition and inferior to CP-APR for a larger number of parameters. The
reason is that Legendre decomposition uses the information of the index order that is based on the
structure of the face images (pixels); that is  rows or columns cannot be replaced with each other in
the data. In terms of running time  it is slower than Tucker and CP decompositions as the number of
parameters increases  while it is still faster than CP-APR.
Next we used the MNIST dataset (LeCun et al.  1998)  which consists of a collection of images
of handwritten digits and has been used as the standard benchmark datasets in a number of recent
studies such as deep learning. We picked up the ﬁrst 500 images for each digit  resulting in 10
third-order tensors with the size of 28 (cid:2) 28 (cid:2) 500  where the ﬁrst two modes correspond to image
pixels. In Legendre decomposition  the decomposition basis B was simply set as B = B3(l) and
removed zero entries from Ω. Again  for every decomposition method  we varied the number of
parameters by increasing l  m  and n and evaluated the performance in terms of RMSE.
Means (cid:6) standard error of the mean (SEM) across all digits from “0” to “9” are plotted in Figure 5(c 
d). Results for all digits are presented in the supplementary material. Legendre decomposition
clearly shows the smallest RMSE  and the diﬀerence is larger if the number of parameters is smaller.
The reason is that Legendre decomposition ignores zero entries and decomposes only nonzero entries 
while such decomposition is not possible for other methods. Running time shows the same trend
as that of the face dataset; that is  Legendre decomposition is slower than other methods when the
number of parameters increases.

4 Conclusion

In this paper  we have proposed Legendre decomposition  which incorporates tensor structure into
information geometry. A given tensor is converted into the dual parameters ((cid:18); (cid:17)) connected via
the Legendre transformation  and the optimization is performed in the parameter space instead of
directly treating the tensors. We have theoretically shown the desired properties of the Legendre
decomposition  namely  that its results are well-deﬁned  unique  and globally optimized  in that it
always ﬁnds the decomposable tensor that minimizes the KL divergence from the input tensor. We
have also shown the connection between Legendre decomposition and Boltzmann machine learning.
We have experimentally shown that Legendre decomposition can more accurately reconstruct input
tensors than three standard tensor decomposition methods (nonnegative Tucker decomposition  non-
negative CP decomposition  and CP-APR) using the same number of parameters. Since the shape
of the decomposition basis B is arbitrary  Legendre decomposition has the potential to achieve even
more-accurate decomposition. For example  one can incorporate the domain knowledge into the set
B such that speciﬁc entries of the input tensor are known to dominate the other entries.
Our work opens the door to both further theoretical investigation of information geometric algorithms
for tensor analysis and a number of practical applications such as missing value imputation.

9

0248<0640248<064"#()"#()#D<14A>5?0A0<4C4AB'"(	
												
		
	
	
#D<14A>5?0A0<4C4AB'D==8=6C8<4(sec.)'D==8=6C8<4(sec.)	
									
	x
	y
	z#D<14A>5?0A0<4C4AB'"(							
							#D<14A>5?0A0<4C4AB							
				
	x
	y
	z
	{0123!464=3A4#>==460C8E4)D2:4A#>==460C8E4%%%'Acknowledgments
This work was supported by JSPS KAKENHI Grant Numbers JP16K16115  JP16H02870  and JST 
PRESTO Grant Number JPMJPR1855  Japan (M.S.); JSPS KAKENHI Grant Numbers 26120732
and 16H06570 (H.N.); and JST CREST JPMJCR1502 (K.T.).

References
D. H. Ackley  G. E. Hinton  and T. J. Sejnowski. A learning algorithm for Boltzmann machines.

Cognitive Science  9(1):147–169  1985.

S. Amari. Natural gradient works eﬃciently in learning. Neural Computation  10(2):251–276  1998.
S. Amari. Information geometry on hierarchy of probability distributions. IEEE Transactions on

Information Theory  47(5):1701–1711  2001.

S. Amari. Information geometry and its applications: Convex function and dually ﬂat manifold.
In F. Nielsen  editor  Emerging Trends in Visual Computing: LIX Fall Colloquium  ETVC 2008 
Revised Invited Papers  pages 75–102. Springer  2009.

S. Amari. Information Geometry and Its Applications. Springer  2016.
B. W. Bader and T. G. Kolda. Eﬃcient MATLAB computations with sparse and factored tensors.

SIAM Journal on Scientiﬁc Computing  30(1):205–231  2007.

B. W. Bader  T. G. Kolda  et al. MATLAB tensor toolbox version 3.0-dev  2017.
C. F. Beckmann and S. M. Smith. Tensorial extensions of independent component analysis for

multisubject FMRI analysis. NeuroImage  25(1):294–311  2005.

Y. Censor and A. Lent. An iterative row-action method for interval convex programming. Journal

of Optimization Theory and Applications  34(3):321–353  1981.

J. Chen  S. Cheng  H. Xie  L. Wang  and T. Xiang. Equivalence of restricted Boltzmann machines

and tensor network states. Physical Review B  97:085104  2018.

E. C. Chi and T. G. Kolda. On tensors  sparsity  and nonnegative factorizations. SIAM Journal on

Matrix Analysis and Applications  33(4):1272–1299  2012.

A. Cichocki  D. Mandic  L. De Lathauwer  G. Zhou  Q. Zhao  C. Caiafa  and H. A. Phan. Tensor
decompositions for signal processing applications: From two-way to multiway component analysis.
IEEE Signal Processing Magazine  32(2):145–163  2015.

B. A. Davey and H. A. Priestley. Introduction to Lattices and Order. Cambridge University Press  2

edition  2002.

G. Gierz  K. H. Hofmann  K. Keimel  J. D. Lawson  M. Mislove  and D. S. Scott. Continuous Lattices

and Domains. Cambridge University Press  2003.

R. A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an “ex-
planatory” multi-modal factor analysis. Technical report  UCLA Working Papers in Phonetics 
1970.

G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computa-

tion  14(8):1771–1800  2002.

Y. D. Kim and S. Choi. Nonnegative Tucker decomposition. In 2007 IEEE Conference on Computer

Vision and Pattern Recognition  pages 1–8  2007.

T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

J. Kossaiﬁ  Y. Panagakis  and M. Pantic. TensorLy: Tensor learning in Python. arXiv:1610.09555 

2016.

10

Y. LeCun  C. Cortes  and C. J. C. Burges. The MNIST database of handwritten digits  1998. URL

http://yann.lecun.com/exdb/mnist/.

Y. LeCun  S. Chopra  R. Hadsell  M. Ranzato  and F. J. Huang. A tutorial on energy-based learning.
In G. Bakir  T. Hofmann  B. Schölkopf  A. J. Smola  B. Taskar  and S. V. N. Vishwanathan  editors 
Predicting Structured Data. The MIT Press  2007.

D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization.

Nature  401(6755):788–791  1999.

D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural

Information processing Systems 13  pages 556–562  2001.

J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual

data. IEEE Transactions on Pattern Analysis and Machine Intelligence  35(1):208–220  2013.

H. Nakahara and S. Amari. Information-geometric measure for neural spikes. Neural Computation 

14(10):2269–2316  2002.

R. Salakhutdinov. Learning and evaluating Boltzmann machines. UTML TR 2008-002  2008.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines.

In Proceedings of the 12th

International Conference on Artiﬁcial Intelligence and Statistics  pages 448–455  2009.

R. Salakhutdinov and G. E. Hinton. An eﬃcient learning procedure for deep Boltzmann machines.

Neural Computation  24(8):1967–2006  2012.

R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings

of the 25th International Conference on Machine learning  pages 872–879  2008.

T. J. Sejnowski. Higher-order Boltzmann machines. In AIP Conference Proceedings  volume 151 

pages 398–403  1986.

A. Shashua and T. Hazan. Non-negative tensor factorization with applications to statistics and
In Proceedings of the 22nd International Conference on Machine Learning 

computer vision.
pages 792–799  2005.

P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In D. E.
Rumelhart  J. L. McClelland  and PDP Research Group  editors  Parallel Distributed Processing:
Explorations in the Microstructure of Cognition  Vol. 1  pages 194–281. MIT Press  1986.

M. Sugiyama  H. Nakahara  and K. Tsuda. Information decomposition on structured space. In 2016

IEEE International Symposium on Information Theory  pages 575–579  2016.

M. Sugiyama  H. Nakahara  and K. Tsuda. Tensor balancing on statistical manifold. In Proceedings

of the 34th International Conference on Machine Learning  pages 3270–3279  2017.

P. Symeonidis. Matrix and tensor decomposition in recommender systems. In Proceedings of the

10th ACM Conference on Recommender Systems  pages 429–430  2016.

R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regularization.

In Advances in Neural Information Processing Systems 26  pages 1331–1339  2013.

L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):

279–311  1966.

M. A. O. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: TensorFaces. In
Proceedings of The 7th European Conference on Computer Vision (ECCV)  volume 2350 of LNCS 
pages 447–460  2002.

M. A. O. Vasilescu and D. Terzopoulos. Multilinear (tensor) image synthesis  analysis  and recogni-

tion [exploratory dsp]. IEEE Signal Processing Magazine  24(6):118–123  2007.

K. Y. Yılmaz  A. T. Cemgil  and U. Simsekli. Generalised coupled tensor factorisation. In Advances

in Neural Information Processing Systems 24  pages 2151–2159  2011.

Y. K. Yılmaz and A. T. Cemgil. Algorithms for probabilistic latent tensor factorization. Signal

Processing  92(8):1853–1863  2012.

11

,Mahito Sugiyama
Hiroyuki Nakahara
Koji Tsuda
Qian Lou
Lei Jiang