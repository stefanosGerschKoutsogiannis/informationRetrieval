2019,Are sample means in multi-armed bandits positively or negatively biased?,It is well known that in stochastic multi-armed bandits (MAB)  the sample mean of an arm is typically not an unbiased estimator of its true mean. In this paper  we decouple three different sources of this selection bias: adaptive \emph{sampling} of arms  adaptive \emph{stopping} of the experiment  and adaptively \emph{choosing} which arm to study.  Through a new notion called ``optimism'' that captures certain natural monotonic behaviors of algorithms  we provide a clean and unified analysis of how optimistic rules affect the sign of the bias. The main takeaway message is that optimistic sampling induces a negative bias  but optimistic stopping and optimistic choosing both induce a positive bias. These results are derived in a general stochastic MAB setup that is entirely agnostic to the final aim of the experiment (regret minimization or best-arm identification or anything else). We provide examples of optimistic rules of each type  demonstrate that simulations confirm our theoretical predictions  and pose some natural but hard open problems.,Are sample means in multi-armed bandits

positively or negatively biased?

Jaehyeok Shin1  Aaditya Ramdas1 2 and Alessandro Rinaldo1

Department of Statistics and Data Science1

Machine Learning Department2

Carnegie Mellon University

{shinjaehyeok  aramdas  arinaldo}@cmu.edu

Abstract

It is well known that in stochastic multi-armed bandits (MAB)  the sample mean of
an arm is typically not an unbiased estimator of its true mean. In this paper  we
decouple three different sources of this selection bias: adaptive sampling of arms 
adaptive stopping of the experiment  and adaptively choosing which arm to study.
Through a new notion called “optimism” that captures certain natural monotonic
behaviors of algorithms  we provide a clean and uniﬁed analysis of how optimistic
rules affect the sign of the bias. The main takeaway message is that optimistic
sampling induces a negative bias  but optimistic stopping and optimistic choosing
both induce a positive bias. These results are derived in a general stochastic MAB
setup that is entirely agnostic to the ﬁnal aim of the experiment (regret minimization
or best-arm identiﬁcation or anything else). We provide examples of optimistic
rules of each type  demonstrate that simulations conﬁrm our theoretical predictions 
and pose some natural but hard open problems.

1

Introduction

Mean estimation is one of the most fundamental problems in statistics. In the classic nonadaptive
setting  we observe a ﬁxed number of samples drawn i.i.d. from a ﬁxed distribution with an unknown
mean µ. In this case  we know that the sample mean is an unbiased estimator of µ.
However  in many cases the data are collected and analyzed in an adaptive manner  a prototypical
example being the stochastic multi-armed bandits (MAB) framework [Robbins  1952]. During the
data collection stage  in each round an analyst can draw a sample from one among a ﬁnite set of
available distributions (arms) based on the previously observed data (adaptive sampling). The data
collecting procedure can also be terminated based on a data-driven stopping rule rather than at a ﬁxed
time (adaptive stopping). Further  the analyst can choose a speciﬁc target arm based on the collected
data (adaptive choosing)  for example choosing to focus on the arm with the largest empirical mean
at the stopping time. In this setting  the sample mean is no longer unbiased  due to the selection bias
introduced by all three kinds of adaptivity. In this paper  we provide a comprehensive understanding
of the sign of the bias  decoupling the effects of these three sources of adaptivity.
In a general and uniﬁed MAB framework  we ﬁrst deﬁne natural notions of monotonicity (a special
case of which we call “optimism”) of sampling  stopping and choosing rules. Under no assumptions
on the distributions beyond assuming that their means exist  we show that optimistic sampling
provably results in a negative bias  but optimistic stopping and optimistic choosing both provably
result in a positive bias. Thus  the net bias can be positive or negative in general. This message is in
contrast to a recent thought-provoking work by Nie et al. [2018] titled “Why adaptively collected
data has a negative bias...” that is unfortunately misleading for practitioners  since it only analyzed
the bias of adaptive sampling for a ﬁxed arm at a ﬁxed time.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

As a concrete example  consider an ofﬂine analysis of data that was collected by an MAB algorithm
(with any aim). Suppose that a practitioner wants to estimate the mean reward of some of the better
arms that were picked more frequently by the algorithm. Nie et al. [2018] proved that the sample
mean of each arm is negatively biased under fairly common adaptive sampling rules. Although
this result is applicable only to a ﬁxed arm at a ﬁxed time  it could instill a possibly false sense of
comfort with sample mean estimates since the practitioner might possibly think that sample means
are underestimating the effect size. However  we prove that if the algorithm was adaptively stopped
and the arm index was adaptively picked  then the net bias can actually be positive. Indeed  we prove
that this is the case for the lil’UCB algorithm (Corollary 8)  but it is likely true more generally as
captured by our main theorem. Thus  the sample mean may actually overestimate the effect size. This
is an important and general phenomenon for both theoreticians (to study further and quantify) and for
practitioners (to pay heed to) because if a particular arm is later deployed in practice  it may yield a
lower reward than was possibly expected from the ofﬂine analysis.

Related work and our contributions. Adaptive mean estimation  in each of the three senses
described above  has received much attention in both recent and past literature. Below  we discuss
how our work relates to past work  proceeding one notion at a time in approximate historical order.
We begin by noting that a single-armed bandit is simply a random walk  where adaptive stopping has
been extensively studied. The book by Gut [2009] on stopped random walks is an excellent reference 
summarizing almost 60 years of advances in sequential analysis. Most of these extensive results
on random walks have not been extended to the MAB setting  which naturally involves adaptive
sampling and choosing. Of particular relevance is the paper by Starr and Woodroofe [1968] on the
sign of the bias under adaptive stopping  whose work is subsumed by ours in two ways: we not only
extend their insights to the MAB setting  but even for the one-armed setting  our results generalize
theirs.
Characterizing the sign of the bias of the sample mean under adaptive sampling has been a recent
topic of interest due to a surge in practical applications. While estimating MAB ad revenues  Xu
et al. [2013] gave an informal argument of why the sample mean is negatively biased for “optimistic”
algorithms. Later  Villar et al. [2015] encountered this negative bias in a simulation study motivated
by using MAB for clinical trials. Most recently  Bowden and Trippa [2017] derived an exact formula
for the bias and Nie et al. [2018] formally provided conditions under which the bias is negative. Our
results on “optimistic” sampling inducing a negative bias generalize the corresponding results in
these past works.
Most importantly  however  these past results hold only at a predetermined time and for a ﬁxed arm.
Here  we put forth a complementary viewpoint that “optimistic” stopping and choosing induces a
positive bias. Indeed  one of our central conceptual contributions is an appropriate and crisp deﬁnition
of “monotonicity” and “optimism” (Deﬁnition 1)  that enables a clean and general analysis.
Our main theoretical result  Theorem 7  allows the determination of the sign of the bias in several
interesting settings. Importantly  the bias may be of any sign when optimistic sampling  stopping and
choosing are all employed together. We demonstrate the practical validity of our theory using some
simulations that yield interesting insights in their own right.
The rest of this paper is organized as follows. In Section 2  we brieﬂy formalize the three notions
of adaptivity by introducing a stochastic MAB framework. Section 3 derives results on when the
bias can be positive or negative. In Section 4  we demonstrate the correctness of our theoretical
predictions through simulations in a variety of practical situations. We end with a brief summary in
Section 5  and for reasons of space  we defer all proofs to the Appendix.

2 The stochastic MAB framework
Let P1  . . .   PK be K distributions of interest (also called arms) with ﬁnite means µk = EY ∼Pk [Y ].
Every inequality and equality between two random variables is understood in the almost sure sense.

2.1 Formalizing the three notions of adaptivity

For those not familiar with MAB algorithms  Lattimore and Szepesvári [2019] is a good reference.
The following general problem setup is critical in the rest of the paper:

2

Draw an initial random seed W0 ∼ U [0  1]  and set t = 1.

• Let W−1 denote all external sources of randomness that are independent of everything else.
• At time t  let Dt−1 be the data we have so far  which is given by
Dt−1 := {A1  Y1  . . .   At−1  Yt−1} 

where As is the (random) index of arm sampled at time s and Ys is the observation from
the arm As. Based on the previous data (and possibly an external source of randomness) 
let νt(k | Dt−1) ∈ [0  1] be the conditional probability of sampling the k-th arm for all
k=1 νt(k | Dt−1) = 1. Different choices for νt capture
commonly used methods such as random allocation  -greedy [Sutton and Barto  1998] 
upper conﬁdence bound algorithms [Auer et al.  2002  Audibert and Bubeck  2009  Garivier
and Cappé  2011  Kalyanakrishnan et al.  2012  Jamieson et al.  2014] and Thompson
sampling [Thompson  1933  Agrawal and Goyal  2012  Kaufmann et al.  2012].

k ∈ [K] := {1  . . .   K} with(cid:80)K
• If Wt−1 ∈(cid:16)(cid:80)k−1
j=1 νt(j | Dt−1) (cid:80)k

for some k ∈ [K]  then set At = k
which is equivalent to sample At from a multinomial distribution with probabilities {νt(k |
Dt−1)}K
k=1. Let Yt be a fresh independent draw from distribution Pk. This yields a natural
ﬁltration {Ft} which is deﬁned  starting with F0 = σ (W−1  W0)  as
Ft := σ (W−1  W0  Y1  W1  . . .   Yt  Wt)   ∀t ≥ 1.

j=1 νt(j | Dt−1)

Then  {Yt} is adapted to {Ft}  and {At} {νt} are predictable with respect to {Ft}.

• For each k ∈ [K] and t ≥ 1  deﬁne the running sum and number of draws for arm k
1(As = k). Assuming that arm k is

as Sk(t) :=(cid:80)t

(cid:17)

s=1

sampled at least once  we deﬁne the sample mean for arm k as

s=1

1(As = k)Ys  Nk(t) :=(cid:80)t
(cid:98)µk(t) :=

Sk(t)
Nk(t)

.

Then  {St}  {(cid:98)µk(t)} are adapted to {F t} and {Nk(t)} is predictable with respect to {F t}.

• Let T be a stopping time with respect to {F t}. If T is nonadaptively chosen  it is denoted
T . If t < T   draw a random seed Wt ∼ U [0  1] for the next round  and increment t. Else
return the collected data DT = {A1  Y1  . . .   AT   YT } ∈ FT .
• After stopping  choose a data-dependent arm based on a possibly randomized rule κ :
DT ∪ {W−1} (cid:55)→ [K]  but we denote the index κ(DT ∪ {W−1}) as just κ for short  so that
the target of estimation is µκ. Note that κ ∈ FT   but when κ is nonadaptively chosen (is
independent of FT )  we called it a ﬁxed arm and denote it as k.

The phrase “fully adaptive setting” refers to the scenario of running an adaptive sampling algorithm
until an adaptive stopping time T   and asking about the sample mean of an adaptively chosen arm κ.
When we are not in the fully adaptive setting  we explicitly mention what aspects are adaptive.

2.2 The tabular perspective on stochastic MABs
It will be useful to imagine the above fully adaptive MAB experiment using a N×K table  X∗
∞  whose
rows index time and columns index arms. Here  we put an asterisk to clarify that it is counterfactual
and not necessarily observable. We imagine this entire table to be populated even before the MAB
experiments starts  where for every i ∈ N  k ∈ [K]  the (i  k)-th entry of the table contains an
independent draw from Pk called X∗
i k. At each step  our observation Yt corresponds to the element
X∗
Given the above tabular MAB setup (which is statistically indistinguishable from the setup described
in the previous subsection)  one may then ﬁnd deterministic functions ft k and f∗
k (D∗

∞ ∪ {W−1  W0  . . .   Wt  . . .}.

. Finally  we denote D∗

1 (At = k) 1(T ≥ t)

ft k(Dt−1) ≡ f∗

Nk(T ) =

∞ = X∗

k such that

(cid:88)

Nk(t) At

∞).

(1)

(cid:88)

t≥1

(cid:125)

=

(cid:124)

t≥1

(cid:123)(cid:122)

F t−1-measurable

Speciﬁcally  the function ft k(·) evaluates to one if and only if we do not stop at time t − 1  and pull
arm k at time t. Indeed  given D∗
∞  the stopping time T is deterministic and so is the number of times

3

Nk(T ) that a ﬁxed arm k is pulled  and this is what f∗
draws from a chosen arm κ at stopping time T can be written in terms of the tabular data as

k captures. Along the same lines  the number of

k(D∗
g∗

∞)f∗

k (D∗
∞)

(2)

k evaluates to one if after stopping  we choose

Nκ(T ) =

K(cid:88)

k=1

1 (κ = k) Nk(T ) ≡ k(cid:88)

k=1

k}. Indeed  g∗
for some deterministic set of functions {g∗
arm k  which is a fully deterministic choice given D∗
∞.

3 The sign of the bias under adaptive sampling  stopping and choosing

3.1 Examples of positive bias due to “optimistic” stopping or choosing

In MAB problems  collecting higher rewards is a common objective of adaptive sampling strategies 
and hence they are often designed to sample more frequently from a distribution which has larger
sample mean than the others. Nie et al. [2018] proved that the bias of the sample mean for any
ﬁxed arm and at any ﬁxed time is negative when the sampling strategy satisﬁes two conditions
called “Exploit” and “Independence of Irrelevant Options” (IIO). However  the emphasis on ﬁxed is
important: their conditions are not enough to determine the sign of the bias under adaptive stopping
or choosing  even in the simple nonadaptive sampling setting. Before formally deﬁning our crucial
notions of “optimism” in the next subsection  it is instructive to look at some examples.
Example 1. Suppose we continuously alternate between drawing a sample from each of two Bernoulli
distributions with mean parameters µ1  µ2 ∈ (0  1). This sampling strategy is fully deterministic  and
thus it satisﬁes the Exploit and IIO conditions in Nie et al. [2018]. For any ﬁxed time t  the bias
equals zero for both sample means. Deﬁne a stopping time T as the ﬁrst time we observe +1 from
the ﬁrst arm. Then the sample size of the ﬁrst arm  N1(T )  follows a geometric distribution with

parameter µ1  which implies that the bias of(cid:98)µ1(T ) is
(cid:21)

(cid:20)

E [(cid:98)µ1(T ) − µ1] = E

1

N1(T )

− µ1 =

µ1 log(1/µ1)

1 − µ1

− µ1 

which is positive for all µ1 ∈ (0  1).
This example shows that for nonadaptive sampling  adaptive stopping can induce a positive bias.
In fact  this example is not atypical  but is an instance of a more general phenomenon explored in
the one-armed setting in sequential analysis. For example  Siegmund [1978  Ch. 3] contains the
following classical result for a Brownian motion W (t) with positive drift µ > 0.
Example 2. If we deﬁne a stopping time as the ﬁrst time W (t) exceeds a line with slope η and
intercept b > 0  that is TB := inf{t ≥ 0 : W (t) ≥ ηt + b}  then for any slope η ≤ µ  we have
= 1/b. Note that a sum of Gaussians with mean µ behaves like a time-discretization
of a Brownian motion with drift µ; since EW (t) = tµ  we may interpret W (TB)/TB as a stopped
sample mean  and the last equation implies that its bias is 1/b  which is positive.

E(cid:104) W (TB )

− µ

(cid:105)

TB

Generalizing further  Starr and Woodroofe [1968] proved the following remarkable result.
Example 3. If we stop when the sample mean crosses any predetermined upper boundary  the stopped
sample mean is always positive biased (whenever the stopping time is a.s. ﬁnite). Explicitly  choosing

any arbitrary sequence of real-valued constants {ck}  deﬁne Tc := inf{t :(cid:98)µ1(t) > ct}  then as long
as the observations Xi have a ﬁnite mean and Tc is a.s. ﬁnite  we have E [(cid:98)µ1(Tc) − µ1] − µ1 > 0.

Surprisingly  we will generalize the above strong result even further. Additionally  stopping times
in the MAB literature can be thought of as extensions of Tc and TB to a setting with multiple arms 
and we will prove that indeed the bias induced will still be positive. We end with an example of the
positive bias induced by “optimistic” choosing:
Example 4. Given K standard normals {Zi} (to be thought of as one sample from each of K arms) 
let κ = argmaxk Zk  that is  we choose the arm with the largest observation. It is well known that
2 log K. Since EZk = 0 for all k  but EZκ > 0  the “optimistic”

E [Zκ] = E(cid:2)maxk∈[K] Zk

(cid:3) (cid:16) √

choice κ induces a positive bias.

In many typical MAB settings  we should expect sample means to have two contradictory sources of
bias: negative bias from “optimistic sampling” and positive bias from “optimistic stopping/choosing”.

4

3.2 Positive or negative bias under monotonic sampling  stopping and choosing

Based on the expression (2)  we formally state a characteristic of data collecting strategies which
fully determines the sign of the bias as follows.
Deﬁnition 1. A data collecting strategy is “monotonically increasing (or decreasing)” if for any
∞) ≡ 1 (κ = k) /Nk(T )  is an increasing
k(D∗
i ∈ N and k ∈ [K]  the function D∗
(or decreasing) function of X∗

i k while keeping all other entries in D∗

∞ ﬁxed. Further  we say that

∞ (cid:55)→ g∗

∞)/f∗

k (D∗

• a data collecting strategy has an optimistic sampling rule if the function D∗

i k while keeping all other entries in D∗

increasing function of X∗
t ≥ 1 and k ∈ [K];

∞ (cid:55)→ Nk(t) is an
∞ ﬁxed for any ﬁxed i ∈ N 

• a data collecting strategy has an optimistic stopping rule if D∗

i k while keeping all other entries in D∗

∞ (cid:55)→ T is a decreasing
∞ ﬁxed for any ﬁxed i ∈ N and

function of X∗
k ∈ [K];

• a data collecting strategy has an optimistic choosing rule if D∗

i k while keeping all other entries in D∗

∞ (cid:55)→ 1(κ = k) is an
∞ ﬁxed for any ﬁxed i ∈ N

increasing function of X∗
and k ∈ [K].

Note that if a data collecting strategy has an optimistic sampling (or stopping or choosing) rule  with
the other components being nonadaptive  then the strategy is monotonically decreasing (increasing).
We remark that nonadaptive just means independent of the entries X∗
i k  but it is not necessarily
deterministic1. The above deﬁnition warrants some discussion to provide intuition.
Roughly speaking  under optimistic stopping  if a sample from the k-th distribution was increased
while keeping all other values ﬁxed  the algorithm would reach its termination criterion sooner. For
instance  TB from Example 2 and the criterion in Example 1 are both optimistic stopping rules. Most
importantly  boundary-crossing is optimistic:
Fact 1. The general boundary-crossing stopping rule of Starr and Woodroofe [1968]  denoted Tc in
Example 3  is an optimistic stopping rule (and hence optimistic stopping is a weaker condition).
Optimistic stopping rules do not need to be based on the sample mean; for example  if {ct} is an
arbitrary sequence  then T := inf{t ≥ 3 : Xt + Xt−2 ≥ ct} is an optimistic stopping rule. In
fact  T(cid:96) := inf{t ≥ 3 : (cid:96)t(X1  . . .   Xt) ≥ ct} is optimistic  as long as each (cid:96)t is coordinatewise
nondecreasing.
For optimistic choosing  the previously discussed argmax rule (Example 4) is optimistic. More
generally  it is easy to verify the following:
Fact 2. For any probabilities p1 ≥ p2 ··· ≥ pK that sum to one  a rule that chooses the arm with the
k-th largest empirical mean with probability pk  is an optimistic choosing rule.

Turning to the intuition for optimistic sampling  if a sample from the k-th distribution was increased
while keeping all other values ﬁxed  the algorithm would sample the k-th arm more often. We claim
that optimistic sampling is a weaker condition than the Exploit and IIO conditions employed by Nie
et al. [2018].
Fact 3. The “Exploit” and “IIO” conditions in Nie et al. [2018] together imply that the sampling
strategy is optimistic (and hence optimistic sampling is a weaker condition). Further  as summarized
in Appendix A  -greedy  UCB and Thompson sampling (Gaussian-Gaussian and Beta-Bernoulli  for
instance) are all optimistic sampling methods.

For completeness  we prove the ﬁrst part formally in Appendix A.2  which builds heavily on
observations already made in the proof of Theorem 1 in Nie et al. [2018]. Beyond the instances
mentioned above  Corollary 10 in the supplement captures a sufﬁcient condition for Thompson
sampling with one-dimensional exponential families and conjugate priors to be optimistic. We now
provide an expression for the bias that holds at any stopping time and for any sampling algorithm.

1An example of a random but nonadaptive stopping rule: ﬂip a (potentially biased) coin at each step to decide
whether to stop. An example of a random but nonadaptive sampling rule: with probability half pick a uniformly
random arm  and with probability half pick the arm that has been sampled most often thus far.

5

Proposition 5. Let T be a stopping time with respect to the natural ﬁltration {Ft}. For each ﬁxed

k ∈ [K] such that 0 < ENk(T ) < ∞  the bias of(cid:98)µk(T ) is given as
E [(cid:98)µk(T ) − µk] = − Cov ((cid:98)µk(T )  Nk(T ))

.

(3)

E [Nk(T )]

MAB setting. Speciﬁcally  recalling that Sk(t) =(cid:98)µk(t)Nk(t)  we show the following:

The proof may be found in Appendix B.3. A similar expression was derived in Bowden and Trippa
[2017]  but only for a ﬁxed time T . In order to extend it to stopping times (that are allowed to be
inﬁnite  as long as ENk(T ) < ∞)  we derive a simple generalization of Wald’s ﬁrst identity to the
Lemma 6. Let T be a stopping time with respect to the natural ﬁltration {Ft}. For each ﬁxed
k ∈ [K] such that ENk(T ) < ∞  we have E[Sk(T )] = µkE[Nk(T )].
This lemma is also proved in Appendix B.3. Proposition 5 provides a simple  and somewhat intuitive 
expression of the bias for each arm. It implies that if the covariance of the sample mean of an arm
and the number of times it was sampled is positive (negative)  then the bias is negative (positive). We
now formalize this intuition below  including for adaptively chosen arms. The following theorem
shows that if the adaptive sampling  stopping and choosing rules are monotonically increasing (or
decreasing)  then the sample mean is positively (or negatively) biased.
Theorem 7. Let T be a stopping time with respect to the natural ﬁltration {Ft} and let κ : DT (cid:55)→ [K]
be a choosing rule. Suppose each arm has ﬁnite expectation and  for all k with P (κ = k) > 0  we
have E [Nk(T )] < ∞ and Nk(T ) ≥ 1. If the data collecting strategy is monotonically decreasing 
for example under optimistic sampling with nonadaptive stopping and choosing  then we have

E [(cid:98)µκ(T ) | κ = k] ≤ µk  ∀k : P(κ = k) > 0 

(4)

which also implies that

E [(cid:98)µκ(T ) − µκ] ≤ 0.

(6)

E [(cid:98)µκ(T ) − µκ] ≥ 0.

(5)
Similarly if the data collecting strategy is monotonically increasing  for example under optimistic
stopping with nonadaptive sampling and choosing  or under optimistic choosing with nonadaptive
sampling and stopping  then we have

E [(cid:98)µκ(T ) | κ = k] ≥ µk  ∀k : P(κ = k) > 0 

(7)

which also implies that
If each arm has a bounded distribution then the condition E [Nk(T )] < ∞ can be dropped.
Remark 1. In fact  if each arm has a ﬁnite p-th moment for a ﬁxed p > 2 then the condition
E [Nk(T )] < ∞ can be dropped.
The proofs of Theorem 7 and Remark 1 can be found in Appendix B.1 and are based on martingale
arguments that are quite different from the ones used in Nie et al. [2018]. See also Appendix A.4
for an intuitive explanation of the sign of the bias under optimistic sampling  stopping or choosing

rules. The expression (3) intuitively suggests situations when the sample mean estimator(cid:98)µk(T ) is

biased  while the inequalities in (4) and (6) determine the direction of bias under the monotonic or
optimistic conditions. Due to Facts 1  2 and 3  several existing results are immediately subsumed and
generalized by Theorem 7. Further  the following corollary is a particularly interesting special case
dealing with the lil’UCB algorithm by Jamieson et al. [2014] which uses adaptive sampling  stopping
and choosing  as summarized in Section 4.3.
Corollary 8. The lil’UCB algorithm is a monotonically increasing strategy  and thus the sample
mean of the reported arm when lil’UCB stops is always positively biased.

The proof is described in Appendix B.2. The above result is interesting because of the following
reasons: (a) when viewed separately  the sampling  stopping and choosing rules of the lil’UCB
algorithm all seem to be optimistic (however  they are not optimistic  because our deﬁnition requires
two out of three to be nonadaptive); hence it is apriori unclear which rule dominates and whether the
net bias should be positive or negative; (b) we did not have to alter anything about the algorithm in
order to prove that it is a monotonically increasing strategy (for any distribution over arms  for any
number of arms). The generality of the above result showcases the practical utility of our theorem 
whose message is in sharp contrast to the title of the paper by Nie et al. [2018].
Next  we provide simulation results that verify that our monotonic and optimistic conditions accurately
capture the sign of the bias of the sample mean.

6

4 Numerical experiments

4.1 Negative bias from optimistic sampling rules in multi-armed bandits

Recall Fact 3  which stated that common MAB adaptive sampling strategies like greedy (or -greedy) 
upper conﬁdence bound (UCB) and Thompson sampling are optimistic. Thus  for a deterministic
stopping time  Theorem 7 implies that the sample mean of each arm is always negatively biased. To
demonstrate this  we conduct a simulation study in which we have three unit-variance Gaussian arms
with µ1 = 1  µ2 = 2 and µ3 = 3. After sampling once from each arm  greedy  UCB and Thompson
sampling are used to continue sampling until T = 200. We repeat the whole process from scratch
104 times for each algorithm to get an accurate estimate for the bias.2 Due to limited space  we
present results from UCB and Thompson sampling only but detailed conﬁgurations of algorithms
and a similar result for the greedy algorithm can be found in Appendix C.1. Figure 1 shows the
distribution of observed differences between sample means and the true mean for each arm. Vertical
lines correspond to biases. The example demonstrates that the sample mean is negatively biased
under optimistic sampling rules.
Remark 2. The main goal in our simulations is to visualize and corroborate our theoretical results
about the sign of the bias. As a result  we do not make any attempt to optimize the parameters for
UCB or Thompon sampling for the purpose of minimizing the regret  since the latter is not the paper’s
aim. However  investigating the relationship between the performance of MAB algorithms and the
bias at the time horizon would be an interesting future direction of research.

Figure 1: Data is collected by UCB (left) and Thompson sampling (right) algorithms from three
unit-variance Gaussian arms with µ1 = 1  µ2 = 2 and µ3 = 3. For all three arms  sample means
are negatively biased (at ﬁxed times). A similar result for the greedy algorithm can be found in
Appendix C.1.

4.2 Bias from stopping a one-sided sequential likelihood ratio test

Suppose we have two independent sub-Gaussian arms with common and known parameter σ2 but
unknown means µ1 and µ2. Consider the following testing problem:
H0 : µ1 ≤ µ2 vs H1 : µ1 > µ2.

To test this hypothesis  suppose we draw a sample from arm 1 for every odd time and from arm 2
for every even time. Instead of conducting a test at a ﬁxed time  we can use the following one-sided
sequential likelihood ratio test [Robbins  1970  Howard et al.  2018]: for any ﬁxed w > 0 and
α ∈ (0  1)  deﬁne a stopping time T as

T w := inf

t ∈ Neven :(cid:98)µ1(t) −(cid:98)µ2(t) ≥ 2σ

t

(cid:118)(cid:117)(cid:117)(cid:116)(t + 2w) log

(cid:32)

(cid:114) t + 2w

1
2α

+ 1

2w

(cid:33)  

(8)

2In all experiments  sizes of reported biases are larger than at least 3 times the Monte Carlo standard error.

7

012345−2.50.02.5difference between sample and true meansdensityArmsN(1 1)N(2 1)N(3 1)Bias = (−0.291  −0.363  −0.006)UCB algorithm012345−2.50.02.5difference between sample and true meansdensityArmsN(1 1)N(2 1)N(3 1)Bias = (−0.081  −0.262  −0.106)Thompson samplingM := min{T w  M}. Then  we reject the null H0 if T w

From Theorem 7  we have that µ1 ≤ E(cid:98)µ1(T w

where Neven := {2n : n ∈ N}. For a given ﬁxed maximum even time M ≥ 2  we stop sampling at
time T w
M < M. It can be checked [Howard
et al.  2018  Section 8] that  for any ﬁxed w > 0  this test controls the type-1 error at level α and the
power goes to 1 as M goes to inﬁnity.
For the arms 1 and 2  these are special cases of optimistic and pessimistic stopping rules respectively.
M ). To demonstrate this  we
conduct two simulation studies with unit variance Gaussian errors: one under the null hypothesis
(µ1  µ2) = (0  0)  and one under the alternative hypothesis (µ1  µ2) = (1  0). We choose M = 200 
w = 10 and α = 0.1. As before  we repeat each experiment 104 times for each setting. Figure 2
shows the distribution of observed differences between sample means and the true mean for each arm
under null and alternative hypothesis cases. Vertical lines correspond to biases. The simulation study
demonstrates that the sample mean for arm 1 is positively biased and the sample mean for arm 2 is
negatively biased as predicted.

M ) and µ2 ≥ E(cid:98)µ2(T w

Figure 2: Data is collected from the one-sided sequential likelihood ratio test procedure described
in Section 4.2. The sample mean for arm 1 is positively biased and the sample mean for arm 2 is
negatively biased under both null and alternative hypothesis cases. Note that the size of the bias
under the null hypothesis is smaller than the one under the alternative hypothesis since the number
of collected samples is larger under the null hypothesis.

4.3 Positive bias of the lil’UCB algorithm in best-arm identiﬁcation

Suppose we have K sub-Gaussian arms with mean µ1  . . .   µK and known parameter σ. In the
best-arm identiﬁcation problem  our target of inference is the arm with the largest mean. There
exist many algorithms for this task including lil’UCB [Jamieson et al.  2014]  Top-Two Thompson
Sampling [Russo  2016] and Track-and-Stop [Garivier and Kaufmann  2016].
In Corollary 8  we showed that the lil’UCB algorithm is monotonically increasing  and thus the
sample mean of the chosen arm is positively biased. In this subsection  we verify it with a simulation.
It is an interesting open question whether different types of best-arm identiﬁcation algorithms also
yield positively biased sample means.
The lil’UCB algorithm consists of the following optimistic sampling  stopping and choosing:
• Sampling: For any k ∈ [K] and t = 1  . . . K  deﬁne νt(k) = 1(t = k). For t > K 

if k = argmaxj∈[K](cid:98)µj(t − 1) + ulil

t (Nj(t − 1))  

νt(k) =

(cid:26)1

0

where δ    λ and β are algorithm parameters and

ulil
t (n) := (1 + β)(1 +

otherwise 
√

(cid:110)
t > K : Nk(t) ≥ 1 + λ(cid:80)

)(cid:112)2σ2(1 + ) log (log((1 + )n)/δ) /n.
(cid:111)
j(cid:54)=k Nj(t) for some k ∈ [K]

.

• Stopping: T = inf

8

• Choosing: κ = argmaxk∈[K] Nk(T ).

Once we stop sampling at time T   the lil’UCB algorithm guarantees that κ is the index of the arm
with largest mean with some probability depending on input parameters. Based on this  we can

also estimate the largest mean by the chosen stopped sample mean(cid:98)µκ (T ). The performance of
chosen stopped sample mean(cid:98)µκ (T ) is always positively biased for any choice of parameters.

this sequential procedure can vary based on underlying distribution of the arm and the choice of
parameters. However  we can check this optimistic sampling and optimistic stopping/choosing rules
which would yield negative and positive biases respectively are monotonic increasing and thus the

To verify it with a simulation  we set 3 unit-variance Gaussian arms with means (µ1  µ2  µ3) =
(g  0 −g) for each gap parameter g = 1  3  5. We conduct 104 trials of the lil’UCB algorithm
with a valid choice of parameters described in Jamieson et al. [2014  Section 5]. Figure 3 shows
the distribution of observed differences between the chosen sample means and the corresponding
true mean for each δ. Vertical lines correspond to biases. The simulation study demonstrates that 

in all conﬁgurations  the chosen stopped sample mean (cid:98)µκ (T ) is always positively biased. (see

Appendix B.2 for a formal proof.)

Figure 3: Data is collected by the lil’UCB algorithm run on three unit-variance Gaussian arms with
µ1 = g  µ2 = 0 and µ3 = −g for each gap parameter g = 1  3  5. For all cases  chosen sample
means are positively biased. The bias is larger for a larger gap since the number of collected samples
is smaller on an easier task.

5 Summary

This paper provides a general and comprehensive characterization of the sign of the bias of the sample
mean in multi-armed bandits. Our main conceptual innovation was to deﬁne new weaker conditions
(monotonicity and optimism) that capture a wide variety of practical settings in both the random
walk (one-armed bandit) setting and the MAB setting. Using this  our main theoretical contribution 
Theorem 7  signiﬁcantly generalizes the kinds of algorithms or rules for which we can mathematically
determine the sign of the bias for any problem instance. Our simulations conﬁrm the accuracy of
our theoretical predictions for a variety of practical situations for which such sign characterizations
were previously unknown. There are several natural followup directions: (a) extending results like
Corollary 8 to other bandit algorithms  (b) extending all our results to hold for other functionals of the
data like the sample variance  (c) characterizing the magnitude of the bias. We have recently made
signiﬁcant progress on the last question [Shin et al.  2019]  but the other two remain open.

References
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem.

In Conference on Learning Theory  pages 39–1  2012.

9

Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits.

In COLT  pages 217–226  2009.

Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine learning  47(2-3):235–256  2002.

Jack Bowden and Lorenzo Trippa. Unbiased estimation for response adaptive clinical trials. Statistical

methods in medical research  26(5):2376–2388  2017.

Aurélien Garivier and Olivier Cappé. The KL-UCB algorithm for bounded stochastic bandits and
beyond. In Proceedings of the 24th Annual Conference On Learning Theory  pages 359–376  2011.

Aurélien Garivier and Emilie Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence. In

Conference on Learning Theory  pages 998–1027  2016.

Allan Gut. Stopped random walks. Springer  2009.

Steven R Howard  Aaditya Ramdas  Jon McAuliffe  and Jasjeet Sekhon. Uniform  nonparametric 

non-asymptotic conﬁdence sequences. arXiv preprint arXiv:1810.08240  2018.

Kevin Jamieson  Matthew Malloy  Robert Nowak  and Sébastien Bubeck. lil’ UCB: An Optimal
Exploration Algorithm for Multi-Armed Bandits. In Proceedings of The 27th Conference on
Learning Theory  volume 35 of Proceedings of Machine Learning Research  pages 423–439  2014.

Shivaram Kalyanakrishnan  Ambuj Tewari  Peter Auer  and Peter Stone. Pac subset selection in

stochastic multi-armed bandits. In ICML  volume 12  pages 655–662  2012.

Emilie Kaufmann  Nathaniel Korda  and Rémi Munos. Thompson sampling: An asymptotically
optimal ﬁnite-time analysis. In International Conference on Algorithmic Learning Theory  pages
199–213. Springer  2012.

Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press  2019.

Xinkun Nie  Xiaoying Tian  Jonathan Taylor  and James Zou. Why adaptively collected data have
negative bias and how to correct for it. In International Conference on Artiﬁcial Intelligence and
Statistics  pages 1261–1269  2018.

Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematical Society  58(5):527–535  1952.

Herbert Robbins. Statistical methods related to the law of the iterated logarithm. The Annals of

Mathematical Statistics  41(5):1397–1409  1970.

Daniel Russo. Simple bayesian algorithms for best arm identiﬁcation. In Conference on Learning

Theory  pages 1417–1418  2016.

Jaehyeok Shin  Aaditya Ramdas  and Alessandro Rinaldo. On the bias  risk and consistency of

sample means in multi-armed bandits. arXiv preprint arXiv:1902.00746  2019.

David Siegmund. Estimation following sequential tests. Biometrika  65(2):341–349  1978.

Norman Starr and Michael B Woodroofe. Remarks on a stopping time. Proceedings of the National

Academy of Sciences of the United States of America  61(4):1215  1968.

Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT press Cambridge 

1998.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

Sofía S Villar  Jack Bowden  and James Wason. Multi-armed bandit models for the optimal design
of clinical trials: beneﬁts and challenges. Statistical science: a review journal of the Institute of
Mathematical Statistics  30(2):199  2015.

Min Xu  Tao Qin  and Tie-Yan Liu. Estimation bias in multi-armed bandit algorithms for search

advertising. In Advances in Neural Information Processing Systems  pages 2400–2408  2013.

10

,Xujie Si
Hanjun Dai
Mukund Raghothaman
Mayur Naik
Le Song
Jaehyeok Shin
Aaditya Ramdas
Alessandro Rinaldo