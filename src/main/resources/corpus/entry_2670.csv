2019,Input Similarity from the Neural Network Perspective,Given a trained neural network  we aim at understanding how similar it considers any two samples. For this  we express a proper definition of similarity from the neural network perspective (i.e. we quantify how undissociable two inputs A and B are)  by taking a machine learning viewpoint: how much a parameter variation designed to change the output for A would impact the output for B as well?

We study the mathematical properties of this similarity measure  and show how to estimate sample density with it  in low complexity  enabling new types of statistical analysis for neural networks. We also propose to use it during training  to enforce that examples known to be similar should also be seen as similar by the network.

We then study the self-denoising phenomenon encountered in regression tasks when training neural networks on datasets with noisy labels. We exhibit a multimodal image registration task where almost perfect accuracy is reached  far beyond label noise variance. Such an impressive self-denoising phenomenon can be explained as a noise averaging effect over the labels of similar examples. We analyze data by retrieving samples perceived as similar by the network  and are able to quantify the denoising effect without requiring true labels.,Input Similarity from the Neural Network Perspective

Guillaume Charpiat1

Nicolas Girard2

Loris Felardos1

Yuliya Tarabalka2 3

1 TAU team  INRIA Saclay  LRI  Univ. Paris-Sud

2 TITANE team  INRIA Sophia-Antipolis  Univ. Côte d’Azur

3 LuxCarta Technology

firstname.lastname@inria.fr

Abstract

Given a trained neural network  we aim at understanding how similar it considers
any two samples. For this  we express a proper deﬁnition of similarity from the
neural network perspective (i.e. we quantify how undissociable two inputs A and
B are)  by taking a machine learning viewpoint: how much a parameter variation
designed to change the output for A would impact the output for B as well?
We study the mathematical properties of this similarity measure  and show how to
estimate sample density with it  in low complexity  enabling new types of statistical
analysis for neural networks. We also propose to use it during training  to enforce
that examples known to be similar should also be seen as similar by the network.
We then study the self-denoising phenomenon encountered in regression tasks when
training neural networks on datasets with noisy labels. We exhibit a multimodal
image registration task where almost perfect accuracy is reached  far beyond label
noise variance. Such an impressive self-denoising phenomenon can be explained
as a noise averaging effect over the labels of similar examples. We analyze data by
retrieving samples perceived as similar by the network  and are able to quantify the
denoising effect without requiring true labels.

Introduction

1
The notion of similarity between data points is an important topic in the machine learning literature 
obviously in domains such as image retrieval  where images similar to a query have to be found;
but not only. For instance when training auto-encoders  the quality of the reconstruction is usually
quantiﬁed as the L2 norm between the input and output images. Such a similarity measure is however
questionable  as color comparison  performed pixel per pixel  is a poor estimate of human perception:
the L2 norm can vary a lot with transformations barely noticeable to the human eye such as small
translations or rotations (for instance on textures)  and does not carry semantic information  i.e.
whether the same kind of objects are present in the image. Therefore  so-called perceptual losses [10]
were introduced to quantify image similarity: each image is fed to a standard pre-trained network
such as VGG  and the activations in a particular intermediate layer are used as descriptors of the
image [4  5]. The distance between two images is then set as the L2 norm between these activations.
Such a distance carries implicitly semantic information  as the VGG network was trained for image
classiﬁcation. However  the choice of the layer to consider is arbitrary. In the ideal case  one would
wish to combine information from all layers  as some are more abstract and some more detail-speciﬁc.
Then  how to choose the weights to combine the different layers? Would it be possible to build a
canonical similarity measure  well posed theoretically?
More importantly  the previous literature does not consider the notion of input similarity from the
point of view of the neural network that is being used  but from the point of view of another one
(typically  VGG) which aims at imitating human perception. Yet  neural networks are black boxes
difﬁcult to interpret  and showing which samples a network considers as similar would help to explain
its decisions. Also  the number of such similar examples would be a key element for conﬁdence
estimation at test time. Moreover  to explain the self-denoising phenomenon  i.e. why predictions can

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

be far more accurate than the label noise magnitude in the training set  thanks to a noise averaging
effect over similar examples [11]  one needs to quantify similarity according to the network.
The purpose of this article is to express the notion of similarity from the network’s point of view.
We ﬁrst deﬁne it  and study it mathematically  in Section 2  in the one-dimensional output case for
the sake of simplicity. Higher-dimensional outputs are dealt with in Section 3. We then compute 
in Section 4  the number of neighbors (i.e.  of similar samples)  and propose for this a very fast
estimator. This brings new tools to analyze already-trained networks. As they are differentiable and
fast to compute  they can be used during training as well  e.g.  to enforce that given examples should
be perceived as similar by the network (c.f . supp. mat.). Finally  in Section 5  we apply the proposed
tools to analyze a network trained with noisy labels for a remote sensing image alignment task  and
formalize the self-denoising phenomenon  quantifying its effect  extending [11] to real datasets.

2 Similarity
In this section we deﬁne a proper  intrinsic notion of similarity as seen by the network  relying on
how easily it can distinguish different inputs.

2.1 Similarity from the point of view of the parameterized family of functions
Let fθ be a parameterized function  typically a neural network already trained for some task  and x  x(cid:48)
possible inputs  for instance from the training or test set. For the sake of simplicity  let us suppose in
a ﬁrst step that fθ is real valued. To express the similarity between x and x(cid:48)  as seen by the network 
one could compare the output values fθ(x) and fθ(x(cid:48)). This is however not very informative  and a
same output might be obtained for different reasons.
Instead  we deﬁne similarity as the inﬂuence of x over x(cid:48)  by quantifying how much an additional
training step for x would change the output for x(cid:48) as well. If x and x(cid:48) are very different from the
point of view of the neural network  changing fθ(x) will have little consequence on fθ(x(cid:48)). Vice
versa  if they are very similar  changing fθ(x) will greatly affect fθ(x(cid:48)) as well.

Figure 1: Moves in the space of out-
puts. We quantify the inﬂuence of a
data point x over another one x(cid:48) by
how much the tuning of parameters θ
to obtain a desired output change v for
fθ(x) will affect fθ(x(cid:48)) as well.

Formally  if one wants to change the value of fθ(x) by a small quantity ε  one needs to update θ by
δθ = ε

∇θfθ(x)
(cid:107)∇θfθ(x)(cid:107)2 . Indeed  after the parameter update  the new value at x will be:

fθ+δθ(x) = fθ(x) + ∇θfθ(x) · δθ + O((cid:107)δθ(cid:107)2) = fθ(x) + ε + O(ε2).
∇θfθ(x(cid:48)) · ∇θfθ(x)

This parameter change induces a value change at any other point x(cid:48) :
fθ+δθ(x(cid:48)) = fθ(x(cid:48)) + ∇θfθ(x(cid:48)) · δθ + O((cid:107)δθ(cid:107)2) = fθ(x(cid:48)) + ε

(cid:107)∇θfθ(x)(cid:107)2

+ O(ε2).

∇θfθ(x) · ∇θfθ(x(cid:48))

(cid:107)∇θfθ(x)(cid:107)2

θ (x  x(cid:48)) =

represents the inﬂuence of x over x(cid:48): if one
θ (x  x(cid:48)). In particular 
θ (x  x(cid:48)) is high  then x and x(cid:48) are not distinguishable from the point of view of the network  as
θ (x  x(cid:48)) as a measure

Therefore the kernel kN
wishes to change the output value fθ(x) by ε  then fθ(x(cid:48)) will change by ε kN
if kN
any attempt to move fθ(x) will move fθ(x(cid:48)) as well (see Fig. 1). We thus see kN
of similarity. Note however that kN
Symmetric similarity: correlation Two symmetric kernels natural arise: the inner product:

θ (x  x(cid:48)) is not symmetric.

and its normalized version  the correlation:
θ (x  x(cid:48)) =
kC

θ (x  x(cid:48)) = ∇θfθ(x) · ∇θfθ(x(cid:48))
kI
∇θfθ(x)
(cid:107)∇θfθ(x)(cid:107) · ∇θfθ(x(cid:48))
(cid:107)∇θfθ(x(cid:48))(cid:107)

(2)
which has the advantage of being bounded (in [−1  1])  thus expressing similarity in a usual meaning.

(1)

2

θf (x’)θvv’f (x)2.2 Properties for vanilla neural networks

Intuitively  inputs that are similar from the network perspective should produce similar outputs;
we can check that kC
θ is a good similarity measure in this respect (all proofs are deferred to the
supplementary materials):
Theorem 1. For any real-valued neural network fθ whose last layer is a linear layer (without any
parameter sharing) or a standard activation function thereof (sigmoid  tanh  ReLU...)  and for any
inputs x and x(cid:48) 

∇θfθ(x) = ∇θfθ(x(cid:48)) =⇒ fθ(x) = fθ(x(cid:48)) .

Corollary 1. Under the same assumptions  for any inputs x and x(cid:48) 

θ (x  x(cid:48)) = 1 =⇒ ∇θfθ(x) = ∇θfθ(x(cid:48))  
kC
θ (x  x(cid:48)) = 1 =⇒ fθ(x) = fθ(x(cid:48)) .
hence kC

Furthermore 
Theorem 2. For any real-valued neural network fθ without parameter sharing  if ∇θfθ(x) =
∇θfθ(x(cid:48)) for two inputs x  x(cid:48)  then all useful activities computed when processing x are equal to the
ones obtained when processing x(cid:48).
We name useful activities all activities ai(x) whose variation would have an impact on the output 
(cid:54)= 0. This condition is typically not satisﬁed when the activity is
i.e. all the ones satisfying dfθ(x)
dai
negative and followed by a ReLU  or when it is multiplied by a 0 weight  or when all its contributions
to the output cancel one another (e.g.  a sum of two neurons with opposite weights: fθ(x) =
σ(ai(x)) − σ(ai(x))).

i

ai(x)  where wj

fθ(x) = dfθ(x)
dbj

Link with the perceptual loss For a vanilla network without parameter sharing  the gradient
∇θfθ(x) is a list of coefﬁcients ∇wj
i is the parameter-factor that
multiplies the input activation ai(x) in neuron j  and of coefﬁcients ∇bj fθ(x) = dfθ(x)
for neuron
biases  which we will consider as standard parameters bj = wj
0 that act on a constant activation
a0(x). Thus the gradient ∇θfθ(x) can be seen as a list of all
a0(x) = 1  yielding ∇wj
activation values ai(x) multiplied by the potential impact on the output fθ(x) of the neurons j using
them  i.e. dfθ(x)
. Each activation appears in this list as many times as it is fed to different neurons.
dbj
The similarity between two inputs then rewrites:

fθ(x) = dfθ(x)
dbj

dbj

0

θ (x  x(cid:48)) =
kI

λi(x  x(cid:48)) ai(x) ai(x(cid:48))

where

λi(x  x(cid:48)) =

activities i

neuron j using ai

dfθ(x)

dbj

dfθ(x(cid:48))
dbj

(cid:88)

(cid:88)

are data-dependent importance weights. Such weighting schemes on activation units naturally
arise when expressing intrinsic quantities; the use of natural gradients would bring invariance to
re-parameterization [16  17]. On the other hand  the inner product related to the perceptual loss would
be

λlayer(i) ai(x) ai(x(cid:48))

(cid:88)

activities i(cid:54)=0

for some arbitrary ﬁxed layer-dependent weights λlayer(i).

2.3 Properties for parameter-sharing networks
When sharing weights  as in convolutional networks  the gradient ∇θfθ(x) is made of the same
coefﬁcients (impact-weighted activations) but summed over shared parameters. Denoting by S(i) the
set of (neuron  input activity) pairs where the parameter wi is involved 

(cid:88)

 (cid:88)

 (cid:88)



ak(x(cid:48))

dfθ(x(cid:48))
dbj

θ (x  x(cid:48)) =
kI

ak(x)

dfθ(x)

dbj

(j k)∈Si
Thus  in convolutional networks  kI
θ similarity does not imply similarity of ﬁrst layer activations
anymore  but only of their (impact-weighted) spatial average. More generally  any invariance

(j k)∈Si

params i

3

introduced by a weight sharing scheme in an architecture will be reﬂected in the similarity measure
θ was deﬁned as the input similarity from the neural network perspective.
θ  which is expected as kI
kI
Note that this type of objects was recently studied from an optimization viewpoint under the name of
Neural Tangent Kernel [9  1] in the inﬁnite layer width limit.

3 Higher output dimension

Let us now study the more complex case where fθ(x) is a vector(cid:0)f i
θ(x)(cid:1)

i∈[1 d] in Rd with d > 1.
Under a mild hypothesis on the network (output expressivity)  always satisﬁed unless specially
designed not to:
Theorem 3. The optimal parameter change δθ to push fθ(x) in a direction v ∈ Rd (with a force
ε ∈ R)  i.e. such that fθ+δθ(x) − fθ(x) = εv  induces at any other point x(cid:48) the following output
variation:
where the d × d kernel matrix Kθ(x(cid:48)  x) is deﬁned by K ij
The similarity kernel is now a matrix and not just a single value  as it describes the relation between
moves v ∈ Rd. Note that these matrices Kθ are only d × d where d is the output dimension. They
are thus generally small and easy to manipulate or inverse.

fθ+δθ(x(cid:48)) − fθ(x(cid:48)) = ε Kθ(x(cid:48)  x) Kθ(x  x)−1 v + O(ε2)

θ (x(cid:48)  x) = ∇θf i

θ(x(cid:48)) · ∇θf j

θ (x).

(3)

θ (x  x(cid:48)) = Kθ(x  x)−1/2 Kθ(x  x(cid:48)) Kθ(x(cid:48)  x(cid:48))−1/2 .
K C

Normalized similarity matrix The unitless symmetrized  normalized version of the kernel (3) is:
(4)
It has the following properties: its coefﬁcients are bounded  in [−1  1]; its trace is at most d; its
(Frobenius) norm is at most
θ (x  x) = Id; the kernel is
symmetric  in the sense that K C

√
d; self-similarity is identity: ∀x  K C
θ (x(cid:48)  x) = K C

θ (x  x(cid:48))T .

Similarity in a single value To summarize the similarity matrix K C
in [−1  1]  we consider:

θ (x  x(cid:48)) into a single real value
(5)
θ (x  x(cid:48)) is close to Id  and recipro-
It can be shown indeed that if kC
cally. See the supplementary materials for more details and a discussion about the links between
1
d Tr K C

θ (x  x(cid:48)) .
θ (x  x(cid:48)) =
kC
θ (x  x(cid:48)) is close to 1  then K C

θ (x  x(cid:48)) and(cid:13)(cid:13)K C

θ (x  x(cid:48)) − Id(cid:13)(cid:13)F .

Tr K C

1
d

Metrics on output: rotation invariance Similarity in Rd might be richer than just estimating
distances in L2 norm. For instance  for our 2D image registration task  the network could be known
(or desired) to be equivariant to rotations. The similarity between two output variations v and v(cid:48) can
be made rotation-invariant by applying the rotation that best aligns v and v(cid:48) beforehand. This can
actually be easily computed in closed form and yields:

kC rot
θ

(x  x(cid:48)) =

1
2

(cid:113)(cid:13)(cid:13)K C

θ (x  x(cid:48))(cid:13)(cid:13)2
(cid:12)(cid:12)fθ(x)(v). It has a particular meaning though  and is

F + 2 det K C

θ (x  x(cid:48)) .

Note that other metrics are possible in the output space. For instance  the loss metric quantiﬁes the
norm of a move v by its impact on the loss dL(y)
dy
not always relevant  e.g. in the noisy label case seen in Section 5.

The case of classiﬁcation tasks When the output of the network is a probability distribution
pθ x(c)  over a ﬁnite number of given classes c for example  it is natural from an information theoretic
θ (x) = − log pθ x(c). This is actually the quantities computed in
point of view to rather consider f c
the pre-softmax layer from which common practice directly computes the cross-entropy loss.
It turns out that the L2 norm of variations δf in this space naturally corresponds to the Fisher
information metric  which quantiﬁes the impact of parameter variations δθ on the output probability
c c(cid:48) and Fθ x =

pθ x  as KL(pθ x||pθ+δθ x). The matrices Kθ(x  x) = (cid:0)∇θf c
(cid:2)∇θf c

θ (x)T(cid:3) are indeed to each other what correlation is to covariance. Thus the

Ec
quantities deﬁned in Equation (5) already take into account information geometry when applied to
the pre-softmax layer  and do not need supplementary metric adjustment.

θ (x)(cid:1)

θ (x) · ∇θf c(cid:48)

θ (x) ∇θf c

4

Faster setup for classiﬁcation tasks with many classes
In a classiﬁcation task in d classes with
large d  the computation of d × d matrices may be prohibitive. As a workaround  for a given input
training sample x  the classiﬁcation task can be seen as a binary one (the right label cR vs. the other
ones)  in which case the d outputs of the neural network can be accordingly combined in a single real
value. The 1D similarity measure can then be used to compare any training samples of the same class.

When making statistics on similarity values Ex(cid:48)(cid:2)kC

θ (x  x(cid:48))(cid:3)  another possible task binarization

approach is to sample an adversary class cA along with x(cid:48)  and hence consider ∇θf cR
θ (x).
Both approaches will lead to similar results in the Enforcing Similarity section in the supplementary
materials.

θ (x)−∇θf cA

4 Estimating density

In this section  we use similarity to estimate input neighborhoods and perform statistics on them.

4.1 Estimating the number of neighbors
Given a point x  how many samples x(cid:48) are similar to x according to the network? This can be
θ (x  x(cid:48)) for all x(cid:48) and picking the closest ones  i.e. e.g. the x(cid:48) such that
measured by computing kC
θ (x  x(cid:48)) (cid:62) 0.9. More generally  for any data point x  the histogram of the similarity kC
θ (x  x(cid:48)) over
kC
all x(cid:48) in the dataset (or a representative subset thereof) can be drawn  and turned into an estimate of
the number of neighbors of x. To do this  several types of estimates are possible:

• hard-thresholding  for a given threshold τ ∈ [0  1]:
• soft estimate:
• less-soft positive-only estimate (α > 0):

θ (x x(cid:48))(cid:62)τ
θ (x  x(cid:48))
θ (x  x(cid:48))α
θ is very rarely negative  and thus the soft estimate NS can be justiﬁed

x(cid:48) 1kC
x(cid:48) kC
θ (x x(cid:48))>0 kC

α (x) = (cid:80)

In practice we observe that kC
as an average of the hard-thresholding estimate Nτ over all possible thresholds τ:

x(cid:48) 1kC

N +

Nτ (x) =(cid:80)
NS(x) = (cid:80)

Nτ (x)dτ =

1kC

θ (x x(cid:48))(cid:62)τ dτ =

θ (x  x(cid:48)) 1kC
kC

θ (x x(cid:48))(cid:62)0 = N +

1 (x) (cid:39) NS(x)

(cid:88)

x(cid:48)

(cid:90) 1

τ =0

(cid:88)

(cid:90) 1

(cid:88)

x(cid:48)

τ =0

(cid:88)

4.2 Low complexity of the soft estimate NS(x)

The soft estimate NS(x) is rewritable as:

(cid:88)

x(cid:48)

θ (x  x(cid:48)) =
kC

(cid:107)∇θfθ(x(cid:48))(cid:107) =

(cid:107)∇θfθ(x)(cid:107) · ∇θfθ(x(cid:48))
∇θfθ(x)

∇θfθ(x(cid:48))
(cid:107)∇θfθ(x(cid:48))(cid:107)
x(cid:48)
and consequently NS(x) can be computed jointly for all x in linear time O(|D|p) in the dataset
size |D| and in the number of parameters p  in just two passes over the dataset  when the output
dimension is 1. For higher output dimensions d  a similar trick can be used and the complexity
becomes O(|D|d2p). For classiﬁcation tasks with a large number d of classes  the complexity can be
reduced to O(|D|p) through an approximation consisting in binarizing the task (c.f . end of Section 3).

∇θfθ(x)
(cid:107)∇θfθ(x)(cid:107) · g with g =

x(cid:48)

4.3 Test of the various estimators

In order to rapidly test the behavior of all possible estimators  we applied them to a toy problem
where the network’s goal is to predict a sinusoid. To change the difﬁculty of the problem  we vary its
frequency  while keeping the number of samples constant. More details and results of the toy problem
are in the supplementary materials. Fig. 2 shows for each estimator (with different parameters when
relevant)  the result of their neighbor count estimation. When the frequency f of the sinusoid to
predict increases  the number of neighbors decreases in 1
f for every estimator. This aligns with our
intuition that as the problem gets harder  the network needs to distinguish input samples more to
achieve a good performance  thus the amount of neighbors is lower. In particular we observe that
the proposed NS(x) estimator behaves well  thus we will use that one in bigger studies requiring an
efﬁcient estimator.

5

Figure 2: Density estimation using the various approaches (log scale). All approaches behave
similarly and show good results  except the ones with extreme thresholds.

4.4 Further potential uses for ﬁtness estimation

When the number of neighbors of a training point x is very low  the network is able to set any label to
x  as this won’t interfere with other points  by deﬁnition of our similarity criterion kθ(x  x(cid:48)). This
is thus a typical overﬁt case  where the network can learn by heart a label associated to a particular 
isolated point.
On the opposite  when the set of neighbors of x is a large fraction of the dataset  comprising varied
elements  by deﬁnition of kθ(x  x(cid:48)) the network is not able to distinguish them  and consequently it
can only provide a common output for all of them. Therefore it might not be able to express variety
enough  which would be a typical underﬁt case.
The quality of ﬁt can thus be observed by monitoring the number of neighbors together with the
variance of the desired labels in the neighborhoods (to distinguish underﬁt from just high density).

Prediction uncertainty A measure of the uncertainty of a prediction fθ(x) could be to check how
easy it would have been to obtain another value during training  without disturbing the training of
(cid:107)∇θfθ(x)(cid:107)2 v over other points x(cid:48) of the
other points. A given change v of fθ(x) induces changes
(cid:107)∇θfθ(x)(cid:107)2 v(cid:107). The uncertainty factor would then be

dataset  creating a total L1 disturbance(cid:80)

x(cid:48) (cid:107) kI

the norm of v affordable within a disturbance level  and quickly approximable as (cid:107)∇θfθ(x)(cid:107)2
θ (x x(cid:48)).

x(cid:48) kI

θ (x x(cid:48))
kI

θ (x x(cid:48))

(cid:80)

5 Dataset self-denoising
5.1 Motivation: example of remote sensing image registration with noisy labels

In remote sensing imagery  data is abundant but noisy [14]. For instance RGB satellite images
and binary cadaster maps (delineating buildings) are numerous but badly aligned for various rea-
sons (annotation mistakes  atmosphere disturbance  elevation variations...). In a recent preliminary
work [6]  we tackled the task of automatically registering these two types of images together with
neural networks  training on a dataset [13] with noisy annotations from OSM[18]  and hoping the
network would be able to learn from such a dataset of imperfect alignments. Learning with noisy
labels is indeed an active topic of research [21  15  12].
For this  we designed an iterative approach: train  then use the outputs of the network on the training
set to re-align it; repeat (for 3 iterations). The results were surprisingly good  yielding far better
alignments than the ground truth it learned from  both qualitatively (Figure 3) and quantitatively
(Figure 4  obtained on manually-aligned data): the median registration error dropped from 18 pixels to
3.5 pixels  which is the best score one could hope for  given intrinsic ambiguities in such registration
task. To check that this performance was not due to a subset of the training data that would be
perfectly aligned  we added noise to the ground truth and re-trained from it: the new results were
about as good again (dashed lines). Thus the network did learn almost perfectly just from noisy labels.

6

0.00.51.01.52.02.5Frequency (log)34567Neighbor count (log)Avg of all measures across all samplesneighbors_softneighbors_less_soft_n_2neighbors_less_soft_n_3neighbors_less_soft_n_4neighbors_hard_t_0.5neighbors_hard_t_0.6neighbors_hard_t_0.7neighbors_hard_t_0.8neighbors_hard_t_0.9neighbors_hard_t_0.925neighbors_hard_t_0.95neighbors_hard_t_0.975neighbors_hard_t_0.99Figure 3: Qualitative alignment results [6]
on a crop of bloomington22 from the Inria
dataset [13]. Red: initial dataset annota-
tions; blue: aligned annotations round 1;
green: aligned annotations round 2.

Figure 4: Accuracy cumulative distributions [6] mea-
sured with the manually-aligned annotations of bloom-
ington22 [13]. Read as: fraction of image pixels whose
registration error is less than threshold τ.

An explanation for this self-denoising phenomenon is proposed in [11] as follows. Let us consider a
regression task  with a L2 loss  and where true labels y were altered with i.i.d. noise ε of variance v.

Suppose a same input x appears n times in the training set  thus with n different labels(cid:101)yi = y + εi.
(cid:80)
i(cid:101)yi  whose distance to the

The network can only output the same prediction for all these n cases (since the input is the same) 
and the best option  considering the L2 loss  is to predict the average 1
n
true label y is O( v√
n can be observed. However  the exact
same point x is not likely to appear several times in a dataset (with different labels). Rather  relatively
similar points may appear  and the amplitude of the self-denoising effect will be a function of their
number. Here  the similarity should reﬂect the neural network perception (similar inputs yield the
same output) and not an a priori norm chosen on the input space.

n ). Thus a denoising effect by a factor

√

5.2 Similarity experimentally observed between patches

We studied the multi-round training scheme of [6] by applying our similarity measure to a sampling
of input patches of the training dataset for one network per round. The principle of the multiple
round training scheme is to reduce the noise of the annotations  obtaining aligned annotations in
the end (more details in the supplementary materials). For a certain input patch  we computed its
similarity with all the other patches for the 3 networks. With those similarities we can compute the
nearest neighbors of that patch  see Fig. 5. The input patch is of a suburb area with sparse houses
and individual trees. The closest neighbors look similar as they usually feature the same types of
buildings  building arrangement and vegetation. However sometimes the network sees a patch as
similar when it is not clear from our point of view (for example patches with large buildings).
For more in-depth results  we computed the histogram of similarities for the same patch  see Fig. 6.
We observe that round 2 shows different neighborhood statistics  in that the patch is closer to all
other patches than in other rounds. We observe the same behavior in 19 other input patches (see
suppl. materials). An hypothesis for this phenomenon is that the average gradient was not 0 at the end
of that training round (due to optimization convergence issues  e.g.)  which would shift all similarity
histograms by a same value.
Qualitatively  for patches randomly sampled  their similarity histograms tend to be approximately
symmetric in round 2  but with a longer left tail in round 1 and a longer right tail in round 3.
Neighborhoods thus seem to change across the rounds  with fewer and fewer close points (if removing
the global histogram shift in round 2). A possible interpretation is that this would reﬂect an increasing
ability of the network to distinguish between different patches  with ﬁner features in later training
rounds.

5.3 Comparison to the perceptual loss

We compare our approach to the perceptual loss on a nearest neighbor retrieval task. We notice that
the perceptual loss sometimes performs reasonably well  but often not. For instance  we show in
Fig. 7 the closest neighbors to a structured residential area image  for the perceptual loss (ﬁrst row:
not making sense) and for our similarity measure (second row: similar areas).

7

Figure 5: Example of nearest neighbors for a patch. Each line corresponds to a round. Each patch
has its similarity written under it.

(a) Round 1

(b) Round 2

(c) Round 3

Figure 6: Histograms of similarities for one patch across rounds.

5.4 From similarity statistics to self-denoising effect estimation

We now show how such similarity experimental computations can be used to solve the initial problem
of Section 5  by explicitly turning similarity statistics into a quantiﬁcation of the self-denoising effect.

Let us denote by yi the true (unknown) label for input xi  by(cid:101)yi the noisy label given in the dataset 
and by(cid:98)yi = fθ(xi) the label predicted by the network. We will denote the (unknown) noise by
εi = (cid:101)yi − yi and assume it is centered and i.i.d.  with ﬁnite variance σε. The training criterion
is E(θ) = (cid:80)
j ||(cid:98)yj −(cid:101)yj||2. At convergence  the training leads to a local optimum of the energy
landscape: ∇θE = 0  that is (cid:80)
j((cid:98)yj −(cid:101)yj)∇θ(cid:98)yj = 0. Let’s choose any sample i and multiply by
θ (xi  xj) = ∇θ(cid:98)yi.∇θ(cid:98)yj   we get:
∇θ(cid:98)yi : using kI
(cid:88)
((cid:98)yj −(cid:101)yj) kI
θ (xj  xi)(cid:0)(cid:80)

θ (xj  xi)(cid:1)−1 the column-normalized kernel  and

θ (xj  xi) = 0.

(xj  xi) = kI

j kI

(xj  xi) the mean value of a in the neighborhood of i  that is  the weighted

by Ek[a] = (cid:80)

Let us denote by kIN

θ

j aj kIN

θ

j

l
a
u
t
p
e
c
r
e
P

y
t
i
r
a
l
i

m
S

i

Source | Closest neighbor patches

Figure 7: Closest neighbors to the leftmost patch  using the perceptual loss (ﬁrst row) and our
similarity deﬁnition (second row).

8

0.00.20.40.60.81.0Similarity0.00.10.20.30.40.50.60.70.8FrequencyNeighbors soft: 1276.70.00.20.40.60.81.0Similarity0.00.10.20.30.40.50.60.70.8FrequencyNeighbors soft: 2486.30.00.20.40.60.81.0Similarity0.00.10.20.30.40.50.60.70.8FrequencyNeighbors soft: 394.6k

k

[y] = E
k

(cid:98)yi − E

from the average prediction in its neighborhood.

average of the aj with weights kI
θ (xj  xi) normalized to sum up to 1. This is actually a kernel
regression  in the spirit of Parzen-Rosenblatt window estimators. Then the previous property can be

rewritten as Ek[(cid:98)y] = Ek[(cid:101)y] . As Ek[(cid:101)y] = Ek[y] + Ek[ε]   this yields:
[ε] + ((cid:98)yi − E
[(cid:98)y])
i.e. the difference between the predicted(cid:98)yi and the average of the true labels in the neighborhood of i
is equal to the average of the noise in the neighborhood of i  up to the deviation of the prediction(cid:98)yi
We want to bound the error (cid:107)(cid:98)yi − Ek[y](cid:107) without knowing neither the true labels y nor the noise ε.

One can show that Ek[ε] ∝ varε(Ek[ε])1/2 = σε (cid:107)kIN
similarity kernel norm (cid:107)kIN
hood quality. It is 1/
the other extreme  this factor is 1 when all points are independent: kI
way we extend noise2noise [11] to real datasets with non-identical inputs.
In our remote sensing experiment  we estimate this way a denoising factor of 0.02  consistent across
all training rounds and inputs (±10%)  implying that each training round contributed equally to
denoising the labels. This is conﬁrmed by Fig. 4  which shows the error steadily decreasing  on a

(·  xi)(cid:107)L2. The denoising factor is thus the
N and 1  depending on the neighbor-
θ (xi  xj) = 1. On
θ (xi  xj) = 0 ∀i (cid:54)= j. This

control test where true labels are known. The shift ((cid:98)yi − Ek[(cid:98)y]) on the other hand can be directly

N when all N data points are identical  i.e. all satisfying kC

(·  xi)(cid:107)L2  which is between 1/

√

√

θ

θ

estimated given the network prediction. In our case  it is 4.4px on average  which is close to the
observed median error for the last round in Fig. 4. It is largely input-dependent  with variance 3.2px 
which is reﬂected by the spread distribution of errors in Fig. 4. This input-dependent shift thus
provides a hint about prediction reliability.

It is also possible to bound ((cid:98)yi − Ek[(cid:98)y]) = Ek[(cid:98)yi −(cid:98)y] using only similarity information (without
predictions(cid:98)y). Theorem 1 implies that the application: ∇θfθ(x)
(cid:107)∇θfθ(x)(cid:107) (cid:55)→ fθ(x) is well-deﬁned  and it can
(cid:13)(cid:13)(cid:13)(cid:13) =
(cid:13)(cid:13)(cid:13)(cid:13) ∇θfθ(x)
(cid:113)
actually be shown to be Lipschitz with a network-dependent constant (under mild hypotheses). Thus
(cid:107)∇θfθ(x)(cid:107) − ∇θfθ(x(cid:48))
(cid:107)∇θfθ(x(cid:48))(cid:107)
θ (xi  xj) and thus(cid:12)(cid:12) Ek[(cid:98)yi −(cid:98)y](cid:12)(cid:12) (cid:54) √

yielding (cid:107)(cid:98)yi −(cid:98)yj(cid:107) (cid:54) √

(cid:107)fθ(x) − fθ(x(cid:48))(cid:107) (cid:54) C

θ (x  x(cid:48))  

θ (xi ·)

(cid:104)(cid:113)

1 − kC

1 − kC

1 − kC

(cid:113)

2C Ek

(cid:105)

√

2C

2C

.

6 Conclusion

We deﬁned a proper notion of input similarity as perceived by the neural network  based on the ability
of the network to distinguish the inputs. This brings a new tool to analyze trained networks  in plus
of visualization tools such as grad-CAM [20]. We showed how to turn it into a density estimator 
which was validated on a controlled experiment  and usable to perform fast statistics on large datasets.
It opens the door to underﬁt/overﬁt/uncertainty analyses or even control during training  as it is
differentiable and computable at low cost.
In the supplementary materials  we go further in that direction and show that  if two or more samples
are known to be similar (from a human point of view)  it is possible to incite the network  during
training  to evolve in order to consider these samples as similar. We notice an associated dataset-
dependent boosting effect that should be further studied along with robustness to adversarial attacks 
as such training differs signiﬁcantly from usual methods.
Finally  we extended noise2noise [11] to the case of non-identical inputs  thus expressing self-
denoising effects as a function of inputs’ similarities.
The code is available at https://github.com/Lydorn/netsimilarity .

Acknowledgments

We thank Victor Berger and Adrien Bousseau for useful discussions. This work beneﬁted from the sup-
port of the project EPITOME ANR-17-CE23-0009 of the French National Research Agency (ANR).

9

References
[1] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming. arXiv

preprint arXiv:1812.07956  2018.

[2] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on

machine learning  pages 2990–2999  2016.

[3] Harris Drucker and Yann Le Cun. Double backpropagation increasing generalization performance. In
IJCNN-91-Seattle International Joint Conference on Neural Networks  volume 2  pages 145–150. IEEE 
1991.

[4] Leon Gatys  Alexander S Ecker  and Matthias Bethge. Texture synthesis using convolutional neural

networks. In Advances in neural information processing systems  pages 262–270  2015.

[5] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint

arXiv:1508.06576  2015.

[6] Nicolas Girard  Guillaume Charpiat  and Yuliya Tarabalka. Noisy Supervision for Correcting Misaligned
Cadaster Maps Without Perfect Ground Truth Data. In IGARSS  July 2019. URL https://hal.inria.
fr/hal-02065211.

[7] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville. Improved
training of wasserstein gans. In Advances in Neural Information Processing Systems  pages 5767–5777 
2017.

[8] Sepp Hochreiter and Jürgen Schmidhuber. Simplifying neural nets by discovering ﬂat minima. In Advances

in neural information processing systems  pages 529–536  1995.

[9] Arthur Jacot  Franck Gabriel  and Clément Hongler. Neural tangent kernel: Convergence and generalization

in neural networks. In Advances in neural information processing systems  pages 8571–8580  2018.

[10] Justin Johnson  Alexandre Alahi  and Li Fei-Fei. Perceptual losses for real-time style transfer and

super-resolution. In European conference on computer vision  pages 694–711. Springer  2016.

[11] Jaakko Lehtinen  Jacob Munkberg  Jon Hasselgren  Samuli Laine  Tero Karras  Miika Aittala  and Timo
Aila. Noise2noise: Learning image restoration without clean data. In International Conference on Machine
Learning  pages 2971–2980  2018.

[12] Yuncheng Li  Jianchao Yang  Yale Song  Liangliang Cao  Jiebo Luo  and Li-Jia Li. Learning from noisy
labels with distillation. In Proceedings of the IEEE International Conference on Computer Vision  pages
1910–1918  2017.

[13] Emmanuel Maggiori  Yuliya Tarabalka  Guillaume Charpiat  and Pierre Alliez. Can semantic labeling

methods generalize to any city? the Inria aerial image labeling benchmark. In IGARSS  2017.

[14] Volodymyr Mnih and Geoffrey E Hinton. Learning to label aerial images from noisy data. In Proceedings

of the 29th International conference on machine learning (ICML-12)  pages 567–574  2012.

[15] Nagarajan Natarajan  Inderjit S Dhillon  Pradeep K Ravikumar  and Ambuj Tewari. Learning with noisy

labels. In Advances in neural information processing systems  pages 1196–1204  2013.

[16] Yann Ollivier. Riemannian metrics for neural networks I: feedforward networks.

Information and
Inference: A Journal of the IMA  4(2):108–153  03 2015. ISSN 2049-8772. doi: 10.1093/imaiai/iav006.
URL https://doi.org/10.1093/imaiai/iav006.

[17] Yann Ollivier. Riemannian metrics for neural networks II: recurrent networks and learning symbolic data
sequences. Information and Inference: A Journal of the IMA  4(2):154–193  03 2015. ISSN 2049-8772.
doi: 10.1093/imaiai/iav007. URL https://doi.org/10.1093/imaiai/iav007.

[18] OpenStreetMap contributors. Planet dump retrieved from https://planet.osm.org   2017.

[19] Salah Rifai  Pascal Vincent  Xavier Muller  Xavier Glorot  and Yoshua Bengio. Contractive auto-encoders:
Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on
International Conference on Machine Learning  pages 833–840. Omnipress  2011.

[20] Ramprasaath R Selvaraju  Michael Cogswell  Abhishek Das  Ramakrishna Vedantam  Devi Parikh  and
Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
Proceedings of the IEEE International Conference on Computer Vision  pages 618–626  2017.

[21] Sainbayar Sukhbaatar  Joan Bruna  Manohar Paluri  Lubomir Bourdev  and Rob Fergus. Training convolu-

tional networks with noisy labels. arXiv preprint arXiv:1406.2080  2014.

10

,Anna Volokitin
Gemma Roig
Tomaso Poggio
Guillaume Charpiat
Nicolas Girard
Loris Felardos
Yuliya Tarabalka