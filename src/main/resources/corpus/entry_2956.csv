2019,Learning Mixtures of Plackett-Luce Models from Structured Partial Orders,Mixtures of ranking models have been widely used for heterogeneous preferences. However  learning a mixture model is highly nontrivial  especially when the dataset consists of partial orders. In such cases  the parameter of the model may not be even identifiable. In this paper  we focus on three popular structures of partial orders: ranked top-$l_1$  $l_2$-way  and choice data over a subset of alternatives. We prove that when the dataset consists of combinations of ranked top-$l_1$ and $l_2$-way (or choice data over up to $l_2$ alternatives)  mixture of $k$ Plackett-Luce models is not identifiable when $l_1+l_2\le 2k-1$ ($l_2$ is set to $1$ when there are no $l_2$-way orders). We also prove that under some combinations  including ranked top-$3$  ranked top-$2$ plus $2$-way  and choice data over up to $4$ alternatives  mixtures of two Plackett-Luce models are identifiable. Guided by our theoretical results  we propose efficient generalized method of moments (GMM) algorithms to learn mixtures of two Plackett-Luce models  which are proven consistent. Our experiments demonstrate the efficacy of our algorithms. Moreover  we show that when full rankings are available  learning from different marginal events (partial orders) provides tradeoffs between statistical efficiency and computational efficiency.,Learning Mixtures of Plackett-Luce Models from

Structured Partial Orders

Zhibing Zhao

Department of Computer Science
Rensselaer Polytechnic Institute

Troy  NY 12180
zhaoz6@rpi.edu

Lirong Xia

Department of Computer Science
Rensselaer Polytechnic Institute

Troy  NY 12180

xial@cs.rpi.edu

Abstract

Mixtures of ranking models have been widely used for heterogeneous preferences.
However  learning a mixture model is highly nontrivial  especially when the dataset
consists of partial orders. In such cases  the parameter of the model may not be even
identiﬁable. In this paper  we focus on three popular structures of partial orders:
ranked top-l1  l2-way  and choice data over a subset of alternatives. We prove that
when the dataset consists of combinations of ranked top-l1 and l2-way (or choice
data over up to l2 alternatives)  mixture of k Plackett-Luce models is not identiﬁable
when l1 +l2 ≤ 2k−1 (l2 is set to 1 when there are no l2-way orders). We also prove
that under some combinations  including ranked top-3  ranked top-2 plus 2-way 
and choice data over up to 4 alternatives  mixtures of two Plackett-Luce models
are identiﬁable. Guided by our theoretical results  we propose efﬁcient generalized
method of moments (GMM) algorithms to learn mixtures of two Plackett-Luce
models  which are proven consistent. Our experiments demonstrate the efﬁcacy of
our algorithms. Moreover  we show that when full rankings are available  learning
from different marginal events (partial orders) provides tradeoffs between statistical
efﬁciency and computational efﬁciency.

Introduction

1
Suppose a group of four friends want to choose one of the four restaurants {a1  a2  a3  a4} for dinner.
The ﬁrst person ranks all four restaurants as a2 (cid:31) a3 (cid:31) a4 (cid:31) a1  where a2 (cid:31) a3 means that “a2 is
strictly preferred to a3”. The second person says “a4 and a3 are my top two choices  among which I
prefer a4 to a3”. The third person ranks a3 (cid:31) a4 (cid:31) a1 but has no idea about a2. The fourth person
has no idea about a4  and would choose a3 among {a1  a2  a3}. How should they aggregate their
preferences to choose the best restaurant?
Similar rank aggregation problems exist in social choice  crowdsourcing [20  6]  recommender
systems [5  3  14  24]  information retrieval [1  17]  etc. Rank aggregation can be cast as the
following statistical parameter estimation problem: given a statistical model for rank data and the
agents’ preferences  the parameter of the model is estimated to make decisions. Among the most
widely-applied statistical models for rank aggregation are the Plackett-Luce model [19  28] and its
mixtures [8  9  17  23  30  23]. In a Plackett-Luce model over a set of alternatives A  each alternative
is parameterized by a strictly positive number that represents its probability to be ranked higher than
other alternatives. A mixture of k Plackett-Luce models  denoted by k-PL  combines k component
Plackett-Luce models via the mixing coefﬁcients (cid:126)α = (α1  . . .   αk) ∈ Rk≥0 with (cid:126)α · (cid:126)1 = 1  such that
for any r ≤ k  with probability αr  a data point is generated from the r-th Plackett-Luce component.
One critical limitation of Plackett-Luce model and its mixtures is that their sample space consists of
linear orders over A. In other words  each data point must be a full ranking of all alternatives in A.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

However  this is rarely the case in practice  because agents are often not able to rank all alternatives
due to lack of information [27]  as illustrated in the example in the beginning of Introduction.
In general  each rank datum is a partial order  which can be seen as a collection of pairwise
comparisons among alternatives that satisfy transitivity. However  handling partial orders is more
challenging than it appears. In particular  the pairwise comparisons of the same agent cannot be seen
as independently generated due to transitivity.
Consequently  most previous works focused on structured partial orders  where agents’ preferences
share some common structures. For example  given l ∈ N  in ranked-top-l preferences [23  10] 
agents submit a linear order over their top l choices; in l-way preferences [21  11  22]  agents submit
a linear order over a set of l alternatives  which are not necessarily their top l alternatives; in choice-l
preferences (a.k.a. choice sets) [31]  agents only specify their top choice among a set of l alternatives.
In particular  pairwise comparisons can be seen as 2-way preferences or choice-2 preferences.
However  as far as we know  most previous works assumed that the rank data share the same structure
for their algorithms and theoretical guarantees to apply. It is unclear how rank aggregation can be
done effectively and efﬁciently from structured partial orders of different kinds  as in the example in
the beginning of Introduction. This is the key question we address in this paper.
How can we effectively and efﬁciently learn Plackett-Luce and its mixtures from structured partial
orders of different kinds?
Successfully addressing this question faces two challenges. First  to address the effectiveness concern 
we need a statistical model that combines various structured partial orders to prove desirable statistical
properties  and we are unaware of an existing one. Second  to address the efﬁciency concern  we need
to design new algorithms as either previous algorithms cannot be directly applied  or it is unclear
whether the theoretical guarantee such as consistency will be retained.

1.1 Our Contributions

Our contributions in addressing the key question are three-fold.
Modeling Contributions. We propose a class of statistical models to model the co-existence of the
following three types of structured partial orders mentioned in the Introduction: ranked-top-l  l-way 
and choice-l  by leveraging mixtures of Plackett-Luce models. Our models can be easily generalized
to include other types of structured partial orders.
Theoretical Contributions. Our main theoretical results characterize the identiﬁability of the pro-
posed models. Identiﬁability is fundamental in parameter estimation  which states that different
parameters of the model should give different distributions over data. Clearly  if a model is non-
identiﬁable  then no parameter estimation algorithm can be consistent.
We prove that when only ranked top-l1 and l2-way (l2 is set to 1 if there are no l2-way orders)
orders are available  the mixture of k Plackett-Luce models is not identiﬁable if k ≥ (l1 + l2 + 1)/2
(Theorem 1). We also prove that the mixtures of two Plackett-Luce models is identiﬁable under the
following combinations of structures: ranked top-3 (Theorem 2 (a) extended from [33])  ranked top-2
plus 2 way (Theorem 2 (b))  choice-2  3  4 (Theorem 2 (c))  and 4-way (Theorem 2 (d)). For the case
of mixtures of k Plackett-Luce models over m alternatives  we prove that if there exist m(cid:48) ≤ m s.t.
the mixture of k Plackett-Luce models over m(cid:48) alternatives is identiﬁable  we can learn the parameter
using ranked top-l1 and l2-way orders where l1 + l2 ≥ m(cid:48) (Theorem 3). This theorem  combined
with Theorem 3 in [33]  which provides a condition for mixtures of k Plackett-Luce models to be
generically identiﬁable  can guide the algorithm design for mixtures of arbitrary k Plackett-Luce
models.
Algorithmic Contributions. We propose efﬁcient generalized-method-of-moments (GMM) algo-
rithms for parameter estimation of the proposed model based on 2-PL. Our algorithm runs much
faster while providing better statistical efﬁciency than the EM-algorithm proposed by Liu et al. [16]
on datasets with large numbers of structured partial orders  see Section 6 for more details. Our
algorithms are compared with the GMM algorithm by Zhao et al. [33] under two different settings.
When full rankings are available  our algorithms outperform the GMM algorithm by Zhao et al. [33]
in terms of MSE. When only structured partial orders are available  the GMM algorithm by Zhao
et al. [33] is the best. We believe this difference is caused by the intrinsic information in the data.

2

1.2 Related Work and Discussions

Modeling. We are not aware of a previous model targeting rank data that consists of different types
of structured partial orders. We believe that modeling the coexistence of different types of structured
partial orders is highly important and practical  as it is more convenient  efﬁcient  and accurate for an
agent to report her preferences as a structured partial order of her choice. For example  some voting
websites allow users to use different UIs to submit structured partial orders [4].
There are two major lines of research in rank aggregation from partial orders: learning from struc-
tured partial orders and EM algorithms for general partial orders. Popular structured partial orders
investigated in the literature are pairwise comparisons [13  12]  top-l [23  10]  l-way [21  11  22]  and
choice-l [31]. Khetan and Oh [15] focused on partial orders with “separators"  which is a broader
class of partial orders than top-k. But still  [15] assumes the same structure for everyone. Our model
is more general as it allows the coexistence of different types of structured partial orders in the dataset.
EM algorithms have been designed for learning mixtures of Mallows’ model [18] and mixtures of
random utility models including the Plackett-Luce model [16]  from general partial orders. Our model
is less general  but as EM algorithms are often slow and it is unclear whether they are consistent  our
model allows for theoretically and practically more efﬁcient algorithms. We believe that our approach
provides a principled balance between the ﬂexibility of modeling and the efﬁciency of algorithms.
Theoretical results. Several previous works provided theoretical guarantees such as identiﬁability
and sample complexity of mixtures of Plackett-Luce models and their extensions to structured partial
orders. For linear orders  Zhao et al. [33] proved that the mixture of k Plackett-Luce models over
m alternatives is not identiﬁable when k ≤ 2m − 1 and this bound is tight for k = 2. We extend
their results to the case of structured partial orders of various types. Ammar et al. [2  Theorem 1]
proved that when m = 2k  where k = 2l is a nonnegative integer power of 2  there exist two different
mixtures of k Plackett-Luce models parameters that have the same distribution over (2l + 1)-way
orders. Our Theorem 1 signiﬁcantly extends this result in the following aspects: (i) our results
includes all possible values of k rather than powers of 2; (ii) we show that the model is not identiﬁable
even under (2l+1 − 1)-way (in contrast to (2l + 1)-way) orders; (iii) we allow for combinations
of ranked top-l1 and l2-way structures. Oh and Shah [26] showed that mixtures of Plackett-Luce
models are in general not identiﬁable given partial orders  but under some conditions on the data 
the parameter can be learned using pairwise comparisons. We consider many more structures than
pairwise comparisons.
Recently  Chierichetti et al. [7] proved that at least O(m2) random marginal probabilities of partial
orders are required to identify the parameter of uniform mixture of two Plackett-Luce models. We
show that a carefully chosen set of O(m) marginal probabilities can be sufﬁcient to identify the
parameter of nonuniform mixtures of Plackett-Luce models  which is a signiﬁcant improvement.
Further  our proposed algorithm can be easily modiﬁed to handle the case of uniform mixtures.
Zhao et al. [35] characterized the conditions when mixtures of random utility models are generically
identiﬁable. We focus on strict identiﬁability  which is stronger.
Algorithms. Several learning algorithms for mixtures of Plackett-Luce models have been proposed 
including tensor decomposition based algorithm [26]  a polynomial system solving algorithm [7]  a
GMM algorithm [33]  and EM-based algorithms [8  30  23  16]. In particular  Liu et al. [16] proposed
an EM-based algorithm to learn from general partial orders. However  it is unclear whether their
algorithm is consistent (as for most EM algorithms)  and their algorithm is signiﬁcantly slower than
ours. Our algorithms for linear orders are similar to the one proposed by Zhao et al. [33]  but we
consider different sets of marginal probabilities and our algorithms signiﬁcantly outperforms the one
by Zhao et al. [33] w.r.t. MSE while taking similar running time.

2 Preliminaries
Let A = {a1  a2  . . .   am} denote a set of m alternatives and L(A) denote the set of all linear orders
(full rankings) over A  which are antisymmetric  transitive and total binary relations. A linear order
R ∈ L(A) is denoted as ai1 (cid:31) ai2 (cid:31) . . . (cid:31) aim  where ai1 is the most preferred alternative and aim
is the least preferred alternative. A partial order O is an antisymmetric and transitive binary relation.
In this paper  we consider three types of strict partial orders: ranked-top-l (top-l for short)  l-way 
and choice-l  where l ≤ m. A top-l order is denoted by Otop-l = [ai1 (cid:31) . . . (cid:31) ail (cid:31) others]; an

3

l-way order is denoted by Ol-way = [ai1 (cid:31) . . . (cid:31) ail ]  which means that the agent does not have
= (A(cid:48)  a)  where
preferences over unranked alternatives; and a choice-l order is denoted by Ochoice−l
A(cid:48) ⊆ A  |A(cid:48)| = l  and a ∈ A(cid:48)  which means that the agent chooses a from A(cid:48). We note that the
three types of partial orders are not mutually exclusive. For example  a pairwise comparison is a
2-way order as well as a choice-2 order. Let P(A) denote the set of all partial orders of the three
structures: ranked top-l  l-way  and choice-l (l ≤ m) over A. It is worth noting that L(A) ⊆ P(A).
Let P = (O1  O2  . . .   On) ∈ P(A)n denote the data  also called a preference proﬁle. Let OsA(cid:48)
denote a partial order over a subset A(cid:48) whose structure is s. When s is top-l  A(cid:48) is set to be A. Let
[d] denote the set {1  2  . . .   d}.
Deﬁnition 1. (Plackett-Luce model). The parameter space is Θ = {(cid:126)θ = {θi|1 ≤ i ≤ m  0 < θi <
i=1 θi = 1}}. The sample space is L(A)n. Given a parameter (cid:126)θ ∈ Θ  the probability of any
linear order R = [ai1 (cid:31) ai2 (cid:31) . . . (cid:31) aim] is

1 (cid:80)m

A(cid:48)

PrPL(R|(cid:126)θ) =

m−1(cid:89)

p=1

θip(cid:80)m

.

q=p θiq

Under Plackett-Luce model  a partial order O can be viewed as a marginal event which consists of all
linear orders that extend O  that is  for any extension R  a (cid:31)O b implies a (cid:31)R b. The probabilities of
the aforementioned three types of partial orders are as follows [32].

• Top-l. For any top-l order Otop-l = [ai1 (cid:31) . . . (cid:31) ail (cid:31) others]  we have

PrPL(Otop-l|(cid:126)θ) =

PrPL(Ol-wayA(cid:48)

|(cid:126)θ) =

PrPL(O|(cid:126)θ) =

.

.

l(cid:89)

p=1

θip(cid:80)m

q=p θiq

l−1(cid:89)

p=1

θip(cid:80)l
θi(cid:80)

aj∈A(cid:48) θj

.

q=p θiq

• l-way. For any l-way order Ol-wayA(cid:48) = [ai1 (cid:31) . . . (cid:31) ail ]  where A(cid:48) = {ai1   . . .   ail}  we

have

• Choice-l. For any choice order O = (A(cid:48)  ai)  we have

where (cid:126)α = (α1  . . .   αk) is the mixing coefﬁcients. For all r ≤ k  αr ≥ 0 and(cid:80)k

In this paper  we assume that data points are i.i.d. generated from the model.
Deﬁnition 2 (Mixtures of k Plackett-Luce models for linear orders (k-PL)). Given m ≥ 2 and
k ∈ N+  the sample space of k-PL is L(A)n. The parameter space is Θ = {(cid:126)θ = ((cid:126)α  (cid:126)θ(1)  . . .   (cid:126)θ(k))} 
r=1 αr = 1. For
all 1 ≤ r ≤ k  (cid:126)θ(r) is the parameter of the rth Plackett-Luce component. The probability of a linear
order R is:

k(cid:88)

Prk-PL(R|(cid:126)θ) =

αr PrPL(R|(cid:126)θ(r)).

r=1

We now recall the deﬁnition of identiﬁability of statistical models.
Deﬁnition 3 (Identiﬁability). Let M = {Pr(·|(cid:126)θ) : (cid:126)θ ∈ Θ} be a statistical model  where Θ is the
parameter space and Pr(·|(cid:126)θ) is the distribution over the sample space associated with (cid:126)θ ∈ Θ. M is
identiﬁable if for all (cid:126)θ  (cid:126)γ ∈ Θ  we have

Pr(·|(cid:126)θ) = Pr(·|(cid:126)γ) =⇒ (cid:126)θ = (cid:126)γ.

A mixture model is generally not identiﬁable due to the label switching problem [29]  which means
that labeling the components differently leads to the same distribution over data. In this paper  we
consider identiﬁability of mixture models modulo label switching. That is  in Deﬁnition 3  we further
require that (cid:126)θ and (cid:126)γ cannot be obtained from each other by label switching.

4

Figure 1: The mixture model for structured partial preferences.

3 Mixtures of Plackett-Luce Models for Partial Orders

> 0 and we require(cid:80)u

We propose the class of mixtures of Plackett-Luce models for the aforementioned structures of
partial orders. To this end  each such model should be described by the collection of allowable
types of structured partial orders  denoted by Φ. More precisely  Φ is a set of u structures Φ =
{(s1 A1)  . . .   (su Au)}  where for any t ∈ [u]  (st At) means structure st over At. For the case of
top-l  At is set to be A. Since the three structured considered in this paper are not mutually exclusive 
we require that Φ does not include any pair of overlapping structures simultaneously for the
model to be identiﬁable. There are two types of pairs of overlapping structures: (1) (top-(m − 1) A)
and (m-way A); and (2) for any subset of two alternatives A(cid:48)  (2-way A(cid:48)) and (choice-2 A(cid:48)). Each
structure corresponds to a number φstAt
= 1. A partial order is
generated in two stages as illustrated in Figure 1: (i) a linear order R is generated by k-PL given
(cid:126)α  (cid:126)θ(1)  . . .   (cid:126)θ(k); (ii) with probability φstAt
  R is projected to the randomly-generated partial order
structure (st  At) to obtain a partial order O. Formally  the model is deﬁned as follows.
Deﬁnition 4 (Mixtures of k Plackett-Luce models for partial orders by Φ (k-PL-Φ)). Given m ≥ 2 
k ∈ N+  and the set of structures Φ = {(s1 A1)  . . .   (su Au)}  the sample space is all structured
partial orders deﬁned by Φ. Given l1 ∈ [m − 1]  l2  l3 ∈ [m]  the parameter space is Θ = {(cid:126)θ =
and(cid:80)u
((cid:126)φ  (cid:126)α  (cid:126)θ(1)  . . .   (cid:126)θ(k))}. The ﬁrst part is a vector (cid:126)φ = (φs1A1
(cid:80)k
)  whose entries are all positive
= 1. The second part is (cid:126)α = (α1  . . .   αk) where for all r ≤ k  αr > 0 and
r=1 αr = 1. The remaining part is ((cid:126)θ(1)  . . .   (cid:126)θ(k))  where (cid:126)θ(r) is the parameter of the rth Plackett-
Luce component. Then the probability of any partial order O  whose structure is deﬁned by (s A(cid:48)) 
is

  . . .   φsuAu

t=1 φstAt

t=1 φstAt

k(cid:88)

Prk-PL-Φ(O|(cid:126)θ) = φsA(cid:48)

αr PrPL(OsA(cid:48)|(cid:126)θ(r)).

r=1

For any partial order O whose structure is (s A(cid:48))  we can also write
Prk-PL-Φ(O|(cid:126)θ) = φsA(cid:48) Prk-PL(O|(cid:126)θ)

(1)
where Prk-PL(O|(cid:126)θ) is the marginal probability of O under k-PL. This is a class of models because
the sample space is different when Φ is different.
Example 1. Let the set of alternatives be {a1  a2  a3  a4}. Consider the 2-PL-Φ M where Φ =
{(top-3 A)  (top-2 A)  (3-way {a1  a3  a4})  (choice-3 {a1  a2  a3})}. φtop-3A = 0.2  φtop-2A = 0.1 
φ3-way
{a1 a2 a3} = 0.4  (cid:126)α = [α1  α2] = [0.2  0.8]  (cid:126)θ(1) = [0.1  0.2  0.3  0.4]  (cid:126)θ(2) =
{a1 a3 a4} = 0.3  φchoice-3
[0.2  0.2  0.3  0.3]. Now we compute the probabilities of the following partial orders given the model:
O1 = a2 (cid:31) a3 (cid:31) a4 (cid:31) a1 (top-3)  O2 = a4 (cid:31) a3 (cid:31) {a1  a2} (top-2)  O3 = a3 (cid:31) a4 (cid:31) a1 (3-way) 
and O4 = ({a1  a2  a3}  a3) (choice-3 over {a1  a2  a3}). We ﬁrst compute PrPL(Oj|θ(r)) for all
combinations of j and r  shown in Table 1.

O1
O2
O3
O4

0.2

0.1+0.2+0.3+0.4

0.4

0.1+0.3+0.4
0.3

0.4

0.1+0.4 = 0.06

0.2

0.2+0.2+0.3+0.3

0.3

0.2+0.3+0.3
0.3

0.3

0.2+0.3 = 0.045

r = 1

0.3

r = 2
0.3

0.1+0.2+0.3+0.4

0.3

0.1+0.3+0.4
0.3

0.1+0.2+0.3 = 0.2
0.1+0.4 = 0.3

0.4

0.2+0.2+0.3+0.3

0.3

0.2+0.3+0.3
0.3

0.2+0.2+0.3 = 0.13
0.3
0.2+0.3 = 0.225

0.1+0.2+0.3 = 0.5
Table 1: Pr(Rj|θ(r)) for all j = 1  2  3  4 and r = 1  2.

0.2+0.2+0.3 = 0.43

5

Let PrM(Oj) denote the probability of Oj under model M  we have

r=1

2(cid:88)
2(cid:88)
2(cid:88)
2(cid:88)

r=1

PrM(O1) = φtop-3A

PrM(O2) = φtop-2A

αr Pr(O1|(cid:126)θ(r)) = 0.2 × (0.2 × 0.06 + 0.8 × 0.045) = 0.0096

αr Pr(O2|(cid:126)θ(r)) = 0.1 × (0.2 × 0.2 + 0.8 × 0.13) = 0.014

PrM(O3) = φ2-way

{a3 a4}

r=1

αr Pr(O3|(cid:126)θ(r)) = 0.3 × (0.2 × 0.3 + 0.8 × 0.225) = 0.072

PrM(O4) = φchoice-3

{a1 a2 a3}

αr Pr(O4|(cid:126)θ(r)) = 0.4 × (0.2 × 0.5 + 0.8 × 0.43) = 0.18

r=1

(Non-)identiﬁability of k-PL-Φ

4
Let Φl-way = {(l-way Al)|Al ∈ A |Al| = l} and Φchoice-l = {(choice-l Al)|Al ∈ A |Al| = l}.
The following theorem shows that under some conditions on Φ  k  and m  k-PL-Φ is not identiﬁable.
Theorem 1. Given a set of m alternatives A and any 0 ≤ l1 ≤ m − 1  1 ≤ l2 ≤ m. Let
Φ∗ = {(top-1 A)  . . .   (top-l1 A)} ∪ Φ1-way ∪ . . . ∪ Φl2-way. Given any Φ ⊂ Φ∗  and for any
k ≥ (l1 + l2 + 1)/2  k-PL-Φ is not identiﬁable.
We prove that the theorem holds when Φ = Φ∗. See full proof in the appendix. Considering that any
l-way order implies a choice-l order  we have the following corollary.
Corollary 1. Given a set of m alternatives A and any 0 ≤ l1 ≤ m − 1  1 ≤ l3 ≤ m. Let
Φ∗ = {(top-1 A)  . . .   (top-l1 A)} ∪ Φchoice-1 ∪ . . . ∪ Φchoice-l3. Given any Φ ⊂ Φ∗  and for any
k ≥ (l1 + l3 + 1)/2  k-PL-Φ is not identiﬁable.
Given any k  these results show what structures of data we cannot use if we want to interpret the
learned parameter. Next  we will characterize conditions for 2-PL-Φ’s to be identiﬁable.
Theorem 2. Let Φ∗ be one of the four combinations of structures below. For any Φ ⊃ Φ∗  2-PL-Φ
over m ≥ 4 alternatives is identiﬁable.
(a) Φ∗ = {(top-3 A)}  (b) Φ∗ = {(top-2 A)} ∪ Φ2-way  (c) Φ∗ = ∪4
t=2Φchoice-t  or (d) Φ∗ = Φ4-way.
We ﬁrst show that for any (cid:126)φ1 (cid:54)= (cid:126)φ2  the distribution over sample space must be different. Then given
(cid:126)φ  we prove that for any ((cid:126)α  (cid:126)θ(1)  . . .   (cid:126)θ(k))  there does not exist another parameter leading to the
same distribution over the sample space. See the full proof in the appendix.
Identiﬁability for k ≥ 3 is still an open question and Zhao et al. [33] proved that when k ≤
(cid:98) m−2
2 (cid:99)!  generic identiﬁability holds for k-PL  which means the Lebesgue measure of non-identiﬁable
parameter is zero. We have the following theorem that can guide algorithm design for k-PL-Φ. Full
proof of Theorem 3 can be found in the appendix.
Theorem 3. Let l1 ∈ [m − 1]  l2 ∈ [m]  and Φ∗ = {(top-l1 A)  (l2-way A(cid:48))|A(cid:48) ∈ A |A(cid:48)| = l2}.
Given any Φ ⊃ Φ∗  if k-PL over m(cid:48) alternatives is (generically) identiﬁable  k-PL-Φ over m ≥ m(cid:48)
alternatives is (generically) identiﬁable when l1 + l2 ≥ m(cid:48).

5 Consistent Algorithms for Learning 2-PL-Φ

We propose a two-stage estimation algorithm. In the ﬁrst stage  we make one pass of the dataset to
determine Φ and estimate (cid:126)φ. In the second stage  we estimate the parameter (cid:126)θ. We note that these
two stages only require one pass of the data.
In the ﬁrst stage we check the existence of each structure in the dataset and estimate φtop-lA   φl-wayA(cid:48)
φchoice-l
A(cid:48)(cid:48)
Formally  for any structure (s As) 

  and
for any l  A(cid:48) and A(cid:48)(cid:48) by dividing the occurrences of each structure by the size of the dataset.

# of orders with structure (s As)

φsAs

=

(2)

n

6

In the second stage  we estimate (cid:126)θ using the generalized-method-of-moments (GMM) algorithm.
In a GMM algorithm  a set of q marginal events (partial orders in the case of rank data)  denoted
by E = {E1  . . .  Eq}  are selected. Then q moment conditions (cid:126)g(O  (cid:126)θ) ∈ Rq  which are functions
of a data point O and the parameter (cid:126)θ  are designed. The expectation of any moment condition is
zero at the ground truth parameter (cid:126)θ∗  i.e.  E[g(O  (cid:126)θ∗)] = (cid:126)0. For a dataset P with n rankings  we let
(cid:126)g(P  (cid:126)θ) = 1
n
Now we deﬁne moment conditions (cid:126)g(O  (cid:126)θ). For any t ≤ q  the t-th moment condition gt(O  (cid:126)θ)
corresponds to the event Et. Let (st At) denote the structure of Et. If O = Et  we deﬁne gt(O  (cid:126)θ) =
Prk-PL-Φ(Et|(cid:126)θ). Under this deﬁnition  we have
1
φstAt

(cid:80)
O∈P g(O  (cid:126)θ). Then the estimate is ˆθ = arg min||g(P  (cid:126)θ)||2
2.

(Prk-PL-Φ(Et|(cid:126)θ) − 1); otherwise gt(O  (cid:126)θ) = 1
φstAt

)2

(3)

(cid:126)θ(cid:48) = arg min

q(cid:88)

t=1

(

Prk-PL-Φ(Et|(cid:126)θ)

φstAt

− # of Et
nφstAt

We consider two ways of selecting E for 2-PL-Φ guided by our Theorem 2 (b) and (c) respectively.
Ranked top-2 and 2-way (Φ = {(top-2 A)  (2-way A(cid:48))|A(cid:48) ∈ A |A(cid:48)| = 2}). The selected partial
orders are: ranked top-2 for each pair (m(m − 1) − 1 moment conditions) and all combinations
of 2-way orders (m(m − 1)/2 moment conditions). We remove one of the ranked top-2 orders
because this corresponding moment condition is linearly dependent of the other ranked top-2 moment
conditions. For the same reason  we only choose one for each 2-way comparison  resulting in
m(m − 1)/2 moment conditions. For example. in the case of A = {a1  a2  a3  a4}  we can choose
E = {a1 (cid:31) a2 (cid:31) others  a1 (cid:31) a3 (cid:31) others  a1 (cid:31) a4 (cid:31) others  a2 (cid:31) a1 (cid:31) others  a2 (cid:31) a3 (cid:31)
others  a2 (cid:31) a4 (cid:31) others  a3 (cid:31) a1 (cid:31) others  a3 (cid:31) a2 (cid:31) others  a3 (cid:31) a4 (cid:31) others  a4 (cid:31) a1 (cid:31)
others  a4 (cid:31) a2 (cid:31) others  a1 (cid:31) a2  a1 (cid:31) a3  a1 (cid:31) a4  a2 (cid:31) a3  a2 (cid:31) a4  a3 (cid:31) a4}.
Choice-4. We ﬁrst group A into subsets of four alternatives so that a1 is included in all
subsets. And a small number of groups is desirable for computational considerations. One
possible way is G1 = {a1  a2  a3  a4}  G2 = {a1  a5  a6  a7}  etc. The last group can be
{a1  am−2  am−1  am}. More than one overlapping alternatives across groups is ﬁne.
In this
3 (cid:101) groups. We will deﬁne ΦG and EG for any group G = {ai1  ai2   ai3   ai4}.
way we have (cid:100) m−1
t=1 ΦGt and E = ∪(cid:100) m−1
Then Φ = ∪(cid:100) m−1
3 (cid:101)
3 (cid:101)
t=1 EGt. For any G = {ai1   ai2   ai3  ai4}  ΦG =
{(choice-4  G)  (choice-3  G(cid:48))  (choice-2  G(cid:48)(cid:48))|G(cid:48)  G(cid:48)(cid:48) ∈ G |G(cid:48)| = 3 |G(cid:48)(cid:48)| = 2}. E includes all 17
choice-2 3 4 orders. E = {(G  ai1)  (G  ai2 )  (G  ai3)  ({ai1   ai2   ai3}  ai1)  ({ai1  ai2  ai3}  ai2 ) 
({ai1  ai2  ai4}  ai1)  ({ai1  ai2  ai4}  ai2 )  ({ai1   ai3   ai4}  ai1)  ({ai1  ai3  ai4}  ai3 )  ({ai2  ai3  ai4} 
ai2 )  ({ai2   ai3   ai4}  ai3)  ({ai1  ai2}  ai1 )  ({ai1  ai3}  ai1)  ({ai1   ai4}  ai1)  ({ai2  ai3}  ai2 ) 
({ai2   ai4}  ai2)  ({ai3  ai4}  ai3)}.
Formally our algorithms are collectively represented as Algorithm 1. We note that only one pass of
data is required for estimating (cid:126)φ and computing the frequencies of each partial order. The following
theorem shows that Algorithm 1 is consistent when E is chosen for “ranked top-2 and 2-way" and
“choice-4".

Algorithm 1 Algorithms for 2-PL-Φ.
Input: Preference proﬁle P with n partial orders. A set of preselected partial orders E.
Output: Estimated parameter (cid:126)θ(cid:48).
Estimate (cid:126)φ using (2).
For each E ∈ E  compute the frequency of E.
Compute the output using (3).
Theorem 4. Given m ≥ 4. If there exists  > 0 s.t. for all r = 1  2 and i = 1  . . .   m  θ(r)
i ∈ [  1] 
and E is selected following either of “ranked top-2 and 2-way" and “choice-4"  then Algorithm 1 is
consistent.

Proof. We ﬁrst prove that the estimate of (cid:126)φ is consistent. Let Xt denote a random variable  where
Xt = 1 if a structure (st At) is observed and 0 otherwise. The dataset of n partial orders is
considered as n trials. Let the j-th observation of Xt be xj. Then we have E[
  which
means as n → ∞ 

with probability approaching one.

] = φstAt

j=1 xj
n

(cid:80)n

(cid:80)n

j=1 xj
n

converges to φstAt

7

Now we prove that the estimation of (cid:126)α  (cid:126)θ(1)  (cid:126)θ(2) is also consistent.
We write the moment conditions (cid:126)g(P  (cid:126)θ) as (cid:126)gn((cid:126)θ) and deﬁne

Let (cid:126)θ∗ denote the ground truth parameter. By deﬁnition  we have

(cid:126)g0((cid:126)θ) = E[(cid:126)gn((cid:126)θ)].

(cid:126)g0((cid:126)θ∗) = E[

Prk-PL-Φ(Et|(cid:126)θ∗)

− # of Et
nφstAt

] =

1
φstAt

(Prk-PL-Φ(Et|(cid:126)θ∗) − E[

# of Et

]) = (cid:126)0.

n

φstAt
2  which is minimized at (cid:126)θ(cid:48) (the estimate) and deﬁne Q0((cid:126)θ) = E[Qn((cid:126)θ)] 

Let Qn((cid:126)θ) = ||g(P  (cid:126)θ)||2
which is minimized at (cid:126)θ∗. We ﬁrst prove the following lemma:
Lemma 1. sup(cid:126)θ∈Θ |Qn((cid:126)θ) − Q0((cid:126)θ)| p−→ 0.
Proof. Recall that any moment condition g(Oj  (cid:126)θ) (corresponding to partial order Et where 1 ≤
t ≤ q) has the from Prk-PL-Φ(Et|(cid:126)θ) − Xt j where Xt j = 1 if Et is observed from Oj and Xt j = 0
j=1 (cid:126)g(Oj  (cid:126)θ)  for any moment condition  we have
otherwise. And also from (cid:126)gn((cid:126)θ) = (cid:126)g(P  (cid:126)θ) = 1
n

(cid:80)n
n(cid:88)

j=1

|gn((cid:126)θ) − g0((cid:126)θ)| = | 1
n

Xt j − E[Xt]| p−→ 0.

Therefore  we obtain sup(cid:126)θ∈Θ ||(cid:126)gn((cid:126)θ) − (cid:126)g0((cid:126)θ)|| p−→ (cid:126)0.
Then we have (omitting the independent variable (cid:126)θ)

|Qn − Q0| = |(cid:126)g(cid:62)

n (cid:126)gn − (cid:126)g(cid:62)

0 (cid:126)g0| ≤ |((cid:126)gn − (cid:126)g0)(cid:62)((cid:126)gn − (cid:126)g0)| + 2|(cid:126)g(cid:62)

0 ((cid:126)gn − (cid:126)g0)|

Since all moment conditions fall in [−1  1] for any (cid:126)θ ∈ Θ  we have

|Qn((cid:126)θ) − Q0((cid:126)θ)| p−→ 0.

sup
(cid:126)θ∈Θ

Now we are ready to prove consistency. By our Theorem 2  the model is identiﬁable  which means
i ∈ [  1] for all
g0((cid:126)θ) is uniquely minimized at (cid:126)θ∗. Since Q0((cid:126)θ) is continuous and Θ is compact (θ(r)
r = 0  1 and i = 1  . . .   m)  by Lemma 1 and Theorem 2.1 by Newey and McFadden [25]  we have
(cid:126)θ(cid:48) p−→ (cid:126)θ∗.

6 Experiments

s.t. (cid:80)m

Setup. We conducted experiments on synthetic data to demonstrate the effectiveness of our algorithms.
The data are generated as follows: (i) generate α  (cid:126)θ(1)  and (cid:126)θ(2) uniformly at random and normalize
i = 1 for r = 1  2; (ii) generate linear orders using k-PL-linear; (iii) choose φtop-lA  
i=1 θ(r)
  and φchoice-l
and sample partial orders from the generated linear orders. The partial orders are
A(cid:48)

φl-wayA(cid:48)
generated from the following two models:
• ranked top-2 and 2-way: φtop-2A = 1
• choice-2  3  4: ﬁrst group the alternatives as described in the previous section. Let C =
3 (cid:101) be the number of groups. We ﬁrst sample a group uniformly at random. Let A(4) be
28; for each subset A(3) ⊂ A(4)
28; for each subset
A(3) = 1
28.
A(2) = 1

(cid:100) m−1
the sampled group (of four alternatives). Then φchoice-4
of three alternatives (four such subsets within A(4))  φchoice-3
A(2) ⊂ A(4) of two alternatives (six subsets within A(4))  φchoice-2

m(m−1) for all A(cid:48) ⊂ A and |A(cid:48)| = 2;

2  φ2-wayA(cid:48) =

C

C

A(4) = 1

C

4

3

1

1

8

Besides  we tested our algorithms on linear orders. In this case  all partial orders are marginal events
of linear orders and there is no (cid:126)φ estimation. Our algorithms reduce to the standard generalized-
method-of-moments algorithms.
The baseline algorithms are the GMM algorithm by [33] and ELSR-Gibbs algorithm by [16]. The
GMM algorithm by [33] is for linear order  but it utilizes only ranked top-3 orders. So it can be viewed
as both a linear order algorithm and a partial order algorithm. We apply ELSR-Gibbs algorithm
by [16] on “choice-2 3 4" datasets because the algorithm is expected to run faster than “ranked top-2
and 2-way" dataset.
All algorithms were implemented with MATLAB1 on an Ubuntu Linux server with Intel Xeon
E5 v3 CPUs each clocked at 3.50 GHz. We use Mean Squared Error (MSE)  which is deﬁned as
E[||(cid:126)θ(cid:48) − (cid:126)θ∗||2
2]  and runtime to compare the performance of the algorithms. For fair comparisons with
previous works  we ignore the (cid:126)φ parameter when computing MSE.

Figure 2: MSE and runtime with 95% conﬁdence intervals for 2-PL over 10 alternatives when n
varies. “Choice" denotes the setting of “choice-2  3  4". For ELSR-Gibbs [16]  we used the partial
orders generated by “choice-2  3  4". One linear extension was generated from each partial order and
three EM iterations were run. All values were averaged over 2000 trials.

Results and Discussions. The algorithms are compared when the number of rankings varies (Fig-
ure 2). We have the following observations.

• When learning from partial orders only: “ELSR-gibbs [16]" is much slower than other
algorithms for large datasets. MSEs of all other algorithms converge towards zero as n
increases. We can see “top-2 and 2-way  partial" and “choice  partial" converge slower than
“top-3". Ranked top-l orders are generally more informative for parameter estimation than
other partial orders. However  as was reported in [34]  it is much more time consuming for
human to pick their ranked top alternative(s) from a large set of alternatives than fully rank a
small set of alternatives  which means ranked top-l data are harder or more costly to collect.
• When learning from linear orders: our “ranked top-2 and 2-way  linear" and “choice-2  3  4 
linear" outperform “top-3 [33]" in terms of MSE (left of Figure 2)  but only slightly slower
than “top-3 [33]" (Figure 2 right).

7 Conclusions and Future Work

We extend the mixtures of Plackett-Luce models to the class of models that sample structured partial
orders and theoretically characterize the (non-)identiﬁability of this class of models. We propose
consistent and efﬁcient algorithms to learn mixtures of two Plackett-Luce models from linear orders
or structured partial orders. For future work  we will explore more statistically and computationally
efﬁcient algorithms for mixtures of an arbitrary number of Plackett-Luce models  or the more general
random utility models.

1Code available at https://github.com/zhaozb08/MixPL-SPO

9

Acknowledgments

We thank all anonymous reviewers for helpful comments and suggestions. This work is supported by
NSF #1453542 and ONR #N00014-17-1-2621.

References
[1] Alon Altman and Moshe Tennenholtz. Ranking systems: The PageRank axioms. In Proceedings of the

ACM Conference on Electronic Commerce (EC)  Vancouver  BC  Canada  2005.

[2] Ammar Ammar  Sewoong Oh  Devavrat Shah  and L Voloch. What’s your choice? learning the mixed multi-
nomial logit model. In Proceedings of the ACM SIGMETRICS/international conference on Measurement
and modeling of computer systems  2014.

[3] Linas Baltrunas  Tadas Makcinskas  and Francesco Ricci. Group recommendations with rank aggregation
and collaborative ﬁltering. In Proceedings of the fourth ACM conference on Recommender systems  pages
119–126. ACM  2010.

[4] Felix Brandt and Guillaume Chabinand Christian Geist. Pnyx:: A Powerful and User-friendly Tool for
Preference Aggregation. In Proceedings of the 2015 International Conference on Autonomous Agents and
Multiagent Systems  pages 1915–1916  2015.

[5] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations

of Computational mathematics  9(6):717  2009.

[6] Xi Chen  Paul N Bennett  Kevyn Collins-Thompson  and Eric Horvitz. Pairwise ranking aggregation in a
crowdsourced setting. In Proceedings of the sixth ACM international conference on Web search and data
mining  pages 193–202. ACM  2013.

[7] Flavio Chierichetti  Ravi Kumar  and Andrew Tomkins. Learning a mixture of two multinomial logits. In

Proceedings of the 35rd International Conference on Machine Learning (ICML-18)  2018.

[8] Isobel Claire Gormley and Thomas Brendan Murphy. Exploring voting blocs within the irish exploring
voting blocs within the irish electorate: A mixture modeling approach. Journal of the American Statistical
Association  103(483):1014–1027  2008.

[9] Isobel Claire Gormley and Thomas Brendan Murphy. A grade of membership model for rank data.

Bayesian Analysis  4(2):265–296  2009.

[10] Jonathan Huang  Ashish Kapoor  and Carlos Guestrin. Efﬁcient probabilistic inference with partial ranking
queries. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence  pages
355–362. AUAI Press  2011.

[11] David R. Hunter. MM algorithms for generalized Bradley-Terry models. In The Annals of Statistics 

volume 32  pages 384–406  2004.

[12] Kevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. In Advances in Neural

Information Processing Systems  pages 2240–2248  2011.

[13] Minje Jang  Sunghyun Kim  Changho Suh  and Sewoong Oh. Top-k ranking from pairwise comparisons:

When spectral ranking is optimal. arXiv preprint arXiv:1603.04153  2016.

[14] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from noisy entries.

Journal of Machine Learning Research  11(Jul):2057–2078  2010.

[15] Ashish Khetan and Sewoong Oh. Data-driven rank breaking for efﬁcient rank aggregation. Journal of

Machine Learning Research  17(193):1–54  2016.

[16] Ao Liu  Zhibing Zhao  Chao Liao  Pinyan Lu  and Lirong Xia. Learning plackett-luce mixtures from partial
preferences. In Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19) 
2019.

[17] Tie-Yan Liu. Learning to Rank for Information Retrieval. Springer  2011.

[18] Tyler Lu and Craig Boutilier. Effective sampling and learning for mallows models with pairwise-preference

data. The Journal of Machine Learning Research  15(1):3783–3829  2014.

[19] Robert Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley  1959.

10

[20] Andrew Mao  Ariel D. Procaccia  and Yiling Chen. Better human computation through principled voting.
In Proceedings of the National Conference on Artiﬁcial Intelligence (AAAI)  Bellevue  WA  USA  2013.

[21] John I. Marden. Analyzing and modeling rank data. Chapman & Hall  1995.

[22] Lucas Maystre and Matthias Grossglauser. Fast and accurate inference of plackett–luce models.

Advances in neural information processing systems  pages 172–180  2015.

In

[23] Cristina Mollica and Luca Tardella. Bayesian Plackett–Luce mixture models for partially ranked data.

Psychometrika  82(2):442–458  2017.

[24] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion:

Optimal bounds with noise. Journal of Machine Learning Research  13(May):1665–1697  2012.

[25] Whitney K Newey and Daniel McFadden. Large sample estimation and hypothesis testing. Handbook of

econometrics  4:2111–2245  1994.

[26] Sewoong Oh and Devavrat Shah. Learning mixed multinomial logit model from ordinal data. In Advances

in Neural Information Processing Systems  pages 595–603  2014.

[27] Maria Silvia Pini  Francesca Rossi  Kristen Brent Venable  and Toby Walsh. Incompleteness and incom-
parability in preference aggregation: Complexity results. Artiﬁcial Intelligence  175(7–8):1272—1289 
2011.

[28] Robin L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society. Series C (Applied

Statistics)  24(2):193–202  1975.

[29] Richard A Redner and Homer F Walker. Mixture densities  maximum likelihood and the em algorithm.

SIAM review  26(2):195–239  1984.

[30] Maksim Tkachenko and Hady W Lauw. Plackett-luce regression mixture model for heterogeneous rankings.
In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management 
pages 237–246. ACM  2016.

[31] Kenneth E. Train. Discrete Choice Methods with Simulation. Cambridge University Press  2nd edition 

2009.

[32] Lirong Xia. Learning and Decision-Making from Rank Data. Synthesis Lectures on Artiﬁcial Intelligence

and Machine Learning. Morgan & Claypool Publishers  2019.

[33] Zhibing Zhao  Peter Piech  and Lirong Xia. Learning mixtures of Plackett-Luce models. In Proceedings of

the 33rd International Conference on Machine Learning (ICML-16)  2016.

[34] Zhibing Zhao  Haoming Li  Junming Wang  Jeffrey Kephart  Nicholas Mattei  Hui Su  and Lirong Xia. A
cost-effective framework for preference elicitation and aggregation. In Proceedings of the 34th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI-2018)  2018.

[35] Zhibing Zhao  Tristan Villamil  and Lirong Xia. Learning mixtures of random utility models. In Proceedings

of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI-18)  2018.

11

,Zhibing Zhao
Lirong Xia