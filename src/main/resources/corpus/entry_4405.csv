2011,Prismatic Algorithm for Discrete D.C. Programming Problem,In this paper  we propose the first exact algorithm for minimizing the difference of two submodular functions (D.S.)  i.e.  the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions. We empirically investigate the performance of our algorithm  and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.,Prismatic Algorithm for Discrete D.C. Programming Problem

Yoshinobu Kawahara(cid:3) and Takashi Washio

The Institute of Scientiﬁc and Industrial Research (ISIR)

Osaka University

8-1 Mihogaoka  Ibaraki-shi  Osaka 567-0047 JAPAN

{kawahara washio}@ar.sanken.osaka-u.ac.jp

Abstract

In this paper  we propose the ﬁrst exact algorithm for minimizing the difference of two
submodular functions (D.S.)  i.e.  the discrete version of the D.C. programming problem.
The developed algorithm is a branch-and-bound-based algorithm which responds to the
structure of this problem through the relationship between submodularity and convexity.
The D.S. programming problem covers a broad range of applications in machine learn-
ing. In fact  this generalizes any set-function optimization. We empirically investigate
the performance of our algorithm  and illustrate the difference between exact and approx-
imate solutions respectively obtained by the proposed and existing algorithms in feature
selection and discriminative structure learning.

1 Introduction

Combinatorial optimization techniques have been actively applied to many machine learning appli-
cations  where submodularity often plays an important role to develop algorithms [10  16  27  14 
15  19  1]. In fact  many fundamental problems in machine learning can be formulated as submoular
optimization. One of the important categories would be the D.S. programming problem  i.e.  the
problem of minimizing the difference of two submodular functions. This is a natural formulation
of many machine learning problems  such as learning graph matching [3]  discriminative structure
learning [21]  feature selection [1] and energy minimization [24].
In this paper  we propose a prismatic algorithm for the D.S. programming problem  which is a
branch-and-bound-based algorithm responding to the speciﬁc structure of this problem. To the best
of our knowledge  this is the ﬁrst exact algorithm to the D.S. programming problem (although there
exists an approximate algorithm for this problem [21]). As is well known  the branch-and-bound
method is one of the most successful frameworks in mathematical programming and has been in-
corporated into commercial softwares such as CPLEX [13  12]. We develop the algorithm based
on the analogy with the D.C. programming problem through the continuous relaxation of solution
spaces and objective functions with the help of the Lov´asz extension [17  11  18]. The algorithm is
implemented as an iterative calculation of binary-integer linear programming (BILP).
Also  we discuss applications of the D.S. programming problem in machine learning and investi-
gate empirically the performance of our method and the difference between exact and approximate
solutions through feature selection and discriminative structure-learning problems.
The remainder of this paper is organized as follows. In Section 2  we give the formulation of the
D.S. programming problem and then describe its applications in machine learning. In Section 3 
we give an outline of the proposed algorithm for this problem. Then  in Section 4  we explain the
details of its basic operations. And ﬁnally  we give several empirical examples using artiﬁcial and
real-world datasets in Section 5  and conclude the paper in Section 6.
Preliminaries and Notation: A set function f is called submodular if f (A) + f (B) ≥ f (A ∪
B) + f (A ∩ B) for all A; B ⊆ N  where N = {1;··· ; n} [5  7]. Throughout this paper  we denote

(cid:3)

http://www.ar.sanken.osaka-u.ac.jp/(cid:24)kawahara/

1

by ^f the Lov´asz extension of f  i.e.  a continuous function ^f : Rn → R deﬁned by

∑
j=1 (^pj − ^pj+1)f (Uj) + ^pmf (Um);
m(cid:0)1

^f (p) =

∑

where Uj = {i ∈ N : pi ≥ ^pj} and ^p1 > ··· > ^pm are the m distinct elements of p [17  18]. Also 
we denote by IA ∈ {0; 1}n the characteristic vector of a subset A ⊆ N  i.e.  IA =
i2A ei where
ei is the i-th unit vector. Note  through the deﬁnition of the characteristic vector  any subset A ⊆ N
has the one-to-one correspondence with the vertex of a n-dimensional cube D := {x ∈ Rn : 0 ≤
xi ≤ 1(i = 1; : : : ; n)}. And  we denote by (A; t)(T ) all combinations of a real value plus subset
whose corresponding vectors (IA; t) are inside or on the surface of a polytope T ∈ Rn+1.

2 The D.S. Programming Problem and its Applications

Let f and g are submodular functions. In this paper  we address an exact algorithm to solve the D.S.
programming problem  i.e.  the problem of minimizing the difference of two submodular functions:
(1)

f (A) − g(A):

min
A(cid:18)N

As is well known  any real-valued function whose second partial derivatives are continuous every-
where can be represented as the difference of two convex functions [12]. As well  the problem (1)
generalizes any set-function optimization problem. Problem (1) covers a broad range of applications
in machine learning [21  24  3  1]. Here  we give a few examples.
Feature selection using structured-sparsity inducing norms: Sparse methods for supervised
learning  where we aim at ﬁnding good predictors from as few variables as possible  have attracted
much interests from machine learning community. This combinatorial problem is known to be a
submodular maximization problem with cardinality constraint for commonly used measures such as
least-squared errors [4  14]. And as is well known  if we replace the cardinality function with its
convex envelope such as l1-norm  this can be turned into a convex optimization problem. Recently 
it is reported that submodular functions in place of the cardinality can give a wider family of poly-
hedral norms and may incorporate prior knowledge or structural constraints in sparse methods [1].
Then  the objective (that is supposed to be minimized) becomes the sum of a loss function (often 
supermodular) and submodular regularization terms.
Discriminative structure learning: It is reported that discriminatively structured Bayesian clas-
siﬁer often outperforms generatively structured one [21  22]. One commonly used metric for dis-
criminative structure learning would be EAR (explaining away residual) [2]. EAR is deﬁned as the
difference of the conditional mutual information between variables by class C and non-conditional
one  i.e.  I(Xi; Xj|C) − I(Xi; Xj).
In structure learning  we repeatedly try to ﬁnd a subset in
variables that minimize this kind of measures. Since the (symmetric) mutual information is a sub-
modular function  obviously this problem leads the D.S. programming problem [21].
Energy minimization in computer vision: In computer vision  an image is often modeled with
a Markov random ﬁeld  where each node represents a pixel. Let G = (V;E) be the undirected
graph  where a label xs ∈ L is assigned on each node. Then  many tasks in computer vision
can be naturally formulated in terms of energy minimization where the energy function has the
(p;q)2E (cid:18)(xp; xq)  where (cid:18)p and (cid:18)p;q are univariate and pairwise
form: E(x) =
potentials. In a pairwise potential for binarized energy (i.e.  L = {0; 1})  submodularity is deﬁned
as (cid:18)pq(1; 1) + (cid:18)pq(0; 0) ≥ (cid:18)pq(1; 0) + (cid:18)pq(0; 1) (see  for example  [26]). Based on this  any energy
function in computer vision can be written with a submodular function E1(x) and a supermodular
function E2(x) as E(x) = E1(x) + E2(x) (ex. [24]). Or  in case of binarized energy  even if such
explicit decomposition is not known  a non-unique decomposition to submodular and supermodular
functions can be always given [25].

p2V (cid:18)p(xp) +

∑

∑

3 Prismatic Algorithm for the D.S. Programming Problem
By introducing an additional variable t(∈ R)  Problem (1) can be converted into the equivalent
problem with a supermodular objective function and a submodular feasible set  i.e. 

A(cid:18)N;t2R t − g(A)

min

s.t. f (A) − t ≤ 0:

(2)

2

(cid:3)

; t

(cid:3)
) is an optimal solution of Prob-
Obviously  if (A
(cid:3) is an optimal solution of Problem (1)
lem (2)  then A
(cid:3)
(cid:3)
and t
). The proposed algorithm is a realization
= f (A
of the branch-and-bound scheme which responds to this
speciﬁc structure of the problem.
To this end  we ﬁrst deﬁne a prism T (S) ⊂ Rn+1 by

T = {(x; t) ∈ Rn × R : x ∈ S};

∪

Illustration of the pris-

Figure 1:
matic algorithm.

where S is an n-simplex. S is obtained from the n-
dimensional cube D at the initial iteration (as described
in Section 4.1)  or by the subdivision operation described
in the later part of this section (and the detail will be de-
scribed in Section 4.2). The prism T has n + 1 edges that
are vertical lines (i.e.  lines parallel to the t-axis) which
pass through the n + 1 vertices of S  respectively [11].
Our algorithm is an iterative procedure which mainly consists of two parts; branching and bounding 
as well as other branch-and-bound frameworks [13]. In branching  subproblems are constructed by
dividing the feasible region of a parent problem. And in bounding  we judge whether an optimal
solution exists in the region of a subproblem and its descendants by calculating an upper bound of
the subproblem and comparing it with an lower bound of the original problem. Some more details
for branching and bounding are described as follows.
Branching: The branching operation in our method is carried out using the property of a simplex.
That is  since  in a n-simplex  any r + 1 vertices are not on a r − 1-dimensional hyperplane for
i=1 Si  where p ≥ 2 and Si are n-simplices such
r ≤ n  any n-simplex can be divided as S =
that each pair of simplices Si; Sj(i ̸= j) intersects at most in common boundary points (the way of
i=1 Ti  where Ti = {(x; t) ∈
constructing such partition is explained in Section 4.2). Then  T =
Rn × R : x ∈ Si}  is a natural prismatic partition of T induced by the above simplical partition.
Bounding: For the bounding operation on Sk (resp.  Tk) at the iteration k  we consider a polyhe-
dral convex set Pk such that Pk ⊃ ~D  where ~D = {(x; t) ∈ Rn × R : x ∈ D; ^f (x) ≤ t} is the
region corresponding to the feasible set of Problem (2). At the ﬁrst iteration  such P is obtained as
where ~t is a real number satisfying ~t ≤ min{f (A) : A ⊆ N}. Here  ~t can be determined by using
some existing submodular minimization solver [23  8]. Or  at later iterations  more reﬁned Pk  such
that P0 ⊃ P1 ⊃ ··· ⊃ ~D  is constructed as described in Section 4.4.
As described in Section 4.3  a lower bound (cid:12)(Tk) of t − g(A) on the current prism Tk can be
calculated through the binary-integer linear programming (BILP) (or the linear programming (LP))
using Pk  obtained as described above. Let (cid:11) be the lowest function value (i.e.  an upper bound of
t − g(A) on ~D) found so far. Then  if (cid:12)(Tk) ≥ (cid:11)  we can conclude that there is no feasible solution
which gives a function value better than (cid:11) and can remove Tk without loss of optimality.
The pseudo-code of the proposed algorithm is described in Algorithm 1. In the following section 
we explain the details of the operations involved in this algorithm.

P0 = {(x; t) ∈ Rn × R : x ∈ S; t ≥ ~t};

p

∪

p

4 Basic Operations

Obviously  the procedure described in Section 3 involves the following basic operations:

1. Construction of the ﬁrst prism: A prism needs to be constructed from a hypercube at ﬁrst 
2. Subdivision process: A prism is divided into a ﬁnite number of sub-prisms at each iteration 
3. Bound estimation: For each prism generated throughout the algorithm  a lower bound for the
objective function t− g(A) over the part of the feasible set contained in this prism is computed 
4. Construction of cutting planes: Throughout the algorithm  a sequence of polyhedral convex sets
P0; P1;··· is constructed such that P0 ⊃ P1 ⊃ ··· ⊃ ~D. Each set Pj is generated by a cutting
plane to cut off a part of Pj(cid:0)1  and

5. Deletion of non-optimal prisms: At each iteration  we try to delete prisms that contain no

feasible solution better than the one obtained so far.

3

vTDS(0 0)(1 1)(0 1)rS1S2(1 0)1
2

3
4
5
6
7

8
9

10
11
12

13

14
15

Construct a simplex S0 ⊃ D  its corresponding prism T0 and a polyhedral convex set P0 ⊃ ~D.
Let (cid:11)0 be the best objective function value known in advance. Then  solve the BILP (5)
corresponding to (cid:11)0 and T0  and let (cid:12)0 = (cid:12)(T0; P0; (cid:11)0) and ( (cid:22)A0; (cid:22)t0) be the point satisfying
(cid:12)0 = (cid:22)t0 − g( (cid:22)A0).
Set R0 ← T0.
while Rk ̸= ∅

(cid:3)
k

∈ Rk satisfying (cid:12)k = (cid:12)(T

k ); ((cid:22)vk; (cid:22)tk) ∈ T
(cid:3)

(cid:3)
k .

else

Select a prism T
if ((cid:22)vk; (cid:22)tk) ∈ ~D then
Set Pk+1 = Pk.
Construct lk(x; t) according to (8)  and set Pk+1 = {(x; t) ∈ Pk : lk(x; t) ≤ 0}.
k) into a ﬁnite number of subprisms Tk;j(j∈Jk) (cf. Section 4.2).
(cid:3)

Subdivide T
For each j ∈ Jk  solve the BILP (5) with respect to Tk;j  Pk+1 and (cid:11)k.
Delete all Tk;j(j∈Jk) satisfying (DR1) or (DR2). Let Mk denote the collection of
remaining prisms Tk;j(j ∈ Jk)  and for each T ∈ Mk set

(cid:3)
k = T (S

(cid:12)(T ) = max{(cid:12)(T

k ); (cid:12)(T; Pk+1; (cid:11)k)}:
(cid:3)

Let Fk be the set of new feasible points detected while solving BILP in Step 11  and set
Delete all T∈Mk satisfying (cid:12)(T )≥(cid:11)k+1 and let Rk be Rk(cid:0)1 \ Tk ∈ Mk.
Set (cid:12)k+1 ← min{(cid:12)(T ) : T ∈ Mk} and k ← k + 1.

(cid:11)k+1 = min{(cid:11)k; min{t − g(A) : (A; t) ∈ Fk}}:

Algorithm 1: Pseudo-code of the prismatic algorithm for the D.S programming problem.

i2Av

4.1 Construction of the ﬁrst prism
∑
The initial simplex S0 ⊃ D (which yields the initial prism T0 ⊃ ~D) can be constructed as follows.
Now  let v and Av be a vertex of D and its corresponding subset in N  respectively  i.e.  v =

ei. Then  the initial simplex S0 ⊃ D can be constructed by
∑

S0 = {x ∈ Rn : xi ≤ 1(i ∈ Av); xi ≥ 0(i ∈ N \ Av); aT x ≤ (cid:13)};
i2NnAv

ei and (cid:13) = |N \ Av|. The n + 1 vertices of S0 are v and the n
where a =
points where the hyperplane {x ∈ Rn : aT x = (cid:13)} intersects the edges of the cone {x ∈ Rn : xi ≤
1(i ∈ Av); xi ≥ 0(i ∈ N \ Av)}. Note this is just an option and any n-simplex S ⊃ D is available.

ei −∑

i2Av

4.2 Sub-division of a prism

k

k

k

n+1

n+1

i=1 (cid:21)ivi
k;

k; : : : ; vn+1

] := conv{v1
∑

Let Sk and Tk be the simplex and prism at k-th iteration in the algorithm  respectively. We denote Sk
} which is deﬁned as the convex hull of its vertices
as Sk = [vi
k; : : : ; vn+1
v1

k; : : : ; vn+1
. Then  any r ∈ Sk can be represented as

∑
i=1 (cid:21)i = 1; (cid:21)i ≥ 0 (i = 1; : : : ; n + 1):
r =
k (i = 1; : : : ; n + 1). For each i satisfying (cid:21)i > 0  let Si

Suppose that r ̸= vi
Sk deﬁned by
∪
Then  the collection {Si
k : (cid:21)i > 0} deﬁnes a partition of Sk  i.e.  we have
i ̸= j [12]:
(cid:21)i>0Si
k deﬁned in Eq. (3) form a partition
In a natural way  the prisms T (Si
of Tk. This subdivision process of prisms is exhaustive  i.e.  for every nested (decreasing) sequence
of prisms {Tq} generated by this process  we have
q=0 Tq = (cid:28)  where (cid:28) is a line perpendicular
to Rn (a vertical line) [11]. Although several subdivision process can be applied  we use a classical
bisection one  i.e.  each simplex is divided into subsimplices by choosing in Eq. (3) as

k = ∅ for
k = Sk; int Si
∩1
k
k) generated by the simplices Si

k be the subsimplex of

k; : : : ; vi(cid:0)1

∩ int Sj

Si
k = [v1

; r; vi+1

; : : : ; vn+1

(3)

]:

k

k

k

where ∥vi1

k

− vi2

k

∥ = max{∥vi

k

− vj

k

k + vi2

r = (vi1
∥ : i; j ∈ {0; : : : ; n}; i ̸= j} (see Figure 1).

k )=2;

4

4.3 Lower bounds

Again  let Sk and Tk be the simplex and prism at k-th iteration in the algorithm  respectively. And 
let (cid:11) be an upper bound of t − g(A)  which is the smallest value of t − g(A) attained at a feasible
point known so far in the algorithm. Moreover  let Pk be a polyhedral convex set which contains ~D
and be represented as
(4)
where Ak is a real (m× n)-matrix and ak; bk ∈ Rm.1 Now  a lower bound (cid:12)(Tk; Pk; (cid:11)) of t− g(A)
over Tk ∩ ~D can be computed as follows.
k (i = 1; : : : ; n + 1) denote the vertices of Sk  and deﬁne I(Sk) = {i ∈ {1; : : : ; n + 1} :
First  let vi
vi
k

Pk = {(x; t) ∈ Rn × R : Akx + akt ≤ bk};

∈ Bn} and

{

(cid:22) =

min{(cid:11); min{ ^f (vi
(cid:11);

k) − ^g(vi

k) : i ∈ I(S)}};

if I(S) ̸= ∅;
if I(S) = ∅:

For each i = 1; : : : ; n + 1  consider the point (vi
intersects the level set {(x; t) : t − ^g(x) = (cid:22)}  i.e. 

k; ti

k) where the edge of Tk passing through vi

k

ti
k = ^g(vi

k) + (cid:22) (i = 1; : : : ; n + 1):

k) by H = {(x; t) ∈
Then  let us denote the uniquely deﬁned hyperplane through the points (vi
Rn×R : pT x−t = (cid:13)}  where p ∈ Rn and (cid:13) ∈ R. Consider the upper and lower halfspace generated
by H  i.e.  H+ = {(x; t) ∈ Rn × R : pT x − t ≤ (cid:13)} and H(cid:0) = {(x; t) ∈ Rn × R : pT x − t ≥ (cid:13)}.
If Tk ∩ ~D ⊆ H+  then we see from the supermodularity of g(A) (the concavity of ^g(x)) that
minft (cid:0) g(A) : (A; t) 2 (A; t)(Tk \ ~D)g (cid:21) minft (cid:0) g(A) : (A; t) 2 (A; t)(Tk \ H+)g

k; ti

(cid:21) minft (cid:0) ^g(x) : (x; t) 2 Tk \ H+g
= ti
k)(i = 1; : : : ; n + 1) = (cid:22):

k (cid:0) ^g(xi

(cid:3)

(cid:3)

; t

(cid:3) ∈ Bn) ((x
(cid:3)

) (∈ Tk ∩ Pk ∩ H(cid:0); x

Otherwise  we shift the hyperplane H (downward with respect to t) until it reaches a point z =
) is a point with the largest distance to H and the
(x
(cid:3) ∈ Bn) is in (A; t)(Tk ∩ Pk ∩ H(cid:0))). Let (cid:22)H denote the resulting
corresponding pair (A; t) (since x
supporting hyperplane  and denote by (cid:22)H+ the upper halfspace generated by (cid:22)H. Moreover  for each
k) be the point where the edge of T passing through vi
k intersects
i = 1; : : : ; n + 1  let zi = (vi
(cid:22)H. Then  it follows (A; t)(Tk ∩ ~D) ⊂ (A; t)(Tk ∩ Pk) ⊂ (A; t)(Tk ∩ (cid:22)H+)  and hence

k; (cid:22)ti

(cid:3)
; t

minft (cid:0) g(A) : (A; t) 2 (A; t)(Tk \ ~D)g > minft (cid:0) g(A) : (A; t) 2 (A; t)(Tk \ (cid:22)H+)g

= minf(cid:22)ti

k (cid:0) ^g(vi

k) : i = 1; : : : ; n + 1g:

Now  the above consideration leads to the following BILP in (λ; x; t):

∑

(∑
i=1 ti(cid:21)i − t

n+1

)

max
(cid:21);x;t

k; x ∈ Bn;

n+1

i=1 (cid:21)ivi

∑
s.t. Akx + akt ≤ bk; x =
i=1 (cid:21)i = 1; (cid:21)i ≥ 0 (i = 1; : : : ; n + 1);
∑

n+1

(5)

(cid:3)

(cid:3)

; x

(cid:3)
; t

∑

(cid:3)
) be an optimal solution of BILP (5) and c

where A  a and b are given in Eq. (4).
Proposition 1. (a) If the system (5) has no solution  then intersection (A; t)(Tk ∩ ~D) is empty.
− t
(b) Otherwise  let (λ
optimal value  respectively. Then  the following statements hold:
(cid:3) ≤ 0  then (A; t)(Tk ∩ ~D) ⊂ (A; t)(H+).
(b1) If c
(cid:3)
(b2) If c
(cid:22) − c
(cid:3)
Proof. First  we prove part (a). Since every point in Sk is uniquely representable as x =
i=1 (cid:21)ivi 
we see from Eq. (4) that the set (A; t)(Tk ∩ Pk) coincide with the feasible set of problem (5).
Therefore  if the system (5) has no solution  then (A; t)(Tk∩Pk) = ∅  and hence (A; t)(Tk∩ ~D) = ∅
(because ~D ⊂ Pk). Next  we move to part (b). Since the equation of H is pT x − t = (cid:13)  it follows

(cid:3)
k)  zi = (vi
k; t

− ^g(vi
∑

(i = 1; : : : ; n + 1).

> 0  then z = (

n+1
i=1 (cid:21)ivi

k) = (vi

) and (cid:22)ti
k

n+1
i=1 ti(cid:21)

− c
(cid:3)

(cid:3) its

k) =

k; ti
k

k; (cid:22)ti

n+1

(cid:3)
i

=

1Note that Pk is updated at each iteration  which does not depend on Sk  as described in Section 4.4.

5

− t:

that determining the hyperplane (cid:22)H and the point z amounts to solving the binary integer linear
programming problem:

k

n+1

max pT x − t

s.t. (x; t) ∈ Tk ∩ Pk; x ∈ Bn:

n+1

i=1 (cid:21)ivi

k

(∑

n+1

i=1 (cid:21)ipT vi

k

n+1

i=1 (cid:21)i((cid:13) + ti

On the other hand  since (vi

k; ti
pT x − t =

k = (cid:13) (i = 1; : : : ; n + 1)  and hence

Here  we note that the objective of the above can be represented as

)
pT x − t = pT
∑
k) ∈ H  we have pT vi

∑
− t =
∑
− ti
k) − t =
Thus  the two BILPs (5) and (6) are equivalent. And  if (cid:13)
value in Eq. (6)  then (cid:13)
obtained by a parallel shift of H in the direction H+. Therefore  c
(A; t)(H+)  and hence (A; t)(Tk ∩ ~D) ⊂ (A; t)(H+).
Since (cid:22)H = {(x; t) ∈ Rn × R : pT x − t = (cid:13)
k; (cid:22)ti
we see that for each intersection point (vi
with (cid:22)H (and H)  we have pT vi
k = (cid:13)
(cid:3)  and (using ti
k
(cid:22)ti
k = ti
From the above  we see that  in the case (b1)  (cid:22) constitutes a lower bound of (t−g(A)) wheres  in the
k) : i = 1; : : : ; n + 1}. Thus  Proposition 1
case (b2)  such a lower bound is given by min{(cid:22)ti
provides the lower bound

i=1 ti
(cid:3) denotes the optimal objective function
(cid:3) ≤ (cid:13)  then it follows from the deﬁnition of H+ that (cid:22)H is
(cid:3) ≤ 0 implies (A; t)(Tk ∩ Pk) ⊂
(cid:3)} and H = {(x; t) ∈ Rn × R : pT x − t = (cid:13)}
k)) of the edge of Tk passing through vi
k; ti
− ti
k = (cid:13)  respectively. This implies that
k

k) (and (vi
(cid:3) and pT vi
k = ^g(vi

k) + (cid:22) − c
(cid:3).

k(cid:21)i − t + (cid:13):

k + (cid:13) − (cid:13)

k) + (cid:22)) that (cid:22)ti

k = ^g(vi

= c

+ (cid:13). If (cid:13)

− ^g(vi

k

(cid:3)

= ti
k

− c

(cid:3)

(cid:3)

− (cid:22)ti

{

(cid:12)k(Tk; Pk; (cid:11)) =

+∞;
(cid:22);
(cid:22) − c
(cid:3)

if BILP (5) has no feasible point;
(cid:3) ≤ 0;
if c
(cid:3)
if c
> 0:

(6)

k

(7)

As stated in Section 4.5  Tk can be deleted from further consideration when (cid:12)k = ∞ or (cid:22).

4.4 Outer approximation
The polyhedral convex set Pk ⊃ ~D used in the preceding section is updated in each iteration  i.e. 
a sequence P0; P1;··· is constructed such that P0 ⊃ P1 ⊃ ··· ⊃ ~D. The update from Pk to Pk+1
(k = 0; 1; : : :) is done in a way which is standard for pure outer approximation methods [12]. That
is  a certain linear inequality lk(x; t) ≤ 0 is added to the constraint set deﬁning Pk  i.e.  we set

Pk+1 = Pk ∩ {(x; t) ∈ Rn × R : lk(x; t) ≤ 0}:

lk(x; t) = sT

The function lk(x; t) is constructed as follows. At iteration k  we have a lower bound (cid:12)k of t −
g(A) as deﬁned in Eq. (7)  and a point ((cid:22)vk; (cid:22)tk) satisfying (cid:22)tk − ^g((cid:22)vk) = (cid:12)k. We update the outer
approximation only in the case ((cid:22)vk; (cid:22)tk) =∈ ~D. Then  we can set
k) − t
k [(x; t) − zk] + ( ^f (x
(cid:3)

(8)
where sk is a subgradient of ^f (x) − t at zk. The subgradient can be calculated as  for example 
stated in [9] (see also [7]).
Proposition 2. The hyperplane {(x; t) ∈ Rn × R : lk(x; t) = 0} strictly separates zk from ~D  i.e. 
lk(zk) > 0  and lk(x; t) ≤ 0 for 8
Proof. Since we assume that zk =∈ ~D  we have lk(zk) = ( ^f (x
an immediate consequence of the deﬁnition of a subgradient.

k) − t
(cid:3)
(cid:3)
k). And  the latter inequality is

(x; t) ∈ ~D.

(cid:3)
k);

4.5 Deletion rules

At each iteration of the algorithm  we try to delete certain subprisms that contain no optimal solution.
To this end  we adopt the following two deletion rules:
(DR1) Delete Tk if BILP (5) has no feasible solution.

6

Figure 2: Training errors  test errors and computational time versus (cid:21) for the prismatic algorithm
and the supermodular-sumodular procedure.

p
120
120
120
120

n
150
150
150
150

k
5
10
20
40

exact(PRISM)
1.8e-4 (192.6)
2.0e-4 (262.7)
7.3e-4 (339.2)
1.7e-3 (467.6)

SSP

1.9e-4 (0.93)
2.4e-4 (0.81)
7.8e-4 (1.43)
2.1e-3 (1.17)

greedy

1.8e-4 (0.45)
2.3e-4 (0.56)
8.3e-4 (0.59)
2.9e-3 (0.63)

lasso

1.9e-4 (0.78)
2.4e-4 (0.84)
7.7e-4 (0.91)
1.9e-3 (0.87)

Table 1: Normalized mean-square prediction errors of training and test data by the prismatic algo-
rithm  the supermodular-submodular procedure  the greedy algorithm and the lasso.

(cid:3) of BILP (5) satisﬁes c

(cid:3) ≤ 0.

(DR2) Delete Tk if the optimal value c
The feasibility of these rules can be seen from Proposition 1 as well as the D.C. programing prob-
lem [11]. That is  (DR1) follows from Proposition 1 that in this case Tk ∩ ~D = ∅  i.e.  the prism Tk
is infeasible  and (DR2) from Proposition 1 and from the deﬁnition of (cid:22) that the current best feasible
solution cannot be improved in T .

5 Experimental Results

We ﬁrst provide illustrations of the proposed algorithm and its solution on toy examples from feature
selection in Section 5.1  and then apply the algorithm to an application of discriminative structure
learning using the UCI repository data in Section 5.2. The experiments below were run on a 2.8
GHz 64-bit workstation using Matlab and IBM ILOG CPLEX ver. 12.1.

5.1 Application to feature selection

J XJ )1=2. Thus  our problem is minw2Rp

∥y − Xw∥2
J XJ )1=2  where J is the support of w. Or equivalently  minA2V g(A) + (cid:21) · tr(X T

We compared the performance and solutions by the proposed prismatic algorithm (PRISM)  the
supermodular-submodular procedure (SSP) [21]  the greedy method and the LASSO. To this end 
we generated data as follows: Given p  n and k  the design matrix X ∈ Rn(cid:2)p is a matrix of i.i.d.
Gaussian components. A feature set J of cardinality k is chosen at random and the weights on the
selected features are sampled from a standard multivariate Gaussian distribution. The weights on
(cid:0)1=2∥Xw∥2ϵ  where w is the weights on features
other features are 0. We then take y = Xw + n
and ϵ is a standard Gaussian vector. In the experiment  we used the trace norm of the submatrix
2 + (cid:21) ·
corresponding to J  XJ  i.e.  tr(X T
A XA)1=2 
tr(X T
where g(A) := minwA2RjAj ∥y − XAwA∥2. Since the ﬁrst term is a supermodular function [4] and
the second is a submodular function  this problem is the D.S. programming problem.
First  the graphs in Figure 2 show the training errors  test errors and computational time versus (cid:21) for
PRISM and SSP (for p = 120  n = 150 and k = 10). The values in the graphs are averaged over 20
datasets. For the test errors  we generated another 100 data from the same model and applied the es-
timated model to the data. And  for all methods  we tried several possible regularization parameters.
From the graphs  we can see the following: First  exact solutions (by PRISM) always outperform
approximate ones (by SSP). This would show the signiﬁcance of optimizing the submodular-norm.
That is  we could obtain the better solutions (in the sense of prediction error) by optimizing the
objective with the submodular norm more exactly. And  our algorithm took longer especially when

1
2n

7

(cid:20)(cid:19)(cid:827)(cid:23)(cid:20)(cid:19)(cid:827)(cid:22)(cid:20)(cid:17)(cid:27)(cid:20)(cid:17)(cid:28)(cid:21)(cid:21)(cid:17)(cid:20)(cid:21)(cid:17)(cid:21)(cid:21)(cid:17)(cid:22)(cid:21)(cid:17)(cid:23)(cid:91)(cid:98)(cid:20)(cid:19)(cid:827)(cid:25)(cid:98)(cid:98)Exact (Prismatic)Approx. (Supermodular-submodular)λTraining Error(cid:20)(cid:19)(cid:827)(cid:23)(cid:20)(cid:19)(cid:827)(cid:22)(cid:21)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:91)(cid:98)(cid:20)(cid:19)(cid:827)(cid:23)λTest Error(cid:98)(cid:98)Exact (Prismatic)Approx. (Supermodular-sumodular)(cid:20)(cid:19)(cid:827)(cid:23)(cid:20)(cid:19)(cid:827)(cid:22)(cid:19)(cid:20)(cid:19)(cid:19)(cid:21)(cid:19)(cid:19)(cid:22)(cid:19)(cid:19)(cid:23)(cid:19)(cid:19)(cid:24)(cid:19)(cid:19)(cid:25)(cid:19)(cid:19)(cid:26)(cid:19)(cid:19)λTime [second](cid:98)(cid:98)Exact (Prismatic)Approx. (Supermodular-sumodular)Data
Chess
German
Census-income
Hepatitis

Attr. Class
36
20
40
19

2
2
2
2

exact (PRISM)
96.6 (±0.69)
70.0 (±0.43)
73.2 (±0.64)
86.9 (±1.89)

approx. (SSP)
94.4 (±0.71)
69.9 (±0.43)
71.2 (±0.74)
84.3 (±2.31)

generative
92.3 (±0.79)
69.1 (±0.49)
70.3 (±0.74)
84.2 (±2.11)

Table 2: Empirical accuracy of the classiﬁers in [%] with standard deviation by the TANs discrim-
inatively learned with PRISM or SSP and generatively learned with a submodular minimization
solver. The numbers in parentheses are computational time in seconds.

(cid:21) smaller. This would be because smaller (cid:21) basically gives a larger size subset (solution). Also 
Table 1 shows normalized-mean prediction errors by the prismatic algorithm  the supermodular-
submodular procedure  the greedy method and the lasso for several k. The values are averaged over
10 datasets. This result also seems to show that optimizing the objective with the submodular norm
exactly is signiﬁcant in the meaning of prediction errors.

5.2 Application to discriminative structure learning

Our second application is discriminative structure learning using the UCI machine learning reposi-
tory.2 Here  we used CHESS  GERMAN  CENSUS-INCOME (KDD) and HEPATITIS  which have
two classes. The Bayesian network topology used was the tree augmented naive Bayes (TAN) [22].
We estimated TANs from data both in generative and discriminative manners. To this end  we used
the procedure described in [20] with a submodular minimization solver (for the generative case)  and
the one [21] combined with our prismatic algorithm (PRISM) or the supermodular-submodular pro-
cedure (SSP) (for the discriminative case). Once the structures have been estimated  the parameters
were learned based on the maximum likelihood method.
Table 2 shows the empirical accuracy of the classiﬁer in [%] with standard deviation for these
datasets. We used the train/test scheme described in [6  22]. Also  we removed instances with
missing values. The results seem to show that optimizing the EAR measure more exactly could
improve the performance of classiﬁcation (which would mean that the EAR is signiﬁcant as the
measure of discriminative structure learning in the sense of classiﬁcation).

6 Conclusions

In this paper  we proposed a prismatic algorithm for the D.S. programming problem (1)  which is the
ﬁrst exact algorithm for this problem and is a branch-and-bound method responding to the structure
of this problem. We developed the algorithm based on the analogy with the D.C. programming
problem through the continuous relaxation of solution spaces and objective functions with the help
of the Lov´asz extension. We applied the proposed algorithm to several situations of feature selection
and discriminative structure learning using artiﬁcial and real-world datasets.
The D.S. programming problem addressed in this paper covers a broad range of applications in
machine learning. In future works  we will develop a series of the presented framework specialized
to the speciﬁc structure of each problem. Also  it would be interesting to investigate the extension
of our method to enumerate solutions  which could make the framework more useful in practice.

Acknowledgments

This research was supported in part by JST PRESTO PROGRAM (Synthesis of Knowledge for
Information Oriented Society)  JST ERATO PROGRAM (Minato Discrete Structure Manipulation
System Project) and KAKENHI (22700147). Also  we are very grateful to the reviewers for helpful
comments.

2http://archive.ics.uci.edu/ml/index.html

8

References
[1] F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Infor-

mation Processing Systems 23  pages 118–126  2010.

[2] J. A. Bilmes. Dynamic Bayesian multinets. In Proc. of the 16th Conf. on Uncertainty in Artiﬁcial Intelli-

gence (UAI’00)  pages 38–45  2000.

[3] T. S. Caetano  J. J. McAuley  L. Cheng  Q. V. Le  and A. J. Smola. Learning graph matching. IEEE Trans.

on Pattern Analysis and Machine Intelligence  31(6):1048–1058  2009.

[4] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proc. of the 40th annual

ACM symp. on Theory of computing (STOC’08)  pages 45–54  2008.

[5] J. Edmonds. Submodular functions  matroids  and certain polyhedra. In R. Guy  H. Hanani  N. Sauer  and

J. Sch¨onheim  editors  Combinatorial structures and their applications  pages 69–87  1970.

[6] N. Friedman  D. Geiger  and M. Goldszmidt. Bayesian network classiﬁer. 29:131–163  1997.
[7] S. Fujishige. Submodular Functions and Optimization. Elsevier  2 edition  2005.
[8] S. Fujishige  T. Hayashi  and S. Isotani. The minimum-norm-point algorithm applied submodular function
minimization and linear programming. Technical report  Research Institute for Mathematical Sciences 
Kyoto University  2006.

[9] E. Hazan and S. Kale. Beyond convexity: online submodular minimization.

Information Processing Systems 22  pages 700–708  2009.

In Advances in Neural

[10] S. Hoi  R. Jin  J. Zhu  and M. Lyu. Batch mode active learning and its application to medical image

classiﬁcation. In Proc. of the 23rd Int’l Conf. on Machine learning (ICML’06)  pages 417–424  2006.

[11] R. Horst  T. Q. Phong  Ng. V. Thoai  and J. de Vries. On solving a D.C. programming problem by a

sequence of linear programs. Journal of Global Optimization  1:183–203  1991.

[12] R. Horst and H. Tuy. Global Optimization (Deterministic Approaches). Springer  3 edition  1996.
[13] T. Ibaraki. Enumerative approaches to combinatorial optimization. In J.C. Baltzer and A.G. Basel  editors 

Annals of Operations Research  volume 10 and 11. 1987.

[14] Y. Kawahara  K. Nagano  K. Tsuda  and J. A. Bilmes. Submodularity cuts and applications. In Advances

in Neural Information Processing Systems 22  pages 916–924. MIT Press  2009.

[15] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proc. of the 27th

Int’l Conf. on Machine learning (ICML’10)  pages 567–574. Omnipress  2010.

[16] A. Krause  H. B. McMahan  C. Guestrin  and A. Gupta. Robust submodular observation selection. Journal

of Machine Learning Research  9:2761–2801  2008.

[17] L. Lov´asz. Submodular functions and convexity. In A. Bachem  M. Gr¨otschel  and B. Korte  editors 

Mathematical Programming – The State of the Art  pages 235–257. 1983.

[18] K. Murota. Discrete Convex Analysis. Monographs on Discrete Math and Applications. SIAM  2003.
[19] K. Nagano  Y. Kawahara  and S. Iwata. Minimum average cost clustering. In Advances in Neural Infor-

mation Processing Systems 23  pages 1759–1767  2010.

[20] M. Narasimhan and J. A. Bilmes. PAC-learning bounded tree-width graphical models. In Proc. of the

20th Ann. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’04)  pages 410–417  2004.

[21] M. Narasimhan and J. A. Bilmes. A submodular-supermodular procedure with applications to discrimina-
tive structure learning. In Proc. of the 21st Ann. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’05) 
pages 404–412  2005.

[22] F. Pernkopf and J. A. Bilmes. Discriminative versus generative parameter and structure learning of
bayesian network classiﬁers. In Proc. of the 22nd Int’l Conf. on Machine Learning (ICML’05)  pages
657–664  2005.

[23] M. Queyranne. Minimizing symmetric submodular functions. Math. Prog.  82(1):3–12  1998.
[24] C. Rother  T. Minka  A. Blake  and V. Kolmogorov. Cosegmentation of image pairs by histogram
matching-incorporating a global constraint into mrfs. In Proc. of the 2006 IEEE Comp. Soc. Conf. on
Computer Vision and Pattern Recognition (CVPR’06)  pages 993–1000  2006.

[25] A. Shekhovtsov. Supermodular decomposition of structural labeling problem. Control Systems and Com-

puters  20(1):39–48  2006.

[26] A. Shekhovtsov  V. Kolmogorov  P. Kohli  V. Hlav c  C. Rother  and P. Torr. Lp-relaxation of binarized

energy minimization. Technical Report CTU-CMP-2007-27  Czech Technical University  2007.

[27] M. Thoma  H. Cheng  A. Gretton  H. Han  H. P. Kriegel  A. J. Smola  S. Y. Le Song Philip  X. Yan  and
K. Borgwardt. Near-optimal supervised feature selection among frequent subgraphs. In Proc. of the 2009
SIAM Conf. on Data Mining (SDM’09)  pages 1076–1087  2008.

9

,Dan Alistarh
Jennifer Iglesias
Milan Vojnovic
Remi Lam
Karen Willcox