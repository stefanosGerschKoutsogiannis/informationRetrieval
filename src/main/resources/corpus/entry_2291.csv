2010,Generative Local Metric Learning for Nearest Neighbor Classification,We consider the problem of learning a local metric to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects  and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct  the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction  which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models.,Generative Local Metric Learning for

Nearest Neighbor Classiﬁcation

Yung-Kyun Noh1 2

Byoung-Tak Zhang2

Daniel D. Lee1

1GRASP Lab  University of Pennsylvania  Philadelphia  PA 19104  USA
2Biointelligence Lab  Seoul National University  Seoul 151-742  Korea

nohyung@seas.upenn.edu  btzhang@snu.ac.kr  ddlee@seas.upenn.edu

Abstract

We consider the problem of learning a local metric to enhance the performance of
nearest neighbor classiﬁcation. Conventional metric learning methods attempt to
separate data distributions in a purely discriminative manner; here we show how
to take advantage of information from parametric generative models. We focus
on the bias in the information-theoretic error arising from ﬁnite sampling effects 
and ﬁnd an appropriate local metric that maximally reduces the bias based upon
knowledge from generative models. As a byproduct  the asymptotic theoretical
analysis in this work relates metric learning with dimensionality reduction  which
was not understood from previous discriminative approaches. Empirical exper-
iments show that this learned local metric enhances the discriminative nearest
neighbor performance on various datasets using simple class conditional gener-
ative models.

1

Introduction

The classic dichotomy between generative and discriminative methods for classiﬁcation in machine
learning can be clearly seen in two distinct performance regimes as the number of training examples
is varied [12  18]. Generative models—which employ models ﬁrst to ﬁnd the underlying distribu-
tion p(x|y) for discrete class label y and input data x ∈ RD—typically outperform discriminative
methods when the number of training examples is small  due to smaller variance in the generative
models which compensates for any possible bias in the models. On the other hand  more ﬂexible
discriminative methods—which are interested in a direct measure of p(y|x)—can accurately cap-
ture the true posterior structure p(y|x) when the number of training examples is large. Thus  given
enough training examples  the best performing classiﬁcation algorithms have typically employed
purely discriminative methods.
However  due to the curse of dimensionality when D is large  the number of data examples may
not be sufﬁcient for discriminative methods to approach their asymptotic performance limits. In this
case  it may be possible to improve discriminative methods by exploiting knowledge of generative
models. There has been recent work on hybrid models showing some improvement [14  15  20]  but
mainly the generative models have been improved through the discriminative formulation. In this
work  we consider a very simple discriminative classiﬁer  the nearest neighbor classiﬁer  where the
class label of an unknown datum is chosen according to the class label of the nearest known datum.
The choice of a metric to deﬁne nearest is then crucial  and we show how this metric can be locally
deﬁned based upon knowledge of generative models.
Previous work on metric learning for nearest neighbor classiﬁcation has focused on a purely discrim-
inative approach. The metric is parameterized by a global quadratic form which is then optimized
on the training data to maximize pairwise separation between dissimilar points  and to minimize the
pairwise separation of similar points [3  9  10  21  26]. Here  we show how the problem of learning

1

a metric can be related to reducing the theoretical bias of the nearest neighbor classiﬁer. Though
the performance of the nearest neighbor classiﬁer has good theoretical guarantees in the limit of
inﬁnite data  ﬁnite sampling effects can introduce a bias which can be minimized by the choice of an
appropriate metric. By directly trying to reduce this bias at each point  we will see the classiﬁcation
error is signiﬁcantly reduced compared to the global class-separating metric.
We show how to choose such a metric by analyzing the probability distribution on nearest neighbors 
provided we know the underlying generative models. Analyses of nearest neighbor distributions
have been discussed before [11  19  24  25]  but we take a simpler approach and derive the metric-
dependent term in the bias directly. We then show that minimizing this bias results in a semi-deﬁnite
programming optimization that can be solved analytically  resulting in a locally optimal metric. In
related work  Fukunaga et al. considered optimizing a metric function in a generative setting [7  8] 
but the resulting derivation was inaccurate and does not improve nearest neighbor performance.
Jaakkola et al. ﬁrst showed how a generative model can be used to derive a special kernel  called the
Fisher kernel [12]  which can be related to a distance function. Unfortunately  the Fisher kernel is
quite generic  and need not necessarily improve nearest neighbor performance.
Our generative approach also provides a theoretical relationship between metric learning and the
dimensionality reduction problem.
In order to ﬁnd better projections for classiﬁcation  research
on dimensionality reduction using labeled training data has utilized information-theoretic measures
such as Bhattacharrya divergence [6] and mutual information [2  17]. We argue how these prob-
lems can be connected with metric learning for nearest neighbor classiﬁcation within the general
framework of F-divergences. We will also explain how dimensionality reduction is entirely different
from metric learning in the generative approach  whereas in the discriminative setting  it is simply a
special case of metric learning where particular directions are shrunk to zero.
The remainder of the paper is organized as follows. In section 2  we motivate by comparing the met-
ric dependency of the discriminative and generative approaches for nearest neighbor classiﬁcation.
After we derive the bias due to ﬁnite sampling in section 3  we show  in section 4  how minimizing
this bias results in a local metric learning algorithm. In section 5  we explain how metric learning
should be understood in a generative perspective  in particular  its relationship with dimensionality
reduction. Experiments on various datasets are presented in section 6  comparing our experimental
results with other well-known algorithms. Finally  in section 7  we conclude with a discussion of
future work and possible extensions.

2 Metric and Nearest Neighbor Classiﬁcation

In recent work  determining a good metric for nearest neighbor classiﬁcation is believed to be cru-
cial. However  traditional generative analysis of this problem has simply ignored the metric issue
with good reason  as we will see in section 2.2. In this section  we explain the apparent contra-
diction between two different approaches to this issue  and brieﬂy describe how the resolution of
this contradiction will lead to a metric learning method that is both theoretically and practically
plausible.

2.1 Metric Learning for Nearest Neighbor Classiﬁcation

A nearest neighbor classiﬁer determines the label of an unknown datum according to the label of
its nearest neighbor. In general  the meaning of the term nearest is deﬁned along with the notion
of distance in data space. One common choice for this distance is the Mahalanobis distance with
a positive deﬁnite square matrix A ∈ RD×D where D is the dimensionality of data space. In this
case  the distance between two points x1 and x2 is deﬁned as

(cid:113)

d(x1  x2) =

(x1 − x2)T A(x1 − x2)  

(1)

i=1

and the nearest datum xN N is one having minimal distance to the test point among labeled training
data in {xi}N
In this classiﬁcation task  the results are highly dependent on the choice of matrix A  and prior work
has attempted to improve the performance by a better choice of A. This recent work has assumed
the following common heuristic: the training data in different classes should be separated in a new

2

metric space. Given training data  a global A is optimized such that directions separating different
class data are extended  and directions binding same class data together are shrunk [3  9  10  21  26].
However  in terms of the test results  these conventional methods do not improve the performance
dramatically  which will be shown in our later experiments on large datasets  and we show why only
small improvements arise in our theoretical analysis.

2.2 Theoretical Performance of Nearest Neighbor Classiﬁer

Contrary to recent metric learning approaches  a simple theoretical analysis using a generative model
displays no sensitivity to the choice of the metric. We consider i.i.d. samples generated from two
different distributions p1(x) and p2(x) over the vector space x ∈ RD. With inﬁnite samples  the
probability of misclassiﬁcation using a nearest neighbor classiﬁer can be obtained:

(cid:90)

EAsymp =

p1(x)p2(x)
p1(x) + p2(x) dx 

(2)

which is better known by its relationship to an upper bound  twice the optimal Bayes error [4  7  8].
By looking at the asymptotic error in a linearly transformed z-space  we can show that Eq. (2) is
invariant to the change of metric. If we consider a linear transformation z = LT x using a full
rank matrix L  and the distribution qc(z) for c ∈ {1  2} in z-space satisfying pc(x)dx = qc(z)dz
and accompanying measure change dz = |L|dx  we see EAsymp in z-space is unchanged. Since
any positive deﬁnite A can be decomposed as A = LLT   we can say the asymptotic error remains
constant even as the metric shrinks or expands any spatial directions in data space.
This difference in behavior in terms of metric dependence can be understood as a special property
that arises from inﬁnite data. When we do not have inﬁnite samples  the expectation of error is
biased in that it deviates from the asymptotic error  and the bias is dependent on the metric. From
a theoretical perspective  the asymptotic error is the theoretical limit of expected error  and the bias
reduces as the number of samples increase. Since this difference is not considered in previous
research  the aforementioned metric will not exhibit performance improvements when the sample
number is large.
In the next section  we look at the performance bias associated with ﬁnite sampling directly and ﬁnd
a metric that minimizes the bias from the asymptotic theoretical error.

3 Performance Bias due to Finite Sampling

Here  we obtain the expectation of nearest neighbor classiﬁcation error from the distribution of
nearest neighbors in different classes. As we consider ﬁnite number of samples  the nearest neighbor
from a point x0 appears at a ﬁnite distance dN > 0. This non-zero distance gives rise to the
performance difference from its theoretical limit (2). A twice-differentiable distribution p(x) is
considered and approximated to second order near a test point x0:

p(x) (cid:39) p(x0) + ∇p(x)|T

x=x0(x − x0) +

(3)
with the gradient ∇p(x) and Hessian matrix ∇∇p(x) deﬁned by taking derivatives with respect to
x.
Now  under the condition that the nearest neighbor appears at the distance dN from the test point 
the expectation of the probability p(xN N ) at a nearest neighbor point is derived by averaging the
probability over the D-dimensional hypersphere of radius dN   as in Fig. 1. After averaging  the
gradient term disappears  and the resulting expectation is the sum of the probability at x0 and a
residual term containing the Laplacian of p. We replace this expected probability by ˜p(x0).

(x − x0)

(x − x0)T∇∇p(x)(cid:12)(cid:12)x=x0

1
2

(cid:104)

(cid:105)

(cid:12)(cid:12)(cid:12)dN   x0
(cid:104)
· ∇2p|x=x0 ≡ ˜p(x0)

ExN N

p(xN N )

1
2 ExN N

= p(x0) +
= p(x0) + d2
N
2D

(x − x0)T∇∇p(x)(x − x0)

(cid:12)(cid:12)(cid:12)(cid:107)x − x0(cid:107)2 = d2

N

(cid:105)

(4)

3

Figure 1: The nearest neighbor xN N appears at a ﬁnite distance dN from x0 due to ﬁnite sampling.
Given the data distribution p(x)  the average probability density function over the surface of a D
dimensional hypersphere is ˜p(x0) = p(x0) + d2

4D∇2p|x=x0 for small dN .

N

where the scalar Laplacian ∇2p(x) is given by the sum of the eigenvalues of the Hessian ∇∇p(x).
If we look at the expected error  it is the expectation of the probability that the test point and its
neighbor are labeled differently. In other words  the expectation error EN N is the expectation of
e(x  xN N ) = p(C1|x)p(C2|xN N ) + p(C2|x)p(C1|xN N ) over both the distribution of x and the
distribution of nearest neighbor xN N for a given x:

(cid:34)

(cid:104)

(cid:105)(cid:35)
(cid:12)(cid:12)(cid:12)x

EN N = Ex

ExN N

e(x  xN N )

(5)

(cid:104)

We then replace the posteriors p(C|x) and p(C|xN N ) as pc(x)/(p1(x) + p2(x)) and
pc(xN N )/(p1(xN N ) + p2(xN N )) respectively  and approximate the expectation of the posterior
at a ﬁxed distance dN from test point x using ˜pc(x)/(˜p1(x) + ˜p2(x)). If
ExN N
=
we expand EN N with respect to dN   and take the expectation using the decomposition  ExN N

p(C|xN N )

(cid:105)

(cid:104)

f

  then the expected error is given to leading order by

(cid:12)(cid:12)(cid:12)dN   x
(cid:105)
(cid:12)(cid:12)(cid:12)dN
(cid:105)(cid:105)
(cid:90)

(cid:104)

f

ExN N
EN N (cid:39)

(cid:104)

EdN

dx +

(cid:90)

p1p2
p1 + p2
EdN [d2
N ]
4D

(cid:104)

(cid:105)
2∇2p1 − p1p2(∇2p1 + ∇2p2)

1

1∇2p2 + p2
p2

(6)
N ] → 0 with an inﬁnite number of samples  this error converges to the asymptotic limit
When EdN [d2
in Eq. (2) as expected. The residual term can be considered as the ﬁnite sampling bias of the error
discussed earlier. Under the coordinate transformation z = LT x and the distributions p(x) on x and
q(z) on z  we see that this bias term is dependent upon the choice of a metric A = LLT .

(p1 + p2)2

dx

(cid:104)

1

(q1 + q2)2
(p1 + p2)2 tr

1

=

(cid:104)
1∇2q2 + q2
q2

A−1(cid:16)

2∇2q1 − q1q2
1∇∇p2 + p2
p2

(cid:0)∇2q1 + ∇2q2

(cid:1)(cid:105)

dz

2∇∇p1 − p1p2 (∇∇p1 + ∇∇p2)

(7)

(cid:17)(cid:105)

dx

which is derived using p(x)dx = q(z)dz and |L|∇2q = tr[A−1∇∇p]. Expectation of squared
N ] is related to the determinant |A|  which will be ﬁxed to 1. Thus  ﬁnding the
distance EdN [d2
metric that minimizes the quantity given in Eq. (7) at each point is equivalent to minimizing the
metric-dependent bias in Eq. (6).

4

4 Reducing Deviation from the Asymptotic Performance

Finding the local metric that minimizes the bias can be formulated as a semi-deﬁnite programming
(SDP) problem of minimizing squared residual with respect to a positive semi-deﬁnite metric A:

(tr[A−1B])2

s.t.

min
A

|A| = 1  A (cid:23) 0

(8)

where the matrix B at each point is

B = p2

1∇∇p2 + p2

2∇∇p1 − p1p2(∇∇p1 + ∇∇p2).

(9)
This is a simple SDP having an analytical solution where the solution shares the eigenvectors with
B. Let Λ+ ∈ Rd+×d+ and Λ− ∈ Rd−×d− be the diagonal matrices containing the positive and
negative eigenvalues of B respectively. If U+ ∈ RD×d+ contains the eigenvectors corresponding to
the eigenvalues in Λ+ and U− ∈ RD×d− contains the eigenvectors corresponding to the eigenvalues
in Λ−  we use the solution given by

Aopt = [U+ U−]

[U+ U−]T

(10)

(cid:18) d+Λ+

0

(cid:19)

0

−d−Λ−

The solution Aopt is a local metric since we assumed that the nearest neighbor was close to the test
point satisfying Eq. (3). In principle  distances should then be deﬁned as geodesic distances using
this local metric on a Riemannian manifold. However  this is computationally difﬁcult  so we use
the surrogate distance A = γI + Aopt and treat γ as a regularization parameter that is learned in
addition to the local metric Aopt.
The multiway extension of this problem is straightforward. The asymptotic error with C-class dis-
dx using the posteriors of each
tributions can be extended to 1
/
C
class  and it replaces B in Eq. (9) by the extended matrix:

j(cid:54)=i pj

(cid:17)

i pi

c=1

(cid:80)C
C(cid:88)

i=1

pc

(cid:82)(cid:16)
(cid:80)
(cid:88)

j(cid:54)=i

B =

∇2pi

j − pi
p2

(cid:17)
 .

(cid:16)(cid:80)
(cid:88)

j(cid:54)=i

pj

(11)

5 Metric Learning in Generative Models

Traditional metric learning methods can be understood as being purely discriminative. In contrast to
our method that directly considers the expected error  those methods are focused on maximizing the
separation of data belonging to different classes. In general  their motivations are compared to the
supervised dimensionality reduction methods  which try to ﬁnd a low dimensional space where the
separation between classes is maximized. Their dimensionality reduction is not that different from
metric learning  but often as a special case where metric in particular directions is forced to be zero.
In the generative approach  however  the relationship between dimensionality reduction and metric
learning is different. As in the discriminative case  dimensionality reduction in generative models
tries to obtain class separation in a transformed space. It assumes particular parametric distributions
(typically Gaussians)  and uses a criterion to maximize the separation [2  6  16  17]. One general
form of these criteria is the F-divergence (also known as Csiszer’s general measure of divergence) 
that can be deﬁned with respect to a convex function φ(t) for t ∈ R [13]:

√

dx.

p1(x) φ

F (p1(x)  p2(x)) =

t and the KL-divergence −(cid:82) p1(x) log

The examples of using this divergence include the Bhattacharyya divergence (cid:82)(cid:112)p1(x)p2(x)dx

dx when φ(t) = − log(t). Using
when φ(t) =
mutual information between data and labels can be understood as an extension of KL-divergence.
The well known Linear Discriminant Analysis is a special example of Bhattacharyya criterion when
we assume two-class Gaussians sharing the same covariance matrices.
Unlike dimensionality reduction  we cannot use these criteria for metric learning because any F-
divergence is metric-invariant. The asymptotic error Eq. (2) is related to one particular F-divergence

(12)

p1(x)

(cid:90)

(cid:19)
(cid:18) p2(x)
(cid:17)
(cid:16) p2(x)

p1(x)

5

Figure 2: Optimal local metrics are shown on the left for three example Gaussian distributions in
a 5-dimensional space. The projected 2-dimensional distributions are represented as ellipses (one
standard deviation from the mean)  while the remaining 3 dimensions have an isotropic distribution.
The local ˜p/p of the three classes are plotted on the right using a Euclidean metric I and for the
optimal metric Aopt. The solution Aopt tries to keep the ratio ˜p/p over the different classes as
similar as possible when the distance dN is varied.

by EAsymp = 1 − F (p1  p2) with a convex function φ(t) = 1/(1 + t). Therefore  in generative
models  the metric learning problem is qualitatively different from the dimensionality reduction
problem in this aspect. One interpretation is that the F-measure can be understood as a measure
of dimensionality reduction in an asymptotic situation. In this case  the role of metric learning can
be deﬁned to move the expected F-measure toward the asymptotic F-measure by appropriate metric
adaptation.
Finally  we provide an alternative understanding on the problem of reducing Eq. (7). By reformulat-
ing Eq. (9) into (p2 − p1)(p2∇2p1 − p1∇2p2)  we can see that the optimal metric tries to minimize
the difference between ∇2p1

  this also implies

and ∇2p2

. If ∇2p1

p1

p2

p2

≈ ∇2p2
≈ ˜p2
p2

p1
˜p1
p1

(13)

N

2D∇2p  the average probability at a distance dN in (4). Thus  the algorithm tries to keep
for ˜p = p+ d2
the ratio of the average probabilities ˜p1/ ˜p2 at a distance dN to be as similar as possible to the ratio of
probabilities p1/p2 at the test point. This means that the expected nearest neighbor classiﬁcation at
a distance dN will be least biased due to ﬁnite sampling. Fig. 2 shows how the learned local metric
Aopt varies at a point x for a 3-class Gaussian example  and how the ratio of ˜p/p is kept as similar
as possible.

6 Experiments

We apply our algorithm for learning a local metric to synthetic and various real datasets and see
how well it improves nearest neighbor classiﬁcation performance. Simple standard Gaussian dis-
tributions are used to learn the generative model  with parameters including the mean vector µ and
covariance matrix Σ for each class. The Hessian of a Gaussian distribution is then given by the
expression:

(cid:104)

Σ−1(x − µ)(x − µ)T Σ−1 − Σ−1(cid:105)

∇∇p(x) = p(x)

(14)

This expression is then used to learn the optimal local metric. We compare the performance of
our method (GLML—Generative Local Metric Learning) with recent metric learning discrimina-
tive methods which report state-of-the-art performance on a number of datasets. These include

6

Information-Theoretic Metric Learning (ITML)1 [3]  Boost Metric2 (BM) [21]  and Largest Margin
Nearest Neighbor (LMNN)3 [26]. We used the implementations downloaded from the correspond-
ing authors’ websites. We also compare with a local metric given by the Fisher kernel [12] assuming
a single Gaussian for the generative model and using the location parameter to derive the Fisher in-
formation matrix. The metric from the Fisher kernel was not originally intended for nearest neighbor
classiﬁcation  but it is the only other reported algorithm that learns a local metric from generative
models.
For the synthetic dataset  we generated data from two-class random Gaussian distributions having
two ﬁxed means. The covariance matrices are generated from random orthogonal eigenvectors and
random eigenvalues. Experiments were performed varying the input dimensionality  and the classi-
ﬁcation accuracies are shown in Fig. 3.(a) along with the results of the other algorithms. We used
500 test points and an equal number of training examples. The experiments were performed with
20 different realizations and the results were averaged. As the dimensionality grows  the original
nearest neighbor performance degrades because of the high dimensionality. However  we see that
the proposed local metric highly outperforms the discriminative nearest neighbor performance in a
high dimensional space appropriately. We note that this example is ideal for GLML  and it shows
much improvement compared to the other methods.
The other experiments consist of the following benchmark datasets: UCI machine learning reposi-
tory4 datasets (Ionosphere  Wine)  and the IDA benchmark repository5 (German  Image  Waveform 
Twonorm). We also used the USPS handwritten digits and the TI46 speech dataset. For the USPS
data  we resized the images to 8 × 8 pixels and trained on the 64-dimensional pixel vector data. For
the TI46 dataset  the examples consist of spoken sounds pronounced by 8 different men and 8 dif-
ferent women. We chose the pronunciation of ten digits (“zero” to “nine”)  and performed a 10 class
digit classiﬁcation task. 10 different ﬁlters in the Fourier domain were used as features to preprocess
the acoustic data. The experiments were done on 20 data sampling realizations for Twonorm and
TI46  10 for USPS  200 for Wine  and 100 for the others.
Except the synthetic data in Fig. 3.(a)  the data consist of various number of training data per class.
The regularization parameter γ value is chosen by cross-regularization on a subset of the training
data  then ﬁxed for testing. The covariance matrix of the learned Gaussian distributions is also
regularized by setting Σ = ˆΣ + αI where ˆΣ is the estimated covariance. The parameter α is set
prior to each experiment.
From the results shown in Fig. 3  our local metric algorithm generally outperforms most of the other
metrics across most of the datasets. On quite a number of datasets  many of the other methods
do not outperform the original Euclidean nearest neighbor classiﬁer. This is because on some of
these datasets  performance cannot be improved using a global metric. On the other hand  the local
metric derived from simple Gaussian distributions always shows a performance gain over the naive
nearest neighbor classiﬁer.
In contrast  using Bayes rule with these simple Gaussian generative
models often results in very poor performance. The computational time using a local metric is also
very competitive  since the underlying SDP optimization has a simple spectral solution. This is in
contrast to other methods which numerically solve for a global metric using an SDP over the data
points.

7 Conclusions

In our study  we showed how a local metric for nearest neighbor classiﬁcation can be learned using
generative models. Our experiments show improvement over competitive methods on a number
of experimental datasets. The learning algorithm is derived from an analysis of the asymptotic
performance of the nearest neighbor classiﬁer  such that the optimal metric minimizes the bias of the
expected performance of the classiﬁer. This connection to generative models is very powerful  and
can easily be extended to include missing data—one of the large advantages of generative models

1http://userweb.cs.utexas.edu/ pjain/itml/
2http://code.google.com/p/boosting/
3http://www.cse.wustl.edu/ kilian/Downloads/LMNN.html
4http://archive.ics.uci.edu/ml/
5http://www.fml.tuebingen.mpg.de/Members/raetsch/benchmark

7

(a) Synthetic

(b) Ionosphere

(c) German

(d) Image

(e) Waveform

(f) Twonorm

(g) Wine

(h) USPS 8×8

(i) TI46

Figure 3: (a) Gaussian synthetic data with different dimensionality. As number of dimensions gets
large  most methods degrade except GLML and LMNN. GLML continues to improve vastly over
other methods. (b)∼(h) are the experiments on benchmark datasets varying the number of training
data per class. (i) TI46 is the speech dataset pronounced by 8 men and 8 women. The Fisher kernel
and BM are omitted for (f)∼(i) and (h)∼(i) respectively  since their performances are much worse
than the naive nearest neighbor classiﬁer.

in machine learning. Here we used simple Gaussians for the generative models  but this could be
also easily extended to include other possibilities such as mixture models  hidden Markov models 
or other dynamic generative models.
The kernelization of this work is straightforward  and the extension to the k-nearest neighbor setting
using the theoretical distribution of k-th nearest neighbors is an interesting future direction. Another
possible future avenue of work is to combine dimensionality reduction and metric learning using
this framework.

Acknowledgments

This research was supported by National Research Foundation of Korea (2010-0017734  2010-0018950  314-
2008-1-D00377) and by the MARS (KI002138) and BK-IT Projects.

References

[1] B. Alipanahi  M. Biggs  and A. Ghodsi. Distance metric learning vs. Fisher discriminant analysis. In

Proceedings of the 23rd national conference on Artiﬁcial intelligence  pages 598–603  2008.

8

520501000.60.70.80.91# DimPerformanceNNGLMLITMLBMLMNNFisher1030501000.60.70.80.9# tr. dataPerformance1001502002500.620.630.640.650.66# tr. dataPerformance3005007009000.80.850.90.951# tr. dataPerformance200500100015000.780.80.820.840.86# tr. dataPerformance5001000200030000.90.920.940.96# tr. dataPerformance102030400.70.80.91# tr. dataPerformance10030050010000.90.951# tr. dataPerformance1001802703500.550.60.650.70.75# tr. dataPerformance[2] K. Das and Z. Nenadic. Approximate information discriminant analysis: A computationally simple het-

eroscedastic feature extraction technique. Pattern Recognition  41(5):1548–1557  2008.

[3] J.V. Davis  B. Kulis  P. Jain  S. Sra  and I.S. Dhillon. Information-theoretic metric learning. In Proceedings

of the 24th International Conference on Machine Learning  pages 209–216  2007.

[4] R.O. Duda  P.E. Hart  and D.G. Stork. Pattern Classiﬁcation (2nd Edition). Wiley-Interscience  2000.
[5] A. Frome  Y. Singer  and J. Malik. Image retrieval and classiﬁcation using local distance functions. In

Advances in Neural Information Processing Systems 18  pages 417–424  2006.

[6] K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press  San Diego  CA  1990.
[7] K. Fukunaga and T.E. Flick. The optimal distance measure for nearest neighbour classiﬁcation. IEEE

Transactions on Information Theory  27(5):622–627  1981.

[8] K. Fukunaga and T.E. Flick. An optimal global nearest neighbour measure. IEEE Transactions on Pattern

Analysis and Machine Intelligence  6(3):314–318  1984.

[9] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural Information

Processing Systems 18  pages 451–458. 2006.

[10] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood components analysis. In

Advances in Neural Information Processing Systems 17  pages 513–520. 2005.

[11] M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. Inverardi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics 
17(3):277–297  2005.

[12] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In Advances in

Neural Information Processing Systems 11  pages 487–493  1998.

[13] J.N. Kapur. Measures of Information and Their applications. John Wiley & Sons  New York  NY  1994.
[14] S. Lacoste-Julien  F. Sha  and M. Jordan. DiscLDA: Discriminative learning for dimensionality reduction

and classiﬁcation. In Advances in Neural Information Processing Systems 21  pages 897–904. 2009.

[15] J.A. Lasserre  C.M. Bishop  and T.P. Minka. Principled hybrids of generative and discriminative mod-
In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern

els.
Recognition  pages 87–94  2006.

[16] M. Loog and R.P.W. Duin. Linear dimensionality reduction via a heteroscedastic extension of LDA:
The chernoff criterion. IEEE Transactions on Pattern Analysis and Machine Intelligence  26(6):732–739 
2004.

[17] Z. Nenadic. Information discriminant analysis: Feature extraction with an information-theoretic objective.

IEEE Transactions on Pattern Analysis and Machine Intelligence  29(8):1394–1407  2007.

[18] A.Y. Ng and M.I. Jordan. On discriminative vs. generative classiﬁers: A comparison of logistic regression

and naive Bayes. In Advances in Neural Information Processing Systems 14  pages 841–848  2001.

[19] F. Perez-Cruz. Estimation of information theoretic measures for continuous random variables. In Ad-

vances in Neural Information Processing Systems 21  pages 1257–1264. 2009.

[20] R. Raina  Y. Shen  A.Y. Ng  and A. McCallum. Classiﬁcation with hybrid generative/discriminative

models. In Advances in Neural Information Processing Systems 16  pages 545–552. 2004.

[21] C. Shen  J. Kim  L. Wang  and A. van den Hengel. Positive semideﬁnite metric learning with boosting.

In Advances in Neural Information Processing Systems 22  pages 1651–1659. 2009.

[22] N. Singh-Miller and M. Collins. Learning label embeddings for nearest-neighbor multi-class classiﬁcation
with an application to speech recognition. In Advances in Neural Information Processing Systems 22 
pages 1678–1686. 2009.

[23] D. Tran and A. Sorokin. Human activity recognition with metric learning. In Proceedings of the 10th

European Conference on Computer Vision  pages 548–561  2008.

[24] Q. Wang  S. R. Kulkarni  and S. Verd´u. A nearest-neighbor approach to estimating divergence between
continuous random vectors. In Proceedings of IEEE International Symposium on Information Theory 
pages 242–246  2006.

[25] Q. Wang  S. R. Kulkarni  and S. Verd´u. Divergence estimation for multidimensional densities via k-

nearest-neighbor distances. IEEE Transactions on Information Theory  55(5):2392–2405  2009.

[26] K. Weinberger  J. Blitzer  and L. Saul. Distance metric learning for large margin nearest neighbor classi-

ﬁcation. In Advances in Neural Information Processing Systems 18  pages 1473–1480. 2006.

9

,Srini Turaga
Lars Buesing
Adam Packer
Henry Dalgleish
Noah Pettit
Michael Hausser
Jakob Macke
Shaojie Bai
J. Zico Kolter
Vladlen Koltun