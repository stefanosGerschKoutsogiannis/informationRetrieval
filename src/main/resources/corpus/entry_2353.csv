2017,Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra,Tandem mass spectrometry (MS/MS) is a high-throughput technology used to identify the proteins in a complex biological sample  such as a drop of blood. A collection of spectra is generated at the output of the process  each spectrum of which is representative of a peptide (protein subsequence) present in the original complex sample. In this work  we leverage the log-likelihood gradients of generative models to improve the identification of such spectra. In particular  we show that the gradient of a recently proposed dynamic Bayesian network (DBN) may be naturally employed by a kernel-based discriminative classifier. The resulting Fisher kernel substantially improves upon recent attempts to combine generative and discriminative models for post-processing analysis  outperforming all other methods on the evaluated datasets. We extend the improved accuracy offered by the Fisher kernel framework to other search algorithms by introducing Theseus  a DBN representating a large number of widely used MS/MS scoring functions. Furthermore  with gradient ascent and max-product inference at hand  we use Theseus to learn model parameters without any supervision.,Gradients of Generative Models for Improved

Discriminative Analysis of Tandem Mass Spectra

John T. Halloran

Department of Public Health Sciences

University of California  Davis
jthalloran@ucdavis.edu

David M. Rocke

Department of Public Health Sciences

University of California  Davis
dmrocke@ucdavis.edu

Abstract

Tandem mass spectrometry (MS/MS) is a high-throughput technology used to
identify the proteins in a complex biological sample  such as a drop of blood. A
collection of spectra is generated at the output of the process  each spectrum of
which is representative of a peptide (protein subsequence) present in the original
complex sample. In this work  we leverage the log-likelihood gradients of genera-
tive models to improve the identiﬁcation of such spectra. In particular  we show
that the gradient of a recently proposed dynamic Bayesian network (DBN) [7] may
be naturally employed by a kernel-based discriminative classiﬁer. The resulting
Fisher kernel substantially improves upon recent attempts to combine generative
and discriminative models for post-processing analysis  outperforming all other
methods on the evaluated datasets. We extend the improved accuracy offered by
the Fisher kernel framework to other search algorithms by introducing Theseus 
a DBN representing a large number of widely used MS/MS scoring functions.
Furthermore  with gradient ascent and max-product inference at hand  we use
Theseus to learn model parameters without any supervision.

1

Introduction

In the past two decades  tandem mass spectrometry (MS/MS) has become an indispensable tool
for identifying the proteins present in a complex biological sample. At the output of a typical
MS/MS experiment  a collection of spectra is produced on the order of tens-to-hundreds of thousands 
each of which is representative of a protein subsequence  called a peptide  present in the original
complex sample. The main challenge in MS/MS is accurately identifying the peptides responsible for
generating each output spectrum.
The most accurate identiﬁcation methods search a database of peptides to ﬁrst score peptides  then
rank and return the top-ranking such peptide. The pair consisting of a scored candidate peptide and
observed spectrum is typically referred to as a peptide-spectrum match (PSM). However  PSM scores
returned by such database-search methods are often difﬁcult to compare across different spectra (i.e. 
they are poorly calibrated)  limiting the number of spectra identiﬁed per search [15]. To combat such
poor calibration  post-processors are typically used to recalibrate PSM scores [13  19  20].
Recent work has attempted to exploit generative scoring functions for use with discriminative
classiﬁers to better recalibrate PSM scores; by parsing a DBN’s Viterbi path (i.e.  the most probable
sequence of random variables)  heuristically derived features were shown to improve discriminative
recalibration using support vector machines (SVMs). Rather than relying on heuristics  we look
towards the more principled approach of a Fisher kernel [11]. Fisher kernels allow one to exploit the
sequential-modeling strengths of generative models such as DBNs  which offer vast design ﬂexibility
for representing data sequences of varying length  for use with discriminative classiﬁers such as
SVMs  which offer superior accuracy but often require feature vectors of ﬁxed length. Although

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Example tandem mass spectrum with precursor charge c(s) = 2 and generating peptide x =
LWEPLLDVLVQTK. Plotted in red and blue are  respectively  b- and y-ion peaks (discussed in Section 2.1.1) 
while spurious observed peaks (called insertions) are colored gray. Note y1  b1  b4  and b12 are absent fragment
ions (called deletions).

the number of variables in a DBN may vary given different observed sequences  a Fisher kernel
utilizes the ﬁxed-length gradient of the log-likelihood (i.e.  the Fisher score) in the feature-space of
a kernel-based classiﬁer. Deriving the Fisher scores of a DBN for Rapid Identiﬁcation of Peptides
(DRIP) [7]  we show that the DRIP Fisher kernel greatly improves upon the previous heuristic
approach; at a strict FDR of 1% for the presented datasets  the heuristically derived DRIP features
improve accuracy over the base feature set by an average 6.1%  while the DRIP Fisher kernel raises
this average improvement to 11.7% (Table 2 in [9])  thus nearly doubling the total accuracy of DRIP
post-processing.
Motivated by improvements offered by the DRIP Fisher kernel  we look to extend this to other models
by deﬁning a generative model representative of the large class of existing scoring functions [2 
5  6  16  10  22  17]. In particular  we deﬁne a DBN (called Theseus1) which  given an observed
spectrum  evaluates the universe of all possible PSM scores. In this work  we use Theseus to model
PSM score distributions with respect to the widely used XCorr scoring function [5]. The resulting
Fisher kernel once again improves discriminative post-processing accuracy. Furthermore  with
the generative model in place  we explore inferring parameters of the modeled scoring function
using max-product inference and gradient-based learning. The resulting coordinate ascent learning
algorithm outperforms standard maximum-likelihood learning. Most importantly  this overall learning
algorithm is unsupervised which  to the authors’ knowledge  is the ﬁrst MS/MS scoring function
parameter estimation procedure not to rely on any supervision. We note that this overall training
procedure may be adapted by the many MS/MS search algorithms whose scoring functions lie in the
class modeled by Theseus.
The paper is organized as follows. We discuss background information in Section 2  including the
process by which MS/MS spectra are produced  the means by which spectra are identiﬁed  and related
previous work. In Section 3  we extensively discuss the log-likelihood of the DRIP model and derive
its Fisher scores. In Section 4  we introduce Theseus and derive gradients of its log-likelihood. We
then discuss gradient-based unsupervised learning of Theseus parameters and present an efﬁcient 
monotonically convergent coordinate ascent algorithm. Finally  in Section 5  we show that DRIP
and Theseus Fisher kernels substantially improve spectrum identiﬁcation accuracy and that Theseus’
coordinate ascent learning algorithm provides accurate unsupervised parameter estimation. For
convenience  a table of the notation used in this paper may be found in [9].

2 Background

A typical tandem mass spectrometry experiment begins by cleaving proteins into peptides using a
digesting enzyme. The resulting peptides are then separated via liquid chromatography and subjected
to two rounds of mass spectrometry. The ﬁrst round measures the mass and charge of the intact
peptide  called the precursor mass and precursor charge  respectively. Peptides are then fragmented
and the fragments undergo a second round of mass spectrometry  the output of which is an observed
spectrum indicative of the fragmented peptide. The x-axis of this observed spectrum denotes mass-
to-charge (m/z)  measured in thomsons (Th)  and the y-axis is a unitless intensity measure  roughly
proportional to the abundance of a single fragment ion with a given m/z value. A sample such
observed spectrum is illustrated in Figure 1.

1In honor of Shannon’s magnetic mouse  which could learn to traverse a small maze.

2

200300400500600700800900100011001200130014001500m/z0.00.20.40.60.81.0intensityb2b3b5b6b7b8b9b10b11y2y3y4y5y6y7y8y9y10y11y12insertionsb-ionsy-ions2.1 MS/MS Database Search

(cid:110)

Let s be an observed spectrum with precursor mass m(s) and precursor charge c(s). In order to
identify s  we search a database of peptides  as follows. Let P be the set of all possible peptide
sequences. Each peptide x ∈ P is a string x = x1x2 . . . xn comprised of characters  called amino
acids. Given a peptide database D ⊆ P  we wish to ﬁnd the peptide x ∈ D responsible for generating
s. Using the precursor mass and charge  the set of peptides to be scored is constrained by setting
a mass tolerance threshold  w  such that we score the set of candidate peptides D(s D  w) =
x : x ∈ D 
  where m(x) denotes the mass of peptide x. Note that we’ve
overloaded m(·) to return either a peptide’s or observed spectrum’s precursor mass; we similarly
overload c(·). Given s and denoting an arbitrary scoring function as ψ(x  s)  the output of a search
algorithm is thus x∗ = argmaxx∈D(m(s) c(s) D w) ψ(x  s)  the top-scoring PSM.

(cid:12)(cid:12)(cid:12) ≤ w

(cid:12)(cid:12)(cid:12) m(x)

c(s) − m(s)

(cid:111)

2.1.1 Theoretical Spectra

In order to score a candidate peptide x  fragment ions corresponding to sufﬁx masses (called b-ions)
and preﬁx masses (called y-ions) are collected into a theoretical spectrum. The annotated b- and
y-ions of the generating peptide for an observed spectrum are illustrated in Figure 1. Varying based
on the value of c(s)  the kth respective b- and y-ion pair of x are

(cid:80)n

i=n−k m(xi) + 18 + cy

 

cy

(cid:80)k

i=1 m(xi) + cb

cb

b(x  cb  k) =

 

y(x  cy  k) =

where cb is the charge of the b-ion and cy is the charge of the y-ion. For c(s) = 1  we have
cb = cy = 1  since these are the only possible  detectable fragment ions. For higher observed charge
states c(s) ≥ 2  it is unlikely for a single fragment ion to consume the entire charge  so that we have
cb + cy = c(s)  where cb  cy ∈ [1  c(s)− 1]. The b-ion offset corresponds to the mass of a cb charged
hydrogen atom  while the y-ion offset corresponds to the mass of a water molecule plus a cy charged
hydrogen atom.
Further fragment ions may occur  each corresponding to the loss of a molecular group off a b- or
y-ion. Called neutral losses  these correspond to a loss of either water  ammonia  or carbon monoxide.
These fragment ions are commonly collected into a vector v  whose elements are weighted based on
their corresponding fragment ion. For instance  XCorr [5] assigns all b- and y-ions a weight of 50
and all neutral losses a weight of 10.

2.2 Previous Work

Many scoring functions have been proposed for use in search algorithms. They range from simple
dot-product scoring functions (X!Tandem [2]  Morpheus [22])  to cross-correlation based scoring
functions (XCorr [5])  to exact p-values over linear scoring functions calculated using dynamic
programming (MS-GF+ [16] and XCorr p-values [10]). The recently introduced DRIP [7] scores
candidate peptides without quantization of m/z measurements and allows learning the expected
locations of theoretical peaks given high quality  labeled training data. In order to avoid quantization of
the m/z axis  DRIP employs a dynamic alignment strategy wherein two types of prevalent phenomena
are explicitly modeled: spurious observed peaks  called insertions  and absent theoretical peaks 
called deletions (examples of both are displayed in Figure 1). DRIP then uses max-product inference
to calculate the most probable sequences of insertions and deletions to score candidate peptides  and
was shown to achieve state-of-the-art performance on a variety of datasets.
In practice  scoring functions are often poorly calibrated (i.e.  PSM scores from different spectra are
difﬁcult to compare to one another)  leading to potentially identiﬁed spectra left on the table during
statistical analysis. In order to properly recalibrate such PSM scores  several semi-supervised post-
processing methods have been proposed [13  19  20]. The most popular such method is Percolator [13] 
which  given the output target and decoy PSMs (discussed in Section 5) of a scoring algorithm and
features detailing each PSM  utilizes an SVM to learn a discriminative classiﬁer between target PSMs
and decoy PSMs. PSM scores are then recalibrated using the learned decision boundary.
Recent work has attempted to leverage the generative nature of the DRIP model for discriminative
use by Percolator [8]. As earlier mentioned  the output of DRIP is the most probable sequence of
insertions and deletions  i.e.  the Viterbi path. However  DRIP’s observations are the sequences of

3

observed spectrum m/z and intensity values  so that the lengths of PSM’s Viterbi paths vary depending
on the number of observed spectrum peaks. In order to exploit DRIP’s output in the feature-space of
a discriminative classiﬁer  PSM Viterbi paths were heuristically mapped to a ﬁxed-length vector of
features. The resulting heuristic features were shown to dramatically improve Percolator’s ability to
discriminate between PSMs.

2.3 Fisher Kernels

with a set of parameters θ and likelihood p(O|θ) = (cid:80)

Using generative models to extract features for discriminative classiﬁers has been used to great
effect in many problem domains by using Fisher kernels [11  12  4]. Assuming a generative model
H p(O  H|θ)  where O is a sequence of
observations and H is the set of hidden variables  the Fisher score is then Uo = ∇θ log p(O|θ).
Given observations Oi and Oj of differing length (and  thus  different underlying graphs in the case
of dynamic graphical models)  a kernel-based classiﬁer over these instances is trained using UOi
and UOj in the feature-space. Thus  a similarity measure is learned in the gradient space  under the
intuition that objects which induce similar likelihoods will induce similar gradients.

3 DRIP Fisher Scores

Figure 2: Graph of DRIP  the frames (i.e.  time instances) of which correspond to observed spectrum peaks.
Shaded nodes represent observed variables and unshaded nodes represent hidden variables. Given an observed
spectrum  the middle frame (the chunk) dynamically expands to represent the second observed peak to the
penultimate observed peak.

  Oin

1 )  (Omz

2

1

  Oin

T   Oin

2 )  . . .   (Omz

We ﬁrst deﬁne  in detail  DRIP’s log-likelihood  followed by the Fisher score derivation for DRIP’s
learned parameters. For discussion of the DRIP model outside the score of this work  readers are
directed to [7  8]. Denoting an observed peak as a pair (Omz  Oin) consisting of an m/z measurement
and intensity measurement  respectively  let s = (Omz
T ) be an
MS/MS spectrum of T peaks and x be a candidate (which  given s  we’d like to score). We denote
the theoretical spectrum of x  consisting of its unique b- and y-ions sorted in ascending order  as the
length-d vector v. The graph of DRIP is displayed in Figure 2  where variables which control the
traversal of the theoretical spectrum are highlighted in blue and variables which control the scoring
of observed peak measurements are highlighted in red. Groups of variables are collected into time
instances called frames. The frames of DRIP correspond to the observed peak m/z and intensity
observations  so that there are T frames in the model.
Unless otherwise speciﬁed  let t be an arbitrary frame 1 ≤ t ≤ T . δt is a multinomial random variable
which dictates the number of theoretical peaks traversed in a frame. The random variable Kt  which
denotes the index of the current theoretical peak index  is a deterministic function of its parents  such
that p(Kt = Kt−1 + δt|Kt−1  δt) = 1. Thus  δt > 1 corresponds to the deletion of δt − 1 theoretical
peaks. The parents of δt ensure that DRIP does not attempt to increment past the last theoretical
peak  i.e.  p(δt > d − Kt−1|d  Kt−1  it−1) = 0. Subsequently  the theoretical peak value v(Kt) is
used to access a Gaussian from a collection (the mean of each Gaussian corresponds to a position
along the m/z axis  learned using the EM algorithm [3]) with which to score observations. Hence 
the state-space of the model is all possible traversals  from left to right  of the theoretical spectrum 
accounting for all possible deletions.
When scoring observed peak measurements  the Bernoulli random variable it denotes whether a
peak is scored using learned Gaussians (when it = 0) or considered an insertion and scored using an

4

Theoretical SpectrumVariablesObservedSpectrumVariablest

|v(Kt)  it =
insertion penalty (when it = 1). When scoring m/z observations  we thus have p(Omz
|v(Kt)  it = 1) = amz  where µmz is a vector of
0) = f (Omz
Gaussian means and σ2 the m/z Gaussian variance. Similarly  when scoring intensity observations 
t |it = 1) = ain  where µin and ¯σ2 are the
we have p(Oin
intensity Gaussian mean and variance  respectively. Let i0 = K0 = ∅ and 1{·} denote the indicator
function. Denoting DRIP’s Gaussian parameters as θ  the likelihood is thus

t |µin  ¯σ2) and p(Oin

t

p(s|x  θ) =

|Kt)p(Oin
t )

t

|µmz(v(Kt))  σ2) and p(Omz
t |it = 0) = f (Oin
T(cid:89)
T(cid:89)
T(cid:89)

φ(δt  Kt−1  it  it−1).

t=1

t=1

=

=

p(δt|Kt−1  d  it−1)1{Kt=Kt−1+δt}p(Omz
1(cid:88)

p(δt|Kt−1  d  it−1)1{Kt=Kt−1+δt}(

t

t=1

p(it)p(Omz

t

|Kt  it))(

it=0

it=0

1(cid:88)

p(it)p(Oin

t |it))

The only stochastic variables in the model are it and δt  where all other random variables are either
observed or deterministic given the sequences i1:T and δ1:T . Thus  we may equivalently write
p(s|x  θ) = p(i1:T   δ1:T|θ). The Fisher score of the kth m/z mean is thus
log p(s|x  θ) =
p(s|x θ)

p(s|x  θ)  and we have (please see [9] for the full derivation)

∂µmz(k)

∂

∂

1

∂µmz(k)
p(s|x  θ) =

∂

∂µmz(k)

(cid:88)

∂µmz(k)

∂

(cid:88)

=

i1:T  δ1:T

1{Kt=k}p(s|x  θ)

p(i1:T   δ1:T|θ) =

(cid:32) (cid:89)

t:Kt=k

i1:T  δ1:T :Kt=k 1≤t≤T

1
p(Omz

t

|Kt)

∂

∂µmz(k)

(cid:89)

t:Kt=k

∂

∂µmz(k)

p(i1:T   δ1:T|θ)

(cid:33)

p(Omz

t

|Kt)

.

(cid:88)
(cid:33)(cid:32)

i1:T  δ1:T

⇒ ∂

∂µmz(k)

log p(s|x  θ) =

T(cid:88)

t=1

p(it  Kt = k|s  θ)p(it = 0|Kt  Omz

t

)

(Omz

t − µmz(k))

σ2

.

(1)

Note that the posterior in Equation 1  and thus the Fisher score  may be efﬁciently computed using
sum-product inference. Through similar steps  we have

∂

∂σ2(k)

log p(s|x  θ) =

∂

∂µin log p(s|x  θ) =
∂ ¯σ2 log p(s|x  θ) =

∂

p(it  Kt = k|s  θ)p(it = 0|Kt  Omz

t

)(

(Omz

t − µmz(k))

2σ4

− 1

2σ2 )

p(it  Kt|s  θ)p(it = 0|Oin
t )

p(it  Kt|s  θ)p(it = 0|Oin
t )(

(Oin

t − µin)
¯σ2
t − µin)
(Oin
2¯σ4

− 1

2¯σ2 ) 

(2)

(3)

(4)

t

(cid:88)
(cid:88)
(cid:88)

t

t

where σ2(k) denotes the partial derivative of the variance for the kth m/z Gaussian with mean
µmz(k).
Let Uµ = ∇µmz log p(s  x|θ) and Uσ2 = ∇σ2(k) log p(s  x|θ). Uµ and Uσ2 are length-d vectors
corresponding to the mapping of a peptide’s sequence of b- and y-ions into r-dimensional space
(i.e.  dimension equal to an m/z-discretized observed spectrum). Let 1 be the length-r vector of
ones. Deﬁning zmz  zi ∈ Rr  the elements of which are the quantized observed spectrum m/z and
intensity values  respectively  we use the following DRIP gradient-based features for SVM training in
Section 5: |Uµ|1  |Uσ2|1  U T

∂ ¯σ2 log p(s  x|θ).

∂µin log p(s  x|θ)  and ∂

µ zmz  U T

σ2zi  U T

1  U T

σ2 1 

µ

∂

4 Theseus

Given an observed spectrum s  we focus on representing the universe of linear PSM scores using a
DBN. Let z denote the vector resulting from preprocessing the observed spectrum  s. As a modeling
example  we look to represent the popular XCorr scoring function. Using subscript τ to denote a

5

vector whose elements are shifted τ units  XCorr’s scoring function is deﬁned as

XCorr(s  x) = vT z − 75(cid:88)

vT zτ = vT (z − 75(cid:88)

zτ ) = vT z

(cid:48)

 

τ =−75

τ =−75

where z(cid:48) = z−(cid:80)75
function. The ith element of zθ is zθ(i) =(cid:80)l
(cid:80)l
i=1 zθ(m(xi) + 1) =(cid:80)n
have uT zθ =(cid:80)n
j=1 θ(j)zj(m(xi) + 1) = vT z(cid:48) = XCorr(s  x).

τ =−75 zτ . Let θ ∈ Rl be a vector of XCorr weights for the various types of possible
fragment ions (described in Section 2.1.1). As described in [10]  given c(s)  we reparameterize z(cid:48)
into a vector zθ such that XCorr(x  s) is rendered as a dot-product between zθ and a boolean vector
u in the reparameterized space. This reparameterization readily applies to any linear MS/MS scoring
j=1 θ(j)zj(i)  where zj is a vector whose element zj(i)
is the sum of all higher charged fragment ions added into the singly-charged fragment ions for the jth
fragment ion type. The nonzero elements of u correspond to the singly-charged b-ions of x and we

i=1

Figure 3: Graph of Theseus. Shaded nodes are observed random variables and unshaded nodes are hidden (i.e. 
stochastic). The model is unrolled for n + 1 frames  including B0 in frame zero. Plate notation denotes M
repetitions of the model  where M is the number of discrete precursor masses allowed by the precursor-mass
tolerance threshold  w.

Our generative model is illustrated in Figure 3. n is the maximum possible peptide length and m is
one of M discrete precursor masses (dictated by the precursor-mass tolerance threshold  w  and m(s)).
A hypothesis is an instantiation of random variables across all frames in the model  i.e.  for the set of
all possible sequences of Xi random variables  X1:n = X1  X2  . . .   Xn  a hypothesis is x1:n ∈ X1:n.
In our case  each hypothesis corresponds to a peptide and the corresponding log-likelihood its XCorr
score. Each frame after the ﬁrst contains an amino acid random variable so that we accumulate b-ions
in successive frames and access the score contribution for each such ion.
For frame i  Xi is a random amino acid and Bi the accumulated mass up to the current frame.
B0 and Bn are observed to zero and m  respectively  enforcing the boundary conditions that all
length-n PSMs considered begin with mass zero and end at a particular precursor mass. For i > 0 
Bi is a deterministic function of its parents  p(Bi|Bi−1  Xi) = p(Bi = Bi−1 + m(Xi)) = 1.
Thus  hypotheses which do not respect these mass constraints receive probability zero  i.e.  p(Bn (cid:54)=
m|Bn−1  Xn) = 0. m is observed to the value of the current precursor mass being considered.
Let A be the set of amino acids  where |A| = 20. Given Bi and m  the conditional distribution of Xi
changes such that p(Xi ∈ A|Bi−1 < m) = αU{A}  p(Xi = κ|Bi−1 ≥ m) = 1  where U{·} is the
uniform distribution over the input set and κ /∈ A  m(κ) = 0. Thus  when the accumulated mass is
less than m  Xi is a random amino acid and  otherwise  Xi deterministically takes on a value with
zero mass. To recreate XCorr scores  α = 1/|A|  though  in general  any desired mass function may
be used for p(Xi ∈ A|Bi−1 < m).
Si is a virtual evidence child [18]  i.e.  a leaf node whose conditional distribution need not be
normalized to compute probabilistic quantities of interest in the DBN. For our model  we have
i=1 θizi(Bi)) and p(Si|Bi ≥ m  θ) = 1. Let t(cid:48) denote

p(Si|Bi < m  θ) = exp(zθ(Bi)) = exp((cid:80)|θ|

6

the ﬁrst frame in which m(X1:n) ≥ m. The log-likelihood is then log p(s  X1:n|θ)

= log p(X1:n  B0:n  S1:n−1)

= log(1{B0=0}(

p(Xi|m  Bi−1)p(Bi = Bi−1 + m(Xi))p(Si|m  Bi  θ))1{Bn−1+m(Xn)=m})

n−1(cid:89)

i=1

= log 1{B0=0 ∧m(X1:n)=m} + log(

p(Xi|m  Bi−1)p(Bi = Bi−1 + m(Xi))p(Si|m  Bi  θ))+

t(cid:48)(cid:89)

i=1

log(

p(Xi|m  Bi−1)p(Bi = Bi−1 + m(Xi))p(Si|m  Bi  θ))

= log 1{m(X1:n)=m} + log 1 + log(

exp(zθ(Bi)))

n(cid:89)

i=t(cid:48)+1

t(cid:48)(cid:89)

i=1

log

(cid:88)
(cid:88)

x1:n

t(cid:48)(cid:88)

i=1

= log 1{m(X1:n)=m} +

zθ(Bi) = log 1{B0=0 ∧ m(X1:n)=m} + XCorr(X1:t(cid:48)   s)

The ith element of Theseus’ Fisher score is thus

∂

∂θ(i)

log p(s|θ) =

∂

∂θ(i)

p(s  x1:n|θ) =

1

p(s|θ)

∂

∂θ(i)

p(s  x1:n|θ)

(cid:88)
t(cid:48)(cid:89)

x1:n

t(cid:48)(cid:88)

=

1

p(s|θ)

1{b0=0 ∧ m(x1:n)=m}(

zi(bj))

exp(zθ(bj)) 

(5)

x1:n

j=1

j=1

While Equation 5 is generally difﬁcult to compute  we calculate it efﬁciently using sum-product infer-
∂θ(i) log p(s  ˆx|θ) =
ence. Note that when the peptide sequence is observed  i.e.  X1:n = ˆx  we have

∂

(cid:80)

j z(m(ˆx1:j)).

Using the model’s Fisher scores  Theseus’ parameters θ may be learned via maximum likelihood
estimation. Given a dataset of spectra s1  s2  . . .   sn  we present an alternate learning algorithm
(Algorithm 1) which converges monotonically to a local optimum (proven in [9]). Within each
iteration  Algorithm 1 uses max-product inference to efﬁciently infer the most probable PSMs per
iteration  mitigating the need for training labels. θ is maximized in each iteration using gradient
ascent.

Algorithm 1 Theseus Unsupervised Learning Algorithm
1: while not converged do
for i = 1  . . .   n do
2:
3:
4:
5:
6: end while

(cid:80)n
ˆxi ← argmaxxi∈P log p(si  xi|θ)
i=1 log p(si  ˆxi|θ)

end for
θ ← argmaxθ

5 Results

Measuring peptide identiﬁcation performance is complicated by the fact that ground-truth is unavail-
able for real-world data. Thus  in practice  it is most common to estimate the false discovery rate
(FDR) [1] by searching a decoy database of peptides which are unlikely to occur in nature  typically
generated by shufﬂing entries in the target database [14]. For a particular score threshold  t  FDR
is then calculated as the proportion of decoys scoring better than t to the number of targets scoring
better than t. Once the target and decoy PSMs are calculated  a curve displaying the FDR threshold
vs. the number of correctly identiﬁed targets at each given threshold may be calculated. In place of
FDR along the x-axis  we use the q-value [14]  deﬁned to be the minimum FDR threshold at which a
given score is deemed to be signiﬁcant. As many applications require a search algorithm perform
well at low thresholds  we only plot q ∈ [0  0.1].
The same datasets and search settings used to evaluate DRIP’s heuristically derived features in [8]
are adapted in this work. MS-GF+ (one of the most accurate search algorithms in wide use  plotted

7

for reference) was run using version 9980  with PSMs ranked by E-value and Percolator features
calculated using msgf2pin. All database searches were run using a ±3.0Th mass tolerance  XCorr
ﬂanking peaks not allowed in Crux searches  and all search algorithm settings otherwise left to their
defaults. Peptides were derived from the protein databases using trypsin cleavage rules without
suppression of proline and a single ﬁxed carbamidomethyl modiﬁcation was included.
Gradient-based feature representations derived from DRIP and XCorr were used to train an SVM
classiﬁer [13] and recalibrate PSM scores. Theseus training and computation of XCorr Fisher
scores were performed using a customized version of Crux v2.1.17060 [17]. For an XCorr PSM  a
feature representation is derived directly using both ∇θ log p(s|θ) and ∇θ log p(s  x|θ) as deﬁned in
Section 4  representing gradient information for both the distribution of PSM scores and the individual
PSM score  respectively. DRIP gradient-based features  as deﬁned in Section 3  were derived using
a customized version of the DRIP Toolkit [8].Figure 4 displays the resulting search accuracy for
four worm and yeast datasets. For the uncalibrated search results in Figure 5  we show that XCorr
parameters may be learned without supervision using Theseus  and that the presented coordinate
descent algorithm (which estimates the most probable PSMs to take a step in the objective space)
converges to a much better local optimum than maximum likelihood estimation.

(b) Worm-1

(c) Worm-2

(d) Worm-3

(e) Worm-4

(f) Yeast-1

(g) Yeast-2

(h) Yeast-3

(i) Yeast-4

Figure 4: Search accuracy plots measured by q-value versus number of spectra identiﬁed for worm (C. elegans)
and yeast (Saccharomyces cerevisiae) datasets. All methods are post-processed using the Percolator SVM
classiﬁer [13]. “DRIP” augments the standard set of DRIP features with DRIP-Viterbi-path parsed PSM features
(described in [8]) and “DRIP Fisher” augments the heuristic set with gradient-based DRIP features. “XCorr ”
“XCorr p-value ” and “MS-GF+” use their standard sets of Percolator features (described in [8])  while “XCorr
p-value Fisher” and “XCorr Fisher” augment the standard XCorr feature sets with gradient-based Theseus
features.

8

0.020.040.060.080.10q-value051015Spectraidentiﬁed(1000’s)DRIPFisherDRIPXCorrp-valueFisherXCorrp-valueXCorrFisherXCorrMS-GF+0.020.040.060.080.10q-value57101215Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value46810Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value2468Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value34567Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value68101214Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value68101214Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value68101214Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value68101214Spectraidentiﬁed(1000’s)Figure 5: Search accuracy of Theseus’ learned scoring function parameters. Coordinate ascent parameters are
learned using Algorithm 1 and MLE parameters are learned using gradient ascent.

(b) Yeast-1

(c) Yeast-2

5.1 Discussion

DRIP gradient-based post-processing improves upon the heuristically derived features in all cases 
and does so substantially on a majority of datasets. In the case of the yeast datasets  this distinguishes
DRIP post-processing performance from all competitors and leads to state-of-the-art identiﬁcation
accuracy. Furthermore  we note that both XCorr and XCorr p-value post-processing performance are
greatly improved using the gradient-based features derived using Theseus  raising performance above
the highly similar MS-GF+ in several cases. Particularly noteworthy is the substantial improvement
in XCorr accuracy which  using gradient-based information  is nearly competitive with its p-value
counterpart. Considering the respective runtimes of the underlying search algorithms  this thus
presents a tradeoff for a researcher considering search time and accuracy. In practice  the DRIP and
XCorr p-value computations are at least an order of magnitude slower than XCorr computation in
Crux [21]. Thus  the presented work not only improves state-of-the-art accuracy  but also improves
the accuracy of simpler  yet signiﬁcantly faster  search algorithms.
Owing to max-product inference in graphical models  we also show that Theseus may be used to
effectively learn XCorr model parameters (Figure 5) without supervision. Furthermore  we show that
XCorr p-values are also made more accurate by training the underlying scoring function for which
p-values are computed. This marks a novel step towards unsupervised training of uncalibrated scoring
functions  as unsupervised learning has been extensively explored for post-processor recalibration 
but has remained an open problem for MS/MS database-search scoring functions. The presented
learning framework  as well as the presented XCorr gradient-based feature representation  may be
adapted by many of the widely scoring functions represented by Theseus [2  5  6  16  10  22  17].
Many exciting avenues are open for future work. Leveraging the large breadth of graphical models
research  we plan to explore other learning paradigms using Theseus (for instance  estimating other
PSMs using k-best Viterbi in order to discriminatively learn parameters using algorithms such as
max-margin learning). Perhaps most exciting  we plan to further investigate the peptide-to-observed-
spectrum mapping derived from DRIP Fisher scores. Under this mapping  we plan to explore learning
distance metrics between PSMs in order to identify proteins from peptides.
Acknowledgments: This work was supported by the National Center for Advancing Translational
Sciences (NCATS)  National Institutes of Health  through grant UL1 TR001860.

References
[1] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical and powerful

approach to multiple testing. Journal of the Royal Statistical Society B  57:289–300  1995.

[2] R. Craig and R. C. Beavis. Tandem: matching proteins with tandem mass spectra. Bioinformat-

ics  20:1466–1467  2004.

[3] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological)  39:1–22 
1977.

[4] Charles Elkan. Deriving tf-idf as a ﬁsher kernel.

In International Symposium on String

Processing and Information Retrieval  pages 295–300. Springer  2005.

9

0.020.040.060.080.10q-value0510Spectraidentiﬁed(1000’s)XCorrp-valueCoordinateAscentXCorrp-valueMLEXCorrp-valueXCorrCoordinateAscentXCorrMLEXCorr0.020.040.060.080.10q-value4681012Spectraidentiﬁed(1000’s)0.020.040.060.080.10q-value571012Spectraidentiﬁed(1000’s)[5] J. K. Eng  A. L. McCormack  and J. R. Yates  III. An approach to correlate tandem mass spectral
data of peptides with amino acid sequences in a protein database. Journal of the American
Society for Mass Spectrometry  5:976–989  1994.

[6] Jimmy K Eng  Tahmina A Jahan  and Michael R Hoopmann. Comet: An open-source ms/ms

sequence database search tool. Proteomics  13(1):22–24  2013.

[7] John T. Halloran  Jeff A. Bilmes  and William S. Noble. Learning peptide-spectrum alignment
models for tandem mass spectrometry. In Uncertainty in Artiﬁcial Intelligence (UAI)  Quebec
City  Quebec Canada  July 2014. AUAI.

[8] John T Halloran  Jeff A Bilmes  and William S Noble. Dynamic bayesian network for accurate
detection of peptides from tandem mass spectra. Journal of Proteome Research  15(8):2749–
2759  2016.

[9] John T. Halloran and David M. Rocke. Gradients of Generative Models for Improved Discrimi-

native Analysis of Tandem Mass Spectra: Supplementary Materials  2017.

[10] J Jeffry Howbert and William S Noble. Computing exact p-values for a cross-correlation
shotgun proteomics score function. Molecular & Cellular Proteomics  pages mcp–O113  2014.

[11] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In

Advances in Neural Information Processing Systems  Cambridge  MA  1998. MIT Press.

[12] Tommi S Jaakkola  Mark Diekhans  and David Haussler. Using the ﬁsher kernel method to

detect remote protein homologies. In ISMB  volume 99  pages 149–158  1999.

[13] L. K¨all  J. Canterbury  J. Weston  W. S. Noble  and M. J. MacCoss. A semi-supervised machine
learning technique for peptide identiﬁcation from shotgun proteomics datasets. Nature Methods 
4:923–25  2007.

[14] Uri Keich  Attila Kertesz-Farkas  and William Stafford Noble. Improved false discovery rate
estimation procedure for shotgun proteomics. Journal of proteome research  14(8):3148–3161 
2015.

[15] Uri Keich and William Stafford Noble. On the importance of well-calibrated scores for
identifying shotgun proteomics spectra. Journal of proteome research  14(2):1147–1160  2014.

[16] Sangtae Kim and Pavel A Pevzner. Ms-gf+ makes progress towards a universal database search

tool for proteomics. Nature communications  5  2014.

[17] Sean McIlwain  Kaipo Tamura  Attila Kertesz-Farkas  Charles E Grant  Benjamin Diament 
Barbara Frewen  J Jeffry Howbert  Michael R Hoopmann  Lukas K¨all  Jimmy K Eng  et al.
Crux: rapid open source protein tandem mass spectrometry analysis. Journal of proteome
research  2014.

[18] J. Pearl. Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference.

Morgan Kaufmann  1988.

[19] M. Spivak  J. Weston  L. Bottou  L. K¨all  and W. S. Noble. Improvements to the Percolator
algorithm for peptide identiﬁcation from shotgun proteomics data sets. Journal of Proteome
Research  8(7):3737–3745  2009. PMC2710313.

[20] M. Spivak  J. Weston  D. Tomazela  M. J. MacCoss  and W. S. Noble. Direct maximization
of protein identiﬁcations from tandem mass spectra. Molecular and Cellular Proteomics 
11(2):M111.012161  2012. PMC3277760.

[21] Shengjie Wang  John T Halloran  Jeff A Bilmes  and William S Noble. Faster and more
accurate graphical model identiﬁcation of tandem mass spectra using trellises. Bioinformatics 
32(12):i322–i331  2016.

[22] C. D. Wenger and J. J. Coon. A proteomics search algorithm speciﬁcally designed for high-

resolution tandem mass spectra. Journal of proteome research  2013.

10

,John Halloran
David Rocke