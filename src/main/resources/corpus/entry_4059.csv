2007,Online Linear Regression and Its Application to Model-Based Reinforcement Learning,We provide a provably efficient algorithm for learning Markov Decision Processes (MDPs) with continuous state and action spaces in the online setting. Specifically  we take a model-based approach and show that a special type of online linear regression allows us to learn MDPs with (possibly kernalized) linearly parameterized dynamics. This result builds on Kearns and Singh's work that provides a provably efficient algorithm for finite state MDPs. Our approach is not restricted to the linear setting  and is applicable to other classes of continuous MDPs.,Online Linear Regression and Its Application to

Model-Based Reinforcement Learning

Alexander L. Strehl∗

Yahoo! Research
New York  NY

strehl@yahoo-inc.com

Michael L. Littman

Department of Computer Science

Rutgers University
Piscataway  NJ USA

mlittman@cs.rutgers.edu

Abstract

We provide a provably efﬁcient algorithm for learning Markov Decision Processes
(MDPs) with continuous state and action spaces in the online setting. Speciﬁcally 
we take a model-based approach and show that a special type of online linear
regression allows us to learn MDPs with (possibly kernalized) linearly parame-
terized dynamics. This result builds on Kearns and Singh’s work that provides a
provably efﬁcient algorithm for ﬁnite state MDPs. Our approach is not restricted
to the linear setting  and is applicable to other classes of continuous MDPs.

Introduction

Current reinforcement-learning (RL) techniques hold great promise for creating a general type of
artiﬁcial intelligence (AI)  speciﬁcally autonomous (software) agents that learn difﬁcult tasks with
limited feedback (Sutton & Barto  1998). Applied RL has been very successful  producing world-
class computer backgammon players (Tesauro  1994) and model helicopter ﬂyers (Ng et al.  2003).
Many applications of RL  including the two above  utilize supervised-learning techniques for the
purpose of generalization. Such techniques enable an agent to act intelligently in new situations by
learning from past experience in different but similar situations.

Provably efﬁcient RL for ﬁnite state and action spaces is accomplished by Kearns and Singh (2002)
and hugely contributes to our understanding of the relationship between exploration and sequential
decision making. The achievement of the current paper is to provide an efﬁcient RL algorithm that
learns in Markov Decision Processes (MDPs) with continuous state and action spaces. We prove that
it learns linearly-parameterized MDPs  a model introduced by Abbeel and Ng (2005)  with sample
(or experience) complexity that grows only polynomially with the number of state space dimensions.

Our new RL algorithm utilizes a special linear regresser  based on least-squares regression  whose
analysis may be of interest to the online learning and statistics communities. Although our primary
result is for linearly-parameterized MDPs  our technique is applicable to other classes of continuous
MDPs and our framework is developed speciﬁcally with such future applications in mind. The lin-
ear dynamics case should be viewed as only an interesting example of our approach  which makes
substantial progress in the goal of understanding the relationship between exploration and general-
ization in RL.

An outline of the paper follows. In Section 1  we discuss online linear regression and pose a new
online learning framework that requires an algorithm to not only provide predictions for new data
points but also provide formal guarantees about its predictions. We also develop a speciﬁc algorithm
and prove that it solves the problem. In Section 2  using the algorithm and result from the ﬁrst
section  we develop a provably efﬁcient RL algorithm. Finally  we conclude with future work.

∗Some of the work presented here was conducted while the author was at Rutgers University.

1

1 Online Linear Regression

Linear Regression (LR) is a well-known and tremendously powerful technique for prediction of
the value of a variable (called the response or output) given the value of another variable (called
the explanatory or input). Suppose we are given some data consisting of input-output pairs:
(x1  y1)  (x2  y2)  . . .   (xm  ym)  where xi ∈ Rn and yi ∈ R for i = 1  . . .   m. Further  suppose
that the data satisﬁes a linear relationship  that is yi ≈ θT xi ∀i ∈ {1  . . .   m}  where θ ∈ Rn is an
n-dimensional parameter vector. When a new input x arrives  we would like to make a prediction
of the corresponding output by estimating θ from our data. A standard approach is to approximate
θ with the least-squares estimator ˆθ deﬁned by ˆθ = (X T X)−1X T y  where X ∈ Rm×n is a matrix
whose ith row consists of the ith input xT
i and y ∈ Rn is a vector whose ith component is the ith
output yi.
Although there are many analyses of the linear regression problem  none is quite right for an appli-
cation to model-based reinforcement learning (MBRL). In particular  in MBRL  we cannot assume
that X is ﬁxed ahead of time and we require more than just a prediction of θ but knowledge about
whether this prediction is sufﬁciently accurate. A robust learning agent must not only infer an ap-
proximate model of its environment but also maintain an idea about the accuracy of the parameters
of this model. Without such meta-knowledge  it would be difﬁcult to determine when to explore (or
when to trust the model) and how to explore (to improve the model). We coined the term KWIK
(“know what it knows”) for algorithms that have this special property. With this idea in mind  we
present the following online learning problem related to linear regression. Let ||v|| denote the Eu-
clidean norm of a vector v and let Var [X] denote the variance of a random variable X.

Deﬁnition 1 (KWIK Linear Regression Problem or KLRP) On every timestep t = 1  2  . . . an
input vector xt ∈ Rnsatisf ying||xt|| ≤ 1 and output number yt ∈ [−1  1] is provided. The input
xt may be chosen in any way that depends on the previous inputs and outputs (x1  y1)  . . .   (xt  yt).
The output yt is chosen probabilistically from a distribution that depends only on xt and satisﬁes
E[yt] = θT xt and Var[yt] ≤ σ2  where θ ∈ Rn is an unknown parameter vector satisfying ||θ|| ≤ 1
and σ ∈ R is a known constant. After observing xt and before observing yt  the learning algorithm
must produce an output ˆyt ∈ [−1  1] ∪ {∅} (a prediction of E[yt|xt]). Furthermore  it should be
able to provide an output ˆy(x) for any input vector x ∈ {0  1}n.
A key aspect of our problem that distinguishes it from other online learning models is that the
algorithm is allowed to output a special value ∅ rather than make a valid prediction (an output other
than ∅). An output of ∅ signiﬁes that the algorithm is not sure of what to predict and therefore
declines to make a prediction. The algorithm would like to minimize the number of times it predicts
∅  and  furthermore  when it does make a valid prediction the prediction must be accurate  with
high probability. Next  we formalize the above intuition and deﬁne the properties of a “solution” to
KLRP.

Deﬁnition 2 We deﬁne an admissible algorithm for the KWIK Linear Regression Problem to
be one that takes two inputs 0 ≤  ≤ 1 and 0 ≤ δ < 1 and  with probability at least 1 − δ  satisﬁes
the following conditions:

1. Whenever the algorithm predicts ˆyt(x) ∈ [−1  1]  we have that |ˆyt(x) − θT x| ≤ .
2. The number of timesteps t for which ˆyt(xt) = ∅ is bounded by some function ζ(  δ  n) 

polynomial in n  1/ and 1/δ  called the sample complexity of the algorithm.

1.1 Solution

First  we present an algorithm and then a proof that it solves KLRP. Let X denote an m × n matrix
whose rows we interpret as transposed input vectors. We let X(i) denote the transpose of the ith
row of X. Since X T X is symmetric  we can write it as

X T X = U ΛU T  

(1)
where U = [v1  . . .   vn] ∈ Rn×n  with v1  . . .   vn being a set of orthonormal eigenvectors of X T X.
Let the corresponding eigenvalues be λ1 ≥ λ2 ≥ ··· ≥ λk ≥ 1 > λk+1 ≥ ··· ≥ λn ≥ 0. Note that
Λ = diag(λ1  . . .   λn) is diagonal but not necessarily invertible. Now  deﬁne ¯U = [v1  . . .   vk] ∈

(Singular Value Decomposition)

2

Rn×k and ¯Λ = diag(λ1  . . .   λk) ∈ Rk×k. For a ﬁxed input xt (a new input provided to the
algorithm at time t)  deﬁne
(2)
(3)

¯q := X ¯U ¯Λ−1 ¯U T xt ∈ Rm×n 

¯v = [0  . . .   0  vT

k+1xt  . . .   vT

n xt]T ∈ Rn.

Algorithm 1 KWIK Linear Regression
0: Inputs: α1  α2
1: Initialize X = [ ] and y = [ ].
2: for t = 1  2  3 ··· do
3:
4:
5:
6:

Let xt denote the input at time t.
Compute ¯q and ¯v using Equations 2 and 3.
if ||¯q|| ≤ α1 and ||¯v|| ≤ α2 then
Choose ˆθ ∈ Rn that minimizes Pi [y(i) − ¯θT X(i)]2 subject to ||¯θ|| ≤ 1  where X(i) is

the transpose of the ith row of X and y(i) is the ith component of y.
Output valid prediction xT ˆθ.

else

7:
8:
9:
10:
11:
12:
end if
13:
14: end for

Output ∅.
Receive output yt.
Append xT
Append yt as a new element to the vector y.

t as a new row to the matrix X.

Our algorithm for solving the KWIK Linear Regression Problem uses these quantities and is pro-
vided in pseudocode by Algorithm 1. Our ﬁrst main result of the paper is the following theorem.

Theorem 1 With appropriate parameter settings  Algorithm 1 is an admissible algorithm for the
KWIK Linear Regression Problem with a sample complexity bound of ˜O(n3/4).

Although the analysis of Algorithm 1 is somewhat complicated  the algorithm itself has a simple
interpretation. Given a new input xt  the algorithm considers making a prediction of the output yt
using the norm-constrained least-squares estimator (speciﬁcally  ˆθ deﬁned in line 6 of Algorithm1).
The norms of the vectors ¯q and ¯v provide a quantitative measure of uncertainty about this estimate.
When both norms are small  the estimate is trusted and a valid prediction is made. When either norm
is large  the estimate is not trusted and the algorithm produces an output of ∅.
One may wonder why ¯q and ¯v provide a measure of uncertainty for the least-squares estimate.
Consider the case when all eigenvalues of X T X are greater than 1. In this case  note that x =
X T X(X T X)−1x = X T ¯q. Thus  x can be written as a linear combination of the rows of X  whose
coefﬁcients make up ¯q  of previously experienced input vectors. As shown by Auer (2002)  this
Intuitively 
particular linear combination minimizes ||q|| for any linear combination x = X T q.
if the norm of ¯q is small  then there are many previous training samples (actually  combinations
of inputs) “similar” to x  and hence our least-squares estimate is likely to be accurate for x. For
the case of ill-conditioned X T X (when X T X has eigenvalues close to 0)  X(X T X)−1x may be
undeﬁned or have a large norm. In this case  we must consider the directions corresponding to small
eigenvalues separately and this consideration is dealt with by ¯v.

1.2 Analysis

We provide a sketch of the analysis of Algorithm 1. Please see our technical report for full details.
The analysis hinges on two key lemmas that we now present.

In the following lemma  we analyze the behavior of the squared error of predictions based on an
incorrect estimator ˆθ 6= θ verses the squared error of using the true parameter vector θ. Speciﬁcally 
we show that the squared error of the former is very likely to be larger than the latter when the pre-
dictions based on ˆθ (of the form ˆθT x for input x) are highly inaccurate. The proof uses Hoeffding’s
bound and is omitted.

3

Lemma 1 Let θ ∈ Rn and ˆθ ∈ Rn be two ﬁxed parameter vectors satisfying ||θ|| ≤ 1 and ||ˆθ|| ≤
1. Suppose that (x1  y1)  . . .   (xm  ym) is any sequence of samples satisfying xi ∈ Rn  yi ∈ R 
||xi|| ≤ 1  yi ∈ [−1  1]  E[yi|xi] = θT xi  and Var[yi|xi] ≤ σ2. For any 0 < δ0 < 1 and ﬁxed
positive constant z  if

m

then

Xi=1

[(θ − ˆθ)T xi]2 ≥ 2p8m ln(2/δ) + z 

m

m

(4)

(5)

Xi=1

(yi − ˆθT xi)2 >

(yi − θT xi)2 + z

Xi=1
with probability at least 1 − 2δ0.
The following lemma  whose proof is fairly straight-forward and therefore omitted  relates the error
of an estimate ˆθT x for a ﬁxed input x based on an inaccurate estimator ˆθ to the quantities ||¯q|| 
||¯v||  and ∆E(ˆθ) := qPm
i=1 [(θ − ˆθ)T X(i)]2. Recall that when ||¯q|| and ||¯v|| are both small  our
algorithm becomes conﬁdent of the least-squares estimate. In precisely this case  the lemma shows
that |(θ − ˆθ)T x| is bounded by a quantity proportional to ∆E(ˆθ).
Lemma 2 Let θ ∈ Rn and ˆθ ∈ Rn be two ﬁxed parameter vectors satisfying ||θ|| ≤ 1 and ||ˆθ|| ≤
1. Suppose that (x1  y1)  . . .   (xm  ym) is any sequence of samples satisfying xi ∈ Rn  yi ∈ R 
||xi|| ≤ 1  yi ∈ [−1  1]. Let x ∈ Rn be any vector. Let ¯q and ¯v be deﬁned as above. Let ∆E(ˆθ)
denote the error term qPm

i=1 [(θ − ˆθ)T xi]2. We have that

|(θ − ˆθ)T x| ≤ ||¯q||∆E(ˆθ) + 2||¯v||.

(6)

Proof sketch: (of Theorem 1)

The proof has three steps. The ﬁrst is to bound the sample complexity of the algorithm (the number
of times the algorithm makes a prediction of ∅)  in terms of the input parameters α1 and α2. The
second is to choose the parameters α1 and α2. The third is to show that  with high probability  every
valid prediction made by the algorithm is accurate.
Step 1
We derive an upper bound ¯m on the number of timesteps for which either ||¯q|| > α1 holds or
||¯v|| > α2 holds. Observing that the algorithm trains on only those samples experienced during
pricisely these timesteps and applying Lemma 13 from the paper by Auer (2002) we have that

2

+

(7)

α2
1

n
α2

  and α2 = /4.

n√ln(1/(δ)) ln(n)

¯m = O(cid:18) n ln(n/α1)

2(cid:19) .
Step 2 We choose α1 = C·Q ln Q  where C is a constant and Q =
Step 3 Consider some ﬁxed timestep t during the execution of Algorithm 1 such that the algorithm
makes a valid prediction (not ∅). Let ˆθ denote the solution of the norm-constrained least-squares
minimization (line 6 in the pseudocode). By deﬁnition  since ∅ was not predicted  we have that
¯q ≤ α1 and ¯v ≤ α2. We would like to show that |ˆθT x− θT x| ≤  so that Condition 1 of Deﬁnition 2
is satisﬁed. Suppose not  namely that |(ˆθ − θ)T x| > . Using Lemma 2  we can lower bound the
quantity ∆E(ˆθ)2 = Pm
i=1[(θ − ˆθ)T X(i)]2  where m denotes the number of rows of the matrix X
(equivalently  the number of samples obtained used by the algorithm for training  which we upper-
bounded by ¯m)  and X(i) denotes the transpose of the ith row of X. Finally  we would like to
apply Lemma 1 to prove that  with high probability  the squared error of ˆθ will be larger than the
squared error of predictions based on the true parameter vector θ  which contradicts the fact that ˆθ
was chosen to minimize the term Pm
i=1(yi − ˆθT X(i))2. One problem with this approach is that
Lemma 1 applies to a ﬁxed ˆθ and the least-squares computation of Algorithm 1 may choose any ˆθ in
the inﬁnite set {ˆθ ∈ Rn such that ||ˆθ|| ≤ 1}. Therefore  we use a uniform discretization to form a

4

ﬁnite cover of [−1  1]n and apply the theorem to the member of the cover closest to ˆθ. To guarantee
that the total failure probability of the algorithm is at most δ  we apply the union bound over all
(ﬁnitely many) applications of Lemma 1. 2

1.3 Notes

In our formulation of KLRP we assumed an upper bound of 1 on the the two-norm of the inputs xi 
outputs yi  and the true parameter vector θ. By appropriate scaling of the inputs and/or outputs  we
could instead allow a larger (but still ﬁnite) bound.

Our analysis of Algorithm 1 showed that it is possible to solve KLRP with polynomial sample com-
plexity (where the sample complexity is deﬁned as the number of timesteps t that the algorithm
outputs ∅ for the current input xt)  with high probability. We note that the algorithm also has poly-
nomial computational complexity per timestep  given the tractability of solving norm-constrained
least-squares problems (see Chapter 12 of the book by Golub and Van Loan (1996)).

1.4 Related Work

Work on linear regression is abundant in the statistics community (Seber & Lee  2003). The use
of the quantities ¯v and ¯q to quantify the level of certainty of the linear estimator was introduced by
Auer (2002). Our analysis differs from that by Auer (2002) because we do not assume that the input
vectors xi are ﬁxed ahead of time  but rather that they may be chosen in an adversarial manner. This
property is especially important for the application of regression techniques to the full RL problem 
rather than the Associative RL problem considered by Auer (2002). Our analysis has a similar ﬂavor
to some  but not all  parts of the analysis by Abbeel and Ng (2005). However  a crucial difference
of our framework and analysis is the use of output ∅ to signify uncertainty in the current estimate 
which allows for efﬁcient exploration in the application to RL as described in the next section.

2 Application to Reinforcement Learning

The general reinforcement-learning (RL) problem is how to enable an agent (computer program 
robot  etc.) to maximize an external reward signal by acting in an unknown environment. To ensure
a well-deﬁned problem  we make assumptions about the types of possible worlds. To make the
problem tractable  we settle for near-optimal (rather than optimal) behavior on all but a polynomial
number of timesteps  as well as a small allowable failure probability. This type of performance
metric was introduced by Kakade (2003)  in the vein of recent RL analyses (Kearns & Singh  2002;
Brafman & Tennenholtz  2002).

In this section  we formalize a speciﬁc RL problem where the environment is mathematically mod-
eled by a continuous MDP taken from a rich class of MDPs. We present an algorithm and prove
that it learns efﬁciently within this class. The algorithm is “model-based” in the sense that it con-
structs an explicit MDP that it uses to reason about future actions in the true  but unknown  MDP
environment. The algorithm uses  as a subroutine  any admissible algorithm for the KWIK Linear
Regression Problem introduced in Section 1. Although our main result is for a speciﬁc class of con-
tinuous MDPs  albeit an interesting and previously studied one  our technique is more general and
should be applicable to many other classes of MDPs as described in the conclusion.

2.1 Problem Formulation

The model we use is slightly modiﬁed from the model described by Abbeel and Ng (2005). The
main difference is that we consider discounted rather than undiscounted MDPs and we don’t require
the agent to have a “reset” action that takes it to a speciﬁed start state (or distribution). Let PS denote
the set of all (measurable) probability distributions over the set S. The environment is described by
a discounted MDP M = hS  A  T  R  γi  where S = RnS is the state space  A = RnA is the action
space  T : S × A → PS is the unknown transition dynamics  γ ∈ [0  1) is the discount factor  and
R : S × A → R is the known reward function.1 For each timestep t  let xt ∈ S denote the current
1All of our results can easily be extended to the case of an unknown reward function with a suitable linearity

assumption.

5

state and ut ∈ A the current action. The transition dynamics T satisfy

xt+1 = M φ(xt  ut) + wt 

(8)
where xt+1 ∈ S  φ(· ·) : RnS +nA → Rn is a (basis or kernel) function satisfying ||φ(· ·)|| ≤ 1 
and M is an nS × n matrix. We assume that the 2-norm of each row of M is bounded by 1.2 Each
component of the noise term wt ∈ RnS is chosen i.i.d. from a normal distribution with mean 0
and variance σ2 for a known constant σ. If an MDP satisﬁes the above conditions we say that it
is linearly parameterized  because the next-state xt+1 is a linear function of the vector φ(xt  ut)
(which describes the current state and action) plus a noise term.
We assume that the learner (also called the agent) receives nS  nA  n  R  φ(· ·)  σ  and γ as input 
with T initially being unknown. The learning problem is deﬁned as follows. The agent always
occupies a single state s of the MDP M . The agent is given s and chooses an action a. It then
receives an immediate reward r ∼ R(s  a) and is transported to a next state s0 ∼ T (s  a). This
procedure then repeats forever. The ﬁrst state occupied by the agent may be chosen arbitrarily.

M (s) (Qπ

A policy is any strategy for choosing actions. We assume (unless noted otherwise) that rewards all lie
in the interval [0  1]. For any policy π  let V π
M (s  a)) denote the discounted  inﬁnite-horizon
value (action-value) function for π in M (which may be omitted from the notation) from state s.
Speciﬁcally  let st and rt be the tth encountered state and received reward  respectively  resulting
from execution of policy π in some MDP M from state s0. Then  V π
M (s) = E[P∞j=0 γjrj|s0 = s].
The optimal policy is denoted π∗ and has value functions V ∗M (s) and Q∗M (s  a). Note that a policy
cannot have a value greater than vmax := 1/(1 − γ) by the assumption of a maximum reward of 1.
2.2 Algorithm

First  we discuss how to use an admissible learning algorithm for KLRP to construct an MDP model.
We proceed by specifying the transition model for each of the (inﬁnitely many) state-action pairs.
Given a ﬁxed state-action pair (s  a)  we need to estimate the next-state distribution of the MDP from
past experience  which consists of input state-action pairs (transformed by the nonlinear function φ)
and output next states. For each state component i ∈ {1  . . .   nS}  we have a separate learning
problem that can be solved by any instance Ai of an admissible KLRP algorithm.3 If each instance
makes a valid prediction (not ∅)  then we simply construct an approximate next-state distribution
whose ith component is normally distributed with variance σ2 and whose mean is given by the
prediction of Ai (this procedure is equivalent to constructing an approximate transition matrix ˆM
whose ith row is equal to the transpose of the approximate parameter vector ˆθ learned by Ai).
If any instance of our KLRP algorithm predicts ∅ for state-action pair (s  a)  then we cannot estimate
the next-state distribution. Instead  we make s highly rewarding in the MDP model to encourage
exploration  as done in the R-MAX algorithm (Brafman & Tennenholtz  2002). Following the ter-
minology introduced by Kearns and Singh (2002)  we call such a state (state-action) an “unknown”
state (state-action) and we ensure that the value function of our model assigns vmax (maximum pos-
sible) to state s. The standard way to satisfy this condition for ﬁnite MDPs is to make the transition
function for action a from state s a self-loop with reward 1 (yielding a value of vmax = 1/(1− γ) for
state s). We can affect the exact same result in a continuous MDP by adding a component to each
state vector s and to each vector φ(s  a) for every state-action pair (s  a). If (s  a) is “unknown” we
set the value of the additional components (of φ(s  a) and s) to 1  otherwise we set it to 0. We add an
additional row and column to M that preserves this extra component (during the transformation from
φ(s  a) to the next state s0) and otherwise doesn’t change the next-state distribution. Finally  we give
a reward of 1 to any unknown state  leaving rewards for the known states unchanged. Pseudocode
for the resulting KWIK-RMAX algorithm is provided in Algorithm 2.

Theorem 2 For any  and δ  the KWIK-RMAX algorithm executes an -optimal policy on at most
a polynomial (in n  nS  1/  1/δ  and 1/(1 − γ)) number of steps  with probability at least 1 − δ.
2The algorithm can be modiﬁed to deal with bounds (on the norms of the rows of M) that are larger than

one.

3One minor technical detail is that our KLRP setting requires bounded outputs (see Deﬁnition 1) while our
application to MBRL requires dealing with normal  and hence unbounded outputs. This is easily dealt with by
ignoring any extremely large (or small) outputs and showing that the resulting norm of the truncated normal
distribution learned by the each instance Ai is very close to the norm of the untruncated distribution.

6

Algorithm 2 KWIK-RMAX Algorithm
0: Inputs: nS  nA  n  R  φ(· ·)  σ  γ    δ  and admissible learning algorithm ModelLearn.
1: for all state components i ∈ {1  . . .   nS} do
2:

Initialize a new instantiation of ModelLearn  denoted Ai  with inputs C (1−γ)2
and δ/nS 
2√n
for inputs  and δ  respectively  in Deﬁnition 2  and where C is some constant determined by
the analysis.

γ and transition function speciﬁed by Ai for i ∈ {1  . . .   nS} as described above.

3: end for
4: Initialize an MDP Model with state space S  action space A  reward function R  discount factor
5: for t = 1  2  3 ··· do
6:
7:
8:
9:
10:
11:
12:
13: end for

Let s denote the state at time t.
Choose action a := ˆπ∗(s) where ˆπ∗ is the optimal policy of the MDP Model.
Let s0 be the next state after executing action a.
for all factors i ∈ {1  . . .   n} do
end for
Update MDP Model.

Present input-output pair (φ(s  a)  s0(i)) to Ai a.

2.3 Analysis

Proof sketch: (of Theorem 2)
It can be shown that  with high probability  policy ˆπ∗ is either an -optimal policy (V ˆπ∗
(s) ≥
V ∗(s) − ) or it is very likely to lead to an unknown state. However  the number of times the latter
event can occur is bounded by the maximum number of times the instances Ai can predict ∅  which
is polynomial in the relevant parameters. 2

2.4 The Planning Assumption

We have shown that the KWIK-RMAX Algorithm acts near-optimally on all but a small (poly-
nomial) number of timesteps  with high probability. Unfortunately  to do so  the algorithm must
solve its internal MDP model completely and exactly. It is easy to extend the analysis to allow -
approximate solution. However  it is not clear whether even this approximate computation can be
done efﬁciently. In any case  discretization of the state space can be used  which yields computa-
tional complexity that is exponential in the number of (state and action) dimensions of the problem 
similar to the work of Chow and Tsitsiklis (1991). Alternatively  sparse sampling can be used  whose
complexity has no dependence on the size of the state space but depends exponentially on the time
horizon (≈ 1/(1 − γ)) (Kearns et al.  1999). Practically  there are many promising techniques that
make use of value-function approximation for fast and efﬁcient solution (planning) of MDPs (Sutton
& Barto  1998). Nevertheless  it remains future work to fully analyze the complexity of planning.

2.5 Related Work

The general exploration problem in continuous state spaces was considered by Kakade et al. (2003) 
and at a high level our approach to exploration is similar in spirit. However  a direct application
of Kakade et al.’s (2003) algorithm to linearly-parameterized MDPs results in an algorithm whose
sample complexity scales exponentially  rather than polynomially  with the state-space dimension.
That is because the analysis uses a factor of the size of the “cover” of the metric space. Reinforce-
ment learning in continuous MDPs with linear dynamics was studied by Fiechter (1997). However 
an exact linear relationship between the current state and next state is required for this analysis to
go through  while we allow the current state to be transformed (for instance  adding non-linear state
features) through non-linear function φ. Furthermore  Fiechter’s algorithm relied on the existence
of a “reset” action and a speciﬁc form of reward function. These assumptions admit a solution
that follows a ﬁxed policy and doesn’t depend on the actual history of the agent or the underlying
MDP. The model that we consider  linearly parameterized MDPs  is taken directly from the work by
Abbeel and Ng (2005)  where it was justiﬁed in part by an application to robotic helicopter ﬂight. In

7

that work  a provably efﬁcient algorithm was developed in the apprenticeship RL setting. In this set-
ting  the algorithm is given limited access (polynomial number of calls) to a ﬁxed policy (called the
teacher’s policy). With high probably  a policy is learned that is nearly as good as the teacher’s pol-
icy. Although this framework is interesting and perhaps more useful for certain applications (such as
helicopter ﬂying)  it requires a priori expert knowledge (to construct the teacher) and alleviates the
problem of exploration altogether. In addition  Abbeel and Ng’s (2005) algorithm also relies heavily
on a reset assumption  while ours does not.

Conclusion

We have provided a provably efﬁcient RL algorithm that learns a very rich and important class of
MDPs with continuous state and action spaces. Yet  many real-world MDPs do not satisfy the lin-
earity assumption  a concern we now address. Our RL algorithm utilized a speciﬁc online linear
regression algorithm. We have identiﬁed certain interesting and general properties (see Deﬁnition 2)
of this particular algorithm that support online exploration. These properties are meaningful without
the linearity assumption and should be useful for development of new algorithms for different mod-
eling assumptions. Our real goal of the paper is to work towards developing a general technique for
applying regression algorithms (as black boxes) to model-based reinforcement-learning algorithms
in a robust and formally justiﬁed way. We believe the approach used with linear regression can be
repeated for other important classes  but we leave the details as interesting future work.

Acknowledgements

We thank NSF and DARPA IPTO for support.

References

Abbeel  P.  & Ng  A. Y. (2005). Exploration and apprenticeship learning in reinforcement learning. ICML ’05:
Proceedings of the 22nd international conference on Machine learning (pp. 1–8). New York  NY  USA:
ACM Press.

Auer  P. (2002). Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning

Research  3  397–422.

Brafman  R. I.  & Tennenholtz  M. (2002). R-MAX—a general polynomial time algorithm for near-optimal

reinforcement learning. Journal of Machine Learning Research  3  213–231.

Chow  C.-S.  & Tsitsiklis  J. N. (1991). An optimal one-way multigrid algorithmfor discrete time stochastic

control. IEEE Transactions on Automatic Control  36  898–914.

Fiechter  C.-N. (1997). PAC adaptive control of linear systems. Tenth Annual Conference on Computational

Learning Theory (COLT) (pp. 72–80).

Golub  G. H.  & Van Loan  C. F. (1996). Matrix computations. Baltimore  Maryland: The Johns Hopkins

University Press. 3rd edition.

Kakade  S. M. (2003). On the sample complexity of reinforcement learning. Doctoral dissertation  Gatsby

Computational Neuroscience Unit  University College London.

Kakade  S. M. K.  Kearns  M. J.  & Langford  J. C. (2003). Exploration in metric state spaces. Proceedings of

the 20th International Conference on Machine Learning (ICML-03).

Kearns  M.  Mansour  Y.  & Ng  A. Y. (1999). A sparse sampling algorithm for near-optimal planning in
large Markov decision processes. Proceedings of the Sixteenth International Joint Conference on Artiﬁcial
Intelligence (IJCAI-99) (pp. 1324–1331).

Kearns  M. J.  & Singh  S. P. (2002). Near-optimal reinforcement learning in polynomial time. Machine

Learning  49  209–232.

Ng  A. Y.  Kim  H. J.  Jordan  M. I.  & Sastry  S. (2003). Autonomous helicopter ﬂight via reinforcement

learning. Advances in Neural Information Processing Systems 16 (NIPS-03).

Seber  G. A. F.  & Lee  A. J. (2003). Linear regression analysis. Wiley-Interscience.
Sutton  R. S.  & Barto  A. G. (1998). Reinforcement learning: An introduction. The MIT Press.
Tesauro  G. (1994). TD-Gammon  a self-teaching backgammon program  achieves master-level play. Neural

Computation  6  215–219.

8

,Qirong Ho
James Cipar
Henggang Cui
Seunghak Lee
Jin Kyu Kim
Phillip B. Gibbons
Garth Gibson
Greg Ganger
Eric Xing