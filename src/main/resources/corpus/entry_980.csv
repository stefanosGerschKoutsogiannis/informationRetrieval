2009,Code-specific policy gradient rules for spiking neurons,Although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning  the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood. Here  we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals  i.e.  depending on which neural code is in effect. We use the framework of Williams (1992) to derive learning rules for arbitrary neural codes. For illustration  we present policy-gradient rules for  three different example codes - a spike count code  a spike timing code and the most general ``full spike train code - and test them on simple model problems. In addition to classical synaptic learning  we derive learning rules for intrinsic parameters that control the excitability of the neuron. The spike count learning rule has structural similarities with established Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train features  belongs to the natural exponential family  the learning rules have a characteristic shape that raises interesting prediction problems.,Code-speciﬁc policy gradient rules for

spiking neurons

Henning Sprekeler∗ Guillaume Hennequin Wulfram Gerstner

Laboratory for Computational Neuroscience
´Ecole Polytechnique F´ed´erale de Lausanne

1015 Lausanne

Abstract

Although it is widely believed that reinforcement learning is a suitable tool for
describing behavioral learning  the mechanisms by which it can be implemented
in networks of spiking neurons are not fully understood. Here  we show that dif-
ferent learning rules emerge from a policy gradient approach depending on which
features of the spike trains are assumed to inﬂuence the reward signals  i.e.  de-
pending on which neural code is in effect. We use the framework of Williams
(1992) to derive learning rules for arbitrary neural codes. For illustration  we
present policy-gradient rules for three different example codes - a spike count
code  a spike timing code and the most general “full spike train” code - and test
them on simple model problems. In addition to classical synaptic learning  we
derive learning rules for intrinsic parameters that control the excitability of the
neuron. The spike count learning rule has structural similarities with established
Bienenstock-Cooper-Munro rules. If the distribution of the relevant spike train
features belongs to the natural exponential family  the learning rules have a char-
acteristic shape that raises interesting prediction problems.

1 Introduction

Neural implementations of reinforcement learning have to solve two basic credit assignment prob-
lems: (a) the temporal credit assignment problem  i.e.  the question which of the actions that were
taken in the past were crucial to receiving a reward later and (b) the spatial credit assignment prob-
lem  i.e.  the question  which neurons in a population were important for getting the reward and
which ones were not.
Here  we argue that an additional credit assignment problem arises in implementations of reinforce-
ment learning with spiking neurons. Presume that we know that the spike pattern of one speciﬁc
neuron within one speciﬁc time interval was crucial for getting the reward (that is  we have already
solved the ﬁrst two credit assignment problems). Then  there is still one question that remains:
Which feature of the spike pattern was important for the reward? Would any spike train with the
same number of spikes yield the same reward or do we need precisely timed spikes to get it? This
credit assignment problem is in essence the question which neural code the output neuron is (or
should be) using. It becomes particularly important  if we want to change neuronal parameters like
synaptic weights in order to maximize the likelihood of getting the reward again in the future. If
only the spike count is relevant  it might not be very effective to spend a lot of time and energy on
the difﬁcult task of learning precisely timed spikes.
The most modest and probably most versatile way of solving this problem is not to make any as-
sumption on the neural code but to assume that all features of the spike train were important. In

∗E-Mail: henning.sprekeler@epfl.ch

1

this case  neuronal parameters are changed such that the likelihood of repeating exactly the same
spike train for the same synaptic input is maximized. This approach leads to a learning rule that
was derived in a number of recent publications [3  5  13]. Here  we show that a whole class of
learning rules emerges when prior knowledge about the neural code at hand is available. Using a
policy-gradient framework  we derive learning rules for neural parameters like synaptic weights or
threshold parameters that maximize the expected reward.
Our aims are to (a) develop a systematic framework that allows to derive learning rules for arbitrary
neural parameters for different neural codes  (b) provide an intuitive understanding how the resulting
learning rules work  (c) derive and test learning rules for speciﬁc example codes and (d) to provide
a theoretical basis why code-speciﬁc learning rules should be superior to general-purpose rules.
Finally  we argue that the learning rules contain two types of prediction problems  one related to
reward prediction  the other to response prediction.

2 General framework

2.1 Coding features and the policy-gradient approach

The basic setup is the following: let there be a set of different input spike trains X µ to a single post-
synaptic neuron  which in response generates stochastic output spike trains Y µ. In the language of
partially observable Markov decision processes  the input spike trains are observations that provide
information about the state of the animal and the output spike trains are controls that inﬂuence the
action choice. Depending on both of these spike trains  the system receives a reward. The goal is to
adjust a set of parameters θi of the postsynaptic neuron such that it maximizes the expectation value
of the reward.
Our central assumption is that the reward R does not depend on the full output spike train  but only
on a set of coding features Fj(Y ) of the output spike train: R = R(F  X). Which coding features F
the reward depends on is in fact a choice of a neural code  because all other features of the spike
train are not behaviorally relevant. Note that there is a conceptual difference to the notion of a neural
code in sensory processing  where the coding features convey information about input signals  not
about the output signal or rewards.

The expectation value of the reward is given by (cid:104)R(cid:105) = (cid:80)

F X R(F  X)P (F|X  θ)P (X)  where
P (X) denotes the probability of the presynaptic spike trains and P (F|X  θ) the conditional proba-
bility of generating the coding feature F given the input spike train X and the neuronal parameters
θ. Note that the only component that explicitly depends on the neural parameters θi is the condi-
tional probability P (F|X  θ). The reward is conditionally independent of the neural parameters θi
given the coding feature F. Therefore  if we want to optimize the expected reward by employing a
gradient ascent method  we get a learning rule of the form

(cid:88)
(cid:88)

F X

∂tθi = η

= η

R(F  X)P (X)∂θiP (F|X  θ)

P (X)P (F|X  θ)R(F  X)∂θi ln P (F|X  θ) .

(1)

(2)

F X

If we choose a small learning rate η  the average over presynaptic patterns X and coding features
F can be replaced by a time average. A corresponding online learning rule therefore results from
dropping the average over X and F :

∂tθi = ηR(F  X)∂θi ln P (F|X  θ) .

(3)

This general form of learning rule is well known in policy-gradient approaches to reinforcement
learning [1  12].

2.2 Learning rules for exponentially distributed coding features

distributions P (F|X) = (cid:81)

The joint distribution of the coding features Fj can always be factorized into a set of conditional
i P (Fi|X; F1  ...  Fi−1). We now make the assumption that the con-
ditional distributions belong to the natural exponential family (NEF): P (Fi|X; F1  ...  Fi−1  θ) =

2

h(Fi) exp(CiFi − A(Ci))  where the Ci are parameters that depend on the input spike train X  the
coding features F1  ...  Fi−1 and the neural parameters θi. h(Fi) is a function of Fi and Ai(Ci) is
function that is characteristic for the distribution and depends only on the parameters Ci. Note that
the NEF is a relatively rich class of distributions  which includes many canonical distributions like
the Poisson  Bernoulli and the Gaussian distribution (the latter with ﬁxed variance).
Under these assumptions  the learning rule (3) takes a characteristic shape:

∂tθi = ηR(F  X)(cid:88)

Fj − µj

σ2
j

∂θiµj  

(4)

j

and σ2
i

are

the

the mean and the variance of

where µi
conditional distribution
P (Fi|X  F1  ...  Fi−1  θ) and therefore also depend on the input X  the coding features F1  ...  Fi−1
and the parameters θ. Note that correlations between the coding features are implicitly accounted
for by the dependence of µi and σi on the other features. The summation over different coding
features arises from the factorization of the distribution  while the speciﬁc shape of the summands
relies on the assumption of normal exponential distributions [for a proof  cf. 12].
There is a simple intuition why the learning rule (4) performs gradient ascent on the mean reward.
The term Fj − µj ﬂuctuates around zero on a trial-to-trial basis. If these ﬂuctuations are positively
correlated with the trial ﬂuctuations of the reward R  i.e.  (cid:104)R(Fj − µj)(cid:105) > 0  higher values of Fj
lead to higher reward  so that the mean of the coding feature should be increased. This increase is
implemented by the term ∂θiµj  which changes the neural parameter θi such that µj increases.

3 Examples for Coding Features

In this section  we illustrate the framework by deriving policy-gradient rules for different neural
codes and show that they can solve simple computational tasks.
The neuron type we are using is a simple Poisson-type neuron model where the postsynaptic ﬁring
rate is given by a nonlinear function ρ(u) of the membrane potential u. The membrane potential u 
in turn  is given by the sum of the EPSPs that are evoked by the presynaptic spikes  weighted with
the respective synaptic weights:

u(t) =(cid:88)

i )   =:(cid:88)

wi(t − tf

wiPSPi(t)  

(5)

i f

i

i denote the time of the f-th spike in the i-th presynaptic neuron. (t − tf

where tf
i ) denotes the
shape of the postsynaptic potential evoked by a single presynaptic spike at time tf
i . For future use 
we have introduced PSPi as the postsynaptic potential that would be evoked by the i-th presynaptic
spike train alone  if the synaptic weight were unity.
The parameters that one could optimize in this neuron model are (a) the synaptic weights and (b) pa-
rameters in the dependence of the ﬁring rate ρ on the membrane potential. The ﬁrst case is the
standard case of synaptic plasticity  the second corresponds to a reward-driven version of intrinsic
plasticity [cf. 10].

3.1 Spike Count Codes: Synaptic plasticity

Let us ﬁrst assume that the coding feature is the number N of spikes within a given time win-
dow [0  T ] and that the reward is delivered at the end of this period. The probability distribution for
the spike count is a Poisson distribution P (N) = µN exp(−µ)/N! with a mean µ that is given by
the integral of the ﬁring rate ρ over the interval [0  T ]:

µ =

ρ(t(cid:48)) dt(cid:48) .

(6)

The dependence of the distribution P (N) on the presynaptic spike trains X and the synaptic
weights wi is hidden in the mean spike count µ  which naturally depends on those factors through
the postsynaptic ﬁring rate ρ.

0

3

(cid:90) T

Because the Poisson distribution belongs to the NEF  we can derive a synaptic learning rule by using
equation (4) and calculating the particular form of the term ∂wiµ:

∂twi = ηR

[∂uρ](t(cid:48))PSPi(t(cid:48)) dt(cid:48) .

(7)

(cid:90) T

N − µ

µ

0

This learning rule has structural similarities with the Bienenstock-Cooper-Munro (BCM) rule [2]:
The integral term has the structure of an eligibility trace that is driven by a simple Hebbian learning
rule. In addition  learning is modulated by a factor that compares the current spike count (“rate”)
with the expected spike count (“sliding threshold” in BCM theory). Interestingly  the functional role
of this factor is very different from the one in the original BCM rule: It is not meant to introduce
selectivity [2]  but rather to exploit trial ﬂuctuations around the mean spike count to explore the
structure of the reward landscape.
We test the learning rule on a 2-armed bandit task (Figure 1A). An agent has the choice between
two actions. Depending on which of two states the agent is in  action a1 or action a2 is rewarded
(R = 1)  while the other action is punished (R = −1). The state information is encoded in the rate
pattern of 100 presynaptic neurons. For each state  a different input pattern is generated by drawing
the ﬁring rate of each input neuron independently from an exponential distribution with a mean of
10Hz. In each trial  the input spike trains are generated anew from Poisson processes with these
neuron- and state-speciﬁc rates. The agent chooses its action stochastically with probabilities that
are proportional to the spike counts of two output neurons: p(ak|s) = Nk/(N1 + N2). Because
the spike counts depend on the state via the presynaptic ﬁring rates  the agent can choose different
actions for different states. Figure 1B and C show that the learning rule learns the task by suppressing
activity in the neuron that encodes the punished action.
In all simulations throughout the paper  the postsynaptic neurons have an exponential rate function
g(u) = exp (γ(u − u0))  where the threshold is u0 = 1. The sharpness parameter γ is set to either
γ = 1 (for the 2-armed bandit task) or γ = 3 (for the spike latency task). Moreover  the postsynaptic
neurons have a membrane potential reset after each spike (i.e.  relative refractoriness)  so that the
assumption of a Poisson distribution for the spike counts is not necessarily fulﬁlled. It is worth
noting that this did not have an impeding effect on learning performance.

3.2 Spike Count Codes: Intrinsic plasticity
Let us now assume that the rate of the neuron is given by a function ρ(u) = g (γ(u − u0)) which
depends on the threshold parameters u0 and γ. Typical choices for the function g would be an
exponential (as used in the simulations)  a sigmoid or a threshold linear function g(x) = ln(1 +
exp(x)).
By intrinsic plasticity we mean that the parameters u0 and γ are learned instead of or in addition
to the synaptic weights. The learning rules for these parameters are essentially the same as for the
synaptic weights  only that the derivative of the mean spike count is taken with respect to u0 and γ 
respectively:

∂tu0 = η

∂tγ = η

N − µ

µ

N − µ

µ

∂u0µ = −η

N − µ

µ
N − µ

µ

0

∂γµ = η

(cid:90) T
(cid:90) T

0

γg(cid:48)(γ(u(t) − u0)) dt

g(cid:48)(γ(u(t) − u0))(u(t) − u0) dt .

(8)

(9)

Here  g(cid:48) = ∂xg(x) denotes the derivative of the rate function g with respect to its argument.

3.3 First Spike-Latency Code: Synaptic plasticity

As a second coding scheme  let us assume that the reward depends only on the latency ˆt of the ﬁrst
spike after stimulus onset. More precisely  we assume that each trial starts with the onset of the
presynaptic spike trains X and that a reward is delivered at the time of the ﬁrst spike. The reward
depends on the latency of that spike  so that certain latencies are favored.

4

Figure 1: Simulations for code-speciﬁc learning rules. A 2-armed bandit task: The agent has to choose among
two actions a1 and a2. Depending on the state (s1 or s2)  a different action is rewarded (thick arrows). The
input states are modelled by different ﬁring rate patterns of the input neurons. The probability of choosing the
actions is proportional to the spike counts of two output neurons: p(ak(cid:124) s) = Nk(cid:47) (N1 + N2). B Learning
curves of the 2-armed bandit. Blue: Spike count learning rule (7)  Red: Full spike train rule (16). C Evolution
of the spike count in response to the two input states during learning. Both rewards (panel B) and spike counts
(panel C) are low-pass ﬁltered with a time constant of 4000 trials. D Learning of ﬁrst spike latencies with
the latency rule (11). Two different output neurons are to learn to ﬁre their ﬁrst spike at given target latencies
L1(cid:47) 2. We present one of two ﬁxed input spike train patterns (“stimuli”) to the neurons in randomly interleaved
trials. The input spike train for each input neuron is drawn separately for each stimulus by sampling once from
a Poisson process with a rate of 10Hz. Reward is given by the negative squared difference between the target
latency (stimulus 1: L1 = 10ms  L2 = 30ms  stimulus 2: L1 = 30ms  L2 = 10ms) and the actual latency
of the trial  summed over the two neurons. The colored curves show that the ﬁrst spike latencies of neurons 1
(green  red) and neuron 2 (purple  blue) converge to the target latencies. The black curve (scale on the right
axis) shows the evolution of the reward during learning.

The probability distribution of the spike latency is given by the product of the ﬁring probability at
time (cid:136)t and the probability that the neuron did not ﬁre earlier:

(cid:29)

(cid:31)

(cid:31)

(cid:30)(cid:136)t
(cid:30)(cid:136)t

0

(cid:31)

0

P ((cid:136)t) = (cid:31)((cid:136)t) exp

(cid:31)(t(cid:31)) dt(cid:31)

(cid:46)

Using eq. (3) for this particular distribution  we get the synaptic learning rule:

(cid:30)twi = (cid:29)R

[(cid:30)u(cid:31)]((cid:136)t)PSPi((cid:136)t)

(cid:31)((cid:136)t)

[(cid:30)u(cid:31)](t(cid:31))PSPi(t(cid:31)) dt(cid:31)

(cid:31)

(cid:29)

(10)

(cid:46)

(11)

In Figure 1D  we show that this learning rule can learn to adjust the weights of two neurons such
that their ﬁrst spike latencies approximate a set of target latencies.

3.4 The Full Spike Train Code: Synaptic plasticity

Finally  let us consider the most general coding feature  namely  the full spike train. Let us start with
a time-discretized version of the spike train with a discretization that is sufﬁciently narrow to allow at
most one spike per time bin. In each time bin [t(cid:44) t+(cid:31)t]  the number of spikes Yt follows a Bernoulli
distribution with spiking probability pt  which depends on the input and on the recent history of the
neuron. Because the Bernoulli distribution belongs to the NEF  the associated policy-gradient rule
can be derived using equation (4):

(cid:30)twi = (cid:29)R

Yt (cid:31) pt
pt(1 (cid:31) pt) (cid:30)wi pt (cid:46)

(12)

(cid:28)

t

5

The ﬁring probability pt depends on the instantaneous ﬁring rate ρt: pt = 1−exp(−ρt∆t)  yielding:

∂twi = ηR

= ηR

(cid:88)
(cid:88)

t

t

Yt − pt
pt(1 − pt)

(cid:124)(cid:123)(cid:122)(cid:125)

[∂ρpt]

=∆t(1−pt)

[∂wiρt]

(Yt − pt) ∂uρt
pt

PSPi(t)∆t

(13)

(14)

(cid:88)

(cid:18) Yt

∆t

t

(cid:19) ∂uρt

ρt
[∂uρ](t)

− ρt

(cid:90)

This is the rule that should be used in discretized simulations. In the limit ∆t → 0  pt can be
approximated by pt → ρ∆t  which leads to the continuous time version of the rule:

∂twi = ηR lim
t→0

PSPi(t)∆t

(15)

(Y (t) − ρ(t))

= ηR

Here  Y (t) =(cid:80)

(16)
δ(t− ti) is now a sum of δ-functions. Note that the learning rule (16) was already
proposed by Xie and Seung [13] and Florian [3] and  slightly modiﬁed for supervised learning  by
Pﬁster et al. [5].
Following the same line  policy gradient rules can also be derived for the intrinsic parameters of the
neuron  i.e.  its threshold parameters (see also [3]).

PSPi(t) dt .

ρ(t)

ti

4 Why use code-speciﬁc rules when more general rules are available?

Obviously  the learning rule (16) is the most general in the sense that it considers the whole spike
train as a coding feature. All possible other features are therefore captured in this learning rule. The
natural question is then: what is the advantage of using rules that are specialized for one speciﬁc
code?
Say  we have a learning rule for two coding features F1 and F2  of which only F1 is correlated with
reward. The learning rule for a particular neuronal parameter θ then has the following structure:

∂tθ = ηR(F1)

(cid:32)

(cid:18)(F1 − µ1)
(cid:12)(cid:12)(cid:12)(cid:12)µ1
(cid:12)(cid:12)(cid:12)(cid:12)µ1

σ2
1
R(µ1) + ∂R
∂F1

(F1 − µ1)2

σ2
1

∂R
∂F1

≈ η

= η

+ F2 − µ2

∂µ1
∂θ
(F1 − µ1)

σ2
2

∂µ2
∂θ

(cid:19)
(cid:33)(cid:18) F1 − µ1
(cid:12)(cid:12)(cid:12)(cid:12)µ1

∂R
∂F1

∂µ1
∂θ

+ η

+ηR(µ1) F1 − µ1

σ2
1

∂µ1
∂θ

+ ηR(µ1) F2 − µ2

σ2
2

σ2
2
∂µ2
∂θ

∂µ1
∂θ

+ F2 − µ2
σ2
1
(F1 − µ1)(F2 − µ2)

σ2
2

(cid:19)

∂µ2
∂θ

∂µ2
∂θ

(17)

(18)

(19)

(20)

Of the four terms in lines (19-20)  only the ﬁrst term has non-vanishing mean when taking the trial
average. The other terms are simply noise and therefore more hindrance than help when trying to
maximize the reward. When using the full learning rule for both features  the learning rate needs to
be decreased until an agreeable signal-to-noise ratio between the drift introduced by the ﬁrst term
and the diffusion caused by the other terms is reached. Therefore  it is desirable for faster learning
to reduce the effects of these noise terms. This can be done in two ways:

• The terms in eq. (20) can be reduced by reducing R(µ1). This can be achieved by subtract-
ing a suitable reward baseline from the current reward. Ideally  this should be done in a
stimulus-speciﬁc way (because µ1 depends on the stimulus)  which leads to the notion of a
reward prediction error instead of a pure reward signal. This approach is in line with both
standard reinforcement learning theory [4] and the proposal that neuromodulatory signals
like dopamine represent reward prediction error instead of reward alone.

6

• The term in eq. (20) can be removed by skipping those terms in the original learning that are
related to coding feature F2. This corresponds to using the learning rule for those features
that are in fact correlated with reward while suppressing those that are not correlated with
reward. The central argument for using code-speciﬁc learning rules is therefore the signal-
to-noise ratio. In extreme cases  where a very general rule is used for a very speciﬁc task 
a very large number of coding dimensions may merely give rise to noise in the learning
dynamics  while only one is relevant and causes systematic changes.

These considerations suggest that the spike count rule (7) should outperform the full spike train
rule (16) in tasks where the reward is based purely on spike count. Unfortunately  we could not
yet substantiate this claim in simulations. As seen in Figure 1B  the performance of the two rules
is very similar in the 2-armed bandit task. This might be due to a noise bottleneck effect: there
are several sources of noise in the learning process  the strongest of which limits the performance.
Unless the “code-speciﬁc noise” is dominant  code-speciﬁc learning rules will have about the same
performance as general purpose rules.

5 Inherent Prediction Problems

As shown in section 4  the policy-gradient rule with a reduced amount of noise in the gradient
estimate is one that takes only the relevant coding features into account and subtracts the trial mean
of the reward:

∂tθ = η(R − R(µ1  µ2  ...))(cid:88)

∂θµj

(21)

Fj − µj

σ2
j

j

This learning rule has a conceptually interesting structure: Learning takes place only when two con-
ditions are fulﬁlled: the animal did something unexpected (Fj − µi) and receives an unexpected
reward (R − R(µ1  µ2  ...)). Moreover  it raises two interesting prediction problems: (a) the predic-
tion of the trial average µj of the coding feature conditioned on the stimulus and (b) the reward that
is expected if the coding feature takes its mean value.

5.1 Prediction of the coding feature

In the cases where we could derive the learning rule analytically  the trial average of the coding
feature could be calculated from intrinsic properties of the neuron like its membrane potential. Un-
fortunately  it is not clear a priori that the information necessary for calculating this mean is always
available. This should be particularly problematic when trying to extend the framework to coding
features of populations  where the population would need to know  e.g.  membrane properties of its
members.
An interesting alternative is that the trial mean is calculated by a prediction system  e.g.  by top-
down signals that use prior information or an internal world model to predict the expected value
of the coding feature. Learning would in this case be modulated by the mismatch of a top-down
prediction of the coding feature - represented by µj(X) - and the real value of Fj  which is calculated
by a “bottom-up” approach. This interpretation bears interesting parallels to certain approaches in
sensory coding  where the interpretation of sensory information is based on a comparison of the
sensory input with an internally generated prediction from a generative model [cf. 6]. There is also
some experimental evidence for neural stimulus prediction even in comparably low-level systems as
the retina [e.g. 8].
Another prediction system for the expected response could be a population coding scheme  in which
a population of neurons is receiving the same input and should produce the same output. Any
neuron of the population could receive the average population activity as a prediction of its own
mean response.
It would be interesting to study the relation of such an approach with the one
recently proposed for reinforcement learning in populations of spiking neurons [11].

5.2 Reward prediction

The other quantity that should be predicted in the learning rule is the reward one would get when
the coding feature would take the value of its mean.
If the distribution of the coding feature is

7

R(µ) ≈ (cid:104)R(F )(cid:105)F|X

sufﬁciently narrow so that in the range F takes for a given stimulus  the reward can be approximated
by a linear function  the reward R(µ) at the mean is simply the expectation value of the reward given
the stimulus:

(22)
The relevant quantity for learning is therefore a reward prediction error R(F ) − (cid:104)R(F )(cid:105)F|X. In
classical reinforcement learning  this term is often calculated in an actor-critic architecture  where
some external module - the critic - learns the expected future reward either for states alone or for
state-action pairs. These values are then used to calculate the expected reward for the current state
or state-action pair. The difference between the reward that was really received and the predicted
reward is then used as a reward prediction error that drives learning. There is evidence that dopamine
signals in the brain encode prediction error rather than reward alone [7].

6 Discussion

We have presented a general framework for deriving policy-gradient rules for spiking neurons and
shown that different learning rules emerge depending on which features of the spike trains are as-
sumed to inﬂuence the reward signals. Theoretical arguments suggest that code-speciﬁc learning
rules should be superior to more general rules  because the noise in the estimate of the gradient
should be smaller. More simulations will be necessary to check if this is indeed the case and in
which applications code-speciﬁc learning rules are advantageous.
For exponentially distributed coding features  the learning rule has a characteristic structure  which
allows a simple intuitive interpretation. Moreover  this structure raises two prediction problems 
which may provide links to other concepts: (a) the notion of using a reward prediction error to reduce
the variance in the estimate of the gradient creates a link to actor-critic architectures [9] and (b) the
notion of coding feature prediction is reminiscent of combined top-down–bottom-up approaches 
where sensory learning is driven by the mismatch of internal predictions and the sensory signal [6].
The fact that there is a whole class of code-speciﬁc policy-gradient learning rules opens the interest-
ing possibility that neuronal learning rules could be controlled by metalearning processes that shape
the learning rule according to what neural code is in effect. From the biological perspective  it would
be interesting to compare spike-based synaptic plasticity in different brain regions that are thought
to use different neural codes and see if there are systematic differences.

References
[1] Baxter  J. and Bartlett  P. (2001). Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelli-

gence Research  15(4):319–350.

[2] Bienenstock  E.  Cooper  L.  and Munroe  P. (1982). Theory of the development of neuron selectivity:
orientation speciﬁcity and binocular interaction in visual cortex. Journal of Neuroscience  2:32–48. reprinted
in Anderson and Rosenfeld  1990.

[3] Florian  R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent synaptic

plasticity. Neural Computation  19:1468–1502.

[4] Greensmith  E.  Bartlett  P.  and Baxter  J. (2004). Variance reduction techniques for gradient estimates in

reinforcement learning. The Journal of Machine Learning Research  5:1471–1530.

[5] Pﬁster  J.-P.  Toyoizumi  T.  Barber  D.  and Gerstner  W. (2006). Optimal spike-timing dependent plasticity

for precise action potential ﬁring in supervised learning. Neural Computation  18:1309–1339.

[6] Rao  R. P. and Ballard  D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of

some extra-classical receptive-ﬁeld effects. Nature Neuroscience  2(1):79–87.

[7] Schultz  W.  Dayan  P.  and Montague  R. (1997). A neural substrate for prediction and reward. Science 

275:1593–1599.

[8] Schwartz  G.  Harris  R.  Shrom  D.  and II  M. (2007). Detection and prediction of periodic patterns by

the retina. Nature Neuroscience  10:552–554.

[9] Sutton  R. and Barto  A. (1998). Reinforcement learning. MIT Press  Cambridge.
[10] Triesch  J. (2007). Synergies between intrinsic and synaptic plasticity mechanisms. Neural computation 

19:885 –909.

8

[11] Urbanczik  R. and Senn  W. (2009). Reinforcement learning in populations of spiking neurons. Nat

Neurosci  12(3):250–252.

[12] Williams  R. (1992). Simple statistical gradient-following methods for connectionist reinforcement learn-

ing. Machine Learning  8:229–256.

[13] Xie  X. and Seung  H. (2004). Learning in neural networks by reinforcement of irregular spiking. Physical

Review E  69(4):41909.

9

,Akihiro Kishimoto
Radu Marinescu
Adi Botea