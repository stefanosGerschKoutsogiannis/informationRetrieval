2015,Bounding errors of Expectation-Propagation,Expectation Propagation is a very popular algorithm for variational inference  but comes with few theoretical guarantees. In this article  we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints  which allows us to study EP's convergence with respect to the true posterior. In particular  we show that EP converges at a rate of $O(n^{-2})$ for the mean  up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4  as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition  we introduce bounds on the moments of log-concave distributions that may be of independent interest.,Bounding errors of Expectation-Propagation

Guillaume Dehaene
University of Geneva

guillaume.dehaene@gmail.com

Simon Barthelmé
CNRS  Gipsa-lab

simon.barthelme@gipsa-lab.fr

Abstract

Expectation Propagation is a very popular algorithm for variational inference  but
comes with few theoretical guarantees. In this article  we prove that the approx-
imation errors made by EP can be bounded. Our bounds have an asymptotic in-
terpretation in the number n of datapoints  which allows us to study EP’s conver-
gence with respect to the true posterior. In particular  we show that EP converges
at a rate of O(n−2) for the mean  up to an order of magnitude faster than the tra-
ditional Gaussian approximation at the mode. We also give similar asymptotic ex-
pansions for moments of order 2 to 4  as well as excess Kullback-Leibler cost (de-
ﬁned as the additional KL cost incurred by using EP rather than the ideal Gaussian
approximation). All these expansions highlight the superior convergence proper-
ties of EP. Our approach for deriving those results is likely applicable to many
similar approximate inference methods. In addition  we introduce bounds on the
moments of log-concave distributions that may be of independent interest.

Introduction

Expectation Propagation (EP  1) is an efﬁcient approximate inference algorithm that is known to give
good approximations  to the point of being almost exact in certain applications [2  3]. It is surprising
that  while the method is empirically very successful  there are few theoretical guarantees on its
behavior. Indeed  most work on EP has focused on efﬁciently implementing the method in various
settings. Theoretical work on EP mostly represents new justiﬁcations of the method which  while
they offer intuitive insight  do not give mathematical proofs that the method behaves as expected.
One recent breakthrough is due to Dehaene and Barthelmé [4] who prove that  in the large data-
limit  the EP iteration behaves like a Newton search and its approximation is asymptotically exact.
However  it remains unclear how good we can expect the approximation to be when we have only
ﬁnite data. In this article  we offer a characterization of the quality of the EP approximation in terms
of the worst-case distance between the true and approximate mean and variance.
When approximating a probability distribution p(x) that is  for some reason  close to being Gaussian 
a natural approximation to use is the Gaussian with mean equal to the mode (or argmax) of p(x) and
with variance the inverse log-Hessian at the mode. We call it the Canonical Gaussian Approximation
(CGA)  and its use is usually justiﬁed by appealing to the Bernstein-von Mises theorem  which
shows that  in the limit of a large amount of independent observations  posterior distributions tend
towards their CGA. This powerful justiﬁcation  and the ease with which the CGA is computed
(ﬁnding the mode can be done using Newton methods) makes it a good reference point for any
method like EP which aims to offer a better Gaussian approximation at a higher computational cost.
In section 1  we introduce the CGA and the EP approximation. In section 2  we give our theoretical
results bounding the quality of EP approximations.

1

1 Background

In this section  we present the CGA and give a short introduction to the EP algorithm. In-depth
descriptions of EP can be found in Minka [5]  Seeger [6]  Bishop [7]  Raymond et al. [8].

1.1 The Canonical Gaussian Approximation

What we call here the CGA is perhaps the most common approximate inference method in the
machine learning cookbook. It is often called the “Laplace approximation”  but this is a misnomer:
the Laplace approximation refers to approximating the integral
p from the integral of the CGA.
The reason the CGA is so often used is its compelling simplicity: given a target distribution p(x) =
exp (−φ (x))  we ﬁnd the mode x(cid:63) and compute the second derivatives of φ at x(cid:63):

´

x(cid:63) = argminφ(x)
β(cid:63) = φ(cid:48)(cid:48) (x(cid:63))

to form a Gaussian approximation q(x) = N(cid:16)
Roughly  if pn(x) ∝ (cid:81)n
limn→∞ pn (x) = N(cid:16)

a second-order Taylor expansion  and its use is justiﬁed by the Bernstein-von Mises theorem [9] 
which essentially says that the CGA becomes exact in the large-data (large-n) asymptotic limit.
i=1 p (yi|x) p0 (x)  where y1 . . . yn represent independent datapoints  then
x|x(cid:63)
n  1
β(cid:63)
n

in total variation.

(cid:17) ≈ p(x). The CGA is effectively just

x|x(cid:63)  1

β(cid:63)

(cid:17)

1.2 CGA vs Gaussian EP

Gaussian EP  as its name indicates  provides an alternative way of computing a Gaussian approxima-
tion to a target distribution. There is broad overlap between the problems where EP can be applied
and the problems where the CGA can be used  with EP coming at a higher cost. Our contribution is
to show formally that the higher computational cost for EP may well be worth bearing  as EP approx-
imations can outperform CGAs by an order of magnitude. To be speciﬁc  we focus on the moment
estimates (mean and covariance) computed by EP and CGA  and derive bounds on their distance to
the true mean and variance of the target distribution. Our bounds have an asymptotic interpretation 
and under that interpretation we show for example that the mean returned by EP is within an order

of O(cid:0)n−2(cid:1) of the true mean  where n is the number of datapoints. For the CGA  which uses the
mode as an estimate of the mean  we exhibit a O(cid:0)n−1(cid:1) upper bound  and we compute the error term
responsible for this O(cid:0)n−1(cid:1) behavior. This enables us to show that  in the situations in which this
error is indeed O(cid:0)n−1(cid:1)  EP is better than the CGA.

1.3 The EP algorithm
We consider the task of approximating a probability distribution over a random-variable X : p(x) 
which we call the target distribution. X can be high-dimensional  but for simplicity  we focus on
the one-dimensional case. One important hypothesis that makes EP feasible is that p(x) factorizes
into n simple factor terms:

p(x) =

fi(x)

(cid:89)

i

(cid:18)

EP proposes to approximate each fi(x) (usually referred to as sites) by a Gaussian function qi(x)
(referred to as the site-approximations). It is convenient to use the parametrization of Gaussians in
terms of natural parameters:

qi (x|ri  βi) ∝ exp

rix − βi

(cid:19)

x2
2

which makes some of the further computations easier to understand. Note that EP could also be
used with other exponential approximating families. These Gaussian approximations are computed
i ))  we select a site for update with
iteratively. Starting from a current approximation (qt
index i. We then:

i (x|rt

i  βt

2

• Compute the cavity distribution qt−i(x) ∝(cid:81)
(cid:88)

q−i(x) ∝ exp

eters:

j(cid:54)=1 qt

 x −

rt
j

j(cid:54)=i

(cid:88)

j(cid:54)=i

 x2

2



βt
j

j(x). This is very easy in natural param-

• Compute the hybrid distribution ht
• Compute the Gaussian which minimizes the Kullback-Leibler divergence to the hybrid  ie

i(x) ∝ qt−i(x)fi(x) and its mean and variance

the Gaussian with same mean and variance:

(cid:0)KL(cid:0)ht
i|q(cid:1)(cid:1)

P(ht

i) = argmin

q

• Finally  update the approximation of fi:

qt+1
i =

P(ht
i)
qt−i

where the division is simply computed as a subtraction between natural parameters

We iterate these operations until a ﬁxed point is reached  at which point we return a Gaussian ap-

proximation of p(x) ≈(cid:81) qi(x).

1.4 The “EP-approximation”

In this work  we will characterize the quality of an EP approximation of p(x). We deﬁne this to be
any ﬁxed point of the iteration presented in section 1.3  which could all be returned by the algorithm.
It is known that EP will have at least one ﬁxed-point [1]  but it is unknown under which conditions
the ﬁxed-point is unique. We conjecture that  when all sites are log-concave (one of our hypotheses
to control the behavior of EP)  it is in fact unique but we can’t offer a proof yet. If p (x) isn’t log-
concave  it is straightforward to construct examples in which EP has multiple ﬁxed-points. These
open questions won’t matter for our result because we will show that all ﬁxed-points of EP (should
there be more than one) produce a good approximation of p (x).
Fixed points of EP have a very interesting characterization. If we note q∗
i the site-approximations at
i the corresponding hybrid distributions  and q∗ the global approximation of
a given ﬁxed-point  h∗
p(x)  then the mean and variance of all the hybrids and q∗ is the same1. As we will show in section
2.2  this leads to a very tight bound on the possible positions of these ﬁxed-points.

1.5 Notation

i fi(x) is the target distribution we want
to approximate. The sites fi(x) are each approximated by a Gaussian site-approximation qi(x)
i qi(x). The hybrids hi(x) interpolate between q(x)

We will use repeatedly the following notation. p(x) =(cid:81)
yielding an approximation to p(x) ≈ q(x) =(cid:81)
φi(x) = − log (fi(x)) and φp(x) = − log (p(x)) = (cid:80) φi(x). We will introduce in section 2

and p(x) by replacing one site approximation qi(x) with the true site fi(x).
Our results make heavy use of the log-functions of the sites and the target distribution. We note

hypotheses on these functions. Parameter βm controls their minimum curvature and parameters Kd
control the maximum dth derivative.
We will always consider ﬁxed-points of EP  where the mean and variance under all hybrids and q(x)
is identical. We will note these common values: µEP and vEP . We will also refer to the third and
fourth centered moment of the hybrids  denoted by mi
4 and to the fourth moment of q(x) which
is simply 3v2
EP . We will show how all these moments are related to the true moments of the target
distribution which we will note µ  v for the mean and variance  and mp
3  mp
4 for the third and fourth
(cid:48)(cid:48)
where x(cid:63) is the
p (x(cid:63))

moment. We also investigate the quality of the CGA: µ ≈ x(cid:63) and v ≈(cid:104)

(cid:105)−1

3  mi

φ

the mode of p(x).

1For non-Gaussian approximations  the expected values of all sufﬁcient statistics of the exponential family

are equal.

3

2 Results

In this section  we will give tight bounds on the quality of the EP approximation (ie: of ﬁxed-points
of the EP iteration). Our results lean on the properties of log-concave distributions [10]. In section
2.1  we introduce new bounds on the moments of log-concave distributions. The bounds show that
those distributions are in a certain sense close to being Gaussian. We then apply these results to
study ﬁxed points of EP  where they enable us to compute bounds on the distance between the mean
and variance of the true distribution p(x) and of the approximation given by EP  which we do in
section 2.2.
Our bounds require us to assume that all sites fi(x) are βm-strongly log-concave with slowly-
changing log-function. That is  if we note φi(x) = − log (fi(x)):

∀i ∀x φ

(cid:48)(cid:48)

(cid:12)(cid:12)(cid:12)φ(d)

i (x) ≥ βm > 0
(x)

(cid:12)(cid:12)(cid:12) ≤ Kd

i

∀i ∀d ∈ [3  4  5  6]

(1)
(2)

− log (p(x)) = (cid:80)

bounded:

The target distribution p(x) then inherits those properties from the sites. Noting φp(x) =
i φi(x)  then φp is nβm-strongly log-concave and its higher derivatives are

∀x  φ
(cid:48)(cid:48)

(cid:12)(cid:12)(cid:12)φ(d)

p (x) ≥ nβm
p (x)

(cid:12)(cid:12)(cid:12) ≤ nKd

∀d ∈ [3  4  5  6]

(3)

(4)

A natural concern here is whether or not our conditions on the sites are of practical interest. Indeed 
strongly-log-concave likelihoods are rare. We picked these strong regularity conditions because they
make the proofs relatively tractable (although still technical and long). The proof technique carries
over to more complicated  but more realistic  cases. One such interesting generalization consists
of the case in which p(x) and all hybrids at the ﬁxed-point are log-concave with slowly changing
log-functions (with possibly differing constants). In such a case  while the math becomes more
unwieldy  similar bounds as ours can be found  greatly extending the scope of our results. The
results we present here should thus be understood as a stepping stone and not as the ﬁnal word on
the quality of the EP approximation: we have focused on providing a rigorous but extensible proof.

2.1 Log-concave distributions are strongly constrained

Log-concave distributions have many interesting properties. They are of course unimodal  and the
family is closed under both marginalization and multiplication. For our purposes however  the most
important property is a result due to Brascamp and Lieb [11]  which bounds their even moments. We
give here an extension in the case of log-concave distributions with slowly changing log-functions
(as quantiﬁed by eq. (2)). Our results show that these are close to being Gaussian.
The Brascamp-Lieb inequality states that  if LC(x) ∝ exp (−φ(x)) is βm-strongly log-concave (ie:
(x) ≥ βm)  then centered even moments of LC are bounded by the corresponding moments of a
(cid:48)(cid:48)
φ
Gaussian with variance β−1
m . If we note these moments m2k and µLC = ELC(x) the mean of LC:

(cid:16)

(x − µLC)2k(cid:17)

m2k = ELC
m2k ≤ (2k − 1)!!β−k

m

(5)
where (2k − 1)!! is the double factorial: the product of all odd terms from 1 to 2k − 1. 3!! = 3 
5!! = 15  7!! = 105  etc. This result can be understood as stating that a log-concave distribution
must have a small variance  but doesn’t generally need to be close to a Gaussian.
With our hypothesis of slowly changing log-functions  we were able to improve on this result. Our
improved results include a bound on odd moments  as well as ﬁrst order expansions of even moments
(eqs. (6)-(9)).
Our extension to the Brascamp-Lieb inequality is as follows. If φ is slowly changing in the sense
that some of its higher derivatives are bounded  as per eq. 2  then we can give a bound on φ
(µLC)

(cid:48)

4

(showing that µLC is close to the mode x(cid:63) of LC  see eqs. (10) to (13)) and m3 (showing that LC
is mostly symmetric):

and we can compute the ﬁrst order expansions of m2 and m4  and bound the errors in terms of βm
and the K’s :

(6)

(7)

(8)

(9)

(10)

(cid:12)(cid:12)(cid:12)φ

(cid:48)

(µLC)

(cid:12)(cid:12)(cid:12) ≤ K3

2βm
|m3| ≤ 2K3
β3
m

(cid:12)(cid:12)(cid:12)m−1

(cid:12)(cid:12)(cid:12)φ

(cid:48)(cid:48)

(cid:48)(cid:48)

(µLC)

2 − φ
(µLC)m4 − 3m2

3
β2
m

(cid:12)(cid:12)(cid:12) ≤ K 2
(cid:12)(cid:12)(cid:12) ≤ 19
(cid:17)−1

2

K4
2βm
5
2

+

K4
β3
m

+

K 2
3
β4
m

(cid:16)

(cid:17)−2

With eq. (8) and (9)  we see that m2 ≈ (cid:16)

(cid:48)(cid:48)

and m4 ≈ 3

(cid:48)(cid:48)

and  in that

(cid:48)(cid:48)

φ

φ

φ

(cid:16)

(µLC)

(µLC)

(µLC).

(cid:17)−k

(µLC)
(cid:48)(cid:48)
sense  that LC(x) is close to the Gaussian with mean µLC and inverse-variance φ
These expansions could be extended to further orders and similar formulas can be found for the other
−(k+1)
moments of LC(x): for example  any odd moments can be bounded by |m2k+1| ≤ CkK3β
m
(with Ck some constant) and any even moment can be found to have ﬁrst-order expansion:
m2k ≈ (2k − 1)!!
. The proof  as well as more detailed results  can be found in
the Supplement.
Note how our result relates to the Bernstein-von Mises theorem  which says that  in the limit of a
large amount of observations  a posterior p(x) tends towards its CGA. If we consider the posterior
obtained from n likelihood functions that are all log-concave and slowly changing  our results show
the slightly different result that the moments of that posterior are close to those of a Gaussian with
LC)) . This point is
mean µLC (instead of x(cid:63)
critical. While the CGA still ends up capturing the limit behavior of p  as µLC → x(cid:63) in the large-
data limit (see eq. (13) below)  an approximation that would return the Gaussian approximation at
µLC would be better. This is essentially what EP does  and this is how it improves on the CGA.

LC) and inverse-variance φ

(µLC) (instead of φ

(x(cid:63)

(cid:48)(cid:48)

(cid:48)(cid:48)

2.2 Computing bounds on EP approximations

In this section  we consider a given EP ﬁxed-point q∗

k (x|ri  βi) and the corresponding approximation
µEP and vEP ) are close to the true mean and variance of p (resp. µ and v)  and also investigate the

of p(x): q∗ (x|r =(cid:80) ri  β =(cid:80) βi). We will show that the expected value and variance of q∗(resp.
quality of the CGA (µ ≈ x(cid:63)  v ≈(cid:104)

(cid:105)−1

).

φ

(cid:48)(cid:48)
p (x(cid:63))

Under our assumptions on the sites (eq. (1) and (2))  we are able to derive bounds on the quality
of the EP approximation. The proof is quite involved and long  and we will only present it in the
Supplement. In the main text  we give a partial version: we detail the ﬁrst step of the demonstra-
tion  which consists of computing a rough bound on the distance between the true mean µ  the EP
approximation µEP and the mode x(cid:63)  and give an outline of the rest of the proof.
Let’s show that µ  µEP and x(cid:63) are all close to one another. We start from eq. (6) applied to p(x):

(cid:12)(cid:12)(cid:12)φ

(cid:48)
p(µ)

(cid:12)(cid:12)(cid:12) ≤ K3

2βm

5

which tells us that φ

(cid:48)

p(µ) ≈ 0. µ must thus be close to x(cid:63). Indeed:

(cid:12)(cid:12)(cid:12)φ

(cid:48)
p(µ)

(cid:12)(cid:12)(cid:12)φ
(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)φ
≥ (cid:12)(cid:12)(cid:12)φ

=

(cid:48)

(cid:48)(cid:48)

p(µ) − φ
(cid:48)
p(x(cid:63))
p (ξ) (µ − x(cid:63))

(cid:12)(cid:12)(cid:12)|µ − x(cid:63)|

(cid:48)(cid:48)
p (ξ)

≥ nβm |µ − x(cid:63)|

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12) ξ ∈ [µ  x(cid:63)]

Combining eq. (10) and (12)  we ﬁnally have:

|µ − x(cid:63)| ≤ n−1 K3
2β2
m

Let’s now show that µEP is also close to x(cid:63). We proceed similarly  starting from eq. (6) but applied
to all hybrids hi(x):

(14)
which is not really equivalent to eq. (10) yet. Recall that q(x|r  β) has mean µEP : we thus have:
r = βµEP . Which gives:

2βm

i(µEP ) + β−iµEP − r−i

(cid:12)(cid:12)(cid:12) ≤ n−1 K3

∀i

(cid:48)

(cid:12)(cid:12)(cid:12)φ
(cid:32)(cid:88)

i

(cid:33)

β−i

µEP = ((n − 1)β) µEP

= (n − 1)r
r−i
=

(cid:88)
(cid:12)(cid:12)(cid:12) ≤ K3

i

2βm

(cid:12)(cid:12)(cid:12)φ

(cid:48)
p(µEP )

If we sum all terms in eq. (14)  the β−iµEP and r−i thus cancel  leaving us with:

which is equivalent to eq. (10) but for µEP instead of µ. This shows that µEP is  like µ  close to x(cid:63):

|µEP − x(cid:63)| ≤ n−1 K3
2β2
m

At this point  we can show that  since they are both close to x(cid:63) (eq. (13) and (17))  µ = µEP +

O(cid:0)n−1(cid:1)  which constitutes the ﬁrst step of our computation of bounds on the quality of EP.
via computing(cid:12)(cid:12)v−1 − v−1

(cid:12)(cid:12)(cid:12) for the CGA  from eq. (8). In both cases 

After computing this  the next step is evaluating the quality of the approximation of the variance 

(cid:12)(cid:12) for EP and

(cid:12)(cid:12)(cid:12)v−1 − φ

(cid:48)(cid:48)
p (x(cid:63))

EP

we ﬁnd:

v−1 = v−1
(cid:48)(cid:48)

EP + O (1)
p (x(cid:63)) + O (1)

(18)
(19)
Since v−1 is of order n  because of eq. (5) (Brascamp-Lieb upper bound on variance)  this is a
decent approximation: the relative error is of order n−1.
We can ﬁnd similarly that both EP and CGA do a good job of ﬁnding a good approximation of the
fourth moment of p: m4. For EP this means that the fourth moment of each hybrid and of q are a
close match:

= φ

(11)

(12)

(13)

(15)

(16)

(17)

(20)

(21)

(22)

In contrast  the third moment of the hybrids doesn’t match at all the third moment of p  but their sum
does !

∀i m4 ≈ mi
≈ 3

EP

(cid:17)−2
(cid:16)
4 ≈ 3v2
(cid:48)(cid:48)
p (m)
φ
m3 ≈(cid:88)

mi
3

i

6

Finally  we come back to the approximation of µ by µEP . These obey two very similar relationships:

−1(cid:17)
−1(cid:17)
Since v = vEP + O(cid:0)n−2(cid:1) (a slight rephrasing of eq. (18))  we ﬁnally have:

= O(cid:16)
= O(cid:16)

v
p (µ)
2
vEP
2

(cid:48)
p(µEP ) + φ(3)

(cid:48)
p(µ) + φ(3)

p (µEP )

n

n

φ

φ

µ = µEP + O(cid:0)n−2(cid:1)

(23)

(24)

(25)

(26)

We summarize the results in the following theorem:
Theorem 1. Characterizing ﬁxed-points of EP
Under the assumptions given by eq. (1) and (2) (log-concave sites with slowly changing log)  we
can bound the quality of the EP approximation and the CGA:
|µ − x∗| ≤ n−1 K3
2β2
m

|µ − µEP| ≤ B1(n) = O(cid:0)n−2(cid:1)
(cid:12)(cid:12)(cid:12) ≤ 2K 2
(cid:12)(cid:12)(cid:12)v−1 − φ
(cid:12)(cid:12) ≤ B2(n) = O (1)
(cid:12)(cid:12)v−1 − v−1

p (x∗)

K4
2βm

β2
m

+

(cid:48)(cid:48)

3

EP

We give the full expression for the bounds B1 and B2 in the Supplement
Note that the order of magnitude of the bound on |µ − x(cid:63)| is the best possible  because it is at-
tained for certain distributions. For example  consider a Gamma distribution with natural parameters
nβ . More generally  from
(nα  nβ) whose mean α
eq. (23)  we can compute the ﬁrst order of the error:

β is approximated at order n−1 by its mode α

β − 1

µ − m ≈ − φ(3)
p (µ)
(cid:48)(cid:48)
φ
p (µ)

v
2

≈ − 1
2

(cid:2)φ

p (µ)(cid:3)2

φ(3)
p (µ)
(cid:48)(cid:48)

which is the term causing the order n−1 error. Whenever this term is signiﬁcant  it is thus safe to
conclude that EP improves on the CGA.
Also note that  since v−1 is of order n  the relative error for the v−1 approximation is of order n−1
for both methods. Despite having a convergence rate of the same order  the EP approximation is
demonstrably better than the CGA  as we show next. Let us ﬁrst see why the approximation for v−1
is only of order 1 for both methods. The following relationship holds:

p (µ)

(cid:48)(cid:48)
p (µ) + φ(3)

v−1 = φ
(cid:48)(cid:48)
In this relationship  φ
p (µ) is an order n term while the rest are order 1. If we now compare this to
the CGA approximation of v−1  we ﬁnd that it fails at multiple levels. First  it completely ignores
(cid:48)(cid:48)
the two order 1 terms  and then  because it takes the value of φ
p at x(cid:63) which is at a distance of
p = O (n)). The CGA is thus adding

O(cid:0)n−1(cid:1) from µ  it adds another order 1 error term (since φ(3)

+ φ(4)

p (µ)

(27)

mp
4
3!v

mp
3
2v

+ O(cid:0)n−1(cid:1)

quite a bit of error  even if each component is of order 1.
Meanwhile  vEP obeys a relationship similar to eq. (27):

(cid:20)

v−1
EP = φ

(cid:88)
|µ − µEP| = O(cid:0)n−2(cid:1)  we have φ

(cid:48)(cid:48)
p (µEP ) +

i

(cid:21)

We can see where the EP approximation produces errors. The φ

+ O(cid:0)n−1(cid:1)

(28)

φ(3)
i

(µEP )

mi
3
2vEP

+ φ(4)

p (µEP )

3v2
EP
3!vEP

p (µEP ) +O(cid:0)n−1(cid:1). The term involving m4 is also well

(cid:48)(cid:48)
p term is well approximated: since

(cid:48)(cid:48)

(cid:48)(cid:48)
p (µ) = φ

7

approximated  and we can see that the only term that fails is the m3 term. The order 1 error is thus
entirely coming from this term  which shows that EP performance suffers more from the skewness
of the target distribution than from its kurtosis.
Finally  note that  with our result  we can get some intuitions about the quality of the EP approxima-
tion using other metrics. For example  if the most interesting metric is the KL divergence KL (p  q) 
the excess KL divergence from using the EP approximation q instead of the true minimizer qKL
(which has the same mean µ and variance v as p) is given by:

(cid:19)(cid:33)

(cid:18) v

vEP

(29)

(30)

(31)

ˆ

∆KL =

p log

qKL

q

=

ˆ

(cid:32)
− (x − µ)2
(cid:18) v
(cid:19)2

2v
− 1 − log

+

p(x)

(cid:20) v
(cid:18) v − vEP

vEP

+

vEP

=

1
2
≈ 1
4

(x − µEP )2

2vEP

(cid:19)(cid:21)

− 1
2

log
(µ − µEP )2

2vEP

+
vEP
(µ − µEP )2

2vEP

which we recognize as KL (qKL  q). A similar formula gives the excess KL divergence from using
the CGA instead of qKL. For both methods  the variance term is of order n−2 (though it should be
smaller for EP)  but the mean term is of order n−3 for EP while it is of order n−1 for the CGA. Once
again  EP is found to be the better approximation.
Finally  note that our bounds are quite pessimistic: the true value might be a much better ﬁt than we
have predicted here.
A ﬁrst cause is the bounding of the derivatives of log(p) (eqs. (3) (4)): while those bounds are
correct  they might prove to be very pessimistic. For example  if the contributions from the sites to
the higher-derivatives cancel each other out  a much lower bound than nKd might apply. Similarly 
there might be another lower bound on the curvature much higher than nβm.
Another cause is the bounding of the variance from the curvature. While applying Brascamp-Lieb
requires the distribution to have high log-curvature everywhere  a distribution with high-curvature
close to the mode and low-curvature in the tails still has very low variance: in such a case  the
Brascamp-Lieb bound is very pessimistic.
In order to improve on our bounds  we will thus need to use tighter bounds on the log-derivatives of
the hybrids and of the target distribution  but we will also need an extension of the Brascamp-Lieb
result that can deal with those cases where a distribution is strongly log-concave around its mode
but  in the tails  the log-curvature is much lower.

3 Conclusion

EP has been used for now quite some time without any theoretical concrete guarantees on its per-
formance. In this work  we provide explicit performance bounds and show that EP is superior to the
CGA  in the sense of giving provably better approximations of the mean and variance. There are
now theoretical arguments for substituting EP to the CGA in a number of practical problems where
the gain in precision is worth the increased computational cost. This work tackled the ﬁrst steps in
proving that EP offers an appropriate approximation. Continuing in its tracks will most likely lead
to more general and less pessimistic bounds  but it remains an open question how to quantify the
quality of the approximation using other distance measures. For example  it would be highly use-
ful for machine learning if one could show bounds on prediction error when using EP. We believe
that our approach should extend to more general performance measures and plan to investigate this
further in the future.

References
[1] Thomas P. Minka. Expectation Propagation for approximate Bayesian inference. In UAI ’01:
Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence  pages 362–369 
San Francisco  CA  USA  2001. Morgan Kaufmann Publishers Inc.
ISBN 1-55860-800-1.
URL http://portal.acm.org/citation.cfm?id=720257.

8

[2] Malte Kuss and Carl E. Rasmussen. Assessing Approximate Inference for Binary Gaussian
Process Classiﬁcation. J. Mach. Learn. Res.  6:1679–1704  December 2005. ISSN 1532-4435.
URL http://portal.acm.org/citation.cfm?id=1194901.

[3] Hannes Nickisch and Carl E. Rasmussen. Approximations for Binary Gaussian Process
Classiﬁcation. Journal of Machine Learning Research  9:2035–2078  October 2008. URL
http://www.jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf.

[4] Guillaume Dehaene and Simon Barthelmé. Expectation propagation in the large-data limit.

Technical report  March 2015. URL http://arxiv.org/abs/1503.08060.

[5] T. Minka. Divergence Measures and Message Passing. Technical report  2005. URL

http://research.microsoft.com/en-us/um/people/minka/papers/
message-passing/minka-divergence.pdf.

[6] M. Seeger.

Expectation Propagation for Exponential Families.

report 
2005. URL http://people.mmci.uni-saarland.de/~{}mseeger/papers/
epexpfam.pdf.

Technical

[7] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science
and Statistics).
Springer  1st ed. 2006. corr. 2nd printing 2011 edition  October 2007.
ISBN 0387310738. URL http://www.amazon.com/exec/obidos/redirect?
tag=citeulike07-20&path=ASIN/0387310738.

[8] Jack Raymond  Andre Manoel  and Manfred Opper. Expectation propagation  September

2014. URL http://arxiv.org/abs/1409.6179.

Asymptotic Theory of Statistics and Probability (Springer
[9] Anirban DasGupta.
Texts in Statistics).
URL
http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&
path=ASIN/0387759700.

Springer  1 edition  March 2008.

ISBN 0387759700.

[10] Adrien Saumard and Jon A. Wellner. Log-concavity and strong log-concavity: A review.
Statist. Surv.  8:45–114  2014. doi: 10.1214/14-SS107. URL http://dx.doi.org/10.
1214/14-SS107.

[11] Herm J. Brascamp and Elliott H. Lieb. Best constants in young’s inequality  its converse  and
its generalization to more than three functions. Advances in Mathematics  20(2):151–173 
May 1976. ISSN 00018708. doi: 10.1016/0001-8708(76)90184-5. URL http://dx.doi.
org/10.1016/0001-8708(76)90184-5.

9

,Guillaume Dehaene
Simon Barthelmé