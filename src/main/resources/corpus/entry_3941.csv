2018,On gradient regularizers for MMD GANs,We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function  and devise a method to enforce exact  analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous  and experiments show that it stabilizes and accelerates training  giving image generation models that outperform state-of-the art methods on $160 \times 160$ CelebA and $64 \times 64$ unconditional ImageNet.,On gradient regularizers for MMD GANs

Gatsby Computational Neuroscience Unit

Gatsby Computational Neuroscience Unit

Dougal J. Sutherland⇤

University College London

dougal@gmail.com

Michael Arbel⇤

University College London

michael.n.arbel@gmail.com

Mikołaj Bi´nkowski

Department of Mathematics
Imperial College London
mikbinkowski@gmail.com

Arthur Gretton

Gatsby Computational Neuroscience Unit

University College London

arthur.gretton@gmail.com

Abstract

We propose a principled method for gradient-based regularization of the critic of
GAN-like models trained by adversarially optimizing the kernel of a Maximum
Mean Discrepancy (MMD). We show that controlling the gradient of the critic
is vital to having a sensible loss function  and devise a method to enforce exact 
analytical gradient constraints at no additional cost compared to existing approxi-
mate techniques based on additive regularizers. The new loss function is provably
continuous  and experiments show that it stabilizes and accelerates training  giving
image generation models that outperform state-of-the art methods on 160 ⇥ 160
CelebA and 64 ⇥ 64 unconditional ImageNet.

1

Introduction

There has been an explosion of interest in implicit generative models (IGMs) over the last few years 
especially after the introduction of generative adversarial networks (GANs) [16]. These models
allow approximate samples from a complex high-dimensional target distribution P  using a model
distribution Q✓  where estimation of likelihoods  exact inference  and so on are not tractable. GAN-
type IGMs have yielded very impressive empirical results  particularly for image generation  far
beyond the quality of samples seen from most earlier generative models [e.g. 18  22  23  24  38].
These excellent results  however  have depended on adding a variety of methods of regularization and
other tricks to stabilize the notoriously difﬁcult optimization problem of GANs [38  42]. Some of
this difﬁculty is perhaps because when a GAN is viewed as minimizing a discrepancy DGAN(P  Q✓) 
its gradient r✓ DGAN(P  Q✓) does not provide useful signal to the generator if the target and model
distributions are not absolutely continuous  as is nearly always the case [2].
An alternative set of losses are the integral probability metrics (IPMs) [36]  which can give credit to
models Q✓ “near” to the target distribution P [3  8  Section 4 of 15]. IPMs are deﬁned in terms of a
critic function: a “well behaved” function with large amplitude where P and Q✓ differ most. The IPM
is the difference in the expected critic under P and Q✓  and is zero when the distributions agree. The
Wasserstein IPMs  whose critics are made smooth via a Lipschitz constraint  have been particularly
successful in IGMs [3  14  18]. But the Lipschitz constraint must hold uniformly  which can be hard
to enforce. A popular approximation has been to apply a gradient constraint only in expectation [18]:
the critic’s gradient norm is constrained to be small on points chosen uniformly between P and Q.
Another class of IPMs used as IGM losses are the Maximum Mean Discrepancies (MMDs) [17] 
as in [13  28]. Here the critic function is a member of a reproducing kernel Hilbert space (except
in [50]  who learn a deep approximation to an RKHS critic). Better performance can be obtained 

⇤These authors contributed equally.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

however  when the MMD kernel is not based directly on image pixels  but on learned features of
images. Wasserstein-inspired gradient regularization approaches can be used on the MMD critic
when learning these features: [27] uses weight clipping [3]  and [5  7] use a gradient penalty [18].
The recent Sobolev GAN [33] uses a similar constraint on the expected gradient norm  but phrases it
as estimating a Sobolev IPM rather than loosely approximating Wasserstein. This expectation can be
taken over the same distribution as [18]  but other measures are also proposed  such as (P + Q✓) /2.
A second recent approach  the spectrally normalized GAN [32]  controls the Lipschitz constant of
the critic by enforcing the spectral norms of the weight matrices to be 1. Gradient penalties also
beneﬁt GANs based on f-divergences [37]: for instance  the spectral normalization technique of [32]
can be applied to the critic network of an f-GAN. Alternatively  a gradient penalty can be deﬁned
to approximate the effect of blurring P and Q✓ with noise [40]  which addresses the problem of
non-overlapping support [2]. This approach has recently been shown to yield locally convergent
optimization in some cases with non-continuous distributions  where the original GAN does not [30].
In this paper  we introduce a novel regularization for the MMD GAN critic of [5  7  27]  which
directly targets generator performance  rather than adopting regularization methods intended to
approximate Wasserstein distances [3  18]. The new MMD regularizer derives from an approach
widely used in semi-supervised learning [10  Section 2]  where the aim is to deﬁne a classiﬁcation
function f which is positive on P (the positive class) and negative on Q✓ (negative class)  in the
absence of labels on many of the samples. The decision boundary between the classes is assumed
to be in a region of low density for both P and Q✓: f should therefore be ﬂat where P and Q✓ have
support (areas with constant label)  and have a larger slope in regions of low density. Bousquet et al.
[10] propose as their regularizer on f a sum of the variance and a density-weighted gradient norm.
We adopt a related penalty on the MMD critic  with the difference that we only apply the penalty on P:
thus  the critic is ﬂatter where P has high mass  but does not vanish on the generator samples from Q✓
(which we optimize). In excluding Q✓ from the critic function constraint  we also avoid the concern
raised by [32] that a critic depending on Q✓ will change with the current minibatch – potentially
leading to less stable learning. The resulting discrepancy is no longer an integral probability metric:
it is asymmetric  and the critic function class depends on the target P being approximated.
We ﬁrst discuss in Section 2 how MMD-based losses can be used to learn implicit generative models 
and how a naive approach could fail. This motivates our new discrepancies  introduced in Section 3.
Section 4 demonstrates that these losses outperform state-of-the-art models for image generation.

2 Learning implicit generative models with MMD-based losses
An IGM is a model Q✓ which aims to approximate a target distribution P over a space X✓ Rd.
We will deﬁne Q✓ by a generator function G✓ : Z!X   implemented as a deep network with
parameters ✓  where Z is a space of latent codes  say R128. We assume a ﬁxed distribution on Z 
say Z ⇠ Uniform[1  1]128  and call Q✓ the distribution of G✓(Z). We will consider learning by
minimizing a discrepancy D between distributions  with D(P  Q✓)  0 and D(P  P) = 0  which we
call our loss. We aim to minimize D(P  Q✓) with stochastic gradient descent on an estimator of D.
In the present work  we will build losses D based on the Maximum Mean Discrepancy 

MMDk(P  Q) =

sup

EX⇠P[f (X)]  EY ⇠Q[f (Y )] 

f : kfkHk1

(1)

(2)

an integral probability metric where the critic class is the unit ball within Hk  the reproducing
kernel Hilbert space with a kernel k. The optimization in (1) admits a simple closed-form optimal
critic  f⇤(t) / EX⇠P[k(X  t)]  EY ⇠Q[k(Y  t)]. There is also an unbiased  closed-form estimator of
MMD2
k with appealing statistical properties [17] – in particular  its sample complexity is independent
of the dimension of X   compared to the exponential dependence [52] of the Wasserstein distance

W(P  Q) =

sup

f : kfkLip1

EX⇠P[f (X)]  EY ⇠Q[f (Y )].

The MMD is continuous in the weak topology for any bounded kernel with Lipschitz embeddings [46 
D! P  then MMD(Pn  P) ! 0.
Theorem 3.2(b)]  meaning that if Pn converges in distribution to P  Pn
(W is continuous in the slightly stronger Wasserstein topology [51  Deﬁnition 6.9]; Pn W! P implies

2

D! P  and the two notions coincide if X is bounded.) Continuity means the loss can provide
Pn
better signal to the generator as Q✓ approaches P  as opposed to e.g. Jensen-Shannon where the loss
could be constant until suddenly jumping to 0 [e.g. 3  Example 1]. The MMD is also strict  meaning
it is zero iff P = Q✓  for characteristic kernels [45]. The Gaussian kernel yields an MMD both
continuous in the weak topology and strict. Thus in principle  one need not conduct any alternating
optimization in an IGM at all  but merely choose generator parameters ✓ to minimize MMDk.
Despite these appealing properties  using simple pixel-level kernels leads to poor generator samples
[8  13  28  48]. More recent MMD GANs [5  7  27] achieve better results by using a parameterized
family of kernels  {k } 2   in the Optimized MMD loss previously studied by [44  46]:

(3)

MMD(P  Q) := sup
D 
 2 

MMDk (P  Q).

We primarily consider kernels deﬁned by some ﬁxed kernel K on top of a learned low-dimensional
representation  : X! Rs  i.e. k (x  y) = K( (x)  (y))  denoted k = K   . In practice 
K is a simple characteristic kernel  e.g. Gaussian  and  is usually a deep network with output
dimension say s = 16 [7] or even s = 1 (in our experiments). If  is powerful enough  this choice
is sufﬁcient; we need not try to ensure each k is characteristic  as did [27].
Proposition 1. Suppose k = K     with K characteristic and { } rich enough that for any
P 6= Q  there is a 2 for which  #P 6=  #Q.2 Then if P 6= Q  D 
Proof. Let ˆ 2 be such that  ˆ (P) 6=  ˆ (Q). Then  since K is characteristic 

MMD(P  Q) > 0.

MMD2

MMD is stronger than Wasserstein or MMD.

MMD in an IGM  though  were unsuccessful [48  footnote 7]. This

MMDK( #P  #Q)  MMDK( ˆ #P  ˆ #Q) > 0.

MMD(P  Q) = sup
D 
 2 
MMD  one can conduct alternating optimization to estimate a ˆ and then update the
To estimate D 
generator according to MMDk ˆ   similar to the scheme used in GANs and WGANs. (This form of
estimator is justiﬁed by an envelope theorem [31]  although it is invariably biased [7].) Unlike DGAN
or W  ﬁxing a ˆ and optimizing the generator still yields a sensible distance MMDk ˆ .
Early attempts at minimizing D 
could be because for some kernel classes  D 
Example 1 (DiracGAN [30]). We wish to model a point mass at the origin of R  P = 0  with any
possible point mass  Q✓ = ✓ for ✓ 2 R. We use a Gaussian kernel of any bandwidth  which can be
written as k = K   with  (x) = x for 2 = R and K(a  b) = exp 1
2 (a  b)2. Then
MMD(0  ✓) =⇢p2
k (0  ✓) = 21  exp 1
D 
MMD(0  1/n) = p2 6! 0  even though 1/n W! 0  shows that the Optimized MMD
k (0  ✓). Some sequences following v (e.g. A)

Considering D 
distance is not continuous in the weak or Wasserstein topologies.
This also causes optimization issues. Figure 1 (a) shows gradient vector ﬁelds in parameter space 
v(✓  ) /   r✓ MMD2
converge to an optimal solution (0  )  but some (B) move in the wrong direction  and others (C) are
stuck because there is essentially no gradient. Figure 1 (c  red) shows that the optimal D 
MMD critic
is very sharp near P and Q; this is less true for cases where the algorithm converged.
We can avoid these issues if we ensure a bounded Lipschitz critic:3
Proposition 2. Assume the critics f (x) = (EX⇠P k (X  x)  EY ⇠Q k (Y  x))/ MMDk (P  Q)
are uniformly bounded and have a common Lipschitz constant: supx2X   2 |f (x)| < 1 and
sup 2 kf kLip < 1. In particular  this holds when k = K   and

k (0  ✓) r MMD2

2 2✓2  

✓ 6= 0
✓ = 0

.

0

sup
a2Rs
Then D 

K(a  a) < 1 
MMD is continuous in the weak topology: if Pn

kK(a ·)  K(b ·)kHK  LKka  bkRs 

D! P  then D 

sup

 2 k kLip  L < 1.
MMD(Pn  P) ! 0.

2 f #P denotes the pushforward of a distribution: if X ⇠ P  then f (X) ⇠ f #P.
3[27  Theorem 4] makes a similar claim to Proposition 2  but its proof was incorrect: it tries to uniformly

bound MMDk W 2  but the bound used is for a Wasserstein in terms of kk (x ·)  k (y ·)kHk 

.

3

Figure 1: The setting of Example 1. (a  b): parameter-space gradient ﬁelds for the MMD and the
SMMD (Section 3.3); the horizontal axis is ✓  and the vertical 1/ . (c): optimal MMD critics for
✓ = 20 with different kernels. (d): the MMD and the distances of Section 3 optimized over .

Proof. The main result is [12  Corollary 11.3.4]. To show the claim for k = K     note that
|f (x)  f (y)| k f kHk kk (x ·)  k (y ·)kHk 

  which since kf kHk 

= 1 is

kK( (x) ·)  K( (y) ·)kHK  LKk (x)   (y)kRs  LKLkx  ykRd.

Indeed  if we put a box constraint on [27] or regularize the gradient of the critic function [7] 
the resulting MMD GAN generally matches or outperforms WGAN-based models. Unfortunately 
though  an additive gradient penalty doesn’t substantially change the vector ﬁeld of Figure 1 (a)  as
shown in Figure 5 (Appendix B). We will propose distances with much better convergence behavior.

3 New discrepancies for learning implicit generative models

Our aim here is to introduce a discrepancy that can provide useful gradient information when used as
an IGM loss. Proofs of results in this section are deferred to Appendix A.

3.1 Lipschitz Maximum Mean Discrepancy
Proposition 2 shows that an MMD-like discrepancy can be continuous under the weak topology even
when optimizing over kernels  if we directly restrict the critic functions to be Lipschitz. We can easily
deﬁne such a distance  which we call the Lipschitz MMD: for some > 0 

LipMMDk (P  Q) :=

f2Hk : kfk2

sup
Lip+kfk2

Hk1

EX⇠P [f (X)]  EY ⇠Q [f (Y )] .

(4)

For a universal kernel k  we conjecture that lim!0 LipMMDk (P  Q) !W (P  Q). But for any k
and   LipMMD is upper-bounded by W  as (4) optimizes over a smaller set of functions than (2).
Thus D  
LipMMD(P  Q) := sup 2 LipMMDk  (P  Q) is also upper-bounded by W  and hence is
continuous in the Wasserstein topology. It also shows excellent empirical behavior on Example 1
(Figure 1 (d)  and Figure 5 in Appendix B). But estimating LipMMDk   let alone D  
LipMMD  is in
general extremely difﬁcult (Appendix D)  as ﬁnding kfkLip requires optimization in the input space.
Constraining the mean gradient rather than the maximum  as we will do next  is far more tractable.

4

0246810120246ABC(D) : VeFtor )LeOG of 00D-GA10246810120246ABC(b) : VeFtor )LeOG of 600D-GA1−50510152025−0.50−0.250.000.250.50(F) : 2StLmDO CrLtLFψ 10.0ψ 1.0ψ 0.25−4−2024θ0.00.51.01.5(G) : 2StLmLzeG DLVtDnFeV00D600DGC00DLLS00D3.2 Gradient-Constrained Maximum Mean Discrepancy
We deﬁne the Gradient-Constrained MMD for > 0 and using some measure µ as

(5)

(6)

GCMMDµ k (P  Q) :=

sup

where kfk2

S(µ) k  := kfk2

f2Hk : kfkS(µ) k 1
L2(µ) + krfk2

EX⇠P [f (X)]  EY ⇠Q [f (Y )]  
L2(µ) + kfk2

Hk .

L2(µ) =R k·k2 µ(dx) denotes the squared L2 norm. Rather than directly constraining the Lipschitz
k·k2
constant  the second term krfk2
L2(µ) encourages the function f to be ﬂat where µ has mass. In
experiments we use µ = P  ﬂattening the critic near the target sample. We add the ﬁrst term following
[10]: in one dimension and with µ uniform  k·kS(µ) · 0 is then an RKHS norm with the kernel
(x  y) = exp(kx  yk)  which is also a Sobolev space. The correspondence to a Sobolev norm is
lost in higher dimensions [53  Ch. 10]  but we also found the ﬁrst term to be beneﬁcial in practice.
We can exploit some properties of Hk to compute (5) analytically. Call the difference in kernel mean
embeddings ⌘ := EX⇠P[k(X ·)]  EY ⇠Q[k(Y ·)] 2H k; recall MMD(P  Q) = k⌘kHk.
Proposition 3. Let ˆµ =PM
m=1 Xm. Deﬁne ⌘(X) 2 RM with mth entry ⌘(Xm)  and r⌘(X) 2
RM d with (m  i)th entry4 @i⌘(Xm). Then under Assumptions (A) to (D) in Appendix A.1 

GCMMD2

ˆµ k (P  Q) =

1

MMD2(P  Q)  ¯P (⌘)
r⌘(X)T✓K GT

¯P (⌘) = ⌘(X)

G H + MI M +M d◆1 ⌘(X)
r⌘(X)  

where K is the kernel matrix Km m0 = k(Xm  Xm0)  G is the matrix of left derivatives 5 G(m i) m0 =
@ik(Xm  Xm0)  and H that of derivatives of both arguments H(m i) (m0 j) = @i@j+dk(Xm  Xm0).
As long as P and Q have integrable ﬁrst moments  and µ has second moments  Assumptions (A)
to (D) are satisﬁed e.g. by a Gaussian or linear kernel on top of a differentiable  . We can thus
estimate the GCMMD based on samples from P  Q  and µ by using the empirical mean ˆ⌘ for ⌘.
This discrepancy indeed works well in practice: Appendix F.2 shows that optimizing our estimate
of Dµ   
GCMMD = sup 2 GCMMDµ k   yields a good generative model on MNIST. But the linear
system of size M + M d is impractical: even on 28 ⇥ 28 images and using a low-rank approximation 
the model took days to converge. We therefore design a less expensive discrepancy in the next section.
The GCMMD is related to some discrepancies previously used in IGM training. The Fisher GAN [34]
uses only the variance constraint kfk2
L2(µ)  1 
along with a vanishing boundary condition on f to ensure a well-deﬁned solution (although this was
not used in the implementation  and can cause very unintuitive critic behavior; see Appendix C).
The authors considered several choices of µ  including the WGAN-GP measure [18] and mixtures
(P + Q✓) /2. Rather than enforcing the constraints in closed form as we do  though  these models
used additive regularization. We will compare to the Sobolev GAN in experiments.

L2(µ)  1. The Sobolev GAN [33] constrains krfk2

3.3 Scaled Maximum Mean Discrepancy
We will now derive a lower bound on the Gradient-Constrained MMD which retains many of its
attractive qualities but can be estimated in time linear in the dimension d.
Proposition 4. Make Assumptions (A) to (D). For any f 2H k  kfkS(µ) k   1

µ k kfkHk  where

µ k  := 1.vuut +Z k(x  x)µ(dx) +

dXi=1Z @2k(y  z)

@yi@zi

(y z)=(x x)

µ(dx).

We then deﬁne the Scaled Maximum Mean Discrepancy based on this bound of Proposition 4:

SMMDµ k (P  Q) :=

sup

f : 1

µ k kfkH1

EX⇠P [f (X)]EY ⇠Q [f (Y )] = µ k  MMDk(P  Q). (7)

4We use (m  i) to denote (m  1)d + i; thus r⌘(X) stacks r⌘(X1)  . . .   r⌘(XM ) into one vector.
5We use @ik(x  y) to denote the partial derivative with respect to xi  and @i+dk(x  y) that for yi.

5

k µ  =  + Eµ⇥k (X)k2 + kr (X)k2

Because the constraint in the optimization of (7) is more restrictive than in that of (5)  we have
that SMMDµ k (P  Q)  GCMMDµ k (P  Q). The Sobolev norm kfkS(µ)   and a fortiori the
gradient norm under µ  is thus also controlled for the SMMD critic. We also show in Appendix F.1
that SMMDµ k  behaves similarly to GCMMDµ k  on Gaussians.
k µ  =  + g(0) + 2|g0(0)| Eµ⇥kr (X)k2
F⇤.
If k = K   and K(a  b) = g(ka bk2)  then 2
F⇤. Estimating
Or if K is linear  K(a  b) = aTb  then 2
these terms based on samples from µ is straightforward  giving a natural estimator for the SMMD.
Of course  if µ and k are ﬁxed  the SMMD is simply a constant times the MMD  and so behaves
in essentially the same way as the MMD. But optimizing the SMMD over a kernel family  
Dµ   
SMMD(P  Q) := sup 2 SMMDµ k  (P  Q)  gives a distance very different from D 
Figure 1 (b) shows the vector ﬁeld for the Optimized SMMD loss in Example 1  using the WGAN-
GP measure µ = Uniform(0 ✓ ). The optimization surface is far more amenable: in particular
the location C  which formerly had an extremely small gradient that made learning effectively
impossible  now converges very quickly by ﬁrst reducing the critic gradient until some signal is
available. Figure 1 (d) demonstrates that Dµ   
LipMMD but in sharp contrast
to D 
We can establish that Dµ   
Theorem 1. Let k = K     with  : X! Rs a fully-connected L-layer network with
Leaky-ReLU↵ activations whose layers do not increase in width  and K satisfying mild smoothness
conditions QK < 1 (Assumptions (II) to (V) in Appendix A.2). Let  be the set of parameters where
each layer’s weight matrices have condition number cond(W l) = kW lk/ min(W l)  < 1. If µ
has a density (Assumption (I))  then
Dµ   
SMMD (P  Q) 

MMD  is continuous with respect to the location ✓ and provides a strong gradient towards 0.
SMMD is continuous in the Wasserstein topology under some conditions:

SMMD  like Dµ   

GCMMD and D  

MMD (3).

QKL/2
pdL↵L/2 W(P  Q).

SMMD (Pn  P) ! 0  even if µ is chosen to depend on P and Q.

Thus if Pn W! P  then Dµ   
Uniform bounds vs bounds in expectation Controlling krf k2
L2(µ) = Eµkrf (X)k2 does
not necessarily imply a bound on kfkLip  supx2Xkrf (X)k  and so does not in general give
continuity via Proposition 2. Theorem 1 implies that when the network’s weights are well-conditioned 
it is sufﬁcient to only control krf k2
L2(µ)  which is far easier in practice than controlling kfkLip.
If we instead tried to directly controlled kfkLip with e.g. spectral normalization (SN) [32]  we
could signiﬁcantly reduce the expressiveness of the parametric family. In Example 1  constraining
k kLip = 1 limits us to only = {1}. Thus D{1}MMD is simply the MMD with an RBF kernel
of bandwidth 1  which has poor gradients when ✓ is far from 0 (Figure 1 (c)  blue). The Cauchy-
Schwartz bound of Proposition 4 allows jointly adjusting the smoothness of k and the critic f  while
SN must control the two independently. Relatedly  limiting kkLip by limiting the Lipschitz norm of
each layer could substantially reduce capacity  while krf kL2(µ) need not be decomposed by layer.
Another advantage is that µ provides a data-dependent measure of complexity as in [10]: we do not
needlessly prevent ourselves from using critics that behave poorly only far from the data.

Spectral parametrization When the generator is near a local optimum  the critic might identify
only one direction on which Q✓ and P differ. If the generator parameterization is such that there
is no local way for the generator to correct it  the critic may begin to single-mindedly focus on
this difference  choosing redundant convolutional ﬁlters and causing the condition number of the
weights to diverge. If this occurs  the generator will be motivated to ﬁx this single direction while
ignoring all other aspects of the distributions  after which it may become stuck. We can help avoid
this collapse by using a critic parameterization that encourages diverse ﬁlters with higher-rank weight
matrices. Miyato et al. [32] propose to parameterize the weight matrices as W =  ¯W /k ¯Wkop 
where k ¯Wkop is the spectral norm of ¯W . This parametrization works particularly well with Dµ   
SMMD;
Figure 2 (b) shows the singular values of the second layer of a critic’s network (and Figure 9  in
Appendix F.3  shows more layers)  while Figure 2 (d) shows the evolution of the condition number
during training. The conditioning of the weight matrix remains stable throughout training with
spectral parametrization  while it worsens through training in the default case.

6

4 Experiments

We evaluated unsupervised image generation on three datasets: CIFAR-10 [26] (60 000 images 
32 ⇥ 32)  CelebA [29] (202 599 face images  resized and cropped to 160 ⇥ 160 as in [7])  and the
more challenging ILSVRC2012 (ImageNet) dataset [41] (1 281 167 images  resized to 64 ⇥ 64).
Code for all of these experiments is available at github.com/MichaelArbel/Scaled-MMD-GAN.
Losses All models are based on a scalar-output critic network  : X! R  except MMDGAN-GP
where  : X! R16 as in [7]. The WGAN and Sobolev GAN use a critic f =    while the
GAN uses a discriminator D (x) = 1/(1 + exp( (x))). The MMD-based methods use a kernel
k (x  y) = exp(( (x)   (y))2/2)  except for MMDGAN-GP which uses a mixture of RQ
kernels as in [7]. Increasing the output dimension of the critic or using a different kernel didn’t
substantially change the performance of our proposed method. We also consider SMMD with a linear
top-level kernel  k(x  y) =  (x) (y); because this becomes essentially identical to a WGAN
(Appendix E)  we refer to this method as SWGAN. SMMD and SWGAN use µ = P; Sobolev GAN
uses µ = (P + Q)/2 as in [33]. We choose  and an overall scaling to obtain the losses:

SMMD:

\MMD

2
k (P  Q✓)

1 + 10 EˆP [kr (X)k2
F ]

  SWGAN:

EˆP [ (X)]  EˆQ✓

[ (X)]

q1 + 10EˆP [| (X)|2] + EˆP [kr (X)k2
F ] .

Architecture For CIFAR-10  we used the CNN architecture proposed by [32] with a 7-layer critic
and a 4-layer generator. For CelebA  we used a 5-layer DCGAN discriminator and a 10-layer ResNet
generator as in [7]. For ImageNet  we used a 10-layer ResNet for both the generator and discriminator.
In all experiments we used 64 ﬁlters for the smallest convolutional layer  and double it at each layer
(CelebA/ImageNet) or every other layer (CIFAR-10). The input codes for the generator are drawn

from Uniform[1  1]128. We consider two parameterizations for each critic: a standard one where

the parameters can take any real value  and a spectral parametrization (denoted SN-) as above [32].
Models without explicit gradient control (SN-GAN  SN-MMDGAN  SN-MMGAN-L2  SN-WGAN)
ﬁx  = 1  for spectral normalization; others learn   using a spectral parameterization.
Training All models were trained for 150 000 generator updates on a single GPU  except for ImageNet
where the model was trained on 3 GPUs simultaneously. To limit communication overhead we
averaged the MMD estimate on each GPU  giving the block MMD estimator [54]. We always used
64 samples per GPU from each of P and Q  and 5 critic updates per generator step. We used initial
learning rates of 0.0001 for CIFAR-10 and CelebA  0.0002 for ImageNet  and decayed these rates
using the KID adaptive scheme of [7]: every 2 000 steps  generator samples are compared to those
from 20 000 steps ago  and if the relative KID test [9] fails to show an improvement three consecutive
times  the learning rate is decayed by 0.8. We used the Adam optimizer [25] with 1 = 0.5  2 = 0.9.
Evaluation To compare the sample quality of different models  we considered three different scores
based on the Inception network [49] trained for ImageNet classiﬁcation  all using default parameters
in the implementation of [7]. The Inception Score (IS) [42] is based on the entropy of predicted
labels; higher values are better. Though standard  this metric has many issues  particularly on datasets
other than ImageNet [4  7  20]. The FID [20] instead measures the similarity of samples from the
generator and the target as the Wasserstein-2 distance between Gaussians ﬁt to their intermediate
representations. It is more sensible than the IS and becoming standard  but its estimator is strongly
biased [7]. The KID [7] is similar to FID  but by using a polynomial-kernel MMD its estimates enjoy
better statistical properties and are easier to compare. (A similar score was recommended by [21].)
Results Table 1a presents the scores for models trained on both CIFAR-10 and CelebA datasets. On
CIFAR-10  SN-SWGAN and SN-SMMDGAN performed comparably to SN-GAN. But on CelebA 
SN-SWGAN and SN-SMMDGAN dramatically outperformed the other methods with the same
architecture in all three metrics. It also trained faster  and consistently outperformed other methods
over multiple initializations (Figure 2 (a)). It is worth noting that SN-SWGAN far outperformed
WGAN-GP on both datasets. Table 1b presents the scores for SMMDGAN and SN-SMMDGAN
trained on ImageNet  and the scores of pre-trained models using BGAN [6] and SN-GAN [32].6 The
6These models are courtesy of the respective authors and also trained at 64 ⇥ 64 resolution. SN-GAN used
the same architecture as our model  but trained for 250 000 generator iterations; BS-GAN used a similar 5-layer
ResNet architecture and trained for 74 epochs  comparable to SN-GAN.

7

Figure 2: The training process on CelebA. (a) KID scores. We report a ﬁnal score for SN-GAN
slightly before its sudden failure mode; MMDGAN and SN-MMDGAN were unstable and had scores
around 100. (b) Singular values of the second layer  both early (dashed) and late (solid) in training.
(c) 2
µ k  for several MMD-based methods. (d) The condition number in the ﬁrst layer through
training. SN alone does not control µ k   and SMMD alone does not control the condition number.

(a) Scaled MMD GAN with SN

(b) SN-GAN

(c) Boundary Seeking GAN

(d) Scaled MMD GAN with SN
(f) MMD GAN with GP+L2
Figure 3: Samples from various models. Top: 64 ⇥ 64 ImageNet; bottom: 160 ⇥ 160 CelebA.

(e) Scaled WGAN with SN

8

0246810generDWor LWerDWLonV×104101520253035(D) : .ID×103 600DGA161-600DGA100DGA1-G3-L26obolev-GA161-GA1:GA1-G361-6:GA1020406080100120itK VLngulDr vDlue0.20.40.60.81.0σi(b) : 6LngulDr VDlueV: LDyer 2600DGA1 : 10K LWerDWLonV600DGA1 : 150K LWerDWLonV61-600DGA1 : 10K LWerDWLonV61-600DGA1 : 150K LWerDWLonV0123456generDWor LWerDWLonV×104101102103104(c) : CrLWLc CoPSlexLWy600DGA161-600DGA100DGA161-00DGA10123456generDWor LWerDWLonV×104100200300400500600700(G) : ConGLWLon 1uPber: LDyer 1600DGA161-600DGA100DGA161-00DGA1Table 1: Mean (standard deviation) of score estimates  based on 50 000 samples from each model.

Method
WGAN-GP
MMDGAN-GP-L2
Sobolev-GAN
SMMDGAN
SN-GAN
SN-SWGAN
SN-SMMDGAN

(a) CIFAR-10 and CelebA.

CIFAR-10
IS
6.9±0.2
6.9±0.1
7.0±0.1
7.0±0.1
7.2±0.1
7.2±0.1
7.3±0.1

FID
31.1±0.2
31.4±0.3
30.3±0.3
31.5±0.4
26.7±0.2
28.5±0.2
25.0±0.3

KID⇥103
22.2±1.1
23.3±1.1
22.3±1.2
22.2±1.1
16.1±0.9
17.6±1.1
16.6±2.0

CelebA
IS
2.7±0.0
2.6±0.0
2.9±0.0
2.7±0.0
2.7±0.0
2.8±0.0
2.8±0.0

(b) ImageNet.

FID
29.2±0.2
20.5±0.2
16.4±0.1
18.4±0.2
22.6±0.1
14.1±0.2
12.4±0.2

KID⇥103
22.0±1.0
13.0±1.0
10.6±0.5
11.5±0.8
14.6±1.1
7.7±0.5
6.1±0.4

Method
IS
10.7±0.4
BGAN
11.2±0.1
SN-GAN
SMMDGAN
10.7±0.2
SN-SMMDGAN 10.9±0.1

FID
43.9±0.3
47.5±0.1
38.4±0.3
36.6±0.2

KID⇥103
47.0±1.1
44.4±2.2
39.3±2.5
34.6±1.6

proposed methods substantially outperformed both methods in FID and KID scores. Figure 3 shows
samples on ImageNet and CelebA; Appendix F.4 has more.
Spectrally normalized WGANs / MMDGANs To control for the contribution of the spectral
parametrization to the performance  we evaluated variants of MMDGANs  WGANs and Sobolev-
GAN using spectral normalization (in Table 2  Appendix F.3). WGAN and Sobolev-GAN led to
unstable training and didn’t converge at all (Figure 11) despite many attempts. MMDGAN converged
on CIFAR-10 (Figure 11) but was unstable on CelebA (Figure 10). The gradient control due to SN
is thus probably too loose for these methods. This is reinforced by Figure 2 (c)  which shows that
the expected gradient of the critic network is much better-controlled by SMMD  even when SN is
used. We also considered variants of these models with a learned  while also adding a gradient
penalty and an L2 penalty on critic activations [7  footnote 19]. These generally behaved similarly to
MMDGAN  and didn’t lead to substantial improvements. We ran the same experiments on CelebA 
but aborted the runs early when it became clear that training was not successful.
Rank collapse We occasionally observed the failure mode for SMMD where the critic becomes
low-rank  discussed in Section 3.3  especially on CelebA; this failure was obvious even in the training
objective. Figure 2 (b) is one of these examples. Spectral parametrization seemed to prevent this
behavior. We also found one could avoid collapse by reverting to an earlier checkpoint and increasing
the RKHS regularization parameter   but did not do this for any of the experiments here.

5 Conclusion

We studied gradient regularization for MMD-based critics in implicit generative models  clarifying
MMD loss. Based on these insights  we proposed the Gradient-
how previous techniques relate to the D 
Constrained MMD and its approximation the Scaled MMD  a new loss function for IGMs that
controls gradient behavior in a principled way and obtains excellent performance in practice.
One interesting area of future study for these distances is their behavior when used to diffuse particles
distributed as Q towards particles distributed as P. Mroueh et al. [33  Appendix A.1] began such a
study for the Sobolev GAN loss; [35] proved convergence and studied discrete-time approximations.
Another area to explore is the geometry of these losses  as studied by Bottou et al. [8]  who showed
potential advantages of the Wasserstein geometry over the MMD. Their results  though  do not
address any distances based on optimized kernels; the new distances introduced here might have
interesting geometry of their own.

9

References

[1] B. Amos and J. Z. Kolter. “OptNet: Differentiable Optimization as a Layer in Neural Net-

[2] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

[3] M. Arjovsky  S. Chintala  and L. Bottou. “Wasserstein Generative Adversarial Networks.”

works.” ICML. 2017. arXiv: 1703.00443.

Networks.” ICLR. 2017. arXiv: 1701.04862.

ICML. 2017. arXiv: 1701.07875.

[4] S. Barratt and R. Sharma. A Note on the Inception Score. 2018. arXiv: 1801.01973.
[5] M. G. Bellemare  I. Danihelka  W. Dabney  S. Mohamed  B. Lakshminarayanan  S. Hoyer 
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[6] D. Berthelot  T. Schumm  and L. Metz. BEGAN: Boundary Equilibrium Generative Adversarial

[7] M. Bi´nkowski  D. J. Sutherland  M. Arbel  and A. Gretton. “Demystifying MMD GANs.”

Networks. 2017. arXiv: 1703.10717.

ICLR. 2018. arXiv: 1801.01401.

[8] L. Bottou  M. Arjovsky  D. Lopez-Paz  and M. Oquab. “Geometrical Insights for Implicit
Generative Modeling.” Braverman Readings in Machine Learning: Key Iedas from Inception
to Current State. Ed. by L. Rozonoer  B. Mirkin  and I. Muchnik. LNAI Vol. 11100. Springer 
2018  pp. 229–268. arXiv: 1712.07822.

[9] W. Bounliphone  E. Belilovsky  M. B. Blaschko  I. Antonoglou  and A. Gretton. “A Test of
Relative Similarity For Model Selection in Generative Models.” ICLR. 2016. arXiv: 1511.
04581.

[10] O. Bousquet  O. Chapelle  and M. Hein. “Measure Based Regularization.” NIPS. 2004.
[11] A. Brock  T. Lim  J. M. Ritchie  and N. Weston. “Neural Photo Editing with Introspective

Adversarial Networks.” ICLR. 2017. arXiv: 1609.07093.

[12] R. M. Dudley. Real Analysis and Probability. 2nd ed. Cambridge University Press  2002.
[13] G. K. Dziugaite  D. M. Roy  and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” UAI. 2015. arXiv: 1505.03906.

[14] A. Genevay  G. Peyré  and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” AISTATS. 2018. arXiv: 1706.00292.

[15] T. Gneiting and A. E. Raftery. “Strictly proper scoring rules  prediction  and estimation.” JASA

102.477 (2007)  pp. 359–378.
I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville 
and Y. Bengio. “Generative Adversarial Nets.” NIPS. 2014. arXiv: 1406.2661.

[16]

[17] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. J. Smola. “A Kernel Two-

Sample Test.” JMLR 13 (2012).
I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. Courville. “Improved Training of
Wasserstein GANs.” NIPS. 2017. arXiv: 1704.00028.

[19] A. Güngör. “Some bounds for the product of singular values.” International Journal of

[18]

Contemporary Mathematical Sciences (2007).

[20] M. Heusel  H. Ramsauer  T. Unterthiner  B. Nessler  G. Klambauer  and S. Hochreiter. “GANs
Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.” NIPS. 2017.
arXiv: 1706.08500.

[21] G. Huang  Y. Yuan  Q. Xu  C. Guo  Y. Sun  F. Wu  and K. Weinberger. An empirical study on

evaluation metrics of generative adversarial networks. 2018. arXiv: 1806.07755.

[22] X. Huang  M.-Y. Liu  S. Belongie  and J. Kautz. “Multimodal Unsupervised Image-to-Image

Translation.” ECCV. 2018. arXiv: 1804.04732.

[23] Y. Jin  K. Zhang  M. Li  Y. Tian  H. Zhu  and Z. Fang. Towards the Automatic Anime Characters

Creation with Generative Adversarial Networks. 2017. arXiv: 1708.05509.

[24] T. Karras  T. Aila  S. Laine  and J. Lehtinen. “Progressive Growing of GANs for Improved

Quality  Stability  and Variation.” ICLR. 2018. arXiv: 1710.10196.

[25] D. Kingma and J. Ba. “Adam: A Method for Stochastic Optimization.” ICLR. 2015. arXiv:

1412.6980.

[26] A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. 2009.

10

[27] C.-L. Li  W.-C. Chang  Y. Cheng  Y. Yang  and B. Póczos. “MMD GAN: Towards Deeper

Understanding of Moment Matching Network.” NIPS. 2017. arXiv: 1705.08584.

[28] Y. Li  K. Swersky  and R. Zemel. “Generative Moment Matching Networks.” ICML. 2015.

[29] Z. Liu  P. Luo  X. Wang  and X. Tang. “Deep learning face attributes in the wild.” ICCV. 2015.

[30] L. Mescheder  A. Geiger  and S. Nowozin. “Which Training Methods for GANs do actually

Converge?” ICML. 2018. arXiv: 1801.04406.

[31] P. Milgrom and I. Segal. “Envelope theorems for arbitrary choice sets.” Econometrica 70.2

arXiv: 1502.02761.

arXiv: 1411.7766.

(2002)  pp. 583–601.

[32] T. Miyato  T. Kataoka  M. Koyama  and Y. Yoshida. “Spectral Normalization for Generative

Adversarial Networks.” ICLR. 2018. arXiv: 1802.05927.

[33] Y. Mroueh  C.-L. Li  T. Sercu  A. Raj  and Y. Cheng. “Sobolev GAN.” ICLR. 2018. arXiv:

1711.04894.

[34] Y. Mroueh and T. Sercu. “Fisher GAN.” NIPS. 2017. arXiv: 1705.09675.
[35] Y. Mroueh  T. Sercu  and A. Raj. Regularized Kernel and Neural Sobolev Descent: Dynamic

MMD Transport. 2018. arXiv: 1805.12062.

[36] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” Advances

in Applied Probability 29.2 (1997)  pp. 429–443.

[37] S. Nowozin  B. Cseke  and R. Tomioka. “f-GAN: Training Generative Neural Samplers using

Variational Divergence Minimization.” NIPS. 2016. arXiv: 1606.00709.

[38] A. Radford  L. Metz  and S. Chintala. “Unsupervised Representation Learning with Deep

Convolutional Generative Adversarial Networks.” ICLR. 2016. arXiv: 1511.06434.
J. R. Retherford. “Review: J. Diestel and J. J. Uhl  Jr.  Vector measures.” Bull. Amer. Math.
Soc. 84.4 (July 1978)  pp. 681–685.

[39]

[40] K. Roth  A. Lucchi  S. Nowozin  and T. Hofmann. “Stabilizing Training of Generative Adver-

sarial Networks through Regularization.” NIPS. 2017. arXiv: 1705.09367.

[41] O. Russakovsky et al. ImageNet Large Scale Visual Recognition Challenge. 2014. arXiv:

1409.0575.

[42] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. “Improved

Techniques for Training GANs.” NIPS. 2016. arXiv: 1606.03498.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge University
Press  2004.

[43]

[44] B. K. Sriperumbudur  K. Fukumizu  A. Gretton  G. R. G. Lanckriet  and B. Schölkopf. “Kernel

choice and classiﬁability for RKHS embeddings of probability distributions.” NIPS. 2009.

[45] B. K. Sriperumbudur  K. Fukumizu  and G. R. G. Lanckriet. “Universality  Characteristic
Kernels and RKHS Embedding of Measures.” JMLR 12 (2011)  pp. 2389–2410. arXiv: 1003.
0887.

[46] B. Sriperumbudur. “On the optimal estimation of probability mesaures in weak and strong

topologies.” Bernoulli 22.3 (2016)  pp. 1839–1893. arXiv: 1310.8240.
I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.

[47]
[48] D. J. Sutherland  H.-Y. Tung  H. Strathmann  S. De  A. Ramdas  A. Smola  and A. Gretton.
“Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy.” ICLR.
2017. arXiv: 1611.04488.

[49] C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. “Rethinking the Inception

Architecture for Computer Vision.” CVPR. 2016. arXiv: 1512.00567.

[50] T. Unterthiner  B. Nessler  C. Seward  G. Klambauer  M. Heusel  H. Ramsauer  and S. Hochre-
iter. “Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields.” ICLR. 2018.
arXiv: 1708.08819.

[51] C. Villani. Optimal Transport: Old and New. Springer  2009.
[52]

J. Weed and F. Bach. “Sharp asymptotic and ﬁnite-sample rates of convergence of empirical
measures in Wasserstein distance.” Bernoulli (forthcoming). arXiv: 1707.00087.
[53] H. Wendland. Scattered Data Approximation. Cambridge University Press  2005.
[54] W. Zaremba  A. Gretton  and M. B. Blaschko. “B-tests: Low Variance Kernel Two-Sample

Tests.” NIPS. 2013. arXiv: 1307.1954.

11

,Michael Arbel
Dougal Sutherland
Mikołaj Bińkowski
Arthur Gretton