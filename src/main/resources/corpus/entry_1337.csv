2013,Deep Fisher Networks for Large-Scale Image Classification,As massively parallel computations have become broadly available with modern GPUs  deep architectures trained on very large datasets have risen in popularity. Discriminatively trained convolutional neural networks  in particular  were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as ImageNet. However  elements of these architectures are similar to standard hand-crafted representations used in computer vision. In this paper  we explore the extent of this analogy  proposing a version of the state-of-the-art Fisher vector image encoding that can be stacked in multiple layers. This architecture significantly improves on standard Fisher vectors  and obtains competitive results with deep convolutional networks at a significantly smaller computational cost. Our hybrid architecture allows us to measure the performance improvement brought by a deeper image classification pipeline  while staying in the realms of conventional SIFT features and FV encodings.,Deep Fisher Networks

for Large-Scale Image Classiﬁcation

Karen Simonyan

Andrea Vedaldi

Visual Geometry Group  University of Oxford
{karen vedaldi az}@robots.ox.ac.uk

Andrew Zisserman

Abstract

As massively parallel computations have become broadly available with modern
GPUs  deep architectures trained on very large datasets have risen in popular-
ity. Discriminatively trained convolutional neural networks  in particular  were
recently shown to yield state-of-the-art performance in challenging image classi-
ﬁcation benchmarks such as ImageNet. However  elements of these architectures
are similar to standard hand-crafted representations used in computer vision. In
this paper  we explore the extent of this analogy  proposing a version of the state-
of-the-art Fisher vector image encoding that can be stacked in multiple layers.
This architecture signiﬁcantly improves on standard Fisher vectors  and obtains
competitive results with deep convolutional networks at a smaller computational
learning cost. Our hybrid architecture allows us to assess how the performance of
a conventional hand-crafted image classiﬁcation pipeline changes with increased
depth. We also show that convolutional networks and Fisher vector encodings are
complementary in the sense that their combination further improves the accuracy.

Introduction

1
Discriminatively trained deep convolutional neural networks (CNN) [18] have recently achieved im-
pressive state of the art results over a number of areas  including  in particular  the visual recognition
of categories in the ImageNet Large-Scale Visual Recognition Challenge [4]. This success is built
on many years of tuning and incorporating ideas into CNNs in order to improve their performance.
Many of the key ideas in CNN have now been absorbed into features proposed in the computer vision
literature – some have been discovered independently and others have been overtly borrowed. For
example: the importance of whitening [11]; max pooling and sparse coding [26  33]; non-linearity
and normalization [20]. Indeed  several standard features and pipelines in computer vision  such as
SIFT [19] and a spatial pyramid on Bag of visual Words (BoW) [16] can be seen as corresponding
to layers of a standard CNN. However  image classiﬁcation pipelines used in the computer vision
literature are still generally quite shallow: either a global feature vector is computed over an im-
age  and used directly for classiﬁcation; or  in a few cases  a two layer hierarchy is used  where the
outputs of a number of classiﬁers form the global feature vector for the image (e.g. attributes and
classemes [15  30]).
The question we address in this paper is whether it is possible to improve the performance of off-the-
shelf computer vision features by organising them into a deeper architecture. To this end we make
the following contributions: (i) we introduce a Fisher Vector Layer  which is a generalization of the
standard FV to a layer architecture suitable for stacking; (ii) we demonstrate that by discriminatively
training several such layers and stacking them into a Fisher Vector Network  an accuracy competitive
with the deep CNN can be achieved  whilst staying in the realms of conventional SIFT and colour
features and FV encodings; and (iii) we show that class posteriors  computed by the deep CNN and
FV  are complementary and can be combined to signiﬁcantly improve the accuracy.
The rest of the paper is organised as follows. After a discussion of the related work  we begin
with a brief description of the conventional FV encoding [20] (Sect. 2). We then show how this

1

representation can be modiﬁed to be used as a layer in a deeper architecture (Sect. 3) and how
the latter can be discriminatively learnt to yield a deep Fisher network (Sect. 4). After discussing
important details of the implementation (Sect. 5)  we evaluate our architecture on the ImageNet
image classiﬁcation benchmark (Sect. 6).
Related work. There is a vast literature on large-scale image classiﬁcation  which we brieﬂy review
here. One widely used approach is to extract local features such as SIFT [19] densely from each im-
age  aggregate and encode them as high-dimensional vectors  and feed the latter to a classiﬁer  e.g.
an SVM. There exists a large variety of different encodings that can be used for this purpose  includ-
ing the BoW [9  29] encoding  sparse coding [33]  and the FV encoding [20]. Since FV was shown
to outperform other encodings [6] and achieve very good performance on various image recognition
benchmarks [21  28]  we use it as the basis of our framework. We note that other recently proposed
encodings (e.g. [5]) can be readily employed in the place of FV. Most encodings are designed to dis-
regard the spatial location of features in order to be invariant to image transformations; in practice 
however  retaining weak spatial information yields an improved classiﬁcation performance. This
can be incorporated by dividing the image into regions  encoding each of them individually  and
stacking the result in a composite higher-dimensional code  known as a spatial pyramid [16]. The
alternative  which does not increase the encoding dimensionality  is to augment the local features
with their spatial coordinates [24].
Another vast family of image classiﬁcation techniques is based on Deep Neural Networks (DNN) 
which are inspired by the layered structure of the visual cortex in mammals [22]. DNNs can be
trained greedily  in a layer-by-layer manner  as in Restricted Boltzmann Machines [12] and (sparse)
auto-encoders [3  17]  or by learning all layers simultaneously  which is relatively efﬁcient if the
layers are convolutional [18]. In particular  the advent of massively-parallel GPUs has recently made
it possible to train deep convolutional networks on a large scale with excellent performance [7  14].
It was also shown that techniques such as training and test data augmentation  as well as averaging
the outputs of independently trained DNNs  can signiﬁcantly improve the accuracy.
There have been attempts to bridge these two families  exploring the trade-offs between network
depth and width  as well as the complexity of the layers. For instance  dense feature encoding using
the bag of visual words was considered as a single layer of a deep network in [1  8  32].
2 Fisher vector encoding for image classiﬁcation
The Fisher vector encoding φ of a set of features {xp} (e.g. densely computed SIFT features) is
based on ﬁtting a parametric generative model  e.g. the Gaussian Mixture Model (GMM)  to the
features  and then encoding the derivatives of the log-likelihood of the model with respect to its
parameters [13]. In the particular case of GMMs with diagonal covariances  used here  this leads to
the representation which captures the average ﬁrst and second order differences between the features
and each of the GMM centres [20]:

σ2
k

(cid:105)

(cid:104)

N

πk

Φ(1)

k =

  Φ(2)

k =

αk(xp)

K   Φ(2)

K

αk(xp)

1   Φ(2)
Φ(1)

1   . . .   Φ(1)

(1)
Here  {πk  µk  σk}k are the mixture weights  means  and diagonal covariances of the GMM  which is
computed on the training set and used for the description of all images; αk(xp) is the soft assignment
weight of the p-th feature xp to the k-th Gaussian. An FV is obtained by stacking the differences:
. The encoding describes how the distribution of features of a
φ =
particular image differs from the distribution ﬁtted to the features of all training images. To make
the features amenable to the FV description based on the diagonal-covariance GMM  they are ﬁrst
decorrelated by PCA.
The FV dimensionality is 2Kd  where K is the codebook size (the number of Gaussians in the
GMM)  and d is the dimensionality of the encoded feature vector. For instance  FV encoding of
a SIFT feature (d = 128) using a small GMM codebook (K = 256) is 65.5K-dimensional. This
means that high-dimensional feature encodings can be quickly computed using small codebooks.
Using the same codebook size  BoW and sparse coding are only K-dimensional and less discrimina-
tive  as demonstrated in [6]. From another point of view  given the desired encoding dimensionality 
these methods would require 2d-times larger codebooks than needed for FV  which would lead to
impractical computation times.

N(cid:88)

p=1

√
1

(cid:18) xp − µk

(cid:19)

σk

N(cid:88)

p=1

√
1
2πk

N

(cid:18) (xp − µk)2

(cid:19)

− 1

2

Figure 1: Left: Fisher network (Sect. 4) with two Fisher layers. Right: conventional pipeline
using a shallow Fisher vector encoding. As shown in Sect. 6  making the conventional pipeline
slightly deeper by injecting a single Fisher layer substantially improves the classiﬁcation accuracy.

As can be seen from (1)  the (unnormalised) FV encoding is additive with respect to image features 
i.e. the encoding of an image is an average of the individual encodings of its features. Following [20] 
FV performance is further improved by passing it through Signed Square-Rooting (SSR) and L2
normalisation. Finally  the high-dimensional FV is usually coupled with a one-vs-rest linear SVM
classiﬁer  and together they form a conventional image classiﬁcation pipeline [21] (see Fig. 1)  which
serves as a baseline for our classiﬁcation framework.
3 Fisher layer
The conventional FV representation of an image (Sect. 2)  effectively encodes each local feature
(e.g. SIFT) into a high-dimensional representation  and then aggregates these encodings into a single
vector by global sum-pooling over the whole image (followed by normalisation). This means that
the representation describes the image in terms of the local patch features  and can not capture more
complex image structures. Deep neural networks are able to model the feature hierarchies by passing
an output of one feature computation layer as the input to the next one. We adopt a similar approach
here  and devise a feed-forward feature encoding layer (which we term a Fisher layer)  which is
based on off-the-shelf Fisher vector encoding. The layers can then be stacked into a deep network 
which we call a Fisher network.
The architecture of the l-th Fisher layer is depicted in Fig. 2. On the input  it receives dl-dimensional
features (dl ∼ 102)  densely computed over multiple scales on a regular image grid. The features are
assumed to be decorrelated using PCA. The layer then performs feed-forward feature transformation
in three sub-layers.
The ﬁrst one computes semi-local FV encodings by pooling the input features not from the whole
image  but from a dense set of semi-local regions. The resulting FVs form a new set of densely
sampled features that are more discriminative than the input ones and less local  as they integrate
information from larger image areas. The FV encoder (Sect. 2) uses a layer-speciﬁc GMM with Kl
components  so the dimensionality of each FV is 2Kldl  which  considering that FVs are computed
densely  might be too large for practical applications. Therefore  we decrease FV dimensionality
by projection onto hl-dimensional subspace using a discriminatively trained linear projection Wl ∈
Rhl×2Kldl. In practice  this is carried out using an efﬁcient variant of the FV encoder (Sect. 5). In
the second sub-layer  the spatially adjacent features are stacked in a 2 × 2 window  which produces
4hl-dimensional dense feature representation. Finally  the features are L2-normalised and PCA-
projected to dl+1-dimensional subspace using the linear projection Ul ∈ Rdl+1×4hl  and passed as
the input to the (l + 1)-th layer. Each sub-layer is explained in more detail below.

3

Dense feature extraction SIFT  raw patches  … One vs. rest linear SVMs FV encoder Spatial stacking L2 norm. & PCA FV encoder SSR & L2 norm. SSR & L2 norm. input image 0-th layer 1-st Fisher layer  (with optional global  pooling branched out) 2-nd Fisher layer  (global pooling) classifier layer Dense feature extraction  SIFT  raw patches  … One vs rest linear SVMs FV encoder SSR & L2 norm. Figure 2: The architecture of a single Fisher layer. Left: the arrows illustrate the data ﬂow through
the layer; the dimensionality of densely computed features is shown next to the arrows. Right: spatial
pooling (the blue squares) and stacking (the red square) in sub-layers 1 and 2 respectively.

Fisher vector pooling (sub-layer 1). The key idea behind the ﬁrst sub-layer is to aggregate the FVs
of individual features over a family of semi-local spatial neighbourhoods. These neighbourhoods
are overlapping square regions of size ql × ql  sampled every δl pixels (see Fig. 2); compared to the
regions used in global or spatial pyramid pooling [20]  these are smaller and sampled much more
densely. As a result  instead of a single FV  describing the whole image  the image is represented by
a large number of densely computed semi-local FVs  each of which describes a spatially adjacent set
of local features  computed by the previous layer. Thus  the new feature representation can capture
more complex image statistics with larger spatial support. We note that due to additivity  comput-
ing the FV of a spatial neighbourhood corresponds to the sum-pooling over the neighbourhood  a
stage widely used in DNNs. The high dimensionality of Fisher vectors  however  brings up the
computational complexity issue  as storing and processing thousands of dense FVs per image (each
of which is 2Kldl-dimensional) is prohibitive at large scale. We tackle this problem by employing
discriminative dimensionality reduction for high-dimensional FVs  which makes the layer learning
procedure supervised. The dimensionality reduction is carried out using a linear projection Wl onto
an hl-dimensional subspace. As will be shown in Sect. 5  compressed FVs can be computed very
efﬁciently without the need to compute the full-dimensional FVs ﬁrst  and then project them down.
A similar approach (passing the output of a feature encoder to another encoder) has been previously
employed by [1  8  32]  but in their case they used bag-of-words or sparse coding representations. As
noted in [8]  such encodings require large codebooks to produce discriminative feature representa-
tions. This  in turn  makes these approaches hardly applicable to the datasets of ImageNet scale [4].
As explained in Sect. 2  FV encoders do not require large codebooks  and by employing supervised
dimensionality reduction  we can preserve the discriminative ability of FV even after the projection
onto a low-dimensional space  similarly to [10].
Spatial stacking (sub-layer 2). After the dimensionality-reduced FV pooling (Sect. 3)  an image is
represented as a spatially dense set of low-dimensional multi-scale discriminative features. It should
be noted that local sum-pooling  while making the representation invariant to small translations  is
agnostic to the relative location of aggregated features. To capture the spatial structure within each
feature’s neighbourhood  we incorporate the stacking sub-layer  which concatenates the spatially
adjacent features in a 2× 2 window (Fig. 2). This step is similar to 4× 4 stacking employed in SIFT.
Normalisation and PCA projection (sub-layer 3). After stacking  the features are L2-normalised 
which improves their invariance properties. This procedure is closely related to Local Contrast
Normalisation  widely used in DNNs. Finally  before passing the features to the FV encoder of the
next layer  PCA dimensionality reduction is carried out  which serves two purposes: (i) features
are decorrelated so that they can be modelled using diagonal-covariance GMMs of the next layer;
(ii) dimensionality is reduced from 4hl to dl+1 to keep the image representation compact and the
computational complexity limited.
Multi-scale computation. In practice  the Fisher layer computation is repeated at multiple scales by
changing the pooling window size ql (the PCA projection in sub-layer 3 is the same for all scales).
This allows a single layer to capture multi-scale statistics  which is different from typical DNN
architectures  which use a single pooling window size per layer. The resulting dense multi-scale
features  computed by the layer  form the input of the next layer (similarly to the dense multi-scale
SIFT features). In Sect. 6 we show that a multi-scale Fisher layer indeed brings an improvement 
compared to a ﬁxed pooling window size.

4

local Fisher Compressed semi-local Fisher vector encoding   Spatial  stacking (2×2) norm. L2 norm. & PCA dl hl 4hl dl+1 mixture of  Kl Gaussians projection Wl  from 2Kldl to hl projection Ul  from 4hl to dl+1 4hl ql 2ql 4 Fisher network
Our image classiﬁcation pipeline  which we coin Fisher network (shown in Fig. 1) is constructed by
stacking several (at least one) Fisher layers (Sect. 3) on top of dense features  such as SIFT or raw
image patches. The penultimate layer  which computes a single-vector image representation  is the
special case of the Fisher layer  where sum-pooling is only performed globally over the whole image.
We call this layer the global Fisher layer  and it effectively computes a full-dimensional normalised
Fisher vector encoding (the dimensionality reduction stage is omitted since the computed FV is
directly used for classiﬁcation). The ﬁnal layer is an off-the-shelf ensemble of one-vs-rest binary
linear SVMs. As can be seen  a Fisher network generalises the standard FV pipeline of [20]  as the
latter corresponds to the network with a single global Fisher layer.
Multi-layer image descriptor. Each subsequent Fisher layer is designed to capture more complex 
higher-level image statistics  but the competitive performance of shallow FV-based frameworks [21]
suggests that low-level SIFT features are already discriminative enough to distinguish between a
number of image classes. To fully exploit the hierarchy of Fisher layers  we branch out a globally
pooled  normalised FV from each of the Fisher layers  not just the last one. These image representa-
tions are then concatenated to produce a rich  multi-layer image descriptor. A similar approach has
previously been applied to convolutional networks in [25].
4.1 Learning
The Fisher network is trained in a supervised manner  since each Fisher layer (apart from the global
layer) depends on discriminative dimensionality reduction. The network is trained greedily  layer by
layer. Here we discuss how the (non-global) Fisher layer can be efﬁciently trained in the large-scale
scenario  and introduce two options for the projection learning objective.
Projection learning proxy. As explained in Sect. 3  we need to learn a discriminative projection
W to signiﬁcantly reduce the dimensionality of the densely-computed semi-local FVs. At the same
time  the only annotation available for discriminative learning in our case is the class label of the
whole image. We exploit this information by requiring that projected semi-local FVs are good
predictors of the image class. Taking into account that (i) it may be unreasonable to require all local
feature occurrences to predict the object class (the support of some features may not even cover the
object)  and (ii) there are too many features to use all of them in learning (∼ 104 semi-local FVs for
each of the ∼ 106 training images)  we optimize the average class prediction of all the features in a
layer  rather than the prediction of individual feature occurrences.
In particular  we construct a learning proxy by computing the average ψ of all unnormalised  unpro-
jected semi-local FVs φs of an image  ψ = 1
s=1 φs  and deﬁning the learning constraints on ψ
S
using the image label. Considering that W ψ = 1
s=1 W φs  the projection W   learnt for ψ  is also
S
applicable to individual semi-local FVs φs. The advantages of the proxy are that the image-level
class annotation can now be utilised  and during projection learning we only need to store a single
vector ψ per image. In the sequel  we deﬁne two options for the projection learning objective  which
are then compared in Sect. 6.
Bi-convex max-margin projection learning. One approach to discriminative dimensionality re-
duction learning consists in ﬁnding the projection onto a subspace where the image classes are
as linearly separable as possible [10  31]. This corresponds to the bilinear class scoring function:
c W ψ  where W is the linear projection which we seek to optimise and vc is the linear model (e.g.
vT
an SVM) of the class c in the projected space. The max-margin optimisation problem for W and the
ensemble {vc} takes the following form:

(cid:80)S
(cid:80)S

(cid:105)

(cid:88)

c

+

λ
2

(cid:107)vc(cid:107)2

2 +

(cid:107)W(cid:107)2
F  

µ
2

(2)

(cid:88)

(cid:88)

c(cid:48)(cid:54)=c(i)

i

(cid:104)(cid:0)vc(cid:48) − vc(i)

(cid:1)T

max

W ψi + 1  0

where ci is the ground-truth class of an image i  λ and µ are the regularisation constants. The
learning objective is bi-convex in W and vc  and a local optimum can be found by alternation
between the convex problems for W and {vc}  both of which can be solved in primal using a
stochastic sub-gradient method [27]. We initialise the alternation by setting W to the PCA-whitening
matrix W0. Once the optimisation has converged  the classiﬁers vc are discarded  and we keep the
projection W .

5

Projection onto the space of classiﬁer scores. Another dimensionality reduction technique  which
we consider in this work  is to train one-vs-rest SVM classiﬁers {uc}C
c=1 on the full-dimensional
FVs ψ  and then use the C-dimensional vector of SVM outputs as the compressed representation
of ψ. This corresponds to setting the c-th row of the projection matrix W to the SVM model uc.
This approach is closely related to attribute-based representations and classemes [15  30]  but in our
case we do not use any additional data annotated with a different set of (attribute) classes to train
the models; instead  the C = 1000 classiﬁers trained directly on the ILSVRC dataset are used. If
a speciﬁc target dimensionality is required  PCA dimensionality reduction can be further applied to
the classiﬁer scores [10]  but in our case we applied PCA after spatial stacking (Sect. 3).
The advantage of using SVM models for dimensionality reduction is  mostly  computational. As we
will show in Sect. 6  both formulations exhibit a similar level of performance  but training C one-
vs-rest classiﬁers is much faster than performing alternation between SVM learning and projection
learning in (2). The reason is that one-vs-rest SVM training can be easily parallelised  while projec-
tion learning is signiﬁcantly slower even when using a parallel gradient descent implementation.
5
Efﬁcient computation of hard-assignment Fisher vectors. In the original FV encoding formula-
tion (1)  each feature is soft-assigned to all K Gaussians of the GMM by computing the assignment
weights αk(xp) as the responsibilities of GMM component k for feature p: αk(xp) = πk N k(xp)
j πj N j (xp) 
where N k(xp) is the likelihood of k-th Gaussian. To facilitate an efﬁcient computation of a large
number of dense FVs per image  we introduce and utilise a fast variant of FV (which we term
hard-FV)  which uses hard assignments of features to Gaussians  computed as

Implementation details

(cid:80)

αk(xp) =

if k = arg maxj πj N j(xp)
otherwise

(cid:26)1

0

(cid:16)

(3)

(cid:17)

l

l

l

is easy to show that Wlφ =(cid:80)K

(cid:80)

l

l

k

l

k

k=1

p∈Ωk

W (k 1)

(p) are computed and projected using small hl×dl sub-matrices W (k 1)

Hard-FVs are inherently sparse; this allows for the fast computation of projected FVs Wlφ. Indeed  it
Φ(1)
k (p) + W (k 2)
Φ(2)
  where Ωk is the set
k (p)
of input vectors hard-assigned to the GMM component k  and W (k 1)
  W (k 2)
are the sub-matrices
of Wl which correspond to the 1st and 2nd order differences Φ(1) (2)
(p) between the feature xp
and the k-th GMM mean (1). This suggests the fast computation procedure: each dl-dimensional
input feature xp is ﬁrst hard-assigned to a Gaussian k based on (3). Then  the corresponding dl-D
differences Φ(1) (2)
  W (k 2)
 
which is fast. The algorithm avoids computing high-dimensional FVs  followed by the projection
using a large matrix Wl ∈ Rhl×2Kldl  which is prohibitive since the number of dense FVs is high.
Implementation. Our SIFT feature extraction follows that of [21]. Images are rescaled so that
the number of pixels is 100K. Dense RootSIFT [2] is computed on 24 × 24 patches over 5 scales
√
2) with a 3 pixel step. We also employ SIFT augmentation with the patch spatial
(scale factor 3
coordinates [24]. During training  high-dimensional FVs  computed by the 2nd Fisher layer  are
compressed using product quantisation [23]. The learning framework is implemented in Matlab 
speeded up with C++ MEX. The computation is carried out on CPU without the use of GPU. Train-
ing the Fisher network on top of SIFT descriptors on 1.2M images of ILSVRC-2010 [4] dataset
takes about one day on a 200-core cluster. Image classiﬁcation time is ∼ 2s on a single core.
6 Evaluation
In this section  we evaluate the proposed Fisher network on the dataset  introduced for the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) 2010 [4]. It contains images of 1000 cate-
gories  with 1.2M images available for training  50K for validation  and 150K for testing. Following
the standard evaluation protocol for the dataset  we report both top-1 and top-5 accuracy (%) com-
puted on the test set. Sect. 6.1 evaluates variants of the Fisher network on a subset of ILSVRC to
identify the best one. Then  Sect. 6.2 evaluates the complete framework.
6.1 Fisher network variants
We begin with comparing the performance of the Fisher network under different settings. The
comparison is carried out on a subset of ILSVRC  which was obtained by random sampling of 200

6

Table 1: Evaluation of dimensionality reduction  stacking  and normalisation sub-layers on a
200 class subset of ILSVRC-2010. The following conﬁguration of Fisher layers was used: d1 =
128  K1 = 256  q1 = 5  δ1 = 1  h1 = 200 (number of classes)  d2 = 200   K2 = 256. The baseline
performance of a shallow FV encoding is 57.03% and 78.9% (top-1 and top-5 accuracy).

stacking

L2 norm-n

dim-ty reduction
classiﬁer scores
classiﬁer scores
classiﬁer scores

bi-convex

(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

top-1
59.69
59.42
60.22
59.49

top-5
80.29
80.44
80.93
81.11

Table 2: Evaluation of multi-scale pooling and multi-layer image description on the subset of
ILSVRC-2010. The following conﬁguration of Fisher layers was used: d1 = 128  K1 = 256 
h1 = 200  d2 = 200  K2 = 256. Both Fisher layers used spatial coordinate augmentation. The
baseline performance of a shallow FV encoding is 59.51% and 80.50% (top-1 and top-5 accuracy).

pooling window size q1

pooling stride δ1 multi-layer

5

{5  7  9  11}
{5  7  9  11}

1
2
2

top-1
61.56
62.16
63.79

top-5
82.21
82.43
83.73

(cid:88)

classes out of 1000. To avoid over-ﬁtting indirectly on the test set  comparisons in this section are
carried on the validation set. In our experiments  we used SIFT as the ﬁrst layer of the network 
followed by two Fisher layers (the second one is global  as explained in Sect. 4).
Dimensionality reduction  stacking  and normalisation. Here we quantitatively assess the three
sub-layers of a Fisher layer (Sect. 3). We compare the two proposed dimensionality reduction learn-
ing schemes (bi-convex learning and classiﬁer scores)  and also demonstrate the importance of spa-
tial stacking and L2 normalisation. The results are shown in Table 1. As can be seen  both spatial
stacking and L2 normalisation improve the performance  and dimensionality reduction via projec-
tion onto the space of SVM classiﬁer scores performs on par with the projection learnt using the
bi-convex formulation (2). In the following experiments we used the classiﬁer scores for dimension-
ality reduction  since their training can be parallelised and is signiﬁcantly faster.
Multi-scale pooling and multi-layer image representation. In this experiment  we compare the
performance of semi-local FV pooling using single and multiple window sizes (Sect. 3)  as well as
single- and multi-layer image representations (Sect. 4). From Table 2 it is clear that using multi-
ple pooling window sizes is beneﬁcial compared to a single window size. When using multi-scale
pooling  the pooling stride was increased to keep the number of pooled semi-local FVs roughly the
same. Also  the multi-layer image descriptor obtained by stacking globally pooled and normalised
FVs  computed by the two Fisher layers  outperforms each of these FVs taken separately. We also
note that in this experiment  unlike the previous one  both Fisher layers utilized spatial coordinate
augmentation of the input features  which leads to a noticeable boost in the shallow baseline perfor-
mance (from 78.9% to 80.50% top-5 accuracy). Apart from our Fisher network  multi-scale pooling
can be readily employed in convolutional networks.
6.2 Evaluation on ILSVRC-2010
Now that we have evaluated various Fisher layer conﬁgurations on a subset of ILSVRC  we assess
the performance of our framework on the full ILSVRC-2010 dataset. We use off-the-shelf SIFT and
colour features [20] in the feature extraction layer  and demonstrate that signiﬁcant improvements
can be achieved by injecting a single Fisher layer into the conventional FV-based pipeline [23].
The following conﬁguration of Fisher layers was used: d1 = 80  K1 = 512  q1 = {5  7  9  11} 
δ1 = 2  h1 = 1000  d2 = 256  K2 = 256. On both Fisher layers  we used spatial coordinate
augmentation of the input features. The ﬁrst Fisher layer uses a large number of GMM components
Kl  since it was found to be beneﬁcial for shallow FV encodings [23]  used here as a baseline. The
one-vs-rest SVM scores were Platt-calibrated on the validation set (we did not use calibration for
semi-local FV dimensionality reduction).
The results are shown in Table 3. First  we note that the globally pooled Fisher vector  branched
out of the ﬁrst Fisher layer (which effectively corresponds to the conventional FV encoding [23]) 
results in better accuracy than reported in [23]  which validates our implementation. Using the 2nd
Fisher layer on top of the 1st one leads to a signiﬁcant performance improvement. Finally  stacking
the FVs  produced by the 1st and 2nd Fisher layers  pushes the accuracy even further.

7

Table 3: Performance on ILSVRC-2010 using dense SIFT and colour features. We also specify
the dimensionality of SIFT-based image representations.

pipeline
setting

1st Fisher layer
2nd Fisher layer

multi-layer (1st and 2nd Fisher layers)

S´anchez et al. [23]

dimension

SIFT only
top-1
46.52
48.54
52.57
N/A

82K
131K
213K
524K

top-5
68.45
71.35
73.68
67.9

SIFT & colour
top-1
top-5
76.35
55.35
77.68
56.20
79.20
59.47
54.3
74.3

The state of the art on the ILSVRC-2010 dataset was obtained using an 8-layer convolutional net-
work [14]  i.e. twice as deep as the Fisher network considered here. Using training and test set
augmentation based on jittering (not employed here)  they achieved the top-1 / top-5 accuracy of
62.5% / 83.0%. Without test set augmentation (i.e. using only the original images for class scoring) 
their result is 61% / 81.7%. In our case  we did not augment neither the training  nor the test set  and
achieved 59.5% / 79.2%. For reference  our baseline shallow FV accuracy is 55.4% / 76.4%. We
conclude that injecting a single intermediate layer leads to a signiﬁcant performance boost (+4.1%
top-1 accuracy)  but deep CNNs are still somewhat better (+1.5% top-1 accuracy). These results are
however quite encouraging since they were obtained by using off-the-shelf features and encodings 
reconﬁgured to add a single intermediate layer. Notably  our model did not require an optimised
GPU implementation  nor it was necessary to control over-ﬁtting by techniques such as dropout [14]
and training set augmentation.
Finally  we demonstrate that the Fisher network and deep CNN representations are complementary
by combining the class posteriors obtained from CNN with those of a Fisher network. To this end 
we re-implemented the deep CNN of [14] using their publicly available cuda-convnet toolbox. Our
implementation performs slightly better  giving 62.91% / 83.19% (with test set augmentation). The
multiplication of CNN and Fisher network posteriors leads to a signiﬁcantly improved accuracy:
66.75% / 85.64%. It should be noted that another way of improving the CNN accuracy  used in [14]
on ImageNet-2012 dataset  consists in training several CNNs and averaging their posteriors. Further
study of the complementarity of various deep and shallow representations is beyond the scope of
this paper  and will be addressed in future research.
7 Conclusion
We have shown that Fisher vectors  a standard image encoding method  are amenable to be stacked in
multiple layers  in analogy to the state-of-the-art deep neural network architectures. Adding a single
layer is in fact sufﬁcient to signiﬁcantly boost the performance of these shallow image encodings 
bringing their performance closer to the state of the art in the large-scale classiﬁcation scenario [14].
The fact that off-the-shelf image representations can be simply and successfully stacked indicates
that deep schemes may extend well beyond neural networks.
Acknowledgements
This work was supported by ERC grant VisRec no. 228180.
References
[1] A. Agarwal and B. Triggs. Hyperfeatures - multilevel local coding for visual recognition. In Proc. ECCV 

pages 30–43  2006.

[2] R. Arandjelovi´c and A. Zisserman. Three things everyone should know to improve object retrieval. In

Proc. CVPR  2012.

[3] Y. Bengio  P. Lamblin  D. Popovici  and H. Larochelle. Greedy layer-wise training of deep networks. In

NIPS  pages 153–160  2006.

[4] A Berg  J Deng  and L Fei-Fei. Large scale visual recognition challenge (ILSVRC)  2010. URL http:

//www.image-net.org/challenges/LSVRC/2010/.

[5] J. Carreira  R. Caseiro  J. Batista  and C. Sminchisescu. Semantic segmentation with second-order pool-

ing. In Proc. ECCV  pages 430–443  2012.

[6] K. Chatﬁeld  V. Lempitsky  A. Vedaldi  and A. Zisserman. The devil is in the details: an evaluation of

recent feature encoding methods. In Proc. BMVC.  2011.

8

[7] D. C. Ciresan  U. Meier  and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.

In Proc. CVPR  pages 3642–3649  2012.

[8] A. Coates  A. Y. Ng  and H. Lee. An analysis of single-layer networks in unsupervised feature learning.

In Proc. AISTATS  2011.

[9] G. Csurka  C. Bray  C. Dance  and L. Fan. Visual categorization with bags of keypoints. In Workshop on

Statistical Learning in Computer Vision  ECCV  pages 1–22  2004.

[10] A. Gordo  J. A. Rodr´ıguez-Serrano  F. Perronnin  and E. Valveny. Leveraging category-level labels for

instance-level image retrieval. In Proc. CVPR  pages 3045–3052  2012.

[11] B. Hariharan  J. Malik  and D. Ramanan. Discriminative decorrelation for clustering and classiﬁcation.

In Proc. ECCV  2012.

[12] G. E. Hinton  S. Osindero  and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation  18(7):1527–1554  2006.

[13] T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In NIPS  pages

487–493  1998.

[14] A. Krizhevsky  I. Sutskever  and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural

networks. In NIPS  pages 1106–1114  2012.

[15] C. H. Lampert  H. Nickisch  and S. Harmeling. Learning to detect unseen object classes by between-class

attribute transfer. In Proc. CVPR  pages 951–958  2009.

[16] S. Lazebnik  C. Schmid  and J Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recog-

nizing Natural Scene Categories. In Proc. CVPR  2006.

[17] Q. Le  M. Ranzato  R. Monga  M. Devin  K. Chen  G. Corrado  J. Dean  and A. Ng. Building high-level

features using large scale unsupervised learning. In Proc. ICML  2012.

[18] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[19] D. Lowe. Object recognition from local scale-invariant features. In Proc. ICCV  pages 1150–1157  Sep

1999.

[20] F. Perronnin  J. S´anchez  and T. Mensink. Improving the Fisher kernel for large-scale image classiﬁcation.

In Proc. ECCV  2010.

[21] F. Perronnin  Z. Akata  Z. Harchaoui  and C. Schmid. Towards good practice in large-scale learning for

image classiﬁcation. In Proc. CVPR  pages 3482–3489  2012.

[22] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience 

2(11):1019–1025  1999.

[23] J. S´anchez and F. Perronnin. High-dimensional signature compression for large-scale image classiﬁcation.

In Proc. CVPR  2011.

[24] J. S´anchez  F. Perronnin  and T. Em´ıdio de Campos. Modeling the spatial layout of images beyond spatial

pyramids. Pattern Recognition Letters  33(16):2216–2223  2012.

[25] P. Sermanet and Y. LeCun. Trafﬁc sign recognition with multi-scale convolutional networks. In Interna-

tional Joint Conference on Neural Networks  pages 2809–2813  2011.

[26] T. Serre  L. Wolf  and T. Poggio. A new biologically motivated framework for robust object recognition.

Proc. CVPR  2005.

[27] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient SOlver for SVM.

In Proc. ICML  volume 227  2007.

[28] K. Simonyan  O. M. Parkhi  A. Vedaldi  and A. Zisserman. Fisher Vector Faces in the Wild. In Proc.

BMVC.  2013.

[29] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In Proc.

ICCV  volume 2  pages 1470–1477  2003.

[30] L. Torresani  M. Szummer  and A. Fitzgibbon. Efﬁcient object category recognition using classemes. In

Proc. ECCV  pages 776–789  sep 2010.

[31] J. Weston  S. Bengio  and N. Usunier. WSABIE: Scaling up to large vocabulary image annotation. In

Proc. IJCAI  pages 2764–2770  2011.

[32] S. Yan  X. Xu  D. Xu  S. Lin  and X. Li. Beyond spatial pyramids: A new feature extraction framework

with dense spatial sampling for image classiﬁcation. In Proc. ECCV  pages 473–487  2012.

[33] J. Yang  K. Yu  Y. Gong  and T. S. Huang. Linear spatial pyramid matching using sparse coding for image

classiﬁcation. In Proc. CVPR  pages 1794–1801  2009.

9

,Karen Simonyan
Andrea Vedaldi
Andrew Zisserman