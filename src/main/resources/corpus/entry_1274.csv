2019,DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization,For optimization of a large sum of functions in a distributed computing environment  we present a novel communication efficient Newton-type algorithm that enjoys a variety of advantages over similar existing methods. Our algorithm  DINGO  is derived by optimization of the gradient's norm as a surrogate function. DINGO does not impose any specific form on the underlying functions and its application range extends far beyond convexity and smoothness. The underlying sub-problems of DINGO are simple linear least-squares  for which a plethora of efficient algorithms exist. DINGO involves a few hyper-parameters that are easy to tune and we theoretically show that a strict reduction in the surrogate objective is guaranteed  regardless of the selected hyper-parameters.,DINGO: Distributed Newton-Type Method for

Gradient-Norm Optimization

Rixon Crane

University of Queensland

r.crane@uq.edu.au

Fred Roosta

University of Queensland
fred.roosta@uq.edu.au

Abstract

For optimization of a large sum of functions in a distributed computing environment 
we present a novel communication efﬁcient Newton-type algorithm that enjoys a
variety of advantages over similar existing methods. Our algorithm  DINGO  is
derived by optimization of the gradient’s norm as a surrogate function. DINGO does
not impose any speciﬁc form on the underlying functions and its application range
extends far beyond convexity and smoothness. The underlying sub-problems of
DINGO are simple linear least-squares  for which a plethora of efﬁcient algorithms
exist. DINGO involves a few hyper-parameters that are easy to tune and we
theoretically show that a strict reduction in the surrogate objective is guaranteed 
regardless of the selected hyper-parameters.

1

Introduction

Consider the optimization problem

min
w∈Rd

(cid:26)

f (w) (cid:44) 1
m

(cid:27)

fi(w)

 

m(cid:88)

i=1

(1)

minw∈Rd (cid:80)n

in a centralized distributed computing environment involving one driver machine and m worker
machines  in which the ith worker can only locally access the ith component function  fi. Such
distributed computing settings arise increasingly more frequently as a result of technological and
communication advancements that have enabled the collection of and access to large scale datasets.
As a concrete example  take a data ﬁtting application  in which given n data points  {xi}n
i=1  and
their corresponding loss  (cid:96)i(w; xi)  parameterized by w  the goal is to minimize the overall loss as
i=1 (cid:96)i(w; xi)/n. Such problems appear frequently in machine learning  e.g.  [1  2  3]
and scientiﬁc computing  e.g.  [4  5  6]. However  in “big data” regimes where n (cid:29) 1  lack of
adequate computational resources  in particular storage  can severely limit  or even prevent  any
attempts at solving such optimization problems in a traditional stand-alone way  e.g.  using a single
machine. This can be remedied through distributed computing  in which resources across a network
of stand-alone computational nodes are “pooled” together so as to scale to the problem at hand [7].
In such a setting  where n data points are distributed across m workers  one can instead consider (1)
with

(cid:96)j(w; xj) 

i = 1  2  . . .   m 

(2)

(cid:88)

j∈Si

fi(w) (cid:44) 1
|Si|

where Si ⊆ {1  2  . . .   n}  with cardinality denoted by |Si|  correspond to the distribution of data
across the nodes  i.e.  the ith node has access to a portion of the data indexed by the set Si.
In distributed settings  the amount of communications  i.e.  messages exchanged across the network 
are often considered a major bottleneck of computations (often more so than local computation

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

times)  as they can be expensive in terms of both physical resources and time through latency
[8  9]. First-order methods [10]  e.g.  stochastic gradient descent (SGD) [11]  solely rely on gradient
information and as a result are rather easy to implement in distributed settings. They often require the
performance of many computationally inexpensive iterations  which can be suitable for execution on
a single machine. However  as a direct consequence  they can incur excessive communication costs
in distributed environments and  hence  they might not be able to take full advantage of the available
distributed computational resources.
By employing curvature information in the form of the Hessian matrix  second-order methods aim at
transforming the gradient such that it is a more suitable direction to follow. Compared with ﬁrst-order
alternatives  although second-order methods perform more computations per iteration  they often
require far fewer iterations to achieve similar results. In distributed settings  this feature can directly
translate to signiﬁcantly less communication costs. As a result  distributed second-order methods
have the potential to become the method of choice for distributed optimization tasks.

(cid:107)A(cid:107)  respectively. For x  z ∈ Rd we let [x  z] (cid:44)(cid:8)x + τ (z − x) | 0 ≤ τ ≤ 1(cid:9). The range and null

Notation
We let (cid:104)· ·(cid:105) denote the common Euclidean inner product deﬁned by (cid:104)x  y(cid:105) = xT y for x  y ∈ Rd.
Given a vector v and matrix A  we denote their vector (cid:96)2 norm and matrix spectral norm as (cid:107)v(cid:107) and
space of a matrix A is denoted by R(A) and N (A)  respectively. The Moore–Penrose inverse [12]
of A is denoted by A†. We let wt ∈ Rd denote the point at iteration t. For notational convenience 
we denote gt i (cid:44) ∇fi(wt)  Ht i (cid:44) ∇2fi(wt)  gt (cid:44) ∇f (wt) and Ht (cid:44) ∇2f (wt). We also let

˜Ht i (cid:44)

∈ R2d×d

and

˜gt (cid:44)

∈ R2d 

(3)

(cid:20)Ht i

(cid:21)

φI

(cid:18)gt

(cid:19)

0

where φ > 0  I is the identity matrix  and 0 is the zero vector.

Related Work and Contributions

Owing to the above-mentioned potential  many distributed second-order optimization algorithms have
recently emerged to solve (1). Among them  most notably are GIANT [13]  DiSCO [9]  DANE [14] 
InexactDANE and AIDE [15]. While having many advantages  each of these methods respectively
come with several disadvantages that can limit their applicability in certain regimes. Namely 
some rely on  rather stringent  (strong) convexity assumptions  while for others the underlying sub-
problems involve non-linear optimization problems that are themselves non-trivial to solve. A subtle 
yet potentially severe  draw-back for many of the above-mentioned methods is that their performance
can be sensitive to  and severely affected by  the choice of their corresponding hyper-parameters.
Here  we present a novel communication efﬁcient distributed second-order optimization method
that aims to alleviate many of the aforementioned disadvantages. Our approach is inspired by and
follows many ideas of recent results on Newton-MR [16]  which extends the application range of the
classical Newton-CG beyond (strong) convexity and smoothness. More speciﬁcally  our algorithm 
named DINGO for “DIstributed Newton-type method for Gradient-norm Optimization”  is derived
by optimization of the gradient’s norm as a surrogate function for (1)  i.e. 

(cid:40)

1
2

(cid:13)(cid:13)∇f (w)(cid:13)(cid:13)2

min
w∈Rd

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

=

1

2m2

∇fi(w)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

.

(4)

When f is invex  [17  18]  the problems (1) and (4) have the same solutions. Recall that invexity is the
generalization of convexity  which extends the sufﬁciency of the ﬁrst order optimality condition  e.g. 
Karush-Kuhn-Tucker conditions  to a broader class of problems than simple convex programming. In
other words  invexity is a special case of non-convexity  which subsumes convexity as a sub-class. In
this light  unlike DiSCO and GIANT  by considering the surrogate function (4)  DINGO’s application
range and theoretical guarantees extend far beyond convex settings to invex problems. Naturally  by
considering (4)  DINGO may converge to a local maximum or saddle point in non-invex problems.
Similar to GIANT and DiSCO  and in contrast to DANE  InexactDANE and AIDE  our algorithm
involves a few hyper-parameters that are easy to tune and the underlying sub-problems are simple
linear least-squares  for which a plethora of efﬁcient algorithms exist. However  the theoretical

2

Table 1: Comparison of problem class  function form and data distribution. Note that DINGO doesn’t
assume invexity in analysis  rather it is suited to invex problems in practice.

DINGO
GIANT
DiSCO

InexactDANE

AIDE

Problem Class

Invex

Strongly Convex
Strongly Convex

Non-Convex
Non-Convex

Function Form

Data Distribution

Any

(cid:96)j(w; xj) = ψj((cid:104)w  xj(cid:105)) + γ(cid:107)w(cid:107)2

Any
Any
Any

Any
|Si| > d
Any
Any
Any

Table 2: Comparison of number of sub-problem hyper-parameters and communication rounds per
iteration. Under inexact update  the choice of sub-problem solver will determine additional hyper-
parameters. Most communication rounds of DiSCO arise when iteratively solving its sub-problem.
We assume DINGO and GIANT use two communication rounds for line-search per iteration.

Number of Sub-Problem Hyper-
Parameters (Under Exact Update)

Communication Rounds Per

Iteration (Under Inexact Update)

DINGO
GIANT
DiSCO

InexactDANE

AIDE

2
0
0
2
3

≤ 8
6

2 + 2 · (sub-problem iterations)
4 · (inner InexactDANE iterations)

4

analysis of both GIANT and DiSCO is limited to the case where each fi is strongly convex  and for
GIANT they are also of the special form where in (2) we have (cid:96)j(w; xj) = ψj((cid:104)w  xj(cid:105)) + γ(cid:107)w(cid:107)2 
γ > 0 is a regularization parameter and ψj is convex  e.g.  linear predictor models. In contrast 
DINGO does not impose any speciﬁc form on the underlying functions. Also  unlike GIANT  we
allow for |Si| < d in (2). Moreover  we theoretically show that DINGO is not too sensitive to the
choice of its hyper-parameters in that a strict reduction in the gradient norm is guaranteed  regardless
of the selected hyper-parameters. See Tables 1 and 2 for a summary of high-level algorithm properties.
Finally  we note that  unlike GIANT  DiSCO  InexactDANE and AIDE  our theoretical analysis
requires exact solutions to the sub-problems. Despite the fact that the sub-problems of DINGO are
simple ordinary least-squares  and that DINGO performs well in practice with very crude solutions 
this is admittedly a theoretical restriction  which we aim to address in future.
The distributed computing environment that we consider is also assumed by GIANT  DiSCO  DANE 
InexactDANE and AIDE. Moreover  as with these methods  we restrict communication to vectors of
size linear in d  i.e.  O(d). A communication round is performed when the driver uses a broadcast
operation to send information to one or more workers in parallel  or uses a reduce operation to receive
information from one or more workers in parallel. For example  computing the gradient at iteration t 
i=1 gt i/m  requires two communication rounds  i.e.  the driver broadcasts wt to all
workers and then  by a reduce operation  receives gt i for all i. We further remind that the distributed
computational model considered here is such that the main bottleneck involves the communications
across the network.

namely gt =(cid:80)m

2 DINGO

In this section  we describe the derivation of DINGO  as depicted in Algorithm 1. Each iteration
t involves the computation of two main ingredients: an update direction pt  and an appropriate
step-size αt. As usual  our next iterate is then set as wt+1 = wt + αtpt.

3

We begin iteration t by distributively computing the gradient gt. Thereafter  we distributively compute
†
t igt/m and
i=1 H
†
˜H
t i˜gt/m. Computing the update direction pt involves three cases  all of which involve simple

i=1 Ht igt/m as well as the vectors(cid:80)m

i=1

Update Direction

linear least-squares sub-problems:

the Hessian-gradient product Htgt =(cid:80)m
(cid:80)m
Case 1 If (cid:104)(cid:80)m
pt = (cid:80)m
“−(cid:80)m
direction yields suitable descent. Namely  if (cid:104)(cid:80)m
(cid:80)m
†
i=1 pt i/m  with pt i = − ˜H
t i˜gt.
It (cid:44)(cid:8)i = 1  2  . . .   m | (cid:104) ˜H

i=1 H

†
t igt/m  Htgt(cid:105) ≥ θ(cid:107)gt(cid:107)2  where θ is as in Algorithm 1 

then we let
†
i=1 pt i/m  with pt i = −H
t igt. Here  we check that the potential update direction
†
t igt/m” is a suitable descent direction for our surrogate objective (4). We do this since
i=1 H
we have not imposed any restrictive assumptions on (1)  e.g.  strong convexity of each fi  that would
automatically guarantee descent; see Lemma 1 for an example of such restrictive assumptions.
Case 2 If Case 1 fails  we include regularization and check again that the new potential update
†
t i˜gt/m  Htgt(cid:105) ≥ θ(cid:107)gt(cid:107)2  then we let pt =
˜H

i=1

Case 3 If all else fails  we enforce descent in the norm of the gradient. More speciﬁcally  as Case 2
does not hold  the set

t i˜gt  Htgt(cid:105) < θ(cid:107)gt(cid:107)2(cid:9) 

†

(5)
is non-empty. In parallel  the driver broadcasts Htgt to each worker i ∈ It and has it locally compute
the solution to

arg min

pt i

1
2

(cid:107)Ht ipt i + gt(cid:107)2 +

(cid:107)pt i(cid:107)2 

φ2
2

such that

(cid:104)pt i  Htgt(cid:105) ≤ −θ(cid:107)gt(cid:107)2 

.

t i

t i

λt i =

pt i = − ˜H

˜Ht i)−1Htgt 

†
t i˜gt − λt i( ˜HT

where φ is as in (3). It is easy to show that the solution to this problem is
†
−gT
t i˜gt + θ(cid:107)gt(cid:107)2
t Ht ˜H
˜Ht i)−1Htgt
t Ht( ˜HT
gT

(6) and  using a reduce operation  the driver then computes the update direction pt =(cid:80)m

(6)
The term λt i in (6) is positive by the deﬁnition of It and well-deﬁned by Assumption 5  which
implies that for gt (cid:54)= 0 we have Htgt (cid:54)= 0. In conclusion  for Case 3  each worker i ∈ It computes
i=1 pt i/m 
†
which by construction yields descent in the surrogate objective (4). Note that pt i = − ˜H
t i˜gt for all
i /∈ It have already been obtained as part of Case 2.
Remark 1. The three cases help avoid the need for any unnecessary assumptions on data distribution
or the knowledge of any practically unknowable constants. In fact  given Lemma 1  which imposes a
certain assumption on the data distribution  we could have stated our algorithm in its simplest form 
i.e.  with only Case 1. This would be more in line with some prior works  e.g.  GIANT  but it would
have naturally restricted the applicability of our method in terms of data distributions.
Remark 2. In practice  like GIANT and DiSCO  our method DINGO never requires the computation
or storage of an explicitly formed Hessian. Instead  it only requires Hessian-vector products  which
can be computed at a similar cost to computing the gradient itself. Computing matrix pseudo-inverse
†
and vector products  e.g.  H
t igt  constitute the sub-problems of our algorithm. This  in turn  is
done through solving least-squares problems using iterative methods that only require matrix-vector
products (see Section 4 for some such methods). Thus DINGO is suitable for large dimension d in (1).

Line Search

After computing the update direction pt  DINGO computes the next iterate wt+1 by moving along pt
by an appropriate step-size αt and forming wt+1 = wt + αtpt. We use an Armijo-type line search
to choose this step-size. Speciﬁcally  as we are minimizing the norm of the gradient as a surrogate
function  we choose the largest αt ∈ (0  1] such that

(cid:107)gt+1(cid:107)2 ≤ (cid:107)gt(cid:107)2 + 2αtρ(cid:104)pt  Htgt(cid:105) 

(7)
for some constant ρ ∈ (0  1). By construction of pt we always have (cid:104)pt  Htgt(cid:105) ≤ −θ(cid:107)gt(cid:107)2.
Therefore  after each iteration we are strictly decreasing the norm of the gradient  and line-search
guarantees that this occurs irrespective of all hyper-parameters of DINGO  i.e.  θ  φ and ρ.

4

parameter ρ ∈ (0  1)  parameter θ > 0  and regularization parameter φ > 0 as in (3).

Algorithm 1 DINGO
1: input initial point w0 ∈ Rd  gradient tolerance δ ≥ 0  maximum iterations T   line search
2: for t = 0  1  2  . . .   T − 1 do
3:
4:
5:
6:
7:
8:

†
†
t igt and ˜H
t i˜gt.
The driver broadcasts gt and  in parallel  each worker i computes Ht igt  H
†
By a reduce operation  the driver computes Htgt = 1
t igt and
i=1 H
m
1
m

Distributively compute the full gradient gt.
if (cid:107)gt(cid:107) ≤ δ then
else

i=1 Ht igt  1

(cid:80)m

(cid:80)m

return wt

m

(cid:80)m
if(cid:10) 1
(cid:80)m
else if(cid:10) 1

i=1

m

†
˜H
t i˜gt.
†
i=1 H
t igt  Htgt
m
Let pt = 1
m

(cid:11) ≥ θ(cid:107)gt(cid:107)2 then
(cid:11) ≥ θ(cid:107)gt(cid:107)2 then

(cid:80)m
i=1 pt i  with pt i = −H
(cid:80)m
†
˜H
t i˜gt  Htgt
i=1 pt i  with pt i = − ˜H

(cid:80)m

†
t igt.

†
t i˜gt.

i=1

else

Let pt = 1
m
†
†
t i˜gt for all i such that (cid:104) ˜H
t i˜gt  Htgt(cid:105) ≥ θ(cid:107)gt(cid:107)2.
The driver computes pt i = − ˜H
†
The driver broadcasts Htgt to each worker i such that (cid:104) ˜H
t i˜gt  Htgt(cid:105) < θ(cid:107)gt(cid:107)2 and  in
parallel  they compute

9:
10:
11:
12:
13:
14:
15:

pt i = − ˜H

†
t i˜gt − λt i( ˜HT

t i

˜Ht i)−1Htgt 

λt i =

Using a reduce operation  the driver computes pt = 1
m

Choose the largest αt ∈ (0  1] such that(cid:13)(cid:13)∇f (wt + αtpt)(cid:13)(cid:13)2 ≤ (cid:107)gt(cid:107)2 + 2αtρ(cid:104)pt  Htgt(cid:105).

The driver computes wt+1 = wt + αtpt.

†
t i˜gt + θ(cid:107)gt(cid:107)2
−gT
t Ht ˜H
˜Ht i)−1Htgt
t Ht( ˜HT
gT
i=1 pt i.

(cid:80)m

t i

.

end if

16:
17:
18:
19:
end if
20:
21: end for
22: return wT .

3 Theoretical Analysis

†

i=1 H

{t | (cid:104)(cid:80)m

t igt/m  Htgt(cid:105) ≥ θ(cid:107)gt(cid:107)2}  C2 (cid:44) {t | (cid:104)(cid:80)m

In this section  we present convergence results for DINGO. The reader can ﬁnd proofs of lemmas and
theorems in the supplementary material. For notational convenience  in our analysis we have C1 (cid:44)
†
t i˜gt/m  Htgt(cid:105) ≥ θ(cid:107)gt(cid:107)2  t /∈ C1} 
˜H
and C3 (cid:44) {t| t /∈ (C1 ∪C2)}  which are sets indexing iterations t that are in Case 1  Case 2 and Case
3  respectively. The convergence analysis under these cases are treated separately in Sections 3.2 
3.3 and 3.4. The unifying result is then simply given in Corollary 1. We begin  in Section 3.1  by
establishing general underlying assumptions for our analysis. The analysis of Case 1 and Case 3
require their own speciﬁc assumptions  which are discussed in Sections 3.2 and 3.4  respectively.
Remark 3. As long as the presented assumptions are satisﬁed  our algorithm converges for any choice
of θ and φ  i.e.  these hyper-parameters do not require the knowledge of the  practically unknowable 
parameters from these assumptions. However  in Lemma 3 we give qualitative guidelines for a better
choice of θ and φ to avoid Case 2 and Case 3  which are shown to be less desirable than Case 1.

i=1

3.1 General Assumptions

As DINGO makes use of Hessian-vector products  we make the following straightforward assumption.
Assumption 1 (Twice Differentiability). The functions fi in (1) are twice differentiable.

Notice that we do not require each fi to be twice continuously differentiable. In particular  our
analysis carries through even if the Hessian is discontinuous. This is in sharp contrast to popular
belief that the application of non-smooth Hessian can hurt more so than it helps  e.g.  [19]. Note that

5

even if the Hessian is discontinuous  Assumption 1 is sufﬁcient in ensuring that Ht i is symmetric 
for all t and i  [20]. Following [16]  we also make the following general assumption on f.
Assumption 2 (Moral-Smoothness [16]). For all iterations t  there exists a constant L ∈ (0 ∞)

such that(cid:13)(cid:13)∇2f (w)∇f (w) − ∇2f (wt)∇f (wt)(cid:13)(cid:13) ≤ L(cid:107)w − wt(cid:107)  for all w ∈ [wt  wt + pt]  where

pt is the update direction of DINGO at iteration t.

As discussed in [16] with explicit examples  Assumption 2 is strictly weaker than the common
assumptions of the gradient and Hessian being both Lipschitz continuous. Using [16  Lemma 10]  it
follows from Assumptions 1 and 2 that

(cid:13)(cid:13)∇f (wt + αpt)(cid:13)(cid:13)2 ≤(cid:13)(cid:13)gt

(cid:13)(cid:13)2

+ 2α(cid:10)pt  Htgt

(cid:11) + α2L(cid:107)pt(cid:107)2 

(8)

for all α ∈ [0  1] and all iterations t.

3.2 Analysis of Case 1

†
t UtUT

t + U⊥

t (U⊥

†
†
t gt(cid:107) = (cid:107)H
t

(cid:0)UtUT

t )T(cid:1)gt(cid:107) = (cid:107)H

In this section  we analyze the convergence of iterations of DINGO that fall under Case 1. For such
iterations  we make the following assumption about the action of the pseudo-inverse of Ht i on gt.
Assumption 3 (Pseudo-Inverse Regularity of Ht i on gt). For all t ∈ C1 and all i = 1  2  . . .   m 
†
t igt(cid:107) ≤ γi(cid:107)gt(cid:107).
there exists constants γi ∈ (0 ∞) such that (cid:107)H
Assumption 3 may appear unconventional. However  it may be seen as more general than the
following assumption.
Assumption 4 (Pseudo-Inverse Regularity of Ht on its Range Space [16]). There exists a constant
γ ∈ (0 ∞) such that for all iterates wt we have (cid:107)Htp(cid:107) ≥ γ(cid:107)p(cid:107) for all p ∈ R(Ht).
Assumption 4 implies (cid:107)H
t gt(cid:107) ≤ γ−1(cid:107)gt(cid:107) 
t denote arbitrary orthonormal bases for R(Ht) and R(Ht)⊥  respectively  and
where Ut and U⊥
†
R(Ht)⊥ = N (HT
t ) = N (H
t ). Recall that Assumption 4 is a signiﬁcant relaxation of strong
convexity. As an example  an under-determined least-squares problem f (w) = (cid:107)Aw − b(cid:107)2/2 
which is clearly not strongly convex  satisﬁes Assumption 4 with γ = σ2
min(A)  where σmin(A) is the
smallest non-zero singular value of A.
Theorem 1 (Convergence Under Case 1). Suppose we run DINGO. Then under Assumptions 1  2

and 3  for all t ∈ C1 we have (cid:107)gt+1(cid:107)2 ≤ (1 − 2τ1ρθ)(cid:107)gt(cid:107)2  where τ1 = min(cid:8)1  2(1 − ρ)θ/(Lγ2)(cid:9) 
γ =(cid:80)m

i=1 γi/m  L is as in Assumption 2  γi are as in Assumption 3  ρ and θ are as in Algorithm 1.
From the proof of Theorem 1  it is easy to see that ∀t ∈ C1 we are guaranteed that 0 < 1− 2τ1ρθ < 1.
In Theorem 1  the term γ is the average of the γi’s. This is beneﬁcial as it “smooths out” non-
uniformity in γi’s; for example  γ ≥ mini γi. Under speciﬁc assumptions on (1)  we can theoretically
guarantee that t ∈ C1 for all iterations t. The following lemma provides one such example.
Lemma 1. Suppose Assumption 1 holds and that we run DINGO. Furthermore  suppose that for
all iterations t and all i = 1  2  . . .   m  the Hessian matrix Ht i is invertible and there exists
constants εi ∈ [0 ∞) and νi ∈ (0 ∞) such that (cid:107)Ht i − Ht(cid:107) ≤ εi and νi(cid:107)gt(cid:107) ≤ (cid:107)Ht igt(cid:107). If

(cid:80)m
i=1(1 − εi/νi)/m ≥ θ then t ∈ C1 for all t  where θ is as in Algorithm 1.
small so that(cid:80)m

As an example  the Assumptions of Lemma 1 trivially hold if each fi is strongly convex and we
assume certain data distribution. Under the assumptions of Lemma 1  if the Hessian matrix for each
worker is on average a reasonable approximation to the full Hessian  i.e.  εi is on average sufﬁciently
i=1 εi/νi < m  then we can choose θ small enough to ensure that t ∈ C1 for all t. In
other words  for the iterates to stay in C1  we do not require the Hessian matrix of each individual
worker to be a high-quality approximation to full Hessian (which could indeed be hard to enforce in
many practical applications). As long as the data is distributed in such a way that Hessian matrices
are on average reasonable approximations  we can guarantee to have t ∈ C1 for all t.

3.3 Analysis of Case 2

We now analyze the convergence of DINGO for iterations that fall under Case 2. For this case  we do
not require any additional assumptions to that of Assumptions 1 and 2. Instead  we use the upper

6

†
bound: (cid:107) ˜H
t i(cid:107) ≤ 1/φ for all iterations t and all i = 1  2  . . .   m  where φ is as in Algorithm 1; see
Lemma 4 in the supplementary material for a proof of this upper bound.
Theorem 2 (Convergence Under Case 2). Suppose we run DINGO. Then under Assumptions 1 and

2  for all t ∈ C2 we have (cid:107)gt+1(cid:107)2 ≤ (1 − 2τ2ρθ)(cid:107)gt(cid:107)2  where τ2 = min(cid:8)1  2(1 − ρ)φ2θ/L(cid:9)  L is

as in Assumption 2  and ρ  θ and φ are as in Algorithm 1.

In our experience  we have found that Case 2 does not occur frequently in practice. It serves more of
a theoretical purpose and is used to identify when Case 3 is required. Case 2 may be thought of as
a speciﬁc instance of Case 3  in which It is empty. However  it merits its own case  as in analysis
it does not require additional assumptions to Assumptions 1 and 2  and in practice it may avoid an
additional two communication rounds. If we were to bypass Case 2 to Case 3 and allow It to be
empty  then Theorem 3 of Section 3.4 with |It| = 0  which states the convergence for Case 3  indeed
coincides with Theorem 2.

3.4 Analysis of Case 3

t i)†Htgt

Now we turn to the ﬁnal case  and analyze the convergence of iterations of DINGO that fall under
Case 3. For such iterations  we make the following assumption.
Assumption 5. For all t ∈ C3 and all i = 1  2  . . .   m there exists constants δi ∈ (0 ∞) such that

(cid:13)(cid:13)( ˜HT
that(cid:13)(cid:13)(U⊥
any orthonormal bases for R(cid:0)∇2f (w)(cid:1) and its orthogonal complement  respectively.

Assumption 5  like Assumption 3  may appear unconventional. In Lemma 2 we show how Assump-
tion 5 is implied by three other reasonable assumptions  one of which is as follows.
Assumption 6 (Gradient-Hessian Null-Space Property [16]). There exists a constant ν ∈ (0  1] such
w denote

(cid:13)(cid:13) ≥ δi(cid:107)gt(cid:107).
w)T∇f (w)(cid:13)(cid:13)2 ≤ (1 − ν)ν−1(cid:13)(cid:13)UT

w∇f (w)(cid:13)(cid:13)2  for all w ∈ Rd  where Uw and U⊥

Assumption 6 implies that  as the iterations progress  the gradient will not become arbitrarily
orthogonal to the range space of the Hessian matrix. As an example  any least-squares problem
f (w) = (cid:107)Aw − b(cid:107)2/2 satisﬁes Assumption 6 with ν = 1 ; see [16] for detailed discussion and
many more examples of Assumption 6.
Lemma 2. Suppose Assumptions 4 and 6 hold and (cid:107)Ht i(cid:107)2 ≤ τi  ∀t ∈ C3  i = 1  2  . . .   m  τi ∈

(0 ∞)  i.e.  local Hessians are bounded. Then  Assumption 5 holds with δi = γ(cid:112)ν/(τi + φ2)  where

φ is as in Algorithm 1  and γ and ν are as in Assumptions 4 and 6  respectively.

The following theorem provides convergence properties for iterations of DINGO that are in Case 3.
Theorem 3 (Convergence Under Case 3). Suppose we run DINGO. Then under Assumptions 1 
2 and 5  for all t ∈ C3 we have (cid:107)gt+1(cid:107)2 ≤ (1 − 2ωtρθ)(cid:107)gt(cid:107)2 ≤ (1 − 2τ3ρθ)(cid:107)gt(cid:107)2  where
ωt = min{1  2(1 − ρ)θ/Lc2

(cid:18)
t}  τ3 = min{1  2(1 − ρ)θ/Lc2} 
1
mφ

m + |It| + θ

(cid:88)

(cid:19)

 

c =

1
δi

i∈It

ct =

m(cid:88)

i=1

1
δi

 

2
φ

+

θ
mφ

L is as in Assumption 2  δi are as in Assumption 5  It is as in (5)  and ρ  θ and φ are as in Algorithm 1.
Note that the convergence in Theorem 3 is given in both iteration dependent and independent format 
since the former explicitly relates the convergence rate to the size of It  while the latter simply
upper-bounds this  and hence is qualitatively less informative.
Comparing Theorems 2 and 3  iterations of DINGO should have slower convergence if they are in
Case 3 rather than Case 2. By Theorem 3  if an iteration t resorts to Case 3 then we may have slower
convergence for larger |It|. Moreover  this iteration would require two more communication rounds
than if it were to stop in Case 1 or Case 2. Therefore  one may wish to choose θ and φ appropriately
to reduce the chances that iteration t falls in Case 3 or that |It| is large. Under this consideration 
Lemma 3 presents a necessary condition on a relationship between θ and φ.
Lemma 3. Suppose we run DINGO. Under Assumption 1  if |It| < m for some iteration t  then
θφ ≤ (cid:107)Htgt(cid:107)/(cid:107)gt(cid:107).

7

(a) 10 Workers

(b) 100 Workers

(c) 1000 Workers

(d) 10000 Workers

Figure 1: Softmax regression problem on the CIFAR10 dataset. All algorithms are initialized at
w0 = 0. In all plots  Sync-SGD has a learning rate of 10−2. Async-SGD has a learning rate of: 10−3
in 1(a)  10−4 in 1(b) and 1(c)  and 10−5 in 1(d). SVRG has a learning rate of: 10−3 in 1(a) and 1(d) 
and 10−2 in 1(b) and 1(c). AIDE has τ = 100 in 1(a) and 1(d)  τ = 1 in 1(b)  and τ = 10 in 1(c).
The number of workers is the value of m in (1) and (2).

Lemma 3 suggests that we should pick θ and φ so that their product  θφ  is small. Clearly  choosing
smaller θ will increase the chance of an iteration of DINGO being in Case 1 or Case 2. However 
this also gives a lower rate of convergence in Theorems 1 and 2. Choosing smaller φ will preserve
†
more curvature information of the Hessian Ht i in ˜H
t i. However  φ should still be reasonably large 
as making φ smaller also makes some of the sub-problems of DINGO more ill-conditioned. There is
a non-trivial trade-off between φ and θ  and Lemma 3 gives an appropriate way to set them.
We can ﬁnally present a unifying result on the overall worst-case linear convergence rate of DINGO.
Corollary 1 (Overall Linear Convergence of DINGO). Suppose we run DINGO. Then under Assump-
tions 1  2  3 and 5  for all iterations t we have (cid:107)gt+1(cid:107)2 ≤ (1− 2τ ρθ)(cid:107)gt(cid:107)2 with τ = min{τ1  τ2  τ3} 
where τ1  τ2 and τ3 are as in Theorems 1  2  and 3  respectively  and ρ and θ are as in Algorithm 1.
From Corollary 1  DINGO can achieve (cid:107)gt(cid:107) ≤ ε with O(log(ε)/(τ ρθ)) communication rounds.
Moreover  the term τ is a lower bound on the step-size under all cases  which can determine the
maximum communication cost needed during line-search. For example  knowing τ could determine
the number of step-sizes used in backtracking line-search for DINGO in Section 4.

4 Experiments

In this section  we evaluate the empirical performance of DINGO  GIANT  DiSCO  InexactDANE 
AIDE  Asynchronous SGD (Async-SGD) and Synchronous SGD (Sync-SGD) [11] on the strongly
convex problem of softmax cross-entropy minimization with regularization on the CIFAR10 dataset
[21]  see Figure 1. This dataset has 50000 training samples  10000 test samples and each datapoint
xi ∈ R3072 has a label yi ∈ {1  2  . . .   10}. This problem has dimension d = 27648. In the
supplementary material  the reader can ﬁnd additional experiments on another softmax regression

8

AIDEAsynchronous SGDDINGODiSCOGIANTInexactDANESynchronous SGD0100200300400500Communication Rounds1.61.82.02.2Objective Function: f(w)0100200300400500Communication Rounds103102101100Gradient Norm: ||f(w)||020406080Iteration100Line Search: 0100200300400500Communication Rounds10152025303540Test Classification Accuracy (%)0100200300400500Communication Rounds1.61.82.02.2Objective Function: f(w)0100200300400500Communication Rounds103102101100Gradient Norm: ||f(w)||020406080Iteration101100Line Search: 0100200300400500Communication Rounds10152025303540Test Classification Accuracy (%)0100200300400500Communication Rounds1.61.71.81.92.02.12.22.3Objective Function: f(w)0100200300400500Communication Rounds102101100Gradient Norm: ||f(w)||020406080Iteration102101100Line Search: 0100200300400500Communication Rounds10152025303540Test Classification Accuracy (%)0100200300400500Communication Rounds1.61.71.81.92.02.12.22.3Objective Function: f(w)0100200300400500Communication Rounds101100Gradient Norm: ||f(w)||020406080Iteration105104103102101100Line Search: 0100200300400500Communication Rounds10152025303540Test Classification Accuracy (%)Figure 2: Softmax regression problem on the CIFAR10 dataset. We compare DINGO with θ =
10−4  10−1  1  10  100. All iterations are in Case 1 with θ = 10−4  which implies the same plot
would occur for all θ ≤ 10−4. Case 1 and Case 3 iterations occur when θ = 10−1  1. All iterations
under θ = 10  100 are in Case 3.

t i

as well as on a Gaussian mixture model and autoencoder problem. In all experiments we consider
(1) with (2)  where the sets S1  S2  . . .   Sm randomly partition the index set {1  2  . . .   n}  with each
having equal size s = n/m. Code is available at https://github.com/RixonC/DINGO.
We describe some implementation details. All sub-problem solvers are limited to 50 iterations and
do not employ preconditioning. For DINGO  we use the sub-problem solvers MINRES-QLP [22] 
†
†
˜Ht i)−1(Htgt)  respectively. We
t igt  ˜H
t i˜gt and ( ˜HT
LSMR [23] and CG [24] when computing H
˜Ht i)−1Htgt is guaranteed to satisfy
choose CG for the latter problem as the approximation x of ( ˜HT
t i
(cid:104)Htgt  x(cid:105) > 0 regardless of the number of CG iterations performed. For DINGO  unless otherwise
stated  we set θ = 10−4 and φ = 10−6. We use backtracking line search for DINGO and GIANT
to select the largest step-size in {1  2−1  2−2  . . .   2−50} which passes  with an Armijo line-search
parameter of 10−4. For InexactDANE  we set η = 1 and µ = 0  as in [15]  and use SVRG [25] as a
local solver with the best learning rate from {10−6  10−5  . . .   106}. We have each iteration of AIDE
invoke one iteration of InexactDANE  with the same parameters as in the stand-alone InexactDANE
method  and use the best catalyst acceleration parameter τ ∈ {10−6  10−5  . . .   106}  as in [15]. For
Async-SGD and Sync-SGD we report the best learning rate from {10−6  10−5  . . .   106} and each
worker uses a mini-batch of size n/(5m).
DiSCO has consistent performance  regardless of the number of workers  due to the distributed PCG
algorithm. This essentially allows DiSCO to perform Newton’s method over the full dataset. This
is unnecessarily costly  in terms of communication rounds  when s is reasonably large. Thus we
see it perform comparatively poorly in Plots 1(a)  1(b)  and 1(c). DiSCO outperforms GIANT and
DINGO in Plot 1(d). This is likely because the local directions (−H−1
t i gt and pt i for GIANT and
DINGO  respectively) give poor updates as they are calculated using very small subsets of data  i.e. 
in Plot 1(d) each worker has access to only 5 data points  while d = 27648.
A signiﬁcant advantage of DINGO to InexactDANE  AIDE  Async-SGD and Sync-SGD is that it is
relatively easy to tune hyper-parameters. Namely  making bad choices for ρ  θ and φ in DINGO will
give sub-optimal performance; however  it is still theoretically guaranteed to strictly decrease the
norm of the gradient. In contrast  some choices of hyper-parameters in InexactDANE  AIDE  Async-
SGD and Sync-SGD will cause divergence and these choices can be problem speciﬁc. Moreover 
these methods can be very sensitive to the chosen hyper-parameters with some being very difﬁcult
to select. For example  the acceleration parameter τ in AIDE was found to be difﬁcult and time
consuming to tune and the performance of AIDE was sensitive to it; notice the variation in selected τ
in Figure 1. This difﬁculty was also observed in [13  15]. We found that simply choosing ρ  θ and φ
to be small  in DINGO  gave high performance. Figure 2 compares different values of θ.
5 Future Work
The following is left for future work. First  extending the analysis of DINGO to include convergence
results under inexact update. Second  ﬁnding more efﬁcient methods of line search  for practical
implementations of DINGO  than backtracking line search. Using backtracking line search for
GIANT and DINGO requires the communication of some constant number of scalars and vectors 
respectively. Hence  for DINGO  it may transmit a large amount of data over the network  while still
only requiring two communication rounds per iteration of DINGO. Lastly  considering modiﬁcations
to DINGO that prevent convergence to a local maximum/saddle point in non-invex problems.

9

DINGO  with =0.0001DINGO  with =0.1DINGO  with =1DINGO  with =10DINGO  with =1000100200300400500Communication Rounds1.61.82.02.2Objective Function: f(w)0100200300400500Communication Rounds103102101100Gradient Norm: ||f(w)||0100200300400500Communication Rounds10152025303540Test Classification Accuracy (%)020406080100120Iteration10131010107104101Line Search: Acknowledgments

Both authors gratefully acknowledge the generous support by the Australian Research Council
(ARC) Centre of Excellence for Mathematical & Statistical Frontiers (ACEMS). Fred Roosta was
partially supported by DARPA as well as ARC through a Discovery Early Career Researcher Award
(DE180100923). Part of this work was done while Fred Roosta was visiting the Simons Institute for
the Theory of Computing.

References
[1] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  Cambridge  2014.

[2] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The Elements of Statistical Learning 

volume 1. Springer Series in Statistics New York  NY  USA  2001.

[3] Mehryar Mohri  Afshin Rostamizadeh  Ameet Talwalkar  and Francis Bach. Foundations of

Machine Learning. MIT Press  Cambridge  2012.

[4] Nan Ye  Farbod Roosta-Khorasani  and Tiangang Cui. Optimization Methods for Inverse

Problems  volume 2 of MATRIX Book Series. Springer  2017. arXiv:1712.00154.

[5] Farbod Roosta-Khorasani  Kees van den Doel  and Uri Ascher. Stochastic algorithms for
inverse problems involving PDEs and many measurements. SIAM J. Scientiﬁc Computing 
36(5):S3–S22  2014.

[6] Farbod Roosta-Khorasani  Kees van den Doel  and Uri Ascher. Data completion and stochastic
algorithms for PDE inversion problems with many measurements. Electronic Transactions on
Numerical Analysis  42:177–196  2014.

[7] Rasul Tutunov  Haitham Bou-Ammar  and Ali Jadbabaie. Distributed newton method for large-
scale consensus optimization. IEEE Transactions on Automatic Control  64(10):3983–3994 
2019.

[8] Ron Bekkerman  Mikhail Bilenko  and John Langford. Scaling up Machine Learning: Parallel

and Distributed Approaches. Cambridge University Press  Cambridge  2012.

[9] Yuchen Zhang and Xiao Lin. DiSCO: distributed optimization for self-concordant empirical

loss. In International Conference on Machine Learning  pages 362–370  2015.

[10] Amir Beck. First-Order Methods in Optimization. SIAM  2017.

[11] Jianmin Chen  Rajat Monga  Samy Bengio  and Rafal Jozefowicz. Revisiting distributed
synchronous SGD. In International Conference on Learning Representations Workshop Track 
2016.

[12] Roger Penrose. A generalized inverse for matrices. Mathematical Proceedings of the Cambridge

Philosophical Society  51(3):406–413  1955.

[13] Shusen Wang  Farbod Roosta-Khorasani  Peng Xu  and Michael W. Mahoney. GIANT: globally
improved approximate Newton method for distributed optimization. In Advances in Neural
Information Processing Systems  pages 2338–2348  2018.

[14] Ohad Shamir  Nati Srebro  and Tong Zhang. Communication-efﬁcient distributed optimization
using an approximate Newton-type method. In International Conference on Machine Learning 
pages 1000–1008  2014.

[15] Sashank J. Reddi  Jakub Koneˇcn`y  Peter Richtárik  Barnabás Póczós  and Alex Smola. AIDE:
fast and communication efﬁcient distributed optimization. arXiv preprint arXiv:1608.06879 
2016.

[16] Fred Roosta  Yang Liu  Peng Xu  and Michael W. Mahoney. Newton-MR: Newton’s method

without smoothness or convexity. arXiv preprint arXiv:1810.00303  2018.

10

[17] A. Ben-Israel and B. Mond. What is invexity? The ANZIAM Journal  28(1):1–9  1986.

[18] Shashi K. Mishra and Giorgio Giorgi. Invexity and Optimization. Springer Berlin Heidelberg 

Berlin  Heidelberg  2008.

[19] Yossi Arjevani  Ohad Shamir  and Ron Shiff. Oracle complexity of second-order methods for

smooth convex optimization. Mathematical Programming  pages 1–34  2017.

[20] John H. Hubbard and Barbara Burke Hubbard. Vector Calculus  Linear Algebra  and Differential

Forms: a Uniﬁed Approach. Matrix Editions  5 edition  2015.

[21] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report  2009.

[22] Sou-Cheng T. Choi  Christopher C. Paige  and Michael A. Saunders. MINRES-QLP: a Krylov
subspace method for indeﬁnite or singular symmetric systems. SIAM Journal on Scientiﬁc
Computing  33(4):1810–1836  2011.

[23] David Chin-Lung Fong and Michael Saunders. LSMR: an iterative algorithm for sparse least-

squares problems. SIAM Journal on Scientiﬁc Computing  33(5):2950–2971  2011.

[24] Jorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science & Business

Media  2006.

[25] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.

11

,Rixon Crane
Fred Roosta