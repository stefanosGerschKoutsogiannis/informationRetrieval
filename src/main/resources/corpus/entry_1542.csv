2010,Discriminative Clustering by Regularized Information Maximization,Is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classifier. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation  class balance and classifier complexity. The approach can flexibly incorporate different likelihood functions  express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular  we instantiate the framework to unsupervised  multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets  and demonstrates that RIM is an effective model selection method.,Discriminative Clustering by Regularized

Information Maximization

Ryan Gomes

gomes@vision.caltech.edu

Andreas Krause

krausea@caltech.edu

perona@vision.caltech.edu

California Institute of Technology

Pietro Perona

Pasadena  CA 91106

Abstract

Is there a principled way to learn a probabilistic discriminative classiﬁer from an
unlabeled data set? We present a framework that simultaneously clusters the data
and trains a discriminative classiﬁer. We call it Regularized Information Maxi-
mization (RIM). RIM optimizes an intuitive information-theoretic objective func-
tion which balances class separation  class balance and classiﬁer complexity. The
approach can ﬂexibly incorporate different likelihood functions  express prior as-
sumptions about the relative size of different classes and incorporate partial labels
for semi-supervised learning. In particular  we instantiate the framework to un-
supervised  multi-class kernelized logistic regression. Our empirical evaluation
indicates that RIM outperforms existing methods on several real data sets  and
demonstrates that RIM is an effective model selection method.

Introduction

1
Clustering algorithms group data items into categories without requiring human supervision or def-
inition of categories. They are often the ﬁrst tool used when exploring new data. A great number
of clustering principles have been proposed  most of which can be described as either generative
or discriminative in nature. Generative clustering algorithms provide constructive deﬁnitions of
categories in terms of their geometric properties in a feature space or as statistical processes for
generating data. Examples include k-means and Gaussian mixture model clustering. In order for
generative clustering to be practical  restrictive assumptions must be made about the underlying
category deﬁnitions.
Rather than modeling categories explicitly  discriminative clustering techniques represent the
boundaries or distinctions between categories. Fewer assumptions about the nature of categories
are made  making these methods powerful and ﬂexible in real world applications. Spectral graph
partitioning [1] and maximum margin clustering [2] are example discriminative clustering methods.
A disadvantage of existing discriminative approaches is that they lack a probabilistic foundation 
making them potentially unsuitable in applications that require reasoning under uncertainty or in
data exploration.
We propose a principled probabilistic approach to discriminative clustering  by formalizing the
problem as unsupervised learning of a conditional probabilistic model. We generalize the work of
Grandvalet and Bengio [3] and Bridle et al. [4] in order to learn probabilistic classiﬁers that are
appropriate for multi-class discriminative clustering  as explained in Section 2. We identify two
fundamental  competing quantities  class balance and class separation  and develop an information
theoretic objective function which trades off these quantities. Our approach corresponds to
maximizing mutual information between the empirical distribution on the inputs and the induced

1

label distribution  regularized by a complexity penalty. Thus  we call our approach Regularized
Information Maximization (RIM).
In summary  our contribution is RIM  a probabilistic framework for discriminative clustering with
a number of attractive properties. Thanks to its probabilistic formulation  RIM is ﬂexible:
it is
compatible with diverse likelihood functions and allows speciﬁcation of prior assumptions about
expected class proportions. We show how our approach leads to an efﬁcient  scalable optimization
procedure that also provides a means of automatic model selection (determination of the number
of clusters). RIM is easily extended to semi-supervised classiﬁcation. Finally  we show that RIM
performs better than competing approaches on several real-world data sets.
2 Regularized Information Maximization
Suppose we are given an unlabeled dataset of N feature vectors (datapoints) X = (x1 ···   xN ) 
where xi = (xi1  . . .   xiD)T ∈ RD are D-dimensional vectors with components xid. Our goal is
to learn a conditional model p(y|x  W) with parameters W which predicts a distribution over label
values y ∈ {1  . . .   K} given an input vector x.
Our approach is to construct a functional F (p(y|x  W); X  λ) which evaluates the suitability of
p(y|x  W) as a discriminative clustering model. We then use standard discriminative classiﬁers
such as logistic regression for p(y|x  W)  and maximize the resulting function F (W; X  λ) over
the parameters W. λ is an additional tuning parameter that is ﬁxed during optimization.
We are guided by three principles when constructing F (p(y|x  W); X  λ). The ﬁrst is that the dis-
criminative model’s decision boundaries should not be located in regions of the input space that are
P
densely populated with datapoints. This is often termed the cluster assumption [5]  and also corre-
sponds to the idea that datapoints should be classiﬁed with large margin. Grandvalet & Bengio [3]
show that a conditional entropy term − 1
i H{p(y|xi  W)} very effectively captures the cluster
assumption when training probabilistic classiﬁers with partial labels. However  in the case of fully
unsupervised learning this term alone is not enough to ensure sensible solutions  because conditional
entropy may be reduced by simply removing decision boundaries and unlabeled categories tend to
be removed. We illustrate this in Figure 1 (left) with an example using the multilogit regression
classiﬁer as the conditional model p(y|x  W)  which we will develop in Section 3.
In order to avoid degenerate solutions  we incorporate the notion of class balance: we prefer con-
ﬁgurations in which category labels are assigned evenly across the dataset. We deﬁne the empirical
label distribution

N

Z

ˆp(y; W) =

ˆp(x)p(y|x  W)dx =

p(y|xi  W) 

X

i

1
N

X

i

which is an estimate of the marginal distribution of y. A natural way to encode our preference
towards class balance is to use the entropy H{ˆp(y; W)}  because it is maximized when the labels
are uniformly distributed. Combining the two terms  we arrive at

IW{y; x}= H{ˆp(y; W)}− 1
N

H{p(y|xi  W)}

(1)

which is the empirical estimate of the mutual information between x and y under the conditional
model p(y|x  W).
Bridle et al. [4] were the ﬁrst to propose maximizing IW{y; x} in order to learn probabilistic classi-
ﬁers without supervision. However  they note that IW{y; x} may be trivially maximized by a con-
ditional model that classiﬁes each data point xi into its own category yi  and that classiﬁers trained
with this objective tend to fragment the data into a large number of categories  see Figure 1 (center).
We therefore introduce a regularizing term R(W; λ) whose form will depend on the speciﬁc choice
of p(y|x  W). This term penalizes conditional models with complex decision boundaries in order
to yield sensible clustering solutions. Our objective function is

F (W; X  λ) = IW{y; x} − R(W; λ)

(2)
and we therefore refer to our approach as Regularized Information Maximization (RIM)  see Figure 1
(right). While we motivated this objective with notions of class balance and seperation  our approach
may be interpreted as learning a conditional distribution for y that preserves information from the
data set  subject to a complexity penalty.

2

Grandvalet & Bengio [3]

Bridle et al. [4]

RIM

s
n
o
i
g
e
R
n
o
i
s
i
c
e
D

y
p
o
r
t
n
E

.

d
n
o
C

Figure 1: Example unsupervised multilogit regression solutions on a simple dataset with three clus-
ters. The top and bottom rows show the category label arg maxy p(y|x  W) and conditional entropy
H{p(y|x  W)} at each point x  respectively. We ﬁnd that both class balance and regularization
terms are necessary to learn unsupervised classiﬁers suitable for multi-class clustering.

3 Example application: Unsupervised Multilogit Regression
The RIM framework is ﬂexible in the choice of p(y | x; W) and R(W; λ). As an example instan-
tiation  we here choose multiclass logistic regression as the conditional model. Speciﬁcally  if K is
the maximum number of classes  we choose
p(y = k|x  W) ∝ exp(wT

(3)
where the set of parameters W = {w1  . . .   wK; b1  . . .   bK} consists of weight vectors wk and
bias values bk for each class k. Each weight vector wk ∈ RD is D-dimensional with components
wkd. The regularizer is the squared L2 norm of the weight vectors  and may be interpreted as an
isotropic normal distribution prior on the weights W. The bias terms are not penalized.
In order to optimize Eq. 2 specialized with Eqs. 3  we require the gradients of the objective function.
For clarity  we deﬁne pki ≡ p(y = k|xi  W)  and ˆpk ≡ ˆp(y = k; W). The partial derivatives are

k x + bk) and R(W; λ) = λ

X

k wk 

wT

k

∂F
∂wkd

=

1
N

∂pci
∂wkd

log pci
ˆpc

− 2λwkd and ∂F
∂bk

=

1
N

∂pci
∂bk

log pci
ˆpc

.

(4)

X

ic

X

ic

Naive computation of the gradient requires O(N K 2D)  since there are K(D + 1) parameters and
each derivative requires a sum over N K terms. However  the form of the conditional probability
derivatives for multi-logit regression are:

∂pci
∂wkd

= (δkc − pci)pkixid and ∂pci
∂bk

= (δkc − pci)pki 

(cid:16)

where δkc is equal to one when indices k and c are equal  and zero otherwise. When these expres-
sions are substituted into Eq. 4  we ﬁnd the following expressions:
pci log pci
ˆpc

(cid:17) − 2λwkd
Computing the gradient requires only O(N KD) operations since the termsP

−X
−X

X
X

pci log pci
ˆpc

log pki
ˆpk

log pki
ˆpk

1
N
1
N

∂F
∂wkd

∂F
∂bk

xidpki

(cid:16)

(cid:17)

i

i

=

=

c

c

(5)

pki

c pci log pci
ˆpc

may be

computed once and reused in each partial derivative expression.
The above gradients are used in the L-BFGS [6] quasi-Newton optimization algorithm1. We ﬁnd em-
pirically that the optimization usually converges within a few hundred iterations. When specialized
1We used Mark Schmidt’s implementation at http://www.cs.ubc.ca/∼schmidtm/Software/

minFunc.html.

3

x1x2−2−1012−2−1012x1x2−2−1012−2−1012x1x2−2−1012−2−1012x1x2  −2−1012−2−10120.10.20.30.40.50.6x1x2  −2−1012−2−101200.20.40.60.8x1x2  −2−1012−2−10120.20.40.60.81Figure 2: Demonstration of model selection on the toy problem from Figure 1. The algorithm is
initialized with 50 category weight vectors wk. Upon convergence  only three of the categories
are populated with data examples. The negative bias terms of the unpopulated categories drive the
unpopulated class probabilities ˆpk towards zero. The corresponding weight vectors wk have norms
near zero.

to multilogit regression  the objective function F (W; x  λ) is non-concave. Therefore the algorithm
can only be guaranteed to halt at locally optimal stationary points of F . In Section 3.1  we explain
how we can obtain an initialization that is robust against local optima.
3.1 Model Selection
Setting the derivatives (Eq. 5) equal to zero yields the following condition at stationary points of F :

wk =X
(cid:16)

log pki
ˆpk

i

α0
kixi

−X

c

(cid:17)

.

pci log pci
ˆpc

(6)

(7)

(cid:16)X

i

4

where we have deﬁned

ki ≡ 1
α0
2λN

pki

k wk =P

kjxT

ij α0

kiα0

kjxT

ij α0

P
k wk = P
i pki → 0. This implies that pki → 0 for all i  and therefore α0

The L2 regularizing function R(W; λ) in Eq. 3 is additively composed of penalty terms associated
i xj. It is instructive to observe the limiting behavior
with each category: wT
k wk when datapoints are not assigned to category k; that is  when ˆpk =
of the penalty term wT
ki → 0 for all i. Finally 
1
i xj → 0. This means that the regularizing function does not penalize
N
kiα0
wT
unpopulated categories.
We ﬁnd empirically that when we initialize with a large number of category weights wk  many de-
cay away depending on the value of λ. Typically as λ increases  fewer categories are discovered.
This may be viewed as model selection (automatic determination of the number of categories) since
the regularizing function and parameter λ may be interpreted as a form of prior on the weight pa-
rameters. The bias terms bk are unpenalized and are adjusted during optimization to drive the class
probablities ˆpk arbitrarily close to zero for unpopulated classes. This is illustrated in Figure 2.
This behavior suggests an effective initialization procedure for our algorithm. We ﬁrst oversegment
the data into a large number of clusters (using k-means or other suitable algorithm) and train a
supervised multi-logit classiﬁer using these cluster labels. (This initial classiﬁer may be trained with
a small number of L-BFGS iterations since it only serves as a starting point.) We then use this
classiﬁer as the starting point for our RIM algorithm and optimize with different values of λ in order
to obtain solutions with different numbers of clusters.
4 Example Application: Unsupervised Kernel Multilogit Regression
The stationary conditions have another interesting consequence. Equation 6 indicates that at sta-
tionary points  the weights are located in the span of the input datapoints. We use this insight as
i αkixi during
optimization. Substituting this equation into the multilogit regression conditional likelihood allows
i αkiK(xi  x)  where K is a positive deﬁnite kernel
replacement of all inner products wT
function that evaluates the inner product xT

justiﬁcation to deﬁne explicit coefﬁcients αki and enforce the constraint wk = P

k x withP

i x. The conditional model now has the form

p(y = k|x  α  b) ∝ exp

αkiK(xi  x) + bk

.

(cid:17)

0204000.20.40.6Class ProbabilitiesClass Index02040−30−20−1001020BiasClass Indexbk02040051015Weight Vector NormswTk wkClass IndexSubstituting the constraint into the regularizing functionP
k wk by the Reproducing Hilbert Space (RKHS) norm of the functionP

k wT

wT

k wk yields a natural replacement of

i αkiK(xi ·):

αkiαkjK(xi  xj).

(8)

R(α) =X
(cid:16)

k

X
−X

ij

c

X

i

∂F
∂αkj

=

1
N

We use the L-BFGS algorithm to optimize the kernelized algorithm over the coefﬁcients αki and
biases bk. The partial derivatives for the kernel coefﬁcients are

K(xj  xi)pki

log pki
ˆpk

pci log pci
ˆpc

αkiK(xj  xi)

(cid:17) − 2λ

X

i

and the derivatives for the biases are unchanged. The gradient of the kernelized algorithm requires
O(KN 2) to compute. Kernelized unsupervised multilogit regression exhibits the same model
selection behavior as the linear algorithm.
5 Extensions
We now discuss how RIM can be extended to semi-supervised classiﬁcation  and to encode prior
assumptions about class proportions.
5.1 Semi-supervised Classiﬁcation

1  ···   xU

there are unlabeled examples XU =
In semi-supervised classiﬁcation  we assume that
{xU
M} with labels Y = {y1 ···   yM}.
N} as well as labeled examples XL = {xL
We again use mutual information IW{y; x} (Eq. 1) to deﬁne the relationship between unlabeled
points and the model parameters  but we incorporate an additional parameter τ which will deﬁne
the tradeoff between labeled and unlabeled examples. The conditional likelihood is incorporated for
labeled examples to yield the semi-supervised objective:

S(W; τ  λ) =τ IW{y; x} − R(W; λ) +X

log p(yi|xL

1  ···   xL

i   W)

The gradient is computed and again used in the L-BFGS algorithm in order to optimize this com-
bined objective. Our approach is related to the objective in [3]  which does not contain the class
balance term H(ˆp(y; W)).
5.2 Encoding Prior Beliefs about the Label Distribution

i

X

So far  we have motivated our choice for the objective function F through the notion of class balance.
However  in many classiﬁcation tasks  different classes have different number of members. In the
following  we show how RIM allows ﬂexible expression of prior assumptions about non-uniform
class label proportions.
First  note that the following basic identity holds

(9)
where U is the uniform distribution over the set of labels {1 ···   K}. Substituting the identity  then
dropping the constant log(K) yields another interpretation of the objective

H{ˆp(y; W)} = log(K) − KL{ˆp(y; W)||U}

i

F (W; X  λ) = − 1
N

H{p(y|xi  W)} − KL{ˆp(y; W)||U} − R(W; λ).

(10)
The term −KL{ˆp(y; W)||U} is maximized when the average label distribution is uniform. We
can capture prior beliefs about the average label distribution by substituting a reference distribution
D(y; γ) in place of U (γ is a parameter that may be ﬁxed or optimized during learning). [7] also use
relative entropy as a means of enforcing prior beliefs  although not with respect to class distributions
in multi-class classiﬁcation problems.
This construction may be used in a clustering task in which we believe that the cluster sizes obey
a power law distribution as  for example  considered by [8] who use the Pitman-Yor process for
nonparametric language modeling. Simple manipulation yields the following objective:

where H{ˆp(y; W)||D(y; γ)} is the cross entropy −P

F (W; X  λ  γ) = IW{x; y} − H{ˆp(y; W)||D(y; γ)} − R(W; λ)

k ˆp(y = k; W) log D(y = k; γ). We there-
fore ﬁnd that label distribution priors may be incorporated using an additional cross entropy regu-
larization term.

5

Figure 3: Unsupervised Clustering: Adjusted Rand Index (relative to ground truth) versus number
of clusters.

6 Experiments
We empirically evaluate our RIM approach on several real data sets  in both fully unsupervised and
semisupervised conﬁgurations.
6.1 Unsupervised Learning
Kernelized RIM is initialized according to the procedure outlined in Section 3.1  and run until L-
BFGS converges. Unlabeled examples are then clustered according to arg maxk p(y = k|x  W).
We compare RIM against the spectral clustering (SC) algorithm of [1]  the fast maximum margin
clustering (MMC) algorithm of [9]  and kernelized k-means [10]. MMC is a binary clustering algo-
rithm. We use the recursive scheme outlined by [9] to extend the approach to multiple categories.
The MMC algorithm requires an initial clustering estimate for initialization  and we use SC to pro-
vide this.
We evaluate unsupervised clustering performance in terms of how well the discovered clusters reﬂect
known ground truth labels of the dataset. We report the Adjusted Rand Index (ARI) [11] between an
inferred clustering and the ground truth categories. ARI has a maximum value of 1 when two clus-
terings are identical. We evaluated a number of other measures for comparing clusterings to ground
truth including mutual information  normalized mutual information [12]  and cluster impurity [13].
We found that the relative rankings of the algorithms were the same as indicated by ARI.
We evaluate the performance of each algorithm while varying the number of clusters that are dis-
covered  and we plot ARI for each setting. For SC and k-means the number of clusters is given as
an input parameter. MMC is evaluated at {2  4  8 ···} clusters (powers of two  due to the recursive
scheme.) For RIM  we sweep the regularization parameter λ and allow the algorithm to discover the
ﬁnal number of clusters.
Image Clustering. We test the algorithms on an image clustering task with 350 images from four
Caltech-256 [14] categories (Faces-Easy  Motorbikes  Airplanes  T-Shirt) for a total of N = 1400
images. We use the Spatial Pyramid Match kernel [15] computed between every pair of images.
We sweep RIM’s λ parameter across [ 0.125
N ]. The results are summarized in ﬁgure 3. Overall 
the clusterings that best match ground truth are given by RIM when it discovers four clusters. We
ﬁnd that RIM outperforms both SC and MMC at all settings. RIM outperforms kernelized k-means
when discovering between 4 and 8 clusters. Their performances are comparable for other numbers
of clusters. Figure 4 shows example images taken from clusters discovered by RIM. Our RIM
implementation takes approximately 110 seconds per run on the Caltech Images datset on a quad
core Intel Xeon server. SC requires 38 seconds per run  while MMC requires 44-51 seconds per run
depending on the number of clusters speciﬁed.

N   4

Molecular Graph Clustering. We further test RIM’s unsupervised learning performance on two
molecular graph datasets. D&D [16] contains N = 1178 protein structure graphs with binary
ground truth labels indicating whether or not they function as enzymes. NCI109 [17] is composed
of N = 4127 compounds labeled according to whether or not they are active in an anti-cancer
screening. We use the subtree kernel developed by [18] with subtree height of 1. For D&D  we
N ] and for NCI we sweep through the
sweep RIM’s lambda parameter through the range [ 0.001
interval [ 0.001
N ]. Results are summarized in Figures 3 (center and right). We ﬁnd that of all
methods  RIM produces the clusterings that are nearest to ground truth (when discovering 2 clusters

N   0.05

N   1

6

2468101214160.20.30.40.50.60.70.80.91# of clustersAdjusted Rand IndexCaltech ImagesMMCk−meansRIMSC2468101214161820−0.0500.050.10.150.2# of clustersAdjusted Rand IndexDandD GraphsRIMk−meansMMCSC24681012141618202200.010.020.030.040.050.06# of clustersAdjusted Rand IndexNCI109 GraphsRIMk−meansSCMMCC1

C2

C3
C4
C5

Figure 4: Left: Randomly chosen example images from clusters discovered by unsupervised RIM
on Caltech Image. Right: Semi-supervised learning on Caltech Images.

Average Waveform

Most Uncertain Waveform

Figure 5: Left  Tetrode dataset average waveform. Right  the waveform with the most uncertain
cluster membership according to the classiﬁer learned by RIM.

for D&D and 5 clusters for NCI109). RIM outperforms both SC and MMC at all settings. RIM has
the advantage over k-means when discovering a small number of clusters and is comparable at other
settings. On NCI109  RIM required approximately 10 minutes per run. SC required approximately
13 minutes  while MMC required on average 18 minutes per run.

Neural Tetrode Recordings. We demonstrate RIM on a large scale data set of 319  209 neural
activity waveforms recorded from four co-located electrodes implanted in the hippocampus of a
behaving rat. The waveforms are composed of 38 samples from each of the four electrodes and are
the output of a neural spike detector which aligns signal peaks to the 13-th sample  see the average
waveform in Figure 5 (left). We concatenate the samples into a single 152-dimensional vector and
preprocess by subtracting the mean waveform and divide each vector component by its variance.
We use the linear RIM algorithm given in Section 3  initialized with 100 categories. We set λ to 4
N
and RIM discovers 33 clusters and ﬁnishes in 12 minutes. There is no ground truth available for this
dataset  but we use it to demonstrate RIM’s efﬁcacy as a data exploration tool. Figure 6 shows two
clusters discovered by RIM. The top row consists of cluster member waveforms superimposed on
each other  with the cluster’s mean waveform plotted in red. We ﬁnd that the clustered waveforms
have substantial similarity to each other. Taken as a whole  the clusters give an idea of the typical
waveform patterns. The bottom row shows the learned classiﬁer’s discriminative weights wk for
each category  which can be used to gain a sense for how the cluster’s members differ from the
dataset mean waveform. We can use the probabilistic classiﬁer learned by RIM to discover atypical
waveforms by ranking them according to their conditional entropy H{p(y|xi  W)}. Figure 5 (right)
shows the waveform whose cluster membership is most uncertain.

Cluster 1

Cluster 2

e
v
a
W

.
s
t

W

Figure 6: Two clusters discovered by RIM on the Tetrode data set. Top row: Superimposed
waveform members  with cluster mean in red. Bottom row: The discriminative category weights
wk associated with each cluster.

7

0501001500.650.70.750.80.850.90.95Number of labeled examplesTest AccuracyClassification PerformanceGrandvalet & BengioSupervisedRIM6.2 Semi-supervised Classiﬁcation
We test our semi-supervised classiﬁcation method described in Section 5.1 against [3] on the Cal-
tech Images dataset. The methods were trained using both unlabeled and labeled examples  and
classiﬁcation performance is assessed on the unlabeled portion. As a baseline  a supervised classi-
ﬁer was trained on labeled subsets of the data and tested on the remainder. Parameters were selected
via cross-validation on a subset of the labeled examples. The results are summarized in Figure 4.
We ﬁnd that both semi-supervised methods signiﬁcantly improve classiﬁcation performance rela-
tive to the supervised baseline when the number of labeled examples is small. Additionally  we
ﬁnd that RIM outperforms Grandvalet & Bengio. This suggests that incorporating prior knowledge
about class size distributions (in this case  we use a uniform prior) may be useful in semi-supervised
learning.
7 Related Work
Our work has connections to existing work in both unsupervised learning and semi-supervised clas-
siﬁcation.
Unsupervised Learning. The information bottleneck method [19] learns a conditional model
p(y|x) where the labels y form a lossy representation of the input space x  while preserving in-
formation about a third “relevance” variable z. The method maximizes I(y; z) − λI(x; y)  whereas
we maximize the information between y and x while constraining complexity with a parametric
regularizer. The method of [20] aims to maximize a similarity measure computed between members
within the same cluster while penalizing the mutual information between the cluster label y and the
input x. Again  mutual information is used to enforce a lossy representation of y|x. Song et al. [22]
also view clustering as maximization of the dependence between the input variable and output la-
bel variable. They use the Hilbert-Schmidt Independence Criterion as a measure of dependence 
whereas we use Mutual Information.
There is also an unsupervised variant of the Support Vector Machine  called max-margin cluster-
ing. Like our approach  the works of [2] and [21] use notions of class balance  seperation  and
regularization to learn unsupervised discriminative classiﬁers. However  they are formulated in the
max-margin framework rather than our probabilistic approach. Ours appears more amenable to
incorporating prior beliefs about the class labels. Unsupervised SVMs are solutions to a convex
relaxation of a non-convex problem  while we directly optimize our non-convex objective. The
semideﬁnite programming methods required are much more expensive than our approach.
Semi-supervised Classiﬁcation. Our semi-supervised objective is related to [3]  as discussed in
section 5.1. Another semi-supervised method [23] uses mutual information as a regularizing term to
be minimized  in contrast to ours which attempts to maximize mutual information. The assumption
underlying [23] is that any information between the label variable and unlabeled examples is an
artifact of the classiﬁer and should be removed. Our method encodes the opposite assumption:
there may be variability (e.g. new class label values) not captured by the labeled data  since it is
incomplete.
8 Conclusions
We considered the problem of learning a probabilistic discriminative classiﬁer from an unlabeled
data set. We presented Regularized Information Maximization (RIM)  a probabilistic framework
for tackling this challenge. Our approach consists of optimizing an intuitive information theoretic
objective function that incorporates class separation  class balance and classiﬁer complexity  which
may be interpreted as maximizing the mutual information between the empirical input and implied
label distributions. The approach is ﬂexible  in that it allows consideration of different likelihood
functions. It also naturally allows expression of prior assumptions about expected label proportions
by means of a cross-entropy with respect to a reference distribution. Our framework allows
natural incorporation of partial labels for semi-supervised learning. In particular  we instantiate the
framework to unsupervised  multi-class kernelized logistic regression. Our empirical evaluation
indicates that RIM outperforms existing methods on several real data sets  and demonstrates that
RIM is an effective model selection method.
Acknowledgements
We thank Alex Smola for helpful comments and discussion  and Thanos Siapas for providing the neural tetrode
data. This research was partially supported by NSF grant IIS-0953413  a gift from Microsoft Corporation  and
ONR MURI Grant N00014-06-1-0734.

8

References
[1] A. Ng  M. I. Jordan  and Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS 

2001.

[2] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-

chines. In AAAI  2005.

[3] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS 

2004.

[4] John S. Bridle  Anthony J. R. Heading  and David J. C. MacKay. Unsupervised classiﬁers 
mutual information and ‘phantom targets’. In John E. Moody  Steve J. Hanson  and Richard P.
Lippmann  editors  Advances in Neural Information Processing Systems  volume 4  pages
1096–1101. Morgan Kaufmann Publishers  Inc.  1992.

[5] Olivier Chapelle and Alexander Zien. Semi-supervised classiﬁcation by low density separa-

tion  September 2004.

[6] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization.

Mathematical Programming  45:503–528  1989.

[7] T. Jaakkola  M. Meila  and T. Jebara. Maximum entropy discrimination. In NIPS  1999.
[8] Y. W. Teh. A hierarchical bayesian language model based on pitman-yor processes. In ACL 

2006.

[9] K. Zhang  I. W. Tsang  and J. T. Kwok. Maximum margin clustering made practical. In ICML 

2007.

[10] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge

University Press  New York  NY  USA  2004.

[11] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classiﬁcation  2:193–

218  1985.

[12] Alexander Strehl and Joydeep Ghosh. Cluster ensembles — A knowledge reuse framework for

combining multiple partitions. Journal of Machine Learning Research  3:583–617  2002.

[13] Y. Chen  J. Ze Wang  and R. Krovetz. CLUE: cluster-based retrieval of images by unsupervised

learning. IEEE Trans. Image Processing  14(8):1187–1201  2005.

[14] G. Grifﬁn  A. Holub  and P. Perona. Caltech-256 object category dataset. Technical Report

7694  California Institute of Technology  2007.

[15] S. Lazebnik  C. Schmid  and J. Ponce. Beyond bags of features: Spatial pyramid matching for

recognizing natural scene categories. In CVPR  2006.

[16] P. D. Dobson and A. J. Doig. Distinguishing enzyme structures from non-enzymes without

alignments. J. Mol. Biol.  330:771–783  Jul 2003.

[17] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound

retrieval and classiﬁcation. In ICDM  pages 678–689  2006.

[18] N. Shervashidze and K. M. Borgwardt. Fast subtree kernels on graphs. In NIPS  2010.
[19] N. Tishby  F. C. Pereira  and W. Bialek. The information bottleneck method. CoRR 

physics/0004057  2000.

[20] N. Slonim  G. S. Atwal  G. Tkacik  and W. Bialek. Information-based clustering. Proc Natl

Acad Sci U S A  102(51):18297–18302  December 2005.

[21] Francis Bach and Za¨ıd Harchaoui. DIFFRAC: a discriminative and ﬂexible framework for
clustering. In John C. Platt  Daphne Koller  Yoram Singer  and Sam T. Roweis  editors  NIPS.
MIT Press  2007.

[22] Le Song  Alex Smola  Arthur Gretton  and Karsten M. Borgwardt. A dependence maximization
view of clustering. In ICML ’07: Proceedings of the 24th international conference on Machine
learning  pages 815–822  New York  NY  USA  2007. ACM.

[23] A. Corduneanu and T. Jaakkola. On information regularization. In UAI  2003.

9

,Weiran Wang
Jialei Wang
Dan Garber
Dan Garber
Nati Srebro
Gamaleldin Elsayed
Dilip Krishnan
Hossein Mobahi
Kevin Regan
Samy Bengio