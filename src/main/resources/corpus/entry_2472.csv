2016,Higher-Order Factorization Machines,Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately  despite increasing interest in FMs  there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper  we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters  which greatly reduce model size and prediction times while maintaining similar accuracy.  We demonstrate the proposed approaches on four different link prediction tasks.,Higher-OrderFactorizationMachinesMathieuBlondel AkinoriFujino NaonoriUedaNTTCommunicationScienceLaboratoriesJapanMasakazuIshihataHokkaidoUniversityJapanAbstractFactorizationmachines(FMs)areasupervisedlearningapproachthatcanusesecond-orderfeaturecombinationsevenwhenthedataisveryhigh-dimensional.Unfortunately despiteincreasinginterestinFMs thereexiststodatenoefﬁcienttrainingalgorithmforhigher-orderFMs(HOFMs).Inthispaper wepresenttheﬁrstgenericyetefﬁcientalgorithmsfortrainingarbitrary-orderHOFMs.WealsopresentnewvariantsofHOFMswithsharedparameters whichgreatlyre-ducemodelsizeandpredictiontimeswhilemaintainingsimilaraccuracy.Wedemonstratetheproposedapproachesonfourdifferentlinkpredictiontasks.1IntroductionFactorizationmachines(FMs)[13 14]areasupervisedlearningapproachthatcanusesecond-orderfeaturecombinationsefﬁcientlyevenwhenthedataisveryhigh-dimensional.ThekeyideaofFMsistomodeltheweightsoffeaturecombinationsusingalow-rankmatrix.Thishastwomainbeneﬁts.First FMscanachieveempiricalaccuracyonaparwithpolynomialregressionorkernelmethodsbutwithsmallerandfastertoevaluatemodels[4].Second FMscaninfertheweightsoffeaturecombinationsthatwerenotobservedinthetrainingset.Thissecondpropertyiscrucialforinstanceinrecommendersystems adomainwhereFMshavebecomeincreasinglypopular[14 16].Withoutthelow-rankproperty FMswouldfailtogeneralizetounseenuser-iteminteractions.Unfortunately althoughhigher-orderFMs(HOFMs)werebrieﬂymentionedintheoriginalworkof[13 14] thereexiststodatenoefﬁcientalgorithmfortrainingarbitrary-orderHOFMs.Infact evenjustcomputingpredictionsgiventhemodelparametersnaivelytakespolynomialtimeinthenumberoffeatures.Forthisreason HOFMshave toourknowledge neverbeenappliedtoanyproblem.Inaddition HOFMs asoriginallydeﬁnedin[13 14] modeleachdegreeinthepolynomialexpansionwithadifferentmatrixandthereforerequiretheestimationofalargenumberofparameters.Inthispaper weproposetheﬁrstefﬁcientalgorithmsfortrainingarbitrary-orderHOFMs.Todoso werelyonalinkbetweenFMsandtheso-calledANOVAkernel[4].Weproposelinear-timedynamicprogrammingalgorithmsforevaluatingtheANOVAkernelandcomputingitsgradient.Basedonthese weproposestochasticgradientandcoordinatedescentalgorithmsforarbitrary-orderHOFMs.Toreducethenumberofparameters aswellaspredictiontimes wealsointroducetwonewkernelsderivedfromtheANOVAkernel allowingustodeﬁnenewvariantsofHOFMswithsharedparameters.Wedemonstratetheproposedapproachesonfourdifferentlinkpredictiontasks.2Factorizationmachines(FMs)Second-orderFMs.Factorizationmachines(FMs)[13 14]areanincreasinglypopularmethodforefﬁcientlyusingsecond-orderfeaturecombinationsinclassiﬁcationorregressiontasksevenwhenthedataisveryhigh-dimensional.Letw∈RdandP∈Rd×k wherek∈Nisarankhyper-parameter.WedenotetherowsofPby¯pjanditscolumnsbyps forj∈[d]ands∈[k] 30thConferenceonNeuralInformationProcessingSystems(NIPS2016) Barcelona Spain.where[d]:={1 ... d}.Then FMspredictanoutputy∈Rfromavectorx=[x1 ... xd]TbyˆyFM(x):=hw xi+Xj0>jh¯pj ¯pj0ixjxj0.(1)Animportantcharacteristicof(1)isthatitconsidersonlycombinationsofdistinctfeatures(i.e. thesquaredfeaturesx21 ... x2dareignored).ThemainadvantageofFMscomparedtonaivepolynomialregressionisthatthenumberofparameterstoestimateisO(dk)insteadofO(d2).Inaddition wecancomputepredictionsinO(2dk)time1usingˆyFM(x)=wTx+12(cid:16)kPTxk2−kXs=1kps◦xk2(cid:17) where◦indicateselement-wiseproduct[3].GivenatrainingsetX=[x1 ... xn]∈Rd×nandy=[y1 ... yn]T∈Rn wandPcanbelearnedbyminimizingthefollowingnon-convexobjective1nnXi=1‘(yi ˆyFM(xi))+β12kwk2+β22kPk2 (2)where‘isaconvexlossfunctionandβ1>0 β2>0arehyper-parameters.Thepopularlibfmlibrary[14]implementsefﬁcientstochasticgradientandcoordinatedescentalgorithmsforobtainingastationarypointof(2).BothalgorithmshavearuntimecomplexityofO(2dkn)perepoch.Higher-orderFMs(HOFMs).Althoughnotrainingalgorithmwasprovided FMswereextendedtohigher-orderfeaturecombinationsintheoriginalworkof[13 14].LetP(t)∈Rd×kt wheret∈{2 ... m}istheorderordegreeoffeaturecombinationsconsidered andkt∈Nisarankhyper-parameter.Let¯p(t)jbethejthrowofP(t).Thenm-orderHOFMscanbedeﬁnedasˆyHOFM(x):=hw xi+Xj0>jh¯p(2)j ¯p(2)j0ixjxj0+···+Xjm>···>j1h¯p(m)j1 ... ¯p(m)jmixj1xj2...xjm(3)wherewedeﬁnedh¯p(t)j1 ... ¯p(t)jti:=sum(¯p(t)j1◦···◦¯p(t)jt)(sumofelement-wiseproducts).TheobjectivefunctionofHOFMscanbeexpressedinasimilarwayasfor(2):1nnXi=1‘(yi ˆyHOFM(xi))+β12kwk2+mXt=2βt2kP(t)k2 (4)whereβ1 ... βm>0arehyper-parameters.Toavoidthecombinatorialexplosionofhyper-parametercombinationstosearch inourexperimentswewillsimplysetβ1=···=βmandk2=···=km.While(3)looksquitedaunting [4]recentlyshowedthatFMscanbeexpressedfromasimplerkernelperspective.LetusdeﬁnetheANOVA2kernel[19]ofdegree2≤m≤dbyAm(p x):=Xjm>···>j1mYt=1pjtxjt.(5)Forlaterconvenience wealsodeﬁneA0(p x):=1andA1(p x):=hp xi.ThenitisshownthatˆyHOFM(x)=hw xi+k2Xs=1A2(cid:16)p(2)s x(cid:17)+···+kmXs=1Am(cid:16)p(m)s x(cid:17) (6)wherep(t)sisthesthcolumnofP(t).ThisperspectiveshowsthatwecanviewFMsandHOFMsasatypeofkernelmachinewhose“supportvectors”arelearneddirectlyfromdata.Intuitively theANOVAkernelcanbethoughtasakindofpolynomialkernelthatusesfeaturecombinationswithoutreplacement(i.e. ofdistinctfeatures).AkeypropertyoftheANOVAkernelismulti-linearity[4]:Am(p x)=Am(p¬j x¬j)+pjxjAm−1(p¬j x¬j) (7)wherep¬jdenotesthe(d−1)-dimensionalvectorwithpjremovedandsimilarlyforx¬j.Thatis everythingelsekeptﬁxed Am(p x)isanafﬁnefunctionofpj∀j∈[d].Althoughnotraining1Weincludetheconstantfactorforfairlatercomparisonwitharbitrary-orderHOFMs.2ThenamecomesfromtheANOVAdecompositionoffunctions.[20 19]2algorithmwasprovided [4]showedbasedon(7)that althoughnon-convex theobjectivefunctionofarbitrary-orderHOFMsisconvexinwandineachrowofP(2) ... P(m) separately.InterpretabilityofHOFMs.AnadvantageofFMsandHOFMsistheirinterpretability.Toseewhythisisthecase noticethatwecanrewrite(3)asˆyHOFM(x)=hw xi+Xj0>jW(2)j j0xjxj0+···+Xjm>···>j1W(m)j1 ... jmxj1xj2...xjm wherewedeﬁnedW(t):=Pkts=1p(t)s⊗···⊗p(t)s|{z}ttimes.Intuitively W(t)∈Rdtisalow-rankt-waytensorwhichcontainstheweightsoffeaturecombinationsofdegreet.Forinstance whent=3 W(3)i j kistheweightofxixjxk.SimilarlytotheANOVAdecompositionoffunctions HOFMsconsideronlycombinationsofdistinctfeatures(i.e. xj1xj2...xjmforjm>···>j2>j1).Thispaper.Unfortunately thereexiststodatenoefﬁcientalgorithmfortrainingarbitrary-orderHOFMs.Indeed computing(5)naivelytakesO(dm) i.e. polynomialtime.Inthefollowing wepresentlinear-timealgorithms.Moreover HOFMs asoriginallydeﬁnedin[13 14]requiretheestimationofm−1matricesP(2) ... P(m).Thus HOFMscanproducelargemodelswhenmislarge.Toaddressthisissue weproposenewvariantsofHOFMswithsharedparameters.3Linear-timestochasticgradientalgorithmsforHOFMsThekernelviewpresentedinSection2allowsustofocusontheANOVAkernelasthemain“computationalunit”fortrainingHOFMs.Inthissection wedevelopdynamicprogramming(DP)algorithmsforevaluatingtheANOVAkernelandcomputingitsgradientinonlyO(dm)time.Evaluation.Themainobservation(seealso[18 Section9.2])isthatwecanuse(7)torecursivelyremovefeaturesuntilcomputingthekernelbecomestrivial.Letusdenoteasubvectorofpbyp1:j∈Rjandsimilarlyforx.Letusintroducetheshorthandaj t:=At(p1:j x1:j).Then from(7) aj t=aj−1 t+pjxjaj−1 t−1∀d≥j≥t≥1.(8)Forconvenience wealsodeﬁneaj 0=1∀j≥0sinceA0(p x)=1andaj t=0∀j<tsincetheredoesnotexistanyt-combinationoffeaturesinaj<tdimensionalvector.Table1:ExampleofDPtablej=0j=1j=2...j=dt=011111t=10a1 1a2 1...ad 1t=200a2 2...ad 2..................t=m000...ad mThequantitywewanttocomputeisAm(p x)=ad m.Insteadofnaivelyusingrecursion(8) whichwouldleadtomanyredundantcomputations weuseabottom-upap-proachandorganizecomputationsinaDPtable.Westartfromthetop-leftcornertoinitializetherecursionandgothroughthetabletoarriveatthesolutioninthebottom-rightcorner.Theprocedure summarizedinAlgorithm1 takesO(dm)timeandmemory.Gradients.ForcomputingthegradientofAm(p x)w.r.t.p weusereverse-modedifferentiation[2](a.k.a.backpropagationinaneuralnetworkcontext) sinceitallowsustocomputetheentiregradientinasinglepass.Wesupplementeachvariableaj tintheDPtablebyaso-calledadjoint˜aj t:=∂ad m∂aj t whichrepresentsthesensitivityofad m=Am(p x)w.r.t.aj t.Fromrecursion(8) exceptforedgecases aj tinﬂuencesaj+1 t+1andaj+1 t.Usingthechainrule wethenobtain˜aj t=∂ad m∂aj+1 t∂aj+1 t∂aj t+∂ad m∂aj+1 t+1∂aj+1 t+1∂aj t=˜aj+1 t+pj+1xj+1˜aj+1 t+1∀d−1≥j≥t≥1.(9)Similarly weintroducetheadjoint˜pj:=∂ad m∂pj∀j∈[d].Sincepjinﬂuencesaj t∀t∈[m] wehave˜pj=mXt=1∂ad m∂aj t∂aj t∂pj=mXt=1˜aj taj−1 t−1xj.Wecanrunrecursion(9)inreverseorderoftheDPtablestartingfrom˜ad m=∂ad m∂ad m=1.Usingthisapproach wecancomputetheentiregradient∇Am(p x)=[˜p1 ... ˜pd]Tw.r.t.pinO(dm)timeandmemory.TheprocedureissummarizedinAlgorithm2.3Algorithm1EvaluatingAm(p x)inO(dm)Input:p∈Rd x∈Rdaj t←0∀t∈[m] j∈[d]∪{0}aj 0←1∀j∈[d]∪{0}fort:=1 ... mdoforj:=t ... ddoaj t←aj−1 t+pjxjaj−1 t−1endforendforOutput:Am(p x)=ad mAlgorithm2Computing∇Am(p x)inO(dm)Input:p∈Rd x∈Rd {aj t}d mj t=0˜aj t←0∀t∈[m+1] j∈[d]˜ad m←1fort:=m ... 1doforj:=d−1 ... tdo˜aj t←˜aj+1 t+˜aj+1 t+1pj+1xj+1endforendfor˜pj:=Pmt=1˜aj taj−1 t−1xj∀j∈[d]Output:∇Am(p x)=[˜p1 ... ˜pd]TStochasticgradient(SG)algorithms.BasedonAlgorithm1and2 wecaneasilylearnarbitrary-orderHOFMsusinganygradient-basedoptimizationalgorithm.HerewefocusourdiscussiononSGalgorithms.Ifwealternatinglyminimize(4)w.r.tP(2) ... P(m) thenthesub-problemassociatedwithdegreemisoftheformF(P):=1nnXi=1‘ yi kXs=1Am(ps xi)+oi!+β2kPk2 (10)whereo1 ... on∈Rareﬁxedoffsetswhichaccountforthecontributionofdegreesotherthanmtothepredictions.Thesub-problemisconvexineachrowofP[4].ASGupdatefor(10)w.r.t.psforsomeinstancexicanbecomputedbyps←ps−η‘0(yi ˆyi)∇Am(ps xi)−ηβps whereηisalearningrateandwherewedeﬁnedˆyi:=Pks=1Am(ps xi)+oi.BecauseevaluatingAm(p x)andcomputingitsgradientbothtakeO(dm) thecostperepoch i.e. ofvisitingallinstances isO(mdkn).Whenm=2 thisisthesamecostastheSGalgorithmimplementedinlibfm.Sparsedata.Weconcludethissectionwithafewusefulremarksonsparsedata.Letusdenotethesupportofavectorx=[x1 ... xd]Tbysupp(x):={j∈[d]:xj6=0}andletusdeﬁnexS:=[xj:j∈S]T.Itiseasytoseefrom(7)thatthegradientandxhavethesamesupport i.e. supp(∇Am(p x))=supp(x).AnotherusefulremarkisthatAm(p x)=Am(psupp(x) xsupp(x)) providedthatm≤nz(x) wherenz(x)isthenumberofnon-zeroelementsinx.Hence whenthedataissparse weonlyneedtoiterateovernon-zerofeaturesinAlgorithm1and2.Consequently theirtimeandmemorycostisonlyO(nz(x)m)andthusthecostperepochofSGalgorithmsisO(mknz(X)).4Coordinatedescentalgorithmforarbitrary-orderHOFMsWenowdescribeacoordinatedescent(CD)solverforarbitrary-orderHOFMs.CDisagoodchoiceforlearningHOFMsbecausetheirobjectivefunctioniscoordinate-wiseconvex thankstothemulti-linearityoftheANOVAkernel[4].OuralgorithmcanbeseenasageneralizationtohigherordersoftheCDalgorithmsproposedin[14 4].Analternativerecursion.EfﬁcientCDimplementationstypicallyrequiremaintainingstatisticsforeachtraininginstance suchasthepredictionsatthecurrentiteration.Whenacoordinateisupdated thestatisticsthenneedtobesynchronized.Unfortunately therecursionweusedintheprevioussectionisnotsuitableforaCDalgorithmbecauseitwouldrequiretostoreandsynchronizetheDPtableforeachtraininginstanceuponcoordinate-wiseupdates.Wethereforeturntoanalternativerecursion:Am(p x)=1mmXt=1(−1)t+1Am−t(p x)Dt(p x) (11)wherewedeﬁnedDt(p x):=Pdj=1(pjxj)t.Notethattherecursionwasalreadyknowninthecontextoftraditionalkernelmethods(c.f. [19 Section11.8])butitsapplicationtoHOFMsisnovel.SinceweknowthatA0(p x)=1andA1(p x)=hp xi wecanuse(11)tocomputeA2(p x) thenA3(p x) andsoon.Theoverallevaluationcostforarbitrarym∈NisO(md+m2).4Coordinate-wisederivatives.Wecanapplyreverse-modedifferentiationtorecursion(11)inordertocomputetheentiregradient(c.f. AppendixC).However inCD sinceweonlyneedthederivativeofonevariableatatime wecansimplyuseforward-modedifferentiation:∂Am(p x)∂pj=1mmXt=1(−1)t+1(cid:20)∂Am−t(p x)∂pjDt(p x)+Am−t(p x)∂Dt(p x)∂pj(cid:21) (12)where∂Dt(p x)∂pj=tpt−1jxtj.Theadvantageof(12)isthatweonlyneedtocacheDt(p x)fort∈[m].HencethememorycomplexitypersampleisonlyO(m)insteadofO(dm)for(8).UseinaCDalgorithm.Similarlyto[4] weassumethatthelossfunction‘isµ-smoothandupdatetheelementspj sofPincyclicorderbypj s←pj s−η−1j s∂F(P)∂pj s wherewedeﬁnedηj s:=µnnXi=1(cid:18)∂Am(ps xi)∂pj s(cid:19)2+βand∂F(P)∂pj s=1nnXi=1‘0(yi ˆyi)∂Am(ps xi)∂pj s+βpj s.Theupdateguaranteesthattheobjectivevalueismonotonicallynon-increasingandistheexactcoordinate-wiseminimizerwhen‘isthesquaredloss.Overall thetotalcostperepoch i.e. updatingallcoordinatesonce isO(τ(m)knz(X)) whereτ(m)isthetimeittakestocompute(12).AssumingDt(ps xi)havebeenpreviouslycached fort∈[m] computing(12)takesτ(m)=m(m+1)/2−1operations.Forﬁxedm ifweunrollthetwoloopsneededtocompute(12) moderncompilerscanoftenfurtherreducethenumberofoperationsneeded.Nevertheless thisquadraticdependencyonmmeansthatourCDalgorithmisbestforsmallm typicallym≤4.5HOFMswithsharedparametersHOFMs asoriginallydeﬁnedin[13 14] modeleachdegreewithseparatematricesP(2) ... P(m).Assumingthatweusethesamerankkforallmatrices thetotalmodelsizeofm-orderHOFMsisthereforeO(kdm).Moreover evenwhenusingourO(dm)DPalgorithm thecostofcomputingpredictionsisO(k(2d+···+md))=O(kdm2).Hence HOFMstendtoproducelarge expensive-to-evaluatemodels.Toreducemodelsizeandpredictiontimes weintroducetwonewkernelswhichallowustoshareparametersbetweeneachdegree:theinhomogeneousANOVAkernelandtheall-subsetskernel.BecausebothkernelsarederivedfromtheANOVAkernel theysharethesameappealingproperties:multi-linearity sparsegradientsandsparse-datafriendliness.5.1InhomogeneousANOVAkernelItiswell-knownthatasumofkernelsisequivalenttoconcatenatingtheirassociatedfeaturemaps[18 Section3.4].Letθ=[θ1 ... θm]T.Tocombinedifferentdegrees anaturalkernelisthereforeA1→m(p x;θ):=mXt=1θtAt(p x).(13)Thekernelusesallfeaturecombinationsofdegrees1uptom.WecallitinhomogeneousANOVAkernel sinceitisaninhomogeneouspolynomialofx.Incontrast Am(p x)ishomogeneous.Themaindifferencebetween(13)and(6)isthatallANOVAkernelsinthesumsharethesameparameters.However toincreasemodelingpower wealloweachkerneltohavedifferentweightsθ1 ... θm.Evaluation.DuetotherecursivenatureofAlgorithm1 whencomputingAm(p x) wealsogetA1(p x) ... Am−1(p x)forfree.Indeed lower-degreekernelsareavailableinthelastcolumnoftheDPtable i.e. At(p x)=ad t∀t∈[m].Hence thecostofevaluating(13)isO(dm)time.Thetotalcostforcomputingˆy=Pks=1A1→m(ps x;θ)isO(kdm)insteadofO(kdm2)forˆyHOFM(x).Learning.WhileitiscertainlypossibletolearnPandθbydirectlyminimizingsomeobjectivefunction hereweproposeaneasiersolution whichworkswellinpractice.OurkeyobservationisthatwecaneasilyturnAmintoA1→mbyaddingdummyvaluestofeaturevectors.Letusdenotetheconcatenationofpwithascalarγby[γ p]andsimilarlyforx.From(7) weeasilyobtainAm([γ1 p] [1 x])=Am(p x)+γ1Am−1(p x).5Table2:Datasetsusedinourexperiments.Foradetaileddescription c.f.AppendixA.Datasetn+ColumnsofAnAdAColumnsofBnBdBNIPS[17]4 140Authors2 03713 649Enzyme[21]2 994Enzymes668325GD[10]3 954Diseases3 2093 209Genes12 33125 275Movielens100K[6]21 201Users94349Movies1 68229Similarly ifweapply(7)twice weobtain:Am([γ1 γ2 p] [1 1 x])=Am(p x)+(γ1+γ2)Am−1(p x)+γ1γ2Am−2(p x).Applyingtheabovetom=2andm=3 weobtainA2([γ1 p] [1 x])=A1→2(p x;[γ1 1])andA3([γ1 γ2 p] [1 1 x])=A1→3(p x;[γ1γ2 γ1+γ2 1]).Moregenerally byaddingm−1dummyfeaturestopandx wecanconvertAmtoA1→m.Becausepislearned thismeansthatwecanautomaticallylearnγ1 ... γm−1.Theseweightscanthenbeconvertedtoθ1 ... θmby“unrolling”recursion(7).Althoughsimple weshowinourexperimentsthatthisapproachworksfavorablycomparedtodirectlylearningPandθ.Themainadvantageofthisapproachisthatwecanusethesamesoftwareunmodiﬁed(wesimplyneedtominimize(10)withtheaugmenteddata).Moreover thecostofcomputingtheentiregradientbyAlgorithm2usingtheaugmenteddataisjustO(dm+m2)comparedtoO(dm2)forHOFMswithseparateparameters.5.2All-subsetskernelWenowconsideracloselyrelatedkernelcalledall-subsetskernel[18 Deﬁnition9.5]:S(p x):=dYj=1(1+pjxj).Themaindifferencewiththetraditionaluseofthiskernelisthatwelearnp.Interestingly itcanbeshownthatS(p x)=1+A1→d(p x;1)=1+A1→nz(x)(p x;1) wherenz(x)isthenumberofnon-zerofeaturesinx.Hence thekernelusesallcombinationsofdistinctfeaturesuptoordernz(x)withuniformweights.Evenifdisverylarge thekernelcanbeagoodchoiceifeachtraininginstancecontainsonlyafewnon-zeroelements.Tolearntheparameters wesimplysubstituteAmwithSin(10).InSGorCDalgorithms allitentailsistosubstitute∇Am(p x)with∇S(p x).Forcomputing∇S(p x) itiseasytoverifythatS(p x)=S(p¬j x¬j)(1+pjxj)∀j∈[d]andthereforewehave∇S(p x)=(cid:20)x1S(p¬1 x¬1) ... xdS(p¬d x¬d)(cid:21)T=(cid:20)x1S(p x)1+p1x1 ... xdS(p x)1+pdxd(cid:21)T.Therefore themainadvantageoftheall-subsetskernelisthatwecanevaluateitandcomputeitsgradientinjustO(d)time.Thetotalcostforcomputingˆy=Pks=1S(ps x)isonlyO(kd).6Experimentalresults6.1ApplicationtolinkpredictionProblemsetting.WenowdemonstrateanovelapplicationofHOFMstopredictthepresenceorabsenceoflinksbetweennodesinagraph.Formally weassumetwosetsofpossiblydisjointnodesofsizenAandnB respectively.Weassumefeaturesforthetwosetsofnodes representedbymatricesA∈RdA×nAandB∈RdB×nB.Forinstance AcanrepresentuserfeaturesandBmoviefeatures.WedenotethecolumnsofAandBbyaiandbj respectively.WearegivenamatrixY∈{0 1}nA×nB whoseelementsindicatepresence(positivesample)orabsence(negativesample)oflinkbetweentwonodesaiandbj.Wedenotethenumberofpositivesamplesbyn+.Usingthisdata ourgoalistopredictnewassociations.DatasetsusedinourexperimentsaresummarizedinTable2.NotethatfortheNIPSandEnzymedatasets A=B.Conversiontoasupervisedproblem.WeneedtoconverttheaboveinformationtoaformatFMsandHOFMscanhandle.Topredictanelementyi jofY wesimplyformxi jtobetheconcatenation6Table3:ComparisonofareaundertheROCcurve(AUC)asmeasuredonthetestsets.NIPSEnzymeGDMovielens100KHOFM(m=2)0.8560.8800.7170.778HOFM(m=3)0.8750.8880.7170.786HOFM(m=4)0.8740.8870.7170.786HOFM(m=5)0.8740.8870.7170.786HOFM-shared-augmented(m=2)0.8580.8760.7040.778HOFM-shared-augmented(m=3)0.8740.8870.7040.787HOFM-shared-augmented(m=4)0.8360.8240.6630.779HOFM-shared-augmented(m=5)0.8240.7950.6000.621HOFM-shared-simplex(m=2)0.7160.8650.7210.701HOFM-shared-simplex(m=3)0.7770.8700.7210.709HOFM-shared-simplex(m=4)0.7580.8700.7210.709HOFM-shared-simplex(m=5)0.7220.8690.7210.709All-subsets0.7300.8400.7210.714Polynomialnetwork(m=2)0.7250.8790.7210.761Polynomialnetwork(m=3)0.7890.8530.7190.696Polynomialnetwork(m=4)0.7820.8730.7170.708Polynomialnetwork(m=5)0.5430.5240.6480.501Low-rankbilinearregression0.8550.6940.6110.718ofaiandbjandfeedthistoaHOFMinordertocomputeapredictionˆyi j.BecauseHOFMsusefeaturecombinationsinxi j theycanlearntheweightsoffeaturecombinationsbetweenaiandbj.Attrainingtime weneedbothpositiveandnegativesamples.LetusdenotethesetofpositiveandnegativesamplesbyΩ.Thenourtrainingsetiscomposedof(xi j yi j)pairs where(i j)∈Ω.Modelscompared.•HOFM:ˆyi j=ˆyHOFM(xi j)asdeﬁnedin(3)andasoriginallyproposedin[13 14].Weminimize(4)byalternatingminimizationof(10)foreachdegree.•HOFM-shared:ˆyi j=Pks=1A1→m(ps xi j;θ).WelearnPandθusingthesimpleaugmenteddataapproachdescribedinSection5.1(HOFM-shared-augmented).InspiredbySimpleMKL[12] wealsoreportresultswhenlearningPandθdirectlybyminimizing1|Ω|P(i j)∈Ω‘(yi j ˆyi j)+β2kPk2subjecttoθ≥0andhθ 1i=1(HOFM-shared-simplex).•All-subsets:ˆyi j=Pks=1S(ps xi j).AsexplainedinSection5.2 thismodelisequivalenttotheHOFM-sharedmodelwithm=nz(xi j)andθ=1.•Polynomialnetwork:ˆyi j=Pks=1(γs+hps xi ji)m.ThismodelcanbethoughtasfactorizationmachinevariantthatusesapolynomialkernelinsteadoftheANOVAkernel(c.f. [8 4 22]).•Low-rankbilinearregression:ˆyi j=aiUVTbj whereU∈RdA×kandV∈RdB×k.Suchmodelwasshowntoworkwellforlinkpredictionin[9]and[10].WelearnUandVbyminimizing1|Ω|P(i j)∈Ω‘(yi j ˆyi j)+β2(kUk2+kVk2).Experimentalsetupandevaluation.Inthisexperiment forallmodelsabove weuseCDratherthanSGtoavoidthetuningofalearningratehyper-parameter.Weset‘tobethesquaredloss.Althoughweomitteditfromournotationforclarity wealsoﬁtabiastermforallmodels.WeevaluatedthecomparedmodelsusingtheareaundertheROCcurve(AUC) whichistheprobabilitythatthemodelcorrectlyranksapositivesamplehigherthananegativesample.Wesplitthen+positivesamplesinto50%fortrainingand50%fortesting.Wesamplethesamenumberofnegativesamplesaspositivesamplesfortrainingandusetherestfortesting.Wechoseβfrom10−6 10−5 ... 106bycross-validationandfollowing[9]weempiricallysetk=30.Throughoutourexperiments weinitializedtheelementsofPrandomlybyN(0 0.01).ResultsareindicatedinTable3.OverallthetwobestmodelswereHOFMandHOFM-shared-augmented whichachievedthebestscoreson3outof4datasets.Thetwomodelsoutperformedlow-rankbilinearregressionon3out4datasets showingthebeneﬁtofusinghigher-orderfeaturecombinations.HOFM-shared-augmentedachievedsimilaraccuracytoHOFM despiteusingasmallermodel.Surprisingly HOFM-shared-simplexdidnotimproveoverHOFM-shared-augmentedexcept7(a)Convergencewhenm=2(b)Convergencewhenm=3(c)Convergencewhenm=4(d)Scalabilityw.r.t.degreemFigure1:Solvercomparisonforminimizing(10)whenvaryingthedegreemontheNIPSdatasetwithβ=0.1andk=30.ResultsonotherdatasetsareinAppendixB.ontheGDdataset.Weconcludethatouraugmenteddataapproachisconvenientyetworkswellinpractice.All-subsetsandpolynomialnetworksperformedworsethanHOFMandHOFM-shared-augmented exceptontheGDdatasetwheretheywerethebest.Finally weobservethatHOFMwerequiterobusttoincreasingm whichislikelyabeneﬁtofmodelingeachdegreewithaseparatematrix.6.2SolvercomparisonWecomparedAdaGrad[5] L-BFGSandcoordinatedescent(CD)forminimizing(10)whenvaryingthedegreemontheNIPSdatasetwithβ=0.1andk=30.Weconstructedthedatainthesamewayasexplainedintheprevioussectionandaddedm−1dummyfeatures resultinginn=8 280sparsesamplesofdimensiond=27 298+m−1.ForAdaGradandL-BFGS wecomputedthe(stochastic)gradientsusingAlgorithm2.Allsolversusedthesameinitialization.ResultsareindicatedinFigure1.WeseethatourCDalgorithmperformsverywellwhenm≤3butstartstodeterioratewhenm≥4 inwhichcaseL-BFGSbecomesadvantageous.AsshowninFigure1d) thecostperepochofAdaGradandL-BFGSscaleslinearlywithm abeneﬁtofourDPalgorithmforcomputingthegradient.However tooursurprise wefoundthatAdaGradisquitesensitivetothelearningrateη.AdaGraddivergedforη∈{1 0.1 0.01}andthelargestvaluetoworkwellwasη=0.001.ThisexplainswhyAdaGraddidnotoutperformCDdespitethelowercostperepoch.Inthefuture itwouldbeusefultocreateaCDalgorithmwithabetterdependencyonm.7ConclusionandfuturedirectionsInthispaper wepresentedtheﬁrsttrainingalgorithmsforHOFMsandintroducednewHOFMvariantswithsharedparameters.ApopularwaytodealwithalargenumberofnegativesamplesistouseanobjectivefunctionthatdirectlymaximizeAUC[9 15].ThisisespeciallyeasytodowithSGalgorithmsbecausewecansamplepairsofpositiveandnegativesamplesfromthedatasetuponeachSGupdate.WethereforeexpectthealgorithmsdevelopedinSection3tobeespeciallyusefulinthissetting.Recently [7]proposedadistributedSGalgorithmfortrainingsecond-orderFMs.ItshouldbestraightforwardtoextendthisalgorithmtoHOFMsbasedonourcontributionsinSection3.Finally itshouldbepossibletointegrateAlgorithm1and2intoadeeplearningframeworksuchasTensorFlow[1] inordertoeasilycomposeANOVAkernelswithotherlayers(e.g. convolutional).8References[1]M.Abadietal.TensorFlow:Large-scalemachinelearningonheterogeneoussystems 2015.[2]A.G.Baydin B.A.Pearlmutter andA.A.Radul.Automaticdifferentiationinmachinelearning:asurvey.arXivpreprintarXiv:1502.05767 2015.[3]M.Blondel A.Fujino andN.Ueada.Convexfactorizationmachines.InProceedingsofEuro-peanConferenceonMachineLearningandPrinciplesandPracticeofKnowledgeDiscoveryinDatabases(ECMLPKDD) 2015.[4]M.Blondel M.Ishihata A.Fujino andN.Ueada.Polynomialnetworksandfactorizationmachines:Newinsightsandefﬁcienttrainingalgorithms.InProceedingsofInternationalConferenceonMachineLearning(ICML) 2016.[5]J.Duchi E.Hazan andY.Singer.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.JournalofMachineLearningResearch 12:2121–2159 2011.[6]GroupLens.http://grouplens.org/datasets/movielens/ 1998.[7]M.Li Z.Liu A.Smola andY.-X.Wang.Difacto–distributedfactorizationmachines.InProceedingsofInternationalConferenceonWebSearchandDataMining(WSDM) 2016.[8]R.Livni S.Shalev-Shwartz andO.Shamir.Onthecomputationalefﬁciencyoftrainingneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages855–863 2014.[9]A.K.MenonandC.Elkan.Linkpredictionviamatrixfactorization.InMachineLearningandKnowledgeDiscoveryinDatabases pages437–452.2011.[10]N.NatarajanandI.S.Dhillon.Inductivematrixcompletionforpredictinggene–diseaseassociations.Bioinformatics 30(12):i60–i68 2014.[11]V.Y.Pan.StructuredMatricesandPolynomials:UniﬁedSuperfastAlgorithms.Springer-VerlagNewYork Inc. 2001.[12]A.Rakotomamonjy F.Bach S.Canu andY.Grandvalet.Simplemkl.JournalofMachineLearningResearch 9:2491–2521 2008.[13]S.Rendle.Factorizationmachines.InProceedingsofInternationalConferenceonDataMining pages995–1000.IEEE 2010.[14]S.Rendle.Factorizationmachineswithlibfm.ACMTransactionsonIntelligentSystemsandTechnology(TIST) 3(3):57–78 2012.[15]S.Rendle C.Freudenthaler Z.Gantner andL.Schmidt-Thieme.Bpr:Bayesianpersonalizedrankingfromimplicitfeedback.InProceedingsofthetwenty-ﬁfthconferenceonuncertaintyinartiﬁcialintelligence pages452–461 2009.[16]S.Rendle Z.Gantner C.Freudenthaler andL.Schmidt-Thieme.Fastcontext-awarerecom-mendationswithfactorizationmachines.InSIGIR pages635–644 2011.[17]S.Roweis.http://www.cs.nyu.edu/~roweis/data.html 2002.[18]J.Shawe-TaylorandN.Cristianini.KernelMethodsforPatternAnalysis.CambridgeUniversityPress 2004.[19]V.Vapnik.Statisticallearningtheory.Wiley 1998.[20]G.Wahba.Splinemodelsforobservationaldata volume59.Siam 1990.[21]Y.Yamanishi J.-P.Vert andM.Kanehisa.Supervisedenzymenetworkinferencefromtheintegrationofgenomicdataandchemicalinformation.Bioinformatics 21:i468–i477 2005.[22]J.YangandA.Gittens.Tensormachinesforlearningtarget-speciﬁcpolynomialfeatures.arXivpreprintarXiv:1504.01697 2015.9,Mathieu Blondel
Akinori Fujino
Naonori Ueda
Masakazu Ishihata