2014,Learning the Learning Rate for Prediction with Expert Advice,Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well  but small enough to prevent overfitting. For the exponential weights algorithm  a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate  which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate  our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates  yet runs in linear time regardless of the size of the grid.,Learning the Learning Rate for
Prediction with Expert Advice

Wouter M. Koolen

Tim van Erven

Queensland University of Technology and UC Berkeley

Leiden University  the Netherlands

wouter.koolen@qut.edu.au

tim@timvanerven.nl

Leiden University and Centrum Wiskunde & Informatica  the Netherlands

Peter D. Gr¨unwald

pdg@cwi.nl

Abstract

Most standard algorithms for prediction with expert advice depend on a parameter
called the learning rate. This learning rate needs to be large enough to ﬁt the data
well  but small enough to prevent overﬁtting. For the exponential weights algo-
rithm  a sequence of prior work has established theoretical guarantees for higher
and higher data-dependent tunings of the learning rate  which allow for increas-
ingly aggressive learning. But in practice such theoretical tunings often still per-
form worse (as measured by their regret) than ad hoc tuning with an even higher
learning rate. To close the gap between theory and practice we introduce an ap-
proach to learn the learning rate. Up to a factor that is at most (poly)logarithmic
in the number of experts and the inverse of the learning rate  our method performs
as well as if we would know the empirically best learning rate from a large range
that includes both conservative small values and values that are much higher than
those for which formal guarantees were previously available. Our method em-
ploys a grid of learning rates  yet runs in linear time regardless of the size of the
grid.

1

Introduction

T = mink Lk

T =(cid:80)T

t=1 (cid:96)k

HT =(cid:80)T

t=1 ht is compared to the cumulative losses Lk

Consider a learner who in each round t = 1  2  . . . speciﬁes a probability distribution wt on K
experts  before being told a vector (cid:96)t ∈ [0  1]K with their losses and consequently incurring loss
ht := wt · (cid:96)t. Losses are summed up over trials and after T rounds the learner’s cumulative loss
t of the experts k = 1  . . .   K.
This is essentially the framework of prediction with expert advice [1  2]  in particular the standard
Hedge setting [3]. Ideally  the learner’s predictions would not be much worse than those of the best
expert  who has cumulative loss L∗
Follow-the-Leader (FTL) is a natural strategy for the learner.
In any round t  it predicts with a
t−1  i.e. the expert that was best on the previous
point mass on the expert k with minimum loss Lk
t − 1 rounds. However  in the standard game-theoretic analysis  the experts’ losses are assumed
to be generated by an adversary  and then the regret for FTL can grow linearly in T [4]  which
means that it is not learning. To do better  the predictions need to be less outspoken  which can
be accomplished by replacing FTL’s choice of the expert with minimal cumulative loss by the soft
t−1  which is known as the exponential weights or Hedge algorithm [3]. Here
minimum wk
η > 0 is a regularisation parameter that is called the learning rate. As η → ∞ the soft minimum
approaches the exact minimum and exponential weights converges to FTL. In contrast  the lower η 
the more the soft minimum resembles a uniform distribution and the more conservative the learner.

T   so that the regret RT = HT − L∗

t ∝ e−ηLk

T is small.

1

T(cid:88)

t=1

Let Rη
T denote the regret for exponential weights with learning rate η. To obtain guarantees against
adversarial losses  several tunings of η have been proposed in the literature. Most of these may be
understood by starting with the bound

+

(1)

δη
t  

Rη
T ≤ ln K
η
t ≥ 0 is the approximation error (called mixability
which holds for any sequence of losses. Here δη
gap by [5]) when the loss of the learner in round t is approximated by the so-called mix loss  which
is a certain η-exp-concave lower bound (see Section 2.1). The analysis then proceeds by giving
an upper bound bt(η) ≥ δη
t bt(η). In
particular  the bound δη

t and choosing η to balance the two terms ln(K)/η and(cid:80)
t ≤ η/8 results in the most conservative tuning η = (cid:112)8 ln(K)/T   for
which the regret is always bounded by O((cid:112)T ln(K)); the same guarantee can still be achieved
ln(1 +(cid:112)2 ln(K)/L∗
T ) ≈ (cid:112)2 ln(K)/L∗
unknown in advance  which leads to a bound of O((cid:112)L∗

even if the horizon T is unknown in advance by using  for instance  the so-called doubling trick
[4]. It is possible though to learn more aggressively by using a bound on δη
t that depends on the
t ≤ eηwt · (cid:96)t and choosing η =
data. The ﬁrst such improvement can be obtained by using δη
T is
T ≤ T
this is never worse than the conservative tuning  and it can be better if the best expert has very
small losses (a case sometimes called the “low noise condition”). A further improvement has been
proposed by Cesa-Bianchi et al. [7]  who bound δη
t when k
t = wt · ((cid:96)t − ht)2. Rather than using a constant learning
is distributed according to wt  such that vη
rate  at time t they play the Hedge weights wt based on a time-varying learning rate ηt that is
s . This leads to a so-called second-order

approximately tuned as(cid:112)ln(K)/Vt−1 with Vt =(cid:80)

T   where again the doubling trick can be used if L∗

T ln(K) + ln K) [6  4]. Since L∗

t by a constant times the variance vη

t of (cid:96)k

s≤t vηs

(cid:16)(cid:112)Vt ln(K) + ln K

(cid:17)

bound on the regret of the form

RT = O
which  as Cesa-Bianchi et al. show  implies
T (T − L∗
L∗
T )

(cid:32)(cid:114)

RT = O

 

(cid:33)

ln(K) + ln K

T

(2)

(3)

(4)

s≤t δηs

where ∆t =(cid:80)

and is therefore always better than the tuning in terms of L∗
T (note though that (2) can be much
stronger than (3) on data for which the exponential weights rapidly concentrate on a single expert 
see also [8]). The general pattern that emerges is that the better the bound on δη
t   the higher η
can be chosen and the more aggressive the learning. De Rooij et al. [5] take this approach to its
extreme and do not bound δη
t at all. In their AdaHedge algorithm they tune ηt = ln(K)/∆t−1
s   which is very similar to the second-order tuning of Cesa-Bianchi et al. and
indeed also satisﬁes (2) and (3). Thus  this sequence of prior works appears to have reached the
limit of what is possible based on improving the bound on δη
t . Unfortunately  however  if the data
are not adversarial  then even second-order bounds do not guarantee the best possible tuning of η
for the data at hand. (See the experiments that study the inﬂuence of η in [5].) In practice  selecting
ηt to be the best-performing learning rate so far (that is  running FTL at the meta-level) appears to
work well [9]  but this approach requires a computationally intensive grid search over learning rates
[9] and formal guarantees can only be given for independent and identically distributed (IID) data
[10]. A new technique based on speculatively trying out different η was therefore introduced in the
FlipFlop algorithm [5]. By alternating learning rates ηt = ∞ and ηt that are very similar to those
of AdaHedge  FlipFlop is both able to satisfy the second-order bounds (2) and (3)  and to guarantee
that its regret is never much worse than the regret R∞

T for Follow-the-Leader:

RT = O(cid:0)R∞

T

(cid:1).

Thus FlipFlop covers two extremes: on the one hand it is able to compete with η that are small
enough to deal with the worst case  and on the other hand it can compete with η = ∞ (FTL).

Main Contribution We generalise the FlipFlop approach to cover a large range of η in between.
As before  let Rη
T denote the regret of exponential weights with ﬁxed learning rate η. We introduce

2

T

(5)

ln 1
η

(cid:16)

the learning the learning rate (LLR) algorithm  which satisﬁes (2)  (3) and (4) and in addition
guarantees a regret satisfying
RT = O

(cid:17)1+ε Rη

(cid:18)

ln(K)

for all η ∈ [ηah

(cid:19)
t∗ ≥ (1 − o(1))(cid:112)ln(K)/T (as follows from

t∗   1]

t∗   1] that is

for any ε > 0. Thus  LLR performs almost as well as the learning rate ˆηT ∈ [ηah
optimal with hindsight. Here the lower end-point ηah
(28) below) is a data-dependent value that is sufﬁciently conservative (i.e. small) to provide second-
order guarantees and consequently worst-case optimality. The upper end-point 1 is an artefact of the
analysis  which we introduce because  for general losses in [0  1]K  we do not have a guarantee in
terms of Rη
T for 1 < η < ∞. For the special case of binary losses (cid:96)t ∈ {0  1}K  however  we can
say a bit more: as shown in Appendix B of the supplementary material  in this special case the LLR
algorithm guarantees regret bounded by RT = O(KRη
The additional factor ln(K) ln1+ε(1/η) in (5) comes from a prior on an exponentially spaced grid
of η. It is logarithmic in the number of experts K  and its dependence on 1/η grows slower than
ln1+ε(1/η) ≤ ln1+ε(1/ηah
t∗ ) = O(ln1+ε(T )) for any ε > 0. For the optimally tuned ˆηT   we have
in mind regret that grows like RˆηT
T = O(T α) for some α ∈ [0  1/2]  so an additional polylog factor
seems a small price to pay to adapt to the right exponent α.
Although η ≥ ηah
lower η:

t∗ appear to be most important  the regret for LLR can also be related to Rη

T ) for all η ∈ [1 ∞].

T for

(cid:18) ln K

(cid:19)

RT = O

η

for all η < ηah
t∗ 

T   but still improves on the standard bound (1) because δη

(6)
t ≥ 0 for all η.
which is not in terms of Rη
The LLR algorithm takes two parameters  which determine the trade-off between constants in the
bounds (2)–(6) above. Normally we would propose to set these parameters to moderate values  but if
we do let them approach various limits  LLR becomes essentially the same as FlipFlop  AdaHedge
or FTL (see Section 2).
We emphasise that we do not just have a bound
on LLR that is unavailable for earlier methods;
there also exist actual losses for which the op-
timal learning rate with hindsight ˆηT is funda-
mentally in between the robust learning rates
chosen by AdaHedge and the aggressive choice
η = ∞ of FTL. On such data  Hedge with ﬁxed
learning rate ˆηT performs signiﬁcantly better
than both these extremes; see Figure 1. In Ap-
pendix A we describe the data used to generate
Figure 1 and explain why the regret obtained by
LLR is signiﬁcantly smaller than the regret of
AdaHedge  FTL and all other tunings described
above.

Worst-case bound and η
Hedge(η)
AdaHedge
FlipFlop
LLR and ηah
t∗

t
e
r
g
e
r

1000

2000

3000

4000

5000

6000

7000

9000

8000

1

102

0
10−4

10−2

Computational Efﬁciency Although LLR
employs a grid of η  it does not have to search
over this grid. Instead  in each time step it only
has to do computations for the single η that is
active  and  as a consequence  it runs as fast as
using exponential weights with a single ﬁxed
η  which is linear in K and T . LLR  as pre-
sented here  does store information about all
the grid points  which requires O(ln(K) ln(T ))
storage  but we describe a simple approxima-
tion that runs equally fast and only requires a
constant amount of storage.

learning rate(η)

Figure 1: Example data (details in Appendix A)
on which Hedge/exponential weights with inter-
mediate learning rate (global minimum) performs
much better than both the worst-case optimal
learning rate (local minimum on the left) and large
learning rates (plateau on the right). We also show
the performance of the algorithms mentioned in
the introduction.

3

Outline The paper is organized as follows.
In Section 2 we deﬁne the LLR algorithm and in
Section 3 we make precise how it satisﬁes (2)  (3)  (4)  (5) and (6). Section 4 provides a discussion.
Finally  the appendix contains a description of the data in Figure 1 and most of the proofs.

2 The Learning the Learning Rate Algorithm

In this section we describe the LLR algorithm  which is a particular strategy for choosing a time-
varying learning rate in exponential weights. We start by formally describing the setting and then
explain how LLR chooses its learning rates.

2.1 The Hedge Setting

t   . . .   wK

learner’s loss ht = wt · (cid:96)t =(cid:80)
cumulative loss is HT =(cid:80)T

t ) on K ≥ 2 experts. Then the experts incur losses (cid:96)t = ((cid:96)1

At the start of each round t = 1  2  . . . the learner produces a probability distribution wt =
t ) ∈ [0  1]K and the
(w1
t is the expected loss under wt. After T rounds  the learner’s
t . The
goal is to minimize the regret RT = HT −L∗
T of
the best expert. We consider strategies for the learner that play the exponential weights distribution

t=1 ht and the cumulative losses for the experts are Lk
T with respect to the cumulative loss L∗

T =(cid:80)T

T = mink Lk

t   . . .   (cid:96)K

t=1 (cid:96)k

k wk

t (cid:96)k

wk

t =

(cid:80)K
e−ηtLk
t−1
j=1 e−ηtLj

t−1

ln(cid:80)

ηt

k wk

t e−ηt(cid:96)k

for a choice of learning rate ηt that may depend on all losses before time t. To analyse such methods 
it is common to approximate the learner’s loss ht by the mix loss mt = − 1
t   which
appears under a variety of names in e.g. [7  4  11  5]. The resulting approximation error or mixability
gap δt = ht−mt is always non-negative and cannot exceed 1. This  and some other basic properties
of the mix loss are listed in Lemma 1 of De Rooij et al. [5]  which we reproduce as Lemma C.1 in
the additional material.
As will be explained in the next section  LLR does not monitor the regrets of all learning rates
directly. Instead  it tracks their cumulative mixability gaps  which provide a convenient lower bound
on the regret that is monotonically increasing with the number of rounds T   in contrast to the regret
itself. To show this  let Rη
T denote the regret of the exponential weights strategy with ﬁxed learning
rate ηt = η  and similarly let M η
t denote its cumulative mix loss
and mixability gap.
Lemma 2.1. For any ﬁxed learning rate η ∈ (0 ∞]  the regret of exponential weights satisﬁes

T =(cid:80)T

T =(cid:80)T

t and ∆η

t=1 mη

t=1 δη

Rη
T ≥ ∆η
T .

(7)

Proof. Apply property 3 in Lemma C.1 to the regret decomposition Rη

T = M η

T − L∗

T + ∆η
T .

We will use the following notational conventions. Lower-case letters indicate instantaneous quan-
tities like mt  δt and wt  whereas uppercase letters denote cumulative quantities like MT   ∆T and
RT . In the absence of a superscript the learning rates present in any such quantity are those chosen
by LLR. In contrast  the superscript η refers to using the same ﬁxed learning rate η throughout.

2.2 LLR’s Choice of Learning Rate

The LLR algorithm is a member of the exponential weights family of algorithms. Its deﬁning prop-
erty is its adaptive and non-monotonic selection of the learning rate ηt  which is speciﬁed in Al-
gorithm 1 and explained next. The LLR algorithm works in regimes in which it speculatively tries
out different strategies for ηt. Almost all of these strategies consist of choosing a ﬁxed η from the
following grid:

η1 = ∞ 

ηi = α2−i

for i = 2  3  . . .  

(8)

where the exponential base

α = 1 + 1/ log2 K

4

(9)

Algorithm 1 LLR(πah  π∞). The grid η1  η2  . . . and weights π1  π2  . . . are deﬁned in (8) and (12).

Initialise b0 := 0; ∆ah
for t = 1  2  . . . do

0 := 0 for all i ≥ 1.
if all active indices and ah are bt−1-full then

0 := 0; ∆i

Increase bt := φ∆ah

t−1/πah (with φ as deﬁned in (14))

else

Keep bt := bt−1

end if
Let i be the least non-bt-full index.
if i is active then

Play ηi.
Update ∆i

else

t := ∆i

t−1 + δi

t. Keep ∆j

t := ∆j

t−1 for j (cid:54)= i and ∆ah

t

:= ∆ah

t−1.

Play ηah
Update ∆ah
t

t as deﬁned in (10).
t−1 + δah

:= ∆ah

t . Keep ∆j

t := ∆j

t−1 for all j ≥ 1.

end if

end for

is chosen to ensure that the grid is dense enough so that ηi for i ≥ 2 is representative for all
η ∈ [ηi+1  ηi] (this is made precise in Lemma 3.3). We also include the special value η1 = ∞ 
because it corresponds to FTL  which works well for IID data and data with a small number of
leader changes  as discussed by De Rooij et al. [5].
t ⊆ {1  . . .   t} denote the set of rounds up to trial t in
For each index i = 1  2  . . . in the grid  let Ai
which the LLR algorithm plays ηi. Then LLR keeps track of the performance of ηi by storing the
(cid:88)
sum of mixability gaps δi

for which ηi is responsible:

t ≡ δηi

t

∆i

t =

δi
s.

s∈Ai

t

In addition to the grid in (8)  LLR considers one more strategy  which we will call the AdaHedge
strategy  because it is very similar to the learning rate chosen by the AdaHedge algorithm [5]. In the
AdaHedge strategy  LLR plays ηt equal to

t = (cid:80)

ηah
t =

ln K
∆ah
t−1

 

(10)

t

t

t

δah
s

s∈Aah

t ≡ δηah

during the rounds Aah

is the sum of mixability gaps δah

t does not change during rounds outside Aah
t .

t ⊆
where ∆ah
{1  . . .   t} in which LLR plays the AdaHedge strategy. The only difference to the original Ada-
Hedge is that the latter sums the mixability gaps over all s ∈ {1  . . .   t}  not just those in Aah
t . Note
that  in our variation  ηah
The AdaHedge learning rate ηah
is non-increasing with t  and (as we will show in Theorem 3.6
t
below) it is small enough to guarantee the worst-case bound (3)  which is optimal for adversarial
data. We therefore focus on η > ηah
t and call an index i in the grid active in round t if ηi > ηah
t .
Let imax ≡ imax(t) be the number of grid indices that are active at time t  such that ηimax(t) ≈ ηah
t .
Then LLR cyclically alternates grid learning rates and the AdaHedge learning rate  in a way that
approximately maintains

∆1
t

π1 ≈ ∆2

π2 ≈ . . . ≈ ∆imax

πimax

t

t

≈ ∆ah
t
πah

for all t 

(11)

where πah > 0 and π1  π2  . . . > 0 are ﬁxed weights that control the relative importance of Ada-
Hedge and the grid points (higher weight = more important). The LLR algorithm takes as parameters
πah and π∞  where πah only has to be positive  but π∞ is restricted to (0  1). We then choose

where ρ is a prior probability distribution on {1  2  . . .}. It follows that(cid:80)∞

(12)
i=1 πi = 1  so that πi may
be interpreted as a prior probability mass on grid index i. For ρ  we require a distribution with very

πi = (1 − π∞)ρ(i − 1)

π1 = π∞ 

for i ≥ 2 

5

heavy tails (meaning ρ(i) not much smaller than 1

(cid:90) i

ln K

1

dx =

i )  and we ﬁx the convenient choice

ln(cid:0) i−1
ln K + e(cid:1) −

1

ln(cid:0) i
ln K + e(cid:1) .

1

ρ(i) =

i−1
ln K

(x + e) ln2(x + e)

(13)

We cannot guarantee that the invariant (11) holds exactly  and our algorithm incurs overhead for
changing learning rates  so we do not want to change learning rates too often. LLR therefore uses
an exponentially increasing budget b and tries grid indices and the AdaHedge strategy in sequence
until they exhaust the budget. To make this precise  we say that an index i is b-full in round t if
t−1/πah > b. Let bt be the
t−1/πi > b and similarly that AdaHedge is b-full in round t if ∆ah
∆i
budget at time t  which LLR chooses as follows: ﬁrst it initialises b0 = 0 and then  for t ≥ 1  it
tests whether all active indices and AdaHedge are bt−1-full. If this is the case  LLR approximately
increases the budget by a factor φ > 1 by setting bt = φ∆ah
t−1/πah > φbt−1  otherwise it just keeps
the budget the same: bt = bt−1. In particular  we will ﬁx budget multiplier

√

φ = 1 +

πah 

(14)

which minimises the constants in our bounds. Now if  at time t  there exists an active index that is
not bt-full  then LLR plays the ﬁrst such index. And if all active indices are bt-full  LLR plays the
AdaHedge strategy  which cannot be bt-full in this case by deﬁnition of bt. This guarantees that all
T are approximately within a factor φ of each other for all i that are active at time t∗ 
ratios ∆i
which we deﬁne to be the last time t ≤ T that LLR plays AdaHedge:

T /πi

t∗ = maxAah
T .

(15)

Whenever LLR plays AdaHedge it is possible  however  that a new index i becomes active and it
then takes a while for this index’s cumulative mixability gap ∆i
T to also grow up to the budget.
Since AdaHedge is not played while the new index is catching up  the ratio guarantee always still
holds for all indices that were active at time t∗.

2.3 Choosing the LLR Parameters
LLR has several existing strategies as sub-cases. For πah → ∞ it essentially becomes AdaHedge.
For π∞ → 1 it becomes FlipFlop. For π∞ → 1 and πah → 0 it becomes FTL. Intermediate values
for πah and π∞ retain the beneﬁts of these algorithms  but in addition allow LLR to compete with
essentially all learning rates ranging from worst-case safe to extremely aggressive.

2.4 Run time and storage

LLR  as presented here  runs in constant time per round. This is because  in each round  it only
needs to compute the weights and update the corresponding cumulative mixability gap for a single
learning rate strategy. If the current strategy exceeds its budget (becomes bt-full)  LLR proceeds
to the next1. The memory requirement is dominated by the storage of ∆1
  which 
following the discussion below (5)  is at most

t   . . .   ∆imax(t)

t

imax(T ) = 2 +

ln

1

ηimax (T )
ln α

≤ 2 + logα

1
ηah
T

= O(ln(K) ln(T )).

However  a minor approximation reduces the memory requirement down to a constant: At any point
in time the grid strategies considered by LLR split in three. Let us say that ηi is played at time t.
Then all preceding ηj for j ≤ i are already at (or slightly past) the budget. And all succeeding ηj
for i < j ≤ imax are still at (or slightly past) the previous budget. So we can approximate their
cumulative mixability gaps by simply ignoring these slight overshoots. It then sufﬁces to store only
the cumulative mixability gap for the currently advancing ηi  and the current and previous budget.

1In the early stages it may happen that the next strategy is already over the budget and needs to be skipped 
t/πi ≤

but this start-up effect quickly disappears when the budget exceeds 1  as the weighted increment δi
ηi/8 log1+(1/η) is bounded for all 0 ≤ η ≤ 1.

6

3 Analysis of the LLR algorithm

T and ∆ah

T relates to ∆i

In this section we analyse the regret of LLR. We ﬁrst show that for each loss sequence the regret is
bounded in terms of the cumulative mixability gaps ∆i
T incurred by the active learning rates
(Lemma 3.1). As LLR keeps the cumulative mixability gaps approximately balanced according to
(11)  we can then further bound the regret in terms of each of the individual learning rates in the grid
(Lemma 3.2). The next step is to deal with learning rates between grid points  by showing that their
T for the nearest higher grid point ηi ≥ η (Lemma 3.3).
cumulative mixability gap ∆η
In Lemma 3.4 we put all these steps together. As the cumulative mixability gap ∆η
T does not exceed
the regret Rη
T for ﬁxed learning rates (Lemma 2.1)  we can then derive the bounds (2) through (6)
from the introduction in Theorems 3.5 and 3.6.
We start by showing that the regret of LLR is bounded by the cumulative mixability gaps of the
learning rates that it plays. The proof  which appears in Section C.4  is a generalisation of Lemma 12
in [5]. It crucially uses the fact that the lowest learning rate played by LLR is the AdaHedge rate ηah
t
which relates to ∆ah
t .
Lemma 3.1. On any sequence of losses  the regret of the LLR algorithm with parameters πah > 0
and π∞ ∈ (0  1) is bounded by

RT ≤(cid:16) φ

φ − 1

(cid:17)

+ 2

∆ah

T +

∆i

T  

imax(cid:88)

i=1

where imax is the largest i such that ηi is active in round T and φ is deﬁned in (14).

The LLR budgeting scheme keeps the cumulative mixability gaps from Lemma 3.1 approximately
balanced according to (11). The next result  proved in Section C.5  makes this precise.
Lemma 3.2. Fix t∗ as in (15). Then for each index i that was active at time t∗ and arbitrary j (cid:54)= i:

(cid:19)

(cid:18) πj

T ≤ φ
∆j
T ≤ φ
∆j
T ≤ πah
∆ah

πi ∆i
πj
πah ∆ah
πi ∆i

T + 1.

+ min{1  ηj/8} 

πj
πah

T +
T + min{1  ηj/8} 

(16a)

(16b)

(16c)

LLR employs an exponentially spaced grid of learning rates that are evaluated using — and played
proportionally to — their cumulative mixability gaps. In the next step (which is restated and proved
as Lemma C.7 in the additional material) we show that the mixability gap of a learning rate between
grid-points cannot be much smaller than that of its next higher grid neighbour. This establishes in
particular that an exponential grid is sufﬁciently ﬁne.
Lemma 3.3. For γ ≥ 1 and for any sequence of losses with values in [0  1]:

t ≤ γe(γ−1)(ln K+η)δη
δγη
t .

The preceding results now allow us to bound the regret of LLR in terms of the cumulative mixability
gap of any ﬁxed learning rate (which does not exceed its regret by Lemma 2.1) and in terms of the
cumulative mixability gap of AdaHedge (which we will use to establish worst-case optimality).
Lemma 3.4. Suppose the losses take values in [0  1]  let πah > 0 and π∞ ∈ (0  1) be the parameters

of the LLR algorithm  and abbreviate B =(cid:0) φ

φ−1 + 2(cid:1)πah + φ. Then the regret of the LLR algorithm
(cid:18)

is bounded by

(cid:19)

+

α

8(α − 1)

+

φ
πah +

φ
φ − 1

+ 3

for all η ∈ [ηah
by

t∗   1]  where i(η) = 2 +(cid:98)logα(1/η)(cid:99) is the index of the nearest grid point above η  and

RT ≤ Bαe(α−1)(ln K+1) ∆η
T
πi(η)
(cid:18)

RT ≤ B

∆∞
π∞ +

T

(cid:19)

α

8(α − 1)

+

φ
πah +

φ
φ − 1

+ 3

7

for η = ∞. In addition

and for any η < ηah
t∗

RT ≤ B

∆ah
T
πah +

α

8(α − 1)

+ 1 

T ≤ ln K
∆ah
η

+ 1.

The proof appears in additional material Section C.6.
We are now ready for our main result  which is proved in Section C.7. It shows that LLR competes
with the regret of any learning rate above the worst-case safe rate and below 1 modulo a mild factor.
In addition  LLR also performs well on all data favoured by Follow-the-Leader.
Theorem 3.5. Suppose the losses take values in [0  1]  let πah > 0 and π∞ ∈ (0  1) be the
√
πah + 3πah and
parameters of the LLR algorithm  and introduce the constants B = 1 + 2
CK = (log2 K + 1)/8 + B/πah + 1. Then the regret of LLR is simultaneously bounded by
for all η ∈ [ηah

1 − π∞ (log2 K + 1) ln(7/η) ln2(cid:0)2 log2(5/η)(cid:1)
(cid:125)

RT ≤ 4Be

Rη
T + CK

(cid:123)(cid:122)

t∗   1]

(cid:124)

=O(ln1+ε(1/η)) for any ε > 0

and by

In addition

RT ≤ B

π∞R∞

T + CK

for η = ∞.

RT ≤ B
πah

ln K

η

+ CK

for any η < ηah
t∗.

To interpret the theorem  we recall from the introduction that ln(1/η) is better than O(ln T ) for all
η ≥ ηah
t∗.
We ﬁnally show that LLR is robust to the worst-case. We do this by showing something much
stronger  namely that LLR guarantees a so-called second-order bound (a concept introduced in [7]).

The bound is phrased in terms of the cumulative variance VT =(cid:80)T
parameters of the LLR algorithm  and introduce the constants B = (cid:0) φ

(cid:3)
(cid:2)(cid:96)k
φ−1 + 2(cid:1)πah + φ and

is the variance of (cid:96)k
Theorem 3.6. Suppose the losses take values in [0  1]  let πah > 0 and π∞ ∈ (0  1) be the

t for k distributed according to wt. See Section C.8 for the proof.

t=1 vt  where vt = Vk∼wt

t

CK = (log2 K + 1)/8 + B/πah + 1. Then the regret of LLR is bounded by

(cid:19)

RT ≤ B
πah

VT ln K +

CK +

2B ln K

3πah

(cid:112)

(cid:114)

T (T − L∗
L∗
T )

T

(cid:18)
(cid:18)

and consequently by

RT ≤ B
πah

4 Discussion

ln K + 2

CK +

2B ln K
3πah +

B2 ln K
(πah)2

(cid:19)

.

We have shown that our new LLR algorithm is able to recover the same second-order bounds as
previous methods  which guard against worst-case data by picking a small learning rate if necessary.
What LLR adds is that  at the cost of a (poly)logarithmic overhead factor  it is also able to learn a
range of higher learning rates η  which can potentially achieve much smaller regret (see Figure 1).
This is accomplished by covering this range with a grid of sufﬁcient granularity. The overhead
factor depends on a prior on the grid  for which we have ﬁxed a particular choice with a heavy tail.
However  the algorithm would also work with any other prior  so if it were known a priori that certain
values in the grid were of special importance  they could be given larger prior mass. Consequently 
a more advanced analysis demonstrating that only a subset of learning rates could potentially be
optimal (in the sense of minimizing the regret Rη
T ) would directly lead to factors of improvement in
the algorithm. Thus we raise the open question: what is the smallest subset E of learning rates such
that  for any data  the minimum of the regret over this subset minη∈E Rη
T is approximately the same
as the minimum minη Rη

T over all or a large range of learning rates?

8

References
[1] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-

putation  108(2):212–261  1994.

[2] V. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences 

56(2):153–173  1998.

[3] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  55:119–139  1997.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[5] S. de Rooij  T. van Erven  P. D. Gr¨unwald  and W. M. Koolen. Follow the leader if you can 

Hedge if you must. Journal of Machine Learning Research  15:1281–1316  2014.

[6] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algo-

rithms. Journal of Computer and System Sciences  64:48–75  2002.

[7] N. Cesa-Bianchi  Y. Mansour  and G. Stoltz.

Improved second-order bounds for prediction

with expert advice. Machine Learning  66(2/3):321–352  2007.

[8] T. van Erven  P. Gr¨unwald  W. M. Koolen  and S. de Rooij. Adaptive hedge. In Advances in

Neural Information Processing Systems 24 (NIPS)  2011.

[9] M. Devaine  P. Gaillard  Y. Goude  and G. Stoltz. Forecasting electricity consumption by ag-
gregating specialized experts; a review of the sequential aggregation of specialized experts 
with an application to Slovakian and French country-wide one-day-ahead (half-)hourly predic-
tions. Machine Learning  90(2):231–260  2013.

[10] P. Gr¨unwald. The safe Bayesian: learning the learning rate via the mixability gap. In Proceed-
ings of the 23rd International Conference on Algorithmic Learning Theory (ALT). Springer 
2012.

[11] V. Vovk. Competitive on-line statistics. International Statistical Review  69:213–248  2001.
[12] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley  1991.

9

,Wouter Koolen
Tim van Erven
Peter Grünwald