2015,Cornering Stationary and Restless Mixing Bandits with Remix-UCB,We study the restless bandit problem where arms are associated with stationary $\varphi$-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of carefully recovering some independence by `ignoring' the values of some rewards. As we shall see  the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off  which we do by considering the idea of a {\em waiting arm} in the new Remix-UCB algorithm  a generalization of Improved-UCB for the problem at hand  that we introduce. We provide a regret analysis for this bandit strategy; two noticeable features of Remix-UCB are that i) it reduces to the regular Improved-UCB when the $\varphi$-mixing coefficients are all $0$  i.e. when the i.i.d scenario is recovered  and ii) when $\varphi(n)=O(n^{-\alpha})$  it is able to ensure a controlled regret of order $\Ot\left(  \Delta_*^{(\alpha- 2)/\alpha} \log^{1/\alpha} T\right) $ where $\Delta_*$ encodes the distance between the best arm and the best suboptimal arm  even in the case when $\alpha<1$  i.e. the case when the $\varphi$-mixing coefficients {\em are not} summable.,Cornering Stationary and Restless Mixing Bandits

with Remix-UCB

Julien Audiffren

CMLA

ENS Cachan  Paris Saclay University

94235 Cachan France

audiffren@cmla.ens-cachan.fr

liva.ralaivola@lif.univ-mrs.fr

Liva Ralaivola

QARMA  LIF  CNRS

Aix Marseille University

F-13289 Marseille cedex 9  France

Abstract

We study the restless bandit problem where arms are associated with stationary
ϕ-mixing processes and where rewards are therefore dependent: the question that
arises from this setting is that of carefully recovering some independence by ‘ig-
noring’ the values of some rewards. As we shall see  the bandit problem we tackle
requires us to address the exploration/exploitation/independence trade-off  which
we do by considering the idea of a waiting arm in the new Remix-UCB algo-
rithm  a generalization of Improved-UCB for the problem at hand  that we
introduce. We provide a regret analysis for this bandit strategy; two noticeable
features of Remix-UCB are that i) it reduces to the regular Improved-UCB
when the ϕ-mixing coefﬁcients are all 0  i.e. when the i.i.d scenario is recovered 
and ii) when ϕ(n) = O(n−α)  it is able to ensure a controlled regret of order
  where ∆∗ encodes the distance between the best arm
and the best suboptimal arm  even in the case when α < 1  i.e. the case when the
ϕ-mixing coefﬁcients are not summable.

∆(α−2)/α
∗

log1/α T

(cid:17)

(cid:16)

(cid:101)Θ

1

Introduction

Bandit with mixing arms. The bandit problem consists in an agent who has to choose at each step
between K arms. A stochastic process is associated to each arm  and pulling an arm produces a
reward which is the realization of the corresponding stochastic process. The objective of the agent
is to maximize its long term reward. In the abundant bandit literature  it is often assumed that the
stochastic process associated to each arm is a sequence of independently and identically distributed
(i.i.d) random variables (see  e.g. [12]). In that case  the challenge the agent has to address is the
well-known exploration/exploitation problem: she has to simultaneously make sure that she collects
information from all arms to try to identify the most rewarding ones—this is exploration—and to
maximize the rewards along the sequence of pulls she performs—this is exploitation. Many algo-
rithms have been proposed to solve this trade-off between exploration and exploitation [2  3  6  12].
We propose to go a step further than the i.i.d setting and to work in the situation where the process
associated with each arm is a stationary ϕ-mixing process: the rewards are thus dependent from one
another  with a strength of dependence that weakens over time. From an application point of view 
this is a reasonable dependence structure: if a user clicks on some ad (a typical use of bandit algo-
rithms) at some point in time  it is very likely that her choice will have an inﬂuence on what she will
click in the close future  while it may have a (lot) weaker impact on what ad she will choose to view
in a more distant future. As it shall appear in the sequel  working with such dependent observations
poses the question of how informative are some of the rewards with respect to the value of an arm
since  because of the dependencies and the strong correlation between close-by (in time) rewards 
they might not reﬂect the true ‘value’ of the arms. However  as the dependencies weaken over time 
some kind of independence might be recovered if some rewards are ignored  in some sense. This

1

actually requires us to deal with a new tradeoff  the exploration/exploitation/independence tradeoff 
where the usual exploration/exploitation compromise has to be balanced with the need for some
independence. Dealing with this new tradeoff is the pivotal feature of our work.
Non i.i.d bandit. A closely related setup that addresses the bandit problem with dependent rewards
is when they are distributed according to Markov processes  such as Markov chains and Markov
decision process (MDP) [16  22]  where the dependences between rewards are of bounded range 
which is what distinguishes those works with ours. Contributions in this area study two settings:
the rested case  where the process attached to an arm evolves only when the arm is pulled  and the
restless case  where all processes simultaneously evolve at each time step. In the present work  we
will focus on the restless setting. The adversarial bandit setup (see e.g. [1  4  19]) can be seen as
a non i.i.d setup as the rewards chosen by the adversary might depend on the agent’s past actions.
However  even if the algorithms developed for this framework can be used in our setting  they might
perform very poorly as they are not designed to take advantage of any mixing structure. Finally  we
may also mention the bandit scenario where the dependencies are between the arms instead being
within-arm time-dependent (e.g.  [17]); this is orthogonal to what we propose to study here.
Mixing Processes. Mixing process theory is hardly new. One of the seminal works on the study of
mixing processes was done by Bernstein [5] who introduced the well-known block method  central
to prove results on mixing processes.
In statistical machine learning  one of the ﬁrst papers on
estimators for mixing processes is [23]. More recent works include the contributions of Mohri and
Rostamizadeh [14  15]  which address the problem of stability bound and Rademacher stability for
ϕ- and β-mixing processes; Kulkarni et al [11] establish the consistency of regularized boosting
algorithms learning from β-mixing processes  Steinwart et al [21] prove the consistency of support
vector machines learning from α-mixing processes and Steinwart and Christmann [20] establish a
general oracle inequality for generic regularized learning algorithms and α-mixing observations. As
far as we know  it is the ﬁrst time that mixing processes are studied in a multi-arm bandit framework.
Contribution. Our main result states that a strategy based on the improved Upper Conﬁdence
Bound (or Improved-UCB  in the sequel) proposed by Auer and Ortner [2]  allows us to achieve a
controlled regret in the restless mixing scenario. Namely  our algorithm  Remix-UCB (which stands
log1/α T )  where ∆∗ encodes
the distance between the best arm and the best suboptimal arm  α encodes the rate of decrease

for Restless Mixing UCB)  achieves a regret of the form (cid:101)Θ(∆(α−2)/α
of the ϕ coefﬁcients  i.e. ϕ(n) = O(nα)  and (cid:101)Θ is a O-like notation (that neglects logarithmic

∗

dependencies  see Section 2.2). It is worth noticing that all the results we give hold for α < 1  i.e.
when the dependencies are no longer summable. When the mixing coefﬁcients at hand are all zero 
i.e. in the i.i.d case  the regret of our algorithm naturally reduces to the classical Improved-UCB.
Remix-UCB uses the assumption about known (convergence rates of) ϕ-mixing coefﬁcients  which
is a classical standpoint that has been used by most of the papers studying the behavior of machine
learning algorithms in the case of mixing processes (see e.g. [9  14  15  18  21  23]). The estimation
of the mixing coefﬁcients poses a learning problem on its own (see e.g. [13] for the estimation of
β-mixing coefﬁcients) and is beyond the scope of this paper.
Structure of the paper. Section 2 deﬁnes our setup: ϕ-mixing processes are recalled  together with
a relevant concentration inequality for such processes [10  15]; the notion of regret we focus on is
given. Section 3 is devoted to the presentation of our algorithm  Remix-UCB  and to the statement
of our main result regarding its regret. Finally  Section 4 discusses the obtained results.

2 Overview of the Problem

2.1 Concentration of Stationary ϕ-mixing Processes
Let (Ω F  P) be a probability space. We recall the notions of stationarity and ϕ-mixing processes.
Deﬁnition 1 (Stationarity). A sequence of random variables X = {Xt}t∈Z is stationary if  for any
t  m ≥ 0  s ≥ 0  (Xt  . . .   Xt+m) and (Xt+s  . . .   Xt+m+s) are identically distributed.
Deﬁnition 2 (ϕ-mixing process). Let X = {Xt}t∈Z be a stationary sequence of random variables.
For any i  j ∈ Z ∪ {−∞  +∞}  let σj
i denote the σ-algebra generated by {Xt : i ≤ t ≤ j}. Then 

2

for any positive n  the ϕ-mixing coefﬁcient ϕ(n) of the stochastic process X is deﬁned as

(cid:12)(cid:12)P [A|B] − P [A](cid:12)(cid:12).

(1)

ϕ(n) =

sup

t A∈σ+∞

t+n B∈σt−∞ P(B)>0

X is ϕ-mixing if ϕ(n) → 0. X is algebraically mixing if ∃ϕ0 > 0  α > 0 so that ϕ(n) = ϕ0n−α.
As we recall later  concentration inequalities are the pivotal tools to devise multi-armed bandits
strategy. Hoeffding’s inequality [7  8] is  for instance  at the root of a number of UCB-based methods.
This inequality is yet devoted to characterize the deviation of the sum of independent variables from
its expected value and cannot be used in the framework we are investigating. In the case of stationary
ϕ-mixing distributions  there however is the following concentration inequality  due to [10] and [15].
Theorem 1 ([10  15]). Let ψm : U m → R be a function deﬁned over a countable space U  and X
be a stationary ϕ-mixing process. If ψm is (cid:96)-Lipschitz wrt the Hamming metric for some (cid:96) > 0  then

= 1 + 2(cid:80)m

.

∀ε > 0  PX [|ψm(X) − Eψm(X)| > ε] ≤ 2 exp

 

(2)

where Λm

τ =1 ϕ(τ ) and ψm(X) = ψm(X0  . . .   Xm).

Here  we do not have to use this concentration inequality in its full generality as we will restrict
to the situation where ψm is the mean of its arguments  i.e. ψm(Xt1  . . .   Xtm )
i=1 Xti  
which is obviously 1/m-Lipschitz provided that the Xt’s have range [0; 1]—which will be one of
our working assumptions. If  with a slight abuse of notation  Λm is now used to denote

.
= 1
m

(cid:80)m

(cid:20)

− ε2

2m(cid:96)2Λ2
m

(cid:21)

Λm(t)

.
= 1 + 2

ϕ(ti − t1) 

(3)

m(cid:88)

i=2

(cid:20)

(cid:35)

P{Xt}t∈t

m(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > ε

for an increasing sequence t = (ti)m
serve our purpose is given in the next corollary.
Corollary 1 ([10  15]). Let X be a stationary ϕ mixing process. The following holds: for all ε > 0
and all m-sequence t = (ti)m

i=1 of times steps  then  the concentration inequality that will

i=1 with t1 < . . . < tm 

(cid:21)
(Thanks to the stationarity of {Xt}t∈Z and the linearity of the expectation  E(cid:80)m

(cid:110)
1 + 2(cid:80)m

i=1 Xti = mEXt1.)
Remark 3. According to Kontorovitch’s paper
function Λm should be
. However  when the time lag between two consecutive
maxj
time steps ti and ti+1 is non-decreasing  which will be imposed by the Remix-UCB algorithm
(see below)  and the mixing coefﬁcients are decreasing  which is a natural assumption that simply
says that the amount of dependence between Xt and Xt(cid:48) reduces when |t − t(cid:48)| increases  then Λm
reduces to the more compact expression given by (3).

i=j+1 ϕ(ti − tj)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:111)

− mε2
2Λ2
m(t)

Xti − EX1

[10] 

the

≤ 2 exp

.

(4)

m

Note that when there is independence  then ϕ(τ ) = 0  for all τ  Λm = 1 and  as a consequence 
Equation (4) reduces to Hoefﬁng’s inequality: the precise values of the time instants in t do not
impact the value of the bound and the length m of t is the central parameter that matters. This
is in clear contrast with what happens in the dependent setting  where the bound on the deviation
i=1 Xti/m from its expectation directly depends on the timepoints ti through Λm. For two
i=1 Xti/m may be more sharply con-
/m provided Λm(t) < Λm(t(cid:48))  which can be a consequence

of(cid:80)m
centrated around EX1 than(cid:80)m

i=1 of m timepoints (cid:80)m

sequences t = (ti)m
of a more favorable spacing of the points in t than in t(cid:48).

i=1 and t(cid:48) = (t(cid:48)
i)m
i=1 Xt(cid:48)

i

2.2 Problem: Minimize the Expected Regret

We may now deﬁne the multi-armed bandit problem we consider and the regret we want to control.
Restless ϕ-mixing Bandits. We study the problem of sampling from a K-armed ϕ-mixing bandit.
In our setting  pulling arm k at time t provides the agent with a realization of the random variable X k
t  

3

where the family(cid:8)X k

(cid:9)

t

t∈Z satisﬁes the following assumptions: (A) ∀k  (X k

mixing process with decreasing mixing coefﬁcients ϕk and (B) ∀k  X k
ﬁnite set (by stationarity  the same holds for any X k
Regret The regret we want to bound is the classical pseudo-regret which  after T pulls  is given by

t )t∈Z is a stationary ϕ-
1 takes its values in a discrete

t   with t (cid:54)= 1) included in [0; 1].
T(cid:88)

R(T )

.
= T µ

∗ − E

µIt

(5)

t=1

.
= EX k

1   µ∗ .

where µk
= maxk µk  and It is the index of the arm selected at time t. We want to
devise a strategy is capable to select  at each time t  the arm It so that the obtained regret is minimal.
Bottleneck. The setting we assume entails the possibility of long-term dependencies between the
rewards output by the arms. Hence  as evoked earlier  in order to choose which arm to pull  the agent
is forced to address the exploration/exploitation/independence trade-off where independence may
be partially recovered by taking advantage of the observation regarding spacings of timepoints that
induce sharper concentration of the empirical rewards than others. As emphasized later  targetting
good spacing in the bandit framework translates into the idea of ignoring the rewards provided by
some pulls to compute the empirical averages: this idea is carried by the concept of a waiting arm 
which is formally deﬁned later on. The questions raised by the waiting arm that we address with
the Remix-UCB algorithm are a) how often should the waiting arm be pulled so the concentration
of the empirical means is high enough to be relied on (so the usual exploration/exploitation tradeoff
can be tackled) and b) from the regret standpoint  how hindering is it to pull the waiting arm?

O and(cid:101)Θ analysis. In the analysis of Remix-UCB that we provide  just as is the case for most  if not
heavily rely on the usual O notation and on the (cid:101)Θ notation  that bears the following meaning.
Deﬁnition 4 ((cid:101)Θ notation). For any two functions f  g from R to R  we say that f = (cid:101)Θ(g) if there

all  analyses that exist for bandit algorithms  we will focus in the order of the regret and we will not
be concerned about the precise constants involved in the derived results. We will therefore naturally

exist α  β > 0 so that |f| logα |f| ≤ |g|  and |g| logβ |g| ≤ |f|.

3 Remix-UCB: a UCB Strategy for Restless Mixing Bandits

This section contains our main contributions: the Remix-UCB algorithm. From now on  we use
a ∨ b (resp. a ∧ b) for the maximum (resp. minimum) of two elements a and b. We consider that
the processes attached to the arms are algebraically mixing and for arm k  the exponent is αk > 0:
there exist ϕk 0 such that ϕk(t) = ϕk 0t−αk—this assumption is not very restrictive as considering
rates such as t−αk are appropriate/natural to capture and characterize the decreasing behavior of the
convergent sequence (ϕk(t))t. Also  we will sometimes say that arm k is faster (resp. slower) than
arm k(cid:48) for k (cid:54)= k(cid:48)  to convey the fact that αk > αk(cid:48) (resp. αk < α(cid:48)
k).
For any k and any increasing sequence τ = (τ (n))t

n=1 of t timepoints  the empirical reward(cid:98)µτ

k of
τ (n). The subscripted notation τ k = (τk(n))1≤n≤t is used to denote
k in a similar way as

the sequence of timepoints at which arm k was selected. Finally  we deﬁne Λτ
in (3)  the difference with the former notation being the subscript k  as

k given τ is(cid:98)µτ

k

(cid:80)t

n=1 X k

.
= 1
t

Λτ k
k

.
= 1 + 2

ϕk(τk(n) − τk(1)).

(6)

We feel important to discuss when Improved-UCB may be robust to the mixing process scenario.

n=1

3.1 Robustness of Improved-UCB to Restless ϕ-Mixing Bandits

We will not recall the Improved-UCB algorithm [2] in its entirety as it will turn out to be a special
case of our Remix-UCB algorithm  but it is instructive to identify its distinctive features that make
it a relevant base algorithm for the handling of mixing processes. First  it is essential to keep in mind
that Improved-UCB is designed for the i.i.d case and that it achieves an optimal O(log T ) regret.
Second  it is an algorithm that works in successive rounds/epochs  at the end of each of which a
number of arms are eliminated because they are identiﬁed (with high probability) as being the least

4

t(cid:88)

promising ones  from a regret point of view. More precisely  at each round  the same number of
consecutive pulls is planned for each arm: this number is induced by Hoeffding’s inequality [8] and
devised in such a way that all remaining arms share the same conﬁdence interval for their respective
expected gains  the µk = EX k
1   for k in the set of remaining arms at the current round. From a
technical standpoint  this is what makes it possible to draw conclusions on whether an arm is useless
(i.e. eliminated) or not. It is enlightening to understand what are the favorable and unfavorable
setups for Improved-UCB to keep working when facing restless mixing bandits. The following
Proposition depicts the favorable case.
t ϕk(t) < +∞  ∀k  then the classical Improved-UCB run on the restless
ϕ-mixing bandit preserves its O(log T ) regret.

Proof. Straightforward. Given the assumption on the mixing coefﬁcients  it exists M > 0 such that
t≥0 ϕk(t) < M. Therefore  from Theorem 1  for any arm k  and any sequence τ
  which is akin to Hoeffd-
ing’s inequality up to the multiplicative (1 + 2M )2 constant in the exponential. This  and the lines
to prove the O(log T ) regret of Improved-UCB [2] directly give the desired result.

Proposition 5. If(cid:80)
maxk∈{1 ···  K}(cid:80)
of |τ| consecutive timepoints  P (|µk −(cid:98)µτ
In the general case where(cid:80)
k = O(t3/4) and the concentration inequality from Corollary 1 for(cid:98)µτ
−1/2(cid:17)

t ϕk(t) < +∞ does not hold for every k  then nothing ensures for
Improved-UCB to keep working  the idea of consecutive pulls being the essential culprit. To
illustrate the problem  suppose that ∀k  ϕk(n) = n−1/4. Then  after a sequence τ = (t1 + 1  t1 +
2  . . .   t1 + t) of t consecutive time instances where k was selected  simple calculations give that
Λτ

(cid:16)− |τ|ε2

k| > ε) ≤ 2 exp

k reads as

2(1+2M )2

(cid:17)

P(|µk −(cid:98)µτ

k| > ε) ≤ 2 exp

(cid:16)−Cε2t

where C is some strictly positive constant. The quality of the conﬁdence interval that can be derived
from this concentration inequality degrades when additional pulls are performed  which counters
the usual nature of concentration inequalities and prevents the obtention of a reasonable regret for
Improved-UCB. This is a direct consequence of the dependency of the ϕ-mixing variables. In-
deed  if ϕ(n) decreases slowly  taking the average over multiple consecutive pulls may move the
estimator away from the mean value of the stationary process.
Another way of understanding the difference between the i.i.d. case and the restless mixing case is
to look at the sizes of the conﬁdence intervals around the true value of an arm when the time t to the
next pull increases. Given Corollary 1  Improved-UCB run in the restless mixing scenario would
advocate a pulling strategy based on the lengths κk of the conﬁdence intervals given by

(7)

(8)

= |τ k|−1/2(cid:113)

.

∀k  κk(t)

2(Λτ k

k + 2ϕk(t − τ (1)))2 log(t)

where t is the overall time index. This shows that working in the i.i.d. case or in the mixing case
can imply two different behaviors for the lengths of the conﬁdence interval: in the i.i.d. scenario 
κk has the same form as the classical UCB term (as ϕk = 0 and Λτ k
k = 1) and is an increasing
function of t while in the ϕ-mixing scenario the behavior may be non-monotonic with a decreasing
conﬁdence interval up to some point after which the conﬁdence interval becomes increasingly larger.
As the purpose of exploration is to tighten the conﬁdence interval as much as possible  the mixing
framework points to carefully designed strategies. For instance  when an arm is slow  it is beneﬁcial
to wait between two successive pulls of this arm.
By alternating the pulls of the different arms  it is possible to wait up to K unit of time between
two consecutive pulls of the same arm. However  it is not sufﬁcient to recover enough independence
between the two observed values. For instance  in the case described in (7)  after a sequence τ =
k = O((Kt)3/4) and the concentration
(t1  t1 + K  . . .   t1 + tK)  simple calculations give that Λτ

k| > ε) ≤ 2 exp(cid:0)−CK 3/2ε2t−1/2(cid:1) which

inequality from Corollary 1 for(cid:98)µτ

k reads as P(|µk −(cid:98)µτ

entails the same problem.
The problem exhibited above is that if the decrease of the ϕk is too slow  pulling an arm in the
traditional way  with consecutive pulls  and updating the value of the empirical estimator may lower
the certainty with which the estimation of the expected gain is performed. To solve this problem
and reduce the conﬁdence interval that are computed for each arm  a better independence between

5

Algorithm 1 Remix-UCB  with parameter K  (αi)i=1···K  T   G deﬁned in (11)

B0 ← {1 ···   K} α ← 1 ∧ mini∈B0 αi (cid:98)µi ← 0  ni

for s = 1  . . .  (cid:98)G−1(T )(cid:99) do

0 ← 0    k = 1  . . .   K   i∗ ← 1

Select arm : If |Bs| > 1  then until total time Ts = (cid:100)G(s)(cid:101) pull each arm i ∈ Bs at time τi(·)
deﬁned in (10). If no arm is ready to be pulled  pull the waiting arm i∗ instead.
Update :

Update the empirical mean(cid:98)µi and the number of pulls ni for each arm i ∈ Bs.
(cid:115)
(cid:98)µi +

Obtain Bs+1 by eliminating from Bs each arm i such that

j=1 ϕk(τk(j)))2 log(T 2−2s)

j=1 ϕi(τi(j)))2 log(T 2−2s)

(1 + 2(cid:80)nk

(1 + 2(cid:80)ni

(cid:115)

< max

1.
2.

2

2

ni

nk

k∈Bs(cid:98)µk −
(cid:115)

(cid:98)µi +

2

(1 + 2(cid:80)ni

j=1 ϕi(τi(j)))2 log(T 2−2s)

ni

3.

update

α ← 1 ∧ min
i∈Bs+1

αi  and i

∗ ← argmax
i∈Bs+1

end for

the values observed from a given arm is required. This can only be achieved by waiting for the
time to pass by. Since an arm must be pulled at each time t  simulating the time passing by may be

implemented by the idea to pull an arm but not to update the empirical mean(cid:98)µk of this arm with

the observed reward. At the same time  it is important to note that even if we do not update the
empirical mean of the arm  the resort to the waiting arm may impact the regret. It is therefore crucial
to ensure that we pull the best possible arm to limit the resulting regret  whence the arm with the
best optimistic value  being used as the waiting arm. Note that this arm may change over time. For
the rest of the paper  τ will only refer to signiﬁcant pulls of an arm  that is  pulls that lead to an
update of the empirical value of the arm.

3.2 Algorithm and Regret bound

We may now introduce Remix-UCB  depicted in Algorithm 1. As Improved-UCB  Remix-UCB
works in epochs and eliminates  at each epoch  the signiﬁcantly suboptimal arms.
+ and (δs)s∈N ∈ RN
High-Level View. Let (θs)s∈N be a decreasing sequence of R∗
+. The main idea
promoted by Remix-UCB is to divide the time available in epochs 1  . . .   smax (the outer loop of
the algorithm)  such that at the end of each epoch s  for all the remaining arms k the following holds 
k ≤ µk − θs) ≤ δs  where τ k identiﬁes the time instants up to current

P((cid:98)µτ k
k ≥ µk + θs) ∨ P((cid:98)µτ k

time t when arm k was selected. Using (4)  this means that  for all k  with high probability:

2(Λτ

k )2 log(δs).

(9)

k

with which the empirical rewards (cid:98)µτ k

Thus  at the end of epoch s we have  with high probability  a uniform control of the uncertainty
approximate their corresponding rewards µk. Based on
this  the algorithm eliminates the arms that appear signiﬁcantly suboptimal (step 2 of the update
of Remix-UCB). Just as in Improved-UCB  the process is re-iterated with parameters δs and θs
adjusted as δs = 1/(T θ2
s) and θs = 1/2s  where T is the time budget; the modiﬁcations of the δs
and θs values makes it possible to gain additional information  through new pulls  on the quality of
the remaining arms  so arms associated with close-by rewards can be distinguished by the algorithm.
Policy for pulling arms at epoch s. The objective of the policy is to obtain a uniform control of the
uncertainty/conﬁdence intervals (9) of all the remaining arms. For some arm k and ﬁxed time budget
< ε where
T   such a policy could be obtained as the solution of minηs (ti)ηs
the times of pulls ti’s must be increasing and greater than t0 the last element of τ s−1  τ s = τ s−1 ∪
(t1  ...tηs) and ns−1 (the number of times this arm has already been pulled)  ε  τ s−1 are given. This
conveys our aim to obtain as fast and efﬁciently the targetted conﬁdence interval. However  this
problem does not have a closed-form solution and  even if it could be solved efﬁciently  we are more
interested in assessing whether it is possible to devise relevant sequences of timepoints that induce a
controlled regret  even if they do not solve the optimization problem. To this end  we only focus on

tηs such that

(Λτ s )2
ns−1+ηs

i=1

|(cid:98)µτ k
k − µk| ≤ nk

−1/2(cid:113)

6

the best sampling rate of the arms  which is an approximation of the previous minimization problem:
for each k  we search for sampling schemes of the form τk(n) = tn = O(nβ) for β ≥ 1. For the
case where the ϕk are not summable ( αk ≤ 1)  we have the following result.
Proposition 6. Let αk ∈ (0; 1] (recall that ϕk(n) = n−αk). The optimal sampling rate τk for arm

k is τk(n) = (cid:101)Θ(n1/αk ).
In other words (cid:80)
is to take β = 1/α  which directly comes from the fact that this is the point where(cid:80)

Proof. The idea of the proof is that if the sampling is too frequent (i.e. β close to 1)  then the
dependency between the values of the arm reduces the information obtained by taking the average.
n ϕk(τk(n)) increases too quickly. On the other hand  if the sampling is too scarce
(i.e. β is very large)  the information obtained at each pull is important  but the total amount of pulls
in a given time T is approximately T 1/β and thus is too low. The optimal solution to this trade-off
n ϕk(τk(n))

becomes logarithmic. The complete proof is available in the supplementary material.

If αk < 1  for all k  this result means that the best policy (with a sampling scheme of the form
O(nβ)) should update the empirical means associated with each arm k at a rate O(n1/αk ); contrary
to the i.i.d case it is therefore not relevant to try and update the empirical rewards at each time step.
There henceforth must be gaps between updates of the means: this is precisely the role of the waiting
arm to make this gaps possible. As seen in the depiction of Remix-UCB  when pulled  the waiting
arm provides a reward that will count for the cumulative gains of the agent and help her control her
regret  but that will not be used to update any empirical mean.
As for a precise pulling strategy to implement given Proposition 6  it must be understood that it
is the slowest arm that determines the best uniform control possible  since it is the one which will
be selected the least number of times: it is unnecessary to pull the fastest arms more often than
= 1 ∧
.
the slowest arm. Therefore  if i1  . . .   iks are the ks remaining arms at epoch s  and α
1  then an arm selection strategy based on the rate of the slowest arm suggests to
mini∈{i1 ... iks} αi
pull arm im and update ˆµτ im
im

for the n-th time at time instants

(cid:26) (τi1 (n − 1) + ks) ∨ (cid:100)n1/α(cid:101)

τi1 (n) + m − 1

if m = 1
otherwise

(10)

(i.e. all arms are pulled at the same O(n1/α) frequency) and to pull the waiting arm while waiting.
Time budget per epoch. In the Remix-UCB algorithm  the function G deﬁnes the size of the
rounds. The deﬁnition of G is rather technical: we have G(s) = maxk∈Bs Gk(s) where

= inf(cid:8)t ∈ N+  2(Λτ

.

Gk(s)

k )2 log(1/δs) ≤ tθs

(cid:9)

(11)

s

where the τk(n) are deﬁned above.
In other words  Gk encodes the minimum amount of time
necessary to reach the aimed length of conﬁdence interval by following the aforementioned policy.
log(δs))1/α). This is the key element

But the most interesting property of G is that G(s) = (cid:101)Θ((θ−2

which will be used in the proof of the regret bound which can be found in Theorem 2 below.
Putting it all together. At epoch s  the Remix-UCB algorithm starts by selecting the best empirical
arm and ﬂags it as the waiting arm. It then determines the speed α of the slowest arm  after which it
computes a time budget Ts = G(s). Then  until this time horizon is reached  it pulls arms following
the policy described above. Finally  after the time budget is reached  the algorithm eliminates the
arms whose empirical mean is signiﬁcantly lower than the best available empirical mean.
Note that when all the ϕk are summable  we have α = 1  and thus the algorithm never pulls the
waiting arm: Remix-UCB mainly differs from Improved-UCB by its strategy of alternate pulls.
The result below provides an upper bound for the regret of the Remix-UCB algorithm:
Theorem 2. For all arm k  let 1 ≥ αk > 0 and ϕk(n) = n−αk . Let α = mink∈{1 ···  K} αk and
∆∗ = mink∈{1 ···  K}{∆k > 0}. If α ≤ 1  the regret of Remix-UCB is bounded in order by

(cid:16)

(cid:101)Θ

∆(α−2)/α

∗

log(T )1/α(cid:17)

.

(12)

1Since 1/α encodes the rate of sampling  it cannot be greater than 1.

7

Proof. The proof follows the same line as the proof of the upper bound of the regret of the
Improved-UCB algorithm. The important modiﬁcation is the sizes of the blocks  which depend
in the mixing case of the ϕ mixing coefﬁcient  and might grow arbitrary large  and the waiting arm 
which does not exist in the i.i.d. setting. The dominant term in the regret mentioned in Theorem 2
is related to the pulls of the waiting arm. Indeed  the waiting arm is pulled with an always increas-
ing frequency  but the quality of the waiting arm tends to increase over time  as the arms with the
smallest values are eliminated. The complete proof is available in the supplementary material.

4 Discussion and Particular Cases

We here discuss Theorem 2  and some of its variations for special cases of ϕ-mixing processes.

First  in the i.i.d case  the regret of Improved-UCB is upper bounded by O(cid:0)∆−1∗

log(T )(cid:1) [2].

Observe that (12) comes down to this bound when α tends to 1. Also  note that it is an upper bound
of the regret in the algebraically mixing case. It reﬂects the fact that in this particular case  it is
possible to ignore the dependency of the mixing process. It also implies that  even if α < 1  i.e.
even if the dependency cannot be ignored  by properly using the ϕ mixing property of the different
stationary processes  it is possible to obtain an upper bound of polynomial logarithmic order.
Another question is to see what happens when αk = 1  which is an important threshold in our study.
Indeed  if αk = 1 the ϕk are not summable  but from Proposition 6 we have that τk(n) ≈ O(n)  i.e.
the arms should be sampled as often as possible. Theorem 2 states that the regret is upper bounded
log T ). However  it is not possible to know if this bound is comparable to

in this case by (cid:101)Θ(∆−1∗
that of the i.i.d case due to the (cid:101)Θ. Still  from the proof of Theorem 2 we get the following result:

Corollary 2. For all arm k  let 1 ≥ αk > 0 and ϕk(n) = n−αk . Let α = mink∈{1 ···  K} αk. Then
if α = 1  the regret for Algorithm 1 is upper bounded in order by

O(cid:0)∆

−1∗ Gα(log(T ))(cid:1)

(13)

α (x) = xα/(log(x))2.

where ∆∗ = mink∈{1 ···  K}{∆k > 0} and G is solution of G−1
Although we do not have an explicit formula for the regret in the case α = 1  it is interesting to note

log(T )(cid:1).
that (13) is strictly negligible with respect to (12) ∀α < 1  but strictly dominates O(cid:0)∆−1∗
will only achieve a regret of (cid:101)Θ(cid:0)exp(cid:2)(T /∆∗)1/α(cid:3)(cid:1)   which is no longer logarithmic in T. In other

This comes from that while in the case α = 1 the waiting arm is no longer used  the time budget
necessary to complete step s is still higher that in the i.i.d case.
When ϕ(n) decreases at a logarithmic speed (ϕ(n) ≈ 1/log(n)α for some α > 0)  it is still possible
to apply the same reasoning as the one developed in this paper. But in this case  Remix-UCB

words  if the ϕ mixing coefﬁcients decrease too slowly  the information given by the concentration
inequality in Theorem 1 is not sufﬁcient to deduce interesting information about the mean value of
the arms. In this case  the successive values of the ϕ-mixing processes are too dependent  and the
randomness in the sequence of values is almost negligible; an adversarial bandit algorithm such as
Exp4 [4] may give better results than Remix-UCB.

5 Conclusion

We have studied an extension of the multi-armed bandit problem to the stationary ϕ-mixing frame-
work in the restless case  by providing a functional algorithm and an upper bound of the regret in a
general framework. Future work might include a study of a lower bound for the regret in the mixing
process case: our ﬁrst ﬁndings on the issue are that the analysis of the worst-case scenario in the
mixing framework bears signiﬁcant challenges. Another interesting point would be the study of the
more difﬁcult case of β-mixing processes. A rather different  but very interesting question that we
may address in the future is the possibility to exploit a possible structure of the correlation between
rewards over time. For instance  in the case wher the correlation of an arm with the close past is
much higher than the correlation with the distant past  it might be interesting to see if the analysis
done in [16] can be extended to exploit this correlation structure.
Acknowledgments. This work is partially supported by the ANR-funded projet GRETA – Greedi-
ness: theory and algorithms (ANR-12-BS02-004-01) and the ND project.

8

References
[1] Audibert JY  Bubeck S (2009) Minimax policies for adversarial and stochastic bandits. In: Annual Con-

ference on Learning Theory

[2] Auer P  Ortner R (2010) Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit

problem. Periodica Mathematica Hungarica 61:5565

[3] Auer P  Cesa-Bianchi N  Fischer P (2002) Finite-time analysis of the multi- armed bandit problem. Ma-

chine Learning Journal 47(23):235–256

[4] Auer P  Cesa-Bianchi N  Freund Y  Schapire RE (2002) The nonstochastic multiarmed bandit problem.

SIAM Journal on Computing 32(1):48–77

[5] Bernstein S (1927) Sur l’extension du th´eor`eme limite du calcul des probabilit´es aux sommes de quantit´es

d´ependantes. Mathematische Annalen 97(1):1–59

[6] Bubeck S  Cesa-Bianchi N (2012) Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit

Problems  Foundation and Trends in Machine Learning  vol 5. NOW

[7] Hoeffding W (1948) A Class of Statistics with Asymptotically Normal Distribution. Annals of Mathe-

matical Statistics 19(3):293–325

[8] Hoeffding W (1963) Probability inequalities for sums of bounded random variables. Journal of the Amer-
ican Statistical Association 58(301):13–30  DOI 10.2307/2282952  URL http://dx.doi.org/10.
2307/2282952

[9] Karandikar RL  Vidyasagar M (2002) Rates of uniform convergence of empirical means with mixing

processes. Statistics & probability letters 58(3):297–307

[10] Kontorovich L  Ramanan K (2008) Concentration inequalities for dependent random variables via the

martingale method. The Annals of Probability 36(6):2126–2158

[11] Kulkarni S  Lozano A  Schapire RE (2005) Convergence and consistency of regularized boosting algo-
rithms with stationary β-mixing observations. In: Advances in neural information processing systems  pp
819–826

[12] Lai TL  Robbins H (1985) Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Math-

ematics 6:422

[13] McDonald D  Shalizi C  Schervish M (2011) Estimating beta-mixing coefﬁcients. arXiv preprint

arXiv:11030941

[14] Mohri M  Rostamizadeh A (2009) Rademacher complexity bounds for non-i.i.d. processes. In: Koller D 
Schuurmans D  Bengio Y  Bottou L (eds) Advances in Neural Information Processing Systems 21  pp
1097–1104

[15] Mohri M  Rostamizadeh A (2010) Stability bounds for stationary -mixing and -mixing processes. Journal

of Machine Learning Research 11:789–814

[16] Ortner R  Ryabko D  Auer P  Munos R (2012) Regret bounds for restless markov bandits. In: Proceeding

of the Int. Conf. Algorithmic Learning Theory  pp 214–228

[17] Pandey S  Chakrabarti D  Agarwal D (2007) Multi-armed bandit problems with dependent arms. In: Pro-

ceedings of the 24th international conference on Machine learning  ACM  pp 721–728

[18] Ralaivola L  Szafranski M  Stempfel G (2010) Chromatic pac-bayes bounds for non-iid data: Applications
to ranking and stationary β-mixing processes. The Journal of Machine Learning Research 11:1927–1956
[19] Seldin Y  Slivkins A (2014) One practical algorithm for both stochastic and adversarial bandits. In: Pro-

ceedings of the 31st International Conference on Machine Learning (ICML-14)  pp 1287–1295

[20] Steinwart I  Christmann A (2009) Fast learning from non-iid observations. In: Advances in Neural Infor-

mation Processing Systems  pp 1768–1776

[21] Steinwart I  Hush D  Scovel C (2009) Learning from dependent observations. Journal of Multivariate

Analysis 100(1):175–194

[22] Tekin C  Liu M (2012) Online learning of rested and restless bandits. IEEE Transactions on In-
formation Theory 58(8):5588–5611  URL http://dblp.uni-trier.de/db/journals/tit/
tit58.html#TekinL12

[23] Yu B (1994) Rates of convergence for empirical processes of stationary mixing sequences. Annals of

Probability 22(1):94–116

9

,Julien Audiffren
Liva Ralaivola