2013,Adaptive Submodular Maximization in Bandit Setting,Maximization of submodular functions has wide applications in machine learning and artificial intelligence. Adaptive submodular maximization has been traditionally studied under the assumption that the model of the world  the expected gain of choosing an item given previously selected items and their states  is known. In this paper  we study the scenario where the expected gain is initially unknown and it is learned by interacting repeatedly with the optimized function. We propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time. Our regret bound captures the inherent property of submodular maximization  earlier mistakes are more costly than later ones. We refer to our approach as Optimistic Adaptive Submodular Maximization (OASM) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle. We evaluate our method on a preference elicitation problem and show that non-trivial K-step policies can be learned from just a few hundred interactions with the problem.,Adaptive Submodular Maximization in Bandit Setting

Victor Gabillon

INRIA Lille - team SequeL
Villeneuve d’Ascq  France
victor.gabillon@inria.fr

Branislav Kveton
Technicolor Labs

Palo Alto  CA

branislav.kveton@technicolor.com

Zheng Wen

Electrical Engineering Department

Stanford University

zhengwen@stanford.edu

Brian Eriksson
Technicolor Labs

Palo Alto  CA

brian.eriksson@technicolor.com

S. Muthukrishnan

Department of Computer Science

Rutgers

muthu@cs.rutgers.edu

Abstract

Maximization of submodular functions has wide applications in machine learning
and artiﬁcial intelligence. Adaptive submodular maximization has been tradition-
ally studied under the assumption that the model of the world  the expected gain
of choosing an item given previously selected items and their states  is known. In
this paper  we study the setting where the expected gain is initially unknown  and
it is learned by interacting repeatedly with the optimized function. We propose an
efﬁcient algorithm for solving our problem and prove that its expected cumulative
regret increases logarithmically with time. Our regret bound captures the inherent
property of submodular maximization  earlier mistakes are more costly than later
ones. We refer to our approach as Optimistic Adaptive Submodular Maximization
(OASM) because it trades off exploration and exploitation based on the optimism in
the face of uncertainty principle. We evaluate our method on a preference elicita-
tion problem and show that non-trivial K-step policies can be learned from just a
few hundred interactions with the problem.

1

Introduction

Maximization of submodular functions [14] has wide applications in machine learning and artiﬁcial
intelligence  such as social network analysis [9]  sensor placement [10]  and recommender systems
[7  2]. In this paper  we study the problem of adaptive submodular maximization [5]. This problem
is a variant of submodular maximization where each item has a state and this state is revealed when
the item is chosen. The goal is to learn a policy that maximizes the expected return for choosing K
items.
Adaptive submodular maximization has been traditionally studied in the setting where the model of
the world  the expected gain of choosing an item given previously selected items and their states  is
known. This is the ﬁrst paper that studies the setting where the model is initially unknown  and it is
learned by interacting repeatedly with the environment. We bring together the concepts of adaptive
submodular maximization and bandits  and the result is an efﬁcient solution to our problem.
We make four major contributions. First  we propose a model where the expected gain of choosing
an item can be learned efﬁciently. The main assumption in the model is that the state of each item is
distributed independently of the other states. Second  we propose Optimistic Adaptive Submodular
Maximization (OASM)  a bandit algorithm that selects items with the highest upper conﬁdence bound
on the expected gain. This algorithm is computationally efﬁcient and easy to implement. Third  we
prove that the expected cumulative regret of our algorithm increases logarithmically with time. Our
regret bound captures the inherent property of adaptive submodular maximization  earlier mistakes
are more costly than later ones. Finally  we apply our approach to a real-world preference elicitation

1

problem and show that non-trivial policies can be learned from just a few hundred interactions with
the problem.

2 Adaptive Submodularity

In adaptive submodular maximization  the objective is to maximize  under constraints  a function of
the form:

f : 2I × {−1  1}L → R 

(1)
where I = {1  . . .   L} is a set of L items and 2I is its power set. The ﬁrst argument of f is a subset
of chosen items A ⊆ I. The second argument is the state φ ∈ {−1  1}L of all items. The i-th entry
of φ  φ[i]  is the state of item i. The state φ is drawn i.i.d. from some probability distribution P (Φ).
The reward for choosing items A in state φ is f (A  φ). For simplicity of exposition  we assume that
f (∅  φ) = 0 in all φ. In problems of our interest  the state is only partially observed. To capture this
phenomenon  we introduce the notion of observations. An observation is a vector y ∈ {−1  0  1}L
whose non-zero entries are the observed states of items. We say that y is an observation of state φ 
and write φ ∼ y  if y[i] = φ[i] in all non-zero entries of y. Alternatively  the state φ can be viewed
as a realization of y  one of many. We denote by dom (y) = {i : y[i] (cid:54)= 0} the observed items in y
and by φ(cid:104)A(cid:105) the observation of items A in state φ. We deﬁne a partial ordering on observations and
write y(cid:48) (cid:23) y if y(cid:48)[i] = y[i] in all non-zero entries of y  y(cid:48) is a more speciﬁc observation than y. In
the terminology of Golovin and Krause [5]  y is a subrealization of y(cid:48).
We illustrate our notation on a simple example. Let φ = (1  1 −1) be a state  and y1 = (1  0  0) and
y2 = (1  0 −1) be observations. Then all of the following claims are true:

φ ∼ y1  φ ∼ y2  y2 (cid:23) y1  dom (y2) = {1  3}   φ(cid:104){1  3}(cid:105) = y2  φ(cid:104)dom (y1)(cid:105) = y1.

Our goal is to maximize the expected value of f by adaptively choosing K items. This problem can
be viewed as a K step game  where at each step we choose an item according to some policy π and
then observe its state. A policy π : {−1  0  1}L → I is a function from observations y to items. The
observations represent our past decisions and their outcomes. A k-step policy in state φ  πk(φ)  is a
collection of the ﬁrst k items chosen by policy π. The policy is deﬁned recursively as:

πk(φ) = πk−1(φ) ∪(cid:8)π[k](φ)(cid:9)  

π[k](φ) = π(φ(cid:104)πk−1(φ)(cid:105)) 

π0(φ) = ∅ 

(2)

where π[k](φ) is the k-th item chosen by policy π in state φ. The optimal K-step policy satisﬁes:

π∗ = arg maxπ Eφ[f (πK(φ)  φ)] .

(3)
In general  the problem of computing π∗ is NP-hard [14  5]. However  near-optimal policies can be
computed efﬁciently when the maximized function has a diminishing return property. Formally  we
require that the function is adaptive submodular and adaptive monotonic [5].
Deﬁnition 1. Function f is adaptive submodular if:

Eφ[ f (A ∪ {i}   φ) − f (A  φ)| φ ∼ yA ] ≥ Eφ[ f (B ∪ {i}   φ) − f (B  φ)| φ ∼ yB ]

for all items i ∈ I \ B and observations yB (cid:23) yA  where A = dom (yA) and B = dom (yB).
Deﬁnition 2. Function f is adaptive monotonic if Eφ[ f (A ∪ {i}   φ) − f (A  φ)| φ ∼ yA ] ≥ 0 for
all items i ∈ I \ A and observations yA  where A = dom (yA).
In other words  the expected gain of choosing an item is always non-negative and does not increase
as the observations become more speciﬁc.
Let πg be the greedy policy for maximizing f  a policy that always selects the item with the highest
expected gain:

πg(y) = arg max
i∈I\dom(y)

gi(y) 

(4)

where:

gi(y) = Eφ[ f (dom (y) ∪ {i}   φ) − f (dom (y)   φ)| φ ∼ y ]

(5)
is the expected gain of choosing item i after observing y. Then  based on the result of Golovin and
Krause [5]  πg is a (1 − 1/e)-approximation to π∗  Eφ[f (πg
K(φ)  φ)] 
if f is adaptive submodular and adaptive monotonic. In the rest of this paper  we say that an obser-
vation y is a context if it can be observed under the greedy policy πg. Speciﬁcally  there exist k and
φ such that y = φ(cid:104)πg

K(φ)  φ)] ≥ (1 − 1/e)Eφ[f (π∗

k(φ)(cid:105).

2

3 Adaptive Submodularity in Bandit Setting

The greedy policy πg can be computed only if the objective function f and the distribution of states
P (Φ) are known  because both of these quantities are needed to compute the marginal beneﬁt gi(y)
(Equation 5). In practice  the distribution P (Φ) is often unknown  for instance in a newly deployed
sensor network where the failure rates of the sensors are unknown. In this paper  we study a natural
variant of adaptive submodular maximization that can model such problems. The distribution P (Φ)
is assumed to be unknown and we learn it by interacting repeatedly with the problem.

3.1 Model

The problem of learning P (Φ) can be cast in many ways. One approach is to directly learn the joint
P (Φ). This approach is not practical for two reasons. First  the number of states φ is exponential in
the number of items L. Second  the state of our problem is observed only partially. As a result  it is
generally impossible to identify the distribution that generates φ. Another possibility is to learn the
probability of individual states φ[i] conditioned on context  observations y under the greedy policy
πg in up to K steps. This is impractical because the number of contexts is exponential in K.
Clearly  additional structural assumptions are necessary to obtain a practical solution. In this paper 
we assume that the states of items are independent of the context in which the items are chosen. In
particular  the state φ[i] of each item i is drawn i.i.d. from a Bernoulli distribution with mean pi. In
this setting  the joint probability distribution factors as:

L(cid:89)

P (Φ = φ) =

1{φ[i]=1}
p
i

(1 − pi)1−1{φ[i]=1}

(6)

i=1

and the problem of learning P (Φ) reduces to estimating L parameters  the means of the Bernoullis.
A major question is how restrictive is our independence assumption. We argue that this assumption
is fairly natural in many applications. For instance  consider a sensor network where the sensors fail
at random due to manufacturing defects. The failures of these sensors are independent of each other
and thus can be modeled in our framework. To validate our assumption  we conduct an experiment
(Section 4) that shows that it does not greatly affect the performance of our method on a real-world
problem. Correlations obviously exist and we discuss how to model them in Section 6.
Based on the independence assumption  we rewrite the expected gain (Equation 5) as:

gi(y) = pi¯gi(y) 

(7)

where:

¯gi(y) = Eφ[ f (dom (y) ∪ {i}   φ) − f (dom (y)   φ)| φ ∼ y  φ[i] = 1 ]

(8)
is the expected gain when item i is in state 1. For simplicity of exposition  we assume that the gain
is zero when the item is in state −1. We discuss how to relax this assumption in Appendix.
In general  the gain ¯gi(y) depends on P (Φ) and thus cannot be computed when P (Φ) is unknown.
In this paper  we assume that ¯gi(y) can be computed without knowing P (Φ). This scenario is quite
common in practice. In maximum coverage problems  for instance  it is quite reasonable to assume
that the covered area is only a function of the chosen items and their states. In other words  the gain
can be computed as ¯gi(y) = f (dom (y) ∪ {i}   φ) − f (dom (y)   φ)  where φ is any state such that
φ ∼ y and φ[i] = 1.
Our learning problem comprises n episodes. In episode t  we adaptively choose K items according
to some policy πt  which may differ from episode to episode. The quality of the policy is measured
K(φt)  φt)]. We compare this return
to that of the greedy policy πg and measure the difference between the two returns by the expected
cumulative regret:

by the expected cumulative K-step return Eφ1 ... φn [(cid:80)n
(cid:34) n(cid:88)

(cid:34) n(cid:88)

t=1 f (πt

(cid:35)

R(n) = Eφ1 ... φn

Rt(φt)

= Eφ1 ... φn

f (πg

K(φt)  φt) − f (πt

K(φt)  φt)

.

(9)

(cid:35)

In maximum coverage problems  the greedy policy πg is a good surrogate for the optimal policy π∗
because it is a (1 − 1/e)-approximation to π∗ (Section 2).

t=1

t=1

3

Algorithm 1 OASM: Optimistic adaptive submodular maximization.

Input: States φ1  . . .   φn
for all i ∈ I do Select item i and set ˆpi 1 to its state  Ti(0) ← 1 end for
for all t = 1  2  . . .   n do

A ← ∅
for all k = 1  2  . . .   K do

(cid:40)

y ← φt(cid:104)A(cid:105)
A ← A ∪

arg max
i∈I\A

end for
for all i ∈ I do Ti(t) ← Ti(t − 1) end for
for all i ∈ A do
Ti(t) ← Ti(t) + 1
ˆpi Ti(t) ← 1

Ti(t) (ˆpi Ti(t−1)Ti(t − 1) + 1

2 (φt[i] + 1))

(ˆpi Ti(t−1) + ct−1 Ti(t−1))¯gi(y)

(cid:46) Choose the highest index

(cid:46) Initialization

(cid:46) K-step maximization

(cid:41)

(cid:46) Update statistics

end for

end for

3.2 Algorithm

s(cid:88)

z=1

Our algorithm is designed based on the optimism in the face of uncertainty principle  a strategy that
is at the core of many bandit algorithms [1  8  13]. More speciﬁcally  it is a greedy policy where the
expected gain gi(y) (Equation 7) is substituted for its optimistic estimate. The algorithm adaptively
maximizes a submodular function in an optimistic fashion and therefore we refer to it as Optimistic
Adaptive Submodular Maximization (OASM).
The pseudocode of our method is given in Algorithm 1. In each episode  we maximize the function
f in K steps. At each step  we compute the index (ˆpi Ti(t−1) + ct−1 Ti(t−1))¯gi(y) of each item that
has not been selected yet and then choose the item with the highest index. The terms ˆpi Ti(t−1) and
ct−1 Ti(t−1) are the maximum-likelihood estimate of the probability pi from the ﬁrst t − 1 episodes
and the radius of the conﬁdence interval around this estimate  respectively. Formally:

(cid:114)

ˆpi s =

1
s

1
2

(φτ (i z)[i] + 1) 

ct s =

2 log(t)

s

 

(10)

where s is the number of times that item i is chosen and τ (i  z) is the index of the episode in which
item i is chosen for the z-th time. In episode t  we set s to Ti(t − 1)  the number of times that item
i is selected in the ﬁrst t − 1 episodes. The radius ct s is designed such that each index is with high
probability an upper bound on the corresponding gain. The index enforces exploration of items that
have not been chosen very often. As the number of past episodes increases  all conﬁdence intervals
shrink and our method starts exploiting most proﬁtable items. The log(t) term guarantees that each
item is explored inﬁnitely often as t → ∞  to avoid linear regret.
Algorithm OASM has several notable properties. First  it is a greedy method. Therefore  our policies
can be computed very fast. Second  it is guaranteed to behave near optimally as our estimates of the
gain gi(y) become more accurate. We prove this claim in Section 3.3. Finally  our algorithm learns
only L parameters and therefore is quite practical. Speciﬁcally  note that if an item is chosen in one
context  it helps in reﬁning the estimate of the gain gi(y) in all other contexts.

3.3 Analysis

In this section  we prove an upper bound on the expected cumulative regret of Algorithm OASM in n
episodes. Before we present the main result  we deﬁne notation used in our analysis. We denote by
i∗(y) = πg(y) the item chosen by the greedy policy πg in context y. Without loss of generality  we
assume that this item is unique in all contexts. The hardness of discriminating between items i and
i∗(y) is measured by a gap between the expected gains of the items:

(11)
Our analysis is based on counting how many times the policies πt and πg choose a different item at
step k. Therefore  we deﬁne several variables that describe the state of our problem at this step. We

∆i(y) = gi∗(y)(y) − gi(y).

4

(cid:96)i

Gkαi k

+

π2L(L + 1)

(12)

denote by Yk(π) =(cid:83)

φ {φ(cid:104)πk−1(φ)(cid:105)} the set of all possible observations after policy π is executed
for k − 1 steps. We write Yk = Yk(πg) and Y t
k = Yk(πt) when we refer to the policies πg and πt 
respectively. Finally  we denote by Yk i = Yk ∩ {y : i (cid:54)= i∗(y)} the set of contexts where item i is
suboptimal at step k.
Our main result is Theorem 1. Supplementary material for its proof is in Appendix. The terms item
and arm are treated as synonyms  and we use whichever is more appropriate in a given context.
Theorem 1. The expected cumulative regret of Algorithm OASM is bounded as:

K(cid:88)
(cid:123)(cid:122)

k=1

R(n) ≤ L(cid:88)
(cid:124)
(cid:24)

i=1

i

max

2
3

(cid:124)

(cid:125)

(cid:25)

K(cid:88)

k=1

Gk

 

(cid:125)

(cid:123)(cid:122)

where Gk = (K − k + 1) max
y∈Yk

O(log n)
gi(y) is an upper bound on the expected gain of the policy πg

O(1)

from step k forward  (cid:96)i k =

¯g2
i (y)
i (y) log n
∆2
likely to be pulled suboptimally at step k  (cid:96)i = max

is a weight that associates the regret of arm i to step k such that(cid:80)K

(cid:96)i k  and αi k =

8 max
y∈Yk i

k

1
(cid:96)i

is the number of pulls after which arm i is not

(cid:2)(cid:96)i k − max

k(cid:48)<k

(cid:96)i k(cid:48)(cid:3)+ ∈ [0  1]

k=1 αi k = 1.

Proof. Our theorem is proved in three steps. First  we associate the regret in episode t with the ﬁrst
step where our policy πt selects a different item from the greedy policy πg. For simplicity  suppose
that this step is step k. Then the regret in episode t can be written as:

Rt(φt) = f (πg
= f (πg

(cid:124)

K(φt)  φt) − f (πt
(cid:123)(cid:122)
K(φt)  φt) − f (πg

F g

k→(φt)

K(φt)  φt)
k−1(φt)  φt)

(cid:125)

−[f (πt

(cid:124)

(cid:123)(cid:122)
K(φt)  φt) − f (πt

where the last equality is due to the assumption that πt
k→(φt)
and F t
k→(φt) are the gains of the policies πg and πt  respectively  in state φt from step k forward.
In practice  the ﬁrst step where the policies πt and πg choose a different item is unknown  because
πg is unknown. In this case  the regret can be written as:

[j](φt) = πg

k→(φt)

F t
[j](φt) for all j < k; and F g

k−1(φt)  φt)

] 

(13)

1i k t(φt)(F g

k→(φt)) 

(14)

Rt(φt) =

K(cid:88)
L(cid:88)
(cid:110)(cid:16)∀j < k : πt

k=1

i=1

k→(φt) − F t
(cid:17)

[k](φ) (cid:54)= πg

where:

  πt

[j](φ)

[j](φ) = πg

1i k t(φ) = 1

(15)
is the indicator of the event that the policies πt and πg choose the same ﬁrst k − 1 items in state φ 
disagree in the k-th item  and i is the k-th item chosen by πt. The commas in the indicator function
represent logical conjunction.
Second  in Lemma 1 we bound the expected loss associated with choosing the ﬁrst different item at
step k by the probability of this event and an upper bound on the expected loss Gk  which does not
depend on πt and φt. Based on this result  we bound the expected cumulative regret as:

[k](φ)  πt

[k](φ) = i

(cid:34) n(cid:88)

t=1

Eφ1 ... φn

Rt(φt)

(cid:125)

(cid:111)

1i k t(φt)(F g

k→(φt) − F t

k→(φt))

(cid:2)1i k t(φt)(F g

k→(φt) − F t

(cid:35)
k→(φt))(cid:3)(cid:3)

Eφ1 ... φt−1[Eφt[1i k t(φt)] Gk]

(cid:35)

1i k t(φt)

.

(16)

(cid:35)

L(cid:88)

K(cid:88)

i=1

k=1

Eφ1 ... φt−1

(cid:34) n(cid:88)
n(cid:88)
n(cid:88)

t=1

t=1

t=1

GkEφ1 ... φn

(cid:2)Eφt
(cid:34) n(cid:88)

= Eφ1 ... φn

=

i=1

L(cid:88)
≤ L(cid:88)
L(cid:88)

i=1

=

k=1

K(cid:88)
K(cid:88)
K(cid:88)

k=1

i=1

k=1

t=1

5

Finally  motivated by the analysis of UCB1 [1]  we rewrite the indicator 1i k t(φt) as:

1i k t(φt) = 1i k t(φt)1{Ti(t − 1) ≤ (cid:96)i k} + 1i k t(φt)1{Ti(t − 1) > (cid:96)i k}  

(17)
where (cid:96)i k is a problem-speciﬁc constant. In Lemma 4  we show how to choose (cid:96)i k such that arm i
at step k is pulled suboptimally a constant number of times in expectation after (cid:96)i k pulls. Based on
this result  the regret corresponding to the events 1{Ti(t − 1) > (cid:96)i k} is bounded as:

GkEφ1 ... φn

1i k t(φt)1{Ti(t − 1) > (cid:96)i k}

≤ 2
3

π2L(L + 1)

Gk.

(18)

On the other hand  the regret associated with the events 1{Ti(t − 1) ≤ (cid:96)i k} is trivially bounded by

i=1

k=1 Gk(cid:96)i k. A tighter upper bound is proved below:

(cid:34) n(cid:88)

t=1

L(cid:88)
K(cid:88)
(cid:80)K

k=1

i=1

(cid:80)L

(cid:35)

K(cid:88)

k=1

(cid:35)

(cid:35)

Eφ1 ... φn

1i k t(φt)1{Ti(t − 1) ≤ (cid:96)i k}

1i k t(φt)1{Ti(t − 1) ≤ (cid:96)i k}

i=1

L(cid:88)
≤ L(cid:88)
≤ L(cid:88)

i=1

max

φ1 ... φn

K(cid:88)

Gk

i=1

k=1

Gk

(cid:34) K(cid:88)
n(cid:88)
(cid:34) K(cid:88)
(cid:20)

Gk

k=1

k=1

t=1

n(cid:88)

t=1

(cid:96)i k − max
k(cid:48)<k

(cid:96)i k(cid:48)

(cid:21)+

.

(19)

The last inequality can be proved as follows. Our upper bound on the expected loss at step k  Gk  is
monotonically decreasing with k  and therefore G1 ≥ G2 ≥ . . . ≥ GK. So for any given arm i  the
highest cumulative regret subject to the constraint Ti(t − 1) ≤ (cid:96)i k at step k is achieved as follows.
The ﬁrst (cid:96)i 1 mistakes are made at the ﬁrst step  [(cid:96)i 2 − (cid:96)i 1]+ mistakes are made at the second step 
[(cid:96)i 3 − max{(cid:96)i 1  (cid:96)i 2}]+ mistakes are made at the third step  and so on. Speciﬁcally  the number of
mistakes at step k is [(cid:96)i k − maxk(cid:48)<k (cid:96)i k(cid:48)]+ and the associated loss is Gk.
Our main claim follows from combining the upper bounds in Equations 18 and 19.

3.4 Discussion of Theoretical Results

Algorithm OASM mimics the greedy policy πg. Therefore  we decided to prove Theorem 1 based on
counting how many times the policies πt and πg choose a different item. Our proof has three parts.
First  we associate the regret in episode t with the ﬁrst step where the policy πt chooses a different
item from πg. Second  we bound the expected regret in each episode by the probability of deviating
from the policy πg at step k and an upper bound on the associated loss Gk  which depends only on
k. Finally  we divide the expected cumulative regret into two terms  before and after item i at step k
is selected a sufﬁcient number of times (cid:96)i k  and then set (cid:96)i k such that both terms are O(log n). We
would like to stress that our proof is relatively general. Our modeling assumptions (Section 3.1) are
leveraged only in Lemma 4. In the rest of the proof  we only assume that f is adaptive submodular
and adaptive monotonic.
Our regret bound has several notable properties. First  it is logarithmic in the number of episodes n 
through problem-speciﬁc constants (cid:96)i k. So we recover a classical result from the bandit literature.
Second  the bound is polynomial in all constants of interest  such as the number of items L and the
number of maximization steps K in each episode. We would like to stress that it is not linear in the
number of contexts YK at step K  which is exponential in K. Finally  note that our bound captures
the shape of the optimized function f. In particular  because the function f is adaptive submodular 
the upper bound on the gain of the policy πg from step k forward  Gk  decreases as k increases. As
a result  earlier deviations from πg are penalized more than later ones.

4 Experiments

Our algorithm is evaluated on a preference elicitation problem in a movie recommendation domain.
This problem is cast as asking K yes-or-no movie-genre questions. The users and their preferences
are extracted from the MovieLens dataset [11]  a dataset of 6k users who rated one million movies.

6

Genre
Crime
Children’s
Animation
Horror
Sci-Fi
Musical
Fantasy
Adventure

gi(0)
4.1% 13.0%
4.1% 9.2%
3.2% 6.6%
3.0% 8.0%
2.8% 23.0%
2.6% 6.0%
2.6% 5.8%
2.3% 19.6%

¯gi(0) P (φ[i] = 1)
0.32
0.44
0.48
0.38
0.12
0.44
0.44
0.12

Figure 1: Left. Eight movie genres that cover the largest number of movies in expectation. Right.
Comparison of three greedy policies for solving our preference elicitation problem. For each policy
and K ≤ L  we report the expected percentage of covered movies after K questions.

Figure 2: The expected return of the OASM policy πt (cyan lines) in all episodes up to t = 105. The
return is compared to those of the greedy policies πg (blue lines)  πg
d (gray lines)
in the ofﬂine setting (Figure 1) at the same operating point  the number of asked questions K.

f (red lines)  and πg

(cid:17)

(cid:16) nu

We choose 500 most rated movies from the dataset. Each movie l is represented by a feature vector
xl such that xl[i] = 1 if the movie belongs to genre i and xl[i] = 0 if it does not. The preference of
user j for genre i is measured by tf-idf  a popular importance score in information retrieval [12]. In
particular  it is deﬁned as tf-idf(j  i) = #(j  i) log
  where #(j  i) is the number of movies
from genre i rated by user j  nu is the number of users  and #(·  i) is the number of users that rated
at least one movie from genre i. Intuitively  this score prefers genres that are often rated by the user
but rarely rated overall. Each user j is represented by a genre preference vector φ such that φ[i] = 1
when genre i is among ﬁve most favorite genres of the user. These genres cover on average 25% of
our movies. In Figure 1  we show several popular genres from our dataset.
The reward for asking user φ questions A is:

#(· i)

(cid:80)500
l=1 maxi [xl[i]1{φ[i] = 1} 1{i ∈ A}]  

f (A  φ) = 1
5

(20)
the percentage of movies that belong to at least one genre i that is preferred by the user and queried
in A. The function f captures the notion that knowing more preferred genres is better than knowing
less. It is submodular in A for any given preference vector φ  and therefore adaptive submodular in
A when the preferences are distributed independently of each other (Equation 6). In this setting  the
expected value of f can be maximized near optimally by a greedy policy (Equation 4).
In the ﬁrst experiment  we show that our assumption on P (Φ) (Equation 6) is not very restrictive in
our domain. We compare three greedy policies for maximizing f that know P (Φ) and differ in how
the expected gain of choosing items is estimated. The ﬁrst policy πg makes no assumption on P (Φ)
and computes the gain according to Equation 5. The second policy πg
f assumes that the distribution
P (Φ) is factored and computes the gain using Equation 7. Finally  the third policy πg
d computes the
gain according to Equation 8  essentially ignoring the stochasticity of our problem. All policies are
applied to all users in our dataset for all K ≤ L and their expected returns are reported in Figure 1.
We observe two trends. First  the policy πg
d by a large margin. So
although our independence assumption may be incorrect  it is a better approximation than ignoring

f usually outperforms the policy πg

7

246810121416180102030Number of questions KCovered movies [%]  πgDeterministic πdgFactored πfg1011021031041050246810K = 2Episode tCovered movies [%]101102103104105051015K = 4Episode t101102103104105510152025K = 8Episode tf is a good approximation to πg.

the stochastic nature of the problem. Second  the expected return of πg
We conclude that πg
In the second experiment  we study how the OASM policy πt improves over time. In each episode t 
we randomly choose a new user φt and then the policy πt asks K questions. The expected return of
πt is compared to two ofﬂine baselines  πg
d can be viewed as upper
and lower bounds on the expected return of πt  respectively. Our results are shown in Figure 2. We
observe two major trends. First  πt easily outperforms the baseline πg
d that ignores the stochasticity
of our problem. In two cases  this happens in less than ten episodes. Second  the expected return of
πt approaches that of πg

f is always within 84% of πg.

f   as is expected based on our analysis.

f and πg

d. The policies πg

f and πg

5 Related Work

Our paper is motivated by prior work in the areas of submodularity [14  5] and bandits [1]. Similar
problems to ours were studied by several authors. For instance  Yue and Guestrin [17]  and Guillory
and Bilmes [6]  applied bandits to submodular problems in a non-adaptive setting. In our work  we
focus on the adaptive setting. This setting is more challenging because we learn a K-step policy for
choosing items  as opposing to a single set of items. Wen et al. [16] studied a variant of generalized
binary search  sequential Bayesian search  where the policy for asking questions is learned on-the-
ﬂy by interacting with the environment. A major observation of Wen et al. [16] is that this problem
can be solved near optimally without exploring. As a result  its solution and analysis are completely
different from those in our paper.
Learning with trees was studied in machine learning in many settings  such as online learning with
tree experts [3]. This work is similar to ours only in trying to learn a tree. The notions of regret and
the assumptions on solved problems are completely different. Optimism in the face of uncertainty is
a popular approach to designing learning algorithms  and it was previously applied to more general
problems than ours  such as planning [13] and MDPs [8]. Both of these solutions are impractical in
our setting. The former assumes that the model of the world is known and the latter is computation-
ally intractable.

6 Conclusions

This is the ﬁrst work that studies adaptive submodular maximization in the setting where the model
of the world is initially unknown. We propose an efﬁcient bandit algorithm for solving the problem
and prove that its expected cumulative regret increases logarithmically with time. Our work can be
viewed as reinforcement learning (RL) [15] for adaptive submodularity. The main difference in our
setting is that we can learn near-optimal policies without estimating the value function. Learning of
value functions is typically hard  even when the model of the problem is known. Fortunately  this is
not necessary in our problem and therefore we can develop a very efﬁcient learning algorithm.
We assume that the states of items are distributed independently of each other. In our experiments 
this assumption was less restrictive than we expected (Section 4). Nevertheless  we believe that our
approach should be studied under less restrictive assumptions. In preference elicitation (Section 4) 
for instance  the answers to questions are likely to be correlated due to many factors  such as user’s
preferences  user’s mood  and the similarity of the questions. Our current model cannot capture any
of these dependencies. However  we believe that our approach is quite general and can be extended
to more complex models. We think that any such generalization would comprise three major steps:
choosing a model of P (Φ)  deriving a corresponding upper conﬁdence bound on the expected gain 
and ﬁnally proving an equivalent of Lemma 4.
We also assume that the expected gain of choosing an item (Equation 7) can be written as a product
of some known gain function (Equation 8) and the probability of the item’s states. This assumption
is quite natural in maximum coverage problems but may not be appropriate in other problems  such
as generalized binary search [4].
Our upper bound on the expected regret at step k (Lemma 1) may be loose in practice because it is
obtained by maximizing over all contexts y ∈ Yk. In general  it is difﬁcult to prove a tighter bound.
Such a bound would have to depend on the probability of making a mistake in a speciﬁc context at
step k  which depends on the policy in that episode  and indirectly on the progress of learning in all
earlier episodes. We leave this for future work.

8

References
[1] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine Learning  47:235–256  2002.

[2] Sandilya Bhamidipati  Branislav Kveton  and S. Muthukrishnan. Minimal interaction search:
Multi-way search with item categories. In Proceedings of AAAI Workshop on Intelligent Tech-
niques for Web Personalization and Recommendation  2013.

[3] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction  Learning  and Games. Cambridge Uni-

versity Press  New York  NY  2006.

[4] Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in Neural Infor-

mation Processing Systems 17  pages 337–344  2005.

[5] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in ac-
tive learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research  42:427–
486  2011.

[6] Andrew Guillory and Jeff Bilmes. Online submodular set cover  ranking  and repeated active
learning. In Advances in Neural Information Processing Systems 24  pages 1107–1115  2011.
[7] Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise.
In Proceedings of the 28th International Conference on Machine Learning  pages 369–376 
2011.

[8] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[9] David Kempe  Jon Kleinberg  and ´Eva Tardos. Maximizing the spread of inﬂuence through a
social network. In Proceedings of the 9th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining  pages 137–146  2003.

[10] Andreas Krause  Ajit Paul Singh  and Carlos Guestrin. Near-optimal sensor placements in
Gaussian processes: Theory  efﬁcient algorithms and empirical studies. Journal of Machine
Learning Research  9:235–284  2008.

[11] Shyong Lam and Jon Herlocker. MovieLens 1M Dataset. http://www.grouplens.org/node/12 

2012.

[12] Christopher Manning  Prabhakar Raghavan  and Hinrich Sch¨utze. Introduction to Information

Retrieval. Cambridge University Press  New York  NY  2008.

[13] R´emi Munos. The optimistic principle applied to games  optimization  and planning: Towards
foundations of Monte-Carlo tree search. Foundations and Trends in Machine Learning  2012.
[14] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. An analysis of approximations for maxi-

mizing submodular set functions - I. Mathematical Programming  14(1):265–294  1978.

[15] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press 

Cambridge  MA  1998.

[16] Zheng Wen  Branislav Kveton  Brian Eriksson  and Sandilya Bhamidipati. Sequential Bayesian
search. In Proceedings of the 30th International Conference on Machine Learning  pages 977–
983  2013.

[17] Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversiﬁed
retrieval. In Advances in Neural Information Processing Systems 24  pages 2483–2491  2011.

9

,Victor Gabillon
Branislav Kveton
Zheng Wen
Brian Eriksson
S. Muthukrishnan
Kevin Moon
Alfred Hero
Nikhil Rao
Hsiang-Fu Yu
Pradeep Ravikumar
Inderjit Dhillon