2019,Scalable Deep Generative Relational Model with High-Order Node Dependence,In this work  we propose a probabilistic framework for relational data modelling and latent structure exploring. Given the possible feature information for the nodes in a network  our model builds up a deep architecture that can approximate to the possible nonlinear mappings between the nodes' feature information and latent representations. For each node  we incorporate all its neighborhoods' high-order structure information to generate latent representation  such that these latent representations are ``smooth'' in terms of the network. Since the latent representations are generated from Dirichlet distributions  we further develop a data augmentation trick to enable efficient Gibbs sampling for Ber-Poisson likelihood with Dirichlet random variables. Our model can be ready to apply to large sparse network as its computations cost scales to the number of positive links in the networks. The superior performance of our model is demonstrated through improved link prediction performance on a range of real-world datasets.,Scalable Deep Generative Relational Models

with High-Order Node Dependence

Xuhui Fan1  Bin Li2  Scott A. Sisson1  Caoyuan Li3  and Ling Chen3

1School of Mathematics & Statistics  University of New South Wales  Sydney
2Shanghai Key Lab of IIP & School of Computer Science  Fudan University

3Faculty of Engineering and IT  University of Technology  Sydney

{xuhui.fan  scott.sisson}@unsw.edu.au; libin@fudan.edu.cn

Abstract

We propose a probabilistic framework for modelling and exploring the latent struc-
ture of relational data. Given feature information for the nodes in a network  the
scalable deep generative relational model (SDREM) builds a deep network archi-
tecture that can approximate potential nonlinear mappings between nodes’ feature
information and the nodes’ latent representations. Our contribution is two-fold:
(1) We incorporate high-order neighbourhood structure information to generate
the latent representations at each node  which vary smoothly over the network.
(2) Due to the Dirichlet random variable structure of the latent representations 
we introduce a novel data augmentation trick which permits efﬁcient Gibbs sam-
pling. The SDREM can be used for large sparse networks as its computational
cost scales with the number of positive links. We demonstrate its competitive per-
formance through improved link prediction performance on a range of real-world
datasets.

1 Introduction

Bayesian relational models  which describe the pairwise interactions between nodes in a net-
work  have gained tremendous attention in recent years  with numerous methods developed to
model the complex dependencies within relational data; in particular  probabilistic Bayesian meth-
ods [27  18  1  25  7  6]. Such models have been applied to community detection [27  17]  collab-
orative ﬁltering [29  23]  knowledge graph completion [14] and protein-to-protein interactions [16].
In general  the goal of these Bayesian relational models is to discover the complex latent structure
underlying the relational data and predict the unknown pairwise links [9  8].
Despite improving the understanding of complex networks  existing models typically have one or
more weaknesses: (1) While data commonly exhibit high-order node dependencies within the net-
work  such dependencies are rarely modelled due to limited model capabilities; (2) Although a
node’s feature information closely informs its latent representation  existing models are not sufﬁ-
ciently ﬂexible to describe these (potentially nonlinear) mappings well; (3) While some scalable
network modelling techniques (e.g. Ber-Poisson link functions [30  36]) can help to reduce the
computational complexity to the number of positive links  they require the elements of latent rep-
resentations to be independently generated and cannot be used for modelling dependent variables
(e.g. membership distributions on communities).
In order to address these challenges  we develop a probabilistic framework using a deep network ar-
chitecture on the nodes to model the relational data. The proposed scalable deep generative relational
model (SDREM) builds a deep network architecture to efﬁciently map the nodes’ feature informa-
tion to their latent representations. In particular  the latent representations are modelled via Dirichlet

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

distributions  which permits their interpretation as membership distributions on communities. Based
on the output latent representations (i.e. membership distributions) and an introduced community
compatibility matrix  the relational data is modelled through the Ber-Poisson link function [30  36] 
for which the computational cost scales with the number of positive links in the network.
We make two novel contributions: First  as the nodes’ latent representations are Dirichlet random
variables  we incorporate the full neighbourhood’s structure information into its concentration pa-
rameters. In this way  high-order node dependence can be modelled well and can vary smoothly
over the network. Second  we introduce a new data augmentation trick that enables efﬁcient Gibbs
sampling on the Ber-Poisson link function due to the Dirichlet random variable structure of the latent
representations. The SDREM can be used to analyse large sparse networks and may also be directly
applied to other notable models to improve their scalability (e.g. the mixed-membership stochastic
blockmodel (MMSB) [1] and its variants [22  13  19]).
In comparison to existing approaches  the SDREM has several advantages. (1) Modelling high-order
node dependence: Propagating information between nodes’ connected neighbourhoods can improve
information sharing and dependence modelling between nodes. Also  it can largely reduce computa-
tional costs in contrast to considering all the pairwise nodes’ dependence  as well as avoid spurious
or redundant information complications from unrelated nodes. Moreover  the non-linear real-value
propagation in the deep network architecture can help to approximate the complex nonlinear map-
ping between the node’s feature information and its latent representations. (2) Scalable modelling
on relational data: Our novel data augmentation trick permits an efﬁcient Gibbs sampling imple-
mentation  with computational costs scaling with the number of positive network links only. (3)
Meaningful layer-wise latent representation: Since the nodes’ latent representations are generated
from Dirichlet distributions  they are naturally interpretable as the nodes’ memberships over latent
communities.
In our analyses on a range of real-world relational datasets  we demonstrate that the SDREM can
achieve superior performance compared to traditional Bayesian methods for relational data  and
perform competitively with other approaches. As the SDREM is the ﬁrst Bayesian relational model
to use neighbourhood-wise propagation to build the deep network architecture  we note that it may
straightforwardly integrate other Bayesian methods for modelling high-order node dependencies in
relational data  and further improve relationship predictability.

2 Scalable Deep Generative Relational Models (SDREMs)
The relational data in the SDREM is represented as a binary matrix RRR 2 f0; 1gN(cid:2)N   where N is
the number of nodes and the element Rij (8i; j) indicates whether node i relates to node j (Rij = 1
if the relation exists  otherwise Rij = 0)  with the self-connection relation Rii not considered here.
The matrix RRR can be symmetric (i.e. undirected) or asymmetric (i.e. directed). The network’s feature
information is denoted by a non-negative matrix FFF 2 fR+ [ 0gN(cid:2)D  where D denotes the number
of features  and where each element Fid (8i; d) takes the value of the d-th feature for the i-th node.
The deep network architecture of the SDREM is controlled by two parameters: L  representing the
number of layers  and K  denoting the length of the nodes’ latent representation in each layer. The
latent representation (cid:25)(cid:25)(cid:25)(l)
i of node i in the l-th layer is a Dirichlet random variable (i.e. a normalised
vector with (K(cid:0)1) active elements). In this way  (cid:25)(cid:25)(cid:25)(l)
  which we term the “membership distribution” 
is interpretable as node i’s community distribution  where K communities are modelled and (cid:25)(l)
ik
denotes node i’s interaction with the k-th community in the l-th layer.
The deep network architecture of the SDREM is composed of three parts: (1) The input layer feeding
the feature information; (2) The hidden layers modelling high-order node dependences; (3) The
output layer of the relational data model. These component parts are detailed below.

i

2.1 Feeding the feature information

When nodes’ feature information is available  we introduce a feature-to-community transition coef-
ﬁcient matrix TTT 2 (R+)D(cid:2)K  where Tdk indicates the activity of the d-th feature in contributing to
the k-th latent community. The linear sum of the transition coefﬁcients TTT and feature FFF forms the

2

Figure 1: Illustration and visualization of a SDREM on a 5-node (i.e. A; B; C; D; E) directed network. Left:
the graphical model of a 3-layer SDREM modelling RBA; RED. Shaded nodes (i.e. F(cid:1); R(cid:1)) denote variables
with known values  unshaded nodes denote latent variables. Right top: the generative process of a SDREM.
Right bottom: the directed connection types of all 5 nodes.

prior for the nodes’ ﬁrst layer membership distribution
) 8d; k; (cid:25)(cid:25)(cid:25)(1)

Tdk (cid:24) Gam((cid:13)(1)
d ;

i

1
c(1)

(1)
where Gam((cid:13); 1=c) denotes a gamma random variable with mean (cid:13)=c and variance (cid:13)=c2; f(cid:13)(1)
gd
and c(1) are the hyper-parameters for generating fTdkgd;k. From Eq. (1)  nodes with close feature
information have similar prior knowledge and similar generated membership distributions. A sup-
plementary contribution (cid:11) is included in case that a node has no feature information available. For
(cid:24) Dirichlet((cid:11)(cid:1) 1111(cid:2)K)  which is a common setting
node i without feature information  we have (cid:25)(cid:25)(cid:25)(1)
in Bayesian relational data modelling.

d

i

(cid:24) Dirichlet(FFF iTTT + (cid:11)) 8i:

2.2 Modelling high-order node dependence

High-order node dependence is modelled within the deep network architecture of the SDREM. In
general  node i’s membership distribution (cid:25)(cid:25)(cid:25)(l)
is conditioned on the membership distributions at the
(l (cid:0) 1)-th layer via an information propagation matrix BBB(l(cid:0)1) 2 fR+ [ 0gN(cid:2)N :
i

8<: (cid:24) Gam((cid:13)(l)

(cid:24) Gam((cid:13)(l)
= 0

B(l(cid:0)1)
i′i

1 ; 1
0 ; 1

c(l) )
c(l) )

if Ri′i = 1;
′
if i
otherwise;

= i;

(cid:24) Dirichlet((BBB(l(cid:0)1)

(cid:1)i

⊤ (cid:1) (cid:25)(cid:25)(cid:25)(l(cid:0)1)
1:N );
)

(cid:25)(cid:25)(cid:25)(l)
i

(2)

i′i

1 ; (cid:13)(l)

0

). B(l(cid:0)1)

(cid:24) Gam(e(l)

denotes node i
i′i will make (cid:25)(cid:25)(cid:25)(l)

Following [35]  we set the hyper-parameter distribution as (cid:13)(l)

); c(l) (cid:24)
′’s inﬂuence on node i from the (l (cid:0) 1)-th to the l-th layer
Gam(g0; 1
h0
2 fR+gN(cid:2)K denotes
(e.g. larger values of B(l(cid:0)1)
i more similar to (cid:25)(cid:25)(cid:25)(l(cid:0)1)
the matrix of N nodes’ membership distributions at the l-th layer. When there is no direct con-
′ ̸= i \ Ri′i = 0)  we restrict the corresponding information
nection from node i
propagation coefﬁcients Bi′i at all layers to be 0; otherwise  we generate B(l(cid:0)1)
either from a node
and layer-speciﬁed Gamma distribution (when Ri′i = 1) or a layer-speciﬁed Gamma distribution
′
(when i
= i). This can produce various beneﬁts. On one hand  it promotes the sparseness of BBB(l)
and reduces the cost of calculating BBB(l) from O(N 2) to the scale of the number of positive network
links. On the other hand  since the SDREM uses a Dirichlet distribution (parameterised by the linear

′ to node i (i.e. i

) and (cid:25)(cid:25)(cid:25)(l)
1:N

0 ; 1
f (l)
0

i′i

i′

3

π(1)Dπ(3)Dπ(2)Cπ(3)Eπ(3)CREDπ(1)Eπ(3)Aπ(2)DMΛπ(2)BFAZEDπ(1)BXEFEB(1)π(2)AFCRBAT αZBAB(2)π(2)EXBFBFDXDXAπ(1)Cπ(1)Aπ(3)BHere thepriordistributionforgeneratingXikandthelikelihoodbasedonXikarebothPoissondistributions.Consequently wemayimplementposteriorsamplingbyusingTouchardpolynomi-als[25](detailsinSection3).Tomodelbinaryorcountdata theBer-Poissonlikelihood[24 30]decomposesthelatentcountingvectorXXXiintothelatentintegermatrixZZZij.Anappealingpropertyofthisconstructionisthatwedonotneedtocalculatethelatentintegersfzij;k1k2gk1;k2overthe0-valuedRijdataastheyareequalto0almostsurely.Hence thefocuscanbeonthepositive-valuedrelationaldata.Thisisparticularlyusefulforreal-worldnetworkdataasusuallyonlyasmallfractionofthedataispositive.Hence thecomputationalcostforinferencescalesonlywiththenumberofpositiverelationallinks.Whennodes’featureinformationisnotavailable(i.e.FFF=0N(cid:2)D)andL=1 theSDREMsimpli-ﬁestothesamesettingsastheMMSB[1].Inparticular themembershipdistributionsofboththeMMSBandtheSDREMfollowthesameDirichletdistributionf(cid:25)(cid:25)(cid:25)igi(cid:24)Dirichlet((cid:11)1(cid:2)K).AstheMMSBanditsvariants[18 10 16]introducepairwiselatentlabelsforalltherelationaldata(both1and0-valueddata) itrequiresacomputationalcostofO(N2)toinferalllatentvariables.Incon-trast ournoveldataaugmentationtrickcanbestraightforwardlyappliedinthesemodels(bysimplyreplacingtheBer-Betalikelihood[21 15]withBer-Poissonlikelihood)andreducetheircomputa-tionalcosttothescaleofthenumberofpositivelinks.WeshowinSection5thatwecanalsogetbetterpredictiveperformancewiththisstrategy.2.4ModelsummaryThefullgenerativeprocessofSDREMissummarizedas(seevisualizationinFigure1):Throughintroducingsomeauxiliaryvariables allthselatentvariablescanbeinferredthroughclosed-formGibbssampling.ThissectionmainlyThroughintroducingsomeauxiliaryvariables allthselatent(1)Tdk(cid:24)Gam((cid:13)(1)d;1c(1));(cid:25)(cid:25)(cid:25)(1)i(cid:24)Dirichlet(FFFiTTT+(cid:11));(2)Forl=2;:::;L(cid:15)B(l(cid:0)1)i′i8<:(cid:24)Gam((cid:13)(l)1;1c(l));i′:Ri′i=1;(cid:24)Gam((cid:13)(l)0;1c(l));i′:i′=i;=0;otherwise;;(cid:15)(cid:25)(cid:25)(cid:25)(l)i(cid:24)Dirichlet((B(l(cid:0)1)(cid:1)i)⊤(cid:1)(cid:25)(cid:25)(cid:25)(l(cid:0)1)1:N).(3)Mi(cid:24)Poisson(M);(Xi1;:::;XiK)(cid:24)Multi(Mi;(cid:25)(L)i1;:::;(cid:25)(L)iK);(4)(cid:3)k1k2(cid:24)Gam(k(cid:3);1(cid:18)(cid:3));(5)Zij;k1k2(cid:24)Poisson(Xik1(cid:3)k1k2Xjk2);(6)Rij=111(∑k1;k2Zij;k1k2>0).Throughintroducingsomeauxiliaryvariables allthselatentvariablescanbeinferredthroughclosed-formGibbssampling.ThissectionmainlyfocusesontheinferenceoffXikgi;k thekeyvariablesofgeneratingthelatentintegers.ThesamplingonothervariableseitherfollowssimilarroutinesofTopicmodels-focusedmethodsGBN[31]andDirBN[29]orrequiretrivialefforts.Weprovidethefullsamplingschemeinthesupplementarymaterial.wheref(cid:13)(1)fgf;fc(l)gl;f(cid:13)(l)igi;l;fk(cid:3);(cid:18)(cid:3)g;(cid:11);Marethehyper-parametersofthemodel.Throughintroducingsomeauxiliaryvariables allthselatentvariablescanbeinferredthroughclosed-formGibbssampling.ThissectionmainlyfocusesontheinferenceoffXikgi;k thekeyvariablesofgeneratingthelatentintegers.ThesamplingonothervariableseitherfollowssimilarroutinesofTopicmodels-focusedmethodsGBN[31]andDirBN[29]orrequiretrivialefforts.Weprovidethefullsamplingschemeinthesupplementarymaterial.Throughintroducingsomeauxiliaryvariables allthselatentvariablescanbeinferredthroughclosed-formGibbssampling.ThissectionmainlyfocusesontheinferenceoffXikgi;k thekeyvariablesofgeneratingthelatentintegers.ThesamplingonothervariableseitherfollowssimilarroutinesofTopicmodels-focusedmethodsGBN[31]andDirBN[29]orrequiretrivialefforts.Weprovidethefullsamplingschemeinthesupplementarymaterial.5BACEDi

i

′

(cid:0)1(BBB(l
))

′

))

⊤

](cid:25)(cid:25)(cid:25)(1)

∑

1:N ] = (cid:25)(cid:25)(cid:25)(1)

∏
is conditioned on (cid:25)(cid:25)(cid:25)(l(cid:0)1)

1:N . In the SDREM  we have E[(cid:25)(cid:25)(cid:25)(l)
i′ B(l)

sum of node i’s neighbourhoods’ membership distributions at the (l (cid:0) 1)-th layer) to generate (cid:25)(cid:25)(cid:25)(l)
 
all the nodes’ membership distributions are expected to vary smoothly over the connected graph
structure. That is  connected nodes are expected to have more similar membership distributions than
unconnected ones.
Flexibility in modelling variance and covariance in membership distributions Neighbourhood-
wise information propagation allows for more ﬂexible modelling than the extreme case of inde-
only (i.e. fBBB(l)gl is a diagonal matrix).
pendent propagation whereby (cid:25)(cid:25)(cid:25)(l)
i
Under independent propagation  the expected membership distribution at each layer does not change:
l(cid:0)1
E[(cid:25)(cid:25)(cid:25)(l)
1:N   where D(l)
l′=1(D(l
1:N ] = [
i′i   8i. Based on different choices for fBBB(l)gl  the ex-
is a level l diagonal matrix with D(l)
pected mean of each node’s membership distribution can incorporate information from other nodes’
input layer. In terms of variance and covariance within each (cid:25)(cid:25)(cid:25)(l)
  independent propagation is re-
i
stricted to inducing a larger variance in (cid:25)(l)
due to
the layer stacking architecture (this can be easily veriﬁed through the law of total variance and the
law of total covariance). In contrast  for the SDREM  these variances and covariances can be made
either large or small depending on the choices of fBBB(l)gl through the deep network architecture.
The Dirichlet distribution models the membership distribution f(cid:25)(cid:25)(cid:25)(l)
gi;l in a non-linear way. As
non-linearities are easily captured via deep learning  it is expected that the deep network architec-
ture in the SDREM can approximate the complex nonlinear mapping between the nodes’ feature
information and membership distributions sufﬁciently well. Further  the technique of propagating
real-valued distributions through different layers might be a promising alternative to sigmoid belief
networks [10  11  15]  which mainly propagate binary variables between different layers.

ik and smaller covariance between (cid:25)(l)
ik1

and (cid:25)(l)
ik2

ii =

i

Comparison with spatial graph convolutional networks: Propagating information through
neighbourhoods works in a similar spirit to the spatial graph convolutional network (GCN) [2  5 
12  3] in a frequentist setting. In addition to providing variability estimates for all latent variables
and predictions  the SDREM may conveniently incorporate beliefs on the parameters and exploit
the rich structure within the data. Beyond the likelihood function  the SDREM uses a Dirichlet
distribution as the activation function  whereas GCN algorithms usually use the logistic function.
The resulting membership distribution representation of the SDREM may provide a more intuitive
interpretation than the node representation (node embedding) in the GCN.

2.3 Scalable relational data modelling

(cid:0)∑

k1 k2

We model the ﬁnal-layer relational data via the Ber-Poisson link function [30  36]  Rij (cid:24)
Bernoulli(1 (cid:0) e
Xik1 (cid:3)k1 k2 Xjk2 )  where Xik is the latent count of node i on community k
2 R+ is a compatibility value between communities k1 and k2. In existing work with the
and (cid:3)k1k2
Ber-Poisson link function  all of the fXikgi;k terms are required to be independently generated (ei-
ther from a Gamma [36  34] or Bernoulli distribution [15]) to allow for efﬁcient Gibbs sampling.
However  in the SDREM  the elements of the output latent representation ((cid:25)i1; : : : ; (cid:25)iK) are jointly
generated from a Dirichlet distribution. These normalised elements are dependent on each other and
it is not easy to enable Gibbs sampling for each individual element f(cid:25)ikgk.
To address this problem  we use a decomposition strategy to isolate the elements f(cid:25)ikgk. We use
multinomial distributions  with f(cid:25)(cid:25)(cid:25)igi as event probabilities  to generate K-length counting vectors
fXXX igi. Each XXX i can be regarded as an estimator of (cid:25)(cid:25)(cid:25)i. Since the sum of the fXikgk is ﬁxed as
the number of trials (denoted as Mi) in the multinomial distribution  we further let Mi be gener-
ated as Mi (cid:24) Poisson(M ). Based on the Poisson-Multinomial equivalence [4]  each Xik is then
equivalently distributed Xik (cid:24) Poisson(M (cid:25)ik).
Following the settings of Ber-Poisson link function  a latent integer matrix ZZZ ij 2 NK(cid:2)K is intro-
(cid:24) Poisson(Xik1 (cid:3)k1k2Xjk2 ). Rij is then generated by
duced  where the (k1; k2)-th entry is Zij;k1k2

4

evaluating the degree of positivity of the matrix Zij. That is  8(i; j); k1; k2:
Mi (cid:24) Poisson(M );

∑
(Xi1; : : : ; XiK) (cid:24) Multi(Mi; (cid:25)(L)

i1 ; : : : ; (cid:25)(L)

iK ); (cid:3)k1k2

Zij;k1k2

(cid:24) Poisson(Xik1(cid:3)k1k2 Xjk2)

and Rij = 111(

Zij;k1k2 > 0):

k1;k2

(cid:24) Gam(k(cid:3);

1
(cid:18)(cid:3)

);

(3)

Here  the prior distribution for generating Xik and the likelihood based on Xik are both Poisson
distributions. Consequently  we may implement posterior sampling by using Touchard polynomi-
als [31] (details in Section 3).
To model binary or count data  the Ber-Poisson link function [30  36] decomposes the latent counting
vector XXX i into the latent integer matrix ZZZ ij. An appealing property of this construction is that we do
not need to calculate the latent integers fzij;k1k2
gk1;k2 over the 0-valued Rij data as they are equal
to 0 almost surely. Hence  the focus can be on the positive-valued relational data. This is particularly
useful for real-world network data as usually only a small fraction of the data is positive. Hence  the
computational cost for inference scales only with the number of positive relational links.
When nodes’ feature information is not available (i.e. FFF = 0N(cid:2)D) and L = 1  the SDREM reduces
to the same settings as the MMSB [1]. In particular  the membership distributions of both the MMSB
and the SDREM follow the same Dirichlet distribution f(cid:25)(cid:25)(cid:25)igi (cid:24) Dirichlet((cid:11)1(cid:2)K). As the MMSB
and its variants [22  13  19] introduce pairwise latent labels for all the relational data (both 1 and 0-
valued data)  it requires a computational cost of O(N 2) to infer all latent variables. In contrast  our
novel data augmentation trick can be straightforwardly applied in these models (by simply replacing
the Ber-Beta likelihood [27  18] with Ber-Poisson link function) and reduce their computational
cost to the scale of the number of positive links. We show in Section 5 that we can also get better
predictive performance with this strategy.

3 Inference

The joint distribution of the relational data and all latent variables in the SDREM is:
gi;j;k1;k2 ;fRijgi;j;fXikgi;k; TTTjFFF ; (cid:13)(cid:13)(cid:13); ccc; (cid:11); M; k(cid:3); (cid:18)(cid:3))

]

i

[
n∏
P (f(cid:25)(cid:25)(cid:25)(l)
24∏

i=1

(cid:2)

]
j(cid:11); FFF i; TTT )

[
L(cid:0)1∏
gi;l;fBBB(l)gl; (cid:3)(cid:3)(cid:3);fZij;k1k2
3524 ∏
P (BBB(l)j(cid:13)(l)

P (Xikj(cid:25)(L)

ik ; M )

l=1

i

i

P ((cid:25)(cid:25)(cid:25)(1)

=

i;k

(i;j)jRij =1;k1;k2

n∏

i=1

P (Zij;k1k2

3524∏

f;k

; c(l))

P ((cid:25)(cid:25)(cid:25)(l+1)

i

jf(cid:25)(cid:25)(cid:25)(l)

i′ gi′:Ri′ i=1; (cid:25)(cid:25)(cid:25)(l)

i

; BBB(l))

P ((cid:3)(cid:3)(cid:3)jk(cid:3); (cid:18)(cid:3))

jXik1 ; Xjk2 ; (cid:3)k1k2 )

P (Tdkj(cid:13)(1)

f ; c(1))

35 :

i

(4)
By introducing auxiliary variables  all latent variables can be sampled via efﬁcient Gibbs sampling.
This section focuses on inference for fXikgi;k  which is the key variable involving the data augmen-
tation trick. Sampling the membership distributions f(cid:25)(cid:25)(cid:25)(l)
gi;l is as implemented in Gamma Belief
Networks [37] and Dirichlet Belief Networks [35]  which mainly use a bottom-up mechanism to
propagate the latent count information in each layer. As sampling the other variables is trivial  we
relegate the full sampling scheme to the Supplementary Material (Appendix A).
Sampling fXikgi;k: From the Poisson-Multinomial equivalence [4] we have Mi (cid:24) Poisson(M ) 

(Xi1; : : : ; XiK) (cid:24) Multi(Mi; (cid:25)(L)

iK ) d= Xik (cid:24) Poisson(M (cid:25)(L)
Both the prior distribution for generating Xik and the likelihood parametrised by Xik are Poisson
distributions. The full conditional distribution of Xik (assuming zii;(cid:1)(cid:1) = 0;8i) is then
∑
P (XikjM; (cid:25)(cid:25)(cid:25); (cid:3)(cid:3)(cid:3); ZZZ) /

i1 ; : : : ; (cid:25)(L)

ik );8k:

M (cid:25)(L)
ik e

(cid:0)∑

Zj2 i;k1k :

Zij1 ;kk2

j̸=i;k2

+(cid:3)k2k)

(Xik)

((cid:3)kk2

∑

]

[

Xjk2

j1;k2

j2;k1

Xik

+

This follows the form of Touchard polynomials [31]  where 1 =

∑
available by comparing a Uniform(0; 1) random variable to the cumulative sum of f

(5)
xkkn
k! with Tn(x) =
g is the Stirling number of the second kind. A draw from (5) is then
gk.

gxk and where fn
k

(cid:1) xkkn

fn
k

exTn(x)

n
k=0

k=0

1

1

∑1

Xik!

exTn(x)

k!

5

4 Related Work

There is a long history of using Bayesian methods for relational data. Usually  these models build
latent representations for the nodes and use the interactions between these representations to model
the relational data. Typical examples include the stochastic blockmodel [27  26  18] (which uses
latent labels)  the mixed-membership stochastic blockmodel (MMSB) [1  22] (which uses mem-
bership distributions) and the latent feature relational model (LFRM) [25  28] (which uses binary
latent features). As most of these approaches are constructed using shallow models  their modelling
capability is limited.
The Multiscale-MMSB [13] is a related model  which uses a nested-Chinese Restaurant Process
to construct hierarchical community structures. However  its tree-type structure is quite compli-
cated and hard to implement efﬁciently. The Nonparametric Metadata Dependent Relational model
(NMDR) [19] and the Node Attribute Relational Model (NARM) [34] also use the idea of transform-
ing nodes’ feature information to nodes’ latent representations. However  because of their shallow
latent representation  these methods are unable to describe higher-order node dependencies.
The hierarchical latent feature model (HLFM) [15] may be the closest model to the SDREM  as
they each build up deep network architecture to model relational data. However  the HLFM uses
a sigmoid belief network  and does not consider high-order node dependencies  so that each node
only depends on itself through layers. Finally  feature information enters in the last layer of the
deep network architecture  and so the HLFM is unable to sufﬁciently describe nonlinear mappings
between the feature information and the latent representation.
Recent developments [10  11] in Poisson matrix factorisation also try to build deep network archi-
tecture for latent structure modelling. Since these mainly use sigmoid belief networks  the way of
propagating binary variables is different from our real-valued distributions propagation. Informa-
tion propagation through Dirichlet distributions in the SDREM follows the approaches of [37][35].
However  their focus is on topic modelling and no neighbourhood-wise propagation is discussed in
these methods.
Our SDREM shares similar spirit of the Variational Graph Auto-Encoder (VGAE) [21  24] algo-
rithms. Both of the algorithms aim at combining the graph convolutional networks with Bayesian
relational methods. However  VGAE has a larger computational complexity (O(N 2)). It uses param-
eterized functions to construct the deep network architecture and the probabilistic nature occurs in
the output layer as Gaussian random variables only. In contrast  SDREM constructs multi-stochastic-
layer architectures (with Dirichlet random variables at each layer). Thus  SDREM would have better
model interpretations (see Figure 5).
We note that recent work [33] also claims to estimate uncertainty in the graph convolutional neural
networks setting. This work uses a two-stage strategy: it ﬁrstly takes the observed network as a
realisation from a parametric Bayesian relational model  and then uses Bayesian Neural Networks
to infer the model parameters. The ﬁnal result is a posterior distribution over these variables. Unlike
the SDREM  this work performs the inference in two stages and also lacks inferential interpretability.
Computational complexities The computational complexity of the SDREM is O(N DK+(N K+
NE)L + NEK 2) and scales to the number of positive links  NE. In particular  O(N DK) refers to
the feature information incorporation in the input layer  O((N K + NE)L) refers to the information
propagation in the deep network architecture and O(NEK 2) refers to the relational data modelling
in the output layer. The SDREM’s computational complexity is comparable to that of the HLFM 
which is O(N DK + N KL + NEK 2)  and the NARM  which is O(N DK + NEK 2) [34] and is
signiﬁcantly less than that of the MMSB-type algorithms.

5 Experiments

Dataset Information In the following  we examine four real-world datasets: three standard cita-
tion networks (Citeer  Cora  Pubmed [32] and one protein-to-protein interaction network (PPI) [38].
Summary statistics for these datasets are displayed in Table 1. In the citation datasets  nodes corre-
spond to documents and edges represent citation links. A node’s features comprise the documents’
bag-of-words representations. In the protein-to-protein dataset  we use the pre-processed feature
information provided by [12].

6

Table 1: Dataset information. N is the number of nodes  NE is the number of positive links  D is the number
of features  F.D.= # nonzeros entries=# total entries in F and it refers to the density of features.

Dataset
Citeer
Pubmed

N

3; 312
2; 000

NE
4; 715
17; 522

D

3; 703
500

F.D.
0:86%
1:80%

Dataset
Cora
PPI

N

2; 708
4; 000

NE
5; 429
105; 775

D

1; 433

50

F.D.
1:27%
10:20%

Figure 2: Left: Mean AUC (dots) and per iteration computing time (bar heights) comparison between the
simpliﬁed SDREM and the MMSB for each dataset. Right: Mean AUC performance as a function of the
number of membership distributions (K; with L = 3) and the number of layers (L; with K = 20).

Evaluation Criteria We primarily focus on link prediction and use this to evaluate model per-
formance. We use AUC (Area Under ROC Curve) and Average Negative-Log-likelihood on test
relational data as the two comparison criteria. The AUC value represents the probability that the al-
gorithm will rank a randomly chosen existing-link higher than a randomly chosen non-existing link.
Therefore  the higher the AUC value  the better the predictive performance. For hyper-parameters we
gl;fc(l)gl are all given Gam(1; 1)
specify M (cid:24) Gam(N; 1) for all datasets  and f(cid:13)(1)
priors. Each reported criteria value is the mean of 10 replicate analyses. Each replicate uses 2000
MCMC iterations with the ﬁrst 1000 discarded as burn-in. Unless speciﬁed  reported AUC values
are obtained by using 90% (per row) of the data as training data and the remaining 10% as test data.
The testing relational data are not used when constructing the information propagation matrix (i.e.
we set f(cid:12)(l)
i′i

gl = 0 if Ri′i is testing data).

gd;f(cid:13)(l)

d

1 ; (cid:13)(l)

0

Validating the data augmentation trick: We ﬁrst evaluate the effectiveness of the data augmen-
tation trick through comparisons with the MMSB [1]. To make a fair comparison  we specify the
SDREM as FFF = 0N(cid:2)1; L = 1; K = 20  so that the membership distributions in each model follow
the same Dirichlet distribution f(cid:25)(cid:25)(cid:25)igi (cid:24) Dirichlet((cid:11)(cid:1) 1111(cid:2)20). Figure 2 (left panel) displays the mean
AUC and per iteration running time for these two models. It is clear that the AUC values of the sim-
pliﬁed SDREM are always better than those of the MMSB  and the time required for one iteration in
the SDREM is substantially lower (at least two orders of magnitude lower) than that of the MMSB.
Note that the running time of the SDREM is highest for the PPI dataset  since it contains the largest
number of positive links and the computational cost of the SDREM scales with this value.

Different settings of K and L: We evaluate the SDREM’s behaviour under different architecture
settings  through the inﬂuence of two parameters: K  the length of the membership distributions 
and L  the number of layers. When testing the effect of different values of K we ﬁxed L = 3  and
when varying L we ﬁxed K = 20. Figure 2 (right panel) displays the resulting mean AUC values
under these settings. As might be expected  the SDREM’s AUC value increases with higher model
complexity (i.e. larger values of K and L). The worst performance occurs with L = 1 layer as it has
the least ﬂexible modelling capability. Considering the computational complexity and modelling
power  we set K = 20 and L = 4 for the remaining analyses in this paper.

Deep network architecture: We evaluate the advantage of using neighbourhood connections to
propagate layer-wise information. Three different deep network architectures are compared: (1)
Plain-SDREM. We assume the nodes’ feature information is unavailable and use an identity matrix
to represent the features (i.e. FFF = IN(cid:2)N ) (we tried two cases  FFF = 0N(cid:2)1 and FFF = IN(cid:2)N and found
the latter to perform better). (2) Fully-connected-SDREM (Full-SDREM). The propagation coefﬁ-
cient B(l)
i′i is not restricted to be 0 when Ri′i = 0 and instead a hierarchical Gamma process is spec-
iﬁed as a sparse prior on all the propagation coefﬁcients. (3) Independent-SDREM (Inde-SDREM).

7

CiteerPubmedPPICora0.600.650.700.750.800.850.900.951.00AUCAUC for SDREMAUC for MMSB050100150200250300350400450Running Time (seconds per iteration)Running Time in SDREMRunning Time in MMSB10152030 K0.700.750.800.850.90AUCCoraPubmedPPICiteer12345L0.600.650.700.750.800.850.90Figure 3: Mean AUC ((cid:6)1:96(cid:2) standard errors (of the mean)) and negative Log-Likelihood ((cid:6)1:96(cid:2) standard
errors) on 10% test data for each dataset.

Figure 4: Mean AUC and negative Log-Likelihood values (points) as a function of the proportion of training
data (x-axis)  for each dataset and deep network architecture. Vertical lines correspond to the 95% conﬁdence
interval of reported statistics (cid:6)1:96(cid:2) standard error.

This assumes each node propagates information only to itself and does not exchange information
with other nodes in the deep network architecture (i.e. each fBBB(l)gl is a diagonal matrix).
Figure 3 shows the performance of each of these different conﬁgurations against the non-restricted
SDREM. It is clear that the non-restricted SDREM achieves the best performance in both mean AUC
and negative-Log-Likelihood among all network conﬁgurations. The Full-SDREM consistently per-
forms the worst among all conﬁgurations. This suggests that the fully connected architecture is a
poor candidate  and the sampler may become easily be trapped in local modes.

Performance in the presence of feature information: We compare the SDREM with several
alternative Bayesian methods for relational data and one Graph Convolutional Network model.
We examine: the Hierarchical Latent Feature Relational Model (HLFM) [15]  the Node Attribute
Relational Model (NARM) [34]  the Hierarchical Gamma Process-Edge Partition Model (HGP-
EPM) [36] and a graph convolutional neural network (GCN) [20]. The NARM  HGP-EPM and
GCN methods are executed using their respective authors’ implementations  under their default set-
tings. The HLFM is implemented to the best of our abilities and we set the same number of layers
and length of latent binary representation as the SDREM. For the GCN  the AUC value is calculated
based on the pairwise similarities between the node representations and the ground-truth relational
data and the Negative Log-Likelihood is unavailable due to its frequentist setting.
Figure 4 shows the performance of each method on the four datasets  under different ratios of training
data (x-axis). In terms of AUC  the SDREM performs the best among all the methods when the
proportion of training data ratio is larger than 0:5. However  the performance of the SDREM is
not outstanding when the training data ratio is less than 0:5. This may partly be due to there being
insufﬁcient relational data to effectively model the latent counts. Since the SDREM and the HLFM
are the best performing two algorithms in most cases  this conﬁrms the effectiveness of utilising a
deep network architecture. Similarly conclusions can be drawn based on the negative log-likelihood:
the SDREM and the HLFM are the best performing two algorithms.

8

CiteerPubmedPPICora0.550.600.650.700.750.800.850.900.95AUCSDREMPlain-SDREMFull-SDREMInde-SDREMCiteerPubmedPPICora0.0000.0050.0100.0150.0200.025Neg-Log-likelihood0.10.20.30.40.50.60.70.80.90.550.600.650.700.75AUCCiteerSDREMNARMHLFMHGP-EPMGCN0.10.20.30.40.50.60.70.80.90.00300.00350.00400.0045Neg-Log-likelihoodSDREMNARMHLFMHGP-EPM0.10.20.30.40.50.60.70.80.90.60.70.80.9Pubmed0.10.20.30.40.50.60.70.80.90.01750.02000.02250.02500.02750.03000.03250.10.20.30.40.50.60.70.80.90.800.850.900.95PPI0.10.20.30.40.50.60.70.80.90.0200.0250.0300.0350.10.20.30.40.50.60.70.80.90.600.650.700.750.800.85Cora0.10.20.30.40.50.60.70.80.90.004750.005000.005250.005500.005750.006000.006250.00650g3
Figure 5: Left: visualizations on the membership distributions (f(cid:25)(cid:25)(cid:25)(l)
l=1) and normalized auxiliary count-
ing variable ( (cid:22)XXX 1:50) for the ﬁrst 50 nodes of the Citeer dataset (row represents the nodes and column rep-
resents the latent features); right: visualizations on the non-zero positions (RRR + III) and transition coefﬁcient
matrix (f(cid:12)(cid:12)(cid:12)(l)g2

l=1) for the ﬁrst 200 nodes of the Citeer dataset.

1:50

Table 2: Average latent counts (per node) in different layers.

Dataset
Citeer
Pubmed

Layer 3 Layer 2 Layer 1 Dataset
Cora
533:7
PPI
292:4

7:8
24:8

2:5
10:1

Layer 3 Layer 2 Layer 1
290:1
65:6

2:3
12:7

7:0
20:1

Comparison with Variational Graph Auto-Encoder We also make brief comparisons with the
Variational Graph Auto-Encoder (VGAE) [21]. Taking 90% of the data as training data and the
remaining as testing data  the average AUC scores of 16 random VGAE runs for these datasets
are: Citeseer (0.863)  Cora (0.854)  Pubmed (0.921) and PPI (0.934). Considering the attributes of
these datasets  we ﬁnd that VGAE obtains a better performance than our SDREM in the datasets
with sparse linkages  whereas their performance in other types of datasets are competitive. This
phenomenon might be caused by two reasons: (1) due to the inference nature (backward latent counts
propagating and forward variable sampling)  our SDREM propagates less counting information (see
Table 2) to higher layers. The deep hierarchical structure might be less powerful in sparse networks;
(2) the Sigmod and ReLu activation functions might be more ﬂexible than the Dirichlet distribution
for the case of sparse networks. We will keep on investigating this issue in the future work.

Latent structure visualization: We also visualize the latent structures of the model to get further
insights in Figure 5. According to the left panel  we can see that the membership distributions
gradually become more distinguished along with the layers. The less distinguished membership
distributions might due to two reasons: (1) the higher abstraction of the latent features; (2) the
insufﬁcient latent counting information back-propagated to these higher layers. The normalized
latent counting vector (XXX) looks to be identical to the output membership distribution (cid:25)(cid:25)(cid:25)(3). This
veriﬁes that our introduction of XXX seems to successfully pass the information to the latent integers
variable ZZZ. In the right panel of information propagation matrix  we can see that the neighbourhood-
wise information seems to become weaker from the input layer to the output layer.

6 Conclusion

We have introduced a Bayesian framework by using deep latent representations for nodes to model
relational data. Through efﬁcient neighbourhood-wise information propagation in the deep network
architecture and a novel data augmentation trick  the proposed SDREM is a promising approach
for modelling scalable networks. As the SDREM can provide variability estimates for its latent
variables and predictions  it has the potential to be a competitive alternative to frequentist graph con-
volutional network-type algorithms. The promising experimental results validate the effectiveness of
the SDREM’s deep network architecture and its competitive performance against other approaches.
Since the SDREM is the ﬁrst work to use neighbourhood-wise information propagation in Bayesian
methods  combining this with other Bayesian relational models and other applications with pairwise
data (e.g. collaborative ﬁltering) would be interesting future work.

9

(1)1:50(2)1:50(3)1:50X1:50R+I(1)(2)Acknowledgements

Xuhui Fan and Scott A. Sisson are supported by the Australian Research Council through the Aus-
tralian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS  CE140100049) 
and Scott A. Sisson through the Discovery Project Scheme (DP160102544). Bin Li is supported
by Shanghai Municipal Science & Technology Commission (16JC1420401) and the Program for
Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning.

References
[1] Edoardo M. Airoldi  David M. Blei  Stephen E. Fienberg  and Eric P. Xing. Mixed membership

stochastic blockmodels. In NIPS  pages 33–40  2009.

[2] James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NIPS  pages

1993–2001  2016.

[3] Hanjun Dai  Zornitsa Kozareva  Bo Dai  Alex Smola  and Le Song. Learning steady-states of

iterative algorithms over graphs. In ICML  pages 1114–1122  2018.

[4] David B Dunson and Amy H Herring. Bayesian latent variable models for mixed discrete

outcomes. Biostatistics  6(1):11–25  2005.

[5] David K Duvenaud  Dougal Maclaurin  Jorge Iparraguirre  Rafael Bombarell  Timothy Hirzel 
Alán Aspuru-Guzik  and Ryan P Adams. Convolutional networks on graphs for learning molec-
ular ﬁngerprints. In NIPS  pages 2224–2232  2015.

[6] Xuhui Fan  Bin Li  and Scott Sisson. Rectangular bounding process. In NeurIPS  pages 7631–

7641  2018.

[7] Xuhui Fan  Bin Li  and Scott Sisson. Binary space partitioning forests. In AISTATS  volume 89

of Proceedings of Machine Learning Research  2019.

[8] Xuhui Fan  Bin Li  and Scott A. Sisson. The binary space partitioning-tree process. In AISTATS 

volume 84 of Proceedings of Machine Learning Research  pages 1859–1867  2018.

[9] Xuhui Fan  Bin Li  Yi Wang  Yang Wang  and Fang Chen. The Ostomachion Process. In AAAI

Conference on Artiﬁcial Intelligence  pages 1547–1553  2016.

[10] Zhe Gan  Ricardo Henao  David Carlson  and Lawrence Carin. Learning deep sigmoid belief

networks with data augmentation. In AISTATS  pages 268–276  2015.

[11] Zhe Gan  Chunyuan Li  Ricardo Henao  David E Carlson  and Lawrence Carin. Deep temporal

sigmoid belief networks for sequence modeling. In NIPS  pages 2467–2475. 2015.

[12] Will Hamilton  Zhitao Ying  and Jure Leskovec.

graphs. In NIPS  pages 1024–1034  2017.

Inductive representation learning on large

[13] Qirong Ho  Ankur P. Parikh  and Eric P. Xing. A multiscale community blockmodel for net-

work exploration. Journal of the American Statistical Association  107(499):916–934  2012.

[14] Changwei Hu  Piyush Rai  and Lawrence Carin. Non-negative matrix factorization for discrete

data with hierarchical side-information. In AISTATS  pages 1124–1132  2016.

[15] Changwei Hu  Piyush Rai  and Lawrence Carin. Deep generative models for relational data

with side information. In ICML  pages 1578–1586  2017.

[16] Ilkka Huopaniemi  Tommi Suvitaival  Janne Nikkilä  Matej Orešiˇc  and Samuel Kaski. Multi-

variate multi-way analysis of multi-source data. Bioinformatics  26(12):i391–i398  2010.

[17] Brian Karrer and Mark E.J. Newman. Stochastic blockmodels and community structure in

networks. Physical Review E  83(1):016107  2011.

[18] Charles Kemp  Joshua B. Tenenbaum  Thomas L. Grifﬁths  Takeshi Yamada  and Naonori
Ueda. Learning systems of concepts with an inﬁnite relational model. In AAAI  pages 381–
388  2006.

10

[19] Dae Il. Kim  Michael Hughes  and Erik Sudderth. The nonparametric metadata dependent

relational model. In ICML  pages 1559–1566  2012.

[20] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. 2016.

[21] Thomas N Kipf and Max Welling. Variational graph auto-encoders.

arXiv:1611.07308  2016.

arXiv preprint

[22] Phaedon-Stelios. Koutsourelakis and Tina Eliassi-Rad. Finding mixed-memberships in social

networks. In AAAI  2008.

[23] Bin Li  Qiang Yang  and Xiangyang Xue. Transfer learning for collaborative ﬁltering via a

rating-matrix generative model. In ICML  pages 617–624  2009.

[24] Nikhil Mehta  Lawrence Carin  and Piyush Rai. Stochastic blockmodels meet graph neural

networks. arXiv preprint arXiv:1905.05738  2019.

[25] Kurt Miller  Michael I. Jordan  and Thomas L. Grifﬁths. Nonparametric latent feature models

for link prediction. In NIPS  pages 1276–1284  2009.

[26] Mark EJ Newman and Michelle Girvan. Finding and evaluating community structure in net-

works. Physical review E  69(2):026113  2004.

[27] Krzysztof Nowicki and Tom A.B. Snijders. Estimation and prediction for stochastic block

structures. Journal of the American Statistical Association  96(455):1077–1087  2001.

[28] Konstantina Palla  David A. Knowles  and Zoubin Ghahramani. An inﬁnite latent attribute

model for network data. In ICML. 2012.

[29] Ian Porteous  Evgeniy Bart  and Max Welling. Multi-HDP: A non parametric Bayesian model

for tensor factorization. In AAAI  pages 1487–1490  2008.

[30] Piyush Rai  Changwei Hu  Ricardo Henao  and Lawrence Carin. Large-scale bayesian multi-

label learning via topic-based label embeddings. In NIPS  pages 3222–3230. 2015.

[31] Steven M Roman and Gian-Carlo Rota. The umbral calculus. Advances in Mathematics 

27(2):95 – 188  1978.

[32] Prithviraj Sen  Galileo Namata  Mustafa Bilgic  Lise Getoor  Brian Galligher  and Tina Eliassi-

Rad. Collective classiﬁcation in network data. In AI magazine  pages 29–93  2008.

[33] Yingxue Zhang  Soumyasundar Pal  Mark Coates  and Deniz Üstebay. Bayesian graph convo-
lutional neural networks for semi-supervised classiﬁcation. arXiv preprint arXiv:1811.11103 
2018.

[34] He Zhao  Lan Du  and Wray Buntine. Leveraging node attributes for incomplete relational

data. In ICML  pages 4072–4081  2017.

[35] He Zhao  Lan Du  Wray Buntine  and Mingyuan Zhou. Dirichlet belief networks for topic

structure learning. In NeurIPS  pages 7966–7977  2018.

[36] Mingyuan Zhou. Inﬁnite edge partition models for overlapping community detection and link

prediction. In AISTATS  pages 1135–1143  2015.

[37] Mingyuan Zhou  Yulai Cong  and Bo Chen. Augmentable gamma belief networks. Journal of

Machine Learning Research  17(163):1–44  2016.

[38] Marinka Zitnik and Jure Leskove. Predicting multicellular function through multi-layer tissue

networks. In Bioinformatics  pages i190–i198  2017.

11

,Xuhui Fan
Bin Li
Caoyuan Li
Scott SIsson
Ling Chen