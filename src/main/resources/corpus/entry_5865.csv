2016,Variational Information Maximization for Feature Selection,Feature selection is one of the most fundamental problems in machine learning. An extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels. Practical methods are forced to rely on approximations due to the difficulty of estimating mutual information. We demonstrate that approximations made by existing methods are based on unrealistic assumptions. We formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information. These bounds define a novel information-theoretic framework for feature selection  which we prove to be optimal under tree graphical models with proper choice of variational distributions. Our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.,Variational Information Maximization for

Feature Selection

Shuyang Gao
University of Southern California  Information Sciences Institute
gaos@usc.edu  gregv@isi.edu  galstyan@isi.edu

Greg Ver Steeg

Aram Galstyan

Abstract

Feature selection is one of the most fundamental problems in machine learning.
An extensive body of work on information-theoretic feature selection exists which
is based on maximizing mutual information between subsets of features and class
labels. Practical methods are forced to rely on approximations due to the difﬁculty
of estimating mutual information. We demonstrate that approximations made by
existing methods are based on unrealistic assumptions. We formulate a more ﬂex-
ible and general class of assumptions based on variational distributions and use
them to tractably generate lower bounds for mutual information. These bounds
deﬁne a novel information-theoretic framework for feature selection  which we
prove to be optimal under tree graphical models with proper choice of variational
distributions. Our experiments demonstrate that the proposed method strongly
outperforms existing information-theoretic feature selection approaches.

1

Introduction

Feature selection is one of the fundamental problems in machine learning research [1  2]. Its prob-
lematic issues include a large number of features that are either irrelevant or redundant for the task at
hand. In these cases  it is often advantageous to pick a smaller subset of features to avoid over-ﬁtting 
to speed up computation  or simply to improve the interpretability of the results.
Feature selection approaches are usually categorized into three groups: wrapper  embedded and
ﬁlter [3  4  5]. The ﬁrst two methods  wrapper and embedded  are considered classiﬁer-dependent 
i.e.  the selection of features somehow depends on the classiﬁer being used. Filter methods  on the
other hand  are classiﬁer-independent and deﬁne a scoring function between features and labels in
the selection process.
Because ﬁlter methods may be employed in conjunction with a wide variety of classiﬁers  it is im-
portant that the scoring function of these methods is as general as possible. Since mutual information
(MI) is a general measure of dependence with several unique properties [6]  many MI-based scoring
functions have been proposed as ﬁlter methods [7  8  9  10  11  12]; see [5] for an exhaustive list.
Owing to the difﬁculty of estimating mutual information in high dimensions  most existing MI-based
feature selection methods are based on various low-order approximations for mutual information.
While those approximations have been successful in certain applications  they are heuristic in nature
and lack theoretical guarantees. In fact  as we demonstrate in Sec. 2.2  a large family of approximate
methods are based on two assumptions that are mutually inconsistent.
To address the above shortcomings  in this paper we introduce a novel feature selection method
based on a variational lower bound on mutual information; a similar bound was previously studied
within the Infomax learning framework [13]. We show that instead of maximizing the mutual infor-
mation  which is intractable in high dimensions (hence the introduction of many heuristics)  we can

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

maximize a lower bound on the MI with the proper choice of tractable variational distributions. We
use this lower bound to deﬁne an objective function and derive a forward feature selection algorithm.
We provide a rigorous proof that the forward feature selection is optimal under tree graphical models
by choosing an appropriate variational distribution. This is in contrast with previous information-
theoretic feature selection methods which lack any performance guarantees. We also conduct em-
pirical validation on various datasets and demonstrate that the proposed approach outperforms state-
of-the-art information-theoretic feature selection methods.
In Sec. 2 we introduce general MI-based feature selection methods and discuss their limitations.
Sec. 3 introduces the variational lower bound on mutual information and proposes two speciﬁc vari-
ational distributions. In Sec. 4  we report results from our experiments  and compare the proposed
approach with existing methods.

2

Information-Theoretic Feature Selection Background

2.1 Mutual Information-Based Feature Selection
Consider a supervised learning scenario where x = {x1  x2  ...  xD} is a D-dimensional input fea-
ture vector  and y is the output label. In ﬁlter methods  the mutual information-based feature selec-
tion task is to select T features xS⇤ = {xf1  xf2  ...  xfT } such that the mutual information between
xS⇤ and y is maximized. Formally 

S⇤ = arg max

S

I (xS : y) s.t. |S| = T

(1)

where I(·) denotes the mutual information [6].
Forward Sequential Feature Selection Maximizing the objective function in Eq. 1 is generally
NP-hard. Many MI-based feature selection methods adopt a greedy method  where features are
selected incrementally  one feature at a time. Let St1 = {xf1  xf2  ...  xft1} be the selected
feature set after time step t  1. According to the greedy method  the next feature ft at step t is
selected such that

(2)
where xSt1[i denotes x’s projection into the feature space St1 [ i. As shown in [5]  the mutual
information term in Eq. 2 can be decomposed as:

ft = arg max
i /2St1

I (xSt1[i : y)

I (xSt1[i : y) = I (xSt1 : y) + I (xi : y|xSt1)

= I (xSt1 : y) + I (xi : y)  I (xi : xSt1) + I (xi : xSt1|y)
= I (xSt1 : y) + I (xi : y)

(3)

 (H (xSt1)  H (xSt1|xi)) + (H (xSt1|y)  H (xSt1|xi  y))

where H(·) denotes the entropy [6]. Omitting the terms that do not depend on xi in Eq. 3  we can
rewrite Eq. 2 as follows:

ft = arg max
i /2St1

I (xi : y) + H (xSt1|xi)  H (xSt1|xi  y)

(4)

The greedy learning algorithm has been analyzed in [14].

2.2 Limitations of Previous MI-Based Feature Selection Methods
Estimating high-dimensional
Therefore 
most MI-based feature selection methods propose low-order approximation to H (xSt1|xi) and
H (xSt1|xi  y) in Eq. 4. A general family of methods rely on the following approximations [5]:

information-theoretic quantities is a difﬁcult

task.

(5)

H (xSt1|xi) ⇡

H (xfk|xi)

H (xSt1|xi  y) ⇡

H (xfk|xi  y)

t1Xk=1
t1Xk=1

2

The approximations in Eq. 5 become exact under the following two assumptions [5]:

Assumption 1. (Feature Independence Assumption) p (xSt1|xi) =
p (xfk|xi)
Assumption 2. (Class-Conditioned Independence Assumption) p (xSt1|xi  y) =
p (xfk|xi  y)
Assumption 1 and Assumption 2 mean that the selected features are independent and class-
conditionally independent  respectively  given the unselected feature xi under consideration.

t1Qk=1

t1Qk=1

Assumption 1

Assumption 2

Satisfying both Assumption 1 and
Assumption 2

Figure 1: The ﬁrst two graphical models show the assumptions of traditional MI-based feature selec-
tion methods. The third graphical model shows a scenario when both Assumption 1 and Assumption
2 are true. Dashed line indicates there may or may not be a correlation between two variables.

We now demonstrate that the two assumptions cannot be valid simultaneously unless the data has
a very speciﬁc (and unrealistic) structure. Indeed  consider the graphical models consistent with
either assumption  as illustrated in Fig. 1. If Assumption 1 holds true  then xi is the only common
cause of the previously selected features St1 = {xf1  xf2  ...  xft1}  so that those features become
independent when conditioned on xi. On the other hand  if Assumption 2 holds  then the features
depend both on xi and class label y; therefore  generally speaking  distribution over those features
does not factorize by solely conditioning on xi—there will be remnant dependencies due to y. Thus 
if Assumption 2 is true  then Assumption 1 cannot be true in general  unless the data is generated
according to a very speciﬁc model shown in the rightmost model in Fig. 1. Note  however  that in
this case  xi becomes the most important feature because I(xi : y) > I(xSt1 : y); then we should
have selected xi at the very ﬁrst step  contradicting the feature selection process.
As we mentioned above  most existing methods implicitly or explicitly adopt both assumptions or
their stronger versions  as shown in [5]—including mutual information maximization (MIM) [15] 
joint mutual information (JMI) [8]  conditional mutual information maximization (CMIM) [9] 
maximum relevance minimum redundancy (mRMR) [10]  conditional Infomax feature extrac-
tion (CIFE) [16]  etc. Approaches based on global optimization of mutual information  such as
quadratic programming feature selection (QPFS) [11] and the state-of-the-art conditional mutual
information-based spectral method (SPECCMI) [12]  are derived from the previous greedy methods
and therefore also implicitly rely on those two assumptions.
In the next section we address these issues by introducing a novel information-theoretic framework
for feature selection. Instead of estimating mutual information and making mutually inconsistent
assumptions  our framework formulates a tractable variational lower bound on mutual information 
which allows a more ﬂexible and general class of assumptions via appropriate choices of variational
distributions.

3 Method

3.1 Variational Mutual Information Lower Bound

Let p(x  y) be the joint distribution of input (x) and output (y) variables. Barber & Agkov [13]
derived the following lower bound for mutual information I(x : y) by using the non-negativity of

KL-divergence  i.e. Px p (x|y) log p(x|y)

q(x|y)  0 gives:

I (x : y)  H (x) + hln q (x|y)ip(x y)

(6)

3

where angled brackets represent averages and q(x|y) is an arbitrary variational distribution. This
bound becomes exact if q(x|y) ⌘ p(x|y).
It is worthwhile to note that in the context of unsupervised representation learning  p(y|x) and
q(x|y) can be viewed as an encoder and a decoder  respectively. In this case  y needs to be learned
by maximizing the lower bound in Eq. 6 by iteratively adjusting the parameters of the encoder and
decoder  such as [13  17].

3.2 Variational Information Maximization for Feature Selection
Naturally  in terms of information-theoretic feature selection  we could also try to optimize the
variational lower bound in Eq. 6 by choosing a subset of features S⇤ in x  such that 

S⇤ = arg max

S

nH (xS) + hln q (xS|y)ip(xS  y)o

However  the H(xS) term in RHS of Eq. 7 is still intractable when xS is very high-dimensional.
Nonetheless  by noticing that variable y is the class label  which is usually discrete  and hence H(y)
is ﬁxed and tractable  by symmetry we switch x and y in Eq. 6 and rewrite the lower bound as
follows:

I (x : y)  H (y) + hln q (y|x)ip(x y) =⌧ln✓ q (y|x)

p (y) ◆p(x y)
The equality in Eq. 8 is obtained by noticing that H(y) = h ln p (y)ip(y).
By using Eq. 8  the lower bound optimal subset S⇤ of x becomes:

S⇤ = arg max

S

(⌧ln✓ q (y|xS)

p (y) ◆p(xS  y))

3.2.1 Choice of Variational Distribution
q(y|xS) in Eq. 9 can be any distribution as long as it is normalized. We need to choose q(y|xS) to
be as general as possible while still keeping the term hln q (y|xS)ip(xS  y) tractable in Eq. 9.
As a result  we set q(y|xS) as

(7)

(8)

(9)

(10)

(11)

(12)

q (y|xS) =

q (xS  y)
q (xS)

=

q (xS|y) p (y)
q (xS|y0) p (y0)

Py0

We can verify that Eq. 10 is normalized even if q(xS|y) is not normalized.
If we further denote 

then by combining Eqs. 9 and 10  we get 

q (xS|y0) p (y0)

q (xS) =Xy0
I (xS : y) ⌧ln✓ q (xS|y)

q (xS) ◆p(xS  y) ⌘ ILB (xS : y)

And we also have the following equation which shows the gap between I(xS : y) and ILB(xS : y) 

I (xS : y)  ILB (xS : y) = hKL (p (y|xS)||q (y|xS))ip(xS )

(13)
Auto-Regressive Decomposition.
Now that q(y|xS) is deﬁned  all we need to do is model
q(xS|y) under Eq. 10  and q(xS) is easy to compute based on q(xS|y). Here we decompose
q(xS|y) as an auto-regressive distribution assuming T features in S:
q (xft|xf<t  y)

q (xS|y) = q (xf1|y)

(14)

TYt=2

4

Figure 2: Auto-regressive decomposition for q(xS|y)

where xf<t denotes {xf1  xf2  ...  xft1}. The graphical model in Fig. 2 demonstrates this decom-
position. The main advantage of this model is that it is well-suited for the forward feature selection
procedure where one feature is selected at a time (which we will explain in Sec. 3.2.3). And if
q (xft|xf<t  y) is tractable  then so is the whole distribution q(xS|y). Therefore  we would ﬁnd
tractable Q-distributions over q (xft|xf<t  y). Below we illustrate two such Q-distributions.
Naive Bayes Q-distribution.
A natural idea would be to assume xt is independent of other
variables given y  i.e. 

Then the variational distribution q(y|xS) can be written based on Eqs. 10 and 15 as follows:

q (xft|xf<t  y) = p (xft|y)

q (y|xS) =

p (y) Qj2S
Py0
p (y0) Qj2S

p (xj|y)
p (xj|y0)

(15)

(16)

(18)

And we also have the following theorem:
Theorem 3.1 (Exact Naive Bayes). Under Eq. 16  the lower bound in Eq. 8 becomes exact if and

only if data is generated by a Naive Bayes model  i.e.  p (x  y) = p (y)Qi

The proof for Theorem 3.1 becomes obvious by using the mutual information deﬁnition. Note that
the most-cited MI-based feature selection method mRMR [10] also assumes conditional indepen-
dence given the class label y as shown in [5  18  19]  but they make additional stronger independence
assumptions among only feature variables.
Pairwise Q-distribution. We now consider an alternative approach that is more general than the
Naive Bayes distribution:

p (xi|y).

q (xft|xf<t  y) = t1Yi=1

p (xft|xfi  y)! 1

t1

(17)

In Eq. 17  we assume q (xft|xf<t  y) to be the geometric mean of conditional distributions
q(xft|xfi  y). This assumption is tractable as well as reasonable because if the data is gener-
ated by a Naive Bayes model  the lower bound in Eq. 8 also becomes exact using Eq. 17 due to
p (xft|xfi  y) ⌘ p (xft|y) in that case.
3.2.2 Estimating Lower Bound From Data
Assuming either Naive Bayes Q-distribution or pairwise Q-distribution  it is convenient to estimate
q(xS|y) and q(xS) in Eq. 12 by using plug-in probability estimators for discrete data or one/two-
dimensional density estimators for continuous data. We also use the sample mean to approximate
the expectation term in Eq. 12. Our ﬁnal estimator for ILB (xS : y) is written as follows:

bILB (xS : y) =

1

N Xx(k) y(k)

lnbq⇣x(k)
S |y(k)⌘
bq⇣x(k)
S ⌘

wherex(k)  y(k) are samples from data  andbq(·) denotes the estimate for q(·).

5

3.2.3 Variational Forward Feature Selection Under Auto-Regressive Decomposition
After deﬁning q(y|xS) in Eq. 10 and auto-regressive decomposition of q(xS|y) in Eq. 15  we are
able to do the forward feature selection previously described in Eq. 2  but replace the mutual infor-
mation with its lower bound bILB. Recall that St1 is the set of selected features after step t  1 

then the feature ft will be selected at step t such that

(19)

ft = arg max

i /2St1 bILB (xSt1[i : y)

where bILB (xSt1[i : y) can be obtained from bILB (xSt1 : y) recursively by auto-regressive de-
composition q (xSt1[i|y) = q (xSt1|y) q (xi|xSt1  y) where q (xSt1|y) is stored at step t  1.
This forward feature selection can be done under auto-regressive decomposition in Eqs. 10 and 14
for any Q-distribution. However  calculating q(xi|xSt  y) may vary according to different Q-
distributions. We can verify that it is easy to get q(xi|xSt  y) recursively from q(xi|xSt1  y) under
Naive Bayes or pairwise Q-distribution. We call our algorithm under these two Q-distributions
VMI naive and VMI pairwise respectively.
It is worthwhile noting that the lower bound does not always increase at each step. A decrease in
lower bound at step t indicates that the Q-distribution would approximate the underlying distribu-
tion worse than it did at previous step t  1. In this case  the algorithm would re-maximize the
lower bound from zero with only the remaining unselected features. We summarize the concrete
implementation of our algorithms in supplementary Sec. A.
Time Complexity.
Although our algorithm needs to calculate the distributions at each step 
we only need to calculate the probability value at each sample point. For both VMI naive and
VMI pairwise  the total computational complexity is O(N DT ) assuming N as number of samples 
D as total number of features  T as number of ﬁnal selected features. The detailed time analysis is
left for the supplementary Sec. A. As shown in Table 1  our methods VMI naive and VMI pairwise
have the same time complexity as mRMR [10]  while the state-of-the-art global optimization method
SPECCMI [12] is required to precompute the pairwise mutual information matrix  which gives a
time complexity of O(N D2).

Table 1: Time complexity in number of features D  selected number of features d  and number
of samples N

Method

Complexity O(N DT ) O(N DT )

mRMR VMI naive VMI pairwise SPECCMI
O(N D2)

O(N DT )

Optimality Under Tree Graphical Models. Although our method VMI naive assumes a Naive
Bayes model  we can prove that this method is still optimal if the data is generated according to
tree graphical models. Indeed  both of our methods  VMI naive and VMI pairwise  will always
prioritize the ﬁrst layer features  as shown in Fig. 3. This optimality is summarized in Theorem B.1
in supplementary Sec. B.

4 Experiments

Synthetic Data.
We begin with the experiments on a synthetic model according to the tree
structure illustrated in the left part of Fig. 3. The detailed data generating process is shown in
supplementary section D. The root node Y is a binary variable  while other variables are continuous.
We use VMI naive to optimize the lower bound ILB(x : y). 5000 samples are used to generate the
synthethic data  and variational Q-distributions are estimated by the kernel density estimator. We
can see from the plot in the right-hand part of Fig. 3 that our algorithm  VMI naive  selects x1  x2 
x3 as the ﬁrst three features  although x2 and x3 are only weakly correlated with y. If we continue
to add deeper level features {x4  ...  x9}  the lower bound will decrease. For comparison  we also
illustrate the mutual information between each single feature xi and y in Table 2. We can see from
Table 2 that it would choose x1  x4 and x5 as the top three features by using the maximum relevance
criteria [15].

6

Figure 3: (Left) This is the generative model used for synthetic experiments. Edge thickness repre-
sents the relationship strength. (Right) Optimizing the lower bound by VMI naive. Variables under
the blue line denote the features selected at each step. Dotted blue line shows the decreasing lower
bound if adding more features. Ground-truth mutual information is obtained using N = 100  000
samples.

featurei
x9
I(xi : y) 0.111 0.052 0.022 0.058 0.058 0.025 0.029 0.012 0.013

x5

x6

x4

x1

x2

x3

x7

x8

Table 2: Mutual information between label y and each feature xi for Fig. 3. I(xi : y) is estimated
using N=100 000 samples. Top three variables with highest mutual information are highlighted in
bold.

Real-World Data. We compare our algorithms VMI naive and VMI pairwise with other pop-
ular information-theoretic feature selection methods  including mRMR [10]  JMI [8]  MIM [15] 
CMIM [9]  CIFE [16]  and SPECCMI [12]. We use 17 well-known datasets in previous feature
selection studies [5  12] (all data are discretized). The dataset summaries are illustrated in supple-
mentary Sec. C. We use the average cross-validation error rate on the range of 10 to 100 features to
compare different algorithms under the same setting as [12]. Tenfold cross-validation is employed
for datasets with number of samples N  100 and leave-one-out cross-validation otherwise. The
3-nearest-neighbor classiﬁer is used for Gisette and Madelon  following [5]. For the remaining
datasets  the chosen classiﬁer is Linear SVM  following [11  12].
The experimental results can be seen in Table 3.1 The entries with ⇤ and ⇤⇤ indicate the best perfor-
mance and the second best performance  respectively (in terms of average error rate). We also use
the paired t-test at 5% signiﬁcant level to test the hypothesis that VMI naive or VMI pairwise per-
form signiﬁcantly better than other methods  or vice visa. Overall  we ﬁnd that both of our methods 
VM Inaive and VM Ipairwise  strongly outperform other methods. This indicates that our variational
feature selection framework is a promising addition to the current literature of information-theoretic
feature selection.

Figure 4: Number of selected features versus average cross-validation error in datasets Semeion and
Gisette.

1We omit the results for M IM and CIF E due to space limitations. The complete results are shown in the

supplementary Sec. C.

7

Leukemia
Lymphoma

Table 3: Average cross-validation error rate comparison of VMI against other methods. The
last two lines indicate win(W)/tie (T)/ loss(L) for VMI naive and VMI pairwise respectively.
Dataset
VMI naive VMI pairwise
SPECCMI
7.4±(3.6)⇤
Lung
14.5±(6.0)
11.6±(5.6)
11.2±(2.7)⇤
11.9±(1.7)⇤⇤
Colon
16.1±(2.0)
0.2±(0.5)⇤⇤
0.0±(0.1)⇤
1.8±(1.3)
5.2±(3.1)⇤⇤
3.7±(1.9)⇤
12.0±(6.6)
13.7±(0.5)⇤⇤ 13.7±(0.5)⇤⇤ 13.7±(0.5)⇤⇤
18.8±(0.8)⇤
18.8±(1.0)⇤⇤
21.0±(3.5)
15.9±(0.6)⇤⇤ 15.9±(0.6)⇤⇤
15.9±(0.5)⇤
5.1±(0.7)⇤⇤
5.1±(0.6)⇤
5.3±(0.5)
12.0±(1.0)⇤
12.7±(1.9)⇤⇤
16.8±(1.6)
14.0±(4.0)⇤
14.5±(3.9)⇤⇤
26.0±(9.3)
3.0±(1.1)⇤
3.5±(1.1)⇤⇤
4.8±(3.0)
7.2±(2.5)⇤
9.2±(6.0)
7.6±(3.6)
12.6±(0.5)⇤⇤
12.8±(0.6)
15.1±(1.8)
6.6±(0.3)⇤
6.6±(0.3)⇤
9.0±(2.3)
20.4±(3.1)⇤
21.2±(3.9)⇤⇤
24.0±(3.7)
4.8±(0.9)⇤⇤
4.2±(0.8)⇤
7.1±(1.3)
15.9±(2.5)⇤⇤
16.7±(2.7)
16.6±(2.9)
13/2/2
12/3/2

10.9±(4.7)⇤⇤
11.6±(4.7)
17.3±(3.0)
19.7±(2.6)
0.4±(0.7)
1.4±(1.2)
6.6±(2.2)
5.6±(2.8)
13.6±(0.4)⇤ 13.7±(0.5)⇤⇤
Splice
19.5±(1.2)
18.9±(1.0)
Landsat
15.9±(0.5)⇤
Waveform 15.9±(0.5)⇤
5.1±(0.7)⇤⇤
5.2±(0.6)
KrVsKp
16.6±(1.6)
12.8±(0.9)
Ionosphere
Semeion
23.4±(6.5)
24.8±(7.6)
Multifeat.
4.0±(1.6)
4.0±(1.6)
Optdigits
7.6±(3.3)
7.6±(3.2)
12.4±(0.7)⇤
12.8±(0.7)
Musk2
7.0±(0.8)
6.9±(0.7)
22.4±(4.0)
21.5±(2.8)
5.5±(0.9)
5.9±(0.7)
15.3±(2.6)⇤
30.8±(3.8)
10/6/1
11/4/2
9/6/2
9/6/2

CMIM
11.4±(3.0)
18.4±(2.6)
1.1±(2.0)
8.6±(3.3)
14.7±(0.3)
19.1±(1.1)
16.0±(0.7)
5.3±(0.5)
13.1±(0.8)
16.3±(4.4)
3.6±(1.2)
7.5±(3.4)⇤⇤
13.0±(1.0)
6.8±(0.7)⇤⇤
22.1±(2.9)
5.1±(1.3)
17.4±(2.6)
10/7/0
13/3/1

mRMR

JMI

Spambase
Promoter
Gisette
Madelon

#W1/T1/L1:
#W2/T2/L2:

We also plot the average cross-validation error with respect to number of selected features. Fig. 4
shows the two most distinguishable data sets  Semeion and Gisette. We can see that both of our
methods  VMI N aive and VMI pairwise  have lower error rates in these two data sets.
5 Related Work

There has been a signiﬁcant amount of work on information-theoretic feature selection in the past
twenty years: [5  7  8  9  10  15  11  12  20]  to name a few. Most of these methods are based on
combinations of so-called relevant  redundant and complimentary information. Such combinations
representing low-order approximations of mutual information are derived from two assumptions 
and it has proved unrealistic to expect both assumptions to be true. Inspired by group testing [21] 
more scalable feature selection methods have been developed  but thos methods also require the
calculation of high-dimensional mutual information as a basic scoring function.
Estimating mutual information from data requires a large number of observations—especially when
the dimensionality is high. The proposed variational lower bound can be viewed as a way of esti-
mating mutual information between a high-dimensional continuous variable and a discrete variable.
Only a few examples exist in literature [22] under this setting. We hope our method will shed light
on new ways to estimate mutual information  similar to estimating divergences in [23].

6 Conclusion

Feature selection has been a signiﬁcant endeavor over the past decade. Mutual information gives
a general basis for quantifying the informativeness of features. Despite the clarity of mutual in-
formation  estimating it can be difﬁcult. While a large number of information-theoretic methods
exist  they are rather limited and rely on mutually inconsistent assumptions about underlying data
distributions. We introduced a unifying variational mutual information lower bound to address these
issues and showed that by auto-regressive decomposition  feature selection can be done in a forward
manner by progressively maximizing the lower bound. We also presented two concrete methods
using Naive Bayes and pairwise Q-distributions  which strongly outperform the existing methods.
VMI naive only assumes a Naive Bayes model  but even this simple model outperforms the existing
information-theoretic methods  indicating the effectiveness of our variational information maximiza-
tion framework. We hope that our framework will inspire new mathematically rigorous algorithms
for information-theoretic feature selection  such as optimizing the variational lower bound globally
and developing more powerful variational approaches for capturing complex dependencies.

8

References
[1] Manoranjan Dash and Huan Liu. Feature selection for classiﬁcation. Intelligent data analysis  1(3):131–

[2] Huan Liu and Hiroshi Motoda. Feature selection for knowledge discovery and data mining  volume 454.

Springer Science & Business Media  2012.

[3] Ron Kohavi and George H John. Wrappers for feature subset selection. Artiﬁcial intelligence  97(1):273–

156  1997.

324  1997.

[4] Isabelle Guyon and Andr´e Elisseeff. An introduction to variable and feature selection. The Journal of

Machine Learning Research  3:1157–1182  2003.

[5] Gavin Brown  Adam Pocock  Ming-Jie Zhao  and Mikel Luj´an. Conditional likelihood maximisation: a
unifying framework for information theoretic feature selection. The Journal of Machine Learning Re-
search  13(1):27–66  2012.

[6] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons  2012.
[7] Roberto Battiti. Using mutual information for selecting features in supervised neural net learning. Neural

Networks  IEEE Transactions on  5(4):537–550  1994.

[8] Howard Hua Yang and John E Moody. Data visualization and feature selection: New algorithms for

nongaussian data. In NIPS  volume 99  pages 687–693. Citeseer  1999.

[9] Franc¸ois Fleuret. Fast binary feature selection with conditional mutual information. The Journal of

Machine Learning Research  5:1531–1555  2004.

[10] Hanchuan Peng  Fuhui Long  and Chris Ding. Feature selection based on mutual information criteria of
max-dependency  max-relevance  and min-redundancy. Pattern Analysis and Machine Intelligence  IEEE
Transactions on  27(8):1226–1238  2005.

[11] Irene Rodriguez-Lujan  Ramon Huerta  Charles Elkan  and Carlos Santa Cruz. Quadratic programming

feature selection. The Journal of Machine Learning Research  11:1491–1516  2010.

[12] Xuan Vinh Nguyen  Jeffrey Chan  Simone Romano  and James Bailey. Effective global approaches for
In Proceedings of the 20th ACM SIGKDD international

mutual information based feature selection.
conference on Knowledge discovery and data mining  pages 512–521. ACM  2014.

[13] David Barber and Felix Agakov. The im algorithm: a variational approach to information maximiza-
tion. In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference 
volume 16  page 201. MIT Press  2004.

[14] Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset selection 
sparse approximation and dictionary selection. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11)  pages 1057–1064  2011.

[15] David D Lewis. Feature selection and feature extraction for text categorization. In Proceedings of the
workshop on Speech and Natural Language  pages 212–217. Association for Computational Linguistics 
1992.

[16] Dahua Lin and Xiaoou Tang. Conditional infomax learning: an integrated framework for feature extrac-

tion and fusion. In Computer Vision–ECCV 2006  pages 68–82. Springer  2006.

[17] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning. In Advances in Neural Information Processing Systems  pages 2116–
2124  2015.

[18] Kiran S Balagani and Vir V Phoha. On the feature selection criterion based on an approximation of
multidimensional mutual information. IEEE Transactions on Pattern Analysis & Machine Intelligence 
(7):1342–1343  2010.

[19] Nguyen Xuan Vinh  Shuo Zhou  Jeffrey Chan  and James Bailey. Can high-order dependencies improve

mutual information based feature selection? Pattern Recognition  2015.

[20] Hongrong Cheng  Zhiguang Qin  Chaosheng Feng  Yong Wang  and Fagen Li. Conditional mutual
information-based feature selection analyzing for synergy and redundancy. ETRI Journal  33(2):210–
218  2011.

[21] Yingbo Zhou  Utkarsh Porwal  Ce Zhang  Hung Q Ngo  Long Nguyen  Christopher R´e  and Venu Govin-
daraju. Parallel feature selection inspired by group testing. In Advances in Neural Information Processing
Systems  pages 3554–3562  2014.

[22] Brian C Ross. Mutual information between discrete and continuous data sets. PloS one  9(2):e87357 

[23] XuanLong Nguyen  Martin J Wainwright  and Michael I Jordan. Estimating divergence functionals
Information Theory  IEEE Transactions on 

and the likelihood ratio by convex risk minimization.
56(11):5847–5861  2010.

feature selection code.

http://github.com/BiuBiuBiLL/

[24] Shuyang Gao.

Variational
InfoFeatureSelection.

[25] Chris Ding and Hanchuan Peng. Minimum redundancy feature selection from microarray gene expression

data. Journal of bioinformatics and computational biology  3(02):185–205  2005.

[26] Kevin Bache and Moshe Lichman. Uci machine learning repository  2013.

2014.

9

,Fabian Sinz
Anna Stockl
Jan Grewe
Jan Benda
Shuyang Gao
Greg Ver Steeg
Aram Galstyan