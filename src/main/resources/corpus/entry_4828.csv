2016,Safe Policy Improvement by Minimizing Robust Baseline Regret,An important problem in sequential decision-making under uncertainty is to use limited data to  compute a safe policy  i.e.  a policy that is guaranteed to perform at least as well as a given baseline strategy. In this paper  we develop and analyze a new model-based approach to compute a safe policy when we have access to an inaccurate dynamics model of the system with known accuracy guarantees. Our proposed robust method uses this (inaccurate) model to directly minimize the (negative) regret w.r.t. the baseline policy. Contrary to the existing approaches  minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and seamlessly fall back to the baseline policy  otherwise. We show that our formulation is NP-hard and propose an approximate algorithm. Our empirical results on several domains show that even this relatively simple approximate algorithm can significantly outperform standard approaches.,Safe Policy Improvement by Minimizing Robust

Baseline Regret

Marek Petrik

University of New Hampshire

mpetrik@cs.unh.edu

Mohammad Ghavamzadeh
Adobe Research & INRIA Lille

ghavamza@adobe.com

Abstract

Yinlam Chow

Stanford University

ychow@stanford.edu

An important problem in sequential decision-making under uncertainty is to use
limited data to compute a safe policy  which is guaranteed to outperform a given
In this paper  we develop and analyze a new model-based
baseline strategy.
approach that computes a safe policy  given an inaccurate model of the system’s
dynamics and guarantees on the accuracy of this model. The new robust method
uses this model to directly minimize the (negative) regret w.r.t. the baseline policy.
Contrary to existing approaches  minimizing the regret allows one to improve
the baseline policy in states with accurate dynamics and to seamlessly fall back
to the baseline policy  otherwise. We show that our formulation is NP-hard and
propose a simple approximate algorithm. Our empirical results on several domains
further show that even the simple approximate algorithm can outperform standard
approaches.

1

Introduction

Many problems in science and engineering can be formulated as a sequential decision-making
problem under uncertainty. A common scenario in such problems that occurs in many different ﬁelds 
such as online marketing  inventory control  health informatics  and computational ﬁnance  is to ﬁnd
a good or an optimal strategy/policy  given a batch of data generated by the current strategy of the
company (hospital  investor). Although there are many techniques to ﬁnd a good policy given a batch
of data  only a few of them guarantee that the obtained policy will perform well  when it is deployed.
Since deploying an untested policy can be risky for the business  the product (hospital  investment)
manager does not usually allow it to happen  unless we provide her/him with some performance
guarantees of the obtained strategy  in comparison to the baseline policy (for example the policy that
is currently in use).
In this paper  we focus on the model-based approach to this fundamental problem in the context
of inﬁnite-horizon discounted Markov decision processes (MDPs). In this approach  we use the
batch of data and build a model or a simulator that approximates the true behavior of the dynamical
system  together with an error function that captures the accuracy of the model at each state of the
system. Our goal is to compute a safe policy  i.e.  a policy that is guaranteed to perform at least
as well as the baseline strategy  using the simulator and error function. Most of the work on this
topic has been in the model-free setting  where safe policies are computed directly from the batch of
data  without building an explicit model of the system [Thomas et al.  2015b a]. Another class of
model-free algorithms are those that use a batch of data generated by the current policy and return a
policy that is guaranteed to perform better. They optimize for the policy by repeating this process
until convergence [Kakade and Langford  2002; Pirotta et al.  2013].
A major limitation of the existing methods for computing safe policies is that they either adopt a
newly learned policy with provable improvements or do not make any improvement at all by returning
the baseline policy. These approaches may be quite limiting when model uncertainties are not uniform

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

across the state space. In such cases  it is desirable to guarantee an improvement over the baseline
policy by combining it with a learned policy on a state-by-state basis. In other words  we want to use
the learned policy at the states in which either the improvement is signiﬁcant or the model uncertainty
(error function) is small  and to use the baseline policy everywhere else. However  computing a
learned policy that can be effectively combined with a baseline policy is non-trivial due to the complex
effects of policy changes in an MDP. Our key insight is that this goal can be achieved by minimizing
the (negative) robust regret w.r.t. the baseline policy. This uniﬁes the sources of uncertainties in the
learned and baseline policies and allows a more systematic performance comparison. Note that our
approach differs signiﬁcantly from the standard one  which compares a pessimistic performance
estimate of the learned policy with an optimistic estimate of the baseline strategy. That may result in
rejecting a learned policy with a performance (slightly) better than the baseline  simply due to the
discrepancy between the pessimistic and optimistic evaluations.
The model-based approach of this paper builds on robust Markov decision processes [Iyengar  2005;
Wiesemann et al.  2013; Ahmed and Varakantham  2013]. The main difference is the availability
of the baseline policy that creates unique challenges for sequential optimization. To the best of
our knowledge  such challenges have not yet been fully investigated in the literature. A possible
solution is to solve the robust formulation of the problem and then accept the resulted policy only
if its conservative performance estimate is better than the baseline. While a similar idea has been
investigated in the model-free setting (e.g.  [Thomas et al.  2015a])  we show in this paper that it can
be overly conservative.
As the main contribution of the paper  we propose and analyze a new robust optimization formulation
that captures the above intuition of minimizing robust regret w.r.t. the baseline policy. After a
preliminary discussion in Section 2  we formally describe our model and analyze its main properties
in Section 3. We show that in solving this optimization problem  we may have to go beyond the
standard space of deterministic policies and search in the space of randomized policies; we derive a
bound on the performance loss of its solutions; and we prove that solving this problem is NP-hard.
We also propose a simple and practical approximate algorithm. Then  in Section 4  we show that
the standard model-based approach is really a tractable approximation of robust baseline regret
minimization. Finally  our experimental results in Section 5 indicate that even the simple approximate
algorithm signiﬁcantly outperforms the standard model-based approach when the model is uncertain.

2 Preliminaries

We consider problems in which the agent’s interaction with the environment is modeled as an inﬁnite-
horizon γ-discounted MDP. A γ-discounted MDP is a tuple M = (cid:104)X  A  r  P  p0  γ(cid:105)  where X and
A are the state and action spaces  r(x  a) ∈ [−Rmax  Rmax] is the bounded reward function  P (·|x  a)
is the transition probability function  p0(·) is the initial state distribution  and γ ∈ (0  1] is a discount
factor. We use ΠR = {π : X → ∆A
} and ΠD = {π : X → A} to denote the sets of randomized
and deterministic stationary Markovian policies  respectively  where ∆A is the set of probability
distributions over the action space A.
Throughout the paper  we assume that the true reward r of the MDP is known  but the true transition
probability denoted by (cid:98)P . Due to limited number of samples and other modeling issues  it is unlikely
probability is not given. The generalization to include reward estimation is straightforward and is
omitted for the sake of brevity. We use historical data to build a MDP model with the transition
that (cid:98)P matches the true transition probability of the system P (cid:63). We also require that the estimated
model (cid:98)P deviates from the true transition probability P (cid:63) as stated in the following assumption:
Assumption 1. For each (x  a) ∈ X ×A  the error function e(x  a) bounds the (cid:96)1 difference between
the estimated transition probability and true transition probability  i.e. 

(cid:107)P (cid:63)(·|x  a) − (cid:98)P (·|x  a)(cid:107)1 ≤ e(x  a).

(1)

The error function e can be derived either directly from samples using high probability concentration
bounds  as we brieﬂy outline in Appendix A  or based on speciﬁc domain properties.
To model the uncertainty in the transition probability  we adopt the notion of robust MDP
(RMDP) [Iyengar  2005; Nilim and El Ghaoui  2005; Wiesemann et al.  2013]  i.e.  an extension of

2

.

MDP in which nature adversarially chooses the transitions from a given uncertainty set

(cid:110)
(cid:111)
ξ : X × A → ∆X : (cid:107)ξ(·|x  a) − (cid:98)P (·|x  a)(cid:107)1 ≤ e(x  a)  ∀x  a ∈ X × A

Ξ((cid:98)P   e) =

From Assumption 1  we notice that the true transition probability is in the set of uncertain tran-

sition probabilities  i.e.  P (cid:63) ∈ Ξ((cid:98)P   e). The above (cid:96)1 constraint is common in the RMDP litera-

ture (e.g.  [Iyengar  2005; Wiesemann et al.  2013; Petrik and Subramanian  2014]). The uncertainty
set Ξ in RMDP is (x  a)-rectangular and randomized [Le Tallec  2007; Wiesemann et al.  2013].
One of the motivations for considering (x  a)-rectangular sets in RMDP is that they lead to tractable
solutions in the conventional reward maximization setting. However  in the robust regret minimization
problem that we propose in this paper  even if we assume that the uncertainty set is (x  a)-rectangular 
it does not guarantee tractability of the solution. While it is of great interest to investigate the structure
of uncertainty sets that lead to tractable algorithms in robust regret minimization  it is beyond the
main scope of this paper and we leave it as future work.
For each policy π ∈ ΠR and nature’s choice ξ ∈ Ξ  the discounted return is deﬁned as
0 vξ
π 

γtr(cid:0)Xt  At

(cid:34)T−1(cid:88)

= p(cid:62)

(cid:35)

(cid:1)

Eξ

| X0 ∼ p0  At ∼ π(Xt)

ρ(π  ξ) = lim
T→∞

t=0

π is the corresponding
where Xt and At are the state and action random variables at time t  and vξ
value function. An optimal policy for a given ξ is deﬁned as π(cid:63)
ξ ∈ arg maxπ∈ΠR ρ(π  ξ). Similarly 
under the true transition probability P (cid:63)  the true return of a policy π and a truly optimal policy are
deﬁned as ρ(π  P (cid:63)) and π(cid:63) ∈ arg maxπ∈ΠR ρ(π  P (cid:63))  respectively. Although we deﬁne the optimal
policy using arg maxπ∈ΠR  it is known that every reward maximization problem in MDPs has at
least one optimal policy in ΠD.
Finally  given a deterministic baseline policy πB  we call a policy π safe  if its "true" performance is
guaranteed to be no worse than that of the baseline policy  i.e.  ρ(π  P (cid:63)) ≥ ρ(πB  P (cid:63)).
3 Robust Policy Improvement Model

In this section  we introduce and analyze an optimization procedure that robustly improves over a
given baseline policy πB. As described above  the main idea is to ﬁnd a policy that is guaranteed to
be an improvement for any realization of the uncertain model parameters. The following deﬁnition
formalizes this intuition.

Deﬁnition 2 (The Robust Policy Improvement Problem). Given a model uncertainty set Ξ((cid:98)P   e)
ρ(π  ξ) ≥ ρ(πB  ξ) + ζ  for every ξ ∈ Ξ((cid:98)P   e).1
and a baseline policy πB  ﬁnd a maximal ζ ≥ 0 such that there exists a policy π ∈ ΠR for which

The problem posed in Deﬁnition 2 readily translates to the following optimization problem:

πS ∈ arg max
π∈ΠR

min
ξ∈Ξ

ρ(π  ξ) − ρ(πB  ξ)

.

(2)

Note that since the baseline policy πB achieves value 0 in (2)  ζ in Deﬁnition 2 is always non-negative.

Therefore  any solution πS of (2) is safe  because under the true transition probability P (cid:63) ∈ Ξ((cid:98)P   e) 

we have the guarantee that

ρ(π  P (cid:63)) − ρ(πB  P (cid:63)) ≥ min
ξ∈Ξ

ρ(π  ξ) − ρ(πB  ξ)

≥ 0 .

It is important to highlight how Deﬁnition 2 differs from the standard approach (e.g.  [Thomas et
al.  2015a]) on determining whether a policy π is an improvement over the baseline policy πB. The
standard approach considers a statistical error bound that translates to the test: minξ∈Ξ ρ(π  ξ) ≥
maxξ∈Ξ ρ(πB  ξ). The uncertainty parameters ξ on both sides of (2) are not necessarily the same.
Therefore  any optimization procedure derived based on this test is more conservative than the
1From now on  for brevity  we omit the parameters (cid:98)P and e  and use Ξ to denote the model uncertainty set.
problem in (2). Indeed when the error function in Ξ is large  even the baseline policy (π = πB)

3

(cid:16)

(cid:16)

(cid:17)

(cid:17)

x11

ξ1

a11

a12

2

3

a1

x1

ξ2

1

start

a2

2

πB

x0

π(cid:63)

a1

a2

0

1

x1

π(cid:63)
πB

ξ(cid:63)

a1

ξ1

+10/γ

−10/γ

Figure 1: (left) A robust/uncertain MDP used in Example 4 that illustrates the sub-optimality of
deterministic policies in solving the optimization problem (2). (right) A Markov decision process
with signiﬁcant uncertainty in the baseline policy.

may not pass this test. In Section 5.1  we show the conditions under which this approach fails. Our
approach also differs from other related work in that we consider regret with respect to the baseline
policy  and not the optimal policy  as considered in [Xu and Mannor  2009].
In the remainder of this section  we highlight some major properties of the optimization problem (2).
Speciﬁcally  we show that its solution policy may be purely randomized  we compute a bound on the
performance loss of its solution policy w.r.t. π(cid:63)  and we ﬁnally prove that it is a NP-hard problem.

3.1 Policy Class

The following theorem shows that we should search for the solutions of the optimization problem (2)
in the space of randomized policies ΠR.
Theorem 3. The optimal solution to the optimization problem (2) may not be attained by a determin-
istic policy. Moreover  the loss due to considering deterministic policies cannot be bounded  i.e.  there
exists no constant c ∈ R such that

(cid:17)

(cid:16)

(cid:17)

(cid:16)

max
π∈ΠR

min
ξ∈Ξ

ρ(π  ξ) − ρ(πB  ξ)

≤ c · max
π∈ΠD

min
ξ∈Ξ

ρ(π  ξ) − ρ(πB  ξ)

.

Proof. The proof follows directly from Example 4. The optimal policy in this example is randomized
and achieves a guaranteed improvement ζ = 1/2. There is no deterministic policy that guarantees a
positive improvement over the baseline policy  which proves the second part of the theorem.
Example 4. Consider the robust/uncertain MDP on the left panel of Figure 1 with states {x1  x11} ⊂
X   actions A = {a1  a2  a11  a12}  and discount factor γ = 1. Actions a1 and a2 are shown as solid
black nodes. A number with no state represents a terminal state with the corresponding reward.
The robust outcomes {ξ1  ξ2} correspond to the uncertainty set of transition probabilities Ξ. The
baseline policy πB is deterministic and is denoted by double edges. It can be readily seen from
the monotonicity of the Bellman operator that any improved policy π will satisfy π(a12|x11) = 1.
Therefore  we will only focus on the policy at state x1. The robust improvement as a function of
π(·|x1) and the uncertainties {ξ1  ξ2} is given as follows:
ξ2
1
2

(cid:0)ρ(π  ξ) − ρ(πB  ξ)(cid:1) = min

(cid:32)(cid:34) π \ ξ

(cid:20) π \ ξ

a1

(cid:21)(cid:33)

ξ1
3
2

min
ξ∈Ξ

(cid:35)

ξ∈Ξ

a1
a2

−

ξ1
2

ξ2
1

= 0.

This shows that no deterministic policy can achieve a positive improvement in this problem. However 
a randomized policy π(a1|x1) = π(a2|x1) = 1/2 returns the maximum improvement ζ = 1/2.
Randomized policies can do better than their deterministic counterparts  because they allow for
hedging among various realizations of the MDP parameters. Example 4 shows a problem such that
there exists a realization of the parameters with improvement over the baseline when any deterministic
policy is executed. However in this example  there is no single realization of parameters that provides
an improvement for all the deterministic policies simultaneously. Therefore  randomizing the policy
guarantees an improvement independent of the parameters’ choice.

4

3.2 Performance Bound

Generally  one cannot compute the truly optimal policy π(cid:63) using an imprecise model. Nevertheless  it
is still crucial to understand how errors in the model translates to a performance loss w.r.t. an optimal
policy. The following theorem (proved in Appendix C) provides a bound on the performance loss of
any solution πS to the optimization problem (2).
Theorem 5. A solution πS to the optimization problem (2) is safe and its performance loss is bounded
by the following inequality:

(cid:26)2γRmax

(cid:16)

(cid:17)

(cid:27)

(cid:107)eπ(cid:63)(cid:107)1 u(cid:63)

π(cid:63)+(cid:107)eπB(cid:107)1 u(cid:63)

πB

  Φ(πB)

 

Φ(πS) ∆= ρ(π(cid:63)  P (cid:63)) − ρ(πS  P (cid:63)) ≤ min

(1 − γ)2
where u(cid:63)
true MDP P (cid:63). Furthermore  the above bound is tight.

π(cid:63) and u(cid:63)
πB

are the state occupancy distributions of the optimal and baseline policies in the

3.3 Computational Complexity

(cid:16)

In this section  we analyze the computational complexity of solving the optimization problem (2)
and prove that the problem is NP-hard. In particular  we proceed by showing that the following
sub-problem of (2):

arg min
ξ∈Ξ

(3)
for a ﬁxed π ∈ ΠR  is NP-hard. The optimization problem (3) can be interpreted as computing a
policy that simultaneously minimizes the returns of two MDPs  whose transitions induced by policies
π and πB. The proof of Theorem 6 is given in Appendix D.
Theorem 6. Both optimization problems (2) and (3) are NP-hard.

ρ(π  ξ) − ρ(πB  ξ)

 

(cid:17)

Although the optimization problem (2) is NP-hard in general  but it can be tractable in certain settings.
One such setting is when the Markov chain induced by the baseline policy is known precisely  as the
following proposition states. See Appendix E for the proof.
Proposition 7. Assume that for each x ∈ X   the error function induced by the baseline policy is
MDP (RMDP) problem and can be solved in polynomial time:

zero  i.e.  e(cid:0)x  πB(x)(cid:1) = 0.2 Then  the optimization problem (2) is equivalent to the following robust

arg max
π∈ΠR

min
ξ∈Ξ

ρ(π  ξ).

(4)

3.4 Approximate Algorithm

Solving for the optimal solution of (2) may not be possible in practice  since the problem is NP hard.
In this section  we propose a simple and practical approximate algorithm. The empirical results
of Section 5 indicate that this algorithm holds promise and also suggest that the approach may be a
good starting point for building better approximate algorithms in the future.

Algorithm 1: Approximate Robust Baseline Regret Minimization Algorithm

:Empirical transition probabilities: (cid:98)P   baseline policy πB  and the error function e

input
output :Policy ˜πS
1 foreach x ∈ X   a ∈ A do

2

˜e(x  a) ←

(cid:26)e(x  a) when πB(x) (cid:54)= a
(cid:0)ρ(cid:0)π  ξ(cid:1)

otherwise

0

3 end

4 ˜πS ← arg maxπ∈ΠR minξ∈Ξ((cid:98)P  ˜e)

5 return ˜πS

;

− ρ(cid:0)πB  ξ(cid:1)(cid:1) ;

Algorithm 1 contains the pseudocode of the proposed approximate method. The main idea is to
use a modiﬁed uncertainty model by assuming no error in transition probabilities of the baseline

2Note that this is equivalent to precisely knowing the Markov chain induced by the baseline policy P (cid:63)

πB .

5

policy. Then it is possible to minimize the robust baseline regret in polynomial time as suggested
by Theorem 7. Assuming no error in baseline transition probabilities is reasonable because of
two main reasons. First  in practice  data is often generated by executing the baseline policy  and
thus  we may have enough data for a good approximation of the baseline’s transition probabilities:

· |x  πB(x)(cid:1). Second  transition probabilities often affect baseline

and improved policies similarly  and as a result  have little effect on the difference between their
returns (i.e.  the regret). See Section 5.1 for an example of such behavior.

∀x ∈ X  (cid:98)P(cid:0)

· |x  πB(x)(cid:1)

≈ P (cid:63)(cid:0)

4 Standard Policy Improvement Methods

In Section 3  we showed that ﬁnding an exact solution to the optimization problem (2) is computa-
tionally expensive and proposed an approximate algorithm. In this section  we describe and analyze
two standard methods for computing safe policies and show how they can be interpreted as an
approximation of our proposed baseline regret minimization. Due to space limitations  we describe
another method  called reward-adjusted MDP  in Appendix H  but report its performance in Section 5.

4.1 Solving the Simulator

The simplest solution to (2) is to assume that our simulator is accurate and to solve the reward maxi-

mization problem of an MDP with the transition probability (cid:98)P   i.e.  πsim ∈ arg maxπ∈ΠR ρ(π (cid:98)P ).
transition probability (cid:98)P . Then under Assumption 1  the performance loss of πsim is bounded by

Theorem 8 quantiﬁes the performance loss of the resulted policy πsim.
Theorem 8. Let πsim be an optimal policy of the reward maximization problem of an MDP with

Φ(πsim) ∆= ρ(π(cid:63)  P (cid:63)) − ρ(πsim  P (cid:63)) ≤

2γRmax

(1 − γ)2(cid:107)e(cid:107)∞.

The proof is available in Appendix F. Note that there is no guarantee that πsim is safe  and thus 
deploying it may lead to undesirable outcomes due to model uncertainties. Moreover  the performance
guarantee of πsim  reported in Theorem 8  is weaker than that in Theorem 5 due to the L∞ norm.

4.2 Solving Robust MDP

Another standard solution to the problem in (2) is based on solving the RMDP problem (4). We
prove that the policy returned by this algorithm is safe and has better (sharper) worst-case guarantees
than the simulator-based policy πsim. Details of this algorithm are summarized in Algorithm 2. The
algorithm ﬁrst constructs and solves an RMDP. It then returns the solution policy if its worst-case
performance over the uncertainty set is better than the robust performance maxξ∈Ξ ρ(πB  ξ)  and it
returns the baseline policy πB  otherwise.

input
output :Policy πR

Algorithm 2: RMDP-based Algorithm

:Simulated MDP (cid:98)P   baseline policy πB  and the error function e

1 π0 ← arg maxπ∈ΠR minξ∈Ξ((cid:98)P  e) ρ(cid:0)π  ξ(cid:1) ;
2 if minξ∈Ξ((cid:98)P  e) ρ(cid:0)π0  ξ(cid:1) > maxξ∈Ξ ρ(πB  ξ) then return π0 else return πB ;
(cid:17)

Algorithm 2 makes use of the following approximation to the solution of (2):

(cid:16)

≥ max
π∈ΠR

min
ξ∈Ξ

ρ(π  ξ) − max
ξ∈Ξ

max
π∈ΠR

min
ξ∈Ξ

ρ(π  ξ) − ρ(πB  ξ)

ρ(πB  ξ) 

and guarantees safety by designing π such that the RHS of this inequality is always non-negative.
The performance bound of πR is identical to that in Theorem 5 and is stated and proved in Theorem 12
in Appendix G. Although the worst-case bounds are the same  we show in Section 5.1 that the
performance loss of πR may be worse than that of πS by an arbitrarily large margin.

6

It is important to discuss the difference between Algorithms 1 and 2. Although both solve an RMDP 
they use different uncertainty sets Ξ. The uncertainty set used in Algorithm 2 is the true error function
in building the simulator  while the uncertainty set used in Algorithm 1 assumes that the error function
is zero for all the actions suggested by the baseline policy. As a result  both algorithms approximately
solve (2) but approximate the problem in different ways.

5 Experimental Evaluation

In this section  we experimentally evaluate the beneﬁts of minimizing the robust baseline regret. First 
we demonstrate that solving the problem in (2) may outperform the regular robust formulation by an
arbitrarily large margin. Then  in the remainder of the section  we compare the solution quality of
Algorithm 1 with simpler methods in more complex and realistic experimental domains. The purpose
of our experiments is to show how solution quality depends on the degree of model uncertainties.

5.1 An Illustrative Example

Consider the example depicted on the right panel of Figure 1. White nodes represent states and black
nodes represent state-action pairs. Labels on the edges originated from states indicate the policy
according to which the action is taken; labels on the edges originated from actions denote the rewards
and  if necessary  the name of the uncertainty realization. The baseline policy is πB  the optimal
policy is π(cid:63)  and the discount factor is γ ∈ (0  1).
This example represents a setting in which the level of uncertainty varies signiﬁcantly across the
individual states: the transition model is precise in state x0 and uncertain in state x1. The baseline
policy πB takes a suboptimal action in state x0 and the optimal action in the uncertain state x1. To
prevent being overly conservative in computing a safe policy  one needs to consider that the realization
of uncertainty in x1 inﬂuences both the baseline and improved policies.
Using the plain robust optimization formulation in Algorithm 2  even the optimal policy π(cid:63) is not
considered safe in this example. In particular  the robust return of π(cid:63) is minξ ρ(π(cid:63)  ξ) = −9  while
the optimistic return of πB is maxξ ρ(πB  ξ) = +10. On the other hand  solving (2) will return the
optimal policy since: minξ ρ(π(cid:63)  ξ) − ρ(πB  ξ) = 11 − 10 = −9 − (−10) = 1. Even the heuristic
method of Section 3.4 will return the optimal policy. Note that since the reward-adjusted formulation
(see its description in Appendix H) is even more conservative than the robust formulation  it will also
fail to improve on the baseline policy.

5.2 Grid Problem

In this section  we use a simple grid problem to compare the solution quality of Algorithm 1 with
simpler methods. The grid problem is motivated by modeling customer interactions with an online
system. States in the problem represent a two dimensional grid. Columns capture states of interaction
with the website and rows capture customer states such as overall satisfaction. Actions can move
customers along either dimension with some probability of failure. A more detailed description of
this domain is provided in Section I.1.
Our goal is to evaluate how the solution quality of various methods depends on the magnitude of the
model error e. The model is constructed from samples  and thus  its magnitude of error depends on
the number of samples used to build it. We use a uniform random policy to gather samples. Model
error function e is then constructed from this simulated data using bounds in Section B. The baseline
policy is constructed to be optimal when ignoring the row part of state; see Section I.1 for more
details.
All methods are compared in terms of the improvement percentage in total return over the baseline
policy. Figure 2 depicts the results as a function of the number of transition samples used in
constructing the uncertain model and represents the mean of 40 runs. Methods used in the comparison
are as follows: 1) EXP represents solving the nominal model as described in Section 4.1  2) RWA
represent the reward-adjusted formulation in Algorithm 3 of Appendix H  3) ROB represents the
robust method in Algorithm 2  and 4) RBC represents our approximate solution of Algorithm 1.
Figure 2 shows that Algorithm 1 not only reliably computes policies that are safe  but also signiﬁcantly
improves on the quality of the baseline policy when the model error is large. When the number of

7

Figure 2: Improvement in return over the baseline policy in: (left) the grid problem and (right) the
energy arbitrage problem. The dashed line shows the return of the optimal policy.

samples is small  Algorithm 1 is signiﬁcantly better than other methods by relying on the baseline
policy in states with a large model error and only taking improving actions when the model error is
small. Note that EXP can be signiﬁcantly worse than the baseline policy  especially when the number
of samples is small.

5.3 Energy Arbitrage

In this section  we compare model-based policy improvement methods using a more complex domain.
The problem is to determine an energy arbitrage policy in given limited energy storage (a battery)
and stochastic prices. At each time period  the decision-maker observes the available battery charge
and a Markov state of energy price  and decides on the amount of energy to purchase or to sell.
The set of states in the energy arbitrage problem consists of three components: current state of charge 
current capacity  and a Markov state representing price; the actions represent the amount of energy
purchased or sold; the rewards indicate proﬁt/loss in the transactions. We discretize the state of
charge and action sets to 10 separate levels. The problem is based on the domain from [Petrik and
Wu  2015]  whose description is detailed in Appendix I.2.
Energy arbitrage is a good ﬁt for model-based approaches because it combines known and unknown
dynamics. Physics of battery charging and discharging can be modeled with high conﬁdence  while
the evolution of energy prices is uncertain. As a result  using an explicit battery model  the only
uncertainty is in transition probabilities between the 10 states of the price process instead of the entire
1000 state-action pairs. This signiﬁcantly reduces the number of samples needed.
As in the previous experiments  we estimate the uncertainty model in a data-driven manner. Notice
that the inherent uncertainty is only in price transitions and is independent of the policy used (which
controls the storage dynamics). Here the uncertainty set of transition probabilities is estimated using
the method in Appendix A  but the uncertainty set is only a non-singleton w.r.t. price states. Figure 2
shows the percentage improvement on the baseline policy averaged over 5 runs. We clearly observe
that the heuristic RBC method  described in Section 3.4  effectively interleaves the baseline policy (in
states with high level of uncertainty) and an improved policy (in states with low level of uncertainty) 
and results in the best performance in most cases. Solving a robust MDP with no baseline policy
performed similarly to directly solving the simulator.

6 Conclusion

In this paper  we study the model-based approach to the fundamental problem of learning safe
policies given a batch of data. A policy is considered safe  if it is guaranteed to have an improved
performance over a baseline policy. Solving the problem of safety in sequential decision-making can
immensely increase the applicability of the existing technology to real-world problems. We show
that the standard robust formulation may be overly conservative and formulate a better approach
that interleaves an improved policy with the baseline policy  based on the error at each state. We
propose and analyze an optimization problem based on this idea (see (2)) and prove that solving it is
NP-hard. Furthermore  we propose several approximate solutions and experimentally evaluated their
performance.

8

050010001500200025003000NumberofSamples−20−10010203040ImprovementoverBaseline(%)EXPRWAROBRBC1000150020002500300035004000450050005500Numberofsamples−0.20−0.15−0.10−0.050.000.050.100.15Improvementoverbaseline(%)EXPROBRBCReferences
A. Ahmed and P Varakantham. Regret based Robust Solutions for Uncertain Markov Decision

Processes. Advances in neural information processing systems  pages 1–9  2013.

T. Hansen  P. Miltersen  and U. Zwick. Strategy iteration is strongly polynomial for 2-player
turn-based stochastic games with a constant discount factor. Journal of the ACM  60(1):1–16 
2013.

G. Iyengar. Robust dynamic programming. Mathematics of Operations Research  30(2):257–280 

2005.

S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceed-

ings of the 19th International Conference on Machine Learning  pages 267–274  2002.

Y. Le Tallec. Robust  Risk-Sensitive  and Data-driven Control of Markov Decision Processes. PhD

thesis  MIT  2007.

A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition

matrices. Operations Research  53(5):780–798  2005.

M. Petrik and D. Subramanian. RAAM : The beneﬁts of robustness in approximating aggregated

MDPs in reinforcement learning. In Neural Information Processing Systems  2014.

M. Petrik and X. Wu. Optimal Threshold Control for Energy Arbitrage with Degradable Battery

Storage. In Uncertainty in Artiﬁcial Intelligence  pages 692–701  2015.

M. Pirotta  M. Restelli  and D. Calandriello. Safe Policy Iteration. In Proceedings of the 30th

International Conference on Machine Learning  2013.

P. Thomas  G. Teocharous  and M. Ghavamzadeh. High Conﬁdence Policy Improvement.

International Conference on Machine Learning  pages 2380–2388  2015.

In

P. Thomas  G. Theocharous  and M. Ghavamzadeh. High conﬁdence off-policy evaluation. In

Proceedings of the Twenty-Ninth Conference on Artiﬁcial Intelligence  2015.

T. Weissman  E. Ordentlich  G. Seroussi  S. Verdu  and M. Weinberger. Inequalities for the L1

deviation of the empirical distribution. Hewlett-Packard Labs  Tech. Rep  2003.

W. Wiesemann  D. Kuhn  and B. Rustem. Robust Markov decision processes. Mathematics of

Operations Research  38(1):153–183  2013.

H. Xu and S. Mannor. Parametric regret in uncertain Markov decision processes. Proceedings of the

IEEE Conference on Decision and Control  pages 3606–3613  2009.

9

,Ahmed Hefny
Carlton Downey
Geoffrey Gordon
Mohammad Ghavamzadeh
Marek Petrik
Yinlam Chow
Lixin Fan