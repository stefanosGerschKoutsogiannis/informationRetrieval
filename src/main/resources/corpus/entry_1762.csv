2015,Efficient Thompson Sampling for Online ￼Matrix-Factorization Recommendation,Matrix factorization (MF) collaborative filtering is an effective and widely used method in recommendation systems. However  the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem)  a crucial problem in collaborative filtering from cold-start  has not been previously addressed.In this paper  we present a novel algorithm for online MF recommendation that automatically combines finding the most relevantitems with exploring new or less-recommended items.Our approach  called Particle Thompson Sampling for Matrix-Factorization  is based on the general Thompson sampling framework  but augmented with a novel efficient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle filter.Extensive experiments in collaborative filtering using several real-world datasets demonstrate that our proposed algorithm significantly outperforms the current state-of-the-arts.,Efﬁcient Thompson Sampling for Online
Matrix-Factorization Recommendation

Jaya Kawale  Hung Bui  Branislav Kveton

Adobe Research

San Jose  CA

{kawale  hubui  kveton}@adobe.com

Long Tran Thanh

University of Southampton

Southampton  UK

ltt08r@ecs.soton.ac.uk

Sanjay Chawla

Qatar Computing Research Institute  Qatar

University of Sydney  Australia

sanjay.chawla@sydney.edu.au

Abstract

Matrix factorization (MF) collaborative ﬁltering is an effective and widely used
method in recommendation systems. However  the problem of ﬁnding an optimal
trade-off between exploration and exploitation (otherwise known as the bandit
problem)  a crucial problem in collaborative ﬁltering from cold-start  has not been
previously addressed. In this paper  we present a novel algorithm for online MF
recommendation that automatically combines ﬁnding the most relevant items with
exploring new or less-recommended items. Our approach  called Particle Thomp-
son sampling for MF (PTS)  is based on the general Thompson sampling frame-
work  but augmented with a novel efﬁcient online Bayesian probabilistic matrix
factorization method based on the Rao-Blackwellized particle ﬁlter. Extensive ex-
periments in collaborative ﬁltering using several real-world datasets demonstrate
that PTS signiﬁcantly outperforms the current state-of-the-arts.

1

Introduction

Matrix factorization (MF) techniques have emerged as a powerful tool to perform collaborative
ﬁltering in large datasets [1]. These algorithms decompose a partially-observed matrix R ∈ RN×M
into a product of two smaller matrices  U ∈ RN×K and V ∈ RM×K  such that R ≈ U V T .
A variety of MF-based methods have been proposed in the literature and have been successfully
applied to various domains. Despite their promise  one of the challenges faced by these methods
is recommending when a new user/item arrives in the system  also known as the problem of cold-
start. Another challenge is recommending items in an online setting and quickly adapting to the
user feedback as required by many real world applications including online advertising  serving
personalized content  link prediction and product recommendations.
In this paper  we address these two challenges in the problem of online low-rank matrix completion
by combining matrix completion with bandit algorithms. This setting was introduced in the previous
work [2] but our work is the ﬁrst satisfactory solution to this problem.
In a bandit setting  we
can model the problem as a repeated game where the environment chooses row i of R and the
learning agent chooses column j. The Rij value is revealed and the goal (of the learning agent) is
to minimize the cumulative regret with respect to the optimal solution  the highest entry in each row
of R. The key design principle in a bandit setting is to balance between exploration and exploitation
which solves the problem of cold start naturally. For example  in online advertising  exploration
implies presenting new ads  about which little is known and observing subsequent feedback  while
exploitation entails serving ads which are known to attract high click through rate.

1

While many solutions have been proposed for bandit problems  in the last ﬁve years or so  there
has been a renewed interest in the use of Thompson sampling (TS) which was originally proposed
in 1933 [3  4]. In addition to having competitive empirical performance  TS is attractive due to its
conceptual simplicity. An agent has to choose an action a (column) from a set of available actions so
as to maximize the reward r  but it does not know with certainty which action is optimal. Following
TS  the agent will select a with the probability that a is the best action. Let θ denotes the unknown
parameter governing reward structure  and O1:t the history of observations currently available to the
agent. The agent chooses a∗ = a with probability

(cid:90)

I(cid:104)E [r|a  θ] = max

a(cid:48)

(cid:105)

(cid:48)
E [r|a

  θ]

P (θ|O1:t)dθ

which can be implemented by simply sampling θ from the posterior P (θ|O1:t) and let a∗ =
arg maxa(cid:48) E [r|a(cid:48)  θ]. However for many realistic scenarios (including for matrix completion)  sam-
pling from P (θ|O1:t) is not computationally efﬁcient and thus recourse to approximate methods is
required to make TS practical.
We propose a computationally-efﬁcient algorithm for solving our problem  which we call Particle
Thompson sampling for matrix factorization (PTS). PTS is a combination of particle ﬁltering for
online Bayesian parameter estimation and TS in the non-conjugate case when the posterior does
not have a closed form. Particle ﬁltering uses a set of weighted samples (particles) to estimate
the posterior density. In order to overcome the problem of the huge parameter space  we utilize
Rao-Blackwellization and design a suitable Monte Carlo kernel to come up with a computationally
and statistically efﬁcient way to update the set of particles as new data arrives in an online fashion.
Unlike the prior work [2] which approximates the posterior of the latent item features by a single
point estimate  our approach can maintain a much better approximation of the posterior of the latent
features by a diverse set of particles. Our results on ﬁve different real datasets show a substantial
improvement in the cumulative regret vis-a-vis other online methods.
2 Probabilistic Matrix Factorization

We ﬁrst review the probabilistic matrix factorization approach to
the low-rank matrix completion problem. In matrix completion  a
portion Ro of the N × M matrix R = (rij) is observed  and the
goal is to infer the unobserved entries of R. In probabilistic matrix
factorization (PMF) [5]  R is assumed to be a noisy perturbation of
a rank-K matrix ¯R = U V (cid:62) where UN×K and VM×K are termed
the user and item latent features (K is typically small). The full
generative model of PMF is
Ui i.i.d. ∼
Vj i.i.d. ∼

N (0  σ2
N (0  σ2

(1)

uIK)
vIK)
(cid:62)
i Vj  σ2)

rij|U  V i.i.d. ∼ N (U

Figure 1: Graphical model of
probabilistic matrix factoriza-
tion model

U   σ2

−2
U ∼ Γ(α  β); λV = σ

where the variances (σ2  σ2
V ) are the parameters of the model.
We also consider a full Bayesian treatment where the variances
V are drawn from an inverse Gamma prior (while σ2
U and σ2
σ2
−2
is held ﬁxed)  i.e.  λU = σ
V ∼ Γ(α  β) (this is a special case of the
Bayesian PMF [6] where we only consider isotropic Gaussians)1. Given this generative model 
from the observed ratings Ro  we would like to estimate the parameters U and V which will al-
low us to “complete” the matrix R. PMF is a MAP point-estimate which ﬁnds U  V to maximize
Pr(U  V |Ro  σ  σU   σV ) via (stochastic) gradient ascend (alternate least square can also be used [1]).
Bayesian PMF [6] attempts to approximate the full posterior Pr(U  V |Ro  σ  α  β). The joint pos-
terior of U and V are intractable; however  the structure of the graphical model (Fig. 1) can be
exploited to derive an efﬁcient Gibbs sampler.
We now provide the expressions for the conditional probabilities of interest. Supposed that V and
σU are known. Then the vectors Ui are independent for each user i. Let rts(i) = {j|rij ∈ Ro} be
ij|j ∈ rts(i)} are generated i.i.d. from Ui
the set of items rated by user i  observe that the ratings {Ro
1[6] considers the full covariance structure  but they also noted that isotropic Gaussians are effective enough.

2

↵ vVjuUiMN⇥MRijfollowing a simple conditional linear Gaussian model. Thus  the posterior of Ui has the closed form
(2)

(cid:88)
(cid:88)
i rts(i)  σU   σ) = N (Ui|µu
Pr(Ui|V  Ro  σ  σU ) = Pr(Ui|Vrts(i)  Ro
i =
VjV

i )−1)
ro
ijVj.

i )−1ζ u

where µu

i ; Λu

(cid:62)
j +

i   (Λu

ζ u
i =

i =

IK;

(3)

1
σ2

j∈rts(i)

1
σ2
u

1
σ2 (Λu

j∈rts(i)

conditional

j=1 N (Vj|µv

j   (Λv

of V   Pr(V |U  Ro  σV   σ)

posterior
into
j )−1) where the mean and precision are similarly deﬁned. The posterior
−2
U given U (and simiarly for λV ) is obtained from the conjugacy of the

factorized

similarly

is

of the precision λU = σ
Gamma prior and the isotropic Gaussian

(cid:81)M

The

Pr(λU|U  α  β) = Γ(λU|

N K

2

+ α 

1

2 (cid:107)U(cid:107)2

F + β).

Although not required for Bayesian PMF  we give the likelihood expression

Pr(Rij = r|V  Ro  σU   σ) = N (r|V

(cid:62)
j µu
i  

1
σ2 + V

(cid:62)
j ΛV iVj).

(4)

(5)

The advantage of the Bayesian approach is that uncertainty of the estimate of U and V are available
which is crucial for exploration in a bandit setting. However  the bandit setting requires maitaining
online estimates of the posterior as the ratings arrive over time which makes it rather awkward
for MCMC. In this paper  we instead employ a sequential Monte-Carlo (SMC) method for online
Bayesian inference [7  8]. Similar to the Gibbs sampler [6]  we exploit the above closed form updates
to design an efﬁcient Rao-Blackwellized particle ﬁlter [9] for maintaining the posterior over time.

3 Matrix-Factorization Recommendation Bandit
In a typical deployed recommendation system  users and observed ratings (also called rewards)
arrive over time  and the task of the system is to recommend item for each user so as to maximize
the accumulated expected rewards. The bandit setting arises from the fact that the system needs to
learn over time what items have the best ratings (for a given user) to recommend  and at the same
time sufﬁciently explore all the items.
We formulate the matrix factorization bandit as follows. We assume that ratings are generated
following Eq. (1) with a ﬁxed but unknown latent features (U∗  V ∗). At time t  the environment
chooses user it and the system (learning agent) needs to recommend an item jt. The user then
V ∗
rates the recommended item with rating rit jt ∼ N (U∗
jt  σ2) and the agent receives this rating
t = rit jt. The system recommends item jt using a policy
as a reward. We abbreviate this as ro
1:t−1  where ro
that takes into account the history of the observed ratings prior to time t  ro
1:t =
V ∗
j   and
{(ik  jk  ro
is recommended. Since (U∗  V ∗)
this is achieved if the optimal item j∗(i) = arg maxj U∗
are unknown  the optimal item j∗(i) is also not known a priori. The quality of the recommendation
system is measured by its expected cumulative regret:

k=1. The highest expected reward the system can earn at time t is maxj U∗

k)}t

V ∗

(cid:62)

(cid:62)

(cid:62)

it

j

i

i

CR = E

[ro
t − rit j∗(it)]

= E

[ro
t − max

j

∗
it

U

(cid:62)

∗
j ]
V

(6)

where the expectation is taken with respect to the choice of the user at time t and also the randomness
in the choice of the recommended items by the algorithm.

3.1 Particle Thompson Sampling for Matrix Factorization Bandit

While it is difﬁcult to optimize the cumulative regret directly  TS has been shown to work well in
practice for contextual linear bandit [3]. To use TS for matrix factorization bandit  the main difﬁculty
is to incrementally update the posterior of the latent features (U  V ) which control the reward struc-
ture. In this subsection  we describe an efﬁcient Rao-Blackwellized particle ﬁlter (RBPF) designed
to exploit the speciﬁc structure of the probabilistic matrix factorization model. Let θ = (σ  α  β) be
1:t  θ). The standard
the control parameters and let posterior at time t be pt = Pr(U  V  σU   σV  |ro

3

(cid:34) n(cid:88)

t=1

(cid:35)

(cid:34) n(cid:88)

t=1

(cid:35)

˜Vj

i

(cid:46) sample new Ui due to Rao-Blackwellization

(cid:46) ˆp has the structure (w  particles) where particles[d] = (U (d)  V (d)  σ(d)

U   σ(d)
V ).

U

1:t−1)

Algorithm 1 Particle Thompson Sampling for Matrix Factorization (PTS)
Global control params: σ  σU   σV ; for Bayesian version (PTS-B): σ  α  β
1: ˆp0 ← InitializeParticles()
2: Ro = ∅
3: for t = 1  2 . . . do
i ← current user
4:
Sample d ∼ ˆpt−1.w
5:
˜V ← ˆpt−1.V (d)
6:
[If PTS-B] ˜σU ← ˆpt−1.σ(d)
7:
Sample ˜Ui ∼ Pr(Ui| ˜V   ˜σU   σ  ro
8:
ˆj ← arg maxj ˜U(cid:62)
9:
Recommend ˆj for user i and observe rating r.
10:
t ← (i  ˆj  r)
11:
ro
ˆpt ← UpdatePosterior(ˆpt−1  ro
12:
13: end for
14: procedure UPDATEPOSTERIOR(ˆp  ro
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29: end procedure

(i  j  r) ← ro
∀d  Λu
1:t−1)  ζ u
1:t−1)
i
∀d  wd ∝ Pr(Rij = r|V (d)  σ(d)
∀d  i ∼ ˆp.w; ˆp(cid:48).particles[d] ← ˆp.particles[i]; ∀d  ˆp(cid:48).wd ← 1
for all d do

1:t−1)  see Eq.(5) (cid:80) wd = 1

(d) ← Λu
j ; ζ u
i
i ∼ Pr(Ui|ˆp(cid:48).V (d)  ˆp(cid:48).σ(d)
(d) ← Λv
j ∼ Pr(Vj|ˆp(cid:48).U (d)  ˆp(cid:48).σ(d)

(d) ← ζ u
Λu
i
i
ˆp(cid:48).U (d)
U   σ  ro
1:t)
[If PTS-B] Update the norm of ˆp(cid:48).U (d)
(d) ← ζ u
Λv
j
ˆp(cid:48).V (d)
[If PTS-B] ˆp(cid:48).σ(d)

1:t)
U ∼ Pr(σU|ˆp(cid:48).U (d)  α  β)

end for
return ˆp(cid:48)

σ2 VjV (cid:62)

j (V (d)  ro

1:t)  ζ v
j

i (V (d)  ro

1:t)

t

(d) ← Λu

(d) ← ζ u

i (V (d)  ro

i (V (d)  ro

1:t)

1:t)

(d) + 1

i

(d) + rVj

V   σ  ro

i

U   σ  ro

D

(cid:46) see Eq. (3)
(cid:46) Reweighting; see Eq.(5)
(cid:46) Resampling
(cid:46) Move

(cid:46) see Eq. (2)

(cid:46) see Eq.(4)

(cid:80)D

particle ﬁlter would sample all of the parameters (U  V  σU   σV ). Unfortunately  in our experi-
ments  degeneracy is highly problematic for such a vanilla particle ﬁlter (PF) even when σU   σV
are assumed known (see Fig. 4(b)). Our RBPF algorithm maintains the posterior distribution pt as
follows. Each of the particle conceptually represents a point-mass at V  σU (U and σV are integrated
out analytically whenever possible)2. Thus  pt(V  σU ) is approximated by ˆpt = 1
d=1 δ(V (d) σ(d)
D
U )
where D is the number of particles.
Crucially  since the particle ﬁlter needs to estimate a set of non-time-vayring parameters  having
an effective and efﬁcient MCMC-kernel move Kt(V (cid:48)  σ(cid:48)
U ; V  σU ) stationary w.r.t. pt is essential.
Our design of the move kernel Kt are based on two observations. First  we can make use of
U and σV as auxiliary variables  effectively sampling U  σV |V  σU ∼ pt(U  σV |V  σU )  and then
V (cid:48)  σ(cid:48)
U|U  σV ). However  this move would be highly inefﬁcient due to the num-
ber of variables that need to be sampled at each update. Our second observation is the key to an
efﬁcient implementation. Note that latent features for all users except the current user U−it are in-
dependent of the current observed rating ro
t : pt(U−it|V  σU ) = pt−1(U−it|V  σU )  therefore at time
t we only have to resample Uit as there is no need to resample U−it. Furthermore  it sufﬁces to
resample the latent feature of the current item Vjt. This leads to an efﬁcient implementation of the
RBPF where each particle in fact stores3 U  V  σU   σV   where (U  σV ) are auxiliary variables  and
for the kernel move Kt  we sample Uit|V  σU then V (cid:48)
The PTS algorithm is given in Algo. 1. At each time t  the complexity is O((( ˆN + ˆM )K 2 + K 3)D)
where ˆN and ˆM are the maximum number of users who have rated the same item and the maximum

U|U  σV ∼ pt(V (cid:48)  σ(cid:48)

jt|U  σV and σ(cid:48)

U|U  α  β.

2When there are fewer users than items  a similar strategy can be derived to integrate out U and σV instead.
3This is not inconsistent with our previous statement that conceptually a particle represents only a point-

mass distribution δV σU .

4

j and ζ v

number of items rated by the same user  respectively. The dependency on K 3 arises from having
to invert the precision matrix  but this is not a concern since the rank K is typically small. Line
24 can be replaced by an incremental update with caching: after line 22  we can incrementally
j for all item j previously rated by the current user i. This reduces the complexity
update Λv
to O(( ˆM K 2 + K 3)D)  a potentially signiﬁcant improvement in a real recommendation systems
where each user tends to rate a small number of items.
4 Analysis
We believe that the regret of PTS can be bounded. However  the existing work on TS and bandits
does not provide sufﬁcient tools for proper analysis of our algorithm. In particular  while existing
techniques can provide O(log T ) (or O(√T ) for gap-independent) regret bounds for our problem 
these bounds are typically linear in the number of entries of the observation matrix R (or at least
linear in the number of users)  which is typically very large  compared to T . Thus  an ideal regret
bound in our setting is the one that has sub-linear dependency (or no dependency at all) on the
number of users. A key obstacle of achieving this is that  while the conditional posteriors of U and
V are Gaussians  neither their marginal and joint posteriors belong to well behaved classes (e.g. 
conjugate posteriors  or having closed forms). Thus  novel tools  that can handle generic posteriors 
are needed for efﬁcient analysis. Moreover  in the general setting  the correlation between Ro and
the latent features U and V are non-linear (see  e.g.  [10  11  12] for more details). As existing
techniques are typically designed for efﬁciently learning linear regressions  they are not suitable for
our problem. Nevertheless  we show how to bound the regret of TS in a very speciﬁc case of n × m
rank-1 matrices  and we leave the generalization of these results for future work.
In particular  we analyze the regret of PTS in the setting of Gopalan et al. [13]. We model our

∈ Θu and v∗
j∗ > u∗

j and assume that it is uniquely optimal  u∗

problem as follows. The parameter space is Θu × Θv  where Θu = {d  2d  . . .   1}N×1 and Θv =
{d  2d  . . .   1}M×1 are discretizations of the parameter spaces of rank-1 factors u and v for some
integer 1/d. For the sake of theoretical analysis  we assume that PTS can sample from the full
posterior. We also assume that ri j ∼ N (u∗
i v∗
∈ Θu. Note that
in this setting  the highest-rated item in expectation is the same for all users. We denote this item
by j∗ = arg max 1≤j≤M v∗
j for any j (cid:54)= j∗. We
leverage these properties in our analysis. The random variable Xt at time t is a pair of a random
rating matrix Rt = {ri j}N M
i=1 j=1 and a random row 1 ≤ it ≤ N. The action At at time t is a
column 1 ≤ jt ≤ M. The observation is Yt = (it  rit jt). We bound the regret of PTS as follows.
Theorem 1. For any δ ∈ (0  1) and  ∈ (0  1)  there exists T ∗ such that PTS on Θu × Θv recom-
mends items j (cid:54)= j∗ in T ≥ T ∗ steps at most (2M 1+
σ2
d4 log T + B) times with probability of at
1−
least 1 − δ  where B is a constant independent of T .
Proof. By Theorem 1 of Gopalan et al. [13]  the number of recommendations j (cid:54)= j∗ is bounded by
C(log T ) + B  where B is a constant independent of T . Now we bound C(log T ) by counting the
number of times that PTS selects models that cannot be distinguished from (u∗  v∗) after observing
Yt under the optimal action j∗. Let:

j   σ2) for some u∗

Θj =(cid:8)(u  v) ∈ Θu × Θv : ∀i : uivj∗ = u∗

i v∗

j∗   vj ≥ maxk(cid:54)=j vk

be the set of such models where action j is optimal. Suppose that our algorithm chooses model
(u  v) ∈ Θj. Then the KL divergence between the distributions of ratings ri j under models (u  v)
and (u∗  v∗) is bounded from below as:

(cid:9)

∗
DKL(uivj (cid:107) u
i v

∗
j ) =

i v∗
(uivj − u∗
j )2

d4
2σ2 .

≥

2σ2

cause j∗ is uniquely optimal in (u∗  v∗). We know that(cid:12)(cid:12)uivj − u∗
for any i. The last inequality follows from the fact that uivj ≥ uivj∗ = u∗
i v∗
is(cid:80)n
ularity of our discretization is d. Let i1  . . .   in be any n row indices. Then the KL divergence
between the distributions of ratings in positions (i1  j)  . . .   (in  j) under models (u  v) and (u∗  v∗)
are unlikely to be chosen by PTS in T steps when(cid:80)n
t=1 DKL(uitvj (cid:107) u∗
itv∗
2σ2 . By Theorem 1 of Gopalan et al. [13]  the models (u  v) ∈ Θj
j ) ≥ log T . This happens
after at most n ≥ 2 1+
d4 log T selections of (u  v) ∈ Θj. Now we apply the same argument to all
1−
Θj  M − 1 in total  and sum up the corresponding regrets.

(cid:12)(cid:12) ≥ d2 because the gran-

itv∗
t=1 DKL(uitvj (cid:107) u∗

j ) ≥ n d4

j∗ > u∗

j   be-

i v∗

i v∗

σ2

j

5

σ2
Remarks: Note that Theorem 1 implies at O(2M 1+
d4 log T ) regret bound that holds with high
1−
probability. Here  d2 plays the role of a gap ∆  the smallest possible difference between the expected
ratings of item j (cid:54)= j∗ in any row i.
In this sense  our result is O((1/∆2) log T ) and is of a
similar magnitude as the results in Gopalan et al. [13]. While we restrict u∗  v∗
∈ (0  1]K×1 in
the proof  this does not affect the algorithm. In fact  the proof only focuses on high probability
events where the samples from the posterior are concentrated around the true parameters  and thus 
are within (0  1]K×1 as well. Extending our proof to the general setting is not trivial. In particular 
moving from discretized parameters to continuous space introduces the abovementioned ill behaved
posteriors. While increasing the value of K will violate the fact that the best item will be the same
for all users  which allowed us to eliminate N from the regret bound.
5 Experiments and Results
The goal of our experimental evaluation is twofold: (i) evaluate the PTS algorithm for making online
recommendations with respect to various baseline algorithms on several real-world datasets and (ii)
understand the qualitative performance and intuition of PTS.
5.1 Dataset description
We use a synthetic dataset and ﬁve real world datasets to evaluate our approach. The synthetic
dataset is generated as follows - At ﬁrst we generate the user and item latent features (U and V )
v) respectively. The true
of rank K by drawing from a Gaussian distribution N (0  σ2
rating matrix is then R∗ = U V T . We generate the observed rating matrix R from R∗ by adding
Gaussian noise N (0  σ2) to the true ratings. We use ﬁve real world datasets as follows: Movielens
100k  Movielens 1M  Yahoo Music4  Book crossing5 and EachMovie as shown in Table 1.

u) and N (0  σ2

# users
# items
# ratings

Movielens 100k Movielens 1M Yahoo Music Book crossing

943
1682
100k

6841
5644
90k
Table 1: Characteristics of the datasets used in our study

6040
3900
1M

15400
1000

311 704

EachMovie

36656
1621
2.58M

5.2 Baseline measures

There are no current approaches available that simultaneously learn both the user and item factors
by sampling from the posterior in a bandit setting. From the currently available algorithms  we
choose two kinds of baseline methods - one that sequentially updates the the posterior of the user
features only while ﬁxing the item features to a point estimate (ICF) and another that updates the
MAP estimates of user and item features via stochastic gradient descent (SGD-Eps). A key chal-
lenge in online algorithms is unbiased ofﬂine evaluation. One problem in the ofﬂine setting is the
partial information available about user feedback  i.e.  we only have information about the items
that the user rated. In our experiment  we restrict the recommendation space of all the algorithms
to recommend among the items that the user rated in the entire dataset which makes it possible to
empirically measure regret at every interaction. The baseline measures are as follows:
1) Random : At each iteration  we recommend a random movie to the user.
2) Most Popular : At each iteration  we recommend the most popular movie restricted to the movies
rated by the user on the dataset. Note that this is an unrealistically optimistic baseline for an online
algorithm as it is not possible to know the global popularity of the items beforehand.
3) ICF: The ICF algorithm [2] proceeds by ﬁrst estimating the user and item latent factors (U and
V ) on a initial training period and then for every interaction thereafter only updates the user features
(U) assuming the item features (V ) as ﬁxed. We run two scenarios for the ICF algorithm one in
which we use 20% (icf-20) and 50% (icf-50) of the data as the training period respectively. During
this period of training  we randomly recommend a movie to the user to compute the regret. We use
the PMF implementation by [5] for estimating the U and V .
4) SGD-Eps: We learn the latent factors using an online variant of the PMF algorithm [5]. We use
the stochastic gradient descent to update the latent factors with a mini-batch size of 50. In order
to make a recommendation  we use the -greedy strategy and recommend the highest UiV T with a
probability  and make a random recommendations otherwise. ( is set as 0.95 in our experiments.)

4http://webscope.sandbox.yahoo.com/
5http://www.bookcrossing.com

6

5.3 Results on Synthetic Dataset

We generated the synthetic dataset as mentioned earlier and run the PTS algorithm with 100 particles
for recommendations. We simulate the setting as mentioned in Section 3 and assume that at time t 
a random user it arrives and the system recommends an item jt. The user rates the recommended
item rit jt and we evaluate the performance of the model by computing the expected cumulative
regret deﬁned in Eq(6). Fig. 2 shows the cumulative regret of the algorithm on the synthetic data
averaged over 100 runs using different size of the matrix and latent features K. The cumulative
regret increases sub-linearly with the number of interactions and this gives us conﬁdence that our
approach works well on the synthetic dataset.

(a) N  M=10 K=1

(b) N  M=20 K=1

(c) N  M=30 K=1

(d) N  M=10 K=2

(e) N  M=10 K=3

Figure 2: Cumulative regret on different sizes of the synthetic data and K averaged over 100 runs.

5.4 Results on Real Datasets

(a) Movielens 100k

(b) Movielens 1M

(c) Yahoo Music

(d) Book Crossing

(e) EachMovie

Figure 3: Comparison with baseline methods on ﬁve datasets.

Next  we evaluate our algorithms on ﬁve real datasets and compare them to the various baseline
algorithms. We subtract the mean ratings from the data to centre it at zero. To simulate an extreme
cold-start scenario we start from an empty set of user and rating. We then iterate over the datasets
and assume that a random user it has arrived at time t and the system recommends an item jt
constrained to the items rated by this user in the dataset. We use K = 2 for all the algorithms and
use 30 particles for our approach. For PTS we set the value of σ2 = 0.5 and σ2
v = 1.
For PTS-B (Bayesian version  see Algo. 1 for more details)  we set σ2 = 0.5 and the initial shape
parameters of the Gamma distribution as α = 2 and β = 0.5. For both ICF-20 and ICF-50  we set
u = 1. Fig. 3 shows the cumulative regret of all the algorithms on the ﬁve datasets6.
σ2 = 0.5 and σ2
Our approach performs signiﬁcantly better as compared to the baseline algorithms on this diverse
set of datasets. PTS-B with no parameter tuning performs slightly better than PTS and achieves the
best regret. It is important to note that both PTS and PTS-B performs comparable to or even better
than the “most popular” baseline despite not knowing the global popularity in advance. Note that
ICF is very sensitive to the length of the initial training period; it is not clear how to set this apriori.

u = 1  σ2

6ICF-20 fails to run on the Bookcrossing dataset as the 20% data is too sparse for the PMF implementation.

7

0204060801000102030405060IterationsCummulative Regret0100200300400500020406080100120140160180200IterationsCummulative Regret02004006008001000050100150200250300350400450IterationsCummulative Regret020406080100020406080100120IterationsCummulative Regret020406080100020406080100120140IterationsCummulative Regret0246810x 104051015x 104IterationsCummulative Regret PTSrandompopularicf−20icf−50sgd−epsPTS−B024681012x 105051015x 105IterationsCummulative Regret PTSrandompopularicf−20icf−50sgd−epsPTS−B00.511.522.533.5x 1050123456x 105IterationsCummulative Regret PTSrandompopularicf−20icf−50sgd−epsPTS−B0246810x 104024681012141618x 104IterationsCummulative Regret PTSrandompopularicf−50sgd−epsPTS−B00.511.522.53x 1060123456x 106IterationsCummulative Regret PTSrandompopularicf−20icf−50sgd−epsPTS−B(a) Movielens 1M

(b) RB particle ﬁlter

(c) Movie feature vector

Figure 4: a) shows MSE on movielens 1M dataset  the red line is the MSE using the PMF algorithm
b) shows performance of a RBPF (blue line) as compared to vanilla PF (red line) on a synthetic
dataset N M=10 and c) shows movie feature vectors for a movie with 384 ratings  the red dot is the
feature vector from the ICF-20 algorithm (using 73 ratings). PTS-20 is the feature vector at 20% of
the data (green dots) and PTS-100 at 100% (blue dots).

We also evaluate the performance of our model in an ofﬂine setting as follows: We divide the
datasets into training and test set and iterate over the training data triplets (it  jt  rt) by pretending
that jt is the movie recommended by our approach and update the latent factors according to RBPF.
We compute the recovered matrix ˆR as the average prediction U V T from the particles at each time
step and compute the mean squared error (MSE) on the test dataset at each iteration. Unlike the
batch method such as PMF which takes multiple passes over the data  our method was designed to
have bounded update complexity at each iteration. We ran the algorithm using 80% data for training
and the rest for testing and computed the MSE by averaging the results over 5 runs. Fig. 4(a) shows
the average MSE on the movielens 1M dataset. Our MSE (0.7925) is comparable to the PMF MSE
(0.7718) as shown by the red line. This demonstrates that the RBPF is performing reasonably well
for matrix factorization. In addition  Fig. 4(b) shows that on the synthetic dataset  the vanilla PF
suffers from degeneration as seen by the high variance. To understand the intuition why ﬁxing the
latent item features V as done in the ICF does not work  we perform an experiment as follows: We
run the ICF algorithm on the movielens 100k dataset in which we use 20% of the data for training.
At this point the ICF algorithm ﬁxes the item features V and only updates the user features U. Next 
we run our algorithm and obtain the latent features. We examined the features for one selected movie
from the particles at two time intervals - one when the ICF algorithm ﬁxes them at 20% and another
one in the end as shown in the Fig. 4(c). It shows that movie features have evolved into a different
location and hence ﬁxing them early is not a good idea.
6 Related Work
Probabilistic matrix completion in a bandit setting setting was introduced in the previous work by
Zhao et al. [2]. The ICF algorithm in [2] approximates the posterior of the latent item features by
a single point estimate. Several other bandit algorithms for recommendations have been proposed.
Valko et al. [14] proposed a bandit algorithm for content-based recommendations. In this approach 
the features of the items are extracted from a similarity graph over the items  which is known in
advance. The preferences of each user for the features are learned independently by regressing the
ratings of the items from their features. The key difference in our approach is that we also learn
the features of the items. In other words  we learn both the user and item factors  U and V   while
[14] learn only U. Kocak et al. [15] combine the spectral bandit algorithm in [14] with TS. Gentile
et al. [16] propose a bandit algorithm for recommendations that clusters users in an online fashion
based on the similarity of their preferences. The preferences are learned by regressing the ratings of
the items from their features. The features of the items are the input of the learning algorithm and
they only learn U. Maillard et al. [17] study a bandit problem where the arms are partitioned into
unknown clusters unlike our work which is more general.
7 Conclusion
We have proposed an efﬁcient method for carrying out matrix factorization (M ≈ U V T ) in a bandit
setting. The key novelty of our approach is the combined use of Rao-Blackwellized particle ﬁltering
and Thompson sampling (PTS) in matrix factorization recommendation. This allows us to simul-
taneously update the posterior probability of U and V in an online manner while minimizing the
cumulative regret. The state of the art  till now  was to either use point estimates of U and V or use
a point estimate of one of the factor (e.g.  U) and update the posterior probability of the other (V ).
PTS results in substantially better performance on a wide variety of real world data sets.

8

−200020040060080010000.511.522.533.54Iterations x 1000MSE test errorpmf020406080100−2−101234IterationsMSE RBNo RB−0.4−0.200.20.40.6−0.8−0.6−0.4−0.200.20.40.6 ICF−20PTS−20PTS−100References
[1] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recom-

mender systems. Computer  42(8):30–37  2009.

[2] Xiaoxue Zhao  Weinan Zhang  and Jun Wang. Interactive collaborative ﬁltering. In Proceed-
ings of the 22nd ACM international conference on Conference on information & knowledge
management  pages 1411–1420. ACM  2013.

[3] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS 

pages 2249–2257  2011.

[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear

payoffs. In ICML (3)  pages 127–135  2013.

[5] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In NIPS  volume 1 

pages 2–1  2007.

[6] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using

markov chain monte carlo. In ICML  pages 880–887  2008.

[7] Nicolas Chopin. A sequential particle ﬁlter method for static models. Biometrika  89(3):539–

552  2002.

[8] Pierre Del Moral  Arnaud Doucet  and Ajay Jasra. Sequential monte carlo samplers. Journal

of the Royal Statistical Society: Series B (Statistical Methodology)  68(3):411–436  2006.

[9] Arnaud Doucet  Nando De Freitas  Kevin Murphy  and Stuart Russell. Rao-blackwellised
particle ﬁltering for dynamic bayesian networks. In Proceedings of the Sixteenth conference
on Uncertainty in artiﬁcial intelligence  pages 176–183. Morgan Kaufmann Publishers Inc. 
2000.

[10] A. Gelman and X. L Meng. A note on bivariate distributions that are conditionally normal.

Amer. Statist.  45:125–126  1991.

[11] B. C. Arnold  E. Castillo  J. M. Sarabia  and L. Gonzalez-Vega. Multiple modes in densities

with normal conditionals. Statist. Probab. Lett.  49:355–363  2000.

[12] B. C. Arnold  E. Castillo  and J. M. Sarabia. Conditionally speciﬁed distributions: An intro-

duction. Statistical Science  16(3):249–274  2001.

[13] Aditya Gopalan  Shie Mannor  and Yishay Mansour. Thompson sampling for complex online
problems. In Proceedings of The 31st International Conference on Machine Learning  pages
100–108  2014.

[14] Michal Valko  R´emi Munos  Branislav Kveton  and Tom´aˇs Koc´ak. Spectral bandits for smooth

graph functions. In 31th International Conference on Machine Learning  2014.

[15] Tom´aˇs Koc´ak  Michal Valko  R´emi Munos  and Shipra Agrawal. Spectral thompson sampling.

In Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence  2014.

[16] Claudio Gentile  Shuai Li  and Giovanni Zappella. Online clustering of bandits. arXiv preprint

arXiv:1401.8257  2014.

[17] Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31th In-
ternational Conference on Machine Learning  ICML 2014  Beijing  China  21-26 June 2014 
pages 136–144  2014.

9

,Aijun Bai
Feng Wu
Xiaoping Chen
Hsiao-Yu Tung
Alexander Smola
Jaya Kawale
Hung Bui
Branislav Kveton
Long Tran-Thanh
Sanjay Chawla
Stephan Rabanser
Stephan Günnemann
Zachary Lipton