2017,Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers,We study stochastic convex optimization subjected to linear equality constraints. Traditional Stochastic Alternating Direction Method of Multipliers and its Nesterov's acceleration scheme can only achieve ergodic O(1/\sqrt{K}) convergence rates  where K is the number of iteration. By introducing Variance Reduction (VR) techniques  the convergence rates improve to ergodic O(1/K). In this paper  we propose a new stochastic ADMM which elaborately integrates Nesterov's extrapolation and VR techniques. With Nesterov’s extrapolation  our algorithm can achieve a non-ergodic O(1/K) convergence rate which is optimal for separable linearly constrained non-smooth convex problems  while the convergence rates of VR based ADMM methods are actually tight O(1/\sqrt{K}) in non-ergodic sense. To the best of our knowledge  this is the first work that achieves a truly accelerated  stochastic convergence rate for constrained convex problems. The experimental results demonstrate that our algorithm is significantly faster than the existing state-of-the-art stochastic ADMM methods.,Faster and Non-ergodic O(1/K) Stochastic
Alternating Direction Method of Multipliers

Cong Fang

Feng Cheng

Zhouchen Lin∗

Key Laboratory of Machine Perception (MOE)  School of EECS  Peking University  P. R. China

Cooperative Medianet Innovation Center  Shanghai Jiao Tong University  P. R. China

fangcong@pku.edu.cn

fengcheng@pku.edu.cn

zlin@pku.edu.cn

Abstract

√

We study stochastic convex optimization subjected to linear equality constraints.
Traditional Stochastic Alternating Direction Method of Multipliers [1] and its Nes-
terov’s acceleration scheme [2] can only achieve ergodic O(1/
K) convergence
rates  where K is the number of iteration. By introducing Variance Reduction (VR)
techniques  the convergence rates improve to ergodic O(1/K) [3  4]. In this paper 
we propose a new stochastic ADMM which elaborately integrates Nesterov’s ex-
trapolation and VR techniques. With Nesterov’s extrapolation  our algorithm can
achieve a non-ergodic O(1/K) convergence rate which is optimal for separable
√
linearly constrained non-smooth convex problems  while the convergence rates of
VR based ADMM methods are actually tight O(1/
K) in non-ergodic sense. To
the best of our knowledge  this is the ﬁrst work that achieves a truly accelerated 
stochastic convergence rate for constrained convex problems. The experimental
results demonstrate that our algorithm is faster than the existing state-of-the-art
stochastic ADMM methods.

1

Introduction

We consider the following general convex ﬁnite-sum problem with linear constraints:

min
x1 x2

h1(x1) + f1(x1) + h2(x2) +

1
n

f2 i(x2) 

n(cid:88)

i=1

n

s.t.

A1x1 + A2x2 = b 

(cid:80)n
i=1 f2 i(x). And we use ∇f to denote the gradient of f.

(1)
where f1(x1) and f2 i(x2) with i ∈ {1  2 ···   n} are convex and have Lipschitz continuous gradients 
h1(x1) and h2(x2) are also convex  but can be non-smooth. We use the following notations:
L1 denotes the Lipschitz constant of f1(x1)  L2 is the Lipschitz constant of f2 i(x2) with i ∈
{1  2 ···   n}  and f2(x) = 1
Problem (1) is of great importance in machine learning. The ﬁnite-sum functions f2(x2) are typically
a loss over training samples  and the remaining functions control the structure or regularize the model
to aid generalization [2]. The idea of using linear constraints to decouple the loss and regularization
terms enables researchers to consider some more sophisticated regularization terms which might
be very complicated to solve through proximity operators for Gradient Descent [5] methods. For
example  for multitask learning problems [6  7]  the regularization term is set as µ1(cid:107)x(cid:107)∗ + µ2(cid:107)x(cid:107)1 
for most graph-guided fused Lasso and overlapping group Lasso problem [8  4]  the regularization
term can be written as µ(cid:107)Ax(cid:107)1  and for many multi-view learning tasks [9]  the regularization terms
always involve µ1(cid:107)x(cid:107)2 1 + µ2(cid:107)x(cid:107)∗.

∗Corresponding author.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Table 1: Convergence rates of ADMM type methods solving Problem (1).

Type

Batch

Stochastic

Algorithm
ADMM [13]

LADM-NE [15]
STOC-ADMM [1]
OPG-ADMM [16]
OPT-ADMM [2]

Convergence Rate
Tight non-ergodic O( 1√
)
K
Optimal non-ergodic O( 1
K )

ergodic O( 1√
ergodic O( 1√
ergodic O( 1√
unknown

)
)
)

K

K

K

SDCA-ADMM [17]
SAG-ADMM [3]
SVRG-ADMM [4]

Tight non-ergodic O( 1√
)
Tight non-ergodic O( 1√
)
K
ACC-SADMM (ours) Optimal non-ergodic O( 1
K )

K

√
K). They also construct a family of functions showing that non-ergodic O(1/

Alternating Direction Method of Multipliers (ADMM) is a very popular optimization method to solve
Problem (1)  with its advantages in speed  easy implementation and good scalability shown in lots of
literatures (see survey [10]). A popular criterion of the algorithms’ convergence rate is its ergodic
convergence. And it is proved in [11  12] that ADMM converges with an O(1/K) ergodic rate.
However  in this paper  it is noteworthy that we consider the convergence in the non-ergodic sense.
The reasons are two folded: 1) in real applications  the output of ADMM methods are non-ergodic
results (xK)  rather than the ergodic one (convex combination of x1  x2 ···   xK)  as the non-ergodic
results are much faster (see detailed discussions in Section 5.3); 2) The ergodic convergence rate
is not trivially the same as general-case’s rate. For a sequence {ak} = {1 −1  1 −1  1 −1 ···}
(When k is odd  ak is 1  and −1 when k is even)  it is divergent  while in ergodic sense  it converges
in O(1/K). So the analysis in the non-ergodic are closer to reality. 2) is especially suit for ADMM
√
methods. In [13]  Davis et al. prove that the Douglas-Rachford (DR) splitting converges in non-
√
ergodic O(1/
K) is
tight. Chen et al. establish O(1/
K) for Linearized ADMM [14]. Then Li et al. accelerate ADMM
through Nesterov’s extrapolation and obtain a non-ergodic O(1/K) convergence rate[15]. They also
prove that the lower complexity bound of ADMM type methods for the separable linearly constrained
nonsmooth convex problems is exactly O(1/K)  which demonstrates that their algorithm is optimal.
The convergence rates for different ADMM based algorithms are shown in Table 1.
On the other hand  to meet the demands of solving large-scale machine learning problems  stochastic
algorithms [18] have drawn a lot of interest in recent years. For stochastic ADMM (SADMM)  the
√
prior works are from STOC-ADMM [1] and OPG-ADMM [16]. Due to the noise of gradient  both of
the two algorithms can only achieve an ergodic O(1/
K) convergence rate. There are two lines of
research to accelerate SADMM. The ﬁrst is to introduce the Variance Reduction (VR) [19  20  21]
techniques into SADMM. VR methods ensure the descent direction to have a bounded variance
and so can achieve faster convergence rates. The existing VR based SADMM algorithms include
SDCA-ADMM [17]  SAG-ADMM [3] and SVRG-ADMM [4]. SAG-ADMM and SVRG-ADMM
can provably achieve ergodic O(1/K) rates for Porblem (1). The second way to accelerate SADMM
is through the Nesterov’s acceleration [22]. This work is from [2]  in which the authors propose
an ergodic O( R2
) stochastic algorithm (OPT-ADMM). The dependence on the
√
smoothness constant of the convergence rate is O(1/K 2) and so each term in the convergence rate
seems to have been improved to optimal. However  the worst convergence rate of it is still O(1/
K).
In this paper  we propose Accelerated Stochastic ADMM (ACC-SADMM) for large scale general con-
vex ﬁnite-sum problems with linear constraints. By elaborately integrating Nesterov’s extrapolation
and VR techniques  ACC-SADMM provably achieves a non-ergodic O(1/K) convergence rate which
√
is optimal for non-smooth problems. As in non-ergodic sense  the VR based SADMM methods (e.g.
K) (please see detailed discussions in
SVRG-ADMM  SAG-ADMM) converges in a tight O(1/
Section 5.3)  ACC-SADMM improve the convergence rates from O(1/
K) to (1/K) in the ergodic
sense and ﬁll the theoretical gap between the stochastic and batch (deterministic) ADMM. The
original idea to design our ACC-SADMM is by explicitly considering the snapshot vector ˜x (approxi-
mately the mean value of x in the last epoch) into the extrapolation terms. This is  to some degree 
inspired by [23] who proposes an O(1/K 2) stochastic gradient algorithm named Katyusha for convex

K2 + Dy+ρ

K + σ√

√

K

2

Table 2: Notations and Variables
√

Meaning

Notation

(cid:104)x  y(cid:105)G (cid:107)x(cid:107)G xT Gy 

Fi(xi)

x
y

xT Gx
hi(xi) + fi(xi)

(x1  x2)
(y1  y2)

F (x)

F1(x1) + F2(x2)

Variable
yk
s 1  yk
s 2
xk
s 1  xk
s 2
˜λk
s   λk
s
˜xs 1  ˜xs 2  ˜bs
x∗
2  λ∗
1  x∗

Meaning

extrapolation variables

primal variables

dual and temp variables

snapshot vectors

optimal solution of Eq. (1)

s = xk

s + (1− θ1 s − θ2)(xk

problems. However  there are many distinctions between the two algorithms (please see detailed
discussions in Section 5.1). Our method is also very efﬁcient in practice since we have sufﬁciently
considered the noise of gradient into our acceleration scheme. For example  we adopt extrapolation
as yk
) in the inner loop  where θ2 is a constant and θ1 s decreases
(xk − xk−1)
after every epoch  instead of directly adopting extrapolation as yk = xk + θk
in the original Nesterov’s scheme and adding proximal term
as [2] does. There are also
variants on updating of multiplier and the snapshot vector. We list the contributions of our work as
follows:

s − xk−1

(cid:107)xk+1−xk(cid:107)2

1 (1−θk−1

θk−1

σk3/2

s

)

1

1

• We propose ACC-SADMM for large scale convex ﬁnite-sum problems with linear constraints
which integrates Nesterov’s extrapolation and VR techniques. We prove that our algorithm
converges in non-ergodic O(1/K) which is optimal for separable linearly constrained non-
smooth convex problems. To our best knowledge  this is the ﬁrst work that achieves a truly
accelerated  stochastic convergence rate for constrained convex problems.
• We do experiments on four bench-mark datasets to demonstrate the superiority of our
algorithm. We also do experiments on the Multitask Learning [6] problem to demonstrate
that our algorithm can be used on very large datasets.

2 Preliminary

Most SADMM methods alternately minimize the following variant surrogate of the augmented
Lagrangian:

G1

β
2

(cid:48)
L

G2 +

(cid:107)A1x1 + A2x2 − b +

(x1  x2  λ  β) = h1(x1) + (cid:104)∇f1(x1)  x1(cid:105) +
(cid:107)x1 − xk
1(cid:107)2
L1
2
(cid:107)x2 − xk
+h2(x2) + (cid:104) ˜∇f2(x2)  x2(cid:105) +
2(cid:107)2
L2
2

(2)
(cid:107)2 
where ˜∇f2(x2) is an estimator of ∇f2(x2) from one or a mini-batch of training samples. So the
computation cost for each iteration reduces from O(n) to O(b) instead  where b is the mini-batch size.
When fi(x) = 0 and Gi = 0  with i = 1  2  Problem (1) is solved as exact ADMM. When there
is no hi(xi)  Gi is set as the identity matrix I  with i = 1  2  the subproblem in xi can be solved
through matrix inversion. This scheme is advocated in many SADMM methods [1  3]. Another
common approach is linearization (also called the inexact Uzawa method) [24  25]  where Gi is set
as ηiI − β
i Ai(cid:107).
(cid:88)
For STOC-ADMM [1]  ˜∇f2(x2) is simply set as:

i Ai with ηi ≥ 1 + β
AT

(cid:107)AT

λ
β

Li

Li

(3)

∇f2 ik (x2) 

1
b

ik∈Ik

where Ik is the mini-batch of size b from {1  2 ···   n}. For SVRG-ADMM [4]  the gradient
estimator can be written as:
˜∇f2(x2) =

(∇f2 ik (x2) − ∇f2 ik (˜x2)) + ∇f2(˜x2) 

(4)

˜∇f2(x2) =
(cid:88)

1
b

ik∈Ik

where ˜x2 is a snapshot vector (mean value of last epoch).

3

Algorithm 1 Inner loop of ACC-SADMM

for k = 0 to m − 1 do

(cid:16)

Update dual variable: λk
Update xk+1
s 1 through Eq. (6).
Update xk+1
s 2 through Eq. (7).
Update dual variable: ˜λk+1
Update yk+1
through Eq. (5).

s

s = ˜λk

s + βθ2
θ1 s

A1xk

s 1 + A2xk

s + β(cid:0)A1xk+1

s = λk

s 1 + A2xk+1

s 2 − ˜bs

(cid:17)
s 2 − b(cid:1) .

.

end for k.

3 Our Algorithm

3.1 ACC-SADMM

To help readers easier understand our algorithm  we list the notations and the variables in Table
2. Our algorithm has double loops as we use SVRG [19]  which also have two layers of nested
loops to estimate the gradient. We denote subscript s as the index of the outer loop and superscript
k as the index in the inner loops. For example  xk
s 1 is the value of x1 at the k-th step of the inner
iteration and the s-th step of the outer iteration. And we use xk
s 2)  and
(yk
s 1  yk
s 2  extrapolation
terms yk
s  and s remains unchanged. In the outer loop  we maintain
snapshot vectors ˜xs+1 1  ˜xs+1 2 and ˜bs+1  and then assign the initial value to the extrapolation
terms y0
s+1 2. We directly linearize both the smooth term fi(xi) and the augmented term
2(cid:107)A1x1 + A2x2 − b + λ
β

s 2)  respectively. In each inner loop  we update primal variables xk
s 1  yk

β (cid:107)2. The whole algorithm is shown in Algorithm 2.

s 2 and dual variable λk

s to denote (xk

s+1 1 and y0

s 1 and xk

s and yk

s 1  xk

3.2 Inner Loop

The inner loop of ACC-SAMM is straightforward  shown as Algorithm 1. In each iteration  we do
extrapolation  and then update the primal and dual variables. There are two critical steps which
ensures us to obtain a non-ergodic results. The ﬁrst is extrapolation. We do extrapolation as:

s + (1 − θ1 s − θ2)(xk+1

s − xk
s ) 

yk+1
s = xk+1

(5)
We can ﬁnd that 1 − θ1 s − θ2 ≤ 1 − θ1 s. So comparing with original Nesterov’s scheme  our way is
more “mild” to tackle the noise of gradient. The second step is on the updating primal variables.
xk+1

h1(x1) + (cid:104)∇f1(yk

s 1 = argmin

s 1)  x1(cid:105)

(6)

x1

(cid:0)A1yk

s 2 − b(cid:1) + λk

1 A1(cid:107)
β(cid:107)AT
2θ1 s
And then update x2 with the latest information of x1  which can be written as:

s   A1x1(cid:105) +

+(cid:104) β
θ1 s

s 1 + A2yk

2

(cid:107)x1 − yk

s 1(cid:107)2.

(cid:19)
s 2 − b(cid:1)

(7)

xk+1

s 2 = argmin

x2

h2(x2) + (cid:104) ˜∇f2(yk

(cid:32)

+λk

s   A2x2(cid:105) +

(1 + 1
bθ2
2

s 1)  x2(cid:105) + (cid:104) β
θ1 s
2 A2(cid:107)
β(cid:107)AT
2θ1 s

)L2

+

s 1 + A2yk

(cid:107)x2 − yk

s 2(cid:107)2 

+

(cid:18) L1
(cid:0)A1xk+1
(cid:33)

where ˜∇f2(yk

s 2) is obtained by the technique of SVRG [19] with the form:

(cid:88)

(cid:0)∇f2 ik s (yk

s 2) − ∇f2 ik s (˜xs 2) + ∇f2(˜xs 2)(cid:1) .

˜∇f2(yk

s 2) =

1
b

ik s∈I(k s)

Comparing with unaccelerated SADMM methods  which alternately minimize Eq. (2)  our method is
distincted in two ways. The ﬁrst is that the gradient estimator are computed on the yk
s 2. The second
is that we have chosen a slower increasing penalty factor β
  instead of a ﬁxed one.
θ1 s

4

Algorithm 2 ACC-SADMM
Input: epoch length m > 2  β  τ = 2  c = 2  x0
for s = 0 to S − 1 do

c+τ s  θ2 = m−τ
τ (m−1).

θ1 s = 1

0 = 0  ˜λ0

0 = 0  ˜x0 = x0

0  y0

0 = x0
0 

s+1 = xm
s .

Do inner loop  as stated in Algorithm 1.
Set primal variables: x0
Update snapshot vectors ˜xs+1 through Eq. (8).
Update dual variable:
Update dual snapshot variable:
Update extrapolation terms y0

s+1 = λm−1
˜λ0

s+1 through Eq. (9).

+ β(1 − τ )(A1xm

s
˜bs+1 = A1˜xs+1 1 + A2˜xs+1 2.

s 1 + A2xm

ˆxS =

1

(m − 1)(θ1 S + θ2) + 1

xm

S +

θ1 S + θ2

(m −1)(θ1 S + θ2) + 1

s 2 − b).
m−1(cid:88)

xk
S.

k=1

end for s.

Output:

3.3 Outer Loop

The outer loop of our algorithm is a little complex  in which we preserve snapshot vectors  and
then resets the initial value. The main variants we adpot is on the snapshot vector ˜xs+1 and the
extrapolation term y0

(cid:32)(cid:20)
1 − (τ − 1)θ1 s+1

s+1. For the snapshot vector ˜xs+1  we update it as:
(τ − 1)θ1 s+1
(m − 1)θ2

(cid:21)

(cid:20)

xm

θ2

1
m

(cid:21) m−1(cid:88)

˜xs+1 =

˜xs+1 is not the average of {xk
generating ˜x guarantees a faster convergence rate for the constraints. Then we reset y0

(8)
s}  different from most SVRG-based methods [19  4]. The way of

s +

1 +

k=1

.

xk
s

s+1 as:

(cid:33)

(cid:2)(1 − θ1 s)xm

s − (1 − θ1 s − θ2)xm−1

s

− θ2˜xs

(9)

(cid:3) .

s+1 = (1 − θ2)xm
y0

s + θ2˜xs+1 +

θ1 s+1
θ1 s

4 Convergence Analysis

In this section  we give the convergence results of ACC-SADMM. The proof and a outline can be
found in Supplementary Material. As we have mentioned in Section 3.2  the main strategy that enable
us to obtain a non-ergodic results is that we adopt extrapolation as Eq. (5). We ﬁrst analyze each
inner iteration  shown in Lemma 1. We ignore subscript s as s is unchanged in the inner iteration.
Lemma 1 Assume that f1(x1) and f2 i(x2) with i ∈ {1  2 ···   n} are convex and have Lipschitz
continuous gradients. L1 is the Lipschitz constant of f1(x1). L2 is the Lipschitz constant of f2 i(x2)
with i ∈ {1  2 ···   n} . h1(x1) and h2(x2) is also convex. For Algorithm 2  in any epoch  we have

)(cid:3) − θ2L(˜x1  ˜x2  λ∗
(cid:104)(cid:107)ˆλk+1 − λ∗(cid:107)2(cid:105)(cid:17)

2

1

  λ∗

  xk+1

(cid:2)L(xk+1
(cid:16)(cid:107)ˆλk − λ∗(cid:107)2 − Eik
(cid:0)(cid:107)xk+1
(cid:0)(cid:107)xk+1

Eik
(cid:107)yk

Eik

Eik
≤ θ1
2β
− 1
2
1
+
2
− 1
2

1 − (1 − θ1 − θ2)xk

2 − (1 − θ1 − θ2)xk

2 − θ2˜x2 − θ1x∗
2(cid:107)2

2 − (1 − θ1 − θ2)xk

+

1
2

) − (1 − θ2 − θ1)L(xk

1  xk
1 − (1 − θ1 − θ2)xk
(cid:107)yk
(cid:1)
1 − θ2˜x1 − θ1x∗
1(cid:107)2
(cid:1)  

2 − θ2˜x2 − θ1x∗
2(cid:107)2

G4

G3

G4

)

2  λ∗
1 − θ2˜x1 − θ1x∗
1(cid:107)2

G3

where Eik denotes that the expectation is taken over the random samples in the minibatch Ik s 
L(x1  x2  λ) = F1(x1) + F2(x2) + (cid:104)λ  A1x1 + A2x2 − b(cid:105) and ˆλk = ˜λk + β(1−θ1)
(Axk − b) 
G3 =

)L2 + β(cid:107)AT

L1 + β(cid:107)AT

I − βAT

  and G4 =

(cid:16)

(cid:16)

(cid:17)

(cid:17)

I.

θ1

(1 + 1
bθ2

2 A2(cid:107)
θ1

1 A1(cid:107)
θ1

1 A1
θ1

Then Theorem 1 analyses ACC-SADMM in the whole iteration  which is the key convergence result
of the paper.

5

Theorem 1 If the conditions in Lemma 1 hold  then we have

2β

(cid:18) 1
(cid:18) m
(cid:0)F (x0

θ1 S

E

+E

0 − b(cid:1) + ˜λ0
(cid:0)Ax0
(AˆxS−b) − β(m−1)θ2
(cid:19)
(cid:107) βm
θ1 S
(F (ˆxS) − F (x∗
0 − b(cid:105)(cid:1) +
0) − F (x∗
0 1 − x∗
1(cid:107)2
(θ1 0L1+(cid:107)AT

θ1 0
) + (cid:104)λ∗

  AˆxS − b(cid:105))

1 A1(cid:107))I−AT

) + (cid:104)λ∗

  Ax0

(cid:107)˜λ0

(cid:107)x0

1
2β
1
2

1 A1

+

(cid:107)x0

+

1
2

≤ C3

(cid:19)

(10)

0 − λ∗(cid:107)2

β(1 − θ1 0)
0 +
2(cid:107)2(cid:16)
0 2 − x∗

θ1 0

(1+ 1
bθ2

(Ax0

0 − b) − λ∗(cid:107)2
2 A2(cid:107)(cid:17)

 

I

)θ1 0L2+(cid:107)AT

where C3 = 1−θ1 0+(m−1)θ2

θ1 0

.

Corollary 1 directly demonstrates that ACC-SADMM have a non-ergodic O(1/K) convergence rate.
Corollary 1 If the conditions in Lemma 1 holds  we have

) 

E|F (ˆxS) − F (x∗

)| ≤ O(
E(cid:107)AˆxS − b(cid:107) ≤ O(

1
S
1
S
We can ﬁnd that ˆxS depends on the latest m information of xk
S. So our convergence results is in
non-ergodic sense  while the analysis for SVRG-ADMM [4] and SAG-ADMM [3] is in ergodic sense 
since they consider the point ˆxS = 1
s over
mS
all the iterations.
Now we directly use the theoretical results of [15] to demonstrate that our algorithm is optimal when
there exists non-smooth term in the objective function.
Theorem 2 For the following problem:

s  which is the convex combination of xk

(cid:80)m

(cid:80)S

k=1 xk

(11)

s=1

).

F1(x1) + F2(x2)  s.t. x1 − x2 = 0 

min
x1 x2

(12)

let the ADMM type algorithm to solve it be:

2 and yk

2 in any way 

(cid:17)

 

2 − λk
yk

2
βk

1 = ProxF1/βk

• Generate λk
• xk+1
• Generate λk+1
• xk+1

1

2 = ProxF2/βk

(cid:16)
(cid:16)

and yk+1

1

in any way 

1 − λk+1
yk+1

1
βk

(cid:17)

.

Then there exist convex functions F1 and F2 deﬁned on X = {x ∈ R6k+5 : (cid:107)x(cid:107) ≤ B} for the above
general ADMM method  satsifying
1(cid:107) + |F1(ˆxk

2)| ≥ LB

2) − F2(x∗

1) − F1(x∗

1) + F1(ˆxk

2 − ˆxk

L(cid:107)ˆxk

(13)

 

8(k + 1)

1 =(cid:80)k

2 =(cid:80)k

1xi

i=1 αi

1 and ˆxk

where ˆxk
Theorem 2 is Theorem 11 in [15]. More details can be found in it. Problem (12) is a special case of
Problem (1) as we can set each F2 i(x2) = F (x2) with i = 1 ···   n or set n = 1. So there is no
better ADMM type algorithm which converges faster than O(1/K) for Problem (1).

2 with i from 1 to k.

2 for any αi

1 and αi

i=1 αi

2xi

5 Discussions
We discuss some properties of ACC-SADMM and make further comparisons with some related
methods.

6

Table 3: Size of datasets and mini-batch size we adopt in the experiments

Problem

Dataset

Lasso

Multitask

a9a

covertype

mnist
dna

ImageNet

# training
72  876
290  506
60  000

2  400  000
1  281  167

# testing
72  875
290  506
10  000
600  000
50  000

# dimension × # class

74 × 2
54 × 2
784 × 10
800 × 2

4  096 × 1  000

# minibatch

100

500
2  000

5.1 Comparison with Katyusha

As we have mentioned in Introduction  some intuitions of our algorithm are inspired by Katyusha [23] 
which obtains an O(1/K 2) algorithm for convex ﬁnite-sum problems. However  Katyusha cannot
solve the problem with linear constraints. Besides  Katyusha uses the Nesterov’s second scheme
to accelerate the algorithm while our method conducts acceleration through Nesterov’s extrapola-
tion (Nesterov’s ﬁrst scheme). And our proof uses the technique of [26]  which is different from
[23]. Our algorithm can be easily extended to unconstrained convex ﬁnite-sum and can also obtain a
O(1/K 2) rate but belongs to the Nesterov’s ﬁrst scheme 2.

5.2 The Growth of Penalty Factor β
θ1 s

The penalty factor β
increases linearly with the iteration. One might deem that this make our
θ1 s
algorithm impractical because after dozens of epoches  the large value of penalty factor might slow
down the decrement of function value. However  we have not found any bad inﬂuence. There may
be two reasons 1. In our algorithm  θ1 s decreases after each epoch (m iterations)  which is much
slower than LADM-NE [15]. So the growth of penalty factor works as a continuation technique [28] 
which may help to decrease the function value. 2. From Theorem 1  our algorithm converges in
O(1/S) whenever θ1 s is large. So from the theoretical viewpoint  a large θ1 s cannot slow down
our algorithm. We ﬁnd that OPT-ADMM [2] also needs to decrease the step size with the iteration.
However  its step size decreasing rate is O(k

2 ) and is faster than ours.

3

5.3 The Importance of Non-ergodic O(1/K)

SAG-ADMM [3] and SVRG-ADMM [4] accelerate SADMM to ergodic O(1/K). In Theorem
√
9 of [15]  the authors generate a class of functions showing that the original ADMM has a tight
√
K) convergence rate. When n = 1  SAG-ADMM and SVRG-ADMM are the
non-ergodic O(1/
same as batch ADMM  so their convergence rates are no better than O(1/
K). So in non-ergodic
sense  our algorithm does have a faster convergence rate than VR based SADMM methods.
Then we are to highlight the importance of our non-ergodic result. As we have mentioned in the
Introduction  in practice  the output of ADMM methods is the non-ergodic result xK  not the mean
of x1 to xK. For deterministic ADMM  the proof of ergodic O(1/K) rate is proposed in [11]  after
ADMM had become a prevailing method of solving machine learning problems [29]; for stochastic
ADMM  e.g. SVRG-ADMM [4]  the authors give an ergodic O(1/K) proof  but in experiment  what
they emphasize to use is the mean value of the last epoch as the result. As the non-ergodic results
are more close to reality  our algorithm is much faster than VR based SADMM methods  even when
its rate is seemingly the same. Actually  though VR based SADMM methods have provably faster
rates than STOC-ADMM  the improvement in practice is evident only after numbers of iterations 
when point are close to the convergence point  rather than at the early stage. In both [3] and [4]  the
authors claim that SAG-ADMM and SVRG-ADMM are sensitive to initial points. We also ﬁnd that
if the step sizes are set based on the their theoretical guidances  sometimes they are even slower than
STOC-ADMM (see Fig. 1) as the early stage lasts longer when the step size is small. Our algorithm is
faster than the two algorithms which demonstrates that Nesterov’s extrapolation has truly accelerated
the speed and the integration of extrapolation and VR techniques is harmonious and complementary.

2We follow [26] to name the extrapolation scheme as Nesterov’s ﬁrst scheme and the three-step scheme [27]

as the Nesterov’s second scheme.

7

(a) a9a-original

(b) covertype-original

(c) mnist-original

(d) dna-original

(e) a9a-group

(f) covertype-group

(g) mnist-group

(h) dna-group

Figure 1: Experimental results of solving the original Lasso (Top) and Graph-Guided Fused Las-
so (Bottom). The computation time includes the cost of calculating full gradients for SVRG based
methods. SVRG-ADMM and SAG-ADMM are initialized by running STOC-ADMM for 3n
b itera-
tions. “-ERG” represents the ergodic results for the corresponding algorithms.

6 Experiments

(cid:80)n

We conduct experiments to show the effectiveness of our method3. We compare our method with the
following the-state-of-the-art SADMM algorithms: (1) STOC-ADMM [1]  (2) SVRG-ADMM [4] 
(3) OPT-SADMM [2]  (4) SAG-ADMM [3]. We ignore SDCA-ADMM [17] in our comparison since
it gives no analysis on general convex problems and it is also not faster than SVRG-ADMM [4].
Experiments are performed on Intel(R) CPU i7-4770 @ 3.40GHz machine with 16 GB memory. Our
experiments focus on two typical problems [4]: Lasso Problem and Multitask Learning. Due to space
limited  the experiment of Multitask Learning is shown in Supplementary Materials. For the Lasso
problems  we perform experiments under the following typical variations. The ﬁrst is the original
Lasso problem; and the second is Graph-Guided Fused Lasso model: minx µ(cid:107)Ax(cid:107)1 + 1
i=1 li(x) 
where li(x) is the logistic loss on sample i  and A = [G; I] is a matrix encoding the feature sparsity
pattern. G is the sparsity pattern of the graph obtained by sparse inverse covariance estimation [30].
The experiments are performed on four benchmark data sets: a9a  covertype  mnist and dna4. The
details of the dataset and the mini-batch size that we use in all SADMM are shown in Table 3. And
like [3] and [4]  we ﬁx µ = 10−5 and report the performance based on (xt  Axt) to satisfy the
constraints of ADMM. Results are averaged over ﬁve repetitions. And we set m = 2n
b for all the
algorithms. For original Lasso problem  the step sizes are set through theoretical guidances for
each algorithm. For the Graph-Guided Lasso  the best step sizes are obtained through searches on
parameters which give best convergence progress. Except ACC-SADMM  we use the continuation
technique [28] to accelerate algorithms. SAG-ADMM is performed on the ﬁrst three datasets due to
its large memory requirement.
The experimental results are shown in Fig. 1. We can ﬁnd that our algorithm consistently outperforms
other compared methods in all these datasets for both the two problems  which veriﬁes our theoretical
analysis. The details about parameter setting  experimental results where we set a larger ﬁxed step
size for the group guided Lasso problem  curves of the test error  the memory costs of all algorithms 
and Multitask learning experiment are shown in Supplementary Materials.

n

3The code will be available at http://www.cis.pku.edu.cn/faculty/vision/zlin/zlin.htm.
4a9a  covertype and dna are from: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ 

and mnist is from: http://yann.lecun.com/exdb/mnist/.

8

510152025303540number of effective passes 10-510-410-310-2objective gap510152025303540number of effective passes 10-310-2objective gap510152025303540number of effective passes 10-310-210-1objective gap510152025303540number of effective passes 10-410-3objective gap102030405060number of effective passes 10-310-210-1objective gap102030405060number of effective passes 10-510-410-310-210-1100objective gap102030405060number of effective passes 10-310-210-1100objective gap510152025303540number of effective passes 10-810-610-410-2100objective gap880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989FasterandNon-ergodicO(1/K)StochasticAlternatingDirectionMethodofMultipliers510152025303540number of effective passes 10-510-410-310-2objective gapSTOC-ADMMSTOC-ADMM-ERGOPT-ADMMSVRG-ADMMSVRG-ADMM-ERGSAG-ADMMSAG-ADMM-ERGACC-SADMM510152025303540number of effective passes 0.1120.1140.1160.1180.12test loss510152025303540number of effective passes 10-310-2objective gap510152025303540number of effective passes 0.370.3720.3740.376test loss510152025303540number of effective passes 10-310-210-1objective gap510152025303540number of effective passes 0.190.1950.20.2050.21test loss510152025303540number of effective passes 10-410-3objective gap(a) a9a(b) covertype(b) mnist(d) dna510152025303540number of effective passes 2.833.23.43.63.8test loss×10-3Figure3.Illustrationoftheproposedapproach.TheevolutionaryprocessofourPDE(solidarrow)withrespecttothetime(t=0 T/N ··· T )extractsthefeaturefromtheimageandthegradientdescentprocess(hollowarrow)learnsatransformtorepresentthefeature.Frostig Roy Ge Rong Kakade Sham andSidford Aaron.Un-regularizing:approximateproximalpointandfasterstochasticalgorithmsforempiricalriskmin-imization.InProc.Int’l.Conf.onMachineLearning 2015.He BingshengandYuan Xiaoming.OntheO(1/n)con-vergencerateofthedouglas–rachfordalternatingdirec-tionmethod.SIAMJournalonNumericalAnalysis 50(2):700–709 2012.Hien LeThiKhanh Lu Canyi Xu Huan andFeng Ji-ashi.Acceleratedstochasticmirrordescentalgorithmsforcompositenon-stronglyconvexoptimization.arXivpreprintarXiv:1605.06892 2016.Johnson RieandZhang Tong.Acceleratingstochasticgradientdescentusingpredictivevariancereduction.InProc.Conf.AdvancesinNeuralInformationProcessingSystems 2013.Kim Seyoung Sohn Kyung-Ah andXing EricP.Amul-tivariateregressionapproachtoassociationanalysisofaquantitativetraitnetwork.Bioinformatics 25(12):i204–i212 2009.Li HuanandLin Zhouchen.OptimalnonergodicO(1/k)convergencerate:WhenlinearizedADMmeetsnes-terov’sextrapolation.arXivpreprintarXiv:1608.06366 2016.Lin Hongzhou Mairal Julien andHarchaoui Zaid.Auniversalcatalystforﬁrst-orderoptimization.InProc.Conf.AdvancesinNeuralInformationProcessingSys-tems 2015a.Lin Zhouchen Liu Risheng andSu Zhixun.Linearizedalternatingdirectionmethodwithadaptivepenaltyforlow-rankrepresentation.InProc.Conf.AdvancesinNeuralInformationProcessingSystems 2011.Lin Zhouchen Liu Risheng andLi Huan.Linearizedalternatingdirectionmethodwithparallelsplittingandadaptivepenaltyforseparableconvexprogramsinma-chinelearning.MachineLearning 99(2):287–325 2015b.Lu Canyi Li Huan Lin Zhouchen andYan Shuicheng.Fastproximallinearizedalternatingdirectionmethodofmultiplierwithparallelsplitting.arXivpreprintarX-iv:1511.05133 2015.Nesterov Yurii.Amethodforunconstrainedconvexmini-mizationproblemwiththerateofconvergenceO(1/k2).InDokladyanSSSR volume269 pp.543–547 1983.Nesterov Yurii.Onanapproachtotheconstructionofop-timalmethodsofminimizationofsmoothconvexfunc-tions.EkonomikaiMateaticheskieMetody 24(3):509–517 1988.Nesterov Yurii.Introductorylecturesonconvexoptimiza-tion:Abasiccourse volume87.2013.Nitanda Atsushi.Stochasticproximalgradientdescentwithaccelerationtechniques.InProc.Conf.AdvancesinNeuralInformationProcessingSystems 2014.Ouyang Hua He Niao Tran Long andGray Alexan-derG.Stochasticalternatingdirectionmethodofmulti-pliers.Proc.Int’l.Conf.onMachineLearning 2013.7 Conclusion
We propose ACC-SADMM for the general convex ﬁnite-sum problems. ACC-SADMM integrates
Nesterov’s extrapolation and VR techniques and achieves a non-ergodic O(1/K) convergence rate 
which shows theoretical and practical importance. We do experiments to demonstrate that our
algorithm is faster than other SADMM methods.

Acknowledgment

Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no.
2015CB352502) and National Natural Science Foundation (NSF) of China (grant no.s 61625301 
61731018  and 61231002).

References
[1] Hua Ouyang  Niao He  Long Tran  and Alexander G Gray. Stochastic alternating direction

method of multipliers. Proc. Int’l. Conf. on Machine Learning  2013.

[2] Samaneh AzadiSra and Suvrit Sra. Towards an optimal stochastic alternating direction method

of multipliers. In Proc. Int’l. Conf. on Machine Learning  2014.

[3] Wenliang Zhong and James Tin-Yau Kwok. Fast stochastic alternating direction method of

multipliers. In Proc. Int’l. Conf. on Machine Learning  2014.

[4] Shuai Zheng and James T Kwok. Fast-and-light stochastic admm. In Proc. Int’l. Joint Conf. on

Artiﬁcial Intelligence  2016.

[5] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Multi-task feature learning.

Proc. Conf. Advances in Neural Information Processing Systems  2007.

[7] Li Shen  Gang Sun  Zhouchen Lin  Qingming Huang  and Enhua Wu. Adaptive sharing for

image classiﬁcation. In Proc. Int’l. Joint Conf. on Artiﬁcial Intelligence  2015.

[8] Seyoung Kim  Kyung-Ah Sohn  and Eric P Xing. A multivariate regression approach to

association analysis of a quantitative trait network. Bioinformatics  25(12):i204–i212  2009.

[9] Kaiye Wang  Ran He  Liang Wang  Wei Wang  and Tieniu Tan. Joint feature selection and
subspace learning for cross-modal retrieval. IEEE Trans. on Pattern Analysis and Machine
Intelligence  38(10):1–1  2016.

[10] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine Learning  3(1):1–122  2011.

[11] Bingsheng He and Xiaoming Yuan. On the O(1/n) convergence rate of the Douglas–Rachford

alternating direction method. SIAM Journal on Numerical Analysis  50(2):700–709  2012.

[12] Zhouchen Lin  Risheng Liu  and Huan Li. Linearized alternating direction method with parallel
splitting and adaptive penalty for separable convex programs in machine learning. Machine
Learning  99(2):287–325  2015.

[13] Damek Davis and Wotao Yin. Convergence rate analysis of several splitting schemes. In
Splitting Methods in Communication  Imaging  Science  and Engineering  pages 115–163. 2016.

[14] Caihua Chen  Raymond H Chan  Shiqian Ma  and Junfeng Yang. Inertial proximal ADMM
for linearly constrained separable convex optimization. SIAM Journal on Imaging Sciences 
8(4):2239–2267  2015.

[15] Huan Li and Zhouchen Lin. Optimal nonergodic O(1/k) convergence rate: When linearized

ADM meets nesterov’s extrapolation. arXiv preprint arXiv:1608.06366  2016.

9

[16] Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction

multiplier method. In Proc. Int’l. Conf. on Machine Learning  2013.

[17] Taiji Suzuki. Stochastic dual coordinate ascent with alternating direction method of multipliers.

In Proc. Int’l. Conf. on Machine Learning  2014.

[18] Léon Bottou. Stochastic learning. In Advanced lectures on machine learning  pages 146–168.

2004.

[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Proc. Conf. Advances in Neural Information Processing Systems  2013.

[20] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Proc. Conf. Advances in
Neural Information Processing Systems  2014.

[21] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming  pages 1–30  2013.

[22] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of

convergence O(1/k2). In Doklady an SSSR  volume 269  pages 543–547  1983.

[23] Zeyuan Allen-Zhu. Katyusha: The ﬁrst truly accelerated stochastic gradient method. In Annual

Symposium on the Theory of Computing  2017.

[24] Zhouchen Lin  Risheng Liu  and Zhixun Su. Linearized alternating direction method with
adaptive penalty for low-rank representation. In Proc. Conf. Advances in Neural Information
Processing Systems  2011.

[25] Xiaoqun Zhang  Martin Burger  and Stanley Osher. A uniﬁed primal-dual algorithm framework

based on bregman iteration. Journal of Scientiﬁc Computing  46:20–46  2011.

[26] Paul Tseng. On accelerated proximal gradient methods for convex-concave optimization. In

Technical report  2008.

[27] Yurii Nesterov. On an approach to the construction of optimal methods of minimization of

smooth convex functions. Ekonomika i Mateaticheskie Metody  24(3):509–517  1988.

[28] Wangmeng Zuo and Zhouchen Lin. A generalized accelerated proximal gradient approach for
total variation-based image restoration. IEEE Trans. on Image Processing  20(10):2748  2011.

[29] Zhouchen Lin  Minming Chen  and Yi Ma. The augmented lagrange multiplier method for

exact recovery of corrupted low-rank matrices. arXiv preprint arXiv:1009.5055  2010.

[30] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. Sparse inverse covariance estimation

with the graphical lasso. Biostatistics  9(3):432–441  2008.

10

,Cong Fang
Feng Cheng
Zhouchen Lin
Quoc Tran Dinh