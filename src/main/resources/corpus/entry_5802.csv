2014,Unsupervised learning of an efficient short-term memory network,Learning in recurrent neural networks has been a topic fraught with difficulties and problems. We here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal. Specifically  we show how these networks can learn to efficiently represent their present and past inputs  based on local learning rules only. Our results are based on several key insights. First  we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where  on average  feedforward signal inputs are canceled by recurrent inputs. We show that this learning rule minimizes a cost function. Second  we develop a local learning rule for the feedforward weights that  based on networks in which recurrent inputs already predict feedforward inputs  further minimizes the cost. Third  we show how the learning rules can be modified such that the network can directly encode non-whitened inputs. Fourth  we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself. As a consequence  the network starts to efficiently represent both its signal inputs and their history. We develop our main theory for linear networks  but then sketch how the learning rules could be transferred to balanced  spiking networks.,Unsupervised learning of an efﬁcient short-term

memory network

Pietro Vertechi∗

Wieland Brendel∗†

Christian K. Machens

Champalimaud Neuroscience Programme
Champalimaud Centre for the Unknown

Lisbon  Portugal

first.last@neuro.fchampalimaud.org

Abstract

Learning in recurrent neural networks has been a topic fraught with difﬁculties
and problems. We here report substantial progress in the unsupervised learning of
recurrent networks that can keep track of an input signal. Speciﬁcally  we show
how these networks can learn to efﬁciently represent their present and past inputs 
based on local learning rules only. Our results are based on several key insights.
First  we develop a local learning rule for the recurrent weights whose main aim
is to drive the network into a regime where  on average  feedforward signal inputs
are canceled by recurrent inputs. We show that this learning rule minimizes a cost
function. Second  we develop a local learning rule for the feedforward weights
that  based on networks in which recurrent inputs already predict feedforward
inputs  further minimizes the cost. Third  we show how the learning rules can be
modiﬁed such that the network can directly encode non-whitened inputs. Fourth 
we show that these learning rules can also be applied to a network that feeds a
time-delayed version of the network output back into itself. As a consequence 
the network starts to efﬁciently represent both its signal inputs and their history.
We develop our main theory for linear networks  but then sketch how the learning
rules could be transferred to balanced  spiking networks.

1

Introduction

Many brain circuits are known to maintain information over short periods of time in the ﬁring of
their neurons [15]. Such “persistent activity” is likely to arise through reverberation of activity due
to recurrent synapses. While many recurrent network models have been designed that remain active
after transient stimulation  such as hand-designed attractor networks [21  14] or randomly generated
reservoir networks [10  13]  how neural networks can learn to remain active is less well understood.
The problem of learning to remember the input history has mostly been addressed in supervised
learning of recurrent networks. The classical approaches are based on backpropagation through
time [22  6]. However  apart from convergence issues  backpropagation through time is not a fea-
sible method for biological systems. More recent work has drawn attention to random recurrent
neural networks  which already provide a reservoir of time constants that allow to store and read out
memories [10  13]. Several studies have focused on the question of how to optimize such networks
to the task at hand (see [12] for a review)  however  the generality of the underlying learning rules
is often not fully understood  since many rules are not based on analytical results or convergence
proofs.

∗These authors contributed equally.
†Current address: Centre for Integrative Neuroscience  University of T¨ubingen  Germany

1

The unsupervised learning of short-term memory systems  on the other hand  is largely unchartered
territory. While there have been several “bottom-up” studies that use biologically realistic learning
rules and simulations (see e.g.
[11])  we are not aware of any analytical results based on local
learning rules.
Here we report substantial progress in following through a normative  “top-down” approach that
results in a recurrent neural network with local synaptic plasticity. This network learns how to
efﬁciently remember an input and its history. The learning rules are largely Hebbian or covariance-
based  but separate recurrent and feedforward inputs. Based on recent progress in deriving integrate-
and-ﬁre neurons from optimality principles [3  4]  we furthermore sketch how an equivalent spiking
network with local learning rules could be derived. Our approach generalizes analogous work in the
setting of efﬁcient coding of an instantaneous signal  as developed in [16  19  23  4  1].

2 The autoencoder revisited

We start by recapitulating the autoencoder network shown in Fig. 1a. The autoencoder transforms a
K-dimensional input signal  x  into a set of N ﬁring rates  r  while obeying two constraints. First 
the input signal should be reconstructable from the output ﬁring rates. A common assumption is that
the input can be recovered through a linear decoder  D  so that

x ≈ ˆx = Dr.

(1)

Second  the output ﬁring rates  r  should provide an optimal or efﬁcient representation of the input
signals. This optimality can be measured by deﬁning a cost C(r) for the representation r. For
simplicity  we will in the following assume that the costs are quadratic (L2)  although linear (L1)
costs in the ﬁring rates could easily be accounted for as well. We note that autoencoder networks
are sometimes assumed to reduce the dimensionality of the input (undercomplete case  N < K) and
sometimes assumed to increase the dimensionality (overcomplete case  N > K). Our results apply
to both cases.
The optimal set of ﬁring rates for a given input signal can then be found by minimizing the loss
function 

L =

1
2 (cid:107)x − Dr(cid:107)2 +

µ
2 (cid:107)r(cid:107)2  

(2)

with respect to the ﬁring rates r. Here  the ﬁrst term is the error between the reconstructed input
signal  ˆx = Dr  and the actual stimulus  x  while the second term corresponds to the “cost” of
the signal representation. The minimization can be carried out via gradient descent  resulting in the
differential equation

˙r = −

∂L
∂r

= −µr + D(cid:62)x − D(cid:62)Dr.

(3)

This differential equation can be interpreted as a neural network with a ‘leak’  −µr  feedforward
connections  F = DT   and recurrent connections  Ω = D(cid:62)D. The derivation of neural networks
from quadratic loss functions was ﬁrst introduced by Hopﬁeld [7  8]  and the link to the autoencoder
was pointed out in [19]. Here  we have chosen a quadratic cost term which results in a linear
differential equation. Depending on the precise nature of the cost term  one can also obtain non-
linear differential equations  such as the Cowan-Wilson equations [19  8]. Here  we will ﬁrst focus
on linear networks  in which case ‘ﬁring rates’ can be both positive and negative. Further below 
we will also show how our results can be generalized to networks with positive ﬁring rates and to
networks in which neurons spike.
In the case of arbitrarily small costs  the network can be understood as implementing predictive
coding [17]. The reconstructed (“predicted”) input signal  ˆx = Dr  is subtracted from the actual
input signal  x  see Fig. 1b. Predictive coding here enforces a cancellation or ‘balance’ between the
feedforward and recurrent synaptic inputs. If we assume that the actual input acts excitatory  for
instance  then the predicted input is mediated through recurrent lateral inhibition. Recent work has
shown that this cancellation can be mediated by the detailed balance of currents in spiking networks
[3  1]  a result we will return to later on.

2

Figure 1: Autoencoders. (a) Feedforward network. The input signal x is multiplied with the feedforward
weights F. The network generates output ﬁring rates r. (b) Recurrent network. The left panel shows how the
reconstructed input signal ˆx = Dr is fed back and subtracted from the original input signal x. The right panel
shows that this subtraction can also be performed through recurrent connections FD. For the optimal network 
we set F = D(cid:62). (c) Recurrent network with delayed feedback. Here  the output ﬁring rates are fed back
with a delay. This delayed feedback acts as just another input signal  and is thereby re-used  thus generating
short-term memory.

3 Unsupervised learning of the autoencoder with local learning rules

The transformation of the input signal  x  into the output ﬁring rate  r  is largely governed by the
decoder  D  as can be seen in Eq. (3). When the inputs are drawn from a particular distribution  p(x) 
such as the distribution of natural images or natural sounds  some decoders will lead to a smaller
average loss and better performance. The average loss is given by

(cid:10)

(cid:107)x − Dr(cid:107)2 + µ(cid:107)r(cid:107)2(cid:11)

(cid:104)L(cid:105) =

1
2

(4)

where the angular brackets denote an average over many signal presentations. In practice  x will
generally be centered and whitened. While it is straightforward to minimize this average loss with
respect to the decoder  D  biological networks face a different problem.1 A general recurrent neural
network is governed by the ﬁring rate dynamics

(5)
and has therefore no access to the decoder  D  but only to to its feedforward weights  F  and its
recurrent weights  Ω. Furthermore  any change in F and Ω must solely rely on information that is
locally available to each synapse. We will assume that matrix Ω is initially chosen such that the
dynamical system is stable  in which case its equilibrium state is given by

˙r = −µr + Fx − Ωr 

Fx = Ωr + µr.

(6)
If the dynamics of the input signal x are slow compared to the rate dynamics of the autoencoder 
the network will generally operate close to equilibrium. We will assume that this is the case  an
assumption that provides a bridge from ﬁring rate networks to spiking networks  as explained below.
A priori  it is not clear how to change the feedforward weights  F  or the recurrent weights  Ω  since
neither appears in the average loss function  Eq. (4). We might be inclined to solve Eq. (6) for r
and plug the result into Eq. (4). However  we then have to operate on matrix inverses  the resulting
gradients imply heavily non-local synaptic operations  and we would still need to somehow eliminate
the decoder  D  from the picture.
Here  we follow a different approach. We note that the optimal target network in the previous section
implements a form of predictive coding. We therefore suggest a two-step approach to the learning
problem. First  we ﬁx the feedforward weights and we set up learning rules for the recurrent weights
such that the network moves into a regime where the inputs  Fx  are predicted or ‘balanced’ by the
recurrent weights  Ωr  see Fig. 1b.
In this case  Ω = FD  and this will be our ﬁrst target for
learning. Second  once Ω is learnt  we change the feedforward weights F to decrease the average
loss even further. We then return to step 1 and iterate.
Since F is assumed constant in step 1  we can reach the target Ω = FD by investigating how the
decoder D needs to change. The respective learning equation for D can then be translated into a

1Note that minimization of the average loss with respect to D requires either a hard or a soft normalization

constraint on D.

3

--abcˆx=DrxrxrFxF(xˆx)xFxFDrrxtrtDrt-rt1Mrtlearning equation for Ω  which will directly link the learning of Ω to the minimization of the loss
function  Eq. (4).
On an intuitive level  a small change of D in the direction ∆D = xr(cid:62) shifts the signal estimate
ˆx into the direction of the signal  ˆx → ˆx + (cid:107)r(cid:107)2 x  and thereby decreases the reconstruction error
(as generally (cid:107)ˆx(cid:107)2 < (cid:107)x(cid:107)2 due to the regularization). Such a change translates into the following
learning rule for D 
(7)
where  is sufﬁciently small to make the learning slow compared to the dynamics of the input signals
x = x(t). The ‘weight decay’ term  −αD  acts as a soft normalization on D. In turn  to have the
recurrent weights Ω move towards FD  we multiply with F from the left to obtain the learning rule2

˙D = (xr(cid:62) − αD) 

˙Ω = (Fxr(cid:62) − αΩ).

(8)
Importantly  this learning rule is completely local: it only rests on information that is available to
each synapse  namely the presynaptic ﬁring rates  r  and the postsynaptic input signal  Fx. The
recurrent weights adapt in a Hebbian type manner by matching strong inputs with strong recurrent
drives  and thereby learn to ‘predict’ or ‘balance’ the feedforward input.
In step 2  we assume that the recurrent weights have reached their target  Ω = FD  and we learn
the feedforward weights. For that we notice that in the absolute minimum  as shown in the previous
section  the feedforward weights become F = D(cid:62). Hence  the target for the feedforward weights
should be the transpose of the decoder. Over long time intervals  the expected decoder is simply
D = (cid:104)xr(cid:62)(cid:105)/α  since that is the ﬁxed point of the decoder learning rule  Eq. (7). Hence  we learn
the feedforward weights on a yet slower time scale β (cid:28)   according to
(9)
where λF is once more a soft normalization factor. The ﬁxed point of the learning rule is then
F = D(cid:62). We emphasize that this learning rule is also local  based solely on the presynaptic input
signal and postsynaptic ﬁring rates.
The learning rules for F and Ω ensure that the ﬁxed points of the network dynamics correspond
to the optimal topology  Eq. (3). The network dynamics is then fully determined by the decoder
D  and so is the reconstruction error  Eq. (4). To understand the structure of the decoder  we note

from Eq. (7) that the decoder will converge to(cid:10)xr(cid:62)(cid:11). In the equilibrium state  Eq. (6)  the neural
where we used that(cid:10)xx(cid:62)(cid:11) = I. Multiplying with Ω + µI from the right and replacing F and Ω

responses can be replaced by a linear transform of the input signal  and so

(cid:10)xr(cid:62)(cid:11) = F(cid:62) (Ω + µI)−1  

αD →

˙F = β(rx(cid:62) − λF) 

(10)

with their ﬁxed points yields a condition for the ﬁxed points of the decoder 

αDD(cid:62)D = (1 − αµ)D.

(11)
If D is full rank  this condition is fulﬁlled if and only if D is a scaled orthogonal matrix. In other
words  the (white) inputs are projected onto orthogonal axes of the population  which is optimal in
terms of minimizing signal interference and reconstruction error. If D is not full rank  then the ﬁxed
points are unstable  as we show in the supplementary material.
In summary  we note that the autoencoder operates on four separate time scales. On a very fast 
almost instantaneous time scale  the ﬁring rates run into equilibrium for a given input signal  Eq. (6).
On a slower time scale  the input signal  x  changes. On a yet slower time scale  the recurrent
weights  Ω  are learnt  and their learning therefore uses many input signal values. On the ﬁnal and
slowest time scale  the feedforward weights  F  are optimized.

4 Unsupervised learning for non-whitened inputs

Algorithms for efﬁcient coding are generally applied to whitened and centered data (see e.g. [2  16]).
Indeed  if the data are not centered  the read-out of the neurons will concentrate in the direction of
2Note that the ﬁxed point of the decoder learning rule is D = (cid:104)xr(cid:62)(cid:105)/α. Hence  the ﬁxed point of the

recurrent learning is Ω = FD.

4

(cid:0)xc − Drc

the mean input signal in order to represent it  even though the mean may not carry any relevant infor-
mation about the actual  time-varying signal. If the data are not whitened  the choice of the decoder
will be dominated by second-order statistical dependencies  at the cost of representing higher-order
dependencies. The latter are often more interesting to represent  as shown by applications of efﬁcient
or sparse coding algorithms to the visual system [20].
While whitening and centering are therefore common pre-processing steps  we note that  with a
simple correction  our autoencoder network can take care of the pre-processing steps autonomously.
This extra step will be crucial later on  when we feed the time-delayed (and non-whitened) network
activity back into the network. The main idea is simple: we suggest to use a cost function that is
invariant under afﬁne transformations and equals the cost function we have been using until now
in case of centered and whitened data. To do so  we introduce the short-hands xc = x − (cid:104)x(cid:105) and
rc = r − (cid:104)r(cid:105) for the centered input and the centered ﬁring rates  and we write Cx = cov(x  x) for
the covariance matrix of the input signal. The corrected loss function is then 
µ
2 (cid:107)r(cid:107)2 .

(12)
The loss function reduces to Eq. (2) if the data are centered and if Cx = I. Furthermore  the value
of the loss function remains constant if we apply any afﬁne transformation x → Ax + b.3 In turn 
we can interpret the loss function as the likelihood function of a Gaussian.
From hereon  we can follow through exactly the same derivations as in the previous sections. We
ﬁrst notice that the optimal ﬁring rate dynamics becomes
x x − D(cid:62)C−1

(13)
(14)
where V is a placeholder for the overall input. The dynamics differ in two ways from those in
Eq. (3). First  the dynamics now require the subtraction of the averaged input  (cid:104)V(cid:105). Biophysically 
this subtraction could correspond to a slower intracellular process  such as adaptation through hy-
x   and the optimal
perpolarization. Second  the optimal feedforward weights are now F = D(cid:62)C−1
recurrent weights become Ω = D(cid:62)C−1
The derivation of the learning rules follows the outline of the previous section. Initially  the network
starts with some random connectivity  and obeys the dynamical equations 

(cid:0)xc − Drc

V = D(cid:62)C−1
˙r = V − (cid:104)V(cid:105) 

(cid:1)(cid:62)C−1

x Dr − µr 

(cid:1) +

x D.

x

L =

1
2

V = Fx − (Ω + µI)r 
˙r = V − (cid:104)V(cid:105).

˙D = (cid:0)xr(cid:62) − (cid:104)x(cid:105)(cid:104)r(cid:105)(cid:62) − αD(cid:1) 
˙Ω = (cid:0)Fxr(cid:62) − (cid:104)Fx(cid:105)(cid:104)r(cid:105)(cid:62) − αΩ(cid:1).

(15)
(16)

We then apply the following modiﬁed learning rules for D and Ω 

(17)
(18)
We note that in both cases  the learning remains local. However  similar to the rate dynamics  the
dynamics of learning now requires a slower synaptic process that computes the averaged signal
inputs and presynaptic ﬁring rates. Synapses are well-known to operate on a large range of time
scales (e.g.  [5])  so that such slower processes are in broad agreement with physiology.
x . The matrix inverse can be
The target for learning the feedforward weights becomes F → D(cid:62)C−1
eliminated by noticing that the differential equation ˙F = (−FCx + D(cid:62)) has the required target
as its ﬁxed point. The covariance matrix Cx can be estimated by averaging over xcx(cid:62)c   and the
(cid:17)
decoder D(cid:62) can be estimated by averaging over xcr(cid:62)c   just as in the previous section  or as follows
from Eq. (17). Hence  the learning of the feedforward weights becomes
(r − λFx)x(cid:62) − (cid:104)r − λFx(cid:105)(cid:104)x(cid:62)(cid:105)

(19)
As for the recurrent weights  the learning rests on local information  but requires a slower time scale
that computes the mean input signal and presynaptic ﬁring rates.
Under these synaptic dynamics all stable ﬁxed points of the decoder are such that the input is ﬁrst
whitened before being projected onto orthogonal axes of the population. Again  these ﬁxed points
are optimal in minimizing the reconstruction error.

3Under an afﬁne transformation  y = Ax+b and ˆy = Aˆx+b  we obtain:(cid:0)y−ˆy(cid:1)(cid:62)
(cid:0)Ax − Aˆx(cid:1)(cid:62)

cov(Ax  Ax)−1(cid:0)Ax − Aˆx(cid:1) =(cid:0)x − ˆx(cid:1)(cid:62)

cov(y  y)−1(cid:0)y−ˆy(cid:1) =

cov(x  x)−1(cid:0)x − ˆx(cid:1).

˙F = β

(cid:16)

.

5

5 The autoencoder with memory

We are ﬁnally in a position to tackle the problem we started out with  how to build a recurrent
network that efﬁciently represents not just its present input  but also its past inputs. The objective
function used so far  however  completely neglects the input history: even if the dimensionality of
the input is much smaller than the number of neurons available to code it  the network will not try
to use the extra ‘space’ available to remember the input history.

5.1 An objective function for short-term memory

Ideally  we would want to be able to read out both the present input and the past inputs  such that
xt−n ≈ Dnrt  where n is an elementary time step  and Dn are appropriately chosen readouts. We
will in the following assume that there is a matrix M such that DnM = Dn+1 for all n. In other
words  the input history should be accessible via ˆxt−n = Dnr = D0Mnrt. Then the cost function
we would like to minimize is a straightforward generalization of Eq. (2) 

(cid:88)

n=0

L =

1
2

γn(cid:107)xt−n − DMnrt(cid:107)2 +

µ
2(cid:107)rt(cid:107)2.

(20)

where we have set D = D0. We tacitly assume that x and r are centered and that the L2 norm
is deﬁned with respect to the input signal covariance matrix Cx  so that we can work in the full
generality of Eq. (12) without keeping the additional notational baggage.
Unfortunately  the direct minimization of this objective is impossible  since the network has no
access to the past inputs xt−n for n ≥ 1. Rather  information about past inputs will have to be
retrieved from the network activity itself. We can enforce that by replacing the past input signal at
time t  with its estimate in the previous time step  which we will denote by a prime. In other words 
instead of asking that xt−n ≈ ˆxt−n  we ask that ˆx(cid:48)(t−1)−(n−1) ≈ ˆxt−n  so that the estimates of the
input (and its history) are properly propagated through the network. Given the iterative character
of the respective errors  (cid:107)ˆx(cid:48)(t−1)−(n−1) − ˆxt−n(cid:107) = (cid:107)DMn−1(rt−1 − Mrt)(cid:107)  we can deﬁne a loss
function for one time step only 

L =

1
2 (cid:107)xt − Drt(cid:107)2 +

γ
2 (cid:107)rt−1 − Mrt(cid:107)2 +

µ
2 (cid:107)rt(cid:107)2 .

(21)

Here  the ﬁrst term enforces that the instantaneous input signal is properly encoded  while the second
term ensures that the network is remembering past information. The last term is a cost term that
makes the system more stable and efﬁcient.
Note that a network which minimizes this loss function is maximizing its information content  even
if the number of neurons  N  far exceeds the input dimension K  so that N (cid:29) K. As becomes
clear from inspecting the loss function  the network is trying to code an N + K dimensional signal
with only N neurons. Consequently  just as in the undercomplete autoencoder  all of its information
capacity will be used.

5.2 Dynamics and learning

Conceptually  the loss function in Eq. (21) is identical to Eq. (2)  or rather  to Eq. (12)  if we keep full
generality. We only need to vertically stack the feedforward input and the delayed recurrent input
into a single high-dimensional vector x(cid:48) = (xt ; √γrt−1). Similarly  we can horizontally combine
the decoder D and the ‘time travel’ matrix M into a single decoder matrix D(cid:48) = (D √γM). The
above loss function then reduces to

L = (cid:107)x(cid:48)t − D(cid:48)rt(cid:107)2 + µ(cid:107)rt(cid:107)2  

(22)

and all of our derivations  including the learning rules  can be directly applied to this system. Note
that the ‘input’ to the network now combines the actual input signal  xt  and the delayed recurrent
input  rt−1. Consequently  this extended input is neither white nor centered  and we will need to
work with the generalized dynamics and generalized learning rules derived in the previous section.

6

Figure 2: Emergence of working memory in a network of 10 neurons with random initial connectivity. (A)
Mean rates of all neurons over learning. Each time-step corresponds to one input. (B) Rate correlation matrix
after learning (black = 1  white = 0). (C) Number of reconstructed steps from the input history. We trained a
linear decoder on the network responses to 50000 input stimuli and tested the performance on a test set of 5000
inputs. (D) Overlaps in signal projections before learning. Let uτ ∝ (Ωd)τ F be the (normalized) direction in
which the delayed weights project the input in a sequence of τ time-steps. We here show the overlap of these
projections  i.e. U(cid:62)U where Uτ . = uτ . (E-F) Same as (D) but at different moments during learning.

The network dynamics will initially follow the differential equation 4
V = Fxt + Ωdrt−1 − Ωf rt − µrt 
˙r = V − (cid:104)V(cid:105).

Compared to our previous network  we now have effectively three inputs into the network:
the
feedforward inputs with weight F  a delayed recurrent input with weight Ωd and a fast recurrent
input with weight Ωf   see Fig. 1c. The optimal connectivities can be derived from the loss function
and are (see also Fig. 1c  which considers the simpliﬁed case Cx = I and Cr = I)

(23)
(24)

(25)
(26)
(27)

(28)
(29)
(30)

F∗Cx = D(cid:62) 
∗Cr = M(cid:62) 
Ωd
Ωf
∗ = F∗D + Ωd

∗M.

Consequently  there are also three learning rules: one for the fast recurrent weights  which follows
Eq. (18)  one for the feedforward weights  which follows Eq. (19)  and one for the delayed recurrent
weights  which also follows Eq. (19). In summary 

˙Ωf = (cid:0)(Fxt + Ωdrt−1)r(cid:62)t − (cid:104)Fxt + Ωdrt−1(cid:105)(cid:104)r(cid:105)(cid:62)t − αΩf(cid:1)  
˙F = β(cid:0)(rt − αFxt)x(cid:62)t − (cid:104)rt − αFxt(cid:105)(cid:104)x(cid:62)t (cid:105)
˙Ωd = β(cid:0)(rt − αΩdrt−1)r(cid:62)t−1 − (cid:104)rt − αΩdrt−1(cid:105)(cid:104)r(cid:62)t−1(cid:105)
(cid:1).

(cid:1) 

In the supplementary information we prove that all ﬁxed points with rank deﬁcient Cr are unstable 
and that Cr ∝ I ∝ Ωf otherwise. In other words  the neural responses are completely decorrelated.
As a result  the network keeps track of the maximum possible number of inputs.

4We are now dealing with a delay-differential equation  which may be obscured by our notation. In practice 
the term rt−1 would be replaced by a term of the form r(r − τ )  where τ is the actual value of the ‘time step’.
Also  for notational convenience we put γ = 1  but the generalization is straight-forward.

7

0.00.20.40.60.81.0time [1e7]0.100.150.200.250.300.350.40rel. firing rateFiring rates over learning02468Neuron #02468Neuron #Rate correlation matrix (t=10e7)0246810time [1e7]0246810# reconstructed time-stepsLength of reconstructed history02468time-steps in the past02468time-steps in the pastProjection overlaps (t=0)02468time-steps in the past02468Projection overlaps (t=.25e7)02468time-steps in the past02468Projection overlaps (t=2e7)02468time-steps in the past02468Projection overlaps (t=4.5e7)ABCDEFG6 Simulations

We simulated a ﬁring rate network of ten neurons that learn to remember a one-dimensional  tem-
porally uncorrelated white noise stimulus (Fig. 2). We initialized all feedforward weights to one 
whereas the matrices Ωf and Ωd were drawn from a centered Gaussian distributions with small vari-
ance. At the onset  the network has some memory  similar to random networks based on reservoir
computing. However  the recurrent inputs are generally not cancelling out the feedforward inputs.
The effect of such imprecise balance are initially high ﬁring rates and poor coding properties (Fig.
2A C). At the beginning of learning the fast recurrent connections tighten the balance and thus re-
duce the overall ﬁring rates in the network (Fig. 2A). At this point the delayed recurrent connections
Ωd project the input signal F along a sequence uτ = (Ωd)
F that is almost random (Fig. 2D). Over
learning (Fig. 2E-F) this overlap in the projection sequence vanishes. In other words  inputs entering
the network are projected along a sequence of orthogonal subspaces  much like in tapped delay line-
type of networks. This adaptation leads to increasing memory performance of the network (Fig. 2C)
and thus to increasing neural ﬁring rates (Fig. 2A). At the end of learning  the coding properties are
close to the information-theoretic limit (10 time steps). A simulation script that reproduces Figure 2
(implemented as an Ipython Notebook) is registered at ModelDB (accession number 169983).

τ

7 Towards learning in spiking recurrent networks

While we have shown how a recurrent network can learn to efﬁciently represent an input and its
history using only local learning rules  our network is still far from being biologically realistic. A
quite obvious discrepancy with biological networks is that the neurons are not spiking  but rather
emit ‘ﬁring rates’ that can be both positive and negative. How can we make the connection to
spiking networks? Standard solutions have bridged from rate to spiking networks using mean-ﬁeld
approaches [18]. However  more recent work has shown that there is a direct link from the types of
loss functions considered in this paper to balanced spiking networks.
Recently  Hu et al. pointed out that the minimization of Eq. (2) can be done by a network of
neurons that ﬁres both positive and negative spikes [9]  and then argued that these networks can be
translated into real spiking networks. A similar  but more direct approach was introduced in [3  1]
who suggested to minimize the loss function  Eq. (2)  under the constraint that r ≥ 0. The resulting
networks consist of recurrently connected integrate-and-ﬁre neurons that balance their feedforward
and recurrent inputs [3  1  4]. Importantly  Eq. (2) remains a convex function of r  and Eq. (3) still
applies (except that r cannot become negative).
The precise match between the spiking network implementation and the ﬁring rate minimization
[1] opens up the possibility to apply our learning rules to the spiking networks. We note  though 
that this holds only strictly in the regime where the spiking networks are balanced. (For unbalanced
networks  there is no direct link to the ﬁring rate formalism.) If the initial network is not balanced 
we need to ﬁrst learn how to bring it into the balanced state. For white-noise Gaussian inputs  [4]
showed how this can be done. For more general inputs  this problem will have to be solved in the
future.

8 Discussion

In summary  we have shown how a recurrent neural network can learn to efﬁciently represent both
its present and past inputs. A key insight has been the link between balancing of feedforward and
recurrent inputs and the minimization of the cost function. If neurons can compensate both external
feedforward and delayed recurrent excitation with lateral inhibition  then  to some extent  they must
be coding the temporal trajectory of the stimulus. Indeed  in order to be able to compensate an input 
the network must be coding it at some level. Furthermore  if synapses are linear  then so must be the
decoder.
We have shown that this ‘balance’ can be learnt through local synaptic plasticity of the lateral con-
nections  based only on the presynaptic input signals and postsynaptic ﬁring rates of the neurons.
Similar to tapped delay lines  the converged network propagates inputs along a sequence of indepen-
dent directions of the population response. Different from tapped delay lines  this capability arises
dynamically and can adapt to the input statistics.

8

References

[1] D. G. Barrett  S. Den`eve  and C. K. Machens. “Firing rate predictions in optimal balanced
networks”. In: Advances in Neural Information Processing Systems 26. 2013  pp. 1538–1546.
[2] A. J. Bell and T. J. Sejnowski. “An information-maximization approach to blind separation

and blind deconvolution”. In: Neural comp. 7 (1995)  pp. 1129–1159.

[3] M. Boerlin  C. K. Machens  and S. Den`eve. “Predictive coding of dynamical variables in

balanced spiking networks”. In: PLoS Computational Biology 9.11 (2013)  e1003258.

[4] R. Bourdoukan et al. “Learning optimal spike-based representations”. In: Advances in Neural

Information Processing Systems 25. MIT Press  2012  epub.

[5] S. Fusi  P. J. Drew  and L. F. Abbott. “Cascade models of synaptically stored memories”. In:

Neuron 45.4 (2005)  pp. 599–611.

[6] S. Hochreiter and J. Schmidhuber. “Long short-term memory”. In: Neural computation 9.8

[7]

[8]

(1997)  pp. 1735–1780.
J. J. Hopﬁeld. “Neural networks and physical systems with emergent collective computational
abilities”. In: Proceedings of the national academy of sciences 79.8 (1982)  pp. 2554–2558.
J. J. Hopﬁeld. “Neurons with graded response have collective computational properties like
those of two-state neurons”. In: Proc. Natl. Acad. Sci. USA 81 (1984)  pp. 3088–3092.

[9] T. Hu  A. Genkin  and D. B. Chklovskii. “A network of spiking neurons for computing sparse
representations in an energy-efﬁcient way”. In: Neural computation 24.11 (2012)  pp. 2852–
2872.

[10] H. Jaeger. “The ”echo state” approach to analysing and training recurrent neural networks.”

In: German National Research Center for Information Technology. Vol. 48. 2001.

[11] A. Lazaar  G. Pipa  and J. Triesch. “SORN: a self-organizing recurrent neural network”. In:

Frontiers in computational neuroscience 3 (2009)  p. 23.

[12] M. Lukoˇseviˇcius and H. Jaeger. “Reservoir computing approaches to recurrent neural network

training”. In: Computer Science Review 3.3 (2009)  pp. 127–149.

[13] W. Maass  T. Natschl¨ager  and H. Markram. “Real-time computing without stable states:
A new framework for neural computation based on perturbations”. In: Neural computation
14.11 (2002)  pp. 2531–2560.

[14] C. K. Machens  R. Romo  and C. D. Brody. “Flexible control of mutual inhibition: A neural

model of two-interval discrimination”. In: Science 307 (2005)  pp. 1121–1124.

[15] G. Major and D. Tank. “Persistent neural activity: prevalence and mechanisms”. In: Curr.

Opin. Neurobiol. 14 (2004)  pp. 675–684.

[16] B. A. Olshausen and D. J. Field. “Sparse coding with an overcomplete basis set: A strategy

employed by V1?” In: Vision Research 37.23 (1997)  pp. 3311–3325.

[17] R. P. N. Rao and D. H. Ballard. “Predictive coding in the visual cortex: a functional inter-
pretation of some extra-classical receptive-ﬁeld effects”. In: Nature neuroscience 2.1 (1999) 
pp. 79–87.

[18] A. Renart  N. Brunel  and X.-J. Wang. “Mean-ﬁeld theory of irregularly spiking neuronal
populations and working memory in recurrent cortical networks”. In: Computational neuro-
science: A comprehensive approach (2004)  pp. 431–490.

[19] C. J. Rozell et al. “Sparse coding via thresholding and local competition in neural circuits”.

In: Neural computation 20.10 (2008)  pp. 2526–2563.

[20] E. P. Simoncelli and B. A. Olshausen. “Natural image statistics and neural representation”.

In: Ann. Rev. Neurosci. 24 (2001)  pp. 1193–1216.

[21] X.-J. Wang. “Probabilistic decision making by slow reverberation in cortical circuits”. In:

Neuron 36.5 (2002)  pp. 955–968.

[22] P. J. Werbos. “Backpropagation through time: what it does and how to do it”. In: Proceedings

of the IEEE 78.10 (1990)  pp. 1550–1560.
J. Zylberberg  J. T. Murphy  and M. R. DeWeese. “A sparse coding model with synaptically
local plasticity and spiking neurons can account for the diverse shapes of V1 simple cell
receptive ﬁelds”. In: PLoS Computational Biology 7.10 (2011)  e1002250.

[23]

9

,Pietro Vertechi
Wieland Brendel
Christian Machens
Keenon Werling
Arun Tejasvi Chaganty
Percy Liang
Christopher Manning