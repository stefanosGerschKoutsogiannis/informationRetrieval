2013,High-Dimensional Gaussian Process Bandits,Many applications in machine learning require optimizing unknown functions defined over a high-dimensional space from noisy samples that are expensive to obtain. We address this notoriously hard challenge  under the assumptions that the function varies only along some low-dimensional subspace and is smooth (i.e.  it has a low norm in a Reproducible Kernel Hilbert Space). In particular  we present the SI-BO algorithm  which leverages recent low-rank matrix recovery techniques to learn the underlying subspace of the unknown function and applies Gaussian Process Upper Confidence sampling for optimization of the function. We carefully calibrate the exploration–exploitation tradeoff by allocating sampling budget to subspace estimation and function optimization  and obtain the first subexponential cumulative regret bounds and convergence rates for Bayesian optimization in high-dimensions under noisy observations. Numerical results demonstrate the effectiveness of our approach in difficult scenarios.,High-Dimensional Gaussian Process Bandits

Josip Djolonga
ETH Z¨urich

josipd@ethz.ch

Andreas Krause

ETH Z¨urich

krausea@ethz.ch

Volkan Cevher

EPFL

volkan.cevher@epfl.ch

Abstract

Many applications in machine learning require optimizing unknown functions
deﬁned over a high-dimensional space from noisy samples that are expensive to
obtain. We address this notoriously hard challenge  under the assumptions that
the function varies only along some low-dimensional subspace and is smooth
(i.e.  it has a low norm in a Reproducible Kernel Hilbert Space). In particular  we
present the SI-BO algorithm  which leverages recent low-rank matrix recovery
techniques to learn the underlying subspace of the unknown function and applies
Gaussian Process Upper Conﬁdence sampling for optimization of the function.
We carefully calibrate the exploration–exploitation tradeoff by allocating the
sampling budget to subspace estimation and function optimization  and obtain the
ﬁrst subexponential cumulative regret bounds and convergence rates for Bayesian
optimization in high-dimensions under noisy observations. Numerical results
demonstrate the effectiveness of our approach in difﬁcult scenarios.

Introduction

1
The optimization of non-linear functions whose evaluation may be noisy and expensive is a chal-
lenge that has important applications in sciences and engineering. One approach to this notoriously
hard problem takes a Bayesian perspective  which uses the predictive uncertainty in order to trade
exploration (gathering data for reducing model uncertainty) and exploitation (focusing sampling
near likely optima)  and is often called Bayesian Optimization (BO). Modern BO algorithms are
quite successful  surpassing even human experts in learning tasks: e.g.  gait control for the SONY
AIBO  convolutional neural networks  structural SVMs  and Latent Dirichlet Allocation [1  2  3].
Unfortunately  the theoretical efﬁciency of these methods depends exponentially on the—often
high—dimension of the domain over which the function is deﬁned. A way to circumvent this “curse
of dimensionality” is to make the assumption that only a small number of the dimensions actually
matter. For example  the cost function of neural networks effectively varies only along a few dimen-
sions [2]. This idea has been also at the root of nonparametric regression approaches [4  5  6  7].
To this end  we propose an algorithm that learns a low dimensional  not necessarily axis-aligned 
subspace and then applies Bayesian optimization on this estimated subspace. In particular  our SI-
BO approach combines low-rank matrix recovery with Gaussian Process Upper Conﬁdence Bound
sampling in a carefully calibrated manner. We theoretically analyze its performance  and prove
bounds on its cumulative regret. To the best of our knowledge  we prove the ﬁrst subexponential
bounds for Bayesian optimization in high dimensions under noisy observations. In contrast to exist-
ing approaches  which have an exponential dependence on the ambient dimension  our bounds have
in fact polynomial dependence on the dimension. Moreover  our performance guarantees depend
explicitly on what we could have achieved if we had known the subspace in advance.
Previous work. Exploration–exploitation tradeoffs were originally studied in the context of ﬁnite
multi-armed bandits [8]. Since then  results have been obtained for continuous domains  starting
with the linear [9] and Lipschitz-continuous cases [10  11]. A more recent algorithm that enjoys
theoretical bounds for functions sampled from a Gaussian Process (GP)  or belong to some Repro-

1

ducible Kernel Hilbert Space (RKHS) is GP-UCB [12]. The use of GPs to negotiate exploration–
exploitation tradeoffs originated in the areas of response surface and Bayesian optimization  for
which there are a number of approaches (cf.  [13])  perhaps most notably the Expected Improve-
ment [14] approach  which has recently received theoretical justiﬁcation [15]  albeit only in the
noise-free setting.
Bandit algorithms that exploit low-dimensional structure of the function appeared ﬁrst for the linear
setting  where under sparsity assumptions one can obtain bounds  which depend only weakly on the
ambient dimension [16  17]. In [18] the more general case of functions sampled from a GP under the
same sparsity assumptions was considered. The idea of random projections to BO has been recently
introduced [19]. They provide bounds on the simple regret under noiseless observations  while we
also analyze the cumulative regret and allow noisy observations. Also  unless the low-dimensional
space is of dimension 1  our bounds on the simple regret improve on theirs. In [7] the authors ap-
proximate functions that live on low-dimensional subspaces using low-rank recovery and analysis
techniques. While providing uniform approximation guarantees  their approach is not tailored to-
wards exploration–exploitation tradeoffs  and does not achieve sublinear cumulative regret. In [20]
the stochastic and adversarial cases for axis-aligned H¨older continuous functions are considered.
Our speciﬁc contributions in this paper can be summarized as follows:

• We introduce the SI-BO algorithm for Bayesian bandit optimization in high dimensions 
admitting a large family of kernel functions. Our algorithm is a natural but non-trivial
fusion of modern low-rank subspace approximation tools with GP optimization methods.
• We derive theoretical guarantees on SI-BO’s cumulative and simple regret in high-
dimensions with noise. To the best of our knowledge  these are the ﬁrst theoretical results
on the sample complexity and regret rates that are subexponential in the ambient dimension.

• We provide experimental results on synthetic data and classical benchmarks.

time step t. Hence  a natural quantity to consider is the cumulative regret deﬁned as RT =(cid:80)T

2 Background and Problem Statement
Goal.
In plain words  we wish to sequentially optimize a bounded function over a compact  convex
subset D ⊂ Rd. Without loss of generality  we denote the function by f : D → [0  1] and let x∗
be a maximizer. The algorithm proceeds in a total of T rounds. In each round t  it asks an oracle
for the function value at some point xt and it receives back the value f (xt)  possibly corrupted by
noise. Our goal is to choose points such that their values are close to the optimum f (x∗).
As performance metric  we consider the regret  which tells us how much better we could have done
in round t had we known x∗  or formally rt = f (x∗) − f (xt).
In many applications  such as
recommender systems  robotic control  etc.  we care about the quality of the points chosen at every
t=1 rt.
One can also consider the simple regret  deﬁned as ST = minT
t=1 rt  measuring the quality of the
best solution found so far. We will give bounds on the more challenging notion of cumulative regret 
which also bounds the simple regret via ST ≤ RT /T .
Low-dimensional functions in high-dimensions. Unfortunately  our problem cannot be tractably
solved without further assumptions on the properties of the function f. What is worse is that the
usual compact support and smoothness assumptions cannot achieve much: the minimax lower bound
on the sample complexity is exponential in d [21  6  7]. We hence assume that the function effec-
tively varies only along a small number of true active dimensions:
i.e.  the function lives on a
k (cid:28) d-dimensional subspace. Typically  k or an upper bound on k is assumed known [4  5  7  6].
Formally  we suppose that there exists some function g : Rk → [0  1] and a matrix A ∈ Rk×d with
orthogonal rows so that f (x) = g(Ax). We will additionally assume that g ∈ C2  which is necessary
to bound the errors from the linear approximation that we will make. Further  w.l.o.g.  we assume
that D = Bd(1+ ¯ε) for some ¯ε > 0  where we deﬁne Bd(r) to be the closed ball around 0 of radius r
in Rd.1 To be able to recover the subspace we also need the condition that g has Lipschitz continuous
second order derivatives and a full rank Hessian at 0  which is satisﬁed for many functions [7].
Smooth  low-complexity functions.
In addition to the low-dimensional subspace assumption  we
also assume that g is smooth. One way to encode our prior is to assume that the function g resides in

1Our method method can be extended to any convex compact set  see Section 5.2 in [22].

2

Algorithm 1 The SI-BO algorithm
Require: mX   mΦ  λ  ε  k  oracle for f  kernel κ
√
Φi ← mΦ samples uniformly from {±1/

C ← mX samples uniformly from Sd−1
for i ← 1 to mX do
y ← compute using Equation 1
ˆXDS ← Dantzig Selector using y  see Equation 2 and compute the SVD ˆXDS = ˆU ˆΣ ˆV T
ˆA ← ˆU (k) // Principal k vectors of ˆU  D ← all ( ˆAx  y) pairs queried so far
Use GP inference to obtain µ1(·)  σ1(·).
for t ← 1 to T − mX (mΦ + 1) do

mΦ}k

zt ← arg maxz µt(z) + β1/2

t σt(z)   yt ← f ( ˆAT zt) + noise   D.add(zt  yt)

completing the set of functions(cid:80)n

a Reproducing Kernel Hilbert Space (RKHS; cf.  [23])  which allows us to quantify g’s complexity
. The RKHS for some positive semideﬁnite kernel κ(· ·) can be constructed by
via its norm (cid:107)g(cid:107)Hκ
i=1 αiκ(xi ·) under a suitable inner product. In this work  we use
isotropic kernels  i.e.  those that depend only on the distance between points  since the problem is
rotation invariant and we can only recover A up to some rotation.
Here is a ﬁnal summary of our problem and its underlying assumptions:

1. We wish to maximize f : Bd(1 + ¯ε) → [0  1]  where f (x) = g(Ax) for some matrix
A ∈ Rk×d with orthogonal rows and g belongs to some RKHS Hκ.
2. The kernel κ is isotropic κ(x  x(cid:48)) = κ(cid:48)(x − x(cid:48)) = κ(cid:48)(cid:48)((cid:107)x − x(cid:48)(cid:107)2) and κ(cid:48) is continuous 
integrable and with a Fourier transform Fκ(cid:48) that is isotropic and radially non-increasing.2
3. The function g has Lipschitz continuous 2nd-order derivatives and a full rank Hessian at 0.
4. The function g is C2 on a compact support and max|β|≤2(cid:107)Dβg(cid:107)∞ ≤ C2 for some C2 > 0.
5. The oracle noise is Gaussian with zero mean with a known variance σ2.

3 The SI-BO Algorithm
The SI-BO algorithm performs two separate exploration and exploitation stages: (1) subspace iden-
tiﬁcation (SI)  i.e. estimating the subspace on which the function is supported  and then (2) Bayesian
optimization (BO)  in order to optimize the function on the learned subspace. A key challenge here
is to carefully allocate samples between these phases.
We ﬁrst give a detailed outline for SI-BO in Alg. 1  deferring its theoretical analysis to Section 4.
Given the (noisy) oracle for f  we ﬁrst evaluate the function at several suitably chosen points and
then use a low-rank recovery algorithm to compute a matrix ˆA that spans a subspace well aligned
with the one generated by the true matrix A. Once we have computed ˆA  similarly to [22  7]  we
deﬁne the function which we optimize as ˆg(z) = f ( ˆAT z) = g(A ˆAT z). Thus  we effectively work
with an approximation ˆf to f given by ˆf (x) = ˆg( ˆAx) = g(A ˆAT ˆAx). With the approximation at
hand  we apply BO  in particular the GP-UCB algorithm  on ˆg for the remaining steps.
Subspace Learning. We learn A using the approach from [7]  which reduces the learning prob-
lem to that of low rank matrix recovery. We construct a set of mX points C = [ξ1 ···   ξmX ] 
which we call sampling centers  and consider the matrix X of gradients at those points X =
[∇f (ξ1) ···  ∇f (ξmX )]. Using the chain rule  we have X = AT [∇g(Aξ1) ···  ∇g(AξmX )].
Because A is a matrix of size k × d it follows that the rank of X is at most k. This suggests that
using low-rank approximation techniques  one may be able to (up to rotation) infer A from X.
Given that we have no access to the gradients of f directly  we approximate X using a linearization
of f. Consider a ﬁxed sampling center ξ. If we make a linear approximation with step size ε to the
directional derivative at center ξ in direction ϕ then  by Taylor’s theorem  for a suitable ζ(x  ε  ϕ):

(cid:104)ϕ  AT∇g(Aξ)(cid:105) =

1
ε

(f (ξ + εϕ) − f (ξ)) − ε
2

ϕT∇2f (ζ)ϕ

.

(cid:124)

(cid:123)(cid:122)

E(ξ ε ϕ)

(cid:125)

2This is the same assumption as in [15]. Radially non-increasing means that if (cid:107)w(cid:107) ≤ (cid:107)w(cid:48)(cid:107) then Fκ(cid:48)(w) ≥

Fκ(cid:48)(w(cid:48)). Note that this is satisﬁed by the RBF and Mat`ern kernels.

3

Thus  sampling the ﬁnite difference f (ξ + εϕ)− f (ξ) provides (up to the curvature error E(ξ  ε  ϕ) 
and sampling noise) information about the one-dimensional subspace spanned by AT∇g(Aξ).
To estimate it accurately  we must observe multiple directions ϕ. Further  to infer the full k-
dimensional subspace A  we need to consider at least mX ≥ k centers. Consequently  for
each center ξi  we deﬁne a set of mΦ directions and arrange them in a total of mΦ matrices
Φi = [ϕi 1  ϕi 2 ···   ϕi mX ] ∈ Rd×mX . We can now deﬁne the following linear system:

y = A(X) + e + z 

yi =

1
ε

(f (ξj + εϕi j) − f (ξj)) 

(1)

where the linear operator A is deﬁned as A(X)i = tr(ΦT
i X)  the curvature errors have been accu-
mulated in e and the noise has been put in the vector z which is distributed as zi ∼ N (0  2mX σ2/ε).
Given the structure of the problem  we can make use of several low-rank recovery algorithms. For
concreteness  we choose the Dantzig Selector (DS  [24])  which recovers low rank matrices via

mX(cid:88)

j=1

minimize

M

(cid:107)M(cid:107)∗

subject to (cid:107)A∗(y − A(M )

)(cid:107) ≤ λ 

(2)

(cid:124)

(cid:123)(cid:122)

residual

(cid:125)

where (cid:107)·(cid:107)∗ is the nuclear norm and (cid:107)·(cid:107) is the spectral norm. The DS will successfully recover a
matrix ˆX close to the true solution in the Frobenius norm and moreover this distance decreases
linearly with λ. As shown in [7]  choosing the centers C uniformly at random from the unit sphere
√
Sd−1  choosing each direction vector uniformly at random from {±1/
mΦ}k  and—in the case of
noisy observations  resampling f repeatedly—sufﬁces to obtain an accurate ˆX w.h.p.  as long as mΦ
and mX are sufﬁciently large. The precise choices of these quantities are analyzed in Section 4.
Finally  we extract the matrix ˆA from the SVD of ˆX  by taking its top k left singular vectors.
Because the DS will ﬁnd a matrix ˆX close to X  due to a result by Wedin [25] we know that the
learned subspace will be close to the true one.
Optimizing ˆg. Once we have an approximate ˆA  we optimize the function ˆg(z) = f ( ˆAT z) on the
low-dimensional domain Z = Bk(1+ ¯ε). Concretely  we use GP-UCB [12]  because it exhibits state
of the art empirical performance  and enjoys strong theoretical bounds for the cumulative regret. It
requires that ˆg belongs to the RKHS and the noise when conditioned on the history is zero-mean and
almost surely bounded by some ˆσ. Section 4 shows that this is indeed true with high probability.
In order to trade exploration and exploitation  the GP-UCB algorithm computes  for each point z 
a score that combines the predictive mean that we have inferred for that point with its variance 
which quantiﬁes the uncertainty in our estimate. They are combined linearly with a time-dependent
weighting factor βt in the following surrogate function

ucb(z) = µt(z) + β1/2

t σt(z)

(3)

for a suitably chosen βt = 2B + 300γt log3(t/δ). Here  B is an upper bound on the squared RKHS
norm of the function that we optimize  δ is an upper bound on the failure probability and γt depends
on the kernel [12]: cf.  Section 4.3 The algorithm then greedily maximizes the ucb score above.
Note that ﬁnding the maximum of this non-convex and in general multi-modal function  while
considered to be cheaper than evaluating f at a new point  is by itself a hard problem and it is
usually approached by either sampling on a grid in the domain  or using some global Lipschitz
optimizer [13]. Hence  by reducing the dimension of the domain Z over which we have to optimize 
our algorithm has the additional beneﬁt that this process can be performed more efﬁciently.
Handling the noise. The last ingredient that we need is theory on how to pick ˆσ so that it bounds
the noise during the execution of GP-UCB w.h.p.  and how to select λ in (2) so that the true matrix
X is feasible in the DS. Due to the fast decay of the tails of the Gaussian distribution we can pick
σ  where T is the number of GP-UCB iterations and σ2 is the

ˆσ =
variance of the noise. Then the noise will be trapped in [−ˆσ  ˆσ] with probability at least 1 − δ.

(cid:17)1/2

δ + 2 log T + log 1

2π

2 log 1

(cid:16)

3If the bound B is not known beforehand then one can use a doubling trick.

4

Figure 1: A 2-dimensional function f (x  y) varying along a 1-dimensional subspace and its projec-
tions on different subspaces. The numbers are the respective cosine distances.

The analysis on λ comes from [7]. They bound (cid:107)A∗(e + z)(cid:107) using the assumption that the second
order derivatives are bounded and  as shown in [24]  because z has a Gaussian distribution 

(4)

(cid:107)A∗(e + z)(cid:107) ≤ 1.2

(cid:18) C2εdmX k2

√

2

mΦ

(cid:19)

√
5

+

mX mΦσ

ε

If there is no noise it still holds by setting σ = 0. This bound  intuitively  relates the approximation
quality λ of the subspace to the quantities mΦ  mX as well as the step size ε.

4 Theoretical Analysis

Overview. A crucial choice in our algorithm is how to allocate samples (by choosing mΦ and mX
appropriately) to the tasks of subspace learning and function optimization. We now analyze both
phases  and determine how to split the queries in order to optimize the cumulative regret bounds.
Let us ﬁrst consider the regret incurred in the second phase  in the ideal (but unrealistic) case that
the subspace is estimated exactly (i.e.  ˆA = A). This question was answered recently in [12]  where
it is proven that it is bounded by O∗(

√

√

γt + γt)) 4 . Hereby  the quantity γT is deﬁned as
H(yS) − H(yS|f ) 

T (B
γT = max

S⊆D |S|=T

of g w.r.t. kernel κ. Note that generally γT grows exponentially with k  rendering the

the RBF kernel in k dimensions  γT = O(cid:0)(log T )k+1(cid:1). Further  B is a bound on the squared

where yS are the values of f at the points in S  corrupted by Gaussian noise  and H(·) is the entropy.
It quantiﬁes the maximal gain in information that we can obtain about f by picking a set of T points.
In [12] sublinear bounds for γT have been computed for several popular kernels. For example  for
norm (cid:107)g(cid:107)2Hκ
application of GP-UCB directly to the high-dimensional problem intractable.
What happens if the subspace ˆA is estimated incorrectly? Fortunately  w.h.p. the estimated function
ˆg still remains in the RKHS associated with kernel κ. However  the norm (cid:107)ˆg(cid:107)Hκ
may increase  and
consequently may the regret. Moreover  the considered ˆf disagrees with the true f  and consequently
additional regret per sample may be incurred by η = || ˆf − f||∞. As an illustration of the effect of
misestimated subspaces see Figure 1. We can observe that subspaces far from the true one stretch
the function more  thus increasing its RKHS norm.
We now state a general result that formalizes these insights by bounding the cumulative regret in
terms of the samples allocated to subspace learning  and the subspace approximation quality.
Lemma 1 Assume that we spend 0 < n ≤ T samples to learn the subspace such that (cid:107)f− ˆf(cid:107)∞ ≤ η 
(cid:107)ˆg(cid:107) ≤ B and the error is bounded by ˆσ  each w.p. at least 1 − δ/4.
If we run the GP-UCB
algorithm for the remaining T − n steps with the suggested ˆσ and δ/4  then the following bound on
the cumulative regret holds w.p. at least 1 − δ

RT ≤ n + ηT(cid:124)(cid:123)(cid:122)(cid:125)

approx. error

√

+O∗(

(cid:124)

√

(cid:123)(cid:122)

RUCB(T ˆg κ)

T (B

γt + γt))

(cid:125)

4We have used the notation O∗(f ) = O(f log f ) to suppress the log factors. Ω∗(·) is analogously deﬁned.

5

−2−1012−2−1012−20−15−10−50510xyf(x y)−2−1.5−1−0.500.511.52−20−15−10−50510xˆg(x)true subspaceFigure 2: Approximations ˆg resulting from differently aligned subspaces. Note that inaccurate
estimation (the middle two cases) can wildly distort the objective.

where RUCB(T  ˆg  κ) is the regret of GP-UCB when run for T steps using ˆg and kernel κ 5.

Lemma 1 breaks down the regret in terms of the approximation error incurred by subspace-
≤
misestimation  and the optimization error incurred by the resulting increased complexity (cid:107)ˆg(cid:107)2Hκ
B. We now analyze these effects  and then prove our main regret bounds.

Effects of Subspace Alignment. A notion that will prove to be very helpful for analyzing both 
the approximation precision η and the norm of ˆg  is the set of angles between the subspaces that are
deﬁned by A and ˆA. The following deﬁnition [26] makes this notion precise.
Deﬁnition 2 Let A  ˆA ∈ Rk×d be two matrices with orthogonal rows so that AAT = ˆA ˆAT = I. We
deﬁne the vector of cosines between the spanned subspaces cos Θ(A  ˆA) to be equal to the singular
values of A ˆAT . Analogously sin Θ(A  ˆA)i = (1 − cos Θ(A  ˆA)2
Let us see how ˆA affects ˆg. Because ˆg(z) = g(A ˆAT z)  the matrix M = A ˆAT   which converts
any point from its coordinates determined by ˆA to the coordinates deﬁned by A  will be of crucial
importance. First  note that its singular values are cosines and are between −1 and 1. This means
that it can only shrink the vectors that we apply it to (possibly by different amounts in different
directions). The effect on ˆg is that it might only “see” a small part of the whole space  and its shape
might be distorted  which in turn will increase its RKHS complexity (see Figure 2 for an illustration).
Lemma 3 If g ∈ Hκ for a kernel that is isotropic with a radially non-increasing Fourier transform
and ˆg(x) = g(A ˆAT x) for some A  ˆA with orthogonal rows  then for C = C2

i )1/2.

√

2k(1 + ¯ε) 
≤ | prod cos Θ(A  ˆA)|−1(cid:107)g(cid:107)2Hκ

.

(5)

(cid:107)f − ˆf(cid:107)∞ ≤ C(cid:107)sin Θ(A  ˆA)(cid:107)2 and (cid:107)ˆg(cid:107)2Hκ

Here  we use the notation prod x =(cid:81)d

i=1 xi to denote the product of the elements of a vector. By
decreasing the angles we tackle both issues: the approximation error η = (cid:107)f − ˆf(cid:107)∞ is reduced
and the norm of ˆg gets closer to the one of g. There is one nice interpretation of the product of the
cosines. It is equal to the determinant of the matrix M. Hence  ˆg will not be in the RKHS only if M
is rank deﬁcient as dimensions are collapsed.

Regret Bounds. We now present our main bounds on the cumulative regret. In order to achieve
sublinear regret  we need a way of controlling η and (cid:107)ˆg(cid:107)Hκ
. In the following  we show how this
goal can be achieved. As it turns out  subspace learning is substantially harder in the case of noisy
observations. Therefore  we focus on the easier  noise-free setting ﬁrst.
Noiseless Observations. We should note that the theory behind GP-UCB still holds in the deter-
ministic case  as it only requires the noise to be bounded a.s. by ˆσ. The following theorem guarantees
that in this setting for non-linear kernels we have a regret dominated by GP-UCB  which is of order
Ω∗(
Theorem 4 If the observations are noiseless we can pick mx = O(kd log 1/δ)  ε =
and mϕ = O(k2d log 1/δ) so that with probability at least 1 − δ we have the following

T γT )  as it is usually exponential in k.

k2.25d3/2T 1/2

√

1

RT ≤ O(k3d2 log2(1/δ)) + 2 RUCB(T  g  κ).

a term of order O((cid:112)log T + log(1/δ)); c.f. supplementary material.

5 Because the noise parameter ˆσ depends on T   we have to slightly change the bounds from [12] as we have

6

cosΘ=[1.00 1.00]−2−1012−2−1012cosΘ=[0.04 0.00]−2−1012−2−1012cosΘ=[0.99 0.04]−2−1012−2−1012cosΘ=[0.97 0.95]−2−1012−2−1012Noisy Observations. Equation 4 hints that the noise can have a dramatic effect in learning efﬁ-
ciency. As already mentioned  the DS gets better results as we decrease λ. In the noiseless case  it
sufﬁces to increase the number of directions mΦ and decrease the step size ε in estimating the ﬁnite
differences. However  the second term in λ can only be reduced by decreasing the variance σ2.
√
As a result  each point that we evaluate is sampled n times and we take as its value the average.
Moreover  note that because the standard deviation decreases as 1/
n  we have to resample at least
ε−2 times and this signiﬁcantly increases the number of samples that we need. Nevertheless  we
are able to obtain cumulative regret bounds (and thereby the ﬁrst convergence guarantees and rates)
for this setting  which only polynomially depend on d. Unfortunately  the dependence on T is now
weaker than those in the noiseless setting (Theorem 4)  and the regret due to the subspace learning
might dominate that of GP-UCB.
Theorem 5 If the observations are noisy  we can pick ε =
k2.25d1.5T 1/5 and all other parameters
as in the previous theorem. Moreover  we have to resample each point O(σ2k2dT 2/5mΦ/ε2) times.
Then  with probability at least 1 − δ

1

σ2k11.5d7T 4/5 log3(1/δ)

+ 2 RUCB(T  g  κ).

RT ≤ O(cid:16)

(cid:17)

Mismatch on the effective dimension k. All models are imperfect in some sense and the structure
of a general f is impossible to identify unless we have further scientiﬁc evidence beyond the data.
In our case  the assumption f (x) = g(Ax) for some k more or less takes the weakest form for
indicating our hope that BO can succeed from a sub-exponential sample size. In general  we must
tune k in a degree to reﬂect the anticipated complexity in the learning problem. Fortunately  all
the guarantees are preserved if we assume a k > ktrue  for some true synthetic model  where
f (x) = g(Ax) holds. Underﬁtting k leads to additional errors that are well-controlled in low-rank
subspace estimation [24]. The impact of under ﬁtting in our setting is left for future work.

5 Experiments

The main intent of our experiments is to provide a proof of concept  conﬁrming that SI-BO not just
in theory provides the ﬁrst subexponential regret bounds  but also empirically obtains low average
regret for Bayesian optimization in high dimensions.
Baselines. We compare SI-BO against the following baseline approaches:

• RandomS-UCB  which runs GP-UCB on a random subspace.
• RandomH-UCB  which runs GP-UCB on the high-dimensional space. At each iteration
• Exact-UCB  which runs GP-UCB on the exact (but in practice unknown) subspace.

we pick 1000 points at random and choose the one with highest UCB score.

The βt parameter in the GP-UCB score was set as recommended in [12] for ﬁnite sets. To optimize
the UCB score we sampled on a grid on the low dimensional subspace. For all of the measurements
we have added Gaussian zero-mean noise with σ = 0.01.
Data sets. We carry out experiments in the following settings:

with smoothness parameter ν = 5/2  length scale (cid:96) = 1/2 and signal variance σ2
The samples are “hidden” in a random 2-dimensional subspace in 100 dimensions.

• GP Samples. We generate random 2-dimensional samples from a GP with Mat`ern kernel
f = 1.
• Gab`or Filters. The second data set is inspired by experimental design in neuroscience
[27]. The goal is to determine visual stimuli that maximally excite some neuron  which
reacts to edges in the images. We consider the function f (x) = exp(−(θT x − 1)2)  where
θ is a Gab´or ﬁlter of size 17 × 17 and the set of admissible signals is [0  1]d.

In the appendix we also include results for the Branin function  a classical optimization benchmark.
Results. The results are presented in Figure 3. We show the averages of 20 runs (10 runs for
GP-Posterior) and the shaded areas represent the standard error around the mean. We show both
the average regret and simple regret (i.e.  suboptimality of the best solution found so far). We ﬁnd
that although SI-BO spends a total of mX (mΦ + 1) samples to learn the subspace and thus incurs

7

(a) GP-Posterior

(b) GP-Posterior  Different k

(c) Gab´or

(d) GP-Posterior

(e) GP-Posterior  Different k

(f) Gab´or

Figure 3: Performance comparison on different datasets. Our SI-BO approach outperforms the
natural benchmarks in terms of cumulative regret  and competes well with the unrealistic Exact-
UCB approach that knows the true subspace A.

much regret during this phase  learning the subspace pays off  both for average and simple regret 
and SI-BO ultimately outperforms the baseline methods on both data sets. This demonstrates the
value of accurate subspace estimation for Bayesian optimization in high dimensions.
Mis-speciﬁed k. What happens if we do not know the dimensionality k of the low dimensional
subspace? To test this  we experimented with the stability of SI-BO w.r.t. k. We sampled 2-
dimensional GP-Posterior functions and ran SI-BO with k set to 1  2 and 3. From the ﬁgure above
we can see that in this scenario SI-BO is relatively stable to this parameter mis-speciﬁcation.
6 Conclusion
We have addressed the problem of optimizing high dimensional functions from noisy and expensive
samples. We presented the SI-BO algorithm  which tackles this challenge under the assumption that
the objective varies only along a low dimensional subspace  and has low norm in a suitable RKHS.
By fusing modern techniques for low rank matrix recovery and Bayesian bandit optimization in a
carefully calibrated manner  it addresses the exploration–exploitation dilemma  and enjoys cumu-
lative regret bounds  which only polynomially depend on the ambient dimension. Our results hold
for a wide family of RKHS’s  including the popular RBF and Mat`ern kernels. Our experiments on
different data sets demonstrate that our approach outperforms natural benchmarks.

Acknowledgments. A. Krause acknowledges SNF 200021-137971  DARPA MSEE FA8650-11-
1-7156  ERC StG 307036 and a Microsoft Faculty Fellowship. V. Cevher acknowledges MIRG-
268398  ERC Future Proof  SNF 200021-132548  SNF 200021-146750  and SNF CRSII2-147633.

References
[1] D. Lizotte  T. Wang  M. Bowling  and D. Schuurmans. Automatic gait optimization with gaussian process

regression. In Proc. of IJCAI  pages 944–949  2007.

[2] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Journal of Machine

Learning Research  13:281–305  2012.

[3] J. Snoek  H. Larochelle  and R. P. Adams. Practical bayesian optimization of machine learning algorithms.

In Neural Information Processing Systems  2012.

[4] Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical

Association  86(414):316–327  1991.

8

0100020003000350000.20.40.60.81Rt/tNumber of samplesOur approachRandomS−UCBRandomH−UCBExact−UCB50010001500200025003000350000.20.40.60.81Rt/tNumber of samplesUCB−3UCB−1UCB−2500100015002000250000.20.40.60.81Rt/tNumber of samplesExact−UCBRandomH−UCBRandomS−UCBOur approach0100020003000350000.20.40.60.81Simple RegretNumber of samples Our approachRandomH−UCBRandomS−UCBExact−UCB50010001500200025003000350000.20.40.60.81Simple RegretNumber of samples  UCB−1UCB−2UCB−3500100015002000250000.20.40.60.81Simple RegretNumber of samplesExact−UCBRandomS−UCBOur approachRandomH−UCB[5] G. Raskutti  M.J. Wainwright  and B. Yu. Minimax rates of estimation for high-dimensional linear regres-

sion over (cid:96)q-balls. Information Theory  IEEE Transactions on  57(10):6976–6994  2011.

[6] S. Mukherjee  Q. Wu  and D. Zhou. Learning gradients on manifolds. Bernoulli  16(1):181–207  2010.
[7] H. Tyagi and V. Cevher. Learning non-parametric basis independent models from point queries via low-

rank methods. Applied and Computational Harmonic Analysis  (0):–  2014.

[8] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical

Society  58(5):527–535  1952.

[9] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine Learn-

ing Research  3:397–422  2003.

[10] R. Kleinberg  A. Slivkins  and E. Upfal. Multi-armed bandits in metric spaces. In STOC  pages 681–690 

2008.

[11] S. Bubeck  R. Munos  G. Stoltz  and C. Szepesv´ari. Online optimization in X-armed bandits. In NIPS 

2008.

[12] N. Srinivas  A. Krause  S. Kakade  and M. Seeger.

Information-theoretic regret bounds for gaussian
process optimization in the bandit setting. IEEE Transactions on Information Theory  58(5):3250–3265 
May 2012.

[13] E. Brochu  V.M. Cora  and N. De Freitas. A tutorial on bayesian optimization of expensive cost func-
tions  with application to active user modeling and hierarchical reinforcement learning. arXiv preprint
arXiv:1012.2599  2010.

[14] J. Moˇckus. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical

Conference Novosibirsk  July 1–7  1974  pages 400–404. Springer  1975.

[15] A.D. Bull. Convergence rates of efﬁcient global optimization algorithms. The Journal of Machine Learn-

ing Research  12:2879–2904  2011.

[16] A. Carpentier and R. Munos. Bandit theory meets compressed sensing for high dimensional stochastic

linear bandit. Journal of Machine Learning Research - Proceedings Track  22:190–198  2012.

[17] Y. Abbasi-Yadkori  D. Pal  and C. Szepesvari. Online-to-conﬁdence-set conversions and application to

sparse stochastic bandits. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[18] B. Chen  R. Castro  and A. Krause. Joint optimization and variable selection of high-dimensional gaussian

processes. In Proc. International Conference on Machine Learning (ICML)  2012.

[19] Z. Wang  M. Zoghi  F. Hutter  D. Matheson  and N. de Freitas. Bayesian optimization in high dimensions

via random embeddings. In In Proc. IJCAI  2013.

[20] H. Tyagi and B. G¨artner. Continuum armed bandit problem of few variables in high dimensions. CoRR 

abs/1304.5793  2013.

[21] R.A. DeVore and G.G. Lorentz. Constructive approximation  volume 303. Springer Verlag  1993.
[22] M. Fornasier  K. Schnass  and J. Vybiral. Learning functions of few arbitrary linear parameters in high

dimensions. Foundations of Computational Mathematics  pages 1–34  2012.

[23] B. Sch¨olkopf and A.J. Smola. Learning with kernels: Support vector machines  regularization  optimiza-

tion  and beyond. MIT press  2001.

[24] E.J. Candes and Y. Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number

of noisy random measurements. Information Theory  IEEE Transactions on  57(4):2342–2359  2011.

[25] P. A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical

Mathematics  12(1):99–111  1972.

[26] G.W. Stewart and J. Sun. Matrix Perturbation Theory  volume 175. Academic Press New York  1990.
[27] J. G. Daugman. Uncertainty relation for resolution in space  spatial frequency  and orientation optimized
by two-dimensional visual cortical ﬁlters. Optical Society of America  Journal  A: Optics and Image
Science  2:1160–1169  1985.

9

,Josip Djolonga
Andreas Krause
Volkan Cevher
Lifang He
Kun Chen
Wanwan Xu
Jiayu Zhou
Fei Wang