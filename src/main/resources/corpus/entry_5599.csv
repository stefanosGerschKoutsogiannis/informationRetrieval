2016,Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton  an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions  averaging  and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular  it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.,Toward Deeper Understanding of Neural Networks: The Power

of Initialization and a Dual View on Expressivity

Amit Daniely
Google Brain

Roy Frostig∗
Google Brain

Yoram Singer
Google Brain

Abstract

We develop a general duality between neural networks and compositional kernel
Hilbert spaces. We introduce the notion of a computation skeleton  an acyclic
graph that succinctly describes both a family of neural networks and a kernel space.
Random neural networks are generated from a skeleton through node replication
followed by sampling from a normal distribution to assign weights. The kernel
space consists of functions that arise by compositions  averaging  and non-linear
transformations governed by the skeleton’s graph topology and activation functions.
We prove that random networks induce representations which approximate the
kernel space. In particular  it follows that random weight initialization often yields
a favorable starting point for optimization despite the worst-case intractability of
training neural networks.

1

Introduction

Neural network (NN) learning has underpinned state of the art empirical results in numerous applied
machine learning tasks  see for instance [25  26]. Nonetheless  theoretical analyses of neural network
learning are still lacking in several regards. Notably  it remains unclear why training algorithms ﬁnd
good weights and how learning is impacted by network architecture and its activation functions.
This work analyzes the representation power of neural networks within the vicinity of random
initialization. We show that for regimes of practical interest  randomly initialized neural networks
well-approximate a rich family of hypotheses. Thus  despite worst-case intractability of training
neural networks  commonly used initialization procedures constitute a favorable starting point for
training.
Concretely  we deﬁne a computation skeleton that is a succinct description of feed-forward networks.
A skeleton induces a family of network architectures as well as an hypothesis class H of functions
obtained by non-linear compositions mandated by the skeleton’s structure. We then analyze the set of
functions that can be expressed by varying the weights of the last layer  a simple region of the training
domain over which the objective is convex. We show that with high probability over the choice of
initial network weights  any function in H can be approximated by selecting the ﬁnal layer’s weights.
Before delving into technical detail  we position our results in the context of previous research.

Current theoretical understanding of NN learning. Standard results from complexity theory [22]
imply that all efﬁciently computable functions can be expressed by a network of moderate size.
Barron’s theorem [7] states that even two-layer networks can express a very rich set of functions. The
generalization ability of algorithms for training neural networks is also fairly well studied. Indeed 
both classical [3  9  10] and more recent [18  33] results from statistical learning theory show that  as
the number of examples grows in comparison to the size of the network  the empirical risk approaches
the population risk. In contrast  it remains puzzling why and when efﬁcient algorithms  such as
stochastic gradient methods  yield solutions that perform well. While learning algorithms succeed in

∗Most of this work performed while the author was at Stanford University.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

practice  theoretical analyses are overly pessimistic. For example  hardness results suggest that  in
the worst case  even very simple 2-layer networks are intractable to learn. Concretely  it is hard to
construct a hypothesis which predicts marginally better than random [15  23  24].
In the meantime  recent empirical successes of neural networks prompted a surge of theoretical
results on NN learning. For instance  we refer the reader to [1  4  12  14  16  28  32  38  42] and the
references therein.

Compositional kernels and connections to networks. The idea of composing kernels has repeat-
edly appeared in the machine learning literature. See for instance the early work by Grauman and
Darrell [17]  Schölkopf et al. [41]. Inspired by deep networks’ success  researchers considered deep
composition of kernels [11  13  29]. For fully connected two-layer networks  the correspondence
between kernels and neural networks with random weights has been examined in [31  36  37  45].
Notably  Rahimi and Recht [37] proved a formal connection  in a similar sense to ours  for the
RBF kernel. Their work was extended to include polynomial kernels [21  35] as well as other
kernels [5  6]. Several authors have further explored ways to extend this line of research to deeper 
either fully-connected networks [13] or convolutional networks [2  20  29].

This work establishes a common foundation for the above research and expands the ideas therein. We
extend the scope from fully-connected and convolutional networks to a broad family of architectures.
In addition  we prove approximation guarantees between a network and its corresponding kernel in
our general setting. We thus generalize previous analyses which are only applicable to fully connected
two-layer networks.

2 Setting

Notation. We denote vectors by bold-face letters (e.g. x)  and matrices by upper case Greek letters
(e.g. Σ). The 2-norm of x ∈ Rd is denoted by �x�. For functions σ : R → R we let

�σ� :=�EX∼N (0 1) σ2(X) =� 1√2π� ∞

−∞

σ2(x)e− x2

2 dx .

Let G = (V  E) be a directed acyclic graph. The set of neighbors incoming to a vertex v is denoted

in(v) := {u ∈ V | uv ∈ E} .

The d − 1 dimensional sphere is denoted Sd−1 = {x ∈ Rd | �x� = 1}. We provide a brief overview
of reproducing kernel Hilbert spaces in the sequel and merely introduce notation here. In a Hilbert
space H  we use a slightly non-standard notation HB for the ball of radius B  {x ∈ H | �x�H ≤ B}.
We use [x]+ to denote max(x  0) and 1[b] to denote the indicator function of a binary variable b.

Input space. Throughout the paper we assume that each example is a sequence of n elements 
each of which is represented as a unit vector. Namely  we ﬁx n and take the input space to be

X = Xn d =�Sd−1�n. Each input example is denoted 

x = (x1  . . .   xn)  where xi ∈ Sd−1 .
We refer to each vector xi as the input’s ith coordinate  and use xi
j to denote it jth scalar entry.
Though this notation is slightly non-standard  it uniﬁes input types seen in various domains. For
example  binary features can be encoded by taking d = 1  in which case X = {±1}n. Meanwhile 
images and audio signals are often represented as bounded and continuous numerical values; we can
assume in full generality that these values lie in [−1  1]. To match the setup above  we embed [−1  1]
into the circle S1  e.g. through the map

(1)

x �→�sin� πx

2 �   cos� πx

2 �� .

When each coordinate is categorical  taking one of d values  one can represent the category j ∈ [d]
by the unit vector ej ∈ Sd−1. When d is very large or the basic units exhibit some structure—such as
when the input is a sequence of words—a more concise encoding may be useful  e.g. using unit vectors
in a low dimension space Sd� where d� � d (see for instance Levy and Goldberg [27]  Mikolov et al.
[30]).

2

Supervised learning. The goal in supervised learning is to devise a mapping from the input space
X to an output space Y based on a sample S = {(x1  y1)  . . .   (xm  ym)}  where (xi  yi) ∈ X × Y 
drawn i.i.d. from a distribution D over X × Y. A supervised learning problem is further speciﬁed
by an output length k and a loss function � : Rk × Y → [0 ∞)  and the goal is to ﬁnd a predictor
h : X → Rk whose loss 

is small. The empirical loss

LD(h) := E

(x y)∼D

�(h(x)  y)

LS(h) :=

1
m

m�i=1

�(h(xi)  yi)

is commonly used as a proxy for the loss LD. Regression problems correspond to Y = R and  for
instance  the squared loss �(ˆy  y) = (ˆy − y)2. Binary classiﬁcation is captured by Y = {±1} and 
say  the zero-one loss �(ˆy  y) = 1[ˆyy ≤ 0] or the hinge loss �(ˆy  y) = [1 − ˆyy]+  with standard
extensions to the multiclass case. A loss � is L-Lipschitz if |�(y1  y) − �(y2  y)| ≤ L|y1 − y2| for all
y1  y2 ∈ Rk  y ∈ Y  and it is convex if �(·  y) is convex for every y ∈ Y.
Neural network learning. We deﬁne a neural network N to be directed acyclic graph (DAG)
whose nodes are denoted V (N ) and edges E(N ). Each of its internal units  i.e. nodes with both
incoming and outgoing edges  is associated with an activation function σv : R → R. In this paper’s
context  an activation can be any function that is square integrable with respect to the Gaussian
measure on R. We say that σ is normalized if �σ� = 1. The set of nodes having only incoming
edges are called the output nodes. To match the setup of a supervised learning problem  a network N
has nd input nodes and k output nodes  denoted o1  . . .   ok. A network N together with a weight
vector w = {wuv | uv ∈ E} deﬁnes a predictor hN  w : X → Rk whose prediction is given by
“propagating” x forward through the network. Formally  we deﬁne hv w(·) to be the output of the
subgraph of the node v as follows: for an input node v  hv w is the identity function  and for all other
nodes  we deﬁne hv w recursively as

hv w(x) = σv��u∈in(v) wuv hu w(x)� .

Finally  we let hN  w(x) = (ho1 w(x)  . . .   hok w(x)). We also refer to internal nodes as hidden
units. The output layer of N is the sub-network consisting of all output neurons of N along with
their incoming edges. The representation induced by a network N is the network rep(N ) obtained
from N by removing the output layer. The representation function induced by the weights w is
RN  w := hrep(N ) w. Given a sample S  a learning algorithm searches for weights w having small
empirical loss LS(w) = 1
i=1 �(hN  w(xi)  yi). A popular approach is to randomly initialize the
weights and then use a variant of the stochastic gradient method to improve these weights in the
direction of lower empirical loss.

m�m

Kernel learning. A function κ : X × X → R is a reproducing kernel  or simply a kernel  if for
every x1  . . .   xr ∈ X   the r × r matrix Γi j = {κ(xi  xj)} is positive semi-deﬁnite. Each kernel
induces a Hilbert space Hκ of functions from X to R with a corresponding norm �·�Hκ. A kernel and
its corresponding space are normalized if ∀x ∈ X   κ(x  x) = 1. Given a convex loss function �  a
sample S  and a kernel κ  a kernel learning algorithm ﬁnds a function f = (f1  . . .   fk) ∈ Hk
κ whose
m�i �(f (xi)  yi)  is minimal among all functions with�i �fi�2
empirical loss  LS(f ) = 1
κ ≤ R2
for some R > 0. Alternatively  kernel algorithms minimize the regularized loss 
LR
S (f ) =

�(f (xi)  yi) +

�fi�2
κ  

1
R2

1
m

m�i=1

k�i=1

a convex objective that often can be efﬁciently minimized.

3 Computation skeletons

In this section we deﬁne a simple structure that we term a computation skeleton. The purpose of a
computational skeleton is to compactly describe feed-forward computation from an input to an output.
A single skeleton encompasses a family of neural networks that share the same skeletal structure.
Likewise  it deﬁnes a corresponding kernel space.

3

S1

S2

S3
Figure 1: Examples of computation skeletons.

S4

Deﬁnition. A computation skeleton S is a DAG whose non-input nodes are labeled by activations.
Though the formal deﬁnition of neural networks and skeletons appear identical  we make a conceptual
distinction between them as their role in our analysis is rather different. Accompanied by a set of
weights  a neural network describes a concrete function  whereas the skeleton stands for a topology
common to several networks as well as for a kernel. To further underscore the differences we note
that skeletons are naturally more compact than networks. In particular  all examples of skeletons in
this paper are irreducible  meaning that for each two nodes v  u ∈ V (S)  in(v) �= in(u). We further
restrict our attention to skeletons with a single output node  showing later that single-output skeletons
can capture supervised problems with outputs in Rk. We denote by |S| the number of non-input
nodes of S.
Figure 1 shows four example skeletons  omitting the designation of the activation functions. The
skeleton S1 is rather basic as it aggregates all the inputs in a single step. Such topology can be
useful in the absence of any prior knowledge of how the output label may be computed from an input
example  and it is commonly used in natural language processing where the input is represented as a
bag-of-words [19]. The only structure in S1 is a single fully connected layer:
Terminology (Fully connected layer of a skeleton). An induced subgraph of a skeleton with r + 1
nodes  u1  . . .   ur  v  is called a fully connected layer if its edges are u1v  . . .   urv.
The skeleton S2 is slightly more involved: it ﬁrst processes consecutive (overlapping) parts of the
input  and the next layer aggregates the partial results. Altogether  it corresponds to networks with a
single one-dimensional convolutional layer  followed by a fully connected layer. The two-dimensional
(and deeper) counterparts of such skeletons correspond to networks that are common in visual object
recognition.
Terminology (Convolution layer of a skeleton). Let s  w  q be positive integers and denote n =
s(q− 1) + w. A subgraph of a skeleton is a one dimensional convolution layer of width w and stride s
if it has n + q nodes  u1  . . .   un  v1  . . .   vq  and qw edges  us(i−1)+j vi  for 1 ≤ i ≤ q  1 ≤ j ≤ w.
The skeleton S3 is a somewhat more sophisticated version of S2: the local computations are ﬁrst
aggregated  then reconsidered with the aggregate  and ﬁnally aggregated again. The last skeleton 
S4  corresponds to the networks that arise in learning sequence-to-sequence mappings as used in
translation  speech recognition  and OCR tasks (see for example Sutskever et al. [44]).

3.1 From computation skeletons to neural networks
The following deﬁnition shows how a skeleton  accompanied with a replication parameter r ≥ 1 and
a number of output nodes k  induces a neural network architecture. Recall that inputs are ordered sets
of vectors in Sd−1.

4

S

N (S  5)

Figure 2: A 5-fold realizations of the computation skeleton S with d = 1.

Deﬁnition (Realization of a skeleton). Let S be a computation skeleton and consider input coordi-
nates in Sd−1 as in (1). For r  k ≥ 1 we deﬁne the following neural network N = N (S  r  k). For
each input node in S  N has d corresponding input neurons. For each internal node v ∈ S labeled
by an activation σ  N has r neurons v1  . . .   vr  each with an activation σ. In addition  N has k
output neurons o1  . . .   ok with the identity activation σ(x) = x. There is an edge viuj ∈ E(N )
whenever uv ∈ E(S). For every output node v in S  each neuron vj is connected to all output
neurons o1  . . .   ok. We term N the (r  k)-fold realization of S. We also deﬁne the r-fold realization
of S as2 N (S  r) = rep (N (S  r  1)).
Note that the notion of the replication parameter r corresponds  in the terminology of convolutional
networks  to the number of channels taken in a convolutional layer and to the number of hidden units
taken in a fully-connected layer.
Figure 2 illustrates a 5-realization of a skeleton with coordinate dimension d = 1. The realization is a
network with a single (one dimensional) convolutional layer having 5 channels  stride of 1  and width
of 2  followed by two fully-connected layers. The global replication parameter r in a realization
is used for brevity; it is straightforward to extend results when the different nodes in S are each
replicated to a different extent.
We next deﬁne a scheme for random initialization of the weights of a neural network  that is similar
to what is often done in practice. We employ the deﬁnition throughout the paper whenever we refer
to random weights.
Deﬁnition (Random weights). A random initialization of a neural network N is a multivariate
Gaussian w = (wuv)uv∈E(N ) such that each weight wuv is sampled independently from a normal
distribution with mean 0 and variance 1/��σu�2 |in(v)|�.

Architectures such as convolutional nets have weights that are shared across different edges. Again  it
is straightforward to extend our results to these cases and for simplicity we assume no weight sharing.

3.2 From computation skeletons to reproducing kernels
In addition to networks’ architectures  a computation skeleton S also deﬁnes a normalized kernel
κS : X × X → [−1  1] and a corresponding norm � · �S on functions f : X → R. This norm has
the property that �f�S is small if and only if f can be obtained by certain simple compositions of
functions according to the structure of S. To deﬁne the kernel  we introduce a dual activation and
dual kernel. For ρ ∈ [−1  1]  we denote by Nρ the multivariate Gaussian distribution on R2 with
mean 0 and covariance matrix� 1 ρ
ρ 1�.
Deﬁnition (Dual activation and kernel). The dual activation of an activation σ is the function
ˆσ : [−1  1] → R deﬁned as

ˆσ(ρ) =

σ(X)σ(Y ) .

The dual kernel w.r.t. to a Hilbert space H is the kernel κσ : H1 × H1 → R deﬁned as

2Note that for every k  rep (N (S  r  1)) = rep (N (S  r  k)).

κσ(x  y) = ˆσ(�x  y�H) .

E

(X Y )∼Nρ

5

Activation
Identity
2nd Hermite

ReLU
Step
Exponential

x
x2−1√2
√2 [x]+
√2 1[x ≥ 0]

ex−2

Dual Activation
ρ
ρ2
π + ρ
2 + ρ
e + ρ

2 + ρ2
π + ρ3
e + ρ2

1

1

1

√1−ρ2+(π−cos−1(ρ))ρ

2π + ρ4
6π + 3ρ5
2e + ρ3

24π + . . . =
40π + . . . = π−cos−1(ρ)
6e + . . . = eρ−1

π

π

Kernel
linear
poly

arccos1
arccos0
RBF

Ref

[13]
[13]
[29]

Table 1: Activation functions and their duals.

We show in the supplementary material that κσ is indeed a kernel for every activation σ that adheres
with the square-integrability requirement. In fact  any continuous µ : [−1  1] → R  such that
(x  y) �→ µ(�x  y�H) is a kernel for all H  is the dual of some activation. Note that κσ is normalized
iff σ is normalized. We show in the supplementary material that dual activations are closely related
to Hermite polynomial expansions  and that these can be used to calculate the duals of activation
functions analytically. Table 1 lists a few examples of normalized activations and their corresponding
dual (corresponding derivations are in supplementary material). The following deﬁnition gives the
kernel corresponding to a skeleton having normalized activations.3
Deﬁnition (Compositional kernels). Let S be a computation skeleton with normalized activations
and (single) output node o. For every node v  inductively deﬁne a kernel κv : X × X → R as follows.
For an input node v corresponding to the ith coordinate  deﬁne κv(x  y) = �xi  yi�. For a non-input
node v  deﬁne

κv(x  y) = ˆσv��u∈in(v) κu(x  y)

|in(v)|

� .

The ﬁnal kernel κS is κo  the kernel associated with the output node o. The resulting Hilbert space
and norm are denoted HS and � · �S respectively  and Hv and � · �v denote the space and norm
when formed at node v.
As we show later  κS is indeed a (normalized) kernel for every skeleton S. To understand the
kernel in the context of learning  we need to examine which functions can be expressed as moderate
norm functions in HS. As we show in the supplementary material  these are the functions obtained
by certain simple compositions according to the feed-forward structure of S. For intuition  we
contrast two examples of two commonly used skeletons. For simplicity  we restrict to the case
X = Xn 1 = {±1}n  and omit the details of derivations.
Example 1 (Fully connected skeletons). Let S be a skeleton consisting of l fully connected layers 
where the i’th layer is associated with the activation σi. We have κS (x  x�) = ˆσl ◦ . . . ◦ ˆσ1��x y�n �.
For such kernels  any moderate norm function in H is well approximated by a low degree polynomial.
For example  if �f�S ≤ n  then there is a second degree polynomial p such that �f − p�2 ≤ O� 1√n�.
We next argue that convolutional skeletons deﬁne kernel spaces that are quite different from kernels
spaces deﬁned by fully connected skeletons. Concretely  suppose f : X → R is of the form
f =�m
i=1 fi where each fi depends only on q adjacent coordinates. We call f a q-local function. In
Example 1 we stated that for fully-connected skeletons  any function of in the induced space of norm
less then n is well approximated by second degree polynomials. In contrast  the following example
underscores that for convolutional skeletons  we can ﬁnd functions that are more complex  provided
that they are local.
Example 2 (Convolutional skeletons). Let S be a skeleton consisting of a convolutional layer of
stride 1 and width q  followed by a single fully connected layer. (The skeleton S2 from Figure 1 is a
concrete example with q = 2 and n = 4.) To simplify the argument  we assume that all activations
are σ(x) = ex and q is a constant. For any q-local function f : X → R we have

�f�S ≤ C · √n · �f�2 .

3For a skeleton S with unnormalized activations  the corresponding kernel is the kernel of the skeleton S�

obtained by normalizing the activations of S.

6

Here  C > 0 is a constant depending only on q. Hence  for example  any average of functions
from X to [−1  1]  each of which depends on q adjacent coordinates  is in HS and has norm of
merely O (√n).

4 Main results

We review our main results. Proofs can be found in the supplementary material. Let us ﬁx a
compositional kernel S. There are a few upshots to underscore upfront. First  our analysis implies
that a representation generated by a random initialization of N = N (S  r  k) approximates the kernel
κS. The sense in which the result holds is twofold. First  with the proper rescaling we show that
�RN  w(x) RN  w(x�)� ≈ κS (x  x�). Then  we also show that the functions obtained by composing
bounded linear functions with RN  w are approximately the bounded-norm functions in HS. In other
words  the functions expressed by N under varying the weights of the ﬁnal layer are approximately
bounded-norm functions in HS. For simplicity  we restrict the analysis to the case k = 1. We also
conﬁne the analysis to either bounded activations  with bounded ﬁrst and second derivatives  or the
ReLU activation. Extending the results to a broader family of activations is left for future work.
Through this and remaining sections we use � to hide universal constants.
Deﬁnition. An activation σ : R → R is C-bounded if it is twice continuously differentiable and
�σ�∞ �σ��∞ �σ���∞ ≤ �σ�C.
Note that many activations are C-bounded for some constant C > 0. In particular  most of the popular
sigmoid-like functions such as 1/(1 + e−x)  erf(x)  x/√1 + x2  tanh(x)  and tan−1(x) satisfy the
boundedness requirements. We next introduce terminology that parallels the representation layer
of N with a kernel space. Concretely  let N be a network whose representation part has q output
neurons. Given weights w  the normalized representation Ψw is obtained from the representation
RN  w by dividing each output neuron v by �σv�√q. The empirical kernel corresponding to w is
deﬁned as κw(x  x�) = �Ψw(x)  Ψw(x�)�. We also deﬁne the empirical kernel space corresponding
to w as Hw = Hκw. Concretely 

Hw = {hv(x) = �v  Ψw(x)� | v ∈ Rq}  

and the norm of Hw is deﬁned as �h�w = inf{�v� | h = hv}. Our ﬁrst result shows that the
empirical kernel approximates the kernel kS.
Theorem 3. Let S be a skeleton with C-bounded activations. Let w be a random initialization of
N = N (S  r) with

(4C 4)depth(S)+1 log (8|S|/δ)

.

�2

r ≥

Then  for all x  x� ∈ X   with probability of at least 1 − δ 

|kw(x  x�) − kS (x  x�)| ≤ � .

We note that if we ﬁx the activation and assume that the depth of S is logarithmic  then the required
bound on r is polynomial. For the ReLU activation we get a stronger bound with only quadratic
dependence on the depth. However  it requires that � ≤ 1/depth(S).
Theorem 4. Let S be a skeleton with ReLU activations. Let w be a random initialization of N (S  r)
with

Then  for all x  x� ∈ X and � � 1/depth(S)  with probability of at least 1 − δ 

r � depth2(S) log (|S|/δ)

.

�2

|κw(x  x�) − κS (x  x�)| ≤ � .

For the remaining theorems  we ﬁx a L-Lipschitz loss � : R × Y → [0 ∞). For a distribution D on
X × Y we denote by �D�0 the cardinality of the support of the distribution. We note that log (�D�0)
is bounded by  for instance  the number of bits used to represent an element in X × Y. We use the
following notion of approximation.
Deﬁnition. Let D be a distribution on X ×Y. A space H1 ⊂ RX �-approximates the space H2 ⊂ RX
w.r.t. D if for every h2 ∈ H2 there is h1 ∈ H1 such that LD(h1) ≤ LD(h2) + �.

7

Theorem 5. Let S be a skeleton with C-bounded activations and let R > 0. Let w be a random
initialization of N (S  r) with

�δ �
L4 R4 (4C 4)depth(S)+1 log� LRC|S|

�4

.

r �

�-approximates HR
S

Then  with probability of at least 1 − δ over the choices of w we have that  for any data distribution 
√2R
H
w
Theorem 6. Let S be a skeleton with ReLU activations  � � 1/depth(C)  and R > 0. Let w be a
random initialization of N (S  r) with

and H

√2R
S

�-approximates HR
w.
L4 R4 depth2(S) log��D�0|S|
�-approximates HR
w.

�4

δ

�

.

r �

�-approximates HR
S

Then  with probability of at least 1 − δ over the choices of w we have that  for any data distribution 
√2R
H
w
As in Theorems 3 and 4  for a ﬁxed C-bounded activation and logarithmically deep S  the required
bounds on r are polynomial. Analogously  for the ReLU activation the bound is polynomial even
without restricting the depth. However  the polynomial growth in Theorems 5 and 6 is rather large.
Improving the bounds  or proving their optimality  is left to future work.

and H

√2R
S

Acknowledgments

We would like to thank Percy Liang and Ben Recht for fruitful discussions  comments  and sugges-
tions.

References
[1] A. Andoni  R. Panigrahy  G. Valiant  and L. Zhang. Learning polynomials with neural networks. In

Proceedings of the 31st International Conference on Machine Learning  pages 1908–1916  2014.

[2] F. Anselmi  L. Rosasco  C. Tan  and T. Poggio. Deep convolutional networks are hierarchical kernel

machines. arXiv:1508.01084  2015.

[3] M. Anthony and P. Bartlet. Neural Network Learning: Theoretical Foundations. Cambridge University

Press  1999.

[4] S. Arora  A. Bhaskara  R. Ge  and T. Ma. Provable bounds for learning some deep representations. In

Proceedings of The 31st International Conference on Machine Learning  pages 584–592  2014.

[5] F. Bach. Breaking the curse of dimensionality with convex neural networks. arXiv:1412.8690  2014.
[6] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. 2015.
[7] A.R. Barron. Universal approximation bounds for superposition of a sigmoidal function. IEEE Transactions

on Information Theory  39(3):930–945  1993.

[8] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural

results. Journal of Machine Learning Research  3:463–482  2002.

[9] P.L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of the weights
is more important than the size of the network. IEEE Transactions on Information Theory  44(2):525–536 
March 1998.

[10] E.B. Baum and D. Haussler. What size net gives valid generalization? Neural Computation  1(1):151–160 

1989.

[11] L. Bo  K. Lai  X. Ren  and D. Fox. Object recognition with hierarchical kernel descriptors. In Computer

Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on  pages 1729–1736. IEEE  2011.

[12] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Transactions on Pattern Analysis

and Machine Intelligence  35(8):1872–1886  2013.

[13] Y. Cho and L.K. Saul. Kernel methods for deep learning. In Advances in neural information processing

systems  pages 342–350  2009.

[14] A. Choromanska  M. Henaff  M. Mathieu  G. Ben Arous  and Y. LeCun. The loss surfaces of multilayer

networks. In AISTATS  pages 192–204  2015.

[15] A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning DNFs. In COLT  2016.
[16] R. Giryes  G. Sapiro  and A.M. Bronstein. Deep neural networks with random gaussian weights: A

universal classiﬁcation strategy? arXiv preprint arXiv:1504.08291  2015.

8

[17] K. Grauman and T. Darrell. The pyramid match kernel: Discriminative classiﬁcation with sets of image
features. In Tenth IEEE International Conference on Computer Vision  volume 2  pages 1458–1465  2005.
[18] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient descent.

arXiv:1509.01240  2015.

[19] Z.S. Harris. Distributional structure. Word  1954.
[20] T. Hazan and T. Jaakkola.

arXiv:1508.05133  2015.

Steps toward deep kernel methods from inﬁnite neural networks.

[21] P. Kar and H. Karnick. Random feature maps for dot product kernels. arXiv:1201.6530  2012.
[22] R.M. Karp and R.J. Lipton. Some connections between nonuniform and uniform complexity classes. In
Proceedings of the twelfth annual ACM symposium on Theory of computing  pages 302–309. ACM  1980.
[23] M. Kearns and L.G. Valiant. Cryptographic limitations on learning Boolean formulae and ﬁnite automata.

In STOC  pages 433–444  May 1989.

[24] A.R. Klivans and A.A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. In FOCS 

2006.

[25] A. Krizhevsky  I. Sutskever  and G.E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in neural information processing systems  pages 1097–1105  2012.

[26] Y. LeCun  Y. Bengio  and G. Hinton. Deep learning. Nature  521(7553):436–444  2015.
[27] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural

Information Processing Systems  pages 2177–2185  2014.

[28] R. Livni  S. Shalev-Shwartz  and O. Shamir. On the computational efﬁciency of training neural networks.

In Advances in Neural Information Processing Systems  pages 855–863  2014.

[29] J. Mairal  P. Koniusz  Z. Harchaoui  and Cordelia Schmid. Convolutional kernel networks. In Advances in

Neural Information Processing Systems  pages 2627–2635  2014.

[30] T. Mikolov  I. Sutskever  K. Chen  G.S. Corrado  and J. Dean. Distributed representations of words and

phrases and their compositionality. In NIPS  pages 3111–3119  2013.

[31] R.M. Neal. Bayesian learning for neural networks  volume 118. Springer Science & Business Media 

2012.

[32] B. Neyshabur  R. R Salakhutdinov  and N. Srebro. Path-SGD: Path-normalized optimization in deep neural

networks. In Advances in Neural Information Processing Systems  pages 2413–2421  2015.

[33] B. Neyshabur  N. Srebro  and R. Tomioka. Norm-based capacity control in neural networks. In COLT 

2015.

[34] R. O’Donnell. Analysis of boolean functions. Cambridge University Press  2014.
[35] J. Pennington  F. Yu  and S. Kumar. Spherical random features for polynomial kernels. In Advances in

Neural Information Processing Systems  pages 1837–1845  2015.

[36] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  pages 1177–1184 

2007.

[37] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with random-

ization in learning. In Advances in neural information processing systems  pages 1313–1320  2009.

[38] I. Safran and O. Shamir. On the quality of the initial basin in overspeciﬁed neural networks.

arxiv:1511.04210  2015.

[39] S. Saitoh. Theory of reproducing kernels and its applications. Longman Scientiﬁc & Technical England 

1988.

[40] I.J. Schoenberg et al. Positive deﬁnite functions on spheres. Duke Mathematical Journal  9(1):96–108 

1942.

[41] B. Schölkopf  P. Simard  A. Smola  and V. Vapnik. Prior knowledge in support vector kernels. In Advances

in Neural Information Processing Systems 10  pages 640–646. MIT Press  1998.

[42] H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectivity.

arXiv:1412.2693  2014.

[43] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.

Cambridge University Press  2014.

[44] I. Sutskever  O. Vinyals  and Q.V. Le. Sequence to sequence learning with neural networks. In Advances

in neural information processing systems  pages 3104–3112  2014.

[45] C.K.I. Williams. Computation with inﬁnite neural networks. pages 295–301  1997.

9

,Amit Daniely
Roy Frostig
Yoram Singer
Robert Hannah
Yanli Liu
Daniel O'Connor
Wotao Yin