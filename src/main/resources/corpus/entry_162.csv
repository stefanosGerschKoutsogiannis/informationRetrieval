2012,A Polynomial-time Form of Robust Regression,Despite the variety of robust regression methods that have been developed  current regression formulations are either NP-hard  or allow unbounded response to even a single leverage point. We present a general formulation for robust regression --Variational M-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy. We develop an estimator that requires only polynomial-time  while achieving certain robustness and consistency guarantees. An experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.,A Polynomial-time Form of Robust Regression

Department of Computing Science  University of Alberta  Edmonton AB T6G 2E8  Canada

Yaoliang Yu  ¨Ozlem Aslan and Dale Schuurmans

{yaoliang ozlem dale}@cs.ualberta.ca

Abstract

Despite the variety of robust regression methods that have been developed  cur-
rent regression formulations are either NP-hard  or allow unbounded response
to even a single leverage point. We present a general formulation for robust
regression—Variational M-estimation—that uniﬁes a number of robust regression
methods while allowing a tractable approximation strategy. We develop an esti-
mator that requires only polynomial-time  while achieving certain robustness and
consistency guarantees. An experimental evaluation demonstrates the effective-
ness of the new estimation approach compared to standard methods.

Introduction

1
It is well known that outliers have a detrimental effect on standard regression estimators. Even a
single erroneous observation can arbitrarily affect the estimates produced by methods such as least
squares. Unfortunately  outliers are prevalent in modern data analysis  as large data sets are auto-
matically gathered without the beneﬁt of manual oversight. Thus the need for regression estimators
that are both scalable and robust is increasing.
Although the ﬁeld of robust regression is well established  it has not considered computational com-
plexity analysis to be one of its central concerns. Consequently  none of the standard regression
estimators in the literature are both robust and tractable  even in a weak sense: it has been shown
that standard robust regression formulations with non-zero breakdown are NP-hard [1  2]  while
any estimator based on minimizing a convex loss cannot guarantee bounded response to even a sin-
gle leverage point [3] (deﬁnitions given below). Surprisingly  there remain no standard regression
formulations that guarantee both polynomial run-time with bounded response to even single outliers.
It is important to note that robustness and tractability can be achieved under restricted conditions. For
example  if the domain is bounded  then any estimator based on minimizing a convex and Lipschitz-
continuous loss achieves high breakdown [4]. Such results have been extended to kernel-based
regression under the analogous assumption of a bounded kernel [5  6]. Unfortunately  these results
can no longer hold when the domain or kernel is unbounded: in such a case arbitrary leverage can
occur [4  7] and no (non-constant) convex loss  even Lipschitz-continuous  can ensure robustness
against even a single outlier [3]. Our main motivation therefore is to extend these existing results
to the case of an unbounded domain. Unfortunately  the inapplicability of convex losses in this
situation means that computational tractability becomes a major challenge  and new computational
strategies are required to achieve tractable robust estimators.
The main contribution of this paper is to develop a new robust regression strategy that can guarantee
both polynomial run-time and bounded response to individual outliers  including leverage points.
Although such an achievement is modest  it is based on two developments of interest. The ﬁrst
is a general formulation of adaptive M-estimation  Variational M-estimation  that uniﬁes a number
of robust regression formulations  including convex and bounded M-estimators with certain subset-
selection estimators such as Least Trimmed Loss [7]. By incorporating Tikhonov regularization 
these estimators can be extended to reproducing kernel Hilbert spaces (RKHSs). The second devel-
opment is a convex relaxation scheme that ensures bounded outlier inﬂuence on the ﬁnal estimator.

1

The overall estimation procedure is guaranteed to be tractable  robust to single outliers with un-
bounded leverage  and consistent under non-trivial conditions. An experimental evaluation of the
proposed estimator demonstrates effective performance compared to standard robust estimators.
The closest previous works are [8]  which formulated variational representations of certain robust
losses  and [9]  which formulated a convex relaxation of bounded loss minimization. Unfortunately 
[8] did not offer a general characterization  while [9] did not prove their ﬁnal estimator was robust 
nor was any form of consistency established. The formulation we present in this paper generalizes
[8] while the convex relaxation scheme we propose is simpler and tighter than [9]; we are thus able
to establish non-trivial forms of both robustness and consistency while maintaining tractability.
There are many other notions of “robust” estimation in the machine learning literature that do not
correspond to the speciﬁc notion being addressed in this paper. Work on “robust optimization” [10–
12]  for example  considers minimizing the worst case loss achieved given bounds on the maximum
data deviation that will be considered. Such results are not relevant to the present investigation be-
cause we explicitly do not bound the magnitude of the outliers. Another notion of robustness is
algorithmic stability under leave-one-out perturbation [13]  which analyzes speciﬁc learning proce-
dures rather than describing how a stable algorithm might be generally achieved.

2 Preliminaries

We start by considering the standard linear regression model

y = xT θ∗

+ u

(1)
where x is an Rp-valued random variable  u is a real-valued random noise term  and θ∗
∈ Θ ⊆ Rp
is an unknown deterministic parameter vector. Assume we are given a sample of n independent
identically distributed (i.i.d.) observations represented by an n × p matrix X and an n × 1 vector
y  where each row Xi: is drawn from some unknown marginal probability measure Px  and yi are
generated according to (1). Our task is to estimate the unknown deterministic parameter θ∗
∈ Θ.
Clearly  this is a well-studied problem in statistics and machine learning. If the noise distribution
has a known density p(·)  then a standard estimator is given by maximum likelihood

n(cid:80)n
where ri = yi − Xi:θ is the ith residual. When the noise distribution is unknown  one can replace
the negative log-likelihood with a loss function ρ(·) and use the estimator

i=1 − log p(yi − Xi:θ) = arg min
θ∈Θ

ˆθML ∈ arg min
θ∈Θ

i=1 − log p(ri) 

n(cid:80)n

(2)

1

1

ˆθM ∈ arg min
θ∈Θ

1

n 1T ρ(y − Xθ) 

(3)

ual  hence 1T ρ(r) =(cid:80)n

where ρ(r) denotes the vector of losses obtained by applying the loss componentwise to each resid-
i=1 ρ(ri). Such a procedure is known as M-estimation in the robust statis-

tics literature  and empirical risk minimization in the machine learning literature.1
Although uncommon in robust regression  it is conventional in machine learning to include a regu-
larizer. In particular we will use Tikhonov (“ridge”) regularization by adding a squared penalty

2

1

2(cid:107)θ(cid:107)2

for λ ≥ 0 

ˆθMR ∈ arg min
θ∈Θ

n 1T ρ(y − Xθ) + λ

(4)
The signiﬁcance of Tikhonov regularization is that it ensures ˆθMR = X T α for some α ∈ Rn
[14]. More generally  under Tikhonov regularization  the regression problem can be conveniently
expressed in a reproducing kernel Hilbert space (RKHS). If we let H denote the RKHS correspond-
ing to positive semideﬁnite kernel κ : X × X → R  then f (x) = (cid:104)κ(x ·)  f(cid:105)H for any f ∈ H by
the reproducing property [14  15]. We consider the generalized regression model
(5)
where x is an X -valued random variable  u is a real-valued random noise term as above  and f∗
∈ H
is an unknown deterministic function. Given a sample of n i.i.d. observations (x1  y1)  ...  (xn  yn) 
1 Generally one has to introduce an additional scale parameter σ and allow rescaling of the residuals via

y = f

(x) + u

∗

ri/σ  to preserve parameter equivariance [3  4]. However  we will initially assume a known scale.

2

ˆαMR ∈ arg min

α

i=1 αiκ(xi  x)

where each xi is drawn from some unknown marginal probability measure Px  and yi are generated
according to (5) 2 the task is then to estimate the unknown deterministic function f∗
∈ H. To do so
we can express the estimator (4) more generally as
(6)

1

ˆfMR ∈ arg min
f∈H

2(cid:107)f(cid:107)2H.

2 αT Kα such that Kij = κ(xi  xj).

n(cid:80)n
i=1 ρ(yi − f (xi)) + λ
n 1T ρ(y − Kα) + λ

1

By the representer theorem [14]  the solution to (6) can be expressed by ˆfMR(x) =(cid:80)n
for some α ∈ Rn  and therefore (6) can be recovered by solving the ﬁnite dimensional problem
(7)
Our interest is understanding the tractability  robustness and consistency aspects of such estimators.
Consistency: Much is known about the consistency properties of estimators expressed as regular-
ized empirical risk minimizers. For example  the ML-estimator (2) and the M-estimator (3) are both
known to be parameter consistent under general conditions [16].3 The regularized M-estimator in
RKHSs (6)  is loss consistent under some general assumptions on the kernel  loss and training dis-
tribution.4 Furthermore  a weak form of f-consistency has also established in [6]. For bounded
kernel and bounded Lipschitz losses  one can similarly prove the loss consistency of the regularized
M-estimator (6) (in RKHS). See Appendix C.1 of the supplement for more discussion.
Generally speaking  any estimator that can be expressed as a regularized empirical loss minimization
is consistent under “reasonable” conditions. That is  one can consider regularized loss minimization
to be a (generally) sound principle for formulating regression estimators  at least from the perspective
of consistency. However  this is no longer the case when we consider robustness and tractability;
here sharp distinctions begin to arise within this class of estimators.
Robustness: Although robustness is an intuitive notion  it has not been given a unique technical
deﬁnition in the literature. Several deﬁnitions have been proposed  with distinct advantages and
disadvantages [4]. Some standard deﬁnitions consider the asymptotic invariance of estimators to
an inﬁnitesimal but arbitrary perturbation of the underlying distribution  e.g. the inﬂuence function
[4  17]. Although these analyses can be useful  we will focus on ﬁnite sample notions of robustness
since these are most related to concerns of computational tractability. In particular  we focus on the
following deﬁnition related to the ﬁnite sample breakdown point [18  19].
Deﬁnition 1 (Bounded Response). Assuming the parameter set Θ is metrizable  an estimator has
bounded response if for any ﬁnite data sample its output remains in a bounded interior subset of the
closed parameter set Θ (or respectively H)  no matter how a single observation pair is perturbed.
This is a much weaker deﬁnition than having a non-zero breakdown point: a breakdown of  re-
quires that bounded response be guaranteed when any  fraction of the data is perturbed arbitrarily.
Bounded response is obviously a far more modest requirement. However  importantly  the deﬁnition
of bounded response allows the possibility of arbitrary leverage; that is  no bound is imposed on the
magnitude of a perturbed input (i.e. (cid:107)x1(cid:107) → ∞ or κ(x1  x1) → ∞). Surprisingly  we ﬁnd that even
such a weak robustness property is difﬁcult to achieve while retaining computational tractability.
Computational Dilemma: The goals of robustness and computational tractability raise a dilemma:
it is easy to achieve robustness (i.e. bounded response) or tractability (i.e. polynomial run-time) in a
consistent estimator  but apparently not both.
Consider  for example  using a convex loss function. These are the best known class of functions
that admit computationally efﬁcient polynomial-time minimization [20] (see also [21] ). It is suf-
ﬁcient that the objective be polynomial-time evaluable  along with its ﬁrst and second derivatives 
2 We are obviously assuming X is equipped with an appropriate σ-algebra  and R with the standard Borel

(cid:80)n
i θ)  let M (θ) = E(ρ(y1 − xT
i=1 ρ(yi − xT
1 θ))  and equip the parameter
space Θ with the uniform metric (cid:107) · (cid:107)Θ. Then ˆθ(n)
M → θ∗  provided (cid:107)Mn − M(cid:107)Θ → 0 in outer probability
(adopted to avoid measurability issues) and M (θ∗) > supθ∈G M (θ) for every open set G that contains θ∗.
The latter assumption is satisﬁed in particular when M : Θ (cid:55)→ R is upper semicontinuous with a unique
(cid:80)n
maximum at θ∗. It is also possible to derive asymptotic convergence rates for general M-estimators [16].
i=1 ρ(yi − ˆfMR(xi)) → ρ∗
provided the regularization constant λn → 0 and λ2
nn → ∞  the loss ρ is convex and Lipschitz-continuous 
and the RKHS H (induced by some bounded measurable kernel κ) is separable and dense in L1(P) (the space
of P-integrable functions) for all distributions P on X . Also  Y ⊂ R is required to be closed where y ∈ Y.

σ-algebra  such that the joint distribution P over X × R is well deﬁned and κ(· ·) is measurable.

4 Speciﬁcally  let ρ∗ = inf f∈H E[ρ(y1 − f (x1))]. Then [6] showed that 1

3 In particular  let Mn(θ) = 1
n

n

3

and that the objective be self-concordant [20].5 Since a Tikhonov regularizer is automatically self-
concordant  the minimization problems outlined above can all be solved in polynomial time with
Newton-type algorithms  provided ρ(r)  ρ(cid:48)(r)  and ρ(cid:48)(cid:48)(r) can all be evaluated in polynomial time
for a self-concordant ρ [22  Ch.9]. Standard loss functions  such as squared error or Huber’s loss
satisfy these conditions  hence the corresponding estimators are polynomial-time.
Unfortunately  loss minimization with a (non-constant) convex loss yields unbounded response to
even a single outlier [3  Ch.5]. We extend this result to also account for regularization and RKHSs.
Theorem 1. Empirical risk minimization based on a (non-constant) convex loss cannot have
bounded response if the domain (or kernel) is unbounded  even under Tikhonov regularization.
(Proof given in Appendix B of the supplement.)

By contrast  consider the case of a (non-constant) bounded loss function.6 Bounded loss functions
are a common choice in robust regression because they not only ensure bounded response  trivially 
they can also ensure a high breakdown point of (n − p)/(2n) [3  Ch.5]. Unfortunately  estimators
based on bounded losses are inherently intractable.
Theorem 2. Bounded (non-constant) loss minimization is NP-hard. (Proof given in Appendix E.)

These difﬁculties with empirical risk minimization have led the ﬁeld of robust statistics to develop
a variety of alternative estimators [4  Ch.7]. For example  [7] recommends subset-selection based
regression estimators  such as Least Trimmed Loss:

(8)
Here r[i] denotes sorted residuals r[1] ≤ ··· ≤ r[n] and n(cid:48) < n is the number of terms to consider.
Traditionally ρ(r) = r2 is used. These estimators are known to have high breakdown [7] 7 and
obviously demonstrate bounded response to single outliers. Unfortunately  (8) is NP-hard [1].

ˆθLTL ∈ arg minθ∈Θ(cid:80)n

(cid:48)
i=1 ρ(r[i]).

3 Variational M-estimation

To address the dilemma  we ﬁrst adopt a general form of adaptive M-estimator that allows ﬂexibility
while allowing a general approximation strategy. The key construction is a variational representation
of M-estimation that can express a number of standard robust (and non-robust) methods in a common
framework. In particular  consider the following adaptive form of loss function

ρ(r) = min
0≤η≤1

η(cid:96)(r) + ψ(η).

(9)

where r is a residual value  (cid:96) is a closed convex base loss  η is an adaptive weight on the base loss 
and ψ is a convex auxiliary function. The weight can choose to ignore the base loss if (cid:96)(r) is large 
but this is balanced against a prior penalty ψ(η). Different choices of base loss and auxiliary function
will yield different results  and one can represent a wide variety of loss functions ρ in this way [8].
For example  any convex loss ρ can be trivially represented in the form (9) by setting (cid:96) = ρ  and
ψ(η) = δ{1}(η).8 Bounded loss functions can also be represented in this way  for example
ψ(η) = (√η − 1)2
ψ(η) = (√η − 1)2

ρ(r) = r2
1+r2
|r|
1+|r|

(Geman-McClure) [8]

(Geman-Reynolds) [8]
(LeClerc) [8]
(Clipped-loss) [9]

ρ(r) =
ρ(r) = 1 − exp(−(cid:96)(r))
ρ(r) = max(1  (cid:96)(r))

(cid:96)(r) = r2
(cid:96)(r) = |r|
(cid:96)(·) convex ψ(η) = η log η − η + 1
(cid:96)(·) convex ψ(η) = 1 − η.

(10)

(11)
(12)
(13)

Appendix D in the supplement demonstrates how one can represent general functions ρ in the form
(9)  not just speciﬁc examples  signiﬁcantly extending [8] with a general characterization.

5 A function ρ is self-concordant if |ρ(cid:48)(cid:48)(cid:48)(r)| ≤ 2ρ(cid:48)(cid:48)(r)3/2; see e.g. [22  Ch.9].
6 A bounded function obviously cannot be convex over an unbounded domain unless it is constant.
7 When n(cid:48) approaches n/2 the breakdown of (8) approaches 1/2 [7].
8 We use δC (η) to denote the indicator for the point set C; i.e.  δC (η) = 0 if η ∈ C  otherwise δC (η) = ∞.

4

Therefore  all of the previous forms of regularized empirical risk minimization  whether with a
convex or bounded loss ρ  can be easily expressed using only convex base losses (cid:96) and convex
auxiliary functions ψ  as follows
ˆθVM ∈ arg min
min
0≤η≤1
θ∈Θ
ˆfVM ∈ arg min
f∈H min
ˆαVM ∈ arg min
min
0≤η≤1

i=1 {ηi(cid:96)(yi − f (xi)) + ψ(ηi)} + λ

ηT (cid:96)(y − Kα) + 1T ψ(η) + λ

ηT (cid:96)(y − Xθ) + 1T ψ(η) + λ

0≤η≤1(cid:80)n

2(cid:107)η(cid:107)1αT Kα.

2(cid:107)η(cid:107)1(cid:107)f(cid:107)2H

2(cid:107)η(cid:107)1(cid:107)θ(cid:107)2

(14)

(15)

(16)

α

2

Note that we have added a regularizer (cid:107)η(cid:107)1/n  which increases robustness by encouraging η weights
to prefer small values (but adaptively increase on indices with small loss). This particular form of
regularization has two advantages: (i) it is a smooth function of η on 0 ≤ η ≤ 1 (since (cid:107)η(cid:107)1 = 1T η
in this case)  and (ii) it enables a tight convex approximation strategy  as we will see below.
Note that other forms of robust regression can be expressed in a similar framework. For example 
generalized M-estimation (GM-estimation) can be formulated simply by forcing each ηi to take on
a speciﬁc value determined by (cid:107)xi(cid:107) or ri [7]  ignoring the auxilary function ψ. Least Trimmed Loss
(8) can be expressed in the form (9) provided only that we add a shared constraint over η:

min

ˆθLT L ∈ arg min
θ∈Θ

0≤η≤1:1T η=n(cid:48) ηT (cid:96)(r) + ψ(η)

(17)
where ψ(ηi) = 1 − ηi and n(cid:48) < n speciﬁes the number of terms to consider in the sum of losses.
Since η ∈ {0  1}n at a solution (see e.g. [9])  (17) is equivalent to (8) if ψ is the clipped loss (13).
These formulations are all convex in the parameters given the auxiliary weights  and vice versa.
However  they are not jointly convex in the optimization variables (i.e. in θ and η  or in α and η).
Therefore  one is not assured that the problems (14)–(16) have only global minima; in fact local
minima exist and global minima cannot be easily found (or even veriﬁed).

4 Computationally Efﬁcient Approximation

We present a general approximation strategy for the variational regression estimators above that can
guarantee polynomial run-time while ensuring certain robustness and consistency properties. The
approximation is signiﬁcantly tighter than the existing work [9]  which allows us to achieve stronger
guarantees while providing better empirical performance. In developing our estimator we follow
standard methodology from combinatorial optimization: given an intractable optimization problem 
ﬁrst formulate a (hopefully tight) convex relaxation that provides a lower bound on the objective 
then round the relaxed minimizer back to the feasible space  hopefully verifying that the rounded
solution preserves desirable properties  and ﬁnally re-optimize the rounded solution to reﬁne the
result; see e.g. [23].
To maintain generality  we formulate the approximate estimator in the RKHS setting. Consider (16).
Although the problem is obviously convex in α given η  and vice versa  it is not jointly convex (recall
the assumption that (cid:96) and ψ are both convex functions). This suggests that an obvious computational
strategy for computing the estimator (16) is to alternate between α and η optimizations (or use
heuristic methods [2])  but this cannot guarantee anything other than local solutions (and thus may
not even achieve any of the desired theoretical properties associated with the estimator).
Reformulation: We ﬁrst need to reformulate the problem to allow a tight relaxation. Let ∆(η)
denote putting a vector η on the main diagonal of a square matrix  and let ◦ denote componentwise
multiplication. Since (cid:96) is closed and convex by assumption  we know that (cid:96)(r) = supν νr− ν(cid:96)∗(ν) 
where (cid:96)∗ is the Fenchel conjugate of (cid:96) [22]. This allows (16) to be reformulated as follows.
Lemma 1. min
(18)
0≤η≤1
= min
0≤η≤1

ηT (cid:96)(y − Kα) + 1T ψ(η) + λ
∗
1T ψ(η) − ηT ((cid:96)
sup

(ν) − ∆(y)ν) − 1

2(cid:107)η(cid:107)1αT Kα

(19)

min

−1

α

ν

2λ νT(cid:0)K ◦ (η(cid:107)η(cid:107)

1 ηT )(cid:1) ν 

where the function evaluations are componentwise. (Proof given in Appendix A of the supplement.)

Although no relaxation has been introduced  the new form (25) has a more convenient structure.

5

min
N∈Nη
min
M∈Mη

sup

ν

sup

ν

Nη = {N : N (cid:60) 0  N 1 = η  rank(N ) = 1}
Mη = {M : M (cid:60) 0  M 1 = η  tr(M ) ≤ 1}.

Relaxation: Let N = η(cid:107)η(cid:107)
useful properties. We can summarize these by formulating a constraint set N ∈ Nη given by:

−1
1 ηT and note that  since 0 ≤ η ≤ 1  N must satisfy a number of
(20)
(21)
Unfortunately  the set Nη is not convex because of the rank constraint. However  relaxing this
constraint leads to a set Mη ⊇ Nη which preserves much of the key structure  as we verify below.
(22)
Lemma 2. (25) = min
0≤η≤1
≥ min
0≤η≤1

∗
(ν) − ∆(y)ν) − 1
1T ψ(η) − ηT ((cid:96)
∗
(ν) − ∆(y)ν) − 1
1T ψ(η) − ηT ((cid:96)
using the fact that Nη ⊆ Mη. (Proof given in Appendix A of the supplement.)
Crucially  the constraint set {(η  M ) : 0 ≤ η ≤ 1  M ∈ Mη} is jointly convex in η and M  thus
(35) is a convex-concave min-max problem. To see why  note that the inner objective function is
jointly convex in η and M  and concave in ν. Since a pointwise maximum of convex functions is
convex  the problem is convex in (η  M ) [22  Ch.3]. We conclude that all local minima in (η  M )
are global. Therefore  (35) provides the foundation for an efﬁciently solvable relaxation.
Rounding: Unfortunately the solution to M in (35) does not allow direct recovery of an estimator
α achieving the same objective value in (24)  unless M satisﬁes rank(M ) = 1. In general we ﬁrst
need to round M to a rank 1 solution. Fortunately  a trivial rounding procedure is available: we
simply use η (ignoring M) and re-solve for α in (24). This is equivalent to replacing M with the
−1
rank 1 matrix ˜N = η(cid:107)η(cid:107)
1 ηT ∈ Nη  which restores feasibility in the original problem. Of course 
such a rounding step will generally increase the objective value.
Reoptimization: Finally  the rounded solution can be locally improved by alternating between η
and α updates in (24) (or using any other local optimization method)  yielding the ﬁnal estimate ˜α.

2λ νT (K ◦ N ) ν
2λ νT (K ◦ M ) ν.

(23)

5 Properties

Although a tight a priori bound on the size of the optimality gap is difﬁcult to achieve  a rigorous
bound on the optimality gap can be recovered post hoc once the re-optimized estimator is computed.
Let R0 denote the minimum value of (24) (not efﬁciently computable); let R1 denote the minimum
value of (35) (the relaxed solution); let R2 denote the value of (24) achieved by freezing η from
the relaxed solution but re-optimizing α (the rounded solution); and ﬁnally let R3 denote the value
of (24) achieved by re-optimizing η and α from the rounded solution (the re-optimized solution).
Clearly we have the relationships R1 ≤ R0 ≤ R3 ≤ R2. An upper bound on the relative optimality
gap of the ﬁnal solution (R3) can be determined by (R3 − R0)/R3 ≤ (R3 − R1)/R3  since R1 and
R3 are both known quantities.
Tractability: Under mild assumptions on (cid:96) and ψ  computation of the approximate estimator (solv-
ing the relaxed problem  rounding  then re-optimizing) admits a polynomial-time solution; see Ap-
pendix E in the supplement. (Appendix E also provides details for an efﬁcient implementation for
solving (35).) Once η is recovered from the relaxed solution  the subsequent optimizations of (24)
can be solved efﬁciently under weak assumptions about (cid:96) and ψ; namely that they both satisfy the
self-concordance and polynomial-time computation properties discussed in Section 2.
Robustness: Despite the approximation  the relaxation remains sufﬁciently tight to preserve some
of the robustness properties of bounded loss minimization. To establish the robustness (and consis-
tency) properties  we will need to make use of a speciﬁc technical deﬁnition of outliers and inliers.
Deﬁnition 2 (Outliers and Inliers). For an L-Lipschitz loss (cid:96)  an outlier is a point (xi  yi) that
satisﬁes (cid:96)(yi) > L2Kii/(2λ) − ψ(cid:48)(0)  while an inlier satisﬁes (cid:96)(yi) + L2Kii/(2λ) < −ψ(cid:48)(1).
Theorem 3. Assume the loss ρ is bounded and has a variational representation (9) such that (cid:96)
is Lipschitz-continuous and ψ(cid:48) is bounded. Also assume there is at least one (unperturbed) inlier 
and consider the perturbation of a single data point (y1  x1). Under the following conditions  the
rounded (re-optimized) estimator maintains bounded response:
(i) If either y1 remains bounded  or κ(x1  x1) remains bounded.
(ii) If |y1| → ∞  κ(x1  x1) → ∞ and (cid:96)(y1)/κ(x1  x1) → ∞.
(Proof given in Appendix B of the supplement.)

6

Methods

L2
L1

Huber
LTS

GemMc

[9]

AltBndL2
AltBndL1
CvxBndL2
CvxBndL1

43.5
4.89
4.89
6.72
0.53
0.52
0.52
0.73
0.52
0.53

(13)
(2.81)
(2.81)
(7.37)
(0.03)
(0.01)
(0.01)
(0.12)
(0.01)
(0.02)

57.6
3.6
3.62
8.65
0.52
0.52
0.52
0.74
0.52
0.55

Outlier Probability

p = 0.4

p = 0.2

p = 0.0

(21.21)
(2.04)
(2.02)
(14.11)
(0.02)
(0.01)
(0.01)
(0.16)
(0.01)
(0.05)

0.52
0.52
0.52
0.52
0.52
0.52
0.52
0.52
0.52
0.52

(0.01)
(0.01)
(0.01)
(0.01)
(0.01)
(0.01)
(0.02)
(0.01)
(0.01)
(0.01)

Table 1: RMSE on clean test data for an artiﬁcial data set with 5 features and 100 training points 
with outlier probability p  and 10000 test data points. Results are averaged over 10 repetitions.
Standard deviations are given in parentheses.

Note that the latter condition causes any convex loss (cid:96) to demonstrate unbounded response (see
proof of Theorem 5 in Appendix B). Therefore  the approximate estimator is strictly more robust (in
terms of bounded response) than regularized empirical risk minimization with a convex loss (cid:96).
Consistency: Finally  we can establish consistency of the approximate estimator in a limited albeit
non-trivial setting  although we have yet to establish it generally.
Theorem 4. Assume (cid:96) is Lipschitz-continuous and ψ(η) = 1− η. Assume that the data is generated
from a mixture of inliers and outliers  where P (inlier) > P (outlier). Then the estimate ˆθ produced
by the rounded (re-optimized) method is loss consistent.(Proof given in Appendix C.2.)

6 Experimental Evaluation
We conducted a set of experiments to evaluate the effectiveness of the proposed method compared
to standard methods from the literature. Our experimental evaluation was conducted in two parts:
ﬁrst a synthetic experiment where we could control data generation  then an experiment on real data.
The ﬁrst synthetic experiment was conducted as follows. A target weight vector θ was drawn from
N (0  I)  with Xi: sampled uniformly from [0  1]m  m = 5  and outputs yi computed as yi =
Xi:θ + i  i ∼ N (0  1
2 ). We then seeded the data set with outliers by randomly re-sampling each
yi and Xi: from N (0  108) and N (0  104) respectively  governed by an outlier probability p. Then
we randomly sampled 100 points as the training set and another 10000 samples are used for testing.
We implemented the proposed method with two different base losses  L2 and L1  respectively; refer-
ring to these as CvxBndL2 and CvxBndL1. We compared to standard L2 and L1 loss minimization 
as well as minimizing the Huber minimax loss (Huber) [4]. We also considered standard meth-
ods from the robust statistics literature  including the least trimmed square method (LTS) [7  24] 
and bounded loss minimization based on the Geman-McClure loss (GemMc) [8]. Finally we also
compared to the alternating minimization strategies outlined at the end of Section 3 (AltBndL2 and
AltBndL1 for L2 and L1 losses respectively)  and implemented the strategy described in [9]. We
added the Tikhonov regularization to each method and the regularization parameter λ was selected
(optimally for each method) on a separate validation set. Note that LTS has an extra parameter n(cid:48) 
which is the number of inliers. The ideal setting n(cid:48) = (1 − p)n was granted to LTS. We also tried
30 random restarts for LTS and picked the best result.
All experiments are repeated 10 times and the average root mean square errors (RMSE) (with stan-
dard deviations) on the clean test data are reported in Table 1. For p = 0 (i.e. no outliers)  all methods
perform well; their RMSEs are close to optimal (1/2  the standard deviation of i). However  when
outliers start to appear  the result of least squares is signiﬁcantly skewed  while the results of clas-
sic robust statistics methods  Huber  L1 and LTS  indeed turn out to be more robust than the least
squares  but nevertheless are still affected signiﬁcantly. Both implementations of the new method
performs comparably to the the non-convex Geman-McClure loss while substantially improving the
alternating strategy under the L1 loss. Note that the latter improvement clearly demonstrates that

7

Methods

L2
L1

Huber
LTS

GemMc

[9]

AltBndL2
AltBndL1
CvxBndL2
CvxBndL1
Gap(Cvx2)
Gap(Cvx1)

cal-housing
1185
(124.59)
1303
(244.85)
1221
(119.18)
533
(398.92)
28
(88.45)
967
(522.40)
967
(522.40)
1005
(603.00)
9
(0.64)
8
(0.28)
2e-12
(3e-12)
0.005
(0.01)

7.93
7.30
7.73
755.1
2.30
8.39
8.39
7.30
7.60
2.98
3e-9
0.001

(0.67)
(0.40)
(0.49)
(126)
(0.01)
(0.54)
(0.54)
(0.40)
(0.86)
(0.08)
(4e-9)
(0.001)

Datasets

abalone

pumadyn
1.24
1.29
1.24
0.32
0.12
0.81
0.81
1.29
0.07
0.08
0.025
0.267

(0.42)
(0.42)
(0.42)
(0.41)
(0.12)
(0.77)
(0.77)
(0.42)
(0.07)
(0.07)
(0.052)
(0.269)

bank-8fh
(6.57)
(3.09)
(3.18)
(6.67)
(0.80)
(6.18)
(9.40)
(2.51)
(0.05)
(0.07)
(0.003)
(0.028)

18.21
6.54
7.37
10.96
0.93
3.91
7.74
1.61
0.20
0.10
0.001
0.011

Table 2: RMSE on clean test data for 108 training data points and 1000 test data points  with 10 re-
peats. Standard deviations shown parentheses. The mean gap values of CvxBndL2 and CvxBndL1 
Gap(Cvx2) and Gap(Cvx1) respectively  are given in the last two rows.

alternating can be trapped in poor local minima. The proposal from [9] was not effective in this
setting (which differed from the one investigated there).
Next  we conducted an experiment on four real datasets taken from the StatLib repository9 and
DELVE.10 For each data set  we randomly selected 108 points as the training set  and another random
1000 points as the test set. Here the regularization constant is tuned by 10-fold cross validation. To
seed outliers  5% of the training set are randomly chosen and their X and y values are multiplied
by 100 and 10000  respectively. All of these data sets have 8 features  except pumadyn which has
32 features. We also estimated the scale factor on the training set by the mean absolute deviation
method  a common method in robust statistics [3]. Again  the ideal parameter n(cid:48) = (1 − 5%)n is
granted to LTS and 30 random restarts are performed.
The RMSE on test set for all methods are reported in Table 2. It is clear that all methods based on
convex losses (L2  L1  Huber) suffer signiﬁcantly from the added outliers. The method proposed in
this paper consistently outperform all other methods with a noticeable margin  except on the abalone
data set where GemMc performs slightly better.11 Again  we observe evidence that the alternating
strategy can be trapped in poor local minima  while the method from [9] was less effective. We also
measured the relative optimality gaps for the approximate CvxBnd procedures. The gaps were quite
small in most cases (the gaps were very close to zero in the synthetic case  and so are not shown) 
demonstrating the tightness of the proposed approximation scheme.

7 Conclusion
We have developed a new robust regression method that can guarantee a form of robustness (bounded
response) while ensuring tractability (polynomial run-time). The estimator has been proved consis-
tent under some restrictive but non-trivial conditions  although we have not established general con-
sistency. Nevertheless  an empirical evaluation reveals that the method meets or surpasses the gen-
eralization ability of state-of-the-art robust regression methods in experimental studies. Although
the method is more computationally involved than standard approaches  it achieves reasonable scal-
ability in real problems. We are investigating whether the proposed estimator achieves stronger
robustness properties  such as high breakdown or bounded inﬂuence. It would be interesting to ex-
tend the approach to also estimate scale in a robust and tractable manner. Finally  we continue to
investigate whether other techniques from the robust statistics and machine learning literatures can
be incorporated in the general framework while preserving desired properties.
Acknowledgements
Research supported by AICML and NSERC.

9http://lib.stat.cmu.edu/datasets/
10http://www.cs.utoronto.ca/ delve/data/summaryTable.html
11Note that we obtain different results than [9] arising from a very different outlier process.

8

References
[1] T. Bernholt. Robust estimators are hard to compute. Technical Report 52/2005  SFB475  U.

Dortmund  2005.

[2] R. Nunkesser and O. Morell. An evolutionary algorithm for robust regression. Computational

Statistics and Data Analysis  54:3242–3248  2010.

[3] R. Maronna  R. Martin  and V. Yohai. Robust Statistics: Theory and Methods. Wiley  2006.
[4] P. Huber and E. Ronchetti. Robust Statistics. Wiley  2nd edition  2009.
[5] A. Christmann and I. Steinwart. Consistency and robustness of kernel-based regression in

convex risk minimization. Bernoulli  13(3):799–819  2007.

[6] A. Christmann  A. Van Messem  and I. Steinwart. On consistency and robustness properties of
support vector machines for heavy-tailed distributions. Statistics and Its Interface  2:311–327 
2009.

[7] P. Rousseeuw and A. Leroy. Robust Regression and Outlier Detection. Wiley  1987.
[8] M. Black and A. Rangarajan. On the uniﬁcation of line processes  outlier rejection  and robust
statistics with applications in early vision. International Journal of Computer Vision  19(1):
57–91  1996.

[9] Y. Yu  M. Yang  L. Xu  M. White  and D. Schuurmans. Relaxed clipping: A global training
method for robust regression and classiﬁcation. In Advances in Neural Information Processings
Systems (NIPS)  2010.

[10] A. Bental  L. El Ghaoui  and A. Nemirovski. Robust Optimization. Princeton Series in Applied

Mathematics. Princeton University Press  October 2009.

[11] H. Xu  C. Caramanis  and S. Mannor. Robust regression and Lasso. In Advances in Neural

Information Processing Systems (NIPS)  volume 21  pages 1801–1808  2008.

[12] H. Xu  C. Caramanis  and S. Mannor. Robustness and regularization of support vector ma-

chines. Journal of Machine Learning Research  10:1485–1510  2009.

[13] S. Mukherjee  P. Niyogi  T. Poggio  and R. Rifkin. Learning theory: Stability is sufﬁcient
for generalization and necessary and sufﬁcient for consistency of empirical risk minimization.
Advances in Computational Mathematics  25(1-3):161–193  2006.

[14] G. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation on stochastic
processes and smoothing by splines. Annals of Mathematical Statistics  41(2):495–502  1970.

[15] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[16] Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes.

Springer  1996.

[17] F. Hampel  E. Ronchetti  P. Rousseeuw  and W. Stahel. Robust Statistics: The Approach Based

on Inﬂuence Functions. Wiley  1986.

[18] D. Donoho and P. Huber. The notion of breakdown point.

Lehmann  pages 157–184. Wadsworth  1983.

In A Festschrift for Erich L.

[19] P. Davies and U. Gather. The breakdown point—examples and counterexamples. REVSTAT

Statistical Journal  5(1):1–17  2007.

[20] Y. Nesterov and A. Nemiroviskii. Interior-point Polynomial Methods in Convex Programming.

SIAM  1994.

[21] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  2003.
[22] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press  2004.
[23] J. Peng and Y. Wei. Approximating k-means-type clustering via semideﬁnite programming.

SIAM Journal on Optimization  18(1):186–205  2007.

[24] P. Rousseeuw and K. Van Driessen. Computing LTS regression for large data sets. Data Mining

and Knowledge Discovery  12(1):29–45  2006.

[25] R. Horn and C. Johnson. Matrix Analysis. Cambridge  1985.

9

,Michel Besserve
Nikos Logothetis
Bernhard Schölkopf
Pan Xu
Jian Ma
Quanquan Gu