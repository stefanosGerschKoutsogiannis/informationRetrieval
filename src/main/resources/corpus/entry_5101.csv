2008,Sparse Convolved Gaussian Processes for Multi-output Regression,We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework  we apply the convolution process formalism to establish dependencies between output variables  where each latent function is represented as a GP. Based on these latent functions  we establish an approximation scheme using a conditional independence assumption between the output processes  leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.,Sparse Convolved Gaussian Processes for

Multi-output Regression

Mauricio Alvarez

School of Computer Science
University of Manchester  U.K.
alvarezm@cs.man.ac.uk

Neil D. Lawrence

School of Computer Science
University of Manchester  U.K.

neill@cs.man.ac.uk

Abstract

We present a sparse approximation approach for dependent output Gaussian pro-
cesses (GP). Employing a latent function framework  we apply the convolution
process formalism to establish dependencies between output variables  where each
latent function is represented as a GP. Based on these latent functions  we establish
an approximation scheme using a conditional independence assumption between
the output processes  leading to an approximation of the full covariance which is
determined by the locations at which the latent functions are evaluated. We show
results of the proposed methodology for synthetic data and real world applications
on pollution prediction and a sensor network.

1 Introduction

We consider the problem of modeling correlated outputs from a single Gaussian process (GP). Appli-
cations of modeling multiple outputs include multi-task learning (see e.g. [1]) and jointly predicting
the concentration of different heavy metal pollutants [5]. Modelling multiple output variables is a
challenge as we are required to compute cross covariances between the different outputs. In geo-
statistics this is known as cokriging. Whilst cross covariances allow us to improve our predictions
of one output given the others because the correlations between outputs are modelled [6  2  15  12]
they also come with a computational and storage overhead. The main aim of this paper is to address
these overheads in the context of convolution processes [6  2].

One neat approach to account for non-trivial correlations between outputs employs convolution pro-
cesses (CP). When using CPs each output can be expressed as the convolution between a smoothing
kernel and a latent function [6  2]. Let’s assume that the latent function is drawn from a GP. If
we also share the same latent function across several convolutions (each with a potentially differ-
ent smoothing kernel) then  since a convolution is a linear operator on a function  the outputs of
the convolutions can be expressed as a jointly distributed GP. It is this GP that is used to model
the multi-output regression. This approach was proposed by [6  2] who focussed on a white noise
process for the latent function.

Even though the CP framework is an elegant way for constructing dependent output processes  the
fact that the full covariance function of the joint GP must be considered results in signiﬁcant storage
and computational demands. For Q output dimensions and N data points the covariance matrix
scales as QN leading to O(Q3N 3) computational complexity and O(N 2Q2) storage. Whilst other
approaches to modeling multiple output regression are typically more constraining in the types of
cross covariance that can be expressed [1  15]  these constraints also lead to structured covariances
functions for which inference and learning are typically more efﬁcient (typically for N > Q these
methods have O(N 3Q) computation and O(N 2Q) storage). We are interested in exploiting the
richer class of covariance structures allowed by the CP framework  but without the additional com-
putational overhead they imply.

We propose a sparse approximation for the full covariance matrix involved in the multiple output
convolution process  exploiting the fact that each of the outputs is conditional independent of all oth-
ers given the input process. This leads to an approximation for the covariance matrix which keeps
intact the covariances of each output and approximates the cross-covariances terms with a low rank
matrix. Inference and learning can then be undertaken with the same computational complexity as
a set of independent GPs. The approximation turns out to be strongly related to the partially in-
dependent training conditional (PITC) [10] approximation for a single output GP. This inspires us
to consider a further conditional independence function across data points that leads to an approx-
imation which shares the form of the fully independent training conditional (FITC) approximation
[13  10] reducing computational complexity to O(N QM 2) and storage to O(N QM) with M rep-
resenting a user speciﬁed value.

To introduce our sparse approximation some review of the CP framework is required (Section 2).
Then in Section 3  we present sparse approximations for the multi-output GP. We discuss relations
with other approaches in Section 4. Finally  in Section 5  we demonstrate the approach on both
synthetic and real datasets.

2 Convolution Processes
Consider a set of Q functions {fq(x)}Q
between a smoothing kernel {kq(x)}Q

q=1  where each function is expressed as the convolution

q=1  and a latent function u(z) 

fq(x) =

kq(x − z)u(z)dz.

Z ∞

−∞

More generally  we can consider the inﬂuence of more than one latent function  {ur(z)}R
r=1  and
corrupt each of the outputs of the convolutions with an independent process (which could also in-
clude a noise term)  wq(x)  to obtain

yq(x) = fq(x) + wq(x) =

kqr(x − z)ur(z)dz + wq(x).

(1)

The covariance between two different functions yq(x) and ys(x0) is then recovered as
cov [yq(x)  ys(x0)] = cov [fq(x)  fs(x0)] + cov [wq(x)  ws(x0)] δqs 

Z ∞

RX

−∞

r=1

Z ∞

−∞

Z ∞

−∞

RX

r=1

where

cov [fq(x)  fs(x0)] =

RX

RX

r=1

p=1

Z ∞

−∞

kqr(x − z)

ksp(x0 − z0) cov [ur(z)  up(z0)] dz0dz

(2)

This equation is a general result; in [6  2] the latent functions ur(z) are assumed as independent
white Gaussian noise processes  i.e. cov [ur(z)  up(z0)] = σ2
δrpδz z0  so the expression (2) is
simpliﬁed as

ur

cov [fq(x)  fs(x0)] =

σ2
ur

kqr(x − z)ksr(x0 − z)dz.

We are going to relax this constraint on the latent processes  we assume that each inducing function is
an independent GP  i.e. cov [ur(z)  up(z0)] = kurup(z  z0)δrp  where kurur(z  z0) is the covariance
function for ur(z). With this simpliﬁcation  (2) can be written as

cov [fq(x)  fs(x0)] =

kqr(x − z)

ksr(x0 − z0)kurur(z  z0)dz0dz.

(3)

Z ∞

RX

−∞

r=1

Z ∞

−∞

As well as this correlation across outputs  the correlation between the latent function  ur(z)  and
any given output  fq(x)  can be computed 

cov [fq(x)  ur(z))] =

kqr(x − z0)kurur(z0  z)dz0.

(4)

Z ∞

−∞

3 Sparse Approximation

(cid:3)>

where y = (cid:2)y>

Given the convolution formalism  we can construct a full GP over the set of outputs. The likelihood
of the model is given by

p(y|X  φ) = N (0  Kf  f + Σ) 

Q

1   . . .   y>

(5)
is the set of output functions with yq = [yq(x1)  . . .   yq(xN )]>
;
Kf  f ∈ <QN×QN is the covariance matrix relating all data points at all outputs  with elements
cov [fq(x)  fs(x0)] in (3); Σ = Σ ⊗ IN   where Σ is a diagonal matrix with elements {σ2
q=1; φ
is the set of parameters of the covariance matrix and X = {x1  . . .   xN} is the set of training input
vectors at which the covariance is evaluated.
The predictive distribution for a new set of input vectors X∗ is [11]

p(y∗|y  X  X∗  φ) = N(cid:0)Kf∗ f (Kf  f + Σ)−1y  Kf∗ f∗ − Kf∗ f (Kf  f + Σ)−1Kf  f∗ + Σ(cid:1)  

q}Q

where we have used Kf∗ f∗ as a compact notation to indicate when the covariance matrix is evalu-
ated at the inputs X∗  with a similar notation for Kf∗ f . Learning from the log-likelihood involves
the computation of the inverse of Kf  f + Σ  which grows with complexity O((N Q)3). Once the
parameters have been learned  prediction is O(N Q) for the predictive mean and O((N Q)2) for the
predictive variance.

Our strategy for approximate inference is to exploit the natural conditional dependencies in the
model. If we had observed the entire length of each latent function  ur(z)  then from (1) we see that
each yq(x) would be independent  i.e. we can write 

p({yq (x)}Q

q=1 |{ur (z)}R

r=1   θ) =

p(yq (x)|{ur (z)}R

r=1   θ) 

QY

q=1

We deﬁne u = (cid:2)u>

where θ are the parameters of the kernels and covariance functions. Our key assumption is that this
independence will hold even if we have only observed M samples from ur(z) rather than the whole
function. The observed values of these M samples are then marginalized (as they are for the exact
case) to obtain the approximation to the likelihood. Our intuition is that the approximation should
be more accurate for larger M and smoother latent functions  as in this domain the latent function
could be very well characterized from only a few samples.

R

as the samples from the latent

1   . . .   u>
function with ur =
[ur(z1)  . . .   ur(zM )]>
; Ku u is then the covariance matrix between the samples from the latent
functions ur(z)  with elements given by kurur(z  z0); Kf  u = K>
u f are the cross-covariance ma-
trices between the latent functions ur(z) and the outputs fq(x)  with elements cov [fq(x)  ur(z)] in
(4) and Z = {z1  . . .   zM} is the set of input vectors at which the covariance Ku u is evaluated.
We now make the conditional independence assumption given the samples from the latent functions 

(cid:3)>

p(y|u  Z  X  θ) =

u uu  Kfq fq − Kfq uK−1

u uKu fq + σ2

qI(cid:1) .

QY

q=1

p(yq|u  Z  X  θ) =

QY

N(cid:0)Kfq uK−1
p(y|u  Z  X  θ) = N(cid:0)Kf  uK−1

q=1

u uu  D + Σ(cid:1)

We rewrite this product as a single Gaussian with a block diagonal covariance matrix 

where D = blockdiag(cid:2)Kf  f − Kf  uK−1
elements should be set to zero. We can also write this as D =(cid:2)Kf  f − Kf  uK−1
p(y|u  Z  X  θ)p(u|Z)du = N(cid:0)0  D + Kf  uK−1

indicate the block associated with each output of the matrix G should be retained  but all other
(cid:12) is the Hadamard product and M = IQ⊗1N   1N being the N × N matrix of ones and ⊗ being the
Kronecker product. We now marginalize the values of the samples from the latent functions by using
their process priors  i.e. p(u|Z) = N (0  Ku u). This leads to the following marginal likelihood 

(cid:3)  and we have used the notation blockdiag [G] to
(cid:3)(cid:12) M where

u uKu f + Σ(cid:1) .

p(y|Z  X  θ) =

u uKu f

u uKu f

Z

(7)

(6)

Notice that  compared to (5)  the full covariance matrix Kf  f has been replaced by the low rank co-
variance Kf  uK−1
u uKu f in all entries except in the diagonal blocks corresponding to Kfq fq . When
using the marginal likelihood for learning  the computation load is associated to the calculation of
the inverse of D. The complexity of this inversion is O(N 3Q) + O(N QM 2)  storage of the matrix
is O(N 2Q) + O(N QM). Note that if we set M = N these reduce to O(N 3Q) and O(N 2Q)
respectively which matches the computational complexity of applying Q independent GPs to model
the multiple outputs.
Combining eq. (6) with p(u|Z) using Bayes theorem  the posterior distribution over u is obtained
as

p(u|y  X  Z  θ) = N(cid:0)Ku uA−1Ku f (D + Σ)−1y  Ku uA−1Ku u

(8)
where A = Ku u + Ku f (D + Σ)−1Kf  u. The predictive distribution is expressed through the
integration of (6)  evaluated at X∗  with (8)  giving

(cid:1)

Z
=N(cid:0)Kf∗ uA−1Ku f (D + Σ)−1y  D∗ + Kf∗ uA−1Ku f∗ + Σ(cid:1)

p(y∗|u  Z  X∗  θ)p(u|y  X  Z  θ)du

p(y∗|y  X  X∗  Z  θ) =

(9)

with D∗ = blockdiag(cid:2)Kf∗ f∗ − Kf∗ uK−1

u uKu f∗

(cid:3).

The functional form of (7) is almost identical to that of the PITC approximation [10]  with the
samples we retain from the latent function providing the same role as the inducing values in the
partially independent training conditional (PITC) approximation. This is perhaps not surprising
given that the nature of the conditional independence assumptions in PITC is similar to that we have
made. A key difference is that in PITC it is not obvious which variables should be grouped together
when making the conditional independence assumption  here it is clear from the structure of the
model that each of the outputs should be grouped separately. However  the similarities are such that
we ﬁnd it convenient to follow the terminology of [10] and also refer to our approximation as a PITC
approximation.

We have already noted that our sparse approximation reduces the computational complexity of multi-
output regression with GPs to that of applying independent GPs to each output. For larger data sets
the N 3 term in the computational complexity and the N 2 term in the storage is still likely to be
prohibitive. However  we can be inspired by the analogy of our approach to the PITC approximation
and consider a more radical factorization of the outputs. In the fully independent training conditional
p(y|u  Z  X  θ) =QQ
QN
(FITC) [13  14] a factorization across the data points is assumed. For us that would lead to the
D = diag(cid:2)Kf  f − Kf  uK−1
following expression for conditional distribution of the output functions given the inducing variables 
n=1 p(yqn|u  Z  X  θ) which can be brieﬂy expressed through (6) with
u uKu f

(cid:3)(cid:12)M  with M = IQ⊗IN . Similar

(cid:3) =(cid:2)Kf  f − Kf  uK−1

equations are obtained for the posterior (8)  predictive (9) and marginal likelihood distributions (7)
leading to the Fully Independent Training Conditional (FITC) approximation [13  10]. Note that
the marginal likelihood might be optimized both with respect to the parameters associated with the
covariance matrices and with respect to Z. In supplementary material we include the derivatives of
the marginal likelihood wrt the matrices Kf  f   Ku f and Ku u.

u uKu f

q=1

4 Related work

There have been several suggestions for constructing multiple output GPs [2  15  1]. Under the
convolution process framework  the semiparametric latent factor model (SLFM) proposed in [15]
P
corresponds to a speciﬁc choice for the smoothing kernel function in (1) namely  kqr(x) = φqrδ(x).
The latent functions are assumed to be independent GPs and in such a case  cov [fq(x)  fs(x0)] =
r φqrφsrkurur(x  x0). This can be written using matrix notation as Kf  f = (Φ⊗I)Ku u(Φ>⊗I).

For computational speed up the informative vector machine (IVM) is employed [8].

In the multi-task learning model (MTLM) proposed in [1]  the covariance matrix is expressed as
Kf  f = K f ⊗ k(x  x0)  with K f being constrained positive semi-deﬁnite and k(x  x0) a covariance
function over inputs. The Nystr¨om approximation is applied to k(x  x0). As stated in [1] with respect
to SLFM  the convolution process is related with MTLM when the smoothing kernel function is

given again by kqr(x) = φqrδ(x) and there is only one latent function with covariance kuu(x  x0) =
k(x  x0). In this way  cov [fq(x)  fs(x0)] = φqφsk(x  x0) and in matrix notation Kf  f = ΦΦ> ⊗
k(x  x0). In [2]  the latent processes correspond to white Gaussian noises and the covariance matrix
is given by eq. (3). In this work  the complexity of the computational load is not discussed. Finally 
[12] use a similar covariance function to the MTLM approach but use an IVM style approach to
sparsiﬁcation.

Note that in each of the approaches detailed above a δ function is introduced into the integral. In the
dependent GP model of [2] it is introduced in the covariance function. Our approach considers the
more general case when neither kernel nor covariance function is given by the δ function.

5 Results

(2π)p/2

For all our experiments we considered squared exponential covariance functions for the latent pro-
cess of the form kurur(x  x0) = exp
  where Lr is a diagonal matrix
which allows for different length-scales along each dimension. The smoothing kernel had the same
form  kqr(τ ) = Sqr|Lqr|1/2
inite matrix. For this kernel/covariance function combination the necessary integrals are tractable
(see supplementary material).

h− 1
2 τ >Lqrτ(cid:3)   where Sqr ∈ R and Lqr is a symmetric positive def-

i
2 (x − x0)> Lr (x − x0)

exp(cid:2)− 1

We ﬁrst setup a toy problem in which we evaluate the quality of the prediction and the speed of
the approximation. The toy problem consists of Q = 4 outputs  one latent function  R = 1  and
N = 200 observation points for each output. The training data was sampled from the full GP with
the following parameters  S11 = S21 = 1  S31 = S41 = 5  L11 = L21 = 50  L31 = 300  L41 = 200
for the outputs and L1 = 100 for the latent function. For the independent processes  wq (x)  we
4 = 1. For the sparse
simply added white noise with variances σ2
approximations we used M = 30 ﬁxed inducing points equally spaced between the range of the
input and R = 1. We sought the kernel parameters through maximizing the marginal likelihood
using a scaled conjugate gradient algorithm. For test data we removed a portion of one output as
shown in Figure 1 (points in the interval [−0.8  0] were removed). The predictions shown correspond
to the full GP (Figure 1(a))  an independent GP (Figure 1(b))  the FITC approximation (Figure 1(c))
and the PITC approximation (Figure 1(d)). Due to the strong dependencies between the signals  our
model is able to capture the correlations and predicts accurately the missing information.

2 = 0.0125  σ2

3 = 1.2 and σ2

1 = σ2

Table 1 shows prediction results over an independent test set. We used 300 points to compute the
standarized mean square error (SMSE) [11] and ten repetitions of the experiment  so that we also
included one standard deviation for the ten repetitions. The training times for iteration of each model
are 1.45 ± 0.23 secs for the full GP  0.29 ± 0.02 secs for the FITC and 0.48 ± 0.01 for the PITC.
Table 1  shows that the SMSE of the sparse approximations is similar to the one obtained with the
full GP with a considerable reduction of training times.

Method
Full GP
FITC
PITC

Output 1
1.07 ± 0.08
1.08 ± 0.09
1.07 ± 0.08

Output 2
0.99 ± 0.03
1.00 ± 0.03
0.99 ± 0.03

Output 3
1.12 ± 0.07
1.13 ± 0.07
1.12 ± 0.07

Output 4
1.05 ± 0.07
1.04 ± 0.07
1.05 ± 0.07

Table 1: Standarized mean square error (SMSE) for the toy problem over an independent test set. All numbers
are to be multiplied by 10−2. The experiment was repeated ten times. Table included the value of one standard
deviation over the ten repetitions.

We now follow a similar analysis for a dataset consisting of weather data collected from a sensor net-
work located on the south coast of England. The network includes four sensors (named Bramblemet 
Sotonmet  Cambermet and Chimet) each of which measures several environmental variables [12].
We selected one of the sensors signals  tide height  and applied the PITC approximation scheme
with an additional squared exponential independent kernel for each wq (x) [11]. Here Q = 4 and
we chose N = 1000 of the 4320 for the training set  leaving the remaining points for testing. For
comparison we also trained a set of independent GP models. We followed [12] in simulating sensor
failure by introducing some missing ranges for these signals. In particular  we have a missing range

(a) Output 4 using the full GP

(b) Output 4 using an independent GP

(c) Output 4 using the FITC approximation (d) Output 4 using the PITC approximation

Figure 1: Predictive mean and variance using the full multi-output GP  the sparse approximation and an inde-
pendent GP for output 4. The solid line corresponds to the mean predictive  the shaded region corresponds to
2 standard deviations away from the mean and the dash line is the actual value of the signal without noise. The
dots are the noisy training points. There is a range of missing data in the interval [−0.8  0.0]. The crosses in
ﬁgures 1(c) and 1(d) corresponds to the locations of the inducing inputs.

of [0.6  1.2] for the Bramblemet tide height sensor and [1.5  2.1] for the Cambermet. For the other
two sensors we used all 1000 training observations. For the sparse approximation we took M = 100
equally spaced inducing inputs. We see from Figure 2 that the PITC approximation captures the de-
pendencies and predicts closely the behavior of the signal in the missing range. This contrasts with
the behavior of the independent model  which is not able to follow the original signal.

As another example we employ the Jura dataset  which consists of measurements of concentrations
of several heavy metals collected in the topsoil of a 14.5 km2 region of the Swiss Jura. The data is
divided into a prediction set (259 locations) and a validation set (100 locations)1. In a typical situ-
ation  referred as undersampled or heterotopic case  a few expensive measurements of the attribute
of interest are supplemented by more abundant data on correlated attributes that are cheaper to sam-
ple. We follow the experiments described in [5  p. 248 249] in which a primary variable (cadmium
and copper) at prediction locations in conjunction with some secondary variables (nickel and zinc
for cadmium; lead  nickel and zinc for copper) at prediction and validation locations  are employed
to predict the concentration of the primary variable at validation locations. We compare results of
independent GP  the PITC approximation  the full GP and ordinary co-kriging. For the PITC ex-
periments  a k-means procedure is employed ﬁrst to ﬁnd the initial locations of the inducing values
and then these locations are optimized in the same optimization procedure used for the parameters.
Each experiment is repeated ten times. The results for ordinary co-kriging were obtained from [5 
p. 248 249]. In this case  no values for standard deviation are reported. Figure 3 shows results of
prediction for cadmium (Cd) and copper (Cu). From ﬁgure 3(a)  it can be noticed that using 50 in-
ducing values  the approximation exhibits a similar performance to the co-kriging method. As more

1This data is available at http://www.ai-geostats.org/

−1−0.500.51−10−8−6−4−20246810−1−0.500.51−10−8−6−4−20246810−1−0.500.51−10−8−6−4−20246810−1−0.500.51−10−8−6−4−20246810(a) Bramblemet using an independent GP

(b) Bramblemet using PITC

(c) Cambermet using an independent GP

(d) Cambermet using PITC

Figure 2: Predictive Mean and variance using independent GPs and the PITC approximation for the tide height
signal in the sensor dataset. The dots indicate the training observations while the dash indicates the testing
observations. We have emphasized the size of the training points to differentiate them from the testing points.
The solid line corresponds to the mean predictive. The crosses in ﬁgures 2(b) and 2(d) corresponds to the
locations of the inducing inputs.

inducing values are included  the approximation follows the performance of the full GP  as it would
be expected. From ﬁgure 3(b)  it can be observed that  although the approximation is better that the
independent GP  it does not obtain similar results to the full GP. Summary statistics of the prediction
data ([5  p. 15]) shows higher variability for the copper dataset than for the cadmium dataset  which
explains in some extent the different behaviors.

(a) Cadmium (Cd)

(b) Copper (Cu)

Figure 3: Mean absolute error and standard deviation for ten repetitions of the experiment for the Jura dataset
In the bottom of each ﬁgure  IGP stands for independent GP  P(M) stands for PITC with M inducing values 
FGP stands for full GP and CK stands for ordinary co-kriging (see [5] for detailed description).

00.511.522.530.511.522.533.544.55Tide Height (m)Time (days)00.511.522.530.511.522.533.544.55Tide Height (m)Time (days)00.511.522.530.511.522.533.544.55Tide Height (m)Time (days)00.511.522.530.511.522.533.544.55Tide Height (m)Time (days)IGPP(50)P(100)P(200)P(500)FGPCK0.420.440.460.480.50.520.540.560.58MEAN ABSOLUTE ERROR   CdIGPP(50)P(100)P(200)P(500)FGPCK78910111213141516MEAN ABSOLUTE ERROR   Cu6 Conclusions

We have presented a sparse approximation for multiple output GPs  capturing the correlated in-
formation among outputs and reducing the amount of computational load for prediction and opti-
mization purposes. The reduction in computational complexity for the PITC approximation is from
O(N 3Q3) to O(N 3Q). This matches the computational complexity for modeling with independent
GPs. However  as we have seen  the predictive power of independent GPs is lower.

Linear dynamical systems responses can be expressed as a convolution between the impulse re-
sponse of the system with some input function. This convolution approach is an equivalent way of
representing the behavior of the system through a linear differential equation. For systems involving
high amounts of coupled differential equations [4]  the approach presented here is a reasonable way
of obtaining approximate solutions and incorporating prior domain knowledge to the model.

One could optimize with respect to positions of the values of the latent functions. As the input
dimension grows  it might be more difﬁcult to obtain an acceptable response. Some solutions to this
problem have already been proposed [14].

Acknowledgments

We thank the authors of [12] who kindly made the sensor network database available.

References

[1] E. V. Bonilla  K. M. Chai  and C. K. I. Williams. Multi-task Gaussian process prediction. In J. C. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  NIPS  volume 20  Cambridge  MA  2008. MIT Press. In
press.

[2] P. Boyle and M. Frean. Dependent Gaussian processes. In L. Saul  Y. Weiss  and L. Bouttou  editors 

NIPS  volume 17  pages 217–224  Cambridge  MA  2005. MIT Press.

[3] M. Brookes. The matrix reference manual. Available on-line.  2005. http://www.ee.ic.ac.uk/

hp/staff/dmb/matrix/intro.html.

[4] P. Gao  A. Honkela  M. Rattray  and N. D. Lawrence. Gaussian process modelling of latent chemical

species: Applications to inferring transcription factor activities. Bioinformatics  24(16):i70–i75  2008.

[5] P. Goovaerts. Geostatistics For Natural Resources Evaluation. Oxford University Press  1997. ISBN

[6] D. M. Higdon. Space and space-time modelling using process convolutions. In C. Anderson  V. Barnett 
P. Chatwin  and A. El-Shaarawi  editors  Quantitative methods for current environmental issues  pages
37–56. Springer-Verlag  2002.

[7] N. D. Lawrence. Learning for larger datasets with the Gaussian process latent variable model. In Meila

0-19-511538-4.

and Shen [9].

[8] N. D. Lawrence  M. Seeger  and R. Herbrich. Fast sparse Gaussian process methods: The informative
vector machine. In S. Becker  S. Thrun  and K. Obermayer  editors  NIPS  volume 15  pages 625–632 
Cambridge  MA  2003. MIT Press.

[9] M. Meila and X. Shen  editors. AISTATS  San Juan  Puerto Rico  21-24 March 2007. Omnipress.
[10] J. Qui˜nonero Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process

[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press  Cam-

regression. JMLR  6:1939–1959  2005.

bridge  MA  2006. ISBN 0-262-18253-X.

[12] A. Rogers  M. A. Osborne  S. D. Ramchurn  S. J. Roberts  and N. R. Jennings. Towards real-time informa-
tion processing of sensor network data using computationally efﬁcient multi-output Gaussian processes.
In Proceedings of the International Conference on Information Processing in Sensor Networks (IPSN
2008)  2008. In press.

[13] E. Snelson and Z. Ghahramani.

Sparse Gaussian processes using pseudo-inputs.

In Y. Weiss 

B. Sch¨olkopf  and J. C. Platt  editors  NIPS  volume 18  Cambridge  MA  2006. MIT Press.

[14] E. Snelson and Z. Ghahramani. Local and global sparse Gaussian process approximations. In Meila and

Shen [9].

[15] Y. W. Teh  M. Seeger  and M. I. Jordan. Semiparametric latent factor models.

In R. G. Cowell and
Z. Ghahramani  editors  AISTATS 10  pages 333–340  Barbados  6-8 January 2005. Society for Artiﬁcial
Intelligence and Statistics.

,Jason Chang
John Fisher III