2016,Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions,Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time  users' and items' feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. To learn parameters  we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.,Coevolutionary Latent Feature Processes for

Continuous-Time User-Item Interactions

Yichen Wang⇧  Nan Du⇤  Rakshit Trivedi⇧  Le Song⇧

⇤Google Research

⇧College of Computing  Georgia Institute of Technology

{yichen.wang  rstrivedi}@gatech.edu  dunan@google.com

lsong@cc.gatech.edu

Abstract

Matching users to the right items at the right time is a fundamental task in recom-
mendation systems. As users interact with different items over time  users’ and
items’ feature may evolve and co-evolve over time. Traditional models based on
static latent features or discretizing time into epochs can become ineffective for
capturing the ﬁne-grained temporal dynamics in the user-item interactions. We
propose a coevolutionary latent feature process model that accurately captures the
coevolving nature of users’ and items’ feature. To learn parameters  we design
an efﬁcient convex optimization algorithm with a novel low rank space sharing
constraints. Extensive experiments on diverse real-world datasets demonstrate sig-
niﬁcant improvements in user behavior prediction compared to state-of-the-arts.

Introduction

1
Online social platforms and service websites  such as Reddit  Netﬂix and Amazon  are attracting
thousands of users every minute. Effectively recommending the appropriate service items is a
fundamentally important task for these online services. By understanding the needs of users and
serving them with potentially interesting items  these online platforms can improve the satisfaction of
users  and boost the activities or revenue of the sites due to increased user postings  product purchases 
virtual transactions  and/or advertisement clicks [30  9].
As the famous saying goes “You are what you eat and you think what you read”  both users’ interests
and items’ semantic features are dynamic and can evolve over time [18  4]. The interactions between
users and service items play a critical role in driving the evolution of user interests and item features.
For example  for movie streaming services  a long-time fan of comedy watches an interesting science
ﬁction movie one day  and starts to watch more science ﬁction movies in place of comedies. Likewise 
a single movie may also serve different segment of audiences at different times. For example  a movie
initially targeted for an older generation may become popular among the younger generation  and the
features of this movie need to be redeﬁned.
Another important aspect is that users’ interests and items’ features can co-evolve over time  that
is  their evolutions are intertwined and can inﬂuence each other. For instance  in online discussion
forums  such as Reddit  although a group (item) is initially created for political topics  users with very
different interest proﬁles can join this group (user ! item). Therefore  the participants can shape
the actual direction (or features) of the group through their postings and responses. It is not unlikely
that this group can eventually become one about education simply because most users here concern
about education (item ! user). As the group is evolving towards topics on education  some users
may become more attracted to education topics  and to the extent that they even participate in other
dedicated groups on education. On the opposite side  some users may gradually gain interests in
sports groups  lose interests in political topics and become inactive in this group. Such coevolutionary
nature of user-item interactions raises very interesting questions on how to model them elegantly and
how to learn them from observed interaction data.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Nowadays  user-item interaction data are archived in increasing temporal resolution and becoming
increasingly available. Each individual user-item iteration is typically logged in the database with
the precise time-stamp of the interaction  together with additional context of that interaction  such
as tag  text  image  audio and video. Furthermore  the user-item interaction data are generated in an
asynchronous fashion in a sense that any user can interact with any item at any time and there may
not be any coordination or synchronization between two interaction events. These types of event data
call for new representations  models  learning and inference algorithms.
Despite the temporal and asynchronous nature of such event data  for a long-time  the data has
been treated predominantly as a static graph  and ﬁxed latent features have been assigned to each
user and item [21  5  2  10  29  30  25]. In more sophisticated methods  the time is divided into
epochs  and static latent feature models are applied to each epoch to capture some temporal aspects
of the data [18  17  28  6  13  4  20  17  28  12  15  24  23]. For such epoch-based methods  it is not
clear how to choose the epoch length parameter due to the asynchronous nature of the user-item
interactions. First  different users may have very different time-scale when they interact with those
service items  making it very difﬁcult to choose a uniﬁed epoch length. Second  it is not easy for
the learned model to answer ﬁne-grained time-sensitive queries such as when a user will come
back for a particular service item. It can only make such predictions down to the resolution of the
chosen epoch length. Most recently  [9] proposed an efﬁcient low-rank point process model for
time-sensitive recommendations from recurrent user activities. However  it still fails to capture the
heterogeneous coevolutionary properties of user-item interactions with much more limited model
ﬂexibility. Furthermore  it is difﬁcult for this approach to incorporate observed context features.
In this paper  we propose a coevolutionary latent feature process for continuous-time user-item
interactions  which is designed speciﬁcally to take into account the asynchronous nature of event
data  and the co-evolution nature of users’ and items’ latent features. Our model assigns an evolving
latent feature process for each user and item  and the co-evolution of these latent feature processes is
considered using two parallel components:
• (Item ! User) A user’s latent feature is determined by the latent features of the items he interacted
with. Furthermore  the contributions of these items’ features are temporally discounted by an
exponential decaying kernel function  which we call the Hawkes [14] feature averaging process.
• (User ! Item) Conversely  an item’s latent features are determined by the latent features of the
users who interact with the item. Similarly  the contribution of these users’ features is also modeled
as a Hawkes feature averaging process.

Besides the two sets of intertwined latent feature processes  our model can also take into account
the presence of potentially high dimensional observed context features and links the latent features
to the observed context features using a low dimensional projection. Despite the sophistication of
our model  we show that the model parameter estimation  a seemingly non-convex problem  can
be transformed into a convex optimization problem  which can be efﬁciently solved by the latest
conditional gradient-like algorithm. Finally  the coevolutionary latent feature processes can be used
for down-streaming inference tasks such as the next-item and the return-time prediction. We evaluate
our method over a variety of datasets  verifying that our method can lead to signiﬁcant improvements
in user behavior prediction compared to the state-of-the-arts.
2 Background on Temporal Point Processes
This section provides necessary concepts of the temporal point process [7]. It is a random process
whose realization consists of a list of events localized in time  {ti} with ti 2 R+. Equivalently  a given
temporal point process can be represented as a counting process  N (t)  which records the number of
events before time t. An important way to characterize temporal point processes is via the conditional
intensity function (t)  a stochastic model for the time of the next event given all the previous events.
Formally  (t)dt is the conditional probability of observing an event in a small window [t  t+dt) given
the history T (t) up to t  i.e.  (t)dt := P{event in [t  t + dt)|T (t)} = E[dN (t)|T (t)]  where one
typically assumes that only one event can happen in a small window of size dt  i.e.  dN (t) 2{ 0  1}.
The function form of the intensity is often designed to capture the phenomena of interests. One
commonly used form is the Hawkes process [14  11  27  26]  whose intensity models the excitation
between events  i.e.  (t) = µ + ↵Pti2T (t) !(t ti)  where !(t) := exp(!t) is an exponential

triggering kernel  µ > 0 is a baseline intensity independent of the history. Here  the occurrence of
each historical event increases the intensity by a certain amount determined by the kernel ! and
the weight ↵ > 0  making the intensity history dependent and a stochastic process by itself. From

2

1

2

K

1

2

K

1

2

K

(

1 2

(

1 2

(

1 2

(

1 2

David

Alice

Christine

Jacob

Interaction 

Item 
feature

!"($)
feature)($)
&'($)

User 
feature

Base drift

# ) =	/⋅1 )	
45)−)('(
+3⋅
+45)−)+'+
+ 45)−)(%78()()
+45)−)+%79()+)

Interaction feature 

Coevolution: Item feature

1

2

K

!

(# %+ '+ )+)

1 2

(# %& '( )()

!

1 2

Alice

&4( =6	84(	

1

2

K

Base drift

1 2

1 2

!

!

(#$ & '$ ($)(#* & '* (*) (#+ & '+ (+)+ -.(−($#01(($)
+-.(−($#02((*)
+-.(−($#03((+)

Interaction feature 

!

1 2

David

Alice

Jacob

Coevolution: User feature

1

2

K

1

2

K

1

2

K

1

2

K

1

2

K

1

2

K

1

2

K

1

2

K

(a) Data as a bipartite graph

(b) User latent feature process

(c) Item latent feature process

tn

Figure 1: Model illustration. (a) User-item interaction events data. Each edge contains user  item 
time  and interaction feature. (b) Alice’s latent feature consists of three components: the drift of
baseline feature  the time-weighted average of interaction feature  and the weighted average of item
feature. (c) The symmetric item latent feature process. A  B  C  D are embedding matrices from
high dimension feature space to latent space. !(t) = exp(!t) is an exponential decaying kernel.
the survival analysis theory [1]  given the history T = {t1  . . .   tn}  for any t > tn  we characterize
(⌧ ) d⌧.
the conditional probability that no event happens during [tn  t) as S(t|T ) = exp R t
Moreover  the conditional density that an event occurs at time t is f (t|T ) = (t) S(t|T ).
3 Coevolutionary Latent Feature Processes
In this section  we present the framework to model the temporal dynamics of user-item interactions.
We ﬁrst explicitly capture the co-evolving nature of users’ and items’ latent features. Then  based on
the compatibility between a user’ and item’s latent feature  we model the user-item interaction by a
temporal point process and parametrize the intensity function by the feature compatibility.
3.1 Event Representation
Given m users and n items  the input consists of all users’ history events: T = {ek}  where
ek = (uk  ik  tk  qk) means that user uk interacts with item ik at time tk and generates an interaction
feature vector qk 2 RD. For instance  the interaction feature can be a textual message delivered
from the user to the chatting-group in Reddit or a review of the business in Yelp. It can also be
unobservable if the data only contains the temporal information.
3.2 Latent Feature Processes
We associate a latent feature vector uu(t) 2 RK with a user u and ii(t) 2 RK with an item i. These
features represent the subtle properties which cannot be directly observed  such as the interests of a
user and the semantic topics of an item. Speciﬁcally  we model uu(t) and ii(t) as follows:
User latent feature process. For each user u  we formulate uu(t) as:

!(t  tk)qk

!(t  tk)iik (tk)

 

(1)

Hawkes interaction feature averaging

co-evolution: Hawkes item feature averaging

Item latent feature process. For each item i  we specify ii(t) as:

uu(t) = A u(t)
base drift

|{z}

ii(t) = C i(t)
base drift

|{z}

+B X{ek|uk=u tk<t}
|
{z
+D X{ek|ik=i tk<t}
{z

|

+ X{ek|uk=u tk<t}
|
+ X{ek|ik=i tk<t}
|

}

}

{z

{z

}

}

!(t  tk)qk

!(t  tk)uuk (tk)

 

(2)

Hawkes interaction feature averaging

co-evolution: Hawkes user feature averaging

where A  B  C  D 2 RK⇥D are the embedding matrices mapping from the explicit high-dimensional
feature space into the low-rank latent feature space. Figure 1 highlights the basic setting of our model.
Next we discuss the rationale of each term in detail.
Drift of base features. u(t) 2 RD and i(t) 2 RD are the explicitly observed properties of user u
and item i  which allows the basic features of users (e.g.  a user’s self-crafted interests) and items (e.g. 
textual categories and descriptions) to smoothly drift through time. Such changes of basic features
normally are caused by external inﬂuences. One can parametrize u(t) and i(t) in many different
ways  e.g.  the exponential decaying basis to interpolate these features observed at different times.

3

Evolution with interaction feature. Users’ and items’ features can evolve and be inﬂuenced by
the characteristics of their interactions. For instance  the genre changes of movies indicate the
changing tastes of users. The theme of a chatting-group can be easily shifted to certain topics of
the involved discussions. In consequence  this term captures the cumulative inﬂuence of the past
interaction features to the changes of the latent user (item) features. The triggering kernel !(t  tk)
associated with each past interaction at tk quantiﬁes how such inﬂuence can change through time. Its
parametrization depends on the phenomena of interest. Without loss of generality  we choose the
exponential kernel !(t) = exp (!t) to reduce the inﬂuence of each past event. In other words 
only the most recent interaction events will have bigger inﬂuences. Finally  the embedding B  D
map the observable high dimension interaction feature to the latent space.
Coevolution with Hawkes feature averaging processes. Users’ and items’ latent features can
mutually inﬂuence each other. This term captures the two parallel processes:
• Item ! User. A user’s latent feature is determined by the latent features of the items he interacted
with. At each time tk  the latent item feature is iik (tk). Furthermore  the contributions of these
items’ features are temporally discounted by a kernel function !(t)  which we call the Hawkes
feature averaging process. The name comes from the fact that Hawkes process captures the
temporal inﬂuence of history events in its intensity function. In our model  we capture both the
temporal inﬂuence and feature of each history item as a latent process.
• User ! Item. Conversely  an item’s latent features are determined by the latent features of all
the users who interact with the item. At each time tk  the latent feature is uuk (tk). Similarly  the
contribution of these users’ features is also modeled as a Hawkes feature averaging process.

Note that to compute the third co-evolution term  we need to keep track of the user’s and item’s latent
features after each interaction event  i.e.  at tk  we need to compute uuk (tk) and iik (tk) in (1) and
(2)  respectively. Set I(·) to be the indicator function  we can show by induction that

uuk (tk) = AhXk
+ ChXk1
iik (tk) = ChXk
+ AhXk1

j=1 I[uj = uk]!(tk  tj)uj (tj)i + BhXk
j=1 I[uj = uk]!(tk  tj)ij (tj)i + DhXk1
j=1 I[ij = ik]!(tk  tj)ij (tj)i + DhXk
j=1 I[ij = ik]!(tk  tj)uj (tj)i + BhXk1

j=1 I[uj = uk]!(tk  tj)qji
j=1 I[uj = uk]!(tk  tj)qji
j=1 I[ij = ik]!(tk  tj)qji
j=1 I[ij = ik]!(tk  tj)qji

In summary  we have incorporated both of the exogenous and endogenous inﬂuences into a single
model. First  each process evolves according to the respective exogenous base temporal user (item)
features u(t) (i(t)). Second  the two processes also inter-depend on each other due to the endoge-
nous inﬂuences from the interaction features and the entangled latent features. We present our model
in the most general form and the speciﬁc choices of uu(t)  ii(t) are dependent on applications. For
example  if no interaction feature is observed  we drop the second term in (1) and (2).
3.3 User-Item Interactions as Temporal Point Processes
For each user  we model the recurrent occurrences of user u’s interaction with all items as a multi-
dimensional temporal point process. In particular  the intensity in the i-th dimension (item i) is:

u i(t) =

long-term preference

+ uu(t)>ii(t)
short-term preference

 

(3)

⌘u i

|{z}

|

{z

}

where ⌘ = (⌘u i) is a baseline preference matrix. The rationale of this formulation is threefold.
First  instead of discretizing the time  we explicitly model the timing of each event occurrence as a
continuous random variable  which naturally captures the heterogeneity of the temporal interactions
between users and items. Second  the base intensity ⌘u i represents the long-term preference of user
u to item i  independent of the history. Third  the tendency for user u to interact with item i at time t
depends on the compatibility of their instantaneous latent features. Such compatibility is evaluated
through the inner product of their time-varying latent features.
Our model inherits the merits from classic content ﬁltering  collaborative ﬁltering  and the most
recent temporal models. For the cold-start users having few interactions with the items  the model
adaptively utilizes the purely observed user (item) base properties and interaction features to adjust
its predictions  which incorporates the key idea of feature-based algorithms. When the observed

4

features are missing and non-informative  the model makes use of the user-item interaction patterns to
make predictions  which is the strength of collaborative ﬁltering algorithms. However  being different
from the conventional matrix-factorization models  the latent user and item features in our model are
entangled and able to co-evolve over time. Finally  the general temporal point process ingredient of
the model makes it possible to capture the dynamic preferences of users to items and their recurrent
interactions  which is more ﬂexible and expressive.
4 Parameter Estimation
In this section  we propose an efﬁcient framework to learn the parameters. A key challenge is that
the objective function is non-convex in the parameters. However  we reformulate it as a convex
optimization by creating new parameters. Finally  we present the generalized conditional gradient
algorithm to efﬁciently solve the objective function.
Given a collection of events T recorded within a time window [0  T )  we estimate the parameters
using maximum likelihood estimation of all events. The joint negative log-likelihood [1] is:

` = Xek

loguk ik (tk) +

mXu=1

nXi=1Z T

0

u i(⌧ ) d⌧

(4)

1CA

The objective function is non-convex in variables {A  B  C  D} due to the inner product term in (3).
To learn these parameters  one way is to ﬁx the matrix rank and update each matrix using gradient
based methods. However  it is easily trapped in local optima and one needs to tune the rank for the
best performance. However  with the observation that the product of two low rank matrices yields a
low rank matrix  we will optimize over the new matrices and obtain a convex objective function.
4.1 Convex Objective Function
We will create new parameters such that the intensity function is convex. Since uu(t) contains the
averaging of iik (tk) in (1)  C  D will appear in uu(t). Similarly  A  B will appear in ii(t). Hence

these matrices X =A>A  B>B  C>C  D>D  A>B  A>C  A>D  B>C  B>D  C>D will
appear in (3) after expansion  due to the inner product ii(t)>uu(t). For each matrix product in
X   we denote it as a new variable Xi and optimize the objective function over the these variables.
We denote the corresponding coefﬁcient of Xi as xi(t)  which can be exactly computed. Denote
⇤(t) = (u i(t))  we can rewrite the intensity in (3) in the matrix form as:

⇤(t) = ⌘ +X10

i=1

xi(t)Xi

(5)
The intensity is convex in each new variable Xi  hence the objective function. We will optimize over
the new set of variables X subject to the constraints that i) some of them share the same low rank
space  e.g.  A> is shared as the column space inA>A  A>B  A>C  A>D and ii) new variables
are low rank (the product of low rank matrices is low rank). Next  we show how to incorporate the
space sharing constraint for general objective function with an efﬁcient algorithm.
First  we create a symmetric block matrix X 2 R4D⇥4D and place each Xi as follows:
A>A A>B A>C A>D
B>A B>B B>C B>D
C>A C>B C>C C>D
D>A D>B D>C D>D

X1 X2 X3 X4
X>2 X5 X6 X7
X>3 X>6 X8 X9
X>4 X>7 X>9 X10

(6)

X =0B@

Intuitively  minimizing the nuclear norm of X ensures all the low rank space sharing constraints.
First  nuclear norm k·k ⇤ is a summation of all singular values  and is commonly used as a convex
surrogate for the matrix rank function [22]  hence minimizing kXk⇤ ensures it to be low rank and
gives the unique low rank factorization of X. Second  since X1  X2  X3  X4 are in the same row
and share A>  the space sharing constraints are naturally satisﬁed.
Finally  since it is typically believed that users’ long-time preference to items can be categorized into
a limited number of prototypical types  we set ⌘ to be low rank. Hence the objective is:

min

(7)
where ` is deﬁned in (4) and k·k F is the Frobenius norm and the associated constraint ensures X to
be symmetric. {↵    } control the trade-off between the constraints. After obtaining X  one can
directly apply (5) to compute the intensity and make predictions.

`(X  ⌘) + ↵k⌘k⇤ + kXk⇤ + kX  X>k2

⌘>0 X>0

F

5

1CA =0B@

t + 1

tu i
n

4.2 Generalized Conditional Gradient Algorithm
We use the latest generalized conditional gradient algorithm [9] to solve the optimization problem (7).
We provide details in the appendix. It has an alternating updates scheme and efﬁciently handles
the nonnegative constraint using the proximal gradient descent and the the nuclear norm constraint
using conditional gradient descent. It is guaranteed to converge in O( 1
t2 )  where t is the number
of iterations. For both the proximal and the conditional gradient parts  the algorithm achieves
the corresponding optimal convergence rates. If there is no nuclear norm constraint  the results
t2 ) rate achieved by proximal gradient method for smooth convex
recover the well-known optimal O( 1
optimization. If there is no nonnegative constraints  the results recover the well-known O( 1
t ) rate
attained by conditional gradient method for smooth convex minimization. Moreover  the per-iteration
complexity is linear in the total number of events with O(mnk)  where m is the number of users  n
is the number of items and k is the number of events per user-item pair.
5 Experiments
We evaluate our framework  COEVOLVE  on synthetic and real-world datasets. We use all the events
up to time T · p as the training data  and the rest as testing data  where T is the length of the
observation window. We tune hyper-parameters and the latent rank of other baselines using 10-fold
cross validation with grid search. We vary the proportion p 2{ 0.7  0.72  0.74  0.76  0.78} and report
the averaged results over ﬁve runs on two tasks:
(a) Item recommendation: for each user u  at every testing time t  we compute the survival probabil-
ity Su i(t) = exp R t
n is the last training
event time of (u  i). We then rank all the items in the ascending order of Su i(t) to produce a
recommendation list. Ideally  the item associated with the testing time t should rank one  hence
smaller value indicates better predictive performance. We repeat the evaluation on each testing
moment and report the Mean Average Rank (MAR) of the respective testing items across all users.
(b) Time prediction: we predict the time when a testing event will occur between a given user-item
pair (u  i) by calculating the density of the next event time as f (t) = u i(t)Su i(t). With the
density  we compute the expected time of next event by sampling future events as in [9]. We report
the Mean Absolute Error (MAE) between the predicted and true time. Furthermore  we also report
the relative percentage of the prediction error with respect to the entire testing time window.

u i(⌧ )d⌧ of each item i up to time t  where tu i

5.1 Competitors
TimeSVD++ is the classic matrix factorization method [18]. The latent factors of users and items are
designed as decay functions of time and also linked to each other based on time. FIP is a static low
rank latent factor model to uncover the compatibility between user and item features [29]. TSVD++
and FIP are only designed for data with explicit ratings. We convert the series of user-item interaction
events into an explicit rating using the frequency of a user’s item consumptions [3]. STIC ﬁts
a semi-hidden markov model to each observed user-item pair [16] and is only designed for time
prediction. PoissonTensor uses Poisson regression as the loss function [6] and has been shown to
outperform factorization methods based on squared loss [17  28] on recommendation tasks. There are
two choices of reporting performance: i) use the parameters ﬁtted only in the last time interval and
ii) use the average parameters over all intervals. We report the best performance between these two
choices. LowRankHawkes is a Hawkes process based model and it assumes user-item interactions
are independent [9].
5.2 Experiments on Synthetic Data
We simulate 1 000 users and 1 000 items. For each user  we further generate 10 000 events by Ogata’s
thinning algorithm [19]. We compute the MAE by comparing estimated ⌘  X with the ground-truth.
The baseline drift feature is set to be constant. Figure 2 (a) shows that it only requires a few hundred
iterations to descend to a decent error  and (b) indicates that it only requires a modest number of
events to achieve a good estimation. Finally  (c) demonstrates that our method scales linearly as the
total number of training events grows.
Figure 2 (d-f) show that COEVOLVE achieves the best predictive performance. Because POISSON-
TENSOR applies an extra time dimension and ﬁts each time interval as a Poisson regression  it
outperforms TIMESVD++ by capturing the ﬁne-grained temporal dynamics. Finally  our method
automatically adapts different contributions of each past item factors to better capture the users’
current latent features  hence it can achieve the best prediction performance overall.

6

(a) MAE by iterations

(b) MAE by events

(c) Scalability

(d) Item recommendation

Figure 2: Estimation error (a) vs. #iterations and (b) vs. #events per user; (c) scalability vs. #events
per user; (d) average rank of the recommended items; (e) and (f) time prediction error.

(e) Time prediction (MAE)

(f) Time prediction (relative)

5.3 Experiments on Real-World Data

Datasets. Our datasets are obtained from three different domains from the TV streaming services
(IPTV)  the commercial review website (Yelp) and the online media services (Reddit). IPTV contains
7 100 users’ watching history of 436 TV programs in 11 months  with 2 392 010 events  and 1 420
movie features  including 1 073 actors  312 directors  22 genres  8 countries and 5 years. Yelp is
available from Yelp Dataset challenge Round 7. It contains reviews for various businesses from
October  2004 to December  2015. We ﬁlter users with more than 100 posts and it contains 100
users and 17 213 businesses with around 35 093 reviews. Reddit contains the discussions events in
January 2014. Furthermore  we randomly selected 1 000 users and collect 1 403 groups that these
users have discussion in  with a total of 10 000 discussion events. For item base feature  IPTV has
movie feature  Yelp has business description  and Reddit does not have it. In experiments we ﬁx the
baseline features. There is no base feature for user. For interaction feature  Reddit and Yelp have
reviews in bag-of-words  and no such feature in IPTV.
Figure 3 shows the predictive performance. For time prediction  COEVOLVE outperforms the baselines
signiﬁcantly  since we explicitly reason and model the effect that past consumption behaviors change
users’ interests and items’ features. In particular  compared with LOWRANKHAWKES  our model
captures the interactions of each user-item pair with a multi-dimensional temporal point processes. It is
more expressive than the respective one-dimensional Hawkes process used by LOWRANKHAWKES 
which ignores the mutual inﬂuence among items. Furthermore  since the unit time is hour  the
improvement over the state-of-art on IPTV is around two weeks and on Reddit is around two days.
Hence our method signiﬁcantly helps online services make better demand predictions.
For item recommendation  COEVOLVE also achieves competitive performance comparable with
LOWRANKHAWKES on IPTV and Reddit. The reason behind the phenomena is that one needs to
compute the rank of the intensity function for the item prediction task  and the value of intensity
function for time prediction. LOWRANKHAWKES might be good at differentiating the rank of
intensity better than COEVOLVE. However  it may not be able to learn the actual value of the intensity
accurately. Hence our method has the order of magnitude improvement in the time prediction task.
In addition to the superb predictive performance  COEVOLVE also learns the time-varying latent
features of users and items. Figure 4 (a) shows that the user is initially interested in TV programs
of adventures  but then the interest changes to Sitcom  Family and Comedy and ﬁnally switches to
the Romance TV programs. Figure 4 (b) shows that Facebook and Apple are the two hot topics in
the month of January 2014. The discussions about Apple suddenly increased on 01/21/2014  which

7

0.100.150.200.250.300100200300400500#iterationsMAEParameters Xη0.10.20.30.4200040006000800010000#eventsMAEParameters Xη101102103104105106#eventstime(s)23.342.8347.2410.3415.2425.31101001000MethodsMARMethodsCoevolvingDynamicPoissonLowRankHawkesPoissonTensorTimeSVD++FIP103408109001101001000MethodsMAEMethodsCoevolvingLowRankHawkesPoissonTensorSTIC0.26.816.2180.2051015MethodsErr %MethodsCoevolvingLowRankHawkesPoissonTensorSTICV
T
P
I

t
i
d
d
e
R

p

l
e
Y

(a) Item recommendation

(b) Time prediction (MAE)

(c) Time prediction (relative)

Figure 3: Prediction results on IPTV  Reddit and Yelp. Results are averaged over ﬁve runs with
different portions of training data and error bar represents the variance.

(a) Feature for a user in IPTV

(b) Feature for the Technology group in Reddit

Figure 4: Learned time-varying features of a user in IPTV and a group in Reddit.

can be traced to the news that Apple won lawsuit against Samsung1. It further demonstrates that our
model can better explain and capture the user behavior in the real world.
6 Conclusion
We have proposed an efﬁcient framework for modeling the co-evolution nature of users’ and items’
latent features. Empirical evaluations on large synthetic and real-world datasets demonstrate its scala-
bility and superior predictive performance. Future work includes extending it to other applications
such as modeling dynamics of social groups  and understanding peoples’ behaviors on Q&A sites.
Acknowledge. This project was supported in part by NSF/NIH BIGDATA 1R01GM108341  ONR
N00014-15-1-2340  NSF IIS-1218749  and NSF CAREER IIS-1350983.

1http://techcrunch.com/2014/01/22/apple-wins-big-against-samsung-in-court/

8

10.41.8150.3177.2191.3110100MethodsMARMethodsCoevolvingLowRankHawkesPoissonTensorTimeSVD++FIP34.5356830.2901.1101000MethodsMAEMethodsCoevolvingLowRankHawkesPoissonTensorSTIC0.44.410.311.20.4036912MethodsErr %MethodsCoevolvingLowRankHawkesPoissonTensorSTIC13.22.5450.1510.7540.7110100MethodsMARMethodsCoevolvingLowRankHawkesPoissonTensorTimeSVD++FIP8.167.2186.4203110100MethodsMAEMethodsCoevolvingLowRankHawkesPoissonTensorSTIC1.19.125.127.21.101020MethodsErr %MethodsCoevolvingLowRankHawkesPoissonTensorSTIC80.190.17800.18100.38320.5101000MethodsMARMethodsCoevolvingLowRankHawkesPoissonTensorTimeSVD++FIP125.9724.3768.4883101000MethodsMAEMethodsCoevolvingLowRankHawkesPoissonTensorSTIC1.821718.821.605101520MethodsErr %MethodsCoevolvingLowRankHawkesPoissonTensorSTIC0.000.250.500.751.0001/0101/2102/1003/0103/2104/1004/3005/2006/0906/2907/1908/0808/2809/1710/0710/2711/16CategoryActionHorrorModernHistoryChildIdolDramaAdventureCostumeCartonSitcomComedyCrimeRomanceSuspenseThrillerFamilyFantasyFictionKung.fuMysteryWar0.000.250.500.751.0001/0101/0301/0501/0701/0901/1101/1301/1501/1701/1901/2101/2301/2501/2701/29CategoryMacbookAntivirusIntelCameraInterfaceSamsungBillPrivacyTwitterCableWikipediaDesktopWatchPriceSoftwareComputerPowerYoutubeNetworkServiceFacebookAppleReferences
[1] O. Aalen  O. Borgan  and H. Gjessing. Survival and event history analysis: a process point of view.

Springer  2008.

[2] D. Agarwal and B.-C. Chen. Regression-based latent factor models. In J. Elder  F. Fogelman-Soulié 

P. Flach  and M. Zaki  editors  KDD  2009.

[3] L. Baltrunas and X. Amatriain. Towards time-dependant recommendation based on implicit feedback 

2009.

[4] L. Charlin  R. Ranganath  J. McInerney  and D. M. Blei. Dynamic poisson factorization. In RecSys  2015.
[5] Y. Chen  D. Pavlov  and J. Canny. Large-scale behavioral targeting. In J. Elder  F. Fogelman-Soulié 

P. Flach  and M. J. Zaki  editors  KDD  2009.

[6] E. C. Chi and T. G. Kolda. On tensors  sparsity  and nonnegative factorizations. SIAM Journal on Matrix

Analysis and Applications  33(4):1272–1299  2012.

[7] D. Cox and P. Lewis. Multivariate point processes. Selected Statistical Papers of Sir David Cox: Volume 1 

Design of Investigations  Statistical Methods and Applications  1:159  2006.

[8] J. K. Cullum and R. A. Willoughby. Lanczos Algorithms for Large Symmetric Eigenvalue Computations:

Vol. 1: Theory  volume 41. SIAM  2002.

[9] N. Du  Y. Wang  N. He  and L. Song. Time sensitive recommendation from recurrent user activities. In

NIPS  2015.

[10] M. D. Ekstrand  J. T. Riedl  and J. A. Konstan. Collaborative ﬁltering recommender systems. Foundations

and Trends in Human-Computer Interaction  4(2):81–173  2011.

[11] M. Farajtabar  Y. Wang  M. Gomez-Rodriguez  S. Li  H. Zha  and L. Song. Coevolve: A joint point

process model for information diffusion and network co-evolution. In NIPS  2015.

[12] P. Gopalan  J. M. Hofman  and D. M. Blei. Scalable recommendation with hierarchical poisson factoriza-

tion. UAI  2015.

[13] S. Gultekin and J. Paisley. A collaborative kalman ﬁlter for time-evolving dyadic processes. In ICDM 

pages 140–149  2014.

[14] A. G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika  58(1):83–

90  1971.

[15] B. Hidasi and D. Tikk. General factorization framework for context-aware recommendations. Data Mining

and Knowledge Discovery  pages 1–30  2015.

[16] K. Kapoor  K. Subbian  J. Srivastava  and P. Schrater. Just in time recommendations: Modeling the

dynamics of boredom in activity streams. In WSDM  2015.

[17] A. Karatzoglou  X. Amatriain  L. Baltrunas  and N. Oliver. Multiverse recommendation: n-dimensional

tensor factorization for context-aware collaborative ﬁltering. In Recsys  2010.

[18] Y. Koren. Collaborative ﬁltering with temporal dynamics. In KDD  2009.
[19] Y. Ogata. On lewis’ simulation method for point processes. IEEE Transactions on Information Theory 

27(1):23–31  1981.

[20] J. Z. J. L. Preeti Bhargava  Thomas Phan. Who  what  when  and where: Multi-dimensional collaborative

recommendations using tensor factorization on sparse user-generated data. In WWW  2015.

[21] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte

carlo. In ICML  2008.

[22] S. Sastry. Some np-complete problems in linear algebra. Honors Projects  1990.
[23] X. Wang  R. Donaldson  C. Nell  P. Gorniak  M. Ester  and J. Bu. Recommending groups to users using

user-group engagement and time-dependent matrix factorization. In AAAI  2016.

[24] Y. Wang  R. Chen  J. Ghosh  J. C. Denny  A. Kho  Y. Chen  B. A. Malin  and J. Sun. Rubik: Knowledge

guided tensor factorization and completion for health data analytics. In KDD  2015.

[25] Y. Wang and A. Pal. Detecting emotions in social media: A constrained optimization approach. In IJCAI 

2015.

[26] Y. Wang  E. Theodorou  A. Verma  and L. Song. A stochastic differential equation framework for guiding

information diffusion. arXiv preprint arXiv:1603.09021  2016.

[27] Y. Wang  B. Xie  N. Du  and L. Song. Isotonic hawkes processes. In ICML  2016.
[28] L. Xiong  X. Chen  T.-K. Huang  J. G. Schneider  and J. G. Carbonell. Temporal collaborative ﬁltering

with bayesian probabilistic tensor factorization. In SDM  2010.

[29] S.-H. Yang  B. Long  A. Smola  N. Sadagopan  Z. Zheng  and H. Zha. Like like alike: joint friendship and

interest propagation in social networks. In WWW  2011.

[30] X. Yi  L. Hong  E. Zhong  N. N. Liu  and S. Rajan. Beyond clicks: Dwell time for personalization. In

RecSys  2014.

9

,Yichen Wang
Nan Du
Rakshit Trivedi
Le Song