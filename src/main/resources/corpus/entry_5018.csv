2011,Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment,While signal estimation under random amplitudes  phase shifts  and additive noise is studied frequently  the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored. We present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals. First  we derive an estimator for the equivalence class of the unknown signal using the notion of Karcher mean on the quotient space of equivalence classes. This step requires the use of Fisher-Rao Riemannian metric  and a square-root representation of signals to enable computations of distances and means under this metric. Then  we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal. This estimation algorithm has many applications: (1)registration/alignment of functional data  (2) separation of phase/amplitude components of functional data  (3) joint demodulation and carrier estimation  and (4) sparse modeling of functional data. Here we demonstrate only (1) and (2):  Given signals are temporally aligned using nonlinear warpings and  thus  separated into their phase and amplitude components. The proposed method for signal alignment is shown to have state of the art performance using Berkeley growth  handwritten signatures  and neuroscience spike train data.,Signal Estimation Under Random Time-Warpings

and Nonlinear Signal Alignment

Sebastian Kurtek

Anuj Srivastava Wei Wu

Department of Statistics

Florida State University  Tallahassee  FL 32306

skurtek anuj wwu@stat.fsu.edu

Abstract

While signal estimation under random amplitudes  phase shifts  and additive noise
is studied frequently  the problem of estimating a deterministic signal under ran-
dom time-warpings has been relatively unexplored. We present a novel framework
for estimating the unknown signal that utilizes the action of the warping group to
form an equivalence relation between signals. First  we derive an estimator for
the equivalence class of the unknown signal using the notion of Karcher mean on
the quotient space of equivalence classes. This step requires the use of Fisher-Rao
Riemannian metric and a square-root representation of signals to enable compu-
tations of distances and means under this metric. Then  we deﬁne a notion of
the center of a class and show that the center of the estimated class is a consis-
tent estimator of the underlying unknown signal. This estimation algorithm has
many applications: (1) registration/alignment of functional data  (2) separation of
phase/amplitude components of functional data  (3) joint demodulation and car-
rier estimation  and (4) sparse modeling of functional data. Here we demonstrate
only (1) and (2): Given signals are temporally aligned using nonlinear warpings
and  thus  separated into their phase and amplitude components. The proposed
method for signal alignment is shown to have state of the art performance using
Berkeley growth  handwritten signatures  and neuroscience spike train data.

1 Introduction

Consider the problem of estimating signal using noisy observation under the model:

f (t) = cg(a t − φ) + e(t)  

where the random quantities c ∈ R is the scale  a ∈ R is the rate  φ ∈ R is the phase shift  and
e(t) ∈ R is the additive noise. There has been an elaborate theory for estimation of the underlying
signal g  given one or several observations of the function f. Often one assumes that g takes a
parametric form  e.g. a superposition of Gaussians or exponentials with different parameters  and
estimates these parameters from the observed data [12]. For instance  the estimation of sinusoids or
exponentials in additive Gaussian noise is a classical problem in signal and speech processing. In
this paper we consider a related but fundamentally different estimation problem where the observed
functional data is modeled as: for t ∈ [0  1] 

fi(t) = cig(γi(t)) + ei 

(1)
Here γi : [0  1] → [0  1] are diffeomorphisms with γi(0) = 0 and γi(1) = 1. The fis represent ob-
servations of an unknown  deterministic signal g under random warpings γi  scalings ci and vertical
translations ei ∈ R. (A more general model would be to use full functions for additive noise but that
requires further discussion due to identiﬁability issues. Thus  we restrict to the above model in this
paper.) This problem is interesting because in many situations  including speech  SONAR  RADAR 

i = 1  2  . . .   n  

1

Figure 1: Separation of phase and amplitude variability in function data.

NMR  fMRI  and MEG applications  the noise can actually affect the instantaneous phase of the sig-
nal  resulting in an observation that is a phase (or frequency) modulation of the original signal. This
problem is challenging because of the nonparametric  random nature of the warping functions γis. It
seems difﬁcult to be able to recover g where its observations have been time-warped nonlinearly in
a random fashion. The past papers have either restricted to linear warpings (e.g. γi(t) = ait− φi) or
known g (e.g. g(t) = cos(t)). It turns out that without any further restrictions on γis one can recover
g only up to an arbitrary warping function. This is easy to see since g ◦ γi = (g ◦ γ) ◦ (γ−1 ◦ γi)
for any warping function γ. (As described later  the warping functions are restricted to be automor-
phisms of a domain and  hence  form a group.) Under an additional condition related to the mean of
(inverses of) γis  we can reach the exact signal g  as demonstrated in this paper.
In fact  this model describes several related  some even equivalent  problems but with distinct
applications:

Problem 1: Joint Phase Demodulation and Carrier Estimation: One can view this problem as
that of phase (or frequency) demodulation but without the knowledge of the carrier signal g. Thus 
it becomes a problem of joint estimation of the carrier signal (g) and phase demodulation (γ−1
)
of signals that share the same carrier. In case the carrier signal g is known  e.g. g is a sinusoid 
then it is relatively easier to estimate the warping functions using dynamic time warping or other
estimation theoretic methods [15  13]. So  we consider problem of estimation of g from {fi} under
the model given in Eqn. 1.

i

Problem 2: Phase-Amplitude Separation: Consider the set of signals shown in the top-left panel
of Fig. 1. These functions differ from each other in both heights and locations of their peaks and
valleys. One would like to separate the variability associated with the heights  called the amplitude
variability  from the variability associated with the locations  termed the phase variability. Although
this problem has been studied for almost two decades in the statistics community  see e.g.
[7 
9  4  11  8]  it is still considered an open problem. Extracting the amplitude variability implies
temporally aligning the given functions using nonlinear time warping  with the result shown in the
bottom right. The corresponding set of warping functions  shown in the top right  represent the
phase variability. The phase component can also be illustrated by applying these warping functions
to the same function  also shown in the top right. The main reason for separating functional data into
these components is to better preserve the structure of the observed data  since a separate modeling
of amplitude and phase variability will be more natural  parsimonious and efﬁcient. It may not be
obvious but the solution to this separation problem is intimately connected to the estimation of g in
Eqn. 1.

Problem 3: Multiple Signal/Image Registration: The problem of phase-amplitude separation is
intrinsically same as the problem of joint registration of multiple signals. The problem here is: Given
a set of observed signals {fi} estimate the corresponding points in their domains. In other words 

2

−3−2−101230.40.50.60.70.80.911.11.21.300.20.40.60.8100.10.20.30.40.50.60.70.80.91−3−2−101230.40.50.60.70.80.911.11.21.3−3−2−101230.40.50.60.70.80.911.11.21.3−3−2−101230.20.30.40.50.60.70.80.911.11.2−3−2−101230.40.50.60.70.80.911.11.21.3original datamean +/- STD before warpingmean +/- STD after warpingwarping functionsphase componentsamplitude componentsi

what are the γis such that  for any t0  fi(γ−1
(t0)) correspond to each other. The bottom right panels
of Fig. 1 show the registered signals. Although this problem is more commonly studied for images 
its one-dimensional version is non-trivial and helps understand the basic challenges. We will study
the 1D problem in this paper but  at least conceptually  the solutions extend to higher-dimensional
problems also.
In this paper we provide the following speciﬁc contributions. We study the problem of estimating
g given a set {fi} under the model given in Eqn. 1 and propose a consistent estimator for this
problem  along with the supporting asymptotic theory. Also  we illustrate the use of this solution in
automated alignment of sets of given signals. Our framework is based on an equivalence relation
between signals deﬁned as follows. Two signals  are deemed equivalent if one can be time-warped
into the other; since the warping functions form a group  the equivalence class is an orbit of the
warping group. This relation partitions the set of signals into equivalence classes  and the set of
equivalence classes (orbits) forms a quotient space. Our estimation of g is based on two steps. First 
we estimate the equivalence class of g using the notion of Karcher mean on quotient space which 
in turn  requires a distance on this quotient space. This distance should respect the equivalence
structure  i.e. the distance between any elements should be zero if and only if they are in the same
class. We propose to use a distance that results from the Fisher-Rao Riemannian metric. This
metric was introduced in 1945 by C. R. Rao [10] and studied rigorously in the 70s and 80s by
Amari [1]  Efron [3]  Kass [6]  Cencov [2]  and others. While those earlier efforts were focused
on analyzing parametric families  we use the nonparametric version of the Fisher-Rao Riemannian
metric in this paper. The difﬁculty in using this metric directly is that it is not straightforward to
compute geodesics (remember that geodesics lengths provide the desired distances). However  a
simple square-root transformation converts this metric into the standard L2 metric and the distance
is obtainable as a simple L2 norm between the square-root forms of functions. Second  given an
estimate of the equivalence class of g  we deﬁne the notion of a center of an orbit and use that to
derive an estimator for g.

2 Background Material

We introduce some notation. Let Γ be the set of orientation-preserving diffeomorphisms of the unit
interval [0  1]: Γ = {γ : [0  1] → [0  1]|γ(0) = 0  γ(1) = 1  γ is a diffeo}. Elements of Γ form a
(cid:82) 1
group  i.e. (1) for any γ1  γ2 ∈ Γ  their composition γ1 ◦ γ2 ∈ Γ; and (2) for any γ ∈ Γ  its inverse
γ−1 ∈ Γ  where the identity is the self-mapping γid(t) = t. We will use (cid:107)f(cid:107) to denote the L2 norm
0 |f (t)|2dt)1/2.
(
2.1 Representation Space of Functions

(cid:189)

(cid:112)|x|

0

x/

Let f be a real-valued function on the interval [0  1]. We are going to restrict to those f that are
absolutely continuous on [0  1]; let F denote the set of all such functions. We deﬁne a mapping:
if |x| (cid:54)= 0
Q : R → R according to: Q(x) ≡
otherwise . Note that Q is a continuous map.
For the purpose of studying the function f  we will represent it using a square-root velocity function
(SRVF) deﬁned as q : [0  1] → R  where q(t) ≡ Q( ˙f (t)) = ˙f (t)/
| ˙f (t)|. It can be shown that if
the function f is absolutely continuous  then the resulting SRVF is square integrable. Thus  we will
deﬁne L2([0  1]  R) (or simply L2) to be the set of all SRVFs. For every q ∈ L2 there exists a function
f (unique up to a constant  or a vertical translation) such that the given q is the SRVF of that f. If
we warp a function f by γ  the SRVF of f ◦ γ is given by: ˜q(t) =
˙γ(t).
√
We will denote this transformation by (q  γ) = (q ◦ γ)

√
dt (f◦γ)(t)
dt (f◦γ)(t)| = (q ◦ γ)(t)
| d

(cid:113)

(cid:112)

˙γ.

d

2.2 Elastic Riemannian Metric
Deﬁnition 1 For any f ∈ F and v1  v2 ∈ Tf (F)  where Tf (F) is the tangent space to F at f  the
Fisher-Rao Riemannian metric is deﬁned as the inner product:

(cid:104)(cid:104)v1  v2(cid:105)(cid:105)f =

1
4

1
| ˙f (t)| dt .

(2)

(cid:90) 1

˙v1(t) ˙v2(t)

0

3

This metric has many fundamental advantages  including the fact that it is the only Riemannian
metric that is invariant to the domain warping [2]. This metric is somewhat complicated since it
changes from point to point on F  and it is not straightforward to derive equations for computing
geodesics in F. However  a small transformation provide an enormous simpliﬁcation of this task.
This motivates the use of SRVFs for representing and aligning elastic functions.
Lemma 1 Under the SRVF representation  the Fisher-Rao Riemannian metric becomes the stan-
dard L2 metric.

This result can be used to compute the distance dF R between any two functions by computing the
L2 distance between the corresponding SRVFs  that is  dF R(f1  f2) = (cid:107)q1 − q2(cid:107). The next question
is: What is the effect of warping on dF R? This is answered by the following result of isometry.
Lemma 2 For any two SRVFs q1  q2 ∈ L2 and γ ∈ Γ  (cid:107)(q1  γ) − (q2  γ)(cid:107) = (cid:107)q1 − q2(cid:107).

2.3 Elastic Distance on Quotient Space

Our next step is to deﬁne an elastic distance between functions as follows. The orbit of an SRVF
q ∈ L2 is given by: [q] = closure{(q  γ)|γ ∈ Γ}. It is the set of SRVFs associated with all the
warpings of a function  and their limit points. Let S denote the set of all such orbits. To compare
any two orbits we need a metric on S. We will use the Fisher-Rao distance to induce a distance
between orbits  and we can do that only because under this the action of Γ is by isometries.
Deﬁnition 2 For any two functions f1  f2 ∈ F and the corresponding SRVFs  q1  q2 ∈ L2  we
deﬁne the elastic distance d on the quotient space S to be: d([q1]  [q2]) = inf γ∈Γ (cid:107)q1 − (q2  γ)(cid:107).
Note that the distance d between a function and its domain-warped version is zero. However  it can
be shown that if two SRVFs belong to different orbits  then the distance between them is non-zero.
Thus  this distance d is a proper distance (i.e. it satisﬁes non-negativity  symmetry  and the triangle
inequality) on S but not on L2 itself  where it is only a pseudo-distance.

3 Signal Estimation Method
Our estimation is based on the model fi = ci(g ◦ γi) + ei  i = 1 ···   n  where g  fi ∈ F  ci ∈ R+ 
γi ∈ Γ and ei ∈ R. Given {fi}  our goal is to identify warping functions {γi} so as to reconstruct g.
We will do so in three steps: 1) For a given collection of functions {fi}  and their SRVFs {qi}  we
compute the mean of the corresponding orbits {[qi]} in the quotient space S; we will call it [µ]n. 2)
We compute an appropriate element of this mean orbit to deﬁne a template µn in L2. The optimal
warping functions {γi} are estimated by align individual functions to match the template µn. 3) The
estimated warping functions are then used to align {fi} and reconstruct the underlying signal g.

√

(cid:82) 1

3.1 Pre-step: Karcher Mean of Points in Γ
In this section we will deﬁne a Karcher mean of a set of warping functions {γi}  under the Fisher-
Rao metric  using the differential geometry of Γ. Analysis on Γ is not straightforward because it is a
nonlinear manifold. To understand its geometry  we will represent an element γ ∈ Γ by the square-
(cid:82) t
root of its derivative ψ =
˙γ. Note that this is the same as the SRVF deﬁned earlier for elements
of F  except that ˙γ > 0 here. Since γ(0) = 0  the mapping from γ to ψ is a bijection and one
(cid:82) 1
can reconstruct γ from ψ using γ(t) =
0 ψ(s)2ds. An important advantage of this transformation
is that since (cid:107)ψ(cid:107)2 =
0 ˙γ(t)dt = γ(1) − γ(0) = 1  the set of all such ψs is S∞ 
the unit sphere in the Hilbert space L2. In other words  the square-root representation simpliﬁes the
complicated geometry of Γ to the unit sphere. Recall that the distance between any two points on
the unit sphere  under the Euclidean metric  is simply the length of the shortest arc of a great circle
connecting them on the sphere. Using Lemma 1  the Fisher-Rao distance between any two warping
functions is found to be dF R(γ1  γ2) = cos−1(
˙γ2(t)dt). Now that we have a proper
distance on Γ  we can deﬁne a Karcher mean as follows.
Deﬁnition 3 For a given set of warping functions γ1  γ2  . . .   γn ∈ Γ  deﬁne their Karcher mean to
be ¯γn = argminγ∈Γ

0 ψ(t)2dt =

i=1 dF R(γ  γi)2.

(cid:80)n

(cid:112)

(cid:82) 1

0

˙γ1(t)

(cid:112)

4

The search for this minimum is performed using a standard iterative algorithm that is not repeated
here to save space.

3.2 Step 1: Karcher Mean of Points in S = L2/Γ
Next we consider the problem of ﬁnding means of points in the quotient space S.
Deﬁnition 4 Deﬁne the Karcher mean [µ]n of the given SRVF orbits {[qi]} in the space S as a local
minimum of the sum of squares of elastic distances:

n(cid:88)

i=1

[µ]n = argmin

[q]∈S

d([q]  [qi])2 .

(3)

We emphasize that the Karcher mean [µ]n is actually an orbit of functions  rather than a function.
The full algorithm for computing the Karcher mean in S is given next.
Algorithm 1: Karcher Mean of {[qi]} in S

1. Initialization Step: Select µ = qj  where j is any index in argmin1≤i≤n ||qi− 1
2. For each qi ﬁnd γ∗

(cid:80)n
k=1 qk||.
i = argminγ∈Γ (cid:107)µ − (qi  γ)(cid:107). The solution to this

i by solving: γ∗

optimization comes from a dynamic programming algorithm in a discretized domain.

3. Compute the aligned SRVFs using ˜qi (cid:55)→ (qi  γ∗
i ).
4. If the increment (cid:107) 1

(cid:80)n
i=1 ˜qi − µ(cid:107) is small  then stop. Else  update the mean using µ (cid:55)→

n

n

(cid:80)n

1
n

i=1 ˜qi and return to step 2.

(cid:80)n
i=1 (cid:107)µ(k) − ˜q(k)

The iterative update in Steps 2-4 is based on the gradient of the cost function given in Eqn. 3.
Denote the estimated mean in the kth iteration by µ(k). In the kth iteration  let γ(k)
denote the
optimal domain warping from qi to µ(k) and let ˜q(k)
i=1 d([µ(k)]  [qi])2 =
i=1 d([µ(k+1)]  [qi])2. Thus  the cost function
i=1 d([µ(k)]  [qi])2 will always converge.

i (cid:107)2 ≥(cid:80)n
(cid:80)n

decreases iteratively and as zero is a lower bound 

i (cid:107)2 ≥(cid:80)n

i=1 (cid:107)µ(k+1) − ˜q(k)

i = (qi  γ(k)

). Then 

i

i

(cid:80)n

3.3 Step 2: Center of an Orbit

Here we ﬁnd a particular element of this mean orbit so that it can be used as a template to align the
given functions.

Deﬁnition 5 For a given set of SRVFs q1  q2  . . .   qn and q  deﬁne an element ˜q of [q] as the center
of [q] with respect to the set {qi} if the warping functions {γi}  where γi = argminγ∈Γ (cid:107)˜q−(qi  γ)(cid:107) 
have the Karcher mean γid.

We will prove the existence of such an element by construction.
Algorithm 2: Finding Center of an Orbit : WLOG  let q be any element of the orbit [q].

1. For each qi ﬁnd γi by solving: γi = argminγ∈Γ (cid:107)q − (qi  γ)(cid:107).
2. Compute the mean ¯γn of all {γi}. The center of [q] wrt {qi} is given by ˜q = (q  ¯γ−1
n ).

(cid:80)n

We need to show that ˜q resulting from Algorithm 2 satisﬁes the mean condition in Deﬁnition 5. Note
n ) − (qi  γ)(cid:107) =
that γi is chosen to minimize (cid:107)q − (qi  γ)(cid:107)  and also that (cid:107)˜q − (qi  γ)(cid:107) = (cid:107)(q  ¯γ−1
(cid:80)n
(cid:107)q − (qi  γ ◦ ¯γn)(cid:107). Therefore  γ∗
n minimizes (cid:107)˜q − (qi  γ)(cid:107). That is  γ∗
i is a warping
that aligns qi to ˜q. To verify the Karcher mean of γ∗
i   we compute the sum of squared distances
i=1 dF R(γ ◦ ¯γn  γi)2. As ¯γn is already the
n )2 =
mean of γi  this sum of squares is minimized when γ = γid. That is  the mean of γ∗
We will apply this setup in our problem by ﬁnding the center of [µ]n with respect to the SRVFs {qi}.

(cid:80)n
i = γi ◦ ¯γ−1
i=1 dF R(γ  γi ◦ ¯γ−1

i=1 dF R(γ  γ∗

i is γid.

i )2 =

5

g

{fi}

{ ˜fi}

estimate of g

error w.r.t. n

Figure 2: Example of consistent estimation.

3.4 Steps 1-3: Complete Estimation Algorithm
Consider the observation model fi = ci(g ◦ γi) + ei  i = 1  . . .   n  where g is an unknown signal 
and ci ∈ R+  γi ∈ Γ and ei ∈ R are random. Given the observations {fi}  the goal is to estimate the
signal g. To make the system identiﬁable  we need some constraints on γi  ci  and ei. In this paper 
the constraints are: 1) the population mean of {γ−1
i } is identity γid  and 2) the population Karcher
means of {ci} and {ei} are known  denoted by E(¯c) and E(¯e)  respectively. Now we can utilize
Algorithms 1 and 2 to present the full procedure for function alignment and signal estimation.
Complete Estimation Algorithm: Given a set of functions {fi}n
E(¯c) and E(¯e). Let {qi}n

i=1 denote the SRVFs of {fi}n

i=1 on [0  1]  and population means

i=1  respectively.

(cid:80)n

1. Computer the Karcher mean of {[qi]} in S using Algorithm 1; Denote it by [µ]n.
2. Find the center of [µ]n wrt {qi} using Algorithm 2; call it µn.
3. For i = 1  2  . . .   n  ﬁnd γ∗
i by solving: γ∗
4. Compute the aligned SRVFs ˜qi = (qi  γ∗
5. Return the warping functions {γ∗

i = argminγ∈Γ (cid:107)µn − (qi  γ)(cid:107).
i ) and aligned functions ˜fi = fi ◦ γ∗
i .

i } and the estimated signal ˆg = ( 1

˜fi−E(¯e))/E(¯c).
Illustration. We illustrate the estimation process using an example which is a quadratically-
enveloped sine-wave function g(t) = (1 − (1 − 2t)2) sin(5πt)  t ∈ [0  1]. We randomly generate
n = 50 warping functions {γi} such that {γ−1
i } are i.i.d with mean γid. We also generate i.i.d
sequences {ci} and {ei} from the exponential distribution with mean 1 and the standard normal
distribution  respectively. Then we compute functions fi = ci(g ◦ γi) + ei to form the functional
data. In Fig. 2  the ﬁrst panel shows the function g  and the second panel shows the data {fi}. The
Complete Estimation Algorithm results in the aligned functions { ˜fi = fi ◦ γ∗
i } that are are shown
in the third panel in Fig. 2. In this case  E(¯c)) = 1  E(¯e) = 0. This estimated g (red) using the
Complete Estimation Algorithm as well as the true g (blue) are shown in the fourth panel. Note
that the estimate is very successful despite large variability in the raw data. Finally  we examine
the performance of the estimator with respect to the sample size  by performing this estimation for
n equal to 5  10  20  30  and 40. The estimation errors  computed using the L2 norm between esti-
mated g’s and the true g  are shown in the last panel. As we will show in the following theoretical
development  this estimate converges to the true g when the sample size n grows large.

n

i=1

4 Estimator Consistency and Asymptotics

In this section we mathematically demonstrate that the proposed algorithms in Section 3 provide
a consistent estimator for the underlying function g. This or related problems have been consid-
ered previously by several papers  including [14  9]  but we are not aware of any formal statistical
solution.
At ﬁrst  we establish the following useful result.
Lemma 3 For any q1  q2 ∈ L2 and a constant c > 0  we have argminγ∈Γ (cid:107)q1 − (q2  γ)(cid:107) =
argminγ∈Γ (cid:107)cq1 − (q2  γ)(cid:107).
Corollary 1 For any function q ∈ L2 and constant c > 0  we have γid ∈ argminγ∈Γ (cid:107)cq − (q  γ)(cid:107).
Moreover  if the set {t ∈ [0  1]|q(t) = 0} has (Lebesgue) measure 0  γid = argminγ∈Γ (cid:107)cq−(q  γ)(cid:107).

6

00.51−4−202400.51−4−202400.51−4−202400.51−4−2024  true gestimated g5102030405000.30.6Based on Lemma 3 and Corollary 1  we have the following result on the Karcher mean in the quotient
space S.
Theorem 1 For a function g  consider a sequence of functions fi(t) = cig(γi(t)) + ei  where ci
is a positive constant  ei is a constant  and γi is a time warping  i = 1 ···   n. Denote by qg
and qi the SRVFs of g and fi  respectively  and let ¯s = 1
ci. Then  the Karcher mean of
{[qi]  i = 1  2  . . .   n} in S is ¯s[qg]. That is 
n

(cid:80)n

√

i=1

(cid:195)

N(cid:88)

(cid:33)

[µ]n ≡ argmin

d2([qi]  [q])

= ¯s[qg] = ¯s{(qg  γ)  γ ∈ Γ} .

[q]

i=1

Next  we present a simple fact about the Karcher mean (see Deﬁnition 3) of warping functions.
Lemma 4 Given a set {γi ∈ Γ|i = 1  ...  n} and a γ0 ∈ Γ  if the Karcher mean of {γi} is ¯γ  then
the Karcher mean of {γi ◦ γ0} is ¯γ ◦ γ0.
Theorem 1 ensures that [µ]n belongs to the orbit of [qg] (up to a scale factor) but we are interested in
estimating g itself  rather than its orbit. We will show in two steps (Theorems 2 and 3) that ﬁnding
the center of the orbit [µ]n leads to a consistent estimator for g.
Theorem 2 Under the same conditions as in Theorem 1  let µ = (¯sqg  γ0)  for γ0 ∈ Γ  denote an
arbitrary element of the Karcher mean class [µ]n = ¯s[qg]. Assume that the set {t ∈ [0  1]| ˙g(t) = 0}
i } is γid  then the center of the
has Lebesgue measure zero. If the population Karcher mean of {γ−1
orbit [µ]n  denoted by µn  satisﬁes limn→∞ µn = E(¯s)qg.
This result shows that asymptotically one can recover the SRVF of the original signal by the Karcher
mean of the SRVFs of the observed signals. Next in Theorem 3  we will show that one can also
reconstruct g using aligned functions { ˜fi} generated by the Alignment Algorithm in Section 3.
Theorem 3 Under the same conditions as in Theorem 2  let γ∗
fi◦ γ∗

i = argminγ (cid:107)(qi  γ)− µn(cid:107) and ˜fi =
˜fi = E(¯c)g + E(¯e).

i=1 ei  then limn→∞ 1

i . If we denote ¯c = 1
n

i=1 ci and ¯e = 1

(cid:80)n

(cid:80)n

(cid:80)n

n

i=1

n

5 Application to Signal Alignment

In this section we will focus on function alignment and comparison of alignment performance with
some previous methods on several datasets. In this case  the given signals are viewed as {fi} in the
previous set up and we estimate the center of the orbit and then use it for alignment of all signals.
The datasets include 3 real experimental applications listed below. The data are shown in Column 1
in Fig. 3.

1. Real Data 1. Berkeley Growth Data: The Berkeley growth dataset for 39 male subjects
[11]. For better illustrations  we have used the ﬁrst derivatives of the growth (i.e. growth
velocity) curves as the functions {fi} in our analysis.

2. Real Data 2. Handwriting Signature Data: 20 handwritten signatures and the acceler-
ation functions along the signature curves [8]. Let (x(t)  y(t)) denote the x and y coor-
dinates of a signature traced as a function of time t. We study the acceleration functions
f (t) =

¨x(t)2 + ¨y(t)2 of the signatures.

(cid:112)

3. Real Data 3. Neural Spike Data: Spiking activity of one motor cortical neuron in a
Macaque monkey was recorded during arm-movement behavior [16]. The smoothed (using
a Gaussian kernel) spike trains over 10 movement trials are used in this alignment analysis.

There are no standard criteria on evaluating function alignment in the current literature. Here we use
the following three criteria so that together they provide a comprehensive evaluation  where fi and
˜fi  i = 1  ...  N  denote the original and the aligned functions  respectively.

1. Least Squares: ls = 1
N

j(cid:54)=i
j(cid:54)=i fj (t))2dt. ls measures the cross-sectional
variance of the aligned functions  relative to original values. The smaller the value of ls 
the better the alignment is in general.

i=1

˜fj (t))2dt

( ˜fi(t)− 1
N−1
(fi(t)− 1
N−1

(cid:80)N

(cid:82)
(cid:82)

(cid:80)
(cid:80)

7

Original

PACE [11]

SMR [4]

MBM [5]

F-R

Growth-male

(0.91  1.09  0.68)

(0.45  1.17  0.77)

(0.70  1.17  0.62)

(0.64  1.18  0.31)

Signature

(0.91  1.18  0.84)

(0.62  1.59  0.31)

(0.64  1.57  0.46)

(0.56  1.79  0.31)

Neural data

(0.87  1.35  1.10)

(0.69  2.54  0.95)

(0.48  3.06  0.40)

(0.40  3.77  0.28)

Figure 3: Empirical evaluation of four methods on 3 real datasets  with the alignment performance
computed using three criteria (ls  pc  sls). The best cases are shown in boldface.

2. Pairwise Correlation: pc =

(cid:80)
(cid:80)

correlation between functions. Large values of pc indicate good sychronization.

i(cid:54)=j cc( ˜fi(t)  ˜fj (t))
i(cid:54)=j cc(fi(t) fj (t))   where cc(f  g) is the pairwise Pearson’s
(cid:80)N
(cid:80)N

(cid:80)N
(cid:80)N

(cid:82)
(cid:82)

i=1

N

˙˜fi(t)− 1
(
( ˙fi(t)− 1

3. Sobolev Least Squares: sls =

  This criterion measures the
total cross-sectional variance of the derivatives of the aligned functions  relative to the
original value. The smaller the value of sls  the better synchronization the method achieves.

j=1

j=1

i=1

N

˙˜fj )2dt
˙fj )2dt

We compare our Fisher-Rao (F-R) method with the Tang-M¨uller method [11] provided in principal
analysis by conditional expectation (PACE) package  the self-modeling registration (SMR) method
presented in [4]  and the moment-based matching (MBM) technique presented in [5]. Fig. 3 sum-
marizes the values of (ls  pc  sls) for these four methods using 3 real datasets. From the results  we
can see that the F-R method does uniformly well in functional alignment under all the evaluation
metrics. We have found that the ls criterion is sometimes misleading in the sense that a low value
can result even if the functions are not very well aligned. This is the case  for example  in the male
growth data under SMR method. Here the ls = 0.45  while for our method ls = 0.64  even though
it is easy to see that latter has performed a better alignment. On the other hand  the sls criterion
seems to best correlate with a visual evaluation of the alignment. The neural spike train data is the
most challenging and no other method except ours does a good job.

6 Summary

In this paper we have described a parameter-free approach for reconstructing underlying signal using
given functions with random warpings  scalings  and translations. The basic idea is to use the Fisher-
Rao Riemannian metric and the resulting geodesic distance to deﬁne a proper distance  called elastic
distance  between warping orbits of SRVF functions. This distance is used to compute a Karcher
mean of the orbits  and a template is selected from the mean orbit using an additional condition
that the mean of the warping functions is identity. By applying these warpings on the original func-
tions  we provide a consistent estimator of the underlying signal. One interesting application of this
framework is in aligning functions with signiﬁcant x-variability. We show the the proposed Fisher-
Rao method provides better alignment performance than the state-of-the-art methods in several real
experimental data.

8

5101501020305101501020305101501020305101501020305101501020302040608000.511.52040608000.511.52040608000.511.52040608000.511.52040608000.511.50.511.522.500.511.50.511.522.500.511.50.511.522.500.511.50.511.522.500.511.50.511.522.500.511.5References
[1] S. Amari. Differential Geometric Methods in Statistics. Lecture Notes in Statistics  Vol. 28.

Springer  1985.

[2] N. N. ˇCencov. Statistical Decision Rules and Optimal Inferences  volume 53 of Translations

of Mathematical Monographs. AMS  Providence  USA  1982.

[3] B. Efron. Deﬁning the curvature of a statistical problem (with applications to second order

efﬁciency). Ann. Statist.  3:1189–1242  1975.

[4] D. Gervini and T. Gasser. Self-modeling warping functions. Journal of the Royal Statistical

Society  Ser. B  66:959–971  2004.

[5] G. James. Curve alignment by moments. Annals of Applied Statistics  1(2):480–501  2007.
[6] R. E. Kass and P. W. Vos. Geometric Foundations of Asymptotic Inference. John Wiley &

Sons  Inc.  1997.

[7] A. Kneip and T. Gasser. Statistical tools to analyze data representing a sample of curves. The

Annals of Statistics  20:1266–1305  1992.

[8] A. Kneip and J. O. Ramsay. Combining registration and ﬁtting for functional models. Journal

of American Statistical Association  103(483)  2008.

[9] J. O. Ramsay and X. Li. Curve registration. Journal of the Royal Statistical Society  Ser. B 

60:351–363  1998.

[10] C. R. Rao.

Information and accuracy attainable in the estimation of statistical parameters.

Bulletin of Calcutta Mathematical Society  37:81–91  1945.

[11] R. Tang and H. G. Muller. Pairwise curve synchronization for functional data. Biometrika 

95(4):875–889  2008.

[12] H.L. Van Trees. Detection  Estimation  and Modulation Theory  vol. I. John Wiley  N.Y.  1971.
[13] M. Tsang  J. H. Shapiro  and S. Lloyd. Quantum theory of optical temporal phase and instan-

taneous frequency. Phys. Rev. A  78(5):053820  Nov 2008.

[14] K. Wang and T. Gasser. Alignment of curves by dynamic time warping. Annals of Statistics 

25(3):1251–1276  1997.

[15] A. Willsky. Fourier series and estimation on the circle with applications to synchronous
communication–I: Analysis. IEEE Transactions on Information Theory  20(5):577 – 583  sep
1974.

[16] W. Wu and A. Srivastava. Towards Statistical Summaries of Spike Train Data. Journal of

Neuroscience Methods  195:107–110  2011.

9

,Wouter Koolen
Oluwasanmi Koyejo
Rajiv Khanna
Joydeep Ghosh
Russell Poldrack
Zhaobin Kuang
Sinong Geng
David Page
Jianlong Chang
xinbang zhang
Yiwen Guo
GAOFENG MENG
SHIMING XIANG
Chunhong Pan