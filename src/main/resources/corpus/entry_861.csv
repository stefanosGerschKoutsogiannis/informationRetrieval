2019,Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks,Recently  neural network based approaches have achieved significant progress for solving large  complex  graph-structured problems. Nevertheless  the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper  we first analyze key factors constraining the expressive power of existing Graph Convolutional Networks (GCNs)  including the activation function and shallow learning mechanisms. Then  we generalize spectral graph convolution and deep GCN in block Krylov subspace forms  upon which we devise two architectures  both scalable in depth however making use of multi-scale information differently. On several node classification tasks  the proposed architectures achieve state-of-the-art performance.,Break the Ceiling: Stronger Multi-scale Deep

Graph Convolutional Networks

Sitao Luan1 2 ⇤  Mingde Zhao1 2 ⇤  Xiao-Wen Chang1  Doina Precup1 2 3
{sitao.luan@mail  mingde.zhao@mail  chang@cs  dprecup@cs}.mcgill.ca

1McGill University; 2Mila; 3DeepMind

⇤Equal Contribution

Abstract

Recently  neural network based approaches have achieved signiﬁcant
progress for solving large  complex  graph-structured problems. Never-
theless  the advantages of multi-scale information and deep architectures
have not been suciently exploited. In this paper  we ﬁrst analyze key
factors constraining the expressive power of existing Graph Convolutional
Networks (GCNs)  including the activation function and shallow learning
mechanisms. Then  we generalize spectral graph convolution and deep
GCN in block Krylov subspace forms  upon which we devise two architec-
tures  both scalable in depth however making use of multi-scale information
di↵erently. On several node classiﬁcation tasks  the proposed architectures
achieve state-of-the-art performance.

1

Introduction & Motivation

Many real-world problems can be modeled as graphs [14  18  25  12  27  7]. Inspired by
the success of Convolutional Neural Networks (CNNs) [20] in computer vision [22]  graph
convolution deﬁned on graph Fourier domain stands out as the key operator and one of
the most powerful tools for using machine learning to solve graph problems. In this paper 
we focus on spectrum-free Graph Convolutional Networks (GCNs) [2  29]  which have
demonstrated state-of-the-art performance on many transductive and inductive learning
tasks [7  18  25  3  4].
One major problem of the existing GCNs is the low expressive power limited by their
shallow learning mechanisms [38  36]. There are mainly two reasons why people have
not yet achieved an architecture that is scalable in depth. First  this problem is dicult:
considering graph convolution as a special form of Laplacian smoothing [21]  networks with
multiple convolutional layers will su↵er from an over-smoothing problem that makes the
representation of even distant nodes indistinguishable [38]. Second  some people think it is
unnecessary: for example  [2] states that it is not necessary for the label information to totally
traverse the entire graph and one can operate on the multi-scale coarsened input graph
and obtain the same ﬂow of information as GCNs with more layers. Acknowledging the
diculty  we hold on to the objective of deepening GCNs since the desired compositionality1
will yield easy articulation and consistent performance for problems with di↵erent scales.
In this paper  we break the performance ceiling of the GCNs. First  we analyze the limits of
the existing GCNs brought by the shallow learning mechanisms and the activation functions.
Then  we show that any graph convolution with a well-deﬁned analytic spectral ﬁlter can

1The expressive power of a sound deep NN architecture should be expected to grow with the

increment of network depth [19  16].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

be written as a product of a block Krylov matrix and a learnable parameter matrix in a
special form. Based on this  we propose two GCN architectures that leverage multi-scale
information in di↵erent ways and are scalable in depth  with stronger expressive powers
and abilities to extract richer representations of graph-structured data. We also show that
the equivalence of the two architectures can be achieved under certain conditions. For
empirical validation  we test di↵erent instances of the proposed architectures on multiple
node classiﬁcation tasks. The results show that even the simplest instance of the architectures
achieves state-of-the-art performance  and the complex ones achieve surprisingly higher
performance  with or without validation sets.

2 Why Deep GCN Does Not Work Well?

2.1 Foundations

As in [11]  we use bold font for vectors (e.g. v)  block vectors (e.g. V) and matrix blocks
(e.g. Vi). Suppose we have an undirected graph G = (V E  A)  where V is the node set
with |V| = N  E is the edge set with |E| = E  A 2 RN⇥N is a symmetric adjacency matrix
and D is a diagonal degree matrix  i.e. Dii =Pj Aij. A di↵usion process [6  5] on G can
be deﬁned by a di↵usion operator L  which is a symmetric matrix  e.g. graph Laplacian
L = D  A  normalized graph Laplacian L = I  D1/2AD1/2 and anity matrix L = A + I 
etc.. In this paper  we use L for a general di↵usion operator  unless speciﬁed otherwise. The
eigendecomposition of L gives us L = U⇤UT  where ⇤ is a diagonal matrix whose diagonal
elements are eigenvalues and the columns of U are the orthonormal eigenvectors  named
graph Fourier basis. We also have a feature matrix (graph signals) X 2 RN⇥F (which can be
regarded as a block vector) deﬁned on V and each node i has a feature vector Xi :  which is
the i-th row of X.
Spectral graph convolution is deﬁned in graph Fourier domain s.t. x⇤G y = U((UTx) (UTy)) 
where x  y 2 RN and  is the Hadamard product [7]. Following this deﬁnition  a graph
signal x ﬁltered by g✓ can be written as

y = g✓(L)x = g✓(U⇤UT)x = Ug✓(⇤)UTx

(1)
where g✓ is any function which is analytic inside a closed contour which encircles (L)  e.g.
Chebyshev polynomial [7]. GCN generalizes this deﬁnition to signals with F input channels
and O output channels and its network structure can be described as

Y = softmax(L ReLU(LXW0) W1)

(2)

where

˜A ⌘ A + I 

(3)
This is called spectrum-free method [2] since it requires no explicit computation of eigende-
composition and operations on the frequency domain [38].

˜D ⌘ diag(Pj ˜A1j  . . .  Pj ˜ANj)

L ⌘ ˜D1/2 ˜A ˜D1/2 

2.2 Problems

Suppose we deepen GCN in the same way as [18  21]  we have

Y = softmax(L ReLU(··· L ReLU(L ReLU(LXW0) W1) W2 ··· ) Wn) ⌘ softmax(Y0)
(4)
For this architecture  [21] gives an analysis on the e↵ect of L without considering the ReLU
activation function. Our analyses on (4) can be summarized in the following theorems.
Theorem 1. Suppose that G has k connected components and the di↵usion operator L is
deﬁned as that in (3). Let X 2 RN⇥F be any block vector and let Wj be any non-negative
parameter matrix with kWjk2  1 for j = 0  1  . . .. If G has no bipartite components  then in
(4)  as n ! 1  rank(Y0)  k.
Proof

See Appendix A.

⇤

2

Conjecture 1. Theorem 1 still holds without the non-negative constraint on the parameter
matrices.

Theorem 2. Suppose the n-dimensional x and y are independently sampled from a contin-
uous distribution and the activation function Tanh(z) = ezez
ez+ez is applied to [x  y] pointwisely 
then

P(rankTanh([x  y]) = rank([x  y])) = 1

See Appendix A.

Proof
⇤
Theorem 1 shows that if we simply deepen GCN  the extracted features will degrade  i.e.
Y0 only contains the stationary information of the graph structure and loses all the local
information in node for being smoothed.
In addition  from the proof we see that the
pointwise ReLU transformation is a conspirator. Theorem 2 tells us that Tanh is better at
keeping linear independence among column features. We design a numerical experiment on
synthetic data (see Appendix) to test  under a 100-layer GCN architecture  how activation
functions a↵ect the rank of the output in each hidden layer during the feedforward process.
As Figure 1(a) shows  the rank of hidden features decreases rapidly with ReLU  while having
little ﬂuctuation under Tanh  and even the identity function performs better than ReLU (see
Appendix for more comparisons). So we propose to replace ReLU by Tanh.

140

120

100

80

60

40

20

0

0

Identity
Sigmoid
ReLU
LeakyReLU
Tanh

20

40

60

80

100

(a) Deep GCN

140

120

100

80

60

40

20

0

0

140

120

100

80

60

40

20

0

0

Identity
Sigmoid
ReLU
LeakyReLU
Tanh

20

40

60

80

100

(b) Snowball

Identity
Sigmoid
ReLU
LeakyReLU
Tanh

20

40

60

80

100

(c) Truncated Block Krylov

Figure 1: Changes in the number of independent features with the increment of network
depth

3 Spectral Graph Convolution and Block Krylov Subspaces

3.1 Block Krylov Subspaces

Let S be a vector subspace of RF⇥F containing the identity matrix IF that is closed under
matrix multiplication and transposition. We deﬁne an inner product h· ·iS in the block vector
space RN⇥F as follows [11]:
Deﬁnition 1 A mapping h· ·iS from RN⇥F ⇥ RN⇥F to S is called a block inner product onto S if
8X  Y  Z 2 RN⇥F and 8C 2 S:
1. S-linearity: hX  YCiS = hX  YiSC and hX + Y  ZiS = hX  ZiS + hY  ZiS
2. symmetry: hX  YiS = hY  XiT
3. deﬁniteness: hX  XiS is positive deﬁnite if X has full rank  and hX  XiS = 0F i↵ X = 0.
There are mainly three ways to deﬁneh· ·iS [11]: 1) (Classical.) SCl = RF⇥F andhX  YiCl
2) (Global.) SGl = cIF  c 2 R and hX  YiGl
set of diagonal matrices and hX  YiLi
we will use the classical one for our contribution.
For further explanations  we give the deﬁnition of block vector subspace of RN⇥F.

S = XTY;
S = trace(XTY)IF; 3) (Loop-interchange.) SLi is the
S = diag(XTY). The three deﬁnitions are all useful yet

S

3

XkCk : Ck 2 S}

k=1 ⇢ RN⇥F  the S-span of {Xk}m

Deﬁnition 2 Given a set of block vectors {Xk}m
mPk=1
spanS{X1  . . .   Xm} := {
Given the above deﬁnition  the order-m block Krylov subspace with respect to the matrix
A 2 RN⇥N  the block vector B 2 RN⇥F and the vector space S can be deﬁned as K S
m(A  B) :=
spanS{B  AB  . . .   Am1B}. The corresponding block Krylov matrix is deﬁned as Km(A  B) :=
[B  AB  . . .   Am1B].

k=1 is deﬁned as

3.2 Spectral Graph Convolution in Block Krylov Subspace Form

In this section  we show that any graph convolution with well-deﬁned analytic spectral ﬁlter
deﬁned on L 2 RN⇥N can be written as the product of a block Krylov matrix with a learnable
parameter matrix in a speciﬁc form. We take S = SCl = RF⇥F.
For any real analytic scalar function g  its power series expansion around center 0 is

g(x) =

anxn =

g(n)(0)

n!

xn  |x| < R

1Xn=0

where R is the radius of convergence.
The function g can be used to deﬁne a ﬁlter. Let ⇢(L) denote the spectrum radius of L and
suppose ⇢(L) < R. The spectral ﬁlter g(L) 2 RN⇥N can be deﬁned as
Ln ⇢ (L) < R

g(L) :=

g(n)(0)

anLn =

1Xn=0

n!

1Xn=0

1Xn=0

According to the deﬁnition of spectral graph convolution in (1)  graph signal X is ﬁltered by
g(L) as follows 

g(L)X =

g(n)(0)

n!

1Xn=0

LnX =hX  LX  L2X ···i" g(0)(0)

0!

IF 

g(1)(0)

1!

IF 

g(2)(0)

2!

IF ···#T

= A0B0

m(L  X). From (5)  the convolution can be written as

spanS{X  LX  L2X ···} = spanS{X  LX  L2X  . . .   Lm1X}

where A0 2 RN⇥1 and B0 2 R1⇥F. We can see that A0 is a block Krylov matrix and Range(A0B0)
✓ Range(A0). It is shown in [13  11] that for S = RF⇥F there exists a smallest m such that
(5)
where m depends on L and X and will be written as m(L  X) later. This means for any k  m 
LkX 2K S
g(L)X =

1Xn=0
⌘ Km(L  X)S (6)
i 2 RF⇥F for i = 1  . . .   m1 are parameter matrix blocks. Then  a graph convolutional
(7)

LnX ⌘hX  LX  . . .   Lm1Xih(0

where S
layer can be be generally written as

g(L)XW0 = Km(L  X)SW0 = Km(L  X)WS

m1)TiT

g(n)(0)

S)T  (1

S)T ···   (S

n!

where WS ⌘ SW0 2 RmF⇥O. The essential number of learnable parameters is mF ⇥ O.
3.3 Deep GCN in the Block Krylov Subspace Form

Since the spectral graph convolution can be simpliﬁed as (6)(7)  we can build deep GCN in
the following way.
Suppose that we have a sequence of analytic spectral ﬁlters G = {g0  g1  . . .   gn} and a sequence
of pointwise nonlinear activation functions H = {h0  h1  . . .   hn}. Then  a deep spectral graph
convolution network can be written as

Y = softmaxngn(L) hn1n··· g2(L) h1ng1(L) h0ng0(L)XW00o W01o W02 ···o W0no

4

(8)

Deﬁne

Then  we have

H0 = X 

From (7) and (8)  we see we can write

i = 0  . . .   n  1
Hi+1 = hi{gi(L)HiWi} 
Y = softmax{Kmn(L  Hn)WSn
n }

Hi+1 = hi{Kmi(L  Hi)WSi

i }  mi ⌘ m(L  Hi)

It is easy to see that  when gi(L) = I  (8) is a fully connected network [21]; when n = 1 
g0(L) = g1(L) = L  where L is deﬁned in (3)  it is just GCN [18]; when gi(L) is deﬁned by the
Chebyshev polynomial [15]  W0i = I  (8) is ChebNet [7].

3.4 Diculties & Inspirations

In the last subsection  we gave a general form of deep GCN in the block Krylov form.
Following this idea  we can leverage the existing block Lanczos algorithm [11  10] to ﬁnd mi
and compute orthogonal basis of K S
mi(L  Hi) which makes the ﬁlter coecients compact [25]
and improve numerical stability. But there are some diculties in practice:

1. During the training phase  Hi changes every time when parameters are updated. This

makes mi a variable and thus requires adaptive size for parameter matrices WSi
i .

2. For classical inner product  the QR factorization that is needed in block Lanczos algorithm

[11] is dicult to be implemented in backpropagation framework.

Despite implementation intractability  block Krylov form is still meaningful for constructing
GCNs that are scalable in depth as we illustrate below.
For each node v 2{ 1  . . .   N} in the graph  denote N(v) as the set of its neighbors and Nk(v) as
the set of its k-hop neighbors. Then  LX(v  :) can be interpreted as a weighted mean of the
feature vectors of v and N(v). If the network goes deep as (4)  Y0(v  :) becomes the “weighted
mean” of the feature vectors of v and N(n+1)(v) (not exactly weighted mean because we have
ReLU in each layer). As the scope grows  the nodes in the same connected component
tend to have the same (global) features  while losing their individual (local) features  which
makes them indistinguishable. Such phenomenon is recognized as “oversmoothing” [21].
Though it is reasonable to assume that the nodes in the same cluster share many similar
properties  it will be harmful to omit the individual di↵erences between each node.
Therefore  the inspiration from the block Krylov form is that  to get a richer representation
of each node  we need to concatenate the multi-scale information (local and global) together
instead of merely doing smoothing in each hidden layer. If we have a smart way to stack
multi-scale information  the network will be scalable in depth. To this end  we naturally
come up with a densely connected architecture [17]  which we call snowball network and a
compact architecture  which we call the truncated Krylov network  in which the multi-scale
information is used di↵erently.

4 Deep GCN Architectures

4.1 Snowball

The block Krylov form inspires ﬁrst an architecture that concatenates multi-scale features
incrementally  resulting in a densely-connected graph network (Figure 2(a)) as follows:

H0 = X  Hl+1 = f (L [H0  H1  . . .   Hl] Wl)  
C = g ([H0  H1  . . .   Hn] Wn)
output = softmax (LpCWC)
i=0 Fi)⇥Fl+1  Wn 2 R(Pn

where Wl 2 R(Pl

i=0 Fi)⇥FC and WC 2 RFC⇥FO are learnable parameter matrices 
Fl+1 is the number of output channels in layer l; f and g are pointwise activation functions;

l = 0  1  . . .   n  1

(9)

5

Hl are extracted features; C is the output of a classiﬁer of any kind  e.g.  a fully connected
neural network or even an identity layer  in which case C = [H0  H1  . . .   Hn]; p 2{ 0  1}. When
p = 0  Lp = I and when p = 1  LP = L  which means that we project C back onto graph
Fourier basis  which is necessary when the graph structure encodes much information.
Following this construction  we can stack all learned features as the input of the subsequent
hidden layer  which is an ecient way to concatenate multi-scale information. The size of
input will grow like a snowball and this construction is similar to DenseNet [17]  which
is designed for regular grids (images). Thus  some advantages of DenseNet are naturally
inherited  e.g.  alleviate the vanishing-gradient problem  encourage feature reuse  increase
the variation of input for each hidden layer  reduce the number of parameters  strengthen
feature propagation and improve model compactness.

4.2 Truncated Krylov

The block Krylov form inspires then an architecture that concatenates multi-scale features
directly together in each layer. However  as stated in Section 3.4  the fact that mi is a variable
makes GCN dicult to be merged into the block Krylov framework. Thus we compromise
and set mi as a hyperparameter and get a truncated block Krylov network (Figure 2(b)) as
shown below:

H0 = X  Hl+1 = f⇣hHl  LHl . . .   Lml1Hli Wl⌘  

C = g (HnWn)
output = softmax (LpCWC)

l = 0  1  . . .   n  1

(10)

where Wl 2 R(mlFl)⇥Fl+1  Wn 2 RFn⇥FC and WC 2 RFC⇥FO are learnable parameter matrices; f
and g are activation functions; C is the output of a classiﬁer of any kind; p 2{ 0  1}. In the
truncated Krylov network  the local information will not be diluted in each layer because in
each layer l  we start the concatenation from L0Hl so that the extracted local information can
be kept.
There are works on the analysis of error bounds of doing truncation in block Krylov methods
[11]. But the results need many assumptions either on X  e.g.  X is a standard Gaussian
matrix [34]  or on L  e.g.  some conditions on the smallest and largest eigenvalues of L have
to be satisﬁed [28]. Instead of doing truncation for a speciﬁc function or a ﬁxed X  we are
dealing with variable X during training. So we cannot get a practical error bound since we
cannot put any restriction on X and its relation to L.

X

H1

H2

H3

H4

X

H1

H2

H3

H4

X

H1

X

H2

H1

X

LX

LH1

LH2

LH3

O

L2X

L3X

L2H1

L2H2

L2H3

O

L3H1

L3H2

L3H3

Lm0-1X

Lm1-1H1

Lm2-1H2

Lm3-1H3

(a) Snowball

(b) Truncated Block Krylov

Figure 2: Proposed Architectures

The Krylov subspace methods are often associated with low-rank approximation methods
for large sparse matrices. Here we would like to mention [25] does low-rank approximation
of L by the Lanczos algorithm. It su↵ers from the tradeo↵ between accuracy and eciency:
the information in L will be lost if L is not low-rank  while keeping more information via
increasing the Lanczos steps will hurt the eciency. Since most of the graphs we are dealing

6

with have sparse connectivity structures  they are actually not low-rank  e.g.  the Erd˝os-Rényi
graph G(n  p) with p = !( 1
n) [32] and examples in Appendix IV. Thus  we do not propose to
do low-rank approximation in our architecture.

4.3 Equivalence of Linear Snowball GCN and Truncated Block Krylov Network

In this part  we will show that the two proposed architectures are inherently connected. In
fact their equivalence can be established when using identify functions as f   identity layer
as C and constraining the parameter matrix of truncated Krylov to be in a special form.
In linear snowball GCN  we can split the parameter matrix Wi into i + 1 blocks and write it

as Wi =h(W(1)

i

)T ···   (W(i+1)

i

)TiT

and then following (9) we have

0 #2666664
2666666666666666664
3777777777777777775

W(1)
C
W(2)
C
...
W(n)
C

1 3777775  . . .
3777777777777777775

H0 = X  H1 = LXW0  H2 = L[X  H1]W1 = LXW(1)

1 +L2XW(1)

0 W(2)

As in (9)  we have CWC = L[H0  H1  . . .   Hn]WC. Thus we can write

1 = L[X  LX]"I

0
0 W(1)

W(1)
1
W(2)

[H0  H1 ···   Hn]WC
I
0
...
0

= [X  LX ···   LnX]2666666666666666664

0
I
...
0

0
···
0
···
...
...
··· W(1)

0

I
0
...
0

0
I
...
0

0
···
0
···
...
...
··· W(1)

1

3777777777777777775

2666666666666666664

3777777777777777775

0
I
0 W(n)
n1
...
...
0
0

···

2666666666666666664

0
···
0
···
...
...
··· W(1)
n1

which is in the form of (7)  where the parameter matrix is the multiplication of a sequence
of block diagonal matrices whose entries consist of identity blocks and blocks from other
parameter matrices. Though the two proposed architectures stack multi-scale information
in di↵erent ways  i.e. incremental and direct respectively  the equivalence reveals that the
truncated block Krylov network can be constrained to leverage multi-scale information in a
way similar to the snowball architecture. While it is worth noting that when there are no
constraints  truncated Krylov is capable of achieving more than what snowball does.

4.4 Relation to Message Passing Framework
If we consider L as a general aggregation
We denote the concatenation operator as k.
operator which aggregates node features with its neighborhood features  we see that the
two proposed architectures both have close relationships with message passing framework
[12]  which are illustrated in the following table  where N0(v) = {v}  Mt is a message function 
Ut is a vertex update function  m(t+1)
are messages and hidden states at each node
respectively  m(t+1) = [m(t+1)
 ···   m(t+1)
]T and  is a nonlinear
N
activation function.
Compared to our proposed architectures  we can see that the message passing paradigm
cannot avoid oversmoothing problem because it does not leverage multi-scale infor-
mation in each layer and will ﬁnally lose local information. An alternate solution
to address the oversmoothing problem could be to modify the readout function to
ˆy = R({h(0)
5 Experiments

v
]T  h(t+1) = [h(t+1)

 ···   h(t+1)

v |v 2V} ).

v   h(1)

v   . . .   h(T)

  h(t+1)

N

v

1

1

On node classiﬁcation tasks  we test 2 instances of the snowball GCN and 1 instance of
the truncated Krylov GCN  which include linear snowball GCN ( f = g = identity  p = 1) 
snowball GCN ( f = Tanh  g = identity  p = 1) and truncated Krylov ( f = g = Tanh  p = 0).
The test cases include on public splits [37  25] of Cora  Citeseer and PubMed2  as well as

2Source code to be found at https://github.com/PwnerHarry/Stronger_GCN

7

Table 1: Algorithms in Matrix and Nodewise Forms

v

Algorithms

Message Passing

GraphSAGE-GCN m(t+1) = Lh(t)

Matrix
m(t+1) = Mt(A  h(t))
h(t+1) = Ut(h(t)  m(t+1))

Forms
Nodewise
m(t+1)
h(t+1)
v
m(t+1)
h(t+1)
v
m(t+1)
h(t+1)
v
Truncated Krylov m(t+1) = h(t)k . . .kLmt1h(t) m(t+1)
h(t+1)
v

h(t+1) = (m(t+1)Wt)
m(t+1) = L[h(0)k . . .kh(t)]
h(t+1)
v

h(t+1) = (m(t+1)Wt)

= Pw2N(v)
Mt(h(t)
v   h(t)
= Ut(h(t)
v   m(t+1)
)
= mean({h(t)
t m(t+1)
= (WT
= kt
= (WT
= kmt1
= (WT

v }[{ h(t)
)
i=0mean({h(i)
)
i=0 mean([i
)

t m(t+1)

t m(t+1)

= (m(t+1)Wt)

Snowball

v

v

v

v

v

v

v }[{ h(i)

N(v)})

k=0{h(t)

Nk(v)})

v

w   evw)

N(v)})

the crafted smaller splits that are more dicult [25  21  31]. We compare the instances
against several methods under 2 experimental settings  with or without validations sets. The
compared methods with validation sets include graph convolutional networks for ﬁngerprint
(GCN-FP) [8]  gated graph neural networks (GGNN) [23]  di↵usion convolutional neural
networks (DCNN) [1]  Chebyshev networks (Cheby) [7]  graph convolutional networks
(GCN) [18]  message passing neural networks (MPNN) [12]  graph sample and aggregate
(GraphSAGE) [14]  graph partition neural networks (GPNN) [24]  graph attention networks
(GAT) [33]  LanczosNet (LNet) [25] and AdaLanczosNet (AdaLNet) [25]. The copmared
methods without validation sets include label propagation using ParWalks (LP) [35]  Co-
training [21]  Self-training [21]  Union [21]  Intersection [21]  GCN without validation [21] 
Multi-stage training [31]  Multi-stage self-supervised (M3S) training [31]  GCN with sparse
virtual adversarial training (GCN-SVAT) [30] and GCN with dense virtual adversarial
training (GCN-DVAT) [30].

(a) Linear Snowball

(c) Truncated Krylov
Figure 3: t-SNE for the extracted features trained on Cora (7 classes) public (5.2%).

(b) Snowball

In Table 2 and 3  for each test case  we report the accuracy averaged from 10 independent
runs using the best searched hyperparameters. These hyperparameters are reported in the
appendix  which include learning rate and weight decay for the optimizers RMSprop or
Adam for cases with validation or without validation  respectively  taking values in the
intervals [106  5 ⇥ 103] and [105  102]  respectively  width of hidden layers taking value
in the set {100  200 ···   5000}  number of hidden layers in the set {1  2  . . .   50}  dropout in
(0  0.99]  and the number of Krylov blocks taking value in {1  2  . . .   100}. An early stopping
trick is also used to achieve better training. Speciﬁcally we terminate the training after 100
update steps of not improving the training loss.
We see that the instances of the proposed architectures achieve overwhelming performance
in all test cases. We visualize a representative case using t-SNE [26] in Figure 3. From
these visualization  we can see the instances can extract good features with small training

8

data  especially for the truncated block Krylov network. Particularly  when the training
splits are small  they perform astonishingly better than the existing methods. This may
be explained by the fact that when there is less labeled data  larger scope of vision ﬁeld is
needed to make recognition of each node or to let the label signals propagate. We would
also highlight that the linear snowball GCN can achieve state-of-the-art performance with
much less computational cost. If G has no bipartite components  then in (4)  as n ! 1 
rank(Y0)  k almost surely.

Table 2: Accuracy without Validation

Cora

CiteSeer

PubMed

Algorithms

LP

Union

Cheby

Intersection
MultiStage

Co-training
Self-training

0.5% 1% 2% 3% 4% 5% 0.5% 1% 2% 3% 4% 5% 0.03% 0.05% 0.1% 0.3%
66.4 65.4 66.8
56.4 62.3 65.4 67.5 69.0 70.2 34.8 40.2 43.6 45.3 46.4 47.3 61.4
47.3 51.2 72.8
38.0 52.0 62.4 70.8 74.1 77.6 31.7 42.8 59.9 66.2 68.3 69.3 40.4
68.3 72.7 78.2
56.6 66.4 73.5 75.9 78.9 80.8 47.3 55.7 62.1 62.5 64.5 65.5 62.2
58.7 66.8 77.0
53.7 66.1 73.8 77.2 79.4 80.0 43.3 58.1 68.2 69.8 70.4 71.0 51.9
58.5 69.9 75.9 78.5 80.4 81.7 46.3 59.1 66.7 66.7 67.6 68.2 58.4
64.0 70.7 79.2
59.3 69.7 77.6
49.7 65.0 72.9 77.1 79.4 80.2 42.9 59.1 68.6 70.1 70.8 71.2 52.0
64.3 70.2
57.4
61.1 63.7 74.4 76.1 77.2
64.4 70.6
61.5 67.2 75.6 77.8 78.0
59.2
49.7 56.3 76.6
42.6 56.9 67.8 74.9 77.6 79.3 33.4 46.5 62.6 66.9 68.7 69.6 46.4
56.9 63.5 77.2
43.6 53.9 71.4 75.6 78.3 78.5 47.0 52.4 65.8 68.6 69.5 70.7 52.1
49 61.8 71.9 75.9 78.4 78.6 51.5 58.5 67.4 69.2 70.8 71.3 53.3
58.6 66.3 77.3
68.5 73.6 79.7
linear Snowball 67.6 74.6 78.9 80.9 82.3 82.9 56.0 63.4 69.3 70.6 72.5 72.6 65.5
68.6 73.2 80.1
68.4 73.2 78.4 80.8 82.3 83.0 56.4 63.9 68.7 70.5 71.8 72.8 66.5
truncated Krylov 71.8 76.5 80.0 82.0 83.0 84.1 59.9 66.1 69.8 71.3 72.3 73.7 68.7
71.4 75.5 80.4
For each (column)  the greener the cell  the better the performance. The redder  the worse. If our
methods achieve better performance than all others  the corresponding cell will be in bold.

53.0 57.8 63.8 68.0 69.0
56.1 62.1 66.4 70.3 70.5

GCN-SVAT
GCN-DVAT

M3S
GCN

Snowball

Table 3: Accuracy with Validation

Cora
0.5% 1% 3%

Algorithms

Cheby
GCN-FP
GGNN
DCNN
MPNN

5.2%
public
33.9 44.2 62.1 78.0
50.5 59.6 71.7 74.6
48.2 60.5 73.1 77.6
59.0 66.4 76.7 79.7
46.5 56.7 72.0 78.0
37.5 49.0 64.2 74.5
41.4 48.6 56.8 83.0
50.9 62.3 76.5 80.5
58.1 66.1 77.3 79.5
60.8 67.5 77.7 80.4
linear Snowball 72.5 76.3 82.2 83.3
71.2 76.6 81.9 83.2
truncated Krylov 74.8 78.0 82.7 83.2

GAT
GCN
LNet

GraphSAGE

Snowball

AdaLNet

CiteSeer

0.5% 1%

3.6%
public
45.3 59.4 70.1
43.9 54.3 61.5
44.3 56.0 64.6
53.1 62.2 69.4
41.8 54.3 64.0
33.8 51.0 67.2
38.2 46.5 72.5
43.6 55.3 68.7
53.2 61.3 66.2
53.8 63.3 68.7
62.0 66.7 72.9
61.0 66.4 73.3
64.0 68.3 73.9

PubMed

0.03% 0.05% 0.1%

0.3%
public
55.2 69.8
70.3 76.0
70.4 75.8
73.1 76.8
67.3 75.6
65.4 76.8
59.6 79.0
73.0 77.8
73.4 78.3
72.8 78.1
75.6 79.1
75.2 79.2
78.0 80.1

45.3
56.2
55.8
60.9
53.9
45.4
50.9
57.9
60.4
61.0
70.8
69.9
72.2

48.2
63.2
63.3
66.7
59.6
53.0
50.4
64.6
68.8
66.0
72.1
72.7
74.9

6 Future Works

Future research of this like includes: 1) Investigating how the pointwise nonlinear activation
functions inﬂuence block vectors  e.g.  the feature block vector X and hidden feature block
vectors Hi  so that we can ﬁnd possible activation functions better than Tanh; 2) Finding a
better way to leverage the block Krylov algorithms instead of conducting simple truncation.

Acknowledgements

The authors wish to express sincere gratitude for the computational resources of Compute
Canada provided by Mila  as well as for the proofreading done by Sitao and Mingde’s good
friend & coworker Ian P. Porada.

9

References
[1] J. Atwood and D. Towsley. Di↵usion-convolutional neural networks.

abs/1511.02136  2015.

arXiv 

[2] M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep

learning: going beyond euclidean data. arXiv  abs/1611.08097  2016.

[3] J. Chen  T. Ma  and C. Xiao. Fastgcn: fast learning with graph convolutional networks

via importance sampling. arXiv preprint arXiv:1801.10247  2018.

[4] J. Chen  J. Zhu  and L. Song. Stochastic training of graph convolutional networks with

variance reduction. arXiv preprint arXiv:1710.10568  2017.

[5] R. R. Coifman and S. Lafon. Di↵usion maps. Applied and computational harmonic analysis 

21(1):5–30  2006.

[6] R. R. Coifman and M. Maggioni. Di↵usion wavelets. Applied and Computational Harmonic

Analysis  21(1):53–94  2006.

[7] M. De↵errard  X. Bresson  and P. Vandergheynst. Convolutional neural networks on

graphs with fast localized spectral ﬁltering. arXiv  abs/1606.09375  2016.

[8] D. K. Duvenaud  D. Maclaurin  J. Iparraguirre  R. Bombarell  T. Hirzel  A. Aspuru-
Guzik  and R. P. Adams. Convolutional networks on graphs for learning molecular
ﬁngerprints. In Advances in neural information processing systems  pages 2224–2232  2015.
[9] X. Feng and Z. Zhang. The rank of a random matrix. Applied mathematics and computation 

185(1):689–694  2007.

[10] A. Frommer  K. Lund  M. Schweitzer  and D. B. Szyld. The radau–lanczos method for
matrix functions. SIAM Journal on Matrix Analysis and Applications  38(3):710–732  2017.
[11] A. Frommer  K. Lund  and D. B. Szyld. Block Krylov subspace methods for functions

of matrices. Electronic Transactions on Numerical Analysis  47:100–126  2017.

[12] J. Gilmer  S. S. Schoenholz  P. F. Riley  O. Vinyals  and G. E. Dahl. Neural message
passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70  pages 1263–1272. JMLR. org  2017.

[13] M. H. Gutknecht and T. Schmelzer. The block grade of a block krylov space. Linear

Algebra and its Applications  430(1):174–185  2009.

[14] W. L. Hamilton  R. Ying  and J. Leskovec. Inductive representation learning on large

graphs. arXiv  abs/1706.02216  2017.

[15] D. K. Hammond  P. Vandergheynst  and R. Gribonval. Wavelets on graphs via spectral

graph theory. Applied and Computational Harmonic Analysis  30(2):129–150  2011.

[16] G. E. Hinton  S. Osindero  and Y.-W. Teh. A fast learning algorithm for deep belief nets.

Neural computation  18(7):1527–1554  2006.

[17] G. Huang  Z. Liu  L. Van Der Maaten  and K. Q. Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition  pages 4700–4708  2017.

[18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional

networks. arXiv  abs/1609.02907  2016.

[19] Y. LeCun  Y. Bengio  and G. Hinton. Deep learning. nature  521(7553):436  2015.
[20] Y. LeCun  L. Bottou  Y. Bengio  P. Ha↵ner  et al. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[21] Q. Li  Z. Han  and X. Wu. Deeper insights into graph convolutional networks for

semi-supervised learning. arXiv  abs/1801.07606  2018.

[22] R. Li  S. Wang  F. Zhu  and J. Huang. Adaptive graph convolutional neural networks.

In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

[23] Y. Li  D. Tarlow  M. Brockschmidt  and R. Zemel. Gated graph sequence neural networks.

arXiv preprint arXiv:1511.05493  2015.

10

[24] R. Liao  M. Brockschmidt  D. Tarlow  A. L. Gaunt  R. Urtasun  and R. Zemel. Graph parti-
tion neural networks for semi-supervised classiﬁcation. arXiv preprint arXiv:1803.06272 
2018.

[25] R. Liao  Z. Zhao  R. Urtasun  and R. S. Zemel. Lanczosnet: Multi-scale deep graph

convolutional networks. arXiv  abs/1901.01484  2019.

[26] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[27] F. Monti  D. Boscaini  J. Masci  E. Rodola  J. Svoboda  and M. M. Bronstein. Geometric
deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition  pages 5115–5124  2017.

[28] C. Musco  C. Musco  and A. Sidford. Stability of the lanczos method for matrix function
approximation. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms  pages 1605–1624. Society for Industrial and Applied Mathematics 
2018.

[29] D. I. Shuman  S. K. Narang  P. Frossard  A. Ortega  and P. Vandergheynst. The emerging
ﬁeld of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. arXiv preprint arXiv:1211.0053  2012.

[30] K. Sun  H. Guo  Z. Zhu  and Z. Lin. Virtual adversarial training on graph convolutional

networks in node classiﬁcation. arXiv preprint arXiv:1902.11045  2019.

[31] K. Sun  Z. Zhu  and Z. Lin. Multi-stage self-supervised learning for graph convolutional

networks. arXiv  abs/1902.11038  2019.

[32] L. V. Tran  V. H. Vu  and K. Wang. Sparse random graphs: Eigenvalues and eigenvectors.

Random Structures & Algorithms  42(1):110–134  2013.

[33] P. Veliˇckovi´c  G. Cucurull  A. Casanova  A. Romero  P. Lio  and Y. Bengio. Graph

attention networks. arXiv  abs/1710.10903  2017.

[34] S. Wang  Z. Zhang  and T. Zhang. Improved analyses of the randomized power method

and block lanczos method. arXiv preprint arXiv:1508.06429  2015.

[35] X.-M. Wu  Z. Li  A. M. So  J. Wright  and S.-F. Chang. Learning with partially absorbing
random walks. In Advances in Neural Information Processing Systems  pages 3077–3085 
2012.

[36] Z. Wu  S. Pan  F. Chen  G. Long  C. Zhang  and P. S. Yu. A comprehensive survey on

graph neural networks. arXiv  abs/1901.00596  2019.

[37] Z. Yang  W. W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with

graph embeddings. arXiv preprint arXiv:1603.08861  2016.

[38] S. Zhang  H. Tong  J. Xu  and R. Maciejewski. Graph convolutional networks: Algo-
rithms  applications and open challenges. In International Conference on Computational
Social Networks  pages 79–91. Springer  2018.

11

,Sitao Luan
Mingde Zhao
Xiao-Wen Chang
Doina Precup