2019,Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel,Recent works have shown that on sufficiently over-parametrized neural nets  gradient descent with relatively large initialization optimizes a prediction function in the RKHS of the Neural Tangent Kernel (NTK). This analysis leads to global convergence results but does not work when there is a standard $\ell_2$ regularizer  which is useful to have in practice. We show that sample efficiency can indeed depend on the presence of the regularizer: we construct a simple distribution in $d$ dimensions which the optimal regularized neural net learns with $O(d)$ samples but the NTK requires $\Omega(d^2)$ samples to learn. To prove this  we establish two analysis tools: i) for multi-layer feedforward ReLU nets  we show that the global minimizer of a weakly-regularized cross-entropy loss is the max normalized margin solution among all neural nets  which generalizes well; ii) we develop a new technique for proving lower bounds for kernel methods  which relies on showing that the kernel cannot focus on informative features. Motivated by our generalization results  we study whether the regularized global optimum is attainable. We prove that for infinite-width two-layer nets  noisy gradient descent optimizes the regularized neural net loss to a global minimum in polynomial iterations.,RegularizationMatters:GeneralizationandOptimizationofNeuralNetsv.s.theirInducedKernelColinWeiDepartmentofComputerScienceStanfordUniversitycolinwei@stanford.eduJasonD.LeeDepartmentofElectricalEngineeringPrincetonUniversityjasonlee@princeton.eduQiangLiuDepartmentofComputerScienceUniversityofTexasatAustinlqiang@cs.texas.eduTengyuMaDepartmentofComputerScienceStanfordUniversitytengyuma@stanford.eduAbstractRecentworkshaveshownthatonsufﬁcientlyover-parametrizedneuralnets gra-dientdescentwithrelativelylargeinitializationoptimizesapredictionfunctionintheRKHSoftheNeuralTangentKernel(NTK).Thisanalysisleadstoglobalconvergenceresultsbutdoesnotworkwhenthereisastandard‘2regularizer whichisusefultohaveinpractice.Weshowthatsampleefﬁciencycanindeeddependonthepresenceoftheregularizer:weconstructasimpledistributioninddimensionswhichtheoptimalregularizedneuralnetlearnswithO(d)samplesbuttheNTKrequiresΩ(d2)samplestolearn.Toprovethis weestablishtwoanalysistools:i)formulti-layerfeedforwardReLUnets weshowthattheglobalminimizerofaweakly-regularizedcross-entropylossisthemaxnormalizedmarginsolutionamongallneuralnets whichgeneralizeswell;ii)wedevelopanewtechniqueforprovinglowerboundsforkernelmethods whichreliesonshowingthatthekernelcannotfocusoninformativefeatures.Motivatedbyourgeneralizationresults westudywhethertheregularizedglobaloptimumisattainable.Weprovethatforinﬁnite-widthtwo-layernets noisygradientdescentoptimizestheregularizedneuralnetlosstoaglobalminimuminpolynomialiterations.1IntroductionIndeeplearning over-parametrizationreferstothewidely-adoptedtechniqueofusingmorepa-rametersthannecessary[35 40].Over-parametrizationiscrucialforsuccessfuloptimization andalargebodyofworkhasbeendevotedtowardsunderstandingwhy.Onelineofrecentworks[17 37 22 21 2 76 31 6 16 72]offersanexplanationthatinvitesanalogywithker-nelmethods provingthatwithsufﬁcientover-parameterizationandacertaininitializationscaleandlearningrateschedule gradientdescentessentiallylearnsalinearclassiﬁerontopoftheinitialrandomfeatures.Forthissamesetting Daniely[17] Duetal.[22 21] Jacotetal.[31] Aroraetal.[6 5]makethisconnectionexplicitbyestablishingthatthepredictionfunctionfoundbygradientdescentisinthespanofthetrainingdatainareproducingkernelHilbertspace(RKHS)inducedbytheNeuralTangentKernel(NTK).ThegeneralizationerroroftheresultingnetworkcanbeanalyzedviatheRademachercomplexityofthekernelmethod.Theseworksprovidesomeoftheﬁrstalgorithmicresultsforthesuccessofgradientdescentinoptimizingneuralnets;however theresultinggeneralizationerrorisonlyasgoodasthatofﬁxed33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.Figure1:DatapointsfromDhaveﬁrsttwocoordinatesdisplayedabove withredandbluedenotinglabelsof-1 +1 respectively.Theremainingcoordinatesareuniformin{−1 +1}d−2.kernels[6].Ontheotherhand theequivalenceofgradientdescentandNTKisbrokenifthelosshasanexplicitregularizersuchasweightdecay.Inthispaper westudytheeffectofanexplicitregularizeronneuralnetgeneralizationviathelensofmargintheory.Weﬁrstconstructasimpledistributiononwhichthetwo-layernetworkoptimizingexplicitlyregularizedlogisticlosswillachievealargemargin andtherefore goodgeneralization.Ontheotherhand anypredictionfunctioninthespanofthetrainingdataintheRKHSinducedbytheNTKwilloverﬁttonoiseandthereforeachievepoormarginandbadgeneralization.Theorem1.1(InformalversionofTheorem2.1).ConsiderthesettingoflearningthedistributionDdeﬁnedinFigure1usingatwo-layernetworkwithreluactivationswiththegoalofachievingsmallgeneralizationerror.Usingo(d2)samples nofunctioninthespanofthetrainingdataintheRKHSinducedbytheNTKcansucceed.Ontheotherhand theglobaloptimizerofthe‘2-regularizedlogisticlosscanlearnDwithO(d)samples.ThefullresultisstatedinSection2.TheintuitionisthatregularizationallowstheneuralnettoobtainabettermarginthantheﬁxedNTKkernelandthusachievebettergeneralization.OursamplecomplexitylowerboundforNTKappliestoabroadclassoflossesincludingstandard0-1classiﬁcationlossandsquared‘2.Tothebestofourknowledge theprooftechniquesforobtainingthisboundarenovelandofindependentinterest(seeourproofoverviewinSection2).InSection5 weconﬁrmempiricallythatanexplicitregularizercanindeedimprovethemarginandgeneralization.YehudaiandShamir[73]alsoprovealowerboundonthelearnabilityofneuralnetkernels.TheyshowthatanapproximationresultthatΩ(exp(d))randomrelufeaturesarerequiredtoﬁtasingleneuronin‘2squaredloss whichlowerboundstheamountofover-parametrizationnecessarytoapproximateasingleneuron.Incontrast weprovesample-complexitylowerboundswhichholdforbothclassiﬁcationand‘2lossevenwithinﬁniteover-parametrization.Motivatedbytheprovablybettergeneralizationofregularizedneuralnetsforourconstructedinstance inSection3westudytheiroptimization asthepreviouslycitedresultsonlyapplywhentheneuralnetbehaveslikeakernel.Weshowoptimizationispossibleforinﬁnite-widthregularizednets.Theorem1.2(Informal seeTheorem3.3).Forinﬁnite-widthtwolayernetworkswith‘2-regularizedloss noisygradientdescentﬁndsaglobaloptimizerinapolynomialnumberofiterations.Thisimprovesuponpriorworks[43 15 65 61]whichstudyoptimizationinthesameinﬁnite-widthlimitbutdonotprovidepolynomialconvergencerates.(SeemorediscussionsinSection3.)ToestablishTheorem1.1 werelyontoolsfrommargintheory.InSection4 weproveanumberofresultsofindependentinterestregardingthemarginofaregularizedneuralnet.Weshowthattheglobalminimumofweakly-regularizedlogisticlossofanyhomogeneousnetwork(regardlessofdepthorwidth)achievesthemaxnormalizedmarginamongallnetworkswiththesamearchitecture(Theorem4.1).By“weak”regularizer wemeanthatthecoefﬁcientoftheregularizerinthelossisverysmall(approaching0).Bycombiningwitharesultof[25] weconcludethattheminimizerenjoysawidth-freegeneralizationbounddependingononlytheinversenormalizedmargin(normalizedbythenormoftheweights)anddepth(Corollary4.2).Thisexplainswhyoptimizingthe‘2-regularizedlosstypicallyusedinpracticecanleadtoparameterswithalargemarginandgoodgeneralization.Wefurthernotethatthemaximumpossiblemarginisnon-decreasinginthewidthofthearchitecture sothegeneralizationboundofCorollary4.2improvesasthesizeofthenetworkgrows(seeTheorem4.3).Thus evenifthedatasetisalreadyseparable itcouldstillbeusefultofurtherover-parameterizetoachievebettergeneralization.Finally weempiricallyvalidateseveralclaimsmadeinthispaperinSection5.First weconﬁrmonsyntheticdatathatneuralnetworksdogeneralizebetterwithanexplicitregularizervs.without.Second weshowthatfortwo-layernetworks thetesterrordecreasesandmarginincreasesasthehiddenlayergrows aspredictedbyourtheory.21.1AdditionalRelatedWorkZhangetal.[74]andNeyshaburetal.[52]showthatneuralnetworkgeneralizationdeﬁesconventionalexplanationsandrequiresnewones.Neyshaburetal.[48]initiatethesearchforthe“inductivebias”ofneuralnetworkstowardssolutionswithgoodgeneralization.Recentpapers[30 12 14]studyinductivebiasthroughtrainingtimeandsharpnessoflocalminima.Neyshaburetal.[49]proposeasteepestdescentalgorithminageometryinvarianttoweightrescalingandshowthisimprovesgeneralization.Morcosetal.[45]relategeneralizationtothenumberof“directions”intheneurons.Otherpapers[26 68 46 28 38 27 38 32]studyimplicitregularizationtowardsaspeciﬁcsolution.Maetal.[41]showthatimplicitregularizationhelpsgradientdescentavoidovershootingoptima.Rossetetal.[58 59]studylinearlogisticregressionwithweakregularizationandshowconvergencetothemaxmargin.InSection4 weadopttheirtechniquesandextendtheirresults.AlineofworkinitiatedbyNeyshaburetal.[50]hasfocusedonderivingtighternorm-basedRademachercomplexityboundsfordeepneuralnetworks[9 51 25]andnewcompressionbasedgeneralizationproperties[4].Bartlettetal.[9]highlighttheimportantroleofnormalizedmargininneuralnetgeneralization.WeiandMa[70]provegeneralizationboundsdependingonadditionaldata-dependentproperties.DziugaiteandRoy[23]computenon-vacuousgeneralizationboundsfromPAC-Bayesbounds.Neyshaburetal.[53]investigatetheRademachercomplexityoftwo-layernetworksandproposeaboundthatisdecreasingwiththedistancetoinitialization.LiangandRakhlin[39]andBelkinetal.[10]studythegeneralizationofkernelmethods.Foroptimization SoudryandCarmon[67]explainwhyover-parametrizationcanremovebadlocalminima.SafranandShamir[63]showover-parametrizationcanimprovethequalityofarandominitialization.HaeffeleandVidal[29] NguyenandHein[55] andVenturietal.[69]showthatforsufﬁcientlyoverparametrizednetworks alllocalminimaareglobal butdonotshowhowtoﬁndtheseminimaviagradientdescent.DuandLee[19]showfortwo-layernetworkswithquadraticactivations allsecond-orderstationarypointsareglobalminimizers.Aroraetal.[3]interpretover-parametrizationasameansofacceleration.Meietal.[43] ChizatandBach[15] SirignanoandSpiliopoulos[65] DouandLiang[18] Meietal.[44]analyzeadistributionalviewofover-parametrizednetworks.ChizatandBach[15]showthatWassersteingradientﬂowconvergestoglobaloptimizersunderstructuralassumptions.Weextendthistoapolynomial-timeresult.Finally manypapershaveshownconvergenceofgradientdescentonneuralnets[2 1 37 22 21 6 76 13 31 16]usinganalyseswhichprovetheweightsdonotmovefarfrominitialization.Theseanalysesdonotapplytotheregularizedloss andourexperimentsinSectionFsuggestthatmovingawayfromtheinitializationisimportantforbettertestperformance.AnotherlineofworktakesaBayesianperspectiveonneuralnets.Underanappropriatechoiceofprior theyshowanequivalencebetweentherandomneuralnetandGaussianprocessesinthelimitofinﬁnitewidthorchannels[47 71 36 42 24 56].Thisprovidesanotherkernelperspectiveofneuralnets.YehudaiandShamir[73] ChizatandBach[16]alsoarguethatthekernelperspectiveofneuralnetsisnotsufﬁcientforunderstandingthesuccessofdeeplearning.ChizatandBach[16]arguethatthekernelperspectiveofgradientdescentiscausedbyalargeinitializationanddoesnotnecessarilyexplaintheempiricalsuccessesofover-parametrization.YehudaiandShamir[73]provethatΩ(exp(d))randomrelufeaturescannotapproximateasingleneuroninsquarederrorloss.Incomparison ourlowerboundsareforthesamplecomplexityratherthanwidthoftheNTKpredictionfunctionandapplyevenwithinﬁniteover-parametrizationforbothclassiﬁcationandsquaredloss.1.2NotationLetRdenotethesetofrealnumbers.Wewillusek·ktoindicateageneralnorm withk·k2denotingthe‘2normandk·kFtheFrobeniusnorm.Weuse¯ontopofasymboltodenoteaunitvector:whenapplicable ¯u u/kuk withthenormk·kclearfromcontext.LetN(0 σ2)denotethenormaldistributionwithmean0andvarianceσ2.Forvectorsu1∈Rd1 u2∈Rd2 weusethenotation(u1 u2)∈Rd1+d2todenotetheirconcatenation.Wealsosayafunctionfisa-homogeneousininputxiff(cx)=caf(x)foranyc andwesayfisa-positive-homogeneousifthereistheadditionalconstraintc>0.WereservethesymbolX=[x1 ... xn]todenotethecollectionofdatapoints(asamatrix) andY=[y1 ... yn]todenotelabels.Weusedtodenotethedimensionofourdata.3Wewillusethenotationsa.b a&btodenotelessthanorgreaterthanuptoauniversalconstant respectively andwhenusedinacondition todenotetheexistenceofsuchaconstantsuchthattheconditionistrue.Unlessstatedotherwise O(·) Ω(·)denotesomeuniversalconstantinupperandlowerbounds.Thenotationpolydenotesauniversalconstant-degreepolynomialinthearguments.2GeneralizationofRegularizedNeuralNetvs.NTKKernelWewillcompareneuralnetsolutionsfoundviaregularizationandmethodsinvolvingtheNTKandconstructadatadistributionDinddimensionswhichtheneuralnetoptimizerofregularizedlogisticlosslearnswithsamplecomplexityO(d).ThekernelmethodwillrequireΩ(d2)samplestolearn.WestartbydescribingthedistributionDofexamples(x y).Hereeiisthei-thstandardbasisvectorandweusex>eitorepresentthei-coordinateofx(sincethesubscriptisreservedtoindextrainingexamples).First foranyk≥3 x>ek∼{−1 +1}isauniformrandombit andforx>e1 x>e2andy choosey=+1 x>e1=+1 x>e2=0w/prob.1/4y=+1 x>e1=−1 x>e2=0w/prob.1/4y=−1 x>e1=0 x>e2=+1w/prob.1/4y=−1 x>e1=0 x>e2=−1w/prob.1/4(2.1)ThedistributionDcontainsallofitssignalintheﬁrst2coordinates andtheremainingd−2coordinatesarenoise.Wevisualizeitsﬁrst2coordinatesinFigure1.Next weformallydeﬁnethetwolayerneuralnetwithreluactivationsanditsassociatedNTK.Weparameterizeatwo-layernetworkwithmunitsbylastlayerweightsw1 ... wm∈Randweightvectorsu1 ... um∈Rd.WedenotebyΘthecollectionofparametersandbyθjtheunit-jparameters(uj wj).ThenetworkcomputesfNN(x;Θ) Pmj=1wj[u>jx]+ where[·]+denotesthereluactivation.Forbinarylabelsy1 ... yn∈{−1 +1} the‘2regularizedlogisticlossisLλ(Θ) 1nnXi=1log(1+exp(−yifNN(xi;Θ)))+λkΘk2F(2.2)LetΘλ∈argminΘLλ(Θ)beitsglobaloptimizer.DeﬁnetheNTKkernelassociatedwiththearchitecture(withrandomweights):K(x0 x)=Ew∼N(0 r2w) u∼N(0 r2uI)(cid:2)(cid:10)∇θfNN(x;Θ) ∇θfNN(x0;Θ)(cid:11)(cid:3)where∇θfNN(x;Θ)=(w1(x>u≥0)x [x>u]+)isthegradientofthenetworkoutputwithrespecttoagenerichiddenunit andrw ruarerelativescalingparameters.NotethatthetypicalNTKisrealizedspeciﬁcallywithscalesrw=ru=1 butourboundappliesforallchoicesofrw ru.Forcoefﬁcientsβ wecanthendeﬁnethepredictionfunctionfkernel(x;β)intheRKHSinducedbyKasfkernel(x;β) Pni=1βiK(xi x).Forexample suchaclassiﬁerwouldbeattainedbyrunninggradientdescentonsquaredlossforawidenetworkusingtheappropriaterandominitialization(see[31 22 21 6]).WenowpresentourcomparisontheorembelowandﬁllinitsproofinSectionB.Theorem2.1.LetDbethedistributiondeﬁnedinequation2.1.Withprobability1−d−5overtherandomdrawofn.d2samples(x1 y1) ... (xn yn)fromD forallchoicesofβ thekernelpredictionfunctionfkernel(·;β)willhaveatleastΩ(1)error:Pr(x y)∼D[fkernel(x;β)y≤0]=Ω(1)Meanwhile forλ≤poly(n)−1 theregularizedneuralnetsolutionfNN(·;Θλ)withatleast4hiddenunitscanhavegoodgeneralizationwithO(d2)samplesbecausewehavethefollowinggeneralizationerrorbound:Pr(x y)∼D[fNN(x;Θλ)y≤0].rdnThisimpliesaΩ(d)sample-complexitygapbetweentheregularizedneuralnetandkernelpredictionfunction.4Whiletheabovetheoremisstatedforclassiﬁcation thesameDcanbeusedtostraightforwardlyproveaΩ(d)samplecomplexitygapforthetruncatedsquaredloss‘(ˆy;y)=min((y−ˆy)2 1).1WeprovidemoredetailsinSectionB.3.Ourintuitionofthisgapisthattheregularizationallowstheneuralnettoﬁndinformativefeatures(weightvectors) thatareadaptivetothedatadistributionandeasierforthelastlayers’weightstoseparate.Forexample theneurons[e1x]+ [−e1x]+ [e2x]+ [−e2x]+areenoughtoﬁtourparticulardistribution.Incomparison theNTKmethodisunabletochangethefeaturespaceandisonlysearchingforthecoefﬁcientsinthekernelspace.Prooftechniquesfortheupperbound:Fortheupperbound neuralnetswithsmallEuclideannormwillbeabletoseparateDwithlargemargin(atwo-layernetwithwidth4canalreadyachievealargemargin).AsweshowinSection4 asolutionwithamaxneural-netmarginisattainedbytheglobaloptimizeroftheregularizedlogisticloss—infact weshowthisholdsforgenerallyhomogeneousnetworksofanydepthandwidth(Theorem4.1).Then bytheclassicalconnectionbetweenmarginandgeneralization[34] thisoptimizerwillgeneralizewell.Prooftechniquesforthelowerbound:Ontheotherhand theNTKwillhaveaworsemarginwhenﬁttingsamplesfromDthantheregularizedneuralnetworksbecauseNTKoperatesinaﬁxedkernelspace.2However provingthattheNTKhasasmallmargindoesnotsufﬁcebecausethegeneralizationerrorboundswhichdependonmarginmaynotbetight.Wedevelopanewtechniquetoprovelowerboundsforkernelmethods whichwebelieveisofindependentinterest astherearefewpriorworksthatprovelowerboundsforkernelmethods.(Onethatdoesis[54] buttheirresultsrequireconstructinganartiﬁcialkernelanddatadistribution whereasourlowerboundsareforaﬁxedkernel.)ThemainintuitionisthatbecauseNTKusesinﬁnitelymanyrandomfeatures itisdifﬁcultfortheNTKtofocusonasmallnumberofinformativefeatures–doingsowouldrequireaveryhighRKHSnorm.Infact weshowthatwithalimitednumberofexamples anyfunctionthatinthespanofthetrainingexamplesmustheavilyuserandomfeaturesratherthaninformativefeatures.Therandomfeaturescancollectivelyﬁtthetrainingdata butwillgiveworsegeneralization.3PerturbedWassersteinGradientFlowFindsGlobalOptimizersinPolynomialTimeInthepriorsection wearguedthataneuralnetwith‘2regularizationcanachievemuchbettergeneralizationthantheNTK.Ourresultrequiredattainingtheglobalminimumoftheregularizedloss;however existingoptimizationtheoryonlyallowsforsuchconvergencetoaglobalminimizerwithalargeinitializationandnoregularizer.Unfortunately thesearetheregimeswheretheneuralnetlearnsakernelpredictionfunction[31 22 6].Inthissection weshowthatatleastforinﬁnite-widthtwo-layernets optimizationisnotanissue:noisygradientdescentﬁndsglobaloptimizersofthe‘2regularizedlossinpolynomialiterations.Priorwork[43 15]hasshownthatasthehiddenlayersizegrowstoinﬁnity gradientdescentforaﬁniteneuralnetworkapproachestheWassersteingradientﬂowoverdistributionsofhiddenunits(deﬁnedinequation3.1).Withtheassumptionthatthegradientﬂowconverges whichisnon-trivialsincethespaceofdistributionsisinﬁnite-dimensional ChizatandBach[15]provethatWassersteingradientﬂowconvergestoaglobaloptimizerbutdonotspecifyarate.Meietal.[43]addanentropyregularizertoformanobjectivethatistheinﬁnite-neuronlimitofstochasticLangevindynamics.Theyshowglobalconvergencebutalsodonotprovideexplicitrates.Intheworstcase theirconvergencecanbeexponentialindimension.Incontrast weprovideexplicitpolynomialconvergenceratesforaslightlydifferentalgorithm perturbedWassersteingradientﬂow.Inﬁnite-widthneuralnetsaremodeledmathematicallyasadistributionoverweights:formally weoptimizethefollowingfunctionaloverdistributionsρonRd+1:L[ρ] R(RΦdρ)+RVdρ whereΦ:Rd+1→Rk R:Rk→R andV:Rd+1→R.RandVcanbethoughtofasthelossandregularizer respectively.Inthiswork weconsider2-homogeneousΦandV.Wewilladditionally1Thetruncationisrequiredtoprovegeneralizationoftheregularizedneuralnetusingstandardtools.2TherecouldbesomevariationsoftheNTKspacedependingonthescalesoftheinitializationofthetwolayers butourTheorem2.1showsthatthesevariationsalsosufferfromaworsesamplecomplexity.5requirethatRisconvexandnonnegativeandVispositiveontheunitsphere.Finally weneedstandardregularityassumptionsonR Φ andV:Assumption3.1(RegularityconditionsonΦ R V).ΦandVaredifferentiableaswellasupperboundedandLipschitzontheunitsphere.RisLipschitzanditsHessianhasboundedoperatornorm.Weprovidemoredetailsonthespeciﬁcparameters(forboundedness Lipschitzness etc.)inSec-tionE.1.WenotethatrelunetworkssatisfyeveryconditionbutdifferentiabilityofΦ.3Wecanﬁta‘2regularizedneuralnetworkunderourframework:Example3.2(Logisticlossforneuralnetworks).Weinterpretρasadistributionovertheparametersofthenetwork.Letk nandΦi(θ) wφ(u>xi)forθ=(w u).Inthiscase RΦdρisadistributionalneuralnetworkthatcomputesanoutputforeachofthentrainingexamples(likeastandardneuralnetwork italsocomputesaweightedsumoverhiddenunits).Wecancomputethedistributionalversionoftheregularizedlogisticlossinequation2.2bysettingV(θ) λkθk22andR(a1 ... an) Pni=1log(1+exp(−yiai)).WewilldeﬁneL0[ρ]:Rd+1→RwithL0[ρ](θ) hR0(RΦdρ) Φ(θ)i+V(θ)andv[ρ](θ) −∇θL0[ρ](θ).Informally L0[ρ]isthegradientofLwithrespecttoρ andvistheinducedvelocityﬁeld.ForthestandardWassersteingradientﬂowdynamics ρtevolvesaccordingtoddtρt=−∇·(v[ρt]ρt)(3.1)where∇·denotesthedivergenceofavectorﬁeld.Forneuralnetworks thesedynamicsformallydeﬁnecontinuous-timegradientdescentwhenthehiddenlayerhasinﬁnitesize(seeTheorem2.6of[15] forinstance).Moregenerally equation3.1isduetotheformulaforWassersteingradientﬂowdynamics(seeforexample[64]) whicharederivedviacontinuous-timesteepestdescentwithrespecttoWassersteindistanceoverthespaceofprobabilitydistributionsontheneurons.Weproposethefollowingmodiﬁeddynamics:ddtρt=−σρt+σUd−∇·(v[ρt]ρt)(3.2)whereUdistheuniformdistributiononSd.Inourperturbeddynamics weaddverysmalluniformnoiseoverUd whichensuresthatatalltime-steps thereissufﬁcientmassinadescentdirectionforthealgorithmtodecreasetheobjective.Forinﬁnite-sizeneuralnetworks onecaninformallyinterpretthisasre-initializingaverysmallfractionoftheneuronsateverystepofgradientdescent.Weproveconvergencetoaglobaloptimizerintimepolynomialin1/ d andtheregularityparameters.Theorem3.3(TheoremE.4withregularityparametersomitted).SupposethatΦandVare2-homogeneousandtheregularityconditionsofAssumption3.1aresatisﬁed.Alsoassumethatfromstartingdistributionρ0 asolutiontothedynamicsinequation3.2exists.DeﬁneL? infρL[ρ].Let>0beadesirederrorthresholdandchooseσ exp(−dlog(1/)poly(k L[ρ0]−L?))andt d24poly(log(1/) k L[ρ0]−L?) wheretheregularityparametersforΦ V andRarehiddeninthepoly(·).Then perturbedWassersteingradientﬂowconvergestoan-approximateglobalminimuminttime:min0≤t≤tL[ρt]−L?≤WestateandproveaversionofTheorem3.3thatincludesregularityparametersinSec-tionsE.1andE.3.Thekeyideafortheproofisasfollows:asRisconvex theoptimizationproblemwillbeconvexoverthespaceofdistributionsρ.Thisconvexityallowsustoarguethatifρissuboptimal thereeitherexistsadescentdirection¯θ∈SdwhereL0[ρ](¯θ)(cid:28)0 orthegradientﬂowdynamicswillresultinalargedecreaseintheobjective.Ifsuchadirection¯θexists theuniformnoiseσUdalongwiththe2-homogeneityofΦandVwillallowtheoptimizationdynamicstoincreasethemassinthisdirectionexponentiallyfast whichcausesapolynomialdecreaseintheloss.Asatechnicaldetail Theorem3.3requiresthatasolutiontothedynamicsexists.Wecanremovethisassumptionbyanalyzingadiscrete-timeversionofequation3.2:ρt+1 ρt+η(−σρt+σUd−∇·3Thereluactivationisnon-differentiableat0andhencethegradientﬂowisnotwell-deﬁned.ChizatandBach[15]acknowledgethissamedifﬁcultywithrelu.6(v[ρt]ρt)) andadditionallyassumingΦandVhaveLipschitzgradients.Inthissetting apolynomialtimeconvergenceresultalsoholds.WestatetheresultinSectionE.4.AnimplicationofourTheorem3.3isthatforinﬁnitenetworks wecanoptimizetheweakly-regularizedlogisticlossintimepolynomialintheproblemparametersandλ−1.InTheorem2.1weonlyrequireλ−1=poly(n);thus aninﬁnitewidthneuralnetcanlearnthedistributionDuptoerror˜O(pd/n)inpolynomialtimeusingnoisygradientdescent.4WeakRegularizerGuaranteesMaxMarginSolutionsInthissection wecollectanumberofresultsregardingthemarginofaregularizedneuralnet.Theseresultsprovidethetoolsforprovinggeneralizationoftheweakly-regularizedNNsolutioninTheorem2.1.Thekeytechniqueisshowingthatwithsmallregularizerλ→0 theglobaloptimizerofregularizedlogisticlosswillobtainamaximummargin.Itiswell-understoodthatalargeneuralnetmarginimpliesgoodgeneralizationperformance[9].Infact ourresultappliestoafunctionclassmuchbroaderthantwo-layerrelunets:inTheorem4.1weshowthatwhenweaddaweakregularizertocross-entropylosswithanypositive-homogeneouspredictionfunction thenormalizedmarginoftheoptimumconvergestothemaxmargin.Forexample Theorem4.1appliestofeedforwardrelunetworksofarbitrarydepthandwidth.InTheoremC.2 weboundtheapproximationerrorinthemaximummarginwhenweonlyobtainanapproximateoptimizeroftheregularizedloss.InCorollary4.2 weleveragetheseresultsandpre-existingRademachercomplexityboundstoconcludethattheoptimizeroftheweakly-regularizedlogisticlosswillhavewidth-freegeneralizationboundscalingwiththeinverseofthemaxmarginandnetworkdepth.Finally wenotethatthemaximumpossiblemargincanonlyincreasewiththewidthofthenetwork whichsuggeststhatincreasingwidthcanimprovegeneralizationofthesolution(seeTheorem4.3).WeworkwithafamilyFofpredictionfunctionsf(·;Θ):Rd→Rthatarea-positive-homogeneousintheirparametersforsomea>0:f(x;cΘ)=caf(x;Θ) ∀c>0.WeadditionallyrequirethatfiscontinuouswhenviewedasafunctioninΘ.Forsomegeneralnormk·kandλ>0 westudytheλ-regularizedlogisticlossLλ deﬁnedasLλ(Θ) 1nnXi=1log(1+exp(−yif(xi;Θ)))+λkΘkr(4.1)forﬁxedr>0.LetΘλ∈argminLλ(Θ).4Deﬁnethenormalizedmarginγλandmax-marginγ?byγλ miniyif(xi;¯Θλ)andγ? maxkΘk≤1miniyif(xi;Θ).LetΘ?achievethismaximum.Weshowthatwithsufﬁcientlysmallregularizationlevelλ thenormalizedmarginγλapproachesthemaximummarginγ?.OurtheoremandproofareinspiredbytheresultofRossetetal.[58 59] whoanalyzethespecialcasewhenfisalinearfunction.Incontrast ourresultcanbeappliedtonon-linearfaslongasfishomogeneous.Theorem4.1.Assumethetrainingdataisseparablebyanetworkf(·;Θ?)∈Fwithanoptimalnor-malizedmarginγ?>0.Then thenormalizedmarginoftheglobaloptimumoftheweakly-regularizedobjective(equation4.1)convergestoγ?astheregularizationgoestozero.Mathematically γλ→γ?asλ→0Anintuitiveexplanationforourresultisasfollows:becauseofthehomogeneity thelossL(Θλ)roughlysatisﬁesthefollowing(forsmallλ andignoringparameterssuchasn):Lλ(Θλ)≈exp(−kΘλkaγλ)+λkΘλkrThus thelossselectsparameterswithlargermargin whiletheregularizationfavorssmallernorms.ThefullproofofthetheoremisdeferredtoSectionC.Thoughtheresultinthissectionisstatedforbinaryclassiﬁcation itextendstothemulti-classsettingwithcross-entropyloss.WeprovideformaldeﬁnitionsandresultsinSectionC.InTheoremC.2 wealsoshowthatanapproximateminimizerofLλcanobtainmarginthatapproximatesγ?.4WeformallyshowthatLλhasaminimizerinClaimC.3ofSectionC.7Althoughweconsideranexplicitregularizer ourresultisrelatedtorecentworksonalgorithmicregularizationofgradientdescentfortheunregularizedobjective.Recentworksshowthatgradientdescentﬁndstheminimumnormormax-marginsolutionforproblemsincludinglogisticregression linearizedneuralnetworks andmatrixfactorization[68 28 38 27 32].Manyoftheseproofsrequireadelicateanalysisofthealgorithm’sdynamics andsomearenotfullyrigorousduetoassumptionsontheiterates.Tothebestofourknowledge itisanopenquestiontoproveanalogousresultsforeventwo-layerrelunetworks.Incontrast byaddingtheexplicit‘2regularizertoourobjective wecanprovebroaderresultsthatapplytomulti-layerrelunetworks.Inthefollowingsectionweleverageourresultandexistinggeneralizationbounds[25]tohelpjustifyhowover-parameterizationcanimprovegeneralization.4.1GeneralizationoftheMax-MarginNeuralNetWeconsiderdepth-qnetworkswith1-Lipschitz 1-positive-homogeneousactivationφforq≥2.Notethatthenetworkfunctionisq-positive-homogeneous.SupposethatthecollectionofparametersΘisgivenbymatricesW1 ... Wq.Forsimplicityweworkinthebinaryclasssetting sotheq-layernetworkcomputesareal-valuedscorefNN(x;Θ) Wqφ(Wq−1φ(···φ(W1x)···))(4.2)whereweoverloadnotationtoletφ(·)denotetheelement-wiseapplicationoftheactivationφ.Letmidenotethesizeofthei-thhiddenlayer soW1∈Rm1×d W2∈Rm2×m1 ··· Wq∈R1×mq−1.WewillletM (m1 ... mq−1)denotethesequenceofhiddenlayersizes.Wewillfocuson‘2-regularizedlogisticloss(seeequation4.1 usingk·kFandr=2)anddenoteitbyLλ M.Followingnotationestablishedinthissection wedenotetheoptimizerofLλ MbyΘλ M thenormalizedmarginofΘλ Mbyγλ M themax-marginsolutionbyΘ? M andthemax-marginbyγ? M assumedtobepositive.Ournotationemphasizesthearchitectureofthenetwork.Wecandeﬁnethepopulation0-1lossofthenetworkparameterizedbyΘbyL(Θ) Pr(x y)∼pdata[yfNN(x;Θ)≤0].WeletXdenotethedatadomainandC supx∈Xkxk2denotethelargestpossiblenormofasingledatapoint.BycombiningtheneuralnetcomplexityboundsofGolowichetal.[25]withourTheorem4.1 wecanconcludethatoptimizingweakly-regularizedlogisticlossgivesgeneralizationboundsthatdependonthemaximumpossiblenetworkmarginforthegivenarchitecture.Corollary4.2.Supposeφis1-Lipschitzand1-positive-homogeneous.Withprobabilityatleast1−δoverthedrawof(x1 y1) ... (xn yn)i.i.d.frompdata wecanboundthetesterroroftheoptimizeroftheregularizedlossbylimsupλ→0L(Θλ M).Cγ? Mqq−12√n+(γ? M)(4.3)where(γ) qloglog24Cγn+qlog(1/δ)n.Notethat(γ? M)isprimarilyasmallerorderterm sotheboundmainlyscaleswithCγ? Mq(q−1)/2√n.5Finally weobservethatthemaximumnormalizedmarginisnon-decreasingwiththesizeofthearchi-tecture.Formally fortwodepth-qarchitecturesM=(m1 ... mq−1)andM0=(m01 ... m0q−1) wesayM≤M0ifmi≤m0i∀i=1 ...q−1.Theorem4.3statesifM≤M0 themax-marginovernetworkswitharchitectureM0isatleastthemax-marginovernetworkswitharchitectureM.Theorem4.3.Recallthatγ? Mdenotesthemaximumnormalizedmarginofanetworkwitharchi-tectureM.IfM≤M0 wehaveγ? M≤γ? M0Asaimportantconsequence thegeneralizationerrorboundofCorollary4.2forM0isatleastasgoodasthatforM.5Althoughthe1q(q−1)/2factorofequationD.1decreaseswithdepthq themarginγwillalsotendtodecreaseastheconstraintk¯ΘkF≤1becomesmorestringent.8Figure2:Comparingregularizationandnoregularizationstartingfromthesameinitialization.Left:Normalizedmargin.Center:Testaccuracy.Right:Percentageofactivationpatternschanged.ThistheoremissimpletoproveandfollowsbecausewecandirectlyimplementanynetworkofarchitectureMusingoneofarchitectureM0 ifM≤M0.Thishighlightsoneofthebeneﬁtsofover-parametrization:themargindoesnotdecreasewithalargernetworksize andthereforeCorollary4.2givesabettergeneralizationbound.InSectionF weprovideempiricalevidencethatthetesterrordecreaseswithlargernetworksizewhilethemarginisnon-decreasing.ThephenomenoninTheorem4.3contrastswithstandard‘2-normalizedlinearprediction.Inthissetting addingmorefeaturesincreasesthenormofthedata andthereforethegeneralizationerrorboundscouldalsoincrease.Ontheotherhand Theorem4.3showsthataddingmoreneurons(whichcanbeviewedaslearnedfeatures)canonlyimprovethegeneralizationofthemax-marginsolution.5SimulationsWeempiricallyvalidateourtheorywithseveralsimulations.First wetrainatwo-layernetonsyntheticdatawithandwithoutexplicitregularizationstartingfromthesameinitializationinordertodemonstratetheeffectofanexplicitregularizerongeneralization.Weconﬁrmthattheregularizednetworkdoesindeedgeneralizebetterandmovesfurtherfromitsinitialization.Forthisexperiment weusealargeinitializationscale soeveryweight∼N(0 1).Weaveragethisexperimentover20trialsandplotthetestaccuracy normalizedmargin andpercentagechangeinactivationpatternsinFigure2.Wecomputethepercentageofactivationpatternschangedovereverypossiblepairofhiddenunitandtrainingexample.Sincealowpercentageofactivationschangewhenλ=0 theunregularizedneuralnetlearnsinthekernelregime.Oursimulationsdemonstratethatanexplicitregularizerimprovesgeneralizationerroraswellasthemargin aspredictedbyourtheory.Thedatacomesfromagroundtruthnetworkwith10hiddennetworks inputdimension20 andagroundtruthunnormalizedmarginofatleast0.01.Weuseatrainingsetofsize200andtrainfor20000stepswithlearningrate0.1 onceusingregularizerλ=5×10−4andonceusingregularizationλ=0.Wenotethatthetrainingerrorhits0extremelyquickly(within50trainingiterations).Theinitialnormalizedmarginisnegativebecausethetrainingerrorhasnotyethitzero.Wealsocomparethegeneralizationofaregularizedneuralnetandkernelmethodasthesamplesizeincreases.Furthermore wedemonstratethatfortwo-layernets thetesterrordecreasesandmarginincreasesasthewidthofthehiddenlayergrows aspredictedbyourtheory.WeprovideﬁguresandfulldetailsinSectionF.6ConclusionWehaveshowntheoreticallyandempiricallythatexplicitly‘2regularizedneuralnetscangeneralizebetterthanthecorrespondingkernelmethod.Wealsoarguethatmaximizingmarginisoneoftheinductivebiasesofrelunetworksobtainedfromoptimizingweakly-regularizedcross-entropyloss.Tocomplementthesegeneralizationresults westudyoptimizationandprovethatitispossibletoﬁndaglobalminimizeroftheregularizedlossinpolynomialtimewhenthenetworkwidthisinﬁnite.Anaturaldirectionforfutureworkistoapplyourtheorytooptimizethemarginofﬁnite-sizedneuralnetworks.9AcknowledgmentsCWacknowledgesthesupportofaNSFGraduateResearchFellowship.JDLacknowledgessupportoftheAROunderMURIAwardW911NF-11-1-0303.ThisispartofthecollaborationbetweenUSDOD UKMODandUKEngineeringandPhysicalResearchCouncil(EPSRC)undertheMultidisciplinaryUniversityResearchInitiative.WealsothankNatiSrebroandSuriyaGunasekarforhelpfuldiscussionsinvariousstagesofthiswork.References[1]ZeyuanAllen-Zhu YuanzhiLi andYingyuLiang.Learningandgeneralizationinoverpa-rameterizedneuralnetworks goingbeyondtwolayers.arXivpreprintarXiv:1811.04918 2018.[2]ZeyuanAllen-Zhu YuanzhiLi andZhaoSong.Aconvergencetheoryfordeeplearningviaover-parameterization.arXivpreprintarXiv:1811.03962 2018.[3]SanjeevArora NadavCohen andEladHazan.Ontheoptimizationofdeepnetworks:Implicitaccelerationbyoverparameterization.arXivpreprintarXiv:1802.06509 2018.[4]SanjeevArora RongGe BehnamNeyshabur andYiZhang.Strongergeneralizationboundsfordeepnetsviaacompressionapproach.arXivpreprintarXiv:1802.05296 2018.[5]SanjeevArora SimonS.Du WeiHu ZhiyuanLi RuslanSalakhutdinov andRuosongWang.OnExactComputationwithanInﬁnitelyWideNeuralNet.arXive-prints art.arXiv:1904.11955 Apr2019.[6]SanjeevArora SimonSDu WeiHu ZhiyuanLi andRuosongWang.Fine-grainedanalysisofoptimizationandgeneralizationforoverparameterizedtwo-layerneuralnetworks.arXivpreprintarXiv:1901.08584 2019.[7]FrancisBach.Breakingthecurseofdimensionalitywithconvexneuralnetworks.JournalofMachineLearningResearch 18(19):1–53 2017.[8]KeithBall.Anelementaryintroductiontomodernconvexgeometry.Flavorsofgeometry 31:1–58 1997.[9]PeterLBartlett DylanJFoster andMatusJTelgarsky.Spectrally-normalizedmarginboundsforneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages6240–6249 2017.[10]MikhailBelkin SiyuanMa andSoumikMandal.Tounderstanddeeplearningweneedtounderstandkernellearning.arXivpreprintarXiv:1802.01396 2018.[11]YoshuaBengio NicolasLRoux PascalVincent OlivierDelalleau andPatriceMarcotte.Convexneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages123–130 2006.[12]AlonBrutzkus AmirGloberson EranMalach andShaiShalev-Shwartz.Sgdlearnsover-parameterizednetworksthatprovablygeneralizeonlinearlyseparabledata.arXivpreprintarXiv:1710.10174 2017.[13]YuanCaoandQuanquanGu.Ageneralizationtheoryofgradientdescentforlearningover-parameterizeddeeprelunetworks.arXivpreprintarXiv:1902.01384 2019.[14]PratikChaudhari AnnaChoromanska StefanoSoatto YannLeCun CarloBaldassi ChristianBorgs JenniferChayes LeventSagun andRiccardoZecchina.Entropy-sgd:Biasinggradientdescentintowidevalleys.arXivpreprintarXiv:1611.01838 2016.[15]LenaicChizatandFrancisBach.Ontheglobalconvergenceofgradientdescentforover-parameterizedmodelsusingoptimaltransport.arXivpreprintarXiv:1805.09545 2018.[16]LenaicChizatandFrancisBach.Anoteonlazytraininginsuperviseddifferentiableprogram-ming.arXivpreprintarXiv:1812.07956 2018.[17]AmitDaniely.SGDlearnstheconjugatekernelclassofthenetwork.InAdvancesinNeuralInformationProcessingSystems pages2422–2430 2017.[18]XialiangDouandTengyuanLiang.Trainingneuralnetworksaslearningdata-adaptivekernels:Provablerepresentationandapproximationbeneﬁts.arXivpreprintarXiv:1901.07114 2019.10[19]SimonSDuandJasonDLee.Onthepowerofover-parametrizationinneuralnetworkswithquadraticactivation.arXivpreprintarXiv:1803.01206 2018.[20]SimonSDu JasonDLee YuandongTian BarnabasPoczos andAartiSingh.Gradientdescentlearnsone-hidden-layercnn:Don’tbeafraidofspuriouslocalminima.arXivpreprintarXiv:1712.00779 2017.[21]SimonSDu JasonDLee HaochuanLi LiweiWang andXiyuZhai.Gradientdescentﬁndsglobalminimaofdeepneuralnetworks.arXivpreprintarXiv:1811.03804 2018.[22]SimonSDu XiyuZhai BarnabasPoczos andAartiSingh.Gradientdescentprovablyoptimizesover-parameterizedneuralnetworks.arXivpreprintarXiv:1810.02054 2018.[23]GintareKarolinaDziugaiteandDanielMRoy.Computingnonvacuousgeneralizationboundsfordeep(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata.arXivpreprintarXiv:1703.11008 2017.[24]AdriàGarriga-Alonso LaurenceAitchison andCarlEdwardRasmussen.Deepconvolutionalnetworksasshallowgaussianprocesses.arXivpreprintarXiv:1808.05587 2018.[25]NoahGolowich AlexanderRakhlin andOhadShamir.Size-independentsamplecomplexityofneuralnetworks.arXivpreprintarXiv:1712.06541 2017.[26]SuriyaGunasekar BlakeEWoodworth SrinadhBhojanapalli BehnamNeyshabur andNatiSrebro.Implicitregularizationinmatrixfactorization.InAdvancesinNeuralInformationProcessingSystems pages6151–6159 2017.[27]SuriyaGunasekar JasonLee DanielSoudry andNathanSrebro.Characterizingimplicitbiasintermsofoptimizationgeometry.arXivpreprintarXiv:1802.08246 2018.[28]SuriyaGunasekar JasonLee DanielSoudry andNathanSrebro.Implicitbiasofgradientdescentonlinearconvolutionalnetworks.arXivpreprintarXiv:1806.00468 2018.[29]BenjaminDHaeffeleandRenéVidal.Globaloptimalityintensorfactorization deeplearning andbeyond.arXivpreprintarXiv:1506.07540 2015.[30]MoritzHardt BenjaminRecht andYoramSinger.Trainfaster generalizebetter:Stabilityofstochasticgradientdescent.arXivpreprintarXiv:1509.01240 2015.[31]ArthurJacot FranckGabriel andClémentHongler.Neuraltangentkernel:Convergenceandgeneralizationinneuralnetworks.arXivpreprintarXiv:1806.07572 2018.[32]ZiweiJiandMatusTelgarsky.Riskandparameterconvergenceoflogisticregression.arXivpreprintarXiv:1803.07300 2018.[33]ShamMKakade KarthikSridharan andAmbujTewari.Onthecomplexityoflinearprediction:Riskbounds marginbounds andregularization.InAdvancesinneuralinformationprocessingsystems pages793–800 2009.[34]VladimirKoltchinskii DmitryPanchenko etal.Empiricalmargindistributionsandboundingthegeneralizationerrorofcombinedclassiﬁers.TheAnnalsofStatistics 30(1):1–50 2002.[35]AlexKrizhevsky IlyaSutskever andGeoffreyEHinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages1097–1105 2012.[36]JaehoonLee YasamanBahri RomanNovak SamuelSSchoenholz JeffreyPennington andJaschaSohl-Dickstein.Deepneuralnetworksasgaussianprocesses.arXivpreprintarXiv:1711.00165 2017.[37]YuanzhiLiandYingyuLiang.Learningoverparameterizedneuralnetworksviastochasticgradientdescentonstructureddata.InAdvancesinNeuralInformationProcessingSystems pages8168–8177 2018.[38]YuanzhiLi TengyuMa andHongyangZhang.Algorithmicregularizationinover-parameterizedmatrixsensingandneuralnetworkswithquadraticactivations.InConferenceOnLearningTheory pages2–47 2018.[39]T.LiangandA.Rakhlin.JustInterpolate:Kernel“Ridgeless”RegressionCanGeneralize.ArXive-prints August2018.[40]RoiLivni ShaiShalev-Shwartz andOhadShamir.Onthecomputationalefﬁciencyoftrainingneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages855–863 2014.11[41]CongMa KaizhengWang YuejieChi andYuxinChen.Implicitregularizationinnonconvexstatisticalestimation:Gradientdescentconvergeslinearlyforphaseretrieval matrixcompletionandblinddeconvolution.arXivpreprintarXiv:1711.10467 2017.[42]AlexanderGdeGMatthews MarkRowland JiriHron RichardETurner andZoubinGhahramani.Gaussianprocessbehaviourinwidedeepneuralnetworks.arXivpreprintarXiv:1804.11271 2018.[43]SongMei AndreaMontanari andPhan-MinhNguyen.Ameanﬁeldviewofthelandscapeoftwo-layersneuralnetworks.arXivpreprintarXiv:1804.06561 2018.[44]SongMei TheodorMisiakiewicz andAndreaMontanari.Mean-ﬁeldtheoryoftwo-layersneuralnetworks:dimension-freeboundsandkernellimit.arXivpreprintarXiv:1902.06015 2019.[45]AriSMorcos DavidGTBarrett NeilCRabinowitz andMatthewBotvinick.Ontheimportanceofsingledirectionsforgeneralization.arXivpreprintarXiv:1803.06959 2018.[46]MorShpigelNacson JasonLee SuriyaGunasekar NathanSrebro andDanielSoudry.Conver-genceofgradientdescentonseparabledata.arXivpreprintarXiv:1803.01905 2018.[47]RadfordMNeal.Priorsforinﬁnitenetworks.InBayesianLearningforNeuralNetworks pages29–53.Springer 1996.[48]BehnamNeyshabur RyotaTomioka andNathanSrebro.Insearchoftherealinductivebias:Ontheroleofimplicitregularizationindeeplearning.arXivpreprintarXiv:1412.6614 2014.[49]BehnamNeyshabur RuslanRSalakhutdinov andNatiSrebro.Path-sgd:Path-normalizedoptimizationindeepneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages2422–2430 2015.[50]BehnamNeyshabur RyotaTomioka andNathanSrebro.Norm-basedcapacitycontrolinneuralnetworks.InConferenceonLearningTheory pages1376–1401 2015.[51]BehnamNeyshabur SrinadhBhojanapalli DavidMcAllester andNathanSrebro.Apac-bayesianapproachtospectrally-normalizedmarginboundsforneuralnetworks.arXivpreprintarXiv:1707.09564 2017.[52]BehnamNeyshabur SrinadhBhojanapalli DavidMcAllester andNatiSrebro.Exploringgeneralizationindeeplearning.InAdvancesinNeuralInformationProcessingSystems pages5947–5956 2017.[53]BehnamNeyshabur ZhiyuanLi SrinadhBhojanapalli YannLeCun andNathanSrebro.Towardsunderstandingtheroleofover-parametrizationingeneralizationofneuralnetworks.arXivpreprintarXiv:1805.12076 2018.[54]AndrewYNg.Featureselection l1vs.l2regularization androtationalinvariance.InProceedingsofthetwenty-ﬁrstinternationalconferenceonMachinelearning page78.ACM 2004.[55]QuynhNguyenandMatthiasHein.Thelosssurfaceofdeepandwideneuralnetworks.arXivpreprintarXiv:1704.08045 2017.[56]RomanNovak LechaoXiao JaehoonLee YasamanBahri DanielAAbolaﬁa JeffreyPenning-ton andJaschaSohl-Dickstein.Bayesianconvolutionalneuralnetworkswithmanychannelsaregaussianprocesses.arXivpreprintarXiv:1810.05148 2018.[57]RyanO’Donnell.Analysisofbooleanfunctions.CambridgeUniversityPress 2014.[58]SaharonRosset JiZhu andTrevorHastie.Boostingasaregularizedpathtoamaximummarginclassiﬁer.JournalofMachineLearningResearch 5(Aug):941–973 2004.[59]SaharonRosset JiZhu andTrevorJHastie.Marginmaximizinglossfunctions.InAdvancesinneuralinformationprocessingsystems pages1237–1244 2004.[60]SaharonRosset GrzegorzSwirszcz NathanSrebro andJiZhu.l1regularizationininﬁnitedimensionalfeaturespaces.InInternationalConferenceonComputationalLearningTheory pages544–558.Springer 2007.[61]GrantMRotskoffandEricVanden-Eijnden.Neuralnetworksasinteractingparticlesystems:Asymptoticconvexityofthelosslandscapeanduniversalscalingoftheapproximationerror.arXivpreprintarXiv:1805.00915 2018.12[62]MarkRudelson RomanVershynin etal.Hanson-wrightinequalityandsub-gaussianconcentra-tion.ElectronicCommunicationsinProbability 18 2013.[63]ItaySafranandOhadShamir.Onthequalityoftheinitialbasininoverspeciﬁedneuralnetworks.InInternationalConferenceonMachineLearning pages774–782 2016.[64]FilippoSantambrogio.{Euclidean metric andWasserstein}gradientﬂows:anoverview.BulletinofMathematicalSciences 7(1):87–154 2017.[65]JustinSirignanoandKonstantinosSpiliopoulos.Meanﬁeldanalysisofneuralnetworks.arXivpreprintarXiv:1805.01053 2018.[66]MahdiSoltanolkotabi AdelJavanmard andJasonDLee.Theoreticalinsightsintotheop-timizationlandscapeofover-parameterizedshallowneuralnetworks.IEEETransactionsonInformationTheory 65(2):742–769 2019.[67]DanielSoudryandYairCarmon.Nobadlocalminima:Dataindependenttrainingerrorguaranteesformultilayerneuralnetworks.arXivpreprintarXiv:1605.08361 2016.[68]DanielSoudry EladHoffer andNathanSrebro.Theimplicitbiasofgradientdescentonseparabledata.InInternationalConferenceonLearningRepresentations 2018.URLhttps://openreview.net/forum?id=r1q7n9gAb.[69]LucaVenturi AfonsoBandeira andJoanBruna.Neuralnetworkswithﬁniteintrinsicdimensionhavenospuriousvalleys.arXivpreprintarXiv:1802.06384 2018.[70]ColinWeiandTengyuMa.Data-dependentSampleComplexityofDeepNeuralNetworksviaLipschitzAugmentation.arXive-prints art.arXiv:1905.03684 May2019.[71]ChristopherKIWilliams.Computingwithinﬁnitenetworks.InAdvancesinneuralinformationprocessingsystems pages295–301 1997.[72]GregYang.Scalinglimitsofwideneuralnetworkswithweightsharing:Gaussianpro-cessbehavior gradientindependence andneuraltangentkernelderivation.arXivpreprintarXiv:1902.04760 2019.[73]GiladYehudaiandOhadShamir.Onthepowerandlimitationsofrandomfeaturesforunder-standingneuralnetworks.arXivpreprintarXiv:1904.00687 2019.[74]ChiyuanZhang SamyBengio MoritzHardt BenjaminRecht andOriolVinyals.Understandingdeeplearningrequiresrethinkinggeneralization.arXivpreprintarXiv:1611.03530 2016.[75]JiZhu SaharonRosset RobertTibshirani andTrevorJHastie.1-normsupportvectormachines.InAdvancesinneuralinformationprocessingsystems pages49–56 2004.[76]DifanZou YuanCao DongruoZhou andQuanquanGu.Stochasticgradientdescentoptimizesover-parameterizeddeeprelunetworks.arXivpreprintarXiv:1811.08888 2018.13,Colin Wei
Jason Lee
Qiang Liu
Tengyu Ma