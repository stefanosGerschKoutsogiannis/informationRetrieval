2016,Coin Betting and Parameter-Free Online Learning,In the recent years  a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice.  These algorithms achieve optimal regret bounds that depend on the unknown competitors  without having to tune the learning rates with oracle choices.  We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice  based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator.  The resulting algorithms are simple  with no parameters to be tuned  and they improve or match previous results in terms of regret guarantee and per-round complexity.,Coin Betting and Parameter-Free Online Learning

Francesco Orabona

Stony Brook University  Stony Brook  NY

D´avid P´al

Yahoo Research  New York  NY

francesco@orabona.com

dpal@yahoo-inc.com

Abstract

In the recent years  a number of parameter-free algorithms have been developed
for online linear optimization over Hilbert spaces and for learning with expert ad-
vice. These algorithms achieve optimal regret bounds that depend on the unknown
competitors  without having to tune the learning rates with oracle choices.
We present a new intuitive framework to design parameter-free algorithms for both
online linear optimization over Hilbert spaces and for learning with expert advice 
based on reductions to betting on outcomes of adversarial coins. We instantiate
it using a betting algorithm based on the Krichevsky-Troﬁmov estimator. The
resulting algorithms are simple  with no parameters to be tuned  and they improve
or match previous results in terms of regret guarantee and per-round complexity.

Introduction

1
We consider the Online Linear Optimization (OLO) [4  25] setting. In each round t  an algorithm
chooses a point wt from a convex decision set K and then receives a reward vector gt. The algo-
rithm’s goal is to keep its regret small  deﬁned as the difference between its cumulative reward and
the cumulative reward of a ﬁxed strategy u ∈ K  that is

T(cid:88)

(cid:104)gt  u(cid:105) − T(cid:88)

(cid:104)gt  wt(cid:105) .

RegretT (u) =

t=1

t=1

We focus on two particular decision sets  the N-dimensional probability simplex ∆N = {x ∈
RN : x ≥ 0 (cid:107)x(cid:107)1 = 1} and a Hilbert space H. OLO over ∆N is referred to as the problem of
Learning with Expert Advice (LEA). We assume bounds on the norms of the reward vectors: For
OLO over H  we assume that (cid:107)gt(cid:107) ≤ 1  and for LEA we assume that gt ∈ [0  1]N .
OLO is a basic building block of many machine learning problems. For example  Online Convex
Optimization (OCO)  the problem analogous to OLO where (cid:104)gt  u(cid:105) is generalized to an arbitrary
convex function (cid:96)t(u)  is solved through a reduction to OLO [25]. LEA [17  27  5] provides a
way of combining classiﬁers and it is at the heart of boosting [12]. Batch and stochastic convex
optimization can also be solved through a reduction to OLO [25].
To achieve optimal regret  most of the existing online algorithms require the user to set the learning
rate (step size) η to an unknown/oracle value. For example  to obtain the optimal bound for Online
Gradient Descent (OGD)  the learning rate has to be set with the knowledge of the norm of the
competitor u  (cid:107)u(cid:107); second entry in Table 1. Likewise  the optimal learning rate for Hedge depends on
the KL divergence between the prior weighting π and the unknown competitor u  D (u(cid:107)π); seventh
entry in Table 1. Recently  new parameter-free algorithms have been proposed  both for LEA [6  8 
18  19  15  11] and for OLO/OCO over Hilbert spaces [26  23  21  22  24]. These algorithms adapt
to the number of experts and to the norm of the optimal predictor  respectively  without the need
to tune parameters. However  their design and underlying intuition is still a challenge. Foster et al.
[11] proposed a uniﬁed framework  but it is not constructive. Furthermore  all existing algorithms for
LEA either have sub-optimal regret bound (e.g. extra O(log log T ) factor) or sub-optimal running
time (e.g. requiring solving a numerical problem in every round  or with extra factors); see Table 1.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Algorithm

T

[25]
[25]

OGD  η = 1√
OGD  η = U√
[23]
[22  24]
This paper  Sec. 7.1

T

(cid:113) ln N

T

Hedge  η =
Hedge  η = U√
[6]
[8]
[8  19  15]2
[11]
This paper  Sec. 7.2

T   πi = 1
[12]

N [12]

U
O((cid:107)u(cid:107) ln(1 + (cid:107)u(cid:107) T )

Worst-case regret guarantee
√
T )  ∀u ∈ H
O((1 + (cid:107)u(cid:107)2)
√
T for any u ∈ H s.t. (cid:107)u(cid:107) ≤ U
√
T )  ∀u ∈ H

O((cid:107)u(cid:107)(cid:112)T ln(1 + (cid:107)u(cid:107) T ))  ∀u ∈ H
O((cid:107)u(cid:107)(cid:112)T ln(1 + (cid:107)u(cid:107) T ))  ∀u ∈ H
T ) for any u ∈ ∆N s.t.(cid:112)D (u(cid:107)π) ≤ U
O((cid:112)T (1 + D (u(cid:107)π)) + ln2 N )  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (ln ln T + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N

T ln N )  ∀u ∈ ∆N

O(

√

√

O(U

Per-round time

complexity

Adaptive Uniﬁed
analysis

O(1)
O(1)
O(1)
O(1)
O(1)
O(N )
O(N )
O(N K)1
O(N K)1
O(N )
O(N )

O(N ln maxu∈∆N D (u(cid:107)π))3

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

Table 1: Algorithms for OLO over Hilbert space and LEA.

Contributions. We show that a more fundamental notion subsumes both OLO and LEA parameter-
free algorithms. We prove that the ability to maximize the wealth in bets on the outcomes of coin
ﬂips implies OLO and LEA parameter-free algorithms. We develop a novel potential-based frame-
work for betting algorithms. It gives intuition to previous constructions and  instantiated with the
Krichevsky-Troﬁmov estimator  provides new and elegant algorithms for OLO and LEA. The new
algorithms also have optimal worst-case guarantees on regret and time complexity; see Table 1.

2 Preliminaries

crete distributions p and q is D (p(cid:107)q) =(cid:80)

We begin by providing some deﬁnitions. The Kullback-Leibler (KL) divergence between two dis-
i pi ln (pi/qi). If p  q are real numbers in [0  1]  we denote
by D (p(cid:107)q) = p ln (p/q)+(1−p) ln ((1 − p)/(1 − q)) the KL divergence between two Bernoulli dis-
tributions with parameters p and q. We denote by H a Hilbert space  by (cid:104)· ·(cid:105) its inner product  and by
(cid:107)·(cid:107) the induced norm. We denote by (cid:107)·(cid:107)1 the 1-norm in RN . A function F : I → R+ is called loga-
rithmically convex iff f (x) = ln(F (x)) is convex. Let f : V → R ∪ {±∞}  the Fenchel conjugate
of f is f∗ : V ∗ → R∪{±∞} deﬁned on the dual vector space V ∗ by f∗(θ) = supx∈V (cid:104)θ  x(cid:105)−f (x).
A function f : V → R ∪ {+∞} is said to be proper if there exists x ∈ V such that f (x) is ﬁnite. If
f is a proper lower semi-continuous convex function then f∗ is also proper lower semi-continuous
convex and f∗∗ = f.
Coin Betting. We consider a gambler making repeated bets on the outcomes of adversarial coin
ﬂips. The gambler starts with an initial endowment  > 0. In each round t  he bets on the outcome
of a coin ﬂip gt ∈ {−1  1}  where +1 denotes heads and −1 denotes tails. We do not make any
assumption on how gt is generated  that is  it can be chosen by an adversary.
The gambler can bet any amount on either heads or tails. However  he is not allowed to borrow any
additional money. If he loses  he loses the betted amount; if he wins  he gets the betted amount back
and  in addition to that  he gets the same amount as a reward. We encode the gambler’s bet in round t
by a single number wt. The sign of wt encodes whether he is betting on heads or tails. The absolute
value encodes the betted amount. We deﬁne Wealtht as the gambler’s wealth at the end of round t
and Rewardt as the gambler’s net reward (the difference of wealth and initial endowment)  that is

Wealtht =  +

wigi

and

Rewardt = Wealtht −  .

(1)

i=1

In the following  we will also refer to a bet with βt  where βt is such that

(2)
The absolute value of βt is the fraction of the current wealth to bet  and sign of βt encodes whether
he is betting on heads or tails. The constraint that the gambler cannot borrow money implies that
βt ∈ [−1  1]. We also generalize the problem slightly by allowing the outcome of the coin ﬂip gt to
be any real number in the interval [−1  1]; wealth and reward in (1) remain exactly the same.

wt = βt Wealtht−1 .

1These algorithms require to solve a numerical problem at each step. The number K is the number of steps

needed to reach the required precision. Neither the precision nor K are calculated in these papers.

2The proof in [15] can be modiﬁed to prove a KL bound  see http://blog.wouterkoolen.info.
3A variant of the algorithm in [11] can be implemented with the stated time complexity [10].

2

t(cid:88)

3 Warm-Up: From Betting to One-Dimensional Online Linear Optimization

t=1 be its sequence of predictions on a sequence of rewards {gt}∞

reward of the algorithm after t rounds is Rewardt =(cid:80)t

In this section  we sketch how to reduce one-dimensional OLO to betting on a coin. The reasoning
for generic Hilbert spaces (Section 5) and for LEA (Section 6) will be similar. We will show that
the betting view provides a natural way for the analysis and design of online learning algorithms 
where the only design choice is the potential function of the betting algorithm (Section 4). A speciﬁc
example of coin betting potential and the resulting algorithms are in Section 7.
As a warm-up  let us consider an algorithm for OLO over one-dimensional Hilbert space R. Let
{wt}∞
t=1  gt ∈ [−1  1]. The total
i=1 giwi. Also  even if in OLO there is no
concept of “wealth”  deﬁne the wealth of the OLO algorithm as Wealtht =  + Rewardt  as in (1).
We now restrict our attention to algorithms whose predictions wt are of the form of a bet  that is
wt = βt Wealtht−1  where βt ∈ [−1  1]. We will see that the restriction on βt does not prevent us
from obtaining parameter-free algorithms with optimal bounds.
Given the above  it is immediate to see that any coin betting algorithm that  on a sequence of
coin ﬂips {gt}∞
t=1  gt ∈ [−1  1]  bets the amounts wt can be used as an OLO algorithm in a one-
dimensional Hilbert space R. But  what would be the regret of such OLO algorithms?

Assume that the betting algorithm at hand guarantees that its wealth is at least F ((cid:80)T

t=1 gt) starting

from an endowment   for a given potential function F   then

RewardT =

gtwt = WealthT −  ≥ F

−  .

(3)

gt

T(cid:88)

(cid:32) T(cid:88)

(cid:33)

t=1

t=1

(cid:125)

t=1

t=1

≥ F

− 

gt

(cid:33)

(cid:123)(cid:122)

(cid:104)gt  wt(cid:105)

(cid:32) T(cid:88)

Intuitively  if the reward is big we can expect the regret to be small. Indeed  the following lemma
converts the lower bound on the reward to an upper bound on the regret.
Lemma 1 (Reward-Regret relationship [22]). Let V  V ∗ be a pair of dual vector spaces. Let F :
V → R∪{+∞} be a proper convex lower semi-continuous function and let F ∗ : V ∗ → R∪{+∞}
be its Fenchel conjugate. Let w1  w2  . . .   wT ∈ V and g1  g2  . . .   gT ∈ V ∗. Let  ∈ R. Then 

T(cid:88)
(cid:124)
To summarize  if we have a betting algorithm that guarantees a minimum wealth of F ((cid:80)T
Applying the lemma  we get a regret upper bound: RegretT (u) ≤ F ∗(u) +  for all u ∈ H.
algorithm that is adaptive to u is equivalent to designing an algorithm that is adaptive to(cid:80)T

t=1 gt) 
it can be used to design and analyze a one-dimensional OLO algorithm. The faster the growth of
the wealth  the smaller the regret will be. Moreover  the lemma also shows that trying to design an
t=1 gt.
Also  most importantly  methods that guarantee optimal wealth for the betting scenario are already
known  see  e.g.  [4  Chapter 9]. We can just re-use them to get optimal online algorithms!

≤ F ∗(u) +  .

T(cid:88)
(cid:124)

(cid:104)gt  u − wt(cid:105)

∀u ∈ V ∗ 

if and only if

RegretT (u)

(cid:123)(cid:122)

RewardT

(cid:125)

t=1

4 Designing a Betting Algorithm: Coin Betting Potentials

For sequential betting on i.i.d. coin ﬂips  an optimal strategy has been proposed by Kelly [14].
t=1  gt ∈ {+1 −1}  are generated i.i.d. with known
The strategy assumes that the coin ﬂips {gt}∞
probability of heads. If p ∈ [0  1] is the probability of heads  the Kelly bet is to bet βt = 2p − 1 at
each round. He showed that  in the long run  this strategy will provide more wealth than betting any
other ﬁxed fraction of the current wealth [14].
For adversarial coins  Kelly betting does not make sense. With perfect knowledge of the future  the
gambler could always bet everything on the right outcome. Hence  after T rounds from an initial
endowment   the maximum wealth he would get is 2T . Instead  assume he bets the same fraction β
of its wealth at each round. Let Wealtht(β) the wealth of such strategy after t rounds. As observed

in [21]  the optimal ﬁxed fraction to bet is β∗ = ((cid:80)T
(cid:80)T
t=1 gt)/T and it gives the wealth
t=1 gt)2
t=1 gt
2T
2T

(cid:17)(cid:17) ≥  exp

WealthT (β∗) =  exp

(cid:16) ((cid:80)T

 

(4)

(cid:16) 1

T · D

(cid:17)

(cid:16)

2 +

(cid:13)(cid:13)(cid:13) 1

2

3

where the inequality follows from Pinsker’s inequality [9  Lemma 11.6.1].
However  even without knowledge of the future  it is possible to go very close to the wealth in (4).
This problem was studied by Krichevsky and Troﬁmov [16]  who proposed that after seeing the coin
should be used instead of p.

i=1 1[gi=+1]

Their estimate is commonly called KT estimator.1 The KT estimator results in the betting

ﬂips g1  g2  . . .   gt−1 the empirical estimate kt = 1/2+(cid:80)t−1
(cid:80)t−1
(cid:16)

βt = 2kt − 1 =

t

t

i=1 gi

WealthT ≥ WealthT (β∗)

√
2

T

√
= 
2

T

exp

T · D

which we call adaptive Kelly betting based on the KT estimator. It looks like an online and slightly
biased version of the oracle choice of β∗. This strategy guarantees2

(5)

(cid:16) 1

(cid:80)T

2 +

t=1 gt
2T

(cid:17)(cid:17)

(cid:13)(cid:13)(cid:13) 1

2

.

This guarantee is optimal up to constant factors [4] and mirrors the guarantee of the Kelly bet.
Here  we propose a new set of deﬁnitions that allows to generalize the strategy of adaptive Kelly
betting based on the KT estimator. For these strategies it will be possible to prove that  for any
g1  g2  . . .   gt ∈ [−1  1] 

(cid:33)

Wealtht ≥ Ft

gi

 

(6)

(cid:32) t(cid:88)

i=1

where Ft(x) is a certain function. We call such functions potentials. The betting strategy will be
determined uniquely by the potential (see (c) in the Deﬁnition 2)  and we restrict our attention to
potentials for which (6) holds. These constraints are speciﬁed in the deﬁnition below.
Deﬁnition 2 (Coin Betting Potential). Let  > 0. Let {Ft}∞
Ft : (−at  at) → R+ where at > t. The sequence {Ft}∞
potentials for initial endowment   if it satisﬁes the following three conditions:

t=0 be a sequence of functions
t=0 is called a sequence of coin betting

(a) F0(0) = .
(b) For every t ≥ 0  Ft(x) is even  logarithmically convex  strictly increasing on [0  at)  and

(c) For every t ≥ 1  every x ∈ [−(t − 1)  (t − 1)] and every g ∈ [−1  1]  (1 + gβt) Ft−1(x) ≥

limx→at Ft(x) = +∞.

Ft(x + g)  where

βt = Ft(x+1)−Ft(x−1)
Ft(x+1)+Ft(x−1) .

(7)

t=0 is called a sequence of excellent coin betting potentials for initial endow-

The sequence {Ft}∞
ment  if it satisﬁes conditions (a)–(c) and the condition (d) below.
t (x) for every x ∈ [0  at).
(d) For every t ≥ 0  Ft is twice-differentiable and satisﬁes x· F (cid:48)(cid:48)
Let’s give some intuition on this deﬁnition. First  let’s show by induction on t that (b) and (c) of the
deﬁnition together with (2) give a betting strategy that satisﬁes (6). The base case t = 0 is trivial.
At time t ≥ 1  bet wt = βt Wealtht−1 where βt is deﬁned in (7)  then

t (x) ≥ F (cid:48)

Wealtht = Wealtht−1 +wtgt = (1 + gtβt) Wealtht−1

(cid:32)t−1(cid:88)

(cid:33)

(cid:32)t−1(cid:88)

(cid:33)

(cid:32) t(cid:88)

(cid:33)

≥ (1 + gtβt)Ft−1

≥ Ft

gi

gi + gt

= Ft

gi

.

i=1

i=1

i=1

The formula for the potential-based strategy (7) might seem strange. However  it is derived—see
Theorem 8 in Appendix B—by minimizing the worst-case value of the right-hand side of the in-
equality used w.r.t. to gt in the induction proof above: Ft−1(x) ≥ Ft(x+gt)
The last point  (d)  is a technical condition that allows us to seamlessly reduce OLO over a Hilbert
space to the one-dimensional problem  characterizing the worst case direction for the reward vectors.

1+gtβt

.

1Compared to the maximum likelihood estimate
2See Appendix A for a proof. For lack of space  all the appendices are in the supplementary material.

  KT estimator shrinks slightly towards 1/2.

i=1 1[gi=+1]

t−1

(cid:80)t−1

4

possible wealth in (4) to be a good candidate. In fact  Ft(x) =  exp(cid:0)x2/(2t)(cid:1) /

Regarding the design of coin betting potentials  we expect any potential that approximates the best
t  essentially
the potential used in the parameter-free algorithms in [22  24] for OLO and in [6  18  19] for LEA 
approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B. Hence 
our framework provides intuition to previous constructions and in Section 7 we show new examples
of coin betting potentials.
In the next two sections  we presents the reductions to effortlessly solve both the generic OLO case
and LEA with a betting potential.

√

5 From Coin Betting to OLO over Hilbert Space

We deﬁne reward and wealth analogously to the one-dimensional case: Rewardt =(cid:80)t

In this section  generalizing the one-dimensional construction in Section 3  we show how to use
a sequence of excellent coin betting potentials {Ft}∞
t=0 to construct an algorithm for OLO over a
Hilbert space and how to prove a regret bound for it.
i=1(cid:104)gi  wi(cid:105)
t=0  using (7) we

and Wealtht =  + Rewardt. Given a sequence of coin betting potentials {Ft}∞
deﬁne the fraction

βt =

.

(8)

The prediction of the OLO algorithm is deﬁned similarly to the one-dimensional case  but now we
also need a direction in the Hilbert space:

i=1 gi(cid:107)−1)
i=1 gi(cid:107)−1)

Ft((cid:107)(cid:80)t−1
Ft((cid:107)(cid:80)t−1
(cid:80)t−1
(cid:13)(cid:13)(cid:13)(cid:80)t−1

i=1 gi(cid:107)+1)−Ft((cid:107)(cid:80)t−1
i=1 gi(cid:107)+1)+Ft((cid:107)(cid:80)t−1
(cid:80)t−1
(cid:13)(cid:13)(cid:13) = βt
(cid:13)(cid:13)(cid:13)(cid:80)t−1

i=1 gi
i=1 gi

i=1 gi
i=1 gi

(cid:13)(cid:13)(cid:13)

(cid:32)

t−1(cid:88)

i=1

(cid:33)

If(cid:80)t−1

wt = βt Wealtht−1

 +

(cid:104)gi  wi(cid:105)

.

(9)

i=1 gi is the zero vector  we deﬁne wt to be the zero vector as well. For this prediction strategy
we can prove the following regret guarantee  proved in Appendix C. The proof reduces the general
Hilbert case to the 1-d case  thanks to (d) in Deﬁnition 2  then it follows the reasoning of Section 3.
Theorem 3 (Regret Bound for OLO in Hilbert Spaces). Let {Ft}∞
t=0 be a sequence of excellent coin
betting potentials. Let {gt}∞
t=1 be any sequence of reward vectors in a Hilbert space H such that
(cid:107)gt(cid:107) ≤ 1 for all t. Then  the algorithm that makes prediction wt deﬁned by (9) and (8) satisﬁes

∀T ≥ 0 ∀u ∈ H

RegretT (u) ≤ F ∗

T ((cid:107)u(cid:107)) +  .

6 From Coin Betting to Learning with Expert Advice
In this section  we show how to use the algorithm for OLO over one-dimensional Hilbert space R
from Section 3—which is itself based on a coin betting strategy—to construct an algorithm for LEA.
Let N ≥ 2 be the number of experts and ∆N be the N-dimensional probability simplex. Let
π = (π1  π2  . . .   πN ) ∈ ∆N be any prior distribution. Let A be an algorithm for OLO over the
one-dimensional Hilbert space R  based on a sequence of the coin betting potentials {Ft}∞
t=0 with
initial endowment3 1. We instantiate N copies of A.
Consider any round t. Let wt i ∈ R be the prediction of the i-th copy of A. The LEA algorithm

computes(cid:98)pt = ((cid:98)pt 1 (cid:98)pt 2  . . .  (cid:98)pt N ) ∈ RN

(10)
where [x]+ = max{0  x} is the positive part of x. Then  the LEA algorithm predicts pt =
(pt 1  pt 2  . . .   pt N ) ∈ ∆N as

If (cid:107)(cid:98)pt(cid:107)1 = 0  the algorithm predicts the prior π. Then  the algorithm receives the reward vector

gt = (gt 1  gt 2  . . .   gt N ) ∈ [0  1]N . Finally  it feeds the reward to each copy of A. The reward for
3Any initial endowment  > 0 can be rescaled to 1. Instead of Ft(x) we would use Ft(x)/. The wt would

(11)

become wt/  but pt is invariant to scaling of wt. Hence  the LEA algorithm is the same regardless of .

0 + as

(cid:98)pt i = πi · [wt i]+ 
pt = (cid:98)pt(cid:107)(cid:98)pt(cid:107)1

.

5

the i-th copy of A is(cid:101)gt i ∈ [−1  1] deﬁned as

(cid:26)gt i − (cid:104)gt  pt(cid:105)

(cid:101)gt i =

if wt i > 0  
[gt i − (cid:104)gt  pt(cid:105)]+ if wt i ≤ 0 .

(12)

t

The construction above deﬁnes a LEA algorithm deﬁned by the predictions pt  based on the algo-
rithm A. We can prove the following regret bound for it.
Theorem 4 (Regret Bound for Experts). Let A be an algorithm for OLO over the one-dimensional
Hilbert space R  based on the coin betting potentials {Ft}∞
t=0 for an initial endowment of 1. Let
be the inverse of ft(x) = ln(Ft(x)) restricted to [0 ∞). Then  the regret of the LEA algorithm
f−1
with prior π ∈ ∆N that predicts at each round with pt in (11) satisﬁes
RegretT (u) ≤ f−1

The proof  in Appendix D  is based on the fact that (10)–(12) guarantee that(cid:80)N

∀T ≥ 0 ∀u ∈ ∆N

i=1 πi(cid:101)gt iwt i ≤ 0

and on a variation of the change of measure lemma used in the PAC-Bayes literature  e.g. [20].

T (D (u(cid:107)π)) .

7 Applications of the Krichevsky-Troﬁmov Estimator to OLO and LEA

(cid:16) t+1

(cid:17)·Γ
(cid:16) t+1
2 − x

2

(cid:17)

In the previous sections  we have shown that a coin betting potential with a guaranteed rapid growth
of the wealth will give good regret guarantees for OLO and LEA. Here  we show that the KT
estimator has associated an excellent coin betting potential  which we call KT potential. Then  the
optimal wealth guarantee of the KT potentials will translate to optimal parameter-free regret bounds.
The sequence of excellent coin betting potentials for an initial endowment  corresponding to the
adaptive Kelly betting strategy βt deﬁned by (5) based on the KT estimator are

2t·Γ

2 + x

2

where Γ(x) = (cid:82) ∞

t ≥ 0 

x ∈ (−t − 1  t + 1) 

0

π·t!

Ft(x) = 

(13)
tx−1e−tdt is Euler’s gamma function—see Theorem 13 in Appendix E. This
potential was used to prove regret bounds for online prediction with the logarithmic loss [16][4 
Chapter 9.7]. Theorem 13 also shows that the KT betting strategy βt as deﬁned by (5) satisﬁes (7).
This potential has the nice property that is satisﬁes the inequality in (c) of Deﬁnition 2 with equality
when gt ∈ {−1  1}  i.e. Ft(x + gt) = (1 + gtβt) Ft−1(x).
We also generalize the KT potentials to δ-shifted KT potentials  where δ ≥ 0  deﬁned as
(cid:19)

(cid:19)

2t·Γ(δ+1)·Γ

Ft(x) =

(cid:18) t+δ+1
(cid:18) δ+1

Γ

2

·Γ

2 + x

(cid:19)2·Γ(t+δ+1)

2

(cid:18) t+δ+1
2 − x

2

.

The reason for its name is that  up to a multiplicative constant  Ft is equal to the KT potential
shifted in time by δ. Theorem 13 also proves that the δ-shifted KT potentials are excellent coin
j=1 gj
.
betting potentials with initial endowment 1  and the corresponding betting fraction is βt =
δ+t

(cid:80)t−1

7.1 OLO in Hilbert Space
We apply the KT potential for the construction of an OLO algorithm over a Hilbert space H. We
will use (9)  and we just need to calculate βt. According to Theorem 13 in Appendix E  the formula
for βt simpliﬁes to βt =

i=1(cid:104)gi  wi(cid:105)(cid:17)(cid:80)t−1

 +(cid:80)t−1

(cid:107)(cid:80)t−1
i=1 gi(cid:107)

so that wt = 1
t

i=1 gi.

(cid:16)

t

The resulting algorithm is stated as Algorithm 1. We derive a regret bound for it as a very simple
corollary of Theorem 3 to the KT potential (13). The only technical part of the proof  in Appendix F 
is an upper bound on F ∗
Corollary 5 (Regret Bound for Algorithm 1). Let  > 0. Let {gt}∞
vectors in a Hilbert space H such that (cid:107)gt(cid:107) ≤ 1. Then Algorithm 1 satisﬁes
1 + 24T 2(cid:107)u(cid:107)2

t since it cannot be expressed as an elementary function.

t=1 be any sequence of reward

∀ T ≥ 0 ∀u ∈ H

RegretT (u) ≤ (cid:107)u(cid:107)

1 − 1
√

(cid:114)

(cid:17)

(cid:17)

(cid:16)

(cid:16)

T ln

+ 

.

2

e

πT

6

Algorithm 1 Algorithm for OLO over Hilbert space H based on KT potential
Require: Initial endowment  > 0
1: for t = 1  2  . . . do
2:
3:
4: end for

Predict with wt ← 1
i=1 gi
Receive reward vector gt ∈ H such that (cid:107)gt(cid:107) ≤ 1

i=1(cid:104)gi  wi(cid:105)(cid:17)(cid:80)t−1

(cid:16)
 +(cid:80)t−1

t

Algorithm 2 Algorithm for Learning with Expert Advice based on δ-shifted KT potential
Require: Number of experts N  prior distribution π ∈ ∆N   number of rounds T
1: for t = 1  2  . . .   T do
2:
3:

For each i ∈ [N ]  set wt i ←

j=1(cid:101)gj iwj i

1 +(cid:80)t−1

(cid:16)

(cid:17)

t+T /2

(cid:80)t−1
j=1(cid:101)gj i
For each i ∈ [N ]  set(cid:98)pt i ← πi[wt i]+
(cid:26)(cid:98)pt/(cid:107)(cid:98)pt(cid:107)1
if (cid:107)(cid:98)pt(cid:107)1 > 0
if (cid:107)(cid:98)pt(cid:107)1 = 0
(cid:26)gt i − (cid:104)gt  pt(cid:105)
For each i ∈ [N ]  set(cid:101)gt i ←

Predict with pt ←
Receive reward vector gt ∈ [0  1]N

π

if wt i > 0
[gt i − (cid:104)gt  pt(cid:105)]+ if wt i ≤ 0

4:

5:

6:
7: end for

It is worth noting the elegance and extreme simplicity of Algorithm 1 and contrast it with the algo-
rithms in [26  22–24]. Also  the regret bound is optimal [26  23]. The parameter  can be safely set
to any constant  e.g. 1. Its role is equivalent to the initial guess used in doubling tricks [25].

7.2 Learning with Expert Advice

We will now construct an algorithm for LEA based on the δ-shifted KT potential. We set δ to T /2 
requiring the algorithm to know the number of rounds T in advance; we will ﬁx this later with the
standard doubling trick.
To use the construction in Section 6  we need an OLO algorithm for the 1-d Hilbert space R. Using

the δ-shifted KT potentials  the algorithm predicts for any sequence {(cid:101)gt}∞
1 +
t−1(cid:88)

(cid:80)t−1
i=1(cid:101)gi

1 +

wt = βt Wealtht−1 = βt

 =

 .

t=1 of reward

t−1(cid:88)

(cid:101)gjwj

(cid:101)gjwj

j=1

T /2 + t

j=1

Then  following the construction in Section 6  we arrive at the ﬁnal algorithm  Algorithm 2. We can
derive a regret bound for Algorithm 2 by applying Theorem 4 to the δ-shifted KT potential.
Corollary 6 (Regret Bound for Algorithm 2). Let N ≥ 2 and T ≥ 0 be integers. Let π ∈ ∆N be a
prior. Then Algorithm 2 with input N  π  T for any rewards vectors g1  g2  . . .   gT ∈ [0  1]N satisﬁes

RegretT (u) ≤(cid:112)3T (3 + D (u(cid:107)π)) .

∀u ∈ ∆N

t

Hence  the Algorithm 2 has both the best known guarantee on worst-case regret and per-round time
complexity  see Table 1. Also  it has the advantage of being very simple.
The proof of the corollary is in the Appendix F. The only technical part of the proof is an upper
bound on f−1
The reason for using the shifted potential comes from the analysis of f−1

gorithm would have a O((cid:112)T (log T + D (u(cid:107)π)) regret bound; the shifting improves the bound to
O((cid:112)T (1 + D (u(cid:107)π)). By changing T /2 in Algorithm 2 to another constant fraction of T   it is pos-

(x)  which we conveniently do by lower bounding Ft(x).

(x). The unshifted al-

sible to trade-off between the two constants 3 present in the square root in the regret upper bound.
The requirement of knowing the number of rounds T in advance can be lifted by the standard dou-
bling trick [25  Section 2.3.1]  obtaining an anytime guarantee with a bigger leading constant 

t

∀ T ≥ 0 ∀u ∈ ∆N

RegretT (u) ≤ √

2√
2−1

(cid:112)3T (3 + D (u(cid:107)π)) .

7

Figure 1: Total loss versus learning rate parameter of OGD (in log scale)  compared with parameter-free
algorithms DFEG [23]  Adaptive Normal [22]  PiSTOL [24] and the KT-based Algorithm 1.

Figure 2: Regrets to the best expert after T = 32768 rounds  versus learning rate parameter of Hedge (in
log scale). The “good” experts are  = 0.025 better than the others. The competitor algorithms are Normal-
Hedge [6]  AdaNormalHedge [19]  Squint [15]  and the KT-based Algorithm 2. πi = 1/N for all algorithms.

8 Discussion of the Results

We have presented a new interpretation of parameter-free algorithms as coin betting algorithms. This
interpretation  far from being just a mathematical gimmick  reveals the common hidden structure
of previous parameter-free algorithms for both OLO and LEA and also allows the design of new
algorithms. For example  we show that the characteristic of parameter-freeness is just a consequence
of having an algorithm that guarantees the maximum reward possible. The reductions in Sections 5
and 6 are also novel and they are in a certain sense optimal. In fact  the obtained Algorithms 1 and 2
achieve the optimal worst case upper bounds on the regret  see [26  23] and [4] respectively.
We have also run an empirical evaluation to show that the theoretical difference between classic
online learning algorithms and parameter-free ones is real and not just theoretical. In Figure 1  we
have used three regression datasets4  and solved the OCO problem through OLO. In all the three
cases  we have used the absolute loss and normalized the input vectors to have L2 norm equal to 1.
From the empirical results  it is clear that the optimal learning rate is completely data-dependent  yet
parameter-free algorithms have performance very close to the unknown optimal tuning of the learn-
ing rate. Moreover  the KT-based Algorithm 1 seems to dominate all the other similar algorithms.
For LEA  we have used the synthetic setting in [6]. The dataset is composed of Hadamard matrices
of size 64  where the row with constant values is removed  the rows are duplicated to 126 inverting
their signs  0.025 is subtracted to k rows  and the matrix is replicated in order to generate T = 32768
samples. For more details  see [6]. Here  the KT-based algorithm is the one in Algorithm 2  where
the term T /2 is removed  so that the ﬁnal regret bound has an additional ln T term. Again  we
see that the parameter-free algorithms have a performance close or even better than Hedge with an
oracle tuning of the learning rate  with no clear winners among the parameter-free algorithms.
Notice that since the adaptive Kelly strategy based on KT estimator is very close to optimal  the only
possible improvement is to have a data-dependent bound  for example like the ones in [24  15  19].
In future work  we will extend our deﬁnitions and reductions to the data-dependent case.

4Datasets available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.

8

10−210−110010110233.053.13.153.23.253.33.353.43.453.5x 105UTotal lossYearPredictionMSD dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based10−11001011025.566.577.588.59x 104UTotal losscpusmall dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based1001021041061.71.751.81.851.91.9522.05x 109UTotal losscadata dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based100101200250300350400450Replicated Hadamard matrices  N=126  k=2 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-based100101180200220240260280300320340360380400Replicated Hadamard matrices  N=126  k=8 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-based100101150200250300350Replicated Hadamard matrices  N=126  k=32 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-basedAcknowledgments. The authors thank Jacob Abernethy  Nicol`o Cesa-Bianchi  Satyen Kale  Chan-
soo Lee  Giuseppe Molteni  and Manfred Warmuth for useful discussions on this work.

References

[1] E. Artin. The Gamma Function. Holt  Rinehart and Winston  Inc.  1964.
[2] N. Batir. Inequalities for the gamma function. Archiv der Mathematik  91(6):554–563  2008.
[3] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer Publishing Company  Incorporated  1st edition  2011.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
[5] N. Cesa-Bianchi  Y. Freund  D. Haussler  D. P. Helmbold  R. E. Schapire  and M. K. Warmuth. How to

use expert advice. J. ACM  44(3):427–485  1997.

[6] K. Chaudhuri  Y. Freund  and D. Hsu. A parameter-free hedging algorithm.

Information Processing Systems 22  pages 297–305  2009.

In Advances in Neural

[7] C.-P. Chen.
65–72  2005.

Inequalities for the polygamma functions with application. General Mathematics  13(3):

[8] A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Proc. of the 26th

Conf. on Uncertainty in Artiﬁcial Intelligence. AUAI Press  2010.

[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  2nd edition  2006.
[10] D. J. Foster. personal communication  2016.
[11] D. J. Foster  A. Rakhlin  and K. Sridharan. Adaptive online learning. In Advances in Neural Information

Processing Systems 28  pages 3375–3383. Curran Associates  Inc.  2015.

[12] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. J. Computer and System Sciences  55(1):119–139  1997.

[13] A. Hoorfar and M. Hassani. Inequalities on the Lambert W function and hyperpower function. J. Inequal.

Pure and Appl. Math  9(2)  2008.

[14] J. L. Kelly. A new interpretation of information rate. Information Theory  IRE Trans. on  2(3):185–189 

September 1956.

[15] W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games. In

Proc. of the 28th Conf. on Learning Theory  pages 1155–1175  2015.

[16] R. E. Krichevsky and V. K. Troﬁmov. The performance of universal encoding. IEEE Trans. on Information

Theory  27(2):199–206  1981.

[17] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation  108

(2):212–261  1994.

[18] H. Luo and R. E. Schapire. A drifting-games analysis for online learning and applications to boosting. In

Advances in Neural Information Processing Systems 27  pages 1368–1376  2014.

[19] H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proc. of the 28th

Conf. on Learning Theory  pages 1286–1304  2015.

[20] D. McAllester. A PAC-Bayesian tutorial with a dropout bound  2013. arXiv:1307.2118.
[21] H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In

Advances in Neural Information Processing Systems 26  pages 2724–2732  2013.

[22] H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax al-
gorithms and normal approximations. In Proc. of the 27th Conf. on Learning Theory  pages 1020–1039 
2014.

[23] F. Orabona. Dimension-free exponentiated gradient.

In Advances in Neural Information Processing

Systems 26 (NIPS 2013)  pages 1806–1814. Curran Associates  Inc.  2013.

[24] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning.

In Advances in Neural Information Processing Systems 27 (NIPS 2014)  pages 1116–1124  2014.

[25] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine

Learning  4(2):107–194  2011.

[26] M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimization. In

Advances in Neural Information Processing Systems 25 (NIPS 2012)  pages 2402–2410  2012.

[27] V. Vovk. A game of prediction with expert advice. J. Computer and System Sciences  56:153–173  1998.
[28] E. T. Whittaker and G. N. Watson. A Course of Modern Analysis. Cambridge University Press  fourth

edition  1962. Reprinted.

[29] F. M. J. Willems  Y. M. Shtarkov  and T. J. Tjalkens. The context tree weighting method: Basic properties.

IEEE Trans. on Information Theory  41:653–664  1995.

9

A From Log Loss to Wealth

Guarantees for betting or sequential investement algorithm are often expressed as upper bounds on
the regret with respect to the log loss. Here  for the sake of completeness  we show how to convert
such a guarantee to a lower bound on the wealth of the corresponding betting algorithm.
We consider the problem of predicting a binary outcome. The algorithm predicts at each round
probability pt ∈ [0  1]. The adversary generates a sequences of outcomes xt ∈ {0  1} and the
algorithm’s loss is

(cid:96)(pt  xt) = −xt ln pt − (1 − xt) ln(1 − pt) .

We deﬁne the regret with respect to a ﬁxed probability vector β as

Regretlogloss

T

=

(cid:96)(pt  xt) − min
β∈[0 1]

(cid:96)(β  xt) .

Lemma 7. Assume that an algorithm that predicts pt guarantees Regretlogloss
coin betting strategy with endowement  and βt = 2pt − 1 guarantees

T

(cid:32)

(cid:80)T

1
2

+

t=1 gt
2T

− RT

≤ RT . Then  the

(cid:33)

t=1

T(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

2

(cid:33)

T(cid:88)

t=1

(cid:32)

T · D
against any sequence of outcomes gt ∈ [−1  +1].

WealthT ≥  exp

Proof. Deﬁne xt = 1+gt

2 . We have

ln WealthT = ln(Wealtht−1 +wtgt)

= ln(Wealtht−1(1 + gtβt))

t=1

T(cid:89)
T(cid:88)
T(cid:88)
T(cid:88)

t=1

= ln 

(1 + gtβt)

ln (1 + βt) +

ln (2pt) +

(cid:19)

2

t=1

= ln  +

ln(1 + gtβt)

≥ ln  +

(cid:18) 1 + gt
(cid:19)
(cid:18) 1 + gt
(cid:19)
(cid:18) 1 + gt
T(cid:88)
= ln  + T ln(2) − T(cid:88)

= ln  + T ln(2) +

= ln  +

(cid:96)(pt  xt)

t=1

t=1

2

2

t=1

= ln  + T ln(2) − Regretlogloss

T

≥ ln  + T ln(2) − RT − min
β∈[0 1]

2

(cid:19)

(cid:18) 1 − gt
(cid:18) 1 − gt
(cid:19)
(cid:18) 1 − gt

2

ln(pt) +

2

(cid:19)

ln (1 − βt)

ln (2(1 − pt))

ln(1 − pt)

T(cid:88)

t=1

(cid:96)(β  xt)

− min
β∈[0 1]

T(cid:88)

t=1

(cid:96)(β  xt)  

where the ﬁrst inequality is due to the concavity of ln and the second one is due to the assumption
of the regret.
It is easy to see that the β∗ = arg minβ∈[0 1]

. Hence  we have

(cid:80)T

t=1 (cid:96)(β  xt) =

(cid:80)T

t=1 xt
T

T(cid:88)

min
β∈[0 1]

t=1

10

(cid:96)(β  xt) = T (−β∗ ln β∗ − (1 − β∗) ln(1 − β∗)) .

Also  we have that for any β ∈ [0  1]

−β ln β − (1 − β) ln(1 − β) = − D

(cid:18)

β

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:13) 1

2

+ ln 2 .

Putting all together  we have the stated lemma.

The lower bound on the wealth of the adaptive Kelly betting based on the KT estimator is obtained
simply by the stated Lemma and reminding that the log loss regret of the KT estimator is upper
bounded by 1

2 ln T + ln 2.

B Optimal Betting Fraction
Theorem 8 (Optimal Betting Fraction). Let x ∈ R. Let F : [x− 1  x + 1] → R be a logarithmically
convex function. Then 

F (x + g)

1 + βg

=

F (x + 1) − F (x − 1)
F (x + 1) + F (x − 1)

.

max
g∈[−1 1]

arg min
β∈(−1 1)
Moreover  β∗ = F (x+1)−F (x−1)

F (x+1)+F (x−1) satisﬁes

ln(F (x + 1)) − ln(1 + β∗) = ln(F (x − 1)) − ln(1 − β∗) .

Proof. We deﬁne the functions h  f : [−1  1] × (−1  1) → R as

h(g  β) =

F (x + g)

1 + βg

and

f (g  β) = ln(h(g  β)) = ln(F (x + g)) − ln(1 + βg) .

Clearly  arg minβ∈(−1 1) maxg∈[−1 1] h(g  β) = arg minβ∈(−1 1) maxg∈[−1 1] f (g  β) and we can
work with f instead of h. The function h is logarithmically convex in g and thus f is convex in g.
Therefore 

∀β ∈ (−1  1)

f (g  β) = max{f (+1  β)  f (−1  β)} .

max
g∈[−1 1]

Let φ(β) = max{f (+1  β)  f (−1  β)}. We seek to ﬁnd the arg minβ∈(−1 1) φ(β). Since f (+1  β)
is decreasing in β and f (−1  β) is increasing in β  the minimum of φ(β) is at a point β∗ such that
f (+1  β∗) = f (−1  β∗). In other words  β∗ satisﬁes

ln(F (x + 1)) − ln(1 + β∗) = ln(F (x − 1)) − ln(1 − β∗) .

The only solution of this equation is

β∗ =

F (x + 1) − F (x − 1)
F (x + 1) + F (x − 1)

.

Theorem 9. The functions Ft(x) =  exp( x2

2t − 1

2

(cid:80)t

i=1

1

i ) are excellent coin betting potentials.

Proof. The ﬁrst and second properties of Deﬁnition 2 are trivially true. For the third property  we
ﬁrst use Theorem 8 to have

ln(1 + βtg) − ln Ft(x + g) ≥ ln(1 + βt) − ln Ft(x + 1) = ln

2

Ft(x + 1) + Ft(x − 1)

 

11

Coin Betting and Parameter-Free Online Learning

Francesco Orabona

Stony Brook University  Stony Brook  NY

D´avid P´al

Yahoo Research  New York  NY

francesco@orabona.com

dpal@yahoo-inc.com

Abstract

In the recent years  a number of parameter-free algorithms have been developed
for online linear optimization over Hilbert spaces and for learning with expert ad-
vice. These algorithms achieve optimal regret bounds that depend on the unknown
competitors  without having to tune the learning rates with oracle choices.
We present a new intuitive framework to design parameter-free algorithms for both
online linear optimization over Hilbert spaces and for learning with expert advice 
based on reductions to betting on outcomes of adversarial coins. We instantiate
it using a betting algorithm based on the Krichevsky-Troﬁmov estimator. The
resulting algorithms are simple  with no parameters to be tuned  and they improve
or match previous results in terms of regret guarantee and per-round complexity.

Introduction

1
We consider the Online Linear Optimization (OLO) [4  25] setting. In each round t  an algorithm
chooses a point wt from a convex decision set K and then receives a reward vector gt. The algo-
rithm’s goal is to keep its regret small  deﬁned as the difference between its cumulative reward and
the cumulative reward of a ﬁxed strategy u ∈ K  that is

T(cid:88)

(cid:104)gt  u(cid:105) − T(cid:88)

(cid:104)gt  wt(cid:105) .

RegretT (u) =

t=1

t=1

We focus on two particular decision sets  the N-dimensional probability simplex ∆N = {x ∈
RN : x ≥ 0 (cid:107)x(cid:107)1 = 1} and a Hilbert space H. OLO over ∆N is referred to as the problem of
Learning with Expert Advice (LEA). We assume bounds on the norms of the reward vectors: For
OLO over H  we assume that (cid:107)gt(cid:107) ≤ 1  and for LEA we assume that gt ∈ [0  1]N .
OLO is a basic building block of many machine learning problems. For example  Online Convex
Optimization (OCO)  the problem analogous to OLO where (cid:104)gt  u(cid:105) is generalized to an arbitrary
convex function (cid:96)t(u)  is solved through a reduction to OLO [25]. LEA [17  27  5] provides a
way of combining classiﬁers and it is at the heart of boosting [12]. Batch and stochastic convex
optimization can also be solved through a reduction to OLO [25].
To achieve optimal regret  most of the existing online algorithms require the user to set the learning
rate (step size) η to an unknown/oracle value. For example  to obtain the optimal bound for Online
Gradient Descent (OGD)  the learning rate has to be set with the knowledge of the norm of the
competitor u  (cid:107)u(cid:107); second entry in Table 1. Likewise  the optimal learning rate for Hedge depends on
the KL divergence between the prior weighting π and the unknown competitor u  D (u(cid:107)π); seventh
entry in Table 1. Recently  new parameter-free algorithms have been proposed  both for LEA [6  8 
18  19  15  11] and for OLO/OCO over Hilbert spaces [26  23  21  22  24]. These algorithms adapt
to the number of experts and to the norm of the optimal predictor  respectively  without the need
to tune parameters. However  their design and underlying intuition is still a challenge. Foster et al.
[11] proposed a uniﬁed framework  but it is not constructive. Furthermore  all existing algorithms for
LEA either have sub-optimal regret bound (e.g. extra O(log log T ) factor) or sub-optimal running
time (e.g. requiring solving a numerical problem in every round  or with extra factors); see Table 1.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Algorithm

T

[25]
[25]

OGD  η = 1√
OGD  η = U√
[23]
[22  24]
This paper  Sec. 7.1

T

(cid:113) ln N

T

Hedge  η =
Hedge  η = U√
[6]
[8]
[8  19  15]2
[11]
This paper  Sec. 7.2

T   πi = 1
[12]

N [12]

U
O((cid:107)u(cid:107) ln(1 + (cid:107)u(cid:107) T )

Worst-case regret guarantee
√
T )  ∀u ∈ H
O((1 + (cid:107)u(cid:107)2)
√
T for any u ∈ H s.t. (cid:107)u(cid:107) ≤ U
√
T )  ∀u ∈ H

O((cid:107)u(cid:107)(cid:112)T ln(1 + (cid:107)u(cid:107) T ))  ∀u ∈ H
O((cid:107)u(cid:107)(cid:112)T ln(1 + (cid:107)u(cid:107) T ))  ∀u ∈ H
T ) for any u ∈ ∆N s.t.(cid:112)D (u(cid:107)π) ≤ U
O((cid:112)T (1 + D (u(cid:107)π)) + ln2 N )  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (ln ln T + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N
O((cid:112)T (1 + D (u(cid:107)π)))  ∀u ∈ ∆N

T ln N )  ∀u ∈ ∆N

O(

√

√

O(U

Per-round time

complexity

Adaptive Uniﬁed
analysis

O(1)
O(1)
O(1)
O(1)
O(1)
O(N )
O(N )
O(N K)1
O(N K)1
O(N )
O(N )

O(N ln maxu∈∆N D (u(cid:107)π))3

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

Table 1: Algorithms for OLO over Hilbert space and LEA.

Contributions. We show that a more fundamental notion subsumes both OLO and LEA parameter-
free algorithms. We prove that the ability to maximize the wealth in bets on the outcomes of coin
ﬂips implies OLO and LEA parameter-free algorithms. We develop a novel potential-based frame-
work for betting algorithms. It gives intuition to previous constructions and  instantiated with the
Krichevsky-Troﬁmov estimator  provides new and elegant algorithms for OLO and LEA. The new
algorithms also have optimal worst-case guarantees on regret and time complexity; see Table 1.

2 Preliminaries

crete distributions p and q is D (p(cid:107)q) =(cid:80)

We begin by providing some deﬁnitions. The Kullback-Leibler (KL) divergence between two dis-
i pi ln (pi/qi). If p  q are real numbers in [0  1]  we denote
by D (p(cid:107)q) = p ln (p/q)+(1−p) ln ((1 − p)/(1 − q)) the KL divergence between two Bernoulli dis-
tributions with parameters p and q. We denote by H a Hilbert space  by (cid:104)· ·(cid:105) its inner product  and by
(cid:107)·(cid:107) the induced norm. We denote by (cid:107)·(cid:107)1 the 1-norm in RN . A function F : I → R+ is called loga-
rithmically convex iff f (x) = ln(F (x)) is convex. Let f : V → R ∪ {±∞}  the Fenchel conjugate
of f is f∗ : V ∗ → R∪{±∞} deﬁned on the dual vector space V ∗ by f∗(θ) = supx∈V (cid:104)θ  x(cid:105)−f (x).
A function f : V → R ∪ {+∞} is said to be proper if there exists x ∈ V such that f (x) is ﬁnite. If
f is a proper lower semi-continuous convex function then f∗ is also proper lower semi-continuous
convex and f∗∗ = f.
Coin Betting. We consider a gambler making repeated bets on the outcomes of adversarial coin
ﬂips. The gambler starts with an initial endowment  > 0. In each round t  he bets on the outcome
of a coin ﬂip gt ∈ {−1  1}  where +1 denotes heads and −1 denotes tails. We do not make any
assumption on how gt is generated  that is  it can be chosen by an adversary.
The gambler can bet any amount on either heads or tails. However  he is not allowed to borrow any
additional money. If he loses  he loses the betted amount; if he wins  he gets the betted amount back
and  in addition to that  he gets the same amount as a reward. We encode the gambler’s bet in round t
by a single number wt. The sign of wt encodes whether he is betting on heads or tails. The absolute
value encodes the betted amount. We deﬁne Wealtht as the gambler’s wealth at the end of round t
and Rewardt as the gambler’s net reward (the difference of wealth and initial endowment)  that is

Wealtht =  +

wigi

and

Rewardt = Wealtht −  .

(1)

i=1

In the following  we will also refer to a bet with βt  where βt is such that

(2)
The absolute value of βt is the fraction of the current wealth to bet  and sign of βt encodes whether
he is betting on heads or tails. The constraint that the gambler cannot borrow money implies that
βt ∈ [−1  1]. We also generalize the problem slightly by allowing the outcome of the coin ﬂip gt to
be any real number in the interval [−1  1]; wealth and reward in (1) remain exactly the same.

wt = βt Wealtht−1 .

1These algorithms require to solve a numerical problem at each step. The number K is the number of steps

needed to reach the required precision. Neither the precision nor K are calculated in these papers.

2The proof in [15] can be modiﬁed to prove a KL bound  see http://blog.wouterkoolen.info.
3A variant of the algorithm in [11] can be implemented with the stated time complexity [10].

2

t(cid:88)

3 Warm-Up: From Betting to One-Dimensional Online Linear Optimization

t=1 be its sequence of predictions on a sequence of rewards {gt}∞

reward of the algorithm after t rounds is Rewardt =(cid:80)t

In this section  we sketch how to reduce one-dimensional OLO to betting on a coin. The reasoning
for generic Hilbert spaces (Section 5) and for LEA (Section 6) will be similar. We will show that
the betting view provides a natural way for the analysis and design of online learning algorithms 
where the only design choice is the potential function of the betting algorithm (Section 4). A speciﬁc
example of coin betting potential and the resulting algorithms are in Section 7.
As a warm-up  let us consider an algorithm for OLO over one-dimensional Hilbert space R. Let
{wt}∞
t=1  gt ∈ [−1  1]. The total
i=1 giwi. Also  even if in OLO there is no
concept of “wealth”  deﬁne the wealth of the OLO algorithm as Wealtht =  + Rewardt  as in (1).
We now restrict our attention to algorithms whose predictions wt are of the form of a bet  that is
wt = βt Wealtht−1  where βt ∈ [−1  1]. We will see that the restriction on βt does not prevent us
from obtaining parameter-free algorithms with optimal bounds.
Given the above  it is immediate to see that any coin betting algorithm that  on a sequence of
coin ﬂips {gt}∞
t=1  gt ∈ [−1  1]  bets the amounts wt can be used as an OLO algorithm in a one-
dimensional Hilbert space R. But  what would be the regret of such OLO algorithms?

Assume that the betting algorithm at hand guarantees that its wealth is at least F ((cid:80)T

t=1 gt) starting

from an endowment   for a given potential function F   then

RewardT =

gtwt = WealthT −  ≥ F

−  .

(3)

gt

T(cid:88)

(cid:32) T(cid:88)

(cid:33)

t=1

t=1

(cid:125)

t=1

t=1

≥ F

− 

gt

(cid:33)

(cid:123)(cid:122)

(cid:104)gt  wt(cid:105)

(cid:32) T(cid:88)

Intuitively  if the reward is big we can expect the regret to be small. Indeed  the following lemma
converts the lower bound on the reward to an upper bound on the regret.
Lemma 1 (Reward-Regret relationship [22]). Let V  V ∗ be a pair of dual vector spaces. Let F :
V → R∪{+∞} be a proper convex lower semi-continuous function and let F ∗ : V ∗ → R∪{+∞}
be its Fenchel conjugate. Let w1  w2  . . .   wT ∈ V and g1  g2  . . .   gT ∈ V ∗. Let  ∈ R. Then 

T(cid:88)
(cid:124)
To summarize  if we have a betting algorithm that guarantees a minimum wealth of F ((cid:80)T
Applying the lemma  we get a regret upper bound: RegretT (u) ≤ F ∗(u) +  for all u ∈ H.
algorithm that is adaptive to u is equivalent to designing an algorithm that is adaptive to(cid:80)T

t=1 gt) 
it can be used to design and analyze a one-dimensional OLO algorithm. The faster the growth of
the wealth  the smaller the regret will be. Moreover  the lemma also shows that trying to design an
t=1 gt.
Also  most importantly  methods that guarantee optimal wealth for the betting scenario are already
known  see  e.g.  [4  Chapter 9]. We can just re-use them to get optimal online algorithms!

≤ F ∗(u) +  .

T(cid:88)
(cid:124)

(cid:104)gt  u − wt(cid:105)

∀u ∈ V ∗ 

if and only if

RegretT (u)

(cid:123)(cid:122)

RewardT

(cid:125)

t=1

4 Designing a Betting Algorithm: Coin Betting Potentials

For sequential betting on i.i.d. coin ﬂips  an optimal strategy has been proposed by Kelly [14].
t=1  gt ∈ {+1 −1}  are generated i.i.d. with known
The strategy assumes that the coin ﬂips {gt}∞
probability of heads. If p ∈ [0  1] is the probability of heads  the Kelly bet is to bet βt = 2p − 1 at
each round. He showed that  in the long run  this strategy will provide more wealth than betting any
other ﬁxed fraction of the current wealth [14].
For adversarial coins  Kelly betting does not make sense. With perfect knowledge of the future  the
gambler could always bet everything on the right outcome. Hence  after T rounds from an initial
endowment   the maximum wealth he would get is 2T . Instead  assume he bets the same fraction β
of its wealth at each round. Let Wealtht(β) the wealth of such strategy after t rounds. As observed

in [21]  the optimal ﬁxed fraction to bet is β∗ = ((cid:80)T
(cid:80)T
t=1 gt)/T and it gives the wealth
t=1 gt)2
t=1 gt
2T
2T

(cid:17)(cid:17) ≥  exp

WealthT (β∗) =  exp

(cid:16) ((cid:80)T

 

(4)

(cid:16) 1

T · D

(cid:17)

(cid:16)

2 +

(cid:13)(cid:13)(cid:13) 1

2

3

where the inequality follows from Pinsker’s inequality [9  Lemma 11.6.1].
However  even without knowledge of the future  it is possible to go very close to the wealth in (4).
This problem was studied by Krichevsky and Troﬁmov [16]  who proposed that after seeing the coin
should be used instead of p.

i=1 1[gi=+1]

Their estimate is commonly called KT estimator.1 The KT estimator results in the betting

ﬂips g1  g2  . . .   gt−1 the empirical estimate kt = 1/2+(cid:80)t−1
(cid:80)t−1
(cid:16)

βt = 2kt − 1 =

t

t

i=1 gi

WealthT ≥ WealthT (β∗)

√
2

T

√
= 
2

T

exp

T · D

which we call adaptive Kelly betting based on the KT estimator. It looks like an online and slightly
biased version of the oracle choice of β∗. This strategy guarantees2

(5)

(cid:16) 1

(cid:80)T

2 +

t=1 gt
2T

(cid:17)(cid:17)

(cid:13)(cid:13)(cid:13) 1

2

.

This guarantee is optimal up to constant factors [4] and mirrors the guarantee of the Kelly bet.
Here  we propose a new set of deﬁnitions that allows to generalize the strategy of adaptive Kelly
betting based on the KT estimator. For these strategies it will be possible to prove that  for any
g1  g2  . . .   gt ∈ [−1  1] 

(cid:33)

Wealtht ≥ Ft

gi

 

(6)

(cid:32) t(cid:88)

i=1

where Ft(x) is a certain function. We call such functions potentials. The betting strategy will be
determined uniquely by the potential (see (c) in the Deﬁnition 2)  and we restrict our attention to
potentials for which (6) holds. These constraints are speciﬁed in the deﬁnition below.
Deﬁnition 2 (Coin Betting Potential). Let  > 0. Let {Ft}∞
Ft : (−at  at) → R+ where at > t. The sequence {Ft}∞
potentials for initial endowment   if it satisﬁes the following three conditions:

t=0 be a sequence of functions
t=0 is called a sequence of coin betting

(a) F0(0) = .
(b) For every t ≥ 0  Ft(x) is even  logarithmically convex  strictly increasing on [0  at)  and

(c) For every t ≥ 1  every x ∈ [−(t − 1)  (t − 1)] and every g ∈ [−1  1]  (1 + gβt) Ft−1(x) ≥

limx→at Ft(x) = +∞.

Ft(x + g)  where

βt = Ft(x+1)−Ft(x−1)
Ft(x+1)+Ft(x−1) .

(7)

t=0 is called a sequence of excellent coin betting potentials for initial endow-

The sequence {Ft}∞
ment  if it satisﬁes conditions (a)–(c) and the condition (d) below.
t (x) for every x ∈ [0  at).
(d) For every t ≥ 0  Ft is twice-differentiable and satisﬁes x· F (cid:48)(cid:48)
Let’s give some intuition on this deﬁnition. First  let’s show by induction on t that (b) and (c) of the
deﬁnition together with (2) give a betting strategy that satisﬁes (6). The base case t = 0 is trivial.
At time t ≥ 1  bet wt = βt Wealtht−1 where βt is deﬁned in (7)  then

t (x) ≥ F (cid:48)

Wealtht = Wealtht−1 +wtgt = (1 + gtβt) Wealtht−1

(cid:32)t−1(cid:88)

(cid:33)

(cid:32)t−1(cid:88)

(cid:33)

(cid:32) t(cid:88)

(cid:33)

≥ (1 + gtβt)Ft−1

≥ Ft

gi

gi + gt

= Ft

gi

.

i=1

i=1

i=1

The formula for the potential-based strategy (7) might seem strange. However  it is derived—see
Theorem 8 in Appendix B—by minimizing the worst-case value of the right-hand side of the in-
equality used w.r.t. to gt in the induction proof above: Ft−1(x) ≥ Ft(x+gt)
The last point  (d)  is a technical condition that allows us to seamlessly reduce OLO over a Hilbert
space to the one-dimensional problem  characterizing the worst case direction for the reward vectors.

1+gtβt

.

1Compared to the maximum likelihood estimate
2See Appendix A for a proof. For lack of space  all the appendices are in the supplementary material.

  KT estimator shrinks slightly towards 1/2.

i=1 1[gi=+1]

t−1

(cid:80)t−1

4

possible wealth in (4) to be a good candidate. In fact  Ft(x) =  exp(cid:0)x2/(2t)(cid:1) /

Regarding the design of coin betting potentials  we expect any potential that approximates the best
t  essentially
the potential used in the parameter-free algorithms in [22  24] for OLO and in [6  18  19] for LEA 
approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B. Hence 
our framework provides intuition to previous constructions and in Section 7 we show new examples
of coin betting potentials.
In the next two sections  we presents the reductions to effortlessly solve both the generic OLO case
and LEA with a betting potential.

√

5 From Coin Betting to OLO over Hilbert Space

We deﬁne reward and wealth analogously to the one-dimensional case: Rewardt =(cid:80)t

In this section  generalizing the one-dimensional construction in Section 3  we show how to use
a sequence of excellent coin betting potentials {Ft}∞
t=0 to construct an algorithm for OLO over a
Hilbert space and how to prove a regret bound for it.
i=1(cid:104)gi  wi(cid:105)
t=0  using (7) we

and Wealtht =  + Rewardt. Given a sequence of coin betting potentials {Ft}∞
deﬁne the fraction

βt =

.

(8)

The prediction of the OLO algorithm is deﬁned similarly to the one-dimensional case  but now we
also need a direction in the Hilbert space:

i=1 gi(cid:107)−1)
i=1 gi(cid:107)−1)

Ft((cid:107)(cid:80)t−1
Ft((cid:107)(cid:80)t−1
(cid:80)t−1
(cid:13)(cid:13)(cid:13)(cid:80)t−1

i=1 gi(cid:107)+1)−Ft((cid:107)(cid:80)t−1
i=1 gi(cid:107)+1)+Ft((cid:107)(cid:80)t−1
(cid:80)t−1
(cid:13)(cid:13)(cid:13) = βt
(cid:13)(cid:13)(cid:13)(cid:80)t−1

i=1 gi
i=1 gi

i=1 gi
i=1 gi

(cid:13)(cid:13)(cid:13)

(cid:32)

t−1(cid:88)

i=1

(cid:33)

If(cid:80)t−1

wt = βt Wealtht−1

 +

(cid:104)gi  wi(cid:105)

.

(9)

i=1 gi is the zero vector  we deﬁne wt to be the zero vector as well. For this prediction strategy
we can prove the following regret guarantee  proved in Appendix C. The proof reduces the general
Hilbert case to the 1-d case  thanks to (d) in Deﬁnition 2  then it follows the reasoning of Section 3.
Theorem 3 (Regret Bound for OLO in Hilbert Spaces). Let {Ft}∞
t=0 be a sequence of excellent coin
betting potentials. Let {gt}∞
t=1 be any sequence of reward vectors in a Hilbert space H such that
(cid:107)gt(cid:107) ≤ 1 for all t. Then  the algorithm that makes prediction wt deﬁned by (9) and (8) satisﬁes

∀T ≥ 0 ∀u ∈ H

RegretT (u) ≤ F ∗

T ((cid:107)u(cid:107)) +  .

6 From Coin Betting to Learning with Expert Advice
In this section  we show how to use the algorithm for OLO over one-dimensional Hilbert space R
from Section 3—which is itself based on a coin betting strategy—to construct an algorithm for LEA.
Let N ≥ 2 be the number of experts and ∆N be the N-dimensional probability simplex. Let
π = (π1  π2  . . .   πN ) ∈ ∆N be any prior distribution. Let A be an algorithm for OLO over the
one-dimensional Hilbert space R  based on a sequence of the coin betting potentials {Ft}∞
t=0 with
initial endowment3 1. We instantiate N copies of A.
Consider any round t. Let wt i ∈ R be the prediction of the i-th copy of A. The LEA algorithm

computes(cid:98)pt = ((cid:98)pt 1 (cid:98)pt 2  . . .  (cid:98)pt N ) ∈ RN

(10)
where [x]+ = max{0  x} is the positive part of x. Then  the LEA algorithm predicts pt =
(pt 1  pt 2  . . .   pt N ) ∈ ∆N as

If (cid:107)(cid:98)pt(cid:107)1 = 0  the algorithm predicts the prior π. Then  the algorithm receives the reward vector

gt = (gt 1  gt 2  . . .   gt N ) ∈ [0  1]N . Finally  it feeds the reward to each copy of A. The reward for
3Any initial endowment  > 0 can be rescaled to 1. Instead of Ft(x) we would use Ft(x)/. The wt would

(11)

become wt/  but pt is invariant to scaling of wt. Hence  the LEA algorithm is the same regardless of .

0 + as

(cid:98)pt i = πi · [wt i]+ 
pt = (cid:98)pt(cid:107)(cid:98)pt(cid:107)1

.

5

the i-th copy of A is(cid:101)gt i ∈ [−1  1] deﬁned as

(cid:26)gt i − (cid:104)gt  pt(cid:105)

(cid:101)gt i =

if wt i > 0  
[gt i − (cid:104)gt  pt(cid:105)]+ if wt i ≤ 0 .

(12)

t

The construction above deﬁnes a LEA algorithm deﬁned by the predictions pt  based on the algo-
rithm A. We can prove the following regret bound for it.
Theorem 4 (Regret Bound for Experts). Let A be an algorithm for OLO over the one-dimensional
Hilbert space R  based on the coin betting potentials {Ft}∞
t=0 for an initial endowment of 1. Let
be the inverse of ft(x) = ln(Ft(x)) restricted to [0 ∞). Then  the regret of the LEA algorithm
f−1
with prior π ∈ ∆N that predicts at each round with pt in (11) satisﬁes
RegretT (u) ≤ f−1

The proof  in Appendix D  is based on the fact that (10)–(12) guarantee that(cid:80)N

∀T ≥ 0 ∀u ∈ ∆N

i=1 πi(cid:101)gt iwt i ≤ 0

and on a variation of the change of measure lemma used in the PAC-Bayes literature  e.g. [20].

T (D (u(cid:107)π)) .

7 Applications of the Krichevsky-Troﬁmov Estimator to OLO and LEA

(cid:16) t+1

(cid:17)·Γ
(cid:16) t+1
2 − x

2

(cid:17)

In the previous sections  we have shown that a coin betting potential with a guaranteed rapid growth
of the wealth will give good regret guarantees for OLO and LEA. Here  we show that the KT
estimator has associated an excellent coin betting potential  which we call KT potential. Then  the
optimal wealth guarantee of the KT potentials will translate to optimal parameter-free regret bounds.
The sequence of excellent coin betting potentials for an initial endowment  corresponding to the
adaptive Kelly betting strategy βt deﬁned by (5) based on the KT estimator are

2t·Γ

2 + x

2

where Γ(x) = (cid:82) ∞

t ≥ 0 

x ∈ (−t − 1  t + 1) 

0

π·t!

Ft(x) = 

(13)
tx−1e−tdt is Euler’s gamma function—see Theorem 13 in Appendix E. This
potential was used to prove regret bounds for online prediction with the logarithmic loss [16][4 
Chapter 9.7]. Theorem 13 also shows that the KT betting strategy βt as deﬁned by (5) satisﬁes (7).
This potential has the nice property that is satisﬁes the inequality in (c) of Deﬁnition 2 with equality
when gt ∈ {−1  1}  i.e. Ft(x + gt) = (1 + gtβt) Ft−1(x).
We also generalize the KT potentials to δ-shifted KT potentials  where δ ≥ 0  deﬁned as
(cid:19)

(cid:19)

2t·Γ(δ+1)·Γ

Ft(x) =

(cid:18) t+δ+1
(cid:18) δ+1

Γ

2

·Γ

2 + x

(cid:19)2·Γ(t+δ+1)

2

(cid:18) t+δ+1
2 − x

2

.

The reason for its name is that  up to a multiplicative constant  Ft is equal to the KT potential
shifted in time by δ. Theorem 13 also proves that the δ-shifted KT potentials are excellent coin
j=1 gj
.
betting potentials with initial endowment 1  and the corresponding betting fraction is βt =
δ+t

(cid:80)t−1

7.1 OLO in Hilbert Space
We apply the KT potential for the construction of an OLO algorithm over a Hilbert space H. We
will use (9)  and we just need to calculate βt. According to Theorem 13 in Appendix E  the formula
for βt simpliﬁes to βt =

i=1(cid:104)gi  wi(cid:105)(cid:17)(cid:80)t−1

 +(cid:80)t−1

(cid:107)(cid:80)t−1
i=1 gi(cid:107)

so that wt = 1
t

i=1 gi.

(cid:16)

t

The resulting algorithm is stated as Algorithm 1. We derive a regret bound for it as a very simple
corollary of Theorem 3 to the KT potential (13). The only technical part of the proof  in Appendix F 
is an upper bound on F ∗
Corollary 5 (Regret Bound for Algorithm 1). Let  > 0. Let {gt}∞
vectors in a Hilbert space H such that (cid:107)gt(cid:107) ≤ 1. Then Algorithm 1 satisﬁes
1 + 24T 2(cid:107)u(cid:107)2

t since it cannot be expressed as an elementary function.

t=1 be any sequence of reward

∀ T ≥ 0 ∀u ∈ H

RegretT (u) ≤ (cid:107)u(cid:107)

1 − 1
√

(cid:114)

(cid:17)

(cid:16)

(cid:16)

(cid:17)

T ln

+ 

.

2

e

πT

6

Algorithm 1 Algorithm for OLO over Hilbert space H based on KT potential
Require: Initial endowment  > 0
1: for t = 1  2  . . . do
2:
3:
4: end for

Predict with wt ← 1
i=1 gi
Receive reward vector gt ∈ H such that (cid:107)gt(cid:107) ≤ 1

i=1(cid:104)gi  wi(cid:105)(cid:17)(cid:80)t−1

(cid:16)
 +(cid:80)t−1

t

Algorithm 2 Algorithm for Learning with Expert Advice based on δ-shifted KT potential
Require: Number of experts N  prior distribution π ∈ ∆N   number of rounds T
1: for t = 1  2  . . .   T do
2:
3:

For each i ∈ [N ]  set wt i ←

j=1(cid:101)gj iwj i

1 +(cid:80)t−1

(cid:16)

(cid:17)

t+T /2

(cid:80)t−1
j=1(cid:101)gj i
For each i ∈ [N ]  set(cid:98)pt i ← πi[wt i]+
(cid:26)(cid:98)pt/(cid:107)(cid:98)pt(cid:107)1
if (cid:107)(cid:98)pt(cid:107)1 > 0
if (cid:107)(cid:98)pt(cid:107)1 = 0
(cid:26)gt i − (cid:104)gt  pt(cid:105)
For each i ∈ [N ]  set(cid:101)gt i ←

Predict with pt ←
Receive reward vector gt ∈ [0  1]N

π

if wt i > 0
[gt i − (cid:104)gt  pt(cid:105)]+ if wt i ≤ 0

4:

5:

6:
7: end for

It is worth noting the elegance and extreme simplicity of Algorithm 1 and contrast it with the algo-
rithms in [26  22–24]. Also  the regret bound is optimal [26  23]. The parameter  can be safely set
to any constant  e.g. 1. Its role is equivalent to the initial guess used in doubling tricks [25].

7.2 Learning with Expert Advice

We will now construct an algorithm for LEA based on the δ-shifted KT potential. We set δ to T /2 
requiring the algorithm to know the number of rounds T in advance; we will ﬁx this later with the
standard doubling trick.
To use the construction in Section 6  we need an OLO algorithm for the 1-d Hilbert space R. Using

the δ-shifted KT potentials  the algorithm predicts for any sequence {(cid:101)gt}∞
1 +
t−1(cid:88)

(cid:80)t−1
i=1(cid:101)gi

1 +

wt = βt Wealtht−1 = βt

 =

 .

t=1 of reward

t−1(cid:88)

(cid:101)gjwj

(cid:101)gjwj

j=1

T /2 + t

j=1

Then  following the construction in Section 6  we arrive at the ﬁnal algorithm  Algorithm 2. We can
derive a regret bound for Algorithm 2 by applying Theorem 4 to the δ-shifted KT potential.
Corollary 6 (Regret Bound for Algorithm 2). Let N ≥ 2 and T ≥ 0 be integers. Let π ∈ ∆N be a
prior. Then Algorithm 2 with input N  π  T for any rewards vectors g1  g2  . . .   gT ∈ [0  1]N satisﬁes

RegretT (u) ≤(cid:112)3T (3 + D (u(cid:107)π)) .

∀u ∈ ∆N

t

Hence  the Algorithm 2 has both the best known guarantee on worst-case regret and per-round time
complexity  see Table 1. Also  it has the advantage of being very simple.
The proof of the corollary is in the Appendix F. The only technical part of the proof is an upper
bound on f−1
The reason for using the shifted potential comes from the analysis of f−1

gorithm would have a O((cid:112)T (log T + D (u(cid:107)π)) regret bound; the shifting improves the bound to
O((cid:112)T (1 + D (u(cid:107)π)). By changing T /2 in Algorithm 2 to another constant fraction of T   it is pos-

(x)  which we conveniently do by lower bounding Ft(x).

(x). The unshifted al-

sible to trade-off between the two constants 3 present in the square root in the regret upper bound.
The requirement of knowing the number of rounds T in advance can be lifted by the standard dou-
bling trick [25  Section 2.3.1]  obtaining an anytime guarantee with a bigger leading constant 

t

∀ T ≥ 0 ∀u ∈ ∆N

RegretT (u) ≤ √

2√
2−1

(cid:112)3T (3 + D (u(cid:107)π)) .

7

Figure 1: Total loss versus learning rate parameter of OGD (in log scale)  compared with parameter-free
algorithms DFEG [23]  Adaptive Normal [22]  PiSTOL [24] and the KT-based Algorithm 1.

Figure 2: Regrets to the best expert after T = 32768 rounds  versus learning rate parameter of Hedge (in
log scale). The “good” experts are  = 0.025 better than the others. The competitor algorithms are Normal-
Hedge [6]  AdaNormalHedge [19]  Squint [15]  and the KT-based Algorithm 2. πi = 1/N for all algorithms.

8 Discussion of the Results

We have presented a new interpretation of parameter-free algorithms as coin betting algorithms. This
interpretation  far from being just a mathematical gimmick  reveals the common hidden structure
of previous parameter-free algorithms for both OLO and LEA and also allows the design of new
algorithms. For example  we show that the characteristic of parameter-freeness is just a consequence
of having an algorithm that guarantees the maximum reward possible. The reductions in Sections 5
and 6 are also novel and they are in a certain sense optimal. In fact  the obtained Algorithms 1 and 2
achieve the optimal worst case upper bounds on the regret  see [26  23] and [4] respectively.
We have also run an empirical evaluation to show that the theoretical difference between classic
online learning algorithms and parameter-free ones is real and not just theoretical. In Figure 1  we
have used three regression datasets4  and solved the OCO problem through OLO. In all the three
cases  we have used the absolute loss and normalized the input vectors to have L2 norm equal to 1.
From the empirical results  it is clear that the optimal learning rate is completely data-dependent  yet
parameter-free algorithms have performance very close to the unknown optimal tuning of the learn-
ing rate. Moreover  the KT-based Algorithm 1 seems to dominate all the other similar algorithms.
For LEA  we have used the synthetic setting in [6]. The dataset is composed of Hadamard matrices
of size 64  where the row with constant values is removed  the rows are duplicated to 126 inverting
their signs  0.025 is subtracted to k rows  and the matrix is replicated in order to generate T = 32768
samples. For more details  see [6]. Here  the KT-based algorithm is the one in Algorithm 2  where
the term T /2 is removed  so that the ﬁnal regret bound has an additional ln T term. Again  we
see that the parameter-free algorithms have a performance close or even better than Hedge with an
oracle tuning of the learning rate  with no clear winners among the parameter-free algorithms.
Notice that since the adaptive Kelly strategy based on KT estimator is very close to optimal  the only
possible improvement is to have a data-dependent bound  for example like the ones in [24  15  19].
In future work  we will extend our deﬁnitions and reductions to the data-dependent case.

4Datasets available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.

8

10−210−110010110233.053.13.153.23.253.33.353.43.453.5x 105UTotal lossYearPredictionMSD dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based10−11001011025.566.577.588.59x 104UTotal losscpusmall dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based1001021041061.71.751.81.851.91.9522.05x 109UTotal losscadata dataset  absolute loss OGD ηt=Up1/tDFEGAdaptiveNormalPiSTOLKT-based100101200250300350400450Replicated Hadamard matrices  N=126  k=2 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-based100101180200220240260280300320340360380400Replicated Hadamard matrices  N=126  k=8 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-based100101150200250300350Replicated Hadamard matrices  N=126  k=32 good expertsURegret to best expert after T=32768 Hedge ηt=Up1/tNormalHedgeAdaNormalHedgeSquintKT-basedAcknowledgments. The authors thank Jacob Abernethy  Nicol`o Cesa-Bianchi  Satyen Kale  Chan-
soo Lee  Giuseppe Molteni  and Manfred Warmuth for useful discussions on this work.

References

[1] E. Artin. The Gamma Function. Holt  Rinehart and Winston  Inc.  1964.
[2] N. Batir. Inequalities for the gamma function. Archiv der Mathematik  91(6):554–563  2008.
[3] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces.

Springer Publishing Company  Incorporated  1st edition  2011.

[4] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
[5] N. Cesa-Bianchi  Y. Freund  D. Haussler  D. P. Helmbold  R. E. Schapire  and M. K. Warmuth. How to

use expert advice. J. ACM  44(3):427–485  1997.

[6] K. Chaudhuri  Y. Freund  and D. Hsu. A parameter-free hedging algorithm.

Information Processing Systems 22  pages 297–305  2009.

In Advances in Neural

[7] C.-P. Chen.
65–72  2005.

Inequalities for the polygamma functions with application. General Mathematics  13(3):

[8] A. Chernov and V. Vovk. Prediction with advice of unknown number of experts. In Proc. of the 26th

Conf. on Uncertainty in Artiﬁcial Intelligence. AUAI Press  2010.

[9] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons  2nd edition  2006.
[10] D. J. Foster. personal communication  2016.
[11] D. J. Foster  A. Rakhlin  and K. Sridharan. Adaptive online learning. In Advances in Neural Information

Processing Systems 28  pages 3375–3383. Curran Associates  Inc.  2015.

[12] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application

to boosting. J. Computer and System Sciences  55(1):119–139  1997.

[13] A. Hoorfar and M. Hassani. Inequalities on the Lambert W function and hyperpower function. J. Inequal.

Pure and Appl. Math  9(2)  2008.

[14] J. L. Kelly. A new interpretation of information rate. Information Theory  IRE Trans. on  2(3):185–189 

September 1956.

[15] W. M. Koolen and T. van Erven. Second-order quantile methods for experts and combinatorial games. In

Proc. of the 28th Conf. on Learning Theory  pages 1155–1175  2015.

[16] R. E. Krichevsky and V. K. Troﬁmov. The performance of universal encoding. IEEE Trans. on Information

Theory  27(2):199–206  1981.

[17] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation  108

(2):212–261  1994.

[18] H. Luo and R. E. Schapire. A drifting-games analysis for online learning and applications to boosting. In

Advances in Neural Information Processing Systems 27  pages 1368–1376  2014.

[19] H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proc. of the 28th

Conf. on Learning Theory  pages 1286–1304  2015.

[20] D. McAllester. A PAC-Bayesian tutorial with a dropout bound  2013. arXiv:1307.2118.
[21] H. B. McMahan and J. Abernethy. Minimax optimal algorithms for unconstrained linear optimization. In

Advances in Neural Information Processing Systems 26  pages 2724–2732  2013.

[22] H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax al-
gorithms and normal approximations. In Proc. of the 27th Conf. on Learning Theory  pages 1020–1039 
2014.

[23] F. Orabona. Dimension-free exponentiated gradient.

In Advances in Neural Information Processing

Systems 26 (NIPS 2013)  pages 1806–1814. Curran Associates  Inc.  2013.

[24] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning.

In Advances in Neural Information Processing Systems 27 (NIPS 2014)  pages 1116–1124  2014.

[25] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine

Learning  4(2):107–194  2011.

[26] M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimization. In

Advances in Neural Information Processing Systems 25 (NIPS 2012)  pages 2402–2410  2012.

[27] V. Vovk. A game of prediction with expert advice. J. Computer and System Sciences  56:153–173  1998.
[28] E. T. Whittaker and G. N. Watson. A Course of Modern Analysis. Cambridge University Press  fourth

edition  1962. Reprinted.

[29] F. M. J. Willems  Y. M. Shtarkov  and T. J. Tjalkens. The context tree weighting method: Basic properties.

IEEE Trans. on Information Theory  41:653–664  1995.

9

A From Log Loss to Wealth

Guarantees for betting or sequential investement algorithm are often expressed as upper bounds on
the regret with respect to the log loss. Here  for the sake of completeness  we show how to convert
such a guarantee to a lower bound on the wealth of the corresponding betting algorithm.
We consider the problem of predicting a binary outcome. The algorithm predicts at each round
probability pt ∈ [0  1]. The adversary generates a sequences of outcomes xt ∈ {0  1} and the
algorithm’s loss is

(cid:96)(pt  xt) = −xt ln pt − (1 − xt) ln(1 − pt) .

We deﬁne the regret with respect to a ﬁxed probability vector β as

Regretlogloss

T

=

(cid:96)(pt  xt) − min
β∈[0 1]

(cid:96)(β  xt) .

Lemma 7. Assume that an algorithm that predicts pt guarantees Regretlogloss
coin betting strategy with endowement  and βt = 2pt − 1 guarantees

T

(cid:32)

(cid:80)T

1
2

+

t=1 gt
2T

− RT

≤ RT . Then  the

(cid:33)

t=1

T(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

2

(cid:33)

T(cid:88)

t=1

(cid:32)

T · D
against any sequence of outcomes gt ∈ [−1  +1].

WealthT ≥  exp

Proof. Deﬁne xt = 1+gt

2 . We have

ln WealthT = ln(Wealtht−1 +wtgt)

= ln(Wealtht−1(1 + gtβt))

t=1

T(cid:89)
T(cid:88)
T(cid:88)
T(cid:88)

t=1

= ln 

(1 + gtβt)

ln (1 + βt) +

ln (2pt) +

(cid:19)

2

t=1

= ln  +

ln(1 + gtβt)

≥ ln  +

(cid:18) 1 + gt
(cid:19)
(cid:18) 1 + gt
(cid:19)
(cid:18) 1 + gt
T(cid:88)
= ln  + T ln(2) − T(cid:88)

= ln  + T ln(2) +

= ln  +

(cid:96)(pt  xt)

t=1

t=1

2

2

t=1

= ln  + T ln(2) − Regretlogloss

T

≥ ln  + T ln(2) − RT − min
β∈[0 1]

2

(cid:19)

(cid:18) 1 − gt
(cid:18) 1 − gt
(cid:19)
(cid:18) 1 − gt

2

ln(pt) +

2

(cid:19)

ln (1 − βt)

ln (2(1 − pt))

ln(1 − pt)

T(cid:88)

t=1

(cid:96)(β  xt)

− min
β∈[0 1]

T(cid:88)

t=1

(cid:96)(β  xt)  

where the ﬁrst inequality is due to the concavity of ln and the second one is due to the assumption
of the regret.
It is easy to see that the β∗ = arg minβ∈[0 1]

. Hence  we have

(cid:80)T

t=1 (cid:96)(β  xt) =

(cid:80)T

t=1 xt
T

T(cid:88)

min
β∈[0 1]

t=1

10

(cid:96)(β  xt) = T (−β∗ ln β∗ − (1 − β∗) ln(1 − β∗)) .

Also  we have that for any β ∈ [0  1]

−β ln β − (1 − β) ln(1 − β) = − D

(cid:18)

β

(cid:19)

(cid:13)(cid:13)(cid:13)(cid:13) 1

2

+ ln 2 .

Putting all together  we have the stated lemma.

The lower bound on the wealth of the adaptive Kelly betting based on the KT estimator is obtained
simply by the stated Lemma and reminding that the log loss regret of the KT estimator is upper
bounded by 1

2 ln T + ln 2.

B Optimal Betting Fraction
Theorem 8 (Optimal Betting Fraction). Let x ∈ R. Let F : [x− 1  x + 1] → R be a logarithmically
convex function. Then 

F (x + g)

1 + βg

=

F (x + 1) − F (x − 1)
F (x + 1) + F (x − 1)

.

max
g∈[−1 1]

arg min
β∈(−1 1)
Moreover  β∗ = F (x+1)−F (x−1)

F (x+1)+F (x−1) satisﬁes

ln(F (x + 1)) − ln(1 + β∗) = ln(F (x − 1)) − ln(1 − β∗) .

Proof. We deﬁne the functions h  f : [−1  1] × (−1  1) → R as

h(g  β) =

F (x + g)

1 + βg

and

f (g  β) = ln(h(g  β)) = ln(F (x + g)) − ln(1 + βg) .

Clearly  arg minβ∈(−1 1) maxg∈[−1 1] h(g  β) = arg minβ∈(−1 1) maxg∈[−1 1] f (g  β) and we can
work with f instead of h. The function h is logarithmically convex in g and thus f is convex in g.
Therefore 

∀β ∈ (−1  1)

f (g  β) = max{f (+1  β)  f (−1  β)} .

max
g∈[−1 1]

Let φ(β) = max{f (+1  β)  f (−1  β)}. We seek to ﬁnd the arg minβ∈(−1 1) φ(β). Since f (+1  β)
is decreasing in β and f (−1  β) is increasing in β  the minimum of φ(β) is at a point β∗ such that
f (+1  β∗) = f (−1  β∗). In other words  β∗ satisﬁes

ln(F (x + 1)) − ln(1 + β∗) = ln(F (x − 1)) − ln(1 − β∗) .

The only solution of this equation is

β∗ =

F (x + 1) − F (x − 1)
F (x + 1) + F (x − 1)

.

Theorem 9. The functions Ft(x) =  exp( x2

2t − 1

2

(cid:80)t

i=1

1

i ) are excellent coin betting potentials.

Proof. The ﬁrst and second properties of Deﬁnition 2 are trivially true. For the third property  we
ﬁrst use Theorem 8 to have

ln(1 + βtg) − ln Ft(x + g) ≥ ln(1 + βt) − ln Ft(x + 1) = ln

2

Ft(x + 1) + Ft(x − 1)

 

11

,Francesco Orabona
David Pal