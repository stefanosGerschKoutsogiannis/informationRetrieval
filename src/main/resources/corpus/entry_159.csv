2010,A POMDP Extension with Belief-dependent Rewards,Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately  some problems cannot be modeled with state-dependent reward functions  e.g.  problems whose objective explicitly implies reducing the uncertainty on the state. To that end  we introduce rho-POMDPs  an extension of POMDPs where the reward function rho depends on the belief state. We show that  under the common assumption that rho is convex  the value function is also convex  what makes it possible to (1) approximate rho arbitrarily well with a piecewise linear and convex (PWLC) function  and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.,A POMDP Extension with Belief-dependent Rewards

Mauricio Araya-L´opez

Olivier Buffet

Vincent Thomas

Franc¸ois Charpillet

LORIA – Campus Scientiﬁque – BP 239

54506 Vandoeuvre-l`es-Nancy Cedex – France

Nancy Universit´e / INRIA

firstname.lastname@loria.fr

Abstract

Partially Observable Markov Decision Processes (POMDPs) model sequential
decision-making problems under uncertainty and partial observability. Unfortu-
nately  some problems cannot be modeled with state-dependent reward functions 
e.g.  problems whose objective explicitly implies reducing the uncertainty on the
state. To that end  we introduce ρPOMDPs  an extension of POMDPs where the
reward function ρ depends on the belief state. We show that  under the com-
mon assumption that ρ is convex  the value function is also convex  what makes
it possible to (1) approximate ρ arbitrarily well with a piecewise linear and con-
vex (PWLC) function  and (2) use state-of-the-art exact or approximate solving
algorithms with limited changes.

1

Introduction

Sequential decision-making problems under uncertainty and partial observability are typically mod-
eled using Partially Observable Markov Decision Processes (POMDPs) [1]  where the objective is
to decide how to act so that the sequence of visited states optimizes some performance criterion.
However  this formalism is not expressive enough to model problems with any kind of objective
functions.
Let us consider active sensing problems  where the objective is to act so as to acquire knowledge
about certain state variables. Medical diagnosis for example is about asking the good questions and
performing the appropriate exams so as to diagnose a patient at a low cost and with high certainty.
This can be formalized as a POMDP by rewarding—if successful—a ﬁnal action consisting in ex-
pressing the diagnoser’s “best guess”. Actually  a large body of work formalizes active sensing with
POMDPs [2  3  4].
An issue is that  in some problems  the objective needs to be directly expressed in terms of the
uncertainty/information on the state  e.g.  to minimize the entropy over a given state variable. In such
cases  POMDPs are not appropriate because the reward function depends on the state and the action 
not on the knowledge of the agent. Instead  we need a model where the instant reward depends on
the current belief state. The belief MDP formalism provides the needed expressiveness for these
problems. Yet  there is not much research on speciﬁc algorithms to solve them  so they are usually
forced to ﬁt in the POMDP framework  which means changing the original problem deﬁnition. One
can argue that acquiring information is always a means  not an end  and thus  a “well-deﬁned”
sequential-decision making problem with partial observability must always be modeled as a normal
POMDP. However  in a number of cases the problem designer has decided to separate the task of
looking for information from that of exploiting information. Let us mention two examples: (i) the

1

surveillance [5] and (ii) the exploration [2] of a given area  in both cases when one does not know
what to expect from these tasks—and thus how to react to the discoveries.
After reviewing some background knowledge on POMDPs in Section 2  Section 3 introduces
ρPOMDPs—an extension of POMDPs where the reward is a (typically convex) function of the
belief state—and proves that the convexity of the value function is preserved. Then we show how
classical solving algorithms can be adapted depending whether the reward function is piecewise
linear (Sec. 3.3) or not (Sec. 4).

2 Partially Observable MDPs

The general problem that POMDPs address is for the agent to ﬁnd a decision policy π choosing 
at each time step  the best action based on its past observations and actions in order to maximize
its future gain (which can be measured for example through the total accumulated reward or the
average reward per time step). Compared to classical deterministic planning  the agent has to face
the difﬁculty to account for a system not only with uncertain dynamics but also whose current state
is imperfectly known.

2.1 POMDP Description
Formally  POMDPs are deﬁned by a tuple (cid:104)S A  Ω  T  O  r  b0(cid:105) where  at any time step  the system
being in some state s ∈ S (the state space)  the agent performs an action a ∈ A (the action
space) that results in (1) a transition to a state s(cid:48) according to the transition function T (s  a  s(cid:48)) =
P r(s(cid:48)|s  a)  (2) an observation o ∈ Ω (the observation space) according to the observation function
O(s(cid:48)  a  o) = P r(o|s(cid:48)  a)  and (3) a scalar reward r(s  a). b0 is the initial probability distribution
over states. Unless stated otherwise  the state  action and observation sets are ﬁnite [6].
The agent can typically reason about the state of the system by computing a belief state b ∈ ∆ =
Π(S) (the set of probability distributions over S) 1 using the following update formula (based on the
Bayes rule) when performing action a and observing o:

where P r(o|a  b) = (cid:80)

ba o(s(cid:48)) =

O(s(cid:48)  a  o)
P r(o|a  b)

T (s  a  s(cid:48))b(s) 

s s(cid:48)(cid:48)∈S

O(s(cid:48)(cid:48)  a  o)T (s  a  s(cid:48)(cid:48))b(s). Using belief states  a POMDP can be
rewritten as an MDP over the belief space  or belief MDP  (cid:104)∆ A  τ  ρ(cid:105)  where the new transition
τ and reward functions ρ are deﬁned respectively over ∆ × A × ∆ and ∆ × A. With this refor-
mulation  a number of theoretical results about MDPs can be extended  such as the existence of a
deterministic policy that is optimal. An issue is that  even if a POMDP has a ﬁnite number of states 
the corresponding belief MDP is deﬁned over a continuous—and thus inﬁnite—belief space.
In this continuous MDP  the objective is to maximize the cumulative reward by looking for a policy
taking the current belief state as input. More formally  we are searching for a policy verifying

π∗ = argmaxπ∈A∆ J π(b0) where J π(b0) = E [(cid:80)∞t=0 γρt|b0  π]  ρt being the expected immediate

reward obtained at time step t  and γ a discount factor. Bellman’s principle of optimality [7] lets us
compute the function J π

recursively through the value function

∗

(cid:88)

s∈S

ρ(b  a) + γ
(cid:34)

ρ(b  a) + γ

(cid:90)
(cid:88)

b(cid:48)∈∆

o

Vn(b) = max
a∈A

= max
a∈A


(cid:35)

τ (b  a  b(cid:48))Vn−1(b(cid:48))db(cid:48)

P r(o|a  b)Vn−1(ba o)

 

(1)

where  for all b ∈ ∆  V0(b) = 0  and J π
horizon of the problem).
The POMDP framework presents a reward function r(s  a) based on the state and action. On the
other hand  the belief MDP presents a reward function ρ(b  a) based on beliefs. This belief-based

(b) = Vn=H (b) (where H is the—possibly inﬁnite—

∗

1Π(S) forms a simplex because (cid:107)b(cid:107)1 = 1  that is why we use ∆ as the set of all possible b.

2

(cid:88)

(cid:88)

s(cid:48)

(cid:88)

(cid:88)

(cid:34)

(cid:35)

 

reward function is derived as the expectation of the POMDP rewards:

ρ(b  a) =

b(s)r(s  a).

(2)

s

An important consequence of Equation 2 is that the recursive computation described in Eq. 1 has
the property to generate piecewise-linear and convex (PWLC) value functions for each horizon [1] 
i.e.  each function is determined by a set of hyperplanes (each represented by a vector)  the value
at a given belief point being that of the highest hyperplane. For example  if Γn is the set of vectors
representing the value function for horizon n  then Vn(b) = maxα∈Γn
2.2 Solving POMDPs with Exact Updates

s b(s)α(s).

(cid:80)

Using the PWLC property  one can perform the Bellman update using the following factorization of
Eq. 1:

r(s  a)
|Ω| +

o

s

b(s)

T (s  a  s(cid:48))O(s(cid:48)  a  o)χn−1(ba o  s(cid:48))

b · α. If we consider the term in brackets in Eq. 3  this generates |Ω| × |A|

Vn(b) = max
a∈A
with2 χn(b) = argmax
(cid:26) ra
α∈Γn
Γ-sets  each one of size |Γn−1|. These sets are deﬁned as
|Ω| + P a o · αn−1
representation of the value function  one can compute ((cid:76) being the cross-sum between two sets):
(cid:77)

where P a o(s  s(cid:48)) = T (s  a  s(cid:48))O(s(cid:48)  a  o) and ra(s) = r(s  a). Therefore  for obtaining an exact

(cid:12)(cid:12)(cid:12)(cid:12) αn−1 ∈ Γn−1

(cid:91)

(cid:27)

(4)

a o

Γn

=

 

(3)

Γn =

a o

.

Γn

a

o

a o sets—and also the ﬁnal Γn—are non-parsimonious: some α-vectors may be use-
Yet  these Γn
less because the corresponding hyperplanes are below the value function. Pruning phases are then
required to remove dominated vectors. There are several algorithms based on pruning techniques
like Batch Enumeration [8] or more efﬁcient algorithms such as Witness or Incremental Pruning [6].

2.3 Solving POMDPs with Approximate Updates

The value function updating processes presented above are exact and provide value functions that
can be used whatever the initial belief state b0. A number of approximate POMDP solutions have
been proposed to reduce the complexity of these computations  using for example heuristic estimates
of the value function  or applying the value update only on selected belief points [9]. We focus here
on the latter point-based (PB) approximations  which have largely contributed to the recent progress
in solving POMDPs  and whose relevant literature goes from Lovejoy’s early work [10] via Pineau
et al.’s PBVI [11]  Spaan and Vlassis’ Perseus [12]  Smith and Simmons’ HSVI2 [13]  through to
Kurniawati et al.’s SARSOP [14].
At each iteration n until convergence  a typical PB algorithm:

1. selects a new set of belief points Bn based on Bn−1 and the current approximation Vn−1;
2. performs a Bellman backup at each belief point b ∈ Bn  resulting in one α-vector per point;
3. prunes points whose associated hyperplanes are dominated or considered negligible.

The various PB algorithms differ mainly in how belief points are selected  and in how the update
is performed. Existing belief point selection methods have exploited ideas like using a regular
discretization or a random sampling of the belief simplex  picking reachable points (by simulating
action sequences starting from b0)  adding points that reduce the approximation error  or looking in
particular at regions relevant to the optimal policy [15].

2The χ function returns a vector  so χn(b  s) = (χn(b))(s).

3

3 POMDP extension for Active Sensing

3.1

Introducing ρPOMDPs

All problems with partial observability confront the issue of getting more information to achieve
some goal. This problem is usually implicitly addressed in the resolution process  where acquiring
information is only a means for optimizing an expected reward based on the system state. Some
active sensing problems can be modeled this way (e.g. active classiﬁcation)  but not all of them. A
special kind of problem is when the performance criterion incorporates an explicit measure of the
agent’s knowledge about the system  which is based on the beliefs rather than states. Surveillance
for example is a never-ending task that does not seem to allow for a modeling with state-dependent
rewards. Indeed  if we consider the simple problem of knowing the position of a hidden object  it
is possible to solve this without even having seen the object (for instance if all the locations but one
have been visited). However  the reward of a POMDP cannot model this since it is only based on
the current state and action. One solution would be to include the whole history in the state  leading
to a combinatorial explosion. We prefer to consider a new way of deﬁning rewards based on the
acquired knowledge represented by belief states. The rest of the paper explores the fact that belief
MDPs can be used outside the speciﬁc deﬁnition of ρ(b  a) in Eq. 2  and therefore discusses how to
solve this special type of active sensing problems.
As Eq. 2 is no longer valid  the direct link with POMDPs is broken. We can however still use
all the other components of POMDPs such as states  observations  etc. A way of ﬁxing this is
to generalize the POMDP framework to a ρ-based POMDP (ρPOMDP)  where the reward is not
deﬁned as a function r(s  a)  but directly as a function ρ(b  a). The nature of the ρ(b  a) function
depends on the problem  but is usually related to some uncertainty or error measure [3  2  4]. Most
common methods are those based on Shannon’s information theory  in particular Shannon’s entropy
(cid:80)
or the Kullback-Leibler distance [16]. In order to present these functions as rewards  they have to
measure information rather than uncertainty  so the negative entropy function ρent(b) = log2(|S|) +
s∈S b(s) log2(b(s))—which is maximal in the corners of the simplex and minimal in the center—
is used rather than Shannon’s original entropy. Also  other simpler functions based on the same idea
can be used  such as the distance from the simplex center (DSC)  ρdsc(b) = (cid:107)b − c(cid:107)m  where c is
the center of the simplex and m a positive integer that denotes the order of the metric space. Please
note that ρ(b  a) is not restricted to be only an uncertainty measurement  but can be a combination
of the expected state-action rewards—as in Eq. 2—and an uncertainty or error measurement. For
example  Mihaylova et al.’s work [3] deﬁnes the active sensing problem as optimizing a weighted
sum of uncertainty measurements and costs  where the former depends on the belief and the latter
on the system state.
In the remainder of this paper  we show how to apply classical POMDP algorithms to ρPOMDPs. To
that end  we discuss the convexity of the value function  which permits extending these algorithms
using PWLC approximations.

3.2 Convexity Property

An important property used to solve normal POMDPs is the result that a belief-based value function
is convex  because r(s  a) is linear with respect to the belief  and the expectation  sum and max
operators preserve this property [1]. For ρPOMDPs  this property also holds if the reward function
ρ(b  a) is convex  as shown in Theorem 3.1.
Theorem 3.1. If ρ and V0 are convex functions over ∆  then the value function Vn of the belief MDP
is convex over ∆ at any time step n. [Proof in [17  Appendix]]

This last theorem is based on ρ(b  a) being a convex function over b  which is a natural property for
uncertainty (or information) measures  because the objective is to avoid belief distributions that do
not give much information on which state the system is in  and to assign higher rewards to those
beliefs that give higher probabilities of being in a speciﬁc state. Thus  a reward function meant to
reduce the uncertainty must provide high payloads near the corners of the simplex  and low payloads
near its center. For that reason  we will focus only on reward functions that comply with convexity
in the rest of the paper.
The initial value function V0 might be any convex function for inﬁnite-horizon problems  but by

4

ρ(b  a) = max
α∈Γa
ρ

b(s)α(s)

.

(cid:35)

(cid:34)(cid:88)
(cid:88)

s

(cid:88)

(cid:34)

(cid:88)

(cid:35)

 

deﬁnition V0 = 0 for ﬁnite-horizon problems. We will use the latter case for the rest of the paper 
to provide fairly general results for both kinds of problems. Plus  starting with V0 = 0  it is also
easy to prove by induction that  if ρ is continuous (respectively differentiable)  then Vn is continuous
(respectively piecewise differentiable).

3.3 Piecewise Linear Reward Functions

This section focuses on the case where ρ is a PWLC function and shows that only a small adaptation
of the exact and approximate updates in the POMDP case is necessary to compute the optimal value
function. The complex case where ρ is not PWLC is left for Sec. 4.

3.3.1 Exact Updates

From now on  ρ(b  a)  being a PWLC function  can be represented as several Γ-sets  one Γa
a. The reward is computed as:

ρ for each

Using this deﬁnition leads to the following changes in Eq. 3

Vn(b) = max
a∈A

s
ρ(b  s) = argmax

χa

ρ(b  s) +

b(s)
(b · α). This uses the Γ-set Γa

T (s  a  s(cid:48))O(s(cid:48)  a  o)χn−1(ba o  s(cid:48))
ρ and generates |Ω| × |A| Γ-sets:

s(cid:48)

o

where χa

α∈Γa

ρ

a o

Γn

= {P a o · αn−1| αn−1 ∈ Γn−1} 

where P a o(s  s(cid:48)) = T (s  a  s(cid:48))O(s(cid:48)  a  o).
Exact algorithms like Value Iteration or Incremental Pruning can then be applied to this POMDP
extension in a similar way as for POMDPs. The difference is that the cross-sum includes not only
one αa o for each observation Γ-set Γn
ρ corresponding to the
reward:

a o  but also one αρ from the Γ-set Γa

(cid:35)

(cid:34)(cid:77)

(cid:91)

Γn =

a o ⊕ Γa

ρ

Γn

.

Thus  the cross-sum generates |R| times more vectors than with a classic POMDP  |R| being the
number of α-vectors specifying the ρ(b  a) function3.

a

o

3.3.2 Approximate Updates

Point-based approximations can be applied in the same way as PBVI or SARSOP do to the original
POMDP update. The only difference is again the reward function representation as an envelope of
hyperplanes. PB algorithms select the hyperplane that maximizes the value function at each belief
point  so the same simpliﬁcation can be applied to the set Γa
ρ.

4 Generalizing to Other Reward Functions
Uncertainty measurements such as the negative entropy or the DSC (with m > 1 and m (cid:54)= ∞) are
not piecewise linear functions. In theory  each step of value iteration can be analytically computed
using these functions  but the expressions are not closed as in the linear case  growing in complexity
and making them unmanageable after a few steps. Moreover  pruning techniques cannot be applied
directly to the resulting hypersurfaces  and even second order measures do not exhibit standard
quadratic forms to apply quadratic programming. However  convex functions can be efﬁciently
approximated by piecewise linear functions  making it possible to apply the techniques described in
Section 3.3 with a bounded error  as long as the approximation of ρ is bounded.

3More precisely  the number |R| depends on the considered action.

5

4.1 Approximating ρ

Consider a continuous  convex and piecewise differentiable reward function ρ(b) 4 and an arbitrary
(and ﬁnite) set of points B ⊂ ∆ where the gradient is well deﬁned. A lower PWLC approximation
of ρ(b) can be obtained by using each element b(cid:48) ∈ B as a base point for constructing a tangent
hyperplane which is always a lower bound of ρ(b). Concretely  ωb(cid:48)(b) = ρ(b(cid:48)) + (b − b(cid:48)) · ∇ρ(b(cid:48))
is the linear function that represents the tangent hyperplane. Then  the approximation of ρ(b) using
a set B is deﬁned as ωB(b) = maxb(cid:48)(ωb(cid:48)(b)).
At any point b ∈ ∆ the error of the approximation can be written as

B(b) = |ρ(b) − ωB(b)| 

(5)
and if we speciﬁcally pick b as the point where B(b) is maximal (worst error)  then we can try to
bound this error depending on the nature of ρ.
It is well known that a piecewise linear approximation of a Lipschitz function is bounded because
the gradient ∇ρ(b(cid:48)) that is used to construct the hyperplane ωb(cid:48)(b) has bounded norm [18]. Unfor-
tunately  the negative entropy is not Lipschitz (f (x) = x log2(x) has an inﬁnite slope when x → 0) 
so this result is not generic enough to cover a wide range of active sensing problems. Yet  under
certain mild assumptions a proper error bound can still be found.
The aim of the rest of this section is to ﬁnd an error bound in three steps. First  we will introduce
some basic results over the simplex and the convexity of ρ. Informally  Lemma 4.1 will show that 
for each b  it is possible to ﬁnd a belief point in B far enough from the boundary of the simplex but
within a bounded distance to b. Then  in a second step  we will assume the function ρ(b) veriﬁes
the α-H¨older condition to be able to bound the norm of the gradient in Lemma 4.2. In the end 
Theorem 4.3 will use both lemmas to bound the error of ρ’s approximation under these assumptions.

Figure 1: Simplices ∆ and ∆ε  and the points b  b(cid:48) and b(cid:48)(cid:48).

For each point b ∈ ∆  it is possible to associate a point b∗ = argmaxx∈B ωx(b) corresponding to
the point in B whose tangent hyperplane gives the best approximation of ρ at b. Consider the point
b ∈ ∆ where B(b) is maximum: this error can be easily computed using the gradient ∇ρ(b∗).
Unfortunately  some partial derivatives of ρ may diverge to inﬁnity on the boundary of the simplex
intermediate point b(cid:48) in an inner simplex ∆ε  where ∆ε = {b ∈ [ε  1]N|(cid:80)
in the non-Lipschitz case  making the error hard to analyze. Therefore  to ensure that this error can
be bounded  instead of b∗  we will take a safe b(cid:48)(cid:48) ∈ B (far enough from the boundary) by using an
i bi = 1} with N = |S|.
Thus  for a given b ∈ ∆ and ε ∈ (0  1
]  we deﬁne the point b(cid:48) = argminx∈∆ε (cid:107)x − b(cid:107)1 as the
closest point to b in ∆ε and b(cid:48)(cid:48) = argminx∈B (cid:107)x− b(cid:48)(cid:107)1 as the closest point to b(cid:48) in B (see Figure 1).
N
These two points will be used to ﬁnd an upper bound for the distance (cid:107)b− b(cid:48)(cid:48)(cid:107)1 based on the density
(cid:107)b − b(cid:48)(cid:107)1.
of B  deﬁned as δB = min
b∈∆
Lemma 4.1. The distance (1-norm) between the maximum error point b ∈ ∆ and the selected
b(cid:48)(cid:48) ∈ B is bounded by (cid:107)b − b(cid:48)(cid:48)(cid:107)1 ≤ 2(N − 1)ε + δB. [Proof in [17  Appendix]]
If we pick ε > δB  then we are sure that b(cid:48)(cid:48) is not on the boundary of the simplex ∆  with a
minimum distance from the boundary of η = ε − δB. This will allow ﬁnding bounds for the PWLC

max
b(cid:48)∈B

4For convenience—and without loss of generality—we only consider the case where ρ(b  a) = ρ(b).

6

∆∆εbb′b”εε′approximation of convex α-H¨older functions  which is a broader family of functions including the
negative entropy  convex Lipschitz functions and others. The α-H¨older condition is a generalization
of the Lipschitz condition. In our setting it means  for a function f : D (cid:55)→ R with D ⊂ Rn  that it
complies with

∃α ∈ (0  1]  ∃Kα > 0  s.t. |f (x) − f (y)| ≤ Kα(cid:107)x − y(cid:107)α
1 .

The limit case  where a convex α-H¨older function has inﬁnite-valued norm for the gradient  is always
on the boundary of the simplex ∆ (due to the convexity)  and therefore the point b(cid:48)(cid:48) will be free of
this predicament because of η. More precisely  an α-H¨older function in ∆ with constant Kα in
1-norm complies with the Lipschitz condition on ∆η with a constant Kαηα (see [17  Appendix]).
Moreover  the norm of the gradient (cid:107)∇f (b(cid:48)(cid:48))(cid:107)1 is also bounded as stated by Lemma 4.2.
Lemma 4.2. Let η > 0 and f be an α-H¨older (with constant Kα)  bounded and convex function
from ∆ to R  f being differentiable everywhere in ∆o (the interior of ∆). Then  for all b ∈ ∆η 
(cid:107)∇f (b)(cid:107)1 ≤ Kαηα−1. [Proof in [17  Appendix]]
Under these conditions  we can show that the PWLC approximation is bounded.
Theorem 4.3. Let ρ be a continuous and convex function over ∆  differentiable everywhere in
∆o (the interior of ∆)  and satisfying the α-H¨older condition with constant Kα. The error of an
b   where C is a scalar constant. [Proof in [17  Appendix]]
approximation ωB can be bounded by Cδα

4.2 Exact Updates

Knowing that the approximation of ρ is bounded for a wide family of functions  the techniques
described in Sec. 3.3.1 can be directly applied using ωB(b) as the PWLC reward function. These
algorithms can be safely used because the propagation of the error due to exact updates is bounded.
This can be proven using a similar methodology as in [11  10]. Let Vt be the value function using the
PWLC approximation described above and V ∗t
the optimal value function both at time t  H being
the exact update operator and ˆH the same operator with the PWLC approximation. Then  the error
from the real value function is
(cid:107)Vt − V ∗t (cid:107)

∞ + (cid:107)HVt−1 − HV ∗t−1(cid:107)

∞ = (cid:107) ˆHVt−1 − HV ∗t−1(cid:107)
∞
≤ (cid:107) ˆHVt−1 − HVt−1(cid:107)
≤ |ωb∗ + αb∗ · b − ρ(b) − αb∗ · b| + (cid:107)HVt−1 − HV ∗t−1(cid:107)
≤ Cδα
B + (cid:107)HVt−1 − HV ∗t−1(cid:107)
≤ Cδα
B + γ(cid:107)Vt−1 − V ∗t−1(cid:107)
≤ Cδα
1 − γ

∞

∞

B

(By deﬁnition)
(By triangular inequality)
∞ (Maximum error at b)
(By Theorem 4.3)
(By contraction)

(By sum of a geometric series)

For these algorithms  the selection of the set B remains open  raising similar issues as the selection
of belief points in PB algorithms.

4.3 Approximate Updates

In the case of PB algorithms  the extension is also straightforward  and the algorithms described in
Sec. 3.3.2 can be used with a bounded error. The selection of B  the set of points for the PWLC
approximation  and the set of points for the algorithm  can be shared5. This simpliﬁes the study of
the bound when using both approximation techniques at the same time. Let ˆVt be the value function
at time t calculated using the PWLC approximation and a PB algorithm. Then the error between ˆVt
is (cid:107) ˆVt − V ∗t (cid:107)
and V ∗t
∞. The second term is the same as in Sec. 4.2 
∞
so it is bounded by Cδα
1−γ . The ﬁrst term can be bounded by the same reasoning as in [11]  where
(cid:107) ˆVt − Vt(cid:107)
  with Rmin and Rmax the minimum and maximum values for

∞ + (cid:107)Vt − V ∗t (cid:107)

≤ (cid:107) ˆVt − Vt(cid:107)

B )δB

B

∞ = (Rmax−Rmin+Cδα

1−γ

5Points from ∆’s boundary can be removed where the gradient is not deﬁned  as the proofs only rely on

interior points.

7

ρ(b) respectively. This is because the worst case for an α vector is Rmin−
is only Rmax

1−γ because the approximation is always a lower bound.

1−γ

  meanwhile the best case

5 Conclusions

We have introduced ρPOMDPs  an extension of POMDPs that allows for expressing sequential
decision-making problems where reducing the uncertainty on some state variables is an explicit
objective. In this model  the reward ρ is typically a convex function of the belief state.
Using the convexity of ρ  a ﬁrst important result that we prove is that a Bellman backup Vn =
HVn−1 preserves convexity. In particular  if ρ is PWLC and the value function V0 is equal to 0  then
Vn is also PWLC and it is straightforward to adapt many state-of-the-art POMDP algorithms. Yet  if
ρ is not PWLC  performing exact updates is much more complex. We therefore propose employing
PWLC approximations of the convex reward function at hand to come back to a simple case  and
show that the resulting algorithms converge to the optimal value function in the limit.
Previous work has already introduced belief-dependent rewards  such as Spaan’s discussion about
POMDPs and Active Perception [19]  or Hero et al.’s work in sensor management using POMDPs
[5]. Yet  the ﬁrst one only presents the problem of non-PWLC value functions without giving a
speciﬁc solution  meanwhile the second solves the problem using Monte-Carlo techniques that do
not rely on the PWLC property. In the robotics ﬁeld  uncertainty measurements within POMDPs
have been widely used as heuristics [2]  with very good results but no convergence guarantees.
These techniques use only state-dependent rewards  but uncertainty measurements are employed to
speed up the solving process  at the cost of losing some basic properties (e.g. Markovian property).
Our work paves the way for solving problems with belief-dependent rewards  using new algorithms
approximating the value function (e.g. point-based ones) in a theoretically sound manner.
An important point is that the time complexity of the new algorithms only changes due to the size
of the approximation of ρ. Future work includes conducting experiments to measure the increase in
complexity. A more complex task is to evaluate the quality of the resulting approximations due to
the lack of other algorithms for ρPOMDPs. An option is to look at online Monte-Carlo algorithms
[20] as they should require little changes.

Acknowledgements

This research was supported by the CONICYT-Embassade de France doctoral grant and the CO-
MAC project. We would also like to thank Bruno Scherrer for the insightful discussions and the
anonymous reviewers for their helpful comments and suggestions.

References

[1] R. Smallwood and E. Sondik. The optimal control of partially observable Markov decision

processes over a ﬁnite horizon. Operation Research  21:1071–1088  1973.

[2] S. Thrun. Probabilistic algorithms in robotics. AI Magazine  21(4):93–109  2000.
[3] L. Mihaylova  T. Lefebvre  H. Bruyninckx  K. Gadeyne  and J. De Schutter. Active sensing for

robotics - a survey. In Proc. 5th Intl. Conf. On Numerical Methods and Applications  2002.

[4] S. Ji and L. Carin. Cost-sensitive feature acquisition and classiﬁcation. Pattern Recogn. 

40(5):1474–1485  2007.

[5] A. Hero  D. Castan  D. Cochran  and K. Kastella. Foundations and Applications of Sensor

Management. Springer Publishing Company  Incorporated  2007.

[6] A. Cassandra. Exact and approximate algorithms for partially observable Markov decision

processes. PhD thesis  Providence  RI  USA  1998.

[7] R. Bellman. The theory of dynamic programming. Bull. Amer. Math. Soc.  60:503–516  1954.
[8] G. Monahan. A survey of partially observable Markov decision processes. Management Sci-

ence  28:1–16  1982.

8

[9] M. Hauskrecht. Value-function approximations for partially observable Markov decision pro-

cesses. Journal of Artiﬁcial Intelligence Research  13:33–94.

[10] W. Lovejoy. Computationally feasible bounds for partially observed Markov decision pro-

cesses. Operations Research  39(1):162–175.

[11] J. Pineau  G. Gordon  and S. Thrun. Anytime point-based approximations for large POMDPs.

Journal of Artiﬁcial Intelligence Research (JAIR)  27:335–380  2006.

[12] M. Spaan and N. Vlassis. Perseus: Randomized point-based value iteration for POMDPs.

Journal of Artiﬁcial Intelligence Research  24:195–220  2005.

[13] T. Smith and R. Simmons. Point-based POMDP algorithms: Improved analysis and imple-

mentation. In Proc. of the Int. Conf. on Uncertainty in Artiﬁcial Intelligence (UAI)  2005.

[14] H. Kurniawati  D. Hsu  and W. Lee. SARSOP: Efﬁcient point-based POMDP planning by
approximating optimally reachable belief spaces. In Robotics: Science and Systems IV  2008.
[15] R. Kaplow. Point-based POMDP solvers: Survey and comparative analysis. Master’s thesis 

Montreal  Quebec  Canada  2010.

[16] T. Cover and J. Thomas. Elements of Information Theory. Wiley-Interscience  1991.
[17] M. Araya-L´opez  O. Buffet  V. Thomas  and F. Charpillet. A POMDP extension with belief-
dependent rewards – extended version. Technical Report RR-7433  INRIA  Oct 2010. (See
also NIPS supplementary material).

[18] R. Saigal. On piecewise linear approximations to smooth mappings. Mathematics of Opera-

tions Research  4(2):153–161  1979.

[19] M. Spaan. Cooperative active perception using POMDPs. In AAAI 2008 Workshop on Ad-

vancements in POMDP Solvers  July 2008.

[20] S. Ross  J. Pineau  S. Paquet  and B. Chaib-draa. Online planning algorithms for POMDPs.

Journal of Artiﬁcial Intelligence Research (JAIR)  32:663–704  2008.

9

,Alexander Zimin
Gergely Neu
Meisam Razaviyayn
Mingyi Hong
Zhi-Quan Luo
Jie Wang
Jieping Ye
Antti Tarvainen
Harri Valpola
Pei Wang
Nuno Nvasconcelos