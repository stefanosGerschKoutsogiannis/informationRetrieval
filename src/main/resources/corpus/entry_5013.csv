2016,Short-Dot: Computing Large Linear Transforms Distributedly Using Coded Short Dot Products,Faced with saturation of Moore's law and increasing size and dimension of data  system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However  distributed computing is often bottle necked by a small fraction of slow processors called "stragglers" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers  recent literature proposes introducing redundancy in computations across processors  e.g.  using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors  ignoring the stragglers. In this paper  we propose a novel technique - that we call "Short-Dot" - to introduce redundant computations in a coding theory inspired fashion  for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform  we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further  only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish)  for any such strategy and compare it to that achieved by our strategy.,“Short-Dot”: Computing Large Linear Transforms

Distributedly Using Coded Short Dot Products

Sanghamitra Dutta

Carnegie Mellon University
sanghamd@andrew.cmu.edu

Viveck Cadambe

Pennsylvania State University

viveck@engr.psu.edu

Pulkit Grover

Carnegie Mellon University
pgrover@andrew.cmu.edu

Abstract

Faced with saturation of Moore’s law and increasing size and dimension of data 
system designers have increasingly resorted to parallel and distributed computing
to reduce computation time of machine-learning algorithms. However  distributed
computing is often bottle necked by a small fraction of slow processors called
“stragglers” that reduce the speed of computation because the fusion node has
to wait for all processors to complete their processing. To combat the effect
of stragglers  recent literature proposes introducing redundancy in computations
across processors  e.g.  using repetition-based strategies or erasure codes. The
fusion node can exploit this redundancy by completing the computation using
outputs from only a subset of the processors  ignoring the stragglers. In this paper 
we propose a novel technique – that we call “Short-Dot” – to introduce redundant
computations in a coding theory inspired fashion  for computing linear transforms
of long vectors. Instead of computing long dot products as required in the original
linear transform  we construct a larger number of redundant and short dot products
that can be computed more efﬁciently at individual processors. Further  only a
subset of these short dot products are required at the fusion node to ﬁnish the
computation successfully. We demonstrate through probabilistic analysis as well
as experiments on computing clusters that Short-Dot offers signiﬁcant speed-up
compared to existing techniques. We also derive trade-offs between the length of
the dot-products and the resilience to stragglers (number of processors required to
ﬁnish)  for any such strategy and compare it to that achieved by our strategy.

1

Introduction

This work proposes a coding-theory inspired computation technique for speeding up computing
linear transforms of high-dimensional data by distributing it across multiple processing units that
compute shorter dot products. Our main focus is on addressing the “straggler effect ” i.e.  the problem
of delays caused by a few slow processors that bottleneck the entire computation. To address this
problem  we provide techniques (building on [1] [2] [3] [4] [5]) that introduce redundancy in the
computation by designing a novel error-correction mechanism that allows the size of individual dot
products computed at each processor to be shorter than the length of the input.
The problem of computing linear transforms of high-dimensional vectors is “the" critical step [6] in
several machine learning and signal processing applications. Dimensionality reduction techniques
such as Principal Component Analysis (PCA)  Linear Discriminant Analysis (LDA)  taking random
projections  require the computation of short and fat linear transforms on high-dimensional data.
Linear transforms are the building blocks of solutions to various machine learning problems  e.g. 
regression and classiﬁcation etc.  and are also used in acquiring and pre-processing the data through
Fourier transforms  wavelet transforms  ﬁltering  etc. Fast and reliable computation of linear trans-
forms are thus a necessity for low-latency inference [6]. Due to saturation of Moore’s law  increasing

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

speed of computing in a single processor is becoming difﬁcult  forcing practitioners to adopt parallel
processing to speed up computing for ever increasing data dimensions and sizes.
Classical approaches of computing linear transforms across parallel processors  e.g.  Block-Striped
Decomposition [7]  Fox’s method [8  7]  and Cannon’s method [7]  rely on dividing the computational
task equally among all available processors1 without any redundant computation. The fusion node
collects the outputs from each processors to complete the computation and thus has to wait for
all the processors to ﬁnish. In almost all distributed systems  a few slow or faulty processors –
called “stragglers”[11] – are observed to delay the entire computation. This unpredictable latency in
distributed systems is attributed to factors such as network latency  shared resources  maintenance
activities  and power limitations. In order to combat with stragglers  cloud computing frameworks
like Hadoop [12] employ various straggler detection techniques and usually reset the task allotted
to stragglers. Forward error-correction techniques offer an alternative approach to deal with this
“straggler effect” by introducing redundancy in the computational tasks across different processors.
The fusion node now requires outputs from only a subset of all the processors to successfully ﬁnish.
In this context  the use of preliminary erasure codes dates back to the ideas of algorithmic fault
tolerance [13] [14]. Recently optimized Repetition and Maximum Distance Separable (MDS) [19]
codes have been explored [2] [3] [1] [16] to speed up computations.
We consider the problem of computing Ax where A(M×N ) is a given matrix and x(N×1) is a vector
that is input to the computation (M (cid:28) N ). In contrast with [1]  which also uses codes to compute
linear transforms in parallel  we allow the size of individual dot products computed at each processor
to be smaller than N  the length of the input. Why might one be interested in computing short
dot products while performing an overall large linear transform? This is because for distributed
digital processors  the computation time is reduced with the number of operations (length of the
dot-products). In Sections 4 and 5  we show that the computation speed-up can be increased beyond
that obtained in [1]. Another interesting example comes from recent work on designing processing
units that exclusively compute dot-products using analog components [17  18]. These devices are
prone to errors and increased delays in convergence when designed for larger dot products.
To summarize  our main contributions are:
1. To compute Ax for a given matrix A(M×N )  we instead compute F x where we construct F(P×N )
(total no. of processors P > Required no. of dot-products M) such that each N-length row of F has
at most N (P − K + M )/P non-zero elements. Because the locations of zeros in a row of F are
known by design  this reduces the complexity of computing dot-products of rows of F with x. Here
K parameterizes the resilience to stragglers: any K of the P dot products of rows of F with x are
sufﬁcient to recover Ax  i.e.  any K rows of F can be linearly combined to generate the rows of A.
2. We provide fundamental limits on the trade-off between the length of the dot-products and the
straggler resilience (number of processors to wait for) for any such strategy in Section 3. This
suggests a lower bound on the length of task allotted per processor. However  we believe that these
limits are loose and point to an interesting direction for future work.
3. Assuming exponential tails of service-times at each server (used in [1])  we derive the expected
computation time required by our strategy and compare it to uncoded parallel processing  repetition
strategy and MDS codes [19] (see Fig. 2). Short-Dot offers speed-up by a factor of Ω(log(P )) over
uncoded  parallel processing and repetition  and nearly by a factor of Ω( P
M ) compared to MDS
codes when M is linear in P . The strategy out-performs repetition or MDS codes by a factor of
Ω
4. We provide experimental results showing that Short-Dot is faster than existing strategies.
For the rest of the paper  we deﬁne the sparsity of a vector u ∈ RN as the number of nonzero
j=1 I(uj (cid:54)= 0). We also assume that P divides N (P (cid:28) N).
Comparison with existing strategies: Consider the problem of computing a single dot product of
an input vector x ∈ RN with a pre-speciﬁed vector a ∈ RN . By an “uncoded” parallel processing
strategy (which includes Block Striped Decomposition [7])  we mean a strategy that does not use
redundancy to overcome delays caused by stragglers. One uncoded strategy is to partition the dot
product into P smaller dot products  where P is the number of available processors. E.g. a can

(cid:16)
elements in the vector  i.e.  (cid:107)u(cid:107)0 =(cid:80)N

when M is sub-linear in P .

(cid:17)

P

M log(P/M )

1Strassen’s algorithm [9] and its generalizations offer a recursive approach to faster matrix multiplications

over multiple processors  but they are often not preferred because of their high communication cost [10].

2

Figure 1: A dot-product of length N = 12 is being computed parallely using P = 6 processors.
(Left) Uncoded Parallel Processing - Divide into P parts  (Right) Repetition with block partitioning.

be divided into P parts – constructing P short vectors of sparsity N/P – with each vector stored
in a different processor (as shown in Fig. 1 left). Only the nonzero values of the vector need to be
stored since the locations of the nonzero values is known apriori at every node. One might expect
the computation time for each processor to reduce by a factor of P . However  now the fusion node
has to wait for all the P processors to ﬁnish their computation  and the stragglers can now delay the
entire computation. Can we construct P vectors such that dot products of a subset of them with x
are sufﬁcient to compute (cid:104)a  x(cid:105)? A simple coded strategy is Repetition with block partitioning i.e. 
constructing L vectors of sparsity N/L by partitioning the vector of length N into L parts (L < P ) 
and repeating the L vectors P/L times so as to obtain P vectors of sparsity N/L as shown in Fig. 1
(right). For each of the L parts of the vector  the fusion node only needs the output of one processor
among all its repetitions. Instead of a single dot-product  if one requires the dot-product of x with M
vectors {a1  . . .   aM}  one can simply repeat the aforementioned strategy M times.
For multiple dot-products  an alternative repetition-based strategy is to compute M dot products
P/M times in parallel at different processors. Now we only have to wait for at least one processor
corresponding to each of the M vectors to ﬁnish. Improving upon repetition  it is shown in [1]
that an (P  M )-MDS code allows constructing P coded vectors such that any M of P dot-products
can be used to reconstruct all the M original vectors (see Fig. 2b). This strategy is shown  both
experimentally and theoretically  to perform better than repetition and uncoded strategies.

(a) Uncoded Parallel Processing
Figure 2: Different strategies of parallel processing: Here M = 3 dot-products of length N = 12 are
being computed using P = 6 processors.

(b) Using MDS codes

(c) Using Short-Dot

Can we go beyond MDS codes? MDS codes-based strategies require N-length dot-products to be
computed on each processor. Short-Dot instead constructs P vectors of sparsity s (less than N)  such
that the dot product of x with any K (≥ M ) out of these P short vectors is sufﬁcient to compute the
dot-product of x with all the M given vectors (see Fig. 2c). Compared to MDS Codes  Short-Dot
waits for some more processors (since K ≥ M)  but each processor computes a shorter dot product.
We also propose Short-MDS  an extension of the MDS codes-based strategy in [1] to create short
dot-products of length s  through block partitioning  and compare it with Short-Dot. In regimes
where N
s is not
an integer  Short-MDS has to wait for more processors in worst case than Short-Dot for the same
sparsity s  as discussed in Remark 1 in Section 2.

s is an integer  Short-MDS may be viewed as a special case of Short-Dot. But when N

2 Our coded parallelization strategy: Short-Dot
In this section  we provide our strategy of computing the linear transform Ax where x ∈ RN
is the input vector and A(M×N ) = [a1  a2  . . .   aM ]T is a given matrix. Short-Dot constructs a

3

Figure 3: Short-Dot: Distributes short dot-products over P parallel processors  such that outputs from
any K out of P processors are sufﬁcient to compute successfully.

1   . . .   aT

2   . . .   aT

1   aT

2   . . .   aT

P (P − K + M ). Each sparse row of F (say f T

1   aT
P (P − K + M )  provided P divides N.

P × N matrix F = [f1  f2  . . .   fP ]T such that M predetermined linear combinations of any K
M}  and any row of F has sparsity at most
rows of F are sufﬁcient to generate each of {aT
i ) is sent to the i-th processor (i = 1  . . .   P )
s = N
and dot-products of x with all sparse rows are computed in parallel. Let Si denote the support
(set of non-zero indices) of fi. Thus  for any unknown vector x  short dot products of length
P (P − K + M ) are computed on each processor. Since the linear combination of any
|Si| ≤ s = N
K rows of F can generate the rows of A  i.e.  {aT
M}  the dot-product from the earliest
K out of P processors can be linearly combined to obtain the linear transform Ax. Before formally
stating our algorithm  we ﬁrst provide an insight into why such a matrix F exists in the following
theorem  and develop an intuition on the construction strategy.
M}  there exists a P × N matrix F such that a linear
Theorem 1 Given row vectors {aT
combination of any K(> M ) rows of the matrix is sufﬁcient to generate the row vectors and each
row of F has sparsity at most s = N
Proof: We may append (K − M ) rows to A = [a1  a2  . . .   aM ]T   to form a K × N matrix
˜A = [a1  a2  . . .   aM   z1  . . .   zK−M ]T . The precise choice of these additional vectors will be made
explicit later. Next  we choose B  a P × K matrix such that any square sub-matrix of B is invertible.
E.g.  A Vandermonde or Cauchy Matrix  or a matrix with i.i.d. Gaussian entries can be shown to
satisfy this property with probability 1. The following lemma shows that any K rows of the matrix
B ˜A are sufﬁcient to generate any row of ˜A  including {aT
Lemma 1 Let F = B ˜A where ˜A is a K × N matrix and B is any (P × K) matrix such that every
square sub-matrix is invertible. Then  any K rows of F can be linearly combined to generate any
row of ˜A.
Proof: Choose an arbitrary index set χ ⊂ {1  2  . . .   P} such that |χ| = K. Let F χ be the sub-matrix
formed by chosen K rows of F indexed by χ. Then  F χ = Bχ ˜A. Now  Bχ is a K × K sub-matrix
of B  and is thus invertible. Thus  ˜A = (Bχ)−1F χ. The i-th row of ˜A is [i-th Row of (Bχ)−1]F χ
(cid:4)
for i = 1  2  . . .   K. Thus  each row of ˜A is generated by the chosen K rows of F .
P (P−K+M )
In the next lemma  we show how the row sparsity of F can be constrained to be at most N
by appropriately choosing the appended vectors z1  . . .   zK−M .
Lemma 2 Given an M × N matrix A = [a1  . . .   aM ]T   let ˜A = [a1  . . .   aM   z1  . . .   zK−M ]T
be a K × N matrix formed by appending K − M row vectors to A. Also let B be a P × K matrix
such that every square matrix is invertible. Then there exists a choice of the appended vectors
z1  . . .   zK−M such that each row of F = B ˜A has sparsity at most s = N

P (P − K + M ).

1   aT

2   . . .   aT

M}:

Proof: We select a sparsity pattern that we want to enforce on F and then show that there exists a
choice of the appended vectors z1  . . .   zK−M such that the pattern can be enforced.
Sparsity Pattern enforced on F : This is illustrated in Fig. 4. First  we construct a P × P “unit
block” with a cyclic structure of nonzero entries  where (K − M ) zeros in each row and column
are arranged as shown in Fig. 4. Each row and column have at most sc = P − K + M non-zero
entries. This unit block is replicated horizontally N/P times to form an P × N matrix with at most

4

sc non-zero entries in each column  and and at most s = N sr/P non-zero entries in each row. We
now show how choice of z1  . . .   zK−M can enforce this pattern on F .

Figure 4: Sparsity pattern of F : (Left) Unit Block (P × P ); (Right) Unit Block concatenated N/P
times to form N × P matrix F with row sparsity at most s.

From F = B ˜A  the j-th column of F can be written as  Fj = B ˜Aj. Each column of F has at
least K − M zeros at locations indexed by U ⊂ {1  2  . . .   P}. Let BU denote a ((K − M ) × K)
sub-matrix of B consisting of the rows of B indexed by U. Thus  BU ˜Aj = [0](K−M )×1. Divide
˜Aj into two portions of lengths M and K − M as follows:
˜Aj = [AT
j
Here Aj = [a1(j) a2(j) . . . aM (j)]T is actually the j-th column of given matrix A and z =
[z1(j)  . . . zK−M (j)]T depends on the choice of the appended vectors. Thus 

| zT ]T = [a1(j) a2(j) . . . aM (j) z1(j) . . . zK−M (j)]T

BU

cols 1:M Aj + BU

cols M +1:K z = [0]K−M×1

⇒ BU

cols M +1:K z = −BU

cols 1:M [Aj]

⇒ [ z ] = −(BU

cols M +1:K)−1 BU

cols 1:M [Aj]

(1)
cols M +1:K] is invertible because it is a (K − M ) × (K − M )
where the last step uses the fact that [BU
square sub-matrix of B. This explicitly provides the vector z which completes the j-th column of ˜A.
(cid:4)
The other columns of ˜A can be completed similarly  proving the lemma.
From Lemmas 1 and 2  for a given M × N matrix A  there always exists a P × N matrix F such
that a linear combination of any K columns of F is sufﬁcient to generate our given vectors and each
(cid:4)
row of F has sparsity at most s = N
With this insight in mind  we now formally state our computation strategy:

P (P − K + M ). This proves the theorem.

Algorithm 1 Short-Dot

cols 1:M [Aj]

U ← ({(j − 1)  . . .   (j + K − M − 1)} mod P ) + 1

[A] Pre-Processing Step: Encode F (Performed Ofﬂine)
Given: AM×N = [a1  . . .   aM ]T = [A1  A2  . . .   AN ]  parameter K  M atrix BP×K
1: For j = 1 to N do
2:
3:
4:
5:
6:

Set
Set BU ← Rows of B indexed by U
cols M +1:K)−1 BU
Set
j |zT ]T
Set

(cid:66) The set of (K − M ) indices that are 0 for the j-th column of F
(cid:66) z(K−M )×1 is a row vector.
(cid:66) Fj is a column vector ( j-th col of F )
(cid:66) Row representation of matrix F
(cid:66) Indices of non-zero entries in the i-th row of F
(cid:66) i-th row of F sent to i-th processor

Encoded Output: FP×N = [f1f2 . . . fP ]T

[ z ] = −(BU
Fj = B[AT

Store Si ← Support(fi)
Send f Si
to i-th processor
i

7: For i = 1 to P do
8:
9:
[B] Online computations
External Input : x
Resources: P parallel processors (P > M )
[B1] Parallelization Strategy: Divide task among parallel processors:
1: For i = 1 to P do
2:
3:
Output: (cid:104)f Si

Send xSi to the i-th processor
Compute at i-th processor: (cid:104)f Si

  xSi(cid:105) from K earliest processors

  xSi(cid:105) (cid:66) uS denotes only the rows of vector u indexed by S

i

i

5

V ← Indices of the K processors that ﬁnished ﬁrst
vK×1 ← [(cid:104)f Si

[B2] Fusion Node: Decode the dot-products from the processor outputs:
1: Set
2: Set BV ← Rows of B indexed by V
  xSi(cid:105)  ∀ i ∈ V ]
3: Set
4: Set Ax = [(cid:104)a1  x(cid:105)  . . .  (cid:104)aM   x(cid:105)]T ← [(BV )−1]rows 1:M v
5: Output: (cid:104)x  a1(cid:105)  . . .  (cid:104)x  aM(cid:105)

i

(cid:66) Col Vector of outputs from ﬁrst K processors

Table 1: Trade-off between the length of the dot-products and parameter K for different strategies

Strategy

Length Parameter K

Length

Strategy
Repetition N
MDS
N
Short-Dot
s

Parameter K

P −(cid:4) P
P −(cid:4) P s

M

M

(cid:5) + 1
(cid:5) + M

N

Repetition with
block partition
Short-MDS

s

s

(cid:107)

P −(cid:106)
P −(cid:106) P(cid:100)N/s(cid:101)

P

(cid:107)

M(cid:100)N/s(cid:101)

+ 1

+ M

Remark 1: Short-MDS - a special case of Short-Dot An extension of the MDS codes-based
strategy proposed in [1]  that we call Short-MDS can be designed to achieve row-sparsity s. First
block-partition the matrix of N columns  into (cid:100)N/s(cid:101) sub-matrices of size M × s  and also divide
the total processors P equally into (cid:100)N/s(cid:101) parts. Now  each sub-matrix can be encoded using
a ( P(cid:100)N/s(cid:101)   M ) MDS code. In the worst case  including all integer effects  this strategy requires

+ M processors to ﬁnish. In comparison  Short-Dot requires K = P −(cid:4) P s

K = P −(cid:106) P(cid:100)N/s(cid:101)

(cid:5) + M

(cid:107)

processors to ﬁnish. In the regime where  s exactly divides N  Short-MDS can be viewed as a special
case of Short-Dot  as both the expressions match. However  in the regime where s does not exactly
divide N  Short-MDS requires more processors to ﬁnish in the worst case than Short-Dot. Short-Dot
is a generalized framework that can achieve a wider variety of pre-speciﬁed sparsity patterns as
required by the application. In Table 1  we compare the lengths of the dot-products and straggler
resilience K  i.e.  the number of processors to wait for in worst case  for different strategies.

N

3 Limits on trade-off between the length of dot-products and parameter K

Theorem 2 Let AM×N be any matrix such that each column has at least one non-zero element. If
the linear combination of any K rows of F(P×N ) can generate M rows of AM×N   then the average

sparsity s of each row of F(P×N ) must satisfy s ≥ N(cid:0)1 − K

(cid:1) + N

P

P .

Proof: We claim that K is strictly greater than the maximum number of zeros that can occur
in any column of the matrix F . If not  suppose the j-th column of F has more than K zeros.
Then there exists a linear combination of K rows of F that will always have 0 at the j-th column
index and it is not possible to generate any row of the given matrix A. Thus  K is no less than
1 + M ax N o. of 0s in any column of F . Since  maximum value is always greater than average 

K ≥ 1 + Avg. N o. of 0s in any column of F ≥ 1 +

A slight re-arrangement establishes the aforementioned lower bound.

Short-Dot achieves a row-sparsity of at most s = N(cid:0)1 − K
such strategy is s ≥ N(cid:0)1 − K

(2)
(cid:4)
P while the lower bound for any
P . Notice that the bounds only differ in the second term. We
believe that the difference in the bounds arises due to the looseness of the fundamental limit: our
technique is based on derivation for M = 1 (bound is tight)  and could be tightened for M > 1.

(cid:1) + NM

(cid:1) + N

N

P

P

.

(N − s)P

4 Analysis of expected computation time for exponential tail models

We now provide a probabilistic analysis of the computational time required by Short-Dot and compare
it with uncoded parallel processing  repetition and MDS codes as shown in Fig. 5. Table 2 shows
the order-sense expected computation time in the regimes where M is linear and sub-linear in P .
A detailed analysis is provided in the supplement. Assume that the time required by a processor to

6

Figure 5: Expected computation time: Short-Dot is faster than MDS when M (cid:28) P and Uncoded
when M ≈ P   and is universally faster over the entire range of M. For the choice of straggling
parameter  Repetition is slowest. When M does not exactly divide P   the distribution of computation
time for repetition and uncoded strategies is the maximum of non-identical but independent random
variables  which produce the ripples in these curves (see supplement for details).

compute a single dot-product follows an exponential distribution and is independent of the other
processors  as described in [1]. Let the time required to compute a single dot-product of length N

be distributed as: Pr(TN ≤ t) = 1 − exp(cid:0)−µ (cid:0) t

N − 1(cid:1)(cid:1) ∀ t ≥ N. Here  µ is the “straggling

parameter” that determines the unpredictable latency in computation time. For an s length dot product 
we simply replace N by s .The expected computation time for Short-Dot is the expected value of the
K-th order statistic of these P iid exponential random variables  which is given by:

E(T ) = s

1 +

log( P
P−K )
µ

(P − K + M )N

P

1 +

log( P
P−K )
µ

.

(3)

variables with parameter 1 is(cid:80)P

Here  (3) uses the fact that the expected value of the K-th statistic of P iid exponential random
i ≈ log(P ) − log(P − K) [1]. The expected
computation time in the RHS of (3) is minimized when P − K = Θ(M ). This minimal expected
time is O( M N

P ) for M linear in P and is O(cid:16) M N log(P/M )

for M sub-linear in P .

(cid:17)

i=1

P

1

1

(cid:33)
i −(cid:80)P−K

i=1

=

(cid:32)

(cid:33)

(cid:32)

Strategy

Only one Processor
Uncoded (M divides P)2
P
Repetition (M divides P) 2 N

M N

M N

MDS

Short-Dot

Table 2: Probabilistic Computation Times
E(T )

M linear in P

(cid:16)
(cid:16)

(cid:17)
(cid:17)
(cid:19)

(cid:16)
(cid:18)

µ

1 + 1
µ
1 + log(P )
1 + M log(M )
P µ
P −M )
log( P
µ

1 +

N

(cid:17)

(cid:18)

N (P−K+M )

P

1 +

P −K )
log( P
µ

M sub-linear in P

Θ (M N )

Θ (M N )

P log(P )(cid:1) Θ(cid:0) M N
P log(P )(cid:1)
Θ(cid:0) M N
Θ(cid:0) M N
P log(P )(cid:1) Θ (N )
(cid:1)(cid:1)
O(cid:0) M N
P log(cid:0) P

Θ(N )
O( M N
P )

Θ(N )

M

(cid:19)

2 Refer to Supplement for more accurate analysis taking integer effects into account

Encoding and Decoding Complexity: Even though encoding is a pre-processing step (since A is
assumed to be given in advance)  we include a complexity analysis for the sake of completeness. The
P matrix inversions of size (K − M )  and a P × K matrix multiplication with
encoding requires N
a K × N matrix. The naive encoding complexity is therefore O( N
P (K − M )3 + N KP ). This is
higher than MDS codes that has an encoding complexity of O(N M P ))  but it is only a one-time cost
that provides savings in online steps (as discussed earlier in this section). The decoding complexity
of Short-Dot is O(K 3 + KM ) which does not depend on N when M  K (cid:28) N. This is nearly the
same as O(M 3 + M 2) complexity of MDS codes. We believe that the complexities might be reduced
further  based on special choices of encoding matrix B.

7

Table 3: Experimental computation time of 10000 dot products (N = 785  M = 10  P = 20)

Strategy
Uncoded
Short-Dot
MDS

Parameter K Mean
20
18
10

STDEV
11.8653 2.8427
10.4306 0.9253
15.3411 0.8987

Minimum Time Maximum Time
9.5192
8.2145
13.8232

27.0818
11.8340
17.5416

5 Experimental Results

We perform experiments on computing clusters at CMU to test the computational time. We use
HTCondor [20] to schedule jobs simultaneously among the P processors. We compare the time
required to classify 10000 handwritten digits of the MNIST [21] database  assuming we are given a
trained 1-layer Neural Network. We separately trained the Neural network using training samples  to
form a matrix of weights  denoted by A10×785. For testing  the multiplication of this given 10 × 785
matrix  with the test data matrix X785×10000 is considered. The total number of processors was 20.
Assuming that A10×785 is encoded into F20×785 in a pre-processing step  we store the rows of F
in each processor apriori. Now portions of the data matrix X of size s × 10000 are sent to each of
the P parallel processors as input. We also send a C-program to compute dot-products of length
P (P − K + M ) with appropriate rows of F using command condor-submit. Each processor
s = N
outputs the value of one dot-product. The computation time reported in Fig. 6 includes the total time
required to communicate inputs to each processor  compute the dot-products in parallel  fetch the
required outputs  decode and classify all the 10000 test-images  based on 35 experimental runs.

Figure 6: Experimental results: (Left) Mean computation time for Uncoded Strategy  Short-Dot
(K=18) and MDS codes: Short-Dot is faster than MDS by 32% and Uncoded by 12%. (Right) Scatter
plot of computation time for different experimental runs: Short-Dot is faster most of the time.

Key Observations: (See Table 3 for detailed results). Computation time varies based on nature of
straggling  at the particular instant of the experimental run. Short-Dot outperforms both MDS and
Uncoded  in mean computation time. Uncoded is faster than MDS since per-processor computation
time for MDS is larger  and it increases the straggling  even though MDS waits for only for 10 out of
20 processors. However  note that Uncoded has more variability than both MDS and Short-Dot  and
its maximum time observed during the experiment is much greater than both MDS and Short-Dot.
The classiﬁcation accuracy was 85.98% on test data.

6 Discussion

While we have presented the case of M < P here  Short-Dot easily generalizes to the case where
M ≥ P . The matrix can be divided horizontally into several chunks along the row dimension (shorter
matrices) and Short-Dot can be applied on each of those chunks one after another. Moreover if rows
with same sparsity pattern are grouped together and stored in the same processor initially  then the
communication cost is also signiﬁcantly reduced during the online computations  since only some
elements of the unknown vector x are sent to a particular processor.
Acknowledgments: Systems on Nanoscale Information fabriCs (SONIC)  one of the six SRC
STARnet Centers  sponsored by MARCO and DARPA. We also acknowledge NSF Awards 1350314 
1464336 and 1553248. S Dutta also received Prabhu and Poonam Goel Graduate Fellowship.

8

References
[1] Kangwook Lee  Maximilian Lam  Ramtin Pedarsani  Dimitris Papailiopoulos  and Kannan
Ramchandran. Speeding Up Distributed Machine Learning Using Codes. NIPS Workshop on
Learning Systems  2015.

[2] Da Wang  Gauri Joshi  and Gregory Wornell. Using straggler replication to reduce latency
in large-scale parallel computing. In ACM SIGMETRICS Performance Evaluation Review 
volume 43  pages 7–11  2015.

[3] Da Wang  Gauri Joshi  and Gregory Wornell. Efﬁcient Task Replication for Fast Response Times
in Parallel Computation. In ACM SIGMETRICS Performance Evaluation Review  volume 42 
pages 599–600  2014.

[4] Gauri Joshi  Yanpei Liu  and Emina Soljanin. On the delay-storage trade-off in content download
from coded distributed storage systems. IEEE Journal on Selected Areas in Communications 
32(5):989–997  2014.

[5] Longbo Huang  Sameer Pawar  Hao Zhang  and Kannan Ramchandran. Codes can reduce
queueing delay in data centers. In Proceedings IEEE International Symposium on Information
Theory (ISIT)  pages 2766–2770  2012.

[6] William Dally. High-performance hardware for machine learning. NIPS Tutorial  2015.
[7] Vipin Kumar  Ananth Grama  Gupta Anshul  and George Karypis. Introduction to Parallel
Computing: Design and Analysis of Algorithms. The Benjamin/Cummings Publishing Company 
Inc.  Redwood City  1994.

[8] Geoffrey C Fox  Steve W Otto  and Anthony JG Hey. Matrix algorithms on a hypercube I:

Matrix multiplication. Parallel computing  4(1):17–31  1987.

[9] Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik  13(4):354–356 

1969.

[10] Grey Ballard  James Demmel  Olga Holtz  and Oded Schwartz. Communication costs of

strassen’s matrix multiplication. Communications of the ACM  57(2):107–114  2014.

[11] Jeffrey Dean and Luiz André Barroso. The tail at scale. Communications of the ACM  56(2):74–

80  2013.

[12] Konstantin Shvachko  Hairong Kuang  Sanjay Radia  and Robert Chansler. The Hadoop
In Proceedings IEEE Symposium on Mass Storage Systems and

Distributed File System.
Technologies (MSST)  pages 1–10  2010.

[13] Kuang-Hua Huang and Jacob A. Abraham. Algorithm-based fault tolerance for matrix opera-

tions. IEEE transactions on computers  100(6):518–528  1984.

[14] Thomas Herault and Yves Robert. Fault-Tolerance Techniques for High Performance Computing.

Springer  2015.

[15] William Ryan and Shu Lin. Channel codes: Classical and Modern. Cambridge University

Press  2009.

[16] Songze Li  Mohammad Ali Maddah-Ali  and A Salman Avestimehr. A uniﬁed coding framework

for distributed computing with straggling servers. arXiv:1609.01690v1 [cs.IT]  2016.

[17] Ihab Nahlus  Eric P Kim  Naresh R Shanbhag  and David Blaauw. Energy-efﬁcient Dot-Product
Computation using a Switched Analog Circuit Architecture. In International Symposium on
Low Power Electronics and Design (ISLPED)  pages 315–318  2014.

[18] Ning C Wang  Sujan K Gonugondla  Ihab Nahlus  Naresh Shanbhag  and Eric Pop. GDOT: a
Graphene-Based Nanofunction for Dot-Product Computation. In IEEE Symposium on VLSI
Technology  2016.

[19] HTCondor. https://research.cs.wisc.edu/htcondor/.
[20] Yann LeCun  Corinna Cortes  and Christopher JC Burges. The MNIST database of handwritten

digits. http://yann.lecun.com/exdb/mnist  1998.

9

,Tuo Zhao
Han Liu
Sanghamitra Dutta
Viveck Cadambe
Pulkit Grover