2009,Noisy Generalized Binary Search,This paper addresses the problem of noisy Generalized Binary Search (GBS).  GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.  At each step  a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets  a natural generalization of the idea underlying classic binary search.  GBS is used in many applications  including fault testing  machine diagnostics  disease diagnosis  job scheduling  image processing  computer vision  and active learning. In most of these cases  the responses to queries can be noisy.  Past work has provided a partial characterization of GBS  but existing noise-tolerant versions of GBS are suboptimal in terms of sample complexity.  This paper presents the first optimal algorithm for noisy GBS and demonstrates its application to learning multidimensional threshold functions.,Noisy Generalized Binary Search

University of Wisconsin-Madison

1415 Engineering Drive  Madison WI 53706

Robert Nowak

nowak@ece.wisc.edu

Abstract

This paper addresses the problem of noisy Generalized Binary Search (GBS).
GBS is a well-known greedy algorithm for determining a binary-valued hypothe-
sis through a sequence of strategically selected queries. At each step  a query is
selected that most evenly splits the hypotheses under consideration into two dis-
joint subsets  a natural generalization of the idea underlying classic binary search.
GBS is used in many applications  including fault testing  machine diagnostics 
disease diagnosis  job scheduling  image processing  computer vision  and active
learning. In most of these cases  the responses to queries can be noisy. Past work
has provided a partial characterization of GBS  but existing noise-tolerant ver-
sions of GBS are suboptimal in terms of query complexity. This paper presents
an optimal algorithm for noisy GBS and demonstrates its application to learning
multidimensional threshold functions.

Introduction

1
This paper studies learning problems of the following form. Consider a ﬁnite  but potentially very
large  collection of binary-valued functions H deﬁned on a domain X . In this paper  H will be called
the hypothesis space and X will be called the query space. Each h ∈ H is a mapping from X to
{−1  1}. Assume that the functions in H are unique and that one function  h∗ ∈ H  produces the
correct binary labeling. The goal is to determine h∗ through as few queries from X as possible. For
each query x ∈ X   the value h∗(x)  corrupted with independently distributed binary noise  is ob-
served. If the queries were noiseless  then they are usually called membership queries to distinguish
them from other types of queries [Ang01]; here we will simply refer to them as queries. Problems
of this nature arise in many applications   including channel coding [Hor63]  experimental design
[R´en61]  disease diagnosis [Lov85]  fault-tolerant computing [FRPU94]  job scheduling [KPB99] 
image processing [KK00]  computer vision [SS93  GJ96]  computational geometry [AMM+98]  and
active learning [Das04  BBZ07  Now08].
Past work has provided a partial characterization of this problem. If the responses to queries are
noiseless  then selecting the optimal sequence of queries from X is equivalent to determining an
optimal binary decision tree  where a sequence of queries deﬁnes a path from the root of the tree
(corresponding to H) to a leaf (corresponding to a single element of H).
In general the deter-
mination of the optimal tree is NP-complete [HR76]. However  there exists a greedy procedure
that yields query sequences that are within an O(log |H|) factor of the optimal search tree depth
[GG74  KPB99  Lov85  AMM+98  Das04]  where |H| denotes the cardinality of H. The greedy
procedure is referred to as Generalized Binary Search (GBS) [Das04  Now08] or the splitting al-
gorithm [KPB99  Lov85  GG74])  and it reduces to classic binary search in special cases [Now08].
The GBS algorithm is outlined in Figure 1(a). At each step GBS selects a query that results in
the most even split of the hypotheses under consideration into two subsets responding +1 and −1 
respectively  to the query. The correct response to the query eliminates one of these two subsets
from further consideration. Since the hypotheses are assumed to be distinct  it is clear that GBS
terminates in at most |H| queries (since it is always possible to ﬁnd query that eliminates at least

1

1) Select xi = arg minx∈X |(cid:80)

Generalized Binary Search (GBS)
initialize: i = 0  H0 = H.
while |Hi| > 1
2) Obtain response yi = h∗(xi).
3) Set Hi+1 = {h ∈ Hi : h(xi) = yi} 
i = i + 1.

h∈Hi

h(x)|.

Noisy Generalized Binary Search (NGBS)
initialize: p0 uniform over H.
for i = 0  1  2  . . .

1) xi = arg minx∈X |(cid:80)
(cid:98)hi := arg maxh∈H pi(h)

2) Obtain noisy response yi.
3) Bayes update pi → pi+1; Eqn. (1).
hypothesis selected at each step:

h∈H pi(h)h(x)|.

(a)

(b)

Figure 1: Generalized binary search (GBS) algorithm and a noise-tolerant variant (NGBS).

one hypothesis at each step). In fact  there are simple examples demonstrating that this is the best
one can hope to do in general [KPB99  Lov85  GG74  Das04  Now08]. However  it is also true that
in many cases the performance of GBS can be much better [AMM+98  Now08]. In general  the
number of queries required can be bounded in terms of a combinatorial parameter of H called the
extended teaching dimension [Ang01  Heg95] (also see [HPRW96] for related work). Alternatively 
there exists a geometric relation between the pair (X  H)  called the neighborly condition  that is
sufﬁcient to bound the number of queries needed [Now08].
The focus of this paper is noisy GBS. In many (if not most) applications it is unrealistic to assume
that the responses to queries are without error. Noise-tolerant versions of classic binary search have
been well-studied. The classic binary search problem is equivalent to learning a one-dimensional
binary-valued threshold function by selecting point evaluations of the function according to a bisec-
tion procedure. A noisy version of classic binary search was studied ﬁrst in the context of channel
coding with feedback [Hor63]. Horstein’s probabilistic bisection procedure [Hor63] was shown to
be optimal (optimal decay of the error probability) [BZ74] (also see[KK07]).
One straightforward approach to noisy GBS was explored in [Now08]. The idea is to follow the GBS
algorithm  but to repeat the query at each step multiple times in order to decide whether the response
is more probably +1 or −1. The strategy of repeating queries has been suggested as a general
approach for devising noise-tolerant learning algorithms [K¨a¨a06]. This simple approach has been
studied in the context of noisy versions of classic binary search and shown to be suboptimal [KK07].
Since classic binary search is a special case of the general problem  it follows immediately that the
approach proposed in [Now08] is suboptimal. This paper addresses the open problem of determining
an optimal strategy for noisy GBS. An optimal noise-tolerant version of GBS is developed here. The
number of queries an algorithm requires to conﬁdently identify h∗ is called the query complexity of
the algorithm. The query complexity of the new algorithm is optimal  and we are not aware of any
other algorithm with this capability.
It is also shown that optimal convergence rate and query complexity is achieved for a broad class
of geometrical hypotheses arising in image recovery and binary classiﬁcation. Edges in images and
decision boundaries in classiﬁcation problems are naturally viewed as curves in the plane or sur-
faces embedded in higher-dimensional spaces and can be associated with multidimensional thresh-
old functions valued +1 and −1 on either side of the curve/surface. Thus  one important setting for
GBS is when X is a subset of d dimensional Euclidean space and the set H consists of multidimen-
sional threshold functions. We show that our algorithm achieves the optimal query complexity for
actively learning multidimensional threshold functions in noisy conditions.
The paper is organized as follows. Section 2 describes the Bayesian algorithm for noisy GBS and
presents the main results. Section 3 examines the proposed method for learning multidimensional
threshold functions. Section 4 discusses an agnostic algorithm that performs well even if h∗ is not
in the hypothesis space H. Proofs are given in Section 5.
2 A Bayesian Algorithm for Noisy GBS
In noisy GBS  one must cope with erroneous responses. Speciﬁcally  assume that the binary response
y ∈ {−1  1} to each query x ∈ X is an independent realization of the random variable Y satisfying
P(Y = h∗(x)) > P(Y = −h∗(x))  where h∗ ∈ H is ﬁxed but unknown. In other words  the
response is only probably correct. If a query x is repeated more than once  then each response is

2

sure over H. That is  p0 : H → [0  1] and(cid:80)

an independent realization of Y . Deﬁne the noise-level for the query x as αx := P(Y = −h∗(x)).
Throughout the paper we will let α := supx∈X αx and assume that α < 1/2.
A Bayesian approach to noisy GBS is investigated in this paper. Let p0 be a known probability mea-
h∈H p0(h) = 1. The measure p0 can be viewed as an
initial weighting over the hypothesis class  expressing the fact that all hypothesis are equally reason-
able prior to making queries. After each query and response (xi  yi)  i = 0  1  . . .   the distribution
is updated according to

pi+1(h) ∝ pi(h) β(1−zi(h))/2(1 − β)(1+zi(h))/2 

(1)
where zi(h) = h(xi)yi  h ∈ H  β is any constant satisfying 0 < β < 1/2  and pi+1(h) is
h∈H pi+1(h) = 1 . The update can be viewed as an application of Bayes rule
and its effect is simple; the probability masses of hypotheses that agree with the label yi are boosted
relative to those that disagree. The parameter β controls the size of the boost. The hypothesis with

normalized to satisfy(cid:80)
the largest weight is selected at each step:(cid:98)hi := arg maxh∈H pi(h). If the maximizer is not unique 
one of the maximizers is selected at random. The goal of noisy GBS is to drive the error P((cid:98)hi (cid:54)= h∗)
if the weighted prediction(cid:80)

to zero as quickly as possible by strategically selecting the queries. A similar procedure has been
shown to be optimal for noisy (classic) binary search problem [BZ74  KK07]. The crucial distinction
here is that GBS calls for a fundamentally different approach to query selection.
The query selection at each step must be informative with respect to the distribution pi. For example 
h∈H pi(h)h(x) is close to zero for a certain x  then a label at that point is
informative due to the large disagreement among the hypotheses. This suggests the following noise-
tolerant variant of GBS outlined in Figure 1. This paper shows that a slight variation of the query
selection in the NGBS algorithm in Figure 1 yields an algorithm with optimal query complexity.
It is shown that as long as β is larger than the noise-level of each query  then the NGBS produces

a sequence of hypotheses (cid:98)h0 (cid:98)h1  . . .   such that P((cid:98)hn (cid:54)= h∗) is bounded above by a monotonically

each such subset. Let A denote the smallest such partition. Note that X = (cid:83)

decreasing sequence (see Theorem 1). The main interest of this paper is an algorithm that drives the
error to zero exponentially fast  and this requires the query selection criterion to be modiﬁed slightly.
To see why this is necessary  suppose that at some step of the NGBS algorithm a single hypothesis
(e.g.  h∗) has the majority of the probability mass. Then the weighted prediction will be almost
equal to the prediction of that hypothesis (i.e.  close to +1 or −1 for all queries)  and therefore the
responses to all queries are relatively certain and non-informative. Thus  the convergence of the
algorithm could become quite slow in such conditions. A similar effect is true in the case of noisy
(classic) binary search [BZ74  KK07]. To address this issue  the query selection criterion is modiﬁed
via randomization so that the response to the selected query is always highly uncertain.
In order to state the modiﬁed selection procedure and the main results  observe that the query space
X can be partitioned into equivalence subsets such that every h ∈ H is constant for all queries in
A∈A A. For every
A ∈ A and h ∈ H  the value of h(x) is constant (either +1 or −1) for all x ∈ A; denote this value
by h(A). As ﬁrst noted in [Now08]  A can play an important role in GBS. In particular  observe that
the query selection step in NGBS is equivalent to an optimization over A rather that X itself. The
randomization of the query selection step is based on the notion of neighboring sets in A.
Deﬁnition 1 Two sets A  A(cid:48) ∈ A are said to be neighbors if only a single hypothesis (and its
complement  if it also belongs to H) outputs a different value on A and A(cid:48).
The modiﬁed NGBS algorithm is outlined in Figure 2. Note that the query selection step is identical
to that of the original NGBS algorithm  unless there exist two neighboring sets with strongly bipolar
weighted responses. In the latter case  a query is randomly selected from one of these two sets with
equal probability  which guarantees a highly uncertain response.
Theorem 1 Let P denotes the underlying probability measure (governing noises and algorithm ran-
domization). If β > α  then both the NGBS and modiﬁed NGBS algorithms  in Figure 1(b) and
{an}n≥0 is a monotonically decreasing sequence.
The condition β > α ensures that the update (1) is not overly aggressive. We now turn to the

Figure 2  respectively  generate a sequence of hypotheses such that P((cid:98)hn (cid:54)= h∗) ≤ an < 1  where
matter of sufﬁcient conditions guaranteeing that P((cid:98)hn (cid:54)= h∗) → 0 exponentially fast with n. The

3

Modiﬁed NGBS
initialize: p0 uniform over H.
for i = 0  1  2  . . .

1) Let b = minA∈A |(cid:80)
with(cid:80)
arg minA∈A |(cid:80)

h∈H pi(h)h(A) > b and(cid:80)

h∈H pi(h)h(A)|. If there exists neighboring sets A and A(cid:48)
h∈H pi(h)h(A(cid:48)) < −b   then select xi from
A or A(cid:48) with probability 1/2 each. Otherwise select xi from the set Amin =
h∈H pi(h)h(A)|. In the case that the sets above are non-unique 

choose at random any one satisfying the requirements.

2) Obtain noisy response yi.
3) Bayes update pi → pi+1; Eqn. (1).
hypothesis selected at each step:

(cid:98)hi := arg maxh∈H pi(h)

Figure 2: Modiﬁed NGBS algorithm.

exponential convergence rate of classic binary search hinges on the fact that the hypotheses can be
ordered with respect to X . In general situations  the hypothesis space cannot be ordered in such a
fashion  but the neighborhood graph of A provides a similar local structure.
Deﬁnition 2 The pair (X  H) is said to be neighborly if the neighborhood graph of A is connected
(i.e.  for every pair of sets in A there exists a sequence of neighboring sets that begins at one of the
pair and ends with the other).

In essence  the neighborly condition simply means that each hypothesis is locally distinguishable
from all others. By ‘local’ we mean in the vicinity of points x where the output of the hypothesis
changes from +1 to −1. The neighborly condition was ﬁrst introduced in [Now08] in the analysis
of GBS. It is shown in Section 3 that the neighborly condition holds for the important case of
hypothesis spaces consisting of multidimensional threshold functions. If (X  H) is neighborly  then

the modiﬁed NGBS algorithm guarantees that P((cid:98)hi (cid:54)= h∗) → 0 exponentially fast.

Theorem 2 Let P denotes the underlying probability measure (governing noises and algorithm ran-
domization). If β > α and (X  H) is neighborly  then the modiﬁed NGBS algorithm in Figure 2
generates a sequence of hypotheses satisfying

P((cid:98)hn (cid:54)= h∗) ≤ |H| (1 − λ)n ≤ |H| e−λn   n = 0  1  . . .

with exponential constant λ = min

1 − β(1−α)

1−β − α(1−β)

β

  where

(2)

(cid:110) 1−c∗

  1
4

2

(cid:111)(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

X

c∗

:= min

P

max
h∈H

h(x) dP (x)

(cid:17)

(cid:12)(cid:12)(cid:12)(cid:12) .

The exponential convergence rate1 is governed by the key parameter 0 ≤ c∗ < 1. The minimizer in
(2) exists because the minimization can be computed over the space of ﬁnite-dimensional probability
mass functions over the elements of A. As long as no hypothesis is constant over the whole of
X   the value of c∗ is typically a small constant much less than 1 that is independent of the size
of H (see [Now08  Now09] and the next section for concrete examples). In such situations  the
convergence rate of modiﬁed NGBS is optimal  up to constant factors. No other algorithm can solve
the noisy GBS problem with a lower query complexity. The query complexity of the modiﬁed NGBS
algorithm can be derived as follows. Let δ > 0 be a prespeciﬁed conﬁdence parameter. The number
δ )  which is the
optimal query complexity. Intuitively  O(log |H|) bits are required to encode each hypothesis. More
formally  the classic noisy binary search problem satisﬁes the assumptions of Theorem 2 [Now08] 

of queries required to ensure that P((cid:98)hn (cid:54)= h∗) ≤ δ is n ≥ λ−1 log |H|

δ = O(log |H|

1Note that the factor

in the exponential rate parameter λ is a positive constant
strictly less than 1. For a noise level α this factor is maximized by a value β ∈ (α  1/2) which tends to
(1/2 + α)/2 as α tends to 1/2.

1 − β(1−α)

1−β − α(1−β)

β

“

”

4

δ ) [BZ74  KK07].

and hence it is a special case of the general problem. It is known that the optimal query complexity
for noisy classic binary search is O(log |H|
We contrast this with the simple noise-tolerant GBS algorithm based on repeating each query in the
standard GBS algorithm of Figure 1(a) multiple times to control the noise (see [K¨a¨a06  Now08] for
related derivations). It follows from Chernoff’s bound that the query complexity of determining the
correct label for a single query with conﬁdence at least 1 − δ is O( log(1/δ)
|1/2−α|2 ). Suppose that GBS
requires n0 queries in the noiseless situation. Then using the union bound  we require O( log(n0/δ)
|1/2−α|2 )
queries at each step to guarantee that the labels determined for all n0 queries are correct with prob-
ability 1 − δ. If (X  H) is neighborly  then GBS requires n0 = O(log |H|) queries in noiseless
conditions [Now08]. Therefore  under the conditions of Theorem 2  the query complexity of the
simple noise-tolerant GBS algorithm is O(log |H| log log |H|
)  a logarithmic factor worse than the
optimal query complexity.
3 Noisy GBS for Learning Multidimensional Thresholds
We now apply the theory and modiﬁed NGBS algorithm to the problem of learning multidimensional
threshold functions from point evaluations  a problem that arises commonly in computer vision
[SS93  GJ96  AMM+98]  image processing [KK00]  and active learning [Das04  BBZ07  CN08 
Now08]. In this case  the hypotheses are determined by (possibly nonlinear) decision surfaces in
d-dimensional Euclidean space (i.e.  X is a subset of Rd)  and the queries are points in Rd.
It
sufﬁces to consider linear decision surfaces of the form ha b(x) := sign((cid:104)a  x(cid:105) + b)  where a ∈ Rd 
(cid:107)a(cid:107)2 = 1  b ∈ R  |b| ≤ c for some constant c < ∞  and (cid:104)a  x(cid:105) denotes the inner product in Rd.
Note that hypotheses of this form can be used to represent nonlinear decision surfaces by applying
a nonlinear mapping to the query space.
Theorem 3 Let H be a ﬁnite collection of hypotheses of form sign((cid:104)a  x(cid:105) + b)  for some constant
c < ∞. Then the hypotheses selected by the modiﬁed NGBS algorithm with β > α satisfy

δ

P((cid:98)hn (cid:54)= h∗) ≤ |H| e−λn  
(cid:17)
. Moreover (cid:98)hn can be computed in time polynomial in |H|.

(cid:16)

with λ = 1
4

1 − β(1−α)

1−β − α(1−β)

β

Based on the discussion at the end of the previous section  we conclude that the query complexity
of the modiﬁed NGBS algorithm is O(log |H|); this is the optimal up to constant factors. The only
other algorithm with this capability that we are aware of was analyzed in [BBZ07]  and it is based
on a quite different approach tailored speciﬁcally to linear threshold problem.
4 Agnostic Algorithms
We also mention the possibility of agnostic algorithms guaranteed to ﬁnd the best hypothesis in H
even if the optimal hypothesis h∗ is not in H and/or the assumptions of Theorem 2 or 3 do not hold.
The best hypothesis in H is the one that minimizes the error with respect to a given probability mea-
sure on X   denoted by PX. The following theorem  proved in [Now09]  demonstrates an agnostic
algorithm that performs almost as well as empirical risk minimization (ERM) in general  and has
the optimal O(log |H|/δ) query complexity when the conditions of Theorem 2 hold.
Theorem 4 Let PX denote a probability distribution on X and suppose we have a query budget
of n. Let h1 denote the hypothesis selected by modiﬁed NGBS using n/3 of the queries and let h2
denote the hypothesis selected by ERM from n/3 queries drawn independently from PX. Draw the
remaining n/3 queries independently from P∆  the restriction of PX to the set ∆ ⊂ X on which h1

and h2 disagree  and let (cid:98)R∆(h1) and (cid:98)R∆(h2) denote the average number of errors made by h1 and
h2 on these queries. Select(cid:98)h = arg min{(cid:98)R∆(h1) (cid:98)R∆(h2)}. Then  in general 
E[R((cid:98)h)] ≤ min{E[R(h1)]  E[R(h2)]} + (cid:112)3/n  
P((cid:98)h (cid:54)= h∗) ≤ N e−λn/3 + 2e−n|1−2α|2/6 .

where R(h)  h ∈ H  denotes the probability of error of h with respect to PX and E denotes the
expectation with respect to all random quantities. Furthermore  if the assumptions of Theorem 2
hold with noise bound α  then

5

5 Appendix: Proofs

5.1 Proof of Theorem 1
Let E denote expectation with respect to P  and deﬁne Cn := (1 − pn(h∗))/pn(h∗). Note that
Cn ∈ [0 ∞) reﬂects the amount of mass that pn places on the suboptimal hypotheses. First note
that

P((cid:98)hn (cid:54)= h∗) ≤ P(pn(h∗) < 1/2) = P(Cn > 1) ≤ E[Cn]   by Markov’s inequality.

Next  observe that

E[Cn] = E[(Cn/Cn−1) Cn−1] = E [E[(Cn/Cn−1) Cn−1|pn−1]]

= E [Cn−1 E[(Cn/Cn−1)|pn−1]] ≤ E[Cn−1] max

E[(Cn/Cn−1)|pn−1]

(cid:19)n

pn−1

(cid:18)

≤ C0

max

i=0 ... n−1

E[(Ci+1/Ci)|pi]

max
pi

.

Note that because p0 is assumed to be uniform  C0 = |H| − 1. A similar conditioning tech-
nique is employed for interval estimation in [BZ74]. The rest of the proof entails showing that
E[(Ci+1/Ci)|pi] < 1  which proofs the result  and requires a very different approach than [BZ74].
h pi(h) zi(h))/2  the
The precise form of p1  p2  . . .
weighted proportion of hypotheses that agree with yi. The factor that normalizes the updated dis-
h pi(h) β(1−zi(h))/2(1 − β)(1+zi(h))/2 =

tribution in (1) is related to δi as follows. Note that(cid:80)
(cid:80)
h:zi(h)=−1 pi(h)β +(cid:80)

is derived as follows. Let δi = (1 +(cid:80)

h:zi(h)=1 pi(h)(1 − β) = (1 − δi)β + δi(1 − β). Thus 

pi+1(h) = pi(h) β(1−zi(h))/2(1 − β)(1+zi(h))/2

(1 − δi)β + δi(1 − β)

Denote the reciprocal of the update factor for pi+1(h∗) by

γi :=

(1 − δi)β + δi(1 − β)

β(1−Zi(h∗))/2(1 − β)(1+Zi(h∗))/2

 

(3)

where zi(h∗) = h∗(xi)yi  and observe that pi+1(h∗) = pi(h∗)/γi. Thus 
= γi − pi(h∗)
1 − pi(h∗) .

(1 − pi(h∗)/γi)pi(h∗)
pi(h∗)/γi(1 − pi(h∗))

Ci+1
Ci

=

(1 +(cid:80)

E[Ci+1/Ci|pi] < 1 we will show that maxpi

Now to bound maxpi
this  we will assume that pi is arbitrary.
For every A ∈ A and every h ∈ H let h(A) denote the value of h on the set A. Deﬁne δ+

A =
h pi(h)h(A))/2  the proportion of hypotheses that take the value +1 on A. Note that for
A < 1  since at least one hypothesis has the value −1 on A and p(h) > 0 for
every A we have 0 < δ+
all h ∈ H. Let Ai denote that set that xi is selected from  and consider the four possible situations:

E[γi|pi] < 1. To accomplish

h∗(xi) = +1  yi = +1 : γi = (1−δ+
h∗(xi) = +1  yi = −1 : γi = δ+
h∗(xi) = −1  yi = +1 : γi = (1−δ+
h∗(xi) = −1  yi = −1 : γi = δ+

Ai

Ai

Ai

)β+δ+
Ai
1−β
β+(1−δ+
Ai

(1−β)

)(1−β)

)β+δ+
Ai

Ai

(1−β)

β

β

β+(1−δ+
Ai
1−β

)(1−β)

To bound E[γi|pi] it is helpful to condition on Ai. Deﬁne qi := Px y|Ai(h∗(x) (cid:54)= Y ). If h∗(Ai) =
+1  then

E[γi|pi  Ai] =

(1 − δ+

Ai

)β + δ+
Ai
1 − β
+ (1 − δ+
)

Ai

(1 − β)

(cid:20) β(1 − qi)

1 − β

δ+
Ai

(1 − qi) +
+ qi(1 − β)

(cid:21)

β

= δ+
Ai

β

.

β + (1 − δ+

)(1 − β)

Ai

qi

6

Deﬁne γ+

i (Ai) := δ+
Ai

+ (1 − δ+

Ai

)

E[γi|pi  Ai] = (1 − δ+

Ai

) + δ+
Ai

(cid:104) β(1−qi)
1−β + qi(1−β)

(cid:105)
(cid:20) β(1 − qi)

β

. Similarly  if h∗(Ai) = −1  then

(cid:21)

=: γ−

i (Ai)

+ qi(1 − β)

β

1 − β

By assumption qi ≤ α < 1/2  and since α < β < 1/2 the factor β(1−qi)
α(1−β)

1−β + qi(1−β)

β

β < 1. Deﬁne

ε0 := 1 − β(1 − α)
1 − β

− α(1 − β)

β

 

to obtain the bounds

i (Ai) ≤ δ+
γ+
i (Ai) ≤ δ+
γ−

Ai

Ai

+ (1 − δ+
(1 − ε0) + (1 − δ+

)(1 − ε0)  
) .

Ai

Ai

Since both γ+

i (Ai) and γ−

i (Ai) are less than 1  it follows that E[γi|pi] < 1.

≤ β(1−α)

1−β +

(4)
(5)
(cid:3)

5.2 Proof of Theorem 2

X

X

to be W (p  A) :=(cid:80)

i (Ai)  deﬁned above in (4) and
The proof amounts to obtaining upper bounds for γ+
(5). For every A ∈ A and any probability measure p on H the weighted prediction on A is deﬁned
h∈H p(h)h(A)  where h(A) is the constant value of h for every x ∈ A. The

i (Ai) and γ−

h∈H p(h)|(cid:82)

X h(x) dP (x)| ≤ maxh∈H |(cid:82)

(cid:80)
(cid:80)
h∈H p(h)h(x)dP (x) ≤(cid:80)

distribution P on X we have(cid:82)
c∗ since(cid:82)

following lemma plays a crucial role in the analysis of the modiﬁed NGBS algorithm.
Lemma 1 If (X  H) is neighborly  then for every probability measure p on H there either exists a set
A ∈ A such that |W (p  A)| ≤ c∗ or a pair of neighboring sets A  A(cid:48) ∈ A such that W (p  A) > c∗
and W (p  A(cid:48)) < −c∗.
Proof of Lemma 1: Suppose that minA∈A |W (p  A)| > c∗. Then there must exist A  A(cid:48) ∈ A
such that W (p  A) > c∗ and W (p  A(cid:48)) < −c∗  otherwise c∗ cannot be the minimax moment
of H. To see this suppose  for instance  that W (p  A) > c∗ for all A ∈ A. Then for every
h∈H p(h)h(x)dP (x) > c∗. This contradicts the deﬁnition of
X h(x) dP (x)|.
The neighborly condition guarantees that there exists a sequence of neighboring sets beginning at A
and ending at A(cid:48). Since |W (p  A)| > c∗ on every set and the sign of W (p ·) must change at some
(cid:3)
point in the sequence  it follows that there exist neighboring sets satisfying the claim.
Now consider two distinct situations. Deﬁne bi := minA∈A |W (pi  A)|. First suppose that there do
not exist neighboring sets A and A(cid:48) with W (pi  A) > bi and W (pi  A(cid:48)) < −bi. Then by Lemma 1 
this implies that bi ≤ c∗  and according the query selection step of the modiﬁed NGBS algorithm 
Ai = arg minA |W (pi  A)|. Note that because |W (pi  Ai)| ≤ c∗  (1− c∗)/2 ≤ δ+
≤ (1 + c∗)/2.
Hence  both γ+
Now suppose that there exist neighboring sets A and A(cid:48) with W (pi  A) > bi and W (pi  A(cid:48)) < −bi.
Recall that in this case Ai is randomly chosen to be A or A(cid:48) with equal probability. Note that
A > (1 + bi)/2 and δ+
δ+
) ≤ 1 − ε0/4  
E[γi|pi  Ai ∈ {A  A(cid:48)}] <
since bi > 0. Similarly  if h∗(A) = h∗(A(cid:48)) = −1  then (5) yields E[γi|pi  Ai ∈ {A  A(cid:48)}] <
1 − ε0/4. If h∗(A) = −1 on A and h∗(A(cid:48)) = +1  then applying (5) on A and (4) on A(cid:48) yields

A(cid:48) < (1 − bi)/2. If h∗(A) = h∗(A(cid:48)) = +1  then applying (4) results in

i (Ai) are bounded above by 1 − ε0(1 − c∗)/2.

i (Ai) and γ−

(1 − ε0)) =

(2 − ε0

1 − bi

1 + bi

1 + bi

(1 +

1
2

1
2

+

Ai

2

2

2

(cid:0)δ+

7

E[γi|pi  Ai ∈ {A  A(cid:48)}] ≤ 1
2
1
2
1
2

=

A(cid:48) + (1 − δ+
A(cid:48)))

A − δ+

A(1 − ε0) + (1 − δ+

A) + δ+

(1 − δ+
A + δ+
(2 − ε0(1 + δ+
(1 + δ+

A(cid:48) + (1 − ε0)(1 + δ+
A − δ+
A − δ+

A(cid:48)))
A(cid:48)) ≤ 1 − ε0/2  

=
= 1 − ε0
2

A(cid:48))(1 − ε0)(cid:1)

(cid:0)δ+

1
2

A(cid:48))(cid:1)

since 0 ≤ δ+
A and (5) on A(cid:48) to obtain

A − δ+

A(cid:48) ≤ 1. The ﬁnal possibility is that h∗(A) = +1 and h∗(A(cid:48)) = −1. Apply (4) on

E[γi|pi  Ai ∈ {A  A(cid:48)}] ≤ 1
2

=

A + (1 − δ+
A − δ+
(1 + δ+

A)(1 − ε0) + δ+
A(cid:48) + (1 − ε0)(1 − δ+

A(cid:48)(1 − ε0) + (1 − δ+

A + δ+

A(cid:48)))

A − δ+

A − δ+

A(cid:48) + (1 − 0)(1 − δ+

A(cid:48) = pi(h∗) − pi(−h∗); if −h∗ does

Next  use the fact that because A and A(cid:48) are neighbors  δ+
not belong to H  then pi(−h∗) = 0. Hence 
E[γi|pi  Ai ∈ {A  A(cid:48)}] ≤ 1
2
1
=
2
≤ 1
2

(1 + δ+
(1 + pi(h∗) − pi(−h∗) + (1 − 0)(1 − pi(h∗) + pi(−h∗)))
(1 + pi(h∗) + (1 − 0)(1 − pi(h∗))) = 1 − ε0
2

(1 − pi(h∗))  
since the bound is maximized when pi(−h∗) = 0. Now bound E[γi|pi] by the maximum of the
conditional bounds above to obtain
E[γi|pi] ≤ max

(1 − pi(h∗))   1 − ε0

A + δ+

A(cid:48)))

(cid:110)

(cid:111)

 

1 − ε0
2

and thus it is easy to see that

(cid:20) Ci+1

Ci

E

(cid:21)

|pi

=

E [γi|pi] − pi(h∗)

1 − pi(h∗)

≤ 1 − min

4   1 − (1 − c∗) ε0
2
(cid:111)
(cid:110) ε0

(1 − c∗) 

.

ε0
4

2

(cid:3)

5.3 Proof of Theorem 3
First we show that the pair (Rd H) is neighborly (Deﬁnition 2). Each A ∈ A is a polytope in Rd.
These polytopes are generated by intersections of the halfspaces corresponding to the hypotheses.
Any two polytopes that share a common face are neighbors (the hypothesis whose decision boundary
deﬁnes the face  and its complement if it exists  are the only ones that predict different values on
these two sets). Since the polytopes tessellate Rd  the neighborhood graph of A is connected.
Next consider the ﬁnal bound in the proof of Theorem 2  above. We next show that the value of c∗ 
deﬁned in (2)  is 0. Since the offsets b of the hypotheses are all less than c in magnitude  it follows
that the distance from the origin to the nearest point of the decision surface of every hypothesis is at
most c. Let Pr denote the uniform probability distribution on a ball of radius r centered at the origin
in Rd. Then for every h of the form sign((cid:104)a  x(cid:105) + b)

Rd

h(x) dPr(x)

and limr→∞(cid:12)(cid:12)(cid:82)
Lastly  note that the modiﬁed NGBS algorithm involves computing(cid:80)
known that |A| =(cid:80)d

X h(x) dPr(x)(cid:12)(cid:12) = 0 and so c∗ = 0.
(cid:0)|H|
(cid:1) = O(|H|d) [Buc43].

h∈H pi(h)h(A) for all A ∈ A
at each step. The computational complexity of each step is therefore proportional to the cardinality
of A  which is equal to the number of polytopes generated by intersections of half-spaces. It is
(cid:3)

 

i=0

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ c

r

8

References
[AMM+98] E. M. Arkin  H. Meijer  J. S. B. Mitchell  D. Rappaport  and S.S. Skiena. Decision trees
for geometric models. Intl. J. Computational Geometry and Applications  8(3):343–
363  1998.
D. Angluin. Queries revisited. Springer Lecture Notes in Comp. Sci.: Algorithmic
Learning Theory  pages 12–31  2001.

[Ang01]

[BBZ07] M.-F. Balcan  A. Broder  and T. Zhang. Margin based active learning. In Conf. on

[FRPU94] U. Feige  E. Raghavan  D. Peleg  and E. Upfal. Computing with noisy information.

[HPRW96] L. Hellerstein  K. Pillaipakkamnatt  V. Raghavan  and D. Wilkins. How many queries

[Buc43]
[BZ74]

[CN08]

[Das04]

[GG74]

[GJ96]

[Heg95]

[Hor63]

[HR76]

[K¨a¨a06]

[KK00]

[KK07]

[KPB99]

[Lov85]

[Now08]

[Now09]

[R´en61]

[SS93]

Learning Theory (COLT)  2007.
R. C. Buck. Partition of space. The American Math. Monthly  50(9):541–544  1943.
M. V. Burnashev and K. Sh. Zigangirov. An interval estimation problem for controlled
observations. Problems in Information Transmission  10:223–231  1974.
R. Castro and R. Nowak. Minimax bounds for active learning.
Theory  pages 2339–2353  2008.
S. Dasgupta. Analysis of a greedy active learning strategy.
Processing Systems  2004.

In Neural Information

IEEE Trans. Info.

SIAM J. Comput.  23(5):1001–1018  1994.
M. R. Garey and R. L. Graham. Performance bounds on the splitting algorithm for
binary testing. Acta Inf.  3:347–355  1974.
D. Geman and B. Jedynak. An active testing model for tracking roads in satellite
images. IEEE Trans. PAMI  18(1):1–14  1996.
T. Heged¨us. Generalized teaching dimensions and the query complexity of learning.
In 8th Annual Conference on Computational Learning Theory  pages 108–117  1995.
M. Horstein. Sequential decoding using noiseless feedback. IEEE Trans. Info. Theory 
9(3):136–143  1963.

are needed to learn? J. ACM  43(5):840–862  1996.
L. Hyaﬁl and R. L. Rivest. Constructing optimal binary decision trees is NP-complete.
Inf. Process. Lett.  5:15–17  1976.
M. K¨a¨ari¨ainen. Active learning in the non-realizable case. In Algorithmic Learning
Theory  pages 63–77  2006.
A. P. Korostelev and J.-C. Kim. Rates of convergence fo the sup-norm risk in image
models under sequential designs. Statistics & Probability Letters  46:391–399  2000.
R. Karp and R. Kleinberg. Noisy binary search and its applications. In Proceedings
of the 18th ACM-SIAM Symposium on Discrete Algorithms (SODA 2007)  pages 881–
890  2007.
S. R. Kosaraju  T. M. Przytycka  and R. Borgstrom. On an optimal split tree problem.
Lecture Notes in Computer Science: Algorithms and Data Structures  1663:157–168 
1999.
D. W. Loveland. Performance bounds for binary testing with arbitrary weights. Acta
Informatica  22:101–114  1985.
R. Nowak. Generalized binary search. In Proceedings of the 46th Allerton Conference
on Communications  Control  and Computing  pages 568–574  2008.
R. Nowak. The geometry of generalized binary search. 2009. Preprint available at
http://arxiv.org/abs/0910.4397.
A. R´enyi. On a problem in information theory. MTA Mat. Kut. Int. Kozl.  page 505516 
1961. reprinted in Selected Papers of Alfred R´enyi  vol. 2  P. Turan  ed.  pp. 631-638.
Akademiai Kiado  Budapest  1976.
M.J. Swain and M.A. Stricker. Promising directions in active vision. Int. J. Computer
Vision  11(2):109–126  1993.

9

,Maximilian Nickel
Xueyan Jiang
Volker Tresp
Chen Huang
Chen Change Loy
Xiaoou Tang
Songbai Yan
Chicheng Zhang