2017,Streaming Robust Submodular Maximization: A Partitioned Thresholding Approach,We study the classical problem of maximizing a monotone submodular function subject to a cardinality constraint k  with two additional twists: (i) elements arrive in a streaming fashion  and (ii) m items from the algorithm’s memory are removed after the stream is finished. We develop a robust submodular algorithm STAR-T. It is based on a novel partitioning structure and an exponentially decreasing thresholding rule. STAR-T makes one pass over the data and retains a short but robust summary. We show that after the removal of any m elements from the obtained summary  a simple greedy algorithm STAR-T-GREEDY that runs on the remaining elements achieves a constant-factor approximation guarantee. In two different data summarization tasks  we demonstrate that it matches or outperforms existing greedy and streaming methods  even if they are allowed the benefit of knowing the removed subset in advance.,Streaming Robust Submodular Maximization:

A Partitioned Thresholding Approach

Slobodan Mitrovi´c∗

EPFL

Ilija Bogunovic†

EPFL

Ashkan Norouzi-Fard‡

EPFL

Jakub Tarnawski§

EPFL

Volkan Cevher¶

EPFL

Abstract

We study the classical problem of maximizing a monotone submodular function
subject to a cardinality constraint k  with two additional twists: (i) elements arrive
in a streaming fashion  and (ii) m items from the algorithm’s memory are removed
after the stream is ﬁnished. We develop a robust submodular algorithm STAR-T.
It is based on a novel partitioning structure and an exponentially decreasing thresh-
olding rule. STAR-T makes one pass over the data and retains a short but robust
summary. We show that after the removal of any m elements from the obtained
summary  a simple greedy algorithm STAR-T-GREEDY that runs on the remaining
elements achieves a constant-factor approximation guarantee. In two different
data summarization tasks  we demonstrate that it matches or outperforms existing
greedy and streaming methods  even if they are allowed the beneﬁt of knowing the
removed subset in advance.

1

Introduction

A central challenge in many large-scale machine learning tasks is data summarization – the extraction
of a small representative subset out of a large dataset. Applications include image and document
summarization [1  2]  inﬂuence maximization [3]  facility location [4]  exemplar-based clustering [5] 
recommender systems [6]  and many more. Data summarization can often be formulated as the
problem of maximizing a submodular set function subject to a cardinality constraint.
On small datasets  a popular algorithm is the simple greedy method [7]  which produces solutions
provably close to optimal. Unfortunately  it requires repeated access to all elements  which makes it
infeasible for large-scale scenarios  where the entire dataset does not ﬁt in the main memory. In this
setting  streaming algorithms prove to be useful  as they make only a small number of passes over the
data and use sublinear space.
In many settings  the extracted representative set is also required to be robust. That is  the objective
value should degrade as little as possible when some elements of the set are removed. Such removals
may arise for any number of reasons  such as failures of nodes in a network  or user preferences
which the model failed to account for; they could even be adversarial in nature.

∗e-mail: slobodan.mitrovic@epﬂ.ch
†e-mail: ilija.bogunovic@epﬂ.ch
‡e-mail: ashkan.norouzifard@epﬂ.ch
§e-mail: jakub.tarnawski@epﬂ.ch
¶e-mail: volkan.cevher@epﬂ.ch

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

A robustness requirement is especially challenging for large datasets  where it is prohibitively
expensive to reoptimize over the entire data collection in order to ﬁnd replacements for the removed
elements. In some applications  where data is produced so rapidly that most of it is not being stored 
such a search for replacements may not be possible at all.
These requirements lead to the following two-stage setting. In the ﬁrst stage  we wish to solve the
robust streaming submodular maximization problem – one of ﬁnding a small representative subset of
elements that is robust against any possible removal of up to m elements. In the second  query stage 
after an arbitrary removal of m elements from the summary obtained in the ﬁrst stage  the goal is to
return a representative subset  of size at most k  using only the precomputed summary rather than the
entire dataset.
For example  (i) in dominating set problem (also studied under inﬂuence maximization) we want
to efﬁciently (in a single pass) compute a compressed but robust set of inﬂuential users in a social
network (whom we will present with free copies of a new product)  (ii) in personalized movie
recommendation we want to efﬁciently precompute a robust set of user-preferred movies. Once we
discard those users who will not spread the word about our product  we should ﬁnd a new set of
inﬂuential users in the precomputed robust summary. Similarly  if some movies turn out not to be
interesting for the user  we should still be able to provide good recommendations by only looking
into our robust movie summary.

Contributions.
In this paper  we propose a two-stage procedure for robust submodular maximiza-
tion. For the ﬁrst stage  we design a streaming algorithm which makes one pass over the data
and ﬁnds a summary that is robust against removal of up to m elements  while containing at most

O(cid:0)(m log k + k) log2 k(cid:1) elements.

In the second (query) stage  given any set of size m that has been removed from the obtained summary 
we use a simple greedy algorithm that runs on the remaining elements and produces a solution of
size at most k (without needing to access the entire dataset). We prove that this solution satisﬁes a
constant-factor approximation guarantee.
Achieving this result requires novelty in the algorithm design as well as the analysis. Our streaming
algorithm uses a structure where the constructed summary is arranged into partitions consisting of
buckets whose sizes increase exponentially with the partition index. Moreover  buckets in different
partitions are associated with greedy thresholds  which decrease exponentially with the partition index.
Our analysis exploits and combines the properties of the described robust structure and decreasing
greedy thresholding rule.
In addition to algorithmic and theoretical contributions  we also demonstrate in several practical
scenarios that our procedure matches (and in some cases outperforms) the SIEVE-STREAMING
algorithm [8] (see Section 5) – even though we allow the latter to know in advance which elements
will be removed from the dataset.

2 Problem Statement

We consider a potentially large universe of elements V of size n equipped with a normalized monotone
submodular set function f : 2V → R≥0 deﬁned on V . We say that f is monotone if for any two sets
X ⊆ Y ⊆ V we have f (X) ≤ f (Y ). The set function f is said to be submodular if for any two sets
X ⊆ Y ⊆ V and any element e ∈ V \ Y it holds that

f (X ∪ {e}) − f (X) ≥ f (Y ∪ {e}) − f (Y ).

We use f (Y | X) to denote the marginal gain in the function value due to adding the elements of set
Y to set X  i.e. f (Y | X) := f (X ∪ Y ) − f (X). We say that f is normalized if f (∅) = 0.
The problem of maximizing a monotone submodular function subject to a cardinality constraint  i.e. 
(1)

f (Z) 

max

Z⊆V |Z|≤k

has been studied extensively. It is well-known that a simple greedy algorithm (henceforth refered to
as GREEDY) [7]  which starts from an empty set and then iteratively adds the element with highest
marginal gain  provides a (1 − e−1)-approximation. However  it requires repeated access to all
elements of the dataset  which precludes it from use in large-scale machine learning applications.

2

We say that a set S is robust for a parameter m if  for any set E ⊆ V such that |E| ≤ m  there is a
subset Z ⊆ S \ E of size at most k such that

f (Z) ≥ cf (OPT(k  V \ E)) 

where c > 0 is an approximation ratio. We use OPT(k  V \ E) to denote the optimal subset of size
k of V \ E (i.e.  after the removal of elements in E):

OPT(k  V \ E) ∈ argmax

Z⊆V \E |Z|≤k

f (Z).

In this work  we are interested in solving a robust version of Problem (1) in the setting that consists
of the following two stages: (i) streaming and (ii) query stage.
In the streaming stage  elements from the ground set V arrive in a streaming fashion in an arbitrary
order. Our goal is to design a one-pass streaming algorithm that has oracle access to f and retains a
small set S of elements in memory. In addition  we want S to be a robust summary  i.e.  S should both
contain elements that maximize the objective value  and be robust against the removal of prespeciﬁed
number of elements m. In the query stage  after any set E of size at most m is removed from V   the
goal is to return a set Z ⊆ S \ E of size at most k such that f (Z) is maximized.
Related work. A robust  non-streaming version of Problem (1) was ﬁrst introduced in [9]. In that
setting  the algorithm must output a set Z of size k which maximizes the smallest objective value
guaranteed to be obtained after a set of size m is removed  that is 

max

Z⊆V |Z|≤k

min

E⊆Z |E|≤m

f (Z \ E).

√
The work [10] provides the ﬁrst constant (0.387) factor approximation result to this problem  valid
for m = o(
k). Their solution consists of buckets of size O(m2 log k) that are constructed greedily 
one after another. Recently  in [11]  a centralized algorithm PRO has been proposed that achieves the
same approximation result and allows for a greater robustness m = o(k). PRO constructs a set that is
arranged into partitions consisting of buckets whose sizes increase exponentially with the partition
index. In this work  we use a similar structure for the robust set but  instead of ﬁlling the buckets
greedily one after another  we place an element in the ﬁrst bucket for which the gain of adding the
element is above the corresponding threshold. Moreover  we introduce a novel analysis that allows us
to be robust to any number of removals m as long as we are allowed to use O(m log2 k) memory.
Recently  submodular streaming algorithms (e.g. [5]  [12] and [13]) have become a prominent
option for scaling submodular optimization to large-scale machine learning applications. A popular
submodular streaming algorithm SIEVE-STREAMING [8] solves Problem (1) by performing one pass
over the data  and achieves a (0.5 − )-approximation while storing at most O
Our algorithm extends the algorithmic ideas of SIEVE-STREAMING  such as greedy thresholding  to
the robust setting. In particular  we introduce a new exponentially decreasing thresholding scheme
that  together with an innovative analysis  allows us to obtain a constant-factor approximation for the
robust streaming problem.
Recently  robust versions of submodular maximization have been considered in the problems of
inﬂuence maximization (e.g  [3]  [14]) and budget allocation ([15]). Increased interest in interactive
machine learning methods has also led to the development of interactive and adaptive submodular
optimization (see e.g. [16]  [17]). Our procedure also contains the interactive component  as we can
compute the robust summary only once and then provide different sub-summaries that correspond to
multiple different removals (see Section 5.2).
Independently and concurrently with our work  [18] gave a streaming algorithm for robust submodular
maximization under the cardinality constraint. Their approach provides a 1/2 − ε approximation
guarantee. However  their algorithm uses O(mk log k/ε) memory. While the memory requirement
of their method increases linearly with k  in the case of our algorithm this dependence is logarithmic.

(cid:16) k log k

(cid:17)

elements.



3

Figure 1: Illustration of the set S returned by STAR-T. It consists of (cid:100)log k(cid:101) + 1 partitions such that
each partition i contains w(cid:100)k/2i(cid:101) buckets of size 2i (up to rounding). Moreover  each partition i has
its corresponding threshold τ /2i.

3 A Robust Two-stage Procedure

memory parameter that depends on m; we use w ≥ (cid:108) 4(cid:100)log k(cid:101)m

Our approach consists of the streaming Algorithm 1  which we call Streaming Robust submodular
algorithm with Partitioned Thresholding (STAR-T). This algorithm is used in the streaming stage 
while Algorithm 2  which we call STAR-T-GREEDY  is used in the query stage.
As the input  STAR-T requires a non-negative monotone submodular function f  cardinality
constraint k  robustness parameter m and thresholding parameter τ. The parameter τ is an α-
approximation to f (OPT(k  V \ E))  for some α ∈ (0  1] to be speciﬁed later. Hence  it depends on
f (OPT(k  V \ E))  which is not known a priori. For the sake of clarity  we present the algorithm
as if f (OPT(k  V \ E)) were known  and in Section 4.1 we show how f (OPT(k  V \ E)) can be
approximated. The algorithm makes one pass over the data and outputs a set of elements S that is
later used in the query stage in STAR-T-GREEDY.
The set S (see Figure 1 for an illustration) is divided into (cid:100)log k(cid:101) + 1 partitions  where every partition
i ∈ {0  . . .  (cid:100)log k(cid:101)} consists of w(cid:100)k/2i(cid:101) buckets Bi j  j ∈ {1  . . .   w(cid:100)k/2i(cid:101)}. Here  w ∈ N+ is a
in our asymptotic theory  while
our numerical results show that w = 1 works well in practice. Every bucket Bi j stores at most
min{k  2i} elements. If |Bi j| = min{2i  k}  then we say that Bi j is full.
Every partition has a corresponding threshold that is exponentially decreasing with the partition index
i as τ /2i. For example  the buckets in the ﬁrst partition will only store elements that have marginal
value at least τ. Every element e ∈ V arriving on the stream is assigned to the ﬁrst non-full bucket
Bi j for which the marginal value f (e | Bi j) is at least τ /2i. If there is no such bucket  the element
will not be stored. Hence  the buckets are disjoint sets that in the end (after one pass over the data) can
have a smaller number of elements than speciﬁed by their corresponding cardinality constraints  and
some of them might even be empty. The set S returned by STAR-T is the union of all the buckets.
In the second stage  STAR-T-GREEDY receives as input the set S constructed in the streaming stage 
a set E ⊂ S that we think of as removed elements  and the cardinality constraint k. The algorithm
then returns a set Z  of size at most k  that is obtained by running the simple greedy algorithm
GREEDY on the set S \ E. Note that STAR-T-GREEDY can be invoked for different sets E.

(cid:109)

k

4 Theoretical Bounds

In this section we discuss our main theoretical results. We initially assume that the value
f (OPT(k  V \ E)) is known; later  in Section 4.1  we remove this assumption. The more de-
tailed versions of our proofs are given in the supplementary material. We begin by stating the main
result.

4

Data Stream k  buckets (k / 2)  buckets 2 1Set Spartitions / 2 / kdecreasing thresholdsfor all 0 ≤ i ≤ (cid:100)log k(cid:101) and 1 ≤ j ≤ w(cid:100)k/2i(cid:101)

Algorithm 1 STreAming Robust - Thresholding submodular algorithm (STAR-T)
Input: Set V   k  τ  w ∈ N+
1: Bi j ← ∅
2: for each element e in the stream do
3:
4:
5:
6:
7:

for j ← 1 to w(cid:100)k/2i(cid:101) do
Bi j ← Bi j ∪ {e}
break: proceed to the next element in the stream

if |Bi j| < min{2i  k} and f (e | Bi j) ≥ τ / min{2i  k} then

for i ← 0 to (cid:100)log k(cid:101) do

8: S ←(cid:83)

9: return S

i j Bi j

(cid:46) loop over partitions
(cid:46) loop over buckets

Algorithm 2 STAR-T- GREEDY
Input: Set S  query set E and k
1: Z ← GREEDY(k  S \ E)
2: return Z

Given a cardinality constraint k and parameter m  for a setting of parameters w ≥(cid:108) 4(cid:100)log k(cid:101)m

Theorem 4.1 Let f be a normalized monotone submodular function deﬁned over the ground set V .
and

(cid:109)

k

τ =

2+

1
(1−e−1)
(1−e−1/3)

(cid:16)
1− 1(cid:100)log k(cid:101)

(cid:17) f (OPT(k  V \ E)) 

STAR-T performs a single pass over the data set and constructs a set S of size at most O((k +
m log k) log k) elements.
For such a set S and any set E ⊆ V such that |E| ≤ m  STAR-T-GREEDY yields a set Z ⊆ S \ E
of size at most k with

f (Z) ≥ c · f (OPT(k  V \ E)) 

for c = 0.149

1 − 1(cid:100)log k(cid:101)

. Therefore  as k → ∞  the value of c approaches 0.149.

(cid:16)

(cid:17)

f (Z) ≥(cid:0)1 − e−1(cid:1)(cid:18)

(cid:19)

1 − 4m
wk

Proof sketch. We ﬁrst consider the case when there is a partition i(cid:63) in S such that at least half
of its buckets are full. We show that there is at least one full bucket Bi(cid:63) j such that f (Bi(cid:63) j \ E)
is only a constant factor smaller than f (OPT(k  V \ E))  as long as the threshold τ is set close to
f (OPT(k  V \ E)). We make this statement precise in the following lemma:
Lemma 4.2 If there exists a partition in S such that at least half of its buckets are full  then for the
set Z produced by STAR-T-GREEDY we have

τ.

(2)

To prove this lemma  we ﬁrst observe that from the properties of GREEDY it follows that

f (Z) = f (GREEDY(k  S \ E)) ≥(cid:0)1 − e−1(cid:1) f (Bi(cid:63) j \ E) .

Now it remains to show that f (Bi(cid:63) j \ E) is close to τ. We observe that for any full bucket Bi(cid:63) j  we
have |Bi(cid:63) j| = min{2i  k}  so its objective value f (Bi(cid:63) j) is at least τ (every element added to this
bucket increases its objective value by at least τ / min{2i  k}). On average  |Bi(cid:63) j ∩ E| is relatively
small  and hence we can show that there exists some full bucket Bi(cid:63) j such that f (Bi(cid:63) j \ E) is close
to f (Bi(cid:63) j).
Next  we consider the other case  i.e.  when for every partition  more than half of its buckets are not
full after the execution of STAR-T. For every partition i  we let Bi denote a bucket that is not fully
populated and for which |Bi ∩ E| is minimized over all the buckets of that partition. Then  we look
at such a bucket in the last partition: B(cid:100)log k(cid:101).
We provide two lemmas that depend on f (B(cid:100)log k(cid:101)). If τ is set to be small compared to f (OPT(k  V \
E)):

5

within a constant factor of f (OPT(k  V \ E));

• Lemma 4.3 shows that if f (B(cid:100)log k(cid:101)) is close to f (OPT(k  V \ E))  then our solution is
• Lemma 4.4 shows that if f (B(cid:100)log k(cid:101)) is small compared to f (OPT(k  V \ E))  then our

solution is again within a constant factor of f (OPT(k  V \ E)).

Lemma 4.3 If there does not exist a partition of S such that at least half of its buckets are full  then
for the set Z produced by STAR-T-GREEDY we have

f (Z) ≥(cid:16)

1 − e−1/3(cid:17)(cid:18)

f(cid:0)B(cid:100)log k(cid:101)(cid:1) − 4m

(cid:19)

where B(cid:100)log k(cid:101) is a not-fully-populated bucket in the last partition that minimizes(cid:12)(cid:12)B(cid:100)log k(cid:101) ∩ E(cid:12)(cid:12) and

|E| ≤ m.
Using standard properties of submodular functions and the GREEDY algorithm we can show that

wk

τ

 

f (Z) = f (GREEDY(k  S \ E)) ≥(cid:16)

1 − e−1/3(cid:17)(cid:18)

f(cid:0)B(cid:100)log k(cid:101)(cid:1) − 4m

(cid:19)

τ

.

wk

The complete proof of this result can be found in Lemma B.2  in the supplementary material.

Lemma 4.4 If there does not exist a partition of S such that at least half of its buckets are full  then
for the set Z produced by STAR-T-GREEDY 

f (Z) ≥ (1 − e−1)(cid:0)f (OP T (k  V \ E)) − f (B(cid:100)log k(cid:101)) − τ(cid:1) 

where B(cid:100)log k(cid:101) is any not-fully-populated bucket in the last partition.

To prove this lemma  we look at two sets X and Y   where Y contains all the elements from
OPT(k  V \ E) that are placed in the buckets that precede bucket B(cid:100)log k(cid:101) in S  and set X :=
OPT(k  V \ E) \ Y . By monotonicity and submodularity of f  we bound f (Y ) by:

f (Y ) ≥ f (OPT(k  V \ E)) − f (X) ≥ f (OPT(k  V \ E)) − f(cid:0)B(cid:100)log k(cid:101)(cid:1) −(cid:88)
f(cid:0)e(cid:12)(cid:12) B(cid:100)log k(cid:101)(cid:1) .
To bound the sum on the right hand side we use that for every e ∈ X we have f(cid:0)e(cid:12)(cid:12) B(cid:100)log k(cid:101)(cid:1) < τ
We conclude the proof by showing that f (Z) = f (GREEDY(k  S \ E)) ≥(cid:0)1 − e−1(cid:1) f (Y ).

which holds due to the fact that B(cid:100)log k(cid:101) is a bucket in the last partition and is not fully populated.

e∈X

k  

|S| =

Equipped with the above results  we proceed to prove our main result.
Proof of Theorem 4.1. First  we prove the bound on the size of S:

(cid:100)log k(cid:101)(cid:88)
By setting w ≥(cid:108) 4(cid:100)log k(cid:101)m
wk   α1 := (cid:0)1 − e−1/3(cid:1)  and
α2 := (cid:0)1 − e−1(cid:1). Lemma 4.3 and 4.4 provide two bounds on f (Z)  one increasing and one

Next  we show the approximation guarantee. We ﬁrst deﬁne γ := 4m

w(k/2i + 1)2i ≤ (log k + 5)wk.

we obtain S = O((k + m log k) log k).

w(cid:100)k/2i(cid:101) min{2i  k} ≤

(cid:100)log k(cid:101)(cid:88)

(cid:109)

(3)

i=0

i=0

k

decreasing in f (B(cid:100)log k(cid:101)). By balancing out the two bounds  we derive

(cid:18) α1α2

(cid:19)

α1 + α2

f (Z) ≥

(f (OPT(k  V \ E)) − (1 + γ)τ ) 

(4)

with equality for f (B(cid:100)log k(cid:101)) = α2f (OPT(k V \E))−(α2−γα1)τ
Next  as γ ≥ 0  we can observe that Eq. (4) is decreasing  while the bound on f (Z) given by
Lemma 4.2 is increasing in τ for γ < 1. Hence  by balancing out the two inequalities  we obtain our
ﬁnal bound

α2+α1

.

f (Z) ≥

1

2

α2(1−γ) + 1
α1

f (OPT(k  V \ E)).

(5)

6

For w ≥(cid:108) 4(cid:100)log k(cid:101)m

(cid:109)

k

prove our main result:

we have γ ≤ 1/(cid:100)log k(cid:101)  and hence  by substituting α1 and α2 in Eq. (5)  we

f (Z) ≥

f (OPT(k  V \ E))

(cid:0)1 − e−1/3(cid:1)(cid:0)1 − e−1(cid:1)(cid:16)
2(cid:0)1 − e−1/3(cid:1) + (1 − e−1)
(cid:19)
(cid:18)

1 − 1(cid:100)log k(cid:101)

(cid:17)

≥ 0.149

1 − 1

(cid:100)log k(cid:101)

f (OPT(k  V \ E)).

2

4.1 Algorithm without access to f (OPT(k  V \ E))
Algorithm STAR-T requires in its input a parameter τ which is a function of an unknown value
f (OPT(k  V \ E)). To deal with this shortcoming  we show how to extend the idea of [8] of
maintaining multiple parallel instances of our algorithm in order to approximate f (OPT(k  V \ E)).
For a given constant  > 0  this approach increases the space by a factor of log1+ k and provides a
(1 + )-approximation compared to the value obtained in Theorem 4.1. More precisely  we prove the
following theorem.

pass over the stream and outputs a collection of sets S of total size O(cid:0)(k + m log k) log k log1+ k(cid:1)

Theorem 4.5 For any given constant  > 0 there exists a parallel variant of STAR-T that makes one
with the following property: There exists a set S ∈ S such that applying STAR-T-GREEDY on S
yields a set Z ⊆ S \ E of size at most k with

(cid:18)

(cid:19)

f (Z) ≥ 0.149
1 + 

1 − 1

(cid:100)log k(cid:101)

f (OPT(k  V \ E)).

The proof of this theorem  along with a description of the corresponding algorithm  is provided in
Appendix E.

5 Experiments

In this section  we numerically validate the claims outlined in the previous section. Namely  we
test the robustness and compare the performance of our algorithm against the SIEVE-STREAMING
algorithm that knows in advance which elements will be removed. We demonstrate improved or
matching performance in two different data summarization applications: (i) the dominating set
problem  and (ii) personalized movie recommendation. We illustrate how a single robust summary
can be used to regenerate recommendations corresponding to multiple different removals.

5.1 Dominating Set

In the dominating set problem  given a graph G = (V  M )  where V represents the set of nodes and
M stands for edges  the objective function is given by f (Z) = |N (Z) ∪ Z|  where N (Z) denotes
the neighborhood of Z (all nodes adjacent to any node of Z). This objective function is monotone
and submodular.
We consider two datasets: (i) ego-Twitter [19]  consisting of 973 social circles from Twitter  which
form a directed graph with 81306 nodes and 1768149 edges; (ii) Amazon product co-purchasing
network [20]: a directed graph with 317914 nodes and 1745870 edges.
Given the dominating set objective function  we run STAR-T to obtain the robust summary S. Then
we compare the performance of STAR-T-GREEDY  which runs on S  against the performance of
SIEVE-STREAMING  which we allow to know in advance which elements will be removed. We
also compare against a method that chooses the same number of elements as STAR-T  but does
so uniformly at random from the set of all elements that will not be removed (V \ E); we refer to
it as RANDOM. Finally  we also demonstrate the peformance of STAR-T-SIEVE  a variant of our
algorithm that uses the same robust summary S  but instead of running GREEDY in the second stage 
it runs SIEVE-STREAMING on S \ E.

7

Figure 2: Numerical comparisons of the algorithms STAR-T-GREEDY  STAR-T-SIEVE and SIEVE-
STREAMING.
Figures 2(a c) show the objective value after the random removal of k elements from the set S  for
different values of k. Note that E is sampled as a subset of the summary of our algorithm  which hurts
the performance of our algorithm more than the baselines. The reported numbers are averaged over
100 iterations. STAR-T-GREEDY  STAR-T-SIEVE and SIEVE-STREAMING perform comparably
(STAR-T-GREEDY slightly outperforms the other two)  while RANDOM is signiﬁcantly worse.
In Figures 2(b d) we plot the objective value for different values of k after the removal of 2k elements
from the set S  chosen greedily (i.e.  by iteratively removing the element that reduces the objective
value the most). Again  STAR-T-GREEDY  STAR-T-SIEVE and SIEVE-STREAMING perform
comparably  but this time SIEVE-STREAMING slightly outperforms the other two for some values
of k. We observe that even when we remove more than k elements from S  the performance of our
algorithm is still comparable to the performance of SIEVE-STREAMING (which knows in advance
which elements will be removed). We provide additional results in the supplementary material.

5.2

Interactive Personalized Movie Recommendation

The next application we consider is personalized movie recommendation. We use the MovieLens
1M database [21]  which contains 1000209 ratings for 3900 movies by 6040 users. Based on these
ratings  we obtain feature vectors for each movie and each user by using standard low-rank matrix
completion techniques [22]; we choose the number of features to be 30.
For a user u  we use the following monotone submodular function to recommend a set of movies Z:

fu(Z) = (1 − α) ·(cid:88)

(cid:104)vu  vz(cid:105) + α · (cid:88)

z∈Z

m∈M

(cid:104)vm  vz(cid:105) .

max
z∈Z

The ﬁrst term aggregates the predicted scores of the chosen movies z ∈ Z for the user u (here vu
and vz are non-normalized feature vectors of user u and movie z  respectively). The second term
corresponds to a facility-location objective that measures how well the set Z covers the set of all
movies M [4]. Finally  α is a user-dependent parameter that speciﬁes the importance of global movie
coverage versus high scores of individual movies.
Here  the robust setting arises naturally since we do not have complete information about the user:
when shown a collection of top movies  it will likely turn out that they have watched (but not rated)
many of them  rendering these recommendations moot. In such an interactive setting  the user may
also require (or exclude) movies of a speciﬁc genre  or similar to some favorite movie.
We compare the performance of our algorithms STAR-T-GREEDY and STAR-T-SIEVE in such
scenarios against two baselines: GREEDY and SIEVE-STREAMING (both being run on the set V \ E 
i.e.  knowing the removed elements in advance). Note that in this case we are able to afford running

8

Cardinalityk102030405060708090100Avg.obj.value020004000600080001000012000(a)Amazoncommunities |E|=kStar-T-GreedyStar-T-SieveSieve-StrRandomCardinalityk102030405060708090100Avg.obj.value×10400.511.522.5(c)ego-Twitter |E|=kStar-T-GreedyStar-T-SieveSieve-StrRandomCardinalityk1030507090Obj.value0102030405060(e)Movies already-seenStar-T-GreedyStar-T-SieveSieve-StrGreedyCardinalityk102030405060708090100Obj.value01000200030004000500060007000(b)Amazoncommunities |E|=2kStar-T-GreedyStar-T-SieveSieve-StrRandomCardinalityk102030405060708090100Obj.value×10400.511.52(d)ego-Twitter |E|=2kStar-T-GreedyStar-T-SieveSieve-StrRandomCardinalityk1030507090110130150170190Obj.value102030405060(f)Movies bygenreStar-T-GreedyStar-T-SieveSieve-StrGreedyGREEDY  which may be infeasible when working with larger datasets. Below we discuss two concrete
practical scenarios featured in our experiments.

Movies by genre. After we have built our summary S  the user decides to watch a drama today;
we retrieve only movies of this genre from S. This corresponds to removing 59% of the universe
V . In Figure 2(f) we report the quality of our output compared to the baselines (for user ID 445
and α = 0.95) for different values of k. The performance of STAR-T-GREEDY is within several
percent of the performance of GREEDY (which we can consider as a tractable optimum)  and the two
sieve-based methods STAR-T-SIEVE and SIEVE-STREAMING display similar objective values.

Already-seen movies. We randomly sample a set E of movies already watched by the user (500
out of all 3900 movies). To obtain a realistic subset  each movie is sampled proportionally to its
popularity (number of ratings). Figure 2(e) shows the performance of our algorithm faced with the
removal of E (user ID = 445  α = 0.9) for a range of settings of k. Again  our algorithm is able to
almost match the objective values of GREEDY (which is aware of E in advance).
Recall that we are able to use the same precomputed summary S for different removed sets E. This
summary was built for parameter w = 1  which theoretically allows for up to k removals. However 
despite having |E| (cid:29) k in the above scenarios  our performance remains robust; this indicates that
our method is more resilient in practice than what the proved bound alone would guarantee.

6 Conclusion

We have presented a new robust submodular streaming algorithm STAR-T based on a novel parti-
tioning structure and an exponentially decreasing thresholding rule. It makes one pass over the data

and retains a set of size O(cid:0)(k + m log k) log2 k(cid:1). We have further shown that after the removal of

any m elements  a simple greedy algorithm that runs on the obtained set achieves a constant-factor
approximation guarantee for robust submodular function maximization. In addition  we have pre-
sented two numerical studies where our method compares favorably against the SIEVE-STREAMING
algorithm that knows in advance which elements will be removed.

Acknowledgment.
IB and VC’s work was supported in part by the European Research Council
(ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement
number 725594)  in part by the Swiss National Science Foundation (SNF)  project 407540_167319/1 
in part by the NCCR MARVEL  funded by the Swiss National Science Foundation  in part by
Hasler Foundation Switzerland under grant agreement number 16066 and in part by Ofﬁce of Naval
Research (ONR) under grant agreement number N00014-16-R-BA01. JT’s work was supported by
ERC Starting Grant 335288-OptApprox.

9

References
[1] S. Tschiatschek  R. K. Iyer  H. Wei  and J. A. Bilmes  “Learning mixtures of submodular
functions for image collection summarization ” in Advances in neural information processing
systems  2014  pp. 1413–1421.

[2] H. Lin and J. Bilmes  “A class of submodular functions for document summarization ” in Assoc.

for Comp. Ling.: Human Language Technologies-Volume 1  2011.

[3] D. Kempe  J. Kleinberg  and É. Tardos  “Maximizing the spread of inﬂuence through a social

network ” in Int. Conf. on Knowledge Discovery and Data Mining (SIGKDD)  2003.

[4] E. Lindgren  S. Wu  and A. G. Dimakis  “Leveraging sparsity for efﬁcient submodular data
summarization ” in Advances in Neural Information Processing Systems  2016  pp. 3414–3422.
[5] A. Krause and R. G. Gomes  “Budgeted nonparametric learning from data streams ” in ICML 

2010  pp. 391–398.

[6] K. El-Arini and C. Guestrin  “Beyond keyword search: discovering relevant scientiﬁc literature ”
in Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery
and data mining. ACM  2011  pp. 439–447.

[7] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher  “An analysis of approximations for maxi-
mizing submodular set functions—i ” Mathematical Programming  vol. 14  no. 1  pp. 265–294 
1978.

[8] A. Badanidiyuru  B. Mirzasoleiman  A. Karbasi  and A. Krause  “Streaming submodular
maximization: Massive data summarization on the ﬂy ” in Proceedings of the 20th ACM
SIGKDD. ACM  2014  pp. 671–680.

[9] A. Krause  H. B. McMahan  C. Guestrin  and A. Gupta  “Robust submodular observation

selection ” Journal of Machine Learning Research  vol. 9  no. Dec  pp. 2761–2801  2008.

[10] J. B. Orlin  A. S. Schulz  and R. Udwani  “Robust monotone submodular function maximization ”

in Int. Conf. on Integer Programming and Combinatorial Opt. (IPCO). Springer  2016.

[11] I. Bogunovic  S. Mitrovi´c  J. Scarlett  and V. Cevher  “Robust submodular maximization: A

non-uniform partitioning approach ” in Int. Conf. Mach. Learn. (ICML)  2017.

[12] R. Kumar  B. Moseley  S. Vassilvitskii  and A. Vattani  “Fast greedy algorithms in MapReduce

and streaming ” ACM Transactions on Parallel Computing  vol. 2  no. 3  p. 14  2015.

[13] A. Norouzi-Fard  A. Bazzi  I. Bogunovic  M. El Halabi  Y.-P. Hsieh  and V. Cevher  “An efﬁcient
streaming algorithm for the submodular cover problem ” in Adv. Neur. Inf. Proc. Sys. (NIPS) 
2016.

[14] W. Chen  T. Lin  Z. Tan  M. Zhao  and X. Zhou  “Robust inﬂuence maximization ” in Proceed-

ings of the ACM SIGKDD  2016  p. 795.

[15] M. Staib and S. Jegelka  “Robust budget allocation via continuous submodular functions ” in

Int. Conf. Mach. Learn. (ICML)  2017.

[16] D. Golovin and A. Krause  “Adaptive submodularity: Theory and applications in active learning

and stochastic optimization ” Journal of Artiﬁcial Intelligence Research  vol. 42  2011.

[17] A. Guillory and J. Bilmes  “Interactive submodular set cover ” arXiv preprint arXiv:1002.3345 

2010.

[18] B. Mirzasoleiman  A. Karbasi  and A. Krause  “Deletion-robust submodular maximization:
Data summarization with “the right to be forgotten” ” in International Conference on Machine
Learning  2017  pp. 2449–2458.

[19] J. Mcauley and J. Leskovec  “Discovering social circles in ego networks ” ACM Trans. Knowl.

Discov. Data  2014.

[20] J. Yang and J. Leskovec  “Deﬁning and evaluating network communities based on ground-truth ”

Knowledge and Information Systems  vol. 42  no. 1  pp. 181–213  2015.

[21] F. M. Harper and J. A. Konstan  “The MovieLens datasets: History and context ” ACM Transac-

tions on Interactive Intelligent Systems (TiiS)  vol. 5  no. 4  p. 19  2016.

[22] O. Troyanskaya  M. Cantor  G. Sherlock  P. Brown  T. Hastie  R. Tibshirani  D. Botstein 
and R. B. Altman  “Missing value estimation methods for DNA microarrays ” Bioinformatics 
vol. 17  no. 6  pp. 520–525  2001.

10

,Peter Schulam
Raman Arora
Slobodan Mitrovic
Ilija Bogunovic
Ashkan Norouzi-Fard
Jakub Tarnawski
Volkan Cevher