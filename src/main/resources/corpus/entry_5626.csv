2011,Trace Lasso: a trace norm regularization for correlated designs,Using the $\ell_1$-norm to regularize the estimation of  the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper  we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm  called the trace Lasso  uses the trace norm of the selected covariates  which is a convex surrogate of their rank  as the criterion of model complexity. We analyze the properties of our norm  describe an optimization algorithm based on reweighted least-squares  and illustrate the behavior of this norm on synthetic data  showing that it is more adapted to strong correlations than competing methods such as the elastic net.,Trace Lasso: a trace norm regularization for

correlated designs

´Edouard Grave

INRIA  Sierra Project-team

´Ecole Normale Sup´erieure  Paris
edouard.grave@inria.fr

Guillaume Obozinski

INRIA  Sierra Project-team

´Ecole Normale Sup´erieure  Paris

guillaume.obozinski@inria.fr

Francis Bach

INRIA  Sierra Project-team

´Ecole Normale Sup´erieure  Paris
francis.bach@inria.fr

Abstract

Using the (cid:96)1-norm to regularize the estimation of the parameter vector of a linear
model leads to an unstable estimator when covariates are highly correlated. In this
paper  we introduce a new penalty function which takes into account the correla-
tion of the design matrix to stabilize the estimation. This norm  called the trace
Lasso  uses the trace norm of the selected covariates  which is a convex surrogate
of their rank  as the criterion of model complexity. We analyze the properties of
our norm  describe an optimization algorithm based on reweighted least-squares 
and illustrate the behavior of this norm on synthetic data  showing that it is more
adapted to strong correlations than competing methods such as the elastic net.

1

Introduction

The concept of parsimony is central in many scientiﬁc domains. In the context of statistics  signal
processing or machine learning  it takes the form of variable or feature selection problems  and is
commonly used in two situations: ﬁrst  to make the model or the prediction more interpretable or
cheaper to use  i.e.  even if the underlying problem does not admit sparse solutions  one looks for the
best sparse approximation. Second  sparsity can also be used given prior knowledge that the model
should be sparse. Many methods have been designed to learn sparse models  namely methods based
on greedy algorithms [1  2]  Bayesian inference [3] or convex optimization [4  5].
In this paper  we focus on the regularization by sparsity-inducing norms. The simplest example
of such norms is the (cid:96)1-norm  leading to the Lasso  when used within a least-squares framework.
In recent years  a large body of work has shown that the Lasso was performing optimally in high-
dimensional low-correlation settings  both in terms of prediction [6]  estimation of parameters or
estimation of supports [7  8]. However  most data exhibit strong correlations  with various correla-
tion structures  such as clusters (i.e.  close to block-diagonal covariance matrices) or sparse graphs 
such as for example problems involving sequences (in which case  the covariance matrix is close to
a Toeplitz matrix [9]). In these situations  the Lasso is known to have stability problems: although
its predictive performance is not disastrous  the selected predictor may vary a lot (typically  given
two correlated variables  the Lasso will only select one of the two  at random).
Several remedies have been proposed to this instability. First  the elastic net [10] adds a strongly
convex penalty term (the squared (cid:96)2-norm) that will stabilize selection (typically  given two cor-
related variables  the elastic net will select the two variables). However  it is blind to the exact

1

correlation structure  and while strong convexity is required for some variables  it is not for other
variables. Another solution is to consider the group Lasso  which will divide the predictors into
groups and penalize the sum of the (cid:96)2-norm of these groups [11]. This is known to accomodate
strong correlations within groups [12]; however it requires to know the groups in advance  which is
not always possible. A third line of research has focused on sampling-based techniques [13  14  15].
An ideal regularizer should thus take into account the design (like the group Lasso  with oracle
groups)  but without requiring human intervention (like the elastic net); it should thus add strong
convexity only where needed  and not modifying variables where things behave correctly. In this
paper  we propose a new norm towards this end.
More precisely we make the following contributions:

interpolates between the (cid:96)1-norm and the (cid:96)2-norm depending on correlations.

• We propose in Section 2 a new norm based on the trace norm (a.k.a. nuclear norm) that
• We show that there is a unique minimum when penalizing with this norm in Section 2.2.
• We provide optimization algorithms based on reweighted least-squares in Section 3.
• We study the second-order expansion around independence and relate it to existing work
• We perform synthetic experiments in Section 5  where we show that the trace Lasso out-

on including correlations in Section 4.

performs existing norms in strong-correlation regimes.

Notations. Let M ∈ Rn×p. We use superscripts for the columns of M  i.e.  M(i) denotes the i-th
column  and subscripts for the rows  i.e.  Mi denotes the i-th row. For M ∈ Rp×p  diag(M) ∈ Rp
is the diagonal of the matrix M  while for u ∈ Rp  Diag(u) ∈ Rp×p is the diagonal matrix whose
diagonal elements are the ui. Let S be a subset of {1  ...  p}  then uS is the vector u restricted to
the support S  with 0 outside the support S. We denote by Sp the set of symmetric matrices of size
p. We will use various matrix norms  here are the notations we use: (cid:107)M(cid:107)∗ is the trace norm  i.e. 
the sum of the singular values of the matrix M  (cid:107)M(cid:107)op is the operator norm  i.e.  the maximum
singular value of the matrix M  (cid:107)M(cid:107)F is the Frobenius norm  i.e.  the (cid:96)2-norm of the singular

values  which is also equal to(cid:112)tr(M(cid:62)M) and (cid:107)M(cid:107)2 1 is the sum of the (cid:96)2-norm of the columns

p(cid:88)

of M: (cid:107)M(cid:107)2 1 =

(cid:107)M(i)(cid:107)2.

i=1

2 Deﬁnition and properties of the trace Lasso
We consider the problem of predicting y ∈ R  given a vector x ∈ Rp  assuming a linear model

y = w(cid:62)x + ε 

where ε is an additive (typically Gaussian) noise with mean 0 and variance σ2. Given a training set
X = (x1  ...  xn)(cid:62) ∈ Rn×p and y = (y1  ...  yn)(cid:62) ∈ Rn  a widely used method to estimate the
parameter vector w is penalized empirical risk minimization

n(cid:88)

i=1

ˆw ∈ argmin

w

1
n

(cid:96)(yi  w(cid:62)xi) + λf (w) 

(1)

where (cid:96) is a loss function used to measure the error we make by predicting w(cid:62)xi instead of yi  while
f is a regularization term used to penalize complex models. This second term helps avoiding over-
ﬁtting  especially in the case where we have many more parameters than observation  i.e.  n (cid:28) p.

2.1 Related work

We will now present some classical penalty functions for linear models which are widely used in the
machine learning and statistics community. The ﬁrst one  known as Tikhonov regularization [16] or
ridge [17]  is the squared (cid:96)2-norm. When used with the square loss  estimating the parameter vector
w is done by solving a linear system. One of the main drawbacks of this penalty function is the fact

2

that it does not perform variable selection and thus does not behave well in sparse high-dimensional
settings.
Hence  it is natural to penalize linear models by the number of variables used by the model. Un-
fortunately  this criterion  sometimes denoted by (cid:107) · (cid:107)0 ((cid:96)0-penalty)  is not convex and solving the
problem in Eq. (1) is generally NP-Hard [18]. Thus  a convex relaxation for this problem was in-
troduced  replacing the size of the selected subset by the (cid:96)1-norm of w. This estimator is known
as the Lasso [4] in the statistics community and basis pursuit [5] in signal processing. Under some
assumptions  the two problems are in fact equivalent (see for example [19] and references therein).
When two predictors are highly correlated  the Lasso has a very unstable behavior: it often only
selects the variable that is the most correlated with the residual. On the other hand  Tikhonov
regularization tends to shrink coefﬁcients of correlated variables together  leading to a very stable
behavior. In order to get the best of both worlds  stability and variable selection  Zou and Hastie
introduced the elastic net [10]  which is the sum of the (cid:96)1-norm and squared (cid:96)2-norm. Unfortunately 
this estimator needs two regularization parameters and is not adaptive to the precise correlation
structure of the data. Some authors also proposed to use pairwise correlations between predictors
to interpolate more adaptively between the (cid:96)1-norm and squared (cid:96)2-norm  by introducing a method
called pairwise elastic net [20] (see comparisons with our approach in Section 5).
Finally  when one has more knowledge about the data  for example clusters of variables that should
be selected together  one can use the group Lasso [11]. Given a partition (Si) of the set of variables 
it is deﬁned as the sum of the (cid:96)2-norms of the restricted vectors wSi:

k(cid:88)

(cid:107)w(cid:107)GL =

(cid:107)wSi(cid:107)2.

The effect of this penalty function is to introduce sparsity at the group level: variables in a group are
selected all together. One of the main drawback of this method  which is also sometimes one of its
quality  is the fact that one needs to know the partition of the variables  and so one needs to have a
good knowledge of the data.

i=1

2.2 The ridge  the Lasso and the trace Lasso

In this section  we show that Tikhonov regularization and the Lasso penalty can be viewed as norms
of the matrix X Diag(w). We then introduce a new norm involving this matrix.
The solution of empirical risk minimization penalized by the (cid:96)1-norm or (cid:96)2-norm is not equivariant
to rescaling of the predictors X(i)  so it is common to normalize the predictors. When normalizing
the predictors X(i)  and penalizing by Tikhonov regularization or by the Lasso  people are implic-
itly using a regularization term that depends on the data or design matrix X. In fact  there is an
equivalence between normalizing the predictors and not normalizing them  using the two following
reweighted (cid:96)2 and (cid:96)1-norms instead of Tikhonov regularization and the Lasso:

(cid:107)w(cid:107)2

2 =

(cid:107)X(i)(cid:107)2

2 w2
i

and

(cid:107)w(cid:107)1 =

(cid:107)X(i)(cid:107)2 |wi|.

(2)

i=1

i=1

These two norms can be expressed using the matrix X Diag(w):

(cid:107)w(cid:107)2 = (cid:107)X Diag(w)(cid:107)F

and

(cid:107)w(cid:107)1 = (cid:107)X Diag(w)(cid:107)2 1 

and a natural question arises: are there other relevant choices of functions or matrix norms? A
classical measure of the complexity of a model is the number of predictors used by this model 
which is equal to the size of the support of w. This penalty being non-convex  people use its convex
relaxation  which is the (cid:96)1-norm  leading to the Lasso.
Here  we propose a different measure of complexity which can be shown to be more adapted in
model selection settings [21]: the dimension of the subspace spanned by the selected predictors.
This is equal to the rank of the selected predictors  or also to the rank of the matrix X Diag(w).
Like the size of the support  this function is non-convex  and we propose to replace it by a convex
surrogate  the trace norm  leading to the following penalty that we call “trace Lasso”:

Ω(w) = (cid:107)X Diag(w)(cid:107)∗.

3

p(cid:88)

p(cid:88)

The trace Lasso has some interesting properties: if all the predictors are orthogonal  then  it is equal
to the (cid:96)1-norm. Indeed  we have the decomposition:

(cid:16)(cid:107)X(i)(cid:107)2wi

(cid:17) X(i)

e(cid:62)
i  

(cid:107)X(i)(cid:107)2

X Diag(w) =

p(cid:88)

i=1

p(cid:88)

where ei are the vectors of the canonical basis. Since the predictors are orthogonal and the ei are
orthogonal too  this gives the singular value decomposition of X Diag(w) and we get

(cid:107)X Diag(w)(cid:107)∗ =

(cid:107)X(i)(cid:107)2|wi| = (cid:107)X Diag(w)(cid:107)2 1.

On the other hand  if all the predictors are equal to X(1)  then
X Diag(w) = X(1)w(cid:62) 

i=1

and we get (cid:107)X Diag(w)(cid:107)∗ = (cid:107)X(1)(cid:107)2(cid:107)w(cid:107)2 = (cid:107)X Diag(w)(cid:107)F   which is equivalent to Tikhonov
regularization. Thus when two predictors are strongly correlated  our norm will behave like
Tikhonov regularization  while for almost uncorrelated predictors  it will behave like the Lasso.
Always having a unique minimum is an important property for a statistical estimator  as it is a ﬁrst
step towards stability. The trace Lasso  by adding strong convexity exactly in the direction of highly
correlated covariates  always has a unique minimum  and is thus much more stable than the Lasso.
Proposition 1. If the loss function (cid:96) is strongly convex with respect to its second argument  then the
solution of the empirical risk minimization penalized by the trace Lasso  i.e.  Eq. (1)  is unique.

The technical proof of this proposition can be found in [22]  and consists in showing that in the ﬂat
directions of the loss function  the trace Lasso is strongly convex.

2.3 A new family of penalty functions

In this section  we introduce a new family of penalties  inspired by the trace Lasso  allowing us to
write the (cid:96)1-norm  the (cid:96)2-norm and the newly introduced trace Lasso as special cases. In fact  we
note that (cid:107) Diag(w)(cid:107)∗ = (cid:107)w(cid:107)1 and (cid:107)p−1/21(cid:62) Diag(w)(cid:107)∗ = (cid:107)w(cid:62)(cid:107)∗ = (cid:107)w(cid:107)2. In other words 
we can express the (cid:96)1 and (cid:96)2-norms of w using the trace norm of a given matrix times the matrix
Diag(w). A natural question to ask is: what happens when using a matrix P other than the identity
or the line vector p−1/21(cid:62)  and what are good choices of such matrices? Therefore  we introduce
the following family of penalty functions:
Deﬁnition 1. Let P ∈ Rk×p  all of its columns having unit norm. We introduce the norm ΩP as

ΩP(w) = (cid:107)P Diag(w)(cid:107)∗.

Proof. The positive homogeneity and triangle inequality are direct consequences of the linearity of
w (cid:55)→ P Diag(w) and the fact that (cid:107) · (cid:107)∗ is a norm. Since all the columns of P are not equal to zero 
we have

P Diag(w) = 0 ⇔ w = 0 

and so  ΩP separates points and thus is a norm.

As stated before  the (cid:96)1 and (cid:96)2-norms are special cases of the family of norms we just introduced.
Another important penalty that can be expressed as a special case is the group Lasso  with non-
overlapping groups. Given a partition (Sj) of the set {1  ...  p}  the group Lasso is deﬁned by

(cid:88)

(cid:107)w(cid:107)GL =

(cid:107)wSj(cid:107)2.

We deﬁne the matrix PGL by

PGL

ij =

(cid:26)

1/(cid:112)|Sk|

0

Sj

if i and j are in the same group Sk 
otherwise.

4

Figure 1: Unit balls for various value of P(cid:62)P. See the text for the value of P(cid:62)P. (Best seen in
color).

(cid:88)

1Sj(cid:112)|Sj| w(cid:62)

Then 

PGL Diag(w) =

(3)
Using the fact that (Sj) is a partition of {1  ...  p}  the vectors 1Sj are orthogonal and so are the
vectors wSj . Hence  after normalizing the vectors  Eq. (3) gives a singular value decomposition of
PGL Diag(w) and so the group Lasso penalty can be expressed as a special case of our family of
norms:

Sj

Sj

.

(cid:107)PGL Diag(w)(cid:107)∗ =

(cid:107)wSj(cid:107)2 = (cid:107)w(cid:107)GL.

(cid:88)

Sj

In the following proposition  we show that our norm only depends on the value of P(cid:62)P. This is an
important property for the trace Lasso  where P = X  since it underlies the fact that this penalty
only depends on the correlation matrix X(cid:62)X of the covariates.
Proposition 2. Let P ∈ Rk×p  all of its columns having unit norm. We have

ΩP(w) = (cid:107)(P(cid:62)P)1/2 Diag(w)(cid:107)∗.

(cid:32) 1

We plot the unit ball of our norm for the following value of P(cid:62)P (see ﬁgure 1):
1
1
0

0.49
0.7
1

0.1
0.1
1

0.7
1
0.7

0.9
1
0.1

0.7
0.49

0.9
0.1

(cid:32) 1

1
0

(cid:33)

(cid:32) 1

(cid:33)

(cid:33)

0
0
1

We can lower bound and upper bound our norms by the (cid:96)2-norm and (cid:96)1-norm respectively. This
shows that  as for the elastic net  our norms interpolate between the (cid:96)1-norm and the (cid:96)2-norm. But
the main difference between the elastic net and our norms is the fact that our norms are adaptive 
and require a single regularization parameter to tune. In particular for the trace Lasso  when two
covariates are strongly correlated  it will be close to the (cid:96)2-norm  while when two covariates are
almost uncorrelated  it will behave like the (cid:96)1-norm. This is a behavior close to the one of the
pairwise elastic net [20].
Proposition 3. Let P ∈ Rk×p  all of its columns having unit norm. We have

(cid:107)w(cid:107)2 ≤ ΩP(w) ≤ (cid:107)w(cid:107)1.

2.4 Dual norm

The dual norm is an important quantity for both optimization and theoretical analysis of the estima-
tor. Unfortunately  we are not able in general to obtain a closed form expression of the dual norm for
the family of norms we just introduced. However we can obtain a bound  which is exact for some
special cases:
Proposition 4. The dual norm  deﬁned by Ω∗

u(cid:62)v  can be bounded by:

P(u) = max

ΩP(v)≤1
P(u) ≤ (cid:107)P Diag(u)(cid:107)op.
Ω∗

5

Proof. Using the fact that diag(P(cid:62)P) = 1  we have

u(cid:62)v = tr(cid:0)Diag(u)P(cid:62)P Diag(v)(cid:1)

≤ (cid:107)P Diag(u)(cid:107)op(cid:107)P Diag(v)(cid:107)∗ 

where the inequality comes from the fact that the operator norm (cid:107) · (cid:107)op is the dual norm of the trace
norm. The deﬁnition of the dual norm then gives the result.

As a corollary  we can bound the dual norm by a constant times the (cid:96)∞-norm:

P(u) ≤ (cid:107)P Diag(u)(cid:107)op ≤ (cid:107)P(cid:107)op(cid:107) Diag(u)(cid:107)op = (cid:107)P(cid:107)op(cid:107)u(cid:107)∞.
Ω∗

Using proposition (3)  we also have the inequality Ω∗

P(u) ≥ (cid:107)u(cid:107)∞.

3 Optimization algorithm

In this section  we introduce an algorithm to estimate the parameter vector w when the loss function
2 (y − w(cid:62)x)2 and the penalty is the trace Lasso. It is
is equal to the square loss: (cid:96)(y  w(cid:62)x) = 1
straightforward to extend this algorithm to the family of norms indexed by P. The problem we
consider is thus

min

w

1
2

(cid:107)y − Xw(cid:107)2

2 + λ(cid:107)X Diag(w)(cid:107)∗.

We could optimize this cost function by subgradient descent  but this is quite inefﬁcient: computing
the subgradient of the trace Lasso is expensive and the rate of convergence of subgradient descent
is quite slow. Instead  we consider an iteratively reweighted least-squares method. First  we need to
introduce a well-known variational formulation for the trace norm [23]:
Proposition 5. Let M ∈ Rn×p. The trace norm of M is equal to:

(cid:107)M(cid:107)∗ =

(cid:16)

1
2

inf
S(cid:23)0

tr(cid:0)M(cid:62)S−1M(cid:1) + tr (S)  

MM(cid:62)(cid:17)1/2
w(cid:62) Diag(cid:0) diag(X(cid:62)S−1X)(cid:1)w +

.

and the inﬁmum is attained for S =

Using this proposition  we can reformulate the previous optimization problem as

min

w

inf
S(cid:23)0

1
2

(cid:107)y − Xw(cid:107)2

2 +

λ
2

λ
2

tr(S).

This problem is jointly convex in (w  S) [24]. In order to optimize this objective function by alter-
2 tr(S−1). Otherwise  the inﬁmum
nating the minimization over w and S  we need to add a term λµi
over S could be attained at a non invertible S  leading to a non convergent algorithm. The inﬁmum

over S is then attained for S =(cid:0)X Diag(w)2X(cid:62) + µiI(cid:1)1/2.
where D = Diag(cid:0)diag(X(cid:62)S−1X)(cid:1). It is equivalent to solving the linear system

Optimizing over w is a least-squares problem penalized by a reweighted (cid:96)2-norm equal to w(cid:62)Dw 

(X(cid:62)X + λD)w = X(cid:62)y.

This can be done efﬁciently by using a conjugate gradient method. Since the cost of multiplying
(X(cid:62)X+λD) by a vector is O(np)  solving the system has a complexity of O(knp)  where k ≤ n+1
is the number of iterations needed to converge (see theorem 10.2.5 of [9]). Using warm restarts  k
can be even smaller than n  since the linear system we are solving does not change a lot from an
iteration to another. Below we summarize the algorithm:

ITERATIVE ALGORITHM FOR ESTIMATING w

Input: the design matrix X  the initial guess w0  number of iteration N  sequence µi.
For i = 1...N:

• Compute the eigenvalue decomposition U Diag(sk)U(cid:62) of X Diag(wi−1)2X(cid:62).
• Set D = Diag(diag(X(cid:62)S−1X))  where S−1 = U Diag(1/
• Set wi by solving the system (X(cid:62)X + λD)w = X(cid:62)y.

sk + µi)U(cid:62).

√

For the sequence µi  we use a decreasing sequence converging to ten times the machine precision.

6

3.1 Choice of λ

We now give a method to choose the initial parameter λ of the regularization path. In fact  we know
that the vector 0 is solution if and only if λ ≥ Ω∗(X(cid:62)y) [25]. Thus  we need to start the path at
λ = Ω∗(X(cid:62)y)  corresponding to the empty solution 0  and then decrease λ. Using the inequalities
on the dual norm we obtained in the previous section  we get

(cid:107)X(cid:62)y(cid:107)∞ ≤ Ω∗(X(cid:62)y) ≤ (cid:107)X(cid:107)op(cid:107)X(cid:62)y(cid:107)∞.

Therefore  starting the path at λ = (cid:107)X(cid:107)op(cid:107)X(cid:62)y(cid:107)∞ is a good choice.

4 Approximation around the Lasso
We recall that when P = I ∈ Rp×p  our norm is equal to the (cid:96)1-norm  and we want to understand
its behavior when P departs from the identity. Thus  we compute a second order approximation of
our norm around the Lasso: we add a small perturbation ∆ ∈ Sp to the identity matrix  and using
Prop. 6 of [22]  we obtain the following second order approximation:

(cid:107)(I + ∆) Diag(w)(cid:107)∗ = (cid:107)w(cid:107)1 + diag(∆)(cid:62)|w|+

(∆ji|wi| − ∆ij|wj|)2

4(|wi| + |wj|)

(cid:88)

(cid:88)

|wi|>0

|wj|>0

We can rewrite this approximation as

(cid:107)(I + ∆) Diag(w)(cid:107)∗ = (cid:107)w(cid:107)1 + diag(∆)(cid:62)|w| +

(cid:88)

(cid:88)

|wi|=0

|wj|>0

+

(∆ij|wj|)2
2|wj| + o((cid:107)∆(cid:107)2).

(cid:88)

i j

ij(|wi| − |wj|)2
∆2
4(|wi| + |wj|)

+ o((cid:107)∆(cid:107)2) 

using a slight abuse of notation  considering that the last term is equal to 0 when wi = wj = 0. The
second order term is quite interesting: it shows that when two covariates are correlated  the effect of
the trace Lasso is to shrink the corresponding coefﬁcients toward each other. We also note that this
term is very similar to pairwise elastic net penalties  which are of the form |w|(cid:62)P|w|  where Pij is
a decreasing function of ∆ij.

5 Experiments

In this section  we perform experiments on synthetic data to illustrate the behavior of the trace Lasso
and other classical penalties when there are highly correlated covariates in the design matrix. The
support S of w is equal to {1  ...  k}  where k is the size of the support. For i in the support of
w  wi is independently drawn from a uniform distribution over [−1  1]. The observations xi are
drawn from a multivariate Gaussian with mean 0 and covariance matrix Σ. For the ﬁrst setting  Σ
is set to the identity  for the second setting  Σ is block diagonal with blocks equal to 0.2I + 0.811(cid:62)
corresponding to clusters of four variables  ﬁnally for the third setting  we set Σij = 0.95|i−j| 
corresponding to a Toeplitz design. For each method  we choose the best λ. We perform a ﬁrst
series of experiments (p = 1024  n = 256) for which we report the estimation error. For the second
series of experiments (p = 512  n = 128)  we report the Hamming distance between the estimated
support and the true support.
In all six graphs of Figure 2  we observe behaviors that are typical of Lasso  ridge and elastic net:
the Lasso performs very well on very sparse models but its performance degrades for denser models.
The elastic net performs better than the Lasso for settings where there are strongly correlated covari-
ates  thanks to its strongly convex (cid:96)2 term. In setting 1  since the variables are uncorrelated  there
is no reason to couple their selection. This suggests that the Lasso should be the most appropriate
convex regularization. The trace Lasso approaches the Lasso when n is much larger than p  but the
weak coupling induced by empirical correlations is sufﬁcient to slightly decrease its performance
compared to that of the Lasso. By contrast  in settings 2 and 3  the trace Lasso outperforms other
methods (including the pairwise elastic net) since variables that should be selected together are in-
deed correlated. As for the penalized elastic net  since it takes into account the correlations between
variables  it is not surprising that in experiments 2 and 3 it performs better than methods that do not.
We do not have a compelling explanation for its superior performance in experiment 1.

7

Figure 2: Left: estimation error (p = 1024  n = 256)  right: support recovery (p = 512  n = 128).
(Best seen in color. e-net stands for elastic net  pen stands for pairwise elastic net and trace
stands for trace Lasso. Error bars are obtained over 20 runs.)

6 Conclusion

We introduce a new penalty function  the trace Lasso  which takes advantage of the correlation
between covariates to add strong convexity exactly in the directions where needed  unlike the elastic
net for example  which blindly adds a squared (cid:96)2-norm term in every directions. We show on
synthetic data that this adaptive behavior leads to better estimation performance. In the future  we
want to show that if a dedicated norm using prior knowledge such as the group Lasso can be used 
the trace Lasso will behave similarly and its performance will not degrade too much  providing
theoretical guarantees to such adaptivity. Finally  we will seek applications of this estimator in
inverse problems such as deblurring  where the design matrix exhibits strong correlation structure.

Acknowledgments

Guillaume Obozinski and Francis Bach are supported in part by the European Research Council
(SIERRA ERC-239993).

8

References
[1] S.G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. Signal Pro-

cessing  IEEE Transactions on  41(12):3397–3415  1993.

[2] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models.

Advances in Neural Information Processing Systems  22  2008.

[3] M.W. Seeger. Bayesian inference and optimal design for the sparse linear model. The Journal

of Machine Learning Research  9:759–813  2008.

[4] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  58(1):267–288  1996.

[5] S.S. Chen  D.L. Donoho  and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM

journal on scientiﬁc computing  20(1):33–61  1999.

[6] P.J. Bickel  Y. Ritov  and A.B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

The Annals of Statistics  37(4):1705–1732  2009.

[7] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning

Research  7:2541–2563  2006.

[8] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using
(cid:96)1-constrained quadratic programming (Lasso). Information Theory  IEEE Transactions on 
55(5):2183–2202  2009.

[9] G.H. Golub and C.F. Van Loan. Matrix computations. Johns Hopkins Univ Pr  1996.
[10] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

[11] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  68(1):49–67  2006.
[12] F. Bach. Consistency of the group Lasso and multiple kernel learning. The Journal of Machine

Learning Research  9:1179–1225  2008.

[13] F. Bach. Bolasso: model consistent Lasso estimation through the bootstrap. In Proceedings of

the 25th international conference on Machine learning  pages 33–40. ACM  2008.

[14] H. Liu  K. Roeder  and L. Wasserman. Stability approach to regularization selection (stars) for
high dimensional graphical models. Advances in Neural Information Processing Systems  23 
2010.

[15] N. Meinshausen and P. B¨uhlmann. Stability selection. Journal of the Royal Statistical Society:

Series B (Statistical Methodology)  72(4):417–473  2010.

[16] A. Tikhonov. Solution of incorrectly formulated problems and the regularization method. In

Soviet Math. Dokl.  volume 5  page 1035  1963.

[17] A.E. Hoerl and R.W. Kennard. Ridge regression: Biased estimation for nonorthogonal prob-

lems. Technometrics  12(1):55–67  1970.

[18] G. Davis  S. Mallat  and M. Avellaneda. Adaptive greedy approximations. Constructive ap-

proximation  13(1):57–98  1997.

[19] E.J. Cand`es and T. Tao. Decoding by linear programming. Information Theory  IEEE Trans-

actions on  51(12):4203–4215  2005.

[20] A. Lorbert  D. Eis  V. Kostina  D. M. Blei  and P. J. Ramadge. Exploiting covariate similarity
in sparse regression via the pairwise elastic net. JMLR - Proceedings of the 13th International
Conference on Artiﬁcial Intelligence and Statistics  9:477–484  2010.

[21] T. Hastie  R. Tibshirani  and J. Friedman. The elements of statistical learning. 2001.
[22] E. Grave  G. Obozinski  and F. Bach. Trace lasso: a trace norm regularization for correlated

designs. Technical report  arXiv:1109.1990  2011.

[23] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. Advances in neural

information processing systems  19:41  2007.

[24] S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Pr  2004.
[25] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing

norms. S. Sra  S. Nowozin  S. J. Wright.  editors  Optimization for Machine Learning  2011.

9

,Max Jaderberg
Karen Simonyan
Andrew Zisserman
koray kavukcuoglu