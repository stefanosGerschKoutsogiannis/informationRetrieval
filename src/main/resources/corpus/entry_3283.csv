2014,Low Rank Approximation Lower Bounds in Row-Update Streams,We study low-rank approximation in the streaming model in which the rows of an $n \times d$ matrix $A$ are presented one at a time in an arbitrary order. At the end of the stream  the streaming algorithm should output a $k \times d$ matrix $R$ so that $\|A-AR^{\dagger}R\|_F^2 \leq (1+\eps)\|A-A_k\|_F^2$  where $A_k$ is the best rank-$k$ approximation to $A$. A deterministic streaming algorithm of Liberty (KDD  2013)  with an improved analysis of Ghashami and Phillips (SODA  2014)  provides such a streaming algorithm using $O(dk/\epsilon)$ words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of $\Omega(dk/\epsilon)$ bits of space  even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size  improving on a simple $\Omega(dk)$ space lower bound.,Low Rank Approximation Lower Bounds in

Row-Update Streams

David P. Woodruff

IBM Research Almaden

dpwoodru@us.ibm.com

Abstract

F ≤ (1 + )(cid:107)A− Ak(cid:107)2

We study low-rank approximation in the streaming model in which the rows of
an n × d matrix A are presented one at a time in an arbitrary order. At the end
of the stream  the streaming algorithm should output a k × d matrix R so that
(cid:107)A− AR†R(cid:107)2
F   where Ak is the best rank-k approximation
to A. A deterministic streaming algorithm of Liberty (KDD  2013)  with an im-
proved analysis of Ghashami and Phillips (SODA  2014)  provides such a stream-
ing algorithm using O(dk/) words of space. A natural question is if smaller
space is possible. We give an almost matching lower bound of Ω(dk/) bits of
space  even for randomized algorithms which succeed only with constant proba-
bility. Our lower bound matches the upper bound of Ghashami and Phillips up to
the word size  improving on a simple Ω(dk) space lower bound.

1

Introduction

In the last decade many algorithms for numerical linear algebra problems have been proposed  often
providing substantial gains over more traditional algorithms based on the singular value decomposi-
tion (SVD). Much of this work was inﬂuenced by the seminal work of Frieze  Kannan  and Vempala
[8]. These include algorithms for matrix product  low rank approximation  regression  and many
other problems. These algorithms are typically approximate and succeed with high probability.
Moreover  they also generally only require one or a small number of passes over the data.
When the algorithm only makes a single pass over the data and uses a small amount of memory 
it is typically referred to as a streaming algorithm. The memory restriction is especially important
for large-scale data sets  e.g.  matrices whose elements arrive online and/or are too large to ﬁt in
main memory. These elements may be in the form of an entry or entire row seen at a time; we
refer to the former as the entry-update model and the latter as the row-update model. The row-
update model often makes sense when the rows correspond to individual entities. Typically one is
interested in designing robust streaming algorithms which do not need to assume a particular order
of the arriving elements for their correctness. Indeed  if data is collected online  such an assumption
may be unrealistic.
Muthukrishnan asked the question of determining the memory required of data stream algorithms
for numerical linear algebra problems  including best rank-k approximation  matrix product  eigen-
values  determinants  and inverses [18]. This question was posed again by Sarl´os [21]. A number
of exciting streaming algorithms now exist for matrix problems. Sarl´os [21] gave 2-pass algorithms
for matrix product  low rank approximation  and regression  which were sharpened by Clarkson and
Woodruff [5]  who also proved lower bounds in the entry-update model for a number of these prob-
lems. See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2]  and work in
[1  4  6] which implicitly provides algorithms for approximate matrix product.
In this work we focus on the low rank approximation problem. In this problem we are given an
n × d matrix A and would like to compute a matrix B of rank at most k for which (cid:107)A − B(cid:107)F ≤

1

(cid:113)(cid:80)n

(cid:80)d

j=1 A2

i j and

i=1

(1 + )(cid:107)A− Ak(cid:107)F . Here  for a matrix A  (cid:107)A(cid:107)F denotes its Frobenius norm
Ak is the best rank-k approximation to A in this norm given by the SVD.
Clarkson and Woodruff [5] show in the entry-update model  one can compute a factorization B =
L· U · R with L ∈ Rn×k  U ∈ Rk×k  and R ∈ Rk×d  with a streaming algorithm using O(k−2(n +
d/2) log(nd)) bits of space. They also show a lower bound of Ω(k−1(n + d) log(nd)) bits of
space. One limitation of these bounds is that they hold only when the algorithm is required to output
a factorization L · U · R. In many cases n (cid:29) d  and using memory that grows linearly with n (as
the above lower bounds show is unavoidable) is prohibitive. As observed in previous work [9  16] 
in downstream applications we are often only interested in an approximation to the top k principal
components  i.e.  the matrix R above  and so the lower bounds of Clarkson and Woodruff can be
too restrictive. For example  in PCA the goal is to compute the most important directions in the row
space of A.
By reanalyzing an algorithm of Liberty [16]  Ghashami and Phillips [9] were able to overcome this
restriction in the row-update model  showing that Liberty’s algorithm is a streaming algorithm which
ﬁnds a k× d matrix R for which (cid:107)A− AR†R(cid:107)F ≤ (1 + )(cid:107)A− Ak(cid:107)F using only O(dk/) words of
space. Here R† is the Moore-Penrose pseudoinverse of R and R†R denotes the projection onto the
row space of R. Importantly  this space bound no longer depends on n. Moreover  their algorithm
is deterministic and achieves relative error. We note that Liberty’s algorithm itself is similar in spirit
to earlier work on incremental PCA [3  10  11  15  19]  but that work missed the idea of using a
Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then
improved to relative error by Ghashami and Phillips). It also seems possible to obtain a streaming
algorithm using O(dk(log n)/) words of space  using the coreset approach in an earlier paper by
Feldman et al. [7].
This work is motivated by the following questions: Is the O(dk/) space bound tight or can one
achieve an even smaller amount of space? What if one also allows randomization?
In this work we answer the above questions. Our main theorem is the following.
Theorem 1. Any  possibly randomized  streaming algorithm in the row-update model which outputs
a k× d matrix R and guarantees that (cid:107)A− AR†R(cid:107)2
F with probability at least
2/3  must use Ω(kd/) bits of space.

F ≤ (1 + )(cid:107)A− Ak(cid:107)2

Up to a factor of the word size (which is typically O(log(nd)) bits)  our main theorem shows that the
algorithm of Liberty is optimal. It also shows that allowing for randomization and a small probability
of error does not signiﬁcantly help in reducing the memory required. We note that a simple argument
gives an Ω(kd) bit lower bound  see Lemma 2 below  which intuitively follows from the fact that
if A itself is rank-k  then R needs to have the same rowspace of A  and specifying a random k-
dimensional subspace of Rd requires Ω(kd) bits. Hence  the main interest here is improving upon
this lower bound to Ω(kd/) bits of space. This extra 1/ factor is signiﬁcant for small values of  
e.g.  if one wants approximations as close to machine precision as possible with a given amount of
memory.
The only other lower bounds for streaming algorithms for low rank approximation that we know of
are due to Clarkson and Woodruff [5]. As in their work  we use the Index problem in communication
complexity to establish our bounds  which is a communication game between two players Alice and
Bob  holding a string x ∈ {0  1}r and an index i ∈ [r] =: {1  2  . . .   r}  respectively.
In this
game Alice sends a single message to Bob who should output xi with constant probability. It is
known (see  e.g.  [13]) that this problem requires Alice’s message to be Ω(r) bits long. If Alg is a
streaming algorithm for low rank approximation  and Alice can create a matrix Ax while Bob can
create a matrix Bi (depending on their respective inputs x and i)  then if from the output of Alg
on the concatenated matrix [Ax; Bi] Bob can output xi with constant probability  then the memory
required of Alg is Ω(r) bits  since Alice’s message is the state of Alg after running it on Ax.
The main technical challenges are thus in showing how to choose Ax and Bi  as well as showing
how the output of Alg on [Ax; Bi] can be used to solve Index. This is where our work departs
signiﬁcantly from that of Clarkson and Woodruff [5]. Indeed  a major challenge is that in Theorem
1  we only require the output to be the matrix R  whereas in Clarkson and Woodruff’s work from
the output one can reconstruct AR†R. This causes technical complications  since there is much less
information in the output of the algorithm to use to solve the communication game.

2

The intuition behind the proof of Theorem 1 is that given a 2 × d matrix A = [1  x; 1  0d]  where
x is a random unit vector  then if P = R†R is a sufﬁciently good projection matrix for the low
rank approximation problem on A  then the second row of AP actually reveals a lot of information
about x. This may be counterintuitive at ﬁrst  since one may think that [1  0d; 1  0d] is a perfectly
good low rank approximation. However  it turns out that [1  x/2; 1  x/2] is a much better low rank
approximation in Frobenius norm  and even this is not optimal. Therefore  Bob  who has [1  0d]
together with the output P   can compute the second row of AP   which necessarily reveals a lot of
information about x (e.g.  if AP ≈ [1  x/2; 1  x/2]  its second row would reveal a lot of information
about x)  and therefore one could hope to embed an instance of the Index problem into x. Most of
the technical work is about reducing the general problem to this 2 × d primitive problem.
2 Main Theorem

This section is devoted to proving Theorem 1. We start with a simple lemma showing an Ω(kd)
lower bound  which we will refer to. The proof of this lemma is in the full version.
Lemma 2. Any streaming algorithm which  for every input A  with constant probability (over its
internal randomness) succeeds in outputting a matrix R for which (cid:107)A − AR†R(cid:107)F ≤ (1 + )(cid:107)A −
Ak(cid:107)F must use Ω(kd) bits of space.
Returning to the proof of Theorem 1  let c > 0 be a small constant to be determined. We consider
the following two player problem between Alice and Bob: Alice has a ck/ × d matrix A which
can be written as a block matrix [I  R]  where I is the ck/ × ck/ identity matrix  and R is a
ck/ × (d − ck/) matrix in which the entries are in {−1/(d − ck/)1/2  +1/(d − ck/)1/2}. Here
[I  R] means we append the columns of I to the left of the columns of R. Bob is given a set of k
standard unit vectors ei1   . . .   eik  for distinct i1  . . .   ik ∈ [ck/] = {1  2  . . .   ck/}. Here we need
c/ > 1  but we can assume  is less than a sufﬁciently small constant  as otherwise we would just
need to prove an Ω(kd) lower bound  which is established by Lemma 2.
Let B be the matrix [A; ei1   . . .   eik ] obtained by stacking A on top of the vectors ei1  . . .   eik.
The goal is for Bob to output a rank-k projection matrix P ∈ Rd×d for which (cid:107)B − BP(cid:107)F ≤
(1 + )(cid:107)B − Bk(cid:107)F .
Denote this problem by f. We will show the randomized 1-way communication complexity of this
problem R1−way
(f )  in which Alice sends a single message to Bob and Bob fails with probability
at most 1/4  is Ω(kd/) bits. More precisely  let µ be the following product distribution on Alice
and Bob’s inputs: the entries of R are chosen independently and uniformly at random in {−1/(d −
ck/)1/2  +1/(d − ck/)1/2}  while {i1  . . .   ik} is a uniformly random set among all sets of k
distinct indices in [ck/]. We will show that D1−way
µ 1/4 (f ) denotes
the minimum communication cost over all deterministic 1-way (from Alice to Bob) protocols which
fail with probability at most 1/4 when the inputs are distributed according to µ. By Yao’s minimax
principle (see  e.g.  [14])  R1−way
We use the following two-player problem Index in order to lower bound D1−way
µ 1/4 (f ). In this prob-
lem Alice is given a string x ∈ {0  1}r  while Bob is given an index i ∈ [r]. Alice sends a single
message to Bob  who needs to output xi with probability at least 2/3. Again by Yao’s minimax prin-
ciple  we have that R1−way
ν 1/3 (Index)  where ν is the distribution for which x and
i are chosen independently and uniformly at random from their respective domains. The following
is well-known.
Fact 3. [13] D1−way
Theorem 4. For c a small enough positive constant  and d ≥ k/  we have D1−way
Proof. We will reduce from the Index problem with r = (ck/)(d− ck/). Alice  given her string x
to Index  creates the ck/× d matrix A = [I  R] as follows. The matrix I is the ck/× ck/ identity
matrix  while the matrix R is a ck/×(d−ck/) matrix with entries in {−1/(d−ck/)1/2  +1/(d−
ck/)1/2}. For an arbitrary bijection between the coordinates of x and the entries of R  Alice sets a

µ 1/4 (f ) = Ω(kd/)  where D1−way

(Index) ≥ D1−way

µ 1/4 (f ) = Ω(dk/).

(f ) ≥ D1−way

µ 1/4 (f ).

1/4

1/4

1/3

ν 1/3 (Index) = Ω(r).

3

given entry in R to −1/(d− ck/)1/2 if the corresponding coordinate of x is 0  otherwise Alice sets
the given entry in R to +1/(d− ck/)1/2. In the Index problem  Bob is given an index  which under
the bijection between coordinates of x and entries of R  corresponds to being given a row index i
and an entry j in the i-th row of R that he needs to recover. He sets i(cid:96) = i for a random (cid:96) ∈ [k] 
and chooses k − 1 distinct and random indices ij ∈ [ck/] \ {i(cid:96)}  for j ∈ [k] \ {(cid:96)}. Observe that
if (x  i) ∼ ν  then (R  i1  . . .   ik) ∼ µ. Suppose there is a protocol in which Alice sends a single
message to Bob who solves f with probability at least 3/4 under µ. We show that this can be used
to solve Index with probability at least 2/3 under ν. The theorem will follow by Fact 3. Consider
the matrix B which is the matrix A stacked on top of the rows ei1  . . .   eik  in that order  so that B
has ck/ + k rows.
F in a certain way  which will allow our reduction to Index
We proceed to lower bound (cid:107)B − BP(cid:107)2
to be carried out. We need the following fact:
Fact 5. ((2.4) of [20]) Let A be an m × n matrix with i.i.d. entries which are each +1/√n with
probability 1/2 and −1/√n with probability 1/2  and suppose m/n < 1. Then for all t > 0 

−α
(cid:48)

nt3/2

.

Pr[(cid:107)A(cid:107)2 > 1 + t +(cid:112)

m/n] ≤ αe

where α  α(cid:48) > 0 are absolute constants. Here (cid:107)A(cid:107)2 is the operator norm supx (cid:107)Ax(cid:107)/(cid:107)x(cid:107) of A.
We apply Fact 5 to the matrix R  which implies 

Pr[(cid:107)R(cid:107)2 > 1 + √c +(cid:112)(ck/)/(d − (ck/))] ≤ αe

(d−(ck/))c3/4

 

−α
(cid:48)

−βd 

and using that d ≥ k/ and c > 0 is a sufﬁciently small constant  this implies

Pr[(cid:107)R(cid:107)2 > 1 + 3√c] ≤ e

2 ≤ 1 + 7√c  which we condition on.

(1)
where β > 0 is an absolute constant (depending on c). Note that for c > 0 sufﬁciently small 
(1 + 3√c)2 ≤ 1 + 7√c. Let E be the event that (cid:107)R(cid:107)2
We partition the rows of B into B1 and B2  where B1 contains those rows whose projection onto
the ﬁrst ck/ coordinates equals ei for some i /∈ {i1  . . .   ik}. Note that B1 is (ck/ − k) × d and
B2 is 2k × d. Here  B2 is 2k × d since it includes the rows in A indexed by i1  . . .   ik  together with
the rows ei1   . . .   eik. Let us also partition the rows of R into RT and RS  so that the union of the
rows in RT and in RS is equal to R  where the rows of RT are the rows of R in B1  and the rows
of RS are the non-zero rows of R in B2 (note that k of the rows are non-zero and k are zero in B2
restricted to the columns in R).
Lemma 6. For any unit vector u  write u = uR + uS + uT   where S = {i1  . . .   ik}  T = [ck/]\ S 
and R = [d] \ [ck/]  and where uA for a set A is 0 on indices j /∈ A. Then  conditioned on E
occurring  (cid:107)Bu(cid:107)2 ≤ (1 + 7√c)(2 − (cid:107)uT(cid:107)2 − (cid:107)uR(cid:107)2 + 2(cid:107)uS + uT(cid:107)(cid:107)uR(cid:107)).

4

111111100000000000000000000000000000B2B1STck/"kck/"dck/"RSRTRRAliceBobProof. Let C be the matrix consisting of the top ck/ rows of B  so that C has the form [I  R] 
where I is a ck/ × ck/ identity matrix. By construction of B  (cid:107)Bu(cid:107)2 = (cid:107)uS(cid:107)2 + (cid:107)Cu(cid:107)2. Now 
Cu = uS + uT + RuR  and so

(cid:107)Cu(cid:107)2

2 = (cid:107)uS + uT(cid:107)2 + (cid:107)RuR(cid:107)2 + 2(us + uT )T RuR

≤ (cid:107)uS + uT(cid:107)2 + (1 + 7√c)(cid:107)uR(cid:107)2 + 2(cid:107)uS + uT(cid:107)(cid:107)RuR(cid:107)
≤ (1 + 7√c)((cid:107)uS(cid:107)2 + (cid:107)uT(cid:107)2 + (cid:107)uR(cid:107)2) + (1 + 3√c)2(cid:107)uS + uT(cid:107)(cid:107)uR(cid:107)
≤ (1 + 7√c)(1 + 2(cid:107)uS + uT(cid:107)(cid:107)uR(cid:107)) 
(cid:107)Bu(cid:107)2 ≤ (1 + 7√c)(1 + (cid:107)uS(cid:107)2 + 2(cid:107)uS + uT(cid:107)(cid:107)uR(cid:107))

= (1 + 7√c)(2 − (cid:107)uR(cid:107)2 − (cid:107)uT(cid:107)2 + 2(cid:107)uS + UT(cid:107)(cid:107)uR(cid:107)).

and so

We will also make use of the following simple but tedious fact  shown in the full version.
(cid:113)
the function f (x) = 2x√1 − x2 − x2 is maximized when x =
Fact 7. For x ∈ [0  1] 
1/2 − √5/10. We deﬁne ζ to be the value of f (x) at its maximum  where ζ = 2/√5 + √5/10 −
1/2 ≈ .618.
Corollary 8. Conditioned on E occurring  (cid:107)B(cid:107)2
Proof. By Lemma 6  for any unit vector u 

2 ≤ (1 + 7√c)(2 + ζ).

(cid:107)Bu(cid:107)2 ≤ (1 + 7√c)(2 − (cid:107)uT(cid:107)2 − (cid:107)uR(cid:107)2 + 2(cid:107)uS + uT(cid:107)(cid:107)uR(cid:107)).

it is maximized when (cid:107)uT(cid:107) = 0  for which it equals (1 + 7√c)(2−(cid:107)uR(cid:107)2 + 2(cid:112)1 − (cid:107)uR(cid:107)2(cid:107)uR(cid:107)) 

Suppose we replace the vector uS + uT with an arbitrary vector supported on coordinates in S with
the same norm as uS +uT . Then the right hand side of this expression cannot increase  which means
and setting (cid:107)uR(cid:107) to equal the x in Fact 7  we see that this expression is at most (1+7√c)(2+ζ).
Write the projection matrix P output by the streaming algorithm as U U T   where U is d × k with
orthonormal columns ui (so R†R = P in the notation of Section 1). Applying Lemma 6 and Fact 7
to each of the columns ui  we show in the full version:

k(cid:88)

i=1

(cid:107)ui

T(cid:107)2).

(2)

Using the matrix Pythagorean theorem  we thus have 
(cid:107)B − BP(cid:107)2

F = (cid:107)B(cid:107)2

F − (cid:107)BP(cid:107)2

(cid:107)BP(cid:107)2

F ≤ (1 + 7√c)((2 + ζ)k −
k(cid:88)

F

≥ 2ck/ + k − (1 + 7√c)((2 + ζ)k −
(cid:107)ui
≥ 2ck/ + k − (1 + 7√c)(2 + ζ)k + (1 + 7√c)

i=1

T(cid:107)2) using (cid:107)B(cid:107)2
k(cid:88)

(cid:107)ui

T(cid:107)2.

i=1

F = 2ck/ + k

(3)

F cannot be too large if Alice and Bob succeed in solving f. First  we
We now argue that (cid:107)B − BP(cid:107)2
F . To do so  we create a matrix ˜Bk of rank-k and bound (cid:107)B− ˜Bk(cid:107)2
need to upper bound (cid:107)B− Bk(cid:107)2
F .
Matrix ˜Bk will be 0 on the rows in B1. We can group the rows of B2 into k pairs so that each pair
has the form ei + vi  ei  where i ∈ [ck/] and vi is a unit vector supported on [d] \ [ck/]. We let
Yi be the optimal (in Frobenius norm) rank-1 approximation to the matrix [ei + vi; ei]. By direct
computation 1 the maximum squared singular value of this matrix is 2 + ζ. Our matrix ˜Bk then
consists of a single Yi for each pair in B2. Observe that ˜Bk has rank at most k and

(cid:107)B − Bk(cid:107)2

F ≤ (cid:107)B − ˜Bk(cid:107)2

F ≤ 2ck/ + k − (2 + ζ)k 

1For an online SVD calculator  see http://www.bluebit.gr/matrix-calculator/

5

Therefore  if Bob succeeds in solving f on input B  then 

(cid:107)B − BP(cid:107)2

F ≤ (1 + )(2ck/ + k − (2 + ζ)k) ≤ 2ck/ + k − (2 + ζ)k + 2ck.

Comparing (3) and (4)  we arrive at  conditioned on E:

(cid:107)ui

T(cid:107)2 ≤

1

1 + 7√c · (7√c(2 + ζ)k + 2ck) ≤ c1k 

k(cid:88)

i=1

(4)

(5)

where c1 > 0 is a constant that can be made arbitrarily small by making c > 0 an arbitrarily small.
Since P is a projector  (cid:107)BP(cid:107)F = (cid:107)BU(cid:107)F . Write U = ˆU + ¯U  where the vectors in ˆU are supported
on T   and the vectors in ¯U are supported on [d] \ T . We have 

(cid:107)B ˆU(cid:107)2

F ≤ (cid:107)B(cid:107)2

2c1k ≤ (1 + 7√c)(2 + ζ)c1k ≤ c2k 

where the ﬁrst inequality uses (cid:107)B ˆU(cid:107)F ≤ (cid:107)B(cid:107)2(cid:107) ˆU(cid:107)F and (5)  the second inequality uses that event
E occurs  and the third inequality holds for a constant c2 > 0 that can be made arbitrarily small by
making the constant c > 0 arbitrarily small.
Combining with (4) and using the triangle inequality 

(cid:107)B ¯U(cid:107)F ≥ (cid:107)BP(cid:107)F − (cid:107)B ˆU(cid:107)F using the triangle inequality

≥ (cid:107)BP(cid:107)F −
=

(cid:112)
(cid:113)
c2k using our bound on (cid:107)B ˆU(cid:107)2
(cid:112)(2 + ζ)k − 2ck −
(cid:112)(2 + ζ)k − c3k 

F −
c2k by (4)

F − (cid:107)B − BP(cid:107)2

(cid:107)B(cid:107)2

(cid:112)

(cid:112)

F

≥
≥

c2k by the matrix Pythagorean theorem

(6)
where c3 > 0 is a constant that can be made arbitrarily small for c > 0 an arbitrarily small constant
(note that c2 > 0 also becomes arbitrarily small as c > 0 becomes arbitrarily small). Hence 
F ≥ (2 + ζ)k − c3k  and together with Corollary 8  that implies (cid:107) ¯U(cid:107)2
(cid:107)B ¯U(cid:107)2
F ≥ k − c4k for a
constant c4 that can be made arbitrarily small by making c > 0 arbitrarily small.
F . Consider any column ¯u of ¯U 
Our next goal is to show that (cid:107)B2
and write it as ¯uS + ¯uR. Hence 

F is almost as large as (cid:107)B ¯U(cid:107)2

¯U(cid:107)2

(cid:107)B ¯u(cid:107)2 = (cid:107)RT ¯uR(cid:107)2 + (cid:107)B2 ¯u(cid:107)2 using B1 ¯u = RT ¯uR

≤ (cid:107)RT ¯uR(cid:107)2 + (cid:107)¯uS + RS ¯uR(cid:107)2 + (cid:107)¯uS(cid:107)2 by deﬁnition of the components
= (cid:107)R¯uR(cid:107)2 + 2(cid:107)¯uS(cid:107)2 + 2¯uT
≤ 1 + 7√c + (cid:107)¯uS(cid:107)2 + 2(cid:107)¯uS(cid:107)(cid:107)RS ¯uR(cid:107)

S RS ¯uR using the Pythagorean theorem

(also using Cauchy-Schwarz to bound the other term).

Suppose (cid:107)RS ¯uR(cid:107) = τ(cid:107)¯uR(cid:107) for a value 0 ≤ τ ≤ 1 + 7√c. Then
(cid:107)B ¯u(cid:107)2 ≤ 1 + 7√c + (cid:107)¯uS(cid:107)2 + 2τ(cid:107)¯uS(cid:107)

using (cid:107)R¯uR(cid:107)2 ≤ (1 + 7√c)(cid:107)¯uR(cid:107)2 and (cid:107)¯uR(cid:107)2 + (cid:107)¯uS(cid:107)2 ≤ 1
(cid:112)1 − (cid:107)¯uS(cid:107)2.
(cid:107)B ¯u(cid:107)2 ≤ 1 + 7√c + (1 − τ )(cid:107)¯uS(cid:107)2 + τ ((cid:107)¯uS(cid:107)2 + 2(cid:107)¯uS(cid:107)

We thus have 

(cid:112)1 − (cid:107)¯uS(cid:107)2)

≤ 1 + 7√c + (1 − τ ) + τ (1 + ζ) by Fact 7
≤ 2 + τ ζ + 7√c 

(7)
and hence  letting τ1  . . .   τk denote the corresponding values of τ for the k columns of ¯U  we have

Comparing the square of (6) with (8)  we have

k(cid:88)

i=1

(cid:107)B ¯U(cid:107)2

F ≤ (2 + 7√c)k + ζ
k(cid:88)

τi ≥ k − c5k 

i=1

6

τi.

(8)

(9)

where c5 > 0 is a constant that can be made arbitrarily small by making c > 0 an arbitrarily small
constant. Now  (cid:107) ¯U(cid:107)2
F ≥ k − c4k as shown above  while since (cid:107)Rs ¯uR(cid:107) = τi(cid:107)¯uR(cid:107) if ¯uR is the i-th
column of ¯U  by (9) we have

(cid:107)RS

¯UR(cid:107)2

F ≥ (1 − c6)k

for a constant c6 that can be made arbitrarily small by making c > 0 an arbitarily small constant.
Now (cid:107)R ¯UR(cid:107)2
the rows of R are the concatenation of rows of RS and RT   so combining with (10)  we arrive at

F ≤ (1 + 7√c)k since event E occurs  and (cid:107)R ¯UR(cid:107)2

F = (cid:107)RT

F +(cid:107)RS

¯UR(cid:107)2

¯UR(cid:107)2

F since

(cid:107)RT

¯UR(cid:107)2

F ≤ c7k 

for a constant c7 > 0 that can be made arbitrarily small by making c > 0 arbitrarily small.
Combining the square of (6) with (11)  we thus have
F = (cid:107)B ¯U(cid:107)2

F = (cid:107)B ¯U(cid:107)2

F ≥ (2 + ζ)k − c3k − c7k

F − (cid:107)RT

¯UR(cid:107)2

¯U(cid:107)2

¯U(cid:107)2

(cid:107)B2

F − (cid:107)B1
≥ (2 + ζ)k − c8k 

where the constant c8 > 0 can be made arbitrarily small by making c > 0 arbitrarily small.
By the triangle inequality 

(10)

(11)

(12)

(13)

(cid:113)
(cid:113)
(cid:113)

(cid:107)B2U(cid:107)F ≥ (cid:107)B2

¯U(cid:107)F − (cid:107)B2

ˆU(cid:107)F ≥ ((2 + ζ)k − c8k)1/2 − (c2k)1/2.

Hence 

(cid:107)B2 − B2P(cid:107)F =
≤
≤

F − (cid:107)B2U(cid:107)2
F − ((cid:107)B2

F Matrix Pythagorean  (cid:107)B2U(cid:107)F = (cid:107)B2P(cid:107)F
¯U(cid:107)F − (cid:107)B2

(cid:107)B2(cid:107)2
(cid:107)B2(cid:107)2
3k − (((2 + ζ)k − c8k)1/2 − (c2k)1/2)2 Using (13) (cid:107)B2(cid:107)2

ˆU(cid:107)F )2 Triangle Inequality

F = 3k (14)
(15)
or equivalently  (cid:107)B2 − B2P(cid:107)2
F ≤ 3k − ((2 + ζ)k − c8k) − (c2k) + 2k(((2 + ζ) − c8)c2)1/2 ≤
(1 − ζ)k + c8k + 2k(((2 + ζ) − c8)c2)1/2 ≤ (1 − ζ)k + c9k for a constant c9 > 0 that can be made
arbitrarily small by making the constant c > 0 small enough. This intuitively says that P provides a
good low rank approximation for the matrix B2. Notice that by (14) 

(cid:107)B2P(cid:107)2

F = (cid:107)B2(cid:107)2

F − (cid:107)B2 − B2P(cid:107)2

(cid:96) . By direct computation2 Z T

F ≥ 3k − (1 − ζ)k − c9k ≥ (2 + ζ)k − c9k.

(16)
Now B2 is a 2k × d matrix and we can partition its rows into k pairs of rows of the form Z(cid:96) =
(ei(cid:96) +Ri(cid:96)  ei(cid:96))  for (cid:96) = 1  . . .   k. Here we abuse notation and think of Ri(cid:96) as a d-dimensional vector 
its ﬁrst ck/ coordinates set to 0. Each such pair of rows is a rank-2 matrix  which we abuse notation
and call Z T
(cid:96) has squared maximum singular value 2 + ζ. We would
like to argue that the projection of P onto the row span of most Z(cid:96) has length very close to 1. To
this end  for each Z(cid:96) consider the orthonormal basis V T
(cid:96) of right singular vectors for its row space
(cid:96) 2 be these two right singular vectors with corresponding
(which is span(ei(cid:96)   Ri(cid:96))). We let vT
singular values σ1 and σ2 (which will be the same for all (cid:96)  see below). We are interested in the
F which intuitively measures how much of P gets projected onto the

quantity ∆ =(cid:80)k
row spaces of the Z T
Lemma 9. Conditioned on event E  ∆ ∈ [k − c10k  k + c10k]  where c10 > 0 is a constant that can
be made arbitrarily small by making c > 0 arbitrarily small.

(cid:96)=1 (cid:107)V T
(cid:96) . The following lemma and corollary are shown in the full version.

(cid:96) P(cid:107)2

(cid:96) 1  vT

The following corollary is shown in the full version.

Corollary 10. Conditioned on event E  for a 1−√c9 + 2c10 fraction of (cid:96) ∈ [k]  (cid:107)V T
and for a 99/100 fraction of (cid:96) ∈ [k]  we have (cid:107)V T
can be made arbitrarily small by making the constant c > 0 arbitrarily small.

F ≤ 1+c11 
F ≥ 1 − c11  where c11 > 0 is a constant that

(cid:96) P(cid:107)2

(cid:96) P(cid:107)2

2We again used the calculator at http://www.bluebit.gr/matrix-calculator/

7

(cid:96) P(cid:107)2

(cid:96) P(cid:107)2

Recall that Bob holds i = i(cid:96) for a random (cid:96) ∈ [k]. It follows (conditioned on E) by a union bound
that with probability at least 49/50  (cid:107)V T
F ∈ [1 − c11  1 + c11]  which we call the event F and
condition on. We also condition on event G that (cid:107)Z T
F ≥ (2+ζ)−c12  for a constant c12 > 0 that
can be made arbitrarily small by making c > 0 an arbitrarily small constant. Combining the ﬁrst part
of Corollary 10 together with (16)  event G holds with probability at least 99.5/100  provided c > 0
is a sufﬁciently small constant. By a union bound it follows that E  F  and G occur simultaneously
with probability at least 49/51.
As (cid:107)Z T
1 = 1 − ζ  events E F  and G
(cid:96) P(cid:107)2
F = σ2
imply that (cid:107)vT
(cid:96) 1P(cid:107)2 ≥ 1 − c13  where c13 > 0 is a constant that can be made arbitrarily small by
(cid:96) 1P(cid:107)2 = (cid:104)v(cid:96) 1  z(cid:105)2  where z is a unit
making the constant c > 0 arbitrarily small. Observe that (cid:107)vT
vector in the direction of the projection of v(cid:96) 1 onto P .
By the Pythagorean theorem  (cid:107)v(cid:96) 1 − (cid:104)v(cid:96) 1  z(cid:105)z(cid:107)2 = 1 − (cid:104)v(cid:96) 1  z(cid:105)2  and so

(cid:96) 2P(cid:107)2  with σ2

1 = 2 + ζ and σ2

(cid:96) 1P(cid:107)2 + σ2

1(cid:107)vT

2(cid:107)vT

(cid:107)v(cid:96) 1 − (cid:104)v(cid:96) 1  z(cid:105)z(cid:107)2 ≤ c14 

(17)

for a constant c14 > 0 that can be made arbitrarily small by making c > 0 arbitrarily small.
(cid:96) P = σ1(cid:104)v(cid:96) 1  z(cid:105)u(cid:96) 1zT + σ2(cid:104)v(cid:96) 2  w(cid:105)u(cid:96) 2wT   where w is a unit vector in the
We thus have Z T
direction of the projection of of v(cid:96) 2 onto P   and u(cid:96) 1  u(cid:96) 2 are the left singular vectors of Z T
(cid:96) . Since
F occurs  we have that |(cid:104)v(cid:96) 2  w(cid:105)| ≤ c11  where c11 > 0 is a constant that can be made arbitrarily
small by making the constant c > 0 arbitrarily small. It follows now by (17) that

(cid:107)Z T

(cid:96) 1(cid:107)2

F ≤ c15 

(cid:96) P − σ1u(cid:96) 1vt

i(cid:96)P − (2 + ζ)(.448ei(cid:96) + .277Ri(cid:96))(cid:107)2 ≤ c15.

(18)
where c15 > 0 is a constant that can be made arbitrarily small by making the constant c > 0
arbitrarily small.
By direct calculation3   u(cid:96) 1 = −.851ei(cid:96) − .526Ri(cid:96) and v(cid:96) 1 = −.851ei(cid:96) − .526Ri(cid:96). It follows that
F ≤ c15. Since ei(cid:96) is the second row of
(cid:96) P − (2 + ζ)[.724ei(cid:96) + .448Ri(cid:96) ; .448ei(cid:96) + .277Ri(cid:96)](cid:107)2
(cid:107)Z T
(cid:96)   it follows that (cid:107)eT
Z T
Observe that Bob has ei(cid:96) and P   and can therefore compute eT
i(cid:96)P . Moreover  as c15 > 0 can be made
arbitrarily small by making the constant c > 0 arbitrarily small  it follows that a 1 − c16 fraction of
the signs of coordinates of eT
i(cid:96)P   restricted to coordinates in [d] \ [ck/]  must agree with those of
(2 + ζ).277Ri(cid:96)  which in turn agree with those of Ri(cid:96). Here c16 > 0 is a constant that can be made
arbitrarily small by making the constant c > 0 arbitrarily small. Hence  in particular  the sign of the
j-th coordinate of Ri(cid:96)  which Bob needs to output  agrees with that of the j-th coordinate of eT
i(cid:96)P
with probability at least 1 − c16. Call this event H.
By a union bound over the occurrence of events E F  G  and H  and the streaming algorithm suc-
ceeding (which occurs with probability 3/4)  it follows that Bob succeeds in solving Index with
probability at least 49/51 − 1/4 − c16 > 2/3  as required. This completes the proof.
3 Conclusion

We have shown an Ω(dk/) bit lower bound for streaming algorithms in the row-update model for
outputting a k × d matrix R with (cid:107)A − AR†R(cid:107)F ≤ (1 + )(cid:107)A − Ak(cid:107)F   thus showing that the
algorithm of [9] is optimal up to the word size. The next natural goal would be to obtain multi-pass
lower bounds  which seem quite challenging. Such lower bound techniques may also be useful for
showing the optimality of a constant-round O(sdk/) + (sk/)O(1) communication protocol in [12]
for low-rank approximation in the distributed communication model.

Acknowledgments.
I would like to thank Edo Liberty and Jeff Phillips for many useful discusions
and detailed comments on this work (thanks to Jeff for the ﬁgure!).
I would also like to thank
the XDATA program of the Defense Advanced Research Projects Agency (DARPA)  administered
through Air Force Research Laboratory contract FA8750-12-C0323 for supporting this work.

3Using the online calculator in earlier footnotes.

8

References
[1] N. Alon  P. B. Gibbons  Y. Matias  and M. Szegedy. Tracking join and self-join sizes in limited

storage. J. Comput. Syst. Sci.  64(3):719–747  2002.

[2] A. Andoni and H. L. Nguyen. Eigenvalues of a matrix in the streaming model. In SODA  pages

1729–1737  2013.

[3] M. Brand. Incremental singular value decomposition of uncertain data with missing values. In

ECCV (1)  pages 707–720  2002.

[4] M. Charikar  K. Chen  and M. Farach-Colton. Finding frequent items in data streams. Theor.

Comput. Sci.  312(1):3–15  2004.

[5] K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In STOC 

pages 205–214  2009.

[6] G. Cormode and S. Muthukrishnan. An improved data stream summary: the count-min sketch

and its applications. J. Algorithms  55(1):58–75  2005.

[7] D. Feldman  M. Schmidt  and C. Sohler. Turning big data into tiny data: Constant-size coresets

for k-means  pca and projective clustering. In SODA  pages 1434–1453  2013.

[8] A. M. Frieze  R. Kannan  and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank

approximations. J. ACM  51(6):1025–1041  2004.

[9] M. Ghashami and J. M. Phillips. Relative errors for deterministic low-rank matrix approxima-

tions. In SODA  pages 707–717  2014.

[10] G. H. Golub and C. F. van Loan. Matrix computations (3. ed.). Johns Hopkins University

Press  1996.

[11] P. M. Hall  A. D. Marshall  and R. R. Martin. Incremental eigenanalysis for classiﬁcation. In

BMVC  pages 1–10  1998.

[12] R. Kannan  S. Vempala  and D. P. Woodruff. Nimble algorithms for cloud computing. CoRR 

2013.

[13] I. Kremer  N. Nisan  and D. Ron. On randomized one-round communication complexity.

Computational Complexity  8(1):21–49  1999.

[14] E. Kushilevitz and N. Nisan. Communication complexity. Cambridge University Press  1997.
[15] A. Levy and M. Lindenbaum. Efﬁcient sequential karhunen-loeve basis extraction. In ICCV 

page 739  2001.

[16] E. Liberty. Simple and deterministic matrix sketching. In KDD  pages 581–588  2013.
[17] J. Misra and D. Gries. Finding repeated elements. Sci. Comput. Program.  2(2):143–152  1982.
[18] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in

Theoretical Computer Science  1(2)  2005.

[19] D. A. Ross  J. Lim  R.-S. Lin  and M.-H. Yang. Incremental learning for robust visual tracking.

International Journal of Computer Vision  77(1-3):125–141  2008.

[20] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular

values. CoRR  2010.

[21] T. Sarl´os. Improved approximation algorithms for large matrices via random projections. In

FOCS  pages 143–152  2006.

9

,David Woodruff
Wataru Kumagai