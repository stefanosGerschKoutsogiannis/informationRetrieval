2012,Fused sparsity and robust estimation for linear models with unknown variance,In this paper  we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm  termed Scaled Fused Dantzig Selector (SFDS)  that accomplishes the aforementioned learning task by means of a second-order cone program. A special  emphasize is put on the particular instance of fused sparsity corresponding to  the learning in presence of outliers. We establish finite sample risk bounds and  carry out an experimental evaluation on both synthetic and real data.,Fused sparsity and robust estimation for linear

models with unknown variance

Yin Chen

University Paris Est  LIGM

77455 Marne-la-Valle  FRANCE
yin.chen@eleves.enpc.fr

Arnak S. Dalalyan

ENSAE-CREST-GENES

92245 MALAKOFF Cedex  FRANCE
arnak.dalalyan@ensae.fr

Abstract

In this paper  we develop a novel approach to the problem of learning sparse rep-
resentations in the context of fused sparsity and unknown noise level. We propose
an algorithm  termed Scaled Fused Dantzig Selector (SFDS)  that accomplishes
the aforementioned learning task by means of a second-order cone program. A
special emphasize is put on the particular instance of fused sparsity corresponding
to the learning in presence of outliers. We establish ﬁnite sample risk bounds and
carry out an experimental evaluation on both synthetic and real data.

1

Introduction

Consider the classical problem of Gaussian linear regression1:

∗

+ σ∗ξ 

ξ ∼ Nn(0  In) 

Y = Xβ

∗. Even if the ambient dimensionality p of β

(1)
where Y ∈ Rn and X ∈ Rn×p are observed  in the neoclassical setting of very large dimensional
∗ is larger than n  it has proven
unknown vector β
possible to consistently estimate this vector under the sparsity assumption. The letter states that the
∗  denoted by s and called intrinsic dimension  is small compared
number of nonzero elements of β
to the sample size n. Most famous methods of estimating sparse vectors  the Lasso and the Dantzig
Selector (DS)  rely on convex relaxation of (cid:96)0-norm penalty leading to a convex program that in-
volves the (cid:96)1-norm of β. More precisely  for a given ¯λ > 0  the Lasso and the DS [26  4  5  3] are
deﬁned as

(cid:27)

L

= arg min
β∈Rp

(cid:107)Y − Xβ(cid:107)2

2 + ¯λ(cid:107)β(cid:107)1

DS

= arg min(cid:107)β(cid:107)1 subject to (cid:107)X(cid:62)(Y − Xβ)(cid:107)∞ ≤ ¯λ.

(Lasso)

(DS)

(cid:26) 1

2

(cid:98)β

(cid:98)β

The performance of these algorithms depends heavily on the choice of the tuning parameter ¯λ.
Several empirical and theoretical studies emphasized that ¯λ should be chosen proportionally to the
noise standard deviation σ∗. Unfortunately  in most applications  the latter is unavailable.
It is
therefore vital to design statistical procedures that estimate β and σ in a joint fashion. This topic
received special attention in last years  cf. [10] and the references therein  with the introduction of
computationally efﬁcient and theoretically justiﬁed σ-adaptive procedures the square-root Lasso [2]
(a.k.a. scaled Lasso [24]) and the (cid:96)1 penalized log-likelihood minimization [20].
In the present work  we are interested in the setting where β
known q × p matrix M  the vector Mβ

∗ is not necessarily sparse  but for a
∗ is sparse. We call this setting “fused sparsity scenario”.
1We denote by In the n × n identity matrix. For a vector v  we use the standard notation (cid:107)v(cid:107)1  (cid:107)v(cid:107)2 and
(cid:107)v(cid:107)∞ for the (cid:96)1  (cid:96)2 and (cid:96)∞ norms  corresponding respectively to the sum of absolute values  the square root
of the sum of squares and the maximum of the coefﬁcients of v.

1

The term “fused” sparsity  introduced by [27]  originates from the case where Mβ is the discrete
derivative of a signal β and the aim is to minimize the total variation  see [12  19] for a recent
overview and some asymptotic results. For general matrices M  tight risk bounds were proved in
[14]. We adopt here this framework of general M and aim at designing a computationally efﬁcient
procedure capable to handle the situation of unknown noise level and for which we are able to
provide theoretical guarantees along with empirical evidence for its good performance.
This goal is attained by introducing a new procedure  termed Scaled Fused Dantzig Selector (SFDS) 
which is closely related to the penalized maximum likelihood estimator but has some advantages in
terms of computational complexity. We establish tight risk bounds for the SFDS  which are nearly
as strong as those proved for the Lasso and the Dantzig selector in the case of known σ∗. We also
show that the robust estimation in linear models can be seen as a particular example of the fused
sparsity scenario. Finally  we carry out a “proof of concept” type experimental evaluation to show
the potential of our approach.

2 Estimation under fused sparsity with unknown level of noise

2.1 Scaled Fused Dantzig Selector
We will only consider the case rank(M) = q ≤ p  which is more relevant for the applications
we have in mind (image denoising and robust estimation). Under this condition  one can ﬁnd a
(p− q)× p matrix N such that the augmented matrix M = [M(cid:62) N(cid:62)](cid:62) is of full rank. Let us denote
by mj the jth column of the matrix M −1  so that M −1 = [m1  ...  mp]. We also introduce:
M −1 = [M†  N†]  M† = [m1  ...  mq] ∈ Rp×q  N† = [mq+1  ...  mp] ∈ Rp×(p−q).

Given two positive tuning parameters λ and µ  we deﬁne the Scaled Fused Dantzig Selector (SFDS)

((cid:98)β (cid:98)σ) as a solution to the following optimization problem:


(cid:107)Xmj(cid:107)2|(Mβ)j| subject to

minimize

q(cid:88)

j=1

j X(cid:62)(Xβ − Y )| ≤ λσ(cid:107)Xmj(cid:107)2  j ≤ q;
|m(cid:62)
† X(cid:62)(Xβ − Y ) = 0 
N(cid:62)
nµσ2 + Y (cid:62)Xβ ≤ (cid:107)Y (cid:107)2
2.

(P1)

This estimator has several attractive properties: (a) it can be efﬁciently computed even for very large
scale problems using a second-order cone program  (b) it is equivariant with respect to the scale
transformations both in the response Y and in the lines of M and  ﬁnally  (c) it is closely related to
the penalized maximum likelihood estimator. Let us give further details on these points.

2.2 Relation with the penalized maximum likelihood estimator

One natural way to approach the problem of estimating β
procedure of penalized log-likelihood minimization.
Nn(0  In)  the negative log-likelihood (up to irrelevant additive terms) is given by

∗ in our setup is to rely on the standard
If the noise distribution is Gaussian  ξ ∼

(cid:96)(Y   X; β  σ) = n log(σ) +

(cid:107)Y − Xβ(cid:107)2

2

2σ2

.

In the context of large dimension we are concerned with  i.e.  when p/n is not small  the maximum
likelihood estimator is subject to overﬁtting and is of very poor quality. If it is plausible to expect
∗ such that for some matrix M  only a
that the data can be ﬁtted sufﬁciently well by a vector β
∗ are nonzero  then one can considerably improve the quality of
small fraction of elements of Mβ
estimation by adding a penalty term to the log-likelihood. However  the most appealing penalty 
(cid:80)
the number of nonzero elements of Mβ  leads to a nonconvex optimization problem which cannot
be efﬁciently solved even for moderately large values of p. Instead  convex penalties of the form
at a relatively low computational cost. This corresponds to deﬁning the estimator ((cid:98)βPL (cid:98)σPL) as the
j ωj|(Mβ)j|  where wj > 0 are some weights  have proven to provide high accuracy estimates

2

minimizer of the penalized log-likelihood

¯(cid:96)(Y   X; β  σ) = n log(σ) +

(cid:107)Y − Xβ(cid:107)2

2

2σ2

+

q(cid:88)

j=1

ωj|(Mβ)j|.

To ensure the scale equivariance  the weights ωj should be chosen inversely proportionally to σ:
ωj = σ−1 ¯ωj. This leads to the estimator

((cid:98)βPL (cid:98)σPL) = arg min

β σ

(cid:26)

n log(σ) +

(cid:107)Y − Xβ(cid:107)2

2

2σ2

+

|(Mβ)j|

σ

¯ωj

q(cid:88)

j=1

(cid:27)

.

Although this problem can be cast [20] as a problem of convex minimization (by making the change
of parameters φ = β/σ and ρ = 1/σ)  it does not belong to the standard categories of convex
problems that can be solved either by linear programming or by second-order cone programming or
by semideﬁnite programming. Furthermore  the smooth part of the objective function is not Lips-
chitz which makes it impossible to directly apply most ﬁrst-order optimization methods developed in
recent years. Our goal is to propose a procedure that is close in spirit to the penalized maximum like-
lihood but has the additional property of being computable by standard algorithms of second-order
cone programming.
To achieve this goal  at the ﬁrst step  we remark that it can be useful to introduce a penalty term that
depends exclusively on σ and that prevents the estimator of σ∗ from being too large or too small. One
can show that the only function (up to a multiplicative constant) that can serve as penalty without
breaking the property of scale equivariance is the logarithmic function. Therefore  we introduce an
additional tuning parameter µ > 0 and look for minimizing the criterion
|(Mβ)j|

(cid:107)Y − Xβ(cid:107)2

q(cid:88)

nµ log(σ) +

2σ2

2

+

¯ωj

j=1

.

σ

If we make the change of variables φ1 = Mβ/σ  φ2 = Nβ/σ and ρ = 1/σ  we get a convex
function for which the ﬁrst-order conditions [20] take the form

(2)

(3)
(4)

(5)

j X(cid:62)(Y − Xβ) ∈ ¯ωjsign({Mβ}j) 
m(cid:62)
† X(cid:62)(Y − Xβ) = 0 
N(cid:62)

(cid:0)(cid:107)Y (cid:107)2

2 − Y (cid:62)Xβ(cid:1) = σ2.

1
nµ

norm(cid:80)

Thus  any minimizer of (2) should satisfy these conditions. Therefore  to simplify the problem of
optimization we propose to replace minimization of (2) by the minimization of the weighted (cid:96)1-
j ¯ωj|(Mβ)j| subject to some constraints that are as close as possible to (3-5). The only
problem here is that the constraints (3) and (5) are not convex. The “convexiﬁcation” of these
constraints leads to the procedure described in (P1). As we explain below  the particular choice of
¯ωjs is dictated by the desire to enforce the scale equivariance of the procedure.

2.3 Basic properties

A key feature of the SFDS is its scale equivariance. Indeed  one easily checks that if ((cid:98)β (cid:98)σ) is a
solution to (P1) for some inputs X  Y and M  then α((cid:98)β (cid:98)σ) will be a solution to (P1) for the inputs
More precisely  if ((cid:98)β (cid:98)σ) is a solution to (P1) for some inputs X  Y and M  then ((cid:98)β (cid:98)σ) will be a

X  αY and M  whatever the value of α ∈ R is. This is the equivariance with respect to the scale
change in the response Y . Our method is also equivariant with respect to the scale change in M.
solution to (P1) for the inputs X  Y and DM  whatever the q × q diagonal matrix D is. The latter
∗ is sparse  then
property is important since if we believe that for a given matrix M the vector Mβ
∗  for any diagonal matrix D. Having a procedure the output
this is also the case for the vector DMβ
of which is independent of the choice of D is of signiﬁcant practical importance  since it leads to a
solution that is robust with respect to small variations of the problem formulation.
The second attractive feature of the SFDS is that it can be computed by solving a convex optimiza-
tion problem of second-order cone programming (SOCP). Recall that an SOCP is a constrained

3

optimization problem that can be cast as minimization with respect to w ∈ Rd of a linear function
a(cid:62)w under second-order conic constraints of the form (cid:107)Aiw + bi(cid:107)2 ≤ c(cid:62)
i w + di  where Ais are
some ri × d matrices  bi ∈ Rri  ci ∈ Rd are some vectors and dis are some real numbers. The
problem (P1) belongs well to this category  since it can be written as min(u1 + . . . + uq) subject to

(cid:107)Xmj(cid:107)2|(Mβ)j| ≤ uj;

† X(cid:62)(Xβ − Y ) = 0 
N(cid:62)

|m(cid:62)

(cid:113)
j X(cid:62)(Xβ − Y )| ≤ λσ(cid:107)Xmj(cid:107)2 

4nµ(cid:107)Y (cid:107)2

2σ2 + (Y (cid:62)Xβ)2 ≤ 2(cid:107)Y (cid:107)2

∀j = 1  . . .   q;
2 − Y (cid:62)Xβ.

Note that all these constraints can be transformed into linear inequalities  except the last one which
is a second order cone constraint. The problems of this type can be efﬁciently solved by various
standard toolboxes such as SeDuMi [22] or TFOCS [1].

2.4 Finite sample risk bound

To provide theoretical guarantees for our estimator  we impose the by now usual assumption of
restricted eigenvalues on a suitably chosen matrix. This assumption  stated in Deﬁnition 2.1 below 
was introduced and thoroughly discussed by [3]; we also refer the interested reader to [28].
D´eﬁnition 2.1. We say that a n × q matrix A satisﬁes the restricted eigenvalue condition RE(s  1) 
if

We say that A satisﬁes the strong restricted eigenvalue condition RE(s  s  1)  if

κ(s  1) ∆= min|J|≤s

min

(cid:107)δJc(cid:107)1≤(cid:107)δJ(cid:107)1

κ(s  s  1) ∆= min|J|≤s

min

(cid:107)δJc(cid:107)1≤(cid:107)δJ(cid:107)1

(cid:107)Aδ(cid:107)2
√
n(cid:107)δJ(cid:107)2

> 0.

√

(cid:107)Aδ(cid:107)2
n(cid:107)δJ∪J0(cid:107)2

> 0 

n M(cid:62)

where J0 is the subset of {1  ...  q} corresponding to the s largest in absolute value coordinates of δ.
For notational convenience  we assume that M is normalized in such a way that the diagonal ele-
† X(cid:62)XM† are all equal to 1. This can always be done by multiplying M from the
ments of 1
left by a suitably chosen positive deﬁnite diagonal matrix. Furthermore  we will repeatedly use the
† X(cid:62) onto the subspace of Rn spanned by the columns of
projector2 Π = XN†(N(cid:62)
XN†. We denote by r = rank{Π} the rank of this projector which is typically very small compared
to n ∧ p  and is always smaller than n ∧ (p − q). In all theoretical results  the matrices X and M are
assumed deterministic.

Theorem 2.1. Let us ﬁx a tolerance level δ ∈ (0  1) and deﬁne λ =(cid:112)2nγ log(q/δ). Assume that

† X(cid:62)XN†)−1N(cid:62)

the tuning parameters γ  µ > 0 satisfy
≤ 1 − r
n

µ
γ

(6)
∗ is s-sparse and the matrix (In − Π)XM† satisﬁes the condition RE(s  1) with

n

.

If the vector Mβ
some κ > 0 then  with probability at least 1 − 6δ  it holds:

− 2

(cid:112)(n − r) log(1/δ) + log(1/δ)
(cid:114)
(cid:114)
κ2 ((cid:98)σ + σ∗)s
(cid:112)2γs log(q/δ)
)(cid:107)2 ≤ 2((cid:98)σ + σ∗)
(cid:114)
∗(cid:107)2 ≤ 4((cid:98)σ + σ∗)

)(cid:107)1 ≤ 4

2γ log(q/δ)

2s log(q/δ)

(cid:114)

σ∗
κ

+

n

κ2

n

σ∗
κ

∗

∗

(cid:107)M((cid:98)β − β
(cid:107)X((cid:98)β − β
(cid:107)M(cid:98)β − Mβ

(8)
If  in addition  (In − Π)XM† satisﬁes the condition RE(s  s  1) with some κ > 0 then  with a
probability at least 1 − 6δ  we have:

κ

+

2s log(1/δ)

+ σ∗(cid:0)(cid:112)8 log(1/δ) + r(cid:1).

n

Moreover  with a probability at least 1 − 7δ  we have:

(cid:98)σ ≤ σ∗

µ1/2

∗(cid:107)1

λ(cid:107)Mβ
nµ

+

+

s1/2σ∗ log(q/δ)

nκµ1/2

+ (σ∗ + (cid:107)Mβ

∗(cid:107)1)µ−1/2

2 log(1/δ)

n

.

(10)

2 log(1/δ)

n

(cid:114)

(7)

(9)

2Here and in the sequel  the inverse of a singular matrix is understood as MoorePenrose pseudoinverse.

4

∗

)(cid:107)2

n(cid:107)X((cid:98)β− β

Before looking at the consequences of these risk bounds in the particular case of robust estimation 
let us present some comments highlighting the claims of Theorem 2.1. The ﬁrst comment is about
the conditions on the tuning parameters µ and γ. It is interesting to observe that the roles of these
∗ while µ determines the
parameters are very clearly deﬁned: γ controls the quality of estimating β
quality of estimating σ∗. One can note that all the quantities entering in the right-hand side of (6)
are known  so that it is not hard to choose µ and γ in such a way that they satisfy the conditions of
Theorem 2.1. However  in practice  this theoretical choice may be too conservative in which case it
could be a better idea to rely on cross validation.
The second remark is about the rates of convergence. According to (8)  the rate of estimation
measured in the mean prediction loss 1
2 is of the order of s log(q)/n  which is known
∗ is also estimated with the nearly parametric rate in both
as fast or parametric rate. The vector Mβ
(cid:96)1 and (cid:96)2-norms. To the best of our knowledge  this is the ﬁrst work where such kind of fast rates
are derived in the context of fused sparsity with unknown noise-level. With some extra work  one
can check that if  for instance  γ = 1 and |µ − 1| ≤ cn−1/2 for some constant c  then the estimator
to the noise level is the presence of (cid:107)Mβ
estimation in the case of large signal-to-noise ratio.
Even if Theorem 2.1 requires the noise distribution to be Gaussian  the proposed algorithm remains
valid in a far broader context and tight risk bounds can be obtained under more general conditions
on the noise distribution. In fact  one can see from the proof that we only need to know conﬁdence
sets for some linear and quadratic functionals of ξ. For instance  such kind of conﬁdence sets can be
readily obtained in the case of bounded errors ξi using the Bernstein inequality. It is also worthwhile
to mention that the proof of Theorem 2.1 is not a simple adaptation of the arguments used to prove
analogous results for ordinary sparsity  but contains some qualitatively novel ideas. More precisely 
the cornerstone of the proof of risk bounds for the Dantzig selector [4  3  9] is that the true parameter
∗ is a feasible solution. In our case  this argument cannot be used anymore. Our proposal is then
β

(cid:98)σ has also a risk of the order of sn−1/2. However  the price to pay for being adaptive with respect
∗(cid:107)1 in the bound on(cid:98)σ  which deteriorates the quality of

to specify another vector(cid:101)β that simultaneously satisﬁes the following three conditions: M(cid:101)β has the

same sparsity pattern as Mβ
A last remark is about the restricted eigenvalue conditions. They are somewhat cumbersome in this
abstract setting  but simplify a lot when the concrete example of robust estimation is considered 
cf. the next section. At a heuristical level  these conditions require from the columns of XM† to
be not very strongly correlated. Unfortunately  this condition fails for the matrices appearing in
the problem of multiple change-point detection  which is an important particular instance of fused
sparsity. There are some workarounds to circumvent this limitation in that particular setting  see
[17  11]. The extension of these kind of arguments to the case of unknown σ∗ is an open problem
we intend to tackle in the near future.

∗ (cid:101)β is close to β

∗ and lies in the feasible set.

3 Application to robust estimation
This methodology can be applied in the context of robust estimation  i.e.  when we observe Y ∈ Rn
and A ∈ Rn×k such that the relation

Yi = (Aθ

∗

)i + σ∗ξi 

iid∼ N (0  1)

ξi

holds only for some indexes i ∈ I ⊂ {1  ...  n}  called inliers. The indexes does not belonging to
I will be referred to as outliers. The setting we are interested in is the one frequently encountered
∗ is small as compared to n but the presence
in computer vision [13  25]: the dimensionality k of θ
of outliers causes the complete failure of the least squares estimator. In what follows  we use the
standard assumption that the matrix 1
Following the ideas developed in [6  7  8  18  15]  we introduce a new vector ω ∈ Rn that serves to
characterize the outliers. If an entry ωi of ω is nonzero  then the corresponding observation Yi is an
outlier. This leads to the model:

n A(cid:62)A has diagonal entries equal to one.

+

nω∗ + σ∗ξ = Xβ

](cid:62).
Y = Aθ
Thus  we have rewritten the problem of robust estimation in linear models as a problem of
estimation in high dimension under the fused sparsity scenario. Indeed  we have X ∈ Rn×(n+k)

and β = [ω∗ ; θ

n In A] 

√
+ σ∗ξ  where X = [

√

∗

∗

∗

5

∗ ∈ Rn+k  and we are interested in ﬁnding an estimator(cid:98)β of β

and β
contains as many zeros as possible. This means that we expect that the number of outliers is
signiﬁcantly smaller than the sample size. We are thus in the setting of fused sparsity with
M = [In 0n×k]. Setting N = [0k×n Ik]  we deﬁne the Scaled Robust Dantzig Selector (SRDS) as

∗ for which (cid:98)ω = [In0n×k](cid:98)β

a solution ((cid:98)θ (cid:98)ω (cid:98)σ) of the problem:



√
n ω − Y (cid:107)∞ ≤ λσ 
√
n ω − Y ) = 0 

√
n(cid:107)Aθ +
A(cid:62)(Aθ +
nµσ2 + Y (cid:62)(Aθ +

√

nω) ≤ (cid:107)Y (cid:107)2
2.

(P2)

minimize (cid:107)ω(cid:107)1

subject to

n A denoted by ν∗ and ν∗ respectively.

Once again  this can be recast in a SOCP and solved with great efﬁciency by standard algorithms.
Furthermore  the results of the previous section provide us with strong theoretical guarantees for the
SRDS. To state the corresponding result  we will need a notation for the largest and the smallest
singular values of 1√

Theorem 3.1. Let us ﬁx a tolerance level δ ∈ (0  1) and deﬁne λ =(cid:112)2nγ log(n/δ). Assume that
(cid:0)(cid:112)(n − k) log(1/δ) + log(1/δ)(cid:1). Let Π

γ ≤ 1 − k
n − 2
the tuning parameters γ  µ > 0 satisfy µ
denote the orthogonal projector onto the k-dimensional subspace of Rn spanned by the columns of
√
n(In − Π) satisﬁes the condition RE(s  1) with
A. If the vector ω∗ is s-sparse and the matrix
(cid:114)
some κ > 0 then  with probability at least 1 − 5δ  it holds:
κ2 ((cid:98)σ + σ∗)s
(cid:114)
(cid:107)(In − Π)((cid:98)ω − ω∗)(cid:107)2 ≤ 2((cid:98)σ + σ∗)

(12)
n (In − Π) satisﬁes the condition RE(s  s  1) with some κ > 0 then  with a proba-

(cid:107)(cid:98)ω − ω∗(cid:107)1 ≤ 4

n
2 log(1/δ)

2γ log(n/δ)

2s log(n/δ)

2s log(1/δ)

(cid:114)

σ∗
κ

(cid:114)

+ σ∗

(11)

√

+

n

n

n

κ

n

.

 

(cid:114)

If  in addition 
bility at least 1 − 6δ  we have:

(cid:114)
(cid:107)(cid:98)ω − ω∗(cid:107)2 ≤ 4((cid:98)σ + σ∗)
(cid:26) 4((cid:98)σ + σ∗)
(cid:107)(cid:98)θ − θ

κ2

∗(cid:107)2 ≤ ν∗
ν2∗
(cid:98)σ ≤ σ∗

µ1/2

2s log(n/δ)

(cid:114)

σ∗
n
κ
2s log(n/δ)

+

2 log(1/δ)

(cid:114)

σ∗
κ

+

n
2 log(1/δ)

√
σ∗(

+

n

κ2

n

Moreover  with a probability at least 1 − 7δ  the following inequality holds:
+ (σ∗ + (cid:107)ω∗(cid:107)1)µ−1/2

s1/2σ∗ log(n/δ)

λ(cid:107)ω∗(cid:107)1

+

+

nµ

nκµ1/2

(cid:27)

k +(cid:112)2 log(1/δ))
(cid:114)

√

n

2 log(1/δ)

.

(13)

n

√

All the comments made after Theorem 2.1  especially those concerning the tuning parameters and
the rates of convergence  hold true for the risk bounds in Theorem 3.1 as well. Furthermore  the
restricted eigenvalue condition in the latter theorem is much simpler and deserves a special attention.
n(In − Π) implies that there is
In particular  one can remark that the failure of RE(s  1) for
a unit vector δ in Im(A) such that |δ(1)| + . . . + |δ(n−s)| ≤ |δ(n−s+1)| + . . . + |δ(n)|  where
δ(k) stands for the kth smallest (in absolute value) entry of δ. To gain a better understanding of
how restrictive this assumption is  let us consider the case where the rows a1  . . .   an of A are
i.i.d. zero mean Gaussian vectors. Since δ ∈ Im(A)  its coordinates δi are also i.i.d. Gaussian
random variables (they can be considered N (0  1) due to the homogeneity of the inequality we are
(cid:80)
interested in). The inequality |δ(1)| + . . . + |δ(n−s)| ≤ |δ(n−s+1)| + . . . + |δ(n)| can be written
n (|δ(n−s+1)| + . . . + |δ(n)|). While the left-hand side of this inequality tends to
i |δi| ≤ 2
as 1
n
E[|δ1|] > 0  the right-hand side is upper-bounded by 2s
.
√
Therefore  if 2s
is small  the condition RE(s  1) is satisﬁed. This informal discussion can be
made rigorous by studying large deviations of the quantity maxδ∈Im(A)\{0} (cid:107)δ(cid:107)∞/(cid:107)δ(cid:107)1. A simple
sufﬁcient condition entailing RE(s  1) for
Lemma 3.2. Let us set ζs(A) = inf u∈Sk−1

Π) satisﬁes both RE(s  1) and RE(s  s  1) with κ(s  1) ≥ κ(s  s  1) ≥ ζs(A)/(cid:112)(ν∗)2 + ζs(A)2.

n(In − Π) is presented in the following lemma.
√

√
n maxi |δi|  which is on the order of 2s

(cid:80)n
i=1 |aiu|− 2s(cid:107)A(cid:107)2 ∞√

. If ζs(A) > 0  then

n (In−

log n
n

log n
n

√

1
n

n

6

SFDS

|(cid:98)β − β∗|2

|(cid:98)σ − σ∗|

Lasso

|(cid:98)β − β∗|2

Ave
0.04
0.09
0.23
0.06
0.20
0.34
0.10
0.19
1.90

StD
0.03
0.05
0.17
0.01
0.05
0.11
0.01
0.09
0.20

Ave
0.18
0.42
0.75
0.28
0.56
0.34
0.36
0.27
4.74

StD
0.14
0.35
0.55
0.11
0.10
0.21
0.02
0.26
1.01

Ave
0.07
0.16
0.31
0.13
0.31
0.73
0.15
0.31
0.61

StD
0.05
0.11
0.21
0.09
0.04
0.25
0.00
0.04
0.08

( T  p  s∗  σ∗)
(200  400  2  .5)
(200  400  2  1)
(200  400  2  2)
(200  400  5  .5)
(200  400  5  1)
(200  400  5  2)
(200  400  10  .5)
(200  400  10  1)
(200  400  10  2)

Square-Root Lasso

|(cid:98)β − β∗|2

|(cid:98)σ − σ∗|

Ave
0.06
0.13
0.25
0.11
0.25
0.47
0.10
0.19
1.80

StD
0.04
0.09
0.18
0.06
0.02
0.29
0.01
0.09
0.04

Ave
0.20
0.46
0.79
0.18
0.66
0.69
0.36
0.27
3.70

StD
0.14
0.37
0.56
0.27
0.05
0.70
0.02
0.26
0.48

Table 1: Comparing our procedure SFDS with the (oracle) Lasso and the SqRL on a synthetic dataset. The

average values and the standard deviations of the quantities |(cid:98)β− β∗|2 and |(cid:98)σ− σ∗| over 500 trials are reported.

They represent respectively the accuracy in estimating the regression vector and the level of noise.

The proof of the lemma can be found in the supplementary material.

One can take note that the problem (P2) boils down to computing ((cid:98)ω (cid:98)σ) as a solution to
(cid:26) √
and then setting(cid:98)θ = (A(cid:62)A)−1A(cid:62)(Y − √

n[(In − Π)Y ](cid:62)ω ≤ (cid:107)(In − Π)Y (cid:107)2
2.

n(cid:107)(In − Π)(
nµσ2 +

√

minimize (cid:107)ω(cid:107)1

subject to

√

nω − Y )(cid:107)∞ ≤ λσ 

n(cid:98)ω).

4 Experiments

For the empirical evaluation we use a synthetic dataset with randomly drawn Gaussian design matrix
X and the real-world dataset fountain-P113  on which we apply our methodology for computing the
fundamental matrices between consecutive images.

4.1 Comparative evaluation on synthetic data
We randomly generated a n × p matrix X with independent entries distributed according to the
∗ ∈ Rp that has exactly s nonzero elements
standard normal distribution. Then we chose a vector β
all equal to one. The indexes of these elements were chosen at random. Finally  the response
Y ∈ Rn was computed by adding a random noise σ∗Nn(0  In) to the signal Xβ
∗. Once Y and X
available  we computed three estimators of the parameters using the standard sparsity penalization
(in order to be able to compare our approach to the others): the SFDS  the Lasso and the square-
root Lasso (SqRL). We used the “universal” tuning parameters for all these methods: (λ  µ) =

((cid:112)2n log(p)  1) for the SFDS  λ =(cid:112)2 log(p) for the SqRL and λ = σ∗(cid:112)2 log(p) for the Lasso.

Note that the latter is not really an estimator but rather an oracle since it exploits the knowledge of
the true σ∗. This is why the accuracy in estimating σ∗ is not reported in Table 1. To reduce the
well known bias toward zero [4  23]  we performed a post-processing for all of three procedures. It
consisted in computing least squares estimators after removing all the covariates corresponding to
∗. The results summarized in Table 1 show that the SFDS
vanishing coefﬁcients of the estimator of β
is competitive with the state-of-the-art methods and  a bit surprisingly  is sometimes more accurate
than the oracle Lasso using the true variance in the penalization. We stress however that the SFDS
is designed for being applied in—and has theoretical guarantees for—the broader setting of fused
sparsity.

4.2 Robust estimation of the fundamental matrix

To provide a qualitative evaluation of the proposed methodology on real data  we applied the SRDS
to the problem of fundamental matrix estimation in multiple-view geometry  which constitutes an

3available at http://cvlab.epfl.ch/˜strecha/multiview/denseMVS.html

7

(cid:98)σ
(cid:107)(cid:98)ω(cid:107)0
n (cid:107)(cid:98)ω(cid:107)0

100

1
0.13
218
1.3

2
0.13
80
0.46

3
0.13
236
1.37

4
0.17
90
0.52

5
0.16
198
1.13

6
0.17
309
1.84

7
0.20
17
0.12

8
0.18
31
0.19

9
0.17
207
1.49

10 Average
0.15
139.4
0.94

0.11
8
1.02

Table 2: Quantitative results on fountain dataset.

Figure 1: Qualitative results on fountain dataset. Top left: the values of(cid:98)ωi for the ﬁrst pair of images. There

is a clear separation between outliers and inliers. Top right: the ﬁrst pair of images and the matches classiﬁed
as wrong by SRDS. Bottom: the eleven images of the dataset.
essential step in almost all pipelines of 3D reconstruction [13  25]. In short  if we have two images I
and I(cid:48) representing the same 3D scene  then there is a 3×3 matrix F  called fundamental matrix  such
that a point x = (x  y) in I1 matches with the point x(cid:48) = (x(cid:48)  y(cid:48)) in I(cid:48) only if [x; y; 1] F [x(cid:48); y(cid:48); 1](cid:62) =
0. Clearly  F is deﬁned up to a scale factor: if F33 (cid:54)= 0  one can assume that F33 = 1. Thus  each
pair x ↔ x(cid:48) of matching points in images I and I(cid:48) yields a linear constraint on the eight remaining
coefﬁcients of F. Because of the quantiﬁcation and the presence of noise in images  these linear
relations are satisﬁed up to some error. Thus  estimation of F from a family of matching points
{xi ↔ x(cid:48)
i; i = 1  . . .   n} is a problem of linear regression. Typically  matches are computed by
comparing local descriptors (such as SIFT [16]) and  for images of reasonable resolution  hundreds
of matching points are found. The computation of the fundamental matrix would not be a problem in
this context of large sample size / low dimension  if the matching algorithms were perfectly correct.
However  due to noise  repetitive structures and other factors  a non-negligible fraction of detected
matches are wrong (outliers). Elimination of these outliers and robust estimation of F are crucial
steps for performing 3D reconstruction.
Here  we apply the SRDS to the problem of estimation of F for 10 pairs of consecutive images
provided by the fountain dataset [21]: the 11 images are shown at the bottom of Fig. 1. Using SIFT
descriptors  we found more than 17.000 point matches in most pairs of images among the 10 pairs
we are considering. The CPU time for computing each matrix using the SeDuMi solver [22] was
about 7 seconds  despite such a large dimensionality. The number of outliers and the estimated
noise-level for each pair of images are reported in Table 2. We also showed in Fig. 1 the 218 outliers
for the ﬁrst pair of images. They are all indeed wrong correspondncies  even those which correspond
to the windows (this is due to the repetitive structure of the window).

5 Conclusion and perspectives

We have presented a new procedure  SFDS  for the problem of learning linear models with unknown
noise level under the fused sparsity scenario. We showed that this procedure is inspired by the
penalized maximum likelihood but has the advantage of being computable by solving a second-
order cone program. We established tight  nonasymptotic  theoretical guarantees for the SFDS with
a special attention paid to robust estimation in linear models. The experiments we have carried out
are very promising and support our theoretical results.
In the future  we intend to generalize the theoretical study of the performance of the SFDS to the case
of non-Gaussian errors ξi  as well as to investigate its power in variable selection. The extension to
the case where the number of lines in M is larger than the number of columns is another interesting
topic for future research.

8

References
[1] Stephen Becker  Emmanuel Cand`es  and Michael Grant. Templates for convex cone problems with appli-

cations to sparse signal recovery. Math. Program. Comput.  3(3):165–218  2011.

[2] A. Belloni  Victor Chernozhukov  and L. Wang. Square-root lasso: Pivotal recovery of sparse signals via

conic programming. Biometrika  to appear  2012.

[3] Peter J. Bickel  Ya’acov Ritov  and Alexandre B. Tsybakov. Simultaneous analysis of lasso and Dantzig

selector. Ann. Statist.  37(4):1705–1732  2009.

[4] Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much larger

than n. Ann. Statist.  35(6):2313–2351  2007.

[5] Emmanuel J. Cand`es. The restricted isometry property and its implications for compressed sensing. C.

R. Math. Acad. Sci. Paris  346(9-10):589–592  2008.

[6] Emmanuel J. Cand`es and Paige A. Randall. Highly robust error correction by convex programming. IEEE

Trans. Inform. Theory  54(7):2829–2840  2008.

[7] Arnak S. Dalalyan and Renaud Keriven. L1-penalized robust estimation for a class of inverse problems

arising in multiview geometry. In NIPS  pages 441–449  2009.

[8] Arnak S. Dalalyan and Renaud Keriven. Robust estimation for an inverse problem arising in multiview

geometry. J. Math. Imaging Vision.  43(1):10–23  2012.

[9] Eric Gautier and Alexandre Tsybakov. High-dimensional instrumental variables regression and conﬁ-

dence sets. Technical Report arxiv:1105.2454  September 2011.

[10] Christophe Giraud  Sylvie Huet  and Nicolas Verzelen. High-dimensional regression with unknown vari-

ance. submitted  page arXiv:1109.5587v2 [math.ST].

[11] Z. Harchaoui and C. L´evy-Leduc. Multiple change-point estimation with a total variation penalty. J.

Amer. Statist. Assoc.  105(492):1480–1493  2010.

[12] Za¨ıd Harchaoui and C´eline L´evy-Leduc. Catching change-points with lasso. In John Platt  Daphne Koller 

Yoram Singer  and Sam Roweis  editors  NIPS. Curran Associates  Inc.  2007.

[13] R. I. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University

Press  June 2004.

[14] A. Iouditski  F. Kilinc Karzan  A. S. Nemirovski  and B. T. Polyak. On the accuracy of l1-ﬁltering of

signals with block-sparse structure. In NIPS 24  pages 1260–1268. 2011.

[15] S. Lambert-Lacroix and L. Zwald. Robust regression through the Huber’s criterion and adaptive lasso

penalty. Electron. J. Stat.  5:1015–1053  2011.

[16] David G. Lowe. Distinctive image features from scale-invariant keypoints.

Computer Vision  60(2):91–110  2004.

International Journal of

[17] E. Mammen and S. van de Geer. Locally adaptive regression splines. Ann. Statist.  25(1):387–413  1997.
[18] Nam H. Nguyen  Nasser M. Nasrabadi  and Trac D. Tran. Robust lasso with missing and grossly corrupted
observations. In J. Shawe-Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors 
Advances in Neural Information Processing Systems 24  pages 1881–1889. 2011.

[19] A. Rinaldo. Properties and reﬁnements of the fused lasso. Ann. Statist.  37(5B):2922–2952  2009.
[20] Nicolas St¨adler  Peter B¨uhlmann  and Sara van de Geer. (cid:96)1-penalization for mixture regression models.

TEST  19(2):209–256  2010.

[21] C. Strecha  W. von Hansen  L. Van Gool  P. Fua  and U. Thoennessen. On benchmarking camera calibra-
tion and multi-view stereo for high resolution imagery. In Conference on Computer Vision and Pattern
Recognition  pages 1–8  2009.

[22] J. F. Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones. Optim.

Methods Softw.  11/12(1-4):625–653  1999.

[23] T. Sun and C.-H. Zhang. Comments on: (cid:96)1-penalization for mixture regression models. TEST  19(2):

270–275  2010.

[24] T. Sun and C.-H. Zhang. Scaled sparse linear regression. arXiv:1104.4595  2011.
[25] R. Szeliski. Computer Vision: Algorithms and Applications. Texts in Computer Science. Springer  2010.
[26] Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B  58(1):

267–288  1996.

[27] Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and smoothness

via the fused lasso. J. R. Stat. Soc. Ser. B Stat. Methodol.  67(1):91–108  2005.

[28] Sara A. van de Geer and Peter B¨uhlmann. On the conditions used to prove oracle results for the Lasso.

Electron. J. Stat.  3:1360–1392  2009.

9

,Yingzhen Li
Richard Turner