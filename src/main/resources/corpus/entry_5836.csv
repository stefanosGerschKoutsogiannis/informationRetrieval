2019,Variance Reduced Policy Evaluation with Smooth Function Approximation,Policy evaluation with smooth and nonlinear function approximation has shown great potential for reinforcement learning. Compared to linear function approxi- mation  it allows for using a richer class of approximation functions such as the neural networks. Traditional algorithms are based on two timescales stochastic approximation whose convergence rate is often slow. This paper focuses on an offline setting where a trajectory of $m$ state-action pairs are observed. We formulate the policy evaluation problem as a non-convex primal-dual  finite-sum optimization problem  whose primal sub-problem is non-convex and dual sub-problem is strongly concave. We suggest a single-timescale primal-dual gradient algorithm with variance reduction  and show that it converges to an $\epsilon$-stationary point using $O(m/\epsilon)$ calls (in expectation) to a gradient oracle.,Variance Reduced Policy Evaluation with Smooth

Function Approximation

Hoi-To Wai

The Chinese University of Hong Kong

Shatin  Hong Kong

htwai@se.cuhk.edu.hk

Mingyi Hong

University of Minnesota
Minneapolis  MN  USA

mhong@umn.edu

Zhuoran Yang

Princeton University
Princeton  NJ  USA
zy6@princeton.edu

Zhaoran Wang

Northwestern University

Evanston  IL  USA

zhaoranwang@gmail.com

Kexin Tang

University of Minnesota
Minneapolis  MN  USA

tangk@umn.edu

Abstract

Policy evaluation with smooth and nonlinear function approximation has shown
great potential for reinforcement learning. Compared to linear function approxi-
mation  it allows for using a richer class of approximation functions such as the
neural networks. Traditional algorithms are based on two timescales stochastic
approximation whose convergence rate is often slow. This paper focuses on an
ofﬂine setting where a trajectory of m state-action pairs are observed. We formulate
the policy evaluation problem as a non-convex primal-dual  ﬁnite-sum optimiza-
tion problem  whose primal sub-problem is non-convex and dual sub-problem is
strongly concave. We suggest a single-timescale primal-dual gradient algorithm
with variance reduction  and show that it converges to an �-stationary point using
O(m/�) calls (in expectation) to a gradient oracle.

1

Introduction

In reinforcement learning (RL) [39]  policy evaluation aims to estimate the value function that
corresponds to a given policy. It serves as a crucial step in policy optimization algorithms [19  17  34 
35] for solving RL tasks. Perhaps the most popular family of methods is temporal-difference (TD)
[9]  which estimates the value function by minimizing loss functions that are based on the Bellman
equation. These methods can readily incorporate function approximations and have received huge
empirical success  e.g.  when the value functions are parametrized by deep neural networks [26  36].
In contrast to the wide application of policy evaluation with nonlinear function approximation  most
analytical results on policy evaluation focus on the linear setting [41  40  23  14  42  45  3  37  8].
However  when it comes to nonlinear function approximation  TD methods can be divergent [2  43].
To remedy  Bhatnagar et al. [4] proposed an online algorithm for minimizing a generalized mean-
squared projected Bellman error (MSPBE) with smooth and nonlinear value functions. Asymptotic
convergence of this algorithm is established based on two-timescale stochastic approximation [5  18]
with diminishing step size. In a similar vein  Chung et al. [7] established the convergence of TD-
learning with neural networks utilizing different step sizes for the top layer and the lower layers.
However  non-asymptotic convergence results for nonlinear policy evaluation remains an open
problem  illustrating a clear gap between theory and practice.
In this work  we make the ﬁrst attempt to bridge this gap studying policy evaluation with smooth and
nonlinear function approximation. We focus on the ofﬂine setting where we are provided with m

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

consecutive transitions from the policy to be evaluated  which is an important RL regime [20] and is
closely related to the technique of experience replay [21]. Our contributions are two-fold:

• We recast the MSPBE minimization problem as a primal-dual optimization via the Fenchel’s
duality. Here  the objective function is a ﬁnite-sum  and is non-convex in the primal  strongly
concave in the dual — constituting a one-sided non-convex primal-dual optimization problem.

• A variance reduced algorithm [cf. nPD-VR algorithm in Algorithm 1] is developed and applied
to tackle the nonlinear policy evaluation problem. The algorithm performs primal-dual updates
based on a single transition and has low computational complexity per-iteration. Unlike the
existing algorithms  the proposed algorithm uses a ﬁxed set of step sizes which is easier to tune
and requires only a single for-loop to implement. We analyze the non-asymptotic performance
of the algorithm and show that it converges to an �-stationary point of the MSPBE within
O(m/�) calls to a gradient oracle  in expectation.

Note that the optimization problem arisen is strictly more challenging than plain non-convex min-
imization problems that are of recent interest  e.g.  [30  1  15]. For instance  naive gradient-based
updates for this problem might exhibit bizarre behaviors such as cycling [10]. To the best of our
knowledge  the result in this paper constitutes the ﬁrst convergence rate analysis for variance reduced
policy evaluation with smooth and nonlinear function approximation.

Related Work Our work extends the research on policy evaluation with linear function approxi-
mation [41  40  23  38  14  42  8  45  3  44  6  37  13]; see [9] for a comprehensive review. Among
these work  our work is closely related to [14  44  6]  which study single- and multi-agent policy
evaluation in the ofﬂine setting. Besides  they utilize the Fenchel’s duality to obtain primal-dual
optimization problems with a ﬁnite-sum structure  for which they provide variance-reduced opti-
mization algorithms. Thanks to the linear function approximation  their objectives are strongly
convex-concave  which enables linear rate of convergence. Furthermore  [4  7] seem to be the only
convergent policy evaluation results with nonlinear function approximation. Both of their algorithms
utilize two-timescale step sizes  which may yield slow convergence. Moreover  their convergence
results depends on two-timescale stochastic approximation [5  18]  which uses the trajectory of an
ODE to approximate that of a stochastic process. When specialized to an ofﬂine setting similar to
ours  [4] can be viewed as the primal-dual stochastic gradient algorithm for our problem.
From the optimization point of view  the non-convex primal-dual optimization (a.k.a non-convex min-
max problem) that arise in the above non-linear policy evaluation setting is difﬁcult to tackle. Although
recent works have focused on the non-convex minimization problems [30  1  15]  only a few have
focused on the non-convex min-max problems. Recently  Daskalakis and Panageas [10]  Daskalakis
et al. [11] study the convergence of vanilla gradient descent/ascent (GDA)  and the authors focused on
bilinear problems (thus without the non-convex component). An optimistic mirror descent algorithm
is proposed in [25]  and its convergence to a saddle point is established under certain strong coherence
assumption. In [28]  algorithms for robust machine learning problems have been proposed  where the
problem is linear in one side and non-convex in another side. In [29]  a proximally guided stochastic
mirror descent method (PG-SMD) is proposed  which updates the variables simultaneously  while
adopting a double loop update rule in which the variables are updated in “stages". These algorithms

yield the convergence rate in the order of O(1/√K) and O(1/K 1/4)  respectively. Recently  an

oracle based non-convex stochastic gradient descent for generative adversarial networks was proposed
in [31  32]  where the algorithm solves the maximization subproblem up to some small error. It
was shown in [24] that a deterministic gradient descent/ascent min-max algorithm has O(1/K)
convergence rate. In [27]  the O(1/K) convergence rate was proved for nonconvex-nonconcave
min-max problems under Polyak- Łojasiewicz conditions.

Organization In §2 we describe the setup for the policy evaluation problem with smooth (possibly
nonlinear) function approximation.
In §3 we describe the variance reduced method for policy
evaluation and present the results from a preliminary numerical experiment. In §4 we provide the
main convergence result in this paper for the proposed variance reduced method  in which a few key
lemmas and a proof outline will be presented.

2

2 Markov Decision Process and Nonlinear Function Approximation

Consider a Markov Decision Process (MDP) deﬁned by�S A P R  γ�. We have denoted S as the
state space and A as the action space  notice that both S  A can be inﬁnite. Let s ∈ S  a ∈ A be a
state and an action  respectively. For each a ∈ A  the operator P a is a Markov kernel describing the
state transition upon taking action a. For any measurable function f on S  we have

P a(s  s�)f (s�)µ(ds�)  ∀ s ∈ S.

(1)

�P af�(s) =�S

Lastly  the reward function R(s  a) is the reward received after taking action a in state s and γ ∈ (0  1)
is the discount factor.
A policy π is deﬁned through the conditional probability π(a|s) of taking action a given the current
state s. Given a policy π  the expected instantaneous reward at state s is deﬁned as:

(2)
In the policy evaluation problem  we are interested in the value function V : S → R that is deﬁned as
the discounted total reward over an inﬁnite horizon with the initial state ﬁxed at s ∈ S:

Rπ(s) := Ea∼π(·|s)�R(s  a)�  ∀ s ∈ S.

Let M(S) be the manifold of value function given the state space S  we deﬁne the Bellman operator
T π : M(S) → M(S) as:

V (s) := E��∞t=0 γtR(st  at)|s0 = s  at ∼ π(·|st)  st+1 ∼ P at (st ·)�.
�T πf�(s) := E�R(s  a) + γf (s�)|a ∼ π(·|s)  s� ∼ P a(s ·)�  ∀ s ∈ S 

where f is any measurable function deﬁned on S. Denote V (s) (resp. Rπ(s)) as the average reward
for the policy when initialized at a state s ∈ S. The Bellman equation [39] shows that the value
function V : S → R satisﬁes

(3)

(4)

where we have deﬁned the operator P π(· ·) as the expected Markov kernel of the policy π:

V (s) = Rπ(s) + γ�P πV�(s) = T πV (s)  ∀ s ∈ S 
P π(s  s�) :=�A
P a(s  s�)π(a|s�)µ(da)  ∀ s  s� ∈ S × S.

(5)

(6)

In the above sense  the policy evaluation problem refers to solving for V : S → R which satisﬁes (5).
2.1 Nonlinear Function Approximation
Solving for the function V : S → R in (5) is a non-trivial task since the state space S is large (or even
inﬁnite) and the expected Markov kernel P π(· ·) is unknown. To address the ﬁrst issue  a common
approach is to approximate V (s) by a parametric family of functions.
This paper considers approximating V : S → R from the family of parametric and smooth functions
given by F = {Vθ : θ ∈ Θ}  where θ is a d-dimensional parameter vector and Θ is a compact 
convex subset of Rd. Note that F forms a differentiable manifold. For each θ  Vθ is a map from S to
R and the function is non-linear w.r.t. θ. As we consider the family of smooth functions  the gradient
and Hessian of Vθ(s) w.r.t. θ exists and they are denoted as

gθ(s) :=�∇θVθ�(s) ∈ Rd  Hθ(s) :=�∇2

(7)
for each s ∈ S and θ ∈ Θ. We deﬁne Gθ := Es∼pπ(·)[gθ(s)g�θ (s)] ∈ Rd×d  where pπ(·) is the
stationary distribution of the MDP under policy π. Throughout this paper  we assume that Gθ is a
positive deﬁnite matrix for all θ ∈ Θ.
To ﬁnd the best parameter θ� such that Vθ� : S → R is the closest approximation to a value function
V that satisﬁes (5)  Bhatnagar et al. [4] proposed to minimize the mean squared projected bellman
error (MSPBE) deﬁned as follows:

θVθ�(s) ∈ Rd×d 

J(θ) := 1

2��Πθ�T πVθ − Vθ���2

pπ(·) 

3

(8)

min
θ∈Θ

max
w∈Rd

pπ(·) =�S

J(θ) =

where the weighted norm �V �2
pπ(s)|V (s)|2µ(ds) is deﬁned with the stationary distribu-
tion pπ(s) and Πθ is a projection onto the space of nonlinear functions F w.r.t. the metric � · �pπ(·) 
i.e.  for any f : S → R  we have Πθf = arg minVθ∈F �f − Vθ�2
pπ(·). The following identities are
shown in [4]:
1
2
1

Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)�� G−1
2���Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)����
w∈Rd� −

θ Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)�
Es∼pπ(·)�(w�gθ(s))2� +�w  Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)���

where the last equality is due to the Fenchel’s duality. With the above equivalence  the MSPBE
minimization problem can be reformulated as a primal-dual optimization problem:

= max

G−1

(9)

1
2

=

2

θ

L(θ  w)  where

(10)

1
2

L(θ  w) :=�w  Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)�� −

(11)
For convenience  we call θ as the primal variable and w as the dual variable. For any ﬁxed θ ∈ Θ 
the function L(θ  w) is strongly concave in w since Gθ is positive deﬁnite. Moreover  the primal
and dual gradients are given respectively by:

Es∼pπ(·)�(w�gθ(s))2�.

∇θL(θ  w) = Es∼pπ(·)��T πVθ(s) − Vθ(s) − g�θ (s)w�Hθ(s)w�
+ Es∼pπ(·)�(g�θ (s)w)�γEs�∼pπ(·|s)[gθ(s�)] − gθ(s)�� 
∇wL(θ  w) = Es∼pπ(·)�(T πVθ(s) − Vθ(s))gθ(s)� − Es∼pπ(·)�gθ(s)g�θ (s)w�.
∇θ�T πVθ(s) − Vθ(s)� = γE[gθ(s�)|s� ∼ p(·|s  a)  a ∼ p(·|s)] − gθ(s)

(13)
and we have denoted Es�∼pπ(·|s)[gθ(s�)] as the expectation considered in the above. The primal dual
projected gradient algorithm proceeds as

The above follows from the gradient of the temporal difference error:

(12)

θ(k+1) = PΘ�θ(k) − αk+1∇θL(θ(k)  w(k))�
w(k+1) = w(k) + βk+1∇wL(θ(k)  w(k)) 

(14)

where PΘ denotes the Euclidean projection onto the set Θ. Applying the primal dual gradient
algorithm (14) is difﬁcult as evaluating the gradients ∇θL(θ(k)  w(k)) ∇wL(θ(k)  w(k)) requires
computing the expectations in (12) (and may require computing the second order moment of the
quantities). In addition  while the problem (10) is strongly concave in w  it is potentially non-convex
in θ as the function Vθ(·) is non-linear with respect to θ ∈ Θ. It is unknown if the primal dual
gradient algorithm will converge to a stationary (or saddle) point solution  and if it converges  the rate
of convergence is unknown.

3 Variance Reduced Policy Evaluation with Nonlinear Approximation

We tackle the policy evaluation problem with smooth function approximation via focusing on a
sampled average version of problem (10). To ﬁx idea  we observe a trajectory of state-action pairs
{s1  a1  s2  a2  ...  sm  am  sm+1} generated from the policy π that we wish to evaluate and consider
a sample average approximation of the stochastic objective function (11) as:

m�m

L(θ  w) := 1

i=1 Li(θ  w)  where

Li(θ  w) :=�w �R(si  ai) + γVθ(si+1) − Vθ(si)�gθ(si)� −

1
2

(w�gθ(si))2.

(15)

Our goal is to evaluate the stationary point (to be deﬁned later) of the ﬁnite-sum  non-convex  primal-
dual problem:

min
θ∈Θ

max

w∈W L(θ  w) = 1

i=1Li(θ  w).

(16)

m�m

4

Algorithm 1 Nonconvex Primal-Dual Gradient with Variance Reduction (nPD-VR) Algorithm.
1: Input: a trajectory of the state-action pairs {s1  a1  s2  a2  ...  sm  am  sm+1} generated from a
2: Compute the initial averaged gradients as:

given policy; step sizes α  β > 0; initialization points θ0 ∈ Θ  w0 ∈ Rd.

G(0)
θ = 1

m�m
3: for k = 0  1  2  ...  K − 1 do
4:
5:

i=1 ∇θLi(θ(0)  w(0))  G(0)

w = 1

i=1 ∇wLi(θ(0)  w(0))

m�m

Select two indices ik  jk independently and uniformly from {1  ...  m}.
Perform the primal-dual updates:
θ(k+1) = PΘ�θ(k) − β�G(k)
θ +�∇θLik (θ(k)  w(k)) − ∇θLik (θ(k)
w(k+1) = w(k) + α�G(k)
w +�∇wLik (θ(k)  w(k)) − ∇wLik (θ(k)
=�θ(k)

where the gradients can be given by (17).
Update the variables as:

=�w(k)

  w(k+1)

θ(k+1)
i

ik

ik

i

  w(k)
ik

)���
)�� 

  w(k)
ik

6:

G(k+1)

θ

= G(k)

θ +

G(k+1)

w

= G(k)

w +

θ(k)
i
1

w(k)

if i = jk
if i �= jk
m�∇θLjk (θ(k)  w(k)) − ∇θLjk (θ(k)
m�∇wLjk (θ(k)  w(k)) − ∇wLjk (θ(k)

if i = jk
if i �= jk
)� 
  w(k)
jk
)� 

  w(k)
jk

jk

jk

1

i

7: end for
8: Return: (θ( ˜K)  w( ˜K))  where ˜K is independently and uniformly picked from {1  ...  K} — an

approximate stationary point to (16).

(18)

(19)

(20)

(21)

Observe that if m is sufﬁciently large and as Gθ is positive deﬁnite  the primal-dual objective function
is strongly concave in w but is possibly non-convex in θ due to non-linearity. The above problem is
hence a one-sided non-convex problem which remains challenging to tackle.
An exact primal-dual gradient (PDG) algorithm following (14) but replacing the gradients of L(θ  w)
by that of L(θ  w) may be applied to (15). In fact  through exploiting the one-sided non-convexity 
Lu et al. [24] showed that a similar algorithm to the PDG algorithm indeed converges sublinearly to
a stationary point of (16). However  for large m � 1  implementing the PDG algorithm involves a
high per-iteration complexity since evaluating the full gradient requires Ω(m) FLOPS. Our idea is
to derive a fast stochastic algorithm for function approximation through borrowing techniques from
variance reduction methods [16  12  33  30].
To ﬁx notations  let i ∈ {1  ...  m} and we deﬁne the primal-dual gradient of the ith samples:
� ∇θLi(θ  w)
∇wLi(θ  w) � =� �δi(θ) − g�θ (si)w�Hθ(si)w + (g�θ (si)w)�γgθ(si+1) − gθ(si)�
where δi(θ) := R(si  ai) + γVθ(si+1) − Vθ(si) is the ith sampled temporal difference.
We propose the Nonconvex Primal-Dual Gradient with Variance Reduction (nPD-VR) algorithm
for (16) in Algorithm 1. The algorithm is a natural extension of the non-convex SAGA algorithm
introduced by [30] to the primal-dual  ﬁnite-sum setting of interest. In speciﬁc  line 5 performs the
primal dual gradient update through an unbiased estimate of the gradient — by denoting

δi(θ)gθ(si) −�gθ(si)�w�gθ(si)

�

(17)

�G(k)
�G(k)

:= G(k)
θ
w := G(k)

θ +�∇θLik (θ(k)  w(k)) − ∇θLik (θ(k)
w +�∇wLik (θ(k)  w(k)) − ∇wLik (θ(k)
as ik is uniformly picked from {1  ...  m}  therefore (when conditioned on the past random variable
generated up to iteration k) the expected values of the quantities �G(k)
w are the primal-dual

)� 
)� 
θ  �G(k)

  w(k)
ik
  w(k)
ik

(22)

ik

ik

5

gradients ∇θL(θ(k)  w(k))  ∇wL(θ(k)  w(k))  respectively. Meanwhile  the updates in line 6 keep
refreshing the stored variables in the memory. We remark that these updates are based on the index
jk which is independent from the ik used in line 5. As we shall see in the analysis  this subtle detail
in the algorithm allows for proving that the variance in gradient is reduced [cf. Lemma 3].
As the nPD-VR algorithm employs an incremental update rule similar to the SAGA method  this
algorithm is suitable for the big-data setting when m � 1. Particularly  the cost for the updates in
line 5 and line 6 are independent of m. Moreover  the proposed algorithm utilizes a ﬁxed step size
rule which allows for adaptation to more dynamical data.
We remark that existing approach [4  7] have studied a two-timescale stochastic approximation
algorithm for tackling the stochastic problem (10); and in a similar vein  a recent related work [22]
proposed a double loop algorithm that requires solving the dual problem (nearly) optimally. In
contrast  the nPD-VR algorithm runs on a single-timescale. The nPD-VR algorithm is more ﬂexible
and numerically stable  as we shall show in the convergence analysis.

3.1 Preliminary Numerical Experiments

�

�

�

�

�

�

�

�

�
�

�

�

�

�
�

��

��

��

��

�

�

�

����

����

������
���

We present preliminary experiments
of learning the value function from
the MountainCar dataset with m =
5000 via the nPD-VR algorithm. We
ran Sarsa [39] to obtain a good pol-
icy  then we generate a trajectory of
the state-action pairs. To learn the
value function  we parameterize Vθ(·)
as a 2-layer neural network with n
hidden neurons and consider a for-
getting factor γ = 0.95. We set the
constraints in (16) with Θ = [0  1]n 
and in addition we consider w to be
bounded in [0  100]n for better numer-
ical stability  which can be enforced
by incorporating a projection step after (19). For the nPD-VR algorithm  we set the step sizes as
α = 10−4  β = 10−8. Note we have approximated the Hessian in gradient computation (17) with
diagonal approximation. For benchmark  we also experiment with a single-timescale SGD on (16)
with a diminishing step size. Trajectory of the objective L(θ(k)  w(k)) is shown in Fig. 1. As seen  the
objective of nPD-VR converges to (close to) zero in 4-5 passes on the data  while a single timescale
SGD on (16) takes a long time (or fail) to converge.

Figure 1: Trajectory of the nPD-VR on the MountainCar
dataset such that the value function is approximated as a 2-
layer neural network with n neurons. (Left) n = 50 neurons
(Right) n = 100 neurons.

������
���
���

���
������

���
������

���

���

���

��

�

4 Convergence Analysis

Before stating the main results  let us list a few assumptions on the nPD-VR algorithm and the
primal-dual problem (16).
Assumption 1. For any θ ∈ Θ  the sum function L(θ  w) is µ-strongly concave in w.
In the case of policy evaluation problem  Assumption 1 can be implied by taking a sufﬁcient number
of samples m and exploiting the fact that Gθ in (7) is positive deﬁnite.
Assumption 2. The iterates {θ(k)  w(k)}k≥0 generated by the nPD-VR algorithm stay within a
compact set Θ × W  for some W ⊆ Rd which is compact and convex.
Due to the Euclidean projection in the primal-update of θ  the condition θ(k) ∈ Θ holds straightfor-
wardly. Meanwhile it maybe difﬁcult to verify w(k) ∈ W as the update is unconstrained in general.
An intuition is that as L(θ  w) is strongly concave in w  for each ¯θ ∈ Θ  the maximizer to L( ¯θ  w)
is unique  i.e.  denoted as w�( ¯θ). Also due to the strong concavity  at each iteration k and with
a sufﬁciently small step size  the dual update of wk pulls the dual variable towards w�(θ(k)) and
therefore wk also stays within a compact set. Nevertheless  in our numerical experiments in Sec. 3.1 
we ﬁnd that incorporating an additional projection step to the dual update improves the numerical
performance. Lastly  we assume that:

6

Assumption 3. For each i ∈ {1  ...  m}  the gradient ∇θLi(θ  w) (resp. ∇wLi(θ  w)) is Lθ
(resp. Lw) Lipschitz. We have:

�∇θLi(θ  w) − ∇θLi(θ�  w�)� ≤ Lθ��θ − θ�� + �w − w��� 
�∇wLi(θ  w) − ∇wLi(θ�  w�)� ≤ Lw��θ − θ�� + �w − w��� 

for any θ  θ� ∈ Θ and any w  w� ∈ W  where W is deﬁned in Assumption 2.
Assumption 3 is mild and it can be veriﬁed by using the compactness of W and checking (17).
In particular  the assumption holds when the parametric family of functions has bounded  smooth
gradient and Hessian.

(23)

Summary of Main Results The primal-dual optimization (16) is a one-sided constrained problem 
i.e.  only θ is constrained to Θ while w is unconstrained. We quantify its convergence via the

following stationarity measure. Deﬁne θ = PΘ�θ− β∇θL(θ  w)� for any θ  w ∈ Θ×Rd. Observe
that if �θ − θ� = 0 and ∇wL(θ  w) = 0  then (θ  w) is a (ﬁrst order) stationary point. Inspired by
such observation  the following stationarity measure emerges as a natural metric:

G(θ(k)  w(k)) :=

(k)

1
β2�θ

− θ(k)�2 + �∇wL(θ(k)  w(k))�2 

(24)

where θ

(k) is deﬁned through (θ(k)  w(k)) as

θ

(k)

:= PΘ�θ(k) − β∇θL(θ(k)  w(k))�.
(25)
Observe that if G(θ(k)  w(k)) = 0  then the primal-dual solution (θ(k)  w(k)) is a stationary point.
Furthermore  the metric is roughly invariant with the step size since �θ
− θ(k)�2 = O(β2). The
following theorem shows the convergence of the nPD-VR algorithm:
Theorem 1. Assume Assumption 1–3 hold true. There exist step size parameters – of the order
β = Θ(1/m)  α = Θ(1/m) – such that it holds for any K ∈ N that

(k)

E�G(θ( ˜K)  w( ˜K))� ≤

F (K) + 4

µ�3 + 2m�2L2

wα + L2
K min{α  β
4}

θβ���∇wL(θ(0)  w(0))�2

 

(26)

where F (K) := E[L(θ(0)  w(0)) − L(θ(K)  w(K))] and we recall that ˜K is a uniform random
variable drawn from {1  ...  K}.
The above shows that the stationarity measure decays to zero at a sublinear rate. In particular  with the
step size order α = Θ(1/m)  β = Θ(1/m)  the number of iterations required to reach an �-stationary
point [with G(θ  w) = O(�)] is O(m/�)  provided that the strong concavity constant µ  Lipschitz
constants of the functions Lθ  Lw are independent of m.

Comparison to Prior Work Note that non-asymptotic convergence of primal-dual gradient type
algorithms to stationary points with (one sided) non-convex problems has only been recently re-
searched. Of close relationship is [24] which study a block coordinate descent version of single loop
primal-dual gradient method – the primal and dual updates are performed in sequence and complete
gradients are evaluated at each iteration – Lu et al. [24] showed that their algorithm converges to an
�-stationary point using O(1/�) iterations  under a similar set of assumptions as ours. Since each
iteration of [24] requires a complete gradient evaluation  the number of calls to a gradient oracle is
thus O(m/�). In [22  29]  several proximally guided stochastic mirror descent methods (PG-SMD)
are proposed for primal-dual problems following a closely related set of assumptions. However 
the PG-SMD methods in [22  29] rely on a double-loop update in which the primal variables are
updated in a faster pace than the dual variables. Nevertheless  [22  29] show that these methods
converges to an �-stationary point using O(m/�) gradient oracle calls. To the best of our knowledge 
our algorithm is the ﬁrst stochastic algorithm that can deal with the ﬁnite-sum primal-dual problem
such as (16)  using a single-loop  and variance reduced techniques. Furthermore  the convergence
rate of the proposed nPD-VR algorithm is on-par with the state-of-the-art methods.

7

4.1 Proof Outline

Our analysis follows from combining and improving recent techniques for analyzing non-convex
optimization algorithms in [30  24]. To facilitate our analysis  we denote the errors in gradient by
e(k)
w −∇wL(θ(k)  w(k))  respectively. Detailed proofs
θ
of results in this section can be found in the supplementary materials.

θ −∇θL(θ(k)  w(k)) and e(k)

w :=�G(k)

:=�G(k)

Key Lemmas We begin by establishing a few lemmas for the convergence analysis. In speciﬁc  the
ﬁrst step is to control the change in objective function value with the primal update:
Lemma 1. Under Assumption 3. For any k ∈ N  we have
L(θ(k+1)  w(k)) − L(θ(k)  w(k)) ≤�Lθ −

(27)

(k)

1

2β��θ
2β��θ(k+1) − θ(k)�2 +

1

− θ(k)�2
β
2 �e(k)
θ �2

+� Lθ

2 −

The proof follows by the standard descent property of smooth functions combined with the variance
controlling technique introduced by [30].
Secondly  the progress made by the dual update obeys the following bounds:
Lemma 2. Under Assumption 1-3. For any k ∈ N  the change in objective value is bounded as:

(28)

(29)

L(θ(k+1)  w(k+1)) − L(θ(k+1)  w(k)) ≤ αL2

�2α +

µα2

α3L2
w

2 −

and the dual gradient is controlled by:

�∇wL(θ(k)  w(k))�2 ≤�1 + α2L2

w�θ(k+1) − θ(k)�2
2 ��∇wL(θ(k)  w(k))�2 +�α −
µα2
y − 2µα��∇wL(θ(k−1)  w(k−1))�2

2 ��e(k)
w �2 

+ µα�∇wL(θ(k)  w(k))�2 +

L2
w

µα��θ(k) − θ(k−1)�2 + α2�e(k−1)

w

�2�

The bound (28) is a standard relation for dual gradient update  while (29) is a consequence of the
strong-concavity of L(θ(k)  w(k)) – it shows that �∇wL(θ(k)  w(k))� contracts after a dual update.
To control the gradient error terms in expectation �e(k)
w �2  we consider [also see Lemma 4 in
the supplementary materials]
m�i=1��θ(k) − θ(k)

i �2�
i �2 + �w(k) − w(k)

and notice that Δ(0) = 0. Using Assumption 3 and when the step size is sufﬁciently small  we can

θ �2  �e(k)

Δ(k) :=

1
m

(30)

establish a bound on�K

k=0 E[Δ(k)] via the below lemma:

Lemma 3. Under Assumption 3 and the condition on the step sizes that:
1
m

w(α2 + α(1 −

δ(α  β) :=

)) > 0.

For any K ≥ 1  we have
E�Δ(k)� ≤

K�k=0

1

δ(α  β)

1
m − max{α  β} − 4L2
K�k=0

β�θ(k+1) − θ(k)�2 + 4α�∇wL(θ(k)  w(k))�2�.
E� 2

The proof of the lemma makes use of the property of the nPD-VR algorithm and uses a new technique
in proving the contraction of variance in SAGA-type algorithms. Furthermore  note that if the step
sizes satisﬁes

then one has ( 1

m − max{α  β} − 4L2

K�k=0

E�Δ(k)� ≤

K�k=0

1
2m ≥ max{α  β} + 8αL2
w 
w(α2 + α(1 − 1
m )))−1 ≤ m
β �θ(k+1) − θ(k)�2 + 2mα�∇wL(θ(k)  w(k))�2�.
E� m

2 . We simplify (32) into

(31)

(32)

(a0)

(33)

8

Proof of Theorem 1 Equipped with the lemmas above on the progress made by primal-dual updates
and the SAGA gradient estimation  our proof follows by analyzing (27)  (28)  (29). We remark that
the proof technique used is new  which departs from the common Lyapunov/potential function
approach pursued in recent papers [30  24] on non-convex analysis.
To illustrate the idea  through carefully controlling the step size  we show that by summing up the
inequalities (27)  (28) from k = 0 to k = K − 1  we get
β )�K−1

Ω� min{α  β}��K−1
k=0 E[G(θ(k)  w(k))] ≤ O(α)�K−1
Using (29)  the sum �K−1
k=0 E[�∇wL(θ(k)  w(k))�2] can be further upper bounded as the form
constant ×�K−1
k=0 E[�θ(k+1) − θ(k)�2] + constant. Substituting the newly obtained bound  one
can ﬁnd a step size β > 0 such that the constant in front of the term E[�θ(k+1) − θ(k)�2] is negative.
It follows that we can upper bound the right hand side of (34) with a constant independent of K.
Subsequently  we observe that as ˜K is an independent r.v. uniformly distributed on {0  ...  K − 1} 
one has E�G(θ( ˜K)  w( ˜K))� = K−1�K−1
k=0 E[G(θ(k)  w(k))] and applying (34) yields Theorem 1.

k=0 E[�∇wL(θ(k)  w(k))�2]
k=0 E[�θ(k+1) − θ(k)�2] + constant.

5 Conclusions and Extensions

+ O(m − 1

(34)

In this paper  we have studied the policy evaluation problem in the case of smooth (possibly non-linear)
function approximation. We consider an ofﬂine setting via sample average approximation of the
Bellman equation. Albeit the sample size m can be large  we propose a simple and efﬁcient  variance
reduced primal dual update strategy to handle the one-sided non-convex optimization problem arisen.
We analyze the non-asymptotic convergence rate of the algorithm towards a stationary point and
demonstrate that it performs on par with state-of-the-art optimization methods  while the latter
requires higher implementation complexity.
Several extensions are worth studying — similar to the SAGA algorithm considered here  the SVRG
algorithm [16] may beneﬁt the nonconvex primal-dual optimization; as suggested by [30]  using
mini-batch can accelerate the convergence rate from O(m/K) to O(m
Acknowledgement

3 /K).

2

H.-T. Wai is supported by the CUHK Direct Grant #4055113. M. Hong is supported in part by NSF
under Grant CCF-1651825  CMMI-172775  CIF-1910385 and by AFOSR under grant 19RT0424.

9

References
[1] Z. Allen-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. In Interna-

tional conference on machine learning  pages 699–707  2016.

[2] L. Baird. Residual algorithms: Reinforcement learning with function approximation.

International Conference on Machine Learning  pages 30–37  1995.

In

[3] J. Bhandari  D. Russo  and R. Singal. A ﬁnite time analysis of temporal difference learning with

linear function approximation. arXiv preprint arXiv:1806.02450  2018.

[4] S. Bhatnagar  D. Precup  D. Silver  R. S. Sutton  H. R. Maei  and C. Szepesvári. Convergent
temporal-difference learning with arbitrary smooth function approximation. In Advances in
Neural Information Processing Systems  pages 1204–1212  2009.

[5] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint  volume 48. Springer 

2009.

[6] L. Cassano  K. Yuan  and A. H. Sayed. Multi-agent fully decentralized off-policy learning with

linear convergence rates. arXiv preprint arXiv:1810.07792  2018.

[7] W. Chung  S. Nath  A. Joseph  and M. White. Two-timescale networks for nonlinear value

function approximation. In ICLR  2019.

[8] G. Dalal  B. Szorenyi  G. Thoppe  and S. Mannor.

Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning. arXiv preprint
arXiv:1703.05376  2017.

[9] C. Dann  G. Neumann  and J. Peters. Policy evaluation with temporal differences: A survey and

comparison. Journal of Machine Learning Research  15(1):809–883  2014.

[10] C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max
optimization. In Advances in Neural Information Processing Systems  pages 9256–9266  2018.

[11] C. Daskalakis  A. Ilyas  V. Syrgkanis  and H. Zeng. Training GANs with optimism. arXiv

preprint arXiv:1711.00141  2017.

[12] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with
support for non-strongly convex composite objectives. In Advances in neural information
processing systems  pages 1646–1654  2014.

[13] T. T. Doan  S. T. Maguluri  and J. Romberg. Convergence rates of distributed td (0)
with linear function approximation for multi-agent reinforcement learning. arXiv preprint
arXiv:1902.07393  2019.

[14] S. S. Du  J. Chen  L. Li  L. Xiao  and D. Zhou. Stochastic variance reduction methods for policy

evaluation. In International Conference on Machine Learning  pages 1049–1058  2017.

[15] S. Ghadimi and G. Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[16] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[17] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information

Processing Systems  pages 1008–1014  2000.

[18] H. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applica-

tions. Springer Science & Business Media  2003.

[19] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning

research  4(Dec):1107–1149  2003.

[20] S. Lange  T. Gabel  and M. Riedmiller. Batch reinforcement learning. In Reinforcement learning 

pages 45–73. Springer  2012.

10

[21] L.-J. Lin. Self-improving reactive agents based on reinforcement learning  planning and teaching.

Machine learning  8(3-4):293–321  1992.

[22] Q. Lin  M. Liu  H. Raﬁque  and T. Yang. Solving weakly-convex-weakly-concave saddle-point
problems as weakly-monotone variational inequality. arXiv preprint arXiv:1810.10207  2018.

[23] B. Liu  J. Liu  M. Ghavamzadeh  S. Mahadevan  and M. Petrik. Finite-sample analysis of
proximal gradient TD algorithms. In Conference on Uncertainty in Artiﬁcial Intelligence  pages
504–513  2015.

[24] S. Lu  I. Tsaknakis  Y. Chen  and M. Hong. Hybrid block successive approximation for one-sided
non-convex min-max problems: Algorithms and applications. arXiv preprint arXiv:1902.08294 
2019.

[25] P. Mertikopoulos  H. Zenati  B. Lecouat  C. Foo  V. Chandrasekhar  and G. Piliouras. Mirror
descent in saddle-point problems: Going the extra (gradient) mile. CoRR  abs/1807.02629 
2018. URL http://arxiv.org/abs/1807.02629.

[26] V. Mnih  A. P. Badia  M. Mirza  A. Graves  T. Lillicrap  T. Harley  D. Silver  and K. Kavukcuoglu.
In International Conference on

Asynchronous methods for deep reinforcement learning.
Machine Learning  pages 1928–1937  2016.

[27] M. Nouiehed  M. Sanjabi  J. D. Lee  and M. Razaviyayn. Solving a class of non-convex

min-max games using iterative ﬁrst order methods. arXiv preprint arXiv:1902.08297  2019.

[28] Q. Qian  S. Zhu  J. Tang  R. Jin  B. Sun  and H. Li. Robust optimization over multiple domains.

CoRR  abs/1805.07588  2018. URL http://arxiv.org/abs/1805.07588.

[29] H. Raﬁque  M. Liu  Q. Lin  and T. Yang. Non-convex min-max optimization: Provable

algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060  2018.

[30] S. J. Reddi  S. Sra  B. Poczos  and A. J. Smola. Proximal stochastic methods for nonsmooth
nonconvex ﬁnite-sum optimization. In Advances in Neural Information Processing Systems 
pages 1145–1153  2016.

[31] M. Sanjabi  B. Jimmy  M. Razaviyayn  and J. D. Lee. On the convergence and robustness
of training GANs with regularized optimal transport. In Proceedings of Advances in Neural
Information Processing Systems  pages 7088–7098  2018.

[32] M. Sanjabi  M. Razaviyayn  and J. D. Lee. Solving non-convex non-concave min-max games

under polyak-lojasiewicz condition. arXiv preprint arXiv:1812.02878  2018.

[33] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112  2017.

[34] J. Schulman  S. Levine  P. Abbeel  M. Jordan  and P. Moritz. Trust region policy optimization.

In International Conference on Machine Learning  pages 1889–1897  2015.

[35] J. Schulman  F. Wolski  P. Dhariwal  A. Radford  and O. Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347  2017.

[36] D. Silver  J. Schrittwieser  K. Simonyan  I. Antonoglou  A. Huang  A. Guez  T. Hubert  L. Baker 
M. Lai  A. Bolton  et al. Mastering the game of go without human knowledge. Nature  550
(7676):354  2017.

[37] R. Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD

learning. arXiv preprint arXiv:1902.00923  2019.

[38] M. S. Stankovi´c and S. S. Stankovi´c. Multi-agent temporal-difference learning with linear
function approximation: Weak convergence under time-varying network topologies. In 2016
American Control Conference (ACC)  pages 167–172. IEEE  2016.

[39] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press  2018.

11

[40] R. S. Sutton  H. R. Maei  D. Precup  S. Bhatnagar  D. Silver  C. Szepesvári  and E. Wiewiora.
Fast gradient-descent methods for temporal-difference learning with linear function approxima-
tion. In International Conference on Machine Learning  pages 993–1000  2009.

[41] R. S. Sutton  H. R. Maei  and C. Szepesvári. A convergent o(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In Advances in Neural Information
Processing Systems  pages 1609–1616  2009.

[42] A. Touati  P.-L. Bacon  D. Precup  and P. Vincent. Convergent tree-backup and retrace with

function approximation. arXiv preprint arXiv:1705.09322  2017.

[43] J. N. Tsitsiklis and B. Van Roy. Analysis of temporal-diffference learning with function
approximation. In Advances in Neural Information Processing Systems  pages 1075–1081 
1997.

[44] H.-T. Wai  Z. Yang  P. Z. Wang  and M. Hong. Multi-agent reinforcement learning via double
averaging primal-dual optimization. In Advances in Neural Information Processing Systems 
pages 9649–9660  2018.

[45] Y. Wang  W. Chen  Y. Liu  Z.-M. Ma  and T.-Y. Liu. Finite sample analysis of the GTD policy
evaluation algorithms in Markov setting. In Advances in Neural Information Processing Systems 
pages 5504–5513  2017.

12

,Hoi-To Wai
Mingyi Hong
Zhuoran Yang
Zhaoran Wang
Kexin Tang