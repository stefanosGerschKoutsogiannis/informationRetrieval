2019,Universal Invariant and Equivariant Graph Neural Networks,Graph Neural Networks (GNN) come in many flavors  but should always be either invariant (permutation of the nodes of the input graph does not affect the output) or \emph{equivariant} (permutation of the input permutes the output). In this paper  we consider a specific class of invariant and equivariant networks  for which we prove new universality theorems. More precisely  we consider networks with a single hidden layer  obtained by summing channels formed by applying an equivariant linear operator  a pointwise non-linearity  and either an invariant or equivariant linear output layer. Recently  Maron et al. (2019) showed that by allowing higher-order tensorization inside the network  universal invariant GNNs can be obtained. As a first contribution  we propose an alternative proof of this result  which relies on the Stone-Weierstrass theorem for algebra of real-valued functions. Our main contribution is then an extension of this result to the \emph{equivariant} case  which appears in many practical applications but has been less studied from a theoretical point of view. The proof relies on a new generalized Stone-Weierstrass theorem for algebra of equivariant functions  which is of independent interest. Additionally  unlike many previous works that consider a fixed number of nodes  our results show that a GNN defined by a single set of parameters can approximate uniformly well a function defined on graphs of varying size.,Universal Invariant and Equivariant

Graph Neural Networks

Nicolas Keriven

Gabriel Peyré

École Normale Supérieure

Paris  France

nicolas.keriven@ens.fr

CNRS and École Normale Supérieure

Paris  France

gabriel.peyre@ens.fr

Abstract

Graph Neural Networks (GNN) come in many ﬂavors  but should always be either
invariant (permutation of the nodes of the input graph does not affect the output)
or equivariant (permutation of the input permutes the output). In this paper  we
consider a speciﬁc class of invariant and equivariant networks  for which we prove
new universality theorems. More precisely  we consider networks with a single
hidden layer  obtained by summing channels formed by applying an equivariant
linear operator  a pointwise non-linearity  and either an invariant or equivariant
linear output layer. Recently  Maron et al. (2019b) showed that by allowing higher-
order tensorization inside the network  universal invariant GNNs can be obtained.
As a ﬁrst contribution  we propose an alternative proof of this result  which relies
on the Stone-Weierstrass theorem for algebra of real-valued functions. Our main
contribution is then an extension of this result to the equivariant case  which
appears in many practical applications but has been less studied from a theoretical
point of view. The proof relies on a new generalized Stone-Weierstrass theorem
for algebra of equivariant functions  which is of independent interest. Additionally 
unlike many previous works that consider a ﬁxed number of nodes  our results
show that a GNN deﬁned by a single set of parameters can approximate uniformly
well a function deﬁned on graphs of varying size.

1

Introduction

Designing Neural Networks (NN) to exhibit some invariance or equivariance to group operations is
a central problem in machine learning (Shawe-Taylor  1993). Among these  Graph Neural Networks
(GNN) are primary examples that have gathered a lot of attention for a large range of applications.
Indeed  since a graph is not changed by permutation of its nodes  GNNs must be either invariant
to permutation  if they return a result that must not depend on the representation of the input  or
equivariant to permutation  if the output must be permuted when the input is permuted  for instance
when the network returns a signal over the nodes of the input graph. In this paper  we examine
universal approximation theorems for invariant and equivariant GNNs.
From a theoretical point of view  invariant GNNs have been much more studied than their equivariant
counterpart (see the following subsection). However  many practical applications deal with equivari-
ance instead  such as community detection (Chen et al.  2019)  recommender systems (Ying et al. 
2018)  interaction networks of physical systems (Battaglia et al.  2016)  state prediction (Sanchez-
Gonzalez et al.  2018)  protein interface prediction (Fout et al.  2017)  among many others. See (Zhou
et al.  2018; Bronstein et al.  2017) for thorough reviews. It is therefore of great interest to increase
our understanding of equivariant networks  in particular  by extending arguably one of the most
classical result on neural networks  namely the universal approximation theorem for multi-layers
perceptron (MLP) with a single hidden layer (Cybenko  1989; Hornik et al.  1989; Pinkus  1999).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Maron et al. (2019b) recently proved that certain invariant GNNs were universal approximators of
invariant continuous functions on graphs. The main goal of this paper is to extend this result to the
equivariant case  for similar architectures.

Outline and contribution. The outline of our paper is as follows. After reviewing previous works
and notations in the rest of the introduction  in Section 2 we provide an alternative proof of the
result of (Maron et al.  2019b) for invariant GNNs (Theorem 1)  which will serve as a basis for the
equivariant case. It relies on a non-trivial application of the classical Stone-Weierstrass theorem for
algebras of real-valued functions (recalled in Theorem 2). Then  as our main contribution  in Section
3 we prove this result for the equivariant case (Theorem 3)  which to the best of our knowledge was
not known before. The proof relies on a new version of Stone-Weierstrass theorem (Theorem 4).
Unlike many works that consider a ﬁxed number of nodes n  in both cases we will prove that a GNN
described by a single set of parameters can approximate uniformly well a function that acts on graphs
of varying size.

1.1 Previous works

The design of neural network architectures which are equivariant or invariant under group actions is
an active area of research  see for instance (Ravanbakhsh et al.  2017; Gens and Domingos  2014;
Cohen and Welling  2016) for ﬁnite groups and (Wood and Shawe-Taylor  1996; Kondor and Trivedi 
2018) for inﬁnite groups. We focus here our attention to discrete groups acting on the coordinates of
the features  and more speciﬁcally to the action of the full set of permutations on tensors (order-1
tensors corresponding to sets  order-2 to graphs  order-3 to triangulations  etc).

Convolutional GNN. The most appealing construction of GNN architectures is through the use of
local operators acting on vectors indexed by the vertices. Early deﬁnitions of these “message passing”
architectures rely on ﬁxed point iterations (Scarselli et al.  2009)  while more recent constructions
make use of non-linear functions of the adjacency matrix  for instance using spectral decomposi-
tions (Bruna et al.  2014) or polynomials (Defferrard et al.  2016). We refer to (Bronstein et al. 
2017; Xu et al.  2019) for recent reviews. For regular-grid graphs  they match classical convolutional
networks (LeCun et al.  1989) which by design can only approximate translation-invariant or equiv-
ariant functions (Yarotsky  2018). It thus comes at no surprise that these convolutional GNN are not
universal approximators (Xu et al.  2019) of permutation-invariant functions.

Fully-invariant GNN. Designing Graph (and their higher-dimensional generalizations) NN which
are equivariant or invariant to the whole permutation group (as opposed to e.g. only translations)
requires the use of a small sub-space of linear operators  which is identiﬁed in (Maron et al.  2019a).
This generalizes several previous constructions  for instance for sets (Zaheer et al.  2017; Hartford
et al.  2018) and points clouds (Qi et al.  2017). Universality results are known to hold in the special
cases of sets  point clouds (Qi et al.  2017) and discrete measures (de Bie et al.  2019) networks.
In the invariant GNN case  the universality of architectures built using a single hidden layer of
such equivariant operators followed by an invariant layer is proved in (Maron et al.  2019b) (see
also (Kondor et al.  2018)). This is the closest work from our  and we will provide an alternative
proof of this result in Section 2  as a basis for our main result in Section 3.
Universality in the equivariant case has been less studied. Most of the literature focuses on equivari-
ance to translation and its relation to convolutions (Kondor et al.  2018; Cohen and Welling  2016) 
which are ubiquitous in image processing. In this context  Yarotsky (2018) proved the universality
of some translation-equivariant networks. Closer to our work  universality of NNs equivariant to
permutations acting on point clouds has been recently proven in (Sannai et al.  2019)  however their
theorem does not allow for high-order inputs like graphs. It is the purpose of our paper to ﬁll this
missing piece and prove the universality of a class of equivariant GNNs for high-order inputs such as
(hyper-)graphs.

1.2 Notations and deﬁnitions

In this paper  (hyper-)graphs with n nodes are represented by tensors G ∈ Rnd indexed
Graphs.
by 1 (cid:54) i1  . . .   id (cid:54) n. For instance  “classical” graphs are represented by edge weight matrices
(d = 2)  and hyper-graphs by high-order tensors of “multi-edges” connecting more than two nodes.

2

Note that we do not impose G to be symmetric  or to contain only non-negative elements. In the rest
of the paper  we ﬁx some d (cid:62) 1 for the order of the inputs  however we allow n to vary.
Permutations. Let [n] def.= {1  . . .   n}. The set of permutations σ : [n] → [n] (bijections from [n]
to itself) is denoted by On  or simply O when there is no ambiguity. Given a permutation σ and an
order-k tensor G ∈ Rnk  a “permutation of nodes” on G is denoted by σ (cid:63) G and deﬁned as

(σ (cid:63) G)σ(i1) ... σ(ik) = Gi1 ... ik .

We denote by Pσ ∈ {0  1}n×n the permutation matrix corresponding to σ  or simply P when there is
no ambiguity. For instance  for G ∈ Rn2 we have σ (cid:63) G = P GP (cid:62).
Two graphs G1  G2 are said isomorphic if there is a permutation σ such that G1 = σ(cid:63)G2. If G = σ(cid:63)G 
we say that σ is a self-isomorphism of G. Finally  we denote by O(G) def.= {σ (cid:63) G ; σ ∈ O} the orbit
of all the permuted versions of G.
Invariant and equivariant linear operators. A function f : Rnk → R is said to be invariant if
f (σ (cid:63) G) = f (G) for every permutation σ. A function f : Rnk → Rn(cid:96) is said to be equivariant if
f (σ (cid:63)G) = σ (cid:63)f (G). Our construction of GNNs alternates between linear operators that are invariant
or equivariant to permutations  and non-linearities. Maron et al. (2019a) elegantly characterize all
such linear functions  and prove that they live in vector spaces of dimension  respectively  exactly
b(k) and b(k + (cid:96))  where b(i) is the ith Bell number. An important corollary of this result is that the
dimension of this space does not depend on the number of nodes n  but only on the order of the input
and output tensors. Therefore one can parameterize linearly for all n such an operator by the same set
of coefﬁcients. For instance  a linear equivariant operator F : Rn2 → Rn2 from matrices to matrices
is formed by a linear combination of b(4) = 15 basic operators such as “sum of rows replicated on
the diagonal”  “sum of columns replicated on the rows”  and so on. The 15 coefﬁcients used in this
linear combination deﬁne the “same” linear operator for every n.

Invariant and equivariant Graph Neural Nets. As noted by Yarotsky (2018)  it is in fact trivial
to build invariant universal networks for ﬁnite groups of symmetry: just take a non-invariant universal
architecture  and perform a group averaging. However  this holds little interest in practice  since
the group of permutation is of size n!. Instead  researchers use architectures for which invariance is
hard-coded into the construction of the network itself. The same remark holds for equivariance.
In this paper  we consider one-layer GNNs of the form:

S(cid:88)

(cid:104)

(cid:105)

f (G) =

Hs

ρ(Fs[G] + Bs)

+ b 

(1)

s=1

described in the previous paragraph  a GNN of the form (1) is described by 1+(cid:80)S
parameters in the invariant case and 1 +(cid:80)S

where Fs : Rnd → Rnks are linear equivariant functions that yield ks-tensors (i.e. they potentially
increase or decrease the order of the input tensor)  and Hs are invariant linear operators Hs : Rnks →
R (resp. equivariant linear operators Hs : Rnks → Rn)  such that the GNN is globally invariant
(resp. equivariant). The invariant case is studied in Section 2  and the equivariant in Section 3. The
bias terms Bs ∈ Rnks are equivariant  so that Bs = σ (cid:63) Bs for all σ. They are also characterized
by Maron et al. (2019a) and belong to a linear space of dimension b(ks). We illustrate this simple
architecture in Fig. 1.
In light of the characterization by Maron et al. (2019a) of linear invariant and equivariant operators
s=1 b(ks+d)+2b(ks)
s=1 b(ks + d) + b(ks + 1) + b(ks) in the equivariant. As
mentioned earlier  this number of parameters does not depend on the number of nodes n  and a GNN
described by a single set of parameters can be applied to graphs of any size. In particular  we are
going to show that a GNN approximates uniformly well a continuous function for several n at once.
The function ρ is any locally Lipschitz pointwise non-linearity for which the Universal Approximation
Theorem for MLP applies. We denote their set FMLP. This includes in particular any continuous
function that is not a polynomial (Pinkus  1999). Among these  we denote the sigmoid ρsig(x) =
ex/(1 + ex).

3

F1

F2

FS

Rnk1

Rnk2
...
RnkS

ρ(· + B1)

ρ(· + B2)

ρ(· + BS)

Rnk1

Rnk2
...
RnkS

Rnd

H1

H2

HS

(cid:80)

(cid:40)

y ∈ R
y ∈ Rn

(invariant)
(equivariant)

Figure 1: The model of GNNs studied in this paper. For each channel s (cid:54) S  the input tensor is passed
through an equivariant operator Fs : Rnd → Rnks   a non-linearity with some added equivariant bias Bs  and
a ﬁnal operator Hs that is either invariant (Section 2) or equivariant (Section 3). These GNNs are universal
approximators of invariant or equivariant continuous functions (Theorems 1 and 3).

We denote by Ninv.(ρ) (resp. Neq.(ρ)) the class of invariant (resp. equivariant) 1-layer networks of
the form (1) (with S and ks being arbitrarily large). Our contributions show that they are dense in the
spaces of continuous invariant (resp. equivariant) functions.

2 The case of invariant functions

Maron et al. (2019b) recently proved that invariant GNNs similar to (1) are universal approximators
of continuous invariant functions. As a warm-up  we propose an alternative proof of (a variant of)
this result  that will serve as a basis for our main contribution  the equivariant case (Section 3).

Edit distance. For invariant functions  isomorphic graphs are undistinguishable  and therefore we
work with a set of equivalence classes of graphs  where two graphs are equivalent if isomorphic. We
deﬁne such a set for any number n (cid:54) nmax of nodes and bounded G

(cid:110)O (G) ; G ∈ Rnd with n (cid:54) nmax (cid:107)G(cid:107) (cid:54) R

(cid:111)

Ginv.

def.=

 

where we recall that O (G) = {σ (cid:63) G ; σ ∈ O} is the set of every permuted versions of G  here seen
as an equivalence class.
We need to equip this set with a metric that takes into account graphs with different number of nodes.
A distance often used in the literature is the graph edit distance (Sanfeliu and Fu  1983). It relies on
deﬁning a set of elementary operations o and a cost c(o) associated to each of them  here we consider
node addition and edge weight modiﬁcation. The distance is then deﬁned as

k(cid:88)

i=1

dedit(O (G1)  O (G2)) def.=

min

(o1 ... ok)∈P(G1 G2)

c(oi)

(2)

where P(G1  G2) contains every sequence of operation to transform G1 into a graph isomor-
phic to G2  or G2 into G1. Here we consider c(node_addition) = c for some constant c > 0 
c(edge_weight_change) = |w − w(cid:48)| where the weight change is from w to w(cid:48)  and “edge”
refers to any element of the tensor G ∈ Rnd. Note that  if we have dedit(O (G1)  O (G2)) < c 
then G1 and G2 have the same number of nodes  and in that case dedit(O (G1)  O (G2)) =
minσ∈On (cid:107)G1 − σ (cid:63) G2(cid:107)1   where (cid:107)·(cid:107)1 is the element-wise (cid:96)1 norm  since each edge must be trans-
formed into another.
We denote by C(Ginv.  dedit) the space of real-valued functions on Ginv. that are continuous with respect
to dedit  equipped with the inﬁnity norm of uniform convergence. We then have the following result.
Theorem 1. For any ρ ∈ FMLP  Ninv.(ρ) is dense in C(Ginv.  dedit).

Comparison with (Maron et al.  2019b). A variant of Theorem 1 was proved in (Maron et al. 
2019b). The two proofs are however different: their proof relies on the construction of a basis of
invariant polynomials and on classical universality of MLPs  while our proof is a direct application of
Stone-Weierstrass theorem for algebras of real-valued functions. See the next subsection for details.

4

One improvement of our result with respect to the one of (Maron et al.  2019b) is that it can handle
graphs of varying sizes. As mentioned in the introduction  a single set of parameters deﬁnes a GNN
that can be applied to graphs of any size. Theorem 1 shows that any continuous invariant function
is uniformly well approximated by a GNN on the whole set Ginv.  that is  for all numbers of nodes
n (cid:54) nmax simultaneously. On the contrary  Maron et al. (2019b) work with a ﬁxed n  and it does
not seem that their proof can extend easily to encompass several n at once. A weakness of our
proof is that it does not provide an upper bound on the order of tensorization ks. Indeed  through
Noether’s theorem on polynomials  the proof of Maron et al. (2019b) shows that ks (cid:54) nd(nd − 1)/2
is sufﬁcient for universality  which we cannot seem to deduce from our proof. Moreover  they provide
a lower-bound ks (cid:62) nd below which universality cannot be achieved.

2.1 Sketch of proof of Theorem 1

The proof for the invariant case will serve as a basis for the equivariant case in the Section 3. It relies
on Stone-Weierstrass theorem  which we recall below.
Theorem 2 (Stone-Weierstrass (Rudin (1991)  Thm. 5.7)). Suppose X is a compact Hausdorff
space and A is a subalgebra of the space of continuous real-valued functions C(X) which contains a
non-zero constant function. Then A is dense in C(X) if and only if it separates points  that is  for all
x (cid:54)= y in X there exists f ∈ A such that f (x) (cid:54)= f (y).
We will construct a class of GNNs that satisfy all these properties in Ginv.. As we will see  unlike
classical applications of this theorem to e.g. polynomials  the main difﬁculty here will be to prove the
separation of points. We start by observing that Ginv. is indeed a compact set for dedit.
Properties of (Ginv.  dedit). Let us ﬁrst note that the metric space (Ginv.  dedit) is Hausdorff (i.e. sepa-
rable  all metric spaces are). For each O (G1)  O (G2) ∈ Ginv. we have: if dedit(O (G1)  O (G2)) < c 
then the graphs have the same number of nodes  and in that case dedit(O (G1)O (G2)) (cid:54)
(cid:107)G1 − G2(cid:107)1. Therefore  the embedding G (cid:55)→ O (G) is continuous (locally Lipschitz). As the
  the set Ginv. is indeed compact.

continuous image of the compact(cid:83)nmax

; (cid:107)G(cid:107) (cid:54) R

G ∈ Rnd

(cid:110)

(cid:111)

n=1

Algebra of invariant GNNs. Unfortunately  Ninv.(ρ) is not a subalgebra. Following Hornik et al.
(1989)  we ﬁrst need to extend it to be closed under multiplication. We do that by allowing Kronecker
products inside the invariant functions:

(cid:104)
ρ (Fs1[G] + Bs1) ⊗ . . . ⊗ ρ (FsTs[G] + BsTs )

(cid:105)

+ b

(3)

S(cid:88)

s=1

f (G) =

Hs

(cid:80)

where Fst yields kst-tensors  Hs : Rn
(σ (cid:63) G) ⊗ (σ (cid:63) G(cid:48)) = σ (cid:63) (G ⊗ G(cid:48))  they are indeed invariant. We denote by N ⊗
GNNs of this form  with S  Ts  kst arbitrarily large.
Lemma 1. For any locally Lipschitz ρ  N ⊗
inv.(ρ) is a subalgebra in C(Ginv.  dedit).
The proof  presented in Appendix A.1.1 follows from manipulations of Kronecker products.

t kst → R are invariant  and Bst are equivariant bias. By
inv.(ρ) the set of all

inv.(ρsig) separates points.

Separability. The main difﬁculty in applying Stone-Weierstrass theorem is the separation of points 
which we prove in the next Lemma.
Lemma 2. N ⊗
The proof  presented in Appendix A.1.2  proceeds by contradiction: we show that two graphs G  G(cid:48)
that coincides for every GNNs are necessarily permutation of each other. Applying Stone-Weierstrass
theorem  we have thus proved that N ⊗
Then  following Hornik et al. (1989)  we go back to the original class Ninv.(ρ)  by applying: (i) a
Fourier approximation of ρsig  (ii) the fact that a product of cos is also a sum of cos  and (iii) an
approximation of cos by any other non-linearity. The following Lemma is proved in Appendix A.1.3 
and concludes the proof of Thm 1.
inv.(cos) = Ninv.(cos);
Lemma 3. We have the following: (i) N ⊗
(iii) for any ρ ∈ FMLP  Ninv.(ρ) is dense in Ninv.(cos).

inv.(ρsig) is dense in C(Ginv.  dedit).

inv.(cos) is dense in N ⊗

inv.(ρsig); (ii) N ⊗

5

3 The case of equivariant functions

This section contains our main contribution. We examine the case of equivariant functions that return
a vector f (G) ∈ Rn when G has n nodes  such that f (σ (cid:63) G) = σ (cid:63) f (G). In that case  isomorphic
graphs are not equivalent anymore. Hence we consider a compact set of graphs

G ∈ Rnd

; n (cid:54) nmax (cid:107)G(cid:107) (cid:54) R

 

(cid:111)

def.=

Geq.

(cid:110)
(cid:26)(cid:107)G − G(cid:48)(cid:107)

∞

Like the invariant case  we consider several numbers of nodes n (cid:54) nmax and will prove uniform
approximation over them. We do not use the edit distance but a simpler metric:

d(G  G(cid:48)) =

if G and G(cid:48) have the same number of nodes 
otherwise.

for any norm (cid:107)·(cid:107) on Rnd.
The set of equivariant continuous functions is denoted by Ceq.(Geq.  d)  equipped with the inﬁnity
norm (cid:107)f(cid:107)∞ = supG∈Geq. (cid:107)f (G)(cid:107)∞. We recall that Neq.(ρ) ⊂ Ceq.(Geq.  d) denotes one-layer GNNs
of the form (1)  with equivariant output operators Hs. Our main result is the following.
Theorem 3. For any ρ ∈ FMLP  Neq.(ρ) is dense in Ceq.(Geq.  d).
The proof  detailed in the next section  follows closely the previous proof for invariant functions 
but is signiﬁcantly more involved. Indeed  the classical version of Stone-Weierstrass only provides
density of a subalgebra of functions in the whole space of continuous functions  while in this case
Ceq.(Geq.  d) is already a particular subset of continuous functions. On the other hand  it seems difﬁcult
to make use of fully general versions of Stone-Weierstrass theorem  for which some questions are still
open (Glimm  1960). Hence we prove a new  specialized Stone-Weierstrass theorem for equivariant
functions (Theorem 4)  obtained with a non-trivial adaptation of the constructive proof by Brosowski
and Deutsch (1981).
Like the invariant case  our theorem proves uniform approximation for all numbers of nodes n (cid:54) nmax
at once by a single GNN. As is detailed in the next subsection  our proof of the generalized Stone-
Weierstrass theorem relies on being able to sort the coordinates of the output space Rn  and therefore
our current proof technique does not extend to high-order output Rn(cid:96) (graph to graph mappings) 
which we leave for future work. For the same reason  while the previous invariant case could be easily
extended to invariance to subgroups of On  as is done by Maron et al. (2019b)  for the equivariant
case our theorem only applies when considering the full permuation group On. Nevertheless  our
generalized Stone-Weierstrass theorem may be applicable in other contexts where equivariance to
permutation is a desirable property.

Comparison with (Sannai et al.  2019). Sannai et al. (2019) recently proved that equivariant NNs
acting on point clouds are universal  that is  for d = 1 in our notations. Despite the apparent similarity
with our result  there is a fundamental obstruction to extending their proof to high-order input tensors
like graphs. Indeed  it strongly relies on Theorem 2 of (Zaheer et al.  2017) that characterizes invariant
functions Rn → R  which is no longer valid for high-order inputs.

3.1 Sketch of proof of Theorem 3: an equivariant version of Stone-Weierstrass theorem
We ﬁrst need to introduce a few more notations. For a subset I ⊂ [n]  we deﬁne OI
def.=
{σ ∈ On ; ∃i ∈ I  j ∈ I c  σ(i) = j or σ(j) = i} the set of permutations that exchange at least one
index between I and I c. Indexing of vectors (or multivariate functions) is denoted by brackets  e.g.
[x]I or [f ]I  and inequalities x (cid:62) a are to be understood element-wise.

A new Stone-Weierstrass theorem. We deﬁne the “multiplication” of two multivariate functions
using the Hadamard product (cid:12)  i.e. the component-wise multiplication. Since (σ (cid:63) x) (cid:12) (σ (cid:63) x(cid:48)) =
σ (cid:63) (x (cid:12) x(cid:48))  it is easy to see that Ceq.(Geq.  d) is closed under multiplication  and is therefore a (strict)
subalgebra of the set of all continuous functions that return a vector in Rn for an input graph with n
nodes. As mentioned before  because of this last fact we cannot directly apply Stone-Weierstrass
theorem. We therefore prove a new generalized version.

6

Figure 2: Illustration of strategy of proof for the equivariant Stone-Weierstrass theorem (Theorem 4). Consider-
ing a function f that we are trying to approximate and a graph G for which the coordinates of f (G) are sorted
by decreasing order  we approximate f (G) by summing step-functions fi  whose ﬁrst coordinates are close to 1 
and otherwise close to 0.

Theorem 4 (Stone-Weierstrass for equivariant functions). Let A be a subalgebra of Ceq.(Geq.  d) 
such that A contains the constant function 1 and:
– (Separability) for all G  G(cid:48) ∈ Geq. with number of nodes respectively n and n(cid:48) such that G /∈ O(G(cid:48)) 
– (“Self”-separability) for all number of nodes n (cid:54) nmax  I ⊂ [n]  G ∈ Geq. with n nodes that has

for any k ∈ [n]  k(cid:48) ∈ [n(cid:48)]  there exists f ∈ A such that [f (G)]k (cid:54)= [f (G(cid:48))]k(cid:48) ;
no self-isomorphism in OI  and k ∈ I  (cid:96) ∈ I c  there is f ∈ A such that [f (G)]k (cid:54)= [f (G)](cid:96).

Then A is dense in Ceq.(Geq.  d).
In addition to a “separability” hypothesis  which is similar to the classical one  Theorem 4 requires a
“self”-separability condition  which guarantees that f (G) can have different values on its coordinates
under appropriate assumptions on G. We give below an overview of the proof of Theorem 4  the full
details can be found in Appendix B.
Our proof is inspired by the one for the classical Stone-Weierstrass theorem (Thm. 2) of Brosowski
and Deutsch (1981). Let us ﬁrst give a bit of intuition on this earlier proof.
It relies on the
explicit construction of “step”-functions: given two disjoint closed sets A and B  they show that
A contains functions that are approximately 0 on A and approximately 1 on B. Then  given a
function f : X → R (non-negative w.l.o.g.) that we are trying to approximate and ε > 0  they deﬁne
Ak = {x ; f (x) (cid:54) (k − 1/3)ε} and Bk = {x ; f (x) (cid:62) (k + 1/3)ε} as the lower (resp. upper)
level sets of f for a grid of values with precision ε. Then  taking the step-functions fk between Ak
k fk  since for each x only the
right number of fk is close to 1  the others are close to 0.
Given a function f ∈ Ceq.(Geq.  d)
The situation is more complicated in our case.
to approximate  we work in the compact subset of Geq. where the co-
that we want
def.=
ordinates of f are ordered 
G ∈ Geq. ; if G ∈ Rnd: [f (G)]1 (cid:62) [f (G)]2 (cid:62) . . . (cid:62) [f (G)]n
. Then  we will prove the existence
of step-functions such that: when A and B satisfy some appropriate hypotheses  the step-function is
close to 0 on A  and only the ﬁrst coordinates are close to 1 on B  the others are close to 0. Indeed 
by combining such functions  we can approximate a vector of ordered coordinates (Fig. 2). The
construction of such step-functions is done in Lemma 7. Finally  we consider modiﬁed level-sets

and Bk  it is easy to prove that f is well-approximated by g = ε(cid:80)

since by permutation it covers every case:

(cid:110)

(cid:111)

Gf

G ∈ Gf ∩ Rnd

; [f (G)](cid:96) − [f (G)](cid:96)+1 (cid:54) (k − 1/3)ε

G ∈ Gf ∩ Rnd

; [f (G)](cid:96) − [f (G)](cid:96)+1 (cid:62) (k + 1/3)ε

(cid:16)Gf ∩ R(n(cid:48))d(cid:17)

 

(cid:111) ∪ (cid:91)
(cid:111)

n(cid:48)(cid:54)=n

(cid:110)
(cid:110)

An (cid:96)

k

def.=

Bn (cid:96)

k

def.=

and show that g = ε(cid:80)

that distinguish “jumps” between (ordered) coordinates. We deﬁne the associated step-functions f n (cid:96)
k  

k n (cid:96) f n (cid:96)

k

is a valid approximation of f.

End of the proof. The rest of the proof of Theorem 3 is similar to the invariant case. We ﬁrst
build an algebra of GNNs  again by considering nets of the form (3)  where we replace the Hs’s by
equivariant linear operators in this case. We denote this space by N ⊗
Lemma 4. N ⊗

eq.(ρ) is a subalgebra of Ceq.(Geq.  d).

eq.(ρ).

7

eq.(ρsig.) satisﬁes both the separability and self-separability conditions.

The proof  presented in Appendix A.2.1  is very similar to that of Lemma 1. Then we show the two
separation conditions for equivariant GNNs.
Lemma 5. N ⊗
The proof is presented in Appendix A.2.2. The “normal” separability is in fact equivalent to the
previous one (Lemma 2)  since we can construct an equivariant network by simply stacking an
invariant network on every coordinate. The self-separability condition is proved in a similar way.
Finally we go back to Neq.(ρ) in exactly the same way. The proof of Lemma 6 is exactly similar to
that of Lemma 3 and is omitted.
Lemma 6. We have the following: (i) N ⊗
eq.(cos) = Neq.(cos);
(iii) for any ρ ∈ FMLP  Neq.(ρ) is dense in Neq.(cos).

eq.(cos) is dense in N ⊗

eq.(ρsig); (ii) N ⊗

4 Numerical illustrations

This section provides numerical illustrations of our ﬁndings on
simple synthetic examples. The goal is to examine the impact
of the tensorization orders ks and the width S. The code is
available at https://github.com/nkeriven/univgnn. We
emphasize that the contribution of the present paper is ﬁrst and
foremost theoretical  and that  like MLPs with a single hidden
layer  we cannot expect the shallow GNNs (1) to be state-of-the-
art and compete with deep models  despite their universality. A
benchmarking of deep GNNs that use invariant and equivariant
linear operators is done in (Maron et al.  2019a).
We consider graphs  represented using their adjacency matrices
(i.e. 2-ways tensor  so that d = 2). The synthetic graphs are
drawn uniformly among 5 graph topologies (complete graph 
star  cycle  path or wheel) with edge weights drawn indepen-
dently as the absolute value of a centered Gaussian variable.
Since our approximation results are valid for several graph
sizes simultaneously  both training and testing datasets contain
1.4 · 104 graphs  half with 5 nodes and half with 10 nodes.
The training is performed by minimizing a square Euclidean
loss (MSE) on the training dataset. The minimization is per-
formed by stochastic gradient descent using the ADAM opti-
mizer (Kingma and Ba  2014). We consider two different regression tasks: (i) in the invariant case 
the scalar to predict is the geodesic diameter of the graph  (ii) in the equivariant case  the vector to
predict assigns to each node the length of the longest shortest-path emanating from it. While these
functions can be computed using polynomial time all-pairs shortest paths algorithms  they are highly
non-local  and are thus challenging to learn using neural network architectures. The GNNs (1) are
implemented with a ﬁxed tensorization order ks = k ∈ {1  2  3} and ρ = ρsig..
Figure 3 shows that  on these two cases  when increasing the width S  the out-of-sample prediction
error quickly stagnates (and sometime increasing too much S can slightly degrade performances by
making the training harder). In sharp contrast  increasing the tensorization order k has a signiﬁcant
impact and lowers this optimal error value. This support the fact that universality relies on the use of
higher tensorization order. This is a promising direction of research to integrate higher order tensors
withing deeper architecture to better capture complex functions on graphs.

Figure 3: MSE results after 150
epochs  in the invariant (top) and equiv-
ariant (bottom) cases  averaged over 5
experiments. Dashed lines represent
the testing error.

5 Conclusion

In this paper  we proved the universality of a class of one hidden layer equivariant networks. Handling
this vector-valued setting required to extend the classical Stone-Weierstrass theorem. It remains an
open problem to extend this technique of proof for more general equivariant networks whose outputs
are graph-valued  which are useful for instance to model dynamic graphs using recurrent architectures
(Battaglia et al.  2016). Another outstanding open question  formulated in (Maron et al.  2019b)  is
the characterization of the approximation power of networks whose tensorization orders ks inside the
layers are bounded  since they are much more likely to be implemented on large graphs in practice.

8

101102s1234567lossk=1k=2k=3101102s102030lossk=1k=2k=3References
P. W. Battaglia  R. Pascanu  M. Lai  D. Rezende  and K. Kavukcuoglu. Interaction Networks for
Learning about Objects  Relations and Physics. In Advances in Neural Information and Processing
Systems (NIPS)  pages 4509–4517  2016.

M. M. Bronstein  J. Bruna  Y. Lecun  A. Szlam  and P. Vandergheynst. Geometric Deep Learning:

Going beyond Euclidean data. IEEE Signal Processing Magazine  34(4):18–42  2017.

B. Brosowski and F. Deutsch. An elementary proof of the Stone-Weierstrass Theorem. Proceedings

of the American Mathematical Society  81(1):89–92  1981.

J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral Networks and Locally Connected Networks

on Graphs. In ICLR  pages 1–14  2014.

Z. Chen  X. Li  and J. Bruna. Supervised Community Detection with Line Graph Neural Networks.

In ICLR  2019.

T. Cohen and M. Welling. Group equivariant convolutional networks. In International conference on

machine learning  pages 2990–2999  2016.

G. Cybenko. Approximation by superpositions of a sigmoidal function. Mahematics of Control 

Signals  and Systems  2:303–314  1989.

G. de Bie  G. Peyré  and M. Cuturi. Stochastic deep networks. In Proceedings of ICML 2019  2019.

M. Defferrard  X. Bresson  and P. Vandergheynst. Convolutional Neural Networks on Graphs with
Fast Localized Spectral Filtering. In Advances in Neural Information and Processing Systems
(NIPS)  2016.

A. Fout  B. Shariat  J. Byrd  and A. Ben-Hur. Protein Interface Prediction using Graph Convolutional

Networks. Nips  (Nips):6512–6521  2017.

R. Gens and P. M. Domingos. Deep symmetry networks.

processing systems  pages 2537–2545  2014.

In Advances in neural information

J. Glimm. A Stone-Weierstrass Theorem for C * -Algebras. Annals of Mathematics  72(2):216–244 

1960.

J. Hartford  D. R. Graham  K. Leyton-Brown  and S. Ravanbakhsh. Deep models of interactions

across sets. arXiv preprint arXiv:1803.02879  2018.

K. Hornik  M. Stinchcombe  and H. White. Multilayer Feedforward Networks are Universal Approx-

imators. Neural Networks  2:359–366  1989.

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR  2014.

R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks

to the action of compact groups. arXiv preprint arXiv:1802.03690  2018.

R. Kondor  H. T. Son  H. Pan  B. Anderson  and S. Trivedi. Covariant compositional networks for

learning graphs. arXiv preprint arXiv:1801.02144  2018.

Y. LeCun  B. Boser  J. S. Denker  D. Henderson  R. E. Howard  W. Hubbard  and L. D. Jackel.
Backpropagation applied to handwritten zip code recognition. Neural computation  1(4):541–551 
1989.

H. Maron  H. Ben-Hamu  N. Shamir  and Y. Lipman. Invariant and Equivariant Graph Networks. In

ICLR  pages 1–13  2019a.

H. Maron  E. Fetaya  N. Segol  and Y. Lipman. On the Universality of Invariant Networks. In

International Conference on Machine Learning (ICML)  2019b.

A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica  8(May):

143–195  1999.

9

C. R. Qi  H. Su  K. Mo  and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation
In Proceedings of the IEEE Conference on Computer Vision and Pattern

and segmentation.
Recognition  pages 652–660  2017.

S. Ravanbakhsh  J. Schneider  and B. Poczos. Equivariance through parameter-sharing. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70  pages 2892–2901. JMLR.
org  2017.

W. Rudin. Functional Analysis. 1991.

A. Sanchez-Gonzalez  N. Heess  J. T. Springenberg  J. Merel  M. Riedmiller  R. Hadsell 
and P. Battaglia. Graph networks as learnable physics engines for inference and control.
arxiv:1806.01242  2018.

A. Sanfeliu and K.-S. Fu. A distance measure between attributed relational graphs for pattern

recognition. IEEE transactions on systems  man  and cybernetics  (3):353–362  1983.

A. Sannai  Y. Takai  and M. Cordonnier. Universal approximations of permutation invari-

ant/equivariant functions by deep neural networks. ArXiv: 1903.01939  2019.

F. Scarselli  M. Gori  A. C. Tsoi  G. Monfardini  M. Hagenbuchner  and G. Monfardini. The graph

neural network model. IEEE Transactions on Neural Networks  20(1):61–80  2009.

J. Shawe-Taylor. Symmetries and discriminability in feedforward network architectures. IEEE

Transactions on Neural Networks  4(5):816–826  1993.

J. Wood and J. Shawe-Taylor. Representation theory and invariant neural networks. Discrete applied

mathematics  69(1-2):33–60  1996.

K. Xu  W. Hu  J. Leskovec  and S. Jegelka. How Powerful are Graph Neural Networks? In ICLR 

pages 1–15  2019.

D. Yarotsky. Universal approximations of invariant maps by neural networks. ArXiv: 1804.10306 

pages 1–64  2018.

R. Ying  R. He  K. Chen  P. Eksombatchai  W. L. Hamilton  and J. Leskovec. Graph Convolutional
Neural Networks for Web-Scale Recommender Systems. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining  pages 974–983  2018.

M. Zaheer  S. Kottur  S. Ravanbakhsh  B. Poczos  R. R. Salakhutdinov  and A. J. Smola. Deep sets.

In Advances in neural information processing systems  pages 3391–3401  2017.

J. Zhou  G. Cui  Z. Zhang  C. Yang  Z. Liu  L. Wang  C. Li  and M. Sun. Graph Neural Networks: A

Review of Methods and Applications. ArXiv: 1812.08434  pages 1–20  2018.

10

,Nicolas Keriven
Gabriel Peyré