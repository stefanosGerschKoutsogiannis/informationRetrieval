2013,Geometric optimisation on positive definite matrices for elliptically contoured distributions,Hermitian positive definite matrices (HPD) recur throughout statistics and machine learning. In this paper we develop \emph{geometric optimisation} for globally optimising certain nonconvex loss functions arising in the modelling of data via elliptically contoured distributions (ECDs). We exploit the remarkable structure of the convex cone of positive definite matrices which allows one to uncover hidden geodesic convexity of objective functions that are nonconvex in the ordinary Euclidean sense. Going even beyond manifold convexity we show how further metric properties of HPD matrices can be exploited to globally optimise several ECD log-likelihoods that are not even geodesic convex. We present key results that help recognise this geometric structure  as well as obtain efficient fixed-point algorithms to optimise the corresponding objective functions. To our knowledge  ours are the most general results on geometric optimisation of HPD matrices known so far. Experiments reveal the benefits of our approach---it avoids any eigenvalue computations which makes it very competitive.,Geometric optimisation on positive deﬁnite matrices
with application to elliptically contoured distributions

Suvrit Sra

Max Planck Institute for Intelligent Systems

T¨ubingen  Germany

Reshad Hosseini

School of ECE  College of Engineering

University of Tehran  Tehran  Iran

Abstract

Hermitian positive deﬁnite (hpd) matrices recur throughout machine learning 
statistics  and optimisation. This paper develops (conic) geometric optimisation
on the cone of hpd matrices  which allows us to globally optimise a large class of
nonconvex functions of hpd matrices. Speciﬁcally  we ﬁrst use the Riemannian
manifold structure of the hpd cone for studying functions that are nonconvex
in the Euclidean sense but are geodesically convex (g-convex)  hence globally
optimisable. We then go beyond g-convexity  and exploit the conic geometry
of hpd matrices to identify another class of functions that remain amenable to
global optimisation without requiring g-convexity. We present key results that
help recognise g-convexity and also the additional structure alluded to above. We
illustrate our ideas by applying them to likelihood maximisation for a broad family
of elliptically contoured distributions: for this maximisation  we derive novel 
parameter free ﬁxed-point algorithms. To our knowledge  ours are the most general
results on geometric optimisation of hpd matrices known so far. Experiments show
that advantages of using our ﬁxed-point algorithms.

1

Introduction

The geometry of Hermitian positive deﬁnite (hpd) matrices is remarkably rich and forms a founda-
tional pillar of modern convex optimisation [21] and of the rapidly evolving area of convex algebraic
geometry [4]. The geometry exhibited by hpd matrices  however  goes beyond what is typically
exploited in these two areas. In particular  hpd matrices form a convex cone which is also a dif-
ferentiable Riemannian manifold that is also a CAT(0) space (i.e.  a metric space of nonpositive
curvature [7]). This rich structure enables “geometric optimisation” with hpd matrices  which allows
solving many problems that are nonconvex in the Euclidean sense but convex in the manifold sense
(see §2 or [29])  or have enough metric structure (see §3) to permit efﬁcient optimisation.
This paper develops (conic) geometric optimisation1 (GO) for hpd matrices. We present key results
that help recognise geodesic convexity (g-convexity); we also present sufﬁcient conditions that put a
class of even non g-convex functions within the grasp of GO. To our knowledge  ours are the most
general results on geometric optimisation with hpd matrices known so far.
Motivation for GO. We begin by noting that the widely studied class of geometric programs is
ultimately nothing but the 1D version of GO on hpd matrices. Given that geometric programming
has enjoyed great success in numerous applications—see e.g.  the survey of Boyd et al. [6]—we
hope GO also gains broad applicability. For this paper  GO arises naturally while performing
maximum likelihood parameter estimation for a rich class of elliptically contoured distributions

1To our knowledge the name “geometric optimisation” has not been previously attached to hpd matrix
optimisation  perhaps because so far only scattered few examples were known. Our theorems provide a starting
point for recognising and constructing numerous problems amenable to geometric optimisation.

1

(ECDs) [8  13  20]. Perhaps the best known GO problem is the task of computing the Karcher /
Fr´echet-mean of hpd matrices: a topic that has attracted great attention within matrix theory [2  3  27] 
computer vision [10]  radar imaging [22; Part II]  and medical imaging [11  31]—we refer the reader
to the recent book [22] for additional applications  references  and details. Another GO problem
arises as a subroutine in nearest neighbour search over hpd matrices [12]. Several other areas involve
GO problems: statistics (covariance shrinkage) [9]  nonlinear matrix equations [17]  Markov decision
processes and the wider encompassing area of nonlinear Perron-Frobenius theory [18].
Motivating application. We use ECDs as a platform for illustrating our ideas for two reasons:
(i) ECDs are important in a variety of settings (see the recent survey [23]); and (ii) they offer an
instructive setup for presenting key ideas from the world of geometric optimisation.
Let us therefore begin by recalling some basics. An ECD with density on Rd takes the form 2

∀ x ∈ Rd 

Eϕ(x; S) ∝ det(S)−1/2ϕ(xT S−1x) 

(1)
where S ∈ Pd (i.e.  the set of d × d symmetric positive deﬁnite matrices) is the scatter matrix while
ϕ : R → R++ is positive density generating function (dgf). If ECDs have ﬁnite covariance matrix 
then the scatter matrix is proportional to the covariance matrix [8].
Example 1. With ϕ(t) = e− t

2   density (1) reduces to the multivariate normal density. For the choice
(2)
where α  b and β are ﬁxed positive numbers  density (1) yields the rich class called Kotz-type
distributions that are known to have powerful modelling abilities [15; §3.2]; they include as special
cases multivariate power exponentials  elliptical gamma  multivariate W-distributions  for instance.
MLE. Let (x1  . . .   xn) be i.i.d. samples from an ECD Eϕ(S). Up to constants  the log-likelihood is
(3)

ϕ(t) = tα−d/2 exp(cid:0)−(t/b)β(cid:1) 

L(S) = − 1

(cid:88)n

log ϕ(xT

i S−1xi).

i=1
Equivalently  we may consider the minimisation problem

2 n log det S +

2 n log det(S) −(cid:88)

i S−1xi).

log ϕ(xT

minS(cid:31)0 Φ(S) := 1

(4)
Problem (4) is in general difﬁcult as Φ may be nonconvex and may have multiple local minima.
Since statistical estimation theory relies on having access to global optima  it is important to be able
to solve (4) to global optimality. These difﬁculties notwithstanding  using GO ideas  we identify a
rich class of ECDs for which we can indeed solve (4) optimally. Some examples already exist in
the literature [16  23  30]; this paper develops techniques that are strictly more general and subsume
previous examples  while advancing the broader idea of geometric optimisation.
We illustrate our ideas by studying the following two main classes of dgfs in (1):

i

(i) Geodesically Convex (GC): This class contains functions for which the negative log-likelihood
Φ(S) is g-convex  i.e.  convex along geodesics in the manifold of hpd matrices. Some members
of this class have been previously studied (though sometimes without recognising or directly
exploiting the g-convexity);

(ii) Log-Nonexpansive (LN): This is a new class that we introduce in this paper. It exploits the

“non-positive curvature” property of the manifold of hpd matrices.

There is a third important class: LC  the class of log-convex dgfs ϕ. Though  since (4) deals with
− log ϕ  the optimisation problem is still nonconvex. We describe class LC only in [28] primarily
due to paucity of space and also because the ﬁrst two classes contain our most novel results. These
classes of dgfs are neither mutually disjoint nor proper subsets of each other. Each captures unique
analytic or geometric structure crucial to efﬁcient optimisation. Class GC characterises the “hidden”
convexity found in several instances of (4)  while LN is a novel class of models that might not have
this hidden convexity  but nevertheless admit global optimisation.
Contributions. The key contributions of this paper are the following:
– New results that characterise and help recognise g-convexity (Thm. 1  Cor. 2  Cor. 3  Thm. 4).
Though initially motivated by ECDs  our matrix-theoretic proofs are more generally applicable and
should be of wider interest. All technical proofs  and several additional results that help recognise
g-convexity are in the longer version of this paper [28].
2For simplicity we describe only mean zero families; the extension to the general case is trivial.

2

– New ﬁxed-point theory for solving GO problems  including some that might even lack g-convexity.
Here too  our results go beyond ECDs—in fact  they broaden the class of problems that admit
ﬁxed-point algorithms in the metric space (Pd  δT )—Thms. 11 and 14 are the key results here.

Our results on geodesic convexity subsume the more specialised results reported recently in [29].
We believe our matrix-theoretic proofs  though requiring slightly more advanced machinery  are
ultimately simpler and more widely applicable. Our ﬁxed-point theory offers a uniﬁed framework
that not only captures the well-known M-estimators of [16]  but applies to a larger class of problems
than possible using previous methods. Our experimental illustrate computational beneﬁts of one of
resulting algorithms.

2 Geometric optimisation with geodesic convexity: class GC

Geodesic convexity (g-convexity) is a classical concept in mathematics and is used extensively in
the study of Hadamard manifolds and metric spaces of nonpositive curvature [7  24] (i.e.  spaces
whose distance function is g-convex). This concept has been previously studied in nonlinear optimi-
sation [25]  but its full importance and applicability in statistical applications and optimisation is only
recently emerging [29  30].
We begin our presentation by recalling some deﬁnitions—please see [7  24] for extensive details.
Deﬁnition 2 (gc set). Let M denote a d-dimensional connected C 2 Riemannian manifold. A set
X ⊂ M  where is called geodesically convex if any two points of X are joined by a geodesic lying in
X . That is  if x  y ∈ X   then there exists a path γ : [0  1] → X such that γ(0) = x and γ(1) = y.
Deﬁnition 3 (gc function). Let X ⊂ M be a gc set. A function φ : X → R is geodesically convex 
if for any x  y ∈ X and a unit speed geodesic γ : [0  1] → X with γ(0) = x and γ(1) = y  we have
(5)

φ(γ(t)) ≤ (1 − t)φ(γ(0)) + tφ(γ(1)) = (1 − t)φ(x) + tφ(y).

The power of gc functions in the context of solving (4) comes into play because the set Pd (the
convex cone of positive deﬁnite matrices) is also a differentiable Riemannian manifold where
geodesics between points can be computed efﬁciently. Indeed  the tangent space to Pd at any point
can be identiﬁed with the set of Hermitian matrices  and the inner product on this space leads to
a Riemannian metric on Pd. At any point A ∈ Pd  this metric is given by the differential form
ds = (cid:107)A−1/2dAA−1/2(cid:107)F; also  between A  B ∈ Pd there is a unique geodesic [1; Thm. 6.1.6]

A#tB := γ(t) = A1/2(A−1/2BA−1/2)tA1/2 

(6)
The midpoint of this path  namely A#1/2B is called the matrix geometric mean  which is an object
of great interest in numerous areas [1–3  10  22]. As per convention  we denote it simply by A#B.
Example 4. Let z ∈ Cd be any vector. The function φ(X) := z∗X−1z is gc.
Proof. Since φ is continuous  it sufﬁces to verify midpoint convexity: φ(X#Y ) ≤ 1
for X  Y ∈ Pd. Since (X#Y )−1 = X−1#Y −1 and X−1#Y −1 (cid:22) X−1+Y −1
that φ(X#Y ) = z∗(X#Y )−1z ≤ 1

2 φ(Y ) 
([1; 4.16])  it follows

2 φ(X) + 1

t ∈ [0  1].

2 (z∗X−1z + z∗Y −1z) = 1

2 (φ(X) + φ(Y )).

2

We are ready to state our ﬁrst main theorem  which vastly generalises the above example and provides
a foundational tool for recognising and constructing gc functions.
Theorem 1. Let Π : Pd → Pk be a strictly positive linear map. Let A  B ∈ Pd we have

Π(A#tB) (cid:22) Π(A)#tΠ(B) 

t ∈ [0  1].

(7)

Proof. Although positive linear maps are well-studied objects (see e.g.  [1; Ch. 4])  we did not ﬁnd
an explicit proof of (7) in the literature  so we provide a proof in the longer version [28].

A useful corollary of Thm. 1 is the following (notice this corollary subsumes Example 4).
Corollary 2. For positive deﬁnite matrices A  B ∈ Pd and matrices 0 (cid:54)= X ∈ Cd×k we have

tr X∗(A#tB)X ≤ [tr X∗AX]1−t[tr X∗BX]t 

t ∈ (0  1).

(8)

3

Proof. Use the map A (cid:55)→ tr X∗AX in Thm. 1.

Note: Cor. 2 actually constructs a log-g-convex function  from which g-convexity is immediate.
A notable corollary to Thm. 1 that subsumes a nontrivial result [14; Lem. 3.2] is mentioned below.
Corollary 3. Let Xi ∈ Cd×k with k ≤ d such that rank([Xi]m
i=1) = k. Then the function φ(S) :=

log det((cid:80)
Proof. By our assumption on the Xi  the map Π = S (cid:55)→(cid:80)
Thm 1 it follows that Π(S#R) =(cid:80)

i X∗
and determinant is multiplicative  the previous inequality yields

i SXi) is gc on Pd.

i SXi is strictly positive. Thus  from
i (S#R)Xi (cid:22) Π(S)#Π(R). Since log det is monotonic 

i X∗

i X∗

φ(S#R) = log det Π(S#R) ≤ log det(Π(S)) + log det(Π(R)) = 1

2 φ(S) + 1

2 φ(R).

We are now ready to state our second main theorem.
Theorem 4. Let h : Pk → R be gc function that is nondecreasing in L¨owner order. Let r ∈ {±1} 
and let Π : Pd → Pk be a strictly positive linear map. Then  φ(S) = h(Π(Sr)) ± log det(S) is gc.
Proof. Since φ is continuous  it sufﬁces to prove midpoint geodesic convexity. Since r ∈ {±1} 
(S#R)r = Sr#Rr; thus  from Thm. 1 and since h is matrix nondecreasing  it follows that

h(Π(S#R)r) = h(Π(Sr#Rr)) ≤ h(Π(Sr)#Π(Rr)).

(9)

(10)

Since h is also gc  inequality (9) further yields
h(Π(Sr)#Π(Rr)) ≤ 1

Since ± log det(S#R) = ± 1

2

(cid:0)log det(S) + log det(R)(cid:1)  on combining with (10) we obtain

2 h(Π(Sr)) + 1

2 h(Π(Rr)).

φ(S#R) ≤ 1

2 φ(S) + 1

2 φ(R) 

as desired. Notice also that if h is strictly gc  then φ(S) is also strictly gc.

Finally  we state a corollary of Thm. 4 helpful towards recognising geodesic convexity of ECDs.
We mention here that a result equivalent to Corr. 5 was recently also discovered in [30]. Thm. 4 is
more general and uses a completely different argument founded on the matrix-theoretic results; our
techniques may also be of wider independent interest.
Corollary 5. Let h : R++ → R be nondecreasing and gc (i.e.  h(x1−λyλ) ≤ (1− λ)h(x) + λh(y)).

Then  for r ∈ {±1}  φ : Pd → R : S (cid:55)→(cid:80)

i Srxi) ± log det(S) is gc.

i h(xT

2.1 Application to ECDs in class GC

We begin with a straightforward corollary of the above discussion.
Corollary 6. For the following distributions the negative log-likelihood (4) is gc: (i) Kotz with α ≤ d
2
(its special cases include Gaussian  multivariate power exponential  multivariate W-distribution with
shape parameter smaller than one  elliptical gamma with shape parameter ν ≤ d
2 ); (ii) Multivariate-t;
(iii) Multivariate Pearson type II with positive shape parameter; (iv) Elliptical multivariate logistic
distribution. 3

If the log-likelihood is strictly gc then (4) cannot have multiple solutions. Moreover  for any local
optimisation method that computes a solution to (4)  geodesic convexity ensures that this solution is
globally optimal. Therefore  the key question to answer is: (i) does (4) have a solution?
Note that answering this question is nontrivial even in special cases [16  30]. We provide below a
fairly general result that helps establish existence.

3The dgfs of different distributions are brought here for the reader’s convenience. Multivariate power
ν > 0;
ν > 0;
ν > −1  0 ≤ t ≤ 1; Elliptical multivariate logistic:

ν > 0; Multivariate W-distribution: φ(t) = tν−1 exp(−tν /b) 
ν > 0; Multivariate t: φ(t) = (1 + t/ν)−(ν+d)/2 

exponential: φ(t) = exp(−tν /b) 
Elliptical gamma: φ(t) = tν−d/2 exp(−t/b) 
Multivariate Pearson type II: φ(t) = (1 − t)ν  
φ(t) = exp(−√
t)/(1 + exp(−√

t))2.

4

Theorem 7. If Φ(S) satisﬁes the following properties: (i) − log ϕ(t) is lower semi-continuous (lsc)
for t > 0  and (ii) Φ(S) → ∞ as (cid:107)S(cid:107) → ∞ or (cid:107)S−1(cid:107) → ∞  then Φ(S) attains its minimum.

Proof. Consider the metric space (Pd  dR)  where dR is the Riemannian distance 

dR(A  B) = (cid:107)log(A−1/2BA−1/2)(cid:107)F

(11)
If Φ(S) → ∞ as (cid:107)S(cid:107) → ∞ or as (cid:107)S−1(cid:107) → ∞  then Φ(S) has bounded lower-level sets in (Pd  dR).
It is a well-known result in variational analysis that a function that has bounded lower-level sets in
a metric space and is lsc  then the function attains its minimum [26]. Since − log ϕ(t) is lsc and
log det(S−1) is continuous  Φ(S) is lsc on (Pd  dR). Therefore it attains its minimum.

A  B ∈ Pd.

A key consequence of Thm. 7 is its ability to show existence of solutions to (4) for a variety of
different ECDs. Let us look at an application to Kotz-type distributions below. For these distributions 
the function Φ(S) assumes the form

(cid:88)n

(cid:88)n

(cid:16) xT

i=1

i S−1xi

b

(cid:17)β

K(S) = n

2 log det(S) + ( d

2 − α)

log xT

i S−1xi +

i=1

Lemma 8 shows that K(S) → ∞ whenever (cid:107)S−1(cid:107) → ∞ or (cid:107)S(cid:107) → ∞.
Lemma 8. Let the data X = {x1  . . .   xn} span the whole space and satisfy for α < d

.

(12)

2 the condition

|X ∩ L|

dL

|X| <

(13)
where L is an arbitrary subspace with dimension dL < d and |X ∩ L| is the number of datapoints
that lie in the subspace L. If (cid:107)S−1(cid:107) → ∞ or (cid:107)S(cid:107) → ∞  then K(S) → ∞.
Proof. If (cid:107)S−1(cid:107) → ∞ and since the data span the whole space  it is possible to ﬁnd a datum x1 such
that t1 = xT

1 S−1x1 → ∞. Since

d − 2α

 

t→∞ c1 log(t) + tc2 + c3 → ∞

lim

for constants c1 c3 and c2 > 0  it follows that K(S) → ∞ whenever (cid:107)S−1(cid:107) → ∞.
If (cid:107)S(cid:107) → ∞ and (cid:107)S−1(cid:107) is bounded  then the third term in expression of K(S) is bounded. Assume
that dL is the number of eigenvalues of S that go to ∞ and |X ∩ L| is the number of data that lie
in the subspace span by these eigenvalues. Then in the limit when eigenvalues of S go to ∞  K(S)
converges to the following limit

n

2 dL log λ + ( d

2 − α)|X ∩ L| log λ−1 + c

lim
λ→∞

Apparently if n

2 dL − ( d

2 − α)|X ∩ L| > 0  then K(S) → ∞ and the proof is complete.

It is important to note that overlap condition (13) can be fulﬁlled easily by assuming that the number
of data is larger than their dimensionality and that they are noisy. Using Lemma 8  we can invoke
Thm. 7 to immediately state the following result.
Theorem 9 (Existence Kotz-distr.). If the data samples satisfy condition (13)  then the Kotz negative
log-likelihood has a minimiser.

As previously mentioned  once existence is ensured  one may use any local optimisation method to
minimise (4) to obtain the desired mle. This brings us to the next question. What if Φ(S) is neither
convex nor g-convex? The ideas introduced in Sec. 3 below offer a partial one answer.
3 Geometric optimisation for class LN
Without convexity or g-convexity  in general at best we might obtain local minima. However  as
alluded to previously  the set Pd of hpd matrices possesses remarkable geometric structure that allows
us to extend global optimisation to a rich class beyond just gc functions. To our knowledge  this class
of ECDs was beyond the grasp of previous methods [16  29  30]. We begin with a key deﬁnition.

5

Deﬁnition 5 (Log-nonexpansive). Let f : R++ → (0 ∞). We say f is log-nonexpansive (LN) on a
compact interval I ⊂ R+ if there exists a ﬁxed constant 0 ≤ q ≤ 1 such that
∀s  t ∈ I.
If q < 1  we say f is log-contractive. Finally  if for every s (cid:54)= t it holds that
s (cid:54)= t 

| log f (t) − log f (s)| ≤ q| log t − log s| 

| log f (t) − log f (s)| < | log t − log s| 

∀s  t

(14)

we say f is weakly log-contractive (wlc); an important point to note here is the absence of a ﬁxed q.

(cid:88)n

i=1

∂Φ(S)

Next we study existence  uniqueness  and computation of solutions to (4). To that end  momentarily
ignore the constraint S (cid:31) 0  to see that the ﬁrst-order necessary optimality condition for (4) is

xih(xT

ϕ(cid:48)(xT
ϕ(xT

i S−1 = 0.

i S−1xi)
i S−1xi) S−1xixT
Deﬁning h ≡ −ϕ(cid:48)/ϕ  condition (15) may be rewritten more compactly as
n Xh(DS)X T  

∂S = 0 ⇐⇒ 1
(cid:88)n

i S−1xi)xT

2 nS−1 +

(16)
S = 2
n
i S−1xi)  and X = [x1  . . .   xm]. If (16) has a positive deﬁnite solution  then
where DS := Diag(xT
it is a candidate mle; if it is unique  then it is the desired solution (observe that if we have a Gaussian 
then h(t) ≡ 1/2  and as expected (16) reduces to the sample covariance matrix).
But how should we solve (16)? This question is in general highly nontrivial to answer because (16) is
difﬁcult nonlinear equation in matrix variables. This is the point where the class LN introduced above
comes into play. More speciﬁcally  we solve (16) via a ﬁxed-point iteration. Introduce therefore the
nonlinear map G : Pd → Pd that maps S to the right hand side of (16); then  starting with a feasible
S0 (cid:31) 0  simply perform the iteration

i = 2

(15)

i=1

k = 0  1  . . .  

(17)

Sk+1 ← G(Sk) 
which is shown more explicitly as Alg. 1 below.

Algorithm 1 Fixed-point iteration for mle

Input: Observations x1  . . .   xn; function h
Initialize: k ← 0; S0 ← In
while ¬ converged do

i=1 xih(xT

i S−1

k xi)xT
i

(cid:80)n

Sk+1 ← 2

n

end while
return Sk

The most interesting twist to analysing iteration (17) is that the map G is usually not contractive with
respect to the Euclidean metric. But the metric geometry of Pd alluded to previously suggests that it
might be better to analyse the iteration using a non-Euclidean metric. Unfortunately  the Riemannnian
distance (11) on Pd  while canonical  also turns out to be unsuitable. This impasse is broken by
selecting a more suitable “hyperbolic distance” that captures the crucial non-Euclidean geometry of
Pd  while still respecting its convex conical structure.
Such a suitable choice is provided by the Thompson metric—an object of great interest in nonlinear
matrix equations [17]—which is known to possess geometric properties suitable for analysing convex
cones  of which Pd is a shining example [18]. On Pd  the Thompson metric is given by

δT (X  Y ) := (cid:107)log(Y −1/2XY −1/2)(cid:107) 

(18)
where (cid:107)·(cid:107) is the usual operator 2-norm  and ‘log’ is the matrix logarithm. The core properties of (18)
that prove useful for analysis ﬁxed point iterations are listed below—for proofs please see [17  19].
Proposition 10. Unless noted otherwise  all matrices are assumed to be hpd..

δT (X−1  Y −1) = δT (X  Y )
δT (B∗XB  B∗Y B) = δT (X  Y ) 

δT (X t  Y t) ≤ |t|δT (X  Y ) 

B ∈ GLn(C)

for t ∈ [−1  1]

(cid:16)(cid:88)

δT

(cid:88)

(cid:17) ≤

i

wiYi

wiXi 
δT (X + A  Y + A) ≤

i

δT (Xi  Yi) 

max
1≤i≤m
α
α+β δT (X  Y ) 

wi ≥ 0  w (cid:54)= 0

A (cid:23) 0 

(19a)
(19b)
(19c)

(19d)

(19e)

where α = max{(cid:107)X(cid:107) (cid:107)Y (cid:107)} and β = λmin(A).

6

We need one more crucial result (see [28] for a proof)  which we state below. This theorem should be
of wider interest as it enlarges the class of maps that one can study using the Thompson metric.
Theorem 11. Let X ∈ Cd×p  where p ≤ d  and rank(X) = p. Let A  B ∈ Pd. Then 

δT (X∗AX  X∗BX) ≤ δT (A  B).

(20)

We now show how to use Prop. 10 and Thm. 11 to analyse contractions on Pd.
Proposition 12. Let h be a LN function. Then  the map G in (17) is nonexpansive in δT . Moreover  if
h is wlc  then G is weakly-contractive in δT .
Proof. Let S  R (cid:31) 0 be arbitrary. Then  we have the following chain of inequalities

δT (G(S) G(R)) = δT
≤ δT
≤ max
1≤i≤n

(cid:0)xT

n Xh(DR)X T(cid:1)
(cid:0) 2
(cid:0)h(DS)  h(DR)(cid:1) ≤ max

n Xh(DS)X T   2

δT

i S−1xi  xT

i R−1xi

(cid:0)h(xT
(cid:1) ≤ δT

δT

i R−1xi)(cid:1)
(cid:0)S−1  R−1(cid:1) = δT (S  R) 

i S−1xi)  h(xT

1≤i≤n

where the ﬁrst inequality follows from (19b) and Thm. 11; the second inequality follows since
h(DS) and h(DS) are diagonal; the third follows from (19d); the fourth from another application of
Thm. 11; while the ﬁnal equality is via (19a). This proves nonexpansivity. If in addition h is weakly
log-contractive and S (cid:54)= R  then the second inequality above is strict  that is 
δT (G(S) G(R)) < δT (S  R) ∀S  R and S (cid:54)= R.

Consequently  we obtain the following main convergence theorem for (17).
Theorem 13. If G is weakly contractive and (16) has a solution  then this solution is unique and
iteration (17) converges to it.

When h is merely LN (not wlc)  it is still possible to show uniqueness of (16) up to a constant. Our
proof depends on the following new property of δT   which again should be of broader interest.
Theorem 14. Let G be nonexpansive in the δT metric  that is δT (G(X) G(Y )) ≤ δT (X  Y )  and F
be weakly contractive  that is δT (F(X) F(Y )) < δT (X  Y )  then G + F is also weakly contractive.
Observe that the property proved in Thm. 14 is a striking feature of the nonpositive curvature of
Pd; clearly  such a result does not usually hold in Banach spaces. As a consequence  Thm. 14 helps
establish the following “robustness” result for iteration (17).
Theorem 15. If h is LN  and S1 (cid:54)= S2 are solutions to the nonlinear equation (16)  then iteration
(17) converges to a solution  and S1 ∝ S2.
As an illustrative example of these results  consider the problem of ﬁnding the minimum of negative
log-likelihood solution of Kotz type distribution. The convergence of the iterative algorithm in (17)
can be obtained from Thm. 15. But for the Kotz distribution we can show a stronger result  which
helps obtain geometric convergence rates for the ﬁxed-point iteration.
Lemma 16. If c > 0 and −1 < b < 1  the function h(x) = x + cxb is weakly log-contractive.

2 is wlc. Based on Thm. 9  K(S) has a minimum. Therefore  we have the following.

According to this lemma  h in the iterative algorithm 16 for the Kotz-type distributions with 0 < β < 2
and α < d
Corollary 17. The iterative algorithm (16) for the Kotz-type distribution with 0 < β < 2 and α < d
2
converges to a unique ﬁxed point.

4 Numerical results
We brieﬂy highlight the numerical performance of our ﬁxed-point iteration. The key message here
is that our ﬁxed-point iterations solve nonconvex likelihood maximisation problems that involve a
complicating hpd constraint. But since the ﬁxed-point iterations always generate hpd iterates  no
extra eigenvalue computation is needed  which leads to substantial computational advantages. In
contrast  a nonlinear solver must perform constrained optimisation  which can be unduly expensive.

7

Figure 1: Running times comparison of the ﬁxed-point iteration compared with MATLAB’s fmincon to
maximise a Kotz-likelihood (see text for details). The plots show (from left to right)  running times for estimating
S ∈ Pd  for d ∈ {4  16  32}. Larger d was not tried because fmincon does not scale.

Figure 2: In the Kotz-type distribution  when β gets close to zero or 2  the contraction factor becomes smaller
which could impact the convergence rate. This ﬁgure shows running time variance for Kotz-type distributions
with ﬁxed d = 16  and α = 2  for different values of β: β = 0.1  β = 1  β = 1.7.

We show two short experiments (Figs. 1 and 2) showing scalability of the ﬁxed-point iteration with
increasing dimensionality of the input matrix  and for varying β parameter of the Kotz distribution; this
parameter inﬂuences the convergence rate of the ﬁxed-point iteration. For three different dimensions
d = 4  d = 16  and d = 32  we sample 10 000 datapoints from a Kotz-type distribution with
β = 0.5  α = 2  and a random covariance matrix. The convergence speed is shown as blue curves
in Figure 1. For comparison  the result of constrained optimisation (red curves) using MATLAB’S
optimisation toolbox are shown. The ﬁxed-point algorithm clearly outperforms MATLAB’S toolbox 
especially as dimensionality increases. These results indicate that the ﬁxed-point approach can be very
competitive. Also note that the problems are nonconvex with an open constraint set—this precludes
direct application simple approaches such as gradient-projection (since projection requires closed
sets; moreover  projection also requires eigenvector decompositions). Additional comparisons in the
longer version [28] show that the ﬁxed-point iteration also signiﬁcantly outperforms sophisticated
manifold optimisation techniques [5]  especially for increasing data dimensionality.
5 Conclusion
We developed geometric optimisation for minimising potentially nonconvex functions over the set of
positive deﬁnite matrices. We showed key results that help recognise geodesic convexity; we also
introduced the class of log-nonexpansive functions that contains functions that need not be g-convex 
but can still be optimised efﬁciently. Key to our ideas here was a careful construction of ﬁxed-point
iterations in a suitably chosen metric space. We motivated  developed  and applied our results to
the task of maximum likelihood estimation for various elliptically contoured distributions  covering
classes and examples substantially beyond what had been known so far in the literature. We believe
that the general geometric optimisation techniques that we developed in this paper will prove to be of
wider use and interest beyond our motivating application. Developing a more extensive geometric
optimisation numerical package is part of our ongoing project.

References
[1] R. Bhatia. Positive Deﬁnite Matrices. Princeton University Press  2007.
[2] R. Bhatia and R. L. Karandikar. The matrix geometric mean. Technical Report isid/ms/2-11/02  Indian

Statistical Institute  2011.

[3] D. A. Bini and B. Iannazzo. Computing the karcher mean of symmetric positive deﬁnite matrices. Linear

Algebra and its Applications  438(4):1700 – 1710  2013.

8

−1.9 −1.52−1.14−0.76−0.38  0   −5  −3.18−1.360.46 2.28  4.1 log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon−1.4 −0.84−0.280.28 0.84  1.4  −5  −2.97−0.961.06 3.08  5.1 log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon−1.3 −0.460.38 1.22 2.06  2.9  −5  −2.89−0.79 1.3 3.41  5.5 log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon−1.4 −0.640.12 0.88 1.64  2.4 −5−3−11 3 5 log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon−1.4 −0.84−0.280.28 0.84  1.4  −5  −2.94−0.871.19 3.24  5.3 log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon−1.3 −0.72−0.140.44 1.02  1.6  −5  −2.8 −0.59 1.6 3.81   6  log Running time (seconds) log Φ(S)−Φ(Smin) fixed−pointfmincon[4] G. Blekherman and P. A. Parrilo  editors. Semideﬁnite Optimization and Convex Algebraic Geometry.

[5] N. Boumal  B. Mishra  P.-A. Absil  and R. Sepulchre. Manopt: a matlab toolbox for optimization on

[6] S. Boyd  S.-J. Kim  L. Vandenberghe  and A. Hassibi. A Tutorial on Geometric Programming. Optimization

SIAM  2013.

manifolds. arXiv Preprint 1308.5200  2013.

and Engineering  8(1):67–127  2007.

[7] M. R. Bridson and A. Haeﬂinger. Metric Spaces of Non-Positive Curvature. Springer  1999.
[8] S. Cambanis  S. Huang  and G. Simons. On the theory of elliptically contoured distributions. Journal of

Multivariate Analysis  11(3):368–385  1981.

[9] Y. Chen  A. Wiesel  and A. Hero. Robust shrinkage estimation of high-dimensional covariance matrices.

IEEE Transactions on Signal Processing  59(9):4097–4107  2011.

[10] G. Cheng and B. Vemuri. A novel dynamic system in the space of spd matrices with applications to

appearance tracking. SIAM Journal on Imaging Sciences  6(1):592–615  2013.

[11] G. Cheng  H. Salehian  and B. C. Vemuri. Efﬁcient Recursive Algorithms for Computing the Mean
Diffusion Tensor and Applications to DTI Segmentation. In European Conference on Computer Vision
(ECCV)  volume 7  pages 390–401  2012.

[12] A. Cherian  S. Sra  A. Banerjee  and N. Papanikolopoulos. Jensen-Bregman LogDet Divergence for

Efﬁcient Similarity Computations on Positive Deﬁnite Tensors. IEEE TPAMI  2012.

[13] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chapman and Hall/CRC  1999.
[14] L. Gurvits and A. Samorodnitsky. A deterministic algorithm for approximating mixed discriminant and

mixed volume  and a combinatorial corollary. Disc. Comp. Geom.  27(4)  2002.

[15] S. K. K.-T. Fang and K. W. Ng. Symmetric multivariate and related distributions. Chapman & Hall  1990.
[16] J. T. Kent and D. E. Tyler. Redescending M-estimates of multivariate location and scatter. The Annals of

[17] H. Lee and Y. Lim. Invariant metrics  contractions and nonlinear matrix equations. Nonlinearity  21:

Statistics  19(4):2102–2119  Dec. 1991.

857–878  2008.

[18] B. Lemmens and R. Nussbaum. Nonlinear Perron-Frobenius Theory. Cambridge Univ. Press  2012.
[19] Y. Lim and M. P´alﬁa. Matrix power means and the Karcher mean. J. Functional Analysis  262:1498–1514 

[20] R. J. Muirhead. Aspects of multivariate statistical theory. John-Wiley  1982.
[21] Y. Nesterov and A. S. Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM 

[22] F. Nielsen and R. Bhatia  editors. Matrix Information Geometry. Springer  2013.
[23] E. Ollila  D. Tyler  V. Koivunen  and H. V. Poor. Complex elliptically symmetric distributions: Survey 

new results and applications. IEEE Transactions on Signal Processing  60(11):5597–5625  2011.
[24] A. Papadopoulos. Metric spaces  convexity and nonpositive curvature. Europ. Math. Soc.  2005.
[25] T. Rapcs´ak. Geodesic convexity in nonlinear optimization. J. Optim. Theory and Appl.  69(1):169–183 

2012.

1994.

1991.

[26] R. T. Rockafellar and R. J.-B. Wets. Variational analysis. Springer  1998.
[27] S. Sra. Positive Deﬁnite Matrices and the Symmetric Stein Divergence. arXiv:1110.1773  Oct. 2012.
[28] S. Sra and R. Hosseini. Conic geometric optimisation on the manifold of positive deﬁnite matrices. arXiv

[29] A. Wiesel. Geodesic convexity and covariance estimation. IEEE Transactions on Signal Processing  60

preprint  2013.

(12):6182–89  2012.

[30] T. Zhang  A. Wiesel  and S. Greco. Multivariate generalized gaussian distribution: Convexity and graphical

models. arXiv preprint arXiv:1304.3206  60(11):5597–5625  Nov. 2013.

[31] H. Zhu  H. Zhang  J. Ibrahim  and B. Peterson. Statistical analysis of diffusion tensors in diffusion-weighted
magnetic resonance imaging data. Journal of the American Statistical Association  102(480):1085–1102 
2007.

9

,Suvrit Sra
Reshad Hosseini
Siqi Nie
Denis Maua
Cassio de Campos
Qiang Ji
Feng Nan
Joseph Wang
Venkatesh Saligrama