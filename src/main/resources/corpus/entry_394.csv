2008,Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models,Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First  we show that neither idea is adequate. Second  we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory.,Adaptive Forward-Backward Greedy Algorithm for

Sparse Learning with Linear Models

Tong Zhang

Statistics Department
Rutgers University  NJ

tzhang@stat.rutgers.edu

Abstract

Consider linear prediction models where the target function is a sparse linear com-
bination of a set of basis functions. We are interested in the problem of identifying
those basis functions with non-zero coefﬁcients and reconstructing the target func-
tion from noisy observations. Two heuristics that are widely used in practice are
forward and backward greedy algorithms. First  we show that neither idea is ad-
equate. Second  we propose a novel combination that is based on the forward
greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We
prove strong theoretical results showing that this procedure is effective in learning
sparse representations. Experimental results support our theory.

1 Introduction
Consider a set of input vectors x1  . . .   xn ∈ Rd  with corresponding desired output variables
y1  . . .   yn. The task of supervised learning is to estimate the functional relationship y ≈ f(x)
between the input x and the output variable y from the training examples {(x1  y1)  . . .   (xn  yn)}.
The quality of prediction is often measured through a loss function φ(f(x)  y). In this paper  we
consider linear prediction model f(x) = wT x. As in boosting or kernel methods  nonlinearity can
be introduced by including nonlinear features in x.
We are interested in the scenario that d (cid:29) n. That is  there are many more features than the number
of samples. In this case  an unconstrained empirical risk minimization is inadequate because the
solution overﬁts the data. The standard remedy for this problem is to impose a constraint on w to
obtain a regularized problem. An important target constraint is sparsity  which corresponds to the
(non-convex) L0 regularization  where we deﬁne kwk0 = |{j : wj 6= 0}| = k. If we know the
sparsity parameter k  a good learning method is L0 regularization:

φ(wT xi  yi)

subject to kwk0 ≤ k.

(1)

ˆw = arg min
w∈Rd

1
n

nX

i=1

If k is not known  then one may regard k as a tuning parameter  which can be selected through cross-
validation. This method is often referred to as subset selection in the literature. Sparse learning is an
essential topic in machine learning  which has attracted considerable interests recently. Generally
speaking  one is interested in two closely related themes: feature selection  or identifying the basis
functions with non-zero coefﬁcients; estimation accuracy  or reconstructing the target function from
noisy observations. It can be shown that the solution of the L0 regularization problem in (1) achieves
good prediction accuracy if the target function can be approximated by a sparse ¯w.
It can also
solve the feature selection problem under extra identiﬁability assumptions. However  a fundamental
difﬁculty with this method is the computational cost  because the number of subsets of {1  . . .   d}
of cardinality k (corresponding to the nonzero components of w) is exponential in k. There are no
efﬁcient algorithms to solve the subset selection formulation (1).

1

Due to the computational difﬁcult  in practice  there are three standard methods for learning sparse
representations by solving approximations of (1). The ﬁrst approach is L1-regularization (Lasso).
The idea is to replace the L0 regularization in (1) by L1 regularization. It is the closest convex
approximation to (1). It is known that L1 regularization often leads to sparse solutions. Its perfor-
mance has been theoretically analyzed recently. For example  if the target is truly sparse  then it
was shown in [10] that under some restrictive conditions referred to as irrepresentable conditions 
L1 regularization solves the feature selection problem. The prediction performance of this method
has been considered in [6  2  1  9]. Despite its popularity  there are several problems with L1 regu-
larization: ﬁrst  the sparsity is not explicitly controlled  and good feature selection property requires
strong assumptions; second  in order to obtain very sparse solution  one has to use a large regulariza-
tion parameter that leads to suboptimal prediction accuracy because the L1 penalty not only shrinks
irrelevant features to zero  but also shrinks relevant features to zero. A sub-optimal remedy is to
threshold the resulting coefﬁcients; however this requires additional tuning parameters  making the
resulting procedures more complex and less robust. The second approach to approximately solve
the subset selection problem is forward greedy algorithm  which we will describe in details in Sec-
tion 2. The method has been widely used by practitioners. The third approach is backward greedy
algorithm. Although this method is widely used by practitioners  there isn’t any theoretical analysis
when n (cid:28) d (which is the case we are interested in here). The reason will be discussed later.
In this paper  we are particularly interested in greedy algorithms because they have been widely
used but the effectiveness has not been well analyzed. As we shall explain later  neither the standard
forward greedy idea nor th standard backward greedy idea is adequate for our purpose. However 
the ﬂaws of these methods can be ﬁxed by a simple combination of the two ideas. This leads to a
novel adaptive forward-backward greedy algorithm which we present in Section 3. The general idea
works for all loss functions. For least squares loss  we obtain strong theoretical results showing that
the method can solve the feature selection problem under moderate conditions.
For clarity  this paper only considers the ﬁxed design formulation. To simplify notations in our
description  we will replace the optimization problem in (1) with a more general formulation. In-
stead of working with n input data vectors xi ∈ Rd  we work with d feature vectors fj ∈ Rn
(j = 1  . . .   d)  and y ∈ Rn. Each fj corresponds to the j-th feature component of xi for
i = 1  . . .   n. That is  fj i = xi j. Using this notation  we can generally rewrite (1) with in the
form ˆw = arg minw∈Rd R(w) subject to kwk0 ≤ k  where weight w = [w1  . . .   wd] ∈ Rd 
and R(w) is a real-valued cost function which we are interested in optimization. For least squares
2. In the following  we also let ej ∈ Rd be the
vector of zeros  except for the j-component which is one. For convenience  we also introduce the
following notations.
Deﬁnition 1.1 Deﬁne supp(w) = {j : wj 6= 0} as the set of nonzero coefﬁcients of a vector
w = [w1  . . .   wd] ∈ Rd. For a weight vector w ∈ Rd  we deﬁne mapping f : Rd → Rn as:
j=1 wjfj. Given f ∈ Rd and F ⊂ {1  . . .   d}  let ˆw(F  f) = minw∈Rd kf(w) −
fk2
subject to supp(w) ⊂ F   and let ˆw(F ) = ˆw(F  y) be the solution of the least squares
problem using features F .

regression  we have R(w) = n−1kP

j wjfj − yk2

f(w) = Pd

2

2 Forward and Backward Greedy Algorithms

Forward greedy algorithms have been widely used in applications. The basic algorithm is presented
in Figure 1. Although a number of variations exist  they all share the basic form of greedily picking
an additional feature at every step to aggressively reduce the cost function. The intention is to make
most signiﬁcant progress at each step in order to achieve sparsity. In this regard  the method can be
considered as an approximation algorithm for solving (1).
A major ﬂaw of this method is that it can never correct mistakes made in earlier steps. As an
illustration  we consider the situation plotted in Figure 2 with least squares regression. In the ﬁgure 
y can be expressed as a linear combination of f1 and f2 but f3 is closer to y. Therefore using the
forward greedy algorithm  we will ﬁnd f3 ﬁrst  then f1 and f2. At this point  we have already found
all good features as y can be expressed by f1 and f2  but we are not able to remove f3 selected
in the ﬁrst step. The above argument implies that forward greedy method is inadequate for feature
selection. The method only works when small subsets of the basis functions {fj} are near orthogonal

2

Input: f1  . . .   fd  y ∈ Rn and  > 0
Output: F (k) and w(k)
let F (0) = ∅ and w(0) = 0
for k = 1  2  . . .
let i(k) = arg mini minα R(w(k−1) + αei)
let F (k) = {i(k)} ∪ F (k−1)
let w(k) = ˆw(F (k))
if (R(w(k−1)) − R(w(k)) ≤ ) break
end

Figure 1: Forward Greedy Algorithm

Figure 2: Failure of Forward Greedy Algorithm

[7]. In general  Figure 2 (which is the case we are interested in in this paper) shows that forward
greedy algorithm will make errors that are not corrected later on.
In order to remedy the problem  the so-called backward greedy algorithm has been widely used by
practitioners. The idea is to train a full model with all the features  and greedily remove one feature
(with the smallest increase of cost function) at a time. Although at the ﬁrst sight  backward greedy
method appears to be a reasonable idea that addresses the problem of forward greedy algorithm  it is
computationally very costly because it starts with a full model with all features. Moreover  there are
no theoretical results showing that this procedure is effective. In fact  under our setting  the method
may only work when d (cid:28) n (see  for example  [3])  which is not the case we are interested in. In
the case d (cid:29) n  during the ﬁrst step  we start with a model with all features  which can immediately
overﬁt the data with perfect prediction. In this case  the method has no ability to tell which feature
is irrelevant and which feature is relevant because removing any feature still completely overﬁts
the data. Therefore the method will completely fail when d (cid:29) n  which explains why there is no
theoretical result for this method.

3 Adaptive Forward-Backward Greedy Algorithm

The main strength of forward greedy algorithm is that it always works with a sparse solution ex-
plicitly  and thus computationally efﬁcient. Moreover  it does not signiﬁcantly overﬁt the data due
to the explicit sparsity. However  a major problem is its inability to correct any error made by the
algorithm. On the other hand  backward greedy steps can potentially correct such an error  but need
to start with a good model that does not completely overﬁt the data — it can only correct errors with
a small amount of overﬁtting. Therefore a combination of the two can solve the fundamental ﬂaws
of both methods. However  a key design issue is how to implement a backward greedy strategy
that is provably effective. Some heuristics exist in the literature  although without any effectiveness
proof. For example  the standard heuristics  described in [5] and implemented in SAS  includes
another threshold 0 in addition to : a feature is deleted if the cost-function increase by performing
the deletion is no more than 0. Unfortunately we cannot provide an effectiveness proof for this
heuristics: if the threshold 0 is too small  then it cannot delete any spurious features introduced in
the forward steps; if it is too large  then one cannot make progress because good features are also
deleted. In practice it can be hard to pick a good 0  and even the best choice may be ineffective.

3

f5yf1f2f3f4This paper takes a more principled approach  where we speciﬁcally design a forward-backward
greedy procedure with adaptive backward steps that are carried out automatically. The procedure
has provably good performance and ﬁxes the drawbacks of forward greedy algorithm illustrated in
Figure 2. There are two main considerations in our approach: we want to take reasonably aggressive
backward steps to remove any errors caused by earlier forward steps  and to avoid maintaining a
large number of basis functions; we want to take backward step adaptively and make sure that any
backward greedy step does not erase the gain made in the forward steps. Our algorithm  which we
refer to as FoBa  is listed in Figure 3. It is designed to balance the above two aspects. Note that we
only take a backward step when the increase of cost function is no more than half of the decrease of
cost function in earlier forward steps. This implies that if we take ‘ forward steps  then no matter
how many backward steps are performed  the cost function is decreased by at least an amount of
‘/2. It follows that if R(w) ≥ 0 for all w ∈ Rd  then the algorithm terminates after no more than
2R(0)/ steps. This means that the procedure is computationally efﬁcient.

Input: f1  . . .   fd  y ∈ Rn and  > 0
Output: F (k) and w(k)
let F (0) = ∅ and w(0) = 0
let k = 0
while true

let k = k + 1
// forward step
let i(k) = arg mini minα R(w(k−1) + αei)
let F (k) = {i(k)} ∪ F (k−1)
let w(k) = ˆw(F (k))
let δ(k) = R(w(k−1)) − R(w(k))
if (δ(k) ≤ )
k = k − 1
break

endif
// backward step (can be performed after each few forward steps)
while true
let j(k) = arg minj∈F (k) R(w(k) − w(k)
j ej)
let δ0 = R(w(k) − w(k)
j(k)ej(k)) − R(w(k))
if (δ0 > 0.5δ(k)) break
let k = k − 1
let F (k) = F (k+1) − {j(k+1)}
let w(k) = ˆw(F (k))

end

end

Figure 3: FoBa: Forward-Backward Greedy Algorithm

Now  consider an application of FoBa to the example in Figure 2. Again  in the ﬁrst three forward
steps  we will be able to pick f3  followed by f1 and f2. After the third step  since we are able
to express y using f1 and f2 only  by removing f3 in the backward step  we do not increase the
cost. Therefore at this stage  we are able to successfully remove the incorrect basis f3 while keeping
the good features f1 and f2. This simple illustration demonstrates the effectiveness of FoBa. In
the following  we formally characterize this intuitive example  and prove the effectiveness of FoBa
for feature selection as well as parameter estimation. Our analysis assumes the least squares loss.
However  it is possible to handle more general loss functions with a more complicated derivation.
We introduce the following deﬁnition  which characterizes how linearly independent small subsets
of {fj} of size k are. For k (cid:28) n  the number ρ(k) can be bounded away from zero even when
d (cid:29) n. For example  for random basis functions fj  we may take ln d = O(n/k) and still have ρ(k)
to be bounded away from zero. This quantity is the smallest eigenvalue of the k × k diagonal blocks
of the d × d design matrix [f T
i fj]i j=1 ... d  and has appeared in recent analysis of L1 regularization

4

2/kwk2

nkf(w)k2
nkf(w(k)) − yk2

methods such as in [2  8]  etc. We shall refer it to as the sparse eigenvalue condition. This condition
is the least restrictive condition when compared to other conditions in the literature [1].

Deﬁnition 3.1 Deﬁne for all 1 ≤ k ≤ d: ρ(k) = inf(cid:8) 1

2 : kwk0 ≤ k(cid:9).

nkfjk2

Assumption 3.1 Consider least squares loss R(w) = 1
2. Assume that the basis
2 = 1 for all j = 1  . . .   d  and assume that {yi}i=1 ... n
functions are normalized such that 1
are independent (but not necessarily identically distributed) sub-Gaussians: there exists σ ≥ 0 such
that ∀i and ∀t ∈ R  Eyiet(yi−Eyi) ≤ eσ2t2/2.
Both Gaussian and bounded random variables are sub-Gaussian using the above deﬁnition. For
If a random variable ξ ∈ [a  b]  then
example  we have the following Hoeffding’s inequality.
Eξet(ξ−Eξ) ≤ e(b−a)2t2/8. If a random variable is Gaussian: ξ ∼ N(0  σ2)  then Eξetξ ≤ eσ2t2/2.
The following theorem is stated with an explicit  for convenience. In applications  one can always
run the algorithm with a smaller  and use cross-validation to determine the optimal stopping point.

Theorem 3.1 Consider the FoBa algorithm in Figure 3  where Assumption 3.1 holds. Assume also
that the target is sparse: there exists ¯w ∈ Rd such that ¯wT xi = Eyi for i = 1  . . .   n  and
¯F = supp( ¯w). Let ¯k = | ¯F|  and  > 0 be the stopping criterion in Figure 3. Let s ≤ d be an
integer which either equals d or satisﬁes the condition 8¯k ≤ sρ(s)2.
If minj∈supp( ¯w) | ¯wj|2 ≥
25 ρ(s)−2  and for some η ∈ (0  1/3)   ≥ 64ρ(s)−2σ2 ln(2d/η)/n  then with probability
larger than 1 − 3η  when the algorithm terminates  we have F (k) = ¯F and kw(k) − ¯wk2 ≤

64

σp¯k/(nρ(¯k))

h

i
1 +p20 ln(1/η)

.

The result shows that one can identify the correct set of features ¯F as long as the weights ¯wj are
not close to zero when j ∈ ¯F . This condition is necessary for all feature selection algorithms
including previous analysis of Lasso. The theorem can be applied as long as eigenvalues of small
s × s diagonal blocks of the design matrix [f T
i fj]i j=1 ... d are bounded away from zero (i.e.  sparse
eigenvalue condition). This is the situation under which the forward greedy step can make mistakes 
but such mistakes can be corrected using FoBa. Because the conditions of the theorem do not prevent
forward steps from making errors  the example described in Figure 2 indicates that it is not possible
to prove a similar result for the forward greedy algorithm. The result we proved is also better than
that of Lasso  which can successfully select features under irrepresentable conditions of [10]. It is
known that the sparse eigenvalue condition considered here is generally weaker [8  1].

Our result relies on the assumption that | ¯wj| (j ∈ ¯F ) is larger than the noise level O(σpln d/n) in

order to select features effectively. If any nonzero weight is below the noise level  then no algorithm
can distinguish it from zero with large probability. That is  in this case  one cannot reliably perform
feature selection due to the noise. Therefore FoBa is near optimal in term of its ability to perform
reliable feature selection  except for the constant hiding in O(·). For target that is not truly sparse 
similar results can be obtained. In this case  it is not possible to correctly identify all the features
with large probability. However  we can show that FoBa can still select part of the features reliably 
with good parameter estimation accuracy. Such results can be found in the full version of the paper 
available from the author’s website.

4 Experiments

We compare FoBa described in Section 3) to forward-greedy and L1-regularization on artiﬁcial
and real data. They show that in practice  FoBa is closer to subset selection than the other two
approaches  in the sense that FoBa achieves smaller training error given any sparsity level. In oder
to compare with Lasso  we use the LARS [4] package in R  which generates a path of actions for
adding and deleting features  along the L1 solution path. For example  a path of {1  3  5 −3  . . .}
means that in the ﬁst three steps  feature 1  3  5 are added; and the next step removes feature 3.
Using such a solution path  we can compare Lasso to Forward-greedy and FoBa under the same
framework. Similar to the Lasso path  FoBa also generates a path with both addition and deletion
operations  while forward-greedy algorithm only adds features without deletion.

5

Our experiments compare the performance of the three algorithms using the corresponding feature
addition/deletion paths. We are interested in features selected by the three algorithms at any sparsity
level k  where k is the desired number of features presented in the ﬁnal solution. Given a path  we
can keep an active feature set by adding or deleting features along the path. For example  for path
{1  3  5 −3}  we have two potential active feature sets of size k = 2: {1  3} (after two steps) and
{1  5} (after four steps). We then deﬁne the k best features as the active feature set of size k with
the smallest least squares error because this is the best approximation to subset selection (along the
path generated by the algorithm). From the above discussion  we do not have to set  explicitly in
the FoBa procedure. Instead  we just generate a solution path which is ﬁve times as long as the
maximum desired sparsity k  and then generate the best k features for any sparsity level using the
above described procedure.

4.1 Simulation Data
Since for real data  we do not know the true feature set ¯F   simulation is needed to compare feature
selection performance. We generate n = 100 data points of dimension d = 500. The target vector
¯w is truly sparse with ¯k = 5 nonzero coefﬁcients generated uniformly from 0 to 10. The noise
level is σ2 = 0.1. The basis functions fj are randomly generated with moderate correlation: that
is  some basis functions are correlated to the basis functions spanning the true target. Note that
if there is no correlation (i.e.  fj are independent random vectors)  then both forward-greedy and
L1-regularization work well because the basis functions are near orthogonal (this is the well-known
case considered in the compressed sensing literature). Therefore in this experiment  we generate
moderate correlation so that the performance of the three methods can be differentiated. Such mod-
erate correlation does not violate the sparse eigenvalue condition in our analysis  but violates the
more restrictive conditions for forward-greedy method and Lasso.

least squares training error
parameter estimation error

feature selection error

FoBa

0.093 ± 0.02
0.057 ± 0.2
0.76 ± 0.98

Forward-greedy
0.16 ± 0.089
0.52 ± 0.82
1.8 ± 1.1

L1

0.25 ± 0.14
1.1 ± 1
3.2 ± 0.77

Table 1: Performance comparison on simulation data at sparsity level k = 5

Table 1 shows the performance of the three methods (including two versions of FoBa)  where we
repeat the experiments 50 times  and report the average ± standard-deviation. We use the three
methods to select ﬁve best features  using the procedure described above. We report three metrics.
Training error is the squared error of the least squares solution with the selected ﬁve features. Pa-
rameter estimation error is the 2-norm of the estimated parameter (with the ﬁve features) minus the
true parameter. Feature selection error is the number of incorrectly selected features. It is clear
from the table that for this data  FoBa achieves signiﬁcantly smaller training error than the other two
methods  which implies that it is closest to subset selection. Moreover  the parameter estimation
performance and feature selection performance are also better. The two versions of FoBa perform
very similarly for this data.

4.2 Real Data

Instead of listing results for many datasets without gaining much insights  we present a more detailed
study on a typical dataset  which reﬂect typical behaviors of the algorithms. Our study shows that
FoBa does what it is designed to do well: that is  it gives a better approximation to subset selection
than either forward-greedy or L1 regularization. Moreover  the difference between aggressive FoBa
and conservative FoBa become more signiﬁcant.
In this study  we use the standard Boston Housing data  which is the housing data for 506 cen-
sus tracts of Boston from the 1970 census  available from the UCI Machine Learning Database
Repository: http://archive.ics.uci.edu/ml/. Each census tract is a data-point  with 13 features (we
add a constant offset one as the 14th feature)  and the desired output is the housing price. In the
experiment  we randomly partition the data into 50 training plus 456 test points. We perform the
experiments 50 times  and for each sparsity level from 1 to 10  we report the average training and test
squared error. The results are plotted in Figure 4. From the results  we can see that FoBa achieves

6

better training error for any given sparsity  which is consistent with the theory and the design goal of
FoBa. Moreover  it achieves better test accuracy with small sparsity level (corresponding to a more
sparse solution). With large sparsity level (corresponding to a less sparse solution)  the test error
increase more quickly with FoBa. This is because it searches a larger space by more aggressively
mimic subset selection  which makes it more prone to overﬁtting. However  at the best sparsity level
of 2 or 3 (for aggressive and conservative FoBa  respectively)  FoBa achieves signiﬁcantly better test
error. Moreover  we can observe with small sparsity level (a more sparse solution)  L1 regularization
performs poorly  due to the bias caused by using a large L1-penalty.

Figure 4: Performance of the algorithms on Boston Housing data Left: average training squared
error versus sparsity; Right: average test squared error versus sparsity

For completeness  we also compare FoBa to the backward-greedy algorithm and the classical heuris-
tic forward-backward greedy algorithm as implemented in SAS (see its description at the beginning
of Section 3). We still use the Boston Housing data  but plot the results separately  in order to avoid
cluttering. As we have pointed out  there is no theory for the SAS version of forward-backward
greedy algorithm. It is difﬁcult to select an appropriate backward threshold 0: a too small value
leads to few backward steps  and a too large value leads to overly aggressive deletion  and the pro-
cedure terminates very early. In this experiment  we pick a value of 10  because it is a reasonably
large quantity that does not lead to an extremely quick termination of the procedure. The perfor-
mance of the algorithms are reported in Figure 5. From the results  we can see that backward greedy
algorithm performs reasonably well on this problem. Note that for this data  d (cid:28) n  which is the
scenario that backward does not start with a completely overﬁtted full model. Still  it is inferior to
FoBa at small sparsity level  which means that some degree of overﬁtting still occurs. Note that
backward-greedy algorithm cannot be applied in our simulation data experiment  because d (cid:29) n
which causes immediate overﬁtting. From the graph  we also see that FoBa is more effective than
the SAS implementation of forward-backward greedy algorithm. The latter does not perform signif-
icant better than the forward-greedy algorithm with our choice of 0. Unfortunately  using a larger
backward threshold 0 will lead to an undesirable early termination of the algorithm. This is why the
provably effective adaptive backward strategies introduced in this paper are superior.

5 Discussion

This paper investigates the problem of learning sparse representations using greedy algorithms. We
showed that neither forward greedy nor backward greedy algorithms are adequate by themselves.
However  through a novel combination of the two ideas  we showed that an adaptive forward-back
greedy algorithm  referred to as FoBa  can effectively solve the problem under reasonable condi-
tions. FoBa is designed to be a better approximation to subset selection. Under the sparse eigenvalue
condition  we obtained strong performance bounds for FoBa for feature selection and parameter es-
timation. In fact  to the author’s knowledge  in terms of sparsity  the bounds developed for FoBa in
this paper are superior to all earlier results in the literature for other methods.

7

llllllllll2468102030405060sparsitytraining errorlFoBaforward−greedyL1llllllllll2468103540455055606570sparsitytest errorlFoBaforward−greedyL1Figure 5: Performance of greedy algorithms on Boston Housing data. Left: average training squared
error versus sparsity; Right: average test squared error versus sparsity

Our experiments also showed that FoBa achieves its design goal: that is  it gives smaller training
error than either forward-greedy or L1 regularization for any given level of sparsity. Therefore the
experiments are consistent with our theory. In real data  better sparsity helps on some data such
as Boston Housing. However  we shall point out that while FoBa always achieves better training
error for a given sparsity in our experiments on other datasets (thus it achieves our design goal)  L1-
regularization some times achieves better test performance. This is not surprising because sparsity is
not always the best complexity measure for all problems. In particular  the prior knowledge of using
small weights  which is encoded in the L1 regularization formulation but not in greedy algorithms 
can lead to better generalization performance on some data (when such a prior is appropriate).

References
[1] Peter Bickel  Yaacov Ritov  and Alexandre Tsybakov. Simultaneous analysis of Lasso and

Dantzig selector. Annals of Statistics  2008. to appear.

[2] Florentina Bunea  Alexandre Tsybakov  and Marten H. Wegkamp. Sparsity oracle inequalities

for the Lasso. Electronic Journal of Statistics  1:169–194  2007.

[3] Christophe Couvreur and Yoram Bresler. On the optimality of the backward greedy algorithm

for the subset selection problem. SIAM J. Matrix Anal. Appl.  21(3):797–808  2000.

[4] Bradley Efron  Trevor Hastie  Iain Johnstone  and Robert Tibshirani. Least angle regression.

Annals of Statistics  32(2):407–499  2004.

[5] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer 

2001.

[6] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization. Annales de l’Institut

Henri Poincaré  2008.

[7] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Trans. Info.

Theory  50(10):2231–2242  2004.

[8] Cun-Hui Zhang and Jian Huang. Model-selection consistency of the Lasso in high-dimensional

linear regression. Technical report  Rutgers University  2006.

[9] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regulariza-

tion. The Annals of Statistics  2009. to appear.

[10] Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning

Research  7:2541–2567  2006.

8

llllllllll24681020304050sparsitytraining errorllllllllllllFoBaForward−Backward (SAS)forward−greedybackward−greedyllllllllll24681040506070sparsitytest errorllllllllllllFoBaForward−Backward (SAS)forward−greedybackward−greedy,Pablo Sprechmann
Roee Litman
Tal Ben Yakar
Alexander Bronstein
Guillermo Sapiro
Yuxin Chen
Emmanuel Candes