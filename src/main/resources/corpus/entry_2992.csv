2017,Learning ReLUs via Gradient Descent,In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $\vct{x}\mapsto \max(0 \langle \vct{w} \vct{x}\rangle)$ with $\vct{w}\in\R^d$ denoting the weight vector.  We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent  when initialized at $\vct{0}$  converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.,Learning ReLUs via Gradient Descent

Ming Hsieh Department of Electrical Engineering

University of Southern California

Mahdi Soltanolkotabi

Los Angeles  CA

soltanol@usc.edu

Abstract

which are functions of the form x max(0 w  x) with wâˆˆ Rd denoting the

In this paper we study the problem of learning Rectiï¬ed Linear Units (ReLUs)

weight vector. We study this problem in the high-dimensional regime where the
number of observations are fewer than the dimension of the weight vector. We
assume that the weight vector belongs to some closed set (convex or nonconvex)
which captures known side-information about its structure. We focus on the
realizable model where the inputs are chosen i.i.d. from a Gaussian distribution
and the labels are generated according to a planted weight vector. We show that
projected gradient descent  when initialized at 0  converges at a linear rate to the
planted model with a number of samples that is optimal up to numerical constants.
Our results on the dynamics of convergence of these very shallow neural nets may
provide some insights towards understanding the dynamics of deeper architectures.

1

Introduction

Nonlinear data-ï¬tting problems are fundamental to many supervised learning tasks in signal process-

ing and machine learning. Given training data consisting of n pairs of input features xiâˆˆ Rd and
desired outputs yiâˆˆ R we wish to infer a function that best explains the training data. In this paper
we focus on ï¬tting Rectiï¬ed Linear Units (ReLUs) to the data which are functions Ï†wâˆ¶ Rdâ†’ R of

the form

Ï†w(x)= max(0 w  x) .
(max(0 w  xi)âˆ’ yi)2

subject to R(w)â‰¤ R 

wâˆˆRd L(w)âˆ¶= 1

min

nQ
i=1

A natural approach to ï¬tting ReLUs to data is via minimizing the least-squares misï¬t aggregated over
the data. This optimization problem takes the form

(1.1)

n

withRâˆ¶ Rdâ†’ R denoting a regularization function that encodes prior information on the weight

vector.
Fitting nonlinear models such as ReLUs have a rich history in statistics and learning theory [12]
with interesting new developments emerging [6] (we shall discuss all these results in greater detail in
Section 5). Most recently  nonlinear data ï¬tting problems in the form of neural networks (a.k.a. deep
learning) have emerged as powerful tools for automatically extracting interpretable and actionable
information from raw forms of data  leading to striking breakthroughs in a multitude of applications
[13  15  4]. In these and many other empirical domains it is common to use local search heuristics
such as gradient or stochastic gradient descent for nonlinear data ï¬tting. These local search heuristics
are surprisingly effective on real or randomly generated data. However  despite their empirical success
the reasons for their effectiveness remains mysterious.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Focusing on ï¬tting ReLUs  a-priori it is completely unclear why local search heuristics such as
gradient descent should converge for problems of the form (1.1)  as not only the regularization
function maybe nonconvex but also the loss function! Efï¬cient ï¬tting of ReLUs in this high-
dimensional setting poses new challenges: When are the iterates able to escape local optima and
saddle points and converge to global optima? How many samples do we need? How does the number
of samples depend on the a-priori prior knowledge available about the weights? What regularizer
is best suited to utilizing a particular form of prior knowledge? How many passes (or iterations) of
the algorithm is required to get to an accurate solution? At the heart of answering these questions is
the ability to predict convergence behavior/rate of (non)convex constrained optimization algorithms.
In this paper we build up on a new framework developed in the context of phase retrieval [21] for
analyzing nonconvex optimization problems to address such challenges.

2 Precise measures for statistical resources

âˆ—+ h)â‰¤R(w

âˆ—).

âˆ—)=hâˆ¶ R(w

We wish to characterize the rates of convergence for the projected gradient updates (3.2) as a function
of the number of samples  the available prior knowledge and the choice of the regularizer. To make
these connections precise and quantitative we need a few deï¬nitions. Naturally the required number

properties of the weight vector w. For example  if we know that the weight vector is approximately
sparse  naturally using an (cid:96)1 norm for the regularizer is superior to using an (cid:96)2 regularizer. To quantify
this capability we ï¬rst need a couple of standard deï¬nitions which we adapt from [17  18  21].

of samples for reliable data ï¬tting depends on how well the regularization functionR can capture the
Deï¬nition 2.1 (Descent set and cone) The set of descent of a functionR at a point wâˆ— is deï¬ned as
DR(w
The cone of descent is deï¬ned as a closed coneCR(wâˆ—) that contains the descent set  i.e.DR(wâˆ—)âŠ‚
CR(wâˆ—). The tangent cone is the conic hull of the descent set. That is  the smallest closed cone
CR(wâˆ—) obeyingDR(wâˆ—)âŠ‚CR(wâˆ—).
We note that the capability of the regularizerR in capturing the properties of the unknown weight
vector wâˆ— depends on the size of the descent coneCR(wâˆ—). The smaller this cone is the more suited
the functionR is at capturing the properties of wâˆ—. To quantify the size of this set we shall use the
Deï¬nition 2.2 (Gaussian width) The Gaussian width of a setCâˆˆ Rd is deï¬ned as:
where the expectation is taken over gâˆ¼N(0  Ip). Throughout we useBd~Sdâˆ’1 to denote the the unit
We now have all the deï¬nitions in place to quantify the capability of the functionR in capturing the
properties of the unknown parameter wâˆ—. This naturally leads us to the deï¬nition of the minimum
Deï¬nition 2.3 (minimal number of samples) LetCR(wâˆ—) be a cone of descent ofR at wâˆ—. We
deï¬ne the minimal sample function asM(R  w
We shall often use the short hand n0=M(R  wâˆ—) with the dependence onR  wâˆ— implied.

Ï‰(C)âˆ¶= Eg[sup

zâˆˆCg  z] 

âˆ—)= Ï‰2(CR(w

âˆ—)âˆ©Bd).

required number of samples.

notion of mean width.

ball/sphere of Rd.

We note that n0 is exactly the minimum number of samples required for structured signal recovery
from linear measurements when using convex regularizers [3  1]. Speciï¬cally  the optimization
problem

(yrâˆ’xi  w

âˆ—)2

subject to R(w)â‰¤R(w

âˆ—) 

nQ
i=1

(2.1)

2

succeeds at recovering an unknown weight vector wâˆ— with high probability from n observations of
the form yi=ai  wâˆ— if and only if nâ‰¥ n0.1 While this result is only known to be true for convex
compared to linear observations) we can not hope to recover the weight vector from nâ‰¤ n0 when

regularization functions we believe that n0 also characterizes the minimal number of samples even for
nonconvex regularizers in (2.1). See [17] for some results in the nonconvex case as well as the role
this quantity plays in the computational complexity of projected gradient schemes for linear inverse
problems. Given that with nonlinear samples we have less information (we loose some information

using (1.1). Therefore  we can use n0 as a lower-bound on the minimum number of observations
required for projected gradient descent iterations (3.2) to succeed at ï¬nding the right model.

3 Theoretical results for learning ReLUs

A simple heuristic for optimizing (1.1) is to use gradient descent. One challenging aspect of the
above loss function is that it is not differentiable and it is not clear how to run projected gradient
descent. However  this does not pose a fundamental challenge as the loss function is differentiable
except for isolated points and we can use the notion of generalized gradients to deï¬ne the gradient at
a non-differentiable point as one of the limit points of the gradient in a local neighborhood of the
non-differentiable point. For the loss in (1.1) the generalized gradient takes the form

âˆ‡L(w)âˆ¶= 1

n

(ReLU(w  xi)âˆ’ yi)(1+ sgn(w  xi)) xi.
nQ
i=1
wÏ„+1=PK(wÏ„âˆ’ ÂµÏ„âˆ‡L(wÏ„))  

Therefore  projected gradient descent takes the form

(3.2)

Euclidean projection onto this set.

where ÂµÏ„ is the step size andK={wâˆˆ Rdâˆ¶R(w)â‰¤ R} is the constraint set withPK denoting the
Theorem 3.1 Let wâˆ— âˆˆ Rd be an arbitrary weight vector andRâˆ¶ Rd â†’ R be a proper function
(convex or nonconvex). Suppose the feature vectors xi âˆˆ Rd are i.i.d. Gaussian random vectors
distributed asN(0  I) with the corresponding labels given by
yi= max(0 xi  w
âˆ—) .
To estimate wâˆ—  we start from the initial point w0= 0 and apply the Projected Gradient Descent
wÏ„+1=PK(wÏ„âˆ’ ÂµÏ„âˆ‡L(wÏ„))  
withKâˆ¶={wâˆˆ Rdâˆ¶ R(w)â‰¤R(wâˆ—)} andâˆ‡L deï¬ned via (3.1). Also set the learning parameter
sequence to Âµ0= 2 and ÂµÏ„ = 1 for all Ï„= 1  2  . . . and let n0=M(R  wâˆ—)  per Deï¬nition 2.3  be
holds for a ï¬xed numerical constant c. Then there is an event of probability at least 1âˆ’ 9eâˆ’Î³n such

our lower bound on the number of observations. Also assume

(PGD) updates of the form

(3.4)

(3.3)

that on this event the updates (3.3) obey

(3.1)

n> cn0 
âˆ—(cid:96)2â‰¤ 1

wÏ„âˆ’ w

2Ï„w

âˆ—(cid:96)2

.

(3.5)

Here Î³ is a ï¬xed numerical constant.

The ï¬rst interesting and perhaps surprising aspect of this result is its generality: it applies not only to
convex regularization functions but also nonconvex ones! As we mentioned earlier the optimization
problem in (1.1) is not known to be tractable even for convex regularizers. Despite the nonconvexity
of both the objective and regularizer  the theorem above shows that with a near minimal number

more precise characterization is Ï†âˆ’1(Ï‰2(CR(wâˆ—)âˆ©Bd))â‰ˆ Ï‰2(CR(wâˆ—)âˆ©Bd) where Ï†(t)=âˆš

1We would like to note that n0 only approximately characterizes the minimum number of samples required. A
t.

2 â‰ˆâˆš
2 
Î“ t+1
Î“ t

However  since our results have unspeciï¬ed constants we avoid this more accurate characterization.

2

3

average behavior over 100 trials. None bold color depict the estimation error of some sample trials.

number of iterations Ï„. The plots are for two different observations models: 1) ReLU observations of

Figure 1: Estimation error (wÏ„âˆ’ wâˆ—(cid:96)2) obtained via running PGD iterates as a function of the
the form y=ReLU(Xwâˆ—) and 2) linear observations of the form y= Xwâˆ—. The bold colors depict
of data samples  projected gradient descent provably learns the original weight vector wâˆ— without
achieve a relative error of  the total number of iterations is on the order ofO(log(1~)). Thus the
overall computational complexity is on the order ofO(nd log(1~)) (in general the cost is the total
one matrix-vector multiplication which takesO(nd) time.

number of iterations multiplied by the cost of applying the feature matrix X and its transpose). As
a result  the computational complexity is also now optimal in terms of dependence on the matrix
dimensions. Indeed  for a dense matrix even verifying that a good solution has been achieved requires

getting trapped in any local optima.
Another interesting aspect of the above result is that the convergence rate is linear. Therefore  to

4 Numerical experiments

In this section we carry out a simple numerical experiment to corroborate our theoretical results. For

this purpose we generate a unit norm sparse vector wâˆ—âˆˆ Rd of dimension d= 1000 containing s=
d~50 non-zero entries. We also generate a random feature matrix Xâˆˆ RnÃ—d with n=Ã¦8s log(d~s)Ã¦
and containing i.i.d.N(0  1) entries. We now take two sets of observations of size n from Î¸âˆ—:
We apply the projected gradient iterations to both observation models starting from w0= 0. For the

â€¢ ReLU observations: the response vector is equal to y=ReLU(Xwâˆ—).
â€¢ Linear observations: the response is y= Xwâˆ—.

ReLU observations we use the step size discussed in Theorem 3.1. For the linear model we apply
projected gradient descent updates of the form

wÏ„+1=PKwÏ„âˆ’ 1

X T(XwÏ„âˆ’ y) .

In both cases we use the regularizerR(w)=w(cid:96)0 so that the projection only keeps the top s
(wÏ„âˆ’ wâˆ—(cid:96)2) is depicted as a function of the number of iterations Ï„. The bold colors depict average

entries of the vector (a.k.a. iterative hard thresholding). In Figure 1 the resulting estimation errors

behavior over 100 trials. The estimation error of some sample trials are also depicted in none bold

n

4

0510152000.20.40.60.81Estimation errorReLU samplesLinear samplescolors. This plot clearly show that PGD iterates applied to ReLU observations converge quickly
to the ground truth. This ï¬gure also clearly demonstrates that the behavior of the PGD iterates
applied to both models are similar  further corroborating the results of Theorem 3.1. We note that

the sample complexity used in this simulation is 8s log(n~s) which is a constant factor away from
n0âˆ s log(n~s) conï¬rming our assertion that the required sample complexity is a constant factor

away from n0 (as predicted by Theorem 3.1).

5 Discussions and prior art

There is a large body of work on learning nonlinear models. A particular class of such problems
that have been studied are the so called idealized Single Index Models (SIMs) [9  10]. In these

problems the inputs are labeled examples{(xi  yi)}n
i=1âˆˆ RdÃ— R which are guaranteed to satisfy
yi= f(w  xi) for some wâˆˆ Rd and nondecreasing (Lipchitz continuous) fâˆ¶ Râ†’ R. The goal in

this problem is to ï¬nd a (nearly) accurate such f and w. An interesting polynomial-time algorithm
called the Isotron exists for this problem [12  11]. In principle  this approach can also be used to
ï¬t ReLUs. However  these results differ from ours in term of both assumptions and results. On the
one had  the assumptions are slightly more restrictive as they require bounded features xi  outputs
yi and weights. On the other hand  these result hold for much more general distributions and more
general models than the realizable model studied in this paper. These results also do not apply in the
high dimensional regime where the number of observations is signiï¬cantly smaller than the number
of parameters (see [5] for some results in this direction). In the realizable case  the Isotron result

requireO( 1
) iterations to achieve  error in objective value. In comparison  our results guarantee
convergence to a solution with relative error  (wÏ„âˆ’ wâˆ—(cid:96)2~wâˆ—(cid:96)2â‰¤ ) after log(1~) iterations.
an accuracy of  the algorithm runs in poly(1~) time. In comparison  as mentioned earlier our result
rquires log(1~) iterations for reliable parameter estimation. We note however we study the problem
that with two hidden unites (k= 2) there are no critical points for conï¬gurations where both weight

in different settings and a direct comparison is not possible between the two results.
We would like to note that there is an interesting growing literature on learning shallow neural
networks with a single hidden layer with i.i.d. inputs  and under a realizable model (i.e. the labels are
generated from a network with planted weights) [23  2  25]. For isotropic Gaussian inputs  [23] shows

Focusing on the speciï¬c case of ReLU functions  an interesting recent result [6] shows that reliable
learning of ReLUs is possible under very general but bounded distributional assumptions. To achieve

vectors fall into (or outside) the cone of ground truth weights. With the same assumptions  [2] proves
that for a single-hidden ReLU network with a single non-overlapping convolutional ï¬lter  all local
minimizers of the population loss are global; they also give counter-examples in the overlapping case
and prove the problem is NP-hard when inputs are not Gaussian. [25] studies general single-hidden
layer networks and shows that a version of gradient descent which uses a fresh batch of samples in
each iteration converges to the planted model. This holds using an initialization obtained via a tensor
decomposition method. Our approach and convergence results differ from this literature in a variety
of different ways. First  we focus on zero hidden layers with a regularization term. Some of this
literature focuses on one-hidden layers without (or with speciï¬c) regularization. Second  unlike some
of these results such as [2  14]  we study the optimization properties of the empirical function  not its
expected value. Third  we initialize at zero in lieu of sophisticated initialization schemes. Finally 
our framework does not require a fresh batch of samples per new gradient iteration as in [25]. We
also note that several publications study the effect of over-parametrization on the training of neural
networks without any regularization [19  8  16  22]. Therefore  the global optima are not unique
and hence the solutions may not generalize. In comparison we study the problem with an arbitrary
regularization which allows for a unique global optima.

6 Proofs

6.1 Preliminaries

In this section we gather some useful results on concentration of stochastic processes which will be
crucial in our proofs. These results are mostly adapted from [21]. We begin with a lemma which is a
direct consequence of Gordonâ€™s escape from the mesh lemma [7].

5

Î´2

n

 

(cid:96)2

 

2

 

Î´2

1

 

360 n.

1440 n.

 1

n

Rd. Also assume that

We also need a generalization of the above lemma stated below.

We next state a generalization of Gordonâ€™s escape through the mesh lemma also from [21].

2Î´âˆ’ 1  
(cid:96)2â‰¤ Î´h2

Î´âˆ’ 1  
hâ‰¤ Î´u(cid:96)2h(cid:96)2

Lemma 6.1 AssumeCâŠ‚ Rd is a cone and Sdâˆ’1 is the unit sphere of Rd. Also assume that
Ï‰2(Câˆ© Sdâˆ’1)
nâ‰¥ max20
for a ï¬xed numerical constant c. Then for all hâˆˆC
 1
(xi  h)2âˆ’h2
nQ
i=1
holds with probability at least 1âˆ’ 2eâˆ’ Î´2
Lemma 6.2 ([21]) AssumeCâŠ‚ Rd is a cone (not necessarily convex) and Sdâˆ’1 is the unit sphere of
Ï‰2(Câˆ© Sdâˆ’1)
nâ‰¥ max80
for a ï¬xed numerical constant c. Then for all u  hâˆˆC
xi  uxi  hâˆ’ u
nQ
âˆ—
i=1
holds with probability at least 1âˆ’ 6eâˆ’ Î´2
Lemma 6.3 ([21]) Let sâˆˆ Rd be ï¬xed vector with nonzero entries and construct the diagonal matrix
S= diag(s). Also  let X âˆˆ RnÃ—d have i.i.d.N(0  1) entries. Furthermore  assumeT âŠ‚ Rd and
deï¬ne bd(s)= E[Sg(cid:96)2]  where gâˆˆ Rd is distributed asN(0  In). Also  deï¬ne
Then for all uâˆˆT
holds with probability at least 1âˆ’ 6e
Corollary 6.4 Let sâˆˆ Rd be ï¬xed vector with nonzero entries and assumeT âŠ‚Bd. Furthermore 
s2
(cid:96)2â‰¥ max20s2
Then for all uâˆˆT  
RRRRRRRRRRRâˆ‘n
i(xi  u)2
i=1 s2
s2
holds with probability at least 1âˆ’ 6e
1440s2
âˆ’ Î´2
In this section we shall prove Theorem 3.1. Throughout  we use the shorthandC to denote the descent
cone ofR at wâˆ—  i.e.C=CR(wâˆ—). We begin by analyzing the ï¬rst iteration. Using w0= 0 we have

SAu(cid:96)2âˆ’ bd(s)u(cid:96)2â‰¤s(cid:96)âˆ Ï‰(T)+ Î· 

(cid:96)âˆ Ï‰2(T)
âˆ’u2

Î´2

3

2Î´âˆ’ 1 .
RRRRRRRRRRRâ‰¤ Î´ 

Ïƒ(T)âˆ¶= max

vâˆˆT v(cid:96)2

6.2 Convergence proof (Proof of Theorem 3.1)

The previous lemma leads to the following Corollary.

8s2

Î·2

(cid:96)âˆ Ïƒ2(T)

assume

(cid:96)2

(cid:96)2 .

 

(cid:96)2

w1âˆ¶=PK(w0âˆ’ Âµ0âˆ‡L(w0))=PK 2

nQ
i=1

yixi=PK 2

n

nQ
i=1

n

ReLU(xi  w

âˆ—)xi .

âˆ’

.

.

6

n

(cid:96)2

sup

(cid:96)2

(cid:96)2

(6.1)

âˆ— 

2
n

n

(cid:96)2

(cid:96)2

(6.3)

(6.4)

(cid:96)2

n

n

n

2 we have

X T Xâˆ’ I w

We use the argument of [21][Page 25  inequality (7.34)] which shows that

We proceed by bounding the ï¬rst term in the above equality. To this aim we decompose u in the

âˆ— .
âˆ—)xiâˆ’ w
ReLU(xi  w
uT 2
âˆ—(cid:96)2â‰¤ 2â‹…
w1âˆ’ w
nQ
uâˆˆCâˆ©Bd
i=1
Using ReLU(z)= z+z
xi  w
âˆ—+ 1
X T Xâˆ’ I w
âˆ—= uT 1
âˆ—)xi  uâˆ’u  w
ReLU(xi  w
âˆ—xi  u. (6.2)
nQ
nQ
i=1
i=1
direction parallel/perpendicular to that of wâˆ— and arrive at
nXÃ¯Ã¯Iâˆ’ wâˆ—(wâˆ—)T
Ã¯ u  Xw
âˆ—=(uT wâˆ—)
âˆ—+ 1
(w
âˆ—)T 1
uT 1
X T Xâˆ’ I w
wâˆ—2
wâˆ—2
âˆ—)Ã¯Ã¯g2
n âˆ’ 1Ã¯+wâˆ—(cid:96)2âˆšn
aTÃ¯Ã¯Iâˆ’ wâˆ—(wâˆ—)T
Ã¯ u 
âˆ¼(uT w
wâˆ—2
RRRRRRRRRRR
RRRRRRRRRRR+wâˆ—(cid:96)2âˆšn
g2
Ã¯ u 
aTÃ¯Ã¯Iâˆ’ wâˆ—(wâˆ—)T
n âˆ’ 1
âˆ—(cid:96)2
â‰¤w
wâˆ—2
uâˆˆCâˆ©Bd
with gâˆˆ Rn and aâˆˆ Rd are independent random Gaussian random vectors distributed asN(0  Id)
andN(0  In). By concentration of Chi-squared random variables
g2
(cid:96)2~nâˆ’ 1â‰¤ âˆ† 
holds with probability at least 1âˆ’ 2eâˆ’n âˆ†2
Ã¯ uâ‰¤ 1âˆšnÏ‰Câˆ©Bd+ Î·  
aTÃ¯Ã¯Iâˆ’ wâˆ—(wâˆ—)T
1âˆšn
wâˆ—2
6 and (6.5) with Î·= Î´
holds with probability at least 1âˆ’ eâˆ’ Î·2
2 . Plugging (6.4) with âˆ†= Î´
Î´2 Ï‰2Câˆ©Bd  then
(6.3)  as long as nâ‰¥ 36
uT 1
âˆ—â‰¤ Î´
2w
X T Xâˆ’ I w
âˆ—(cid:96)2
holds with probability at least 1âˆ’ 3eâˆ’n Î´2
parallel/perpendicular to that of wâˆ— and arrive at
âˆ—xi  u=RRRRRRRRRRR(uT w
xi  wâˆ—xi  wâˆ—
 1
+ 1
âˆ—) 1
nQ
nQ
wâˆ—2
i=1
i=1
RRRRRRRRRRR+ 1
RRRRRRRRRRR 1
xi  wâˆ—xi  wâˆ—
â‰¤w
âˆ—(cid:96)2
nQ
wâˆ—2
i=1
with uâŠ¥=Iâˆ’ w
 u. Now note that
xi w
âˆ—xi w
âˆ—
âˆ—(w
âˆ—)T
wâˆ—2
wâˆ—2
xi  wâˆ—xi  wâˆ—
â‰¤ c 

wâˆ—2
RRRRRRRRRRRâ‰¤ t 
RRRRRRRRRRR 1
xi  wâˆ—xi  wâˆ—
nQ
wâˆ—2
i=1

âˆ—xi  uâŠ¥RRRRRRRRRRR  
xi  w
nQ
i=1
âˆ—xi  uâŠ¥ . (6.7)
xi  w
nQ
i=1

We now focus on bounding the second term in (6.2). To this aim we decompose u in the direction

with ï¬xed numerical constant. Thus by Bernsteinâ€™s type inequality ([24][Proposition 5.16])

(6.5)

âˆšn into

xi  w

is sub-exponential and

uâˆˆCâˆ©Bd

sup

(cid:96)2

(cid:96)2

(cid:96)2

 

(6.6)

8 . Also 

(cid:96)2

n

288 .

n

n

(cid:96)2

Ïˆ1

n

(cid:96)2

6

(6.8)

sup

n

n

n

(cid:96)2

7

1
n

n

(cid:96)2

sup

8 and

Furthermore  1

xi  w

xi  w

2 . Combining the last two inequalities we conclude that

xi  wâˆ—2 1âˆšng  uâŠ¥.
nQ
i=1
  holds with probability at least 1âˆ’ 2eâˆ’n âˆ†2

holds with probability at least 1âˆ’ 2eâˆ’Î³n mint2 t with Î³ a ï¬xed numerical constant.. Also note that

âˆ—xi  uâŠ¥âˆ¼Â¿``(cid:192) 1
nQ
i=1
i=1xi  wâˆ—2â‰¤(1+ âˆ†)wâˆ—2
nâˆ‘n
g  uâŠ¥â‰¤(2Ï‰Câˆ© Sdâˆ’1+ Î·) 
uâˆˆCâˆ©Sdâˆ’1
holds with probability at least 1âˆ’ eâˆ’ Î·2
âˆ—xi  uâŠ¥â‰¤âˆš
(2Ï‰Câˆ© Sdâˆ’1+ Î·)
âˆšn
1+ âˆ†
holds with probability at least 1âˆ’ 2eâˆ’n âˆ†2
8 âˆ’ eâˆ’ Î·2
2 . Plugging (6.8) and (6.9) with t= Î´
Î·= Î´
âˆš
âˆ—xi  uâ‰¤ Î´
xi  w
2w
8 as long as nâ‰¥ 288 Ï‰2Câˆ©Sdâˆ’1
âˆ—â‰¤ 2Î´w
âˆ—)xiâˆ’ w

holds with probability at least 1âˆ’ 3eâˆ’Î³nÎ´2âˆ’ 2eâˆ’ n
and (6.10) into (6.1) we conclude that for Î´= 7~400
w1âˆ’ w
âˆ—(cid:96)2
ReLU(xi  w
holds with probability at least 1âˆ’ 8eâˆ’Î³n as long as nâ‰¥ cÏ‰2Câˆ© Sdâˆ’1 for a ï¬xed numerical constant

 1
nQ
i=1
âˆšn into (6.7)

nQ
i=1
uT 2

âˆ—(cid:96)2â‰¤ 7
200w

6  âˆ†= 1  and

âˆ—(cid:96)2â‰¤ 2â‹…

uâˆˆCâˆ©Bd

sup

. Thus pluggin (6.6)

âˆ—(cid:96)2

âˆ—(cid:96)2

w

nQ
i=1

 1

(6.10)

(6.9)

6

2

 

Î´2

n

n

n

 

 

projected gradient descent update wÏ„+1=PK(wÏ„âˆ’âˆ‡L(wÏ„))  the error hÏ„= wÏ„âˆ’ wâˆ— obeys

To prove Theorem 3.1 we use [21][Page 25  inequality (7.34)] which shows that if we apply the

200

.

(6.11)

(6.13)

We will instead prove that the following stronger result holds for all uâˆˆCâˆ©Bn and wâˆˆ E()

.

(6.12)

âˆ—(cid:96)2 with = 7
âˆ—(hÏ„âˆ’âˆ‡L(wÏ„)) .
âˆ—(cid:96)2
4wÏ„âˆ’ w
âˆ—(cid:96)2

.

c. To introduce our general convergence analysis we begin by deï¬ning

u

u

sup

sup

uâˆˆCâˆ©Bn

âˆ—(cid:96)2â‰¤ w

To complete the convergence analysis it is then sufï¬cient to prove

E()=wâˆˆ Rdâˆ¶R(w)â‰¤R(w
hÏ„+1(cid:96)2=wÏ„+1âˆ’ w

âˆ—)  wâˆ’ w
âˆ—(cid:96)2â‰¤ 2â‹…
uâˆˆCâˆ©Bn
âˆ—(hÏ„âˆ’âˆ‡L(wÏ„))â‰¤ 1
4hÏ„(cid:96)2= 1
4wâˆ’ w
âˆ—âˆ’âˆ‡L(w))â‰¤ 1
âˆ—(wâˆ’ w
(6.13). To this aim note that ReLU(xi  w)= xi w+xi w
âˆ‡L(w)  u= 1
sgn(xi  w
âˆ—xi  u+ 1
xi  wâˆ’ w
nQ
nQ
i=1
i=1
(sgn(xi  w)âˆ’ sgn(xi  w
âˆ—))xi  wâˆ’ w
+ 1
nQ
i=1
âˆ—))(sgn(xi  w
(1âˆ’ sgn(xi  w
+ 1
nQ
i=1

paper [20] for more detailed derivation of the identity below)

2n

u

n

n

n

2

âˆ—)xi  wâˆ’ w
âˆ—xi  u
âˆ—xi  u
âˆ—)âˆ’ sgn(xi  w))xi  w

âˆ—xi  u

The equation (6.13) above implies (6.12) which when combined with (6.11) proves the convergence
result of the Theorem (speciï¬cally equation (3.5)). The rest of this section is dedicated to proving
. Thus (see the extended version of this

8

1
n

n

n

nQ
i=1

n

n

(cid:96)2

2n

âˆ—))

+ 1
+ 1

n

n

the form (see the proof in the extended version of this paper [20] for more detailed derivation)

âˆ—)xi  hxi  u 
âˆ—)) sgn(xi  w
âˆ—))(1âˆ’ sgn(xi  w)sgn(xi  w

(6.14)
We now proceed by stating bounds on each of the four terms in (6.14). The detailed derivation of
these bounds appear in the the extended version of this paper [20].

Now deï¬ning h= wâˆ’ wâˆ— we conclude thatu  wâˆ’ wâˆ—âˆ’âˆ‡L(w)=u  hâˆ’âˆ‡L(w) is equal to
sgn(xi  w
XX T hâˆ’ 1
u  hâˆ’âˆ‡L(w)=uTIâˆ’ 1
+h  wâˆ—
(1âˆ’ sgn(xi  w)sgn(xi  w
âˆ—)xi  hxi  u 
nQ
wâˆ—2
i=1
+ sgn(xi  w)
(1âˆ’ sgn(xi  w
nQ
i=1
âˆ—xi  u.
xi  w
Now deï¬ne hâŠ¥= hâˆ’(hT wâˆ—)~(wâˆ—2
(cid:96)2)wâˆ—. Using this we can rewrite the previous expression in
XX T hâˆ’ 1
u  wâˆ’ w
âˆ—âˆ’âˆ‡L(w)=uTIâˆ’ 1
sgn(xi  w
âˆ—)xi  hxi  u 
nQ
i=1
âˆ—)xi  hâŠ¥xi  u 
(1âˆ’ sgn(xi  w)sgn(xi  w
âˆ—)) sgn(xi  w
nQ
i=1
âˆ—))+h  wâˆ—
sgn(xi  w)

(1âˆ’ sgn(xi  w
nQ
wâˆ—2
i=1
âˆ—))xi  w
(1âˆ’ sgn(xi  w)sgn(xi  w
âˆ—xi  u
Lemma 6.5 Assume the setup of Theorem 3.1. Then as long as nâ‰¥ cn0  we have
âˆ—Iâˆ’ 1
X hâ‰¤ Î´h(cid:96)2
âˆ—)xi  hxi  uâ‰¤ Î´h(cid:96)2
sgn(xi  w
1+ Î´Ã¯Ã¯Î´+
âˆš
âˆ—)xi  hâŠ¥xi  uâ‰¤2
âˆ—)) sgn(xi  w
âˆ—))+h  wâˆ—

(1âˆ’ sgn(xi  w
wâˆ—2
Ã¯Ã¯Î´+
âˆš
1+ Î´
âˆ—xi  uâ‰¤ 4
âˆ—))xi  w
(1âˆ’ )2
holds for all uâˆˆCâˆ© Sdâˆ’1 and wâˆˆ E() with probability at least 1âˆ’ 9eâˆ’Î³n.
2(1âˆ’ )2Ã¯Ã¯Î´+
Ã¯Ã¯wâˆ’ w
âˆ—(cid:96)2
holds for all uâˆˆCâˆ© Sdâˆ’1 and wâˆˆ E() with probability at least 1âˆ’ 16eâˆ’Î³Î´2nâˆ’(n+ 10)eâˆ’Î³n. Using
4wâˆ’ wâˆ—(cid:96)2
this inequality with Î´= 10âˆ’4 and = 7~200 we conclude thatu  wâˆ’ wâˆ—âˆ’âˆ‡L(w)â‰¤ 1
holds for all uâˆˆCâˆ© Sdâˆ’1 and wâˆˆ E() with high probability.

nQ
i=1
(1âˆ’ sgn(xi  w)sgn(xi  w
 sgn(xi  w)
nQ
i=1
(1âˆ’ sgn(xi  w)sgn(xi  w

âˆ—âˆ’âˆ‡L(w)â‰¤ 2Ã¯Ã¯Î´+âˆš

Combining (6.15)  (6.16)  (6.17)  and (6.18) we conclude that

u  wâˆ’ w

1+ Î´1+

(6.16)

(6.17)

Ã¯h(cid:96)2
Ã¯h(cid:96)2

(6.18)

nQ
i=1

1
n

âˆ—

X

n

2

u

âˆ’ 1

n

21
20

21
20

 

1
n

2

 

 

(cid:96)2

21
20

(cid:96)2

(6.15)

 

 

 

Acknowledgements

This work was done in part while the author was visiting the Simonâ€™s Institute for the Theory of
Computing. M.S. would like to thank Adam Klivans and Matus Telgarsky for discussions related to
[6] and the Isotron algorithm.

9

References
[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: Phase transitions

in convex programs with random data. Information and Inference  2014.

[2] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian

inputs. International Conference on Machine Learning (ICML)  2017.

[3] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805â€“849  2012.

[4] R. Collobert and J. Weston. A uniï¬ed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference
on Machine learning  pages 160â€“167. ACM  2008.

[5] R. Ganti  N. Rao  R. M. Willett  and R. Nowak. Learning single index models in high dimensions.

arXiv preprint arXiv:1506.08910  2015.

[6] S. Goel  V. Kanade  A. Klivans  and J. Thaler. Reliably learning the ReLU in polynomial time.

arXiv preprint arXiv:1611.10258  2016.

[7] Y. Gordon. On Milmanâ€™s inequality and random subspaces which escape through a mesh in Rn.

Springer  1988.

[8] B. D. Haeffele and R. Vidal. Global optimality in tensor factorization  deep learning  and

beyond. arXiv preprint arXiv:1506.07540  2015.

[9] J. L. Horowitz and W. Hardle. Direct semiparametric estimation of single-index models with
discrete covariates. Journal of the American Statistical Association  91(436):1632â€“1640  1996.

[10] H. Ichimura. Semiparametric least squares (SLS) and weighted SLS estimation of single-index

models. Journal of Econometrics  58(1-2):71â€“120  1993.

[11] S. M. Kakade  V. Kanade  O. Shamir  and A. Kalai. Efï¬cient learning of generalized linear and
single index models with isotonic regression. In Advances in Neural Information Processing
Systems  pages 927â€“935  2011.

[12] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In

COLT  2009.

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiï¬cation with deep convolutional
neural networks. In Advances in neural information processing systems  pages 1097â€“1105 
2012.

[14] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with ReLU activation.

arXiv preprint arXiv:1705.09886  2017.

[15] A. Mohamed  G. E. Dahl  and G. Hinton. Acoustic modeling using deep belief networks. IEEE

Transactions on Audio  Speech  and Language Processing  20(1):14â€“22  2012.

[16] Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv

preprint arXiv:1704.08045  2017.

[17] S. Oymak  B. Recht  and M. Soltanolkotabi. Sharp timeâ€“data tradeoffs for linear inverse

problems. arXiv preprint arXiv:1507.04793  2015.

[18] S. Oymak and M. Soltanolkotabi. Fast and reliable parameter estimation from nonlinear

observations. arXiv preprint arXiv:1610.07108  2016.

[19] T. Poston  C-N. Lee  Y. Choie  and Y. Kwon. Local minima and back propagation. In Neural
Networks  1991.  IJCNN-91-Seattle International Joint Conference on  volume 2  pages 173â€“176.
IEEE  1991.

[20] M. Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591 

2017.

10

[21] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample

complexity barriers via nonconvex optimization. arXiv preprint arXiv:1702.06175  2017.

[22] M. Soltanolkotabi  A. Javanmard  and J. D. Lee. Theoretical insights into the optimization

landscape of over-parameterized shallow neural networks. 07 2017.

[23] Y. Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. International Conference on Machine
Learning (ICML)  2017.

[24] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027  2010.

[25] K. Zhong  Z. Song  P. Jain  P. L. Bartlett  and I. S. Dhillon. Recovery guarantees for one-hidden-

layer neural networks. arXiv preprint arXiv:1706.03175  2017.

11

,Shane Griffith
Kaushik Subramanian
Jonathan Scholz
Charles Isbell
Andrea Thomaz
Ryan Kiros
Richard Zemel
Russ Salakhutdinov
Mahdi Soltanolkotabi
Piotr Mirowski
Matt Grimes
Mateusz Malinowski
Karl Moritz Hermann
Keith Anderson
Denis Teplyashin
Karen Simonyan
koray kavukcuoglu
Andrew Zisserman
Raia Hadsell