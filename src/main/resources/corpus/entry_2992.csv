2017,Learning ReLUs via Gradient Descent,In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form $\vct{x}\mapsto \max(0 \langle \vct{w} \vct{x}\rangle)$ with $\vct{w}\in\R^d$ denoting the weight vector.  We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.~from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent  when initialized at $\vct{0}$  converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.,Learning ReLUs via Gradient Descent

Ming Hsieh Department of Electrical Engineering

University of Southern California

Mahdi Soltanolkotabi

Los Angeles  CA

soltanol@usc.edu

Abstract

which are functions of the form x max(0 w  x) with w‚àà Rd denoting the

In this paper we study the problem of learning RectiÔ¨Åed Linear Units (ReLUs)

weight vector. We study this problem in the high-dimensional regime where the
number of observations are fewer than the dimension of the weight vector. We
assume that the weight vector belongs to some closed set (convex or nonconvex)
which captures known side-information about its structure. We focus on the
realizable model where the inputs are chosen i.i.d. from a Gaussian distribution
and the labels are generated according to a planted weight vector. We show that
projected gradient descent  when initialized at 0  converges at a linear rate to the
planted model with a number of samples that is optimal up to numerical constants.
Our results on the dynamics of convergence of these very shallow neural nets may
provide some insights towards understanding the dynamics of deeper architectures.

1

Introduction

Nonlinear data-Ô¨Åtting problems are fundamental to many supervised learning tasks in signal process-

ing and machine learning. Given training data consisting of n pairs of input features xi‚àà Rd and
desired outputs yi‚àà R we wish to infer a function that best explains the training data. In this paper
we focus on Ô¨Åtting RectiÔ¨Åed Linear Units (ReLUs) to the data which are functions œÜw‚à∂ Rd‚Üí R of

the form

œÜw(x)= max(0 w  x) .
(max(0 w  xi)‚àí yi)2

subject to R(w)‚â§ R 

w‚ààRd L(w)‚à∂= 1

min

nQ
i=1

A natural approach to Ô¨Åtting ReLUs to data is via minimizing the least-squares misÔ¨Åt aggregated over
the data. This optimization problem takes the form

(1.1)

n

withR‚à∂ Rd‚Üí R denoting a regularization function that encodes prior information on the weight

vector.
Fitting nonlinear models such as ReLUs have a rich history in statistics and learning theory [12]
with interesting new developments emerging [6] (we shall discuss all these results in greater detail in
Section 5). Most recently  nonlinear data Ô¨Åtting problems in the form of neural networks (a.k.a. deep
learning) have emerged as powerful tools for automatically extracting interpretable and actionable
information from raw forms of data  leading to striking breakthroughs in a multitude of applications
[13  15  4]. In these and many other empirical domains it is common to use local search heuristics
such as gradient or stochastic gradient descent for nonlinear data Ô¨Åtting. These local search heuristics
are surprisingly effective on real or randomly generated data. However  despite their empirical success
the reasons for their effectiveness remains mysterious.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Focusing on Ô¨Åtting ReLUs  a-priori it is completely unclear why local search heuristics such as
gradient descent should converge for problems of the form (1.1)  as not only the regularization
function maybe nonconvex but also the loss function! EfÔ¨Åcient Ô¨Åtting of ReLUs in this high-
dimensional setting poses new challenges: When are the iterates able to escape local optima and
saddle points and converge to global optima? How many samples do we need? How does the number
of samples depend on the a-priori prior knowledge available about the weights? What regularizer
is best suited to utilizing a particular form of prior knowledge? How many passes (or iterations) of
the algorithm is required to get to an accurate solution? At the heart of answering these questions is
the ability to predict convergence behavior/rate of (non)convex constrained optimization algorithms.
In this paper we build up on a new framework developed in the context of phase retrieval [21] for
analyzing nonconvex optimization problems to address such challenges.

2 Precise measures for statistical resources

‚àó+ h)‚â§R(w

‚àó).

‚àó)=h‚à∂ R(w

We wish to characterize the rates of convergence for the projected gradient updates (3.2) as a function
of the number of samples  the available prior knowledge and the choice of the regularizer. To make
these connections precise and quantitative we need a few deÔ¨Ånitions. Naturally the required number

properties of the weight vector w. For example  if we know that the weight vector is approximately
sparse  naturally using an (cid:96)1 norm for the regularizer is superior to using an (cid:96)2 regularizer. To quantify
this capability we Ô¨Årst need a couple of standard deÔ¨Ånitions which we adapt from [17  18  21].

of samples for reliable data Ô¨Åtting depends on how well the regularization functionR can capture the
DeÔ¨Ånition 2.1 (Descent set and cone) The set of descent of a functionR at a point w‚àó is deÔ¨Åned as
DR(w
The cone of descent is deÔ¨Åned as a closed coneCR(w‚àó) that contains the descent set  i.e.DR(w‚àó)‚äÇ
CR(w‚àó). The tangent cone is the conic hull of the descent set. That is  the smallest closed cone
CR(w‚àó) obeyingDR(w‚àó)‚äÇCR(w‚àó).
We note that the capability of the regularizerR in capturing the properties of the unknown weight
vector w‚àó depends on the size of the descent coneCR(w‚àó). The smaller this cone is the more suited
the functionR is at capturing the properties of w‚àó. To quantify the size of this set we shall use the
DeÔ¨Ånition 2.2 (Gaussian width) The Gaussian width of a setC‚àà Rd is deÔ¨Åned as:
where the expectation is taken over g‚àºN(0  Ip). Throughout we useBd~Sd‚àí1 to denote the the unit
We now have all the deÔ¨Ånitions in place to quantify the capability of the functionR in capturing the
properties of the unknown parameter w‚àó. This naturally leads us to the deÔ¨Ånition of the minimum
DeÔ¨Ånition 2.3 (minimal number of samples) LetCR(w‚àó) be a cone of descent ofR at w‚àó. We
deÔ¨Åne the minimal sample function asM(R  w
We shall often use the short hand n0=M(R  w‚àó) with the dependence onR  w‚àó implied.

œâ(C)‚à∂= Eg[sup

z‚ààCg  z] 

‚àó)= œâ2(CR(w

‚àó)‚à©Bd).

required number of samples.

notion of mean width.

ball/sphere of Rd.

We note that n0 is exactly the minimum number of samples required for structured signal recovery
from linear measurements when using convex regularizers [3  1]. SpeciÔ¨Åcally  the optimization
problem

(yr‚àíxi  w

‚àó)2

subject to R(w)‚â§R(w

‚àó) 

nQ
i=1

(2.1)

2

succeeds at recovering an unknown weight vector w‚àó with high probability from n observations of
the form yi=ai  w‚àó if and only if n‚â• n0.1 While this result is only known to be true for convex
compared to linear observations) we can not hope to recover the weight vector from n‚â§ n0 when

regularization functions we believe that n0 also characterizes the minimal number of samples even for
nonconvex regularizers in (2.1). See [17] for some results in the nonconvex case as well as the role
this quantity plays in the computational complexity of projected gradient schemes for linear inverse
problems. Given that with nonlinear samples we have less information (we loose some information

using (1.1). Therefore  we can use n0 as a lower-bound on the minimum number of observations
required for projected gradient descent iterations (3.2) to succeed at Ô¨Ånding the right model.

3 Theoretical results for learning ReLUs

A simple heuristic for optimizing (1.1) is to use gradient descent. One challenging aspect of the
above loss function is that it is not differentiable and it is not clear how to run projected gradient
descent. However  this does not pose a fundamental challenge as the loss function is differentiable
except for isolated points and we can use the notion of generalized gradients to deÔ¨Åne the gradient at
a non-differentiable point as one of the limit points of the gradient in a local neighborhood of the
non-differentiable point. For the loss in (1.1) the generalized gradient takes the form

‚àáL(w)‚à∂= 1

n

(ReLU(w  xi)‚àí yi)(1+ sgn(w  xi)) xi.
nQ
i=1
wœÑ+1=PK(wœÑ‚àí ¬µœÑ‚àáL(wœÑ))  

Therefore  projected gradient descent takes the form

(3.2)

Euclidean projection onto this set.

where ¬µœÑ is the step size andK={w‚àà Rd‚à∂R(w)‚â§ R} is the constraint set withPK denoting the
Theorem 3.1 Let w‚àó ‚àà Rd be an arbitrary weight vector andR‚à∂ Rd ‚Üí R be a proper function
(convex or nonconvex). Suppose the feature vectors xi ‚àà Rd are i.i.d. Gaussian random vectors
distributed asN(0  I) with the corresponding labels given by
yi= max(0 xi  w
‚àó) .
To estimate w‚àó  we start from the initial point w0= 0 and apply the Projected Gradient Descent
wœÑ+1=PK(wœÑ‚àí ¬µœÑ‚àáL(wœÑ))  
withK‚à∂={w‚àà Rd‚à∂ R(w)‚â§R(w‚àó)} and‚àáL deÔ¨Åned via (3.1). Also set the learning parameter
sequence to ¬µ0= 2 and ¬µœÑ = 1 for all œÑ= 1  2  . . . and let n0=M(R  w‚àó)  per DeÔ¨Ånition 2.3  be
holds for a Ô¨Åxed numerical constant c. Then there is an event of probability at least 1‚àí 9e‚àíŒ≥n such

our lower bound on the number of observations. Also assume

(PGD) updates of the form

(3.4)

(3.3)

that on this event the updates (3.3) obey

(3.1)

n> cn0 
‚àó(cid:96)2‚â§ 1

wœÑ‚àí w

2œÑw

‚àó(cid:96)2

.

(3.5)

Here Œ≥ is a Ô¨Åxed numerical constant.

The Ô¨Årst interesting and perhaps surprising aspect of this result is its generality: it applies not only to
convex regularization functions but also nonconvex ones! As we mentioned earlier the optimization
problem in (1.1) is not known to be tractable even for convex regularizers. Despite the nonconvexity
of both the objective and regularizer  the theorem above shows that with a near minimal number

more precise characterization is œÜ‚àí1(œâ2(CR(w‚àó)‚à©Bd))‚âà œâ2(CR(w‚àó)‚à©Bd) where œÜ(t)=‚àö

1We would like to note that n0 only approximately characterizes the minimum number of samples required. A
t.

2 ‚âà‚àö
2 
Œì t+1
Œì t

However  since our results have unspeciÔ¨Åed constants we avoid this more accurate characterization.

2

3

average behavior over 100 trials. None bold color depict the estimation error of some sample trials.

number of iterations œÑ. The plots are for two different observations models: 1) ReLU observations of

Figure 1: Estimation error (wœÑ‚àí w‚àó(cid:96)2) obtained via running PGD iterates as a function of the
the form y=ReLU(Xw‚àó) and 2) linear observations of the form y= Xw‚àó. The bold colors depict
of data samples  projected gradient descent provably learns the original weight vector w‚àó without
achieve a relative error of  the total number of iterations is on the order ofO(log(1~)). Thus the
overall computational complexity is on the order ofO(nd log(1~)) (in general the cost is the total
one matrix-vector multiplication which takesO(nd) time.

number of iterations multiplied by the cost of applying the feature matrix X and its transpose). As
a result  the computational complexity is also now optimal in terms of dependence on the matrix
dimensions. Indeed  for a dense matrix even verifying that a good solution has been achieved requires

getting trapped in any local optima.
Another interesting aspect of the above result is that the convergence rate is linear. Therefore  to

4 Numerical experiments

In this section we carry out a simple numerical experiment to corroborate our theoretical results. For

this purpose we generate a unit norm sparse vector w‚àó‚àà Rd of dimension d= 1000 containing s=
d~50 non-zero entries. We also generate a random feature matrix X‚àà Rn√ód with n=√¶8s log(d~s)√¶
and containing i.i.d.N(0  1) entries. We now take two sets of observations of size n from Œ∏‚àó:
We apply the projected gradient iterations to both observation models starting from w0= 0. For the

‚Ä¢ ReLU observations: the response vector is equal to y=ReLU(Xw‚àó).
‚Ä¢ Linear observations: the response is y= Xw‚àó.

ReLU observations we use the step size discussed in Theorem 3.1. For the linear model we apply
projected gradient descent updates of the form

wœÑ+1=PKwœÑ‚àí 1

X T(XwœÑ‚àí y) .

In both cases we use the regularizerR(w)=w(cid:96)0 so that the projection only keeps the top s
(wœÑ‚àí w‚àó(cid:96)2) is depicted as a function of the number of iterations œÑ. The bold colors depict average

entries of the vector (a.k.a. iterative hard thresholding). In Figure 1 the resulting estimation errors

behavior over 100 trials. The estimation error of some sample trials are also depicted in none bold

n

4

0510152000.20.40.60.81Estimation errorReLU samplesLinear samplescolors. This plot clearly show that PGD iterates applied to ReLU observations converge quickly
to the ground truth. This Ô¨Ågure also clearly demonstrates that the behavior of the PGD iterates
applied to both models are similar  further corroborating the results of Theorem 3.1. We note that

the sample complexity used in this simulation is 8s log(n~s) which is a constant factor away from
n0‚àù s log(n~s) conÔ¨Årming our assertion that the required sample complexity is a constant factor

away from n0 (as predicted by Theorem 3.1).

5 Discussions and prior art

There is a large body of work on learning nonlinear models. A particular class of such problems
that have been studied are the so called idealized Single Index Models (SIMs) [9  10]. In these

problems the inputs are labeled examples{(xi  yi)}n
i=1‚àà Rd√ó R which are guaranteed to satisfy
yi= f(w  xi) for some w‚àà Rd and nondecreasing (Lipchitz continuous) f‚à∂ R‚Üí R. The goal in

this problem is to Ô¨Ånd a (nearly) accurate such f and w. An interesting polynomial-time algorithm
called the Isotron exists for this problem [12  11]. In principle  this approach can also be used to
Ô¨Åt ReLUs. However  these results differ from ours in term of both assumptions and results. On the
one had  the assumptions are slightly more restrictive as they require bounded features xi  outputs
yi and weights. On the other hand  these result hold for much more general distributions and more
general models than the realizable model studied in this paper. These results also do not apply in the
high dimensional regime where the number of observations is signiÔ¨Åcantly smaller than the number
of parameters (see [5] for some results in this direction). In the realizable case  the Isotron result

requireO( 1
) iterations to achieve  error in objective value. In comparison  our results guarantee
convergence to a solution with relative error  (wœÑ‚àí w‚àó(cid:96)2~w‚àó(cid:96)2‚â§ ) after log(1~) iterations.
an accuracy of  the algorithm runs in poly(1~) time. In comparison  as mentioned earlier our result
rquires log(1~) iterations for reliable parameter estimation. We note however we study the problem
that with two hidden unites (k= 2) there are no critical points for conÔ¨Ågurations where both weight

in different settings and a direct comparison is not possible between the two results.
We would like to note that there is an interesting growing literature on learning shallow neural
networks with a single hidden layer with i.i.d. inputs  and under a realizable model (i.e. the labels are
generated from a network with planted weights) [23  2  25]. For isotropic Gaussian inputs  [23] shows

Focusing on the speciÔ¨Åc case of ReLU functions  an interesting recent result [6] shows that reliable
learning of ReLUs is possible under very general but bounded distributional assumptions. To achieve

vectors fall into (or outside) the cone of ground truth weights. With the same assumptions  [2] proves
that for a single-hidden ReLU network with a single non-overlapping convolutional Ô¨Ålter  all local
minimizers of the population loss are global; they also give counter-examples in the overlapping case
and prove the problem is NP-hard when inputs are not Gaussian. [25] studies general single-hidden
layer networks and shows that a version of gradient descent which uses a fresh batch of samples in
each iteration converges to the planted model. This holds using an initialization obtained via a tensor
decomposition method. Our approach and convergence results differ from this literature in a variety
of different ways. First  we focus on zero hidden layers with a regularization term. Some of this
literature focuses on one-hidden layers without (or with speciÔ¨Åc) regularization. Second  unlike some
of these results such as [2  14]  we study the optimization properties of the empirical function  not its
expected value. Third  we initialize at zero in lieu of sophisticated initialization schemes. Finally 
our framework does not require a fresh batch of samples per new gradient iteration as in [25]. We
also note that several publications study the effect of over-parametrization on the training of neural
networks without any regularization [19  8  16  22]. Therefore  the global optima are not unique
and hence the solutions may not generalize. In comparison we study the problem with an arbitrary
regularization which allows for a unique global optima.

6 Proofs

6.1 Preliminaries

In this section we gather some useful results on concentration of stochastic processes which will be
crucial in our proofs. These results are mostly adapted from [21]. We begin with a lemma which is a
direct consequence of Gordon‚Äôs escape from the mesh lemma [7].

5

Œ¥2

n

 

(cid:96)2

 

2

 

Œ¥2

1

 

360 n.

1440 n.

 1

n

Rd. Also assume that

We also need a generalization of the above lemma stated below.

We next state a generalization of Gordon‚Äôs escape through the mesh lemma also from [21].

2Œ¥‚àí 1  
(cid:96)2‚â§ Œ¥h2

Œ¥‚àí 1  
h‚â§ Œ¥u(cid:96)2h(cid:96)2

Lemma 6.1 AssumeC‚äÇ Rd is a cone and Sd‚àí1 is the unit sphere of Rd. Also assume that
œâ2(C‚à© Sd‚àí1)
n‚â• max20
for a Ô¨Åxed numerical constant c. Then for all h‚ààC
 1
(xi  h)2‚àíh2
nQ
i=1
holds with probability at least 1‚àí 2e‚àí Œ¥2
Lemma 6.2 ([21]) AssumeC‚äÇ Rd is a cone (not necessarily convex) and Sd‚àí1 is the unit sphere of
œâ2(C‚à© Sd‚àí1)
n‚â• max80
for a Ô¨Åxed numerical constant c. Then for all u  h‚ààC
xi  uxi  h‚àí u
nQ
‚àó
i=1
holds with probability at least 1‚àí 6e‚àí Œ¥2
Lemma 6.3 ([21]) Let s‚àà Rd be Ô¨Åxed vector with nonzero entries and construct the diagonal matrix
S= diag(s). Also  let X ‚àà Rn√ód have i.i.d.N(0  1) entries. Furthermore  assumeT ‚äÇ Rd and
deÔ¨Åne bd(s)= E[Sg(cid:96)2]  where g‚àà Rd is distributed asN(0  In). Also  deÔ¨Åne
Then for all u‚ààT
holds with probability at least 1‚àí 6e
Corollary 6.4 Let s‚àà Rd be Ô¨Åxed vector with nonzero entries and assumeT ‚äÇBd. Furthermore 
s2
(cid:96)2‚â• max20s2
Then for all u‚ààT  
RRRRRRRRRRR‚àën
i(xi  u)2
i=1 s2
s2
holds with probability at least 1‚àí 6e
1440s2
‚àí Œ¥2
In this section we shall prove Theorem 3.1. Throughout  we use the shorthandC to denote the descent
cone ofR at w‚àó  i.e.C=CR(w‚àó). We begin by analyzing the Ô¨Årst iteration. Using w0= 0 we have

SAu(cid:96)2‚àí bd(s)u(cid:96)2‚â§s(cid:96)‚àû œâ(T)+ Œ∑ 

(cid:96)‚àû œâ2(T)
‚àíu2

Œ¥2

3

2Œ¥‚àí 1 .
RRRRRRRRRRR‚â§ Œ¥ 

œÉ(T)‚à∂= max

v‚ààT v(cid:96)2

6.2 Convergence proof (Proof of Theorem 3.1)

The previous lemma leads to the following Corollary.

8s2

Œ∑2

(cid:96)‚àû œÉ2(T)

assume

(cid:96)2

(cid:96)2 .

 

(cid:96)2

w1‚à∂=PK(w0‚àí ¬µ0‚àáL(w0))=PK 2

nQ
i=1

yixi=PK 2

n

nQ
i=1

n

ReLU(xi  w

‚àó)xi .

‚àí

.

.

6

n

(cid:96)2

sup

(cid:96)2

(cid:96)2

(6.1)

‚àó 

2
n

n

(cid:96)2

(cid:96)2

(6.3)

(6.4)

(cid:96)2

n

n

n

2 we have

X T X‚àí I w

We use the argument of [21][Page 25  inequality (7.34)] which shows that

We proceed by bounding the Ô¨Årst term in the above equality. To this aim we decompose u in the

‚àó .
‚àó)xi‚àí w
ReLU(xi  w
uT 2
‚àó(cid:96)2‚â§ 2‚ãÖ
w1‚àí w
nQ
u‚ààC‚à©Bd
i=1
Using ReLU(z)= z+z
xi  w
‚àó+ 1
X T X‚àí I w
‚àó= uT 1
‚àó)xi  u‚àíu  w
ReLU(xi  w
‚àóxi  u. (6.2)
nQ
nQ
i=1
i=1
direction parallel/perpendicular to that of w‚àó and arrive at
nX√Ø√ØI‚àí w‚àó(w‚àó)T
√Ø u  Xw
‚àó=(uT w‚àó)
‚àó+ 1
(w
‚àó)T 1
uT 1
X T X‚àí I w
w‚àó2
w‚àó2
‚àó)√Ø√Øg2
n ‚àí 1√Ø+w‚àó(cid:96)2‚àön
aT√Ø√ØI‚àí w‚àó(w‚àó)T
√Ø u 
‚àº(uT w
w‚àó2
RRRRRRRRRRR
RRRRRRRRRRR+w‚àó(cid:96)2‚àön
g2
√Ø u 
aT√Ø√ØI‚àí w‚àó(w‚àó)T
n ‚àí 1
‚àó(cid:96)2
‚â§w
w‚àó2
u‚ààC‚à©Bd
with g‚àà Rn and a‚àà Rd are independent random Gaussian random vectors distributed asN(0  Id)
andN(0  In). By concentration of Chi-squared random variables
g2
(cid:96)2~n‚àí 1‚â§ ‚àÜ 
holds with probability at least 1‚àí 2e‚àín ‚àÜ2
√Ø u‚â§ 1‚àönœâC‚à©Bd+ Œ∑  
aT√Ø√ØI‚àí w‚àó(w‚àó)T
1‚àön
w‚àó2
6 and (6.5) with Œ∑= Œ¥
holds with probability at least 1‚àí e‚àí Œ∑2
2 . Plugging (6.4) with ‚àÜ= Œ¥
Œ¥2 œâ2C‚à©Bd  then
(6.3)  as long as n‚â• 36
uT 1
‚àó‚â§ Œ¥
2w
X T X‚àí I w
‚àó(cid:96)2
holds with probability at least 1‚àí 3e‚àín Œ¥2
parallel/perpendicular to that of w‚àó and arrive at
‚àóxi  u=RRRRRRRRRRR(uT w
xi  w‚àóxi  w‚àó
 1
+ 1
‚àó) 1
nQ
nQ
w‚àó2
i=1
i=1
RRRRRRRRRRR+ 1
RRRRRRRRRRR 1
xi  w‚àóxi  w‚àó
‚â§w
‚àó(cid:96)2
nQ
w‚àó2
i=1
with u‚ä•=I‚àí w
 u. Now note that
xi w
‚àóxi w
‚àó
‚àó(w
‚àó)T
w‚àó2
w‚àó2
xi  w‚àóxi  w‚àó
‚â§ c 

w‚àó2
RRRRRRRRRRR‚â§ t 
RRRRRRRRRRR 1
xi  w‚àóxi  w‚àó
nQ
w‚àó2
i=1

‚àóxi  u‚ä•RRRRRRRRRRR  
xi  w
nQ
i=1
‚àóxi  u‚ä• . (6.7)
xi  w
nQ
i=1

We now focus on bounding the second term in (6.2). To this aim we decompose u in the direction

with Ô¨Åxed numerical constant. Thus by Bernstein‚Äôs type inequality ([24][Proposition 5.16])

(6.5)

‚àön into

xi  w

is sub-exponential and

u‚ààC‚à©Bd

sup

(cid:96)2

(cid:96)2

(cid:96)2

 

(6.6)

8 . Also 

(cid:96)2

n

288 .

n

n

(cid:96)2

œà1

n

(cid:96)2

6

(6.8)

sup

n

n

n

(cid:96)2

7

1
n

n

(cid:96)2

sup

8 and

Furthermore  1

xi  w

xi  w

2 . Combining the last two inequalities we conclude that

xi  w‚àó2 1‚àöng  u‚ä•.
nQ
i=1
  holds with probability at least 1‚àí 2e‚àín ‚àÜ2

holds with probability at least 1‚àí 2e‚àíŒ≥n mint2 t with Œ≥ a Ô¨Åxed numerical constant.. Also note that

‚àóxi  u‚ä•‚àº¬ø``(cid:192) 1
nQ
i=1
i=1xi  w‚àó2‚â§(1+ ‚àÜ)w‚àó2
n‚àën
g  u‚ä•‚â§(2œâC‚à© Sd‚àí1+ Œ∑) 
u‚ààC‚à©Sd‚àí1
holds with probability at least 1‚àí e‚àí Œ∑2
‚àóxi  u‚ä•‚â§‚àö
(2œâC‚à© Sd‚àí1+ Œ∑)
‚àön
1+ ‚àÜ
holds with probability at least 1‚àí 2e‚àín ‚àÜ2
8 ‚àí e‚àí Œ∑2
2 . Plugging (6.8) and (6.9) with t= Œ¥
Œ∑= Œ¥
‚àö
‚àóxi  u‚â§ Œ¥
xi  w
2w
8 as long as n‚â• 288 œâ2C‚à©Sd‚àí1
‚àó‚â§ 2Œ¥w
‚àó)xi‚àí w

holds with probability at least 1‚àí 3e‚àíŒ≥nŒ¥2‚àí 2e‚àí n
and (6.10) into (6.1) we conclude that for Œ¥= 7~400
w1‚àí w
‚àó(cid:96)2
ReLU(xi  w
holds with probability at least 1‚àí 8e‚àíŒ≥n as long as n‚â• cœâ2C‚à© Sd‚àí1 for a Ô¨Åxed numerical constant

 1
nQ
i=1
‚àön into (6.7)

nQ
i=1
uT 2

‚àó(cid:96)2‚â§ 7
200w

6  ‚àÜ= 1  and

‚àó(cid:96)2‚â§ 2‚ãÖ

u‚ààC‚à©Bd

sup

. Thus pluggin (6.6)

‚àó(cid:96)2

‚àó(cid:96)2

w

nQ
i=1

 1

(6.10)

(6.9)

6

2

 

Œ¥2

n

n

n

 

 

projected gradient descent update wœÑ+1=PK(wœÑ‚àí‚àáL(wœÑ))  the error hœÑ= wœÑ‚àí w‚àó obeys

To prove Theorem 3.1 we use [21][Page 25  inequality (7.34)] which shows that if we apply the

200

.

(6.11)

(6.13)

We will instead prove that the following stronger result holds for all u‚ààC‚à©Bn and w‚àà E()

.

(6.12)

‚àó(cid:96)2 with = 7
‚àó(hœÑ‚àí‚àáL(wœÑ)) .
‚àó(cid:96)2
4wœÑ‚àí w
‚àó(cid:96)2

.

c. To introduce our general convergence analysis we begin by deÔ¨Åning

u

u

sup

sup

u‚ààC‚à©Bn

‚àó(cid:96)2‚â§ w

To complete the convergence analysis it is then sufÔ¨Åcient to prove

E()=w‚àà Rd‚à∂R(w)‚â§R(w
hœÑ+1(cid:96)2=wœÑ+1‚àí w

‚àó)  w‚àí w
‚àó(cid:96)2‚â§ 2‚ãÖ
u‚ààC‚à©Bn
‚àó(hœÑ‚àí‚àáL(wœÑ))‚â§ 1
4hœÑ(cid:96)2= 1
4w‚àí w
‚àó‚àí‚àáL(w))‚â§ 1
‚àó(w‚àí w
(6.13). To this aim note that ReLU(xi  w)= xi w+xi w
‚àáL(w)  u= 1
sgn(xi  w
‚àóxi  u+ 1
xi  w‚àí w
nQ
nQ
i=1
i=1
(sgn(xi  w)‚àí sgn(xi  w
‚àó))xi  w‚àí w
+ 1
nQ
i=1
‚àó))(sgn(xi  w
(1‚àí sgn(xi  w
+ 1
nQ
i=1

paper [20] for more detailed derivation of the identity below)

2n

u

n

n

n

2

‚àó)xi  w‚àí w
‚àóxi  u
‚àóxi  u
‚àó)‚àí sgn(xi  w))xi  w

‚àóxi  u

The equation (6.13) above implies (6.12) which when combined with (6.11) proves the convergence
result of the Theorem (speciÔ¨Åcally equation (3.5)). The rest of this section is dedicated to proving
. Thus (see the extended version of this

8

1
n

n

n

nQ
i=1

n

n

(cid:96)2

2n

‚àó))

+ 1
+ 1

n

n

the form (see the proof in the extended version of this paper [20] for more detailed derivation)

‚àó)xi  hxi  u 
‚àó)) sgn(xi  w
‚àó))(1‚àí sgn(xi  w)sgn(xi  w

(6.14)
We now proceed by stating bounds on each of the four terms in (6.14). The detailed derivation of
these bounds appear in the the extended version of this paper [20].

Now deÔ¨Åning h= w‚àí w‚àó we conclude thatu  w‚àí w‚àó‚àí‚àáL(w)=u  h‚àí‚àáL(w) is equal to
sgn(xi  w
XX T h‚àí 1
u  h‚àí‚àáL(w)=uTI‚àí 1
+h  w‚àó
(1‚àí sgn(xi  w)sgn(xi  w
‚àó)xi  hxi  u 
nQ
w‚àó2
i=1
+ sgn(xi  w)
(1‚àí sgn(xi  w
nQ
i=1
‚àóxi  u.
xi  w
Now deÔ¨Åne h‚ä•= h‚àí(hT w‚àó)~(w‚àó2
(cid:96)2)w‚àó. Using this we can rewrite the previous expression in
XX T h‚àí 1
u  w‚àí w
‚àó‚àí‚àáL(w)=uTI‚àí 1
sgn(xi  w
‚àó)xi  hxi  u 
nQ
i=1
‚àó)xi  h‚ä•xi  u 
(1‚àí sgn(xi  w)sgn(xi  w
‚àó)) sgn(xi  w
nQ
i=1
‚àó))+h  w‚àó
sgn(xi  w)

(1‚àí sgn(xi  w
nQ
w‚àó2
i=1
‚àó))xi  w
(1‚àí sgn(xi  w)sgn(xi  w
‚àóxi  u
Lemma 6.5 Assume the setup of Theorem 3.1. Then as long as n‚â• cn0  we have
‚àóI‚àí 1
X h‚â§ Œ¥h(cid:96)2
‚àó)xi  hxi  u‚â§ Œ¥h(cid:96)2
sgn(xi  w
1+ Œ¥√Ø√ØŒ¥+
‚àö
‚àó)xi  h‚ä•xi  u‚â§2
‚àó)) sgn(xi  w
‚àó))+h  w‚àó

(1‚àí sgn(xi  w
w‚àó2
√Ø√ØŒ¥+
‚àö
1+ Œ¥
‚àóxi  u‚â§ 4
‚àó))xi  w
(1‚àí )2
holds for all u‚ààC‚à© Sd‚àí1 and w‚àà E() with probability at least 1‚àí 9e‚àíŒ≥n.
2(1‚àí )2√Ø√ØŒ¥+
√Ø√Øw‚àí w
‚àó(cid:96)2
holds for all u‚ààC‚à© Sd‚àí1 and w‚àà E() with probability at least 1‚àí 16e‚àíŒ≥Œ¥2n‚àí(n+ 10)e‚àíŒ≥n. Using
4w‚àí w‚àó(cid:96)2
this inequality with Œ¥= 10‚àí4 and = 7~200 we conclude thatu  w‚àí w‚àó‚àí‚àáL(w)‚â§ 1
holds for all u‚ààC‚à© Sd‚àí1 and w‚àà E() with high probability.

nQ
i=1
(1‚àí sgn(xi  w)sgn(xi  w
 sgn(xi  w)
nQ
i=1
(1‚àí sgn(xi  w)sgn(xi  w

‚àó‚àí‚àáL(w)‚â§ 2√Ø√ØŒ¥+‚àö

Combining (6.15)  (6.16)  (6.17)  and (6.18) we conclude that

u  w‚àí w

1+ Œ¥1+

(6.16)

(6.17)

√Øh(cid:96)2
√Øh(cid:96)2

(6.18)

nQ
i=1

1
n

‚àó

X

n

2

u

‚àí 1

n

21
20

21
20

 

1
n

2

 

 

(cid:96)2

21
20

(cid:96)2

(6.15)

 

 

 

Acknowledgements

This work was done in part while the author was visiting the Simon‚Äôs Institute for the Theory of
Computing. M.S. would like to thank Adam Klivans and Matus Telgarsky for discussions related to
[6] and the Isotron algorithm.

9

References
[1] D. Amelunxen  M. Lotz  M. B. McCoy  and J. A. Tropp. Living on the edge: Phase transitions

in convex programs with random data. Information and Inference  2014.

[2] A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian

inputs. International Conference on Machine Learning (ICML)  2017.

[3] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805‚Äì849  2012.

[4] R. Collobert and J. Weston. A uniÔ¨Åed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference
on Machine learning  pages 160‚Äì167. ACM  2008.

[5] R. Ganti  N. Rao  R. M. Willett  and R. Nowak. Learning single index models in high dimensions.

arXiv preprint arXiv:1506.08910  2015.

[6] S. Goel  V. Kanade  A. Klivans  and J. Thaler. Reliably learning the ReLU in polynomial time.

arXiv preprint arXiv:1611.10258  2016.

[7] Y. Gordon. On Milman‚Äôs inequality and random subspaces which escape through a mesh in Rn.

Springer  1988.

[8] B. D. Haeffele and R. Vidal. Global optimality in tensor factorization  deep learning  and

beyond. arXiv preprint arXiv:1506.07540  2015.

[9] J. L. Horowitz and W. Hardle. Direct semiparametric estimation of single-index models with
discrete covariates. Journal of the American Statistical Association  91(436):1632‚Äì1640  1996.

[10] H. Ichimura. Semiparametric least squares (SLS) and weighted SLS estimation of single-index

models. Journal of Econometrics  58(1-2):71‚Äì120  1993.

[11] S. M. Kakade  V. Kanade  O. Shamir  and A. Kalai. EfÔ¨Åcient learning of generalized linear and
single index models with isotonic regression. In Advances in Neural Information Processing
Systems  pages 927‚Äì935  2011.

[12] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In

COLT  2009.

[13] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiÔ¨Åcation with deep convolutional
neural networks. In Advances in neural information processing systems  pages 1097‚Äì1105 
2012.

[14] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with ReLU activation.

arXiv preprint arXiv:1705.09886  2017.

[15] A. Mohamed  G. E. Dahl  and G. Hinton. Acoustic modeling using deep belief networks. IEEE

Transactions on Audio  Speech  and Language Processing  20(1):14‚Äì22  2012.

[16] Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv

preprint arXiv:1704.08045  2017.

[17] S. Oymak  B. Recht  and M. Soltanolkotabi. Sharp time‚Äìdata tradeoffs for linear inverse

problems. arXiv preprint arXiv:1507.04793  2015.

[18] S. Oymak and M. Soltanolkotabi. Fast and reliable parameter estimation from nonlinear

observations. arXiv preprint arXiv:1610.07108  2016.

[19] T. Poston  C-N. Lee  Y. Choie  and Y. Kwon. Local minima and back propagation. In Neural
Networks  1991.  IJCNN-91-Seattle International Joint Conference on  volume 2  pages 173‚Äì176.
IEEE  1991.

[20] M. Soltanolkotabi. Learning ReLUs via gradient descent. arXiv preprint arXiv:1705.04591 

2017.

10

[21] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample

complexity barriers via nonconvex optimization. arXiv preprint arXiv:1702.06175  2017.

[22] M. Soltanolkotabi  A. Javanmard  and J. D. Lee. Theoretical insights into the optimization

landscape of over-parameterized shallow neural networks. 07 2017.

[23] Y. Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. International Conference on Machine
Learning (ICML)  2017.

[24] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027  2010.

[25] K. Zhong  Z. Song  P. Jain  P. L. Bartlett  and I. S. Dhillon. Recovery guarantees for one-hidden-

layer neural networks. arXiv preprint arXiv:1706.03175  2017.

11

,Shane Griffith
Kaushik Subramanian
Jonathan Scholz
Charles Isbell
Andrea Thomaz
Ryan Kiros
Richard Zemel
Russ Salakhutdinov
Mahdi Soltanolkotabi
Piotr Mirowski
Matt Grimes
Mateusz Malinowski
Karl Moritz Hermann
Keith Anderson
Denis Teplyashin
Karen Simonyan
koray kavukcuoglu
Andrew Zisserman
Raia Hadsell