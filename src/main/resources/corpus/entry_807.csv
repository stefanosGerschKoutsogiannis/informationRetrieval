2019,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting,A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling  where samples from the model are weighted  by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown  it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We employ this likelihood-free importance weighting method to correct for the bias in generative models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models  suggesting reduced bias. Finally  we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks  and b) model-based policy evaluation using off-policy data.,Bias Correction of Learned Generative Models using

Likelihood-Free Importance Weighting

Aditya Grover1  Jiaming Song1  Alekh Agarwal2  Kenneth Tran2 

Ashish Kapoor2  Eric Horvitz2  Stefano Ermon1
1Stanford University  2Microsoft Research  Redmond

Abstract

A learned generative model often produces biased statistics relative to the under-
lying data distribution. A standard technique to correct this bias is importance
sampling  where samples from the model are weighted by the likelihood ratio
under model and true distributions. When the likelihood ratio is unknown  it can
be estimated by training a probabilistic classiﬁer to distinguish samples from the
two distributions. We show that this likelihood-free importance weighting method
induces a new energy-based model and employ it to correct for the bias in existing
models. We ﬁnd that this technique consistently improves standard goodness-of-ﬁt
metrics for evaluating the sample quality of state-of-the-art deep generative mod-
els  suggesting reduced bias. Finally  we demonstrate its utility on representative
applications in a) data augmentation for classiﬁcation using generative adversarial
networks  and b) model-based policy evaluation using off-policy data.

1

Introduction

Learning generative models of complex environments from high-dimensional observations is a long-
standing challenge in machine learning. Once learned  these models are used to draw inferences and
to plan future actions. For example  in data augmentation  samples from a learned model are used to
enrich a dataset for supervised learning [1]. In model-based off-policy policy evaluation (henceforth
MBOPE)  a learned dynamics model is used to simulate and evaluate a target policy without real-world
deployment [2]  which is especially valuable for risk-sensitive applications [3]. In spite of the recent
successes of deep generative models  existing theoretical results show that learning distributions in an
unbiased manner is either impossible or has prohibitive sample complexity [4  5]. Consequently  the
models used in practice are inherently biased 1 and can lead to misleading downstream inferences.
In order to address this issue  we start from the observation that many typical uses of generative
models involve computing expectations under the model. For instance  in MBOPE  we seek to ﬁnd
the expected return of a policy under a trajectory distribution deﬁned by this policy and a learned
dynamics model. A classical recipe for correcting the bias in expectations  when samples from
a different distribution than the ground truth are available  is to importance weight the samples
according to the likelihood ratio [6]. If the importance weights were exact  the resulting estimates are
unbiased. But in practice  the likelihood ratio is unknown and needs to be estimated since the true
data distribution is unknown and even the model likelihood is intractable or ill-deﬁned for many deep
generative models  e.g.  variational autoencoders [7] and generative adversarial networks [8].
Our proposed solution to estimate the importance weights is to train a calibrated  probabilistic
classiﬁer to distinguish samples from the data distribution and the generative model. As shown in
prior work  the output of such classiﬁers can be used to extract density ratios [9]. Appealingly  this
estimation procedure is likelihood-free since it only requires samples from the two distributions.

1We call a generative model biased if it produces biased statistics relative to the true data distribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Together  the generative model and the importance weighting function (speciﬁed via a binary classiﬁer)
induce a new energy function. While exact density estimation and sampling from this induced energy-
based model is intractable  we can derive a particle based approximation which permits efﬁcient
sampling via resampling based methods. We derive conditions on the quality of the weighting
function such that the induced model provably improves the ﬁt to the the data distribution.
Empirically  we evaluate our bias reduction framework on three main sets of experiments. First  we
consider goodness-of-ﬁt metrics for evaluating sample quality metrics of a likelihood-based and a
likelihood-free state-of-the-art (SOTA) model on the CIFAR-10 dataset. All these metrics are deﬁned
as Monte Carlo estimates from the generated samples. By importance weighting samples  we observe
a bias reduction of 23.35% and 13.48% averaged across commonly used sample quality metrics on
PixelCNN++ [10] and SNGAN [11] models respectively.
Next  we demonstrate the utility of our approach on the task of data augmentation for multi-class
classiﬁcation on the Omniglot dataset [12]. We show that  while naively extending the model with
samples from a data augmentation  a generative adversarial network [1] is not very effective for multi-
class classiﬁcation  we can improve classiﬁcation accuracy from 66.03% to 68.18% by importance
weighting the contributions of each augmented data point.
Finally  we demonstrate bias reduction for MBOPE [13]. A typical MBOPE approach is to ﬁrst
estimate a generative model of the dynamics using off-policy data and then evaluate the policy via
Monte Carlo [2  14]. Again  we observe that correcting the bias of the estimated dynamics model via
importance weighting reduces RMSE for MBOPE by 50.25% on 3 MuJoCo environments [15].

2 Preliminaries

Notation. Unless explicitly stated otherwise  we assume that probability distributions admit abso-
lutely continuous densities on a suitable reference measure. We use uppercase notation X  Y  Z to
denote random variables and lowercase notation x  y  z to denote speciﬁc values in the corresponding
sample spaces X  Y Z. We use boldface for multivariate random variables and their vector values.
Background. Consider a ﬁnite dataset Dtrain of instances x drawn i.i.d. from a ﬁxed (unknown)
distribution pdata. Given Dtrain  the goal of generative modeling is to learn a distribution p✓ to
approximate pdata. Here  ✓ denotes the model parameters  e.g. weights in a neural network for deep
generative models. The parameters can be learned via maximum likelihood estimation (MLE) as in
the case of autoregressive models [16]  normalizing ﬂows [17]  and variational autoencoders [7  18] 
or via adversarial training e.g.  using generative adversarial networks [8  19] and variants.
Monte Carlo Evaluation We are interested in use cases where the goal is to evaluate or optimize
expectations of functions under some distribution p (either equal or close to the data distribution
pdata). Assuming access to samples from p as well some generative model p✓  one extreme is to
evaluate the sample average using the samples from p alone. However  this ignores the availability of
p✓  through which we have a virtually unlimited access of generated samples ignoring computational
constraints and hence  could improve the accuracy of our estimates when p✓ is close to p. We begin
by presenting a direct motivating use case of data augmentation using generative models for training
classiﬁers which generalize better.
Example Use Case: Sufﬁcient labeled training data for learning classiﬁcation and regression system
is often expensive to obtain or susceptible to noise. Data augmentation seeks to overcome this
shortcoming by artiﬁcially injecting new datapoints into the training set. These new datapoints are
derived from an existing labeled dataset  either by manual transformations (e.g.  rotations  ﬂips for
images)  or alternatively  learned via a generative model [1  20].
Consider a supervised learning task over a labeled dataset Dcl. The dataset consists of feature and
label pairs (x  y)  each of which is assumed to be sampled independently from a data distribution
pdata(x  y) deﬁned over X⇥Y . Further  let Y✓ Rk. In order to learn a classiﬁer f : X! Rk
with parameters   we minimize the expectation of a loss ` : Y⇥ Rk ! R over the dataset Dcl:

Epdata(x y)[`(y  f (x))] ⇡

1

|Dcl| X(x y)⇠Dcl

2

`(y  f (x)).

(1)

E.g.  ` could be the cross-entropy loss. A generative model for the task of data augmentation learns a
joint distribution p✓(x  y). Several algorithmic variants exist for learning the model’s joint distribution
and we defer the speciﬁcs to the experiments section. Once the generative model is learned  it can be
used to optimize the expected classiﬁcation loss in Eq. (1) under a mixture distribution of empirical
data distributions and generative model distributions given as:

pmix(x  y) = mpdata(x  y) + (1  m)p✓(x  y)

(2)
for a suitable choice of the mixture weights m 2 [0  1]. Notice that  while the eventual task here
is optimization  reliably evaluating the expected loss of a candidate parameter is an important
ingredient. We focus on this basic question ﬁrst in advance of leveraging the solution for data
augmentation. Further  even if evaluating the expectation once is easy  optimization requires us to do
repeated evaluation (for different values of ) which is signiﬁcantly more challenging. Also observe
that the distribution p under which we seek expectations is same as pdata here  and we rely on the
generalization of p✓ to generate transformations of an instance in the dataset which are not explicitly
present  but plausibly observed in other  similar instances [21].

3 Likelihood-Free Importance Weighting

Whenever the distribution p  under which we seek expectations  differs from p✓  model-based
estimates exhibit bias. In this section  we start out by formalizing bias for Monte Carlo expectations
and subsequently propose a bias reduction strategy based on likelihood-free importance weighting
(LFIW). We are interested in evaluating expectations of a class of functions of interest f 2F w.r.t.
the distribution p. For any given f : X! R  we have Ex⇠p[f (x)] =R p(x)f (x)dx.
Given access to samples from a generative model p✓  if we knew the densities for both p and p✓ 
then a classical scheme to evaluate expectations under p using samples from p✓ is to use importance
sampling [6]. We reweight each sample from p✓ according to its likelihood ratio under p and p✓ and
compute a weighted average of the function f over these samples.

Ex⇠p[f (x)] = Ex⇠p✓ p(x)

p✓(x)

f (x) ⇡

1
T

TXi=1

w(xi)f (xi)

(3)

where w(xi) := p(xi)/p✓(xi) is the importance weight for xi ⇠ p✓. The validity of this procedure
is subject to the use of a proposal p✓ such that for all x 2X where p✓(x) = 0  we also have
f (x)p(x) = 0.2
To apply this technique to reduce the bias of a generative sampler p✓ w.r.t. p  we require knowledge
of the importance weights w(x) for any x ⇠ p✓. However  we typically only have a sampling access
to p via ﬁnite datasets. For instance  in the data augmentation example above  where p = pdata  the
unknown distribution used to learn p✓. Hence we need a scheme to learn the weights w(x)  using
samples from p and p✓  which is the problem we tackle next.In order to do this  we consider a binary
classiﬁcation problem over X⇥Y where Y = {0  1} and the joint distribution is denoted as q(x  y).
Let  = q(y=0)
q(y=1) > 0 denote any ﬁxed odds ratio. To specify the joint q(x  y)  we additionally need
the conditional q(x|y) which we deﬁne as follows:

q(x|y) =⇢p✓(x) if y = 0

p(x) otherwise.

(4)

Since we only assume sample access to p and p✓(x)  our strategy would be to estimate the conditional
above via learning a probabilistic binary classiﬁer. To train the classiﬁer  we only require datasets
of samples from p✓(x) and p(x) and estimate  to be the ratio of the size of two datasets. Let
c : X! [0  1] denote the probability assigned by the classiﬁer with parameters  to a sample x
belonging to the positive class y = 1. As shown in prior work [9  22]  if c is Bayes optimal  then
the importance weights can be obtained via this classiﬁer as:

w(x) =

p(x)
p✓(x)

= 

c(x)
1  c(x)

.

(5)

2A stronger sufﬁcient  but not necessary condition that is independent of f  states that the proposal p✓ is

valid if it has a support larger than p  i.e.  for all x 2X   p✓(x) = 0 implies p(x) = 0.

3

(a) Setup

(b) n = 50

(c) n = 100

(d) n = 1000

Figure 1: Importance Weight Estimation using Probabilistic Classiﬁers. (a) A univariate Gaussian
(blue) is ﬁt to samples from a mixture of two Gaussians (red). (b-d) Estimated class probabilities
(with 95% conﬁdence intervals based on 1000 bootstraps) for varying number of points n  where n is
the number of points used for training the generative model and multilayer perceptron.

In practice  we do not have access to a Bayes optimal classiﬁer and hence  the estimated importance
weights will not be exact. Consequently  we can hope to reduce the bias as opposed to eliminating it
entirely. Hence  our default LFIW estimator is given as:

Ex⇠p[f (x)] ⇡

1
T

TXi=1

ˆw(xi)f (xi)

(6)

1c(xi) is the importance weight for xi ⇠ p✓ estimated via c(x).

where ˆw(xi) =  c(xi)
Practical Considerations. Besides imperfections in the classiﬁer  the quality of a generative model
also dictates the efﬁcacy of importance weighting. For example  images generated by deep generative
models often possess distinct artifacts which can be exploited by the classiﬁer to give highly-conﬁdent
predictions [23  24]. This could lead to very small importance weights for some generated images 
and consequently greater relative variance in the importance weights across the Monte Carlo batch.
Below  we present some practical variants of LFIW estimator to offset this challenge.

1. Self-normalization: The self-normalized LFIW estimator for Monte Carlo evaluation normalizes
the importance weights across a sampled batch:

Ex⇠p[f (x)] ⇡

TXi=1

ˆw(xi)
j=1 ˆw(xj)

f (xi) where xi ⇠ p✓.

(7)

2. Flattening: The ﬂattened LFIW estimator interpolates between the uniform importance weights
and the default LFIW weights via a power scaling parameter ↵  0:

PT

TXi=1

Ex⇠p[f (x)] ⇡

1
T

ˆw(xi)↵f (xi) where xi ⇠ p✓.

(8)

For ↵ = 0  there is no bias correction  and ↵ = 1 returns the default estimator in Eq. (6). For
intermediate values of ↵  we can trade-off bias reduction with any undesirable variance introduced.
3. Clipping: The clipped LFIW estimator speciﬁes a lower bound   0 on the importance weights:
(9)

max( ˆw(xi)  )f (xi) where xi ⇠ p✓.

Ex⇠p[f (x)] ⇡

1
T

TXi=1

When  = 0  we recover the default LFIW estimator in Eq. (6). Finally  we note that these estimators
are not exclusive and can be combined e.g.  ﬂattened or clipped weights can be normalized.

Conﬁdence intervals. Since we have real and generated data coming from a ﬁnite dataset and
parametric model respectively  we propose a combination of empirical and parametric bootstraps to
derive conﬁdence intervals around the estimated importance weights. See Appendix A for details.
Synthetic experiment. We visually illustrate our importance weighting approach in a toy experiment
(Figure 1a). We are given a ﬁnite set of samples drawn from a mixture of two Gaussians (red). The
model family is a unimodal Gaussian  illustrating mismatch due to a parametric model. The mean

4

Algorithm 1 SIR for the Importance Resampled Energy-Based Model p✓ 

Input: Generative Model p✓  Importance Weight Estimator ˆw  budget T

1: Sample x1  x2  . . .   xT independently from p✓
2: Estimate importance weights ˆw(x1)  ˆw(x2)  . . .   ˆw(xT )

3: Compute ˆZ PT
4: Sample j ⇠ Categorical⇣ ˆw(x1)

5: return xj

t=1 ˆw(xt)

ˆZ

  ˆw(x2)

ˆZ

  . . .   ˆw(xT )

ˆZ ⌘

and variance of the model are estimated by the empirical means and variances of the observed data.
Using estimated model parameters  we then draw samples from the model (blue).
In Figure 1b  we show the probability assigned by a binary classiﬁer to a point to be from true data
distribution. Here  the classiﬁer is a single hidden-layer multi-layer perceptron. The classiﬁer is not
Bayes optimal  which can be seen by the gaps between the optimal probabilities curve (black) and the
estimated class probability curve (green). However  as we increase the number of real and generated
examples n in Figures 1c-d  the classiﬁer approaches optimality. Furthermore  even its uncertainty
shrinks with increasing data  as expected. In summary  this experiment demonstrates how a binary
classiﬁer can mitigate this bias due to a mismatched generative model.

4

Importance Resampled Energy-Based Model

In the previous section  we described a procedure to augment any base generative model p✓ with
an importance weighting estimator ˆw for debiased Monte Carlo evaluation. Here  we will use this
augmentation to induce an importance resampled energy-based model with density p✓  given as:

(10)

p✓ (x) / p✓(x) ˆw(x)

where the partition function is expressed as Z✓  =R p✓(x) ˆw(x)dx = Ep✓ [ ˆw(x)].

Density Estimation. Exact density estimation requires a handle on the density of the base model p✓
(typically intractable for models such as VAEs and GANs) and estimates of the partition function.
Exactly computing the partition function is intractable. If p✓ permits fast sampling and importance
weights are estimated via LFIW (requiring only a forward pass through the classiﬁer network) 
we can obtain unbiased estimates via a Monte Carlo average  i.e.  Z✓  ⇡ 1
i=1 ˆw(xi) where
xi ⇠ p✓. To reduce the variance  a potentially large number of samples are required. Since samples
are obtained independently  the terms in the Monte Carlo average can be evaluated in parallel.
Sampling-Importance-Resampling. While exact sampling from p✓  is intractable  we can instead
perform sample from a particle-based approximation to p✓  via sampling-importance-resampling [25 
26] (SIR). We deﬁne the SIR approximation to p✓  via the following density:

T PT

✓  (x; T ) := Ex2 x3 ... xT ⇠p✓"

pSIR

ˆw(x)

ˆw(x) +PT

i=2 ˆw(xi)

p✓(x)#

(11)

(12)

where T > 0 denotes the number of independent samples (or “particles"). For any ﬁnite T   sampling
from pSIR
✓  is tractable  as summarized in Algorithm 1. Moreover  any expectation w.r.t. the SIR
approximation to the induced distribution can be evaluated in closed-form using the self-normalized
LFIW estimator (Eq. 7). In the limit of T ! 1  we recover the induced distribution p✓ :

lim
T!1

pSIR
✓  (x; T ) = p✓ (x) 8x

Next  we analyze conditions under which the resampled density p✓  provably improves the model ﬁt
to pdata. In order to do so  we further assume that pdata is absolutely continuous w.r.t. p✓ and p✓ .
We deﬁne the change in KL via the importance resampled density as:

(pdata  p✓  p✓ ) := DKL(pdata  p✓ )  DKL(pdata  p✓).

Substituting Eq. 10 in Eq. 13  we can simplify the above quantity as:

(pdata  p✓  p✓ ) = Ex⇠pdata[ log(p✓(x) ˆw(x)) + log Z✓  + log p✓(x)]

= Ex⇠pdata[log ˆw(x)]  log Ex⇠p✓ [ ˆw(x)].

(13)

(14)
(15)

5

Table 1: Goodness-of-ﬁt evaluation on CIFAR-10 dataset for PixelCNN++ and SNGAN. Standard
errors computed over 10 runs. Higher IS is better. Lower FID and KID scores are better.

Model
-
PixelCNN++ Default (no debiasing)

Evaluation
Reference

SNGAN

LFIW
Default (no debiasing)
LFIW

IS (")

11.09 ± 0.1263
5.16 ± 0.0117
6.68 ± 0.0773
8.33 ± 0.0280
8.57 ± 0.0325

FID (#)

5.20 ± 0.0533
58.70 ± 0.0506
55.83 ± 0.9695
20.40 ± 0.0747
17.29 ± 0.0698

KID (#)

0.008 ± 0.0004
0.196 ± 0.0001
0.126 ± 0.0009
0.094 ± 0.0002
0.073 ±0.0004

The above expression provides a necessary and sufﬁcient condition for any positive real valued
function (such as the LFIW classiﬁer in Section 3) to improve the KL divergence ﬁt to the underlying
data distribution. In practice  an unbiased estimate of the LHS can be obtained via Monte Carlo
averaging of log- importance weights based on Dtrain. The empirical estimate for the RHS is however
biased.3 To remedy this shortcoming  we consider the following necessary but insufﬁcient condition.
Proposition 1. If (pdata  p✓  p✓ )  0  then the following conditions hold:

Ex⇠pdata[ ˆw(x)]  Ex⇠p✓ [ ˆw(x)] 

Ex⇠pdata[log ˆw(x)]  Ex⇠p✓ [log ˆw(x)].

(16)
(17)

The conditions in Eq. 16 and Eq. 17 follow directly via Jensen’s inequality applied to the LHS and
RHS of Eq. 15 respectively. Here  we note that estimates for the expectations in Eqs. 16-17 based on
Monte Carlo averaging of (log-) importance weights are unbiased.

5 Application Use Cases

In all our experiments  the binary classiﬁer for estimating the importance weights was a calibrated
deep neural network trained to minimize the cross-entropy loss. The self-normalized LFIW in Eq. (7)
worked best. Additional analysis on the estimators and experiment details are in Appendices B and C.

5.1 Goodness-of-ﬁt testing

In the ﬁrst set of experiments  we highlight the beneﬁts of importance weighting for a debiased
evaluation of three popularly used sample quality metrics viz. Inception Scores (IS) [27]  Frechet
Inception Distance (FID) [28]  and Kernel Inception Distance (KID) [29]. All these scores can be
formally expressed as empirical expectations with respect to the model. For all these metrics  we can
simulate the population level unbiased case as a “reference score" wherein we artiﬁcially set both the
real and generated sets of samples used for evaluation as ﬁnite  disjoint sets derived from pdata.
We evaluate the three metrics for two state-of-the-art models trained on the CIFAR-10 dataset viz.
an autoregressive model PixelCNN++ [10] learned via maximum likelihood estimation and a latent
variable model SNGAN [11] learned via adversarial training. For evaluating each metric  we draw
10 000 samples from the model. In Table 1  we report the metrics with and without the LFIW bias
correction. The consistent debiased evaluation of these metrics via self-normalized LFIW suggest
that the SIR approximation to the importance resampled distribution (Eq. 11) is a better ﬁt to pdata.

5.2 Data Augmentation for Multi-Class Classiﬁcation

We consider data augmentation via Data Augmentation Generative Adversarial Networks (DA-
GAN) [1]. While DAGAN was motivated by and evaluated for the task of meta-learning  it can also
be applied for multi-class classiﬁcation scenarios  which is the setting we consider here. We trained a
DAGAN on the Omniglot dataset of handwritten characters [12]. The DAGAN training procedure is
described in the Appendix. The dataset is particularly relevant because it contains 1600+ classes but
only 20 examples from each class and hence  could potentially beneﬁt from augmented data.

3If ˆZ is an unbiased estimator for Z  then log ˆZ is a biased estimator for log Z via Jensen’s inequality.

6

(a)

(d)

(b)

(e)

(c)

(f)

Figure 2: Qualitative evaluation of importance weighting for data augmentation. (a-f) Top row shows
held-out data samples from a speciﬁc class in Omniglot. Bottom row shows generated samples from
the same class ranked in decreasing order of importance weights.
Table 2: Classiﬁcation accuracy on the Omniglot dataset. Standard errors computed over 5 runs.

Dataset
Accuracy

Dcl

Dg

0.6603 ± 0.0012

0.4431 ± 0.0054

Dg w/ LFIW
0.4481 ± 0.0056

Dcl + Dg

0.6600 ± 0.0040

Dcl + Dg w/ LFIW
0.6818 ± 0.0022

Once the model has been trained  it can be used for data augmentation in many ways. In particular  we
consider ablation baselines that use various combinations of the real training data Dcl and generated
data Dg for training a downstream classiﬁer. When the generated data Dg is used  we can either
use the data directly with uniform weighting for all training points  or choose to importance weight
(LFIW) the contributions of the individual training points to the overall loss. The results are shown in
Table 2. While generated data (Dg) alone cannot be used to obtain competitive performance relative
to the real data (Dcl) on this task as expected  the bias it introduces for evaluation and subsequent
optimization overshadows even the naive data augmentation (Dcl + Dg). In contrast  we can obtain
signiﬁcant improvements by importance weighting the generated points (Dcl + Dg w/ LFIW).
Qualitatively  we can observe the effect of importance weighting in Figure 2. Here  we show true
and generated samples for 6 randomly choosen classes (a-f) in the Omniglot dataset. The generated
samples are ranked in decreasing order of the importance weights. There is no way to formally test
the validity of such rankings and this criteria can also prefer points which have high density under
pdata but are unlikely under p✓ since we are looking at ratios. Visual inspection suggests that the
classiﬁer is able to appropriately downweight poorer samples  as shown in Figure 2 (a  b  c  d - bottom
right). There are also failure modes  such as the lowest ranked generated images in Figure 2 (e  f -
bottom right) where the classiﬁer weights reasonable generated samples poorly relative to others.
This could be due to particular artifacts such as a tiny disconnected blurry speck in Figure 2 (e -
bottom right) which could be more revealing to a classiﬁer distinguishing real and generated data.

5.3 Model-based Off-policy Policy Evaluation
So far  we have seen use cases where the generative model was trained on data from the same
distribution we wish to use for Monte Carlo evaluation. We can extend our debiasing framework to
more involved settings when the generative model is a building block for specifying the full data
generation process  e.g.  trajectory data generated via a dynamics model along with an agent policy.
In particular  we consider the setting of off-policy policy evaluation (OPE)  where the goal is to
evaluate policies using experiences collected from a different policy. Formally  let (S A  r  P ⌘  T )
denote an (undiscounted) Markov decision process with state space S  action space A  reward
function r  transition P   initial state distribution ⌘ and horizon T . Assume ⇡e : S⇥A! [0  1]
is a known policy that we wish to evaluate. The probability of generating a certain trajectory
⌧ = {s0  a0  s1  a1  ...  sT   aT} of length T with policy ⇡e and transition P is given as:

p?(⌧ ) = ⌘(s0)

⇡e(at|st)P (st+1|st  at).

(18)

The return on a trajectory R(⌧ ) is the sum of the rewards across the state  action pairs in ⌧: R(⌧ ) =

t=1 r(st  at)  where we assume a known reward function r.

PT

T1Yt=0

7

Table 3: Off-policy policy evaluation on MuJoCo tasks. Standard error is over 10 Monte Carlo
estimates where each estimate contains 100 randomly sampled trajectories.

Environment
Swimmer
HalfCheetah
HumanoidStandup

v(⇡e) (Ground truth)

˜v(⇡e)

ˆv(⇡e) (w/ LFIW)

ˆv80(⇡e) (w/ LFIW)

36.7 ± 0.1
241.7 ± 3.56
14170 ± 53

100.4 ± 3.2
204.0 ± 0.8
8417 ± 28

25.7 ± 3.1
217.8 ± 4.0
9372 ± 375

47.6 ± 4.8
219.1 ± 1.6
9221 ± 381

Figure 3: Estimation error (v) = v(⇡e)  ˆvH(⇡e) for different values of H (minimum 0  maximum
100). Shaded area denotes standard error over different random seeds.

We are interested in the value of a policy deﬁned as v(⇡e) = E⌧⇠p⇤(⌧ ) [R(⌧ )]. Evaluating ⇡e requires
the (unknown) transition dynamics P . The dynamics model is a conditional generative model of
the next states st+1 conditioned on the previous state-action pair (st  at). If we have access to
historical logged data D⌧ of trajectories ⌧ = {s0  a0  s1  a1  . . .  } from some behavioral policy
⇡b : S⇥A! [0  1]  then we can use this off-policy data to train a dynamics model P✓(st+1|st  at).
The policy ⇡e can then be evaluated under this learned dynamics model as ˜v(⇡e) = E⌧⇠˜p(⌧ )[R(⌧ )] 
where ˜p uses P✓ instead of the true dynamics in Eq. (18).
However  the trajectories sampled with P✓ could signiﬁcantly deviate from samples from P due to
compounding errors [30]. In order to correct for this bias  we can use likelihood-free importance
weighting on entire trajectories of data. The binary classiﬁer c(st  at  st+1) for estimating the
importance weights in this case distinguishes between triples of true and generated transitions.
For any true triple (st  at  st+1) extracted from the off-policy data  the corresponding generated
triple (st  at  ˆst+1) only differs in the ﬁnal transition state  i.e.  ˆst+1 ⇠ P✓(ˆst+1|st  at). Such a
classiﬁer allows us to obtain the importance weights ˆw(st  at  ˆst+1) for every predicted state transition
(st  at  ˆst+1). The importance weights for the trajectory ⌧ can be derived from the importance weights
of these individual transitions as:

p?(⌧ )
˜p(⌧ )

= QT1
QT1

Our ﬁnal LFIW estimator is given as:

=

t=0 P (st+1|st  at)
t=0 P✓(st+1|st  at)

T1Yt=0
T1Yt=0
P (st+1|st  at)
P✓(st+1|st  at) ⇡
ˆw(st  at  ˆst+1) · R(⌧ )# .
ˆv(⇡e) = E⌧⇠˜p(⌧ )"T1Yt=0

ˆw(st  at  ˆst+1).

(19)

(20)

We consider three continuous control tasks in the MuJoCo simulator [15] from OpenAI gym [31]
(in increasing number of state dimensions): Swimmer  HalfCheetah and HumanoidStandup. High
dimensional state spaces makes it challenging to learning a reliable dynamics model in these environ-
ments. We train behavioral and evaluation policies using Proximal Policy Optimization [32] with
different hyperparameters for the two policies. The dataset collected via trajectories from the behavior
policy are used train a ensemble neural network dynamics model. We the use the trained dynamics
model to evaluate ˜v(⇡e) and its IW version ˆv(⇡e)  and compare them with the ground truth returns
v(⇡e). Each estimation is averaged over a set of 100 trajectories with horizon T = 100. Speciﬁcally 
for ˆv(⇡e)  we also average the estimation over 10 classiﬁer instances trained with different random
seeds on different trajectories. We further consider performing IW over only the ﬁrst H steps  and
use uniform weights for the remainder  which we denote as ˆvH(⇡e). This allow us to interpolate
between ˜v(⇡e) ⌘ ˆv0(⇡e) and ˆv(⇡e) ⌘ ˆvT (⇡e). Finally  as in the other experiments  we used the
self-normalized variant (Eq. (7)) of the importance weighted estimator in Eq. (20).
We compare the policy evaluations under different environments in Table 3. These results show that
the rewards estimated with the trained dynamics model differ from the ground truth by a large margin.

8

By importance weighting the trajectories  we obtain much more accurate policy evaluations. As
expected  we also see that while LFIW leads to higher returns on average  the imbalance in trajectory
importance weights due to the multiplicative weights of the state-action pairs can lead to higher
variance in the importance weighted returns. In Figure 3  we demonstrate that policy evaluation
becomes more accurate as more timesteps are used for LFIW evaluations  until around 80  100
timesteps and thus empirically validates the beneﬁts of importance weighting using a classiﬁer. Given
that our estimates have a large variance  it would be worthwhile to compose our approach with
other variance reduction techniques such as (weighted) doubly robust estimation in future work [33] 
as well as incorporate these estimates within a framework such as MAGIC to further blend with
model-free OPE [14]. In Appendix C.5.1  we also consider a stepwise LFIW estimator for MBOPE
which applies importance weighting at the level of every decision as opposed to entire trajectories.
Overall. Across all our experiments  we observe that importance weighting the generated samples
leads to uniformly better results  whether in terms of evaluating the quality of samples  or their utility
in downstream tasks. Since the technique is a black-box wrapper around any generative model  we
expect this to beneﬁt a diverse set of tasks in follow-up works.
However  there is also some caution to be exercised with these techniques as evident from the results
of Table 1. Note that in this table  the conﬁdence intervals (computed using the reported standard
errors) around the model scores after importance weighting still do not contain the reference scores
obtained from the true model. This would not have been the case if our debiased estimator was
completely unbiased and this observation reiterates our earlier claim that LFIW is reducing bias 
as opposed to completely eliminating it. Indeed  when such a mismatch is observed  it is a good
diagnostic to either learn more powerful classiﬁers to better approximate the Bayes optimum  or ﬁnd
additional data from pdata in case the generative model fails the full support assumption.

6 Related Work & Discussion
Density ratios enjoy widespread use across machine learning e.g.  for handling covariate shifts 
class imbalance etc. [9  34]. In generative modeling  estimating these ratios via binary classiﬁers
is frequently used for deﬁning learning objectives and two sample tests [19  35  35–41]. In partic-
ular  such classiﬁers have been used to deﬁne learning frameworks such as generative adversarial
networks [8  42]  likelihood-free Approximate Bayesian Computation (ABC) [43] and earlier work
in unsupervised-as-supervised learning [44] and noise contrastive estimation [43] among others.
Recently  [45] used importance weighting to reweigh datapoints based on differences in training
and test data distributions i.e.  dataset bias. The key difference is that these works are explicitly
interested in learning the parameters of a generative model. In contrast  we use the binary classiﬁer
for estimating importance weights to correct for the model bias of any ﬁxed generative model.
Recent concurrent works [46–48] use MCMC and rejection sampling to explicitly transform or reject
the generated samples. These methods require extra computation beyond training a classiﬁer  in
rejecting the samples or running Markov chains to convergence  unlike the proposed importance
weighting strategy. For many model-based Monte Carlo evaluation usecases (e.g.  data augmentation 
MBOPE)  this extra computation is unnecessary. If samples or density estimates are explicitly needed
from the induced resampled distribution  we presented a particle-based approximation to the induced
density where the number of particles is a tunable knob allowing for trading statistical accuracy with
computational efﬁciency. Finally  we note resampling based techniques have been extensively studied
in the context of improving variational approximations for latent variable generative models [49–52].

7 Conclusion
We identiﬁed bias with respect to a target data distribution as a fundamental challenge restricting
the use of generative models as proposal distributions for Monte Carlo evaluation. We proposed a
bias correction framework based on importance weighting. Here  any base generative model can
be boosted with an importance weight estimator to induce an energy-based generative model. The
importance weights are estimated in a likelihood-free fashion via a binary classiﬁer. Empirically  we
ﬁnd the bias correction to be useful across a variety of tasks including goodness-of-ﬁt sample quality
tests  data augmentation  and off-policy policy evaluation. The ability to characterize the bias of a
generative model is an important step towards using these models to guide decisions in high-stakes
applications under uncertainty [53  54]  such as healthcare [55–57] and anomaly detection [58  59].

9

Acknowledgments

This project was initiated when AG was an intern at Microsoft Research. We are thankful to Daniel
Levy  Rui Shu  Yang Song  and members of the Reinforcement Learning  Deep Learning  and
Adaptive Systems and Interaction groups at Microsoft Research for helpful discussions and comments
on early drafts. This research was supported by NSF (#1651565  #1522054  #1733686)  ONR 
AFOSR (FA9550-19-1-0024)  and FLI.

References
[1] Antreas Antoniou  Amos Storkey  and Harrison Edwards. Data augmentation generative

adversarial networks. arXiv preprint arXiv:1711.04340  2017.

[2] Shie Mannor  Duncan Simester  Peng Sun  and John N Tsitsiklis. Bias and variance approxima-

tion in value function estimates. Management Science  53(2):308–322  2007.

[3] Philip S Thomas. Safe reinforcement learning. PhD thesis  University of Massachusetts

Libraries  2015.

[4] Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The

Annals of Mathematical Statistics  pages 832–837  1956.

[5] Sanjeev Arora  Andrej Risteski  and Yi Zhang. Do gans learn the distribution? some theory and

empirics. In International Conference on Learning Representations  2018.

[6] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement

from a ﬁnite universe. Journal of the American statistical Association  1952.

[7] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[8] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems  2014.

[9] Masashi Sugiyama  Taiji Suzuki  and Takafumi Kanamori. Density ratio estimation in machine

learning. Cambridge University Press  2012.

[10] Tim Salimans  Andrej Karpathy  Xi Chen  and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint
arXiv:1701.05517  2017.

[11] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization

for generative adversarial networks. arXiv preprint arXiv:1802.05957  2018.

[12] Brenden M Lake  Ruslan Salakhutdinov  and Joshua B Tenenbaum. Human-level concept

learning through probabilistic program induction. Science  350(6266):1332–1338  2015.

[13] Doina Precup  Richard S. Sutton  and Satinder P. Singh. Eligibility traces for off-policy policy

evaluation. In International Conference on Machine Learning  2000.

[14] Philip Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforce-

ment learning. In International Conference on Machine Learning  2016.

[15] Emanuel Todorov  Tom Erez  and Yuval Tassa. Mujoco: A physics engine for model-based

control. In International Conference on Intelligent Robots and Systems. IEEE  2012.

[16] Benigno Uria  Marc-Alexandre Côté  Karol Gregor  Iain Murray  and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research  17(1):
7184–7220  2016.

[17] Laurent Dinh  David Krueger  and Yoshua Bengio. Nice: Non-linear independent components

estimation. arXiv preprint arXiv:1410.8516  2014.

10

[18] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082  2014.

[19] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

preprint arXiv:1610.03483  2016.

[20] Alexander J Ratner  Henry Ehrenberg  Zeshan Hussain  Jared Dunnmon  and Christopher Ré.
Learning to compose domain-speciﬁc transformations for data augmentation. In Advances in
Neural Information Processing Systems  2017.

[21] Shengjia Zhao  Hongyu Ren  Arianna Yuan  Jiaming Song  Noah Goodman  and Stefano Ermon.
Bias and generalization in deep generative models: An empirical study. In Advances in Neural
Information Processing Systems  2018.

[22] Aditya Grover and Stefano Ermon. Boosted generative models. In AAAI Conference on Artiﬁcial

Intelligence  2018.

[23] Augustus Odena  Vincent Dumoulin  and Chris Olah. Deconvolution and checkerboard
artifacts. Distill  2016. doi: 10.23915/distill.00003. URL http://distill.pub/2016/
deconv-checkerboard.

[24] Augustus Odena. Open questions about generative adversarial networks. Distill  4(4):e18  2019.

[25] Jun S Liu and Rong Chen. Sequential monte carlo methods for dynamic systems. Journal of

the American statistical association  93(443):1032–1044  1998.

[26] Arnaud Doucet  Simon Godsill  and Christophe Andrieu. On sequential monte carlo sampling

methods for bayesian ﬁltering. Statistics and computing  10(3):197–208  2000.

[27] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems 
pages 2234–2242  2016.

[28] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems  pages 6626–6637  2017.

[29] Mikołaj Bi´nkowski  Dougal J Sutherland  Michael Arbel  and Arthur Gretton. Demystifying

mmd gans. arXiv preprint arXiv:1801.01401  2018.

[30] Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In International

Conference on Artiﬁcial Intelligence and Statistics  2010.

[31] Greg Brockman  Vicki Cheung  Ludwig Pettersson  Jonas Schneider  John Schulman  Jie Tang 

and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540  2016.

[32] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[33] Mehrdad Farajtabar  Yinlam Chow  and Mohammad Ghavamzadeh. More robust doubly robust

off-policy evaluation. In International Conference on Machine Learning  2018.

[34] Jonathon Byrd and Zachary C Lipton. What is the effect of importance weighting in deep

learning? arXiv preprint arXiv:1812.03372  2018.

[35] Mihaela Rosca  Balaji Lakshminarayanan  David Warde-Farley  and Shakir Mohamed. Vari-
arXiv preprint

ational approaches for auto-encoding generative adversarial networks.
arXiv:1706.04987  2017.

[36] Arthur Gretton  Karsten M Borgwardt  Malte Rasch  Bernhard Schölkopf  and Alex J Smola.
A kernel method for the two-sample-problem. In Advances in Neural Information Processing
Systems  2007.

11

[37] Samuel R Bowman  Luke Vilnis  Oriol Vinyals  Andrew M Dai  Rafal Jozefowicz  and Samy
Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 
2015.

[38] David Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. arXiv preprint

arXiv:1610.06545  2016.

[39] Ivo Danihelka  Balaji Lakshminarayanan  Benigno Uria  Daan Wierstra  and Peter Dayan.
Comparison of maximum likelihood and gan-based training of real nvps. arXiv preprint
arXiv:1705.05263  2017.

[40] Daniel Jiwoong Im  He Ma  Graham Taylor  and Kristin Branson. Quantitatively evaluating

gans with divergences proposed for training. arXiv preprint arXiv:1803.01045  2018.

[41] Ishaan Gulrajani  Colin Raffel  and Luke Metz. Towards gan benchmarks which require

generalization. In International Conference on Learning Representations  2019.

[42] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[43] Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized
statistical models  with applications to natural image statistics. Journal of Machine Learning
Research  13(Feb):307–361  2012.

[44] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The elements of statistical learning 

volume 1. Springer series in statistics New York  NY  USA:  2001.

[45] Maurice Diesendruck  Ethan R Elenberg  Rajat Sen  Guy W Cole  Sanjay Shakkottai 
arXiv preprint

Importance weighted generative networks.

and Sinead A Williamson.
arXiv:1806.02512  2018.

[46] Ryan Turner  Jane Hung  Yunus Saatci  and Jason Yosinski. Metropolis-hastings generative

adversarial networks. arXiv preprint arXiv:1811.11357  2018.

[47] Samaneh Azadi  Catherine Olsson  Trevor Darrell  Ian Goodfellow  and Augustus Odena.

Discriminator rejection sampling. arXiv preprint arXiv:1810.06758  2018.

[48] Chenyang Tao  Liqun Chen  Ricardo Henao  Jianfeng Feng  and Lawrence Carin. Chi-square

generative adversarial network. In International Conference on Machine Learning  2018.

[49] Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Importance weighted autoencoders.

arXiv preprint arXiv:1509.00519  2015.

[50] Tim Salimans  Diederik Kingma  and Max Welling. Markov chain monte carlo and variational

inference: Bridging the gap. In International Conference on Machine Learning  2015.

[51] Christian A Naesseth  Scott W Linderman  Rajesh Ranganath  and David M Blei. Variational

sequential monte carlo. arXiv preprint arXiv:1705.11140  2017.

[52] Aditya Grover  Ramki Gummadi  Miguel Lazaro-Gredilla  Dale Schuurmans  and Stefano
Ermon. Variational rejection sampling. In International Conference on Artiﬁcial Intelligence
and Statistics  2018.

[53] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model

uncertainty in deep learning. In International Conference on Machine Learning  2016.

[54] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems  2017.

[55] Matthieu Komorowski  A Gordon  LA Celi  and A Faisal. A markov decision process to suggest
optimal treatment of severe infections in intensive care. In Neural Information Processing
Systems Workshop on Machine Learning for Health  2016.

12

[56] Zhengyuan Zhou  Daniel Miller  Neal Master  David Scheinker  Nicholas Bambos  and Peter
In International

Glynn. Detecting inaccurate predictions of pediatric surgical durations.
Conference on Data Science and Advanced Analytics  2016.

[57] Aniruddh Raghu  Matthieu Komorowski  Leo Anthony Celi  Peter Szolovits  and Marzyeh
Ghassemi. Continuous state-space models for optimal sepsis treatment-a deep reinforcement
learning approach. arXiv preprint arXiv:1705.08422  2017.

[58] Eric Nalisnick  Akihiro Matsukawa  Yee Whye Teh  Dilan Gorur  and Balaji Lakshminarayanan.
Do deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136 
2018.

[59] Hyunsun Choi and Eric Jang. Generative ensembles for robust anomaly detection. arXiv

preprint arXiv:1810.01392  2018.

[60] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press  1994.
[61] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised

learning. In International Conference on Machine learning  2005.

[62] Chuan Guo  Geoff Pleiss  Yu Sun  and Kilian Q Weinberger. On calibration of modern neural

networks. In International Conference on Machine Learning  2017.

[63] Martín Abadi  Paul Barham  Jianmin Chen  Zhifeng Chen  Andy Davis  Jeffrey Dean  Matthieu
Devin  Sanjay Ghemawat  Geoffrey Irving  Michael Isard  et al. Tensorﬂow: a system for
large-scale machine learning. In Operating Systems Design and Implementation  2016.

[64] Christian Szegedy  Vincent Vanhoucke  Sergey Ioffe  Jon Shlens  and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In IEEE conference on Computer Vision and
Pattern Recognition  2016.

[65] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[66] Oriol Vinyals  Charles Blundell  Timothy Lillicrap  Daan Wierstra  et al. Matching networks

for one shot learning. In Advances in Neural Information Processing Systems  2016.

[67] Prafulla Dhariwal  Christopher Hesse  Oleg Klimov  Alex Nichol  Matthias Plappert  Alec
Radford  John Schulman  Szymon Sidor  and Yuhuai Wu. Openai baselines. GitHub  GitHub
repository  2017.

[68] Prajit Ramachandran  Barret Zoph  and Quoc V Le. Searching for activation functions. arXiv

preprint arXiv:1710.05941  2017.

13

,Aditya Grover
Jiaming Song
Ashish Kapoor
Kenneth Tran
Alekh Agarwal
Eric Horvitz
Stefano Ermon