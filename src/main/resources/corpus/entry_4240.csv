2008,Partially Observed Maximum Entropy Discrimination Markov Networks,Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive  sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored  to our knowledge  learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper  we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overfitting  but is also built on the desirable discriminative laws resemble those of the M$^3$N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M$^3$N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task.,Partially Observed Maximum Entropy

Discrimination Markov Networks

Jun Zhu†

Eric P. Xing‡

Bo Zhang†

†State Key Lab of Intelligent Tech & Sys  Tsinghua National TNList Lab  Dept. Comp Sci & Tech 
†Tsinghua University  Beijing China. jun-zhu@mails.thu.edu.cn; dcszb@thu.edu.cn
‡School of Comp. Sci.  Carnegie Mellon University  Pittsburgh  PA 15213  epxing@cs.cmu.edu

Abstract

Learning graphical models with hidden variables can offer semantic insights to
complex data and lead to salient structured predictors without relying on expen-
sive  sometime unattainable fully annotated training data. While likelihood-based
methods have been extensively explored  to our knowledge  learning structured
prediction models with latent variables based on the max-margin principle remains
largely an open problem. In this paper  we present a partially observed Maximum
Entropy Discrimination Markov Network (PoMEN) model that attempts to com-
bine the advantages of Bayesian and margin based paradigms for learning Markov
networks from partially labeled data. PoMEN leads to an averaging prediction rule
that resembles a Bayes predictor that is more robust to overﬁtting  but is also built
on the desirable discriminative laws resemble those of the M3N. We develop an
EM-style algorithm utilizing existing convex optimization algorithms for M3N as
a subroutine. We demonstrate competent performance of PoMEN over existing
methods on a real-world web data extraction task.

1 Introduction
Inferring structured predictions based on high-dimensional  often multi-modal and hybrid covari-
ates remains a central problem in data mining (e.g.  web-info extraction)  machine intelligence (e.g. 
machine translation)  and scientiﬁc discovery (e.g.  genome annotation). Several recent approaches
to this problem are based on learning discriminative graphical models deﬁned on composite fea-
tures that explicitly exploit the structured dependencies among input elements and structured in-
terpretational outputs. Different learning paradigms have been explored  including the maximum
conditional likelihood [7] and max-margin learning [2  12  13]  with remarkable success.
However  the problem of structured input/output learning can be intriguing and signiﬁcantly more
difﬁcult when there exist hidden substructures in the data  which is not uncommon in realistic prob-
lems. As is well-known in the probabilistic graphical model literature  hidden variables can facili-
tate natural incorporation of structured domain knowledge such as latent semantic concepts or unob-
served dependence hierarchies into the model  which can often result in more intuitive representation
and more compact parameterization of the model; but learning a partially observed model is often
non-trivial because it involves optimizing against a more complex cost function  which is usually
not convex and requires additional efforts to impute or marginalize out hidden variables. Most exist-
ing work along this line  such as the hidden CRF for object recognition [9] and scene segmentation
[14] and the dynamic hierarchical MRF for web data extraction [18]  falls in the likelihood-based
learning. For the max-margin learning  which is arguably a more desirable discriminative learning
paradigm in many application scenarios  learning a Makov network with hidden variables can be
extremely difﬁcult and little work has been done except [11]  where  in order to obtain a convex pro-
gram  the uncertainty in mixture modeling is simpliﬁed by a reduction using the MAP component.

1

A major reason for the difﬁculty of considering latent structures in max-margin models is the lack of
a natural probabilistic interpretation of such models  which on the other hand offers the key insight
in likelihood-based learning to design algorithms such as EM for learning partially observed mod-
els. Recent work on semi-supervised or unsupervised max-margin learning [1  4  16] was all short of
an explicit probabilistic interpretation of their algorithms of handling latent variables. The recently
proposed Maximum Entropy Discrimination Markov Networks (MaxEnDNet) [20  19] represent a
key advance in this direction. MaxEnDNet offers a general framework to combine Bayesian-style
learning and max-margin learning in structured prediction. Given a prior distribution of a structured-
prediction model  and leveraging a new prediction-rule that is based on a weighted average over an
ensemble of prediction models  MaxEnDNet adopts a structured minimum relative entropy prin-
ciple to learn a posterior distribution of the prediction model in a subspace deﬁned by a set of ex-
pected margin constraints. This elegant combination of probabilistic and maximum margin concepts
provides a natural path to incorporate hidden structured variables in learning max-margin Markov
networks (M3N)  which is the focus of this paper.
It has been shown in [20] that  in the fully observed case  MaxEnDNet subsumes the standard M3N
[12]. But MaxEnDNet in its full generality offers a number of important advantages while retaining
all the merits of the M3N. For example  structured prediction under MaxEnDNet is based on an av-
eraging model and therefore enjoys a desirable smoothing effect  with a uniform convergence bound
on generalization error  as shown in [20]; MaxEnDNet admits a prior that can be designed to intro-
duce useful regularization effects  such as a sparsity bias  as explored in the Laplace M3N [19  20].
In this paper  we explore yet another advantage of MaxEnDNet stemmed from the Bayesian-style
max-margin learning formalism on incorporating hidden variables. We present the partially ob-
served MaxEnDNet (PoMEN)  which offers a principled way to incorporate latent structures carry-
ing domain knowledge and learn a discriminative model with partially labeled data. The reducibil-
ity of MaxEnDNet to M3N renders many existing convex optimization algorithms developed for
learning M3N directly applicable as subroutines for learning our proposed model. We describe an
EM-style algorithm for PoMEN based on existing algorithms for M3N. As a practical application 
we apply the proposed model to a web data extraction task–product information extraction  where
collecting fully labeled training data is very difﬁcult. The results show the promise of max-margin
learning as opposed to likelihood-based estimation in the presence of hidden variables.
The paper is organized as follows. Section 2 reviews the basic max-margin structured prediction
formalism and MaxEnDNet. Section 3 presents the partially observed MaxEnDNet. Section 4
applies the model to real web data extraction  and Section 5 brings this paper to a conclusion.

2 Preliminaries
Our goal is to learn a predictive function h : X 7→ Y from a structured input x ∈ X to a structured
output y ∈ Y  where Y = Y1×···×Yl represents a combinatorial space of structured interpretations
of multi-facet objects. For example  in part-of-speech (POS) tagging  Yi consists of all the POS tags
and each label y = (y1 ···   yl) is a sequence of POS tags  and each input x is a sentence (word
sequence). We assume that the feasible set of labels Y(x) ⊆ Y is ﬁnite for any x.
Let F (x  y; w) be a parametric discriminant function. A common choice of F is a linear model 
where F is deﬁned by a set of K feature functions fk : X × Y 7→ R and their weights wk:
F (x  y; w) = w>f(x  y). A commonly used predictive function is:
F (x  y; w).

(1)
By using different loss functions  the parameters w can be estimated by maximizing the conditional
likelihood [7] or by maximizing the margin [2  12  13] on labeled training data.
2.1 Maximum margin Markov networks
Under the M3N formalism  which we will generalize in this paper  given a set of fully labeled
training data D = {(xi  yi)}N
i=1  the max-margin learning [12] solves the following optimization
problem and achieves an optimum point estimate of the weight vector w:

h0(x; w) = arg max
y∈Y(x)

P0 (M3N) :

(2)
where ξi represents a slack variable absorbing errors in training data  C is a positive constant  R+
denotes non-negative real numbers  and F0 is the feasible space for w: F0 = {w : w>∆fi(y) ≥

w∈F0 ξ∈RN
+

min

ξi 

NX

i=1

kwk2 + C

1
2

2

j=1

I(yj 6= yi

∆‘i(y) =P|xi|

∆‘i(y) − ξi; ∀i ∀y 6= yi}  of which ∆fi(y) = f(xi  yi) − f(xi  y)  w>∆fi(y) is the “margin”
between the true label yi and a prediction y  and ∆‘i(y) is a loss function with respect to yi.
Various loss functions have been proposed for P0. In this paper  we adopt the hamming loss [12]:
j)  where I(·) is an indicator function that equals to 1 if the argument is
true and 0 otherwise. The optimization problem P0 is intractable because of the exponential number
of constraints in F0. Exploring sparse dependencies among individual labels yi in y  as reﬂected
in the speciﬁc design of the feature functions (e.g.  based on pair-wise labeling potentials)  efﬁcient
optimization algorithms based on cutting-plane [13] or message-passing [12]  and various gradient-
based methods [3  10] have been proposed to obtain approximate solution to P0. As described
shortly  these algorithms can be directly employed as subroutines in solving our proposed model.
2.2 Maximum Entropy Discrimination Markov Networks
Instead of predicting based on a single rule F (·; w) as in M3N using w  the structured maximum
entropy discrimination formalism [19] facilitates a Bayes-style prediction by averaging F (·; w) over
a distribution of rules according to a posterior distribution of the weights  p(w):

Z

p(w)F (x  y; w) dw  

h1(x) = arg max
y∈Y(x)

(3)
where p(w) is learned by solving an optimization problem referred to as a maximum entropy dis-
crimination Markov network (MaxEnDNet  or MEN) [20] that elegantly combines Bayesian-style
learning with max-margin learning. In a MaxEnDNet  a prior over w is introduced to regularize its
distribution  and the margins resulting from predictor (3) are used to deﬁne a feasible distribution
subspace. More formally  given a set of fully observed training data D and a prior distribution
p0(w)  MaxEnDNet solves the following problem for an optimal posterior p(w|D) or p(w):

KL(p(w)||p0(w)) + U (ξ) 

min

P1 (MaxEnDNet) :

(4)
where the objective function KL(p(w)||p0(w)) + U(ξ) is known as the generalized entropy [8  5] 
or regularized KL-divergence  and U(ξ) is a closed proper convex function over the slack variables
ξ. U is also known as an additional “potential” term in the maximum entropy principle. The feasible
distribution subspace F1 is deﬁned as follows:

p(w)∈F1 ξ∈RN
+

F1 =

p(w) :

p(w)[∆Fi(y; w) − ∆‘i(y)] dw ≥ −ξi  ∀i  ∀y

n

Z

o

 

where ∆Fi(y; w) = F (xi  yi; w) − F (xi  y; w).
P1 is a variational optimization problem over p(w) in the feasible subspace F1. Since both the KL-
divergence and the U function in P1 are convex  and the constraints in F1 are linear  P1 is a convex
program. Thus  one can apply the calculus of variations to the Lagrangian to obtain a variational
extremum  followed by a dual transformation of P1. As proved in [20]  solution to P1 leads to a
GLIM for p(w)  whose parameters are closely connected to the solution of the M3N.
Theorem 1 (MaxEnDNet (adapted from [20])) The variational optimization problem P1 under-
lying a MaxEnDNet gives rise to the following optimum distribution of Markov network parameters:
(5)

αi(y)[∆Fi(y; w) − ∆‘i(y)](cid:9) 

Z(α) p0(w) exp(cid:8)X

p(w) =

1

where Z(α) is a normalization factor and the Lagrangian multipliers αi(y) (corresponding to
constraints in F1) can be obtained by solving the following dual problem of P1:

i y

D1 :

− log Z(α) − U ?(α)

max
s.t. αi(y) ≥ 0  ∀i  ∀y 

α

(cid:0)P
i y αi(y)ξi− U(ξ)(cid:1).

where U ?(·) is the conjugate of the slack function U(·)  i.e.  U ?(α) = supξ

It can be shown that when F (x  y; w) = w>f(x  y)  U(ξ) = CP
Gaussian N (w|0  I)  then p(w) is also a Gaussian with shifted meanP

i ξi  and p0(w) is a standard
i y αi(y)∆fi(y) and co-
variance matrix I  where the Lagrangian multipliers αi(y) can be obtained by solving problem D1
of the form that is isomorphic to the dual of M3N. When applying this p(w) to Eq. (3)  one can
obtain a predictor that is identical to that of the M3N.
From the above reduction  it should be clear that M3N is a special case of MaxEnDNet. But the
MaxEnDNet in its full generality offers a number of important advantages while retaining all the

3

(a)

(b)

(c)

Figure 1: (a) A web page with two data records containing 7 and 8 elements respectively; (b) A partial vision
tree of the page in Figure 1(a)  where grey nodes are the roots of the two records; (c) A label hierarchy for
product information extraction  where the root node represents an entire instance (a web page); leaf nodes are
the attributes (i.e. Name  Image  Price  and Description); and inner nodes are the intermediate class labels
deﬁned for parts of a web page  e.g. {N  I} is a class label for blocks containing both Name and Image.
merits of the M3N. First  the MaxEnDNet prediction is based on model averaging and therefore
enjoys a desirable smoothing effect  with a uniform convergence bound on generalization error  as
shown in [20]. Second  MaxEnDNet admits a prior that can be designed to introduce useful regular-
ization effects  such as a sparsity bias  as explored in the Laplace M3N [19  20]. Third  as explored
in this paper  MaxEnDNet offers a principled way to incorporate hidden generative models underly-
ing the structured predictions  but allows the predictive model to be discriminatively trained based
on partially labeled data. In the sequel  we introduce partially observed MaxEnDNet (PoMEN)  that
combines (possibly latent) generative model and discriminative training for structured prediction.
3 Partially Observed MaxEnDNet
Consider  for example  the problem of web data extraction  which is to identify interested informa-
tion from web pages. Each sample is a data record or an entire web page which is represented as a set
of HTML elements. One striking characteristic of web data extraction is that various types of struc-
tural dependencies between HTML elements exist  e.g. the HTML tag tree or the Document Object
Model (DOM) structure is itself hierarchical. In [17]  fully observed hierarchical CRFs are shown
to have great promise and achieve better performance than ﬂat models like linear-chain CRFs [7].
One method to construct a hierarchical model is to ﬁrst use a parser to construct a so called vision
tree [17]. For example  Figure 1(b) is a part of the vision tree of the page in Figure 1(a). Then  based
on the vision tree  a hierarchical model can be constructed accordingly to extract the interested at-
tributes  e.g. a product’s name  image  price  description  etc.
In such a hierarchical extraction
model  inner nodes are useful to incorporate long distance dependencies  and the variables at one
level are reﬁnements of the variables at upper levels. To reﬂect the reﬁnement relationship  the class
labels deﬁned as in [17] are also organized in a hierarchy as in Figure 1(c). Due to concerns over
labeling cost and annotation-ambiguity caused by the overlapping of class labels as in Figure 1(c) 
it is desirable to effectively learn a hierarchical extraction model with partially labeled data.
Without loss of generality  assume that the structured labeling of a sample consists of two parts—an
observed part y and a hidden part z. Both y and z are structured labels  and furthermore the hidden
variables are not isolated  but are statistically dependent on each other and on the observed data
according to a graphical model p(y  z  w|x) = p(w  z|x)p(y|x  z  w)  where p(y|x  z  w) takes
the form of a Boltzmann distribution p(y|x  z  w) = 1
Z exp{−F (x  y  z; w)} and x is a global
condition as in CRFs [7]. Following the spirit of a margin-based structured predictor such as M3N 
we employ only the unnormalized energy function F (x  y  z; w) (which usually consists of linear
combinations of feature functions or potentials) as the cost function for structured prediction  and
we adopt a prediction rule directly extended from the MaxEnDNet—average over all the possible
models deﬁned by different w  and at the same time marginalized over all hidden variables z. That is 

Z

X

h2(x) = arg max
y∈Y(x)

(6)
Now our problem is learning the optimum p(w  z) from data. Let {z} ≡ (z1  . . .   zN ) denote the
ensemble of hidden labels of all the samples. Analogous to the setup for learning the MaxEnDNet 
we specify a prior distribution p0({z}) over all the hidden structured labels. The feasible space F2
of p(w {z}) can be deﬁned as follows according to the margin constraints:

p(w  z)F (x  y  z; w) dw .

z

n

Z

X

F2 =

p(w {z}) :

p(w  z)[∆Fi(y  z; w) − ∆‘i(y)] dw ≥ −ξi  ∀i  ∀y

o

 

z

4

where ∆Fi(y  z; w) = F (xi  yi  z; w)− F (xi  y  z; w)  and p(w  z) is the marginal distribution of
p(w {z}) on a single sample  which will be used in (6) to compute the structured prediction.
Again we learn the optimum p(w {z}) based on a structured minimum relative entropy principle
as in MaxEnDNet. Speciﬁcally  let p0(w {z}) represent a given joint prior over the parameters and
the hidden variables  we deﬁne the PoMEN problem that gives rise to the optimum p(w {z}):

min

KL(p(w {z})||p0(w {z})) + U (ξ).

P2 (PoMEN) :

p(w {z})∈F2 ξ∈RN
+

(7)
Analogous to P1  P2 is a variational optimization problem over p(w {z}) in the feasible space F2.
Again since both the KL and the U function in P2 are convex  and the constraints in F2 are linear 
P2 is a convex program. Thus  we can employ a technique similar to that used to solve MaxEnDNet
to solve the PoMEN problem.

that is  p0(w {z}) = p0(w)QN

i=1 p0(zi) and p(w {z}) = p(w)QN

3.1 Learning PoMEN
For a fully general p(w {z}) where hidden variables in all samples are coupled  solving P2 based on
an extension of Theorem 1 would involve very high-dimensional integration and summation that is
in practice intractable. In this paper we consider a simpler case where the hidden labels of different
samples are iid and independent of the parameter w in both the prior and the posterior distributions 
i=1 p(zi). This assumption
will hold true in a graphical model where w corresponds to only the observed y variables at the
bottom of a hierarchical model. For many practical applications such as the hierarchical web-info
extraction  such a model is realistic and adequate. For more general models where dependencies are
more global  we can use the above factored model as a generalized mean ﬁeld approximation to the
true distribution  but this extension is beyond the scope of this paper  and will be explored later in
the full paper. Generalizing Theorem 1  following a coordinate descent principle  now we present
an alternating minimization (EM-style) procedure for P2:
Step 1: keep p(z) ﬁxed  infer p(w) by solving the following problem:
ξi 

KL(p(w)||p0(w)) + C

R p(w)Ep(z)[∆Fi(y  z; w) − ∆‘i(y)] dw ≥ −ξi  ∀i  ∀y}  which is a

where F0
generalized version of F1 with hidden variables. Thus  we can apply the same convex optimization
techniques as being used for solving the problem P1. Speciﬁcally  assume that the prior distribution
p0(w) is a standard normal and F (x  y  z; w) = w>f(x  y  z)  then the solution (i.e. posterior
i y αi(y)Ep(z)[∆fi(y  z)]. The dual variables

distribution) is p(w) = N (w|µw  I)  where µw =P
kX
where P(C) = {α : P

X
(9)
y αi(y) = C; αi(y) ≥ 0  ∀i  ∀y}. This dual problem is isomorphic
to the dual form of the M3N optimization problem  and we can use existing algorithms developed
for M3N  such as [12  3] to solve it. Alternatively  we can solve the following primal problem via
employing existing subgradient [10] or cutting plane [13] algorithms:

α are achieved by solving a dual problem:

αi(y)Ep(z)[∆fi(y  z)]k2 

αi(y)∆‘i(y) − 1
2

1 = {p(w) :

min
p(w)∈F0

X

max
α∈P(C)

1 ξ∈RN

(8)

i y

i y

+

i

NX

>

1
2

+

w

i=1

ξi 

w∈F0

w + C

min
0 ξ∈RN

(10)
0 = {w : w>Ep(z)[∆fi(y  z)] ≥ ∆‘i(y) − ξi; ξi ≥ 0  ∀i  ∀y}  which is a generalized
where F0
version of F0. It is easy to show that the solution to this primal problem is the posterior mean of
p(w)  which will be used to make prediction in the predictive function h2. Note that the primal
problem is very similar to that of M3N  except the expectations in F0
0. This is not surprising since it
can be shown that M3N is a special case of MaxEnDNet. We will discuss how to efﬁciently compute
the expectations Ep(z)[∆fi(y  z)] in Step 2.

Step 2: keep p(w) ﬁxed  based on the factorization assumption p({z}) = Q
p0({z}) = Q

i p(zi) and
i p0(zi)  the distribution p(z) for each sample i can be obtained by solving the

following problem:

p(z)∈F ?

min
1  ξi∈R+

KL(p(z)||p0(z)) + Cξi 

(11)

5

1 = {p(z) : P

1 = {p(z) : P
where F ?
is a normal distribution as shown in Step 1  F ?
nX
−ξi  ∀y}. Similarly  by introducing a set of Lagrangian multipliers β(y)  we can get:
p0(z) exp{X

z p(z)R p(w)[w>∆fi(y  z) − ∆‘i(y)] dw ≥ −ξi  ∀y}. Since p(w)
w∆fi(y  z) − ∆‘i(y)(cid:3) ≥
o
w∆fi(y  z) − ∆‘i(y)]}(cid:17)

(12)
y β(y) = C  β(y) ≥ 0  ∀y}. This non-linear constrained optimization
problem can be solved with existing solvers  like IPOPT [15]. With a little algebra  we can compute
the gradients as follows:

and the dual variables β(y) can be obtained by solving the following dual problem:

where Pi(C) = {P

z p(z)[µ>
w∆fi(y  z) − ∆‘i(y)]
>

(cid:16)X

p0(z) exp

− log

max

β∈Pi(C)

β(y)[µ

1

Z(β)

>

β(y)[µ

z

y

p(z) =

 

 

y

∂log Z(β)

∂β(y)

wEp(z)[∆fi(y  z)] − ∆‘i(y).
>

= µ

To efﬁciently calculate the expectations Ep(z)[∆fi(y  z)] as required in Step1 and in the above gra-
dients. We make a gentle assumption that the prior distribution p0(z) is an exponential distribution
of the following form:

nX

o

p0(z) = exp

φm(z)

.

(13)

m

ample 

i. Log-linear Prior: deﬁned by a set of feature functions and their weights.

This assumption is general enough for our purpose  and covers the following commonly used priors:
For ex-
in a pairwise Markov network  we can deﬁne the prior model as: p0(z) ∝
(i j)∈E

exp(cid:8)P
k λkgk(zi  zj)(cid:9)  where gk(zi  zj) are feature functions and λk are weights.
P
ii. Independent Prior: deﬁned as p0(z) =Q‘
as: p0(z) = exp{P‘
ple  for a chain graph  the prior distribution can be written as: p0(z) = p(z1)Q‘
Similarly  in the logarithm space  p0(z) = exp{log p0(z1) +P‘

iii. Markov Prior: the prior model have the Markov property w.r.t the model’s structure. For exam-
j=2 p0(zj|zj−1).

j=1 p0(zj). In the logarithm space  we can write it

j=1 log p0(zj)}.

j=2 log p0(zj|zj−1)}.

With the above assumption  p(z) is an exponential family distribution  and the expectations 
Ep(z)[∆fi(y  z)]  can be efﬁciently calculated by exploring the sparseness of the model’s structure
to compute marginal probabilities  e.g. p(zi) and p(zi  zj) in pairwise Markov networks. When the
model’s tree width is not large  this can be done exactly. For complex models  approximate inference
like loopy belief propagation and variational methods can be applied. However  since the number of
constraints in (12) is exponential to the size of the observed labels  the optimization problem cannot
be efﬁciently solved. A key observation  as explored in [12]  is that we can interpret β(y) as a prob-
y β(y) = C  β(y) ≥ 0  ∀y. Thus 
we can introduce a set of marginal dual variables and transfer the dual problem (12) to an equivalent
form with a polynomial number of constraints. The derivatives with respect to each marginal dual
parameter is of the same structure as the above gradients.

ability distribution of y because of the regularity constraints:P

4 Experiments
We apply PoMEN to the problem of web data extraction  and compare it with partially observed
CRFs (PoHCRF) [9]  and fully observed hierarchical CRFs (HCRF) [17] and hierarchical M3N
(HM3N) which has the same hierarchical model structure as the HCRF.

4.1 Data Sets  Evaluation Criteria  and Prior for Latent Variables
We concern ourselves with the problem of identifying product items for sale on the web. For each
product item  four attributes – Name  Image  Price  and Description are extracted in our experiments.
The evaluation data consists of product web pages generated from 37 different templates. For each
template  there are 5 pages for training and 10 for testing. We evaluate all the methods on two
different levels of inputs  record level and page level. For record-level evaluation  we assume that
data records are given  and we compare different models on accuracy of extracting attributes in the
given records. For page-level evaluation  the inputs are raw web pages and all the models perform

6

(a)

(b)

Figure 2: (a) The F1 and block instance accuracy of record-level evaluation from 4 models under different
amount of training data. (b) The F1 and its variance on the attributes: Name  Image  Price  and Description.

(a)

(b)

Figure 3: The average F1 and block instance accuracy of different models with different ratios of training data
for two types of page-level evaluation: (a) ST1; and (b) ST2.
both record detection and attribute extraction simultaneously as in [17]. In the 185 training pages 
there are 1585 data records in total; in the 370 testing pages  3391 data records are collected. As
for evaluation criteria  we use the standard precision  recall  and their harmonic value F1 for each
attribute and the two comprehensive measures  i.e. average F1 and block instance accuracy  as
deﬁned in [17]. We adopt an independent prior described earlier for the latent variables  each factor
p0(zi) over a single latent label is assumed to be uniform.

4.2 Record-Level Evaluation
In this evaluation  partially observed training data are the data records whose leaf nodes are labeled
and inner nodes are hidden. We randomly select m = 5  10  20  30  40  or  50 percent of the training
records as training data  and test on all the testing records. For each m  10 independent experiments
were conducted and the average performance is summarized in Figure 2. From Figure 2(a)  it can
be seen that the HM3N performs slightly better than HCRF trained on fully labeled data. For the
two partially observed models  PoMEN performs much better than PoHCRF in both average F1
and block instance accuracy  and with lower variances of the score  especially when the training
set is small. As the number of training data increases  PoMEN performs comparably w.r.t.
the
fully observed HM3N. For all the models  higher scores and lower variances are achieved with
more training data. Figure 2(b) shows the F1 score on each attribute. Overall  for attributes Image 
Price  and Description  although all models generally perform better with more training data  the
improvement is small; and the differences between different models are small. This is possibly
because the features of these attributes are usually consistent and distinctive  and therefore easier to
learn and predict. For the attribute Name  however  a large number of training data are needed to
learn a good model because its underlying features have diverse appearance on web pages.

4.3 Page-Level Evaluation
Experiments on page-level prediction is conducted similarly as above  and the results are summa-
rized in Figure 3. Two different partial labeling strategies are used to generate training data. ST1:
label the leaf nodes and the nodes that represent data records; ST2: label more information based
on ST1  e.g.  label also the nodes above the “Data Record” nodes in the hierarchy as in Figure 1(c).
Due to space limitation  we only report average F1 and block instance accuracy.
For ST1  PoMEN achieves better scores and lower variances than PoHCRF in both average F1 and
block instance accuracy. The HM3N performs slightly better than HCRF (both trained on full label-
ing)  and PoMEN performs comparably with the fully observed HCRF in block instance accuracy.
For ST2  with more supervision information  PoHCRF achieves higher performance that is compa-
rable to that of HM3N in average F1  but slightly lower than HM3N in block instance accuracy. For

7

020400.850.860.870.880.890.90.910.92Training RatioAverage F1  HCRFPoHCRFHM3NPoM3N020400.650.70.750.80.85Training RatioBlock Instance Accuracy  HCRFPoHCRFHM3NPoM3N0500.60.650.70.750.80.850.9Training RatioF1Name  HCRFPoHCRFHM3NPoM3N0500.930.9350.940.9450.950.9550.960.9650.97Training RatioF1Image  HCRFPoHCRFHM3NPoM3N0500.930.940.950.960.970.98Training RatioF1Price  HCRFPoHCRFHM3NPoM3N0500.780.790.80.810.820.830.840.850.86Training RatioF1Description  HCRFPoHCRFHM3NPoM3N020400.60.650.70.750.80.850.90.951Training RatioAverage F1  020400.60.650.70.750.80.850.90.951Training RatioBlock Instance Accuracy  HCRFPoHCRFHM3NPoM3NHCRFPoHCRFHM3NPoM3N020400.60.650.70.750.80.850.90.951Training RatioAverage F1  020400.60.650.70.750.80.850.90.951Training RatioBlock Instance Accuracy  HCRFPoHCRFHM3NPoM3NHCRFPoHCRFHM3NPoM3Nthe latent models  PoHCRF performs slightly better in average F1  and PoMEN does better in block
instance accuracy; moreover  the variances of PoMEN are much smaller than those of PoHCRF in
both average F1 and block instance accuracy. We can also see that PoMEN does not change much
when additional label information is provided in ST2. Thus  the max-margin principle could provide
a better paradigm than the likelihood-based estimation for learning latent hierarchical models.
For the second step of learning PoMEN  the IPOPT solver [15] was used to compute the distribution
p(z). Interestingly  the performance of PoMEN does not change much during the iteration  and
our results were achieved within 3 iterations. It is possible that in hierarchical models  since inner
variables usually represent overlapping concepts  the initial distribution are already reasonably good
to describe conﬁdence on the labeling due to implicit consistence across the labels. This is unlike
the multi-label learning [6] where only one of the multiple labels is true and during the iteration
more probability mass should be redistributed on the true label during the EM iterations.
5 Conclusions
We have presented an extension of the standard max-margin learning to address the challenging
problem of learning Markov networks with the existence of structured hidden variables. Our ap-
proach is a generalization of the maximum entropy discrimination Markov networks (MaxEnDNet) 
which offer a general framework to combine Bayesian-style and max-margin learning and subsume
the standard M3N as a special case  to consider structured hidden variables. For the partially ob-
served MaxEnDNet  we developed an EM-style algorithm based on existing convex optimization
algorithms developed for the standard M3N. We applied the proposed model to a real-world web
data extraction task and showed that learning latent hierarchical models based on the max-margin
principle could be better than the likelihood-based learning with hidden variables.

Acknowledgments
This work was done while J.Z. was a visiting researcher at CMU under a State Scholarship from China  and sup-
ports from NSF DBI-0546594 and DBI-0640543 awarded to E.X.; J.Z. and B.Z. are also supported by Chinese
NSF Grant 60621062 and 60605003; National Key Foundation R&D Projects 2003CB317007  2004CB318108
and 2007CB311003; and Basic Research Foundation of Tsinghua National Lab for Info Sci & Tech.
References
[1] Y. Altun  D. McAllester  and M. Belkin. Maximum margin semi-supervised learning for structured

variables. In NIPS  2006.

[2] Y. Altun  I. Tsochantaridis  and T. Hofmann. Hidden markov support vector machines. In ICML  2003.
[3] P. Bartlett  M. Collins  B. Taskar  and D. McAllester. Exponentiated gradient algorithms for larg-margin

structured classiﬁcation. In NIPS  2004.

[4] U. Brefeld and T. Scheffer. Semi-supervised learning for structured output variables. In ICML  2006.
[5] M. Dud´ık  S.J. Phillips  and R.E. Schapire. Maximum entropy density estimation with generalized

regularization and an application to species distribution modeling. JMLR  (8):1217–1260  2007.

[6] R. Jin and Z. Ghahramani. Learning with multiple labels. In NIPS  2002.
[7] J. Lafferty  A. McCallum  and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting

and labeling sequence data. In ICML  2001.

[8] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. In NIPS  2001.
[9] A. Quattoni  M. Collins  and T. Darrell. Conditional random ﬁelds for object recognition. In NIPS  2004.
[10] N.D. Ratliff  J.A. Bagnell  and M.A. Zinkevich. (online) subgradient methods for structured prediction.

In AISTATS  2007.

[11] F. Sha and L. Saul. Large margin hidden markov models for automatic speech recognition. In NIPS  2006.
[12] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In NIPS  2003.
[13] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun.

Support vector machine learning for

interdependent and structured output spaces. In ICML  2004.

[14] J. Verbeek and B. Triggs. Scene segmentation with conditional random ﬁelds learned from partially

labeled images. In NIPS  2007.

[15] A. W¨achter and L.T. Biegler. On the implementation of a primal-dual interior point ﬁlter line search

algorithm for large-scale nonlinear programming. Mathematical Programming  (106(1)):25–57  2006.

[16] L. Xu  D. Wilkinson  F. Southey  and D. Schuurmans. Discriminative unsupervised learning of structured

predictors. In ICML  2006.

in web data extraction. In SIGKDD  2006.

to web data extraction. In ICML  2007.

[17] J. Zhu  Z. Nie  J.-R. Wen  B. Zhang  and W.-Y. Ma. Simultaneous record detection and attribute labeling

[18] J. Zhu  Z. Nie  B. Zhang  and J.-R. Wen. Dynamic hierarchical markov random ﬁelds and their application

[19] J. Zhu  E.P. Xing  and B. Zhang. Laplace maximum margin markov networks. In ICML  2008.
[20] J. Zhu  E.P. Xing  and B. Zhang. Maximum entropy discrimination markov networks. Technical Report

CMU-ML-08-104  Machine Learning Department  Carnegie Mellon University  2008.

8

,Celestine Dünner
Thomas Parnell
Dimitrios Sarigiannis
Nikolas Ioannou
Andreea Anghel
Gummadi Ravi
Madhusudanan Kandasamy
Haralampos Pozidis