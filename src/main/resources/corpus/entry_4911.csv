2019,Screening Sinkhorn Algorithm for Regularized Optimal Transport,We introduce in this paper a novel strategy for efficiently approximating the Sinkhorn distance between two discrete measures. After identifying neglectable components of the dual solution of the regularized Sinkhorn problem  we propose to screen those components by directly setting them at that value before entering the Sinkhorn problem. This allows us to solve a smaller Sinkhorn problem while ensuring approximation with provable guarantees. More formally  the approach is based on a new formulation of dual of Sinkhorn divergence problem and on the KKT optimality conditions of this problem  which enable identification of dual components to be screened. This new analysis leads to the Screenkhorn algorithm. We illustrate the efficiency of Screenkhorn on complex tasks such as dimensionality reduction and domain adaptation involving regularized optimal transport.,Screening Sinkhorn Algorithm for Regularized

Optimal Transport

Mokhtar Z. Alaya

LITIS EA4108

University of Rouen Normandy

mokhtarzahdi.alaya@gmail.com

Maxime Bérar
LITIS EA4108

University of Rouen Normandy
maxime.berar@univ-rouen.fr

Gilles Gasso
LITIS EA4108

INSA  University of Rouen Normandy

gilles.gasso@insa-rouen.fr

Alain Rakotomamonjy

LITIS EA4108

University of Rouen Normandy
and Criteo AI Lab  Criteo Paris
alain.rakoto@insa-rouen.fr

Abstract

We introduce in this paper a novel strategy for efﬁciently approximating the
Sinkhorn distance between two discrete measures. After identifying neglectable
components of the dual solution of the regularized Sinkhorn problem  we propose
to screen those components by directly setting them at that value before entering
the Sinkhorn problem. This allows us to solve a smaller Sinkhorn problem while
ensuring approximation with provable guarantees. More formally  the approach
is based on a new formulation of dual of Sinkhorn divergence problem and on
the KKT optimality conditions of this problem  which enable identiﬁcation of
dual components to be screened. This new analysis leads to the SCREENKHORN
algorithm. We illustrate the efﬁciency of SCREENKHORN on complex tasks such
as dimensionality reduction and domain adaptation involving regularized optimal
transport.

1

Introduction

Computing optimal transport (OT) distances between pairs of probability measures or histograms 
such as the earth mover’s distance [39  34] and Monge-Kantorovich or Wasserstein distance [38] 
are currently generating an increasing attraction in different machine learning tasks [37  28  4  22] 
statistics [18  32  14  6  17]  and computer vision [8  34  36]  among other applications [27  33]. In
many of these problems  OT exploits the geometric features of the objects at hand in the underlying
spaces to be leveraged in comparing probability measures. This effectively leads to improved
performance of methods that are oblivious to the geometry  for example the chi-squared distances or
the Kullback-Leibler divergence. Unfortunately  this advantage comes at the price of an enormous
computational cost of solving the OT problem  that can be prohibitive in large scale applications.
For instance  the OT between two histograms with supports of equal size n can be formulated as a
linear programming problem that requires generally super O(n2.5) [29] arithmetic operations  which
is problematic when n becomes larger.
A remedy to the heavy computation burden of OT lies in a prevalent approach referred to as regularized
OT [11] and operates by adding an entropic regularization penalty to the original problem. Such a
regularization guarantees a unique solution  since the objective function is strongly convex  and a
greater computational stability. More importantly  this regularized OT can be solved efﬁciently with
celebrated matrix scaling algorithms  such as Sinkhorn’s ﬁxed point iteration method [35  26  23].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Several works have considered further improvements in the resolution of this regularized OT problem.
A greedy version of Sinkhorn algorithm  called Greenkhorn [3]  allows to select and update columns
and rows that most violate the polytope constraints. Another approach based on low-rank approxima-
tion of the cost matrix using the Nyström method induces the Nys-Sink algorithm [2]. Other classical
optimization algorithms have been considered for approximating the OT  for instance accelerated
gradient descent [40  13  30]  quasi-Newton methods [7  12] and stochastic gradient descent [20  1].
In this paper  we propose a novel technique for accelerating the Sinkhorn algorithm when computing
regularized OT distance between discrete measures. Our idea is strongly related to a screening
strategy when solving a Lasso problem in sparse supervised learning [21]. Based on the fact that
a transport plan resulting from an OT problem is sparse or presents a large number of neglectable
values [7]  our objective is to identify the dual variables of an approximate Sinkhorn problem  that are
smaller than a predeﬁned threshold  and thus that can be safely removed before optimization while
not altering too much the solution of the problem. Within this global context  our contributions are
the following:
• From a methodological point of view  we propose a new formulation of the dual of the Sinkhorn
divergence problem by imposing variables to be larger than a threshold. This formulation allows
us to introduce sufﬁcient conditions  computable beforehand  for a variable to strictly satisfy its
constraint  leading then to a “screened” version of the dual of Sinkhorn divergence.
• We provide some theoretical analysis of the solution of the “screened” Sinkhorn divergence 
showing that its objective value and the marginal constraint satisfaction are properly controlled as
the number of screened variables decreases.
• From an algorithmic standpoint  we use a constrained L-BFGS-B algorithm [31  9] but provide a
careful analysis of the lower and upper bounds of the dual variables  resulting in a well-posed and
efﬁcient algorithm denoted as SCREENKHORN.
• Our empirical analysis depicts how the approach behaves in a simple Sinkhorn divergence
computation context. When considered in complex machine learning pipelines  we show that
SCREENKHORN can lead to strong gain in efﬁciency while not compromising on accuracy.

The remainder of the paper is organized as follow. In Section 2 we brieﬂy review the basic setup
of regularized discrete OT. Section 3 contains our main contribution  that is  the SCREENKHORN
algorithm. Section 4 is devoted to theoretical guarantees for marginal violations of SCREENKHORN.
In Section 5 we present numerical results for the proposed algorithm  compared with the state-of-art
Sinkhorn algorithm as implemented in [16]. The proofs of theoretical results are postponed to the
supplementary material as well as additional empirical results.

Notation. For any positive matrix T ∈ Rn×m  we deﬁne its entropy as H(T ) = −(cid:80)
(cid:104)T  W(cid:105) = tr(T (cid:62)W ) =(cid:80)

i j Tij log(Tij).
Let r(T ) = T 1m ∈ Rn and c(T ) = T (cid:62)1n ∈ Rm denote the rows and columns sums of T
respectively. The coordinates ri(T ) and cj(T ) denote the i-th row sum and the j-th column sum of
T   respectively. The scalar product between two matrices denotes the usual inner product  that is
i j TijWij  where T (cid:62) is the transpose of T . We write 1 (resp. 0) the vector
having all coordinates equal to one (resp. zero). ∆(w) denotes the diag operator  such that if w ∈ Rn 
then ∆(w) = diag(w1  . . .   wn) ∈ Rn×n. For a set of indices L = {i1  . . .   ik} ⊆ {1  . . .   n}
satisfying i1 < ··· < ik  we denote the complementary set of L by L
= {1  . . .   n}\L. We also
(cid:123)
denote |L| the cardinality of L. Given a vector w ∈ Rn  we denote wL = (wi1  . . .   wik )(cid:62) ∈ Rk
and its complementary wL(cid:123) ∈ Rn−k. The notation is similar for matrices; given another subset
of indices S = {j1  . . .   jl} ⊆ {1  . . .   m} with j1 < ··· < jl  and a matrix T ∈ Rn×m  we use
T(L S)  to denote the submatrix of T   namely the rows and columns of T(L S) are indexed by L and
S respectively. When applied to matrices and vectors  (cid:12) and (cid:11) (Hadamard product and division)
and exponential notations refer to elementwise operators. Given two real numbers a and b  we write
a ∨ b = max(a  b) and a ∧ b = min(a  b).

2 Regularized discrete OT

µ =(cid:80)n

We brieﬂy expose in this section the setup of OT between two discrete measures. We then consider
the case when those distributions are only available through a ﬁnite number of samples  that is
j=1 νiδyj ∈ Σm  where Σn is the probability simplex with n bins 

i=1 µiδxi ∈ Σn and ν =(cid:80)m

2

namely the set of probability vectors in Rn
probabilistic couplings set as Π(µ  ν) = {P ∈ Rn×m

+  i.e.  Σn = {w ∈ Rn

  P 1m = µ  P (cid:62)1n = ν}.

i=1 wi = 1}. We denote their

+

+ :(cid:80)n

Sinkhorn divergence. Computing OT distance between the two discrete measures µ and ν amounts
to solving a linear problem [25] given by

S(µ  ν) = min

(cid:104)C  P(cid:105) 

P∈Π(µ ν)

where P = (Pij) ∈ Rn×m is called the transportation plan  namely each entry Pij represents the
fraction of mass moving from xi to yj  and C = (Cij) ∈ Rn×m is a cost matrix comprised of
nonnegative elements and related to the energy needed to move a probability mass from xi to yj. The
entropic regularization of OT distances [11] relies on the addition of a penalty term as follows:

Sη(µ  ν) = min

P∈Π(µ ν)

{(cid:104)C  P(cid:105) − ηH(P )} 

(1)

where η > 0 is a regularization parameter. We refer to Sη(µ  ν) as the Sinkhorn divergence [11].
Dual of Sinkhorn divergence. Below we provide the derivation of the dual problem for the
regularized OT problem (1). Towards this end  we begin with writing its Lagrangian dual function:

exp(cid:0) − 1

η (wi + zj + Cij) − 1(cid:1)  for all i = 1  . . .   n and j = 1  . . .   m. Plugging this solution  and

(cid:62)1n − ν(cid:105).
L (P  w  z) = (cid:104)C  P(cid:105) + η(cid:104)log P  P(cid:105) + (cid:104)w  P 1m − µ(cid:105) + (cid:104)z  P
L (P  w  z). It is easy
The dual of Sinkhorn divergence can be derived by solving minP∈Rn×m
to check that objective function P (cid:55)→ L (P  w  z) is strongly convex and differentiable. Hence 
one can solve the latter minimum by setting ∇P L (P  w  z) to 0n×m. Therefore  we get P (cid:63)
ij =
setting the change of variables u = −w/η − 1/2 and v = −z/η − 1/2  the dual problem is given by
(2)
where B(u  v) := ∆(eu)K∆(ev) and K := e−C/η stands for the Gibbs kernel associated to the cost
matrix C. We refer to problem (2) as the dual of Sinkhorn divergence. Then  the optimal solution P (cid:63)
of the primal problem (1) takes the form P (cid:63) = ∆(eu(cid:63)
) where the couple (u(cid:63)  v(cid:63)) satisﬁes:
(u(cid:63)  v(cid:63)) = argmin
u∈Rn v∈Rm

n B(u  v)1m − (cid:104)u  µ(cid:105) − (cid:104)v  ν(cid:105)(cid:9) 

(cid:8)Ψ(u  v) := 1(cid:62)

)K∆(ev(cid:63)
{Ψ(u  v)}.

u∈Rn v∈Rm

min

+

Note that the matrices ∆(eu(cid:63)
) are unique up to a constant factor [35]. Moreover  P (cid:63)
can be solved efﬁciently by iterative Bregman projections [5] referred to as Sinkhorn iterations  and
the method is referred to as SINKHORN algorithm which  recently  has been proven to achieve a
near-O(n2) complexity [3].

) and ∆(ev(cid:63)

3 Screened dual of Sinkhorn divergence

Motivation. The key idea of our approach is motivated by the
so-called static screening test [21] in supervised learning  which is
a method able to safely identify inactive features  i.e.  features that
have zero components in the solution vector. Then  these inactive
features can be removed from the optimization problem to reduce its
scale. Before diving into detailed algorithmic analysis  let us present
a brief illustration of how we adapt static screening test to the dual
of Sinkhorn divergence. Towards this end  we deﬁne the convex set
Cr
α ⊆ Rr  for r ∈ N and α > 0  by Cr
α = {w ∈ Rr : ewi ≥ α}.
In Figure 1  we plot (eu(cid:63)
) where (u(cid:63)  v(cid:63)) is the pair solution
of the dual of Sinkhorn divergence (2) in the particular case of:
n = m = 500  η = 1  µ = ν = 1
the cost matrix C corresponds to the pairwise euclidean distance  i.e.  Cij = (cid:107)xi − yj(cid:107)2. We also
plot two lines corresponding to eu(cid:63) ≡ αu and ev(cid:63) ≡ αv for some αu > 0 and αv > 0  choosing
randomly and playing the role of thresholds to select indices to be discarded. If we are able to identify
these indices before solving the problem  they can be ﬁxed at the thresholds and removed then from
the optimization procedure yielding an approximate solution.

Figure 1: Plots of (eu(cid:63)
  ev(cid:63)
)
with (u(cid:63)  v(cid:63)) is the pair solu-
tion of dual of Sinkhorn diver-
gence (2) and the thresholds
αu  αv.

0 1 ))  yj ∼ N ((3  3)(cid:62) (cid:0) 1 −0.8

n 1n  xi ∼ N ((0  0)(cid:62)  ( 1 0

(cid:1)) and

  ev(cid:63)

−0.8

1

3

01002003004005000.00200.0025eu?ev?αuαvStatic screening test. Based on this idea  we deﬁne a so-called approximate dual of Sinkhorn
divergence

n B(u  v)1m − (cid:104)κu  µ(cid:105) − (cid:104) v
κ

(3)

(cid:8)Ψκ(u  v) := 1(cid:62)

min

 v∈Cm

εκ

u∈Cn

ε
κ

  ν(cid:105)(cid:9) 

which is simply a dual of Sinkhorn divergence with lower-bounded variables  where the bounds are
αu = εκ−1 and αv = εκ with ε > 0 and κ > 0 being ﬁxed numeric constants which values will be
clear later. The new formulation (3) has the form of (κµ  ν/κ)-scaling problem under constraints on
the variables u and v. Those constraints make the problem signiﬁcantly different from the standard
scaling-problems [24]. We further emphasize that κ plays a key role in our screening strategy. Indeed 
without κ  eu and ev can have inversely related scale that may lead in  for instance eu being too large
and ev being too small  situation in which the screening test would apply only to coefﬁcients of eu or
ev and not for both of them. Moreover  it is clear that the approximate dual of Sinkhorn divergence
coincides with the dual of Sinkhorn divergence (2) when ε = 0 and κ = 1. Intuitively  our hope is
to gain efﬁciency in solving problem (3) compared to the original one in Equation (2) by avoiding
optimization of variables smaller than the threshold and by identifying those that make the constraints
active. More formally  the core of the static screening test aims at locating two subsets of indices
(I  J) in {1  . . .   n} × {1  . . .   m} satisfying: eui > αu  and evj > αv  for all (i  j) ∈ I × J and
eui(cid:48) = αu  and evj(cid:48) = αv  for all (i(cid:48)  j(cid:48)) ∈ I
αv. The following
key result states sufﬁcient conditions for identifying variables in I
Lemma 1. Let (u∗  v∗) be an optimal solution of problem (3). Deﬁne

(cid:123)  namely (u  v) ∈ Cn
(cid:123).
(cid:123) and J

(cid:123) × J

× Cm

αu

Iε κ =(cid:8)i = 1  . . .   n : µi ≥ ε2

ri(K)(cid:9)  Jε κ =(cid:8)j = 1  . . .   m : νj ≥ κε2cj(K)(cid:9)

(4)

κ

j = εκ for all i ∈ I

i = εκ−1 and ev∗

Then one has eu∗
Proof of Lemma 1 is postponed to the supplementary material. It is worth to note that ﬁrst order
optimality conditions applied to (u∗  v∗) ensure that if eu∗
)i = κµi and if
ev∗
)j = κ−1νj  that correspond to the Sinkhorn marginal conditions [33] up
to the scaling factor κ.

i > εκ−1 then eu∗

j > εκ then ev∗

ε κ and j ∈ J
(cid:123)

j (K(cid:62)eu∗

i (Kev∗

(cid:123)
ε κ.

(cid:115)

Screening with a ﬁxed number budget of points. The approximate dual of Sinkhorn divergence
is deﬁned with respect to ε and κ. As those parameters are difﬁcult to interpret  we exhibit their
relations with a ﬁxed number budget of points from the supports of µ and ν. In the sequel  we denote
by nb ∈ {1  . . .   n} and mb ∈ {1  . . .   m} the number of points that are going to be optimized in
problem (3)  i.e  the points we cannot guarantee that eu∗
Let us deﬁne ξ ∈ Rn and ζ ∈ Rm to be the ordered decreasing vectors
of µ (cid:11) r(K) and ν (cid:11) c(K) respectively  that is ξ1 ≥ ξ2 ≥ ··· ≥ ξn
and ζ1 ≥ ζ2 ≥ ··· ≥ ζm. To keep only nb-budget and mb-budget of
points  the parameters κ and ε satisfy ε2κ−1 = ξnb and ε2κ = ζmb.
Hence

i = εκ−1 and ev∗

j = εκ .

.

ζmb
ξnb

ε = (ξnb ζmb )1/4 and κ =

(5)
This guarantees that |Iε κ| = nb and |Jε κ| = mb by construction. In
addition  (nb  mb) tends to the full number budget of points (n  m) 
when the couple parameters (ε  κ) converges to (0  1). In Figure 2 
we plot these convergences  and hence the objective in problem (3)
converges to the objective of dual of Sinkhorn divergence (2).
We are now in position to formulate the optimization problem related to the screened dual of Sinkhorn.
i ≥ εκ−1 and
Indeed  using the above analyses  any solution (u∗  v∗) of problem (3) satisﬁes eu∗
j ≥ εκ for all (i  j) ∈ (Iε κ × Jε κ)  and eu∗
(cid:123)
ev∗
ε κ).
Hence  we can restrict the problem (3) to variables in Iε κ and Jε κ. This boils down to restricting the
constraints feasibility Cn

Figure 2: Plots of ε and κ
as a function of number bud-
get of points for a screening
test with nb = mb and the
parameters µ  ν  η  C are set
as in Figure (1). (ε  κ) tends
to (0  1) as (nb  mb) tends to
(n  m).

εκ to the screened domain deﬁned by Usc ∩ Vsc 

j = εκ for all (i  j) ∈ (I

i = εκ−1 and ev∗

ε κ × J
(cid:123)

∩ Cm

ε
κ

Usc = {u ∈ Rnb : euIε κ (cid:23) ε
κ

1nb} and Vsc = {v ∈ Rmb : evJε κ (cid:23) εκ1mb}

4

2505000510ε2505000.00.51.0κwhere the vector comparison (cid:23) has to be understood elementwise. And  by replacing in Equation (3) 
(cid:123)
ε κ) by εκ−1 and εκ  we derive the screened dual of Sinkhorn
the variables belonging to (I
divergence problem as

ε κ × J
(cid:123)

min

u∈Usc v∈Vsc

{Ψε κ(u  v)}

(6)

−11(cid:62)

nb K(I(cid:123)

ε κ Jε κ)evJε κ

where
K(Iε κ Jε κ)evJε κ + εκ(euIε κ )(cid:62)
Ψε κ(u  v) = (euIε κ )(cid:62)
Iε κ uIε κ − κ
− κµ
(cid:62)

(cid:62)
Jε κvJε κ + Ξ

−1ν

Kij − κ log(εκ−1)(cid:80)

with Ξ = ε2(cid:80)

K(Iε κ J(cid:123)

ε κ)1mb + εκ

µi − κ−1 log(εκ)(cid:80)

ε κ

ε κ

ε κ

i∈I(cid:123)

i∈I(cid:123)

j∈J(cid:123)

ε κ j∈J(cid:123)

νj.
The above problem uses only the restricted parts K(Iε κ Jε κ)  K(Iε κ J(cid:123)
ε κ)  and K(I(cid:123)
ε κ Jε κ) of the
Gibbs kernel K for calculating the objective function Ψε κ. Hence  a gradient descent scheme will
also need only those rows/columns of K. This is in contrast to Sinkhorn algorithm which performs
alternating updates of all rows and columns of K. In summary  SCREENKHORN consists of two steps:
the ﬁrst one is a screening pre-processing providing the active sets Iε κ  Jε κ. The second one consists
in solving Equation (6) using a constrained L-BFGS-B [9] for the stacked variable θ = (uIε κ  vJε κ).
Pseudocode of our proposed algorithm is shown in Algorithm 1. Note that in practice  we initialize
the L-BFGS-B algorithm based on the output of a method  called RESTRICTED SINKHORN (see
Algorithm 2 in the supplementary)  which is a Sinkhorn-like algorithm applied to the active dual
variables θ = (uIε κ  vJε κ). While simple and efﬁcient  the solution of this RESTRICTED SINKHORN
algorithm does not satisfy the lower bound constraints of Problem (6) but provide a good candidate
solution. Also note that L-BFGS-B handles box constraints on variables  but it becomes more
efﬁcient when these box bounds are carefully determined for problem (6). The following proposition
(proof in supplementary material) expresses these bounds that are pre-calculated in the initialization
step of SCREENKHORN.
Proposition 1. Let (usc  vsc) be an optimal pair solution of problem (6) and Kmin =
Then  one has

i∈Iε κ j∈Jε κ

min

Kij.

and

∨

ε
κ

εκ ∨

for all i ∈ Iε κ and j ∈ Jε κ.

mini∈Iε κ µi

ε(m − mb) + ε ∨ maxj∈Jε κ νj

nεκKmin

≤ eusc

i ≤ ε
κ

∨ maxi∈Iε κ µi
mεKmin

 

mb

minj∈Jε κ νj

ε(n − nb) + ε ∨ κ maxi∈Iε κ µi

mεKmin

≤ evsc

j ≤ εκ ∨ maxj∈Jε κ νj

nεKmin

nb

(7)

(8)

4 Theoretical analysis and guarantees

This section is devoted to establishing theoretical guarantees for SCREENKHORN algorithm. We ﬁrst
deﬁne the screened marginals µsc = B(usc  vsc)1m and νsc = B(usc  vsc)(cid:62)1n. Our ﬁrst theoretical
result  Proposition 2  gives an upper bound of the screened marginal violations with respect to
(cid:96)1-norm.
Proposition 2. Let (usc  vsc) be an optimal pair solution of problem (6). Then one has

(cid:107)µ − µsc(cid:107)2

nbcκ + (n − nb)

+

mb

√
nmcµν K 3/2
min

+

m − mb
√
nmKmin

+ log

nm
mbc5/2
µν

(9)

and
(cid:107)ν − νsc(cid:107)2
  (10)
where cz = z − log z − 1 for z > 0 and cµν = µ ∧ ν with µ = mini∈Iε κ µi and ν = minj∈Jε κ νj.

√
nmcµν K 3/2
min

n − nb
√
nmKmin

+ (m − mb)

nm
nbc5/2
µν

mbc 1
κ

+ log

nb

+

+

η

(cid:16) √
(cid:16) √

(cid:17)(cid:17)(cid:17)
(cid:17)(cid:17)(cid:17)

1 = O(cid:16)
1 = O(cid:16)

(cid:16)(cid:107)C(cid:107)∞
(cid:16)(cid:107)C(cid:107)∞

η

5

Algorithm 1: SCREENKHORN(C  η  µ  ν  nb  mb)
Step 1: Screening pre-processing

1. ξ ← sort(µ (cid:11) r(K))  ζ ← sort(ν (cid:11) c(K)); //(decreasing order)
3. Iε κ ← {i = 1  . . .   n : µi ≥ ε2κ−1ri(K)}  Jε κ ← {j = 1  . . .   m : νj ≥ ε2κcj(K)};
4. µ ← mini∈Iε κ µi  ¯µ ← maxi∈Iε κ µi  ν ← minj∈Jε κ νi  ¯ν ← maxj∈Jε κ νi;

ζmb /ξnb;

2. ε ← (ξnb ζmb )1/4  κ ←(cid:112)
5. u ← log(cid:0) ε
6. v ← log(cid:0)εκ ∨

κ ∨

µ
ε(m−mb)+ε∨
ν
ε(n−nb)+ε∨ κ ¯µ

¯ν

nεκKmin

mb

nb

(cid:1)  ¯u ← log(cid:0) ε
(cid:1)  ¯v ← log(cid:0)εκ ∨

κ ∨

(cid:1);
(cid:1);

¯µ

mεKmin

¯ν

nεKmin

mεKmin

7. ¯θ ← stack(¯u1nb   ¯v1mb )  θ ← stack(u1nb   v1mb );
Step 2: L-BFGS-B solver on the screened variables

8. u(0) ← log(εκ−1)1nb   v(0) ← log(εκ)1mb;
9. ˆu  ˆv ← RESTRICTED SINKHORN(u(0)  u(0));
10. θ(0) ← stack(ˆu  ˆv);
11. θ ← L-BFGS-B(θ(0)  θ  ¯θ);
12. θu ← (θ1  . . .   θnb )(cid:62)  θv ← (θnb+1  . . .   θnb+mb )(cid:62);
i ← (θu)i if i ∈ Iε κ and ui ← log(εκ−1) if i ∈ I
13. usc
j ← (θv)j if j ∈ Jε κ and vj ← log(εκ) if j ∈ J
(cid:123)
14. vsc
ε κ;
15. return B(usc  vsc).

(cid:123)
ε κ;

κ

Proof of Proposition 2 is presented in supplementary material and it is based on ﬁrst order optimality
conditions for problem (6) and on a generalization of Pinsker inequality (see Lemma 2 in supplemen-
tary). Note that cκ and c 1
tend to zeros as κ goes to one  which is the case when the number budget
of points (nb  mb) tends to the full one (n  m).
Our second theoretical result  Proposition 3  is an upper bound of the difference between objective
values of SCREENKHORN and dual of Sinkhorn divergence (2).
Proposition 3. Let (usc  vsc) be an optimal pair solution of problem (6) and (u(cid:63)  v(cid:63)) is the pair
solution of dual of Sinkhorn divergence (2). Then we have

Ψε κ(usc  vsc) − Ψ(u(cid:63)  v(cid:63)) = O(cid:0)R((cid:107)µ − µsc(cid:107)1 + (cid:107)ν − νsc(cid:107)1 + ωκ)(cid:1).
η + log(cid:0) (n∨m)2

(cid:1) and ωκ = (1 − κ)(cid:107)µsc(cid:107)1 + (κ−1 − 1)(cid:107)νsc(cid:107)1 + κ−1 − κ.

where R =

(cid:107)C(cid:107)∞

nmc7/2
µν

Proof of Proposition 3 is exposed in the supplementary material. Comparing to some other analysis
results of this quantity  see for instance Lemma 2 in [13] and Lemma 3.1 in [30]  our bound involves
an additional term ωκ (with ω1 = 0)  that tends to zero as the pair budget (nb  mb) goes to the
full number budget of points (n  m) (i.e.  κ goes to 1). To better characterize ωκ  a control of the
(cid:96)1-norms of the screened marginals µsc and νsc are given in Lemma 3 in the supplementary material.

5 Numerical experiments

In this section  we present some numerical analyses of our SCREENKHORN algorithm and show how
it behaves when integrated into some complex machine learning pipelines.

5.1 Setup

We have implemented our SCREENKHORN algorithm in Python and used the L-BFGS-B of Scipy. Re-
garding the machine-learning based comparison  we have based our code on the ones of Python Opti-
mal Transport toolbox (POT) [16] and just replaced the sinkhorn function call with a screenkhorn
one. We have considered the POT’s default SINKHORN stopping criterion parameters and for
SCREENKHORN  the L-BFGS-B algorithm is stopped when the largest component of the projected
gradient is smaller than 10−6  when the number of iterations or the number of objective function
evaluations reach 105. For all applications  we have set η = 1 unless otherwise speciﬁed.

6

Figure 3: Empirical evaluation of SCREENKHORN vs SINKHORN for normalized cost matrix i.e.
(cid:107)C(cid:107)∞ = 1. (most-lefts): marginal violations in relation with the budget of points on n and m .
(center-right) ratio of computation times TSINKHORN
and  (right) relative divergence variation. The
TSCREENKHORN
results are averaged over 30 trials.

5.2 Analysing on toy problem

)K∆(ev(cid:63)

TSINKHORN

TSCREENKHORN

) and P sc = ∆(eusc

We compare SCREENKHORN to SINKHORN as implemented in POT toolbox1 on a synthetic example.
The dataset we use consists of source samples generated from a bi-dimensional gaussian mixture
and target samples following the same distribution but with different gaussian means. We consider
an unsupervised domain adaptation using optimal transport with entropic regularization. Several
settings are explored: different values of η  the regularization parameter  the allowed budget nb
n = mb
m
ranging from 0.01 to 0.99  different values of n and m. We empirically measure marginal violations
as the norms (cid:107)µ − µsc(cid:107)1 and (cid:107)ν − νsc(cid:107)1  running time expressed as
and the relative
divergence difference |(cid:104)C  P (cid:63)(cid:105)−(cid:104)C  P sc(cid:105)|/(cid:104)C  P (cid:63)(cid:105) between SCREENKHORN and SINKHORN  where
P (cid:63) = ∆(eu(cid:63)
). Figure 3 summarizes the observed behaviors
of both algorithms under these settings. We choose to only report results for n = m = 1000 as we
get similar ﬁndings for other values of n and m.
SCREENKHORN provides good approximation of the marginals µ and ν for “high” values of the
regularization parameter η (η > 1). The approximation quality diminishes for small η. As expected
(cid:107)µ − µsc(cid:107)1 and (cid:107)ν − νsc(cid:107)1 converge towards zero when increasing the budget of points. Remarkably
marginal violations are almost negligible whatever the budget for high η. According to computation
gain  SCREENKHORN is almost 2 times faster than SINKHORN at high decimation factor n/nb (low
budget) while the reverse holds when n/nb gets close to 1. Computational beneﬁt of SCREENKHORN
also depends on η with appropriate values η ≤ 1. Finally except for η = 0.1 SCREENKHORN achieves
a divergence (cid:104)C  P(cid:105) close to the one of Sinkhorn showing that our static screening test provides a
reasonable approximation of the Sinkhorn divergence. As such  we believe that SCREENKHORN will
be practically useful in cases where modest accuracy on the divergence is sufﬁcient. This may be the
case of a loss function for a gradient descent method (see next section).

)K∆(evsc

5.3

Integrating SCREENKHORN into machine learning pipelines

Here  we analyse the impact of using SCREENKHORN instead of SINKHORN in a complex machine
learning pipeline. Our two applications are a dimensionality reduction technique  denoted as Wasser-
stein Discriminant Analysis (WDA)  based on Wasserstein distance approximated through Sinkhorn
divergence [17] and a domain-adaptation using optimal transport mapping [10]  named OTDA.
WDA aims at ﬁnding a linear projection which minimize the ratio of distance between intra-class
samples and distance inter-class samples  where the distance is understood in a Sinkhorn divergence
sense. We have used a toy problem involving Gaussian classes with 2 discriminative features
and 8 noisy features and the MNIST dataset. For the former problem  we aim at ﬁnd the best
two-dimensional linear subspace in a WDA sense whereas for MNIST  we look for a subspace
of dimension 20 starting from the original 728 dimensions. Quality of the retrieved subspace are
evaluated using classiﬁcation task based on a 1-nearest neighbour approach.
Figure 4 presents the average gain (over 30 trials) in computational time we get as the number of
examples evolve and for different decimation factors of the SCREENKHORN problem. Analysis of

1https://pot.readthedocs.io/en/stable/index.html

7

1.11.2525102050100Decimation factor n/nb103102101100||sc||1n=m=1000=0.1=0.5=1=101.11.2525102050100Decimation factor mb/m103102101100||sc||1n=m=1000=0.1=0.5=1=101.11.2525102050100Decimation factor n/nb0.51.01.52.02.5Running Time Gainn=m=1000=0.1=0.5=1=101.11.2525102050100Decimation factor n/nb103102101100Relative Divergence Variationn=m=1000=0.1=0.5=1=10Figure 4: Wasserstein Discriminant Analysis : running time gain for (left) a toy dataset and (right)
MNIST as a function of the number of examples and the data decimation factor in SCREENKHORN.

Figure 5: OT Domain adaptation : running time gain for MNIST as a function of the number of
examples and the data decimation factor in SCREENKHORN. Group-lasso hyperparameter values
(left) 1. (right) 10.

the quality of the subspace have been deported to the supplementary material (see Figure 7)  but we
can remark a small loss of performance of SCREENKHORN for the toy problem  while for MNIST 
accuracies are equivalent regardless of the decimation factor. We can note that the minimal gains are
respectively 2 and 4.5 for the toy and MNIST problem whereas the maximal gain for 4000 samples
is slightly larger than an order of magnitude.
For the OT based domain adaptation problem  we have considered the OTDA with (cid:96) 1
2  1 group-lasso
regularizer that helps in exploiting available labels in the source domain. The problem is solved using
a majorization-minimization approach for handling the non-convexity of the problem. Hence  at each
iteration  a SINKHORN/SCREENKHORN has to be computed and the number of iteration is sensitive to
the regularizer strength. As a domain-adaptation problem  we have used a MNIST to USPS problem
in which features have been computed from the ﬁrst layers of a domain adversarial neural networks
[19] before full convergence of the networks (so as to leave room for OT adaptation). Figure 5 reports
the gain in running time for 2 different values of the group-lasso regularizer hyperparameter  while
the curves of performances are reported in the supplementary material. We can note that for all the
SCREENKHORN with different decimation factors  the gain in computation goes from a factor of 4 to
12  without any loss of the accuracy performance.

6 Conclusion

The paper introduces a novel efﬁcient approximation of the Sinkhorn divergence based on a screening
strategy. Screening some of the Sinkhorn dual variables has been made possible by deﬁning a novel
constrained dual problem and by carefully analyzing its optimality conditions. From the latter  we
derived some sufﬁcient conditions depending on the ground cost matrix  that some dual variables are
smaller than a given threshold. Hence  we need just to solve a restricted dual Sinkhorn problem using
an off-the-shelf L-BFGS-B algorithm. We also provide some theoretical guarantees of the quality
of the approximation with respect to the number of variables that have been screened. Numerical
experiments show the behaviour of our SCREENKHORN algorithm and computational time gain it
can achieve when integrated in some complex machine learning pipelines.

8

010002000300040005000Number of samples24681012Running Time GainScreened WDA on toydec=1.5dec=2dec=5dec=10dec=20dec=50dec=1005001000150020002500300035004000Number of samples510152025Running Time GainScreened WDA on mnistdec=1.5dec=2dec=5dec=10dec=20dec=50dec=1005001000150020002500300035004000Number of samples46810Running Time GainScreened OTDA on mnistdec=1.5dec=2dec=5dec=10dec=20dec=50dec=1005001000150020002500300035004000Number of samples4681012Running Time GainScreened OTDA on mnistdec=1.5dec=2dec=5dec=10dec=20dec=50dec=100Acknowledgments

This work was supported by grants from the Normandie Projet GRR-DAISI  European funding
FEDER DAISI and OATMIL ANR-17-CE23-0012 Project of the French National Research Agency
(ANR).

References
[1] B. K. Abid and R. Gower. Stochastic algorithms for entropy-regularized optimal transport problems. In
Amos Storkey and Fernando Perez-Cruz  editors  Proceedings of the Twenty-First International Conference
on Artiﬁcial Intelligence and Statistics  volume 84 of Proceedings of Machine Learning Research  pages
1505–1512  Playa Blanca  Lanzarote  Canary Islands  2018. PMLR.

[2] J. Altschuler  F. Bach  A. Rudi  and J. Weed. Massively scalable Sinkhorn distances via the Nyström

method  2018.

[3] J. Altschuler  J. Weed  and P. Rigollet. Near-linear time approximation algorithms for optimal transport via
Sinkhorn iteration. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and
R. Garnett  editors  Advances in Neural Information Processing Systems 30  pages 1964–1974. Curran
Associates  Inc.  2017.

[4] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein generative adversarial networks. In Doina Precup and
Yee Whye Teh  editors  Proceedings of the 34th International Conference on Machine Learning  volume 70
of Proceedings of Machine Learning Research  pages 214–223  International Convention Centre  Sydney 
Australia  2017. PMLR.

[5] J. D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyré. Iterative bregman projections for regularized

transportation problems. SIAM J. Scientiﬁc Computing  37  2015.

[6] J. Bigot  R. Gouet  T. Klein  and A. López. Geodesic PCA in the Wasserstein space by convex PCA. Ann.

Inst. H. Poincaré Probab. Statist.  53(1):1–26  2017.

[7] M. Blondel  V. Seguy  and A. Rolet. Smooth and sparse optimal transport. In Amos Storkey and Fernando
Perez-Cruz  editors  Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence
and Statistics  volume 84 of Proceedings of Machine Learning Research  pages 880–889  Playa Blanca 
Lanzarote  Canary Islands  2018. PMLR.

[8] N. Bonneel  M. van de Panne  S. Paris  and W. Heidrich. Displacement interpolation using Lagrangian

mass transport. ACM Trans. Graph.  30(6):158:1–158:12  2011.

[9] R. Byrd  P. Lu  J. Nocedal  and C. Zhu. A limited memory algorithm for bound constrained optimization.

SIAM Journal on Scientiﬁc Computing  16(5):1190–1208  1995.

[10] Nicolas Courty  Rémi Flamary  Devis Tuia  and Alain Rakotomamonjy. Optimal transport for domain

adaptation. IEEE transactions on pattern analysis and machine intelligence  39(9):1853–1865  2017.

[11] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges  L. Bottou 
M. Welling  Z. Ghahramani  and K. Q. Weinberger  editors  Advances in Neural Information Processing
Systems 26  pages 2292–2300. Curran Associates  Inc.  2013.

[12] M. Cuturi and G. Peyré. A smoothed dual approach for variational Wasserstein problems. SIAM Journal

on Imaging Sciences  9(1):320–343  2016.

[13] P. Dvurechensky  A. Gasnikov  and A. Kroshnin. Computational optimal transport: Complexity by
accelerated gradient descent is better than by Sinkhorn’s algorithm. In Jennifer Dy and Andreas Krause 
editors  Proceedings of the 35th International Conference on Machine Learning  volume 80 of Proceedings
of Machine Learning Research  pages 1367–1376  Stockholmsmässan  Stockholm Sweden  2018. PMLR.

[14] J. Ebert  V. Spokoiny  and A. Suvorikova. Construction of non-asymptotic conﬁdence sets in 2-Wasserstein

space  2017.

[15] Y. Fei  G. Rong  B. Wang  and W. Wang. Parallel L-BFGS-B algorithm on GPU. Computers & Graphics 

40:1 – 9  2014.

[16] R. Flamary and N. Courty. POT: Python optimal transport library  2017.

[17] R. Flamary  M. Cuturi  N. Courty  and A. Rakotomamonjy. Wasserstein discriminant analysis. Machine

Learning  107(12):1923–1945  2018.

9

[18] C. Frogner  C. Zhang  H. Mobahi  M. Araya  and T. A. Poggio. Learning with a Wasserstein loss.
In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural
Information Processing Systems 28  pages 2053–2061. Curran Associates  Inc.  2015.

[19] Y. Ganin  E. Ustinova  H. Ajakan  P. Germain  H. Larochelle  F. Laviolette  M. Marchand  and V. Lempitsky.
Domain-adversarial training of neural networks. The Journal of Machine Learning Research  17(1):2096–
2030  2016.

[20] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal transport. In
D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information
Processing Systems 29  pages 3440–3448. Curran Associates  Inc.  2016.

[21] L. El Ghaoui  V. Viallon  and T. Rabbani. Safe feature elimination in sparse supervised learning. CoRR 

abs/1009.4219  2010.

[22] N. Ho  X. L. Nguyen  M. Yurochkin  H. H. Bui  V. Huynh  and D. Phung. Multilevel clustering via
Wasserstein means. In Proceedings of the 34th International Conference on Machine Learning - Volume
70  ICML’17  pages 1501–1509. JMLR.org  2017.

[23] B. Kalantari  I. Lari  F. Ricca  and B. Simeone. On the complexity of general matrix scaling and entropy

minimization via the ras algorithm. Mathematical Programming  112(2):371–401  2008.

[24] B. Kalantari and L.Khachiyan. On the complexity of nonnegative-matrix scaling. Linear Algebra and its

Applications  240:87 – 103  1996.

[25] L. Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk  2:227–229  1942.

[26] P. Knight. The Sinkhorn–Knopp algorithm: Convergence and applications. SIAM Journal on Matrix

Analysis and Applications  30(1):261–275  2008.

[27] S. Kolouri  S. R. Park  M. Thorpe  D. Slepcev  and G. K. Rohde. Optimal mass transport: Signal processing

and machine-learning applications. IEEE Signal Processing Magazine  34(4):43–59  2017.

[28] M. Kusner  Y. Sun  N. Kolkin  and K. Weinberger. From word embeddings to document distances. In
Francis Bach and David Blei  editors  Proceedings of the 32nd International Conference on Machine
Learning  volume 37 of Proceedings of Machine Learning Research  pages 957–966  Lille  France  2015.
PMLR.

[29] Y. T. Lee and A. Sidford. Path ﬁnding methods for linear programming: Solving linear programs in
Õ(vrank) iterations and faster algorithms for maximum ﬂow. In Proceedings of the 2014 IEEE 55th Annual
Symposium on Foundations of Computer Science  FOCS ’14  pages 424–433  Washington  DC  USA  2014.
IEEE Computer Society.

[30] T. Lin  N. Ho  and M. I. Jordan. On efﬁcient optimal transport: An analysis of greedy and accelerated

mirror descent algorithms. CoRR  abs/1901.06482  2019.

[31] J. Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of Computation 

35(151):773–782  1980.

[32] V. M. Panaretos and Y. Zemel. Amplitude and phase variation of point processes. Ann. Statist.  44(2):771–

812  2016.

[33] G. Peyré and M. Cuturi. Computational optimal transport. Foundations and Trends R(cid:13) in Machine Learning 

11(5-6):355–607  2019.

[34] Y. Rubner  C. Tomasi  and L. J. Guibas. The earth mover’s distance as a metric for image retrieval.

International Journal of Computer Vision  40(2):99–121  2000.

[35] R. Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The American

Mathematical Monthly  74(4):402–405  1967.

[36] J. Solomon  F. de Goes  G. Peyré  M. Cuturi  A. Butscher  A. Nguyen  T. Du  and L. Guibas. Convolutional
Wasserstein distances: Efﬁcient optimal transportation on geometric domains. ACM Trans. Graph. 
34(4):66:1–66:11  2015.

[37] J. Solomon  R. Rustamov  L. Guibas  and A. Butscher. Wasserstein propagation for semi-supervised
learning. In Eric P. Xing and Tony Jebara  editors  Proceedings of the 31st International Conference on
Machine Learning  volume 32 of Proceedings of Machine Learning Research  pages 306–314  Bejing 
China  2014. PMLR.

10

[38] C. Villani. Optimal Transport: Old and New  volume 338 of Grundlehren der mathematischen Wis-

senschaften. Springer Berlin Heidelberg  2009.

[39] M. Werman  S. Peleg  and A. Rosenfeld. A distance metric for multidimensional histograms. Computer

Vision  Graphics  and Image Processing  32(3):328 – 336  1985.

[40] Y. Xie  X.Wang  R. Wang  and H. Zha. A fast proximal point method for computing Wasserstein distance 

2018.

11

,Mokhtar Z. Alaya
Maxime Berar
Gilles Gasso
Alain Rakotomamonjy