2009,Asymptotically Optimal Regularization in Smooth Parametric Models,Many types of regularization schemes have been employed in statistical learning  each one motivated by some assumption about the problem domain.  In this paper  we present a unified asymptotic analysis of smooth regularizers  which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.  In addition  our analysis motivates an algorithm for optimizing regularization parameters  which in turn can be analyzed within our framework.  We apply our analysis to several examples  including hybrid generative-discriminative learning and multi-task learning.,Asymptotically Optimal Regularization

in Smooth Parametric Models

Percy Liang

University of California  Berkeley

pliang@cs.berkeley.edu

Francis Bach

INRIA - ´Ecole Normale Sup´erieure  France

francis.bach@ens.fr

Guillaume Bouchard

Xerox Research Centre Europe  France

Michael I. Jordan

University of California  Berkeley

Guillaume.Bouchard@xrce.xerox.com

jordan@cs.berkeley.edu

Abstract

Many types of regularization schemes have been employed in statistical learning 
each motivated by some assumption about the problem domain.
In this paper 
we present a uniﬁed asymptotic analysis of smooth regularizers  which allows us
to see how the validity of these assumptions impacts the success of a particular
regularizer. In addition  our analysis motivates an algorithm for optimizing regu-
larization parameters  which in turn can be analyzed within our framework. We
apply our analysis to several examples  including hybrid generative-discriminative
learning and multi-task learning.

1

Introduction

Many problems in machine learning and statistics involve the estimation of parameters from ﬁnite
data. Although empirical risk minimization has favorable limiting properties  it is well known that
this procedure can overﬁt on ﬁnite data. Hence  various forms of regularization have been employed
to control this overﬁtting. Regularizers are usually chosen based on assumptions about the problem
domain at hand. For example  in classiﬁcation  we might use L2 regularization if we expect the data
to be separable with a large margin. We might regularize with a generative model if we think it is
roughly well-speciﬁed [7  20  15  17]. In multi-task learning  we might penalize deviation between
parameters across tasks if we believe the tasks to be similar [3  12  2  13].
In each case  we would like (1) a procedure for choosing the parameters of the regularizer (for exam-
ple  its strength) and (2) an analysis that shows the amount by which regularization reduces expected
risk  expressed as a function of the compatibility between the regularizer and the problem domain.
In this paper  we address these two points by developing an asymptotic analysis of smooth regular-
izers for parametric problems. The key idea is to derive a second-order Taylor approximation of the
expected risk  yielding a simple and interpretable quadratic form which can be directly minimized
with respect to the regularization parameters. We ﬁrst develop the general theory (Section 2) and
then apply it to some examples of common regularizers used in practice (Section 3).

2 General theory
We use uppercase letters (e.g.  L  R  Z) to denote random variables and script letters (e.g.  L R I)
to denote constant limits of random variables. For a λ-parametrized differentiable function θ (cid:55)→
...
f(λ; θ)  let
f denote the ﬁrst  second and third derivatives of f with respect to θ  and
let ∇f(λ; θ) denote the derivative with respect to λ. Let Xn = Op(n−α) denote a sequence of

˙f  ¨f  and

1

P−→ X denote convergence in
random variables for which nαXn is bounded in probability. Let Xn
probability. For a vector v  let v⊗ = vv(cid:62). Expectation and variance operators are denoted as E[·]
and V[·]  respectively.

2.1 Setup
We are given a loss function (cid:96)(·; θ) parametrized by θ ∈ Rd (e.g.  (cid:96)((x  y); θ) = 1
linear regression). Our goal is to minimize the expected risk 

2(y − x(cid:62)θ)2 for

θ∞ def= argmin
θ∈Rd

(1)
which averages the loss over some true data generating distribution p∗(Z). We do not have access
to p∗  but instead receive a sample of n i.i.d. data points Z1  . . .   Zn drawn from p∗. The standard
unregularized estimator minimizes the empirical risk:

L(θ)  L(θ) def= EZ∼p∗[(cid:96)(Z; θ)] 

n(cid:88)

i=1

ˆθ0

n

def= argmin
θ∈Rd

Ln(θ)  Ln(θ) def=

1
n

(cid:96)(Zi  θ).

(2)

Although ˆθ0
n is consistent (that is  it converges in probability to θ∞) under relatively weak condi-
tions  it is well known that regularization can improve performance substantially for ﬁnite n. Let
Rn(λ  θ) be a (possibly data-dependent) regularization function  where λ ∈ Rb are the regulariza-
2n(cid:107)θ(cid:107)2) 
tion parameters. For linear regression  we might use squared regularization (Rn(λ  θ) = λ
where λ ∈ R determines the strength. Deﬁne the regularized estimator as follows:

ˆθλ

n

def= argmin
θ∈Rd

Ln(θ) + Rn(λ  θ).

(3)

n) − L(ˆθ0

The goal of this paper is to choose good values of λ and analyze the subsequent impact on perfor-
mance. Speciﬁcally  we wish to minimize the relative risk:
Ln(λ) def= EZ1 ... Zn∼p∗[L(ˆθλ

(4)
which is the difference in risk (averaged over the training data) between the regularized and unreg-
ularized estimators; Ln(λ) < 0 is desirable. Clearly  argminλ
Ln(λ) is the optimal regularization
parameter. However  it is difﬁcult to get a handle on Ln(λ). Therefore  the main focus of this work is
on deriving an asymptotic expansion for Ln(λ). In this paper  we make the following assumptions:1
Assumption 1 (Compact support). The true distribution p∗(Z) has compact support.
Assumption 2 (Smooth loss). The loss function (cid:96)(z  θ) is thrice-differentiable with respect to θ.
Furthermore  assume the expected Hessian of the loss function is positive deﬁnite ( ¨L(θ∞) (cid:31) 0).2
Assumption 3 (Smooth regularizer). The regularizer Rn(λ  θ) is thrice-differentiable with respect
to θ and differentiable with respect to λ. Assume Rn(0  θ) ≡ 0 and Rn(λ  θ) P−→ 0 as n → ∞.

n)] 

2.2 Rate of regularization strength

n

Let us establish some basic properties that the regularizer Rn(λ  θ) should satisfy. First  a desirable
P−→ θ∞)  i.e.  convergence to the parameters that achieve the minimum
property is consistency (ˆθλ
possible risk in our hypothesis class. To achieve this  it sufﬁces (and in general also necessitates)
that (1) the loss class satisﬁes standard uniform convergence properties [22] and (2) the regularizer
has a vanishing impact in the limit of inﬁnite data (Rn(λ  θ) P−→ 0). These two properties can be
veriﬁed given our assumptions.
The next question is at what rate Rn(λ  θ) should converge to 0? As we show in [16]  Rn(λ  θ) =
Op(n−1) is the rate that minimizes the relative risk Ln. With this rate  it is natural to consider the
regularizer as a prior p(θ | λ) ∝ exp{−Rn(λ  θ)} (and −(cid:96)(z  θ) as the log-likelihood)  in which
case ˆθλ

n is the maximum a posteriori (MAP) estimate.

1While we do not explicitly assume convexity of (cid:96) and Rn  the local nature of our analysis means that we
are essentially working under strong convexity.
2This assumption can be weakened. If ¨L (cid:54)(cid:31) 0  the parameters can only be estimated up to the row space of
¨L. But since we are interested in the parameters θ only in terms of L(θ)  this particular non-identiﬁability of
the parameters is irrelevant.

2

2.3 Asymptotic expansion

Our main result is the following theorem  which provides a simple interpretable asymptotic expres-
sion for the relative risk  characterizing the impact of regularization (see [16] for proof):
Theorem 1. Assume Rn(λ  θ∞) = Op(n−1). The relative risk admits the following asymptotic
expansion:

Ln(λ) = L(λ) · n−2 + Op(n− 5
2 )

(5)

in terms of the asymptotic relative risk:

1
2

L(λ) def=

tr{ ˙R(λ)⊗ ¨L−1} − tr{I(cid:96)(cid:96) ¨L−1 ¨R(λ) ¨L−1} − 2B(cid:62) ˙R(λ) + tr{I(cid:96)r(λ) ¨L−1} 

(6)
where ¨L def= E[¨(cid:96)(Z; θ∞)]  R(λ) def= limn→∞ nRn(λ  θ∞) (derivatives thereof are deﬁned analo-
gously)  I(cid:96)(cid:96)
n − θ∞].
The most important equation of this paper is (6)  which captures the lowest-order terms of the relative
risk deﬁned in (4).

def= E[ ˙(cid:96)(Z; θ∞)⊗]  I(cid:96)r(λ) def= limn→∞ nE[ ˙Ln ˙Rn(λ)(cid:62)]  B def= limn→∞ nE[ˆθ0

Interpretation The signiﬁcance of Theorem 1 is in identifying the three problem-dependent con-
tributions to the asymptotic relative risk:
˙R(λ) is the gradient of the regularizer at the lim-
Squared bias of the regularizer tr{ ˙R(λ)⊗ ¨L−1}:
˙R(λ) with respect to the
iting parameters θ∞; the squared regularizer bias is the squared norm of
Mahalanobis metric given by ¨L. Note that the squared regularizer bias is always positive: it always
increases the risk by an amount which depends on how “wrong” the regularizer is.
Variance reduction provided by the regularizer tr{I(cid:96)(cid:96) ¨L−1 ¨R(λ) ¨L−1}: The key quantity is ¨R(λ) 
the Hessian of the regularizer  whose impact on the relative risk is channeled through ¨L−1 and
I(cid:96)(cid:96). For convex regularizers  ¨R(λ) (cid:23) 0  so we always improve the stability of the estimate by
regularizing. Furthermore  if the loss is the negative log-likelihood and our model is well-speciﬁed
(that is  p∗(z) = exp{−(cid:96)(z; θ∞)})  then I(cid:96)(cid:96) = ¨L by the ﬁrst Bartlett identity [4]  and the variance
reduction term simpliﬁes to tr{ ¨R(λ) ¨L−1}.
Alignment between regularizer bias and unregularized estimator bias 2B(cid:62) ˙R(λ) − tr{I(cid:96)r(λ) ¨L−1}:
The alignment has two parts  the ﬁrst of which is nonzero only for non-linear models and the second
of which is nonzero only when the regularizer depends on the training data. The unregularized
estimator errs in direction B; we can reduce the risk if the regularizer bias ˙R(λ) helps correct for the
estimator bias (B(cid:62) ˙R(λ) > 0). The second part carries the same intuition: the risk is reduced when
the random regularizer compensates for the loss (tr{I(cid:96)r(λ) ¨L−1} < 0).

λ∗ = argmin

L(λ) and call ˆθλ∗

2.4 Oracle regularizer
The principal advantage of having a simple expression for L(λ) is that we can minimize it with
respect to λ. Let λ∗ def= argminλ
n the oracle estimator. We have a closed form for
λ∗ in the important special case that the regularization parameter λ is the strength of the regularizer:
Corollary 1 (Oracle regularization strength). If Rn(λ  θ) = λ
tr{I(cid:96)(cid:96) ¨L−1¨r ¨L−1} + 2B(cid:62) ˙r

n r(θ) for some r(θ)  then
  L(λ∗) = − C2
2C2
Proof. (6) is a quadratic in λ; solve by differentiation. Compute L(λ∗) by substitution.
In general  λ∗ will depend on θ∞ and hence is not computable from data; Section 2.5 will remedy
this. Nevertheless  the oracle regularizer provides an upper bound on performance and some insight
into the relevant quantities that make a regularizer useful.
Note L(λ∗) ≤ 0  since optimizing λ∗ must be no worse than not regularizing since L(0) = 0.
But what might be surprising at ﬁrst is that the oracle regularization parameter λ∗ can be negative

˙r(cid:62) ¨L−1 ˙r

L(λ) =

(7)

def=

C1C2

λ

1

.

3

UNREGULARIZED ORACLE

Estimator
Notation

Relative risk

ˆθ0
n
0

ˆθλ∗
L(λ∗)

n

PLUGIN
n = ˆθ•1
ˆθ ˆλn
L•(1)

n

ORACLEPLUGIN

ˆθ•λ•∗
L•(λ•∗)

n

Table 1: Notation for the various estimators and their relative risks.

(corresponding to “anti-regularization”). But if ∂L(λ)
helps (λ∗ > 0 and L(λ) < 0 for 0 < λ < 2λ∗).

∂λ = −C1 < 0  then (positive) regularization

n

n

n = ˆθ•1

n(λ•) = E[L(ˆθ•λ•

n ) − L(ˆθ•0

def= argminθ Ln(θ) + R•

n(λ•  θ) has relative risk L•

2 ). We then use the plugin estimator ˆθˆλn

n   which means the asymptotic risk of the plugin estimator ˆθˆλn

def= argminθ Ln(θ) + Rn(ˆλn  θ).
n ) − L(ˆθ0

2.5 Plugin regularizer
While the oracle regularizer Rn(λ∗  θ) given by (7) is asymptotically optimal  λ∗ depends on the
unknown θ∞  so ˆθλ∗
n is actually not implementable. In this section  we develop the plugin regularizer
as a way to avoid this dependence. The key idea is to substitute λ∗ with an estimate ˆλn
def= λ∗ + εn
where εn = Op(n− 1
How well does this plugin estimator work  that is  what is its relative risk E[L(ˆθˆλn
n)]?
We cannot simply write Ln(ˆλn) and apply Theorem 1 because L(·) can only be applied to non-
random arguments. However  we can still leverage existing machinery by deﬁning a new plugin
n(λ•  θ) def= λ•Rn(ˆλn  θ) with regularization parameter λ• ∈ R. Henceforth  the
regularizer R•
superscript • will denote quantities concerning the plugin regularizer. The corresponding estimator
ˆθ•λ•
n )]. The key
n is simply L•(1).
identity is ˆθˆλn
We could try to squeeze more out of the plugin regularizer by further optimizing λ• according to
rather than just using λ• =
λ•∗ def= argminλ• L•(λ•) and use the oracle plugin estimator ˆθ•λ•∗
In general  this is not useful since λ•∗ might depend on θ∞  and the whole point of plugin
1.
is to remove this dependence. However  in a fortuitous turn of events  for some linear models
(Sections 3.1 and 3.4)  λ•∗ is in fact independent of θ∞  and so ˆθ•λ•∗
is actually implementable.
Table 1 summarizes all the estimators we have discussed.
The following theorem relates the risks of all estimators we have considered (see [16] for the proof):
Theorem 2 (Relative risk of plugin). The relative risk of the plugin estimator is L•(1) = L(λ∗)+E 
where E def= limn→∞ nE[tr{ ˙Ln(∇ ˙Rn(λ∗)εn)(cid:62) ¨L−1}]. If Rn(λ) is linear in λ  then the relative risk
of the oracle plugin estimator is L•(λ•∗) = L•(1) + E2
Note that the sign of E depends on the nature of the error εn  so PLUGIN could be either better or
worse than ORACLE. On the other hand  ORACLEPLUGIN is always better than PLUGIN. We can
get a simpler expression for E if we know more about εn (see [16] for the proof):
Theorem 3. Suppose λ∗ = f(θ∞) for some differentiable f : Rd → Rb. If ˆλn = f(ˆθ0
results of Theorem 2 hold with E = −tr{I(cid:96)(cid:96) ¨L−1∇ ˙R(λ∗) ˙f ¨L−1}.

4L(λ∗) with λ•∗ = 1 + E

2L(λ∗) .

n)  then the

n

n

3 Examples

In this section  we apply our results from Section 2 to speciﬁc problems. Having made all the
asymptotic derivations in the general setting  we now only need to make a few straightforward
calculations to obtain the asymptotic relative risks and regularization parameters for a given problem.
We ﬁrst explore two classical examples from statistics (Sections 3.1 and 3.2) to get some intuition
for the theory. Then we consider two important examples in machine learning (Sections 3.3 and 3.4).

3.1 Estimation of normal means

Assume that data are generated from a multivariate normal distribution with d independent compo-
2(x−θ)2 
nents (p∗ = N (θ∞  I)). We use the negative log-likelihood as the loss function: (cid:96)(x; θ) = 1
so the model is well-speciﬁed.

4

(cid:16)

(cid:17)
2(cid:107)θ(cid:107)2).

(cid:80)n
n )] < E[L(ˆθ0

n

n

def= ¯X

2(cid:107)θ∞(cid:107)2 .

1 − d−2
n(cid:107) ¯X(cid:107)2

i=1 Xi is beaten by the James-Stein estimator ˆθJS

(cid:107)θ(cid:107)4 . By Theorems 2
2(cid:107)θ∞(cid:107)2 . Note that since E > 0  PLUGIN is always (asymptoti-

In his seminal 1961 paper [14]  Stein showed that  surprisingly  the standard empirical risk minimizer
ˆθ0
n = ¯X def= 1
in the sense
that E[L(ˆθJS
n)] for all n and θ∞ if d > 2. We will show that the James-Stein estimator
is essentially equivalent to ORACLEPLUGIN with quadratic regularization (r(θ) = 1
First compute ˙Ln = θ∞ − ¯X  ¨L = I  B = 0  ˙r = θ∞  and ¨r = I. By (7)  the oracle regularization
weight is λ∗ = d(cid:107)θ∞(cid:107)2   which yields a relative risk of L(λ∗) = − d2
Now let us derive PLUGIN (Section 2.5). We have f(θ) = d(cid:107)θ(cid:107)2 and ˙f(θ) = −2dθ
and 3  E = 2d(cid:107)θ∞(cid:107)2 and L•(1) = − d(d−4)
cally) worse than ORACLE but better than UNREGULARIZED if d > 4.
To get ORACLEPLUGIN  compute λ•∗ = 1− 2
in R•
small improvement over PLUGIN (and is superior to UNREGULARIZED when d > 2).
Note that the ORACLEPLUGIN and PLUGIN are adaptive: We regularize more or less depend-
ing on whether our preliminary estimate of ¯X is small or large  respectively. By simple al-
gebra  ORACLEPLUGIN has a closed form ˆθ•λ•∗
  which differs from
n − ˆθJS
JAMESSTEIN by a very small amount: ˆθ•λ•∗
2 ). ORACLEPLUGIN has the added
beneﬁt that it always shrinks towards zero by an amount between 0 and 1  whereas JAMESSTEIN can
overshoot. Empirically  we found that ORACLEPLUGIN generally had a lower expected risk than
JAMESSTEIN when (cid:107)θ∞(cid:107) is large  but JAMESSTEIN was better when (cid:107)θ∞(cid:107) ≤ 1.

1− 2
(cid:107) ¯X(cid:107)2(cid:107)θ(cid:107)2. By Theorem 2  its relative risk is L•(λ•∗) = − (d−2)2
(cid:17)

d (note that this doesn’t depend on θ∞)  which results
2(cid:107)θ∞(cid:107)2   which offers a

1 −
n = Op(n− 5

n(θ) = 1

n(cid:107) ¯X(cid:107)2+d−2

= ¯X

(cid:16)

d

2

d−2

n

3.2 Binomial estimation

2(θ + 2 log(1 + e−θ))  which corresponds to a Beta( λ

Consider the estimation of θ  the log-odds of a coin coming up heads. We use the negative log-
likelihood loss (cid:96)(x; θ) = −xθ + log(1 + eθ)  where x ∈ {0  1} is the outcome of the coin. This
example serves to provide intuition for the bias B appearing in (6)  which is typically ignored in
ﬁrst-order asymptotics or is zero (for linear models).
Consider a regularizer r(θ) = 1
2 ) prior.
Choosing λ has been studied extensively in statistics. Some common choices are the Haldane prior
(λ = 0)  the reference (Jeffreys) prior (λ = 1)  the uniform prior (λ = 2)  and Laplace smoothing
(λ = 4). We will choose λ to minimize expected risk adaptively based on data.
2. Then compute ¨L = v 

Deﬁne µ def=
¨r = v  B = −v−1b. ORACLE corresponds to λ∗ = 2 + v
regularization always helps.
√
4   E > 0 
We can compute the difference between ORACLE and PLUGIN: E = 2 − 2v
which means that PLUGIN is worse; otherwise PLUGIN is actually better. Even when PLUGIN
is worse than ORACLE  PLUGIN is still better than UNREGULARIZED  which can be veriﬁed by
checking that L•(1) = − 5

...L = −2vb  ˙r = b 
b2 . Note that λ∗ > 0  so again (positive)

1+e−θ∞   v def= µ(1 − µ)  and b def= µ − 1

2 vb−2 − 2v−1b2 < 0 for all θ∞.

b2 . If |b| >

2   λ

1

2

3.3 Hybrid generative-discriminative learning
In prediction tasks  we wish to learn a mapping from some input x ∈ X to an output y ∈ Y. A
common approach is to use probabilistic models deﬁned by exponential families  which is deﬁned
by a vector of sufﬁcient statistics (features) φ(x  y) ∈ Rd and an accompanying vector of parameters
θ ∈ Rd. These features can be used to deﬁne a generative model (8) or a discriminative model (9):

pθ(x  y) = exp{φ(x  y)(cid:62)θ − A(θ)}  A(θ) = log

X
pθ(y | x) = exp{φ(x  y)(cid:62)θ − A(θ; x)}  A(θ; x) = log

exp{φ(x  y)(cid:62)θ}dydx 

exp{φ(x  y)(cid:62)θ}dy.

(8)

(9)

(cid:90)

(cid:90)

(cid:90)

Y

Y

5

Misspeciﬁcation
0%
5%
50%

tr{I(cid:96)(cid:96)v−1
x }
x vv−1
5

5.38
13.8

2B(cid:62)(µ − µxy)

tr{(µ − µxy)⊗v−1
x }

0

-0.073
-1.0

0

0.00098
0.034

λ∗
L(λ∗)
∞ -0.65
-48
310
230
-808

Table 2: The oracle regularizer for the hybrid generative-discriminative estimator. As misspeci-
ﬁcation increases  we regularize less  but the relative risk is reduced more (due to more variance
reduction).

n

n

n

n

def= argminθ Gn(θ)  where
def= argminθ Dn(θ)  where

(cid:80)n
Given these deﬁnitions  we can either use a generative estimator ˆθgen
(cid:80)n
Gn(θ) = − 1
i=1 log pθ(x  y) or a discriminative estimator ˆθdis
i=1 log pθ(y | x).
Dn(θ) = − 1
There has been a ﬂurry of work on combining generative and discriminative learning [7  20  15 
18  17]. [17] showed that if the generative model is well-speciﬁed (p∗(x  y) = pθ∞(x  y))  then
the generative estimator is better in the sense that L(ˆθgen
2 ) for some
c ≥ 0; if the model is misspeciﬁed  the discriminative estimator is asymptotically better. To create a
hybrid estimator  let us treat the discriminative and generative objectives as the empirical risk and the
regularizer  respectively  so (cid:96)((x  y); θ) = − log pθ(y | x)  so Ln = Dn and Rn(λ  θ) = λ
n Gn(θ).
As n → ∞  the discriminative objective dominates as desired. Our approach generalizes the analysis
of [6]  which applies only to unbiased estimators for conditionally well-speciﬁed models.
By moment-generating properties of the exponential family  we arrive at the following quanti-
ties (write φ for φ(X  Y )): ¨L = vx
˙R(λ) = λ(µ − µxy) def=
λ(Epθ∞ (X Y )[φ] − Ep∗(X Y )[φ])  and ¨R(λ) = λv def= λVpθ∞ (X Y )[φ]. The oracle regularization
parameter is then

def= Ep∗(X)[Vpθ∞ (Y |X)[φ|X]] 

n + Op(n− 3

n ) ≤ L(ˆθdis

n ) − c

λ∗ =

tr{I(cid:96)(cid:96)v−1

x vv−1

x }
x } + 2B(cid:62)(µ − µxy) − tr{I(cid:96)rv−1
tr{(µ − µxy)⊗v−1
x }

.

(10)
The sign and magnitude of λ∗ provides insight into how generative regularization improves pre-
diction as a function of the model and problem: Speciﬁcally  a large positive λ∗ suggests regu-
larization is helpful. To simplify  assume that the discriminative model is well-speciﬁed  that is 
p∗(y | x) = pθ∞(y | x) (note that the generative model could still be misspeciﬁed). In this case 
I(cid:96)(cid:96) = ¨L  I(cid:96)r = vx  and so the numerator reduces to tr{(v − vx)v−1
Since v (cid:23) vx (the key fact used in [17])  the variance reduction (plus the random alignment term
from I(cid:96)r) is always non-negative with magnitude equal to the fraction of missing information pro-
vided by the generative model. There is still the non-random alignment term 2B(cid:62)(µ − µxy)  whose
sign depends on the problem. Finally  the denominator (always positive) affects the optimal magni-
tude of the regularization. If the generative model is almost well-speciﬁed  µ will be close to µxy 
and the regularizer should be trusted more (large λ∗). Since our analysis is local  misspeciﬁcation
(how much pθ∞(x  y) deviates from p∗(x  y)) is measured by a Mahalanobis distance between µ
and µxy  rather than something more stringent and global like KL-divergence.

x } + 2B(cid:62)(µ − µxy).

An empirical example To provide some concrete intuition  we investigated the oracle regularizer
for a synthetic binary classiﬁcation problem of predicting y ∈ {0  1} from x ∈ {0  1}k. Using
features φ(x  y) = (I[y = 0]x(cid:62)  I[y = 1]x(cid:62))(cid:62) deﬁnes the corresponding generative (Naive Bayes)
10)(cid:62) 
and discriminative (logistic regression) estimators. We set k = 5  θ∞ = ( 1
and p∗(x  y) = (1 − ε)pθ∞(x  y) + εpθ∞(y)pθ∞(x1 | y)I[x1 = ··· = xk]. The amount of mis-
speciﬁcation is controlled by 0 ≤ ε ≤ 1  the fraction of examples whose features are perfectly
correlated.
Table 2 shows how the oracle regularizer changes with ε. As ε increases  λ∗ decreases (we regularize
less) as expected. But perhaps surprisingly  the relative risk is reduced with more misspeciﬁcation;
this is due to the fact that the variance reduction term increases and has a quadratic effect on L(λ∗).
Figure 1(a) shows the relative risk Ln(λ) for various values of λ. The vertical line corresponds
to λ∗  which was computed numerically by sampling. Note that the minimum of the curves

10  ···   1

10  ···   3

10   3

6

Ln(λ))  the desired quantity  is quite close to λ∗ and approaches λ∗ as n increases  which

(argminλ
empirically justiﬁes our asymptotic approximations.

(cid:80)m

Unlabeled data One of the main advantages of having a generative model is that we can lever-
age unlabeled examples by marginalizing out their hidden outputs. Speciﬁcally  suppose we have
m i.i.d. unlabeled examples Xn+1  . . .   Xn+m ∼ p∗(x)  with m → ∞ as n → ∞. Deﬁne the
unlabeled regularizer as Rn(λ  θ) = − λ
We can compute ˙R = µ − µxy using the stationary conditions of the loss function at θ∞. Also 
¨R = v − vx  and I(cid:96)r = 0 (the regularizer doesn’t depend on the labeled data). If the model is
conditionally well-speciﬁed  we can verify that the oracle regularization parameter λ∗ is the same as
if we had regularized with Gn. This equivalence suggests that the dominant concern asymptotically
is developing an adequate generative model with small bias and not exactly how it is used in learning.

i=1 log pθ(Xn+i).

nm

3.4 Multi-task regression

1

2tr{Λ2Θ(cid:62)

2 d2tr{(Θ(cid:62)

  . . .   X K(cid:62)

i

i

i ∼ p∗(X k

i ) and Y k

i ∼ N (X k(cid:62)

i = Id  which implies that I(cid:96)(cid:96) = ¨L = IKd.

The intuition behind multi-task learning is to share statistical strength between tasks [3  12  2  13].
Suppose we have K regression tasks. For each task k = 1  . . .   K  we generate each data point
i θk∞  1). We can treat this
i = 1  . . .   n independently as follows: X k
)(cid:62) ∈
as a single task problem by concatenating the vectors for all the tasks: Xi = (X 1(cid:62)
RKd  Y = (Y 1  . . .   Y K)(cid:62) ∈ RK  and θ = (θ1(cid:62)  . . .   θK(cid:62))(cid:62) ∈ RKd. It will also be useful to
represent θ ∈ RKd by the matrix Θ = (θ1  . . .   θK) ∈ Rd×K. The loss function is (cid:96)((x  y)  θ) =

(cid:80)K
k=1(yk − xk(cid:62)θk)2. Assume the model is conditionally well-speciﬁed.

1
2
We would like to be ﬂexible in case some tasks are more related than others  so let us deﬁne a positive
deﬁnite matrix Λ ∈ RK×K of inter-task afﬁnities and use the quadratic regularizer: r(Λ  θ) =
2 θ(cid:62)(Λ ⊗ Id)θ. For simplicity  assume EX k⊗
Most of the computations that follow parallel those of Section 3.1  only extended to matrices. Sub-
∞Θ∞} − dtr{Λ}.
stituting the relevant quantities into (6) yields the relative risk: L(Λ) = 1
Optimizing with respect to Λ produces the oracle regularization parameter Λ∗ = d(Θ(cid:62)
∞Θ∞)−1 and
its associated relative risk L(Λ∗) = − 1
To analyze PLUGIN  ﬁrst compute ˙f = −d(Θ(cid:62)
increases the asymptotic risk by E = 2dtr{(Θ(cid:62)
still favorable when d > 4  as L•(1) = − 1
We can do slightly better using ORACLEPLUGIN (λ•∗ = 1 − 2
d)  which results in a relative risk of
L•(λ•∗) = − 1
∞Θ∞)−1}. For comparison  if we had solved the K regression tasks
completely independently with K independent regularization parameters  our relative risk would
have been − 1
We now compare joint versus independent regularization. Let A = Θ(cid:62)
∞Θ∞ with eigendecompo-
sition A = U DU(cid:62). The difference in relative risks between joint and independent regularization
is ∆ = − 1
kk ) (∆ < 0 means joint regularization is better). The gap
between joint and independent regularization is large when the tasks are non-trivial but similar (θk∞s
are close  but (cid:107)θk∞(cid:107) is large). In that case  D−1
kk s are small.

∞Θ∞)−1}.
∞Θ∞)−1; we ﬁnd that PLUGIN
∞Θ∞)−1(2Θ(cid:62)
∞Θ∞)−1}. However  the relative risk of PLUGIN is
2 d(d − 4)tr{(Θ(cid:62)

2(d − 2)2((cid:80)K
2(d − 2)2((cid:80)

k=1 (cid:107)θk∞(cid:107)−2) (following similar but simpler computations).

∞(·))(Θ(cid:62)
∞Θ∞)−1} < 0 for d > 4.

kk is quite large for k > 1  but all the A−1

kk −(cid:80)

k D−1

2(d − 2)2tr{(Θ(cid:62)

k A−1

MHC-I binding prediction We evaluated our multitask regularization method on the IEDB
MHC-I peptide binding dataset created by [19] and used by [13]. The goal here is to predict the
binding afﬁnity (represented by log IC50) of a MHC-I molecule given its amino-acid sequence (rep-
resented by a vector of binary features  reduced to a 20-dimensional real vector using SVD). We
created ﬁve regression tasks corresponding to the ﬁve most common MHC-I molecules.
We compared four estimators: UNREGULARIZED  DIAGCV (Λ = cI)  UNIFORMCV (using
the same task-afﬁnity for all pairs of tasks with Λ = c(1⊗ + 10−5I))  and PLUGINCV (Λ =
cd( ˆΘ(cid:62)
ˆΘn)−1)  where c was chosen by cross-validation.3 Figure 1 shows the results averaged over

n

3We performed three-fold cross-validation to select c from 21 candidates in [10−5  105].

7

(a)

(b)

Figure 1: (a) Relative risk (Ln(λ)) of the hybrid generative/discriminative estimator for various λ;
the λ attaining the minimum of Ln(λ) is close to the oracle λ∗ (the vertical line). (b) On the MHC-
I binding prediction task  test risk for the four multi-task estimators; PLUGINCV (estimating all
pairwise task afﬁnities using PLUGIN and cross-validating the strength) works best.

30 independent train/test splits. Multi-task regularization actually performs worse than independent
learning (DIAGCV) if we assume all tasks are equally related (UNIFORMCV). By learning the full
matrix of task afﬁnities (PLUGINCV)  we obtain the best results. Note that setting the O(K 2) entries
of Λ via cross-validation is not computationally feasible  though other approaches are possible [13].

4 Related work and discussion

The subject of choosing regularization parameters has received much attention. Much of the learning
theory literature focuses on risk bounds  which approximate the expected risk (L(ˆθλ
n)) with upper
bounds. Our analysis provides a different type of approximation—one that is exact in the ﬁrst few
terms of the expansion. Though we cannot make a precise statement about the risk for any given n 
exact control over the ﬁrst few terms offers other advantages  e.g.  the ability to compare estimators.
To elaborate further  risk bounds are generally based on the complexity of the hypothesis class 
whereas our analysis is based on the variance of the estimator. Vanilla uniform convergence bounds
yield worst-case analyses  whereas our asymptotic analysis is tailored to a particular problem (p∗
and θ∞) and algorithm (estimator). Localization techniques [5]  regret analyses [9]  and stability-
based bounds [8] all allow for some degree of problem- and algorithm-dependence. As bounds 
however  they necessarily have some looseness  whereas our analysis provides exact constants  at
least the ones associated with the lowest-order terms.
Asymptotics has a rich tradition in statistics.
In fact  our methodology of performing a Taylor
expansion of the risk is reminiscent of AIC [1]. However  our aim is different: AIC is intended
for model selection  whereas we are interested in optimizing regularization parameters. The Stein
unbiased risk estimate (SURE) is another method of estimating the expected risk for linear models
[21]  with generalizations to non-linear models [11].
In practice  cross-validation procedures [10] are quite effective. However  they are only feasible
when the number of hyperparameters is very small  whereas our approach can optimize many hy-
perparameters. Section 3.4 showed that combining the two approaches can be effective.
To conclude  we have developed a general asymptotic framework for analyzing regularization  along
with an efﬁcient procedure for choosing regularization parameters. Although we are so far restricted
to parametric problems with smooth losses and regularizers  we think that these tools provide a
complementary perspective on analyzing learning algorithms to that of risk bounds  deepening our
understanding of regularization.

8

100102104−0.025−0.02−0.015−0.01−0.0050relative riskregularization n= 75n=100n=150minimumoracle reg.200300500800100015001314151617number of training points (n)test risk "unregularized""diag CV""uniform CV""plugin CV"References
[1] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic

Control  19:716–723  1974.

[2] A. Argyriou  T. Evgeniou  and M. Pontil. Multi-task feature learning. In Advances in Neural

Information Processing Systems (NIPS)  pages 41–48  2007.

[3] B. Bakker and T. Heskes. Task clustering and gating for Bayesian multitask learning. Journal

of Machine Learning Research  4:83–99  2003.

[4] M. S. Bartlett. Approximate conﬁdence intervals. II. More than one unknown parameter.

Biometrika  40:306–317  1953.

[5] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. Annals of

Statistics  33(4):1497–1537  2005.

[6] G. Bouchard. Bias-variance tradeoff in hybrid generative-discriminative models.

In Sixth
International Conference on Machine Learning and Applications (ICMLA)  pages 124–129 
2007.

[7] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classiﬁers. In

International Conference on Computational Statistics  pages 721–728  2004.

[8] O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning

Research  2:499–526  2002.

[9] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[10] P. Craven and G. Wahba. Smoothing noisy data with spline functions. estimating the correct
degree of smoothing by the method of generalized cross-validation. Numerische Mathematik 
31(4):377–403  1978.

[11] Y. C. Eldar. Generalized SURE for exponential families: Applications to regularization. IEEE

Transactions on Signal Processing  57(2):471–481  2009.

[12] T. Evgeniou  C. Micchelli  and M. Pontil. Learning multiple tasks with kernel methods. Jour-

nal of Machine Learning Research  6:615–637  2005.

[13] L. Jacob  F. Bach  and J. Vert. Clustered multi-task learning: A convex formulation. In Ad-

vances in Neural Information Processing Systems (NIPS)  pages 745–752  2009.

[14] W. James and C. Stein. Estimation with quadratic loss.

In Fourth Berkeley Symposium in

Mathematics  Statistics  and Probability  pages 361–380  1961.

[15] J. A. Lasserre  C. M. Bishop  and T. P. Minka. Principled hybrids of generative and discrimi-

native models. In Computer Vision and Pattern Recognition (CVPR)  pages 87–94  2006.

[16] P. Liang  F. Bach  G. Bouchard  and M. I. Jordan. Asymptotically optimal regularization in

smooth parametric models. Technical report  ArXiv  2010.

[17] P. Liang and M. I. Jordan. An asymptotic analysis of generative  discriminative  and pseudo-

likelihood estimators. In International Conference on Machine Learning (ICML)  2008.

[18] A. McCallum  C. Pal  G. Druck  and X. Wang. Multi-conditional learning: Genera-
tive/discriminative training for clustering and classiﬁcation. In Association for the Advance-
ment of Artiﬁcial Intelligence (AAAI)  2006.

[19] B. Peters  H. Bui  S. Frankild  M. Nielson  C. Lundegaard  E. Kostem  D. Basch  K. Lam-
berth  M. Harndahl  W. Fleri  S. S. Wilson  J. Sidney  O. Lund  S. Buus  and A. Sette. A
community resource benchmarking predictions of peptide binding to MHC-I molecules. PLoS
Compututational Biology  2  2006.

[20] R. Raina  Y. Shen  A. Ng  and A. McCallum.

Classiﬁcation with hybrid genera-
In Advances in Neural Information Processing Systems (NIPS) 

tive/discriminative models.
2004.

[21] C. M. Stein. Estimation of the mean of a multivariate normal distribution. Annals of Statistics 

9(6):1135–1151  1981.

[22] A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press  1998.

9

,Yining Wang
Jun Zhu
Boris Belousov
Gerhard Neumann
Constantin Rothkopf
Jan Peters