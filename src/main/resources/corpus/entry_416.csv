2017,Adaptive SVRG Methods under Error Bound Conditions with Unknown Growth Parameter,Error bound  an inherent property of an optimization problem  has recently revived in the development of algorithms with improved global convergence without strong convexity. The most studied error bound  is the quadratic error bound  which generalizes strong convexity and is satisfied by a large family of machine learning problems. Quadratic error bound have been leveraged to achieve linear convergence in many first-order methods including the stochastic variance reduced gradient (SVRG) method  which is one of the most important stochastic optimization methods in machine learning. However  the studies along this direction face the critical issue that the algorithms must depend on an unknown growth parameter (a generalization of strong convexity modulus) in the error bound. This parameter is difficult to estimate exactly and the algorithms choosing this parameter heuristically do not have theoretical convergence guarantee. To address this issue  we propose novel SVRG methods that automatically search for this unknown parameter on the fly of optimization while still obtain almost the same convergence rate as when this parameter is known. We also analyze the convergence property of SVRG methods under H\"{o}lderian error bound  which generalizes the quadratic error bound.,Adaptive SVRG Methods under Error Bound
Conditions with Unknown Growth Parameter

Yi Xu†  Qihang Lin‡  Tianbao Yang†

†Department of Computer Science  The University of Iowa  Iowa City  IA 52242  USA
‡Department of Management Sciences  The University of Iowa  Iowa City  IA 52242  USA

{yi-xu  qihang-lin  tianbao-yang}@uiowa.edu

Abstract

Error bound  an inherent property of an optimization problem  has recently revived
in the development of algorithms with improved global convergence without strong
convexity. The most studied error bound is the quadratic error bound  which
generalizes strong convexity and is satisﬁed by a large family of machine learning
problems. Quadratic error bound have been leveraged to achieve linear convergence
in many ﬁrst-order methods including the stochastic variance reduced gradient
(SVRG) method  which is one of the most important stochastic optimization
methods in machine learning. However  the studies along this direction face the
critical issue that the algorithms must depend on an unknown growth parameter (a
generalization of strong convexity modulus) in the error bound. This parameter is
difﬁcult to estimate exactly and the algorithms choosing this parameter heuristically
do not have theoretical convergence guarantee. To address this issue  we propose
novel SVRG methods that automatically search for this unknown parameter on the
ﬂy of optimization while still obtain almost the same convergence rate as when this
parameter is known. We also analyze the convergence property of SVRG methods
under Hölderian error bound  which generalizes the quadratic error bound.

1

Introduction

Finite-sum optimization problems have broad applications in machine learning  including regres-
sion by minimizing the (regularized) empirical square losses and classiﬁcation by minimizing the
(regularized) empirical logistic losses. In this paper  we consider the following ﬁnite-sum problem:

fi(x) + Ψ(x) 

(1)

n(cid:88)

i=1

min
x∈Ω

F (x) (cid:44) 1
n

where fi(x) is a continuously differential convex function whose gradient is Lipschitz continuous
and Ψ(x) is a proper  lower-semicontinuous convex function [24]. Traditional proximal gradient (PG)
methods or accelerated proximal gradient (APG) methods for solving (1) become prohibited when
the number of components n is very large  which has spurred many studies on developing stochastic
optimization algorithms with fast convergence [4  8  25  1].
An important milestone among several others is the stochastic variance reduced gradient (SVRG)
method [8] and its proximal variant [26]. Under the strong convexity of the objective function F (x) 
linear convergence of SVRG and its proximal variant has been established. Many variations of SVRG
have also been proposed [2  1]. However  the key assumption of strong convexity limits the power of
SVRG for many interesting problems in machine learning without strong convexity. For example  in
regression with high-dimensional data one is usually interested in solving the least-squares regression
with an (cid:96)1 norm regularization or constraint (known as the LASSO-type problem). A common
practice for solving non-strongly convex ﬁnite-sum problems by SVRG is to add a small strongly
convex regularizer (e.g.  λ
2) [26]. Recently  a variant of SVRG (named SVRG++ [2]) was

2(cid:107)x(cid:107)2

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

designed that can cope with non-strongly convex problems without adding the strongly convex
term. However  these approaches only have sublinear convergence (e.g.  requiring a O(1/) iteration
complexity to achieve an -optimal solution).
Promisingly  recent studies on optimization showed that leveraging the quadratic error bound (QEB)
condition can open a new door to the linear convergence without strong convexity [9  20  6  30  5  3].
The problem (1) obeys the QEB condition if the following holds:

(cid:107)x − x∗(cid:107)2 ≤ c(F (x) − F (x∗))1/2 ∀x ∈ Ω 

(2)

where x∗ denotes the closest optimal solution to x and Ω is usually a compact set. Indeed  the
aforementioned LASSO-type problems satisfy the QEB condition. It is worth mentioning that the
above condition (or similar conditions) has been explored extensively and has different names in
the literature  e.g.  the second-order growth condition  the weak strong convexity [20]  essential
strong convexity [13]  restricted strong convexity [31]  optimal strong convexity [13]  semi-strong
convexity [6]. Interestingly  [6  9] have showed that SVRG can enjoy a linear convergence under the
QEB condition. However  the issue is that SVRG requires to know the parameter c (analogous to the
strong convexity parameter) in the QEB for setting the number of iterations of inner loops  which is
usually unknown and difﬁcult to estimate. A naive trick for setting the number of iterations of inner
loops to a certain multiplicative factor (e.g.  2) of the number of components n is usually sub-optimal
and worrisome because it may not be large enough for bad conditioned problems or it could be too
large for good conditioned problems. In the former case  the algorithm may not converge as the
theory indicates and in the latter case  too many iterations may be wasted for inner loops.
To address this issue  we develop a new variant of SVRG that embeds an efﬁcient automatic search
step for c into the optimization. The challenge for developing such an adaptive variant of SVRG is
that one needs to develop an appropriate machinery to check whether the current value of c is large
enough. One might be reminded of some restarting procedure for searching the unknown strong
convexity parameter in APG methods [21  11]. However  there are several differences that make
the development of such a search scheme much more daunting for SVRG than for APG. The ﬁrst
difference is that  although SVRG has a lower per-iteration cost than APG  it also makes smaller
progress towards the optimality after each iteration  which provides less information on the correctness
of the current c. The second difference lies at that the SVRG is inherently stochastic  making the
analysis for bounding the number of search steps much more difﬁcult. To tackle this challenge  we
propose to perform the proximal gradient updates occasionally at the reference points in SVRG where
the full gradient is naturally computed. The normal of the proximal gradient provides a probabilistic
“certiﬁcate" for checking whether the value of c is large enough. We then provide a novel analysis to
bound the expected number of search steps with a consideration that the probabilistic “certiﬁcate"
might fail with some probability. The ﬁnal result shows that the new variant of SVRG enjoys a linear
convergence under the QEB condition with unknown c and the corresponding complexity is only
worse by a logarithmic factor than that in the setting where the parameter c is assumed to be known.
Besides the QEB condition  we also consider more general error bound conditions (aka the Hölderian
error bound (HEB) conditions [3]) whose deﬁnition is given below  and develop adaptive variants of
SVRG under the HEB condition with θ ∈ (0  1/2) to enjoy intermediate faster convergence rates
than SVRG under only the smoothness assumption (e.g  SVRG++ [2]). It turns out that the adaptive
variants of SVRG under HEB with θ < 1/2 are simpler than that under the QEB.
Deﬁnition 1 (Hölderian error bound (HEB)). Problem (1) is said to satisfy a Hölderian error bound
condition on a compact set Ω if there exist θ ∈ (0  1/2] and c > 0 such that for any x ∈ Ω

(cid:107)x − x∗(cid:107)2 ≤ c(F (x) − F∗)θ 

(3)

where x∗ denotes the closest optimal solution to x.

It is notable that the above inequality can always hold for θ = 0 on a compact set Ω. Therefore the
discussion in the paper regarding the HEB condition also applies to the case θ = 0. In addition  if
a HEB condition with θ ∈ (1/2  1] holds  we can always reduce it to the QEB condition provided
that F (x) − F∗ is bounded over Ω. However  we are not aware of any interesting examples of (1) for
such cases. We defer several examples satisfying the HEB conditions with explicit θ ∈ (0  1/2] in
machine learning to Section 5. We refer the reader to [29  28  27  14] for more examples.

2

2 Related work

The use of error bound conditions in optimization for deriving fast convergence dates back to [15  16 
17]  where the (local) error bound condition bounds the distance of a point in the local neighborhood
of the optimal solution to the optimal set by a multiple of the norm of the proximal gradient at the
point. Based on their local error bound condition  they have derived local linear convergence for
descent methods (e.g.  proximal gradient methods). Several recent works have established the same
local error bound conditions for several interesting problems in machine learning [7  32  33].
Hölderian error bound (HEB) conditions have been studied extensively in variational analysis [10]
and recently revived in optimization for developing fast convergence of optimization algorithms.
Many studies have leveraged the QEB condition in place of strong convexity assumption to develop
fast convergence (e.g.  linear convergence) of many optimization algorithms (e.g.  the gradient
method [3]  the proximal gradient method [5]  the accelerated gradient method [20]  coordinate
descent methods [30]  randomized coordinate descent methods [9  18]  subgradient methods [29  27] 
primal-dual style of methods [28]  and etc.). This work is closely related to several recent studies that
have shown that SVRG methods can also enjoy linear convergence for ﬁnite-sum (composite) smooth
optimization problems under the QEB condition [6  9  12]. However  these approach all require
knowing the growth parameter in the QEB condition  which is unknown in many practical problems.
It is worth mentioning that several recent studies have also noticed the similar issue in SVRG-type
of methods that the strong convexity constant is unknown and suggested some practical heuristics
for either stopping the inner iterations early or restarting the algorithm [2  22  19]. Nonetheless  no
theoretical convergence guarantee is provided for the suggested heuristics.
Our work is also related to studies that focus on searching for unknown strong convexity parameter
in accelerated proximal gradient (APG) methods [21  11] but with striking differences as mentioned
before. Recently  Liu & Yang [14] considered the HEB for composite smooth optimization problems
and developed an adaptive restarting accelerated gradient method without knowing the c constant in
the HEB. As we argued before  their analysis can not be trivially extended to SVRG.

3 SVRG under the HEB condition in the oracle setting

(cid:80)n

n

(cid:80)n

n

2 (cid:107)x − y(cid:107)2
(cid:80)n

n

i=1 Li. For simplicity  we can take Lf = 1

In this section  we will present SVRG methods under the HEB condition in the oracle setting
assuming that the c parameter is given. We ﬁrst give some notations. Denote by Li the smoothness
constant of fi  i.e.  for all x  y ∈ Ω fi(x) − fi(y) ≤ (cid:104)∇fi(y)  x − y(cid:105) + Li
2. It implies
that f (x) (cid:44) 1
i=1 fi(x) is also continuously differential convex function whose gradient is Lf -
Lipschitz continuous  where Lf ≤ 1
i=1 Li. In the
sequel  we let L (cid:44) maxi Li and assume that it is given or can be estimated for the problem. Denote
by Ω∗ the optimal set of the problem (1)  and by F∗ = minx∈Ω F (x). The detailed steps of SVRG
under the HEB condition are presented in Algorithm 1. The formal guarantee of SVRGHEB is given
in the following theorem.
Theorem 2. Suppose problem (1) satisﬁes the HEB condition with θ ∈ (0  1/2] and F (x0)−F∗ ≤ 0 
where x0 is an initial solution. Let η = 1/(36L)  and T1 ≥ 81Lc2 (1/0)1−2θ. Algorithm 1 ensures
(4)
In particular  by running Algorithm 1 with R = (cid:100)log2
 (cid:101)  we have E[F (¯x(R)) − F∗] ≤   and
the computational complexity for achieving an -optimal solution in expectation is O(n log(0/) +
Lc2 max{ 1
Remark: We make several remarks about the Algorithm 1 and the results in Theorem 2. First  the
constant factors in η and T1 should not be treated literally  because we have made no effort to optimize
them. Second  when θ = 1/2 (i.e  the QEB condition holds)  the Algorithm 1 reduces to the standard
SVRG method under strong convexity  and the iteration complexity becomes O((n + Lc2) log(0/)) 
which is the same as that of the standard SVRG with Lc2 mimicking the condition number of the
problem. Third  when θ = 0 (i.e.  with only the smoothness assumption)  the Algorithm 1 reduces
to SVRG++ [2] with one difference  where in SVRGHEB the initial point and the reference point
for each outer loop are the same but are different in SVRG++  and the iteration complexity of
SVRGHEB becomes O(n log(0/) + Lc2
 ) that is similar to that of SVRG++. Fourth  for intermediate

E[F (¯x(R)) − F∗] ≤ (1/2)R 0.

1−2θ   log(0/)}).

0

3

0 = ¯x(r−1)

¯gr = ∇f (¯x(r−1))  x(r)
for t = 1  2  . . .   Tr do

Algorithm 1 SVRG method under HEB (SVRGHEB(x0  T1  R  θ))
1: Input: x0 ∈ Ω  the number of inner initial iterations T1  and the number of outer loops R.
2: ¯x(0) = x0
3: for r = 1  2  . . .   R do
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
13: Output: ¯x(R)

Choose it ∈ {1  . . .   n} uniformly at random.
t = ∇fit(x(r)
g(r)
t = arg minx∈Ω(cid:104)g(r)
(cid:80)Tr
x(r)
end for
¯x(r) = 1
Tr
Tr+1 = 21−2θTr

t−1) − ∇fit(¯x(r−1)) + ¯gr.
t−1(cid:105) + 1

2η(cid:107)x − x(r)

  x − x(r)

2 + Ψ(x).

t=1 x(r)

t−1(cid:107)2

t

t

1 = 81Lc2

0 (1/0)1−2θ

Algorithm 2 SVRG method under HEB with Restarting: SVRGHEB-RS
1: Input: x(0) ∈ Ω  a small value c0 > 0  and θ ∈ (0  1/2).
2: Initialization: T (1)
3: for s = 1  2  . . .   S do
4:
5:
6: end for
θ ∈ (0  1/2) we can obtain faster convergence than SVRG++. Lastly  note that the number of
iterations for each outer loop depends on the c parameter in the HEB condition. The proof the
Theorem 2 is simply built on previous analysis of SVRG and is deferred to the supplement.

x(s)=SVRGHEB (x(s−1)  T (s)
T (s+1)
1

= 21−2θT (s)

1   R  θ)

1

4 Adaptive SVRG under the HEB condition in the dark setting

In this section  we will present adaptive variants of SVRGHEB that can be run in the dark setting  i.e 
without assuming c is known. We ﬁrst present the variant for θ < 1/2  which is simple and can help
us understand the difﬁculty for θ = 1/2.
4.1 Adaptive SVRG for θ ∈ (0  1/2)
An issue of SVRGHEB is that when c is unknown the initial number of iterations T1 in Algorithm 1
is difﬁcult to estimate . A small value of T1 may not guarantee SVRGHEB converges as Theorem 2
indicates. To address this issue  we can use the restarting trick  i.e  restarting SVRGHEB with an
increasing sequences of T1. The steps are shown in Algorithm 2. We can start with a small value of
c0  which is not necessarily larger than c. If c0 is larger than c  the ﬁrst call of SVRGHEB will yield
an -optimal solution as Theorem 2 indicates. Below  we assume that c0 ≤ c.
Theorem 3. Suppose problem (1) satisﬁes the HEB with θ ∈ (0  1/2) and F (x0) − F∗ ≤ 0  where
0 (1/0)1−2θ. Then
x0 is an initial solution. Let c0 ≤ c   ≤ 0
+ 1 calls of SVRGHEB in Algorithm 2  we ﬁnd
with at most a total number of S =
2−θ log2
a solution x(S) such that E[F (x(S)) − F∗] ≤ . The computaional complexity of SVRGHEB-RS for
obtaining such an -optimal solution is O

(cid:16) c
(cid:17)(cid:109)
2   R = (cid:100)log2
(cid:16)

 (cid:101) and T (1)

n log(0/) log(c/c0) + Lc2
1−2θ

1 = 81Lc2

(cid:108) 1

(cid:17)

c0

0

.

1

Remark: The proof is in the supplement. We can see that Algorithm 2 cannot be applied to θ = 1/2 
which gives a constant sequence of T (s)
and therefore cannot provide any convergence guarantee
for a small value of c0 < c. We have to develop a different variant for tackling θ = 1/2. A minor
point of worth mentioning is that if necessary we can stop Algorithm 2 appropriately by performing
a proximal gradient update at x(s) (whose full gradient will be computed for the next stage) and
checking if the proximal gradient’s Euclidean norm square is less than a predeﬁned level (c.f. (7)).

1

4

√

cs+1 =

2 + Ψ(x)

2 + Ψ(x)  s = 0

end if
s = s + 1

2 (cid:107)x − ˜x0(cid:107)2

2 (cid:107)x − ˜x(s+1)(cid:107)2

2 >  do
Set Rs and Ts = (cid:100)81Lc2
˜x(s+1)=SVRGHEB(¯x(s)  Ts  Rs  0.5)
¯x(s+1) = arg minx∈Ω(cid:104)∇f (˜x(s+1))  x − ˜x(s+1)(cid:105) + L
cs+1 = cs
if (cid:107)¯x(s+1) − ˜x(s+1)(cid:107)2 ≥ ϑ(cid:107)¯x(s) − ˜x(s)(cid:107)2 then
2cs  ¯x(s+1) = ¯x(s)  ˜x(s+1) = ˜x(s)

Algorithm 3 SVRG method under QEB with Restarting and Search: SVRGQEB-RS
1: Input: ˜x(0) ∈ Ω  an initial value c0 > 0   > 0  ρ = 1/ log(1/) and ϑ ∈ (0  1).
2: ¯x(0) = arg minx∈Ω(cid:104)∇f (˜x0)  x − ˜x0(cid:105) + L
3: while (cid:107)¯x(s) − ˜x(s)(cid:107)2
s(cid:101) as in Lemma 2
4:
5:
6:
7:
8:
9:
10:
11:
12: end while
13: Output: ¯x(s)
4.2 Adaptive SVRG for θ = 1/2
In light of the value of T1 in Theorem 2 for θ = 1/2  i.e.  T1 = (cid:100)81Lc2(cid:101)  one might consider to start
with a small value for c and then increase its value by a constant factor at certain points in order to
increase the value of T1. But the challenge is to decide when we should increase the value of c. If
one follows a similar procedure as in Algorithm 2  we may end up with a worse iteration complexity.
To tackle this challenge  we need to develop an appropriate machinery to check whether the value of
c is already large enough for SVRG to decrease the objective value. However  we cannot afford the
cost for computing the objective value due to large n. To this end  we develop a “certiﬁcate” that can
be easily veriﬁed and can act as signal for a sufﬁcient decrease in the objective value. The developed
certiﬁcate is motivated by a property of proximal gradient update under the QEB as shown in (5).
Lemma 1. Let ¯x = arg minx∈Ω(cid:104)∇f (˜x)  x− ˜x(cid:105)+ L
of the problem (1)  we have

2 +Ψ(x). Then under the QEB condition

2 (cid:107)x− ˜x(cid:107)2

F (¯x) − F∗ ≤ (L + Lf )2c2(cid:107)¯x − ˜x(cid:107)2
2.

(5)

The above lemma indicates that we can perform a proximal gradient update at a point ˜x and use
(cid:107)¯x − ˜x(cid:107)2 as a gauge for monitoring the decrease in the objective value. However  the proximal
gradient update is too expensive to compute due to the computation of full gradient ∇f (˜x). Luckily 
SVRG allows to compute the full gradient at a small number of reference points. We propose to
leverage these full gradients to conduct the proximal gradient updates and develop the certiﬁcate for
searching the value of c. The detailed steps of the proposed algorithm are presented in Algorithm 3
to which we refer as SVRGQEB-RS. Similar to SVRGHEB-RS  SVRGQEB-RS also calls SVRGHEB for
multiple stages. We conduct the proximal gradient update at the returned solution of each SVRGHEB 
which also serves as the initial solution and the initial reference point for the next stage of SVRGHEB
when our check in Step 7 fails. At each stage  at most Rs + 1 full gradients are computed  where
Rs is a logarithmic number as revealed later. Step 7 - Step 11 in Algorithm 3 are considered as our
search step for searching the value of c. We will show that  if cs is larger than c  the condition in Step
7 is true with small probability. This can be seen from the following lemma.
Lemma 2. Suppose problem (1) satisﬁes the QEB condition. Let G0 ⊆ G1 . . . ⊆ Gs . . . be a ﬁltration
with the sigma algebra Gs generated by all random events before line 4 of stage s of Algorithm 3.
. Then for any ϑ ∈ (0  1)  we have
Let η = 1

(cid:108)
(cid:16)(cid:107)¯x(s+1) − ˜x(s+1)(cid:107)2 ≥ ϑ(cid:107)¯x(s) − ˜x(s)(cid:107)2

(cid:17)(cid:109)
(cid:17) ≤ ρ.

36L   Ts = (cid:100)81Lc2

(cid:16) 2c2
(cid:12)(cid:12)(cid:12)Gs  cs ≥ c

s(cid:101)  Rs =

s(L+Lf )2
ϑ2ρL

log2

Pr

Proof. By Lemma 1  we have F (¯x(s)) − F∗ ≤ (L + Lf )2 c2(cid:107)¯x(s) − ˜x(s)(cid:107)2
2 for all s. Below we
consider stages such that cs ≥ c. Following Theorem 2 and the above inequality  when Ts =
(cid:100)81Lc2

s(cid:101) ≥ (cid:100)81Lc2(cid:101)  we have
E[F (˜x(s+1)) − F∗|Gs] ≤ 0.5Rs(F (¯x(s)) − F∗) ≤ 0.5Rs (L + Lf )2 c2(cid:107)¯x(s) − ˜x(s)(cid:107)2
2.

(6)
Moreover  the smoothness of f (x) and the deﬁnition of ¯x(s+1) imply (see Lemma 4 in the supplemnt).

F (˜x(s+1)) − F∗ ≥ L
2

(cid:107)¯x(s+1) − ˜x(s+1)(cid:107)2
2.

(7)

5

By combining (7) and (6) and using Markov inequality  we have

Pr

(cid:107)¯x(s+1) − ˜x(s+1)(cid:107)2

2 ≥ |Gs

≤ 0.5Rs (L + Lf )2 c2(cid:107)¯x(s) − ˜x(s)(cid:107)2

2



.

(cid:18) L

2

(cid:19)

If we choose  = ϑ2L(cid:107)¯x(s)−˜x(s)(cid:107)2
conclusion follows.
Theorem 4. Under the same conditions as in Lemma 2 with ρ = 1/ log(1/)  the expected computa-
tional complexity of SVRGQEB-RS for ﬁnding an -optimal solution is at most

in the inequality above and let Rs deﬁned as in the assumption  the

2

(cid:18) c2(L + Lf )2

(cid:18) 1

(cid:19)(cid:19)(cid:18)

(cid:18)(cid:107)¯x(0) − ˜x(0)(cid:107)2

(cid:19)

2

O

(Lc2 + n) log2

ϑ2L

log



log1/ϑ2



+ log2

c0

(cid:18) c

(cid:19)(cid:19)(cid:19)

.

(cid:18)



(cid:17)(cid:105)

(cid:17)

2

log2( c
c0

(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2

(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2

Proof. We call stage s with s = 0  1  . . . a successful stage if (cid:107)¯x(s+1)− ˜x(s+1)(cid:107)2 < ϑ(cid:107)¯x(s)− ˜x(s)(cid:107)2;
otherwise  the stage s is called an unsuccessful stage. The condition (cid:107)¯x(s) − ˜x(s)(cid:107)2
2 ≤  will hold
after S1 := log1/ϑ2
successful stages and then Algorithm 3 will stop. Let S denote
the total number of stages when the algorithm stops. Although stage s = S − 1 is the last stage 
for the convenience in the proof  we still deﬁne stage s = S as a post-termination stage where no
computation is performed.
In stage s with 0 ≤ s ≤ S − 1  the computational complexity is proportional to the number of
stochastic gradient computations (#SGC)  which is TsRs + n(Rs + 1) ≤ (Ts + 2n)Rs. If stage s
is successful  then Rs+1 = Rs and Ts+1 = Ts. If stage s is unsuccessful  then Rs+1 = Rs + 1 ≤
2Rs and Ts+1 = 2Ts so that Rs+1Ts+1 ≤ 4RsTs. In either case  Rs and Ts are non-decreasing.
Note that  after S2 := (cid:100)2 log2(c/c0)(cid:101) unsuccessful stages  we will have cs ≥ c. We will consider two
scenarios: (I) the algorithm stops with cS < c and (I) the algorithm stops with cS ≥ c.
(cid:16)(cid:104)
In the ﬁrst scenario  we have S1 successful stages and at most S2 unsuccessfully stages so that
S ≤ S1 + S2 and cS < c. The #SGC of all stages can be bounded by (S1 + S2)(TS−1 + 2n)RS−1 ≤
O
Then  we consider the second scenario. Let ˆs be the ﬁrst stage with cs ≥ c  i.e.  ˆs := min{s|cs ≥
c}. It is easy to see that cˆs <
2c and there are S2 unsuccessful and less than S1 successful
stages before stage ˆs. Since the #SGC in any stage before ˆs is bounded by (Tˆs + 2n)Rˆs ≤
  the total #SGC in stages 0  1  . . .   ˆs−1 is at most (S1 +S2)(Tˆs +
O
2n)Rˆs ≤ O
Next  we bound the total #SGC in stages ˆs  ˆs + 1  . . .   S. In the rest of the proof  we consider stage s
with ˆs ≤ s ≤ S. We deﬁne C(˜x  ¯x  i  j  s) as the expected #SGC in stages s  s+1  . . .   S  conditioning
on that the initial state of stage s are ˜x(s) = ˜x and ¯x(s) = ¯x and the numbers of successful and
unsuccessful stages before stage s are i and j  respectively. Note that s = i + j. Because stage s
depends on the historical path only through the state variables (˜x  ¯x  i  j  s)  C(˜x  ¯x  i  j  s) is well
deﬁned and (˜x  ¯x  i  j  s) transits in a Markov chain with the next state being (˜x  ¯x  i  j + 1  s + 1) if
stage s does not succeed and being (˜x+  ¯x+  i+1  j  s+1) if stage s succeeds  where ˜x+=SVRGHEB(¯x 
Ts  Rs  0.5) and ¯x+ = arg minx∈Ω(cid:104)∇f (˜x+)  x − ˜x+(cid:105) + L
In the next  we will use backward induction to derive an upper bound for C(˜x  ¯x  i  j  s) that only
depends on i and j but not on s  ˜x and ¯x. In particular  we want to show that

(cid:17)(cid:17)
(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2

(cid:16) 8c2(L+Lf )2

(cid:16) 2c2(L+Lf )2

(cid:16) 2c2(L+Lf )2

2 (cid:107)x − ˜x+(cid:107)2

(Lc2 + n) log2

(Lc2 + n)

.

(Lc2 + n)

.

2 + Ψ(x).

) + log1/ϑ2

) + log1/ϑ2

log2( c
c0

2

log2

2

log2

(cid:16)(cid:104)

(cid:17)(cid:105)



√

(cid:16)

(cid:17)

(cid:17)

(cid:17)

(cid:17)



ϑ2ρL

ϑ2ρL

ϑ2ρL

C(˜x  ¯x  i  j  s) ≤ 4j−S2 (Tˆs + 2n)Rˆs

where Ai :=(cid:80)S1−i−1

r=0

(cid:17)r

(cid:16) 1−ρ

1−4ρ

Ai  for i ≥ 0  j ≥ 0  i + j = s  s ≥ ˆs 

1 − 4ρ
if 0 ≤ i ≤ S1 − 1 and Ai := 0 if i = S1.

(8)

We start with the base case where i = S1. By deﬁnitions  the only stage with i = S1 is the post-
termination stage  namely  stage s = S. In this case  C(˜x  ¯x  i  j  s) = 0 since stage S performs no
computation. Then  (8) holds trivially with Ai = 0.

6

Suppose i < S1 and (8) holds for i + 1  i + 2  . . .   S1. We want to prove it also holds i. We deﬁne
X = X(˜x  ¯x  i  j  s) as the random variable that equals the number of unsuccessful stages from stage
s (including stage s) to the ﬁrst successful stage among stages s  s + 1  s + 2  . . .   S − 1  conditioning
on s ≥ ˆs and the state variables at the beginning of stage s are (˜x  ¯x  i  j  s). Note that X = 0 means
stage s is successful. For simplicity of notation  we use Pr(·) to represent the conditional probability
Pr(·|s ≥ ˆs  (˜x  ¯x  i  j  s)). Since cs ≥ cˆs ≥ c for s ≥ ˆs  we can show by Lemma 2 that 1
Pr(X = r|X ≥ r) 

(cid:105)
t=0 Pr(X ≥ t + 1|X ≥ t)

(cid:104)(cid:81)r−1

Pr(X ≥ r + 1|X ≥ r) = Pr(s + r fails |stages s  s + 1  . . .   s + r − 1 fail) ≤ ρ 
Pr(X = r|X ≥ r) = Pr(s + r succeeds |stages s  s + 1  . . .   s + r − 1 fail) 

When X = r  the #SGC from stage s to the end of the algorithms will be(cid:80)r

= 1 − Pr(X ≥ r + 1|X ≥ r) ≥ 1 − ρ.

t=0(Ts+t + 2n)Rs+t +
EC(˜x+  ¯x+  i + 1  j + r  s + r + 1)  where E denotes the expectation over ˜x+ and ¯x+ conditioning on
2 (cid:107)x −
(˜x  ¯x) and ˜x+=SVRGHEB(¯x  Ts+r  Rs+r  0.5) and ¯x+ = arg minx∈Ω(cid:104)∇f (˜x+)  x − ˜x+(cid:105) + L
˜x+(cid:107)2

2 + Ψ(x). Since stages s  s + 1  . . .   s + r − 1 are unsuccessful  we have

Pr(X = r) =

(9)

(Ts+t + 2n)Rs+t ≤ 4t(Ts + 2n)Rs ≤ 4j+t−S2 (Tˆs + 2n)Rˆs for t = 0  1  . . .   r − 1.

Because (8) holds for i + 1 and for any ˜x+ and ¯x+  we have

C(˜x+  ¯x+  i + 1  j + r  s + r + 1) ≤ 4j+r−S2 (Tˆs + 2n)Rˆs

(10)
Based on the above inequality and the connection between C(˜x  ¯x  i  j  s) and C(˜x+  ¯x+  i + 1  j +
(cid:33)
r  s + r + 1)  we will prove that (8) holds for i  j  s.

1 − 4ρ

Ai+1.

C(˜x  ¯x  i  j  s) =

Pr(X = r)

(Ts+t + 2n)Rs+t + EC(˜x+  ¯x+  i + 1  j + r  s + r + 1)

(cid:32) r(cid:88)

t=0

(Ts+t + 2n)Rs+t +

4j+r−S2 (Tˆs + 2n)Rˆs

1 − 4ρ

[(1 − ρ)/(1 − 4ρ)]S1−i−1 − 1

((1 − ρ)/(1 − 4ρ) − 1)

(cid:16)

(cid:33)

∞(cid:88)
∞(cid:88)

r=0

r=0

≤

≤

Pr(X = r)

Pr(X = r)

≤ 4j−S2 (Tˆs + 2n)Rˆs

Pr(X = r)

4t +

4j+t−S2 (Tˆs + 2n)Rˆs +

4j+r−S2 (Tˆs + 2n)Rˆs

Ai+1

(cid:32) r(cid:88)

t=0

1 − 4ρ

(cid:33)

Ai+1

4r

1 − 4ρ

(cid:35)

= 4j−S2 (Tˆs + 2n)Rˆs

Pr(X ≥ t + 1|X ≥ t)

Pr(X = r|X ≥ r)

(cid:18) 4r+1 − 1

3

+

4rAi+1
1 − 4ρ

t=0

r=0

∞(cid:88)
 r(cid:88)
(cid:32) r(cid:88)
∞(cid:88)
∞(cid:88)

r=0

t=0

r=0

(cid:34)r−1(cid:89)
(cid:19)

t=0

(cid:17)



(cid:19)

.

(cid:19)

Since 1 − ρ ≥ 1

3
≤ (1 − ρ)

≤

+

(cid:19)

4aAi+1
1 − 4ρ

(cid:18) 4a+1 − 1
4  for any a ≥ 0 and any b ≥ a + 1  we have
(cid:18) 4a+2 − 1
(cid:34) r−1(cid:89)
b(cid:88)

3
Pr(X ≥ t + 1|X ≥ t)

4a+1Ai+1
1 − 4ρ

b(cid:88)

Pr(X ≥ t + 1|X ≥ t)

(cid:35)

(cid:35)

Pr(X = r|X ≥ r)

+

which implies

:=

t=a+1

r=a+1

Db
a−1

(cid:34)r−1(cid:89)
(cid:18) 4a+1 − 1
1We follow the convention that(cid:81)j

= Pr(X = a|X ≥ a)

≤ (1 − ρ)

r=a

t=a

3

(cid:18) 4a+1 − 1

3

+

4aAi+1
1 − 4ρ

i = 1 if j < i.

+

(cid:19)

4aAi+1
1 − 4ρ
+ ρDb
a.

7

≤ Pr(X = a + 1|X ≥ a + 1)

(cid:18) 4a+2 − 1

3

+

4rAi+1
1 − 4ρ

(cid:18) 4r+1 − 1

3

+

(cid:19)

4a+1Ai+1
1 − 4ρ
:= Db
a 

(cid:18) 4r+1 − 1

(cid:19)

Pr(X = r|X ≥ r)

(cid:19)

4rAi+1
1 − 4ρ
+ Pr(X ≥ a + 1|X ≥ a)Db

+

3

a

Applying this inequality for a = 0  1  . . .   b − 1 and the fact Db

Db−1 ≤ (1 − ρ)

ρr

b−1(cid:88)

r=0

(cid:18) 4r+1 − 1

3

(cid:19)

+

4rAi+1
1 − 4ρ

+ ρb

Since 4ρ < 1  letting b in the inequality above increase to inﬁnity gives

= 4j−S2 (Tˆs + 2n)Rˆs

C(˜x  ¯x  i  j  s) ≤ 4j−S2 (Tˆs + 2n)Rˆs(1 − ρ)
Ai+1(1 − ρ)
(1 − 4ρ)2

(cid:18) 1
(cid:17)r ≤ (S1 + S2 − ˆs)
(cid:16) 1−ρ

1 − 4ρ

+

which is (8). Then by induction  (8) holds for any state (˜x  ¯x  i  j  s) with s ≥ ˆs. At the moment
when the algorithm enters stage ˆs  we must have j = S2 and i = ˆs − S2. By (8) and the facts that
  the expected #SGC

ˆs ≥ S2 and that Ai =(cid:80)S1−i−1

1−4ρ
from stage ˆs to the end of algorithm is

r=0

C(˜x  ¯x  ˆs − S2  S2  ˆs) ≤ (Tˆs + 2n)Rˆs

(S1 + S2 − ˆs)

1 − 4ρ

(cid:32)

≤ O

(Lc2 + n) log2

.

(cid:19)

1−4ρ gives

(cid:19)
3 + 4bAi+1

+

4bAi+1
1 − 4ρ

b−1 ≤ 4b+1−1
(cid:18) 4b+1 − 1
(cid:18) 4r+1 − 1

3

 

3

+

r=0

ρr

1−4ρ

1 − 4ρ

4rAi+1
1 − 4ρ

∞(cid:88)
(cid:19) 4j−S2 (Tˆs + 2n)RˆsAi
(cid:17)S1+S2−ˆs
(cid:16) 1−ρ
(cid:18) 1 − ρ
(cid:19)S1+S2−ˆs
(cid:19)S1(cid:33)
(cid:18) 1 − ρ
(cid:19)
(cid:18) 8c2(L + Lf )2
(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2
(cid:17) log( 1−ρ
(cid:17)S1
(cid:16) 1−ρ
(cid:17)

1 − 4ρ

1 − 4ρ

1−4ρ

ϑ2ρL

S1

=



.

2

3ρ

(1−4ρ) log 1/ϑ

Therefore  by adding the #SGC be-
fore and after the ˆs stages in the second scenario  we have the expected total #SGC is
O

(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2

(Lc2 + n)

= O

+ log

log

log

.

2



ρL

1

(cid:16)(cid:0) 1
(cid:17)(cid:17)

log(1/)  we have

(cid:1)3ρ(cid:17) ≤ O(1).
(cid:17)
(cid:16) c2(L+Lf )2



1−4ρ )

log 1/ϑ2 ≤

In light of the value of ρ  i.e.  ρ =

(cid:17)
(cid:16)(cid:107)¯x(0)−˜x(0)(cid:107)2
(cid:16) c
(cid:16)(cid:16)
(cid:17)



c0

5 Applications and Experiments

In this section  we consider some applications in machine learning and present some experimental
results. We will consider ﬁnite-sum problems in machine learning where fi(x) = (cid:96)(x(cid:62)ai  bi)
denotes a loss function on an observed training feature and label pair (ai  bi)  and Ψ(x) denotes a
regularization on the model x. Let us ﬁrst consider some examples of loss functions and regularizers
that satisfy the QEB condition. More examples can be found in [29  28  27  14].
Piecewise convex quadratic (PCQ) problems. According to the global error bound of piecewise
convex polynomials by Li [10]  PCQ problems satisfy the QEB condition. Examples of such problems
include empirical square loss  squared hinge loss or Huber loss minimization with (cid:96)1 norm  (cid:96)∞ norm
or (cid:96)1 ∞ norm regularization or constraint.
A family of structured smooth composite functions. This family include functions of the form
F (x) = h(Ax) + Ψ(x)  where Ψ(x) is a polyhedral function or an indicator function of a polyhedral
set and h(·) is a smooth and strongly convex function on any compact set. Accoding to studies
in [6  20]  the QEB holds on any compact set or the involved polyhedral set. Examples of interesting
loss functions include the aforementioned square loss and the logisitc loss as well.
For examples satisfying the HEB condition with intermediate values of θ ∈ (0  1/2)  we can
i=1(x(cid:62)ai − bi)p with
p ∈ 2N+ [23]. According to the reasoning in [14]  the HEB condition holds with θ = 1/p.
Before presenting the experimental results  we would like to remark that in many regularized machine
learning formulations  no constraint in a compact domain x ∈ Ω is included. Nevertheless  we can
explicitly add a constraint Ψ(x) ≤ B into the problem to ensure that intermediate solutions generated
by the proposed algorithms always stay in a compact set  where B can be set to a large value without
affecting the optimal solutions. The proximal mapping of Ψ(x) with such an explicit constraint can
be efﬁciently handled by combining the proximal mapping and a binary search for the Lagrangian

consider (cid:96)1 constrained (cid:96)p norm regression  where the objective f (x) = 1/n(cid:80)n

8

Figure 1: Comparison of different algorithms for solving different problems on different datasets.

multiplier. In practice  as long as B is sufﬁciently large  the constraint remains inactive and the
computational cost remains the same.
Next  we conduct some experiments to demostrate the effectiveness of the proposed algorithms
on several tasks  including (cid:96)1 regularized squared hinge loss minimization  (cid:96)1 regularized logistic
loss minimization for linear classiﬁcation problems; and (cid:96)1 constrained (cid:96)p norm regression  (cid:96)1
regularized square loss minimization and (cid:96)1 regularized Huber loss minimization for linear regression
problems. We use three datasets from libsvm website: Adult (n = 32561  d = 123)  E2006-tﬁdf
(n = 16087  d = 150360)  and YearPredictionMSD (n = 51630  d = 90). Note that we use the
testing set of YearPredictionMSD data for our experiment because some baselines need a lot of
time to converge on the large training set. We set the regularization parameter of (cid:96)1 norm and the
upper bound of (cid:96)1 constraint to be 10−4 and 100  respectively. In each plot  the difference between
objective value and optimum is presented in log scale.
Our ﬁrst experiment is to justify the proposed SVRGQEB-RS algorithm by comparing it with SVRGHEB
with different estimations of c (corresponding to the different initial values of T1). We try four
different values of T1 ∈ {1000  2000  8000  2n}. The result is plotted in the top left of Figure 1.
We can see that SVRGHEB with some underestimated values of T1 (e.g  1000  2000) converge very
slowly. However  the performance of SVRGQEB-RS is not affected too much by the initial value of T1 
which is consistent with our theory showing the log dependence on the initial value of c. Moreover 
SVRGQEB-RS with different values of T1 perform always better than their counterparts of SVRGHEB.
Then we compare SVRGQEB-RS and SVRGHEB-RS to other baselines for solving different problems
on different data sets. We choose SAGA  SVRG++ as the baselines. We also notice that a heuristic
variant of SVRG++ was suggested in [2] where epoch length is automatically determined based on the
change in the variance of gradient estimators between two consecutive epochs. However  according
to our experiments we ﬁnd that this heuristic automatic strategy cannot always terminate one epoch
because their suggested criterion cannot be met. This is also conﬁrmed by our communication
with the authors of SVRG++. To make it work  we manually add an upper bound constraint of
each epoch length equal to 2n following the suggestion in [8]. The resulting baseline is denoted
by SVRG-heuristics. For all algorithms  the step size is best tuned. The initial epoch length of
SVRG++ is set to n/4 following the suggestion in [2]  and the same initial epoch length is also
used in our algorithms. The comparison with these baselines are reported in remaining ﬁgures of
Figure 1. We can see that SVRGQEB-RS (resp. SVRGHEB-RS) always has superior performance  while
SVRG-heuristics sometimes performs well sometimes bad.

Acknowlegements

We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially
supported by National Science Foundation (IIS-1463988  IIS-1545995).

9

#grad/n0100200300400500objective-optimum-15-10-50squaredhinge+ℓ1norm AdultSVRGHEB(1000)SVRGHEB(2000)SVRGHEB(8000)SVRGHEB(2n=65122)SVRGQEB−RS(1000)SVRGQEB−RS(2000)SVRGQEB−RS(8000)SVRGQEB−RS(2n=65122)#grad/n0100200300400500objective-optimum-16-14-12-10-8-6-4-20squaredhinge+ℓ1norm AdultSAGASVRG++SVRG-heuristicsSVRGQEB−RS#grad/n0100200300400500objective-optimum-15-10-50logistic+ℓ1norm AdultSAGASVRG++SVRG-heuristicsSVRGQEB−RS#grad/n0100200300400500objective-optimum-5-4.5-4-3.5-3-2.5-2square+ℓ1norm millionsongsSAGASVRG++SVRG-heuristicsSVRGQEB−RS#grad/n0100200300400500objective-optimum-5.5-5-4.5-4-3.5-3-2.5-2-1.5-1huberloss+ℓ1norm millionsongsSAGASVRG++SVRG-heuristicsSVRGQEB−RS#grad/n0100200300400500objective-optimum-2.9-2.8-2.7-2.6-2.5-2.4-2.3-2.2-2.1-2ℓpregression(p=4) E2006SAGASVRG++SVRG-heuristicsSVRGHEB−RSReferences
[1] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods.

In
Proceedings of the 49th Annual ACM Symposium on Theory of Computing  STOC ’17  2017.

[2] Z. Allen-Zhu and Y. Yuan. Improved svrg for non-strongly-convex or sum-of-non-convex
objectives. In Proceedings of The 33rd International Conference on Machine Learning  pages
1080–1089  2016.

[3] J. Bolte  T. P. Nguyen  J. Peypouquet  and B. Suter. From error bounds to the complexity of

ﬁrst-order descent methods for convex functions. CoRR  abs/1510.08234  2015.

[4] A. Defazio  F. R. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems (NIPS)  pages 1646–1654  2014.

[5] D. Drusvyatskiy and A. S. Lewis. Error bounds  quadratic growth  and linear convergence of

proximal methods. arXiv:1602.06661  2016.

[6] P. Gong and J. Ye. Linear convergence of variance-reduced projected stochastic gradient without

strong convexity. CoRR  abs/1406.1102  2014.

[7] K. Hou  Z. Zhou  A. M. So  and Z. Luo. On the linear convergence of the proximal gradient
method for trace norm regularization. In Advances in Neural Information Processing Systems
(NIPS)  pages 710–718  2013.

[8] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS  pages 315–323  2013.

[9] H. Karimi  J. Nutini  and M. W. Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge
Discovery in Databases - European Conference (ECML-PKDD)  pages 795–811  2016.

[10] G. Li. Global error bounds for piecewise convex polynomials. Math. Program.  137(1-2):37–64 

2013.

[11] Q. Lin and L. Xiao. An adaptive accelerated proximal gradient method and its homotopy
In Proceedings of the International Conference on

continuation for sparse optimization.
Machine Learning  (ICML)  pages 73–81  2014.

[12] J. Liu and M. Takác. Projected semi-stochastic gradient descent method with mini-batch scheme

under weak strong convexity assumption. CoRR  abs/1612.05356  2016.

[13] J. Liu and S. J. Wright. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization  25(1):351–376  2015.

[14] M. Liu and T. Yang. Adaptive accelerated gradient converging methods under holderian error

bound condition. CoRR  abs/1611.07609  2017.

[15] Z.-Q. Luo and P. Tseng. On the convergence of coordinate descent method for convex differen-

tiable minization. Journal of Optimization Theory and Applications  72(1):7–35  1992.

[16] Z.-Q. Luo and P. Tseng. On the linear convergence of descent methods for convex essenially

smooth minization. SIAM Journal on Control and Optimization  30(2):408–425  1992.

[17] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a

general approach. Annals of Operations Research  46:157–178  1993.

[18] C. Ma  R. Tappenden  and M. Takác. Linear convergence of the randomized feasible descent

method under the weak strong convexity assumption. CoRR  abs/1506.02530  2015.

[19] T. Murata and T. Suzuki. Doubly accelerated stochastic variance reduced dual averaging method

for regularized empirical risk minimization. CoRR  abs/1703.00439  2017.

[20] I. Necoara  Y. Nesterov  and F. Glineur. Linear convergence of ﬁrst order methods for non-

strongly convex optimization. CoRR  abs/1504.06298  2015.

10

[21] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Program-

ming  140(1):125–161  2013.

[22] L. Nguyen  J. Liu  K. Scheinberg  and M. Takác. SARAH: A novel method for machine learning

problems using stochastic recursive gradient. CoRR  2017.

[23] H. Nyquist. The optimal lp norm estimator in linear regression models. Communications in

Statistics - Theory and Methods  12(21):2511–2524  1983.

[24] R. Rockafellar. Convex Analysis. Princeton mathematical series. Princeton University Press 

1970.

[25] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss
minimization. In Proceedings of the International Conference on Machine Learning (ICML) 
pages 567–599  2013.

[26] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[27] Y. Xu  Q. Lin  and T. Yang. Stochastic convex optimization: Faster local growth implies faster
global convergence. In Proceedings of the 34th International Conference on Machine Learning
(ICML)  pages 3821–3830  2017.

[28] Y. Xu  Y. Yan  Q. Lin  and T. Yang. Homotopy smoothing for non-smooth problems with lower
complexity than O(1/). In Advances In Neural Information Processing Systems 29 (NIPS) 
pages 1208–1216  2016.

[29] T. Yang and Q. Lin. Rsg: Beating sgd without smoothness and/or strong convexity. CoRR 

abs/1512.03107  2016.

[30] H. Zhang. New analysis of linear convergence of gradient-type methods via unifying error

bound conditions. CoRR  abs/1606.00269  2016.

[31] H. Zhang and W. Yin. Gradient methods for convex minimization: better rates under weaker

conditions. arXiv preprint arXiv:1303.4645  2013.

[32] Z. Zhou and A. M.-C. So. A uniﬁed approach to error bounds for structured convex optimization

problems. arXiv:1512.03518  2015.

[33] Z. Zhou  Q. Zhang  and A. M. So. L1p-norm regularization: Error bounds and convergence
rate analysis of ﬁrst-order methods. In Proceedings of the 32nd International Conference on
Machine Learning  (ICML)  pages 1501–1510  2015.

11

,Judy Hoffman
Sergio Guadarrama
Eric Tzeng
Ronghang Hu
Jeff Donahue
Ross Girshick
Trevor Darrell
Kate Saenko
Yi Xu
Qihang Lin
Tianbao Yang
Vaishak Belle
Brendan Juba