2019,Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently,Experimental data has revealed that in addition to feedforward connections  there exist abundant feedback connections in a neural pathway. Although the importance of feedback in neural information processing has been widely recognized in the field  the detailed mechanism of how it works remains largely unknown. Here  we investigate the role of feedback in hierarchical information retrieval. Specifically  we consider a hierarchical network storing the hierarchical categorical information of objects  and information retrieval goes from rough to fine  aided by dynamical push-pull feedback from higher to lower layers. We elucidate that the push (positive) and pull (negative) feedbacks suppress the interferences due to neural correlations between different and the same categories  respectively  and their joint effect improves retrieval performance significantly. Our model agrees with the push-pull phenomenon observed in neural data and sheds light on our understanding of the role of feedback in neural information processing.,Push-pull Feedback Implements Hierarchical

Information Retrieval Efﬁciently

Xiao Liu1

Xiaolong Zou1

Zilong Ji2

Gengshuo Tian3

Yuanyuan Mi4

Tiejun Huang1

K. Y. Michael Wong5

1School of Electronics Engineering & Computer Science  IDG/McGovern Institute for Brain Research 

Peking-Tsinghua Center for Life Sciences  Academy for Advanced Interdisciplinary Studies 

Peking University  Beijing  China.

Si Wu1

2State Key Laboratory of Cognitive Neuroscience and Learning  Beijing Normal University  China.

3Department of Mathematics  Beijing Normal University  China

4Center for Neurointelligence  Chongqing University  China

5Department of Physics  Hong Kong University of Science and Technology  China.

{xiaoliu23 xiaolz tjhuang siwu}@pku.edu.cn 

jizilong@mail.bnu.edu.cn  gengshuo_tian@163.com 

miyuanyuan0102@cqu.edu.cn  phkywong@ust.hk

Abstract

Experimental data has revealed that in addition to feedforward connections  there
exist abundant feedback connections in a neural pathway. Although the impor-
tance of feedback in neural information processing has been widely recognized
in the ﬁeld  the detailed mechanism of how it works remains largely unknown.
Here  we investigate the role of feedback in hierarchical information retrieval.
Speciﬁcally  we consider a hierarchical network storing the hierarchical categorical
information of objects  and information retrieval goes from rough to ﬁne  aided
by dynamical push-pull feedback from higher to lower layers. We elucidate that
the push (positive) and pull (negative) feedbacks suppress the interferences due to
neural correlations between different and the same categories  respectively  and
their joint effect improves retrieval performance signiﬁcantly. Our model agrees
with the push-pull phenomenon observed in neural data and sheds light on our
understanding of the role of feedback in neural information processing.

1

Introduction

Deep neural networks (DNNs)  which mimic hierarchical information processing in the ventral visual
pathway  have achieved great success in object recognition [15]. The structure of DNNs mainly
contains feedforward connections from lower to higher layers. The experimental data  however  has
revealed that there also exist abundant feedback connections from higher to lower layers  whose
number is even larger than that of feedforward ones [23]. It has been widely suggested that these
feedback connections play an important role in visual information processing. For instance  the
theory of analysis-by-synthesis proposes that the feedback connections  in coordination with the
feedforward ones  enable the neural system to recognize an object in an interactive manner [16]  that
is  the feedforward pathway extracts the object information from external inputs  while the feedback
pathway generates hypotheses about the object; and the interaction between the two pathways

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

accomplishes the recognition task. Based on a similar idea  the theory of predictive coding proposes
that the feedback from the higher cortex predicts the output of the lower cortex [22]. Although the
importance of feedback has been widely recognized in the ﬁeld  computational models elucidating
how it works exactly remain poorly developed. Interestingly  the experiment data has unveiled a
salient characteristic of feedback in the visual system [8  6]. Fig.1A displays the neural population
activities in V1 when a monkey was performing a contour integration task [8]. In response to the visual
stimulus  the neural activity in V1 increased at the early phase  displaying the push characteristic;
and decreased at the late phase  displaying the pull characteristic. Multi-unit recording revealed that
in the pull phase  there was strong negative feedback from the higher cortex (V4) [6]; while in the
push phase  although the contributions of the feedforward input and feedback were mixed  causality
analysis conﬁrmed that there indeed existed a feedback component [7].

Figure 1: A. The push-pull phenomenon of neural population activity. Data was recorded at V1 of an
awake monkey. The visual stimulus was a contour embedded in noises  which was onset at t = 0.
The blue curve shows the change of neural response over time  which increased at the early phase
and decreased at the late phase  when the monkey recognized the contour. The red curve represents
the condition when the monkey did not recognize the contour  and the green curve the condition of
no visual stimulus. Adopted from [8]. B. A three-layer network storing hierarchical categories of
objects  denoted  from bottom to top  as child  parent  and grandparent patterns  respectively. Between
layers  neurons are connected by both feedforward and feedback connections. C. The branching tree
displaying the categorical relationships between hierarchical memory patterns.

The categorization of objects is based on their similarity/dissimilarity either at the image level or in
the semantic sense  and it is organized hierarchically  in the sense that objects belonging to the same
category are more similar than those belonging to different ones. The experimental data has revealed
that the brain encodes these similarities using overlapping neural representations  with the greater the
similarity  the larger the correlation between neural responses [26]. However  it is well known that
neural networks have difﬁculties of storing and retrieving correlated memory patterns; a small amount
of correlation in the Hopﬁeld model will deteriorate memory retrieval dramatically [9]. Notably 
this instability is intrinsic to a neural network  as it utilizes synapses to carry out both information
encoding and retrieving: the synaptic strengths are affected by the correlations of stored patterns 
which in turn interfere with the retrieval of a stored pattern. Thus  a dilemma is raised to neural
coding: on one hand  to encode the categorical relationships between objects  a neural system needs
to exploit correlated neural representations; on the other hand  to retrieve information reliably  these
patterns correlations are harmful. How to achieve reliable hierarchical information retrieval in a
neural network remains unresolved in the ﬁeld [2  13].
In the present study  motivated by the push-pull phenomenon in neural data  we investigate the role
of feedback in hierarchical information retrieval. Speciﬁcally  we consider that a neural system
employs a rough-to-ﬁne retrieval procedure  in which higher categorical representations of objects
are retrieved ﬁrst  since they are less correlated than lower categorical ones and hence have better
retrieval accuracy; subsequently  through feedback  the retrieved higher categorical information
facilitates the retrieval of lower categorical representations. We elucidate that the optimal feedback
should be dynamical  varying from positive (push) to negative (pull) over time  and they suppress
the interferences due to pattern correlations from different and the same categories  respectively.
Using synthetic and real data  we demonstrate that the push-pull feedback implements hierarchical
information retrieval efﬁciently.

2

ABCxl

i(t + 1) = sign(cid:2)hl
i(t)(cid:3)  
(cid:88)
(cid:88)

W 1

ijx1

j (t) +

h1
i (t) =

W 1 2

ij x2

j (t).

(1)

(2)

2 A model for Hierarchical Information Representation

To elucidate the role of feedback clearly  we consider a simple model for hierarchical information
representation. The model is kept simple to illustrate insights derived from the role played by different
sources of interferences during different stages of dynamical retrieval. Speciﬁcally  the model consists
of three layers which store three-level hierarchical memory patterns. For convenience  we call the
three layers  from bottom to top  child  parent  and grandparent layers  respectively  to reﬂect their
ascending category relationships (Fig.1B). Neurons in the same layer are connected recurrently
with each other to function as an associative memory. Between layers  neurons communicate via
feedforward and feedback connections. Denote the state of neuron i in layer l at time t as xl
i(t) for
i = 1  . . .   N  which takes value of ±1  the symmetric recurrent connections from neuron j to i in
ij  the feedforward connections from neuron j of layer l to neuron i in layer l + 1 as
layer l as W l
W l+1 l
and the feedback connections from neuron j of layer l + 1 to neuron i of layer l as W l l+1
.
The neuronal dynamics follows the Hopﬁeld model [10]  which is written as

ij

ij

where sign(h) = 1 for h > 0 and sign(h) = −1 otherwise. hl
i(t) is the total input received by the
neuron  which is given by (only the result for layer 1 is shown  and the results for other layers are
similar) 

j

j

We generate synthetic patterns to study information retrieval  which are denoted as: {ξα} for
α = 1  . . .   Pα represents the grandparent patterns  {ξα β} for β = 1  . . .   Pβ the parent patterns of
grandparent α  and {ξα β γ} for γ = 1  . . .   Pγ the child patterns of parent (α  β)  where Pα  Pβ 
and Pγ are denoted as the number of grandparent patterns  the number of parent patterns belonging
to the same grandparent  and the number of child patterns belonging to the same parent  respectively.
These hierarchical memory patterns are constructed as follows [1] (Fig.1C).
First  grandparent patterns are statistically independent of each other. The value of each element in a
grandparent pattern is drawn from the distribution

P (ξα

i ) =

1
2

δ(ξα

i + 1) +

δ(ξα

i − 1) 

1
2

(3)

where δ(x) = 1 for x = 0 and δ(x) = 0 otherwise. Each element of a grandparent has equal
probabilities of taking a value of 1 or −1.
Secondly  for each grandparent pattern  its descending parent patterns are drawn from the distribution

P (ξα β

i

) = (

1 + b2

2

)δ(ξα β

i − ξα

i ) + (

1 − b2

2

)δ(ξα β

i + ξα

i ) 

(4)

where 0 < b2 < 1 implies that each element of a parent pattern has a probability of (1 + b2)/2 > 0.5
to have the same value as the corresponding element of the grandparent. This establishes the
relationship between a grandparent and the parent patterns.
Thirdly  for each parent pattern  its descending child patterns are drawn from the distribution

P (ξα β γ

i

) = (

1 + b1

2

)δ(ξα β γ

i

− ξα β

i

) + (

1 − b1

2

)δ(ξα β γ

i

+ ξα β

i

) 

(5)

where 0 < b1 < 1  which speciﬁes the relationship between a parent and its child patterns.
The above stochastic pattern generation process speciﬁes the categorical relationships among memory
patterns  in the sense that patterns in the same group have stronger correlation than those belong-
ing to different groups. For example  the correlation between two child patterns belonging to the
1  referred to as the intra-class corre-
lation; the correlation between two child patterns belonging to different parents but sharing the
2  referred to as the inter-class
correlation; and the correlation between two child patterns belonging to different grandparents is
/N = 0. These correlation values satisfy the hierarchical relationship  i.e. 

same parent (siblings) is given by(cid:80)
same grandparent (cousins) is given by(cid:80)
given by(cid:80)

i ξα β γξα β(cid:48) γ(cid:48)

i ξα β γξα β γ(cid:48)

/N = b2

/N = b2

i ξα β γξα(cid:48) β(cid:48) γ(cid:48)

1b2

3

1b2

2 > 0. Other correlation relationships can be obtained similarly (see Sec.1 in Supplementary

b2
1 > b2
Information (SI)).
Each layer of the network behaves as an associative memory. Using the standard Hebbian learning
rule  the recurrent connections between neurons in the same layer are constructed to be W 1
ij =
j /N. The feedforward
ξα β γ
ij =
j
/N. It is easy to understand the effect of feedforward connections. For example  if layer
1 is at the state of the memory pattern ξα0 β0 γ0  then the feedforward input to layer 2 is given by
  which contributes to improving the retrieval of the parent pattern ξα0 β0 at

connections from lower to higher layers are set to be W 2 1

ij = (cid:80)
ij =(cid:80)

ij = (cid:80)

(cid:80)
(cid:80)
(cid:80)

α ξα

i ξα
α β γ ξα β

/N  and W 3 2

α β γ ξα β γ

/N  and W 3

≈ ξα0 β0

α β ξα

i ξα β

α β ξα β

/N  W 2

ξα β γ
j

ξα β
j

j

i

i

j W 2 1

ij ξα β γ

j

i

layer 2. The form of feedback connections is the focus of this study and will be introduced later.
To quantify the retrieval performance  we deﬁne a macroscopic variable m(t)  measuring the overlap
between the neural state x(t) and a memory pattern  which is calculated to be [9] (again  for simplicity 
only the result for layer 1 is shown) 

i

mα β γ(t) =

1
N

N(cid:88)

i=1

ξα β γ
i

x1
i (t).

(6)

where −1 < mα β γ(t) < 1 represents the retrieval accuracy of the memory pattern ξα β γ  and the
larger the value of m  the higher the retrieval accuracy.

3

Information retrieval without feedback

To elucidate the role of feedback  it is valuable to ﬁrst check information retrieval without feedback 
and without loss of generality  we focus on layer 1. Following the standard stability analysis [9] 
we consider that the initial state of layer 1 is a memory pattern  x1(0) = ξα0 β0 γ0  and investigate
what are the key factors determining the retrieval performance. After one step of iteration  we get the
retrieval accuracy 

mα0 β0 γ0 (1) =

=

1
N

1
N

N(cid:88)
N(cid:88)

i=1

i=1

(cid:104)

N(cid:88)
(cid:105)

i=1

.

ξα0 β0 γ0
i

x1
i (1) =

1
N

ξα0 β0 γ0
i

sign

ξα0 β0 γ0
i

h1
i (0)

sign(cid:2)h1

i (0)(cid:3)  

We see that the retrieval of a memory pattern is determined by the alignment between the neural input
and the memory pattern  which is further written as (see Sec.2 in SI) 

ξα0 β0 γ0 h1

i (0) = ξα0 β0 γ0

1
N

(cid:88)

j

j (0) = 1 + Ci +(cid:102)Ci.

W 1

ijx1

Here  the input received by the neuron is decomposed into the signal and noise parts  and the latter

is further divided into two components  Ci and (cid:101)Ci  which represent  respectively  the interferences

to memory retrieval due to: 1) the correlation of the pattern to be retrieved with siblings from the
same parent  called the intra-class interference; 2) the correlation of the pattern to be retrieved
with cousins from the same grandparent but different parents  called the inter-class interference.
It can be checked that in the limits of large N  Pγ and Pβ  the intra- and inter- class interferences 

Ci and (cid:101)Ci  satisfy the distributions  P (Ci) = N (EC  VC)(1 + b1)/2 + N (−EC  VC)(1 − b1)/2 
P ((cid:101)Ci) = N (E(cid:101)C  V(cid:101)C)(1 + b1b2)/2 + N (−E(cid:101)C  V(cid:101)C)(1 − b1b2)/2  where N (E  V ) represents a
1(Pγ − 1)  E(cid:101)C = b3
2Pγ(Pβ − 1) 
1b3
2) (see Sec.2 in SI).

normal distribution with mean E and variance V   and EC = b3
1)(1 − b2
VC = b4
The breadth of the above noise distributions  as a consequence of pattern correlations  implies that
even starting from a noiseless state  the network dynamics still incur retrieving instability [9]  and the

1)  V(cid:101)C = b4
2Pγ(Pβ − 1)(1 − b2
1b4
error occurs when noises are large (i.e.  Ci + (cid:101)Ci < −1).

1(Pγ − 1)(1 − b2

(7)

(8)

4

4 Hierarchical Information Retrieval with the push-pull feedback

According to the above theoretical analysis  to improve memory retrieval  the key is to suppress
the inter- and intra- class noises due to pattern correlations. Note that  in practice  the correlations
between higher categorical patterns tend to be smaller than that between lower categorical patterns.
For example  the similarity between cats and dogs is usually smaller than that between two sub-types
of cats. In our model  this corresponds to the condition of b1 > b2. For an associative memory  this
implies that given the same amount of input information (e.g.  an ambiguous image of a Siamese cat) 
the parent pattern (e.g.  a cat) can be better retrieved than the child pattern (e.g.  a Siamese cat). Thus 
we consider a rough-to-ﬁne retrieval procedure  in which the parent pattern in layer 2 is ﬁrst retrieved 
whose result is subsequently fed back to layer 1 to improve the retrieval of the child pattern.
Below  for the convenience of analysis  we assume that the parent pattern is ﬁrst perfectly retrieved
(m = 1) and explore the appropriate form of feedback which can efﬁciently utilize the parent
information to enhance the retrieval of the child pattern. Later  we carry out simulations demonstrating
that the model works in general cases when the parent pattern is not perfectly retrieved.

4.1 The form and the role of push feedback

We ﬁrst show that a push (positive) feedback of a proper form can suppress the inter-class interference
in memory retrieval effectively. Without loss of generality  we consider that for a given input  the
corresponding child pattern to be retrieved in layer 1 is ξα0 β0 γ0 and that the corresponding parent
pattern in layer 2 is ξα0 β0. Consider the push feedback of the below form 

(cid:88)
feedback to neuron i in layer 1 is calculated to be(cid:80)

W 1 2
ij

N Pγ

=

1

α β γ

which follows the standard Hebb rule between the parent and their child patterns  and its contribution
is intuitively understandable. Given that the parent pattern ξα0 β0 in layer 2 is retrieved  its push
/Pγ. Obviously 
this positive current increases the activities of all child patterns belonging to the parent  i.e.  those
ξα0 β0 γ for any γ  and it has little inﬂuence on other child patterns from different parents  i.e.  those
ξα0 β γ for β (cid:54)= β0. Due to the competition between memory patterns in the network dynamics  this
effectively suppresses the inter-class interference in memory retrieval (Fig.2A ).

γ ξα0 β0 γ

ij ξα0 β0

j W 1 2

j

i

≈(cid:80)

ξα β γ
i

ξα β
j

 

(9)

contribution to sibling patterns is measured by(cid:80)
by(cid:80)

Figure 2: A. Illustrating the effect of push-feedback. The parent pattern is ξ1 1  whose feedback
γ m1 1 γ/Pγ  and to cousin patterns is measured
γ m1 2 γ/Pγ. The results averaged over 100 trials are shown. B. Illustrating the effect of pull-
feedback. The distributions of the intra-class noise Ci without feedback and the noise C∗
i with the
pull-feedback are presented (see Eqs.(11 14) in SI). Retrieval errors occur when Ci < −1 or C∗
i < −1
(indicated by the yellow line). The parameters are N = 2000  Pα = 2  Pβ = 10  Pγ = 70  b1 = 0.2 
and b2 = 0.15.

4.2 The form and the role of pull feedback

We further show that a pull (negative) feedback of an appropriate form can suppress the intra-class
interference in memory retrieval. Consider the pull feedback of the below form 

(10)

ij = −b1δij.
W 1 2

5

-0.100.10.20.30.4without feedbackwith push-feedbackm1 1 /PMm1 2 /PM0.40.30.20.10-0.1-2-1012Ci Ci*00.20.40.6P(Ci) P(Ci*)ABcici*0.81 is calculated to be(cid:80)

Given the parent pattern ξα0 β0 in layer 2 is retrieved  its negative feedback to neuron i in layer
. In the large Pγ limit  the parent pattern is
approximated to be the mean of its child patterns (Sec.1 in SI)  thus  in effect the pull feedback is to
subtract a portion of the mean value from sibling patterns. The retrieval accuracy of the target child
pattern after applying the pull feedback is calculated to be (Sec.3 in SI) 

= −b1ξα0 β0

ij ξα0 β0

j W 1 2

j

i

N(cid:88)

i=1

(cid:104)

(cid:105)

i + (cid:101)Ci

mα0 β0 γ0 =

1
N

sign

1 + C∗

 

(11)

(cid:88)

(cid:88)

j

k
i )  m  n = 1  2 

i ≡ Ci − b1ξα0 β0 γ0
where C∗
is the new noise term after applying the pull-feedback. As shown
in Fig.2B  with the pull feedback  the negative tail of the noise distribution (where retrieval errors
occur) is considerably reduced.

ξα0 β0
i

i

4.3 The joint effect of the push-pull feedback

Summarizing the above results  we come to the conclusion that to achieve good information retrieval 
the neural feedback needs to be dynamical  exerting the push and pull components at different stages 
so that they can suppress the inter- and intra- class interferences  respectively.
To better demonstrate the joint effect of the push-pull feedback  we consider a continuous version of
the Hopﬁeld model  so that the network state changes smoothly and the joint effects of push- and
pull- feedbacks are integrated over time (the discrete Hopﬁeld model still works  but the overall effect
is less signiﬁcant). The network dynamics are given by [11]

τ

dhn
i
dt

= −hn

i +

W n

ijxn

j +

W n m

ik

(t)xm

k + I ext n

i

 

(12)

i

i and xn

ik = a+Pγ

α β γ(ξα β γ

xn
i = f (hn

− (cid:104)ξ(cid:105))(ξα β γ

i < 1.To match the strength of ﬁring rate xn

(13)
where hn
i denote the synaptic input and the ﬁring rate of neuron i in layer n  respectively 
and their relationship is given by a sigmoid function  f (x) = arctan(8πx)/π + 1/2  therefore
i   we also align all the hierarchical patterns ξi
0 < xn
into 0  1. The parameter τ is the time constant. The recurrent and feedforward connections follow
(cid:80)
the standard Hebb rule as described above. The feedback connections are slightly modiﬁed from
Eqs.(9-10) to accommodate positive values of neural activities in the continuous model. They are
− (cid:104)ξ(cid:105))/N  with a+ being
given by: the push-feedback W 1 2
ik = −a−b1δik  with a− being a positive number. I ext
a positive number  and the pull-feedback W 1 2
is the external input conveying the object information. The push and pull feedbacks are applied
sequentially  with each of them lasting in the time order of τ (τ ∼ 10 − 20ms)  as suggested by the
data [6]. For details of the model  see Sec.4.1 in SI.
Fig.3 displays a typical example of the memory retrieval process in the network  demonstrating that:
1) the neural population activity at layer 1 exhibits the push-pull phenomenon  agreeing qualitatively
with the experimental data (Fig.3A compared to Fig.1A); 2) the retrieval accuracy of layer 1 with
the push-pull feedback is improved signiﬁcantly compared to the case of no feedback (Fig.3B).
Interestingly  we note that when the push feedback is applied  the retrieval accuracy of the target
child pattern is decreased a little bit. This is due to that the push feedback only aims at reducing the
inter-class interference without differentiating sibling patterns.
We evaluate the performances of the model by varying the amplitude of pattern correlations and
conﬁrm that the push-pull feedback always improves the network performance statistically (Sec.4.2
in SI).

k

i

5 Applying to Real Images

We test our model in the processing of real images. As shown in the top of Fig.4A  the dataset we use
consists of Pβ = 2 types of animals  cats and dogs  corresponding to parents in our model. For each
type of animal  it is further divided into Pγ = 9 sub-types  corresponding to children. A total of 1800
images  with K = 100 for each sub-type of animals  are chosen from ImageNet. It has been shown
that the neural representations generated by a DNN (after being trained with ImageNet) capture the

6

Figure 3: A. The neural population activity (cid:104)x(cid:105) in layer 1 as a function of time in a typical trial (blue
curve)  which exhibits the push-pull phenomenon as observed in the experiment [8]. The red curve is
the case without feedback. (cid:104)x(cid:105) is obtained by averaging over all neurons in layer 1. B. The retrieval
accuracies of the child (red curve) and the parent patterns (yellow curve) as functions of time in the
same trial as in A. The blue curve is the case without feedback. The lower panel in both A and B
displays the time course of applying an external input (t ∈ (0  4τ ))  the push-feedback (t ∈ (τ  2τ )) 
and the pull-feedback (t ∈ (2τ  3τ )). The child pattern conveyed by the external input is ξ1 1 1  and
the corresponding parent pattern is ξ1 1. The parameters used are: N = 2000  Pγ = 25  Pβ = 4 
Pα = 2  b1 = 0.2  b2 = 0.1  τ = 5 aext = 1 a1
ext = 0.1  a+ = 1  a− = 10  λ1=0.1 
λ2=0.1.

r = 1  a2

r = 2 a2

categorical relationships between objects  in the sense that the overlap between neural representations
reﬂect the closeness of objects in category  rather than their similarity in pixels [26]. This indicates
that the memory patterns are hierarchically organized. We therefore pre-process images by ﬁltering
them through VGG  a type of DNN [24]  and use the neural representations generated by VGG
(i.e.  the neural activities before the read-out layer) to construct the memory patterns. The details
of pre-processing are described in Sec.5 in SI. The lower panel of Fig.4A shows the correlations
between the memory patterns generated by VGG  which exhibits a hierarchical structure  i.e.  siblings
from the same parent have stronger correlations than cousins from different parents  similar to the
correlation structure in our model.

Figure 4: The model performances with real images. A. The dataset. Top panel: example images 
one for each sub-type of cat or dog. Lower panel: the correlations between child patterns after
pre-processing by VGG. Cat: 1 − 9; Dog: 10 − 18. B. Retrieval accuracies of the child (cat A  blue
curve) and parent (cat  yellow curve) patterns as functions of time in a typical trial. The red curve is
the case without feedback. C. Different effects of the push and pull feedbacks in the example trial
as in B. The blue  purple  and green curves represent  respectively  the retrieval accuracies of the
target child (cat A  a Siamese cat)  the siblings (other sub-types of cats)  and other child patterns
(all sub-types of dogs) in layer 1. In B-C  the image presented to the network is cat Siamese  and
the lower panel displays the time course of applying the external input (0  4τ )  the push feedback
(τ  2.4τ )  and the pull feedback (2.4τ  3.8τ ). The parameters: N = 4096  a1
ext = 6 
ext = 1  a+ = 2  a− = 1.5  a21 = 1. Other parameters are the same as in Fig.3.
a2

r = 1  a2

r = 2  a1

We present each image to the network and measure its retrieval accuracy by calculating mβ γ  i.e.  the
overlap between the network response and the memory pattern corresponding to the image. Fig.4B
shows a typical example of the retrieval process. We see that the retrieval accuracy of layer 1 keeps

7

Time ( )0 1 23 4 5InputPushPullTime ( )PushRetrieval AccuracyuPll00.20.40.60.81012345without fbwith fbm1 1 11 1 1mm1 1Population Activity < >00.20.40.60.81<x>without fbwith fb<x>0 1 2 3 4 50 1 2 3 4 5InputABACBPullInputRetrieval AccuracyPush-0.200.20.40.6PullInputPushRetrieval Accuracy00.20.40.60.813691215183 6 9 12 15 18catdogcatdogmcat Awithout fbmcat Awith fbmcat01234Time(τ)00.20.40.60.81012340123401234Time(τ)mcat A<m>cat<m>dog 0.8 0.6 0.4 0.2 0-0.1increasing when the push and pull feedbacks are applied sequentially  and the result is signiﬁcantly
improved compared to the case without feedback. Over 1800 images  the averaged improvement is
71.04% (measured at the moment when the pull feedback stops).
To illustrate the individual effects of the push and pull feedbacks  we also calculate the retrieval
accuracies of sibling and cousin patterns. As shown in Fig.4C  we see that: 1) at the early phase
of push feedback  both the retrieval accuracies of the target child pattern and its siblings increase 
whereas the retrieval accuracy of cousins drops  indicating that the push feedback has the effect of
suppressing the inter-class interference; 2) at the later phase of pull feedback  the retrieval accuracy of
the target child pattern experiences another signiﬁcant increase much larger than that for other child
patterns  indicating that the pull feedback has the effect of suppressing the intra-class interference.

6 Conclusion and Discussion

The present study investigates the role of feedback in hierarchical information retrieval. Hierarchical
associative memory models have been studied previously [21  1  25  20]  but these works considered
only a single layer network without feedback. To our knowledge  our paper is the ﬁrst one studying the
contributions of feedback. In machine learning  there were studies which utilize the semantics-based
higher category knowledge of objects as side information to enhance image recognition [14  19  3] 
but they are very different from our network model in the use of dynamical feedback between layers
to enhance information retrieval.
Feedback connections have been widely observed in neural signalling pathways  but their exact
computational functions remain largely unclear. Here  in the task of information retrieval  our
study reals that the neural feedback  which varies from positive (push) to negative (pull) over time 
contributes to the suppression of the inter- and intra- class noises in information retrieval. This
push-pull characteristic agrees with the push-pull phenomenon of neural activities observed in the
experiments [8  6]. Notably  the neural systems have resources to realize such a dynamical feedback 
and they are likely implemented via different signal pathways. For instance  the push feedback may
be realized via direct excitatory synapses from higher to lower layers  and the stopping of push
feedback can be controlled by short-term synaptic depression; on the other hand  the pull feedback
may go through a separate path mediated by inhibitory interneurons  which is naturally delayed
compared to the direct excitatory path [12].
Through studying feedback  the present study also addresses a dilemma in neural coding  which
concerns the conﬂicting roles of neural correlation: on one hand  pattern correlations are essential
to encode the categorical relationships between objects; on the other hand  they inevitably incur
interference to memory retrieval. To diminish the correlation interference  we propose that neural
systems employ a rough-to-ﬁne information retrieval procedure. Upon receiving the external informa-
tion  the higher categorical pattern is ﬁrst retrieved  whose result is subsequently utilized to enhance
the retrieval of the lower categorical pattern via dynamical push-pull feedback. In such a way  the
highly correlated neural representations for objects are reliably retrieved. The idea of rough-to-ﬁne
information retrieval is in agreement with the concept of “global ﬁrst" in cognitive science  which
states that the brain extracts ﬁrst the global (e.g.  topological)  rather than the local (e.g.  Euclidean) 
geometrical features of objects [4]. This phenomenon has been conﬁrmed by a large volume of
psychophysical experiments [5]. Here  our study unveils a computational advantage of “global ﬁrst"
not realized previously  that is  extracting global features ﬁrst  aided by the push-pull feedback  serves
as an efﬁcient strategy to overcome the interference due to neural correlations. It has been suggested
by experimental ﬁndings that the dorsal pathway [17]  and/or the subcortical pathway from retina
to superior colliculus [18]  carry out the rapid computation of extracting global features of objects;
while  along the ventral pathway  the push-pull feedback assists the feedforward input to extract the
ﬁne structures of objects in a relatively slow manner. In our future work  we will extend the present
study to explore the role of feedback in biologically more detailed models.

8

Acknowledgments

This work was supported by BMSTC (Beijing municipal science and technology commission) under
grant No: Z171100000117007 (D.H. Wang & Y.Y. Mi)  the National Natural Science Foundation
of China (N0: 31771146  11734004  Y.Y. Mi) the National Natural Science Foundation of China
(N0: 61425025  T.J. Huang) Beijing Nova Program (N0: Z181100006218118  Y. Y. Mi)  Guangdong
Province with Grant (No. 2018B03033800  Si Wu & Y.Y. Mi) and grants from the Research Grants
Council of Hong Kong (grant numbers 16322616  16306817 and 16302419  K. Y. Michael Wong).
This work received support from Huawei Technology Co.  Ltd..

References
[1] S. Amari  Statistical neurodynamics of associative memory. Neural Networks  1  63-73 (1988).

[2] B. Blumenfeld  S. Preminger  D. Sagi  M. Tsodyks  Dynamics of memory representations in

networks with novelty-facilitated synaptic plasticity. Neuron  52  383-394 (2006).

[3] S. Chandar  S. Ahn  H. Larochelle  P. Vincent  G. Tesauro  Y. Bengio. Hierarchical memory

networks. arXiv preprint arXiv:1605.07427(2016).

[4] L. Chen. Topological structure in visual perception. Science  218:699-700 (1982).

[5] L. Chen. The topological approach to perceptual organization. Visual Cognition  12  553-637

(2015).

[6] M. Chen  Y. Yan  X. Gong  C. Gilbert  H. Liang  W. Li  Incremental integration of global contours

through interplay between visual cortical areas. Neuron  82  682-694 (2014).

[7] R. Chen  F. Wang  H. Liang  W. Li. Synergistic processing of visual contours across cortical

layers in V1 and V2. Neuron  96(6)  1388-1402(2017).

[8] A. Gilad  E. Meirovithz  H. Slovin  Population responses to contour integration: early encoding

of discrete elements and late perceptual grouping. Neuron  78  389-402 (2013).

[9] J. Hertz  A. S. Krogh  R. G. Palmer  Introduction to the theory of Neural Computation  Addison-

Wesley (1991).

[10] J. J. Hopﬁeld  Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the National Academy of Sciences of the USA  79  2554 -2558 (1982).

[11] J. J. Hopﬁeld  Neurons with graded response have collective computational properties like those
of two-sate neurons. Proceedings of the National Academy of Sciences of the USA  81  3088-3092
(1984).

[12] E. R. Kandel  J. H. Schwartz  T. M. Jessell. Principles of Neural Science  4th ed. McGraw-Hill 

New York (2000).

[13] E. Kropff  A. Treves  Uninformative memories will prevail: the storage of correlated representa-

tions and its consequences. HFSP journal  1  249-262 (2007).

[14] S. Kumar  M. Hebert. A hierarchical ﬁeld framework for uniﬁed context-based classiﬁcation.

Computer Vision  2005. ICCV 2005. Tenth IEEE International Conference on. IEEE  2(2005).

[15] Y. LeCun  Y. Bengio  G. Hinton  Deep learning. Nature  521  436-444 (2015).

[16] T. Lee  D. Mumford  Hierarchical Bayesian inference in the visual cortex. Journal of the Optical

Society of America A 20  1434-1448 (2003).

[17] L. Liu and F. Wang and K. Zhou et al. Perceptual integration rapidly activates dorsal visual

pathway to guide local processing in early visual areas. Plos Biology  15  e2003646 (2017).

[18] J. Mcfadyen and M. Mermillod and J B Mattingley et al. A Rapid Subcortical Amygdala Route

for Faces Irrespective of Spatial Frequency and Emotion. J. Neurosci. 37:3864-3874 (2017).

9

[19] F. Morin  Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. Aistats  5

246-252(2005).

[20] M. Okada  Turorial series on brain-inspired computing  part 3: brain science  information

science and associative memory model. New Generation Computing  24  185-201 (2006).

[21] N. Parga  M. Virasoro  The ultrametric organization of memories in a neural network. Journal

de Physique  47  1857-1864 (1986).

[22] R. Rao  D. Ballard  Predictive coding in the visual cortex: A functional interpretation of some

extra-classical receptive-ﬁeld effects. Nature Neuroscience  2  79-87 (1999).

[23] A. Sillito  J. Cudeiro  H. Jones  Always returning: Feedback and sensory processing in visual

cortex and thalamus. Trends in neurosciences  29  307-316 (2006).

[24] K. Simonyan  A. Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint  arXiv:1409.1556 (2014).

[25] N. Sourlas  Multilayer neural networks for hierarchical patterns. Europhysics Letters  7  749-753

(1988).

[26] D. L. Yamins  H. Hong  C. F. Cadieu  E. A. Solomon  D. Seibert  J. J. DiCarlo. Performance-
optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of
the National Academy of Sciences  111(23)  8619-8624(2014).

10

,Xiao Liu
Xiaolong Zou
Zilong Ji
Gengshuo Tian
Yuanyuan Mi
Tiejun Huang
K. Y. Michael Wong
Si Wu