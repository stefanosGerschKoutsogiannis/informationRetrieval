2018,Plug-in Estimation in High-Dimensional Linear Inverse Problems: A Rigorous Analysis,Estimating a vector $\mathbf{x}$ from noisy linear measurements $\mathbf{Ax+w}$ often requires use of prior knowledge or structural constraints
on $\mathbf{x}$ for accurate reconstruction. Several recent works have considered combining linear least-squares estimation with a generic or plug-in ``denoiser" function that can be designed in a modular manner based on the prior knowledge about $\mathbf{x}$. While these methods have shown excellent performance  it has been difficult to obtain rigorous performance guarantees. This work considers plug-in denoising combined with the recently-developed Vector Approximate Message Passing (VAMP) algorithm  which is itself derived via Expectation Propagation techniques. It shown that the mean squared error of this ``plug-in"  VAMP can be exactly predicted for a large class of high-dimensional random $\Abf$ and denoisers. The method is illustrated in image reconstruction and parametric bilinear estimation.,Plug-in Estimation in High-Dimensional Linear

Inverse Problems: A Rigorous Analysis

Alyson K. Fletcher

Dept. Statistics
UC Los Angeles

Parthe Pandit

Dept. ECE

UC Los Angeles

Sundeep Rangan

Dept. ECE

NYU

akfletcher@ucla.edu

parthepandit@ucla.edu

srangan@nyu.edu

Subrata Sarkar

Dept. ECE

The Ohio State Univ.
sarkar.51@osu.edu

Philip Schniter

Dept. ECE

The Ohio State Univ.

schniter.1@osu.edu

Abstract

Estimating a vector x from noisy linear measurements Ax + w often requires
use of prior knowledge or structural constraints on x for accurate reconstruction.
Several recent works have considered combining linear least-squares estimation
with a generic or “plug-in” denoiser function that can be designed in a modu-
lar manner based on the prior knowledge about x. While these methods have
shown excellent performance  it has been difﬁcult to obtain rigorous performance
guarantees. This work considers plug-in denoising combined with the recently-
developed Vector Approximate Message Passing (VAMP) algorithm  which is
itself derived via Expectation Propagation techniques.
It shown that the mean
squared error of this “plug-and-play" VAMP can be exactly predicted for high-
dimensional right-rotationally invariant random A and Lipschitz denoisers. The
method is demonstrated on applications in image recovery and parametric bilinear
estimation.

1

Introduction

The estimation of an unknown vector x0 ∈ RN from noisy linear measurements y of the form

y = Ax0 + w ∈ RM  

where A ∈ RM×N is a known transform and w is disturbance  arises in a wide-range of learning
and inverse problems. In many high-dimensional situations  such as when the measurements are
fewer than the unknown parameters (i.e.  M ≪ N )  it is essential to incorporate known structure on
x0 in the estimation process. A fundamental challenge is how to perform structured estimation of
x0 while maintaining computational efﬁciency and a tractable analysis.

Approximate message passing (AMP)  originally proposed in [1]  refers to a powerful class of algo-
rithms that can be applied to reconstruction of x0 from (1) that can easily incorporate a wide class
of statistical priors. In this work  we restrict our attention to w ∼ N (0  γ−1
w I)  noting that AMP
was extended to non-Gaussian measurements in [2  3  4]. AMP is computationally efﬁcient  in that
it generates a sequence of estimates {bxk}∞k=0 by iterating the steps

bxk = g(rk  γk)
vk = y − Abxk + N
rk+1 = bxk + ATvk 

M h∇g(rk−1  γk−1)ivk−1

γk+1 = M/kvkk2 

(1)

(2a)

(2b)

(2c)

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

N PN

n=1

∂rn

∂gn(r γ)

−1 = 0  and assuming A is scaled so that kAk2

initialized with r0 = ATy  γ0 = M/kyk2  v
F ≈ N .
In (2)  g : RN × R → RN is an estimation function chosen based on prior knowledge about x0  and
h∇g(r  γ)i := 1
denotes the divergence of g(r  γ). For example  if x0 is known to
be sparse  then it is common to choose g(·) to be the componentwise soft-thresholding function  in
which case AMP iteratively solves the LASSO [5] problem.
Importantly  for large  i.i.d.  sub-Gaussian random matrices A and Lipschitz denoisers g(·)  the
performance of AMP can be exactly predicted by a scalar state evolution (SE)  which also provides
testable conditions for optimality [6  7  8]. The initial work [6  7] focused on the case where g(·) is
a separable function with identical components (i.e.  [g(r  γ)]n = g(rn  γ) ∀n)  while the later work
[8] allowed non-separable g(·). Interestingly  these SE analyses establish the fact that

rk = x0 + N (0  I/γk) 

(3)
leading to the important interpretation that g(·) acts as a denoiser. This interpretation provides
guidance on how to choose g(·). For example  if x is i.i.d. with a known prior  then (3) suggests
to choose a separable g(·) composed of minimum mean-squared error (MMSE) scalar denoisers
g(rn  γ) = E(xn|rn = xn + N (0  1/γ)). In this case  [6  7] established that  whenever the SE has
a unique ﬁxed point  the estimates bxk generated by AMP converge to the Bayes optimal estimate of
x0 from y. As another example  if x is a natural image  for which an analytical prior is lacking  then
(3) suggests to choose g(·) as a sophisticated image-denoising algorithm like BM3D [9] or DnCNN
[10]  as proposed in [11]. Many other examples of structured estimators g(·) can be considered; we
refer the reader to [8] and Section 5. Prior to [8]  AMP SE results were established for special cases
of g(·) in [12  13]. Plug-in denoisers have been combined in related algorithms [14  15  16].
An important limitation of AMP’s SE is that it holds only for large  i.i.d.  sub-Gaussian A. AMP
itself often fails to converge with small deviations from i.i.d. sub-Gaussian A  such as when A is
mildly ill-conditioned or non-zero-mean [4  17  18]. Recently  a robust alternative to AMP called
vector AMP (VAMP) was proposed and analyzed in [19]  based closely on expectation propagation
[20]—see also [21  22  23]. There it was established that  if A is a large right-rotationally invariant
random matrix and g(·) is a separable Lipschitz denoiser  then VAMP’s performance can be exactly
predicted by a scalar SE  which also provides testable conditions for optimality. Importantly  VAMP
applies to arbitrarily conditioned matrices A  which is a signiﬁcant beneﬁt over AMP  since it is
known that ill-conditioning is one of AMP’s main failure mechanisms [4  17  18].

Unfortunately  the SE analyses of VAMP in [24] and its extension in [25] are limited to separable
denoisers. This limitation prevents a full understanding of VAMP’s behavior when used with non-
separable denoisers  such as state-of-the-art image-denoising methods as recently suggested in [26].
The main contribution of this work is to show that the SE analysis of VAMP can be extended to
a large class of non-separable denoisers that are Lipschitz continuous and satisfy a certain conver-
gence property. The conditions are similar to those used in the analysis of AMP with non-separable
denoisers in [8]. We show that there are several interesting non-separable denoisers that satisfy these
conditions  including group-structured and convolutional neural network based denoisers.

An extended version with all proofs and other details are provided in [27].

2 Review of Vector AMP

The steps of VAMP algorithm of [19] are shown in Algorithm 1. Each iteration has two parts: A
denoiser step and a Linear MMSE (LMMSE) step. These are characterized by estimation functions

g1(·) and g2(·) producing estimates bx1k and bx2k. The estimation functions take inputs r1k and r2k

that we call partial estimates. The LMMSE estimation function is given by 

g2(r2k  γ2k) := (cid:0)γwATA + γ2kI(cid:1)−1 (cid:0)γwATy + γ2kr2k(cid:1)  

(4)

where γw > 0 is a parameter representing an estimate of the precision (inverse variance) of the noise

w in (1). The estimate bx2k is thus an MMSE estimator  treating the x as having a Gaussian prior
with mean given by the partial estimate r2k. The estimation function g1(·) is called the denoiser and
can be designed identically to the denoiser g(·) in the AMP iterations (2). In particular  the denoiser
is used to incorporate the structural or prior information on x. As in AMP  in lines 5 and 11  h∇gii
denotes the normalized divergence.

2

// Denoising

Algorithm 1 Vector AMP (LMMSE form)
Require: LMMSE estimator g2(·  γ2k) from (4)  denoiser g1(·  γ1k)  and number of iterations Kit.
1: Select initial r10 and γ10 ≥ 0.
2: for k = 0  1  . . .   Kit do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end for

bx2k = g2(r2k  γ2k)
α2k = h∇g2(r2k  γ2k)i
η2k = γ2k/α2k  γ1 k+1 = η2k − γ2k
r1 k+1 = (η2kbx2k − γ2kr2k)/γ1 k+1

bx1k = g1(r1k  γ1k)
α1k = h∇g1(r1k  γ1k)i
η1k = γ1k/α1k  γ2k = η1k − γ1k
r2k = (η1kbx1k − γ1kr1k)/γ2k

// LMMSE estimation

15: Return bx1Kit .

The main result of [24] is that  under suitable conditions  VAMP admits a state evolution (SE) anal-

ysis that precisely describes the mean squared error (MSE) of the estimates bx1k and bx2k in a certain

large system limit (LSL). Importantly  VAMP’s SE analysis applies to arbitrary right rotationally
invariant A. This class is considerably larger than the set of sub-Gaussian i.i.d. matrices for which
AMP applies. However  the SE analysis in [24] is restricted separable Lipschitz denoisers that can
be described as follows: Let g1n(r1  γ1) be the n-th component of the output of g1(r1  γ1). Then  it
is assumed that 

bx1n = g1n(r1  γ1) = φ(r1n  γ1) 

(5)
for some function scalar-output function φ(·) that does not depend on the component index n. Thus 
the estimator is separable in the sense that the n-th component of the estimate  bx1n depends only on
the n-th component of the input r1n as well as the precision level γ1. In addition  it is assumed that
φ(r1  γ1) satisﬁes a certain Lipschitz condition. The separability assumption precludes the analysis
of more general denoisers mentioned in the Introduction.

3 Extending the Analysis to Non-Separable Denoisers

The main contribution of the paper is to extend the state evolution analysis of VAMP to a class
of denoisers that we call uniformly Lipschitz and convergent under Gaussian noise. This class
is signiﬁcantly larger than separable Lipschitz denoisers used in [24]. To state these conditions
precisely  consider a sequence of estimation problems  indexed by a vector dimension N . For each
N   suppose there is some “true" vector u = u(N ) ∈ RN that we wish to estimate from noisy
measurements of the form  r = u + z  where z ∈ RN is Gaussian noise. Let bu = g(r  γ) be some
estimator  parameterized by γ.
Deﬁnition 1. The sequence of estimators g(·) are said to be uniformly Lipschitz continuous if there
exists constants A  B and C > 0  such that

kg(r2  γ2) − g(r1  γ1)k ≤ (A + B|γ2 − γ1|)kr2 − r1k + C√N|γ2 − γ1| 

for any r1  r2  γ1  γ2 and N .
Deﬁnition 2. The sequence of random vectors u and estimators g(·) are said to be
convergent under Gaussian noise if the following condition holds: Let z1  z2 ∈ RN be two se-
quences where (z1n  z2n) are i.i.d. with (z1n  z2n) = N (0  S) for some positive deﬁnite covariance
S ∈ R2×2. Then  all the following limits exist almost surely:
lim
N→∞

g(u + z1  γ1)Tg(u + z2  γ2) 

g(u + z1  γ1)Tu 

1
N

(7a)

(6)

1
N
1
N

lim
N→∞
lim
N→∞
N→∞h∇g(u + z1  γ1)i =
lim

lim
N→∞

uTz1 

1
N kuk2
1

N S12

3

g(u + z1  γ1)Tz2 

(7b)

(7c)

for all γ1  γ2 and covariance matrices S. Moreover  the values of the limits are continuous in S  γ1
and γ2.

With these deﬁnitions  we make the following key assumption on the denoiser.

Assumption 1. For each N   suppose that we have a “true" random vector x0 ∈ RN and a denoiser
g1(r1  γ1) acting on signals r1 ∈ RN . Following Deﬁnition 1  we assume the sequence of denoiser
functions indexed by N   is uniformly Lipschitz continuous. In addition  the sequence of true vectors
x0 and denoiser functions are convergent under Gaussian noise following Deﬁnition 2.

The ﬁrst part of Assumption 1 is relatively standard: Lipschitz and uniform Lipschitz continuity
of the denoiser is assumed several AMP-type analyses including [6  28  24] What is new is the
assumption in Deﬁnition 2. This assumption relates to the behavior of the denoiser g1(r1  γ1) in the
case when the input is of the form  r1 = x0 + z. That is  the input is the true signal with a Gaussian
noise perturbation. In this setting  we will be requiring that certain correlations converge. Before
continuing our analysis  we brieﬂy show that separable denoisers as well as several interesting non-
separable denoisers satisfy these conditions.

Separable Denoisers. We ﬁrst show that the class of denoisers satisfying Assumption 1 includes
the separable Lipschitz denoisers studied in most AMP analyses such as [6]. Speciﬁcally  suppose

that the true vector x0 has i.i.d. components with bounded second moments and the denoiser g1(·)

is separable in that it is of the form (5). Under a certain uniform Lipschitz condition  it is shown in
the extended version of this paper [27] that the denoiser satisﬁes Assumption 1.

Group-Based Denoisers. As a ﬁrst non-separable example  let us suppose that the vector x0 can
be represented as an L × K matrix. Let x0
ℓ ∈ RK denote the ℓ-th row and assume that the rows are
i.i.d. Each row can represent a group. Suppose that the denoiser g1(·) is groupwise separable. That
is  if we denote by g1ℓ(r  ℓ) the ℓ-th row of the output of the denoiser  we assume that

g1ℓ(r  γ) = φ(rℓ  γ) ∈ RK  

(8)

for a vector-valued function φ(·) that is the same for all rows. Thus  the ℓ-th row output gℓ(·) de-
pends only on the ℓ-th row input. Such groupwise denoisers have been used in AMP and EP-type
methods for group LASSO and other structured estimation problems [29  30  31]. Now  consider the
limit where the group size K is ﬁxed  and the number of groups L → ∞. Then  under suitable Lips-
chitz continuity conditions  the extended version of this paper [27] shows that groupwise separable
denoiser also satisﬁes Assumption 1.

Convolutional Denoisers. As another non-separable denoiser  suppose that  for each N   x0 is an
N sample segment of a stationary  ergodic process with bounded second moments. Suppose that the
denoiser is given by a linear convolution 

g1(r1) := TN (h ∗ r1) 

(9)

where h is a ﬁnite length ﬁlter and TN (·) truncates the signal to its ﬁrst N samples. For simplicity 
we assume there is no dependence on γ1. Convolutional denoising arises in many standard linear es-
timation operations on wide sense stationary processes such as Weiner ﬁltering and smoothing [32].
If we assume that h remains constant and N → ∞  the extended version of this paper [27] shows
that the sequence of random vectors x0 and convolutional denoisers g1(·) satisﬁes Assumption 1.

Convolutional Neural Networks.
In recent years  there has been considerable interest in using
trained deep convolutional neural networks for image denoising [33  34]. As a simple model for
such a denoiser  suppose that the denoiser is a composition of maps 

g1(r1) = (FL ◦ FL−1 ◦ ··· ◦ F1)(r1) 

(10)

where Fℓ(·) is a sequence of layer maps where each layer is either a multi-channel convolutional op-
erator or Lipschitz separable activation function  such as sigmoid or ReLU. Under mild assumptions
on the maps  it is shown in the extended version of this paper [27] that the estimator sequence g1(·)
can also satisfy Assumption 1.

4

Singular-Value Thresholding (SVT) Denoiser. Consider the estimation of a low-rank matrix X0
from linear measurements y = A(X0)  where A is some linear operator [35]. Writing the SVD of
R as R = Pi σiuivT

i   the SVT denoiser is deﬁned as

g1(R  γ) := X

i

(σi − γ)+uivT
i  

(11)

where (x)+ := max{0  x}. In the extended version of this paper [27]  we show that g1(·) satisﬁes
Assumption 1.

4 Large System Limit Analysis

4.1 System Model

Our main theoretical contribution is to show that the SE analysis of VAMP in [19] can be extended to
the non-separable case. We consider a sequence of problems indexed by the vector dimension N . For
each N   we assume that there is a “true" random vector x0 ∈ RN observed through measurements
y ∈ RM of the form in (1) where w ∼ N (0  γ−1
w0 I). We use γw0 to denote the “true" noise precision
to distinguish this from the postulated precision  γw  used in the LMMSE estimator (4). Without
loss of generality (see below)  we assume that M = N . We assume that A has an SVD 

A = USVT  S = diag(s) 

s = (s1  . . .   sN ) 

(12)

where U and V are orthogonal and S is non-negative and diagonal. The matrix U is arbitrary  s is an
i.i.d. random vector with components si ∈ [0  smax] almost surely. Importantly  we assume that V
is Haar distributed  meaning that it is uniform on the N × N orthogonal matrices. This implies that
d= AV0 for any orthogonal matrix V0. We also
A is right rotationally invariant meaning that A
assume that w  x0  s and V are all independent. As in [19]  we can handle the case of rectangular
V by zero padding s.

These assumptions are similar to those in [19]. The key new assumption is Assumption 1. Given
such a denoiser and postulated variance γw  we run the VAMP algorithm  Algorithm 1. We assume
that the initial condition is given by 

for some initial error variance τ10. In addition  we assume

r = x0 + N (0  τ10I) 

γ10 = γ10 

lim
N→∞

(13)

(14)

almost surely for some γ10 ≥ 0.
Analogous to [24]  we deﬁne two key functions: error functions and sensitivity functions. The error
functions characterize the MSEs of the denoiser and LMMSE estimator under AWGN measure-
ments. For the denoiser g1(·  γ1)  we deﬁne the error function as
1
N kg1(x0 + z  γ1) − x0k2 

z ∼ N (0  τ1I) 

(15)

E1(γ1  τ1) := lim
N→∞
and  for the LMMSE estimator  as

E2(γ2  τ2) := lim
N→∞

1
N

Ekg2(r2  γ2) − x0k2 

r2 = x0 + N (0  τ2I)  y = Ax0 + N (0  γ−1

w0 I).

(16)

The limit (15) exists almost surely due to the assumption of g1(·) being convergent under Gaussian
noise. Although E2(γ2  τ2) implicitly depends on the precisions γw0 and γw  we omit this depen-
dence to simplify the notation. We also deﬁne the sensitivity functions as

Ai(γi  τi) := lim

N→∞h∇gi(x0 + zi  γi)i 

zi ∼ N (0  τiI).

(17)

5

4.2 State Evolution of VAMP

We now show that the VAMP algorithm with a non-separable denoiser follows the identical state
evolution equations as the separable case given in [19]. Deﬁne the error vectors 

pk := r1k − x0  qk := VT(r2k − x0).

(18)
Thus  pk represents the error between the partial estimate r1k and the true vector x0. The error
vector qk represents the transformed error r2k − x0. The SE analysis will show that these errors
estimate errors (18) and estimate errors  bxi − x0. These variances are computed recursively through

are asymptotically Gaussian. In addition  the analysis will exactly predict the variance on the partial

what we will call the state evolution equations:

η1k =

γ1k
α1k

 

α1k = A1(γ1k  τ1k) 
τ2k =

1

(1 − α1k)2 (cid:2)E1(γ1k  τ1k) − α2

γ2k = η1k − γ1k
1kτ1k(cid:3)  

η2k =

γ2k
α2k

 

α2k = A2(γ2k  τ2k) 
τ1 k+1 =

1

(1 − α2k)2 (cid:2)E2(γ2k  τ2k) − α2

γ1 k+1 = η2k − γ2k
2kτ2k(cid:3)  

(19a)

(19b)

(19c)

(19d)

which are initialized with k = 0  τ10 in (13) and γ10 deﬁned from the limit (14). The SE equations in
(19) are identical to those in [19] with the new error and sensitivity functions for the non-separable
denoisers. We can now state our main result  which is proven in the extended version of this paper
[27].

Theorem 1. Under the above assumptions and deﬁnitions  assume that the sequence of true random
vectors x0 and denoisers g1(r1  γ1) satisfy Assumption 1. Assume additionally that  for all iterations
k  the solution α1k from the SE equations (19) satisﬁes α1k ∈ (0  1) and γik > 0. Then 
(a) For any k  the error vectors on the partial estimates  pk and qk in (18) can be written as 

pk = epk + O( 1

√N

)  qk = eqk + O( 1

√N

) 

(20)

where  epk and eqk ∈ RN are each i.i.d. Gaussian random vectors with zero mean and per

component variance τ1k and τ2k  respectively.

(b) For any ﬁxed iteration k ≥ 0  and i = 1  2  we have  almost surely

1

lim
N→∞

N kbxi − x0k2 =

1
ηik

 

lim
N→∞

(αik  ηik  γik) = (αik  ηik  γik).

(21)

In (20)  we have used the notation  that when u eu ∈ RN are sequences of random vectors  u =
eu + O( 1
N ku − euk2 = 0 almost surely. Part (a) of Theorem 1 thus shows

that the error vectors pk and qk in (18) are approximately i.i.d. Gaussian. The result is a natural
extension to the main result on separable denoisers in [19]. Moreover  the variance on the variance

) means limN→∞

√N

1

on the errors  along with the mean squared error (MSE) of the estimates bxik can be exactly predicted

by the same SE equations as the separable case. The result thus provides an asymptotically exact
analysis of VAMP extended to non-separable denoisers.

5 Numerical Experiments

5.1 Compressive Image Recovery

We ﬁrst consider the problem of compressive image recovery  where the goal is to recover an image

x0 ∈ RN from measurements y ∈ RM of the form (1) with M ≪ N . This problem arises in many
imaging applications  such as magnetic resonance imaging  radar imaging  computed tomography 
etc.  although the details of A and x0 change in each case.

One of the most popular approaches to image recovery is to exploit sparsity in the wavelet transform
coefﬁcients c := Ψx0  where Ψ is a suitable orthonormal wavelet transform. Rewriting (1) as

6

45

40

35

R
N
S
P

30

25

20

15

0.1

0

10

)
c
e
s
(
e
m

10

i
t

n
u
r

-1

-2

10

DnCNN-VAMP

DnCNN-AMP

LASSO-VAMP

LASSO-AMP

DnCNN-VAMP

DnCNN-AMP

LASSO-VAMP

LASSO-AMP

0.2

0.3

0.4

0.5

0.1

0.2

0.3

0.4

0.5

sampling ratio M/N

sampling ratio M/N

hﬁll

35

30

25

20

R
N
S
P

15

10

5

0

0

10

0

10

)
c
e
s
(
e
m

10

i
t

n
u
r

-1

DnCNN-VAMP

DnCNN-AMP

LASSO-VAMP

LASSO-AMP

DnCNN-VAMP

DnCNN-AMP

LASSO-VAMP

LASSO-AMP

2

10

cond(A)

4

10

-2

10

0

10

2

10

4

10

cond(A)

(a) Average PSNR and runtime with vs. M/N with
well-conditioned A and no noise after 12 iterations.

(b) Average PSNR and runtime versus cond(A) at
M/N = 0.2 and no noise after 10 iterations.

Figure 1: Compressive image recovery: PSNR and runtime vs. rate M/N and cond(A)

y = AΨc + w  the idea is to ﬁrst estimate c from y (e.g.  using LASSO) and then form the image

estimate via bx = ΨTbc. Although many algorithms exist to solve the LASSO problem  the AMP

algorithms are among the fastest (see  e.g.  [36  Fig.1]). As an alternative to the sparsity-based
approach  it was recently suggested in [11] to recover x0 directly using AMP (2) by choosing the
estimation function g as a sophisticated image-denoising algorithm like BM3D [9] or DnCNN [10].
Figure 1a compares the LASSO- and DnCNN-based versions of AMP and VAMP for 128×128 im-
age recovery under well-conditioned A and no noise. Here  A = JPHD  where D is a diagonal
matrix with random ±1 entries  H is a discrete Hadamard transform (DHT)  P is a random permu-
tation matrix  and J contains the ﬁrst M rows of IN . The results average over the well-known lena 
barbara  boat  house  and peppers images using 10 random draws of A for each. The ﬁgure shows
that AMP and VAMP have very similar runtimes and PSNRs when A is well-conditioned  and that
the DnCNN approach is about 10 dB more accurate  but 10× as slow  as the LASSO approach. Fig-
ure 2 shows the state-evolution prediction of VAMP’s PSNR on the barbara image at M/N = 0.5 
averaged over 50 draws of A. The state-evolution accurately predicts the PSNR of VAMP.

To test the robustness to the condition number of A  we repeated the experiment from Fig. 1a
using A = JDiag(s)PHD  where Diag(s) is a diagonal matrix of singular values. The singular
values were geometrically spaced  i.e.  sm/sm−1 = ρ ∀m  with ρ chosen to achieve a desired
cond(A) := s1/sM . The sampling rate was ﬁxed at M/N = 0.2  and the measurements were
noiseless  as before. The results  shown in Fig. 1b  show that AMP diverged when cond(A) ≥ 10 
while VAMP exhibited only a mild PSNR degradation due to ill-conditioned A. The original images
and example image recoveries are included in the extended version of this paper.

5.2 Bilinear Estimation via Lifting

We now use the structured linear estimation model (1) to tackle problems in bilinear estimation
through a technique known as “lifting” [37  38  39  40]. In doing so  we are motivated by applications
like blind deconvolution [41]  self-calibration [39]  compressed sensing (CS) with matrix uncertainty
[42]  and joint channel-symbol estimation [43]. All cases yield measurements y of the form

y = (cid:0)PL

l=1 blΦl(cid:1)c + w ∈ RM  

(22)

l=1 are known  w ∼ N (0  I/γw)  and the objective is to recover both b := [b1  . . .   bL]T
where {Φl}L
and c ∈ RP . This bilinear problem can be “lifted” into a linear problem of the form (1) by setting

A = [Φ1 Φ2

··· ΦL] ∈ RM×LP and x = vec(cbT) ∈ RLP  

(23)

where vec(X) vectorizes X by concatenating its columns. When b and c are i.i.d. with known priors 
the MMSE denoiser g(r  γ) = E(x|r = x + N (0  I/γ)) can be implemented near-optimally by the
rank-one AMP algorithm from [44] (see also [45  46  47])  with divergence estimated as in [11].

We ﬁrst consider CS with matrix uncertainty [42]  where b1 is known. For these experiments  we
generated the unknown {bl}L
l=2 as i.i.d. N (0  1) and the unknown c ∈ RP as K-sparse with N (0  1)
nonzero entries. Fig. 2 shows that the MSE on x of lifted VAMP is very close to its SE prediction
when K = 12. We then compared lifted VAMP to PBiGAMP from [48]  which applies AMP
directly to the (non-lifted) bilinear problem  and to WSS-TLS from [42]  which uses non-convex
optimization. We also compared to MMSE estimation of b under oracle knowledge of c  and MMSE

7

45

40

B
d
n

35

i

30

25

R
N
S
P

image recovery

CS with matrix uncertainty

-10

-15

-20

VAMP

SE

B
d
n

i

E
S
M
N

-25

-30

-35

-40

-45

20

15

0

VAMP

SE

5

10

15

iteration

-50

-55

-60

0

10

i

L
n
o
s
n
e
m
d

i

e
c
a
p
s
b
u
s

9 

8 

7 

6 

5 

4 

3 

2 

1 

Lifted VAMP

1

0.8

0.6

0.4

0.2

0

10

i

L
n
o
s
n
e
m
d

i

e
c
a
p
s
b
u
s

9 

8 

7 

6 

5 

4 

3 

2 

1 

SparseLift

1

0.8

0.6

0.4

0.2

0

5

10

15

iteration

5 

10

15

20

25

30

35

40

5 

10

15

20

25

30

35

40

sparsity K

sparsity K

Figure 2: SE prediction & VAMP for image re-
covery and CS with matrix uncertainty

Figure 3: Self-calibration: Success rate vs. spar-
sity K and subspace dimension L

B
d
n

i

)
b
(
E
S
M
N

-15

-20

-25

-30

-35

-40

-45

-50

-55

P-BiG-AMP

VAMP-Lift

WSS-TLS

oracle

B
d
n

i

)
c
(
E
S
M
N

-15

-20

-25

-30

-35

-40

-45

-50

-55

-60

P-BiG-AMP

VAMP-Lift

WSS-TLS

oracle

B
d
n

i

)
b
(
E
S
M
N

10

0

-10

-20

-30

-40

-50

-60

B
d
n

i

)
c
(
E
S
M
N

10

0

-10

-20

-30

-40

-50

-60

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

0

10

1

10

2

10

0

10

1

10

2

10

sampling ratio M/P

sampling ratio M/P

cond(A)

cond(A)

(a) NMSE vs. M/P with i.i.d. N (0  1) A.

(b) NMSE vs. cond(A) at M/P = 0.6.

Figure 4: Compressive sensing with matrix uncertainty

estimation of c under oracle knowledge of support(c) and b. For b1 = √20  L = 11  P = 256 
K = 10  i.i.d. N (0  1) matrix A  and SNR = 40 dB  Fig. 4a shows the normalized MSE on b (i.e. 
NMSE(b) := Ekbb − b0k2/Ekb0k2) and c versus sampling ratio M/P . This ﬁgure demonstrates
that lifted VAMP and PBiGAMP perform close to the oracles and much better than WSS-TLS.

Although lifted VAMP performs similarly to PBiGAMP in Fig. 4a  its advantage over PBiGAMP
becomes apparent with non-i.i.d. A. For illustration  we repeated the previous experiment  but with
A constructed using the SVD A = UDiag(s)VT with Haar distributed U and V and geometrically
spaced s. Also  to make the problem more difﬁcult  we set b1 = 1. Figure 4b shows the normalized
MSE on b and c versus cond(A) at M/P = 0.6. There it can be seen that lifted VAMP is much
more robust than PBiGAMP to the conditioning of A.

We next consider the self-calibration problem [39]  where the measurements take the form

y = Diag(Hb)Ψc + w ∈ RM .

(24)

Here the matrices H ∈ RM×L and Ψ ∈ RM×P are known and the objective is to recover the un-
known vectors b and c. Physically  the vector Hb represents unknown calibration gains that lie in
a known subspace  speciﬁed by H. Note that (24) is an instance of (22) with Φl = Diag(hl)Ψ 
where hl denotes the lth column of H. Different from “CS with matrix uncertainty ” all ele-
ments in b are now unknown  and so WSS-TLS [42] cannot be applied.
Instead  we compare
lifted VAMP to the SparseLift approach from [39]  which is based on convex relaxation and has

provable guarantees. For our experiment  we generated Ψ and b ∈ RL as i.i.d. N (0  1); c as
K-sparse with N (0  1) nonzero entries; H as randomly chosen columns of a Hadamard matrix;
and w = 0. Figure 3 plots the success rate versus L and K  where “success” is deﬁned as
EkbcbbT − c0(b0)Tk2
F < −60 dB. The ﬁgure shows that  relative to SparseLift  lifted

VAMP gives successful recoveries for a wider range of L and K.

F /Ekc0(b0)Tk2

6 Conclusions

We have extended the analysis of the method in [24] to a class of non-separable denoisers. The
method provides a computational efﬁcient method for reconstruction where structural information
and constraints on the unknown vector can be incorporated in a modular manner. Importantly  the
method admits a rigorous analysis that can provide precise predictions on the performance in high-
dimensional random settings.

8

Acknowledgments

A. K. Fletcher and P. Pandit were supported in part by the National Science Foundation under Grants
1738285 and 1738286 and the Ofﬁce of Naval Research under Grant N00014-15-1-2677. S. Rangan
was supported in part by the National Science Foundation under Grants 1116589  1302336  and
1547332  and the industrial afﬁliates of NYU WIRELESS. The work of P. Schniter was supported
in part by the National Science Foundation under Grant CCF-1527162.

References

[1] D. L. Donoho  A. Maleki  and A. Montanari  “Message-passing algorithms for compressed sensing ” Proc.

Nat. Acad. Sci.  vol. 106  no. 45  pp. 18 914–18 919  Nov. 2009.

[2] S. Rangan  “Generalized approximate message passing for estimation with random linear mixing ” in

Proc. IEEE ISIT  2011  pp. 2174–2178.

[3] S. Rangan  P. Schniter  E. Riegler  A. Fletcher  and V. Cevher  “Fixed points of generalized approximate

message passing with arbitrary matrices ” in Proc. IEEE ISIT  Jul. 2013  pp. 664–668.

[4] S. Rangan  P. Schniter  and A. K. Fletcher  “On the convergence of approximate message passing with

arbitrary matrices ” in Proc. IEEE ISIT  Jul. 2014  pp. 236–240.

[5] R. Tibshirani  “Regression shrinkage and selection via the lasso ” J. Royal Stat. Soc.  Ser. B  vol. 58  no. 1 

pp. 267–288  1996.

[6] M. Bayati and A. Montanari  “The dynamics of message passing on dense graphs  with applications to

compressed sensing ” IEEE Trans. Inform. Theory  vol. 57  no. 2  pp. 764–785  Feb. 2011.

[7] A. Javanmard and A. Montanari  “State evolution for general approximate message passing algorithms 

with applications to spatial coupling ” Information and Inference  vol. 2  no. 2  pp. 115–144  2013.

[8] R. Berthier  A. Montanari  and P.-M. Nguyen  “State evolution for approximate message passing with

non-separable functions ” arXiv preprint arXiv:1708.03950  2017.

[9] K. Dabov  A. Foi  V. Katkovnik  and K. Egiazarian  “Image denoising by sparse 3-D transform-domain

collaborative ﬁltering ” IEEE Trans. Image Process.  vol. 16  no. 8  pp. 2080–2095  2007.

[10] K. Zhang  W. Zuo  Y. Chen  D. Meng  and L. Zhang  “Beyond a Gaussian denoiser: Residual learning of

deep CNN for image denoising ” IEEE Trans. Image Process.  vol. 26  no. 7  pp. 3142–3155  2017.

[11] C. A. Metzler  A. Maleki  and R. G. Baraniuk  “From denoising to compressed sensing ” IEEE Trans. Info.

Thy.  vol. 62  no. 9  pp. 5117–5144  2016.

[12] D. Donoho  I. Johnstone  and A. Montanari  “Accurate prediction of phase transitions in compressed
sensing via a connection to minimax denoising ” IEEE Trans. Info. Thy.  vol. 59  no. 6  pp. 3396–3433 
2013.

[13] Y. Ma  C. Rush  and D. Baron  “Analysis of approximate message passing with a class of non-separable

denoisers ” in Proc. ISIT  2017  pp. 231–235.

[14] S. V. Venkatakrishnan  C. A. Bouman  and B. Wohlberg  “Plug-and-play priors for model based recon-
struction ” in Proc. IEEE Global Conference on Signal and Information Processing (GlobalSIP)  2013 
pp. 945–948.

[15] S. Chen  C. Luo  B. Deng  Y. Qin  H. Wang  and Z. Zhuang  “BM3D vector approximate message passing

for radar coded-aperture imaging ” in PIERS-FALL  2017  pp. 2035–2038.

[16] X. Wang and S. H. Chan  “Parameter-free plug-and-play ADMM for image restoration ” in Proc. IEEE

Acoustics  Speech and Signal Processing (ICASSP).

IEEE  2017  pp. 1323–1327.

[17] F. Caltagirone  L. Zdeborová  and F. Krzakala  “On convergence of approximate message passing ” in

Proc. IEEE ISIT  Jul. 2014  pp. 1812–1816.

[18] J. Vila  P. Schniter  S. Rangan  F. Krzakala  and L. Zdeborová  “Adaptive damping and mean removal for
the generalized approximate message passing algorithm ” in Proc. IEEE ICASSP  2015  pp. 2021–2025.

[19] S. Rangan  P. Schniter  and A. K. Fletcher  “Vector approximate message passing ” in Proc. IEEE ISIT 

2017  pp. 1588–1592.

[20] M. Opper and O. Winther  “Expectation consistent approximate inference ” J. Mach. Learning Res.  vol. 1 

pp. 2177–2204  2005.

[21] A. K. Fletcher  M. Sahraee-Ardakan  S. Rangan  and P. Schniter  “Expectation consistent approximate

inference: Generalizations and convergence ” in Proc. IEEE ISIT  2016  pp. 190–194.

[22] J. Ma and L. Ping  “Orthogonal AMP ” IEEE Access  vol. 5  pp. 2020–2033  2017.

9

[23] K. Takeuchi  “Rigorous dynamics of expectation-propagation-based signal recovery from unitarily invari-

ant measurements ” in Proc. ISIT  2017  pp. 501–505.

[24] S. Rangan  P. Schniter  and A. K. Fletcher  “Vector approximate message passing ” arXiv:1610.03082 

2016.

[25] A. K. Fletcher  M. Sahraee-Ardakan  S. Rangan  and P. Schniter  “Rigorous dynamics and consistent

estimation in arbitrarily conditioned linear systems ” in Proc. NIPS  2017  pp. 2542–2551.

[26] P. Schniter  A. K. Fletcher  and S. Rangan  “Denoising-based vector AMP ” in Proc. Intl. Biomedical and

Astronomical Signal Process. (BASP) Workshop  2017  p. 77.

[27] A. K. Fletcher  P. Pandit  S. Rangan  S. Sarkar  and P. Schniter  “Plug-in estimation in high-dimensional

linear inverse problems: A rigorous analysis ” arxiv preprint 1806.10466  2018.

[28] U. S. Kamilov  S. Rangan  A. K. Fletcher  and M. Unser  “Approximate message passing with consistent
parameter estimation and applications to sparse learning ” IEEE Trans. Info. Theory  vol. 60  no. 5  pp.
2969–2985  Apr. 2014.

[29] A. Taeb  A. Maleki  C. Studer  and R. Baraniuk  “Maximin analysis of message passing algorithms for

recovering block sparse signals ” arXiv preprint arXiv:1303.2389  2013.

[30] M. R. Andersen  O. Winther  and L. K. Hansen  “Bayesian inference for structured spike and slab priors ”

in Advances in Neural Information Processing Systems  2014  pp. 1745–1753.

[31] S. Rangan  A. K. Fletcher  V. K. Goyal  E. Byrne  and P. Schniter  “Hybrid approximate message passing ”

IEEE Transactions on Signal Processing  vol. 65  no. 17  pp. 4577–4592  Sept 2017.

[32] L. L. Scharf and C. Demeure  Statistical Signal Processing: Detection  Estimation  and Time Series

Analysis. Addison-Wesley Reading  MA  1991  vol. 63.

[33] J. Xie  L. Xu  and E. Chen  “Image denoising and inpainting with deep neural networks ” in Advances in

Neural Information Processing Systems  2012  pp. 341–349.

[34] L. Xu  J. S. Ren  C. Liu  and J. Jia  “Deep convolutional neural network for image deconvolution ” in

Advances in Neural Information Processing Systems  2014  pp. 1790–1798.

[35] J.-F. Cai  E. J. Candès  and Z. Shen  “A singular value thresholding algorithm for matrix completion ”

SIAM J. Optim.  vol. 20  no. 4  pp. 1956–1982  2010.

[36] M. Borgerding  P. Schniter  and S. Rangan  “AMP-inspired deep networks for sparse linear inverse prob-

lems ” IEEE Trans. Signal Process.  vol. 65  no. 16  pp. 4293–4308  2017.

[37] E. J. Candès  T. Strohmer  and V. Voroninski  “PhaseLift: Exact and stable signal recovery from magnitude
measurements via convex programming ” Commun. Pure Appl. Math.  vol. 66  no. 8  pp. 1241–1274 
2013.

[38] A. Ahmed  B. Recht  and J. Romberg  “Blind deconvolution using convex programming ” IEEE Trans.

Inform. Theory  vol. 60  no. 3  pp. 1711–1732  2014.

[39] S. Ling and T. Strohmer  “Self-calibration and biconvex compressive sensing ” Inverse Problems  vol. 31 

no. 11  p. 115002  2015.

[40] M. A. Davenport and J. Romberg  “An overview of low-rank matrix recovery from incomplete observa-

tions ” IEEE J. Sel. Topics Signal Process.  vol. 10  no. 4  pp. 608–622  2016.

[41] S. S. Haykin  Ed.  Blind Deconvolution. Upper Saddle River  NJ: Prentice-Hall  1994.

[42] H. Zhu  G. Leus  and G. B. Giannakis  “Sparsity-cognizant total least-squares for perturbed compressive

sampling ” IEEE Trans. Signal Process.  vol. 59  no. 5  pp. 2002–2016  2011.

[43] P. Sun  Z. Wang  and P. Schniter  “Joint channel-estimation and equalization of single-carrier systems via

bilinear AMP ” IEEE Trans. Signal Process.  vol. 66  no. 10  pp. 2772–2785  2018.

[44] S. Rangan and A. K. Fletcher  “Iterative estimation of constrained rank-one matrices in noise ” in Proc.

IEEE ISIT  Cambridge  MA  Jul. 2012  pp. 1246–1250.

[45] Y. Deshpande and A. Montanari  “Information-theoretically optimal sparse PCA ” in Proc. ISIT  2014  pp.

2197–2201.

[46] R. Matsushita and T. Tanaka  “Low-rank matrix reconstruction and clustering via approximate message

passing ” in Proc. NIPS  2013  pp. 917–925.

[47] T. Lesieur  F. Krzakala  and L. Zdeborova  “Phase transitions in sparse PCA ” in Proc. IEEE ISIT  2015 

pp. 1635–1639.

[48] J. Parker and P. Schniter  “Parametric bilinear generalized approximate message passing ” IEEE J. Sel.

Topics Signal Proc.  vol. 10  no. 4  pp. 795–808  2016.

10

,Alyson Fletcher
Parthe Pandit
Sundeep Rangan
Subrata Sarkar
Philip Schniter