2017,Bayesian GAN,Generative adversarial networks (GANs) can implicitly learn rich distributions over images  audio  and data which are hard to model with an explicit likelihood.  We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs.  Within this framework  we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks.  The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator  the Bayesian GAN avoids mode-collapse  produces interpretable and diverse candidate samples  and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN  CelebA  and CIFAR-10  outperforming DCGAN  Wasserstein GANs  and DCGAN ensembles.,Bayesian GAN

Yunus Saatchi
Uber AI Labs

Andrew Gordon Wilson

Cornell University

Abstract

Generative adversarial networks (GANs) can implicitly learn rich distributions over
images  audio  and data which are hard to model with an explicit likelihood. We
present a practical Bayesian formulation for unsupervised and semi-supervised
learning with GANs. Within this framework  we use stochastic gradient Hamilto-
nian Monte Carlo to marginalize the weights of the generator and discriminator
networks. The resulting approach is straightforward and obtains good performance
without any standard interventions such as label smoothing or mini-batch discrimi-
nation. By exploring an expressive posterior over the parameters of the generator 
the Bayesian GAN avoids mode-collapse  produces interpretable and diverse candi-
date samples  and provides state-of-the-art quantitative results for semi-supervised
learning on benchmarks including SVHN  CelebA  and CIFAR-10  outperforming
DCGAN  Wasserstein GANs  and DCGAN ensembles.

1

Introduction

Learning a good generative model for high-dimensional natural signals  such as images  video
and audio has long been one of the key milestones of machine learning. Powered by the learning
capabilities of deep neural networks  generative adversarial networks (GANs) [4] and variational
autoencoders [6] have brought the ﬁeld closer to attaining this goal.
GANs transform white noise through a deep neural network to generate candidate samples from
a data distribution. A discriminator learns  in a supervised manner  how to tune its parameters
so as to correctly classify whether a given sample has come from the generator or the true data
distribution. Meanwhile  the generator updates its parameters so as to fool the discriminator. As
long as the generator has sufﬁcient capacity  it can approximate the CDF inverse-CDF composition
required to sample from a data distribution of interest. Since convolutional neural networks by design
provide reasonable metrics over images (unlike  for instance  Gaussian likelihoods)  GANs using
convolutional neural networks can in turn provide a compelling implicit distribution over images.
Although GANs have been highly impactful  their learning objective can lead to mode collapse  where
the generator simply memorizes a few training examples to fool the discriminator. This pathology
is reminiscent of maximum likelihood density estimation with Gaussian mixtures: by collapsing
the variance of each component we achieve inﬁnite likelihood and memorize the dataset  which is
not useful for a generalizable density estimate. Moreover  a large degree of intervention is required
to stabilize GAN training  including label smoothing and mini-batch discrimination [9  10]. To
help alleviate these practical difﬁculties  recent work has focused on replacing the Jensen-Shannon
divergence implicit in standard GAN training with alternative metrics  such as f-divergences [8] or
Wasserstein divergences [1]. Much of this work is analogous to introducing various regularizers for
maximum likelihood density estimation. But just as it can be difﬁcult to choose the right regularizer 
it can also be difﬁcult to decide which divergence we wish to use for GAN training.
It is our contention that GANs can be improved by fully probabilistic inference. Indeed  a posterior
distribution over the parameters of the generator could be broad and highly multimodal. GAN
training  which is based on mini-max optimization  always estimates this whole posterior distribution

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

over the network weights as a point mass centred on a single mode. Thus even if the generator
does not memorize training examples  we would expect samples from the generator to be overly
compact relative to samples from the data distribution. Moreover  each mode in the posterior over the
network weights could correspond to wildly different generators  each with their own meaningful
interpretations. By fully representing the posterior distribution over the parameters of both the
generator and discriminator  we can more accurately model the true data distribution. The inferred
data distribution can then be used for accurate and highly data-efﬁcient semi-supervised learning.
In this paper  we propose a simple Bayesian formulation for end-to-end unsupervised and semi-
supervised learning with generative adversarial networks. Within this framework  we marginalize the
posteriors over the weights of the generator and discriminator using stochastic gradient Hamiltonian
Monte Carlo. We interpret data samples from the generator  showing exploration across several
distinct modes in the generator weights. We also show data and iteration efﬁcient learning of the true
distribution. We also demonstrate state of the art semi-supervised learning performance on several
benchmarks  including SVHN  MNIST  CIFAR-10  and CelebA. The simplicity of the proposed
approach is one of its greatest strengths: inference is straightforward  interpretable  and stable. Indeed
all of the experimental results were obtained without many of the ad-hoc techniques often used to
train standard GANs.
We have made code and tutorials available at
https://github.com/andrewgordonwilson/bayesgan.

2 Bayesian GANs
Given a dataset D = {x(i)} of variables x(i) ∼ pdata(x(i))  we wish to estimate pdata(x). We
transform white noise z ∼ p(z) through a generator G(z; θg)  parametrized by θg  to produce
candidate samples from the data distribution. We use a discriminator D(x; θd)  parametrized by θd 
to output the probability that any x comes from the data distribution. Our considerations hold for
general G and D  but in practice G and D are often neural networks with weight vectors θg and θd.
By placing distributions over θg and θd  we induce distributions over an uncountably inﬁnite space of
generators and discriminators  corresponding to every possible setting of these weight vectors. The
generator now represents a distribution over distributions of data. Sampling from the induced prior
distribution over data instances proceeds as follows:
(1) Sample θg ∼ p(θg); (2) Sample z(1)  . . .   z(n) ∼ p(z); (3) ˜x(j) = G(z(j); θg) ∼ pgenerator(x).
For posterior inference  we propose unsupervised and semi-supervised formulations in Sec 2.1 - 2.2.
We note that in an exciting recent work Tran et al. [11] brieﬂy mention using a variational approach
to marginalize weights in a generative model  as part of a general exposition on hierarchical implicit
models (see also Karaletsos [5] for a nice theoretical exploration of related topics in graphical model
message passing). While this related work is promising  our approach has several key differences:
(1) our GAN representation is quite different  with novel formulations for the conditional posteriors 
preserving a clear competition between generator and discriminator; (2) our representation for the
posteriors is straightforward  provides novel formulations for unsupervised and semi-supervised
learning  and has state of the art results on many benchmarks. Conversely  Tran et al. [11] is only
pursued for fully supervised learning on a few small datasets; (3) we use sampling to explore a full
posterior over the weights  whereas Tran et al. [11] perform a variational approximation centred on
one of the modes of the posterior (and due to the properties of the KL divergence is prone to an
overly compact representation of even that mode); (4) we marginalize z in addition to θg  θd; and (5)
the ratio estimation approach in [11] limits the size of the neural networks they can use  whereas in
our experiments we can use comparably deep networks to maximum likelihood approaches. In the
experiments we illustrate the practical value of our formulation.
Although the high level concept of a Bayesian GAN has been informally mentioned in various
contexts  to the best of our knowledge we present the ﬁrst detailed treatment of Bayesian GANs 
including novel formulations  sampling based inference  and rigorous semi-supervised learning
experiments.

2

2.1 Unsupervised Learning

(cid:32) ng(cid:89)
p(θd|z  X  θg) ∝ nd(cid:89)

p(θg|z  θd) ∝

i=1

(cid:33)

ng(cid:89)

To infer posteriors over θg  θd  we propose to iteratively sample from the following conditional
posteriors:

D(G(z(i); θg); θd)

p(θg|αg)

(1)

i=1

i=1

D(x(i); θd) ×

(1 − D(G(z(i); θg); θd)) × p(θd|αd) .

(2)
p(θg|αg) and p(θd|αd) are priors over the parameters of the generator and discriminator  with
hyperparameters αg and αd  respectively. nd and ng are the numbers of mini-batch samples for the
discriminator and generator  respectively.1 We deﬁne X = {x(i)}nd
i=1.
We can intuitively understand this formulation starting from the generative process for data samples.
Suppose we were to sample weights θg from the prior p(θg|αg)  and then condition on this sample
of the weights to form a particular generative neural network. We then sample white noise z from
p(z)  and transform this noise through the network G(z; θg) to generate candidate data samples.
The discriminator  conditioned on its weights θd  outputs a probability that these candidate samples
came from the data distribution. Eq. (1) says that if the discriminator outputs high probabilities  then
the posterior p(θg|z  θd) will increase in a neighbourhood of the sampled setting of θg (and hence
decrease for other settings). For the posterior over the discriminator weights θd  the ﬁrst two terms of
Eq. (2) form a discriminative classiﬁcation likelihood  labelling samples from the actual data versus
the generator as belonging to separate classes. And the last term is the prior on θd.

Classical GANs as maximum likelihood Moreover  our proposed probabilistic approach is a
natural Bayesian generalization of the classical GAN: if one uses uniform priors for θg and θd  and
performs iterative MAP optimization instead of posterior sampling over Eq. (1) and (2)  then the
local optima will be the same as for Algorithm 1 of Goodfellow et al. [4]. We thus sometimes refer
to the classical GAN as the ML-GAN. Moreover  even with a ﬂat prior  there is a big difference
between Bayesian marginalization over the whole posterior versus approximating this (often broad 
multimodal) posterior with a point mass as in MAP optimization (see Figure 3  Supplement).

Jd

j=1

=p(z)

(cid:90)

(cid:90)

Marginalizing the noise
samples z. We can instead marginalize z from our posterior updates using simple Monte Carlo:

In prior work  GAN updates are implicitly conditioned on a set of noise

Jg(cid:88)

(cid:122) (cid:125)(cid:124) (cid:123)
(cid:80)Jd
j p(θd|z(j)  X  θg)  z(j) ∼ p(z).

p(z|θd) dz ≈ 1
Jg

p(θg|θd) =

p(θg  z|θd)dz =

p(θg|z(j)  θd)   z(j) ∼ p(z)

p(θg|z  θd)
By following a similar derivation  p(θd|θg) ≈ 1
This speciﬁc setup has several nice features for Monte Carlo integration. First  p(z) is a white noise
distribution from which we can take efﬁcient and exact samples. Secondly  both p(θg|z  θd) and
p(θd|z  X  θg)  when viewed as a function of z  should be reasonably broad over z by construction 
since z is used to produce candidate data samples in the generative procedure. Thus each term in the
simple Monte Carlo sum typically makes a reasonable contribution to the total marginal posterior
estimates. We do note  however  that the approximation will typically be worse for p(θd|θg) due to
the conditioning on a minibatch of data in Equation 2.
Posterior samples By iteratively sampling from p(θg|θd) and p(θd|θg) at every step of an epoch
one can  in the limit  obtain samples from the approximate posteriors over θg and θd. Having such
samples can be very useful in practice. Indeed  one can use different samples for θg to alleviate
GAN collapse and generate data samples with an appropriate level of entropy  as well as forming
a committee of generators to strengthen the discriminator. The samples for θd in turn form a
committee of discriminators which ampliﬁes the overall adversarial signal  thereby further improving
the unsupervised learning process. Arguably  the most rigorous method to assess the utility of these
posterior samples is to examine their effect on semi-supervised learning  which is a focus of our
experiments in Section 4.

1For mini-batches  one must make sure the likelihood and prior are scaled appropriately. See Supplement.

3

2.2 Semi-supervised Learning

We extend the proposed probabilistic GAN formalism to semi-supervised learning. In the semi-
supervised setting for K-class classiﬁcation  we have access to a set of n unlabelled observations 
{x(i)}  as well as a (typically much smaller) set of ns observations  {(x(i)
i=1  with class
s ∈ {1  . . .   K}. Our goal is to jointly learn statistical structure from both the unlabelled
labels y(i)
and labelled examples  in order to make much better predictions of class labels for new test examples
x∗ than if we only had access to the labelled training inputs.
In this context  we redeﬁne the discriminator such that D(x(i) = y(i); θd) gives the probability that
sample x(i) belongs to class y(i). We reserve the class label 0 to indicate that a data sample is the
output of the generator. We then infer the posterior over the weights as follows:

s )}Ns

s   y(i)

(cid:32) ng(cid:89)
K(cid:88)
p(θd|z  X  ys  θg) ∝ nd(cid:89)
K(cid:88)

p(θg|z  θd) ∝

y=1

i=1

i=1

y=1

(cid:33)

ng(cid:89)

i=1

D(G(z(i); θg) = y; θd)

p(θg|αg)

D(x(i) = y; θd)

D(G(z(i); θg) = 0; θd)

Ns(cid:89)

(3)

(D(x(i)

s = y(i)

s ; θd))p(θd|αd)

i=1

(cid:90)

T(cid:88)

k=1

(4)
During every iteration we use ng samples from the generator  nd unlabeled samples  and all of the
Ns labeled samples  where typically Ns (cid:28) n. As in Section 2.1  we can approximately marginalize
z using simple Monte Carlo sampling.
Much like in the unsupervised learning case  we can marginalize the posteriors over θg and θd. To
compute the predictive distribution for a class label y∗ at a test input x∗ we use a model average over
all collected samples with respect to the posterior over θd:

p(y∗|x∗ D) =

p(y∗|x∗  θd)p(θd|D)dθd ≈ 1
T

p(y∗|x∗  θ(k)

d )   θ(k)

d ∼ p(θd|D) .

(5)

We will see that this model average is effective for boosting semi-supervised learning performance.
In Section 3 we present an approach to MCMC sampling from the posteriors over θg and θd.

3 Posterior Sampling with Stochastic Gradient HMC

In the Bayesian GAN  we wish to marginalize the posterior distributions over the generator and
discriminator weights  for unsupervised learning in 2.1 and semi-supervised learning in 2.2. For this
purpose  we use Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) [3] for posterior sampling.
The reason for this choice is three-fold: (1) SGHMC is very closely related to momentum-based
SGD  which we know empirically works well for GAN training; (2) we can import parameter settings
(such as learning rates and momentum terms) from SGD directly into SGHMC; and most importantly 
(3) many of the practical beneﬁts of a Bayesian approach to GAN inference come from exploring
a rich multimodal distribution over the weights θg of the generator  which is enabled by SGHMC.
Alternatives  such as variational approximations  will typically centre their mass around a single
mode  and thus provide a unimodal and otherwise compact representation for the distribution  due to
asymmetric biases of the KL-divergence.
The posteriors in Equations 3 and 4 are both amenable to HMC techniques as we can compute the
gradients of the loss with respect to the parameters we are sampling. SGHMC extends HMC to the
case where we use noisy estimates of such gradients in a manner which guarantees mixing in the
limit of a large number of minibatches. For a detailed review of SGHMC  please see Chen et al. [3].
Using the update rules from Eq. (15) in Chen et al. [3]  we propose to sample from the posteriors over
the generator and discriminator weights as in Algorithm 1. Note that Algorithm 1 outlines standard
momentum-based SGHMC: in practice  we found it helpful to speed up the “burn-in” process by
replacing the SGD part of this algorithm with Adam for the ﬁrst few thousand iterations  after which
we revert back to momentum-based SGHMC. As suggested in Appendix G of Chen et al. [3]  we
employed a learning rate schedule which decayed according to γ/d where d is set to the number of
unique “real” datapoints seen so far. Thus  our learning rate schedule converges to γ/N in the limit 
where we have deﬁned N = |D|.

4

Algorithm 1 One iteration of sampling for the Bayesian GAN. α is the friction term for SGHMC  η is the
learning rate. We assume that the stochastic gradient discretization noise term ˆβ is dominated by the main
friction term (this assumption constrains us to use small step sizes). We take Jg and Jd simple MC samples for
the generator and discriminator respectively  and M SGHMC samples for each simple MC sample. We rescale
to accommodate minibatches as in the supplementary material.

• Represent posteriors with samples {θj m
for number of MC iterations Jg do

j=1 m=1 from previous iteration
• Sample Jg noise samples {z(1)  . . .   z(Jg)} from noise prior p(z). Each z(i) has ng samples.
• Update sample set representing p(θg|θd) by running SGHMC updates for M iterations:

j=1 m=1 and {θj m

d }Jd M

g }Jg M

g ← θj m
θj m

g + v; v ← (1 − α)v + η

∂ log

k p(θg|z(i)  θk m
∂θg

d

)

+ n; n ∼ N (0  2αηI)

(cid:16)(cid:80)

i

(cid:80)

(cid:17)

• Append θj m

g

to sample set.

end for
for number of MC iterations Jd do

• Sample minibatch of Jd noise samples {z(1)  . . .   z(Jd)} from noise prior p(z).
• Sample minibatch of nd data samples x.
• Update sample set representing p(θd|z  θg) by running SGHMC updates for M iterations:
d ← θj m
θj m
• Append θj m

(cid:80)
k p(θd|z(i)  x  θk m
∂θd

d + v; v ← (1 − α)v + η

∂ log(cid:0)(cid:80)

to sample set.

)(cid:1)

g

i

+ n; n ∼ N (0  2αηI)

end for

d

4 Experiments

We evaluate our proposed Bayesian GAN (henceforth titled BayesGAN) on six benchmarks (synthetic 
MNIST  CIFAR-10  SVHN  and CelebA) each with four different numbers of labelled examples. We
consider multiple alternatives  including: the DCGAN [9]  the recent Wasserstein GAN (W-DCGAN)
[1]  an ensemble of ten DCGANs (DCGAN-10) which are formed by 10 random subsets 80% the
size of the training set  and a fully supervised convolutional neural network. We also compare to the
reported MNIST result for the LFVI-GAN  brieﬂy mentioned in a recent work [11]  where they use
fully supervised modelling on the whole dataset with a variational approximation. We interpret many
of the results from MNIST in detail in Section 4.2  and ﬁnd that these observations carry forward to
our CIFAR-10  SVHN  and CelebA experiments.
For all real experiments except MNIST we use a 6-layer Bayesian deconvolutional GAN (BayesGAN)
for the generative model G(z|θg) (see Radford et al. [9] for further details about structure). The
corresponding discriminator is a 6-layer 2-class DCGAN for the unsupervised GAN and a 6-layer 
K + 1 class DCGAN for a semi-supervised GAN performing classiﬁcation over K classes. The
connectivity structure of the unsupervised and semi-supervised DCGANs were the same as for the
BayesGAN. As recommended by [10]  we used feature matching for all models on semi-supervised
experiments. For MNIST we found that 4-layers for all networks worked slightly better across the
board  due to the added simplicity of the dataset. Note that the structure of the networks in Radford
et al. [9] are slightly different from [10] (e.g. there are 4 hidden layers and fewer ﬁlters per layer)  so
one cannot directly compare the results here with those in Salimans et al. [10]. Our exact architecture
speciﬁcation is also given in our codebase. The performance of all methods could be improved
through further calibrating architecture design for each individual benchmark. For the Bayesian
GAN we place a N (0  10I) prior on both the generator and discriminator weights and approximately
integrate out z using simple Monte Carlo samples. We run Algorithm 1 for 5000 iterations and then
collect weight samples every 1000 iterations and record out-of-sample predictive accuracy using
Bayesian model averaging (see Eq. 5). For Algorithm 1 we set Jg = 10  Jd = 1  M = 2  and
nd = ng = 64. All experiments were performed on a single TitanX GPU for consistency  but
BayesGAN and DCGAN-10 could be sped up to approximately the same runtime as DCGAN through
multi-GPU parallelization.

5

In Table 1 we summarize the semi-supervised results  where we see consistently improved perfor-
mance over the alternatives. All runs are averaged over 10 random subsets of labeled examples for
estimating error bars on performance (Table 1 shows mean and 2 standard deviations). We also
qualitatively illustrate the ability for the Bayesian GAN to produce complementary sets of data
samples  corresponding to different representations of the generator produced by sampling from the
posterior over the generator weights (Figures 1  2  5). The supplement also contains additional plots
of accuracy per epoch for semi-supervised experiments.

4.1 Synthetic Dataset

We present experiments on a multi-modal synthetic dataset to test the ability to infer a multi-modal
posterior p(θg|D). This ability not only helps avoid the collapse of the generator to a couple training
examples  an instance of overﬁtting in regular GAN training  but also allows one to explore a set of
generators with different complementary properties  harmonizing together to encapsulate a rich data
distribution. We generate D-dimensional synthetic data as follows:

z ∼ N (0  10 ∗ Id)  A ∼ N (0  ID×d)  x = Az +  

 ∼ N (0  0.01 ∗ ID) 

d (cid:28) D

We then ﬁt both a regular GAN and a Bayesian GAN to such a dataset with D = 100 and d = 2. The
generator for both models is a two-layer neural network: 10-1000-100  fully connected  with ReLU
activations. We set the dimensionality of z to be 10 in order for the DCGAN to converge (it does not
converge when d = 2  despite the inherent dimensionality being 2!). Consistently  the discriminator
network has the following structure: 100-1000-1  fully-connected  ReLU activations. For this dataset
we place an N (0  I) prior on the weights of the Bayesian GAN and approximately integrate out z
using J = 100 Monte-Carlo samples. Figure 1 shows that the Bayesian GAN does a much better
job qualitatively in generating samples (for which we show the ﬁrst two principal components)  and
quantitatively in terms of Jensen-Shannon divergence (JSD) to the true distribution (determined
through kernel density estimates). In fact  the DCGAN (labelled ML GAN as per Section 2) begins to
eventually increase in testing JSD after a certain number of training iterations  which is reminiscent
of over-ﬁtting. When D = 500  we still see good performance with the Bayesian GAN. We also see 
with multidimensional scaling [2]  that samples from the posterior over Bayesian generator weights
clearly form multiple distinct clusters  indicating that the SGHMC sampling is exploring multiple
distinct modes  thus capturing multimodality in weight space as well as in data space.

4.2 MNIST

MNIST is a well-understood benchmark dataset consisting of 60k (50k train  10k test) labeled images
of hand-written digits. Salimans et al. [10] showed excellent out-of-sample performance using only
a small number of labeled inputs  convincingly demonstrating the importance of good generative
modelling for semi-supervised learning. Here  we follow their experimental setup for MNIST.
We evaluate the Bayesian DCGAN for semi-supervised learning using Ns = {20  50  100  200}
labelled training examples. We see in Table 1 that the Bayesian GAN has improved accuracy over the
DCGAN  the Wasserstein GAN  and even an ensemble of 10 DCGANs! Moreover  it is remarkable
that the Bayesian GAN with only 100 labelled training examples (0.2% of the training data) is able to
achieve 99.3% testing accuracy  which is comparable with a state of the art fully supervised method
using all 50  000 training examples! We show a fully supervised model using ns samples to generally
highlight the practical utility of semi-supervised learning.
Moreover  Tran et al. [11] showed that a fully supervised LFVI-GAN  on the whole MNIST training
set (50  000 labelled examples) produces 140 classiﬁcation errors – almost twice the error of our
proposed Bayesian GAN approach using only ns = 100 (0.2%) labelled examples! We suspect
this difference largely comes from (1) the simple practical formulation of the Bayesian GAN in
Section 2  (2) marginalizing z via simple Monte Carlo  and (3) exploring a broad multimodal
posterior distribution over the generator weights with SGHMC with our approach versus a variational
approximation (prone to over-compact representations) centred on a single mode.
We can also see qualitative differences in the unsupervised data samples from our Bayesian DCGAN
and the standard DCGAN in Figure 2. The top row shows sample images produced from six generators
produced from six samples over the posterior of the generator weights  and the bottom row shows
sample data images from a DCGAN. We can see that each of the six panels in the top row have

6

Figure 1: Left: Samples drawn from pdata(x) and visualized in 2-D after applying PCA. Right 2 columns:
Samples drawn from pMLGAN(x) and pBGAN(x) visualized in 2D after applying PCA. The data is inherently
2-dimensional so PCA can explain most of the variance using 2 principal components. It is clear that the
Bayesian GAN is capturing all the modes in the data whereas the regular GAN is unable to do so. Right:
(Top 2) Jensen-Shannon divergence between pdata(x) and p(x; θ) as a function of the number of iterations of
GAN training for D = 100 (top) and D = 500 (bottom). The divergence is computed using kernel density
estimates of large sample datasets drawn from pdata(x) and p(x; θ)  after applying dimensionality reduction
to 2-D (the inherent dimensionality of the data). In both cases  the Bayesian GAN is far more effective at
minimizing the Jensen-Shannon divergence  reaching convergence towards the true distribution  by exploring
a full distribution over generator weights  which is not possible with a maximum likelihood GAN (no matter
how many iterations). (Bottom) The sample set {θk
g} after convergence viewed in 2-D using Multidimensional
Scaling (using a Euclidean distance metric between weight samples) [2]. One can clearly see several clusters 
meaning that the SGHMC sampling has discovered pronounced modes in the posterior over the weights.

qualitative differences  almost as if a different person were writing the digits in each panel. Panel
1 (top left)  for example  is quite crisp  while panel 3 is fairly thick  and panel 6 (top right) has
thin and fainter strokes. In other words  the Bayesian GAN is learning different complementary
generative hypotheses to explain the data. By contrast  all of the data samples on the bottom row
from the DCGAN are homogenous. In effect  each posterior weight sample in the Bayesian GAN
corresponds to a different style  while in the standard DCGAN the style is ﬁxed. This difference
is further illustrated for all datasets in Figure 5 (supplement). Figure 3 (supplement) also further
emphasizes the utility of Bayesian marginalization versus optimization  even with vague priors.
However  we do not necessarily expect high ﬁdelity images from any arbitrary generator sampled
from the posterior over generators; in fact  such a generator would probably have less posterior
probability than the DCGAN  which we show in Section 2 can be viewed as a maximum likelihood
analogue of our approach. The advantage in the Bayesian approach comes from representing a whole
space of generators alongside their posterior probabilities.
Practically speaking  we also stress that for reasonable sample generation from the maximum-
likelihood DCGAN we had to resort to using tricks including minibatch discrimination  feature
normalization and the addition of Gaussian noise to each layer of the discriminator. The Bayesian
DCGAN needed none of these tricks. This robustness arises from a Gaussian prior over the weights
which provides a useful inductive bias  and due to the MCMC sampling procedure which alleviates

7

Table 1: Detailed supervised and semi-supervised learning results for all datasets. In almost all experiments
BayesGAN outperforms DCGAN and W-DCGAN substantially  and typically even outperforms ensembles of
DCGANs. The runtimes  per epoch  in minutes  are provided in rows including the dataset name. While all
experiments were performed on a single GPU  note that DCGAN-10 and BayesGAN methods can be sped up
straightforwardly using multiple GPUs to obtain a similar runtime to DCGAN. Note also that the BayesGAN is
generally much more efﬁcient per epoch than the alternatives  as per Figure 4 (supplement). Results are averaged
over 10 random supervised subsets ± 2 stdev. Standard train/test splits are used for MNIST  CIFAR-10 and
SVHN. For CelebA we use a test set of size 10k. Test error rates are across the entire test set.
No. of misclassiﬁcations for MNIST. Test error rate for others.
Supervised
N=50k  D = (28  28)

Ns

DCGAN-10
112
1453 ± 532
329 ± 139
102 ± 11
88 ± 6
217
39.6 ± 2.8
32.4 ± 2.9
27.4 ± 3.2
22.6 ± 2.2
286
31.8 ± 4.1
19.8 ± 2.1
17.1 ± 2.3
13.0 ± 1.9
767
43.3 ± 5.3
28.2 ± 1.3
21.3 ± 1.2
20.1 ± 1.4

BayesGAN
39
1402 ± 422
321 ± 194
98 ± 13
82 ± 5
102
41.3 ± 5.1
31.4 ± 3.6
25.9 ± 3.7
23.1 ± 3.9
107
32.8 ± 4.4
21.9 ± 3.5
16.3 ± 2.4
12.7 ± 1.4
387
42.4 ± 6.7
26.8 ± 4.2
22.6 ± 3.7
19.4 ± 3.4

MNIST
20
50
100
200
CIFAR-10
1000
2000
4000
8000
SVHN
500
1000
2000
4000

2134 ± 525
1389 ± 438
N=50k  D = (32  32  3)
63.4 ± 2.6
56.1 ± 2.1
51.4 ± 2.9
47.2 ± 2.2
N=75k  D = (32  32  3)
53.5 ± 2.5
37.3 ± 3.1
26.3 ± 2.1
20.8 ± 1.8
CelebA N=100k  D = (50  50  3)
53.8 ± 4.2
36.7 ± 3.2
34.3 ± 3.8
31.1 ± 4.2

1000
2000
4000
8000

DCGAN W-DCGAN
19
1623 ± 325
412 ± 199
134 ± 28
91 ± 10
38
46.1 ± 3.6
35.8 ± 3.8
31.1 ± 4.7
24.4 ± 5.5
34
36.1 ± 4.2
22.1 ± 4.8
21.0 ± 1.3
17.1 ± 1.2
117
45.5 ± 5.9
30.1 ± 3.3
26.0 ± 2.1
21.0 ± 1.9

16
— 1618 ± 388
— 432 ± 187
121 ± 18
95 ± 7
34
48.6 ± 3.4
34.1 ± 4.1
30.8 ± 4.6
25.1 ± 3.3
31
38.2 ± 3.1
23.6 ± 4.6
21.2 ± 3.1
18.2 ± 1.7
109
48.1 ± 4.8
31.1 ± 3.2
28.3 ± 3.2
22.5 ± 1.5

the risk of collapse and helps explore multiple modes (and uncertainty within each mode). To be
balanced  we also stress that in practice the risk of collapse is not fully eliminated – indeed  some
samples from p(θg|D) still produce generators that create data samples with too little entropy. In
practice  sampling is not immune to becoming trapped in sharply peaked modes. We leave further
analysis for future work.

Figure 2: Top: Data samples from six different generators corresponding to six samples from the posterior over
θg. The data samples show that each explored setting of the weights θg produces generators with complementary
high-ﬁdelity samples  corresponding to different styles. The amount of variety in the samples emerges naturally
using the Bayesian approach. Bottom: Data samples from a standard DCGAN (trained six times). By contrast 
these samples are homogenous in style.

4.3 CIFAR-10

CIFAR-10 is also a popular benchmark dataset [7]  with 50k training and 10k test images  which is
harder to model than MNIST since the data are 32x32 RGB images of real objects. Figure 5 shows

8

datasets produced from four different generators corresponding to samples from the posterior over
the generator weights. As with MNIST  we see meaningful qualitative variation between the panels.
In Table 1 we also see again (but on this more challenging dataset) that using Bayesian GANs as a
generative model within the semi-supervised learning setup signiﬁcantly decreases test set error over
the alternatives  especially when ns (cid:28) n.

4.4 SVHN

The StreetView House Numbers (SVHN) dataset consists of RGB images of house numbers taken
by StreetView vehicles. Unlike MNIST  the digits signiﬁcantly differ in shape and appearance. The
experimental procedure closely followed that for CIFAR-10. There are approximately 75k training
and 25k test images. We see in Table 1 a particularly pronounced difference in performance between
BayesGAN and the alternatives. Data samples are shown in Figure 5.

4.5 CelebA

The large CelebA dataset contains 120k celebrity faces amongst a variety of backgrounds (100k
training  20k test images). To reduce background variations we used a standard face detector [12] to
crop the faces into a standard 50 × 50 size. Figure 5 shows data samples from the trained Bayesian
GAN. In order to assess performance for semi-supervised learning we created a 32-class classiﬁcation
task by predicting a 5-bit vector indicating whether or not the face: is blond  has glasses  is male  is
pale and is young. Table 1 shows the same pattern of promising performance for CelebA.

5 Discussion

By exploring rich multimodal distributions over the weight parameters of the generator  the Bayesian
GAN can capture a diverse set of complementary and interpretable representations of data. We have
shown that such representations can enable state of the art performance on semi-supervised problems 
using a simple inference procedure.
Effective semi-supervised learning of natural high dimensional data is crucial for reducing the
dependency of deep learning on large labelled datasets. Often labeling data is not an option  or
it comes at a high cost – be it through human labour or through expensive instrumentation (such
as LIDAR for autonomous driving). Moreover  semi-supervised learning provides a practical and
quantiﬁable mechanism to benchmark the many recent advances in unsupervised learning.
Although we use MCMC  in recent years variational approximations have been favoured for inference
in Bayesian neural networks. However  the likelihood of a deep neural network can be broad with
many shallow local optima. This is exactly the type of density which is amenable to a sampling based
approach  which can explore a full posterior. Variational methods  by contrast  typically centre their
approximation along a single mode and also provide an overly compact representation of that mode.
Therefore in the future we may generally see advantages in following a sampling based approach in
Bayesian deep learning. Aside from sampling  one could try to better accommodate the likelihood
functions common to deep learning using more general divergence measures (for example based on
the family of α-divergences) instead of the KL divergence in variational methods  alongside more
ﬂexible proposal distributions.
In the future  one could also estimate the marginal likelihood of a probabilistic GAN  having integrated
away distributions over the parameters. The marginal likelihood provides a natural utility function for
automatically learning hyperparameters  and for performing principled quantiﬁable model comparison
between different GAN architectures. It would also be interesting to consider the Bayesian GAN in
conjunction with a non-parametric Bayesian deep learning framework  such as deep kernel learning
[13  14]. We hope that our work will help inspire continued exploration into Bayesian deep learning.

Acknowledgements We thank Pavel Izmailov and Ben Athiwaratkun for helping to create a tutorial
for the codebase  helpful comments and validation. We also thank Soumith Chintala for helpful
advice. We thank NSF IIS-1563887 for support.

9

References
[1] Arjovsky  M.  Chintala  S.  and Bottou  L. (2017). Wasserstein GAN.

arXiv:1701.07875.

arXiv preprint

[2] Borg  I. and Groenen  P. J. (2005). Modern multidimensional scaling: Theory and applications.

Springer Science & Business Media.

[3] Chen  T.  Fox  E.  and Guestrin  C. (2014). Stochastic gradient Hamiltonian Monte Carlo. In

Proc. International Conference on Machine Learning.

[4] Goodfellow  I.  Pouget-Abadie  J.  Mirza  M.  Xu  B.  Warde-Farley  D.  Ozair  S.  Courville  A. 
and Bengio  Y. (2014). Generative adversarial nets. In Advances in neural information processing
systems  pages 2672–2680.

[5] Karaletsos  T. (2016). Adversarial message passing for graphical models. arXiv preprint

arXiv:1612.05048.

[6] Kingma  D. P. and Welling  M. (2013). Auto-encoding variational Bayes. arXiv preprint

arXiv:1312.6114.

[7] Krizhevsky  A.  Nair  V.  and Hinton  G. (2010). Cifar-10 (Canadian institute for advanced

research).

[8] Nowozin  S.  Cseke  B.  and Tomioka  R. (2016). f-GAN: Training generative neural samplers
using variational divergence minimization. In Advances in Neural Information Processing Systems 
pages 271–279.

[9] Radford  A.  Metz  L.  and Chintala  S. (2015). Unsupervised representation learning with deep

convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.

[10] Salimans  T.  Goodfellow  I. J.  Zaremba  W.  Cheung  V.  Radford  A.  and Chen  X. (2016).

Improved techniques for training gans. CoRR  abs/1606.03498.

[11] Tran  D.  Ranganath  R.  and Blei  D. (2017). Hierarchical implicit models and likelihood-free
variational inference. In Advances in Neural Information Processing Systems  pages 5529–5539.

[12] Viola  P. and Jones  M. J. (2004). Robust real-time face detection. Int. J. Comput. Vision 

57(2):137–154.

[13] Wilson  A. G.  Hu  Z.  Salakhutdinov  R.  and Xing  E. P. (2016a). Deep kernel learning.

Artiﬁcial Intelligence and Statistics.

[14] Wilson  A. G.  Hu  Z.  Salakhutdinov  R. R.  and Xing  E. P. (2016b). Stochastic variational
deep kernel learning. In Advances in Neural Information Processing Systems  pages 2586–2594.

10

,Mingjun Zhong
Charles Sutton
Navdeep Jaitly
Quoc Le
Oriol Vinyals
Ilya Sutskever
David Sussillo
Samy Bengio
Yunus Saatci
Andrew Wilson
Kevin Bello
Jean Honorio