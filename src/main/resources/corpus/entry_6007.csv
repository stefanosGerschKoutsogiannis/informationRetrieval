2013,Σ-Optimality for Active Learning on Gaussian Random Fields,A common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes  equivalent to the harmonic predictor on Gaussian random fields (GRFs). For active learning on GRFs  the commonly used V-optimality criterion queries nodes that reduce the L2 (regression) loss. V-optimality satisfies a submodularity property showing that greedy reduction produces a (1 − 1/e) globally optimal solution. However  L2 loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning. We consider a new criterion we call Σ-optimality  which queries the node that minimizes the sum of the elements in the predictive covariance. Σ-optimality directly optimizes the risk of the surveying problem  which is to determine the proportion of nodes belonging to one class. In this paper we extend submodularity guarantees from V-optimality to Σ-optimality using properties specific to GRFs. We further show that GRFs satisfy the suppressor-free condition in addition to the conditional independence inherited from Markov random fields. We test Σ-optimality on real-world graphs with both synthetic and real data and show that it outperforms V-optimality and other related methods on classification.,Σ-Optimality for Active Learning on Gaussian

Random Fields

Yifei Ma

Machine Learning Department
Carnegie Mellon University
yifeim@cs.cmu.edu

Roman Garnett

rgarnett@uni-bonn.de

Computer Science Department

University of Bonn

Jeff Schneider
Robotics Institute

Carnegie Mellon University
schneide@cs.cmu.edu

Abstract

A common classiﬁer for unlabeled nodes on undirected graphs uses label propaga-
tion from the labeled nodes  equivalent to the harmonic predictor on Gaussian ran-
dom ﬁelds (GRFs). For active learning on GRFs  the commonly used V-optimality
criterion queries nodes that reduce the L2 (regression) loss. V-optimality satis-
ﬁes a submodularity property showing that greedy reduction produces a (1− 1/e)
globally optimal solution. However  L2 loss may not characterise the true nature
of 0/1 loss in classiﬁcation problems and thus may not be the best choice for active
learning.
We consider a new criterion we call Σ-optimality  which queries the node that
minimizes the sum of the elements in the predictive covariance. Σ-optimality
directly optimizes the risk of the surveying problem  which is to determine the
proportion of nodes belonging to one class. In this paper we extend submodularity
guarantees from V-optimality to Σ-optimality using properties speciﬁc to GRFs.
We further show that GRFs satisfy the suppressor-free condition in addition to
the conditional independence inherited from Markov random ﬁelds. We test Σ-
optimality on real-world graphs with both synthetic and real data and show that it
outperforms V-optimality and other related methods on classiﬁcation.

1

Introduction

Real-world data are often presented as a graph where the nodes in the graph bear labels that vary
smoothly along edges. For example  for scientiﬁc publications  the content of one paper is highly
correlated with the content of papers that it references or is referenced by  the ﬁeld of interest of a
scholar is highly correlated with other scholars s/he coauthors with  etc. Many of these networks
can be described using an undirected graph with nonnegative edge weights set to be the strengths of
the connections between nodes.
The model for label prediction in this paper is the harmonic function on the Gaussian random ﬁeld
(GRF) by Zhu et al. (2003). It can generalize two popular and intuitive algorithms: label propagation
(Zhu & Ghahramani  2002)  and random walk with absorptions (Wu et al.  2012). GRFs can be
seen as a Gaussian process (GP) (Rasmussen & Williams  2006) with its (maybe improper) prior
covariance matrix whose (pseudo)inverse is set to be the graph Laplacian.
Like other learning problems  labels may be insufﬁcient and expensive to gather  especially if one
wants to discover a new phenomenon on a graph. Active learning addresses these issues by making
automated decisions on which nodes to query for labels from experts or the crowd. Some popular
criteria are empirical risk minimization (Settles  2010; Zhu et al.  2003)  mutual information gain
(Krause et al.  2008)  and V-optimality (Ji & Han  2012). Here we consider an alternative criterion 
Σ-optimality  and establish several related theoretical results. Namely  we show that greedy reduc-
tion of Σ-optimality provides a (1− 1/e) approximation bound to the global optimum. We also show

1

that Gaussian random ﬁelds satisfy the suppressor-free condition  described below. Finally  we show
that Σ-optimality outperforms other approaches for active learning with GRFs for classiﬁcation.

1.1 V-optimality on Gaussian Random Fields

Ji & Han (2012) proposed greedy variance minimization as a cheap and high proﬁle surrogate active
classiﬁcation criterion. To decide which node to query next  the active learning algorithm ﬁnds the
unlabeled node which leads to the smallest average predictive variance on all other unlabeled nodes.
It corresponds to standard V-optimality in optimal experiment design.
We will discuss several aspects of V-optimality on GRFs below: 1. The motivation behind V-
optimality can be paraphrased as the expected risk minimization with the L2-surrogate loss (Sec-
tion 2.1). 2. The greedy solution to the set optimization problem in V-optimality is comparable to
the global solution up to a constant (Theorem 1). 3. The greedy application of V-optimality can
also be interpreted as a heuristic which selects nodes that have high correlation to nodes with high
variances (Observation 4).
Some previous work is related to point 2 above. Nemhauser et al. (1978) shows that any submodular 
monotone and normalized set function yields a (1 − 1/e) global optimality guarantee for greedy
solutions. Our proof techniques coincides with Friedland & Gaubert (2011) in principle  but we
are not restricted to spectral functions. Krause et al. (2008) showed a counter example where the
V-optimality objective function with GP models does not satisfy submodularity.

1.2 Σ-optimality on Gaussian Random Fields

We deﬁne Σ-optimality on GRFs to be another variance minimization criterion that minimizes the
sum of all entries in the predictive covariance matrix. As we will show in Lemma 7  the predictive
covariance matrix is nonnegative entry-wise and thus the deﬁnition is proper. Σ-optimality was orig-
inally proposed by Garnett et al. (2012) in the context of active surveying  which is to determine the
proportion of nodes belonging to one class. However  we focus on its performance as a criterion in
active classiﬁcation heuristics. The survey-risk of Σ-optimality replaces the L2-risk of V-optimality
as an alternative surrogate risk for the 0/1-risk.
We also prove that the greedy application of Σ-optimality has a similar theoretical bound as V-
optimality. We will show that greedily minimizing Σ-optimality empirically outperforms greedily
minimizing V-optimality on classiﬁcation problems. The exact reason explaining the superiority of
Σ-optimality as a surrogate loss in the GRF model is still an open question  but we observe that
Σ-optimality tends to select cluster centers whereas V-optimality goes after outliers (Section 3.1).
Finally  greedy application of both Σ-optimality and V-optimality need O(N ) time per query candi-
date evaluation after one-time inverse of a N × N matrix.

1.3

GRFs Are Suppressor Free

In linear regression  an explanatory variable is called a suppressor if adding it as a new variable
enhances correlations between the old variables and the dependent variable (Walker  2003; Das &
Kempe  2008). Suppressors are persistent in real-world data. We show GRFs to be suppressor-
free.
Intuitively  this means that with more labels acquired  the conditional correlation between
unlabeled nodes decreases even when their Markov blanket has not formed. That GRFs present
natural examples for the otherwise obscure suppressor-free condition is interesting.

2 Learning Model & Active Learning Objectives

Laplacian of G to be L = diag (W 1) − W   i.e.  lii = (cid:80)

We use Gaussian random ﬁeld/label propagation (GRF/LP) as our learning model. Suppose the
dataset can be represented in the form of a connected undirected graph G = (V  E) where each
node has an (either known or unknown) label and each edge eij has a ﬁxed nonnegative weight
wij(= wji) that reﬂects the proximity  similarity  etc. between nodes vi and vj. Deﬁne the graph
j wij and lij = −wij when i (cid:54)= j. Let
Lδ = L + δI be the generalized Laplacian obtained by adding self-loops. In the following  we will
write L to also encompass βLδ for the set of hyper-parameters β > 0 and δ ≥ 0.

2

The binary GRF is a Bayesian model to generate yi ∈ {0  +1} for every node vi according to 

(cid:16)(cid:88)

(cid:110) − β

2

p(y) ∝ exp

(cid:88)

(cid:17)(cid:111)

(cid:18)

(cid:19)

.

wij(yi − yj)2 + δ

y2
i

= exp

− 1
2

yT Ly

i j

i

(2.1)

(cid:19)

(v−(cid:96))) 

Lu(cid:96) Lu

u ) = N (ˆyu  L−1

. By convention  L−1

Pr(yu|y(cid:96)) ∝ N (ˆyu  L−1

(cid:18) L(cid:96) L(cid:96)u

Suppose nodes (cid:96) = {v(cid:96)1  . . .   v(cid:96)|(cid:96)|} are labeled as y(cid:96) = (y(cid:96)1   . . .   y(cid:96)|(cid:96)|)T ; A GRF infers the output
distribution on unlabeled nodes  yu = (yu1  . . .   yu|u|)T by the conditional distribution given y(cid:96)  as
(2.2)
where ˆyu = (−L−1
u Lu(cid:96)y(cid:96)) is the vector of predictive means on unlabeled nodes and Lu is the
principal submatrix consisting of the unlabeled row and column indices in L  that is  the lower-right
block of L =
(v−(cid:96)) means the inverse of the principal submatrix.
We use L(v−(cid:96)) and Lu interchangeably because (cid:96) and u partition the set of all nodes v.
Finally  GRF  or GRF/LP  is a relaxation of the binary GRF to continuous outputs  because the latter is
computationally intractable even for a-priori generations. LP stands for label propagation  because
the predictive mean on a node is the probability of a random walk leaving that node hitting a positive
label before hitting a zero label. For multi-class problems  Zhu et al. (2003) proposed the harmonic
predictor which looks at predictive means in one-versus-all comparisons.
Remark: An alternative approximation to the binary GRF is the GRF-sigmoid model  which draws
the binary outputs from Bernoulli distributions with means set to be the sigmoid function of the GRF
(latent) variables. However  this alternative is very slow to compute and may not be compatible with
the theoretical results in this paper.

2.1 Active Learning Objective 1: L2 Risk Minimization (V-Optimality)

Since in GRFs  regression responses are taken directly as probability predictions  it is computation-
ally and analytically more convenient to apply the regression loss directly in the GRF as in Ji & Han
(2012). Assume the L2 loss to be our classiﬁcation loss. The risk function  whose input variable is
the labeled subset (cid:96)  is:

(yui − ˆyui)2 = E

E

(yui − ˆyui)2

= tr(L−1
u ).

(2.3)

RV ((cid:96)) = Ey(cid:96)yu (cid:88)

ui∈u

(cid:34)

(cid:34)(cid:88)

ui∈u

(cid:35)(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)y(cid:96)

This risk is written with a subscript V because minimizing (2.3) is also the V-optimality criterion 
which minimizes mean prediction variance in active learning.
In active learning  we strive to select a subset (cid:96) of nodes to query for labels  constrained by a given
budget C  such that the risk is minimized. Formally 

arg min
(cid:96): |(cid:96)|≤C

R((cid:96)) = RV ((cid:96)) = tr(L−1

(v−(cid:96))).

(2.4)

2.2 Active Learning Objective 2: Survey Risk Minimization (Σ-Optimality)

Another objective building on the GRF model (2.2) is to determine the proportion of nodes belonging
to class 1  as would happen when performing a survey. For active surveying  the risk would be:

(cid:1)2

= E(cid:2)E(cid:2)(cid:0)1T yu − 1T ˆyu

(cid:1)2|y(cid:96)

(cid:3)(cid:3) = 1T L−1

(2.5)

RΣ((cid:96)) = Ey(cid:96)yu(cid:0) (cid:88)

yui − (cid:88)

ˆyui

u 1 

ui∈u

ui∈u

which could substitute the risk R((cid:96)) in (2.4) and yield another heuristic for selecting nodes in batch
active learning. We will refer to this modiﬁed optimization objective as the Σ-optimality heuristic:
(2.6)

R((cid:96)) = RΣ((cid:96)) = 1T L−1

(v−(cid:96))1.

arg min
(cid:96): |(cid:96)|≤C

Further  we will also consider the application of Σ-optimality in active classiﬁcation because (2.6) is
another metric of the predictive variance. Surprisingly  although both (2.3) and (2.5) are approxima-
tions of the real objective (the 0/1 risk)  greedy reduction of the Σ-optimality criterion outperforms
greedy reduction of the V-optimality criterion in active classiﬁcation (Section 3.1 and 5.1)  as well
as several other methods including expected error reduction.

3

2.3 Greedy Sequential Application of V/Σ-Optimality

Both (2.4) and (2.6) are subset optimization problems. Calculating the global optimum may be
intractable. As will be shown later in the theoretical results  the reduction of both risks are submod-
ular set functions and the greedy sequential update algorithm yields a solution that has a guaranteed
approximation ratio to the optimum (Theorem 1).
At the k-th query decision  denote the covariance matrix conditioned on the previous (k− 1) queries
as C = (L(v−(cid:96)(k−1)))−1. By Shur’s Lemma (or the GP-regression update rule)  the one-step look-
ahead covariance matrix conditioned on (cid:96)(k−1) ∪ {v}  denoted as C(cid:48) = (L(v−((cid:96)(k−1)∪{v})))−1  has

the following update formula: (cid:18)C(cid:48)

(cid:19)

0
0

0

= C − 1
Cvv

· C:vCv: 

(2.7)

where without loss of generality v was positioned as the last node. Further denoting Cij = ρijσiσj 
we can put (2.7) inside RΣ(·) and RV (·) to get the following equivalent criteria:

V-optimality : v(k)∗ = arg max

v∈u

Σ-optimality : v(k)∗ = arg max

v∈u

3 Theoretical Results & Insights

(cid:80)
((cid:80)

t∈u(Cvt)2

Cvv
t∈u Cvt)2
Cvv

(cid:88)
(cid:88)

t∈u

t∈u

=

vtσ2
ρ2
t  

= (

ρvtσt)2.

(2.8)

(2.9)

For the general GP model  greedy optimization of the L2 risk has no guarantee that the solution
can be comparable to the brute-force global optimum (taking exponential time to compute)  because
the objective function  the trace of the predictive covariance matrix  fails to satisfy submodularity
in all cases (Krause et al.  2008). However  in the special case of GPs with kernel matrix equal to
the inverse of a graph Laplacian (with (cid:96) (cid:54)= ∅ or δ > 0)  the GRF does provide such theoretical
guarantees  both for V-optimality and Σ-optimality. The latter is a novel result.
The following theoretical results concern greedy maximization of the risk reduction function (which
is shown to be submodular): R∆((cid:96)) = R(∅) − R((cid:96)) for either R(·) = RV (·) or RΣ(·).
Theorem 1 (Near-optimal guarantee for greedy applications of V/Σ-optimality). In risk reduction 
(3.1)
where R∆((cid:96)) = R(∅) − R((cid:96)) for either R(·) = RV (·) or RΣ(·)  e is Euler’s number  (cid:96)g is the
greedy optimizer  and (cid:96)∗ is the true global optimizer under the constraint |(cid:96)∗| ≤ |(cid:96)g|.
According to Nemhauser et al. (1978)  it sufﬁces to show the following properties of R∆((cid:96)):
Lemma 2 (Normalization  Monotonicity  and Submodularity). ∀(cid:96)1 ⊂ (cid:96)2 ⊂ v  v ∈ v 

R∆((cid:96)g) ≥ (1 − 1/e) · R∆((cid:96)∗) 

R∆(∅) = 0 
R∆((cid:96)2) ≥ R∆((cid:96)1) 

(cid:0)(cid:96)1 ∪ {v}(cid:1) − R∆((cid:96)1) ≥ R∆

(cid:0)(cid:96)2 ∪ {v}(cid:1) − R∆((cid:96)2).

R∆

(3.2)
(3.3)
(3.4)

Another sufﬁcient condition for Theorem 1  which is itself an interesting observation  is the
suppressor-free condition. Walker (2003) describes a suppressor as a variable  knowing which will
suddenly create a strong correlation between the predictors. An example is yi + yj = yk. Knowing
any one of these will create correlations between the others. Walker further states that suppressors
are common in regression problems. Das & Kempe (2008) extend the suppressor-free condition to
sets and showed that this condition is sufﬁcient to prove (2.3). Formally  the condition is:

(cid:12)(cid:12)corr(yi  yj | (cid:96)1 ∪ (cid:96)2)(cid:12)(cid:12) ≤(cid:12)(cid:12)corr(yi  yj | (cid:96)1)(cid:12)(cid:12)

(3.5)
It may be easier to understand (3.5) as a decreasing correlation property.
It is well known for
Markov random ﬁelds that the labels of two nodes on a graph become independent given labels of
their Markov blanket. Here we establish that GRF boasts more than that: the correlation between any
two nodes decreases as more nodes get labeled  even before a Markov blanket is formed. Formally:

∀vi  vj ∈ v ∀(cid:96)1  (cid:96)2 ⊂ v.

4

Theorem 3 (Suppressor-Free Condition). (3.5) holds for pairs of nodes in the GRF model. Note
that since the conditional covariance of the GRF model is L−1
(v−(cid:96))  we can properly deﬁne the corre-
sponding conditional correlation to be
corr(yu|(cid:96)) = D− 1

2   with D = diag

(3.6)

2 L−1

(v−(cid:96))D− 1

(cid:16)

L−1
(v−(cid:96))

.

(cid:17)

3.1

Insights From Comparing the Greedy Applications of the Σ/V-Optimality Criteria

Both the V/Σ-optimality are approximations to the 0/1 risk minimization objective. Unfortunately 
we cannot theoretically reason why greedy Σ-optimality outperforms V-optimality in the experi-
ments. However  we made two observations during our investigation that provide some insights. An
illustrative toy example is also provided in Section 5.1.
Observation 4. Eq. (2.8) and (2.9) suggest that both the greedy Σ/V-optimality selects nodes that
(1) have high variance and (2) are highly correlated to high-variance nodes  conditioned on the
labeled nodes. Notice Lemma 7 proves that predictive correlations are always nonnegative.

In order to contrast Σ/V-optimality  rewrite (2.9) as:

((cid:80)
t∈u ρvtσt)2 =(cid:80)

t +(cid:80)

(Σ-optimality) : arg max

v∈u

t∈u ρ2

vtσ2

t1(cid:54)=t2∈u ρvt1ρvt2σt1 σt2 .

(3.7)

Observation 5. Σ-optimality has one more term that involves cross products of (ρvt1σt1) and
(ρvt2 σt2) (which are nonnegative according to Lemma 9). By the Cauchy–Schwartz Inequality 
the sum of these cross products are maximized when they are equal. So  the Σ-optimality addition-
ally favors nodes that (3) have consistent global inﬂuence  i.e.  that are more likely to be in cluster
centers.

4 Proof Sketches

Our results predicate on and extend to GPs whose inverse covariance matrix meets Proposition 6.
Proposition 6. L satisﬁes the following. 1

#

Textual description
p6.1 L has proper signs.
p6.2 L is undirected and connected.
p6.3 Node degree no less than number of edges.
p6.4 L is nonsingular and positive-deﬁnite.

Mathematical expression
lij ≥ 0 if i = j and lij ≤ 0 if i (cid:54)= j.
j(cid:54)=i(−lij) > 0.

lij = lji∀i  j and(cid:80)
lii ≥(cid:80)
j(cid:54)=i(−lij) =(cid:80)
∃i : lii >(cid:80)
j(cid:54)=i(−lij) =(cid:80)

j(cid:54)=i(−lji) > 0 ∀i.
j(cid:54)=i(−lji) > 0.

Although the properties of V-optimality fall into the more general class of spectral functions (Fried-
land & Gaubert  2011)  we have seen no proof of either the suppressor-free condition or the submod-
ularity of Σ-optimality on GRFs. We write the ideas behind the proofs. Details are in the appendix.2
Lemma 7. For any L satisfying (p6.1-4)  L−1 ≥ 0 entry-wise.3

Proof. Sketch: Suppose L = D − W = D(I − D−1W )  with D = diag (L). Then we can show
the convergence of the Taylor expansion (Appendix A.1):

L−1 = [I +(cid:80)∞

r=1(D−1W )r]D−1.

(4.1)

It sufﬁces to observe that every term on the right hand side (RHS) is nonnegative.
Corollary 8. The GRF prediction operator L−1
[0  1]|u|. When L is singular  the mapping is onto.

u Lul maps y(cid:96) ∈ [0  1]|(cid:96)| to ˆyu = −L−1

u Luly(cid:96) ∈

1Property p6.4 holds after the ﬁrst query is done or when the regularizor δ > 0 in (2.1).
2Available at http://www.autonlab.org/autonweb/21763.html
3In the following  for any vector or matrix A  A ≥ 0 always stands for A being (entry-wise) nonnegative.

5

Proof. For y(cid:96) = 1  (Lu  Lul) · 1 ≥ 0 and L−1
−L−1
As both Lu ≥ 0 and −Lul ≥ 0  we have y(cid:96) ≥ 0 ⇒ ˆyu ≥ 0 and y(cid:96) ≥ y(cid:48)

u Lul1 = ˆyu.

u Lul

(cid:96) ⇒ ˆyu ≥ ˆy(cid:48)
u.

(cid:1) · 1 ≥ 0  i.e. 1 ≥

u ≥ 0 imply (cid:0)I  L−1
(cid:18)L−1
(cid:19)
11 (−L12)

(cid:19)

11
0

0
0

. Then L−1 −

(cid:18)L11 L12
(cid:18)L−1
(cid:19)

L21 L22

(cid:19)
(cid:18)L−1

11
0

0
0

I

Lemma 9. Suppose L =

≥ 0 and is positive-semideﬁnite.

Proof. As L−1 ≥ 0 and is PSD  the RHS below is term-wise nonnegative and the middle term PSD
(Appendix A.2): L−1−

11 L12)−1(cid:0)(−L21)L−1
11   I(cid:1)

(L22−L21L−1

=

(cid:18)Au bu

(cid:19)

bT
u

cu

L(v−(cid:96)) :=

As a corollary  the monotonicity in (3.3) for both R(·) = RV (·) or RΣ(·) can be shown.
Both proofs for submodularity in (3.4) and Theorem 3 result from more careful execution of matrix
inversions similar to Lemma 9 (detailed in Appendix A.4). We sketch Theorem 3 for example.
Proof. Without loss of generality  let u = v − (cid:96) = {1  . . .   k}. By Shur’s Lemma (Appendix A.3):

⇒ Cov(yi  yk|(cid:96))
Var(yk|(cid:96))

=

(L−1
(L−1

(v−(cid:96)))ik
(v−(cid:96)))kk

= (A−1

u (−bu))i ∀i (cid:54)= k

(4.2)

≥ A−1

where the LHS is a reparamatrization with cu being a scaler. Lemma 9 shows that u1 ⊃ u2 ⇒
u2 at corresponding entries. Also notice that −bu1 ≥ −bu2 at corresponding entries and
A−1
so the RHS of (4.2) is larger with u1. It sufﬁces to draw a similar inequality in the other direction 
Cov(yk  yi|(cid:96))/ Var(yi|(cid:96)).

u1

5 A Toy Example and Some Simulations

5.1 Comparing V-Optimality and Σ-Optimality: Active Node Classiﬁcation on a Graph

To visualize the intuitions described in Sec-
tion 3.1  Figure 1 shows the ﬁrst few nodes
selected by different optimality criteria. This
graph is constructed by a breadth-ﬁrst search
from a random node in a larger DBLP coau-
thorship network graph that we will introduce
in the next section. On this toy graph  both cri-
teria pick the same center node to query ﬁrst.
However  for the second and third queries  V-
optimality weighs the uncertainty of the can-
didate node more  choosing outliers  whereas
Σ-optimality favors nodes with universal inﬂu-
ence over the graph and goes to cluster centers.

5.2 Simulating Labels on a Graph

Figure 1: Toy graph demonstrating the behavior
of Σ-optimality vs. V-optimality.

To further investigate the behavior of Σ- and V -
optimality  we conducted experiments on syn-
thetic labels generated on real-world network graphs. The node labels were ﬁrst simulated using the
model in order to compare the active learning criteria directly without raising questions of model ﬁt.
We carry out tests on the same graphs with real data in the next section.
We simulated the binary labels with the GRF-sigmoid model and performed active learning with
the GRF/LP model for predictions. The parameters in the generation phase were β = 0.01 and
δ = 0.05  which maximizes the average classiﬁcation accuracy increases from 50 random training
nodes to 200 random training nodes using the GRF/LP model for predictions. Figure 2 shows the
binary classiﬁcation accuracy versus the number of queries on both the DBLP coauthorship graph

6

 class 1 class 2 class 3 Σ−optimality V−optimality(a) DBLP coauthorship. 68.3% LOO accuracy.

(b) CORA citation. 60.5% LOO accuracy.

Figure 2: Simulating binary labels by the GRF-Sigmoid; learning with the GRF/LP  480 repetitions.

and the CORA citation graph that we will describe below. The best possible classiﬁcation results are
indicated by the leave-one-out (LOO) accuracies given under each plot.
Figure 2 can be a surprise due to the reasoning behind the L2 surrogate loss  especially when the
predictive means are trapped between [−1  1]  but we see here that our reasoning in Sections (3.1
and 5.1) can lead to the greedy survey loss actually making a better active learning objective.
We have also performed experiments with different values of β and δ. Despite the fact that larger
β and δ increase label independence on the graph structure and undermine the effectiveness of both
V/Σ-optimality heuristics  we have seen that whenever the V-optimality establishes a superiority
over random selections  Σ-optimality yields better performance.

6 Real-World Experiments

The active learning heuristics to be compared are:4

1. The new Σ-optimality with greedy sequential updates: minv(cid:48)(cid:0)1(cid:62)(Luk\{v(cid:48)})−11(cid:1).
2. Greedy V-optimality (Ji & Han  2012): minv(cid:48) tr(cid:0)(Luk\{v(cid:48)})−1(cid:1) .
3. Mutual information gain (MIG) (Krause et al.  2008): maxv(cid:48)(cid:0)L−1
(cid:104)(cid:16)(cid:80)

4. Uncertainty sampling (US) picking the largest prediction margin: maxv(cid:48) ˆy(1)
5. Expected error reduction (EER) (Settles  2010; Zhu et al.  2003). Selected nodes maximize

(cid:14)(cid:0)(L(cid:96)k∪{v(cid:48)})−1(cid:1)
(cid:17)(cid:12)(cid:12)(cid:12)y(cid:96)k
(cid:12)(cid:12)(cid:12)yv(cid:48)
(cid:105)
v(cid:48) − ˆy(2)
v(cid:48) .

the average prediction conﬁdence in expectation: maxv(cid:48) Eyv(cid:48)

ui∈u ˆy(1)

ui

.

(cid:1)

uk

v(cid:48) v(cid:48)

v(cid:48) v(cid:48)

6. Random selection with 12 repetitions.

Comparisons are made on three real-world network graphs.

1. DBLP coauthorship network.5 The nodes represent scholars and the weighted edges are the
number of papers bearing both scholars’ names. The largest connected component has 1711
nodes and 2898 edges. The node labels were hand assigned in Ji & Han (2012) to one of the
four expertise areas of the scholars: machine learning  data mining  information retrieval  and
databases. Each class has around 400 nodes.

2. Cora citation network.6 This is a citation graph of 2708 publications  each of which is classiﬁed
into one of seven classes: case based  genetic algorithms  neural networks  probabilistic methods 
reinforcement learning  rule learning  and theory. The network has 5429 links. We took its
largest connected component  with 2485 nodes and 5069 undirected and unweighted edges.
4Code available at http://www.autonlab.org/autonweb/21763
5http://www.informatik.uni-trier.de/˜ley/db/
6http://www.cs.umd.edu/projects/linqs/projects/lbc/index.html

7

0501001502000.50.520.540.560.580.60.620.640.660.68number of queriesclassification accuracy Σ−optimalityV−optimalityRandom0501001502000.50.520.540.560.580.6number of queriesclassification accuracy Σ−optimalityV−optimalityRandom(a) DBLP. 84% LOO accuracy.

(b) CORA. 86& LOO accuracy.

(c) CITESEER 76% LOO accuracy.

Figure 3: Classiﬁcation accuracy vs the number of queries. β = 1  δ = 0. Randomized ﬁrst query.

3. CiteSeer citation network.6 This is another citation graph of 3312 publications  each of which
is classiﬁed into one of six classes: agents  artiﬁcial intelligence  databases  information retrieval 
machine learning  human computer interaction. The network has 4732 links. We took its largest
connected component  with 2109 nodes and 3665 undirected and unweighted edges.

On all three datasets  Σ-optimality outperforms other methods by a large margin especially during
the ﬁrst ﬁve to ten queries. The runner-up  EER  catches up to Σ-optimality in some cases  but EER
does not have theoretical guarantees.
The win of Σ-optimality over V-optimality has been intuitively explained in Section 5.1 as Σ-
optimality having better exploration ability and robustness against outliers. The node choices by
both criteria were also visually inspected after embedding the graph to the 2-dimensional space us-
ing OpenOrd method developed by Martin et al. (2011). The analysis there was similar to Figure 1.
We also performed real-world experiments on the root-mean-square-error of the class proportion es-
timations  which is the survey risk that the Σ-optimality minimizes. Σ-optimality beats V-optimality.
Details were omitted for space concerns.

7 Conclusion

For active learning on GRFs  it is common to use variance minimization criteria with greedy one-
step lookahead heuristics. V-optimality and Σ-optimality are two criteria based on statistics of the
predictive covariance matrix. They both are also risk minimization criteria: V-optimality minimizes
the L2 risk (2.3)  whereas Σ-optimality minimizes the survey risk (2.5).
Active learning with both criteria can be seen as subset optimization problems (2.4)  (2.6). Both
objective functions are supermodular set functions. Therefore  risk reduction is submodular and the
greedy one-step lookahead heuristics can achieve a (1 − 1/e) global optimality ratio. Moreover  we
have shown that GRFs serve as a tangible example of the suppressor-free condition.
While the V-optimality on GRFs inherits from label propagation (and random walk with absorptions)
and have good empirical performance  it is not directly minimizing the 0/1 classiﬁcation risk. We
found that the Σ-optimality performs even better. The intuition is described in Section 5.1.
Future work include deeper understanding of the direct motivations behind Σ-optimality on the GRF
classiﬁcation model and extending the GRF to continuous spaces.

Acknowledgments

This work is funded in part by NSF grant IIS0911032 and DARPA grant FA87501220324.

8

010203040500.20.250.30.350.40.450.50.550.60.65 Σ−optV−optRandMIGUncEER010203040500.10.20.30.40.50.60.70.8 Σ−optV−optRandMIGUncEER010203040500.20.30.40.50.60.7 Σ−optV−optRandMIGUncEERReferences

Das  Abhimanyu and Kempe  David. Algorithms for subset selection in linear regression. In Pro-

ceedings of the 40th annual ACM symposium on Theory of computing  pp. 45–54. ACM  2008.

Friedland  S and Gaubert  S. Submodular spectral functions of principal submatrices of a hermitian

matrix  extensions and applications. Linear Algebra and its Applications  2011.

Garnett  Roman  Krishnamurthy  Yamuna  Xiong  Xuehan  Schneider  Jeff  and Mann  Richard.

Bayesian optimal active search and surveying. In ICML  2012.

Ji  Ming and Han  Jiawei. A variance minimization criterion to active learning on graphs. In AISTAT 

2012.

Krause  Andreas  Singh  Ajit  and Guestrin  Carlos. Near-optimal sensor placements in gaussian
processes: Theory  efﬁcient algorithms and empirical studies. Journal of Machine Learning Re-
search (JMLR)  9:235–284  February 2008.

Martin  Shawn  Brown  W Michael  Klavans  Richard  and Boyack  Kevin W. Openord: an open-
source toolbox for large graph layout. In IS&T/SPIE Electronic Imaging  pp. 786806–786806.
International Society for Optics and Photonics  2011.

Nemhauser  George L  Wolsey  Laurence A  and Fisher  Marshall L. An analysis of approximations

for maximizing submodular set functionsi. Mathematical Programming  14(1):265–294  1978.

Rasmussen  Carl Edward and Williams  Christopher KI. Gaussian processes for machine learning 

volume 1. MIT press Cambridge  MA  2006.

Settles  Burr. Active learning literature survey. University of Wisconsin  Madison  2010.
Walker  David A. Suppressor variable (s) importance within a regression model: an example of
salary compression from career services. Journal of College Student Development  44(1):127–
133  2003.

Wu  Xiao-Ming  Li  Zhenguo  So  Anthony Man-Cho  Wright  John  and Chang  Shih-Fu. Learning
with partially absorbing random walks. In Advances in Neural Information Processing Systems
25  pp. 3086–3094  2012.

Zhu  Xiaojin and Ghahramani  Zoubin. Learning from labeled and unlabeled data with label prop-
agation. Technical report  Technical Report CMU-CALD-02-107  Carnegie Mellon University 
2002.

Zhu  Xiaojin  Lafferty  John  and Ghahramani  Zoubin. Combining active learning and semi-
supervised learning using gaussian ﬁelds and harmonic functions. In ICML 2003 workshop on The
Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining  pp. 58–65 
2003.

9

,Yifei Ma
Roman Garnett
Jeff Schneider
Abhishek Sharma
Oncel Tuzel
Ming-Yu Liu
Pinar Yanardag
S.V.N. Vishwanathan