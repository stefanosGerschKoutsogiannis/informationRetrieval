2016,Exponential expressivity in deep neural networks through transient chaos,We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in deep neural networks with random weights. Our results reveal a phase transition in the expressivity of random deep networks  with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth  but not with width. We prove that this generic class of random functions cannot be efficiently computed by any shallow network  going beyond prior work that restricts their analysis to single functions. Moreover  we formally quantify and demonstrate the long conjectured idea that deep networks can disentangle exponentially curved manifolds in input space into flat manifolds in hidden space.  Our theoretical framework for analyzing the expressive power of deep networks is broadly applicable and provides a basis for quantifying previously abstract notions about the geometry of deep functions.,Exponential expressivity in deep neural networks

through transient chaos

Ben Poole1  Subhaneil Lahiri1  Maithra Raghu2  Jascha Sohl-Dickstein2  Surya Ganguli1

{benpoole sulahiri sganguli}@stanford.edu  {maithra jaschasd}@google.com

1Stanford University  2Google Brain

Abstract

We combine Riemannian geometry with the mean ﬁeld theory of high dimensional
chaos to study the nature of signal propagation in generic  deep neural networks
with random weights. Our results reveal an order-to-chaos expressivity phase
transition  with networks in the chaotic phase computing nonlinear functions whose
global curvature grows exponentially with depth but not width. We prove this
generic class of deep random functions cannot be efﬁciently computed by any shal-
low network  going beyond prior work restricted to the analysis of single functions.
Moreover  we formalize and quantitatively demonstrate the long conjectured idea
that deep networks can disentangle highly curved manifolds in input space into ﬂat
manifolds in hidden space. Our theoretical analysis of the expressive power of deep
networks broadly applies to arbitrary nonlinearities  and provides a quantitative
underpinning for previously abstract notions about the geometry of deep functions.

1

Introduction

Deep feedforward neural networks have achieved remarkable performance across many domains
[1–6]. A key factor thought to underlie their success is their high expressivity. This informal notion
has manifested itself primarily in two forms of intuition. The ﬁrst is that deep networks can compactly
express highly complex functions over input space in a way that shallow networks with one hidden
layer and the same number of neurons cannot. The second piece of intuition  which has captured
the imagination of machine learning [7] and neuroscience [8] alike  is that deep neural networks can
disentangle highly curved manifolds in input space into ﬂattened manifolds in hidden space. These
intuitions  while attractive  have been difﬁcult to formalize mathematically and thus test rigorously.
For the ﬁrst intuition  seminal works have exhibited examples of particular functions that can be
computed with a polynomial number of neurons (in the input dimension) in a deep network but
require an exponential number of neurons in a shallow network [9–13]. This raises a central open
question: are such functions merely rare curiosities  or is any function computed by a generic deep
network not efﬁciently computable by a shallow network? The theoretical techniques employed in
prior work both limited the applicability of theory to speciﬁc nonlinearities and dictated the particular
measure of deep functional complexity involved. For example  [9] focused on ReLU nonlinearities
and number of linear regions as a complexity measure  while [10] focused on sum-product networks
and the number of monomials as complexity measure  and [14] focused on Pfafﬁan nonlinearities and
topological measures of complexity  like the sum of Betti numbers of a decision boundary (however 
see [15] for an interesting analysis of a general class of compositional functions). The limits of
prior theoretical techniques raise another central question: is there a unifying theoretical framework
for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities  generic
networks  and a natural  general measure of functional complexity?

Code to reproduce all results available at: https://github.com/ganguli-lab/deepchaos

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Here we attack both central problems of deep neural expressivity by combining Riemannian geometry
[16] and dynamical mean ﬁeld theory [17]. This novel combination of tools enables us to show that
for very broad classes of nonlinearities  even random deep neural networks can construct hidden
internal representations whose global extrinsic curvature grows exponentially with depth but not width.
Our geometric framework enables us to quantitatively deﬁne a notion of disentangling and verify
this notion in deep random networks. Furthermore  our methods yield insights into the emergent 
deterministic nature of signal propagation through large random feedforward networks  revealing the
existence of an order to chaos transition as a function of the statistics of weights and biases. We ﬁnd
that the transient  ﬁnite depth evolution in the chaotic regime underlies the origins of exponential
expressivity in deep random networks. In a companion paper [18]  we study several related measures
of expressivity in deep random neural networks with piecewise linear activations.

2 A mean ﬁeld theory of deep nonlinear signal propagation

Consider a deep feedforward network with D layers of weights W1  . . .   WD and D + 1 layers of
neural activity vectors x0  . . .   xD  with Nl neurons in each layer l  so that xl ∈ RNl and Wl is an
Nl × Nl−1 weight matrix. The feedforward dynamics elicited by an input x0 is given by

hl = Wl xl−1 + bl

xl = φ(hl)

for l = 1  . . .   D 

ij are drawn i.i.d. from a zero mean Gaussian with variance σ2

(1)
where bl is a vector of biases  hl is the pattern of inputs to neurons at layer l  and φ is a single
neuron scalar nonlinearity that acts component-wise to transform inputs hl to activities xl. We
wish to understand the nature of typical functions computable by such networks  as a consequence
of their depth. We therefore study ensembles of random networks in which each of the synaptic
weights Wl
w/Nl−1  while the biases
b . This weight scaling ensures that the
are drawn i.i.d. from a zero mean Gaussian with variance σ2
input contribution to each individual neuron at layer l from activities in layer l − 1 remains O(1) 
independent of the layer width Nl−1. This ensemble constitutes a maximum entropy distribution over
deep neural networks  subject to constraints on the means and variances of weights and biases. This
ensemble induces no further structure in the resulting set of deep functions  so its analysis provides
an opportunity to understand the speciﬁc contribution of depth alone to the nature of typical functions
computed by deep networks.
In the limit of large layer widths  Nl (cid:29) 1  certain aspects of signal propagation through deep
random neural networks take on an essentially deterministic character. This emergent determinism
in large random neural networks enables us to understand how the Riemannian geometry of simple
manifolds in the input layer x0 is typically modiﬁed as the manifold propagates into the deep layers.
For example  consider the simplest case of a single input vector x0. As it propagates through the
network  its length in downstream layers will change. We track this changing length by computing
the normalized squared length of the input vector at each layer:

ql =

1
Nl

(hl

i)2.

(2)

Nl(cid:88)

i=1

(cid:16)(cid:112)

(cid:90)

(cid:17)2

i =(cid:80)

This length is the second moment of the empirical distribution of inputs hl
i across all Nl neurons
in layer l. For large Nl  this empirical distribution converges to a zero mean Gaussian since each
ijφ(hl−1
i is a weighted sum of a large number of uncorrelated random variables
hl
- i.e. the weights Wl
i  which are independent of the activity in previous layers. By
propagating this Gaussian distribution across one layer  we obtain an iterative map for ql in (2):

) + bl
ij and biases bl

j Wl

j

ql = V(ql−1 | σw  σb) ≡ σ2

w

Dz φ

ql−1z

+ σ2
b  

for

l = 2  . . .   D 

(3)

2π

2 is the standard Gaussian measure  and the initial condition is q1 = σ2

where Dz = dz√
e− z2
b  
wq0 + σ2
x0 · x0 is the length in the initial activity layer. See Supplementary Material (SM)
where q0 = 1
N0
for a derivation of (3). Intuitively  the integral over z in (3) replaces an average over the empirical
distribution of hl
The function V in (3) is an iterative variance  or length  map that predicts how the length of an input in
(2) changes as it propagates through the network. This length map is plotted in Fig. 1A for the special

i across neurons i in layer l at large layer width Nl.

2

Figure 1: Dynamics of the squared length ql for a sigmoidal network (φ(h) = tanh(h)) with 1000
hidden units. (A) The iterative length map in (3) for 3 different σw at σb = 0.3. Theoretical
predictions (solid lines) match well with individual network simulations (dots). Stars reﬂect ﬁxed
points q∗ of the map. (B) The iterative dynamics of the length map yields rapid convergence of ql
to its ﬁxed point q∗   independent of initial condition (lines=theory; dots=simulation). (C) q∗ as a
function of σw and σb. (D) Number of iterations required to achieve ≤ 1% fractional deviation off
the ﬁxed point. The (σb  σw) pairs in (A B) are marked with color matched circles in (C D).

case of a sigmoidal nonlinearity  φ(h) = tanh(h). For monotonic nonlinearities  this length map is
a monotonically increasing  concave function whose intersections with the unity line determine its
ﬁxed points q∗(σw  σb). For σb = 0 and σw < 1  the only intersection is at q∗ = 0. In this bias-free 
small weight regime  the network shrinks all inputs to the origin. For σw > 1 and σb = 0  the q∗ = 0
ﬁxed point becomes unstable and the length map acquires a second nonzero ﬁxed point  which is
stable. In this bias-free  large weight regime  the network expands small inputs and contracts large
inputs. Also  for any nonzero bias σb  the length map has a single stable non-zero ﬁxed point. In such
a regime  even with small weights  the injected biases at each layer prevent signals from decaying to
0. The dynamics of the length map leads to rapid convergence of length to its ﬁxed point with depth
(Fig. 1B D)  often within only 4 layers. The ﬁxed points q∗(σw  σb) are shown in Fig. 1C.

3 Transient chaos in deep networks

Now consider the layer-wise propagation of two inputs x0 1 and x0 2. The geometry of these two
inputs as they propagate through the network is captured by the 2 by 2 matrix of inner products:

Nl(cid:88)

i=1

ql
ab =

1
Nl

hl
i(x0 a) hl

i(x0 b)

a  b ∈ {1  2}.

(4)

(cid:113)
12   ql−1
11   ql−1
ql−1
11 z1 

(cid:113)
22 | σw  σb) ≡ σ2

(cid:20)

(cid:113)

(cid:21)

The dynamics of the two diagonal terms are each theoretically predicted by the length map in (3). We
derive (see SM) a correlation map C that predicts the layer-wise dynamics of ql
12:
Dz1 Dz2 φ (u1) φ (u2) + σ2
b  

12 = C(cl−1
ql

(5)

w

(cid:90)

ql−1

22

cl−1
12 z1 +

1 − (cl−1

 

12 )2z2

11ql

u2 =

12(ql

u1 =
22)−1/2 is the correlation coefﬁcient. Here z1 and z2 are independent standard
where cl
12 = ql
Gaussian variables  while u1 and u2 are correlated Gaussian variables with covariance matrix
(cid:104)uaub(cid:105) = ql−1
ab . Together  (3) and (5) constitute a theoretical prediction for the typical evolution of
the geometry of 2 points in (4) in a ﬁxed large network.
Analysis of these equations reveals an interesting order to chaos transition in the σw and σb plane. In
particular  what happens to two nearby points as they propagate through the layers? Their relation to
12 between the two points  which approaches
each other can be tracked by the correlation coefﬁcient cl
a ﬁxed point c∗(σw  σb) at large depth. Since the length of each point rapidly converges to q∗(σw  σb) 
22 = q∗(σw  σb) in (5) and
as shown in Fig. 1BD  we can compute c∗ by simply setting ql
dividing by q∗ to obtain an iterative correlation coefﬁcient map  or C-map  for cl
12:

11 = ql

cl
12 =

1

q∗C(cl−1

12   q∗  q∗ | σw  σb).

(6)

3

This C-map is shown in Fig. 2A. It always has a ﬁxed point at c∗ = 1 as can be checked by direct
calculation. However  the stability of this ﬁxed point depends on the slope of the map at 1  which is

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)c=1

(cid:90)

Dz(cid:2)φ(cid:48)(cid:0)√

q∗z(cid:1)(cid:3)2

χ1 ≡ ∂cl
∂cl−1

12

12

= σ2
w

.

(7)

See SM for a derivation of (7). If the slope χ1 is less than 1  then the C-map is above the unity line 
the ﬁxed point at 1 under the C-map in (6) is stable  and nearby points become more similar over time.

12  in a sigmoidal network with φ(h) = tanh(h). (A) The
Figure 2: Dynamics of correlations  cl
C-map in (6) for the same σw and σb = 0.3 as in Fig. 1A. (B) The C-map dynamics  derived from
both theory  through (6) (solid lines) and numerical simulations of (1) with Nl = 1000 (dots) (C)
Fixed points c∗ of the C-map. (D) The slope of the C-map at 1  χ1  partitions the space (black dotted
line at χ1 = 1) into chaotic (χ1 > 1  c∗ < 1) and ordered (χ1 < 1  c∗ = 1) regions.

ij = Wl

j

i

2/||u||2

ijφ(cid:48)(hl−1

) at a point hl−1

Conversely  if χ1 > 1 then this ﬁxed point is unstable  and nearby points separate as they propagate
through the layers. Thus we can intuitively understand χ1 as a multiplicative stretch factor. This
intuition can be made precise by considering the Jacobian Jl
j with
length q∗. Jl is a linear approximation of the network map from layer l− 1 to l in the vicinity of hl−1.
Therefore a small random perturbation hl−1 + u will map to hl + Ju. The growth of the perturbation 
||Ju||2
2 becomes χ1(q∗) after averaging over the random perturbation u  weight matrix Wl 
and Gaussian distribution of hl−1
across i. Thus χ1 directly reﬂects the typical multiplicative growth
or shrinkage of a random perturbation across one layer.
The dynamics of the iterative C-map and its agreement with network simulations is shown in Fig.
2B. The correlation dynamics are much slower than the length dynamics because the C-map is closer
to the unity line (Fig. 2A) than the length map (Fig. 1A). Thus correlations typically take about 20
layers to approach the ﬁxed point  while lengths need only 4. The ﬁxed point c∗ and slope χ1 of
the C-map are shown in Fig. 2CD. For any ﬁxed  ﬁnite σb  as σw increases three qualitative regions
occur. For small σw  c∗ = 1 is the only ﬁxed point  and it is stable because χ1 < 1. In this strong
bias regime  any two input points converge to each other as they propagate through the network. As
σw increases  χ1 increases and crosses 1  destabilizing the c∗ = 1 ﬁxed point. In this intermediate
regime  a new stable ﬁxed point c∗ appears  which decreases as σw increases. Here an equal footing
competition between weights and nonlinearities (which de-correlate inputs) and the biases (which
correlate them)  leads to a ﬁnite c∗. At larger σw  the strong weights overwhelm the biases and
maximally de-correlate inputs to make them orthogonal  leading to a stable ﬁxed point at c∗ = 0.
Thus the equation χ1(σw  σb) = 1 yields a phase transition boundary in the (σw  σb) plane  separating
it into a chaotic (or ordered) phase  in which nearby points separate (or converge). In dynamical
systems theory  the logarithm of χ1 is related to the well known Lyapunov exponent which is positive
(or negative) for chaotic (or ordered) dynamics. However  in a feedforward network  the dynamics is
truncated at a ﬁnite depth D  and hence the dynamics are a form of transient chaos.

4 The propagation of manifold geometry through deep networks

Now consider a 1 dimensional manifold x0(θ) in input space  where θ is an intrinsic scalar coordinate
on the manifold. This manifold propagates to a new manifold hl(θ) = hl(x0(θ)) in the vector
space of inputs to layer l. The typical geometry of the manifold in the l’th layer is summarized
by ql(θ1  θ2)  which for any θ1 and θ2 is deﬁned by (4) with the choice x0 a = x0(θ1) and x0 b =

4

x0(θ2). The theory for the propagation of pairs of points applies to all pairs of points on the
manifold  so intuitively  we expect that in the chaotic phase of a sigmoidal network  the manifold
should in some sense de-correlate  and become more complex  while in the ordered phase the
manifold should contract around a central point. This theoretical prediction of equations (3) and
(5) is quantitatively conﬁrmed in simulations in Fig. 3  when the input is a simple manifold  the
circle  h1(θ) =
dimensional subspace of RN1 in which the circle lives. The scaling is chosen so that each neuron has
input activity O(1). Also  for simplicity  we choose the ﬁxed point radius q = q∗ in Fig. 3.

N1q(cid:2)u0 cos(θ) + u1 sin(θ)(cid:3)  where u0 and u1 form an orthonormal basis for a 2

√

Figure 3: Propagating a circle through three random sigmoidal networks with varying σw and ﬁxed
σb = 0.3. (A) Projection of hidden inputs of simulated networks at layer 5 and 10 onto their ﬁrst
three principal components. Insets show the fraction of variance explained by the ﬁrst 5 singular
values. For large weights (bottom)  the distribution of singular values gets ﬂatter and the projected
curve is more tangled. (B) The autocorrelation  cl
as a function of layer for simulated networks. (C) The theoretical predictions from (6) (solid lines)
compared to the average (dots) and standard deviation across θ (shaded) in a simulated network.

12(∆θ) =(cid:82) dθ ql(θ  θ + ∆θ)/q∗  of hidden inputs

To quantitatively understand the layer-wise growth of complexity of this manifold  it is useful to turn
to concepts in Riemannian geometry [16]. First  at each point θ  the manifold h(θ) (we temporarily
suppress the layer index l) has a tangent  or velocity vector v(θ) = ∂θh(θ). Intuitively  curvature
is related to how quickly this tangent vector rotates in the ambient space RN as one moves along
the manifold  or in essence the acceleration vector a(θ) = ∂θv(θ). Now at each point θ  when both
are nonzero  v(θ) and a(θ) span a 2 dimensional subspace of RN . Within this subspace  there is a
unique circle of radius R(θ) that has the same position  velocity and acceleration vector as the curve
h(θ) at θ. This circle is known as the osculating circle (Fig. 4A)  and the extrinsic curvature κ(θ) of
the curve is deﬁned as κ(θ) = 1/R(θ). Thus  intuitively  small radii of curvature R(θ) imply high
extrinsic curvature κ(θ). The extrinsic curvature of a curve depends only on its image in RN and
is invariant with respect to the particular parameterization θ → h(θ). For any parameterization  an
under a unit speed parameterization of the curve  so that v(θ) · v(θ) = 1  we have v(θ) · a(θ) = 0 
and κ(θ) is simply the norm of the acceleration vector.
Another measure of the curve’s complexity is the length LE of its image in the ambient Euclidean
space. The Euclidean metric in RN induces a metric gE(θ) = v(θ) · v(θ) on the curve  so that the

explicit expression for κ(θ) is given by κ(θ) = (v· v)−3/2(cid:112)(v · v)(a · a) − (v · a)2 [16]. Note that
distance dLE moved in RN as one moves from θ to θ + dθ on the curve is dLE =(cid:112)gE(θ)dθ. The
total curve length is LE =(cid:82)(cid:112)gE(θ)dθ. However  even straight line segments can have a large

Euclidean length. Another interesting measure of length that takes into account curvature  is the
length of the image of the curve under the Gauss map. For a K dimensional manifold M embedded in

5

Figure 4: Propagation of extrinsic curvature and length in a network with 1000 hidden units. (A)
An osculating circle.
(B) A curve with unit tangent vectors at 4 points in ambient space  and
the image of these points under the Gauss map. (C-E) Propagation of curvature metrics based
on both theory derived from iterative maps in (3)  (6) and (8) (solid lines) and simulations using
(1) (dots). (F) Schematic of the normal vector  tangent plane  and principal curvatures for a 2D
manifold embedded in R3. (G) average principal curvatures for the largest and smallest 4 principal
curvatures (κ±1  . . .   κ±4) across locations θ within one network. The principal curvatures all grow
exponentially as we backpropagate to the input layer. Panels F G are discussed in Sec. 5.
RN   the Gauss map (Fig. 4B) maps a point θ ∈ M to its K dimensional tangent plane TθM ∈ GK N  
where GK N is the Grassmannian manifold of all K dimensional subspaces in RN . In the special case
of K = 1  GK N is the sphere SN−1 with antipodal points identiﬁed  since a 1-dimensional subspace
can be identiﬁed with a unit vector  modulo sign. The Gauss map takes a point θ on the curve and
SN−1 induces a Gauss metric on the curve  given by gG(θ) = (∂θ ˆv(θ)) · (∂θ ˆv(θ))  which measures
how quickly the unit tangent vector ˆv(θ) changes as θ changes. Thus the distance dLG moved in

maps it to the unit velocity vector ˆv(θ) = v(θ)/(cid:112)v(θ) · v(θ). In particular  the natural metric on
the Grassmannian GK N as one moves from θ to θ + dθ on the curve is dLG =(cid:112)gG(θ)dθ  and the
length of the curve under the Gauss map is LG =(cid:82)(cid:112)gG(θ)dθ. Furthermore  the Gauss metric is

√

√

√
N q  κ(θ) = 1/

related to the extrinsic curvature and the Euclidean metric via the relation gG(θ) = κ(θ)2gE(θ) [16].
To illustrate these concepts  it is useful to compute all of them for the circle h1(θ) deﬁned above:
gE(θ) = N q  LE = 2π
N q  gG(θ) = 1  and LG = 2π. As expected  κ(θ) is
the inverse of the radius of curvature  which is
N q. Now consider how these quantities change
if the circle is scaled up so that h(θ) → χh(θ). The length LE and radius scale up by χ  but the
curvature κ scales down as χ−1  and so LG does not change. Thus linear expansion increases length
and decreases curvature  thereby maintaining constant Grassmannian length LG.
We now show that nonlinear propagation of this same circle through a deep network can behave very
differently from linear expansion: in the chaotic regime  length can increase without any decrease
in extrinsic curvature! To remove the scaling with N in the above quantities  we will work with the
LE. Thus  1/(¯κ)2 can be thought
renormalized quantities ¯κ =
of as a radius of curvature squared per neuron of the osculating circle  while ( ¯LE)2 is the squared
Euclidean length of the curve per neuron. For the circle  these quantities are q and 2πq respectively.
For simplicity  in the inputs to the ﬁrst layer of neurons  we begin with a circle h1(θ) with squared
radius per neuron q1 = q∗  so this radius is already at the ﬁxed point of the length map in (3). In the
SM  we derive an iterative formula for the extrinsic curvature and Euclidean metric of this manifold
as it propagates through the layers of a deep network:

N gE  and ¯LE = 1√

N κ  ¯gE = 1

√

N

¯gE l = χ1 ¯gE l−1

(¯κl)2 = 3

χ2
χ2
1

(¯κl−1)2 

1
χ1

¯gE 1 = q∗ 

(¯κ1)2 = 1/q∗.

(8)

+

(cid:90)

Dz(cid:2)φ(cid:48)(cid:48)(cid:0)√

q∗z(cid:1)(cid:3)2

6

where χ1 is the stretch factor deﬁned in (7) and χ2 is deﬁned analogously as

χ2 = σ2
w

.

(9)

12 = 1; this second derivative is

χ2 is closely related to the second derivative of the C-map in (6) at cl−1
χ2q∗. See SM for a derivation of the evolution equations for extrinsic geometry in (8).
Intriguingly for a sigmoidal neural network  these evolution equations behave very differently in
the chaotic (χ1 > 1) versus ordered (χ1 < 1) phase. In the chaotic phase  the Euclidean metric ¯gE
grows exponentially with depth due to multiplicative stretching through χ1. This stretching does
multiplicatively attenuate any curvature in layer l − 1 by a factor 1/χ1 (see the update equation for
¯κl in (8))  but new curvature is added in due to a nonzero χ2  which originates from the curvature of
the single neuron nonlinearity in (9). Thus  unlike in linear expansion  extrinsic curvature is not lost 
but maintained  and ultimately approaches a ﬁxed point ¯κ∗. This implies that the global curvature
measure ¯LG grows exponentially with depth. These highly nontrivial predictions of the metric and
curvature evolution equations in (8) are quantitatively conﬁrmed in simulations in Figure 4C-E.
Intuitively  this exponential growth of global curvature ¯LG in the chaotic phase implies that the curve
explores many different tangent directions in hidden representation space. This further implies that
the coordinate functions of the embedding hl
i(θ) become highly complex curved basis functions
on the input manifold coordinate θ  allowing a deep network to compute exponentially complex
functions over simple low dimensional manifolds (Figure 5A-C  details in SM).

Figure 5: Deep networks in the chaotic regime are more expressive than shallow networks. (A)
Activity of four different neurons in the output layer as a function of the input  θ for three networks
of different depth (width Nl = 1  000). (B) Linear regression of the output activity onto a random
function (black) shows closer predictions (blue) with deeper networks (bottom) than shallow networks
(top). (C) Decomposing the prediction error by frequency shows shallow networks cannot capture high
frequency content in random functions but deep networks can (yellow=high error). (D) Increasing
the width of a one hidden layer network up to 10  000 does not decrease error at high frequencies.
5 Shallow networks cannot achieve exponential expressivity

Consider a shallow network with 1 hidden layer x1  one input layer x0  with x1 = φ(W1x0) + b1 
and a linear readout layer. How complex can the hidden representation be as a function of its width
N1  relative to the results above for depth? We prove a general upper bound on LE (see SM):
Theorem 1. Suppose φ(h) is monotonically non-decreasing with bounded dynamic range R  i.e.
maxh φ(h) − minh φ(h) = R. Further suppose that x0(θ) is a curve in input space such that no 1D
projection of ∂θx(θ) changes sign more than s times over the range of θ. Then for any choice of W1
and b1 the Euclidean length of x1(θ)  satisﬁes LE ≤ N1(1 + s)R.
√
For the circle input  s = 1 and for the tanh nonlinearity  R = 2  so in this special case  the normalized
length ¯LE ≤ 2
N1. In contrast  for deep networks in the chaotic regime ¯LE grows exponentially
with depth in h space  and so consequently also in x space. Therefore the length of curves typically
expand exponentially in depth even for random deep networks  but can only expand as the square
root of width no matter what shallow network is chosen. Moreover  as we have seen above  it is the
exponential growth of ¯LE that fundamentally drives the exponential growth of ¯LG with depth. Indeed
shallow random networks exhibit minimal growth in expressivity even at large widths (Figure 5D).

6 Classiﬁcation boundaries acquire exponential local curvature with depth
We have focused so far on how simple manifolds in input space can acquire both exponential
Euclidean and Grassmannian length with depth  thereby exponentially de-correlating and ﬁlling up

7

2 P ∂2G

∂x∂xT P  where P = I −(cid:98)∇G(cid:98)∇GT is the projection operator
onto Tx∗M and (cid:98)∇G is the unit normal vector [16]. Intuitively  near x∗  the decision boundary M

hidden representation space. Another natural question is how the complexity of a decision boundary
grows as it is backpropagated to the input layer. Consider a linear classiﬁer y = sgn(β · xD − β0)
acting on the ﬁnal layer. In this layer  the N − 1 dimensional decision boundary is the hyperplane
β·xD−β0 = 0. However  in the input layer x0  the decision boundary is a curved N −1 dimensional
manifold M that arises as the solution set of the nonlinear equation G(x0) ≡ β · xD(x0) − β0 = 0 
where xD(x0) is the nonlinear feedforward map from input to output.
At any point x∗ on the decision boundary in layer l  the gradient (cid:126)∇G is perpendicular to the N − 1
dimensional tangent plane Tx∗M (see Fig. 4F). The normal vector (cid:126)∇G  along with any unit tangent
vector ˆv ∈ Tx∗M  spans a 2 dimensional subspace whose intersection with M yields a geodesic
curve in M passing through x∗ with velocity vector ˆv. This geodesic will have extrinsic curvature
κ(x∗  ˆv). Maximizing this curvature over ˆv yields the ﬁrst principal curvature κ1(x∗). A sequence
of successive maximizations of κ(x∗  ˆv)  while constraining ˆv to be perpendicular to all previous
solutions  yields the sequence of principal curvatures κ1(x∗) ≥ κ2(x∗) ≥ ··· ≥ κN−1(x∗). These
principal curvatures arise as the eigenvalues of a normalized Hessian operator projected onto the
tangent plane Tx∗M: H = ||(cid:126)∇G||−1
can be approximated as a paraboloid with a quadratic form H whose N − 1 eigenvalues are the
principal curvatures κ1  . . .   κN−1 (Fig. 4F).
We compute these curvatures numerically as a function of depth in Fig. 4G (see SM for details).
We ﬁnd  remarkably  that a subset of principal curvatures grow exponentially with depth. Here
the principal curvatures are signed  with positive (negative) curvature indicating that the associated
geodesic curves towards (away from) the normal vector (cid:126)∇G. Thus the decision boundary can
become exponentially curved with depth  enabling highly complex classiﬁcations. Moreover  this
exponentially curved boundary is disentangled and mapped to a ﬂat boundary in the output layer.
7 Discussion
Fundamentally  neural networks compute nonlinear maps between high dimensional spaces  for
example from RN1 → RND  and it is unclear what the most appropriate mathematics is for under-
standing such daunting spaces of maps. Previous works have attacked this problem by restricting
the nature of the nonlinearity involved (e.g. piecewise linear  sum-product  or Pfafﬁan) and thereby
restricting the space of maps to those amenable to special theoretical analysis methods (combinatorics 
polynomial relations  or topological invariants). We have begun a preliminary exploration of the
expressivity of such deep functions based on Riemannian geometry and dynamical mean ﬁeld theory.
We demonstrate that networks in a chaotic phase compactly exhibit functions that exponentially grow
the global curvature of simple one dimensional manifolds from input to output and the local curvature
of simple co-dimension one manifolds from output to input. The former captures the notion that deep
neural networks can efﬁciently compute highly expressive functions in ways that shallow networks
cannot  while the latter quantiﬁes and demonstrates the power of deep neural networks to disentangle
curved input manifolds  an attractive idea that has eluded formal quantiﬁcation.
Moreover  our analysis of a maximum entropy distribution over deep networks constitutes an im-
portant null model of deep signal propagation that can be used to assess and understand different
behavior in trained networks. For example  the metrics we have adapted from Riemannian geometry 
combined with an understanding of their behavior in random networks  may provide a basis for
understanding what is special about trained networks. Furthermore  while we have focused on the
notion of input-output chaos  the duality between inputs and synaptic weights imply a form of weight
chaos  in which deep neural networks rapidly traverse function space as weights change (see SM).
Indeed  just as autocorrelation lengths between outputs as a function of inputs shrink exponentially
with depth  so too will autocorrelations between outputs as a function of weights. Finally  while our
length and correlation maps can be applied directly to piecewise linear nonlinearities (e.g. ReLUs) 
deep piecewise linear functions have 0 local curvature. To characterize how such functions twist
across input space  our methods can compute tangent vector auto-correlations instead of curvature.
But more generally  to understand functions  we often look to their graphs. The graph of a map from
RN1 → RND is an RN1 dimensional submanifold of RN1+ND  and therefore has both high dimension
and co-dimension. We speculate that many of the secrets of deep learning may be uncovered by
studying the geometry of this graph as a Riemannian manifold  and understanding how it changes
with both depth and learning.

8

References
[1] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[2] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529–533  2015.

[3] Awni Hannun  Carl Case  Jared Casper  Bryan Catanzaro  Greg Diamos  Erich Elsen  Ryan
Prenger  Sanjeev Satheesh  Shubho Sengupta  Adam Coates  et al. Deep speech: Scaling up
end-to-end speech recognition. arXiv preprint arXiv:1412.5567  2014.

[4] Yann LeCun  Yoshua Bengio  and Geoffrey Hinton. Deep learning. Nature  521(7553):436–444 

2015.

[5] Chris Piech  Jonathan Bassen  Jonathan Huang  Surya Ganguli  Mehran Sahami  Leonidas J
Guibas  and Jascha Sohl-Dickstein. Deep knowledge tracing. In Advances in Neural Information
Processing Systems  pages 505–513  2015.

[6] Lane T. McIntosh  Niru Maheswaranathan  Aran Nayebi  Surya Ganguli  and Stephen A.
Baccus. Deep learning models of the retinal response to natural scenes. In Advances in Neural
Information Processing Systems  2016.

[7] Yoshua Bengio  Aaron Courville  and Pascal Vincent. Representation learning: A review and
new perspectives. Pattern Analysis and Machine Intelligence  IEEE Transactions on  35(8):
1798–1828  2013.

[8] James J DiCarlo and David D Cox. Untangling invariant object recognition. Trends in cognitive

sciences  11(8):333–341  2007.

[9] Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of
linear regions of deep neural networks. In Advances in neural information processing systems 
pages 2924–2932  2014.

[10] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in

Neural Information Processing Systems  pages 666–674  2011.

[11] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv

preprint arXiv:1512.03965  2015.

[12] Matus Telgarsky. Representation beneﬁts of deep feedforward networks. arXiv preprint

arXiv:1509.08101  2015.

[13] James Martens  Arkadev Chattopadhya  Toni Pitassi  and Richard Zemel. On the representational
efﬁciency of restricted boltzmann machines. In Advances in Neural Information Processing
Systems  pages 2877–2885  2013.

[14] Monica Bianchini and Franco Scarselli. On the complexity of neural network classiﬁers: A
comparison between shallow and deep architectures. Neural Networks and Learning Systems 
IEEE Transactions on  25(8):1553–1565  2014.

[15] Hrushikesh Mhaskar  Qianli Liao  and Tomaso Poggio. Learning real and boolean functions:

When is deep better than shallow. arXiv preprint arXiv:1603.00988  2016.

[16] John M Lee. Riemannian manifolds: an introduction to curvature  volume 176. Springer

Science & Business Media  2006.

[17] Haim Sompolinsky  A Crisanti  and HJ Sommers. Chaos in random neural networks. Physical

Review Letters  61(3):259  1988.

[18] Maithra Raghu  Ben Poole  Jon Kleinberg  Surya Ganguli  and Jascha Sohl-Dickstein. On the

expressive power of deep neural networks. arXiv preprint arXiv:1606.05336  2016.

9

,Nicolas Heess
Gregory Wayne
David Silver
Timothy Lillicrap
Tom Erez
Yuval Tassa
Ben Poole
Subhaneil Lahiri
Maithra Raghu
Jascha Sohl-Dickstein
Surya Ganguli
Oisín Moran
Piergiorgio Caramazza
Daniele Faccio
Roderick Murray-Smith