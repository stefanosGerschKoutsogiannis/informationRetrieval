2010,Heavy-Tailed Process Priors for Selective Shrinkage,Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space.  Often  however  we are confronted with ``outliers'' in input space  which are isolated observations in sparsely populated regions. We show that heavy-tailed process priors (which we construct from Gaussian processes via a copula)  can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.,Heavy-Tailed Process Priors for Selective Shrinkage

Fabian L. Wauthier

University of California  Berkeley

flw@cs.berkeley.edu

Michael I. Jordan

University of California  Berkeley
jordan@cs.berkeley.edu

Abstract

Heavy-tailed distributions are often used to enhance the robustness of regression
and classiﬁcation methods to outliers in output space. Often  however  we are con-
fronted with “outliers” in input space  which are isolated observations in sparsely
populated regions. We show that heavy-tailed stochastic processes (which we con-
struct from Gaussian processes via a copula)  can be used to improve robustness
of regression and classiﬁcation estimators to such outliers by selectively shrinking
them more strongly in sparse regions than in dense regions. We carry out a theo-
retical analysis to show that selective shrinkage occurs when the marginals of the
heavy-tailed process have sufﬁciently heavy tails. The analysis is complemented
by experiments on biological data which indicate signiﬁcant improvements of es-
timates in sparse regions while producing competitive results in dense regions.

1

Introduction

Gaussian process classiﬁers (GPCs) [12] provide a Bayesian approach to nonparametric classiﬁca-
tion with the key advantage of producing predictive class probabilities. Unfortunately  when training
data are unevenly sampled in input space  GPCs tend to overﬁt in the sparsely populated regions.
Our work is motivated by an application to protein folding where this presents a major difﬁculty.
In particular  while Nature provides samples of protein conﬁgurations near the global minima of
free energy functions  protein-folding algorithms  which imitate Nature by minimizing an estimated
energy function  necessarily explore regions far from the minimum. If the estimate of free energy is
poor in those sparsely-sampled regions then the algorithm has a poor guide towards the minimum.
More generally this problem can be viewed as one of “covariate shift ” where the sampling pattern
differs in the training and testing phase.
In this paper we investigate a GPC-based approach that addresses overﬁtting by shrinking predictive
class probabilities towards conservative values. For an unevenly sampled input space it is natural
to consider a selective shrinkage strategy: we wish to shrink probability estimates more strongly in
sparse regions than in dense regions. To this end several approaches could be considered. If sparse
regions can be readily identiﬁed  selective shrinkage could be induced by tailoring the Gaussian
process (GP) kernel to reﬂect that information. In the absence of such knowledge  Goldberg and
Williams [5] showed that Gaussian process regression (GPR) can be augmented with a GP on the
log noise level. More recent work has focused on partitioning input space into discrete regions
and deﬁning different kernel functions on each. Treed Gaussian process regression [6] and Treed
Gaussian process classiﬁcation [1] represent advanced variations of this theme that deﬁne a prior
distribution over partitions and their respective kernel hyperparameters. Another line of research
which could be adapted to this problem posits that the covariate space is a nonlinear deformation
of another space on which a Gaussian process prior is placed [3  13]. Instead of directly modifying
the kernel matrix  the observed non-uniformity of measurements is interpreted as being caused by
the spatial deformation. A difﬁculty with all these approaches is that posterior inference is based on
MCMC  which can be overly slow for the large-scale problems that we aim to address.

1

This paper shows that selective shrinkage can be more elegantly introduced by replacing the Gaus-
sian process underlying GPC with a stochastic process that has heavy-tailed marginals (e.g.  Laplace 
hyperbolic secant  or Student-t). While heavy-tailed marginals are generally viewed as providing ro-
bustness to outliers in the output space (i.e.  the response space)  selective shrinkage can be viewed
as a form of robustness to outliers in the input space (i.e.  the covariate space). Indeed  selective
shrinkage means the data points that are far from other data points in the input space are regularized
more strongly. We provide a theoretical analysis and empirical results to show that inference based
on stochastic processes with heavy-tailed marginals yields precisely this kind of shrinkage.
The paper is structured as follows: Section 2 provides background on GPCs and highlights how
selective shrinkage can arise. We present a construction of heavy-tailed processes in Section 3 and
show that inference reduces to standard computations in a Gaussian process. An analysis of our
approach is presented in Section 4 and details on inference algorithms are presented in Section 5.
Experiments on biological data in Section 6 demonstrate that heavy-tailed process classiﬁcation
substantially outperforms GPC in sparse regions while performing competitively in dense regions.
The paper concludes with an overview of related research and ﬁnal remarks in Sections 7 and 8.

2 Gaussian process classiﬁcation and shrinkage
A Gaussian process (GP) [12] is a prior on functions z : X → R deﬁned through a mean function
(usually identically zero) and a symmetric positive semideﬁnite kernel k(· ·). For a ﬁnite set of
locations X = (x1  . . .   xn) we write z(X) ∼ p(z(X)) = N (0  K(X  X)) as a random variable
distributed according to the GP with ﬁnite-dimensional kernel matrix [K(X  X)]i j = k(xi  xj). Let
y denote an n-vector of binary class labels associated with measurement locations X1. For Gaussian
process classiﬁcation (GPC) [12] the probability that a test point x∗ is labeled as class y∗ = 1  given
training data (X  y)  is computed as

(cid:18)

(cid:19)

p(y∗ = 1|X  y  x∗) = Ep(z(x∗)|X y x∗)
p(z(x∗)|X  y  x∗) =

(cid:90)

p(z(x∗)|X  z(X)  x∗)p(z(X)|X  y)dz(X).

1

1 + exp{−z(x∗)}

(1)

The predictive distribution p(z(x∗)|X  y  x∗) represents a regression on z(x∗) with a complicated
observation model y|z. The central observation from Eq. (1) is that we could selectively shrink
the prediction p(y∗ = 1|X  y  x∗) towards a conservative value 1/2 by selectively shrinking
p(z(x∗)|X  y  x∗) closer to a point mass at zero.

3 Heavy-tailed process priors via the Gaussian copula

In this section we construct the heavy-tailed stochastic process by transforming a GP. As with the
GP  we will treat the new process as a prior on functions. Suppose that diag (K(X  X)) = σ21. We
deﬁne the heavy-tailed process f (X) with marginal c.d.f. Gb as

z(X) ∼ N (0  K(X  X))
u(X) = Φ0 σ2(z(X))
f (X) = G−1

b (u(X)) = G−1

b (Φ0 σ2 (z(X))).

(2)
(3)

Here the function Φ0 σ2(·) is the c.d.f. of a centered Gaussian with variance σ2. Presently  we
only consider the case when Gb is the (continuous) c.d.f. of a heavy-tailed density gb with scale
parameter b that is symmetric about the origin. Examples include the Laplace  hyperbolic secant
and Student-t distribution. We note that other authors have considered asymmetric or even discrete
distributions [2  11  16] while Snelson et al. [15] use arbitrary monotonic transformations in place
b (Φ0 σ2 (·)). The process u(X) has the density of a Gaussian copula [10  16] and is critical
of G−1
in transferring the correlation structure encoded by K(X  X) from z(X) to f (X). If we deﬁne

1To improve the clarity of exposition  we only deal with binary classiﬁcation for now. A full multiclass

classiﬁcation model is used in our experiments.

2

0 σ2 (Gb(f (X)))  it is well known [7  9  11  15  16] that the density of f (X) satisﬁes

(cid:81)

z(f (X)) = Φ−1

z(f (X))(cid:62)(cid:20)
Observe that if K(X  X) = σ2I then p(f (X)) = (cid:81)

i=1 gb(f (xi))
|K(X  X)/σ2|1/2

p(f (X)) =

− 1
2

(cid:26)

exp

(cid:21)

(cid:27)

K(X  X)−1 − I
σ2

z(f (X))

.

(4)

i=1 gb(f (xi)). Also note that if Gb were
chosen to be Gaussian  we would recover the Gaussian process. The predictive distribution
p(f (x∗)|X  f (X)  x∗) can be interpreted as a Heavy-tailed process regression (HPR). It is easy to
see that its computation can be reduced to standard computations in a Gaussian model by nonlinearly
transforming observations f (X) into z-space. The predictive distribution in z-space satisﬁes

(5)
(6)
(7)
The corresponding distribution in f-space follows by another change of variables. Having deﬁned
the heavy-tailed stochastic process in general we now turn to an analysis of its shrinkage properties.

p(z(x∗)|X  f (X)  x∗) = N (µ∗  Σ∗)
µ∗ = K(x∗  X)K(X  X)−1z(f (X))
Σ∗ = K(x∗  x∗) − K(x∗  X)K(X  X)−1K(X  x∗).

4 Selective shrinkage

By “selective shrinkage” we mean that the degree of shrinkage applied to a collection of estimators
varies across estimators. As motivated in Section 2  we are speciﬁcally interested in selectively
shrinking posterior distributions near isolated observations more strongly than in dense regions.
This section shows that we can achieve this by changing the form of prior marginals (heavy-tailed
instead of Gaussian) and that this induces stronger selective shrinkage than any GPR could induce.
Since HPR uses a GP in its construction  which can induce some selective shrinkage on its own  care
b (Φ0 σ2(·)) has on
must be taken to investigate only the additional beneﬁts the transformation G−1
shrinkage. For this reason we assume a particular GP prior which leads to a special type of shrinkage
in GPR and then check how an HPR model built on top of that GP changes the observed behavior.
In this section we provide an idealized analysis that allows us to compare the selective shrinkage
obtained by GPR and HPR. Note that we focus on regression in this section so that we can obtain
analytical results. We work with n measurement locations  X = (x1  . . .   xn)  whose index set
{1  . . .   n} can be partitioned into a “dense” set D with |D| = n− 1 and a single “sparse” index s /∈
D. Assume that xd = xd(cid:48) ∀d  d(cid:48) ∈ D  so that we may let (without loss of generality) ˜K(xd  xd(cid:48)) =
1 ∀d (cid:54)= d(cid:48) ∈ D. We also assert that xd (cid:54)= xs ∀d ∈ D and let ˜K(xd  xs) = ˜K(xs  xd) = 0 ∀d ∈ D.
Assuming that n > 2 we ﬁx the remaining entry ˜K(xs  xs) = /( + n − 2)  for some  > 0. We
interpret  as a noise variance and let K = ˜K + I.
Denote any distributions computed under the GPR model by pgp(·) and those computed in HPR
by php(·). Using K(X  X) = K  deﬁne z(X) as in Eq. (2). Let y denote a vector of real-valued
measurements for a regression task. The posterior distribution of z(xi) given y  with xi ∈ X  is
derived by standard Gaussian computations as

pgp(z(xi)|X  y) = N(cid:0)µi  σ2

(cid:1)

i

s for d ∈ D. To ensure that the posterior
For our choice of K(X  X) one can show that σ2
distributions agree at the two locations we require µd = µs  which holds if measurements y satisfy

d = σ2

µi = ˜K(xi  X)K(X  X)−1y
i = K(xi  xi) − ˜K(xi  X)K(X  X)−1 ˜K(X  xi).
σ2
(cid:40)

(cid:111)

K(X  X)−1y = 0

=

y

yd = ys

.

A similar analysis can be carried out for the induced HPR model. By Eqs. (5)–(7) HPR inference
leads to identical distributions php(z(xd)|X  y(cid:48)) = php(z(xs)|X  y(cid:48)) with d ∈ D if measurements
y(cid:48) in f-space satisfy

K(X  X)−1Φ−1

0 σ2(Gb(y(cid:48))) = 0

y ∈ Ygp (cid:44)(cid:110)
y(cid:48) ∈ Yhp (cid:44)(cid:110)

y|(cid:16) ˜K(xd  X) − ˜K(xs  X)
(cid:17)
y(cid:48)|(cid:16) ˜K(xd  X) − ˜K(xs  X)
(cid:17)
=(cid:8)y(cid:48) = G−1
b (Φ0 σ2 (y))|y ∈ Ygp

(cid:9) .

3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

d∈D

(cid:41)

(cid:111)

(a) gb(x) = 1

2b exp

(cid:110)− |x|

b

(cid:111)

(b) gb(x) = 1

2b sech

(cid:17)

(cid:16) πx

2b

(c) gb(x) =

2+(x/b)2(cid:17)3/2
(cid:16)

1

b

Figure 1: Illustration of G−1
b (Φ0 σ2(x))  for σ2 = 1.0 with Gb the c.d.f. of (a) the Laplace dis-
tribution (b) the hyperbolic secant distribution (c) a Student-t inspired distribution  all with scale
parameter b. Each plot shows three samples—dotted  dashed  solid—for growing b. As b increases
the distributions become heavy-tailed and the gradient of G−1

b (Φ0 σ2(x)) increases.

To compare the shrinkage properties of GPR and HPR we analyze select pairs of measurements
b (Φ0 σ2(·)) is strongly concave on (−∞  0] 
in Ygp and Yhp. The derivation requires that G−1
strongly convex on [0  +∞) and has gradient > 1 on R. To see intuitively why this should hold 
note that for Gb with fatter tails than a Gaussian  |G−1
b (Φ0 σ2(x))| should eventually dominate
|Φ−1
0 b2(Φ0 σ2(x))| = (b/σ)|x|. Figure 1 demonstrates graphically that the assumption holds for sev-
eral choices of Gb  provided b is large enough  i.e.  that gb has sufﬁciently heavy tails. Indeed  it can
b (Φ0 σ2(·)) scale lin-
be shown that for scale parameters b > 0  the ﬁrst and second derivatives of G−1
early with b. Consider a measurement 0 (cid:54)= y ∈ Ygp with sign (y(xd)) = sign (y(xd(cid:48)))  ∀d  d(cid:48) ∈ D.
Analyzing such y is relevant  as we are most interested in comparing how multiple reinforcing ob-
servations at clustered locations and a single isolated observation are absorbed during inference. By
deﬁnition of Ygp  for d∗ = argmaxd∈D|yd| we have |yd∗| < |ys| as long as n > 2. The correspond-
ing element y(cid:48) = G−1

b (Φ0 σ2 (y)) ∈ Yhp then satisﬁes

|y(cid:48)(xs)| =(cid:12)(cid:12)G−1

b (Φ0 σ2(y(xs)))(cid:12)(cid:12) >

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) G−1

b (Φ0 σ2(y(xd∗ )))

y(xs)

y(xd∗ )

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12) y(cid:48)(xd∗ )

y(xd∗ )

(cid:12)(cid:12)(cid:12)(cid:12) .

y(xs)

(8)

Thus HPR inference leads to identical predictive distributions in f-space at the two locations even
though the isolated observation y(cid:48)(xs) has disproportionately larger magnitude than y(cid:48)(xd∗ )  relative
to the GPR measurements y(xs) and y(xd∗ ). As this statement holds for any y ∈ Ygp satisfying
our earlier sign requirement  it indicates that HPR systematically shrinks isolated observations more
b (Φ0 σ2(·)) scales linearly with scale b > 0 
strongly than GPR. Since the second derivative of G−1
an intuitive connection suggests itself when looking at inequality (8): the heavier the marginal tails 
the stronger the inequality and thus the stronger the selective shrinkage effect.
The previous derivation exempliﬁes in an idealized setting that HPR leads to improved shrinkage of
predictive distributions near isolated observations. More generally  because GPR transforms mea-
surements only linearly  while HPR additionally pre-transforms measurements nonlinearly  our anal-
ysis suggests that for any GPR we can ﬁnd an HPR model which leads to stronger selective shrink-
age. The result has intuitive parallels to the parametric case:
just as (cid:96)1-regularization improves
shrinkage of parametric estimators  heavy-tailed processes improve shrinkage of nonparametric es-
timators. We note that although our analysis kept K(X  X) ﬁxed for GPR and HPR  in practice we
are free to tune the kernel to yield a desired scale of predictive distributions. The above analysis
has been carried out for regression  but motivates us to now explore heavy-tailed processes in the
classiﬁcation case.

5 Heavy-tailed process classiﬁcation

The derivation of heavy-tailed process classiﬁcation (HPC) is similar to that of standard multiclass
GPC with Laplace approximation in Rasmussen and Williams [12]. However  due to the nonlinear
transformations involved  some nice properties of their derivation are lost. We revert notation and
let y denote a vector of class labels. For a C-class classiﬁcation problem with n training points we

4

−10010−505xGb−1(Φ(x))  −10010−505xGb−1(Φ(x))−10010−505xGb−1(Φ(x))n  f 2

1   . . .   f 2

1   . . .   f 1

n )(cid:62).
introduce a vector of nC latent function measurements (f 1
For each block c ∈ {1  . . .   C} of n variables we deﬁne an independent heavy-tailed process prior
using Eq. (4) with kernel matrix Kc. Equivalently  we can deﬁne the prior jointly on f by letting
K be a block-diagonal kernel matrix with blocks K1  . . .   KC. Each kernel matrix Kc is deﬁned
by a (possibly different) symmetric positive semideﬁnite kernel with its own set of parameters. The
following construction relaxes the earlier condition that diag (K) = σ21 and instead views Φ0 σ2 (·)
as some nonlinear transformation with parameter σ2. By this relaxation we effectively adopt Liu et
al.’s [9] interpretation that Eq. (4) deﬁnes the copula. The scale parameters b could in principle vary
across the nC variables  but we keep them constant at least within each block of n. Labels y are
represented in a 1-of-n form and generated by the following observation model

1   . . .   f C

n  . . .   f C

(cid:80)
i }
exp{f c
i } .
c(cid:48) exp{f c(cid:48)
(cid:18) exp{f c∗}
(cid:80)
c(cid:48) exp{f c(cid:48)∗ }

(cid:19)

 

(9)

(10)

p(yc

i = 1|fi) = πc

i =

For inference we are ultimately interested in computing
p(yc∗ = 1|X  y  x∗) = Ep(f∗|X y x∗)

where f∗ = (f 1∗   . . .   f C∗ )(cid:62). The previous section motivates that improved selective shrinkage will
occur in p(f∗|X  y  x∗)  provided the prior marginals have sufﬁciently heavy tails.

5.1 Inference
As in GPC  most of the intractability lies in computing the predictive distribution p(f∗|X  y  x∗). We
use the Laplace approximation to address this issue: a Gaussian approximation to p(z|X  y) is found
and then combined with the Gaussian p(z∗|X  z  x∗) to give us an approximation to p(z∗|X  y  x∗).
This is then transformed to a (typically non-Gaussian) distribution in f-space using a change of
variables. Hence we ﬁrst seek to ﬁnd a mode and corresponding Hessian matrix of the log posterior
log p(z|X  y). Recalling the relation f = G−1

b (Φ0 σ2(z))  the log posterior can be written as

J(z) (cid:44) log p(y|z) + log p(z) = y(cid:62)f −(cid:88)

(cid:88)

log

i

c

exp{f c

i )} − 1
2

z(cid:62)K−1z − 1
2

log |K| + const.

Let Π be an nC × n matrix of stacked diagonal matrices diag (πc) for n-subvectors πc of π. With
W = diag (π) − ΠΠ(cid:62)  the gradients are

(cid:18) df
(cid:19)
(cid:18) d2f
(cid:19)

dz

dz2

∇J(z) = diag

∇2J(z) = diag

(y − π) − K−1z

diag (y − π) − diag

(cid:18) df

(cid:19)

dz

W diag

(cid:18) df

(cid:19)

dz

− K−1.

Unlike in Rasmussen and Williams [12]  −∇2J(z) is not generally positive deﬁnite owing to its ﬁrst
term. For that reason we cannot use a Newton step to ﬁnd the mode and instead resort to a simpler
gradient method. Once the mode ˆz has been found we approximate the posterior as

p(z|X  y) ≈ q(z|X  y) = N(cid:0)ˆz −∇2J(ˆz)−1(cid:1)  

and use this to approximate the predictive distribution by

q(z∗|X  y  x∗) =

p(z∗|X  z  x∗)q(z|X  y)df.

(cid:90)

Since we arranged for both distributions in the integral to be Gaussian  the resulting Gaussian can
be straightforwardly evaluated. Finally  to approximate the one-dimensional integral with respect
to p(f∗|X  y  x∗) in Eq. (10) we could either use a quadrature method  or generate samples from
q(z∗|X  y  x∗)  convert them to f-space using G−1
b (Φ0 σ2(·)) and then approximate the expectation
by an average. We have compared predictions of the latter method with those of a Gibbs sampler;
the Laplace approximation matched Gibbs results well  while being much faster to compute.

5

R
e
s
i
d
u
e

Rotamer r ∈ {1  2  3}

R

e

s

i

d

u

e

R

esid

u

e

O

C(cid:48)

H

N

Ψ Cα

H

N

H

Φ

C(cid:48)

O

(a)

(b)

Figure 2: (a) Schematic of a protein segment. The backbone is the sequence of C(cid:48)  N  Cα  C(cid:48)  N
atoms. An amino-acid-speciﬁc sidechain extends from the Cα atom at one of three discrete an-
gles known as “rotamers.” (b) Ramachandran plot of 400 (Φ  Ψ) measurements and corresponding
rotamers (by shapes/colors) for amino-acid arginine (arg). The dark shading indicates the sparse
region we considered in producing results in Figure 3. Progressively lighter shadings indicate how
the sparse region was grown to produce Figure 4.

5.2 Parameter estimation
Using a derivation similar to that in [12]  we have for ˆf = G−1
imation of the marginal log likelihood is
log p(y|x) ≈ log q(y|x) = J(ˆz) − 1
2

log | − 2π∇2J(ˆz)|

= y(cid:62) ˆf −(cid:88)

(cid:88)

log

exp

i

c

(cid:110) ˆf c

i

(cid:111) − 1

ˆz(cid:62)K−1 ˆz − 1
2

log |K| − 1
2

2

b (Φ0 σ2 (ˆz)) that the Laplace approx-

(11)
log | − ∇2J(ˆz)| + const.

We optimize kernel parameters θ by taking gradient steps on log q(y|x). The derivative needs to
take into account that perturbing the parameters can also perturb the mode ˆz found for the Laplace
approximation. At an optimum ∇J(ˆz) must be zero  so that

(cid:32)

(cid:33)

d ˆf
dˆz

(cid:32)
(cid:19)

(cid:33)

d ˆf
dˆz
− 1
2

ˆz = Kdiag

(y − ˆπ) 

(12)

where ˆπ is deﬁned as in Eq. (9) but using ˆf rather than f. Taking derivatives of this equation allows
us to compute the gradient dˆz/dθ. Differentiating the marginal likelihood we have

d log q(y|x)

dθ

= (y − ˆπ)(cid:62)diag

(cid:18)

1
2

tr

K−1 dK
dθ

dˆz
dθ

(cid:18)

tr

ˆz(cid:62)K−1 dK
dθ

1
2

K−1 ˆz +

− dˆz
dθ
∇2J(ˆz)−1 d∇2J(ˆz)

(cid:19)

.

dθ

K−1 ˆz −

The remaining gradient computations are straightforward  albeit tedious. In addition to optimizing
the kernel parameters  it may also be of interest to optimize the scale parameter b of marginals Gb.
Again  differentiating Eq. (12) with respect to b allows us to compute dˆz/db. We note that when
perturbing b we change ˆf by changing the underlying mode ˆz as well as by changing the parameter
b which is used to compute ˆf from ˆz. Suppressing the detailed computations  the derivative of the
marginal log likelihood with respect to b is
= (y − ˆπ)(cid:62) d ˆf
db

∇2J(ˆz)−1 d∇2J(ˆz)

K−1 ˆz − 1
2

d log q(y|x)

− dˆz
db

(cid:18)

(cid:19)

db

db

tr

(cid:62)

.

6

ΦΨ  −pi−pi/20pi/2pi−pi−pi/20pi/2pir = 1r = 2r = 3(a)

(b)

Figure 3: Rotamer prediction rates in percent in (a) sparse and (b) dense regions. Both ﬂavors
of HPC (hyperbolic secant and Laplace marginals) signiﬁcantly outperform GPC in sparse regions
while performing competitively in dense regions.

6 Experiments

To a ﬁrst approximation  the three-dimensional structure of a folded protein is deﬁned by pairs
of continuous backbone angles (Φ  Ψ)  one pair for each amino-acid  as well as discrete angles 
so-called rotamers  that deﬁne the conformations of the amino-acid sidechains that extend from
the backbone. The geometry is outlined in Figure 2(a). There is a strong dependence between
backbone angles (Φ  Ψ) and rotamer values; this is illustrated in the “Ramachandran plot” shown
in Figure 2(b)  which plots the backbone angles for each rotamer (indicated by the shapes/colors).
The dependence is exploited in computational approaches to protein structure prediction  where
estimates of rotamer probabilities given backbone angles are used as one term in an energy function
that models native protein states as minima of the energy. Poor estimates of rotamer probabilities
in sparse regions can derail the prediction procedure. Indeed  sparsity has been a serious problem
in state-of-the-art rotamer models based on kernel density estimates (Roland Dunbrack  personal
communication). Unfortunately  we have found that GPC is not immune to the sparsity problem.
To evaluate our algorithm we consider rotamer-prediction tasks on the 17 amino-acids (out of 20)
that have three rotamers at the ﬁrst dihedral angle along the sidechain2. Our previous work thus
applies with the number of classes C = 3 and the covariates being (Φ  Ψ) angle pairs. Since the
input space is a torus we deﬁned GPC and HPC using the following von Mises-inspired kernel for
d-dimensional angular data:

(cid:40)

(cid:32)(cid:32) d(cid:88)

(cid:33)

(cid:33)(cid:41)

k(xi  xj) = σ2 exp

λ

cos(xi k − xj k)

− d

 

k=1

where xi k  xj k ∈ [0  2π] and σ2  λ ≥ 03. To ﬁnd good GPC kernel parameters we optimize
an (cid:96)2-regularized version of the Laplace approximation to the log marginal likelihood reported in
Eq. 3.44 of [12]. For HPC we let Gb be either the centered Laplace distribution or the hyperbolic
secant distribution with scale parameter b. We estimate HPC kernel parameters as well as b by
similarly maximizing an (cid:96)2-regularized form of Eq. (11). In both cases we restricted the algorithms
to training sets of only 100 datapoints. Since good regularization parameters for the objectives are
not known a priori we train with and test them on a grid for each of the 17 rotameric residues in
ten-fold cross-validation. To ﬁnd good regularization parameters for a particular residue we look up
that combination which  averaged over the ten folds of the remaining 16 residues  produced the best
test results. Having chosen the regularization constants we report average test results computed in
ten-fold cross validation.
We evaluate the algorithms on predeﬁned sparse and dense regions in the Ramachandran plot  as
indicated by the background shading in Figure 2(b). Across 17 residues the sparse regions usually
contained more than 70 measurements (and often more than 150)  each of which appears in one
of the 10 cross validations. Figure 3 compares the label prediction rates on the dense and sparse

2Residues alanine and glycine are non-discrete while proline has two rotamers at the ﬁrst dihedral angle.
3The function cos(xi k − xj k) = [cos(xi.k)  sin(xi k)][cos(xj.k)  sin(xj k)](cid:62) is a symmetric positive
semi-deﬁnite kernel. By Propositions 3.22 (i) and (ii) and Proposition 3.25 in Shawe-Taylor and Cristian-
ini [14]  so is k(xi  xj) above.

7

trptyrserphegluasnleuthrhisaspargcyslysmetglnileval00.20.40.60.81Prediction rate  HPC Hyp. sec.HPC LaplaceGPCtrptyrserphegluasnleuthrhisaspargcyslysmetglnileval00.20.40.60.81Prediction rate  HPC Hyp. sec.HPC LaplaceGPCFigure 4: Average rotamer prediction rate in the sparse region for two ﬂavors of HPC  standard GPC
well as CTGP [1] as a function of the average number of points per residue in the sparse region.

regions. Averaged over all 17 residues HPC outperforms GPC by 5.79% with Laplace and 7.89%
with hyperbolic secant marginals. With Laplace marginals HPC underperforms GPC on only two
residues in sparse regions: by 8.22% on glutamine (gln)  and by 2.53% on histidine (his). On
dense regions HPC lies within 0.5% on 16 residues and only degrades once by 3.64% on his.
Using hyperbolic secant marginals HPC often improves GPC by more than 10% on sparse regions
and degrades by more than 5% only on cysteine (cys) and his. On dense regions HPC usually
performs within 1.5% of GPC. In Figure 4 we show how the average rotamer prediction rate across
17 residues changes for HPC  GPC  as well as CTGP [1] as we grow the sparse region to include
more measurements from dense regions. The growth of the sparse region is indicated by progres-
sively lighter shadings in Figure 2(b). As more points are included the signiﬁcant advantage of HPC
lessens. Eventually GPC does marginally better than HPC and much better than CTGP. The values
reported in Figure 3 correspond to the dark shaded region  with an average of 155 measurements.

7 Related research

Copulas [10] allow convenient modelling of multivariate correlation structures as separate from
marginal distributions. Early work by Song [16] used the Gaussian copula to generate complex
multivariate distributions by complementing a simple copula form with marginal distributions of
choice. Popularity of the Gaussian copula in the ﬁnancial literature is generally credited to Li [8]
who used it to model correlation structure for pairs of random variables with known marginals. More
recently  the Gaussian process has been modiﬁed in a similar way to ours by Snelson et al. [15].
They demonstrate that posterior distributions can better approximate the true noise distribution if
the transformation deﬁning the warped process is learned. Jaimungal and Ng [7] have extended
this work to model multiple parallel time series with marginally non-Gaussian stochastic processes.
Their work uses a “binding copula” to combine several subordinate copulas into a joint model.
Bayesian approaches focusing on estimation of the Gaussian copula covariance matrix for a given
dataset are given in [4  11]. Research also focused on estimation in high-dimensional settings [9].

8 Conclusions

This paper analyzed learning scenarios where outliers are observed in the input space  rather than
the output space as commonly discussed in the literature. We illustrated heavy-tailed processes as
a straightforward extension of GPs and an economical way to improve the robustness of estimators
in sparse regions beyond those of GP-based methods.
Importantly  because these processes are
based on a GP  they inherit many of its favorable computational properties; predictive inference
in regression  for instance  is straightforward. Moreover  because heavy-tailed processes have a
parsimonious representation  they can be used as building blocks in more complicated models where
currently GPs are used. In this way the beneﬁts of heavy-tailed processes extend to any GP-based
model that struggles with covariate shift.

Acknowledgements

We thank Roland Dunbrack for helpful discussions and providing access to the rotamer datasets.

8

1552463906189801554246339060.450.50.550.60.65’Density of test data’Prediction rate  HPC Hyp. sec.HPC LaplaceCTGPGPCReferences
[1] Tamara Broderick and Robert B. Gramacy. Classiﬁcation and Categorical Inputs with Treed

Gaussian Process Models. Journal of Classiﬁcation. To appear.

[2] Wei Chu and Zoubin Ghahramani. Gaussian Processes for Ordinal Regression. Journal of

Machine Learning Research  6:1019–1041  2005.

[3] Doris Damian  Paul D. Sampson  and Peter Guttorp. Bayesian Estimation of Semi-Parametric

Non-Stationary Spatial Covariance Structures. Environmetrics  12:161–178.

[4] Adrian Dobra and Alex Lenkoski. Copula Gaussian Graphical Models. Technical report 

Department of Statistics  University of Washington  2009.

[5] Paul W. Goldberg  Christopher K. I. Williams  and Christopher M. Bishop. Regression with
Input-dependent Noise: A Gaussian Process Treatment. In Advances in Neural Information
Processing Systems  volume 10  pages 493–499. MIT Press  1998.

[6] Robert B. Gramacy and Herbert K. H. Lee. Bayesian Treed Gaussian Process Models with an

Application to Computer Modeling. Journal of the American Statistical Association  2007.

[7] Sebastian Jaimungal and Eddie K. Ng. Kernel-based Copula Processes. In Proceedings of the
European Conference on Machine Learning and Knowledge Discovery in Databases  pages
628–643. Springer-Verlag  2009.

[8] David X. Li. On Default Correlation: A Copula Function Approach. Technical Report 99-07 

Riskmetrics Group  New York  April 2000.

[9] Han Liu  John Lafferty  and Larry Wasserman. The Nonparanormal: Semiparametric Esti-
mation of High Dimensional Undirected Graphs. Journal of Machine Learning Research 
10:1–37  2009.

[10] Roger B. Nelsen. An Introduction to Copulas. Springer  1999.
[11] Michael Pitt  David Chan  and Robert J. Kohn. Efﬁcient Bayesian Inference for Gaussian

Copula Regression Models. Biometrika  93(3):537–554  2006.

[12] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning.

MIT Press  2006.

[13] Alexandra M. Schmidt and Anthony O’Hagan. Bayesian Inference for Nonstationary Spa-
tial Covariance Structure via Spatial Deformations. Journal of the Royal Statistical Society 
65(3):743–758  2003. Ser. B.

[14] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge

University Press  2004.

[15] Ed Snelson  Carl E. Rasmussen  and Zoubin Ghahramani. Warped Gaussian Processes. In

Advances in Neural Information Processing Systems  volume 16  pages 337–344  2004.

[16] Peter Xue-Kun Song. Multivariate Dispersion Models Generated From Gaussian Copula.

Scandinavian Journal of Statistics  27(2):305–320  2000.

9

,James Newling
François Fleuret