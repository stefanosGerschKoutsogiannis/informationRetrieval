2013,Online Learning of Nonparametric Mixture Models via Sequential Variational Approximation,Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem  we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm -- random initialization plus iterative update  we take an progressive approach. Starting with a given prior  our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process  new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass  making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art.,Online Learning of Nonparametric Mixture Models

via Sequential Variational Approximation

Dahua Lin

Toyota Technological Institute at Chicago

dhlin@ttic.edu

Abstract

Reliance on computationally expensive algorithms for inference has been limiting
the use of Bayesian nonparametric models in large scale applications. To tackle this
problem  we propose a Bayesian learning algorithm for DP mixture models. In-
stead of following the conventional paradigm – random initialization plus iterative
update  we take an progressive approach. Starting with a given prior  our method
recursively transforms it into an approximate posterior through sequential varia-
tional approximation. In this process  new components will be incorporated on the
ﬂy when needed. The algorithm can reliably estimate a DP mixture model in one
pass  making it particularly suited for applications with massive data. Experiments
on both synthetic data and real datasets demonstrate remarkable improvement on
efﬁciency – orders of magnitude speed-up compared to the state-of-the-art.

1

Introduction

Bayesian nonparametric mixture models [7] provide an important framework to describe complex
In this family of models  Dirichlet process mixture models (DPMM) [1  15  18] are among
data.
the most popular in practice. As opposed to traditional parametric models  DPMM allows the num-
ber of components to vary during inference  thus providing great ﬂexibility for explorative analy-
sis. Nonetheless  the use of DPMM in practical applications  especially those with massive data 
has been limited due to high computational cost. MCMC sampling [12  14] is the conventional ap-
proach to Bayesian nonparametric estimation. With heavy reliance on local updates to explore the
solution space  they often show slow mixing  especially on large datasets. Whereas the use of split-
merge moves and data-driven proposals [9 17 20] has substantially improved the mixing performance 
MCMC methods still require many passes over a dataset to reach the equilibrium distribution.
Variational inference [4  11  19  22]  an alternative approach based on mean ﬁeld approximation  has
become increasingly popular recently due to better run-time performance. Typical variational meth-
ods for nonparametric mixture models rely on a truncated approximation of the stick breaking con-
struction [16]  which requires a ﬁxed number of components to be maintained and iteratively updated
during inference. The truncation level are usually set conservatively to ensure approximation accu-
racy  incurring considerable amount of unnecessary computation.
The era of Big Data presents new challenges for machine learning research. Many real world appli-
cations involve massive amount of data that even cannot be accommodated entirely in the memory.
Both MCMC sampling and variational inference maintain the entire conﬁguration and perform it-
erative updates of multiple passes  which are often too expensive for large scale applications. This
challenge motivated us to develop a new learning method for Bayesian nonparametric models that
can handle massive data efﬁciently. In this paper  we propose an online Bayesian learning algorithm
for generic DP mixture models. This algorithm does not require random initialization of components.
Instead  it begins with the prior DP(αµ) and progressively transforms it into an approximate posterior
of the mixtures  with new components introduced on the ﬂy as needed. Based on a new way of varia-
tional approximation  the algorithm proceeds sequentially  taking in one sample at a time to make the

1

update. We also devise speciﬁc steps to prune redundant components and merge similar ones  thus
further improving the performance. We tested the proposed method on synthetic data as well as two
real applications: modeling image patches and clustering documents. Results show empirically that
the proposed algorithm can reliably estimate a DP mixture model in a single pass over large datasets.

2 Related Work

Recent years witness lots of efforts devoted to developing efﬁcient learning algorithms for Bayesian
nonparametric models. A n important line of research is to accelerate the mixing in MCMC through
better proposals. Jain and Neal [17] proposed to use split-merge moves to avoid being trapped in
local modes. Dahl [6] developed the sequentially allocated sampler  where splits are proposed by
sequentially allocating observations to one of two split components through sequential importance
sampling. This method was recently extended for HDP [20] and BP-HMM [9].
There has also been substantial advancement in variational inference. A signiﬁcant development along
is line is the Stochastic Variational Inference  a framework that incorporates stochastic optimization
with variational inference [8]. Wang et al. [23] extended this framework to the non-parametric realm 
and developed an online learning algorithm for HDP [18]. Wang and Blei [21] also proposed a
truncation-free variational inference method for generic BNP models  where a sampling step is used
for updating atom assignment that allows new atoms to be created on the ﬂy.
Bryant and Sudderth [5] recently developed an online variational inference algorithm for HDP  using
mini-batch to handle streaming data and split-merge moves to adapt truncation levels. They tried to
tackle the problem of online BNP learning as we do  but via a different approach. First  we propose a
generic method while they focuses on topic models. The designs are also different – our method starts
from scratch and progressively adds new components. Its overall complexity is O(nK)  where n and
K are number of samples and expected number of components. Bryant’s method begins with random
initialization and relies on splits over mini-batch to create new topics  resulting in the complexity of
O(nKT )  where T is the number of iterations for each mini-batch. The differences stem from the
theoretical basis – our method uses sequential approximation based on the predictive law  while theirs
is an extension of the standard truncation-based model.
Nott et al. [13] recently proposed a method  called VSUGS  for fast estimation of DP mixture models.
Similar to our algorithm  the VSUGS method proposed takes a sequential updating approach  but
relies on a different approximation. Particularly  what we approximate is a joint posterior over both
data allocation and model parameters  while VSUGS is based on the approximating the posterior of
data allocation. Also  VSUGS requires ﬁxing a truncation level T in advance  which may lead to
difﬁculties in practice (especially for large data). Our algorithm provides a way to tackle this  and no
longer requires ﬁxed truncation.

3 Nonparametric Mixture Models

This section provide a brief review of Dirichlet Process Mixture Model – one of the most widely
used nonparametric mixture models. A Dirichlet Process (DP)  typically denoted by DP(αµ) is
characterized by a concentration parameter α and a base distribution µ.
It has been shown that
sample paths of a DP are almost surely discrete [16]  and can be expressed as

∞(cid:88)

k−1(cid:89)

D =

πkδφk   with πk = vk

vl  vk ∼ Beta(1  αk)  ∀k = 1  2  . . . .

(1)

k=1

l=1

This is often referred to as the stick breaking representation  and φk is called an atom. Since an
atom can be repeatedly generated from D with positive probability  the number of distinct atoms is
usually less than the number of samples. The Dirichlet Process Mixture Model (DPMM) exploits this
property  and uses a DP sample as the prior of component parameters. Below is a formal deﬁnition:

D ∼ DP (αµ) 

(2)
Consider a partition {C1  . . .   CK} of {1  . . .   n} such that θi are identical for all i ∈ Ck  which
we denote by φk. Instead of maintaining θi explicitly  we introduce an indicator zi for each i with

θi ∼ µ  xi ∼ F (·|θi)  ∀i = 1  . . .   n.

2

θi = φzi. Using this clustering notation  this formulation can be rewritten equivalently as follows:

(3)
Here  CRP(α) denotes a Chinese Restaurant Prior  which is a distribution over exchangeable parti-
tions. Its probability mass function is given by
pCRP (z1:n|α) =

z1:n ∼ CRP(α)  φk ∼ µ  ∀k = 1  2  . . . K
xi ∼ F (·|φzi)  ∀i = 1  2  . . .   n.
K(cid:89)

Γ(|Ck|).

(4)

Γ(α)αK
Γ(α + n)

k=1

4 Variational Approximation of Posterior

Generally  there are two approaches to learning a mixture model from observed data  namely Max-
imum likelihood estimation (MLE) and Bayesian learning. Speciﬁcally  maximum likelihood esti-
mation seeks an optimal point estimate of ν  while Bayesian learning aims to derive the posterior
distribution over the mixtures. Bayesian learning takes into account the uncertainty about ν  often
resulting in better generalization performance than MLE.
In this paper  we focus on Bayesian learning. In particular  for DPMM  the predictive distribution of
component parameters  conditioned on a set of observed samples x1:n  is given by

(5)
Here  ED|x1:n takes the expectation w.r.t. p(D|x1:n). In this section  we derive a tractable approxima-
tion of this predictive distribution based on a detailed analysis of the posterior.

p(θ(cid:48)|x1:n) = ED|x1:n [p(θ(cid:48)|D)] .

4.1 Posterior Analysis
Let D ∼ DP(αµ) and θ1  . . .   θn be iid samples from D  {C1  . . .   CK} be a partition of {1  . . .   n}
such that θi for all i ∈ Ck are identical  and φk = θi ∀i ∈ Ck. Then the posterior distribution of D
remains a DP  as D|θ1:n ∼ DP(˜α˜µ)  where ˜α = α + n  and
|Ck|
α + n

K(cid:88)

α + n

δφk .

˜µ =

µ +

(6)

α

k=1

The atoms are generally unobservable  and therefore it is more interesting in practice to consider the
posterior distribution of D given the observed samples. For this purpose  we derive the lemma below
that provides a constructive characterization of the posterior distribution given both the observed
samples x1:n and the partition z.
Lemma 1. Consider the DPMM in Eq.(3). Drawing a sample from the posterior distribution
p(D|z1:n  x1:n) is equivalent to constructing a random probability measure as follows

K(cid:88)

β0D(cid:48) +

βkδφk  
with D(cid:48) ∼ DP(αµ)  (β0  β1  . . .   βk) ∼ Dir(α  m1  . . .   mK)  φk ∼ µ|Ck .

Here  mk = |Ck|  µ|Ck is a posterior distribution given by i.e. µ|Ck (dθ) ∝ µ(dθ)(cid:81)

k=1

F (xi|θ).

i∈Ck

(7)

This lemma immediately follows from the Theorem 2 in [10] as DP is a special case of the so-
called Normalized Random Measures with Independent Increments (NRMI). It is worth emphasizing
that p(D|x  z) is no longer a Dirichlet process  as the locations of the atoms φ1  . . .   φK are non-
deterministic  instead they follow the posterior distributions µ|Ck.
By marginalizing out the partition z1:n  we obtain the posterior distribution p(D|x1:n):

p(D|x1:n) =

p(z1:n|x1:n)p(D|x1:n  z1:n).

(cid:88)

z1:n

Let {C (z)

1   . . .   C (z)

K } be the partition corresponding to z1:n  we have
(cid:89)
p(z1:n|x1:n) ∝ pCRF (z1:n|α)

K(z)(cid:89)

µ(dφk)

(cid:90)

F (xi|φk).

(8)

(9)

i∈C(z)

k

k=1

3

4.2 Variational Approximation

Computing the predictive distribution based on Eq.(8) requires enumerating all possible partitions 
which grow exponentially as n increases. To tackle this difﬁculty  we resort to variational approxi-
mation  that is  to choose a tractable distribution to approximate p(D|x1:n  z1:n).
In particular  we consider a family of random probability measures that can be expressed as follows:

q(D|ρ  ν) =

ρi(zi)q(z)

ν (D|z1:n).

(cid:88)

n(cid:89)

z1:n

i=1

Here  q(z)

ν (D|z1:n) is a stochastic process conditioned on z1:n  deﬁned as

ν (D|z1:n) d∼ β0D(cid:48) +
q(z)

βkδφk  

K(cid:88)

k=1

with D(cid:48) ∼ DP(αµ)  (β0  β1  . . .   βK) ∼ Dir(α  m(z)

1   . . .   m(z)

K )  φk ∼ νk.

ν

k = |C (z)

is equivalent to constructing one according
k | is the cardinality of the k-th cluster w.r.t. z1:n  and

differences: (1) p(z1:n|x1:n) is replaced by a product distribution(cid:81)

Here  we use d∼ to indicate that drawing a sample from q(z)
to the right hand side. In addition  m(z)
νk is a distribution over component parameters that is independent from z.
The variational construction in Eq.(10) and (11) is similar to Eq.(7) and (8)  except for two signiﬁcant
i ρi(zi)  and (2) µ|Ck  which
depends on z1:n  is replaced by an independent distribution νk. With this design  zi for different i and
φk for different k are independent w.r.t. q  thus resulting in a tractable predictive law below: Let q be
a random probability measure given by Eq.(10) and (11)  then

Eq(D|ρ ν) [p(θ(cid:48)|D)] =

α

α + n

µ(θ(cid:48)) +

i=1 ρi(k)
α + n

νk(θ(cid:48)).

(12)

The approximate posterior has two sets of parameters: ρ (cid:44) (ρ1  . . .   ρn) and ν (cid:44) (ν1  . . .   νn). With
this approximation  the task of Bayesian learning reduces to the problem of ﬁnding the optimal setting
of these parameters such that q(D|ρ  ν) best approximates the true posterior distribution.

(cid:80)n

K(cid:88)

k=1

(10)

(11)

4.3 Sequential Approximation

The ﬁrst problem here is to determine the value of K. A straightforward approach is to ﬁx K to a large
number as in the truncated methods. This way  however  would incur substantial computational costs
on unnecessary components. We take a different approach here. Rather than randomly initializing a
ﬁxed number of components  we begin with an empty model (i.e. K = 1) and progressively reﬁne
the model as samples come in  adding new components on the ﬂy when needed.
Speciﬁcally  when the ﬁrst sample x1 is observed  we introduce the ﬁrst component and denote the
posterior for this component by ν1. As there is only one component at this point  we have z1 = 1 
1 (dθ) ∝
i.e. ρ1(z1 = 1) = 1  and the posterior distribution over the component parameter is ν(1)
µ(dθ)F (x1|θ). Samples are brought in sequentially. In particular  we compute ρi  and update ν(i−1)
to νi upon the arrival of the i-th sample xi.
Suppose we have ρ = (ρ1  . . .   ρi) and ν(i) = (ν(i)
K ) after processing i samples. To explain
xi+1  we can use either of the K existing components or introduce a new component φk+1. Then the
posterior distribution of zi+1  φ1  . . .   φK+1 given x1  . . .   xn  xn+1 is

p(zi+1  φ1:K+1|x1:i+1) ∝ p(zi+1  φ1:K+1|x1:i)p(xi+1|zi+1  φ1:K+1).

(13)
Using the tractable distribution q(·|ρ1:i  ν(i)) in Eq.(10) to approximate the posterior p(·|x1:i)  we get
(14)
Then  the optimal settings of qi+1 and ν(i+1) that minimizes the Kullback-Leibler divergence between
q(zi+1  φ1:K+1|q1:i+1  ν(i+1)) and the approximate posterior in Eq.(14) are given as follows:

p(zi+1  φ1:K+1|x1:i+1) ∝ q(zi+1|ρ1:i  ν(i))p(xi+1|zi+1  φ1:K+1).

1   . . .   ν(i)

(k ≤ K) 
(k = K + 1) 

(15)

(cid:40)

(cid:82)
θ F (xi+1|θ)ν(i)
θ F (xi+1|θ)µ(dθ)

α(cid:82)

w(i)
k

k (dθ)

ρi+1 ∝

4

Algorithm 1 Sequential Bayesian Learning of DPMM (for conjugate cases).
Require: base measure params: λ  λ0  observed samples: x1  . . .   xn  and threshold 

Let K = 1  ρ1(1) = 1  w1 = ρ1  ζ1 = φ(x1)  and ζ(cid:48)
for i = 2 : n do

1 = 1.

Ti ← T (xi)  and bi ← b(xi)
marginal log-likelihood: hi(k) ←

ρi(k) ← wkehi(k)/(cid:80)

if ρi(K + 1) >  then

k) − bi
B(ζk + Ti  ζ(cid:48)
B(λ + Ti  λ(cid:48) + τ ) − B(λ  λ(cid:48)) − bi
l wlehi(l) for k = 1  . . .   K + 1 with wK+1 = α

k + τ ) − B(ζk  ζ(cid:48)

(cid:40)

(k = 1  . . .   K)
(k = K + 1)

wk ← wk + ρi(k)  ζk ← ζk + ρi(k)Ti  and ζ(cid:48)
k ← ζ(cid:48)
wK+1 ← ρi(K + 1)  ζK+1 ← ρi(K + 1)Ti  and ζ(cid:48)
K ← K + 1

re-normalize ρi such that(cid:80)K

wk ← wk + ρi(k)  ζk ← ζk + ρi(k)Ti  and ζ(cid:48)

k=1 ρi(k) = 1

k ← ζ(cid:48)

else

k + ρi(k)τ  for k = 1  . . .   K
K+1 ← ρi(K + 1)τ

k + ρi(k)τ  for k = 1  . . .   K

end if
end for

k =(cid:80)i

with w(i)

j=1 ρj(k)  and

(cid:40)
µ(dθ)(cid:81)i+1

j=1 F (xj|θ)ρj (k)

µ(dθ)F (xi+1|θ)ρi+1(k)

(k ≤ K) 
(k = K + 1).

(16)

ν(i+1)
k

(dθ) ∝

Discussion. There is a key distinction between this approximation scheme and conventional ap-
proaches: Instead of seeking the approximation of p(D|x1:n)  which is very difﬁcult (D is inﬁnite)
and unnecessary (only a ﬁnite number of components are useful)  we try to approximate the posterior
of a ﬁnite subset of latent variables that are truly relevant for prediction  namely z and φ1:K+1.
This sequential approximation scheme introduces a new component for each sample  resulting in
n components over the entire dataset. This  however  is unnecessary. We ﬁnd empirically that for
most samples  ρi(K + 1) is negligible  indicating that the sample is adequately explained by existing
component  and there is no need of new components. In practice  we set a small value  and increase
K only when ρi(K + 1) > . This simple strategy is very effective in controlling the model size.

5 Algorithm and Implementation

This section discusses the implementation of the sequential Bayesian learning algorithm under two
different circumstances: (1) µ and F are exponential family distributions that form a conjuate pair 
and (2) µ is not a conjugate prior w.r.t. F .

Conjugate Case.

In general  when µ is conjugate to F   they can be written as follows:

µ(dθ|λ  λ(cid:48)) = exp(cid:0)λT η(θ) − λ(cid:48)A(θ) − B(λ  λ(cid:48))(cid:1) h(dθ) 
F (x|θ) = exp(cid:0)η(θ)T T (x) − τ A(θ) − b(x)(cid:1) .

(17)
(18)
Here  the prior measure µ has a pair of natural parameters: (λ  λ(cid:48)). Conditioned on a set of ob-
servations x1  . . .   xn  the posterior distribution remains in the same family as µ with parameters

i=1 T (xi)  λ(cid:48) + nτ ). In addition  the marginal likelihood is given by

(λ +(cid:80)n

F (x|θ)µ(dθ|λ  λ(cid:48)) = exp (B(λ + T (x)  λ(cid:48) + τ ) − B(λ  λ(cid:48)) − b(x)) .

(19)

θ

In such cases  both the base measure µ and the component-speciﬁc posterior measures νk can be
represented using the natural parameter pairs  which we denote by (λ  λ(cid:48)) and (ζk  ζ(cid:48)
k). With this
notation  we derive a sequential learning algorithm for conjugate cases  as shown in Alg 1.

Non-conjugate Case.
In practical models  it is not uncommon that µ and F are not a conjugate
pair. Unlike in the conjugate cases discussed above  there exist no formulas to update posterior

5

(cid:90)

stochastic optimization. Consider a posterior distribution given by p(θ|x1:n) ∝ µ(θ)(cid:81)n

parameters or to compute marginal likelihood in general. Here  we propose to address this issue using
i=1 F (xi|θ).

A stochastic optimization method ﬁnds the MAP estimate of θ through update steps as below:

(20)
The basic idea here is to use the gradient computed at a particular sample xi to approximate the
true gradient. This procedure converges to a (local) maximum  as long as the step size σi satisfy

θ ← θ + σi (∇θ log µ(θ) + n∇θ log F (xi|θ)) .

(cid:80)∞
i=1 σi = ∞ and(cid:80)∞

i=1 σ2

i < ∞.

Incorporating the stochastic optimization method into our algorithm  we obtain a variant of Alg 1. The
general procedure is similar  except for the following changes: (1) It maintains point estimates of the
component parameters instead of the posterior  which we denote by ˆφ1  . . .   ˆφK. (2) It computes the
log-likelihood as hi(k) = log F (xi| ˆφk). (3) The estimates of the component parameters are updated
using the formula below:
k ← ˆφ(i−1)
ˆφ(i)

+ σi (∇θ log µ(θ) + nρi(k)∇θ log F (xi|θ)) .

Following the common practice of stochastic optimization  we set σi = i−κ/n with κ ∈ (0.5  1].

(21)

k

l

l w(i)

(with w(i)

k = (cid:80)i

Prune and Merge. As opposed to random initialization  components created during this sequen-
tial construction are often truly needed  as the decisions of creating new components are based on
knowledge accumulated from previous samples. However  it is still possible that some components
introduced at early iterations would become less useful and that multiple components may be similar.
We thus introduce a mechanism to remove undesirable components and merge similar ones.

k /(cid:80)
tween ρi(k) and ρi(k(cid:48)) over all processed samples  as dρ(k  k(cid:48)) = i−1(cid:80)i

We identify opportunities to make such adjustments by looking at the weights. Let ˜w(i)
k =
w(i)
j=1 ρj(k)) be the relative weight of a component at the i-th itera-
tion. Once the relative weight of a component drops below a small threshold εr  we remove it to save
unnecessary computation on this component in the future.
The similarity between two components φk and φk(cid:48) can be measured in terms of the distance be-
j=1 |ρj(k) − ρj(k(cid:48))|. We
k are merged (i.e. dρ(k  k(cid:48)) < εd). We also merge
increment ρi(k) to ρi(k) + ρi(k(cid:48)) when φk and φ(cid:48)
the associated sufﬁcient statistics (for conjugate case) or take an weighted average of the parameters
(for non-conjugate case). Generally  there is no need to perform such checks at every iteration. Since
computing this distance between a pair of components takes O(n)  we propose to examine similarities
at an O(i · K)-interval so that the amortized complexity is maintained at O(nK).

Discussion. As compared to existing methods  the proposed method has several important advan-
tages. First  it builds up the model on the ﬂy  thus avoiding the need of randomly initializing a set of
components as required by truncation-based methods. The model learned in this way can be readily
extended (e.g. adding more components or adapting existing components) when new data is available.
More importantly  the algorithm can learn the model in one pass  without the need of iterative updates
over the data set. This distinguishes it from MCMC methods and conventional variational learning
algorithms  making it a great ﬁt for large scale problems.

6 Experiments

To test the proposed algorithm  we conducted experiments on both synthetic data and real world
applications – modeling image patches and document clustering. All algorithms are implemented
using Julia [2]  a new language for high performance technical computing.

6.1 Synthetic Data

First  we study the behavior of the proposed algorithm on synthetic data. Speciﬁcally  we constructed
a data set comprised of 10000 samples in 9 Gaussian clusters of unit variance. The distances between
these clusters were chosen such that there exists moderate overlap between neighboring clusters. The
estimation of these Gaussian components are based on the DPMM below:

D ∼ DP(cid:0)α · N (0  σ2
pI)(cid:1)  

θi ∼ D  xi ∼ N (θi  σ2

xI).

(22)

6

Figure 1: Gaussian clusters on syn-
thetic data obtained using different
methods. Both MC-SM and SVA-PM
identiﬁed the 9 clusters correctly. The
result of MC-SM is omitted here  as it
looks the same as SVA-PM.

Figure 2: Joint log-likelihood on synthetic data as func-
tions of run-time. The likelihood values were evaluated
on a held-out testing set. (Best to view with color)

Here  we set α = 1  σp = 100 and σx = 1.
We tested the following inference algorithms: Collapsed Gibbs sampling (CGS) [12]  MCMC with
Split-Merge (MC-SM) [6]  Truncation-Free Variational Inference (TFV) [21]  Sequential Variational
Approximation (SVA)  and its variant Sequential Variational Approximation with Prune and Merge
(SVA-PM). For CGS  MC-SM  and TFV  we run the updating procedures iteratively for one hour 
while for SVA and SVA-PM  we run only one-pass.
Figure 1 shows the resulting components. CGS and TFV yield obviously redundant components.
This corroborates observations in previous work [9]. Such nuisances are signiﬁcantly reduced in
SVA  which only occasionally brings in redundant components. The key difference that leads to this
improvement is that CGS and TFV rely on random initialization to bootstrap the algorithm  which
would inevitably introduce similar components  while SVA leverages information gained from pre-
vious samples to decide whether new components are needed. Both MC-SM and SVA-PM produce
desired mixtures  demonstrating the importance of an explicit mechanism to remove redundancy.
Figure 2 plots the traces of joint log-likelihoods evaluated on a held-out set of samples. We can see that
SVA-PM quickly reaches the optimal solution in a matter of seconds. SVA also gets to a reasonable
solution within seconds  and then the progress slows down. Without the prune-and-merge steps  it
takes much longer for redundant components to fade out. MC-SM eventually reaches the optimal
solution after many iterations. Methods relying on local updates  including CGS and TFV  did not
even come close to the optimal solution within one hour. These results clearly demonstrate that our
progressive strategy  which gradually constructs the model through a series of informed decisions  is
much more efﬁcient than random initialization followed by iterative updating.

6.2 Modeling Image Patches

Image patches  which capture local characteristics of images  play a fundamental role in various
computer vision tasks  such as image recovery and scene understanding. Many vision algorithms rely
on a patch dictionary to work. It has been a common practice in computer vision to use parametric
methods (e.g. K-means) to learn a dictionary of ﬁxed size. This approach is inefﬁcient when large
datasets are used. It is also difﬁcult to be extended when new data with a ﬁxed K.
To tackle this problem  we applied our method to learn a nonparametric dictionary from the SUN
database [24]  a large dataset comprised of over 130K images  which capture a broad variety of
scenes. We divided all images into two disjoint sets: a training set with 120K images and a testing
set with 10K. We extracted 2000 patches of size 32 × 32 from each image  and characterize each
patch by a 128-dimensional SIFT feature. In total  the training set contains 240M feature vectors.
We respectively run TFV  SVA  and SVA-SM to learn a DPMM from the training set  based on the

7

CGSTVFSVASVA-PM−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468−8−6−4−202468020406080100−8.5−8−7.5−7−6.5−6−5.5x 104minutejoint log−lik  CGSMC−SMTVFSVASVA−PMFigure 3: Examples of im-
age patche clusters learned us-
ing SVA-PM. Each row corre-
sponds to a cluster. We can see
similar patches are in the same
cluster.

Figure 4:
log-
likelihood on image modeling
as functions of run-time.

Average

Figure 5:
log-
likelihood of document clusters
as functions of run-time.

Average

formulation given in Eq.(22)  and evaluate the average predictive log-likelihood over the testing set as
the measure of performance. Figure 3 shows a small subset of patch clusters obtained using SVA-PM.
Figure 4 compares the trajectories of the average log-likelihoods obtained using different algorithms.
TFV takes multiple iterations to move from a random conﬁguration to a sub-optimal one and get
trapped in a local optima. SVA steadily improves the predictive performance as it sees more samples.
We notice in our experiments that even without an explicit redundancy-removal mechanism  some
unnecessary components can still get removed when their relative weights decreases and becomes
negligible. SVM-PM accelerates this process by explicitly merging similar components.

6.3 Document Clustering

Next  we apply the proposed method to explore categories of documents. Unlike standard topic mod-
eling task  this is a higher level application that builds on top of the topic representation. Speciﬁcally 
we ﬁrst obtain a collection of m topics from a subset of documents  and characterize all documents by
topic proportions. We assume that the topic proportion vector is generated from a category-speciﬁc
Dirichlet distribution  as follows

D ∼ DP (α · Dirsym(γp))  

θi ∼ D  xi ∼ Dir(γxθi).

(23)
Here  the base measure is a symmetric Dirichlet distribution. To generate a document  we draw a
mean probability vector θi from D  and generates the topic proportion vector xi from Dir(γxθi).
The parameter γx is a design parameter that controls how far xi may deviate from the category-
speciﬁc center θi. Note that this is not a conjugate model  and we use stochastic optimization instead
of Bayesian updates in SVA (see section 5).
We performed the experiments on the New York Times database  which contains about 1.8M articles
from year 1987 to 2007. We pruned the vocabulary to 5000 words by removing stop words and
those with low TF-IDF scores  and obtained 150 topics by running LDA [3] on a subset of 20K
documents. Then  each document is represented by a 150-dimensional vector of topic proportions.
We held out 10K documents for testing and use the remaining to train the DPMM. We compared SVA 
SVA-PM  and TVF. The traces of log-likelihood values are shown in Figure 5. We observe similar
trends as above: SVA and SVA-PM attains better solution more quickly  while TVF is less efﬁcient
and is prune to being trapped in local maxima. Also  TVF tends to generate more components than
necessary  while SVA-PM maintains a better performance using much less components.

7 Conclusion

We presented an online Bayesian learning algorithm to estimate DP mixture models. The proposed
method does not require random initialization. Instead  it can reliably and efﬁciently learn a DPMM
from scratch through sequential approximation in a single pass. The algorithm takes in data in a
streaming fashion  and thus can be easily adapted to new data. Experiments on both synthetic data
and real applications have demonstrated that our algorithm achieves remarkable speedup – it can
attain nearly optimal conﬁguration within seconds or minutes  while mainstream methods may take
hours or even longer. It is worth noting that the approximation is derived based on the predictive law
of DPMM. It is an interesting future direction to investigate how it can be generalized to a broader
family of BNP models  such as HDP  Pitman-Yor processes  and NRMIs [10].

8

02468−180−160−140−120−100houravg. pred. log−lik  TVFSVASVA−PM0246810300350400450500550houravg. pred. log−lik  TVFSVASVA−PMReferences
[1] C. Antoniak. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The

Annals of Statistics  2(6):1152–1174  1974.

[2] Jeff Bezanson  Stefan Karpinski  Viral B. Shah  and Alan Edelman. Julia: A fast dynamic language for

technical computing. CoRR  abs/1209.5145  2012.

[3] David Blei  Ng Andrew  and Michael Jordan. Latent dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[4] David M. Blei and Michael I. Jordan. Variational methods for the Dirichlet process. In Proc. of ICML’04 

2004.

[5] Michael Bryant and Erik Sudderth. Truly nonparametric online variational inference for hierarchical dirich-

let processes. In Proc. of NIPS’12  2012.

[6] David B. Dahl. Sequentially-allocated merge-split sampler for conjugate and nonconjugate dirichlet process

mixture models  2005.

[7] Nils Lid Hjort  Chris Holmes  Peter Muller  and Stephen G. Walker. Bayesian Nonparametrics: Principles

and Practice. Cambridge University Press  2010.

[8] Matt Hoffman  David M. Blei  Chong Wang  and John Paisley. Stochastic variational inference. arXiv

eprints  1206.7501  2012.

[9] Michael C. Hughes  Emily B. Fox  and Erik B. Sudderth. Effective split-merge monte carlo methods for

nonparametric models of sequential data. 2012.

[10] Lancelot F. James  Antonio Lijoi  and Igor Pr¨unster. Posterior analysis for normalized random measures

with independent increments. Scaninavian Journal of Stats  36:76–97  2009.

[11] Kenichi Kurihara  Max Welling  and Yee Whye Teh. Collapsed variational dirichlet process mixture mod-

els. In Proc. of IJCAI’07  2007.

[12] Radford M. Neal. Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Journal of

computational and graphical statistics  9(2):249–265  2000.

[13] David J. Nott  Xiaole Zhang  Christopher Yau  and Ajay Jasra. A sequential algorithm for fast ﬁtting of

dirichlet process mixture models. In Arxiv: 1301.2897  2013.

[14] Ian Porteous  Alex Ihler  Padhraic Smyth  and Max Welling. Gibbs Sampling for (Coupled) Inﬁnite Mixture

Models in the Stick-breaking Representation. In Proc. of UAI’06  2006.

[15] Carl Edward Rasmussen. The Inﬁnite Gaussian Mixture Model. In Proc. of NIPS’00  2000.
[16] Jayaram Sethuraman. A constructive deﬁnition of dirichlet priors. Statistical Sinica  4:639–650  1994.
[17] S.Jain and R.M. Neal. A split-merge markov chain monte carlo procedure for the dirichlet process mixture

model. Journal of Computational and Graphical Statistics  13(1):158–182  2004.

[18] Yee Whye Teh  Michael I. Jordan  Matthew J. Beal  and David M. Blei. Hierarchical Dirichlet Processes.

Journal of the American Statistical Association  101(476):1566–1581  2007.

[19] Y.W. Teh  K. Kurihara  and Max Welling. Collapsed Variational Inference for HDP. In Proc. of NIPS’07 

volume 20  2007.

[20] Chong Wang and David Blei. A split-merge mcmc algorithm for the hierarchical dirichlet process. arXiv

eprints  1201.1657  2012.

[21] Chong Wang and David Blei. Truncation-free stochastic variational inference for bayesian nonparametric

models. In Proc. of NIPS’12  2012.

[22] Chong Wang and David M Blei. Variational Inference for the Nested Chinese Restaurant Process. In Proc.

of NIPS’09  2009.

[23] Chong Wang  John Paisley  and David Blei. Online variational inference for the hierarchical dirichlet

process. In AISTATS’11  2011.

[24] J. Xiao  J. Hays  K. Ehinger  A. Oliva  and A. Torralba. Sun database: Large-scale scene recognition from

abbey to zoo. In Proc. of CVPR’10  2010.

9

,Dahua Lin
Aryeh Kontorovich
Pinhas Nisnevitch