2017,Hindsight Experience Replay,Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular  we run experiments on three different tasks: pushing  sliding  and pick-and-place  in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.,Hindsight Experience Replay

Marcin Andrychowicz⇤  Filip Wolski  Alex Ray  Jonas Schneider  Rachel Fong 
Peter Welinder  Bob McGrew  Josh Tobin  Pieter Abbeel†  Wojciech Zaremba†

OpenAI

Abstract

Dealing with sparse rewards is one of the biggest challenges in Reinforcement
Learning (RL). We present a novel technique called Hindsight Experience Replay
which allows sample-efﬁcient learning from rewards which are sparse and binary
and therefore avoid the need for complicated reward engineering. It can be com-
bined with an arbitrary off-policy RL algorithm and may be seen as a form of
implicit curriculum.
We demonstrate our approach on the task of manipulating objects with a robotic
arm. In particular  we run experiments on three different tasks: pushing  sliding 
and pick-and-place  in each case using only binary rewards indicating whether or
not the task is completed. Our ablation studies show that Hindsight Experience
Replay is a crucial ingredient which makes training possible in these challenging
environments. We show that our policies trained on a physics simulation can
be deployed on a physical robot and successfully complete the task. The video
presenting our experiments is available at https://goo.gl/SMrQnI.

Introduction

1
Reinforcement learning (RL) combined with neural networks has recently led to a wide range of
successes in learning policies for sequential decision-making problems. This includes simulated
environments  such as playing Atari games (Mnih et al.  2015)  and defeating the best human player
at the game of Go (Silver et al.  2016)  as well as robotic tasks such as helicopter control (Ng et al. 
2006)  hitting a baseball (Peters and Schaal  2008)  screwing a cap onto a bottle (Levine et al.  2015) 
or door opening (Chebotar et al.  2016).
However  a common challenge  especially for robotics  is the need to engineer a reward function
that not only reﬂects the task at hand but is also carefully shaped (Ng et al.  1999) to guide the
policy optimization. For example  Popov et al. (2017) use a cost function consisting of ﬁve relatively
complicated terms which need to be carefully weighted in order to train a policy for stacking a
brick on top of another one. The necessity of cost engineering limits the applicability of RL in the
real world because it requires both RL expertise and domain-speciﬁc knowledge. Moreover  it is
not applicable in situations where we do not know what admissible behaviour may look like. It is
therefore of great practical relevance to develop algorithms which can learn from unshaped reward
signals  e.g. a binary signal indicating successful task completion.
One ability humans have  unlike the current generation of model-free RL algorithms  is to learn
almost as much from achieving an undesired outcome as from the desired one. Imagine that you are
learning how to play hockey and are trying to shoot a puck into a net. You hit the puck but it misses
the net on the right side. The conclusion drawn by a standard RL algorithm in such a situation would
be that the performed sequence of actions does not lead to a successful shot  and little (if anything)

⇤ marcin@openai.com
† Equal advising.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

would be learned. It is however possible to draw another conclusion  namely that this sequence of
actions would be successful if the net had been placed further to the right.
In this paper we introduce a technique called Hindsight Experience Replay (HER) which allows the
algorithm to perform exactly this kind of reasoning and can be combined with any off-policy RL
algorithm. It is applicable whenever there are multiple goals which can be achieved  e.g. achieving
each state of the system may be treated as a separate goal. Not only does HER improve the sample
efﬁciency in this setting  but more importantly  it makes learning possible even if the reward signal is
sparse and binary. Our approach is based on training universal policies (Schaul et al.  2015a) which
take as input not only the current state  but also a goal state. The pivotal idea behind HER is to replay
each episode with a different goal than the one the agent was trying to achieve  e.g. one of the goals
which was achieved in the episode.
2 Background
2.1 Reinforcement Learning
We consider the standard reinforcement learning formalism consisting of an agent interacting with
an environment. To simplify the exposition we assume that the environment is fully observable.
An environment is described by a set of states S  a set of actions A  a distribution of initial states
p(s0)  a reward function r : S ⇥ A ! R  transition probabilities p(st+1|st  at)  and a discount factor
 2 [0  1].
A deterministic policy is a mapping from states to actions: ⇡ : S ! A. Every episode starts with
sampling an initial state s0. At every timestep t the agent produces an action based on the current state:
at = ⇡(st). Then it gets the reward rt = r(st  at) and the environment’s new state is sampled from
the distribution p(·|st  at). A discounted sum of future rewards is called a return: Rt =P1i=t itri.
The agent’s goal is to maximize its expected return Es0[R0|s0]. The Q-function or action-value
function is deﬁned as Q⇡(st  at) = E[Rt|st  at].
Let ⇡⇤ denote an optimal policy i.e. any policy ⇡⇤ s.t. Q⇡⇤(s  a)  Q⇡(s  a) for every s 2 S  a 2 A
and any policy ⇡. All optimal policies have the same Q-function which is called optimal Q-function
and denoted Q⇤. It is easy to show that it satisﬁes the following equation called the Bellman equation:

Q⇤(s0  a0) .

Q⇤(s  a) = Es0⇠p(·|s a)r(s  a) +  max

a02A

2.2 Deep Q-Networks (DQN)
Deep Q-Networks (DQN) (Mnih et al.  2015) is a model-free RL algorithm for discrete action
spaces. Here we sketch it only informally  see Mnih et al. (2015) for more details. In DQN we
maintain a neural network Q which approximates Q⇤. A greedy policy w.r.t. Q is deﬁned as
⇡Q(s) = argmaxa2AQ(s  a). An ✏-greedy policy w.r.t. Q is a policy which with probability ✏ takes
a random action (sampled uniformly from A) and takes the action ⇡Q(s) with probability 1  ✏.
the current approximation of
During training we generate episodes using ✏-greedy policy w.r.t.
the action-value function Q. The transition tuples (st  at  rt  st+1) encountered during training are
stored in the so-called replay buffer. The generation of new episodes is interleaved with neural
network training. The network is trained using mini-batch gradient descent on the loss L which
encourages the approximated Q-function to satisfy the Bellman equation: L = E (Q(st  at)  yt)2 
where yt = rt +  maxa02A Q(st+1  a0) and the tuples (st  at  rt  st+1) are sampled from the replay
buffer1.
2.3 Deep Deterministic Policy Gradients (DDPG)
Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al.  2015) is a model-free RL algorithm
for continuous action spaces. Here we sketch it only informally  see Lillicrap et al. (2015) for more
details. In DDPG we maintain two neural networks: a target policy (also called an actor) ⇡ : S ! A
and an action-value function approximator (called the critic) Q : S ⇥ A ! R. The critic’s job is to
approximate the actor’s action-value function Q⇡.

1The targets yt depend on the network parameters but this dependency is ignored during backpropagation.
Moreover  DQN uses the so-called target network to make the optimization procedure more stable but we omit it
here as it is not relevant to our results.

2

Episodes are generated using a behavioral policy which is a noisy version of the target policy  e.g.
⇡b(s) = ⇡(s) + N (0  1). The critic is trained in a similar way as the Q-function in DQN but the
targets yt are computed using actions outputted by the actor  i.e. yt = rt + Q(st+1  ⇡(st+1)).
The actor is trained with mini-batch gradient descent on the loss La = EsQ(s  ⇡(s))  where s
is sampled from the replay buffer. The gradient of La w.r.t. actor parameters can be computed by
backpropagation through the combined critic and actor networks.
2.4 Universal Value Function Approximators (UVFA)
Universal Value Function Approximators (UVFA) (Schaul et al.  2015a) is an extension of DQN to
the setup where there is more than one goal we may try to achieve. Let G be the space of possible
goals. Every goal g 2 G corresponds to some reward function rg : S ⇥ A ! R. Every episode starts
with sampling a state-goal pair from some distribution p(s0  g). The goal stays ﬁxed for the whole
episode. At every timestep the agent gets as input not only the current state but also the current goal
⇡ : S ⇥ G ! A and gets the reward rt = rg(st  at). The Q-function now depends not only on a
state-action pair but also on a goal Q⇡(st  at  g) = E[Rt|st  at  g]. Schaul et al. (2015a) show that in
this setup it is possible to train an approximator to the Q-function using direct bootstrapping from the
Bellman equation (just like in case of DQN) and that a greedy policy derived from it can generalize
to previously unseen state-action pairs. The extension of this approach to DDPG is straightforward.
3 Hindsight Experience Replay
3.1 A motivating example
Consider a bit-ﬂipping environment with the state space S = {0  1}n and the action space A =
{0  1  . . .   n  1} for some integer n in which executing the i-th action ﬂips the i-th bit of the state.
For every episode we sample uniformly an initial state as well as a target state and the policy gets a
reward of 1 as long as it is not in the target state  i.e. rg(s  a) = [s 6= g].
Standard RL algorithms are bound to fail in this environment for
n > 40 because they will never experience any reward other than 1.
Notice that using techniques for improving exploration (e.g. VIME
(Houthooft et al.  2016)  count-based exploration (Ostrovski et al. 
2017) or bootstrapped DQN (Osband et al.  2016)) does not help
here because the real problem is not in lack of diversity of states
being visited  rather it is simply impractical to explore such a large
state space. The standard solution to this problem would be to use
a shaped reward function which is more informative and guides the
agent towards the goal  e.g. rg(s  a) = ||s  g||2. While using a
shaped reward solves the problem in our toy environment  it may be
difﬁcult to apply to more complicated problems. We investigate the
results of reward shaping experimentally in Sec. 4.4.
Instead of shaping the reward we propose a different solution which does not require any domain
knowledge. Consider an episode with a state sequence s1  . . .   sT and a goal g 6= s1  . . .   sT which
implies that the agent received a reward of 1 at every timestep. The pivotal idea behind our approach
is to re-examine this trajectory with a different goal — while this trajectory may not help us learn
how to achieve the state g  it deﬁnitely tells us something about how to achieve the state sT . This
information can be harvested by using an off-policy RL algorithm and experience replay where we
replace g in the replay buffer by sT . In addition we can still replay with the original goal g left intact
in the replay buffer. With this modiﬁcation at least half of the replayed trajectories contain rewards
different from 1 and learning becomes much simpler. Fig. 1 compares the ﬁnal performance of
DQN with and without this additional replay technique which we call Hindsight Experience Replay
(HER). DQN without HER can only solve the task for n  13 while DQN with HER easily solves
the task for n up to 50. See Appendix A for the details of the experimental setup. Note that this
approach combined with powerful function approximators (e.g.  deep neural networks) allows the
agent to learn how to achieve the goal g even if it has never observed it during training.
We more formally describe our approach in the following sections.
3.2 Multi-goal RL
We are interested in training agents which learn to achieve multiple different goals. We follow the
approach from Universal Value Function Approximators (Schaul et al.  2015a)  i.e. we train policies

Figure 1: Bit-ﬂipping experi-
ment.

3

and value functions which take as input not only a state s 2 S but also a goal g 2 G. Moreover  we
show that training an agent to perform multiple tasks can be easier than training it to perform only
one task (see Sec. 4.3 for details) and therefore our approach may be applicable even if there is only
one task we would like the agent to perform (a similar situation was recently observed by Pinto and
Gupta (2016)).
We assume that every goal g 2 G corresponds to some predicate fg : S ! {0  1} and that the agent’s
goal is to achieve any state s that satisﬁes fg(s) = 1. In the case when we want to exactly specify the
desired state of the system we may use S = G and fg(s) = [s = g]. The goals can also specify only
some properties of the state  e.g. suppose that S = R2 and we want to be able to achieve an arbitrary
state with the given value of x coordinate. In this case G = R and fg((x  y)) = [x = g].
Moreover  we assume that given a state s we can easily ﬁnd a goal g which is satisﬁed in this state.
More formally  we assume that there is given a mapping m : S ! G s.t. 8s2Sfm(s)(s) = 1. Notice
that this assumption is not very restrictive and can usually be satisﬁed. In the case where each goal
corresponds to a state we want to achieve  i.e. G = S and fg(s) = [s = g]  the mapping m is just an
identity. For the case of 2-dimensional state and 1-dimensional goals from the previous paragraph
this mapping is also very simple m((x  y)) = x.
A universal policy can be trained using an arbitrary RL algorithm by sampling goals and initial states
from some distributions  running the agent for some number of timesteps and giving it a negative
reward at every timestep when the goal is not achieved  i.e. rg(s  a) = [fg(s) = 0]. This does not
however work very well in practice because this reward function is sparse and not very informative.
In order to solve this problem we introduce the technique of Hindsight Experience Replay which is
the crux of our approach.
3.3 Algorithm
The idea behind Hindsight Experience Replay (HER) is very simple: after experiencing some episode
s0  s1  . . .   sT we store in the replay buffer every transition st ! st+1 not only with the original
goal used for this episode but also with a subset of other goals. Notice that the goal being pursued
inﬂuences the agent’s actions but not the environment dynamics and therefore we can replay each
trajectory with an arbitrary goal assuming that we use an off-policy RL algorithm like DQN (Mnih
et al.  2015)  DDPG (Lillicrap et al.  2015)  NAF (Gu et al.  2016) or SDQN (Metz et al.  2017).
One choice which has to be made in order to use HER is the set of additional goals used for replay.
In the simplest version of our algorithm we replay each trajectory with the goal m(sT )  i.e. the goal
which is achieved in the ﬁnal state of the episode. We experimentally compare different types and
quantities of additional goals for replay in Sec. 4.5. In all cases we also replay each trajectory with
the original goal pursued in the episode. See Alg. 1 for a more formal description of the algorithm.
HER may be seen as a form of implicit curriculum as the goals used for replay naturally shift from
ones which are simple to achieve even by a random agent to more difﬁcult ones. However  in contrast
to explicit curriculum  HER does not require having any control over the distribution of initial
environment states. Not only does HER learn with extremely sparse rewards  in our experiments
it also performs better with sparse rewards than with shaped ones (See Sec. 4.4). These results are
indicative of the practical challenges with reward shaping  and that shaped rewards would often
constitute a compromise on the metric we truly care about (such as binary success/failure).
4 Experiments
The video presenting our experiments is available at https://goo.gl/SMrQnI.
4.1 Environments
The are no standard environments for multi-goal RL and therefore we created our own environments.
We decided to use manipulation environments based on an existing hardware robot to ensure that the
challenges we face correspond as closely as possible to the real world. In all experiments we use a
7-DOF Fetch Robotics arm which has a two-ﬁngered parallel gripper. The robot is simulated using
the MuJoCo (Todorov et al.  2012) physics engine. The whole training procedure is performed in
the simulation but we show in Sec. 4.6 that the trained policies perform well on the physical robot
without any ﬁnetuning.
Policies are represented as Multi-Layer Perceptrons (MLPs) with Rectiﬁed Linear Unit (ReLU)
activation functions. Training is performed using the DDPG algorithm (Lillicrap et al.  2015) with

4

Algorithm 1 Hindsight Experience Replay (HER)

Given:

• an off-policy RL algorithm A 
• a strategy S for sampling goals for replay 
• a reward function r : S ⇥ A ⇥ G ! R.

Initialize A
Initialize replay buffer R
for episode = 1  M do

Sample a goal g and an initial state s0.
for t = 0  T  1 do

Sample an action at using the behavioral policy from A:

at ⇡b(st||g)

Execute the action at and observe a new state st+1

end for
for t = 0  T  1 do
rt := r(st  at  g)
Store the transition (st||g  at  rt  st+1||g) in R
Sample a set of additional goals for replay G := S(current episode)
for g0 2 G do

r0 := r(st  at  g0)
Store the transition (st||g0  at  r0  st+1||g0) in R

Sample a minibatch B from the replay buffer R
Perform one step of optimization using A and minibatch B

end for

end for
for t = 1  N do

end for

end for

. e.g. DQN  DDPG  NAF  SDQN
. e.g. S(s0  . . .   sT ) = m(sT )
. e.g. r(s  a  g) = [fg(s) = 0]
. e.g. initialize neural networks

. || denotes concatenation

. standard experience replay

. HER

Adam (Kingma and Ba  2014) as the optimizer. See Appendix A for more details and the values of all
hyperparameters.
We consider 3 different tasks:

1. Pushing. In this task a box is placed on a table in front of the robot and the task is to move
it to the target location on the table. The robot ﬁngers are locked to prevent grasping. The
learned behaviour is a mixture of pushing and rolling.

2. Sliding. In this task a puck is placed on a long slippery table and the target position is outside
of the robot’s reach so that it has to hit the puck with such a force that it slides and then
stops in the appropriate place due to friction.

3. Pick-and-place. This task is similar to pushing but the target position is in the air and the
ﬁngers are not locked. To make exploration in this task easier we recorded a single state in
which the box is grasped and start half of the training episodes from this state2.

The images showing the tasks being performed can be found in Appendix C.
States: The state of the system is represented in the MuJoCo physics engine.
Goals: Goals describe the desired position of the object (a box or a puck depending on the task) with
some ﬁxed tolerance of ✏ i.e. G = R3 and fg(s) = [|g  sobject|  ✏]  where sobject is the position
of the object in the state s. The mapping from states to goals used in HER is simply m(s) = sobject.
Rewards: Unless stated otherwise we use binary and sparse rewards r(s  a  g) = [fg(s0) = 0]
where s0 if the state after the execution of the action a in the state s. We compare sparse and shaped
reward functions in Sec. 4.4.
State-goal distributions: For all tasks the initial position of the gripper is ﬁxed  while the initial
position of the object and the target are randomized. See Appendix A for details.
Observations: In this paragraph relative means relative to the current gripper position. The policy is
2This was necessary because we could not successfully train any policies for this task without using the
demonstration state. We have later discovered that training is possible without this trick if only the goal position
is sometimes on the table and sometimes in the air.

5

given as input the absolute position of the gripper  the relative position of the object and the target3 
as well as the distance between the ﬁngers. The Q-function is additionally given the linear velocity of
the gripper and ﬁngers as well as relative linear and angular velocity of the object. We decided to
restrict the input to the policy in order to make deployment on the physical robot easier.
Actions: None of the problems we consider require gripper rotation and therefore we keep it ﬁxed.
Action space is 4-dimensional. Three dimensions specify the desired relative gripper position at
the next timestep. We use MuJoCo constraints to move the gripper towards the desired position but
Jacobian-based control could be used instead4. The last dimension speciﬁes the desired distance
between the 2 ﬁngers which are position controlled.
Strategy S for sampling goals for replay: Unless stated otherwise HER uses replay with the goal
corresponding to the ﬁnal state in each episode  i.e. S(s0  . . .   sT ) = m(sT ). We compare different
strategies for choosing which goals to replay with in Sec. 4.5.
4.2 Does HER improve performance?
In order to verify if HER improves performance we evaluate DDPG with and without HER on all
3 tasks. Moreover  we compare against DDPG with count-based exploration5 (Strehl and Littman 
2005; Kolter and Ng  2009; Tang et al.  2016; Bellemare et al.  2016; Ostrovski et al.  2017). For
HER we store each transition in the replay buffer twice: once with the goal used for the generation
of the episode and once with the goal corresponding to the ﬁnal state from the episode (we call this
strategy final). In Sec. 4.5 we perform ablation studies of different strategies S for choosing goals
for replay  here we include the best version from Sec. 4.5 in the plot for comparison.

Figure 2: Multiple goals.

Figure 3: Single goal.

Fig. 2 shows the learning curves for all 3 tasks6. DDPG without HER is unable to solve any of the
tasks7 and DDPG with count-based exploration is only able to make some progress on the sliding
task. On the other hand  DDPG with HER solves all tasks almost perfectly. It conﬁrms that HER is a
crucial element which makes learning from sparse  binary rewards possible.
4.3 Does HER improve performance even if there is only one goal we care about?
In this section we evaluate whether HER improves performance in the case where there is only one
goal we care about. To this end  we repeat the experiments from the previous section but the goal
state is identical in all episodes.
From Fig. 3 it is clear that DDPG+HER performs much better than pure DDPG even if the goal state
is identical in all episodes. More importantly  comparing Fig. 2 and Fig. 3 we can also notice that
HER learns faster if training episodes contain multiple goals  so in practice it is advisable to train on
multiple goals even if we care only about one of them.

movements which are reproducible on the physical robot despite not being fully physically plausible.

3The target position is relative to the current object position.
4The successful deployment on a physical robot (Sec. 4.6) conﬁrms that our control model produces
5 We discretize the state space and use an intrinsic reward of the form ↵/pN  where ↵ is a hyper-
parameter and N is the number of times the given state was visited. The discretization works as fol-
lows. We take the relative position of the box and the target and then discretize every coordinate using
a grid with a stepsize  which is a hyperparameter. We have performed a hyperparameter search over
↵ 2 {0.032  0.064  0.125  0.25  0.5  1  2  4  8  16  32}   2 {1cm  2cm  4cm  8cm}. The best results were
obtained using ↵ = 1 and  = 1cm and these are the results we report.
6An episode is considered successful if the distance between the object and the goal at the end of the episode
is less than 7cm for pushing and pick-and-place and less than 20cm for sliding. The results are averaged across 5
random seeds and shaded areas represent one standard deviation.

7We also evaluated DQN (without HER) on our tasks and it was not able to solve any of them.

6

Figure 4: Ablation study of different strategies for choosing additional goals for replay. The top row
shows the highest (across the training epochs) test performance and the bottom row shows the average
test performance across all training epochs. On the right top plot the curves for final  episode and
future coincide as all these strategies achieve perfect performance on this task.

4.4 How does HER interact with reward shaping?
So far we only considered binary rewards of the form r(s  a  g) = [|g  sobject| > ✏]. In this
section we check how the performance of DDPG with and without HER changes if we replace
this reward with one which is shaped. We considered reward functions of the form r(s  a  g) =
|g  sobject|p  |g  s0object|p  where s0 is the state of the environment after the execution of the
action a in the state s and  2 {0  1}  p 2 {1  2} are hyperparameters.
Surprisingly neither DDPG  nor DDPG+HER was able to successfully solve any of the tasks with any
of these reward functions8(learning curves can be found in Appendix D). Our results are consistent
with the fact that successful applications of RL to difﬁcult manipulation tasks which does not use
demonstrations usually have more complicated reward functions than the ones we tried (e.g. Popov
et al. (2017)).
The following two reasons can cause shaped rewards to perform so poorly: (1) There is a huge
discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e.:
is the object within some radius from the goal at the end of the episode); (2) Shaped rewards penalize
for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration.
It can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we
noticed such behaviour in some of our experiments.
Our results suggest that domain-agnostic reward shaping does not work well (at least in the simple
forms we have tried). Of course for every problem there exists a reward which makes it easy (Ng
et al.  1999) but designing such shaped rewards requires a lot of domain knowledge and may in some
cases not be much easier than directly scripting the policy. This strengthens our belief that learning
from sparse  binary rewards is an important problem.

4.5 How many goals should we replay each trajectory with and how to choose them?
In this section we experimentally evaluate different strategies (i.e. S in Alg. 1) for choosing goals
to use with HER. So far the only additional goals we used for replay were the ones corresponding
to the ﬁnal state of the environment and we will call this strategy final. Apart from it we consider
the following strategies: future — replay with k random states which come from the same episode
as the transition being replayed and were observed after it  episode — replay with k random
states coming from the same episode as the transition being replayed  random — replay with k
random states encountered so far in the whole training procedure. All of these strategies have a
hyperparameter k which controls the ratio of HER data to data coming from normal experience replay
in the replay buffer.

8We also tried to rescale the distances  so that the range of rewards is similar as in the case of binary rewards 
clipping big distances and adding a simple (linear or quadratic) term encouraging the gripper to move towards
the object but none of these techniques have led to successful training.

7

Figure 5: The pick-and-place policy deployed on the physical robot.

The plots comparing different strategies and different values of k can be found in Fig. 4. We can
see from the plots that all strategies apart from random solve pushing and pick-and-place almost
perfectly regardless of the values of k. In all cases future with k equal 4 or 8 performs best and it
is the only strategy which is able to solve the sliding task almost perfectly. The learning curves for
future with k = 4 can be found in Fig. 2. It conﬁrms that the most valuable goals for replay are the
ones which are going to be achieved in the near future9. Notice that increasing the values of k above
8 degrades performance because the fraction of normal replay data in the buffer becomes very low.
4.6 Deployment on a physical robot
We took a policy for the pick-and-place task trained in the simulator (version with the future strategy
and k = 4 from Sec. 4.5) and deployed it on a physical fetch robot without any ﬁnetuning. The box
position was predicted using a separately trained CNN using raw fetch head camera images. See
Appendix B for details.
Initially the policy succeeded in 2 out of 5 trials. It was not robust to small errors in the box position
estimation because it was trained on perfect state coming from the simulation. After retraining the
policy with gaussian noise (std=1cm) added to observations10 the success rate increased to 5/5. The
video showing some of the trials is available at https://goo.gl/SMrQnI.
5 Related work
The technique of experience replay has been introduced in Lin (1992) and became very popular
after it was used in the DQN agent playing Atari (Mnih et al.  2015). Prioritized experience replay
(Schaul et al.  2015b) is an improvement to experience replay which prioritizes transitions in the
replay buffer in order to speed up training. It it orthogonal to our work and both approaches can be
easily combined.
Learning simultaneously policies for multiple tasks have been heavily explored in the context of
policy search  e.g. Schmidhuber and Huber (1990); Caruana (1998); Da Silva et al. (2012); Kober et al.
(2012); Devin et al. (2016); Pinto and Gupta (2016). Learning off-policy value functions for multiple
tasks was investigated by Foster and Dayan (2002) and Sutton et al. (2011). Our work is most heavily
based on Schaul et al. (2015a) who considers training a single neural network approximating multiple
value functions. Learning simultaneously to perform multiple tasks has been also investigated for
a long time in the context of Hierarchical Reinforcement Learning  e.g. Bakker and Schmidhuber
(2004); Vezhnevets et al. (2017).
Our approach may be seen as a form of implicit curriculum learning (Elman  1993; Bengio et al. 
2009). While curriculum is now often used for training neural networks (e.g. Zaremba and Sutskever
(2014); Graves et al. (2016))  the curriculum is almost always hand-crafted. The problem of automatic
curriculum generation was approached by Schmidhuber (2004) who constructed an asymptotically
optimal algorithm for this problem using program search. Another interesting approach is PowerPlay
(Schmidhuber  2013; Srivastava et al.  2013) which is a general framework for automatic task selection.
Graves et al. (2017) consider a setup where there is a ﬁxed discrete set of tasks and empirically
evaluate different strategies for automatic curriculum generation in this settings. Another approach
investigated by Sukhbaatar et al. (2017) and Held et al. (2017) uses self-play between the policy and
a task-setter in order to automatically generate goal states which are on the border of what the current
policy can achieve. Our approach is orthogonal to these techniques and can be combined with them.

9We have also tried replaying the goals which are close to the ones achieved in the near future but it has not

performed better than the future strategy

10The Q-function approximator was trained using exact observations. It does not have to be robust to noisy

observations because it is not used during the deployment on the physical robot.

8

6 Conclusions
We introduced a novel technique called Hindsight Experience Replay which makes possible applying
RL algorithms to problems with sparse and binary rewards. Our technique can be combined with an
arbitrary off-policy RL algorithm and we experimentally demonstrated that with DQN and DDPG.
We showed that HER allows training policies which push  slide and pick-and-place objects with a
robotic arm to the speciﬁed positions while the vanilla RL algorithm fails to solve these tasks. We
also showed that the policy for the pick-and-place task performs well on the physical robot without
any ﬁnetuning. As far as we know  it is the ﬁrst time so complicated behaviours were learned using
only sparse  binary rewards.

Acknowledgments
We would like to thank Ankur Handa  Jonathan Ho  John Schulman  Matthias Plappert  Tim Salimans 
and Vikash Kumar for providing feedback on the previous versions of this manuscript. We would
also like to thank Rein Houthooft and the whole OpenAI team for fruitful discussions as well as
Bowen Baker for performing some additional experiments.

References
Abadi  M.  Agarwal  A.  Barham  P.  Brevdo  E.  Chen  Z.  Citro  C.  Corrado  G. S.  Davis  A.  Dean  J.  Devin 
M.  et al. (2016). Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv
preprint arXiv:1603.04467.

Bakker  B. and Schmidhuber  J. (2004). Hierarchical reinforcement learning based on subgoal discovery and

subpolicy specialization. In Proc. of the 8-th Conf. on Intelligent Autonomous Systems  pages 438–445.

Bellemare  M.  Srinivasan  S.  Ostrovski  G.  Schaul  T.  Saxton  D.  and Munos  R. (2016). Unifying count-
based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems  pages
1471–1479.

Bengio  Y.  Louradour  J.  Collobert  R.  and Weston  J. (2009). Curriculum learning. In Proceedings of the 26th

annual international conference on machine learning  pages 41–48. ACM.

Caruana  R. (1998). Multitask learning. In Learning to learn  pages 95–133. Springer.

Chebotar  Y.  Kalakrishnan  M.  Yahya  A.  Li  A.  Schaal  S.  and Levine  S. (2016). Path integral guided policy

search. arXiv preprint arXiv:1610.00529.

Da Silva  B.  Konidaris  G.  and Barto  A. (2012). Learning parameterized skills. arXiv preprint arXiv:1206.6398.

Devin  C.  Gupta  A.  Darrell  T.  Abbeel  P.  and Levine  S. (2016). Learning modular neural network policies

for multi-task and multi-robot transfer. arXiv preprint arXiv:1609.07088.

Elman  J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition 

48(1):71–99.

Foster  D. and Dayan  P. (2002). Structure in the space of value functions. Machine Learning  49(2):325–346.

Graves  A.  Bellemare  M. G.  Menick  J.  Munos  R.  and Kavukcuoglu  K. (2017). Automated curriculum

learning for neural networks. arXiv preprint arXiv:1704.03003.

Graves  A.  Wayne  G.  Reynolds  M.  Harley  T.  Danihelka  I.  Grabska-Barwi´nska  A.  Colmenarejo  S. G. 
Grefenstette  E.  Ramalho  T.  Agapiou  J.  et al. (2016). Hybrid computing using a neural network with
dynamic external memory. Nature  538(7626):471–476.

Gu  S.  Lillicrap  T.  Sutskever  I.  and Levine  S. (2016). Continuous deep q-learning with model-based

acceleration. arXiv preprint arXiv:1603.00748.

Held  D.  Geng  X.  Florensa  C.  and Abbeel  P. (2017). Automatic goal generation for reinforcement learning

agents. arXiv preprint arXiv:1705.06366.

Houthooft  R.  Chen  X.  Duan  Y.  Schulman  J.  De Turck  F.  and Abbeel  P. (2016). Vime: Variational
information maximizing exploration. In Advances in Neural Information Processing Systems  pages 1109–
1117.

Kingma  D. and Ba  J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.

9

Kober  J.  Wilhelm  A.  Oztop  E.  and Peters  J. (2012). Reinforcement learning to adjust parametrized motor

primitives to new situations. Autonomous Robots  33(4):361–379.

Kolter  J. Z. and Ng  A. Y. (2009). Near-bayesian exploration in polynomial time. In Proceedings of the 26th

Annual International Conference on Machine Learning  pages 513–520. ACM.

Levine  S.  Finn  C.  Darrell  T.  and Abbeel  P. (2015). End-to-end training of deep visuomotor policies. arXiv

preprint arXiv:1504.00702.

Lillicrap  T. P.  Hunt  J. J.  Pritzel  A.  Heess  N.  Erez  T.  Tassa  Y.  Silver  D.  and Wierstra  D. (2015).

Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

Lin  L.-J. (1992). Self-improving reactive agents based on reinforcement learning  planning and teaching.

Machine learning  8(3-4):293–321.

Metz  L.  Ibarz  J.  Jaitly  N.  and Davidson  J. (2017). Discrete sequential prediction of continuous actions for

deep rl. arXiv preprint arXiv:1705.05035.

Mnih  V.  Kavukcuoglu  K.  Silver  D.  Rusu  A. A.  Veness  J.  Bellemare  M. G.  Graves  A.  Riedmiller  M. 
Fidjeland  A. K.  Ostrovski  G.  et al. (2015). Human-level control through deep reinforcement learning.
Nature  518(7540):529–533.

Ng  A.  Coates  A.  Diel  M.  Ganapathi  V.  Schulte  J.  Tse  B.  Berger  E.  and Liang  E. (2006). Autonomous

inverted helicopter ﬂight via reinforcement learning. Experimental Robotics IX  pages 363–372.

Ng  A. Y.  Harada  D.  and Russell  S. (1999). Policy invariance under reward transformations: Theory and

application to reward shaping. In ICML  volume 99  pages 278–287.

Osband  I.  Blundell  C.  Pritzel  A.  and Van Roy  B. (2016). Deep exploration via bootstrapped dqn. In

Advances In Neural Information Processing Systems  pages 4026–4034.

Ostrovski  G.  Bellemare  M. G.  Oord  A. v. d.  and Munos  R. (2017). Count-based exploration with neural

density models. arXiv preprint arXiv:1703.01310.

Peters  J. and Schaal  S. (2008). Reinforcement learning of motor skills with policy gradients. Neural networks 

21(4):682–697.

Pinto  L. and Gupta  A. (2016). Learning to push by grasping: Using multiple tasks for effective learning. arXiv

preprint arXiv:1609.09025.

Popov  I.  Heess  N.  Lillicrap  T.  Hafner  R.  Barth-Maron  G.  Vecerik  M.  Lampe  T.  Tassa  Y.  Erez  T.  and
Riedmiller  M. (2017). Data-efﬁcient deep reinforcement learning for dexterous manipulation. arXiv preprint
arXiv:1704.03073.

Schaul  T.  Horgan  D.  Gregor  K.  and Silver  D. (2015a). Universal value function approximators.
Proceedings of the 32nd International Conference on Machine Learning (ICML-15)  pages 1312–1320.

In

Schaul  T.  Quan  J.  Antonoglou  I.  and Silver  D. (2015b). Prioritized experience replay. arXiv preprint

arXiv:1511.05952.

Schmidhuber  J. (2004). Optimal ordered problem solver. Machine Learning  54(3):211–254.

Schmidhuber  J. (2013). Powerplay: Training an increasingly general problem solver by continually searching

for the simplest still unsolvable problem. Frontiers in psychology  4.

Schmidhuber  J. and Huber  R. (1990). Learning to generate focus trajectories for attentive vision. Institut für

Informatik.

Silver  D.  Huang  A.  Maddison  C. J.  Guez  A.  Sifre  L.  Van Den Driessche  G.  Schrittwieser  J.  Antonoglou 
I.  Panneershelvam  V.  Lanctot  M.  et al. (2016). Mastering the game of go with deep neural networks and
tree search. Nature  529(7587):484–489.

Srivastava  R. K.  Steunebrink  B. R.  and Schmidhuber  J. (2013). First experiments with powerplay. Neural

Networks  41:130–136.

Strehl  A. L. and Littman  M. L. (2005). A theoretical analysis of model-based interval estimation. In Proceedings

of the 22nd international conference on Machine learning  pages 856–863. ACM.

Sukhbaatar  S.  Kostrikov  I.  Szlam  A.  and Fergus  R. (2017). Intrinsic motivation and automatic curricula via

asymmetric self-play. arXiv preprint arXiv:1703.05407.

10

Sutton  R. S.  Modayil  J.  Delp  M.  Degris  T.  Pilarski  P. M.  White  A.  and Precup  D. (2011). Horde: A
scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The
10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2  pages 761–768.
International Foundation for Autonomous Agents and Multiagent Systems.

Tang  H.  Houthooft  R.  Foote  D.  Stooke  A.  Chen  X.  Duan  Y.  Schulman  J.  De Turck  F.  and Abbeel  P.
(2016). # exploration: A study of count-based exploration for deep reinforcement learning. arXiv preprint
arXiv:1611.04717.

Tobin  J.  Fong  R.  Ray  A.  Schneider  J.  Zaremba  W.  and Abbeel  P. (2017). Domain randomization for

transferring deep neural networks from simulation to the real world. arXiv preprint arXiv:1703.06907.

Todorov  E.  Erez  T.  and Tassa  Y. (2012). Mujoco: A physics engine for model-based control. In Intelligent

Robots and Systems (IROS)  2012 IEEE/RSJ International Conference on  pages 5026–5033. IEEE.

Vezhnevets  A. S.  Osindero  S.  Schaul  T.  Heess  N.  Jaderberg  M.  Silver  D.  and Kavukcuoglu  K. (2017).

Feudal networks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161.

Zaremba  W. and Sutskever  I. (2014). Learning to execute. arXiv preprint arXiv:1410.4615.

11

,Marcin Andrychowicz
Filip Wolski
Alex Ray
Jonas Schneider
Rachel Fong
Peter Welinder
Bob McGrew
Josh Tobin
OpenAI Pieter Abbeel
Wojciech Zaremba