2007,A New View of Automatic Relevance Determination,Automatic relevance determination (ARD)  and the closely-related sparse Bayesian learning (SBL) framework  are effective tools for pruning large numbers of irrelevant features. However  popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties. This paper furnishes an alternative means of optimizing a general ARD cost function using an auxiliary function that can naturally be solved using a series of re-weighted L1 problems. The result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods. The analysis also leads to additional insights into the behavior of previous ARD updates as well as the ARD cost function. For example  the standard fixed-point updates of MacKay (1992) are shown to be iteratively solving a particular min-max problem  although they are not guaranteed to lead to a stationary point. The analysis also reveals that ARD is exactly equivalent to performing MAP estimation using a particular feature- and noise-dependent \textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection. In particular  it provides a tighter approximation to the L0 quasi-norm sparsity measure than the L1 norm. Overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.,A New View of Automatic Relevance Determination

David Wipf and Srikantan Nagarajan  (cid:3)
Biomagnetic Imaging Lab  UC San Francisco
fdavid.wipf  srig@mrsc.ucsf.edu

Abstract

Automatic relevance determination (ARD) and the closely-related sparse
Bayesian learning (SBL) framework are effective tools for pruning large numbers
of irrelevant features leading to a sparse explanatory subset. However  popular up-
date rules used for ARD are either dif(cid:2)cult to extend to more general problems of
interest or are characterized by non-ideal convergence properties. Moreover  it re-
mains unclear exactly how ARD relates to more traditional MAP estimation-based
methods for learning sparse representations (e.g.  the Lasso). This paper furnishes
an alternative means of expressing the ARD cost function using auxiliary func-
tions that naturally addresses both of these issues. First  the proposed reformu-
lation of ARD can naturally be optimized by solving a series of re-weighted ‘1
problems. The result is an ef(cid:2)cient  extensible algorithm that can be implemented
using standard convex programming toolboxes and is guaranteed to converge to
a local minimum (or saddle point). Secondly  the analysis reveals that ARD is
exactly equivalent to performing standard MAP estimation in weight space using
a particular feature- and noise-dependent  non-factorial weight prior. We then
demonstrate that this implicit prior maintains several desirable advantages over
conventional priors with respect to feature selection. Overall these results suggest
alternative cost functions and update procedures for selecting features and promot-
ing sparse solutions in a variety of general situations. In particular  the method-
ology readily extends to handle problems such as non-negative sparse coding and
covariance component estimation.

1 Introduction
Here we will be concerned with the generative model

y = (cid:8)x + (cid:15);

(1)
where (cid:8) 2 Rn(cid:2)m is a dictionary of features  x 2 Rm is a vector of unknown weights  y is an
observation vector  and (cid:15) is uncorrelated noise distributed as N ((cid:15); 0; (cid:21)I). When large numbers
of features are present relative to the signal dimension  the estimation problem is fundamentally
ill-posed. Automatic relevance determination (ARD) addresses this problem by regularizing the
solution space using a parameterized  data-dependent prior distribution that effectively prunes away
redundant or super(cid:3)uous features [10]. Here we will describe a special case of ARD called sparse
Bayesian learning (SBL) that has been very successful in a variety of applications [15]. Later in
Section 4 we will address extensions to more general models.
The basic ARD prior incorporated by SBL is p(x; (cid:13)) = N (x; 0; diag[(cid:13)])  where (cid:13) 2 Rm
+ is a vector
of m non-negative hyperperparameters governing the prior variance of each unknown coef(cid:2)cient.
These hyperparameters are estimated from the data by (cid:2)rst marginalizing over the coef(cid:2)cients x
and then performing what is commonly referred to as evidence maximization or type-II maximum
likelihood [7  10  15]. Mathematically  this is equivalent to minimizing

L((cid:13))   (cid:0) logZ p(yjx)p(x; (cid:13))dx = (cid:0) log p(y; (cid:13)) (cid:17) log j(cid:6)yj + y

T (cid:6)(cid:0)1

y y;

(2)

(cid:3)This research was supported by NIH grants R01DC04855 and R01DC006435.

where a (cid:3)at hyperprior on (cid:13) is assumed  (cid:6)y   (cid:21)I + (cid:8)(cid:0)(cid:8)T   and (cid:0)   diag[(cid:13)]. Once some (cid:13)(cid:3) =
arg min(cid:13) L((cid:13)) is computed  an estimate of the unknown coef(cid:2)cients can be obtained by setting
xARD to the posterior mean computed using (cid:13)(cid:3):

y(cid:3) y:

xARD = E[xjy; (cid:13)(cid:3)] = (cid:0)(cid:3)(cid:8)T (cid:6)(cid:0)1

(3)
Note that if any (cid:13)(cid:3);i = 0  as often occurs during the learning process  then xARD;i = 0 and the
corresponding feature is effectively pruned from the model. The resulting weight vector xARD is
therefore sparse  with nonzero elements corresponding with the ‘relevant’ features.
There are (at least) two outstanding issues related to this model which we consider to be signi(cid:2)cant.
First  while several methods exist for optimizing (2)  limitations remain in each case. For example 
an EM version operates by treating the unknown x as hidden data  leading to the E-step
y y;

(cid:6)   Cov[xjy; (cid:13)] = (cid:0) (cid:0) (cid:0)(cid:8)T (cid:6)(cid:0)1

(cid:22)   E[xjy; (cid:13)] = (cid:0)(cid:8)T (cid:6)(cid:0)1

y (cid:8)(cid:0);

(4)

and the M-step

(5)
While convenient to implement  the convergence can be prohibitively slow in practice. In contrast 
the MacKay update rules are considerably faster to converge [15]. The idea here is to form the
gradient of (2)  equate to zero  and then form the (cid:2)xed-point update

8i = 1; : : : ; m:

(cid:13)i ! (cid:22)2

i + (cid:6)ii;

(cid:13)i !

(cid:22)2
i
1 (cid:0) (cid:13)(cid:0)1
i (cid:6)ii

;

8i = 1; : : : ; m:

(6)

However  neither the EM nor MacKay updates are guaranteed to converge to a local minimum or
even a saddle point of L((cid:13)); both have (cid:2)xed points whenever a (cid:13)i = 0  whether at a minimizing
solution or not. Finally  a third algorithm has recently been proposed that optimally updates a single
hyperparameter (cid:13)i at a time  which can be done very ef(cid:2)ciently in closed form [16]. While extremely
fast to implement  as a greedy-like method it can sometimes be more prone to becoming trapped in
local minima when the number of features is large  e.g.  m > n (results will be presented in a
forthcoming publication). Additionally  none of these methods are easily extended to more general
problems such as non-negative sparse coding  covariance component estimation  and classi(cid:2)cation
without introducing additional approximations.
A second issue pertaining to the ARD model involves its connection with more traditional maximum
a posteriori (MAP) estimation methods for extracting sparse  relevant features using (cid:2)xed  sparsity
promoting prior distributions (i.e.  heavy-tailed and peaked). Presently  it is unclear how ARD 
which invokes a parameterized prior and transfers the estimation problem to hyperparameter space 
relates to MAP approaches which operate directly in x space. Nor is it intuitively clear why ARD
often works better in selecting optimal feature sets.
This paper introduces an alternative formulation of the ARD cost function using auxiliary func-
tions that naturally addresses the above issues. In Section 2  the proposed reformulation of ARD is
conveniently optimized by solving a series of re-weighted ‘1 problems. The result is an ef(cid:2)cient al-
gorithm that can be implemented using standard convex programming methods and is guaranteed to
converge to a local minimum (or saddle point) of L((cid:13)). Section 3 then demonstrates that ARD is ex-
actly equivalent to performing standard MAP estimation in weight space using a particular feature-
and noise-dependent  non-factorial weight prior. We then show that this implicit prior maintains
several desirable advantages over conventional priors with respect to feature selection. Additionally 
these results suggest modi(cid:2)cations of ARD for selecting relevant features and promoting sparse so-
lutions in a variety of general situations. In particular  the methodology readily extends to handle
problems involving non-negative sparse coding  covariance component estimation  and classi(cid:2)cation
as discussed in Section 4.

2 ARD/SBL Optimization via Iterative Re-Weighted Minimum ‘1

In this section we re-express L((cid:13)) using auxiliary functions which leads to an alternative update
procedure that circumvents the limitations of current approaches. In fact  a wide variety of alterna-
tive update rules can be derived by decoupling L((cid:13)) using upper bounding functions that are more
conveniently optimized. Here we focus on a particular instantiation of this idea that leads to an
iterative minimum ‘1 procedure. The utility of this selection being that many powerful convex pro-
gramming toolboxes have already been developed for solving these types of problems  especially
when structured dictionaries (cid:8) are being used.

2.1 Algorithm Derivation

To start we note that the log-determinant term of L((cid:13)) is concave in (cid:13) (see Section 3.1.5 of [1]) 
and so can be expressed as a minimum over upper-bounding hyperplanes via

where g(cid:3)(z) is the concave conjugate of log j(cid:6)yj that is de(cid:2)ned by the duality relationship [1]

log j(cid:6)yj = min

z

T

z

(cid:13) (cid:0) g(cid:3)(z);

(7)

(8)
although for our purposes we will never actually compute g (cid:3)(z). This leads to the following upper-
bounding auxiliary cost function

g(cid:3)(z) = min

(cid:13) (cid:0) log j(cid:6)yj ;

z

T

(cid:13)

(9)
For any (cid:2)xed (cid:13)  the optimal (tightest) bound can be obtained by minimizing over z. The optimal
value of z equals the slope at the current (cid:13) of log j(cid:6)yj. Therefore  we have

y y (cid:21) L((cid:13)):

(cid:13) (cid:0) g(cid:3)(z) + y

L((cid:13); z)   z

T (cid:6)(cid:0)1

T

zopt = O

y (cid:8)(cid:3) :
This formulation naturally admits the following optimization scheme:
Step 1: Initialize each zi  e.g.  zi = 1; 8i.
Step 2: Solve the minimization problem

(cid:13) log j(cid:6)yj = diag(cid:2)(cid:8)T (cid:6)(cid:0)1

(cid:13) ! arg min

(cid:13)

Lz((cid:13))   z

T

(cid:13) + y

T (cid:6)(cid:0)1

y y:

(10)

(11)

Step 3: Compute the optimal z using (10).
Step 4: Iterate Steps 2 and 3 until convergence to some (cid:13)(cid:3).
Step 5: Compute xARD = E[xjy; (cid:13)(cid:3)] = (cid:0)(cid:3)(cid:8)T (cid:6)(cid:0)1

y(cid:3) y.
Lemma 1. The objective function in (11) is convex.
This can be shown using Example 3.4 and Section 3.2.2 in [1]. Lemma 1 implies that many standard
optimization procedures can be used for the minimization required by Step 2. For example  one
attractive option is to convert the problem to an equivalent least absolute shrinkage and selector
operator or ‘Lasso’ [14] optimization problem according to the following:
Lemma 2. The objective function in (11) can be minimized by solving the weighted convex ‘1-
regularized cost function

x(cid:3) = arg min

x

ky (cid:0) (cid:8)xk2

2 + 2(cid:21)Xi

z1=2
i

jxij

(12)

and then setting (cid:13)i ! z(cid:0)1=2
The proof of Lemma 2 can be brie(cid:3)y summarized using a re-expression of the data dependent term
in (11) using
(13)

jx(cid:3);ij for all i (note that each zi will always be positive).

x2
i
(cid:13)i
This leads to an upper-bounding auxiliary function for Lz((cid:13)) given by

2 +Xi

ky (cid:0) (cid:8)xk2

y y = min

T (cid:6)(cid:0)1

1
(cid:21)

y

:

i

x

Lz((cid:13); x)   Xi

(cid:18)zi(cid:13)i +

x2
i

(cid:13)i (cid:19) +

1
(cid:21)

ky (cid:0) (cid:8)xk2

2 (cid:21) Lz((cid:13));

(14)

i

which is jointly convex in x and (cid:13) (see Example 3.4 in [1]) and can be globally minimized by
jxij minimizes Lz((cid:13); x). When substituted into
solving over (cid:13) and then x. For any x  (cid:13)i = z(cid:0)1=2
(14) we obtain (12). When solved for x  the global minimum of (14) yields the global minimum of
(11) via the stated transformation.
In summary then  by iterating the above algorithm using Lemma 2 to implement Step 2  a conve-
nient optimization method is obtained. Moreover  we do not even need to globally solve for x (or
equivalently (cid:13)) at each iteration as long as we strictly reduce (11) at each iteration. This is read-
ily achievable using a variety of simple strategies. Additionally  if z is initialized to a vector of
ones  then the starting point (assuming Step 2 is computed in full) is the exact Lasso estimator. The
algorithm then re(cid:2)nes this estimate through the speci(cid:2)ed re-weighting procedure.

2.2 Global Convergence Analysis

00) < L((cid:13)

0) for all (cid:13)

+ the subset of Rm

+ which satis(cid:2)es
Let A((cid:1)) denote a mapping that assigns to every point in Rm
Steps 2 and 3 of the proposed algorithm. Such a mapping can be implemented via the methodology
described above. We allow A((cid:1)) to be a point-to-set mapping to handle the case where the global
minimum of (11) is not unique  which could occur  for example  if two columns of (cid:8) are identical.
Theorem 1. From any initialization point (cid:13)(0) 2 Rm
+ the sequence of hyperparameter estimates
f(cid:13)(k)g generated via (cid:13)(k+1) 2 A((cid:13)(k+1)) is guaranteed to converge monotonically to a local mini-
mum (or saddle point) of (2).
The proof is relatively straightforward and stems directly from the Global Convergence Theorem
(see for example [6]). A sketch is as follows: First  it must be shown that the the mapping A((cid:1))
is compact. This condition is satis(cid:2)ed because if any element of (cid:13) is unbounded  L((cid:13)) diverges to
in(cid:2)nity. If fact  for any (cid:2)xed y  (cid:8) and (cid:21)  there will always exist a radius r such that for any k(cid:13)(0)k (cid:20)
r  k(cid:13)(k)k (cid:20) r for all k. Second  we must show that for any non-minimizing point of L((cid:13)) denoted
0 the auxiliary cost function
0  L((cid:13)
(cid:13)
0 ((cid:13)) obtained from Step 3 will be strictly tangent to L((cid:13)) at (cid:13)
0. It will therefore necessarily have
Lz
0 is nonzero by de(cid:2)nition. Moreover  because the log j (cid:1) j
a minimum elsewhere since the slope at (cid:13)
function is strictly concave  at this minimum the actual cost function will be reduced still further.
Consequently  the proposed updates represent a valid descent function. Finally  it must be shown
that A((cid:1)) is closed at all non-stationary points. This follows from related arguments. The algorithm
could of course theoretically converge to a saddle point  but this is rare and any minimal perturbation
leads to escape.
Both EM and MacKay updates provably fail to satisfy one or more of the above criteria and so global
convergence cannot be guaranteed. With EM  the failure occurs because the associated updates do
not always strictly reduce L((cid:13)). Rather  they only ensure that L((cid:13)
0) at all points. In
contrast  the MacKay updates do not even guarantee cost function decrease. Consequently  both
methods can become trapped at a solution such as (cid:13) = 0; a (cid:2)xed point of the updates but not a
stationary point or local minimum of L((cid:13)). However  in practice this seems to be more of an issue
with the MacKay updates. Related shortcomings of EM in this regard can be found in [19]. Finally 
the fast Tipping updates could potentially satisfy the conditions for global convergence  although
this matter is not discussed in [16].

0). At any non-minimizing (cid:13)

00) (cid:20) L((cid:13)

00 2 A((cid:13)

3 Relating ARD to MAP Estimation
In hierarchical models such as ARD and SBL there has been considerable debate over how to best
perform estimation and inference [8]. Do we add a hyperprior and then integrate out (cid:13) and perform
MAP estimation directly on x? Or is it better to marginalize over the coef(cid:2)cients x and optimize the
hyperparameters (cid:13) as we have described in this paper? In speci(cid:2)c cases  arguments have been made
for the merits of one over the other based on intuition or heuristic arguments [8  15]. But we would
argue that this distinction is somewhat tenuous because  as we will now show using ideas from the
previous section  the weights obtained from the ARD type-II ML procedure can equivalently be
viewed as arising from an explicit MAP estimate in x space. This notion is made precise as follows:
Theorem 2. Let x
m ]T . Then the ARD coef(cid:2)cients
from (3) solve the MAP problem

m]T and (cid:13)

1 ; : : : ; (cid:13)(cid:0)1

(cid:0)1   [(cid:13)(cid:0)1

1; : : : ; x2

2   [x2

xARD = arg min
2) is the concave conjugate of h((cid:13)

x

2 + (cid:21)h(cid:3)(x

(15)
ky (cid:0) (cid:8)xk2
(cid:0)1)   (cid:0) log j(cid:6)yj and is a concave  non-decreasing

2);

where h(cid:3)(x
function of x.
This result can be established using much of the same analysis used in previous sections. Omitting
some details for the sake of brevity  using (13) we can create a strict upper bounding auxiliary
function on L((cid:13)):

(16)

L((cid:13); x) =

ky (cid:0) (cid:8)xk2

+ log j(cid:6)yj:

1
(cid:21)

2 +Xi

x2
i
(cid:13)i

If we optimize (cid:2)rst over (cid:13) instead of x (allowable)  the last two terms form the stated concave
conjugate function h(cid:3)(x
2). In turn  the minimizing x  which solves (15)  is identical to that obtained
by ARD. The concavity of h(cid:3)(x

2) with respect each jxij follows from similar ideas.

Corollary 1. The regularization term in (15)  and hence the implicit prior distribution on x given
2)]  is not generally factorable  meaning p(x) 6= Qi pi(xi). Addition-
by p(x) / exp[(cid:0) 1
ally  unlike traditional MAP procedures (e.g.  Lasso  ridge regression  etc.)  this prior is explicitly
dependent on both the dictionary (cid:8) and the regularization term (cid:21).

2 h(cid:3)(x

This result stems directly from the fact that h((cid:13)
(cid:21). The only exception occurs when (cid:8)T (cid:8) = I; here h(cid:3)(x
form independently of (cid:8)  although (cid:21) dependency remains.

(cid:0)1) is non-factorable and is dependent on (cid:8) and
2) factors and can be expressed in closed

3.1 Properties of the implicit ARD prior

To begin at the most super(cid:2)cial level  the (cid:8) dependency of the ARD prior leads to scale invariant
solutions  meaning the value of xARD is not affected if we rescale (cid:8)  i.e.  (cid:8) ! (cid:8)D  where D is a
diagonal matrix. Rather  any rescaling D only affects the implicit initialization of the algorithm  not
the shape of the cost function.
More signi(cid:2)cantly  the ARD prior is particularly well-designed for (cid:2)nding sparse solutions. We
should note that concave  non-decreasing regularization functions are well-known to encourage
sparse representations. Since h(cid:3)(x
2) is such a function  it should therefore not be surprising that it
promotes sparsity to some degree. However  when selecting highly sparse subsets of features  the
factorial ‘0 quasi-norm is often invoked as the ideal regularization term given unlimited computa-
tional resources. It is expressed via kxk0   Pi I[xi 6= 0]  where I[(cid:1)] denotes the indicator function 
and so represents a count of the number of nonzero coef(cid:2)cients (and therefore features). By applying
a exp[(cid:0)1=2((cid:1))] transformation  we obtain the implicit (improper) prior distribution. The associated
MAP estimation problem (assuming the same standard Gaussian likelihood) involves solving

min

x

ky (cid:0) (cid:8)xk2

2 + (cid:21)kxk0:

(17)

The dif(cid:2)culty here is that (17) is nearly impossible to solve in general; it is NP-hard owing to a
combinatorial number of local minima and so the traditional idea is to replace k (cid:1) k0 with a tractable
approximation. For this purpose  the ‘1 norm is the optimal or tightest convex relaxation of the ‘0
quasi-norm  and therefore it is commonly used leading to the Lasso algorithm [14]. However  the
‘1 norm need not be the best relaxation in general. In Sections 3.2 and 3.3 we demonstrate that
the non-factorable  (cid:21)-dependent h(cid:3)(x
2) provides a tighter  albeit non-convex  approximation that
promotes greater sparsity than kxk1 while conveniently producing many fewer local minima than
using kxk0 directly. We also show that  in certain settings  no (cid:21)-independent  factorial regularization
term can achieve similar results. Consequently  the widely used family of ‘p quasi-norms  i.e. 
kxkp   Pi jxijp  p < 1 [2]  or the Gaussian entropy measure Pi log jxij based on the Jeffreys
prior [4] provably fail in this regard.

3.2 Beneﬁts of (cid:21) dependency

To explore the properties of h(cid:3)(x
2) regarding (cid:21) dependency alone  we adopt the simplifying as-
sumption (cid:8)T (cid:8) = I. (Later we investigate the bene(cid:2)ts of a non-factorial prior.) In this special case 
h(cid:3)(x

2) is factorable and can be expressed in closed form via

2jxij

i + 4(cid:21)

+ log(cid:18)2(cid:21) + x2

h(cid:3)(x

i + 4(cid:21)(cid:19) ;

(18)

h(cid:3)(x2

i ) / Xi

2) = Xi

i + jxijqx2
i ) is shown in Figure 1 (left) below.

jxij +px2
which is independent of (cid:8). A plot of h(cid:3)(x2
The (cid:21) dependency is retained however and contributes two very desirable properties: (i) As a strictly
concave function of each jxij  h(cid:3)(x
2) more closely approximates the ‘0 quasi-norm than the ‘1 norm
while  (ii) The associated cost function (15) is unimodal unlike when (cid:21)-independent approximations 
e.g.  the ‘p quasi-norm  are used. This can be explained as follows. When (cid:21) is small  the Gaussian
likelihood is highly restrictive  constraining most of its relative mass to a very localized region of x
space. Therefore  a tighter prior more closely resembling the ‘0 quasi-norm can be used without the
risk of local minima  which occur when the spines of a sparse prior overlap non-negligible portions
of the likelihood (see Figure 6 in [15] for a good 2D visual of a sparse prior with characteristic spines
running alone the coordinate axis). In the limit as (cid:21) ! 0  h(cid:3)(x
2) converges to a scaled version of the

‘0 quasi-norm  yet no local minimum exist because the likelihood in this case only permits a single
feasible solution with x = (cid:8)T
y. In contrast  when (cid:21) is large  the likelihood is less constrained and a
looser prior is required to avoid local minima troubles  which will arise whenever the now relatively
diffuse likelihood intersects the sharp spines of a highly sparse prior. In this situation h(cid:3)(x
2) more
closely resembles a scaled version of the ‘1 norm. The implicit ARD prior naturally handles this
transition becoming sparser as (cid:21) decreases and vice versa. Hence the following property  which is
easy to show [18]:
Lemma 3. When (cid:8)T (cid:8) = I  (15) has no local minima whereas (17) has 2M local minima.
2) also yields no local minima; however  it is a much looser
Use of the ‘1 norm in place of h(cid:3)(x
approximation of ‘0 and penalizes coef(cid:2)cients linearly unlike h(cid:3)(x
2). The bene(cid:2)ts of (cid:21) dependency
in this regard can be formalized and will be presented in a subsequent paper. As a (cid:2)nal point of
comparison  the actual weight estimate obtained from solving (15) when (cid:8)T (cid:8) = I is equivalent to
the non-negative garrote estimator that has been advocated for wavelet shrinkage [5  18].

2  

1.8

1.6

1.4

1.2

2

1.6

1.2

0.8

0.4

)
i

x
(
p
g
o
l

(cid:0)

I[xi 6= 0]
jxij
ARD

PSfrag replacements
xi
(cid:0) log p(xi)
I[xi 6= 0]
jxij
ARD(cid:0)

)
x
(
p
g
o
l

)
d
e
z
i
l
a
m
r
o
n
(

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

maximally

sparse
solution

ARD
Pi jxij0:01

0  
−2

−1.5

−1

−0.5

0 
xi

0.5 

1 

1.5 

2

0
−8

−6

−4

−2

2

4

6

8

0

(cid:11)

Figure 1: Left: 1D example of the implicit ARD prior. The ‘1 and ‘0 norms are included for com-
parison. Right: Plot of the ARD prior across the feasible region as parameterized by (cid:11). A factorial
prior given by (cid:0) log p(x) / Pi jxij0:01 (cid:25) kxk0 is included for comparison. Both approximations
to the ‘0 norm retain the correct global minimum  but only ARD smooths out local minima.

PSfrag replacements

(cid:0) log p(x)

(normalized)

(cid:11)
ARD
Pi jxij0:01

x0   arg min

kxk0

x

3.3 Beneﬁts of a non-factorial prior
In contrast  the bene(cid:2)ts the typically non-factorial nature of h(cid:3)(x
2) are most pronounced when
m > n  meaning there are more features than the signal dimension y. In a noiseless setting (with
(cid:21) ! 0)  we can explicitly quantify the potential of this property of the implicit ARD prior. In this
limiting situation  the canonical sparse MAP estimation problem (17) reduces to (cid:2)nding

s.t. y = (cid:8)x:

(19)
By simple extension of results in [18]  the global minimum of (15) in the limit as (cid:21) ! 0 will
equal x0  assuming the latter is unique. The real distinction then is regarding the number of local
minimum. In this capacity the ARD MAP problem is superior to any possible factorial variant:
In the limit as (cid:21) ! 0 and assuming m > n  no factorial prior p(x) =
Theorem 3.
Qi exp[(cid:0)1=2fi(xi)] exists such that
the corresponding MAP problem minx ky (cid:0) (cid:8)xk2
2 +
(cid:21)Pi fi(xi) is: (i) Always globally minimized by a maximally sparse solution x0 and  (ii) Has
fewer local minima than when solving (15).
A sketch of the proof is as follows. First  for any factorial prior and associated regularization term
Pi fi(xi)  the only way to satisfy (i) is if @fi(xi)=@xi ! 1 as xi ! 0. Otherwise  it will always be
possible to have a (cid:8) and y such that x0 is not the global minimum. It is then straightforward to show
that any fi(xi) with this property will necessarily have between (cid:2)(cid:0)m(cid:0)1
n(cid:1)(cid:3) local minimum.
Using results from [18]  this is provably an upper bound on the number of local minimum to (15).
Moreover  with the exception of very contrived situations  the number of ARD local minima will
be considerably less. In general  this result speaks directly to the potential limitations of restricting
oneself to factorial priors when maximal feature pruning is paramount.
While generally dif(cid:2)cult to visualize  in restricted situations it is possible to explicitly illustrate
the type of smoothing over local minima that is possible using non-factorial priors. For example 

n (cid:1) + 1;(cid:0)m

consider the case where m = n + 1 and Rank((cid:8)) = n  implying that (cid:8) has a null-space dimension
of one. Consequently  any feasible solution to y = (cid:8)x can be expressed as x = x
0 + (cid:11)v  where
v 2 Null((cid:8))  (cid:11) is any real-valued scalar  and x
0 is any (cid:2)xed  feasible solution (e.g.  the minimum
norm solution). We can now plot any prior distribution p(x)  or equivalently (cid:0) log p(x)  over the
1D feasible region of x space as a function of (cid:11) to view the local minima pro(cid:2)le.
To demonstrate this idea  we chose n = 10  m = 11 and generated a (cid:8) matrix using iid N (0; 1)
entries. We then computed y = (cid:8)x0  where kx0k0 = 9 and nonzero entries are also iid unit
Gaussian. Figure 1 (right) displays the plots of two example priors in the feasible region of y = (cid:8)x:
2 Pi jxijp)  p = 0:01. The
(i) the non-factorial implicit ARD prior  and (ii) the prior p(x) / exp((cid:0) 1
later is a factorial prior which converges to the ideal sparsity penalty when p ! 0. From the (cid:2)gure 
we observe that  while both priors peak at the x0  the ARD prior has substantially smoothed away
local minima. While the implicit Lasso prior (which is equivalent to the assumption p = 1) also
smooths out local minima  the global minimum may be biased away from the maximally sparse
solution in many situations  unlike the ARD prior which provides a non-convex approximation with
its global minimum anchored at x0.

4 Extensions

Thus far we have restricted attention to one particularly useful ARD-based model. But much of the
analysis can be extended to handle a variety of alternative data likelihoods and priors. A particularly
useful adaptation relevant to compressed sensing [17]  manifold learning [13]  and neuroimaging
[12  18] is as follows. First  the data y can be replaced with a n (cid:2) t observation matrix Y which is
generated via an unknown coef(cid:2)cient matrix X. The assumed likelihood model and prior are
d(cid:13)
p(Y jX) / exp(cid:18)(cid:0)
(cid:13)iCi:
(20)
Here each of the d(cid:13) matrices Ci’s are known covariance components of which the irrelevant ones
are pruned by minimizing the analogous type-II likelihood function

x X(cid:3)(cid:19) ; (cid:6)x  

trace(cid:2)X T (cid:6)(cid:0)1

kY (cid:0) (cid:8)Xk2

F(cid:19) ;

p(X) / exp(cid:18)(cid:0)

1
2

Xi=1

1
2(cid:21)

L((cid:13)) = log j(cid:21)I + (cid:8)(cid:6)x(cid:8)T j + trace(cid:20) 1

t

XX T (cid:0)(cid:21)I + (cid:8)(cid:6)x(cid:8)T(cid:1)

(cid:0)1(cid:21) :

(21)

With minimal effort  this extension can be solved using the methodology described herein. The
primary difference is that Step 2 becomes a second-order cone (SOC) optimization problem for
which a variety of techniques exist for its minimization [2  9].
Another very useful adaptation involves adding a non-negativity constraint on the coef(cid:2)cients x 
e.g.  non-negative sparse coding. This is easily incorporated into the MAP cost function (15) and
optimization problem (12); performance is often signi(cid:2)cantly better than the non-negative Lasso.
Results will be presented in a subsequent paper. It may also be possible to develop an effective
variant for handling classi(cid:2)cation problems that avoids additional approximations such as those
introduced in [15].

5 Discussion
While ARD-based approaches have enjoyed remarkable success in a number of disparate (cid:2)elds  they
remain hampered to some degree by implementational limitations and a lack of clarity regarding the
nature of the cost function and existing update rules. This paper addresses these issues by presenting
a principled alternative algorithm based on auxiliary functions and a dual representation of the ARD
objective. The resulting algorithm is initialized at the well-known Lasso solution and then iterates
via a globally convergent re-weighted ‘1 procedure that in many ways approximates ideal subset
selection using the ‘0 norm. Preliminary results using this methodology on toy problems as well
as large neuroimaging simulations with m (cid:25) 100; 000 are very promising (and will be reported in
future papers). A good (highly sparse) solution is produced at every iteration and so early stopping is
always feasible if desired. This produces a highly ef(cid:2)cient  global competition among features that
is potentially superior to the sequential (greedy) updates of [16] in terms of local minima avoidance
in certain cases when (cid:8) is highly overcomplete (i.e.  m (cid:29) n). Moreover  it is also easily extended
to handle additional constraints (e.g.  non-negativity) or model complexity as occurs with general
covariance component estimation. A related optimization strategy has also been reported in [3].

The analysis used in deriving this algorithm reveals that ARD is exactly equivalent to performing
MAP estimation in x space using a principled  sparsity-inducing prior that is non-factorable and
dependent on both the feature set and noise parameter. We have shown that these qualities allow it
to promote maximally sparse solutions at the global minimum while relenting drastically fewer local
minima than competing priors. This might possibly explain the superior performance of ARD/SBL
over Lasso in a variety of disparate disciplines where sparsity is crucial [11  12  18]. These ideas
raise a key question: If we do not limit ourselves to factorable  (cid:8)- and (cid:21)-independent regularization
terms/priors as is commonly done  then what is the optimal prior p(x) in the context of feature
selection? Perhaps there is a better choice that does not neatly (cid:2)t into current frameworks linked
to empirical priors based on the Gaussian distribution. Note that the ‘1 re-weighting scheme for
optimization can be applied to a broad family of non-factorial  sparsity-inducing priors.

References
[1] S. Boyd and L. Vandenberghe  Convex Optimization  Cambridge University Press  2004.
[2] S.F. Cotter  B.D. Rao  K. Engan  and K. Kreutz-Delgado  (cid:147)Sparse solutions to linear inverse
problems with multiple measurement vectors (cid:148) IEEE Trans. Signal Processing  vol. 53  no. 7 
pp. 2477(cid:150)2488  April 2005.

[3] M. Fazel  H. Hindi  and S. Boyd (cid:147)Log-Det Heuristic for Matrix Rank Minimization with Appli-
cations to Hankel and Euclidean Distance Matrices (cid:148) Proc. American Control Conf.  vol. 3  pp.
2156(cid:150)2162  June 2003.

[4] M.A.T. Figueiredo  (cid:147)Adaptive sparseness using Jeffreys prior (cid:148) Advances in Neural Information

Processing Systems 14  pp. 697(cid:150)704  2002.

[5] H. Gao  (cid:147)Wavelet shrinkage denoising using the nonnegative garrote (cid:148) Journal of Computational

and Graphical Statistics  vol. 7  no. 4  pp. 469(cid:150)488  1998.

[6] D.G. Luenberger  Linear and Nonlinear Programming  Addison(cid:150)Wesley  Reading  Mas-

sachusetts  2nd ed.  1984.

[7] D.J.C. MacKay  (cid:147)Bayesian interpolation (cid:148) Neural Comp.  vol. 4  no. 3  pp. 415(cid:150)447  1992.
[8] D.J.C. MacKay  (cid:147)Comparison of approximate methods for handling hyperparameters (cid:148) Neural

Comp.  vol. 11  no. 5  pp. 1035(cid:150)1068  1999.

[9] D.M. Malioutov  M. C‚ etin  and A.S. Willsky  (cid:147)Sparse signal reconstruction perspective for
source localization with sensor arrays (cid:148) IEEE Trans. Signal Processing  vol. 53  no. 8  pp.
3010(cid:150)3022  August 2005.

[10] R.M. Neal  Bayesian Learning for Neural Networks  Springer-Verlag  New York  1996.
[11] R. Pique-Regi  E.S. Tsau  A. Ortega  R.C. Seeger  and S. Asgharzadeh  (cid:147)Wavelet footprints
and sparse Bayesian learning for DNA copy number change analysis (cid:148) Int. Conf. Acoustics
Speech and Signal Processing  April 2007.

[12] R.R. Ram·(cid:17)rez  Neuromagnetic Source Imaging of Spontaneous and Evoked Human Brain

Dynamics  PhD Thesis  New York University  2005.

[13] J.G. Silva  J.S. Marques  and J.M. Lemos  (cid:147)Selecting landmark points for sparse manifold

learning (cid:148) Advances in Neural Information Processing Systems 18  pp. 1241(cid:150)1248  2006.

[14] R. Tibshirani  (cid:147)Regression shrinkage and selection via the Lasso (cid:148) Journal of the Royal

Statistical Society  vol. 58  no. 1  pp. 267(cid:150)288  1996.

[15] M.E. Tipping  (cid:147)Sparse Bayesian learning and the relevance vector machine (cid:148) Journal of

Machine Learning Research  vol. 1  pp. 211(cid:150)244  2001.

[16] M.E. Tipping and A.C. Faul  (cid:147)Fast marginal likelihood maximisation for sparse Bayesian

models (cid:148) Ninth Int. Workshop Artiﬁcial Intelligence and Statistics  Jan. 2003.

[17] M.B. Wakin  M.F. Duarte  S. Sarvotham  D. Baron  and R.G. Baraniuk  (cid:147)Recovery of jointly
sparse signals from a few random projections (cid:148) Advances in Neural Information Processing
Systems 18  pp. 1433(cid:150)1440  2006.

[18] D.P. Wipf  (cid:147)Bayesian Methods for Finding Sparse Representations (cid:148) PhD Thesis  UC San

Diego  2006.

[19] C.F. Wu  (cid:147)On the convergence properties of the EM algorithm (cid:148) The Annals of Statistics  vol.

11  pp. 95(cid:150)103  1983.

,Jerry Zhu
Damien Garreau
Rémi Lajugie
Sylvain Arlot
Francis Bach
Steven Cheng-Xian Li
Benjamin Marlin