2019,Low-Complexity Nonparametric Bayesian Online Prediction with Universal Guarantees,We propose a novel nonparametric online predictor for discrete labels conditioned on multivariate continuous features. The predictor is based on a feature space discretization induced by a full-fledged k-d tree with randomly picked directions and a recursive Bayesian distribution  which allows to automatically learn the most relevant feature scales characterizing the conditional distribution. We prove its pointwise universality  i.e.  it achieves a normalized log loss performance asymptotically as good as the true conditional entropy of the labels given the features. The time complexity to process the n-th sample point is O(log n) in probability with respect to the distribution generating the data points  whereas other exact nonparametric methods require to process all past observations. Experiments on challenging datasets show the computational and statistical efficiency of our algorithm in comparison to standard and state-of-the-art methods.,Low-Complexity Nonparametric Bayesian
Online Prediction with Universal Guarantees

Alix Lhéritier
Amadeus SAS

F-06902 Sophia-Antipolis  France
alix.lheritier@amadeus.com

Frédéric Cazals

Université Côte d’Azur

Inria

F-06902 Sophia-Antipolis  France
frederic.cazals@inria.fr

Abstract

We propose a novel nonparametric online predictor for discrete labels conditioned
on multivariate continuous features. The predictor is based on a feature space
discretization induced by a full-ﬂedged k-d tree with randomly picked directions
and a recursive Bayesian distribution  which allows to automatically learn the
most relevant feature scales characterizing the conditional distribution. We prove
its pointwise universality  i.e.  it achieves a normalized log loss performance
asymptotically as good as the true conditional entropy of the labels given the
features. The time complexity to process the n-th sample point is O (log n) in
probability with respect to the distribution generating the data points  whereas other
exact nonparametric methods require to process all past observations. Experiments
on challenging datasets show the computational and statistical efﬁciency of our
algorithm in comparison to standard and state-of-the-art methods.

Introduction

1
Universal online predictors. An online (or sequential) probability predictor processes sequentially
input symbols l1  l2  . . . belonging to some alphabet L. Before observing the next symbol in the
sequence  it predicts it by estimating the probability of observing each symbol of the alphabet. Then 
it observes the symbol and some loss is incurred depending on the estimated probability of the
current symbol. Subsequently  it adapts its model in order to better predict future symbols. The
goal of universal prediction is to achieve an asymptotically optimal performance independently of
the generating mechanism (see  e.g.  the survey of Merhav and Feder [22]). When performance
is measured in terms of the logarithmic loss  prediction is intimately related to data compression 
gambling and investing (see  e.g.  [7  6]).
Barron’s theorem [3] (see also [10  Ch.15]) establishes a fundamental link between prediction under
logarithmic loss and learning: the better we can sequentially predict data from a probabilistic source 
the faster we can identify a good approximation of it. This is of paramount importance when applied
to nonparametric models of inﬁnite dimensionality  where overﬁtting is a serious concern. This is our
case  since the predictor observes some associated side information (i.e. features) zi ∈ Rd before
predicting li ∈ L  where L = {λ1  . . .   λ|L|}. We consider the probabilistic setup where the pairs
of observations (zi  li) are i.i.d. realizations of some random variables (Z  L) with joint probability
measure P. Therefore  we aim at estimating a nonparametric model of the conditional measure PL|Z.
Nonparametric distributions can be approximated by universal distributions over countable unions
of parametric models (see e.g.  [10  Ch. 13]). This approach requires deﬁning parametric models
that can arbitrarily approximate the nonparametric distribution as the number of parameters tend to
inﬁnity. For example  models based on histograms with arbitrarily many bins have been proposed to
approximate univariate nonparametric densities (e.g.  [13  26  36]).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Bayesian mixtures allow to obtain universal distributions for countable unions of parametric models
(e.g.  [35  34]). Nevertheless  standard Bayesian mixtures suffer from the catch-up phenomenon  i.e. 
their convergence rate is not optimal. In [31]  it has been shown that a better convergence rate can be
achieved by allowing models to change over time  by considering  instead of a set of distributions M 
a (larger) set constituted by sequences of distributions of M. The resulting switch distribution has
still a Bayesian form but the mixture is done over sequences of models.
Previous works on prediction with side information either are non-sequential (e.g. PAC learning [30]) 
or use other losses (e.g. [11  12] ) or consider side information in more restrictive spaces (e.g. [1  5]).
Our work bears similarities to [16  29  32] but the objectives are different and so are the guarantees.
Recently  [19] proposed a universal online predictor for side information in Rd based on a mixture of
nearest-neighbors regressors with different k(n) functions specifying the number of neighbors at time
n. Practically  the performances depend on the particular set of functions—a design choice—and
its time complexity is linear in n due to the exact nearest neighbor search. Gaussian Processes (see 
e.g.  [25]) are nonparametric Bayesian methods which can be used for online prediction with side
information. It is conjectured that exact Gaussian processes with the radial basis function (RBF)
kernel are universal under some conditions on the marginal measure PZ [10  Sec. 13.5.3]. In practice 
approximations are required to compute the predictive posterior for discrete labels (e.g. Laplace)
and the kernel width strongly affects the results. In addition  their time complexity to predict each

observation is O(cid:0)n3(cid:1)  making them practical for small samples sizes only.

We propose a novel nonparametric online predictor with universal guarantees for continuous side
information exhibiting two distinctive features. First  it relies on a hierarchical feature space dis-
cretization and a recursive Bayesian distribution  automatically learning the relevant feature scales
and making it scale-hyperparameter free. Second  in contrast to other nonparametric approaches  its
time complexity to process the n-th sample point is O (log n) in probability. Due to space constraints 
proofs are presented in the supplementary material.

2 Basic deﬁnitions and notations
In order to represent sequences  we use the notation xn ≡ x1  . . .   xn. The functions |·| and |·|λ  give 
respectively  the length of a sequence and the number of occurrences of a symbol λ in it. Let P be the
joint probability measure of L  Z. Let PL  PZ be their respective marginal measures and PZ|L the
probability measure of Z conditioned on L. The entropy of random variables is denoted H (·)  while
the entropy of L conditioned on Z is denoted H (L|Z). The mutual information between L and Z is
denoted I (L; Z). Logarithms are taken in base 2.
A ﬁnite-measurable partition A = (γ1  . . .   γn) of some set Ω is a subdivision of Ω into a ﬁnite
number of disjoint measurable sets or cells γi whose union is Ω. An n-sample partition rule πn(·) is
a mapping from Ωn to the space of ﬁnite-measurable partitions for Ω  denoted A(Ω). A partitioning
scheme for Ω is a countable collection of k-sample partition rules Π ≡ {πk}k∈N+. The partitioning
scheme at time n deﬁnes the set of partition rules Πn ≡ {πk}k=1..n. For a given n-sample partition
rule πn(·) and a sequence zn ∈ Ωn  πn(z|zn) denotes the unique cell in πn(zn) containing a given
point z ∈ Ω. For a given partition A  let A(z) denote the unique cell of A containing z. Let γ (·)
denote the operator that extracts the subsequences whose symbols have corresponding zi ∈ γ.

3 The kd-switch distribution

We deﬁne the kd-switch distribution Pkds using a k-d tree based hierarchical partitioning and a
switch distribution deﬁned over the union of multinomial distributions implied by the partitioning.
Full-ﬂedged k-d tree based spatial partitioning. We obtain a hierarchical partitioning of Ω = Rd
using a full-ﬂedged k-d tree [8  Sec. 20.4] that is naturally amenable to an online construction
since pivot points are chosen in the same order as sample points are observed. Instead of rotating
deterministically the axis of the projections  we sample the axis uniformly at each node of the tree.
Formally  let Πkd ≡ {πk}k∈N+ be the nested partitioning scheme such that πn(zn) is the spatial
partition generated by a full-ﬂedged k-d tree after observing zn. In order to deﬁne it recursively 
let the base case be π0(z0) ≡ Rd  where z0 is the empty string. Then  πn+1(zn+1) is obtained by
uniformly drawing a direction J in 1..d and by replacing the cell γ ∈ πn(zn) such that zn+1 ∈ γ by

2

the following cells

(cid:26)γ1 ≡ {z ∈ γ : z[J] ≤ zn+1[J]}

γ2 ≡ {z ∈ γ : z[J] > zn+1[J]}

(cid:8)γ1  γ2  . . .   γ|A|(cid:9) of Rd deﬁnes a class of piecewise multinomial distributions characterized by

(1)
where ·[J] extracts the J-th coordinate of the given vector. A spatial partition A =
θA ≡ [θ1  . . .   θ|A|]  θi ∈ ∆|L|  where ∆|L| is the standard |L|-simplex. More precisely  PθA (·|z) is
multinomial with parameter θi if z ∈ γi.

Context Tree Switching. We adapt the Con-
text Tree Switching (CTS) distribution [33] to
use spatial cells as contexts. Since these contexts
are created as sample points zi are observed  the
chronology of their creation has to be taken into
account. Given a nested partitioning scheme Π
whose instantiation with zn creates a cell γ and
splits it into γ1 and γ2  we deﬁne the cell split-
ting index τn(γ) as the index in the subsequence
γ (zn) when γ1 and γ2 are created (see Fig. 1).
If γ is not split by Π instantiated with zn  then
we deﬁne τn(γ) ≡ ∞.
At each cell γ  two models  deﬁned later and
denoted a and b  are considered. Let wγ(·)
be a prior over model index sequences im ≡
i1  . . .   im ∈ {a  b}m at cell γ  recursively de-
ﬁned by

1 if m = 0

1

2 if m = 1
wγ(im−1) ((1 − αγ

wγ(im) ≡

(a) π1(z1): γ1 and γ2
are created  z1 ∈ γ1.

(b) π2(z2): γ2 1 and
γ2 2 are created. z2 ∈
γ2 2.

Figure 1: Cell creation process and cell split-
ting index. The cell splitting index is deﬁned
w.r.t. its subsequence: τ2(Ω) = 1  τ2(γ2) = 1
since γ2
τ2(γ2 2) = ∞.

(cid:0)z2(cid:1) = z2  and τ2(γ1) = τ2(γ2 1) =

  E ≡ {im = im−1}  αγ

m = m−1.

1¬E) if m > 1

m)1E + αγ
m

In order to deﬁne the CTS distribution  we need the Jeffreys’ mixture over multinomial distributions
also known as the Krichevsky-Troﬁmov estimator [17]

|ln|λj w(θ)dθ

θ[j]

(2)

(cid:90)

Pkt(ln) ≡

(cid:89)

θ∈∆|L|

j∈1...|L|

with θ[j] being the j-th component of the vector θ  |ln|λj
the number of occurrences of λj in ln
and w(·) the Jeffreys’ prior for the multinomial distribution [14] i.e. a Dirichlet distribution with
parameters (1/2  . . .   1/2).
Consider any cell γ created by the partitioning scheme Π instantiated with zn. γ can either be reﬁned
into two child cells γ1 and γ2 or have τn(γ) = ∞. Given a sequence of labels ln such that all the
corresponding positions zi ∈ γ  the modiﬁed CTS distribution is given by

cts (ln|zn) ≡ (cid:88)

P Π γ

wγ(in)

in∈{a b}n

where the predictive distributions of models a and b are given by

k=1

n(cid:89)

(cid:2)1{ik=a}φa(lk|lk−1) +1{ik=b}φγ
(cid:0)lk|lk−1(cid:1) ≡ Pkt
(cid:0)lk|lk−1(cid:1) if k < τk(γ)

Pkt(lk−1)

(cid:0)lk(cid:1)

φa(lk|lk−1) ≡ Pkt

Pkt

P

b (lk|lk−1  zk)(cid:3)

(3)

(4)

b (lk|lk−1  zk) ≡
φγ

(cid:0)l0(cid:1) ≡ 1.

where · -1 removes the last symbol of a sequence and  for the empty sequences l0  z0  P Π γ
and Pkt

cts

Π γj

cts (γj(lk)|γj(zk))
P
(γj (lk) -1 |γj (zk) -1)
Π γj
cts

with j : zk ∈ γj  otherwise

(5)

(cid:0)l0|z0(cid:1) ≡ 1

and

3

z1γ1γ2γ2 1z1z2γ2 2Deﬁnition of Pkds. The kd-switch distribution is obtained from the modiﬁed CTS distribution on
the context cells deﬁned by the full-ﬂedged k-d tree spatial partitioning scheme i.e.

Pkds(ln|zn) ≡ P Πkd Rd

cts

(ln|zn) .

(6)
m = n−1 for any cell γ 
Remark 1. In [33]  the authors observe better empirical performance with αγ
where n is the number of samples observed at the root partition Ω when the m-th sample is observed
in γ. With this switching rate they were able to provide a good redundancy bound for bounded depth
trees. In our unbounded case  we observed a better empirical performance with αγ
Remark 2. A Context Tree Weighting [35] scheme can be obtained by setting αγ
corresponding distribution is denoted Pkdw.

m = m−1.

m = 0. The

4 Pointwise universality

In this section  we show that Pkds is pointwise universal  i.e. it achieves a normalized log loss
asymptotically as good as the true conditional entropy of the source generating the samples. More
formally  we state the following theorem.
Theorem. 1. The kd-switch distribution is pointwise universal  i.e.

(7)
for any probability measure P generating the samples such that PZ|L are absolutely continuous with
respect to the Lebesgue measure.

log Pkds(Ln|Z n) ≤ H (L|Z) a.s.

− lim
n→∞

1
n

In order to prove Thm. 1  we ﬁrst show that P Π Ω
is universal with respect to the class of piecewise
cts
multinomial distributions deﬁned by any nested partitioning scheme Π. Then  we show that Πkd
allows to approximate arbitrarily well any conditional distribution.
Universality with respect to the class of piecewise multinomial distributions. Consider a nested
partitioning scheme Π for Ω. Πn instantiated with some zn ∈ Ωn naturally deﬁnes a tree structure
whose root node represents Ω. Given an arbitrary set of internal nodes  we can prune the tree by
transforming these internal nodes into leaves and discarding the corresponding subtrees. The new set
of leaf nodes deﬁne a partition of Ω. Let Pn(zn) be the set of all the partitions that can be obtained
by pruning the tree induced by Πn instantiated with zn.
The next lemma shows that P Π Ω
cts   deﬁned in Eq. 3  is universal with respect to the class of piecewise
multinomial distributions deﬁned on the partitions Pn(zn).
Lemma. 1. Consider arbitrary sequences ln ∈ Ln  zn ∈ Ωn  n ≥ 0. Then  for any A ∈ Pn(zn)
and for any piecewise multinomial distribution PθA  the following holds

− log P Π Ω

cts (ln|zn) ≤ − log PθA (ln|zn) + |A|ζ

+ ΓA log 2n + O (1)

(cid:19)

(cid:18) n

|A|

(8)

(9)

and

− lim
n→∞

1
n

log P Π Ω

cts (Ln|Z n) ≤ H (L|A(Z)) a.s.

where ΓA is the number of nodes in the tree  induced by Πn instantiated with zn  that represents A
(i.e.  the code length given by a natural code for unbounded trees) and

ζ(x) ≡

x log|L|
|L|−1

2

log x + log|L|

if 0 ≤ x < 1
if x ≥ 1

.

(10)

(cid:40)

Remark 3. In a Context Tree Weighting scheme (αγ
See proof of Lemma 1. Thus  universality holds for this case too.
Universal discretization of the feature space. In order to prove that the k-d tree based partitions
allow to approximate arbitrarily well the conditional entropy H (L|Z)  we use the following corollary
of [28  Thm. 4.2].

m = 0)  the log 2n factor in Eq. 8 disappears.

4

Corollary. 1. Let (γ) ≡ supx y∈γ (cid:107)x − y(cid:107). Let P be any probability measure such that PZ|L
are absolutely continuous with respect to the Lebesgue measure. Given a partition scheme Π ≡
{πk}k∈N+  if ∀δ > 0

(cid:0)(cid:8)z ∈ Rd : (πn(z|Z n)) > δ(cid:9)(cid:1) a.s.−−→ 0

PZ

then Π universally discretizes the feature space  i.e.
H (L|πn(Z|Z n))

a.s.−−→ H (L|Z) .

(11)

(12)

The next lemma provides the required shrinking condition for the k-d tree based partitioning.
Lemma. 2. Πkd satisﬁes the shrinking condition of Eq. 11 and  thus  universally discretizes the
feature space.
Pointwise universality. The proof of Thm. 1 on the pointwise universality of Pkds stems from a
combination of Lemmas 1 and 2—see Appendix.

5 Online algorithm

cts

γPkt

(ln|zn).

(cid:0)lt|lt−1(cid:1) corresponds to the contribution of all possible model sequences
(cid:0)lt|lt−1  zt(cid:1) corresponds to the contribution of all possible model sequences ending in model b

Since a direct computation of Eq. (3) is intractable and an online implementation is desired  we
use the recursive form of [33  Algorithm 1]  which performs the exact calculation. We denote by
the sequentially computed kd-switch distribution at node γ. In Section 5.2  we show that
P γ
s
s (ln|zn) = P Πkd γ
P γ
5.1 Algorithm
Outline. For each node of the k-d tree  the algorithm maintains two weights denoted wa
follows from [33  Lemma 2]  if lt  zt are the subsequences observed in γ and wa
processing lt  then  wa
ending in model a (KT) to the total probability assigned to lt by the CTS distribution. Analogously 
γP γ
wb
s
(CTS).
s (ln|zn) given by Eq. 14.
We now describe the three steps that allow the online computation of P γ
The algorithm starts with only a root node representing Rd. When a new point z∗ is observed  the
following steps are performed.
Step 1: k-d tree update and new cells’ initialization. The point z∗ is passed down the k-d tree
until it reaches a leaf cell γ. Then  a coordinate J is uniformly drawn from 1 . . . d and two child
nodes  corresponding to the new cells deﬁned in Eq. 1 with z∗ as splitting point  are created.
Let ln  zn be the subsequences observed in γ and thus zn = z∗. Since the new cells may contain some
of the symbols in ln−1  the following initialization is performed at each new node γi  i ∈ {1  2}:

γ. As
γ is the weight before

γ and wb

wa
γi

wb
γi

← 1
2
← 1
2

Pkt

Pkt

(cid:0)γi
(cid:0)γi

(cid:0)ln−1(cid:1)(cid:1)
(cid:0)ln−1(cid:1)(cid:1)  with Pkt

(cid:0)γi
(cid:0)ln|ln−1(cid:1) + wb
(cid:0)ln|ln−1(cid:1) if n < τn(γ)

γPkt

γP γ
r

where

P γ
r

(cid:0)ln|ln−1  zn(cid:1) ←

s (ln|zn) ← wa
P γ

(cid:40)Pkt

(cid:0)ln−1(cid:1)(cid:1) = 1 if γi

(cid:0)ln−1(cid:1) is empty.

(cid:0)ln|ln−1  zn(cid:1)

Step 2: Prediction. The probability assigned to the subsequence ln given zn observed in γ is

s (γj (ln)|γj (zn))
γj
P
s (γj (ln) -1 |γj (zn) -1)
γj
P

with j : zn ∈ γj  otherwise

.

(15)

Step 3: Updates. Having computed the probability assignment of Eq. 14  the weights of the nodes
corresponding to the cells {γ : z∗ ∈ γ} are updated. Given a node γ to be updated  let ln  zn be the
subsequences observed in γ. The following updates are applied:

γ ← αγ
wa
γ ← αγ
wb

n+1P γ
n+1P γ

s (ln|zn) + βγ
s (ln|zn) + βγ

n+1wa
n+1wb

γPkt
γP γ
r

(cid:0)ln|ln−1(cid:1)
(cid:0)ln|ln−1  zn(cid:1)

5

(13)

(14)

(16)

where βγ

n ≡ (1 − 2αγ

n). When γ has just been created (i.e. γ is a leaf node)  these updates reduce to

γ ← wa
wa
γ ← wb
wb

γPkt
γPkt

(cid:0)ln|ln−1(cid:1) =

(cid:0)ln|ln−1(cid:1)
(cid:0)ln|ln−1(cid:1).
(cid:12)(cid:12)ln−1(cid:12)(cid:12)ln

+ 1
2
|L|
2

(17)

(18)

Remark 4. The KT estimator can be computed sequentially using the following formula [27]:

Pkt

Therefore  the sequential computation only requires maintaining the counters(cid:12)(cid:12)ln−1(cid:12)(cid:12)ln

for each cell.
Remark 5. Samples zi only need to be stored at leaf nodes. Once a leaf node is split  they are moved
to their corresponding child nodes.

|ln−1| +

.

5.2 Correctness

The steps of our algorithm are the same as those of [33  Algorithm 1] except for the initialization of
Eq. 13. In fact  as shown in the next lemma  it is equivalent to building the partitioning tree from
the beginning (assuming  without loss of generality  that zn is known in advance) and applying the
original algorithm at every relevant context.
Lemma. 3. Let n ∈ N+ and assume the partitioning tree for zn is built from the beginning. Let γ
be any node of the tree. If the original initialization and update equations from [33  Algorithm 1]
(corresponding respectively to Eq. 13 with an empty sequence and Eq. 16) are applied  the weights 
after observing lt in γ with t < τn(γ)  are wa
2 Pkt(lt)  which correspond to
those obtained after the initialization of Eq. 13 and the updates of Eq. 17.
The correctness of our algorithm follows from Lem. 3 and [33  Thm. 4]  since for t ≥ τn(γ) the
original update equations are used.

2 Pkt(lt) and wb

γ = 1

γ = 1

5.3 Complexity

The cost of processing ln  zn is linear in the depth Dn of the node split by the insertion of zn  since
the algorithm updates the weights at each node in the path leading to this node. If PZ is absolutely
continuous with respect to the Lebesgue measure  since the full-ﬂedged k-d tree is monotone
transformation invariant  we can assume without loss of generality that the marginal distributions
of Z are uniform in [0  1] (see [8  Sec. 20.1]) and thus its proﬁle is equivalent to that of a random
binary search tree under the random permutation model (see [21  Sec. 2.3]). Then  Dn corresponds
2 log n → 1 in probability (see [21  Sec. 2.4]). Therefore 
to the cost of an unsuccessful search and Dn
the complexity of processing ln  zn is O (log n) in probability with respect to PZn.

6 Experiments
Software-hardware setup. Python code and data used for the experiments are available at https:
//github.com/alherit/kd-switch. Experiments were carried out on a machine running Debian
3.16  equipped with two Intel(R) Xeon(R) E5-2667 v2 @ 3.30GHz processors and 62 GB of RAM.
Boosting ﬁnite length performance with ensembling. When considering ﬁnite length performance 
we can be unlucky and obtain a bad set of hierarchical partitions (i.e.  with low discrimination power).
In order to boost the probability of ﬁnding good partitions  we can use a Bayesian mixture of J trees.
Bayesian mixtures trivially maintain universality.
Two sampling scenarii for labels. In the ﬁrst one  labels are sampled from a Bernoulli distribution
such that P (L = 0) = θ0  where θ0 is a known parameter. We then we sample from PZ|L. In this
(1 − θ0)1{ln=1} 
case  the root node distribution Pkt
since θ0 is known. In the second one  observations come in random order and P (L) is unknown.

(cid:0)ln|ln−1(cid:1) is replaced by P (Ln = ln) = θ

1{ln=0}
0

6.1 Normalized log loss (NLL) convergence
Datasets. We use the following datasets  detailed in Appendix B.1: (L-i) A 2D dataset consists
of two Gaussian Mixtures spanning three different scales. (L-ii) A dataset in dimension d = 784

6

Figure 2: (Left and Middle) Convergence of NLL as a function of n  for a 30’ calculation. (Right)
Running time as a function of n. Error bands represent the std dev. w.r.t. the randomness in the tree
generation except for dataset (L-iv) where they represent the std dev. w.r.t. the shufﬂing of the data.

composed of both real MNIST digits  as well as digits generated by a Generative Adversarial Network
[24] trained on the MNIST dataset. (L-iii) The Higgs dataset [20]  the goal being to distinguish the
signature of processes producing Higgs bosons. (L-iv) The Breast Cancer Wisconsin (Diagnostic)
Data Set [20]—dimension d = 30.
For cases (L-i L-ii L-iii)  in order to feed the online predictors  we apply the ﬁrst sampling scenario
for labels. For case (L-iv)  we apply the second one and  in each trial  we take the pooled dataset in a
random order to feed the online predictors.
Results. We focus on the cumulative normalized log loss performance (NLL)  and the trade-off with
the computational requirements—by limiting the running time to 30’.
We compare the performance of our online predictors Pkds and Pkdw (see Rmk. 2) with a number of
trees J ∈ {1  50}  against the following contenders. The Bayesian mixture of knn-based sequential
regressors proposed in [19]  with a switch distribution using a horizon-free prior as Pkds. Practically 
this predictor depends on a given set of functions of n specifying the number of neighbors. We
use the same set speciﬁed in [19]. We also consider a Bayesian Mixture of Gaussian Processes
Classiﬁers (gp) with RBF kernel width σ ∈ {24i}i=−5...7. (Our implementation uses the scikit-
learn GaussianProcessClassiﬁer [23]. For each observation  we retrain the classiﬁer using all past
observations—a step requiring samples from the two populations. Thus  we predict with a uniform
distribution (or P (L) when known) until at least one instance of each label has been observed.) In
the case (L-i)  we also compare to the true conditional probability  which can be easily derived since
PZ|L are known. Note that the normalized log loss of the true conditional probability converges to
the conditional entropy by the Shannon–McMillan–Breiman theorem [7].
Fig. 2 (Left and Middle) shows the NLL convergence with respect to the number of samples. Notice
that due to the 30’ running time budget  curves stop at different n. Fig. 2 (Right) illustrates the
computational complexity of each method. For our predictors  the statistical efﬁciency increases with
the number of trees—at the expense of the computational burden. Weighting performs better than
switching for datasets (L-i  L-ii  L-iv)  and the other way around for (L-iii). knn takes some time to
get the right scale  then converges fast in most cases—with a plateau though on dataset L-ii though.
This owes to the particular set of exponents used to deﬁne the mixture of regressors [19]. Also  knn
is computationally demanding  in particular when compared to our predictors with J = 1.

7

102103104105106n0.9960.9981.0001.0021.0041.006NLLHIGGS n_trials=10algorithmgpkds-J1kds-J50kdw-J1kdw-J50knn102103104105106n0.860.880.900.920.940.960.981.001.02NLLGMM n_trials=10algorithmgpkds-J1kds-J50kdw-J1kdw-J50knntrue02000004000006000008000001000000n02505007501000125015001750elapsed_time (s)GMM n_trials=10algorithmgpkds-J1kds-J50kdw-J1kdw-J50knn0100200300400500n0.40.60.81.01.2NLLBREASTCANCER n_trials=30algorithmgpkds-J1kds-J50kdw-J1kdw-J50knn102103104105n0.00.20.40.60.81.0NLLGANMNIST n_trials=10algorithmgpkds-J1kds-J50kdw-J1kdw-J50knn020000400006000080000100000n02505007501000125015001750elapsed_time (s)GANMNIST n_trials=10algorithmgpkds-J1kds-J50kdw-J1kdw-J50knn(a) SG. d = 50.

(b) GMD. d = 100.

(c) GVD. d = 50.

(d) Blobs. d = 2.

Figure 3: Tests on randomly rotated Gaussian datasets from [15]. The abscissa represents the
test sample size ntest for each of the two samples. Thus  for sequential methods  n = 4ntest.

6.2 Two-sample testing (TST)
Construction. Given samples from two distributions  whose corresponding random variables X ∈
Rd and Y ∈ Rd are i.i.d.  a nonparametric two-sample test tries to determine whether the null
hypothesis PX = PY holds or not (see  e.g.  [18  Section 6.9]). Consistent sequential two-sample
tests with optional stop (i.e. the p-value is valid at any time n) can be built from a pointwise universal
online predictor Q [19] by deﬁning (L  Z) as: (0  X) with probability θ0  or (1  Y ) with probability
1 − θ0  where θ0 is a design parameter set to 1/2 in the following experiments. The p-value is
P(ln)
the likelihood ratio
Q(ln|zn). Note this corresponds to the ﬁrst sampling scenario for labels. The
instantiation of this construction with Pkds and J = 50 is denoted KDS-seq.
Contenders. We compare KDS-seq against SWπS from [19]  denoted KNN-seq: a sequential two-
sample test obtained by instantiating the construction described above with the online knn predictor
described in the previous section. We also compare KDS-seq against the kernel tests from [15]:
ME-full  ME-grid  SCF-full  SCF-grid  MMD-quad  MMD-lin  and the classical Hotelling’s T2for
differences in means under Gaussian assumptions. These tests depend on a kernel width σ learned
on a trained set—the train-test paradigm—as opposed to KDS-seq which automatically detects the
pertinent scales. Contenders were launched with the hyperparameters speciﬁed in their respective
paper. For a fair comparison between sequential methods and those tests using the train-test paradigm
with ntest used for testing  we use a number of samples n = 4ntest—detail in Appendix B.2.
Datasets. We use the four datasets from [15  Table 1]: (T-i) Same Gaussians in dimension d = 50 
to assess the type I error; (T-ii) Gaussian Mean Difference (GMD): normal distributions with a
difference in means along one direction  d = 100; (T-iii) Gaussian Variance Difference (GVD):
normal distributions with a difference in variance along one direction  d = 50; (T-iv) Blobs (Mixture
of Gaussian distributions centered on a lattice) [9]. Datasets (T-ii  T-iii  T-iv) are meant to assess type
II error. To prevent k-d tree cuts to exploit the particular direction where the difference lies  these
datasets undergo a random rotation (one per tree). See Appendix B.3 for results without rotations.
Results. The signiﬁcance level is set to α = .01 in all the cases. The Type I error rate and the power
(1 − Type II error rate) are computed over 500 trials. In the SG case (Fig. 3(a))  all the tests have
a Type I error rate around the speciﬁed α as expected. In the GMD and Blobs cases (Fig. 3(b d)) 
KDS-seq matches or outperforms all the contenders. On Blobs  KDS-seq outperforms KNN-seq
thanks to its automatic scale detection  even though the mixture used by the latter allows it to handle
the multiple scales. For GVD (Fig. 3(c))  our results are weaker. To see why  recall that GMD
is generated by adding one unit to one coordinate of the mean vector  while GVD is obtained by
doubling the variance along one direction. The span of the latter dataset is larger  and upon rotating
the data—see comment above—all directions are impacted. Given the high dimensionality  the
partitioning of k-d trees faces more difﬁculties to reduce the diameter of cells  which is key to
convergence—see Corollary 1.

7 Outlook

We foresee the following research directions. A ﬁrst open question is to characterize the situations
where switching should be preferred over weighting. A second core question is to quantify the ability
of our k-d tree based construction to cope with multiple scales in the data. A third one is the derivation
of ﬁnite length bounds related to the complexity of the underlying conditional distribution. Finally 
accommodating data in a metric (non Euclidean) space  using e.g. metric trees  would widen the
application spectrum of the method.

8

10002000300040005000Test sample size0.0050.0100.015Type-I error10002000300040005000Test sample size0.20.40.60.81.0Test power10002000300040005000Test sample size0.00.20.40.60.81.0Test power10002000300040005000Test sample size0.00.20.40.60.81.0Test powerME-fullME-gridSCF-fullSCF-gridMMD-quadMMD-linT2KDS-seqKNN-seqAcknowledgments

We would like to thank María Zuluaga  Eoin Thomas  Nicolas Bondoux and Rodrigo Acuña-Agost for
insightful comments and  also  Wittawat Jitkrittum and Arthur Gretton for providing us the complete
output of their experiments.

References
[1] P. Algoet. Universal schemes for prediction  gambling and portfolio selection. The Annals of Probability 

20(2):901–941  1992.

[2] P. Baldi  P. Sadowski  and D. Whiteson. Searching for exotic particles in high-energy physics with deep

learning. Nature communications  5:4308  2014.

[3] A. R. Barron. Information-theoretic characterization of Bayes performance and the choice of priors in
parametric and nonparametric problems. In A. D. J.M. Bernardo  J.O. Berger and A. Smith  editors 
Bayesian Statistics 6  pages 27–52. Oxford University Press  1998.

[4] R. Begleiter and R. El-Yaniv. Superior guarantees for sequential prediction and lossless compression via

alphabet decomposition. Journal of Machine Learning Research  7(Feb):379–411  2006.

[5] H. Cai  S. R. Kulkarni  and S. Verdú. A universal lossless compressor with side information based on
context tree weighting. In Information Theory  2005. ISIT 2005. Proceedings. International Symposium on 
pages 2340–2344. IEEE  2005.

[6] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge university press  2006.

[7] T. Cover and J. Thomas. Elements of Information Theory. Wiley & Sons  2006.

[8] L. Devroye  L. Györﬁ  and G. Lugosi. A probabilistic theory of pattern recognition  volume 31. Springer

Verlag  1996.

[9] A. Gretton  D. Sejdinovic  H. Strathmann  S. Balakrishnan  M. Pontil  K. Fukumizu  and B. Sriperumbudur.
Optimal kernel choice for large-scale two-sample tests. In Advances in Neural Information Processing
Systems  pages 1205–1213  2012.

[10] P. D. Grünwald. The minimum description length principle. MIT press  2007.

[11] L. Gyöﬁ and G. Lugosi. Strategies for sequential prediction of stationary time series.

uncertainty  pages 225–248. Springer  2005.

In Modeling

[12] L. Gyorﬁ  G. Lugosi  and G. Morvai. A simple randomized algorithm for sequential prediction of ergodic

time series. IEEE Transactions on Information Theory  45(7):2642–2650  1999.

[13] P. Hall and E. Hannan. On stochastic complexity and nonparametric density estimation. Biometrika 

75(4):705–714  1988.

[14] H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal

Society of London. Series A. Mathematical and Physical Sciences  186(1007):453–461  1946.

[15] W. Jitkrittum  Z. Szabó  K. P. Chwialkowski  and A. Gretton. Interpretable distribution features with
maximum testing power. In Advances in Neural Information Processing Systems  pages 181–189  2016.

[16] S. S. Kozat  A. C. Singer  and G. C. Zeitler. Universal piecewise linear prediction via context trees. IEEE

Transactions on Signal Processing  55(7):3730–3745  2007.

[17] R. Krichevsky and V. Troﬁmov. The performance of universal encoding. Information Theory  IEEE

Transactions on  27(2):199–207  1981.

[18] E. L. Lehmann and J. P. Romano. Testing statistical hypotheses. Springer Texts in Statistics. Springer 

New York  third edition  2005.

[19] A. Lhéritier and F. Cazals. A sequential non-parametric multivariate two-sample test. IEEE Transactions

on Information Theory  64(5):3361–3370  2018.

[20] M. Lichman. UCI machine learning repository  2013.

[21] H. Mahmoud. Evolution of random search trees. Wiley-Interscience  1992.

9

[22] N. Merhav and M. Feder. Universal prediction. Information Theory  IEEE Transactions on  44(6):2124–

2147  1998.

[23] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Prettenhofer 
R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research  12:2825–2830  2011.

[24] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolutional

generative adversarial networks. In ICLR  2016.

[25] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation

and Machine Learning). The MIT Press  2005.

[26] J. Rissanen  T. Speed  and B. Yu. Density estimation by stochastic complexity. Information Theory  IEEE

Transactions on  38(2):315–323  1992.

[27] Y. Shtar’kov. Universal sequential coding of single messages. Problemy Peredachi Informatsii  23(3):3–17 

1987.

[28] J. Silva. On optimal signal representation for statistical learning and pattern recognition. PhD thesis 

University of Southern California  2008.

[29] N. Tziortziotis  C. Dimitrakakis  and K. Blekas. Cover tree bayesian reinforcement learning. The Journal

of Machine Learning Research  15(1):2313–2335  2014.

[30] L. G. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142  1984.

[31] T. van Erven  P. Grünwald  and S. de Rooij. Catching up faster by switching sooner: a predictive approach
to adaptive estimation with an application to the AIC–BIC dilemma. Journal of the Royal Statistical
Society: Series B (Statistical Methodology)  74(3):361–417  2012.

[32] J. Veness  T. Lattimore  A. Bhoopchand  A. Grabska-Barwinska  C. Mattern  and P. Toth. Online learning

with gated linear networks. arXiv preprint arXiv:1712.01897  2017.

[33] J. Veness  K. S. Ng  M. Hutter  and M. Bowling. Context tree switching. In Data Compression Conference

(DCC)  2012  pages 327–336. IEEE  2012.

[34] F. M. J. Willems. The context-tree weighting method: Extensions. Information Theory  IEEE Transactions

on  44(2):792–798  1998.

[35] F. M. J. Willems  Y. M. Shtarkov  and T. J. Tjalkens. The context-tree weighting method: Basic properties.

Information Theory  IEEE Transactions on  41(3):653–664  1995.

[36] B. Yu and T. Speed. Data compression and histograms. Probability Theory and Related Fields  92(2):195–

229  1992.

10

,Alix LHERITIER
Frederic Cazals