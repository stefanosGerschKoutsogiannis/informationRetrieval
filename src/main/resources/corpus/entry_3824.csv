2019,Nonparametric Contextual Bandits in Metric Spaces with Unknown Metric,Consider a nonparametric contextual multi-arm bandit problem where each arm $a \in [K]$ is associated to a nonparametric reward function $f_a: [0 1] \to \mathbb{R}$ mapping from contexts to the expected reward. Suppose that there is a large set of arms  yet there is a simple but unknown structure amongst the arm reward functions  e.g. finite types or smooth with respect to an unknown metric space. We present a novel algorithm which learns data-driven similarities amongst the arms  in order to implement adaptive partitioning of the context-arm space for more efficient learning. We provide regret bounds along with simulations that highlight the algorithm's dependence on the local geometry of the reward functions.,Nonparametric Contextual Bandits

in an Unknown Metric Space

Nirandika Wanigasekara

Computer Science

National University of Singapore
nirandiw@comp.nus.edu.sg

Operations Research and Information Engineering

Christina Lee Yu

Cornell University

cleeyu@cornell.edu

Abstract

Consider a nonparametric contextual multi-arm bandit problem where each arm
a ∈ [K] is associated to a nonparametric reward function fa : [0  1] → R mapping
from contexts to the expected reward. Suppose that there is a large set of arms 
yet there is a simple but unknown structure amongst the arm reward functions 
e.g. ﬁnite types or smooth with respect to an unknown metric space. We present a
novel algorithm which learns data-driven similarities amongst the arms  in order
to implement adaptive partitioning of the context-arm space for more efﬁcient
learning. We provide regret bounds along with simulations that highlight the
algorithm’s dependence on the local geometry of the reward functions.

1

Introduction

Contextual multi-arm bandits have been used to model the task of sequential decision making in
which the rewards of different decisions must be learned over trial via trial-and-error. The decision
maker receives reward for each of the arms (i.e. actions or options) she chooses across the time
horizon T . In each trial t  the decision maker observes the context xt  which represents the set of
observable factors of the environment that could impact the performance of the action she chooses.
The decision maker must select an action based on the context and all past observations. Upon
choosing action a ∈ [K]  she observes a reward  which is assumed to be a stochastic observation of
fa(x)  the expected reward of action a at context x. In each trial  she faces the dilemma of whether
to choose an action in order to learn about its performance (i.e. exploration)  or to choose an action
that she believes will perform well as estimated from the limited previous data (i.e. exploitation).
Consider a setting when the number of actions is very large  e.g. there is a large number of users
and products on an e-commerce platform such that fully exploring the entire space of possible
recommendations is costly. It is often the case that there is additional structure amongst the large
space of actions  which the algorithm could exploit to learn more efﬁciently. In real-world applications
however  this additional structure is often unknown a priori and must be learned from the data  which
itself could be costly as well. It becomes important to understand the tradeoff and costs of learning
relationships amongst the arm from data over the course of the contextual bandit time horizon. We
consider a stochastic nonparametric contextual bandit setting in which the algorithm is not given
any information a priori about the relationship between the actions. The key question is: Can an
algorithm exploit hidden structure in a nonparametric contextual bandit problem with no a priori
knowledge of the underlying metric?

Contributions To our knowledge  we propose the ﬁrst nonparametric contextual multi-arm bandit
algorithm that incorporates latent arm similarities in a setting where no a priori information about
the features or metric amongst the arms is given to the algorithm. The algorithm can learn more
efﬁciently by sharing data across similar arms  but the tradeoff between the cost of estimating arm

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

similarities must be carefully accounted for. Our algorithm builds upon Slivkin’s Zooming algorithm
[22]  adaptively partitioning the context-arm space using pairwise arm similarities estimated from the
data. The adaptive partitioning allows the algorithm to naturally adapt the precision of its estimates
around regions of the context-arm space that are nearly optimal  enabling the algorithm to more
efﬁciently allocate its observations to regions of high reward.
We provide upper bounds on the regret that show the algorithm’s dependence on the local geometry
of the reward functions. If we let f∗(x) := maxa∈[K] fa(x) denote the optimal reward at context
x  then the regret depends on how the mass of the set {(a  x) : f∗(x) − fa(x) ∈ (0  δ]} scales as δ
goes to zero. This set represents the δ-optimal region of the context-arm space except for the exactly
optimal arms  i.e. the local measure of nearly optimal options centered around the optimal policy.
The scaling of this set captures the notion of “gap” used in classical multi-arm bandit problems  but
in the general contextual bandit setting with a large number of arms  it may be reasonable that the
second optimal arm is very close in value to the optimal arm such that the gap is always very small.
Instead the relevant quantity is the relative measure of arms that are δ-optimal yet not optimal  i.e.
have gap less than δ. If the mass of such arms decreases linearly with respect to δ  then we show that
our algorithm achieves regret of O(
An interesting property of our algorithm is that it is fully data-dependent and thus does not depend on
the chosen representation of the arm space. The arm similarities (or distances) are measured from
data collected by the algorithm itself  and thus approximates a notion of distance that is deﬁned
with respect to the reward functions {fa}a∈[K]. The algorithm would perform the same for any
permutation of the arms. In contrast  consider existing algorithms which assume a given distance
metric or kernel function which the reward function is assumed to be smooth with respect to. Those
algorithms are sensitive to the metric or kernel given to it  which itself could be expensive to learn or
approximate from data. Suppose that nature applied a measure preserving transformation to the arm
metric space such that the function is still Lipschitz but has a signiﬁcantly larger Lispchitz constant.
For example  consider a periodic function that repeats across the arm metric space. The performance
of existing algorithms would degrade with poorer arm feature representations  whereas the algorithm
we propose would remain agnostic to such manipulations.
We provide simulations that compare our algorithm to oracle variants that have special knowledge of
the arms and a naive benchmark that learns over each arm separately. Initially our algorithm has a
high cost due to learning the similarities  but for settings with a large number of arms and a long time
horizon  the learned similarities pay off and improve the algorithm’s long run performance.

KT ).

√

Related Work As there is a vast literature on multi-arm bandits  we speciﬁcally focus on literature
related to the stochastic contextual bandit problem  with an emphasis on nonparametric models. In
contextual bandits  in each trial the learner ﬁrst observers a feature vector  refer to as “context” 
associated with each arm. The optimal reward is measured with respect to the context revealed at the
beginning of each trial. One approach is to directly optimize and learn over a given space of policies
rather than learn the reward functions [3  5  12  14]. These methods do not require strict assumptions
on the reward functions but instead depend on the complexity or size of the model class.
We focus on the alternative approach of approximating reward functions  which then depend on
assumptions about the structure of the reward function. A common assumption to make is that
the reward function is linear with respect to the observed context vector [15  1  2  13]  such that
the reward estimation task reduces to learning coefﬁcient vectors of the reward functions.
[2]
incorporates sparsity assumptions for the high dimensional covariate setting  and [13] imposes low
rank assumptions on the coefﬁcient vectors to reduce the effective dimension.
In the linear bandit setting with K arms but only Θ arm types for Θ (cid:28) K  Gentile et al proposed an
adaptive clustering algorithm which maintains an undirected graph between the arms and progressively
erase edges when the estimated coefﬁcient vectors of the pair of arms is above a set threshold [6].
Two arms of the same type are assumed to have the same coefﬁcient vector. The threshold is
chosen as a function of the minimum separation condition between coefﬁcients vectors of different
types  such that eventually the graph converges to Θ connected components corresponding to the
Θ types. Collaborative ﬁltering bandits [16] applies the same adaptive clustering concept to the
recommendation system setting where both users and item types must be learned.
In the nonparametric setting  instead of ﬁxing a parametric model class such as linear  most work
imposes smoothness conditions on the reward functions  and subsequently use nonparametric estima-

2

d+1

tors such as histogram binning  k nearest neighbor  or kernel methods to learn the reward functions
[24  20  18  19  7]. As the contexts are observed  the estimator is applied to learn the reward of each
arm separately  essentially assuming the number of arms is not too large. [7] provides an upper bound
on regret of ˜O(KT
d+2 )  where d is the dimension of the context space  and K is the number of arms.
The setting of continnum arm bandits has been introduced to approximate setting with very large
action spaces. As there are inﬁnitely many arms  it is common to impose smoothness with respect to
some metric amongst the arms [17  22  8  10  9]. As the joint context-arm metric is known  these
methods apply various smoothing techniques implemented via averaging datapoints with respect to a
partitioning of the context-arm space  reﬁning the smoothing parameter as more data is collected.
[7] uses a k nearest neighbor estimator using the joint context-arm metric. The contextual zooming
algorithm from Slivkins [22] was a key inspiration for our proposed algorithm; it uses the given
context-arm metric to adaptively partition the context-arm product space [22]. This enables the
algorithm to efﬁciently allocate more observations to regions of the context-arm space that are near
optimal. When T is the time horizon and d is the covering dimension of the context-arm product
space  the regret of the contextual zooming algorithm is bounded above by ˜O(T
For settings with large but ﬁnite number of arms  there are nonparametric models which assume
different types information is known about the relationship amongst the arms. Gaussian process
bandits use a known covariance matrix to ﬁt a Gaussian process over the joint context-arm space
[11]. Taxonomy MAB assumes that similarity structure amongst the arm is given in terms of a
hierarchical tree rather than metric [21]. Deshmukh et al assume that the kernel matrix between
pairs of arms is known  and they subsequently use kernel methods to estimate the reward functions.
Cesa-Bianchi et al assumes that a graph reﬂecting arm similarities is given to the algorithm  and their
algorithm subsequently uses the Laplacian matrix of the given graph to regularize their estimates of
the reward functions [4]. Wu et al assumes an inﬂuence matrix amongst arms is known and used to
share datapoints among connected arms in the estimation [23]. A limitation of these approaches is
that they assumes similarity information is provided to the algorithm either as a metric  kernel  or via
a graph structure. In real-world applications  this similarity information is often not readily available
and must be itself learned from the data.

d+2 ).

d+1

2 Problem Statement
Assume that the context at each trial t ∈ [T ] is sampled independently and uniformly over the unit
interval  xt ∼ U (0  1)  and revealed to the algorithm. Assume there are K arms  or options  that the
algorithm can choose amongst at each trial t. If the algorithm chooses arm at at trial t  it observes
and receives a reward πt ∈ R according to πt = fat(xt) + t  where t ∼ N (0  σ2) is an i.i.d
Gaussian noise term with mean 0 and variance σ2  and fa(x) denotes the expected reward for arm
a as a function of the context x. We assume that each arm reward function fa : [0  1] → [0  1] is
L-Lipschitz  i.e. for all x  x(cid:48) ∈ [0  1]2  |fa(x) − fa(x(cid:48))| ≤ L|x − x(cid:48)|.

The goal of our problem setting is to maximize the total expected payoff(cid:80)T

t=1 πt over the time

horizon T . We provide upper bounds on the expected contextual regret 

E [R(T )] := E(cid:104)(cid:80)T

(cid:105)
t=1 (f∗(xt) − fat(xt))

where f∗(x) := maxa∈[K] fa(x).

(1)

We would like to understand whether an algorithm can efﬁciently exploit latent structure amongst the
arm reward functions if it exists. Although the number of arms may be large  they could be drawn
from a smaller set of ﬁnite arm types. Alternatively the arms could be draw from a continuum metric
space such that the reward function is jointly Lipschitz over the context-arm space; however our
algorithm would not have access to or knowledge of the underlying representation in the metric space.

3 Algorithm Intuition

We begin by describing an oracle algorithm that is given special knowledge of the relationship
between the arms in the form of the context-arm metric. Assume that the arms are embedded into
a metric space  and the function is Lipschitz with respect to that metric. The contextual zooming
algorithm proposed by Slivkins in [22] reduces the large continuum arm set to the effective dimension

3

of the underlying metric space. Essentially  their model assumes that each arm is associated to some
known parameter θa ∈ [0  1]  and that the expected joint payoff function is 1-Lipschitz continuous in
the context-arm product space with respect to a known metric D  such that for all context-arm pairs
(x  a) and (x(cid:48)  a(cid:48))  |f (x  θa) − f (x(cid:48)  θa(cid:48))| ≤ D((x  θa)  (x(cid:48)  θa(cid:48))).
The key idea of Slivkin’s zooming algorithm is to use adaptive discretization to encourage the
algorithm to obtain more reﬁned estimates in the nearly optimal regions of the space while allowing
coarse estimates in suboptimal regions of the context-arm space. The algorithm maintains a partition
of the context-arm space consisting of “balls”  or sets  of various sizes. The algorithm estimates
the reward function within a ball by averaging observed samples that lie within this ball. An upper
conﬁdence bound is obtained by accounting for the bias (proportional to the “diameter” of the ball
due to Lipschitzness) and the variance due to averaging noisy observations within the ball. When a
context arrives  the UCB rule is used to select a ball in the partition  and subsequently an arm in that
ball. When the number of observations in a ball increases beyond the threshold such that the variance
of the estimate is less than the bias  then the algorithm splits the ball into smaller balls  reﬁning the
partition locally in this region of the context-arm space.
The main intuition of the analysis is that the UCB selection rule guarantees (with high probability)
that when a ball with diameter ∆ is selected  the regret incurred by selecting this ball is bounded
above by order ∆. As a result  this algorithm is able to exploit the arm similarities via the joint metric
in order to aggregate samples of similar arms such that the estimates will converge more quickly.
Subsequently the algorithm reﬁnes the estimates and subpartitions the space as needed for regions
that are near optimal and thus require tighter estimates in order to allow the algorithm to narrow in on
the optimal arm. The limitation of the previous Zooming algorithm is that it depends crucially on the
given knowledge of the context-arm joint metric  which could be unknown in advance.

Arm Similarity Estimation In our model  we are not given any metric or features of the arm  thus
the key question is whether it is still possible for an algorithm to exploit good structure amongst
the arms if it exists. We propose an algorithm inspired by Slivkin’s contextual zooming algorithm 
which also adaptively partitions the context-arm space with the goal to allow for coarse estimates that
converge quickly initially  and subsequently selectively reﬁne the partition and the corresponding
estimates in regions of the context-arm space that are nearly optimal. The key challenge to deal with
is determining how to subpartition amongst the arms when we do not know any underlying metric or
feature space. Our algorithm estimates a similarity (or distance) from the collected data itself  and
uses the data-dependent distances to cluster/subpartition amongst the arms. This concept is similar to
clustering bandits which also learns data-driven similarities  except that the clustering bandits works
assume linear reward functions and ﬁnite types  whereas our model and algorithm is more general for
nonparametric functions and arms drawn from an underlying continuous space [6].
We want our algorithm to partition the context-arm product space into balls  or subsets  within
which the maximum diameter is bounded  where diameter of a subset is deﬁned as diam(S) :=
sup(x a)∈S fa(x) − inf (x(cid:48) a(cid:48))∈S fa(cid:48)(x(cid:48)). We consider balls ρ ⊆ [0  1] × [K] which have the form of
[c0(ρ)  c1(ρ)]×A(ρ)  where c0(ρ) ∈ [0  1] denotes the start of the context interval  c1(ρ) ∈ [c0(ρ)  1]
denotes the end of the context interval  and A(ρ) ⊆ [K] denotes the subset of arms. We use
∆(ρ) := c1(ρ) − c0(ρ) to denote the “width” of the context interval pertaining to the ball ρ.
In order to ﬁgure out which set of arms to include in a “ball” such that the diameter is bounded 
we ideally would like to measure the L∞ distance with respect to the context interval of the ball 
maxx∈[c0(ρ) c1(ρ)] |fa(x) − fa(cid:48)(x)|. As the functions are assumed to be Lipschitz with respect to
the context space  a bound on the L2 distance also implies a bound on the L∞. Our algorithm
approximates the L2 distance  deﬁned with respect to an interval [u  v] according to

(cid:17)2

(cid:16)

4

where zi(u  v) =(cid:0)1 − i

Dv
u(a  a(cid:48)) :=

200

(cid:80)

(cid:114)
(cid:1) u+ i

1

200

i∈[200]

fa(zi(u  v)) − fa(cid:48)(zi(u  v))

(2)

200 v. This is a ﬁnite sum approximation to the integrated L2 distance

between fa and fa(cid:48) within the interval [u  v].
Our algorithm uses the data collected for an arm in order to approximate the reward functions using a
k nearest neighbor estimator  and subsequently uses the estimated reward functions to approximate
Dv
u. These approximate distances are then used to cluster the arms when subpartitioning. With high
probability  we show that the diameter of the constructed balls is bounded by 2L∆(ρ). Our algorithm

collects extra samples to compute these distances  and a key part of the analysis is to understand
when the improvement in the learning rate of the reward functions is sufﬁcient enough to offset the
cost of estimating arm distances.

4 Algorithm Statement

Let nt(ρ) =(cid:80)t−1
(cid:80)t−1

µt(ρ) = 1

s=1

nt(ρ)

s=1

I (ρs = ρ) denote the number of times ρ has been selected before trial t. Let
I (ρs = ρ) πs denote the average observed reward from ρ before trial t. Deﬁne

U CBt(ρ) = µt(ρ) + 2L∆(ρ) +(cid:112)6σ2 ln(T )/nt(ρ) 

(3)

which gives an upper conﬁdence bound for the maximum reward achievable by any context-arm pair
in the ball ρ. The algorithm maintains two sets of balls  P and P∗  such that P ∪ P∗ is a partition
of the context-arm space  i.e. all balls are disjoint and the union cover the entire space. We refer to
balls in P∗ as ﬂagged. They are given ultimate priority in the algorithm  until sufﬁcient samples are
collected to further subpartition this ball via clustering. We refer to balls in P as “active”  within
which priority is given to balls with higher upper conﬁdence bound (UCB).

Ball-Arm Selection Rule
In a given trial t  when the context xt arrives  the algorithm identiﬁes the
ﬂagged balls ρ ∈ P∗ which contain context xt  i.e. xt ∈ [c0(ρ)  c1(ρ)]  and gives priority amongst
them to balls with larger width ∆(ρ) 

ρt = argmaxρ∈P∗ ∆(ρ)I (xt ∈ [c0(ρ)  c1(ρ)]) .

If there are no ﬂagged balls in P∗ which contain xt  then the algorithm selects an active ball ρ ∈ P
containing xt  and gives priority to the ball with the highest upper conﬁdence bound U CBt(ρ) 

ρt = argmaxρ∈P U CBt(ρ)I (xt ∈ [c0(ρ)  c1(ρ)]) .

(4)
When a ball ρt is chosen  the algorithm plays an arm at ∈ Aρt via a round robin ordering. The
algorithm observes a noisy reward πt for arm at and updates nt(ρ)  µt(ρ)  and U CBt(ρ) accordingly.
By grouping the context-arm pairs into balls  the algorithm aggregates the observed rewards within a
ball to trade-off between bias and variance. For any given trial  the algorithm reduces the decision
problem from selecting amongst a large number of arms to selecting amongst a smaller set of balls 
which each consist of a subset of arms. Whenever the ball is subpartitioned  the width of the context
interval is halved  such that balls never repeat  and are always strictly nested within a hierarchy.
Furthermore  the fact that the algorithm gives priority to ﬂagged balls with larger context widths
implies that the data collected in the “ﬂagged” phase of every ball will be uniformly distributed over
context width of that ball.

Flagging Rule At the beginning of the algorithm  the entire context-arm space is ﬂagged as a single
large ball to be subpartitioned  i.e. P∗ = {([0  1] × [K])} and P = ∅. In subsequent rounds  we ﬂag
a ball ρ ∈ P whenever it satisﬁes the condition nt(ρt) > 6σ2 ln(T )/L2∆2(ρt). Upon being ﬂagged 
ρ is removed from P and added to P∗. Let stopping time τf (ρ) denote the trial t that ball ρ is ﬂagged.
Intuitively  the threshold is chosen at a point where the conﬁdence radius  i.e. natural variation in the
estimates due to the additive Gaussian observation error  is on the order of the diameter of the ball.
As a result  further collecting samples does not improve the overall UCB because the diameter of the
ball will dominate the expression.
Sub-Partitioning via Clustering Recall that ﬂagged balls in P∗ are always given priority over
active balls in P. The observations collected in the ﬂagged phase are used to estimate distances 
or similarities between the arms for the purpose of subpartitioning the ball into smaller balls. In
particular  the algorithm splits the context space [c0(ρ)  c1(ρ)] into 64 evenly sized intervals and waits
until it collects at least k samples within each of the 64 intervals for each of the arms a ∈ A(ρ)  where
k is chosen according to k = 5431σ2 ln(T|A(ρ)|)/(L2∆2(ρ)). This condition is mathematically

stated as(cid:81)

a∈A(ρ) SUFFDATA(a) == 1 where

SUFFDATA(a) :=(cid:81)64

i=1

I(cid:16)(cid:80)

I (ρs = ρ  as = a) I (xs ∈ [wi−1  wi]) ≥ k

s>τf (ρ)

5

(cid:17)

 

for wi = c0(ρ) + i∆(ρ)/64. When this sufﬁcient data condition is satisﬁed  the algorithm uses the
observations collected in the ﬂagged phase to compute pairwise arm distances approximating (2). Let
τcl(ρ) denote the trial in which the sufﬁcient data condition is satisﬁed and ρ is subpartitioned.
The SUBPARTITION subroutine estimates the reward functions via a k-nearest neighbor estimator 

ˆfa(x) = 1
k

I (ρs = ρ  as = a) I (xs ∈ k-NN) πs 

where xs is a k nearest neighbor of x if(cid:80)τcl(ρ)

(5)
I (ρ(cid:96) = ρ  a(cid:96) = a) I (|x(cid:96) − x| ≤ |xs − x|) ≤ k.
Given the estimated functions { ˆfa}a∈A(ρ) and a pair of arms a  a(cid:48) ∈ A(ρ)  we compute ˆDv
u(a  a(cid:48))
for intervals [u  v] = [c0(ρ)  (c0(ρ) + c1(ρ))/2] and [u  v] = [(c0(ρ) + c1(ρ))/2  c1(ρ)] according to

s=τf (ρ)+1

(cid:96)=τf (ρ)+1

(cid:80)τcl(ρ)

(cid:16) ˆfa(zi(u  v)) − ˆfa(cid:48)(zi(u  v))

(cid:17)2 − 2σ2

k

i∈[200]

(6)

ˆDv
u(a  a(cid:48)) :=

where zi(u  v) =(cid:0)1 − i

(cid:80)

(cid:114)
(cid:1) u + i

200

1

200

k accounts for bias due to the noise.

200 v and the term 2σ2
We use the computed distances ˆDv
u(a  a(cid:48)) to subpartition ρ by clustering the arms for each half of
the context interval separately. For an arbitray ordering of the arms  we test if the next arm has
distance less than 3L(v − u)/16 to any of the existing cluster centers. If so  we assign it to the cluster
associated to the closest cluster center. Otherwise  we create a new cluster and assign this arm to be
the cluster center. This results in a clustering in which all pairs of cluster centers are guaranteed to be
distance 3L(v − u)/16 apart  and all members of a cluster must be within distance 3L(v − u)/16
to the cluster center. These distances are measured with respect to the data dependent estimates
u(a  a(cid:48)). In our analysis  we show that with high probability ˆDv
ˆDv
Once the clusters are created  then ρ is unﬂagged (removed from P∗) and new balls corresponding
to each of the clusters for each half of the context interval are added to the active set P. See the
appendix for a pseudocode description of the algorithm.

u(a  a(cid:48)) ≈ Dv

u(a  a(cid:48)).

5 Simulation

We test our algorithm on a model with 50  100  200 arms and a context space of [0  1]. Each arm a
corresponds to a parameter θa uniformly spaced out within [0  1]. The expected reward for arm a and
context x is

fa(x) := g(x  θa) = 1 −(cid:12)(cid:12)x − 4 minz∈{0 0.5 1} |θa − z|(cid:12)(cid:12).

u(a  a(cid:48)) approximates Dv

This function is periodic with respect to θ  and can be depicted as a zigzag. Our distance
estimate ˆDv
u(a  a(cid:48))  which is deﬁned with respect fa and fa(cid:48) directly
and does not depend on θa. Consider a measure preserving transformation that maps θa to
φa = 4 minz∈{0 0.5 1} |θa − z|  such that the reward function is equivalently described by
fa(x) = 1 − |x − φa|. An algorithm which partitions with respect to Dv
u(a  a(cid:48)) would be ag-
nostic to such a transformation  as opposed to an algorithm which depends on a metric deﬁned with
respect the arm’s representation  which would perform worse on θa than φa.
We benchmark the performance of our Approx-Zooming algorithm against three variations:
• Approx-Zooming -With-True-Reward-Function: We give the Approx-Zooming algorithm oracle
u(a  a(cid:48)) at no cost  which is used to subpartition whenever a ball is ﬂagged.
• Approx-Zooming -With-Similarity-Metric: We give the Approx-Zooming algorithm oracle access
• Approx-Zooming -With-No-Arm-Similarity: This naive variant uses no arm similarities  estimating
each arm’s reward independently. The context space is adaptively partitioned via our algorithm.

access to evaluate Dv
to evaluate |θa − θa(cid:48)| at no cost  which is used to subpartition whenever a ball is ﬂagged.

We chose the model parameters that led to the highest average cumulative reward in each baseline
algorithm. For all algorithms the ﬂagging rule is set to nt(ρ) ≥ 4 ln(T )/∆2  and σ was set to 1e − 2.
For Approx-Zooming   k was set to 10. We set the number of trials T to 100  000 as all the algorithms
had converged to their optimal point by then. Additional details on how the model parameters were
chosen is given in Appendix F.

6

(cid:80)T

t=1 πt  where T is the
In ﬁgure 1  we plot the average cumulative reward over the trials  i.e. 1
total number of trials and πt ∈ (0  1) is the reward observed in the tth trial. We plot the result for
T
the 200 arm setting with σ set to 1e − 2. As we can see  the oracle variant of the algorithm that
uses the true reward function to calculate Dv
u(a  a(cid:48)) achieves the best cummulative reward across the
entire time horizon. Not surprisingly  the algorithm which learns each arm separately takes more
time to converge to the optimal policy compared to all the other methods. Our Approx-Zooming
algorithm has a heavy cost up front due to the clustering of the arms globally  but the algorithm
improves over the time horizon as it learns the correct arm similarities. The oracle variant which uses
the similarity metric |θa − θa(cid:48)| performs worse than the true Dv
u(a  a(cid:48)) variant  as it does not account
for the periodic nature of the function. This supports our intuition that algorithms which depend on a
given metric are sensitive to the choice of a good vs bad metric.

Figure 1: Avg. cumulative reward vs. number of trials

Figure 2: Approx-Zooming Selected Arm Frequency Over The Trials

In ﬁgure 2 we plot the frequencies an arm is selected in different contexts over the T trials. Each of
the four plots corresponds to averaging the frequency over T /4 trials across the time horizon. The
x-axis refers to the context space  and the y-axis refers to the set of arms. Initially the frequency plot is
very blurry  indicating that our algorithm is not necessarily playing the optimal arm but selecting arms
to learn the latent arm structure. As time progresses our algorithm learns the similarities amongst
arms and gradually plays the arms using the latent structure  which is depicted by the zigzag shape
sharpening. Finally  in the last trials Approx-Zooming plays the optimal policy  which corresponds to
the clear zigzag. In Appendix F we present similar plots for the benchmark algorithms.
Our simulations show that when the number of arms is large  it is important to use similarities
amongst arms to more quickly learn the optimal policy. In addition our results highlight the fact
that metric-based algorithms may be sensitive to the choice of metric  which is not a trivial task. In
contrast  our approach relies on samples from the reward distribution to learn the latent structure  and
is thus agnostic to any choice of metric. However  the parameter k needs to be carefully tuned for
our algorithm to avoid unnecessary sampling for estimating similarities. In Appendix F we include
similar plots for other parameters of the problem  in particular for smaller number of arms. We see
that for 50 arms or 100 arms  the cost due to the added extra exploration may exceed the gain from
learning the metric  and thus we anticipate that the beneﬁts of learning the metric only dominates in
regimes where the number of arms is large and the time horizon is sufﬁciently long.

7

02004006008001000No. of trials * 1000.70.80.9Avg cumm. rewardApprox-ZoomingApprox-Zooming-No-Arm-SimilarityApprox-Zooming-Similarity-MetricApprox-Zooming-True02468t = 2500002346699211513816118402468t = 5000002346699211513816118402468t = 7500002346699211513816118402468t = 100000023466992115138161184Context IntervalFinite Arms6 Upper bound on the Regret

Mi =(cid:80)2i

(cid:96)=1

We present a general bound on the regret expressed as a function of a quantity relating to the local
geometry of the reward function nearby the optimal policy. Let us denote wi((cid:96)) = [((cid:96) − 1)2−i  (cid:96)2−i] 
κ(x) = f∗(x) − maxa∈[K] fa(x) I (fa(x) (cid:54)= f∗(x))  and

I(cid:0)minx∈wi((cid:96)) κ(x) ≤ 20 L 2−i(cid:1)(cid:80)
(cid:16)

σ2L−2K ln(T K) + min
imax∈Z+

a∈[K]

I(cid:0)(f∗(2−i(cid:96)) − fa(2−i(cid:96))) ≤ 22 L 2−i(cid:1) .
σ2L−1Mi2i ln(T K)(cid:1)(cid:17)

imax−1(cid:88)

.

(cid:0)LT 2−imax +

Theorem 6.1. The expected contextual regret of Approx-Zooming is bounded above by

E [R(T )] = O

i=1

The analysis relies on showing that the instantaneous regret incurred by choosing a ball with context
width ∆ is bounded above by O(L∆). The ﬁrst term in the regret is due to the very ﬁrst intitial
clustering phase. The second term L T 2−imax bounds the regret incurred by all balls with context
width at most 2−imax. The terms in the summation bound the regret incurred by balls with context
width equal to 2−i. The function κ(x) represents the lowest regret achieved by the second-most
optimal arm  which lower bounds the suboptimality gap. In alignment with our intuition from
classical MAB  when the suboptimality gap is large  the algorithm is able to more quickly converge
to the optimal arm at context x. When we bound the regret incurred by all balls with context width
2−i  we can thus remove subintervals of the context for which κ(x) is large as the algorithm will
have already converged to the optimal arm. This is reﬂected in the ﬁrst indicator function within
the expression Mi. Once restricted to context subintervals where the suboptimality gap is not too
which the suboptimality gap is at most 22 L 2−i; arms for which the suboptimality gap is larger will
have already been deemed suboptimal. As the speciﬁc bounds on Mi depend on the model and local
geometry amongst the arms  we provide bounds for two concrete examples to give more intuition.
Finite Types Suppose that the reward functions for the K arms  {fa}a∈[K] only takes Θ different
values. Essentially  this implies that there are Θ different types of arms  but we don’t know the arm
types a priori. Within each type  the reward function is exactly the same. Let us deﬁne

I(cid:0)(f∗(2−i(cid:96)) − fa(2−i(cid:96))) ≤ 22 L 2−i(cid:1) counts the number of arms for

large  the expression(cid:80)

a∈[K]

µκ(z) := µ({x ∈ [0  1] s.t. κ(x) ≤ z})

where µ is the Lebesgue measure. Then we can show that Mi ≤ 2iKµκ(22 L 2−i). The regret is
bounded by the local measure function µκ. In the ﬁnite types setting  the optimal policy corresponds
to partitioning the context space [0  1] into a set of intervals  S∗  such that across each interval
∫ ∈ S∗  the optimal policy does not change. Let us consider the setting that κ(x) decreases
linearly fast nearby the points where the optimal policy changes  so that for some constant L(cid:48) 
µκ(22 L 2−i) ≤ 22 |S∗| L 2−i/L(cid:48). By plugging the bound on Mi into the main theorem and
choosing imax = log(L(cid:48)LT /22σ2|S∗|K ln(T K))/2  it follows that

(cid:17)
σ2L−2K ln(T K) +(cid:112)σ2|S∗|LL(cid:48)−1T K ln(T K)

E [R(T )] ≤ O

(cid:16)

(7)

Lipschitz with respect to continuous arm metric space Suppose that each arm a is associated
to a latent feature θa ∈ [0  1]  and the expected reward function fa(x) = g(x  θa)  where g :
[0  1] × [0  1] → [0  1] is a L-Lipschitz function with respect to both the contexts and the arm latent
features such that |g(x  θ) − g(x(cid:48)  θ(cid:48))| ≤ L(|x − x(cid:48)| + |θ − θ(cid:48)|). If we assume that the arm latent
features are uniformly spread out  {θa} = {i/K}i∈[K]  then

Mi ≤(cid:80)

(cid:80)

I(cid:0)(f∗(2−i(cid:96)) − g(2−i(cid:96)  j

K )) ≤ 22 L 2−i(cid:1)  

j∈[K]

(cid:96)∈[2i]

is at most 22 L 2−i. We can visualize(cid:80)2i

which is a discrete approximation to the area of the context-arm space for which the suboptimality gap
(cid:96)=1 Mi((cid:96)) by considering the contour plot of f∗(x)−g(x  θ) 
and counting how many grid points {(2−i(cid:96)  j/K)}(cid:96)∈[2i] j∈[K] are lower than 22L2−i. For large i and
K  this is approximately 2iKµ({(x  θ) : g(x  θ) − f∗(x) ≥ −22L2−i})  where µ is the Lebesgue
measure. The curve at the lowest level of the contour plot corresponds to the set {(x  θ) s.t. g(x  θ)−

(8)

8

f∗(x) = 0}  which contains for each context the set of arm features that optimize the expected
reward. The ﬁnal regret depends on the local measure of the joint reward function.
As an example  if we consider the reward function g(x  θ) = 1 − L|x − θ| for some L ∈ (0  1)  we
can show that Mi ≤ 44K  i.e. it is bounded by a constant with respect to i. Therefore by plugging

into the main theorem and choosing imax = log(cid:0)20L2T /σ2K ln(T K)(cid:1) /2 it follows that

(cid:16)
(cid:17)
σ2L−2K ln(T K) +(cid:112)σ2KT ln(T K)

.

(9)

E [R(T )] ≤ O

7 Discussion

Interpreting the results. We began this paper with the question: Can an algorithm exploit hidden
structure in a nonparametric contextual bandit problem with no a priori knowledge of the underlying
metric? The results of our simulations suggest that our proposed algorithm (with empirically tuned
hyperparameters) can perform better than the corresponding algorithm that learns over each arm
separately  or that uses a suboptimal metric. However  the regret bounds we present are not sufﬁciently
strong to provably show that the algorithm outperforms learning on arms separately. The stated upper
bound on regret in [7] is linear in the number of arms K  however this may be simply due to the fact
that they did not optimize with respect to K in their analysis. Our regret bound is most comparable
to the regret for the inﬁnite arm setting presented in [22]  and it can be recovered from their bound by
imposing the discrete metric amongst the arms.
Generalizing to higher context dimension. For simplicity  we have stated our algorithm and
analysis for the 1D context space  but the results extend to the general d-dimensional setting. The
only change required algorithmically is in the subpartitioning/clustering step. Let us deﬁne Cd(q) to
be the number of balls of radius r/q needed to cover a ball of radius r  which scales exponentially in
the dimension d  e.g. qd. Since we are now estimating the reward function f over a d-dimensional
context space  the number of sub-regions of the context space that need to be clustered will be
Cd(2)  and the number of samples needed to guarantee that the k-nearest neighbor samples are within
16radius  will be equal to ˜O(kCd(32)). To compute ˆD  we will instead have a d-dimensional
distance 1
summation over the subset of the context space. Once ˆD is computed  then the clustering of arms
will have the same computational cost  i.e. linear in number of arms to be clustered. The analysis can
be modiﬁed to account for the d-dimensional setting  and the ﬁnal regret bound will look like

(cid:16)

O

Cd(2)Cd(32)σ2L−2K ln(T K)+ min
imax∈Z+

(cid:0)LT 2−imax +

imax−1(cid:88)

Cd(2)Cd(32)σ2L−1Mi2i ln(T K)(cid:1)(cid:17)

 

i=1

where Mi instead sums over an -net of the context space for  = 2−i  and thus we may expect Mi to
grow exponentially in i × d  depending on the distribution of the reward function and the ﬁnite arms.
The growth of Mi will dominate the regret bound with respect to the dependence on the dimension d.
Choice of metric. Nature could apply a measure preserving transformation to the arm metric space
such that the joint function has a signiﬁcantly higher Lipschitz constant. This representation would
incur a worse performance by the previous Zooming algorithm  indicating that the algorithm is
critically dependent on the choice of representation and metric. As an example  suppose that arms are
each associated to some latent parameter θ ∈ (0  2π)  and the reward function associated to an arm a
is fa(x) = x + sin(Lθa). The Lipschitz constant with respect to θ is L. By applying a change of
variables from θ to t(θ) = Lθ mod 2π  the associated reward function in terms of the representation
ta = t(θa) would be fa(x) = x + sin(ta)  which only has Lipschitz constant 1 with respect to the
reparamerization t. This is only a simple example amongst many that illustrate the importance of
the choice of metric for learning. In contrast  our algorithm estimates similarity amongst the arms
directly from data collected from the reward functions  which essentially estimates distance in the
function space; as a result our algorithm is invariant to any speciﬁc covariate representation.
Future Work. The current results are stated only for Lipschitz reward functions  where the tuning
parameters depend on the Lipschitz constant. It may be interesting to generalize the algorithm to
Holder continuous reward functions  and consider how to adapt the algorithm to the smoothness
parameters if unknown. It would also be interesting to explore the connections to Gaussian process
bandits. One would need to specify the covariance matrix amongst arms  and it may be possible to
consider empirically estimating the covariance matrix in the process of learning.

9

Acknowledgement

This work is supported by the National University of Singapore and A*STAR - SERC PSF Grant
1521200084. We thank Professor David S. Rosenblum for his support of this project through insightful
discussions and feedback.

References
[1] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning  pages 127–135  2013.

[2] H. Bastani and M. Bayati. Online decision-making with high-dimensional covariates. Available

at SSRN 2661896  2015.

[3] A. Beygelzimer  J. Langford  L. Li  L. Reyzin  and R. E. Schapire. Contextual bandit algorithms

with supervised learning guarantees. arXiv preprint arXiv:1002.4058  2010.

[4] A. A. Deshmukh  U. Dogan  and C. Scott. Multi-task learning for contextual bandits. In

Advances in Neural Information Processing Systems  pages 4848–4856  2017.

[5] M. Dudik  D. Hsu  S. Kale  N. Karampatziakis  J. Langford  L. Reyzin  and T. Zhang. Efﬁcient

optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369  2011.

[6] C. Gentile  S. Li  and G. Zappella. Online clustering of bandits. In International Conference on

Machine Learning  pages 757–765  2014.

[7] M. Y. Guan and H. Jiang. Nonparametric stochastic contextual bandits. In Thirty-Second AAAI

Conference on Artiﬁcial Intelligence  2018.

[8] E. Hazan and N. Megiddo. Online learning with prior knowledge. In International Conference

on Computational Learning Theory  pages 499–513. Springer  2007.

[9] R. Kleinberg  A. Slivkins  and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings
of the fortieth annual ACM symposium on Theory of computing  pages 681–690. ACM  2008.

[10] R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in

Neural Information Processing Systems  pages 697–704  2005.

[11] A. Krause and C. S. Ong. Contextual gaussian process bandit optimization. In Advances in

Neural Information Processing Systems  pages 2447–2455  2011.

[12] A. Krishnamurthy  J. Langford  A. Slivkins  and C. Zhang. Contextual bandits with continuous

actions: Smoothing  zooming  and adapting. arXiv preprint arXiv:1902.01520  2019.

[13] S. Lale  K. Azizzadenesheli  A. Anandkumar  and B. Hassibi. Stochastic linear bandits with
hidden low rank structure. CoRR  abs/1901.09490  2019. URL http://arxiv.org/abs/
1901.09490.

[14] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information Processing Systems 20  pages 817–824. 2008.

[15] L. Li  W. Chu  J. Langford  and R. E. Schapire. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World
wide web  pages 661–670. ACM  2010.

[16] S. Li  A. Karatzoglou  and C. Gentile. Collaborative ﬁltering bandits. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information Retrieval 
pages 539–548. ACM  2016.

[17] T. Lu  D. Pál  and M. Pál. Showing relevant ads via context multi-armed bandits. Technical

report  Tech. rep  2009.

[18] V. Perchet  P. Rigollet  et al. The multi-armed bandit problem with covariates. The Annals of

Statistics  41(2):693–721  2013.

10

[19] W. Qian and Y. Yang. Kernel estimation and model combination in a bandit problem with

covariates. The Journal of Machine Learning Research  17(1):5181–5217  2016.

[20] P. Rigollet and A. Zeevi.

arXiv:1003.1630  2010.

Nonparametric bandits with covariates.

arXiv preprint

[21] A. Slivkins. Multi-armed bandits on implicit metric spaces. In Advances in Neural Information

Processing Systems  pages 1602–1610  2011.

[22] A. Slivkins. Contextual bandits with similarity information. The Journal of Machine Learning

Research  15(1):2533–2568  2014.

[23] Q. Wu  H. Wang  Q. Gu  and H. Wang. Contextual bandits in a collaborative environment. In
Proceedings of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval  pages 529–538. ACM  2016.

[24] Y. Yang  D. Zhu  et al. Randomized allocation with nonparametric estimation for a multi-armed

bandit problem with covariates. The Annals of Statistics  30(1):100–121  2002.

11

,Nirandika Wanigasekara
Christina Yu