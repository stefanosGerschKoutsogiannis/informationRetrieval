2019,PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments,Given a set of a reference RGBD views of an indoor environment  and a new viewpoint  our goal is to predict the view from that location. Prior work on new-view generation has predominantly focused on significantly constrained scenarios  typically involving artificially rendered views of isolated CAD models. Here we tackle a much more challenging version of the problem. We devise an approach that exploits known geometric properties of the scene (per-frame camera extrinsics and depth) in order to warp reference views into the new ones. The defects in the generated views are handled by a novel RGBD inpainting network  PerspectiveNet  that is fine-tuned for a given scene in order to obtain images that are geometrically consistent with all the views in the scene camera system. Experiments conducted on the ScanNet and SceneNet datasets reveal performance superior to strong baselines.,PerspectiveNet: A Scene-consistent Image Generator
for New View Synthesis in Real Indoor Environments

David Novotny

Jeremy Reizenstein

Benjamin Graham
Facebook AI Research

London

{dnovotny benjamingraham reizenstein}@fb.com

Abstract

Given a set of a reference RGBD views of an indoor environment  and a new
viewpoint  our goal is to predict the view from that location. Prior work on new-
view generation has predominantly focused on signiﬁcantly constrained scenarios 
typically involving artiﬁcially rendered views of isolated CAD models. Here we
tackle a much more challenging version of the problem. We devise an approach
that exploits known geometric properties of the scene (per-frame camera extrinsics
and depth) in order to warp reference views into the new ones. The defects in the
generated views are handled by a novel RGBD inpainting network  PerspectiveNet 
that is ﬁne-tuned for a given scene in order to obtain images that are geometrically
consistent with all the views in the scene camera system. Experiments conducted
on the ScanNet and SceneNet datasets reveal performance superior to strong
baselines.

1

Introduction

Decisions often have to be made on the basis of incomplete information about our visual environment.
Humans instinctively ﬁll the gaps in from prior experience. This is an enabler of many tasks such
as navigation  and machine learning should strive to match this ability. One way of quantitatively
measuring it is via generating new views within a partially explored environment. Many variants of
this problem  known as new view synthesis  exist  ranging from a category-speciﬁc setup  where the
hallucinated views are conditioned on image(s) of an isolated instance of a well deﬁned visual object
category (car  chair) [9]  to inferring new photo-realistic pictures of outdoor or indoor scenes given
a set of reference images [13]. The former can be seen as a subtask of the latter  since real scenes
contain many instances of various object categories in an arbitrary geometric conﬁguration. Perhaps
due to the challenging nature of the more unconstrained setup  the community in recent years mostly
focused on the category-speciﬁc scenario  restricting a large portion of the experimental evaluation to
clean synthetic datasets such as ShapeNet [4].
In this work  we take a step toward the more complex task of generating new views of real indoor
environments. Historically  the new view synthesis task has been addressed with either learning
based methods [43  20  19]  that leverage deep nets to map an encoding of a viewpoint and style to a
new view  or methods that exploit geometric properties of a given scene to warp reference images
into a target viewpoint  usually with some human intervention  possibly followed by an inpainting
step that ﬁlls the newly appearing holes [17  7]. While learning based methods are suitable for the
category-speciﬁc setup  where the viewpoint-to-image mapping is less complex  due to the regular
geometric structure of object categories  the indoor scene synthesis was predominantly addressed
with different variants of the render-inpaint technique. Similar to previous approaches  we tackle the
task by devising a novel variant of the render-inpaint approach.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Input
views

{vi} ⊂ R4×H×W



Partial
point cloud
X ⊂ R3 × R3

RGB

Partial

new views
{¯vi} ⊂ R4×H×W

Completed
new views
{ˆvi} ⊂ R4×H×W

t
e
N
e
v
i
t
c
e
p
s
r
e
P







Figure 1: New view synthesis in real indoor environments. Given a sparse set of reference RGBD
room views  our goal is to generate new views of the same room.

Our main contribution is a novel scene-level multi-camera optimization scheme termed PerspectiveNet.
The crux of the method lies in regarding the joint set of reference and generated test views as a
calibrated optical system. Its known physical properties allow for exploiting powerful constraints
that enforce consistency of the newly generated views across all present cameras. Combined with a
latent representation of pixels in each view  we optimize over the set of latent image codes to generate
globally consistent set of views of a considered room.
We conduct one of the ﬁrst systematic evaluations of the new view synthesis task in the context of
real indoor environments. Our method is compared with a variety of strong baselines that either
follow the render-inpaint paradigm  or reason about scene contents in 3D space with 3D convolutions.
Evaluation on the ScanNet and SceneNet datasets reveals that our method outperforms all baselines
both qualitatively and quantitatively.

2 Related Work

Category-speciﬁc new view synthesis Neural networks can successully learn complex mappings 
including changes of appearance induced by a camera movement. Therefore they have been the
method of choice in tackling new view synthesis. The task was mostly explored for isolated object
categories such as chairs or faces  since their regular structure signiﬁcantly constrains the problem.
Given an encoding of a relative transformation and a reference image  Transforming Autoencoder [16]
produced its transformed version. While [16] applied the architecture to images of digits  Multiview
Perceptron [46] synthesized views of human faces.
With the advent of deep learning  convolutional neural networks (CNNs) enabled new view synthesis
for more complex object categories. Dosovitskiy et al. [9] generated new views of chairs from
a synthetic dataset (ShapeNet). Following [34]  [34  39  22] proposed a similar encoder-decoder
architecture that  differently from [9]  generated views of object instances previously unseen in the
train set. Similar to our approach  several other methods proposed an alternative that transfers pixels
from the reference views  followed by an image reﬁnement step [31  43  19]. Other approaches [20]
involve an intermediate 3DCNN that aggregates information from the reference views  followed by a
learned 3D-to-image decoder. While the aforementioned methods show impressive results  they are
restricted to isolated views of object categories from a synthetic dataset. We differ by considering
a much more challenging setup with the reference views coming from real indoor environments
captured with a hand-held camera.

New view synthesis in the wild Only very few works explored unconstrained generation of new
views in real environments. Flynn et al. [13] consider a simpler version of the task where the reference
views cover most of the frustum of the test views allowing to form the majority of the synthesized
image by copying pixels from the reference views. Eslami et al. [12] proposed an end-to-end trained
Generative Query Network (GQN) that renders new viewpoints given a latent encoding of the scene
and a novel viewpoint. GQN can effortlessly browse simple synthetic environments  however it has

2

Figure 2: Scene-consistent optimization. For a given test scene  PerspectiveNet optimizes the
latent representations of new views in order to obtain a scene-consistent set of images that satisfy
geometric re-projection constraints of the scene camera system and have similar visual style.

not been tested in a real world setup. The recent method of Meshry et al. [29] captures a complete
distribution of possible appearance variations of a  mostly hole-free  image. This differs from our aim
of inpainting large undeﬁned regions. Recently  [33] trained a 3D ConvNet for generation of new
views of a single non-synthetic object instance.

Image inpainting Our method is also related to image inpainting. Early approaches [2  1  35  10 
23] were recently outperformed by deep methods. Isola et al. [18] used a Conditional Generative
Adversarial Network cGAN to translate between different types of pixel-wise labels. Many improve-
ments of the original cGAN architecture  including [38  45  44]  were later proposed. Avoiding the
use of GANs  [26] leverage partial convolutions in combination with the perceptual and style losses
[14] and achieve state-of-the-art results in semantic image inpainting. Recently  Ulyanov et al. [37]
demonstrated that convolutional layers constitute a strong prior for image denoising and  as such  can
be used for image denoising without prior training on a dataset of images.

3 Method
Task and naming conventions Our goal in this paper is to generate new views of an indoor scene
given a set of reference views captured by a handheld RGBD camera. More formally  we take as input
a set of Nref reference RGBD views {vi}Nref
i=1   vi ∈ R4×H×W annotated with their corresponding
camera extrinsic and intrinsic matrices gi ∈ SE(3) and K i ∈ R4×4 respectively1. At some pixels 
the depth value is incorrectly recorded as zero to denote missing data. Given camera parameters
{( ˆK i  ˆgi)}Ntest
i=1 of Ntest test views  our method attempts to generate their RGBD content with a
prediction {ˆvi}Ntest
Throughout this paper we denote image spatial locations u = (u1  u2) ∈ {1  ...  W} × {1  ...  H}.
At each pixel u  we can identify the corresponding per-pixel depth du ∈ R and color cu ∈ [0  1]3.
The knowledge of camera parameters and depth allows to back-project each pixel ui = (u1  u2) from
u ∼ (K igi)−1[u1  u2  du  1]T in the common coordinate
image i to its corresponding 3D point xi
frame of the corresponding scene. Since we work with rendering algorithms that occasionally produce
holes in images (i.e. pixels with undeﬁned color)  we denote by Ω(v) a set of all locations u in a view
v that are non-holes (pixels with deﬁned color).
In what follows  we describe a render-inpaint baseline followed by our main contribution consisting
of an extension of the baseline to a novel scene-consistent inpainting method  PerspectiveNet.

i=1 . We denote V = {vi}Nref

i=1 ∪ {ˆvi}Ntest

i=1 as a set of all views in a given scene.

3.1

Inpainting with a denoising RGBD autoencoder

As outlined above  we take a pragmatic approach and start by “copying” all possible pixels from the
reference views into the test ones. While this can be achieved with depth-based image rendering

1We use upper indices to index frames  while lower indices stand for spatial locations of pixels within a frame

3

Optimizedimage codesˆv1<latexit sha1_base64="Fq4+OWAwvIHhDQtx4TNtWOOObLI=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+r46mH</latexit><latexit sha1_base64="Fq4+OWAwvIHhDQtx4TNtWOOObLI=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+r46mH</latexit><latexit sha1_base64="Fq4+OWAwvIHhDQtx4TNtWOOObLI=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+r46mH</latexit><latexit sha1_base64="Fq4+OWAwvIHhDQtx4TNtWOOObLI=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+r46mH</latexit>ˆv2<latexit sha1_base64="JEFHTXxhLXn9iTgbRzW8AjjMJXg=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCraepiA==</latexit><latexit sha1_base64="JEFHTXxhLXn9iTgbRzW8AjjMJXg=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCraepiA==</latexit><latexit sha1_base64="JEFHTXxhLXn9iTgbRzW8AjjMJXg=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCraepiA==</latexit><latexit sha1_base64="JEFHTXxhLXn9iTgbRzW8AjjMJXg=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGr0wfig95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCraepiA==</latexit>ˆv3<latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="nRxmeUJaYR7N9/FkHHFLvbIWypE=">AAACGXicbZDLSgMxFIYz9VbHqu3azWARXJUZN7oU3LisYC/QDiWTOdOG5jIkZypl6Au49SV8Gncivo3pBbzUA4Gf/0sI50tywS2G4adX2dnd2z+oHvpHNf/45LRe61pdGAYdpoU2/YRaEFxBBzkK6OcGqEwE9JLp3ZL3ZmAs1+oR5znEko4Vzzij6Kr2qN4MW+Fqgu0QbUKTbGbU8BrDVLNCgkImqLWDKMwxLqlBzgQs/KEBBU9MS0lVWg6NTCGjhcBFmaPcwjb7xpOZw4WFnLIpHcPARUUl2LhcbbkILlyTBpk27igMVu3PFyWV1s5l4m5KihP7ly3L/9igwOwmLrnKCwTF1h9lhQhQB0tlQcoNMBRzFygz3G0asAk1lKET6/vOYfTX2HboXrWisBU9hKRKzsg5uSQRuSa35J60SYcwkpJn8uK9em/e+9p1xdtIb5Bf4318AW3JpAg=</latexit><latexit sha1_base64="cAUqjpeqLG44348rEjSgtkAnpGk=">AAACJHicbZDLahsxFIbPJM2ljnNxttmImkJXZiZZJMtAN1mmECcG25gzmjO2iKQZpDMOZvBLdNu+RJ+mdBPyKJEdQ1q7BwQ//ychzpeWWnmO4z/R1vaHnd29/Y+Ng+bh0fFJq3nvi8pJ6spCF66XoietLHVZsaZe6QhNqukhffy64A9Tcl4V9o5nJQ0Njq3KlUQOVW8wQRbT0cXopB134uWIzZCsQhtWcztqRa1BVsjKkGWp0ft+Epc8rNGxkprmjYEjS0+yMAZtVg+cySjHSvO8LtlsYJ+/48k04MpTifIRx9QP0aIhP6yX687F59BkIi9cOJbFsv37RY3G+5lJw02DPPHrbFH+j/Urzq+GtbJlxWTl20d5pQUXYuFOZMqRZD0LAaVTYVMhJ+hQcjDcaASJybqyzXB/3kniTvIthn04g0/wBRK4hGu4gVvoggQN3+EH/Ix+Rb+j5zfdW9HK+yn8M9HLK2kNqAE=</latexit><latexit sha1_base64="cAUqjpeqLG44348rEjSgtkAnpGk=">AAACJHicbZDLahsxFIbPJM2ljnNxttmImkJXZiZZJMtAN1mmECcG25gzmjO2iKQZpDMOZvBLdNu+RJ+mdBPyKJEdQ1q7BwQ//ychzpeWWnmO4z/R1vaHnd29/Y+Ng+bh0fFJq3nvi8pJ6spCF66XoietLHVZsaZe6QhNqukhffy64A9Tcl4V9o5nJQ0Njq3KlUQOVW8wQRbT0cXopB134uWIzZCsQhtWcztqRa1BVsjKkGWp0ft+Epc8rNGxkprmjYEjS0+yMAZtVg+cySjHSvO8LtlsYJ+/48k04MpTifIRx9QP0aIhP6yX687F59BkIi9cOJbFsv37RY3G+5lJw02DPPHrbFH+j/Urzq+GtbJlxWTl20d5pQUXYuFOZMqRZD0LAaVTYVMhJ+hQcjDcaASJybqyzXB/3kniTvIthn04g0/wBRK4hGu4gVvoggQN3+EH/Ix+Rb+j5zfdW9HK+yn8M9HLK2kNqAE=</latexit><latexit sha1_base64="p8AJNMKJdfIgydDnz1mMdjbqhRI=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjO60GXBjUsFq4W2lDuZOzaYZIbkTqUMfQm3+hI+jbgRt76FaS34Uw8EDue7l3BPnCvpKAxfg7n5hcWl5ZXVytr6xubWdnXn2mWFFdgUmcpsKwaHShpskiSFrdwi6FjhTXx3NuY3A7ROZuaKhjl2NdwamUoB5KNWpw/EB73j3nYtrIcT8VkTTU2NTXXRqwbVTpKJQqMhocC5dhTm1C3BkhQKR5WORYP3ItMaTFJ2rE4whULRqMxJz2CXfuP+wOPCYQ7iDm6x7a0Bja5bTs4d8QOfJDzNrH+G+CT9uVGCdm6oYz+pgfruLxuH/7F2Qelpt5QmLwiN+PooLRSnjI+744m0KEgNvQFhpb+Uiz5YEOQbrlR8idHfymbN9VE9CuvRZVhrhNM6V9ge22eHLGInrMHO2QVrMsEUe2CP7Cl4Dl6Ct+D9a3QumO7ssl8KPj4BriuphQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit><latexit sha1_base64="W7wW2qvllFfZ8vo5YciSBZcIoiM=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZ7gHxfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEr2upiQ==</latexit>ˆ1<latexit sha1_base64="ZyMerhphuNlPAoDUZZHCD3J1Uk0=">AAACMXicbZDNSsNAFIUn/lt/q0s3wSK4KokIuhTcuKxgVWxKuZncmMGZSZi5qZTQt3CrL+HTdCdufQmnteBPPTBwON+9DPfEhRSWgmDkzc0vLC4tr6zW1tY3Nre26zvXNi8NxzbPZW5uY7AohcY2CZJ4WxgEFUu8iR/Ox/ymj8aKXF/RoMCugnstUsGBXHQXZUBRkYle2NtuBM1gIn/WhFPTYFO1enWvHiU5LxVq4hKs7YRBQd0KDAkucViLDGp85LlSoJMqMirBFEpJw6ogNYNt+o2zvsOlxQL4A9xjx1kNCm23mhw89A9ckvhpbtzT5E/SnxsVKGsHKnaTCiizf9k4/I91SkpPu5XQRUmo+ddHaSl9yv1xe34iDHKSA2eAG+Eu9XkGBji5jms1V2L4t7JZc33UDINmeHncOAumda6wPbbPDlnITtgZu2At1macafbEntmL9+qNvDfv/Wt0zpvu7LJf8j4+Ac4/qqI=</latexit><latexit sha1_base64="ZyMerhphuNlPAoDUZZHCD3J1Uk0=">AAACMXicbZDNSsNAFIUn/lt/q0s3wSK4KokIuhTcuKxgVWxKuZncmMGZSZi5qZTQt3CrL+HTdCdufQmnteBPPTBwON+9DPfEhRSWgmDkzc0vLC4tr6zW1tY3Nre26zvXNi8NxzbPZW5uY7AohcY2CZJ4WxgEFUu8iR/Ox/ymj8aKXF/RoMCugnstUsGBXHQXZUBRkYle2NtuBM1gIn/WhFPTYFO1enWvHiU5LxVq4hKs7YRBQd0KDAkucViLDGp85LlSoJMqMirBFEpJw6ogNYNt+o2zvsOlxQL4A9xjx1kNCm23mhw89A9ckvhpbtzT5E/SnxsVKGsHKnaTCiizf9k4/I91SkpPu5XQRUmo+ddHaSl9yv1xe34iDHKSA2eAG+Eu9XkGBji5jms1V2L4t7JZc33UDINmeHncOAumda6wPbbPDlnITtgZu2At1macafbEntmL9+qNvDfv/Wt0zpvu7LJf8j4+Ac4/qqI=</latexit><latexit sha1_base64="ZyMerhphuNlPAoDUZZHCD3J1Uk0=">AAACMXicbZDNSsNAFIUn/lt/q0s3wSK4KokIuhTcuKxgVWxKuZncmMGZSZi5qZTQt3CrL+HTdCdufQmnteBPPTBwON+9DPfEhRSWgmDkzc0vLC4tr6zW1tY3Nre26zvXNi8NxzbPZW5uY7AohcY2CZJ4WxgEFUu8iR/Ox/ymj8aKXF/RoMCugnstUsGBXHQXZUBRkYle2NtuBM1gIn/WhFPTYFO1enWvHiU5LxVq4hKs7YRBQd0KDAkucViLDGp85LlSoJMqMirBFEpJw6ogNYNt+o2zvsOlxQL4A9xjx1kNCm23mhw89A9ckvhpbtzT5E/SnxsVKGsHKnaTCiizf9k4/I91SkpPu5XQRUmo+ddHaSl9yv1xe34iDHKSA2eAG+Eu9XkGBji5jms1V2L4t7JZc33UDINmeHncOAumda6wPbbPDlnITtgZu2At1macafbEntmL9+qNvDfv/Wt0zpvu7LJf8j4+Ac4/qqI=</latexit><latexit sha1_base64="ZyMerhphuNlPAoDUZZHCD3J1Uk0=">AAACMXicbZDNSsNAFIUn/lt/q0s3wSK4KokIuhTcuKxgVWxKuZncmMGZSZi5qZTQt3CrL+HTdCdufQmnteBPPTBwON+9DPfEhRSWgmDkzc0vLC4tr6zW1tY3Nre26zvXNi8NxzbPZW5uY7AohcY2CZJ4WxgEFUu8iR/Ox/ymj8aKXF/RoMCugnstUsGBXHQXZUBRkYle2NtuBM1gIn/WhFPTYFO1enWvHiU5LxVq4hKs7YRBQd0KDAkucViLDGp85LlSoJMqMirBFEpJw6ogNYNt+o2zvsOlxQL4A9xjx1kNCm23mhw89A9ckvhpbtzT5E/SnxsVKGsHKnaTCiizf9k4/I91SkpPu5XQRUmo+ddHaSl9yv1xe34iDHKSA2eAG+Eu9XkGBji5jms1V2L4t7JZc33UDINmeHncOAumda6wPbbPDlnITtgZu2At1macafbEntmL9+qNvDfv/Wt0zpvu7LJf8j4+Ac4/qqI=</latexit>ˆ2<latexit sha1_base64="p1trdzKm+t89OcLMc8/QWypQuX4=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlE0KXgxqWCrWJTys3kxgzOTMLMTaWEvoVbfQmfxp249SWc1oI/9cDA4Xz3MtwTF1JYCoJXb25+YXFpubZSX11b39jcamx3bF4ajm2ey9zcxGBRCo1tEiTxpjAIKpZ4Hd+fjfn1AI0Vub6iYYE9BXdapIIDueg2yoCiIhP9w/5WM2gFE/mzJpyaJpvqot/wGlGS81KhJi7B2m4YFNSrwJDgEkf1yKDGB54rBTqpIqMSTKGUNKoKUjPYpt84GzhcWiyA38Mddp3VoND2qsnBI3/fJYmf5sY9Tf4k/blRgbJ2qGI3qYAy+5eNw/9Yt6T0pFcJXZSEmn99lJbSp9wft+cnwiAnOXQGuBHuUp9nYICT67hedyWGfyubNZ3DVhi0wsuj5mkwrbPGdtkeO2AhO2an7JxdsDbjTLNH9sSevRfv1Xvz3r9G57zpzg77Je/jE9ADqqM=</latexit><latexit sha1_base64="p1trdzKm+t89OcLMc8/QWypQuX4=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlE0KXgxqWCrWJTys3kxgzOTMLMTaWEvoVbfQmfxp249SWc1oI/9cDA4Xz3MtwTF1JYCoJXb25+YXFpubZSX11b39jcamx3bF4ajm2ey9zcxGBRCo1tEiTxpjAIKpZ4Hd+fjfn1AI0Vub6iYYE9BXdapIIDueg2yoCiIhP9w/5WM2gFE/mzJpyaJpvqot/wGlGS81KhJi7B2m4YFNSrwJDgEkf1yKDGB54rBTqpIqMSTKGUNKoKUjPYpt84GzhcWiyA38Mddp3VoND2qsnBI3/fJYmf5sY9Tf4k/blRgbJ2qGI3qYAy+5eNw/9Yt6T0pFcJXZSEmn99lJbSp9wft+cnwiAnOXQGuBHuUp9nYICT67hedyWGfyubNZ3DVhi0wsuj5mkwrbPGdtkeO2AhO2an7JxdsDbjTLNH9sSevRfv1Xvz3r9G57zpzg77Je/jE9ADqqM=</latexit><latexit sha1_base64="p1trdzKm+t89OcLMc8/QWypQuX4=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlE0KXgxqWCrWJTys3kxgzOTMLMTaWEvoVbfQmfxp249SWc1oI/9cDA4Xz3MtwTF1JYCoJXb25+YXFpubZSX11b39jcamx3bF4ajm2ey9zcxGBRCo1tEiTxpjAIKpZ4Hd+fjfn1AI0Vub6iYYE9BXdapIIDueg2yoCiIhP9w/5WM2gFE/mzJpyaJpvqot/wGlGS81KhJi7B2m4YFNSrwJDgEkf1yKDGB54rBTqpIqMSTKGUNKoKUjPYpt84GzhcWiyA38Mddp3VoND2qsnBI3/fJYmf5sY9Tf4k/blRgbJ2qGI3qYAy+5eNw/9Yt6T0pFcJXZSEmn99lJbSp9wft+cnwiAnOXQGuBHuUp9nYICT67hedyWGfyubNZ3DVhi0wsuj5mkwrbPGdtkeO2AhO2an7JxdsDbjTLNH9sSevRfv1Xvz3r9G57zpzg77Je/jE9ADqqM=</latexit><latexit sha1_base64="p1trdzKm+t89OcLMc8/QWypQuX4=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlE0KXgxqWCrWJTys3kxgzOTMLMTaWEvoVbfQmfxp249SWc1oI/9cDA4Xz3MtwTF1JYCoJXb25+YXFpubZSX11b39jcamx3bF4ajm2ey9zcxGBRCo1tEiTxpjAIKpZ4Hd+fjfn1AI0Vub6iYYE9BXdapIIDueg2yoCiIhP9w/5WM2gFE/mzJpyaJpvqot/wGlGS81KhJi7B2m4YFNSrwJDgEkf1yKDGB54rBTqpIqMSTKGUNKoKUjPYpt84GzhcWiyA38Mddp3VoND2qsnBI3/fJYmf5sY9Tf4k/blRgbJ2qGI3qYAy+5eNw/9Yt6T0pFcJXZSEmn99lJbSp9wft+cnwiAnOXQGuBHuUp9nYICT67hedyWGfyubNZ3DVhi0wsuj5mkwrbPGdtkeO2AhO2an7JxdsDbjTLNH9sSevRfv1Xvz3r9G57zpzg77Je/jE9ADqqM=</latexit>ˆ3<latexit sha1_base64="wZf48FHF9UaQcWPD7I3mtauHPC0=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlU0KXgxqWCVbEp5WZyYwZnJmHmRimhb+FWX8Kn6U7c+hJOa8GfemDgcL57Ge6JCyksBcHQm5mdm19YrC3Vl1dW19Y3GptXNi8NxzbPZW5uYrAohcY2CZJ4UxgEFUu8ju9PR/z6AY0Vub6kfoFdBXdapIIDueg2yoCiIhO9g95GM2gFY/nTJpyYJpvovNfwGlGS81KhJi7B2k4YFNStwJDgEgf1yKDGR54rBTqpIqMSTKGUNKgKUlPYpt84e3C4tFgAv4c77DirQaHtVuODB/6uSxI/zY17mvxx+nOjAmVtX8VuUgFl9i8bhf+xTknpcbcSuigJNf/6KC2lT7k/as9PhEFOsu8McCPcpT7PwAAn13G97koM/1Y2ba72W2HQCi8OmyfBpM4a22Y7bI+F7IidsDN2ztqMM82e2DN78V69offmvX+NzniTnS32S97HJ9HHqqQ=</latexit><latexit sha1_base64="wZf48FHF9UaQcWPD7I3mtauHPC0=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlU0KXgxqWCVbEp5WZyYwZnJmHmRimhb+FWX8Kn6U7c+hJOa8GfemDgcL57Ge6JCyksBcHQm5mdm19YrC3Vl1dW19Y3GptXNi8NxzbPZW5uYrAohcY2CZJ4UxgEFUu8ju9PR/z6AY0Vub6kfoFdBXdapIIDueg2yoCiIhO9g95GM2gFY/nTJpyYJpvovNfwGlGS81KhJi7B2k4YFNStwJDgEgf1yKDGR54rBTqpIqMSTKGUNKgKUlPYpt84e3C4tFgAv4c77DirQaHtVuODB/6uSxI/zY17mvxx+nOjAmVtX8VuUgFl9i8bhf+xTknpcbcSuigJNf/6KC2lT7k/as9PhEFOsu8McCPcpT7PwAAn13G97koM/1Y2ba72W2HQCi8OmyfBpM4a22Y7bI+F7IidsDN2ztqMM82e2DN78V69offmvX+NzniTnS32S97HJ9HHqqQ=</latexit><latexit sha1_base64="wZf48FHF9UaQcWPD7I3mtauHPC0=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlU0KXgxqWCVbEp5WZyYwZnJmHmRimhb+FWX8Kn6U7c+hJOa8GfemDgcL57Ge6JCyksBcHQm5mdm19YrC3Vl1dW19Y3GptXNi8NxzbPZW5uYrAohcY2CZJ4UxgEFUu8ju9PR/z6AY0Vub6kfoFdBXdapIIDueg2yoCiIhO9g95GM2gFY/nTJpyYJpvovNfwGlGS81KhJi7B2k4YFNStwJDgEgf1yKDGR54rBTqpIqMSTKGUNKgKUlPYpt84e3C4tFgAv4c77DirQaHtVuODB/6uSxI/zY17mvxx+nOjAmVtX8VuUgFl9i8bhf+xTknpcbcSuigJNf/6KC2lT7k/as9PhEFOsu8McCPcpT7PwAAn13G97koM/1Y2ba72W2HQCi8OmyfBpM4a22Y7bI+F7IidsDN2ztqMM82e2DN78V69offmvX+NzniTnS32S97HJ9HHqqQ=</latexit><latexit sha1_base64="wZf48FHF9UaQcWPD7I3mtauHPC0=">AAACMXicbZDNSsNAFIUn/tb6W126CRbBVUlU0KXgxqWCVbEp5WZyYwZnJmHmRimhb+FWX8Kn6U7c+hJOa8GfemDgcL57Ge6JCyksBcHQm5mdm19YrC3Vl1dW19Y3GptXNi8NxzbPZW5uYrAohcY2CZJ4UxgEFUu8ju9PR/z6AY0Vub6kfoFdBXdapIIDueg2yoCiIhO9g95GM2gFY/nTJpyYJpvovNfwGlGS81KhJi7B2k4YFNStwJDgEgf1yKDGR54rBTqpIqMSTKGUNKgKUlPYpt84e3C4tFgAv4c77DirQaHtVuODB/6uSxI/zY17mvxx+nOjAmVtX8VuUgFl9i8bhf+xTknpcbcSuigJNf/6KC2lT7k/as9PhEFOsu8McCPcpT7PwAAn13G97koM/1Y2ba72W2HQCi8OmyfBpM4a22Y7bI+F7IidsDN2ztqMM82e2DN78V69offmvX+NzniTnS32S97HJ9HHqqQ=</latexit>¯v3<latexit sha1_base64="1EtXUrzjIRUPUOEGfyDV4wVz0sk=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZjsHyfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEoR+pgQ==</latexit><latexit sha1_base64="1EtXUrzjIRUPUOEGfyDV4wVz0sk=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZjsHyfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEoR+pgQ==</latexit><latexit sha1_base64="1EtXUrzjIRUPUOEGfyDV4wVz0sk=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZjsHyfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEoR+pgQ==</latexit><latexit sha1_base64="1EtXUrzjIRUPUOEGfyDV4wVz0sk=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMq6FJw41LBaqEt5U7mjg0mmSG5UylDX8KtvoRPI27ErW9hWgv+1AOBw/nuJdwT50o6CsPXYGZ2bn5hcWm5srK6tr6xWd26dllhBTZEpjLbjMGhkgYbJElhM7cIOlZ4E9+djfhNH62TmbmiQY4dDbdGplIA+ajZjsHyfvewu1kL6+FYfNpEE1NjE110q0G1nWSi0GhIKHCuFYU5dUqwJIXCYaVt0eC9yLQGk5RtqxNMoVA0LHPSU9il37jX97hwmIO4g1tseWtAo+uU43OHfM8nCU8z658hPk5/bpSgnRvo2E9qoJ77y0bhf6xVUHrSKaXJC0Ijvj5KC8Up46PueCItClIDb0BY6S/logcWBPmGKxVfYvS3smlzfVCPwnp0eVQ7DSd1LrEdtsv2WcSO2Sk7ZxeswQRT7IE9sqfgOXgJ3oL3r9GZYLKzzX4p+PgEoR+pgQ==</latexit>¯v2<latexit sha1_base64="uI795dLJ2bYzFmKC7mv/FeGykgo=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCn1upgA==</latexit><latexit sha1_base64="uI795dLJ2bYzFmKC7mv/FeGykgo=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCn1upgA==</latexit><latexit sha1_base64="uI795dLJ2bYzFmKC7mv/FeGykgo=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCn1upgA==</latexit><latexit sha1_base64="uI795dLJ2bYzFmKC7mv/FeGykgo=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg95Rb7sW1sOJ+KyJpqbGprroVYNqJ8lEodGQUOBcOwpz6pZgSQqFo0rHosF7kWkNJik7VieYQqFoVOakZ7BLv3F/4HHhMAdxB7fY9taARtctJ+eO+IFPEp5m1j9DfJL+3ChBOzfUsZ/UQH33l43D/1i7oPS0W0qTF4RGfH2UFopTxsfd8URaFKSG3oCw0l/KRR8sCPINVyq+xOhvZbPm+qgehfXo8rjWCKd1rrA9ts8OWcROWIOdswvWZIIp9sAe2VPwHLwEb8H71+hcMN3ZZb8UfHwCn1upgA==</latexit>¯v1<latexit sha1_base64="ZMvLhHDNj+WllAQ8Tw3UuPc8QGc=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+dl6l/</latexit><latexit sha1_base64="ZMvLhHDNj+WllAQ8Tw3UuPc8QGc=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+dl6l/</latexit><latexit sha1_base64="ZMvLhHDNj+WllAQ8Tw3UuPc8QGc=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+dl6l/</latexit><latexit sha1_base64="ZMvLhHDNj+WllAQ8Tw3UuPc8QGc=">AAACL3icbZDNSgMxFIUz/lt/q0s3wSK4KjMi6LLgxqWC1UJbyp3MHRtMMkNyp1KGvoRbfQmfRtyIW9/CtBb8qQcCh/PdS7gnzpV0FIavwdz8wuLS8spqZW19Y3Nru7pz7bLCCmyKTGW2FYNDJQ02SZLCVm4RdKzwJr47G/ObAVonM3NFwxy7Gm6NTKUA8lGrE4Plg17U266F9XAiPmuiqamxqS561aDaSTJRaDQkFDjXjsKcuiVYkkLhqNKxaPBeZFqDScqO1QmmUCgalTnpGezSb9wfeFw4zEHcwS22vTWg0XXLybkjfuCThKeZ9c8Qn6Q/N0rQzg117Cc1UN/9ZePwP9YuKD3tltLkBaERXx+lheKU8XF3PJEWBamhNyCs9Jdy0QcLgnzDlYovMfpb2ay5PqpHYT26PK41wmmdK2yP7bNDFrET1mDn7II1mWCKPbBH9hQ8By/BW/D+NToXTHd22S8FH5+dl6l/</latexit>v1<latexit sha1_base64="crcEdfdJfNBPlQyfdTIl9Xlf7Js=">AAACKnicbZDNSgMxFIUz/tb61+rSzWARXJUZEXRZcOOyov2BtpRM5k4bmmSG5E6lDH0Et/oSPo274tYHMW0H1NYDgcP57iXcEySCG/S8mbOxubW9s1vYK+4fHB4dl8onTROnmkGDxSLW7YAaEFxBAzkKaCcaqAwEtILR3Zy3xqANj9UTThLoSTpQPOKMoo0ex32/X6p4VW8hd934uamQXPV+2Sl3w5ilEhQyQY3p+F6CvYxq5EzAtNjVoOCZxVJSFWZdLUOIaCpwmiUo17CJfvBwbHFqIKFsRAfQsVZRCaaXLS6duhc2Cd0o1vYpdBfp742MSmMmMrCTkuLQrLJ5+B/rpBjd9jKukhRBseVHUSpcjN15bW7INTAUE2so09xe6rIh1ZShLbdYtCX6q5Wtm+ZV1feq/sN1pebldRbIGTknl8QnN6RG7kmdNAgjA/JCXsmb8+58ODPnczm64eQ7p+SPnK9v3h6nnA==</latexit><latexit sha1_base64="crcEdfdJfNBPlQyfdTIl9Xlf7Js=">AAACKnicbZDNSgMxFIUz/tb61+rSzWARXJUZEXRZcOOyov2BtpRM5k4bmmSG5E6lDH0Et/oSPo274tYHMW0H1NYDgcP57iXcEySCG/S8mbOxubW9s1vYK+4fHB4dl8onTROnmkGDxSLW7YAaEFxBAzkKaCcaqAwEtILR3Zy3xqANj9UTThLoSTpQPOKMoo0ex32/X6p4VW8hd934uamQXPV+2Sl3w5ilEhQyQY3p+F6CvYxq5EzAtNjVoOCZxVJSFWZdLUOIaCpwmiUo17CJfvBwbHFqIKFsRAfQsVZRCaaXLS6duhc2Cd0o1vYpdBfp742MSmMmMrCTkuLQrLJ5+B/rpBjd9jKukhRBseVHUSpcjN15bW7INTAUE2so09xe6rIh1ZShLbdYtCX6q5Wtm+ZV1feq/sN1pebldRbIGTknl8QnN6RG7kmdNAgjA/JCXsmb8+58ODPnczm64eQ7p+SPnK9v3h6nnA==</latexit><latexit sha1_base64="crcEdfdJfNBPlQyfdTIl9Xlf7Js=">AAACKnicbZDNSgMxFIUz/tb61+rSzWARXJUZEXRZcOOyov2BtpRM5k4bmmSG5E6lDH0Et/oSPo274tYHMW0H1NYDgcP57iXcEySCG/S8mbOxubW9s1vYK+4fHB4dl8onTROnmkGDxSLW7YAaEFxBAzkKaCcaqAwEtILR3Zy3xqANj9UTThLoSTpQPOKMoo0ex32/X6p4VW8hd934uamQXPV+2Sl3w5ilEhQyQY3p+F6CvYxq5EzAtNjVoOCZxVJSFWZdLUOIaCpwmiUo17CJfvBwbHFqIKFsRAfQsVZRCaaXLS6duhc2Cd0o1vYpdBfp742MSmMmMrCTkuLQrLJ5+B/rpBjd9jKukhRBseVHUSpcjN15bW7INTAUE2so09xe6rIh1ZShLbdYtCX6q5Wtm+ZV1feq/sN1pebldRbIGTknl8QnN6RG7kmdNAgjA/JCXsmb8+58ODPnczm64eQ7p+SPnK9v3h6nnA==</latexit><latexit sha1_base64="crcEdfdJfNBPlQyfdTIl9Xlf7Js=">AAACKnicbZDNSgMxFIUz/tb61+rSzWARXJUZEXRZcOOyov2BtpRM5k4bmmSG5E6lDH0Et/oSPo274tYHMW0H1NYDgcP57iXcEySCG/S8mbOxubW9s1vYK+4fHB4dl8onTROnmkGDxSLW7YAaEFxBAzkKaCcaqAwEtILR3Zy3xqANj9UTThLoSTpQPOKMoo0ex32/X6p4VW8hd934uamQXPV+2Sl3w5ilEhQyQY3p+F6CvYxq5EzAtNjVoOCZxVJSFWZdLUOIaCpwmiUo17CJfvBwbHFqIKFsRAfQsVZRCaaXLS6duhc2Cd0o1vYpdBfp742MSmMmMrCTkuLQrLJ5+B/rpBjd9jKukhRBseVHUSpcjN15bW7INTAUE2so09xe6rIh1ZShLbdYtCX6q5Wtm+ZV1feq/sN1pebldRbIGTknl8QnN6RG7kmdNAgjA/JCXsmb8+58ODPnczm64eQ7p+SPnK9v3h6nnA==</latexit>v2<latexit sha1_base64="mtvJcmu/0k+HIBlqJOz3/KQxcSI=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozRdBlwY3LivYCbSmZzJk2NMkMyZlKGfoIbvUlfBp3xa0PYnoBtfWHwM//nUM4f5AIbtDzZk5ua3tndy+/Xzg4PDo+KZZOmyZONYMGi0Ws2wE1ILiCBnIU0E40UBkIaAWjuzlvjUEbHqsnnCTQk3SgeMQZRRs9jvvVfrHsVbyF3E3jr0yZrFTvl5xSN4xZKkEhE9SYju8l2MuoRs4ETAtdDQqeWSwlVWHW1TKEiKYCp1mCcgOb6AcPxxanBhLKRnQAHWsVlWB62eLSqXtpk9CNYm2fQneR/t7IqDRmIgM7KSkOzTqbh/+xTorRbS/jKkkRFFt+FKXCxdid1+aGXANDMbGGMs3tpS4bUk0Z2nILBVuiv17ZpmlWK75X8R+uyzVvVWeenJMLckV8ckNq5J7USYMwMiAv5JW8Oe/OhzNzPpejOWe1c0b+yPn6Bt/ip50=</latexit><latexit sha1_base64="mtvJcmu/0k+HIBlqJOz3/KQxcSI=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozRdBlwY3LivYCbSmZzJk2NMkMyZlKGfoIbvUlfBp3xa0PYnoBtfWHwM//nUM4f5AIbtDzZk5ua3tndy+/Xzg4PDo+KZZOmyZONYMGi0Ws2wE1ILiCBnIU0E40UBkIaAWjuzlvjUEbHqsnnCTQk3SgeMQZRRs9jvvVfrHsVbyF3E3jr0yZrFTvl5xSN4xZKkEhE9SYju8l2MuoRs4ETAtdDQqeWSwlVWHW1TKEiKYCp1mCcgOb6AcPxxanBhLKRnQAHWsVlWB62eLSqXtpk9CNYm2fQneR/t7IqDRmIgM7KSkOzTqbh/+xTorRbS/jKkkRFFt+FKXCxdid1+aGXANDMbGGMs3tpS4bUk0Z2nILBVuiv17ZpmlWK75X8R+uyzVvVWeenJMLckV8ckNq5J7USYMwMiAv5JW8Oe/OhzNzPpejOWe1c0b+yPn6Bt/ip50=</latexit><latexit sha1_base64="mtvJcmu/0k+HIBlqJOz3/KQxcSI=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozRdBlwY3LivYCbSmZzJk2NMkMyZlKGfoIbvUlfBp3xa0PYnoBtfWHwM//nUM4f5AIbtDzZk5ua3tndy+/Xzg4PDo+KZZOmyZONYMGi0Ws2wE1ILiCBnIU0E40UBkIaAWjuzlvjUEbHqsnnCTQk3SgeMQZRRs9jvvVfrHsVbyF3E3jr0yZrFTvl5xSN4xZKkEhE9SYju8l2MuoRs4ETAtdDQqeWSwlVWHW1TKEiKYCp1mCcgOb6AcPxxanBhLKRnQAHWsVlWB62eLSqXtpk9CNYm2fQneR/t7IqDRmIgM7KSkOzTqbh/+xTorRbS/jKkkRFFt+FKXCxdid1+aGXANDMbGGMs3tpS4bUk0Z2nILBVuiv17ZpmlWK75X8R+uyzVvVWeenJMLckV8ckNq5J7USYMwMiAv5JW8Oe/OhzNzPpejOWe1c0b+yPn6Bt/ip50=</latexit><latexit sha1_base64="mtvJcmu/0k+HIBlqJOz3/KQxcSI=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozRdBlwY3LivYCbSmZzJk2NMkMyZlKGfoIbvUlfBp3xa0PYnoBtfWHwM//nUM4f5AIbtDzZk5ua3tndy+/Xzg4PDo+KZZOmyZONYMGi0Ws2wE1ILiCBnIU0E40UBkIaAWjuzlvjUEbHqsnnCTQk3SgeMQZRRs9jvvVfrHsVbyF3E3jr0yZrFTvl5xSN4xZKkEhE9SYju8l2MuoRs4ETAtdDQqeWSwlVWHW1TKEiKYCp1mCcgOb6AcPxxanBhLKRnQAHWsVlWB62eLSqXtpk9CNYm2fQneR/t7IqDRmIgM7KSkOzTqbh/+xTorRbS/jKkkRFFt+FKXCxdid1+aGXANDMbGGMs3tpS4bUk0Z2nILBVuiv17ZpmlWK75X8R+uyzVvVWeenJMLckV8ckNq5J7USYMwMiAv5JW8Oe/OhzNzPpejOWe1c0b+yPn6Bt/ip50=</latexit>v3<latexit sha1_base64="Zs7Q9j3xcaWV4UmgsMV8hNGUdyo=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozKuiy4MZlRXuBtpRM5kwbmmSG5EylDH0Et/oSPo274tYHMb2A2vpD4Of/ziGcP0gEN+h5Uye3sbm1vZPfLeztHxweFUvHDROnmkGdxSLWrYAaEFxBHTkKaCUaqAwENIPh3Yw3R6ANj9UTjhPoStpXPOKMoo0eR72rXrHsVby53HXjL02ZLFXrlZxSJ4xZKkEhE9SYtu8l2M2oRs4ETAodDQqeWSwlVWHW0TKEiKYCJ1mCcg2b6AcPRhanBhLKhrQPbWsVlWC62fzSiXtuk9CNYm2fQnee/t7IqDRmLAM7KSkOzCqbhf+xdorRbTfjKkkRFFt8FKXCxdid1eaGXANDMbaGMs3tpS4bUE0Z2nILBVuiv1rZumlcVnyv4j9cl6vess48OSVn5IL45IZUyT2pkTphpE9eyCt5c96dD2fqfC5Gc85y54T8kfP1DeGmp54=</latexit><latexit sha1_base64="Zs7Q9j3xcaWV4UmgsMV8hNGUdyo=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozKuiy4MZlRXuBtpRM5kwbmmSG5EylDH0Et/oSPo274tYHMb2A2vpD4Of/ziGcP0gEN+h5Uye3sbm1vZPfLeztHxweFUvHDROnmkGdxSLWrYAaEFxBHTkKaCUaqAwENIPh3Yw3R6ANj9UTjhPoStpXPOKMoo0eR72rXrHsVby53HXjL02ZLFXrlZxSJ4xZKkEhE9SYtu8l2M2oRs4ETAodDQqeWSwlVWHW0TKEiKYCJ1mCcg2b6AcPRhanBhLKhrQPbWsVlWC62fzSiXtuk9CNYm2fQnee/t7IqDRmLAM7KSkOzCqbhf+xdorRbTfjKkkRFFt8FKXCxdid1eaGXANDMbaGMs3tpS4bUE0Z2nILBVuiv1rZumlcVnyv4j9cl6vess48OSVn5IL45IZUyT2pkTphpE9eyCt5c96dD2fqfC5Gc85y54T8kfP1DeGmp54=</latexit><latexit sha1_base64="Zs7Q9j3xcaWV4UmgsMV8hNGUdyo=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozKuiy4MZlRXuBtpRM5kwbmmSG5EylDH0Et/oSPo274tYHMb2A2vpD4Of/ziGcP0gEN+h5Uye3sbm1vZPfLeztHxweFUvHDROnmkGdxSLWrYAaEFxBHTkKaCUaqAwENIPh3Yw3R6ANj9UTjhPoStpXPOKMoo0eR72rXrHsVby53HXjL02ZLFXrlZxSJ4xZKkEhE9SYtu8l2M2oRs4ETAodDQqeWSwlVWHW0TKEiKYCJ1mCcg2b6AcPRhanBhLKhrQPbWsVlWC62fzSiXtuk9CNYm2fQnee/t7IqDRmLAM7KSkOzCqbhf+xdorRbTfjKkkRFFt8FKXCxdid1eaGXANDMbaGMs3tpS4bUE0Z2nILBVuiv1rZumlcVnyv4j9cl6vess48OSVn5IL45IZUyT2pkTphpE9eyCt5c96dD2fqfC5Gc85y54T8kfP1DeGmp54=</latexit><latexit sha1_base64="Zs7Q9j3xcaWV4UmgsMV8hNGUdyo=">AAACKnicbZDLSgMxFIYz9VbrrdWlm8EiuCozKuiy4MZlRXuBtpRM5kwbmmSG5EylDH0Et/oSPo274tYHMb2A2vpD4Of/ziGcP0gEN+h5Uye3sbm1vZPfLeztHxweFUvHDROnmkGdxSLWrYAaEFxBHTkKaCUaqAwENIPh3Yw3R6ANj9UTjhPoStpXPOKMoo0eR72rXrHsVby53HXjL02ZLFXrlZxSJ4xZKkEhE9SYtu8l2M2oRs4ETAodDQqeWSwlVWHW0TKEiKYCJ1mCcg2b6AcPRhanBhLKhrQPbWsVlWC62fzSiXtuk9CNYm2fQnee/t7IqDRmLAM7KSkOzCqbhf+xdorRbTfjKkkRFFt8FKXCxdid1eaGXANDMbaGMs3tpS4bUE0Z2nILBVuiv1rZumlcVnyv4j9cl6vess48OSVn5IL45IZUyT2pkTphpE9eyCt5c96dD2fqfC5Gc85y54T8kfP1DeGmp54=</latexit>Partial rendersScene-consistentRGBD predictionsConsistency loss`cons<latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit>Style-consistency loss`style<latexit sha1_base64="ooKTz9hn+ambPgjLtVfcgNm0dvc=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKthaaUDbbTbt0swm7EyGE/g0vHhTx6p/x5r9x2+agrQ8GHu/N7M68MJXCoOt+O5W19Y3Nrep2bWd3b/+gfnjUNUmmGe+wRCa6F1LDpVC8gwIl76Wa0ziU/DGc3M78xyeujUjUA+YpD2I6UiISjKKVfJ9LOSgM5pJPB/WG23TnIKvEK0kDSrQH9S9/mLAs5gqZpMb0PTfFoKAaBbPv1fzM8JSyCR3xvqWKxtwExXznKTmzypBEibalkMzV3xMFjY3J49B2xhTHZtmbif95/Qyj66AQKs2QK7b4KMokwYTMAiBDoTlDmVtCmRZ2V8LGVFOGNqaaDcFbPnmVdC+antv07i8brZsyjiqcwCmcgwdX0II7aEMHGKTwDK/w5mTOi/PufCxaK045cwx/4Hz+AKhikhM=</latexit><latexit sha1_base64="ooKTz9hn+ambPgjLtVfcgNm0dvc=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKthaaUDbbTbt0swm7EyGE/g0vHhTx6p/x5r9x2+agrQ8GHu/N7M68MJXCoOt+O5W19Y3Nrep2bWd3b/+gfnjUNUmmGe+wRCa6F1LDpVC8gwIl76Wa0ziU/DGc3M78xyeujUjUA+YpD2I6UiISjKKVfJ9LOSgM5pJPB/WG23TnIKvEK0kDSrQH9S9/mLAs5gqZpMb0PTfFoKAaBbPv1fzM8JSyCR3xvqWKxtwExXznKTmzypBEibalkMzV3xMFjY3J49B2xhTHZtmbif95/Qyj66AQKs2QK7b4KMokwYTMAiBDoTlDmVtCmRZ2V8LGVFOGNqaaDcFbPnmVdC+antv07i8brZsyjiqcwCmcgwdX0II7aEMHGKTwDK/w5mTOi/PufCxaK045cwx/4Hz+AKhikhM=</latexit><latexit sha1_base64="ooKTz9hn+ambPgjLtVfcgNm0dvc=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKthaaUDbbTbt0swm7EyGE/g0vHhTx6p/x5r9x2+agrQ8GHu/N7M68MJXCoOt+O5W19Y3Nrep2bWd3b/+gfnjUNUmmGe+wRCa6F1LDpVC8gwIl76Wa0ziU/DGc3M78xyeujUjUA+YpD2I6UiISjKKVfJ9LOSgM5pJPB/WG23TnIKvEK0kDSrQH9S9/mLAs5gqZpMb0PTfFoKAaBbPv1fzM8JSyCR3xvqWKxtwExXznKTmzypBEibalkMzV3xMFjY3J49B2xhTHZtmbif95/Qyj66AQKs2QK7b4KMokwYTMAiBDoTlDmVtCmRZ2V8LGVFOGNqaaDcFbPnmVdC+antv07i8brZsyjiqcwCmcgwdX0II7aEMHGKTwDK/w5mTOi/PufCxaK045cwx/4Hz+AKhikhM=</latexit><latexit sha1_base64="ooKTz9hn+ambPgjLtVfcgNm0dvc=">AAAB83icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cKthaaUDbbTbt0swm7EyGE/g0vHhTx6p/x5r9x2+agrQ8GHu/N7M68MJXCoOt+O5W19Y3Nrep2bWd3b/+gfnjUNUmmGe+wRCa6F1LDpVC8gwIl76Wa0ziU/DGc3M78xyeujUjUA+YpD2I6UiISjKKVfJ9LOSgM5pJPB/WG23TnIKvEK0kDSrQH9S9/mLAs5gqZpMb0PTfFoKAaBbPv1fzM8JSyCR3xvqWKxtwExXznKTmzypBEibalkMzV3xMFjY3J49B2xhTHZtmbif95/Qyj66AQKs2QK7b4KMokwYTMAiBDoTlDmVtCmRZ2V8LGVFOGNqaaDcFbPnmVdC+antv07i8brZsyjiqcwCmcgwdX0II7aEMHGKTwDK/w5mTOi/PufCxaK045cwx/4Hz+AKhikhM=</latexit>ReferenceRGBD views0<latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit>0<latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit>0<latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit><latexit sha1_base64="NxNS9oriLUgtLmg32GFSPW+y3Nc=">AAACLHicbZDNSsNAFIUn9a/Wv1aXboJFdFUSEXRZcOOygm2FtpTJ5KYdOjMJMzeVEvoMbvUlfBo3Im59DqdtQG09MHA4370M9wSJ4AY9790prK1vbG4Vt0s7u3v7B+XKYcvEqWbQZLGI9UNADQiuoIkcBTwkGqgMBLSD0c2Mt8egDY/VPU4S6Ek6UDzijKKNmt3GkJ/1y1Wv5s3lrho/N1WSq9GvOJVuGLNUgkImqDEd30uwl1GNnAmYlroaFDyyWEqqwqyrZQgRTQVOswTlCjbRDx6OLU4NJJSN6AA61ioqwfSy+a1T99QmoRvF2j6F7jz9vZFRacxEBnZSUhyaZTYL/2OdFKPrXsZVkiIotvgoSoWLsTsrzg25BoZiYg1lmttLXTakmjK09ZZKtkR/ubJV07qo+V7Nv7us1r28ziI5JifknPjkitTJLWmQJmGEkyfyTF6cV+fN+XA+F6MFJ985In/kfH0DQdSoTg==</latexit>Consistency loss`cons<latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit><latexit sha1_base64="+W9Qx7R38C1ozYGezFS/1Lj42/Y=">AAAB8nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBxEByhL3NXLJkb/fY3RPCkZ9hY6GIrb/Gzn/jJrlCEx8MPN6bYWZelApurO9/e6W19Y3NrfJ2ZWd3b/+genjUNirTDFtMCaU7ETUouMSW5VZgJ9VIk0jgYzS+nfmPT6gNV/LBTlIMEzqUPOaMWid1eyhEP2dKmmm/WvPr/hxklQQFqUGBZr/61RsoliUoLRPUmG7gpzbMqbacCZxWepnBlLIxHWLXUUkTNGE+P3lKzpwyILHSrqQlc/X3RE4TYyZJ5DoTakdm2ZuJ/3ndzMbXYc5lmlmUbLEozgSxisz+JwOukVkxcYQyzd2thI2opsy6lCouhGD55VXSvqgHfj24v6w1boo4ynACp3AOAVxBA+6gCS1goOAZXuHNs96L9+59LFpLXjFzDH/gff4Av2GRiw==</latexit>DenoisingAEDenoisingAEDenoisingAEFigure 3: We ﬁrst train a denoising RGBD autoencoder Φ for an inpainting task on a large dataset of
partial renders of indoor scenes (a). During the scene-level optimization stage (b)  Φ is altered by
adding learnable feature residuals ∆φ that are summed with the outputs of intermediate layers φ in
order to deﬁne a latent parametrization of test scene views.

(DBIR) [42  3]  this approach is not applicable in our case due to the large distances between camera
centers that cause occlusions between pixels that DBIR cannot resolve. Instead  we make use of a
differentiable point tracer [36] that projects the whole scene point cloud X into each of the test views 
accounting for occlusions in the process (description of the renderer is deferred to the supplementary).
Since the reference views are selected in a sparse manner  the resulting scene point cloud  upon
rendering  generates a mere partial render ¯vi in each of the test cameras ( ˆK i  ˆgi).
The next step aims to inﬁll the missing parts of the partial renders ¯vi. To this end  we train a deep
denoising RGBD autoencoder Φ(¯vi) = ˆvi which accepts ¯vi and returns a prediction of the full
image ˆvi (ﬁg. 3a). Φ comprises a feature pyramid network (FPN) [25] trunk terminated by a 3x3
convolutional ﬁlter with 5 output channels (three RGB channels and additional two for depth and
its conﬁdence) and a bilinear upsampler that resizes the output producing a clean RGBD frame ˆvi
of the same spatial resolution as ¯vi. Training minimizes the inpainting loss from [26] for the RGB
channels and the uncertainty-based error deﬁned as a likelihood of predicted parameters of a Laplace
distribution over the set of output depth values [30]. Additionally  the network contains two more
RGBD prediction branches attached to the output of the 2nd and 3rd upsample&add layer. These
predict two more additional inpainted RGBD frames at a lower resolution  which are later passed
together with the high-resolution output to the RGBD inpainting losses.
In practice  Φ is capable of correcting small defects caused by e.g. rendering irregularly sampled
surfaces. However  it struggles with larger missing areas where semantically consistent structures 
such as pieces of furniture  have to be hallucinated.
Indeed this problem is known to be very
challenging due to its ambiguous nature where different inpaintings provide a reasonable explanation
of the partial render. Instead of sharp predictions  Φ produces an average over possible solutions
manifesting as a blurry RGBD output. Surprisingly  we observed the same behavior for a GAN-based
architecture [45] which has been designed for our scenario with a strongly multimodal output space.
The second main failure mode is an inpainting inconsistent across different views of the same
underlying 3D surface. This is expected since the denoiser Φ is applied independently to each partial
render ¯v. A possible solution is to reason in a common 3D space by applying a 3D ConvNet as in
[20]. Unfortunately  our experiments revealed that the low resolution of the underlying voxel grid
again leads to blurry RGB predictions. The next section describes how we deal with both problems.

3.2 Scene-consistent inpainting

In this paper  we propose to tackle the problem of ambiguity and inconsistency with PerspectiveNet 
a novel approach that jointly reﬁnes the set of test and reference views in order to obtain a globally

4

Partial RGBD renderReference views: Full RGBDDenoising RGBD autoencoder RGBD reconstruction lossInpainted RGBDGround truth RGBDa) RGBD autoencoder trainingb) Scene-level optimizationwTest viewModified denoising RGBD autoencoder ϕ3ϕ2Elementwise sumϕ1Δϕ1Δϕ2Δϕ3⊕⊕⊕Partial RGBD renderPost-processable RGBD¯v̂vvΦvΦ′̂v¯vLearnable feature residuals−∥∥consistent solution that respects the geometric constraints of the scene camera system. Performing
local scene-speciﬁc optimization allows to select one of the possible solutions  resolving the ambiguity
issue. To deal with the scene-consistency conundrum  we leverage the depth predicted by the denoiser
Φ and back-project every pixel into the scene point cloud to derive multi-view consistency constraints
that guide the image inpainting on a global scene level.
In abstract terms  we pose the scene-centric image inpainting task as a minimization problem of an
objective L of the following form:

Ntest(cid:88)

(cid:0)ˆvi(cid:12)(cid:12) V \{ˆvi}(cid:1)

L =

min

(cid:96)cons

{ ˆφ1 ...  ˆφNtest}

i=1

 

ˆvi = Ψ( ˆφi)

(1)

Here  (cid:96)cons(ˆvi|V \ ˆvi) measures how geometrically consistent an inpainted image ˆvi is with the set
V \ ˆvi of the other inpainted / reference views in the camera system of the scene. The optimization
is over the set of latent representations ˆφi of each test view ˆvi = Ψ( ˆφi). Ψ is a mapping between a
latent space and the space of RGBD images. Intuitively  the minimizer of L constitutes a globally
scene-consistent set of RGBD views ˆvi. We now describe two main ingredients of our method: (a)
the parametrization function Ψ of the input images  (b) our choice of consistency losses (cid:96)cons.
Latent image parametrization Since optimizing over raw RGBD values is known to be difﬁcult
and often requires multiple regularizers  so we know we need a sophisticated function Ψ. We
follow [3]  who demonstrated that a deep latent coding of depth images can overcome the need for
complex regularizers. In this work  we opt for a simple solution that leverages the RGBD denoising
autoencoder Φ from section 3.1 above.
We make use of the intermediate feature planes of Φ to create a latent representation φ(ˆv) of each test
view ˆv. Since denoising autoencoders are known to learn generic image representations  optimizing
over φ(ˆv) is likely to produce a tensor lying on the manifold of plausible RGBD images. This
effectively avoids using additional complex regularizers.
More speciﬁcally  after training Φ  we convert it into its modiﬁable version Φ(cid:48)(¯v  ∆φ(¯v)) (illustrated
in ﬁg. 3 (b)). The input to Φ(cid:48) is a partial render ¯v as well as a tuple ∆φ(¯v) = (∆φ1(¯v)  ...  ∆φL(¯v))
of feature residuals ∆φl(¯v) that are element-wise added to each of the L intermediate feature tensors
produced by the feed-forward pass of Φ(¯v). More formally  Φ(cid:48) is deﬁned as:
Φ(cid:48)(¯v  ∆φ(¯v)) = ΦL( ... Φ1(Φ0(¯v) + ∆φ1) ... + ∆φL) 

(2)
where Φl stands for the l-th layer of network Φ. To avoid unnecessary image overparametrization we
add the feature residuals ∆φl only to a preselected subset of feature layers l from the decoding part of
Φ. In this manner  Φ then replaces the latent mapping Ψ in eq. (1)  while the tuple ∆φ(¯vi) corresponds
to the latent image representation ˆφi. Having deﬁned a convenient way of parametrizing images in
our camera system  next we devise constraints that drive our scene-level inpainting optimization.

Reprojection consistency loss Our main constraint ensures that newly generated points in a given
inpainted test view ˆvi are consistent with the projection of the scene point cloud ˆX i formed by
rendering all other views into vi.
More formally  for each test view ˆvi  we form a view-speciﬁc point cloud ˆX i by back-projecting into
the common scene coordinate frame all pixels from the set V \ {ˆvi} consisting of all reference and
test views excluding ˆvi itself. The point cloud ˆX i is then rendered into camera (ˆgi  ˆK i) forming a
contextual render ˇvi. For each test view ˆvi we then deﬁne a multiview inpainting consistency loss
(cid:96)cons(ˆvi) as follows:

(cid:0)ˆvi(cid:12)(cid:12) V \{ˆvi}(cid:1) =

(cid:96)cons

(cid:88)

h(ˆvi

u  ˇvi

u) 

h(a  b) = δ

6(cid:88)

(cid:16)(cid:112)

(cid:17)
1 + δ−1(ac − bc)2 − 1

 

c=1

u∈Ω(ˆvi)∩Ω(ˇvi)

(3)
which is deﬁned over all pixel locations u that have a non-hole status in both ˆvi and ˇvi. h(a  b) is
an accumulation of Pseudo-Huber losses [5] across dimensions c of per-pixel RGBXYZ vectors
a  b ∈ R6. Here  the XYZ component is a backprojection xi
u ∈ R3 of the depth value du into the
3D coordinate frame of camera i. h(a  b) is further accumulated over 6 scales of a Gaussian image
pyramid. We set δ = 1.

5

Style consistency loss The style loss [14] has been shown to facilitate more realistic results for
image generation [6] as well as image inpainting [26] tasks. We adopt the loss in the following form:

(cid:0){ˆvi}Ntest

i=1

(cid:12)(cid:12){vi}Nref

i=1

(cid:1) =

(cid:88)

(cid:96)style

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Nref(cid:88)

i=1

l∈1 2 3

− Ntest(cid:88)

i=1

Ψl(vi)Ψl(vi)T

Nref HlWl

Ψl(ˆvi)Ψl(ˆvi)T

NtestHlWl

 

(4)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1

where Ψl(v) ∈ RD×HlWl denotes a set of features from l-th intermediate layer of an ImageNet
pre-trained VGG16 network [32] reshaped into a D × HlWl matrix after ﬂattening the last two
dimensions of the feature tensor of the original size D × Hl × Wl. As in [27]  we use features
extracted after each of the ﬁrst 3 convolutional layers of VGG16.
Intuitively  the loss pools a style descriptor from reference images and ensures that the newly
inpainted pixels match this distribution. Since the style transfer loss is known to produce ﬁsh-
scale artifacts  following [26]  we use it in conjunction with a total variation regularizer (cid:96)TV =
(u1 u2) is the RGB

(u1 u2)| + |ci
NtestW H
value of a pixel at position (u1  u2) in a test view ˆvi.

(u1 u2)|  where ci

(u1 u2+1) − ci
|ci

(u1+1 u2) − ci

(cid:80)Ntest

(cid:80)

u1 u2

i=1

1

Scene-level optimization Having deﬁned the main constraints and image parametrization  we can
now write the objective L of the PerspectiveNet scene-level optimization:

L = arg min
{∆φ(¯vi)}Ntest

i=1

(cid:96)style

(cid:0){ˆvi}Ntest

i=1

(cid:12)(cid:12){vi}Nref

i=1

(cid:1) +

(cid:0)ˆvi(cid:12)(cid:12) V \{ˆvi}(cid:1)

(cid:96)cons

Ntest(cid:88)

i=1

(5)
For a given scene  L is minimized with an Adam [21] optimizer for 50 iterations with an initial
learning rate of 0.01 decaying 10-fold after 35 iterations.

ˆvi = Φ(cid:48)(¯vi  ∆φ(¯vi))

PerspectiveNet in a nutshell Our algorithm thus works as follows. For each testing scene  the set
of reference views {vi} is rendered into the target cameras {ˆgj} producing partial renders {¯vj}. The
ensuing scene-consistent optimization process  that minimizes L (eq. 5)  then ﬁnds the optimal set of
latent image representations {∆φ(¯vi)} that  after being passed with {¯vj} to the modiﬁable denoising
autoencoder Φ(cid:48)  leads to the ﬁnal set of scene-consistent new views {ˆvj = Φ(cid:48)(¯vj  ∆φ(¯vj))}.

3.3 Technical details
Additional regularizers While (cid:96)cons ensures that newly generated pixels stay consistent with all
views in the scene  in principle  there is nothing stopping the the global optimization process from
producing a set of consistent inpaintings that get “detached” from the reference views. We thus
add two regularization terms that prevent the solution from diverging too far from the ground truth
provided by the reference frames:

Ntest(cid:88)

(cid:20) (cid:88)

(cid:88)

(cid:21)

(cid:96)R =

h(¯vi

u  ˆvi

u) +

h(ˆvi

u t=0  ˆvi
u)

 

(6)

i=1

u∈Ω(¯vi
u)

u∈Ω(ˆvi
u)

u close
u  and the second term prevents the result of
u t=0 obtained by Φ at the

where the ﬁrst term of the main sum brings the non-holes of the partial ground truth render ¯vi
to the corresponding pixels in the inpainted image ˆvi
the optimization ˆvi from grossly differing from the initial inpainting ˆvi
beginning of the global optimization.
Training the RGBD denoiser Φ In order to train Φ  we collect a dataset of image pairs {(¯vi  vi)}
generated by randomly sampling 8 reference views from the training scenes of a considered dataset of
indoor scenes and rendering those using our point tracer into a random test view  for which the ground
truth RGBD frame vi is known. We further ﬁlter out the pairs where less than 50% of the input pixels
are deﬁned. The RGBD autoencoder Φ is trained with an initial learning rate 10−5 decaying 10-fold
once the loss plateaus. Where possible  the convolutional layers were initialized with weights of an
ImageNet pre-trained ResNet-50 network. Batch size was set to 4 and training on a single GPU took
approximately 7 days. For each of the 2 datasets considered in this paper (ScanNet [8]  SceneNet
[28])  we train a separate autoencoder.

6

Method
PerspectiveNet
PerspectiveNet w/o opt
PartialConv [26]
3DConvNet
BiGAN [45]

Method
PerspectiveNet
PerspectiveNet w/o opt
PartialConv [26]
3DConvNet
BiGAN [45]

↓
(cid:96)RGB
1
68.022
66.511
93.604
78.590
77.313

↓
(cid:96)RGB
1
49.698
48.521
76.470
75.942
55.815

(a) ScanNet

Color metrics

PSNR ↑
13.762
13.986
11.374
12.190
12.742

LPIPS ↓
0.422
0.426
0.461
0.531
0.523

(b) SceneNet

PSNR ↑
15.687
16.324
12.377
12.614
15.112

LPIPS ↓
0.424
0.442
0.481
0.570
0.485

1 [m] ↓
(cid:96)D
0.115
0.120
0.750
0.138
0.215

1 [m] ↓
(cid:96)D
0.219
0.227
1.846
0.653
0.249

Depth metrics
δ2 ↑
0.411
0.230
0.236
0.359
0.212

δ1 ↑
0.352
0.188
0.194
0.301
0.169

δ1 ↑
0.366
0.101
0.008
0.040
0.319

δ2 ↑
0.431
0.125
0.010
0.050
0.375

δ3 ↑
0.471
0.279
0.283
0.426
0.265

δ3 ↑
0.494
0.155
0.013
0.062
0.431

Table 1: Quantitative evaluation of depth and image generation on the test sets of ScanNet (a)
and SceneNet (b) comparing our method with 2D and 3D inpainting baselines.

4 Experiments

Datasets and evaluation protocol We chose 2 datasets for evaluation: ScanNet [8] and SceneNet
[28]. ScanNet currently comprises one of the largest 3D datasets of real indoor scenes with 1500
training and 100 test scenes. Contrasted to the realistic ScanNet  SceneNet is a dataset of 33k/1k
synthetic train/test scenes. SceneNet was chosen in order to benchmark the performance in a clean
setting free of challenging factors such as lighting changes or inaccurate camera extrinsics.
Each dataset contains RGBD views of indoor scenes annotated with camera extrinsic and intrinsic
parameters  allowing for evaluation of the new view synthesis. In order to benchmark a method on
a given test scene  we sample 4 reference views  for which we assume knowledge of their RGBD
as well as camera parameters  and at most 8 reference views for which only the camera parameters
are given. For the test views  we then generate the color and depth channels and compare them to
the corresponding ground truth frames. In order to obtain good coverage of the scene contents  the
reference views were selected by clustering the camera pose descriptors (consisting of a concatenation
of the vectorized camera rotation matrix and the camera translation vector) into four clusters and
picking the typical point from each cluster as a reference camera. The test views were chosen in a
similar fashion by clustering the parameters of the remaining cameras and picking the views that
contain at least 50%/40% deﬁned pixels for ScanNet and SceneNet respectively after rendering the
contents of the reference views. For each dataset  we ﬁrst train all methods on the frames coming
from its training scenes. For ScanNet  the evaluation is conducted on all 100 test scenes  and for
SceneNet we randomly sampled 100 scenes from the test set for evaluation. We produce images of
width/height 320/240 pixels and compare with the ground truth images at the resolution of 640/480.
Following standard practice [45  38  26  40  18]  we quantitatively evaluate generated images by
reporting the per-pixel (cid:96)1 error ((cid:96)RGB
) and the peak-signal-to-noise-ratio (PSNR). Since (cid:96)1 loss and
PSNR are known to be overly sensitive to errors in low-level image details while being insensitive to
more abstract semantic visual structures  we also evaluate the perceptive error LPIPS [41]  which is a
calibrated version of a distance between images in a feature space of a pre-trained image classiﬁcation
network (VGG16 in our case). In order to evaluate the generated depth maps  following [24  11] 
1 measured in meters) and metrics δi for i = {1  2  3}
we report per-pixel absolute depth error ((cid:96)D
which measure the portion of test pixels that have their absolute depth error lower than a threshold
ti = 1.25i cm.

1

Inpainting baselines Evaluation focuses mainly on inpainting baselines that consist of rendering
the reference views into the target ones  followed by ﬁlling the holes with an algorithm. The baseline
abbreviated as PartialConv uses a state-of-the-art inpainting architecture from [26] trained on the
same dataset as our RGBD denoiser. We also compare with BiGAN [45] trained on the same
dataset. Finally  PerspectiveNet w/o opt is an ablation of our method and comprises the initial

7

GT

Partial render

BiGAN

PartialConv

3DConvNet PerspectiveNet

t
e
N
n
a
c
S
)
a
(

t
e
N
e
n
e
c
S
)
b
(

Figure 4: Qualitative evaluation of new RGBD view synthesis on the ScanNet (a) and SceneNet
(b) datasets comparing our method (PerspectiveNet) to inpainting with partial convolutions (Partial-
Conv [26]) or Bicycle GAN (BiGAN [45])  and a sparse 3DConvNet that inpaints voxels directly
in 3D. The ﬁrst column of each row denotes the ground truth for a given test view while the second
shows a partial render ¯v of the reference views into the camera of the ground truth view. For each of
the 6 displayed test cases  we show the RGB (upper row) and depth prediction (lower row).

8

inpainting produced by the RGBD denoiser from section 3.1 without any iterations performed by the
scene-consistent optimizer.

3D inpainting Apart from the inpainting methods  we further compare with an approach that
operates on 3D voxel grids (3DConvNet). Since our application requires a voxel grid of a very high
resolution and spatial extent  due to high memory requirements of the classic dense 3D ConvNet
architectures  we implemented a sparse U-Net convolutional network [15]. Detailed explanation of
the architecture is included in the supplementary material.
Table 1 contains quantitative results on the ScanNet and SceneNet datasets. A qualitative comparison
is present in ﬁg. 4. Additional qualitative results are present in the supplementary material.

1

1

Discussion of results Table 1 reveals that our method outperforms the considered baselines on all
depth metrics. For the color metrics (cid:96)RGB
and PSNR  we are on par with the ablation “PerspectiveNet
w/o opt”. However  we outperform it on the more semantically meaningful LPIPS metric. Intuitively 
since PSNR and (cid:96)RGB
are sensitive to low-frequency image details and LPIPS better assesses image
realism  the relative differences in the color metrics between PerspectiveNet and “PerspectiveNet
w/o opt” signify that  while the local color distributions are roughly correct in both cases  adding the
scene-consistent optimizer brings better image realism.
Qualitatively  compared to our approach  the inpainting baseline PartialConv generates more blurry
results due to a suboptimal loss function that does not take into account the ambiguity in the output.
Furthermore  PartialConv records low performance of depth inpainting. Our method also outperforms
BiGAN. For BiGAN  we observed that changes in the latent code z mostly lead to global change
of the color statistics of the output image  rather than altering the geometry of the inpainted scene.
3DConvNet records a competitive depth prediction accuracy  but lags behind in color prediction. This
is most likely due to the reconstructions being optimized to match the partial point clouds in 3D 
without considering the need for perceptual realism when rendering the voxels into the 2D test views.

5 Conclusion

In this work  we tackled a previously seldom explored problem of new-view synthesis in real indoor
environments. A novel approach  termed PerspectiveNet  based on the render-inpaint paradigm is
proposed. The main technical contribution is a bundle-adjustment technique that jointly optimizes all
views in a given room in order to obtain a set of new views that is globally scene-consistent in terms
of geometry and style. Evaluation on two large datasets of indoor scenes [8  28] reveals performance
superior to several strong baselines.

References
[1] Coloma Ballester  Marcelo Bertalmio  Vicent Caselles  Guillermo Sapiro  and Joan Verdera. Filling-in by
joint interpolation of vector ﬁelds and gray levels. IEEE Transactions on Image Processing  10(8):1200–
1211  Aug 2001.

[2] Marcelo Bertalmio  Guillermo Sapiro  Vincent Caselles  and Coloma Ballester. Image inpainting. In
Proceedings of the 27th annual conference on Computer graphics and interactive techniques  pages
417–424. ACM Press/Addison-Wesley Publishing Co.  2000.

[3] Michael Bloesch  Jan Czarnowski  Ronald Clark  Stefan Leutenegger  and Andrew J Davison.
CodeSLAM—learning a compact  optimisable representation for dense visual SLAM. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  pages 2560–2568  2018.

[4] Angel X. Chang  Thomas Funkhouser  Leonidas Guibas  Pat Hanrahan  Qixing Huang  Zimo Li  Silvio
Savarese  Manolis Savva  Shuran Song  Hao Su  Jianxiong Xiao  Li Yi  and Fisher Yu. ShapeNet: An
Information-Rich 3D Model Repository. Technical report  Stanford University — Princeton University —
Toyota Technological Institute at Chicago  2015.

[5] Pierre Charbonnier  Laure Blanc-Féraud  Gilles Aubert  and Michel Barlaud. Deterministic edge-preserving

regularization in computed imaging. IEEE Transactions on image processing  6(2):298–311  1997.

[6] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded reﬁnement networks. In

Proceedings of the IEEE International Conference on Computer Vision  pages 1511–1520  2017.

9

[7] Tao Chen  Zhe Zhu  Ariel Shamir  Shi-Min Hu  and Daniel Cohen-Or. 3-sweep: Extracting editable objects

from a single photo. ACM Trans. Graph.  32(6)  November 2013.

[8] Angela Dai  Angel X. Chang  Manolis Savva  Maciej Halber  Thomas Funkhouser  and Matthias Nießner.
Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern
Recognition (CVPR)  IEEE  2017.

[9] Alexey Dosovitskiy  Jost Tobias Springenberg  and Thomas Brox. Learning to generate chairs with
convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 1538–1546  2015.

[10] Alexei A Efros and William T Freeman. Image quilting for texture synthesis and transfer. In Proceedings
of the 28th annual conference on Computer graphics and interactive techniques  pages 341–346. ACM 
2001.

[11] David Eigen and Rob Fergus. Predicting depth  surface normals and semantic labels with a common
multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer
vision  pages 2650–2658  2015.

[12] SM Ali Eslami  Danilo Jimenez Rezende  Frederic Besse  Fabio Viola  Ari S Morcos  Marta Garnelo 
Avraham Ruderman  Andrei A Rusu  Ivo Danihelka  Karol Gregor  et al. Neural scene representation and
rendering. Science  360(6394):1204–1210  2018.

[13] John Flynn  Ivan Neulander  James Philbin  and Noah Snavely. Deepstereo: Learning to predict new
views from the world’s imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 5515–5524  2016.

[14] Leon A Gatys  Alexander S Ecker  and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint

arXiv:1508.06576  2015.

[15] Benjamin Graham  Martin Engelcke  and Laurens van der Maaten. 3d semantic segmentation with
submanifold sparse convolutional networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 9224–9232  2018.

[16] Geoffrey E Hinton  Alex Krizhevsky  and Sida D Wang. Transforming auto-encoders. In International

Conference on Artiﬁcial Neural Networks  pages 44–51. Springer  2011.

[17] Youichi Horry  Ken-Ichi Anjyo  and Kiyoshi Arai. Tour into the picture: Using a spidery mesh interface to
make animation from a single image. In Proceedings of the 24th Annual Conference on Computer Graphics
and Interactive Techniques  SIGGRAPH ’97  pages 225–232. ACM Press/Addison-Wesley Publishing Co. 
1997.

[18] Phillip Isola  Jun-Yan Zhu  Tinghui Zhou  and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In Computer Vision and Pattern Recognition (CVPR)  2017 IEEE Conference on 
2017.

[19] Dinghuang Ji  Junghyun Kwon  Max McFarland  and Silvio Savarese. Deep view morphing. In Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition  pages 2155–2163  2017.

[20] Abhishek Kar  Christian Häne  and Jitendra Malik. Learning a multi-view stereo machine. In Advances in

neural information processing systems  pages 365–376  2017.

[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[22] Tejas D Kulkarni  William F Whitney  Pushmeet Kohli  and Josh Tenenbaum. Deep convolutional inverse

graphics network. In Advances in neural information processing systems  pages 2539–2547  2015.

[23] Vivek Kwatra  Irfan Essa  Aaron Bobick  and Nipun Kwatra. Texture optimization for example-based

synthesis. 24(3):795–802  2005.

[24] Iro Laina  Christian Rupprecht  Vasileios Belagiannis  Federico Tombari  and Nassir Navab. Deeper depth
prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D
vision (3DV)  pages 239–248. IEEE  2016.

[25] Tsung-Yi Lin  Piotr Dollár  Ross Girshick  Kaiming He  Bharath Hariharan  and Serge Belongie. Feature
pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition  pages 2117–2125  2017.

10

[26] Guilin Liu  Fitsum A. Reda  Kevin J. Shih  Ting-Chun Wang  Andrew Tao  and Bryan Catanzaro. Image
inpainting for irregular holes using partial convolutions. In Proceedings of the European Conference on
Computer Vision (ECCV)  pages 85–100  2018.

[27] Guilin Liu  Kevin J. Shih  Ting-Chun Wang  Fitsum A. Reda  Karan Sapra  Zhiding Yu  Andrew Tao  and

Bryan Catanzaro. Partial convolution based padding. 2018.

[28] John McCormac  Ankur Handa  Stefan Leutenegger  and Andrew J.Davison. Scenenet RGB-D: Can 5M

synthetic images beat generic ImageNet pre-training on indoor segmentation? 2017.

[29] Moustafa Meshry  Dan B Goldman  Sameh Khamis  Hugues Hoppe  Rohit Pandey  Noah Snavely  and
Ricardo Martin-Brualla. Neural rerendering in the wild. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 6878–6887  2019.

[30] David Novotny  Diane Larlus  and Andrea Vedaldi. Capturing the geometry of object categories from

video supervision. IEEE transactions on pattern analysis and machine intelligence  2018.

[31] Eunbyung Park  Jimei Yang  Ersin Yumer  Duygu Ceylan  and Alexander C Berg. Transformation-grounded
image generation network for novel 3d view synthesis. In Proceedings of the IEEE conference on computer
vision and pattern recognition  pages 3500–3509  2017.

[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. ICLR  2014.

[33] Vincent Sitzmann  Justus Thies  Felix Heide  Matthias Nießner  Gordon Wetzstein  and Michael Zollhöfer.

Deepvoxels: Learning persistent 3d feature embeddings. CoRR  abs/1812.01024  2018.

[34] Maxim Tatarchenko  Alexey Dosovitskiy  and Thomas Brox. Multi-view 3d models from single images
with a convolutional network. In European Conference on Computer Vision  pages 322–337. Springer 
2016.

[35] Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics

tools  9(1):23–34  2004.

[36] Shubham Tulsiani  Richard Tucker  and Noah Snavely. Layer-structured 3d scene inference via view

synthesis. In ECCV  2018.

[37] Dmitry Ulyanov  Andrea Vedaldi  and Victor Lempitsky. Deep image prior. 2017.

[38] Ting-Chun Wang  Ming-Yu Liu  Jun-Yan Zhu  Andrew Tao  Jan Kautz  and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional GANs. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pages 8798–8807  2018.

[39] Jimei Yang  Scott E Reed  Ming-Hsuan Yang  and Honglak Lee. Weakly-supervised disentangling with
recurrent transformations for 3d view synthesis. In Advances in Neural Information Processing Systems 
pages 1099–1107  2015.

[40] Jiahui Yu  Zhe Lin  Jimei Yang  Xiaohui Shen  Xin Lu  and Thomas S Huang. Generative image inpainting
In Proceedings of the IEEE Conference on Computer Vision and Pattern

with contextual attention.
Recognition  pages 5505–5514  2018.

[41] Richard Zhang  Phillip Isola  Alexei A Efros  Eli Shechtman  and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 586–595  2018.

[42] Tinghui Zhou  Matthew Brown  Noah Snavely  and David G Lowe. Unsupervised learning of depth
and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 1851–1858  2017.

[43] Tinghui Zhou  Shubham Tulsiani  Weilun Sun  Jitendra Malik  and Alexei A Efros. View synthesis by

appearance ﬂow. In European conference on computer vision  pages 286–301. Springer  2016.

[44] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In Computer Vision (ICCV)  2017 IEEE International Conference
on  2017.

[45] Jun-Yan Zhu  Richard Zhang  Deepak Pathak  Trevor Darrell  Alexei A Efros  Oliver Wang  and Eli
Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing
Systems  pages 465–476  2017.

11

[46] Zhenyao Zhu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Multi-view perceptron: a deep model for
learning face identity and view representations. In Advances in Neural Information Processing Systems 
pages 217–225  2014.

12

,David Novotny
Ben Graham
Jeremy Reizenstein