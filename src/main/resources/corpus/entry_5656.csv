2012,Trajectory-Based Short-Sighted Probabilistic Planning,Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment  and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem  planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed  e.g.  consider all paths at once  perform determinization of actions  and sampling. In this paper  we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs)  a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [ref] by proving that SSiPP always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted SSPs.  We empirically compare SSiPP using trajectory-based short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60  a problem in which the optimal solution contains approximately $10^{70}$ states.,Trajectory-Based Short-Sighted Probabilistic

Planning

Felipe W. Trevizan

Machine Learning Department

Manuela M. Veloso

Computer Science Department

Carnegie Mellon University - Pittsburgh  PA

{fwt mmv}@cs.cmu.edu

Abstract

Probabilistic planning captures the uncertainty of plan execution by probabilisti-
cally modeling the effects of actions in the environment  and therefore the proba-
bility of reaching different states from a given state and action. In order to compute
a solution for a probabilistic planning problem  planners need to manage the un-
certainty associated with the different paths from the initial state to a goal state.
Several approaches to manage uncertainty were proposed  e.g.  consider all paths
at once  perform determinization of actions  and sampling. In this paper  we in-
troduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs) 
a novel approach to manage uncertainty for probabilistic planning problems in
which states reachable with low probability are substituted by artiﬁcial goals that
heuristically estimate their cost to reach a goal state. We also extend the theoretical
results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP
always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the
structure of short-sighted SSPs. We empirically compare SSiPP using trajectory-
based short-sighted SSPs with the winners of the previous probabilistic planning
competitions and other state-of-the-art planners in the triangle tireworld problems.
Trajectory-based SSiPP outperforms all the competitors and is the only planner
able to scale up to problem number 60  a problem in which the optimal solution
contains approximately 1070 states.

1

Introduction

The uncertainty of plan execution can be modeled by using probabilistic effects in actions  and
therefore the probability of reaching different states from a given state and action. This search space 
deﬁned by the probabilistic paths from the initial state to a goal state  challenges the scalability of
planners. Planners manage the uncertainty by choosing a search strategy to explore the space. In
this work  we present a novel approach to manage uncertainty for probabilistic planning problems
that improves its scalability while still being optimal.

One approach to manage uncertainty while searching for the solution of probabilistic planning prob-
lems is to consider the complete search space at once. Examples of such algorithms are value
iteration and policy iteration [2]. Planners based on these algorithms return a closed policy  i.e.  a
universal mapping function from every state to the optimal action that leads to a goal state. Assum-
ing the model correctly captures the cost and uncertainty of the actions in the environment  closed
policies are extremely powerful as their execution never “fails ” and the planner does not need to
be re-invoked ever. Unfortunately the computation of such policies is prohibitive in complexity as
problems scale up. Value iteration based probabilistic planners can be improved by combining asyn-
chronous updates and heuristic search [3–7]. Although these techniques allow planners to compute
compact policies  in the worst case  these policies are still linear in the size of the state space  which
itself can be exponential in the size of the state or goals.

1

Another approach to manage uncertainty is basically to ignore uncertainty during planning  i.e.  to
approximate the probabilistic actions as deterministic actions. Examples of replanners based on
determinization are FF-Replan [8]  the winner of the ﬁrst International Probabilistic Planning Com-
petition (IPPC) [9]  Robust FF [10]  the winner of the third IPPC [11] and FF-Hindsight [12  13].
Despite the major success of determinization  this simpliﬁcation in the action space results in algo-
rithms oblivious to probabilities and dead-ends  leading to poor performance in speciﬁc problems 
e.g.  the triangle tireworld [14].

Besides the action space simpliﬁcation  uncertainty management can be performed by simplifying
the problem horizon  i.e.  look-ahead search [15]. Based on sampling  the Upper Conﬁdence bound
for Trees (UCT) algorithm [16] approximates the look-ahead search by focusing the search in the
most promising nodes.

The state space can also be simpliﬁed to manage uncertainty in probabilistic planning. One example
of such approach is Envelope Propagation (EP) [17]. EP computes an initial partial policy π and
then prunes all the states not considered by π. The pruned states are represented by a special meta
state. Then EP iteratively improves its approximation of the state space. Previously  we introduced
short-sighted planning [1]  a new approach to manage uncertainty in planning problems: given a
state s  only the uncertainty structure of the problem in the neighborhood of s is taken into account
and the remaining states are approximated by artiﬁcial goals that heuristically estimate their cost to
reach a goal state.

In this paper  we introduced trajectory-based short-sighted Stochastic Shortest Path Problems
(SSPs)  a novel model to manage uncertainty in probabilistic planning problems. Trajectory-based
short-sighted SSPs manage uncertainty by pruning the state space based on the most likely trajectory
between states and deﬁning artiﬁcial goal states that guide the solution towards the original goal. We
also contribute by deﬁning a class of short-sighted models and proving that the Short-Sighted Proba-
bilistic Planner (SSiPP) [1] always terminates and is asymptotically optimal for models in this class
of short-sighted models.

The remainder of this paper is organized as follows: Section 2 introduces the basic concepts and
notation. Section 3 deﬁnes formally trajectory-based short-sighted SSPs. Section 4 presents our
new theoretical results for SSiPP. Section 5 empirically evaluates SSiPP using trajectory-based short-
sighted SSPs with the winners of the previous IPPCs and other state-of-the-art planner. Section 6
concludes the paper.

2 Background

A Stochastic Shortest Path Problem (SSP) is deﬁned by the tuple S = hS  s0  G  A  P  Ci  in which
[1  18]: S is the ﬁnite set of state; s0 ∈ S is the initial state; G ⊆ S is the set of goal states; A is
the ﬁnite set of actions; P (s′|s  a) represents the probability that s′ ∈ S is reached after applying
action a ∈ A in state s ∈ S; C(s  a  s′) ∈ (0  +∞) is the cost incurred when state s′ is reached after
applying action a in state s and this function is required to be deﬁned for all s ∈ S  a ∈ A  s′ ∈ S
such that P (s′|s  a) > 0.
A solution to an SSP is a policy π  i.e.  a mapping from S to A. If π is deﬁned over the entire space
S  then π is a closed policy. A policy π deﬁned only for the states reachable from s0 when following
π is a closed policy w.r.t. s0 and S(π  s0) denotes this set of reachable states. For instance  in the
SSP depicted in Figure 1(a)  the policy π0 = {(s0  a0)  (s′
3  a0)} is a closed policy
w.r.t. s0 and S(π0  s0) = {s0  s′
Given a policy π  we deﬁne trajectory as a sequence Tπ = hs(0)  . . .   s(k)i such that  for all
i ∈ {0  · · ·   k − 1}  π(s(i)) is deﬁned and P (s(i+1)|s(i)  π(s(i))) > 0. The probability of a tra-
i=0 P (s(i+1)|s(i)  π(s(i))) and maximum probability of a

trajectory between two states Pmax(s  s′) is deﬁned as maxπ P (Tπ = hs  . . .   s′i).
An optimal policy π∗ for an SSP is any policy that always reaches a goal state when followed from
s0 and also minimizes the expected cost of Tπ∗. For a given SSP  π∗ might not be unique  however
the optimal value function V ∗  i.e.  the mapping from states to the minimum expected cost to reach
a goal state  is unique. V ∗ is the ﬁxed point of the set of equations deﬁned by (1) for all s ∈ S \ G
and V ∗(s) = 0 for all s ∈ G. Notice that under the optimality criterion given by (1)  SSPs are

jectory Tπ is deﬁned as P (Tπ) = Qi<|Tπ|

1  a0)  (s′

2  a0)  (s′

1  s′

2  s′

3  sG}.

2

a0
a1

.6

s 0

.4

.75

.25

t>1

t>2

t>3

t>4

p<1.0

p<0.4

p<0.4

2

.6

.6

s 1

.4

.75

’
s 1

s 2

’
s 2

.4

.75

s 3

’
s 3

.25

.25

.6

.4

.75

.25

.6

sG

s 0

.4

.75

25

.6

.6

s 1

.4

s 2

.4

’
s 1

.75

.25

.75

’
s 2

.25

s 3

’
s 3

.6

.4

.75

.25

sG

.6

s 0

25

.4

.75

s 1

’
s 1

.6

.4

.6

s 2

.4

.75

.25

.75

’
s 2

.25

s 3

’
s 3

.6

.4

.75

.25

sG

p<0.753
Figure 1: (a) Example of an SSP. The initial state is s0  the goal state is sG  C(s  a  s′) = 1  ∀s ∈ S 
a ∈ A and s′ ∈ S. (b) State-space partition of (a) according to the depth-based short-sighted SSPs:
Gs0 t contains all the states in dotted regions which their conditions hold for the given value of t. (c)
State-space partition of (a) according to the trajectory-based short-sighted SSPs: Gs0 ρ contains all
the states in dotted regions which their conditions hold for the given value of ρ.

p<0.75

p<0.75

2

more general than Markov Decision Processes (MDPs) [19]  therefore all the work presented here
is directly applicable to MDPs.

V ∗(s) = min

a∈A Xs′∈ShC(s  a  s′) + P (s′|s  a)V ∗(s′)i

(1)

Deﬁnition 1 (reachability assumption). An SSP satisﬁes the reachability assumption if  for all s ∈ S 
there exists sG ∈ G such that Pmax(s  sG) > 0.

Given an SSP S  if a goal state can be reached with positive probability from every state s ∈ S 
then the reachability assumption (Deﬁnition 1) holds for S and 0 ≤ V ∗(s) < ∞ [19]. Once V ∗ is
known  any optimal policy π∗ can be extracted from V ∗ by substituting the operator min by argmin
in equation (1).

A possible approach to compute V ∗ is the value iteration algorithm: deﬁne V i+1(s) as in (1)
with V i in the right hand side instead of V ∗ and the sequence hV 0  V 1  . . .   V ki converges to
V ∗ as k → ∞ [19]. The process of computing V i+1 from V i is known as Bellman up-
date and V 0(s) can be initialized with an admissible heuristic H(s)  i.e.  a lower bound for
In practice we are interested in reaching ǫ-convergence  that is  given ǫ  ﬁnd V such that
V ∗.

maxs |V (s) − minaPs′ [C(s  a  s′) + P (s′|s  a)V (s′)]| ≤ ǫ. The following well-known result is

necessary in most of our proofs [2  Assumption 2.2 and Lemma 2.1]:
Theorem 1. Given an SSP S  if the reachability assumption holds for S  then the admissibility and
monotonicity of V are preserved through the Bellman updates.

3 Trajectory-Based Short-Sighted Stochastic SSPs

Short-sighted Stochastic Path Problems (short-sighted SSPs) [1] are a special case of SSPs in which
the original problem is transformed into a smaller one by: (i) pruning the state space; and (ii) adding
artiﬁcial goal states to heuristically guide the search towards the goals of the original problem.
Depth-based short-sighted SSPs are deﬁned based on the action-distance between states [1]:
Deﬁnition 2 (action-distance). The non-symmetric action-distance δ(s  s′) between two states s and
s′ is argmink{Tπ = hs  s(1)  . . .   s(k−1)  s′i|∃π and Tπ is a trajectory}.
Deﬁnition 3 (Depth-Based Short-Sighted SSP). Given an SSP S = hS  s0  G  A  P  Ci  a state s ∈
S  t > 0 and a heuristic H  the (s  t)-depth-based short-sighted SSP Ss t = hSs t  s  Gs t  A  P  Cs ti
associated with S is deﬁned as:

• Ss t = {s′ ∈ S|δ(s  s′) ≤ t};
• Gs t = {s′ ∈ S|δ(s  s′) = t} ∪ (G ∩ Ss t);

• Cs t(s′  a  s′′) = (cid:26)C(s′  a  s′′) + H(s′′)

C(s′  a  s′′)

if s′′ ∈ Gs t
otherwise

 

∀s′ ∈ Ss t  a ∈ A  s′′ ∈ Ss t

Figure 1(b) shows  for different values of t  Ss0 t for the SSP in Figure 1(a); for instance  if t = 2
then Ss0 2 = {s0  s1  s′
2}. In the example shown in Figure 1(b)  we can

2} and Gs0 2 = {s2  s′

1  s2  s′

3

see that generation of Ss0 t is independent of the trajectories probabilities: for t = 2  s2 ∈ Ss0 2 and
3 6∈ Ss0 2  however Pmax(s0  s2) = 0.16 < Pmax(s0  s′
s′

3) = 0.753 ≈ 0.42.

Deﬁnition 4 (Trajectory-Based Short-Sighted SSP). Given an SSP S = hS  s0  G  A  P  Ci  a
state s ∈ S  ρ ∈ [0  1] and a heuristic H  the (s  ρ)-trajectory-based short-sighted SSP Ss ρ =
hSs ρ  s  Gs ρ  A  P  Cs ρi associated with S is deﬁned as:

• Ss ρ = {s′ ∈ S|∃ˆs ∈ S and a ∈ A s.t. Pmax(s  ˆs) ≥ ρ and P (s′|ˆs  a) > 0};

• Gs ρ = (G ∩ Ss ρ) ∪ (Ss ρ ∩ {s′ ∈ S|Pmax(s  s′) < ρ});

• Cs ρ(s′  a  s′′) = (cid:26)C(s′  a  s′′) + H(s′′)

C(s′  a  s′′)

if s′′ ∈ Gs ρ
otherwise

 

∀s′ ∈ Ss ρ  a ∈ A  s′′ ∈ Ss ρ

For simplicity  when H is not clear by context nor explicit  then H(s) = 0 for all s ∈ S.

Our novel model  the trajectory-based short-sighted SSPs (Deﬁnition 4)  addresses the issue of
states with low trajectory probability by explicitly deﬁning its state space Ss ρ based on maxi-
mum probability of a trajectory between s and the candidate states s′ (Pmax(s  s′)). Figure 1(c)
shows  for all values of ρ ∈ [0  1]  the trajectory-based Ss0 ρ for the SSP in Figure 1(a): for in-
3  sG} and Gs0 0.75 = {s1  sG}. This example
stance  if ρ = 0.753 then Ss0 0.753 = {s0  s1  s′
shows how trajectory-based short-sighted SSP can manage uncertainty efﬁciently: for ρ = 0.753 
|Ss0 ρ| = 6 and the goal of the original SSP sG is already included in Ss0 ρ while  for the depth-
based short-sighted SSPs  sG ∈ Ss0 t only for t ≥ 4 case in which |Ss0 t| = |S| = 8.
Notice that the deﬁnition of Ss ρ cannot be simpliﬁed to {ˆs ∈ S|Pmax(s  ˆs) ≥ ρ} since not all
the resulting states of actions would be included in Ss ρ. For example  consider S = {s  s′  s′′} 
P (s′|s  a) = 0.9 and P (s′′|s  a) = 0.1  then for ρ ∈ (0.1  1]  {ˆs ∈ S|Pmax(s  ˆs) ≥ ρ} = {s  s′} 
generating an invalid SSP since not all the resulting states of a would be contained in the model.

1  s′

2  s′

4 Short-Sighted Probabilistic Planner

The Short-Sighted Probabilistic Planner (SSiPP) is an algorithm that solves SSPs based on short-
sighted SSPs [1]. SSiPP is reviewed in Algorithm 1 and consists of iteratively generating and solving
short-sighted SSPs of the given SSP. Due to the reduced size of the short-sighted problems  SSiPP
solves each of them by computing a closed policy w.r.t.
their initial state. Therefore  we obtain
a “fail-proof” solution for each short-sighted SSP  thus if this solution is directly executed in the
environment  then replanning is not needed until an artiﬁcial goal is reached. Alternatively  an
anytime behavior is obtained if the execution of the computed closed policy for the short-sighted
SSP is simulated (Algorithm 1 line 4) until an artiﬁcial goal sa is reached and this procedure is
repeated  starting sa  until convergence or an interruption.
In [1]  we proved that SSiPP always terminates and is asymptotically optimal for depth-based short-
sighted SSPs. We generalize the results regarding SSiPP by: (i) providing the sufﬁcient conditions
for the generation of short-sighted problems (Algorithm 1  line 1) in Deﬁnition 5; and (ii) proving
that SSiPP always terminates (Theorem 3) and is asymptotically optimal (Corollary 4) when the
short-sighted SSP generator respects Deﬁnition 5. Notice that  by deﬁnition  both depth-based and
trajectory-based short-sighted SSPs meet the sufﬁcient conditions presented on Deﬁnition 5.

Deﬁnition 5. Given an SSP hS  s0  G  A  P  Ci  the sufﬁcient conditions on the short-sighted SSPs
hS′  ˆs  G′  A  P ′  C ′i returned by the generator in Algorithm 1 line 1 are:

1. G ∩ S′ ⊆ G′;
2. ˆs 6∈ G → ˆs 6∈ G′; and

3. for all s ∈ S  a ∈ A and s′ ∈ S′ \ G′  if P (s|s′  a) > 0 then s ∈ S′ and P ′(s|s′  a) =

P (s|s′  a).

Lemma 2. SSiPP performs Bellman updates on the original SSP S.

4

1

2

3
4

SSIPP(SSP S = hS  s0  G  A  P  Ci  H a heuristic for V ∗ and params the parameters to generate
short-sighted SSPs)
begin

V ← Value function for S initialized by H
s ← s0
while s 6∈ G do

hS′  s  G′  A  P  C ′i ← GENERATE-SHORT-SIGHTED-SSP(S  s  V   params)
(ˆπ∗  ˆV ∗) ← OPTIMAL-SSP-SOLVER(hS′  s  G′  A  P  C ′i  V )
forall s′ ∈ S′(ˆπ∗  s) do

V (s′) ← ˆV ∗(s′)

while s 6∈ G′ do

s ← execute-action(ˆπ∗(s))

return V

end
Algorithm 1: SSiPP algorithm [1]. GENERATE-SHORT-SIGHTED-SSP represents a procedure to
generate short-sighted SSPs  either depth-based or trajectory-based. In the former case params = t
and params = ρ for the latter. OPTIMAL-SSP-SOLVER returns an optimal policy π∗ w.r.t. s0 for S
and V ∗ associated to π∗  i.e.  V ∗ needs to be deﬁned only for s ∈ S(π∗  s0).

Proof. In order to show that SSiPP performs Bellman updates implicitly  consider the loop
Since OPTIMAL-SOLVER computes ˆV ∗  by deﬁnition of short-
in line 2 of Algorithm 1.
sighted SSP: (i) ˆV ∗(sG) equals V (sG) for all sG ∈ G′  therefore the value of V (sG) remains

the same; and (ii) mina∈APs′∈S [C(s  a  s′) + P (s′|s  a)V (s′)] ≤ ˆV ∗(s) for s ∈ S′ \ G′ 

the assignment V (s) ← ˆV ∗ is equivalent to at least one Bellman update on V (s)  be-
i.e. 
cause V is a lower bound on ˆV ∗ and Theorem 1. Because s 6∈ G′ and Deﬁnition 5 

mina∈A(cid:2)Ps′∈S C(s  a  s′) + P (s′|s  a)V (s′)(cid:3) ≤ ˆV ∗(s) is equivalent to the one Bellman update

in the original SSP S.

Theorem 3. Given an SSP S = hS  s0  G  A  P  Ci such that the reachability assumption holds  an
admissible heuristic H and a short-sighted problem generator that respects Deﬁnition 5  then SSiPP
always terminates.

Proof. Since OPTIMAL-SOLVER always ﬁnishes and the short-sighted SSP is an SSP by deﬁnition 
then a goal state sG of the short-sighted SSP is always reached  therefore the loop in line 3 of
Algorithm 1 always ﬁnishes. If sG ∈ G  then SSiPP terminates in this iteration. Otherwise  sG
is an artiﬁcial goal and sG 6= s (Deﬁnition 5)  i.e.  sG differs from the state s used as initial state
for the short-sighted SSP generation. Thus another iteration of SSiPP is performed using sG as
s. Suppose  for contradiction purpose  that every goal state reached during SSiPP execution is an
artiﬁcial goal  i.e.  SSiPP does not terminate. Then inﬁnitely many short-sighted SSPs are solved.
Since S is ﬁnite  then there exists s ∈ S that is updated inﬁnitely often  therefore V (s) → ∞.
However  V ∗(s) < ∞ by the reachability assumption. Since SSiPP performs Bellman updates
(Lemma 2) then V (s) ≤ V ∗(s) by monotonicity of Bellman updates (Theorem 1) and admissibility
G ∈ G and therefore
of H  a contradiction. Thus every execution of SSiPP reaches a goal state s′
terminates.

Corollary 4. Under the same assumptions of Theorem 3  the sequence hV 0  V 1  · · ·   V ti  where
V 0 = H and V t = SSiPP(S  t  V t−1)  converges to V ∗ as t → ∞ for all s ∈ S(π∗  s0).

Proof. Let S∗ ⊆ S be the set of states being visited inﬁnitely many times. Clearly  S(π∗  s0) ⊆ S∗
since a partial policy cannot be executed ad inﬁnitum without reaching a state in which it is not
deﬁned. Since SSiPP performs Bellman updates in the original SSP space (Lemma 2) and ev-
ery execution of SSiPP terminates (Theorem 3)  then we can view the sequence of lower bounds
hV 0  V 1  · · ·   V ti generated by SSiPP as asynchronous value iteration. The convergence of V t−1(s)
to V ∗(s) as t → ∞ for all s ∈ S(π∗  s0) ⊆ S∗ follows by [2  Proposition 2.2  p. 27] and guarantees
the convergence of SSiPP.

5

l

)
e
a
c
s
 

g
o
l
(
 
s
e
a
S

t

t

 
f

o

 
r
e
b
m
u
N

1080

1070

1060

1050

1040

1030

1020

1010

100

 
0

 

|S(π* s0)|
|S|

5

10

15

20

25

30

35

40

45

50

55

60

Triangle Tireworld Problem Size

(a)

(b)

Figure 2: (a) Map of the triangle tireworld for the sizes 1  2 and 3. Circles (squares) represent
locations in which there is one (no) spare tire. The shades of gray represent  for each location l 
maxπ P (car reaches l and the tire is not ﬂat when following the policy π from s0). (b) Log-lin plot
of the state space size (|S|) and the size of the states reachable from s0 when following the optimal
policy π∗ (|S(π∗  s0)|) versus the number of the triangle tireworld problem.

5 Experiments

We present two sets of experiments using the triangle tireworld problems [9  11  20]  a series of
probabilistic interesting problems [14] in which a car has to travel between locations in order to
reach a goal location from its initial location. The roads are represented as directed graph in a shape
of a triangle and  every time the car moves between locations  a ﬂat tire happens with probability
0.5. Some locations have a spare tire and in these locations the car can deterministically replace
its ﬂat tire by new one. When the car has a ﬂat tire  it cannot change its location  therefore the car
can get stuck in locations that do not have a spare tire (dead-ends). Figure 2(a) depicts the map of
the triangle tireworld problems 1  2 and 3 and Figure 2(b) shows the size of S and S(π∗  s0) for
problems up to size 60. For example  the problem number 3 has 28 locations  i.e.  28 nodes in the
corresponding graph on Figure 2(a)  its state space has 19562 states and its optimal policy reaches
8190 states.

Every triangle tireworld problem is a probabilistic interesting problem [14]: there is only one policy
that reaches the goal with probability 1 and all the other policies have probability at most 0.5 of
reaching the goal. Also  the solution based on the shortest path has probability 0.52n−1 of reaching
the goal  where n is the problem number. This property is illustrated by the shades of gray in
Figure 2(a) that represents  for each location l  maxπ P (car reaches l and the tire is not ﬂat when
following the policy π from s0).
For the experiments in this section  we use the zero-heuristic for all the planners  i.e.  V (s) = 0 for
all s ∈ S and LRTDP [4] as OPTIMAL-SOLVER for SSiPP. For all planners  the parameter ǫ (for
ǫ-convergence) is set to 10−4. For UCT  we disabled the random rollouts because the probability
of any policy other than the optimal policy to reach a dead-end is at least 0.5 therefore  with high-
probability  UCT would assign ∞ (cost of a dead-end) as the cost of all the states including the
initial state.

The experiments are conducted in a Linux machine with 4 cores running at 3.07GHz using MDP-
SIM [9] as environment simulator. The following terminology is used for describing the experi-
ments: round  the computation for a solution for the given SSP; and run  a set of rounds in which
learning is allowed between rounds  i.e.  the knowledge obtained from one round can be used to
solve subsequent rounds. The solution computed during one round is simulated by MDPSIM in a
client-server loop: MDPSIM sends a state s and requests an action from the planner  then the plan-
ner replies by sending the action a to be executed in s. The evaluation is done by the number of
rounds simulated by MDPSIM that reached a goal state. The maximum number of actions allowed
per round is 2000 and rounds that exceed this limit are stopped by MDPSIM and declared as failure 
i.e.  goal not reached.

6

Planner

SSiPP depth=8
UCT
SSiPP trajectory

5

50.0
50.0
50.0

10
40.7
50.0
50.0

15
41.2
50.0
50.0

Triangle Tireworld Problem Number

20
40.8
50.0
50.0

25
41.1
50.0
50.0

30
41.0
43.1
50.0

35
40.9
15.7
50.0

40
40.0
12.1
50.0

45
40.6
8.2
50.0

50
40.8
6.8
50.0

55
40.3
5.0
50.0

60
40.4
4.0
50.0

Table 1: Number of rounds solved out of 50 for experiment in Section 5.1. Results are averaged
over 10 runs and the 95% conﬁdence interval is always less than 1.0. In all the problems  SSiPP
using trajectory-based short-sighted SSPs solves all the 50 round in all the 10 runs  therefore its 95%
conﬁdence interval is 0.0 for all the problems. Best results shown in bold font.

Planner

SSiPP depth=8
LRTDP
UCT (4  100)
UCT (8  100)
UCT (2  100)
SSiPP ρ = 1.0
SSiPP ρ = 0.50
SSiPP ρ = 0.25
SSiPP ρ = 0.125

5

50.0
50.0
50.0
50.0
50.0
50.0
50.0
50.0
50.0

10
45.4
23.0
50.0
50.0
50.0
27.9
50.0
50.0
50.0

15
41.2
14.1
50.0
50.0
50.0
29.1
50.0
50.0
50.0

Triangle Tireworld Problem Number

20
42.3
0.3
48.8
46.3
49.5
26.8
50.0
50.0
50.0

25
41.2
-
24.0
24.0
23.2
26.0
50.0
47.6
50.0

30
44.1
-
12.3
12.3
12.0
26.6
50.0
45.0
50.0

35
42.4
-
6.5
6.7
7.5
28.6
50.0
41.1
50.0

40
32.7
-
4.0
3.7
3.5
27.2
50.0
42.7
50.0

45
20.6
-
2.5
2.2
2.2
26.6
50.0
41.9
49.8

50
14.1
-
1.3
1.2
1.2
27.6
50.0
40.7
37.4

55
9.9
-
1.0
1.0
1.0
26.2
50.0
40.1
26.4

60
7.0
-
0.7
0.6
0.6
26.9
50.0
40.4
18.9

Table 2: Number of rounds solved out of 50 for experiment in Section 5.2. Results are averaged
over 10 runs and the 95% conﬁdence interval is always less than 2.6. UCT (c  w) represents UCT
using c as bias parameter and w samples per decision. In all the problems  trajectory-based SSiPP
for ρ = 0.5 solves all the 50 round in all the 10 runs  therefore its 95% conﬁdence interval is 0.0 for
all the problems. Best results shown in bold font.

5.1 Fixed number of search nodes per decision

In this experiment  we compare the performance of UCT  depth-based SSiPP  and trajectory-based
SSiPP with respect to the number of nodes explored by depth-based SSiPP. Formally  to decide what
action to apply in a given state s  each planner is allowed to use at most B = |Ss t| search nodes 
i.e.  the size of the search space is bounded by the equivalent (s  t)-short-sighted SSP. We choose t
equals to 8 since it obtains the best performance in the triangle tireworld problems [1]. Given the
search nodes budget B  for UCT we sample the environment until the search tree contains B nodes;
and for trajectory-based SSiPP we use ρ = argmaxρ{|Ss ρ| s.t. B ≥ |Ss ρ|}.
The methodology for this experiment is as follows: for each problem  10 runs of 50 rounds are
performed for each planner using the search nodes budget B. The results  averaged over the 10 runs 
are presented in Table 1. We set as time and memory cut-off 8 hours and 8 Gb  respectively  and
UCT for problems 35 to 60 was the only planner preempted by the time cut-off. Trace-based SSiPP
outperforms both depth-based SSiPP and UCT  solving all the 50 rounds in all the 10 runs for all the
problems.

5.2 Fixed maximum planning time

In this experiment  we compare planners by limiting the maximum planning time. The methodology
used in this experiment is similar to the one in IPPC’04 and IPPC’06: for each problem  planners
need to solve 1 run of 50 rounds in 20 minutes. For this experiment  the planners are allowed to per-
form internal simulations  for instance  a planner can spend 15 minutes solving rounds using internal
simulations and then use the computed policy to solve the required 50 rounds through MDPSIM in
the remaining 5 minutes. The memory cut-off is 3Gb.

For this experiment  we consider the following planners: depth-based SSiPP for t = 8 [1]  trajectory-
based SSiPP for ρ ∈ {1.0  0.5  0.25  0.125}  LRTDP using 3-look-ahead [1] and 12 different
parametrizations of UCT obtained by using the bias parameter c ∈ {1  2  4  8} and the number
of samples per decision w ∈ {10  100  1000}. The winners of IPPC’04  IPPC’06 and IPPC’08 are

7

omitted since their performance on the triangle tireworld problems are strictly dominated by depth-
base SSiPP for t = 8. Table 2 shows the results of this experiment and due to space limitations we
show only the top 3 parametrizations of UCT: 1st (c = 4  w = 100); 2nd (c = 8  w = 100); and 3rd
(c = 2  w = 100).
All the four parametrizations of trajectory-based SSiPP outperform the other planners for problems
of size equal or greater than 45. Trajectory-based SSiPP using ρ = 0.5 is especially noteworthy
because it achieves the perfect score in all problems  i.e.  it reaches a goal state in all the 50 rounds
in all the 10 runs for all the problems. The same happens for ρ = 0.125 and problems up to size
40. For larger problems  trajectory-based SSiPP using ρ = 0.125 reaches the 20 minutes time
cut-off before solving 50 rounds  however all the solved rounds successfully reach the goal. This
interesting behavior of trajectory-based SSiPP for the triangle tireworld can be explained by the
following theorem:
Theorem 5. For the triangle tireworld  trajectory-based SSiPP using an admissible heuristic never
falls in a dead-end for ρ ∈ (0.5i+1  0.5i] for i ∈ {1  3  5  . . . }.

Proof Sketch. The optimal policy for the triangle tireworld is to follow the longest path: move from
the initial location l0 to the goal location lG passing through location lc  where l0  lc and lG are the
vertices of the triangle formed by the problem’s map. The path from lc to lG is unique  i.e.  there
is only one applicable move-car action for all the locations in this path. Therefore all the decision
making to ﬁnd the optimal policy happens between the locations l0 and lc. Each location l′ in the
path from l0 to lc has either two or three applicable move-car actions and we refer to the set of
locations l′ with three applicable move-car actions as N. Every location l′ ∈ N is reachable from
l0 by applying an even number of move-car actions (Figure 2(a)) and the three applicable move-car
actions in l′ are: (i) the optimal action ac  i.e.  move the car towards lc; (ii) the action aG that moves
the car towards lG; and (iii) the action ap that moves the car parallel to the shortest-path from l0 to
lG. The location reached by ap does not have a spare tire  therefore ap is never selected by a greedy
choice over any admissible heuristic since it reaches a dead-end with probability 0.5. The locations
reached by applying either ac or aG have a spare tire and the greedy choice between them depends
on the admissible heuristic used  thus aG might be selected instead of ac. However  after applying
aG  only one move-car action a is available and it reaches a location that does not have a spare
tire. Therefore  the greedy choice between ac and aG considering two or more move-car actions is
optimal under any admissible heuristic: every sequence of actions haG  a  . . . i reaches a dead-end
with probability at least 0.5 and at least one sequence of actions starting with ac has probability 0 to
reach a dead-end  e.g.  the optimal solution.
Given ρ  we denote as Ls ρ the set of all locations corresponding to states in Ss ρ and as ls the
location corresponding to the state s. Thus  Ls ρ contains all the locations reachable from ls using
up to m = ⌊log0.5 ρ⌋ + 1 move-car actions.
If m is even and ls ∈ N  then every location in
Ls ρ ∩ N represents a state either in Gs ρ or at least two move-car actions away from any state
in Gs ρ. Therefore the solution of the (s  ρ)-trajectory-based short-sighted SSP only chooses the
action ac to move the car. Also  since m is even  every state s used by SSiPP for generating
(s  ρ)-trajectory-based short-sighted SSPs has ls ∈ N. Therefore  for even values of m  i.e.  for
ρ ∈ (0.5i+1  0.5i] and i ∈ {1  3  5  . . . }  trajectory-based SSiPP always chooses the actions ac to
move the car to lc  thus avoiding the all dead-ends.

6 Conclusion

In this paper  we introduced trajectory-based short-sighted SSPs  a new model to manage uncertainty
in probabilistic planning problems. This approach consists of pruning the state space based on the
most likely trajectory between states and deﬁning artiﬁcial goal states that guide the solution towards
the original goals. We also deﬁned a class of short-sighted models that includes depth-based and
trajectory-based short-sighted SSPs and proved that SSiPP always terminates and is asymptotically
optimal for short-sighted models in this class.

We empirically compared trajectory-based SSiPP with depth-based SSiPP and other state-of-the-art
planners in the triangle tireworld. Trajectory-based SSiPP outperforms all the other planners and it
is the only planner able to scale up to problem number 60  a problem in which the optimal solution
contains approximately 1070 states  under the IPPC evaluation methodology.

8

References

[1] F. W. Trevizan and M. M. Veloso. Short-sighted stochastic shortest path problems. In In Proc.
of the 22nd International Conference on Automated Planning and Scheduling (ICAPS)  2012.

[2] D. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc  1996.
[3] A.G. Barto  S.J. Bradtke  and S.P. Singh. Learning to act using real-time dynamic program-

ming. Artiﬁcial Intelligence  72(1-2):81–138  1995.

[4] B. Bonet and H. Geffner. Labeled RTDP: Improving the convergence of real-time dynamic
In Proc. of the 13th International Conference on Automated Planning and

programming.
Scheduling (ICAPS)  2003.

[5] H.B. McMahan  M. Likhachev  and G.J. Gordon. Bounded real-time dynamic programming:
RTDP with monotone upper bounds and performance guarantees. In Proc. of the 22nd Inter-
national Conference on Machine Learning (ICML)  2005.

[6] Trey Smith and Reid G. Simmons. Focused Real-Time Dynamic Programming for MDPs:
In Proc. of the 21st National Conference on Artiﬁcial

Squeezing More Out of a Heuristic.
Intelligence (AAAI)  2006.

[7] S. Sanner  R. Goetschalckx  K. Driessens  and G. Shani. Bayesian real-time dynamic program-
ming. In Proc. of the 21st International Joint Conference on Artiﬁcial Intelligence (IJCAI) 
2009.

[8] S. Yoon  A. Fern  and R. Givan. FF-Replan: A baseline for probabilistic planning. In Proc. of

the 17th International Conference on Automated Planning and Scheduling (ICAPS)  2007.

[9] H.L.S. Younes  M.L. Littman  D. Weissman  and J. Asmuth. The ﬁrst probabilistic track of the
international planning competition. Journal of Artiﬁcial Intelligence Research  24(1):851–887 
2005.

[10] F. Teichteil-Koenigsbuch  G. Infantes  and U. Kuter. RFF: A robust  FF-based mdp planning
algorithm for generating policies with low probability of failure. 3rd International Planning
Competition (IPPC-ICAPS)  2008.

[11] D. Bryce and O. Buffet. 6th International Planning Competition: Uncertainty Track. In 3rd

International Probabilistic Planning Competition (IPPC-ICAPS)  2008.

[12] S. Yoon  A. Fern  R. Givan  and S. Kambhampati. Probabilistic planning via determinization
in hindsight. In Proc. of the 23rd National Conference on Artiﬁcial Intelligence (AAAI)  2008.
Improving Determinization in Hindsight for
Online Probabilistic Planning. In Proc. of the 20th International Conference on Automated
Planning and Scheduling (ICAPS)  2010.

[13] S. Yoon  W. Ruml  J. Benton  and M. B. Do.

[14] I. Little and S. Thi´ebaux. Probabilistic planning vs replanning. In Proc. of ICAPS Workshop

on IPC: Past  Present and Future  2007.

[15] J. Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving. Addison-

Wesley  Menlo Park  California  1985.

[16] Levente Kocsis and Csaba Szepesvri. Bandit based Monte-Carlo Planning.

European Conference on Machine Learning (ECML)  2006.

In Proc. of the

[17] T. Dean  L.P. Kaelbling  J. Kirman  and A. Nicholson. Planning under time constraints in

stochastic domains. Artiﬁcial Intelligence  76(1-2):35–74  1995.

[18] D.P. Bertsekas and J.N. Tsitsiklis. An analysis of stochastic shortest path problems. Mathe-

matics of Operations Research  16(3):580–595  1991.

[19] D.P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientiﬁc  1995.
[20] Blai Bonet and Robert Givan. 2th International Probabilistic Planning Competition (IPPC-
ICAPS). http://www.ldc.usb.ve/˜bonet/ipc5/ (accessed on Dec 13  2011) 
2007.

9

,Pratyusha Sharma
Deepak Pathak
Abhinav Gupta