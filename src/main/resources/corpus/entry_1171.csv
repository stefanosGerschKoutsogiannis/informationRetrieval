2013,Convergence of Monte Carlo Tree Search in Simultaneous Move Games,In this paper  we study Monte Carlo tree search (MCTS) in zero-sum extensive-form games with perfect information and simultaneous moves. We present a general template of MCTS algorithms for these games  which can be instantiated by various selection methods. We formally prove that if a selection method is $\epsilon$-Hannan consistent in a matrix game and satisfies additional requirements on exploration  then the MCTS algorithm eventually converges to an approximate Nash equilibrium (NE) of the extensive-form game. We empirically evaluate this claim using regret matching and Exp3 as the selection methods on randomly generated and worst case games. We confirm the formal result and show that additional MCTS variants also converge to approximate NE on the evaluated games.,Convergence of Monte Carlo Tree Search in

Simultaneous Move Games

Viliam Lis´y1

Vojtˇech Kovaˇr´ık1

Marc Lanctot2

Branislav Boˇsansk´y1

1Agent Technology Center

Dept. of Computer Science and Engineering
FEE  Czech Technical University in Prague

<name>.<surname>

2Department of Knowledge Engineering
Maastricht University  The Netherlands

marc.lanctot

@maastrichtuniversity.nl

@agents.fel.cvut.cz

Abstract

We study Monte Carlo tree search (MCTS) in zero-sum extensive-form games
with perfect information and simultaneous moves. We present a general tem-
plate of MCTS algorithms for these games  which can be instantiated by various
selection methods. We formally prove that if a selection method is -Hannan con-
sistent in a matrix game and satisﬁes additional requirements on exploration  then
the MCTS algorithm eventually converges to an approximate Nash equilibrium
(NE) of the extensive-form game. We empirically evaluate this claim using regret
matching and Exp3 as the selection methods on randomly generated games and
empirically selected worst case games. We conﬁrm the formal result and show
that additional MCTS variants also converge to approximate NE on the evaluated
games.

1

Introduction

Non-cooperative game theory is a formal mathematical framework for describing behavior of inter-
acting self-interested agents. Recent interest has brought signiﬁcant advancements from the algo-
rithmic perspective and new algorithms have led to many successful applications of game-theoretic
models in security domains [1] and to near-optimal play of very large games [2]. We focus on an
important class of two-player  zero-sum extensive-form games (EFGs) with perfect information and
simultaneous moves. Games in this class capture sequential interactions that can be visualized as a
game tree. The nodes correspond to the states of the game  in which both players act simultaneously.
We can represent these situations using the normal form (i.e.  as matrix games)  where the values
are computed from the successor sub-games. Many well-known games are instances of this class 
including card games such as Goofspiel [3  4]  variants of pursuit-evasion games [5]  and several
games from general game-playing competition [6].
Simultaneous-move games can be solved exactly in polynomial time using the backward induction
algorithm [7  4]  recently improved with alpha-beta pruning [8  9]. However  the depth-limited
search algorithms based on the backward induction require domain knowledge (an evaluation func-
tion) and computing the cutoff conditions requires linear programming [8] or using a double-oracle
method [9]  both of which are computationally expensive. For practical applications and in situations
with limited domain knowledge  variants of simulation-based algorithms such as Monte Carlo Tree
Search (MCTS) are typically used in practice [10  11  12  13]. In spite of the success of MCTS and
namely its variant UCT [14] in practice  there is a lack of theory analyzing MCTS outside two-player
perfect-information sequential games. To the best of our knowledge  no convergence guarantees are
known for MCTS in games with simultaneous moves or general EFGs.

1

Figure 1: A game tree of a game with perfect information and simultaneous moves. Only the leaves
contain the actual rewards; the remaining numbers are the expected reward for the optimal strategy.

In this paper  we present a general template of MCTS algorithms for zero-sum perfect-information
simultaneous move games. It can be instantiated using any regret minimizing procedure for matrix
games as a function for selecting the next actions to be sampled. We formally prove that if the algo-
rithm uses an -Hannan consistent selection function  which assures attempting each action inﬁnitely
many times  the MCTS algorithm eventually converges to a subgame perfect -Nash equilibrium of
the extensive form game. We empirically evaluate this claim using two different -Hannan consis-
tent procedures: regret matching [15] and Exp3 [16]. In the experiments on randomly generated and
worst case games  we show that the empirical speed of convergence of the algorithms based on our
template is comparable to recently proposed MCTS algorithms for these games. We conjecture that
many of these algorithms also converge to -Nash equilibrium and that our formal analysis could be
extended to include them.

2 Deﬁnitions and background

A ﬁnite zero-sum game with perfect information and simultaneous moves can be described by a
tuple (N  H Z A T   u1  h0)  where N = {1  2} contains player labels  H is a set of inner states
and Z denotes the terminal states. A = A1 × A2 is the set of joint actions of individual players and
we denote A1(h) = {1 . . . mh} and A2(h) = {1 . . . nh} the actions available to individual players
in state h ∈ H. The transition function T : H×A1×A2 (cid:55)→ H∪Z deﬁnes the successor state given
a current state and actions for both players. For brevity  we sometimes denote T (h  i  j) ≡ hij.
The utility function u1 : Z (cid:55)→ [vmin  vmax] ⊆ R gives the utility of player 1  with vmin and vmax
denoting the minimum and maximum possible utility respectively. Without loss of generality we
assume vmin = 0  vmax = 1  and ∀z ∈ Z  u2(z) = 1 − u1(z). The game starts in an initial state h0.
A matrix game is a single-stage simultaneous move game with action sets A1 and A2. Each entry
in the matrix M = (aij) where (i  j) ∈ A1 × A2 and aij ∈ [0  1] corresponds to a payoff (to player
1) if row i is chosen by player 1 and column j by player 2. A strategy σq ∈ ∆(Aq) is a distribution
over the actions in Aq. If σ1 is represented as a row vector and σ2 as a column vector  then the
expected value to player 1 when both players play with these strategies is u1(σ1  σ2) = σ1M σ2.
Given a proﬁle σ = (σ1  σ2)  deﬁne the utilities against best response strategies to be u1(br  σ2) =
2. A strategy proﬁle (σ1  σ2) is an
maxσ(cid:48)
-Nash equilibrium of the matrix game M if and only if

1M σ2 and u1(σ1  br) = minσ(cid:48)

2∈∆(A2) σ1M σ(cid:48)

1∈∆(A1) σ(cid:48)

u1(br  σ2) − u1(σ1  σ2) ≤ 

and

u1(σ1  σ2) − u1(σ1  br) ≤ 

(1)

Two-player perfect information games with simultaneous moves are sometimes appropriately called
stacked matrix games because at every state h each joint action from set A1(h)×A2(h) either leads
to a terminal state or to a subgame which is itself another stacked matrix game (see Figure 1).
A behavioral strategy for player q is a mapping from states h ∈ H to a probability distribution over
the actions Aq(h)  denoted σq(h). Given a proﬁle σ = (σ1  σ2)  deﬁne the probability of reaching
a terminal state z under σ as πσ(z) = π1(z)π2(z)  where each πq(z) is a product of probabilities of
the actions taken by player q along the path to z. Deﬁne Σq to be the set of behavioral strategies for
player q. Then for any strategy proﬁle σ = (σ1  σ2) ∈ Σ1 × Σ2 we deﬁne the expected utility of the
strategy proﬁle (for player 1) as

πσ(z)u1(z)

(2)

(cid:88)

z∈Z

u(σ) = u(σ1  σ2) =

2

An -Nash equilibrium proﬁle (σ1  σ2) in this case is deﬁned analogously to (1). In other words 
none of the players can improve their utility by more than  by deviating unilaterally. If the strategies
are an -NE in each subgame starting in an arbitrary game state  the equilibrium strategy is termed
subgame perfect. If σ = (σ1  σ2) is an exact Nash equilibrium (i.e.  -NE with  = 0)  then we
denote the unique value of the game vh0 = u(σ1  σ2). For any h ∈ H  we denote vh the value of
the subgame rooted in state h.

3 Simultaneous move Monte-Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a simulation-based state space search algorithm often used
in game trees. The nodes in the tree represent game states. The main idea is to iteratively run
simulations to a terminal state  incrementally growing a tree rooted at the initial state of the game. In
its simplest form  the tree is initially empty and a single leaf is added each iteration. Each simulation
starts by visiting nodes in the tree  selecting which actions to take based on a selection function and
information maintained in the node. Consequently  it transitions to the successor states. When a
node is visited whose immediate children are not all in the tree  the node is expanded by adding a
new leaf to the tree. Then  a rollout policy (e.g.  random action selection) is applied from the new
leaf to a terminal state. The outcome of the simulation is then returned as a reward to the new leaf
and the information stored in the tree is updated.
In Simultaneous Move MCTS (SM-MCTS)  the main difference is that a joint action of both players
is selected. The algorithm has been previously applied  for example in the game of Tron [12]  Urban
Rivals [11]  and in general game-playing [10]. However  guarantees of convergence to NE remain
unknown. The convergence to a NE depends critically on the selection and update policies applied 
which are even more non-trivial than in purely sequential games. The most popular selection policy
in this context (UCB) performs very well in some games [12]  but Shaﬁei et al. [17] show that it
does not converge to Nash equilibrium  even in a simple one-stage simultaneous move game. In this
paper  we focus on variants of MCTS  which provably converge to (approximate) NE; hence we do
not discuss UCB any further. Instead  we describe variants of two other selection algorithms after
explaining the abstract SM-MCTS algorithm.
Algorithm 1 describes a single simulation of SM-MCTS. T represents the MCTS tree in which
each state is represented by one node. Every node h maintains a cumulative reward sum over all
simulations through it  Xh  and a visit count nh  both initially set to 0. As depicted in Figure 1 
a matrix of references to the children is maintained at each inner node. The critical parts of the
algorithm are the updates on lines 8 and 14 and the selection on line 10. Each variant below will
describe a different way to select an action and update a node. The standard way of deﬁning the
value to send back is RetVal(u1  Xh  nh) = u1  but we discuss also RetVal(u1  Xh  nh) = Xh/nh 
which is required for the formal analysis in Section 4. We denote this variant of the algorithms

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

SM-MCTS(node h)

if h ∈ Z then return u1(h)
else if h ∈ T and ∃(i  j) ∈ A1(h) × A2(h) not previously selected then
Choose one of the previously unselected (i  j) and h(cid:48) ← T (h  i  j)
Add h(cid:48) to T
u1 ← Rollout(h(cid:48))
Xh(cid:48) ← Xh(cid:48) + u1; nh(cid:48) ← nh(cid:48) + 1
Update(h  i  j  u1)
return RetVal(u1  Xh(cid:48)  nh(cid:48))

(i  j) ← Select(h)
h(cid:48) ← T (h  i  j)
u1 ← SM-MCTS(h(cid:48))
Xh ← Xh + u1; nh ← nh + 1
Update(h  i  j  u1)
return RetVal(u1  Xh  nh)

Algorithm 1: Simultaneous Move Monte Carlo Tree Search

3

with additional “M” for mean. Algorithm 1 and the variants below are expressed from player 1’s
perspective. Player 2 does the same except using negated utilities.

3.1 Regret matching

This variant applies regret-matching [15] to the current estimated matrix game at each stage. Sup-
pose iterations are numbered from s ∈ {1  2  3 ···} and at each iteration and each inner node h there
is a mixed strategy σs(h) used by each player  initially set to uniform random: σ0(h  i) = 1/|A(h)|.
Each player maintains a cumulative regret rh[i] for having played σs(h) instead of i ∈ A1(h). The
values are initially set to 0.
On iteration s  the Select function (line 10 in Algorithm 1) ﬁrst builds the player’s current strategies
from the cumulative regret. Deﬁne x+ = max(x  0) 

σs(h  a) =

r+
h [a]
R+
sum

if R+

sum > 0 oth.

1

|A1(h)|   where R+

sum =

r+
h [i].

(3)

(cid:88)

i∈A1(h)

The strategy is computed by assigning higher weight proportionally to actions based on the regret of
having not taken them over the long-term. To ensure exploration  an γ-on-policy sampling procedure
is used choosing action i with probability γ/|A(h)| + (1 − γ)σs(h  i)  for some γ > 0.
The Updates on lines 8 and 14 add regret accumulated at the iteration to the regret tables rh. Suppose
joint action (i1  j2) is sampled from the selection policy and utility u1 is returned from the recursive
call on line 12. Deﬁne x(h  i  j) = Xhij if (i  j) (cid:54)= (i1  j2)  or u1 otherwise. The updates to the
regret are:

∀i(cid:48) ∈ A1(h)  rh[i(cid:48)] ← rh[i(cid:48)] + (x(h  i(cid:48)  j) − u1).

3.2 Exp3

In Exp3 [16]  a player maintains an estimate of the sum of rewards  denoted xh i  and visit counts
nh i for each of their actions i ∈ A1. The joint action selected on line 10 is composed of an action
independently selected for each player. The probability of sampling action a in Select is

σs(h  a) =

(cid:80)

(1 − γ) exp(ηwh a)
i∈A1(h) exp(ηwh i)

+

γ

|A1(h)|   where η =

γ

|A1(h)| and wh i = xh i

1.

(4)

The Update after selecting actions (i  j) and obtaining a result (u1  u2) updates the visits count
(nh i ← nh i + 1) and adds to the corresponding reward sum estimates the reward divided by the
probability that the action was played by the player (xh i ← xh i + u1/σs(h  i)). Dividing the value
by the probability of selecting the corresponding action makes xh i estimate the sum of rewards over
all iterations  not only the once where action i was selected.

4 Formal analysis

We focus on the eventual convergence to approximate NE  which allows us to make an important
simpliﬁcation: We disregard the incremental building of the tree and assume we have built the
complete tree. We show that this will eventually happen with probability 1 and that the statistics
collected during the tree building phase cannot prevent the eventual convergence.
The main idea of the proof is to show that the algorithm will eventually converge close to the optimal
strategy in the leaf nodes and inductively prove that it will converge also in higher levels of the tree.
In order to do that  after introducing the necessary notation  we start by analyzing the situation in
simple matrix games  which corresponds mainly to the leaf nodes of the tree. In the inner nodes of
the tree  the observed payoffs are imprecise because of the stochastic nature of the selection functions
and bias caused by exploration  but the error can be bounded. Hence  we continue with analysis of
repeated matrix games with bounded error. Finally  we compose the matrices with bounded errors in
1In practice  we set wh i = xh i−maxi(cid:48)∈A1(h) xh i(cid:48) since exp(xh i) can easily cause numerical overﬂows.

This reformulation computes the same values as the original algorithm but is more numerically stable.

4

a multi-stage setting to prove convergence guarantees of SM-MCTS. Any proofs that are omitted in
the paper are included in the appendix available in the supplementary material and on http://arxiv.org
(arXiv:1310.8613).

4.1 Notation and deﬁnitions

Consider a repeatedly played matrix game where at time s players 1 and 2 choose actions is and js
respectively. We will use the convention (|A1| |A2|) = (m  n). Deﬁne

G(t) =

aisjs  

g(t) =

1
t

G(t) 

and Gmax(t) = max
i∈A1

aijs 

t(cid:88)

s=1

t(cid:88)

s=1

where G(t) is the cumulative payoff  g(t) is the average payoff  and Gmax is the maximum cumula-
tive payoff over all actions  each to player 1 and at time t. We also denote gmax(t) = Gmax(t)/t
and by R(t) = Gmax(t) − G(t) and r(t) = gmax(t) − g(t) the cumulative and average regrets.
For actions i of player 1 and j of player 2  we denote ti  tj the number of times these actions were
chosen up to the time t and tij the number of times both of these actions has been chosen at once.
By empirical frequencies we mean the strategy proﬁle (ˆσ1(t)  ˆσ2(t)) ∈ (cid:104)0  1(cid:105)m×(cid:104)0  1(cid:105)n given by
the formulas ˆσ1(t  i) = ti/t  ˆσ2(t  j) = tj/t. By average strategies  we mean the strategy proﬁle
2(j)/t  where σs
1 

(¯σ1(t)  ¯σ2(t)) given by the formulas ¯σ1(t  i) =(cid:80)t

1(i)/t  ¯σ2(t  j) =(cid:80)t

2 are the strategies used at time s.
σs
Deﬁnition 4.1. We say that a player is -Hannan-consistent if  for any payoff sequences (e.g. 
against any opponent strategy)  lim supt→∞  r(t) ≤  holds almost surely. An algorithm A is -
Hannan consistent  if a player who chooses his actions based on A is -Hannan consistent.

s=1 σs

s=1 σs

Hannan consistency (HC) is a commonly studied property in the context of online learning in re-
peated (single stage) decisions. In particular  RM and variants of Exp3 has been shown to be Hannan
consistent in matrix games [15  16]. In order to ensure that the MCTS algorithm will eventually visit
each node inﬁnitely many times  we need the selection function to satisfy the following property.
Deﬁnition 4.2. We say that A is an algorithm with guaranteed exploration  if for players 1 and 2
both using A for action selection limt→∞ tij = ∞ holds almost surely ∀(i  j) ∈ A1 × A2.
Note that most of the HC algorithms  namely RM and Exp3  guarantee exploration without any
modiﬁcation. If there is an algorithm without this property  it can be adjusted the following way.
Deﬁnition 4.3. Let A be an algorithm used for choosing action in a matrix game M. For ﬁxed
exploration parameter γ ∈ (0  1) we deﬁne a modiﬁed algorithm A∗ as follows: In each time 
with probability (1 − γ) run one iteration of A and with probability γ choose the action randomly
uniformly over available actions  without updating any of the variables belonging to A.

4.2 Repeated matrix games

First we show that the -Hannan consistency is not lost due to the additional exploration.
Lemma 4.4. Let A be an -Hannan consistent algorithm. Then A∗ is an ( + γ)-Hannan consistent
algorithm with guaranteed exploration.

In previous works on MCTS in our class of games  RM variants generally suggested using the
average strategy and Exp3 variants the empirical frequencies to obtain the strategy to be played.
The following lemma says there eventually is no difference between the two.
Lemma 4.5. As t approaches inﬁnity  the empirical frequencies and average strategies will almost
surely be equal. That is  lim supt→∞ maxi∈A1 |ˆσ1(t  i) − ¯σ1(t  i)| = 0 holds with probability 1.
The proof is a consequence of the martingale version of Strong Law of Large Numbers.
It is well known that two Hannan consistent players will eventually converge to NE (see [18  p. 11]
and [19]). We prove a similar result for the approximate versions of the notions.
Lemma 4.6. Let  > 0 be a real number. If both players in a matrix game with value v are -Hannan
consistent  then the following inequalities hold for the empirical frequencies almost surely:

u (br  ˆσ2(t)) ≤ v + 2

lim sup
t→∞

t→∞ u (ˆσ1(t)  br) ≥ v − 2.

lim inf

(5)

and

5

The proof shows that if the value caused by the empirical frequencies was outside of the interval
inﬁnitely many times with positive probability  it would be in contradiction with deﬁnition of -HC.
The following corollary is than a direct consequence of this lemma.
Corollary 4.7. If both players in a matrix game are -Hannan consistent  then there almost surely
exists t0 ∈ N  such that for every t ≥ t0 the empirical frequencies and average strategies form
(4 + δ)-equilibrium for arbitrarly small δ > 0.

The constant 4 is caused by going from a pair of strategies with best responses within 2 of the game
value guaranteed by Lemma 4.6 to the approximate NE  which multiplies the distance by two.

4.3 Repeated matrix games with bounded error

After deﬁning the repeated games with error  we present a variant of Lemma 4.6 for these games.
Deﬁnition 4.8. We deﬁne M (t) = (aij(t)) to be a game  in which if players chose actions i and
j  they receive randomized payoffs aij (t  (i1  ...it−1)  (j1  ...jt−1)). We will denote these simply
as aij(t)  but in fact they are random variables with values in [0  1] and their distribution in time
t depends on the previous choices of actions. We say that M (t) = (aij(t)) is a repeated game
with error η  if there is a matrix game M = (aij) and almost surely exists t0 ∈ N  such that
|aij(t) − aij| < η holds for all t ≥ t0.

In this context  we will denote G(t) =(cid:80)
variables without errors ( ˜G(t) =(cid:80) aisjs etc.). Symbols v and u (· ·) will still be used with respect

s∈{1...t} aisjs (s) etc. and use tilde for the corresponding

to M without errors. The following lemma states that even with the errors  -HC algorithms still
converge to an approximate NE of the game.
Lemma 4.9. Let  > 0 and c ≥ 0. If M (t) is a repeated game with error c and both players are
-Hannan consistent then the following inequalities hold almost surely:

u (br  ˆσ2) ≤ v + 2(c + 1) 

lim sup
t→∞
and v − (c + 1) ≤ lim inf

lim inf

t→∞ u (ˆσ1  br) ≥ v − 2(c + 1)
t→∞ g(t) ≤ lim sup
t→∞

g(t) ≤ v + (c + 1).

(6)

(7)

The proof is similar to the proof of Lemma 4.6. It needs an additional claim that if the algorithm is
-HC with respect to the observed values with errors  it still has a bounded regret with respect to the
exact values. In the same way as in the previous subsection  a direct consequence of the lemma is
the convergence to an approximate Nash equilibrium.
Theorem 4.10. Let   c > 0 be real numbers. If M (t) is a repeated game with error c and both
players are -Hannan consistent  then for any δ > 0 there almost surely exists t0 ∈ N  such that for
all t ≥ t0 the empirical frequencies form (4(c + 1) + δ)-equilibrium of the game M.

4.4 Perfect-information extensive-form games with simultaneous moves

Theorem 4.11. Let(cid:0)M h(cid:1)

Now we have all the necessary components to prove the main theorem.

h∈H be a game with perfect information and simultaneous moves with
maximal depth D. Then for every -Hannan consistent algorithm A with guaranteed exploration
and arbitrary small δ > 0  there almost surely exists t0  so that the average strategies (ˆσ1(t)  ˆσ2(t))
form a subgame perfect

(cid:0)2D2 + δ(cid:1) -Nash equilibrium for all t ≥ t0.

Once we have established the convergence of the -HC algorithms in games with errors  we can
proceed by induction. The games in the leaf nodes are simple matrix game so they will eventually
converge and they will return the mean reward values in a bounded distance from the actual value
of the game (Lemma 4.9 with c = 0). As a result  in the level just above the leaf nodes  the -
HC algorithms are playing a matrix game with a bounded error and by Lemma 4.9  they will also
eventually return the mean values within a bounded interval. On level d from the leaf nodes  the
errors of returned values will be in the order of d and players can gain 2d by deviating. Summing
the possible gain of deviations on each level leads to the bound in the theorem. The subgame
perfection of the equilibrium results from the fact that for proving the bound on approximation in the
whole game (i.e.  in the root of the game tree)  a smaller bound on approximation of the equilibrium
is proven for all subgames in the induction. The formal proof is presented in the appendix.

6

Figure 2: Exploitability of strategies given by the empirical frequencies of Regret matching with
propagating values (RM) and means (RMM) for various depths and branching factors.

5 Empirical analysis

In this section  we ﬁrst evaluate the inﬂuence of propagating the mean values instead of the current
sample value in MCTS to the speed of convergence to Nash equilibrium. Afterwards  we try to
assess the convergence rate of the algorithms in the worst case. In most of the experiments  we
use as the bases of the SM-MCTS algorithm Regret matching as the selection strategy  because a
superior convergence rate bound is known for this algorithm and it has been reported to be very
successful also empirically in [20]. We always use the empirical frequencies to create the evaluated
strategy and measure the exploitability of the ﬁrst player’s strategy (i.e.  vh0 − u(ˆσ1  br)).

5.1

Inﬂuence of propagation of the mean

The formal analysis presented in the previous section requires the algorithms to return the mean of
all the previous samples instead of the value of the current sample. The latter is generally the case in
previous works on SM-MCTS [20  11]. We run both variants with the Regret matching algorithm on
a set of randomly generated games parameterized by depth and branching factor. Branching factor
was always the same for both players. For the following experiments  the utility values are randomly
selected uniformly from interval (cid:104)0  1(cid:105). Each experiment uses 100 random games and 100 runs of
the algorithm.
Figure 2 presents how the exploitability of the strategies produced by Regret matching with prop-
agation of the mean (RMM) and current sample value (RM) develops with increasing number of
iterations. Note that both axes are in logarithmic scale. The top graph is for depth of 2  differ-
ent branching factors (BF) and γ ∈ {0.05  0.1  0.2}. The bottom one presents different depths for
BF = 2. The results show that both methods converge to the approximate Nash equilibrium of the
game. RMM converges slightly slower in all cases. The difference is very small in small games  but
becomes more apparent in games with larger depth.

5.2 Empirical convergence rate

Although the formal analysis guarantees the convergence to an -NE of the game  the rate of the con-
vergence is not given. Therefore  we give an empirical analysis of the convergence and speciﬁcally
focus on the cases that reached the slowest convergence from a set of evaluated games.

7

BF=2BF=3BF=50.010.100.010.100.010.100.050.10.2101000101000101000tExploitabilityDepth=2Depth=3Depth=40.4000.2000.1000.0500.0250.1100100001001000010010000tExploitabilityMethodRMRMMFigure 3: The games with maximal exploitability after 1000 iterations with RM (left) and RMM
(right) and the corresponding exploitabililty for all evaluated methods.

We have performed a brute force search through all games of depth 2 with branching factor 2 and
utilities form the set {0  0.5  1}. We made 100 runs of RM and RMM with exploration set to γ =
0.05 for 1000 iterations and computed the mean exploitability of the strategy. The games with the
highest exploitability for each method are presented in Figure 3. These games are not guaranteed to
be the exact worst case  because of possible error caused by only 100 runs of the algorithm  but they
are representatives of particularly difﬁcult cases for the algorithms. In general  the games that are
most difﬁcult for one method are difﬁcult also for the other. Note that we systematically searched
also for games in which RMM performs better than RM  but this was never the case with sufﬁcient
number of runs of the algorithms in the selected games.
Figure 3 shows the convergence of RM and Exp3 with propagating the current sample values and
the mean values (RMM and Exp3M) on the empirically worst games for the RM variants. The RM
variants converge to the minimal achievable values (0.0119 and 0.0367) after a million iterations.
This values corresponds exactly to the exploitability of the optimal strategy combined with the uni-
form exploration with probability 0.05. The Exp3 variants most likely converge to the same values 
however  they did not fully make it in the ﬁrst million iterations in WC RM. The convergence rate of
all the variants is similar and the variants with propagating means always converge a little slower.

6 Conclusion

We present the ﬁrst formal analysis of convergence of MCTS algorithms in zero-sum extensive-form
games with perfect information and simultaneous moves. We show that any -Hannan consistent
algorithm can be used to create a MCTS algorithm that provably converges to an approximate Nash
equilibrium of the game. This justiﬁes the usage of the MCTS as an approximation algorithm for
this class of games from the perspective of algorithmic game theory. We complement the formal
analysis with experimental evaluation that shows that other MCTS variants for this class of games 
which are not covered by the proof  also converge to the approximate NE of the game. Hence  we
believe that the presented proofs can be generalized to include these cases as well. Besides this  we
will focus our future research on providing ﬁnite time convergence bounds for these algorithms and
generalizing the results to more general classes of extensive-form games with imperfect information.

Acknowledgments

This work is partially funded by the Czech Science Foundation (grant no. P202/12/2054)  the Grant
Agency of the Czech Technical University in Prague (grant no. OHK3-060/12)  and the Netherlands
Organisation for Scientiﬁc Research (NWO) in the framework of the project Go4Nature  grant num-
ber 612.000.938. The access to computing and storage facilities owned by parties and projects con-
tributing to the National Grid Infrastructure MetaCentrum  provided under the programme “Projects
of Large Infrastructure for Research  Development  and Innovations” (LM2010005) is appreciated.

8

WC_RMWC_RMM0.01250.02500.05000.10000.20000.40000.80001e+021e+041e+061e+021e+041e+06tExploitabilityMethodExp3Exp3MRMRMMReferences
[1] Manish Jain  Dmytro Korzhyk  Ondrej Vanek  Vincent Conitzer  Michal Pechoucek  and Milind Tambe. A
double oracle algorithm for zero-sum security games. In Tenth International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 2011)  pages 327–334  2011.

[2] Michael Johanson  Nolan Bard  Neil Burch  and Michael Bowling. Finding optimal abstract strategies in
extensive-form games. In Proceedings of the Twenty-Sixth Conference on Artiﬁcial Intelligence (AAAI-
12)  pages 1371–1379  2012.

[3] S. M. Ross. Goofspiel — the game of pure strategy. Journal of Applied Probability  8(3):621–625  1971.
[4] Glenn C. Rhoads and Laurent Bartholdi. Computer solution to the game of pure strategy. Games 

3(4):150–156  2012.

[5] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In In Pro-
ceedings of the Eleventh International Conference on Machine Learning (ICML-1994)  pages 157–163.
Morgan Kaufmann  1994.

[6] M. Genesereth and N. Love. General game-playing: Overview of the AAAI competition. AI Magazine 

26:62–72  2005.

[7] Michael Buro. Solving the Oshi-Zumo game. In Proceedings of Advances in Computer Games 10  pages

361–366  2003.

[8] Abdallah Safﬁdine  Hilmar Finnsson  and Michael Buro. Alpha-beta pruning for games with simultaneous
moves. In Proceedings of the Thirty-Second Conference on Artiﬁcial Intelligence (AAAI-12)  pages 556–
562  2012.

[9] Branislav Bosansky  Viliam Lisy  Jiri Cermak  Roman Vitek  and Michal Pechoucek. Using double-oracle
method and serialized alpha-beta search for pruning in simultaneous moves games. In Proceedings of the
Twenty-Third International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages 48–54  2013.

[10] H. Finnsson and Y. Bj¨ornsson. Simulation-based approach to general game-playing. In The Twenty-Third

AAAI Conference on Artiﬁcial Intelligence  pages 259–264. AAAI Press  2008.

[11] Olivier Teytaud and S´ebastien Flory. Upper conﬁdence trees with short term partial information.

In
Applications of Eolutionary Computation (EvoApplications 2011)  Part I  volume 6624 of LNCS  pages
153–162  Berlin  Heidelberg  2011. Springer-Verlag.

[12] Pierre Perick  David L. St-Pierre  Francis Maes  and Damien Ernst. Comparison of different selection
strategies in monte-carlo tree search for the game of Tron. In Proceedings of the IEEE Conference on
Computational Intelligence and Games (CIG)  pages 242–249  2012.

[13] Hilmar Finnsson. Simulation-Based General Game Playing. PhD thesis  Reykjavik University  2012.
[14] L. Kocsis and C. Szepesv´ari. Bandit-based Monte Carlo planning.

In 15th European Conference on

Machine Learning  volume 4212 of LNCS  pages 282–293  2006.

[15] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica 

68(5):1127–1150  2000.

[16] Peter Auer  Nicol`o Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing  32(1):48–77  2002.

[17] M. Shaﬁei  N. R. Sturtevant  and J. Schaeffer. Comparing UCT versus CFR in simultaneous games. In

Proceeding of the IJCAI Workshop on General Game-Playing (GIGA)  pages 75–82  2009.

[18] Kevin Waugh. Abstraction in large extensive games. Master’s thesis  University of Alberta  2009.
[19] A. Blum and Y. Mansour. Learning  regret minimization  and equilibria. In Noam Nisan  Tim Rough-
garden  Eva Tardos  and Vijay V. Vazirani  editors  Algorithmic Game Theory  chapter 4. Cambridge
University Press  2007.

[20] Marc Lanctot  Viliam Lis´y  and Mark H.M. Winands. Monte Carlo tree search in simultaneous move

games with applications to Goofspiel. In Workshop on Computer Games at IJCAI  2013.

9

,Viliam Lisy
Vojta Kovarik
Marc Lanctot
Branislav Bosansky
Mohammad Norouzi
Maxwell Collins
Matthew Johnson
David Fleet
Pushmeet Kohli
Rodrigo Nogueira
Kyunghyun Cho