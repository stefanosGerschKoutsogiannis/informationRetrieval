2017,Nonlinear random matrix theory for deep learning,Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices  the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear  which prevents the straightforward utilization of many of the existing mathematical results. In this work  we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix $Y^TY$  $Y=f(WX)$  where $W$ is a random weight matrix  $X$ is a random data matrix  and $f$ is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix  which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature methods on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis  we identify an intriguing new class of activation functions with favorable properties.,Nonlinear random matrix theory for deep learning

Jeffrey Pennington

Google Brain

jpennin@google.com

Pratik Worah
Google Research

pworah@google.com

Abstract

Neural network conﬁgurations with random weights play an important role in the
analysis of deep learning. They deﬁne the initial loss landscape and are closely
related to kernel and random feature methods. Despite the fact that these networks
are built out of random matrices  the vast and powerful machinery of random matrix
theory has so far found limited success in studying them. A main obstacle in this
direction is that neural networks are nonlinear  which prevents the straightforward
utilization of many of the existing mathematical results. In this work  we open
the door for direct applications of random matrix theory to deep learning by
demonstrating that the pointwise nonlinearities typically applied in neural networks
can be incorporated into a standard method of proof in random matrix theory
known as the moments method. The test case for our study is the Gram matrix
Y T Y   Y = f (W X)  where W is a random weight matrix  X is a random data
matrix  and f is a pointwise nonlinear activation function. We derive an explicit
representation for the trace of the resolvent of this matrix  which deﬁnes its limiting
spectral distribution. We apply these results to the computation of the asymptotic
performance of single-layer random feature networks on a memorization task and
to the analysis of the eigenvalues of the data covariance matrix as it propagates
through a neural network. As a byproduct of our analysis  we identify an intriguing
new class of activation functions with favorable properties.

1

Introduction

The list of successful applications of deep learning is growing at a staggering rate.
Image
recognition (Krizhevsky et al.  2012)  audio synthesis (Oord et al.  2016)  translation (Wu et al. 
2016)  and speech recognition (Hinton et al.  2012) are just a few of the recent achievements. Our
theoretical understanding of deep learning  on the other hand  has progressed at a more modest pace.
A central difﬁculty in extending our understanding stems from the complexity of neural network loss
surfaces  which are highly non-convex functions  often of millions or even billions (Shazeer et al. 
2017) of parameters.

In the physical sciences  progress in understanding large complex systems has often come by
approximating their constituents with random variables; for example  statistical physics and
thermodynamics are based in this paradigm. Since modern neural networks are undeniably large
complex systems  it is natural to consider what insights can be gained by approximating their
parameters with random variables. Moreover  such random conﬁgurations play at least two privileged
roles in neural networks: they deﬁne the initial loss surface for optimization  and they are closely
related to random feature and kernel methods. Therefore it is not surprising that random neural
networks have attracted signiﬁcant attention in the literature over the years.

Another useful technique for simplifying the study of large complex systems is to approximate
their size as inﬁnite. For neural networks  the concept of size has at least two axes: the number

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

of samples and the number of parameters. It is common  particularly in the statistics literature  to
consider the mean performance of a ﬁnite-capacity model against a given data distribution. From this
perspective  the number of samples  m  is taken to be inﬁnite relative to the number of parameters  n 
i.e. n/m ! 0. An alternative perspective is frequently employed in the study of kernel or random
feature methods. In this case  the number of parameters is taken to be inﬁnite relative to the number
of samples  i.e. n/m ! 1. In practice  however  most successful modern deep learning architectures
tend to have both a large number of samples and a large number of parameters  often of roughly the
same order of magnitude. (One simple explanation for this scaling may just be that the other extremes
tend to produce over- or under-ﬁtting). Motivated by this observation  in this work we explore the
inﬁnite size limit in which both the number of samples and the number of parameters go to inﬁnity at
the same rate  i.e. n  m ! 1 with n/m =   for some ﬁnite constant . This perspective puts us
squarely in the regime of random matrix theory.
An abundance of matrices are of practical and theoretical interest in the context of random neural
networks. For example  the output of the network  its Jacobian  and the Hessian of the loss function
with respect to the weights are all interesting objects of study.
In this work we focus on the
m Y T Y   where Y = f (W X)  W is a Gaussian
computation of the eigenvalues of the matrix M ⌘ 1
random weight matrix  X is a Gaussian random data matrix  and f is a pointwise activation function.
In many ways  Y is a basic primitive whose understanding is necessary for attacking more complicated
cases; for example  Y appears in the expressions for all three of the matrices mentioned above. But
studying Y is also quite interesting in its own right  with several interesting applications to machine
learning that we will explore in Section 4.

1.1 Our contribution

The nonlinearity of the activation function prevents us from leveraging many of the existing mathemat-
ical results from random matrix theory. Nevertheless  most of the basic tools for computing spectral
densities of random matrices still apply in this setting. In this work  we show how to overcome
some of the technical hurdles that have prevented explicit computations of this type in the past. In
particular  we employ the so-called moments method  deducing the spectral density of M from the
traces tr M k. Evaluating the traces involves computing certain multi-dimensional integrals  which
we show how to evaluate  and enumerating a certain class of graphs  for which we derive a generating
function. The result of our calculation is a quartic equation which is satisﬁed by the trace of the
resolvent of M  G(z) = E[tr(M  zI)1]. It depends on two parameters that together capture the
only relevant properties of the nonlinearity f: ⌘  the Gaussian mean of f 2  and ⇣  the square of the
Gaussian mean of f0. Overall  the techniques presented here pave the way for studying other types of
nonlinear random matrices relevant for the theoretical understanding of neural networks.

1.2 Applications of our results

We show that the training loss of a ridge-regularized single-layer random-feature least-squares
memorization problem with regularization parameter  is related to 2G0(). We observe
increased memorization capacity for certain types of nonlinearities relative to others. In particular 
for a ﬁxed value of   the training loss is lower if ⌘/⇣ is large  a condition satisﬁed by a large class of
activation functions  for example when f is close to an even function. We believe this observation
could have an important practical impact in designing next-generation activation functions.
We also examine the eigenvalue density of M and observe that if ⇣ = 0 the distribution collapses to
the Marchenko-Pastur distribution (Marˇcenko & Pastur  1967)  which describes the eigenvalues of the
Wishart matrix X T X. We therefore make the surprising observation that there exist functions f such
that f (W X) has the same singular value distribution as X. Said another way  the eigenvalues of the
data covariance matrix are unchanged in distribution after passing through a single nonlinear layer
of the network. We conjecture that this property is actually satisﬁed through arbitrary layers of the
network  and ﬁnd supporting numerical evidence. This conjecture may be regarded as a claim about
the universality of our results with respect to the distribution of X. Note that preserving the ﬁrst
moment of this distribution is also an effect achieved through batch normalization (Ioffe & Szegedy 
2015)  although higher moments are not necessarily preserved. We therefore offer the hypothesis that
choosing activation functions with ⇣ = 0 might lead to improved training performance  in the same
way that batch normalization does  at least early in training.

2

1.3 Related work

The study of random neural networks has a relatively long history  with much of the initial work
focusing on approaches from statistical physics and the theory of spin glasses. For example  Amit
et al. (1985) analyze the long-time behavior of certain dynamical models of neural networks in terms
of an Ising spin-glass Hamiltonian  and Gardner & Derrida (1988) examine the storage capacity of
neural networks by studying the density of metastable states of a similar spin-glass system. More
recently  Choromanska et al. (2015) studied the critical points of random loss surfaces  also by
examining an associated spin-glass Hamiltonian  and Schoenholz et al. (2017) developed an exact
correspondence between random neural networks and statistical ﬁeld theory.
In a somewhat tangential direction  random neural networks have also been investigated through their
relationship to kernel methods. The correspondence between inﬁnite-dimensional neural networks
and Gaussian processes was ﬁrst noted by Neal (1994a b). In the ﬁnite-dimensional setting  the
approximate correspondence to kernel methods led to the development random feature methods that
can accelerate the training of kernel machines (Rahimi & Recht  2007). More recently  a duality
between random neural networks with general architectures and compositional kernels was explored
by Daniely et al. (2016).
In the last several years  random neural networks have been studied from many other perspectives.
Saxe et al. (2014) examined the effect of random initialization on the dynamics of learning in deep
linear networks. Schoenholz et al. (2016) studied how information propagates through random
networks  and how that affects learning. Poole et al. (2016) and Raghu et al. (2016) investigated
various measures of expressivity in the context of deep random neural networks.
Despite this extensive literature related to random neural networks  there has been relatively little
research devoted to studying random matrices with nonlinear dependencies. The main focus in this
direction has been kernel random matrices and robust statistics models (El Karoui et al.  2010; Cheng
& Singer  2013). In a closely-related contemporaneous work  Louart et al. (2017) examined the
resolvent of Gram matrix Y Y T in the case where X is deterministic.

2 Preliminaries

Throughout this work we will be relying on a number of basic concepts from random matrix theory.
Here we provide a lightning overview of the essentials  but refer the reader to the more pedagogical
literature for background (Tao  2012).

2.1 Notation
Let X 2 Rn0⇥m be a random data matrix with i.i.d. elements Xiµ ⇠N (0  2
x) and W 2 Rn1⇥n0 be
w/n0). As discussed in Section 1  we are
a random weight matrix with i.i.d. elements Wij ⇠N (0  2
interested in the regime in which both the row and column dimensions of these matrices are large and
approach inﬁnity at the same rate. In particular  we deﬁne
n0
n1

  ⌘

(1)

 ⌘

n0
m

 

to be ﬁxed constants as n0  n1  m ! 1. In what follows  we will frequently consider the limit that
n0 ! 1 with the understanding that n1 ! 1 and m ! 1  so that eqn. (1) is satisﬁed.
We denote the matrix of pre-activations by Z = W X. Let f : R!R be a function with
zero mean and ﬁnite moments 

Z

dz
p2⇡

e z2

2 f (wxz) = 0 

Z

and denote the matrix of post-activations Y = f (Z)  where f is applied pointwise. We will be
interested in the Gram matrix 

dz
p2⇡

e z2

2 f (wxz)k < 1 for k > 1  

(2)

M =

1
m

Y Y T 2 Rn1⇥n1 .

3

(3)

2.2 Spectral density and the Stieltjes transform

The empirical spectral density of M is deﬁned as 

⇢M (t) =

1
n1

n1Xj=1

 (t  j(M ))  

(4)

where  is the Dirac delta function  and the j(M )  j = 1  . . .   n1  denote the n1 eigenvalues of M 
including multiplicity. The limiting spectral density is deﬁned as the limit of eqn. (4) as n1 ! 1  if
it exists.
For z 2 C \ supp(⇢M ) the Stieltjes transform G of ⇢M is deﬁned as 

G(z) =Z ⇢M (t)

z  t

dt = 

1
n1

E⇥tr(M  zIn1)1⇤  

where the expectation is with respect to the random variables W and X. The quantity (M  zIn1)1
is the resolvent of M. The spectral density can be recovered from the Stieltjes transform using the
inversion formula 

1
⇡

Im G( + i✏) .

lim
✏!0+

(6)

⇢M () = 

2.3 Moment method

One of the main tools for computing the limiting spectral distributions of random matrices is the
moment method  which  as the name suggests  is based on computations of the moments of ⇢M. The
asymptotic expansion of eqn. (5) for large z gives the Laurent series 

(5)

(7)

(8)

(9)

where mk is the kth moment of the distribution ⇢M 

mk
zk+1  

G(z) =

1Xk=0
mk =Z dt ⇢M (t)tk =
E24 Xi1 ... ik2[n1]

1
n1

1
n1

E⇥tr M k⇤ .
Mi1i2Mi2i3 ··· Mik1ik Miki135  

1
n1

E⇥tr M k⇤ =

If one can compute mk  then the density ⇢M can be obtained via eqns. (7) and (6). The idea behind
the moment method is to compute mk by expanding out powers of M inside the trace as 

and evaluating the leading contributions to the sum as the matrix dimensions go to inﬁnity  i.e. as
n0 ! 1. Determining the leading contributions involves a complicated combinatorial analysis 
combined with the evaluation of certain nontrivial high-dimensional integrals. In the next section and
the supplementary material  we provide an outline for how to tackle these technical components of
the computation.

3 The Stieltjes transform of M

3.1 Main result

The following theorem characterizes G as the solution to a quartic polynomial equation.
Theorem 1. For M      w  and x deﬁned as in Section 2.1  and constants ⌘ and ⇣ deﬁned as 

⌘ =Z dz

ez2/2
p2⇡

f (wxz)2

and

⇣ ="wxZ dz

ez2/2
p2⇡

f0(wxz)#2

 

(10)

4

the Stieltjes transform of the spectral density of M satisﬁes 

where 

and

G(z) =

 
z

P✓ 1
z ◆ +

1  

z

 

P = 1 + (⌘  ⇣)tPP +

PP t⇣
1  PP t⇣

 

P = 1 + (P  1)  P = 1 + (P  1) .

(11)

(12)

(13)

The proof of Theorem 1 is relatively long and complicated  so it’s deferred to the supplementary
material. The main idea underlying the proof is to translate the calculation of the moments in
eqn. (7) into two subproblems  one of enumerating certain connected outer-planar graphs  and another
of evaluating integrals that correspond to cycles in those graphs. The complexity resides both in
characterizing which outer-planar graphs contribute at leading order to the moments  and also in
computing those moments explicitly. A generating function encapsulating these results (P from
Theorem 1) is shown to satisfy a relatively simple recurrence relation. Satisfying this recurrence
relation requires that P solve eqn. (12). Finally  some bookkeeping relates G to P .

⌘ = ⇣

3.2 Limiting cases
3.2.1
In Section 3 of the supplementary material  we use a Hermite polynomial expansion of f to show that
⌘ = ⇣ if and only if f is a linear function. In this case  M = ZZT   where Z = W X is a product of
Gaussian random matrices. Therefore we expect G to reduce to the Stieltjes transform of a so-called
product Wishart matrix. In (Dupic & Castillo  2014)  a cubic equation deﬁning the Stieltjes transform
of such matrices is derived. Although eqn. (11) is generally quartic  the coefﬁcient of the quartic term
vanishes when ⌘ = ⇣ (see Section 4 of the supplementary material). The resulting cubic polynomial
is in agreement with the results in (Dupic & Castillo  2014).

⇣ = 0

3.2.2
Another interesting limit is when ⇣ = 0  which signiﬁcantly simpliﬁes the expression in eqn. (12).
Without loss of generality  we can take ⌘ = 1 (the general case can be recovered by rescaling z). The
resulting equation is 

z G2 +⇣1 

 

z  1⌘G +

 


= 0  

(14)

which is precisely the equation satisﬁed by the Stieltjes transform of the Marchenko-Pastur distribution
with shape parameter / . Notice that when = 1  the latter is the limiting spectral distribution of
XX T   which implies that Y Y T and XX T have the same limiting spectral distribution. Therefore we
have identiﬁed a novel type of isospectral nonlinear transformation. We investigate this observation
in Section 4.1.

4 Applications

4.1 Data covariance
Consider a deep feedforward neural network with lth-layer post-activation matrix given by 

Y 0 = X .

Y l = f (W lY l1) 

(15)
The matrix Y l(Y l)T is the lth-layer data covariance matrix. The distribution of its eigenvalues (or the
singular values of Y l) determine the extent to which the input signals become distorted or stretched
as they propagate through the network. Highly skewed distributions indicate strong anisotropy in
the embedded feature space  which is a form of poor conditioning that is likely to derail or impede
learning. A variety of techniques have been developed to alleviate this problem  the most popular of
which is batch normalization. In batch normalization  the variance of individual activations across the
batch (or dataset) is rescaled to equal one. The covariance is often ignored — variants that attempt to

5

(a) L = 1

(b) L = 10

Figure 1: Distance between the (a) ﬁrst-layer and (b) tenth-layer empirical eigenvalue distributions
of the data covariance matrices and our theoretical prediction for the ﬁrst-layer limiting distribution
¯⇢1  as a function of network width n0. Plots are for shape parameters  = 1 and = 3/2. The
different curves correspond to different piecewise linear activation functions parameterize by ↵:
↵ = 1 is linear  ↵ = 0 is (shifted) relu  and ↵ = 1 is (shifted) absolute value. In (a)  for all ↵  we
see good convergence of the empirical distribution ⇢1 to our asymptotic prediction ¯⇢1. In (b)  in
accordance with our conjecture  we ﬁnd good agreement between ¯⇢1 and the tenth-layer empirical
distribution ⇣ = 0  but not for other values of ⇣. This provides evidence that when ⇣ = 0 the
eigenvalue distribution is preserved by the nonlinear transformations.

fully whiten the activations can be very slow. So one aspect of batch normalization  as it is used in
practice  is that it preserves the trace of the covariance matrix (i.e. the ﬁrst moment of its eigenvalue
distribution) as the signal propagates through the network  but it does not control higher moments of
the distribution. A consequence is that there may still be a large imbalance in singular values.
An interesting question  therefore  is whether there exist efﬁcient techniques that could preserve or
approximately preserve the full singular value spectrum of the activations as they propagate through
the network. Inspired by the results of Section 3.2.2  we hypothesize that choosing an activation
function with ⇣ = 0 may be one way to approximately achieve this behavior  at least early in training.
From a mathematical perspective  this hypothesis is similar to asking whether our results in eqn. (11)
are universal with respect to the distribution of X. We investigate this question empirically.
Let ⇢l be the empirical eigenvalue density of Y l(Y l)T   and let ¯⇢1 be the limiting density determined
by eqn. (11) (with = 1). We would like to measure the distance between ¯⇢1 and ⇢l in order to
see whether the eigenvalues propagate without getting distorted. There are many options that would
sufﬁce  but we choose to track the following metric 

d(¯⇢1 ⇢ l) ⌘Z d|¯⇢1()  ⇢l()| .

f↵(x) =

[x]+ + ↵[x]+  1+↵p2⇡
q 1
2 (1 + ↵2)  1

2⇡ (1 + ↵)2

⌘ = 1 ⇣ =

(1  ↵)2
2(1 + ↵2)  2

⇡ (1 + ↵)2  

6

(16)

(18)

To observe the effect of varying ⇣  we utilize a variant of the relu activation function with non-zero
slope for negative inputs 

.

(17)

One may interpret ↵ as (the negative of) the ratio of the slope for negative x to the slope for positive
x. It is straightforward to check that f↵ has zero Gaussian mean and that 

so we can adjust ⇣ (without affecting ⌘) by changing ↵. Fig. 1(a) shows that for any value of ↵ (and
thus ⇣) the distance between ¯⇢1 and ⇢1 approaches zero as the network width increases. This offers

(cid:1)=1((cid:2)=0)(cid:1)=1/4((cid:2)=0.498)(cid:1)=0((cid:2)=0.733)(cid:1)=-1/4((cid:2)=0.884)(cid:1)=-1((cid:2)=1)51050100500100050000.0050.0100.0500.1000.5001n0d((cid:3)1 (cid:3)1)(cid:1)=1((cid:2)=0)(cid:1)=1/4((cid:2)=0.498)(cid:1)=0((cid:2)=0.733)(cid:1)=-1/4((cid:2)=0.884)(cid:1)=-1((cid:2)=1)51050100500100050000.0050.0100.0500.1000.5001n0d((cid:3)1 (cid:3)10)(a)  = 1

2   = 1

2

(b)  = 1

2   = 3

4

Figure 2: Memorization performance of random feature networks versus ridge regularization pa-
rameter . Theoretical curves are solid lines and numerical solutions to eqn. (19) are points.
 ⌘ log10(⌘/⇣  1) distinguishes classes of nonlinearities  with  = 1 corresponding to a
linear network. Each numerical simulation is done with a different randomly-chosen function f and
the speciﬁed . The good agreement conﬁrms that no details about f other than  are relevant. In
(a)  there are more random features than data points  allowing for perfect memorization unless the
function f is linear  in which case the model is rank constrained. In (b)  there are fewer random
features than data points  and even the nonlinear models fail to achieve perfect memorization. For
a ﬁxed amount of regularization   curves with larger values of  (smaller values of ⇣) have lower
training loss and hence increased memorization capacity.

numerical evidence that eqn. (11) is in fact the correct asymptotic limit. It also shows how quickly
the asymptotic behavior sets in  which is useful for interpreting Fig. 1(b)  which shows the distance
between ¯⇢1 and ⇢10. Observe that if ⇣ = 0  ⇢10 approaches ¯⇢1 as the network width increases. This
provides evidence for the conjecture that the eigenvalues are in fact preserved as they propagate
through the network  but only when ⇣ = 0  since we see the distances level off at some ﬁnite value
when ⇣ 6= 0. We also note that small non-zero values of ⇣ may not distort the eigenvalues too much.
These observations suggest a new method of tuning the network for fast optimization. Re-
cent work (Pennington et al.  2017) found that inducing dynamical isometry  i.e. equilibrating the
singular value distribution of the input-output Jacobian  can greatly speed up training. In our context 
by choosing an activation function with ⇣ ⇡ 0  we can induce a similar type of isometry  not of the
input-output Jacobian  but of the data covariance matrix as it propagates through the network. We
conjecture that inducing this additional isometry may lead to further training speed-ups  but we leave
further investigation of these ideas to future work.

4.2 Asymptotic performance of random feature methods
Consider the ridge-regularized least squares loss function deﬁned by 

L(W2) =

1
2n2mkY  W T

2 Y k2

F + kW2k2
F  

Y = f (W X)  

(19)

where X 2 Rn0⇥m is a matrix of m n0-dimensional features  Y2 Rn2⇥m is a matrix of regression
targets  W 2 Rn1⇥n0 is a matrix of random weights and W2 2 Rn1⇥n2 is a matrix of parameters to
be learned. The matrix Y is a matrix of random features1. The optimal parameters are 

W ⇤2 =

1
m

Y QY T   Q =✓ 1

m

Y T Y + Im◆1

.

(20)

1We emphasize that we are using an unconvential notation for the random features – we call them Y in order

to make contact with the previous sections.

7

β=-∞β=-8β=-6β=-4β=-2β=0β=2-8-6-4-20240.00.20.40.60.81.0log10(γ/η)Etrainβ=-∞β=-8β=-6β=-4β=-2β=0β=2-8-6-4-20240.00.20.40.60.81.0log10(γ/η)EtrainOur problem setup and analysis are similar to that of (Louart et al.  2017)  but in contrast to that work 
we are interested in the memorization setting in which the network is trained on random input-output
pairs. Performance on this task is then a measure of the capacity of the model  or the complexity of
the function class it belongs to. In this context  we take the data X and the targets Y to be independent
Gaussian random matrices. From eqns. (19) and (20)  the expected training loss is given by 

(21)

Etrain = EW X Y [L(W ⇤2 )] = EW X Y 2
= EW X 2

m

m

trY TYQ2
tr Q2
@
@ EW X [tr Q] .

2
m

= 

It is evident from eqn. (5) and the deﬁnition of Q that EW X [tr Q] is related to G(). However  our
results from the previous section cannot be used directly because Q contains the trace Y T Y   whereas
G was computed with respect to Y Y T . Thankfully  the two matrices differ only by a ﬁnite number of
zero eigenvalues. Some simple bookkeeping shows that
(1  / )

(22)

1
mEW X [tr Q] =




 



G() .

From eqn. (11) and its total derivative with respect to z  an equation for G0(z) can be obtained by
computing the resultant of the two polynomials and eliminating G(z). An equation for Etrain follows;
see Section 4 of the supplementary material for details. An analysis of this equation shows that it is
homogeneous in   ⌘  and ⇣  i.e.  for any > 0 

Etrain(  ⌘  ⇣ ) = Etrain(  ⌘  ⇣) .

(23)
In fact  this homogeneity is entirely expected from eqn. (19): an increase in the regularization constant
 can be compensated by a decrease in scale of W2  which  in turn  can be compensated by increasing
the scale of Y   which is equivalent to increasing ⌘ and ⇣. Owing to this homogeneity  we are free to
choose  = 1/⌘. For simplicity  we set ⌘ = 1 and examine the two-variable function Etrain(  1 ⇣ ).
The behavior when  = 0 is a measure of the capacity of the model with no regularization and
depends on the value of ⇣ 

Etrain(0  1 ⇣ ) =⇢[1  ]+

[1  / ]+ otherwise.

if ⇣ = 1 and < 1 

(24)

As discussed in Section 3.2  when ⌘ = ⇣ = 1  the function f reduces to the identity. With this in
mind  the various cases in eqn. (24) are readily understood by considering the effective rank of the
random feature matrix Y.
In Fig. 2  we compare our theoretical predictions for Etrain to numerical simulations of solutions
to eqn. (19). The different curves explore various ratios of  ⌘ log10(⌘/⇣  1) and therefore
probe different classes of nonlinearities. For each numerical simulation  we choose a random
quintic polynomial f with the speciﬁed value of  (for details on this choice  see Section 3 of the
supplementary material). The excellent agreement between theory and simulations conﬁrms that
Etrain depends only on  and not on any other details of f. The black curves correspond to the
performance of a linear network. The results show that for ⇣ very close to ⌘  the models are unable to
utilize their nonlinearity unless the regularization parameter is very small. Conversely  for ⇣ close to
zero  the models exploits the nonlinearity very efﬁciently and absorb large amounts of regularization
without a signiﬁcant drop in performance. This suggests that small ⇣ might provide an interesting
class of nonlinear functions with enhanced expressive power. See Fig. 3 for some examples of
activation functions with this property.

5 Conclusions

In this work we studied the Gram matrix M = 1
m Y T Y   where Y = f (W X) and W and X are
random Gaussian matrices. We derived a quartic polynomial equation satisﬁed by the trace of the
resolvent of M  which deﬁnes its limiting spectral density. In obtaining this result  we demonstrated

8

Figure 3: Examples of activation functions and their derivatives for which ⌘ = 1 and ⇣ = 0. In

red  f (1) = c1  1 + p5e2x2; in green  f (2)(x) = c2 sin(2x) + cos(3x/2)  2e2x  e9/8;
in orange  f (3)(x) = c3|x| p2/⇡; and in blue  f (4)(x) = c41  4p3
2 erf(x). If we let

w = x = 1  then eqn. (2) is satisﬁed and ⇣ = 0 for all cases. We choose the normalization
constants ci so that ⌘ = 1.

f (1)(x)
f (2)(x)
f (3)(x)
f (4)(x)

e x2

that pointwise nonlinearities can be incorporated into a standard method of proof in random matrix
theory known as the moments method  thereby opening the door for future study of other nonlinear
random matrices appearing in neural networks.
We applied our results to a memorization task in the context of random feature methods and obtained
an explicit characterizations of the training error as a function of a ridge regression parameter. The
training error depends on the nonlinearity only through two scalar quantities  ⌘ and ⇣  which are
certain Gaussian integrals of f. We observe that functions with small values of ⇣ appear to have
increased capacity relative to those with larger values of ⇣.
We also make the surprising observation that for ⇣ = 0  the singular value distribution of f (W X) is
the same as the singular value distribution of X. In other words  the eigenvalues of the data covariance
matrix are constant in distribution when passing through a single nonlinear layer of the network.
We conjectured and found numerical evidence that this property actually holds when passing the
signal through multiple layers. Therefore  we have identiﬁed a class of activation functions that
maintains approximate isometry at initialization  which could have important practical consequences
for training speed.
Both of our applications suggest that functions with ⇣ ⇡ 0 are a potentially interesting class of
activation functions. This is a large class of functions  as evidenced in Fig. 3  among which are many
types of nonlinearities that have not been thoroughly explored in practical applications. It would be
interesting to investigate these nonlinearities in future work.

References
Amit  Daniel J  Gutfreund  Hanoch  and Sompolinsky  Haim. Spin-glass models of neural networks.

Physical Review A  32(2):1007  1985.

Cheng  Xiuyuan and Singer  Amit. The spectrum of random inner-product kernel matrices. Random

Matrices: Theory and Applications  2(04):1350010  2013.

Choromanska  Anna  Henaff  Mikael  Mathieu  Michael  Arous  Gérard Ben  and LeCun  Yann. The

loss surfaces of multilayer networks. In AISTATS  2015.

Daniely  A.  Frostig  R.  and Singer  Y. Toward Deeper Understanding of Neural Networks: The

Power of Initialization and a Dual View on Expressivity. arXiv:1602.05897  2016.

Dupic  Thomas and Castillo  Isaac Pérez. Spectral density of products of wishart dilute random

matrices. part i: the dense case. arXiv preprint arXiv:1401.7802  2014.

El Karoui  Noureddine et al. The spectrum of kernel random matrices. The Annals of Statistics  38

(1):1–50  2010.

Gardner  E and Derrida  B. Optimal storage properties of neural network models. Journal of Physics

A: Mathematical and general  21(1):271  1988.

9

Hinton  Geoffrey  Deng  Li  Yu  Dong  Dahl  George E.  Mohamed  Abdel-rahman  Jaitly  Navdeep 
Senior  Andrew  Vanhoucke  Vincent  Nguyen  Patrick  Sainath  Tara N  et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine  29(6):82–97  2012.

Ioffe  Sergey and Szegedy  Christian. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine
Learning  pp. 448–456  2015.

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey E.

Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pp. 1097–
1105  2012.

Louart  Cosme  Liao  Zhenyu  and Couillet  Romain. A random matrix approach to neural networks.

arXiv preprint arXiv:1702.05419  2017.

Marˇcenko  Vladimir A and Pastur  Leonid Andreevich. Distribution of eigenvalues for some sets of

random matrices. Mathematics of the USSR-Sbornik  1(4):457  1967.

Neal  Radford M. Priors for inﬁnite networks (tech. rep. no. crg-tr-94-1). University of Toronto 

1994a.

Neal  Radford M. Bayesian Learning for Neural Networks. PhD thesis  University of Toronto  Dept.

of Computer Science  1994b.

Oord  Aaron van den  Dieleman  Sander  Zen  Heiga  Simonyan  Karen  Vinyals  Oriol  Graves  Alex 
Kalchbrenner  Nal  Senior  Andrew  and Kavukcuoglu  Koray. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499  2016.

Pennington  J  Schoenholz  S  and Ganguli  S. Resurrecting the sigmoid in deep learning through
dynamical isometry: theory and practice. In Advances in neural information processing systems 
2017.

Poole  B.  Lahiri  S.  Raghu  M.  Sohl-Dickstein  J.  and Ganguli  S. Exponential expressivity in deep

neural networks through transient chaos. arXiv:1606.05340  June 2016.

Raghu  M.  Poole  B.  Kleinberg  J.  Ganguli  S.  and Sohl-Dickstein  J. On the expressive power of

deep neural networks. arXiv:1606.05336  June 2016.

Rahimi  Ali and Recht  Ben. Random features for large-scale kernel machines.

Infomration Processing Systems  2007.

In In Neural

Saxe  A. M.  McClelland  J. L.  and Ganguli  S. Exact solutions to the nonlinear dynamics of learning

in deep linear neural networks. International Conference on Learning Representations  2014.

Schoenholz  S. S.  Gilmer  J.  Ganguli  S.  and Sohl-Dickstein  J. Deep Information Propagation.

ArXiv e-prints  November 2016.

Schoenholz  S. S.  Pennington  J.  and Sohl-Dickstein  J. A Correspondence Between Random Neural

Networks and Statistical Field Theory. ArXiv e-prints  2017.

Shazeer  N.  Mirhoseini  A.  Maziarz  K.  Davis  A.  Le  Q.  Hinton  G.  and Dean  J. Outrageously
ICLR  2017. URL

large neural language models using sparsely gated mixtures of experts.
http://arxiv.org/abs/1701.06538.

Tao  Terence. Topics in random matrix theory  volume 132. American Mathematical Society

Providence  RI  2012.

Wu  Yonghui  Schuster  Mike  Chen  Zhifeng  Le  Quoc V.  Norouzi  Mohammad  Macherey 
Wolfgang  Krikun  Maxim  Cao  Yuan  Gao  Qin  Macherey  Klaus  et al. Google’s neural
machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144  2016.

10

,Jeffrey Pennington
Pratik Worah