2015,Competitive Distribution Estimation: Why is Good-Turing Good,Estimating distributions over large alphabets is a fundamental machine-learning tenet. Yet no method is known to estimate all distributions well.  For example  add-constant estimators are nearly min-max optimal but often perform poorly in practice  and practical estimators such as absolute discounting  Jelinek-Mercer  and Good-Turing are not known to be near optimal for essentially any distribution.We describe the first universally near-optimal probability estimators. For every discrete distribution  they are provably nearly the best in the following two competitive ways. First they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation. Second  they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution  but as all natural estimators  restricted to assign the same probability to all symbols appearing the same number of times.Specifically  for distributions over $k$ symbols and $n$ samples  we show that for both comparisons  a simple variant of Good-Turing estimator is always within KL divergence of $(3+o(1))/n^{1/3}$ from the best estimator  and that a more involved estimator is within $\tilde{\mathcal{O}}(\min(k/n 1/\sqrt n))$.  Conversely  we show that any estimator must have a KL divergence $\ge\tilde\Omega(\min(k/n 1/ n^{2/3}))$ over the best estimator for the first comparison  and $\ge\tilde\Omega(\min(k/n 1/\sqrt{n}))$ for the second.,Competitive Distribution Estimation:

Why is Good-Turing Good

Alon Orlitsky
UC San Diego

alon@ucsd.edu

Abstract

Ananda Theertha Suresh

UC San Diego

asuresh@ucsd.edu

Estimating distributions over large alphabets is a fundamental machine-learning
tenet. Yet no method is known to estimate all distributions well. For example 
add-constant estimators are nearly min-max optimal but often perform poorly in
practice  and practical estimators such as absolute discounting  Jelinek-Mercer 
and Good-Turing are not known to be near optimal for essentially any distribution.
We describe the ﬁrst universally near-optimal probability estimators. For every
discrete distribution  they are provably nearly the best in the following two com-
petitive ways. First they estimate every distribution nearly as well as the best
estimator designed with prior knowledge of the distribution up to a permutation.
Second  they estimate every distribution nearly as well as the best estimator de-
signed with prior knowledge of the exact distribution  but as all natural estimators 
restricted to assign the same probability to all symbols appearing the same number
of times.
Speciﬁcally  for distributions over k symbols and n samples  we show that for
both comparisons  a simple variant of Good-Turing estimator is always within KL
divergence of (3 + on(1))/n1/3 from the best estimator  and that a more involved
estimator is within ˜On(min(k/n  1/
n)). Conversely  we show that any esti-
mator must have a KL divergence at least ˜Ωn(min(k/n  1/n2/3)) over the best
√
estimator for the ﬁrst comparison  and at least ˜Ωn(min(k/n  1/
n)) for the sec-
ond.

√

1

Introduction

1.1 Background

Many learning applications  ranging from language-processing staples such as speech recognition
and machine translation to biological studies in virology and bioinformatics  call for estimating large
discrete distributions from their samples. Probability estimation over large alphabets has therefore
long been the subject of extensive research  both by practitioners deriving practical estimators [1  2] 
and by theorists searching for optimal estimators [3].
Yet even after all this work  provably-optimal estimators remain elusive. The add-constant esti-
mators frequently analyzed by theoreticians are nearly min-max optimal  yet perform poorly for
many practical distributions  while common practical estimators  such as absolute discounting [4] 
Jelinek-Mercer [5]  and Good-Turing [6]  are not well understood and lack provable performance
guarantees.
To understand the terminology and approach a solution we need a few deﬁnitions. The performance
of an estimator q for an underlying distribution p is typically evaluated in terms of the Kullback-

1

Leibler (KL) divergence [7] 

D(p||q)

def
=

(cid:88)

x

px log

px
qx

 

reﬂecting the expected increase in the ambiguity about the outcome of p when it is approximated by
q. KL divergence is also the increase in the number of bits over the entropy that q uses to compress
the output of p  and is also the log-loss of estimating p by q. It is therefore of interest to construct
estimators that approximate a large class of distributions to within small KL divergence. We now
describe one of the problem’s simplest formulations.

1.2 Min-max loss
A distribution estimator over a support set X associates with any observed sample sequence x∗ ∈
X ∗ a distribution q(x∗) over X . Given n samples X n def
= X1  X2  . . .   Xn  generated independently
according to a distribution p over X   the expected KL loss of q is
[D(p||q(X n))].

rn(q  p) = E

X n∼pn

Let P be a known collection of distributions over a discrete set X . The worst-case loss of an
estimator q over all distributions in P is

rn(q P)

def
= max

p∈P rn(q  p) 

and the lowest worst-case loss for P  achieved by the best estimator  is the min-max loss

rn(P)

def
= min

q

rn(q P) = min

q

max
p∈P rn(q  p).

(1)

(2)

Min-max performance can be viewed as regret relative to an oracle that knows the underlying dis-
tribution. Hence from here on we refer to it as regret.
The most natural and important collection of distributions  and the one we study here  is the set
of all discrete distributions over an alphabet of some size k  which without loss of generality we
assume to be [k] = {1  2  . . . k}. Hence the set of all distributions is the simplex in k dimensions 
∆k
related quantities  for example see [9]. We outline some of the results derived.

= {(p1  . . .   pk) : pi ≥ 0 and (cid:80) pi = 1}. Following [8]  researchers have studied rn(∆k) and

def

1.3 Add-constant estimators

The add-β estimator assigns to a symbol that appeared t times a probability proportional to t+β. For
example  if three coin tosses yield one heads and two tails  the add-1/2 estimator assigns probability
1.5/(1.5 + 2.5) = 3/8 to heads  and 2.5/(1.5 + 2.5) = 5/8 to tails. [10] showed that as for every
k  as n → ∞  an estimator related to add-3/4 is near optimal and achieves

rn(∆k) =

k − 1
2n

· (1 + o(1)).

(3)

The more challenging  and practical  regime is where the sample size n is not overwhelmingly larger
than the alphabet size k. For example in English text processing  we need to estimate the distribution
of words following a context. But the number of times a context appears in a corpus may not be
much larger than the vocabulary size. Several results are known for other regimes as well. When the
sample size n is linear in the alphabet size k  rn(∆k) can be shown to be a constant  and [3] showed
that as k/n → ∞  add-constant estimators achieve the optimal
· (1 + o(1)) 

rn(∆k) = log

(4)

k
n

While add-constant estimators are nearly min-max optimal  the distributions attaining the min-max
regret are near uniform. In practice  large-alphabet distributions are rarely uniform  and instead  tend
to follow a power-law. For these distributions  add-constant estimators under-perform the estimators
described in the next subsection.

2

1.4 Practical estimators

For real applications  practitioners tend to use more sophisticated estimators  with better empirical
performance. These include the Jelinek-Mercer estimator that cross-validates the sample to ﬁnd the
best ﬁt for the observed data. Or the absolute-discounting estimators that rather than add a positive
constant to each count  do the opposite  and subtract a positive constant.
Perhaps the most popular and enduring have been the Good-Turing estimator [6] and some of its
def
variations. Let nx
= ϕt(xn)
be the number of symbols appearing t times in xn. The basic Good-Turing estimator posits that if
nx = t 

def
= nx(xn) be the number of times a symbol x appears in xn and let ϕt

qx(xn) =

ϕt+1
ϕt

· t + 1
n

 

surprisingly relating the probability of an element not just to the number of times it was observed 
but also to the number other elements appearing as many  and one more  times. It is easy to see
that this basic version of the estimator may not work well  as for example it assigns any element
appearing ≥ n/2 times 0 probability. Hence in practice the estimator is modiﬁed  for example 
using empirical frequency to elements appearing many times.
The Good-Turing Estimator was published in 1953  and quickly adapted for language-modeling
use  but for half a century no proofs of its performance were known. Following [11]  several papers 
e.g.  [12  13]  showed that Good-Turing variants estimate the combined probability of symbols
appearing any given number of times with accuracy that does not depend on the alphabet size  and
[14] showed that a different variation of Good-Turing similarly estimates the probabilities of each
previously-observed symbol  and all unseen symbols combined.
However  these results do not explain why Good-Turing estimators work well for the actual proba-
bility estimation problem  that of estimating the probability of each element  not of the combination
of elements appearing a certain number of times. To deﬁne and derive uniformly-optimal estimators 
we take a different  competitive  approach.

2 Competitive optimality

2.1 Overview

To evaluate an estimator  we compare its performance to the best possible performance of two es-
timators designed with some prior knowledge of the underlying distribution. The ﬁrst estimator is
designed with knowledge of the underlying distribution up to a permutation of the probabilities 
namely knowledge of the probability multiset  e.g.  {.5  .3  .2}  but not of the association between
probabilities and symbols. The second estimator is designed with exact knowledge of the distribu-
tion  but like all natural estimators  forced to assign the same probabilities to symbols appearing the
same number of times. For example  upon observing the sample a  b  c  a  b  d  e  the estimator must
assign the same probability to a and b  and the same probability to c  d  and e.
These estimators cannot be implemented in practice as in reality we do not have prior knowledge
of the estimated distribution. But the prior information is chosen to allow us to determine the best
performance of any estimator designed with that information  which in turn is better than the perfor-
mance of any data-driven estimator designed without prior information. We then show that certain
variations of the Good-Turing estimators  designed without any prior knowledge  approach the per-
formance of both prior-knowledge estimators for every underlying distribution.

2.2 Competing with near full information

We ﬁrst deﬁne the performance of an oracle-aided estimator  designed with some knowledge of the
underlying distribution. Suppose that the estimator is designed with the aid of an oracle that knows
the value of f (p) for some given function f over the class ∆k of distributions.
The function f partitions ∆k into subsets  each corresponding to one possible value of f. We denote
the subsets by P   and the partition by P  and as before  denote the individual distributions by p.
Then the oracle knows the unique partition part P such that p ∈ P ∈ P. For example  if f (p) is

3

the multiset of p  then each subset P corresponds to set of distributions with the same probability
multiset  and the oracle knows the multiset of probabilities.
For every partition part P ∈ P  an estimator q incurs the worst-case regret in (1) 

The oracle  knowing the unique partition part P   incurs the least worst-case regret (2) 

rn(q  P ) = max
p∈P

rn(q  p).

rn(P ) = min

q

rn(q  P ).

The competitive regret of q over the oracle  for all distributions in P is

rn(q  P ) − rn(P ) 

the competitive regret over all partition parts and all distributions in each is

P
r
n(q  ∆k)

def
= max

P∈P (rn(q  P ) − rn(P ))  

and the best possible competitive regret is

P
r
n(∆k)

Consolidating the intermediate deﬁnitions 

P
r
n(q  ∆k).

def
= min

q

(cid:18)

(cid:19)

P
r
n(∆k) = min

q

max
P∈P

max
p∈P

rn(q  p) − rn(P )

.

Namely  an oracle-aided estimator who knows the partition part incurs a worst-case regret rn(P )
P
over each part P   and the competitive regret r
n(∆k) of data-driven estimators is the least overall
increase in the part-wise regret due to not knowing P . In Appendix A.1  we give few examples of
such partitions.
A partition P(cid:48) reﬁnes a partition P if every part in P is partitioned by some parts in P(cid:48). For example
{{a  b} {c} {d  e}} reﬁnes {{a  b  c} {d  e}}. In Appendix A.2  we show that if P(cid:48) reﬁnes P then
for every q

n (q  ∆k) ≥ r
P(cid:48)
r

P
n(q  ∆k).

(5)

Considering the collection ∆k of all distributions over [k]  it follows that as we start with single-part
partition {∆k} and keep reﬁning it till the oracle knows p  the competitive regret of estimators will
increase from 0 to rn(q  ∆k). A natural question is therefore how much information can the oracle
have and still keep the competitive regret low? We show that the oracle can know the distribution
exactly up to permutation  and still the regret will be very small.
Two distributions p and p(cid:48) permutation equivalent if for some permutation σ of [k] 

p(cid:48)
σ(i) = pi 

Pσ
n (q  ∆k)  thus the same estimator uniformly bounds r

for all 1 ≤ i ≤ k. For example  (0.5  0.3  0.2) and (0.3  0.5  0.2) are permutation equivalent.
Permutation equivalence is clearly an equivalence relation  and hence partitions the collection of
distributions over [k] into equivalence classes. Let Pσ be the corresponding partition. We construct
P
estimators q that uniformly bound r
n(q  ∆k)
for any coarser partition of ∆k  such as partitions into classes of distributions with the same support
size  or entropy. Note that the partition Pσ corresponds to knowing the underlying distribution up
Pσ
to permutation  hence r
n (∆k) is the additional KL loss compared to an estimator designed with
knowledge of the underlying distribution up to permutation.
This notion of competitiveness has appeared in several contexts. In data compression it is called
twice-redundancy [15  16  17  18]  while in statistics it is often called adaptive or local min-
max [19  20  21  22  23]  and recently in property testing it is referred as competitive [24  25  26]
√
or instance-by-instance [27]. Subsequent to this work  [28] studied competitive estimation in (cid:96)1
distance  however their regret is poly(1/ log n)  compared to our ˜O(1/

n).

4

2.3 Competing with natural estimators

Our second comparison is with an estimator designed with exact knowledge of p  but forced to be
natural  namely  to assign the same probability to all symbols appearing the same number of times
in the sample. For example  for the observed sample a  b  c  a  b  d  e  the same probability must be
assigned to a and b  and the same probability to c  d  and e. Since data-driven estimators derive all
their knowledge of the distribution from the data  we expect them to be natural.
We compare the regret of data-driven estimators to that of natural oracle-aided estimators. Let Qnat
be the set of all natural estimators. For a distribution p  the lowest regret of a natural estimator 
designed with prior knowledge of p is

rnat
n (p)

def
= min
q∈Qnat

rn(q  p) 

and the regret of an estimator q relative to the least-regret natural-estimator is

n (q  p) = rn(q  p) − rnat
rnat
Thus the regret of an estimator q over all distributions in ∆k is
rnat
n (q  p) 

n (p).

rnat
n (q  ∆k) = max
p∈∆k

and the best possible competitive regret is rnat
In the next section we state the results  showing in particular that rnat
n (∆k) is uniformly bounded. In
Section 5  we outline the proofs  and in Section 4 we describe experiments comparing the perfor-
mance of competitive estimators to that of min-max motivated estimators.

n (∆k) = minq rnat

n (q  ∆k).

3 Results

Good-Turing estimators are often used in conjunction with empirical frequency  where Good-Turing
estimates low probabilities and empirical frequency estimates large probabilities. We ﬁrst show that
even this simple Good-Turing version  deﬁned in Appendix C and denoted q(cid:48)  is uniformly optimal
for all distributions. For simplicity we prove the result when the number of samples is n(cid:48) ∼ poi(n) 
poi(n)(q(cid:48)  ∆k) be the regrets in this
a Poisson random variable with mean n. Let r
sampling process. A similar result holds with exactly n samples  but the proof is more involved as
the multiplicities are dependent.
Theorem 1 (Appendix C). For any k and n 
poi(n)(q(cid:48)  ∆k) ≤ rnat
Pσ

poi(n)(q(cid:48)  ∆k) ≤ 3 + on(1)

Pσ
poi(n)(q(cid:48)  ∆k) and rnat

r

.

n1/3

Furthermore  a lower bound in [13] shows that this bound is optimal up to logarithmic factors.
A more complex variant of Good-Turing  denoted q(cid:48)(cid:48)  was proposed in [13]. We show that its regret
diminishes uniformly in both the partial-information and natural-estimator formulations.
Theorem 2 (Section 5). For any k and n 
n (q(cid:48)(cid:48)  ∆k) ≤ rnat
Pσ
r

n (q(cid:48)(cid:48)  ∆k) ≤ ˜On

(cid:19)(cid:19)

(cid:18)

min

 

.

k
n

n

Where ˜On  and below also ˜Ωn  hide multiplicative logarithmic factors in n. Lemma 6 in Section 5
and a lower bound in [13] can be combined to prove a matching lower bound on the competitive
regret of any estimator for the second formulation 

(cid:18) 1√
(cid:19)(cid:19)

.

(cid:18)

(cid:18) 1√

k
n

 

n

n (∆k) ≥ ˜Ωn
rnat

min

Hence q(cid:48)(cid:48) has near-optimal competitive regret relative to natural estimators.
Fano’s inequality usually yields lower bounds on KL loss  not regret. By carefully constructing
distribution classes  we lower bound the competitive regret relative to the oracle-aided estimators.
Theorem 3 (Appendix D). For any k and n 
n (∆k) ≥ ˜Ωn
Pσ
r

(cid:18) 1

(cid:19)(cid:19)

(cid:18)

min

 

.

k
n

n2/3

5

3.1

Illustration and implications

Figure 1 demonstrates some of the results. The horizontal axis reﬂects the set ∆k of distributions
illustrated on one dimension. The vertical axis indicates the KL loss  or absolute regret  for clarity 
shown for k (cid:29) n. The blue line is the previously-known min-max upper bound on the regret 
which by (4) is very high for this regime  log(k/n). The red line is the regret of the estimator
designed with prior knowledge of the probability multiset. Observe that while for some probability
multisets the regret approaches the log(k/n) min-max upper bound  for other probability multisets
it is much lower  and for some  such as uniform over 1 or over k symbols  where the probability
multiset determines the distribution it is even 0. For many practically relevant distributions  such
as power-law distributions and sparse distributions  the regret is small compared to log(k/n). The
green line is an upper bound on the absolute regret of the data-driven estimator q(cid:48)(cid:48). By Theorem 2 
√
it is always at most 1/
n larger than the red line. It follows that for many distributions  possibly for
distributions with more structure  such as those occurring in nature  the regret of q(cid:48)(cid:48) is signiﬁcantly
smaller than the pessimistic min-max bound implies.

KL loss

rn(∆k) = log k
n

≤ ˜O(cid:16)

(cid:16) 1√

(cid:17)(cid:17)

min(

n   k

n

Distributions

Uniform distribution

Figure 1: Qualitative behavior of the KL loss as a function of distributions in different formulations

√
diminish to zero at least as fast as 1/n1/3  and 1/
alphabet size k is.

We observe a few consequences of these results.
• Theorems 1 and 2 establish two uniformly-optimal estimators q(cid:48) and q(cid:48)(cid:48). Their relative regrets
n respectively  independent of how large the
• Although the results are for relative regret  as shown in Figure 1  they lead to estimator with
• The same regret upper bounds hold for all coarser partitions of ∆k i.e.  where instead of knowing

smaller absolute regret  namely  the expected KL divergence.

the multiset  the oracle knows some property of multiset such as entropy.

4 Experiments

Recall that for a sequence xn  nx denotes the number of times a symbol x appears and ϕt denotes
the number of symbols appearing t times. For small values of n and k  the estimator proposed
in [13] simpliﬁes to a combination of Good-Turing and empirical estimators. By [13  Lemmas 10
and 11]  for symbols appearing t times  if ϕt+1 ≥ ˜Ω(t)  then the Good-Turing estimate is close
to the underlying total probability mass  otherwise the empirical estimate is closer. Hence  for a
symbol appearing t times  if ϕt+1 ≥ t we use the Good-Turing estimator  otherwise we use the
empirical estimator. If nx = t 

(cid:40) t

qx =

N
ϕt+1+1

ϕt

· t+1

N

if t > ϕt+1 
else 

where N is a normalization factor. Note that we have replaced ϕt+1 in the Good-Turing estimator
by ϕt+1 + 1 to ensure that every symbol is assigned a non-zero probability.

6

(a) Uniform

(b) Step

(c) Zipf with parameter 1

(d) Zipf with parameter 1.5

(e) Uniform prior (Dirichlet 1)

(f) Dirichlet 1/2 prior

Figure 2: Simulation results for support 10000  number of samples ranging from 1000 to 50000 
averaged over 200 trials.

We compare the performance of this estimator to four estimators: three popular add-β estimators
and the optimal natural estimator. An add-beta estimator ˆS has the form

q ˆS
x =

nx + β ˆS
nx
N ( ˆS)

 

1 = 1  βBS

where N ( ˆS) is a normalization factor to ensure that the probabilities add up to 1. The Laplace
t = 1∀ t  minimizes the expected loss when the underlying distribution is generated
estimator  βL
t = 1/2∀ t  is asymptotically
by a uniform prior over ∆k. The Krichevsky-Troﬁmov estimator  βKT
min-max optimal for the cumulative regret  and minimizes the expected loss when the underlying
distribution is generated according to a Dirichlet-1/2 prior. The Braess-Sauer estimator  βBS
0 =
t = 3/4 ∀ t > 1  is asymptotically min-max optimal for rn(∆k). Finally 
1/2  βBS
as shown in Lemma 10  the optimal estimator qx = Snx
achieves the lowest loss of any natural
ϕnx
estimator designed with knowledge of the underlying distribution.
We compare the performance of the proposed estimator to that of the four estimators above. We
consider six distributions: uniform distribution  step distribution with half the symbols having prob-
ability 1/2k and the other half have probability 3/2k  Zipf distribution with parameter 1 (pi ∝ i−1) 
Zipf distribution with parameter 1.5 (pi ∝ i−1.5)  a distribution generated by the uniform prior
on ∆k  and a distribution generated from Dirichlet-1/2 prior. All distributions have support size
k = 10000. n ranges from 1000 to 50000 and the results are averaged over 200 trials.
Figure 2 shows the results. Observe that the proposed estimator performs similarly to the best
natural estimator for all six distributions. It also signiﬁcantly outperforms the other estimators for
Zipf  uniform  and step distributions.
The performance of other estimators depends on the underlying distribution. For example  since
Laplace is the optimal estimator when the underlying distribution is generated from the uniform
prior  it performs well in Figure 2(e)  however performs poorly on other distributions.
Furthermore  even though for distributions generated by Dirichlet priors  all the estimators have
similar looking regrets (Figures 2(e)  2(f))  the proposed estimator performs better than estimators
which are not designed speciﬁcally for that prior.

7

Number of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empiricalNumber of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empiricalNumber of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empiricalNumber of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empiricalNumber of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empiricalNumber of samples#1040.511.522.533.544.55Expected KL divergence00.050.10.150.20.250.30.350.40.450.5Best-naturalLaplaceBraess-SauerKrichevsky-TrofimovGood-Turing + empirical5 Proof sketch of Theorem 2

The proof consists of two parts. We ﬁrst show that for every estimator q  r
and then upper bound rnat
Lemma 4 (Appendix B.1). For every estimator q 

n (q  ∆k) ≤ rnat
Pσ
n (q  ∆k) using results on combined probability mass.

n (q  ∆k)

n (q  ∆k) ≤ rnat
Pσ
r

n (q  ∆k).

The proof of the above lemma relies on showing that the optimal estimator for every class in P ∈ Pσ
is natural.

5.1 Relation between rnat

n (q  ∆k) and combined probability estimation

We now relate the regret in estimating distribution to that of estimating the combined or total prob-
ability mass  deﬁned as follows. Recall that ϕt denotes the number of symbols appearing t times.
def
For a sequence xn  let St
= St(xn) denote the total probability of symbols appearing t times. For
notational convenience  we use St to denote both St(xn) and St(X n) and the usage becomes clear
in the context. Similar to KL divergence between distributions  we deﬁne KL divergence between S
and their estimates ˆS as

n(cid:88)

t=0

D(S|| ˆS) =

St log

St
ˆSt

.

Since the natural estimator assigns same probability to symbols that appear the same number of
times  estimating probabilities is same as estimating the total probability of symbols appearing a
given number of times. We formalize it in the next lemma.

Lemma 5 (Appendix B.2). For a natural estimator q let ˆSt(xn) =(cid:80)
Lemma 6. For a natural estimator q let ˆSt(xn) =(cid:80)

n (q  p) = E[D(S|| ˆS)].
rnat

In Lemma 11(Appendix B.3)  we show that there is a natural estimator that achieves rnat
maximum over all distributions p and minimum over all estimators q results in

x:nx=t qx(xn)  then

n (∆k). Taking

rnat
n (q  ∆k) = max
p∈∆k

x:nx=t qx(xn)  then
E[D(S|| ˆS)].

Furthermore 

rnat
n (∆k) = min
ˆS

max
p∈∆k

E[D(S|| ˆS)].

Thus ﬁnding the best competitive natural estimator is same as ﬁnding the best estimator for the
combined probability mass S. [13] proposed an algorithm for estimating S such that for all k and
for all p ∈ ∆k  with probability ≥ 1 − 1/n  

D(S|| ˆS) = ˜On

.

n

(cid:18) 1√
(cid:19)
(cid:18) 1√
(cid:18) 1√

min

(cid:18)

(cid:19)

(cid:19)(cid:19)

.

 

k
n

n

The result is stated in Theorem 2 of [13]. One can convert this result to a result on expectation easily
using the property that their estimator is bounded below by 1/2n and show that

A slight modiﬁcation of their proofs for Lemma 17 and Theorem 2 in their paper using(cid:80)n
(cid:80)n
t=1 ϕt ≤ k shows that their estimator ˆS for the combined probability mass S satisﬁes

E[D(S|| ˆS)] = ˜On

max
p∈∆k

n

.

√

ϕt ≤

t=1

E[D(S|| ˆS)] = ˜On

max
p∈∆k

The above equation together with Lemmas 4 and 6 results in Theorem 2.

6 Acknowledgements

We thank Jayadev Acharya  Moein Falahatgar  Paul Ginsparg  Ashkan Jafarpour  Mesrob Ohannes-
sian  Venkatadheeraj Pichapati  Yihong Wu  and the anonymous reviewers for helpful comments.

8

References
[1] William A. Gale and Geoffrey Sampson. Good-turing frequency estimation without tears. Journal of

Quantitative Linguistics  2(3):217–237  1995.

[2] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In ACL 

1996.

[3] Liam Paninski. Variational minimax estimation of discrete distributions under KL loss. In NIPS  2004.
[4] Hermann Ney  Ute Essen  and Reinhard Kneser. On structuring probabilistic dependences in stochastic

language modelling. Computer Speech & Language  8(1):1–38  1994.

[5] Fredrick Jelinek and Robert L. Mercer. Probability distribution estimation from sparse data. IBM Tech.

Disclosure Bull.  1984.

[6] Irving J. Good. The population frequencies of species and the estimation of population parameters.

Biometrika  40(3-4):237–264  1953.

[7] Thomas M. Cover and Joy A. Thomas. Elements of information theory (2. ed.). Wiley  2006.
[8] R. Krichevsky. Universal Compression and Retrieval. Dordrecht The Netherlands: Kluwer  1994.
[9] Sudeep Kamath  Alon Orlitsky  Dheeraj Pichapati  and Ananda Theertha Suresh. On learning distributions

from their samples. In COLT  2015.

[10] Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. Journal of Approximation

Theory  128(2):187–206  2004.

[11] David A. McAllester and Robert E. Schapire. On the convergence rate of Good-Turing estimators. In

COLT  2000.

[12] Evgeny Drukh and Yishay Mansour. Concentration bounds for unigrams language model.

2004.

In COLT 

[13] Jayadev Acharya  Ashkan Jafarpour  Alon Orlitsky  and Ananda Theertha Suresh. Optimal probability

estimation with applications to prediction and classiﬁcation. In COLT  2013.

[14] Alon Orlitsky  Narayana P. Santhanam  and Junan Zhang. Always Good Turing: Asymptotically optimal

probability estimation. In FOCS  2003.

[15] Boris Yakovlevich Ryabko. Twice-universal coding. Problemy Peredachi Informatsii  1984.
[16] Boris Yakovlevich Ryabko. Fast adaptive coding algorithm. Problemy Peredachi Informatsii  26(4):24–

37  1990.

[17] Dominique Bontemps  St´ephane Boucheron  and Elisabeth Gassiat. About adaptive coding on countable

alphabets. IEEE Transactions on Information Theory  60(2):808–821  2014.

[18] St´ephane Boucheron  Elisabeth Gassiat  and Mesrob I. Ohannessian. About adaptive coding on countable

alphabets: Max-stable envelope classes. CoRR  abs/1402.6305  2014.

[19] David L Donoho and Jain M Johnstone.

81(3):425–455  1994.

Ideal spatial adaptation by wavelet shrinkage. Biometrika 

[20] Felix Abramovich  Yoav Benjamini  David L Donoho  and Iain M Johnstone. Adapting to unknown

sparsity by controlling the false discovery rate. The Annals of Statistics  2006.

[21] Peter J Bickel  Chris A Klaassen  YA’Acov Ritov  and Jon A Wellner. Efﬁcient and adaptive estimation

for semiparametric models. Johns Hopkins University Press Baltimore  1993.

[22] Andrew Barron  Lucien Birg´e  and Pascal Massart. Risk bounds for model selection via penalization.

Probability theory and related ﬁelds  113(3):301–413  1999.

[23] Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer  2004.
[24] Jayadev Acharya  Hirakendu Das  Ashkan Jafarpour  Alon Orlitsky  and Shengjun Pan. Competitive

closeness testing. COLT  2011.

[25] Jayadev Acharya  Hirakendu Das  Ashkan Jafarpour  Alon Orlitsky  Shengjun Pan  and Ananda Theertha

Suresh. Competitive classiﬁcation and closeness testing. In COLT  2012.

[26] Jayadev Acharya  Ashkan Jafarpour  Alon Orlitsky  and Ananda Theertha Suresh. A competitive test for

uniformity of monotone distributions. In AISTATS  2013.

[27] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity testing.

In FOCS  2014.

[28] Gregory Valiant and Paul Valiant. Instance optimal learning. CoRR  abs/1504.05321  2015.
[29] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and proba-

bilistic analysis. Cambridge University Press  2005.

9

,Alon Orlitsky
Ananda Theertha Suresh
Xinyun Chen
Chang Liu
Dawn Song