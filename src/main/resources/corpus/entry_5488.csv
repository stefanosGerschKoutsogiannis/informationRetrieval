2014,The Noisy Power Method: A Meta Algorithm with Applications,We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call noisy power method. Our result characterizes the convergence behavior of the algorithm when a large amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion  streaming principal component analysis (PCA)  and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications. A recent work of Mitliagkas et al.~(NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions. Moreover  even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. As a second application  we provide an algorithm for differentially private principal component analysis that runs in nearly linear time in the input sparsity and achieves nearly tight worst-case error bounds. Complementing our worst-case bounds  we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013) and leads to strong average-case improvements over the optimal worst-case bound.,The Noisy Power Method:

A Meta Algorithm with Applications

Moritz Hardt∗

IBM Research Almaden

Eric Price†

IBM Research Almaden

Abstract

We provide a new robust convergence analysis of the well-known power method for
computing the dominant singular vectors of a matrix that we call the noisy power
method. Our result characterizes the convergence behavior of the algorithm when
a signiﬁcant amount noise is introduced after each matrix-vector multiplication.
The noisy power method can be seen as a meta-algorithm that has recently found a
number of important applications in a broad range of machine learning problems
including alternating minimization for matrix completion  streaming principal
component analysis (PCA)  and privacy-preserving spectral analysis. Our general
analysis subsumes several existing ad-hoc convergence bounds and resolves a
number of open problems in multiple applications:
Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-
efﬁcient algorithm for PCA in a streaming model where samples are drawn from a
gaussian spiked covariance model. We give a simpler and more general analysis that
applies to arbitrary distributions conﬁrming experimental evidence of Mitliagkas
et al. Moreover  even in the spiked covariance model our result gives quantitative
improvements in a natural parameter regime. It is also notably simpler and follows
easily from our general convergence analysis of the noisy power method together
with a matrix Chernoff bound.
Private PCA. We provide the ﬁrst nearly-linear time algorithm for the problem
of differentially private principal component analysis that achieves nearly tight
worst-case error bounds. Complementing our worst-case bounds  we show that the
error dependence of our algorithm on the matrix dimension can be replaced by an
essentially tight dependence on the coherence of the matrix. This result resolves the
main problem left open by Hardt and Roth (STOC 2013). The coherence is always
bounded by the matrix dimension but often substantially smaller thus leading to
strong average-case improvements over the optimal worst-case bound.

1

Introduction

Computing the dominant singular vectors of a matrix is one of the most important algorithmic
tasks underlying many applications including low-rank approximation  PCA  spectral clustering 
dimensionality reduction  matrix completion and topic modeling. The classical problem is well-
understood  but many recent applications in machine learning face the fundamental problem of
approximately ﬁnding singular vectors in the presence of noise. Noise can enter the computation
through a variety of sources including sampling error  missing entries  adversarial corruptions and
privacy constraints. It is desirable to have one robust method for handling a variety of cases without
the need for ad-hoc analyses. In this paper we consider the noisy power method  a fast general purpose
method for computing the dominant singular vectors of a matrix when the target matrix can only be
accessed through inaccurate matrix-vector products.

∗Email: mhardt@us.ibm.com
†Email: ecprice@cs.utexas.edu

1

Figure 1 describes the method when the target matrix A is a symmetric d× d matrix—a generalization
to asymmetric matrices is straightforward. The algorithm starts from an initial matrix X0 ∈ Rd×p
and iteratively attempts to perform the update rule X(cid:96) → AX(cid:96). However  each such matrix product is
followed by a possibly adversarially and adaptively chosen perturbation G(cid:96) leading to the update rule
X(cid:96) → AX(cid:96) + G(cid:96). It will be convenient though not necessary to maintain that X(cid:96) has orthonormal
columns which can be achieved through a QR-factorization after each update.

Input: Symmetric matrix A ∈ Rd×d  number of iterations L  dimension p

1. Choose X0 ∈ Rd×p.
2. For (cid:96) = 1 to L:

(a) Y(cid:96) ← AX(cid:96)−1 + G(cid:96) where G(cid:96) ∈ Rd×p is some perturbation
(b) Let Y(cid:96) = X(cid:96)R(cid:96) be a QR-factorization of Y(cid:96)

Output: Matrix XL

Figure 1: Noisy Power Method (NPM)

The noisy power method is a meta algorithm that when instantiated with different settings of G(cid:96)
and X0 adapts to a variety of applications. In fact  there have been a number of recent surprising
applications of the noisy power method:

1. Jain et al. [JNS13  Har14] observe that the update rule of the well-known alternating least
squares heuristic for matrix completion can be considered as an instance of NPM. This lead
to the ﬁrst provable convergence bounds for this important heuristic.

2. Mitgliakas et al. [MCJ13] observe that NPM applies to a streaming model of principal
component analysis (PCA) where it leads to a space-efﬁcient and practical algorithm for
PCA in settings where the covariance matrix is too large to process directly.

3. Hardt and Roth [HR13] consider the power method in the context of privacy-preserving

PCA where noise is added to achieve differential privacy.

In each setting there has so far only been an ad-hoc analysis of the noisy power method. In the ﬁrst
setting  only local convergence is argued  that is  X0 has to be cleverly chosen. In the second setting 
the analysis only holds for the spiked covariance model of PCA. In the third application  only the
case p = 1 was considered.
In this work we give a completely general analysis of the noisy power method that overcomes
limitations of previous analyses. Our result characterizes the global convergence properties of the
algorithm in terms of the noise G(cid:96) and the initial subspace X0. We then consider the important
case where X0 is a randomly chosen orthonormal basis. This case is rather delicate since the initial
correlation between a random matrix X0 and the target subspace is vanishing in the dimension d for
small p. Another important feature of the analysis is that it shows how X(cid:96) converges towards the ﬁrst
k (cid:54) p singular vectors. Choosing p to be larger than the target dimension leads to a quantitatively
stronger result. Theorem 2.3 formally states our convergence bound. Here we highlight one useful
corollary to illustrate our more general result.
Corollary 1.1. Let k (cid:54) p. Let U ∈ Rd×k represent the top k singular vectors of A and let
σ1 (cid:62) ··· (cid:62) σn (cid:62) 0 denote its singular values. Suppose X0 is an orthonormal basis of a random
p-dimensional subspace. Further suppose that at every step of NPM we have
p−√
√
√

5(cid:107)G(cid:96)(cid:107) (cid:54) ε(σk − σk+1) and

5(cid:107)U(cid:62)G(cid:96)(cid:107) (cid:54) (σk − σk+1)

for some ﬁxed parameter τ and ε < 1/2. Then with all but τ−Ω(p+1−k) + e−Ω(d) probability  there
exists an L = O(

log(dτ /ε)) so that after L steps we have that(cid:13)(cid:13)(I − XLX(cid:62)

L )U(cid:13)(cid:13) (cid:54) ε.

σk

k−1
d

τ

σk−σk+1

The corollary shows that the algorithm converges in the strong sense that the entire spectral norm
of U up to an ε error is contained in the space spanned by XL. To achieve this the result places two
assumptions on the magnitude of the noise. The total spectral norm of G(cid:96) must be bounded by ε
times the separation between σk and σk+1. This dependence on the singular value separation arises
even in the classical perturbation theory of Davis-Kahan [DK70]. The second condition is speciﬁc to
the power method and requires that the noise term is proportionally smaller when projected onto the
space spanned by the top k singular vectors. This condition ensures that the correlation between X(cid:96)

2

and U that is initially very small is not destroyed by the noise addition step. If the noise term has
some spherical properties (e.g. a Gaussian matrix)  we expect the projection onto U to be smaller

by a factor of(cid:112)k/d  since the space U is k-dimensional. In the case where p = k + Ω(k) this is

precisely what the condition requires. When p = k the requirement is stronger by a factor of k. This
phenomenon stems from the fact that the smallest singular value of a random p × k gaussian matrix
behaves differently in the square and the rectangular case.
We demonstrate the usefulness of our convergence bound with several novel results in some of the
aforementioned applications.

1.1 Application to memory-efﬁcient streaming PCA
In the streaming PCA setting we receive a stream of samples z1  z2  . . . zn ∈ Rd drawn i.i.d. from
an unknown distribution D over Rd. Our goal is to compute the dominant k eigenvectors of the
covariance matrix A = Ez∼D zz(cid:62). The challenge is to do this in space linear in the output size 
namely O(kd). Recently  Mitgliakas et al. [MCJ13] gave an algorithm for this problem based on the
noisy power method. We analyze the same algorithm  which we restate here and call SPM:

Input: Stream of samples z1  z2  . . .   zn ∈ Rd  iterations L  dimension p
1. Let X0 ∈ Rd×p be a random orthonormal basis. Let T = (cid:98)m/L(cid:99)
2. For (cid:96) = 1 to L:

(a) Compute Y(cid:96) = A(cid:96)X(cid:96)−1 where A(cid:96) =(cid:80)(cid:96)T

i=((cid:96)−1)T +1 ziz(cid:62)

i

(b) Let Y(cid:96) = X(cid:96)R(cid:96) be a QR-factorization of Y(cid:96)

Output: Matrix XL

Figure 2: Streaming Power Method (SPM)

The algorithm can be executed in space O(pd) since the update step can compute the d × p matrix
A(cid:96)X(cid:96)−1 incrementally without explicitly computing A(cid:96). The algorithm maps to our setting by
deﬁning G(cid:96) = (A(cid:96) − A)X(cid:96)−1. With this notation Y(cid:96) = AX(cid:96)−1 + G(cid:96). We can apply Corollary 1.1
directly once we have suitable bounds on (cid:107)G(cid:96)(cid:107) and (cid:107)U(cid:62)G(cid:96)(cid:107).
The result of [MCJ13] is speciﬁc to the spiked covariance model. The spiked covariance model
is deﬁned by an orthonormal basis U ∈ Rd×k and a diagonal matrix Λ ∈ Rk×k with diagonal
entries λ1 (cid:62) λ2 (cid:62) ··· (cid:62) λk > 0. The distribution D(U  Λ) is deﬁned as the normal distribution
N(0  (U Λ2U(cid:62) + σ2Idd×d)). Without loss of generality we can scale the examples such that λ1 = 1.

One corollary of our result shows that the algorithm outputs XL such that(cid:13)(cid:13)(I − XLX(cid:62)

L )U(cid:13)(cid:13) (cid:54) ε

with probability 9/10 provided p = k + Ω(k) and the number of samples satisﬁes

(cid:19)

(cid:18) σ6 + 1

ε2λ6
k

n = Θ

kd

.

Previously  the same bound1 was known with a quadratic dependence on k in the case where p = k.
Here we can strengthen the bound by increasing p slightly.
While we can get some improvements even in the spiked covariance model  our result is substantially
more general and applies to any distribution. The sample complexity bound we get varies according
to a technical parameter of the distribution. Roughly speaking  we get a near linear sample complexity
if the distribution is either “round” (as in the spiked covariance setting) or is very well approximated
by a k dimensional subspace. To illustrate the latter condition  we have the following result without
making any assumptions other than scaling the distribution:
Corollary 1.2. Let D be any distribution scaled so that Pr{(cid:107)z(cid:107) > t} (cid:54) exp(−t) for every t (cid:62) 1.
Let U represent the top k eigenvectors of the covariance matrix E zz(cid:62) and σ1 (cid:62) ··· (cid:62) σd (cid:62) 0 its
eigenvalues. Then  SPM invoked with p = k + Ω(k) outputs a matrix XL such with probability

L )U(cid:13)(cid:13) (cid:54) ε provided SPM receives n samples where n satisﬁes n =

(cid:17)
ε2k(σk−σk+1)3 · d
1That the bound stated in [MCJ13] has a σ6 dependence is not completely obvious. There is a O(σ4) in the
k)) in the denominator which simpliﬁes to O(1/σ2) for constant

9/10 we have(cid:13)(cid:13)(I − XLX(cid:62)
(cid:16)

k)/(σ2 + 0.5λ2

˜O

σk

.

numerator and log((σ2 + 0.75λ2
λk and σ2 (cid:62) 1.

3

The corollary establishes a sample complexity that’s linear in d provided that the spectrum decays
quickly  as is common in applications. For example  if the spectrum follows a power law so that
σj ≈ j−c for a constant c > 1/2  the bound becomes n = ˜O(k2c+2d/ε2).

1.2 Application to privacy-preserving spectral analysis

Many applications of singular vector computation are plagued by the fact that the underlying matrix
contains sensitive information about individuals. A successful paradigm in privacy-preserving data
analysis rests on the notion of differential privacy which requires all access to the data set to be
randomized in such a way that the presence or absence of a single data item is hidden. The notion of
data item varies and could either refer to a single entry  a single row  or a rank-1 matrix of bounded
norm. More formally  Differential Privacy requires that the output distribution of the algorithm
changes only slightly with the addition or deletion of a single data item. This requirement often
necessitates the introduction of signiﬁcant levels of noise that make the computation of various
objectives challenging. Differentially private singular vector computation has been studied actively
since the work of Blum et al. [BDMN05]. There are two main objectives. The ﬁrst is computational
efﬁciency. The second objective is to minimize the amount of error that the algorithm introduces.
In this work  we give a fast algorithm for differentially private singular vector computation based
on the noisy power method that leads to nearly optimal bounds in a number of settings that were
considered in previous work. The algorithm is described in Figure 3. It’s a simple instance of NPM
in which each noise matrix G(cid:96) is a gaussian random matrix scaled so that the algorithm achieves
(ε  δ)-differential privacy (as formally deﬁned in Deﬁnition E.1). It is easy to see that the algorithm
can be implemented in time nearly linear in the number of nonzero entries of the input matrix (input
sparsity). This will later lead to strong improvements in running time compared with several previous
works.

Input: Symmetric A ∈ Rd×d  L  p  privacy parameters ε  δ > 0

1. Let X0 be a random orthonormal basis and put σ =

ε−1(cid:112)4pL log(1/δ)

2. For (cid:96) = 1 to L:

(a) Y(cid:96) ← AX(cid:96)−1 + G(cid:96) where G(cid:96) ∼ N(0 (cid:107)X(cid:96)−1(cid:107)2∞σ2)d×p.
(b) Compute the QR-factorization Y(cid:96) = X(cid:96)R(cid:96)

Output: Matrix XL

Figure 3: Private Power Method (PPM). Here (cid:107)X(cid:107)∞ = maxij |Xij|.

We ﬁrst state a general purpose analysis of PPM that follows from Corollary 1.1.
Theorem 1.3. Let k (cid:54) p. Let U ∈ Rd×k represent the top k singular vectors of A and let
σ1 (cid:62) ··· (cid:62) σd (cid:62) 0 denote its singular values. Then  PPM satisﬁes (ε  δ)-differential privacy and
after L = O(

σk

σk−σk+1

(cid:13)(cid:13)(I − XLX(cid:62)

log(d)) iterations we have with probability 9/10 that
√
p − √

L )U(cid:13)(cid:13) (cid:54) O

√
σk − σk+1

σ max(cid:107)X(cid:96)(cid:107)∞

d log L

·

√

p
k − 1

.

(cid:32)

(cid:33)

When p = k + Ω(k) the trailing factor becomes a constant. If p = k it creates a factor k overhead.
In the worst-case we can always bound (cid:107)X(cid:96)(cid:107)∞ by 1 since X(cid:96) is an orthonormal basis. However  in
principle we could hope that a much better bound holds provided that the target subspace U has small
coordinates. Hardt and Roth [HR12  HR13] suggested a way to accomplish a stronger bound by
considering a notion of coherence of A  denoted as µ(A). Informally  the coherence is a well-studied
parameter that varies between 1 and n  but is often observed to be small. Intuitively  the coherence
measures the correlation between the singular vectors of the matrix with the standard basis. Low
coherence means that the singular vectors have small coordinates in the standard basis. Many results
on matrix completion and robust PCA crucially rely on the assumption that the underlying matrix
has low coherence [CR09  CT10  CLMW11] (though the notion of coherence here will be somewhat
different).

4

Theorem 1.4. Under the assumptions of Theorem 1.3  we have the conclusion

(cid:33)

.

√
p − √

p
k − 1

·

√

(cid:13)(cid:13)(I − XLX(cid:62)

L )U(cid:13)(cid:13) (cid:54) O

(cid:32)

σ(cid:112)µ(A) log d log L

σk − σk+1

√

Hardt and Roth proved this result for the case where p = 1. The extension to p > 1 lost a factor
of
d in general and therefore gave no improvement over Theorem 1.3. Our result resolves the
main problem left open in their work. The strength of Theorem 1.4 is that the bound is essentially
dimension-free under a natural assumption on the matrix and never worse than our worst-case result.
It is also known that in general the dependence on d achieved in Theorem 1.3 is best possible in the
worst case (see discussion in [HR13]) so that further progress requires making stronger assumptions.
Coherence is a natural such assumption. The proof of Theorem 1.4 proceeds by showing that each

iterate X(cid:96) satisﬁes (cid:107)X(cid:96)(cid:107)∞ (cid:54) O((cid:112)µ(A) log(d)/d) and applying Theorem 1.3. To do this we exploit

a non-trivial symmetry of the algorithm that we discuss in Section E.3.

Other variants of differential privacy. Our discussion above applied to (ε  δ)-differential privacy
under changing a single entry of the matrix. Several works consider other variants of differential
privacy. It is generally easy to adapt the power method to these settings by changing the noise
distribution or its scaling. To illustrate this aspect  we consider the problem of privacy-preserving
principal component analysis as recently studied by [CSS12  KT13]. Both works consider an
algorithm called exponential mechanism. The ﬁrst work gives a heuristic implementation that may
not converge  while the second work gives a provably polynomial time algorithm though the running
√
time is more than cubic. Our algorithm gives strong improvements in running time while giving
nearly optimal accuracy guarantees as it matches a lower bound of [KT13] up to a ˜O(
k) factor. We
also improve the error dependence on k by polynomial factors compared to previous work. Moreover 
we get an accuracy improvement of O(
d) for the case of (ε  δ)-differential privacy  while these
previous works only apply to (ε  0)-differential privacy. Section E.2 provides formal statements.

√

1.3 Related Work

Numerical Analysis. One might expect that a suitable analysis of the noisy power method would
have appeared in the numerical analysis literature. However  we are not aware of a reference and
there are a number of points to consider. First  our noise model is adaptive thus setting it apart from
the classical perturbation theory of the singular vector decomposition [DK70]. Second  we think
of the perturbation at each step as large making it conceptually different from ﬂoating point errors.
Third  research in numerical analysis over the past decades has largely focused on faster Krylov
subspace methods. There is some theory of inexact Krylov methods  e.g.  [SS07] that captures the
effect of noisy matrix-vector products in this context. Related to our work are also results on the
perturbation stability of the QR-factorization since those could be used to obtain convergence bounds
for subspace iteration. Such bounds  however  must depend on the condition number of the matrix
that the QR-factorization is applied to. See Chapter 19.9 in [Hig02] and the references therein for
background. Our proof strategy avoids this particular dependence on the condition number.

Streaming PCA. PCA in the streaming model is related to a host of well-studied problems that we
cannot survey completely here. We refer to [ACLS12  MCJ13] for a thorough discussion of prior
work. Not mentioned therein is a recent work on incremental PCA [BDF13] that leads to space
efﬁcient algorithms computing the top singular vector; however  it’s not clear how to extend their
results to computing multiple singular vectors.

Privacy. There has been much work on differentially private spectral analysis starting with Blum
et al. [BDMN05] who used an algorithm known as Randomized Response which adds a single
noise matrix N either to the input matrix A or the covariance matrix AA(cid:62). This approach appears
in a number of papers  e.g. [MM09]. While often easy to analyze it has the disadvantage that it
converts sparse matrices to dense matrices and is often impractical on large data sets. Chaudhuri
et al. [CSS12] and Kapralov-Talwar [KT13] use the so-called exponential mechanism to sample
approximate eigenvectors of the matrix. The sampling is done using a heuristic approach without
convergence polynomial time convergence guarantees in the ﬁrst case and using a polynomial time
algorithm in the second. Both papers achieve a tight dependence on the matrix dimension d (though

5

the dependence on k is suboptimal in general). Most closely related to our work are the results of
Hardt and Roth [HR13  HR12] that introduced matrix coherence as a way to circumvent existing
worst-case lower bounds on the error. They also analyzed a natural noisy variant of power iteration
for the case of computing the dominant eigenvector of A. When multiple eigenvectors are needed 
their algorithm uses the well-known deﬂation technique. However  this step loses control of the

coherence of the original matrix and hence results in suboptimal bounds. In fact  a(cid:112)rank(A) factor

is lost.

1.4 Open Questions

We believe Corollary 1.1 to be a fairly precise characterization of the convergence of the noisy power
method to the top k singular vectors when p = k. The main ﬂaw is that the noise tolerance depends
on the eigengap σk − σk+1  which could be very small. We have some conjectures for results that do
not depend on this eigengap.
First  when p > k  we think that Corollary 1.1 might hold using the gap σk − σp+1 instead of
σk − σk+1. Unfortunately  our proof technique relies on the principal angle decreasing at each step 
which does not necessarily hold with the larger level of noise. Nevertheless we expect the principal
angle to decrease fairly fast on average  so that XL will contain a subspace very close to U. We are
actually unaware of this sort of result even in the noiseless setting.
Conjecture 1.5. Let X0 be a random p-dimensional basis for p > k. Suppose at every step we have

100(cid:107)G(cid:96)(cid:107) (cid:54) ε(σk − σp+1) and 100(cid:107)U T G(cid:96)(cid:107) (cid:54)

√

p − √
√

k − 1
d

Then with high probability  after L = O(

σk

σk−σp+1

(cid:107)(I − XLX(cid:62)

log(d/ε)) iterations we have
L )U(cid:107) (cid:54) ε.

The second way of dealing with a small eigengap would be to relax our goal. Corollary 1.1 is quite
stringent in that it requires XL to approximate the top k singular vectors U  which gets harder when
the eigengap approaches zero and the kth through p + 1st singular vectors are nearly indistinguishable.
A relaxed goal would be for XL to spectrally approximate A  that is

(cid:107)(I − XLX(cid:62)

L )A(cid:107) (cid:54) σk+1 + ε.

(1)

This weaker goal is known to be achievable in the noiseless setting without any eigengap at all.
In particular  [?] shows that (1) happens after L = O( σk+1
log n) steps in the noiseless setting. A
ε
plausible extension to the noisy setting would be:
Conjecture 1.6. Let X0 be a random 2k-dimensional basis. Suppose at every step we have

(cid:107)G(cid:96)(cid:107) (cid:54) ε and
Then with high probability  after L = O( σk+1
(cid:107)(I − XLX(cid:62)

ε

log d) iterations we have that
L )A(cid:107) (cid:54) σk+1 + O(ε).

(cid:107)U T G(cid:96)(cid:107) (cid:54) ε(cid:112)k/d

1.5 Organization

All proofs can be found in the supplementary material. In the remaining space  we limit ourselves to
a more detailed discussion of our convergence analysis and the application to streaming PCA. The
entire section on privacy is in the supplementary materials in Section E.

2 Convergence of the noisy power method

Figure 1 presents our basic algorithm that we analyze in this section. An important tool in our analysis
are principal angles  which are useful in analyzing the convergence behavior of numerical eigenvalue
methods. Roughly speaking  we will show that the tangent of the k-th principal angle between X and
the top k eigenvectors of A decreases as σk+1/σk in each iteration of the noisy power method.

6

Deﬁnition 2.1 (Principal angles). Let X and Y be subspaces of Rd of dimension at least k. The
principal angles 0 (cid:54) θ1 (cid:54) ··· (cid:54) θk between X and Y and associated principal vectors x1  . . .   xk
and y1  . . .   yk are deﬁned recursively via

θi(X  Y) = min

arccos

: x ∈ X   y ∈ Y  x ⊥ xj  y ⊥ yj for all j < i

(cid:26)

(cid:19)

(cid:18) (cid:104)x  y(cid:105)

(cid:107)x(cid:107)2(cid:107)y(cid:107)2

(cid:27)

and xi  yi are the x and y that give this value. For matrices X and Y   we use θk(X  Y ) to denote the
kth principal angle between their ranges.

2.1 Convergence argument
Fix parameters 1 (cid:54) k (cid:54) p (cid:54) d. In this section we consider a symmetric d × d matrix A with singular
values σ1 (cid:62) σ2 (cid:62) ··· (cid:62) σd. We let U ∈ Rd×k contain the ﬁrst k eigenvectors of A. Our main
lemma shows that tan θk(U  X) decreases multiplicatively in each step.
Lemma 2.2. Let U contain the largest k eigenvectors of a symmetric matrix A ∈ Rd×d  and let
X ∈ Rd×p for p (cid:62) k. Let G ∈ Rd×p satisfy

4(cid:107)U(cid:62)G(cid:107) (cid:54) (σk − σk+1) cos θk(U  X)

4(cid:107)G(cid:107) (cid:54) (σk − σk+1)ε.

for some ε < 1. Then

tan θk(U  AX + G) (cid:54) max

(cid:32)

(cid:32)

(cid:18) σk+1

σk

(cid:19)1/4(cid:33)

(cid:33)

tan θk(U  X)

.

ε  max

ε 

We can inductively apply the previous lemma to get the following general convergence result.
Theorem 2.3. Let U represent the top k eigenvectors of the matrix A and γ = 1− σk+1/σk. Suppose
that the initial subspace X0 and noise G(cid:96) is such that

5(cid:107)U(cid:62)G(cid:96)(cid:107) (cid:54) (σk − σk+1) cos θk(U  X0)

5(cid:107)G(cid:96)(cid:107) (cid:54) ε(σk − σk+1)

(cid:16) tan θk(U X0)

(cid:17)

such that for all

ε

at every stage (cid:96)  for some ε < 1/2. Then there exists an L (cid:46) 1
(cid:96) (cid:62) L we have tan θ(U  XL) (cid:54) ε.

γ log

2.2 Random initialization

The next lemma essentially follows from bounds on the smallest singular value of gaussian random
matrices [RV09].
Lemma 2.4. For an arbitrary orthonormal U and random subspace X  we have

tan θk(U  X) (cid:54) τ

√

√
p − √

d
k − 1

with all but τ−Ω(p+1−k) + e−Ω(d) probability.

With this lemma we can prove the corollary that we stated in the introduction.

. Hence cos θk(U  X0) (cid:62) 1/(1 + tan θk(U  X0)) (cid:62) √

Proof of Corollary 1.1. By Lemma 2.4  with the desired probability we have tan θk(U  X0) (cid:54)
p−√
√
k−1
p−√
√
√
. Rescale τ and ap-
2·τ
d
L )U(cid:107) = sin θk(U  XL) (cid:54)
ply Theorem 2.3 to get that tan θk(U  XL) (cid:54) ε. Then (cid:107)(I − XLX(cid:62)
tan θk(U  XL) (cid:54) ε.
(cid:4)

τ

d
k−1

7

3 Memory efﬁcient streaming PCA
In the streaming PCA setting we receive a stream of samples z1  z2 ··· ∈ Rd. Each sample is drawn
i.i.d. from an unknown distribution D over Rd. Our goal is to compute the dominant k eigenvectors
of the covariance matrix A = Ez∼D zz(cid:62). The challenge is to do this with small space  so we cannot
store the d2 entries of the sample covariance matrix. We would like to use O(dk) space  which is
necessary even to store the output.
The streaming power method (Figure 2  introduced by [MCJ13]) is a natural algorithm that performs
streaming PCA with O(dk) space. The question that arises is how many samples it requires to
achieve a given level of accuracy  for various distributions D. Using our general analysis of the noisy
power method  we show that the streaming power method requires fewer samples and applies to more
distributions than was previously known. We analyze a broad class of distributions:
Deﬁnition 3.1. A distribution D over Rd is (B  p)-round if for every p-dimensional projection P and
all t (cid:62) 1 we have Prz∼D {(cid:107)z(cid:107) > t} (cid:54) exp(−t) and Prz∼D
The ﬁrst condition just corresponds to a normalization of the samples drawn from D. Assuming the
ﬁrst condition holds  the second condition always holds with B = d/p. For this reason our analysis
in principle applies to any distribution  but the sample complexity will depend quadratically on B.
Let us illustrate this deﬁnition through the example of the spiked covariance model studied
by [MCJ13]. The spiked covariance model is deﬁned by an orthonormal basis U ∈ Rd×k and a
diagonal matrix Λ ∈ Rk×k with diagonal entries λ1 (cid:62) λ2 (cid:62) ··· (cid:62) λk > 0. The distribution D(U  Λ)
i λ2
i )
is a normalization factor chosen so that the distribution satisﬁes the norm bound. Note that the the
i + σ2)/D for 1 (cid:54) i (cid:54) k and σi = σ2/D for
i-th eigenvalue of the covariance matrix is σi = (λ2
i > k. We show in Lemma D.2 that the spiked covariance model D(U  Λ) is indeed (B  p)-round for
B = O(
Theorem 3.2. Let D be a (B  p)-round distribution over Rd with covariance matrix A whose
eigenvalues are σ1 (cid:62) σ2 (cid:62) ··· (cid:62) σd (cid:62) 0. Let U ∈ Rd×k be an orthonormal basis for the
eigenvectors corresponding to the ﬁrst k eigenvalues of A. Then  the streaming power method SPM
returns an orthonormal basis X ∈ Rd×p such that tan θ(U  X) (cid:54) ε with probability 9/10 provided
that SPM receives n samples from D for some n satisfying

is deﬁned as the normal distribution N(0  (U Λ2U(cid:62) + σ2Idd×d)/D) where D = Θ(dσ2 +(cid:80)

(cid:110)(cid:107)P z(cid:107) > t ·(cid:112)Bp/d

λ2
1+σ2

tr(Λ)/d+σ2 )  which is constant for σ (cid:38) λ1. We have the following main theorem.

(cid:111) (cid:54) exp(−t) .

if p = k + Θ(k). More generally  for all p (cid:62) k one can get the slightly stronger result

√
Bpσk max{1/ε2  Bp/(

k − 1)2} log2 d

(cid:32)

n (cid:54) ˜O

(cid:33)

.

Instantiating with the spiked covariance model gives the following:
Corollary 3.3. In the spiked covariance model D(U  Λ) the conclusion of Theorem 3.2 holds for
p = 2k with

n (cid:54) ˜O

(cid:19)

(cid:18) B2σkk log2 d
ε2(σk − σk+1)3d
p − √
(σk − σk+1)3d

(cid:18) (λ2

n = ˜O

1 + σ2)2(λ2

ε2λ6
k

k + σ2)

(cid:16) σ6+1

ε2

(cid:19)
(cid:17)

dk

.

· dk

.

(cid:19)

When λ1 = O(1) and λk = Ω(1) this becomes n = ˜O

We can apply Theorem 3.2 to all distributions that have exponentially concentrated norm by setting
B = d/p. This gives the following result.
Corollary 3.4. Let D be any distribution scaled such that Prz∼D[(cid:107)z(cid:107) > t] (cid:54) exp(−t) for all t (cid:62) 1.
Then the conclusion of Theorem 3.2 holds for p = 2k with

(cid:18)

n = ˜O

σk

ε2k(σk − σk+1)3 · d

.

If the eigenvalues follow a power law  σj ≈ j−c for a constant c > 1/2  this gives an n =
˜O(k2c+2d/ε2) bound on the sample complexity.

8

References
[ACLS12] Raman Arora  Andrew Cotter  Karen Livescu  and Nathan Srebro. Stochastic optimiza-
tion for pca and pls. In Communication  Control  and Computing (Allerton)  2012 50th
Annual Allerton Conference on  pages 861–868. IEEE  2012.
Akshay Balsubramani  Sanjoy Dasgupta  and Yoav Freund. The fast convergence of
incremental PCA. In Proc. 27th Neural Information Processing Systems (NIPS)  pages
3174–3182  2013.

[BDF13]

[BDMN05] Avrim Blum  Cynthia Dwork  Frank McSherry  and Kobbi Nissim. Practical privacy:

the SuLQ framework. In Proc. 24th PODS  pages 128–138. ACM  2005.

[CLMW11] Emmanuel J. Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal compo-

[CR09]

[CSS12]

[CT10]

[DK70]

[Har14]

[Hig02]

[HR12]

[HR13]

[JNS13]

[KT13]

[MCJ13]

[MM09]

[RV09]

[SS07]

nent analysis? J. ACM  58(3):11  2011.
Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex opti-
mization. Foundations of Computional Mathematics  9:717–772  December 2009.
Kamalika Chaudhuri  Anand Sarwate  and Kaushik Sinha. Near-optimal differentially
private principal components. In Proc. 26th Neural Information Processing Systems
(NIPS)  2012.
Emmanuel J. Candès and Terence Tao. The power of convex relaxation: near-optimal
matrix completion. IEEE Transactions on Information Theory  56(5):2053–2080  2010.
Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii.
SIAM J. Numer. Anal.  7:1–46  1970.
Moritz Hardt. Understanding alternating minimization for matrix completion. In Proc.
55th Foundations of Computer Science (FOCS). IEEE  2014.
Nicholas J. Higham. Accuracy and Stability of Numerical Algorithms. Society for
Industrial and Applied Mathematics  2002.
Moritz Hardt and Aaron Roth. Beating randomized response on incoherent matrices.
In Proc. 44th Symposium on Theory of Computing (STOC)  pages 1255–1268. ACM 
2012.
Moritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector
computation. In Proc. 45th Symposium on Theory of Computing (STOC). ACM  2013.
Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion
using alternating minimization. In Proc. 45th Symposium on Theory of Computing
(STOC)  pages 665–674. ACM  2013.
Michael Kapralov and Kunal Talwar. On differentially private low rank approximation.
In Proc. 24rd Symposium on Discrete Algorithms (SODA). ACM-SIAM  2013.
Ioannis Mitliagkas  Constantine Caramanis  and Prateek Jain. Memory limited  stream-
ing PCA. In Proc. 27th Neural Information Processing Systems (NIPS)  pages 2886–
2894  2013.
Frank McSherry and Ilya Mironov. Differentially private recommender systems: build-
ing privacy into the net. In Proc. 15th KDD  pages 627–636. ACM  2009.
Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular
matrix. Communications on Pure and Applied Mathematics  62(12):1707–1739  2009.
Valeria Simoncini and Daniel B. Szyld. Recent computational developments in krylov
subspace methods for linear systems. Numerical Linear Algebra With Applications 
14:1–59  2007.

9

,Moritz Hardt
Eric Price
Motoya Ohnishi
Masahiro Yukawa
Mikael Johansson
Masashi Sugiyama