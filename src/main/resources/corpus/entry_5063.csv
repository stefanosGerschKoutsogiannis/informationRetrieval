2014,Smoothed Gradients for Stochastic Variational Inference,Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution  following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods  SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper  we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First  its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second  it enjoys significant variance reduction over the unbiased estimates  smaller bias than averaged gradients  and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.,Smoothed Gradients for

Stochastic Variational Inference

Stephan Mandt

Department of Physics
Princeton University

smandt@princeton.edu

David Blei

david.blei@columbia.edu

Department of Computer Science

Department of Statistics

Columbia University

Abstract

Stochastic variational inference (SVI) lets us scale up Bayesian computation to
massive data. It uses stochastic optimization to ﬁt a variational distribution  fol-
lowing easy-to-compute noisy natural gradients. As with most traditional stochas-
tic optimization methods  SVI takes precautions to use unbiased stochastic gradi-
ents whose expectations are equal to the true gradients. In this paper  we explore
the idea of following biased stochastic gradients in SVI. Our method replaces
the natural gradient with a similarly constructed vector that uses a ﬁxed-window
moving average of some of its previous terms. We will demonstrate the many ad-
vantages of this technique. First  its computational cost is the same as for SVI and
storage requirements only multiply by a constant factor. Second  it enjoys signif-
icant variance reduction over the unbiased estimates  smaller bias than averaged
gradients  and leads to smaller mean-squared error against the full gradient. We
test our method on latent Dirichlet allocation with three large corpora.

1

Introduction

Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data [1]. SVI
has been applied to many types of models  including topic models [1]  probabilistic factorization [2] 
statistical network analysis [3  4]  and Gaussian processes [5].
SVI uses stochastic optimization [6] to ﬁt a variational distribution  following easy-to-compute noisy
natural gradients that come from repeatedly subsampling from the large data set. As with most
traditional stochastic optimization methods  SVI takes precautions to use unbiased  noisy gradients
whose expectations are equal to the true gradients. This is necessary for the conditions of [6] to
apply  and guarantees that SVI climbs to a local optimum of the variational objective. Innovations
on SVI  such as subsampling from data non-uniformly [2] or using control variates [7  8]  have
maintained the unbiasedness of the noisy gradient.
In this paper  we explore the idea of following a biased stochastic gradient in SVI. We are inspired
by the recent work in stochastic optimization that uses biased gradients. For example  stochastic
averaged gradients (SAG) iteratively updates only a subset of terms in the full gradient [9]; averaged
gradients (AG) follows the average of the sequence of stochastic gradients [10]. These methods lead
to faster convergence on many problems.
However  SAG and AG are not immediately applicable to SVI. First  SAG requires storing all of the
terms of the gradient. In most applications of SVI there is a term for each data point  and avoiding
such storage is one of the motivations for using the algorithm. Second  the SVI update has a form
where we update the variational parameter with a convex combination of the previous parameter
and a new noisy version of it. This property falls out of the special structure of the gradient of
the variational objective  and has the signiﬁcant advantage of keeping the parameter in its feasible

1

∇λL = η + N ˆSi − λi 

space. (E.g.  the parameter may be constrained to be positive or even on the simplex.) Averaged
gradients  as we show below  do not enjoy this property. Thus  we develop a new method to form
biased gradients in SVI.
To understand our method  we must brieﬂy explain the special structure of the SVI stochastic natural
gradient. At any iteration of SVI  we have a current estimate of the variational parameter λi  i.e.  the
parameter governing an approximate posterior that we are trying to estimate. First  we sample a data
point wi. Then  we use the current estimate of variational parameters to compute expected sufﬁcient
statistics ˆSi about that data point. (The sufﬁcient statistics ˆSi is a vector of the same dimension as
λi.) Finally  we form the stochastic natural gradient of the variational objective L with this simple
expression:

(1)
where η is a prior from the model and N is an appropriate scaling. This is an unbiased noisy
gradient [11  1]  and we follow it with a step size ρi that decreases across iterations [6]. Because of
its algebraic structure  each step amounts to taking a weighted average 

λi+1 = (1 − ρi)λi + ρi(η + N ˆSi).

(2)

j=0

Note that this keeps λi in its feasible set.
With these details in mind  we can now describe our method. Our method replaces the natural
gradient in Eq. (1) with a similarly constructed vector that uses a ﬁxed-window moving average
of the previous sufﬁcient statistics. That is  we replace the sufﬁcient statistics with an appropriate
ˆSi−j. Note this is different from averaging the gradients  which also involves the

scaled sum (cid:80)L−1

current iteration’s estimate.
We will demonstrate the many advantages of this technique. First  its computational cost is the
same as for SVI and storage requirements only multiply by a constant factor (the window length
L). Second  it enjoys signiﬁcant variance reduction over the unbiased estimates  smaller bias than
averaged gradients  and leads to smaller mean-squared error against the full gradient. Finally  we
tested our method on latent Dirichlet allocation with three large corpora. We found that it leads to
faster convergence.

Related work We ﬁrst discuss the related work from the SVI literature. Both Ref. [8] and Ref. [7]
introduce control variates to reduce the gradient’s variance. The method leads to unbiased gradient
estimates. On the other hand  every few hundred iterations  an entire pass through the data set is
necessary  which makes the performance and expenses of the method depend on the size of the
data set. Ref. [12] develops a method to pre-select documents according to their inﬂuence on the
global update. For large data sets  however  it also suffers from high storage requirements. In the
stochastic optimization literature  we have already discussed SAG [9] and AG [10]. Similarly  Ref.
[13] introduces an exponentially fading momentum term. It too suffers from the issues of SAG and
AG  mentioned above.

2 Smoothed stochastic gradients for SVI

Latent Dirichlet Allocation and Variational Inference We start by reviewing stochastic varia-
tional inference for LDA [1  14]  a topic model that will be our running example. We are given a
corpus of D documents with words w1:D 1:N . We want to infer K hidden topics  deﬁned as multino-
mial distributions over a vocabulary of size V . We deﬁne a multinomial parameter β1:V 1:K  termed
the topics. Each document d is associated with a normalized vector of topic weights Θd. Further-
more  each word n in document d has a topic assignment zdn. This is a K−vector of binary entries 
such that zk
In the generative process  we ﬁrst draw the topics from a Dirichlet  βk ∼ Dirichlet(η). For each
document  we draw the topic weights  Θd ∼ Dirichlet(α). Finally  for each word in the document 
we draw an assignment zdn ∼ Multinomial(Θd)  and we draw the word from the assigned topic 
wdn ∼ Multinomial(βzdn). The model has the following joint probability distribution:
p(zdn|Θd)p(wdn|β1:K  zdn)

dn = 1 if word n in document d is assigned to topic k  and zk

p(w  β  Θ  z|η  α) =

dn = 0 otherwise.

p(Θd|α)

p(βk|η)

N(cid:89)

(3)

K(cid:89)

D(cid:89)

k=1

d=1

n=1

2

Following [1]  the topics β are global parameters  shared among all documents. The assignments z
and topic proportions Θ are local  as they characterize a single document.
In variational inference [15]  we approximate the posterior distribution 

p(β  Θ  z|w) =

p(β  Θ  z  w)

(cid:82) dβdΘ p(β  Θ  z  w)
(cid:80)
(cid:32) D(cid:89)
(cid:33)(cid:32) D(cid:89)
N(cid:89)

q(zdn|φdn)

z

 

(cid:33)

which is intractable to compute. The posterior is approximated by a factorized distribution 

q(Θd|γd)

q(β  Θ  z) = q(β|λ)

d=1

d=1

n=1

(5)
Here  q(β|λ) and q(Θd|γd) are Dirichlet distributions  and q(zdn|φdn) are multinomials. The param-
eters λ  γ and φ minimize the Kullback-Leibler (KL) divergence between the variational distribution
and the posterior [16]. As shown in Refs. [1  17]  the objective to maximize is the evidence lower
bound (ELBO) 

L(q) = Eq[log p(x  β  Θ  z)] − Eq[log q(β  Θ  z)].

(6)
This is a lower bound on the marginal probability of the observations. It is a sensible objective
function because  up to a constant  it is equal to the negative KL divergence between q and the
posterior. Thus optimizing the ELBO with respect to q is equivalent to minimizing its KL divergence
to the posterior.
In traditional variational methods  we iteratively update the local and global parameters. The local
parameters are updated as described in [1  17] . They are a function of the global parameters  so
at iteration i the local parameter is φdn(λi). We are interested in the global parameters. They are
updated based on the (expected) sufﬁcient statistics S(λi) 

(4)

S(λi) =

φdn(λi) · W T

dn

(7)

(cid:88)

N(cid:88)

d∈{1 ... D}

n=1

λi+1 = η + S(λi)

For ﬁxed d and n  the multinomial parameter φdn is K×1. The binary vector Wdn is V×1; it satisﬁes
W v
dn = 1 if the word n in document d is v  and else contains only zeros. Hence  S is K×V and
therefore has the same dimension as λ. Alternating updates lead to convergence.

Stochastic variational inference for LDA The computation of the sufﬁcient statistics is inefﬁ-
cient because it involves a pass through the entire data set. In Stochastic Variational Inference for
LDA [1  14]  it is approximated by stochastically sampling a ”minibatch” Bi ⊂ {1  ...  D} of |Bi|
documents  estimating S on the basis of the minibatch  and scaling the result appropriately 

ˆS(λi  Bi) =

D
|Bi|

φdn(λi) · W T
dn.

(cid:88)

N(cid:88)

d∈Bi

n=1

Because it depends on the minibatch  ˆSi = ˆS(λi  Bi) is now a random variable. We will denote
variables that explicitly depend on the random minibatch Bi at the current time i by circumﬂexes 
such as ˆg and ˆS.
In SVI  we update λ by admixing the random estimate of the sufﬁcient statistics to the current value
of λ. This involves a learning rate ρi < 1 

λi+1 = (1 − ρi)λi + ρi(η + ˆS(λi  Bi))

(8)
The case of ρ = 1 and |Bi| = D corresponds to batch variational inference (when sampling without
replacement) . For arbitrary ρ  this update is just stochastic gradient ascent  as a stochastic estimate
of the natural gradient of the ELBO [1] is

ˆg(λi  Bi) = (η − λi) + ˆS(λi  Bi) 

(9)
This interpretation opens the world of gradient smoothing techniques. Note that the above stochastic
gradient is unbiased: its expectation value is the full gradient. However  it has a variance. The goal
of this paper will be to reduce this variance at the expense of introducing a bias.

3

Algorithm 1: Smoothed stochastic gradients for Latent Dirichlet Allocation
Input: D documents  minibatch size B  number of stored
sufﬁcient statistics L  learning rate ρt  hyperparameters α  η.
Output: Hidden variational parameters λ  φ  γ.
1 Initialize λ randomly and ˆgL
2 Initialize empty queue Q = {}.
3 for i = 0 to ∞ do

i = 0.

Sample minibatch Bi ⊂ {1  . . .   D} uniformly.
initialize γ
repeat

n φdn

For d ∈ Bi and n ∈ {1  . . .   N} set

γd = α +(cid:80)
dn ∝ exp(E[log Θdk] + E[log βk wd ])  k ∈ {1  . . .   K}
φk
(cid:80)
(cid:80)N
n=1 φdnW T

until φdn and γd converge.
For each topic k  calculate sufﬁcient statistics for minibatch Bi:
ˆSi = D|Bi|
d∈Bi
Add new sufﬁcient statistic in front of queue Q:
Q ← { ˆSi} + Q
Remove last element when length L has been reached:
if length(Q) > L then
Q ← Q − { ˆSi−L}

dn

4
5
6
7

8
9

10
11

12

13

14
15
16

17
18
19

20

21

end
Update λ  using stored sufﬁcient statistics:
i ← ˆSL
ˆSL
i ← (η − λi) + ˆSL
ˆgL
t .
λt+1 = λt + ρt ˆgL

i−1 + ( ˆSi − ˆSi−L)/L

i

22
23 end

Smoothed stochastic gradients for SVI Noisy stochastic gradients can slow down the conver-
gence of SVI or lead to convergence to bad local optima. Hence  we propose a smoothing scheme
to reduce the variance of the noisy natural gradient. To this end  we average the sufﬁcient statistics
over the past L iterations. Here is a sketch:

1. Uniformly sample a minibatch Bi ⊂ {1  . . .   D} of documents. Compute the local varia-

tional parameters φ from a given λi.

2. Compute the sufﬁcient statistics ˆSi = ˆS(φ(λi)  Bi).
3. Store ˆSi  along with the L most recent sufﬁcient statistics. Compute ˆSL

i = 1
L

as their mean.

4. Compute the smoothed stochastic gradient according to

i = (η − λi) + ˆSL
ˆgL

i

5. Use the smoothed stochastic gradient to calculate λi+1. Repeat.

(cid:80)L−1

j=0

ˆSi−j

(10)

Details are in Algorithm 1. We now explore its properties. First  note that smoothing the sufﬁcient
statistics comes at almost no extra computational costs. In fact  the mean of the stored sufﬁcient
statistics does not explicitly have to be computed  but rather amounts to the update

i ← ˆSL
ˆSL

i−1 + ( ˆSi − ˆSi−L)/L 

(11)

after which ˆSi−L is deleted. Storing the sufﬁcient statistics can be expensive for large values of L:
In the context of LDA involving the typical parameters K = 102 and V = 104  using L = 102
amounts to storing 108 64-bit ﬂoats which is in the Gigabyte range.
Note that when L = 1 we obtain stochastic variational inference (SVI) in its basic form. This
includes deterministic variational inference for L = 1  B = D in the case of sampling without
replacement within the minibatch.

Biased gradients Let us now investigate the algorithm theoretically. Note that the only noisy part
in the stochastic gradient in Eq. (9) is the sufﬁcient statistics. Averaging over L stochastic sufﬁcient
statistics thus promises to reduce the noise in the gradient. We are interested in the effect of the
additional parameter L.

4

When we average over the L most recent sufﬁcient statistics  we introduce a bias. As the variational
parameters change during each iteration  the averaged sufﬁcient statistics deviate in expectation
from its current value. This induces biased gradients. In a nutshell  large values of L will reduce the
variance but increase the bias.
To better understand this tradeoff  we need to introduce some notation. We deﬁned the stochastic
gradient ˆgi = ˆg(λi  Bi) in Eq. (9) and refer to gi = EBi[ˆg(λi  Bi)] as the full gradient (FG). We also
L in Eq. (10). Now  we need to introduce an auxiliary
deﬁned the smoothed stochastic gradient ˆgi
j=0 Si−j. This is the time-averaged full gradient. It involves
variable  gL
i
the full sufﬁcient statistics Si = S(λi) evaluated along the sequence λ1  λ2 ... generated by our
algorithm.
We can expand the smoothed stochastic gradient into three terms:
(cid:125)
(cid:123)(cid:122)
i − gL
i )

:= (η − λi) + 1

i = gi(cid:124)(cid:123)(cid:122)(cid:125)

(cid:125)
(cid:123)(cid:122)
i − gi)

(cid:80)L−1

+ (ˆgL

+ (gL

(12)

(cid:124)

(cid:124)

ˆgL

L

FG

bias

noise

E[(ˆgL

i − gi)2] ≈ E[(ˆgL

This involves the full gradient (FG)  a bias term and a stochastic noise term. We want to minimize
the statistical error between the full gradient and the smoothed gradient by an optimal choice of L.
We will show this the optimal choice is determined by a tradeoff between variance and bias.
For the following analysis  we need to compute expectation values with respect to realizations of
our algorithm  which is a stochastic process that generates a sequence of λi’s. Those expectation
values are denoted by E[·]. Notably  not only the minibatches Bi are random variables under this
expectation  but also the entire sequences λ1  λ2  ... . Therefore  one needs to keep in mind that even
the full gradients gi = g(λi) are random variables and can be studied under this expectation.
We ﬁnd that the mean squared error of the smoothed stochastic gradient dominantly decomposes
into a mean squared bias and a noise term:

To see this  consider the mean squared error of the smoothed stochastic gradient with respect to the
full gradient  E[(ˆgL
i + gL

i − gi)2(cid:3) .

(cid:125)
(cid:123)(cid:122)
i − gL
i − gi)2(cid:3) = E(cid:2)(ˆgL
i )2(cid:3) + 2 E(cid:2)(ˆgL
i − gi)2]  adding and subtracting gL
i :
(cid:80)L−1
E(cid:104)
L−1(cid:88)
E(cid:2)(ˆgL
(cid:105) E(cid:2)(gL
i − gi)(cid:3) = 0. Therefore  the cross-term

(cid:125)
(cid:123)(cid:122)
i − gi)2]
i − gi)(cid:3) + E(cid:2)(gL
(cid:105)

i − gi)(cid:3) =
(cid:105) ≈ E(cid:104)

We encounter a cross-term  which we argue to be negligible. In deﬁning ∆ ˆSi = ( ˆSi − Si) we ﬁnd
that (ˆgL

The ﬂuctuations of the sufﬁcient statistics ∆ ˆSi is a random variable with mean zero  and the ran-
i − gi) enters only via λi. One can assume a very small statistical correlation between
domness of (gL

those two terms  E(cid:104)

j=0 ∆Si−j. Therefore 

E(cid:2)(ˆgL

+ E[(gL

i − gi)

i − gi)

∆ ˆSi−j(gL

∆ ˆSi−j(gL

i − gL

i )(gL

i − gL

i )(gL

i − gL

i ) = 1
L

i − gL

i − gL

mean squared bias

∆ ˆSi−j

i )2]

1
L

j=0

(13)

variance

(cid:124)

(cid:124)

.

can be expected to be negligible. We conﬁrmed this fact empirically in our numerical experiments:
the top row of Fig. 1 shows that the sum of squared bias and variance is barely distinguishable from
the squared error.
By construction  all bias comes from the sufﬁcient statistics:

E[(gL

i − gi)2] = E

(cid:20)(cid:16) 1

L

(cid:80)L−1
j=0 (Si−j − Si)

(cid:17)2(cid:21)

.

(14)

At this point  little can be said in general about the bias term  apart from the fact that it should shrink
with the learning rate. We will explore it empirically in the next section. We now consider the
variance term:

(cid:20)(cid:16) 1

L

(cid:80)L−1

j=0 ∆ ˆSi−j

(cid:17)2(cid:21)

E[(ˆgL

i − gL

i )2] = E

E(cid:104)
(∆ ˆSi−j)2(cid:105)

L−1(cid:88)

j=0

L−1(cid:88)

j=0

=

1
L2

E[(ˆgi−j − gi−j)2].

=

1
L2

5

Figure 1: Empirical test of the variance-bias tradeoff on 2 000 abstracts from the Arxiv repository
(ρ = 0.01  B = 300). Top row. For ﬁxed L = 30 (left)  L = 100 (middle)  and L = 300
(right)  we compare the squared bias  variance  variance+bias and the squared error as a function
of iterations. Depending on L  the variance or the bias give the dominant contribution to the error.
Bottom row. Squared bias (left)  variance (middle) and squared error (right) for different values of
L. Intermediate values of L lead to the smallest squared error and hence to the best tradeoff between
small variance and small bias.

This can be reformulated as var(ˆgL
little during those L successive updates  we can approximate var(ˆgi−j) ≈ var(ˆgi)  which yields

j=0 var(ˆgi−j). Assuming that the variance changes

i ) = 1
L2

var(ˆgL

i ) ≈ 1
L

var(ˆgi).

(15)

The smoothed gradient has therefore a variance that is approximately L times smaller than the
variance of the original stochastic gradient.

(cid:80)L−1

Bias-variance tradeoff To understand and illustrate the effect of L in our optimization problem 
we used a small data set of 2000 abstracts from the Arxiv repository. This allowed us to compute
the full sufﬁcient statistics and the full gradient for reference. More details on the data set and the
corresponding parameters will be given below.
We computed squared bias (SB)  variance (VAR) and squared error (SE) according to Eq. (13) for a
single stochastic optimization run. More explicitly 

K(cid:88)

V(cid:88)

(cid:0)ˆgL

(cid:1)2

K(cid:88)

V(cid:88)

(cid:0)ˆgL

(cid:1)2

K(cid:88)

V(cid:88)

(cid:0)gL

i − gi

(cid:1)2

SBi =

kv   VARi =

i − gL

i

kv   SEi =

i − gi

kv .

(16)

k=1

v=1

k=1

v=1

k=1

v=1

In Fig. 1  we plot those quantities as a function of iteration steps (time). As argued before  we arrive
at a drastic variance reduction (bottom  middle) when choosing large values of L.
In contrast  the squared bias (bottom  left) typically increases with L. The bias shows a complex
time-evolution as it maintains memory of L previous steps. For example  the kinks in the bias
curves (bottom  left) occur at iterations 3  10  30  100 and 300  i.e. they correspond to the values of
L. Those are the times from which on the smoothed gradient looses memory of its initial state  typ-
ically carrying a large bias. The variances become approximately stationary at iteration L (bottom 
middle). Those are the times where the initialization process ends and the queue Q in Algorithm 1
has reached its maximal length L. The squared error (bottom  right) is to a good approximation just
the sum of squared bias and variance. This is also shown in the top panel of Fig. 1.

6

Due to the long-time memory of the smoothed gradients  one can associate some ”inertia” or ”mo-
mentum” to each value of L. The larger L  the smaller the variance and the larger the inertia. In
a non-convex optimization setup with many local optima as in our case  too much inertia can be
harmful. This effect can be seen for the L = 100 and L = 300 runs in Fig. 1 (bottom)  where the
mean squared bias and error curves bend upwards at long times. Think of a marble rolling in a wavy
landscape: with too much momentum it runs the danger of passing through a good optimum and
eventually getting trapped in a bad local optimum. This picture suggests that the optimal value of
L depends on the ”ruggedness” of the potential landscape of the optimization problem at hand. Our
empirical study suggest that choosing L between 10 and 100 produces the smallest mean squared
error.

Aside: connection to gradient averaging Our algorithm was inspired by various gradient aver-
aging schemes. However  we cannot easily used averaged gradients in SVI. To see the drawbacks of
gradient averaging  let us consider L stochastic gradients ˆgi  ˆgi−1  ˆgi−2  ...  ˆgi−L+1 and replace

ˆgi −→ 1

j=0 ˆgi−j.

L

(cid:80)L−1
L−1(cid:88)

η +

L−1(cid:88)

j=0

 .

(17)

(18)

One arrives at the following parameter update for λi:

λi+1 = (1 − ρi)λi + ρi

1
L

j=0

ˆSi−j − 1
L

(λi−j − λi)

This update can lead to the violation of optimization constraints  namely to a negative variational
parameter λ. Note that for L = 1 (the case of SVI)  the third term is zero  guaranteeing positivity of
the update. This is no longer guaranteed for L > 1  and the gradient updates will eventually become
negative. We found this in practice. Furthermore  we ﬁnd that there is an extra contribution to the
bias compared to Eq. (14) 

(cid:20)(cid:16) 1

L

(cid:80)L−1
j=0 (λi − λi−j) + 1

L

(cid:80)L−1
j=0 (Si−j − Si)

(cid:17)2(cid:21)

E[(gL

i − gi)2] = E

.

(19)

Hence  the averaged gradient carries an additional bias in λ - it is the same term that may violate
optimization constraints. In contrast  the variance of the averaged gradient is the same as the variance
of the smoothed gradient. Compared to gradient averaging  the smoothed gradient has a smaller bias
while proﬁting from the same variance reduction.

3 Empirical study

We tested SVI for LDA  using the smoothed stochastic gradients  on three large corpora:

• 882K scientiﬁc abstracts from the Arxiv repository  using a vocabulary of 14K words.
• 1.7M articles from the New York Times  using a vocabulary of 8K words.
• 3.6M articles from Wikipedia  using a vocabulary of 7.7K words.

We set the minibatch size to B = 300 and furthermore set the number of topics to K = 100  and
the hyper-parameters α = η = 0.5. We ﬁxed the learning rate to ρ = 10−3. We also compared our
results to a decreasing learning rate and found the same behavior.
For a quantitative test of model ﬁtness  we evaluate the predictive probability over the vocabulary [1].
To this end  we separate a test set from the training set. This test set is furthermore split into two
parts: half of it is used to obtain the local variational parameters (i.e. the topic proportions by ﬁtting
LDA with the ﬁxed global parameters λ). The second part is used to compute the likelihoods of the
contained words:
p(wnew|wold  D) ≈

q(Θ)q(β)dΘdβ = (cid:80)K

(cid:90) (cid:16)(cid:80)K

Eq[θk]Eq[βk wnew ].

k=1 Θkβk wnew

(cid:17)

(20)

k=1

We show the predictive probabilities as a function of effective passes through the data set in Fig. 2
for the New York Times  Arxiv  and Wikipedia corpus  respectively. Effective passes through the
data set are deﬁned as (minibatch size * iterations / size of corpus). Within each plot  we compare

7

Figure 2: Per-word predictive probabilitiy as a function of the effective number of passes through
the data (minibatch size * iterations / size of corpus). We compare results for the New York Times 
Arxiv  and Wikipedia data sets. Each plot shows data for different values of L. We used a constant
learning rate of 10−3  and set a time budget of 24 hours. Highest likelihoods are obtained for L
between 10 and 100  after which strong bias effects set in.

different numbers of stored sufﬁcient statistics  L ∈ {1  10  100  1000  10000 ∞}. The last value
of L = ∞ corresponds to a version of the algorithm where we average over all previous sufﬁcient
statistics  which is related to averaged gradients (AG)  but which has a bias too large to compete
with small and ﬁnite values of L. The maximal values of 30  5 and 6 effective passes through the
Arxiv  New York Times and Wikipedia data sets  respectively  approximately correspond to a run
time of 24 hours  which we set as a hard cutoff in our study.
We obtain the highest held-out likelihoods for intermediate values of L. E.g.  averaging only over
10 subsequent sufﬁcient statistics results in much faster convergence to higher likelihoods at very
little extra storage costs. As we discussed above  we attribute this fact to the best tradeoff between
variance and bias.

4 Discussion and Conclusions

SVI scales up Bayesian inference  but suffers from noisy stochastic gradients. To reduce the mean
squared error relative to the full gradient  we averaged the sufﬁcient statistics of SVI successively
over L iteration steps. The resulting smoothed gradient is biased  however  and the performance
of the method is governed by the competition between bias and variance. We argued theoretically
and showed empirically that for a ﬁxed time budget  intermediate values of the number of stored
sufﬁcient statistics L give the highest held-out likelihoods.
Proving convergence for our algorithm is still an open problem  which is non-trivial especially be-
cause the variational objective is non-convex. To guarantee convergence  however  we can simply
phase out our algorithm and reduce the number of stored gradients to one as we get close to conver-
gence. At this point  we recover SVI.

Acknowledgements We thank Laurent Charlin  Alp Kucukelbir  Prem Gopolan  Rajesh Ran-
ganath  Linpeng Tang  Neil Houlsby  Marius Kloft  and Matthew Hoffman for discussions. We
acknowledge ﬁnancial support by NSF CAREER NSF IIS-0745520  NSF BIGDATA NSF IIS-
1247664  NSF NEURO NSF IIS-1009542  ONR N00014-11-1-0651  the Alfred P. Sloan founda-
tion  DARPA FA8750-14-2-0009 and the NSF MRSEC program through the Princeton Center for
Complex Materials Fellowship (DMR-0819860).

8

References
[1] Matthew D Hoffman  David M Blei  Chong Wang  and John Paisley. Stochastic variational

inference. The Journal of Machine Learning Research  14(1):1303–1347  2013.

[2] Prem Gopalan  Jake M Hofman  and David M Blei. Scalable recommendation with Poisson

factorization. Preprint  arXiv:1311.1704  2013.

[3] Prem K Gopalan and David M Blei. Efﬁcient discovery of overlapping communities in massive

networks. Proceedings of the National Academy of Sciences  110(36):14534–14539  2013.

[4] Edoardo M Airoldi  David M Blei  Stephen E Fienberg  and Eric P Xing. Mixed membership
stochastic blockmodels. In Advances in Neural Information Processing Systems  pages 33–40 
2009.

[5] James Hensman  Nicolo Fusi  and Neil D Lawrence. Gaussian processes for big data. Uncer-

tainty in Artiﬁcial Intelligence  2013.

[6] Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Math-

ematical Statistics  pages 400–407  1951.

[7] Chong Wang  Xi Chen  Alex Smola  and Eric Xing. Variance reduction for stochastic gradient
optimization. In Advances in Neural Information Processing Systems  pages 181–189  2013.
[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.
[9] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. Technical report  HAL 00860051  2013.

[10] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Pro-

gramming  120(1):221–259  2009.

[11] Masa-Aki Sato. Online model selection based on the variational Bayes. Neural Computation 

13(7):1649–1681  2001.

[12] Mirwaes Wahabzada and Kristian Kersting. Larger residuals  less work: Active document
scheduling for latent Dirichlet allocation. In Machine Learning and Knowledge Discovery in
Databases  pages 475–490. Springer  2011.

[13] Paul Tseng. An incremental gradient (-projection) method with momentum term and adaptive

stepsize rule. SIAM Journal on Optimization  8(2):506–531  1998.

[14] Matthew Hoffman  Francis R Bach  and David M Blei. Online learning for latent Dirichlet

allocation. In Advances in Neural Information Processing Systems  pages 856–864  2010.

[15] Martin J Wainwright and Michael I Jordan. Graphical models  exponential families  and vari-

ational inference. Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[16] Christopher M Bishop et al. Pattern Recognition and Machine Learning  volume 1. Springer

New York  2006.

[17] David M Blei  Andrew Y Ng  and Michael I Jordan. Latent Dirichlet allocation. The Journal

of Machine Learning Research  3:993–1022  2003.

9

,Brendan McMahan
Jacob Abernethy
Stephan Mandt
David Blei
Shakir Mohamed
Danilo Jimenez Rezende
Stéphanie van der Pas
Veronika Ročková
Faidra Georgia Monachou
Itai Ashlagi