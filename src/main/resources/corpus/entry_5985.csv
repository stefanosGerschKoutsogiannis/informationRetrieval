2014,Zeta Hull Pursuits: Learning Nonconvex Data Hulls,Selecting a small informative subset from a given dataset  also called column sampling  has drawn much attention in machine learning. For incorporating structured data information into column sampling  research efforts were devoted to the cases where data points are fitted with clusters  simplices  or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls  a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure  we present a greedy algorithmic framework  dubbed Zeta Hulls  to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well  we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.,Zeta Hull Pursuits:

Learning Nonconvex Data Hulls

Yuanjun Xiong† Wei Liu‡ Deli Zhao(cid:2) Xiaoou Tang†

†Information Engineering Department  The Chinese University of Hong Kong  Hong Kong

‡IBM T. J. Watson Research Center  Yorktown Heights  New York  USA

{yjxiong xtang}@ie.cuhk.edu.hk weiliu@us.ibm.com deli zhao@htc.com

(cid:2)Advanced Algorithm Research Group  HTC  Beijing  China

Abstract

Selecting a small informative subset from a given dataset  also called column sam-
pling  has drawn much attention in machine learning. For incorporating structured
data information into column sampling  research efforts were devoted to the cases
where data points are ﬁtted with clusters  simplices  or general convex hulls. This
paper aims to study nonconvex hull learning which has rarely been investigated in
the literature. In order to learn data-adaptive nonconvex hulls  a novel approach
is proposed based on a graph-theoretic measure that leverages graph cycles to
characterize the structural complexities of input data points. Employing this mea-
sure  we present a greedy algorithmic framework  dubbed Zeta Hulls  to perform
structured column sampling. The process of pursuing a Zeta hull involves the
computation of matrix inverse. To accelerate the matrix inversion computation
and reduce its space complexity as well  we exploit a low-rank approximation to
the graph adjacency matrix by using an efﬁcient anchor graph technique. Exten-
sive experimental results show that data representation learned by Zeta Hulls can
achieve state-of-the-art accuracy in text and image classiﬁcation tasks.

Introduction

1
In the era of big data  a natural idea is to select a small subset of m samples Ce = {xe1
}
  . . .   xem
from a whole set of n data points X = {x1  . . .   xn} such that the selected points Ce can capture
the underlying properties or structures of X . Then machine learning and data mining algorithms can
be carried out with Ce instead of X   thereby leading to signiﬁcant reductions in computational and
space complexities. Let us write the matrix forms of Ce and X as C = [xe1   . . .   xem] ∈ R
d×m
and X = [x1  . . .   xn] ∈ R
d×n  respectively. Here d is the dimensions of input data points. In
other words  C is a column subset selection of X. The task of selecting C from X is also called by
column sampling in the literature  and maintains importance in a variety of ﬁelds besides machine
learning  such as signal processing  geoscience and remote sensing  and applied mathematics. This
paper concentrates on solving the column sampling problem by means of graph-theoretic methods.
Existing methods in column sampling fall into two main categories according to their objectives: 1)
approximate the data matrix X  and 2) discover the underlying data structures. For machine learning
methods using kernel or similar “N-Body” techniques  the Nystr¨om matrix approximation is usually
applied to approximate large matrices. Such circumstances include fast training of nonlinear kernel
support vector machines (SVM) in the dual form [30]  spectral clustering [8]  manifold learning [25] 
etc. Minimizing a relative approximation error is typically harnessed as the objective of column sam-
pling  by which the most intuitive solution is to perform uniform sampling [30]. Other non-uniform
sampling schemes choose columns via various criteria  such as probabilistic samplings according
to diagonal elements of a kernel matrix [7]  reconstruction errors [15]  determinant measurements
[1]  cluster centroids [33]  and statistical leverage scores [21]. On the other hand  column sampling

1

may be cast into a combinatorial optimization problem  which can be tackled by using greedy strate-
gies in polynomial time [4] and boosted by using advanced sampling strategies to further reduce the
relative approximation error [14].
From another perspective  we are aware that data points may form some interesting structures. Un-
derstanding these structures has been proven beneﬁcial to approximate or represent data inputs [11].
One of the most famous algorithms for dimensionality reduction  Non-negative Matrix Factorization
(NMF) [16]  learns a low-dimensional convex hull from data points through a convex relaxation [3].
This idea was extended to signal separation by pursuing a convex hull with a maximized volume
[27] to enclose input data points. Assuming that vertices are equally distant  the problem of ﬁtting
a simplex with a maximized volume to data reduces to a simple greedy column selection procedure
[26]. The simplex ﬁtting approach demonstrated its success in face recognition tasks [32]. Paral-
lel research in geoscience and remote sensing is also active  where the vertices of a convex hull
are coined as endmembers or extreme points  leading to a classic “N-Finder” algorithm [31]. The
above approaches tried to learn data structures that are usually characterized by convexity. Hence 
they may fail to reveal the intrinsic data structures when the distributions of data points are diverse 
e.g.  data being on manifolds or concave structures. Probabilistic models like Determinantal Point
Process (DPP) [13] measure data densities  so they are likely to overcome the convexity issue. How-
ever  few previous work accessed structural information of possibly nonconvex data for column
sampling/subset selection tasks.
This paper aims to address the issue of learning nonconvex structures of data in the case where
the data distributions can be arbitrary. More speciﬁcally  we learn a nonconvex hull to encapsulate
the data structure. The on-hull points tightly enclose the dataset but do not need to form a convex
set. Thus  nonconvex hulls can be more adaptive to capture practically complex data structures.
Akin to convex hull learning  our proposed approach also extracts extreme points from an input
dataset. To complete this task  we start with exploring the property of graph cycles in a neighborhood
graph built over the input data points. Using cycle-based measures to characterize data structures
has been proven successful in clustering data of multiple types of distributions [34]. To induce a
measure of structural complexities stemming from graph cycles  we introduce the Zeta Function
which applies the integration of graph cycles to model the linkage properties of the neighborhood
graph. The key advantage of the Zeta function is uniting both global and local connection properties
of the graph. As such  we are able to learn a hull which encompasses almost all input data points
but is not necessary to be convex. With structural complexities captured in the form of the Zeta
function  we present a leave-one-out strategy to ﬁnd the extreme points. The basic idea is that
removing the on-hull points only has weak impact on structural complexities of the graph. The
decision of removal will be based on extremeness of a data point. Our model  dubbed Zeta Hulls  is
derived by computing and analyzing the extremeness of data points. The greedy pursuit of the Zeta
Hull model requires the computation of the inversion of a matrix obtained from the graph afﬁnity
matrix  which is computationally prohibitive for massive-scale data. To accelerate such a matrix
manipulation  we employ the Anchor Graph [18] technique in the sense that the original graph can
be approximated with respect to the anchors originating from a randomly sampled data subset. Our
model is testiﬁed through extensive experiments on toy data and real-world text and image datasets.
Experimental results show that in terms of unsupervised data representation learning  the Zeta Hull
based methods outperform the state-of-the-art methods used in convex hull learning  clustering 
matrix factorization  and dictionary learning.

2 Nonconvex Hull Learning

To elaborate on our approach  we ﬁrst introduce and deﬁne the point extremeness. It measures the
degree of a data point being prone to lie on or near a nonconvex hull by virtue of a neighborhood
graph drawn from an input dataset. As an intuitive criterion  the data point with strong connections
in the graph should have the low point extremeness. To obtain the extremeness measure  we need to
explore the underlying structure of the graph  where graph cycles are employed.

2.1 Zeta Function and Structural Complexity

We model graph cycles by means of a sum-product rule and then integrate them using a Zeta func-
tion. There are many variants of original Riemann Zeta Function  one of which is specialized in

2

(a) Original Graph

(b) Remaining Graph

Figure 1: An illustration of pursuing on-hull points using the graph measure. (a) shows a point set
with a k-nearest neighbor graph. Points in red are ones lying on the hull of the point set  e.g.  the
points we tend to select by the Zeta Hull Pursuit algorithm. (b) shows the remaining point set and the
graph after removing the on-hull points together with their corresponding edges. We observe that
the removal of the on-hull (i.e.  “extreme”) points yields little impact on the structural complexity
of the graph.

weighted adjacency graphs. Applying the theoretical results of Zeta functions provides us a pow-
erful tool for characterizing structural complexities of graphs. The numerical description of graph
structures will play a critical role in column sampling/subset selection tasks.
Formally  given a graph G(X   E) with n nodes being data points in X = {xi}n
i=1  let the n × n
matrix W denote the weighted adjacency (or afﬁnity) matrix of the graph G built over the dataset X .
Usually the graph afﬁnities are calculated with a proper distance metric. To be generic  we assume
that G is directed. Then an edge leaving from xi to xj is denoted as eij. A path of length (cid:2) from
xi to xj is deﬁned as P (i  j  (cid:2)) = {ehktk
}(cid:3)
k=1 with h1 = i and t(cid:3) = j. Note that the nodes in
(cid:2)
this path can be duplicate. A graph cycle  as a special case of paths of length (cid:2)  is also deﬁned as
γ(cid:3) = P (i  i  (cid:2)) (i = 1  . . .   n). The sum-product path afﬁnity ν(cid:3) for all (cid:2)-length cycles can then
be computed by ν(cid:3) =
  where κ(cid:3) denotes the set of all
possible cycles of length (cid:2) and whktk denotes the (hk  tk)-entry of W  i.e.  the afﬁnity from node
xhk to node xtk. The edge et(cid:2)−1h1 is the last edge that closes the cycle. The computed compound
afﬁnity ν(cid:3) provides a measure for all cyclic connections of length (cid:2). Then we integrate such afﬁnities
for the cycles of lengths being from one to inﬁnity to derive the graph Zeta function as follows 

(cid:3)(cid:3)−1

k=1 whktk

γ(cid:2)∈κ(cid:2)

νγ(cid:2) =

γ(cid:2)∈κ(cid:2)

wt(cid:2)−1h1

(cid:2)

(cid:6)

(cid:4) ∞(cid:5)

(cid:3)=1

ν(cid:3)

z(cid:3)
(cid:2)

ζz(G) = exp

 

(1)

where z is a constant. We only consider the situation where z is real-valued. The Zeta function
in Eq. (1) has been proven to enjoy a closed form. Its convergence is also guaranteed when z <
1/ρ(W)  where ρ(W) is the spectral radius of W. These lead to Theorem 1 [23].
Theorem 1. Let I be the identity matrix and ρ(W) be the spectral radius of the matrix W  respec-
tively. If 0 < z < 1/ρ(W)  then ζz(G) = 1/ det(I − zW).
Note that W can be asymmetric  implying that λi can be complex. In this case  Theorem 1 still
holds. Theorem 1 indicates that the graph Zeta function we formulate in Eq. (1) provides a closed-
form expression for describing the structural complexity of a graph. The next subsection will give
the deﬁnition of the point extremeness by analyzing the structural complexity.

2.2 Zeta Hull Pursuits

From now on  for simplicity we use G = ζz(G) to represent the structural complexity of the original
graph G. To measure the point extremeness numerically  we perform a leave-one-out strategy in the
sense that each point in C is successively left out and the variation of G is investigated. This is a
natural way to pursue extreme points  because if a point xj lies on the hull it has few communications
with the other points. After removing this point and its corresponding edges  the reductive structural
complexity of the remaining graph G/xj  which we denote as G/xj   will still be close to G. Hence 
the point extremeness εxj is modeled as the relative change of the structural complexity G  that is
εxj = G
G/xj
Theorem 2. Given G and G/xj as in Theorem 1  the point extremeness measure εxj of point xj
satisﬁes εxj = (I − zW)
−1
(jj)  i.e.  the point extremeness measure of point xj is equal to the j-th
diagonal entry of the matrix (I − zW)

. Now we have the following theorem.

−1.

3

Algorithm 1 Zeta Hull Pursuits

Input: A dataset X   the number m of data points to be selected  and free parameters z  λ and k.
Output: The hull of sampled columns Ce := Cm+1.
Initialize: construct W  C1 ← ∅  X1 = X   c1 = 0  and W1 = W
for i = 1 to m do

(jj)  for xj ∈ Xi
−1
i e(cid:4)

εxj := (I − zWi)
xei := arg minxj∈Xi (εxj + λ
Ci+1 := Ci ∪ xei
ci+1 := ci + eei
Xi+1 := Ci/xei
Wi+1 := Wi with the ei-th row and column removed

j Wci)

end for

  . . .   xem

(cid:2)m

C⊂X g(C) + λc(cid:4)Wc 

According to previous analysis  the data point with a small εxj tends to be on the hull and therefore
has a strong extremeness. To seek the on-hull points  we need to select a subset of m points Ce =
{xe1
} from X such that they have the strongest point extremenesses. We formulate this
goal into the following optimization problem:
Ce = arg min

(2)
where c is a selection vector with m nonzero elements cei = 1 (i = 1  . . .   m)  and g(C) is the
function which measures the impact on the structural complexity after removing the extracted points.
In our case  g(C) =
i=1 εxci . The second term in Eq. (2) is a regularization term enforcing that
the selected data points do not intersect with each other. It will enable the selection process to have
a better representative capability. The parameter λ controls the extent of the regularization.
Naively solving the combinatorial optimization problem in Eq. (2) requires exponential time. By
adopting a greedy strategy  we can solve this optimization problem in an iterative manner and with
a feasible time complexity. Speciﬁcally  in each iteration we extract one point from the current data
set and add it to the subset of the selected points. Sticking to this greedy strategy  we will attain the
desired m on-hull points after m iterations. In the i-th iteration  we extract the point xei according
to the criterion

xei = arg min
xj∈Xi−1

(3)
where ej is the j-th standard basis vector  and ci−1 is the selection vector according to i− 1 selected
points before the i-th iteration.
We name our algorithm Zeta Hull Pursuits in order to emphasize that we use the Zeta function to
pursue the nonconvex data hull. Algorithm 1 summarizes the Zeta Hull Pursuits algorithm.

εxj +

e(cid:4)
j Wci−1 

λ
i

3 Zeta Hull Pursuits via Anchors
Algorithm 1 is applicable to small to medium-scale data X due to its cubical time complexity and
quadratic space complexity with respect to the data size |X|. Here we propose a scalable algorithm
facilitated by a reasonable prior to tackle the nonconvex hull learning problem efﬁciently. The idea is
to build a low-rank approximation to the graph adjacency matrix W with a small number of sampled
data points  namely anchor points. We resort to the Anchor Graph technique [18]  which has been
successfully applied to handle large-scale hashing[20] and semi-supervised learning problems.

3.1 Anchor Graphs
The anchor graph framework is an elegant way to approximate neighborhood graphs. It ﬁrst chooses
a subset of l anchor points U = {uj}l
j=1 from X . Then for each data point in X   its s nearest anchors
in U are sought  thereby forming an s-nearest anchor graph. The anchor graph theory assumes that
the original graph afﬁnity matrix W can be reconstructed from the anchor graph with a small number
of anchors (l (cid:5) n). Anchor points can be selected by random sampling or a rough clustering
process. Many algorithms are available to embed a data point to its s nearest anchor points  as
suggested in [18]. Here we adopt the simplest approach to build the anchor embedding matrix ˆH;
say  ˆhij =
  where dij is the distance from data

  uj ∈ {s nearest anchors of xi}

(cid:8)−d2

ij/σ2

(cid:7)

(cid:9)

exp
0 

otherwise

4

Algorithm 2 Anchor-based Zeta Hull Pursuits

Input: A dataset X   the number m of data points to be sampled  the number l of anchors  the
number s of nearest anchors  and a free parameter z.
Output: The hull of sampled columns Ce := Cm+1.
Initialize: construct H  X1 = X   C1 = ∅  and H1 = H
for i = 1 to m do

(cid:2)l

λ2
j

k=1

1−zλ2

(cid:2)

(Ujk)2  for xj ∈ Xi
(cid:4)
λ
i hjht

perform SVD to obtain Hi := UΣVT
εxj := z
xei := arg minxj∈Xi (εxj +
Ci+1 := Ci ∪ xei
Xi+1 := Xi/xei
Hi+1 := Hi with the ei-th row removed

xt∈Ci

k

)

end for

point xi to anchor uj  and σ is a parameter controlling the bandwidth of the exponential function.
The matrix ˆH is then normalized so that its every row sums to one. In doing so  we can approximate
the afﬁnity matrix of the original graph as ˆW = ˆHΛ−1 ˆH(cid:4)  where Λ is a diagonal matrix whose i-th
diagonal element is equal to the sum of the i-th column of ˆH. As a result  all matrix manipulations
upon the original graph afﬁnity matrix W can be approximated by substituting the anchor graph
afﬁnity matrix ˆW for W.

−1. Using the anchor graph technique  we can write (I − zW)

3.2 Extremeness Computation via Anchors
Note that the computation of the point extremeness for εxj depends on the diagonal elements of
(I − zW)
−1 
where H = ˆHΛ− 1
2 . Thus we have the following theorem that enables an efﬁcient computation of
εxj . The proof is detailed in the supplementary material.
Theorem 3. Let the singular vector decomposition of H be H = UΣV(cid:4)
diag(λ1  . . .   λl).
HVΣ−1 and Ujk denotes the (i  j)-th entry of U.

If H(cid:4)H is not singular  then ε−1

  where Σ =
(Ujk)2  where U =

−1 = (I − zHH(cid:4)

xj = 1 + z

(cid:2)l

1−zλ2

k=1

λ2
k

)

k

(cid:2)

Theorem 3 reveals that the major computation of εxj will reduce to the eigendecomposition of a
much smaller matrix H(cid:4)H  which results in a direct acceleration of the Zeta hull pursuit process.
At the same time  the second term of Eq. (3) encountered in the i-th iteration can be estimated by
(cid:4)  where hj denotes the j-th row of H and ci−1 is the selection vector
e(cid:4)
j Wci = 1
i
of the extracted point set before the i-th iteration. These lead to the Anchor-based Zeta Hull Pursuits
algorithm shown in Algorithm 2.

xt∈Ci

hjht

3.3 Downdating SVD
In Algorithm 2  the singular value decomposition dominates the total time cost. We notice that
reusing information in previous iterations can save the computation time. The removal of one row
from H is equivalent to a rank-one modiﬁcation to the original matrix. Downdating SVD [10] was
proposed to handle this operation. Given the diagonal singular value matrix Σi and the point xei
chosen in the i-th iteration  the singular value matrix Σi+1 for the next iteration can be calculated
by the eigendecomposition of an l × l matrix D derived from Σi  where D = (I − 1
ei )Σi 
(cid:6)2
and μ2 + (cid:6)hei
2 = 1. The decomposition of D can be efﬁciently performed in O(l2) time [10].
Then the computation of Ui+1 is achieved by a multiplication of Ui with an l × l matrix produced
by the decomposition operation on D  which permits a natural parallelism. Consequently  we can
further accelerate Algorithm 2 by using a parallel computing scheme.

1+μ hei h(cid:4)

3.4 Complexity Analysis
We now analyze the complexities of Algorithms 1 and 2. For Algorithm 1  the most time-consuming
step is to solve the matrix inverse of n × n size  which costs a time complexity of O(n3). The
overall time complexity is thus O(mn3) for extracting m points. In the implementation we can use

5

(a) m = 20  ZHP

(b) m = 40  ZHP

(c) m = 80  ZHP

(d) m = 200  ZHP

(e) m = 20  A-ZHP

(f) m = 40  A-ZHP

(g) m = 80  A-ZHP

(h) m = 200  A-ZHP

(k) m = 40  CUR

(l) m = 40  K-medoids

(i) m = 40  Leverage Score

(j) m = 40  Simplex

Figure 2: Zeta hull pursuits on the two-moon toy dataset. We select m data points from the dataset
with various methods. In the sub-ﬁgures  blue dots are data points. The selected samples are sur-
rounded with red circles. The caption of each sub-ﬁgure describes the number of selected points m
and the method used to select those data points. First two rows shows the results of our algorithms
with different m. The third row illustrates the comparisons with other methods when m = 40. For
the leverage score approach  we follow the steps in [21].
the sparse matrix computation to reduce the constant factor [5]. For Algorithm 2  the most time-
consuming step is to perform SVD over H  so the overall time complexity is O(mnl2). Leveraging
downdating SVD  we only need to calculate the full SVD of H once in O(nl2) time and iteratively
update the decomposition in O(l2) time per iteration. The matrix multiplication operation then
dominates the total time cost. Also  it can be parallelized using a multi-core CPU or a modern GPU 
resulting in a very small constant factor in the time complexity. Since l is usually less than 10% of n 
Algorithm 2 is orders of magnitude faster than Algorithm 1. For cases where l needs to be relatively
large (20% of n for example)  the computational cost will not show a considerable increase since H
is usually a very sparse matrix.
4 Experiments
The Zeta Hull model aims at learning the structures of dataset. We evaluate how well our model
achieves this goal by performing classiﬁcation experiments. For simplicity  we abbreviate our al-
gorithms as follows: the original Zeta Hull Pursuit algorithm (Algorithm 1)  ZHP and its anchor
version (Algorithm 2)  A-ZHP. To compare with the state-of-the-art  we choose some renowned
methods: K-medoids  CUR matrix factorization (CUR) [29]  simplex volume maximization (Sim-
plex) [26]  sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization
(C-NMF) [6]. Basically  we use the extracted data points to learn a representation for each data
point in an unsupervised manner. Classiﬁcation is done by feeding the representation into a clas-
siﬁer. The representation will be built in two ways: 1) the sparse coding [22] and 2) the locality
simplex coding [26]. To differentiate our algorithms from the original anchor graph framework  we
conduct a set of experiments using the left singular vectors of the anchor embedding matrix H as
the representation. In these experiments  anchors used in the anchor graph technique are randomly
selected from the training set. To compare with existing low-dimension embedding approaches  we
run the Large-Scale Manifold method [24] using the same number of landmarks as that of extracted
points.
4.1 Toy Dataset
First we illustrate our algorithms on a toy dataset. The dataset  commonly known as ”the two
moons”  consists of 2000 data points on the 2D plane which are manifold-structured and comprise
nonconvex distributions. This experiment on the two moons provides illustrative results of our
algorithms in the presence of nonconvexity. We select different numbers of column subsets m =
{20  40  80  200} and compare with various other methods. A visualization of the results is shown
in Figure 2. We can see that our algorithms can extract the nonconvex hull of the data cloud more
accurately.
4.2 Text and Image Datasets
For the classiﬁcation experiments in this section  we derive the two types of data representations (the
sparse coding and the local simplex coding) from the points/columns extracted by compared meth-

6

Table 1: Classiﬁcation error rates in percentage (%) on texts (TDT2 and Newsgroups) and hand-
written number datasets (MNIST). The numbers in bold font highlight best results under the settings.
In this table  “SC” refers to the results using the sparse coding to form the representation  while
“LSC” refers to the results using local simplex coding. The cells with “-” indicate that the ZHP
method is too expensive to be performed under the associated settings. The “Anchor Graph” refers
to the additional experiments using the original anchor graph framework [18].

TDT2

Newsgroups

m = 500
SC
2.31
2.52
3.79
3.73
4.83
6.82
9.14

LSC
1.97
2.68
1.73
5.62
3.46
3.73
7.87

m = 1000
LSC
SC
0.48
1.53
0.96
2.08
1.51
1.77
1.18
2.57
2.31
2.07
1.52
2.37
3.73
4.69

m = 500
SC
-

-

LSC

11.79
13.55
9.51
11.68
15.32
19.73

10.77
10.41
10.76
11.83
11.44
12.02

-

m = 1000
SC
LSC
-
7.1
8.16
6.72
7.72
12.38
19.67

6.58
8.04
9.63
7.42
9.47
10.04

MNIST

m = 500
SC
-

-

LSC

m = 2000
SC
LSC
-

-

3.45
5.79
3.16
5.07
10.13
9.28

3.07
5.79
3.16
5.27
10.13
9.28

1.43
2.27
1.36
3.01
3.79
2.72

1.19
1.51
2.11
3.04
5.27
2.31

Methods

ZHP
A-ZHP

Simplex [26]
DictLearn [22]

C-NMF [6]
CUR [29]

K-medoids [12]
Anchor Graph [18]

5.81

2.68

12.32

8.76

3.17

2.33

Table 2: Recognition error rates in percentage (%) on object and face datasets. We select L samples
for each class in the training set for training or forming the gallery. The numbers in bold font
highlight best results under the settings. In this table  “SC” refers to the results using the sparse
coding to form the representation  while “LSC” refers to the results using local simplex coding. The
“Raw Feature” refers to the experiments conducted on the raw features vectors. The face recognition
process is described in Sec. (4.2).

Methods

Caltech101

d = 21504  L = 30

Caltech101

d = 5120  L = 30

MultiPIE

d = 2000  L = 30

A-ZHP

Simplex [26]
DictLearn [22]

C-NMF [6]
CUR [21]

K-medoids [12]
Anchor Graph [18]
Large Manifold [24]
Raw Feature [28]

m = 500
SC
25.77
29.83
26.95
30.66
29.74
27.82

LSC
26.82
26.16
29.73
27.83
28.77
27.64

26.32
28.71

m = 1000
LSC
SC
23.13
25.81
25.18
26.83
26.73
29.51
27.62
28.72
26.81
26.16
26.09
25.73

25.15
27.92

m = 500
SC
29.61
32.43
29.15
32.57
31.69
29.85

LSC
28.95
29.66
31.83
31.13
32.57
29.63

30.53
32.67

m = 1000
LSC
SC
26.59
25.62
27.47
30.62
28.93
29.67
28.73
31.15
31.13
30.72
28.97
28.28

28.14
30.19

m = 500
SC
20.8
19.9
19.6
20.4
21.3
29.7

LSC
14.2
15.8
20.8
17.5
21.9
19.8

17.6
31.4

m = 2000
LSC
SC
11.3
19.6
17.7
13.7
18.5
19.7
14.8
19.9
21.6
20.7
25.4
17.7

14.4
30.1

26.7

31.18

27.6

ods. By measuring the performance of applying these representations to solving the classiﬁcation
tasks  we can evaluate the representative power of the compared point/column selection methods.
The sparse coding is widely used for obtaining the representation for classiﬁcation. Here a standard
(cid:2)1-regularized projection algorithm (LASSO) [22] is adopted to learn the sparse representation from
the extracted data points. LASSO will deliver a sparse coefﬁcient vector  which is applied as the
representation of the data point. We use “SC” to indicate the related results in Table 1 and Table 2.
The local simplex coding reconstructs one data point as a convex combination of a set of nearest
exemplar points  which form local simplexes [26]. Imposing this convex reconstruction constraint
leads to non-negative combination coefﬁcients. The sparse coefﬁcients vector will be used as data
representation. “LSC” indicates the related results in Table 1 and Table 2.
The classiﬁcation pipeline is as follows. After extracting m points/columns from the training set 
all data points will be represented with these selected points using the two approaches above. Then
we feed the representations into a linear SVM for the training and testing. The better classiﬁcation
accuracy will reveal the stronger representative power of the column selection algorithm.
In all
experiments  the parameter z is ﬁxed at 0.05 to guarantee the convergence of the Zeta function. We
ﬁnd that ﬁnal results are robust to z once the convergence is guaranteed. For the A-ZHP algorithm 
the parameter s is ﬁxed at 10 and the number of anchor points l is set as 10% of the training set
size. The bandwidth parameter σ of the exponential function is tuned on the training set to obtain a
reasonable anchor embedding.
The classiﬁcation of text contents relies on the informative representation of the plain words or sen-
tences. Two text datasets are adopted for classiﬁcation  i.e. the TDT2 dataset and the Newsgroups
dataset [2]. In experiments  a subset of TDT2 is used (TDT2-30). It has 9394 samples from 30
classes. Each feature vector is of 36771 dimensions and normalized into unit length. The training
set contains 6000 samples randomly selected from the dataset and rest of the samples are used for

7

testing. The parameter m is set to be 500 and 1000 on this dataset. The Newsgroups dataset con-
tains 18846 samples from 20 classes. The training set contains 11314  while the testing set has 7532.
The two sets are separated in advance [2] and ordered in time sequence to be more challenging for
classiﬁers. The parameter m is set to be 500 and 1000 on this dataset. The classiﬁcation results are
reported in Table 1.
For object and face recognition tasks we conduct experiments under three classic scenarios  the
hand-written digits classiﬁcation  the image recognition  and the human face recognition. Related
experimental results are reported in Table 1 and Table 2.
The MNIST dataset serves as a standard benchmark for machine learning algorithms. It contains 10
classes of images corresponding to hand-written numbers from 0 to 9. The training set has 60000
images and the testing set has 10000 images. Each sample is a 784-dimensional vector.
The Caltech101 dataset [17] is a widely used benchmark for object recognition systems. It consists
of images from 102 classes of objects (101 object classes and one background class). We randomly
select 30 labeled images from every class for training the classiﬁer and 3000 images for testing.
The recognition rates averaged over all classes are reported. Every image is processed into a feature
vector of 21504 dimensions by the method in [28]. We also conduct experiment on a feature subset
In this experiment  m is set to be 500 and 1000.
of the top 5000 dimensions (Caltech101-5k).
On-hull points are extracted on the training set.
The MultiPIE human face dataset is a widely applied benchmark for face recognition [9]. We follow
a standard gallery-probe protocol of face recognition. The testing set is divided into the gallery set
and the probe set. The identity predication of a probe image comes from its nearest neighbor of
Euclidean distance in the gallery. We randomly select 30  000 images of 200 subjects as the training
set for learning the data representation. Then we pick out 3000 images of the other 100 subjects
(L = 30) to form the gallery set and 6000 images as the probes. The head poses of all these faces
are between ±15 degrees. Each face image is processed into a vector of 5000 dimensions using the
local binary pattern descriptor and PCA. We vary the parameter m from 500 to 2000 to evaluate the
inﬂuence of number of sampled points.
Discussion. For the experiments on these high-dimensional datasets  the methods based on the
Zeta Hull model outperform most compared methods and also show promising performance im-
provements over raw data representation. When the number of extracted points grows  the resulting
classiﬁcation accuracy increases. This corroborates that the Zeta Hull model can effectively capture
intrinsic structures of given datasets. More importantly  the discriminative information is preserved
through learning these Zeta hulls. The representation yielded by the Zeta Hull model is sparse and of
manageable dimensionality (500-2000)  which substantially eases the workload of classiﬁer train-
ing. This property is also favorable for tackling other large-scale learning problems. Due to the
graph-theoretic measure that uniﬁes the local and global connection properties of a graph  the Zeta
Hull model leads to better data representation compared against existing graph-based embedding
and manifold learning methods. For the comparison with the Large-Scale Manifold method [24]
on the MultiPIE dataset  we ﬁnd that even using 10K landmarks  its accuracy is still inferior to our
methods relying on the Zeta Hull model. We also notice that noise may also affect the quality of Zeta
hulls. This difﬁculty can be circumvented by running a number of well-established outlier removal
methods such as [19].

5 Conclusion
In this paper  we proposed a geometric model  dubbed Zeta Hulls  for column sampling through
learning nonconvex hulls of input data. The Zeta Hull model was built upon a novel graph-theoretic
measure which quantiﬁes the point extremeness to unify local and global connection properties of
individual data point in an adjacency graph. By means of the Zeta function deﬁned on the graph 
the point extremeness measure amounts to the diagonal elements of a matrix related to the graph
adjacency matrix. We also reduced the time and space complexities for computing a Zeta hull by
incorporating an efﬁcient anchor graph technique. A synthetic experiment ﬁrst showed that the Zeta
Hull model can detect appropriate hulls for non-convexly distributed data. The extensive real-world
experiments conducted on benchmark text and image datasets further demonstrated the superiority
of the Zeta Hull model over competing methods including convex hull learning  clustering  matrix
factorization  and dictionary learning.
Acknowledgement This research is partially supported by project #MMT-8115038 of the Shun
Hing Institute of Advanced Engineering  The Chinese University of Hong Kong.

8

References
[1] M.-A. Belabbas and P. J. Wolfe. Spectral methods in machine learning and new strategies for very large

datasets. PNAS  106(2):369–374  2009.

[2] D. Cai  X. Wang  and X. He. Probabilistic dyadic data analysis with local and global consistency. In Proc.

[3] M. Chu and M. Lin. Low dimensional polytope approximation and its application to nonnegative matrix

factorization. SIAM Journal of Computing  pages 1131–1155  2008.

[4] A. Das and D. Kempe. Submodular meets spectral: greedy algorithms for subset selection  sparse ap-

proximation and dictionary selection. In Proc. ICML  2011.

[5] T. Davis. SPARSEINV: a MATLAB toolbox for computing the sparse inverse subset using the Takahashi

ICML  2009.

equations  2011.

2010.

[6] C. Ding  T. Li  and M. Jordan. Convex and semi-nonnegative matrix factorizations. TPAMI  32(1):45–55 

[7] P. Drineas and M. Mahoney. On the Nystr¨om method for approximating a gram matrix for improved

kernel-based learning. JMLR  6:2153–2175  2005.

[8] C. Fowlkes  S. Belongie  F. Chung  and J. Malik. Spectral grouping using the Nystr¨om method. TPAMI 

[9] R. Gross  I. Matthews  J. Cohn  T. Kanade  and S. Baker. Multi-pie. In Proc. Automatic Face Gesture

26:214–225  2004.

Recognition  pages 1–8  Sept 2008.

[10] M. Gu and S. C. Eisenstat. Downdating the singular value decomposition. SIAM Journal on Matrix

Analysis and Applications  16(3):793–810  1995.

[11] T. Hastie  R. Tibshirani  and J. J. H. Friedman. The elements of statistical learning  volume 1. Springer

[12] L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to cluster analysis  volume

New York  2001.

344. John Wiley & Sons  2009.

in Machine Learning  5(2–3)  2012.

1006  2012.

401(6755):788–791  1999.

[13] A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations and Trends

[14] S. Kumar  M. Mohri  and A. Talwalkar. Ensemble Nystr¨om method. In NIPS 23  2009.
[15] S. Kumar  M. Mohri  and A. Talwalkar. Sampling methods for the Nystr¨om method. JMLR  13(1):981–

[16] D. D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature 

[17] F. Li  B. Fergus  and P. Perona. Learning generative visual models from few training examples: An

incremental bayesian approach tested on 101 object categories. CVIU  106(1):59–70  2007.

[18] W. Liu  J. He  and S.-F. Chang. Large graph construction for scalable semi-supervised learning. In Proc.

[19] W. Liu  G. Hua  and J. Smith. Unsupervised one-class learning for automatic outlier removal. In Proc.

[20] W. Liu  J. Wang  S. Kumar  and S.-F. Chang. Hashing with graphs. In Proc. ICML  2011.
[21] M. W. Mahoney and P. Drineas. Cur matrix decompositions for improved data analysis. PNAS 

[22] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse coding.

[23] S. Savchenko. The Zeta-function and gibbs measures. Russian Mathematical Surveys  48(1):189–190 

[24] A. Talwalkar  S. Kumar  M. Mohri  and H. Rowley. Large-scale SVD and manifold learning. JMLR 

[25] A. Talwalkar  S. Kumar  and H. Rowley. Large-scale manifold learning. In Proc. CVPR  2008.
[26] C. Thurau  K. Kersting  and C. Bauckhage. Yes we can: simplex volume maximization for descriptive

web-scale matrix factorization. In Proc. CIKM  2010.

[27] F. Wang  C. Chi  T. Chan  and Y. Wang. Nonnegative least correlated component analysis for separation

of dependent sources by volume maximization. TPAMI  32:875–888  2010.

[28] J. Wang  J. Yang  K. Yu  F. Lv  T. Huang  and Y. Gong. Locality-constrained linear coding for image

classiﬁcation. In Proc. CVPR  2010.

bound. In NIPS 26  2012.

[29] S. Wang and Z. Zhang. A scalable cur matrix decomposition algorithm: lower time complexity and tighter

[30] C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In NIPS 14  2000.
[31] M. E. Winter. N-ﬁnder: an algorithm for fast autonomous spectral end-member determination in hyper-
spectral data. In SPIE’s International Symposium on Optical Science  Engineering  and Instrumentation.
International Society for Optics and Photonics  1999.

[32] Y. Xiong  W. Liu  D. Zhao  and X. Tang. Face recognition via archetype hull ranking. In Proc. ICCV 

[33] K. Zhang and J. Kwok. Density weighted Nystr¨om method for computing large kernel eigensystems.

Neural Computation  21:121–146  2009.

[34] D. Zhao and X. Tang. Cyclizing clusters via Zeta function of a graph. In NIPS 22  2008.

2013.

ICML  2010.

CVPR  2014.

106(3):697–702  2009.

JMLR  11:19–60  2010.

1993.

14(1):3129–3152  2013.

9

,Yuanjun Xiong
Wei Liu
Deli Zhao
Xiaoou Tang