2019,Connective Cognition Network for Directional Visual Commonsense Reasoning,Visual commonsense reasoning (VCR) has been introduced to boost research of cognition-level visual understanding  i.e.  a thorough understanding of correlated details of the scene plus an inference with related commonsense knowledge. Recent studies on neuroscience have suggested that brain function or cognition can be described as a global and dynamic integration of local neuronal connectivity  which is context-sensitive to specific cognition tasks. Inspired by this idea  towards VCR  we propose a connective cognition network (CCN) to dynamically reorganize the visual neuron connectivity that is contextualized by the meaning of questions and answers. Concretely  we first develop visual neuron connectivity to fully model correlations of visual content. Then  a contextualization process is introduced to fuse the sentence representation with that of visual neurons. Finally  based on the output of contextualized connectivity  we propose directional connectivity to infer answers or rationales. Experimental results on the VCR dataset demonstrate the effectiveness of our method. Particularly  in $Q \to AR$ mode  our method is around 4\% higher than the state-of-the-art method.,Connective Cognition Network for Directional Visual

Commonsense Reasoning

Aming Wu1∗ Linchao Zhu2 Yahong Han1† Yi Yang2

1College of Intelligence and Computing  Tianjin University  Tianjin  China

2ReLER  University of Technology Sydney  Australia

{tjwam yahong}@tju.edu.cn  {Linchao.Zhu  yi.yang}@uts.edu.au

Abstract

Visual commonsense reasoning (VCR) has been introduced to boost research of
cognition-level visual understanding  i.e.  a thorough understanding of correlated
details of the scene plus an inference with related commonsense knowledge. Recent
studies on neuroscience have suggested that brain function or cognition can be
described as a global and dynamic integration of local neuronal connectivity  which
is context-sensitive to speciﬁc cognition tasks. Inspired by this idea  towards VCR 
we propose a connective cognition network (CCN) to dynamically reorganize the
visual neuron connectivity that is contextualized by the meaning of questions and
answers. Concretely  we ﬁrst develop visual neuron connectivity to fully model
correlations of visual content. Then  a contextualization process is introduced to
fuse the sentence representation with that of visual neurons. Finally  based on the
output of contextualized connectivity  we propose directional connectivity to infer
answers or rationales. Experimental results on the VCR dataset demonstrate the
effectiveness of our method. Particularly  in Q → AR mode  our method is around
4% higher than the state-of-the-art method.

1

Introduction

Recent advances in visual understanding mainly make progress on the recognition-level perception
of visual content  e.g.  object detection [13  23] and segmentation [9  5]  or even on the recognition-
level grounding of visual concepts with image regions  e.g.  image captioning [40  24] and visual
question answering [1  6]. Towards complete visual understanding  a model must move forward
from perception to reasoning  which includes cognitive inferences with correlated details of the scene
and related commonsense knowledge. As a key step towards complete visual understanding  the
task of Visual Commonsense Reasoning (VCR) [42] is proposed along with a well-devised new
dataset. In VCR  given an image  a machine is required to not only answer a question about the
thorough understanding of the correlated details of the visual content  but also provide a rationale  e.g. 
contextualized with related visual details and background knowledge  to justify why the answer is true.
As a ﬁrst attempt to narrow the gap between recognition- and cognition-level visual understanding 
Recognition-to-Cognition Networks (R2C) [42] conducts visual commonsense reasoning step by step 
i.e.  grounding the meaning of natural language with respect to the referred objects  contextualizing
the meaning of an answer with respect to the question and related global objects  and ﬁnally reasoning
over the shared representation to obtain a decision of an answer. Due to the large discrepancy between
the reasoning scheme of VCR and cognition function of human brain  R2C’s performance is not in
competition with humans score  e.g.  65% vs. 91% in Q → A mode.

∗This work was done when Aming Wu visited ReLER Lab  UTS.
†Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Overview of our CCN method. The yellow  blue  and green circles indicate visual
elements  question and answer representation  respectively. Our method mainly includes visual
neuron connectivity  contextualized connectivity  and directional connectivity. For semantic context 
two LSTM units are used to extract sentence representations.

Recent studies [31  8] on brain networks have suggested that brain function or cognition can be
described as the global and dynamic integration of local (segregated) neuronal connectivity. And
such a global and dynamic integration is context-sensitive with respect to a speciﬁc cognition task.
Inspired by this idea  in this paper  we propose a Connective Cognition Network (CCN) for visual
commonsense reasoning. As is shown in Fig. 1  the main process of CCN is to dynamically
reorganize (integrate) the visual neuron connectivity that is contextualized by the meaning of answers
and questions in the current reasoning task.
Concretely  taking visual words as visual neurons and object features as segregated visual modules  we
ﬁrst devise an approach of Conditional GraphVLAD to represent image’s visual neuron connectivity 
which includes connections among visual neurons and visual modules. The visual neuron connectivity
serves as the base function for the dynamic integration in the process of reasoning. Meanwhile  as
a context-sensitive integration  the meaning is speciﬁed by the semantic context of questions and
answers. After obtaining the sequential information of sentences via an LSTM network [16]  we fuse
the sentence representation with that of the visual neurons  which stands for a contextualization.
Then we employ graph convolution neural network (GCN) to fully integrate both the local and global
connectivity. For example  in Fig. 1  connections between “He” and “Person4”  “Person4” and
“Person3”  as well as “Person3” and “table” could all be incorporated in the contextualized connectiv-
ity  where the last connection between “Person3” and “table” belongs to the global integration not
mentioned here. Though the contextualized connectivity is ready for reasoning  it lacks direction
information  which is an important clue for cognitive reasoning [32]. Taking the answer sentence
in Fig. 1 as an example  there exists directional connection from “Person4” to “Person3” via the
predicate “tell”  as well as from “Person1” to “sandwich” via the predicate “order”. Though easy
to be deﬁned in ﬁrst-order logic (FOL) [36]  it is nontrivial to be incorporated into a data-driven
learning process. In this paper  we make an attempt to devise a direction learner on the GCN  so as to
further improve the reasoning performance. Particularly  a network is ﬁrst used to learn the semantic
direction of input features. Then  we add the direction to the computation of the adjacency matrix of
GCN to obtain a directional adjacency matrix  which serves as directional connectivity for reasoning.
Thus  we develop a novel connective cognition network for directional visual commonsense reasoning.
The main contributions lie in that  this is the ﬁrst attempt to use an end-to-end training neural network
for the cognitive reasoning process  i.e.  global and dynamic integration of local (segregated) visual
neuron connectivity  which is context-sensitive with respect to a speciﬁc VQA task. Moreover 
we also try to incorporate directional reasoning into a data-driven learning process. Experimental
results on the VCR dataset [42] demonstrate the effectiveness of the proposed method. On the three
reasoning modes of VCR task  i.e.  Q → A  QA → R  and Q → AR  the CCN with directional
reasoning signiﬁcantly outperforms R2C by 3.4%  3.2%  and 4.4%  respectively.

2 Related Work

Visual Question Answering: Recently  many effective methods are proposed in the VQA task 
which includes those based on attention [21  26]  multi-modal fusion [33  12]  and visual reasoning

2

pointingHe is telling [Person3] that [Person1] ordered the sandwich.[Bottle] [Cup] [Person3] [Person1] [Person4] [Sandwich] Answer:Why is [Person4] pointing at [Person1]?Question:Why[Person4]ispointingatHe[Person3]tellingthatisHe[Person3]istellingthat[Person3]He[Person1] sandwichtellstoContextualized ConnectivityWhyisPerson4atHeistellingPerson3thatSemantic ContextVisual Neuron ConnectivityDirectional Connectivity for ReasoningPerson1Person1orderFigure 2: The framework of the CCN method.
It mainly includes visual neuron connectivity 
contextualized connectivity  and directional connectivity for reasoning. Here  ‘{U  O}’ indicates
the set including the output U of GraphVLAD and object features O. ‘fθ’ indicates the prediction
function for responses (answers or rationales). ‘F’ indicates fusion operation.

[35  29]. Most methods focus on the recognition of visual content and spatial positions  but they lack
the ability of commonsense reasoning. To advance the research of reasoning  a new task of VCR [42]
is proposed. Given a query-image pair  this task needs models to choose correct answer and rationale
justifying why the answer is true. The challenges mainly include a thorough understanding of vision
and language as well as a method to infer responses (answers or rationales). In this paper  we propose
a CCN model for VCR  which has been proved to be effective in the experiment.
NetVLAD: The work [4] proposes NetVLAD which is used to extract local features. Particularly  it
includes an aggregation layer for clustering the local features into a VLAD [19] global descriptor.
Recently  NetVLAD has been demonstrated to be effective in many tasks [2  37]. Particularly  the
work [2] proposes a PointNetVLAD to extract the global descriptor from a given 3D point cloud.
Besides  the state-of-the-art models [7  27] of video classiﬁcation most use NetVLAD pooling to
aggregate information from all the frames of a video. However  the original NetVLAD learns multiple
centers from the overall dataset to represent each input data  which ignores the characteristic of the
input data and reduces the accuracy of the representation. To alleviate this problem  in this paper  we
propose a conditional GraphVLAD to integrate the characteristic of the input data.
Graph Convolutional Network: GCN [22  39  28  43] aims to generalize the Convolutional Neural
Network (CNN) to graph-structured data. By encoding both the structure of the graph surrounding a
node and the feature of the node  GCN could learn representation for every node effectively. As GCN
has the beneﬁt of capturing relations between nodes  many works have employed GCN for reasoning
[17  29]. Particularly  the work [29] uses GCN to infer answers. However  it only constructs an
undirected graph for reasoning [29]  which ignores the directional information between nodes. The
directional information is often considered an important factor for inference [32]. Here  we propose a
directional connectivity to infer answers  which has been proved to be effective.

3 Connective Cognition Network

Fig. 2 shows the framework of CCN model. It mainly includes visual neuron connectivity  contextu-
alized connectivity  and directional connectivity for reasoning.

3.1 Visual Neuron Connectivity

The goal of visual neuron connectivity (Fig. 3(a)) is to obtain a global representation of an image 
which is helpful for a thorough understanding of visual content. It mainly includes visual element
connectivity and the computation of both conditional centers and GraphVLAD.
Visual Element Connectivity. We ﬁrst use a pre-trained network  e.g.  ResNet [15]  to obtain the
feature map X ∈ Rw×h×m of an image  where w  h  and m separately indicate the width  height 
and number of channels. Here  we take each element of the feature map as a visual element. We take
the output Y ∈ Rn of LSTM [16] at the last time step as the representation of query (question or
question with a correct answer).

3

Why is [Person4] pointing at [Person1]?QueryqHe is telling [Person3] that [Person1] ordered the sandwich . Response(cid:1853)(cid:4666)(cid:3036)(cid:4667)FLSTMFLSTMGCNGCNFDirection learnerGCN(cid:1858)(cid:3087)(cid:4666)(cid:1870)(cid:4666)(cid:3036)(cid:4667)(cid:481)(cid:1853)(cid:4666)(cid:3036)(cid:4667)(cid:4667)(cid:1870)(cid:4666)(cid:3036)(cid:4667)ImageI + objectsoGraphVLADObject FeaturesVisual Neuron ConnectivityContextualized Connectivity{U  O}UODirectional Connectivity for ReasoningFigure 3: (a) shows the process of visual neuron connectivity. ‘AT’ indicates afﬁne transformation. (b)
shows the initial state of NetVLAD. (c) shows the conditional centers after an afﬁne transformation.
Here  we use the fusion of image and question to compute the parameter γ and β. (d) and (e) show
the result of NetVLAD and GraphVLAD  respectively.

In general  there exists certain relation between objects of an image [10]. As is shown in the left part
of Fig. 1  relations (solid and dotted lines) exist not only between elements (yellow circles) in the
same object region  but also between various objects (Person1  Person3  Person4  and background).
Obviously  capturing these relations is helpful for a thorough understanding of the entire scene. In this
paper  we employ GCN to capture these relations. Speciﬁcally  we seek to construct an undirected
graph Gg = {V  ξ  A}  where ξ is the set of graph edges to learn and A ∈ RN×N (N = wh) is the
corresponding adjacency matrix. Each node ν ∈ V corresponds to one element of the feature map.

And the size of V is set to N. We ﬁrst reshape X to (cid:101)X ∈ RN×m. Then  we deﬁne an adjacency
matrix for an undirected graph as A = sof tmaxr((cid:101)X(cid:101)X T ) + Id  where Id indicates the identity matrix

(cid:102)M = tanh(wc

M = A(cid:101)X 
f ∈ R1×m×n  wc

and sof tmaxr indicates we make sof tmax operation across the row direction.
g ∗ M + bc
g) 

(1)
g ∈ Rn indicate the trainable parameters. ‘*’
f ∈ Rn  and bc
where wc
indicates the convolutional operation. ‘(cid:12)’ indicates element-wise product. Each row of the matrix M
the current node. (cid:102)M ∈ RN×n indicates the output of GCN.
represents a feature vector of a node  which is a weighted sum of the neighboring node features of
The Computation of Conditional Centers. Since (cid:102)M only captures relations between visual ele-

g ∈ R1×m×n  bc

f ∗ M + bc

f ) (cid:12) σ(wc

ments and does not have the capability to fully understand the image  we consider using NetVLAD
[19  4] to further enhance the representation of an image. By learning multiple centers  i.e.  visual
words  NetVLAD could use these centers to describe a scene [4]. However  these centers are learned
based on the overall dataset and reﬂect the attributes of the dataset. In other words  these centers are
independent of the current input data  which ignore the characteristic of the input data and reduce
the accuracy of the representation. Here  we consider making an afﬁne transformation for the initial
centers and using these transformed centers to represent an image.
Concretely  we ﬁrst deﬁne the initial centers C = {ci ∈ Rn  i = 1  ...  K}. Next  based on the current
input query-image pairs  we make the afﬁne transformation [34] for the initial centers.

γ = f ((cid:104)(cid:102)M  (cid:101)Y (cid:105)) 

β = h((cid:104)(cid:102)M  (cid:101)Y (cid:105)) 

where (cid:104)a  b(cid:105) represents the concatenation of a and b. By stacking Y   we obtain (cid:101)Y ∈ RN×n. We

separately use a two-layer convolutional network to deﬁne f and h. zi ∈ Rn indicates the i-th

zi = γci + β 

(2)

4

(e)(a)(d)(c)(cid:28685)(cid:28595)(cid:28624)(cid:28595)(cid:2011)(cid:1855)(cid:3397)(cid:2010)ConditionalNetVLADGraphWhyis[Person4]pointing at [Person1]?+(b)CNNGCNFeature MapWhy is [Person4] pointing at [Person1 ]?ImageIQueryqBERTLSTMFATInitial CentersVLADcoreConvSoftmaxIntra-normalizationL2 normalizationFGCNConditional GraphVLADConditionalVisualElementConnectivityOutput Mapgenerated conditional center. Here  we take the concatenated result of the representations of both
input images and their corresponding queries as the input of f and h to compute parameter γ and
β. Since parameter γ and β are learned based on the input query-image pairs  these two parameters
reﬂect the character of the current input data. Equipped with the afﬁne transformation  the initial
centers are made to move towards the input features  which improves the accuracy of the residual
operation (Fig. 3(d)) of NetVLAD. As is shown in Fig. 3(b) and (c)  after the afﬁne transformation 
the centers move towards the features (color circles). Finally  we use Z = {z1 ···   zK} to indicate
the new conditional centers.

The Computation of GraphVLAD. Next  we use Z and(cid:102)M to perform NetVLAD operation 

N(cid:88)

i=1

(cid:80)

Dj =

j (cid:102)Mi+bj
(cid:48)(cid:102)Mi+b

wT
j

ewT

(cid:48) e

j

((cid:102)Mi − zj) 

(cid:48)

j

(3)

where {wj} and {bj} are sets of trainable parameters for each center zj and j = 1  ...  K. Finally 
we use D ∈ RK×n to indicate the output of NetVLAD.
Besides  as is shown in Fig. 3(d)  NetVLAD only captures relations between elements and centers.
As NetVLAD is computed based on visual elements where relations are existed  we consider there
should exist certain relations between outputs. Here  we employ GCN to capture these relations.

Concretely  we ﬁrst concatenate the NetVLAD output and conditional centers  i.e.  (cid:101)Z = (cid:104)z1 ···   zK(cid:105) 
(cid:101)Z ∈ RK×n  and H = (cid:104)D (cid:101)Z(cid:105). Then  we deﬁne an adjacency matrix for an undirected graph as

B = sof tmaxr(HH T ) + Id. The following processes are the same as Eq. (1). Finally  we use
U ∈ RK×n to indicate the output of GraphVLAD. By this operation  we obtain the global information
of an image  which is as complementation of local object features O ∈ RL×n (L indicates the number
of objects) extracted by a pre-trained network and GCN network. Finally  the set S = {U  O} is
taken as the global representation of an image.

3.2 Contextualized Connectivity

The goal of contextualized connectivity is to not only capture the relevance between linguistic features
and the global representation S  but also extract deep semantic existing in sentences according

to visual information. Concretely  LSTM is employed to obtain representation (cid:101)Q ∈ RP×n and
(cid:101)A ∈ RJ×n of query and response  respectively  where P and J separately indicate the length of

query and response. Next  we introduce the processing of the query. An attention operation is ﬁrst
used to obtain the relevance between the query and global representation.

Fqu = sof tmaxr((cid:101)QU T )  Fqo = sof tmaxr((cid:101)QOT )  QU = FquU  QO = FqoO.

Then  we take the concatenation of QU   QO  and (cid:101)Q as QF ∈ RP×3n. U and O are the output of

the GraphVLAD. Here  we only obtain sequential features  rather than the structural information
[41] which is helpful for a better understanding of the sentence semantic. Meanwhile  LSTM has
the limitation of long-term information dilution [38]  which weakens the capacity of the sentence
representation. In this paper  we consider using GCN to extract structural information. Concretely  we
deﬁne an adjacency matrix for an undirected graph as Q = sof tmaxr(QF QT
F ) + Id. The following
processes are the same as Eq. (1). Finally  we use Qg ∈ RP×n to indicate the output of this network.
The processing of responses is the same as that of queries. And the representation of response
generated by a GCN network is deﬁned as Ag ∈ RJ×n.

(4)

3.3 Directional Connectivity for Reasoning

GCN for reasoning. Concretely  we ﬁrst use (cid:101)A to obtain the attention representation Qa ∈ RJ×n of

Directional information is an important clue for cognitive reasoning. And using directional infor-
mation could improve the accuracy of reasoning [32]. Here  we propose a semantic direction based
Qg. The processes are the same as Eq. (4). Then  Qa and Ag are concatenated as Eqa ∈ RJ×2n.
Next  based on Eqa  we ﬁrst try to learn the direction information.

5

Dqa = φ(Eqa) 

Gt = DqaDT

qa 

Dt = sign(Gt) 

Ve = sof tmaxr(abs(Gt)) 

(5)

where abs indicates the operation of absolute value. Here  φ is deﬁned as a directional function 
which is a one-layer convolutional network without activation. Besides  to learn the direction  we do
not use ReLU activation at the last layer of the network φ. By using the sign function  we obtain the
direction Dt  where -1 and 1 separately indicate the negative and positive correlation. Next  based on
the output Dt of the sign function  we compute the adjacency matrix.

H = Dt (cid:12) Ve + Id 

Mt = HEqa 

g ∗ Mt + br
(6)
g) 
f ∈ Rn  and br
g ∈ Rn
where H indicates the adjacency matrix. wr
indicate the trainable parameters. Finally  we take Rt ∈ RJ×n as the GCN output. By this operation 
we could make our model not only learn the direction information between nodes  but also leverage
the information in the computation of GCN  which results in accurate inference. In the experiment 
compared with undirected GCN  our method could improve performance signiﬁcantly.

f ∗ Mt + br
g ∈ R1×2n×n  br

Rt = tanh(wr
f ∈ R1×2n×n  wr

f ) (cid:12) σ(wr

3.4 Prediction Layer and Loss Function

After obtaining the output of the reasoning module  we concatenate Rt and (cid:101)A across the channel
dimension  i.e.  Fc = (cid:104)Rt  (cid:101)A(cid:105) and Fc ∈ RJ×2n. Next  we compute a global vector representation
(cid:101)F ∈ R2n via a max-pooling operation across the node dimension of Fc. This operation is helpful for
l(y  ˆy) = −(cid:80)4

getting a permutation invariant output and focusing on the impact of the graph structure [30]. Finally 
we compute classiﬁcation logits through a two-layer MLP with ReLU activation.
For VCR task  given a query-image pair  this task gives four response choices. In this paper  we train
our model using a multi-class cross-entropy loss between the set of responses and the labels  i.e. 

i=1 yilog(ˆyi)  where y denotes the ground truth and ˆy is the predicted result.

4 Experiments

In this section  we evaluate our method on the VCR dataset. And this dataset contains 290k pairs of
questions  answers  and rationales  over 110k unique movie scenes. Moreover  this task considers
three modes  i.e.  Q → A (given a question  select the correct answer)  QA → R (given a question
and the correct answer  select the correct rationale)  and Q → AR (given a question  select the correct
answer  then the correct rationale). For Q → AR mode  if it gets either the wrong answer or the wrong
rationale  no points will be received. The code is available at https://github.com/AmingWu/CCN.
Implementation details. We use ResNet50 [15] to extract image and object features. BERT [11]
is used as the word embedding. The feature map is X ∈ R12×24×512. The size of the hidden state
of LSTM is set to 512. For Eq. (1)  we use a one-layer GCN. And 32 centers are used to compute
GraphVLAD. For Eq. (2)  we separately use a two-layer network to deﬁne f and h. Their parameters
are all set to 1 × 1024 × 512 and 1 × 512 × 512. Next  we use a one-layer GCN to capture relations
between centers. And the parameter settings of the GCN are the same as those of Eq. (1). For
contextualized connectivity  we separately use a one-layer GCN to process query and response. Their
parameter settings are the same as those of Eq. (1). For Eq. (5)  a one-layer GCN is used for
reasoning. Besides  the parameters of the network φ are set to 1 × 1024 × 512. During training  we
use Adam optimizer with a learning rate of 2 × 10−3.

4.1 The Performance of Our Method

We evaluate our method on the three modes of VCR task. The results are shown in Table 1. We
can see that some of state-of-the-art VQA methods  e.g.  MUTAN [6] and BottomUpTopDown [1] 
do not perform well on this task. This shows that these VQA methods lack the inference ability 
which results in unsatisﬁed performance on the task requiring high-level commonsense reasoning.
Meanwhile  compared with the baseline method  on the three modes of VCR task  our method is
3.4%  3.2%  and 4.4% higher than R2C  respectively. This shows that our method is effective.

6

Figure 4: Qualitative examples from CCN. Correct choices are highlighted in blue.
Incorrect
inferences are in red. The number after each option indicates the score given by our model. The ﬁrst
row is a successful case. The second and last row correspond to two fail cases.

In Fig. 4  we show some qualitative examples. As is shown in these examples  compared with
classical VQA dataset [3  14]  both questions and answers of VCR dataset are much more complex.
Directly leveraging the recognition of visual content is difﬁcult to choose the right answers and
rationales. Besides  the ﬁrst row of Fig. 4 is a successful case. Our model deduces the correct answer
and its corresponding correct rationale with a high score. The second row shows a fail case  where the
model chooses the right answer and the wrong rationale. However  the rationale chosen by our model
is an explanation for the answer based on the understanding of the entire scene. Though from this
view  the rationale is reasonable  compared with the ground truth  our rationale is slightly indirect and
unclear. This shows when the visual reasoning involves more commonsense  the task of interpreting
the answer is more difﬁcult. Besides  though the model fails  the wrong rationale indeed matches
the visual content  which shows the GraphVLAD module is helpful for obtaining an effective visual
representation. The third row is also a fail case  where our model chooses the wrong answer and
rationale. From these two fail cases  we can see that when the question  answer  and rationale involve
much commonsense  the model is easy to make an error selection and indeed requires a strong ability
of inference to choose the right answer and rationale. More examples can be found in Appendix.

Table 1: The performance of our CCN model on the VCR dataset.

Model

Revisited VQA [18]

BottomUpTopDown [1]

MLB [20]
MUTAN [6]

R2C (baseline) [42]

CCN

Q → A
Test
Val
39.4
40.5
44.1
42.8
46.2
45.5
45.5
44.4
65.1
63.8
67.4
68.5

QA → R
Test
Val
34.0
33.7
25.1
25.1
36.8
36.1
32.2
32.0
67.3
67.2
70.6
70.5

Q → AR
Test
Val
13.5
13.8
11.0
10.7
17.2
17.0
14.6
14.6
44.0
43.1
47.7
48.4

4.2 Ablation Analysis

In this section  based on the validation set  we make ablation analysis for our proposed conditional
GraphVLAD  contextualized connectivity for extracting of the sentence semantic  and directional
connectivity for reasoning  respectively.

7

Person1Person2How do [Person1  Person2] feel about each other?a) They love each other romantically. 35.2%b) [Person1  Person2] are starting to fall in love. 22.9%The rationale is …c) [Person1  Person2]are friends and agree with each other. 36.8%d) They feel sad. 5.1%a) They are sitting very close and smiling at each other lovingly. 34.2%b) [Person1] is looking at [Person2] lovingly. 26.4%c) They are passionately kissing each other. 7%d) They are at a dinner together and are holding on to each other’s arms closely. 32.4%Wrong answer and rationaleBackpackWhy did [Person1] come here instead of a healthy restaurant?Person2a) [Person2] went to his mother’s house for dinner. 0.0%b) [Person1] cannot spend a lot of money to satisfy his hunger. 55.9%c) He could have been watching his barbecue. 39.8%d) Because he ate before he comes. 4.3%a) [Person1] looks like a student based on [Backpack]. Students usually have limited budget. 27.3%b) This restaurant has no tables and chairs visible. It appears to be takeout only. 67.8%c) Champagne is expensive and the restaurant is high end. 2.1%d) [Person1]’s clothing is of working class. He likes a worker. 2.8%The rationale is …Right answer  wrong rationalePerson1Person1Person2Person4Person3Why has [Person1  Person2  Person3  Person4] turned around at the table?a) They are judging a competition. 9.9%b) A noise has attracted their attention. 56.1%c) [Person1  Person2  Person3  Person4] want to pick up fork. 2.7%d) They are eating. 31.3%a) [Person1  Person2  Person3  Person4] are looking at something. 3.2%b) They appear to be worried and paying attention to their surroundings. 0.5%d) The only thing making them turn around would be a noise. 95.5%Person5Person6c) [Person5  Person6] have a startled facial expression. 0.8%The rationale is …Right answer and rationaleFigure 5: t-SNE plot of conditional centers. Here  the red pentagrams  blue circles  and green
rhombuses indicate the initial centers and two different conditional centers  respectively. (a) and (b)
are used to compute blue and green centers  respectively.

GraphVLAD. The number of centers is an important hyper-parameter for GraphVLAD. If few
centers are used  it will weaken the representation ability of GraphVLAD. Conversely  if many
centers are used  it will increase the number of parameters and computational costs. In Q → A 
QA → R  and Q → AR modes  the performance of 16 centers and 48 centers separately is 66.4% 
69.2%  46.4% and 67.1%  69.8%  46.9%. For our method  the performance of 32 centers is the best.
In Table 2  we analyze the effect of conditional centers and GCN for GraphVLAD. Here  ‘No-C +
No-G’ indicates we use neither conditional centers nor GCN in the computation of GraphVLAD.
And other components of our model are kept unchanged. We can see that employing conditional
centers and GCN could improve performance signiﬁcantly. Particularly  compared with NetVLAD
corresponding to the case of ‘No-C + No-G’  our Conditional GraphVLAD outperforms NetVLAD
signiﬁcantly. This shows our method is effective. Besides  in Fig. 5  we show two t-SNE [25]
examples of conditional centers. And the queries of Fig. 5(a) and (b) are “Who does the dog belong
to?" and “What will happen after the person pushes the lifeboat over the edge of the ship?". We can
see that the positions of centers vary depending on the visual content and its corresponding queries.
When an image contains rich content and its corresponding query is complex  e.g.  Fig. 5(b)  in
order to capture rich visual information to answer the query  these centers will learn to spread further
apart from each other. Meanwhile  when the image content and its corresponding query contain
relatively less information  e.g.  Fig. 5(a)  in order to focus on visual information which is related to
the query  these centers will adaptively adjust to being more concentrated. In this way  we can obtain
an effective visual representation  which is helpful for the following contextualization and reasoning.
Contextualized Connectivity. In this paper  we separately employ a GCN to capture the semantic of
queries and responses. To prove this operation is effective  we compare it with a common operation 
i.e.  using a GCN to process the concatenation of vision  query  and response. In Q → A  QA → R 
and Q → AR mode  the performance of the common operation is 66.5%  68.1%  and 45.7%  which
is obviously weaker than our method.

No-C + No-G

Table 2: Ablation analysis of GraphVLAD.
Q → A QA → R Q → AR
Method
65.8
66.5
66.9
67.4

No-C
No-G
C + G

45.6
46.6
46.5
47.7

68.3
69.6
69.4
70.6

Table 3: Ablation of Directional Reasoning.
Q → A QA → R Q → AR
Method
65.9
No-R
64.8
66.5
67.4

45.3
43.9
46.4
47.7

67.9
67.1
69.4
70.6

GCN
D-GCN

LSTM-R

Directional Connectivity for Reasoning. In this paper  we propose a directional reasoning method.
We compare our method with other reasoning methods. The results are shown in Table 3. Here 
‘No-R’ indicates we do not use reasoning. ‘LSTM-R’  ‘GCN’  and ‘D-GCN’ indicate reasoning based
on LSTM  undirected GCN  and directed GCN  respectively. And other components of our model are
kept the same. We can see that for the method without reasoning  the performance is obviously weak.
This shows reasoning is a necessary step for our method. Besides  the performance of the reasoning
based on LSTM is also weak. This shows that LSTM could not capture complex relations effectively.

8

(a)(b)Finally  compared with undirected GCN reasoning  our directional GCN outperforms it signiﬁcantly.
This shows using directional information in reasoning could improve the accuracy of inference.

5 Conclution

We propose a cognition connectivity network for directional visual commonsense reasoning. This
model mainly includes visual neuron connectivity  contextualized connectivity  and directional
connectivity for reasoning. Particularly  for visual neuron connectivity  we propose a conditional
GraphVLAD module to represent an image. Meanwhile  we propose a directional GCN for reasoning.
The experimental results demonstrate the effectiveness of our method. Particularly  in the Q → AR
mode  our method is 4.4% higher than R2C.

Acknowledgement

This work is supported by the NSFC (under Grant 61876130  61932009  U1509206).

References
[1] Peter Anderson  Xiaodong He  Chris Buehler  Damien Teney  Mark Johnson  Stephen Gould  and Lei Zhang.

Bottom-up and top-down attention for image captioning and visual question answering. In CVPR  2018.

[2] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad: Deep point cloud based retrieval for large-scale

place recognition. In CVPR  2018.

[3] Stanislaw Antol  Aishwarya Agrawal  Jiasen Lu  Margaret Mitchell  Dhruv Batra  C Lawrence Zitnick  and

Devi Parikh. Vqa: Visual question answering. In ICCV  2015.

[4] Relja Arandjelovic  Petr Gronat  Akihiko Torii  Tomas Pajdla  and Josef Sivic. Netvlad: Cnn architecture

for weakly supervised place recognition. In CVPR  2016.

[5] Vijay Badrinarayanan  Alex Kendall  and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder
IEEE transactions on pattern analysis and machine intelligence 

architecture for image segmentation.
39(12):2481–2495  2017.

[6] Hedi Ben-Younes  Rémi Cadene  Matthieu Cord  and Nicolas Thome. Mutan: Multimodal tucker fusion for

visual question answering. In ICCV  2017.

[7] Shweta Bhardwaj  Mukundhan Srinivasan  and Mitesh M Khapra. Efﬁcient video classiﬁcation using fewer

frames. In CVPR  2019.

[8] Michał Bola and Bernhard A Sabel. Dynamic reorganization of brain functional networks during cognition.

Neuroimage  114:398–413  2015.

[9] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets  atrous convolution  and fully connected crfs.
IEEE transactions on pattern analysis and machine intelligence  40(4):834–848  2018.

[10] Yunpeng Chen  Marcus Rohrbach  Zhicheng Yan  Shuicheng Yan  Jiashi Feng  and Yannis Kalantidis.

Graph-based global reasoning networks. arXiv preprint arXiv:1811.12814  2018.

[11] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. Bert: Pre-training of deep bidirec-

tional transformers for language understanding. arXiv preprint arXiv:1810.04805  2018.

[12] Peng Gao  Hongsheng Li  Shuang Li  Pan Lu  Yikang Li  Steven CH Hoi  and Xiaogang Wang. Question-

guided hybrid convolution for visual question answering. In ECCV  2018.

[13] Ross Girshick. Fast r-cnn. In ICCV  2015.

[14] Yash Goyal  Tejas Khot  Douglas Summers-Stay  Dhruv Batra  and Devi Parikh. Making the v in vqa

matter: Elevating the role of image understanding in visual question answering. In CVPR  2017.

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In CVPR  2016.

9

[16] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780 

1997.

[17] Tamir Hazan Alexander Schwing Idan Schwartz  Seunghak Yu. Factor graph attention. In CVPR  2019.

[18] Allan Jabri  Armand Joulin  and Laurens Van Der Maaten. Revisiting visual question answering baselines.

In ECCV. Springer  2016.

[19] Hervé Jégou  Matthijs Douze  Cordelia Schmid  and Patrick Pérez. Aggregating local descriptors into a

compact image representation. In CVPR. IEEE Computer Society  2010.

[20] Jin-Hwa Kim  Kyoung-Woon On  Woosang Lim  Jeonghee Kim  Jung-Woo Ha  and Byoung-Tak Zhang.

Hadamard product for low-rank bilinear pooling. arXiv preprint arXiv:1610.04325  2016.

[21] Kyung-Min Kim  Seong-Ho Choi  Jin-Hwa Kim  and Byoung-Tak Zhang. Multimodal dual attention

memory for video story question answering. In ECCV  2018.

[22] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.

arXiv preprint arXiv:1609.02907  2016.

[23] Wei Liu  Dragomir Anguelov  Dumitru Erhan  Christian Szegedy  Scott Reed  Cheng-Yang Fu  and

Alexander C Berg. Ssd: Single shot multibox detector. In ECCV. Springer  2016.

[24] Jiasen Lu  Caiming Xiong  Devi Parikh  and Richard Socher. Knowing when to look: Adaptive attention

via a visual sentinel for image captioning. In CVPR  2017.

[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[26] Mateusz Malinowski  Carl Doersch  Adam Santoro  and Peter Battaglia. Learning visual question answering

by bootstrapping hard attention. In ECCV  2018.

[27] Antoine Miech  Ivan Laptev  and Josef Sivic. Learnable pooling with context gating for video classiﬁcation.

arXiv preprint arXiv:1706.06905  2017.

[28] Federico Monti  Davide Boscaini  Jonathan Masci  Emanuele Rodola  Jan Svoboda  and Michael M

Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR  2017.

[29] Medhini Narasimhan  Svetlana Lazebnik  and Alexander Schwing. Out of the box: Reasoning with
graph convolution nets for factual visual question answering. In Advances in Neural Information Processing
Systems  2018.

[30] Will Norcliffe-Brown  Stathis Vafeias  and Sarah Parisot. Learning conditioned graph structures for

interpretable visual question answering. In Advances in Neural Information Processing Systems  2018.

[31] Hae-Jeong Park and Karl Friston. Structural and functional brain networks: from connections to cognition.

Science  342(6158):1238411  2013.

[32] Vimla L. Patel and Marco F. Ramoni. Expertise in context. chapter Cognitive Models of Directional

Inference in Expert Medical Reasoning  pages 67–99. MIT Press  Cambridge  MA  USA  1997.

[33] Gao Peng  Hongsheng Li  Haoxuan You  Zhengkai Jiang  Pan Lu  Steven Hoi  and Xiaogang Wang.
Dynamic fusion with intra-and inter-modality attention ﬂow for visual question answering. arXiv preprint
arXiv:1812.05252  2018.

[34] Ethan Perez  Florian Strub  Harm De Vries  Vincent Dumoulin  and Aaron Courville. Film: Visual

reasoning with a general conditioning layer. In AAAI  2018.

[35] Matthieu Cord Nicolas Thome Remi Cadene  Hedi Ben-younes. Murel: Multimodal relational reasoning

for visual question answering. In CVPR  2019.

[36] Stuart Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach. Prentice Hall Press  Upper

Saddle River  NJ  USA  3rd edition  2009.

[37] Yongyi Tang  Xing Zhang  Lin Ma  Jingwen Wang  Shaoxiang Chen  and Yu-Gang Jiang. Non-local

netvlad encoding for video classiﬁcation. In ECCV  2018.

[38] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems 
2017.

10

[39] Petar Veliˇckovi´c  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua Bengio.

Graph attention networks. arXiv preprint arXiv:1710.10903  2017.

[40] Kelvin Xu  Jimmy Ba  Ryan Kiros  Kyunghyun Cho  Aaron Courville  Ruslan Salakhudinov  Rich Zemel 
and Yoshua Bengio. Show  attend and tell: Neural image caption generation with visual attention. In ICML 
2015.

[41] Kun Xu  Lingfei Wu  Zhiguo Wang  Mo Yu  Liwei Chen  and Vadim Sheinin. Exploiting rich syntactic

information for semantic parsing with graph-to-sequence model. arXiv preprint arXiv:1808.07624  2018.

[42] Rowan Zellers  Yonatan Bisk  Ali Farhadi  and Yejin Choi. From recognition to cognition: Visual

commonsense reasoning. In CVPR  2019.

[43] Jie Zhou  Ganqu Cui  Zhengyan Zhang  Cheng Yang  Zhiyuan Liu  and Maosong Sun. Graph neural

networks: A review of methods and applications. arXiv preprint arXiv:1812.08434  2018.

11

,Aming Wu
Linchao Zhu
Yahong Han
Yi Yang