2016,Total Variation Classes Beyond 1d: Minimax Rates  and the Limitations of Linear Smoothers,We consider the problem of estimating a function defined over $n$ locations on a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$).  When the function is constrained to have discrete total variation bounded by $C_n$  we derive the minimax optimal (squared) $\ell_2$ estimation error rate  parametrized by $n  C_n$. Total variation denoising  also known as the fused lasso  is seen to be rate optimal.  Several simpler estimators exist  such as Laplacian smoothing and Laplacian eigenmaps.  A natural question is: can these simpler estimators perform just as well?  We prove that these estimators  and more broadly all estimators given by linear transformations of the input data  are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone (1998) on 1-dimensional total variation spaces to higher dimensions.  The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks  without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over $d$-dimensional grids  which are  in some sense  smaller than the total variation function spaces.  Indeed  these are small enough spaces that linear estimators can be optimal---and a few well-known ones are  such as Laplacian smoothing and Laplacian eigenmaps  as we show.  Lastly  we investigate the adaptivity of the total variation denoiser to these smaller Sobolev function spaces.,Total Variation Classes Beyond 1d: Minimax Rates 

and the Limitations of Linear Smoothers

Veeranjaneyulu Sadhanala
Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA 15213

vsadhana@cs.cmu.edu

Yu-Xiang Wang

Ryan J. Tibshirani

Machine Learning Department
Carnegie Mellon University

Department of Statistics

Carnegie Mellon University

Pittsburgh  PA 15213

yuxiangw@cs.cmu.edu

Pittsburgh  PA 15213

ryantibs@stat.cmu.edu

Abstract

We consider the problem of estimating a function deﬁned over n locations on a
d-dimensional grid (having all side lengths equal to n1/d). When the function is
constrained to have discrete total variation bounded by Cn  we derive the minimax
optimal (squared) (cid:96)2 estimation error rate  parametrized by n  Cn. Total variation
denoising  also known as the fused lasso  is seen to be rate optimal. Several simpler
estimators exist  such as Laplacian smoothing and Laplacian eigenmaps. A natural
question is: can these simpler estimators perform just as well? We prove that these
estimators  and more broadly all estimators given by linear transformations of the
input data  are suboptimal over the class of functions with bounded variation. This
extends fundamental ﬁndings of Donoho and Johnstone [12] on 1-dimensional total
variation spaces to higher dimensions. The implication is that the computationally
simpler methods cannot be used for such sophisticated denoising tasks  without
sacriﬁcing statistical accuracy. We also derive minimax rates for discrete Sobolev
spaces over d-dimensional grids  which are  in some sense  smaller than the total
variation function spaces. Indeed  these are small enough spaces that linear estima-
tors can be optimal—and a few well-known ones are  such as Laplacian smoothing
and Laplacian eigenmaps  as we show. Lastly  we investigate the adaptivity of the
total variation denoiser to these smaller Sobolev function spaces.

1

Introduction

Let G = (V  E) be a d-dimensional grid graph  i.e.  lattice graph  with equal side lengths. Label the
nodes as V = {1  . . .   n}  and edges as E = {e1  . . .   em}. Consider data y = (y1  . . .   yn) ∈ Rn
observed over the nodes  from a model

yi ∼ N (θ0 i  σ2) 

(1)
where θ0 = (θ0 1  . . .   θ0 n) ∈ Rn is an unknown mean parameter to be estimated  and σ2 > 0 is the
marginal noise variance. It is assumed that θ0 displays some kind of regularity over the grid G  e.g. 
θ0 ∈ Td(Cn) for some Cn > 0  where

i.i.d.  for i = 1  . . .   n 

Td(Cn) =(cid:8)θ : (cid:107)Dθ(cid:107)1 ≤ Cn

(cid:9) 

(2)
and D ∈ Rm×n is the edge incidence matrix of G. This has (cid:96)th row D(cid:96) = (0  . . .  −1  . . .   1  . . .   0) 
with a −1 in the ith location  and 1 in the jth location  provided that the (cid:96)th edge is e(cid:96) = (i  j) with
i < j. Equivalently  L = DT D is the graph Laplacian matrix of G  and thus

(cid:107)Dθ(cid:107)1 =

|θi − θj| 

and

(cid:107)Dθ(cid:107)2

2 = θT Lθ =

(θi − θj)2.

(cid:88)

(i j)∈E

(cid:88)

(i j)∈E

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

We will refer to the class in (2) as a discrete total variation (TV) class  and to the quantity (cid:107)Dθ0(cid:107)1 as
the discrete total variation of θ0  though for simplicity we will often drop the word “discrete”.
The problem of estimating θ0 given a total variation bound as in (2) is of great importance in both
nonparametric statistics and signal processing  and has many applications  e.g.  changepoint detection
for 1d grids  and image denoising for 2d and 3d grids. There has been much methodological and
computational work devoted to this problem  resulting in practically efﬁcient estimators in dimensions
1  2  3  and beyond. However  theoretical performance  and in particularly optimality  is only really
well-understood in the 1-dimensional setting. This paper seeks to change that  and offers theory in
d-dimensions that parallel more classical results known in the 1-dimensional case.

Estimators under consideration. Central role to our work is the total variation (TV) denoising or
fused lasso estimator (e.g.  [21  25  7  15  27  23  2])  deﬁned by the convex optimization problem

(cid:107)y − θ(cid:107)2

2 + λ(cid:107)Dθ(cid:107)1 

ˆθTV = argmin
θ∈Rn

(3)
where λ ≥ 0 is a tuning parameter. Another pair of methods that we study carefully are Laplacian
smoothing and Laplacian eigenmaps  which are most commonly seen in the context of clustering 
dimensionality reduction  and semi-supervised learning  but are also useful tools for estimation in a
regression setting like ours (e.g.  [3  4  24  30  5  22]). The Laplacian smoothing estimator is given
by

ˆθLS = argmin
θ∈Rn

(4)
for a tuning parameter λ ≥ 0  where in the second expression we have written ˆθLS in closed-form
(this is possible since it is the minimizer of a convex quadratic). For Laplacian eigenmaps  we must
introduce the eigendecomposition of the graph Laplacian  L = V ΣV T   where Σ = diag(ρ1  . . .   ρn)
with 0 = ρ1 < ρ2 ≤ . . . ≤ ρn  and where V = [V1  V2  . . .   Vn] ∈ Rn×n has orthonormal columns.
The Laplacian eigenmaps estimator is

i.e. 

(cid:107)y − θ(cid:107)2

2 + λ(cid:107)Dθ(cid:107)2
2 

ˆθLS = (I + λL)−1y 

ˆθLE = V[k]V T

[k]y  where V[k] = [V1  V2  . . .   Vk] ∈ Rn×k 

(5)

where now k ∈ {1  . . .   n} acts as a tuning parameter.
Laplacian smoothing and Laplacian eigenmaps are appealing because they are (relatively) simple:
they are just linear transformations of the data y. Indeed  as we are considering G to be a grid  both
estimators in (4)  (5) can be computed very quickly  in nearly O(n) time  since the columns of V
here are discrete cosine transform (DCT) basis vectors when d = 1  or Kronecker products thereof 
when d ≥ 2 (e.g.  [9  17  20  28]). The TV denoising estimator in (3)  on the other hand  cannot be
expressed in closed-form  and is much more difﬁcult to compute  especially when d ≥ 2  though
several advances have been made over the years (see the references above  and in particular [2] for an
efﬁcient operator-splitting algorithm and nice literature survey). Importantly  these computational
difﬁculties are often worth it: TV denoising often practically outperforms (cid:96)2-regularized estimators
like Laplacian smoothing (and also Laplacian eigenmaps) in image denoising tasks  as it is able to
better preserve sharp edges and object boundaries (this is now widely accepted  early references are 
e.g.  [1  10  8]). See Figure 1 for an example  using the often-studied “cameraman” image.
In the 1d setting  classical theory from nonparametric statistics draws a clear distinction between the
performance of TV denoising and estimators like Laplacian smoothing and Laplacian eigenmaps.
Perhaps surprisingly  this theory has not yet been fully developed in dimensions d ≥ 2. Arguably  the
comparison between TV denoising and Laplacian smoothing and Laplacian eigenmaps is even more
interesting in higher dimensions  because the computational gap between the methods is even larger
(the former method being much more expensive  say in 2d and 3d  than the latter two). Shortly  we
review the 1d theory  and what is known in d-dimensions  for d ≥ 2. First  we introduce notation.

Notation. For deterministic (nonrandom) sequences an  bn we write an = O(bn) to denote that
an/bn is upper bounded for all n large enough  and an (cid:16) bn to denote that both an = O(bn) and
a−1
n = O(b−1
n ). Also  for random sequences An  Bn  we write An = OP(Bn) to denote that An/Bn
is bounded in probability. We abbreviate a∧ b = min{a  b} and a∨ b = max{a  b}. For an estimator
ˆθ of the parameter θ0 in (1)  we deﬁne its mean squared error (MSE) to be

MSE(ˆθ  θ0) =

(cid:107)ˆθ − θ0(cid:107)2
2.

1
n

2

Noisy image

Laplacian smoothing

TV denoising

Figure 1: Comparison of Laplacian smoothing and TV denoising for the common “cameraman” image. TV
denoising provides a more visually appealing result  and also achieves aboutx a 35% reduction in MSE compared
to Laplacian smoothing (MSE being measured to the original image). Both methods were tuned optimally.
The risk of ˆθ is the expectation of its MSE  and for a set K ⊆ Rn  we deﬁne the minimax risk and
minimax linear risk to be

E(cid:2)MSE(ˆθ  θ0)(cid:3)

R(K) = inf
ˆθ

sup
θ0∈K

and RL(K) = inf

ˆθ linear

sup
θ0∈K

E(cid:2)MSE(ˆθ  θ0)(cid:3) 

respectively  where the inﬁmum on in the ﬁrst expression is over all estimators ˆθ  and in the second
expression over all linear estimators ˆθ  meaning that ˆθ = Sy for a matrix S ∈ Rn×n. We will
also refer to linear estimators as linear smoothers. Note that both Laplacian smoothing in (4) and
Laplacian eigenmaps in (5) are linear smoothers  but TV denoising in (3) is not. Lastly  in somewhat
of an abuse of nomenclature  we will often call the parameter θ0 in (1) a function  and a set of possible
values for θ0 as in (2) a function space; this comes from thinking of the components of θ0 as the
evaluations of an underlying function over n locations on the grid. This embedding has no formal
importance  but it is convenient notationally  and matches the notation in nonparametric statistics.

Review: TV denoising in 1d. The classical nonparametric statistics literature [13  12  18] provides
a more or less complete story for estimation under total variation constraints in 1d. See also [26] for
a translation of these results to a setting more consistent (notationally) to that in the current paper.
Assume that d = 1 and Cn = C > 0  a constant (not growing with n). The results in [12] imply that
(6)

R(T1(C)) (cid:16) n−2/3.

Furthermore  [18] proved that the TV denoiser ˆθTV in (3)  with λ (cid:16) n1/3  satisﬁes

MSE(ˆθTV  θ0) = OP(n−2/3) 

(7)
for all θ0 ∈ T1(C)  and is thus minimax rate optimal over T1(C). (In assessing rates here and through-
out  we do not distinguish between convergence in expectation versus convergence in probability.)
Wavelet denoising  under various choices of wavelet bases  also achieves the minimax rate. However 
many simpler estimators do not. To be more precise  it is shown in [12] that

RL(T1(C)) (cid:16) n−1/2.

(8)
Therefore  a substantial number of commonly used nonparametric estimators—such as running mean
estimators  smoothing splines  kernel smoothing  Laplacian smoothing  and Laplacian eigenmaps 
which are all linear smoothers—have a major deﬁciency when it comes to estimating functions of
bounded variation. Roughly speaking  they will require many more samples to estimate θ0 within
the same degree of accuracy as an optimal method like TV or wavelet denoising (on the order of
−1/2 times more samples to achieve an MSE of ). Further theory and empirical examples (e.g. 
[11  12  26]) offer the following perspective: linear smoothers cannot cope with functions in T (C)
that have spatially inhomogeneous smoothness  i.e.  that vary smoothly at some locations and vary
wildly at others. Linear smoothers can only produce estimates that are smooth throughout  or wiggly
throughout  but not a mix of the two. They can hence perform well over smaller  more homogeneous
function classes like Sobolev or Holder classes  but not larger ones like total variation classes (or
more generally  Besov and Triebel classes)  and for these  one must use more sophisticated  nonlinear
techniques. A motivating question: does such a gap persist in higher dimensions  between optimal
nonlinear and linear estimators  and if so  how big is it?

3

Review: TV denoising in multiple dimensions. Recently  [29] established rates for TV denoising
over various graph models  including grids  and [16] made improvements  particularly in the case of
d-dimensional grids with d ≥ 2. We can combine Propositions 4 and 6 of [16] with Theorem 3 of
[29] to give the following result: if d ≥ 2  and Cn is an arbitrary sequence (potentially unbounded
with n)  then the TV denoiser ˆθTV in (3) satisﬁes  over all θ0 ∈ Td(Cn) 

(cid:18) Cn

√

(cid:19)

(cid:19)
(cid:18) Cn log n
MSE(ˆθTV  θ0) = OP
with λ (cid:16) log n for d = 2  and λ (cid:16) √

n

result from the 1d case. We expand on this next.

for d = 2  and MSE(ˆθTV  θ0) = OP

for d ≥ 3 
(9)
log n for d ≥ 3. Note that  at ﬁrst glance  this is a very different

log n
n

2 Summary of results
A gap in multiple dimensions. For estimation of θ0 in (1) when d ≥ 2  consider  e.g.  the simplest
possible linear smoother: the mean estimator  ˆθmean = ¯y1 (where 1 = (1  . . .   1) ∈ Rn  the vector
of all 1s). Lemma 4  given below  implies that over θ0 ∈ Td(Cn)  the MSE of the mean estimator is
n/n for d ≥ 3. Compare this to (9). When
bounded in probability by C 2
Cn = C > 0 is a constant  i.e.  when the TV of θ0 is assumed to be bounded (which is assumed for
the 1d results in (6)  (7)  (8))  this means that the TV denoiser and the mean estimator converge to θ0
at the same rate  basically (ignoring log terms)  the “parametric rate” of 1/n  for estimating a ﬁnite-
dimensional parameter! That TV denoising and such a trivial linear smoother perform comparably
over 2d and 3d grids could not be farther from the story in 1d  where TV denoising is separated by an
unbridgeable gap from all linear smoothers  as shown in (6)  (7)  (8).
Our results in Section 3 clarify this conundrum  and can be summarized by three points.

n log n/n for d = 2  and C 2

• We argue in Section 3.1 that there is a proper “canonical” scaling for the TV class deﬁned in
(2). E.g.  when d = 1  this yields Cn (cid:16) 1  a constant  but when d = 2  this yields Cn (cid:16) √
n 
and Cn also diverges with n for all d ≥ 3. Sticking with d = 2 as an interesting example 
we see that under such a scaling  the MSE rates achieved by TV denoising and the mean
estimator respectively  are drastically different; ignoring log terms  these are

Cn
n

(cid:16) 1√
n

and C 2
n
n

(cid:16) 1 
√
respectively. Hence  TV denoising has an MSE rate of 1/
estimator has a constant rate  i.e.  a setting where it is not even known to be consistent.

n  in a setting where the mean
• We show in Section 3.3 that our choice to study the mean estimator here is not somehow
“unlucky” (it is not a particularly bad linear smoother  nor is the upper bound on its MSE
n/n  for all d ≥ 2. Thus  even
loose): the minimax linear risk over Td(Cn) is on the order C 2
the best linear smoothers have the same poor performance as the mean over Td(Cn).
• We show in Section 3.2 that the TV estimator is (essentially) minimax optimal over Td(Cn) 

(10)

as the minimax risk over this class scales as Cn/n (ignoring log terms).

To summarize  these results reveal a signiﬁcant gap between linear smoothers and optimal estimators
like TV denoising  for estimation over Td(Cn) in d dimensions  with d ≥ 2  as long as Cn scales
appropriately. Roughly speaking  the TV classes encompass a challenging setting for estimation
because they are very broad  containing a wide array of functions—both globally smooth functions 
said to have homogeneous smoothness  and functions with vastly different levels of smoothness at
different grid locations  said to have heterogeneous smoothness. Linear smoothers cannot handle
heterogeneous smoothness  and only nonlinear methods can enjoy good estimation properties over
n 
√
n rate (up to log factors)  meanwhile  the

the entirety of Td(Cn). To reiterate  a telling example is d = 2 with the canonical scaling Cn (cid:16) √
√
where we see that TV denoising achieves the optimal 1/
best linear smoothers have max risk that is constant over T2(

n). See Figure 2 for an illustration.

Minimax rates over smaller function spaces  and adaptivity. Sections 4 and 5 are focused on
different function spaces  discrete Sobolev spaces  which are (cid:96)2 analogs of discrete TV spaces as we
have deﬁned them in (2). Under the canonical scaling of Section 3.1  Sobolev spaces are contained in

4

Trivial scaling  Cn (cid:16) 1

Canonical scaling  Cn (cid:16) √

n

√
Figure 2: MSE curves for estimation over a 2d grid  under two very different scalings of Cn: constant and
n.
The parameter θ0 was a “one-hot” signal  with all but one component equal to 0. For each n  the results were
averaged over 5 repetitions  and Laplacian smoothing and TV denoising were tuned for optimal average MSE.

TV spaces  and the former can be roughly thought of as containing functions of more homogeneous
smoothness. The story now is more optimistic for linear smoothers  and the following is a summary.
• In Section 4  we derive minimax rates for Sobolev spaces  and prove that linear smoothers—
in particular  Laplacian smoothing and Laplacian eigenmaps—are optimal over these spaces.
• In Section 5  we discuss an interesting phenomenon  a phase transition of sorts  at d = 3
dimensions. When d = 1 or 2  the minimax rates for a TV space and its inscribed Sobolev
space match; when d ≥ 3  they do not  and the inscribed Sobolev space has a faster minimax
rate. Aside from being an interesting statement about the TV and Sobolev function spaces
in high dimensions  this raises an important question of adaptivity over the smaller Sobolev
function spaces. As the minimax rates match for d = 1 and 2  any method optimal over TV
spaces in these dimensions  such as TV denoising  is automatically optimal over the inscribed
Sobolev spaces. But the question remains open for d ≥ 3—does  e.g.  TV denoising adapt
to the faster minimax rate over Sobolev spaces? We present empirical evidence to suggest
that this may be true  and leave a formal study to future work.

Other considerations and extensions. There are many problems related to the one that we study
in this paper. Clearly  minimax rates for the TV and Sobolev classes over general graphs  not just
d-dimensional grids  are of interest. Our minimax lower bounds for TV classes actually apply to
generic graphs with bounded max degree  though it is unclear whether to what extent they are sharp
beyond grids; a detailed study will be left to future work. Another related topic is that of higher-order
smoothness classes  e.g.  classes containing functions whose derivatives are of bounded variation.
The natural extension of TV denoising here is called trend ﬁltering  deﬁned via the regularization of
discrete higher-order derivatives. In the 1d setting  minimax rates  the optimality of trend ﬁltering 
and the suboptimality of linear smoothers is already well-understood [26]. Trend ﬁltering has been
deﬁned and studied to some extent on general graphs [29]  but no notions of optimality have been
investigated beyond 1d. This will also be left to future work. Lastly  it is worth mentioning that there
are other estimators (i.e.  other than the ones we study in detail) that attain or nearly attain minimax
rates over various classes we consider in this paper. E.g.  wavelet denoising is known to be optimal
over TV classes in 1d [12]; and comparing recent upper bounds from [19  16] with the lower bounds
in this work  we see that wavelet denoising is also nearly minimax in 2d (ignoring log terms).

3 Analysis over TV classes

3.1 Canonical scalings for TV and Sobolev classes
We start by establishing what we call a “canonical” scaling for the radius Cn of the TV ball Td(Cn)
in (2)  as well as the radius C(cid:48)

n of the Sobolev ball Sd(C(cid:48)

n)  deﬁned as

n) =(cid:8)θ : (cid:107)Dθ(cid:107)2 ≤ C(cid:48)

n

(cid:9).

Sd(C(cid:48)

5

(11)

n102103104105MSE10-410-310-210-1100TVdenoising(-ttedslope-0.88)Laplaciansmoothing(-ttedslope-0.99)Meanestimator(-ttedslope-1.01)Trivialrate:n!1n102103104105MSE10-410-310-210-1100TVdenoising(-ttedslope-0.84)Laplaciansmoothing(-ttedslope-0.01)Meanestimator(-ttedslope0.00)Minimaxrate:n!1=2n). To study (2)  (11)  it helps to introduce a third function space 

Proper scalings for Cn  C(cid:48)
n will be critical for properly interpreting our new results in d dimensions 
(cid:110)
√
in a way that is comparable to known results for d = 1 (which are usually stated in terms of the 1d
n (cid:16) 1/
scalings Cn (cid:16) 1  C(cid:48)
θ : θi = f (i1/(cid:96) . . .   id/(cid:96))  i = 1  . . .   n  for some f ∈ Hcont
Hd(1) =

(12)
Above  we have mapped each location i on the grid to a multi-index (i1  . . .   id) ∈ {1  . . .   (cid:96)}d  where
(cid:96) = n1/d  and Hcont
(1) denotes the (usual) continuous Holder space on [0  1]d  i.e.  functions that are
1-Lipschitz with respect to the (cid:96)∞ norm. We seek an embedding that is analogous to the embedding
of continuous Holder  Sobolev  and total variation spaces in 1d functional analysis  namely 

(cid:111)

(1)

d

d

.

Hd(1) ⊆ Sd(C(cid:48)

n) ⊆ Td(Cn).

Our ﬁrst lemma provides a choice of Cn  C(cid:48)
in this paper  can be found in the supplementary document.
Lemma 1. For d ≥ 1  the embedding in (13) holds with choices Cn (cid:16) n1−1/d and C(cid:48)
Such choices are called the canonical scalings for the function classes in (2)  (11).

(13)
n that makes the above true. Its proof  as with all proofs
n (cid:16) n1/2−1/d.

As a sanity check  both the (usual) continuous Holder and Sobolev function spaces in d dimensions
are known to have minimax risks that scale as n−2/(2+d)  in a standard nonparametric regression
n (cid:16) n1/2−1/d  our results in Section 4 show that the
setup (e.g.  [14]). Under the canonical scaling C(cid:48)
discrete Sobolev class Sd(n1/2−1/d) also admits a minimax rate of n−2/(2+d).

3.2 Minimax rates over TV classes
The following is a lower bound for the minimax risk of the TV class Td(Cn) in (2).
Theorem 2. Assume n ≥ 2  and denote dmax = 2d. Then  for constants c > 0  ρ1 ∈ (2.34  2.35) 

(cid:112)1 + log(σdmaxn/Cn)

dmaxn

maxn) ∨ σ2/n



σCn

n/(d2
C 2
σ2/ρ1

R(Td(Cn)) ≥ c ·

√
if Cn ∈ [σdmax
√
if Cn < σdmax
if Cn > σdmaxn/

log n  σdmaxn/
√
log n
ρ1

√

ρ1]

. (14)

√

√

log n  σdmaxn/

(cid:112)log(n/Cn)/n. When d = 2  we see that this is very close to the upper

The proof uses a simplifying reduction of the TV class  via Td(Cn) ⊇ B1(Cn/dmax)  the latter set
denoting the (cid:96)1 ball of radius Cn/dmax in Rn. It then invokes a sharp characterization of the minimax
risk in normal means problems over (cid:96)p balls due to [6]. Several remarks are in order.
Remark 1. The ﬁrst line on the right-hand side in (14) often provides the most useful lower bound.
To see this  recall that under the canonical scaling for TV classes  we have Cn = n1−1/d. For all
d ≥ 2  this certainly implies Cn ∈ [σdmax
ρ1]  for large n.
√
Remark 2. Even though its construction is very simple  the lower bound on the minimax risk in (14)
is sharp or nearly sharp in many interesting cases. Assume that Cn ∈ [σdmax
ρ1].
The lower bound rate is Cn
bound rate of Cn log n/n achieved by the TV denoiser  as stated in (9). These two differ by at most a
log n factor (achieved when Cn (cid:16) n). When d ≥ 3  we see that the lower bound rate is even closer
to the upper bound rate of Cn
log n/n achieved by the TV denoiser  as in (9). These two now differ
log n factor (again achieved when Cn (cid:16) n). We hence conclude that the TV denoiser
by at most a
is essentially minimax optimal in all dimensions d ≥ 2.
Remark 3. When d = 1  and (say) Cn (cid:16) 1  the lower bound rate of
log n/n given by Theorem 2
is not sharp; we know from [12] (recall (6)) that the minimax rate over T1(1) is n−2/3. The result in
the theorem (and also Theorem 3) in fact holds more generally  beyond grids: for an arbitrary graph
G  its edge incidence matrix D  and Td(Cn) as deﬁned in (2)  the result holds for dmax equal to the
max degree of G. It is unclear to what extent this is sharp  for different graph models.

log n  σdmaxn/

√

√

√

√

3.3 Minimax linear rates over TV classes
We now turn to a lower bound on the minimax linear risk of the TV class Td(Cn) in (2).
Theorem 3. Recall the notation dmax = 2d. Then

(cid:32)

(cid:33)

RL(Td(Cn)) ≥

σ2C 2
n
n + σ2d2
maxn

C 2

∨ σ2
n

≥ 1
2

6

C 2
n
d2
maxn

∧ σ2

∨ σ2
n

.

(15)

The proof relies on an elegant meta-theorem on minimax rates from [13]  which uses the concept of a
“quadratically convex” set  whose minimax linear risk is the same as that of its hardest rectangular
subproblem. An alternative proof can be given entirely from ﬁrst principles.
Remark 4. When C 2
n  at most)  the lower bound rate in
n/n. Compared to the Cn/n minimax rate from Theorem 2 (ignoring log terms)  we
(15) will be C 2
see a clear gap between optimal nonlinear and linear estimators. In fact  under the canonical scaling
Cn (cid:16) n1−1/d  for any d ≥ 2  this gap is seemingly huge: the lower bound for the minimax linear
rate will be a constant  whereas the minimax rate from Theorem 2 (ignoring log terms) will be n−1/d.

n grows with n  but not too fast (scales as

√

We now show that the lower bound in Theorem 3 is essentially tight  and remarkably  it is certiﬁed by
analyzing two trivial linear estimators: the mean estimator and the identity estimator.
Lemma 4. Let Mn denote the largest column norm of D†. For the mean estimator ˆθmean = ¯y1 

E(cid:2)MSE(ˆθmean  θ0)(cid:3) ≤ σ2 + C 2

nM 2
n

 

n

sup

θ0∈Td(Cn)

√
From Proposition 4 in [16]  we have Mn = O(

log n) when d = 2 and Mn = O(1) when d ≥ 3.
The risk of the identity estimator ˆθid = y is clearly σ2. Combining this logic with Lemma 4 gives the
n)/n ∧ σ2. Comparing this with the lower bound described
upper bound RL(Td(Cn)) ≤ (σ2 + C 2
in Remark 4  we see that the two rates basically match  modulo the M 2
n factor in the upper bound 
which only provides an extra log n factor when d = 2. The takeaway message: in the sense of max
risk  the best linear smoother does not perform much better than the trivial estimators.
Additional empirical experiments  similar to those shown in Figure 2  are given in the supplement.

nM 2

4 Analysis over Sobolev classes
Our ﬁrst result here is a lower bound on the minimax risk of the Sobolev class Sd(C(cid:48)
Theorem 5. For a universal constant c > 0 

n) in (11).

(cid:16)

R(Sd(C(cid:48)

n)) ≥ c
n

(nσ2)

2

d+2 (C(cid:48)
n)

2d

d+2 ∧ nσ2 ∧ n2/d(C(cid:48)

n)2(cid:17)

+

σ2
n

.

Elegant tools for minimax analysis from [13]  which leverage the fact that the ellipsoid Sd(C(cid:48)
orthosymmetric and quadratically convex (after a rotation)  are used to prove the result.
The next theorem gives upper bounds  certifying that the above lower bound is tight  and showing
that Laplacian eigenmaps and Laplacian smoothing  both linear smoothers  are optimal over Sd(C(cid:48)
n).
n)d)2/(d+2) ∨ 1) ∧ n  we have
Theorem 6. For Laplacian eigenmaps  ˆθLE in (5)  with k (cid:16) ((n(C(cid:48)

n) is

E(cid:2)MSE(ˆθLE  θ0)(cid:3) ≤ c

(cid:16)

n

sup

θ0∈Sd(C(cid:48)
n)

(nσ2)

2

d+2 (C(cid:48)
n)

2d

d+2 ∧ nσ2 ∧ n2/d(C(cid:48)

n)2(cid:17)

+

cσ2
n

 

for a universal constant c > 0  and n large enough. When d = 1  2  or 3  the same bound holds for
Laplacian smoothing ˆθLS in (5)  with λ (cid:16) (n/(C(cid:48)
n)2)2/(d+2) (and a possibly different constant c).

5 A phase transition  and adaptivity
The TV and Sobolev classes in (2) and (11)  respectively  display a curious relationship. We reﬂect on
Theorems 2 and 5  using  for concreteness  the canonical scalings Cn (cid:16) n1−1/d and C(cid:48)
n (cid:16) n1/2−1/d
(that  recall  guarantee Sd(C(cid:48)
n) ⊆ Td(Cn))). When d = 1  both the TV and Sobolev classes have a
minimax rate of n−2/3 (this TV result is actually due to [12]  as stated in (6)  not Theorem 2). When
d = 2  both the TV and Sobolev classes again have the same minimax rate of n−1/2  the caveat being
log n factor. But for all d ≥ 3  the rates for the canonical TV
that the rate for TV class has an extra
and Sobolev classes differ  and the smaller Sobolev spaces have faster rates than their inscribing TV
spaces. This may be viewed as a phase transition at d = 3; see Table 1.
We may paraphrase to say that 2d is just like 1d  in that expanding the Sobolev ball into a larger TV
ball does not hurt the minimax rate  and methods like TV denoising are automatically adaptive  i.e. 

√

7

Function class

TV ball Td(n1−1/d)

Sobolev ball Sd(n1/2−1/d)

Dimension 1 Dimension 2 Dimension d ≥ 3
log n

n−1/d√
n− 2
Table 1: Summary of rates for canonically-scaled TV and Sobolev spaces.

n−1/2√
n−1/2

n−2/3
n−2/3

log n

2+d

Linear signal in 2d

Linear signal in 3d

Figure 3: MSE curves for estimating a “linear” signal  a very smooth signal  over 2d and 3d grids. For each n 
the results were averaged over 5 repetitions  and Laplacian smoothing and TV denoising were tuned for best
average MSE performance. The signal was set to satisfy (cid:107)Dθ0(cid:107)2 (cid:16) n1/2−1/d  matching the canonical scaling.

optimal over both the bigger and smaller classes. However  as soon as we enter the 3d world  it is no
longer clear whether TV denoising can adapt to the smaller  inscribed Sobolev ball  whose minimax
rate is faster  n−2/5 versus n−1/3 (ignoring log factors). Theoretically  this is an interesting open
problem that we do not approach in this paper and leave to future work.
We do  however  investigate the matter empirically: see Figure 3  where we run Laplacian smoothing
and TV denoising on a highly smooth “linear” signal θ0. This is constructed so that each component
θi is proportional to i1 + i2 + . . . + id (using the multi-index notation (i1  . . .   id) of (12) for grid
location i)  and the Sobolev norm is (cid:107)Dθ0(cid:107)2 (cid:16) n1/2−1/d. Arguably  these are among the “hardest”
types of functions for TV denoising to handle. The left panel  in 2d  is a case in which we know that
TV denoising attains the minimax rate; the right panel  in 3d  is a case in which we do not  though
empirically  TV denoising surely seems to be doing better than the slower minimax rate of n−1/3
(ignoring log terms) that is associated with the larger TV ball.
Even if TV denoising is shown to be minimax optimal over the inscribed Sobolev balls when d ≥ 3 
note that this does not necessarily mean that we should scrap Laplacian smoothing in favor of TV
denoising  in all problems. Laplacian smoothing is the unique Bayes estimator in a normal means
model under a certain Markov random ﬁeld prior (e.g.  [22]); statistical decision theory therefore tells
that it is admissible  i.e.  no other estimator—TV denoising included—can uniformly dominate it.

6 Discussion
We conclude with a quote from Albert Einstein: “Everything should be made as simple as possible 
but no simpler”. In characterizing the minimax rates for TV classes  deﬁned over d-dimensional grids 
we have shown that simple methods like Laplacian smoothing and Laplacian eigenmaps—or even in
fact  all linear estimators—must be passed up in favor of more sophisticated  nonlinear estimators 
like TV denoising  if one wants to attain the optimal max risk. Such a result was previously known
when d = 1; our work has extended it to all dimensions d ≥ 2. We also characterized the minimax
rates over discrete Sobolev classes  revealing an interesting phase transition where the optimal rates
over TV and Sobolev spaces  suitably scaled  match when d = 1 and 2 but diverge for d ≥ 3. It is an
open question as to whether an estimator like TV denoising can be optimal over both spaces  for all d.
Acknolwedgements. We thank Jan-Christian Hutter and Philippe Rigollet  whose paper [16] inspired
us to think carefully about problem scalings (i.e.  radii of TV and Sobolev classes) in the ﬁrst place.
YW was supported by NSF Award BCS-0941518 to CMU Statistics  a grant by Singapore NRF under
its International Research Centre @ Singapore Funding Initiative  and a Baidu Scholarship. RT was
supported by NSF Grants DMS-1309174 and DMS-1554123.

8

n102103104105MSE10-310-210-1100TVdenoising(-ttedslope-0.54)Laplaciansmoothing(-ttedslope-0.62)TV-ballminimaxrate:n!1=2Sobolev-ballminimaxrate:n!1=2n102103104105MSE10-310-210-1100TVdenoising(-ttedslope-0.44)Laplaciansmoothing(-ttedslope-0.50)TV-ballminimaxrate:n!1=3Sobolev-ballminimaxrate:n!2=5References
[1] Robert Acar and Curtis R. Vogel. Analysis of total variation penalty methods. Inverse Problems  10:

[2] Alvero Barbero and Suvrit Sra. Modular proximal optimization for multidimensional total-variation

1217–1229  1994.

regularization. arXiv: 1411.0589  2014.

[3] Mikhail Belkin and Partha Niyogi. Using manifold structure for partially labelled classiﬁcation. Advances

[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representa-

in Neural Information Processing Systems  15  2002.

tion. Neural Computation  15(6):1373–1396  2003.

[5] Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for Laplacian-based manifold methods.

Conference on Learning Theory (COLT-05)  18  2005.

[6] Lucien Birge and Pascal Massart. Gaussian model selection. Journal of the European Mathematical

Society  3(3):203–268  2001.

[7] Antonin Chambolle and Jerome Darbon. On total variation minimization and surface evolution using

parametric maximum ﬂows. International Journal of Computer Vision  84:288–307  2009.

[8] Antonin Chambolle and Pierre-Louis Lions. Image recovery via total variation minimization and related

problems. Numerische Mathematik  76(2):167–188  1997.

[9] Samuel Conte and Carl de Boor. Elementary Numerical Analysis: An Algorithmic Approach. McGraw-Hill 

New York  1980. International Series in Pure and Applied Mathematics.

[10] David Dobson and Fadil Santosa. Recovery of blocky images from noisy and blurred data. SIAM Journal

on Applied Mathematics  56(4):1181–1198  1996.

[11] David Donoho and Iain Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika  81(3):

[12] David Donoho and Iain Johnstone. Minimax estimation via wavelet shrinkage. Annals of Statistics  26(8):

425–455  1994.

879–921  1998.

[13] David Donoho  Richard Liu  and Brenda MacGibbon. Minimax risk over hyperrectangles  and implications.

[14] Laszlo Gyorﬁ  Michael Kohler  Adam Krzyzak  and Harro Walk. A Distribution-Free Theory of Nonpara-

[15] Holger Hoeﬂing. A path algorithm for the fused lasso signal approximator. Journal of Computational and

[16] Jan-Christian Hutter and Philippe Rigollet. Optimal rates for total variation denoising. In Conference on

[17] Hans Kunsch. Robust priors for smoothing and image restoration. Annals of the Institute of Statistical

Annals of Statistics  18(3):1416–1437  1990.

metric Regression. Springer  New York  2002.

Graphical Statistics  19(4):984–1006  2010.

Learning Theory (COLT-16)  2016. to appear.

Mathematics  46(1):1–19  1994.

387–413  1997.

[18] Enno Mammen and Sara van de Geer. Locally apadtive regression splines. Annals of Statistics  25(1):

[19] Deanna Needell and Rachel Ward. Stable image reconstruction using total variation minimization. SIAM

Journal on Imaging Sciences  6(2):1035–1058  2013.

[20] Michael Ng  Raymond Chan  and Wun-Cheung Tang. A fast algorithm for deblurring models with

Neumann boundary conditions. SIAM Journal on Scientiﬁc Computing  21(3):851–866  1999.

[21] Leonid Rudin  Stanley Osher  and Emad Faterni. Nonlinear total variation based noise removal algorithms.

Physica D: Nonlinear Phenomena  60:259–268  1992.

[22] James Sharpnack and Aarti Singh. Identifying graph-structured activation patterns in networks. Advances

in Neural Information Processing Systems  13  2010.

[23] James Sharpnack  Alessandro Rinaldo  and Aarti Singh. Sparsistency of the edge lasso over graphs.
Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics  15:1028–1036  2012.
[24] Alexander Smola and Risi Kondor. Kernels and regularization on graphs. Proceedings of the Annual

Conference on Learning Theory  16  2003.

[25] Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and smoothness

via the fused lasso. Journal of the Royal Statistical Society: Series B  67(1):91–108  2005.

[26] Ryan J. Tibshirani. Adaptive piecewise polynomial estimation via trend ﬁltering. Annals of Statistics  42

[27] Ryan J. Tibshirani and Jonathan Taylor. The solution path of the generalized lasso. Annals of Statistics  39

(1):285–323  2014.

(3):1335–1371  2011.

[28] Yilun Wang  Junfeng Yang  Wotao Yin  and Yin Zhang. A new alternating minimization algorithm for

total variation image reconstruction. SIAM Journal on Imaging Sciences  1(3):248–272  2008.

[29] Yu-Xiang Wang  James Sharpnack  Alex Smola  and Ryan J. Tibshirani. Trend ﬁltering on graphs. Journal

of Machine Learning Research  2016. To appear.

[30] Xiaojin Zhu  Zoubin Ghahramani  and John Lafferty. Semi-supervised learning using Gaussian ﬁelds and

harmonic functions. International Conference on Machine Learning (ICML-03)  20  2003.

9

,Veeranjaneyulu Sadhanala
Yu-Xiang Wang
Ryan Tibshirani
Qiang Liu