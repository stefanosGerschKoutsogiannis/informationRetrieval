2015,Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition,Since being analyzed by Rokhlin  Szlam  and Tygert and popularized by Halko  Martinsson  and Tropp  randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms  yet still converges quickly for *any* matrix  independently of singular value gaps. After ~O(1/epsilon) iterations  it gives a low-rank approximation within (1+epsilon) of optimal for spectral norm error.We give the first provable runtime improvement on Simultaneous Iteration: a randomized block Krylov method  closely related to the classic Block Lanczos algorithm  gives the same guarantees in just ~O(1/sqrt(epsilon)) iterations and performs substantially better experimentally. Our analysis is the first of a Krylov subspace method that does not depend on singular value gaps  which are unreliable in practice.Furthermore  while it is a simple accuracy benchmark  even (1+epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components  a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods.,Randomized Block Krylov Methods for Stronger and
Faster Approximate Singular Value Decomposition

Cameron Musco

Christopher Musco

Massachusetts Institute of Technology  EECS

Massachusetts Institute of Technology  EECS

Cambridge  MA 02139  USA

cnmusco@mit.edu

Cambridge  MA 02139  USA

cpmusco@mit.edu

Abstract

Since being analyzed by Rokhlin  Szlam  and Tygert [1] and popularized by
Halko  Martinsson  and Tropp [2]  randomized Simultaneous Power Iteration has
become the method of choice for approximate singular value decomposition. It is
more accurate than simpler sketching algorithms  yet still converges quickly for
any matrix  independently of singular value gaps. After ˜O(1/) iterations  it gives
a low-rank approximation within (1 + ) of optimal for spectral norm error.
We give the ﬁrst provable runtime improvement on Simultaneous Iteration: a ran-
√
domized block Krylov method  closely related to the classic Block Lanczos algo-
rithm  gives the same guarantees in just ˜O(1/
) iterations and performs substan-
tially better experimentally. Our analysis is the ﬁrst of a Krylov subspace method
that does not depend on singular value gaps  which are unreliable in practice.
Furthermore  while it is a simple accuracy benchmark  even (1 + ) error for
spectral norm low-rank approximation does not imply that an algorithm returns
high quality principal components  a major issue for data applications. We address
this problem for the ﬁrst time by showing that both Block Krylov Iteration and
Simultaneous Iteration give nearly optimal PCA for any matrix. This result further
justiﬁes their strength over non-iterative sketching methods.

Introduction

1
Any matrix A ∈ Rn×d with rank r can be written using a singular value decomposition (SVD) as
A = UΣVT . U ∈ Rn×r and V ∈ Rd×r have orthonormal columns (A’s left and right singular
vectors) and Σ ∈ Rr×r is a positive diagonal matrix containing A’s singular values: σ1 ≥ . . . ≥ σr.
A rank k partial SVD algorithm returns just the top k left or right singular vectors of A. These are
the ﬁrst k columns of U or V  denoted Uk and Vk respectively.
Among countless applications  the SVD is used for optimal low-rank approximation and principal
component analysis (PCA). Speciﬁcally  for k < r  a partial SVD can be used to construct a rank k
approximation Ak such that both (cid:107)A − Ak(cid:107)F and (cid:107)A − Ak(cid:107)2 are as small as possible. We simply
set Ak = UkUT
k A. That is  Ak is A projected onto the space spanned by its top k singular vectors.
For principal component analysis  A’s top singular vector u1 provides a top principal component 
which describes the direction of greatest variance within A. The ith singular vector ui provides the
ith principal component  which is the direction of greatest variance orthogonal to all higher principal
components. Formally  denoting A’s ith singular value as σi 

i AAT ui = σ2
uT

i =

max

x:(cid:107)x(cid:107)2=1  x⊥uj∀j<i

xT AAT x.

Traditional SVD algorithms are expensive  typically running in O(nd2) time  so there has been sub-
stantial research on randomized techniques that seek nearly optimal low-rank approximation and

1

PCA [3  4  1  2  5]. These methods are quickly becoming standard tools in practice and implemen-
tations are widely available [6  7  8  9]  including in popular learning libraries [10].
Recent work focuses on algorithms whose runtimes do not depend on properties of A. In contrast 
classical literature typically gives runtime bounds that depend on the gaps between A’s singular
values and become useless when these gaps are small (which is often the case in practice – see
Section 6). This limitation is due to a focus on how quickly approximate singular vectors converge
to the actual singular vectors of A. When two singular vectors have nearly identical values they are
difﬁcult to distinguish  so convergence inherently depends on singular value gaps.
Only recently has a shift in approximation goal  along with an improved understanding of random-
ization  allowed for algorithms that avoid gap dependence and thus run provably fast for any matrix.
For low-rank approximation and PCA  we only need to ﬁnd a subspace that captures nearly as much
variance as A’s top singular vectors – distinguishing between two close singular values is overkill.

1.1 Prior Work
The fastest randomized SVD algorithms [3  5] run in O(nnz(A)) time1  are based on non-iterative
sketching methods  and return a rank k matrix Z with orthonormal columns z1  . . .   zk satisfying

i>k σ2

(cid:107)A − ZZT A(cid:107)F ≤ (1 + )(cid:107)A − Ak(cid:107)F .

Frobenius Norm Error:

F =(cid:80)

(1)
Unfortunately  as emphasized in prior work [1  2  11  12]  Frobenius norm error is often hopelessly
insufﬁcient  especially for data analysis and learning applications. When A has a “heavy-tail” of
singular values  which is common for noisy data  (cid:107)A − Ak(cid:107)2
i can be huge  potentially
much larger than A’s top singular value. This renders (1) meaningless since Z does not need to
align with any large singular vectors to obtain good multiplicative error.
To address this shortcoming  a number of papers target spectral norm low-rank approximation error 
(2)
which is intuitively stronger. When looking for a rank k approximation  A’s top k singular vectors
are often considered data and the remaining tail is considered noise. A spectral norm guarantee
roughly ensures that ZZT A recovers A up to this noise threshold.
A series of work [1  2  13  14  15] shows that the decades old Simultaneous Power Iteration (also
called subspace iteration or orthogonal iteration) implemented with random start vectors  achieves
(2) after ˜O(1/) iterations. Hence  this method  which was popularized by Halko  Martinsson  and
Tropp in [2]  has become the randomized SVD algorithm of choice for practitioners [10  16].

(cid:107)A − ZZT A(cid:107)2 ≤ (1 + )(cid:107)A − Ak(cid:107)2 

Spectral Norm Error:

2 Our Results

Algorithm 1 SIMULTANEOUS ITERATION
input: A ∈ Rn×d  error  ∈ (0  1)  rank k ≤ n  d
output: Z ∈ Rn×k
1: q := Θ( log d

 )  Π ∼ N (0  1)d×k

2: K :=(cid:0)AAT(cid:1)q

AΠ

Q ∈ Rn×k.

3: Orthonormalize the columns of K to obtain
4: Compute M := QT AAT Q ∈ Rk×k.
5: Set ¯Uk to the top k singular vectors of M.
6: return Z = Q ¯Uk.

Algorithm 2 BLOCK KRYLOV ITERATION
input: A ∈ Rn×d  error  ∈ (0  1)  rank k ≤ n  d
output: Z ∈ Rn×k
1: q := Θ( log d√

2: K :=(cid:2)AΠ  (AAT )AΠ  ...  (AAT )qAΠ(cid:3)

 )  Π ∼ N (0  1)d×k

Q ∈ Rn×qk.

3: Orthonormalize the columns of K to obtain
4: Compute M := QT AAT Q ∈ Rqk×qk.
5: Set ¯Uk to the top k singular vectors of M.
6: return Z = Q ¯Uk.

2.1 Faster Algorithm
We show that Algorithm 2  a randomized relative of the Block Lanczos algorithm [17  18]  which
√
we call Block Krylov Iteration  gives the same guarantees as Simultaneous Iteration (Algorithm 1)
in just ˜O(1/
) iterations. This not only gives the fastest known theoretical runtime for achieving
(2)  but also yields substantially better performance in practice (see Section 6).

1Here nnz(A) is the number of non-zero entries in A and this runtime hides lower order terms.

2

Even though the algorithm has been discussed and tested for potential improvement over Simulta-
neous Iteration [1  19  20]  theoretical bounds for Krylov subspace and Lanczos methods are much
more limited. As highlighted in [11] 

“Despite decades of research on Lanczos methods  the theory for [randomized
power iteration] is more complete and provides strong guarantees of excellent
accuracy  whether or not there exist any gaps between the singular values.”

Our work addresses this issue  giving the ﬁrst gap independent bound for a Krylov subspace method.
2.2 Stronger Guarantees

In addition to runtime improvements  we target a much stronger notion of approximate SVD that is
needed for many applications  but for which no gap-independent analysis was known.
Speciﬁcally  as noted in [21]  while intuitively stronger than Frobenius norm error  (1 + ) spec-
tral norm low-rank approximation error does not guarantee any accuracy in Z for many matrices2.
Consider A with its top k + 1 squared singular values all equal to 10 followed by a tail of smaller
singular values (e.g. 1000k at 1). (cid:107)A − Ak(cid:107)2
2 = 10 for any rank
k Z  leaving the spectral norm bound useless. At the same time  (cid:107)A − Ak(cid:107)2
F is large  so Frobenius
error is meaningless as well. For example  any Z obtains (cid:107)A − ZZT A(cid:107)2
With this scenario in mind  it is unsurprising that low-rank approximation guarantees fail as an
accuracy measure in practice. We ran a standard sketch-and-solve approximate SVD algorithm
(see Section 3) on SNAP/AMAZON0302  an Amazon product co-purchasing dataset [22  23]  and
achieved very good low-rank approximation error in both norms for k = 30:

2 = 10 but in fact (cid:107)A − ZZT A(cid:107)2

F ≤ (1.01)(cid:107)A − Ak(cid:107)2
F .

(cid:107)A − ZZT A(cid:107)F < 1.001(cid:107)A − Ak(cid:107)F

and

(cid:107)A − ZZT A(cid:107)2 < 1.038(cid:107)A − Ak(cid:107)2.

However  the approximate principal components given by Z are of signiﬁcantly lower quality than
A’s true singular vectors (see Figure 1). We saw similar results for a number of other datasets.

Figure 1: Poor per vector error (3) for SNAP/AMAZON0302 returned by a sketch-and-solve ap-
proximate SVD that gives very good low-rank approximation in both spectral and Frobenius norm.

We address this issue by introducing a per vector guarantee that requires each approximate singular
vector z1  . . .   zk to capture nearly as much variance as the corresponding true singular vector:

Per Vector Error:

(3)
The error bound (3) is very strong in that it depends on σ2
k+1  which is better then relative error
for A’s large singular values. While it is reminiscent of the bounds sought in classical numerical
analysis [24]  we stress that (3) does not require each zi to converge to ui in the presence of small
singular value gaps. In fact  we show that both randomized Block Krylov Iteration and our slightly
modiﬁed Simultaneous Iteration algorithm achieve (3) in gap-independent runtimes.

i AAT zi

k+1.

(cid:12)(cid:12) ≤ σ2

∀i  (cid:12)(cid:12)uT

i AAT ui − zT

2.3 Main Result

Our contributions are summarized in Theorem 1. Its detailed proof is relegated to the full version of
this paper [25]. The runtimes are given in Theorems 6 and 7  and the three error bounds shown in
Theorems 10  11  and 12. In Section 4 we provide a sketch of the main ideas behind the result.

2In fact  it does not even imply (1 + ) Frobenius norm error.

3

5101520253050100150200250300350400450Index iSingular Value σi 2 = uiT(AAT)ui ziT(AAT)ziTheorem 1 (Main Theorem). With high probability  Algorithms 1 and 2 ﬁnd approximate singular
vectors Z = [z1  . . .   zk] satisfying guarantees (1) and (2) for low-rank approximation and (3) for
PCA. For error   Algorithm 1 requires q = O(log d/) iterations while Algorithm 2 requires q =
) iterations. Excluding lower order terms  both algorithms run in time O(nnz(A)kq).
O(log d/

√

In the full version of this paper we also use our results to give an alternative analysis that does
depend on singular value gaps and can offer signiﬁcantly faster convergence when A has decaying
singular values. It is possible to take further advantage of this result by running Algorithms 1 and 2
with a Π that has > k columns  a simple modiﬁcation for accelerating either method.
In Section 6 we test both algorithms on a number of large datasets. We justify the importance of gap
independent bounds for predicting algorithm convergence and we show that Block Krylov Iteration
in fact signiﬁcantly outperforms the more popular Simultaneous Iteration.
2.4 Comparison to Classical Bounds
Decades of work has produced a variety of gap dependent bounds for Krylov methods [26]. Most
relevant to our work are bounds for block Krylov methods with block size equal to k [27]. Roughly
speaking  with randomized initialization  these results offer guarantees equivalent to our strong equa-

tion (3) for the top k singular directions after O(log(d/)/(cid:112)σk/σk+1 − 1) iterations.

This bound is recovered in Section 7 of this paper’s full version [25]. When the target accuracy 
is smaller than the relative singular value gap (σk/σk+1 − 1)  it is tighter than our gap independent
results. However  as discussed in Section 6  for high dimensional data problems where  is set far
above machine precision  gap independent bounds more accurately predict required iteration count.
Prior work also attempts to analyze algorithms with block size smaller than k [24]. While “small
block” algorithms offer runtime advantages  it is well understood that with b duplicate singular
values  it is impossible to recover the top k singular directions with a block of size < b [28]. More
generally  large singular value clusters slow convergence  so any small block algorithm must have
runtime dependence on the gaps between each adjacent pair of top singular values [29].

3 Analyzing Simultaneous Iteration
Before discussing our proof of Theorem 1  we review prior work on Simultaneous Iteration to
demonstrate how it can achieve the spectral norm guarantee (2).
Algorithms for Frobenius norm error (1) typically work by sketching A into very few dimensions
using a Johnson-Lindenstrauss random projection matrix Π with poly(k/) columns.

An×d × Πd×poly(k/) = (AΠ)n×poly(k/)

Π is usually a random Gaussian or (possibly sparse) random sign matrix and Z is computed using
the SVD of AΠ or of A projected onto AΠ [3  5  30]. This “sketch-and-solve” approach is very
efﬁcient – the computation of AΠ is easily parallelized and  regardless  pass-efﬁcient in a single
processor setting. Furthermore  once a small compression of A is obtained  it can be manipulated
in fast memory for the ﬁnal computation of Z.
However  Frobenius norm error seems an inherent limitation of sketch-and-solve methods. The
noise from A’s lower r − k singular values corrupts AΠ  making it impossible to extract a good
partial SVD if the sum of these singular values (equal to (cid:107)A − Ak(cid:107)2
In order to achieve spectral norm error (2)  Simultaneous Iteration must reduce this noise down to
the scale of σk+1 = (cid:107)A − Ak(cid:107)2. It does this by working with the powered matrix Aq [31].3 By the
spectral theorem  Aq has exactly the same singular vectors as A  but its singular values are equal to
those of A raised to the qth power. Powering spreads the values apart and accordingly  Aq’s lower
singular values are relatively much smaller than its top singular values (see example in Figure 2a).
 ) is sufﬁcient to increase any singular value ≥ (1 + )σk+1 to be signiﬁ-
Speciﬁcally  q = O( log d
cantly (i.e. poly(d) times) larger than any value ≤ σk+1. This effectively denoises our problem –
if we use a sketching method to ﬁnd a good Z for approximating Aq up to Frobenius norm error  Z
will have to align very well with every singular vector with value ≥ (1 + )σk+1. It thus provides
an accurate basis for approximating A up to small spectral norm error.

F ) is too large.

3For nonsymmetric matrices we work with (AAT )qA  but present the symmetric case here for simplicity.

4

(a) A’s singular values compared to those of
Aq  rescaled to match on σ1. Notice the sig-
niﬁcantly reduced tail after σ8.

√
(b) An O(1/
)-degree Chebyshev polyno-
√
)(x)  pushes low values nearly
mial  TO(1/
as close to zero as xO(1/).

Figure 2: Replacing A with a matrix polynomial facilitates higher accuracy approximation.

Computing Aq directly is costly  so AqΠ is computed iteratively – start with a random Π and
repeatedly multiply by A on the left. Since even a rough Frobenius norm approximation for Aq
sufﬁces  Π can be chosen to have just k columns. Each iteration thus takes O(nnz(A)k) time.
When analyzing Simultaneous Iteration  [15] uses the following randomized sketch-and-solve result
to ﬁnd a Z that gives a coarse Frobenius norm approximation to B = Aq and therefore a good
spectral norm approximation to A. The lemma is numbered for consistency with our full paper.
Lemma 4 (Frobenius Norm Low-Rank Approximation). For any B ∈ Rn×d and Π ∈ Rd×k where
the entries of Π are independent Gaussians drawn from N (0  1). If we let Z be an orthonormal
basis for span (BΠ)  then with probability at least 99/100  for some ﬁxed constant c 

(cid:107)B − ZZT B(cid:107)2

F ≤ c · dk(cid:107)B − Bk(cid:107)2
F .

For analyzing block methods  results like Lemma 4 can effectively serve as a replacement for earlier
random initialization analysis that applies to single vector power and Krylov methods [32].
σk+1(Aq) ≤ 1

poly(d) σm(Aq) for any m with σm(A) ≥ (1 + )σk+1(A). Plugging into Lemma 4:

r(cid:88)

(cid:107)Aq − ZZT Aq(cid:107)2

F ≤ cdk ·

i (Aq) ≤ cdk · d · σ2
σ2

k+1(Aq) ≤ σ2

m(Aq)/ poly(d).

i=k+1

Rearranging using Pythagorean theorem  we have (cid:107)ZZT Aq(cid:107)2
m(Aq)
poly(d) . That is  Aq’s
projection onto Z captures nearly all of its Frobenius norm. This is only possible if Z aligns very
well with the top singular vectors of Aq and hence gives a good spectral norm approximation for A.

F ≥ (cid:107)Aq(cid:107)2

F − σ2

4 Proof Sketch for Theorem 1
The intuition for beating Simultaneous Iteration with Block Krylov Iteration matches that of many
accelerated iterative methods. Simply put  there are better polynomials than Aq for denoising tail
singular values. In particular  we can use a lower degree polynomial  allowing us to compute fewer
powers of A and thus leading to an algorithm with fewer iterations. For example  an appropriately
) degree Chebyshev polynomial can push the tail of A nearly as close to
shifted q = O(log(d)/
zero as AO(log d/)  even if the long run growth of the polynomial is much lower (see Figure 2b).
Speciﬁcally  we prove the following scalar polynomial lemma in the full version of our paper [25] 
which can then be applied to effectively denoising A’s singular value tail.
Lemma 5 (Chebyshev Minimizing Polynomial). For  ∈ (0  1] and q = O(log d/
a degree q polynomial p(x) such that p((1 + )σk+1) = (1 + )σk+1 and 

)  there exists

√

√

1) p(x) ≥ x for x ≥ (1 + )σk+1

poly(d) for x ≤ σk+1.
Furthermore  we can choose the polynomial to only contain monomials with odd powers.

|p(x)| ≤ σk+1

2)

5

05101520051015Index iSingular Value σi Spectrum of ASpectrum of Aq00.20.40.60.81−5051015202530354045x xO(1/ε)TO(1/√ε)(x)Block Krylov Iteration takes advantage of such polynomials by working with the Krylov subspace 

K =(cid:2)Π AΠ A2Π A3Π . . . AqΠ(cid:3)  

from which we can construct pq(A)Π for any polynomial pq(·) of degree q.4 Since the polynomial
from Lemma 5 must be scaled and shifted based on the value of σk+1  we cannot easily compute it
directly. Instead  we argue that the very best k rank approximation to A lying in the span of K at
least matches the approximation achieved by projecting onto the span of pq(A)Π. Finding this best
approximation will therefore give a nearly optimal low-rank approximation to A.
Unfortunately  there’s a catch. Surprisingly  it is not clear how to efﬁciently compute the best spectral
norm error low-rank approximation to A lying in a given subspace (e.g. K’s span) [14  33]. This
challenge precludes an analysis of Krylov methods parallel to recent work on Simultaneous Iteration.
Nevertheless  since our analysis shows that projecting to Z captures nearly all the Frobenius norm
of pq(A)  we can show that the best Frobenius norm low-rank approximation to A in the span of K
gives good enough spectral norm approximation. By the following lemma  this optimal Frobenius
norm low-rank approximation is given by ZZT A  where Z is exactly the output of Algorithm 2.
Lemma 6 (Lemma 4.1 of [15]). Given A ∈ Rn×d and Q ∈ Rm×n with orthonormal columns 

(cid:107)A − (QQT A)k(cid:107)F = (cid:107)A − Q(cid:0)QT A(cid:1)

min

C|rank(C)=k

Q(cid:0)QT A(cid:1)
letting M = ¯U ¯Σ2 ¯UT be the SVD of M  and Z = Q ¯Uk then Q(cid:0)QT A(cid:1)

k can be obtained using an SVD of the m × m matrix M = QT (AAT )Q. Speciﬁcally 

(cid:107)A − QC(cid:107)F .

k (cid:107)F =

k = ZZT A.

4.1 Stronger Per Vector Error Guarantees
Achieving the per vector guarantee of (3) requires a more nuanced understanding of how Simultane-
ous Iteration and Block Krylov Iteration denoise the spectrum of A. The analysis for spectral norm
low-rank approximation relies on the fact that Aq (or pq(A) for Block Krylov Iteration) blows up
any singular value ≥ (1 + )σk+1 to much larger than any singular value ≤ σk+1. This ensures that
our output Z aligns very well with the singular vectors corresponding to these large singular values.
If σk ≥ (1 + )σk+1  then Z aligns well with all top k singular vectors of A and we get good
Frobenius norm error and the per vector guarantee (3). Unfortunately  when there is a small gap
between σk and σk+1  Z could miss intermediate singular vectors whose values lie between σk+1
and (1 + )σk+1. This is the case where gap dependent guarantees of classical analysis break down.
However  Aq or  for Block Krylov Iteration  some q-degree polynomial in our Krylov subspace  also
signiﬁcantly separates singular values > σk+1 from those < (1 − )σk+1. Thus  each column of Z
at least aligns with A nearly as well as uk+1. So  even if we miss singular values between σk+1 and
(1 + )σk+1  they will be replaced with approximate singular values > (1− )σk+1  enough for (3).
For Frobenius norm low-rank approximation (1)  we prove that the degree to which Z falls outside of
the span of A’s top k singular vectors depends on the number of singular values between σk+1 and
(1−)σk+1. These are the values that could be ‘swapped in’ for the true top k singular values. Since
their weight counts towards A’s tail  our total loss compared to optimal is at worst (cid:107)A − Ak(cid:107)2
F .
5
For both Algorithm 1 and 2  Π can be replaced by a random sign matrix  or any matrix achieving
the guarantee of Lemma 4. Π may also be chosen with p > k columns. In our full paper [25]  we
discuss in detail how this approach can give improved accuracy.
5.1 Simultaneous Iteration
In our implementation we set Z = Q ¯Uk  which is necessary for achieving per vector guarantees for
approximate PCA. However  for near optimal low-rank approximation  we can simply set Z = Q.
Projecting A to Q ¯Uk is equivalent to projecting to Q as these matrices have the same column spans.
Since powering A spreads its singular values  K = (AAT )qAΠ could be poorly conditioned. To
improve stability we orthonormalize K after every iteration (or every few iterations). This does not
change K’s column span  so it gives an equivalent algorithm in exact arithmetic.

Implementation and Runtimes

4Algorithm 2 in fact only constructs odd powered terms in K  which is sufﬁcient for our choice of pq(x).

6

Theorem 7 (Simultaneous Iteration Runtime). Algorithm 1 runs in time

O(cid:0)nnz(A)k log(d)/ + nk2 log(d)/(cid:1) .

(cid:0)AAT(cid:1)i

AΠ given (cid:0)AAT(cid:1)i−1

Proof. Computing K requires ﬁrst multiplying A by Π  which takes O(nnz(A)k) time. Computing
AΠ then takes O(nnz(A)k) time to ﬁrst multiply our (n × k)
matrix by AT and then by A. Reorthogonalizing after each iteration takes O(nk2) time via Gram-
Schmidt. This gives a total runtime of O(nnz(A)kq + nk2q) for computing K. Finding Q takes
O(nk2) time. Computing M by multiplying from left to right requires O(nnz(A)k + nk2) time.
M’s SVD then requires O(k3) time using classical techniques. Finally  multiplying ¯Uk by Q takes
time O(nk2). Setting q = Θ(log d/) gives the claimed runtime.

5.2 Block Krylov Iteration
In the traditional Block Lanczos algorithm  one starts by computing an orthonormal basis for AΠ 
the ﬁrst block in K. Bases for subsequent blocks are computed from previous blocks using a three
term recurrence that ensures QT AAT Q is block tridiagonal  with k × k sized blocks [18]. This
technique can be useful if qk is large  since it is faster to compute the top singular vectors of a block
tridiagonal matrix. However  computing Q using a recurrence can introduce a number of stability
issues  and additional steps may be required to ensure that the matrix remains orthogonal [28].
An alternative  uesd in [1]  [19]  and our Algorithm 2  is to compute K explicitly and then ﬁnd Q
using a QR decomposition. This method does not guarantee that QT AAT Q is block tridiagonal 
but avoids stability issues. Furthermore  if qk is small  taking the SVD of QT AAT Q will still be
fast and typically dominated by the cost of computing K.
As with Simultaneous Iteration  we orthonormalize each block of K after it is computed  avoiding
poorly conditioned blocks and giving an equivalent algorithm in exact arithmetic.
Theorem 8 (Block Krylov Iteration Runtime). Algorithm 2 runs in time

 + nk2 log2(d)/ + k3 log3(d)/3/2(cid:17)

.

√

(cid:16)

O

nnz(A)k log(d)/

Proof. Computing K  including reorthogonalization  requires O(nnz(A)kq + nk2q) time. The re-
maining steps are analogous to those in Simultaneous Iteration except somewhat more costly as we
work with a k · q rather than k dimensional subspace. Finding Q takes O(n(kq)2) time. Computing
M take O(nnz(A)(kq) + n(kq)2) time and its SVD then requires O((kq)3) time. Finally  multi-
plying ¯Uk by Q takes time O(nk(kq)). Setting q = Θ(log d/

) gives the claimed runtime.

√

6 Experiments
We close with several experimental results. A variety of empirical papers  not to mention widespread
adoption  already justify the use of randomized SVD algorithms. Prior work focuses in particular on
benchmarking Simultaneous Iteration [19  11] and  due to its improved accuracy over sketch-and-
solve approaches  this algorithm is popular in practice [10  16]. As such  we focus on demonstrating
that for many data problems Block Krylov Iteration can offer signiﬁcantly better convergence.
We implement both algorithms in MATLAB using Gaussian random starting matrices with exactly
k columns. We explicitly compute K for both algorithms  as described in Section 5  and use re-
orthonormalization at each iteration to improve stability [34]. We test the algorithms with varying
iteration count q on three common datasets  SNAP/AMAZON0302 [22  23]  SNAP/EMAIL-ENRON
[22  35]  and 20 NEWSGROUPS [36]  computing column principal components in all cases. We plot
error vs. iteration count for metrics (1)  (2)  and (3) in Figure 3. For per vector error (3)  we plot the
maximum deviation amongst all top k approximate principal components (relative to σk+1).
Unsurprisingly  both algorithms obtain very accurate Frobenius norm error  (cid:107)A − ZZT A(cid:107)F /(cid:107)A −
Ak(cid:107)F   with very few iterations. This is our intuitively weakest guarantee and  in the presence of a
heavy singular value tail  both iterative algorithms will outperform the worst case analysis.
On the other hand  for spectral norm low-rank approximation and per vector error  we conﬁrm that
Block Krylov Iteration converges much more rapidly than Simultaneous Iteration  as predicted by

7

(a) SNAP/AMAZON0302  k = 30

(b) SNAP/EMAIL-ENRON  k = 10

(c) 20 NEWSGROUPS  k = 20

(d) 20 NEWSGROUPS  k = 20  runtime cost

Figure 3: Low-rank approximation and per vector error convergence rates for Algorithms 1 and 2.

our theoretical analysis. It it often possible to achieve nearly optimal error with < 8 iterations where
as getting to within say 1% error with Simultaneous Iteration can take much longer.
The ﬁnal plot in Figure 3 shows error verses runtime for the 11269× 15088 dimensional 20 NEWS-
GROUPS dataset. We averaged over 7 trials and ran the experiments on a commodity laptop with
16GB of memory. As predicted  because its additional memory overhead and post-processing costs
are small compared to the cost of the large matrix multiplication required for each iteration  Block
Krylov Iteration outperforms Simultaneous Iteration for small .
More generally  these results justify the importance of convergence bounds that are independent of
singular value gaps. Our analysis in Section 6 of the full paper predicts that  once  is small in
− 1  we should see much more rapid convergence since q will depend
comparison to the gap σk
σk+1
on log(1/) instead of 1/. However  for Simultaneous Iteration  we do not see this behavior with
SNAP/AMAZON0302 and it only just begins to emerge for 20 NEWSGROUPS.
While all three datasets have rapid singular value decay  a careful look conﬁrms that their singular
value gaps are actually quite small! For example  σk/σk+1 − 1 is .004 for SNAP/AMAZON0302
and .011 for 20 NEWSGROUPS  in comparison to .042 for SNAP/EMAIL-ENRON. Accordingly  the
frequent claim that singular value gaps can be taken as constant is insufﬁcient  even for small .
References
[1] Vladimir Rokhlin  Arthur Szlam  and Mark Tygert. A randomized algorithm for principal component

analysis. SIAM Journal on Matrix Analysis and Applications  31(3):1100–1124  2009.

[2] Nathan Halko  Per-Gunnar Martinsson  and Joel Tropp. Finding structure with randomness: Probabilistic

algorithms for constructing approximate matrix decompositions. SIAM Review  53(2):217–288  2011.

[3] Tam´as Sarl´os. Improved approximation algorithms for large matrices via random projections. In Pro-

ceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS)  2006.

[4] Per-Gunnar Martinsson  Vladimir Rokhlin  and Mark Tygert. A randomized algorithm for the approxi-

mation of matrices. Technical Report 1361  Yale University  2006.

[5] Kenneth Clarkson and David Woodruff. Low rank approximation and regression in input sparsity time. In
Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC)  pages 81–90  2013.

[6] Antoine Liutkus. Randomized SVD  2014. MATLAB Central File Exchange.
[7] Daisuke Okanohara. redsvd: RandomizED SVD. https://code.google.com/p/redsvd/  2010.

8

51015202500.050.10.150.20.250.3Iterations qError ε Block Krylov − Frobenius ErrorBlock Krylov − Spectral ErrorBlock Krylov − Per Vector ErrorSimult. Iter. − Frobenius ErrorSimult. Iter. − Spectral ErrorSimult. Iter. − Per Vector Error51015202500.050.10.150.20.250.30.350.4Iterations qError ε Block Krylov − Frobenius ErrorBlock Krylov − Spectral ErrorBlock Krylov − Per Vector ErrorSimult. Iter. − Frobenius ErrorSimult. Iter. − Spectral ErrorSimult. Iter. − Per Vector Error51015202500.050.10.150.20.250.30.35Error εIterations q Block Krylov − Frobenius ErrorBlock Krylov − Spectral ErrorBlock Krlyov − Per Vector ErrorSimult. Iter. − Frobenius ErrorSimult. Iter. − Spectral ErrorSimult. Iter. − Per Vector Error0123456700.050.10.150.20.250.30.35Runtime (seconds)Error ε Block Krylov − Frobenius ErrorBlock Krylov − Spectral ErrorBlock Krylov − Per Vector ErrorSimult. Iter. − Frobenius ErrorSimult. Iter. − Spectral ErrorSimult. Iter. − Per Vector Error[8] David Hall et al. ScalaNLP: Breeze. http://www.scalanlp.org/  2009.
[9] IBM Reseach Division  Skylark Team. libskylark: Sketching-based Distributed Matrix Computations for

Machine Learning. IBM Corporation  Armonk  NY  2014.

[10] F. Pedregosa et al. Scikit-learn: Machine learning in Python. JMLR  12:2825–2830  2011.
[11] Arthur Szlam  Yuval Kluger  and Mark Tygert. An implementation of a randomized algorithm for princi-

pal component analysis. arXiv:1412.3510  2014.

[12] Zohar Karnin and Edo Liberty. Online PCA with spectral bounds. In Proceedings of the 28th Annual

Conference on Computational Learning Theory (COLT)  pages 505–509  2015.

[13] Raﬁ Witten and Emmanuel J. Cand`es. Randomized algorithms for low-rank matrix factorizations: Sharp

performance bounds. Algorithmica  31(3):1–18  2014.

[14] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Near-optimal column-based matrix recon-

struction. SIAM Journal on Computing  43(2):687–717  2014.

[15] David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends in Theoretical

Computer Science  10(1-2):1–157  2014.

[16] Andrew Tulloch. Fast randomized singular value decomposition. http://research.facebook.

com/blog/294071574113354/fast-randomized-svd/  2014.

[17] Jane Cullum and W.E. Donath. A block Lanczos algorithm for computing the q algebraically largest
eigenvalues and a corresponding eigenspace of large  sparse  real symmetric matrices. In IEEE Conference
on Decision and Control including the 13th Symposium on Adaptive Processes  pages 505–509  1974.

[18] Gene Golub and Richard Underwood. The block Lanczos method for computing eigenvalues. Mathemat-

ical Software  (3):361–377  1977.

[19] Nathan Halko  Per-Gunnar Martinsson  Yoel Shkolnisky  and Mark Tygert. An algorithm for the principal

component analysis of large data sets. SIAM Journal on Scientiﬁc Computing  33(5):2580–2594  2011.

[20] Nathan Halko. Randomized methods for computing low-rank approximations of matrices. PhD thesis  U.

of Colorado  2012.

[21] Ming Gu. Subspace iteration randomization and singular value problems. arXiv:1408.2208  2014.
[22] Timothy A. Davis and Yifan Hu. The university of ﬂorida sparse matrix collection. ACM Transactions on

Mathematical Software  38(1):1:1–1:25  December 2011.

[23] Jure Leskovec  Lada A. Adamic  and Bernardo A. Huberman. The dynamics of viral marketing. ACM

Transactions on the Web  1(1)  May 2007.

[24] Y. Saad. On the rates of convergence of the Lanczos and the Block-Lanczos methods. SIAM Journal on

Numerical Analysis  17(5):687–706  1980.

[25] Cameron Musco and Christopher Musco. Randomized block Krylov methods for stronger and faster

approximate singular value decomposition. arXiv:1504.05477  2015.

[26] Yousef Saad. Numerical Methods for Large Eigenvalue Problems: Revised Edition  volume 66. 2011.
[27] Gene Golub  Franklin Luk  and Michael Overton. A block Lanczos method for computing the singular

values and corresponding singular vectors of a matrix. ACM Trans. Math. Softw.  7(2):149–169  1981.

[28] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press  3rd edition  1996.
[29] Ren-Cang Li and Lei-Hong Zhang. Convergence of the block Lanczos method for eigenvalue clusters.

Numerische Mathematik  131(1):83–113  2015.

[30] Michael B. Cohen  Sam Elder  Cameron Musco  Christopher Musco  and Madalina Persu. Dimensionality
reduction for k-means clustering and low rank approximation. In Proceedings of the 47th Annual ACM
Symposium on Theory of Computing (STOC)  2015.

[31] Friedrich L. Bauer. Das verfahren der treppeniteration und verwandte verfahren zur l¨osung algebraischer

eigenwertprobleme. Zeitschrift f¨ur angewandte Mathematik und Physik ZAMP  8(3):214–235  1957.

[32] J. Kuczy´nski and H. Wo´zniakowski. Estimating the largest eigenvalue by the power and Lanczos algo-
rithms with a random start. SIAM Journal on Matrix Analysis and Applications  13(4):1094–1122  1992.
[33] Kin Cheong Sou and Anders Rantzer. On the minimum rank of a generalized matrix approximation
problem in the maximum singular value norm. In Proceedings of the 19th International Symposium on
Mathematical Theory of Networks and Systems (MTNS)  2010.

[34] Per-Gunnar Martinsson  Arthur Szlam  and Mark Tygert. Normalized power iterations for the computation

of SVD  2010. NIPS Workshop on Low-rank Methods for Large-scale Machine Learning.

[35] Jure Leskovec  Jon Kleinberg  and Christos Faloutsos. Graphs over time: Densiﬁcation laws  shrinking
diameters and possible explanations. In Proceedings of the 11th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD)  pages 177–187  2005.

[36] Jason Rennie. 20 newsgroups. http://qwone.com/˜jason/20Newsgroups/  May 2015.

9

,Ofer Dekel
Elad Hazan
Tomer Koren
Cameron Musco
Christopher Musco
Naganand Yadati
Madhav Nimishakavi
Prateek Yadav
Vikram Nitin
Anand Louis
Partha Talukdar