2009,Heavy-Tailed Symmetric Stochastic Neighbor Embedding,Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently  the most popular implementation  t-SNE  is restricted to a particular Student t-distribution as its embedding distribution. Moreover  it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size  momentum  etc.  in finding its optimum. In this paper  we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method  which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization  we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding  we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies  one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.,Heavy-Tailed Symmetric Stochastic Neighbor

Embedding

Zhirong Yang

The Chinese University of Hong Kong

Helsinki University of Technology

zhirong.yang@tkk.fi

Zenglin Xu

The Chinese University of Hong Kong

Saarland University & MPI for Informatics

Irwin King

The Chinese University of Hong Kong

king@cse.cuhk.edu.hk

Erkki Oja

Helsinki University of Technology

erkki.oja@tkk.fi

zlxu@cse.cuhk.edu.hk

Abstract

Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data
visualization. Currently  the most popular implementation  t-SNE  is restricted to
a particular Student t-distribution as its embedding distribution. Moreover  it uses
a gradient descent algorithm that may require users to tune parameters such as
the learning step size  momentum  etc.  in ﬁnding its optimum. In this paper  we
propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE)
method  which is a generalization of the t-SNE to accommodate various heavy-
tailed embedding similarity functions. With this generalization  we are presented
with two difﬁculties. The ﬁrst is how to select the best embedding similarity
among all heavy-tailed functions and the second is how to optimize the objective
function once the heavy-tailed function has been selected. Our contributions then
are: (1) we point out that various heavy-tailed embedding similarities can be char-
acterized by their negative score functions. Based on this ﬁnding  we present a pa-
rameterized subset of similarity functions for choosing the best tail-heaviness for
HSSNE; (2) we present a ﬁxed-point optimization algorithm that can be applied to
all heavy-tailed functions and does not require the user to set any parameters; and
(3) we present two empirical studies  one for unsupervised visualization showing
that our optimization algorithm runs as fast and as good as the best known t-SNE
implementation and the other for semi-supervised visualization showing quanti-
tative superiority using the homogeneity measure as well as qualitative advantage
in cluster separation over t-SNE.

1 Introduction

Visualization as an important tool for exploratory data analysis has attracted much research effort
in recent years. A multitude of visualization approaches  especially the nonlinear dimensionality
reduction techniques such as Isomap [9]  Laplacian Eigenmaps [1]  Stochastic Neighbor Embedding
(SNE) [6]  manifold sculpting [5]  and kernel maps with a reference point [8]  have been proposed.
Although they are reported with good performance on tasks such as unfolding an artiﬁcial manifold 
they are often not successful at visualizing real-world data with high dimensionalities.
A common problem of the above methods is that most mapped data points are crowded together in
the center without distinguished gaps that isolate data clusters. It was recently pointed out by van
der Maaten and Hinton [10] that the “crowding problem” can be alleviated by using a heavy-tailed

distribution in the low-dimensional space. Their method  called t-Distributed Stochastic Neighbor
Embedding (t-SNE)  is adapted from SNE with two major changes: (1) it uses a symmetrized cost
function; and (2) it employs a Student t-distribution with a single degree of freedom (T1). In this
way  t-SNE can achieve remarkable superiority in the discovery of clustering structure in high-
dimensional data.
The t-SNE development procedure in [10] is restricted to the T1 distribution as its embedding sim-
ilarity. However  different data sets or other purposes of dimensionality reduction may require gen-
eralizing t-SNE to other heavy-tailed functions. The original t-SNE derivation provides little infor-
mation for users on how to select the best embedding similarity among all heavy-tailed functions.
Furthermore  the original t-SNE optimization algorithm is not convenient when the symmetric SNE
is generalized to use various heavy-tailed embedding similarity functions since it builds on the gra-
dient descent approach with momenta. As a result  several optimization parameters need to be
manually speciﬁed. The performance of the t-SNE algorithm depends on laborious selection of the
optimization parameters. For instance  a large learning step size might cause the algorithm to di-
verge  while a conservative one might lead to slow convergence or poor annealed results. Although
comprehensive strategies have been used to improve the optimization performance  they might be
still problematic when extended to other applications or embedding similarity functions.
In this paper we generalize t-SNE to accommodate various heavy-tailed functions with two major
contributions: (1) we propose to characterize heavy-tailed embedding similarities in symmetric SNE
by their negative score functions. This further leads to a parameterized subset facilitating the choice
of the best tail-heaviness; and (2) we present a general algorithm for optimizing the symmetric SNE
objective with any heavy-tailed embedding similarities.
The paper is organized as follows. First we brieﬂy review the related work of SSNE and t-SNE in
Section 2. In Section 3  we present the generalization of t-SNE to our Heavy-tailed Symmetric SNE
(HSSNE) method. Next  a ﬁxed-point optimization algorithm for HSSNE is provided and its con-
vergence is discussed in Section 4. In Section 5  we relate the EM-like behavior of the ﬁxed-point
algorithm to a pairwise local mixture model for an in-depth analysis of HSSNE. Section 6 presents
two sets of experiments  one for unsupervised and the other for semi-supervised visualization. Fi-
nally  conclusions are drawn in Section 7.

2 Symmetric Stochastic Neighbor Embedding
Suppose the pairwise similarities of a set of m-dimensional data points X = {xi}n
i=1 are encoded
in a symmetric matrix P ∈ Rn×n
ij Pij = 1. Symmetric Stochastic Neighbor
Embedding (SSNE) [4  10] seeks r-dimensional (r (cid:191) m) representations of X   denoted by Y =
{yi}n

+   where Pii = 0 and

i=1  such that

J (Y ) = DKL(P||Q) =

Pij log Pij
Qij

is minimized  where Qij = qij/
bedding and

a(cid:54)=b qab are the normalized similarities in low-dimensional em-

i(cid:54)=j

(cid:80)
(cid:88)
(cid:161)−(cid:107)yi − yj(cid:107)2(cid:162)
(cid:88)
(cid:175)(cid:175)(cid:175)

j

∂J
∂Y

(cid:80)

∂J
∂yi

(1)

(2)

(3)

qij = exp

  qii = 0.

The optimization of SSNE uses the gradient descent method with
(Pij − Qij)(yi − yj).

= 4

(cid:179)

(cid:180)

A momentum term is added to the gradient in order to speed up the optimization:

+ β(t)

1 . . . y(t)

Y (t+1) = Y (t) + η

(4)
n ] ∈ Rr×n is the solution in matrix form at iteration t; η is the learning
where Y (t) = [y(t)
rate; and β(t) is the momentum amount at iteration t. Compared with an earlier method Stochastic
Neighbor Embedding (SNE) [6]  SSNE uses a symmetrized cost function with simpler gradients.
Most mapped points in the SSNE visualizations are often compressed near the center of the visualiz-
ing map without clear gaps that separate clusters of the data. The t-Distributed Stochastic Neighbor

Y =Y (t)

 

Y (t) − Y (t−1)

Embedding (t-SNE) [10] addresses this crowding problem by using the Student t-distribution with a
single degree of freedom

(5)
as the embedding similarity distribution  which has a heavier tail than the Gaussian used in SNE and
SSNE. For brevity we denote such distribution by T1. Using this distribution yields the gradient of
t-SNE:

qij = (1 + (cid:107)yi − yj(cid:107)2)−1  qii = 0 

= 4

(Pij − Qij)(yi − yj)(1 + (cid:107)yi − yj(cid:107)2)−1.

(6)

(cid:88)

j

∂J
∂yi

In addition  t-SNE employs a number of strategies to overcome the difﬁculties in the optimization
based on gradient descent.

3 Heavy-tailed SNE characterized by negative score functions

As the gradient derivation in [10] is restricted to the T1 distribution  we derive the gradient with a
general function that converts squared distances to similarities  with T1 as a special case. In addition 
the direct chain rule used in [10] may cause notational clutter and conceal the working components in
the gradients. We instead employ the Lagrangian technique to simplify the derivation. Our approach
can provide more insights of the working factor brought by the heavy-tailed functions.
Minimizing J (Y ) in Equation (1) with respect to Y is equivalent to the optimization problem:

(cid:88)

qij(cid:80)

a(cid:54)=b qab

L(q  Y ) =

q Y

maximize

Pij log
subject to qij = H((cid:107)yi − yj(cid:107)2) 

ij

(7)

(cid:195)

(cid:88)
(cid:88)

j

1(cid:80)

(8)
where the embedding similarity function H(τ) ≥ 0 can be any function that is monotonically de-
creasing with respect to τ for τ > 0. Note that H is not required to be deﬁned as a probability
function because the symmetric SNE objective already involves normalization over all data pairs.
The extended objective using the Lagrangian technique is given by

(cid:88)

(cid:163)

qij(cid:80)
(cid:80)
a(cid:54)=b qab − Pij/qij. Inserting these Lagrangian multi-

qij − H((cid:107)yi − yj(cid:107)2)

a(cid:54)=b qab

(9)

λij

+

ij

.

˜L(q  Y ) =

Pij log

Setting ∂ ˜L(q  Y )/∂qij = 0 yields λij = 1/
pliers to the gradient with respect to yi  we have

(cid:88)

ij

(cid:33)

(cid:181)

(cid:164)

(cid:182)

∂J (Y )
∂yi

= − ∂ ˜L(q  Y )

∂yi

= 4

= 4

− Pij
qij

· qij ·

a(cid:54)=b qab

− h((cid:107)yi − yj(cid:107)2)

qij

(Pij − Qij)S((cid:107)yi − yj(cid:107)2)(yi − yj) 

(yi − yj)

(10)

(11)

where h(τ) = dH(τ)/dτ and

j

S(τ) = − d log H(τ)

dτ

(12)
is the negative score function of H. For notational simplicity  we also write Sij = S((cid:107)yi − yj(cid:107)2).
We propose to characterize the tail heaviness of the similarity function H  relative to the one that
leads to the Gaussian  by its negative score function S  also called tail-heaviness function in this
paper. In this characterization  there is a functional operator S that maps every similarity function to
a tail-heaviness function. For the baseline Gaussian similarity  H(τ) = exp(−τ)  we have S(H) =
1  i.e. S(H)(τ) = 1 for all τ. As for the Student t-distribution of a single degree of freedom 
H(τ) = (1 + τ)−1 and thus S(H) = H.
The above observation inspires us to further parameterize a family of tail-heaviness functions by
the power of H: S(H  α) = H α for α ≥ 0  where a larger α value corresponds to a heavier-tailed
embedding similarity function. Such a function H can be determined by solving the ﬁrst-order
differential equation −d log H(τ)/dτ = [H(τ)]α  which gives
H(τ) = (ατ + c)−1/α

(13)

Figure 1: Several functions in the power family.

with c a constant. Here we set c = 1 for a consistent generalization of SNE and t-SNE. Thus the
Gaussian embedding similarity function  i.e. H(τ) = exp(−τ)  is achieved when α → 0. Figure 1
shows a number of functions in the power family.

4 A ﬁxed-Point optimization algorithm

(cid:80)

Unlike many other dimensionality reduction approaches that can be solved by eigendecomposition
in a single step  SNE and its variants require iterative optimization methods. Substantial efforts have
been devoted to improve the efﬁciency and robustness of t-SNE optimization. However it remains
unknown whether such a comprehensive implementation also works for other types of embedding
similarity functions. Manually adjusting the involved parameters such as the learning rate and the
momentum for every function is rather time-consuming and infeasible in practice.
(cid:80)
Here we propose to optimize symmetric SNE by a ﬁxed-point algorithm. After rearranging the terms
in ∂J /∂yi = 0 (see Equation (11))  we obtain the following update rule:
(cid:80)
j (Aij − Bij) Y (t)
j Aij
j (cid:107)2) and Bij = QijS((cid:107)y(t)

where Aij = PijS((cid:107)y(t)
j (cid:107)2). Our optimization algorithm
for HSSNE simply involves the iterative application of Equation (14). Compared with the original
t-SNE optimization algorithm  our method requires no user-provided parameters such as the learn-
ing step size and momentum  which is more convenient for applications. The ﬁxed-point algorithm
usually converges  with the result satisfying the stationary condition ∂J /∂Y = 0. However  it is
known that the update rule (14) can diverge in some cases  for example  when Yki are large. There-
fore  a proof without extra conditions cannot be constructed. Here we provide two approximative
theoretical justiﬁcations for the algorithm.
Denote ∆ = Y − Y (t) and ∇ the gradient of J with respect to Y . Let us ﬁrst approximate the
HSSNE objective by the ﬁrst-order Taylor expansion at the current estimate Y (t):

i − y(t)

i − y(t)

Y (t+1)
ki

=

Y (t)
ki

j Bij +

kj

 

(14)

(cid:88)

J (Y ) ≈ Jlin(Y ) = J (Y (t)) +

∆ki∇(t)
ki .

Then we can construct an upper bound of Jlin(Y ):
G(Y  Y (t)) = Jlin(Y ) +

(cid:88)

ki

∆2
ki

(cid:88)

ki

a

1
2

Aia

(15)

(16)

as Pia and Sia are all nonnegative. The bound is tight at Y = Y (t)  i.e. G(Y (t)  Y (t)) = Jlin(Y (t)).
Equating ∂G(Y  Y (t))/∂Y = 0 implements minimization of G(Y  Y (t)) and yields the update rule
(14). Iteratively applying the update rule (14) thus results in a monotonically decreasing sequence
of the linear approximation of HSSNE objective: Jlin(Y (t)) ≥ G(Y (t+1)  Y (t)) ≥ Jlin(Y (t+1)).

01234500.10.20.30.40.50.60.70.80.91τH(τ)=(1+ατ)−1/α  α→ 0 (Gaussian)α=0.5α=1 (T1)α=1.5α=2(cid:80)

Even if the second-order terms in the Taylor expansion of J (Y ) are also considered  the update rule
(14) is still justiﬁed if Yki or Y (t+1)
ki are small. Let DA and DB be diagonal matrices with
DA

(cid:80)
j Bij. We can write J (Y ) = Jquad(Y ) + O(∆3)  where
Jquad(Y ) = Jlin(Y ) +

j Aij and DB

∆ki∆ljHijkl.

(cid:88)

− Y (t)

ii =

ii =

(17)

ki

1
2

ijkl

With the approximated Hessian Hijkl = δkl
ij  the updating term Uki in
Newton’s method Y (t)
lj . Solving this
equation by directly inverting the huge tensor H is however infeasible in practice and thus usually
implemented by iterative methods such as

− Uki can be determined by

ki = Y (t−1)

ki

(DA − A) − (DB − B)

(cid:164)
(cid:80)
ki HijklUki = ∇(t)
(cid:162)

(cid:161)

U (v+1)

ki

=

(A + DB − B)U (v) + ∇(t)

DA
ii

ki

.

(18)

Such iterations albeit still form a costly inner loop over v. To overcome this  we initialize U (0) = 0
and only employ the ﬁrst iteration of each inner loop. Then one can ﬁnd that such an approximated
Newton’s update rule Y (t+1)
is identical to Equation (14). Such a ﬁrst-step approx-
imation technique has also been used in the Mean Shift algorithm as a generalized Expectation-
Maximization solution [2].

ki − ∇(t)

= Y (t)

ki
DA
ii

ki

5 A local mixture interpretation

(cid:163)

(cid:104)

Further rearranging the update rule can give us more insights of the properties of SSNE solutions:

(cid:80)

j Aij

Y (t+1)
ki

=

(cid:105)

(cid:80)

Y (t)
kj + Qij
Pij
j Aij

ki − Y (t)
(Y (t)
kj )

.

(19)

One can see that the above update rule mimics the maximization step in the EM-algorithm for
classical Gaussian mixture model (e.g. [7])  or more particularly  the Mean Shift method [3  2].
This resemblance inspires us to ﬁnd an alternative interpretation of the SNE behavior in terms of a
particular mixture model.
Given the current estimate Y (t)  the ﬁxed-point update rule actually performs minimization of

(cid:179)

where µ(t)
bound of

ij = y(t)

j + Qij
Pij

PijSij(cid:107)yi − µ(t)

ij (cid:107)2 

(20)

. This problem is equivalent to maximizing the Jensen lower

log

PijSij exp

−(cid:107)yi − µ(t)
ij (cid:107)2

.

(21)

(cid:179)

(cid:180)

(cid:88)
(cid:180)
(cid:88)
i − y(t)
y(t)

ij

j

ij

In this form  µ(t)
ij can be regarded as the mean of the j-th mixture component for the i-th embedded
data point  while the product PijSij can be thought as the mixing coefﬁcients1. Note that each data
sample has its own mixing coefﬁcients because of locality sensitivity.
For the converged estimate  i.e.  Y (t+1) = Y (t) = Y ∗  we can rewrite the mixture without the
logarithm as

(cid:41)

(cid:40)

(cid:181)

PijSij exp

−

1 − Qij
Pij

i − y∗
j(cid:107)2

.

(22)

(cid:182)2 (cid:107)y∗

(cid:88)

ij

Maximizing this quantity clearly explains the ingredients of symmetric SNE: (1) Pij reﬂects that
symmetric SNE favors close pairs in the input space  which is also adopted by most other locality

1The data samples in such a symmetric mixture model do not follow the independent and identically dis-
tributed (i.i.d.) assumption because the mixing coefﬁcient rows are not summed to the same number. Never-
theless  this does not affect our subsequent pairwise analysis.

preserving methods. (2) As discussed in Section 3  Sij characterizes the tail heaviness of the em-
bedding similarity function. For the baseline Gaussian similarity  this reduces to one and thus has no
effect. For heavy-tailed similarities  Sij can compensate for mismatched dimensionalities between
the input space and its embedding. (3) The ﬁrst factor in the exponential emphasizes the distance
graph matching  which underlies the success of SNE and its variants for capturing the global data
structure compared with many other approaches that rely on only variance constraints [10]. A pair
of Qij that approximates Pij well can increase the exponential  while a pair with a poor mismatch
yields little contribution to the mixture. (4) Finally  as credited in many other continuity preserv-
ing methods  the second factor in the exponential forces that close pairs in the input space are also
situated nearby in the embedding space.

6 Experiments

6.1

t-SNE for unsupervised visualization

In this section we present experiments of unsupervised visualization with T1 distribution  where our
Fixed-Point t-SNE is compared with the original Gradient t-SNE optimization method as well as
another dimensionality reduction approach  Laplacian Eigenmap [1]. Due to space limitation  we
only focus on three data sets  iris  wine  and segmentation (training subset) from the UCI repository2.
We followed the instructions in [10] for calculating Pij and choosing the learning rate η and momen-
tum amount β(t) for Gradient t-SNE. Alternatively  we excluded two tricks  “early compression”
and “early exaggeration”  that are described in [10] from the comparison of long-run optimization
because they apparently belong to the initialization stage. Here both Fixed-Point and Gradient t-
SNEs execute with the same initialization which uses the “early compression” trick and pre-runs the
Gradient t-SNE for 50 iterations as suggested in [10].
The visualization quality can be quantiﬁed using the ground truth class information. We adopt the
measurement of the homogeneity of nearest neighbors:

homogeneity = γ/n 

(23)

where γ is the number of mapped points belonging to the same class with their nearest neighbor and
n again is the total number of points. A larger homogeneity generally indicates better separability
of the classes.
The experimental results are shown in Figure 2. Even though having a globally optimal solution  the
Laplacian Eigenmap yields poor visualizations  since none of the classes can be isolated. By con-
trast  both t-SNE methods achieve much higher homogeneities and most clusters are well separated
in the visualization plots. Comparing the two t-SNE implementations  one can see that our sim-
ple ﬁxed-point algorithm converges even slightly faster than the comprehensive and carefully tuned
Gradient t-SNE. Besides efﬁciency  our approach performs as good as Gradient t-SNE in terms of
both t-SNE objectives and homogeneities of nearest neighbors for these data sets.

6.2 Semi-supervised visualization

Unsupervised symmetric SNE or t-SNE may perform poorly for some data sets in terms of iden-
tifying classes. In such cases it is better to include some supervised information and apply semi-
supervised learning to enhance the visualization.
Let us consider another data set vehicle from the LIBSVM repository3. The top-left plot in Figure 3
demonstrates a poor visualization using unsupervised Gradient t-SNE. Next  suppose 10% of the
intra-class relationships are known. We can construct a supervised matrix u where uij = 1 if xi and
xj are known to belong to the same class and 0 otherwise. After normalizing Uij = uij/
a(cid:54)=b uab 
we calculate the semi-supervised similarity matrix ˜P = (1−ρ)P +ρU  where the trade-off parameter
ρ is set to 0.5 in our experiments. All SNE learning algorithms remain unchanged except that P is
replaced with ˜P .

(cid:80)

2http://archive.ics.uci.edu/ml/
3http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

Figure 2: Unsupervised visualization on three data sets. Column 1 to 3 are results of iris  wine and
segmentation  respectively. The ﬁrst row comprises the learning times of Gradient and Fixed-Point
t-SNEs. The second to fourth rows are visualizations using Laplacian Eigenmap  Gradient t-SNE 
and Fixed-Point t-SNE  respectively.

00.20.40.60.811.200.20.40.60.811.21.41.6learning time (seconds)t−SNE costiris  Gradient t−SNEFixed−Point t−SNE00.20.40.60.811.21.41.61.80.20.40.60.811.21.41.61.82learning time (seconds)t−SNE costwine  Gradient t−SNEFixed−Point t−SNE00.511.522.50.20.40.60.811.21.41.6learning time (seconds)t−SNE costsegmentation  Gradient t−SNEFixed−Point t−SNE−0.05−0.04−0.03−0.02−0.0100.010.020.030.040.05−0.05−0.04−0.03−0.02−0.0100.010.020.030.040.05Laplacian Eigenmap  homogeneity=0.47  DKL=1.52  123−0.04−0.03−0.02−0.0100.010.020.03−0.03−0.02−0.0100.010.020.03Laplacian Eigenmap  homogeneity=0.38  DKL=1.67  123−0.03−0.02−0.0100.010.020.030.04−0.03−0.02−0.0100.010.020.03Laplacian Eigenmap  homogeneity=0.42  DKL=1.86  1234567−20−15−10−5051015−20−15−10−5051015Gradient t−SNE  homogeneity=0.96  DKL=0.15  123−10−50510−15−10−50510Gradient t−SNE  homogeneity=0.96  DKL=0.36  123−20−15−10−50510152025−20−15−10−505101520Gradient t−SNE  homogeneity=0.86  DKL=0.24  1234567−10−8−6−4−20246810−12−10−8−6−4−202468Fixed−Point t−SNE  homogeneity=0.95  DKL=0.16  123−10−50510−10−50510Fixed−Point t−SNE  homogeneity=0.97  DKL=0.37  123−15−10−5051015−10−5051015Fixed−Point t−SNE  homogeneity=0.83  DKL=0.24  1234567Figure 3: Semi-supervised visualization for the vehicle data set. The plots titled with α values are
produced using the ﬁxed-point algorithm of the power family of HSSNE.

The top-middle plot in Figure 3 shows that inclusion of some supervised information improves the
homogeneity (0.92) and visualization  where Class 3 and 4 are identiﬁable  but the classes are still
very close to each other  especially Class 1 and 2 heavily mixed. We then tried the power family
of HSSNE with α ranging from 0 to 1.5  using our ﬁxed-point algorithm. It can be seen that with
α increased  the cyan and magenta clusters become more separate and Class 1 and 2 can also be
identiﬁed. With α = 1 and α = 2  the HSSNEs implemented by our ﬁxed-point algorithm achieve
even higher homogeneities (0.94 and 0.96  respectively) than the Gradient t-SNE. On the other hand 
too large α may increase the number of outliers and the Kullback-Leibler divergence.

7 Conclusions

The working mechanism of Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) has
been investigated rigorously. The several ﬁndings are: (1) we propose to use a negative score func-
tion to characterize and parameterize the heavy-tailed embedding similarity functions; (2) this ﬁnd-
ing has provided us with a power family of functions that convert distances to embedding similari-
ties; and (3) we have developed a ﬁxed-point algorithm for optimizing SSNE  which greatly saves the
effort in tuning program parameters and facilitates the extensions and applications of heavy-tailed
SSNE. We have compared HSSNE against t-SNE and Laplacian Eigenmap using UCI and LIBSVM
repositories. Two sets of experimental results from unsupervised and semi-supervised visualization
indicate that our method is efﬁcient  accurate  and versatile over the other two approaches.
Our future work might include further empirical studies on the learning speed and robustness of
HSSNE by using more extensive  especially large-scale  experiments. It also remains important to
investigate acceleration techniques in both initialization and long-run stages of the learning.

8 Acknowledgement

The authors appreciate the reviewers for their extensive and informative comments for the improve-
ment of this paper. This work is supported by a grant from the Research Grants Council of the Hong
Kong Special Administrative Region  China (Project No. CUHK 4128/08E).

−40−2002040−50−40−30−20−1001020304050unsupervised Gradient t−SNE  homogeneity=0.69  DKL=3.24  1234−30−20−10010203040−40−30−20−10010203040semi−supervised Gradient t−SNE  homogeneity=0.92  DKL=2.58  1234−3−2−1012−3−2−10123α=0  homogeneity=0.79  DKL=2.78  1234−6−4−20246−6−4−20246α=0.5  homogeneity=0.87  DKL=2.71  1234−20−15−10−5051015−20−15−10−5051015α=1  homogeneity=0.94  DKL=2.60  1234−30−20−100102030−25−20−15−10−50510152025α=1.5  homogeneity=0.96  DKL=2.61  1234References

[1] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and

clustering. Advances in neural information processing systems  14:585–591  2002.

[2] M. A. Carreira-Perpi˜n´an. Gaussian mean-shift is an em algorithm.

Pattern Analysis And Machine Intelligence  29(5):767–776  2007.

IEEE Transactions On

[3] D. Comaniciu and M. Peter. Mean Shift: A robust approach toward feature space analysis.

IEEE Transactions on Pattern Analysis and Machine Intelligence  24(5):603–619  2002.

[4] J. A. Cook  I. Sutskever  A. Mnih  and G. E. Hinton. Visualizing similarity data with a mixture
of maps. In Proceedings of the 11th International Conference on Artiﬁcial Intelligence and
Statistics  volume 2  pages 67–74  2007.

[5] M. Gashler  D. Ventura  and T. Martinez. Iterative non-linear dimensionality reduction with
manifold sculpting. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in
Neural Information Processing Systems 20  pages 513–520. MIT Press  Cambridge  MA  2008.
[6] G. Hinton and S. Roweis. Stochastic neighbor embedding. Advances in Neural Information

Processing Systems  15:833–840  2003.

[7] G. J. McLachlan and D. Peel. Finite Mixture Models. Wiley  2000.
[8] J. A. K. Suykens. Data visualization and dimensionality reduction using kernel maps with a

reference point. IEEE Transactions on Neural Networks  19(9):1501–1517  2008.

[9] J. B. Tenenbaum  V. Silva  and J. C. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  Dec. 2000.

[10] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research  9:2579–2605  2008.

,Boqing Gong
Kristen Grauman
Fei Sha
Lingqiao Liu
Chunhua Shen
Lei Wang
Anton van den Hengel
Chao Wang