2014,Accelerated Mini-batch Randomized Block Coordinate Descent Method,We consider regularized empirical risk minimization problems. In particular  we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable  we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However  such a ``batch setting may be computationally expensive in practice. In this paper  we propose a mini-batch randomized block coordinate descent (MRBCD) method  which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme  which effectively reduces the variance of the partial gradient estimators. Theoretically  we show that for strongly convex functions  the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application  we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.",Accelerated Mini-batch Randomized Block

Coordinate Descent Method

Tuo Zhao†§⇤ Mo Yu‡⇤ Yiming Wang† Raman Arora† Han Liu§

†Johns Hopkins University ‡Harbin Institute of Technology §Princeton University

{tzhao5 myu25 freewym}@jhu.edu arora@cs.jhu.edu hanliu@princeton.edu

Abstract

We consider regularized empirical risk minimization problems. In particular  we
minimize the sum of a smooth empirical risk function and a nonsmooth regulariza-
tion function. When the regularization function is block separable  we can solve
the minimization problems in a randomized block coordinate descent (RBCD)
manner. Existing RBCD methods usually decrease the objective value by ex-
ploiting the partial gradient of a randomly selected block of coordinates in each
iteration. Thus they need all data to be accessible so that the partial gradient of the
block gradient can be exactly obtained. However  such a “batch” setting may be
computationally expensive in practice. In this paper  we propose a mini-batch ran-
domized block coordinate descent (MRBCD) method  which estimates the partial
gradient of the selected block based on a mini-batch of randomly sampled data
in each iteration. We further accelerate the MRBCD method by exploiting the
semi-stochastic optimization scheme  which effectively reduces the variance of
the partial gradient estimators. Theoretically  we show that for strongly convex
functions  the MRBCD method attains lower overall iteration complexity than ex-
isting RBCD methods. As an application  we further trim the MRBCD method to
solve the regularized sparse learning problems. Our numerical experiments shows
that the MRBCD method naturally exploits the sparsity structure and achieves
better computational performance than existing methods.

Introduction

1
Big data analysis challenges both statistics and computation. In the past decade  researchers have
developed a large family of sparse regularized M-estimators  such as Sparse Linear Regression [16 
23]  Group Sparse Linear Regression [21]  Sparse Logistic Regression [9]  Sparse Support Vector
Machine [22  18]  and etc. These estimators are usually formulated as regularized empirical risk
minimization problems in a generic form as follows [10] 
P(✓) = argmin

F(✓) + R(✓) 

(1.1)

✓

where ✓ is the parameter of the working model. Here we assume the empirical risk function F(✓)
is smooth  and the regularization function R(✓) is non-differentiable. Some ﬁrst order algorithms 
mostly variants of proximal gradient methods [11]  have been proposed for solving (1.1) . For
strongly convex P(✓)  these methods achieve linear rates of convergence [1].
The proximal gradient methods  though simple  are not necessarily efﬁcient for large problems. Note
that empirical risk function F(✓) is usually composed of many smooth component functions:

b✓ = argmin

✓

⇤Both authors contributed equally.

F(✓) =

1
n

nXi=1

nXi=1

rfi(✓) 

fi(✓)

and rF(✓) =

1
n

1

where each fi is associated with a few samples of the whole date set. Since the proximal gradient
methods need to calculate the gradient of F in every iteration  the computational complexity scales
linearly with the sample size (or the number of components functions). Thus the overall computation
can be expensive especially when the sample size is very large in such a “batch” setting [15].
To overcome the above drawback  recent work has focused on stochastic proximal gradient methods
In particular  the
(SPG)  which exploit the additive nature of the empirical risk function F(✓).
SPG methods randomly sample only a few fi’s to estimate the gradient rF(✓)  i.e.  given an index
set B  also as known as a mini-batch [15]  where all elements are independently sampled from
|B|Pi2B rfi(✓). Thus calculating
{1  ...  n} with replacement  we consider a gradient estimator 1
such a “stochastic” gradient can be far less expensive than the proximal gradient methods within
each iteration. Existing literature has established the global convergence results for the stochastic
proximal gradient methods [3  7] based on the unbiasedness of the gradient estimator  i.e. 

EB" 1

|B|Xi2B

rfi(✓)# = rF(✓)

for 8 ✓ 2 Rd.

However  owing to the variance of the gradient estimator introduced by the stochastic sampling  SPG
methods only achieve sublinear rates of convergence even when P(✓) is strongly convex [3  7].
A second line of research has focused randomized block coordinate descent (RBCD) methods.
These methods exploit the block separability of the regularization function R  i.e.  given a parti-
tion {G1  ... Gk} of d coordinates  we use vGj to denote the subvector of v with all indices in Gj 
and then we can write

R(✓) =

rj(✓Gj ) with ✓ = (✓T

G1  ...  ✓T

Gk )T .

kXj=1

Accordingly  they develop the randomized block coordinate descent (RBCD) methods. In particular 
the block coordinate descent methods randomly select a block of coordinates in each iteration  and
then only calculate the gradient of F with respect to the selected block [14  12]. Since the variance
introduced by the block selection asymptotically goes to zero  the RBCD methods also attain lin-
ear rates of convergence when P(✓) is strongly convex. For sparse learning problems  the RBCD
methods have a natural advantage over the proximal gradient methods. Because many blocks of
coordinates stay at zero values throughout most of iterations  we can integrate the active set strategy
into the computation. The active set strategy maintains an only iterates over a small subset of all
blocks [2]  which greatly boosts the computational performance. Recent work has corroborated the
empirical advantage of RBCD methods over the proximal gradient method [4  19  8]. The RBCD
methods  however  still requires that all component functions are accessible within every iteration
so that the partial gradient can be exactly obtained.
To address this issue  we propose a stochastic variant of the RBCD methods  which shares the ad-
vantage with both the SPG and RBCD methods. More speciﬁcally  we randomly select a block of
coordinates in each iteration  and estimate the corresponding partial gradient based on a mini-batch
of fi’s sampled from all component functions. To address the variance introduced by stochastic sam-
pling  we exploit the semi-stochastic optimization scheme proposed in [5  6]. The semi-stochastic
optimization scheme contains two nested loops: For each iteration of the outer loop  we calculate
an exact gradient. Then in the follow-up inner loop  we adjust all estimated partial gradients by the
obtained exact gradient. Such a modiﬁcation  though simple  has a profound impact: the amortized
computational complexity in each iteration is similar to the stochastic optimization  but the rate of
convergence is not compromised. Theoretically  we show that when P(✓) is strongly convex  the
MRBCD method attains better overall iteration complexity than existing RBCD methods. We then
apply the MRBCD method combined with the active set strategy to solve the regularized sparse
learning problems. Our numerical experiments shows that the MRBCD method achieves much bet-
ter computational performance than existing methods.
A closely related method is the stochastic proximal variance reduced gradient method proposed in
[20]. Their method is a variant of the stochastic proximal gradient methods using the same semi-
stochastic optimization scheme as ours  but their method inherits the same drawback as the proximal
gradient method  and does not fully exploit the underlying sparsity structure for large sparse learning
problems. We will compare its computational performance with the MRBCD method in numerical

2

experiments. Note that their method can be viewed as a special example of the MRBCD method
with one single block.
While this paper was under review  we learnt that a similar method was independently proposed by
[17]. They also apply the variance reduction technique into the randomized block coordinate descent
method  and obtain similar theoretical results to ours.

2 Notations and Assumptions

Given a vector v = (v1  ...  vd)T 2 Rd  we deﬁne vector norms: ||v||1 =Pj |vj|  ||v||2 =Pj v2
j  
and ||v||1 = maxj |vj|. Let {G1  ... Gk} be a partition of all d coordinates with |Gj| = pj and
Pk
j=1 pj = d. We use vGj to denote the subvector of v with all indices in Gj  and v\Gj to denote
the subvector of v with all indices in Gj removed.
Throughout the rest of the paper  if not speciﬁed  we make the following assumptions on P(✓).
Assumption 2.1. Each fi(✓) is convex and differentiable. Given the partition {G1  ... Gk}  all
rGj fi(✓) = [rfi(✓)]Gj ’s are Lipschitz continuous  i.e.  there exists a positive constants Lmax such
that for all ✓  ✓0 2 Rd and ✓Gj 6= ✓0

Gj   we have

||rGj fi(✓)  rGj fi(✓0)||  Lmax||✓Gj  ✓0

Gj||.

Moreover  rfi(✓) is Lipschitz continuous  i.e.  there exists a positive constant Tmax for all ✓  ✓0 2
Rd and ✓ 6= ✓0  we have

||rfi(✓)  rfi(✓0)||  Tmax||✓  ✓0||.

Assumption 2.1 also implies that rF(✓) is Lipschitz continuous  and given the tightest Tmax and
Lmax in Assumption 2.1  we have Tmax  kLmax.
Assumption 2.2. F (✓) is strongly convex  i.e.  for all ✓ and ✓0  there exists a positive constant µ
such that

F(✓0) F (✓) + rF(✓)T (✓0  ✓) 

µ
2||✓0  ✓||2.

Note that Assumption 2.2 also implies that P(✓) is strongly convex.
Assumption 2.3. R(✓) is a simple convex nonsmooth function such that given some positive con-
stant ⌘  we can obtain a closed form solution to the following optimization problem 

⌘ (✓0
T j

Gj ) = argmin
✓Gj 2Rpj

1
2⌘||✓Gj  ✓0

Gj||2 + rj(✓).

Assumptions 2.1-2.3 are satisﬁed by many popular regularized empirical risk minimization prob-
lems. We give some examples in the experiments section.

3 Method

The MRBCD method is doubly stochastic  in the sense that we not only randomly select a block
of coordinates  but also randomly sample a mini-batch of component functions from all fi’s. The
partial gradient of the selected block is estimated based on the selected component functions  which
yields a much lower computational complexity than existing RBCD methods in each iteration.
A naive implementation of the MRBCD method is summarized in Algorithm 1. Since the variance
introduced by stochastic sampling over component functions does not go to zero as the number of
iteration increases  we have to choose a sequence of diminishing step sizes (e.g. ⌘t = µ1t1) to
ensure the convergence. When t is large  we only gain very limited descent in each iteration. Thus
the MRBCD-I method can only attain a sublinear rate of convergence.

3

Algorithm 1 Mini-batch Randomized Block Coordinate Descent Method-I: A Naive Implementa-
tion. The stochastic sampling over component functions introduces variance to the partial gradient
estimator. To ensure the convergence  we adopt a sequence of diminishing step sizes  which eventu-
ally leads to sublinear rates of convergence.

Parameter: Step size ⌘t
Initialize: ✓(0)
For t = 1  2  ...

Randomly sample a mini-batch B from {1  ...  n} with equal probability
Randomly sample j from {1  ...  k} with equal probability
\Gj ✓(t1)
✓(t)
Gj T j
\Gj
End for

Gj  ⌘trGj fB(✓(t1))⌘  ✓(t)

⌘t⇣✓(t1)

3.1 MRBCD with Variance Reduction
A recent line of work shows how to reduce the variance in the gradient estimation without deterio-
rating rates of convergence using a semi-stochastic optimization scheme [5  6]. The semi-stochastic
optimization contains two nested loops: In each iteration of the outer loop  we calculate an exact
gradient; Then within the follow-up inner loop  we use the obtained exact gradient to adjust all esti-
mated partial gradients. These adjustments can guarantee that the variance introduced by stochastic
sampling over component functions asymptotically goes to zero (see [5]).

Algorithm 2 Mini-batch Randomized Block Coordinate Descent Method-II: MRBCD + Variance
Reduction. We periodically calculate the exact gradient at the beginning of each outer loop  and
then use the obtained exact gradient to adjust all follow-up estimated partial gradients. These ad-
justments guarantee that the variance introduced by stochastic sampling over component functions
asymptotically goes to zero  and help the MRBCD II method attain linear rates of convergence.

Parameter: update frequency m and step size ⌘

The MRBCD method with variance reduction is summarized in Algorithm 2. In the next section 
we will show that the MRBCD II method attains linear rates of convergence  and the amortized
computational complexity within each iteration is almost the same as that of the MRBCD I method.
Remark 3.1. Another option for the variance reduction is the stochastic averaging scheme as pro-
posed in [13]  which stores the gradients of most recently subsampled component functions. But the
MRBCD method iterates randomly over different blocks of coordinates  which makes the stochastic
averaging scheme inapplicable.

3.2 MRBCD with Variance Reduction and Active Set Strategy
When applying the MRBCD II method to regularized sparse learning problems  we further incor-
porate the active set strategy to boost the empirical performance. Different from existing RBCD
methods  which usually identify the active set by cyclic search  we exploit a proximal gradient pilot
to identify the active set. More speciﬁcally  within each iteration of the outer loop  we conduct a
proximal gradient descent step  and select the support of the resulting solution as the active set. This
is very natural to the MRBCD-II method. Because at the beginning of each outer loop  we always
calculate an exact gradient  and delivering a proximal gradient pilot will not introduce much addi-

4

For s = 1 2 ...

Initialize: e✓(0)

End for

e✓(s) Pm

e✓ e✓(s1) eµ rF (e✓(s1))  ✓(0) e✓(s1)

For t = 1  2  ...  m
Randomly sample a mini-batch B from {1  ...  n} with equal probability
Randomly sample j from {1  ...  k} with equal probability
✓(t)
Gj T j
End for

⌘ ⇣✓(t1)
Gj  ⌘hrGj fB(✓(t1))  rGj fB(e✓) +eµGji⌘  ✓(t)

l=1 ✓(l)

\Gj ✓(t1)
\Gj

For s = 1 2 ...

Initialize: e✓(0)

For j = 1  2  ...  k

6= 0}  |B| = |A|

e✓ e✓(s1) eµ rF (e✓(s1))
⌘/k⇣e✓Gj  ⌘eµGj /k⌘
✓(0)
Gj T j
End for
A { j | ✓(0)
Gj
For t = 1  2  ...  m|A|/k
Randomly sample a mini-batch B from {1  ...  n} with equal probability
Randomly sample j from {1  ...  k} with equal probability
For all j 2 eA
⌘ ⇣✓(t1)
Gj  ⌘hrGj fB(✓(t1))  rGj fB(e✓) +eµGji⌘  ✓(t)
✓(t)
Gj T j
End for
e✓(s) Pm

l=1 ✓(l)

End for

\Gj ✓(t1)
\Gj

tional computational cost. Once the active set is identiﬁed  all randomized block coordinate descent
steps within the follow-up inner loop only iterates over blocks of coordinates in the active set.

Algorithm 3 Mini-batch Randomized Block Coordinate Descent Method-III: MRBCD with Vari-
ance Reduction and Active Set. To fully take advantage of the obtained exact gradient  we adopt
a proximal gradient pilot ✓(0) to identify the active set at each iteration of the outer loop. Then
all randomized coordinate descent steps within the follow-up inner loop only iterate over blocks of
coordinates in the active set.

Parameter: update frequency m and step size ⌘

The MRBCD method with variance reduction and active set strategy is summarized in Algorithm 3.
Since we integrate the active set into the computation  a successive |A| coordinate decent iterations
in MRBCD-III will have similar performance as k iterations in MRBCD-II. Therefore we change
the maximum number of iterations within each inner loop to |A|m/k. Moreover  since the support
is only |A| blocks of coordinates  we only need to take |B| = |A| to guarantee sufﬁcient variance
reduction. These modiﬁcations will further boost the computational performance of MRBCD-III.
Remark 3.2. The exact gradient can be also used to determine the convergence of the MRBCD-
III method. We terminate the iteration when the approximate KKT condition is satisﬁed 

min⇠2@R(e✓) ||eµ + ⇠||  "  where " is a positive preset convergence parameter. Since evaluat-

ing whether the approximate KKT condition holds is based on the exact gradient obtained at each
iteration of the outer loop  it does not introduce much additional computational cost  either.

4 Theory
Before we proceed with our main results of the MRBCD-II method  we ﬁrst introduce the important
lemma for controlling the variance introduced by stochastic sampling.
Lemma 4.1. Let B be a mini-batch sampled from {1  ...  n}. Deﬁne vB = 1

|B|Pi2|B| rfi(e✓) +eµ. Conditioning on ✓(t1)  we have EBvB = rF(✓(t1)) and
hP(✓(t1)) P (b✓) + P(e✓) P (b✓)i .

EB||vB  rF (✓(t1))||2 

4Tmax
|B|

|B|Pi2|B| rfi(✓(t1))

1

The proof of Lemma 4.1 is provided in Appendix A. Lemma 4.1 guarantees that v is an unbiased
estimator of F(✓)  and its variance is bounded by the objective value gap. Therefore we do not need
to choose a sequence diminishing step sizes to reduce the variance.

4.1 Strongly Convex Functions
We then present the concrete rates of convergence of MRBCD-II when P is strongly convex.

5

Theorem 4.2. Suppose that Assumptions 2.1-2.3 hold. Lete✓(s) be a random point generated by the
MRBCD-II method in Algorithm 2. Given a large enough batch B and a small enough learning rate
⌘ such that |B|  Tmax/Lmax and ⌘< L 1

max/4  we have

EP(e✓(s)) P (b✓) ✓

k

µ⌘(1  4⌘Lmax)m

+

4⌘Lmax(m + 1)

(1  4⌘Lmax)m◆s

[P(e✓(0)) P (b✓)].

Here we only present a sketch. The detailed proof is provided in Appendix B. The expected succes-
sive descent of the objective value is composed of two terms: The ﬁrst one is the same as the ex-
pected successive descent of the “batch” RBCD methods; The second one is the variance introduced
by the stochastic sampling. The descent term can be bounded by taking the average of the successive
descent over all blocks of coordinates. The variance term can be bounded using Lemma 4.1. The
mini-batch sampling and adjustments of µ’s guarantees that the variance asymptotically goes to zero
at a proper scale. By taking expectation over the randomness of component functions and blocks of
coordinates throughout all iterations  we derive a geometric rate of convergence.
The next corollary present the concrete iteration complexity of the MRBCD-II method.
Corollary 4.3. Suppose that Assumptions 2.1-2.3 hold. Let |B| = Tmax/Lmax  m = 65kLmax/µ 
and ⌘ = L1

max/16. Given the target accuracy ✏ and some ⇢ 2 (0  1)  for any
s  3 log[P(e✓(0)) P (b✓)/⇢] + 3 log(1/✏) 

we have P(e✓(s)) P (b✓)  ✏ with at last probability 1  ⇢.

Corollary 4.3 is a direct result of Theorem 4.2 and Markov inequality. The detailed proof is provided
in Appendix C.
To characterize the overall iteration complexity  we count the number of partial gradients we es-
timate. In each iteration of the outer loop  we calculate an exact gradient. Thus the number of
estimated partial gradients is O(nk). Within each iteration of the inner loop (m in total)  we esti-
mate the partial gradients based on a mini-batch B. Thus the number of estimate partial gradients
is O(m|B|). If we choose ⌘  m  and B as in Corollary (4.3) and consider ⇢ as a constant  then
the iteration complexity of the MRBCD-II method with respect to the number of estimated partial
gradients is

O ((nk + kTmax/µ) · log(1/✏))  

which is much lower than that of existing “batch” RBCD methods  O (nkLmax/µ · log(1/✏)).
Remark 4.4 (Connection to the MRBCD-III method). There still exists a gap between the theory
and empirical success of the active set strategy and its in existing literature  even for the “batch”
RBCD methods. When incorporating the active set strategy to the RBCD-style methods  we have
known that the empirical performance can be greatly boosted. How to exactly characterize the
theoretical speed up is still largely unknown. Therefore Theorem 4.2 and 4.3 can only serve as an
imprecise characterization of the MRBCD-III method. A rough understanding is that if the solution
has at most q nonzero entries throughout all iterations  then the MRBCD-III method should have an
approximate overall iteration complexity

O ((nk + qTmax/µ) · log(1/✏)) .

4.2 Nonstrongly Convex Functions
When P(✓) is not strongly convex  we can adopt a perturbation approach. Instead of solving (1.1) 
we consider the minimization problem as follows 

~✓ = argmin

✓2Rd F(✓) + ||✓(0)  ✓||2 + R(✓) 

(4.1)

where  is some positive perturbation parameter  and ✓(0) is the initial value. If we consider eF(✓) =
F(✓) + ||✓(0)  ✓||2 in (4.1) as the smooth empirical risk function  then eF(✓) is a strongly convex
function. Thus Corollary 4.3 can be applied to (4.1): When B  m  ⌘  and ⇢ are suitably chosen  given

s  3 log([P(✓(0)) P (~✓)  ||✓(0)  ~✓||2]/⇢) + 3 log(2/✏) 

6

we have P(e✓(s)) P (~✓)  ||✓(0)  ~✓||2  ✏/2 with at least probability 1  ⇢. We then have
P(e✓(s)) P (b✓) P (e✓(s)) P (b✓)  ||✓(0) b✓||2 + ||✓(0) b✓||2
P (e✓(s)) P (~✓)  ||✓(0)  ~✓||2 + ||✓(0) b✓||2  ✏/2 + ||✓(0) b✓||2.
where the second inequality comes from the fact that P(~✓)+||✓(0) ~✓||2 P (✓)+||✓(0)b✓||2 
because ~✓ is the minimizer to (4.1). If we choose  = ✏/||✓(0) b✓||2  we have P(e✓(s))P (b✓)  ✏.
on ✏. Thus if we consider ||✓(0)  b✓||2 as a constant  then the overall iteration complexity of the
perturbation approach becomes O ((nk + kTmax/✏) · log(1/✏)).
5 Numerical Simulations
The ﬁrst sparse learning problem of our interest is Lasso  which solves

Since  depends on the desired accuracy ✏  the number of estimated partial gradients also depends

b✓ = argmin

✓2Rd

1
n

nXi=1

fi(✓) + ||✓||1 with fi =

1
2

(yi  xT

i ✓)2.

(5.1)

We set n = 2000 and d = 1000  and all covariate vectors xi’s are independently sampled from a
1000-dimensional Gaussian distribution with mean 0 and covariance matrix ⌃  where ⌃jj = 1 and
⌃jk = 0.5 for all k 6= j. The ﬁrst 50 entries of the regression coefﬁcient vector ✓ are independently
samples from a uniform distribution over support (2 1)S(+1  +2). The responses yi’s are
generated by the linear model yi = xT
i ✓ + ✏i  where all ✏i’s are independently sampled from a
standard Gaussian distribution N (0  1).
We choose  =plog d/n  and compare the proposed MRBCD-I and MRBCD-II methods with the

“batch” proximal gradient (BPG) method [11]  the stochastic proximal variance reduced gradient
method (SPVRG) [20]  and the “batch” randomized block coordinate descent (BRBCD) method
[12]. We set k = 100. All blocks are of the same size (10 coordinates). For BPG  the step
size is 1/T   where T is the largest singular value of 1
i . For BRBCD  the step size
as 1/L  where L is the maximum over the largest singular values of 1
i=1[xi]Gj of all blocks.
For SPVRG  we choose m = n  and the step size is 1/(4T ). For MRBCD-I  the step size is
1/(Ldt/8000e)  where t is the iteration index. For MRBCD-II  we choose m = n  and the step size
is 1/(4L). Note that the step size and number of iterations m within each inner loop for MRBCD-II
and SPVRG are tuned over a reﬁned grid such that the best computational performance is obtained.

nPn

nPn

i=1 xixT

2
10

0
10

−2

10

−4

10

−6

10

−8

10

l

p
a
G
 
e
u
a
V
e
v
i
t
c
e
b
O

 

j

10

 

−10
0

MRBCD−II
SPVRG
BRBCD
BPG
MRBCD−I

1

2

 

s
e
t
a
m

i
t
s
e
 
s
t
n
e
d
a
r
g

i

 
l

a

i
t
r
a
p

 
f

o

 
r
e
b
m
u
N

8

9

10
6
x 10

9
10

8
10

7
10

6
10
 
0

2

4

6

8

10

12

Regularization Index

 

MRBCD−III
SPVRG
BRBCD

14

16

18

20

3

Number of partial gradient estimates

4

5

6

7

(a) Comparison between different methods for a sin-
gle regularization parameter.

(b) Comparison between different methods for a se-
quence of regularization parameters.

Figure 5.1: [a] The vertical axis corresponds to objective value gaps P(✓) P (b✓) in log scale.

The horizontal axis corresponds to numbers of partial gradient estimates. [b] The horizontal axis
corresponds to indices of regularization parameters. The vertical axis corresponds to numbers of
partial gradient estimates in log scale. We see that MRBCD attains the best performance among all
methods for both settings

We evaluate the computational performance by the number of estimated partial gradients  and the
results averaged over 100 replications are presented in Figure 5.1 [a]. As can be seen  MRBCD-II
outperforms SPVRG  and attains the best performance among all methods. The BRBCD and BPG

7

nPi yixi||1  which yields a null solution (all entries are zero)  and 20 =plog d/n.

perform worse than MRBCD-II and SPVRG due to high computational complexity within each
iteration. MRBCD-I is actually the fastest among all methods at the ﬁrst few iterations  and then
falls behind SPG and SPVRG due to its sublinear rate of convergence.
We then compare the proposed MRBCD-III method with SPVRG and BRBCD for a sequence of
regularization parameters. The sequence contains 21 regularization parameters {0  ...  20}. We
set 0 = || 1
For K = 1  ...  19  we set K = ↵K1  where ↵ = (20/0)1/20. When solving (5.1) with
respect to K  we use the output solution for K1 as the initial solution. The above setting is
often referred to the warm start scheme in existing literature  and it is very natural to sparse learning
problems  since we always need to tune the regularization parameter  to secure good ﬁnite sample
performance. For each regularization parameter  the algorithm terminates the iteration when the
approximate KKT condition is satisﬁed with ✏ = 1010.
The results over 50 replications are presented in Figure 5.1 [b]. As can be seen  MRBCD-III outper-
forms SPVRG and BRBCD  and attains the best performance among all methods. Since BRBCD
is also combined with the active set strategy  it attains better performance than SPVRG. See more
detailed results in Table E.1 in Appendix E

6 Real Data Example

The second sparse learning problem is the elastic-net regularized logistic regression  which solves

b✓ = argmin

✓2Rd

1
n

nXi=1

fi(✓) + 1||✓||1 with fi = log(1 + exp(yixT

i ✓)) +

2
2 ||✓||2.

i=1[xi]2

i=1 xixT

nPn

nPn

n maxjPn

We adopt the rcv1 dataset with n = 20242 and d = 47236. We set k = 200  and each block contains
approximately 237 coordinates.
We choose 2 = 104  and 1 = 104  and compare MRBCD-II with SPVRG and BRBCD.
For BRBCD  the step size as 1/(4L)  where L is the maximum of the largest singular values of
i=1[xi]Gj over all blocks for BRBCD. For SPVRG  m = n and the step size is 1/(16T )  where
1
i . For MRBCD-II  m = n and the step size is
T is the largest singular value of 1/ 1
1/(16T ). For BRBCD  the step size as 1/(4L)  where L = 1
j for BRBCD. Note
that the step size and number of iterations m within each inner loop for MRBCD-II and SPVRG are
tuned over a reﬁned grid such that the best computational performance is obtained.
The results averaged over 30 replications are presented in Figure F.1 [a] of Appendix F. As can be
seen  MRBCD-II outperforms SPVRG  and attains the best performance among all methods. The
BRBCD performs worse than MRBCD-II and SPVRG due to high computational complexity within
each iteration.
We then compare the proposed MRBCD-III method with SPVRG and BRBCD for a sequence of
regularization parameters. The sequence contains 11 regularization parameters {0  ...  10}. We set
0 = || 1Pi rfi(0)||1  which yields a null solution (all entries are zero)  and 10 = 1e  4. For
K = 1  ...  9  we set K = ↵K1  where ↵ = (10/0)1/10. For each regularization parameter 
we set ✏ = 107 for the approximate KKT condition.
The results over 30 replications are presented in Figure F.1 [b] of Appendix F. As can be seen 
MRBCD-III outperforms SPVRG and BRBCD  and attains the best performance among all meth-
ods. Since BRBCD is also combined with the active set strategy  it attains better performance than
SPVRG.

Acknowledgements This work is partially supported by the grants NSF IIS1408910  NSF
IIS1332109  NIH R01MH102339  NIH R01GM083084  and NIH R01HG06841. Yu is supported
by China Scholarship Council and by NSFC 61173073.

8

References
[1] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse prob-

lems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2009.
[3] John Duchi and Yoram Singer. Efﬁcient online and batch learning using forward backward splitting. The

Journal of Machine Learning Research  10:2899–2934  2009.

[4] Jerome Friedman  Trevor Hastie  Holger H¨oﬂing  and Robert Tibshirani. Pathwise coordinate optimiza-

tion. The Annals of Applied Statistics  1(2):302–332  2007.

[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduc-

tion. In Advances in Neural Information Processing Systems  pages 315–323  2013.

[6] Jakub Koneˇcn`y and Peter Richt´arik.

arXiv:1312.1666  2013.

Semi-stochastic gradient descent methods.

arXiv preprint

[7] John Langford  Lihong Li  and Tong Zhang. Sparse online learning via truncated gradient. Journal of

Machine Learning Research  10(777-801):65  2009.

[8] Han Liu  Mark Palatucci  and Jian Zhang. Blockwise coordinate descent procedures for the multi-task
lasso  with applications to neural semantic basis discovery. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning  pages 649–656  2009.

[9] L. Meier  S. Van De Geer  and P B¨uhlmann. The group lasso for logistic regression. Journal of the Royal

Statistical Society: Series B  70(1):53–71  2008.

[10] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  and Bin Yu. A uniﬁed framework
Statistical Science 

for high-dimensional analysis of m-estimators with decomposable regularizers.
27(4):538–557  2012.

[11] Yu Nesterov. Gradient methods for minimizing composite objective function. Technical report  Universit´e

catholique de Louvain  Center for Operations Research and Econometrics (CORE)  2007.

[12] Peter Richt´arik and Martin Tak´aˇc. Iteration complexity of randomized block-coordinate descent methods

for minimizing a composite function. Mathematical Programming  pages 1–38  2012.

[13] Nicolas L Roux  Mark Schmidt  and Francis R Bach. A stochastic gradient method with an exponential
convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems  pages
2672–2680  2012.

[14] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for `1-regularized loss minimization. The

Journal of Machine Learning Research  12:1865–1892  2011.

[15] Suvrit Sra  Sebastian Nowozin  and Stephen J Wright. Optimization for machine learning. Mit Press 

2012.

[16] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[17] Huahua Wang and Arindam Banerjee. Randomized block coordinate descent for online and stochastic

optimization. CoRR  abs/1407.0107  2014.

[18] Li Wang  Ji Zhu  and Hui Zou. The doubly regularized support vector machine. Statistica Sinica 

16(2):589  2006.

[19] Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression. The

Annals of Applied Statistics  2:224–244  2008.

[20] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.

arXiv preprint arXiv:1403.4699  2014.

[21] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika 

94(1):19–35  2007.

[22] Ji Zhu  Saharon Rosset  Trevor Hastie  and Robert Tibshirani. 1-norm support vector machines. In NIPS 

volume 15  pages 49–56  2003.

[23] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal

Statistical Society: Series B (Statistical Methodology)  67(2):301–320  2005.

9

,Tuo Zhao
Mo Yu
Yiming Wang
Raman Arora
Han Liu
Aude Genevay
Marco Cuturi
Gabriel Peyré
Francis Bach