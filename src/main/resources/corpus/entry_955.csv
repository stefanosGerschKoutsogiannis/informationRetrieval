2013,Cluster Trees on Manifolds,We investigate the problem of estimating the cluster tree for a density $f$ supported on or near a smooth $d$-dimensional manifold $M$ isometrically embedded in $\mathbb{R}^D$. We study a $k$-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta. Under mild assumptions on $f$ and $M$  we obtain rates of convergence that depend on $d$ only but not on the ambient dimension $D$. We also provide a sample complexity lower bound for a natural class of clustering algorithms that use $D$-dimensional neighborhoods.,Cluster Trees on Manifolds

Sivaraman Balakrishnan†
sbalakri@cs.cmu.edu

Srivatsan Narayanan†

Alessandro Rinaldo‡

srivatsa@cs.cmu.edu

arinaldo@stat.cmu.edu

Aarti Singh†

aarti@cs.cmu.edu

Larry Wasserman‡

larry@stat.cmu.edu

School of Computer Science† and Department of Statistics‡

Carnegie Mellon University

In this paper we investigate the problem of estimating the cluster tree for a density f supported on
or near a smooth d-dimensional manifold M isometrically embedded in RD. We analyze a modi-
ﬁed version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta
(2010). The main results of this paper show that under mild assumptions on f and M  we obtain
rates of convergence that depend on d only but not on the ambient dimension D. Finally  we sketch
a construction of a sample complexity lower bound instance for a natural class of manifold oblivious
clustering algorithms.

1

Introduction

In this paper  we study the problem of estimating the cluster tree of a density when the density
is supported on or near a manifold. Let X := {X1  . . .   Xn} be a sample drawn i.i.d.
from a
distribution P with density f. The connected components Cf (λ) of the upper level set {x : f (x) ≥
λ} are called density clusters. The collection C = {Cf (λ) : λ ≥ 0} of all such clusters is called the
cluster tree and estimating this cluster tree is referred to as density clustering.

The density clustering paradigm is attractive for various reasons. One of the main difﬁculties of
clustering is that often the true goals of clustering are not clear and this makes clusters  and clustering
as a task seem poorly deﬁned. Density clustering however is estimating a well deﬁned population
quantity  making its goal  consistent recovery of the population density clusters  clear. Typically
only mild assumptions are made on the density f and this allows extremely general shapes and
numbers of clusters at each level. Finally  the cluster tree is an inherently hierarchical object and
thus density clustering algorithms typically do not require speciﬁcation of the “right” level  rather
they capture a summary of the density across all levels.

The search for a simple  statistically consistent estimator of the cluster tree has a long history.
Hartigan (1981) showed that the popular single-linkage algorithm is not consistent for a sample
from RD  with D > 1. Recently  Chaudhuri and Dasgupta (2010) analyzed an algorithm which is
both simple and consistent. The algorithm ﬁnds the connected components of a sequence of care-
fully constructed neighborhood graphs. They showed that  as long as the parameters of the algorithm
are chosen appropriately  the resulting collection of connected components correctly estimates the
cluster tree with high probability.

In this paper  we are concerned with the problem of estimating the cluster tree when the density
f is supported on or near a low dimensional manifold. The motivation for this work stems from
the problem of devising and analyzing clustering algorithms with provable performance that can be
used in high dimensional applications. When data live in high dimensions  clustering (as well as
other statistical tasks) generally become prohibitively difﬁcult due to the curse of dimensionality 

1

which demands a very large sample size. In many high dimensional applications however data is
not spread uniformly but rather concentrates around a low dimensional set. This so-called manifold
hypothesis motivates the study of data generated on or near low dimensional manifolds and the study
of procedures that can adapt effectively to the intrinsic dimensionality of this data.

Here is a brief summary of the main contributions of our paper: (1) We show that the simple al-
gorithm studied in the paper Chaudhuri and Dasgupta (2010) is consistent and has fast rates of
convergence for data on or near a low dimensional manifold M. The algorithm does not require
the user to ﬁrst estimate M (which is a difﬁcult problem). In other words  the algorithm adapts to
the (unknown) manifold. (2) We show that the sample complexity for identifying salient clusters is
independent of the ambient dimension. (3) We sketch a construction of a sample complexity lower
bound instance for a natural class of clustering algorithms that we study in this paper. (4) We intro-
duce a framework for studying consistency of clustering when the distribution is not supported on
a manifold but rather  is concentrated near a manifold. The generative model in this case is that the
data are ﬁrst sampled from a distribution on a manifold and then noise is added. The original data
are latent (unobserved). We show that for certain noise models we can still efﬁciently recover the
cluster tree on the latent samples.

1.1 Related Work

The idea of using probability density functions for clustering dates back to Wishart Wishart (1969).
Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering  of
the cluster tree and of consistency and fractional consistency of clustering algorithms. In partic-
ular  Hartigan (1981) showed that single linkage clustering is consistent when D = 1 but is only
fractionally consistent when D > 1. Stuetzle and R. (2010) and Stuetzle (2003) have also proposed
procedures for recovering the cluster tree. None of these procedures however  come with the theoret-
ical guarantees given by Chaudhuri and Dasgupta (2010)  which demonstrated that a generalization
of Wishart’s algorithm allows one to estimate parts of the cluster tree for distributions with full-
dimensional support near-optimally under rather mild assumptions. This paper forms the starting
point for our work and is reviewed in more detail in the next section.

In the last two decades  much of the research effort involving the use of nonparametric density
estimators for clustering has focused on the more specialized problems of optimal estimation of the
support of the distribution or of a ﬁxed level set. However  consistency of estimators of a ﬁxed level
set does not imply cluster tree consistency  and extending the techniques and analyses mentioned
above to hold simultaneously over a variety of density levels is non-trivial. See for example the
papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al.
(2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman
(2010); Rinaldo et al. (2012)  and references therein. Estimating the cluster tree has more recently
been considered by Kpotufe and von Luxburg (2011) who also give a simple pruning procedure
for removing spurious clusters. Steinwart (2011) and Sriperumbudur and Steinwart (2012) propose
procedures for determining recursively the lowest split in the cluster tree and give conditions for
asymptotic consistency with minimal assumptions on the density.

2 Background and Assumptions

Let P be a distribution supported on an unknown d-dimensional manifold M. We assume that the
manifold M is a d-dimensional Riemannian manifold without boundary embedded in a compact set
X ⊂ RD with d < D. We further assume that the volume of the manifold is bounded from above by
a constant  i.e.  vold(M ) ≤ C. The main regularity condition we impose on M is that its condition
number be not too large. The condition number of M is 1/τ   where τ is the largest number such
that the open normal bundle about M of radius r is imbedded in RD for every r < τ . The condition
number is discussed in more detail in the paper Niyogi et al. (2008).

The Euclidean norm is denoted by k · k and vd denotes the volume of the d-dimensional unit ball in
Rd. B(x  r) denotes the full-dimensional ball of radius r centered at x and BM (x  r) ..= B(x  r) ∩

2

M. For Z ⊂ Rd and σ > 0  deﬁne Zσ = Z + B(0  σ) and ZM σ = (Z + B(0  σ)) ∩ M. Note that
Zσ is full dimensional  while if Z ⊆ M then ZM σ is d-dimensional.
Let f be the density of P with respect to the uniform measure on M. For λ ≥ 0  let Cf (λ) be the
collection of connected components of the level set {x ∈ X : f (x) ≥ λ} and deﬁne the cluster tree
of f to be the hierarchy C = {Cf (λ) : λ ≥ 0}. For a ﬁxed λ  any member of Cf (λ) is a cluster.
For a cluster C its restriction to the sample X is deﬁned to be C[X] = C ∩ X. The restriction of
the cluster tree C to X is deﬁned to be C[X] = {C ∩ X : C ∈ C}. Informally  this restriction is a
dendrogram-like hierarchical partition of X.

To give ﬁnite sample results  following Chaudhuri and Dasgupta (2010)  we deﬁne the notion of
salient clusters. Our deﬁnitions are slight modiﬁcations of those in Chaudhuri and Dasgupta (2010)
to take into account the manifold assumption.

Deﬁnition 1 Clusters A and A′ are (σ  ǫ) separated if there exists a nonempty S ⊂ M such that:

1. Any path along M from A to A′ intersects S.
2. supx∈SM σ f (x) < (1 − ǫ) inf x∈AM σ∪A′

M σ

f (x).

Chaudhuri and Dasgupta (2010) analyze a robust single linkage (RSL) algorithm (in Figure 1). An
RSL algorithm estimates the connected components at a level λ in two stages. In the ﬁrst stage 
the sample is cleaned by thresholding the k-nearest neighbor distance of the sample points at a
radius r and then  in the second stage  the cleaned sample is connected at a connection radius R.
The connected components of the resulting graph give an estimate of the restriction Cf (λ)[X]. In
Section 4 we prove a sample complexity lower bound for the class of RSL algorithms which we now
deﬁne.

Deﬁnition 2 The class of RSL algorithms refers to any algorithm that is of the form described in
the algorithm in Figure 1 and relying on Euclidean balls  with any choice of k  r and R.

We deﬁne two notions of consistency for an estimator bC of the cluster tree:
Deﬁnition 3 (Hartigan consistency) For any sets A  A′ ⊂ X   let An (resp.  A′
est cluster of bC containing A ∩ X (resp  A′ ∩ X). We say bC is consistent if  whenever A and A′ are
different connected components of {x : f (x) ≥ λ} (for some λ > 0)  the probability that An is
disconnected from A′

n) denote the small-

n approaches 1 as n → ∞.

Deﬁnition 4 ((σ  ǫ) consistency) For any sets A  A′ ⊂ X such that A and A′ are (σ  ǫ) separated 
n) denote the smallest cluster of bC containing A ∩ X (resp  A′ ∩ X). We say bC is
let An (resp.  A′
consistent if  whenever A and A′ are different connected components of {x : f (x) ≥ λ} (for some
λ > 0)  the probability that An is disconnected from A′

n approaches 1 as n → ∞.

The notion of (σ  ǫ) consistency is similar that of Hartigan consistency except restricted to (σ  ǫ)
separated clusters A and A′.

Chaudhuri and Dasgupta (2010) prove a theorem  establishing ﬁnite sample bounds for a particular
RSL algorithm. In their result there is no manifold and f is a density with respect to the Lebesgue
measure on RD. Their result in essence says that if

n ≥ O(cid:18)

D

λǫ2vD(σ/2)D log

λǫ2vD(σ/2)D(cid:19)

D

then an RSL algorithm with appropriately chosen parameters can resolve any pair of (σ  ǫ) clusters
at level at least λ.
It is important to note that this theorem does not apply to the setting when
distributions are supported on a lower dimensional set for at least two reasons: (1) the density f is
singular with respect to the Lebesgue measure on X and so the cluster tree is trivial  and (2) the
deﬁnitions of saliency with respect to X are typically not satisﬁed when f has a lower dimensional
support.

3

1. For each Xi  rk(Xi) := inf{r : B(Xi  r) contains k data points}.
2. As r grows from 0 to ∞:

(a) Construct a graph Gr R with nodes {Xi : rk(Xi) ≤ r} and edges (Xi  Xj) if
(b) Let C(r) be the connected components of Gr R.

kXi − Xjk ≤ R.

3. Denote bC = {C(r) : r ∈ [0 ∞)} and return bC.

Figure 1: Robust Single Linkage (RSL) Algorithm

3 Clustering on Manifolds

In this section we show that the RSL algorithm can be adapted to recover the cluster tree of a
distribution supported on a manifold of dimension d < D with the rates depending only on d. In
place of the cluster salience parameter σ  our rates involve a new parameter ρ

ρ := min(cid:18) 3σ

16

 

ǫτ
72d

 

τ

16(cid:19) .

The precise reason for this deﬁnition of ρ will be clear from the proofs (particularly of Lemma 7)
but for now notice that in addition to σ it is dependent on the condition number 1/τ and deteriorates
as the condition number increases. Finally  to succinctly present our results we use µ := log n +
d log(1/ρ).

Theorem 5 There are universal constants C1 and C2 such that the following holds. For any δ > 0 
0 < ǫ < 1/2  run the algorithm in Figure 1 on a sample X drawn from f  where the parameters are
set according to the equations

R = 4ρ and k = C1 log2(1/δ)(µ/ǫ2).

Then with probability at least 1−δ  bC is (σ  ǫ) consistent. In particular  the clusters containing A[X]

and A′[X]  where A and A′ are (σ  ǫ) separated  are internally connected and mutually disconnected
in C(r) for r deﬁned by

vdrdλ =

1

1 − ǫ/6(cid:18) k

n

+

C2 log(1/δ)

n

pkµ(cid:19)

provided λ ≥ 2
Before we prove this theorem a few remarks are in order:

vdρd

k
n .

1. To obtain an explicit sample complexity we plug in the value of k and solve for n from the in-
equality restricting λ. The sample complexity of the RSL algorithm for recovering (σ  ǫ) clusters
at level at least λ on a manifold M with condition number at most 1/τ is

n = O(cid:18)

d

λǫ2vdρd log

d

λǫ2vdρd(cid:19)

where ρ = C min (σ  ǫτ /d  τ ). Ignoring constants that depend on d the main difference between
this result and the result of Chaudhuri and Dasgupta (2010) is that our results only depend on
the manifold dimension d and not the ambient dimension D (typically D ≫ d). There is also a
dependence of our result on 1/(ǫτ )d  for ǫτ ≪ σ. In Section 4 we sketch the construction of an
instance that suggests that this dependence is not an artifact of our analysis and that the sample
complexity of the class of RSL algorithms is at least n ≥ 1/(ǫτ )Ω(d).
2. Another aspect is that our choice of the connection radius R depends on the (typically) unknown
ρ  while for comparison  the connection radius in Chaudhuri and Dasgupta (2010) is chosen to be

4

√2r. Under the mild assumption that λ ≤ nO(1) (which is satisﬁed for instance  if the density

on M is bounded from above)  we show in Appendix A.8 that an identical theorem holds for
R = 4r. k is the only real tuning parameter of this algorithm whose choice depends on ǫ and an
unknown leading constant.

3. It is easy to see that this theorem also establishes consistency for recovering the entire cluster
tree by selecting an appropriate schedule on σn  ǫn and kn that ensures that all clusters are
distinguished for n large enough (see Chaudhuri and Dasgupta (2010) for a formal proof).

Our proofs structurally mirror those in Chaudhuri and Dasgupta (2010). We begin with a few tech-
nical results in 3.1. In Section 3.2 we establish (σ  ǫ) consistency by showing that the clusters are
mutually disjoint and internally connected. The main technical challenge is that the curvature of the
manifold  modulated by its condition number 1/τ   limits our ability to resolve the density level sets
from a ﬁnite sample  by limiting the maximum cleaning and connection radii the algorithm can use.
In what follows  we carefully analyze this effect and show that somewhat surprisingly  despite this
curvature  essentially the same algorithm is able to adapt to the unknown manifold and produce a
consistent estimate of the entire cluster tree. Similar manifold adaptivity results have been shown in
classiﬁcation Dasgupta and Freund (2008) and in non-parametric regression Kpotufe and Dasgupta
(2012); Bickel and Li (2006).

3.1 Technical results

In our proof  we use the uniform convergence of the empirical mass of Euclidean balls to their true
mass. In the full dimensional setting of Chaudhuri and Dasgupta (2010)  this follows from standard
VC inequalities. To the best of our knowledge however sharp (ambient dimension independent)
inequalities for manifolds are unknown. We get around this obstacle by using the insight that  in
order to analyze the RSL algorithms  uniform convergence for Euclidean balls around the sample
points and around a ﬁxed minimum s-net N of M (for an appropriately chosen s) sufﬁce to analyze
the RSL algorithm.

Recall  an s-net N ⊆ M is such that every point of M is at a distance at most s from some point
in N . Let Bn N := nB(z  s) : z ∈ N ∪ X  s ≥ 0o be the collection of balls whose centers are

sample or net points. We now state our uniform convergence lemma. The proof is in Appendix A.3.

Lemma 6 (Uniform Convergence) Assume k ≥ µ. Then there exists a constant C0 such that the
following holds. For every δ > 0  with probability > 1 − δ  for all B ∈ Bn N   we have:

Cδµ

n

+

P (B) ≥
k
Cδ
n
k
n −

=⇒ Pn(B) > 0 
n pkµ =⇒ Pn(B) ≥
k
n
n pkµ =⇒ Pn(B) <
k
n

Cδ

 

 

P (B) ≥
P (B) ≤

where Cδ := 2C0 log(2/δ)  and µ := 1 + log n + log |N| = Cd + log n + d log(1/s). Here
Pn(B) = |X∩ B|/n denotes the empirical probability measure of B  and C is a universal constant.
Next we provide a tight estimate of the volume of a small ball intersected with M. This bounds
the distortion of the apparent density due to the curvature of the manifold and is central to many of
our arguments. Intuitively  the claim states that the volume is approximately that of a d-dimensional
Euclidean ball  provided that its radius is small enough compared to τ . The lower bound is based
on Lemma 5.3 of Niyogi et al. (2008) while the upper bound is based on a modiﬁcation of the main
result of Chazal (2013).

Lemma 7 (Ball volumes) Assume r < τ /2. Deﬁne S := B(x  r) ∩ M for a point x ∈ M. Then

(cid:18)1 −

r2

4τ 2(cid:19)d/2

vdrd ≤ vold(S) ≤ vd(cid:18) τ

τ − 2r1(cid:19)d

rd
1 

5

where r1 = τ − τp1 − 2r/τ . In particular  if r ≤ ǫτ /72d for 0 ≤ ǫ < 1  then

vdrd(1 − ǫ/6) ≤ vold(S) ≤ vdrd(1 + ǫ/6).

3.2 Separation and Connectedness

Lemma 8 (Separation) Assume that we pick k  r and R to satisfy the conditions:

vdrd(1 − ǫ/6)λ ≥

k
n

+

Cδ

r ≤ ρ 
n pkµ 

R = 4ρ 
vdrd(1 + ǫ/6)λ(1 − ǫ) ≤

k
n −

Cδ

n pkµ.

Then with probability 1 − δ  we have: (1) All points in Aσ−r and A′
Sσ−r are removed. (2) The two point sets A ∩ X and A′ ∩ X are disconnected in Gr R.
Proof. The proof is analogous to the separation proof of Chaudhuri and Dasgupta (2010) with sev-
eral modiﬁcations. Most importantly  we need to ensure that despite the curvature of the manifold
we can still resolve the density well enough to guarantee that we can identify and eliminate points
in the region of separation.

σ−r are kept  and all points in

Throughout the proof  we will assume that the good event in Lemma 6 (uniform convergence for
Bn N ) occurs. Since r ≤ ǫτ /72d  by Lemma 7 vol(BM (x  r)) is between vdrd(1 − ǫ/6) and
vdrd(1+ǫ/6)  for any x ∈ M. So if Xi ∈ A∪A′  then BM (Xi  r) has mass at least vdrd(1−ǫ/6)·λ.
n √kµ by assumption  this ball contains at least k sample points  and hence
Since this is ≥ k
Xi is kept. On the other hand  if Xi ∈ Sσ−r  then the set BM (Xi  r) contains mass at most
n √kµ. Thus by Lemma 6 BM (Xi  r) contains fewer than
n − Cδ
vdrd(1 + ǫ/6)· λ(1− ǫ). This is ≤ k
k sample points  and hence Xi is removed.

n + Cδ

To prove the graph is disconnected  we ﬁrst need a bound on the geodesic distance between two
points that are at most R apart in Euclidean distance. Such an estimate follows from Proposition
6.3 in Niyogi et al. (2008) who show that if kp − qk = R ≤ τ /2  then the geodesic distance
dM (p  q) ≤ τ − τq1 − 2R
τ (cid:1) ≤ 2R. Now 
notice that if the graph is connected there must be an edge that connects two points that are at a
geodesic distance of at least 2(σ − r). Any path between a point in A and a point in A′ along M
must pass through Sσ−r and must have a geodesic length of at least 2(σ − r). This is impossible if
the connection radius satisﬁes 2R < 2(σ − r)  which follows by the assumptions on r and R. (cid:3)
All the conditions in Lemma 8 can be simultaneously satisﬁed by setting k := 16C 2
δ (µ/ǫ2)  and

τ . In particular  if R ≤ τ /4  then dM (p  q) < R(cid:0)1 + 4R

vdrd(1 − ǫ/6) · λ =

k
n

+

Cδ

n pkµ.

(1)

k

vdρd

n and the condition on R is satisﬁed by its deﬁnition.

The condition on r is satisﬁed since λ ≥ 2
Lemma 9 (Connectedness) Assume that the parameters k  r and R satisfy the separation condi-
tions (in Lemma 8). Then  with probability at least 1 − δ  A[X] is connected in Gr R.
Proof. Let us show that any two points in A ∩ X are connected in Gr R. Consider y  y′ ∈ A ∩ X.
Since A is connected  there is a path P between y  y′ lying entirely inside A  i.e.  a continuous map
P : [0  1] → A such that P (0) = y and P (1) = y′. We can ﬁnd a sequence of points y0  . . .   yt ∈ P
such that y0 = y  yt = y′  and the geodesic distance on M (and hence the Euclidean distance)
between yi−1 and yi is at most η  for an arbitrarily small constant η.
Let N be minimal R/4-net of M. There exist zi ∈ N such that kyi − zik ≤ R/4. Since yi ∈ A  we
have zi ∈ AM R/4  and hence the ball BM (zi  R/4) lies completely inside AM R/2 ⊆ AM σ−r. In
particular  the density inside the ball is at least λ everywhere  and hence the mass inside it is at least

vd(R/4)d(1 − ǫ/6)λ ≥

Cδµ

n

.

6

Observe that R ≥ 4r and so this condition is satisﬁed as a consequence of satisfying Equation 1.
Thus Lemma 6 guarantees that the ball BM (zi  R/4) contains at least one sample point  say xi.
(Without loss of generality  we may assume x0 = y and xt = y′.) Since the ball lies completely in
AM σ−r  the sample point xi is not removed in the cleaning step (Lemma 8).

Finally  we bound d(xi−1  xi) by considering the sequence of points (xi−1  zi−1  yi−1  yi  zi  xi).
The pair (yi−1  yi) are at most s apart and the other successive pairs at most R/4 apart  hence
d(xi−1  xi) ≤ 4(R/4) + η = R + η. The claim follows by letting η → 0. (cid:3)

4 A lower bound instance for the class of RSL algorithms

1

λǫ2vDσD log

1

d

d

λǫ2vdρd log

ρ = C min (σ  ǫτ /d  τ ). For full dimensional densities  Chaudhuri and Dasgupta (2010) showed

Recall that the sample complexity in Theorem 5 scales as n = O(cid:16)
λǫ2vdρd(cid:17) where
the information theoretic lower bound n = Ω(cid:16)
λǫ2vDσD(cid:17) . Their construction can be
straightforwardly modiﬁed to a d-dimensional instance on a smooth manifold. Ignoring constants
that depend on d  these upper and lower bounds can still differ by a factor of 1/(ǫτ )d  for ǫτ ≪ σ.
In this section we provide an informal sketch of a hard instance for the class of RSL algorithms (see
Deﬁnition 2) that suggests a sample complexity lower bound of n ≥ 1/(ǫτ )Ω(d).
We ﬁrst describe our lower bound instance. The manifold M consists of two disjoint components  C
and C ′ (whose sole function is to ensure f integrates to 1). The component C in turn contains three
parts  which we call ‘top’  ‘middle’  and ‘bottom’ respectively. The middle part  denoted M2  is the

portion of the standard d-dimensional unit sphere Sd(0  1) between the planes x1 = +√1 − 4τ 2
and x1 = −√1 − 4τ 2. The top part  denoted M1  is the upper hemisphere of radius 2τ centered
at (+√1 − 4τ 2  0  0  . . .   0). The bottom part  denoted M3  is a symmetric hemisphere centered at
(−√1 − 4τ 2  0  0  . . .   0). Thus C is obtained by gluing a portion of the unit sphere with two (small)
hemispherical caps. C as described does not have a condition number at most 1/τ because of the
“corners” at the intersection of M2 and M1 ∪ M3. This can be ﬁxed without affecting the essence
of the construction by smoothing this intersection by rolling a ball of radius τ around it (a similar
construction is made rigorous in Theorem 6 of Genovese et al. (2012)). Let P be the distribution
on M whose density over C is λ if |x1| > 1/2  and λ(1 − ǫ) if |x1| ≤ 1/2  where λ is chosen
small enough such that λ vold(C) ≤ 1. The density over C ′ is chosen such that the total mass of the
manifold is 1. Now M1 and M3 are (σ  ǫ) separated at level λ for σ = Ω(1). The separator set S is
the equator of M2 in the plane x1 = 0.

We now provide some intuition for why RSL algorithms will require n ≥ 1/(ǫτ )Ω(d) to succeed on
this instance. We focus our discussion on RSL algorithms with k > 2  i.e. on algorithms that do in
fact use a cleaning step  ignoring the single linkage algorithm which is known to be inconsistent for
full dimensional densities. Intuitively  because of the curvature of the described instance  the mass
of a sufﬁciently large Euclidean ball in the separator set is larger than the mass of a corresponding
ball in the true clusters. This means that any algorithm that uses large balls cannot reliably clean the
sample and this restricts the size of the balls that can be used. Now if points in the regions of high
density are to survive then there must be k sample points in the small ball around any point in the
true clusters and this gives us a lower bound on the necessary sample size.

The RSL algorithms work by counting the number of sample points inside the balls B(x  r) centered
at the sample points x  for some radius r. In order for the algorithm to reliably resolve (σ  ǫ) clusters 
it should distinguish points in the separator set S ⊂ M2 from those in the level λ clusters M1∪M3. A
necessary condition for this is that the mass of a ball B(x  r) for x ∈ Sσ−r should be strictly smaller
than the mass inside B(y  r) for y ∈ M1 ∪ M3. In Appendix A.4  we show that this condition
restricts the radius r to be at most O(τpǫ/d). Now  consider any sample point x0 in M1 ∪ M3
(such an x exists with high probability). Since x0 should not be removed during the cleaning step 
the ball B(x0  r) must contain some other sample point (indeed  it must contain at least k − 1
more sample points). By a union bound  this happens with probability at most (n − 1)vdrdλ ≤

7

O(d−d/2nτ dǫd/2λ). If we want the algorithm to succeed with probability at least 1/2 (say) then

n ≥ Ω(cid:16) dd/2

τ dλǫd/2(cid:17) .

5 Cluster tree recovery in the presence of noise

So far we have considered the problem of recovering the cluster tree given samples from a density
supported on a lower dimensional manifold. In this section we extend these results to the more
general situation when we have noisy samples concentrated near a lower dimensional manifold.
Indeed it can be argued that the manifold + noise model is a natural and general model for high-
dimensional data.
In the noisy setting  it is clear that we can infer the cluster tree of the noisy
density in a straightforward way. A stronger requirement would be consistency with respect to
the underlying latent sample. Following the literature on manifold estimation (Balakrishnan et al.
(2012); Genovese et al. (2012)) we consider two main noise models. For both of them  we specify a
distribution Q for the noisy sample.
1. Clutter Noise: We observe data Y1  . . .   Yn from the mixture Q := (1 − π)U + πP where
0 < π ≤ 1 and U is a uniform distribution on X . Denote the samples drawn from P in this mixture
X = {X1  . . .   Xm}. The points drawn from U are called background clutter. In this case  we can
show:

R := 4ρ

k := C1 log2(1/δ)(µ/ǫ2).

A[X] and A′[X] are internally connected and mutually disconnected in C(r) for r deﬁned by

Theorem 10 There are universal constants C1 and C2 such that the following holds. For any δ > 0 
0 < ǫ < 1/2  run the algorithm in Figure 1 on a sample {Y1  . . .   Yn}  with parameters
Then with probability at least 1 − δ  bC is (σ  ǫ) consistent. In particular  the clusters containing
1 − ǫ/6(cid:18) k
provided λ ≥ max(cid:26) 2
n(cid:1)1−d/D(cid:27) where ρ is now slightly modiﬁed (in con-
(cid:0) k
stants)  i.e.  ρ := min(cid:0) σ
2. Additive Noise: The data are of the form Yi = Xi+ηi where X1  . . .   Xn ∼ P  and η1  . . .   ηn are
a sample from any bounded noise distribution Φ  with ηi ∈ B(0  θ). Note that Q is the convolution
of P and Φ  Q = P ⋆ Φ.

n   2vd/D
24(cid:1).
72d   τ

pkµ(cid:19)

vdρd
7   ǫτ

C2 log(1/δ)

πvdrdλ =

D (1−π)d/D

+

n

vdǫd/Dπ

n

1

k

Theorem 11 There are universal constants C1 and C2 such that the following holds. For any δ > 0 
0 < ǫ < 1/2  run the algorithm in Figure 1 on the sample {Y1  . . .   Yn} with parameters
Then with probability at least 1 − δ  bC is (σ  ǫ) consistent for θ ≤ ρǫ/24d. In particular  the clusters
containing {Yi : Xi ∈ A} and {Yi : Xi ∈ A′} are internally connected and mutually disconnected
in C(r) for r deﬁned by

k := C1 log2(1/δ)(µ/ǫ2).

R := 5ρ

+

Cδ

n pkµ

k
n
ǫτ

vdrd(1 − ǫ/12)(1 − ǫ/6)λ =
7   τ
24  

k

vdρd

144d(cid:1) .

n and θ ≤ ρǫ/24d  where ρ := min(cid:0) σ

if λ ≥ 2
The proofs for both Theorems 10 and 11 appear in Appendix A.5. Notice that in each case we receive
samples from a full D-dimensional distribution but are still able to achieve rates independent of D
because these distributions are concentrated around the lower dimensional M. For the clutter noise
case we produce a tree that is consistent for samples drawn from P (which are exactly on M)  while
in the additive noise case we produce a tree on the observed Yis which is (σ  ǫ) consistent for the
latent Xis (for θ small enough). It is worth noting that in the case of clutter noise we can still
consistently recover the entire cluster tree. Intuitively  this is because the k-NN distances for points
on M are much smaller than for clutter points that are far away from M. As a result the clutter noise
only affects a vanishingly low level set of the cluster tree.

8

References

S. Balakrishnan  A. Rinaldo  D. Sheehy  A. Singh  and L. Wasserman. Minimax rates for homology inference.

AISTATS  2012.

P. Bickel and B. Li. Local polynomial regression on unknown manifolds. In Technical report  Department of

Statistics  UC Berkeley. 2006.

K. Chaudhuri and S. Dasgupta. Rates of convergence for the cluster tree. In J. Lafferty  C. K. I. Williams 
J. Shawe-Taylor  R. Zemel  and A. Culotta  editors  Advances in Neural Information Processing Systems 23 
pages 343–351. 2010.

F. Chazal. An upper bound for the volume of geodesic balls in submanifolds of euclidean spaces. Personal Com-
munication  available at http://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf  2013.
A. Cuevas and R. Fraiman. A plug-in approach to support estimation. Annals of Statistics  25(6):2300–2312 

1997.

A. Cuevas  W. González-Manteiga  and A. Rodríguez-Casal. Plug-in estimation of general level sets. Aust. N.

Z. J. Stat.  48(1):7–19  2006.

S. Dasgupta and Y. Freund. Random projection trees and low dimensional manifolds. In STOC  pages 537–546.

2008.

C. R. Genovese  M. Perone-Paciﬁco  I. Verdinelli  and L. Wasserman. Minimax manifold estimation. Journal

of Machine Learning Research  13:1263–1291  2012.

J. A. Hartigan. Consistency of single linkage for high-density clusters. Journal of the American Statistical

Association  76(374):pp. 388–394  1981.

S. Kpotufe and S. Dasgupta. A tree-based regressor that adapts to intrinsic dimension. J. Comput. Syst. Sci. 

78(5):1496–1515  2012.

S. Kpotufe and U. von Luxburg. Pruning nearest neighbor cluster trees. In L. Getoor and T. Scheffer  editors 
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  ICML ’11  pages 225–
232. ACM  New York  NY  USA  2011.

M. Maier  M. Hein  and U. von Luxburg. Optimal construction of k-nearest-neighbor graphs for identifying

noisy clusters. Theor. Comput. Sci.  410(19):1749–1764  2009.

P. Niyogi  S. Smale  and S. Weinberger. Finding the homology of submanifolds with high conﬁdence from

random samples. Discrete & Computational Geometry  39(1-3):419–441  2008.

W. Polonik. Measuring mass concentrations and estimating density contour clusters: an excess mass approach.

Annals of Statistics  23(3):855–882  1995.

P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Bernoulli  15(4):1154–1178 

2009.

A. Rinaldo  A. Singh  R. Nugent  and L. Wasserman. Stability of density-based clustering. Journal of Machine

Learning Research  13:905–948  2012.

A. Rinaldo and L. Wasserman. Generalized density clustering. The Annals of Statistics  38(5):2678–2722 

2010. 0907.3454.

A. Singh  C. Scott  and R. Nowak. Adaptive {H}ausdorff estimation of density level sets. Ann. Statist. 

37(5B):2760–2782  2009.

B. K. Sriperumbudur and I. Steinwart. Consistency and rates for clustering with dbscan. Journal of Machine

Learning Research - Proceedings Track  22:1090–1098  2012.

I. Steinwart. Adaptive density level set clustering. Journal of Machine Learning Research - Proceedings Track 

19:703–738  2011.

W. Stuetzle. Estimating the cluster tree of a density by analyzing the minimal spanning tree of a sample. J.

Classiﬁcation  20(1):025–047  2003.

W. Stuetzle and N. R. A generalized single linkage method for estimating the cluster tree of a density. Journal

of Computational and Graphical Statistics  19(2):397–418  2010.

A. B. Tsybakov. On nonparametric estimation of density level sets. Ann. Statist.  25(3):948–969  1997.
G. Walther. Granulometric smoothing. Annals of Statistics  25(6):2273–2299  1997.
D. Wishart. Mode analysis: a generalization of nearest neighbor which reduces chaining. In Proceedings of the

Colloquium on Numerical Taxonomy held in the University of St. Andrews  pages 282–308. 1969.

9

,Sivaraman Balakrishnan
Srivatsan Narayanan
Alessandro Rinaldo
Aarti Singh
Larry Wasserman
Tom Goldstein
Min Li
Xiaoming Yuan