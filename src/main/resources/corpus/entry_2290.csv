2017,Generative Local Metric Learning for Kernel Regression,This paper shows how metric learning can be used with Nadaraya-Watson (NW) kernel regression.  Compared with standard approaches  such as bandwidth selection  we show how metric learning can significantly reduce the mean square error (MSE) in kernel regression  particularly for high-dimensional data.  We propose a method for efficiently learning a good metric function based upon analyzing the performance of the NW estimator for Gaussian-distributed data.  A key feature of our approach is that the NW estimator with a learned metric uses information from both the global and local structure of the training data.  Theoretical and empirical results confirm that the learned metric can considerably reduce the bias and MSE for kernel regression even when the data are not confined to Gaussian.,Generative Local Metric Learning for

Kernel Regression

Yung-Kyun Noh

Masashi Sugiyama

Seoul National University  Rep. of Korea

RIKEN / The University of Tokyo  Japan

nohyung@snu.ac.kr

sugi@k.u-tokyo.ac.jp

Kee-Eung Kim

KAIST  Rep. of Korea

kekim@cs.kaist.ac.kr

Seoul National University  Rep. of Korea

Frank C. Park

fcp@snu.ac.kr

Daniel D. Lee

University of Pennsylvania  USA

ddlee@seas.upenn.edu

Abstract

This paper shows how metric learning can be used with Nadaraya-Watson (NW)
kernel regression. Compared with standard approaches  such as bandwidth selec-
tion  we show how metric learning can signiﬁcantly reduce the mean square error
(MSE) in kernel regression  particularly for high-dimensional data. We propose a
method for efﬁciently learning a good metric function based upon analyzing the
performance of the NW estimator for Gaussian-distributed data. A key feature of
our approach is that the NW estimator with a learned metric uses information from
both the global and local structure of the training data. Theoretical and empirical
results conﬁrm that the learned metric can considerably reduce the bias and MSE
for kernel regression even when the data are not conﬁned to Gaussian.

1

Introduction

(cid:80)N
(cid:80)N

(cid:98)y(x) =

The Nadaraya-Watson (NW) estimator has long been widely used for nonparametric regression
[16  26]. The NW estimator uses paired samples to compute a locally weighted average via a kernel
function  K(· ·): RD × RD → R  where D is the dimensionality of data samples. The resulting
estimated output for an input x ∈ RD is given by the equation:

i=1 K(xi  x)yi
i=1 K(xi  x)

(1)
for data D = {xi  yi}N
i=1 with xi ∈ RD and yi ∈ R  and a translation-invariant kernel
K(xi  x) = K((x − xi)2). This estimator is regarded as a fundamental canonical method in
supervised learning for modeling non-linear relationships using local information. It has previously
been used to interpret predictions using kernel density estimation [11]  memory retrieval  decision
making models [19]  minimum empirical mean square error (MSE) with local weights [10  23]  and
sampling-based Bayesian inference [25]. All of these interpretations utilize the fact that the estimator
will asymptotically converge to the optimal Ep(y|x)[y] with minimum MSE given an inﬁnite number
of data samples.

However  with ﬁnite samples  the NW output(cid:98)y(x) is no longer optimal and can deviate signiﬁcantly

from the true conditional expectation. In particular  the weights given along the directions of large

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Metric dependency of kernels. The level curves of kernels are hyper-spheres for isotropic
kernels in (a)  while they are hyper-ellipsoids for kernels with the Mahalanobis metric as shown in (b).
The principal directions of hyper-ellipsoids are the eigenvectors of the symmetric positive deﬁnite
matrix A which is used in the Mahalanobis distance. When the target variable y varies along the ∇y
direction in the ﬁgure  the weighted average will give less bias if the metric is extended along the
orthogonal direction of ∇y as shown in (b).

variability in y—e.g. the direction of ∇y as in Fig. 1(a)—causes signiﬁcant deviation. In this case 
naively changing the kernel shape  as shown in Fig. 1(b)  can alleviate the deviation. In this work  we
investigate more sophisticated methods for ﬁnding appropriate kernel shapes via metric learning.
Metric learning is used to ﬁnd speciﬁc directions with increased variability. Using information from
the training examples  metric learning shrinks or extends distances in directions that are more or
less important. A number of studies have focused on using metric learning for nearest neighbor
classiﬁcation [3  6  8  17  27]  and many recent works have applied it to kernel methods as well
[12  13  28]. Most of these approaches focus on modifying relative distances using triplet relationships
or minimizing empirical error with some regularization.
In conventional NW regression  the deviation due to ﬁnite sampling is mitigated by controlling the
bandwidth of the kernel function. The bandwidth controls the balance between the bias and the
variance of the estimator  and the ﬁnite-sample deviation is reduced with appropriate selection of the
bandwidth [9  20  21]. Other approaches include trying to explicitly subtract an estimated bias [5  24]
or using a higher-order kernel which eliminates the leading-order terms of the bias [22]. However 
many of these direct approaches behave improperly in high-dimensional spaces for two reasons;
distance information is dominated by noise  and by using only nearby data  local algorithms suffer
due to the small number of data used effectively by the algorithms.
In this work  we apply a metric learning method for mitigating the bias. Differently from conventional
metric learning methods  we analyze the metric effect on the asymptotic bias and variance of the NW
estimator. Then we apply a generative model to alleviate the bias in a high-dimensional space. Our
theoretical analysis shows that with a jointly Gaussian assumption on x and y  the metric learning
method reduces to a simple eigenvector problem of ﬁnding a two-dimensional embedding space
where the noise is effectively removed. Our approach is similar to the previous method in applying a
simple generative model to mitigate the bias [18]  but our analysis shows that there always exists a
metric that eliminates the leading-order bias for any shape of Gaussians  and two dimensionality is
enough to achieve the zero bias. The algorithm based on this analysis shows a good performance for
many benchmark datasets. We interpret the result to mean that the NW estimator indirectly uses the
global information through the rough generative model  and the results are improved because the
information from the global covariance structure is additionally used  which would never be used in
NW estimation otherwise.
One well-known extension of NW regression for reducing its bias is locally linear regression (LLR)
[23]. LLR shows a zero-bias as well for data from Gaussian  but the parameter is solely estimated
locally  which is prone to overﬁtting in high-dimensional problems. In our experiments  we compare
our method with LLR and demonstrate that our method compares favorably with LLR and other
competitive methods..
The rest of the paper is organized as follows. In Section 2  we explain our metric learning formulation
for kernel regression. In Section 3  we derive the bias and its relationship to the metric  and our
proposed algorithm is introduced in Section 4. In Section 5  we provide experiments with other
standard regression methods  and conclude with a discussion in Section 6.

2

2 Metric Learning in Kernel Methods

(cid:113)

We consider a Mahalanobis-type distance for metric learning. The Mahalanobis-type distance between
two data points xi ∈ RD and xj ∈ RD is deﬁned in this work as

A (cid:31) 0  A(cid:62) = A 

|A| = 1

||xi − xj||A =

(xi − xj)(cid:62)A(xi − xj) 

(2)
with a symmetric positive deﬁnite matrix A ∈ RD×D and |A|  the determinant of A. By using this
metric  we consider a metric space where the distance is extended or shrunk along the directions
of eigenvectors of A  while the volume of the hypersphere is kept the same due to the determinant
constraint. With an identity matrix A = I  we obtain the conventional Euclidean distance.
A kernel function capturing the local information typically decays rapidly outside a certain distance;
conventionally a bandwidth parameter h is used to control the effective number of data within the
range of interests. If we use the Gaussian kernel as an example  with the aforementioned metric and
bandwidth  the kernel function can be written as

(cid:17)

(cid:19)

(cid:16)

(cid:18)

(cid:18)||xi − x||A

(cid:19)

h

K(xi  x) = K

=

1√
2π

D

hD

exp

(cid:62)
− 1
2h2 (xi − x)

A (xi − x)

 

(3)

where the “relative” bandwidths along individual directions are determined by A  and the overall size
of the kernel is determined by h.

3 Bias of Nadaraya-Watson Kernel Estimator
We ﬁrst note that our target function is the conditional expectation y(x) = E[y|x]  and it is invariant
to metric change. When we consider a D-dimensional vector x ∈ RD and its stochastic prediction
y ∈ R  the conditional expectation y(x) = E[y|x] minimizes the MSE. If we consider two different
spaces with coordinates x ∈ RD and z ∈ RD and a linear transformation between these two spaces 
z = L(cid:62)x  with a full-rank square matrix L ∈ RD×D  the expectation of y is invariant to the
coordinate change satisfying E[y|x] = E[y|z]  because the conditional density is preserved by the
metric change: p(y|x) = p(y|z) for all corresponding x and z  and

(cid:90)

(cid:90)

E[y|x] =

y p(y|x)dy =

y p(y|z)dy = E[y|z].

(4)

The equivalence means that the target function is invariant to metric change with A = LL(cid:62)  and
considering that the NW estimator achieves the optimal prediction E[y|x] with inﬁnite data  optimal
prediction is achieved with inﬁnite data regardless of the choice of metric. Thus the metric dependency
is actually a ﬁnite sampling effect along with the bias and the variance.

3.1 Metric Effects on Bias

The bias is the expected deviation of the estimator from the true mean of the target variable y(x):

Bias = E [(cid:98)y(x) − y(x)] = E

(cid:34)(cid:80)N
(cid:80)N

(cid:35)

i=1 K(xi  x)yi
i=1 K(xi  x)

− y(x)

.

(5)

Standard methods for calculating the bias assume asymptotic concentration around the means  both in
the numerator and in the denominator of the NW estimator. Usually  the numerator and denominator
of the bias are approximated separately  and the bias of the whole NW estimator is calculated using

a plug-in method [15  23]. We assume a kernel satisfying(cid:82) K(z)dz = 1 (cid:82) zK(z)dz = 0  and
(cid:82) zz(cid:62)K(z)dz = I. For example  the Gaussian kernel in Eq. (3) satisﬁes all of these conditions.

Then we can ﬁrst approximate the denominator as1

Ex1 ... xN

K(xi  x)

= p(x) +

∇2p(x) + O(h4) 

h2
2

(6)

(cid:35)

(cid:34)

1
N

N(cid:88)

i=1

1See Appendix in the supplementary material for the detailed derivation.

3

with Laplacian ∇2  the trace of the Hessian with respect to x. Similarly  the expectation of the
numerator becomes

K(x  xi)yi

= p(x)y(x) +

∇2[p(x)y(x)] + O(h4).

(7)

h2
2

(cid:34)

N(cid:88)

i=1

Ex1  . . .   xN  

y1  . . .   yN

1
N

(cid:35)

Using the plug-ins of Eq. (6) and Eq. (7)  we can ﬁnd the leading-order terms of the NW estimation 
and the bias of the NW estimator can be obtained as follows:

(cid:34)(cid:80)N
(cid:80)N

E

(cid:35)

(cid:18)∇(cid:62)p(x)∇y(x)

p(x)

∇2y(x)

2

+

(cid:19)

i=1 K(x  xi)yi
i=1 K(x  xi)

− y(x)

= h2

+ O(h4).

(8)

Here  all gradients ∇ and Laplacians ∇2 are with respect to x. We have noted that the target
y(x) = E[y|x] is invariant to the metric change  and the metric dependency comes from the ﬁnite
sample deviation terms. Here  both the gradient and the Laplacian in the deviation are dependent on
the change of metric A.

3.2 Conventional Methods of Reducing Bias

N(cid:88)
N(cid:88)

i=1

i=1

Previously  there have been works intended to reduce the deviation [9  20  21]. A standard approach
is to adapt the size of bandwidth parameter h under the minimum MSE criterion. Bandwidth selection
has an intuitive motivation of balancing the tradeoff between the bias and the variance; the bias can
be reduced by using a small bandwidth but at the cost of increasing the variance. Therefore  for
bandwidth selection  the bias and variance criteria have to be used at the same time.
Another straightforward and well-known extension of the NW estimator is the locally linear regression
(LLR) [2  23]. Considering that Eq. (1) is the solution minimizing the local empirical MSE:

y(x) = arg min
α∈R

the LLR extends this objective function to

[y(x)  β∗(x)] = arg min

α∈R β∈RD

(yi − α)2K(xi  x) 

(cid:0)yi − α − β(cid:62)(xi − x)(cid:1)2

K(xi  x) 

(9)

(10)

to eliminate the noise produced by the linear component of the target function. The vector parameter
β∗(x) ∈ RD is the estimated local gradient using local data  and this vector often overﬁts in a
high-dimensional space resulting in a poor solution of α.
However  LLR asymptotically produces the bias of

BiasLLR =

∇2y(x) + O(h4).

h2
2

(11)

Eq. (11) can be compared with the NW bias in Eq. (8)  where the bias term from the linear variation
of y with respect to x  h2 ∇(cid:62)p∇y

  is eliminated.

p

4 Metric for Nadaraya-Watson Regression

In this section  we propose a metric that appropriately reduces the metric-dependent bias of the NW
estimator.

4.1 Nadaraya-Watson Regression for Gaussian

In order to obtain a metric  we ﬁrst provide the following theorem which guarantees the existence of
a good metric that eliminates the leading order bias at any point regardless of the conﬁguration of
Gaussian.
Theorem 1: At any point x  there exists a metric matrix A  such that for data x ∈ RD and the output
y ∈ R jointly generated from any (D + 1)-dimensional Gaussian  the NW regression with distance
d(x  x(cid:48)) = ||x − x(cid:48)||A  for x  x(cid:48) ∈ RD  has a zero leading-order bias.

4

Based on the theorem  we will consider using the corresponding metric space for NW regression at
each point. The theorem is proven using the following Proposition 2 and Lemma 3  which are general
claims without the Gaussian assumptions.
Proposition 2: There exists a symmetric positive deﬁnite matrix A that eliminates the ﬁrst term
∇(cid:62)p(x)∇y(x)
inside the bias in Eq. (8)  when used with the metric in Eq. (2)  and when there exist two

linearly independent gradients of p(x) and y(x)  and p(x) is away from zero.
Proof: We consider a coordinate transformation z = L(cid:62)x with L satisfying A = LL(cid:62). The gradient
of a differentiable function y(.) and a density function p(.) with respect to z is

p(x)

∇zy(z)

= L−1∇xy(x)   ∇zp(z)

|L| L−1∇xp(x) 

and the scalar ∇(cid:62)p(x)∇y(x) in the Euclidean space can be rewritten in the transformed space as

(cid:12)(cid:12)(cid:12)z=L(cid:62)x

1

=

(cid:12)(cid:12)(cid:12)z=L(cid:62)x
zy(z)∇z p(z)(cid:1)

∇(cid:62)
z p(z)∇zy(z) =

=

x p(x)L−(cid:62)L−1∇xy(x) + ∇xy(x)L−(cid:62)L−1∇(cid:62)

=

(15)
The symmetric matrix B = ∇y(x)∇(cid:62)p(x) + ∇p(x)∇(cid:62)y(x) has rank two with independent ∇y(x)
and ∇p(x) and can be eigen-decomposed as

x p(x) + ∇xp(x)∇(cid:62)

1
2
1
2|L|
1
2|A| 1

z p(z)∇zy(z) + ∇(cid:62)

(cid:0)∇(cid:62)
(cid:0)∇(cid:62)
tr(cid:2)A−1(cid:0)∇xy(x)∇(cid:62)
(cid:19)(cid:104)
(cid:105)(cid:18) λ1

(cid:104)

2

0
λ2

0

B =

u1 u2

u1 u2

(cid:105)(cid:62)

x p(x)(cid:1)

x y(x)(cid:1)(cid:3) .

(12)

(13)

(14)

(16)

with eigenvectors u1 and u2 and nonzero eigenvalues λ1 and λ2. A sufﬁcient condition for the
existence of A is that the two eigenvalues have different signs  in other words  λ1λ2 < 0.
Let λ1 > 0 and λ2 < 0 without loss of generality  and we choose a positive deﬁnite matrix having
the following eigenvector decomposition:

(cid:104)

u1 u2 ···(cid:105) λ1

A =

0
0 −λ2
...

(cid:104)

···

...

u1 u2 ···(cid:105)(cid:62)

.

(17)

Then Eq. (15) becomes zero  yielding a zero value for the ﬁrst term of the bias with nonzero p(x).
Therefore  we can always ﬁnd A that eliminates the ﬁrst term of the bias once B has one positive and
one negative eigenvalue  and the following Lemma 3 proves that B always has one positive and one
negative eigenvalue. (cid:4)
Lemma 3: A symmetric matrix B = (B(cid:48) +B(cid:48)(cid:62))/2 has two nonzero eigenvalues for a rank one matrix
B(cid:48) = v1v(cid:62)
2 with two linearly independent vectors  v1 and v2. Here  one of the two eigenvalues is
positive  and the other is negative.
Proof: We can reformulate B as
(v1v(cid:62)

2 + v2v(cid:62)

v1 v2

v1 v2

1 ) =

(18)

B =

(cid:104)

(cid:105)(cid:18) 0
(cid:104)

1

(cid:19)(cid:104)
(cid:104)

1
0

(cid:105)(cid:62)

(cid:105)(cid:62)
(cid:105)

.

If we make a new square matrix of size two  M =
  the determinant of the
matrix is as follows using the eigen-decomposition of B with eigenvectors u1 and u2 and eigenvalues
λ1 and λ2:

v1 v2

v1 v2

B

1
2

1
2

|M| =

=

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:105)(cid:20) λ1

v1 v2

v1 v2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)

B

(cid:104)

(cid:105)(cid:62)
(cid:105)(cid:62)(cid:104)
(cid:0)v(cid:62)
1 u1v(cid:62)

v1 v2

= λ1λ2

0
u1 u2
0
λ2
2 u2 − v(cid:62)
1 u2v(cid:62)

2 u1

 

u1 u2

(cid:21)(cid:104)
(cid:1)2

(cid:105)(cid:62)(cid:104)

v1 v2

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)

(19)

(20)

(21)

5

and at the same time  |M| is always negative by the following derivation:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)

|M| =

(cid:105)(cid:62)

(cid:104)

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)

1
2

(cid:105)(cid:62)(cid:104)

(cid:105)(cid:12)(cid:12)(cid:12)(cid:12)2(cid:12)(cid:12)(cid:12)(cid:12)(cid:18) 0

1

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) < 0.

1
0

v1 v2

B

v1 v2

v1 v2

v1 v2

(22)

From these calculations  λ1λ2 < 0  and λ1 and λ2 always have different signs. (cid:4)
With Proposition 2 and Lemma 3  we always have a metric space associated with A in Eq. (17) that
eliminates the leading order bias of a Gaussian  because ∇2y(x) = 0 is always satisﬁed for x and y
which are jointly Gaussian  eliminating the second term of Eq. (8) as well.

4.2 Gaussian Model for Metric Learning

(cid:18) λ+

(cid:19)

We now know there exists an interesting scaling by a metric change where the NW regression achieves
the bias O(h4). The metric we use is as follows:

|ANW| = 1.

0
0 −λ−

[u+u−](cid:62) + γI 

ANW = β[u+u−]

(23)
Here  β is the constant determined from the constraint |ANW| = 1. We use one positive and one
negative eigenvalue  λ+ > 0 and λ− < 0  from matrix B:

for

h2

2p(x)β

λ−+γ

(cid:17)

(cid:17)

B = ∇y(x)∇(cid:62)p(x) + ∇p(x)∇(cid:62)y(x) 

= γh2
2p(x)β

NWB] = h2

2p(x) tr[A−1

(cid:16) λ+ − λ−

(cid:16) λ+
λ++γ − λ−

(24)
and their corresponding eigenvectors u+ and u−. A small positive regularization constant γ is added
after being multiplied by the identity matrix.
By adding a regularization term to the metric  the deviation with exact ∇p(x) and ∇y(x) becomes
nonzero  but a small value 
+
O(γ2). However  with small γ  the deviation is still low unless p(x) is close to zero  or ∇p(x) and
∇y(x) are parallel.
The matrix ANW is obtained for every point of interest  and the NW regression of each point is
performed with a different ANW calculated at each point. ANW is a function of x  but the changing
part is only the rank two matrix  and the calculation is simple  since we only have to solve the
eigenvector problem of a 2 × 2 matrix for each query point regardless of the original dimensionality.
Note that the bandwidth h is not yet included for the optimization when we obtain the metric. After
we obtain the metric  we can still use bandwidth selection for even better MSE.
In order to obtain the metric ANW  at every query  we need the information of ∇p(x) and ∇y(x).
The knowledge of true y(x) and p(x) is unknown  and we need to obtain the gradient information
from data again. Previously  the gradient information was obtained locally with a small number of
samples [4  7]  but such methods are not preferred here because we need to overcome the corruption
of the local information in high-dimensional cases. Instead  we use a global parametric model: Using
a single Gaussian model for all data  we estimate the gradient of true y(x) and p(x) at each point
from the global conﬁguration of data ﬁtted by a single Gaussian:

λ+λ−

(cid:18)(cid:18) y

(cid:19)(cid:19)

= N

(cid:18)(cid:18) µy

(cid:19)

(cid:18) Σy Σyx

(cid:19)(cid:19)

 

p

x

(25)
µx
x (x − µx) + µy (See Appendix) can be analytically
In fact  the target function y(x) = ΣyxΣ−1
obtained in a closed form when we estimate the parameters of the Gaussian  but we reuse y(x) for
gradients for metric learning can be obtained using ∇y(x) =(cid:98)Σ−1
x (x −(cid:98)µx)
enhancement of the NW regression  and the NW regression updates y(x) using local information. The
from the estimated parameters(cid:98)Σx (cid:98)Σxy  and(cid:98)µx if the global model is Gaussian. A pseudo-code of

x (cid:98)Σxy and ∇p(x)

p(x) = −(cid:98)Σ−1

Σxy Σx

.

the proposed method is presented in Algorithm 1.

Interpretation of the Metric

4.3
The learned metric ANW considers the two-dimensional subspace spanned by ∇p(x) =
−p(x)Σ−1
x Σxy. The two-dimensionality analysis of the metric shows
that the distant points are used for those in the space orthogonal to this two-dimensional subspace.

x (x − µx) and ∇y(x) = Σ−1

6

Algorithm 1 Generative Local Metric Learning for NW Regression

Output: regression output(cid:98)y(x)

Input: data D = {xi  yi}N
Procedure:
1: Find joint covariance matrix Σ =

i=1 and point for regression x

(cid:18) Σy Σyx

Σxy Σx

(cid:19)

and mean vector µ =

(cid:19)

(cid:18) µy

µx

from data D.

2: Obtain two eigenvectors
∇p(x)
||∇p(x)|| +
and their corresponding eigenvalues

u1 =

∇y
||∇y||

and u2 =

||∇p(x)|| − ∇y
∇p(x)
||∇y||  

(26)

λ1 =

1

2p(x)

(∇y(cid:62)∇p + ||∇y||||∇p||)

and λ2 =

1

2p(x)

(∇y(cid:62)∇p − ||∇y||||∇p||) 

(27)

using

∇p(x) = −p(x)Σ−1

x (x − µx)

and ∇y = Σ−1

x Σxy.

3: Obtain the transform matrix L using u1  u2  λ1  and λ2:
λ1 + γ/T√−λ2 + γ/T√

√

 |

|

|

|




L =

u1||u1|| u2||u2||

Uo

with T = (cid:0)(λ1 + 1)(−λ2 + γ)γD−2(cid:1) 1



. . .√

γ/T

γ/T√

γ/T

2D   a small constant γ  and an orthonomal matrix Uo ∈

RD×(D−2) spanning the normal space of u1 and u2.
4: Perform NW regression at z = L(cid:62)x using transformed data zi = L(cid:62)xi  i = 1  . . .   N.

(28)

(29)

This fact has the effect of virtually increasing the amount of data compared with algorithms with
isotropic kernels  particularly in high-dimensional space.
The following proposition gives an intuitive explanation that the bias reduction is more important
in high-dimensional space than the reduction of the variance once the optimal bandwidth has been
selected balancing the leading terms of the bias and variance after the change of metric. Proposition
2  Lemma 3  and the following Proposition 4 are obtained without any Gaussian assumption.
Proposition 4: Let us simplify the MSE as the squared bias obtained from the leading terms in
Eq. (8) and the variance2  i.e. 

(31)
Then  at some h∗  it has the the minimum f (h∗) = C1 in the limit with inﬁnite D  where D is the
dimensionality of data.

f (h) = h4C1 +

N hD C2.

Proof: The optimal h can be obtained using ∂f (h)
∂h

= 0  and the optimal h is

h∗ = N− 1

D+4

2See Section 6 of the Appendix:

(cid:18)∇(cid:62)p(x)∇y(x)

p(x)

C1 =

∇2y(x)

2

+

.

(32)

(30)

√
1
π)D
(2

σ2
y(x)
p(x)

1

(cid:12)(cid:12)(cid:12)h=h∗
(cid:18) D · C2
(cid:19)2

4 · C1

(cid:19) 1

D+4

and C2 =

7

(a)

(b)

(c)

Figure 2: (a) Metric calculation for a Gaussian and gradient ∇y. (b) Empirical MSEs with and
without the metric. (c) Leading order terms in MSE with optimal bandwidth for various numbers of
data.

By plugging h∗ into f (h) in Eq. (31)  we obtain

(cid:32)(cid:18) D

(cid:19) 4

D+4

4

D+4(cid:33)
(cid:19) D

(cid:18) 4

D

+

f (h∗) = N− 4

D+4

D

4

C

D+4

D+4
1 C
2

(cid:39) C1.

(for D (cid:29) 4). (cid:4)

(33)

N hD C2 is the
In Proposition 4  the ﬁrst term h4C1 is the square of the bias  and the second term 1
derived variance. The MSE is minimized in a high-dimensional space only through the minimization
of the bias when it is accompanied by the optimization with respect to the bandwidth h. The plot of
MSE in Fig. 2(c) shows that the MSE with bandwidth selection quickly approaches C1 in particular
with a small number of data. The derivation shows that we can ignore the variance optimization with
respect to the metric change. We only focus on achieving a small bias and rather than minimizing the
variance  the bandwidth selection follows later.

5 Experiments

The proposed algorithm is evaluated using both synthetic and real datasets. For a Gaussian  Fig. 2(a)
depicts the eigenvectors along with the eigenvalues of the matrix B = ∇y∇(cid:62)p + ∇p∇(cid:62)y at different
points in the two-dimensional subspace spanned by ∇y and ∇p. The metric can be compared with
the adaptive scaling proposed in [14]  which determines the metric according to the average amount
of ∇y. Our metric also uses ∇y  but the metric is determined using the relationship with ∇p.
Fig. 2(a) shows the metric eigenvalues and eigenvectors at each point for a two-dimensional Gaussian
with a covariance contour in the ﬁgure. With Gaussian data  the MSE with the proposed metric is
shown along with MSE with the Euclidean metric in Fig. 2(b). The metric is obtained from the
estimated parameter of a jointly Gaussian model  where the result with a learned metric shows a huge
difference in the MSE.
For real-data experiments  we used the Delve datasets (Abalone  Bank-8fm  Bank-32fh  CPU)  UCI
datasets (Community  NavalC  NavalT  Protein  Slice)  KEEL datasets (Ailerons  Elevators  Puma32h)
[1]  and datasets from a previous paper (Pendulum  Pol) [15]. The datasets include dozens of features
and several thousands to tens of thousands of data. Using a Gaussian model with regularized
maximum likelihood estimated parameters  we apply a metric which minimizes the bias with a ﬁxed
γ = max(|λ1| |λ2|) × 10−2  and we choose h from a pre-chosen validation set. NW estimation with
the proposed metric (NW+GMetric) is compared with the conventional NW estimation (NW)  LLR
(LLR)  the previous metric learning method for NW regression (NW+WMetric [28]  NW+KMetric
[14])  a more ﬂexible Gaussian process regression (GPR) with the Gaussian kernel  and the Gaussian

globally linear model (GGL) using y(x) =(cid:98)Σyx(cid:98)Σ−1

x (x −(cid:98)µx) +(cid:98)µy.

For eleven datasets among a total of fourteen datasets  the NW estimation with the proposed metric
statistically achieves one of the best performances. Even when the estimation does not achieve
the best performance  the metric always reduces the MSE from the original NW estimation. In
particular  in the Slice  Pol  CPU  NavalC  and NavalT datasets  GGL performs poorly showing the
non-Gaussianity of data  while the metric using the same information effectively reduces the MSE

8

−101−101 ∇y(x)Figure 3: Regression with real-world datasets. NW is the NW regression with conventionial kernels 
NW+GMetric is the NW regression with the proposed metric  LLR is the locally linear regression 
NW+WMetric [28] and NW+KMetric [14] are different metrics for NW regression  GPR is the
Gaussian process regression  and GGL is the Gaussian globally linear model. Normalized MSE
(NMSE) is the ratio between the MSE and the variance of the target value. If we constantly choose
the mean of the target  we get an NMSE of 1.

from the original NW estimator. A detailed discussion comparing the proposed method with other
methods for non-Gaussian data is provided in Section 3 and 4 of the Appendix.

6 Conclusions

An effective metric function is investigated for reducing the bias of NW regression. Our analysis has
shown that the bias can be minimized under certain generative assumptions. The optimal metric is
obtained by solving a series of eigenvector problems of size 2 by 2 and needs no explicit gradients or
curvature information.
The Gaussian model captures only the rough covariance structure of whole data. The proposed
approach uses the global covariance to identify the directions that are most likely to have gradient
components  and the experiments with real data show that the method is effective for more reliable
and less biased estimation. This is in contrast to LLR which attempts to eliminate the linear noise  but
the noise elimination relies on a small number of local data. In contrast  our model uses additional
information from distant data only if they are close in the projected two-dimensional subspace. As a
result  the metric allows a more reliable unbiased estimation of the NW estimator.
We have also shown that minimizing the variance is relatively unimportant in high-dimensional
spaces compared to minimizing the bias  especially when the bandwidth selection method is used.
Consequently  our bias minimization method can achieve sufﬁciently low MSE without the additional
computational cost incurred by empirical MSE minimization.

9

Acknowledgments

YKN acknowledges support from NRF/MSIT-2017R1E1A1A03070945  BK21Plus in Korea  MS from KAK-
ENHI 17H01760 in Japan  KEK from IITP/MSIT 2017-0-01778 in Korea  FCP from BK21Plus  MITIP-
10048320 in Korea  and DDL from the NSF  ONR  ARL  AFOSR  DOT  DARPA in US.

References

[1] J. Alcalá-Fdez  A. Fernandez  J. Luengo  J. Derrac  S. García  L. Sánchez  and F. Herrera. KEEL
data-mining software tool: Data set repository  integration of algorithms and experimental
analysis framework. Journal of Multiple-Valued Logic and Soft Computing  17(2-3):255–287 
2011.

[2] C. G. Atkeson  A. W. Moore  and S. Schaal. Locally weighted learning. Artiﬁcial Intelligence

Review  11(1-5):11–73  1997.

[3] A. Bellet  A. Habrard  and M. Sebban. A survey on metric learning for feature vectors and

structured data. CoRR  abs/1306.6709  2013.

[4] Y. Cheng. Mean shift  mode seeking  and clustering. IEEE Transactions on Pattern Analysis

and Machine Intelligence  17:790–799  1995.

[5] E. Choi  P. Hall  and V. Rousson1. Data sharpening methods for bias reduction in nonparametric

regression. Annals of Statistics  28(5):1339–1355  2000.

[6] J.V. Davis  B. Kulis  P. Jain  S. Sra  and I.S. Dhillon. Information-theoretic metric learning. In
Proceedings of the 24th International Conference on Machine Learning  pages 209–216  2007.
[7] K. Fukunaga and D. H. Larry. The estimation of the gradient of a density function  with
applications in pattern recognition. IEEE Transactions on Information Theory  21:32–40  1975.
[8] J. Goldberger  S. Roweis  G. Hinton  and R. Salakhutdinov. Neighbourhood components

analysis. In Advances in Neural Information Processing Systems 17  pages 513–520. 2005.

[9] P. Hall  S. J. Sheather  M. C. Jones  and J. S. Marron. On optimal data-based bandwidth selection

in kernel density estimation. Biometrika  78:263–269  1991.

[10] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer Series

in Statistics. Springer New York Inc.  New York  NY  USA  2001.

[11] S. Haykin. Neural Networks and Learning Machines (3rd Edition). Prentice Hall  2008.
[12] R. Huang and S. Sun. Kernel regression with sparse metric learning. Journal of Intelligent and

Fuzzy Systems  24(4):775–787  2013.

[13] P. W. Keller  S. Mannor  and D. Precup. Automatic basis function construction for approximate
dynamic programming and reinforcement learning. In Proceedings of the 23rd International
Conference on Machine Learning  pages 449–456  2006.

[14] S. Kpotufe  A. Boularias  T. Schultz  and K. Kim. Gradients weights improve regression and

classiﬁcation. Journal of Machine Learning Research  17(22):1–34  2016.

[15] M. Lazaro-Gredilla and A. R. Figueiras-Vidal. Marginalized neural network mixtures for

large-scale regression. IEEE Transactions on Neural Networks  21(8):1345–1351  2010.

[16] E. A. Nadaraya. On estimating regression. Theory of Probability and its Applications  9:141–

142  1964.

[17] B. Nguyen  C. Morell  and B. De Baets. Large-scale distance metric learning for k-nearest

neighbors regression. Neurocomputing  214:805–814  2016.

[18] Y.-K. Noh  B.-T. Zhang  and D. D. Lee. Generative local metric learning for nearest neighbor
classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence  40(1):106–118 
2018.

[19] R. M. Nosofsky and T. J. Palmeri. An exemplar-based random walk model of speeded classiﬁ-

cation. Psychological Review  104(2):266–300  1997.

[20] B. U. Park and J. S. Marron. Comparison of data-driven bandwidth selectors. Journal of the

American Statistical Association  85:66–72  1990.

[21] B. U. Park and B. A. Turlach. Practical performance of several data driven bandwidth selectors.

Computational Statistics  7:251–270  1992.

10

[22] E. Parzen. On estimation ofa probability density function and mode. Annals of Mathematical

Statistics  33:1065–1076  1962.

[23] D. Ruppert and M. P. Wand. Multivariate Locally Weighted Least Squares Regression. The

Annals of Statistics  22(3):1346–1370  1994.

[24] W. R. Schucany and John P. Sommers. Improvement of kernel type density estimators. Journal

of the American Statistical Association  72:420–423  1977.

[25] L. Shi  T. L. Grifﬁths  N. H. Feldman  and A. N. Sanborn. Exemplar models as a mechanism

for performing Bayesian inference. Psychonomic bulletin & review  17(4):443–464  2010.

[26] Geoffrey S. Watson. Smooth regression analysis. Sankhy¯a: The Indian Journal of Statistics 

Series A  26:359–372  1964.

[27] K. Q. Weinberger  J. Blitzer  and L. K. Saul. Distance metric learning for large margin nearest
neighbor classiﬁcation. In Advances in Neural Information Processing Systems 18  pages
1473–1480. 2006.

[28] K. Q. Weinberger and G. Tesauro. Metric learning for kernel regression. In Eleventh interna-

tional conference on artiﬁcial intelligence and statistics  pages 608–615  2007.

11

,Yuhuai Wu
Saizheng Zhang
Ying Zhang
Yoshua Bengio
Russ Salakhutdinov
Yung-Kyun Noh
Masashi Sugiyama
Kee-Eung Kim
Frank Park
Daniel Lee
Hongzi Mao
Parimarjan Negi
Akshay Narayan
Hanrui Wang
Jiacheng Yang
Haonan Wang
Ryan Marcus
ravichandra addanki
Mehrdad Khani Shirkoohi
Songtao He
Vikram Nathan
Frank Cangialosi
Shaileshh Venkatakrishnan
Wei-Hung Weng
Song Han
Tim Kraska
Dr.Mohammad Alizadeh