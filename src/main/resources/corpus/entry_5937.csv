2013,When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity,Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper  we specify which overcomplete models can be identified given observable moments of a certain order. We consider   probabilistic admixture or topic models  in the overcomplete regime  where the number of latent topics can greatly exceed the size of the observed word vocabulary. While   general  overcomplete topic models are not   identifiable  we establish {\em generic} identifiability under  a  constraint  referred to as  {\em topic persistence}. Our sufficient conditions for identifiability involve a novel set of higher order'' expansion conditions on   the {\em topic-word matrix} or the {\em population structure}   of the   model. This set of higher-order expansion conditions allow for overcomplete models  and require  the existence of a perfect  matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allow for   general (non-degenerate) distributions for modeling the topic proportions  and thus  we can handle arbitrarily correlated topics in our framework.  Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of {\em Tucker} decompositions  but is more general than the {\em Candecomp/Parafac} (CP)   decomposition.",When Are Overcomplete Topic Models Identiﬁable?

Uniqueness of Tensor Tucker Decompositions

with Structured Sparsity

Animashree Anandkumar

University of California

Irvine  CA

Daniel Hsu

Columbia University

New York  NY

a.anandkumar@uci.edu

djhsu@cs.columbia.edu

Majid Janzamin

University of California

Irvine  CA

Sham Kakade

Microsoft Research

Cambridge  MA

mjanzami@uci.edu

skakade@microsoft.com

Abstract

Overcomplete latent representations have been very popular for unsupervised fea-
ture learning in recent years. In this paper  we specify which overcomplete mod-
els can be identiﬁed given observable moments of a certain order. We consider
probabilistic admixture or topic models in the overcomplete regime  where the
number of latent topics can greatly exceed the size of the observed word vocabu-
lary. While general overcomplete topic models are not identiﬁable  we establish
generic identiﬁability under a constraint  referred to as topic persistence. Our suf-
ﬁcient conditions for identiﬁability involve a novel set of “higher order” expan-
sion conditions on the topic-word matrix or the population structure of the model.
This set of higher-order expansion conditions allow for overcomplete models  and
require the existence of a perfect matching from latent topics to higher order ob-
served words. We establish that random structured topic models are identiﬁable
w.h.p. in the overcomplete regime. Our identiﬁability results allow for general
(non-degenerate) distributions for modeling the topic proportions  and thus  we
can handle arbitrarily correlated topics in our framework. Our identiﬁability re-
sults imply uniqueness of a class of tensor decompositions with structured sparsity
which is contained in the class of Tucker decompositions  but is more general than
the Candecomp/Parafac (CP) decomposition.

Keywords: Overcomplete representation  admixture models  generic identiﬁability  tensor decom-
position.

1 Introduction

A probabilistic framework for incorporating features posits latent or hidden variables that can pro-
vide a good explanation to the observed data. Overcomplete probabilistic models can incorporate a
much larger number of latent variables compared to the observed dimensionality. In this paper  we
characterize the conditions under which overcomplete latent variable models can be identiﬁed from
their observed moments.

For any parametric statistical model  identiﬁability is a fundamental question of whether the model
parameters can be uniquely recovered given the observed statistics. Identiﬁability is crucial in a
number of applications where the latent variables are the quantities of interest  e.g. inferring diseases

1

(latent variables) through symptoms (observations)  inferring communities (latent variables) via the
interactions among the actors in a social network (observations)  and so on. Moreover  identiﬁability
can be relevant even in predictive settings  where feature learning is employed for some higher
level task such as classiﬁcation. For instance  non-identiﬁability can lead to the presence of non-
isolated local optima for optimization-based learning methods  and this can affect their convergence
properties  e.g. see [1].

In this paper  we characterize identiﬁability for a popular class of latent variable models  known
as the admixture or topic models [2  3]. These are hierarchical mixture models  which incorporate
the presence of multiple latent states (i.e.
topics) in documents consisting of a tuple of observed
variables (i.e. words). In this paper  we characterize conditions under which the topic models are
identiﬁed through their observed moments in the overcomplete regime. To this end  we introduce
an additional constraint on the model  referred to as topic persistence. Intuitively  this captures the
“locality” effect among the observed words  and goes beyond the usual “bag-of-words” or exchange-
able topic models. Such local dependencies among observations abound in applications such as text 
images and speech  and can lead to more faithful representation. In addition  we establish that the
presence of topic persistence is central to obtaining model identiﬁability in the overcomplete regime 
and we provide an in-depth analysis of this phenomenon in this paper.

1.1 Summary of Results

In this paper  we provide conditions for generic1 model identiﬁability of overcomplete topic models
given observable moments of a certain order (i.e.  a certain number of words in each document). We
introduce a novel constraint  referred to as topic persistence  and analyze its effect on identiﬁability.
We establish identiﬁability in the presence of a novel combinatorial object  named as perfect n-gram
matching  in the bipartite graph from topics to words (observed variables). Finally  we prove that
random models satisfy these criteria  and are thus identiﬁable in the overcomplete regime.

Persistent Topic Model: We ﬁrst introduce the n-persistent topic model  where the parameter n
determines the so-called persistence level of a common topic in a sequence of n successive words  as
seen in Figure 1. The n-persistent model reduces to the popular “bag-of-words” model  when n = 1 
and to the single topic model (i.e. only one topic in each document) when n → ∞. Intuitively  topic
persistence aids identiﬁability since we have multiple views of the common hidden topic generating
a sequence of successive words. We establish that the bag-of-words model (with n = 1) is too
non-informative about the topics to be identiﬁable in the overcomplete regime. On the other hand 
n-persistent overcomplete topic models with n ≥ 2 are generically identiﬁable  and we provide a
set of transparent conditions for identiﬁability.

y1

y2

y2r

h

for

Deterministic Conditions
Identiﬁability:
Our sufﬁcient conditions for identiﬁability are in
the form of expansion conditions from the latent
topic space to the observed word space.
In the
overcomplete regime  there are more topics than
words  and thus it is impossible to have expansion
from topics to words. Instead  we impose a novel
expansion constraint from topics to “higher order”
words  which allows us to handle overcomplete
models. We establish that this condition translates
to the presence of a novel combinatorial object 
referred to as perfect n-gram matching  on the
bipartite graph from topics to words  which encodes
the sparsity pattern of
the topic-word matrix.
Intuitively  this condition implies “diversity” of the
word support for different topics which leads to
identiﬁability. In addition  we present tradeoffs between the topic and word space dimensionality 
topic persistence level  the order of the observed moments at hand  the maximum degree of any

Figure 1: Hierarchical structure of
the n-
2rn number of words
persistent topic model.
(views) are shown for some integer r ≥ 1. A sin-
gle topic yj  j ∈ [2r]  is chosen for each n succes-
sive views {x(j−1)n+1  . . .   x(j−1)n+n}.

x(2r−1)n+1

x2rn

x1

xn

xn+1

x2n

1A model is generically identiﬁable  if all the parameters in the parameter space are identiﬁable  almost

surely. Refer to Deﬁnition 1 for more discussion.

2

topic in the bipartite graph  and the Kruskal rank [4] of the topic-word matrix  for identiﬁability
to hold. We also provide the discussion that how ℓ1-based optimization program can recover the
model under additional constraints.

Identiﬁability of Random Structured Topic Models: We explicitly characterize the regime of
identiﬁability for the random setting  where each topic i is randomly supported on a set of di words 
i.e. the bipartite graph is a random graph. For this random model with q topics  p-dimensional word
vocabulary  and topic persistence level n  when q = O(pn) and Θ(log p) ≤ di ≤ Θ(p1/n)  for all
topics i  the topic-word matrix is identiﬁable from 2nth order observed moments with high probabil-
ity. Furthermore  we establish that the size condition q = O(pn) for identiﬁability is tight.

Implications on Uniqueness of Overcomplete Tucker and CP Tensor Decompositions: We
establish that identiﬁability of an overcomplete topic model is equivalent to uniqueness of the ob-
served moment tensor (of a certain order) decomposition. Our identiﬁability results for persistent
topic models imply uniqueness of a structured class of tensor decompositions  which is contained in
the class of Tucker decompositions  but is more general than the candecomp/parafac (CP) decom-
position [5]. This sub-class of Tucker decompositions involves structured sparsity and symmetry
constraints on the core tensor  and sparsity constraints on the inverse factors of the decomposi-
tion.

Detailed discussion on overview of techniques and related works are provided in the long ver-
sion [12].

2 Model

Notations: The set {1  2  . . .   n} is denoted by [n] := {1  2  . . .   n}. The cardinality of set S is
denoted by |S|. For any vector u (or matrix U )  the support denoted by Supp(u) corresponds to the
location of its non-zero entries. For a vector u ∈ Rq  Diag(u) ∈ Rq×q is the diagonal matrix with
u on its main diagonal. The column space of a matrix A is denoted by Col(A). Operators “⊗” and
“⊙” refer to Kronecker and Khatri-Rao products [6]  respectively.

2.1 Persistent topic model

Rq : ui ≥ 0 Pq

An admixture model speciﬁes a q-dimensional vector of topic proportions h ∈ ∆q−1 := {u ∈
i=1 ui = 1} which generates the observed variables xl ∈ Rp through vectors
a1  . . .   aq ∈ Rp. This collection of vectors ai  i ∈ [q]  is referred to as the population structure or
topic-word matrix [7]. For instance  ai represents the conditional distribution of words given topic
i. The latent variable h is a q dimensional random vector h := [h1  . . .   hq]⊤ known as proportion
vector. A prior distribution P (h) over the probability simplex ∆q−1 characterizes the prior joint
distribution over the latent variables (topics) hi  i ∈ [q].

The n-persistent topic model has a three-level multi-view hierarchy in Figure 1. In this model  a
common hidden topic is persistent for a sequence of n words {x(j−1)n+1  . . .   x(j−1)n+n}  j ∈ [2r].
Note that the random observed variables (words) are exchangeable within groups of size n  where n
is the persistence level  but are not globally exchangeable.

We now describe a linear representation for the n-persistent topic model  on lines of [9]  but with
extensions to incorporate persistence. Each random variable yj  j ∈ [2r]  is a discrete-valued q-
dimensional random variable encoded by the basis vectors ei  i ∈ [q]  where ei is the i-th basis
vector in Rq with the i-th entry equal to 1 and all the others equal to zero. When yj = ei ∈ Rq 
then the topic of j-th group of words is i. Given proportion vector h ∈ Rq  topics yj  j ∈ [2r]  are

independently drawn according to the conditional expectation E(cid:2)yj|h(cid:3) = h  j ∈ [2r]  or equivalently
Pr(cid:2)yj = ei|h(cid:3) = hi  j ∈ [2r]  i ∈ [q].

Each observed variable xl for l ∈ [2rn]  is a discrete-valued p-dimensional random variable (word)
where p is the size of vocabulary. Again  we assume that variables xl  are encoded by the basis
vectors ek  k ∈ [p]  such that xl = ek ∈ Rp when the l-th word in the document is k. Given the

3

corresponding topic yj  j ∈ [2r]  words xl  l ∈ [2rn]  are independently drawn according to the
conditional expectation

E(cid:2)x(j−1)n+k|yj = ei(cid:3) = ai  i ∈ [q]  j ∈ [2r]  k ∈ [n] 

where vectors ai ∈ Rp  i ∈ [q]  are the conditional probability distribution vectors. The matrix
A = [a1|a2| · · · |aq] ∈ Rp×q collecting these vectors is called population structure or topic-word
matrix.

The (2rn)-th order moment of observed variables xl  l ∈ [2rn]  for some integer r ≥ 1  is deﬁned
as (in the matrix form)

M2rn(x) := E(cid:2)(x1 ⊗ x2 ⊗ · · · ⊗ xrn)(xrn+1 ⊗ xrn+2 ⊗ · · · ⊗ x2rn)⊤(cid:3) ∈ Rprn×prn

For the n-persistent topic model with 2rn number of observations (words) xl  l ∈ [2rn]  the corre-
sponding moment is denoted by M (n)
In this paper  we consider the problem of identiﬁability when exact moments are available.
Given M (n)
2rn(x)  what are the sufﬁcient conditions under which the population structure A =
[a1|a2| · · · |aq] ∈ Rp×q is identiﬁable? This is answered in Section 3.

2rn(x).

.

(1)

3 Sufﬁcient Conditions for Generic Identiﬁability

In this section  the identiﬁability result for the n-persistent topic model with access to (2n)-th order
observed moment is provided. First  sufﬁcient deterministic conditions on the population structure
A are provided for identiﬁability in Theorem 1. Next  the deterministic analysis is specialized to a
random structured model in Theorem 2.

We now make the notion of identiﬁability precise. As deﬁned in literature  (strict) identiﬁability
means that the population structure A can be uniquely recovered up to permutation and scaling for
all A ∈ Rp×q.
Instead  we consider a more relaxed notion of identiﬁability  known as generic
identiﬁability.
Deﬁnition 1 (Generic identiﬁability). We refer to a matrix A ∈ Rp×q as generic  with a ﬁxed
sparsity pattern when the nonzero entries of A are drawn from a distribution which is absolutely
continuous with respect to Lebesgue measure 2. For a given sparsity pattern  the class of population
structure matrices is said to be generically identiﬁable [10]  if all the non-identiﬁable matrices form
a set of Lebesgue measure zero.

The (2r)-th order moment of hidden variables h ∈ Rq  denoted by M2r(h)  is deﬁned as

M2r(h) := E(cid:20)(cid:16)

r times

h ⊗ · · · ⊗ h(cid:17)(cid:16)
z
{

}|

r times

{

h ⊗ · · · ⊗ h(cid:17)⊤(cid:21) ∈ Rqr ×qr
z

}|

.

(2)

Condition 1 (Non-degeneracy). The (2r)-th order moment of hidden variables h ∈ Rq  deﬁned in
equation (2)  is full rank (non-degeneracy of hidden nodes).

Note that there is no hope of distinguishing distinct hidden nodes without this non-degeneracy as-
sumption. We do not impose any other assumption on hidden variables and can incorporate arbitrar-
ily correlated topics.

Furthermore  we can only hope to identify the population structure A up to scaling and permutation.
Therefore  we can identify A up to a canonical form deﬁned as:
Deﬁnition 2 (Canonical form). Population structure A is said to be in canonical form if all of its
columns have unit norm.

3.1 Deterministic Conditions for Generic Identiﬁability

Before providing the main result  a generalized notion of (perfect) matching for bipartite graphs is
deﬁned. We subsequently impose these conditions on the bipartite graph from topics to words which
encodes the sparsity pattern of population structure A.

2As an equivalent deﬁnition  if the non-zero entries of an arbitrary sparse matrix are independently perturbed

with noise drawn from a continuous distribution to generate A  then A is called generic.

4

Generalized matching for bipartite graphs: A bipartite graph with two disjoint vertex sets Y
and X and an edge set E between them is denoted by G(Y  X; E). Given the bi-adjacency matrix
A  the notation G(Y  X; A) is also used to denote a bipartite graph. Here  the rows and columns of
matrix A ∈ R|X|×|Y | are respectively indexed by X and Y vertex sets. Furthermore  for any subset
S ⊆ Y   the set of neighbors of vertices in S with respect to A is denoted by NA(S).
Deﬁnition 3 ((Perfect) n-gram matching). A n-gram matching M for a bipartite graph G(Y  X; E)
is a subset of edges M ⊆ E which satisﬁes the following conditions. First  for any j ∈ Y   we
have |NM (j)| ≤ n. Second  for any j1  j2 ∈ Y  j1 6= j2  we have min{|NM (j1)|  |NM (j2)|} >
|NM (j1) ∩ NM (j2)|.
A perfect n-gram matching or Y -saturating n-gram matching for the bipartite graph G(Y  X; E) is
a n-gram matching M in which each vertex in Y is the end-point of exactly n edges in M .

In words  in a n-gram matching M   each vertex j ∈ Y is at most the end-point of n edges in M and
for any pair of vertices in Y (j1  j2 ∈ Y  j1 6= j2)  there exists at least one non-common neighbor in
set X for each of them (j1 and j2).

Note that 1-gram matching is the same as regular matching for bipartite graphs.
Remark 1 (Necessary size bound). Consider a bipartite graph G(Y  X; E) with |Y | = q and

and each combination can at most have one neighbor (a node in Y which is connected to all nodes

|X| = p which has a perfect n-gram matching. Note that there are (cid:0)p
in the combination) through the matching  and therefore we necessarily have q ≤ (cid:0)p
n(cid:1).

n(cid:1) n-combinations on X side

Identiﬁability conditions based on existence of perfect n-gram matching in topic-word graph:
Now  we are ready to propose the identiﬁability conditions and result.
Condition 2 (Perfect n-gram matching on A). The bipartite graph G(Vh  Vo; A) between hidden
and observed variables  has a perfect n-gram matching.

The above condition implies that the sparsity pattern of matrix A is appropriately scattered in the
mapping from hidden to observed variables to be identiﬁable. Intuitively  it means that every hidden
node can be distinguished from another hidden node by its unique set of neighbors under the corre-
sponding n-gram matching.
Furthermore  condition 2 is the key to be able to propose identiﬁability in the overcomplete regime.
As stated in the size bound in Remark 1  for n ≥ 2  the number of hidden variables can be more
than the number of observed variables and we can still have perfect n-gram matching.
Deﬁnition 4 (Kruskal rank  [11]). The Kruskal rank or the krank of matrix A is deﬁned as the
maximum number k such that every subset of k columns of A is linearly independent.
Condition 3 (Krank condition on A). The Kruskal rank of matrix A satisﬁes the bound krank(A) ≥
dmax(A)n  where dmax(A) is the maximum node degree of any column of A.

In the overcomplete regime  it is not possible for A to be full column rank and krank(A) < |Vh| = q.
However  note that a large enough krank ensures that appropriate sized subsets of columns of A are
linearly independent. For instance  when krank(A) > 1  any two columns cannot be collinear and
the above condition rules out the collinear case for identiﬁability. In the above condition  we see
that a larger krank can incorporate denser connections between topics and words.

The main identiﬁability result under a ﬁxed graph structure is stated in the following theorem for
n ≥ 2  where n is the topic persistence level.
Theorem 1 (Generic identiﬁability under deterministic topic-word graph structure). Let M (n)
2rn(x)
in equation (1) be the (2rn)-th order observed moment of the n-persistent topic model  for some
integer r ≥ 1. If the model satisﬁes conditions 1  2 and 3  then  for any n ≥ 2  all the columns of
population structure A are generically identiﬁable from M (n)
2rn(x). Furthermore  the (2r)-th order
moment of the hidden variables  denoted by M2r(h)  is also generically identiﬁable.

The theorem is proved in Appendix A of the long version [12]. It is seen that the population structure
A is identiﬁable  given any observed moment of order at least 2n. Increasing the order of observed
moment results in identifying higher order moments of the hidden variables.
The above theorem does not cover the case of n = 1. This is the usual bag-of-words admixture
model. Identiﬁability of this model has been studied earlier [13]  and we recall it below.
Remark 2 (Bag-of-words admixture model  [13]). Given (2r)-th order observed moments with
r ≥ 1  the structure of the popular bag-of-words admixture model and the (2r)-th order moment of

5

hidden variables are identiﬁable  when A is full column rank and the following expansion condition
holds [13]

|NA(S)| ≥ |S| + dmax(A) 

∀S ⊆ Vh  |S| ≥ 2.

(3)

Our result for n ≥ 2 in Theorem 1  provides identiﬁability in the overcomplete regime with weaker
matching condition 2 and krank condition 3. The matching condition 2 is weaker than the above
expansion condition which is based on the perfect matching and hence  does not allow overcomplete
models. Furthermore  the above result for the bag-of-words admixture model requires full column
rank of A which is more stringent than our krank condition 3.
Remark 3 (Recovery using ℓ1 optimization). It turns out that our conditions for identiﬁability imply

that the columns of the n-gram matrix 3 A⊙n  are the sparsest vectors in Col(cid:16)M (n)

tensor rank of one. See Appendix A in the long version [12]. This implies recovery of the columns
of A through exhaustive search  which is not efﬁcient. Efﬁcient ℓ1-based recovery algorithms have
been analyzed in [13  14] for the undercomplete case (n = 1). They can be employed here for
recovery from higher order moments as well. Exploiting additional structure present in A⊙n  for
n > 1  such as rank-1 test devices proposed in [15] are interesting avenues for future investigation.

2n (x)(cid:17)  having a

3.2 Analysis Under Random Topic-Word Graph Structures

In this section  we specialize the identiﬁability result to the random case. This result is based on more
transparent conditions on the size and the degree of the random bipartite graph G(Vh  Vo; A). We
consider the random model where in the bipartite graph G(Vh  Vo; A)  each node i ∈ Vh is randomly
connected to di different nodes in set Vo. Note that this is a heterogeneous degree model.
Condition 4 (Size condition). The random bipartite graph G(Vh  Vo; A) with |Vh| = q  |Vo| = p 

and A ∈ Rp×q  satisﬁes the size condition q ≤ (cid:0)c p
n(cid:1)n

for some constant 0 < c < 1.

This size condition is required to establish that the random bipartite graph has a perfect n-gram
matching (and hence satisﬁes deterministic condition 2). It is shown that the necessary size con-
straint q = O(pn) stated in Remark 1  is achieved in the random case. Thus  the above constraint
allows for the overcomplete regime  where q ≫ p for n ≥ 2  and is tight.
Condition 5 (Degree condition). In the random bipartite graph G(Vh  Vo; A) with |Vh| = q  |Vo| =
p  and A ∈ Rp×q  the degree di of nodes i ∈ Vh satisﬁes the lower and upper bounds dmin ≥
max{1 + β log p  α log p} for some constants β > n−1
dmax ≤ (cp)

log 1/c   α > max(cid:8)2n2(cid:0)β log 1

c + 1(cid:1)  2βn(cid:9)  and

1
n .

Intuitively  the lower bound on the degree is required to show that the corresponding bipartite graph
G(Vh  Vo; A) has sufﬁcient number of random edges to ensure that it has perfect n-gram match-
ing with high probability. The upper bound on the degree is mainly required to satisfy the krank
condition 3  where dmax(A)n ≤ krank(A).

It is important to see that  for n ≥ 2  the above condition on degree covers a range of models from
sparse to intermediate regimes and it is reasonable in a number of applications that each topic does
not generate a very large number of words.

Probability rate constants: The probability rate of success in the following random identiﬁability
result is speciﬁed by constants β′ > 0 and γ = γ1 + γ2 > 0 as

β′ = −β log c − n + 1 

γ1 = en−1(cid:16) c

nn−1 +

e2

1 − δ1

nβ′+1(cid:17) 

cn−1e2

γ2 =

nn(1 − δ2)

 

(4)

(5)

(6)

where δ1 and δ2 are some constants satisfying e2(cid:16) p

n(cid:17)−β log 1/c

1.

< δ1 < 1 and cn−1e2

nn p−β′

< δ2 <

3A⊙n := A ⊙ · · · ⊙ A

|

{z

.
}

n times

6

h

y

x1

xm xm+1

x2m

h

y1

x1

ym ym+1

y2m

xm xm+1

x2m

(a) Single topic model
(inﬁnite-persistent topic model)

(b) Bag-of-words admixture model
(1-persistent topic model)

Figure 2: Hierarchical structure of the single topic model and bag-of-words admixture model shown for 2m
number of words (views).

Theorem 2 (Random identiﬁability). Let M (n)
2rn(x) in equation (1) be the (2rn)-th order observed
moment of the n-persistent topic model for some integer r ≥ 1. If the model with random population
structure A satisﬁes conditions 1  4 and 5  then whp (with probability at least 1−γp−β′
for constants
β′ > 0 and γ > 0  speciﬁed in (4)-(6))  for any n ≥ 2  all the columns of population structure A are
identiﬁable from M (n)
2rn(x). Furthermore  the (2r)-th order moment of hidden variables  denoted by
M2r(h)  is also identiﬁable  whp.

The theorem is proved in Appendix B of the long version [12]. Similar to the deterministic analysis 
it is seen that the population structure A is identiﬁable given any observed moment with order at
least 2n. Increasing the order of observed moment results in identifying higher order moments of
the hidden variables.

The above identiﬁability theorem only covers for n ≥ 2 and the n = 1 case is addressed in the
following remark.
Remark 4 (Bag-of-words admixture model). The identiﬁability result for the random bag-of-words
admixture model is comparable to the result in [14]  which considers exact recovery of sparsely-used
dictionaries. They assume that Y = DX is given for some unknown arbitrary dictionary D ∈ Rq×q
and unknown random sparse coefﬁcient matrix X ∈ Rq×p. They establish that if D ∈ Rq×q is
full rank and the random sparse coefﬁcient matrix X ∈ Rq×p follows the Bernoulli-subgaussian
model with size constraint p > Cq log q and degree constraint O(log q) < E[d] < O(q log q)  then
the model is identiﬁable  whp. Comparing the size and degree constraints  our identiﬁability result
for n ≥ 2 requires more stringent upper bound on the degree (d = O(p1/n))  while more relaxed
condition on the size (q = O(pn)) which allows to identiﬁability in the overcomplete regime.
Remark 5 (The size condition is tight). The size bound q = O(pn) in the above theorem achieves

the necessary condition that q ≤ (cid:0)p

is argued in Theorem 3 of the long version [12]  where we show that the matching condition 2 holds
under the above size and degree conditions 4 and 5.

n(cid:1) = O(pn) (see Remark 1)  and is therefore tight. The sufﬁciency

4 Why Persistence Helps in Identiﬁability of Overcomplete Models?

In this section  we provide the moment characterization of the 2-persistent topic model. Then  we
provide a discussion and comparison on why persistence helps in providing identiﬁability in the
overcomplete regime. The moment characterization and detailed tensor analysis is provided in the
long version [12].

The single topic model (n → ∞) is shown in Figure 2a and the bag-of-words admixture model
(n = 1) is shown in Figure 2b. In order to have a fair comparison among these different models  we
ﬁx the number of observed variables to 4 (case m = 2) and vary the persistence level. Consider three
different models: 2-persistent topic model  single topic model and bag-of-words admixture model
(1-persistent topic model). From moment characterization results provided in the long version [12] 
we have the following moment forms for each of these models.

For the 2-persistent topic model with 4 words (r = 1  n = 2)  we have

M (2)

4 (x) = (A ⊙ A)E(cid:2)hh⊤](A ⊙ A)⊤.

7

(7)

For the single topic model with 4 words  we have

And for the bag-of-words-admixture model with 4 words (r = 2  n = 1)  we have

4

M (∞)

(x) = (A ⊙ A) Diag(cid:0)E(cid:2)h](cid:1) (A ⊙ A)⊤ 
4 (x) = (A ⊗ A)E(cid:2)(h ⊗ h)(h ⊗ h)⊤(cid:3)(A ⊗ A)⊤.

M (1)

(8)

(9)

Note that for the single topic model in (8)  the Khatri-Rao product matrix A ⊙ A ∈ Rp2×q has
the same as the number of columns (i.e. the latent dimensionality) of the original matrix A  while
the number of rows (i.e. the observed dimensionality) is increased. Thus  the Khatri-Rao product
“expands” the effect of hidden variables to higher order observed variables  which is the key towards
identifying overcomplete models. In other words  the original overcomplete representation becomes
determined due to the ‘expansion effect’ of the Khatri-Rao product structure of the higher order
observed moments.

On the other hand  in the bag-of-words admixture model in (9)  this interesting ‘expansion property’
does not occur  and we have the Kronecker product A ⊗ A ∈ Rp2×q2
  in place of the Khatri-Rao
products. The Kronecker product operation increases both the number of the columns (i.e. latent
dimensionality) and the number of rows (i.e. observed dimensionality)  which implies that higher
order moments do not help in identifying overcomplete models.

Finally  Contrasting equation (7) with (8) and (9)  we ﬁnd that the 2-persistent model retains the
desirable property of possessing Khatri-Rao products  while being more general than the form for
single topic model in (8). This key property enables us to establish identiﬁability of topic models
with ﬁnite persistence levels.
Remark 6 (Relationship to tensor decompositions). In the long version of this work [12]  we es-
tablish that the tensor representation of our model is a special case of the Tucker representation 
but more general than the symmetric CP representation [6]. Therefore  our identiﬁability results
also imply uniqueness of a class of tensor decompositions with structured sparsity which is con-
tained in the class of Tucker decompositions  but is more general than the Candecomp/Parafac (CP)
decomposition.

5 Proof sketch

M (n)

Rest.

The moment of n-persistent

topic model with 2n words is derived as M (n)

2n (x) =
; see [12]. When hidden variables are non-degenerate and A⊙n is full col-

(A⊙n) E(cid:2)hh⊤(cid:3) (A⊙n)⊤
umn rank  we have Col(cid:0)M (n)
2n (x) reduces to ﬁnding A⊙n in Col(cid:0)A⊙n(cid:1). Then  under the expansion condition 4
(cid:12)(cid:12)(cid:12)NA⊙n
we establish that  matrix A is identiﬁable from Col(cid:0)A⊙n(cid:1). This identiﬁability claim is proved
Col(cid:0)A⊙n(cid:1) under the sufﬁcient expansion and genericity conditions.

2n (x)(cid:1) = Col(cid:0)A⊙n(cid:1). Therefore  the problem of recovering A from
(S)(cid:12)(cid:12)(cid:12) ≥ |S| + dmax(cid:16)A⊙n(cid:17) 

by showing that the columns of A⊙n are the sparsest and rank-1 (in the tensor form) vectors in

Then  it is established that  sufﬁcient combinatorial conditions on matrix A (conditions 2 and 3)
ensure that the expansion and rank conditions on A⊙n are satisﬁed. This is shown by proving that
the existence of perfect n-gram matching on A results in the existence of perfect matching on A⊙n.
For further discussion on proof techniques  see the long version [12].

∀S ⊆ Vh  |S| > krank(A) 

Acknowledgments

The authors acknowledge useful discussions with Sina Jafarpour  Adel Javanmard  Alex Dimakis 
Moses Charikar  Sanjeev Arora  Ankur Moitra and Kamalika Chaudhuri. A. Anandkumar is sup-
ported in part by Microsoft Faculty Fellowship  NSF Career award CCF-1254106  NSF Award
CCF-1219234  ARO Award W911NF-12-1-0404  and ARO YIP Award W911NF-13-1-0084. M.
Janzamin is supported by NSF Award CCF-1219234  ARO Award W911NF-12-1-0404 and ARO
YIP Award W911NF-13-1-0084.

4A⊙n

Rest. is the restricted version of n-gram matrix A⊙n  in which the redundant rows of A⊙n are removed.

8

References

[1] Andr´e Uschmajew. Local convergence of the alternating least squares algorithm for canonical
tensor approximation. SIAM Journal on Matrix Analysis and Applications  33(2):639–652 
2012.

[2] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet Allocation. Journal of

Machine Learning Research  3:993–1022  2003.

[3] J. K. Pritchard  M. Stephens  and P. Donnelly. Inference of population structure using multilo-

cus genotype data. Genetics  155:945–959  2000.

[4] J.B. Kruskal. More factors than subjects  tests and treatments: an indeterminacy theorem for
canonical decomposition and individual differences scaling. Psychometrika  41(3):281–293 
1976.

[5] Tamara Kolda and Brett Bader. Tensor decompositions and applications. SIREV  51(3):455–

500  2009.

[6] G.H. Golub and C.F. Van Loan. Matrix Computations. The Johns Hopkins University Press 

Baltimore  Maryland  2012.

[7] XuanLong Nguyen. Posterior contraction of the population polytope in ﬁnite admixture mod-

els. arXiv preprint arXiv:1206.0068  2012.

[8] T. Austin et al. On exchangeable random variables and the statistics of large graphs and hyper-

graphs. Probab. Surv  5:80–145  2008.

[9] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor Methods for Learning
Latent Variable Models. Under Review. J. of Machine Learning. Available at arXiv:1210.7559 
Oct. 2012.

[10] Elizabeth S. Allman  John A. Rhodes  and Amelia Taylor. A semialgebraic description of the

general markov model on phylogenetic trees. Arxiv preprint arXiv:1212.1200  Dec. 2012.

[11] J.B. Kruskal. Three-way arrays: Rank and uniqueness of trilinear decompositions  with appli-
cation to arithmetic complexity and statistics. Linear algebra and its applications  18(2):95–
138  1977.

[12] A. Anandkumar  D. Hsu  M. Janzamin  and S. Kakade. When are Overcomplete Topic Models
Identiﬁable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. Preprint
available on arXiv:1308.2853  Aug. 2013.

[13] A. Anandkumar  D. Hsu  A. Javanmard  and S. M. Kakade. Learning Linear Bayesian Net-

works with Latent Variables. ArXiv e-prints  September 2012.

[14] Daniel A. Spielman  Huan Wang  and John Wright. Exact recovery of sparsely-used dictionar-

ies. ArxXiv preprint  abs/1206.5882  2012.

[15] L. De Lathauwer  J. Castaing  and J.-F Cardoso. Fourth-order cumulant-based blind identiﬁ-
cation of underdetermined mixtures. IEEE Tran. on Signal Processing  55:2965–2973  June
2007.

9

,Anima Anandkumar
Daniel Hsu
Majid Janzamin
Sham Kakade
Mahito Sugiyama
Karsten Borgwardt