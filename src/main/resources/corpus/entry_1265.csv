2019,DAC: The Double Actor-Critic Architecture for Learning Options,We reformulate the option framework as two parallel augmented MDPs. Under this novel formulation  all policy optimization algorithms can be used off the shelf to learn intra-option policies  option termination conditions  and a master policy over options. We apply an actor-critic algorithm on each augmented MDP  yielding the Double Actor-Critic (DAC) architecture. Furthermore  we show that  when state-value functions are used as critics  one critic can be expressed in terms of the other  and hence only one critic is necessary. We conduct an empirical study on challenging robot simulation tasks. In a transfer learning setting  DAC outperforms both its hierarchy-free counterpart and previous gradient-based option learning algorithms.,DAC: The Double Actor-Critic

Architecture for Learning Options

Shangtong Zhang  Shimon Whiteson

Department of Computer Science

{shangtong.zhang  shimon.whiteson}@cs.ox.ac.uk

University of Oxford

Abstract

We reformulate the option framework as two parallel augmented MDPs. Under
this novel formulation  all policy optimization algorithms can be used off the shelf
to learn intra-option policies  option termination conditions  and a master policy
over options. We apply an actor-critic algorithm on each augmented MDP  yielding
the Double Actor-Critic (DAC) architecture. Furthermore  we show that  when
state-value functions are used as critics  one critic can be expressed in terms of the
other  and hence only one critic is necessary. We conduct an empirical study on
challenging robot simulation tasks. In a transfer learning setting  DAC outperforms
both its hierarchy-free counterpart and previous gradient-based option learning
algorithms.

1

Introduction

Temporal abstraction (i.e.  hierarchy) is a key component in reinforcement learning (RL). A good
temporal abstraction usually improves exploration (Machado et al.  2017b) and enhances the inter-
pretability of agents’ behavior (Smith et al.  2018). The option framework (Sutton et al.  1999)  which
is commonly used to formulate temporal abstraction  gives rise to two problems: learning options
(i.e.  temporally extended actions) and learning a master policy (i.e.  a policy over options  a.k.a. an
inter-option policy).
A Markov Decision Process (MDP  Puterman 2014) with options can be interpreted as a Semi-MDP
(SMDP  Puterman 2014)  and a master policy is used in this SMDP for option selection. While
in principle  any SMDP algorithm can be used to learn a master policy  such algorithms are data
inefﬁcient as they cannot update a master policy during option execution. To address this issue  Sutton
et al. (1999) propose intra-option algorithms  which can update a master policy at every time step
during option execution. Intra-option Q-Learning (Sutton et al.  1999) is a value-based intra-option
algorithm and has enjoyed great success (Bacon et al.  2017; Riemer et al.  2018; Zhang et al.  2019b).
However  in the MDP setting  policy-based methods are often preferred to value-based ones because
they can cope better with large action spaces and enjoy better convergence properties with function
approximation. Unfortunately  theoretical study for learning a master policy with policy-based
intra-option methods is limited (Daniel et al.  2016; Bacon  2018) and its empirical success has not
been witnessed. This is the ﬁrst issue we address in this paper.
Recently  gradient-based option learning algorithms have enjoyed great success (Levy and Shimkin 
2011; Bacon et al.  2017; Smith et al.  2018; Riemer et al.  2018; Zhang et al.  2019b). However 
most require algorithms that are customized to the option-based SMDP. Consequently  we cannot
directly leverage recent advances in gradient-based policy optimization from MDPs (e.g.  Schulman
et al. 2015  2017; Haarnoja et al. 2018). This is the second issue we address in this paper.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

To address these issues  we reformulate the SMDP of the option framework as two augmented
MDPs. Under this novel formulation  all policy optimization algorithms can be used for option
learning and master policy learning off the shelf and the learning remains intra-option. We apply an
actor-critic algorithm on each augmented MDP  yielding the Double Actor-Critic (DAC) architecture.
Furthermore  we show that  when state-value functions are used as critics  one critic can be expressed
in terms of the other  and hence only one critic is necessary. Finally  we empirically study the
combination of DAC and Proximal Policy Optimization (PPO  Schulman et al. 2017) in challenging
robot simulation tasks. In a transfer learning setting  DAC+PPO outperforms both its hierarchy-free
counterpart  PPO  and previous gradient-based option learning algorithms.

2 Background
We consider an MDP consisting of a state space S  an action space A  a reward function r : S ×A →
R  a transition kernel p : S × S × A → [0  1]  an initial distribution p0 : S → [0  1] and a discount
factor γ ∈ [0  1). We refer to this MDP as M
= (S A  r  p  p0  γ) and consider episodic tasks. In the
.
option framework (Sutton et al.  1999)  an option o is a triple of (Io  πo  βo)  where Io is an initiation
set indicating where the option can be initiated  πo : A × S → [0  1] is an intra-option policy  and
βo : S → [0  1] is a termination function. In this paper  we consider Io ≡ S following Bacon et al.
(2017); Smith et al. (2018). We use O to denote the option set and assume all options are Markov.
We use π : O × S → [0  1] to denote a master policy and consider the call-and-return execution
model (Sutton et al.  1999). Time-indexed capital letters are random variables. At time step t  an
agent at state St either terminates the previous option Ot−1 w.p. βOt−1(St) and initiates a new option
Ot according to π(·|St)  or proceeds with the previous option Ot−1 w.p. 1 − βOt−1 (St) and sets
= Ot−1. Then an action At is selected according to πOt(·|St). The agent gets a reward Rt+1
.
Ot
satisfying E[Rt+1] = r(St  At) and proceeds to a new state St+1 according to p(·|St  At). Under
this execution model  we have

(cid:88)

p(St+1|St  Ot) =
p(Ot|St  Ot−1) = (1 − βOt−1 (St))IOt−1=Ot + βOt−1(St)π(Ot|St) 

πOt(a|St)p(St+1|St  a) 

a

qπ(s  o  a)

.

(s  o) and an action a  we deﬁne

p(St+1  Ot+1|St  Ot) = p(St+1|St  Ot)p(Ot+1|St+1  Ot) 

= Eπ O p r[(cid:80)∞

i=1 γi−1Rt+i | St = s  Ot = o  At = a].

where I is the indicator function. With a slight abuse of notations  we deﬁne r(s  o)

(cid:80)
.
=
a πo(s  a)r(s  a). The MDP M and the options O form an SMDP. For each state-option pair
The state-option value of π on the SMDP is qπ(s  o) =(cid:80)
on the SMDP is vπ(s) =(cid:80)
qπ(s  o) = r(s  o) + γ(cid:80)

a πo(a|s)qπ(s  o  a). The state value of π
uπ(o  s(cid:48)) = [1 − βo(s(cid:48))]qπ(s(cid:48)  o) + βo(s(cid:48))vπ(s(cid:48)) 
where uπ(o  s(cid:48)) is the option-value upon arrival (Sutton et al.  1999). Correspondingly  we have the
optimal master policy π∗ satisfying vπ∗ (s) ≥ vπ(s)∀(s  π). We use q∗ to denote the state-option
value function of π∗.
Master Policy Learning: To learn the optimal master policy π∗ given a ﬁxed O  one value-based
approach is to learn q∗ ﬁrst and derive π∗ from q∗. We can use SMDP Q-Learning to update an
estimate Q for q∗ as

o π(o|s)qπ(s  o). They are related as
s(cid:48) p(s(cid:48)|s  o)uπ(o  s(cid:48)) 

i=t γi−tRi+1 + γk−t maxo Q(Sk  o) − Q(St  Ot)(cid:1) 

Q(St  Ot) ← Q(St  Ot) + α(cid:0)(cid:80)k

where we assume the option Ot initiates at time t and terminates at time k (Sutton et al.  1999). Here
the option Ot lasts k−t steps. However  SMDP Q-Learning performs only one single update  yielding
signiﬁcant data inefﬁciency. This is because SMDP algorithms simply interpret the option-based
SMDP as a generic SMDP  ignoring the presence of options. By contrast  Sutton et al. (1999) propose
to exploit the fact that the SMDP is generated by options  yielding an update rule:
Q(St  Ot) ← Q(St  Ot) + α[Rt+1 + γU (Ot  St+1) − Q(St  Ot)]

=(cid:0)1 − βOt(St+1)(cid:1)Q(St+1  Ot) + βOt(St+1) max

U (Ot  St+1)

Q(St+1  o).

(1)

.

o

2

This update rule is efﬁcient in that it updates Q every time step. However  it is still inefﬁcient in that it
only updates Q for the executed option Ot. We refer to this property as on-option. Sutton et al. (1999)
further propose Intra-option Q-Learning  where the update (1) is applied to every option o satisfying
πo(At|St) > 0. We refer to this property as off-option. Intra-option Q-Learning is theoretically
justiﬁed only when all intra-option policies are deterministic (Sutton et al.  1999). The convergence
analysis of Intra-option Q-Learning with stochastic intra-option policies remains an open problem
(Sutton et al.  1999). The update (1) and the Intra-option Q-Learning can also be applied to off-policy
transitions.
The Option-Critic Architecture: Bacon et al. (2017) propose a gradient-based option learning
algorithm  the Option-Critic (OC) architecture. Assuming {πo}o∈O is parameterized by ν and
{βo}o∈O is parameterized by φ  Bacon et al. (2017) prove that

∇νvπ(S0) =

(cid:88)
∇φvπ(S0) = −(cid:88)

s o

s(cid:48) o

(cid:88)

ρ(s  o|S0  O0)

qπ(s  o  a)∇νπo(a|s) 

ρ(s(cid:48)  o|S1  O0)(cid:0)qπ(s(cid:48)  o) − vπ(s(cid:48))(cid:1)∇φβo(s(cid:48)) 

a

.

.

where ρ deﬁned in Bacon et al. (2017) is the unnormalized discounted state-option pair occupancy
measure. OC is on-option in that given a transition (St  Ot  At  Rt+1  St+1)  it updates only parame-
ters of the executed option Ot. OC provides the gradient for {πo  βo} and can be combined with any
master policy learning algorithm. In particular  Bacon et al. (2017) combine OC with (1). Hence  in
this paper  we use OC to indicate this exact combination. OC has also been extended to multi-level
options (Riemer et al.  2018) and deterministic intra-option policies (Zhang et al.  2019b).
= {θ  ν  φ}.
.
Inferred Option Policy Gradient: We assume π is parameterized by θ and deﬁne ξ
= (S0  A0  S1  A1  . . .   ST ) to denote a trajectory from {π O  M}  where ST is a terminal
.
We use τ
t=1 γt−1Rt | τ ] to denote the total expected discounted rewards along
state. We use r(τ )
τ. Our goal is to maximize J
along the trajectory as latent variables and marginalize over them when computing ∇ξJ. In the
Inferred Option Policy Gradient (IOPG)  Smith et al. (2018) show

=(cid:82) r(τ )p(τ )dτ. Smith et al. (2018) propose to interpret the options
o mt(o)πo(At|St)(cid:1)] 

= E[(cid:80)T
∇ξJ = Eτ [r(τ )(cid:80)T
t=0 ∇ξ log p(At|Ht)] = Eτ [r(τ )(cid:80)T

t=0 ∇ξ log(cid:0)(cid:80)

.
= (S0  A0  . . .   St−1  At−1  St) is the state-action history and mt(o)

= p(Ot = o|Ht)
.
where Ht
is the probability of occupying an option o at time t. Smith et al. (2018) further show that mt can
be expressed recursively via (mt−1 {πo  βo}  π)  allowing efﬁcient computation of ∇ξJ. IOPG
is an off-line algorithm in that it has to wait for a complete trajectory before computing ∇ξJ. To
admit online updates  Smith et al. (2018) propose to store ∇ξmt−1 at each time step and use the
stored ∇ξmt−1 for computing ∇ξmt  yielding the Inferred Option Actor Critic (IOAC). IOAC is
biased in that a stale approximation of ∇ξmt−1 is used for computing ∇ξmt. The longer a trajectory
is  the more biased the IOAC gradient is. IOPG and IOAC are off-option in that given a transition
(St  Ot  At  Rt+1  St+1)  all options contribute to the gradient explicitly.
Augmented Hierarchical Policy: Levy and Shimkin (2011) propose the Augmented Hierarchical
Policy (AHP) architecture. AHP reformulates the SMDP of the option framework as an augmented
= B × O × A 
MDP. The new state space is SAHP .
where B .
= {stop  continue} indicates whether to terminate the previous option or not. All policy
optimization algorithms can be used to learn an augmented policy

= O × S. The new action space is AAHP .

πAHP(cid:0)(Bt  Ot  At)|(Ot−1  St)(cid:1) .

(cid:16)IBt=contIOt=Ot−1 + IBt=stopπ(Ot|St)
(cid:17)(cid:16)
(cid:17)
=πOt(At|St)
IBt=cont(1 − βOt−1(St)) + IBt=stopβOt−1 (St)

under this new MDP  which learns π and {πo  βo} implicitly. Here Bt ∈ B is a binary random
variable. In the formulation of πAHP  the term π(Ot|St) is gated by IBt=stop. Consequently  the
gradient for the master policy π is non-zero only when an option terminates (also see Equation 23
in Levy and Shimkin (2011)). This suggests that the master policy learning in AHP is SMDP-style.
Moreover  as suggested by the term πOt(At|St) in πAHP  the resulting gradient for an intra-option
policy πo is non-zero only when the option o is being executed (also see Equation 24 in Levy and
Shimkin (2011)). This suggests that the option learning in AHP is on-option. Similar augmented
MDP formulation is also used in Daniel et al. (2016).

3

3 Two Augmented MDPs
In this section  we reformulate the SMDP as two augmented MDPs: the high-MDP MH and the
low-MDP ML. The agent makes high-level decisions (i.e.  option selection) in MH according to
π {βo} and thus optimizes π {βo}. The agent makes low-level decisions (i.e.  action selection) in
ML according to {πo} and thus optimizes {πo}. Both augmented MDPs share the same samples
with the SMDP {O  M}.
We ﬁrst deﬁne a dummy option # and O+ .
= O ∪ {#}. This dummy option is only for simplifying
notations and is never executed. In the high-MDP  we interpret a state-option pair in the SMDP as a
new state and an option in the SMDP as a new action. Formally speaking  we deﬁne
= O+ × S  AH .
= I

= pH(cid:0)(Ot  St+1)|(Ot−1  St)  AH
t )(cid:1) .
= rH(cid:0)(Ot−1  St)  Ot

= {SH AH  pH  pH
MH .
t   AH
.
t )
= p0(S0)IO−1=# 

= O 
(cid:1) .
t =Otp(St+1|St  Ot) 
AH

(cid:0)(O−1  S0)(cid:1) .

0   rH  γ}  SH .

pH(SH
= pH
.

t+1|SH

= r(St  Ot)

rH(SH

t   AH
t )

0

.

0 (SH
pH
0 )
We deﬁne a Markov policy πH on MH as
= πH(Ot|(Ot−1  St))
.

πH(AH

t |SH
t )

= p(Ot|St  Ot−1)IOt−1(cid:54)=# + π(St  Ot)IOt−1=#
.

In the low-MDP  we interpret a state-option pair in the SMDP as a new state and leave the action
space unchanged. Formally speaking  we deﬁne

= pL(cid:0)(St+1  Ot+1)|(St  Ot)  At

(cid:1) .
0   rL  γ}  SL .

= {SL AL  pL  pL
ML .
t+1|SL
pL(SL
t   AL
.
t )
pL
0 (SL
.
0 )

= pL(cid:0)(S0  O0)(cid:1) .

= p0(S0)π(S0  O0) 

= A 

= S × O  AL .
= p(St+1|St  At)p(Ot+1|St+1  Ot) 
rL(SL

= rL(cid:0)(St  Ot)  At

(cid:1) .

t   AL
t )

.

= r(St  At)

We deﬁne a Markov policy πL on ML as
.

πL(AL

t |SL
t )

= πL(cid:0)At|(St  Ot)(cid:1) .

= πOt(At|St)

t

1   AH

.
= Ot  O−1

1   . . .   SH

T )  where SH

= (Ot−1  St)  AH
.

= {τL | p(τL|πL  ML) > 0}. With τ

= {τH | p(τH|πH  MH) > 0}  ΩL .
0   SH

= {τ | p(τ|π O  M ) > 0} 
.
We consider trajectories with nonzero probabilities and deﬁne Ω
ΩH .
.
=
(S0  O0  S1  O1  . . .   ST )  we deﬁne a function fH : Ω → ΩH  which maps τ to τH .
=
(SH
0   AH
Lemma 1 p(τ|π O  M ) = p(τH|πH  MH)  r(τ ) = r(τH)  and fH is a bijection.
Proof. See supplementary materials.
.
We now take action into consideration. With τ
= (S0  O0  A0  S1  O1  A1 . . .   ST )  we deﬁne
a function fL : Ω → ΩL  which maps τ to τL .
= (SL
.
=
(St  Ot)  AL
Lemma 2 p(τ|π O  M ) = p(τL|πL  ML)  r(τ ) = r(τL)  and fL is a bijection.
Proof. See supplementary materials.

T )  where SL

.
= At. We have:

.
= #. We have:

1   . . .   SL

0   AL

1   AL

0   SL

t

t

Proposition 1

=(cid:82) r(τ )p(τ|π O  M )dτ =(cid:82) r(τH)p(τH|πH  MH)dτH =(cid:82) r(τL)p(τL|πL  ML)dτL.

.

J

t

Proof. Follows directly from Lemma 1 and Lemma 2.
Lemma 1 and Lemma 2 indicate that sampling from {π O  M} is equivalent to sampling from
{πH  MH} and {πL  ML}. Proposition 1 indicates that optimizing π O in M is equivalent to
optimizing πH in MH and optimizing πL in ML. We now make two observations:
Observation 1 MH depends on {πo} while πH depends on π and {βo}.
Observation 2 ML depends on π {βo} while πL depends on {πo}.

4

Observation 1 suggests that when we keep the intra-option policies {πo} ﬁxed and optimize πH 
we are implicitly optimizing π and {βo} (i.e.  θ and φ). Observation 2 suggests that when we keep
the master policy π and the termination conditions {βo} ﬁxed and optimize πL  we are implicitly
optimizing {πo} (i.e.  ν). All policy optimization algorithms for MDPs can be used off the shelf
to optimize the two actors πH and πL with samples from {π O  M}  yielding a new family of
algorithms for master policy learning and option learning  which we refer to as the Double Actor-
Critic (DAC) architecture. Theoretically  we should optimize πH and πL alternatively with different
samples to make sure MH and ML are stationary. In practice  optimizing πH and πL with the same
samples improves data efﬁciency. The pseudocode of DAC is provided in the supplementary materials.
We present a thorough comparison of DAC  OC  IOPG and AHP in Table 1. DAC combines the
advantages of both AHP (i.e.  compatibility) and OC (intra-option learning). Enabling off-option
learning of intra-option policies in DAC as IOPG is a possibility for future work.

Learning {πo  βo} Online Learning Compatibility

Learning π

SMDP

AHP
intra-option
OC
IOPG intra-option
DAC
intra-option

on-option
on-option
off-option
on-option

yes
yes
no
yes

yes
no
no
yes

Table 1: A comparison of AHP  OC  IOPG and DAC. (1) For learning {πo  βo}  all four are intra-
option. (2) IOAC is online with bias introduced and consumes extra memory. (3) Compatibility
indicates whether a framework can be combined with any policy optimization algorithm off-the-shelf.

In general  we need two critics in DAC  which can be learned via all policy evaluation algorithms.
However  when state value functions are used as critics  Proposition 2 shows that the state value
function in the high-MDP (vπH) can be expressed by the state value function in the low-MDP (vπL) 
and hence only one critic is needed.

Proposition 2 vπH(cid:0)(o  s(cid:48))(cid:1) =(cid:80)
vπH(cid:0)(o  s(cid:48))(cid:1) .
vπL(cid:0)(s(cid:48)  o(cid:48))(cid:1) .

o(cid:48) πH(cid:0)o(cid:48)|(o  s(cid:48))(cid:1)vπL(cid:0)(s(cid:48)  o(cid:48))(cid:1)  where
= EπH MH[(cid:80)∞
= EπL ML [(cid:80)∞

i=1 γi−1RH
i=1 γi−1RL

t+i | SH
t+i | SL

t = (o  s(cid:48))] 
t = (s(cid:48)  o(cid:48))] 

Proof. See supplementary materials.
Our two augmented MDP formulation differs from the one augmented MDP formulation in AHP
mainly in that we do not need to introduce the binary variable Bt. It is this elimination of Bt that leads
to the intra-option master policy learning in DAC and yields a useful observation: the call-and-return
execution model (with Markov options) is similar to the polling execution model (Dietterich  2000) 
where an agent reselects an option every time step according to πH. This observation opens the
possibility for more intra-option master policy learning algorithms. Note the introduction of Bt is
necessary if one would want to formulate the option SMDP as a single augmented MDP and apply
standard control methods from the MDP setting. Otherwise  the augmented MDP will have πH in
both the augmented policy and the new transition kernel. By contrast  in a canonical MDP setting  a
policy does not overlap with the transition kernel.
Beyond Intra-option Q-Learning: In terms of learning π with a ﬁxed O  Observation 1 suggests
we optimize πH on MH. This immediately yields a family of policy-based algorithms for learning a
master policy  all of which are intra-option. Particularly  when we use Off-Policy Expected Policy
Gradients (Off-EPG  Ciosek and Whiteson 2017) for optimizing πH  we get all the merits of both
Intra-option Q-Learning and policy gradients for free. (1) By deﬁnition of MH and πH  Off-EPG
optimizes π in an intra-option manner and is as data efﬁcient as Intra-option Q-Learning. (2) Off-EPG
is an off-policy algorithm  so off-policy transitions can also be used  as in Intra-option Q-Learning.
(3) Off-EPG is off-option in that all the options  not only the executed one  explicitly contribute to the
policy gradient at every time step. Particularly  this off-option approach does not require deterministic
intra-option policies like Intra-option Q-Learning. (4) Off-EPG uses a policy for decision making 
which is more robust than value-based decision making. We leave an empirical study of this particular
application for future work and focus in this paper on the more general problem  learning π and O
simultaneously. When O is not ﬁxed  the MDP (MH) for learning π becomes non-stationary. We 
therefore  prefer on-policy methods to off-policy methods.

5

4 Experimental Results

We design experiments to answer the following questions: (1) Can DAC outperform existing gradient-
based option learning algorithms (e.g.  AHP  OC  IOPG)? (2) Can options learned in DAC translate
into a performance boost over its hierarchy-free counterparts? (3) What options does DAC learn?
DAC can be combined with any policy optimization algorithm  e.g.  policy gradient (Sutton et al. 
2000)  Natural Actor Critic (NAC  Peters and Schaal 2008)  PPO  Soft Actor Critic (Haarnoja et al. 
2018)  Generalized Off-Policy Actor Critic (Zhang et al.  2019a). In this paper  we focus on the
combination of DAC and PPO  given the great empirical success of PPO (OpenAI  2018). Our PPO
implementation uses the same architecture and hyperparameters reported by Schulman et al. (2017).
Levy and Shimkin (2011) combine AHP with NAC and present an empirical study on an inverted
pendulum domain. In our experiments  we also combine AHP with PPO for a fair comparison. To
the best of our knowledge  this is the ﬁrst time that AHP has been evaluated with state-of-the-art
policy optimization algorithms in prevailing deep RL benchmarks. We also implemented IOPG and
OC as baselines. Previously  Klissarov et al. (2017) also combines OC and PPO in PPOC. PPOC
updates {πo} with a PPO loss and updates {βo} in the same manner as OC. PPOC applies vanilla
policy gradients directly to train π in an intra-option manner  which is not theoretically justiﬁed. We
use 4 options for all algorithms  following Smith et al. (2018). We report the online training episode
return  smoothed by a sliding window of size 20. All curves are averaged over 10 independent runs
and shaded regions indicate standard errors. All implementations are made publicly available 1. More
details about the experiments are provided in the supplementary materials.

4.1 Single Task Learning

We consider four robot simulation tasks used by Smith et al. (2018) from OpenAI gym (Brockman
et al.  2016). We also include the combination of DAC and A2C (Clemente et al.  2017) for reference.
The results are reported in Figure 1.

Figure 1: Online performance on a single task

Results: (1) Our implementations of OC and IOPG reach similar performance to that reported by
Smith et al. (2018)  which is signiﬁcantly outperformed by both vanilla PPO and option-based PPO
(i.e.  DAC+PPO  AHP+PPO). However  the performance of DAC+A2C is similar to OC and IOPG.
These results indicate that the performance boost of DAC+PPO and AHP+PPO mainly comes from
the more advanced policy optimization algorithm (PPO). This is exactly the major advantage of
DAC and AHP. They allow all state-of-the-art policy optimization algorithms to be used off the shelf
to learn options. (2) The performance of DAC+PPO is similar to vanilla PPO in 3 out of 4 tasks.
DAC+PPO outperforms PPO in Swimmer by a large margin. This performance similarity between an
option-based algorithm and a hierarchy-free algorithm is expected and is also reported by Harb et al.
(2018); Smith et al. (2018); Klissarov et al. (2017). Within a single task  it is usually hard to translate
the automatically discovered options into a performance boost  as primitive actions are enough to
express the optimal policy and learning the additional structure  the options  may be overhead. (3)
The performance of DAC+PPO is similar to AHP+PPO  as expected. The main advantage of DAC
over AHP is its data efﬁciency in learning the master policy. Within a single task  it is possible that
an agent focuses on a “mighty” option and ignores other specialized options  making master policy
learning less important. By contrast  when we switch tasks  cooperation among different options

1https://github.com/ShangtongZhang/DeepRL

6

becomes more important. We  therefore  expect that the data efﬁciency in learning the master policy
in DAC translates into a performance boost over AHP in a transfer learning setting.

4.2 Transfer Learning

We consider a transfer learning setting where after the ﬁrst 1M training steps  we switch to a new task
and train the agent for other 1M steps. The agent is not aware of the task switch. The two tasks are
correlated and we expect learned options from the ﬁrst task can be used to accelerate learning of the
second task.
We use 6 pairs of tasks from DeepMind Control Suite (DMControl  Tassa et al. 2018): CartPole =
(balance  balance_sparse)  Reacher = (easy  hard)  Cheetah = (run  backward)  Fish =
(upright  downleft)  Walker1 = (squat  stand)  Walker2 = (walk  backward). Most of them
are provided by DMControl and some of them we constructed similarly as Hafner et al. (2018). The
maximum score is always 1000. More details are provided in the supplementary materials. There are
other possible paired tasks in DMControl but we found that in such pairs  PPO hardly learns anything
in the second task. Hence  we omit those pairs from our experiments. The results are reported in
Figure 2.

Figure 2: Online performance for transfer learning

Results: (1) During the ﬁrst task  DAC+PPO consistently outperforms OC and IOPG by a large
margin and maintains a similar performance to PPO  PPOC  and AHP+PPO. These results are
consistent with our previous observations in the single task learning setting. (2) After the task switch 
the advantage of DAC+PPO becomes clear. DAC+PPO outperforms all other baselines by a large
margin in 3 out of 6 tasks and is among the best algorithms in the other 3 tasks. This satisﬁes our
previous expectation about DAC and AHP in Section 4.1. (3) We further study the inﬂuence of the
number of options in Walker2. Results are provided in the supplementary materials. We ﬁnd 8
options are slightly better than 4 options and 2 options are worse. We conjecture that 2 options are
not enough for transferring the knowledge from the ﬁrst task to the second.

4.3 Option Structures

We visualize the learned options and option occupancy of DAC+PPO on Cheetah in Figure 3. There
are 4 options in total  displayed via different colors. The upper strip shows the option occupancy
during an episode at the end of the training of the ﬁrst task (run). The lower strip shows the option
occupancy during an episode at the end of the training of the second task (backward). Both episodes
last 1000 steps.2 The four options are distinct. The blue option is mainly used when the cheetah is

2The video of the two episodes is available at https://youtu.be/K0ZP-HQtx6M

7

“ﬂying”. The green option is mainly used when the cheetah pushes its left leg to move right. The
yellow option is mainly used when the cheetah pushes its left leg to move left. The red option is
mainly used when the cheetah pushes its right leg to move left. During the ﬁrst task  the red option
is rarely used. The cheetah uses the green and yellow options for pushing its left leg and uses the
blue option for ﬂying. The right leg rarely touches the ground during the ﬁrst episode. After the task
switch  the ﬂying option (blue) transfers to the second task  the yellow option specializes for moving
left  and the red option is developed for pushing the right leg to the left.

Figure 3: Learned options and option occupancy of DAC+PPO in Cheetah

5 Related Work

Many components in DAC are not new. The idea of an augmented MDP is suggested by Levy and
Shimkin (2011); Daniel et al. (2016). The augmented state spaces SH and SL are also used by
Bacon et al. (2017) to simplify the derivation. Applying vanilla policy gradient to πL and ML leads
immediately to the Intra-Option Policy Gradient Theorem (Bacon et al.  2017). The augmented policy
πH is also used by Smith et al. (2018) to simplify the derivation and is discussed in Bacon (2018)
under the name of mixture distribution. Bacon (2018) discusses two mechanisms for sampling from
the mixture distribution: a two-step sampling method (sampling Bt ﬁrst then Ot) and a one-step
sampling method (sampling Ot directly)  where the latter can be viewed as an expected version of
the former. The two-step one is implemented by the call-and-return model and is explicitly modelled
by Levy and Shimkin (2011) via introducing Bt  which is not used in either Bacon (2018) or our
work. Bacon (2018) mentions that the one-step modelling can lead to reduced variance compared to
the two-step one. However  there is another signiﬁcant difference: the one-step modelling is more
data efﬁcient than the two-step one. The two-step one (e.g.  AHP) yields SMDP learning while the
one-step one (e.g.  our approach) yields intra-option learning (for learning the master policy). This
difference is not recognized in Bacon (2018) and we are the ﬁrst to establish it  both conceptually
and experimentally. Although the underlying chain of DAC is the same as that of Levy and Shimkin
(2011); Daniel et al. (2016); Bacon et al. (2017); Bacon (2018)  DAC is the ﬁrst to formulate the two
augmented MDPs explicitly. It is this explicit formulation that allows the off-the-shelf application of
all state-of-the-art policy optimizations algorithm and combines advantages from both OC and AHP.
The gradient of the master policy ﬁrst appeared in Levy and Shimkin (2011). However  due to the
introduction of Bt  that gradient is nonzero only if an option terminates. It is  therefore  SMDP-
learning. The gradient of the master policy later appeared in Daniel et al. (2016) in the probabilistic
inference method for learning options  which  however  assumes a linear structure and is off-line
learning. The gradient of the master policy also appeared in Bacon (2018)  which is mixed with all
other gradients. Unless we work on the augmented MDP directly  we cannot easily drop in other
policy optimization techniques for learning the master policy  which is our main contribution and is
not done by Bacon (2018). Furthermore  that policy gradient is never used in Bacon (2018). All the
empirical study uses Q-Learning for the master policy. By contrast  our explicit formulation of the
two augmented MDPs generates a family of online policy-based intra-option algorithms for master
policy learning  which are compatible with general function approximation.
Besides gradient-based option learning  there are also other option learning approaches based on
ﬁnding bottleneck states or subgoals (Stolle and Precup  2002; McGovern and Barto  2001; Silver
and Ciosek  2012; Niekum and Barto  2011; Machado et al.  2017a). In general  these approaches are
expensive in terms of both samples and computation (Precup  2018).

8

Besides the option framework  there are also other frameworks to describe hierarchies in RL. Di-
etterich (2000) decomposes the value function in the original MDP into value functions in smaller
MDPs in the MAXQ framework. Dayan and Hinton (1993) employ multiple managers on different
levels for describing a hierarchy. Vezhnevets et al. (2017) further extend this idea to FeUdal Networks 
where a manager module sets abstract goals for workers. This goal-based hierarchy description is
also explored by Schmidhuber and Wahnsiedler (1993); Levy et al. (2017); Nachum et al. (2018).
Moreover  Florensa et al. (2017) use stochastic neural networks for hierarchical RL. We leave a
comparison between the option framework and other hierarchical RL frameworks for future work.

6 Conclusions

In this paper  we reformulate the SMDP of the option framework as two augmented MDPs  allowing
in an off-the-shelf application of all policy optimization algorithms in option learning and master
policy learning in an intra-option manner.
In DAC  there is no clear boundary between option termination functions and the master policy. They
are different internal parts of the augmented policy πH. We observe that the termination probability
of the active option becomes high as training progresses  although πH still selects the same option.
This is also observed by Bacon et al. (2017). To encourage long options  Harb et al. (2018) propose a
cost model for option switching. Including this cost model in DAC is a possibility for future work.

Acknowledgments

SZ is generously funded by the Engineering and Physical Sciences Research Council (EPSRC). This
project has received funding from the European Research Council under the European Union’s Hori-
zon 2020 research and innovation programme (grant agreement number 637713). The experiments
were made possible by a generous equipment grant from NVIDIA. The authors thank Matthew Smith
and Gregory Farquhar for insightful discussions. The authors also thank anonymous reviewers for
their valuable feedbacks.

References
Bacon  P.-L. (2018). Temporal Representation Learning. PhD thesis  McGill University.

Bacon  P.-L.  Harb  J.  and Precup  D. (2017). The option-critic architecture. In Proceedings of the

31st AAAI Conference on Artiﬁcial Intelligence.

Brockman  G.  Cheung  V.  Pettersson  L.  Schneider  J.  Schulman  J.  Tang  J.  and Zaremba  W.

(2016). Openai gym. arXiv preprint arXiv:1606.01540.

Ciosek  K. and Whiteson  S. (2017). Expected policy gradients. arXiv preprint arXiv:1706.05374.

Clemente  A. V.  Castejón  H. N.  and Chandra  A. (2017). Efﬁcient parallel methods for deep

reinforcement learning. arXiv preprint arXiv:1705.04862.

Daniel  C.  Van Hoof  H.  Peters  J.  and Neumann  G. (2016). Probabilistic inference for determining

options in reinforcement learning. Machine Learning.

Dayan  P. and Hinton  G. E. (1993). Feudal reinforcement learning. In Advances in Neural Information

Processing Systems.

Dietterich  T. G. (2000). Hierarchical reinforcement learning with the maxq value function decompo-

sition. Journal of Artiﬁcial Intelligence Research.

Florensa  C.  Duan  Y.  and Abbeel  P. (2017). Stochastic neural networks for hierarchical reinforce-

ment learning. arXiv preprint arXiv:1704.03012.

Haarnoja  T.  Zhou  A.  Abbeel  P.  and Levine  S. (2018). Soft actor-critic: Off-policy maximum

entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290.

Hafner  D.  Lillicrap  T.  Fischer  I.  Villegas  R.  Ha  D.  Lee  H.  and Davidson  J. (2018). Learning

latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551.

9

Harb  J.  Bacon  P.-L.  Klissarov  M.  and Precup  D. (2018). When waiting is not an option: Learning
In Proceedings of the 32nd AAAI Conference on Artiﬁcial

options with a deliberation cost.
Intelligence.

Klissarov  M.  Bacon  P.-L.  Harb  J.  and Precup  D. (2017). Learnings options end-to-end for

continuous action tasks. arXiv preprint arXiv:1712.00004.

Levy  A.  Platt  R.  and Saenko  K. (2017). Hierarchical actor-critic. arXiv preprint arXiv:1712.00948.

Levy  K. Y. and Shimkin  N. (2011). Uniﬁed inter and intra options learning using policy gradient

methods. In Proceedings of the 2011 European Workshop on Reinforcement Learning.

Machado  M. C.  Bellemare  M. G.  and Bowling  M. (2017a). A laplacian framework for option

discovery in reinforcement learning. arXiv preprint arXiv:1703.00956.

Machado  M. C.  Rosenbaum  C.  Guo  X.  Liu  M.  Tesauro  G.  and Campbell  M. (2017b).
Eigenoption discovery through the deep successor representation. arXiv preprint arXiv:1710.11089.

McGovern  A. and Barto  A. G. (2001). Automatic discovery of subgoals in reinforcement learning

using diverse density. Proceedings of the 18th International Conference on Machine Learning.

Nachum  O.  Gu  S. S.  Lee  H.  and Levine  S. (2018). Data-efﬁcient hierarchical reinforcement

learning. In Advances in Neural Information Processing Systems.

Niekum  S. and Barto  A. G. (2011). Clustering via dirichlet process mixture models for portable

skill discovery. In Advances in neural information processing systems.

OpenAI (2018). Openai ﬁve. https://openai.com/five/.

Peters  J. and Schaal  S. (2008). Natural actor-critic. Neurocomputing.

Precup  D. (2018). Temporal abstraction. URL: http://videolectures.net/site/normal_dl/

tag=1199094/DLRLsummerschool2018_precup_temporal_abstraction_01.pdf.

Puterman  M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons.

Riemer  M.  Liu  M.  and Tesauro  G. (2018). Learning abstract options. In Advances in Neural

Information Processing Systems.

Schmidhuber  J. and Wahnsiedler  R. (1993). Planning simple trajectories using neural subgoal
generators. In Proceedings of the Second International Conference on Simulation of Adaptive
Behavior.

Schulman  J.  Levine  S.  Abbeel  P.  Jordan  M.  and Moritz  P. (2015). Trust region policy optimiza-

tion. In Proceedings of the 32nd International Conference on Machine Learning.

Schulman  J.  Wolski  F.  Dhariwal  P.  Radford  A.  and Klimov  O. (2017). Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347.

Silver  D. and Ciosek  K. (2012). Compositional planning using optimal option models. arXiv

preprint arXiv:1206.6473.

Smith  M.  Hoof  H.  and Pineau  J. (2018). An inference-based policy gradient method for learning

options. In Proceedings of the 35th International Conference on Machine Learning.

Stolle  M. and Precup  D. (2002). Learning options in reinforcement learning. In International

Symposium on abstraction  reformulation  and approximation.

Sutton  R. S.  McAllester  D. A.  Singh  S. P.  and Mansour  Y. (2000). Policy gradient methods
In Advances in Neural Information

for reinforcement learning with function approximation.
Processing Systems.

Sutton  R. S.  Precup  D.  and Singh  S. (1999). Between mdps and semi-mdps: A framework for

temporal abstraction in reinforcement learning. Artiﬁcial Intelligence.

10

Tassa  Y.  Doron  Y.  Muldal  A.  Erez  T.  Li  Y.  Casas  D. d. L.  Budden  D.  Abdolmaleki  A. 
Merel  J.  Lefrancq  A.  et al. (2018). Deepmind control suite. arXiv preprint arXiv:1801.00690.

Vezhnevets  A. S.  Osindero  S.  Schaul  T.  Heess  N.  Jaderberg  M.  Silver  D.  and Kavukcuoglu 
K. (2017). Feudal networks for hierarchical reinforcement learning. In Proceedings of the 34th
International Conference on Machine Learning.

Zhang  S.  Boehmer  W.  and Whiteson  S. (2019a). Generalized off-policy actor-critic. In Advances

in Neural Information Processing Systems.

Zhang  S.  Chen  H.  and Yao  H. (2019b). Ace: An actor ensemble algorithm for continuous control

with tree search. Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence.

11

,Yasin Abbasi Yadkori
Peter Bartlett
Varun Kanade
Yevgeny Seldin
Csaba Szepesvari
Shangtong Zhang
Shimon Whiteson