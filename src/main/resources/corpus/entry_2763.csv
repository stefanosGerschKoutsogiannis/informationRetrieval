2012,Kernel Latent SVM for Visual Recognition,Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However  a limitation of LSVMs is that they rely on linear models. For many computer vision tasks  linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper  we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.,Kernel Latent SVM for Visual Recognition

Weilong Yang

School of Computing Science

Simon Fraser University

wya16@sfu.ca

Yang Wang

Department of Computer Science

University of Manitoba

ywang@cs.umanitoba.ca

Arash Vahdat

School of Computing Science

Simon Fraser University

avahdat@sfu.ca

Greg Mori

School of Computing Science

Simon Fraser University

mori@cs.sfu.ca

Abstract

Latent SVMs (LSVMs) are a class of powerful tools that have been successfully
applied to many applications in computer vision. However  a limitation of LSVMs
is that they rely on linear models. For many computer vision tasks  linear mod-
els are suboptimal and nonlinear models learned with kernels typically perform
much better. Therefore it is desirable to develop the kernel version of LSVM. In
this paper  we propose kernel latent SVM (KLSVM) – a new learning framework
that combines latent SVMs and kernel methods. We develop an iterative train-
ing algorithm to learn the model parameters. We demonstrate the effectiveness of
KLSVM using three different applications in visual recognition. Our KLSVM for-
mulation is very general and can be applied to solve a wide range of applications
in computer vision and machine learning.

1

Introduction

We consider the problem of learning discriminative classiﬁcation models for visual recognition. In
particular  we are interested in models that have the following two characteristics: 1) can be used on
weakly labeled data; 2) have nonlinear decision boundaries.
Linear classiﬁers are a class of popular learning methods in computer vision. In the case of binary
classiﬁcation  they are prediction models in the form of f (x) = w(cid:62)x  where x is the feature vector 
and w is a vector of model parameters1. The classiﬁcation decision is based on the value of f (x).
Linear classiﬁers are amenable to efﬁcient and scalable learning/inference – an important factor in
many computer vision applications that involve high dimension features and large datasets. The
person detection algorithm in [2] is an example of the success of linear classiﬁers in computer
vision. The detector is trained by learning a linear support vector machine based on HOG descriptors
of positive and negative examples. The model parameter w in this detector can be thought as a
statistical template for HOG descriptors of persons.
The reliance on a rigid template w is a major limitation of linear classiﬁers. As a result  the learned
models usually cannot effectively capture all the variations (shape  appearance  pose  etc.) in natural
images. For example  the detector in [2] usually only works well when a person is in an upright
posture.
In the literature  there are two main approaches for addressing this limitation. The ﬁrst one is to
introduce latent variables into the linear model. In computer vision  this is best exempliﬁed by the
success of deformable part models (DPM) [5] for object detection. DPM captures shape and pose
variations of an object class with a root template covering the whole object and several part tem-
plates. By allowing these parts to deform from their ideal locations with respect to the root template 
DPM provides more ﬂexibility than a rigid template. Learning a DPM involves solving a latent

1Without loss of generality  we assume linear models without the bias term.

1

SVM (LSVM) [5  17] – an extension of regular linear SVM for handling latent variables. LSVM
provides a general framework for handling “weakly labeled data” arising in many applications. For
example  in object detection  the training data are weakly labeled because we are only given the
bounding boxes of the objects without the detailed annotation for each part. In addition to modeling
part deformation  another popular application of LSVM is to use it as a mixture model where the
mixture component is represented as a latent variable [5  6  16].
The other main approach is to directly learn a nonlinear classiﬁer. The kernel method [1] is a
representative example along this line of work. A limitation of kernel methods is that the learning is
more expensive than linear classiﬁers on large datasets  although efﬁcient algorithms exist for certain
types of kernels (e.g. histogram intersection kernel (HIK) [10]). One possible way to address the
computational issue is to use nonlinear mapping to convert the original feature into some higher
dimensional space  then apply linear classiﬁers in the high dimensional space [14].
Latent SVM and kernel methods represent two different  yet complementary approaches for learn-
ing classiﬁcation models that are more expressive than linear classiﬁers. They both have their own
advantages and limitations. The advantage of LSVM is that it provides a general and elegant formu-
lation for dealing with many weakly supervised problems in computer vision. The latent variables
in LSVM can often have some intuitive and semantic meanings. As a result  it is usually easy to
adapt LSVM to capture various prior knowledge about the unobserved variables in various appli-
cations. Examples of latent variables in the literature include part locations in object detection [5] 
subcategories in video annotation [16]  object localization in image classiﬁcation [8]  etc. However 
LSVM is essentially a parametric model. So the capacity of these types of models is limited by the
parametric form. In contrast  kernel methods are non-parametric models. The model complexity is
implicitly determined by the number of support vectors. Since the number of support vectors can
vary depending on the training data  kernel methods can adapt their model complexity to ﬁt the data.
In this paper  we propose kernel latent SVM (KLSVM) – a new learning framework that combines
latent SVMs and kernel methods. As a result  KLSVM has the beneﬁts of both approaches. On
one hand  the latent variables in KLSVM can be something intuitive and semantically meaningful.
On the other hand  KLSVM is nonparametric in nature  since the decision boundary is deﬁned
implicitly by support vectors. We demonstrate KLSVM on three applications in visual recognition:
1) object classiﬁcation with latent localization; 2) object classiﬁcation with latent subcategories; 3)
recognition of object interactions.
2 Preliminaries
In this section  we introduce some background on latent SVM and on the dual form of SVMs used
for deriving kernel SVMs. Our proposed model in Sec. 3 will build upon these two ideas.
Latent SVM: We assume a data instance is in the form of (x  h  y)  where x is the observed variable
and y is the class label. Each instance is also associated with a latent variable h that captures some
unobserved information about the data. For example  say we want to learn a “car” model from a
set of positive images containing cars and a set of negative images without cars. We know there is
a car somewhere in a positive image  but we do not know its exact location. In this case  h can be
used to represent the unobserved location of the car in the image. In this paper  we consider binary
classiﬁcation for simplicity  i.e. y ∈ {+1 −1}. Multi-class classiﬁcation can be easily converted
to binary classiﬁcation  e.g. using one-vs-all or one-vs-one strategy. To simplify the notation  we
also assume the latent variable h takes its value from a discrete set of labels h ∈ H. However  our
formulation is general. We will show how to deal with more complex h in Sec. 3.2 and in one of the
experiments (Sec. 4.3).
In latent SVM  the scoring function of sample x is deﬁned as fw(x) = maxh w(cid:62)φ(x  h)  where
φ(x  h) is the feature vector deﬁned for the pair of (x  h). For example  in the “car model” example 
φ(x  h) can be a feature vector extracted from the image patch at location h of the image x. The
objective function of LSVM is deﬁned as L(w) = 1
i max(0  1 − yifw(xi)). LSVM
is essentially a non-convex optimization problem. However  the learning problem becomes convex
once the latent variable h is ﬁxed for positive examples. Therefore  we can train the LSVM by
an iterative algorithm that alternates between inferring h on positive examples and optimizing the
model parameter w.
Dual form with ﬁxed h on positive examples : Due to its nature of non-convexity  it is not straight-
forward to derive the dual form for the general LSVM. Therefore  as a starting point  we ﬁrst con-
sider a simpler scenario assuming h is ﬁxed (or observed) on the positive training examples. As
previously mentioned  the LSVM is then relaxed to a convex problem with this assumption. Note
that we will relax this assumption in Sec. 3. In the above “car model” example  this means that
we have the ground-truth bounding boxes of the cars in each image. More formally  we are given

2||w||2 + C(cid:80)

2

M positive samples {xi  hi}M
j=M +1. Inspired by linear SVMs 
our goal is to ﬁnd a linear discriminant fw(x  h) = w(cid:62)φ(x  h) by solving the following quadratic
program:

i=1  and N negative samples {xj}M +N

(cid:88)

(cid:88)

P(w∗) = min

||w||2 + C1

1
2

ξj h
s.t. w(cid:62)φ(xi  hi) ≥ 1 − ξi  ∀i ∈ {1  2  ...  M} 

ξi + C2

w ξ

j h

i

(1a)

(1b)
(1c)
(1d)

−w(cid:62)φ(xj  h) ≥ 1 − ξj h ∀j ∈ {M + 1  M + 2  ...  M + N} ∀h ∈ H
ξi ≥ 0  ξj h ≥ 0 ∀i  ∀j  ∀h ∈ H

Similar to standard SVMs  {ξi} and {ξj h} are the slack variables for handling soft margins.
It is interesting to note that the optimization problem in Eq. 1 is almost identical to that of standard
linear SVMs. The only difference lies in the constraint on the negative training examples (Eq. 1c).
Since we assume h’s are not observed on negative images  we need to enumerate all possible values
for h’s in Eq. 1c. Intuitively  this means every image patch from a negative image (i.e. non-car
image) is not a car.
It is easy to show that Eq. 1 is convex. Similar to the dual form of standard SVMs  we can derive
the dual form of Eq. 1 as follows:

D(α∗  β∗) = max

α β

αi +

(cid:88)

i

(cid:88)

(cid:88)

j

h

||(cid:88)

αiφ(xi  hi) −(cid:88)

(cid:88)

i

j

h

βj h − 1
2

βj hφ(xj  h)||2 (2a)

s.t.

0 ≤ αi ≤ C1  ∀i; 0 ≤ βj h ≤ C2  ∀j  ∀h ∈ H

(2b)
The optimal primal parameters w∗ for Eq. 1 and the optimal dual parameters (α∗  β∗) for Eq. 2 are
related as follows:

(cid:88)

i φ(xi  hi) −(cid:88)

α∗

(cid:88)

w∗ =

β∗
j hφ(xj  h)

(3)

i

j

h

Let us deﬁne λ to be the concatenations of {αi : ∀i} and {βj h : ∀j ∀h ∈ H}  so |λ| = M +N×|H|.
Let Ψ be a |λ| × D matrix where D is the dimension of φ(x  h). Ψ is obtained by stacking together
{φ(xi  hi) : ∀i} and {−φ(xj  h) : ∀j ∀h ∈ H}. We also deﬁne Q = ΨΨ(cid:62) and 1 to be a vector of
all 1’s. Then Eq. 2a can be rewritten as (we omit the linear constraints on λ for simplicity):

max

λ

λ(cid:62) · 1 − 1
2

λ(cid:62)Qλ

(4)

(cid:16)(cid:80)

Each entry of Q is a dot-product of

The advantage of working with the dual form in Eq. 4 is that
it only involves a so-
called kernel matrix Q.
two vectors in the
form of φ(x  h)(cid:62)φ(x(cid:48)  h(cid:48)). We can replace the dot-product with any other kernel func-
tions in the form of k(φ(x  h)  φ(x(cid:48)  h(cid:48))) to get nonlinear classiﬁers [1].
The scor-
ing function for
the testing images xnew can be kernelized as follows:
f (xnew) =
i α∗
maxhnew
Another important  yet often overlooked fact is that the optimal values of the two quadratic programs
in Eqs. 1 and 2 have some speciﬁc meanings. They correspond to the inverse of the (soft) margin of
the resultant SVM classiﬁer [9  15]: P(w∗) = D(α∗  β∗) =
SVM margin. In the next section  we will
exploit this fact to develop the kernel latent support vector machines.

i k(φ(xi  hi)  φ(xnew  hnew)) −(cid:80)

j hk(φ(xj  h)  φ(xnew  hnew))

(cid:80)
h β∗

(cid:17)

.

1

j

3 Kernel Latent SVM
Now we assume the variables {hi}M
i=1 on the positive training examples are unobserved. If the scor-
ing function used for classiﬁcation is in the form of f (x) = maxh w(cid:62)φ(x  h)  we can use the LSVM
formulation [5  17] to learn the model parameters w. As mentioned earlier  the limitation of LSVM
is the linearity assumption of w(cid:62)φ(x  h). In this section  we propose kernel latent SVM (KLSVM)
– a new latent variable learning method that only requires a kernel function K(x  h  x(cid:48)  h(cid:48)) between
a pair of (x  h) and (x(cid:48)  h(cid:48)).
Note that when {hi}M
i=1 are observed on the positive training examples  we can plug them in Eq. 2
to learn a nonlinear kernelized decision function that separates the positive and negative examples.

3

i=1 are latent  an intuitive thing to do is to ﬁnd the labeling of {hi}M

When {hi}M
i=1 so that when
we plug them in and solve for Eq. 2  the resultant nonlinear decision function separates the two
classes as widely as possible. In other words  we look for a set of {h∗
i } which can maximize the
SVM margin (equivalent to minimizing D(α∗  β∗ {hi})). The same intuition was previously used
to develop the max-margin clustering method in [15]. Using this intuition  we write the optimal
function value of the dual form as D(α∗  β∗ {hi}) since now it implicitly depends on the labelings
{hi}. We can jointly ﬁnd the labelings {hi} and solve for (α∗  β∗) by the following optimization
problem:

D(α∗  β∗ {hi})

min{hi}

(cid:88)

α β

= min{hi} max
αi +
s.t. 0 ≤ αi ≤ C1  ∀i;

i

(cid:88)

(cid:88)

j

h

βj h − 1
2

||(cid:88)

αiφ(xi  hi) −(cid:88)

(cid:88)

(5a)

βj hφ(xj  h)||2 (5b)

0 ≤ βj h ≤ C2  ∀j  ∀h ∈ H

(5c)
The most straightforward way of solving Eq. 5 is to optimize D(α∗  β∗ {hi}) for every possible
combination of values for {hi}  and then take the minimum. When hi takes its value from a dis-
crete set of K possible choices (i.e. |H| = K)  this naive approach needs to solve M K quadratic
programs. This is obviously too expensive. Instead  we use the following iterative algorithm:

i

j

h

• Fix α and β  compute the optimal {hi}∗ by

αiφ(xi  hi) −(cid:88)

(cid:88)

βj hφ(xj  h)||2

• Fix {hi}  compute the optimal (α∗  β∗) by

1
2

||(cid:88)
(cid:88)

i

βj h − 1
2

{hi}∗ = arg max

{hi}

(cid:88)

i

(cid:88)

αi +

(α∗  β∗) = arg max

α β

j

h

αiφ(xi  hi) −(cid:88)

(cid:88)

βj hφ(xj  h)||2

||(cid:88)

(6)

 (7)

j

h

i

j

h

The optimization problem in Eq. 7 is a quadratic program similar to that of a standard dual SVM.
As a result  Eq. 7 can be kernelized as Eq. 4 and solved using standard dual solver in regular SVMs.
In Sec. 3.1  we describe how to kernelize and solve the optimization problem in Eq. 6.
3.1 Optimization over {hi}
The complexity of a simple enumeration approach for solving Eq. 6 is again O(M K)  which is
clearly too expensive for practical purposes.
Instead  we solve it iteratively using an algorithm
similar to co-ordinate ascent. Within an iteration  we choose one positive training example t. We
update ht while ﬁxing hi for all i (cid:54)= t. The optimal h∗

t can be computed as follows:

h∗
t = arg max

ht

||αtφ(xt  ht) +

⇔ arg max

ht

||αtφ(xt  ht)||2 + 2

βj hφ(xj  h)||2

(cid:62)

(8a)

αtφ(xt  ht) (8b)

βj hφ(xj  h)

i:i(cid:54)=t

j

h

(cid:88)
αiφ(xi  hi) −(cid:88)
(cid:88)
(cid:88)
αiφ(xi  hi) −(cid:88)
(cid:88)

i:i(cid:54)=t

j

h

(cid:88)

i:i(cid:54)=t

By replacing the dot-product φ(x  h)(cid:62)φ(x(cid:48)  h(cid:48)) with a kernel function k(φ(x  h)  φ(x(cid:48)  h(cid:48)))  we ob-
tain the kernerlized version of Eq. 8(b) as follows

αtαtk(φ(xt  ht)  φ(xt  ht)) + 2

αiαtk(φ(xi  hi)  φ(xt  ht))

h∗
t = arg max

ht

(cid:88)

(cid:88)

−2

βj hαtk(φ(xj  h)  φ(xt  ht))

(9)

j

h

It is interesting to notice that if the t-th example is not a support vector (i.e. αt = 0)  the function
value of Eq. 9 will be zero regardless of the value of ht. This means in KLSVM we can improve the
training efﬁciency by only performing Eq. 9 on positive examples corresponding to support vectors.
For other positive examples (non-support vectors)  we can simply set their latent variables the same

4

as the previous iteration. Note that in LSVM  the inference during training needs to be performed
on every positive example.
Connection to LSVM: When a linear kernel is used  the inference problem (Eq. 8) has a very
interesting connection to LSVM in [5]. Recall that for linear kernels  the model parameters w and
dual variables (α  β) are related by Eq. 3. Then Eq. 8 becomes:

||αtφ(xt  ht)||2 + 2(cid:0)w − αtφ(xt  hold
t )(cid:1)(cid:62)

αtφ(xt  ht)

h∗
t = arg max
⇔ arg max

ht

ht

αtw(cid:62)φ(xt  ht) +

1
2

t||φ(xt  ht)||2 − α2
α2

t φ(xt  hold

t )(cid:62)φ(xt  ht)

(10a)

(10b)

where hold
is the value of latent variable of the t-th example in the previous iteration. Let us con-
sider the situation when αt (cid:54)= 0 and the feature vector φ(x  h) is l2 normalized  which is com-
t
t φ(xt  ht)(cid:62)φ(xt  ht) is a constant  and we have
monly used in computer vision.
φ(xt  hold

t )(cid:62)φ(xt  hold
t ) > φ(xt  hold
h∗
t = arg max

In this case  α2
t )(cid:62)φ(xt  ht) if ht (cid:54)= hold
w(cid:62)φ(xt  ht) − αtφ(xt  hold

t

. Then Eq. 10 is equivalent to:
t )(cid:62)φ(xt  ht)

(11)

ht

t = arg maxht w(cid:62)φ(xt  ht)  but
Eq. 11 is very similar to the inference problem in LSVM  i.e.  h∗
t )(cid:62)φ(xt  ht) which penalizes the choice of ht for being the same
with an extra term αtφ(xt  hold
value as previous iteration hold
. This has a very appealing intuitive interpretation. If the t-th positive
t
example is a support vector  the latent variable hold from previous iteration causes this example to
lie very close to (or even on the wrong side) the decision boundary  i.e. the example is not well-
separated. During the current iteration  the second term in Eq. 11 penalizes hold to be chosen again
since we already know the example will not be well-separated if we choose hold again. The amount
)(cid:62)φ(xt  ht). We can interpret αt as how
of penalty depends on the magnitudes of αt and φ(xt  hold
t to
“bad” hold
be “close” to “bad” hold

)(cid:62)φ(xt  ht) as how close ht is to hold

. Eq. 11 penalizes the new h∗

is  and φ(xt  hold

t

t

t

t

.

t

3.2 Composite Kernels
So far we have assumed that the latent variable h takes its value from a discrete set of labels. Given
a pair of (x  h) and (x(cid:48)  h(cid:48))  the types of kernel function k(x  h; x(cid:48)  h(cid:48)) we can choose from are still
limited to a handful of standard kernels (e.g. Gaussian  RBF  HIK  etc). In this section  we consider
more interesting cases where h involves some complex structures. This will give us two important
beneﬁts. First of all  it allows us to exploit structural information in the latent variables. This is in
analog to structured output learning (e.g. [12  13]). More importantly  it gives us more ﬂexibility to
construct new kernel functions by composing from simple kernels.
Before we proceed  let us ﬁrst motivate the composite kernel with an example application. Suppose
we want to detect some complex person-object interaction (e.g. “person riding a bike”) in an image.
One possible solution is to detect persons and bikes in an image  then combine the results by taking
into account of their relationship (i.e. “riding”). Imagine we already have kernel functions corre-
sponding to some components (e.g. person  bike) of the interaction. In the following  we will show
how to compose a new kernel for the “person riding a bike” classiﬁer from those components.
We denote the latent variable using (cid:126)h to emphasize that now it is a vector instead of a single discrete
value. We denote it as (cid:126)h = (z1  z2  ...)  where zu is the u-th component of (cid:126)h and takes its value
from a discrete set of possible labels. For the structured latent variable  it is assumed that there are
certain dependencies between some pairs of (zu  zv). We can use an undirected graph G = (V E) to
capture the structure of the latent variable  where a vertex u ∈ V corresponds to the label zu  and an
edge (u  v) ∈ E corresponds to the dependency between zu and zv. As a concrete example  consider
the “person riding a bike” recognition problem. The latent variable in this case has two components
(cid:126)h = (zperson  zbike) corresponding to the location of person and bike  respectively. On the training
data  we have access to the ground-truth bounding box of “person riding a bike” as a whole  but not
the exact location of “person” or “bike” within the bounding box. So (cid:126)h is latent in this application.
The edge connecting zperson and zbike captures the relationship (e.g. “riding on”  “next to”  etc.)
between these two objects.
Suppose we already have kernel functions corresponding to the vertices and edges in the graph  we
can then deﬁne the composite kernel as the summation of the kernels over all the vertices and edges.

5

Figure 1: Visualization of how the latent variable (i.e. object location) changes during the learning. The red
bounding box corresponds to the initial object location. The blue bounding box corresponds to the object
location after the learning.

Method
Acc (%)

BOF + linear SVM BOF + kernel SVM linear LSVM
75.07 ± 4.18

45.57 ± 4.23

50.53 ± 6.53

KLSVM

84.49 ± 3.63

Table 1: Results on the mammal dataset. We show the mean/std of classiﬁcation accuracies over ﬁve rounds of
experiments.

K(Φ(x  (cid:126)h)  Φ(x(cid:48)  (cid:126)h(cid:48))) =

ku(φ(x  zu)  φ(x(cid:48)  z(cid:48)

u)) +

kuv(ψ(x  zu  zv)  ψ(x(cid:48)  z(cid:48)

u  z(cid:48)

v)) (12)

(cid:88)

u∈V

(cid:88)

(u v)∈E

When the latent variable (cid:126)h forms a tree structure  there exist efﬁcient inference algorithms for
solving Eq. 9  such as dynamic programming.
It is also possible for Eq. 12 to include kernels
deﬁned on higher-order cliques in the graph  as long as we have some pre-deﬁned kernel functions
for them.
4 Experiments
We evaluate KLSVM in three different applications of visual recognition. Each application has a
different type of latent variables. For these applications  we will show that KLSVM outperforms
both the linear LSVM [5] and the regular kernel SVM. Note that we implement the learning of
linear LSVM by ourselves using the same iterative algorithm as the one in [5].

4.1 Object Classiﬁcation with Latent Localization
Problem and Dataset: We consider object classiﬁcation with image-level supervision. Our training
data only have image-level labels indicating the presence/absence of each object category in an
image. The exact object location in the image is not provided and is considered as the latent variable
h in our formulation. We deﬁne the feature vector φ(x  h) as the HOG feature extracted from the
image at location h. During testing  the inference of h is performed by enumerating all possible
locations of the image.
We evaluate our algorithm on the mammal dataset [8] which consists of 6 mammal categories. There
are about 45 images per category. For each category  we use half of the images for training and the
remaining half for testing. We assume the object size is the same for the images of the same category 
which is a reasonable assumption for this dataset. This dataset was used to evaluate the linear LSVM
in [8].
Results: We compare our algorithm with linear LSVM. To demonstrate the beneﬁt of using latent
variables  we also compare with two simple baselines using linear and kernel SVMs based on bag-of-
features (BOF) extracted from the whole image (i.e. without latent variables). For both baselines  we
aggregate the quantized HOG features densely sampled from the whole image. Then  the features are
fed into the standard linear SVM and kernel SVM respectively. We use the histogram intersection
kernel (HIK) [10] since it has been proved to be successful for vision applications  and efﬁcient
learning/inference algorithms exist for this kernel.
We run the experiments for ﬁve rounds. In each round  we randomly split the images from each
category into training and testing sets. For both linear LSVM and KLSVM  we initialize the latent
variable at the center location of each image and we set C1 = C2 = 1. For both algorithms  we use
one-versus-one classiﬁcation scheme. We use the HIK kernel in the KLSVM. Table 1 summarizes
the mean and standard deviations of the classiﬁcation accuracies over ﬁve rounds of experiments.
Across all experiments  both linear LSVM and KLSVM achieve signiﬁcantly better results than
approaches using BOF features from the whole image. This is intuitively reasonable since most of
images on this dataset share very similar scenes. So BOF feature without latent variables cannot
capture the subtle differences between each category. Table 1 also shows KLSVM signiﬁcantly
outperforms linear LSVM.
Fig. 1 shows examples of how the latent variables change on some training images during the learn-
ing of the KLSVM. For each training image  the location of the object (latent variable h) is initialized
to the center of the image. After the learning algorithm terminates  the latent variables accurately
locate the objects.

6

Figure 2: Visualization of some testing examples from the “bird” (left) and “boat” (right) categories. Each row
corresponds to a subcategory. We can see that visually similar images are grouped into the same subcategory.

Method
Acc (%)

non-latent linear SVM linear LSVM non-latent kernel SVM

50.69 ± 0.38

53.13 ± 0.63

52.98 ± 0.22

KLSVM

55.17 ± 0.27

Table 2: Results on CIFAR10 Dataset. We show the mean/std of classiﬁcation accuracies over ﬁve folds of
experiments. Each fold uses a different batch of the training data.

4.2 Object Classiﬁcation with Latent Subcategory
Problem and Dataset: Our second application is also on object classiﬁcation. But here we con-
sider a different type of latent variable. Objects within a category usually have a lot of intra-class
variations. For example  consider the images for the “bird” category shown in the left column of
Fig. 2. Even though they are examples of the same category  they still exhibit very large appearance
variations. It is usually very difﬁcult to learn a single “bird” model that captures all those variations.
One way to handle the intra-class variation is to split the “bird” category into several subcategories.
Examples within a subcategory will be more visually similar than across all subcategories. Here we
use the latent variable h to indicate the subcategory an image belongs to. If a training image belongs
to the class c  its subcategory label h takes value from a set Hc of subcategory labels corresponding
to the c-th class. Note that subcategories are latent on the training data  so they may or may not have
semantic meanings.
The feature vector φ(x  h) is deﬁned as a sparse vector whose feature dimension is |Hc| times of
In the
the dimension of φ(x)  where φ(x) is the HOG descriptor extracted from the image x.
experiments  we set |Hc| = 3 for all c’s. Then we can deﬁne φ(x  h = 1) = (φ(x); 0; 0)  φ(x  h =
2) = (0; φ(x); 0)  and so on. Similar models have been proposed to address the viewpoint changing
in object detection [6] and semantic variations in YouTube video tagging [16].
We use the CIFAR10 [7] dataset in our experiment. It consists of images from ten classes including
airplane  automobile  bird  cat  etc. The training set has been divided into ﬁve batches and each
batch contains 10000 images. There are in total 10000 test images.
Results: Again we compare with three baselines: linear LSVM  non-latent linear SVM  non-latent
kernel SVM. Similarly  we use HIK kernel for the kernel-based methods. For non-latent approaches 
we simply feed feature vector φ(x) to SVMs without using any latent variable.
We run the experiments in ﬁve folds. Each fold use a different training batch but the same testing
batch. We set C1 = C2 = 0.01 for all the experiments and initialize the subcategory labels of
training images by k-means clustering. Table 2 summarizes the results. Again  KLSVM outperforms
other baseline approaches. It is interesting to note that both linear LSVM and KLSVM outperform
their non-latent counterparts  which demonstrates the effectiveness of using latent subcategories in
object classiﬁcation. We visualize examples of the correctly classiﬁed testing images from the “bird”
and “boat” categories in Fig. 2. Images on the same row are assigned the same subcategory labels.
We can see that visually similar images are automatically grouped into the same subcategory.

4.3 Recognition of Object Interaction
Problem and Dataset: Finally  we consider an application where the latent variable is more com-
plex and requires the composite kernel introduced in Sec. 3.2. We would like to recognize complex
interactions between two objects (also called “visual phrases” [11]) in static images. We build a
dataset consisting of four object interaction classes  i.e. “person riding a bicycle”  “person next to
a bicycle”  “person next to a car” and “bicycle next to a car” based on the visual phrase dataset in
[11]. Each class contains 86∼116 images. Each image is only associated with one of the four object
interaction label. There is no ground-truth bounding box information for each object. We use 40
images from each class for training and the rest for testing.
Our approach: We treat the locations of objects as latent variables. For example  when learning
the model for “person riding a bicycle”  we treat the locations of “person” and “bicycle” as latent
In this example  each image is associated with latent variables (cid:126)h = (z1  z2)  where
variables.
z1 denotes the location of the “person” and z2 denotes the location of the “bicycle”. To reduce
the search space of inference  we ﬁrst apply off-the-shelf “person” and “bicycle” detectors [5] on

7

Method
Acc(%)

BOF + linear SVM BOF + kernel SVM linear LSVM
46.33 ± 1.4

42.92

58.46

KLSVM

66.42 ± 0.99

Table 3: Results on object interaction dataset. For the approaches using latent variables  we show the mean/std
of classiﬁcation accuracies over ﬁve folds of experiments.

Figure 3: Visualization of how latent variables (i.e. object locations) change during the learning. The left image
is from the “person riding a bicycle” category  and the right image is from the “person next to a car” category.
Yellow bounding boxes corresponds to the initial object locations. The blue bounding boxes correspond to the
object locations after the learning.
each image. For each object  we generate ﬁve candidate bounding boxes which form a set Zi 
|Z1| = |Z2| = 5 and zi ∈ Zi. Then  the inference of (cid:126)h is performed by enumerating 25
i.e.
combinations of z1 and z2. We also assume there are certain dependencies between the pair of
(z1  z2). Then the kernel between two images can be deﬁned as follows:

(cid:88)

u={1 2}

K(Φ(x  (cid:126)h)  Φ(x(cid:48)  (cid:126)h(cid:48))) =

ku (φ(x  zu)  φ(x(cid:48)  z(cid:48)

u)) + kp (ψ(z1  z2)  ψ(z(cid:48)

1  z(cid:48)

2))

(13)

We deﬁne φ(x  zu) as the bag-of-features (BOF) extracted from the bounding box zu in the image x.
For each bounding box  we split the region uniformly into four equal quadrants. Then we compute
the bag-of-features for each quadrant by aggregating quantized HOG features. The ﬁnal feature
vector is the concatenation of these four bag-of-features histograms. This feature representation
is similar to the spatial pyramid feature representation.
In our experiment  we choose HIK for
ku(·). The kernel kp(·) captures the spatial relationship between z1 and z2 such as above  below 
overlapping  next-to  near  and far. Here ψ(z1  z2) is a sparse binary vector and its k-th element is
set to 1 if the corresponding k-th relation is satisﬁed between bounding boxes z1 and z2. Note that
kp(·) does not depend on the images. Similar representation has been used in [4]. We deﬁne kp(·)
as a simple linear kernel.
Results: We compare with the simple BOF + linear SVM  and BOF + kernel SVM approaches.
These two baselines use the same BOF feature representation as our approach except that the features
are extracted from the whole image. We choose the HIK in the kernel SVM. Note that this is a
strong baseline since [3] has shown that a similar pyramid feature representation with kernel SVM
achieves top performances on the task of person-object interaction recognition. The other baseline
is the standard linear LSVM  in which we build the feature vector φ(x  h) by simply concatenating
both unary features and pairwise features  i.e. φ(x  h) = [φ(x  z1); φ(x  z2); ψ(z1  z2)]. Again  we
set C1 = C2 = 1 for all experiments. We run the experiments for ﬁve rounds for approaches using
latent variables. In each round  we randomly initialize the choices of z1 and z2. Table 3 summarizes
the results. The kernel latent SVM that uses HIK for ku(·) achieves the best performance.
Fig. 3 shows examples of how the latent variables change on some training images during the learn-
ing of the KLSVM. For each training image  both latent variables z1 and z2 are randomly initialized
to one of ﬁve candidate bounding boxes. As we can see  the initial bounding boxes can accurately
locate the target objects but their spatial relations are different to ground-truth labels. After learning
algorithm terminates  the latent variables not only locate the target objects  but more importantly
they also capture the correct spatial relationship between objects.

5 Conclusion
We have proposed kernel latent SVM – a new learning framework that combines the beneﬁts of
LSVM and kernel methods. Our learning framework is very general. The latent variables can not
only be a single discrete value  but also be more complex values with interdependent structures. Our
experimental results on three different applications in visual recognition demonstrate that KLSVM
outperforms using LSVM or using kernel methods alone. We believe our work will open the
possibility of constructing more powerful and expressive prediction models for visual recognition.
Acknowledgement: This work was supported by a Google Research Award and NSERC.
Yang Wang was partially supported by a NSERC postdoc fellowship.

8

References
[1] C. J. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge

Discovery  2(2):121–167  1998.

[2] N. Dalal and B. Triggs. Histogram of oriented gradients for human detection. In IEEE Computer Society

Conference on Computer Vision and Pattern Recognition  2005.

[3] V. Delaitre  I. Laptev  and J. Sivic. Recognizing human actions in still images: a study of bag-of-features

and part-based representations. In British Machine Vision Conference  2010.

[4] C. Desai  D. Ramanan  and C. Fowlkes. Discriminative models for multi-class object layout. In IEEE

International Conference on Computer Vision  2009.

[5] P. F. Felzenszwalb  R. B. Girshick  D. McAllester  and D. Ramanan. Object detection with discrimi-
natively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence 
32(9):1672–1645  2010.

[6] C. Gu and X. Ren. Discriminative mixture-of-templates for viewpoint classiﬁcation. In European Con-

[7] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis  University of

ference on Computer Vision  2010.

Toronto  2009.

[8] M. P. Kumar  B. Packer  and D. Koller. Self-paced learning for latent variable models. In Advances in

Neural Information Processing Systems  2010.

[9] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. R. Ghaoui  and M. I. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine Learning Research  5:24–72  2004.

[10] S. Maji  A. C. Berg  and J. Malik. Classiﬁcation using intersection kernel support vector machines is

efﬁcient. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition  2008.

[11] M. A. Sadeghi and A. Farhadi. Recognition using visual phrases. In IEEE Computer Society Conference

on Computer Vision and Pattern Recognition  2011.

[12] B. Taskar  C. Guestrin  and D. Koller. Max-margin markov networks. In Advances in Neural Information

Processing Systems  volume 16. MIT Press  2004.

[13] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured and

interdependent output variables. Journal of Machine Learning Research  6:1453–1484  2005.

[14] A. Vedaldi and A. Zisserman. Efﬁcient additive kernels via explicit feature maps. Pattern Analysis and

Machine Intellingence  34(3)  2012.

[15] L. Xu  J. Neufeldand  B. Larson  and D. Schuurmans. Maximum margin clustering.

In L. K. Saul 
Y. Weiss  and L. Bottou  editors  Advances in Neural Information Processing Systems  volume 17  pages
1537–1544. MIT Press  Cambridge  MA  2005.

[16] W. Yang and G. Toderici. Discriminative tag learning on youtube videos with latent sub-tags. In IEEE

Computer Society Conference on Computer Vision and Pattern Recognition  2011.

[17] C.-N. Yu and T. Joachims. Learning structural SVMs with latent variables. In International Conference

on Machine Learning  2009.

9

,Christof Seiler
Simon Rubinstein-Salzedo
Susan Holmes
Yong Guo
Yin Zheng
Mingkui Tan
Qi Chen
Jian Chen
Peilin Zhao
Junzhou Huang