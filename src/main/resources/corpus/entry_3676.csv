2018,Natasha 2: Faster Non-Convex Optimization Than SGD,We design a stochastic algorithm to find $\varepsilon$-approximate local minima of any smooth nonconvex function in rate $O(\varepsilon^{-3.25})$  with only oracle access to stochastic gradients. The best result before this work was $O(\varepsilon^{-4})$ by stochastic gradient descent (SGD).,Natasha 2: Faster Non-Convex Optimization Than

SGD

Zeyuan Allen-Zhu‚àó
Microsoft Research AI

zeyuan@csail.mit.edu

Abstract

We design a stochastic algorithm to Ô¨Ånd Œµ-approximate local minima of any
smooth nonconvex function in rate O(Œµ‚àí3.25)  with only oracle access to stochas-
tic gradients. The best result before this work was O(Œµ‚àí4) by stochastic gradient
descent (SGD).2

1

Introduction

In diverse world of deep learning research has given rise to numerous architectures for neural net-
works (convolutional ones  long short term memory ones  etc). However  to this date  the underlying
training algorithms for neural networks are still stochastic gradient descent (SGD) and its heuristic
variants. In this paper  we address the problem of designing a new algorithm that has provably faster
running time than the best known result for SGD.
Mathematically  we study the problem of online stochastic nonconvex optimization:

(cid:110)

(cid:80)n

i=1 fi(x)

(cid:111)

where both f (¬∑) and each fi(¬∑) can be nonconvex. We want to study

minx‚ààRd

f (x) := Ei[fi(x)] = 1

n

(1.1)

online algorithms to Ô¨Ånd appx. local minimum of f (x).

Here  we say an algorithm is online if its complexity is independent of n. This tackles the big-data
scenarios when n is extremely large or even inÔ¨Ånite.3
Nonconvex optimization arises prominently in large-scale machine learning. Most notably  train-
ing deep neural networks corresponds to minimizing f (x) of this average structure: each training
sample i corresponds to one loss function fi(¬∑) in the summation. This average structure allows
one to perform stochastic gradient descent (SGD) which uses a random ‚àáfi(x) ‚Äîcorresponding to
computing backpropagation once‚Äî to approximate ‚àáf (x) and performs descent updates.
The standard goal of nonconvex optimization with provable guarantee is to Ô¨Ånd approximate local
minima. This is not only because Ô¨Ånding the global one is NP-hard  but also because there exist rich
literature on heuristics for turning a local-minima Ô¨Ånding algorithm into a global one. This includes
random seeding  graduated optimization [25] and others. Therefore  faster algorithms for Ô¨Ånding
approximate local minima translate into faster heuristic algorithms for Ô¨Ånding global minimum.
On a separate note  experiments [16  17  24] suggest that fast convergence to approximate local
minima may be sufÔ¨Åcient for training neural nets  while convergence to stationary points (i.e.  points
that may be saddle points) is not. In other words  we need to avoid saddle points.

‚àóThe full version of this paper can be found on https://arxiv.org/abs/1708.08694.
2When this manuscript Ô¨Årst appeared online  the best rate was T = O(Œµ‚àí4) by SGD. Several followups
appeared after this paper. This includes stochastic cubic regularization [44] which gives T = O(Œµ‚àí3.5)  and
Neon+SCSG [10  46] which gives T = O(Œµ‚àí3.333). These rates are worse than T = O(Œµ‚àí3.25).
3All of our results in this paper apply to the case when n is inÔ¨Ånite  meaning f (x) = Ei[fi(x)]  because we

focus on online methods. However  we still introduce n to simplify notations.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr¬¥eal  Canada.

Figure 1: Local minimum (left)  saddle point (right) and its negative-curvature direction.

1.1 Classical Approach: Escaping Saddle Points Using Random Perturbation

One natural way to avoid saddle points is to use randomness to escape from it  whenever we meet
one. For instance  Ge et al. [22] showed  by injecting random perturbation  SGD will not be stuck
in saddle points: whenever SGD moves into a saddle point  randomness shall help it escape. This
partially explains why SGD performs well in deep learning.4 Jin et al. [27] showed  equipped with
random perturbation  full gradient descent (GD) also escapes from saddle points. Being easy to
implement  however  we raise two main efÔ¨Åciency issues regarding this classical approach:
‚Ä¢ Issue 1. If we want to escape from saddle points  is random perturbation the only way? Moving
in a random direction is ‚Äúblind‚Äù to the Hessian information of the function  and thus can we
escape from saddle points faster?
‚Ä¢ Issue 2. If we want to avoid saddle points  is it really necessary to Ô¨Årst move close to saddle
points and then escape from them? Can we design an algorithm that can somehow avoid saddle
points without ever moving close to them?

1.2 Our Resolutions

Resolution to Issue 1: EfÔ¨Åcient Use of Hessian. Mathematically  instead of using a random
perturbation  the negative eigenvector of ‚àá2f (x) (a.k.a. the negative-curvature direction of f (¬∑) at
x) gives us a better direction to escape from saddle points. See Figure 1.
To make it concrete  suppose we apply power method on ‚àá2f (x) to Ô¨Ånd its most negative eigen-
vector. If we run power method for 0 iteration  then it gives us a totally random direction; if we run
it for more iterations  then it converges to the most negative eigenvector of ‚àá2f (x). Unfortunately 
applying power method is unrealistic because f (x) = 1
i fi(x) can possibly have inÔ¨Ånite pieces.
n
We propose to use Oja‚Äôs algorithm [37] to approximate power method. Oja‚Äôs algorithm can be
viewed as an online variant of power method  and requires only (stochastic) matrix-vector product
computations. In our setting  this is the same as (stochastic) Hessian-vector products ‚Äînamely 
computing ‚àá2fi(x) ¬∑ w for arbitrary vectors w ‚àà Rd and random indices i ‚àà [n]. It is a known fact
that computing Hessian-vector products is as cheap as computing stochastic gradients  and thus we
can use Oja‚Äôs algorithm to escape from saddle points.

(cid:80)

(a)

(b)

(c)

Figure 2: Illustration of Natasha2full ‚Äî how to swing by a saddle point.

(a) move in a negative curvature direction if there is any (by applying Oja‚Äôs algorithm)
(b) swing by a saddle point without entering its neighborhood (wishful thinking)
(c) swing by a saddle point using only stochastic gradients (by applying Natasha1.5full)

4In practice  stochastic gradients naturally incur ‚Äúrandom noise‚Äù and adding perturbation may not be needed.

2

Resolution to Issue 2: Swing by Saddle Points.
If the function is sufÔ¨Åciently smooth 5 then any
point close to a saddle point must have a negative curvature. Therefore  as long as we are close to
saddle points  we can already use Oja‚Äôs algorithm to Ô¨Ånd such negative curvature  and move in its
direction to decrease the objective  see Figure 2(a).
Therefore  we are left only with the case that point is not close to any saddle point. Using smoothness
of f (¬∑)  this gives a ‚Äúsafe zone‚Äù near the current point  in which there is no strict saddle point 
see Figure 2(b). Intuitively  we wish to use the property of safe zone to design an algorithm that
decreases the objective faster than SGD. Formally  f (¬∑) inside this safe zone must be of ‚Äúbounded
nonconvexity ‚Äù meaning that its eigenvalues of the Hessian are always greater than some negative
threshold ‚àíœÉ (where œÉ depends on how long we run Oja‚Äôs algorithm). Intuitively  the greater œÉ
is  then the more non-convex f (x) is. We wish to design an (online) stochastic Ô¨Årst-order method
whose running time scales with œÉ.
Unfortunately  classical stochastic methods such as SGD or SCSG [30] cannot make use of this
nonconvexity parameter œÉ. The only known ones that can make use of œÉ are ofÔ¨Çine algorithms. In
this paper  we design a new stochastic Ô¨Årst-order method Natasha1.5

Œµ3 + œÉ1/3
Œµ10/3
Finally  we put Natasha1.5 together with Oja‚Äôs to construct our Ô¨Ånal algorithm Natasha2:
Theorem 2 (informal). Natasha2 Ô¨Ånds x with (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ and ‚àá2f (x) (cid:23) ‚àíŒ¥I in rate T =

Theorem 1 (informal). Natasha1.5 Ô¨Ånds x with (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ in rate T = O(cid:0) 1
(cid:1).
(cid:1). In particular  when Œ¥ ‚â• Œµ1/4  this gives T = (cid:101)O(cid:0) 1
(cid:101)O(cid:0) 1
In contrast  the convergence rate of SGD was T = (cid:101)O(poly(d) ¬∑ Œµ‚àí4) [22].

Œ¥Œµ3 + 1
Œµ3.25

(cid:1) .

Œ¥5 + 1

Œµ3.25

1.3 Follow-Up Results

‚Ä¢ If one applies SGD and only escapes from saddle points using Oja‚Äôs algorithm  the convergence

Since the original appearance of this work  there has been a lot of progress in stochastic nonconvex
optimization. Most notably 
‚Ä¢ If one swings by saddle points using Oja‚Äôs algorithm and SGD variants (instead of Natasha1.5) 

the convergence rate is T = (cid:101)O(Œµ‚àí3.5) [5].
rate is T = (cid:101)O(Œµ‚àí4) [10  46].
rate is T = (cid:101)O(Œµ‚àí3.333) [10  46].
is T = (cid:101)O(Œµ‚àí3.5) [44].
‚Ä¢ If f (x) is of œÉ-bounded nonconvexity  the SGD4 method [5] gives rate T = (cid:101)O(Œµ‚àí2 + œÉŒµ‚àí4).

‚Ä¢ If one applies a stochastic version of cubic regularized Newton‚Äôs method  the convergence rate

‚Ä¢ If one applies SCSG and only escapes from saddle points using Oja‚Äôs algorithm  the convergence

We include these results in Table 1 for a close comparison.

2 Preliminaries
Throughout this paper  we denote by (cid:107) ¬∑ (cid:107) the Euclidean norm. We use i ‚ààR [n] to denote that i
is generated from [n] = {1  2  . . .   n} uniformly at random. We denote by ‚àáf (x) the gradient of
function f if it is differentiable  and ‚àÇf (x) any subgradient if f is only Lipschitz continuous. We
denote by I[event] the indicator function of probabilistic events.
We denote by (cid:107)A(cid:107)2 the spectral norm of matrix A. For symmetric matrices A and B  we write
A (cid:23) B to indicate that A ‚àí B is positive semideÔ¨Ånite (PSD). Therefore  A (cid:23) ‚àíœÉI if and only if
all eigenvalues of A are no less than ‚àíœÉ. We denote by Œªmin(A) and Œªmax(A) the minimum and
maximum eigenvalue of a symmetric matrix A.
DeÔ¨Ånition 2.1. For a function f : Rd ‚Üí R 
‚Ä¢ f is œÉ-strongly convex if ‚àÄx  y ‚àà Rd  it satisÔ¨Åes f (y) ‚â• f (x) + (cid:104)‚àÇf (x)  y ‚àí x(cid:105) + œÉ

2(cid:107)x ‚àí y(cid:107)2.

5As we shall see  smoothness is necessary for Ô¨Ånding approximate local minima with provable guarantees.

3

convex only

approximate
stationary

points

algorithm

SGD1 [5  23]

SGD2 [5]
SGD3 [5]

SGD (folklore)

SCSG [30]

Natasha1.5

SGD4 [5]

variance
bound
needed
needed
needed

needed
needed

Lipschitz
smooth
needed
needed
needed

needed
needed

needed

needed

(cid:93)

needed

needed

2nd-order
smooth

no
no
no

no
no

no

no

(see Appendix B)

(see Theorem 1)

gradient complexity T

O(cid:0)Œµ‚àí2.667(cid:1)
O(cid:0)Œµ‚àí2.5(cid:1)
(cid:101)O(cid:0)Œµ‚àí2(cid:1)
O(cid:0)Œµ‚àí4(cid:1)
O(cid:0)Œµ‚àí3.333(cid:1)
O(cid:0)Œµ‚àí3 + œÉ1/3Œµ‚àí3.333(cid:1)
(cid:101)O(cid:0)Œµ‚àí2 + œÉŒµ‚àí4(cid:1)
(cid:101)O(cid:0)Œµ‚àí4 ¬∑ poly(d)(cid:1)
(cid:101)O(cid:0)Œµ‚àí3.25(cid:1)
(cid:101)O(cid:0)Œµ‚àí4(cid:1)
(cid:101)O(cid:0)Œµ‚àí3.5(cid:1)
(cid:101)O(cid:0)Œµ‚àí3.5(cid:1)
(cid:101)O(cid:0)Œµ‚àí3.333(cid:1)

(cid:93)

(cid:93)

(cid:93)

(cid:93)

(cid:93)

(cid:93)

Natasha2

approximate

local
minima

(see Theorem 2)

perturbed SGD [22]

NEON + SGD [10  46]
cubic Newton [44]

needed
needed
needed
needed
needed
needed
Table 1: Comparison of online methods for Ô¨Ånding (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ. Following tradition  in these complexity
bounds  we assume variance and smoothness parameters as constants  and only show the dependency
on n  d  Œµ and the bounded nonconvexity parameter œÉ ‚àà (0  1). We use (cid:93) to indicate results that
appeared after this paper.

needed
needed
needed
needed
needed
needed

needed
needed
needed
needed
needed
needed

NEON + SCSG [10  46]

SGD5 [5]

Remark 1. Variance bounds must be needed for online methods.
Remark 2. Lipschitz smoothness must be needed for achieving even approximate stationary points.
Remark 3. Second-order smoothness must be needed for achieving approximate local minima.

f (x) + (cid:104)‚àÇf (x)  y ‚àí x(cid:105) ‚àí œÉ

‚Ä¢ f is of œÉ-bounded nonconvexity (or œÉ-nonconvex for short) if ‚àÄx  y ‚àà Rd  it satisÔ¨Åes f (y) ‚â•
‚Ä¢ f is L-Lipschitz smooth (or L-smooth for short) if ‚àÄx  y ‚àà Rd  (cid:107)‚àáf (x) ‚àí ‚àáf (y)(cid:107) ‚â§ L(cid:107)x ‚àí y(cid:107).
‚Ä¢ f is second-order L2-Lipschitz smooth (or L2-second-order smooth for short) if ‚àÄx  y ‚àà Rd  it

2(cid:107)x ‚àí y(cid:107)2. 6

satisÔ¨Åes (cid:107)‚àá2f (x) ‚àí ‚àá2f (y)(cid:107)2 ‚â§ L2(cid:107)x ‚àí y(cid:107).

These deÔ¨Ånitions have other equivalent forms  see textbook [33].
DeÔ¨Ånition 2.2. For composite function F (x) = œà(x) + f (x) where œà(x) is proper convex  given a
parameter Œ∑ > 0  the gradient mapping of F (¬∑) at point x is

(cid:0)x ‚àí x(cid:48)(cid:1)

GF Œ∑(x) :=

1
Œ∑

where

x(cid:48) = arg min

y

In particular  if œà(¬∑) ‚â° 0  then GF Œ∑(x) ‚â° ‚àáf (x).

(cid:8)œà(y) + (cid:104)‚àáf (x)  y(cid:105) +

(cid:107)y ‚àí x(cid:107)2(cid:9)

1
2Œ∑

3 Natasha 1.5: Finding Approximate Stationary Points

We Ô¨Årst make a detour to study how to Ô¨Ånd approximate stationary points using only Ô¨Årst-order
information. A point x ‚àà Rd is an Œµ-approximate stationary point7 of f (x) if it satisÔ¨Åes (cid:107)‚àáf (x)(cid:107) ‚â§
Œµ. Let gradient complexity T be the number of computations of ‚àáfi(x).

6Previous authors also refer to this notion as ‚Äúapproximate convex‚Äù  ‚Äúalmost convex‚Äù  ‚Äúhypo-convex‚Äù 
‚Äúsemi-convex‚Äù  or ‚Äúweakly-convex.‚Äù We call it œÉ-nonconvex to stress the point that œÉ can be as large as L
(any L-smooth function is automatically L-nonconvex).
7Historically  in Ô¨Årst-order literatures  x is called Œµ-approximate if (cid:107)‚àáf (x)(cid:107)2 ‚â§ Œµ; in second-order litera-
tures  x is Œµ-approximate if (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ. We adapt the latter notion following Polyak and Nesterov [34  36].

4

(a) ofÔ¨Çine Ô¨Årst-order methods

(b) online Ô¨Årst-order methods

Figure 3: Comparison of Ô¨Årst-order methods for Ô¨Ånding Œµ-approximate stationary points of a œÉ-nonconvex
function. For simplicity  in the plots we let L = 1 and V = 1. The results SGD2/3/4 appeared after
this work.

Before 2015  nonconvex Ô¨Årst-order methods give rise to two convergence rates. SGD converges in
T = O(Œµ‚àí4) and GD converges T = O(nŒµ‚àí2). The proofs of both are simple (see Appendix B for
completeness). In particular  the convergence of SGD relies on two minimal assumptions
f (x) has bounded variance V  meaning Ei[(cid:107)‚àáfi(x) ‚àí ‚àáf (x)(cid:107)2] ‚â§ V  and
f (x) is L-Lipschitz smooth  meaning (cid:107)‚àáf (x) ‚àí ‚àáf (y)(cid:107) ‚â§ L ¬∑ (cid:107)x ‚àí y(cid:107).

(A1)
(A2‚Äô)
Remark 3.1. Both assumptions are necessary to design online algorithms for Ô¨Ånding stationary
points.8 For ofÔ¨Çine algorithms ‚Äîlike GD‚Äî the Ô¨Årst assumption is not needed.
Since 2016  the convergence rates have been improved to T = O(n + n2/3Œµ‚àí2) for ofÔ¨Çine meth-
ods [6  38]  and to T = O(Œµ‚àí10/3) for online algorithms [30]. Both results are based on the SVRG
(stochastic variance reduced gradient) method  and assume additionally (note (A2) implies (A2‚Äô))

each fi(x) is L-Lipschitz smooth.

(A2)

all the eigenvalues of ‚àá2f (x) lie in [‚àíœÉ  L]

Lei et al. [30] gave their algorithm a new name  SCSG (stochastically controlled stochastic gradient).
Bounded Non-Convexity.
In recent works [3  13]  it has been proposed to study a more reÔ¨Åned
convergence rate  by assuming that f (x) is of œÉ-bounded nonconvexity (or œÉ-nonconvex)  meaning
(A3)
for some œÉ ‚àà (0  L]. This parameter œÉ is analogous to the strong-convexity parameter ¬µ in convex
optimization  where all the eigenvalues of ‚àá2f (x) lie in [¬µ  L] for some ¬µ > 0.
In our illustrative process to ‚Äúswing by a saddle point ‚Äù the function inside safe zone ‚Äîsee
Figure 2(b)‚Äî is also of bounded nonconvexity. Since larger œÉ means the function is ‚Äúmore non-
convex‚Äù and thus harder to optimize  can we design algorithms with gradient complexity T as an
increasing function of œÉ ?
Remark 3.2. Most methods (SGD  SCSG  SVRG and GD) do not run faster in theory if œÉ < L.
In the ofÔ¨Çine setting  two methods are known to make use of parameter œÉ. One is repeatSVRG 
implicitly in [13] and formally in [3]. The other is Natasha1 [3]. repeatSVRG performs better
when œÉ ‚â§ L/
Before this work  no online method is known to take advantage of œÉ.

‚àö
n and Natasha1 performs better when œÉ ‚â• L/

‚àö

n. See Figure 3(a) and Table 2.

3.1 Our Theorem

We show that  under (A1)  (A2) and (A3)  one can non-trivially extend Natasha1 to an online
version  taking advantage of œÉ  and achieving better complexity than SCSG.
Let ‚àÜf be any upper bound on f (x0) ‚àí f (x‚àó) where x0 is the starting point. In this section  to
present the simplest results  we use the big-O notion to hide dependency in ‚àÜf and V. In Section 6 
8For instance  if the variance V is unbounded  we cannot even tell if a point x satisÔ¨Åes (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ using
Ô¨Ånite samples. Also  if f (x) is not Lipschitz smooth  it may contain sharp turning points (e.g.  behaves like
absolute value function |x|); in this case  Ô¨Ånding (cid:107)‚àáf (x)(cid:107) ‚â§ Œµ can be as hard as Ô¨Ånding (cid:107)‚àáf (x)(cid:107) = 0  and is
NP-hard in general.

5

ùúé=1/ùëõrepeatSVRGNatasha1ùëõ2/3ùúé1/3ùúÄ2ùëõ3/4ùúé1/2ùúÄ2ùëõ2/3ùúÄ2ùëõùúÄ2ùëõùúéùúÄ2SVRGGDùëõ1/2ùúÄ2gradient complexity ùëáùúé=1ùúé=0ùúÄùúé=1Natasha1.5ùúé=0ùúé1/3ùúÄ10/3ùúÄ‚àí10/3ùúÄ‚àí4SCSGSGDùúÄ‚àí3ùúéùúÄ‚àí4ùúÄ‚àí2SGD4ùúéùúÄ‚àí4ùúÄ‚àí8/3ùúÄ‚àí2.5gradient complexity ùëáSGD3SGD2SGD1ùúÄ2‚àÖ  epoch length B ‚àà [n]  epoch count T (cid:48) ‚â• 1 

i‚ààS ‚àáfi((cid:101)x) where S is a uniform random subset of [n] with |S| = B;

(cid:5) T (cid:48) epochs each of length B
(cid:5) p sub-epochs each of length m

n

‚àÖ

(cid:80)

  B  T (cid:48)  Œ±)

1: (cid:98)x ‚Üê x

i=1 fi(x)  starting vector x

‚àÖ; p ‚Üê Œò((œÉ/ŒµL)2/3); m ‚Üê B/p; X ‚Üê [];

Algorithm 1 Natasha1.5(F  x
Input: f (¬∑) = 1

(cid:80)n
(cid:101)x ‚Üê(cid:98)x; ¬µ ‚Üê 1
x0 ‚Üê(cid:98)x; X ‚Üê [X (cid:98)x];
(cid:101)‚àá ‚Üê ‚àáfi(xt) ‚àí ‚àáfi((cid:101)x) + ¬µ + 2œÉ(xt ‚àí(cid:98)x) where i ‚ààR [n]
xt+1 = xt ‚àí Œ±(cid:101)‚àá;
(cid:98)x ‚Üê a random choice from {x0  x1  . . .   xm‚àí1};

learning rate Œ± > 0.
2: for k ‚Üê 1 to T (cid:48) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end for

for t ‚Üê 0 to m ‚àí 1 do

for s ‚Üê 0 to p ‚àí 1 do

end for

B

end for

(cid:5) in practice  choose the average
13: (cid:98)y ‚Üê a random vector in X.
(cid:5) in practice  simply return(cid:98)y
14: g(x) := f (x) + œÉ(cid:107)x ‚àí(cid:98)y(cid:107)2 and use convex SGD to minimize g(x) for Tsgd = T (cid:48)B iterations.

15: return xout ‚Üê the output of SGD.

we shall add back such dependency and as well as support the existence of a proximal term. (That
is  to minimize œà(x) + f (x) where œà(x) is a proper convex simple function.)
Under such simpliÔ¨Åed notations  our main theorem can be stated as follows.
Theorem 1 (simple). Under (A1)  (A2) and (A3)  using the big-O notion to hide dependency in ‚àÜf
and V  we have for every Œµ ‚àà (0  œÉ

(cid:16) L2/3œÉ1/3

(cid:17)

L ]  letting
  T = Œò
  B  T /B  Œ±) outputs a point xout with E[(cid:107)‚àáf (xout)(cid:107)] ‚â§ Œµ  and

and Œ± = Œò

œÉ1/3L2/3

Œµ10/3

‚àÖ

(cid:16) Œµ4/3

(cid:17)

B = Œò(cid:0) 1

Œµ2

(cid:1)

we have that Natasha1.5(f  x
needs O(T ) computations of stochastic gradients. (See also Figure 3(b).)

We emphasize that the additional factor œÉ1/3 in the numerator of T shall become our key to achieve
faster algorithm for Ô¨Ånding approximate local minima in Section 4. Also  if the requirement Œµ ‚â§ œÉ

is not satisÔ¨Åed  one can replace œÉ with ŒµL; accordingly  T becomes O(cid:0) L
(cid:1)
We note that the SGD4 method of [5] (which appeared after this paper) achieves T = O(cid:0) L

It is better than Natasha1.5 only when œÉ ‚â§ ŒµL. We compare them in Figure 3(b)  and emphasize
that it is necessary to use Natasha1.5 (rather than SGD4) to design Natasha2 of the next section.

Œµ3 + L2/3œÉ1/3

Œµ2 + œÉ
Œµ4

(cid:1).

Œµ10/3

L

Extension.
In fact  we show Theorem 1 in a more general proximal setting. That is  to minimize
F (x) := f (x)+œà(x) where œà(x) is proper convex function that can be non-smooth. For instance  if
œà(x) is the indicator function of a convex set  then Problem (1.1) becomes constraint minimization;
and if œà(x) = (cid:107)x(cid:107)1  we encourage sparsity. At a Ô¨Årst reading of its proof  one can assume œà(x) ‚â° 0.

3.2 Our Intuition

We Ô¨Årst recall the main idea of the SVRG method [28  48]  which is an ofÔ¨Çine algorithm. SVRG divides

iterations into epochs  each of length n. It maintains a snapshot point(cid:101)x for each epoch  and computes
the full gradient ‚àáf ((cid:101)x) only for snapshots. Then  in each iteration t at point xt  SVRG deÔ¨Ånes
gradient estimator (cid:101)‚àáf (xt) := ‚àáfi(xt) ‚àí ‚àáfi((cid:101)x) + ‚àáf ((cid:101)x) which satisÔ¨Åes Ei[(cid:101)‚àáf (xt)] = ‚àáf (xt) 
and performs proximal update xt+1 ‚Üê xt ‚àí Œ±(cid:101)‚àáf (xt) for learning rate Œ±.

For minimizing non-convex functions  SVRG does not take advantage of parameter œÉ even if the
learning rate can be adapted to œÉ. This is because SVRG (and in fact SGD and GD too) rely on
gradient-descent analysis to argue for objective decrease per iteration. This is blind to œÉ.9

9These results argue for objective decrease per iteration  of the form f (xt) ‚àí f (xt+1) ‚â• Œ±

E(cid:2)(cid:107)‚àáf (xt) ‚àí (cid:101)‚àáf (xt)(cid:107)2(cid:3). Unlike mirror-descent analysis  this inequality cannot take advantage of the

2 (cid:107)‚àáf (xt)(cid:107)2 ‚àí

Œ±2L

2

6

The prior work Natasha1 takes advantage of œÉ. Natasha1 is similar to SVRG  but it further divides

each epoch into sub-epochs  each with a starting vector(cid:98)x. Then  it replaces (cid:101)‚àáf (xt) with (cid:101)‚àáf (xt) +
2œÉ(xt ‚àí(cid:98)x). This is equivalent to replacing f (x) with f (x) + œÉ(cid:107)x‚àí(cid:98)x(cid:107)2  where the center(cid:98)x changes
every sub-epoch. We view this additional term 2œÉ(xt ‚àí(cid:98)x) as a type of retraction. Conceptually 
requires the full gradient computation ‚àáf ((cid:101)x) at snapshots(cid:101)x. A natural Ô¨Åx ‚Äîoriginally studied by
practitioners but Ô¨Årst formally analyzed by Lei et al. [30]‚Äî is to replace the computation of ‚àáf ((cid:101)x)
(cid:80)
i‚ààS ‚àáfi((cid:101)x)  for a random batch S ‚äÜ [n] with Ô¨Åxed cardinality B := |S| (cid:28) n. This

it stabilizes the algorithm by moving a bit in the backward direction. Technically  it enables us to
perform only mirror-descent type of analysis  and thus bypass the issue of SVRG.
Our Algorithm. Both SVRG and Natasha1 are ofÔ¨Çine methods  because the gradient estimator

with 1|S|
allows us to shorten the epoch length from n to B  thus turning SVRG and Natasha1 into online
methods.
How large should we pick B? By Chernoff bound  we wish B ‚âà 1
Œµ2 because our desired accuracy
is Œµ. One can thus hope to replace the parameter n in the complexities of SVRG and Natasha1.5
(ignoring the dependency on L):

T = O(cid:0)n +
(cid:1)
T = O(cid:0)Œµ‚àí10/3(cid:1)

n2/3
Œµ2

Œµ2 . This ‚Äúwishful thinking‚Äù gives
and

with B ‚âà 1

(cid:1)

œÉ1/3n2/3

and

Œµ2

n1/2
Œµ2 +

T = O(cid:0)n +
T = O(cid:0)Œµ‚àí3 + œÉ1/3Œµ‚àí10/3(cid:1).
(cid:80)
i‚ààS ‚àáfi((cid:101)x) ‚àí ‚àáf ((cid:101)x)

These are exactly the results achieved by SCSG [30] and to be achieved by our new Natasha1.5.
Unfortunately  Chernoff bound itself is not sufÔ¨Åcient in getting such rates. Let

e := 1|S|

denote the bias of this new gradient estimator  then when performing iterative updates  this bias
e gives rise to two types of error terms: ‚ÄúÔ¨Årst-order error‚Äù terms ‚Äîof the form (cid:104)e  x ‚àí y(cid:105)‚Äî and
‚Äúsecond-order error‚Äù term (cid:107)e(cid:107)2. Chernoff bound ensures that the second-order error ES[(cid:107)e(cid:107)2] ‚â§ Œµ2
is bounded. However  Ô¨Årst-order error terms are the true bottlenecks.
In the ofÔ¨Çine method SCSG  Lei et al. [30] carefully performed updates so that all ‚ÄúÔ¨Årst-order errors‚Äù
cancel out. To the best of our knowledge  this analysis cannot take advantage of œÉ even if the
algorithm knows œÉ. (Again  for experts  this is because SCSG is based on gradient-descent type of
analysis but not mirror-descent.)
In Natasha1.5  we use the aforementioned retraction to ensure that all points in a single sub-epoch
are close to each other (based on mirror-descent type of analysis). Then  we use Young‚Äôs inequality
2(cid:107)x ‚àí y(cid:107)2. In this equation  (cid:107)e(cid:107)2 is already bounded by Chernoff
to bound (cid:104)e  x ‚àí y(cid:105) by 1
concentration  and (cid:107)x ‚àí y(cid:107)2 can also be bounded as long as x and y are within the same sub-epoch.
This summarizes the high-level technical contribution of Natasha1.5.
We formally state Natasha1.5 in Algorithm 1  and it uses big-O notions to hide dependency in L 
‚àÜf   and V. The more general code to take care of the proximal term is in Algorithm 3 of Section 6.

2(cid:107)e(cid:107)2 + 1

4 Natasha 2: Finding Approximate Local Minima

Stochastic gradient descent (SGD) Ô¨Ånd approximate local minima [22]  under (A1)  (A2) and an
additional assumption (A4):

f (x) is second-order L2-Lipschitz smooth  meaning (cid:107)‚àá2f (x) ‚àí ‚àá2f (y)(cid:107)2 ‚â§ L2 ¬∑ (cid:107)x ‚àí y(cid:107).

(A4)
Remark 4.1. (A4) is necessary to make the task of Ô¨Ånd appx. local minima meaningful  for the same
reason Lipschitz smoothness was needed for Ô¨Ånding stationary points.
DeÔ¨Ånition 4.2. We say x is an (Œµ  Œ¥)-approximate local minimum of f (x) if10

(cid:107)‚àáf (x)(cid:107) ‚â§ Œµ and ‚àá2f (x) (cid:23) ‚àíŒ¥I  

bounded nonconvexity parameter of f (x). For readers interested in the difference between gradient and mirror
descent  see [11].

10The notion ‚Äú‚àá2f (x) (cid:23) ‚àíŒ¥I‚Äù means all the eigenvalues of ‚àá2f (x) are above ‚àíŒ¥.

7

or Œµ-approximate local minimum if it is (Œµ  Œµ1/C)-approximate local minimum for constant C ‚â• 1.

Before our work  Ge et al. [22] is the only result that gives provable online complexity for Ô¨Ånding
approximate local minima. Other previous results  including SVRG  SCSG  Natasha1  and even
Natasha1.5  do not Ô¨Ånd approximate local minima and may be stuck at saddle points.11 Ge et al.
[22] showed that  hiding factors that depend on L  L2 and V  SGD Ô¨Ånds an Œµ-approximate local
minimum of f (x) in gradient complexity T = O(poly(d)Œµ‚àí4). This Œµ‚àí4 factor seems necessary
since SGD needs T ‚â• ‚Ñ¶(Œµ‚àí4) for just Ô¨Ånding stationary points (see Appendix B and Table 1).
Remark 4.3. OfÔ¨Çine methods are often studied under (Œµ  Œµ1/2)-approximate local minima. In the
Œµ4 +

online setting  Ge et al. [22] used (Œµ  Œµ1/4)-approximate local minima  thus giving T = O(cid:0) poly(d)
(cid:1). In general  it is better to treat Œµ and Œ¥ separately to be more general  but nevertheless 

poly(d)

(Œµ  Œµ1/C)-approximate local minima are always better than Œµ-approximate stationary points.

Œ¥16

4.1 Our Theorem

We propose a new method Natasha2full which  very informally speaking  alternatively
‚Ä¢ Ô¨Ånds approximate stationary points of f (x) using Natasha1.5  or
‚Ä¢ Ô¨Ånds negative curvature of the Hessian ‚àá2f (x)  using Oja‚Äôs online eigenvector algorithm.
In this section  we deÔ¨Åne gradient complexity T to be the number of stochastic gradient computa-
tions plus Hessian-vector products. Let ‚àÜf be any upper bound on f (x0) ‚àí f (x‚àó) where x0 is the
starting point. In this section  to present the simplest results  we use the big-O notion to hide de-
pendency in L  L2  ‚àÜf   and V. In Section 7  we shall add back such dependency for a more general
description of the algorithm. Our main result can be stated as follows:
Theorem 2 (informal). Under (A1)  (A2) and (A4)  for any Œµ ‚àà (0  1) and Œ¥ ‚àà (0  Œµ1/4) 
Natasha2(f  y0  Œµ  Œ¥) outputs a point xout so that  with probability at least 2/3:

Furthermore  its gradient complexity is T = (cid:101)O(cid:0) 1
Remark 4.4. If Œ¥ > Œµ1/4 we can replace it with Œ¥ = Œµ1/4. Therefore  T = (cid:101)O(cid:0) 1
(cid:1).
Corollary 4.6. T = (cid:101)O(Œµ‚àí3.25) for Ô¨Ånding (Œµ  Œµ1/4)-approximate local minima. This is better than
Corollary 4.7. T = (cid:101)O(Œµ‚àí3.5) for Ô¨Ånding (Œµ  Œµ1/2)-approximate local minima. This was not known

Remark 4.5. The follow-up work [10] replaced Hessian-vector products in Natasha2 with only
stochastic gradient computations  turning Natasha2 into a pure Ô¨Årst-order method. That paper is
built on ours and thus all the proofs of this paper are still needed.

T = O(Œµ‚àí10/3) of SCSG for Ô¨Ånding only Œµ-approximate stationary points.

(cid:107)‚àáf (xout)(cid:107) ‚â§ Œµ and ‚àá2f (xout) (cid:23) ‚àíŒ¥I .

(cid:1) .12

before and is matched by several follow-up works using different algorithms [5  10  44  46].

Œ¥5 + 1
Œ¥Œµ3

Œ¥5 + 1

Œ¥Œµ3 + 1
Œµ3.25

4.2 Our Intuition

It is known that the problem of Ô¨Ånding (Œµ  Œ¥)-approximate local minima  at a high level  ‚Äúreduces‚Äù
to (repeatedly) Ô¨Ånding Œµ-approximate stationary points for an O(Œ¥)-nonconvex function [1  13].
SpeciÔ¨Åcally  Carmon et al. [13] proposed the following procedure. In every iteration at point yk 
detect whether the minimum eigenvalue of ‚àá2f (yk) is below ‚àíŒ¥:
‚Ä¢ if yes  Ô¨Ånd the minimum eigenvector of ‚àá2f (yk) approximately and move in this direction.

‚Ä¢ if no  let F k(x) := f (x)+L(cid:0) max(cid:8)0 (cid:107)x‚àíyk(cid:107)‚àí Œ¥
F k(x) penalizes us from moving out of the ‚Äúsafe zone‚Äù of(cid:8)x : (cid:107)x ‚àí yk(cid:107) ‚â§ Œ¥

(cid:9)(cid:1)2  which can be proven as 5L-smooth and

3Œ¥-nonconvex; then Ô¨Ånd an Œµ-approximate stationary point of F k(x) to move there. Intuitively 

(cid:9).

L2

L2

11These methods are based on the ‚Äúvariance reduction‚Äù technique to reduce the random noise of SGD. They
have been criticized by practitioners for performing poorer than SGD on training neural networks  because the
noise of SGD allows it to escape from saddle points. Variance-reduction based methods have less noise and
thus cannot escape from saddle points.
(namely  n  d  L  L2 V  1/Œµ  1/Œ¥).

12Throughout this paper  we use the (cid:101)O notion to hide at most one logarithmic factor in all the parameters

8

(cid:80)n
2: else (cid:101)L ‚Üê 1 and(cid:101)œÉ ‚Üê Œò(cid:0) Œµ

Algorithm 2 Natasha2(f  y0  Œµ  Œ¥)
Input: function f (x) = 1
n
1: if Œµ1/3
3: X ‚Üê [];
4: for k ‚Üê 0 to ‚àû do
5:

Œ¥ ‚â• 1 then (cid:101)L =(cid:101)œÉ ‚Üê Œò( Œµ1/3
(cid:1) ‚àà [Œ¥  1].
Apply Oja‚Äôs algorithm to Ô¨Ånd minEV v of ‚àá2f (yk) for(cid:101)Œò( 1

Œ¥ ) ‚â• 1;

Œ¥3

i=1 fi(x)  starting vector y0  target accuracy Œµ > 0 and Œ¥ > 0.

(cid:5) the boundary case for large L2

Œ¥2 ) iterations (cid:5) see Lemma 5.3

(cid:5) it satisÔ¨Åes ‚àá2f (yk) (cid:23) ‚àíŒ¥I

(cid:5) F k(¬∑) is(cid:101)L-smooth and(cid:101)œÉ-nonconvex

L2

else

if v ‚àà Rd is found s.t. v(cid:62)‚àá2f (yk)v ‚â§ ‚àí Œ¥

2 then
yk+1 ‚Üê yk ¬± Œ¥
v where the sign is random.
F k(x) := f (x) + L(max{0 (cid:107)x ‚àí yk(cid:107) ‚àí Œ¥

6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end for
17: deÔ¨Åne convex function g(x) := f (x) + L(max{0 (cid:107)x ‚àí y(cid:107) ‚àí Œ¥

run Natasha1.5(cid:0)F k  yk  Œò(Œµ‚àí2)  1  Œò(ŒµŒ¥)(cid:1).
let(cid:98)yk  yk+1 be the vector(cid:98)y and(cid:98)x when Line 13 is reached in Natasha1.5.
X ‚Üê [X  (yk (cid:98)yk)];
break the for loop if have performed Œò(cid:0) 1
16: (y (cid:98)y) ‚Üê a random pair in X.
18: use SGD to minimize g(x) for(cid:101)Œò( 1

(cid:1) Ô¨Årst-order steps.

Œµ2 ) steps and output xout.

end if

})2.

L2

Œ¥Œµ

(cid:5) in practice  simply output(cid:98)yk
})2 +(cid:101)œÉ(cid:107)x ‚àí(cid:98)y(cid:107)2.

L2

Previously  it was thought necessary to achieve high accuracy for both tasks above. This is
why researchers have only been able to design ofÔ¨Çine methods: in particular  the shift-and-invert
method [21] was applied to Ô¨Ånd the minimum eigenvector  and repeatSVRG was applied to Ô¨Ånd a
stationary point of F k(x).13
In this paper  we apply efÔ¨Åcient online algorithms for the two tasks: namely  Oja‚Äôs algorithm
(see Section 5.1) for Ô¨Ånding minimum eigenvectors  and our new Natasha1.5 algorithm (see
Section 3.2) for Ô¨Ånding stationary points. More speciÔ¨Åcally  for Oja‚Äôs  we only decide if there is
an eigenvalue below threshold ‚àíŒ¥/2  or conclude that the Hessian has all eigenvalues above ‚àíŒ¥.
This can be done in an online fashion using O(Œ¥‚àí2) Hessian-vector products (with high probability)
using Oja‚Äôs algorithm. For Natasha1.5  we only apply it for a single epoch of length B = Œò(Œµ‚àí2).
Conceptually  this shall make the above procedure online and run in a complexity independent of n.
Unfortunately  technical issues arise in this ‚Äúwishful thinking.‚Äù
Most notably  the above process Ô¨Ånishes only if Natasha1.5 Ô¨Ånds an approximate stationary point
inside the safe zone and therefore (cid:107)‚àáF k(x)(cid:107) ‚â§ Œµ also implies (cid:107)‚àáf (x)(cid:107) ‚â§ 2Œµ.
What can we do if we move out of the safe zone? To tackle this case  we show an additional property
of Natasha1.5 (see Lemma 6.5). That is  the amount of objective decrease ‚Äîi.e.  f (yk) ‚àí f (x) if
x moves out of the safe zone‚Äî must be proportional to the distance (cid:107)x ‚àí yk(cid:107)2 we travel in space.
Therefore  if x moves out of the safe zone  then we can decrease sufÔ¨Åciently the objective. This is
also a good case. This summarizes some high-level technical ingredient of Natasha2.
We formally state Natasha2 in Algorithm 2  and it uses the big-O notion to hide dependency in L 
L2  V and ‚àÜf . The more general code to take care of all the parameters can be found in Algorithm 5
of Section 7.

x of F k(x) that is also inside the safe zone(cid:8)x : (cid:107)x ‚àí yk(cid:107) ‚â§ Œ¥

Finally  we stress that although we borrowed the construction of f (x)+L(cid:0) max(cid:8)0 (cid:107)x‚àíyk(cid:107)‚àí Œ¥

(cid:9). This is because F k(x) = f (x)

(cid:9)(cid:1)2

from the ofÔ¨Çine algorithm of Carmon et al. [13]  our Natasha2 algorithm and analysis are different
from them in all other aspects.

L2

L2

13repeatSVRG is an ofÔ¨Çine algorithm  and Ô¨Ånds an Œµ-approximate stationary point for a function f (x) that
is œÉ-nonconvex. It is divided into stages. In each stage t  it considers a modiÔ¨Åed function ft(x) := f (x) +
œÉ(cid:107)x ‚àí xt(cid:107)2  and then apply the accelerated SVRG method to minimize ft(x). Then  it moves to xt+1 which is
a sufÔ¨Åciently accurate minimizer of ft(x).

9

References
[1] Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Finding

Approximate Local Minima for Nonconvex Optimization in Linear Time. In STOC  2017.

[2] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods.

In STOC  2017.

[3] Zeyuan Allen-Zhu. Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-

Convex Parameter. In ICML  2017.

[4] Zeyuan Allen-Zhu. Katyusha X: Practical Momentum Method for Stochastic Sum-of-

Nonconvex Optimization. In ICML  2018.

[5] Zeyuan Allen-Zhu. How To Make the Gradients Small Stochastically: Even Faster Convex

and Nonconvex SGD. In NeurIPS  2018.

[6] Zeyuan Allen-Zhu and Elad Hazan. Variance Reduction for Faster Non-Convex Optimization.

In ICML  2016.

[7] Zeyuan Allen-Zhu and Yuanzhi Li. LazySVD: Even Faster SVD Decomposition Yet Without

Agonizing Pain. In NeurIPS  2016.

[8] Zeyuan Allen-Zhu and Yuanzhi Li. First EfÔ¨Åcient Convergence for Streaming k-PCA: a Global 

Gap-Free  and Near-Optimal Rate. In FOCS  2017.

[9] Zeyuan Allen-Zhu and Yuanzhi Li. Follow the Compressed Leader: Faster Online Learning of

Eigenvectors and Faster MMWU. In ICML  2017.

[10] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding Local Minima via First-Order Oracles. In

NeurIPS  2018.

[11] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear Coupling: An Ultimate UniÔ¨Åcation of Gra-
In Proceedings of the 8th Innovations in Theoretical Computer

dient and Mirror Descent.
Science  ITCS ‚Äô17  2017.

[12] Sanjeev Arora  Rong Ge  Tengyu Ma  and Ankur Moitra. Simple  EfÔ¨Åcient  and Neural Algo-

rithms for Sparse Coding. In COLT  2015.

[13] Yair Carmon  John C. Duchi  Oliver Hinder  and Aaron Sidford. Accelerated Methods for

Non-Convex Optimization. ArXiv e-prints  abs/1611.00756  November 2016.

[14] Yair Carmon  Oliver Hinder  John C. Duchi  and Aaron Sidford. ‚ÄùConvex Until Proven Guilty‚Äù:
Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions. In ICML  2017.

[15] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly
In Advances in Neural Information Processing Systems 

as easy as solving linear systems.
pages 739‚Äì747  2015.

[16] Anna Choromanska  Mikael Henaff  Michael Mathieu  G¬¥erard Ben Arous  and Yann LeCun.

The loss surfaces of multilayer networks. In AISTATS  2015.

[17] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In NeurIPS  pages 2933‚Äì2941  2014.

[18] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient

Method With Support for Non-Strongly Convex Composite Objectives. In NeurIPS  2014.

[19] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research  12:2121‚Äì2159  2011.

[20] Roy Frostig  Rong Ge  Sham M. Kakade  and Aaron Sidford. Un-regularizing: approximate
In ICML 

proximal point and faster stochastic algorithms for empirical risk minimization.
2015.

10

[21] Dan Garber  Elad Hazan  Chi Jin  Sham M. Kakade  Cameron Musco  Praneeth Netrapalli 
and Aaron Sidford. Robust shift-and-invert preconditioning: Faster and more sample efÔ¨Åcient
algorithms for eigenvector computation. In ICML  2016.

[22] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points‚Äîonline
stochastic gradient for tensor decomposition. In Proceedings of the 28th Annual Conference
on Learning Theory  COLT 2015  2015.

[23] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear
and stochastic programming. Mathematical Programming  pages 1‚Äì26  feb 2015. ISSN 0025-
5610.

[24] I. J. Goodfellow  O. Vinyals  and A. M. Saxe. Qualitatively characterizing neural network

optimization problems. ArXiv e-prints  December 2014.

[25] Elad Hazan  KÔ¨År Yehuda Levy  and Shai Shalev-Shwartz. On graduated optimization for
In International Conference on Machine Learning  pages

stochastic non-convex problems.
1833‚Äì1841  2016.

[26] Xi He  Dheevatsa Mudigere  Mikhail Smelyanskiy  and Martin Tak¬¥aÀác. Distributed Hessian-

Free Optimization for Deep Neural Network. ArXiv e-prints  abs/1606.00511  June 2016.

[27] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to Escape

Saddle Points EfÔ¨Åciently. In ICML  2017.

[28] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems  NeurIPS 2013  pages
315‚Äì323  2013.

[29] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ArXiv e-prints 

abs/1412.6980  12 2014.

[30] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex Finite-Sum Optimization

Via SCSG Methods. In NeurIPS  2017.

[31] Yuanzhi Li and Yang Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU

Activation. In NeurIPS  2017.

[32] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A Universal Catalyst for First-Order Opti-

mization. In NeurIPS  2015.

[33] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  vol-

ume I. Kluwer Academic Publishers  2004. ISBN 1402075537.

[34] Yurii Nesterov. Accelerating the cubic regularization of newton‚Äôs method on convex problems.

Mathematical Programming  112(1):159‚Äì181  2008.

[35] Yurii Nesterov. How to make the gradients small. Optima  88:10‚Äì11  2012.

[36] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177‚Äì205  2006.

[37] Erkki Oja. SimpliÔ¨Åed neuron model as a principal component analyzer. Journal of mathemat-

ical biology  15(3):267‚Äì273  1982.

[38] Sashank J. Reddi  Ahmed Hefny  Suvrit Sra  Barnabas Poczos  and Alex Smola. Stochastic

variance reduction for nonconvex optimization. In ICML  2016.

[39] Sashank J Reddi  Manzil Zaheer  Suvrit Sra  Barnabas Poczos  Francis Bach  Ruslan Salakhut-
dinov  and Alexander J Smola. A generic approach for escaping saddle points. ArXiv e-prints 
abs/1709.01434  September 2017.

[40] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing Ô¨Ånite sums with the stochastic

average gradient. ArXiv e-prints  abs/1309.2388  September 2013.

11

[41] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and

Trends in Machine Learning  4(2):107‚Äì194  2012. ISSN 1935-8237.

[42] Shai Shalev-Shwartz. SDCA without Duality  Regularization  and Individual Convexity. In

ICML  2016.

[43] Ruoyu Sun and Zhi-Quan Luo. Guaranteed Matrix Completion via Nonconvex Factorization.

In FOCS  2015.

[44] Nilesh Tripuraneni  Mitchell Stern  Chi Jin  Jeffrey Regier  and Michael I Jordan. Stochas-
tic Cubic Regularization for Fast Nonconvex Optimization. ArXiv e-prints  abs/1711.02838 
November 2017.

[45] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance

Reduction. SIAM Journal on Optimization  24(4):2057‚Äî-2075  2014.

[46] Yi Xu and Tianbao Yang. First-order Stochastic Algorithms for Escaping From Saddle Points

in Almost Linear Time. ArXiv e-prints  abs/1711.01944  November 2017.

[47] Matthew D Zeiler. ADADELTA: an adaptive learning rate method.

abs/1212.5701  12 2012.

ArXiv e-prints 

[48] Lijun Zhang  Mehrdad Mahdavi  and Rong Jin. Linear convergence with condition number
independent access of full gradients. In Advances in Neural Information Processing Systems 
pages 980‚Äì988  2013.

12

,Bo Xie
Yingyu Liang
Le Song
Shantanu Jain
Martha White
Predrag Radivojac
Zeyuan Allen-Zhu