2014,An Accelerated Proximal Coordinate Gradient Method,We develop an accelerated randomized proximal coordinate gradient (APCG) method  for solving a broad class of composite convex optimization problems. In particular  our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods. We show how to apply the APCG method to solve the dual of the regularized empirical risk minimization (ERM) problem  and devise efficient implementations that can avoid full-dimensional vector operations. For ill-conditioned ERM problems  our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (SDCA) method.,An Accelerated Proximal Coordinate Gradient Method

Qihang Lin

University of Iowa
Iowa City  IA  USA

qihang-lin@uiowa.edu

Zhaosong Lu

Simon Fraser University

Burnaby  BC  Canada
zhaosong@sfu.ca

Lin Xiao

Microsoft Research
Redmond  WA  USA

lin.xiao@microsoft.com

Abstract

We develop an accelerated randomized proximal coordinate gradient (APCG)
method  for solving a broad class of composite convex optimization problems.
In particular  our method achieves faster linear convergence rates for minimizing
strongly convex functions than existing randomized proximal coordinate gradi-
ent methods. We show how to apply the APCG method to solve the dual of the
regularized empirical risk minimization (ERM) problem  and devise efﬁcient im-
plementations that avoid full-dimensional vector operations. For ill-conditioned
ERM problems  our method obtains improved convergence rates than the state-of-
the-art stochastic dual coordinate ascent (SDCA) method.

1

Introduction

Coordinate descent methods have received extensive attention in recent years due to their potential
for solving large-scale optimization problems arising from machine learning and other applications.
In this paper  we develop an accelerated proximal coordinate gradient (APCG) method for solving
convex optimization problems with the following form:

where f is differentiable on dom (Ψ)  and Ψ has a block separable structure. More speciﬁcally 

minimize

x∈RN

(cid:8)F (x) def= f (x) + Ψ(x)(cid:9) 

(1)

(2)

Ψ(x) =

Ψi(xi) 

n

Xi=1

where each xi denotes a sub-vector of x with cardinality Ni  and each Ψi : RNi → R ∪ {+∞}
is a closed convex function. We assume the collection {xi : i = 1  . . .   n} form a partition of
the components of x ∈ RN . In addition to the capability of modeling nonsmooth regularization
terms such as Ψ(x) = λkxk1  this model also includes optimization problems with block separable
constraints. More precisely  each block constraint xi ∈ Ci  where Ci is a closed convex set  can be
modeled by an indicator function deﬁned as Ψi(xi) = 0 if xi ∈ Ci and ∞ otherwise.
At each iteration  coordinate descent methods choose one block of coordinates xi to sufﬁciently
reduce the objective value while keeping other blocks ﬁxed. One common and simple approach
for choosing such a block is the cyclic scheme. The global and local convergence properties of the
cyclic coordinate descent method have been studied in  for example  [21  11  16  2  5]. Recently 
randomized strategies for choosing the block to update became more popular. In addition to its the-
oretical beneﬁts [13  14  19]  numerous experiments have demonstrated that randomized coordinate
descent methods are very powerful for solving large-scale machine learning problems [3  6  18  19].

Inspired by the success of accelerated full gradient methods (e.g.  [12  1  22])  several recent work
applied Nesterov’s acceleration schemes to speed up randomized coordinate descent methods. In
particular  Nesterov [13] developed an accelerated randomized coordinate gradient method for min-
imizing unconstrained smooth convex functions  which corresponds to the case of Ψ(x) ≡ 0 in (1).

1

Lu and Xiao [10] gave a sharper convergence analysis of Nesterov’s method  and Lee and Sid-
ford [8] developed extensions with weighted random sampling schemes. More recently  Fercoq
and Richt´arik [4] proposed an APPROX (Accelerated  Parallel and PROXimal) coordinate descent
method for solving the more general problem (1) and obtained accelerated sublinear convergence
rates  but their method cannot exploit the strong convexity to obtain accelerated linear rates.

In this paper  we develop a general APCG method that achieves accelerated linear convergence
rates when the objective function is strongly convex. Without the strong convexity assumption  our
method recovers the APPROX method [4]. Moreover  we show how to apply the APCG method to
solve the dual of the regularized empirical risk minimization (ERM) problem  and devise efﬁcient
implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems 
our method obtains faster convergence rates than the state-of-the-art stochastic dual coordinate as-
cent (SDCA) method [19]  and the improved iteration complexity matches the accelerated SDCA
method [20]. We present numerical experiments to illustrate the advantage of our method.

1.1 Notations and assumptions

For any partition of x ∈ RN into {xi ∈ RNi
matrix U partitioned as U = [U1 ··· Un]  where Ui ∈ RN×Ni   such that

: i = 1  . . .   n}  there is an N × N permutation

n

and xi = U T

i x 

i = 1  . . .   n.

For any x ∈ RN   the partial gradient of f with respect to xi is deﬁned as

i ∇f (x) 

i = 1  . . .   n.

x =

Uixi 

Xi=1
∇if (x) = U T

We associate each subspace RNi   for i = 1  . . .   n  with the standard Euclidean norm  denoted
by k · k. We make the following assumptions which are standard in the literature on coordinate
descent methods (e.g.  [13  14]).
Assumption 1. The gradient of function f is block-wise Lipschitz continuous with constants Li  i.e. 

k∇if (x + Uihi) − ∇if (x)k ≤ Likhik 

∀ hi ∈ RNi  

i = 1  . . .   n 

x ∈ RN .

For convenience  we deﬁne the following norm in the whole space RN :

kxkL = (cid:18) n
Xi=1

Likxik2(cid:19)1/2

 

∀ x ∈ RN .

(3)

Assumption 2. There exists µ ≥ 0 such that for all y ∈ RN and x ∈ dom (Ψ) 

f (y) ≥ f (x) + h∇f (x)  y − xi +

µ
2ky − xk2
L.

The convexity parameter of f with respect to the norm k · kL is the largest µ such that the above
inequality holds. Every convex function satisﬁes Assumption 2 with µ = 0. If µ > 0  the function f
is called strongly convex.

We note that an immediate consequence of Assumption 1 is

f (x + Uihi) ≤ f (x) + h∇if (x)  hii +
This together with Assumption 2 implies µ ≤ 1.

Li
2 khik2 

2 The APCG method

∀ hi ∈ RNi  

i = 1  . . .   n 

x ∈ RN .

(4)

In this section we describe the general APCG method  and several variants that are more suitable
for implementation under different assumptions. These algorithms extend Nesterov’s accelerated
gradient methods [12  Section 2.2] to the composite and coordinate descent setting.

We ﬁrst explain the notations used in our algorithms. The algorithms proceed in iterations  with k
being the iteration counter. Lower case letters x  y  z represent vectors in the full space RN   and
x(k)  y(k) and z(k) are their values at the kth iteration. Each block coordinate is indicated with a
subscript  for example  x(k)
represents the value of the ith block of the vector x(k). The Greek letters
α  β  γ are scalars  and αk  βk and γk represent their values at iteration k.

i

2

Algorithm 1: the APCG method

Input: x(0) ∈ dom (Ψ) and convexity parameter µ ≥ 0.
Initialize: set z(0) = x(0) and choose 0 < γ0 ∈ [µ  1].
Iterate: repeat for k = 0  1  2  . . .

1. Compute αk ∈ (0  1

n ] from the equation

and set

n2α2

k = (1 − αk) γk + αkµ 

γk+1 = (1 − αk)γk + αkµ 

βk =

αkµ
γk+1

.

2. Compute y(k) as

y(k) =

1

αkγk + γk+1 (cid:16)αkγkz(k) + γk+1x(k)(cid:17) .

(5)

(6)

(7)

3. Choose ik ∈ {1  . . .   n} uniformly at random and compute

z(k+1) = arg min

x∈RN n nαk

2 (cid:13)(cid:13)x−(1−βk)z(k)−βky(k)(cid:13)(cid:13)
x(k+1) = y(k) + nαk(z(k+1) − z(k)) +

2

L +h∇ik f (y(k))  xiki+Ψik (xik )o.

µ
n

(z(k) − y(k)).

(8)

4. Set

The general APCG method is given as Algorithm 1. At each iteration k  it chooses a random
coordinate ik ∈ {1  . . .   n} and generates y(k)  x(k+1) and z(k+1). One can observe that x(k+1) and
z(k+1) depend on the realization of the random variable

while y(k) is independent of ik and only depends on ξk−1. To better understand this method  we
make the following observations. For convenience  we deﬁne

ξk = {i0  i1  . . .   ik} 

˜z(k+1) = arg min

x∈RN n nαk

2 (cid:13)(cid:13)x − (1 − βk)z(k) − βky(k)(cid:13)(cid:13)

2

L + h∇f (y(k))  x − y(k)i + Ψ(x)o 

(9)

which is a full-dimensional update version of Step 3. One can observe that z(k+1) is updated as

z(k+1)
i

=( ˜z(k+1)

i

(1 − βk)z(k)

i + βky(k)

i

Notice that from (5)  (6)  (7) and (8) we have

if i = ik 
if i 6= ik.

which together with (10) yields

x(k+1) = y(k) + nαk(cid:16)z(k+1) − (1 − βk)z(k) − βky(k)(cid:17)  
=


i + nαk(cid:16)z(k+1)

i (cid:17) + µ
− z(k)

n(cid:16)z(k)

i − y(k)

y(k)
y(k)
i

i (cid:17) if i = ik 
if i 6= ik.

ik

i

x(k+1)
i

(10)

(11)

(12)

That is  in Step 4  we only need to update the block coordinates x(k+1)

and set the rest to be y(k)

.

i

We now state a theorem concerning the expected rate of convergence of the APCG method  whose
proof can be found in the full report [9].
Theorem 1. Suppose Assumptions 1 and 2 hold. Let F ⋆ be the optimal value of problem (1)  and
{x(k)} be the sequence generated by the APCG method. Then  for any k ≥ 0  there holds:
Eξk−1[F (x(k))] − F ⋆ ≤ min((cid:18)1 −
2n + k√γ0(cid:19)2)(cid:16)F (x(0)) − F ⋆ +
γ0
2

0(cid:17)  

R2

where

2n

√µ
n (cid:19)k
  (cid:18)
x⋆∈X ⋆ kx(0) − x⋆kL 

R0

def= min

and X ⋆ is the set of optimal solutions of problem (1).

3

Our result in Theorem 1 improves upon the convergence rates of the proximal coordinate gradient
methods in [14  10]  which have convergence rates on the order of

For n = 1  our result matches exactly that of the accelerated full gradient method in [12  Section 2.2].

O(cid:16)minn(cid:0)1 − µ
n(cid:1)k

 

n

n+ko(cid:17) .

2.1 Two special cases

Here we give two simpliﬁed versions of the APCG method  for the special cases of µ = 0 and
µ > 0  respectively. Algorithm 2 shows the simpliﬁed version for µ = 0  which can be applied to
problems without strong convexity  or if the convexity parameter µ is unknown.

Algorithm 2: APCG with µ = 0

Input: x(0) ∈ dom (Ψ).
Initialize: set z(0) = x(0) and choose α0 ∈ (0  1
n ].
Iterate: repeat for k = 0  1  2  . . .

z(k+1)
ik
and set z(k+1)

= arg minx∈RNn nαkLik

= z(k)

1. Compute y(k) = (1 − αk)x(k) + αkz(k).
2. Choose ik ∈ {1  . . .   n} uniformly at random and compute
(cid:13)(cid:13)x − z(k)
ik (cid:13)(cid:13)
3. Set x(k+1) = y(k) + nαk(z(k+1) − z(k)).
k(cid:17) .
4. Compute αk+1 = 1
k − α2

for all i 6= ik.
2(cid:16)pα4

k + 4α2

2

2

i

i

+ h∇ik f (y(k))  x − y(k)

ik i + Ψik (x)o.

According to Theorem 1  Algorithm 2 has an accelerated sublinear convergence rate  that is

Eξk−1[F (x(k))] − F ⋆ ≤ (cid:18)

2n

2n + knα0(cid:19)2(cid:18)F (x(0)) − F ⋆ +

1
2

R2

0(cid:19) .

With the choice of α0 = 1/n  Algorithm 2 reduces to the APPROX method [4] with single block
update at each iteration (i.e.  τ = 1 in their Algorithm 1).

For the strongly convex case with µ > 0  we can initialize Algorithm 1 with the parameter γ0 = µ 

which implies γk = µ and αk = βk = √µ/n for all k ≥ 0. This results in Algorithm 3.

Algorithm 3: APCG with γ0 = µ > 0

Input: x(0) ∈ dom (Ψ) and convexity parameter µ > 0.
Initialize: set z(0) = x(0) and and α =
Iterate: repeat for k = 0  1  2  . . .

√µ
n .

1. Compute y(k) = x(k)+αz(k)

1+α

.

2. Choose ik ∈ {1  . . .   n} uniformly at random and compute

z(k+1) = arg min

x∈RN n nα

2 (cid:13)(cid:13)x−(1−α)z(k)−αy(k)(cid:13)(cid:13)

3. Set x(k+1) = y(k) + nα(z(k+1) − z(k)) + nα2(z(k) − y(k)).

2

L+h∇ik f (y(k))  xik−y(k)

ik i+Ψik (xik )o.

As a direct corollary of Theorem 1  Algorithm 3 enjoys an accelerated linear convergence rate:

Eξk−1[F (x(k))] − F ⋆ ≤ (cid:18)1 −

√µ
n (cid:19)k

(cid:16)F (x(0)) − F ⋆ +

µ
2

R2

0(cid:17) .

To the best of our knowledge  this is the ﬁrst time such an accelerated rate is obtained for solving
the general problem (1) (with strong convexity) using coordinate descent type of methods.

4

2.2 Efﬁcient implementation

The APCG methods we presented so far all need to perform full-dimensional vector operations at
each iteration. For example  y(k) is updated as a convex combination of x(k) and z(k)  and this
can be very costly since in general they can be dense vectors. Moreover  for the strongly con-
vex case (Algorithms 1 and 3)  all blocks of z(k+1) need to be updated at each iteration  although
only the ik-th block needs to compute the partial gradient and perform a proximal mapping. These
full-dimensional vector updates cost O(N ) operations per iteration and may cause the overall com-
putational cost of APCG to be even higher than the full gradient methods (see discussions in [13]).

In order to avoid full-dimensional vector operations  Lee and Sidford [8] proposed a change of
variables scheme for accelerated coordinated descent methods for unconstrained smooth minimiza-
tion. Fercoq and Richt´arik [4] devised a similar scheme for efﬁcient implementation in the µ = 0
case for composite minimization. Here we show that such a scheme can also be developed for the
case of µ > 0 in the composite optimization setting. For simplicity  we only present an equivalent
implementation of the simpliﬁed APCG method described in Algorithm 3.

Algorithm 4: Efﬁcient implementation of APCG with γ0 = µ > 0

Input: x(0) ∈ dom (Ψ) and convexity parameter µ > 0.
Initialize: set α =
Iterate: repeat for k = 0  1  2  . . .

√µ
n and ρ = 1−α

1+α   and initialize u(0) = 0 and v(0) = x(0).

1. Choose ik ∈ {1  . . .   n} uniformly at random and compute
∆(k)

ik = arg min

k∆k2 + h∇ik f (ρk+1u(k) +v(k))  ∆i + Ψik (−ρk+1u(k)

ik +v(k)

ik +∆)o.

Nik n nαLik

2

∆∈R

2. Let u(k+1) = u(k) and v(k+1) = v(k)  and update

u(k+1)
ik

= u(k)

ik − 1−nα

2ρk+1 ∆(k)
ik  

v(k+1)
ik

= v(k)

ik + 1+nα

2 ∆(k)
ik .

(13)

Output: x(k+1) = ρk+1u(k+1) + v(k+1)

The following Proposition is proved in the full report [9].

Proposition 1. The iterates of Algorithm 3 and Algorithm 4 satisfy the following relationships:

x(k) = ρku(k) + v(k) 

y(k) = ρk+1u(k) + v(k) 

z(k) = −ρku(k) + v(k).

(14)

We note that in Algorithm 4  only a single block coordinate of the vectors u(k) and v(k) are updated
at each iteration  which cost O(Ni). However  computing the partial gradient ∇ik f (ρk+1u(k)+v(k))
may still cost O(N ) in general. In the next section  we show how to further exploit structure in many
ERM problems to completely avoid full-dimensional vector operations.

3 Application to regularized empirical risk minimization (ERM)

Let A1  . . .   An be vectors in Rd  φ1  . . .   φn be a sequence of convex functions deﬁned on R  and g
be a convex function on Rd. Regularized ERM aims to solve the following problem:

minimize

w∈Rd

P (w)  with P (w) =

1
n

n

Xi=1

φi(AT

i w) + λg(w) 

where λ > 0 is a regularization parameter. For example  given a label bi ∈ {±1} for each vector Ai 
for i = 1  . . .   n  we obtain the linear SVM problem by setting φi(z) = max{0  1−biz} and g(w) =
(1/2)kwk2
2. Regularized logistic regression is obtained by setting φi(z) = log(1+exp(−biz)). This
formulation also includes regression problems. For example  ridge regression is obtained by setting
(1/2)φi(z) = (z − bi)2 and g(w) = (1/2)kwk2

2  and we get Lasso if g(w) = kwk1.

5

Let φ∗i be the convex conjugate of φi  that is  φ∗i (u) = maxz∈R(zu − φi(z)). The dual of the

regularized ERM problem is (see  e.g.  [19])

maximize

−φ∗i (−xi) − λg∗(cid:18) 1
where A = [A1  . . .   An]. This is equivalent to minimize F (x) def= −D(x)  that is 

D(x)  with D(x) =

Xi=1

x∈Rn

1
n

λn

Ax(cid:19)  

n

minimize

x∈Rn

F (x) def=

1
n

n

Xi=1

φ∗i (−xi) + λg∗(cid:18) 1

λn

Ax(cid:19) .

The structure of F (x) above matches the formulation in (1) and (2) with f (x) = λg∗(cid:0) 1

λn Ax(cid:1) and
Ψi(xi) = 1
n φ∗i (−xi)  and we can apply the APCG method to minimize F (x). In order to exploit
the fast linear convergence rate  we make the following assumption.
Assumption 3. Each function φi is 1/γ smooth  and the function g has unit convexity parameter 1.

Here we slightly abuse the notation by overloading γ  which also appeared in Algorithm 1. But
in this section it solely represents the (inverse) smoothness parameter of φi. Assumption 3 implies
that each φ∗i has strong convexity parameter γ (with respect to the local Euclidean norm) and g∗
is differentiable and ∇g∗ has Lipschitz constant 1. In the following  we split the function F (x) =
f (x) + Ψ(x) by relocating the strong convexity term as follows:
n

f (x) = λg∗(cid:18) 1

Xi=1(cid:16)φ∗(−xi) −
As a result  the function f is strongly convex and each Ψi is still convex. Now we can apply the
APCG method to minimize F (x) = −D(x)  and obtain the following guarantee.
Theorem 2. Suppose Assumption 3 holds and kAik ≤ R for all i = 1  . . .   n. In order to obtain an
expected dual optimality gap E[D⋆ − D(x(k))] ≤ ǫ by using the APCG method  it sufﬁces to have

2kxik2(cid:17) .

Ax(cid:19) +

γ
2nkxk2 

Ψ(x) =

(15)

1
n

λn

γ

k ≥(cid:16)n +q nR2

λγ (cid:17) log(C/ǫ).

where D⋆ = maxx∈Rn D(x) and the constant C = D⋆ − D(x(0)) + (γ/(2n))kx(0) − x⋆k2.
Proof. The function f (x) in (15) has coordinate Lipschitz constants Li = kAik2
and convexity parameter γ
parameter of f (x) with respect to the norm k · kL deﬁned in(3) is
λn2 = λγn
R2+λγn .
√µ

n ≤ R2+λγn
n with respect to the unweighted Euclidean norm. The strong convexity

n. R2+λγn

λn2 + γ

µ = γ

√µ

λn2

it sufﬁces to have the number of iterations k to be larger than

According to Theorem 1  we have E[D⋆−D(x(0))] ≤(cid:16)1 −
n√µ log(C/ǫ) = nq R2+λγn
log(C/ǫ) = qn2 + nR2

This ﬁnishes the proof.

λγn

n (cid:17)k

C ≤ exp(cid:16)−

λγ log(C/ǫ) ≤ (cid:16)n +q nR2

n k(cid:17) C. Therefore
λγ (cid:17) log(C/ǫ).

(16)

(17)

Several state-of-the-art algorithms for ERM  including SDCA [19]  SAG [15  17] and SVRG [7  23]
obtain the iteration complexity

O(cid:16)(cid:16)n + R2

λγ(cid:17) log(1/ǫ)(cid:17) .

We note that our result in (16) can be much better for ill-conditioned problems  i.e.  when the condi-
tion number R2
λγ is larger than n. This is also conﬁrmed by our numerical experiments in Section 4.

The complexity bound in (17) for the aforementioned work is for minimizing the primal objective
P (x) or the duality gap P (x)− D(x)  but our result in Theorem 2 is in terms of the dual optimality.
In the full report [9]  we show that the same guarantee on accelerated primal-dual convergence can be
obtained by our method with an extra primal gradient step  without affecting the overall complexity.
The experiments in Section 4 illustrate superior performance of our algorithm on reducing the primal
objective value  even without performing the extra step.

6

We note that Shalev-Shwartz and Zhang [20] recently developed an accelerated SDCA method

which achieves the same complexity O(cid:16)(cid:16)n +q n

λγ(cid:17) log(1/ǫ)(cid:17) as our method. Their method calls

the SDCA method in a full-dimensional accelerated gradient method in an inner-outer iteration pro-
cedure. In contrast  our APCG method is a straightforward single loop coordinate gradient method.

3.1

Implementation details

Here we show how to exploit the structure of the regularized ERM problem to efﬁciently compute

the coordinate gradient ∇ik f (y(k))  and totally avoid full-dimensional updates in Algorithm 4. We
focus on the special case g(w) = 1
2kwk2 and show how to compute ∇ik f (y(k)). According to (15) 

∇ik f (y(k)) = 1

λn2 AT

i (Ay(k)) + γ

n y(k)
ik .

Since we do not form y(k) in Algorithm 4  we update Ay(k) by storing and updating two vectors
in Rd: p(k) = Au(k) and q(k) = Av(k). The resulting method is detailed in Algorithm 5.

Algorithm 5: APCG for solving dual ERM

Input: x(0) ∈ dom (Ψ) and convexity parameter µ > 0.
Initialize: set α =
Iterate: repeat for k = 0  1  2  . . .

√µ
n and ρ = 1−α

1+α   and let u(0) = 0  v(0) = x(0)  p(0) = 0 and q(0) = Ax(0).

∇(k)
ik = 1

2. Compute coordinate increment

1. Choose ik ∈ {1  . . .   n} uniformly at random  compute the coordinate gradient
ik (cid:17) .
ik − v(k)

n(cid:16)ρk+1u(k)
n φ∗ik (ρk+1u(k)

ik q(k)(cid:1) + γ
ik   ∆i + 1

k∆k2 + h∇(k)
3. Let u(k+1) = u(k) and v(k+1) = v(k)  and update

ik = arg min

ik p(k) + AT

ik + v(k)

∆(k)

∆∈R

λn2 (cid:0)ρk+1AT
Nik n α(kAikk2+λγn)
2ρk+1 ∆(k)
ik  
2ρk+1 Aik ∆(k)
ik  

= u(k)

2λn

u(k+1)
ik − 1−nα
ik
p(k+1) = p(k) − 1−nα

ik − ∆)o .

= v(k)

v(k+1)
ik + 1+nα
ik
q(k+1) = q(k) + 1+nα

2 ∆(k)
ik  
2 Aik ∆(k)
ik .

(18)

x(k+1) = ρk+1u(k+1) + v(k+1).

Output: approximate primal and dual solutions

w(k+1) = 1

λn(cid:0)ρk+2p(k+1) + q(k+1)(cid:1)  

Each iteration of Algorithm 5 only involves the two inner products AT
ik q(k) in computing
∇(k)
ik and the two vector additions in (18). They all cost O(d) rather than O(n). When the Ai’s are
sparse (the case of most large-scale problems)  these operations can be carried out very efﬁciently.
Basically  each iteration of Algorithm 5 only costs twice as much as that of SDCA [6  19].

ik p(k)  AT

4 Experiments

In our experiments  we solve ERM problems with smoothed hinge loss for binary classiﬁcation.
That is  we pre-multiply each feature vector Ai by its label bi ∈ {±1} and use the loss function

φ(a) =


0
1 − a − γ
2γ (1 − a)2

2

1

if a ≥ 1 
if a ≤ 1 − γ 
otherwise.

The conjugate function of φ is φ∗(b) = b + γ

Ψi(xi) =

1

n(cid:16)φ∗(−xi) −

2 b2 if b ∈ [−1  0] and ∞ otherwise. Therefore we have
2kxik2(cid:17) =(cid:26) −xi
γ

if xi ∈ [0  1]
∞ otherwise.

n

The dataset used in our experiments are summarized in Table 1.

7

λ

rcv1

covertype

news20

10−5

10−6

10−7

10−8

100

10−3

10−6

10−9

10−12

10−15
0

100

10−3

10−6

10−9
0

100

10−1

10−2

10−3

10−4

10−5

10−6
0

100

10−1

10−2

10−3

10−4
0

AFG

SDCA

APCG

20

40

60

80

100

20

40

60

80

100

20

40

60

80

100

AFG

SDCA

APCG

20

40

60

80

100

100

10−3

10−6

10−9

10−12

10−15
0

100

10−3

10−6

10−9
0

100

10−1

10−2

10−3

10−4

10−5

10−6
0

100

10−1

10−2

10−3

10−4
0

AFG

SDCA

APCG

20

40

60

80

100

20

40

60

80

100

20

40

60

80

100

20

40

60

80

100

100

10−3

10−6

10−9

10−12

10−15
0

100

10−3

10−6

10−9
0

100

10−1

10−2

10−3

10−4

10−5

10−6
0

100

10−1

10−2

10−3

10−4

10−5
0

AFG

SDCA

APCG

20

40

60

80

100

20

40

60

80

100

20

40

60

80

100

20

40

60

80

100

Figure 1: Comparing the APCG method with SDCA and the accelerated full gradient method (AFG)

with adaptive line search. In each plot  the vertical axis is the primal objective gap P (w(k))−P ⋆  and
the horizontal axis is the number of passes through the entire dataset. The three columns correspond
to the three datasets  and each row corresponds to a particular value of the regularization parameter λ.

In our experiments  we compare the APCG method with SDCA and the accelerated full gradient
method (AFG) [12] with an additional line search procedure to improve efﬁciency. When the regu-
larization parameter λ is not too small (around 10−4)  then APCG performs similarly as SDCA as
predicted by our complexity results  and they both outperform AFG by a substantial margin.
Figure 1 shows the results in the ill-conditioned setting  with λ varying form 10−5 to 10−8. Here
we see that APCG has superior performance in reducing the primal objective value compared with
SDCA and AFG  even though our theory only gives complexity for solving the dual ERM problem.
AFG eventually catches up for cases with very large condition number (see the plots for λ = 10−8).

datasets
rcv1
covtype
news20

number of samples n number of features d
47 236
54
1 355 191

20 242
581 012
19 996

sparsity
0.16%
22%
0.04%

Table 1: Characteristics of three binary classiﬁcation datasets (available from the LIBSVM web
page: http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets).

8

References

[1] A. Beck and M. Teboulle. A fast iterative shrinkage-threshold algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[2] A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods.

SIAM Journal on Optimization  13(4):2037–2060  2013.

[3] K.-W. Chang  C.-J. Hsieh  and C.-J. Lin. Coordinate descent method for large-scale l2-loss
linear support vector machines. Journal of Machine Learning Research  9:1369–1398  2008.

[4] O. Fercoq and P. Richt´arik. Accelerated  parallel and proximal coordinate descent. Manuscript 

arXiv:1312.5799  2013.

[5] M. Hong  X. Wang  M. Razaviyayn  and Z. Q. Luo. Iteration complexity analysis of block

coordinate descent methods. Manuscript  arXiv:1310.6957  2013.

[6] C.-J. Hsieh  K.-W. Chang  C.-J. Lin  S.-S. Keerthi  and S. Sundararajan. A dual coordinate
descent method for large-scale linear svm. In Proceedings of the 25th International Conference
on Machine Learning (ICML)  pages 408–415  2008.

[7] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems 26  pages 315–323. 2013.

[8] Y. T. Lee and A. Sidford. Efﬁcient accelerated coordinate descent methods and faster algo-

rithms for solving linear systems. arXiv:1305.1922.

[9] Q. Lin  Z. Lu  and L. Xiao. An accelerated proximal coordinate gradient method and its
application to regularized empirical risk minimization. Technical Report MSR-TR-2014-94 
Microsoft Research  2014. (arXiv:1407.1296).

[10] Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent meth-

ods. Accepted by Mathematical Programming  Series A  2014. (arXiv:1305.4723).

[11] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex dif-
ferentiable minimization. Journal of Optimization Theory & Applications  72(1):7–35  2002.

[12] Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  Boston 

2004.

[13] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization  22(2):341–362  2012.

[14] P. Richt´arik and M. Tak´aˇc. Iteration complexity of randomized block-coordinate descent meth-

ods for minimizing a composite function. Mathematical Programming  144(1):1–38  2014.

[15] N. Le Roux  M. Schmidt  and F. Bach. A stochastic gradient method with an exponential
convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems
25  pages 2672–2680. 2012.

[16] A. Saha and A. Tewari. On the non-asymptotic convergence of cyclic coordinate descent

methods. SIAM Jorunal on Optimization  23:576–601  2013.

[17] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. Technical Report HAL 00860051  INRIA  Paris  France  2013.

[18] S. Shalev-Shwartz and A. Tewari. Stochastic methods for ℓ1 regularized loss minimization. In
Proceedings of the 26th International Conference on Machine Learning (ICML)  pages 929–
936  Montreal  Canada  2009.

[19] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  14:567–599  2013.

[20] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. Proceedings of the 31st International Conference on Machine
Learning (ICML)  JMLR W&CP  32(1):64–72  2014.

[21] P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimiza-

tion. Journal of Optimization Theory and Applications  140:513–535  2001.

[22] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Un-

published manuscript  2008.

[23] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance re-
duction. Technical Report MSR-TR-2014-38  Microsoft Research  2014. (arXiv:1403.4699).

9

,Qihang Lin
Zhaosong Lu
Lin Xiao
Martin Slawski
Ping Li
Matthias Hein
Weijiang Yu
Jingwen Zhou
Weihao Yu
Xiaodan Liang
Nong Xiao