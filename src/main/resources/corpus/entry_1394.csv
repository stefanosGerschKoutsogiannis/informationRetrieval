2018,Deep State Space Models for Time Series Forecasting,We present a novel approach to probabilistic time series forecasting that combines state space models with deep learning. By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network  our method retains desired properties of state space models such as data efficiency and interpretability  while making use of the ability to learn complex patterns from raw data offered by deep learning approaches. Our method scales gracefully from regimes where little training data is available to regimes where data from millions of time series can be leveraged to learn accurate models. We provide qualitative as well as quantitative results with the proposed method  showing that it compares favorably to the state-of-the-art.,Deep State Space Models for Time Series Forecasting

Syama Sundar Rangapuram

Matthias Seeger

Jan Gasthaus

Lorenzo Stella

Yuyang Wang

Tim Januschowski

{rangapur  matthis  gasthaus  stellalo  yuyawang  tjnsch}@amazon.com

Amazon Research

Abstract

We present a novel approach to probabilistic time series forecasting that combines
state space models with deep learning. By parametrizing a per-time-series lin-
ear state space model with a jointly-learned recurrent neural network  our method
retains desired properties of state space models such as data efﬁciency and inter-
pretability  while making use of the ability to learn complex patterns from raw data
offered by deep learning approaches. Our method scales gracefully from regimes
where little training data is available to regimes where data from large collection
of time series can be leveraged to learn accurate models. We provide qualitative
as well as quantitative results with the proposed method  showing that it compares
favorably to the state-of-the-art.

1

Introduction

Time series forecasting is a key component in many industrial and business decision processes. A
typical example of such tasks is demand forecasting: accurate and up-to-date models are fundamen-
tal to successful inventory planning and minimization of operational costs.
State space models [8  13  23] (SSMs) provide a principled framework for modeling and learning
time series patterns such as trend and seasonality. Prominent examples include ARIMA models
[4  8] and exponential smoothing [13]. SSMs are particularly well-suited for applications where the
structure of the time series is well-understood  as they allow for the incorporation of structural as-
sumptions into the model. This allows for the model to be interpretable and the associated learning
procedure to be data efﬁcient  but it requires time series with enough history. In modern forecast-
ing applications with large and diverse time series corpora  SSMs require prohibitively labor- and
compute-intensive tasks such as model and covariate selection. Further  traditional SSMs cannot
infer shared patterns from a dataset of similar time series  as they are ﬁtted on each time series
separately. This makes creating forecasts for time series with little or no history challenging.
Deep neural networks [12  25  26] offer an alternative. With their capability to extract higher order
features  they can identify complex patterns within and across time series  and can do so from
datasets of raw time series with considerably less human effort [9  27  19]. However  as these models
make fewer structural assumptions  they typically require larger training datasets to learn accurate
models. Additionally  these models are difﬁcult to interpret and—often more importantly—make it
difﬁcult to enforce assumptions such as temporal smoothness.
In this paper we propose to bridge the gap between these two approaches by fusing SSMs with deep
(recurrent) neural networks. We present a forecasting method that parametrizes a particular linear
SSM using a recurrent neural network (RNN). The parameters of the RNN are learned jointly from
a dataset of raw time series and associated covariates  allowing the model to automatically extract
features and learn complex temporal patterns. At the same time  as each individual time series
is modeled using an SSM  we can enforce and exploit assumptions such as temporal smoothness.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Furthermore our method is interpretable  in the sense that the SSM parameters for each individual
time series can be inspected (and even changed if necessary). By incorporating prior structural
assumptions  the presented method scales from small to large data regimes seamlessly. When there
is little data to learn from  the structure imposed by the SSM can alleviate overﬁtting.
The rest of the paper is organized as follows. We ﬁrst discuss related work in Section 2 and then
review the general state space approach to time series forecasting in Section 3. In Section 4  we
present our joint forecasting model and describe the training and inference procedure. In Section 5 
we ﬁrst do a qualitative analysis of our method and then present quantitative comparison against the
state-of-the-art. We conclude in Section 6.

2 Related work

Hyndman et al. [13] and Durbin and Koopman [8] provide comprehensive overviews of SSMs. Re-
cent work in the machine learning literature on linear state-space models includes [23  22]. We
follow [13] in their approach to use linear state space models. The assumption of linear dynam-
ics consisting of interpretable components (level/trend/seasonality) makes the forecasts robust and
understandable. Note that non-linear effects can still be captured via exogenous variables. In the
forecasting context  non-linearities are typically associated with interventions such as promotions 
so this assumption is practically reasonable.
Combining state-space models (SSM) with RNNs has been proposed before  through either or both
of the following: (i) extending the Gaussian emission to complex likelihood models; (ii) making the
transition equation non-linear via a multi-layer perceptron (MLP) or interlacing SSM with transi-
tion matrices temporally speciﬁed by RNNs. The so-called Deep Markov Model (DMM) proposed
by [18  17] keeps the Gaussian transition dynamics with mean and covariance matrix parameterized
by MLPs. Stochastic RNNs [10] explicitly incorporate the deterministic dynamics from RNNs by
interlacing them with an SSM while the dynamics of RNNs do not depend on latent variables. Com-
pared to DMM  the difference is the added information from the deterministic state of RNNs. An
alternative way to make the transition equation non-linear is to cut the ties between the latent states
lt’s and associate them with deterministic states ht of RNN. In this way  the transition from lt−1
to lt is non-linearly determined by the RNN and the observation model. Chung et al. [7] ﬁrst pro-
posed such Variational RNNs. They were later used in Latent LSTM Allocation [29] and State-Space
LSTM [30]. [15] discusses unsupervised learning of state space models from sequential data.
Arguably the most relevant to our work is [11]  which aims to keep the linear Gaussian transition
structure intact so that the highly efﬁcient Kalman ﬁlter/smoother is applicable. The non-linear
behavior is approximated by locally linear transition matrices. The so-called called Kalman Vari-
ational Auto-Encoder (KVAE) disentangles the observations (emissions) and the latent dynamics
(transitions) with VAE. By making the locally linear part outside of the standard inference routine
and using a fully factorized Gaussian “decoder ” Kalman smoothing can be readily applied. A sim-
ilar idea appeared in [14] where a recognition network is used to output conjugate graphical model
potentials so that efﬁcient structural inference is feasible. Our model differs from [11] in that instead
of using an RNN to specify the linear combination of a ﬁxed set of K parameters  we directly use
RNNs to output the SSM parameters  eliminating the need to tune additional hyper-parameters.

3 Background

1:Ti

}N
i=1  where z(i)
1:Ti

The general probabilistic forecasting problem is the following. Let N be a set of univariate time
t ∈ R denotes the value of the i-th time
series {z(i)
2   . . .   z(i)
= (z(i)
Ti
series at time t.1 Further  let {x(i)
i=1 be a set of associated  time-varying covariate vectors
t ∈ RD. Our goal is to produce a set of probabilistic forecasts  i.e. for each i = 1  . . .   N
with x(i)
we are interested in the probability distribution of future trajectories z(i)

) and z(i)

Ti+1:Ti+τ given the past:

1   z(i)
1:Ti+τ}N
(cid:16)

p

z(i)
Ti+1:Ti+τ

  x(i)

1:Ti+τ ; Φ

.

(1)

(cid:12)(cid:12)(cid:12)z(i)

1:Ti

(cid:17)

1We consider time series where the the time points are equally spaced  but the units are arbitrary (e.g. hours 
days  months). Further  the time series do not have to be aligned  i.e.  the starting point t = 1 can refer to a
different absolute time point for different time series i.

2

are given also in the prediction range.

Here Φ denotes the set of learnable parameters of the model  which are shared between and learned
jointly from all N time series. For any given i  we refer to time series z(i)
as target time series  to
time range {1  2  . . .   Ti} as training range  and to time {Ti + 1  Ti + 2  . . .   Ti + τ} as prediction
1:Ti
range. The time point Ti + 1 is referred to as forecast start time and τ ∈ N>0 is the forecast horizon.
Note that we assume that the covariate vectors x(i)
t
We make the common simplifying assumption that the time series are independent of each other
when conditioned on the associated covariates x(i)
and the parameters Φ. However  in constrast to
1:Ti
many related methods that make this assumption  in our approach the model parameters Φ are shared
between all time series. So while this assumption precludes us from modeling correlations between
time series  it does not mean that the proposed model is not able to share statistical strength between
and learn patterns across the different time series  as we are learning the parameters Φ jointly from
all time series.
State Space Models. SSMs model the temporal structure of the data via a latent state lt ∈ RL
that can be used to encode time series components such as level  trend  and seasonality patterns.
In the forecasting setting they are typically applied to individual times series (though multivariate
exensions with multi-dimensional targets exist). For this reason  we will drop the superscript i from
the notation in this section. A general SSM is described by the so-called state-transition equation 
deﬁning the stochastic transition dynamics p(lt|lt−1) by which the latent state evolves over time 
and an observation model specifying the conditional probability p(zt|lt) of observations given the
latent state.
We consider linear state space models where the transition equation takes the form

lt = F tlt−1 + gtεt 

εt ∼ N (0  1).

(2)

Here at time t  the latent state lt−1 maintains information about level  trend  and seasonality pat-
terns and evolves by way of a deterministic transition matrix F t and a random innovation gtεt.
The structure of the transition matrix F t and innovation strength gt determine which kind of time
series patterns are encoded by the latent state lt (see [13] or [22] for details on possible structures;
Appendix A.1 in the long version of the paper contains two example instantiations).
The probabilistic observation model then describes how the observations are generated from the
latent state lt. Here we consider a univariate Gaussian observation model of the form

t lt−1 + bt 

zt = yt + σtt 

(3)
where at ∈ RL  σt ∈ R>0 and bt ∈ R are further (time-varying) parameters of the model. Finally 
the initial state l0 is assumed to follow an isotropic Gaussian distribution  l0 ∼ N (µ0  diag(σ2
0)).
Parameter learning.
fully speciﬁed by the parameters
Θt = (µ0  Σ0  F t  gt  at  bt  σt)  ∀t > 0.
In the classical setting the dynamics are assumed to
be time-invariant  that is Θt = Θ  ∀t > 0. One generic way of estimating them is by maximizing
the marginal likelihood  i.e.  Θ∗

The state space model

is

t ∼ N (0  1) 

yt = a(cid:62)

1:T = argmaxΘ1:T pSS(z1:T|Θ1:T )  where
T(cid:89)

p(zt|z1:t−1  Θ1:t) =

p(l0)

(cid:34) T(cid:89)

(cid:90)

(cid:35)

pSS(z1:T|Θ1:T ) := p(z1|Θ1)

p(zt|lt)p(lt|lt−1)

dl0:T (4)

t=2

t=1

denotes the marginal probability of the observations z1:T given the parameters Θ under the state
space model  integrating out the latent state lt. In the linear-Gaussian case considered here  the
required integrals are analytically tractable.
Note that in the classical setting  if there is more than one time series  a separate set of parameters
Θ(i) is learned for each time series z(i)
independently. This has the disadvantage that no informa-
1:Ti
tion is shared across different time series  making it challenging to apply this approach to time series
with limited historical data or high noise levels.

4 Deep State Space Models

Instead of learning the state space parameters Θ(i) for each time series independently  our forecasting
model instead learns a globally shared mapping from the covariate vectors x(i)
associated with each
1:Ti

3

observations

z(i)
1:Ti

Θ(i)
1

h(i)
1

x(i)
1

pSS(z(i)
1:Ti

|Θ(i)

1:Ti

)

. . .

. . .

. . .

Θ(i)
t

h(i)
t

x(i)
t

. . .

. . .

. . .

likelihood

state space parameters

recurrent network

features

Θ(i)
Ti

h(i)
Ti

x(i)
Ti

Figure 1: Summary of the model. During training  the inputs to the network are the features x(i)
as
t
t−1 at each time step t in the training range {1  2  . . .   Ti}. The
well as the previous network output h(i)
network output h(i)
  Φ) is then used to compute the parameters of the state space
model Θ(i)
after mapping it to the corresponding ranges of the parameters. Given the time series
t
observations z(i)
(which
1:Ti
are functions of the shared network parameters Φ) are computed according to Eq. 4. The shared
network parameters Φ are then learned by maximizing the likelihood.

in the training range  the likelihood of the state space parameters Θ(i)
1:Ti

t = h(h(i)

t−1  x(i)

t

target time series z(i)
1:Ti
i-th time series. This mapping 

  to the (time-varying) parameters Θ(i)

t of a linear state space model for the

Θ(i)

t = Ψ(x(i)

1:t  Φ) 

i = 1  . . .   N 

t = 1  . . .   Ti + τ 

(5)

1:t up to (and including) time t  as well as a set of
1:T and the parameters Φ  under our model  the data

is a function of the entire covariate time series x(i)
shared parameters Φ. Then  given the features x(i)
z(i)
1:Ti

is distributed according to
|x(i)

p(z(i)
1:Ti

|Θ(i)

1:Ti

) 

1:Ti

i = 1  . . .   N.

  Φ) = pSS(z(i)
1:Ti

(6)
where pSS denotes the marginal likelihood under a linear state space model as deﬁned in Eq. 4 
given its (time-varying) parameters Θ(i)
t
We parameterize the mapping Ψ from covariates to state space model parameters using a deep
recurrent neural network (RNN). Figure 1 shows a sketch of the overall model structure  unrolled for
all the time steps in the training range. Given the covariates2 x(i)
  a
t
multi-layer recurrent neural network with LSTM cells and parameters Φ computes a representation
of the features via a recurrent function h 
h(i)
t = h(h(i)

associated with time series z(i)
t

t−1  x(i)

  Φ).

.

t

The real-valued output vector of the last LSTM layer is then mapped to the parameters Θ(i)
t of the
state space model  by applying afﬁne mappings followed by suitable elementwise transformations
constraining the parameters to appropriate ranges (see Appendix A.2 in the long version of the
paper). Parameters Θ(i)
are then used to compute the likelihood of the given observations z(i)
 
t
t
which is used for learning of the network parameters Φ.

4.1 Training

(cid:110)
z(i)
The model parameters Φ are learned by maximizing the probability of observing the data
1:Ti
in the training range  i.e.  by maximizing the (log-)likelihood: Φ(cid:63) = argmaxΦ L(Φ)  where

N(cid:88)

i=1

(cid:16)

(cid:12)(cid:12)(cid:12)x(i)

1:Ti

(cid:17)

N(cid:88)

i=1

L(Φ) =

log p

z(i)
1:Ti

  Φ

=

log pSS

z(i)
1:Ti

(cid:16)

(cid:17)

(cid:12)(cid:12)(cid:12)Θ(i)

1:Ti

.

2The covariates (features) can be time dependent (e.g. product price or a set of dummy variables indicating

day-of-week) or time independent (e.g.  product brand  category etc.).

4

(cid:111)N

i=1

(7)

forecast start

z(i)
1:Ti

p(lTi|z(i)

1:Ti

)

Θ(i)
1

h(i)
1

x(i)
1

. . .

. . .

. . .

Θ(i)
Ti

h(i)
Ti

x(i)
Ti

ˆz(i)
Ti+1

Θ(i)

Ti+1

h(i)

Ti+1

x(i)

Ti+1

. . .

. . .

. . .

. . .

ˆz(i)
Ti+τ

Θ(i)

Ti+τ

h(i)

Ti+τ

x(i)

Ti+τ

Illustration of the how the model is used to make forecasts after the network parameters
Figure 2:
in the training range {1  2  . . .   Ti} (not necessarily in the
Φ are learned. Given a time series z(i)
1:Ti
training set) and associated features x(i)
1:Ti+τ for both training and prediction ranges  forecasts are
produced as follows: (i) ﬁrst the posterior of the latent state p(lTi|z1:Ti ) for the last time step Ti in
the training range is computed using the observations z(i)
ob-
1:Ti
tained by unrolling the RNN network in the training range; (ii) given the posterior of the latent state
p(lTi|z1:Ti )  prediction samples are generated by recursively applying the transition equation and
the observation model (Eq. 8) where the state space parameters for the prediction range Θ(i)
are obtained by unrolling the RNN in the prediction range.

and the state space parameters Θ(i)
1:Ti

Ti+1:Ti+τ

We can view each summand of L(Φ) in Eq. 7 as a (negative) loss function  that measures compatibil-
ity between the state space model parameters Θ(i)
produced by the RNN when given input x(i)
 
1:Ti
1:Ti
and the true observations z(i)
. Each of these terms is a standard likelihood computation under
1:Ti
linear-Gaussian state space model  which can be carried out efﬁciently via Kalman ﬁltering (see e.g.
[3  Sec. 24.3] or [22  Appendix A] for details): this involves mainly matrix-matrix and matrix-vector
multiplications  which allows us to implement the overall log-likelihood computation using a neural
network framework (MXNet)  and use automatic differentiation to obtain gradients with respect to
the parameters Φ  which are then used by a stochastic gradient descent-based optimization proce-
dure. Note that a forward pass of our network to compute the loss (i.e.  negative log-likelihood)
essentially uses the same basic primitives as that of classical methods that learn parameters per time
series independently. Thus  one can  in principle  extend our ideas to other instances of state space
models by simply redeﬁning their parameters as the outputs of the RNN.

4.2 Prediction

Once the network parameters Φ are learned  we can use them to address our original problem spec-
iﬁed in Eq. 1  i.e.  to make probabilistic forecasts for each given time series. Given Φ  we can
compute the joint distribution over the prediction range for each time series analytically  as this joint
distribution is a multivariate Gaussian. However  in practice it is often more convenient to represent
the forecast distribution in terms of K Monte Carlo samples 
1:Ti+τ   Θ(i)

k Ti+1:Ti+τ ∼ p(z(i)
ˆz(i)

Ti+1:Ti+τ|z(i)

k = 1  . . .   K.

  x(i)

1:Ti

1:Ti+τ ) 

In order to generate prediction samples from a state space model  one ﬁrst computes the posterior of
the latent state p(lT|z1:T ) for the last time step T in the training range  and then recursively applies
the transition equation and the observation model to generate prediction samples. More precisely 
starting with sample (cid:96)T ∼ p((cid:96)T|z1:T )  we recursively apply

T +t ∼ N (0  1) 
εT +t ∼ N (0  1) 

t = 1  . . . τ 
t = 1  . . . τ 
t = 1  . . . τ − 1.

(8a)
(8b)
(8c)

) for each of the time series z(i)
by unrolling the
1:Ti
as shown in Figure 2  and then using the Kalman

T +t(cid:96)T +t−1 + bT +t 

yT +t = a(cid:62)
ˆzT +t = yT +t + σtt 
lT +t ∼ F T +t(cid:96)T +t−1 + gT +tεT +t 
|z(i)
In our case  we compute the posterior p(l(i)
1:Ti
Ti
RNN network in the training range to obtain Θ(i)
1:Ti

5

ﬁltering algorithm. Next  we unroll the RNN for the prediction range t = Ti + 1  . . .   Ti + τ and
obtain Θ(i)
Ti+1:Ti+τ   then generate the prediction samples by recursively applying above equations K
times.3

Remarks. Note that in our model  in contrast to classical and deep learning-based auto-regressive
models (e.g. DeepAR [9])  target values are not used as inputs directly. This is a key feature of
our method  and brings several advantages: (i) It makes the model more robust to noise  as target
values are only incorporated through the likelihood term  where noise is properly accounted for;
(ii) Missing target values can easily be handled by simply dropping the corresponding likelihood
terms; (iii) Forecast sample path generation is computationally more efﬁcient  as the RNN needs
to be unrolled only once for the entire prediction (independent of the number of samples)  whereas
auto-regressive models (e.g. [9  26]) have to be unrolled for each sample.

5 Experiments

Qualitative experiments.
In our ﬁrst experiment  we test whether our model effectively recovers
the state space parameters if trained on synthetic data. For this  we generate ﬁve groups of time
series from day-of-week seasonality model (see Appendix A.1 in the long version of the paper) but
with different initial states and innovation parameters per group. For simplicity  we use the same
observation noise σt for all time series. Each time series consists of six weeks of daily data and we
use the ﬁrst four weeks of all time series for training our model. We use a group identiﬁer as an
input feature. In the ideal case  for each time series the model should output the parameters of the
state space model from which this time series was generated.
0   σ(i)
The state space model parameters in this case are given by Θ(i)
t = (µ(i)
t =
1  . . .   Ti + τ  where Ti = 28 ∀i and τ = 14. Note that except for σ(i)
  all the other parameters
are different for each group. We encode the day-of-week seasonality using seven components of the
latent state as in [22]  i.e.  L = 7 and µ0 ∈ R7 (each component corresponds to a different day of
the week). For simplicity  we ﬁx the term b(i)
To analyse how much data is required for recovering the parameters  we train three different models
using N = {20  40  140} examples from each group. Figure 3 shows the ground truth values of the
parameters as well as the values obtained by our model for different number of training examples
per group. The columns show the mean of the initial state µ0 (seven values)  innovation parameter
γt as well as the standard deviation σt of the observations while the rows correspond to each of the
ﬁve groups. The innovation parameter and the standard deviation of the observations are shown for
the prediction range (two-weeks). The recovery of state space parameters becomes more accurate
gradually as we increase the number of examples from 20 to 140. Moreover  these parameters are
recovered reasonably well with N = 200 examples per group. In fact  the means of the initial state
are exactly recovered. The standard deviation of the initial state σ0 (not plotted) has converged
to a constant value in all cases. It turns out that the initial state means µ0 are easy to recover but
observation noise σt and the standard deviation of the initial state σ0 are the most difﬁcult to recover.

t = 0 in this experiment.

0   γ(i)

  σ(i)

t ) 

t

t

Quantitative experiments.
In our ﬁrst quantitative experiment we evaluate how our model per-
forms under small data regimes. For this  we use the publicly available datasets electricity
and traffic [28]. The electricity dataset is a hourly time series of electricity consumption
of 370 customers. The traffic dataset contains hourly occupancy rates (between 0 and 1) of 963
car lanes of San Francisco bay area freeways. As one expects  all the time series in these datasets
exhibit hourly as well as daily seasonal patterns. As baselines we use the classical forecasting meth-
ods auto.arima  ets implemented in R’s forecast package and a recent RNN-based method
DeepAR [9]. We obtained results for DeepAR using the Amazon Sagemaker machine learning
platform [1]. Since DeepAR and DeepState ﬁt a joint model across the time series  both are
given a time independent feature representing the category (i.e.  the index) of the time series and
time-based features like hour-of-the-day  day-of-the-week  day-of-the-month. For DeepState the
size of SSM (i.e.  latent dimension) directly depends on the granularity of the time series which
determines the number of seasons. For hourly data  we use hour-of-day (24 seasons) as well as

3Note that
Ti+1:Ti+τ and the distribution of the ﬁnal latent state are computed.

Θ(i)

the sampling procedure is trivially parallelizable over K samples once the parameters

6

Figure 3: Recovery of state space parameters as the number of examples per group is increased.
Columns show state space parameters while the rows correspond to each of the ﬁve groups. Each
plot shows the true and the recovered values of the parameters with increased number of examples.

Datasets

Methods

2-weeks

p50Loss

p90Loss

3-weeks

p50Loss

p90Loss

4-weeks

p50Loss

p90Loss

electricity

traffic

auto.arima
ets
DeepAR
DeepState

auto.arima
ets
DeepAR
DeepState

0.283
0.121
0.153
0.087

0.492
0.621
0.177
0.168

0.109
0.101
0.147
0.05

0.280
0.650
0.153
0.117

0.291
0.130
0.147
0.085

0.492
0.509
0.126
0.170

0.112
0.110
0.132
0.052

0.289
0.529
0.096
0.113

0.30
0.13
0.125
0.085

0.501
0.532
0.219
0.168

0.11
0.11
0.080
0.057

0.298
0.60
0.138
0.114

Table 1: Data efﬁciency. Evaluation on electricity and traffic datasets with increasing
training range. The forecast is evaluated on 7 days.

day-of-week (7 seasons) models and hence latent dimension is 31. We train each method on all time
series of these datasets but vary the size of the training range Ti ∈ {14  21  28} days. We evaluate
all the methods on the next τ = 7 days after the forecast start time using the standard p50 and
p90- quantile losses. For a given collection of time series z and corresponding predictions ˆz  the
ρ-quantile loss for ρ ∈ (0  1) is deﬁned as

(cid:40)

ρ(z − ˆz)
if z > ˆz 
(1 − ρ)(ˆz − z) otherwise.

(cid:80)

(cid:80)

t

i t Pρ(z(i)
i t |z(i)

  ˆz(i)
t )
|

t

QLρ(z  ˆz) = 2

  Pρ(z  ˆz) =

The p50 and p90 losses are reported in Table 1. Overall our method achieves the best performance
except for one case. Moreover  our method achieves very good performance even with 2-weeks data
since it can explicitly incorporate seasonal structures (i.e.  hour-of-day seasonality). Although ets
and auto.arima incorporate such seasonal structures  their results are much worse. Inability to
learn shared patterns across the time series could be a possible reason for their worse performance.
DeepAR tries to learn seasonal patterns purely from the data and its performance generally improves
with increased training size. We show some example predictions of our method in Appendix A.5.

7

020406080100Prior means (µ0) for day-of-week seasonality (x-axis: Days)1020304050607080901000204060801001201401600204060801001201401600123456020406080100120140160TruthRecovered: N = 20Recovered: N = 40Recovered: N = 14001020304050Smoothing parameter (γt) over prediction range (two-weeks)0246810024681012024681012024681012140246810TruthRecovered: N = 20Recovered: N = 40Recovered: N = 140246810Observation nosie (σt) over prediction range (two-weeks)24681024681024681002468101214246810TruthRecovered: N = 20Recovered: N = 40Recovered: N = 140Next  to compare against the matrix factorization method [28]  we repeat the experiment in [28]
that evaluates rolling-day forecasts for seven days (i.e.  prediction horizon is one day and forecasts
start time is shifted by one day after evaluating the prediction for the current day). Note that unlike
MatFact  our method and DeepAR need not retrain after updating the forecast start time. We just
extend the training range by one day and update the posterior of the latent state accordingly. The
results are shown in Table 2. Since MatFact only produces point forecasts  we report normalized
deviation as in [28]  which in our case is equal to p50-loss. For DeepAR and DeepState we
report both p50- and p90-losses. Note that our method is much better than MatFact even though the
latter is retrained after each day of prediction. We get comparable results to DeepAR  which in our
experience performs well with short forecast horizons.

MatFact

electricity
traffic

0.16
0.20

DeepState

0.083/0.056
0.167/0.113

DeepAR
0.075/0.04
0.161/0.099

Table 2: Average p50/p90-loss for rolling-day
prediction for seven days. MatFact outputs points
predictions  so we only report p50-loss.

In the ﬁnal experiment we evaluate our method
on a diverse collection of publicly available
datasets. We selected datasets containing time
series from a single domain as our method is
most suited for datasets of related time series.
This includes monthly and quarterly time se-
ries from the tourism competition dataset [2]
describing tourism demand  hourly time se-
ries from the M4 competition [20] and parts
dataset [6] which contains monthly demand of
spare parts at a US auto-mobile company. The number of time series in these data sets are 414
(M4-Hourly)  366 (tourism-Monthly)  427 (tourism-Quarterly) and 1046 (parts).
For tourism and M4 datasets  train and test splits are already provided. The length of the train-
ing time series as well as the starting date differ for the time series in the M4-Hourly and
tourism datasets. The prediction horizon for these data sets are 48 hours (M4-Hourly)  24 months
(tourism-Monthly) and 8 quarters (tourism-Quarterly). For parts dataset we use the
last 12 months as the prediction range while the training range contains 39 months. For both
tourism-Monthly and tourism-Quarterly we used month-of-year seasonal model along
with a trend component (to accommodate the trend visible in the training range of these time series)
and for parts we used month-of-year seasonal model. For M4-Hourly we used the hour-of-day
as well as day-of-week seasonal models. The p50 and p90 losses are reported in Table 3 for all the
methods. These results further show that our method achieves the best performance overall.

M4-Hourly
parts
tourism-Monthly
tourism-Quarterly

ets

0.054/0.0267
1.639/1.0086
0.093/0.054
0.105/0.055

auto.arima
0.052/0.0354
1.6444/1.0664
0.0999/0.058
0.1241/0.062

DeepAR

0.090/0.0304
1.273/1.086
0.107/0.059
0.11/0.062

DeepState
0.044/0.0266
1.47/0.935
0.138/0.067
0.098/0.047

Table 3: p50/p90-losses for datasets obtained from publicly available sources.

6 Conclusions

In this paper we propose a new approach to time series forecasting by marrying state space models
with deep recurrent neural networks. This combination allows us to explicitly incorporate structural
assumptions to handle small data regimes on one hand and learn complex patterns from raw time
series data for larger data regimes on the other hand. Our experiments on synthetic data suggest that
the model is capable of accurately recovering the parameters of the state space model from which the
data is generated. We also showed  on real-world datasets  that the proposed method achieves state-
of-the-art performance by comparing it against a recent RNN-based method  a matrix factorization
method  as well as classical approaches such as ARIMA and ETS. Under regimes of limited data our
method clearly outperforms the other methods by explicitly modelling seasonal structure. Extending
our approach to other instances of state space models as well as non-Gaussian likelihoods are some
of the directions we are currently pursuing. Some ideas of extending our method to non-Gaussian
likelihoods are discussed in Appendix A.4 in the long version of the paper.

8

References
[1] Amazon Sagemaker: DeepAR Forecasting.
sagemaker/latest/dg/deepar.html.

https://docs.aws.amazon.com/

[2] G. Athanasopoulos  R. Hyndman  H. Song  and D. Wu. The tourism forecasting competition.

International Journal of Forecasting  27(3):822–844  2011.

[3] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press  2012.

[4] George E. P. Box and Gwilym M. Jenkins. Some recent advances in forecasting and control.

Journal of the Royal Statistical Society. Series C (Applied Statistics)  17(2):91–109  1968.

[5] George EP Box and David R Cox. An analysis of transformations. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 211–252  1964.

[6] Nicolas Chapados. Effective bayesian modeling of groups of related count time series.

International Conference on Machine Learning  pages 1395–1403  2014.

In

[7] Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and Yoshua
Bengio. A recurrent latent variable model for sequential data. In Advances in neural informa-
tion processing systems  pages 2980–2988  2015.

[8] James Durbin and Siem Jan Koopman. Time series analysis by state space methods  volume 38.

OUP Oxford  2012.

[9] Valentin Flunkert  David Salinas  and Jan Gasthaus. DeepAR: Probabilistic forecasting with
autoregressive recurrent networks. CoRR  abs/1704.04110  2017. URL http://arxiv.
org/abs/1704.04110.

[10] Marco Fraccaro  Søren Kaae Sønderby  Ulrich Paquet  and Ole Winther. Sequential neural
models with stochastic layers. In Advances in neural information processing systems  pages
2199–2207  2016.

[11] Marco Fraccaro  Simon Kamronn  Ulrich Paquet  and Ole Winther. A disentangled recognition
and nonlinear dynamics model for unsupervised learning. In Advances in Neural Information
Processing Systems  pages 3604–3613  2017.

[12] Alex Graves. Generating sequences with recurrent neural networks.

arXiv:1308.0850  2013.

arXiv preprint

[13] R. Hyndman  A. B. Koehler  J. K. Ord  and R. D. Snyder. Forecasting with Exponential
Smoothing: The State Space Approach. Springer Series in Statistics. Springer  2008. ISBN
9783540719182.

[14] Matthew Johnson  David K Duvenaud  Alex Wiltschko  Ryan P Adams  and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast in-
ference. In Advances in neural information processing systems  pages 2946–2954  2016.

[15] Maximilian Karl  Maximilian Soelch  Justin Bayer  and Patrick van der Smagt. Deep varia-
tional bayes ﬁlters: Unsupervised learning of state space models from raw data. ICLR  2017.

[16] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR  2014.

[17] Rahul G Krishnan  Uri Shalit  and David Sontag. Deep kalman ﬁlters.

arXiv:1511.05121  2015.

arXiv preprint

[18] Rahul G Krishnan  Uri Shalit  and David Sontag. Structured inference networks for nonlinear

state space models. In AAAI  pages 2101–2109  2017.

[19] Nikolay Laptev  Jason Yosinsk  Li Li Erran  and Slawek Smyl. Time-series extreme event

forecasting with neural networks at Uber. In ICML Time Series Workshop. 2017.

[20] S. Makridakis  E. Spiliotis  and V. Assimakopoulos. The M4 competition: Results  ﬁndings 

conclusion and way forward. International Journal of Forecasting  34(4):802–808  2018.

9

[21] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International Conference on Machine
Learning  pages 1278–1286  2014.

[22] Matthias Seeger  Syama Rangapuram  Yuyang Wang  David Salinas  Jan Gasthaus  Tim
Januschowski  and Valentin Flunkert. Approximate Bayesian inference in linear state space
models for intermittent demand forecasting at scale. CoRR  abs/1704.04110  2017. URL
http://arxiv.org/abs/1704.04110.

[23] Matthias W Seeger  David Salinas  and Valentin Flunkert. Bayesian intermittent demand fore-
casting for large inventories. In Advances in Neural Information Processing Systems  pages
4646–4654  2016.

[24] Matthias W Seeger  David Salinas  and Valentin Flunkert. Bayesian intermittent demand fore-
casting for large inventories. In Advances in Neural Information Processing Systems  pages
4646–4654  2016.

[25] Ilya Sutskever  Oriol Vinyals  and Quoc V Le. Sequence to sequence learning with neural

networks. In Advances in Neural Information Processing Systems  pages 3104–3112  2014.

[26] A¨aron van den Oord  Sander Dieleman  Heiga Zen  Karen Simonyan  Oriol Vinyals  Alex
Graves  Nal Kalchbrenner  Andrew W. Senior  and Koray Kavukcuoglu. Wavenet: A genera-
tive model for raw audio. CoRR  abs/1609.03499  2016.

[27] Ruofeng Wen Wen  Kari Torkkola  and Balakrishnan Narayanaswamy. A multi-horizon quan-

tile recurrent forecaster. In NIPS Time Series Workshop. 2017.

[28] Hsiang-Fu Yu  Nikhil Rao  and Inderjit S Dhillon. Temporal regularized matrix factorization
for high-dimensional time series prediction.
In D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29 
pages 847–855. Curran Associates  Inc.  2016.

[29] Manzil Zaheer  Amr Ahmed  and Alexander J Smola. Latent LSTM allocation: Joint clustering
and non-linear dynamic modeling of sequence data. In International Conference on Machine
Learning  pages 3967–3976  2017.

[30] Xun Zheng  Manzil Zaheer  Amr Ahmed  Yuan Wang  Eric P Xing  and Alexander J Smola.
State space LSTM models with particle MCMC inference. arXiv preprint arXiv:1711.11179 
2017.

10

,Syama Sundar Rangapuram
Matthias Seeger
Jan Gasthaus
Lorenzo Stella
Yuyang Wang
Tim Januschowski