2018,Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams,Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes  linked twist maps  and material data. Among them  \textit{persistent homology} is a well-known tool to extract robust topological features  and outputs as \textit{persistence diagrams} (PDs). However  PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it  an emerged approach is to use kernel methods  and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However  Wasserstein distance is not \textit{negative definite}. Thus  it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}. In this work  we rely upon the alternative \textit{Fisher information geometry} to propose a positive definite kernel for PDs \textit{without approximation}  namely the Persistence Fisher (PF) kernel. Then  we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that  we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally  we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore  we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets  we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.,Persistence Fisher Kernel: A Riemannian Manifold

Kernel for Persistence Diagrams

RIKEN Center for Advanced Intelligence Project  Japan

Tam Le

tam.le@riken.jp

Makoto Yamada

Kyoto University  Japan

RIKEN Center for Advanced Intelligence Project  Japan

makoto.yamada@riken.jp

Abstract

Algebraic topology methods have recently played an important role for statistical
analysis with complicated geometric structured data such as shapes  linked twist
maps  and material data. Among them  persistent homology is a well-known tool
to extract robust topological features  and outputs as persistence diagrams (PDs).
However  PDs are point multi-sets which can not be used in machine learning
algorithms for vector data. To deal with it  an emerged approach is to use kernel
methods  and an appropriate geometry for PDs is an important factor to measure the
similarity of PDs. A popular geometry for PDs is the Wasserstein metric. However 
Wasserstein distance is not negative deﬁnite. Thus  it is limited to build positive
deﬁnite kernels upon the Wasserstein distance without approximation. In this work 
we rely upon the alternative Fisher information geometry to propose a positive
deﬁnite kernel for PDs without approximation  namely the Persistence Fisher (PF)
kernel. Then  we analyze eigensystem of the integral operator induced by the
proposed kernel for kernel machines. Based on that  we derive generalization error
bounds via covering numbers and Rademacher averages for kernel machines with
the PF kernel. Additionally  we show some nice properties such as stability and
inﬁnite divisibility for the proposed kernel. Furthermore  we also propose a linear
time complexity over the number of points in PDs for an approximation of our
proposed kernel with a bounded error. Throughout experiments with many different
tasks on various benchmark datasets  we illustrate that the PF kernel compares
favorably with other baseline kernels for PDs.

1

Introduction

Using algebraic topology methods for statistical data analysis has been recently received a lot of
attention from machine learning community [Chazal et al.  2015  Kwitt et al.  2015  Bubenik  2015 
Kusano et al.  2016  Chen and Quadrianto  2016  Carriere et al.  2017  Hofer et al.  2017  Adams et al. 
2017  Kusano et al.  2018]. Algebraic topology methods can produce a robust descriptor which can
give useful insight when one deals with complicated geometric structured data such as shapes  linked
twist maps  and material data. More speciﬁcally  algebraic topology methods are applied in various
research ﬁelds such as biology [Kasson et al.  2007  Xia and Wei  2014  Cang et al.  2015]  brain
science [Singh et al.  2008  Lee et al.  2011  Petri et al.  2014]  and information science [De Silva
et al.  2007  Carlsson et al.  2008]  to name a few.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: An illustration of a persistence diagram on a real-value function f. The orange horizontal
lines are the boundaries of sublevel sets f−1((−∞  t]). For the 0-dimensional topological features
(connected components)  the topological events of births are happened at t = t1  t2  t3 and their
corresponding topological events of deaths are occurred at t = +∞  t5  t4 respectively. Therefore 
the persistent diagram of f is Dgf = {(t1  +∞)  (t2  t5)  (t3  t4)}.
In algebraic topology  persistent homology is an important method to extract robust topological
information  it outputs point multisets  called persistence diagrams (PDs) [Edelsbrunner et al.  2000].
Since PDs can have different number of points  it is not straightforward to plug PDs into traditional
statistical machine learning algorithms  which often assume a vector representation for data.

Related work. There are two main approaches in topological data analysis: (i) explicit vector
representation for PDs such as computing and sampling functions built from PDs (i.e. persistence
lanscapes [Bubenik  2015]  tangent vectors from the mean of the square-root framework with principal
geodesic analysis [Anirudh et al.  2016]  or persistence images [Adams et al.  2017])  using points in
PDs as roots of a complex polynomial for concatenated-coefﬁcient vector representations [Di Fabio
and Ferri  2015]  or using distance matrices of points in PDs for sorted-entry vector representations
[Carriere et al.  2015]  (ii) implicit representation via kernels such as the Persistence Scale Space
(PSS) kernel  motivated by a heat diffusion problem with a Dirichlet boundary condition [Reininghaus
et al.  2015]  the Persistence Weighted Gaussian (PWG) kernel via kernel mean embedding [Kusano
et al.  2016]  or the Sliced Wasserstein (SW) kernel under Wasserstein geometry [Carriere et al.  2017].
In particular  geometry on PDs plays an important role. One of the most popular geometries for PDs
is the Wasserstein metric [Villani  2003  Peyre and Cuturi  2017]. However  it is well-known that the
Wasserstein distance is not negative deﬁnite [Reininghaus et al.  2015] (Appendix A). Consequently 
we may not obtain positive deﬁnite kernels  built upon from the Wasserstein distance. Thus  it may
be necessary to approximate the Wasserstein distance to achieve positive deﬁniteness for kernels 
relied on Wasserstein geometry. For example  [Carriere et al.  2017] used the SW distance—an
approximation of Wasserstein distance—to construct the positive deﬁnite SW kernel.

Contributions.
In this work  we focus on the implicit representation via kernels for PDs approach 
and follow Anirudh et al. [2016] to explore an alternative Riemannian geometry  namely the Fisher
information metric [Amari and Nagaoka  2007  Lee  2006] for PDs. Our contribution is two-fold:
(i) we propose a positive deﬁnite kernel  namely the Persistence Fisher (PF) kernel for PDs. The
proposed kernel well preserves the geometry of the Riemannian manifold since it is directly built
upon the Fisher information metric for PDs without approximation. (ii) We analyze the eigensystem
of the integral operator induced by the PF kernel for kernel machines. Based on that  we derive
generalization error bounds via covering numbers and Rademacher averages for kernel machines with
the PF kernel. Additionally  we provide some nice properties such as a bound for the proposed kernel
induced squared distance with respect to the geodesic distance which can be interpreted as stability
in a similar sense as the work of [Kwitt et al.  2015  Reininghaus et al.  2015] with Wasserstein
geometry  and inﬁnite divisibility for the proposed kernel. Furthermore  we describe a linear time
complexity over the number of points in PDs for an approximation of the PF kernel with a bounded
error via Fast Gauss Transform [Greengard and Strain  1991  Morariu et al.  2009].

2 Background

Persistence diagrams. Persistence homology (PH) [Edelsbrunner and Harer  2008] is a popular
technique to extract robust topological features (i.e. connected components  rings  cavities) on
real-value functions. Given f : X (cid:55)→ R  PH considers the family of sublevel sets of f (i.e.

2

RX<latexit sha1_base64="ytpsRJee1OiV+V5BLqOKb16zEnU=">AAAB7XicbVA9T8MwEL3wWcJXgZHFokFiqpIuwFbBwlgkQiu1UeW4TmvVcSzbQaqi/ggWBkCs/B82/g1umwFannTS03t3ursXS8608f1vZ219Y3Nru7Lj7u7tHxxWj44fdZYrQkOS8Ux1YqwpZ4KGhhlOO1JRnMactuPx7cxvP1GlWSYezETSKMVDwRJGsLFS2/Pcjut5/WrNr/tzoFUSlKQGJVr96ldvkJE8pcIQjrXuBr40UYGVYYTTqdvLNZWYjPGQdi0VOKU6KubnTtG5VQYoyZQtYdBc/T1R4FTrSRrbzhSbkV72ZuJ/Xjc3yVVUMCFzQwVZLEpyjkyGZr+jAVOUGD6xBBPF7K2IjLDCxNiEXBtCsPzyKgkb9eu6f9+oNW/KNCpwCmdwAQFcQhPuoAUhEBjDM7zCmyOdF+fd+Vi0rjnlzAn8gfP5A/usjY0=</latexit><latexit sha1_base64="ytpsRJee1OiV+V5BLqOKb16zEnU=">AAAB7XicbVA9T8MwEL3wWcJXgZHFokFiqpIuwFbBwlgkQiu1UeW4TmvVcSzbQaqi/ggWBkCs/B82/g1umwFannTS03t3ursXS8608f1vZ219Y3Nru7Lj7u7tHxxWj44fdZYrQkOS8Ux1YqwpZ4KGhhlOO1JRnMactuPx7cxvP1GlWSYezETSKMVDwRJGsLFS2/Pcjut5/WrNr/tzoFUSlKQGJVr96ldvkJE8pcIQjrXuBr40UYGVYYTTqdvLNZWYjPGQdi0VOKU6KubnTtG5VQYoyZQtYdBc/T1R4FTrSRrbzhSbkV72ZuJ/Xjc3yVVUMCFzQwVZLEpyjkyGZr+jAVOUGD6xBBPF7K2IjLDCxNiEXBtCsPzyKgkb9eu6f9+oNW/KNCpwCmdwAQFcQhPuoAUhEBjDM7zCmyOdF+fd+Vi0rjnlzAn8gfP5A/usjY0=</latexit><latexit sha1_base64="ytpsRJee1OiV+V5BLqOKb16zEnU=">AAAB7XicbVA9T8MwEL3wWcJXgZHFokFiqpIuwFbBwlgkQiu1UeW4TmvVcSzbQaqi/ggWBkCs/B82/g1umwFannTS03t3ursXS8608f1vZ219Y3Nru7Lj7u7tHxxWj44fdZYrQkOS8Ux1YqwpZ4KGhhlOO1JRnMactuPx7cxvP1GlWSYezETSKMVDwRJGsLFS2/Pcjut5/WrNr/tzoFUSlKQGJVr96ldvkJE8pcIQjrXuBr40UYGVYYTTqdvLNZWYjPGQdi0VOKU6KubnTtG5VQYoyZQtYdBc/T1R4FTrSRrbzhSbkV72ZuJ/Xjc3yVVUMCFzQwVZLEpyjkyGZr+jAVOUGD6xBBPF7K2IjLDCxNiEXBtCsPzyKgkb9eu6f9+oNW/KNCpwCmdwAQFcQhPuoAUhEBjDM7zCmyOdF+fd+Vi0rjnlzAn8gfP5A/usjY0=</latexit>t1<latexit sha1_base64="iRumoAUxyjjX/cbRUaq1nRxzhIo=">AAAB73icbVBNS8NAEN3Urxq/qh69LDaCp5L0ooKHghePFYyttKFstpt26e4m7E6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84EN+D7305pbX1jc6u87e7s7u0fVA6PHkyaa8pCmopUt2NimOCKhcBBsHamGZGxYK14dDPzW09MG56qexhnLJJkoHjCKQErPXqeC73A9bxeperX/DnwKgkKUkUFmr3KV7ef0lwyBVQQYzqBn0E0IRo4FWzqdnPDMkJHZMA6lioimYkm84On+MwqfZyk2pYCPFd/T0yINGYsY9spCQzNsjcT//M6OSSX0YSrLAem6GJRkgsMKZ59j/tcMwpibAmhmttbMR0STSjYjFwbQrD88ioJ67Wrmn9XrzauizTK6ASdonMUoAvUQLeoiUJEkUTP6BW9Odp5cd6dj0VrySlmjtEfOJ8/S8+ORw==</latexit><latexit sha1_base64="iRumoAUxyjjX/cbRUaq1nRxzhIo=">AAAB73icbVBNS8NAEN3Urxq/qh69LDaCp5L0ooKHghePFYyttKFstpt26e4m7E6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84EN+D7305pbX1jc6u87e7s7u0fVA6PHkyaa8pCmopUt2NimOCKhcBBsHamGZGxYK14dDPzW09MG56qexhnLJJkoHjCKQErPXqeC73A9bxeperX/DnwKgkKUkUFmr3KV7ef0lwyBVQQYzqBn0E0IRo4FWzqdnPDMkJHZMA6lioimYkm84On+MwqfZyk2pYCPFd/T0yINGYsY9spCQzNsjcT//M6OSSX0YSrLAem6GJRkgsMKZ59j/tcMwpibAmhmttbMR0STSjYjFwbQrD88ioJ67Wrmn9XrzauizTK6ASdonMUoAvUQLeoiUJEkUTP6BW9Odp5cd6dj0VrySlmjtEfOJ8/S8+ORw==</latexit><latexit sha1_base64="iRumoAUxyjjX/cbRUaq1nRxzhIo=">AAAB73icbVBNS8NAEN3Urxq/qh69LDaCp5L0ooKHghePFYyttKFstpt26e4m7E6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84EN+D7305pbX1jc6u87e7s7u0fVA6PHkyaa8pCmopUt2NimOCKhcBBsHamGZGxYK14dDPzW09MG56qexhnLJJkoHjCKQErPXqeC73A9bxeperX/DnwKgkKUkUFmr3KV7ef0lwyBVQQYzqBn0E0IRo4FWzqdnPDMkJHZMA6lioimYkm84On+MwqfZyk2pYCPFd/T0yINGYsY9spCQzNsjcT//M6OSSX0YSrLAem6GJRkgsMKZ59j/tcMwpibAmhmttbMR0STSjYjFwbQrD88ioJ67Wrmn9XrzauizTK6ASdonMUoAvUQLeoiUJEkUTP6BW9Odp5cd6dj0VrySlmjtEfOJ8/S8+ORw==</latexit>t2<latexit sha1_base64="5uq/4ozjiSEf84akO1qjEcoMPec=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5L0ooKHghePFayttKFstpt26e4m7G6EEvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL0o508b3v53S2vrG5lZ5293Z3ds/qBwePegkU4S2SMIT1YmwppxJ2jLMcNpJFcUi4rQdjW9mfvuJKs0SeW8mKQ0FHkoWM4KNlR49zzX9uut5/UrVr/lzoFUSFKQKBZr9yldvkJBMUGkIx1p3Az81YY6VYYTTqdvLNE0xGeMh7VoqsaA6zOcHT9GZVQYoTpQtadBc/T2RY6H1RES2U2Az0sveTPzP62YmvgxzJtPMUEkWi+KMI5Og2fdowBQlhk8swUQxeysiI6wwMTYj14YQLL+8Slr12lXNv6tXG9dFGmU4gVM4hwAuoAG30IQWEBDwDK/w5ijnxXl3PhatJaeYOYY/cD5/AE1Vjkg=</latexit><latexit sha1_base64="5uq/4ozjiSEf84akO1qjEcoMPec=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5L0ooKHghePFayttKFstpt26e4m7G6EEvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL0o508b3v53S2vrG5lZ5293Z3ds/qBwePegkU4S2SMIT1YmwppxJ2jLMcNpJFcUi4rQdjW9mfvuJKs0SeW8mKQ0FHkoWM4KNlR49zzX9uut5/UrVr/lzoFUSFKQKBZr9yldvkJBMUGkIx1p3Az81YY6VYYTTqdvLNE0xGeMh7VoqsaA6zOcHT9GZVQYoTpQtadBc/T2RY6H1RES2U2Az0sveTPzP62YmvgxzJtPMUEkWi+KMI5Og2fdowBQlhk8swUQxeysiI6wwMTYj14YQLL+8Slr12lXNv6tXG9dFGmU4gVM4hwAuoAG30IQWEBDwDK/w5ijnxXl3PhatJaeYOYY/cD5/AE1Vjkg=</latexit><latexit sha1_base64="5uq/4ozjiSEf84akO1qjEcoMPec=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5L0ooKHghePFayttKFstpt26e4m7G6EEvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL0o508b3v53S2vrG5lZ5293Z3ds/qBwePegkU4S2SMIT1YmwppxJ2jLMcNpJFcUi4rQdjW9mfvuJKs0SeW8mKQ0FHkoWM4KNlR49zzX9uut5/UrVr/lzoFUSFKQKBZr9yldvkJBMUGkIx1p3Az81YY6VYYTTqdvLNE0xGeMh7VoqsaA6zOcHT9GZVQYoTpQtadBc/T2RY6H1RES2U2Az0sveTPzP62YmvgxzJtPMUEkWi+KMI5Og2fdowBQlhk8swUQxeysiI6wwMTYj14YQLL+8Slr12lXNv6tXG9dFGmU4gVM4hwAuoAG30IQWEBDwDK/w5ijnxXl3PhatJaeYOYY/cD5/AE1Vjkg=</latexit>t3<latexit sha1_base64="S98+A9F1A7/X9kKpxKRLJsyNrJs=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5LUgwoeCl48VjBaaUPZbDft0t1N2N0IpfRXePGg4tW/481/47bNQVsfDDzem2FmXpxxpo3vfzulldW19Y3ypru1vbO7V9k/uNdprggNScpT1YqxppxJGhpmOG1limIRc/oQD6+n/sMTVZql8s6MMhoJ3JcsYQQbKz16nmu6Z67ndStVv+bPgJZJUJAqFGh2K1+dXkpyQaUhHGvdDvzMRGOsDCOcTtxOrmmGyRD3adtSiQXV0Xh28ASdWKWHklTZkgbN1N8TYyy0HonYdgpsBnrRm4r/ee3cJBfRmMksN1SS+aIk58ikaPo96jFFieEjSzBRzN6KyAArTIzNyLUhBIsvL5OwXrus+bf1auOqSKMMR3AMpxDAOTTgBpoQAgEBz/AKb45yXpx352PeWnKKmUP4A+fzB07bjkk=</latexit><latexit sha1_base64="S98+A9F1A7/X9kKpxKRLJsyNrJs=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5LUgwoeCl48VjBaaUPZbDft0t1N2N0IpfRXePGg4tW/481/47bNQVsfDDzem2FmXpxxpo3vfzulldW19Y3ypru1vbO7V9k/uNdprggNScpT1YqxppxJGhpmOG1limIRc/oQD6+n/sMTVZql8s6MMhoJ3JcsYQQbKz16nmu6Z67ndStVv+bPgJZJUJAqFGh2K1+dXkpyQaUhHGvdDvzMRGOsDCOcTtxOrmmGyRD3adtSiQXV0Xh28ASdWKWHklTZkgbN1N8TYyy0HonYdgpsBnrRm4r/ee3cJBfRmMksN1SS+aIk58ikaPo96jFFieEjSzBRzN6KyAArTIzNyLUhBIsvL5OwXrus+bf1auOqSKMMR3AMpxDAOTTgBpoQAgEBz/AKb45yXpx352PeWnKKmUP4A+fzB07bjkk=</latexit><latexit sha1_base64="S98+A9F1A7/X9kKpxKRLJsyNrJs=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5LUgwoeCl48VjBaaUPZbDft0t1N2N0IpfRXePGg4tW/481/47bNQVsfDDzem2FmXpxxpo3vfzulldW19Y3ypru1vbO7V9k/uNdprggNScpT1YqxppxJGhpmOG1limIRc/oQD6+n/sMTVZql8s6MMhoJ3JcsYQQbKz16nmu6Z67ndStVv+bPgJZJUJAqFGh2K1+dXkpyQaUhHGvdDvzMRGOsDCOcTtxOrmmGyRD3adtSiQXV0Xh28ASdWKWHklTZkgbN1N8TYyy0HonYdgpsBnrRm4r/ee3cJBfRmMksN1SS+aIk58ikaPo96jFFieEjSzBRzN6KyAArTIzNyLUhBIsvL5OwXrus+bf1auOqSKMMR3AMpxDAOTTgBpoQAgEBz/AKb45yXpx352PeWnKKmUP4A+fzB07bjkk=</latexit>t4<latexit sha1_base64="Oz3Qd5GOKUcfiYhmIw/xV5yCeJ8=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IUQQUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Bhjko=</latexit><latexit sha1_base64="Oz3Qd5GOKUcfiYhmIw/xV5yCeJ8=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IUQQUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Bhjko=</latexit><latexit sha1_base64="Oz3Qd5GOKUcfiYhmIw/xV5yCeJ8=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IUQQUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Bhjko=</latexit>t5<latexit sha1_base64="lxGcpXqoBtNRgstZVj33VIqzKc4=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IURAUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Hnjks=</latexit><latexit sha1_base64="lxGcpXqoBtNRgstZVj33VIqzKc4=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IURAUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Hnjks=</latexit><latexit sha1_base64="lxGcpXqoBtNRgstZVj33VIqzKc4=">AAAB73icbVBNS8NAEJ3Urxq/qh69LDaCp5IURAUPBS8eKxittKFstpt26e4m7G6EUvorvHhQ8erf8ea/cdvmoK0PBh7vzTAzL84408b3v53Syura+kZ5093a3tndq+wf3Os0V4SGJOWpasVYU84kDQ0znLYyRbGIOX2Ih9dT/+GJKs1SeWdGGY0E7kuWMIKNlR49zzXdM9fzupWqX/NnQMskKEgVCjS7la9OLyW5oNIQjrVuB35mojFWhhFOJ24n1zTDZIj7tG2pxILqaDw7eIJOrNJDSapsSYNm6u+JMRZaj0RsOwU2A73oTcX/vHZukotozGSWGyrJfFGSc2RSNP0e9ZiixPCRJZgoZm9FZIAVJsZm5NoQgsWXl0lYr13W/Nt6tXFVpFGGIziGUwjgHBpwA00IgYCAZ3iFN0c5L8678zFvLTnFzCH8gfP5A1Hnjks=</latexit>f−1((−∞  t])  t ∈ R) and records all topological events (i.e. births and deaths of topological
features) in f−1((−∞  t]) when t goes from −∞ to +∞. PH outputs a 2-dimensional point multiset 
called persistence diagram (PD)  illustrated in Figure 1  where each 2-dimensional point represents a
lifespan of a particular topological feature with its birth and death time as its coordinates.

(cid:80)
Wasserstein geometry. Persistence diagram Dg can be considered as a discrete measure µDg =
u∈Dg δu where δu is the Dirac unit mass on u. Therefore  the bottleneck metric (a.k.a. ∞-
Wasserstein metric) is a popular choice to measure distances on the set of PDs with bounded
cardinalities. Given two PDs Dgi and Dgj  the bottleneck distance W∞ [Cohen-Steiner et al.  2007 
Carriere et al.  2017  Adams et al.  2017] is deﬁned as

(cid:107)x − γ(x)(cid:107)∞  

sup

x∈Dgi∪∆

(cid:1) = inf

γ

W∞(cid:0)Dgi  Dgj
 1

ρDg :=

(cid:88)



where ∆ := {(a  a) | a ∈ R} is the diagonal set  and γ : Dgi ∪ ∆ → Dgj ∪ ∆ is bijective.
Fisher information geometry. Given a bandwidth σ > 0  for a set Θ  one can smooth and
normalize µDg as follows 

 

Θ

Z

u∈Dg

(cid:80)

N(x; u  σI)

where Z =(cid:82)
each PD can be regarded as a point in a probability simplex P :=(cid:8)ρ |(cid:82) ρ(x)dx = 1  ρ(x) ≥ 0(cid:9)1.

u∈Dg N(x; u  σI)dx  N is a Gaussian function and I is an identity matrix. Therefore 

In case  one chooses Θ as an entire Euclidean space  each PD turns into a probability distribution as
in [Anirudh et al.  2016  Adams et al.  2017].
Fisher information metric (FIM)2 is a well-known Riemannian geometry on the probability simplex
P  especially in information geometry [Amari and Nagaoka  2007]. Given two points ρi and ρj in P 
the Fisher information metric is deﬁned as

x∈Θ

(1)

(cid:18)(cid:90)(cid:113)

(cid:19)

dP (ρi  ρj) = arccos

ρi(x)ρj(x)dx

.

(2)

3 Persistence Fisher Kernel (PF Kernel)

In this section  we propose the Persistence Fisher (PK) kernel for persistence diagrams (PDs).
For the bottleneck distance  two PDs Dgi and Dgj may be two discrete measures with different
masses. So  the transportation plan γ is bijective between Dgi ∪ ∆ and Dgj ∪ ∆ instead of between
Dgi and Dgj. Carriere et al. [2017]  for instance  used Wasserstein distance between Dgi and Dgj
where its transportation plans operate between Dgi ∪ Dgj∆ and Dgj ∪ Dgi∆ (nonnegative  not
necessarily normalized measures with same masses). Here  we denote Dgi∆ := {Π∆(u) | u ∈ Dgi}
where Π∆(u) is a projection of a point u on the diagonal set ∆. Following this line of work  we also
consider a distance between two measures Dgi ∪ Dgj∆ and Dgi ∪ Dgj∆ as a distance between Dgi
and Dgj for the Fisher information metric.
Deﬁnition 1. Let Dgi  Dgj be two ﬁnite and bounded persistence diagrams. The Fisher information
metric between Dgi and Dgj is deﬁned as follows 

dFIM(Dgi  Dgj) := dP

ρ(Dgi∪Dgj∆)  ρ(Dgj∪Dgj∆)

(3)
Lemma 3.1. Let D be the set of bounded and ﬁnite persistent diagrams. Then  (dFIM − τ ) is negative
deﬁnite on D for all τ ≥ π
2 .
Proof. Let consider the function τ − arccos(ξ) where τ ≥ π
series expansion for arccos(ξ) at 0  we have
τ − arccos(ξ) = τ − π
2

2 and ξ ∈ [0  1]  then apply the Taylor

∞(cid:88)

22i(i!)2(2i + 1)

x2i+1.

(2i)!

+

.

i=0

(cid:16)

(cid:17)

1In case  Θ is an inﬁnite set  then the corresponding probability simplex P has inﬁnite dimensions.
2FIM is also known as a particular pull-back metric on Riemannian manifold [Le and Cuturi  2015b].

3

So  all coefﬁcients of the Taylor series expansion are nonnegative. Following [Schoenberg  1942]
(Theorem 2  p. 102)  for τ ≥ π
2 and ξ ∈ [0  1]  τ − arccos(ξ) is positive deﬁnite. Consequently 
arccos(ξ) − τ is negative deﬁnite. Furthermore  for any PDs Dgi and Dgj in D  we have

(cid:90) (cid:113)

0 ≤

¯ρi(x)¯ρj(x)dx ≤ 1 

2 and α = exp (−tτ ) > 0.

where we denote ¯ρi = ρ(Dgi∪Dgj∆) and ¯ρj = ρ(Dgj∪Dgi∆). The lower bound is due to nonnegativity
of the probability simplex while the upper bound follows from the Cauchy-Schwarz inequality. Hence 
dFIM − τ is negative deﬁnite on D for all τ ≥ π
2 .
Based on Lemma 3.1  we propose a positive deﬁnite kernel for PDs under the Fisher information
geometry by following [Berg et al.  1984] (Theorem 3.2.2  p.74)  namely the Persistence Fisher
kernel 

kPF(Dgi  Dgj) := exp(cid:0)−tdFIM(Dgi  Dgj)(cid:1)  

(4)
where t is a positive scalar since we can rewrite the Persistence Fisher kernel as kPF(Dgi  Dgj) =

α exp(cid:0)−t(cid:0)dFIM(Dgi  Dgj) − τ(cid:1)(cid:1) where τ ≥ π
Remark 1. Let S+ :=(cid:8)ν |(cid:82) ν2(x)dx = 1  ν(x) ≥ 0(cid:9) be the positive orthant of the sphere  and
To the best of our knowledge  the kPF is the ﬁrst kernel relying on the Fisher information geometry
for measuring the similarity of PDs. Moreover  the kPF is positive deﬁnite without any approximation.
√·  where the square root is an element-wise function which
deﬁne the Hellinger mapping h(·) :=
transforms the probability simplex P into S+. The Fisher information metric between ρi and ρj in P
(Equation (2)) is equivalent to the geodesic distance between h(ρi) and h(ρj) in S+. From [Levy
and Loeve  1965]  the geodesic distance in S+ is a measure deﬁnite kernel distance. Following [Istas 
2012] (Proposition 2.8)  the geodesic distance in S+ is negative deﬁnite. This result is also noted in
[Feragen et al.  2015]. From [Berg et al.  1984] (Theorem 3.2.2  p.74)  the Persistence Fisher kernel
is positive deﬁnite. Therefore  our proof technique is not only independent and direct for the Fisher
information metric on the probability simplex without relying on the geodesic distance on S+  but
also valid for the case of inﬁnite dimensions due to [Schoenberg  1942] (Theorem 2  p. 102).
Remark 2. A closely related kernel to the Persistence Fisher kernel is the diffusion kernel [Lafferty
and Lebanon  2005] (p. 140)  based on the heat equation on the Riemannian manifold deﬁned by the
Fisher information metric to exploit the geometric structure of statistical manifolds. A generalized
family of kernels for the diffusion kernel is exploited in [Jayasumana et al.  2015  Feragen et al. 
2015]. To the best of our knowledge  the diffusion kernel has not been used for measuring the
similarity of PDs. If one uses the Fisher information metric (Deﬁnition 1) for PDs  and then plug
the distance into the diffusion kernel  one obtains a similar form to our proposed Persistence Fisher
kernel. A slight difference is that the diffusion kernel relies on d2
FIM while the Persistence Fisher
kernel is built upon dFIM itself. However  the Persistence Fisher kernel is positive deﬁnite while it is
unclear whether the diffusion kernel is positive deﬁnite3.

Computation. Given two ﬁnite PDs Dgi and Dgj with cardinalities bounded by N  in practice 
we consider a ﬁnite set Θ := Dgi ∪ Dgj∆ ∪ Dgj ∪ Dgi∆ without multiplicity in R2 for smoothed
and normalized measures ρ(·) (Equation 1)4. Then  let m be the cardinality of Θ  we have m ≤ 4N.
Consequently  the time complexity of ρ(·) is O(N m). For acceleration  we propose to apply the
Fast Gauss Transform [Greengard and Strain  1991  Morariu et al.  2009] to approximate the sum of
Gaussian functions in ρ(·) with a bounded error. The time complexity of ρ(·) is reduced from O(N m)
to O(N + m). Due to the low dimension of points in PDs (R2)  this approximation by the Fast Gauss
Transform is very efﬁcient in practice. Additionally  dP (Equation (2)) is evaluated between two points

in the m-dimensional probability simplex Pm−1 where Pm−1 :=(cid:8)x | x ∈ Rm

+  (cid:107)x(cid:107)1 = 1(cid:9). So  the

time complexity of the Persistence Fisher kernel kPF between two smoothed and normalized measures
is O(m). Hence  the time complexity of kPF between Dgi and Dgj is O(N 2)  or O(N ) for the
acceleration version with Fast Gauss Transform. We summarize the computation of dFIM in Algorithm
3Although the heat kernel is positive deﬁnite  the diffusion kernel on the probability simplex—the heat kernel
on multinomial manifold—does not have an explicit form. In practice  the diffusion kernel equation [Lafferty
and Lebanon  2005] (p. 140) is only its ﬁrst-order approximation.

4We leave the computation with an inﬁnite set Θ for future work.

4

Table 1: A comparison for time complexities and metric preservation of kernels for PDs. Noted that
the SW kernel is built upon the SW distance (an approximation of Wasserstein metric) while the PF
kernel uses the Fisher information metric without approximation.

Time complexity
Time complexity with approximation O(N )
Metric preservation

O(N 2) O(N 2) O(N 2 log N ) O(N 2)
O(N ) O(M N log N ) O(N )

kSW

(cid:88)

kPF

(cid:88)

kPSS

kPWG

1  where the second and third steps can be approximated with a bounded error via Fast Gaussian
Transform with a linear time complexity O(N ). Source code for Algorithm 1 can be obtained
in http://github.com/lttam/PersistenceFisher. We recall that the time complexity of the
Wasserstein distance between Dgi and Dgj is O(N 3 log N ) [Pele and Werman  2009] (§2.1). For
the Sliced Wasserstein distance (an approximation of Wasserstein distance)  the time complexity is
O(N 2 log N ) [Carriere et al.  2017]  or O(M N log N ) for its approximation with M projections
[Carriere et al.  2017]. We also summary a comparison for the time complexity and metric preservation
of kPF and related kernels for PDs in Table 1.

Algorithm 1 Compute dFIM for persistence diagrams
Input: Persistence diagrams Dgi  Dgj  and a bandwith σ > 0 for smoothing
Output: dFIM
1: Let Θ ← Dgi ∪ Dgj∆ ∪ Dgj ∪ Dgi∆ (a set for smoothed and normalized measures)

2: Compute ¯ρi = ρ(Dgi∪Dgj∆) ←(cid:104) 1
(cid:80)
where Z ←(cid:80)
4: Compute dFIM ← arccos(cid:0)(cid:10)√

Z
u∈Dgi∪Dgj∆
3: Compute ¯ρj = ρ(Dgj∪Dgi∆) similarly as ¯ρi.
√

(cid:80)
(cid:11)(cid:1) where (cid:104)· ·(cid:105) is a dot product and

u∈Dgi∪Dgj∆

N(x; u  σI)

N(x; u  σI)

x∈Θ

¯ρi 

¯ρj

x∈Θ

(cid:105)

√· is element-wise.

4 Theoretical Analysis

d−1 where S+

d−1 :=(cid:8)x | x ∈ Rd

In this section  we analyze for the Persistence Fisher kernel kPF (in Equation (4)) where the Hellinger
mapping h of a smoothed and normalized measure ρ(·) is on the positive orthant of the d-dimension
unit sphere S+
d−1. We denote xi and
bounded and ﬁnite PDs  and µ be the uniform probability distribution on S+
xj ∈ S+
d−1 as corresponding mapped points through the Hellinger mapping h of smoothed and
normalized measures ρ(Dgi∪Dgj∆) and ρ(Dgj∪Dgi∆) respectively. Then  we rewrite the Persistence
Fisher kernel between xi and xj as follows 

+ (cid:107)x(cid:107)2 = 1(cid:9)5. Let Dgi  Dgj be PDs in the set D of

kPF(xi  xj) = exp (−t arccos ((cid:104)xi  xj(cid:105))).

(5)

Eigensystem. Let TkPF
Persistence Fisher kernel kPF  which is deﬁned as

: L2(S+

d−1  µ) → L2(S+

(cid:90)

(TkPFf ) (·) :=

kPF(x ·)f (x)dµ(x).

d−1  µ) be the integral operator induced by the

Following [Smola et al.  2001] (Lemma 4)  we derive an eigensystem of the integral operator TkPF as
in Proposition 1.
Proposition 1. Let {ai}i≥0 be the coefﬁcients of Legendre polynomial expansion of the Persistence
Fisher kernel kPF(x  z) deﬁned on S+

d−1 as in Equation (5) 

d−1 × S+

aiP d

i ((cid:104)x  z(cid:105)) 

(6)

kPF(x  z) =

5It is corresponding to a ﬁnite set Θ.

∞(cid:88)

i=0

5

i is the associated Legendre polynomial of degree i. Let |Sd−1| := 2πd/2

spherical harmonics of order i on Sd−1  and(cid:8)Y d

where P d
of Sd−1 where Γ(·) is the Gamma function  N (d  i) := (d+2i−2)(d+i−3)!

Γ(d/2) denote the surface
denote the multiplicity of
denote any ﬁxed orthonormal basis
for the subspace of all homogeneous harmonics of order i on Sd−1. Then  the eigensystem (λi j  φi j)
of the integral operator TkPF induced by the Persistence Fisher kernel kPF is

1≤j≤N (d i)

(d−2)!i!

(cid:9)

i j

φi j = Y d
i j 
ai |Sd−1|
N (d  i)

λi j =

(7)

(8)

of multiplicity N (d  i).

[Muller  2012] (§4  p. 29)  we have(cid:80)N (d i)
into Equation (6)  and note that(cid:82)

Proof. From the Addition Theorem [Muller  2012] (Theorem 2  p. 18) and the Funk-Hecke formula
i ((cid:104)x  z(cid:105))  then replace P d

i j(z) = N (d i)

j=1 Y d
Y d
i j(x)Y d

i j(x)Y d
i(cid:48) j(cid:48)(x)dx = δi i(cid:48)δj j(cid:48)  we complete the proof.

|Sd−1| P d

i

Sd−1

Proposition 2. All coefﬁcients of Legendre polynomial expansion of the Persistence Fisher kernel
are nonnegative.

Proof. From Lemma 3.1  the kPF is positive deﬁnite. Applying Schoenberg [1942] (Theorem 1  p.
101) for kPF deﬁned on S+

d−1 as in Equation (5)  we obtain the result.

d−1 × S+

The eigensystem of the integral operator TkPF induced by the PF kernel plays an important role to
derive generalization error bounds for kernel machines with the proposed PF kernel via covering
numbers and Rademacher averages as in Proposition 3 and Proposition 4 respectively.

Covering numbers. Given a set of ﬁnite points S = (cid:8)xi | xi ∈ S+

d−1  d ≥ 3(cid:9)  the Persistence

Fisher kernel hypothesis class with R-bounded weight vectors for S is deﬁned as follows

FR(S) = {f | f(xi) = (cid:104)w  φ (xi)(cid:105)H  (cid:107)w(cid:107)H ≤ R}  

where (cid:104)φ (xi)   φ (xj)(cid:105)H = kPF(xi  xj). (cid:104)· ·(cid:105)H and (cid:107)·(cid:107)H are an inner product and a norm in the
corresponding Hilbert space respectively. Following [Guo et al.  1999]  we derive bounds on
the generalization performance of the PF kernel on kernel machines via the covering numbers
N (· FR(S)) [Shalev-Shwartz and Ben-David  2014] (Deﬁnition 27.1  p. 337) as in Proposition 3.
Proposition 3. Assume the number of non-zero coefﬁcients {ai} in Equation (6) is ﬁnite  and r is
the maximum index of the non-zero coefﬁcients. Let q := arg maxi λi ·  choose α ∈ N such that
. Then 
α <

2 with i (cid:54)= q  and deﬁne ε := 6R

(cid:17)N (d q)

(cid:16) λq ·

(cid:114)

N (d  r)

(cid:16)

(cid:17)

i=0 i(cid:54)=q ai

λi ·

aqα−2/N (d q) +(cid:80)∞
(cid:13)(cid:13)∞ ≤ (cid:113) N (d i)

N (ε FR(S)) ≤ α.

sup
xi∈S

Proof. From [Minh et al.  2006] (Lemma 3)  we have(cid:13)(cid:13)Y d
(cid:13)(cid:13)∞ ≤ (cid:113) N (d r)
eigenfunctions of kPF satisfy that(cid:13)(cid:13)Y d

|Sd−1| . It is easy to check
that ∀d ≥ 3  i ≥ j ≥ 0  we have N (d  i) ≥ N (d  j). Therefore  following Proposition 1  all
|Sd−1| . Additionally  the multiplicity of λi · is
N (d  i)  and N (d  i)λi · = ai |Sd−1| (Equation (8)). Hence  from [Guo et al.  1999] (Theorem 1)  we
obtain the result.

i j

i j

Rademacher averages. We provide a different family of generalization error bounds via
Rademacher averages [Bartlett et al.  2005]. By plugging the eigensystem of the PF kernel as
in Proposition 1 into the localized averages of function classes based on the PF kernel with respect to
the uniform probability distribution µ on S+
d−1 [Mendelson  2003] (Theorem 2.1)  we obtain a bound
as in Proposition 4.

6

Proposition 4. Let {xi}1≤i≤m be independent  distributed according to the uniform probabil-
d−1  denote {σi}1≤i≤m for independent Rademacher random variables 
ity distribution µ on S+
HkPF for the unit ball of the reproducing kernel Hilbert space corresponding with the Riema-
If λq · ≥ 1/m  for τ ≥ 1/(m|Sd−1|) 
nian manifold kernel kPF  and let q := arg maxi λi ·.

let Ψ(τ ) :=

N (d  i)

  then there are absolute constants C(cid:96) and Cu

(9)

(cid:118)(cid:117)(cid:117)(cid:116)|Sd−1|

(cid:32) (cid:80)

ai + τ (cid:80)

ai<τ N (d i)

ai≥τ N (d i)

which satisfy

C(cid:96)Ψ(τ ) ≤ E sup
f∈HkPF
Eµ f2
d−1|≤τ
|S

where E is an expectation.

(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ CuΨ(τ ) 

σif(xi)

From Proposition 3 and Proposition 4  a decay rate of the eigenvalues of the integral operator TkPF is
relative with the capacity of the kernel learning machines. When the decay rate of the eigenvalues is
large  the capacity of kernel machines is reduced. So  if the training error of kernel machines is small 
then it can lead to better bounds on generalization error. The resulting bounds for both the covering
number (Proposition 3) and the Rademacher averages (Proposition 4) are essentially the same as the
standard ones for a Gaussian kernel on a Euclidean space.

Bounding for kPF induced squared distance with respect to dFIM. The squared distance induced
by the PF kernel  denoted as d2
kPF  can be computed by the Hilbert norm of the difference between
two corresponding mappings. Given two persistent diagram Dgi and Dgj  we have

(cid:0)Dgi  Dgj

(cid:1) := kPF (Dgi  Dgi) + kPF

(cid:0)Dgj  Dgj

(cid:1) − 2kPF

(cid:0)Dgi  Dgj

(cid:1) .

d2
kPF

We recall that kPF is based on the Fisher information geometry. So  it is of interest to bound the PF
kernel induced squared distance d2
kPF with respect to the corresponding Fisher information metric
dFIM between PDs as in Lemma 4.1.
Lemma 4.1. Let D be the set of bounded and ﬁnite persistent diagrams. Then  ∀Dgi  Dgj ∈ X 

kPF (Dgi  Dgj) ≤ 2tdFIM(Dgi  Dgj) 
d2

where t is a parameter of kPF.

(cid:0)Dgi  Dgj

Proof. We have d2
2tdFIM

kPF(Dgi  Dgj) = 2(cid:0)1 − kPF
(cid:0)Dgi  Dgj
(cid:1)  since 1 − exp(−a) ≤ a ∀a ≥ 0.

(cid:1)(cid:1) = 2(1 − exp(cid:0)−tdFIM

(cid:0)Dgi  Dgj

(cid:1)(cid:1) ≤

From Lemma 4.1  it implies that the Persistence Fisher kernel is stable on Riemannian geometry
in a similar sense as the work of Kwitt et al. [2015]  and Reininghaus et al. [2015] on Wasserstein
geometry.

Inﬁnite divisibility for the Persistence Fisher kernel.
Lemma 4.2. The Persistence Fisher kernel kPF is inﬁnitely divisible.

Proof. For m ∈ N∗  let kPFm := exp(cid:0)− t

m dFIM

(cid:1)  so (kPFm)m = kPF and note that kPFm is positive deﬁnite.

Hence  following Berg et al. [1984] (§3  Deﬁnition 2.6  p. 76)  we have the result.

As for inﬁnitely divisible kernels  the Gram matrix of the PF kernel does not need to be recomputed for
each choice of t (Equation (4))  since it sufﬁces to compute the Fisher information metric between PDs
in training set only once. This property is shared with the Sliced Wasserstein kernel [Carriere et al. 
2017]. However  neither Persistence Scale Space kernel [Reininghaus et al.  2015] nor Persistence
Weighted Gaussian kernel [Kusano et al.  2016] has this property.

7

Table 2: Results on SVM classiﬁcation. The averaged accuracy (%) and standard deviation are shown.

kPSS
kPWG
kSW
Prob+kG
Tang+kG
kPF

MPEG7

73.33 ± 4.17
74.83 ± 4.36
76.83 ± 3.75
55.83 ± 5.45
66.17 ± 4.01
80.00 ± 4.08

Orbit

72.38 ± 2.41
76.63 ± 0.66
83.60 ± 0.87
72.89 ± 0.62
77.32 ± 0.72
85.87 ± 0.77

5 Experimental Results

We evaluated the Persistence Fisher kernel with support vector machines (SVM) on many benchmark
datasets. We consider ﬁve baselines as follows: (i) the Persistence Scale Space kernel (kPSS)  (ii)
the Persistence Weighted Gaussian kernel (kPWG)  (iii) the Sliced Wasserstein kernel (kSW)  (iv) the
smoothed and normalized measures in the probability simplex with the Gaussian kernel (Prob + kG) 
and (v) the tangent vector representation [Anirudh et al.  2016] with the Gaussian kernel (Tang +
kG). Practically  Euclidean metric is not a suitable geometry for the probability simplex [Le and
Cuturi  2015a b]. So  the (Prob + kG) approach may not work well for PDs. For hyper-parameters  we
typically choose them through cross validation. For baseline kernels  we follow their corresponding
authors to form sets of hyper-parameter candidates  and the bandwidth of the Gaussian kernel in
(Prob + kG) and (Tang + kG) is chosen from 10{−3:1:3}. For the Persistence Fisher kernel  there are 2
hyper-parameters: t (Equation (4)) and σ for smoothing measures (Equation (1)). We choose 1/t
from {q1  q2  q5  q10  q20  q50} where qs is the s% quantile of a subset of Fisher information metric

between PDs  observed on the training set  and σ from(cid:8)10−3:1:3(cid:9). For SVM  we use Libsvm (one-
of SVM from(cid:8)10−2:1:2(cid:9). For PDs  we used the DIPHA toolbox6.

vs-one) [Chang and Lin  2011] for multi-class classiﬁcation  and choose a regularization parameter

5.1 Orbit Recognition

(5K/300)

MPEG7
(200/80)

Granular
(35/20.4K)

SiO2

(80/30K)

Table 3: Computational time (seconds) with approximation.
For each dataset  the ﬁrst number in the parenthesis is the
number of PDs while the second one is the maximum number
of points in PDs.
Orbit

It is a synthesized dataset proposed
by [Adams et al.  2017] (§6.4.1) for
linked twist map which is a discrete
dynamical system modeling ﬂow. The
linked twist map is used to model
ﬂows in DNA microarrays [Hertzsch
et al.  2007]. Given a parameter r > 0 
and initial positions (s0  t0) ∈ [0  1]2 
its orbit is described as si+1 = si +
rti(1 − ti) mod 1  and ti+1 = ti +
rsi+1(1−si+1) mod 1. Adams et al.
[2017] proposed 5 classes  corresponding to 5 different parameters r = 2.5  3.5  4  4.1  4.3. For each
parameter r  we generated 1000 orbits where each orbit has 1000 points with random initial posi-
tions. We randomly split 70%/30% for training and test  and repeated 100 times. We extract only
1-dimensional topological features with Vietoris-Rips complex ﬁltration [Edelsbrunner and Harer 
2008] for PDs. The accuracy results on SVM are summarized in the third column of Table 2. The
PF kernel outperforms all other baselines. The (Prob + kG) does not performance well as expected.
Moreover  the kPF and kSW which enjoy the Fisher information geometry and Wasserstein geometry
for PDs respectively  clearly outperform other approaches. As in the second column of Table 3  the
computational time of kPF is faster than kPSS  but slower than kSW and kPWG for PDs.

6473
8756
11024
9891

8.30
17.44
38.14
22.70

kSW
kPWG
kPSS
kPF

1.55
5.23
7.51
6.63

249
288
515
318

5.2 Object Shape Classiﬁcation

We consider a 10-class subset7 of MPEG7 object shape dataset [Latecki et al.  2000]. Each class has
20 samples. We resize each image such that its length is shorter or equal 256  and extract a boundary
for object shapes before computing PDs. For simplicity  we only consider 1-dimensional topological

6https://github.com/DIPHA/dipha
7The 10-classes are: apple  bell  bottle  car  classic  cup  device0  face  Heart and key.

8

Figure 2: The kernel Fisher discriminant ratio (KFDR) graphs.

features with the traditional Vietoris-Rips complex ﬁltration [Edelsbrunner and Harer  2008] for PDs8.
We also randomly split 70%/30% for training and test  and repeated 100 times. The accuracy results
on SVM are summarized in the second column of Table 2. The Persistence Fisher kernel compares
favorably with other baseline kernels for PDs. All approaches based on the implicit representation
via kernels for PDs outperform ones based on the explicit vector representation with Gaussian kernel
by a large margin. Additionally  the kPF and kSW also compares favorably with other approaches. As
in the third column of Table 3  the computational time of kPF is comparative with kPWG and kPSS  but
slower than the kSW.

5.3 Change Point Detection for Material Data Analysis

We evaluated the proposed kernel for the change point detection problem for material data analysis on
granular packing system [Francois et al.  2013] and SiO2[Nakamura et al.  2015] datasets. We use the
kernel Fisher discriminant ratio [Harchaoui et al.  2009] (KFDR) as a statistical quantity and set 10−3
for the regularization of KFDR as in [Kusano et al.  2018]. We use the ball model ﬁltration to extract
the 2-dimensional topological features of PDs for granular packing system dataset  and 1-dimensional
topological features of PDs for SiO2 dataset. We illustrate the KFDR graphs for the granular packing
system and SiO2 datasets in Figure 2. For granular tracking system dataset  all methods obtain
the change point as the 23rd index. They supports the observation result in [Anonymous  1972]
(corresponding id = 23). For the SiO2 datasets  all methods obtain the results within the supported
range (35 ≤ id ≤ 50) from the traditional physical approach [Elliott  1983]. The kPF compares
favorably with other baseline approaches as in Figure 2. As in the fourth and ﬁfth columns of Table 3 
kPF is faster than kPSS  but slower than kSW and kPWG.

6 Conclusions

In this work  we propose the positive deﬁnite Persistence Fisher (PF) kernel for persistence diagrams
(PDs). The PF kernel is relied on the Fisher information geometry without approximation for PDs.
Moreover  the proposed kernel has many nice properties from both theoretical and practical aspects
such as stability  inﬁnite divisibility  linear time complexity over the number of points in PDs  and
improving performances of other baseline kernels for PDs as well as implicit vector representation
with Gaussian kernel for PDs in many different tasks on various benchmark datasets.

8A more advanced ﬁltration for this task was proposed in [Turner et al.  2014].

9

0102030(id = 23)Granular packing systemkPSS0102030(id = 23)kPWG0102030(id = 23)kSW0102030(id = 23)Prob + kG0102030(id = 23)Tang + kG0102030(id = 23)kPF020406080(id = 46)SiO2020406080(id = 37)020406080(id = 43)020406080(id = 35)020406080(id = 35)020406080(id = 42)Acknowledgments

We thank Ha Quang Minh  and anonymous reviewers for their comments. TL acknowledges the
support of JSPS KAKENHI Grant number 17K12745. MY was supported by the JST PRESTO
program JPMJPR165A.

References
Henry Adams  Tegan Emerson  Michael Kirby  Rachel Neville  Chris Peterson  Patrick Shipman 
Sofya Chepushtanova  Eric Hanson  Francis Motta  and Lori Ziegelmeier. Persistence images: A
stable vector representation of persistent homology. The Journal of Machine Learning Research 
18(1):218–252  2017.

Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry  volume 191. American

Mathematical Soc.  2007.

Rushil Anirudh  Vinay Venkataraman  Karthikeyan Natesan Ramamurthy  and Pavan Turaga. A
riemannian framework for statistical analysis of topological persistence diagrams. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition Workshops  pages 68–76 
2016.

Anonymous. What is random packing? Nature  239:488–489  1972.

Peter L Bartlett  Olivier Bousquet  Shahar Mendelson  et al. Local rademacher complexities. The

Annals of Statistics  33(4):1497–1537  2005.

Christian Berg  Jens Peter Reus Christensen  and Paul Ressel. Harmonic analysis on semigroups.

Springer-Verlag  1984.

Peter Bubenik. Statistical topological data analysis using persistence landscapes. The Journal of

Machine Learning Research  16(1):77–102  2015.

Zixuan Cang  Lin Mu  Kedi Wu  Kristopher Opron  Kelin Xia  and Guo-Wei Wei. A topological

approach for protein classiﬁcation. Molecular Based Mathematical Biology  3(1)  2015.

Gunnar Carlsson  Tigran Ishkhanov  Vin De Silva  and Afra Zomorodian. On the local behavior of

spaces of natural images. International journal of computer vision  76(1):1–12  2008.

Mathieu Carriere  Steve Y Oudot  and Maks Ovsjanikov. Stable topological signatures for points on

3d shapes. In Computer Graphics Forum  volume 34  pages 1–12. Wiley Online Library  2015.

Mathieu Carriere  Marco Cuturi  and Steve Oudot. Sliced Wasserstein kernel for persistence dia-
grams. In Proceedings of the 34th International Conference on Machine Learning  volume 70 of
Proceedings of Machine Learning Research  pages 664–673  2017.

Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM

transactions on intelligent systems and technology (TIST)  2(3):27  2011.

Frederic Chazal  Brittany Fasy  Fabrizio Lecci  Bertrand Michel  Alessandro Rinaldo  and Larry
Wasserman. Subsampling methods for persistent homology. In International Conference on
Machine Learning  pages 2143–2151  2015.

Chao Chen and Novi Quadrianto. Clustering high dimensional categorical data via topographical

features. In International Conference on Machine Learning  pages 2732–2740  2016.

David Cohen-Steiner  Herbert Edelsbrunner  and John Harer. Stability of persistence diagrams.

Discrete & Computational Geometry  37(1):103–120  2007.

Vin De Silva  Robert Ghrist  et al. Coverage in sensor networks via persistent homology. Algebraic

& Geometric Topology  7(1):339–358  2007.

Barbara Di Fabio and Massimo Ferri. Comparing persistence diagrams through complex vectors. In

International Conference on Image Analysis and Processing  pages 294–305. Springer  2015.

10

Herbert Edelsbrunner and John Harer. Persistent homology-a survey. Contemporary mathematics 

453:257–282  2008.

Herbert Edelsbrunner  David Letscher  and Afra Zomorodian. Topological persistence and simpliﬁca-
tion. In Proceedings 41st Annual Symposium on Foundations of Computer Science  pages 454–463 
2000.

Stephen Richard Elliott. Physics of amorphous materials. Longman Group  Longman House  Burnt

Mill  Harlow  Essex CM 20 2 JE  England  1983.  1983.

Aasa Feragen  Francois Lauze  and Soren Hauberg. Geodesic exponential kernels: When curvature
and linearity conﬂict. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 3032–3042  2015.

Nicolas Francois  Mohammad Saadatfar  R Cruikshank  and A Sheppard. Geometrical frustration in
amorphous and partially crystallized packings of spheres. Physical review letters  111(14):148001 
2013.

Leslie Greengard and John Strain. The fast gauss transform. SIAM Journal on Scientiﬁc and Statistical

Computing  12(1):79–94  1991.

Ying Guo  Peter L Bartlett  John Shawe-Taylor  and Robert C Williamson. Covering numbers for
support vector machines. In Proceedings of the twelfth annual conference on Computational
learning theory  pages 267–277  1999.

Zaid Harchaoui  Eric Moulines  and Francis R Bach. Kernel change-point analysis. In Advances in

neural information processing systems  pages 609–616  2009.

Jan-Martin Hertzsch  Rob Sturman  and Stephen Wiggins. Dna microarrays: design principles for

maximizing ergodic  chaotic mixing. Small  3(2):202–218  2007.

Christoph Hofer  Roland Kwitt  Marc Niethammer  and Andreas Uhl. Deep learning with topological

signatures. In Advances in Neural Information Processing Systems  pages 1633–1643  2017.

Jacques Istas. Manifold indexed fractional ﬁelds? ESAIM: Probability and Statistics  16:222–276 

2012.

Sadeep Jayasumana  Richard Hartley  Mathieu Salzmann  Hongdong Li  and Mehrtash Harandi.
Kernel methods on riemannian manifolds with gaussian rbf kernels. IEEE transactions on pattern
analysis and machine intelligence  37(12):2464–2477  2015.

Peter M Kasson  Afra Zomorodian  Sanghyun Park  Nina Singhal  Leonidas J Guibas  and Vijay S
Pande. Persistent voids: a new structural metric for membrane fusion. Bioinformatics  23(14):
1753–1759  2007.

Genki Kusano  Yasuaki Hiraoka  and Kenji Fukumizu. Persistence weighted gaussian kernel for
topological data analysis. In International Conference on Machine Learning  pages 2004–2013 
2016.

Genki Kusano  Kenji Fukumizu  and Yasuaki Hiraoka. Kernel method for persistence diagrams via
kernel embedding and weight factor. Journal of Machine Learning Research  18(189):1–41  2018.

Roland Kwitt  Stefan Huber  Marc Niethammer  Weili Lin  and Ulrich Bauer. Statistical topological
data analysis-a kernel perspective. In Advances in neural information processing systems  pages
3070–3078  2015.

John Lafferty and Guy Lebanon. Diffusion kernels on statistical manifolds. Journal of Machine

Learning Research  6(Jan):129–163  2005.

Longin Jan Latecki  Rolf Lakamper  and T Eckhardt. Shape descriptors for non-rigid shapes with a
single closed contour. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  volume 1  pages 424–429  2000.

Tam Le and Marco Cuturi. Adaptive euclidean maps for histograms: generalized aitchison embed-

dings. Machine Learning  99(2):169–187  2015a.

11

Tam Le and Marco Cuturi. Unsupervised riemannian metric learning for histograms using aitchison

transformations. In International Conference on Machine Learning  pages 2002–2011  2015b.

Hyekyoung Lee  Moo K Chung  Hyejin Kang  Bung-Nyun Kim  and Dong Soo Lee. Discriminative
persistent homology of brain networks. In International Symposium on Biomedical Imaging: From
Nano to Macro  pages 841–844  2011.

John M Lee. Riemannian manifolds: an introduction to curvature  volume 176. Springer Science &

Business Media  2006.

Paul Levy and Michel Loeve. Processus stochastiques et mouvement brownien. Gauthier-Villars

Paris  1965.

Shahar Mendelson. On the performance of kernel classes. Journal of Machine Learning Research  4

(Oct):759–771  2003.

Ha Quang Minh  Partha Niyogi  and Yuan Yao. Mercer’s theorem  feature maps  and smoothing. In

International Conference on Computational Learning Theory  pages 154–168. Springer  2006.

Vlad I Morariu  Balaji V Srinivasan  Vikas C Raykar  Ramani Duraiswami  and Larry S Davis.
Automatic online tuning for fast gaussian summation. In Advances in neural information processing
systems  pages 1113–1120  2009.

Claus Muller. Analysis of spherical symmetries in Euclidean spaces  volume 129. Springer Science

& Business Media  2012.

Takenobu Nakamura  Yasuaki Hiraoka  Akihiko Hirata  Emerson G Escolar  and Yasumasa Nishiura.
Persistent homology and many-body atomic structure for medium-range order in the glass. Nan-
otechnology  26(30):304001  2015.

Oﬁr Pele and Michael Werman. Fast and robust earth mover’s distances. In International Conference

on Computer Vision  pages 460–467. IEEE  2009.

Giovanni Petri  Paul Expert  Federico Turkheimer  Robin Carhart-Harris  David Nutt  Peter J Hellyer 
and Francesco Vaccarino. Homological scaffolds of brain functional networks. Journal of The
Royal Society Interface  11(101)  2014.

Gabriel Peyre and Marco Cuturi. Computational Optimal Transport.

//optimaltransport.github.io.

2017. URL http:

Jan Reininghaus  Stefan Huber  Ulrich Bauer  and Roland Kwitt. A stable multi-scale kernel for
topological machine learning. In Proceedings of the IEEE conference on computer vision and
pattern recognition (CVPR)  pages 4741–4748  2015.

I. J. Schoenberg. Positive deﬁnite functions on spheres. Duke Mathematical Journal  9:96–108  1942.

Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press  2014.

Gurjeet Singh  Facundo Memoli  Tigran Ishkhanov  Guillermo Sapiro  Gunnar Carlsson  and Dario L
Ringach. Topological analysis of population activity in visual cortex. Journal of vision  8(8):
11–11  2008.

Alex J Smola  Zoltan L Ovari  and Robert C Williamson. Regularization with dot-product kernels. In

Advances in neural information processing systems  pages 308–314  2001.

Katharine Turner  Sayan Mukherjee  and Doug M Boyer. Persistent homology transform for modeling

shapes and surfaces. Information and Inference: A Journal of the IMA  3(4):310–344  2014.

Cédric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc.  2003.

Kelin Xia and Guo-Wei Wei. Persistent homology analysis of protein structure  ﬂexibility  and folding.

International journal for numerical methods in biomedical engineering  30(8):814–844  2014.

12

,Tam Le
Makoto Yamada
Thomas Lucas
Konstantin Shmelkov
Karteek Alahari
Cordelia Schmid
Jakob Verbeek