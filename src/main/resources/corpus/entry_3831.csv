2017,Nonparametric Online Regression while Learning the Metric,We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix $\boldsymbol{G}$ of the regression function (automatically adapting to the effective rank of this matrix)  while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of $\boldsymbol{G}$. As a preliminary step in our analysis  we extend a nonparametric online learning algorithm by Hazan and Megiddo enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric.,Nonparametric Online Regression

while Learning the Metric

Ilja Kuzborskij

EPFL

Switzerland

ilja.kuzborskij@gmail.com

Nicol`o Cesa-Bianchi

Dipartimento di Informatica

Universit`a degli Studi di Milano

Milano 20135  Italy

nicolo.cesa-bianchi@unimi.it

Abstract

We study algorithms for online nonparametric regression that learn the directions
along which the regression function is smoother. Our algorithm learns the Ma-
halanobis metric based on the gradient outer product matrix G of the regression
function (automatically adapting to the effective rank of this matrix)  while si-
multaneously bounding the regret —on the same data sequence— in terms of the
spectrum of G. As a preliminary step in our analysis  we extend a nonparametric
online learning algorithm by Hazan and Megiddo enabling it to compete against
functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis
metric.

1

Introduction

An online learner is an agent interacting with an unknown and arbitrary environment over a sequence
of rounds. At each round t  the learner observes a data point (or instance) xt ∈ X ⊂ Rd  outputs a

prediction(cid:98)yt for the label yt ∈ R associated with that instance  and incurs some loss (cid:96)t((cid:98)yt)  which in
this paper is the square loss ((cid:98)yt − yt)2. At the end of the round  the label yt is given to the learner 

which he can use to reduce his loss in subsequent rounds. The performance of an online learner is
typically measured using the regret. This is deﬁned as the amount by which the learner’s cumulative
loss exceeds the cumulative loss (on the same sequence of instances and labels) of any function f in
a given reference class F of functions 

∀f ∈ F .

t=1

RT (f ) =

(1)
Note that typical regret bounds apply to all f ∈ F and to all individual data sequences. However 
the bounds are allowed to scale with parameters arising from the interplay between f and the data
sequence.
In order to capture complex environments  the reference class of functions should be large. In this
work we focus on nonparametric classes F  containing all differentiable functions that are smooth
with respect to some metric on X . Our approach builds on the simple and versatile algorithm for
nonparametric online learning introduced in [6]. This algorithm has a bound on the regret RT (f ) of
order (ignoring logarithmic factors)

T(cid:88)

(cid:16)

(cid:96)t((cid:98)yt) − (cid:96)t

(cid:0)f (xt)(cid:1)(cid:17)

1 +

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

 T

Here (cid:107)∂if(cid:107)∞ is the value of the partial derivative ∂f (x)(cid:14)∂xi maximized over x ∈ X . The square

root term is the Lipschitz constant of f  measuring smoothness with respect to the Euclidean metric.

(2)

i=1

d

1+d

∀f ∈ F .

(cid:107)∂if(cid:107)2∞

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

However  in some directions f may be smoother than in others. Therefore  if we knew in advance the
set of directions along which the best performing reference functions f are smooth  we could use
this information to control regret better. In this paper we extend the algorithm from [6] and make it
adaptive to the Mahalanobis distance deﬁned through an arbitrary positive deﬁnite matrix M with
i=1 and unit spectral radius (λ1 = 1). We prove a bound on the regret RT (f ) of

spectrum(cid:8)(ui  λi)(cid:9)d

order (ignoring logarithmic factors)

(cid:112)detκ(M ) +

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

i=1

 T

(cid:107)∇uif(cid:107)2∞

λi

ρT

1+ρT

∀f ∈ F .

(3)

(cid:14)λi remain controlled)  then our bound improves on (2). On the other

Here ρT ≤ d is  roughly  the number of eigenvalues of M larger than a threshold shrinking
polynomially in T   and detκ(M ) ≤ 1 is the determinant of M truncated at λκ (with κ ≤ ρT ). The
quantity (cid:107)∇uif(cid:107)2∞ is deﬁned like (cid:107)∂if(cid:107)∞  but with the directional derivative ∇f (x)(cid:62)u instead of
the partial derivative. When the spectrum of M is light-tailed (so that ρT (cid:28) d and  simultaneously 
detκ(M ) (cid:28) 1)  with the smaller eigenvalues λi corresponding to eigenvectors in which f is smoother
(so that the ratios (cid:107)∇uif(cid:107)2∞
hand  when no preliminary knowledge about good f is available  we may run the algorithm with M
equal to the identity matrix and recover exactly the bound (2).
Given that the regret can be improved by informed choices of M  it is natural to ask whether some
kind of improvement is still possible when M is learned online  from the same data sequence on
which the regret is being measured. Of course  this question makes sense if the data tell us something
about the smoothness of the f against which we are measuring the regret. In the second part of the
paper we implement this idea by considering a scenario where instances are drawn i.i.d. from some
unknown distribution  labels are stochastically generated by some unknown regression function f0 
and we have no preliminary knowledge about the directions along which f0 is smoother.

In this stochastic scenario  the expected gradient outer product matrix G = E(cid:2)∇f0(X)∇f0(X)(cid:62)(cid:3)
provides a natural choice for the matrix M in our algorithm. Indeed  E(cid:2)(cid:0)∇f0(X)(cid:62)ui
(cid:1)2(cid:3) = µi

where u1  . . .   ud are the eigenvectors of G while µ1  . . .   µd are the corresponding eigenvalues.
Thus  eigenvectors u1  . . . ud capture the principal directions of variation for f. In fact  assuming
that the labels obey a statistical model Y = g(BX) + ε where ε is the noise and B ∈ Rk×d projects
X onto a k-dimensional subspace of X   one can show [21] that span(B) ≡ span(u1  . . .   ud). In
this sense  G is the “best” metric  because it recovers the k-dimensional relevant subspace.

When G is unknown  we run our algorithm in phases using a recently proposed estimator (cid:98)G of G.

The estimator is trained on the same data sequence and is fed to the algorithm at the beginning of
each phase. Under mild assumptions on f0  the noise in the labels  and the instance distribution  we
prove a high probability bound on the regret RT (f0) of order (ignoring logarithmic factors)

1 +

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

(cid:0)(cid:13)(cid:13)∇uj f0

(cid:13)(cid:13)∞ +(cid:13)(cid:13)∇V f0

(cid:13)(cid:13)∞

(cid:1)2

µj/µ1

 T

(cid:101)ρT
1+(cid:101)ρT .

(4)

j=1

Observe that the rate at which the regret grows is the same as the one in (3)  though now the effective

dimension parameter(cid:101)ρT is larger than ρT by an amount related to the rate of convergence of the
eigenvalues of (cid:98)G to those of G. The square root term is also similar to (3)  but for the extra quantity
(cid:107)∇V f0(cid:107)∞  which accounts for the error in approximating the eigenvectors of G. More precisely 
(cid:107)∇V f0(cid:107)∞ is (cid:107)∇vf(cid:107)∞ maximized over directions v in the span of V   where V contains those
eigenvectors of G that cannot be identiﬁed because their eigenvalues are too close to each other (we
come back to this issue shortly). Finally  we lose the dependence on the truncated determinant  which
is replaced here by its trivial upper bound 1.
The proof of (2) in [6] is based on the sequential construction of a sphere packing of X   where the
spheres are centered on adaptively chosen instances xt  and have radii shrinking polynomially with
time. Each sphere hosts an online learner  and each new incoming instance is predicted using the
learner hosted in the nearest sphere. Our variant of that algorithm uses an ellipsoid packing  and
computes distances using the Mahalanobis distance (cid:107)·(cid:107)M . The main new ingredient in the analysis
leading to (3) is our notion of effective dimension ρT (we call it the effective rank of M)  which
measures how fast the spectrum of M vanishes. The proof also uses an ellipsoid packing bound and
a lemma relating the Lipschitz constant to the Mahalanobis distance.

2

The proof of (4) is more intricate because G is only known up to a certain approximation. We use an

estimator (cid:98)G  recently proposed in [14]  which is consistent under mild distributional assumptions

when f0 is continuously differentiable. The ﬁrst source of difﬁculty is adjusting the notion of effective
rank (which the algorithm needs to compute) to compensate for the uncertainty in the knowledge of
the eigenvalues of G. A further problematic issue arises because we want to measure the smoothness
of f0 along the eigendirections of G  and so we need to control the convergence of the eigenvectors 

given that (cid:98)G converges to G in spectral norm. However  when two eigenvalues of G are close  then
the corresponding eigenvectors in the estimated matrix (cid:98)G are strongly affected by the stochastic
regret of the extra term(cid:13)(cid:13)∇V f0

perturbation (a phenomenon known as hybridization or spectral leaking in matrix perturbation theory 
see [1  Section 2]). Hence  in our analysis we need to separate out the eigenvectors that correspond to
well spaced eigenvalues from the others. This lack of discrimination causes the appearance in the

(cid:13)(cid:13)∞.

2 Related works

Nonparametric estimation problems have been a long-standing topic of study in statistics  where one
is concerned with the recovery of an optimal function from a rich class under appropriate probabilistic
assumptions. In online learning  the nonparametric approach was investigated in [15  16  17] by
Vovk  who considered regression problems in large spaces and proved bounds on the regret. Minimax
rates for the regret were later derived in [13] using a non-constructive approach. The ﬁrst explicit
online nonparametric algorithms for regression with minimax rates were obtained in [4].
The nonparametric online algorithm of [6] is known to have a suboptimal regret bound for Lipschitz
classes of functions. However  it is a simple and efﬁcient algorithm  well suited to the design of
extensions that exhibit different forms of adaptivity to the data sequence. For example  the paper
[9] derived a variant that automatically adapts to the intrinsic dimension of the data manifold. Our
work explores an alternative direction of adaptivity  mainly aimed at taming the effect of the curse
of dimensionality in nonparametric prediction through the learning of an appropriate Mahalanobis
distance on the instance space. There is a rich literature on metric learning (see  e.g.  the survey [2])
where the Mahalanobis metric (cid:107)·(cid:107)M is typically learned through minimization of the pairwise loss
function of the form (cid:96)(M   x  x(cid:48)). This loss is high whenever dissimilar pairs of x and x(cid:48) are close in
the Mahalanobis metric  and whenever similar ones are far apart in the same metric —see  e.g.  [19].
The works [5  7  18] analyzed generalization and consistency properties of online learning algorithms
employing pairwise losses.
In this work we are primarily interested in using a metric (cid:107)·(cid:107)M where M is close to the gradient
outer product matrix of the best model in the reference class of functions. As we are not aware
whether pairwise loss functions can indeed consistently recover such metrics  we directly estimate
the gradient outer product matrix. This approach to metric learning was mostly explored in statistics
—e.g.  by locally-linear Empirical Risk Minimization on RKHS [12  11]  and through Stochastic
Gradient Descent [3]. Our learning approach combines —in a phased manner— a Mahalanobis
metric extension of the algorithm by [6] with the estimator of [14]. Our work is also similar in spirit
to the “gradient weights” approach of [8]  which learns a distance based on a simpler diagonal matrix.
Preliminaries and notation. Let B(z  r) ⊂ Rd be the ball of center z and radius r > 0 and let
B(r) = B(0  r). We assume instances x belong to X ≡ B(1) and labels y belong to Y ≡ [0  1].
We consider the following online learning protocol with oblivious adversary. Given an unknown
sequence (x1  y1)  (x2  y2) ··· ∈ X × Y of instances and labels  for every round t = 1  2  . . .

1. the environment reveals instance xt ∈ X ;

2. the learner selects an action(cid:98)yt ∈ Y and incurs the square loss (cid:96)t

(cid:0)(cid:98)yt

(cid:1) =(cid:0)(cid:98)yt − yt

(cid:1)2;

Given a positive deﬁnite d × d matrix M  the norm (cid:107)x − z(cid:107)M induced by M (a.k.a. Mahalanobis

3. the learner observes yt.

distance) is deﬁned by(cid:112)(x − z)(cid:62)M (x − z).

Deﬁnition 1 (Covering and Packing Numbers). An ε-cover of a set S w.r.t. some metric ρ is a set
{x(cid:48)
i) ≤ ε. The
covering number N (S  ε  ρ) is the smallest cardinality of a ε-cover.

n} ⊆ S such that for each x ∈ S there exists i ∈ {1  . . .   n} such that ρ(x  x(cid:48)

1  . . .   x(cid:48)

3

An ε-packing of a set S w.r.t. some metric ρ is a set {x(cid:48)
i  j ∈ {1  . . .   m}  we have ρ(x(cid:48)
of a ε-packing.
It is well known that M(S  2ε  ρ) ≤ N (S  ε  ρ) ≤ M(S  ε  ρ). For all differentiable f : X → Y and
for any orthonormal basis V ≡ {u1  . . .   uk} with k ≤ d we deﬁne

m} ⊆ S such that for any distinct
j) > ε. The packing number M(S  ε  ρ) is the largest cardinality

1  . . .   x(cid:48)

i  x(cid:48)

(cid:107)∇V f(cid:107)∞ = max
(cid:107)v(cid:107) = 1

v ∈ span(V )

sup
x∈X

∇f (x)(cid:62)v .

If V = {u} we simply write (cid:107)∇uf(cid:107)∞.
In the following  M is a positive deﬁnite d×d matrix with eigenvalues λ1 ≥ ··· ≥ λd > 0 and eigen-
vectors u1  . . .   ud. For each k = 1  . . .   d the truncated determinant is detk(M ) = λ1 × ··· × λk.
The kappa function for the matrix M is deﬁned by
Figure 1: Quickly decreasing
spectrum of M implies slow
growth of its effective rank in t.

m : λm ≥ t− 2

1+r   m = 1  . . .   d

κ(r  t) = max

(cid:110)

(cid:111)

(5)

for t ≥ 1 and r = 1  . . .   d.

Note that κ(r + 1  t) ≤ κ(r  t). Now deﬁne the effective rank of
M at horizon t by

ρt = min{r : κ(r  t) ≤ r  r = 1  . . .   d} .

(6)
Since κ(d  t) ≤ d for all t ≥ 1  this is a well deﬁned quantity.
Note that ρ1 ≤ ρ2 ≤ ··· ≤ d. Also  ρt = d for all t ≥ 1
when M is the d × d identity matrix. Note that the effective
rank ρt measures the number of eigenvalues that are larger than a
threshold that shrinks with t. Hence matrices M with extremely
light-tailed spectra cause ρt to remain small even when t grows
large. This behaviour is shown in Figure 1.

Throughout the paper  we use f

respectively  f = O(g) and f = (cid:101)O(g).

O
= (g) and f

(cid:101)O

= (g) to denote 

3 Online nonparametric learning with ellipsoid packing

In this section we present a variant (Algorithm 1) of the online nonparametric regression algorithm
introduced in [6]. Since our analysis is invariant to rescalings of the matrix M  without loss of
generality we assume M has unit spectral radius (i.e.  λ1 = 1). Algorithm 1 sequentially constructs
a packing of X using M-ellipsoids centered on a subset of the past observed instances. At each step

t  the label of the current instance xt is predicted using the average(cid:98)yt of the labels of past instances

that fell inside the ellipsoid whose center xs is closest to xt in the Mahalanobis metric. At the end
of the step  if xt was outside of the closest ellipsoid  then a new ellipsoid is created with center xt.
The radii εt of all ellipsoids are shrunk at rate t−1/(1+ρt). Note that efﬁcient (i.e.  logarithmic in the
number of centers) implementations of approximate nearest-neighbor search for the active center xs
exist [10].
The core idea of the proof (deferred to the supplementary material) is to maintain a trade-off between
the regret contribution of the ellipsoids and an additional regret term due to the approximation of f
by the Voronoi partitioning. The regret contribution of each ellipsoid is logarithmic in the number of
predictions made. Since each instance is predicted by a single ellipsoid  if we ignore log factors the
overall regret contribution is equal to the number of ellipsoids  which is essentially controlled by the
packing number w.r.t. the metric deﬁned by M. The second regret term is due to the fact that —at
any point of time— the prediction of the algorithm is constant within the Voronoi cells of X induced
by the current centers (recall that we predict with nearest neighbor). Hence  we pay an extra term
equal to the radius of the ellipsoids times the Lipschitz constant which depends on the directional
Lipschitzness of f with respect to the eigenbasis of M.
Theorem 1 (Regret with Fixed Metric). Suppose Algorithm 1 is run with a positive deﬁnite matrix
M with eigenbasis u1  . . .   ud and eigenvalues 1 = λ1 ≥ ··· ≥ λd > 0. Then  for any differentiable

4

123456789100.00.20.40.60.81.0EigenvaluesofM0200040006000800010000t12345678910ρtEffectiveRankofMAlgorithm 1 Nonparametric online regression
Input: Positive deﬁnite d × d matrix M.
1: S ← ∅
2: for t = 1  2  . . . do
3:
4:
5:
6:
7:
8:

− 1
εt ← t
1+ρt
Observe xt
if S ≡ ∅ then

S ← {t}  Tt ← ∅
(cid:107)xt − xs(cid:107)M
(cid:88)

end if
s ← arg min
if Ts ≡ ∅ then
else

yt = 1
2

s∈S

9:
10:
11:
12:

(cid:98)yt ← 1

|Ts|

yt(cid:48)

t(cid:48)∈Ts

end if
Observe yt
if (cid:107)xt − xs(cid:107)M ≤ εt then
else

Ts ← Ts ∪ {t}
S ← S ∪ {s}  Ts ← ∅

13:
14:
15:
16:
17:
18:
19:
20: end for

end if

(cid:46) Centers

(cid:46) Update radius

(cid:46) Create initial ball

(cid:46) Find active center

(cid:46) Predict using active center

(cid:46) Update list for active center

(cid:46) Create new center

f : X → Y we have that

RT (f )

(cid:112)detκ(M ) +

(cid:101)O

=

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

i=1

 T

ρT

1+ρT

(cid:107)∇uif(cid:107)2∞

λi

where κ = κ(ρT   T ) ≤ ρT ≤ d.
We ﬁrst prove two technical lemmas about packings of ellipsoids.
Lemma 1 (Volumetric packing bound). Consider a pair of norms (cid:107)·(cid:107)  (cid:107)·(cid:107)(cid:48) and let B  B(cid:48) ⊂ Rd be
the corresponding unit balls. Then

) ≤ vol(cid:0)B + ε
2 B(cid:48)(cid:1)
vol(cid:0) ε
2 B(cid:48)(cid:1)

M(B  ε (cid:107)·(cid:107)(cid:48)

Lemma 2 (Ellipsoid packing bound). If B is the unit Euclidean ball then

M(cid:0)B  ε (cid:107)·(cid:107)M

(cid:1) ≤

(cid:32)

(cid:33)s s(cid:89)

(cid:112)

√
8

2

ε

i=1

.

(cid:110)

(cid:112)

(cid:111)

.

λi ≥ ε  i = 1  . . .   d

λi

where

s = max

i :

The following lemma states that whenever f has bounded partial derivatives with respect to the
eigenbase of M  then f is Lipschitz with respect to (cid:107)·(cid:107)M .
Lemma 3 (Bounded derivatives imply Lipschitzness in M-metric). Let f : X → R be everywhere
differentiable. Then for any x  x(cid:48) ∈ X  

(cid:12)(cid:12)f (x) − f (x(cid:48))(cid:12)(cid:12) ≤ (cid:107)x − x(cid:48)(cid:107)M

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

(cid:107)∇uif(cid:107)2∞

λi

.

4 Learning while learning the metric

i=1

In this section  we assume instances xt are realizations of i.i.d. random variables X t drawn according
to some ﬁxed and unknown distribution µ which has a continuous density on its support X . We also

5

assume labels yt are generated according to the noise model yt = f0(xt) + ν(xt)  where f0 is some
unknown regression function and ν(x) is a subgaussian zero-mean random variable for all x ∈ X .
We then simply write RT to denote the regret RT (f0). Note that RT is now a random variable which
we bound with high probability.
We now show how the nonparametric online learning algorithm (Algorithm 1) of Section 3 can be
combined with an algorithm that learns an estimate

n(cid:88)

(cid:98)Gn =

1
n

(cid:98)∇f0(xt)(cid:98)∇f0(xt)(cid:62)

n

 

(cid:17)

of the expected outer product gradient matrix G = E(cid:2)∇f0(X)∇f0(X)(cid:62)(cid:3). The algorithm (described

in the supplementary material) is consistent under the following assumptions. Let X (τ ) be X blown
up by a factor of 1 + τ.
Assumption 1.

(7)

t=1

− 1

2(d+1)

n

d n− 1

d

1. There exists τ0 > 0 such that f0 is continuously differentiable on X (τ0).
2. There exists G > 0 such that max
x∈X (τ0)
3. The distribution µ is full-dimensional: there exists Cµ > 0 such that for all x ∈ X and

(cid:107)∇f0(x)(cid:107) ≤ G.

ε > 0  µ(cid:0)B(x  ε)(cid:1) ≥ Cµεd.

Lemma 4 ([14  Theorem 1]). If Assumption 1 holds  then there exists a nonnegative and nonin-
creasing sequence {γn}n≥1 such that for all n  the estimated gradient outerproduct (7) computed

In particular  the next lemma states that  under Assumption 1  (cid:98)Gn is a consistent estimate of G.
with parameters εn > 0  and 0 < τn < τ0 satisﬁes(cid:13)(cid:13)(cid:98)Gn − G(cid:13)(cid:13)2 ≤ γn with high probability with
(cid:16)(cid:0) ln n(cid:1) 2
(cid:17)
respect do the random draw of X 1  . . .   X n. Moreover  if τn = Θ(cid:0)ε1/4
and εn = O(cid:16)
sequence γ0 ≥ γ1 ≥ ··· > 0. Let (cid:99)M (0) = γ0I. During each phase i  the algorithm predicts the
data points by running Algorithm 1 with M = (cid:99)M (i − 1)(cid:14)(cid:13)(cid:13)(cid:99)M (i − 1)(cid:107)2 (where (cid:107) · (cid:107)2 denotes the
points. At the end of phase i  the current gradient outer product estimate (cid:98)G(i) = (cid:98)GT (i) is used
to form a new matrix (cid:99)M (i) = (cid:98)G(i) + γT (i)I. Algorithm 1 is then restarted in phase i + 1 with
M = (cid:99)M (i)(cid:14)(cid:13)(cid:13)(cid:99)M (i)(cid:107)2. Note that the metric learning algorithm can be also implemented efﬁciently

Our algorithm works in phases i = 1  2  . . . where phase i has length n(i) = 2i. Let T (i) = 2i+1 − 2
be the index of the last time step in phase i. The algorithm uses a nonincreasing regularization

spectral norm). Simultaneously  the gradient outer product estimate (7) is trained over the same data

then γn → 0 as n → ∞.

(cid:1)  εn = Ω

(cid:12)(cid:12)µj − µk

to the following slight variant of the function kappa 

through nearest-neighbor search as explained in [14].
Let µ1 ≥ µ2 ≥ ··· ≥ µd be the eigenvalues and u1  . . .   ud be the eigenvectors of G. We deﬁne the
j-th eigenvalue separation ∆j by

(cid:110)
m : µm + 2γt ≥ µ1t− 2

For any ∆ > 0 deﬁne also V∆ ≡(cid:8)uj : |µj − µk| ≥ ∆  k (cid:54)= j(cid:9) and V ⊥
∆ = {u1  . . .   ud} \ V∆.
account for the error in estimating the eigenvalues of G  we deﬁne the effective rank(cid:101)ρt with respect
Our results are expressed in terms of the effective rank (6) of G at horizon T . However  in order to
Let (cid:99)M (i) be the estimated gradient outer product constructed at the end of phase i  and let(cid:98)µ1(i) +
γ(i) ≥ ··· ≥(cid:98)µd(i) + γ(i) and(cid:98)u1(i)  . . .  (cid:98)ud(i) be the eigenvalues and eigenvectors of (cid:99)M (i)  where
we also write γ(i) to denote γT (i). We use(cid:98)κ to denote the kappa function with estimated eigenvalues
and(cid:98)ρ to denote the effective rank deﬁned through(cid:98)κ. We start with a technical lemma.
Lemma 5. Let µd  α > 0 and d ≥ 1. Then the derivative of F (t) =(cid:0)µd + 2(cid:0)T0 + t(cid:1)−α(cid:1)t
positive for all t ≥ 1 when T0 ≥(cid:16) d+1

(cid:101)κ(r  t) = max

t ≥ 1 and r = 1  . . .   d.

1+r   m = 1  . . .   d

(cid:17)1/α

∆j = min
k(cid:54)=j

(cid:12)(cid:12) .

2

1+d is

(cid:111)

2µd

.

6

Proof. We have that F (cid:48)(t) ≥ 0 if and only if t ≤ 2(T0+t)

α(d+1)

(cid:0)1 + (T0 + t)αµd

(cid:1). This is implied by

t ≤ 2µd(T0 + t)1+α

α(d + 1)

or  equivalently  T0 ≥ A1/(1+α)t1/(1+α) − t

where A = α(d + 1)/(2µd). The right-hand side A1/(1+α)t1/(1+α) − t is a concave function of t.
Hence the maximum is found at the value of t where the derivative is zero  this value satisﬁes

A1/(1+α)

t−α/(1+α) = 1 which solved for t gives

t = A1/α(1 + α)−(1+α)/α .

1 + α

(cid:17)1/α

Substituting this value of t in A1/(1+α)t1/(1+α) − t gives the condition T0 ≥ A1/αα(1 + α)−(1+α)/α

which is satisﬁed when T0 ≥(cid:16) d+1
γ0 = 1 and γt = t−α for some α > 0 such that γt ≥ γt for all t ≥ (cid:0)d + 1(cid:14)2µd
(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)
1 +

Theorem 2. Suppose Assumption 1 holds. If the algorithm is ran with a regularization sequence
γ1 ≥ γ2 ≥ ··· > 0 satisfying Lemma 4  then for any given ∆ > 0

(cid:13)(cid:13)∞ +(cid:13)(cid:13)∇V ⊥

(cid:1)1/α and for

(cid:0)(cid:13)(cid:13)∇uj f0

 T

(cid:13)(cid:13)∞

(cid:101)ρT
1+(cid:101)ρT

(cid:1)2

(cid:101)O

RT

f0

2µd

=

.

∆

j=1

µj/µ1

Note that the asymptotic notation is hiding terms that depend on 1/∆  hence we can not zero out the

with high probability with respect to the random draw of X 1  . . .   X T .

term(cid:13)(cid:13)∇V ⊥

∆

f0

(cid:13)(cid:13)∞ in the bound by taking ∆ arbitrarily small.
(cid:18) d + 1
(cid:19)1/α

T (i0) ≥

Proof. Pick the smallest i0 such that

(8)

2µd

(cid:0)d + 1(cid:14)2µd

(cid:1)1/α

RT (i + 1) ≤

the regret RT (i + 1) of Algorithm 1 in each phase i + 1 > i0 is deterministically upper bounded by

(we need this condition in the proof). The total regret in phases 1  2  . . .   i0 is bounded by

= O(1). Let the value(cid:98)ρT (i) at the end of phase i be denoted by(cid:98)ρ(i). By Theorem 1 
8 ln(cid:0)e2i+1(cid:1)(cid:0)8

 2(i+1) (cid:98)ρ(i+1)
(cid:13)(cid:13)∇(cid:98)uj (i)f0
(cid:13)(cid:13)2
λj(i)(cid:14)λ1(i)
(cid:0)(cid:99)M (i)(cid:14)(cid:13)(cid:13)(cid:99)M (i)(cid:107)2
where λj(i) =(cid:98)µj(i) + γ(i). Here we used the trivial upper bound detκ
all κ = 1  . . .   d. Now assume that(cid:98)µ1(i) + γ(i) ≤(cid:0)(cid:98)µm(i) + γ(i)(cid:1)t
(cid:12)(cid:12) ≤(cid:13)(cid:13)(cid:98)G(i) − G(cid:13)(cid:13)2 ≤ γ(i) with high probability.
and for some t in phase i + 1. Hence  using Lemma 4 and γt ≤ γt  we have that

1+r for some m  r ∈ {1  . . .   d}

(cid:12)(cid:12)(cid:98)µj(i) − µj

(cid:1) ≤ 1 for

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

2(cid:1)(cid:98)ρ(i+1)

1+(cid:98)ρ(i+1)

(10)

max

+ 4

√

(9)

j=1

∞

2

where the ﬁrst inequality is straightforward. Hence we may write

Recall γ(i) = T (i)−α. Using Lemma 5  we observe that the derivative of

j=1 ... d

2

1+r

µ1 ≤ µ1 − γ(i) + γ(i) ≤(cid:98)µ1(i) + γ(i) ≤(cid:0)(cid:98)µm(i) + γ(i)(cid:1)t
≤(cid:0)µm + γ(i) + γ(i)(cid:1)t
≤(cid:0)µm + 2γ(i)(cid:1)t
(cid:16)
µm + 2(cid:0)T (i) + t(cid:1)−α(cid:17)
(cid:19)1/α
(cid:18) r + 1
(cid:18) r + 1

F (t) =

2
1+r .

1+r

t

2

(cid:19)1/α ≥

T (i) ≥

2µd

2µm

is positive for all t ≥ 1 when

7

2

1+r

(using Lemma 4)

which is guaranteed by our choice (8). Hence (cid:0)µm + 2γ(i)(cid:1)t
Recalling the deﬁnitions of(cid:101)κ and(cid:98)κ  this in turn implies(cid:98)κ(r  t) ≤(cid:101)κ(r  T )  which also gives(cid:98)ρt ≤(cid:101)ρT

for all t ≤ T . Next  we bound the approximation error in each individual eigenvalue of G. By (10)
we obtain  for any phase i and for any j = 1  . . .   d 

1+r ≤(cid:0)µm + 2γT )(cid:1)T

(cid:98)µ1(i) + γ(i)
(cid:98)µm(i) + γ(i)

1+r and so

µm + 2γT

implies

≤ T

≤ t

1+r .

µ1

1+r

2

2

2

2

µj + 2γ(i) ≥ µj + γ(i) + γ(i) ≥(cid:98)µj(i) + γ(i) ≥ µj − γ(i) + γ(i) ≥ µj .
 2(i+1) (cid:101)ρT
8 ln(cid:0)e2i+1(cid:1)12(cid:101)ρT + 4

(cid:118)(cid:117)(cid:117)(cid:116)(cid:0)µ1 + 2γ(i)(cid:1) d(cid:88)

(cid:13)(cid:13)∇(cid:98)uj f0

(cid:13)(cid:13)2

∞

RT (i + 1) ≤

1+(cid:101)ρT .

(11)

Hence  bound (9) implies

j=1

µj

The error in approximating the eigenvectors of G is controlled via the following ﬁrst-order eigenvector
approximation result from matrix perturbation theory [20  equation (10.2)]  for any vector v of
constant norm 

v(cid:62)(cid:0)(cid:98)uj(i) − uj

(cid:88)
(cid:1) =
≤(cid:88)

k(cid:54)=j

u(cid:62)

k

(cid:0)(cid:99)M (i) − G(cid:1)uj
v(cid:62)uk + o(cid:0)γ(i)2(cid:1)

µj − µk

v(cid:62)uk + o

2γ(i)
µj − µk

(cid:16)(cid:13)(cid:13)(cid:99)M (i) − G(cid:13)(cid:13)2

(cid:17)

2

where we used u(cid:62)
that uj ∈ V∆ 

k

k(cid:54)=j

(cid:0)(cid:99)M (i) − G(cid:1)uj ≤(cid:13)(cid:13)(cid:99)M (i) − G(cid:13)(cid:13)2 ≤ γ(i) + γ(i) ≤ 2γ(i). Then for all j such
∇f0(x)(cid:62)(cid:0)(cid:98)uj(i) − uj

(cid:1) =

(cid:88)

(12)

Note that the coefﬁcients

2γ(i)
µj − µk
√

k(cid:54)=j

≤ 2γ(i)
∆

(cid:0)(cid:99)M (i) − G(cid:1)uj

µj − µk

u(cid:62)

k

αk =

are a subset of coordinate values of vector(cid:98)uj(i) − uj w.r.t. the orthonormal basis u1  . . .   ud. Then 

by Parseval’s identity 

∇f0(x)(cid:62)uk + o(cid:0)γ(i)2(cid:1)
d(cid:107)∇f0(x)(cid:107)2 + o(cid:0)γ(i)2(cid:1) .
+ o(cid:0)γ(i)2(cid:1)
2 ≥(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2 + o(cid:0)γ(i)2(cid:1) .

k (cid:54)= j

k(cid:54)=j

α2

k .

Therefore  it must be that

max
k(cid:54)=j
For any j such that uj ∈ V ⊥
∆   since µj − µk ≥ ∆ for all uk ∈ V∆  we may write

4 ≥ (cid:107)(cid:98)uj(i) − uj(cid:107)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) u(cid:62)
(cid:0)(cid:99)M (i) − G(cid:1)uj
(cid:1)
∇f0(x)(cid:62)(cid:0)(cid:98)uj(i) − uj

µj − µk

k

(cid:88)

uk∈V∆
√

≤ 2γ(i)
∆

≤ 2γ(i)
∆

∇f0(x)(cid:62)uk +

d(cid:107)P V∆ ∇f0(x)(cid:107)2 +

8

(cid:16)

2 + o(cid:0)γ(i)2(cid:1)(cid:17) (cid:88)
2 + o(cid:0)γ(i)2(cid:1)(cid:17)√
(cid:16)

uk∈V ⊥

∇f0(x)(cid:62)uk + o(cid:0)γ(i)2(cid:1)
∇f0(x)(cid:13)(cid:13)2 + o(cid:0)γ(i)2(cid:1)
d(cid:13)(cid:13)P V ⊥

∆

∆

≤ sup
x∈X

(cid:1)

f0

∆

x∈X

x∈X

2γ(i)
√

RT (i + 1) ≤

∆ . Therefore  we

d(cid:107)∇V∆ f0(cid:107)∞ +

where P V∆ and P V ⊥
have that

∆

are the orthogonal projections onto  respectively  V∆ and V ⊥

∆
d(cid:107)∇V∆ f0(cid:107)∞ +

Letting α∆(i) = 2γ(i)
∆
bound (11) as follows

∇f0(x)(cid:62)uj + sup
x∈X
√

∇f0(x)(cid:62)(cid:98)uj(i) = sup

(cid:13)(cid:13)∞ = sup
(cid:13)(cid:13)∇(cid:98)uj f0
∇f0(x)(cid:62)(cid:0)(cid:98)uj(i) − uj + uj
∇f0(x)(cid:62)(cid:0)(cid:98)uj(i) − uj
(cid:1)
2 + o(cid:0)γ(i)2(cid:1)(cid:17)√
(cid:16)
(cid:13)(cid:13)∞ + o(cid:0)γ(i)2(cid:1)
d(cid:13)(cid:13)∇V ⊥
(cid:13)(cid:13)∞ +
≤(cid:13)(cid:13)∇uj f0
2 + o(cid:0)γ(i)2(cid:1)(cid:17)√
(cid:16)
(cid:13)(cid:13)∞ + o(cid:0)γ(i)2(cid:1) we can upper
d(cid:13)(cid:13)∇V ⊥
(cid:118)(cid:117)(cid:117)(cid:116)(cid:0)µ1 + 2γ(i)(cid:1) d(cid:88)
 2(i+1) (cid:101)ρT
8 ln(cid:0)e2i+1(cid:1)12(cid:101)ρT+ 4
(cid:0)(cid:13)(cid:13)∇uj f0
(cid:13)(cid:13)∞+ α∆(i)(cid:1)2
(cid:13)(cid:13)∞). Hence  by summing
= ((cid:107)∇V∆ f0(cid:107)∞ /∆+(cid:13)(cid:13)∇V ⊥
observe that γ(i) = O(cid:0)2−αi(cid:1) and so α∆(i)
over phases i = 1  . . .  (cid:6) log2 T(cid:7) and applying the union bound 
(cid:118)(cid:117)(cid:117)(cid:116)(cid:0)µ1 + 2γ(i − 1)(cid:1) d(cid:88)
(cid:0)(cid:13)(cid:13)∇uj f0
 T
(cid:17)2
(cid:13)(cid:13)∞ +(cid:13)(cid:13)∇V ⊥
(cid:13)(cid:13)∞
(cid:14)µ1

(cid:100)log2 T(cid:101)(cid:88)
8 ln(cid:0)eT(cid:1)12d+ 4
1 +
(cid:16)(cid:13)(cid:13)∇uj f0
d(cid:88)

Recall that  due to (10)  the above holds at the end of each phase i + 1 with high probability. Now

(cid:13)(cid:13)∞+ α∆(i − 1)(cid:1)2

(cid:18)

(cid:101)ρT
1+(cid:101)ρT

2

(cid:19)i

RT =

RT (i)

i=1

j=1

µj

f0

∆

(cid:101)ρT
1+(cid:101)ρT

(13)

1+(cid:101)ρT .

j=1

O

f0

∆

µj

f0

∆

≤
(cid:101)O

=

j=1

µj

concluding the proof.

5 Conclusions and future work

We presented an efﬁcient algorithm for online nonparametric regression which adapts to the directions
along which the regression function f0 is smoother. It does so by learning the Mahalanobis metric
through the estimation of the gradient outer product matrix E[∇f0(X)∇f0(X)(cid:62)]. As a preliminary
result  we analyzed the regret of a generalized version of the algorithm from [6]  capturing situations
where one competes against functions with directional Lipschitzness with respect to an arbitrary
Mahalanobis metric. Our main result is then obtained through a phased algorithm that estimates the
gradient outer product matrix while running online nonparametric regression on the same sequence.
Both algorithms automatically adapt to the effective rank of the metric.
This work could be extended by investigating a variant of Algorithm 1 for classiﬁcation  in which ball
radii shrink at a nonuniform rate  depending on the mistakes accumulated within each ball rather than
on time. This could lead to the ability of competing against functions f that are only locally Lipschitz.
In addition  it is conceivable that under appropriate assumptions  a fraction of the balls could stop
shrinking at a certain point when no more mistakes are made. This might yield better asymptotic
bounds than those implied by Theorem 1  because ρT would never attain the ambient dimension d.

Acknowledgments

Authors would like to thank S´ebastien Gerchinovitz and Samory Kpotufe for useful discussions on
this work. IK would like to thank Google for travel support. This work also was in parts funded
by the European Research Council (ERC) under the European Union’s Horizon 2020 research and
innovation programme (grant agreement no 637076).

9

References
[1] R. Allez and J.-P. Bouchaud. Eigenvector dynamics: general theory and some applications.

Physical Review E  86(4):046202  2012.

[2] A. Bellet  A. Habrard  and M. Sebban. A Survey on Metric Learning for Feature Vectors and

Structured Data. arXiv preprint arXiv:1306.6709  2013.

[3] X. Dong and D.-X. Zhou. Learning Gradients by a Gradient Descent Algorithm. Journal of

Mathematical Analysis and Applications  341(2):1018–1027  2008.

[4] P. Gaillard and S. Gerchinovitz. A chaining Algorithm for Online Nonparametric Regression.

In Conference on Learning Theory (COLT)  2015.

[5] Z.-C. Guo  Y. Ying  and D.-X. Zhou. Online Regularized Learning with Pairwise Loss Functions.

Advances in Computational Mathematics  43(1):127–150  2017.

[6] E. Hazan and N. Megiddo. Online Learning with Prior Knowledge. In Learning Theory  pages

499–513. Springer  2007.

[7] R. Jin  S. Wang  and Y. Zhou. Regularized Distance Metric Learning: Theory and Algorithm.

In Conference on Neural Information Processing Systems (NIPS)  2009.

[8] S. Kpotufe  A. Boularias  T. Schultz  and K. Kim. Gradients Weights Improve Regression and

Classiﬁcation. Journal of Machine Learning Research  17(22):1–34  2016.

[9] S. Kpotufe and F. Orabona. Regression-Tree Tuning in a Streaming Setting. In Conference on

Neural Information Processing Systems (NIPS)  2013.

[10] R. Krauthgamer and J. R. Lee. Navigating nets: simple algorithms for proximity search. In
Proceedings of the 15th annual ACM-SIAM Symposium on Discrete algorithms  pages 798–807.
Society for Industrial and Applied Mathematics  2004.

[11] S. Mukherjee and Q. Wu. Estimation of Gradients and Coordinate Covariation in Classiﬁcation.

Journal of Machine Learning Research  7(Nov):2481–2514  2006.

[12] S. Mukherjee and D.-X. Zhou. Learning Coordinate Covariances via Gradients. Journal of

Machine Learning Research  7(Mar):519–549  2006.

[13] A. Rakhlin and K. Sridharan. Online Non-Parametric Regression. In Conference on Learning

Theory (COLT)  2014.

[14] S. Trivedi  J. Wang  S. Kpotufe  and G. Shakhnarovich. A consistent Estimator of the Expected

Gradient Outerproduct. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2014.

[15] V. Vovk. Metric entropy in competitive on-line prediction. arXiv preprint cs/0609045  2006.

[16] V. Vovk. On-line regression competitive with reproducing kernel Hilbert spaces. In International

Conference on Theory and Applications of Models of Computation. Springer  2006.

[17] V. Vovk. Competing with wild prediction rules. Machine Learning  69(2):193–212  2007.

[18] Y. Wang  R. Khardon  D. Pechyony  and R. Jones. Generalization Bounds for Online Learning
Algorithms with Pairwise Loss Functions. In Conference on Learning Theory (COLT)  2012.

[19] K. Q. Weinberger and L. K. Saul. Distance Metric Learning for Large Margin Nearest Neighbor

Classiﬁcation. Journal of Machine Learning Research  10:207–244  2009.

[20] J. H. Wilkinson. The Algebraic Eigenvalue Problem  volume 87. Clarendon Press Oxford  1965.

[21] Q. Wu  J. Guinney  M. Maggioni  and S. Mukherjee. Learning gradients: predictive mod-
els that infer geometry and statistical dependence. Journal of Machine Learning Research 
11(Aug):2175–2198  2010.

10

,Ilja Kuzborskij
Nicolò Cesa-Bianchi