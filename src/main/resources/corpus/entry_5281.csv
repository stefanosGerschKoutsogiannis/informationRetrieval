2018,Streaming Kernel PCA with $\tilde{O}(\sqrt{n})$ Random Features,We study the statistical and computational aspects of kernel principal component analysis using random Fourier features and show that under mild assumptions  $O(\sqrt{n} \log n)$ features suffices to achieve $O(1/\epsilon^2)$ sample complexity. Furthermore  we give a memory efficient streaming algorithm based on classical Oja's algorithm that achieves this rate,Streaming Kernel PCA with ˜O(

√

n) Random Features

Enayat Ullah †
enayat@jhu.edu

Poorya Mianjy †
mianjy@jhu.edu

Teodor V. Marinov †
tmarino2@jhu.edu

Raman Arora †

arora@cs.jhu.edu

Abstract

We study the statistical and computational aspects of kernel principal component
√
analysis using random Fourier features and show that under mild assumptions 
n log (n)) features sufﬁce to achieve O(1/2) sample complexity. Further-
O(
more  we give a memory efﬁcient streaming algorithm based on classical Oja’s
algorithm that achieves this rate.

Introduction

1
Kernel methods represent an important class of machine learning algorithms that simultaneously
enjoy strong theoretical guarantees as well as empirical performance. However  it is notoriously
hard to scale them to large datasets due to space and runtime complexity (typically O(n2) and
O(n3)  respectively  for most problems) [Smola and Schölkopf  1998]. There have been many efforts
to overcome these computational challenges  including Nyström method [Williams and Seeger 
2001]  incomplete Cholesky factorization [Fine and Scheinberg  2001]  random Fourier features
(RFF) [Rahimi and Recht  2007] and randomized sketching [Yang et al.  2015]. In this paper  we
focus on random Fourier features due to its broad applicability to a large class of kernel problems.
In a seminal paper by Rahimi and Recht [2007]  the authors appealed to Bochner’s theorem to argue
that any shift-invariant kernel can be approximated as k(x  y) ≈ (cid:104)z(x)  z(y)(cid:105)  where the random
Fourier feature mapping z : Rd → Rm is obtained by sampling from the inverse Fourier transform
of the kernel function. This allows one to invoke fast linear techniques to solve the linear problem
in Rm. However  subsequent work analyzing kernel methods based on RFF for learning problems
suggests that to achieve the same asymptotic rates (as obtained using the true kernel) on the excess
risk  one requires m = Ω(n) random features [Rahimi and Recht  2009]  which defeats the purpose
of using random features from a computational perspective and fails to explain its empirical success.
Last year at NIPS  while Rahimi and Recht won the test-of-time award for their work on RFF [Rahimi
and Recht  2007]  Rudi and Rosasco [2017] showed for the ﬁrst time that at least for the kernel ridge
√
regression problem  under some mild distributional assumptions and for appropriately chosen regular-
ization parameter  one can achieve minimax optimal statistical rates using only m = O(
n log (n))
random features. It is then natural to ask if the same holds for other kernel problems.
In this paper  we focus on Kernel Principal Component Analysis (KPCA) [Schölkopf et al.  1998] 
which is a popular technique for unsupervised nonlinear representation learning. We argue that
scalability is an even bigger issue in the unsupervised setting since big data is largely unlabeled.
Furthermore  when extending the results from the supervised learning to unsupervised learning we
have to deal with additional challenges stemming from the non-convexity of the KPCA problem.
We pose KPCA as a stochastic optimization problem and investigate the tradeoff between statistical
samples and random features needed to guarantee -suboptimality on the population objective (aka a
small generalization error).
KPCA entails computing the top-k principal components of the data mapped into a Reproducing
Kernel Hilbert Space (RKHS) induced by a positive deﬁnite kernel [Aronszajn  1950]. In Schölkopf
et al. [1998]  authors showed that given a sample of n i.i.d. draws from the underlying distribution 
the inﬁnite dimensional problem (over RKHS) can be reduced to a ﬁnite dimensional problem (in Rn)

† Department of Computer Science  Johns Hopkins University  Baltimore  MD 21204

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Algorithm

Reference

Sample complexity

ERM

RF-DSG

RF-ERM

RF-Oja

Shawe-Taylor et al. [2005]
Blanchard et al. [2007]†

Xie et al. [2015]

Lopez-Paz et al. [2014]

Corollary 4.4†
Corollary 4.4†

˜O(1/2)
˜O(1/)
˜O(1/2)
˜O(1/2)
˜O(1/2)
˜O(1/2)

Per-iteration cost Memory
O(1/4)
O(1/2)
O(k/2)
˜O(1/4)
˜O(1/3)
˜O(k/)

˜O(k/4)
˜O(k/2)
˜O(k/2)
˜O(k/4)
˜O(k/3)
˜O(k/)

Table 1: Comparing different approaches to KPCA in terms of sample complexity  per-iteration
computational cost and space complexity. † : Optimistic rates realized under (potentially different)
higher-order distributional assumptions (See Corollary 4.3  and Blanchard et al. [2007]).

using the kernel trick. In particular  the solution entails computing the top-k eigenvectors of the kernel
matrix computed on the given sample. Statistical consistency of this approach was established in
Shawe-Taylor et al. [2005] and further improved in Blanchard et al. [2007]. However  computational
aspects of KPCA are less well understood. Note that the eigendecomposition of the kernel matrix
alone requires O(kn2) computation  which can be prohibitive for large datasets. Several recent works
have attempted to accelerate KPCA using random features. In Lopez-Paz et al. [2014]  authors show
that the kernel matrix computed using random features converges to the true kernel matrix in operator

norm at a rate of O(n(cid:112)(log n)/m). In Ghashami et al. [2016]  authors extended this guarantee to a

streaming setting using the Frequent Direction algorithm [Liberty  2013] on random features. In a
related line of work  Xie et al. [2015] propose a stochastic optimization algorithm based on doubly
√
stochastic gradients with a 1/n convergence in the sense of angle between subspaces. However  all
these results require m = ˜Ω(n) random features to guarantee a O(1/
More recently  Sriperumbudur and Sterge [2017] studied statistical consistency of ERM with ran-
domized Fourier features. They showed that the top-k eigenspace of the empirical covariance matrix
√
in the random feature space converges to that of the population covariance operator in the RKHS
n) 1. This result
when lifted to the space of square integrable functions  at a rate of O(1/
suggests that statistical and computational efﬁciency cannot be achieved at the same time without
making further assumptions. In this paper  we assume a spectral decay on the distribution of the
data in the feature space to show that we can simultaneously guarantee spectral and computational
efﬁciency for KPCA using random features. Our main contributions are as follows.

n) generalization bound.

√
m + 1/

1. We study kernel PCA as stochastic optimization problem and show that under mild distribu-
√
tional assumptions  for a wide range of kernels  the empirical risk minimizer (ERM) in the
random feature space converges in objective as O(1/
n log (n)) 
with overall runtime of O(kn 3

n) whenever m = Ω(k

2 log (n)).

√

2. We propose a stochastic approximation algorithm based on classical Oja’s updates on
random features which enjoys the same statistical guarantees as the ERM above but with
better runtime and space requirements.

3. We overcome a key challenge associated with kernel PCA using random features which
is to ensure that the output of the algorithm corresponds to a projection operator in the
(potentially inﬁnite dimensional) RKHS. We establish that the output of the proposed
algorithms converges to a projection operator.

4. In order to better understand the computational beneﬁts of using random features  we also
consider the KPCA problem in a streaming setting  where at each iteration  the algorithm is
provided with a fresh sample drawn i.i.d. from the underlying distribution and is required to
output a solution based on the samples observed so far. In such a setting  comparison with
other algorithmic approaches suggests that Oja’s algorithm on random Fourier features (see
RF-Oja in Table 1) enjoys the best overall runtime as well as superior space complexity.

5. We contribute novel analytical tools that should be useful broadly when designing algo-
rithms for kernel methods based on random features. We provide crucial and novel insights
that exploit connections between covariance operators in RKHS and the space of square
integrable functions with respect to data distribution. This connection allows us to look

1While our paper was under review  Sriperumbudur and Sterge [2017]  which initially focused on statistical
consistency of kernel PCA with random features  was replaced by Sriperumbudur and Sterge [2018]  with a new
title and focus on computational and statistical tradeoffs of KPCA much like our paper.

2

at the kernel approximation using random features as an estimation problem in the space
of square integrable functions  where we appeal to recent results in local Rademacher
complexity [Massart  2000  Bartlett et al.  2002  Blanchard et al.  2007] to yield faster rates.

6. Finally  we provide empirical results on a real dataset to support our theoretical results.

The rest of the paper is organized as follows. In Section 2  we give the problem setup. In Section 3 
we provide mathematical preliminaries and introduce the key notation. The main algorithm and the
results are in Section 4 and the empirical results are discussed in Section 5.
2 Problem setup
Given a random vector x ∈ Rd with underlying distribution ρ  principal component analysis (PCA)
can be formulated as the following stochastic optimization problem [Arora et al.  2012  2013]:

maximize Ex∼ρ(cid:104)P  xx(cid:62)(cid:105) s.t. P ∈ P k

(1)
where P k is the set of d × d rank-k orthogonal projection matrices. Essentially  PCA seeks a k-
dimensional subspace of Rd that captures maximal variation with respect to the underlying distribution.
It is well understood that the solution to the problem above is given by the projection matrix

corresponding to the subspace spanned by the top-k eigenvectors of the covariance matrix E(cid:2)xx(cid:62)(cid:3).

 

In most real world applications  however  the data does not have a linear structure. In other words  the
underlying distribution may not be well-represented by any low-rank subspace of the ambient space.
In such settings  the representations learned using PCA may not be very informative. This motivates
the need for non-linear dimensionality reduction methods. For example  in kernel PCA [Schölkopf
et al.  1998]  a canonical approach for manifold learning  a nonlinear feature map lifts the data
into a higher (potentially inﬁnite) dimensional Reproducing Kernel Hilbert Space (RKHS)  where a
low-rank subspace corresponds to a (non-linear) low-dimensional manifold in ambient space. Hence 
solving the PCA problem in an RKHS can better capture the complicated nonlinear structure in data.
Formally  given a kernel function k(· ·) : Rd × Rd → R  KPCA can be formulated as the following
stochastic optimization problem:

maximize Ex∼ρ(cid:104)P  k(x ·) ⊗H k(x ·)(cid:105) s.t. P ∈ P k

HS(H)

 

(2)

with a small excess risk:

operator(cid:98)C := 1

where P k
HS(H) is the set of all orthogonal projection operators onto a k-dimensional subspace of the
RKHS. The solution to the above problem is given by Pk
C  the projection operator corresponding to
the top-k eigenfunctions of the covariance operator C := Ex∼ρ[k(x ·) ⊗H k(x ·)]. The primary goal
HS(H)

of any KPCA algorithm is then to guarantee generalization  i.e. providing a solution(cid:98)P ∈ P k

E((cid:98)P) := Ex∼ρ(cid:104)Pk
C  k(x ·) ⊗H k(x ·)(cid:105) − Ex∼ρ(cid:104)(cid:98)P  k(x ·) ⊗H k(x ·)(cid:105).
(3)
Given access to i.i.d. samples {xi}n
i=1 ∼ ρ  one approach to solving Problem (2) is Empirical Risk
(cid:80)n
Minimization (ERM)  which amounts to ﬁnding the top-k eigenfunctions of the empirical covariance
i=1 k(xi ·) ⊗ k(xi ·). Using kernel trick  Schölkopf et al. [1998] showed that
this problem is equivalent of ﬁnding the top-k eigenvectors of the kernel matrix associated with
the samples. Alternatively  when approximating the kernel map with random features  Problem (2)
reduces to the PCA problem (given in Equation (1)) in the random feature space. Here  we discuss
two natural approaches to solve this problem. First  the ERM in the random feature space (called
RF-ERM)  which is given by the top-k eigenvectors of the empirical covariance matrix of data in the
feature space. Second  the classical Oja’s algorithm (called RF-Oja) [Oja  1982].
Note that while the output of ERM is guaranteed to induce a projection operator in the RKHS of
k(· ·)  this may not be the case when using RFF (equivalently  when working in the RKHS associated
with the approximate kernel map). Therefore  a key technical challenge when designing KPCA
algorithm based on RFF is to ensure that the output is close to the set of projection operators in the

n

true RKHS induced by k(· ·)  i.e. d((cid:98)P P k

HS(H)) is small.
3 Mathematical Preliminaries and Notation
In this section  we review basic concepts we need from functional analysis [Reed and Simon  1972].
We begin with a simple observation that given an underlying distribution on data  and a ﬁxed kernel

3

map  it induces a distribution on the feature map. We work with this distribution implicitly by
considering measurable Hilbert spaces. We denote matrices and Hilbert-Schmidt operators with
capital roman letters D  vectors with lower-case roman letters v  and scalars with lower-case letters a.
We denote operators over the space of Hilbert-Schmidt operators with capital Fraktur letters A.
Hilbert space notation and operator norm. Let H and ˜H be two separable Hilbert spaces over
ﬁelds F and ˜F with measures µ and ˜µ  respectively. Let {ei}i≥1 and {˜ei}i≥1 denote some ﬁxed
orthonormal basis for H and ˜H respectively. The inner product between two elements h1  h2 ∈ H is
denoted as (cid:104)h1  h2(cid:105)H  or (cid:104)h1  h2(cid:105)µ. Similarly  we denote the norm of an element h ∈ H as (cid:107)h(cid:107)H  or
(cid:107)h(cid:107)µ. For h1  h2 ∈ H the outer product denoted as h1 ⊗H h2  or h1 ⊗µ h2  is a linear operator on
H that maps any h3 ∈ H to (h1 ⊗H h2)h3 = (cid:104)h2  h3(cid:105)H h1. For a linear operator D : H → ˜H  the
operator norm of D is deﬁned as (cid:107)D(cid:107)2 := sup{(cid:107)Dh(cid:107) ˜H   h ∈ H (cid:107)h(cid:107)H ≤ 1}.
Adjoint  Hilbert-Schmidt  and trace-class operators. The adjoint of a linear operator D : H →
˜H  is given as the linear operator D∗ : ˜H → H such that (cid:104)Dh  ˜h(cid:105) ˜H = (cid:104)h  D∗˜h(cid:105)H  for all h ∈ H  ˜h ∈
˜H. A linear operator D : H → H is self-adjoint if D∗ = D. The linear operator D : H → ˜H is
D : H → H is a Hilbert-Schmidt operator if(cid:80)
i≥1 (cid:107)Dei(cid:107)2H = (cid:80)
compact if the image of any bounded set of H is a relatively compact subset of ˜H. A linear operator
Hilbert-Schmidt norm of D  denoted as (cid:107)D(cid:107)HS(H) or (cid:107)D(cid:107)HS(µ)  is deﬁned as ((cid:80)
i j≥1 (cid:104)Dei  ej(cid:105)2H < ∞. The
(cid:10)(DD∗)1/2ei  ei
(cid:11)
is trace-class if (cid:107)D(cid:107)L1(H) :=(cid:80)
2 . The
space of all Hilbert-Schmidt operators on H is denoted as HS(H). A compact operator D : H → H
H < ∞  where (cid:107)D(cid:107)L1(H) denotes the nuclear
respect to measure ρ  i.e L2(X   ρ) = {f : X → R (cid:82)
space with the inner product denoted as (cid:104)f  g(cid:105)ρ :=(cid:82)
norm of D. For a vector space X   L2(X   ρ) denotes the space of square integrable functions with
X (f (x))2dρ(x) < ∞}. L2(X   ρ) is a Hilbert
X f (x)g(x)dρ(x)  where f  g ∈ L2(X   ρ). The

i≥1 (cid:107)Dei(cid:107)2H) 1

i≥1

for f ∈ L2(X   ρ).

ρ

D =(cid:80)k

k(x  y) = (cid:82)

D; given the spectral decomposition D =(cid:80)∞

norm induced on L2(X   ρ) is denoted as (cid:107)f(cid:107)ρ := (cid:104)f  f(cid:105)1/2
Projection operators  spectral decomposition. Given a vector space X   let P kX denote the set of
rank-k projection operators on X . For a Hilbert-Schmidt operator D over a separable Hilbert space
H  let λi(D) denote its ith largest eigenvalue. The projection operator associated with the ﬁrst k
i=1 µiψi ⊗ ψi  we
eigenfunctions of D is denoted as Pk
i=1 ψi ⊗ ψi. For a ﬁnite dimensional vector v  (cid:107)v(cid:107)p denotes the (cid:96)p-norm of v. For
have that Pk
operators D over ﬁnite dimensional spaces  (cid:107)D(cid:107)2 and (cid:107)D(cid:107)F denote the spectral and Frobenius norm
of D  respectively. For a metric space (Y  d) and a closed subset S ⊆ Y  we denote the distance from
q ∈ Y to S by d(q  S) = mins∈S d(q  s). In a Hilbert space  d is the underlying metric induced by
the respective norm. [n] denotes the set of natural numbers from 1 to n.
Mercer kernels  and random feature maps. Let X ⊆ Rd be a compact (data) domain and ρ
be a distribution on X . We are given n independent and identically distributed samples from ρ 
{xi}n
i=1 ∼ ρn. Let k : X × X → R be a Mercer kernel with the following integral representation 
Ω z(x  ω)z(y  ω)dπ(ω). Here  (Ω  π) is the probability space induced by the Mercer
kernel. Let zω(·) := z(·  ω). We know that zω(·) ∈ L2(X   ρ) almost surely with respect to π. We
draw i.i.d. samples  ωi ∼ π  for i = 1  . . .   m  to approximate the kernel function. Let z(·) denote
m (zω1 (x)  zω2 (x)  . . .   zωm(x)). Let F ⊆ Rm
the random feature map  i.e. z : Rd → Rm  z(x) = 1√
be the linear subspace spanned by the range of z  with the inner product inherited from Rm. The
approximate kernel map is denoted as km(· ·)  where km(x  y) = (cid:104)z(x)  z(y)(cid:105)F . Let H denote the
separable RKHS associated with the kernel function k(· ·).
Assumption 3.1. The kernel function k is a Mercer kernel (see Theorem A.5) and has the following
Ω z(x  ω)z(y  ω)dπ(ω) ∀x  y ∈ X where H is a separable RKHS
of real-valued functions on X with a bounded positive deﬁnite kernel k. We also assume that there
exists τ > 1 such that |z(x  ω)| ≤ τ for all x ∈ X   ω ∈ Ω.
Note that z(x ·) are continuous functions because k(· ·) is continuous. Note that when X is separable
and k(· ·) is continuous  H is separable.
Deﬁnition 3.2. C : H → H is the covariance operator of the random variables k(x ·) with measure ρ 
X k(x ·)f (x)dρ(x). C is compact and self-adjoint  which implies C has a spectral

integral representation  k(x  y) =(cid:82)

deﬁned as Cf :=(cid:82)

4

decomposition C =(cid:80)∞

i=1

¯λi

¯φi ⊗H ¯φi  where ¯λi’s and ¯φi’s are the eigenvalues and eigenfunctions

i=1  forms a unitary basis for H.

of C  respectively. The set of eigenfunctions  { ¯φi}∞
Since we are approximating the kernel k by sampling m i.i.d. copies of zω  this implies an approxi-
mation to the covariance operator C (in the space HS(ρ)) by a sample average of the random linear
operators zω ⊗ρ zω. The tools we use to establish concentration require a sufﬁcient spectral decay of
the variance of this random operator  which we deﬁne next.
Deﬁnition 3.3. Let C1 denote the random linear operator on L2(X   ρ) given by C1 = zω ⊗ρ zω. Let
C2 = C1 ⊗HS(ρ) C1 and deﬁne the covariance operator of C1 to be C(cid:48) = Eπ [C2] − Eπ [C1] ⊗HS(ρ)
Eπ [C1].
We note that C(cid:48) can also be interpreted as the fourth moment of the random variable zω in L2(X   ρ).
The spectrum of C(cid:48) plays a crucial role in our results through the following key-quantity:

 Bkh

m

(cid:115) k

(cid:88)

m

j>h

+

   where Bk :=

(cid:113)Eπ

(cid:2)(cid:104)zω  zω(cid:105)4

ρ

(cid:3)

¯λk − ¯λk+1

λi(C(cid:48))

κ(Bk  k  m) = inf
h≥0

(4)

Essentially  we will see that the constant κ(Bk  k  m) is the dominating factor when bounding the
excess risk  and  therefore  will determine the rate of convergence of our algorithms.
From a practical perspective  working in HS(ρ) is not computationally feasible. However  our
approximation to C has a representation in the ﬁnite dimensional space F  as deﬁned here.
Deﬁnition 3.4. Cm : F → F is the covariance operator in HS(F)  deﬁned as Cm :=
X (cid:104)z(x)  v(cid:105) z(x)dρ(x). Cm is compact

Eρ [z(x) ⊗F z(x)]. Equivalently  for any v ∈ F  Cmv = (cid:82)
and self-adjoint which implies that Cm has a spectral decomposition Cm =(cid:80)m

i=1 λiφi ⊗F φi.

As mentioned at the beginning of the section  our convergence tools work most conveniently when
we can incorporate the randomness with respect to ρ in the geometry of the space we study  hence 
the need to study L2(X   ρ). Since we are essentially dealing with random operators on F H and
L2(X   ρ)  it is most appropriate to also work in the respective spaces of Hilbert-Schmidt operators.
Thus  we introduce the inclusion and approximation operators  which allow us to transition with ease
between the aforementioned spaces.
Deﬁnition 3.5. [Inclusion Operators I and I] The inclusion operator is deﬁned as

Also  for an operator D ∈ HS(H) with spectral decomposition D =(cid:80)∞

I : H → L2(X   ρ)  (If ) = f  where f ∈ H.

µi

⊗

Iψi

Iψi

In Lemma A.8 and Lemma A.9 in the appendix  we show that the adjoint of the Inclusion operator I

I : HS(H) → HS(ρ)  ID :=

(cid:112)(cid:104)Cψi  ψi(cid:105)H

(cid:112)(cid:104)Cψi  ψi(cid:105)H
is I∗ : L2(X   ρ) → H given by (I∗g)(·) =(cid:82) k(x ·)g(x)dρ(x)  and that C = I∗I  L = II∗.
For an operator D ∈ HS(F) with rank k with spectral decomposition D =(cid:80)∞

A : F → L2(X  ρ)  (Av)(·) = (cid:104)z(·)  v(cid:105)   where v ∈ F.

i=1 µiψi ⊗ ψi  let Ψ
be the matrix with eigenvectors ψi as columns and let Φ be the matrix with eigenvectors of Cm as
columns (see Deﬁnition 3.4). Deﬁne

Deﬁnition 3.6. [Approximation Operators A and A] The Approximation operator A is deﬁned as

.

i=1 µiψi ⊗ ψi 

∞(cid:88)

i=1

(cid:107)ΨR − Φ(cid:107)2F   ˜Ψ := ΨR∗.
k(cid:88)

A ˜ψi
˜ψi  ˜ψi(cid:105)F

⊗ρ

(cid:113)(cid:104)Cm

µi

i=1

5

(cid:113)(cid:104)Cm

A ˜ψi
˜ψi  ˜ψi(cid:105)F

.

R∗ = arg min

R(cid:62)R=RR(cid:62)=I

Let ˜ψi be the ith column of ˜Ψ  deﬁne

A : HS(F) → HS(ρ)  AD :=

A∗ : L2(X  ρ) → F  (A∗f )i =(cid:82)

In Lemma A.11 and Lemma A.12  we show that the adjoint of the Approximation Operator is

X f (x)zωi(x)dρ(x)  and Cm = A∗A  Lm = AA∗.

HS(ρ)

A

A

HS(F )

Deﬁnition 3.7. [Operator L] Let ˜P ∈ HS(F). Let A˜P = (cid:80)k
that L1/2pi = ˜pi. The operator L : HS(F) → HS(H) is deﬁned as L(cid:98)P =(cid:80)k

We note that the deﬁnition of the approximation operator A requires knowledge of the covariance
matrix Cm to ﬁnd the optimal rotation matrix R∗  but this is solely for the purpose of analysis and is
not used in the algorithm in any form.
The following deﬁnition enables us to bound the excess risk in HS(H) (Section B in the appendix).
i=1 ˜pi ⊗ρ ˜pi be ˜P lifted to HS(ρ).
Consider the equivalence relation pi ∼ pj if L1/2pi = L1/2pj. Let [pi] be the equivalence class such
i=1 I∗pi ⊗H I∗pi.
Here I∗ is the restriction of the operator I∗ to the quotient space L2(X   ρ)/ ∼.
The quotient space in the deﬁnition above is with respect to the kernel of L  i.e.  L2(X   ρ)/ ∼≡
L2(X   ρ)/ker(L). This quotient is benign since the optimal solution to our optimization problem
lives in the range of L and intuitively we can disregard any components in the kernel of L.
Finally  to conclude the section we give a vi-
sual schematic in Figure 1 to help the reader
connect different spaces. To summarize  the
key spaces of interest are the data domain X  
the RKHS H of the kernel map k(· ·)  and the
feature space F obtained via random feature
approximation. The space L2(X   ρ) consists of
functions over the data domain X that are square
integrable with respect to the data distribution
ρ. The space L2(X   ρ) allows us to embed ob-
jects from different spaces into a common space
so as to compare them. Speciﬁcally  we map
functions from H to L2(X   ρ) via the inclusion
operator I  and vectors from F to L2(X   ρ) via
the approximation operator A. I∗ and A∗ de-
note the adjoints of I and A  respectively. The
space of Hilbert-Schmidt operators on H F and
L2(X   ρ)  are denoted by HS(H)  HS(F) and
HS(ρ)  respectively. Analogous to I and A  I
maps operators from HS(H) to HS(ρ)  and A
maps operators from HS(F) to HS(ρ)  respec-
tively. Speciﬁcally  these are essentially con-
structed by mapping eigenvectors of operators
via I and A respectively. The above mappings thus allow us to embed operators in the common space 
i.e.  HS(ρ) and to bound estimation and approximation errors. However  the problem of Kernel PCA
is formulated in HS(H) and bounds in HS(ρ) are therefore not sufﬁcient. To this end  we establish
an equivalence between kernel PCA in HS(H) and HS(ρ). We use the map A and the established
equivalence to get L  which maps operators from HS(F) to HS(H). We encourage the reader to go
through Sections A and B in the appendix for a gentler and a more rigorous presentation.

Figure 1: Maps between the data domain
(X )  space of square integrable functions on X
(L2(X   ρ))  the RKHS of kernel k(· ·)  and RKHS
of the approximate feature map  as well as maps be-
tween Hilbert-Schmidt operators on these spaces.

k(x ·)

L2(X   ρ)

A∗

F

z

X

I

I

H

I∗

L

HS(H)

4 Main Results
Recall that our primary goal is to study the generalization behaviour of algorithms solving KPCA
using random features. Rather than stick to a particular algorithm  we deﬁne a class of algorithms
that are suitable to the problem. We characterize this class as follows.
Deﬁnition 4.1 (Efﬁcient Subspace Learner (ESL)). Let A be an algorithm which takes as input

n points from F and outputs a rank-k projection matrix over F. Let(cid:98)PA denote the output of the
algorithm A and(cid:98)PA = ˜Φ ˜Φ(cid:62) be an eigendecompostion of(cid:98)PA. Let Φ⊥

k be an orthogonal matrix
corresponding to the orthogonal complement of the top k eigenvectors of Cm. We say that algorithm
A is an Efﬁcient Subspace Learner if the following holds with probability at least 1 − δ 

(cid:13)(cid:13)(cid:13)(Φ⊥

k )(cid:62) ˜Φ

(cid:13)(cid:13)(cid:13)2

F

≤ qρ πA (1/δ  log (m)   log (n))

n

 

6

Algorithm 1 KPCA with Random Features (Meta Algorithm)
Input: Training data X = {xi}n

i=1

1: Obtain Training data X = {xt}n
2: Sample ωi ∼ π  i.i.d  i = 1 to m
3: Z ← RandomFeatures(X {ωi}m

i=1)

i=1 in a batch or stream

Output: (cid:98)PA
4: (cid:98)PA ← A(Z)

//A is an Efficient Subspace Learner  Definition 4.1

where qρ πA is a function given the triple (A  ρ  π) which has polynomial dependence on 1/δ  log (m)
and log (n). For notational convenience  we drop superscripts from qρ πA and write it as qA henceforth.
Intuitively  an ESL is an algorithm which returns a projection onto a k-dimensional subspace such
that the angle between the subspace and the space spanned by the top k eigenvectors of Cm decays at
a sub-linear rate with the number of samples. Our guarantees are in terms of any algorithm which
belongs to this class. Algorithm 1 gives a high-level view of the algorithmic routine. To discuss the
associated computational aspects  we instantiate this with two speciﬁc algorithms  ERM and Oja’s
algorithm  and show how the result looks in terms of their algorithmic parameters. Similar results
can be obtained for other ESL algorithms such as (cid:96)2-RMSG [Mianjy and Arora  2018]. We now give
the main theorem of the paper which characterizes the excess risk of an ESL.

Theorem 4.2 (Main Theorem). Let A be an efﬁcient subspace learner. Let(cid:98)PA be the output of A
(a). E(L(cid:98)PA) ≤ 24κ(Bk  k  m) + log(δ/2)+7Bk
(cid:17) ≤(cid:113) qA(2/δ log(m) log(n))

run with m random features on n ≥ 2λ2
2−1)
Cm. Then  with probability at least 1 − δ it holds that
+

(cid:113) qA(2/δ log(m) log(n))

points  where λi is the ith eigenvalue of

1qA(2/δ log(m) log(n))2

L(cid:98)PA P k

(b). d

(cid:16)

λ2
k(

√

m

n

.

 

HS(H)

n

A few remarks are in order. First  as we forewarned the reader in Section 3  the error bound is
dominated by the additive term κ(Bk  k  m). This  in a sense  determines the hardness of the problem.
As we will see  under appropriate assumptions on data distribution in the feature space  this term can

be bounded by something that is in O(1/m). Second  the output of our algorithm  L(cid:98)P  need not be a
projection operator in the RKHS. This is precisely why we need to bound the difference between L(cid:98)P
of Theorem 4.2  it is easy to see that if we project L(cid:98)PA to the set of rank k projection operators in

and the set of all projection operators in HS(H)  which we see is of the order O(1/
n). Third  note
that the dependence on the number of random features is at worst poly-logarithmic. From part (b)
HS(H)  we get the same rate of convergence. This is presented as Corollary C.12 in the appendix.
Next  we characterize “easy” instances of KPCA problems under which we are guaranteed a fast rate.
Speciﬁcally  we show that if the decay of the spectrum of the fourth order moment  C(cid:48)  of zω  is
exponential  then the dominating factor  κ(Bk  k  m) is in O(1/m). Then  optimizing the number of
random features w.r.t. the sample complexity term gives us the following result.
Corollary 4.3 (Main - Good decay). Along with the assumptions and notation of Theorem 4.2  if the
spectrum of the operator C(cid:48) has an exponential decay  i.e.  λj(C(cid:48)) = αj for some α < 1  then with
√
m = O(

n log (n)) random features  we have

√

c(cid:48)(k + log (δ/2) + 7Bk)

+

√
where c and c(cid:48) are universal constants.
Finally  we instantiate the above corollary with two algorithms  namely ERM and Oja’s algorithm.
Corollary 4.4 (ERM and Oja). With the same assumptions and notation as in Corollary 4.3 

qA(2/δ  log (m)   log (n))

n log (n)

+

n

n

 

E(L(cid:98)PA) ≤ cBk√

(cid:114)

gap2 log(cid:0) δ
(cid:17)
(cid:16) Λ
  where Λ =(cid:80)k

(cid:1)2

2m

.

gap2

i=1 λi.

(a). RF-ERM is an ESL with qERM (1/δ  log (m)   log (n)) = kλ1τ 2
(b). RF-Oja is an ESL with qoja(1/δ  log (m)   log (n)) = ˜Θ

where gap := λk(Cm) − λk+1(Cm).

7

Error Decomposition: There are two sources of error when solving KPCA using random features
– the estimation error (e) resulting from the fact that we have access to the distribution only through
an i.i.d. sample  and approximation error (a) resulting from the approximate feature map. Therefore 
to get a better handle on the excess error  we decompose it as follows.

  C(cid:105)HS(H) − (cid:104)L(cid:98)PA  C(cid:105)HS(H)
(cid:125)

(cid:123)(cid:122)

e: Estimation Error

.

E(L(cid:98)PA) = (cid:104)Pk
(cid:124)

(cid:123)(cid:122)
C  C(cid:105)HS(H) − (cid:104)LPk

a: Approximation Error

  C(cid:105)HS(H)

+(cid:104)LPk

Cm

Cm

(cid:125)

(cid:124)

The main idea behind controlling the approximation error is to interpret it as the error incurred in
eigenspace estimation in L2(X   ρ)  and then use local Rademacher complexity to get faster rates. In
the context of Kernel PCA  this technique was ﬁrst used by Blanchard et al. [2007] which allowed
them to get sharper O(1/n) excess risk. The estimation error is controlled by the deﬁnition of our
lifting map A together with the convergence rate implicit in the deﬁnition of an ESL. Below  we
guide the reader through the main steps taken to bound each of the error terms.
Bounding the Approximation error: Using simple algebraic manipulations  we can show that the
approximation error is exactly the error incurred by the ERM in estimating the top k eigenfunctions of
the kernel integral operator L using m samples drawn from π. This problem of eigenspace estimation
is well studied in the literature and has optimal statistical rates of O (1/
m) [Zwald and Blanchard 
2006]. This appears to be a key bottleneck and reinforces the view that the use of random features
√
cannot provide computational beneﬁts – it suggests m = Ω(n) random features are required to get a
n) rate. However  these rates are conservative when viewed in the sense of excess risk. This
O (1/
has been extensively studied in empirical process theory and one of the primary techniques to get
sharper rates is the use of local Rademacher complexity [Bartlett et al.  2002]. The key idea is to
show that around the best hypothesis in the class  variance of the empirical process is bounded by a
constant times the mean of the difference from the best hypothesis (see Theorem F.3). This technique
was used in the context of Kernel PCA by Blanchard et al. [2007] to get fast O(1/m) rates. We now
state Lemma 4.5 which bounds the approximation error  the proof of which is deferred to appendix.
Lemma 4.5 (Approximation Error). With probability at least 1 − δ  we have

√

a ≤ 24κ(Bk  k  m) +

11τ 2 log (δ) + 7Bk

m

.

Cm

Cm

Bounding the Estimation error: Since the objective with respect to the inner product in HS(ρ)
equals the objective with respect to the inner product in HS(H) (See Lemma B.4)  we focus on
bounding the estimation error in L2(X   ρ). Using a Cauchy-Schwartz type of inequality in HS(ρ) 
we see that it is enough to bound the difference (cid:107)APk
steps – bound the error (cid:107)Pk
A : HS(F) → HS(ρ). We already have a lifting from F to L2(X   ρ) in the form of A. The natural
attempt to lift an operator on F would be by lifting and appropriately rescaling its eigenfunctions.

− A(cid:98)PA(cid:107)HS(ρ). We can do this in two
−(cid:98)PA(cid:107)F (we already have this from the ESL guarantee) and construct
Since the eigendecomposition of (cid:98)PA is not unique  we need to choose an appropriate one to be
decomposition for which the distance(cid:80)k
(cid:118)(cid:117)(cid:117)(cid:116) k(cid:88)

lifted. Since the goal of A is to preserve distances between operators  we choose the unique eigen-
2 is minimized. Notice that the lifting operator
A depends on the eigendecomposition of Cm  which can not be obtained in practice. This is not a
problem  because A is only used for the purposes of showing the main result and is not part of the
proposed algorithms. We now state Lemma 4.6 which bounds the estimation error.
Lemma 4.6 (Estimation Error). With the same assumptions as Theorem 4.2  the following holds with
probability at least 1 − δ 
√
(

(cid:19)2 qA(1/δ  log (m)   log (n))2

(cid:18) 2λi + 4λ1

i=1 (cid:107)Ui − φi(cid:107)2

λ2
1
2 − 1)

e ≤

n

.

i=1

λ2
i

5 Experiments

The goal of this section is to provide empirical evidence supporting our theoretical ﬁndings in
Section 4. As we motivated in Section 2  the success of an algorithm is measured in terms of it’s
generalization ability  i.e. the variance captured by the output of the algorithm on the unseen data2.

2Details on how we evaluate objective for RF-ERM/Oja are deferred to Section E due to space limitations.

8

k = 5

k = 10

k = 15

Figure 2: Comparisons of ERM  Nyström  Oja+RFF  and Oja+ERM for KPCA on the MNIST dataset  in terms
of the objective value as a function of iterations (top) and as a function of CPU runtime (bottom).

We perform experiments on the MNIST dataset that consists of 70K samples  partitioned into a
training  tuning  and a test set of sizes 20K  10K  and 40K  respectively. We use a ﬁxed kernel in all
our experiments  since we are not concerned about model selection here. In particular  we choose the

RBF kernel k(x  x(cid:48)) = exp(cid:0)−(cid:107)x − x(cid:48)(cid:107)2/2σ2(cid:1) with bandwidth parameter σ2 = 50. The bandwidth is

chosen such that ERM converges in objective within observing few thousands training samples. The
objective of the ERM3 is used as the baseline. Furthermore  to evaluate the computational speedup
gained by using random features  we compare against Nyström method [Drineas and Mahoney 
2005] as a secondary baseline. In particular  upon receiving a new sample  we do a full Nyström
approximation and ERM on the set of samples observed so far. Finally  empirical risk minimization
(RF-ERM) and Oja’s algorithm (RF-Oja) are used with random features to verify the theoretical
results presented in Corollary 4.4.
Figure 2 shows the population objective as a function of iteration (top row) as well as the total runtime4
(bottom row). Each curve represents an average over 100 runs of the corresponding algorithm on
training samples drawn independently and uniformly at random from the whole dataset. Number of
random features and the size of Nyström approximation are set to 750 and 100  respectively. We note:
• As predicted by Corollary 4.4  for both RF-ERM and RF-Oja 
n log (n) ≈ 750 random
• The performance of ERM is similar to that of RF-Oja and RF-ERM in terms to overall
runtime. However  due to larger space complexity of O(n2)  ERM becomes infeasible for
large-scale problems; this makes a case for streaming/stochastic approximation algorithms.

features is sufﬁcient to achieve the same suboptimality as that of ERM.

√

Finally  we note that the iterates of RF-ERM and RF-Oja reduce the objective as they approach from
above to the maximizer of the population objective. Although it might seem counter-intuitive  we
note that the output of RF-ERM and RF-Oja are not necessarily projection operators. Hence  they
can achieve higher objective than the maximum. However  as guaranteed by Corollary 4.4  the output
of both algorithms will converge to a projection operator as more training samples are introduced.

3The kernel matrix is computed in an online fashion for computational efﬁciency
4Runtime is recorded in a controlled environment; each run executed on identical unloaded compute node.

9

101102103104Iteration00.10.20.30.40.50.6ObjectiveERMNystromRF-OjaRF-ERMtruth101102103104Iteration00.20.40.60.811.2Objective101102103104Iteration00.511.5Objective100101102Time10-1Objective100101102Time10-1100Objective100101102Time10-1100ObjectiveAcknowledgements

This research was supported in part by NSF BIGDATA grant IIS-1546482.

References
Zeyuan Allen-Zhu and Yuanzhi Li. First efﬁcient convergence for streaming k-pca: a global  gap-free 

and near-optimal rate. arXiv preprint arXiv:1607.07837  2016a.

Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet without agonizing

pain. In Advances in Neural Information Processing Systems  pages 974–982  2016b.

Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical

society  68(3):337–404  1950.

Raman Arora  Andrew Cotter  Karen Livescu  and Nathan Srebro. Stochastic optimization for PCA

and PLS. In Allerton Conference  pages 861–868. Citeseer  2012.

Raman Arora  Andy Cotter  and Nati Srebro. Stochastic optimization of PCA with capped MSG. In

Advances in Neural Information Processing Systems  NIPS  2013.

Peter L Bartlett  Olivier Bousquet  and Shahar Mendelson. Localized rademacher complexities. In

International Conference on Computational Learning Theory  pages 44–58. Springer  2002.

Gilles Blanchard  Olivier Bousquet  and Laurent Zwald. Statistical properties of kernel principal

component analysis. Machine Learning  66(2-3):259–294  2007.

Petros Drineas and Michael W Mahoney. On the nyström method for approximating a gram matrix
for improved kernel-based learning. journal of machine learning research  6(Dec):2153–2175 
2005.

Shai Fine and Katya Scheinberg. Efﬁcient svm training using low-rank kernel representations.

Journal of Machine Learning Research  2(Dec):243–264  2001.

Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems: A

uniﬁed geometric analysis. arXiv preprint arXiv:1704.00708  2017.

Mina Ghashami  Daniel J Perry  and Jeff Phillips. Streaming kernel principal component analysis. In

Artiﬁcial Intelligence and Statistics  pages 1365–1374  2016.

Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD
international conference on Knowledge discovery and data mining  pages 581–588. ACM  2013.

David Lopez-Paz  Suvrit Sra  Alex Smola  Zoubin Ghahramani  and Bernhard Schölkopf. Random-

ized nonlinear component analysis. arXiv preprint arXiv:1402.0119  2014.

Pascal Massart. Some applications of concentration inequalities to statistics. In Annales-Faculte des

Sciences Toulouse Mathematiques  volume 9  pages 245–303. Université Paul Sabatier  2000.

Poorya Mianjy and Raman Arora. Stochastic PCA with l 2 and l 1 regularization. In International

Conference on Machine Learning  pages 3528–3536  2018.

Erkki Oja. Simpliﬁed neuron model as a principal component analyzer. Journal of mathematical

biology  15(3):267–273  1982.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in

neural information processing systems  pages 1177–1184  2007.

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In Advances in neural information processing systems  pages
1313–1320  2009.

M Reed and B Simon. Methods of modern mathematical physics i: Functional analysis (academic 

new york). Google Scholar  page 151  1972.

10

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.

In Advances in Neural Information Processing Systems  pages 3218–3228  2017.

Walter Rudin. Fourier analysis on groups. Courier Dover Publications  2017.

Bernhard Schölkopf  Alexander Smola  and Klaus-Robert Müller. Nonlinear component analysis as a

kernel eigenvalue problem. Neural computation  10(5):1299–1319  1998.

Dino Sejdinovic and Arthur Gretton. What is an rkhs?  2012.

John Shawe-Taylor  Christopher KI Williams  Nello Cristianini  and Jaz Kandola. On the eigen-
spectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on
Information Theory  51(7):2510–2522  2005.

Alex J Smola and Bernhard Schölkopf. Learning with kernels  volume 4. Citeseer  1998.

Bharath Sriperumbudur and Nicholas Sterge. Statistical consistency of kernel pca with random

features. arXiv preprint arXiv:1706.06296v1  2017.

Bharath Sriperumbudur and Nicholas Sterge. Approximate kernel pca using random features:

Computational vs. statistical trade-off. arXiv preprint arXiv:1706.06296v2  2018.

Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations and Trends R(cid:13)

in Machine Learning  8(1-2):1–230  2015.

Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up kernel machines.

In Advances in neural information processing systems  pages 682–688  2001.

Bo Xie  Yingyu Liang  and Le Song. Scale up nonlinear component analysis with doubly stochastic

gradients. CoRR  abs/1504.03655  2015. URL http://arxiv.org/abs/1504.03655.

Yun Yang  Mert Pilanci  and Martin J Wainwright. Randomized sketches for kernels: Fast and

optimal non-parametric regression. arXiv preprint arXiv:1501.06195  2015.

Laurent Zwald and Gilles Blanchard. On the convergence of eigenspaces in kernel principal compo-

nent analysis. In Advances in neural information processing systems  pages 1649–1656  2006.

11

,Enayat Ullah
Poorya Mianjy
Teodor Vanislavov Marinov
Raman Arora