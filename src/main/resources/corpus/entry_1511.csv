2012,Dip-means: an incremental clustering method for estimating the number of clusters,Learning the number of clusters is a key problem in data clustering. We present dip-means  a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions  dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members. Two important advantages are: i) the unimodality test is applied on univariate distance vectors  ii) it can be directly applied with kernel-based methods  since only the pairwise distances are involved in the computations. Experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.,Dip-means:anincrementalclusteringmethodforestimatingthenumberofclustersArgyrisKalogeratosDepartmentofComputerScienceUniversityofIoanninaIoannina Greece45110akaloger@cs.uoi.grAristidisLikasDepartmentofComputerScienceUniversityofIoanninaIoannina Greece45110arly@cs.uoi.grAbstractLearningthenumberofclustersisakeyproblemindataclustering.Wepresentdip-means anovelrobustincrementalmethodtolearnthenumberofdataclustersthatcanbeusedasawrapperaroundanyiterativeclusteringalgorithmofk-meansfamily.Incontrasttomanypopularmethodswhichmakeassumptionsabouttheunderlyingclusterdistributions dip-meansonlyassumesafundamentalclusterproperty:eachclustertoadmitaunimodaldistribution.Theproposedalgorithmconsiderseachclustermemberasanindividual‘viewer’andappliesaunivariatestatistichypothesistestforunimodality(dip-test)onthedistributionofdistancesbetweentheviewerandtheclustermembers.Importantadvantagesare:i)theunimodalitytestisappliedonunivariatedistancevectors ii)itcanbedirectlyappliedwithkernel-basedmethods sinceonlythepairwisedistancesareinvolvedinthecomputations.Experimentalresultsonartiﬁcialandrealdatasetsindicatetheeﬀectivenessofourmethodanditssuperiorityoveranalogousapproaches.1IntroductionDataclusteringisadataanalysismethodologywhichaimstoautomaticallyrevealtheunderlyingstructureofdata.Itproducesapartitionofagivendatasetintokgroupsofsimilarobjectsandasataskiswidelyapplicableinartiﬁcialintelligence datamining statisticsandotherinformationprocessingﬁelds.AlthoughitisanNP-hardproblem variousalgorithmscanﬁndreasonableclus-teringsinpolynomialtime.Mostclusteringmethodsconsiderthenumberofclusterskasarequiredinput andthentheyapplyanoptimizationproceduretoadjusttheparametersoftheassumedclustermodel.Asaconsequence inexploratoryanalysis wherethedatacharacteristicsarenotknowninadvance anappropriatekvaluemustbechosen.Thisisaratherdiﬃcultproblem butatthesametimeveryfundamentalinordertoapplydataclusteringinpractice.Severalalgorithmshavebeenproposedtodetermineaproperkvalue mostofwhichwraparoundaniterativemodel-basedclusteringframework suchasthek-meansorthemoregeneralExpectation-Maximization(EM).Inatop-down(incremental)strategytheystartwithoneclusterandproceedtosplittingaslongasacertaincriterionissatisﬁed.Ateachphase theyevaluatetheclusteringproducedwithaﬁxedkandtheydecidewhethertoincreasethenumberofclustersasfollows:Repeatuntilnochangesoccurinthemodelstructure1.Improvemodelparametersbyrunningaconventionalclusteringalgorithmforaﬁxedkvalue.2.Improvemodelstructure usuallythroughclustersplitting.Oneoftheﬁrstattemptsinextendingk-meansinthisdirectionwasx-means[1]whichusesaregu-larizationpenaltybasedonmodel’scomplexity.Tothisend BayesianInformationCriterion(BIC)[2]wasused andamongmanymodelstheonewithhighestBICisselected.Thiscriterionworks1wellonlyincaseswherethereareplentyofdataandwell-separatedsphericalclusters.Alternativeselectioncriteriahavealsobeenexaminedinliterature[3].G-means[4]isanotherextensiontok-meansthatusesastatisticaltestforthehypothesisthateachclusterhasbeengeneratedfromGaussiandistribution.Sincestatisticaltestsbecomeweakerinhighdimensions thealgorithmﬁrstprojectsthedatapointsofaclusteronanaxisofhighvarianceandthenappliesAnderson-Darlingstatisticwithaﬁxedsigniﬁcancelevelα.Clustersthatarenotac-ceptedaresplitrepeatedlyuntiltheentireassumedmixtureofGaussiansisdiscovered.Projectedg-means(pg-means)[5]againassumesthatthedatasethasbeengeneratedfromaGaussianmixture butitteststheoverallmodelatonceandnoteachclusterseparately.Pg-meansbasesontheEMalgorithm.Usingaseriesofrandomlinearprojections itconstructsaone-dimensionalprojectionofthedatasetandthelearnedmodelandthenteststhemodelﬁtnessintheprojectedspacewithKolmogorov-Smirnov(KS)test.TheadvantageofthismethodistheabilitytodiscoverGaussianclustersofvariousscalesanddiﬀerentcovariances thatmayoverlap.Bayesiank-means[6]intro-ducesMaximization-Expectation(ME)tolearnamixturemodelbymaximizingoverhiddenvari-ables(datapointassignmentstoclusters)andcomputingexpectationoverrandommodelparameters(centersandcovariances).IfthedatacomefromamixtureofGaussiancomponents thismethodcanbeusedtoﬁndthecorrectnumberofclustersandiscompetitivetotheaforementionedapproaches.Otheralternativeshavealsobeenproposed suchasgapstatistic[7] self-tuningspectralclustering[8] dataspectroscopicclustering[9] andstability-basedmodelvalidation[10]-[12] howevertheyarenotcloselyrelatedtotheproposedmethod.Ourworkisprimarilymotivatedbythenongeneralityoftheapproachesin[4]and[5] astheymakeGausssianityassumptionsabouttheunderlyingdatadistribution.Asaconsequence theytendtooverﬁtforclustersthatareuniformlydistributed orhaveanon-Gaussianunimodaldistribution.Additionallimitationsarethattheyaredesignedtohandlenumericalvectorsonlyandrequirethedataintheoriginaldataspace.Thecontributionofourworkistwo-fold.Firstly weproposeasta-tisticaltestforunimodality calleddip-dist tobeappliedintoadatasubsetinordertodetermineifitcontainsasingleormultipleclusterstructures.Thus wemakeamoregeneralassumptionaboutwhatisanacceptablecluster.Moreover thetestinvolvespairwisedistancesorsimilaritiesandnottheoriginaldatavectors.Secondly weproposethedip-meansincrementalclusteringmethodwhichisawrapperaroundk-means.Weexperimentallyshowthatdip-meansisabletocopewithdatasetscontainingclustersofarbitrarydensitydistributions.Moreover itcanbeeasilyextendedinkernelspacebyusingthekernelk-means[13]andmodifyingappropriatelytheclustersplittingprocedure.2Dip-distcriterionforclusterstructureevaluationInclusteranalysis thedetectionofmultipleclusterstructuresinadatasetrequiresassumptionsaboutwhattheclustersweseeklooklike.Theassumptionsaboutthepresenceofcertaindatachar-acteristicsalongwiththetestsemployedforveriﬁcation considerablyinﬂuencetheperformanceofvariousmethods.Itishighlydesirablefortheassumptionstobegeneralinordernottorestricttheapplicabilityofthemethodtocertaintypesofclustersonly(e.g.Gaussian).Moreover itisofgreatvalueforamethodtobeabletoverifytheassumedclusterhypothesiswithwelldesignedstatisticalhypothesisteststhataretheoreticallysound incontrasttovariousalternativeadhoccriteria.Weproposethenoveldip-distcriterionforevaluatingtheclusterstructureofadatasetthatisbasedontestingtheempiricaldensitydistributionofthedataforunimodality.Theunimodalityassumptionimpliesthattheempiricaldensityofanacceptableclustershouldhaveasinglemode;aregionwherethedensitybecomesmaximum whilenon-increasingdensityisobservedwhenmovingawayfromthemode.Therearenootherunderlyingassumptionsabouttheshapeofaclusterandthedistributionthatgeneratedtheempiricallyobservedunimodalproperty.Underthisassumption itispossibletoidentifyclustersgeneratedbyvariousunimodaldistributions suchasGaussian Student-t etc.TheUniformdistributioncanalsobeidentiﬁed sinceitisanextremesinglemodecasewherethemodecoversalltheregionwithnon-zerodensity.Aconvenientissueisthatunimodalitycanbeveriﬁedusingpowerfulstatisticalhypothesistests(especiallyforone-dimensionaldata) suchasSilverman’smethodwhichusesﬁxed-widthkerneldensityestimates[14]orthewidelyusedHartigan’sdipstatistic[15].Asthedimensionalityofthedataincreases thetestsrequireasuﬃcientnumberofdatapointsinordertobereliable.Thus althoughthedatamaybeofarbitrarydimensionality itisimportanttoapplyunimodalitytestson2one-dimensionaldatavalues.Furthermore itwouldbedesirable ifthetestcouldalsobeappliedincaseswherethedistance(orsimilarity)matrixisgivenandnottheoriginaldatapoints.Tomeettheaboverequirementsweproposethedip-distcriterionfordeterminingunimodalityinasetofdatapointsusingonlytheirpairwisedistances(orsimilarities).Morespeciﬁcally ifwecon-sideranarbitrarydatapointasaviewerandformavectorwhosecomponentsarethedistancesoftheviewerfromallthedatapoints thenthedistributionofthevaluesinthisdistancevectorcouldrevealinformationabouttheclusterstructure.Inpresenceofasinglecluster thedistributionofdistancesisexpectedtobeunimodal.Inthecaseoftwodistinctclusters thedistributionofdistancesshouldexhibittwodistinctmodes witheachmodecontainingthedistancestothedatapointsofeachcluster.Consequently aunimodalitytestonthedistributionofthevaluesofthedistancevectorwouldprovideindicationabouttheunimodalityoftheclusterstructure.However thereisadependenceoftheresultsontheselectedviewer.Intuitively viewersattheboundariesofthesetareexpectedtoformdistancevectorswhosedensitymodesaremoredistinctincaseofmorethanoneclusters.Totackletheviewerselectionproblem weconsiderallthedatapointsofthesetasindividualviewersandperformtheunimodalitytestonthedistancevectorofeachviewer.Ifthereexistviewersthatrejectunimodality(calledsplitviewers) weconcludethattheexaminedclusterincludesmultipleclusterstructures.FortestingunimodalityweuseHartigans’diptest[15].AfunctionF(t)isunimodalwithmodetheregionsm={(tL tU):tL≤tU}ifitisconvexinsL=(−∞ tL] constantin[tL tU] andconcaveinsU=[tU ∞).Thisimpliesthenon-increasingprobabilitydensitybehaviorwhenmovingawayfromthemode.ForboundedinputfunctionsF G letρ(F G)=maxt|F(t)−G(t)| andletUbetheclassofallunimodaldistributions.ThenthedipstatisticofadistributionfunctionFisgivenby:dip(F)=minG∈Uρ(F G).(1)Inotherwords thedipstatisticcomputestheminimumamongthemaximumdeviationsobservedbetweenthecdfFandthecdfsfromtheclassofunimodaldistributions.Anicepropertyofdipisthat ifFnisasampledistributionofnobservationsfromF thenlimn→∞dip(Fn)=dip(F).In[15]itisarguedthattheclassofuniformdistributionsUisthemostappropriateforthenullhypothesis sinceitsdipvaluesarestochasticallylargerthanotherunimodaldistributions suchasthosehavingexponentiallydecreasingtails.Givenavectorofobservationsf={fi:fi∈R}ni=1 thenthealgorithmforperformingthediptest[15]isappliedontherespectiveempiricalcdfFn(t)=1nPnI(fi≤t).Itexaminesthen(n-1)/2possiblemodalintervals[tL tU]betweenthesortednindividualobservations.ForallthesecombinationsitcomputesinO(n)timetherespectivegreatestconvexminorantandtheleastconcavemajorantcurvesin(mintFn tL)and(tU maxtFn) respectively.Fortunately foragivenFn thecomplexityofonedipcomputationisO(n)[15].Thecomputationofthep-valueforaunimodalitytestusesbootstrapsamplesandexpressestheprobabilityofdip(Fn)beinglessthanthedipvalueofacdfUrnofnobservationssampledfromtheU[0 1]Uniformdistribution:P=#[dip(Fn)≤dip(Urn)]/b r=1 ... b.(2)ThenullhypothesisH0thatFnisunimodal isacceptedatsigniﬁcancelevelαifp-value>α otherwiseH0isrejectedinfavorofthealternativehypothesisH1whichsuggestsmultimodality.LetadatasetX={xi:xi∈Rd}Ni=1then inthepresentcontext thediptestcanbeappliedonanysubsetc e.g.adatacluster andmorespeciﬁcallyontheecdfFn(xi)(t)=1nPxj∈c{Dist(xi xj)≤t}ofthedistancesbetweenareferenceviewerxiofcandthenmembersoftheset.Wecalltheviewersthatidentifymultimodalityandvoteforthesettosplitassplitviewers.Thedip-distcomputationforasetcwithndatapointmembersissummarizedasfollows:1.ComputeUrnandtherespectivedip(Urn) r=1 ... b fortheUniformsampledistributions.2.ComputeF(xi)nanddip(F(xi)n) i=1 ... n fordatapointviewersusingthesortedmatrixDist.3.Estimatethep-valuesP(xi) i=1 ... n basedonEq.2usingasigniﬁcancelevelαandcomputethepercentageofviewersidentifyingmultimodality.SincetheascendingorderingoftherowsofDist requiredforcomputingF(xi)n canbedoneoncedur-ingoﬄinepreprocessing andthatthesamebsamplesofUniformdistributioncanbeusedfortestingallviewers thedip-distcomputationforasetwithndatapointshasO(bnlogn+n2)complexity.300.20.40.60.811.200.20.40.60.8 no splitsplit 71%max dipmin dip(a)dataset100.20.40.60.8100.010.020.030.040.050.06best split viewer: p=0.00  dip=0.1097distancefrequency(b)strongestsplitviewer00.20.40.60.8100.010.020.030.040.050.06worst split viewer: p=0.00  dip=0.0335distancefrequency(c)weakestsplitviewer00.20.40.60.811.200.20.40.60.8 no splitsplit 24%max dipmin dip(d)dataset200.20.40.60.8100.010.020.030.040.050.06best split viewer: p=0.00  dip=0.0776distancefrequency(e)strongestsplitviewer00.20.40.60.8100.010.020.030.040.050.06worst split viewer: p=0.00  dip=0.0335distancefrequency(f)weakestsplitviewer00.20.40.60.811.200.20.40.60.8 no split(g)dataset3(h)densityplot00.20.40.60.811.200.20.40.60.8 no splitsplit 24%max dipmin dip(i)dataset4(j)densityplotFigure1:Applicationofdip-distcriterionon2dsyntheticdatawithtwostructuresof200datapointseach.Thesplitviewersaredenotedinredcolor.(a)OneUniformsphericalandoneellipticGaussianstructure.(b) (c)Thehistogramsofpairwisedistancesofthestrongestandweakestsplitviewer.(d)Thetwostructurescomecloser;thesplitviewersarereduced sodoesthedipvalueforthesplitviewer.(g)Thetwostructuresarenolongerdistinguishableasthedensitymapin(h)showsonemode.(i)TheUniformsphericalisreplacedwithastructuregeneratedfromaStudent-tdistribution.Figure1illustratesanexampleofapplyingthedip-distcriteriononsyntheticdata.WegeneratedaUniformsphericalandaGaussianellipticstructure andthenconstructedthreediﬀerenttwo-dimensionaldatasetsbydecreasingthedistancebetweenthem.Thediptestparametersaresetα=0andb=1000.Thehistogramsineachrowindicatetheresultofthediptest.Asthestructurescomecloser thenumberofviewersthatobservemultimodalitydecreases.Eventually thestructuresformaunimodaldistribution(Figure1(g)) whichmaybevisuallyveriﬁedfromthepresenteddensitymap.ThefourthdatasetofFigure1(j)wascreatedbyincludingastructuregeneratedbyaStudent-tdistributioncenteredatthesamelocationwherethesphereislocatedinFigure1(g).Therespectivedensitymapshowsclearlytwomodes evidencethatjustiﬁeswhythedip-distcriteriondeterminesmultimodalitywith24%oftheviewerssuggestingthesplit.Moregenerally ifthepercentageofsplitviewersisgreaterthanasmallthreshold e.g.1% wemaydecidethattheclusterismultimodal.3Thedip-meansalgorithmDip-meansisanincrementalclusteringalgorithmthatcombinesthreeindividualcomponents.Theﬁrstisalocalsearchclusteringtechniquethattakesasinputamodelofkclustersandoptimizesthemodelparameters.Forthispurposek-meansisusedwheretheclustermodelsaretheircentroids.Thesecond andmostimportant decideswhetheradatasubsetcontainsmultipleclusterstructuresusingthedip-distpresentedinSection2.Thethirdcomponentisadivisiveprocedure(bisecting)that givenadatasubset performsthesplittingintotwoclustersandprovidesthetwocenters.Dip-meansmethodologytakesasinputthedatasetXandtwoparametersforthedip-distcriterion:thesigniﬁcancelevelαandthepercentagethresholdvthdofclustermembersthatshouldbesplitviewerstodecideforadivision(Algorithm1).Forthesakeofgenerality weassumethatdip-means4Algorithm1Dip-means(X kinit α vthd)input:datasetX={xi}Ni=1 theinitialnumberofclusterskinit astatisticsigniﬁcancelevelαfortheunimo-dalitytest percentagevthdofsplitviewersrequiredforaclustertobeconsideredasasplitcandidate.output:thesetsofclustermembersC={cj}kj=1 themodelsM={mj}kj=1withthecentroidofeachcjset.let:score=unimodalityTest(c α vthd)returnsascorevaluefortheclusterc {C M}=kmeans(X k)thek-meansclustering {C M}=kmeans(X M)wheninitializedwithmodelM {mL mR}=splitCluster(c)thatsplitsaclustercandreturnstwocentersmL mR.1:k←kinit2:{C M}←kmeans(X k)3:dowhilechangesinclusternumberoccur4:forj=1 ... k%foreachclusterj5:scorej←unimodalityTest(cj α vthd)%computethescoreforunimodalitytest6:endfor7:ifmaxj(scorej)>0%thereexistsplitcandidates8:target←argmaxj(scorej)%indexofclustertobesplitted9:{mL mR}←splitCluster(ctarget)10:M←{M-mtarget mL mR}%replacetheoldcentroidwiththetwonewones11:{C M}←kmeans(X M)%reﬁnesolution12:endif13:enddo14:return{C M}maystartfromanyinitialpartitionwithkinit≥1clusters.Ineachiteration allkclustersareexaminedforunimodality thesetofsplitviewersvjisfound andtherespectiveclustercjischaracterizedassplitcandidateif|vj|/nj≥vthd.Inthiscase anon-zeroscorevalueisassignedtoeachclusterbeingasplitcandidate whilezeroscoreisassignedtoclustersthatdonothavesuﬃcientsplitviewers.Variousalternativescanbeemployedinordertocomputeascoreforasplitcandidatebasedonthepercentageofsplitviewers oreventhesizeofclusters.Inourimplementationscorejofasplitcandidateclustercjiscomputedastheaveragevalueofthedipstatisticofitssplitviewers:scorej=1|vj|Pxi∈vjdip(F(xi)) |vj|nj≥vthd0 otherwise.(3)Inordertoavoidtheoverestimationoftherealnumberofclusters onlythecandidatewithmax-imumscoreissplitineachiteration.Aclusterissplitintotwoclustersusinga2-meanslocalsearchapproachstartingfromapairofsuﬃcientlydiversecentroidsmL mRinsidetheclusterandconcerningonlythedatapointsofthatcluster.Weuseasimplewaytosetuptheinitialcentroids{mL mR}←{x m−(x−m)} wherexaclustermemberselectedatrandomandmtheclustercen-troid.InthiswaymL mRlayatequaldistancesfromm thoughinoppositedirections.The2-meansprocedurecanberepeatedstartingfromdiﬀerentmL mRinitializationsinordertodiscoveragoodsplit.Acomputationallymoreexpensivealternativecouldbethedeterministicprincipaldirectiondivisivepartitioning(PDDP)[16]thatsplitstheclusterbasedontheprincipalcomponent.Wereﬁnethesolutionattheendofeachiterationusingk-means whichﬁne-tunesthemodelofk+1clusters.Theprocedureterminateswhennosplitcandidatesareidentiﬁedamongthealreadyformedclusters.Theproposeddip-distcriterionusesonlythepairwisedistances orsimilarities betweendatapointsandnotthevectorrepresentationsthemselves.ThisenablesitsapplicationinkernelspaceΦ pro-videdakernelmatrixKwiththeN×Npairwisedatapointinnerproducts Kij=φ(xi)Tφ(xj).Al-gorithm1canbemodiﬁedappropriatelyforthispurpose.Morespeciﬁcally kerneldip-meansuseskernelk-means[13]aslocalsearchtechnique whichalsoimpliesthatcentroidscannotbecomputedinkernelspace thuseachclusterisnowdescribedexplicitlybythesetofitsmemberscj.Inthiscase sincethetransformeddatavectorsφ(x)arenotavailable theclustersplittingprocedurecouldbeseededbytwoarbitraryclustermembers.However weproposeamoreeﬃcientapproach.AsdiscussedinSection2 thedistributionofpairwisedistancesbetweenareferenceviewerandthemembersofaclusterrevealsinformationaboutthemultimodalityofdatadistributionintheoriginalspace.Thisimpliesthatasplitoftheclustermembersbasedontheirdistancetoareferenceviewerconstitutesareasonablesplitintheoriginalspace aswell.Tothisend wemayuse2-meanstosplittheelementsoftheone-dimensionalsimilarityvector.Weconsiderasreferencesplitviewertheclustermemberwiththemaximumdipvalue.Here 2-meansisseededusingtwovalueslocated500.20.40.60.8100.20.40.60.8  dip−means: ke = 1no split00.20.40.60.8100.20.40.60.8x−means: ke = 100.20.40.60.8100.20.40.60.8g−means: ke = 1000.20.40.60.8100.20.40.60.8pg−means: ke = 4(a)SinglestructuregeneratedbyaStudent-tdistribution00.20.40.60.8100.20.40.60.8  dip−means: ke = 1no split00.20.40.60.8100.20.40.60.8x−means: ke = 200.20.40.60.8100.20.40.60.8g−means: ke = 200.20.40.60.8100.20.40.60.8pg−means: ke = 2(b)SingleUniformrectanglestructure00.20.40.60.8100.20.40.60.8dip−means: ke = 800.20.40.60.8100.20.40.60.8x−means: ke = 2600.20.40.60.8100.20.40.60.8g−means: ke = 3300.20.40.60.8100.20.40.60.8pg−means: ke = 19(c)Eightclustersofvariousdensityandshape00.20.40.60.8100.20.40.60.8kernel dip−means: ke = 200.20.40.60.8100.20.40.60.8kernel k−means: k = 2(d)TwoUniformringstructures00.20.40.60.8100.20.40.60.8kernel dip−means: ke = 300.20.40.60.8100.20.40.60.8kernel k−means: k = 3(e)ThreeUniformringstructuresFigure2:Clusteringresultson2dsyntheticunimodalclusterstructureswith200datapointseach(thecentroidsaremarkedwith⊗).(a) (b)Singleclusterstructures.(c)Variousstructuretypes.Basedontheleftmostsubﬁgure itcontainsaUniformrectangle(green) aspherewithincreasingdensityatitsperiphery(lightgreen) twoGaussianstructures(black pink) aUniformellipse(blue) atriangledenseratacorner(yellow) aStudent-t(lightblue) andaUniformarbitraryshape(red).(d) (e)Non-linearlyseparableringclusters(kernel-basedclusteringwithanRBFkernel).atoppositepositionswithrespecttothedistribution’smean.Afterconvergence theresultingtwo-waypartitionofthedatapoints derivedbythepartitionofthecorrespondingsimilarityvaluestotheselectedreferencesplitviewer initializesalocalsearchwithkernel2-means.4ExperimentsInourevaluationwecomparetheproposeddip-meansmethodwithx-means[1] g-means[4]andpg-means[5]thatarecloselyrelatedtopresentwork.Inallcomparedmethodswestartwithasinglecluster(kinit=1)andi)ateachiterationoneclusterisselectedforabisectingsplit ii)10splittrialsareperformedwith2-meansinitializedwiththesimpletechniquedescribedinSection3 andthesplitwithlowerclusteringerror(thesumofsquareddiﬀerencesbetweenclustercentersandtheirassigneddatapoints)iskept iii)thereﬁnementisappliedaftereachiterationonallk+1clusters.Hence onlythestatisticaltestthatdecideswhethertostopsplittingdiﬀersineachcase.Exception6Table1:Resultsforsyntheticdatasetswithﬁxedk∗=20clusterswith200datapointsineachcluster.Case1 d=4Case1 d=16Case1 d=32MethodskeARIVIkeARIVIkeARIVIdip-means20.0±0.01.00±0.00.00±0.020.0±0.01.00±0.00.00±0.020.0±0.01.00±0.00.00±0.0x-means7.3±9.30.30±0.52.07±1.328.6±7.80.88±0.10.27±0.231.3±5.60.84±0.10.36±0.2g-means20.3±0.50.99±0.00.01±0.020.3±0.50.99±0.00.01±0.020.5±0.60.99±0.00.02±0.0pg-means19.2±2.50.90±0.10.16±0.219.0±0.90.95±0.10.07±0.13.2±5.10.09±0.22.62±0.9Case2 d=4Case2 d=16Case2 d=32MethodskeARIVIkeARIVIkeARIVIdip-means20.0±0.00.99±0.00.05±0.020.0±0.00.99±0.00.02±0.020.0±0.00.99±0.00.01±0.0x-means24.8±39.0.26±0.42.26±1.180.1±15.0.75±0.10.75±0.271.6±14.0.75±0.10.66±0.2g-means79.2±22.0.77±0.10.70±0.2105.9±30.0.83±0.10.66±0.2133.6±42.0.83±0.10.72±0.2pg-means14.2±4.70.67±0.20.65±0.510.4±3.40.30±0.21.26±0.54.0±1.50.06±0.12.40±0.2isthepg-meansmethodthatusesEMforlocalsearchanddoesnotrelyonclustersplittingtoaddanewcluster.Weusethemethodexactlyaspresentedin[5].Forthekernel-basedexperimentsweusethenecessarymodiﬁcationsdescribedattheendofSection3andcomparewithkernelk-means[13].Theparametersofthedip-distcriterionaresetasα=0forsigniﬁcancelevelofdiptestandb=1000forthenumberofbootstraps.Weconsiderassplitcandidatestheclustershavingatleastvthd=1%splitviewers.Thesevalueswereﬁxedinallexperiments.Forbothg-meansandpg-meanswesetthesigniﬁcancelevelα=0.001 whileweuse12randomprojectionsforthelatter.Inordertocomparethegroundtruthlabelingandthegroupingproducedbyclustering weutilizetheVariationofInformation(VI)[17]metricandtheAdjustedRandIndex(ARI)[18].BetterclusteringisindicatedbylowervaluesofVIandhigherforARI.Weﬁrstprovideclusteringresultsforsynthetic2ddatasetsinFigure2(kedenotestheestimatednumberofclusters).InFigures2(a) (b) weprovidetwoindicativeexamplesofsingleclusterstruc-tures.X-meansdecidescorrectlyforthestructuregeneratedfromStudent-tdistribution butoverﬁtsintheUniformrectanglecase whiletheothertwomethodsoverﬁtinbothcases.InthemulticlusterdatasetofFigure2(c) dip-meanssuccessfullydiscoversallclusters incontrasttotheothermethodsthatsigniﬁcantlyoverestimate.Totestthekerneldip-meansextension wecreatedtwo2dsyntheticdatasetcontainingtwoandthreeUniformringstructuresandweusedanRBFkerneltoconstructthekernelmatrixK.Itisclearthatx-means g-means andpg-meansarenotapplicableinthiscase.ThuswepresentinFigures2(d) 2(e)theresultsusingkerneldip-meansandalsothebestsolutionfrom50randomlyinitializedrunsofkernelk-meanswiththetruenumberofclusters.Aswemayob-serve dip-meansestimatesthetruenumberofclustersandﬁndstheoptimalgroupingofdatapointsinbothcases whereaskernelk-meansfailsinthethreeringcase.Furthermore wecreatedsyntheticdatasetswithtruenumberk∗=20clusters with200datapointseach ind=4 16 32dimensionswithlowseparation[19].Twocaseswereconsidered:1)Gaussianmixturesofvaryingeccentricity and2)datasetswithvariousclusterstructures i.e.Gaussian(40%) Student-t(20%) Uniformellipses(20%)orUniformrectangles(20%).Foreachcaseanddimensions wegenerated30datasetstotestthemethods.AstheresultsinTable1indicate dip-meansprovidesexcellentclusteringperformanceinallcasesandestimatesaccuratelythetruenumberofclusters.Moreover itperformsremarkablybetterthantheothermethods especiallyforthedatasetsofCase2.Tworeal-worlddatasetswerealsoused wheretheprovidedclasslabelswereconsideredasgroundtruth.HandwrittenPendigits(UCI)[20]contains16dimensionalvectors eachonerepresentingadigitfrom0-9writtenbyahumansubject.ThedataprovideatrainingPDtrandatestingsetPDtewith7494and3498instances respectively.Wealsoconsidertwosubsetsthatcontainthedigits{0 2 4}(PD3trandPD3te)and{3 6 8 9}(PD4trandPD4te).Wedonotapplyanypreprocessing.Coil-100istheseconddataset[21] whichcontains72imagestakenfromdiﬀerentanglesforeachoneofthe100includedobjects.WeusedtreesubsetsCoil3 Coil4 Coil5 withimagesfrom3 4and5objects respectively.SIFTdescriptors[22]areﬁrstextractedfromthegreyscaleimagesthatareﬁnallyrepresentedbytheBagofVisualWordsmodelusing1000visualwords.AsreportedinTable2 dip-meanscorrectlydiscoversthenumberofclustersforthesubsetsofPendigits whileprovidingareasonableunderestimationkeneartheoptimalforthefulldatasetsPD10trandPD10te.Apartfromtheexcessiveoverﬁttingofx-meansandg-means pg-meansseemstoconcludesinoverestimatedke.InthehighdimensionalandsparsespaceoftheconsideredCoilsubsets x-means7Table2:Clusteringresultsforreal-worlddata.Boldindicatesbestvalues.PD3te(k∗=3)PD4te(k∗=4)PD10te(k∗=10)MethodskeARIVIkeARIVIkeARIVIdip-means30.8790.33240.6260.54570.3431.587x-means1550.0313.7921940.0393.7235150.0413.825g-means210.2261.800360.2092.049730.2951.961pg-means40.8350.359100.5760.954130.4471.660PD3tr(k∗=3)PD4tr(k∗=4)PD10tr(k∗=10)MethodskeARIVIkeARIVIkeARIVIdip-means30.9630.11640.5220.84190.4351.452x-means2880.0184.3783810.0204.3729420.0244.387g-means520.1062.641580.1432.4641490.1602.605pg-means50.6550.74080.4391.320140.4941.504Coil3(k∗=3)Coil4(k∗=4)Coil5(k∗=5)MethodskeARIVIkeARIVIkeARIVIdip-means31.0000.00050.9120.17340.7720.308x-means80.4990.899110.4990.951150.6010.907g-means70.6690.650120.5020.977180.4341.204andg-meansprovidemorereasonablekeestimations butstilloverestimations.Anexplanationforthisbehavioristhattheydiscoversmallergroupsofsimilarimages i.e.imagestakenfromcloseanglestothesameobject butfailtounifythesubclustersathigherlevel.Notealsothatwedidnotmanagetotestpg-meansinCoil-100subsets sincecovariancematriceswerenotpositivedeﬁnite.Thesuperiorityofdip-meansisalsoindicatedbythereportedvaluesforARIandVImeasures.5ConclusionsWehavepresentedanovelapproachfortestingwhethermultipleclusterstructuresarepresentinasetofdataobjects(e.g.adatacluster).Theproposeddip-distcriterionchecksforunimodalityoftheempiricaldatadensitydistribution thusitismuchmoregeneralcomparedtoalternativesthattestforGaussianity.Dip-distusesastatisticalhypothesistest namelyHartigans’diptest inordertoverifyunimodality.Ifadataobjectofthesetisconsideredasaviewer thenthediptestcanbeappliedontheone-dimensionaldistance(orsimilarity)vectorwithcomponentsthedistancesbetweentheviewerandthemembersofthesameset.Weexploittheideathattheobservationofmultimodalityinthedistributionofdistancesindicatesmultimodalityoftheoriginaldatadistribution.Byconsideringallthedataobjectsofthesetasindividualviewersandbycombiningtherespectiveresultsofthetest thepresenceofmultipleclusterstructuresinthesetcanbedetermined.Wehavealsoproposedanewincrementalclusteringalgorithmcalleddip-means thatincorporatesdip-distcriterioninordertodecideforclustersplitting.Theprocedurestartswithonecluster ititer-ativelysplitstheclusterindicatedbydip-distasmoreprobabletocontainmultipleclusterstructures andterminateswhennonewclustersplitissuggested.Bytakingadvantageofthefactthatdip-distutilizesonlyinformationaboutthedistancesbetweendataobjects wehavemodiﬁedappropriatelythemainalgorithmtoproposekerneldip-meanswhichcanbeappliedinkernelspace.Theproposedmethodisfast easytoimplement andworksverywellunderaﬁxedparametersetting.Thereportedclusteringresultsindicatethatdip-meanscanprovidereasonableestimatesofthenumberofclusters andproducemeaningfulclusteringsinbothdatasettypesinavarietyofartiﬁcialandrealdatasets.Apartfromtestingthemethodinreal-worldapplications thereareseveralwaystoimprovetheimplementationdetailsofthemethod especiallythekernel-basedversion.Wealsoplantotestitseﬀectivenessinothersettings suchasonlineclusteringofstreamdata.AcknowledgmentsWethankProf.GregHamerlyforprovidinghiscodeforpg-means.Thedescribedworkissupportedpartiallyandco-ﬁnancedbytheEuropeanRegionalDevelopmentFund(ERDF)(2007-2013)oftheEuropeanUnionandNationalFunds(OperationalProgramme“CompetitivenessandEntrepreneur-ship”(OPCEII) ROPATTICA) undertheAction“SYNERGASIA(COOPERATION)2009”.8References[1]D.PellegandAndrewMoore.X-means:extendingk-meanswitheﬃcientestimationofthenumberofclusters.InternationalConferenceonMachineLearning(ICML) pp.727-734 2000.[2]R.E.KassandL.Wasserman.AreferenceBayesiantestfornestedhypothesesanditsrelationshiptotheSchwarzcriterion.JournaloftheAmericanStatisticalAssociation 90(431) pp.928-934 1995.[3]X.HuandL.Xu.Acomparativestudyofseveralclusternumberselectioncriteria.InJ.Liuetal.(eds.)IntelligentDataEngineeringandAutomatedLearning pp.195–202 Springer 2003.[4]G.HamerlyandC.Elkan.Learningthekink-means.AdvancesinNeuralInformationProcessingSystems(NIPS) pp.281-288 2003.[5]Y.FengandG.Hamerly.PG-means:learningthenumberofclustersindata.AdvancesinNeuralInforma-tionProcessingSystems(NIPS) pp.393–400 2006.[6]K.KuriharaandM.Welling.Bayesiank-meansasamaximization-expectationalgorithm.NeuralCompu-tation 21(4) pp.1145–1172 2009.[7]R.Tibshirani G.WaltherandT.Hastie.EstimatingthenumberofclustersinadatasetviatheGapstatistic.JournaloftheRoyalStatisticalSocietyB 63 pp.411-423 2001.[8]L.Zelnik-ManorandP.Perona.Self-tuningspectralclustering.AdvancesinNeuralInformationProcessingSystems(NIPS) pp.1601–1608 2004.[9]T.Shi M.BelkinandB.Yu.DataSpectroscopy:eigenspacesofconvolutionoperatorsandclustering.TheAnnalsofStatistics 37(6B) pp.3960–3984 2009.[10]E.LevineandE.Domany.Resamplingmethodforunsupervisedestimationofclustervalidity.NeuralComputation 13(11) pp.2573-2593 2001.[11]RobertTibshiraniandG.Walther.Clustervalidationbypredictionstrength.Computational&GraphicalStatistics 14(3) pp.511-528 2005.[12]T.Lange V.Roth MikioL.Braun andJ.M.Buhmann.Stability-basedvalidationofclusteringsolutions.NeuralComputation 16(6) pp.1299-1323 2004.[13]I.S.Dhillon Y.GuanandB.Kulis.Kernelk-means:spectralclusteringandnormalizedcuts.InternationalConferenceonKnowledgeDiscoveryandDataMining(SIGKDD) pp.551–556 2004.[14]B.W.Silverman.UsingKerneldensityestimatestoinvestigatemultimodality.JournalofRoyalStatisticSocietyB 43(1) pp.97-99 1981.[15]J.A.HartiganandP.M.Hartigan.Thediptestofunimodality.TheAnnalsofStatistics 13(1) pp.70-84 1985.[16]D.L.Boley.Principaldirectiondivisivepartitioning.DataMiningandKnowledgeDiscovery 2(4) pp.344 1998.[17]M.Meila.Comparingclusterings–aninformationbaseddistance.MultivariateAnalysis 98(5) pp.873-895 2007.[18]L.HubertandP.Arabie.Comparingpartitions.JournalofClassiﬁcation 2(1) pp.193-218 1985.[19]J.J.Verbeek N.Vlassis andB.Kro”se.EﬃcientGreedyLearningofGaussianMixtureModels.NeuralComputation 15(2) pp.469-485 2003.[20]A.AsuncionandD.Newman.UCIMachineLearningRepository.UniversityofCaliforniaatIrvine Irvine CA 2007.Availableonline:http://www.ics.uci.edu/mlearn/MLRepository.html[21]S.A.Nene S.K.NayarandH.Murase.ColumbiaObjectImageLibrary(COIL-100).TechnicalReportCUCS-006-96 February1996.[22]D.Lowe.Distinctiveimagefeaturesfromscale-invariantkeypoints.JournalofComputerVision 60 pp.91-110 2004.9,Il Memming Park
Evan Archer
Nicholas Priebe
Jonathan Pillow