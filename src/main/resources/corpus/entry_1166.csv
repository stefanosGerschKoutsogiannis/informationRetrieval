2012,Convergence and Energy Landscape for Cheeger Cut Clustering,Unsupervised clustering of scattered  noisy and high-dimensional data points is an important and difficult problem. Continuous relaxations of balanced cut problems  yield excellent clustering results. This paper provides rigorous convergence results for two algorithms that solve the relaxed Cheeger Cut minimization.  The first algorithm is a new steepest descent algorithm and the second one is a slight modification of the Inverse Power Method algorithm \cite{pro:HeinBuhler10OneSpec}. While the steepest descent algorithm has better theoretical convergence properties   in practice both algorithm perform equally.  We also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem  and relate this characterization to the convergence of the algorithms.,Convergence and Energy Landscape for Cheeger Cut

Clustering

Xavier Bresson

City University of Hong Kong

Hong Kong

xbresson@cityu.edu.hk

Thomas Laurent

University of California  Riversize

Riverside  CA 92521

laurent@math.ucr.edu

David Uminsky

University of San Francisco
San Francisco  CA 94117
duminsky@usfca.edu

James H. von Brecht

University of California  Los Angeles

Los Angeles  CA 90095
jub@math.ucla.edu

Abstract

This paper provides both theoretical and algorithmic results for the (cid:96)1-relaxation
of the Cheeger cut problem. The (cid:96)2-relaxation  known as spectral clustering  only
loosely relates to the Cheeger cut; however  it is convex and leads to a simple op-
timization problem. The (cid:96)1-relaxation  in contrast  is non-convex but is provably
equivalent to the original problem. The (cid:96)1-relaxation therefore trades convexity
for exactness  yielding improved clustering results at the cost of a more challeng-
ing optimization. The ﬁrst challenge is understanding convergence of algorithms.
This paper provides the ﬁrst complete proof of convergence for algorithms that
minimize the (cid:96)1-relaxation. The second challenge entails comprehending the (cid:96)1-
energy landscape  i.e.
the set of possible points to which an algorithm might
converge. We show that (cid:96)1-algorithms can get trapped in local minima that are
not globally optimal and we provide a classiﬁcation theorem to interpret these lo-
cal minima. This classiﬁcation gives meaning to these suboptimal solutions and
helps to explain  in terms of graph structure  when the (cid:96)1-relaxation provides the
solution of the original Cheeger cut problem.

1

Introduction

Partitioning data points into sensible groups is a fundamental problem in machine learning. Given a
set of data points V = {x1 ···   xn} and similarity weights {wi j}1≤i j≤n  we consider the balance
Cheeger cut problem [4]:

Minimize C(S) =

(1)
Here |S| denotes the number of data points in S and Sc is the complementary set of S in V . While
this problem is NP-hard  it has the following exact continuous (cid:96)1-relaxation:

over all subsets S (cid:40) V .

xj∈Sc wi j

xi∈S
min(|S| |Sc|)

Minimize E(f ) =

(2)
Here med(f ) denotes the median of f ∈ Rn and fi ≡ f (xi). Recently  various algorithms have
been proposed [12  6  7  1  9  5] to minimize (cid:96)1-relaxations of the Cheeger cut (1) and of other
related problems. Typically these (cid:96)1-algorithms provide excellent unsupervised clustering results

over all non-constant functions f : V → R.

(cid:80)

(cid:80)

(cid:80)
(cid:80)
i j wi j|fi − fj|
i |fi − med(f )|

1
2

1

and improve upon the standard (cid:96)2 (spectral clustering) method [10  13] in terms of both Cheeger
energy and classiﬁcation error. However  complete theoretical guarantees of convergence for such
algorithms do not exist. This paper provides the ﬁrst proofs of convergence for (cid:96)1-algorithms that
attempt to minimize (2).
In this work we consider two algorithms for minimizing (2). We present a new steepest descent (SD)
algorithm and also consider a slight modiﬁcation of the inverse power method (IPM) from [6]. We
provide convergence results for both algorithms and also analyze the energy landscape. Speciﬁcally 
we give a complete classiﬁcation of local minima. This understanding of the energy landscape
provides intuition for when and how the algorithms get trapped in local minima. Our numerical
experiments show that the two algorithms perform equally well with respect to the quality of the
achieved cut. Both algorithms produce state of the art unsupervised clustering results. Finally  we
remark that the SD algorithm has a better theoretical guarantee of convergence. This arises from
the fact that the distance between two successive iterates necessarily converges to zero. In contrast 
we cannot guarantee this holds for the IPM without further assumptions on the energy landscape.
The simpler mathematical structure of the SD algorithm also provides better control of the energy
descent.
Both algorithms take the form of a ﬁxed point iteration f k+1 ∈ A(f k)  where f ∈ A(f ) implies
that f is a critical point. To prove convergence towards a ﬁx point typically requires three key
ingredients: the ﬁrst is monotonicity of A  that is E(z) ≤ E(f ) for all z ∈ A(f ); the second
is some estimate that guarantees the successive iterates remain in a compact domain on which E
is continuous; lastly  some type of continuity of the set-valued map A is required. For set valued
maps  closedness provides the correct notion of continuity [8]. Monotonicity of the IPM algorithm
was proven in [6]. This property alone is not enough to obtain convergence  and the closedness
property proves the most challenging ingredient to establish for the algorithms we consider. Section
2 elucidates the form these properties take for the SD and IPM algorithms. In Section 3 we show
that that if the iterates of either algorithm approach a neighborhood of a strict local minimum then
both algorithms will converge to this minimum. We refer to this property as local convergence.
When the energy is non-degenerate  section 4 extends this local convergence to global convergence
toward critical points for the SD algorithm by using the additional structure afforded by the gradient
ﬂow. In Section 5 we develop an understanding of the energy landscape of the continuous relaxation
problem. For non-convex problems an understanding of local minima is crucial. We therefore
provide a complete classiﬁcation of the local minima of (2) in terms of the combinatorial local
minima of (1) by means of an explicit formula. As a consequence of this formula  the problem
of ﬁnding local minima of the combinatorial problem is equivalent to ﬁnding local minima of the
continuous relaxation. The last section is devoted to numerical experiments.
We now present the SD algorithm. Rewrite the Cheeger functional (2) as E(f ) = T (f )/B(f ) 
where the numerator T (f ) is the total variation term and the denominator B(f ) is the balance term.
If T and B were differentiable  a mixed explicit-implicit gradient ﬂow of the energy would take the
form (f k+1−f k)/τ k = −(∇T (f k+1)−E(f k)∇B(f k))/(B(f k))  where {τ k} denotes a sequence
of time steps. As T and B are not differentiable  particularly at the binary solutions of paramount
interest  we must consider instead their subgradients

∂T (f ) := {v ∈ Rn : T (g) − T (f ) ≥ (cid:104)v  g − f(cid:105) ∀g ∈ Rn}  
∂0B(f ) := {v ∈ Rn : B(g) − B(f ) ≥ (cid:104)v  g − f(cid:105) ∀g ∈ Rn and (cid:104)1  v(cid:105) = 0} .

(3)
(4)
Here 1 ∈ Rn denotes the constant vector of ones. Also note that if f has zero median then B(f ) =
||f||1 and ∂0B(f ) = {v ∈ sign(f )  s.t. mean(v) = 0}. After an appropriate choice of time steps
we arrive to the SD Algorithm summarized in table 1(on left)  i.e. a non-smooth variation of steepest
descent. A key property of the the SD algorithm’s iterates is that (cid:107)f k+1 − f k(cid:107)2 → 0. This property
allows us to conclude global convergence of the SD algorithm in cases where we can not conclude
convergence for the IPM algorithm. We also summarize the IPM algorithm from [6] in Table 1 (on
right). Compared to the original algorithm from [6]  we have added the extra step to project onto
the sphere S n−1  that is f k+1 = hk/||hk||2. While we do not think that this extra step is essential 
it simpliﬁes the proof of convergence.
The successive iterates of both algorithms belong to the space

S n−1

0

:= {f ∈ Rn : ||f||2 = 1 and med(f ) = 0}.

(5)

2

Table 1: ASD : SD Algorithm.

f 0 nonzero function with med(f ) = 0.
c positive constant.
while E(f k) − E(f k+1) ≥ TOL do
vk ∈ ∂0B(f k)
gk = f k + c vk
ˆhk = arg min
hk = ˆhk − med(ˆhk)1
f k+1 = hk
(cid:107)hk(cid:107)2

T (u)+ E(f k)

||u−gk||2

u∈Rn

2c

2

end while

AIPM : Modifed IPM Algorithm [6].
f 0 nonzero function with med(f ) = 0.
while E(f k) − E(f k+1) ≥ TOL do

vk ∈ ∂0B(f k)
Dk = min||u||2≤1 T (u) − E(f k)(cid:104)u  vk(cid:105)
gk = arg min
||u||2≤1

T (u)−E(f k)(cid:104)u  vk(cid:105) if Dk< 0

gk = f k if Dk = 0
hk = gk − med(gk)1
f k+1 = hk

||hk||2

end while

As the successive iterates have zero median  ∂0B(f k) is never empty. For example  we can take
vk ∈ Rn so that vk(xi) = 1 if f (xi) > 0  vk(xi) = −1 if f (xi) < 0 and vk(xi) = (n−− n+)/(n0)
if f (xi) = 0 where n+  n− and n0 denote the cardinalities of the sets {xi : f (xi) > 0}  {xi :
f (xi) > 0} and {xi : f (xi) = 0}  respectively. Other possible choices also exist  so that vk is
not uniquely deﬁned. This idea  i.e. choosing an element from the subdifferential with mean zero 
was introduced in [6] and proves indispensable when dealing with median zero functions. As vk is
not uniquely deﬁned in either algorithm  we must introduce the concepts of a set-valued map and a
closed map  which is the proper notion of continuity in this context:
Deﬁnition 1 (Set-valued Map  Closed Maps). Let X and Y be two subsets of Rn. If for each x ∈ X
there is a corresponding set F (x) ⊂ Y then F is called a set-valued map from X to Y . We denote
this by F : X ⇒ Y . The graph of F   denoted Graph(F) is deﬁned by

Graph(F ) = {(x  y) ∈ Rn × Rn : x ∈ X  y ∈ F (x)}.

A set-valued map F is called closed if Graph(F ) is a closed subset of Rn × Rn.
With these notations in hand we can write f k+1 ∈ ASD(f k) (SD algorithm) and f k+1 ∈ AIPM(f k)
(IPM algorithm) where ASD AIPM : S n−1
are the appropriate set-valued maps. The
notion of a closed map proves useful when analyzing the step ˆhk ∈ H(f k) in the SD algorithm.
Particularly 
Lemma 1 (Closedness of H(f )). The following set-valued map H : S n−1

⇒ Rn is closed.

⇒ S n−1

0

0

0

(cid:27)

(cid:26)

H(f ) := arg min

u

T (u) +

E(f )

2c

||u − (f + c∂0B(f ))||2

2

Currently  we can only show that lemma 1 holds at strict local minima for the analogous step  gk 
of the IPM algorithm. That lemma 1 holds without this further restriction on f ∈ S n−1
will allow
us to demonstrate stronger global convergence results for the SD algorithm. Due to page limitations
the supplementary material contains the proofs of all lemmas and theorems in this paper.
2 Properties of ASD and AIPM
This section establishes the required properties of the of the set-valued maps ASD and AIPM men-
tioned in the introduction. In section 2.1 we ﬁrst elucidate the monotonicity and compactness of
ASD and AIPM. Section 2.2 demonstrates that a local notion of closedness holds for each algorithm.
This form of closedness sufﬁces to show local convergence toward isolated local minima (c.f. Sec-
tion 3). In particular  this more difﬁcult and technical section is necessary as monotonicity alone
does not guarantee this type of convergence.

0

2.1 Monotonicity and Compactness

We provide the monotonicity and compactness results for each algorithm in turn. Lemmas 2 and 3
establish monotonicity and compactness for ASD while Lemmas 4 and 5 establish monotonicity and
compactness for AIPM.

3

Lemma 2 (Monotonicity of ASD). Let f ∈ S n−1
algorithm. Then neither ˆh nor h is a constant vector. Moreover  the energy inequality

and deﬁne v  g  ˆh and h according to the SD

0

E(f ) ≥ E(h) +

E(f )
B(h)

(cid:107)ˆh − f(cid:107)2

2

c

holds. As a consequence  if z ∈ ASD(f ) then E(z) = E(h) < E(f ) unless z = f.
Lemma 3 (Compactness of ASD). Let f 0 ∈ S n−1
(gk  ˆhk  hk  f k+1) according to the SD algorithm. Then for any such sequence
0 < ||hk||2 ≤ (1 +

1 ≤ ||gk||2 ≤ 1 + c

(cid:107)ˆhk(cid:107)2 ≤ (cid:107)gk(cid:107)2 

n and

√

√

0

and deﬁne a sequence of

n)||ˆhk||2.

Moreover  we have

(6)

iterates

(7)

(8)

(cid:107)f k − f k+1(cid:107)2 → 0.

med(ˆhk) → 0 
||ˆhk − f k||2 → 0 
attracts the sequences {ˆhk} and {hk}.

0

Therefore S n−1
By the monotonicity result of Hein and B¨uhler [6] we have
Lemma 4 (Monotonicity of AIPM). Let f ∈ S n−1
z = f.
To prove convergence for AIPM using our techniques  we must also maintain control over the iterates
after subtracting the median. This control is provided by the following lemma.
Lemma 5 (Compactness of AIPM). Let f ∈ S n−1

. If z ∈ AIPM(f ) then E(z) < E(f ) unless

and deﬁne v  D  g and h according to the IPM.

0

0

1. The minimizer is unique when D < 0  i.e. g ∈ S n−1 is a single point.
2. 1 ≤ ||h||2 ≤ 1 +

n. In particular  AIPM(f ) is always well-deﬁned for a given choice of

√

v ∈ ∂0B(f ).

2.2 Closedness Properties

The ﬁnal ingredient to prove local convergence is some form of closedness. We require closedness
of the set valued maps A at strict local minima of the energy. As the energy (2) is invariant under
constant shifts and scalings  the usual notion of a strict local minimum on Rn does not apply. We
must therefore remove the effects of these invariances when referring to a local minimum as strict.
To this end  deﬁne the spherical and annular neighborhoods on S n−1

by

B(f∞) := {||f − f∞||2 ≤ } ∩ S n−1

0

Aδ (f∞) := {δ ≤ ||f − f∞||2 ≤ } ∩ S n−1

.

0

0

0

0

. We say f∞ is a strict local minimum of the

With these in hand we introduce the proper deﬁnition of a strict local minimum.
Deﬁnition 2 (Strict Local Minima). Let f∞ ∈ S n−1
energy if there exists  > 0 so that f ∈ B(f∞) and f (cid:54)= f∞ imply E(f ) > E(f∞).
This deﬁnition then allows us to formally deﬁne closedness at a strict local minimum in Deﬁnition
3. For the IPM algorithm this is the only form of closedness we are able to establish. Closedness at
an arbitrary f ∈ S n−1
(c.f. lemma 1) does in fact hold for the SD algorithm. Once again  this fact
manifests itself in the stronger global convergence results for the SD algorithm in section 4.
Deﬁnition 3 (CLM/CSLM Mappings). Let A(f ) : S n−1
denote a set-valued mapping.
We say A(f ) is closed at local minima (CLM) if zk ∈ A(f k) and f k → f∞ imply zk → f∞
whenever f∞ is a local minimum of the energy. If zk → f∞ holds only when f∞ is a strict local
minimum then we say A(f ) is closed at strict local minima (CSLM).
The CLM property for the SD algorithm  provided by lemma 6  follows as a straight forward conse-
quence of lemma 1. The CSLM property for the IPM algorithm provided by lemma 7 requires the
additional hypothesis that the local minimum is strict.
Lemma 6 (CLM Property for ASD). For f ∈ S n−1
Then ASD(f ) deﬁnes a CLM mapping.
Lemma 7 (CSLM Property for AIPM). For f ∈ S n−1
AIPM(f ) deﬁnes a CSLM mapping.

deﬁne g  ˆh and h according to the SD algorithm.

deﬁne v  D  g  h according to the IPM. Then

⇒ S n−1

0

0

0

0

4

3 Local Convergence of ASD and AIPM at Strict Local Minima
Due to the lack of convexity of the energy (2)   at best we can only hope to obtain convergence
to a local minimum of the energy. An analogue of Lyapunov’s method from differential equations
allows us to show that such convergence does occur provided the iterates reach a neighborhood of
an isolated local minimum. To apply the lemmas from section 2 we must assume that f∞ ∈ S n−1
is a local minimum of the energy. We will assume further that f∞ is an isolated critical point of the
energy according to the following deﬁnition.
Deﬁnition 4 (Isolated Critical Points). Let f ∈ S n−1
. We say that f is a critical point of the energy
E(f ) if there exist w ∈ ∂T (f ) and v ∈ ∂0B(f ) so that 0 = w − E(f )v. This generalizes the usual
quotient rule 0 = ∇T (f ) − E(f )∇B(f ). If there exists  > 0 so that f is the only critical point in
B(f∞) we say f is an isolated critical point of the energy.
Note that as any local minimum is a critical point of the energy  if f∞ is an isolated critical point
and a local minimum then it is necessarily a strict local minimum. The CSLM property therefore
applies.
Finally  to show convergence  the set-valued map A must possess one further property  i.e.
critical point property.
Deﬁnition 5 (Critical Point Property). Let A(f ) : S n−1
denote a set-valued mapping. We
say that A(f ) satisﬁes the critical point property (CP property) if  given any sequence satisfying
f k+1 ∈ A(f k)  all limit points of {f k} are critical points of the energy.

⇒ S n−1

the

0

0

0

0

0

Analogously to the CLM property  for the SD algorithm the CP property follows as a direct conse-
quence of lemma 1. For the IPM algorithm it follows from closedness of the minimization step.
The proof of local convergence utilizes a version of Lyapunov’s direct method for set-valued maps 
and we adapt this technique from the strategy outlined in [8]. We ﬁrst demonstrate that if any
iterate f k lies in a sufﬁciently small neighborhood Bγ(f∞) of the strict local minimum then all
subsequent iterates remain in the neighborhood B(f∞) in which f∞ is an isolated critical point.
By compactness and the CP property  any subsequence of {f k} must have a further subsequence
that converges to the only critical point in B(f∞)  i.e. f∞. This implies that the whole sequence
must converge to f∞ as well. We formalize this argument in lemma 8 and its corollary theorem 1.
Lemma 8 (Lyapunov Stability at Strict Local Minima). Suppose A(f ) is a monotonic  CSLM map-
ping. Fix f 0 ∈ S n−1
and let {f k} denote any sequence satisfying f k+1 ∈ A(f k). If f∞ is a strict
local minimum of the energy  then for any  > 0 there exists a γ > 0 so that if f 0 ∈ Bγ(f∞) then
{f k} ⊂ B(f∞).
Theorem 1 (Local Convergence at Isolated Critical Points). Let A(f ) : S n−1
denote a
and suppose {f k} is any sequence satisfying
monotonic  CSLM  CPP mapping. Let f 0 ∈ S n−1
f k+1 ∈ A(f k). Let f∞ denote a local minimum that is an isolated critical point of the energy. If
f 0 ∈ Bγ(f∞) for γ > 0 sufﬁciently small then f k → f∞.
Note that both algorithms satisfy the hypothesis of theorem 1  and therefore possess identical lo-
cal convergence properties. A slight modiﬁcation of the proof of theorem 1 yields the following
corollary that also applies to both algorithms.
Corollary 1. Let f 0 ∈ S n−1
be arbitrary  and deﬁne f k+1 ∈ A(f k) according to either algorithm.
If any accumulation point f∗ of the sequence {f k} is both an isolated critical point of the energy
and a local minimum  then the whole sequence f k → f∗.
4 Global Convergence for ASD
To this point the convergence properties of both algorithms appear identical. However  we have
yet to take full advantage of the superior mathematical structure afforded by the SD algorithm.
In particular  from lemma 3 we know that ||f k+1 − f k||2 → 0 without any further assumptions
regarding the initialization of the algorithm or the energy landscape. This fact combines with the
fact that lemma 1 also holds globally for f ∈ S n−1
to yield theorem 2. Once again  we arrive at this
conclusion by adapting the proof from [8].

⇒ S n−1

0

0

0

0

0

5

Theorem 2 (Convergence of the SD Algorithm). Take f 0 ∈ S n−1
{f k} denote any sequence satisfying f k+1 ∈ ASD(f k). Then

0

and ﬁx a constant c > 0. Let

1. Any accumulation point f∗ of the sequence is a critical point of the energy.
2. Either the sequence converges  or the set of accumulation points form a continuum in S n−1

0

.

We might hope to rule out the second possibility in statement 2 by showing that E can never have
an uncountable number of critical points. Unfortunately  we can exhibit (c.f.
the supplementary
material) simple examples to show that a continuum of local or global minima can in fact happen.
This degeneracy of a continuum of critical points arises from a lack of uniqueness in the underlying
combinatorial problem. We explore this aspect of convergence further in section 5.
By assuming additional structure in the energy landscape we can generalize the local convergence
result  theorem 1  to yield global convergence of both algorithms. This is the content of corollary 2
for the SD algorithm and the content of corollary 3 for the IPM algorithm. The hypotheses required
for each corollary clearly demonstrate the beneﬁt of knowing apriori that ||f k+1− f k||2 → 0 occurs
for the SD algorithm. For the IPM algorithm  we can only deduce this a posteriori from the fact that
the iterates converge.
Corollary 2. Let f 0 ∈ S n−1
be arbitrary and deﬁne f k+1 ∈ ASD(f k). If the energy has only
countably many critical points in S n−1
Corollary 3. Let f 0 ∈ S n−1
points of the energy are isolated in S n−1
converges.

be arbitrary and deﬁne f k+1 ∈ AIPM(f k). Suppose all critical
and are either local maxima or local minima. Then {f k}

then {f k} converges.

0

0

0

0

While at ﬁrst glance corollary 3 provides hope that global convergence holds for the IPM algorithm 
our simple examples in the supplementary material demonstrate that even benign graphs with well-
deﬁned cuts have critical points of the energy that are neither local maxima nor local minima.

5 Energy Landscape of the Cheeger Functional

This section demonstrates that the continuous problem (2) provides an exact relaxation of the combi-
natorial problem (1). Speciﬁcally  we provide an explicit formula that gives an exact correspondence
between the global minimizers of the continuous problem and the global minimizers of the combi-
natorial problem. This extends previous work [12  11  9] on the relationship between the global
minima of (1) and (2). We also completely classiﬁy the local minima of the continuous problem by
introducing a notion of local minimum for the combinatorial problem. Any local minimum of the
combinatorial problem then determines a local minimum of the combinatorial problem by means of
an explicit formula  and vice-versa. Theorem 4 provides this formula  which also gives a sharp con-
dition for when a global minimum of the continuous problem is two-valued (binary)  three-valued
(trinary)  or k-valued in the general case. This provides an understanding the energy landscape 
which is essential due to the lack of convexity present in the continuous problem. Most importantly 
we can classify the types of local minima encountered and when they form a continuum. This is
germane to the global convergence results of the previous sections. The proofs in this section follow
closely the ideas from [12  11].

5.1 Local and Global Minima

We ﬁrst introduce the two fundamental deﬁnitions of this section. The ﬁrst deﬁnition introduces the
concept of when a set S ⊂ V of vertices is compatible with an increasing sequence S1 (cid:40) S2 (cid:40)
··· (cid:40) Sk of vertex subsets. Loosely speaking  a set S is compatible with S1 (cid:40) S2 (cid:40) ··· (cid:40) Sk
whenever the cut deﬁned by the pair (S  Sc) neither intersects nor crosses any of the cuts (Si  Sc
i ).
Deﬁnition 6 formalizes this notion.
Deﬁnition 6 (Compatible Vertex Set). A vertex set S is compatible with an increasing sequence
S1 (cid:40) S2 (cid:40) ··· (cid:40) Sk if S ⊆ S1  Sk ⊆ S or

S1 (cid:40) S2 (cid:40) ··· (cid:40) Si ⊆ S ⊆ Si+1 (cid:40) ··· (cid:40) Sk

for some 1 ≤ i ≤ k − 1 

6

1) ···   (Sk  Sc

The concept of compatible cuts then allows us to introduce our notion of a local minimum of the
combinatorial problem  i.e. deﬁnition 7.
Deﬁnition 7 (Combinatorial k-Local Minima). An increasing collection of nontrivial sets S1 (cid:40)
S2 (cid:40) ··· (cid:40) Sk is called a k-local minimum of the combinatorial problem if C(S1) = C(S2) =
··· = C(Sk) ≤ C(S) for all S compatible with S1 (cid:40) S2 (cid:40) ··· (cid:40) Sk.
Pursuing the previous analogy  a collection of cuts (S1  Sc
k) forms a k-local minimum
of the combinatorial problem precisely when they do not intersect  have the same energy and all other
non-intersecting cuts (S  Sc) have higher energy. The case of a 1-local minimum is paramount. A cut
1) deﬁnes a 1-local minimum if and only if it has lower energy than all cuts that do not intersect
(S1  Sc
it. As a consequence  if a 1-local minimum is not a global minimum then the cut (S1  Sc
1) necessarily
intersects all of the cuts deﬁned by the global minimizers. This is a fundamental characteristic of
local minima: they are never “parallel” to global minima.
For the continuous problem  combinatorial k-local minima naturally correspond to vertex functions
f ∈ Rn that take (k + 1) distinct values. We therefore deﬁne the concept of a (k + 1)-valued local
minimum of the continuous problem.
Deﬁnition 8 (Continuous (k + 1)-valued Local Minima). We call a vertex function f ∈ Rn a
(k + 1)-valued local minimum of the continuous problem if f is a local minimum of E and if its
range contains exactly k + 1 distinct values.

Theorem 3 provides the intuitive picture connecting these two concepts of minima  and it follows as
a corollary of the more technical and explicit theorem 4.
Theorem 3. The continuous problem has a (k + 1)-valued local minimum if and only if the combi-
natorial problem has a k-local minimum.

For example  if the continuous problem has a trinary local minimum in the usual sense then the com-
binatorial problem must have a 2-local minimum in the sense of deﬁnition 7. As the cuts (S1  Sc
1)
and (S2  Sc
2) deﬁning a 2-local minimum do not intersect  a 2-local minimum separates the vertices
of the graph into three disjoint domains. A trinary function therefore makes intuitive sense. We
make this intuition precise in theorem 4. Before stating it we require two further deﬁnitions.
Deﬁnition 9 (Characteristic Functions). Given ∅ (cid:54)= S ⊂ V   deﬁne its characteristic function fS
as
if |S| > n/2. (9)
fS = Cut(S  Sc)
Note that fS has median zero and T V -norm equal to 1.
Deﬁnition 10 (Strict Convex Hull). Given k functions f1 ···   fk  their strict convex hull is the set
(10)

sch{f1 ···   fk} = {θ1f1 + ··· + θkfk : θi > 0 for 1 ≤ i ≤ k and θ1 + ··· + θk = 1}

fS = −Cut(S  Sc)

−1χSc

−1χS

if |S| ≤ n/2

and

Theorem 4 (Explicit Correspondence of Local Minima).

1. Suppose S1 (cid:40) S2 (cid:40) ··· (cid:40) Sk is a k-local minimum of the combinatorial problem and let
f ∈ sch{fS1  ···   fSk}. Then any function of the form g = αf + β1 deﬁnes a (k + 1)-
valued local minimum of the continuous problem and with E(g) = C(S1).

2. Suppose that f is a (k + 1)-valued local minimum and let c1 > c2 > ··· > ck+1 denote
its range. For 1 ≤ i ≤ k set Ωi = {f = ci}. Then the increasing collection of sets
S1 (cid:40) ··· (cid:40) Sk given by

S1 = Ω1  S2 = Ω1 ∪ Ω2

···

Sk = Ω1 ∪ ··· ∪ Ωk

is a k-local minimum of the combinatorial problem with C(Si) = E(f ).

Remark 1 (Isolated vs Continuum of Local Minima). If a set S1 is a 1-local min then the strict
convext hull (10) of its characteristic function reduces to the single binary function fS1. Thus every
1-local minimum generates exactly one local minimum of the continuous problem in S n−1
  and this
local minimum is binary. On the other hand  if k ≥ 2 then every k-local minimum of the combi-
natorial problem generates a continuum (in S n−1
) of non-binary local minima of the continuous
problem. Thus  the hypotheses of theorem 1  corollary 2 or corollary 3 can hold only if no such
higher order k-local minima exist. When these theorems do apply the algorithms therefore con-
verge to a binary function.

0

0

7

As a ﬁnal consequence  we summarize the fact that theorem 4 implies that the continuous relaxation
of the Cheeger cut problem is exact. In other words 
Theorem 5. Given {f ∈ arg min E} an explicit formula exists to construct the set {S ∈
arg minC}  and vice-versa.

6 Experiments

In all experiments  we take the constant c = 1 in the SD algorithm. We use the method from
[3] to solve the minimization problem in the SD algorithm and the method from [7] to solve the
minimization problem in the IPM algorithm. We terminate each minimization when either a stopping
tolerance of ε = 10−10 (i.e. (cid:107)uj+1 − uj(cid:107)1 ≤ ε) or 2  000 iterations is reached. This yields a
comparison of the idealized cases of the SD algorithm and the IPM algorithm. Our ﬁrst experiment
uses the two-moon dataset [2] in the same setting as in [12]. The second experiment utilizes pairs of
image digits extracted from the MNIST dataset. The ﬁrst table summarizes the results of these tests.
It shows the mean Cheeger energy value (2)  the mean error of classiﬁcation (% of misclassiﬁed data)
and the mean computational time for both algorithms over 10 experiments with the same random
initialization for both algorithms in each of the individual experiments.

SD Algorithm

Modiﬁed IPM Algorithm [7]

Energy Error (%) Time (sec.) Energy Error (%) Time (sec.)
0.126
0.115
0.086

0.145
0.185
0.086

14.12
25.23
1.219

8.69
1.65
1.217

1.98
58.9
48.1

2.06
52.4
49.2

2 moons
4’s and 9’s
3’s and 8’s

Our second set of experiments applies both algorithms to multi-class clustering problems using a
standard  recursive bi-partitioning method. We use the MNIST  USPS and COIL datasets. We
preprocessed the data by projecting onto the ﬁrst 50 principal components  and take k = 10 nearest
neighbors for the MNIST and USPS datasets and k = 5 nearest neighbors for the COIL dataset.
We used the same tolerances for the minimization problems  i.e. ε = 10−10 and 2  000 maximum
iterations. The table below presents the mean Cheeger energy  classiﬁcation error and time over 10
experiments as before.

MNIST (10 classes)
USPS (10 classes)
COIL (20 classes)

SD Algorithm

Modiﬁed IPM Algorithm [7]

Energy Err. (%) Time (min.) Energy Err. (%) Time (min.)
1.30
2.37
0.19

11.75
4.13
2.52

45.01
5.15
4.31

42.83
4.81
4.20

11.78
4.11
1.58

1.29
2.37
0.18

Overall  the results show that both algorithms perform equivalently for both two-class and multi-
class clustering problems.
As our interest here lies in the theoretical properties of both algorithms  we will study practical
implementation details for the SD algorithm in future work. For instance  as Hein and B¨uhler remark
[6]  solving the minimization problem for the IPM algorithm precisely is unnecessary. Analogously
for the SD Algorithm  we only need to lower the energy sufﬁciently before proceeding to the next
iteration of the algorithm. It proves convenient to stop the minimization when a weaker form of the
energy inequality (6) holds  such as

(cid:32)

(cid:33)

E(f ) ≥ E(h) + θ

E(f )
B(h)

||ˆh − f||2

2

c

for some constant 0 < θ < 1. This condition provably holds in a ﬁnite number of iterations and
still guarantees that ||f k+1 − f k||2 → 0. The concrete decay estimate provided by SD algorithm
therefore allows us to give precise meaning to “sufﬁciently lowers the energy.” We investigate these
aspects of the algorithm and prove convergence for this practical implementation in future work.
Reproducible research: The code is available at http://www.cs.cityu.edu.hk/∼xbresson/codes.html
Acknowledgements: This work supported by AFOSR MURI grant FA9550-10-1-0569 and Hong
Kong GRF grant #110311.

8

References
[1] X. Bresson  X.-C. Tai  T.F. Chan  and A. Szlam. Multi-Class Transductive Learning based on

(cid:96)1 Relaxations of Cheeger Cut and Mumford-Shah-Potts Model. UCLA CAM Report  2012.

[2] T. B¨uhler and M. Hein. Spectral Clustering Based on the Graph p-Laplacian. In International

Conference on Machine Learning  pages 81–88  2009.

[3] A. Chambolle and T. Pock. A First-Order Primal-Dual Algorithm for Convex Problems with
Applications to Imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.
[4] J. Cheeger. A Lower Bound for the Smallest Eigenvalue of the Laplacian. Problems in Analy-

sis  pages 195–199  1970.

[5] F. R. K. Chung. Spectral Graph Theory  volume 92 of CBMS Regional Conference Series in
Mathematics. Published for the Conference Board of the Mathematical Sciences  Washington 
DC  1997.

[6] M. Hein and T. B¨uhler. An Inverse Power Method for Nonlinear Eigenproblems with Ap-
In In Advances in Neural Information

plications in 1-Spectral Clustering and Sparse PCA.
Processing Systems (NIPS)  pages 847–855  2010.

[7] M. Hein and S. Setzer. Beyond Spectral Clustering - Tight Relaxations of Balanced Graph

Cuts. In In Advances in Neural Information Processing Systems (NIPS)  2011.

[8] R.R. Meyer. Sufﬁcient conditions for the convergence of monotonic mathematical program-

ming algorithms. Journal of Computer and System Sciences  12(1):108 – 121  1976.

[9] S. Rangapuram and M. Hein. Constrained 1-Spectral Clustering. In International conference

on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 1143–1151  2012.

[10] J. Shi and J. Malik. Normalized Cuts and Image Segmentation. IEEE Transactions on Pattern

Analysis and Machine Intelligence (PAMI)  22(8):888–905  2000.

[11] G. Strang. Maximal Flow Through A Domain. Mathematical Programming  26:123–143 

1983.

[12] A. Szlam and X. Bresson. Total variation and cheeger cuts. In Proceedings of the 27th Inter-

national Conference on Machine Learning  pages 1039–1046  2010.

[13] L. Zelnik-Manor and P. Perona. Self-tuning Spectral Clustering. In In Advances in Neural

Information Processing Systems (NIPS)  2004.

9

,Ben Shababo
Brooks Paige
Ari Pakman
Liam Paninski
Qilong Wang
Zilin Gao
Jiangtao Xie
Wangmeng Zuo
Peihua Li