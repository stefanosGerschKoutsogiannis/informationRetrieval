2016,Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm,We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution  by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets  on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein’s identity and a recently proposed kernelized Stein discrepancy  which is of independent interest.,Stein Variational Gradient Descent: A General

Purpose Bayesian Inference Algorithm

Qiang Liu

Dilin Wang
Department of Computer Science

Dartmouth College
Hanover  NH 03755

{qiang.liu  dilin.wang.gr}@dartmouth.edu

Abstract

We propose a general purpose variational inference algorithm that forms a natural
counterpart of gradient descent for optimization. Our method iteratively trans-
ports a set of particles to match the target distribution  by applying a form of
functional gradient descent that minimizes the KL divergence. Empirical studies
are performed on various real world models and datasets  on which our method is
competitive with existing state-of-the-art methods. The derivation of our method
is based on a new theoretical result that connects the derivative of KL divergence
under smooth transforms with Stein’s identity and a recently proposed kernelized
Stein discrepancy  which is of independent interest.

1

Introduction

Bayesian inference provides a powerful tool for modeling complex data and reasoning under uncer-
tainty  but casts a long standing challenge on computing intractable posterior distributions. Markov
chain Monte Carlo (MCMC) has been widely used to draw approximate posterior samples  but is
often slow and has difﬁculty accessing the convergence. Variational inference instead frames the
Bayesian inference problem into a deterministic optimization that approximates the target distribution
with a simpler distribution by minimizing their KL divergence. This makes variational methods
efﬁciently solvable by using off-the-shelf optimization techniques  and easily applicable to large
datasets (i.e.  "big data") using the stochastic gradient descent trick [e.g.  1]. In contrast  it is much
more challenging to scale up MCMC to big data settings [see e.g.  2  3].
Meanwhile  both the accuracy and computational cost of variational inference critically depend on
the set of distributions in which the approximation is deﬁned. Simple approximation sets  such as
these used in the traditional mean ﬁeld methods  are too restrictive to resemble the true posterior
distributions  while more advanced choices cast more difﬁculties on the subsequent optimization tasks.
For this reason  efﬁcient variational methods often need to be derived on a model-by-model basis 
causing is a major barrier for developing general purpose  user-friendly variational tools applicable
for different kinds of models  and accessible to non-ML experts in application domains.
This case is in contrast with the maximum a posteriori (MAP) optimization tasks for ﬁnding the
posterior mode (sometimes known as the poor man’s Bayesian estimator  in contrast with the full
Bayesian inference for approximating the full posterior distribution)  for which variants of (stochastic)
gradient descent serve as a simple  generic  yet extremely powerful toolbox. There has been a recent
growth of interest in creating user-friendly variational inference tools [e.g.  4–7]  but more efforts are
still needed to develop more efﬁcient general purpose algorithms.
In this work  we propose a new general purpose variational inference algorithm which can be treated
as a natural counterpart of gradient descent for full Bayesian inference (see Algorithm 1). Our

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

algorithm uses a set of particles for approximation  on which a form of (functional) gradient descent
is performed to minimize the KL divergence and drive the particles to ﬁt the true posterior distribution.
Our algorithm has a simple form  and can be applied whenever gradient descent can be applied. In
fact  it reduces to gradient descent for MAP when using only a single particle  while automatically
turns into a full Bayesian sampling approach with more particles.
Underlying our algorithm is a new theoretical result that connects the derivative of KL divergence
w.r.t. smooth variable transforms and a recently introduced kernelized Stein discrepancy [8–10] 
which allows us to derive a closed form solution for the optimal smooth perturbation direction that
gives the steepest descent on the KL divergence within the unit ball of a reproducing kernel Hilbert
space (RKHS). This new result is of independent interest  and can ﬁnd wide application in machine
learning and statistics beyond variational inference.

2 Background

Preliminary Let x be a continuous random variable or parameter of interest taking values in
X ⊂ Rd  and {Dk} is a set of i.i.d. observation. With prior p0(x)  Bayesian inference of x involves
k=1 p(Dk|x)  where

reasoning with the posterior distribution p(x) := ¯p(x)/Z with ¯p(x) := p0(x)(cid:81)N
Z =(cid:82) ¯p(x)dx is the troublesome normalization constant. We have dropped the conditioning on data
(RKHS) H of k(x  x(cid:48)) is the closure of linear span {f : f (x) =(cid:80)m
N  xi ∈ X}  equipped with inner products (cid:104)f  g(cid:105)H =(cid:80)
inner product (cid:104)f   g(cid:105)Hd = (cid:80)d

{Dk} in p(x) for convenience.
Let k(x  x(cid:48)) : X × X → R be a positive deﬁnite kernel. The reproducing kernel Hilbert space
i=1 aik(x  xi)  ai ∈ R  m ∈
i bik(x  xi).
Denote by Hd the space of vector functions f = [f1  . . .   fd](cid:62) with fi ∈ H  equipped with
i=1(cid:104)fi  gi(cid:105)H. We assume all the vectors are column vectors. Let
∇xf = [∇xf1  . . .  ∇xfd].
Stein’s Identity and Kernelized Stein Discrepancy Stein’s identity plays a fundamental role in
our framework. Let p(x) be a continuously differentiable (also called smooth) density supported on
X ⊆ Rd  and φ(x) = [φ1(x) ···   φd(x)](cid:62) a smooth vector function. Stein’s identity states that for
sufﬁciently regular φ  we have

ij aibjk(xi  xj) for g(x) =(cid:80)

Br

Br

Ex∼p[Apφ(x)] = 0 

where

Apφ(x) = ∇x log p(x)φ(x)(cid:62) + ∇xφ(x) 

limr→∞(cid:72)

p(x)φ(x)(cid:62)n(x)dS = 0 when X = Rd  where(cid:72)

(1)
where Ap is called the Stein operator  which acts on function φ and yields a zero mean function
Apφ(x) under x ∼ p. This identity can be easily checked using integration by parts by assuming
mild zero boundary conditions on φ: either p(x)φ(x) = 0  ∀x ∈ ∂X when X is compact  or
is the surface integral on the sphere
Br of radius r centered at the origin and n(x) is the unit normal to Br. We call that φ is in the Stein
class of p if Stein’s identity (1) holds.
Now let q(x) be a different smooth density also supported in X   and consider the expectation of
Apφ(x) under x ∼ q  then Ex∼q[Apφ(x)] would no longer equal zero for general φ. Instead  the
magnitude of Ex∼q[Apφ(x)] relates to how different p and q are  and can be leveraged to deﬁne a
discrepancy measure  known as Stein discrepancy  by considering the “maximum violation of Stein’s
identity” for φ in some proper function set F:
D(q  p) = max
φ∈F

(cid:8)Ex∼q[trace(Apφ(x))](cid:9) 

Here the choice of this function set F is critical  and decides the discriminative power and computa-
tional tractability of Stein discrepancy. Traditionally  F is taken to be sets of functions with bounded
Lipschitz norms  which unfortunately casts a challenging functional optimization problem that is
computationally intractable or requires special considerations (see Gorham and Mackey [11] and
reference therein).
Kernelized Stein discrepancy (KSD) bypasses this difﬁculty by maximizing φ in the unit ball of a
reproducing kernel Hilbert space (RKHS) for which the optimization has a closed form solution.
KSD is deﬁned as

(cid:8)Ex∼q[trace(Apφ(x))] 

||φ||Hd ≤ 1(cid:9) 

s.t.

D(q  p) = max
φ∈Hd

(2)

2

where we assume the kernel k(x  x(cid:48)) of RKHS H is in the Stein class of p as a function of x for any
ﬁxed x(cid:48) ∈ X . The optimal solution of (2) has been shown to be φ(x) = φ
q p||Hd [8–10] 
q p(x)/||φ
∗
∗
where

for which we have

q p(·) = Ex∼q[Apk(x ·)] 
∗
φ

D(q  p) = ||φ
q p||Hd .
∗
(3)
q p(x) ≡ 0) if and only if p = q
∗
One can further show that D(q  p) equals zero (and equivalently φ
once k(x  x(cid:48)) is strictly positive deﬁnite in a proper sense [See 8  10]  which is satisﬁed by commonly
used kernels such as the RBF kernel k(x  x(cid:48)) = exp(− 1
h||x − x(cid:48)||2
2). Note that the RBF kernel is
also in the Stein class of smooth densities supported in X = Rd because of its decaying property.
Both Stein operator and KSD depend on p only through the score function ∇x log p(x)  which can
be calculated without knowing the normalization constant of p  because we have ∇x log p(x) =
∇x log ¯p(x) when p(x) = ¯p(x)/Z. This property makes Stein’s identity a powerful tool for handling
unnormalized distributions that appear widely in machine learning and statistics.

3 Variational Inference Using Smooth Transforms
Variational inference approximates the target distribution p(x) using a simpler distribution q∗(x)
found in a predeﬁned set Q = {q(x)} of distributions by minimizing the KL divergence  that is 

(cid:8)KL(q || p) ≡ Eq[log q(x)] − Eq[log ¯p(x)] + log Z(cid:9) 

q∗ = arg min
q∈Q

(4)

where we do not need to calculate the constant log Z for solving the optimization. The choice of
set Q is critical and deﬁnes different types of variational inference methods. The best set Q should
strike a balance between i) accuracy  broad enough to closely approximate a large class of target
distributions  ii) tractability  consisting of simple distributions that are easy for inference  and iii)
solvability so that the subsequent KL minimization problem can be efﬁciently solved.
In this work  we focus on the sets Q consisting of distributions obtained by smooth transforms from a
tractable reference distribution  that is  we take Q to be the set of distributions of random variables of
form z = T (x) where T : X → X is a smooth one-to-one transform  and x is drawn from a tractable
reference distribution q0(x). By the change of variables formula  the density of z is

q[T ](z) = q(T −1(z)) · | det(∇zT −1(z))| 

where T −1 denotes the inverse map of T and ∇zT −1 the Jacobian matrix of T −1. Such distributions
are computationally tractable  in the sense that the expectation under q[T ] can be easily evaluated by
averaging {zi} when zi = T (xi) and xi ∼ q0. Such Q can also in principle closely approximate
almost arbitrary distributions: it can be shown that there always exists a measurable transform T
between any two distributions without atoms (i.e. no single point carries a positive mass); in addition 
for Lipschitz continuous densities p and q  there always exist transforms between them that are least
as smooth as both p and q. We refer the readers to Villani [12] for in-depth discussion on this topic.
In practice  however  we need to restrict the set of transforms T properly to make the corresponding
variational optimization in (4) practically solvable. One approach is to consider T with certain
parametric form and optimize the corresponding parameters [e.g.  13  14]. However  this introduces a
difﬁcult problem on selecting the proper parametric family to balance the accuracy  tractability and
solvability  especially considering that T has to be an one-to-one map and has to have an efﬁciently
computable Jacobian matrix.
Instead  we propose a new algorithm that iteratively constructs incremental transforms that effectively
perform steepest descent on T in RKHS. Our algorithm does not require to explicitly specify
parametric forms  nor to calculate the Jacobian matrix  and has a particularly simple form that
mimics the typical gradient descent algorithm  making it easily implementable even for non-experts
in variational inference.

3.1 Stein Operator as the Derivative of KL Divergence

To explain how we minimize the KL divergence in (4)  we consider an incremental transform formed
by a small perturbation of the identity map: T (x) = x + φ(x)  where φ(x) is a smooth function

3

that characterizes the perturbation direction and the scalar  represents the perturbation magnitude.
When || is sufﬁciently small  the Jacobian of T is full rank (close to the identity matrix)  and hence
T is guaranteed to be an one-to-one map by the inverse function theorem.
The following result  which forms the foundation of our method  draws an insightful connection
between Stein operator and the derivative of KL divergence w.r.t. the perturbation magnitude .
Theorem 3.1. Let T (x) = x + φ(x) and q[T ](z) the density of z = T (x) when x ∼ q(x)  we have
(5)

∇KL(q[T ] || p)(cid:12)(cid:12)=0 = −Ex∼q[trace(Apφ(x))] 

where Apφ(x) = ∇x log p(x)φ(x)(cid:62) + ∇xφ(x) is the Stein operator.
Relating this to the deﬁnition of KSD in (2)  we can identify the φ
direction that gives the steepest descent on the KL divergence in zero-centered balls of Hd.
Lemma 3.2. Assume the conditions in Theorem 3.1. Consider all the perturbation directions φ in
the ball B = {φ ∈ Hd : ||φ||Hd ≤ D(q  p)} of vector-valued RKHS Hd  the direction of steepest
descent that maximizes the negative gradient in (5) is the φ

∗
q p in (3) as the optimal perturbation

∗
q p in (3)  i.e. 
q p(·) = Ex∼q[∇x log p(x)k(x ·) + ∇xk(x ·)] 
∗
φ

for which (5) equals the square of KSD  that is  ∇KL(q[T ] || p)(cid:12)(cid:12)=0 = −D2(q  p).

(6)

The result in Lemma (3.2) suggests an iterative procedure that transforms an initial reference distri-
0(x) = x + 0 · φ
∗
bution q0 to the target distribution p: we start with applying transform T ∗
q0 p(x)
on q0 which decreases the KL divergence by an amount of 0 · D2(q0  p)  where 0 is a small
step size; this would give a new distribution q1(x) = q0[T 0](x)  on which a further transform
1(x) = x + 1 · φ
q1 p(x) can further decrease the KL divergence by 1 · D2(q1  p). Repeating this
∗
T ∗
process one constructs a path of distributions {q(cid:96)}n

(cid:96)=1 between q0 and p via

(cid:96) (x) = x + (cid:96) · φ
∗
T ∗
q(cid:96) p(x).

where

q(cid:96)+1 = q(cid:96)[T ∗
(cid:96) ] 

(7)
This would eventually converge to the target p with sufﬁciently small step-size {(cid:96)}  under which
p q∞(x) ≡ 0 and T ∗
p q∞(x) ≡ 0.
∗
∗
∞ reduces to the identity map. Recall that q∞ = p if and only if φ
φ
Functional Gradient To gain further intuition on this process  we now reinterpret (6) as a functional
gradient in RKHS. For any functional F [f ] of f ∈ Hd  its (functional) gradient ∇f F [f ] is a function
in Hd such that F [f + g(x)] = F [f ] +  (cid:104)∇f F [f ]  g(cid:105)Hd + O(2) for any g ∈ Hd and  ∈ R.
Theorem 3.3. Let T (x) = x + f (x)  where f ∈ Hd  and q[T ] the density of z = T (x) when x ∼ q 

∇f KL(q[T ] || p)(cid:12)(cid:12)f =0 = −φ

∗
q p(x) 

whose RKHS norm is ||φ
q p||Hd = D(q  p).
∗
This suggests that T ∗(x) = x +  · φ
∗
q p(x) is equivalent to a step of functional gradient descent in
RKHS. However  what is critical in the iterative procedure (7) is that we also iteratively apply the
variable transform so that every time we would only need to evaluate the functional gradient descent
at zero perturbation f = 0 on the identity map T (x) = x. This brings a critical advantage since
the gradient at f (cid:54)= 0 is more complex and would require to calculate the inverse Jacobian matrix
[∇xT (x)]−1 that casts computational or implementation hurdles.

3.2 Stein Variational Gradient Descent

To implement the iterative procedure (7) in practice  one would need to approximate the expectation
i}n
q p(x) in (6). To do this  we can ﬁrst draw a set of particles {x0
∗
for calculating φ
i=1 from the initial
distribution q0  and then iteratively update the particles with an empirical version of the transform in
∗
(7) in which the expectation under q(cid:96) in φ
q(cid:96) p is approximated by the empirical mean of particles
{x(cid:96)
i}n
i=1 at the (cid:96)-th iteration. This procedure is summarized in Algorithm 1  which allows us to
(deterministically) transport a set of points to match our target distribution p(x)  effectively providing

4

Algorithm 1 Bayesian Inference via Variational Gradient Descent

Input: A target distribution with density function p(x) and a set of initial particles {x0
Output: A set of particles {xi}n
for iteration (cid:96) do
i ← x(cid:96)
1
x(cid:96)+1
n
where (cid:96) is the step size at the (cid:96)-th iteration.

i=1 that approximates the target distribution p(x).

(cid:2)k(x(cid:96)

i + (cid:96) ˆφ∗(x(cid:96)

j) + ∇x(cid:96)

j  x)∇x(cid:96)

ˆφ∗(x) =

i ) where

n(cid:88)

k(x(cid:96)

j

log p(x(cid:96)

j

j=1

i}n
i=1.

j  x)(cid:3)  (8)

end for

i

√

i=1 h(x(cid:96)

i )/n − Eq(cid:96) [h(x)] = O(1/

guarantee that(cid:80)n

i} to get the empirical measure ˆq(cid:96)+1 of particles {x(cid:96)+1

a sampling method for p(x). We can see that the implementation of this procedure does not depend
on the initial distribution q0 at all  and in practice we can start with a set of arbitrary points {xi}n
i=1 
possibly generated by a complex (randomly or deterministic) black-box procedure.
We can expect that {x(cid:96)
i}n
i=1 forms increasingly better approximation for q(cid:96) as n increases. To
see this  denote by Φ the nonlinear map that takes the measure of q(cid:96) and outputs that of q(cid:96)+1 in
∗
(7)  that is  q(cid:96)+1 = Φ(cid:96)(q(cid:96))  where q(cid:96) enters the map through both q(cid:96)[T ∗
(cid:96) ] and φ
q(cid:96) p. Then  the
updates in Algorithm 1 can be seen as applying the same map Φ on the empirical measure ˆq(cid:96) of
particles {x(cid:96)
} at the next iteration  that is 
ˆq(cid:96)+1 = Φ(cid:96)(ˆq(cid:96)). Since ˆq0 converges to q0 as n increases  ˆq(cid:96) should also converge to q(cid:96) when the
map Φ is “continuous” in a proper sense. Rigorous theoretical results on such convergence have
been established in the mean ﬁeld theory of interacting particle systems [e.g.  15]  which in general
n) for bounded testing functions h. In addition 
the distribution of each particle x(cid:96)
i0  for any ﬁxed i0  also tends to q(cid:96)  and is independent with any
other ﬁnite subset of particles as n → ∞  a phenomenon called propagation of chaos [16]. We leave
concrete theoretical analysis for future work.
Algorithm 1 mimics a gradient dynamics at the particle level  where the two terms in ˆφ∗(x) in (8)
play different roles: the ﬁrst term drives the particles towards the high probability areas of p(x)
by following a smoothed gradient direction  which is the weighted sum of the gradients of all the
points weighted by the kernel function. The second term acts as a repulsive force that prevents
all the points to collapse together into local modes of p(x); to see this  consider the RBF kernel
h (x − xj)k(xj  x)  which drives
k(x  x(cid:48)) = exp(− 1
j
If we let bandwidth h → 0  the
x away from its neighboring points xj that have large k(xj  x).
repulsive term vanishes  and update (8) reduces to a set of independent chains of typical gradient
ascent for maximizing log p(x) (i.e.  MAP) and all the particles would collapse into the local modes.
Another interesting case is when we use only a single particle (n = 1)  in which case Algorithm 1
reduces to a single chain of typical gradient ascent for MAP for any kernel that satisﬁes ∇xk(x  x) = 0
(for which RBF holds). This suggests that our algorithm can generalize well for supervised learning
tasks even with a very small number n of particles  since gradient ascent for MAP (n = 1) has been
shown to be very successful in practice. This property distinguishes our particle method with the
typical Monte Carlo methods that requires to average over many points. The key difference here is
that we use a deterministic repulsive force  other than Monte Carlo randomness  to get diverse points
for distributional approximation.

h||x − x(cid:48)||2)  the second term reduces to(cid:80)

2

settings when p(x) ∝ p0(x)(cid:81) N

Complexity and Efﬁcient Implementation The major computation bottleneck in (8) lies on cal-
culating the gradient ∇x log p(x) for all the points {xi}n
i=1; this is especially the case in big data
k=1p(Dk|x) with a very large N. We can conveniently address this
problem by approximating ∇x log p(x) with subsampled mini-batches Ω ⊂ {1  . . .   N} of the data

∇x log p(x) ≈ log p0(x) +

log p(Dk | x).

(9)

Additional speedup can be obtained by parallelizing the gradient evaluation of the n particles.

The update (8) also requires to compute the kernel matrix {k(xi  xj)} which costs O(cid:0)n2(cid:1); in practice 

this cost can be relatively small compared with the cost of gradient evaluation  since it can be sufﬁcient
to use a relatively small n (e.g.  several hundreds) in practice. If there is a need for very large n  one

(cid:88)

k∈Ω

N
|Ω|

5

can approximate the summation(cid:80)n

expansion of the kernel k(x  x(cid:48)) [17].

i=1 in (8) by subsampling the particles  or using a random feature

4 Related Works

Our work is mostly related to Rezende and Mohamed [13]  which also considers variational inference
over the set of transformed random variables  but focuses on transforms of parametric form T (x) =
f(cid:96)(··· (f1(f0(x)))) where fi(·) is a predeﬁned simple parametric transform and (cid:96) a predeﬁned length;
this essentially creates a feedforward neural network with (cid:96) layers  whose invertibility requires further
conditions on the parameters and needs to be established case by case. The similar idea is also
discussed in Marzouk et al. [14]  which also considers transforms parameterized in special ways
to ensure the invertible and the computational tractability of the Jacobian matrix. Recently  Tran
et al. [18] constructed a variational family that achieves universal approximation based on Gaussian
process (equivalent to a single-layer  inﬁnitely-wide neural network)  which does not have a Jacobian
matrix but needs to calculate the inverse of the kernel matrix of the Gaussian process. Our algorithm
has a simpler form  and does not require to calculate any matrix determinant or inversion. Several
other works also leverage variable transforms in variational inference  but with more limited forms;
examples include afﬁne transforms [19  20]  and recently the copula models that correspond to
element-wise transforms over the individual variables [21  22].
Our algorithm maintains and updates a set of particles  and is of similar style with the Gaussian
mixture variation inference methods whose mean parameters can be treated as a set of particles.
[23–26  5]. Optimizing such mixture KL objectives often requires certain approximation  and this
was done most recently in Gershman et al. [5] by approximating the entropy using Jensen’s inequality
and the expectation term using Taylor approximation. There is also a large set of particle-based Monte
Carlo methods  including variants of sequential Monte Carlo [e.g.  27  28]  as well as a recent particle
mirror descent for optimizing the variational objective function [7]; compared with these methods 
our method does not have the weight degeneration problem  and is much more “particle-efﬁcient” in
that we reduce to MAP with only one single particle.

5 Experiments

h||x − x(cid:48)||2
j k(xi  xj) ≈ n exp(− 1

i=1; this is based on the intuition that we would have(cid:80)

We test our algorithm on both toy and real world examples  on which we ﬁnd our method tends to
outperform a variety of baseline methods. Our code is available at https://github.com/DartML/
Stein-Variational-Gradient-Descent.
For all our experiments  we use RBF kernel k(x  x(cid:48)) = exp(− 1
2)  and take the bandwidth
to be h = med2/ log n  where med is the median of the pairwise distance between the current points
{xi}n
h med2) = 1 
so that for each xi the contribution from its own gradient and the inﬂuence from the other points
balance with each other. Note that in this way  the bandwidth h actually changes adaptively across
the iterations. We use AdaGrad for step size and initialize the particles using the prior distribution
unless otherwise speciﬁed.
Toy Example on 1D Gaussian Mixture We set our target distribution to be p(x) = 1/3N (x; −
2  1) + 2/3N (x; 2  1)  and initialize the particles using q0(x) = N (x;−10  1). This creates a
challenging situation since the probability mass of p(x) and q0(x) are far away each other (with
almost zero overlap). Figure 1 shows how the distribution of the particles (n = 1) of our method
evolve at different iterations. We see that despite the small overlap between q0(x) and p(x)  our
method can push the particles towards the target distribution  and even recover the mode that is further
away from the initial point. We found that other particle based algorithms  such as Dai et al. [7]  tend
to experience weight degeneracy on this toy example due to the ill choice of q0(x).
Figure 2 compares our method with Monte Carlo sampling when using the obtained particles to
estimate expectation Ep(h(x)) with different test functions h(·). We see that the MSE of our method
tends to perform similarly or better than the exact Monte Carlo sampling. This may be because our
particles are more spread out than i.i.d. samples due to the repulsive force  and hence give higher
estimation accuracy. It remains an open question to formally establish the error rate of our method.

6

Figure 1: Toy example with 1D Gaussian mixture. The red dashed lines are the target density function
and the solid green lines are the densities of the particles at different iterations of our algorithm
(estimated using kernel density estimator) . Note that the initial distribution is set to have almost zero
overlap with the target distribution  and our method demonstrates the ability of escaping the local
mode on the left to recover the mode on the left that is further away. We use n = 100 particles.

(a) Estimating E(x)

(b) Estimating E(x2)

(c) Estimating E(cos(ωx + b))

Figure 2: We use the same setting as Figure 1  except varying the number n of particles. (a)-(c)
show the mean square errors when using the obtained particles to estimate expectation Ep(h(x)) for
h(x) = x  x2  and cos(ωx + b); for cos(ωx + b)  we draw ω ∼ N (0  1) and b ∼ Uniform([0  2π])
and report the average MSE over 20 random draws of ω and b.

Bayesian Logistic Regression We consider Bayesian logistic regression for binary classiﬁcation
using the same setting as Gershman et al. [5]  which assigns the regression weights w with a
Gaussian prior p0(w|α) = N (w  α−1) and p0(α) = Gamma(α  1  0.01). The inference is applied
on posterior p(x|D) with x = [w  log α]. We compared our algorithm with the no-U-turn sampler
(NUTS)1 [29] and non-parametric variational inference (NPV)2 [5] on the 8 datasets (N > 500) used
in Gershman et al. [5]  and ﬁnd they tend to give very similar results on these (relatively simple)
datasets; see Appendix for more details.
We further test the binary Covertype dataset3 with 581 012 data points and 54 features. This dataset
is too large  and a stochastic gradient descent is needed for speed. Because NUTS and NPV do
not have mini-batch option in their code  we instead compare with the stochastic gradient Langevin
dynamics (SGLD) by Welling and Teh [2]  the particle mirror descent (PMD) by Dai et al. [7]  and
the doubly stochastic variational inference (DSVI) by Titsias and Lázaro-Gredilla [19].4 We also
compare with a parallel version of SGLD that runs n parallel chains and take the last point of each
chain as the result. This parallel SGLD is similar with our method and we use the same step-size of
(cid:96) = a/(t + 1).55 for both as suggested by Welling and Teh [2] for fair comparison; 5 we select a
using a validation set within the training set. For PMD  we use a step size of a
t)  and
RBF kernel k(x  x(cid:48)) = exp(−||x − x(cid:48)||2/h) with bandwidth h = 0.002 × med2 which is based on
the guidance of Dai et al. [7] which we ﬁnd works most efﬁciently for PMD. Figure 3(a)-(b) shows
the results when we initialize our method and both versions of SGLD using the prior p0(α)p0(w|α);
we ﬁnd that PMD tends to be unstable with this initialization because it generates weights w with
large magnitudes  so we divided the initialized weights by 10 for PMD; as shown in Figure 3(a) 
this gives some advantage to PMD in the initial stage. We ﬁnd our method generally performs the
best  followed with the parallel SGLD  which is much better than its sequential counterpart; this
comparison is of course in favor of parallel SGLD  since each iteration of it requires n = 100 times of
likelihood evaluations compared with sequential SGLD. However  by leveraging the matrix operation
in MATLAB  we ﬁnd that each iteration of parallel SGLD is only 3 times more expensive than
sequential SGLD.

N /(100 +

√

1code: http://www.cs.princeton.edu/ mdhoffma/
2code: http://gershmanlab.webfactional.com/pubs/npv.v1.zip
3https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html
4code: http://www.aueb.gr/users/mtitsias/code/dsvi_matlabv1.zip.
5We scale the gradient of SGLD by a factor of 1/n to make it match with the scale of our gradient in (8).

7

-100100.10.20.30.40th Iteration-100100.10.20.30.450th Iteration-100100.10.20.30.475th Iteration-100100.10.20.30.4100th Iteration-100100.10.20.30.4150th Iteration-100100.10.20.30.4500th IterationSample Size (n)10 50 250Log10 MSE-1.5-1-0.5Sample Size (n)10 50 250Log10 MSE-3-2-10Sample Size (n)10 50 250Log10 MSE-3.5-3-2.5-2Monte CarloStein Variational Gradient Descent(a) Particle size n = 100

(b) Results at 3000 iteration (≈ 0.32 epoches)

Figure 3: Results on Bayesian logistic regression on Covertype dataset w.r.t. epochs and the particle
size n. We use n = 100 particles for our method  parallel SGLD and PMD  and average the last 100
points for the sequential SGLD. The “particle-based” methods (solid lines) in principle require 100
times of likelihood evaluations compare with DVSI and sequential SGLD (dash lines) per iteration 
but are implemented efﬁciently using Matlab matrix operation (e.g.  each iteration of parallel SGLD
is about 3 times slower than sequential SGLD). We partition the data into 80% for training and 20%
for testing and average on 50 random trials. A mini-batch size of 50 is used for all the algorithms.

Bayesian Neural Network We compare our algorithm with the probabilistic back-propagation
(PBP) algorithm by Hernández-Lobato and Adams [30] on Bayesian neural networks. Our experiment
settings are almost identity  except that we use a Gamma(1  0.1) prior for the inverse covariances and
do not use the trick of scaling the input of the output layer. We use neural networks with one hidden
layers  and take 50 hidden units for most datasets  except that we take 100 units for Protein and Year
which are relatively large; all the datasets are randomly partitioned into 90% for training and 10% for
testing  and the results are averaged over 20 random trials  except for Protein and Year on which 5
and 1 trials are repeated  respectively. We use RELU(x) = max(0  x) as the active function  whose
weak derivative is I[x > 0] (Stein’s identity also holds for weak derivatives; see Stein et al. [31]).
PBP is repeated using the default setting of the authors’ code6. For our algorithm  we only use 20
particles  and use AdaGrad with momentum as what is standard in deep learning. The mini-batch
size is 100 except for Year on which we use 1000.
We ﬁnd our algorithm consistently improves over PBP both in terms of the accuracy and speed; this
is encouraging since PBP were speciﬁcally designed for Bayesian neural network. We also ﬁnd that
our results are comparable with the more recent results reported on the same datasets [e.g.  32–34]
which leverage some advanced techniques that we can also beneﬁt from.

Dataset
Boston
Concrete
Energy
Kin8nm
Naval
Combined
Protein
Wine
Yacht
Year

Avg. Test RMSE

Avg. Test LL

PBP

2.977 ± 0.093
5.506 ± 0.103
1.734 ± 0.051
0.098 ± 0.001
0.006 ± 0.000
4.052 ± 0.031
4.623 ± 0.009
0.614 ± 0.008
0.778 ± 0.042
0.778 ± 0.042
0.778 ± 0.042
8.733 ± NA

PBP

Our Method
Our Method
2.957 ± 0.099
−2.504 ± 0.029
2.957 ± 0.099 −2.579 ± 0.052 −2.504 ± 0.029
2.957 ± 0.099
−2.504 ± 0.029
5.324 ± 0.104
−3.082 ± 0.018
5.324 ± 0.104 −3.137 ± 0.021 −3.082 ± 0.018
5.324 ± 0.104
−3.082 ± 0.018
1.374 ± 0.045
−1.767 ± 0.024
1.374 ± 0.045 −1.981 ± 0.028 −1.767 ± 0.024
1.374 ± 0.045
−1.767 ± 0.024
0.090 ± 0.001
0.984 ± 0.008
0.090 ± 0.001
0.090 ± 0.001
0.984 ± 0.008
0.984 ± 0.008
0.004 ± 0.000
4.089 ± 0.012
0.004 ± 0.000
0.004 ± 0.000
4.089 ± 0.012
4.089 ± 0.012
4.033 ± 0.033
−2.815 ± 0.008
4.033 ± 0.033
4.033 ± 0.033 −2.819 ± 0.008 −2.815 ± 0.008
−2.815 ± 0.008
4.606 ± 0.013
−2.947 ± 0.003
4.606 ± 0.013 −2.950 ± 0.002 −2.947 ± 0.003
4.606 ± 0.013
−2.947 ± 0.003
0.609 ± 0.010
−0.925 ± 0.014
0.609 ± 0.010 −0.931 ± 0.014 −0.925 ± 0.014
0.609 ± 0.010
−0.925 ± 0.014
−1.211 ± 0.044
0.864 ± 0.052 −1.211 ± 0.044
−1.211 ± 0.044 −1.225 ± 0.042
8.684 ± NA
−3.580 ± NA
8.684 ± NA
8.684 ± NA
−3.580 ± NA
−3.580 ± NA
−3.586 ± NA

0.901 ± 0.010
3.735 ± 0.004

Avg. Time (Secs)
PBP
18
33
25
118
173
136
682
26
25

Ours
161616
242424
212121
414141
494949
515151
686868
222222
25
684684684

7777

6 Conclusion

We propose a simple general purpose variational inference algorithm for fast and scalable Bayesian
inference. Future directions include more theoretical understanding on our method  more practical
applications in deep learning models  and other potential applications of our basic Theorem in
Section 3.1.

Acknowledgement This work is supported in part by NSF CRII 1565796.

6https://github.com/HIPS/Probabilistic-Backpropagation

8

0.112Number of Epoches0.650.70.75Testing Accuracy1 10 50 250Particle Size (n)0.650.70.75Testing AccuracyStein Variational Gradient Descent (Our Method)Stochastic Langevin (Parallel SGLD)Particle Mirror Descent (PMD)Doubly Stochastic (DSVI)Stochastic Langevin (Sequential SGLD)References
[1] M. D. Hoffman  D. M. Blei  C. Wang  and J. Paisley. Stochastic variational inference. JMLR  2013.
[2] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML  2011.
[3] D. Maclaurin and R. P. Adams. Fireﬂy Monte Carlo: Exact MCMC with subsets of data. In UAI  2014.
[4] R. Ranganath  S. Gerrish  and D. M. Blei. Black box variational inference. In AISTATS  2014.
[5] S. Gershman  M. Hoffman  and D. Blei. Nonparametric variational inference. In ICML  2012.
[6] A. Kucukelbir  R. Ranganath  A. Gelman  and D. Blei. Automatic variational inference in STAN. In NIPS 

2015.

[7] B. Dai  N. He  H. Dai  and L. Song. Provable Bayesian inference via particle mirror descent. In AISTATS 

2016.

[8] Q. Liu  J. D. Lee  and M. I. Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests and model

evaluation. arXiv preprint arXiv:1602.03253  2016.

[9] C. J. Oates  M. Girolami  and N. Chopin. Control functionals for Monte Carlo integration. Journal of the

Royal Statistical Society  Series B  2017.

[10] K. Chwialkowski  H. Strathmann  and A. Gretton. A kernel test of goodness-of-ﬁt. arXiv preprint

arXiv:1602.02964  2016.

[11] J. Gorham and L. Mackey. Measuring sample quality with Stein’s method. In NIPS  pages 226–234  2015.
[12] C. Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media  2008.
[13] D. J. Rezende and S. Mohamed. Variational inference with normalizing ﬂows. In ICML  2015.
[14] Y. Marzouk  T. Moselhy  M. Parno  and A. Spantini. An introduction to sampling via measure transport.

arXiv preprint arXiv:1602.05023  2016.

[15] P. Del Moral. Mean ﬁeld simulation for Monte Carlo integration. CRC Press  2013.
[16] M. Kac. Probability and related topics in physical sciences  volume 1. American Mathematical Soc.  1959.
[17] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  pages 1177–1184 

2007.

[18] D. Tran  R. Ranganath  and D. M. Blei. Variational Gaussian process. In ICLR  2016.
[19] M. Titsias and M. Lázaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In

ICML  pages 1971–1979  2014.

[20] E. Challis and D. Barber. Afﬁne independent variational inference. In NIPS  2012.
[21] S. Han  X. Liao  D. B. Dunson  and L. Carin. Variational Gaussian copula inference. In AISTATS  2016.
[22] D. Tran  D. M. Blei  and E. M. Airoldi. Copula variational inference. In NIPS  2015.
[23] C. M. B. N. Lawrence and T. J. M. I. Jordan. Approximating posterior distributions in belief networks

using mixtures. In NIPS  1998.

[24] T. S. Jaakkola and M. I. Jordon. Improving the mean ﬁeld approximation via the use of mixture distributions.

In Learning in graphical models  pages 163–173. MIT Press  1999.

[25] N. D. Lawrence. Variational inference in probabilistic models. PhD thesis  University of Cambridge  2001.
[26] T. D. Kulkarni  A. Saeedi  and S. Gershman. Variational particle approximations. arXiv preprint

arXiv:1402.5715  2014.

[27] C. Robert and G. Casella. Monte Carlo statistical methods. Springer Science & Business Media  2013.
[28] A. Smith  A. Doucet  N. de Freitas  and N. Gordon. Sequential Monte Carlo methods in practice. Springer

Science & Business Media  2013.

[29] M. D. Hoffman and A. Gelman. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian

Monte Carlo. The Journal of Machine Learning Research  15(1):1593–1623  2014.

[30] J. M. Hernández-Lobato and R. P. Adams. Probabilistic backpropagation for scalable learning of Bayesian

neural networks. In ICML  2015.

[31] C. Stein  P. Diaconis  S. Holmes  G. Reinert  et al. Use of exchangeable pairs in the analysis of simulations.

In Stein’s Method  pages 1–25. Institute of Mathematical Statistics  2004.

[32] Y. Li  J. M. Hernández-Lobato  and R. E. Turner. Stochastic expectation propagation. In NIPS  2015.
[33] Y. Li and R. E. Turner. Variational inference with Renyi divergence. arXiv preprint arXiv:1602.02311 

2016.

[34] Y. Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep

learning. arXiv preprint arXiv:1506.02142  2015.

9

,Aaron van den Oord
Sander Dieleman
Benjamin Schrauwen
Qiang Liu
Dilin Wang
Cedric Josz
Yi Ouyang
Richard Zhang
Javad Lavaei
Somayeh Sojoudi