2019,Teaching Multiple Concepts to a Forgetful Learner,How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model  existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper  we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees.  Our framework is both generic  allowing the design of teaching schedules for different memory models  and also interactive  allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore  for a well-known memory model  we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images.  Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.,Teaching Multiple Concepts to a Forgetful Learner

Anette Hunziker† Yuxin Chen¶ Oisin Mac Aodha§ Manuel Gomez Rodriguez*

Andreas Krause‡

Pietro Perona(cid:63) Yisong Yue(cid:63) Adish Singla*

†University of Zurich  anette.hunziker@gmail.com 
¶University of Chicago  chenyuxin@uchicago.edu 
§University of Edinburgh  oisin.macaodha@ed.ac.uk 

‡ETH Zurich  krausea@ethz.ch 

(cid:63)Caltech  {perona  yyue}@caltech.edu 

*MPI-SWS  {manuelgr  adishs}@mpi-sws.org

Abstract

How can we help a forgetful learner learn multiple concepts within a limited time
frame? While there have been extensive studies in designing optimal schedules for
teaching a single concept given a learner’s memory model  existing approaches for
teaching multiple concepts are typically based on heuristic scheduling techniques
without theoretical guarantees. In this paper  we look at the problem from the
perspective of discrete optimization and introduce a novel algorithmic framework
for teaching multiple concepts with strong performance guarantees. Our framework
is both generic  allowing the design of teaching schedules for different memory
models  and also interactive  allowing the teacher to adapt the schedule to the
underlying forgetting mechanisms of the learner. Furthermore  for a well-known
memory model  we are able to identify a regime of model parameters where our
framework is guaranteed to achieve high performance. We perform extensive eval-
uations using simulations along with real user studies in two concrete applications:
(i) an educational app for online vocabulary teaching; and (ii) an app for teaching
novices how to recognize animal species from images. Our results demonstrate the
effectiveness of our algorithm compared to popular heuristic approaches.

1

Introduction

In many real-world educational applications  human learners often intend to learn more than one
concept. For example  in a language learning scenario  a learner aims to memorize many vocabulary
words from a foreign language. In citizen science projects such as eBird [34] and iNaturalist [38] 
the goal of a learner is to recognize multiple animal species from a given geographic region. As the
number of concepts increases  the learning problem can become very challenging due to the learner’s
limited memory and propensity to forget. It has been well established in the psychology literature that
in the context of human learning  the knowledge of a learner decays rapidly without reconsolidation
[7]. Somewhat analogously  in the sequential machine learning setting  modern machine learning
methods  such as artiﬁcial neural networks  can be drastically disrupted when presented with new
information from different domains  which leads to catastrophic interference and forgetting [19  14].
Therefore  to retain long-term memory (for both human and machine learners)  it is crucial to devise
teaching strategies that adapt to the underlying forgetting mechanisms of the learner.
Teaching forgetful learners requires repetition. Properly scheduled repetitions and reconsolidations of
previous knowledge have proven effective for a wide variety of real-world learning tasks  including
piano [30]  surgery [39  33]  video games [29]  and vocabulary learning [4]  among others. For many
of the above applications  it has been shown that by carefully designing the scheduling policy  one can

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Illustration of our adaptive teaching framework applied to German vocabulary learning  shown here for
six time steps in the learning phase. Each time step proceeds in three stages: (1) the system displays a ﬂashcard
with an image and its English description  (2) the learner inputs the German translation  and (3) the system
provides feedback in the form of the correct answer if the input is incorrect.

achieve substantial gains over simple heuristics (such as spaced repetition at ﬁxed time intervals  or a
simple round robin schedule) [3]. Unfortunately  while there have been extensive (theoretical) results
in teaching a single concept using spaced repetition algorithms  existing approaches for teaching
multiple concepts are typically based on heuristics without theoretical guarantees.
In this paper  we explore the following research question: Given limited time  can we help a forgetful
learner efﬁciently learn multiple concepts in a principled manner? More concretely  we consider an
adaptive setting where at each time step  the teacher needs to pick a concept from a ﬁnite set based on
the learner’s previous responses  and the process iterates until the learner’s time budget is exhausted.
Given a memory model of the learner  what is an optimal teaching curriculum? How should this
sequence be adapted based on the learner’s performance history?

1.1 Overview of our approach

For a high-level overview of our approach  consider the example in Fig. 1  which illustrates one of our
applications on German vocabulary learning [2]. Here  our goal is to teach the learner three German
words in six time steps. One trivial approach could be to show the ﬂashcards in a round robin fashion.
However  the round robin sequence is deterministic and thus not capable of adapting to the learner’s
performance. In contrast  our algorithm outputs an adaptive teaching sequence based on the learner’s
performance.
Our algorithm is based on a novel formulation of the adaptive teaching problem. In §2  we propose
a novel discrete optimization problem  where we seek to maximize a natural surrogate objective
function that characterizes the learner’s expected performance throughout the teaching session.
Note that constructing the optimal teaching policy boils down to solving a stochastic sequence
optimization problem  which is NP-hard in general. In §3  we introduce our greedy algorithm 
and derive performance guarantees based on two intuitive data-dependent properties. While it can
be challenging to compute these performance bounds  we show that for certain learner memory
models  these bounds can be estimated efﬁciently. Furthermore  we identify parameter settings of the
memory models where the greedy algorithm is guaranteed to achieve high performance. Finally  we
demonstrate that our algorithm achieves signiﬁcant improvements over baselines for both simulated
learners (cf. §4) and human learners (cf. §5).

2 The Teaching Model

We now formalize the problem addressed in this paper.

2.1 Problem setup

Suppose that the teacher aims to teach the learner n concepts in a ﬁnite time horizon T . We highlight
the notion of a concept via two concrete examples: (i) when teaching the vocabulary of a foreign
language  each concept corresponds to a word  and (ii) when teaching to recognize different animal
species  each concept corresponds to an animal name. We consider ﬂashcard-based teaching  where
each concept is associated with a ﬂashcard (cf. Fig. 1).

2

132jouetSubmitLearning Phase (1)Answer: Spielzeug x jouetSubmitLearning Phase (3)Answer: Nachtisch xBuchSubmitLearning Phase (4)Answer: Buch ✓ BuchnachsSubmitLearning Phase (5)Answer: Nachtisch x nachsNachtischSubmitLearning Phase (6)Answer: Nachtisch ✓ NachtischSpielzeugSubmitLearning Phase (2)Answer: Spielzeug ✓ SpielzeugWe study the following interactive teaching protocol: At time step t  the teacher picks a concept from
the set {1  . . .   n} and presents its corresponding ﬂashcard to the learner without revealing its correct
answer. The learner then tries to recall the concept. Let us use yt ∈ {0  1} to denote the learner’s
recall at time step t. Here  yt = 1 means that the learner successfully recalls the concept (e.g.  the
learner correctly recognizes the animal species)  and yt = 0 otherwise. After the learner makes an
attempt  the teacher observes the outcome yt and reveals the correct answer.

2.2 Learner’s memory model

Let us use (σ  y) to denote any sequence of concepts and observations. In particular  we use σ1:t
to denote the sequence of concepts picked by the teacher up to time t. Similarly  we use y1:t to
denote the sequence of observations up to time t. Given the history (σ1:t  y1:t)  we are interested
in modeling the learner’s probability to recall concept i at a future time τ ∈ [t + 1  T ]. In general 
the learner’s probability to recall concept i could depend on the history of teaching concept i or
related concepts.1 Formally  we capture the learner’s recall probability for concept i by a memory
model gi (τ  (σ1:t  y1:t)) that depends on the entire history (σ  y). In §3.2  we study an instance of
the learner model captured by exponential forgetting curve (see Eq. (9)).

2.3 The teaching objective

There are several objectives of interest to the teacher  for instance  maximizing the learner’s perfor-
mance in recalling all concepts measured at the end of the teaching session. However  given that the
learning phase might stretch over a long time duration for language learning  another natural objective
is to measure learner’s performance across the entire teaching session. For any given sequence of
concepts and observations (σ1:T   y1:T ) of length T   we consider the following objective:

n(cid:88)

T(cid:88)

f (σ1:T   y1:T ) =

1
nT

i=1

τ =1

gi (τ + 1  (σ1:τ   y1:τ )) .

(1)

Here  gi (·) denotes the recall probability of concept i at τ + 1  given the history up to time step τ.
Concretely  for a given concept i ∈ [n]  our objective function can be interpreted as the (discrete)
area under the learner’s recall curve for concept i across the teaching session.
The teacher’s teaching strategy can be represented as a policy π : (σ  y) → {1  . . .   n}  which maps
any history (i.e.  sequence of concepts selected σ and observations y) to the next concept to be taught.
For a given policy π  we use (σπ
1:T ) to denote a random trajectory from the policy until time T .
The average utility of a policy π is deﬁned as:

1:T   yπ

F (π) = Eσπ yπ [f (σπ

1:T   yπ

1:T )] .

(2)

Given the learner’s memory model for each concept i and the time horizon T   we seek the optimal
teaching policy that achieves the maximal average utility:

π∗ ∈ max

π

F (π) .

(3)

It can be shown that ﬁnding the optimal solution for Eq. (3) is NP-hard (proof is provided in the
supplemental materials).
Theorem 1. Problem (3) is NP-hard  even when the objective function does not depend on the
learner’s responses.

3 Teaching Algorithm and Analysis

We now present a simple  greedy approach for constructing teaching policies. To measure the teaching
progress at time t < T   we introduce the following generalization of objective deﬁned in Eq. (1):

1As an example  for German vocabulary learning  the recall probability for the concept “Apfelsaft" (apple

juice) could depend on the ﬂashcards shown for “Apfelsaft" and “Apfel" (apple).

3

n(cid:88)

T(cid:88)

i=1

τ =1

1
nT

(cid:0)τ + 1 (cid:0)σ1:min(τ t)  y1:min(τ t)

(cid:1)(cid:1) .

gi

f (σ1:t  y1:t) =

(4)

Note that this is equivalent to extending (σ1:t  y1:t) to length T by ﬁlling in the remaining sequence
from t + 1 to T with empty concepts and observations. Given the history (σ1:t−1  y1:t−1)  we deﬁne
the conditional marginal gain of teaching a concept i at time t as:

∆ (i | σ1:t−1  y1:t−1) = Eyt [f (σ1:t−1 ⊕ i  y1:t−1 ⊕ yt)− f (σ1:t−1  y1:t−1)]  

(5)
where ⊕ denotes the concatenation operation  and the expectation is taken over the randomness of
learner’s recall yt  conditioned on the history (σ1:t−1  y1:t−1). The greedy algorithm  as described in
Algorithm 1  iteratively selects the concept that maximizes this conditional marginal gain.
Algorithm 1 Adaptive Teaching Algorithm

Sequence σ ← ∅; observation history y ← ∅
for t = {1  . . .   T} do

Select it ← arg maxi ∆ (i | σ  y)
Show it to the learner; Observe yt
Update σ ← σ ⊕ it  y ← y ⊕ yt

3.1 Theoretical guarantees

We now present a general theoretical framework for analyzing the performance of the adaptive
teaching algorithm (Algorithm 1). Our bound depends on two natural properties of the objective
function f  both related to a notion of diminishing returns of a sequence function. Intuitively  the
following two properties reﬂect how much a greedy choice can affect the optimality of the solution.
Deﬁnition 1 (Online stepwise submodular coefﬁcient). Consider policy π for time T . The online
submodular coefﬁcient of function f with respect to policy π at step t is deﬁned as

where γ(σ  y) = mini (σ(cid:48) y(cid:48)):|σ|+|σ(cid:48)|<T
of any concept i given the current history (σ  y) and the gain of i in any future steps.
Deﬁnition 2 (Online stepwise backward curvature). Consider policy π for time T . The online
backward curvature of function f with respect to policy π at step t is deﬁned as

∆(i|σ⊕σ(cid:48) y⊕y(cid:48)) denotes the minimal ratio between the gain

γ(σπ

1:t  yπ

1:t)

γπ
t = min
σπ
1:t yπ
1:t
∆(i|σ y)

ω(σπ

ωπ

(cid:20)
1 − f (σ⊕σπ(cid:48)

t = max
σπ
1:t yπ
1:t
 y⊕yπ(cid:48)
f (σ y)−f (∅)

)−f (σπ(cid:48)

 yπ(cid:48)

1:t  yπ

1:t)

(cid:21)

(6)

(7)

where ω(σ  y) = maxσπ(cid:48)
second-order difference when considering the current history (σ  y).

 yπ(cid:48)

)

denotes the normalized maximal

Here  γ(σ  y) and ω(σ  y) generalizes the notion of string submodularity and total backward curvature
for sequence functions [43] to the stochastic setting. Intuitively  γ(σ  y) measures the degree of
diminishing returns of a sequence function in terms of the ratio between the conditional marginal gains.
If ∀(σ  y)  γ(σ  y) = 1  then the conditional marginal gain of adding any concept to any subsequent
observation history is non-decreasing. In contrast  ω(σ  y) measures the degree of diminishing returns
in terms of the difference between the marginal gains. As our ﬁrst main theoretical result  we provide
a data-dependent bound on the average utility of the greedy policy against the optimal policy.
Theorem 2. Let πg be the online greedy policy induced by Algorithm 1  and F be the objective
function as deﬁned in Eq. (2). Then for all policies π∗ 

F (πg) ≥ F (π∗)

 

(8)

(cid:32)

T(cid:88)

t=1

γg
T−t
T

(cid:19)(cid:33)

(cid:18)
τ γg
1 − ωg
T

τ

t−1(cid:89)

τ =0

where γg
curvature of f with respect to the policy πg at time step t.

t and ωg

t denote the online stepwise submodular coefﬁcient and online stepwise backward

4

The summand on the R.H.S. of Eq. (8) is in fact a lower bound on the expected one-step gain of
the greedy policy. We can further relax the bound by considering the worst-case online stepwise
submodularity ratio and curvature across all time steps  given by the following corollary.
Corollary 3. Let γg = mint γg

(cid:0)1 − e−γgωg(cid:1) F (π∗) .
of y1:t  Corollary 3 reduces to f (σg ·) ≥ (cid:0)1 − e−1(cid:1) f (σ∗ ·) where σg  σ∗ denote the sequences

Note that Corollary 3 generalizes the string submodular optimization result from [43] to the stochastic
setting. In particular  for the special case where γg = ωg = 1 and f (σ1:t  y1:t) is independent

t . For all π∗  F (πg) ≥ 1

t and ωg = maxt ωg

selected by the greedy and the optimal algorithm. However  constructing the bounds in Theorem 2
t   which is as expensive as computing F (π∗). In
and Corollary 3 requires us to compute γg
the following subsection  we investigate a speciﬁc learning setting  and provide a polynomial time
approximation algorithm for computing the theoretical lower bound in Theorem 2.

t   ωg

ωg

3.2 Analysis for HLR memory model
Here  we consider the setting where the learner’s memory for each concept i ∈ [n] is captured by
an independent HLR memory model. Concepts being independent means that the memory model
gi (τ  (σ1:t  y1:t)) for concept i only depends on the history when ﬂashcards for concept i was shown.2
More speciﬁcally  we consider the case of an HLR memory model with the following exponential
forgetting curve [28]:

− τ−(cid:96)i
hi

 

gi (τ  (σ1:t  y1:t)) = 2

+  ni−  1)  where ni

(9)
where (cid:96)i is the last time concept i was taught  and hi = 2(cid:104)θi ni(cid:105) denotes the half life of the learner’s
recall probability of concept i. Here  θi = (ai  bi  ci) parameterizes the learner’s retention rate 
+ := |{τ(cid:48) ∈ [t] : στ(cid:48) = i ∧ yτ(cid:48) = 1}| and ni− := |{τ(cid:48) ∈ [t] :
and ni = (ni
στ(cid:48) = i ∧ yτ(cid:48) = 0}| denote the number of correct and incorrect recalls of concept i in (σ1:t  y1:t) 
+  bi scales ni−  and ci is an offset that can be considered as
respectively. Intuitively  ai scales ni
scaling time.
We would like to bound the performance of Algorithm 1. While computing γg
t is intractable in
general  we show that one can efﬁciently approximate γg
Theorem 4. Assume that the learner is characterized by the HLR model (Eq. (9)) where ∀i  ai = bi.
We can compute empirical bounds on γt  ωt in polynomial time.

t for the HLR model with ai = bi.

t   ωg

t   ωg

Theorem 4 shows that it is feasible to compute explicit lower bounds on the utility of Algorithm 1
against the maximal achievable utility. The following theorem shows that for certain types of learners 
the algorithm is guaranteed to achieve a high utility.
Theorem 5. Consider the task of teaching n concepts where each concept is following an independent
HLR memory model sharing the same parameters  i.e.  ∀ i  θi = (a  a  0). A sufﬁcient condition for
the algorithm to achieve 1−  utility is a ≥ max
  where the parameter
a essentially captures the learner’s memory strength.

log T  log (3n)   log

(cid:16) 2n2

(cid:17)(cid:111)

(cid:110)

T

Note that Theorem 5 provides a sufﬁcient condition for our algorithm to achieve a high utility. One
interesting open question is to establish an upper bound for the greedy (or the optimal) algorithm
under particular model conﬁgurations  e.g.  to provide a necessary condition for achieving a certain
target utility under the HLR model.

4 Simulations

In this section  we experimentally evaluate our algorithm by simulating learner responses based on a
known memory model. This allows us to inspect the behavior of our algorithm and compare it with
several baseline algorithms in a controlled setting.

2We note that the hardness result of Theorem 1 doesn’t directly apply to this setting with independent
concepts. Nevertheless  the problem setting is still computationally challenging. If we express the optimal
solution using Bellman equations and apply dynamic programming  the number of states will be exponential in
the number of concepts n and polynomial w.r.t. time horizon T .

5

(a) F vs. T (n = 20)

(c) F vs. n (T = 60)

(d) Recall vs. n (T = 60)
Figure 2: Simulation results comparing our algorithm (GR) and three baseline algorithms (RD  RR  and LR).
Two performance metrics are considered: (i) the objective value in Eq. (4) and (ii) recall at the end of the
teaching session denoted at ‘Recall at T + s” with s = 10.

(b) Recall vs. T (n = 20)

4.1 Experimental setup

Dataset We simulated concepts of two different types: “easy” and “difﬁcult”. The learner’s
memory for each concept is captured by an independent HLR model. Concepts of the same type
share the same parameter conﬁgurations. Speciﬁcally  for “easy” concepts the parameters are
θ1 = (a1 = 10  b1 = 5  c1 = 0)  and for “difﬁcult” concepts the parameters are θ2 = (a2 = 3  b2 =
1.5  c2 = 0)  with the following interpretation in terms of recall probabilities. For “easy” concepts 
the recall probability of a concept i drops to gi (τ = 2  (σ1 = i  y1 = 1)) = 2−1/(2a1+c1 ) = 0.99
and gi (τ = 2  (σ1 = i  y1 = 0)) = 2−1/(2b1+c1 ) = 0.98 in the immediate next step after showing
concept i. For “difﬁcult” concepts these probabilities are (0.92  0.78).

Evaluation metric We consider two different criteria when assessing the performance. Our ﬁrst
evaluation metric is the objective value as deﬁned in Eq. (4)  which measures the learner’s average
cumulative recall probability across the entire teaching session. The second evaluation metric is the
learner’s average recall probability at the end of the teaching session. We call this second objective
“Recall at T + s”  where s > 0 denotes how far in the future we choose to evaluate the learner’s recall.

Baselines To demonstrate the performance of our adaptive greedy policy (referred to as GR)  we
consider three baseline algorithms. The ﬁrst baseline  denoted by RD  is a random teacher that
presents a random concept at each time step. The second baseline  denoted by RR  is a round robin
teaching policy that picks concepts according to a ﬁxed round robin schedule  i.e.  iterating through
concepts at each time step. Our third baseline is a variant of the teaching strategy employed by [28] 
which can be considered as a generalization of the popular Leitner and Pimsleur systems [16  25]. At
each time step  the teacher chooses to display the concept with the lowest recall probability according
to the HLR memory model of the learner. We refer to this algorithm as LR.

4.2 Simulation results

We ﬁrst evaluate the performance as a function of the teaching horizon T . In Fig. 2a and Fig. 2b  we
plot the objective value and average recall at T + s for all algorithms over 10 random trials  where
we set s = 10  n = 20 with half easy and half difﬁcult concepts  and vary T ∈ [40  80]. As we can
see from both plots  GR consistently outperforms baselines in all scenarios. The gap between the
performances of GR and the baselines is more signiﬁcant for smaller T . As we increase the time
budget  the performance of all algorithms improves—this behavior is expected  as it corresponds to
the scenario where all concepts get a fair chance of repetition with abundant time budget. In Fig. 2c
and Fig. 2d  we show the performance plot for a ﬁxed teaching horizon of T = 60 when we vary the
number of concepts n ∈ [10  30]. Here we observe a similar behavior as before—GR is consistently
better; as n increases  the gap between the performances of GR and the baselines becomes more
signiﬁcant. Our results suggest that the advantage of GR is most pronounced for more challenging
settings  i.e.  when we have a tight time budget (small T ) or a large number of concepts (large n).

6

406080T0.40.50.60.7ObjectiveGRRRRDLR406080T0.60.70.80.91.0Recall at T+sGRRRRDLR102030n0.40.50.60.70.8ObjectiveGRRRRDLR102030n0.50.60.70.80.91.0Recall at T+sGRRRRDLRGerman

GR
0.572

–

GR
0.143

–

LR
0.487
0.0652

RR
0.462
0.0197

RD
0.467
0.0151

Biodiversity (common)

LR
0.118
0.3111

RR
0.150
0.8478

RD
0.086
0.0047

GR
0.475

–

GR
0.766

–

avg gain
p-value

avg gain
p-value

Biodiversity
RR
0.390

LR
0.411
0.0017 <0.0001 <0.0001

RD
0.251

Biodiversity (rare)
LR
0.668
0.0001 <0.0001 <0.0001

RR
0.601

RD
0.396

Table 1: Summary of the user study results. Here  the performance is measured as the gain in learner’s
performance from prequiz phase to postquiz phase (see main text for details). We have n = 15  T = 40  and ran
algorithms with a total of 80 participants for German app and 320 participants for Biodiversity app.

5 User Study

We have developed online apps for two concrete real-world applications: (i) German vocabulary
teaching [2]  and (ii) teaching novices to recognize animal species from images  motivated by citizen
science projects for biodiversity monitoring [1]. Next  we brieﬂy introduce the datasets used for these
two apps and then present the user study results of teaching human learners.

5.1 Experimental setup

Dataset For the German vocabulary teaching app  we collected 100 English-German word pairs in
the form of ﬂashcards  each associated with a descriptive image. These word pairs were provided by
a language expert (see [8]) and consist of popular vocabulary words taught in an entry-level German
language course. For the biodiversity teaching app  we collected images of 50 animal species. To
extract a ﬁne-grained signal for our user study  we further categorize the Biodiversity dataset into two
difﬁculty levels  namely “common” and “rare”  based on the prevalence of these species. Examples
from both datasets are provided in the supplemental materials.
For real-world experiments  we do not know the learner’s memory model. While it is possible to
ﬁt the HLR model through an extensive pre-study as in [28]  we instead simply choose a ﬁxed set
of parameters. For the Biodiversity dataset  we set the parameters of each concept based on their
difﬁculty level. Namely  we set θ1 = (10  5  0) for “common” (i.e.  easy) species and θ2 = (3  1.5  0)
for “rare” (i.e.  difﬁcult) species  as also used in our simulation. For the German dataset  since
the parameters associated with a concept (i.e.  vocabulary word) depend heavily on learner’s prior
knowledge  we chose a more robust set of parameters for each of the concepts given by θ = (6  2  0).
We defer the details of our sensitivity study of the HLR parameters to the supplemental materials.

Online teaching interface Our apps provide an online teaching interface where a user (i.e.  human
learner) can participate in a “teaching session”. As in the simulations  here each session corresponds
to teaching n concepts (sampled randomly from our dataset) via ﬂashcards over T time steps. We
demonstrate the teaching interface and present the detailed design ideas in the supplemental materials.

5.2 User study results

Results for German We now present the user study results for our German vocabulary teaching
app [2]. We run our candidate algorithms with n = 15  T = 40 on a total of 80 participants (i.e. 
20 per algorithm) recruited from Amazon Mechanical Turk. Results are shown in Table 1. where
we computed the average gain of each algorithm  and performed statistical analysis on the collected
results. The ﬁrst row (avg gain) is obtained by treating the performance for each (participant  word)
pair as a separate sample (e.g.  we get 20 × 15 samples per algorithm for the German app). The
second row (p-value) indicates the statistical signiﬁcance of the results measured by the χ2 tests
[6] (with contingency tables where rows are algorithms and columns are observed outcomes)  when
comparing GR with the baselines. Overall  GR achieved higher gains compared to the baselines.

7

Results for Biodiversity Next  we present the user study results on our Biodiversity teaching
app [1]. We recruited a total of 320 participants (i.e.  80 per algorithm). Here  we used different
parameters for the learner’s memory as described in §5.1; all other conditions (i.e.  n = 15  T = 40 
and interface) were kept the same as for the German app. In Table 1  in addition to the overall
performance of the algorithms across all concepts  we also provide separate statistics on teaching
the “common” and “rare” concepts. Note that  while the performance of GR is close to the baselines
when teaching the “common" species (given the high prequiz score due to learner’s prior knowledge
about these species)  GR is signiﬁcantly more effective in teaching the “rare” species.

Remarks This user study provides a proof-of-concept that the performance of our algorithm GR
demonstrated on simulated learners is consistent with the performance observed on human learners.
While teaching sessions in our current user study were limited to a span of 25 mins with participants
recruited from Mechanical Turk  we expect that the teaching applications we have developed could
be adapted to real-life educational scenarios for conducting long-term studies.

6 Related Work

Spaced repetition and memory models Numerous studies in neurobiology and psychology have
emphasized the importance of the spacing effects in human learning. The spacing effect is the
observation that spaced repetition (i.e.  introducing appropriate time gaps when learning a concept)
produces greater improvements in learning compared to massed repetition (i.e.  “cramming”) [37].
These ﬁndings have inspired many computational models of human memory  including the Adaptive
Character of Thought–Rational model (ACT-R) [24]  the Multiscale Context model (MCM) [22]  and
the Half-life Regression model (HLR) [28]. In particular  HLR is a trainable spaced repetition model 
which can be viewed as a generalization of the popular Leitner [16] and Pimsleur [25] systems. In this
paper  we adopt a variant of HLR to model the learner. One of the key characteristics of these memory
models is the function used to model the forgetting curve. Power-law and exponential functions are
two popular ways of modeling the forgetting curve (for detailed discussion  see [27  41  24  40]).

Optimal scheduling with spaced repetition models
[13] and [17] studied the ACT-R model and
the MCM model respectively for the optimal review scheduling problem where the goal is to maximize
a learner’s retention through an intelligent review scheduler. One of the key differences between their
setting and ours is that  they consider a ﬁxed curriculum of new concepts to teach  and the scheduler
additionally chooses which previous concept(s) to review at each step; whereas our goal is to design
a complete teaching curriculum. Even though the problem settings are somewhat different  we would
like to note that our theoretical framework can be adapted to their setting.
Recently  [26] presented a queuing model for ﬂashcard learning based on the Leitner system and
consider a “mean-recall approximation" heuristic to tractably optimize the review schedule. One
limitation is that their approach does not adapt to the learner’s performance over time. Furthermore 
the authors leave the problem of obtaining guarantees for the original review scheduling problem as
a question for future work. [35] considered optimizing learning schedules in continuous time for a
single concept  and use control theory to derive optimal scheduling to minimize a penalized recall
probability area-under-the-curve loss function. In addition to being discrete time  the key difference
of our setting is that we aim to teach multiple concepts.

Sequence optimization Our theoretical framework is inspired by recent results on sequence sub-
modular function maximization [43  36] and adaptive submodular optimization [10]. In particular 
[43] introduced the notion of string submodular functions  which  analogous to the classical notion of
submodular set functions [15]  enjoy similar performance guarantees for maximization of determinis-
tic sequence functions. Our setting has two key differences in that we focus on the stochastic setting
with potentially non-submodular objective functions. In fact  our theoretical framework (in particular
Corollary 3) generalizes string submodular function maximization to the adaptive setting.

Forgetful learners in machine learning Here  we highlight the differences with some recent work
in the machine learning literature involving forgetful learners. In particular  [44] aimed to teach
the learner a binary classiﬁer by sequentially providing training examples  where the learner has an
exponential decaying memory of the training examples. In contrast  we study a different problem 
where we focus on teaching multiple concepts  and assume that the learner’s memory of each concept

8

decays over time. [14] explored the problem of how to construct a neural network for learning
multiple concepts. Instead of designing the optimal training schedule  their goal is to design a good
learner that suffers less from the forgetting behavior.

Machine teaching Our work is also closely related to machine/algorithmic teaching literature
(e.g.  [46  45  32  9]). Most of these works in machine teaching consider a non-adaptive setting
where the teacher provides a batch of teaching examples at once without any adaptation. In this
paper  we focus primarily on designing interactive teaching algorithms that adaptively select teaching
examples for a learner based on their responses. The problem of adaptive teaching has been studied
recently (e.g.  [12  42  11  5  18  31]). However  these works in machine teaching have not considered
the phenomena of forgetting. [23  21] have studied the problem of concept learning and machine
teaching when learner has “limited-capacity" in terms of retrieving exemplars in memory during
the decision-making process. They model the learner via the Generalized Context Model [20] and
investigated the problem of choosing the optimal exemplars for teaching a classiﬁcation task. In our
setting  the exemplars for each class are already given (in other words  we have only one exemplar
per class)  and we aim at optimally teaching the learner to memorize the (label of) exemplars.

7 Conclusions

We presented an algorithmic framework for teaching multiple concepts to a forgetful learner. We
proposed a novel discrete formulation of teaching based on stochastic sequence function optimization 
and provided a general theoretical framework for deriving performance bounds. We have implemented
teaching apps for two real-world applications. We believe our results have made an important
step towards bringing the theoretical understanding of algorithmic teaching closer to real-world
applications where the forgetting phenomenon is an intrinsic factor.

Acknowledgements

This work was done when Yuxin Chen and Oisin Mac Aodha were at Caltech. This work was
supported in part by NSF Award #1645832  Northrop Grumman  Bloomberg  AWS Research Credits 
Google as part of the Visipedia project  and a Swiss NSF Early Mobility Postdoctoral Fellowship.

References
[1] App-Biodiversity. Website for teaching animal species. https://www.teaching-biodiversity.cc  2018.

[2] App-German. Website for teaching German vocabulary. https://www.teaching-german.cc  2018.

[3] David A Balota  Janet M Duchek  and Jessica M Logan. Is expanded retrieval practice a
superior form of spaced retrieval? A critical review of the extant literature. Psychology Press
New York  NY  2007.

[4] Kristine C Bloom and Thomas J Shuell. Effects of massed and distributed practice on the
learning and retention of second-language vocabulary. The Journal of Educational Research 
74(4):245–248  1981.

[5] Yuxin Chen  Adish Singla  Oisin Mac Aodha  Pietro Perona  and Yisong Yue. Understanding
the role of adaptivity in machine teaching: The case of version space learners. In NeurIPS 
2018.

[6] William G Cochran. The χ2 test of goodness of ﬁt. The Annals of Mathematical Statistics 

pages 315–345  1952.

[7] Hermann Ebbinghaus. Über das gedächtnis: untersuchungen zur experimentellen psychologie.

Duncker & Humblot  1885.

[8] germanwordoftheday. German Word of the Day: Website for learning German vocabulary.

https://germanwordoftheday.de  2018.

[9] Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of Computer

and System Sciences  50(1):20–31  1995.

9

[10] Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active
learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research  42:427–486 
2011.

[11] Luis Haug  Sebastian Tschiatschek  and Adish Singla. Teaching inverse reinforcement learners

via features and demonstrations. In NeurIPS  2018.

[12] Parameswaran Kamalaruban  Rati Devidze  Volkan Cevher  and Adish Singla. Interactive

teaching algorithms for inverse reinforcement learning. In IJCAI  pages 2692–2700  2019.

[13] Mohammad M Khajah  Robert V Lindsey  and Michael C Mozer. Maximizing students’
retention via spaced review: Practical guidance from computational models of memory. Topics
in cognitive science  2014.

[14] James Kirkpatrick  Razvan Pascanu  Neil Rabinowitz  et al. Overcoming catastrophic forgetting

in neural networks. PNAS  114(13):3521–3526  2017.

[15] Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability:

Practical Approaches to Hard Problems. Cambridge University Press  February 2014.

[16] S. Leitner and R. Totter. So lernt man lernen. Angewandte Lernpsychologie ein Weg zum

Erfolg. Herder  1972.

[17] Robert V Lindsey  Jeffery D Shroyer  Harold Pashler  and Michael C Mozer.

Improving
students’ long-term knowledge retention through personalized review. Psychological science 
25(3):639–647  2014.

[18] Weiyang Liu  Bo Dai  Ahmad Humayun  Charlene Tay  Chen Yu  Linda B. Smith  James M.

Rehg  and Le Song. Iterative machine teaching. In ICML  pages 2149–2158  2017.

[19] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:

The sequential learning problem. In Psychology of learning and motivation. 1989.

[20] Robert M Nosofsky. Attention  similarity  and the identiﬁcation–categorization relationship.

Journal of experimental psychology: General  115(1):39  1986.

[21] Robert M Nosofsky  Craig A Sanders  Xiaojin Zhu  and Mark A McDaniel. Model-guided search
for optimal natural-science-category training exemplars: A work in progress. Psychonomic
bulletin & review  26(1):48–76  2019.

[22] Harold Pashler  Nicholas Cepeda  Robert V Lindsey  Ed Vul  and Michael C Mozer. Predicting
the optimal spacing of study: A multiscale context model of memory. In NIPS  pages 1321–1329 
2009.

[23] Kaustubh R Patil  Jerry Zhu  Łukasz Kope´c  and Bradley C Love. Optimal teaching for

limited-capacity human learners. In NIPS  pages 2465–2473  2014.

[24] Philip I Pavlik Jr and John R Anderson. Practice and forgetting effects on vocabulary memory:

An activation-based model of the spacing effect. Cognitive Science  29(4):559–586  2005.

[25] Paul Pimsleur. A memory schedule. The Modern Language Journal  51(2):73–75  1967.

[26] Siddharth Reddy  Igor Labutov  Siddhartha Banerjee  and Thorsten Joachims. Unbounded
human learning: Optimal scheduling for spaced repetition. In KDD  pages 1815–1824  2016.

[27] David C Rubin and Amy E Wenzel. One hundred years of forgetting: A quantitative description

of retention. Psychological review  1996.

[28] Burr Settles and Brendan Meeder. A trainable spaced repetition model for language learning.

In ACL  volume 1  pages 1848–1858  2016.

[29] Wayne L Shebilske  Barry P Goettl  Kip Corrington  and Eric Anthony Day.

Interlesson
spacing and task-related processing during complex skill acquisition. Journal of Experimental
Psychology: Applied  5(4):413  1999.

10

[30] Amy L Simmons. Distributed practice and procedural memory consolidation in musicians’ skill

learning. Journal of Research in Music Education  59(4):357–368  2012.

[31] Adish Singla  Ilija Bogunovic  G Bartók  A Karbasi  and A Krause. On actively teaching the

crowd to classify. In NIPS Workshop on Data Driven Education  2013.

[32] Adish Singla  Ilija Bogunovic  Gábor Bartók  Amin Karbasi  and Andreas Krause. Near-

optimally teaching the crowd to classify. In ICML  pages 154–162  2014.

[33] Edward N Spruit  Guido PH Band  and Jaap F Hamming. Increasing efﬁciency of surgical
training: effects of spacing practice on skill acquisition and retention in laparoscopy training.
Surgical endoscopy  29(8):2235–2243  2015.

[34] Brian L Sullivan  Christopher L Wood  Marshall J Iliff  Rick E Bonney  Daniel Fink  and Steve
Kelling. ebird: A citizen-based bird observation network in the biological sciences. Biological
Conservation  142(10):2282–2292  2009.

[35] Behzad Tabibian  Utkarsh Upadhyay  Abir De  Ali Zarezade  Bernhard Schölkopf  and Manuel
Gomez-Rodriguez. Enhancing human learning via spaced repetition optimization. PNAS 
116(10):3988–3993  2019.

[36] Sebastian Tschiatschek  Adish Singla  and Andreas Krause. Selecting sequences of items via

submodular maximization. In AAAI  2017.

[37] Ovid J Tzeng. Stimulus meaningfulness  encoding variability  and the spacing effect. Journal

of Experimental Psychology  99(2):162–166  1973.

[38] Grant Van Horn  Oisin Mac Aodha  Yang Song  et al. The inaturalist species classiﬁcation and

detection dataset. In CVPR  2018.

[39] EGG Verdaasdonk  LPS Stassen  RPJ Van Wijk  and J Dankelman. The inﬂuence of different
training schedules on the learning of psychomotor skills for endoscopic surgery. Surgical
endoscopy  21(2):214–219  2007.

[40] Matthew M Walsh  Kevin A Gluck  Glenn Gunzelmann  Tiffany Jastrzembski  Michael Krus-
mark  Jay I Myung  Mark A Pitt  and Ran Zhou. Mechanisms underlying the spacing effect in
learning: A comparison of three computational models. Journal of Experimental Psychology:
General  147(9):1325  2018.

[41] Thomas D Wickens. Measuring the time course of retention. 1999.

[42] Teresa Yeo  Parameswaran Kamalaruban  Adish Singla  Arpit Merchant  Thibault Asselborn 
Louis Faucon  Pierre Dillenbourg  and Volkan Cevher. Iterative classroom teaching. In AAAI 
pages 5684–5692  2019.

[43] Zhenliang Zhang  Edwin KP Chong  Ali Pezeshki  and William Moran. String submodular
functions with curvature constraints. IEEE Transactions on Automatic Control  61(3):601–616 
2016.

[44] Yao Zhou  Arun Reddy Nelakurthi  and Jingrui He. Unlearn what you have learned: Adaptive
crowd teaching with exponentially decayed memory learners. In KDD  pages 2817–2826  2018.

[45] Xiaojin Zhu. Machine teaching: An inverse problem to machine learning and an approach

toward optimal education. In AAAI  pages 4083–4087  2015.

[46] Xiaojin Zhu  Adish Singla  Sandra Zilles  and Anna N. Rafferty. An overview of machine

teaching. CoRR  abs/1801.05927  2018.

11

,Anette Hunziker
Yuxin Chen
Oisin Mac Aodha
Manuel Gomez Rodriguez
Andreas Krause
Pietro Perona
Yisong Yue
Adish Singla