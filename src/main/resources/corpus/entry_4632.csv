2019,Regret Bounds for Learning State Representations in Reinforcement Learning,We consider the problem of online reinforcement learning when several state representations (mapping histories to a discrete state space) are available to the learning agent. At least one of these representations is assumed to induce a Markov decision process (MDP)  and the performance of the agent is measured in terms of cumulative regret against the optimal policy giving the highest average reward in this MDP representation. We propose an algorithm (UCB-MS) with O(sqrt(T)) regret in any communicating Markov decision process. The regret bound shows that UCB-MS automatically adapts to the Markov model. This improves over the currently known best results in the literature that gave regret bounds of order O(T^(2/3)).,Regret Bounds for Learning State Representations in

Reinforcement Learning

Ronald Ortner

Montanuniversität Leoben

rortner@unileoben.ac.at

Matteo Pirotta

Facebook AI Research

pirotta@fb.com

Ronan Fruit

Sequel Team – INRIA Lille
ronan.fruit@inria.fr

Alessandro Lazaric
Facebook AI Research

lazaric@fb.com

Odalric-Ambrym Maillard
Sequel Team – INRIA Lille

odalric.maillard@inria.fr

Abstract

We consider the problem of online reinforcement learning when several state
representations (mapping histories to a discrete state space) are available to the
learning agent. At least one of these representations is assumed to induce a Markov
decision process (MDP)  and the performance of the agent is measured in terms of
cumulative regret against the optimal policy giving the highest average reward in
T ) regret
in any communicating MDP. The regret bound shows that UCB-MS automatically
adapts to the Markov model and improves over the currently known best bound of

this MDP representation. We propose an algorithm (UCB-MS) with (cid:101)O(
order (cid:101)O(T 2/3).

√

Introduction

1
In reinforcement learning  an agent aims to learn a task while interacting with an unknown environ-
ment. We consider online learning (i.e.  non-episodic) problems where the agent has to trade off the
exploration needed to collect information about rewards and dynamics and the exploitation of the
information gathered so far. In this setting  it is commonly assumed that the agent knows a suitable
state representation which makes the process on the state space Markovian. However  designing
such a representation is often highly non-trivial since many “reasonable” representations may lead to
non-Markovian models.
The task of selecting or designing a (suitable and compact) state representation of a dynamical
system is a well-known problem in engineering  especially in robotics. This setting has received a
lot of attention in recent years due to the growing number of applications using images or videos as
observations [e.g.  1  2  3]. It is possible to come up with different approaches for extracting features
from such high-dimensional observation spaces  but not all of them describe the problem well or
exhibit Markovian dynamics. Indeed  the Markovian assumption that transitions and rewards are
independent of history is frequently violated in real-world applications where there is often rather
a dependence on the last k > 1 observations. To deal with this scenario Markov models have been
extended from ﬁrst-order models to kth-order models. The problem of selecting the right order of the
model is a special case of the selection of the correct state representation. In both cases  the goal is to
perform as well as when the true order or compact features of the Markov model are known. For
more details and further examples we refer to [4  5  6].
We consider the following setting that was introduced by Hutter [7]  where it was called feature
reinforcement learning. The agent is provided with a ﬁnite set Φ of representations mapping histories
(sequences of actions  observations  and rewards) to a ﬁnite set of states  such that at least one model
φ◦ ∈ Φ induces a Markov decision process (MDP) [8]. The goal of the agent is to learn to solve the

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

task under an appropriate representation. The ability of testing and quickly discarding non-Markovian
representations (not compatible with the observed dynamics) is fundamental for learning efﬁciently.
This efﬁciency is measured in terms of cumulative regret  which compares the reward collected by
the learner to the one of an agent knowing the Markovian representation and playing the associated
optimal policy (i.e.  the policy giving the highest average reward).
This problem was initially studied by Maillard et al. [4]. Given a ﬁnite set of representations Φ  after
T steps the regret of the Best Lower Bound (BLB) algorithm w.r.t. any optimal policy associated

to a Markov model is upper bounded by (cid:101)O((cid:112)|Φ|T 2/3). The BLB algorithm is based on random

exploration of the models and uses properties of UCRL2 [9] —an efﬁcient algorithm for exploration-
exploitation in communicating MDPs— to control the amount of time spent in non-Markovian
models. BLB requires to estimate the diameter [9] of the true MDP  which leads to an additional
additive term in the regret bound that may be exponential in the true diameter. BLB was successively
extended by Nguyen et al. [6] to the case of countably inﬁnite sets of models. The suggested IBLB
algorithm removes the guessing of the diameter —thus avoiding the additional exponential term in
the regret— but its regret bound is still of order T 2/3. For the Optimistic Model Selection (OMS)

algorithm [5] a regret bound of (cid:101)O((cid:112)|Φ|T ) has been claimed that matches the optimal dependence

in terms of T . However  algorithm and analysis were based on the REGAL.D algorithm [10]  and
recently it has been pointed out that the proof of the regret bound for REGAL.D contains a mistake
that invalidates also the result for OMS  see App. A of [11]. Accordingly  it still has been an open
question whether it is possible to achieve regret bounds of order
In this paper we introduce UCB-MS  an optimistic elimination algorithm that performs efﬁcient

exploration of the representations. For this algorithm we prove a regret bound of order (cid:101)O((cid:112)|Φ|T ).

T in the considered setting.

√

Our algorithm as well as our results are based on and generalize the regret bounds achieved for
the UCRL2 algorithm in [9]. In particular  if Φ consists of a single Markov model we obtain the
same regret bound as for UCRL2. UCB-MS employs optimism to choose a model from Φ. To avoid
suffering too large regret from choosing a non-Markov model  the collected rewards are compared to
regret bounds that are known to hold for Markov models. If a model fails to give sufﬁciently high
reward  it is eliminated. On the other hand  UCB-MS is happy to employ a non-Markov model as
long as it gives as much reward as it would be expected from a corresponding Markov model.
While UCB-MS shares some basic ideas with BLB and OMS  it is simpler than OMS  however
recovers the same regret bounds that have been claimed for OMS. As BLB  UCB-MS has to guess the
diameter  however the guessing scheme we employ comes at little cost w.r.t. regret and in particular
does not cause any additive constants in the bounds that are exponential in the diameter. We also
show how to modify the guessing scheme to guess diameter and the size of the state space of the
Markov model φ◦ at the same time. Last but not least  we introduce the notion of the effective size SΦ
of the state space induced by the model set Φ and give regret bounds in terms of SΦ. This yields
improvements depending on the structure of Φ (like for hierarchical models).
Overview. We start with a detailed description of the learning setting in the following section. In
Section 3  we give some preliminaries concerning the UCRL2 algorithm. Our UCB-MS algorithm is
introduced in Section 4 where we also give the regret analysis in case the diameter of the underlying
Markov model is known. The following Section 5 shows how to guess the diameter otherwise.
Section 6 gives some further improvements and also introduces the notion of effective state space.
2 Setting
The details of the considered learning setting are as follows. At each time step t = 1  2  . . .  the
learner receives an initial observation ot and has to choose an action at from a ﬁnite set of actions A.
In return  the learner receives a reward rt taken from R = [0  1] and the next observation ot+1.
We denote by O the set of observations and deﬁne the history ht up to step t as the sequence
o1  a1  r1  o2  . . .   at  rt  ot+1 of observations  actions and rewards. The set Ht := O × (A × R ×
t≥1 Ht to be the set of all

O)t−1 contains all possible histories up to step t and we set H := (cid:83)

possible histories.

2.1 Models and MDPs
A state-representation model (in the following short: model) φ is a function that maps histories to
states  that is  φ : H → Sφ. If a model φ induces a Markov decision process (MDP) we call it a

2

Markov model. An MDP has the Markov property that any time t  the probability of reward rt and
next state st+1 = φ(ht+1)  given the past history ht  only depends on the current state st = φ(ht)
and the chosen action at  i.e.  P (st+1  rt|ht  at) = P (st+1  rt|st  at). We assume that for MDPs this
probability is also independent of t.
Usually  an MDP M with state space S and action space A is denoted as a tuple M = (S A  r  p) 
where r(s  a) is the mean reward and p(s(cid:48)|s  a) the probability of a transition to state s(cid:48) ∈ S when
choosing action a ∈ A in state s ∈ S. MDPs are called communicating if for any two states s  s(cid:48) it is
possible to reach s(cid:48) from s with positive probability by choosing appropriate actions. The smallest
expected time it takes to connect any two states is called the diameter D of the MDP  cf. [9]. In
communicating MDPs  the optimal average reward ρ∗ is independent of the initial state and will be
achieved by a stationary deterministic policy π∗ ∈ ΠSD that maps states to actions.
For a Markov model φ  we write M (φ) for the induced MDP. Its diameter and optimal average reward
will be denoted as D(φ) and ρ∗(φ)  respectively. For all models φ  we abbreviate Sφ := |Sφ|.
2.2 Problem setting
The learning setting we consider is the following. As already described before  the learner chooses
actions at and obtains a reward rt and an observation ot+1 in return. We assume that the learner has
a ﬁnite set Φ of models at her disposal and at least one model φ◦ in Φ is a Markov model. The goal is
to provide algorithms that perform well with respect to the optimal policy π∗ in the MDP M (φ◦) 
that is  the optimal strategy when the Markov model and the induced underlying MDP are completely
known. Accordingly  the performance of a learning algorithm will be measured by considering its
regret after any T steps deﬁned as (cf. [9  10  4])

T ρ∗(φ◦) − T(cid:88)

rt  

t=1

where rt is the reward received by the learning algorithm at step t.
3 UCRL2 Preliminaries
The algorithm we propose is based on the UCRL2 algorithm of [9]. Accordingly  we start with some
respective preliminaries.
UCRL2 is an algorithm that generalizes the optimism in the face of uncertainty idea of UCB [12] from
the bandit setting to reinforcement learning in MDPs. In the following  we assume an underlying
MDP M with S states  A actions  and diameter D. The UCRL2 algorithm uses conﬁdence intervals
to deﬁne a set of plausible MDPs M. That is  acting in the unknown MDP M  UCRL2 maintains
estimates ˆr(s  a) and ˆp(·|s  a) of rewards and transition probabilities  respectively. The set Mt of
satisfying1

plausible MDPs at step t contains all MDPs with rewards(cid:101)r(s  a) and transition probabilities(cid:101)p(·|s  a)

(cid:12)(cid:12)ˆr(s  a) −(cid:101)r(s  a)(cid:12)(cid:12) ≤
(cid:13)(cid:13)ˆp(·|s  a) −(cid:101)p(·|s  a)(cid:13)(cid:13)1 ≤

(cid:114)
(cid:114)

7 log(4SAt3/δ)

2N (s a)

 

14S log(4At3/δ)

2N (s a)

 

(1)

(2)

where N (s  a) denotes the number of times a has been chosen in s (and is set to 1  if a has not been
chosen in s so far). The true MDP M is in M with high probability.
Lemma 1 (Lemma 17 in the appendix of [9]2). With probability at least 1 − δ
MDP M is contained in the set Mt.
algorithm plays a ﬁxed policy(cid:101)πk  which is chosen to maximize the optimal average reward of an
The UCRL2 algorithm proceeds in episodes k = 1  2  . . .. In each episode k starting at step tk the
MDP in Mk := Mtk. That is  writing ρ(π  M ) for the average reward of policy π in MDP M we
1The conﬁdence intervals shown here are the ones we use in the following and slightly differ from the
conﬁdence intervals given for UCRL2 in [9]. That is  the conﬁdence δ of the original values is replaced by δ/2t2
to guarantee smaller error probability  which is needed in our analysis.

30t8   at step t the true

2As noted before  the error probability δ has been changed from δ to δ/2t2 here.

3

Let vk(s  a) denote the number of times a has been chosen in s in episode k  while Nk(s  a) denotes
the number of times a has been chosen in s before episode k (i.e.  in episodes 1 to k − 1). If there
were no visits in (s  a) before episode k  then Nk(s  a) is set to 1. Episode k is terminated by UCRL2

set(cid:101)ρk := maxπ M∈Mk{ρ(π  M )} = ρ((cid:101)πk (cid:102)Mk)  where(cid:102)Mk is an optimistic MDP chosen from Mk
to maximize(cid:101)ρk. As the true MDP M is in Mk with high probability  we also have that(cid:101)ρk ≥ ρ∗.
when a state s is reached in which vk(s (cid:101)πk(s)) = Nk(s (cid:101)πk(s)).
(cid:113)
(cid:1).
AT log(cid:0) 2T 3

One can show the following upper bound on the regret of UCRL2.
Theorem 2 (Theorem 2 of [9]). With probability 1 − δ  the regret of UCRL2 after any T steps is
bounded by

34DS

δ

The bound is based on an episode-wise decomposition of the regret  which we will use for our
algorithm. Let Tk be the (current) length of episode k. In the following  we abuse notation for Tk as
well as for vk(s  a) by using the same notation for the number of steps in a terminated episode as
well as for the current number of steps in an ongoing episode. Further  recall that tk denotes the time
step when episode k starts. The regret of UCRL2 in any episode k is bounded as follows.
Lemma 3. Consider an arbitrary episode k started at step tk. With probability 1 − δ
UCRL2 at each step Tk in this episode is bounded by

  the regret of

2t2
k

(cid:18)

(cid:113)

2D

14S log(cid:0) 4At3

k

(cid:1) + 2

δ

(cid:19)(cid:88)

s a

(cid:113)
Tk log(cid:0) 16t2

kTk
δ

(cid:1) + D.

√

vk(s a)
Nk(s a)

+ 4D

Proof. The bound in Lemma 3 is not explicitly stated for single episodes in [9] but easily follows
from equations (8)  (9)  (15)–(17)  and the argument given before equation (18)  choosing conﬁdence
δ/t2 instead of δ. For the sake of completeness  we give a brief proof sketch.
First  by replacing the random rewards by the mean rewards r(s  a)  the regret ∆k of episode k can
be bounded by (cf. Eq. 8 in [9])

∆k ≤ (cid:88)

vk(s  a)(ρ∗ − r(s  a)) +

(cid:113)

8 Tk log(cid:0) 16t2

5

kTk
δ

(cid:1).

(3)

s a

difference in the sum of (3) can be bounded and split up as

Let ˜r and ˜p denote the rewards and transition probabilities in the optimistic MDP (cid:102)Mk. Then the
term can be written as(cid:101)ρk − ˜r(s  a) =(cid:80)

ρ∗ − r(s  a) ≤ (cid:101)ρk − r(s  a) ≤ ((cid:101)ρk − ˜r(s  a)) + (˜r(s  a) − r(s  a)) 
(cid:17)
(cid:0)˜p(s(cid:48)|s  a) − w(s)

where the last term is controlled by the conﬁdence intervals in (1)  cf. Eq. (15) in [9]. The other
s(cid:48) ˜p(s(cid:48)|s  a)w(s(cid:48)) − w(s) for the shifted value vector w (cf.

(cid:0)˜p(s(cid:48)|s  a) − p(s(cid:48)|s  a)(cid:1)w(s(cid:48)) +

p. 1576 of [9]) so that splitting up again one has (cf. Eq. 16 in [9])

(cid:16)(cid:88)

(cid:88)

.

(5)

(4)

s(cid:48)

s(cid:48)

(cid:113)

The ﬁrst term is handled by the conﬁdence intervals in (2) and the fact the w(s) ≤ D (cf. Eq. 17
in [9])  while the second term can be written as martingale difference sequence and bounded by

(cid:1) + D using Azuma-Hoeffding (cf. Eq. 18 in [9]). Finally taking into account

D
Tk caused by failing conﬁdence intervals (cf. Eq. 9 in [9]) and
an additional regret term of
combining (3)–(5) gives the claimed bound  where the ﬁrst term stems from the conﬁdence intervals
(1) and (2).

√

(cid:101)ρk − ˜r(s  a) =
2 Tk log(cid:0) 16t2

kTk
δ

5

4 The UCB-MS Algorithm
Now let us turn to the state representation learning setting introduced in Section 2. We start with
the simpler case when an upper bound ¯D on the diameter D := D(φ◦) of the Markov model φ◦ is
known (i.e.  ¯D ≥ D). The case when no bound on the diameter is known is dealt with in Section 5.

4

Algorithm 1 UCB-Model Selection (UCB-MS)

Input: set of models Φ  conﬁdence parameter δ ∈ (0  1)  upper bound ¯D on diameter
Initialization: Let t := 1 be the current time step.
for episodes k = 1  2  . . . do

Let tk := t be the initial step of the current episode k  and let Mt φ be the set of plausible
MDPs deﬁned via the conﬁdence intervals (1) and (2) for the estimates so far.

(cid:66) For each φ ∈ Φ  use Extended Value Iteration (EVI) to compute an optimistic MDP (cid:102)Mk φ
in Mt φ and a (near-)optimal policy(cid:101)πk φ on(cid:102)Mk φ with approximate average reward(cid:101)ρk φ.

(cid:66) Choose model φk ∈ Φ such that

(cid:8)(cid:101)ρk φ

(cid:9)  

and set(cid:101)ρk :=(cid:101)ρk φk (cid:101)πk :=(cid:101)πk φk  and Sk := Sφk.

(cid:66) Repeat till termination of the current episode k:

φ∈Φ

φk = argmax

◦ Choose action at := πk(st)  get reward rt and observe next state st+1 ∈ Sk.
◦ Set t := t + 1.
◦ if vk(st  at) = Ntk (st  at) then terminate current episode.
◦ if

(t − tk + 1)(cid:101)ρk − t(cid:88)

τ =tk

rτ > Γt( ¯D)

then set Φ := Φ \ {φk} and terminate current episode.

end for

(8)

(9)

(6)

The UCB-MS algorithm we propose (shown as Alg. 1) basically performs the policy computation
of UCRL2 for each model φ. That is  in episodes k = 1  2  . . .  UCB-MS constructs for each state
representation φ ∈ Φ a set of plausible MDPs Mk φ and computes the optimistic average reward

(cid:101)ρk φ =

argmax

π∈ΠSD M∈Mk φ

{ρ(π  M )}.

This problem can be solved using Extended Value Iteration (EVI) [9] up to an arbitrary accuracy.3

Among all the models  UCB-MS selects the one with highest average reward(cid:101)ρk  cf. Eq. (8). The
associated optimistic policy(cid:101)πk is executed until the number of visits is doubled in at least one
reward(cid:101)ρk. We deﬁne Γt according to Lemma 3 as
(cid:33)(cid:88)

state-action pair (UCRL2 stopping condition) or this policy does not provide sufﬁciently high average
reward (see Eq. 9)  in which case the model φk is eliminated.
The function Γt in Eq. (9) deﬁnes the allowed deviation from the promised optimistic average

(cid:16) 4At3

(cid:16) 16t2

(cid:114)

(cid:114)

(cid:32)

(cid:17)

(cid:17)

k(t)Tk(t)

Γt(D) :=

2D

14Sφt log

k(t)
δ

+ 2

+ 4D

Tk(t) log

δ

+ D 

√

vk(t)(s a)
Nk(t)(s a)

s a

(7)
where k(t) denotes the episode in which step t occurs. In Eq. 9 we exploit the prior knowledge
¯D ≥ D in order to properly deﬁne the condition for model elimination. We will see below in
Section 5 that it is easy to adapt the algorithm to the case of unknown diameter.
If the set Φ consists only of a single Markov model  UCB-MS basically coincides with UCRL with
an additional checking step that will result in discarding the single model only with small probability.
Note that UCB-MS shares the optimistic model selection and the idea of eliminating underachieving
models with OMS  however its structure is much simpler.
Concerning the computational complexity of our algorithm  note that the EVI subroutine we use
for policy computation works just as ordinary value iteration with the same convergence properties
and the same computational complexity with an additional overhead of O(S2A) per iteration step 
cf. [9]. Further  policy computation is performed for each model φ at most |Φ| + SφA log T times  cf.
Lemma 5 (c) below.

√
3As for UCRL2  we set the accuracy in episode k to be 1/

tk.

5

of state space of the largest model and SΣ :=(cid:80)
(cid:112)

Our ﬁrst result is the following regret bound for UCB-MS. Here Smax := maxφ Sφ denotes the size
φ Sφ the size of the total state space over all models.

Theorem 4. With probability 1 − δ  the regret of UCB-MS using ¯D ≥ D is bounded by

SmaxSΣAT log(cid:0) T

(cid:1).

δ

const · ¯D

Note that the bound of Theorem 4 holds for any Markov model in Φ. Thus  in case there is a
Markov model with smaller state space the regret bound shows that UCB-MS automatically adapts
to this preferable model. When Φ consists of a single Markov model we re-establish the bounds
for UCRL2 (however for an algorithm that unlike UCRL2 needs the diameter D as input). Most
importantly  the bound of Theorem 4 improves over the currently best known bound for BLB  which

is of order (cid:101)O(T 2/3). If all models induce a state space of equal size S  the bound in Theorem 4 is
(cid:101)O(DS(cid:112)|Φ|AT )  which also improves over the claimed regret bound of OMS  which is of order
(cid:101)O(DS3/2A(cid:112)|Φ|T ). We note however that in other cases the state space dependence of the OMS

√

bound may be better. In Section 6 below we show how to regain the OMS bound for our algorithm
and how SΣ in the bounds can be replaced by the effective size of the state space  which in some
cases (like for hierarchical models) can be considerably smaller.
A-dependence is optimal as for UCRL2  by using a reﬁned analysis (see [13]) it is also
While the
D-dependence. On the other hand  the optimality in S and |Φ| is
possible to obtain an optimal
still an open question. While the S-dependence can be reduced using Bernstein inequality  we are
not aware of any lower bound for |Φ| in this setting. The closest result we know is for aggregation
techniques with full information where it is possible to obtain bounds of order log(|Φ|). Obviously 
in our setting we have less information and it is not clear if it is possible to obtain logarithmic
dependence.
Note that while the regret is measured w.r.t. the true Markov model φ◦  it is actually not necessary to
identify φ◦ to obtain the regret bound of Theorem 4. As long as a non-Markov model gives at least
the same reward that would be expected from a Markov model there is no need to discard it. Such a
model could be  for example  a good (non-Markovian) approximation.

√

4.1 Analysis (Proof of Theorem 4)
The following lemma collects some basic facts about UCB-MS.
Lemma 5. With probability 1 − δ  all of the following statements hold:
(a) The conﬁdence intervals (1) and (2) of the Markov model φ◦ hold for all time steps t = 1  . . .   T .
(b) No Markov models are discarded in (9).
(c) The number of episodes of UCB-MS is bounded by |Φ| + SΣA log T .

Proof. (a) follows from Lemma 1 by summing over the error probabilities giving a total error

probability of(cid:80)

δ

6.
30t8 < δ

t

6   which proves (b).

For (b)  if UCB-MS chooses a Markov model  then the regret in the respective episode is bounded
according to Lemma 3. The sum over the respective error probabilities δ/2t2
k over all episodes is
bounded by 5δ
If (b) holds  then there are at most |Φ|− 1 episodes in which a model is discarded. For episodes which
are terminated by doubling the number of visits  we can use Proposition 18 of [9]  as the episode
termination criterion of UCB-MS is the same as for UCRL2. Since we have to take into account all
states of all models  the size of the state space to be considered is the sum over the sizes of the state
spaces of the individual models.

The bound on the number of episodes in the worst case depends on SΣ. Under some assumptions on
the given models in Φ (like having hierarchical models) this can be reduced  see Section 6 for details.
Proof of Theorem 4. We assume that the statements of Lemma 5 all hold  which is the case with
probability 1 − δ. Let φ◦ be a Markov model in Φ and consider any episode k. By Lemma 5 (a) 

6

the optimistic estimate (cid:101)ρk φ◦ ≥ ρ∗(φ◦). By the optimism of the algorithm we further have that
(cid:101)ρk ≥(cid:101)ρk φ◦. Hence  the regret ∆k in each episode k is bounded by

∆k := Tk · ρ∗(φ◦) − tk+Tk(cid:88)

rτ ≤ Tk ·(cid:101)ρk − tk+Tk(cid:88)

τ =tk

rτ .

τ =tk

By the deﬁnition of the algorithm  condition (9) does not hold at least up to the ﬁnal step of the
episode  so that we obtain that (as rewards are upper bounded by 1)

∆k ≤ Γtk( ¯D) + 1.

Using the deﬁnition of Γt( ¯D) in (7) and summing over all K episodes  we obtain a regret bound of

(cid:1)(cid:88)
log(cid:0) 16T 3
Tk ≤ √

√

δ

(cid:112)

Tk + K ¯D.

k
KT . Further  as

(cid:88)

∆k ≤ (cid:88)
(cid:18)
Using that(cid:80)

≤

2 ¯D

k

k

for the analysis of UCRL2  we have that (cf. Eq. 20 of [9])

(cid:88)

(cid:19)(cid:88)

(Γtk( ¯D) + 1)

(cid:1) + 2

(cid:113)
(cid:113)
14Smax log(cid:0) 4AT 3
k Tk = T together with Jensen’s inequality  we have(cid:80)
(cid:88)
(cid:88)

vk(s a)
Nk(s a)

+ 4 ¯D

√

s a

k

δ

√

vk(s a)
Nk(s a)

k

s a

k

SΣAT .

2 + 1(cid:1)(cid:112)
≤ (cid:0)√
(cid:113)
SΣAT (log T )(cid:0) log T

(cid:113)

δ

(cid:1) + const2 · ¯D

SmaxSΣAT log(cid:0) T

(cid:1) + const3 · ¯DSΣA log T 

Summarizing  we obtain using the bounds on the number of episodes of Lemma 5 (c) and noting that
|Φ| ≤ SΣ after some simpliﬁcations a regret bound of
const1 · ¯D
which completes the proof of the theorem.
5 Unknown Diameter
If the diameter is unknown we suggest the following guessing scheme. We run UCB-MS with some
initial value ¯D ≥ 1. If at some step all models have been eliminated then double the value of ¯D and
restart the algorithm  that is  start a new episode now considering all models again.
One can show that the regret of this doubling scheme is basically bounded as before unless D is very
large compared to T .
Theorem 6. With probability 1 − δ  the regret of UCB-MS guessing D by doubling is bounded by

δ

(cid:113)(cid:0)SmaxSΣA + |Φ| log D(cid:1)T log(cid:0) T

(cid:1).

δ

const · D

Proof. Let Dk denote the parameter ¯D used in episode k as an estimate for D. As in the proof of
Theorem 4 we have that a Markov model will not be eliminated with high probability once Dk ≥ D.
Hence  in total there cannot be more than (cid:100)|Φ| log2 D(cid:101) episodes that are terminated by discarding a
model.
Let Γt(D) be deﬁned as in (7). Then the same argument as in the proof of Theorem 4 shows that the
regret in each episode k is bounded by Γtk(Dk) + 1.
The rest of the proof can be rewritten from Theorem 4 using that Dk < 2D for all k with high
probability. The only difference is that the bound on the number of episodes has an additional term of
(cid:100)|Φ| log2 D(cid:101)  so that one obtains a regret bound of

(cid:113)

SmaxSΣAT log(cid:0) T

(cid:1) + const2 · D

δ

(cid:114)(cid:16)

const1 · D

(cid:17)

SΣA(log T ) + |Φ| log D

const3 ·(cid:0)DSΣA + |Φ| log D(cid:1) log T.

T log T

δ +

Summarizing the terms gives the claimed bound.

Theorem 6 shows that the cost of the guessing scheme w.r.t. the regret is small and  in particular  does
not result in any additive constant in the bound that is exponential in the diameter (in contrast to the
bound for BLB [4]). Thus  the improvements over OMS discussed after Theorem 4 hold also for
UCB-MS with guessing the diameter.

7

Improving the Bounds

6
In this section  we consider further improvements of our bounds and introduce the notion of the
effective size of the state space for a set of models Φ.

Improving on the Number of Episodes

6.1
The regret bounds we obtain for UCB-MS are basically of the same order as for standard reinforcement
learning in MDPs (i.e. with a given Markov model) as achieved e.g. by [9]. However  the state space
dependence seems not completely satisfactory  as the bounds do not only depend on the state space
size of the Markov model  but on the total state space size SΣ over all models.
The appearance of the parameter SΣ in the bounds is due to the bound on the number of episodes
in Lemma 5 (c). In the worst case  this bound cannot be improved. That is  without any further
assumptions on the way models in Φ aggregate histories one cannot say how visits in a state under
some model φ translate into state visits under some other model φ(cid:48). For example  when under some
model φ all states have been visited so far  the respective histories may be mapped to just a single
state under some other model φ(cid:48). Consequently  one basically has to assume that the states of different
models φ  φ(cid:48) are completely independent of each other  which leads to the bound of Lemma 5 (c).
However  if there is some particular structure on the set of given models Φ  the bound on the number
of episodes can be improved to not depend on the total number of states SΣ.
Deﬁnition 7. Let Φ be a set of state representation models. We deﬁne the effective size SΦ of the
state space of Φ to be the number of states that are sufﬁcient to cover all states under Φ in the sense
that visits in all SΦ covering states induce visits in all other states.

A simple example is when models are hierarchical. That is  there is some model φ in Φ  such that
all other models φ(cid:48) aggregate the states of φ  i.e.  it holds that if φ(h) = φ(h(cid:48)) then φ(cid:48)(h) = φ(cid:48)(h(cid:48))
for all histories h  h(cid:48) in H. In this case  SΦ = Sφ  while SΣ could be of order 2Sφ  as each subset
of Sφ may correspond to an aggregated state in some other model of Φ. Note that when considering
different orders for an MDP  this also results in a hierarchical model set.
In general  we obviously have that SΦ ≤ SΣ and the bound on the number of episodes of Lemma 5 (c)
can be improved to depend on SΦ instead of SΣ (with the same proof).
Lemma 8. The number of episodes of UCB-MS terminated by the doubling criterion is bounded by
SΦA log T .
Accordingly  we can strengthen the results of Theorems 4 and 6 as follows.
Theorem 9. The regret bounds of Theorems 4 and 6 hold with SΣ replaced by SΦ.

Improving Further on the State Space Dependence

6.2
Even after replacing SΣ by SΦ  there is still room for improvement of the bounds with respect to the
size of the state space. In principle  one would like to have a dependence on the size of the state space
of the Markov model φ◦. As we have seen  with the current analysis the dependence on the effective
number of states SΦ is unavoidable. However  we can improve over the second appearing state space
term Smax by guessing the right size of the state space (i.e.  Sφ◦). We distinguish between two cases 
depending on whether a bound on the diameter is known.
6.2.1 Diameter Known
If there is a known bound on the diameter  we can guess the size of the state space by the same
scheme we have suggested for guessing the diameter in Section 5. That is  starting with S := 1 or

S := minφ Sφ we compare the collected rewards to the optimistic average reward(cid:101)ρk of the current

episode k  as before eliminating underachieving models. As comparison term we choose now (in
accordance with the regret bound for UCRL2 in Theorem 2)

(10)

For this guessing scheme one can show the following regret bound.
Theorem 10. With probability 1 − δ  the regret of UCB-MS guessing S by doubling is bounded by

(cid:113)

A(t − tk + 1) log(cid:0) 2t3

(cid:1).
(SΦA log T + |Φ| log Sφ◦ ) AT log(cid:0) T

δ

(cid:1).

δ

Γt(S) := 34DS

(cid:113)

const · DSφ◦

8

Proof. The proof is like that for Theorem 6 only that now S instead of D is guessed and the
comparison term Γt is different. That is  any Markov model φ◦ will not be discarded with high
probability once S ≥ Sφ◦. Therefore  there will be at most (cid:100)|Φ| log2 Sφ◦(cid:101) episodes that are terminated
by discarding a model.
Let Sk be the guess for the size of the state space in episode k. Then  similar to the proofs of
Theorems 4 and 6  the regret in each episode k is bounded by Γtk(Sk) + 1. As Sk ≤ 2Sφ◦ w.h.p. 
summing over all ≤ (cid:100)|Φ| log2 Sφ◦(cid:101) + SΦA log T episodes  Jensen’s inequality gives the claimed
regret bound.

We see that replacing Smax with Sφ◦ comes at a cost of worse dependence on the number of states
and actions  as the summing over episodes in the proof has to be done differently. Still  if Smax is
quite large  the bound of Theorem 10 can be an improvement over the previously presented bounds.
6.2.2 Unknown Diameter
If the diameter is not known  one can do the guessing for both D and S at the same time. More
precisely  in the comparison term one does not guess D and S separately but the factor DS instead.

That is  one starts with setting(cid:103)DS := 1 or some other ﬁxed value like(cid:103)DS := minφ Sφ and deﬁnes

the comparison term as

Γt((cid:103)DS) := 34(cid:103)DS

(cid:113)
A(t − tk + 1) log(cid:0) 2t3

(cid:1).

δ

(11)

This leads to the following regret bound  which basically corresponds to the bound that has been
claimed for OMS  only with SΣ replaced by the potentially smaller SΦ.
Theorem 11. With probability 1 − δ  the regret of UCB-MS guessing both D and S by doubling is
bounded by

(SΦA log T + |Φ| log(DSφ◦ )) AT log(cid:0) T

const · DSφ◦

(cid:113)

(cid:1).

δ

Proof. The proof is like that for Theorem 10. W.h.p. there will be at most (cid:100)|Φ| log2(DSφ◦ )(cid:101) episodes
Γtk ((cid:103)DSk) + 1  where(cid:103)DSk ≤ 2DSφ◦ is the guess for episode k. A sum over the episodes gives the
that are terminated by eliminating a model  while the regret in each episode k is bounded by

claimed bound.

7 Discussion
While we have decided to use UCRL2 as reference algorithm for the deﬁnition of our UCB-MS
strategy  our approach can actually serve as a blueprint for adapting any optimistic algorithm with
known regret bounds to the state representation setting considered in this paper.
In particular 
improved regret bounds (possible w.r.t. the parameters S and D  cf. [9]) for UCRL2 or a variation of
it (such as the recent [13]) automatically result in improved bounds for a corresponding variant of
UCB-MS.
The OMS algorithm [5] employs some form of regularization so that models with large state space
are less appealing. However  this did not avoid the dependence of the claimed bounds of [5] on SΣ.
It is an interesting question whether some improved regularization approach can give bounds that
only depend on Sφ◦. In general  the right dependence of regret bounds on the size of the model set Φ
is also an open problem.
Another question that is still open also for the MDP setting is whether the diameter can be replaced
by the bias span λ∗ of the optimal policy [10  14]. With an upper bound on λ∗  one could replace
UCRL2 by the SCAL algorithm of [14]. However  the guessing scheme we employ for the diameter
does not work for SCAL  as chosen policies may not be optimistic anymore  if the guess for λ∗ is too
small.
Another direction for future research are generalizations to inﬁnite model sets  which for the case of
discrete sets has already been done for the BLB algorithm [6]. Parametric sets of models would be an
interesting next step from there. In this context  it also makes sense to consider approximate Markov
models  that is  the assumption that there is a Markov model is dropped. The results given in [15] for
this setting are also affected by the mentioned error in the proof of the OMS regret bound. We think
that our approach can be adapted  however the details still have to be worked out.

9

Acknowledgments

This work has been supported by the Austrian Science Fund (FWF): I 3437-N33 in the framework of
the CHIST-ERA ERA-NET (DELTA project). Odalric-Ambrym Maillard was supported by CPER
Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020  the French
Ministry of Higher Education and Research  Inria Lille – Nord Europe  CRIStAL  and the French
Agence Nationale de la Recherche  under grant ANR-16-CE40-0002 (project BADASS).

References
[1] Rico Jonschkowski and Oliver Brock. State representation learning in robotics: Using prior

knowledge about physical interaction. In Robotics: Science and Systems  2014.

[2] Timothée Lesort  Natalia Díaz-Rodríguez  Jean-Franois Goudou  and David Filliat. State

representation learning for control: An overview. Neural Networks  2018.

[3] Tim de Bruin  Jens Kober  Karl Tuyls  and Robert Babuska. Integrating state representation
learning into deep reinforcement learning. IEEE Robotics and Automation Letters  3(3):1394–
1401  2018.

[4] Odalric-Ambrym Maillard  Rémi Munos  and Daniil Ryabko. Selecting the state-representation
in reinforcement learning. In Advances Neural Processing Systems 24  NIPS 2011  pages
2627–2635  2012.

[5] Odalric-Ambrym Maillard  Phuong Nguyen  Ronald Ortner  and Daniil Ryabko. Optimal
regret bounds for selecting the state representation in reinforcement learning. In Proceedings
of the 30th International Conference on Machine Learning  ICML 2013  volume 28 of JMLR
Workshop and Conference Proceedings  pages 543 – 551  2013.

[6] Phuong Nguyen  Odalric-Ambrym Maillard  Daniil Ryabko  and Ronald Ortner. Competing
with an inﬁnite set of models in reinforcement learning. In Proceedings of the 16th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics  AISTATS 2013  volume 31 of JMLR
Workshop and Conference Proceedings  pages 463–471  2013.

[7] Marcus Hutter. Feature reinforcement learning: Part I. Unstructured MDPs. J. Artiﬁcial General

Intelligence  1(1):3–24  2009.

[8] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  New York  NY  USA  1994.

[9] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[10] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the 25th Conference on Uncertainty
in Artiﬁcial Intelligence  UAI 2009  pages 25–42  2009.

[11] Ronan Fruit  Matteo Pirotta  and Alessandro Lazaric. Near optimal exploration-exploitation in
non-communicating Markov decision processes. In Advances in Neural Information Processing
Systems 31  NeurIPS 2018  pages 2998–3008  2018.

[12] Peter Auer  Nicolò Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multi-armed

bandit problem. Machine Learning  47:235–256  2002.

[13] Ronan Fruit  Alessandro Lazaric  and Matteo Pirotta. Regret minimization in inﬁnite-horizon
ﬁnite markov decision processes. Tutorial at ALT’19  2019. URL https://rlgammazero.
github.io/.

[14] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. Efﬁcient bias-span-
constrained exploration-exploitation in reinforcement learning. In Proceedings of the 35th
International Conference on Machine Learning  ICML 2018  volume 80 of JMLR Workshop
and Conference Proceedings  pages 1573–1581  2018.

10

[15] Ronald Ortner  Odalric-Ambrym Maillard  and Daniil Ryabko. Selecting near-optimal approxi-
mate state representations in reinforcement learning. In Algorithmic Learning Theory - 25th
International Conference  ALT 2014  pages 140–154  2014.

11

,Ronald Ortner
Matteo Pirotta
Alessandro Lazaric
Ronan Fruit
Odalric-Ambrym Maillard