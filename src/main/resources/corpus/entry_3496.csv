2008,Model selection and velocity estimation using novel priors for motion patterns,Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These findings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy  our theory formulates motion perception at two levels of inference: we first perform model selection between the competing models (e.g. translation  rotation  and expansion) and then estimate the velocity using the selected model. We define novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments.,Model selection and velocity estimation using novel

priors for motion patterns

Shuang Wu

Department of Statistics

UCLA  Los Angeles  CA 90095
shuangw@stat.ucla.edu

Hongjing Lu

Department of Psychology

UCLA  Los Angeles  CA 90095

hongjing@ucla.edu

Alan Yuille

Department of Statistics

UCLA

Los Angeles  CA 90095

yuille@stat.ucla.edu

Abstract

Psychophysical experiments show that humans are better at perceiving rotation
and expansion than translation. These ﬁndings are inconsistent with standard
models of motion integration which predict best performance for translation [6].
To explain this discrepancy  our theory formulates motion perception at two lev-
els of inference: we ﬁrst perform model selection between the competing models
(e.g. translation  rotation  and expansion) and then estimate the velocity using the
selected model. We deﬁne novel prior models for smooth rotation and expansion
using techniques similar to those in the slow-and-smooth model [17] (e.g. Green
functions of differential operators). The theory gives good agreement with the
trends observed in human experiments.

1 Introduction

As an observer moves through the environment  the retinal image changes over time to create mul-
tiple complex motion ﬂows  including translational  circular and radial motion. Human observers
are able to process different motion patterns and infer ego motion and global structure of the world.
However  the inherent ambiguity of local motion signals requires the visual system to employ an ef-
ﬁcient integration strategy to combine many local measurements in order to perceive global motion.
Psychophysical experiments have identiﬁed a variety of phenomena  such as motion capture and
motion cooperativity [11]  which appear to be consequences of such integration. A number of com-
putational Bayesian models have been proposed to explain these effects based on prior assumptions
about motion. In particular  it has been shown that a slow-and-smooth prior  and related models  can
qualitatively account for a range of experimental results [17  15  16] and can quantitatively account
for others [7  12].
However  the integration strategy modeled by the slow-and-smooth prior may not generalize to more
complex motion types  such as circular and radial motion  which are critically important for estimat-
ing ego motion. In this paper we are concerned with two questions. (1) What integration priors
should be used for a particular motion input? (2) How can local motion measurements be combined
with the proper priors to estimate motion ﬂow? Within the framework of Bayesian inference  the
answers to these two questions are respectively based on model selection and parameter estimation.
In the ﬁeld of motion perception  most work has focused on the second question  using parame-
ter estimation to estimate motion ﬂow. However  Stocker and Simoncelli [13] recently proposed a
conditioned Bayesian model in which strong biases in precise motion direction estimates arise as a
consequence of a preceding decision about a particular hypothesis (left vs. right motion).

The goal of this paper is to provide a computational explanation for both of the above questions
using Bayesian inference. To address the ﬁrst question  we develop new prior models for smooth
rotation and expansion motion. To address the second  we propose that the human visual system has
available multiple models of motion integration appropriate for different motion patterns. The visual
system decides the best integration strategy based upon the perceived motion information  and this
choice in turn affects the estimation of motion ﬂow.
In this paper  we ﬁrst present a computational theory in section (3) that includes three different in-
tegration strategies  all derived within the same framework. We test this theory in sections (4 5) by
comparing its predictions with human performance in psychophysical experiments  in which sub-
jects were asked to discriminate motion direction in translational  rotational  and expanding stimuli.
We employ two commonly used stimuli  random dot patterns and moving gratings  to show that the
model can apply to a variety of inputs.

2 Background

There is an enormous literature on visual motion phenomena and there is only room to summarize
the work most relevant to this paper. Our computational model relates most closely to work [17  15 
7] that formulates motion perception as Bayesian inference with a prior probability biasing towards
slow-and-smooth motion. But psychophysical [4  8  1  6]  physiological [14  3] and fMRI data [9]
suggests that humans are sensitive to a variety of motion patterns including translation  rotation  and
expansion. In particular  Lee et al [6] demonstrated that human performance on discrimination tasks
for translation  rotation  and expansion motion was inconsistent with the predictions of the slow-and-
smooth theory (our simulations independently verify this result). Instead  we propose that human
motion perception is performed at two levels of inference: (i) model selection  and (ii) estimating
the velocity with the selected model. The concept of model selection has been described in the
literature  see [5]  but has only recently been applied to model motion phenomena [13]. Our new
motion models for rotation and expansion are formulated very similarly to the original slow-and-
smooth model [17] and similar mathematical analysis [2] is used to obtain the forms of the solutions
in terms of Greens functions of the differential operators used in the priors.

3 Model Formulation

3.1 Bayesian Framework

We formulate motion perception as a problem of Bayesian inference with two parts. The ﬁrst part
selects a model that best explains the observed motion pattern. The second part estimates motion
properties using the selected model.
The velocity ﬁeld {(cid:126)v} is estimated from velocity measurements {(cid:126)u} at discrete positions {(cid:126)ri  i =
1  . . . N} by maximizing

p({(cid:126)v}|{(cid:126)u}  M) = p({(cid:126)u}|{(cid:126)v})p({(cid:126)v}|M)

 

p({(cid:126)u}|M)

The prior

p({(cid:126)v}|M) = exp(−E({(cid:126)v}|M)/T ) 

differs for different models M and is discussed in section 3.2.
The likelihood function

p({(cid:126)u}|{(cid:126)v}) = exp(−E({(cid:126)u}|{(cid:126)v})/T )
depends on the measurement process and is discussed in section 3.3.
The best model that explains measurement {(cid:126)u} is chosen by maximizing the model evidence

(cid:90)

p({(cid:126)u}|M) =

p({(cid:126)u}|{(cid:126)v})p({(cid:126)v}|M)d{(cid:126)v}

which is equivalent to maximizing the posterior probability of the model M (assuming uniform prior
on the models):

M∗ = arg max

M

P (M|{(cid:126)u}) = arg max

M

P ({(cid:126)u}|M)P (M)

P ({(cid:126)u})

= arg max
M

P ({(cid:126)u}|M).

(5)

(1)

(2)

(3)

(4)

3.2 The Priors

We deﬁne three priors corresponding to the three different types of motion – translation  rotation  and
expansion. For each motion type  we encourage slowness and smoothness. The prior for translation
is very similar to the slow-and-smooth prior [17] except we drop the higher-order derivative terms
and introduce an extra parameter (to ensure that all three models have similar degrees of freedom).
We deﬁne the priors by their energy functions E({(cid:126)v}|M)  see equation (2). We label the models by
M ∈ {t  r  e}  where t  r  e denote translation  rotation  and expansion respectively. (We note that
the prior for expansion will also account for contraction).

E({(cid:126)v}|M = t) =

λ(|(cid:126)v|2 + µ|∇(cid:126)v|2 + η|∇2(cid:126)v|2)d(cid:126)r

(6)

1. slow-and-smooth-translation:

(cid:90)

2. slow-and-smooth-rotation:

3. slow-and-smooth-expansion:

(cid:90)
(cid:90)

E({(cid:126)v}|M = r) =

λ{|(cid:126)v|2 + µ[( ∂vx
∂x

)2 + ( ∂vy
∂y

)2 + ( ∂vx
∂y

+ ∂vy
∂x

)2] + η|∇2(cid:126)v|2}d(cid:126)r (7)

E({(cid:126)v}|M = e) =

λ{|(cid:126)v|2 + µ[( ∂vx
∂y

)2 + ( ∂vy
∂x

)2 + ( ∂vx
∂x

− ∂vy
∂y

)2] + η|∇2(cid:126)v|2}d(cid:126)r (8)

{vx = −ω(y − y0)  vy = ω(x − x0)} {vx = e(x − x0)  vy = e(y − y0)

These models are motivated as follows. The |(cid:126)v|2 and |∇2(cid:126)v|2 bias towards slowness and smoothness
and are common to all models. The ﬁrst derivative term gives the differences among the models.
The translation model prefers constant translation motion with (cid:126)v constant  since ∇(cid:126)v = 0 for this
type of motion. The rotation model prefers rigid rotation and expansion  respectively  of ideal form
(9)
where (x0  y0) are the (unknown) centers  ω is the angular speed and e is the expansion rate. These
forms of motion are preferred by the two models since  for the ﬁrst type of motion (rotation) we have
{ ∂vx
∂y = 0} (independent of (x0  y0) and ω). Similarly  the second type of
∂y + ∂vy
∂x = 0}
motion is preferred by the expansion (or contraction) model since { ∂vx
(again independent of (x0  y0) and e).
The translation model is similar to the ﬁrst three terms of the slow-and-smooth energy function
[17] but with a restriction on the set of parameters. Formally λ(|(cid:126)v|2 + σ2
8 |∇2(cid:126)v|2)d(cid:126)r
m!2m|Dm(cid:126)v|2d(cid:126)r. Our computer simulations showed that the translation model performs

≈ λ(cid:80)∞

∂x = 0  ∂vx

∂x = ∂vy

∂y = 0  ∂vx

∂y = ∂vy

2 |∇(cid:126)v|2 + σ4

∂x − ∂vy

σ2m

m=0

similar to the slow-and-smooth model.

3.3 The Likelihood Functions

The likelihood function differs for the two classes of stimuli we examined: (i) For the moving dot
stimuli  as used in [4]  there is enough information to estimate the local velocity (cid:126)u; (ii) For the
gratings stimuli [10]  there is only enough information to estimate one component of the velocity
ﬁeld.
For the dot stimuli  the energy term in the likelihood function is set to be

E({(cid:126)u|(cid:126)v}) =

|(cid:126)v((cid:126)ri) − (cid:126)u((cid:126)ri)|2

N(cid:88)

i=1

N(cid:88)

For the gratings stimuli  see 2  the likelihood function uses the energy function

En({(cid:126)u}|{(cid:126)v}) =

|(cid:126)v((cid:126)ri) · ˆ(cid:126)u((cid:126)ri) − |(cid:126)u((cid:126)ri)||2

where ˆ(cid:126)u((cid:126)ri) is the unit vector in the direction of (cid:126)u((cid:126)ri) and normally it is the direction of local image
gradient.

i=1

(10)

(11)

3.4 MAP estimator of velocities

The MAP estimate of the velocities for each model is obtained by solving

(cid:126)v∗ = arg max

p({(cid:126)v}|{(cid:126)u}  M) = arg min

{E({(cid:126)u|(cid:126)v}) + E({(cid:126)v}|M)}

(cid:126)v

(cid:126)v

(12)

For the slow-and-smooth model [17]  it was shown using regularization analysis [2] that this solution
can be expressed in terms of a linear combination of the Green function G of the differential operator
which imposes the slow-and-smoothness constraint (the precise form of this constraint was chosen
so that G was a Gaussian).
We can obtain similar results for the three types of models M ∈ {t  r  e} we have introduced in this
paper. The main difference is that the models require two vector valued Green functions (cid:126)GM
1 =
(GM
1y. These
vector-valued Green functions are required to perform the coupling between the different velocity
component required for rotation and expansion  see ﬁgure (1). For the translation model there is no
coupling required and so GM

2y)  with the constraint that GM

1y) and (cid:126)GM

2y and GM

2 = (GM

2x = GM

1x = GM

1x  GM

2x  GM

2x = GM

1y = 0.

1x

  GM =e

Figure 1: The vector-valued Green function (cid:126)G = (G1  G2).
GM =t
  GM =r
1x
right: GM =t
are similar for all models  GM =t
ity components)  and GM =r
of rotation and expansion. Recall that GM

left-to-right:
left-to
for translation  rotation  and expansion models. Observe that the GM
1x
vanishes for the translation model (i.e. no coupling between veloc-
2x
both have two peaks which correspond to the two directions
and GM =e
1y = GM

for the translation  rotation and expansion models. Bottom panel:
2x

2x and GM

2y = GM
1x.

Top panel 

1x
  GM =r

  GM =e

2x

2x

2x

2x

The estimated velocity for the M model is of the form:

(cid:126)v((cid:126)r) =

[αi (cid:126)GM

1 ((cid:126)r − (cid:126)ri) + βi (cid:126)GM

2 ((cid:126)r − (cid:126)ri)] 

(13)

N(cid:88)

i=1

N(cid:88)

For the dot stimuli  the {α} {β} are obtained by solving the linear equations:

j=1

[αj (cid:126)GM

1 ((cid:126)ri − (cid:126)rj) + βj (cid:126)GM

2 ((cid:126)ri − (cid:126)rj)] + αi(cid:126)e1 + βi(cid:126)e2 = (cid:126)u(ri)  i = 1  . . . N 

(14)
where (cid:126)e1  (cid:126)e2 denote the (orthogonal) coordinate axes. If we express the {α} {β} as two N-dim
vectors A and B  the {ux} and {uy} as vectors U = (Ux  Uy)T   and deﬁne N × N matrices
2y((cid:126)ri − (cid:126)rj) re-
1x((cid:126)ri − (cid:126)rj)  GM
1x  gM
gM
spectively  then we can express these linear equations as:

2y to have components GM

2x  gM

1y   gM

(cid:18) gM
(cid:18) ˜gM

1x + I
gM
1y

1x + I
˜gM
1y

(cid:19)(cid:18) A
(cid:19)(cid:18) A

1y((cid:126)ri − (cid:126)rj)  GM
2x((cid:126)ri − (cid:126)rj)  GM
(cid:19)
(cid:19)
(cid:19)
(cid:19)

(cid:18) Ux
(cid:18) Ux

Uy

=

B

=

B

Uy

gM
2x
2y + I
gM

˜gM
2x
˜gM
2y + I

(15)

(16)

Similarly for the gratings stimuli 

1x is the matrix with components ˜GM

1x((cid:126)ri − (cid:126)rj) = [ (cid:126)GM

1 ((cid:126)ri − (cid:126)rj) · ˆ(cid:126)u(ri)]ˆ(cid:126)ux(ri)  and

in which ˜gM
similarly for ˜gM

1y  ˜gM

2x and ˜gM
2y.

3.5 Model Selection
We re-express model evidence p({(cid:126)u}|M) in terms of (A  B):

(cid:90)

p({(cid:126)u}|M) =

p({(cid:126)u}|A  B  M)p(A  B)dAdB

(17)

(cid:18) gM

(cid:19)

We introduce new notation in the form of 2N × 2N matrices: gM =
˜gM .
The model evidence for the dot stimuli can be computed analytically (exploiting properties of multi-
dimensional Gaussians) to obtain:

  similarly for

gM
2x
gM
2y

1x
gM
1y

p({(cid:126)u}|M) =

exp[− 1
T

(U T U − U T

gM

gM + I

U)]

(18)

Similarly  for the gratings stimuli we obtain:

p({(cid:126)u}|M) =

(U T U − U T ˜gM ˜Σ−1(˜gM )T U)]

(19)

1

(πT )N(cid:112)det(gM + I)
(cid:112)det(gM )
(cid:113)

1

det(˜Σ)

(πT )N

exp[− 1
T

where ˜Σ = (˜gM )T ˜gM + gM .

4 Results on random dot motion

We ﬁrst investigate motion perception with the moving dots stimuli used by Freeman and Harris
[4]  as shown in ﬁgure (2). The stimuli consist of 128 moving dots in a random spatial pattern.
All the dots have the same speed in all three motion patterns  including translation  rotation and
expansion. Our simulations ﬁrst select the correct model for each stimulus and then estimate the
speed threshold of detection for each type of motion. The parameter values used are λ = 0.001 
µ = 12.5  η = 78.125 and T = 0.0054.

Figure 2: Moving random dot stimuli. Left panel: translation; middle panel: rotation; right panel:
expansion.

4.1 Model selection

Model selection results are shown in ﬁgure (3). As speed increases in the range of 0.05 to 0.1  model
evidence decreases for all models. This is due to slowness term in all model priors. Nevertheless the
correct model is always selected over the entire range of speed  and for all 3 type of motion stimuli.

−3−2−10123−2.5−2−1.5−1−0.500.511.522.5−2.5−2−1.5−1−0.500.511.522.5−2.5−2−1.5−1−0.500.511.522.5−3−2−10123−3−2−10123Figure 3: Model selection results with random dot motion. Plots the log probability of the model as
a function of speed for each type of stimuli. left: translation stimuli; middle: rotation stimuli; right:
expansion stimuli. Green curves with cross are from translation model. Red curves with circles are
from rotation model. Blue curves with squares are from expansion model.

4.2 Speed threshold of Detection

As reported in [4]  humans have lower speed threshold in detecting rotation/expansion than trans-
lation motion. The experiment is formulated as a model selection task with an additional “static”
motion prior. The “static” motion prior is modeled as a translation prior with µ = 0 and λ sig-
niﬁcantly large to emphasize slowness. In the simulation  λ = 0.3 for this “static” model  while
λ = 0.001 for all other models.
At low speed  the “static” model is favored due to its stronger bias towards slowness  as stimulus
speed increases  it loses its advantage to other models. The speed thresholds of detection for different
motion patterns can be seen from the model evidence plots in ﬁgure (4)  and they are lower for
rotation/expansion than translation. The threshold values are about 0.05 for rotation and expansion
and 0.1 for translation. This is consistent with experimental result in [4].

Figure 4: Speed threshold of detection. Upper left panel: model evidence plot for translation stimuli.
Upper right panel: model evidence plot for rotation stimuli. Lower left panel: model eviddence plot
for expansion stimuli. Lower right panel: bar graph of speed thresholds.

0.050.060.070.080.090.1482.75482.8482.85482.9speedlog(P(u)) rotation modelexpansion modeltranslation model0.050.060.070.080.090.1477478479480481482speedlog(P(u)) rotation modelexpansion modeltranslation model0.050.060.070.080.090.1477478479480481482speedlog(P(u)) rotation modelexpansion modeltranslation model0.10.1020.1040.1060.1080.11478480482484486488Speedlog(P(u)) rotation modelexpansion modeltranslation modelstatic model0.05020.05040.05060.0508480.2480.4480.6480.8481481.2481.4481.6Speedlog(P(u)) rotation modelexpansion modeltranslation modelstatic model0.05020.05040.05060.0508480480.5481481.5Speedlog(P(u)) rotation modelexpansion modeltranslation modelstatic modeltranslationrotationexpansion00.020.040.060.080.10.12Speed threshold5 Results on randomly oriented gratings

5.1 Stimuli

When randomly oriented grating elements drift behind apertures  the perceived direction of motion
is heavily biased by the orientation of the gratings  as well as by the shape and contrast of the aper-
tures. Recently  Nishida and his colleagues developed a novel global motion stimulus consisting of
a number of gratings elements  each with randomly assigned orientation [10]. A coherent motion
is perceived when the drifting velocities of all elements are consistent with a given velocity. Ex-
amples of the stimuli used in these psychophysical experiments are shown in left side of ﬁgure (6).
The stimuli consisted of 728 gratings (drifting sine-wave gratings windowed by stationary Gaus-
sians). The orientations of the gratings were randomly assigned  and their drifting velocities were
determined by a speciﬁed global motion ﬂow pattern. The motions of signal grating elements were
consistent with global motion  but the motions of noise grating elements were randomized. The
task was to identify the global motion direction as one of two alternatives: left/right for translation 
clockwise/counterclockwise for rotation  and inward/outward for expansion. Motion sensitivity was
measured by the coherence threshold  deﬁned as the proportion of signal elements that yielded a
performance level of 75% correct.
Similar stimuli with 328 gratings were generated to test our computational models. The input for
the models is the velocity component perpendicular to the assigned orientation for each grating  as
illustrated in the upper two panels of ﬁgure (5).

Figure 5: Randomly-oriented grating stimuli and estimated motion ﬂow. Upper left panel: rotation
stimulus (with 75% coherence ratio). Upper right panel: expansion stimulus (with 75% coherence
ratio). Lower left panel: motion ﬂow estimated from stimulus in ﬁrst panel with rotation model.
Lower right panel: motion ﬂow estimated from stimulus in second panel with expansion model.

5.2 Result

The results of psychophysical experiments (middle panel of ﬁgure 6) showed worse performance
for perceiving translation than rotation/expansion motion [6]. Clearly  as shown in the third panel
of the same ﬁgure  the model performs best for rotation and expansion  and is worst for translation.
This ﬁnding agrees with human performance in psychophysical experiments.

6 Conclusion

Humans motion sensitivities depend on the motion patterns (translation/rotation/expansion). We
propose a computational model in which different prior motions compete to ﬁt the data by levels

−15−10−5051015−15−10−5051015−15−10−5051015−15−10−5051015−15−10−5051015−15−10−5051015−15−10−5051015−15−10−5051015Figure 6: Stimulus and results. Left panel: illustration of grating stimulus. Blue arrows indicate the
drifting velocity of each grating. Middle panel: human coherence thresholds for different motion
stimuli. Right panel: Model prediction of coherence thresholds which are consistent with human
trends.

of inference. This analysis involves formulating two new prior models for rotation and expansion
model and deriving their properties. This competitive prior approach gives good ﬁts to the empirical
data and accounts for the dominant trends reported in [4  6].
Our current work aims to extend these ﬁndings to a range of different motions (e.g. afﬁne motion)
and to use increasingly naturalistic appearance/intensity models. It is also important to determine
if motion patterns to which humans are sensitive correspond to those appearing regularly in natural
motion sequences.

References
[1] J.F. Barraza and N.M. Grzywacz. Measurement of angular velocity in the perception of rotation. Vision Research  42.2002.
[2] J. Duchon. Lecture Notes in Math. 571  (eds Schempp  W. and Zeller  K.) 85-100. Springer-Verlag  Berlin  1979.
[3] C. J. Duffy  and R. H. Wurtz. Sensitivity of MST neurons to optic ﬂow stimuli. I. A continuum of response selectivity to large ﬁeld

stimuli. Journal of Neurophysiology. 65  1329-1345. 1991.

[4] T. Freeman  and M. Harris. Human sensitivity to expanding and rotating motion: effect of complementary masking and directional

structure. Vision research  32  1992.

[5] D. Knill and W. Richards (Eds). Perception as Bayesian Inference. Cambridge University Press  1996.
[6] A. Lee  A. Yuille  and H. Lu. Superior perception of circular/radial than translational motion cannot be explained by generic priors. VSS

2008.

[7] H. Lu and A.L. Yuille. Ideal Observers for Detecting Motion: Correspondence Noise. NIPS 2005.
[8] M. C. Morrone  D. C. Burr  and L. Vaina. Two stages of visual processing for radial and circular motion. Nature  376  507-509. 1995.
[9] M. Morrone  M. Tosetti  D. Montanaro  A. Fiorentini  G. Cioni  and D. C. Burr. A cortical area that responds speciﬁcally to optic ﬂow

revealed by fMRI. Nature Neuroscience  3  1322 -1328. 2000.

[10] S. Nishida  K. Amano  M. Edwards  and D.R. Badcock. Global motion with multiple Gabors - A tool to investigate motion integration

across orientation and space. VSS 2006.

[11] R. Sekuler  S.N.J. Watamaniuk and R. Blake. Perception of Visual Motion. In Steven’s Handbook of Experimental Psychology. Third

edition. H. Pashler  series editor. S. Yantis  volume editor. J. Wiley Publishers. New York. 2002.

[12] A.A. Stocker and E.P. Simoncelli. Noise characteristics and prior expectations in human visual speed perception Nature Neuroscience 

vol. 9(4)  pp. 578–585  Apr 2006.

[13] A.A. Stocker  and E. Simoncelli. A Bayesian model of conditioned perception. Proceedings of Neural Information Processing Systems.

2007.

[14] K. Tanaka  Y. Fukada  and H. Saito. Underlying mechanisms of the response speciﬁcity of expansion/contraction and rotation cells in the

dorsal part of the MST area of the macaque monkey. Journal of Neurophysiology. 62  642-656. 1989.

[15] Y. Weiss  and E.H. Adelson. Slow and smooth: A Bayesian theory for the combination of local motion signals in human vision Technical

Report 1624. Massachusetts Institute of Technology. 1998.

[16] Y. Weiss  E.P. Simoncelli  and E.H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience  5  598-604. 2002.
[17] A.L. Yuille and N.M. Grzywacz. A computational theory for the perception of coherent visual motion. Nature  333 71-74. 1988.

translationrotationexpansion00.10.20.30.40.5HumanCoherence Ratio Thresholdtranslationrotationexpansion00.050.10.150.20.25ModelCoherence Ratio Threshold,Qiang Liu
Alexander Ihler
Mark Steyvers
Avrim Blum
Nika Haghtalab
Ariel Procaccia
Soroosh Shafieezadeh Abadeh
Peyman Mohajerin Esfahani
Daniel Kuhn