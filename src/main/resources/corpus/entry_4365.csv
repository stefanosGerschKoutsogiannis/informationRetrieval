2015,Precision-Recall-Gain Curves: PR Analysis Done Right,Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracy-based performance assessment  many researchers have taken to report Precision-Recall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties  mainly because of incoherent scale assumptions -- e.g.  the area under a PR curve takes the arithmetic mean of precision values whereas the $F_{\beta}$ score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system  and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular  the area under Precision-Recall-Gain curves conveys an expected $F_1$ score on a harmonic scale  and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine  for each operating point on the convex hull  the interval of $\beta$ values for which the point optimises $F_{\beta}$. We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected $F_1$ score than others  and so the use of Precision-Recall-Gain curves will result in better model selection.,Precision-Recall-Gain Curves:

PR Analysis Done Right

Peter A. Flach

Intelligent Systems Laboratory

University of Bristol  United Kingdom
Peter.Flach@bristol.ac.uk

Meelis Kull

Intelligent Systems Laboratory

University of Bristol  United Kingdom
Meelis.Kull@bristol.ac.uk

Abstract

Precision-Recall analysis abounds in applications of binary classiﬁcation where
true negatives do not add value and hence should not affect assessment of the
classiﬁer’s performance. Perhaps inspired by the many advantages of receiver op-
erating characteristic (ROC) curves and the area under such curves for accuracy-
based performance assessment  many researchers have taken to report Precision-
Recall (PR) curves and associated areas as performance metric. We demonstrate
in this paper that this practice is fraught with difﬁculties  mainly because of in-
coherent scale assumptions – e.g.  the area under a PR curve takes the arithmetic
mean of precision values whereas the Fβ score applies the harmonic mean. We
show how to ﬁx this by plotting PR curves in a different coordinate system  and
demonstrate that the new Precision-Recall-Gain curves inherit all key advantages
of ROC curves. In particular  the area under Precision-Recall-Gain curves con-
veys an expected F1 score on a harmonic scale  and the convex hull of a Precision-
Recall-Gain curve allows us to calibrate the classiﬁer’s scores so as to determine 
for each operating point on the convex hull  the interval of β values for which the
point optimises Fβ . We demonstrate experimentally that the area under traditional
PR curves can easily favour models with lower expected F1 score than others  and
so the use of Precision-Recall-Gain curves will result in better model selection.

1

Introduction and Motivation

In machine learning and related areas we often need to optimise multiple performance measures 
such as per-class classiﬁcation accuracies  precision and recall in information retrieval  etc. We then
have the option to ﬁx a particular way to trade off these performance measures: e.g.  we can use
overall classiﬁcation accuracy which gives equal weight to correctly classiﬁed instances regardless
of their class; or we can use the F1 score which takes the harmonic mean of precision and recall.
However  multi-objective optimisation suggests that to delay ﬁxing a trade-off for as long as possible
has practical beneﬁts  such as the ability to adapt a model or set of models to changing operating
contexts. The latter is essentially what receiver operating characteristic (ROC) curves do for bi-
nary classiﬁcation. In an ROC plot we plot true positive rate (the proportion of correctly classiﬁed
positives  also denoted tpr) on the y-axis against false positive rate (the proportion of incorrectly
classiﬁed negatives  also denoted fpr) on the x-axis. A categorical classiﬁer evaluated on a test set
gives rise to a single ROC point  while a classiﬁer which outputs scores (henceforth called a model)
can generate a set of points (commonly referred to as the ROC curve) by varying the decision thresh-
old (Figure 1 (left)).
ROC curves are widely used in machine learning and their main properties are well understood [3].
These properties can be summarised as follows.

1

Figure 1: (left) ROC curve with non-dominated points (red circles) and convex hull (red dotted line).
(right) Corresponding Precision-Recall curve with non-dominated points (red circles).

Universal baselines: the major diagonal of an ROC plot depicts the line of random performance
which can be achieved without training. More speciﬁcally  a random classiﬁer assigning the
positive class with probability p and the negative class with probability 1− p has expected
true positive rate of p and true negative rate of 1− p  represented by the ROC point (p  p).
The upper-left (lower-right) triangle of ROC plots hence denotes better (worse) than ran-
dom performance. Related baselines include the always-negative and always-positive clas-
siﬁer which occupy ﬁxed points in ROC plots (the origin and the upper right-hand corner 
respectively). These baselines are universal as they don’t depend on the class distribution.
Linear interpolation: any point on a straight line between two points representing the performance
of two classiﬁers (or thresholds) A and B can be achieved by making a suitably biased ran-
dom choice between A and B [14]. Effectively this creates an interpolated contingency
table which is a linear combination of the contingency tables of A and B  and since all
three tables involve the same numbers of positives and negatives it follows that the inter-
polated accuracy as well as true and false positive rates are also linear combinations of
the corresponding quantities pertaining to A and B. The slope of the connecting line de-
termines the trade-off between the classes under which any linear combination of A and
B would yield equivalent performance. In particular  test set accuracy assuming uniform
misclassiﬁcation costs is represented by accuracy isometrics with slope (1− π)/π  where
π is the proportion of positives [5].

Optimality: a point D dominates another point E if D’s tpr and fpr are not worse than E’s and
at least one of them is strictly better. The set of non-dominated points – the Pareto front
– establishes the set of classiﬁers or thresholds that are optimal under some trade-off be-
tween the classes. Due to linearity any interpolation between non-dominated points is both
achievable and non-dominated  giving rise to the convex hull (ROCCH) which can be easily
constructed both algorithmically and by visual inspection.

Area: the proportion of the unit square which falls under an ROC curve (AUROC) has a well-known
meaning as a ranking performance measure: it estimates the probability that a randomly
(cid:82) 1
chosen positive is ranked higher by the model than a randomly chosen negative [7]. More
importantly in a classiﬁcation context  there is a linear relationship between AUROC =
0 tpr d fpr and the expected accuracy acc = πtpr + (1 − π)(1 − fpr) averaged over all
change of variable: E [acc] =(cid:82) 1
possible predicted positive rates rate = πtpr + (1− π)fpr which can be established by a

0 acc d rate = π(1− π)(2AUROC− 1) + 1/2 [8].

Calibration: slopes of convex hull segments can be interpreted as empirical likelihood ratios asso-
ciated with a particular interval of raw classiﬁer scores. This gives rise to a non-parametric
calibration procedure which is also called isotonic regression [19] or pool adjacent viola-
tors [4] and results in a calibration map which maps each segment of ROCCH with slope r
to a calibrated score c = πr/(πr + (1−π)) [6]. Deﬁne a skew-sensitive version of accuracy
as accc (cid:44) 2cπtpr +2(1−c)(1−π)(1−fpr) (i.e.  standard accuracy is accc=1/2) then a per-

2

00.20.40.60.8100.10.20.30.40.50.60.70.80.91False positive rateTrue positive rate00.20.40.60.8100.10.20.30.40.50.60.70.80.91RecallPrecisionfectly calibrated classiﬁer outputs  for every instance  the value of c for which the instance
is on the accc decision boundary.

Alternative solutions for each of these exist. For example  parametric alternatives to ROCCH cali-
bration exist based on the logistic function  e.g. Platt scaling [13]; as do alternative ways to aggregate
classiﬁcation performance across different operating points  e.g. the Brier score [8]. However  the
power of ROC analysis derives from the combination of the above desirable properties  which helps
to explain its popularity across the machine learning discipline.
This paper presents fundamental improvements in Precision-Recall analysis  inspired by ROC anal-
ysis  as follows. (i) We identify in Section 2 the problems with current practice in Precision-Recall
curves by demonstrating that they fail to satisfy each of the above properties in some respect. (ii) We
propose a principled way to remedy all these problems by means of a change of coordinates in Sec-
tion 3. (iii) In particular  our improved Precision-Recall-Gain curves enclose an area that is directly
related to expected F1 score – on a harmonic scale – in a similar way as AUROC is related to ex-
pected accuracy. (iv) Furthermore  with Precision-Recall-Gain curves it is possible to calibrate a
model for Fβ in the sense that the predicted score for any instance determines the value of β for
which the instance is on the Fβ decision boundary. (v) We give experimental evidence in Section 4
that this matters by demonstrating that the area under traditional Precision-Recall curves can easily
favour models with lower expected F1 score than others.
Proofs of the formal results are found in the Supplementary Material; see also http://www.cs.
bris.ac.uk/˜flach/PRGcurves/.

2 Traditional Precision-Recall Analysis

Over-abundance of negative examples is a common phenomenon in many subﬁelds of machine
learning and data mining  including information retrieval  recommender systems and social network
analysis. Indeed  most web pages are irrelevant for most queries  and most links are absent from
most networks. Classiﬁcation accuracy is not a sensible evaluation measure in such situations  as it
over-values the always-negative classiﬁer. Neither does adjusting the class imbalance through cost-
sensitive versions of accuracy help  as this will not just downplay the beneﬁt of true negatives but
also the cost of false positives. A good solution in this case is to ignore true negatives altogether
and use precision  deﬁned as the proportion of true positives among the positive predictions  as
performance metric instead of false positive rate. In this context  the true positive rate is usually
renamed to recall. More formally  we deﬁne precision as prec = TP/(TP + FP) and recall as rec =
TP/(TP + FN)  where TP  FP and FN denote the number of true positives  false positives and false
negatives  respectively.
Perhaps motivated by the appeal of ROC plots  many researchers have begun to produce Precision-
Recall or PR plots with precision on the y-axis against recall on the x-axis. Figure 1 (right) shows the
PR curve corresponding to the ROC curve on the left. Clearly there is a one-to-one correspondence
between the two plots as both are based on the same contingency tables [2]. In particular  precision
associated with an ROC point is proportional to the angle between the line connecting the point with
the origin and the x-axis. However  this is where the similarity ends as PR plots have none of the
aforementioned desirable properties of ROC plots.

Non-universal baselines: a random classiﬁer has precision π and hence baseline performance is a
horizontal line which depends on the class distribution. The always-positive classiﬁer is at
the right-most end of this baseline (the always-negative classiﬁer has undeﬁned precision).
Non-linear interpolation: the main reason for this is that precision in a linearly interpolated con-
tingency table is only a linear combination of the original precision values if the two clas-
siﬁers have the same predicted positive rate (which is impossible if the two contingency
tables arise from different decision thresholds on the same model). [2] discusses this fur-
ther and also gives an interpolation formula. More generally  it isn’t meaningful to take the
arithmetic average of precision values.

Non-convex Pareto front: the set of non-dominated operating points continues to be well-deﬁned
(see the red circles in Figure 1 (right)) but in the absence of linear interpolation this set isn’t
convex for PR curves  nor is it straightforward to determine by visual inspection.

3

Uninterpretable area: although many authors report the area under the PR curve (AUPR) it doesn’t
have a meaningful interpretation beyond the geometric one of expected precision when
uniformly varying the recall (and even then the use of the arithmetic average cannot be
justiﬁed). Furthermore  PR plots have unachievable regions at the lower right-hand side 
the size of which depends on the class distribution [1].

No calibration: although some results exist regarding the relationship between calibrated scores
and F1 score (more about this below) these are unrelated to the PR curve. To the best of
our knowledge there is no published procedure to output scores that are calibrated for Fβ –
that is  which give the value of β for which the instance is on the Fβ decision boundary.

2.1 The Fβ measure
The standard way to combine precision and recall into a single performance measure is through the
F1 score [16]. It is commonly deﬁned as the harmonic mean of precision and recall:

(1)

F1 (cid:44)

2

1/prec + 1/rec =

2prec· rec
prec + rec =

TP

TP + (FP + FN)/2

The last form demonstrates that the harmonic mean is natural here as it corresponds to taking the
arithmetic mean of the numbers of false positives and false negatives. Another way to understand
the F1 score is as the accuracy in a modiﬁed contingency table which copies the true positive count
to the true negatives:

Actual ⊕
Actual (cid:9)

Predicted ⊕ Predicted (cid:9)

TP
FP

TP + FP

FN
TP
Pos

Pos
Neg− (TN − TP)
2TP + FP + FN

We can take a weighted harmonic mean which is commonly parametrised as follows:

Fβ (cid:44)

1

1

1+β 2 /prec + β 2

1+β 2 /rec

=

(1 + β 2)TP

(1 + β 2)TP + FP + β 2FN

(2)

There is a range of recent papers studying the F-score  several of which in last year’s NIPS confer-
ence [12  9  11]. Relevant results include the following: (i) non-decomposability of the Fβ score 
meaning it is not an average over instances (it is a ratio of such averages  called a pseudo-linear
function by [12]); (ii) estimators exist that are consistent: i.e.  they are unbiased in the limit [9  11];
(iii) given a model  operating points that are optimal for Fβ can be achieved by thresholding the
model’s scores [18]; (iv) a classiﬁer yielding perfectly calibrated posterior probabilities has the prop-
erty that the optimal threshold for F1 is half the optimal F1 at that point (ﬁrst proved by [20] and
later by [10]  while generalised to Fβ by [9]). The latter results tell us that optimal thresholds for
Fβ are lower than optimal thresholds for accuracy (or equal only in the case of the perfect model).
They don’t  however  tell us how to ﬁnd such thresholds other than by tuning (and [12] propose a
method inspired by cost-sensitive classiﬁcation). The analysis in the next section signiﬁcantly ex-
tends these results by demonstrating how we can identify all Fβ -optimal thresholds for any β in a
single calibration procedure.

3 Precision-Recall-Gain Curves

In this section we demonstrate how Precision-Recall analysis can be adapted to inherit all the beneﬁts
of ROC analysis. While technically straightforward  the implications of our results are far-reaching.
For example  even something as seemingly innocuous as reporting the arithmetic average of F1
values over cross-validation folds is methodologically misguided: we will deﬁne the corresponding
performance measure that can safely be averaged.

3.1 Baseline

A random classiﬁer that predicts positive with probability p has Fβ score (1 + β 2)pπ/(p + β 2π).
This is monotonically increasing in p ∈ [0 1] hence reaches its maximum for p = 1  the always-

4

Figure 2: (left) Conventional PR curve with hyperbolic F1 isometrics (dotted lines) and the baseline
performance by the always-positive classiﬁer (solid hyperbole). (right) Precision-Recall-Gain curve
with minor diagonal as baseline  parallel F1 isometrics and a convex Pareto front.

positive classiﬁer. Hence Precision-Recall analysis differs from classiﬁcation accuracy in that the
baseline to beat is the always-positive classiﬁer rather than any random classiﬁer. This baseline has
prec = π and rec = 1  and it is easily seen that any model with prec < π or rec < π loses against
this baseline. Hence it makes sense to consider only precision and recall values in the interval [π 1].
Any real-valued variable x ∈ [min max] can be rescaled by the mapping x (cid:55)→ x−min
max−min. However  the
linear scale is inappropriate here and we should use a harmonic scale instead  hence map to

(3)

(4)

FN
TP

max· (x− min)
(max− min)· x
Taking max = 1 and min = π we arrive at the following deﬁnition.
Deﬁnition 1 (Precision Gain and Recall Gain).

1/x− 1/min
1/max− 1/min =

precG =

prec− π
(1− π)prec = 1− π
1− π

FP
TP

recG =

rec− π
(1− π)rec = 1− π
1− π

A Precision-Recall-Gain curve plots Precision Gain on the y-axis against Recall Gain on the x-axis
in the unit square (i.e.  negative gains are ignored).

An example PRG curve is given in Figure 2 (right). The always-positive classiﬁer has recG = 1 and
precG = 0 and hence gets plotted in the lower right-hand corner of Precision-Recall-Gain space 
regardless of the class distribution. Since we show in the next section that F1 isometrics have slope
−1 in this space it follows that all classiﬁers with baseline F1 performance end up on the minor
diagonal in Precision-Recall-Gain space. In contrast  the corresponding F1 isometric in PR space is
hyperbolic (Figure 2 (left)) and its exact location depends on the class distribution.

3.2 Linearity and optimality

One of the main beneﬁts of PRG space is that it allows linear interpolation. This manifests itself
in two ways: any point on a straight line between two endpoints is achievable by random choice
between the endpoints (Theorem 1) and Fβ isometrics are straight lines with slope −β 2 (Theorem 2).
Theorem 1. Let P1 = (precG1 recG1) and P2 = (precG2 recG2) be points in the Precision-Recall-
Gain space representing the performance of Models 1 and 2 with contingency tables C1 and C2.
Then a model with an interpolated contingency table C∗ = λC1 + (1 − λ )C2 has precision gain
precG∗ = µprecG1 + (1− µ)precG2 and recall gain recG∗ = µrecG1 + (1− µ)recG2  where µ =
λ T P1/(λ T P1 + (1− λ )T P2).
Theorem 2. precG + β 2recG = (1 + β 2)FGβ   with FGβ =

= 1− π
1−π

FP+β 2FN
(1+β 2)TP .

Fβ−π
(1−π)Fβ

FGβ is a linearised version of Fβ in the same way as precG and recG are linearised versions of preci-
sion and recall. FGβ measures the gain in performance (on a linear scale) relative to a classiﬁer with

5

00.20.40.60.8100.10.20.30.40.50.60.70.80.91RecallPrecision00.20.40.60.8100.10.20.30.40.50.60.70.80.91Recall GainPrecision Gainboth precision and recall – and hence Fβ – equal to π. F1 isometrics are indicated in Figure 2 (right).
By increasing (decreasing) β 2 these lines of constant Fβ become steeper (ﬂatter) and hence we are
putting more emphasis on recall (precision).
With regard to optimality  we already knew that every classiﬁer or threshold optimal for Fβ for some
β 2 is optimal for accc for some c. The reverse also holds  except for the ROC convex hull points
below the baseline (e.g.  the always-negative classiﬁer). Due to linearity the PRG Pareto front is
convex and easily constructed by visual inspection. We will see in Section 3.4 that these segments
of the PRG convex hull can be used to obtain classiﬁer scores speciﬁcally calibrated for F-scores 
thereby pre-empting the need for any more threshold tuning.

3.3 Area

Deﬁne the area under the Precision-Recall-Gain curve as AUPRG =(cid:82) 1

0 precG d recG. We will
show how this area can be related to an expected FG1 score when averaging over the operating
points on the curve in a particular way. To this end we deﬁne ∆ = recG/π − precG/(1− π)  which
expresses the extent to which recall exceeds precision (reweighting by π and 1−π guarantees that ∆
is monotonically increasing when changing the threshold towards having more positive predictions 
as shown in the proof of Theorem 3 in the Supplementary Material). Hence  −y0/(1−π)≤ ∆≤ 1/π 
where y0 denotes the precision gain at the operating point where recall gain is zero. The following
theorem shows that if the operating points are chosen such that ∆ is uniformly distributed in this
range  then the expected FG1 can be calculated from the area under the Precision-Recall-Gain curve
(the Supplementary Material proves a more general result for expected FGβ .) This justiﬁes the use
of AUPRG as a performance metric without ﬁxing the classiﬁer’s operating point in advance.
Theorem 3. Let the operating points of a model with area under the Precision-Recall-Gain curve
AUPRG be chosen such that ∆ is uniformly distributed within [−y0/1− π 1/π]. Then the expected
FG1 score is equal to

E [FG1] =

AUPRG/2 + 1/4− π(1− y0

1− π(1− y0)

2)/4

(5)

The expected reciprocal F1 score can be calculated from the relationship E [1/F1] = (1 − (1 −
π)E [FG1])/π which follows from the deﬁnition of FGβ .
In the special case where y0 = 1 the
expected FG1 score is AUPRG/2 + 1/4.

3.4 Calibration

Figure 3 (left) shows an ROC curve with empirically calibrated posterior probabilities obtained by
isotonic regression [19] or the ROC convex hull [4]. Segments of the convex hull are labelled with
the value of c for which the two endpoints have the same skew-sensitive accuracy accc. Conversely 
if a point connects two segments with c1 < c2 then that point is optimal for any c such that c1 <
c < c2. The calibrated values c are derived from the ROC slope r by c = πr/(πr + (1− π)) [6].
For example  the point on the convex hull two steps up from the origin optimises skew-sensitive
accuracy accc for 0.29 < c < 0.75 and hence also standard accuracy (c = 1/2). We are now in a
position to calculate similarly calibrated scores for F-score.
Theorem 4. Let two classiﬁers be such that prec1 > prec2 and rec1 < rec2  then these two classiﬁers
have the same Fβ score if and only if

β 2 = −1/prec1 − 1/prec2
1/rec1 − 1/rec2

In line with ROC calibration we convert these slopes into a calibrated score between 0 and 1:

d =

1

(β 2 + 1)

=

1/rec1 − 1/rec2

(1/rec1 − 1/rec2)− (1/prec1 − 1/prec2)

It is important to note that there is no model-independent relationship between ROC-calibrated
scores and PRG-calibrated scores  so we cannot derive d from c. However  we can equip a model
with two calibration maps  one for accuracy and the other for F-score.

6

(6)

(7)

Figure 3: (left) ROC curve with scores empirically calibrated for accuracy. The green dots corre-
(right) Precision-Recall-Gain curve with
spond to a regular grid in Precision-Recall-Gain space.
scores calibrated for Fβ . The green dots correspond to a regular grid in ROC space  clearly indicating
that ROC analysis over-emphasises the high-recall region.

Figure 3 (right) shows the PRG curve for the running example with scores calibrated for Fβ . Score
0.76 corresponds to β 2 = (1− 0.76)/0.76 = 0.32 and score 0.49 corresponds to β 2 = 1.04  so the
point closest to the Precision-Recall breakeven line optimises Fβ for 0.32 < β 2 < 1.04 and hence
also F1 (but note that the next point to the right on the convex hull is nearly as good for F1  on
account of the connecting line segment having a calibrated score close to 1/2).

4 Practical examples

The key message of this paper is that precision  recall and F-score are expressed on a harmonic
scale and hence any kind of arithmetic average of these quantities is methodologically wrong. We
now demonstrate that this matters in practice. In particular  we show that in some sense  AUPR and
AUPRG are as different from each other as AUPR and AUROC. Using the OpenML platform [17]
we took all those binary classiﬁcation tasks which have 10-fold cross-validated predictions using
at least 30 models from different learning methods (these are called ﬂows in OpenML). In each of
the obtained 886 tasks (covering 426 different datasets) we applied the following procedure. First 
we fetched the predicted scores of 30 randomly selected models from different ﬂows and calculated
areas under ROC  PRG and PR curves(with hyperbolic interpolation as recommended by [2])  with
minority class as positives. We then ranked the 30 models with respect to these measures. Figure 4
plots AUPRG-rank against AUPR-rank across all 25980 models.
Figure 4 (left) demonstrates that AUPR and AUPRG often disagree in ranking the models. In par-
ticular  they disagree on the best method in 24% of the tasks and on the top three methods in 58%
of the tasks (i.e.  they agree on top  second and third method in 42% of the tasks). This amount of
disagreement is comparable to the disagreement between AUPR and AUROC (29% and 65% dis-
agreement for top 1 and top 3  respectively) and between AUPRG and AUROC (22% and 57%).
Therefore  AUPR  AUPRG and AUROC are related quantities  but still all signiﬁcantly different.
The same conclusion is supported by the pairwise correlations between the ranks across all tasks:
the correlation between AUPR-ranks and AUPRG-ranks is 0.95  between AUPR and AUROC it is
0.95  and between AUPRG and AUROC it is 0.96.
Figure 4 (right) shows AUPRG vs AUPR in two datasets with relatively low and high rank corre-
lations (0.944 and 0.991  selected as lower and upper quartiles among all tasks). In both datasets
AUPR and AUPRG agree on the best model. However  in the white-clover dataset the second best is
AdaBoost according to AUPRG and Logistic Regression according to AUPR. As seen in Figure 5 
this disagreement is caused by AUPR taking into account the poor performance of AdaBoost in the
early part of the ranking; AUPRG ignores this part as it has negative recall gain.

7

00.20.40.60.8100.10.20.30.40.50.60.70.80.91False positive rateTrue positive rate 1 0.75 0.29 0.180.0530.0480.0280.014 000.20.40.60.8100.10.20.30.40.50.60.70.80.91Recall GainPrecision Gain 0.99 0.76 0.490.0750.0670.0340.016 0Figure 4: (left) Comparison of AUPRG-ranks vs AUPR-ranks. Each cell shows how many models
across 886 OpenML tasks have these ranks among the 30 models in the same task. (right) Compar-
ison of AUPRG vs AUPR in OpenML tasks with IDs 3872 (white-clover) and 3896 (ada-agnostic) 
with 30 models in each task. Some models perform worse than random (AUPRG < 0) and are not
plotted. The models represented by the two encircled triangles are shown in detail in Figure 5.

Figure 5: (left) ROC curves for AdaBoost (solid line) and Logistic Regression (dashed line) on the
white-clover dataset (OpenML run IDs 145651 and 267741  respectively). (middle) Corresponding
PR curves. The solid curve is on average lower with AUPR = 0.724 whereas the dashed curve has
AUPR = 0.773. (right) Corresponding PRG curves  where the situation has reversed: the solid curve
has AUPRG = 0.714 while the dashed curve has a lower AUPRG of 0.687.

5 Concluding remarks

If a practitioner using PR-analysis and the F-score should take one methodological recommendation
from this paper  it is to use the F-Gain score instead to make sure baselines are taken into account
properly and averaging is done on the appropriate scale. If required the FGβ score can be converted
back to an Fβ score at the end. The second recommendation is to use Precision-Recall-Gain curves
instead of PR curves  and the third to use AUPRG which is easier to calculate than AUPR due to
linear interpolation  has a proper interpretation as an expected F-Gain score and allows performance
assessment over a range of operating points. To assist practitioners we have made R  Matlab and
Java code to calculate AUPRG and PRG curves available at http://www.cs.bris.ac.uk/
˜flach/PRGcurves/. We are also working on closer integration of AUPRG as an evaluation
metric in OpenML and performance visualisation platforms such as ViperCharts [15].
As future work we mention the interpretation of AUPRG as a measure of ranking performance: we
are working on an interpretation which gives non-uniform weights to the positives and as such is
related to Discounted Cumulative Gain. A second line of research involves the use of cost curves
for the FGβ score and associated threshold choice methods.

Acknowledgments This work was supported by the REFRAME project granted by the European Coordi-
nated Research on Long-Term Challenges in Information and Communication Sciences & Technologies ERA-
Net (CHIST-ERA)  and funded by the Engineering and Physical Sciences Research Council in the UK under
grant EP/K018728/1. Discussions with Hendrik Blockeel helped to clarify the intuitions underlying this work.

8

11020301102030Rank of AUPRRank of AUPRGCount012−34−78−1516−3132−6364−127128−...●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●0.000.250.500.751.000.000.250.500.751.00AUPRAUPRGDataset●ada−agnosticwhite−clover0.000.250.500.751.000.000.250.500.751.00False positive rateTrue positive rate0.000.250.500.751.000.000.250.500.751.00RecallPrecision0.000.250.500.751.000.000.250.500.751.00Recall GainPrecision GainReferences
[1] K. Boyd  V. S. Costa  J. Davis  and C. D. Page. Unachievable region in precision-recall space
and its effect on empirical evaluation. In International Conference on Machine Learning  page
349  2012.

[2] J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves.

In
Proceedings of the 23rd International Conference on Machine Learning  pages 233–240  2006.
[3] T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters  27(8):861–874 

2006.

[4] T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning  68(1):

97–106  July 2007.

[5] P. A. Flach. The geometry of ROC space: understanding machine learning metrics through
ROC isometrics. In Machine Learning  Proceedings of the Twentieth International Conference
(ICML 2003)  pages 194–201  2003.

[6] P. A. Flach. ROC analysis. In C. Sammut and G. Webb  editors  Encyclopedia of Machine

Learning  pages 869–875. Springer US  2010.

[7] D. J. Hand and R. J. Till. A simple generalisation of the area under the ROC curve for multiple

class classiﬁcation problems. Machine Learning  45(2):171–186  2001.

[8] J. Hern´andez-Orallo  P. Flach  and C. Ferri. A uniﬁed view of performance metrics: Translating
threshold choice into expected classiﬁcation loss. Journal of Machine Learning Research  13:
2813–2869  2012.

[9] O. O. Koyejo  N. Natarajan  P. K. Ravikumar  and I. S. Dhillon. Consistent binary classiﬁcation
with generalized performance metrics. In Advances in Neural Information Processing Systems 
pages 2744–2752  2014.

[10] Z. C. Lipton  C. Elkan  and B. Naryanaswamy. Optimal thresholding of classiﬁers to maximize
F1 measure. In Machine Learning and Knowledge Discovery in Databases  volume 8725 of
Lecture Notes in Computer Science  pages 225–239. Springer Berlin Heidelberg  2014.

[11] H. Narasimhan  R. Vaish  and S. Agarwal. On the statistical consistency of plug-in classiﬁers
for non-decomposable performance measures. In Advances in Neural Information Processing
Systems 27  pages 1493–1501. 2014.

[12] S. P. Parambath  N. Usunier  and Y. Grandvalet. Optimizing F-measures by cost-sensitive
classiﬁcation. In Advances in Neural Information Processing Systems  pages 2123–2131  2014.
[13] J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In Advances in Large Margin Classiﬁers  pages 61–74. MIT Press  Boston 
1999.

[14] F. Provost and T. Fawcett. Robust classiﬁcation for imprecise environments. Machine Learn-

ing  42(3):203–231  2001.

[15] B. Sluban and N. Lavraˇc. Vipercharts: Visual performance evaluation platform. In H. Blockeel 
K. Kersting  S. Nijssen  and F. ˇZelezn´y  editors  Machine Learning and Knowledge Discovery
in Databases  volume 8190 of Lecture Notes in Computer Science  pages 650–653. Springer
Berlin Heidelberg  2013.

[16] C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann  Newton  MA  USA 

2nd edition  1979.

[17] J. Vanschoren  J. N. van Rijn  B. Bischl  and L. Torgo. OpenML: networked science in machine

learning. SIGKDD Explorations  15(2):49–60  2013.

[18] N. Ye  K. M. A. Chai  W. S. Lee  and H. L. Chieu. Optimizing F-measures: A tale of two
approaches. In Proceedings of the 29th International Conference on Machine Learning  pages
289–296  2012.

[19] B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees
and naive Bayesian classiﬁers. In Proceedings of the Eighteenth International Conference on
Machine Learning (ICML 2001)  pages 609–616  2001.

[20] M.-J. Zhao  N. Edakunni  A. Pocock  and G. Brown. Beyond Fano’s inequality: bounds on the
optimal F-score  BER  and cost-sensitive risk and their implications. The Journal of Machine
Learning Research  14(1):1033–1090  2013.

9

,Raif Rustamov
Leonidas Guibas
Shuhang Gu
Lei Zhang
Xiangchu Feng
Peter Flach
Meelis Kull
Arsenii Vanunts
Alexey Drutsa