2009,Speaker Comparison with Inner Product Discriminant Functions,Speaker comparison  the process of finding the speaker similarity between two speech signals  occupies a central role in a variety of applications---speaker verification  clustering  and identification. Speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process.  For a given speech signal  feature vectors are produced and used to adapt a Gaussian mixture model (GMM).  Speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models.  We propose a framework  inner product discriminant functions (IPDFs)  which extends many common techniques for speaker comparison: support vector machines  joint factor analysis  and linear scoring.  The framework uses inner products between the parameter vectors of GMM models motivated by several statistical methods.  Compensation of nuisances is performed via linear transforms on GMM parameter vectors.  Using the IPDF framework  we show that many current techniques are simple variations of each other.  We demonstrate  on a 2006 NIST speaker recognition evaluation task  new scoring methods using IPDFs which produce excellent error rates and require significantly less computation than current techniques.,Speaker Comparison with Inner Product

Discriminant Functions

W. M. Campbell

MIT Lincoln Laboratory
Lexington  MA 02420
wcampbell@ll.mit.edu

Z. N. Karam

DSPG  MIT RLE  Cambridge MA

MIT Lincoln Laboratory  Lexington  MA

zahi@mit.edu

D. E. Sturim

MIT Lincoln Laboratory
Lexington  MA 02420

sturim@ll.mit.edu

Abstract

Speaker comparison  the process of ﬁnding the speaker similarity between two
speech signals  occupies a central role in a variety of applications—speaker ver-
iﬁcation  clustering  and identiﬁcation. Speaker comparison can be placed in a
geometric framework by casting the problem as a model comparison process. For
a given speech signal  feature vectors are produced and used to adapt a Gaussian
mixture model (GMM). Speaker comparison can then be viewed as the process of
compensating and ﬁnding metrics on the space of adapted models. We propose
a framework  inner product discriminant functions (IPDFs)  which extends many
common techniques for speaker comparison—support vector machines  joint fac-
tor analysis  and linear scoring. The framework uses inner products between the
parameter vectors of GMM models motivated by several statistical methods. Com-
pensation of nuisances is performed via linear transforms on GMM parameter
vectors. Using the IPDF framework  we show that many current techniques are
simple variations of each other. We demonstrate  on a 2006 NIST speaker recog-
nition evaluation task  new scoring methods using IPDFs which produce excellent
error rates and require signiﬁcantly less computation than current techniques.

1 Introduction

Comparing speakers in speech signals is a common operation in many applications including foren-
sic speaker recognition  speaker clustering  and speaker veriﬁcation. Recent popular approaches
to text-independent comparison include Gaussian mixture models (GMMs) [1]  support vector ma-
chines [2  3]  and combinations of these techniques. When comparing two speech utterances  these
approaches are used in a train and test methodology. One utterance is used to produce a model which
is then scored against the other utterance. The resulting comparison score is then used to cluster 
verify or identify the speaker.

Comparing speech utterances with kernel functions has been a common theme in the speaker recog-
nition SVM literature [2  3  4]. The resulting framework has an intuitive geometric structure. Vari-
able length sequences of feature vectors are mapped to a large dimensional SVM expansion vector.
These vectors are “smoothed” to eliminate nuisances [2]. Then  a kernel function is applied to the

∗This work was sponsored by the Federal Bureau of Investigation under Air Force Contract FA8721-05-
C-0002. Opinions  interpretations  conclusions  and recommendations are those of the authors and are not
necessarily endorsed by the United States Government.

1

two vectors. The kernel function is an inner product which induces a metric on the set of vectors  so
comparison is analogous to ﬁnding the distances between SVM expansion vectors.

A recent trend in the speaker recognition literature has been to move towards a more linear geo-
metric view for non-SVM systems. Compensation via linear subspaces and supervectors of mean
parameters of GMMs is presented in joint factor analysis [5]. Also  comparison of utterances via
linear scoring is presented in [6]. These approaches have introduced many new ideas and perform
well in speaker comparison tasks.

An unrealized effort in speaker recognition is to bridge the gap between SVMs and some of the new
proposed GMM methods. One difﬁculty is that most SVM kernel functions in speaker comparison
satisfy the Mercer condition. This restricts the scope of investigation of potential comparison strate-
gies for two speaker utterances. Therefore  in this paper  we introduce the idea of inner product
discriminant functions (IPDFs).

IPDFs are based upon the same basic operations as SVM kernel functions with some relaxation in
structure. First  we map input utterances to vectors of ﬁxed dimension. Second  we compensate the
input feature vectors. Typically  this compensation takes the form of a linear transform. Third  we
compare two compensated vectors with an inner product. The resulting comparison function is then
used in an application speciﬁc way.

The focus of our initial investigations of the IPDF structure are the following. First  we show that
many of the common techniques such as factor analysis  nuisance projection  and various types of
scoring can be placed in the framework. Second  we systematically describe the various inner prod-
uct and compensation techniques used in the literature. Third  we propose new inner products and
compensation. Finally  we explore the space of possible combinations of techniques and demon-
strate several novel methods that are computationally efﬁcient and produce excellent error rates.

The outline of the paper is as follows.
In Section 2  we describe the general setup for speaker
comparison using GMMs. In Section 3  we introduce the IPDF framework. Section 4 explores inner
products for the IPDF framework. Section 5 looks at methods for compensating for variability. In
Section 6  we perform experiments on the NIST 2006 speaker recognition evaluation and explore
different combinations of IPDF comparisons and compensations.

2 Speaker Comparison

A standard distribution used for text-independent speaker recognition is the Gaussian mixture
model [1] 

g(x) =

N

X

i=1

λiN (x|mi  Σi).

(1)

Feature vectors are typically cepstral coefﬁcients with associated smoothed ﬁrst- and second-order
derivatives.
We map a sequence of feature vectors  xNx
1   from a speaker to a GMM by adapting a GMM universal
background model (UBM). Here  we use the shorthand xNx
to denote the sequence  x1  · · ·   xNx.
For the purpose of this paper  we will assume only the mixture weights  λi  and means  mi  in (1)
are adapted. Adaptation of the means is performed with standard relevance MAP [1]. We estimate
the mixture weights using the standard ML estimate. The adaptation yields new parameters which
we stack into a parameter vector  ax  where
x(cid:3)t
x mt
· · ·

· · · mt

(2)

1

(3)

ax = (cid:2)λt
= (cid:2)λx 1

λx N mt

x 1

x N(cid:3)t

.

In speaker comparison  the problem is to compare two sequences of feature vectors  e.g.  xNx
and
Ny
1 . To compare these two sequences  we adapt a GMM UBM to produce two sets of parameter
y
vectors  ax and ay  as in (2). The goal of our speaker comparison process can now be recast as a
function that compares the two parameter vectors  C(ax  ay)  and produces a value that reﬂects the
similarity of the speakers. Initial work in this area was performed using kernels from support vector
machines [4  7  2]  but we expand the scope to other types of discriminant functions.

1

2

3 Inner Product Discriminant Functions

The basic framework we propose for speaker comparison functions is composed of two parts—
compensation and comparison. For compensation  the parameter vectors generated by adaptation
in (2) can be transformed to remove nuisances or projected onto a speaker subspace. The second
part of our framework is comparison. For the comparison of parameter vectors  we will consider
natural distances that result in inner products between parameter vectors.

We propose the following inner product discriminant function (IPDF) framework for exploring
speaker comparison 

(4)
where Lx  Ly are linear transforms and potentially dependent on λx and/or λy. The matrix D is
positive deﬁnite  usually diagonal  and possibly dependent on λx and/or λy. Note  we also consider
simple combinations of IPDFs to be in our framework—e.g.  positively-weighted sums of IPDFs.

C(ax  ay) = (Lxax)tD2(Lyay)

Several questions from this framework are: 1) what inner product gives the best speaker comparison
performance  2) what compensation strategy works best  3) what tradeoffs can be made between
accuracy and computational cost  and 4) how do the compensation and the inner product interact.
We explore theoretical and experimental answers to these questions in the following sections.

4 Inner Products for IPDFs

In general  an inner product of the parameters should be based on a distance arising from a statistical
comparison. We derive three straightforward methods in this section. We also relate some other
methods  without being exhaustive  that fall in this framework that have been described in detail in
the literature.

4.1 Approximate KL Comparison (CKL)
A straightforward strategy for comparing the GMM parameter vectors is to use an approximate
form of the KL divergence applied to the induced GMM models. This strategy was used in [2]
successfully with an approximation based on the log-sum inequality; i.e.  for the GMMs  gx and gy 
with parameters ax and ay 

D(gxkgy) ≤

N

X

i=1

λx iD (N (·; mx i  Σi)kN (·; my i  Σi)) .

(5)

Here  D(·k·) is the KL divergence  and Σi is from the UBM.
By symmetrizing (5) and substituting in the KL divergence between two Gaussian distributions  we
obtain a distance  ds  which upper bounds the symmetric KL divergence 

ds(ax  ay) = Ds(λxkλy) +

N

X

i=1

(0.5λx i + 0.5λy i)(mx i − my i)tΣ−1

i

(mx i − my i).

(6)

We focus on the second term in (6) for this paper  but note that the ﬁrst term could also be converted
to a comparison function on the mixture weights. Using polarization on the second term  we obtain
the inner product

CKL(ax  ay) =

N

X

i=1

(0.5λx i + 0.5λy i)mt

x iΣ−1

i my i.

Note that (7) can also be expressed more compactly as

CKL(ax  ay) = mt

x ((0.5λx + 0.5λy) ⊗ In) Σ−1my

(7)

(8)

where Σ is the block matrix with the Σi on the diagonal  n is the feature vector dimension  and ⊗
is the Kronecker product. Note that the non-symmetric form of the KL distance in (5) would result
in the average mixture weights in (8) being replaced by λx. Also  note that shifting the means by
the UBM will not affect the distance in (6)  so we can replace means in (8) by the UBM centered
means.

3

4.2 GLDS kernel (CGLDS)
An alternate inner product approach is to use generalized linear discriminants and the corresponding
kernel [4]. The overall structure of this GLDS kernel is as follows. A per feature vector expansion
function is deﬁned as

b(xi) = [b1(xi)

· · ·

bm(xi)]t .

(9)

(10)

(11)

(12)

The mapping between an input sequence  xNx

1

is then deﬁned as

X
i=1
The corresponding kernel between two sequences is then
Ny
1 ) = bt

KGLDS(xNx

7→ bx =

1   y

xNx

1

xΓ−1by

Nx

1
Nx

b(xi).

where

Γ =

1
Nz

Nz

X

i=1

b(zi)b(zi)t 

is a large set of feature vectors which is representative of the speaker population.

and zNz
1
In the context of a GMM UBM  we can deﬁne an expansion as follows

(13)
where p(j|xi) is the posterior probability of mixture component j given xi  and mj is from a UBM.
Using (13) in (10)  we see that

b(xi) = (cid:2)p(1|xi)(xi − m1)t

p(N |xi)(xi − mN )t(cid:3)t

· · ·

bx = (λx ⊗ In)(mx − m) and by = (λy ⊗ In)(my − m)

(14)

where m is the stacked means of the UBM. Thus  the GLDS kernel inner product is
CGLDS(ax  ay) = (mx − m)t(λx ⊗ In)Γ−1(λy ⊗ In)(my − m).

(15)
Note that Γ in (12) is almost the UBM covariance matrix  but is not quite the same because of a
squaring of the p(j|zi) in the diagonal. As is commonly assumed  we will consider a diagonal
approximation of Γ  see [4].

4.3 Gaussian-Distributed Vectors
A common assumption in the factor analysis literature [5] is that the parameter vector mx as x varies
has a Gaussian distribution. If we assume a single covariance for the entire space  then the resulting
likelihood ratio test between two Gaussian distributions results in a linear discriminant [8].
More formally  suppose that we have a distribution with mean mx and we are trying to distinguish
from a distribution with the UBM mean m  then the discriminant function is [8] 

h(x) = (mx − m)tΥ−1(x − m) + cx

(16)
where cx is a constant that depends on mx  and Υ is the covariance in the parameter vector space.
We will assume that the comparison function can be normalized (e.g.  by Z-norm [1])  so that cx can
be dropped. We now apply the discriminant function to another mean vector  my  and obtain the
following comparison function

CG(ax  ay) = (mx − m)tΥ−1(my − m).

(17)

4.4 Other Methods

Several other methods are possible for comparing the parameter vectors that arise either from ad hoc
methods or from work in the literature. We describe a few of these in this section.
Geometric Mean Comparison (CGM). A simple symmetric function that is similar to the KL (8)
and GLDS (15) comparison functions is arrived at by replacing the arithmetic mean in CKL by a
geometric mean. The resulting kernel is

CGM (ax  ay) = (mx − m)t(λ1/2

x ⊗ In)Σ−1(λ1/2

y ⊗ In)(my − m)

(18)

4

where Σ is the block diagonal UBM covariances.
Fisher Kernel (CF ). The Fisher kernel specialized to the UBM case has several forms [3]. The
main variations are the choice of covariance in the inner product and the choice of normalization
of the gradient term. We took the best performing conﬁguration for this paper—we normalize the
gradient by the number of frames which results in a mixture weight scaling of the gradient. We also
use a diagonal data-trained covariance term. The resulting comparison function is

CF (ax  ay) = (cid:2)(λx ⊗ In)Σ−1(mx − m)(cid:3)t

Φ−1 (cid:2)(λy ⊗ In)Σ−1(my − m)(cid:3)

(19)

where Φ is a diagonal matrix acting as a variance normalizer.
Linearized Q-function (CQ). Another form of inner product may be derived from the linear Q-
scoring shown in [6]. In this case  the scoring is given as (mtrain − m)tΣ−1(F − Nm) where N
and F are the zeroth and ﬁrst order sufﬁcient statistics of a test utterance  m is the UBM means 
mtrain is the mean of a training model  and Σ is the block diagonal UBM covariances. A close
approximation of this function can be made by using a small relevance factor in MAP adaptation of
the means to obtain the following comparison function

CQ(ax  ay) = (mx − m)tΣ−1(λy ⊗ In)(my − m).

(20)
Note that if we symmetrize CQ  this gives us CKL; this analysis ignores for a moment that in [6] 
compensation is also asymmetric.
KL Kernel (KKL). By assuming the mixture weights are constant and equal to the UBM mixture
in the comparison function CKL (7)  we obtain the KL kernel 

(21)
where λ are the UBM mixture weights. This kernel has been used extensively in SVM speaker
recognition [2].

KKL(mx  my) = mt

x (λ ⊗ In) Σ−1my

An analysis of the different inner products in the preceding sections shows that many of the methods
presented in the literature have a similar form  but are interestingly derived with quite disparate
techniques. Our goal in the experimental section is to understand how these comparison function
perform and how they interact with compensation.

5 Compensation in IPDFs

Our next task is to explore compensation methods for IPDFs. Our focus will be on subspace-based
methods. With these methods  the fundamental assumption is that either speakers and/or nuisances
are conﬁned to a small subspace in the parameter vector space. The problem is to use this knowledge
to produce a higher signal (speaker) to noise (nuisance) representation of the speaker. Standard
notation is to use U to represent the nuisance subspace and to have V represent the speaker subspace.
Our goal in this section is to recast many of the methods in the literature in a standard framework
with oblique and orthogonal projections.

To make a cohesive presentation  we introduce some notation. We deﬁne an orthogonal projection
with respect to a metric  PU D  where D and U are full rank matrices as

PU D = U (U tD2U )−1U tD2

(22)
where DU is a linearly independent set  and the metric is kx − ykD = kDx − Dyk2. The
process of projection  e.g.
is equivalent to solving the least-squares problem 
ˆx = argminx kU x − bkD and letting y = U ˆx. For convenience  we also deﬁne the projection
onto the orthogonal complement of U   U ⊥  as QU D = PU ⊥ D = I − PU D. Note that we can reg-
ularize the projection PU D by adding a diagonal term to the inverse in (22); the resulting operation
remains linear but is no longer a projection.

y = PU Db 

We also deﬁne the oblique projection onto V with null space U + (U + V )⊥ and metric induced by
D. Let QR be the (skinny) QR decomposition of the matrix [U V ] in the D norm (i.e.  QtD2Q = I) 
and QV be the columns corresponding to V in the matrix Q. Then  the oblique (non-orthogonal)
projection onto V is

(23)
The use of projections in our development will add geometric understanding to the process of com-
pensation.

OV U D = V (Qt

V D2V )−1Qt

V D2.

5

5.1 Nuisance Attribute Projection (NAP)

A framework for eliminating nuisances in the parameter vector based on projection was shown in [2].
The basic idea is to assume that nuisances are conﬁned to a small subspace and can be removed via
an orthogonal projection  mx 7→ QU Dmx. One justiﬁcation for using subspaces comes from the
perspective that channel classiﬁcation can be performed with inner products along one-dimensional
subspaces. Therefore  the projection removes channel speciﬁc directions from the parameter space.

The NAP projection uses the metric induced by a kernel in an SVM. For the GMM context  the
standard kernel used is the approximate KL comparison (8) [2]. We note that since D is known a
priori to speaker comparison  we can orthonormalize the matrix DU and apply the projection as a
matrix multiply. The resulting projection has D = (cid:16)λ1/2 ⊗ In(cid:17) Σ−1/2.
5.2 Factor Analysis and Joint Factor Analysis

ms sess = m + U x + V y

The joint factor analysis (JFA) model assumes that the mean parameter vector can be expressed as
(24)
where ms sess is the speaker- and session-dependent mean parameter vector  U and V are matrices
with small rank  and m is typically the UBM. Note that for this section  we will use the standard
variables for factor analysis  x and y  even though they conﬂict with our earlier development. The
goal of joint factor analysis is to ﬁnd solutions to the latent variables x and y given training data.
In (24)  the matrix U represents a nuisance subspace  and V represents a speaker subspace. Existing
work on this approach for speaker recognition uses both maximum likelihood (ML) estimates and
MAP estimates of x and y [9  5]. In the latter case  a Gaussian prior with zero mean and diagonal
covariance for x and y is assumed. For our work  we focus on the ML estimates [9] of x and y
in (24)  since we did not observe substantially different performance from MAP estimates in our
experiments.

Another form of modeling that we will consider is factor analysis (FA). In this case  the term V y is
replaced by a constant vector representing the true speaker model  ms; the goal is then to estimate
x. Typically  as a simpliﬁcation  ms is assumed to be zero when calculating sufﬁcient statistics for
estimation of x [10].

The solution to both JFA and FA can be uniﬁed. For the JFA problem  if we stack the matrices [U V ] 
then the problem reverts to the FA problem. Therefore  we initially study the FA problem. Note that
we also restrict our work to only one EM iteration of the estimation of the factors  since this strategy
works well in practice.

The standard ML solution to FA [9] for one EM iteration can be written as
(cid:2)U tΣ−1(N ⊗ In)U(cid:3) x = U tΣ−1 [F − (N ⊗ In)m]

(25)
where F is the vector of ﬁrst order sufﬁcient statistics  and N is the diagonal matrix of zeroth order
statistics (expected counts). The sufﬁcient statistics are obtained from the UBM applied to an input
set of feature vectors. We ﬁrst let Nt = PN
i=1 Ni and multiply both sides of (25) by 1/Nt. Now
use relevance MAP with a small relevance factor and F and N to obtain ms; i.e.  both ms − m and
F − (N ⊗ In)m will be nearly zero in the entries corresponding to small Ni. We obtain

(cid:2)U tΣ−1(λs ⊗ In)U(cid:3) x = U tΣ−1 (λs ⊗ In) [ms − m]

(26)
where λs is the speaker dependent mixture weights. We note that (26) are the normal equations
for the least-squares problem  ˆx = argminx kU x − (ms − m)kD where D is given below. This
solution is not unexpected since ML estimates commonly lead to least-squares problems with GMM
distributed data [11].

Once the solution to (26) is obtained  the resulting U x is subtracted from an estimate of the speaker
mean  ms to obtain the compensated mean. If we assume that ms is obtained by a relevance map
adaptation from the statistics F and N with a small relevance factor  then the FA process is well
approximated by

where

ms 7→ QU Dms

D = (cid:16)λ1/2

s ⊗ In(cid:17) Σ−1/2.

6

(27)

(28)

JFA becomes an extension of the FA process we have demonstrated. One ﬁrst projects onto the
stacked U V space. Then another projection is performed to eliminate the U component of variabil-
ity. This can be expressed as a single oblique projection; i.e.  the JFA process is

ms 7→ OV U I P[UV ] Dms = OV U Dms.

(29)

5.3 Comments and Analysis

Several comments should be made on compensation schemes and their use in speaker comparison.
First  although NAP and ML FA (27) were derived in substantially different ways  they are essen-
tially the same operation  an orthogonal projection. The main difference is in the choice of metrics
under which they were originally proposed. For NAP  the metric depends on the UBM only  and for
FA it is utterance and UBM dependent.

A second observation is that the JFA oblique projection onto V has substantially different properties
than a standard orthogonal projection. When JFA is used in speaker recognition [5  6]  typically
JFA is performed in training  but the test utterance is compensated only with FA. In our notation 
applying JFA with linear scoring [6] gives

CQ(OV U D1 m1  QU D2m2)

(30)

where m1 and m2 are the mean parameter vectors estimated from the training and testing utterances 
respectively; also  D1 = (λ1/2
2 ⊗ In)Σ−1/2. Our goal in the exper-
iments section is to disentangle and understand some of the properties of scoring methods such
as (30). What is signiﬁcant in this process—mismatched train/test compensation  data-dependent
metrics  or asymmetric scoring?

1 ⊗ In)Σ−1/2 and D2 = (λ1/2

A ﬁnal note is that training the subspaces for the various projections optimally is not a process
that is completely understood. One difﬁculty is that the metric used for the inner product may
not correspond to the metric for compensation. As a baseline  we used the same subspace for all
comparison functions. The subspace was obtained with an ML style procedure for training subspaces
similar to [11] but specialized to the factor analysis problem as in [5].

6 Speaker Comparison Experiments

Experiments were performed on the NIST 2006 speaker recognition evaluation (SRE) data set. En-
rollment/veriﬁcation methodology and the evaluation criterion  equal error rate (EER) and minDCF 
were based on the NIST SRE evaluation plan [12]. The main focus of our efforts was the one con-
versation enroll  one conversation veriﬁcation task for telephone recorded speech. T-Norm models
and Z-Norm [13] speech utterances were drawn from the NIST 2004 SRE corpus. Results were
obtained for both the English only task (Eng) and for all trials (All) which includes speakers that
enroll/verify in different languages.

Feature extraction was performed using HTK [14] with 20 MFCC coefﬁcients  deltas  and accelera-
tion coefﬁcients for a total of 60 features. A GMM UBM with 512 mixture components was trained
using data from NIST SRE 2004 and from Switchboard corpora. The dimension of the nuisance
subspace  U   was ﬁxed at 100; the dimension of the speaker space  V   was ﬁxed at 300.
Results are in Table 1. In the table  we use the following notation 

DUBM = (cid:16)λ1/2 ⊗ In(cid:17) Σ−1/2  D1 = (cid:16)λ

1/2

1 ⊗ In(cid:17) Σ−1/2  D2 = (cid:16)λ

1/2

2 ⊗ In(cid:17) Σ−1/2

(31)

where λ are the UBM mixture weights  λ1 are the mixture weights estimated from the enrollment
utterance  and λ2 are the mixture weights estimated from the veriﬁcation utterance. We also use
the notation DL  DG  and DF to denote the parameters of the metric for the GLDS  Gaussian  and
Fisher comparison functions from Sections 4.2  4.3  and 4.4  respectively.

An analysis of the results in Table 1 shows several trends. First  the performance of the best IPDF
conﬁgurations is as good or better than the state of the art SVM and JFA implementations. Second 
the compensation method that dominates good performance is an orthogonal complement of the
nuisance subspace  QU D. Combining a nuisance projection with an oblique projection is ﬁne  but

7

Table 1: A comparison of baseline systems and different IPDF implementations

Comparison

Function

Baseline SVM

Baseline JFA  CQ

CKL
CKL
CKL
CKL
CKL
CGM
CGM
CGM
KKL
KKL

CGLDS

CG
CF

Enroll
Comp.

QU DUBM
OV U D1
OV U D1
OV U D1
QU D1

QU DUBM

Verify
Comp.

QU DUBM

QU D2
QU D2
OV U D2
QU D2

QU DUBM

I − OU V D1

I − OU V D2

QU D1

QU DUBM
QU DUBM
QU DUBM

QU D1
QU DL
QU DG
QU DF

QU D2

QU DUBM

I

QU DUBM

QU D2
QU DL
QU DG
QU DF

EER

minDCF
All (%) All (×100)

EER

Eng (%)

minDCF

Eng (×100)

3.82
3.07
3.21
8.73
2.93
3.03
7.10
2.90
3.01
3.95
4.95
5.52
3.60
5.07
3.56

1.82
1.57
1.70
5.06
1.55
1.55
3.60
1.59
1.66
1.93
2.46
2.85
1.93
2.52
1.89

2.62
2.11
2.32
8.06
1.89
1.92
6.49
1.73
1.89
2.76
3.73
4.43
2.27
3.89
2.22

1.17
1.23
1.32
4.45
0.93
0.95
3.13
0.98
1.05
1.26
1.75
2.15
1.23
1.87
1.12

Table 2: Summary of some IPDF performances and computation time normalized to a baseline system. Com-
pute time includes compensation and inner product only.

Comparison

Function

CQ
CGM
CGM
CGM

Enroll
Comp.
OV U D1
QU D1

Verify
Comp.
QU D2
QU D2

QU DUBM QU DUBM
QU DUBM

I

EER

Eng (%)

minDCF

Eng (×100)

2.11
1.73
1.89
2.76

1.23
0.98
1.05
1.26

Compute

time
1.00
0.17
0.08
0.04

using only oblique projections onto V gives high error rates. A third observation is that comparison
functions whose metrics incorporate λ1 and λ2 perform signiﬁcantly better than ones with ﬁxed λ
from the UBM. In terms of best performance  CKL  CQ  and CGM perform similarly. For example 
the 95% conﬁdence interval for 2.90% EER is [2.6  3.3]%.
We also observe that a nuisance projection with ﬁxed DUBM gives similar performance to a pro-
jection involving a “variable” metric  Di. This property is fortuitous since a ﬁxed projection can
be precomputed and stored and involves signiﬁcantly reduced computation. Table 2 shows a com-
parison of error rates and compute times normalized by a baseline system. For the table  we used
precomputed data as much as possible to minimize compute times. We see that with an order of
magnitude reduction in computation and a signiﬁcantly simpler implementation  we can achieve the
same error rate.

7 Conclusions and future work

We proposed a new framework for speaker comparison  IPDFs  and showed that several recent sys-
tems in the speaker recognition literature can be placed in this framework. We demonstrated that
using mixture weights in the inner product is the key component to achieve signiﬁcant reductions in
error rates over a baseline SVM system. We also showed that elimination of the nuisance subspace
via an orthogonal projection is a computationally simple and effective method of compensation.
Most effective methods of compensation in the literature (NAP  FA  JFA) are straightforward vari-
ations of this idea. By exploring different IPDFs using these insights  we showed that computation
can be reduced substantially over baseline systems with similar accuracy to the best performing
systems. Future work includes understanding the performance of IPDFs for different tasks  incor-
porating them into an SVM system  and hyperparameter training.

8

References
[1] Douglas A. Reynolds  T. F. Quatieri  and R. Dunn  “Speaker veriﬁcation using adapted Gaussian mixture

models ” Digital Signal Processing  vol. 10  no. 1-3  pp. 19–41  2000.

[2] W. M. Campbell  D. E. Sturim  D. A. Reynolds  and A. Solomonoff  “SVM based speaker veriﬁcation
using a GMM supervector kernel and NAP variability compensation ” in Proc. ICASSP  2006  pp. I97–
I100.

[3] C. Longworth and M. J. F. Gales  “Derivative and parametric kernels for speaker veriﬁcation ” in Proc.

Interspeech  2007  pp. 310–313.

[4] W. M. Campbell  “Generalized linear discriminant sequence kernels for speaker recognition ” in Proc.

ICASSP  2002  pp. 161–164.

[5] P. Kenny  P. Ouellet  N. Dehak  V. Gupta  and P. Dumouchel  “A study of inter-speaker variability in

speaker veriﬁcation ” IEEE Transactions on Audio  Speech and Language Processing  2008.

[6] Ondrej Glembek  Lukas Burget  Najim Dehak  Niko Brummer  and Patrick Kenny  “Comparison of

scoring methods used in speaker recognition with joint factor analysis ” in Proc. ICASSP  2009.

[7] Pedro J. Moreno  Purdy P. Ho  and Nuno Vasconcelos  “A Kullback-Leibler divergence based kernel for
SVM classiﬁcation in multimedia applications ” in Adv. in Neural Inf. Proc. Systems 16  S. Thrun  L. Saul 
and B. Schölkopf  Eds. MIT Press  Cambridge  MA  2004.

[8] Keinosuke Fukunaga  Introduction to Statistical Pattern Recognition  Academic Press  1990.
[9] Simon Lucey and Tsuhan Chen  “Improved speaker veriﬁcation through probabilistic subspace adapta-

tion ” in Proc. Interspeech  2003  pp. 2021–2024.

[10] Robbie Vogt  Brendan Baker  and Sridha Sriharan  “Modelling session variability in text-independent

speaker veriﬁcation ” in Proc. Interspeech  2005  pp. 3117–3120.

[11] Mark J. F. Gales  “Cluster adaptive training of hidden markov models ” IEEE Trans. Speech and Audio

Processing  vol. 8  no. 4  pp. 417–428  2000.

[12] M. A. Przybocki  A. F. Martin  and A. N. Le  “NIST speaker recognition evaluations utilizing the Mixer
corpora—2004 2005 2006 ” IEEE Trans. on Speech  Audio  Lang.  vol. 15  no. 7  pp. 1951–1959  2007.
“Score normalization for text-

[13] Roland Auckenthaler  Michael Carey  and Harvey Lloyd-Thomas 

independent speaker veriﬁcation systems ” Digital Signal Processing  vol. 10  pp. 42–54  2000.

[14] J. Odell  D. Ollason  P. Woodland  S. Young  and J. Jansen  The HTK Book for HTK V2.0  Cambridge

University Press  Cambridge  UK  1995.

9

,Siwei Lyu
Xin Wang
Chris Piech
Jonathan Bassen
Jonathan Huang
Surya Ganguli
Mehran Sahami
Leonidas Guibas
Jascha Sohl-Dickstein