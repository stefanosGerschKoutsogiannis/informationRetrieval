2019,Partially Encrypted Deep Learning using Functional Encryption,Machine learning on encrypted data has received a lot of attention thanks to recent breakthroughs in homomorphic encryption and secure multi-party computation. It allows outsourcing computation to untrusted servers without sacrificing privacy of sensitive data. We propose a practical framework to perform partially encrypted and privacy-preserving predictions which combines adversarial training and functional encryption. We first present a new functional encryption scheme to efficiently compute quadratic functions so that the data owner controls what can be computed but is not involved in the calculation: it provides a decryption key which allows one to learn a specific function evaluation of some encrypted data. We then show how to use it in machine learning to partially encrypt neural networks with quadratic activation functions at evaluation time and we provide a thorough analysis of the information leaks based on indistinguishability of data items of the same label. Last  since several encryption schemes cannot deal with the last thresholding operation used for classification  we propose a training method to prevent selected sensitive features from leaking which adversarially optimizes the network against an adversary trying to identify these features. This is of great interest for several existing works using partially encrypted machine learning as it comes with almost no cost on the model's accuracy and significantly improves data privacy.,Partially Encrypted Machine Learning

using Functional Encryption

Théo Ryffel1  2  Edouard Dufour-Sans1  Romain Gay1 3 

Francis Bach2  1 and David Pointcheval1  2

1Département d’informatique de l’ENS  ENS  CNRS  PSL University  Paris  France

2INRIA  Paris  France

3University of California  Berkeley

{theo.ryffel edufoursans romain.gay francis.bach david.pointcheval}@ens.fr

Abstract

Machine learning on encrypted data has received a lot of attention thanks to recent
breakthroughs in homomorphic encryption and secure multi-party computation. It
allows outsourcing computation to untrusted servers without sacriﬁcing privacy of
sensitive data. We propose a practical framework to perform partially encrypted and
privacy-preserving predictions which combines adversarial training and functional
encryption. We ﬁrst present a new functional encryption scheme to efﬁciently
compute quadratic functions so that the data owner controls what can be computed
but is not involved in the calculation: it provides a decryption key which allows one
to learn a speciﬁc function evaluation of some encrypted data. We then show how
to use it in machine learning to partially encrypt neural networks with quadratic
activation functions at evaluation time  and we provide a thorough analysis of
the information leaks based on indistinguishability of data items of the same
label. Last  since most encryption schemes cannot deal with the last thresholding
operation used for classiﬁcation  we propose a training method to prevent selected
sensitive features from leaking  which adversarially optimizes the network against
an adversary trying to identify these features. This is interesting for several existing
works using partially encrypted machine learning as it comes with little reduction
on the model’s accuracy and signiﬁcantly improves data privacy.

1

Introduction

As both public opinion and regulators are becoming increasingly aware of issues of data privacy  the
area of privacy-preserving machine learning has emerged with the aim of reshaping the way machine
learning deals with private data. Breakthroughs in fully homomorphic encryption (FHE) [15  18] and
secure multi-party computation (SMPC) [19  39] have made computation on encrypted data practical
and implementations of neural networks to do encrypted predictions have ﬂourished [34–36  8  13].
However  these protocols require the data owner encrypting the inputs and the parties performing the
computations to interact and communicate in order to get decrypted results  which we would like
to avoid in some cases  like spam ﬁltering  for example  where the email receiver should not need
to be online for the email server to classify incoming email as spam or not. Functional encryption
(FE) [12  32] in return does not need interaction to compute over encrypted data: it allows users to
receive in plaintext speciﬁc functional evaluations of encrypted data: for a function f  a functional
decryption key can be generated such that  given any ciphertext with underlying plaintext x  a user
can use this key to obtain f (x) without learning x or any other information than f (x). It stands in

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

between traditional public key encryption  where data can only be directly revealed  and FHE  where
data can be manipulated but cannot be revealed: it allows the user to tightly control what is disclosed
about his data.

1.1 Use cases

Spam ﬁltering. Consider the following scenario: Alice uses a secure email protocol which makes
use of functional encryption. Bob uses Alice’s public key to send her an email  which lands on Alice’s
email provider’s server. Alice gave the server keys that enable it to process the email and take a
predeﬁned set of appropriate actions without her being online. The server could learn how urgent the
email is and decide accordingly whether to alert Alice. It could also detect whether the message is
spam and store it in the spam box right away.
Privacy-preserving enforcement of content policies Another use case could be to enable platforms 
such as messaging apps  to maintain user privacy through end-to-end encryption  while ﬁltering out
content that is illegal or doesn’t adhere to policies the site may have regarding  for instance  abusive
speech or explicit images.
These applications are not currently feasible within a reasonable computing time  as the construction
of FE for all kinds of circuits is essentially equivalent to indistinguishable obfuscation [7  21] 
concrete instances of which have been shown insecure  let alone efﬁcient. However  there exist
practical FE schemes for the inner-product functionality [1  2] and more recently for quadratic
computations [6]  that is usable for practical applications.

1.2 Our contributions

We introduce a new FE scheme to compute quadratic forms which outperforms that of Baltico et
al. [6] in terms of complexity  and provide an efﬁcient implementation of this scheme. We show
how to use it to build privacy preserving neural networks  which perform well on simple image
classiﬁcation problems. Speciﬁcally  we show that the ﬁrst layers of a polynomial network can be run
on encrypted inputs using this quadratic scheme.
In addition  we present an adversarial training technique to process these ﬁrst layers to improve
privacy  so that their output  which is in plaintext  cannot be used by adversaries to recover speciﬁc
sensitive information at test time. This adversarial procedure is generic for semi-encrypted neural
networks and aims at reducing the information leakage  as the decrypted output is not directly the
classiﬁcation result but an intermediate layer (i.e.  the neuron outputs of the neural network before
thresholding). This has been overlooked in other popular encrypted classiﬁcation schemes (even in
FHE-based constructions like [20] and [15])  where the argmax operation used to select the class
label is made in clear  as it is either not possible with FE  or quite inefﬁcient with FHE and SMPC.
We demonstrate the practicality of our approach using a dataset inspired from MNIST [27]  which is
made of images of digits written using two different fonts. We show how to perform classiﬁcation of
the encrypted digit images in less than 3 seconds with over 97.7% accuracy while making the font
prediction a hard task for a whole set of adversaries.
This paper builds on a preliminary version available on the Cryptology ePrint Archive at
eprint.iacr.org/2018/206. All code and implementations can be found online at github.com/
LaRiffle/collateral-learning and github.com/edufoursans/reading-in-the-dark.

2 Background Knowledge

2.1 Quadratic and Polynomial Neural Networks

Polynomial neural networks are a class of networks which only use linear elements  like fully
connected linear layers  convolutions but with average pooling  and model activation functions with
polynomial approximations when not simply the square function. Despite these simpliﬁcations  they
have proved themselves satisfactorily accurate for relatively simple tasks ([20] learns on MNIST and
[5] on CIFAR10 [26]). The simplicity of the operations they build on guarantees good efﬁciency 
especially for the gradient computations  and works like [28] have shown that they can achieve
convergence rates similar to those of networks with non-linear activations.

2

In particular  they have been used for several early stage implementations in cryptography [20  18  14]
to demonstrate the usability of new protocols for machine learning. However  the argmax or other
thresholding function present at the end of a classiﬁer network to select the class among the output
neurons cannot be conveniently handled  so several protocol implementations (among which ours)
run polynomial networks on encrypted inputs  but take the argmax over the decrypted output of the
network. This results in potential information leakage which could be maliciously exploited.

2.2 Functional Encryption
Functional encryption extends the notion of public key encryption where one uses a public key pk
and a secret key sk to respectively encrypt and decrypt some data. More precisely  pk is still used
to encrypt data  but for a given function f  sk can be used to derive a functional decryption key dkf
which will be shared to users so that  given a ciphertext of x  they can decrypt f (x) but not x. In
particular  someone having access to dkf cannot learn anything about x other than f (x). Note also
that functions cannot be composed  since the decryption happens within the function evaluation.
Hence  only single quadratic functions can be securely evaluated. A formal deﬁnition of functional
encryption is provided in Appendix A.1.
Perfect correctness. Perfect correctness is achieved in functional encryption: ∀x ∈ X   f ∈ F 
Pr[Dec(dkf   ct) = f (x)] = 1  where dkf ← KeyGen(msk  f ) and ct ← Enc(pk  x). Note that this
property is a very strict condition  which is not satisﬁed by exisiting fully homomorphic encryption
schemes (FHE)  such as [16  22].

Indistinguishability and security

2.3
To assess the security of our framework  we ﬁrst consider the FE scheme security and make sure
that we cannot learn anything more than what the function is supposed to output given an encryption
of x. Second  we analyze how sensitive the output f (x) is with respect to the private input x. For
both studies  we will rely on indistinguishability [23]  a classical security notion which can be
summed up in the following game: an adversary provides two input items to the challenger (here
our FE algorithm)  and the challenger chooses one item to be encrypted  runs encryption on it before
returning the output. The adversary should not be able to detect which input was used. This is known
as IND-CPA security in cryptography and a formal deﬁnition of it can be found in Appendix A.2.
We will ﬁrst prove that our quadratic FE scheme achieves IND-CPA security  then  we will use a
relaxed version of indistinguishability to measure the FE output sensitivity. More precisely  we
will make the hypothesis that our input data can be used to predict public labels but also sensitive
private ones  respectively ypub and ypriv. Our quadratic FE scheme q aims at predicting ypub and an
adversary would rather like to infer ypriv. In this case  the security game consists in the adversary
providing two inputs (x0  x1) labelled with the same ypub but a different ypriv and then trying to
distinguish which one was selected by the challenger  given its output q(xb)  b ∈ {0  1}. One way
to do this is to measure the ability of an adversary to predict ypriv for items which all belong to the
same ypub class.
In particular  note that we do not consider approaches based on input reconstruction (as done by [17])
because in many cases  the adversary is not interested in reconstructing the whole input  but rather
wants to get insights into speciﬁc characteristics.
Another way to see this problem is that we want the sensitive label ypriv to be independent from the
decrypted output q(x) (which is a proxy to the prediction)  given the true public label ypub. This
independence notion is known as separation and is used as a fairness criterion in [9] if the sensitive
features can be misused for discrimination.

3 Our Context for Private Inference
3.1 Classifying in two directions
We are interested in speciﬁc types of datasets ((cid:126)xi)i=1 ... n which have public labels ypub but also
private ones ypriv. Moreover  these different types of labels should be entangled  meaning that they
should not be easily separable  unlike the color and the shape of an object in an image for example
which can be simply separated. For example  in the spam ﬁltering use case mentioned above  ypub
would be a spam ﬂag  and ypriv would be some marketing information highlighting areas of interest

3

of the email recipient like technology  culture  etc. In addition  to simplify our analysis  we assume
that classes are balanced for all types of labels  and that labels are independent from each other given
the input: ∀(cid:126)x  P (ypub  ypriv|(cid:126)x) = P (ypub|(cid:126)x)P (ypriv|(cid:126)x). To illustrate our approach in the case of
image recognition  we propose a synthetic dataset inspired by MNIST which consists of 60 000 grey
scaled images of 28 × 28 pixels representing digits using two fonts and some distortion  as shown in
Figure 1. Here  the public label ypub is the digit on the image and the private one ypriv is the font
used to draw it.

Figure 1: Artiﬁcial dataset inspired from MNIST with two types of labels.

We deﬁne two tasks: a main task which tries to predict ypub using a partially-encrypted polynomial
neural network with functional encryption  and a collateral task which is performed by an adversary
who tries to leverage the output of the FE encrypted network at test time to predict ypriv. Our goal
is to perform the main task with high accuracy while making the collateral one as bad as random
predictions. In terms of indistinguishability  given a dataset with the same digit drawn  it should be
infeasible to detect the used font.

3.2 Equivalence with a Quadratic Functional Encryption scheme

We now introduce our new framework for quadratic functional encryption and show that it can be
used to partially encrypt a polynomial network.

3.2.1 Functional Encryption for Quadratic Polynomials
We build an efﬁcient FE scheme for the set of quadratic functions deﬁned as Fn Bx By Bq ⊂
{q : [−Bx  Bx]n × [−By  By]n → Z}  where q is described as a set of bounded coefﬁcients

{qi j ∈ [−Bq  Bq]}i j∈[n] and for all vectors ((cid:126)x  (cid:126)y)  we have q((cid:126)x  (cid:126)y) =(cid:80)

i j∈[n] qi jxiyj.

A complete version of our scheme is given in Figure 2  but here are the main ideas and notations.
First note that we use bilinear groups  i.e.  a set of prime-order groups G1  G2 and GT together
with a bilinear map e : G1 × G2 → GT called pairing which satisﬁes e(ga
2) = e(g1  g2)ab for
any exponents a  b ∈ Z: one can compute quadratic polynomials in the exponent. Here  g1  g2 are
generators of G1 and G2 and gT := e(g1  g2) is a generator of the target group GT . A pair of vectors
((cid:126)s  (cid:126)t) is ﬁrst selected and constitutes the private key msk  while the public key is (g(cid:126)s

1   gb

1  g(cid:126)t
2).

T

2  which allows any user to
Encrypting ((cid:126)x  (cid:126)y) roughly consists of masking g(cid:126)x
compute gq((cid:126)x (cid:126)y)−q((cid:126)s (cid:126)t)
with for any quadratic function q  using the pairing. The functional decryption
key for a speciﬁc q is gq((cid:126)s (cid:126)t)
. Last  taking the discrete logarithm gives
access to q((cid:126)x  (cid:126)y) (discrete logarithm for small exponents is easy). Security uses the fact that it is
hard to compute msk from pk (discrete logarithm for large exponents (cid:126)s  (cid:126)t is hard to compute). More
details are given in Appendix B.11

which allows to get gq((cid:126)x (cid:126)y)

2 with g(cid:126)s

1 and g(cid:126)y

1 and g(cid:126)t

T

T

Theorem 3.1 (Security  correctness and complexity) The FE scheme provided in Figure 2:

• is IND-CPA secure in the Generic Bilinear Group Model 
• veriﬁes log(out) = q((cid:126)x  (cid:126)y) and satisﬁes perfect correctness 
• has a overall decryption complexity of 2n2(E + P ) + P + D 

where E  P and D respectively denote exponentiation  pairing and discrete logarithm complexities.

Our scheme outperforms previous schemes for quadratic FE with the same security assumption  like
the one from [6  Sec. 4] which achieves 3n2(E + P ) + 2P + D complexity and uses larger ciphertexts
and decryption keys. Note that the efﬁciency of the decryption can even be further optimized for
those quadratic polynomials used that are relevant to our application (see Section 3.2.2).

1Note that we only present a simpliﬁed scheme here. In particular  the actual encryption is randomized 

which is necessary to achieve IND-CPA security.

4

SetUp(1λ Fn Bx By Bf ):
PG := (G1  G2  p  g1  g2  e) ← GGen(1λ)  (cid:126)s  (cid:126)t $← Zn
Return (pk  msk).

Enc(cid:0)pk  ((cid:126)x  (cid:126)y)(cid:1):
γ $← Zp  W $← GL2  for all i ∈ [n]  (cid:126)ai := (W−1)(cid:62)(cid:18) xi
(cid:16)

Return ct :=

γsi
1 × G2
2)n

p   msk := ((cid:126)s  (cid:126)t)  pk :=

(cid:19)

  (cid:126)bi := W

(cid:18) yi−ti

(cid:17)

1  g(cid:126)t

2

(cid:16)PG  g(cid:126)s
(cid:19)

1  {g(cid:126)ai
gγ
(cid:16)
(cid:16)

gq((cid:126)s (cid:126)t)

2

1  {g(cid:126)ai
gγ

(cid:126)bi

1   g

2 }i∈[n]

(cid:17) ∈ G1 × (G2
(cid:17) ∈ G2 × Fn Bx By Bq.
(cid:16)
(cid:17)
(cid:1)qi j
i j∈[n] e(cid:0)g(cid:126)ai
) ·(cid:81)

2 }i∈[n]

(cid:126)bj
1   g
2

  dkq :=

1   g

  q

(cid:126)bi

KeyGen(msk  q):
Return dkf :=

(cid:16)

Dec

pk  ct :=

1   gq((cid:126)s (cid:126)t)
out := e(gγ
Return log(out) ∈ Z.

2

(cid:17)(cid:17)

:

  q

gq((cid:126)s (cid:126)t)

2

Figure 2: Our functional encryption scheme for quadratic polynomials.

Computing the discrete logarithm for decryption. Our decryption requires computing discrete
logarithms of group elements in base gT   but contrary to previous works like [25] it is independent of
the ciphertext and the functional decryption key used to decrypt. This allows to pre-compute values
and dramatically speeds-up decryption.

3.2.2 Equivalence of the FE scheme with a Quadratic Network
We classify data which can be represented as a vector (cid:126)x ∈ [0  B]n (in our case  the size B = 255 
and the dimension n = 784) and we ﬁrst build models (qi)i∈[(cid:96)] for each public label i ∈ [(cid:96)]  such
that our prediction ypub for (cid:126)x is argmaxi∈[(cid:96)] qi((cid:126)x).
Quadratic polynomial on Rn. The most straightforward way to use our FE scheme would be for
us to learn a model (Qi)i∈[(cid:96)] ∈ (Rn×n)(cid:96)  which we would then round onto integers  such that
qi((cid:126)x) = (cid:126)x(cid:62)Qi(cid:126)x  ∀i ∈ [(cid:96)]. This is a unnecessarily powerful model in the case of MNIST as it has (cid:96)n2
parameters (n = 784)  and the resulting number of pairings to compute would be unreasonably large.
Linear homomorphism. The encryption algorithm of our FE scheme is linearly homomorphic with
respect to the plaintext: given an encryption of ((cid:126)x  (cid:126)y) under the secret key msk := ((cid:126)s  (cid:126)t)  one can
efﬁciently compute an encryption of ((cid:126)u(cid:62)(cid:126)x  (cid:126)v(cid:62)(cid:126)y) under the secret key msk
:= ((cid:126)u(cid:62)(cid:126)s  (cid:126)v(cid:62)(cid:126)t) for any
linear combination (cid:126)u  (cid:126)v (see proof in Appendix B.2). Any vector (cid:126)v is a column  and (cid:126)v(cid:62) is a row.
Therefore  if q can be written q((cid:126)x  (cid:126)y) = (U(cid:126)x)(cid:62)M(V(cid:126)y) for all ((cid:126)x  (cid:126)y)  with U  V ∈ Zd×n
projection
matrices and M ∈ Zd×d
  it is more efﬁcient to ﬁrst compute the encryption of (U(cid:126)x  V(cid:126)y) from the
encryption of ((cid:126)x  (cid:126)y)  and then to apply the functional decryption on these ciphertexts  because their
underlying plaintexts are of reduced dimension d < n. This reduces the number of exponentiations
from 2n2 to 2dn and the number of pairing computations from 2n2 to 2d2 for a single qi. This is a
major efﬁciency improvement for small d  as pairings are the main bottleneck in the computation.
Projection and quadratic polynomial on Rd. We can use this and apply the quadratic polynomials

on projected vectors: we learn P ∈ Rn×d and (Qi)i∈[(cid:96)] ∈ (cid:0)Rd×d(cid:1)(cid:96)  and our model is qi((cid:126)x) =

(P(cid:126)x)(cid:62)Qi(P(cid:126)x)  ∀i ∈ [(cid:96)]. We only need 2(cid:96)d2 pairings and since the same P is used for all qi  we only
compute once the encryption of P(cid:126)x from the encryption of (cid:126)x. Better yet  we can also perform the
pairings only once  and then compute the scores by exponentiating with different coefﬁcients the
same results of the pairings  thus only requiring 2d2 pairing evaluations  independently of (cid:96).

p

p

(cid:48)

5

Degree 2 polynomial network  with one hidden layer. To further reduce the number of pairings 
we actually limit ourselves to diagonal matrices  and thus rename Qi to Di. We ﬁnd that the gain
in efﬁciency associated with only computing 2d pairings is worth the small drop in accuracy. The
resulting model is actually a polynomial network of degree 2 with one hidden layer of d neurons and
the activation function is the square. In the following experiments we take d = 40.
Our ﬁnal encrypted model can thus be written as qi((cid:126)x) = (P(cid:126)x)(cid:62)Di(P(cid:126)x) ∀i ∈ [(cid:96)]  where we add a
bias term to (cid:126)x by replacing it with (cid:126)x = (1 x1 . . . xn).
Full network. The result of the quadratic (qi((cid:126)x))i∈[(cid:96)] (i.e.  of the private quadratic network) is now
visible in clear. As mentioned above  we cannot compose this block several times as it contains
decryption  so this is currently the best that we can have as an encrypted computation with FE. Instead
of simply applying the argmax to the cleartext output of this privately-evaluated quadratic network to
get the label  we observe that adding more plaintext layers on top of it helps improving the overall
accuracy of the main task. We have therefore a neural network composed of a private and a public
part  as illustrated in Figure 3.

Figure 3: Semi-encrypted net-
work using quadratic FE.

Figure 4: Semi-encrypted network with an adversary trying
to recover private labels from the private quadratic network.

3.3 Threat of Collateral Learning

A typical adversary would have a read access to the main task classiﬁcation process. It would leverage
the output of the quadratic network to try to learn the font used on ciphered images. To do this  all
that is needed is to train another network on top of the quadratic network so that it learns to predict
the font  assuming some access to labeled samples (which is the case if the adversary encrypts itself
images and provides them to the main task at evaluation time). Note that in this case the private
network is not updated by the collateral network as we assume it is only provided in read access after
the main task is trained. Figure 4 summarizes the setting.
We implemented this scenario using as adversary a neural network composed of a ﬁrst layer acting
as a decompression step where we increase the number of neurons from 10 back to 28 × 28 and
add on top of it a classical2 convolutional neural network (CNN). This structure is reminiscent of
autoencoders [38] where the bottleneck is the public output of the private net and the challenge of
this autoencoder is to correctly memorize the public label while forgetting the private one. What we
observed is striking: in less than 10 epochs  the collateral network leverages the 10 public neurons
output and achieves 93.5% accuracy for the font prediction. As expected  it gets even worse when
the adversary is assessed with the indistinguishability criterion because in that case the adversary
can work on a dataset where only a speciﬁc digit is represented: this reduces the variability of the
samples and makes it easier to distinguish the font; the probability of success is indeed of 96.9%.
We call collateral learning this phenomenon of learning unexpected features and will show in the
next section how to implement counter-measures to this threat in order to improve privacy.

2https://github.com/pytorch/examples/blob/master/mnist/main.py

6

4 Defeating Collateral Learning
4.1 Reducing information leakage

Our ﬁrst approach is based on the observation that we leak many bits of information. We ﬁrst
investigate whether we can reduce the number of outputs of the privately-evaluted network  as
adding extra layers on top of the private network makes it no longer necessary to keep 10 of them.
The intuition is that if the information that is
relevant to the main task can ﬁt in less than
10 neurons  then the extra neurons would leak
unnecessary information. We have therefore a
trade-off between reducing too much and losing
desired information or keeping a too large out-
put and having an important leakage. We can
observe this through the respective accuracies
as it is shown in Figure 5  where the main and
adversarial networks are CNNs as in Section 3.3
with 10 epochs of training using 7-fold cross
validation. What we observe here is interesting:
the main task does not exhibit signiﬁcant weak-
nesses even with size 3 where we drop to 97.1%
which is still very good although 2% under the
best accuracy. In return  the collateral accuracy
starts to signiﬁcantly decrease when output size
is below 7. At size 4  it is only 76.4% on aver-
age so 18% less than the baseline. We will keep
an output size of 3 or 4 for the next experiments to keep the main accuracy almost unchanged.
Another hyperparameter that we can consider is the weight compression: how many bits do we need
to represent the weights on the private networks layers? This is of interest for the FE scheme as we
need to convert all weights to integers and those integers will be low provided that the compression
rate is high. Small weight integers mean that the output of the private network has a relatively low
amplitude and can be therefore efﬁciently decrypted using discrete logarithm. We managed to express
all weights and even the input image using 4 bit values with limited impact on the main accuracy and
almost none on the collateral one. Details about compression can be found in Appendix C.1.

Figure 5: Trade-off between main and collateral
accuracies depending on the private output size.

4.2 Adversarial training

We propose a new approach to actively adapt against collateral learning. The main idea is to simulate
adversaries and to try to defeat them. To do this  we use semi-adversarial training and optimize
simultaneously the main classiﬁcation objective and the opposite of the collateral objective of a given
simulated adversary. The function that we want to minimize at each iteration step can be written:

Lpub(θq  θpub) − α min

Lpriv(θq  θpub)].

min
θq

[min
θpub

θpub

This approach is inspired from [29] where the authors train some objective against nuisances parame-
ters to build a classiﬁer independent from these nuisances. Private features leaking in our scheme can
indeed be considered to be a nuisance. However  our approach goes one step further as we do not just
stack a network above another; our global network structure is fork-like: the common basis is the
private network and the two forks are the main and collateral classiﬁers. This allows us to have a
better classiﬁer for the main task which is not as sensitive to the adversarial training as the scheme
exposed by [29  Figure 1]. One other difference is that the collateral classiﬁer is a speciﬁc modeling
of an adversary  and we will discuss this in details in the next section. We deﬁne in Figure 6 the
3-step procedure used to implement this semi-adversarial training using partial back-propagation.

5 Experimental Results

Accurate main task and poor collateral results. In Figures 7 and 8 we show that the output size
has an important inﬂuence on the two tasks’ performances. For this experiment  we use α = 1.7 as
detailed in Appendix C.2  the adversary uses the same CNN as stated above and the main network is

7

Pre-training: Initial phase where both tasks learn and strengthen before the joint optimization
Minimize Lpub(θq  θpub)
Minimize Lpriv(Frozen(θq)  θpriv)

Semi-adversarial training: The joint optimization phase  where θpub and θpriv are updated depending on
the variations of θq and θq is optimized to reduce the loss L = Lpub − αLpriv
Minimize Lpub(Frozen(θq)  θpub)
Minimize Lpriv(Frozen(θq)  θpriv)
Minimize L = Lpub(θq  Frozen(θpub)) − αLpriv(θq  Frozen(θpub))

Recover phase: Both tasks recover from the perturbations induced by the adversarial phase  θq does not
change anymore
Minimize Lpub(Frozen(θq)  θpub)
Minimize Lpriv(Frozen(θq)  θpriv)

Figure 6: Our semi-adversarial training scheme.

Figure 7: Inﬂuence of the output size on the main
task accuracy with adversarial training.

Figure 8: Inﬂuence of the output size on the col-
lateral task accuracy with adversarial training.

a simple feed forward network (FFN) with 4 layers. We observe that both networks behave better
when the output size increases  but the improvement is not synchronous which makes it possible to
have a main task with high accuracy while the collateral task is still very inaccurate. In our example 
this corresponds to an output size between 3 and 5. Note that the collateral result is the accuracy
at the distinction task  i.e.  the digit is ﬁxed for the adversary which trains to distinguish two fonts
during a 50 epoch recover phase using 7-fold cross validation  after 50 epochs of semi-adversarial
training have been spent to reduce leakage from the private network.
Generalizing resistance against multiple adversaries. In practice  it is very likely that the adver-
sary will use a different model than the one against which the protection has been built. We have
therefore investigated how building resistance against a model M can provide resistance against other
models. Our empirical results tend to show that models with less parameters than M do not perform
well. In return  models with more parameters can behave better  provided that the complexity does
not get excessive for the considered task  because it would not provide any additional advantage and
would just lead to learning noise. In particular  the CNN already mentioned above seems to be a
sufﬁciently complex model to resist against a wide range of feed forward (FFN) and convolutional
networks  as illustrated in Figure 9 where the measure used is indistinguishability of the font for a
ﬁxed digit. This study is not exhaustive as the adversary can change the activation function (here we
use relu) or even the training parameters (optimizer  batch size  dropout  etc.)  but these do not seem
to provide any decisive advantage.
We also assessed the resistance to a large range of other models from the sklearn library [33] and
report the collateral accuracy in Figure 10. As can be observed  some models such as k-nearest
neighbors or random forests perform better compared to neural networks  even if their accuracy
remains relatively low. One reason can be that they operate in a very different manner compared
to the model on which the adversarial training is performed: k-nearest neighbors for example just
considers distances between points.
Runtime. Training in semi-adversarial mode can take quite a long time depending on the level
of privacy one wants to achieve. However  the runtime during the test phase is much faster  it is
dominated by the FE scheme part which can be broken down to 4 steps: functional key generation 

8

Linear Ridge Regression
Logistic Regression
Quad. Discriminant Analysis
SVM (RBF kernel)
Gaussian Process Classiﬁer
Gaussian Naive Bayes
K-Neighbors Classiﬁer
Decision Tree Classiﬁer
Random Forest Classiﬁer
Gradient Boosting Classiﬁer

53.5 ± 0.5%
52.5 ± 0.6%
54.9 ± 0.3%
57.9 ± 0.4%
53.8 ± 0.3%
53.2 ± 0.5%
58.1 ± 0.7%
56.8 ± 0.4%
58.9 ± 0.2%
58.9 ± 0.2%
Figure 10: Accuracy on the distinction task for
different adversarial learning models.

Figure 9: Collateral accuracy depending of the
adversarial network complexity seen as the log of
the number of parameters.

encryption of the input  evaluation of the function and discrete logarithm. Regarding encryption and
evaluation  the main overhead comes from the exponentiations and pairings which are implemented in
the crypto library charm [3]. In return  the discrete logarithm is very efﬁcient thanks to the reduction
of the weights amplitude detailed in Figure 4.1.

Functional key generation
Encryption time

94 ± 5ms
12.1 ± 0.3 s

Evaluation time
Discrete logarithms time

2.97 ± 0.07s
24 ± 9ms

Table 1: Average runtime for the FE scheme using a 2 7 GHz Intel Core i7 and 16GB of RAM.

Table 1 shows that encryption time is longer than evaluation time  but a single encryption can be used
with several decryption keys dkqi to perform multiple evaluation tasks.

6 Conclusion

We have shown that functional encryption can be used for practical applications where machine
learning is used on sensitive data. We have raised awareness about the potential information leakage
when not all the network is encrypted and have proposed semi-adversarial training as a solution to
prevent targeted sensitive features from leaking for a vast family of adversaries.
However  it remains an open problem to provide privacy-preserving methods for all features except
the public ones as they can be hard to identify in advance. On the cryptography side  extension of the
range of functions supported in functional encryption would help increase provable data privacy  and
adding the ability to hide the function evaluated would be of interest for sensitive neural networks.

Acknowledgments

This work was supported in part by the European Community’s Seventh Framework Programme
(FP7/2007-2013 Grant Agreement no. 339563 – CryptoCloud)  the European Community’s Horizon
2020 Project FENTEC (Grant Agreement no. 780108)  the Google PhD fellowship  and the French
FUI ANBLIC Project.

References
[1] Michel Abdalla  Florian Bourse  Angelo De Caro  and David Pointcheval. Simple functional
encryption schemes for inner products. In Jonathan Katz  editor  PKC 2015  volume 9020 of
LNCS  pages 733–751. Springer  Heidelberg  March / April 2015.

[2] Shweta Agrawal  Benoît Libert  and Damien Stehlé. Fully secure functional encryption for
inner products  from standard assumptions. In Matthew Robshaw and Jonathan Katz  editors 
CRYPTO 2016  Part III  volume 9816 of LNCS  pages 333–362. Springer  Heidelberg  August
2016.

9

[3] Joseph A. Akinyele  Christina Garman  Ian Miers  Matthew W. Pagano  Michael Rushanan 
Matthew Green  and Aviel D. Rubin. Charm: a framework for rapidly prototyping cryptosystems.
Journal of Cryptographic Engineering  3(2):111–128  2013.

[4] Miguel Ambrona  Gilles Barthe  Romain Gay  and Hoeteck Wee. Attribute-based encryption in
the generic group model: Automated proofs and new constructions. In Bhavani M. Thurais-
ingham  David Evans  Tal Malkin  and Dongyan Xu  editors  ACM CCS 2017  pages 647–664.
ACM Press  October / November 2017.

[5] Ahmad Al Badawi  Jin Chao  Jie Lin  Chan Fook Mun  Jun Jie Sim  Benjamin Hong Meng
Tan  Xiao Nan  Khin Mi Mi Aung  and Vijay Ramaseshan Chandrasekhar. The alexnet moment
for homomorphic encryption: Hcnn  the ﬁrst homomorphic cnn on encrypted data with gpus.
Cryptology ePrint Archive  Report 2018/1056  2018.

[6] Carmen Elisabetta Zaira Baltico  Dario Catalano  Dario Fiore  and Romain Gay. Practical
functional encryption for quadratic functions with applications to predicate encryption. In
Jonathan Katz and Hovav Shacham  editors  CRYPTO 2017  Part I  volume 10401 of LNCS 
pages 67–98. Springer  Heidelberg  August 2017.

[7] Boaz Barak  Oded Goldreich  Russell Impagliazzo  Steven Rudich  Amit Sahai  Salil P. Vad-
han  and Ke Yang. On the (im)possibility of obfuscating programs. In Joe Kilian  editor 
CRYPTO 2001  volume 2139 of LNCS  pages 1–18. Springer  Heidelberg  August 2001.

[8] Mauro Barni  Pierluigi Failla  Riccardo Lazzeretti  Ahmad-Reza Sadeghi  and Thomas Schnei-
der. Privacy-preserving ecg classiﬁcation with branching programs and neural networks. IEEE
Transactions on Information Forensics and Security  6(2):452–468  2011.

[9] Solon Barocas  Moritz Hardt  and Arvind Narayanan. Fairness and Machine Learning. fairml-

book.org  2018.

[10] Gilles Barthe  Edvard Fagerholm  Dario Fiore  John C. Mitchell  Andre Scedrov  and Benedikt
Schmidt. Automated analysis of cryptographic assumptions in generic group models. In Juan A.
Garay and Rosario Gennaro  editors  CRYPTO 2014  Part I  volume 8616 of LNCS  pages
95–112. Springer  Heidelberg  August 2014.

[11] Dan Boneh and Matthew K. Franklin. Identity based encryption from the Weil pairing. SIAM

Journal on Computing  32(3):586–615  2003.

[12] Dan Boneh  Amit Sahai  and Brent Waters. Functional encryption: Deﬁnitions and challenges.
In Yuval Ishai  editor  TCC 2011  volume 6597 of LNCS  pages 253–273. Springer  Heidelberg 
March 2011.

[13] Raphael Bost  Raluca Ada Popa  Stephen Tu  and Shaﬁ Goldwasser. Machine learning classiﬁ-

cation over encrypted data. In NDSS  volume 4324  page 4325  2015.

[14] Florian Bourse  Michele Minelli  Matthias Minihold  and Pascal Paillier. Fast homomorphic
evaluation of deep discretized neural networks. Cryptology ePrint Archive  Report 2017/1114 
2017. https://eprint.iacr.org/2017/1114.

[15] Florian Bourse  Michele Minelli  Matthias Minihold  and Pascal Paillier. Fast homomorphic
evaluation of deep discretized neural networks. In Advances in Cryptology - CRYPTO 2018
- 38th Annual International Cryptology Conference  Santa Barbara  CA  USA  August 19-23 
2018  Proceedings  Part III  pages 483–512  2018.

[16] Z. Brakerski and V. Vaikuntanathan. Efﬁcient fully homomorphic encryption from (standard)
lwe. In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science  pages
97–106  Oct 2011.

[17] Sergiu Carpov  Caroline Fontaine  Damien Ligier  and Renaud Sirdey. Illuminating the dark or

how to recover what should not be seen. IACR Cryptology ePrint Archive  2018:1001  2018.

[18] Ilaria Chillotti  Nicolas Gama  Mariya Georgieva  and Malika Izabachène. Faster fully homo-
morphic encryption: Bootstrapping in less than 0.1 seconds. In Jung Hee Cheon and Tsuyoshi
Takagi  editors  Advances in Cryptology – ASIACRYPT 2016  pages 3–33  Berlin  Heidelberg 
2016. Springer Berlin Heidelberg.

10

[19] Ivan Damgård  Valerio Pastro  Nigel Smart  and Sarah Zakarias. Multiparty computation
from somewhat homomorphic encryption. In Reihaneh Safavi-Naini and Ran Canetti  editors 
Advances in Cryptology – CRYPTO 2012  pages 643–662  Berlin  Heidelberg  2012. Springer
Berlin Heidelberg.

[20] Nathan Dowlin  Ran Gilad-Bachrach  Kim Laine  Kristin Lauter  Michael Naehrig  and John
Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and
accuracy. Technical report  February 2016.

[21] Sanjam Garg  Craig Gentry  Shai Halevi  Mariana Raykova  Amit Sahai  and Brent Waters.
Candidate indistinguishability obfuscation and functional encryption for all circuits. In 54th
FOCS  pages 40–49. IEEE Computer Society Press  October 2013.

[22] Craig Gentry  Amit Sahai  and Brent Waters. Homomorphic encryption from learning with
errors: Conceptually-simpler  asymptotically-faster  attribute-based. In Ran Canetti and Juan A.
Garay  editors  CRYPTO 2013  Part I  volume 8042 of LNCS  pages 75–92. Springer  Heidelberg 
August 2013.

[23] Shaﬁ Goldwasser and Silvio Micali. Probabilistic encryption. Journal of Computer and System

Sciences  28(2):270–299  1984.

[24] Antoine Joux. A one round protocol for tripartite Difﬁe-Hellman. Journal of Cryptology 

17(4):263–276  September 2004.

[25] Sam Kim  Kevin Lewi  Avradip Mandal  Hart Montgomery  Arnab Roy  and David J. Wu.
Function-hiding inner product encryption is practical. In Dario Catalano and Roberto De Prisco 
editors  SCN 18  volume 11035 of LNCS  pages 544–562. Springer  Heidelberg  September
2018.

[26] Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto 

05 2012.

[27] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.

[28] Roi Livni  Shai Shalev-Shwartz  and Ohad Shamir. On the computational efﬁciency of training

neural networks. CoRR  abs/1410.1141  2014.

[29] Gilles Louppe  Michael Kagan  and Kyle Cranmer. Learning to pivot with adversarial networks.
In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Gar-
nett  editors  Advances in Neural Information Processing Systems 30  pages 981–990. Curran
Associates  Inc.  2017.

[30] Ueli M. Maurer. Abstract models of computation in cryptography (invited paper). In Nigel P.
Smart  editor  10th IMA International Conference on Cryptography and Coding  volume 3796
of LNCS  pages 1–12. Springer  Heidelberg  December 2005.

[31] V. I. Nechaev. Complexity of a determinate algorithm for the discrete logarithm. Mathematical

Notes  55(2):165–172  1994.

[32] Adam O’Neill. Deﬁnitional issues in functional encryption. Cryptology ePrint Archive  Report

2010/556  2010.

[33] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research  12:2825–2830  2011.

[34] M. Sadegh Riazi  Christian Weinert  Oleksandr Tkachenko  Ebrahim M. Songhori  Thomas
Schneider  and Farinaz Koushanfar. Chameleon: A hybrid secure computation framework for
machine learning applications. In Proceedings of the 2018 on Asia Conference on Computer
and Communications Security  ASIACCS ’18  pages 707–721  New York  NY  USA  2018.
ACM.

11

[35] Theo Ryffel  Andrew Trask  Morten Dahl  Bobby Wagner  Jason Mancuso  Daniel Rueckert 
and Jonathan Passerat-Palmbach. A generic framework for privacy preserving deep learning.
CoRR  abs/1811.04017  2018.

[36] Microsoft SEAL (release 3.2). https://github.com/Microsoft/SEAL  February 2019.

Microsoft Research  Redmond  WA.

[37] Victor Shoup. Lower bounds for discrete logarithms and related problems. In Walter Fumy 
editor  EUROCRYPT’97  volume 1233 of LNCS  pages 256–266. Springer  Heidelberg  May
1997.

[38] Pascal Vincent  Hugo Larochelle  Yoshua Bengio  and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In Proceedings of the 25th International
Conference on Machine Learning  ICML ’08  pages 1096–1103  New York  NY  USA  2008.
ACM.

[39] Sameer Wagh  Divya Gupta  and Nishanth Chandran. Securenn: Efﬁcient and private neural

network training. (PETS 2019)  February 2019.

12

,Neal Jean
Sang Michael Xie
Stefano Ermon
Théo Ryffel
David Pointcheval
Francis Bach
Edouard Dufour-Sans
Romain Gay