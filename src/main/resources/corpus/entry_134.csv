2016,A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization,In this paper  we propose a multi-step inertial Forward--Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions  one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the scheme with the help of the Kurdyka–Łojasiewicz property. Then  when the non-smooth part is also partly smooth relative to a smooth submanifold  we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on a few problems arising from statistics and machine learning.,A Multi-step Inertial Forward–Backward Splitting

Method for Non-convex Optimization

Jingwei Liang and Jalal M. Fadili

Normandie Univ  ENSICAEN  CNRS  GREYC

{Jingwei.Liang Jalal.Fadili}@greyc.ensicaen.fr

Gabriel Peyré

CNRS  DMA  ENS Paris
Gabriel.Peyre@ens.fr

Abstract

We propose a multi-step inertial Forward–Backward splitting algorithm for mini-
mizing the sum of two non-necessarily convex functions  one of which is proper
lower semi-continuous while the other is differentiable with a Lipschitz continuous
gradient. We ﬁrst prove global convergence of the algorithm with the help of the
Kurdyka-Łojasiewicz property. Then  when the non-smooth part is also partly
smooth relative to a smooth submanifold  we establish ﬁnite identiﬁcation of the
latter and provide sharp local linear convergence analysis. The proposed method is
illustrated on several problems arising from statistics and machine learning.

1

Introduction

1.1 Non-convex non-smooth optimization

Non-smooth optimization has proved extremely useful to all quantitative disciplines of science
including statistics and machine learning. A common trend in modern science is the increase in size
of datasets  which drives the need for more efﬁcient optimization schemes. For large-scale problems
with non-smooth and possibly non-convex terms  it is possible to generalize gradient descent with
the Forward–Backward (FB) splitting scheme [3] (a.k.a proximal gradient descent)  which includes
projected gradient descent as a sub-case.
Formally  we equip Rn the n-dimensional Euclidean space with the standard inner product (cid:104)·  ·(cid:105) and
associated norm || · || respectively. Our goal is the generic minimization of composite objectives of
the form
(P)

(cid:8)Φ(x) def= R(x) + F (x)(cid:9) 

min
x∈Rn

where we have

(A.1) R : Rn → R ∪ {+∞} is the penalty function which is proper lower semi-continuous (lsc) 
(A.2) F : Rn → R is the loss function which is ﬁnite-valued  differentiable and its gradient ∇F

and bounded from below;

is L-Lipschitz continuous.

Throughout  no convexity is imposed neither on R nor on F .
The class of problems we consider is that of non-smooth and non-convex optimization problems.
Here are some examples that are of particular relevance to problems in regression  machine learning
and classiﬁcation.
Example 1.1 (Sparse regression). Let A ∈ Rm×n  y ∈ Rm  µ > 0  and ||x||0 is the (cid:96)0 pseudo-norm
(see Example 4.1). Consider (see e.g. [11])

min
x∈Rn

1
2

||y − Ax||2 + µ||x||0.

(1.1)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Example 1.2 (Principal component pursuit (PCP)). The PCP problem [9] aims at decomposing a
given matrix into sparse and low-rank components

min

(xs xl)∈(Rn1×n2 )2

||y − xs − xl||2
where || · ||F is the Frobenius norm and µ1 and µ2 > 0.
Example 1.3 (Sparse Support Vector Machines). One would like to ﬁnd a linear decision function
which minimizes the objective

F + µ1||xs||0 + µ2rank(xl) 

(1.2)

1
2

1
m

min

i=1 G((cid:104)x  zi(cid:105) + b  yi) + µ||x||0 

(b x)∈R×Rn

(1.3)
where for i = 1 ···   m  (zi  yi) ∈ Rn × {±1} is the training set  and G is a smooth loss function
with Lipschitz-continuous gradient such as the squared hinge loss G(ˆyi  yi) = max(0  1 − ˆyiyi)2 or
the logistic loss G(ˆyi  yi) = log(1 + e−ˆyiyi).
(Inertial) Forward–Backward The Forward–Backward splitting method for solving (P) reads

(cid:0)xk − γk∇F (xk)(cid:1) 

where γk > 0 is a descent step-size  and

xk+1 ∈ proxγkR

(cid:80)m

(1.4)

(1.5)

proxγR(·) def= Argminx∈Rn

1
2

||x − ·||2 + γR(x) 

denotes the proximity operator of R. proxγR(x) is non-empty under (A.1) and is set-valued in
general. Lower-boundedness of R can be relaxed by requiring e.g. coercivity of the objective in (1.5).
Since the pioneering work of Polyak [24] on the heavy-ball method approach to gradient descent 
several works have adapted this methodology to various optimization schemes. For instance  the
inertial proximal point algorithm [1  2]  or the inertial FB methods [22  21  4  20]. The FISTA scheme
[5  10] also belongs to this class. See [20] for a detailed account.
The non-convex case In the context of non-convex optimization  [3] was the ﬁrst to establish
convergence of the FB iterates when the objective Φ satisﬁes the Kurdyka-Łojasiewicz property1.
Following their footprints  [8  23] established convergence of the special inertial schemes in [22] in
the non-convex setting.

1.2 Contributions

In this paper  we introduce a novel inertial scheme (Algorithm 1) and study its global and local
properties to solve the non-smooth and non-convex optimization problem (P). More precisely  our
main contributions can be summarized as follows.
A globally convergent general inertial scheme We propose a general multi-step inertial FB (MiFB)
algorithm to solve (P). This algorithm is very ﬂexible as it allows higher memory and even negative
inertial parameters (unlike previous work [20]). Global convergence of any bounded sequence
of iterates to a critical point is proved when the objective Φ is lower-bounded and satisﬁes the
Kurdyka-Łojasiewicz property.
Local convergence properties under partial smoothness Under the additional assumptions that
the smooth part is locally C 2 around a critical point x(cid:63) (where xk → x(cid:63))  and that the non-smooth
component R is partly smooth (see Deﬁnition 3.1) relative to an active submanifold Mx(cid:63)  we show
that Mx(cid:63) can be identiﬁed in ﬁnite time  i.e. xk ∈ Mx(cid:63) for all k large enough. Building on this
ﬁnite identiﬁcation result  we provide a sharp local linear convergence analysis and we characterize
precisely the corresponding convergence rate which  in particular  reveals the role of Mx(cid:63). Moreover 
this local convergence analysis naturally opens the door to higher-order acceleration  since under
some circumstances  the original problem (P) is eventually equivalent to locally minimizing Φ on
Mx(cid:63)  and partial smoothness implies that Φ is actually C 2 on Mx(cid:63).

1We are aware of the works existing on convergence of the objective sequence Φ(xk) of FB  including
rates  in the non-smooth and non-convex setting. But given that  in general  this does not say anything about
convergence of the sequence of iterates xk  they are irrelevant to our discussion.

2

Let 0 < γ ≤ γk ≤ γ < 1

Algorithm 1: A Multi-step Inertial Forward–Backward (MiFB)
Initial: s ≥ 1 is an integer  I = {0  1  . . .   s − 1}  x0 ∈ Rn and x−s = . . . = x−1 = x0.
repeat
L  {a0 k  a1 k  . . .} ∈] − 1  2]s  {b0 k  b1 k  . . .} ∈] − 1  2]s:
ya k = xk +(cid:80)
yb k = xk +(cid:80)
xk+1 ∈ proxγkR

(cid:0)ya k − γk∇F (yb k)(cid:1).

i∈I ai k(xk−i − xk−i−1) 
i∈I bi k(xk−i − xk−i−1) 

(1.6)

(1.7)

k = k + 1;

until convergence;

1.3 Notations
Throughout the paper  N is the set of non-negative integers. For a nonempty closed convex set
Ω ⊂ Rn  ri(Ω) is its relative interior  and par(Ω) = R(Ω − Ω) is the subspace parallel to it.
Let R : Rn → R ∪ {+∞} be a lsc function  its domain is deﬁned as dom(R) def= {x ∈ Rn : R(x) <
+∞}  and it is said to be proper if dom(R) (cid:54)= ∅. We need the following notions from variational
analysis  see e.g. [25] for details. Given x ∈ dom(R)  the Fréchet subdifferential ∂F R(x) of R at x 
is the set of vectors v ∈ Rn that satisﬁes lim inf z→x  z(cid:54)=x
1||x−z|| (R(z) − R(x) − (cid:104)v  z − x(cid:105)) ≥ 0. If
x /∈ dom(R)  then ∂F R(x) = ∅. The limiting-subdifferential (or simply subdifferential) of R at x 
written as ∂R(x)  is deﬁned as ∂R(x) def= {v ∈ Rn : ∃xk → x  R(xk) → R(x)  vk ∈ ∂F R(xk) →
v}. Denote dom(∂R) def= {x ∈ Rn : ∂R(x) (cid:54)= ∅}. Both ∂F R(x) and ∂R(x) are closed  with ∂F R(x)
convex and ∂F R(x) ⊂ ∂R(x) [25  Proposition 8.5]. Since R is lsc  it is (subdifferentially) regular at
x if and only if ∂F R(x) = ∂R(x) [25  Corollary 8.11].
An lsc function R is r-prox-regular at ¯x ∈ dom(R) for ¯v ∈ ∂R(¯x) if ∃r > 0 such that R(x(cid:48)) >
R(x) + (cid:104)v  x(cid:48) − x(cid:105) − 1
A necessary condition for x to be a minimizer of R is 0 ∈ ∂R(x). The set of critical points of R is
crit(R) = {x ∈ Rn : 0 ∈ ∂R(x)}.

2r||x − x(cid:48)||2 ∀x  x(cid:48) near ¯x  R(x) near R(¯x) and v ∈ ∂R(x) near ¯v.

2 Global convergence of MiFB

2.1 Kurdyka-Łojasiewicz property
Let R : Rn → R ∪ {+∞} be a proper lsc function. For η1  η2 such that −∞ < η1 < η2 < +∞ 
deﬁne the set [η1 < R < η2] def= {x ∈ Rn : η1 < R(x) < η2}.
Deﬁnition 2.1. R is said to have the Kurdyka-Łojasiewicz property at ¯x ∈ dom(R) if there exists
η ∈]0  +∞]  a neighbourhood U of ¯x and a continuous concave function ϕ : [0  η[→ R+ such that

(i) ϕ(0) = 0  ϕ is C 1 on ]0  η[  and for all s ∈]0  η[  ϕ(cid:48)(s) > 0;
(ii) for all x ∈ U ∩ [R(¯x) < R < R(¯x) + η]  the Kurdyka-Łojasiewicz inequality holds

ϕ(cid:48)(cid:0)R(x) − R(¯x)(cid:1)dist(cid:0)0  ∂R(x)(cid:1) ≥ 1.

(2.1)
Proper lsc functions which satisfy the Kurdyka-Łojasiewicz property at each point of dom(∂R) are
called KL functions.
Roughly speaking  KL functions become sharp up to reparameterization via ϕ  called a desingularizing
function for R. Typical KL functions are the class of semi-algebraic functions  see [6  7]. For instance 
the (cid:96)0 pseudo-norm and the rank function (see Example 1.1-1.3 and Section 4.1) are indeed KL.

2.2 Global convergence
Let µ  ν > 0 be two constants. For i ∈ I and k ∈ N  deﬁne the following quantities 

def= 1 − γkL − µ − νγk

βk

2γk

  β def= lim inf

k∈N βk and αi k

def=

sa2
i k
2γkµ

+

sb2

i kL2
2ν

  αi

def= lim sup

k∈N

αi k. (2.2)

3

Theorem 2.2 (Convergence of MiFB (Algorithm 1)). For problem (P)  suppose that (A.1)-(A.2)
hold  and moreover Φ is a proper lsc KL function. For Algorithm 1  choose µ  ν  γk  ai k  bi k such
that

= β −(cid:80)

def

(2.3)

(i) {xk}k∈N has ﬁnite length  i.e.(cid:80)

Then each bounded sequence {xk}k∈N generated by MiFB satisﬁes
k∈N ||xk − xk−1|| < +∞;

i∈I αi > 0.

δ

(ii) There exists a critical point x(cid:63) ∈ crit(Φ) such that limk→∞ xk = x(cid:63).
(iii) If Φ has the KL property at a global minimizer x(cid:63)  then starting sufﬁciently close from x(cid:63) 

any sequence {xk}k∈N converges to a global minimum of Φ and satisﬁes (i).

The proof is detailed in the supplementary material.
Remark 2.3.

(i) The convergence result holds true for any real Hilbert space. The boundedness of {xk}k∈N

is automatically ensured under standard assumptions such as coercivity of Φ.

(ii) It is known from [13] that if the desingularizing function ϕ = C

2   1[ 
then global linear convergence of the objective and the iterates can be derived. However  we
will not pursue this further since our main interest is local linear convergence.

(iii) Unlike existing work  negative inertial parameters are allowed by Theorem 2.2.
(iv) When ai k ≡ 0 and bi k ≡ 0  i.e. the case of FB splitting  condition (2.3) holds naturally as

θ tθ  C > 0 and θ ∈ [ 1

.

2γ

2γµ + b2

(v) From (2.2) and (2.3)  we conclude the following:

long as γ < 1
(a) s = 1: if b0 k ≡ b  a0 k ≡ a (i.e. constant inertial parameters)  then (2.3) implies that

L which recovers the case of [3];

2ν/L2 < β = 1−γL−µ−νγ

a  b belong to an ellipsoid: a2

parameters)  then (2.3) means that the ai’s live in a ball: ( 1

An empirical approach for inertial parameters Besides Theorem 2.2  we also provide an empirical
bound for the choice of the inertial parameters. Consider the setting: γk ≡ γ ∈]0  1/L[ and bi k =

(b) When s ≥ 2  for each i ∈ I  let bi k = ai k ≡ ai (i.e. constant symmetric inertial
i < β.

2ν/L2 )(cid:80)
ai k ≡ ai ∈] − 1  2[  i ∈ I. We have the following empirical bound for the summand(cid:80)
iai ∈(cid:3)0  min(cid:8)1  1/L−γ
(cid:80)
(2.4)
(cid:80)
and choose ai k such that(cid:80)
i ai k = min{(cid:80)
To ensure the convergence {xk}k∈N  an online updating rule should be applied together with the
empirical bound. More precisely  choose ai according to (2.4). Then for each k ∈ N  let bi k = ai k
i∈I ||xk−i −
Note that the allowed choices of the summand(cid:80)
xk−i−1||}k∈N is summable. For instance  ck =
instance  (2.4) allows(cid:80)

i ai  ck} where ck > 0 is such that {ck
k1+q(cid:80)
i∈I ||xk−i−xk−i−1||   c > 0  q > 0.
3L ]. While for Theorem 2.2 (cid:80)
i ai by (2.4) is larger than those of Theorem 2.2. For
i ai = 1 can be reached

i ai = 1 for γ ∈]0  2

2γµ + 1

(cid:9)(cid:2).

|2γ−1/L|

i∈I ai:

i∈I a2

c

only when γ → 0.

3 Local convergence properties of MiFB

3.1 Partial smoothness
Let M ⊂ Rn be a C 2-smooth submanifold  let TM(x) the tangent space of M at any point x ∈ M.
Deﬁnition 3.1. The function R : Rn → R ∪ {+∞} is C 2-partly smooth at ¯x ∈ M relative to M
for ¯v ∈ ∂R(¯x) (cid:54)= ∅ if M is a C 2-submanifold around ¯x  and
(i) (Smoothness): R restricted to M is C 2 around ¯x;
(ii) (Regularity): R is regular at all x ∈ M near ¯x and R is r-prox-regular at ¯x for ¯v;
(iii) (Sharpness): TM(¯x) = par(∂R(x))⊥;
(iv) (Continuity): The set-valued mapping ∂R is continuous at ¯x relative to M.

We denote the class of partly smooth functions at x relative to M for v as PSFx v(M). Partial
smoothness was ﬁrst introduced in [15] and its directional version stated here is due to [18  12].
Prox-regularity is sufﬁcient to ensure that the partly smooth submanifolds are locally unique [18 
Corollary 4.12]  [12  Lemma 2.3 and Proposition 10.12].

4

3.2 Finite activity identiﬁcation

One of the key consequences of partial smoothness is ﬁnite identiﬁcation of the partial smoothness
submanifold associated to R for problem (P). This is formalized in the following statement.
Theorem 3.2 (Finite activity identiﬁcation). Suppose that Algorithm 1 is run under the conditions
of Theorem 2.2  such that the generated sequence {xk}k∈N converges to a critical point x(cid:63) ∈ crit(Φ).
Assume that R ∈ PSFx(cid:63) −∇F (x(cid:63))(Mx(cid:63) ) and the non-degeneracy condition

−∇F (x(cid:63)) ∈ ri(cid:0)∂R(x(cid:63))(cid:1) 

holds. Then  xk ∈ Mx(cid:63) for all k large enough.
See the supplementary material for the proof. This result generalizes that of [20] to the non-convex
case and multiple inertial steps.

(ND)

3.3 Local linear convergence
Given γ ∈]0  1
R ∈ PSFx(cid:63) −∇F (x(cid:63))(Mx(cid:63) ). Denote Tx(cid:63)
symmetric 

L [ and a critical point x(cid:63) ∈ crit(Φ)  let Mx(cid:63) be a C 2-smooth submanifold and
def= TMx(cid:63) (x(cid:63)) and the following matrices which are all

H def= γPTx(cid:63)∇2F (x(cid:63))PTx(cid:63)   G def= Id − H  Q def= γ∇2Mx(cid:63) Φ(x(cid:63))PTx(cid:63) − H 

(3.1)
where ∇2Mx(cid:63) Φ is the Riemannian Hessian of Φ along the submanifold Mx(cid:63) (readers may refer to
the supplementary material from more details on differential calculus on Riemannian manifolds).
To state our local linear convergence result  the following assumptions will play a key role.
Restricted injectivity Besides the local C 2-smoothness assumption on F   following the idea of
[19  20]  we assume the restricted injectivity condition 

ker(cid:0)∇2F (x(cid:63))(cid:1) ∩ Tx(cid:63) = {0}.

(RI)

Positive semi-deﬁniteness of Q Assume that Q is positive semi-deﬁnite  i.e.∀h ∈ Tx(cid:63) 

(3.2)
Under (3.2)  Id + Q is symmetric positive deﬁnite  hence invertible  we denote P def= (Id + Q)−1.
Convergent parameters The parameters of MiFB (Algorithm 1)  are convergent  i.e.

(cid:104)h  Qh(cid:105) ≥ 0.

ai k → ai  bi k → bi  ∀i ∈ I and γk → γ ∈ [γ  min{γ  ¯r}] 

(3.3)

where ¯r < r  and r is the prox-regularity modulus of R (see Deﬁnition 3.1).
Remark 3.3.

(i) Condition (3.2) can be met by various non-convex functions  such as polyhedral functions 
including the (cid:96)0 pseudo-norm. For the rank function  it is also observed that this condition
holds in our numerical experiments of Section 4.

(ii) Condition (3.3) asserts that both the inertial parameters (ai k  bi k) and the step-size γk

should converge to some limit points  and this condition cannot be relaxed in general.
(iii) It can be shown that conditions (3.2) and (RI) together imply that x(cid:63) is a local minimizer
of Φ in (P)  and Φ grows at least quadratically near x(cid:63). The arguments to prove this are
essentially adapted from those used to show [20  Proposition 4.1(ii)].

We need the following notations:

def= (a0 − b0)P + (1 + b0)P G  Ms

def= −(cid:0)(ai−1 − ai) − (bi−1 − bi)(cid:1)P − (bi−1 − bi)P G  i = 1  ...  s − 1 

def= −(as−1 − bs−1)P − bs−1P G 

M0

Mi

(3.4)

M0

Id

...

0

M def=

··· Ms−1 Ms
···
0
...
...
···

Id

...

0

0

5

   dk

def=

 xk − x(cid:63)

...

xk−s − x(cid:63)

 .

Theorem 3.4 (Local linear convergence). Suppose that Algorithm 1is run under the setting of
Theorem 3.2. Moreover  assume that (RI)  (3.2) and (3.3) hold. Then for all k large enough 

dk+1 = M dk + o(||dk||).

If ρ(M ) < 1  then given any ρ ∈]ρ(M )  1[  there exists K ∈ N such that ∀k ≥ K 

In particular  if s = 1  then ρ(M ) < 1 if R is locally polyhedral around x(cid:63) or if a0 = b0.

||xk − x(cid:63)|| = O(ρk−K).

See the supplementary material for the proof.
Remark 3.5.

(3.5)

(3.6)

(i) When s = 1  ρ(M ) can be given explicitly in terms of the parameters of the algorithm (i.e. a0 
b0 and γ)  see [20  Section 4.2] for details. However  the spectral analysis of M becomes
much more complicated to get for s ≥ 2  where the analysis of at least cubic equations are
involved. Therefore  for the sake of brevity  we shall skip the detailed discussion here.

s=1 = 1 − √

(ii) When s = 1  it was shown in [20] that the optimal convergence rate that can be obtained
1 − τ γ  where from condition (RI) 
by 1-step inertial scheme with ﬁxed γ is ρ(cid:63)
continuity of ∇2F at x(cid:63) implies that there exists τ > 0 and a neighbourhood of x(cid:63) such that
(cid:104)h  ∇2F (x(cid:63))h(cid:105) ≥ τ||h||2  for all h ∈ Tx(cid:63). As we will see in the numerical experiments of
Section 4  such a rate can be improved by our multi-step inertial scheme. Taking s = 2 for
example  we will show that for a certain class of functions  the optimal local linear rate is
close to or even is ρ(cid:63)

1 − τ γ  which is obviously faster than ρ(cid:63)

s=2 = 1 − 3√

(iii) Though it can be satisﬁed for many problems in practice  the restricted injectivity (RI) can

s=1.

be removed for some penalties R  for instance  when R is locally polyhedral near x(cid:63).

4 Numerical experiments

In this section  we illustrate our results with some numerical experiments carried out on the problems
in Example 1.1  1.2 and 1.3.

4.1 Examples of KL and partly smooth functions

All the objectives Φ in the above mentioned examples are continuous KL functions. Indeed  in
Example 1.1 and 1.2  Φ is the sum of semi-algebraic functions which is also semi-algebraic. In
Example 1.3  Φ is also algebraic when G is the squared hinge loss  and deﬁnable in an o-minimal
structure for the logistic loss (see e.g. [26] for material on o-minimal structures).
Moreover  R is partly smooth in all these examples as we show now.
Example 4.1 ((cid:96)0 pseudo-norm). The (cid:96)0 pseudo-norm is locally constant. Moreover  it is regular on
Rn ([14  Remark 2]) and its subdifferential is given by (see [14  Theorem 1])

where (ei)i=1 ... n is the standard basis  and supp(x) =(cid:8)i : xi (cid:54)= 0(cid:9). The proximity operator of

∂||x||0 = span(cid:0)(ei)i∈supp(x)c

(cid:1) 

(cid:96)0-norm is given by hard-thresholding 

z

if |z| >
if |z| =
if |z| <

√
√
√

2γ 
2γ 
2γ.

proxγ||x||0

(z) =

sign(z)[0  z]
0

Mx = Tx =(cid:8)z ∈ Rn : supp(z) ⊂ supp(x)(cid:9).

It can then be easily veriﬁed that the (cid:96)0 pseudo-norm is partly smooth at any x relative to the subspace

It is also prox-regular at x for any bounded v ∈ ∂||x||0. Note also condition (ND) is automatically
veriﬁed and that the Riemannian gradient and Hessian along Tx of || · ||0 vanish.
Example 4.2 (Rank). The rank function is the spectral extension of (cid:96)0 pseudo-norm to matrix-
valued data x ∈ Rn1×n2 [17]. Consider a singular value decomposition (SVD) of x  i.e. x =
U diag(σ(x))V ∗  where U = {u1  . . .   un}  V = {v1  . . .   vn} are orthonormal matrices  and

6

σ(x) = (σi(x))i=1 ... n is the vector of singular values. By deﬁnition  rank(x) def= ||σ(x)||0. Thus the
rank function is partly smooth relative at x to the set of ﬁxed rank matrices

Mx =(cid:8)z ∈ Rn1×n2 : rank(z) = rank(x)(cid:9) 

which is a C 2-smooth submanifold [16]. The tangent space of Mx at x is

(cid:9) 

TMx (x) = Tx =(cid:8)z ∈ Rn1×n2 : u∗
∂rank(x) = U ∂(cid:0)||σ(x)||0

i zvj = 0  for all r < i ≤ n1  r < j ≤ n2

(cid:1)V ∗ = U span(cid:0)(ei)i∈supp(σ(x))c

(cid:1)V ∗ 

The rank function is also regular its subdifferential reads

which is a vector space (see [14  Theorem 4 and Proposition 1]). The proximity operator of rank
function amounts to applying hard-thresholding to the singular values. Observe that by deﬁnition of
Mx  the Riemannian gradient and Hessian of the rank function along Mx also vanish.
For Example 1.2  it is worth noting from the above examples and separability of the regularizer that
the latter is also partly smooth relative to the cartesian product of the partial smoothness submanifolds
of (cid:96)0 and the rank function.

4.2 Experimental results

For the problem in Example 1.1  we generated y = Axob + ω with m = 48  n = 128  the entries of
A are i.i.d. zero-mean and unit variance Gaussian  xob is 8-sparse  and ω ∈ Rm is an additive noise
with small variance.
For the problem in Example 1.2  we generated y = xs + xl + ω  with n1 = n2 = 50  xs is 250-sparse 
and the rank of xl is 5  and ω is an additive noise with small variance.
For Example 1.3  we generated m = 64 training samples with n = 96-dimensional feature space.
For all presented numerical results  3 different settings were tested:

• the FB method  with γk ≡ 0.3/L  noted as “FB”;
• MiFB with s = 1  bk = ak ≡ a and γk ≡ 0.3/L  noted as “1-iFB”;
• MiFB with s = 2  bi k = ai k ≡ ai  i = 0  1 and γk ≡ 0.3/L  noted as “2-iFB”.

Tightness of theoretical prediction The convergence proﬁles of ||xk − x(cid:63)|| are shown in Figure 1.
As it can be seen from all the plots  ﬁnite identiﬁcation and local linear convergence indeed occur. The
positions of the green dots indicate the iteration from which xk numerically identiﬁes the submanifold
Mx(cid:63). The solid lines (“P”) represents practical observations  while the dashed lines (“T”) denotes
theoretical predictions.
As the Riemannian Hessians of (cid:96)0 and the rank both vanish in all examples  our predicted rates
coincide exactly with the observed ones (same slopes for the dashed and solid lines).

(a) Sparse regression

(b) PCP

(c) Sparse SVM

Figure 1: Finite identiﬁcation and local linear convergence of MiFB under different inertial settings
in terms of ||xk − x(cid:63)||. “P” stands for practical observation and “T” indicates the theoretical estimate.
We ﬁx γk ≡ 0.3/L for all tests. For the 2 inertial schemes  inertial parameters are ﬁrst chosen such
that (2.3) holds. The position of the green dot in each plot indicates the iteration beyond which
identiﬁcation of Mx(cid:63) occurs.
Comparison of the methods Under the tested settings  we draw the following remarks on the
comparison of the inertial schemes:

7

1002003004005006007008009001000k10-1010-610-2102kxk!x?kFB PFB T1-iFBP1-iFB T2-iFB P2-iFB T2-iFBoptimal1!p1!=.1!3p1!=.50100150200250300350k10-1010-610-2102kxk!x?kFB PFB T1-iFBP1-iFB T2-iFB P2-iFB T2-iFBoptimal1!p1!=.1!(1!=.)1=3200400600800100012001400160018002000k10-1010-610-2102kxk!x?kFB PFB T1-iFBP1-iFB T2-iFB P2-iFB T2-iFBoptimal1!p1!=.1!(1!=.)1=3• The MiFB scheme is much faster than FB both globally and locally. Finite activity identiﬁ-
• Comparing the two MIFB inertial schemes  “2-iFB” outperforms “1-iFB”  showing the

cation also occurs earlier for MiFB than for FB;

advantages of a 2-step inertial scheme over the 1-step one.

Optimal ﬁrst-order method To highlight the potential of multiple steps in MiFB  for the “2-iFB”
scheme  we also added an example where we locally optimized the rate for the inertial parmeters. See
optimal inertial parameters  the dashed line stands for the rate 1 − √
the magenta lines all the examples  where the solid line corresponds to the observed proﬁle for the
1 − τ γ  and the dotted line is
that of 1 − 3√
1 − τ γ  which shows indeed that a faster linear rate can be obtained owing to multiple

inertial parameters.
We refer to [20  Section 4.5] for the optimal choice of inertial parameters for the case s = 1.
The empirical bound (2.4) and inertial steps s We now present a short comparison of the empirical
bound (2.4) of inertial parameters and different choices of s under bigger choice of γ = 0.8/L. MiFB
with 3 inertial steps  i.e. s = 3  is added which is noted as “3-iFB”  see the magenta line in Figure 2.
Similar to the above experiments  we choose bi k = ai k ≡ ai  i ∈ I  and “Thm 2.2” means that ai’s
are chosen according to Theorem 2.2  while “Bnd (2.4)” means that ai’s are chosen based on the
empirical bound (2.4). We can infer from Figure 2 the following. Compared to the results in Figure 1 
a bigger choice of γ leads to faster convergence. Yet still  under the same choice of γ  MiFB is faster
than FB both locally and globally; For either “Thm 2.2” or “Bnd (2.4)”  the performance of the three
i∈I ai for each scheme
are close. Then between “Thm 2.2” and “Bnd (2.4)”  “Bnd (2.4)” shows faster convergence result 
i∈I ai of (2.4) is bigger than that of Theorem 2.2. It should be noted that 
when γ ∈]0  2
i∈I ai equal or
very close to 1  then it can be observed in practice that MiFB locally oscillates  which is a well-known
property of the FISTA scheme [5  10]. We refer to [20  Section 4.4] for discussions of the properties
of such oscillation behaviour.

MiFB schemes are close  this is mainly due to the fact that values of the sum(cid:80)
since the allowed value of(cid:80)

i∈I ai allowed by (2.4) is 1. If we choose(cid:80)

3L ]  the largest value of(cid:80)

(a) Sparse regression

(b) PCP

(c) Sparse SVM

Figure 2: Comparison of MiFB under different inertial settings. We ﬁx γk ≡ 0.8/L for all tests. For
the three inertial schemes  the inertial parameters were chosen such that (2.3) holds.

Acknowledgments

This work was partly supported by the European Research Council (ERC project SIGMA-Vision).

References
[1] F. Alvarez. On the minimizing property of a second order dissipative system in Hilbert spaces. SIAM

Journal on Control and Optimization  38(4):1102–1119  2000.

[2] F. Alvarez and H. Attouch. An inertial proximal method for maximal monotone operators via discretization

of a nonlinear oscillator with damping. Set-Valued Analysis  9(1-2):3–11  2001.

[3] H. Attouch  J. Bolte  and B. F. Svaiter. Convergence of descent methods for semi-algebraic and tame
problems: proximal algorithms  Forward–Backward splitting  and regularized Gauss–Seidel methods.
Mathematical Programming  137(1-2):91–129  2013.

[4] H. Attouch  J. Peypouquet  and P. Redont. A dynamical approach to an inertial Forward–Backward

algorithm for convex minimization. SIAM J. Optim.  24(1):232–256  2014.

8

100200300400500600700k10-1010-610-2102kxk!x?kFB1 - iFB  Thm 2.22 - iFB  Thm 2.23 - iFB  Thm 2.21 - iFB  Bnd (2.4)2 - iFB  Bnd (2.4)3 - iFB  Bnd (2.4)20406080100120k10-1010-610-2102kxk!x?kFB1 - iFB  Thm 2.22 - iFB  Thm 2.23 - iFB  Thm 2.21 - iFB  Bnd (2.4)2 - iFB  Bnd (2.4)3 - iFB  Bnd (2.4)100200300400500600700k10-1010-610-2102kxk!x?kFB1 - iFB  Thm 2.22 - iFB  Thm 2.23 - iFB  Thm 2.21 - iFB  Bnd (2.4)2 - iFB  Bnd (2.4)3 - iFB  Bnd (2.4)[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[6] J. Bolte  A. Daniilidis  and A. Lewis. The Łojasiewicz inequality for nonsmooth subanalytic functions with
applications to subgradient dynamical systems. SIAM Journal on Optimization  17(4):1205–1223  2007.

[7] J. Bolte  A. Daniilidis  O. Ley  and L. Mazet. Characterizations of Lojasiewicz inequalities: subgradient
ﬂows  talweg  convexity. Transactions of the American Mathematical Society  362(6):3319–3363  2010.

[8] R. I. Bo¸t  E. R. Csetnek  and S. C. László. An inertial Forward–Backward algorithm for the minimization
of the sum of two nonconvex functions. EURO Journal on Computational Optimization  pages 1–23  2014.

[9] E. J. Candès  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Journal of the ACM

(JACM)  58(3):11  2011.

[10] A. Chambolle and C. Dossal. On the convergence of the iterates of the “Fast Iterative Shrink-

age/Thresholding Algorithm”. Journal of Optimization Theory and Applications  pages 1–15  2015.

[11] D. L. Donoho  M. Elad  and V. N. Temlyakov. Stable recovery of sparse overcomplete representations in

the presence of noise. IEEE Trans. Inform. Theory  52(1):6–18  2006.

[12] D. Drusvyatskiy and A. S. Lewis. Optimality  identiﬁability  and sensitivity. Mathematical Programming 

pages 1–32  2013.

[13] P. Frankel  G. Garrigos  and J. Peypouquet. Splitting methods with variable metric for kurdyka–łojasiewicz
functions and general convergence rates. Journal of Optimization Theory and Applications  165(3):874–900 
2015.

[14] H. Y. Le. Generalized subdifferentials of the rank function. Optimization Letters  7(4):731–743  2013.

[15] A. S. Lewis. Active sets  nonsmoothness  and sensitivity. SIAM J. on Optimization  13(3):702–725  2003.

[16] A. S. Lewis and J. Malick. Alternating projections on manifolds. Mathematics of Operations Research 

33(1):216–234  2008.

[17] A. S. Lewis and H. S. Sendov. Twice differentiable spectral functions. SIAM Journal on Matrix Analysis

and Applications  23(2):368–386  2001.

[18] A. S. Lewis and S. Zhang. Partial smoothness  tilt stability  and generalized Hessians. SIAM Journal on

Optimization  23(1):74–94  2013.

[19] J. Liang  J. Fadili  and G. Peyré. Local linear convergence of Forward–Backward under partial smoothness.

In Advances in Neural Information Processing Systems  pages 1970–1978  2014.

[20] J. Liang  J. Fadili  and G. Peyré. Activity identiﬁcation and local linear convergence of Forward–Backward-

type methods. arXiv:1503.03703  2015.

[21] D. A. Lorenz and T. Pock. An inertial Forward–Backward algorithm for monotone inclusions. Journal of

Mathematical Imaging and Vision  51(2):311–325  2014.

[22] A. Moudaﬁ and M. Oliny. Convergence of a splitting inertial proximal method for monotone operators.

Journal of Computational and Applied Mathematics  155(2):447–454  2003.

[23] P. Ochs  Y. Chen  T. Brox  and T. Pock. iPiano: inertial proximal algorithm for nonconvex optimization.

SIAM Journal on Imaging Sciences  7(2):1388–1419  2014.

[24] B. T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational

Mathematics and Mathematical Physics  4(5):1–17  1964.

[25] R. T. Rockafellar and R. Wets. Variational analysis  volume 317. Springer Verlag  1998.

[26] L. van den Dries. Tame topology and o-minimal structures  volume 248 of Mathematrical Society Lecture

Notes. Cambridge Univiversity Press  New York  1998.

9

,Jingwei Liang
Jalal Fadili
Gabriel Peyré
Matteo Papini
Matteo Pirotta
Marcello Restelli