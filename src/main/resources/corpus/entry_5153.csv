2008,Spectral Clustering with Perturbed Data,Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis  image processing and data mining. However  the computational and/or communication resources required by the method in processing large-scale data sets are often prohibitively high  and practitioners are often required to perturb the original data in various ways (quantization  downsampling  etc) before invoking a spectral algorithm. In this paper  we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems  suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.,Spectral Clustering with Perturbed Data

Ling Huang
Intel Research

Donghui Yan
UC Berkeley

Michael I. Jordan

UC Berkeley

Nina Taft

Intel Research

ling.huang@intel.com

dhyan@stat.berkeley.edu

jordan@cs.berkeley.edu

nina.taft@intel.com

Abstract

Spectral clustering is useful for a wide-ranging set of applications in areas such as
biological data analysis  image processing and data mining. However  the com-
putational and/or communication resources required by the method in processing
large-scale data are often prohibitively high  and practitioners are often required to
perturb the original data in various ways (quantization  downsampling  etc) before
invoking a spectral algorithm. In this paper  we use stochastic perturbation theory
to study the effects of data perturbation on the performance of spectral clustering.
We show that the error under perturbation of spectral clustering is closely related
to the perturbation of the eigenvectors of the Laplacian matrix. From this result
we derive approximate upper bounds on the clustering error. We show that this
bound is tight empirically across a wide range of problems  suggesting that it can
be used in practical settings to determine the amount of data reduction allowed in
order to meet a speciﬁcation of permitted loss in clustering performance.

1 Introduction

A critical problem in machine learning is that of scaling: Algorithms should be effective compu-
tationally and statistically as various dimensions of a problem are scaled. One general tool for
approaching large-scale problems is that of clustering or partitioning  in essence an appeal to the
principle of divide-and-conquer. However  while the output of a clustering algorithm may yield a
set of smaller-scale problems that may be easier to tackle  clustering algorithms can themselves be
complex  and large-scale clustering often requires the kinds of preprocessing steps that are invoked
for other machine learning algorithms [1]  including proto-clustering steps such as quantization 
downsampling and compression. Such preprocessing steps also arise in the distributed sensing and
distributed computing setting  where communication and storage limitations may preclude transmit-
ting the original data to centralized processors.

A number of recent works have begun to tackle the issue of determining the tradeoffs that arise
under various “perturbations” of data  including quantization and downsampling [2  3  4]. Most of
these analyses have been undertaken in the context of well-studied domains such as classiﬁcation 
regression and density estimation  for which there are existing statistical analyses of the effect of
noise on performance. Although extrinsic noise differs conceptually from perturbations to data
imposed by a data analyst to cope with resource limitations  the mathematical issues arising in the
two cases are similar and the analyses of noise have provided a basis for the study of the tradeoffs
arising from perturbations.

In this paper we focus on spectral clustering  a class of clustering methods that are based on eigen-
decompositions of afﬁnity  dissimilarity or kernel matrices [5  6  7  8]. These algorithms often out-
perform traditional clustering algorithms such as the K-means algorithm or hierarchical clustering.
To date  however  their impact on real-world  large-scale problems has been limited; in particular 
a distributed or “in-network” version of spectral clustering has not yet appeared. Moreover  there
has been little work on the statistical analysis of spectral clustering  and thus there is little theory to
guide the design of distributed algorithms. There is an existing literature on numerical techniques for

1

Procedure SpectralClustering (x1  . . .   xn)
Input:
Output: Bipartition S and ¯S of the input data

n data samples {xi}n

i=1  xi ∈ Rd

1. Compute the similarity matrix K:
Kij = exp“− kxi−xj k2

2σ2
k

2. Compute the diagonal degree matrix D:

”  ∀xi  xj

3. Compute the normalized Laplacian matrix:

j=1 Kij

Di = Pn
L = I − D−1K

4. Find the second eigenvector v2 of L
5. Obtain the two partitions using v2:
6.

S = {[i] : v2i > 0}  ¯S = {[i] : v2i ≤ 0}

Figure 1: A spectral bipartitioning algorithm.

Mis-clustering

rate

6

Eigen error

6

Laplacian
matrix error

6

Similarity
matrix error

6

Data error

Proposition 1

η

?

k˜v2 − v2k2

Eqn. (5)  (6)

?

dL

?

dK

?

σ

Lemma 2 &
Eqn. (7)− (13)

Lemma 3 or 4

Assumption A

Error propagation

Perturbation analysis

Figure 2: Perturbation analysis: from clustering
error to data perturbation error.

scaling spectral clustering (including downsampling [9  10] and the relaxation of precision require-
ments for the eigenvector computation [7])  but this literature does not provide end-to-end  practical
bounds on error rates as a function of data perturbations.

In this paper we present the ﬁrst end-to-end analysis of the effect of data perturbations on spectral
clustering. Our focus is quantization  but our analysis is general and can be used to treat other kinds
of data perturbation. Indeed  given that our approach is based on treating perturbations as random
variables  we believe that our methods will also prove useful in developing statistical analyses of
spectral clustering (although that is not our focus in this paper).

The paper is organized as follows. In Section 2  we provide a brief introduction to spectral clustering.
Section 3 contains the main results of the paper; speciﬁcally we introduce the mis-clustering rate
η  and present upper bounds on η due to data perturbations. In Section 4  we present an empirical
evaluation of our analyses. Finally  in Section 5 we present our conclusions.

2 Spectral clustering and data perturbation

2.1 Background on spectral clustering algorithms

i=1  xi ∈ R1×d and some notion of similarity between all pairs of data
Given a set of data points {xi}n
points xi and xj  spectral clustering attempts to divide the data points into groups such that points in
the same group are similar and points in different groups are dissimilar. The point of departure of a
spectral clustering algorithm is a weighted similarity graph G(V  E)  where the vertices correspond
to data points and the weights correspond to the pairwise similarities. Based on this weighted graph 
spectral clustering algorithms form the graph Laplacian and compute an eigendecomposition of this
Laplacian [5  6  7]. While some algorithms use multiple eigenvectors and ﬁnd a k-way clustering
directly  the most widely studied algorithms form a bipartitioning of the data by thresholding the
second eigenvector of the Laplacian (the eigenvector with the second smallest eigenvalue). Larger
numbers of clusters are found by applying the bipartitioning algorithm recursively. We present a
speciﬁc example of a spectral bipartitioning algorithm in Fig. 1.

2.2

Input data perturbation

Let the data matrix X ∈ Rn×d be formed by stacking n data samples in rows. To this data matrix we
assume that perturbation W is applied  such that we obtain a perturbed version ˜X of the original data
X. We assume that a spectral clustering algorithm is applied to ˜X and we wish to compare the results
of this clustering with respect to the spectral clustering of X. This analysis captures a number of data
perturbation methods  including data ﬁltering  quantization  lossy compression and synopsis-based
data approximation [11]. The multi-scale clustering algorithms that use “representative” samples to
approximate the original data can be treated using our analysis as well [12].

2

3 Mis-clustering rate and effects of data perturbation

Let K and L be the similarity and Laplacian matrix on the original data X  and let ˜K and ˜L be those
on the perturbed data. We deﬁne the mis-clustering rate η as the proportion of samples that have
different cluster memberships when computed on the two different versions of the data  X and ˜X.
We wish to bound η in terms of the “magnitude” of the error matrix W = ˜X − X  which we now
deﬁne. We make the following general stochastic assumption on the error matrix W :

A. All elements of the error matrix W are i.i.d. random variables with zero mean  bounded

variance σ2 and bounded fourth central moment µ4; and are independent of X.

Remark.
(i) Note that we do not make i.i.d. assumptions on the elements of the similarity matrix;
rather  our assumption refers to the input data only. (ii) This assumption is distribution free  and
captures a wide variety of practical data collection and quantization schemes.
(iii) Certain data
perturbation schemes may not satisfy the independence assumption. We have not yet conducted an
analysis of the robustness of our bounds to lack of independence  but in our empirical work we have
found that the bounds are robust to relatively small amounts of correlation.

We aim to produce practically useful bounds on η in terms of σ and the data matrix X. The bounds
should be reasonably tight so that in practice they could be used to determine the degree of pertur-
bation σ given a desired level of clustering performance  or to provide a clustering error guarantee
on the original data even though we have access only to its approximate version.

Fig. 2 outlines the steps in our theoretical analysis. Brieﬂy  when we perturb the input data (e.g.  by
ﬁltering  quantization or compression)  we introduce a perturbation W to the data which is quan-
tiﬁed by σ2. This induces an error dK := ˜K − K in the similarity matrix  and in turn an error
dL := ˜L − L in the Laplacian matrix. This further yields an error in the second eigenvector of
the Laplacian matrix  which results in mis-clustering error. Overall  we establish an analytical re-
lationship between the mis-clustering rate η and the data perturbation error σ2  where η is usually
monotonically increasing with σ2. Our goal is to allow practitioners to specify a mis-clustering
rate η∗  and by inverting this relationship  to determine the right magnitude of the perturbation σ∗
allowed. That is  our work can provide a practical method to determine the tradeoff between data
perturbation and the loss of clustering accuracy due to the use of ˜X instead of X. When the data
perturbation can be related to computational or communications savings  then our analysis yields a
practical characterization of the overall resource/accuracy tradeoff.

Practical Applications Consider in particular a clustering task in a distributed networking system
that allows an application to specify a desired clustering error C ∗ on the distributed data (which is
not available to the coordinator). Through a communication protocol similar to that in [4]  the coor-
dinator (e.g.  network operation center) gets access to the perturbed data ˜X for spectral clustering.
The coordinator can compute a clustering error bound C using our method. By setting C ≤ C ∗  it
determines the tolerable data perturbation error σ∗ and instructs distributed devices to use appropri-
ate numbers of bits to quantize their data. Thus we can provide guarantees on the achieved error 
C ≤ C ∗  with respect to the original distributed data even with access only to the perturbed data.
3.1 Upper bounding the mis-clustering rate

Little is currently known about the connection between clustering error and perturbations to the
Laplacian matrix in the spectral clustering setting. [5] presented an upper bound for the clustering
error  however this bound is usually quite loose and is not viable for practical applications. In this
section we propose a new approach based on a water-ﬁlling argument that yields a tighter  practical
bound. Let v2 and ˜v2 be the unit-length second eigenvectors of L and ˜L  respectively. We derive a
relationship between the mis-clustering rate η and δ2 := k˜v2 − v2k2.
The intuition behind our derivation is suggested in Fig. 3. Let a and b denote the sets of components
in v2 corresponding to clusters of size k1 and k2  respectively  and similarly for a′ and b′ in the case
of ˜v2. If v2 is changed to ˜v2 due to the perturbation  an incorrect clustering happens whenever a
component of v2 in set a jumps to set b′  denoted as a → b′  or a component in set b jumps to set a′ 
denoted as b → a′. The key observation is that each ﬂipping of cluster membership in either a → b′

3

Component values

a

a′

mis-

clustering

mis-

clustering

Component

indices

b

b′

Wisconsin Breast Cancer Data

 

Upper Bound of Kannan
Our Upper Bound
Mis−clustering Rate

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
a
b
r
u
t
r
e
P

 

0
0.005

0.01

0.015

0.02

σ of noise

0.025

0.03

0.035

Figure 3: The second eigenvector v2 and its per-
turbed counterpart ˜v2 (denoted by dashed lines).

Figure 4: An example of the tightness of
the upper bound for η in Eq. (1).

or b → a′ contributes a fairly large amount to the value of δ2  compared to the short-range drifts
in a → a′ or b → b′. Given a ﬁxed value of δ2  the maximum possible number of ﬂippings (i.e. 
missed clusterings) is therefore constrained  and this translates into an upper bound for η.
We make the following assumptions on the data X and its perturbation:

B1. The components of v2 form two clusters (with respect to the spectral bipartitioning algo-

rithm in Fig. 1). The size of each cluster is comparable to n.

B2. The perturbation is small with the total number of mis-clusterings m < min(k1  k2)  and

the components of ˜v2 form two clusters. The size of each cluster is comparable to n.

B3. The perturbation of individual components of v2 in each set of a → a′  a → b′  b → a′
and b → b′ have identical (not necessary independent) distributions with bounded second
moments  respectively  and they are uncorrelated with the components in v2.

Our perturbation bound can now be stated as follows:
Proposition 1. Under assumptions B1  B2 and B3  the mis-clustering rate η of the spectral biparti-
tioning algorithm under the perturbation satisﬁes η ≤ δ2 = k˜v2 − v2k2. If we further assume that
all components of ˜v2 − v2 are independent  then

η ≤ (1 + op(1))Ek˜v2 − v2k2.

(1)

The proof of the proposition is provided in the Appendix.

Remarks.
(i) Assumption B3 was motivated by our empirical work. Although it is difﬁcult to
establish general necessary and sufﬁcient conditions for B3 to hold  in the Appendix we present
some special cases that allow B3 to be veriﬁed a priori. It is also worth noting that B3 appears
to hold (approximately) across a range of experiments presented in Section 4. (ii) If we assume
piecewise constancy for v2  then we can relax the uncorrelated assumption in B3. (iii) Our bound
has a different ﬂavor than that obtained in [5]. Although the bound in Theorem 4.3 in [5] works for
k-way clustering  it assumes a block-diagonal Laplacian matrix and requires the gap between the
kth and (k + 1)th eigenvalues to be greater than 1/2  which is unrealistic in many data sets. In the
setting of 2-way spectral clustering and a small perturbation  our bound is much tighter than that
derived in [5]; see Fig. 4 in particular.

3.2 Perturbation on the second eigenvector of Laplacian matrix

We now turn to the relationship between the perturbation of eigenvectors with that of its matrix.
One approach is to simply draw on the classical domain of matrix perturbation theory; in particular 
applying Theorem V.2.8 from [13]  we have the following bound on the (small) perturbation of the
second eigenvector:

k˜v2 − v2k ≤

 

(2)

k4dLkF

ν − √2kdLkF

where ν is the gap between the second and the third eigenvalue. However  in our experimental
evaluation we found that ν can be quite small in some data sets  and in these cases the right-hand

4

(a) Wisconsin Breast Cancer Data

 

RHS
LHS

0.08

0.06

0.04

0.02

l

e
u
a
V

0.07

0.06

0.05

l

e
u
a
V

0.04

0.03

0.02

0.01

(b) Waveform Data

RHS
LHS

 

0.05

RHS
LHS

(c) Pen−digits Data

 

0.04

l

e
u
a
V

0.03

0.02

0.01

 

0
0.005

0.01

0.015

0.02

σ of noise

0.025

0.03

0.035

 

0
0.005

0.01

0.015

0.02

σ of noise

0.025

0.03

0.035

 

0
0.005

0.01

0.015

0.02

σ of noise

0.025

0.03

0.035

Figure 5: Experimental examples of the ﬁdelity of the approximation in Eq. (5). We add i.i.d. zero mean
Gaussian noise to the input data with different σ  and we see that the right-hand side (RHS) of (5) approximately
upper bounds the left-hand side (LHS).

side of (2) can be quite large even for a small perturbation. Thus the bound given by (2) is often not
useful in practical applications.

To derive a more practically useful bound  we begin with a well-known ﬁrst-order Taylor expansion
to compute the perturbation on the second eigenvector of a Laplacian matrix as follows:

vj + O(dL2) ≈

q=1 vq2dLpq is a random variable determined by the effect of the perturbation on

eigendecomposition of the Laplacian matrix L. Then we have

n

n

=

˜v2 − v2 =

vT
j dLv2
Xj=1 j6=2
λ2 − λj

vq2dLpq! ·
 n
Xp=1
Xq=1

where βp = Pn
the Laplacian matrix L  and the vector up = Pn
Ek˜v2 − v2k2 ≈ E(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

βpup(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Xp=1

Xp=1

=

n

n

2

n

n

n

vj

n

n

(3)

βpup 

Xq=1

vpjvq2dLpq

Xp=1
λ2 − λj

Xp=1
 =

Xj=1 j6=2
λ2 − λj
vpj · vj
Xj=1 j6=2

j=1 j6=2(cid:16) vpj vj
λ2−λj(cid:17) is a constant determined by the
Xi=1

E(cid:0)βiui · βj u

Xj=i+1

j(cid:1) .

(4)

n

n

T

Ekβpupk2 + 2

In our experimental work we have found that for i 6= j  βiui is either very weakly correlated with
βj uj (i.e.  the total sum of all cross terms is typically one or two orders of magnitude less than that
of squared term)  or negatively correlated with βj uj (i.e.  the total sum of all cross terms is less than
zero). This empirical evidence suggests the following approximate bound:

Ek˜v2 − v2k2 .

Eβ2

p · kupk2.

n

Xp=1

Examples of the ﬁdelity of this approximation for particular data sets are shown in Fig. 5.

Finally  Eβ2

p is related to dLpq  and can be upper bounded by

Eβ2

p = E n
Xq=1

≤
where σpi is the variance of dLpi.

vq2dLpq!2

n

n

Xi=1

Xj=1

[vi2vj2 · E (dLpi) E (dLpj) + |vi2vj2|σpiσpj]  

(5)

(6)

Remark. Through Eqs. (5) and (6)  we can bound the squared norm of the perturbation on the
second eigenvector in expectation  which in turn bounds the mis-clustering rate. To compute the
bound  we need to estimate the ﬁrst two moments of dL  which we discuss next.

3.3 Perturbation on the Laplacian matrix

Let D be the diagonal matrix with Di = Pj Kij. We deﬁne the normalized Laplacian matrix as
L = I − D−1K. Letting ∆ = ˜D − D and dK = ˜K − K  we have the following approximation for
dL = ˜L − L:

5

Lemma 2. If perturbation dK is small compared to K  then

dL = (1 + o(1)) ∆D−2K − D−1dK.
Then  element-wise  the ﬁrst two moments of dL can be estimated as

E(dL) ≈ E(∆)D−2K − D−1E(dK)
E(dL2) ≈ E(cid:0)∆D−2K ◦ ∆D−2K − 2D−1dK ◦ ∆D−2K + D−1dK ◦ D−1dK(cid:1)

= E(cid:0)∆2(cid:1) D−4K 2 + D−2E(cid:0)dK 2(cid:1) − 2E(∆dK)D−3 ◦ K 

(9)
where ◦ denotes element-wise product. The quantities needed to estimate E(dL) and E(dL2) can
be obtained from moments and correlations among the elements of the similarity matrix ˜Kij. In
particular  we have

E(dKij) = E(cid:16) ˜Kij(cid:17) − Kij  E(dKij)2 = E ˜K 2

ij − 2KijE(cid:16) ˜Kij(cid:17) + K 2

n

ij

(7)

(8)

(10)

(11)

E∆i = E ˜Di − Di  E ˜Di =
i = E


˜Kij


Xj=1
Xj=1

Xj=1

E ˜D2

=

n

n

2

E(cid:16) ˜Kij(cid:17)   E∆2

i = E ˜D2

i − 2Di · E ˜Di + D2

i

E ˜K 2

ij + 2

n

Xj=1

n

Xq=j+1(cid:16)E ˜KijE ˜Kiq + ρk

ijqσk

ijσk

iq(cid:17) (12)

E(∆dK)ij = E( ˜Di − Di)( ˜Kij − Kij) = E(cid:16) ˜Di ˜Kij(cid:17) − DiE ˜Kij − KijE∆i

n

˜K 2

Xq=1 q6=j

= E


ij + ˜Kij

˜Kiq
 − DiE ˜Kij − KijE∆i


Xq=1 q6=j(cid:16)E ˜KijE ˜Kiq + ρk
ij is the standard deviation of ˜Kij and −1 ≤ ρk

= E ˜K 2

ijqσk

ijσk

ij +

n

′

where σk
ijq ≤ 1 is the correlation coefﬁcient between
˜Kij and ˜Kiq. Estimating all ρk
s would require an intensive effort. For simplicity  we could set
ijq
ijq to 1 in Eq. (12) and to −1 in Eq. (13)  and obtain an upper bound for E(dL2). This bound could
ρk
ijq. However  in our
optionally be tightened by using a simulation method to estimate the values of ρk
experimental work we have found that our results are insensitive to the values of ρk
ijq  and setting
ijq = 0.5 usually achieves good results.
ρk

iq(cid:17) − DiE ˜Kij − KijE∆i 

(13)

Remark. Eqs. (8)–(13) allow us to estimate (i.e.  to upper bound) the ﬁrst two moments of dL
using those of dK  which are computed using Eq. (15) or (16) in Section 3.4.

3.4 Perturbation on the similarity matrix

The similarity matrix ˜K on perturbed data ˜X is

˜Kij = exp(cid:18)−||xi − xj + ǫi − ǫj||2

2σ2
k

(cid:19)  

(14)

where σk is the kernel bandwidth. Then  given data X  the ﬁrst two moments of dKij = ˜Kij − Kij 
the error in the similarity matrix  can be determined by one of the following lemmas.
Lemma 3. Given X  if all components of ǫi and ǫj are i.i.d. Gaussian N (0  σ2)  then

σ2
σ2

E(cid:16) ˜Kij(cid:17) = Mij(cid:18)−

k(cid:19)   E(cid:16) ˜K 2

k (cid:19)  
1−2t(cid:17)/(1 − 2t)d/2i  and λij =(cid:0)||xi − xj||2/2σ2(cid:1).

ij(cid:17) = Mij(cid:18)−

2σ2
σ2

where Mij(t) =hexp(cid:16) λij t

6

(15)

(a) Gaussian data

(b) Sin−sep data

(c) Concentric data

5

4

3

2

1

0

−1

−2

3

2

1

0

−1

−2

−3

10

5

0

−5

−10

−2

0

2

4

−2

−1

0

1

2

−15

−10

−5

0

5

10

Figure 6: Synthetic data sets illustrated in two dimensions.

Lemma 4. Under Assumption A  given X and for large values of the dimension d  the ﬁrst two
moments of ˜Kij can be computed approximately as follows:

E(cid:16) ˜Kij(cid:17) = Mij(cid:18)−

1
2σ2

k(cid:19)   E(cid:16) ˜K 2

ij(cid:17) = Mij(cid:18)−

1
σ2

k(cid:19)  

(16)

where Mij(t) = exp(cid:2)(cid:0)λij + 2dσ2(cid:1) t +(cid:0)dµ4 + dσ4 + 4σ2λ2

ij(cid:1) t2(cid:3)  and λij = ||xi − xj||2.

Remark.
(i) Given data perturbation error σ  kernel bandwidth σk and data X  the ﬁrst two mo-
ments of dKij can be estimated directly using (15) or (16). (ii) Through Eqs. (1)–(16)  we have
established a relationship between the mis-clustering rate η and the data perturbation magnitude σ.
By inverting this relationship (e.g.  using binary search)  we can determine a σ∗ for a given η∗.

4 Evaluation

In this section we present an empirical evaluation of our analysis on 3 synthetic data sets (see Fig. 6)
and 6 real data sets from the UCI repository [14]. The data domains are diverse  including im-
age  medicine  agriculture  etc.  and the different data sets impose different difﬁculty levels on the
underlying spectral clustering algorithm  demonstrating the wide applicability of our analysis.

In the experiments  we use data quantization as the perturbation scheme to evaluate the upper bound
provided by our analysis on the clustering error. Fig. 7 plots the mis-clustering rate and the upper
bound for data sets subject to varying degrees of quantization. As expected  the mis-clustering
rate increases as one decreases the number of quantization bits. We ﬁnd that the error bounds are
remarkably tight  which validate the assumptions we make in the analysis. It is also interesting to
note that even when using as few as 3-4 bits  the clustering degrades very little in both real error and
as assessed by our bound. The effectiveness of our bound should allow the practitioner to determine
the right amount of quantization given a permitted loss in clustering performance.

5 Conclusion

In this paper  we proposed a theoretical analysis of the clustering error for spectral clustering in the
face of stochastic perturbations. Our experimental evaluation has provided support for the assump-
tions made in the analysis  showing that the bound is tight under conditions of practical interest. We
believe that our work  which provides an analytical relationship between the mis-clustering rate and
the variance of the perturbation  constitutes a critical step towards enabling a large class of appli-
cations that seek to perform clustering of objects  machines  data  etc in a distributed environment.
Many networks are bandwidth constrained  and our methods can guide the process of data thinning
so as to limit the amount of data transmitted through the network for the purpose of clustering.

References
[1] L. Bottou and O. Bousquet  “The tradeoffs of large scale learning ” in Advances in Neural Information

Processing Systems 20  2007.

[2] A. Silberstein  G. P. A. Gelfand  K. Munagala  and J. Yang  “Suppression and failures in sensor networks:

A Bayesian approach ” in Proceedings of VLDB  2007.

[3] X. Nguyen  M. J. Wainwright  and M. I. Jordan  “Nonparametric decentralized detection using kernel

methods ” IEEE Transactions on Signal Processing  vol. 53  no. 11  pp. 4053–4066  2005.

7

(a) Sin−sep Data

 

Upper Bound
Test Value

0.25

0.2

0.15

0.1

0.05

0.037

0.018

0.009

0.005

0.002

0.001

0.001

e

t

 

a
R
g
n
i
r
e

l

t
s
u
C
−
s
M

i

0

 

3

4

5

6

7

Number of quantization bits

8

9

x 10−3

(d) Image Segmentation Data

Upper Bound
Test Value

8

6

4

2

0.056

0.029

0.015

0.008

0.004

0.002

0.001

e

t

 

a
R
g
n
i
r
e

l

t
s
u
C
−
s
M

i

4

5

Number of quantization bits

6

7

8

(g) Iris Data

Upper Bound
Test Value

0

 

2

3

0.03

0.025

0.02

0.015

0.01

0.005

 

t

e
a
R
g
n
i
r
e

l

t
s
u
C
−
s
M

i

 

 

(b) Concentric Circle Data

 

Upper Bound
Test Value

e

t

 

a
R
g
n
i
r
e

l

t
s
u
C
−
s
M

i

1.4

1.2

1

0.8

0.6

0.4

0.2

e
t
a
R
 
g
n
i
r
e
t
s
u
C
−
s
M

i

l

0.036

0.018

0.009

0.004

0.002

0.001

0.001

0.07

0.06

0.05

0.04

0.03

0.02

0.01

(c) Gaussian Data

 

Upper Bound
Test Value

0.036

0.018

0.009

0.005

0.002

0.001

0.001

5

6

7

Number of quantization bits

8

9

0

 

3

4

5

6

7

Number of quantization bits

8

9

(e) Pen−digits Data

(f) Wine Data

Upper Bound
Test Value

0

 

3

4

0.06

0.05

0.04

0.03

0.02

0.01

e
t
a
R
 
g
n
i
r
e
t
s
u
C
−
s
M

i

l

0.062

0.030

0.015

0.008

0.004

0.002

0.001

0

 

2

3

4

5

6

7

8

Number of quantization bits

(h) Wisconsin Breast Cancer Data

Upper Bound
Test Value

0.05

0.04

0.03

0.02

0.01

t

 

e
a
R
g
n
i
r
e
t
s
u
C
−
s
M

i

l

 

 

 

 

e
t
a
R
 
g
n
i
r
e
t
s
u
C
−
s
M

i

l

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01

Upper Bound
Test Value

0.071

0.036

0.018

0.009

0.005

0.002

0.001

4

5

6

7

8

Number of quantization bits

(i) Waveform Data

Upper Bound
Test Value

0

 

2

3

0.08

0.06

0.04

0.02

t

 

e
a
R
g
n
i
r
e
t
s
u
C
−
s
M

i

l

0.070 0.037 0.017 0.009 0.004 0.002 0.001

0.074

0.036

0.018

0.009

0.005

0.002

0.001

0.072

0.036

0.018

0.009

0.005

0.002

0.001

0

 

2

3
Number of quantization bits

4

5

6

7

8

0

 

2

3

4

5

6

Number of quantization bits

7

8

0

 

2

3

4

5

6

7

8

Number of quantization bits

Figure 7: Upper bounds of clustering error on approximate data obtained from quantization as a function of
the number of bits. (a–c) Simulated data sets (1000 sample size  2  2  10 features  respectively); (d) Statlog
image segmentation data (2310 sample size  19 features); (e) Handwritten digits data (10992 sample size  16
features); (f) Wine data (178 sample size  13 features); (g) Iris data (150 sample size  4 features). (h) Wisconsin
breast cancer data (569 sample size  30 features); (i) Waveform data (5000 sample size  21 features). The x-axis
shows the number of quantization bits and (above the axis) the corresponding data perturbation error σ. Error
bars are derived from 25 replications. In the experiments  all data values are normalized in range [0  1]. For
data sets with more than two clusters  we choose two of them for the experiments.

[4] L. Huang  X. Nguyen  M. Garofalakis  A. D. Joseph  M. I. Jordan  and N. Taft  “In-network PCA and

anomaly detection ” in Advances in Neural Information Processing Systems (NIPS)  2006.

[5] R. Kannan  S. Vempala  and A. Vetta  “On clusterings: Good  bad and spectral ” Journal of the ACM 

vol. 51  no. 3  pp. 497–515  2004.

[6] A. Y. Ng  M. Jordan  and Y. Weiss  “On spectral clustering: Analysis and an algorithm ” in Advances in

Neural Information Processing Systems (NIPS)  2002.

[7] J. Shi and J. Malik  “Normalized cuts and image segmentation ” IEEE Transactions on Pattern Analysis

and Machine Intelligence  vol. 22  no. 8  pp. 888–905  2000.

[8] U. von Luxburg  M. Belkin  and O. Bousquet  “Consistency of spectral clustering ” Annals of Statistics 

vol. 36  no. 2  pp. 555–586  2008.

[9] P. Drineas and M. W. Mahoney  “On the Nystr¨om method for approximating a Gram matrix for improved

kernel-based learning ” in Proceedings of COLT  2005  pp. 323–337.

[10] C. Fowlkes  S. Belongie  F. Chung  and J. Malik  “Spectral grouping using the Nystr¨om method ” IEEE

Transactions on Pattern Analysis and Machine Intelligence  vol. 26  no. 2  2004.

[11] G. Cormode and M. Garofalakis  “Sketching streams through the net: Distributed approximate query

tracking ” in Proceedings of VLDB  2005  pp. 13–24.

[12] D. Kushnir  M. Galun  and A. Brandt  “Fast multiscale clustering and manifold identiﬁcation ” Pattern

Recognition  vol. 39  no. 10  pp. 1876–1891  2006.

[13] G. W. Stewart and J. Guang Sun  Matrix Perturbation Theory. Academic Press  1990.
[14] A. Asuncion and D. Newman  “UCI Machine Learning Repository  Department of Information and Com-

puter Science ” 2007  http://www.ics.uci.edu/ mlearn/MLRepository.html.

8

,Dongsung Huh
Terrence Sejnowski