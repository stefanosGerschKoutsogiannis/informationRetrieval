2019,Intrinsic dimension of data representations in deep neural networks,Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data
representations  i.e. the minimal number of parameters needed to describe a representation. We find that  in a trained network  the ID is orders of magnitude smaller than the number of units in each layer. Across layers  the ID first increases and then progressively decreases in the final layers. Remarkably  the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g.  with principal component analysis)  nor in representations that had been artificially linearized. They are neither found in untrained networks  nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional  but not necessarily flat manifolds.,Intrinsic dimension of data representations in deep

neural networks

International School for Advanced Studies

International School for Advanced Studies

Technical University of Munich

International School for Advanced Studies

Alessio Ansuini

alessioansuini@gmail.com

Jakob H. Macke

macke@tum.de

Alessandro Laio

laio@sissa.it

Davide Zoccolan

zoccolan@sissa.it

Abstract

Deep neural networks progressively transform their inputs across multiple pro-
cessing layers. What are the geometrical properties of the representations learned
by these networks? Here we study the intrinsic dimensionality (ID) of data-
representations  i.e. the minimal number of parameters needed to describe a repre-
sentation. We Ô¨Ånd that  in a trained network  the ID is orders of magnitude smaller
than the number of units in each layer. Across layers  the ID Ô¨Årst increases and then
progressively decreases in the Ô¨Ånal layers. Remarkably  the ID of the last hidden
layer predicts classiÔ¨Åcation accuracy on the test set. These results can neither be
found by linear dimensionality estimates (e.g.  with principal component analysis) 
nor in representations that had been artiÔ¨Åcially linearized. They are neither found
in untrained networks  nor in networks that are trained on randomized labels. This
suggests that neural networks that can generalize are those that transform the data
into low-dimensional  but not necessarily Ô¨Çat manifolds.

1

Introduction

Deep neural networks (DNNs)  including convolutional neural networks (CNNs) for image data  are
among the most powerful tools for supervised data classiÔ¨Åcation. In DNNs  inputs are sequentially
processed across multiple layers  each performing a nonlinear transformation from a high-dimensional
vector to another high-dimensional vector. Despite the empirical success and widespread use of
DNNs  we still have an incomplete understanding about why and when they work so well ‚Äì in
particular  it is not clear yet why they are able to generalize well to unseen data  not withstanding
their massive overparametrization (1). While progress has been made recently [e.g. (2; 3)]  guidelines
for selecting architectures and training procedures are still largely based on heuristics and domain
knowledge.
A fundamental geometric property of a data representation in a neural network is its intrinsic
dimension (ID)  i.e.  the minimal number of coordinates which are necessary to describe its points
without signiÔ¨Åcant information loss. It is widely appreciated that deep neural networks are over-
parametrized  and that there is substantial redundancy amongst the weights and activations of deep
nets ‚Äì e.g.  several studies in network compression have shown that many weights in deep neural
networks can be pruned without signiÔ¨Åcant loss in classiÔ¨Åcation performance (4; 5). Linear estimates
of the ID in DNNs have been computed theoretically and numerically in simpliÔ¨Åed models (6)  and
local estimates of the ID developed in (7) have been related to robustness properties of deep networks
to adversarial attacks (8; 9)  showing that a low local intrinsic dimension correlates positively with
robustness. Local ID of object manifolds can also be estimated at several locations on the tangent

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

space  and was found to decrease along the last hidden layers of AlexNet (10; 11). Both linear and
nonlinear dimensionality reduction techniques have been used extensively to visualize computations
in deep networks (12; 13; 14). Local ID in the last hidden layer of deep networks has been found to
be much lower than the dimension of the embedding space (15)  consistently with the results of (16) 
in which estimates of the ID is used to signal the onset of overÔ¨Åtting in the presence of noisy labels 
thus showing a connection between intrinsic dimension and generalization during training of speciÔ¨Åc
models.
However  there has not been a direct and systematic characterization of how the intrinsic dimension
of data manifolds varies across the layers of CNNs and how it relates to generalization across a wide
variety of architectures. We here leverage TwoNN (17)  a recently developed estimator for global
ID that exploits the fact that nearest-neighbour statistics depend on the ID (18) (see Fig. 1 for an
illustration). TwoNN can be applied even if the manifold containing the data is curved  topologically
complex  and sampled non-uniformly. This procedure is not only accurate  but also computationally
efÔ¨Åcient. In a few seconds on a desktop PC it provides the estimate of the ID of a data set with O(104)
data  each with O(105) coordinates (for example the activations in an intermediate layer of a CNN) 
thus making it possible to map out ID across multiple layers and networks. Using this estimator  we
investigated the variation of the ID along the layers of a wide range of deep neural networks trained
for image classiÔ¨Åcation. SpeciÔ¨Åcally  we addressed the following questions:

low-dimensional manifolds  or  conversely  seek to expand the dimensionality?

‚Ä¢ How does the ID change along the layers of CNNs? Do CNNs compress representations into
‚Ä¢ How different is the actual ID from the ‚Äòlinear‚Äô dimensionality of a network  i.e.  the dimen-
sionality of the linear subspace containing the data-manifold? A substantial mismatch would
indicate that the underlying manifolds are curved rather than Ô¨Çat.
‚Ä¢ How is the ID of a network related to its generalization performance? Can we Ô¨Ånd empirical
signatures of generalization performance in the geometrical structure of the representations?

Our analyses show that data representations
in CNNs are embedded in manifolds of low
dimensionality  which is typically several
orders of magnitude lower than the dimen-
sionality of the embedding space (the num-
ber of units in a layer). In addition  we
found that the variation of the ID along the
hidden layers of CNNs follows a similar
trend across different architectures ‚Äì the
early layers expand the dimensionality of
the representations  followed by a mono-
tonic decrease that brings the ID to reach
low values in the Ô¨Ånal layers.
Moreover  we observed that  in networks
trained to classify images  the ID of the
training set in the last hidden layer is an
accurate predictor of the network‚Äôs classi-
Ô¨Åcation accuracy on the test set ‚Äì i.e  the
lower the ID in this layer  the better the
network capability of correctly classifying
the image categories in a test set. Conversely  in the last hidden layer  the ID remains high for a
network trained on non predictable data (i.e.  with permuted labels)  on which the network is forced
to memorize rather than generalize. These geometrical properties of representations in trained neural
networks were empirically conserved across multiple architectures  and might point to an operating
principle of deep neural networks.

Figure 1: The TwoNN estimator derives an estimate of
intrinsic dimensionality from the statistics of nearest-
neighbour distances.

2 Estimating the intrinsic dimension of data representations

Inferring the intrinsic dimension of high-dimensional and sparsely sampled data representations is
a challenging statistical problem. To estimate the ID of data-representations in deep networks  we
leverage a recently developed global ID-estimator (‚ÄòTwoNN‚Äô) that is based on computing the ratio

2

ri 1ri 2Activation x1Activation x21) For each data point icompute the distance to its first and second neighbour (ri 1and ri 2)2) For each icompute ùúáThe probability distribution of mis where d is the ID  independently on the local density of points. 3) Infer d from the empirical probability distribution of all the mi.D= # of nodes in the layer = 2Intrinsic dimension ‚âÖ14) Repeat the calculation selecting a fraction of points at random. This gives the ID as a function of the scale.between the distances to the second and Ô¨Årst nearest neighbors (NN) of each data point (17) (see Fig.
1). This allows overcoming the problems related to the curvature of the embedding manifold and to
the local variations in the density of the data points  under the weak assumption that the density is
constant on the scale of the distance between each point and its second nearest neighbor.
Formally  let points xi be uniformly sampled on a manifold with intrinsic dimension d and let N
be the total number of points. Let r(1)
be the distances of the Ô¨Årst and second neighbor
.
i /r(1)
= r(2)
of i respectively. Then ¬µi
  i = 1  2  ...  N follows a Pareto distribution with parameter
‚àí(d+1)
d + 1 on [1  +‚àû)  f (¬µi|d) = d¬µ
. Taking advantage of this observation  we can formulate the
i
likelihood of vector ¬µ¬µ¬µ

.
= (¬µ1  ¬µ2  ...  ¬µN ) as

i

and r(2)

i

i

N(cid:89)

P (¬µ¬µ¬µ|d) = dN

‚àí(d+1)
¬µ
i

.

(1)

i=1

At this point d can be easily computed  for instance by maximizing the likelihood  or  following (17) 
by employing the empirical cumulate of the distribution of the ¬µ values to reduce the ID estimation
task to a linear regression problem. Indeed  the ID can also be estimated by restricting the product in
eq. 1 to non-intersecting triplets of points  for which independence is strictly satisÔ¨Åed  but  as shown
in ref. (17)  in practice this does not signiÔ¨Åcantly affect the estimate.
The ID estimated by this approach is asymptotically correct even for samples harvested from highly
non-uniform probability distributions. For a Ô¨Ånite number of data points  the estimated values remain
very close to the ground truth ID  when this is smaller than ‚àº 20. For larger IDs and Ô¨Ånite sample
size  the approach moderately underestimates the correct value  especially if the density of data is
non-uniform. Therefore  the values reported in the following Ô¨Ågures  when larger ‚àº 20  should be
considered as lower bounds.
For real-world data  the intrinsic dimension always depends on the scale of distances on which the
analysis is performed. This implies that the reliability of the dimensionality estimate needs to be
assessed by measuring the intrinsic dimension at different scales and by checking whether it is  at
least approximately  scale invariant (17). In our analyses  this test was performed by systematically
decimating the dataset  thus gradually reducing its size. The ID was then estimated on the reduced
samples  in which the average distance between data points had become progressively larger. This
allowed estimating the dependence of the ID on the scale. As explained in (17)  if the ID is well-
deÔ¨Åned  its estimated value will only depend weakly on the number of data points N; in particular it
will be not severely affected by the presence of ‚Äúhubs‚Äù  since the decimation procedure would kill
them (see Fig. 2B).
To test the reliability of our ID estimator on embedding spaces with a dimension comparable to that
found in the layers of a deep network  we performed tests on artiÔ¨Åcial data of known ID  embedded
in a 100 000 dimensional space. The test did not reveal any signiÔ¨Åcant degradation of the accuracy.
Indeed  the ID estimator is sensitive only to the value of the distances between pair of points  and this
distance does not depend on the embedding dimension.
For computational efÔ¨Åciency  we analyzed the representations of a subset of layers. We extracted
representations at pooling layers after a convolution or a block of consecutive convolutions  and at
fully connected layers. In the experiments with ResNets  we extracted the representations after each
ResNet block (19) and the average pooling before the output. See A.1 for details.
The code to compute the ID estimates with the TwoNN method and to reproduce our experiments is
available at this repository.

3 Results

3.1 The intrinsic dimension exhibits a characteristic shape across several networks

Our Ô¨Årst goal was to empirically characterize the ID of data representations in different layers of deep
neural networks. Given a layer l of a DNN  an individual data point (e.g.  an image) is mapped onto
the set of activations of all the nl units of the layer  which deÔ¨Åne a point in a nl-dimensional space.
We refer to nl as the embedding dimension (ED) of the representation in layer l. A set of N input

3

Figure 2: Modulation of ID across hidden layers of deep convolutional networks A) ID across
layers of VGG-16-R  error bars are the standard deviation of the ID (see A.1). Numbers in plot
indicate embedding dimensionality of each layer. B Subsampling analysis on VGG-16-R experiment 
reported for the same layers as in the inset in A (see A.1 for details).

samples (e.g.  N images) generate  in each layer l  a set of N nl-dimensional points. We estimated
the dimension of the manifold containing these points using TwoNN.
We Ô¨Årst investigated the variation of the ID across the layers of a VGG-16 network (20)  pre-trained
on ImageNet (11)  and Ô¨Åne-tuned and evaluated on a synthetic data-set of 1440 images (21). The
dataset consisted of 40 3D objects  each rendered in 36 different views (we left out 6 images for each
object as a test set) ‚Äì it thus spanned a spectrum of different appearances  but of a small number
of underlying geometrical objects. When estimating the ID of data representations on this network
(referred to as ‚ÄòVGG-16-R‚Äô)  we found that the ID Ô¨Årst increased in the Ô¨Årst pooling layer  before
successively and monotonically decreasing across the following layers  reaching very low values in
the Ô¨Ånal hidden layers (Fig. 2A). For instance  in the fourth layer of pooling (pool4) of VGG-16-R 
ID ‚âà 19 and ED ‚âà 105  with ID
ED ‚âà 2 √ó 10‚àí4  which is consistent with the values reported by (15)
using a different ID estimator (22).
One potential concern is whether the number of stimuli is sufÔ¨Åcient for the ID-estimate to be robust.
To investigate this  we repeated the analysis on subsamples randomly chosen on the data manifold 
Ô¨Ånding that the estimated IDs were indeed stable across a wide range of sample sizes (Fig. 2B). We
note that  for the early/intermediate layers  the reported values of the ID are likely a lower bound to
the real ID (see discussion in (17)).
Are the ‚Äòhunchback‚Äô shape of the ID variation across the layers (i.e.  the initial steep increase followed
by a gradual monotonic decrease)  and the overall low values of the ID  speciÔ¨Åc to this particular
network architecture and dataset? To investigate this question  we repeated these analyses on several
standard architectures (AlexNet  VGG and ResNet) pre-trained on ImageNet (23). SpeciÔ¨Åcally  we
computed the average ID of the object manifolds corresponding to the 7 biggest ImageNet categories 
using 500 images per category (see section A.1). We found both the hunchback-shape and the low
IDs to be preserved across all networks (Fig. 3A): the ID initially grew  then reached a peak or a
plateau and  Ô¨Ånally  progressively decreased towards its Ô¨Ånal value. As shown in Fig. 8 for AlexNet 
such proÔ¨Åle of ID variation across layers was generally consistent across object classes.
The ID in the output layer was the smallest  often assuming a value of the order of ten. Such a low
value is to be expected  given that the ID of the output layer of a network capable of recognizing Nc
categories is bound by the condition Nc ‚â§ 2ID  which implies that each category is associated with a
binary representation  and that the output layer optimally encodes this representation. For the ‚àº 1000
categories of ImageNet  this bound becomes ID (cid:38) 10  a value consistent with those observed in all
the networks we considered.
Is the relative (rather than the absolute) depth of a layer indicative of the ID? To investigate this 
we plotted ID against relative depth (deÔ¨Åned as the absolute depth of the layer divided by the total
number of layers  not counting batch normalization layers (12)) of the 14 models belonging to the
three classes of networks (Fig. 3B). Remarkably  the ID proÔ¨Åles approximately collapsed onto a

4

01020304050607056789101112inputpool1pool2pool3pool4pool5d1d2outputpool5d1d2161284901442403604807201440outputd1d2pool5ABIDIDnumber of data pointsID1505288028164014082007041003522508840964096404096409625088VGG-16-Routput40Figure 3: ID of object manifolds across networks. A) IDs of data representations for 4 networks:
each point is the average of the IDs of 7 object manifolds. The error bars are the standard deviations
of the ID across the single object‚Äôs estimates (see A.1). B) The ID as a function of the relative depth
in 14 deep convolutional networks spanning different sizes  architectures and training techniques.
Despite the wide diversity of these models  the ID proÔ¨Åle follows a typical hunchback shape (error
bars not shown).

common hunchback shape 1  despite considerable variations in the architecture  number of layers  and
optimization algorithms. For networks belonging to the VGG and ResNet families  the rising portions
of the ID proÔ¨Åles substantially overlapped  with the ID reaching similar large peak values (between
100 and 120) in the relative depth range 0.2-0.4. The dependence on relative depth is consistent with
the results of (12)  where it was observed that similarity between layers depended on relative depth.
Notably  in all networks the ID eventually converged to small values in the last hidden layer. These
results suggest that state-of-the-art deep neural networks ‚Äì after an initial increase in ID ‚Äì perform
a progressive dimensionality reduction of the input feature vectors  a result with is consistent with
the information-theoretical analysis in (24). Based on previous Ô¨Åndings about the evolution of the
ID in the last hidden layer during training (16)  one could speculate that this progressive  gradual
reduction of dimensionality of data-manifolds is a feature of deep neural networks which allows them
to generalize well. In the following  we will investigate this idea further by showing that the ID of the
last hidden layers predicts generalization performance  and by showing that these properties cannot
be found in networks with random weights or trained on non predictable data.

3.2 The intrinsic dimension of the last hidden layer predicts classiÔ¨Åcation performance

Although the hunchback shape was preserved across networks  the IDs in the last hidden layers were
not exactly the same for all the networks. To better resolve such differences  we computed the ID in
the last hidden layer of each network using a much larger pool of images of the training set (‚àº 2  000) 
sampled from all ImageNet categories (see section A.1). This revealed a spread of ID values  ranging
between ‚âà 12 (for ResNet152) and ‚âà 25 (for AlexNet  Fig. 4). These differences may appear small 
compared to the much larger size of the embedding space in the last hidden layer (where the ED was
between 1 and 2 orders of magnitude larger than the ID (range = [512 ‚àí 4096]). However  the ID
in the last hidden layer on the training set was indeed a strong predictor of the performance of the
network on the test set  as measured by top 5-score (Fig. 4  Pearson correlation coefÔ¨Åcient r = 0.94).
A tight correlation was found not only across the full set of networks  but also within each class of
architectures  when such comparison was possible ‚Äì i.e.  in the classes of the VGG with and without
batch normalization and ResNets (r = 0.99 in the latter case  see inset in Fig. 4).

1with the exception of AlexNet  and a small network trained on MNIST in a separate analysis  see section

3.4 for details and analysis

5

20406080100120140160005101520253035AlexNetVGG-16ResNet-18ResNet-34layer depthID0.00.20.40.60.81.0204060801001201401600relative depthAlexNetVGG-11VGG-13VGG-16VGG-19VGG-11-bnVGG-13-bnVGG-16-bnVGG-19-bnResNet-18ResNet-34ResNet-50ResNet-101ResNet-152ABIDOverall  this analysis suggests that the ID in the last hidden layer can be used as a proxy for the
generalization ability of a network. Importantly  this proxy can be measured without estimating the
performance on an external validation set.

3.3 Data representations lie on curved manifolds

Figure 4: ID of the last hidden layer predicts
performance. The ID of data representations
(training set) predicts the top 5-score performance
on the test set. Inset Detail for the ResNet class.

The strength of the TwoNN method lies in its
ability to infer the ID of data representations 
even if they lie on curved manifolds. This raises
the question of whether our observations (low
IDs  hunchback shapes  correlation with test-
error) reÔ¨Çect the fact that data points live on
low-dimensional  yet highly curved manifolds 
or  simply  in low-dimensional  but largely Ô¨Çat
(linear) subspaces.
To test this  we performed linear dimensionality
reduction (principal component analysis  PCA)
on the normalized covariance matrix (i.e.  the
matrix of correlation coefÔ¨Åcients ‚Äì using the
raw covariance resulted in qualitatively similar
results) for each layer and network. We did
not Ô¨Ånd a clear gap in the eigenvalue spectrum
(Fig. 5A)  a result that is qualitatively consistent
with that obtained for stimulus-representations
in primary visual cortex (25). The absence of a
gap in the spectrum  with the magnitude of the
eigenvalues smoothly decreasing as a function
of their rank  is  by itself  an indication that the
data manifolds are not linear. Nevertheless  we deÔ¨Åned an ‚Äòad-hoc‚Äô estimate of dimensionality by
computing the number of components that should be included to describe 90% of the variance in the
data. In what follows  we call this number PC-ID. We found PC-ID to be about one or two orders of
magnitude larger than the value of the ID computed with TwoNN. For example  the PC-ID in the last
hidden layer of VGG-16 was ‚âà 200 (Fig. 5C  solid red line)  while the ID estimated with TwoNN
was ‚âà 18 (solid black line).
The discrepancy between the ID estimated with TwoNN and with PCA points to the existence of
strong non-linearities in the correlations between the data  which are not captured by the covariance
matrix. To verify that this was indeed the case (and  e.g.  not a consequence of estimation bias) 
we used TwoNN to compare the ID of the last hidden layer of VGG-16 with the ID of a synthetic
Gaussian dataset with the same second-order moments. The ID of the original dataset was low and
stable as a function of the size N of the data sample used to estimate it (Fig. 5B  black curve; similar
subsampling analysis as previously shown in Fig. 2B). In contrast  the ID of the synthetic dataset was
two orders of magnitude larger  and grew with N (Fig. 5B  red curve)  as expected in the case of an
ill-deÔ¨Åned estimator (17).
We also computed the PC-ID of the object manifolds across the layers of VGG-16 on randomly
initialized networks  and we found that its proÔ¨Åle was qualitatively the same as in trained networks
(compare solid and dashed red curves in Fig. 5C). By contrast  when the same comparison was
performed on the ID (as computed using TwoNN)  the trends obtained on random weights (dashed
black curve) and after training the network (solid black curve) were very different. While the latter
showed the hunchback proÔ¨Åle (same as in Fig. 3)  the former was remarkably Ô¨Çat. This behaviour
can be explained by observing that the ID of the input is very low (see section 3.4 for a discussion
of this point). For random weights  each layer effectively performs an orthogonal transformation 
thus preserving such low ID across layers. Importantly  the hunchback proÔ¨Åle observed for the ID
in trained networks (Figs 2A  3A B) is a genuine result of training  which does not merely reÔ¨Çect
the initial expansion of the ED from the input to the Ô¨Årst hidden layers  as shown by the fact that  in
VGG-16  the ID kept growing after that the ED had already started to substantially decline (compare
the solid black and blue curves in Fig. 5C).

6

6810121416182022top 5 error (%)12161820222426ID14191817161514ID131267891011top 5 error (%)ResNet5125122048204820481834521011521113161918345210115211131619AlexNetVGGResNetVGG-bnThe analysis shown in Fig. 5C also indicates that intermediate layers and the last hidden layer
undergo opposite trends  as the result of training (compare the solid and dashed black curves): while
the ID of the last hidden layer is reduced with respect to its initial value [consistently with what
reported in (16)]  the ID of intermediate layers increases by a large amount. This prompted us to run
an exploratory analysis to monitor the ID evolution during training in a VGG-16 network trained
with CIFAR-10. We observed a behavior that was consistent with that already reported in Fig. 5C:
a substantial increase of the ID in the initial/intermediate layers  and a decrease in the last layers
(Fig. 9A  black vs. orange curve). Interestingly  a closer inspection of the dynamics in the last hidden
layer revealed a non-monotonic variation of the ID (see Fig. 9B C). Here  after an initial drop  the ID
slowly increased  but  differently from (16)  without resulting in substantial overÔ¨Åtting. Thus  the
evolution of the ID during learning appears to be not strictly monotonic and its trend likely depends
on the speciÔ¨Åc architecture and dataset  calling for further investigation.

Figure 5: Evidence that data-representations are on curved manifolds A) Variance spectra of last
hidden layer do not show a clear gap. B) ID in the last hidden layer of VGG-16 (black)  compared
with the ID of a synthetic Gaussian dataset with the same size and second-order correlations structure
(red). C) The ID and the PC-ID along the layers of VGG-16 for a trained network and an untrained 
randomly initialized network. The ED  rescaled to reach the maximum at 400  is shown in blue.

3.4 The initial increase in intrinsic dimension can arise from irrelevant features

We generally found the ID to increase in the initial layers. However  this was not observed for a
small network trained on the MNIST data-set (Fig. 6B  black curve) and was also less pronounced
for AlexNet (Fig. 3A  orange curve). A mechanism underlying the initial ID rise could be the fact
that the input is dominated by features that are irrelevant for predicting the output  but are highly
correlated between each other. To validate this hypothesis  we generated a modiÔ¨Åed MNIST dataset
(referred to as MNIST(cid:63)) by adding a luminance perturbation that was constant for all pixels within an
image  but random across the various images (Fig. 6A). Given an image i with pixels of xi ‚àà RN
(where N is the number of pixels)  we added shared random perturbations  xi ‚Üí x(cid:63)
i = xi + ŒªŒæi
where Œª is a positive parameter and Œæi are i.i.d. uniformly distributed random variables in the range
[0  1]. This perturbation has the effect of stretching the dataset along a speciÔ¨Åc direction in the input
space (the vector [1  1  . . .   1]) thus reducing the ID of the data manifold in the input layer. Indeed 
with Œª = 100  the ID of the input representation dropped from ‚âà 13 (its original value) to ‚âà 3.
The network trained on MNIST(cid:63) was still able to generalize (accuracy ‚âà 98%). However  the
variation of the ID (blue curve in Fig. 6B) now showed a hunchback shape reminiscent of that already
observed in Figs 2A and 3A B for large architectures. This suggests that the growth of the ID in the
Ô¨Årst hidden layers of a deep network is determined by the presence in the input data of low-level
features that carry no information about the correct labeling ‚Äì for instance  in the case of images 
gradients of luminance or contrast. One can speculate that  in a trained deep network  the Ô¨Årst layers
prune the irrelevant features  formatting the representation for the more advanced processing carried
out by the last layers (24). The initial increase of the dimensionality of the data manifold could be
the signature of such pruning. This notion is consistent with recent evidence gathered in the Ô¨Åeld of
visual neuroscience  where the pruning of low-level confounding features  such as luminance  has

7

ID originalID synthetic10010100010000number of data pointsABCeigenvalue rank eigenvaluesintrinsic dimensionintrinsic dimension0.00.20.40.60.81.0relative depth0100200300400ID trainedPC-ID trainedID untrainedPC-ID untrainedED (rescaled)11010010100 ResNets 18 34ResNets 50 101 152AlexNetVGGsVGGs-bnbeen demonstrated along the progression of visual cortical areas that  in the rat brain  are thought to
support shape processing and object recognition (26).

Figure 6: A The addition of a luminance gradient across the images of the MNIST dataset results in a
stretching of the image manifold along a straight line in the input space of the pixel representation.
B Change of the ID along all the layers of the MNIST network  as obtained in three different
experiments: 1) with the original MNIST dataset (black curve) 2) with the luminance-perturbed
MNIST(cid:63) dataset (blue curve) and 3) with the MNIST‚Ä†  in which the label of the MNIST images
where randomly shufÔ¨Çed (red curve).

3.5 A network trained on random labels does not show the characteristic hunchback proÔ¨Åle

of ID variation

In untrained networks the ID proÔ¨Åle is largely Ô¨Çat (Fig. 5C). Are there other circumstances in which
the ID proÔ¨Åle deviates from the typical hunchback shape of Figs 2A and 3A B  with IDs that do not
decrease progressively towards the output? It turns out that this is the case when generalization is
impossible by construction  as we veriÔ¨Åed by randomly shufÔ¨Çing the labels on MNIST (we refer to
the shufÔ¨Çed data as MNIST‚Ä†).
It has been shown (1) that deep networks can perfectly Ô¨Åt the training set on randomly labelled data 
while necessarily achieving chance level performance on the test set. As a result  when we trained
the same network as in section 3.4 on MNIST‚Ä†  we achieved a training error of zero. However  we
found that the network had an ID proÔ¨Åle which did not decrease monotonically (orange curve in Fig.
6B) ‚Äì in contrast to the same network trained with the original dataset (black curve). Instead  it grew
considerably in the second half of the network  almost saturating the upper bound  which is set by
the ED  in the output layer. This suggests that the reduction of the dimensionality of data manifolds
in the last layers of a trained network reÔ¨Çects the process of learning on a generalizable dataset. By
contrast  overÔ¨Åtting noisy labels leads to an expansion of the dimensionality  as already reported in
(16). As suggested in that study  this indicates that a network trained on inconsistent data can be
recognized without estimating its performance on a test set  but by simply looking at whether the ID
increases substantially across its Ô¨Ånal layers.

4 Conclusions and Discussion

Convolutional neural networks  as well as their biological counterparts  such as the visual system of
primates (27) and other species (26; 28)  transform the input images across a progression of processing
stages  eventually providing an explicit (i.e. transformation-tolerant) representation of visual objects
in the output layer. Leading theories in the Ô¨Åeld of visual neuroscience postulate that such re-
formatting gradually untangles and Ô¨Çattens the manifolds produced by the different images within
the representational space deÔ¨Åned by the activity of all the neurons (or units) in a layer (27; 29; 30).
This suggests that the dimensionality of the object manifolds may progressively decrease along
the layers of a deep network  and that such a decrease may be at the root of the high classiÔ¨Åcation
accuracy achieved by deep networks. Although previous theoretical and empirical studies have
provided support to this hypothesis using small/simple network architectures or focusing on single
layers of large networks (6; 10; 16; 31; 32; 33)  our study is the Ô¨Årst to investigate systematically how

8

the dimensionality of individual object manifolds - or mixtures of object manifolds - vary in large 
state-of-the-art CNNs used for image classiÔ¨Åcation.
Our results can be summarized by making reference to the cartoon shown in Fig. 7. We found that the
ID in the initial layer of a network is low. As shown in Fig. 6  this can be explained by the existence
of gradients of correlated low-level visual features (e.g.  luminance  contrast  etc.) across the image
set (34)  resulting in a stretching of image representations along a few  main variation axes within the
input space (see Fig. 7A). Early layers of DNNs appear to get rid of these correlations  which are
irrelevant for the classiÔ¨Åcation task  thus leading to an increase of the ID of the object manifolds (Fig.
2A and 3A B). As illustrated in Fig. 7B  this can be thought as a sort of whitening of the input data.
Such initial dimensionality-expansion is also thought to be performed in the visual system (30; 34) 
and is consistent with a recent characterization of the dimensionality of image representations in
primary visual cortex (25) and with the pruning of low-level information performed by high-order
visual cortical areas (26).
After this initial expansion  the representation is
squeezed into manifolds of progressively lower ID
(Figs 2  3A B)  as graphically illustrated in Fig.
7C D). This phenomenon has been already observed
by (31) and (6) on simpliÔ¨Åed datasets and architec-
tures  by (10) in the Ô¨Ånal  fully connected layers of
AlexNet  and by (16) in the last hidden layers of two
different DNNs  where the ID evolution was tracked
during training. We here demonstrate that this pro-
gressive reduction of the dimension of data manifolds
is a general behavior and a key signature of every
CNN we tested ‚Äì both small toy models (Fig. 6B)
and large state-of-the-art networks (Fig. 3A B). More
importantly  our experiments show that the extent
to which a deep network is able to compress the di-
mensionality of data representations in the last hidden
layer is a key predictor of its ability to generalize well
to unseen data (Fig. 4) ‚Äì a Ô¨Ånding that is consistent
with the inverse relationship between ID and accu-
racy reported by (16)  although our pilot tests suggest
that the ID  after a large  initial drop  can slightly in-
crease during training without producing overÔ¨Åtting
(Fig. 9). From a theoretical standpoint  this result is
broadly consistent with recent studies linking the clas-
siÔ¨Åcation capacity of data manifolds by perceptrons
to their geometrical properties (32; 35). Our Ô¨Åndings
also resonate with the compression of the information
about the input data during the Ô¨Ånal phase of training
of deep networks (36)  which is progressively larger
as a function of the layer‚Äôs depth  thus displaying a trend that is reminiscent of the one observed for
the ID in our study.
Finally  our experiments also show that the ID values are lower than those identiÔ¨Åed using PCA 
or on ‚Äòlinearized‚Äô data  which is an indication that the data lies on curved manifolds. In addition 
ID measures from PCA did not qualitatively distinguish between trained and randomly initialized
networks (Fig. 5C). This conclusion is at odds with the unfolding of data manifolds reported by (33)
across the layers of a small network tested with simple datasets. It also suggests a slight twist on
theories about transformations in the visual system (27; 29) ‚Äì it indicates that a Ô¨Çattening of data
manifolds may not be a general computational goal that deep networks strive to achieve: progressive
reduction of the ID  rather than gradual Ô¨Çattening  seems to be the key to achieving linearly separable
representations.
To conclude  we hope that data-driven  empirical approaches to investigate deep neural networks 
like the one implemented in our study  will provide intuitions and constraints  which will ultimately
inspire and enable the development of theoretical explanations of their computational capabilities.

Figure 7: A. Input layer. The intrinsic dimen-
sionality of the data can assume low values
due to the presence of irrelevant features un-
correlated with the ground truth. B. The Ô¨Årst
hidden layers pre-process the data raising its
intrinsic dimension. C D. The representation
is squeezed onto manifolds of progressively
lower intrinsic dimension. These manifolds
are typically not hyperplanes. D. In the last
hidden layer (D) the ID shows a remarkable
correlation with the performance in trained
networks. E. The output layer.

9

Acknowledgments

We thank Eis Annavini for providing the custom dataset described in A.1.1; Artur Speiser and Jan-
Matthis L√ºckmann for a careful proofreading of the manuscript. We warmly thank Elena Facco for
her valuable help in the early phases of this project. We also thank Naftali Tishby  Riccardo Zecchina 
Matteo Marsili  Tim Kietzmann  Florent Krzkala  Lenka Zdeborova  Fabio Anselmi  Luca Bortolussi 
Jim DiCarlo and SueYeon Chung for useful discussions and suggestions  and the anonymous referees
for their useful and constructive comments.
This work was supported by a European Research Council (ERC) Consolidator Grant  616803-
LEARN2SEE (D.Z.). JHM is funded by the German Research Foundation (DFG) through SFB 1233
(276693517)  SFB 1089 and SPP 2041  the German Federal Ministry of Education and Research
(BMBF  project ‚ÄòADMIMEM‚Äô  FKZ 01IS18052 A-D)  and the Human Frontier Science Program
(RGY0076/2018).

References
[1] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals  ‚ÄúUnderstanding deep learning requires

rethinking generalization ‚Äù CoRR  vol. abs/1611.03530  2016.

[2] B. Neyshabur  Z. Li  S. Bhojanapalli  Y. LeCun  and N. Srebro  ‚ÄúTowards understanding the role
of over-parametrization in generalization of neural networks ‚Äù arXiv preprint arXiv:1805.12076 
2018.

[3] A. K. Lampinen and S. Ganguli  ‚ÄúAn analytic theory of generalization dynamics and transfer

learning in deep linear networks ‚Äù arXiv preprint arXiv:1809.10374  2018.

[4] M. Denil  B. Shakibi  L. Dinh  M. Ranzato  and N. De Freitas  ‚ÄúPredicting parameters in deep

learning ‚Äù in Advances in Neural Information Processing Systems  pp. 2148‚Äì2156  2013.

[5] Y. LeCun  J. S. Denker  and S. A. Solla  ‚ÄúOptimal brain damage ‚Äù in Advances in neural

information processing systems  pp. 598‚Äì605  1990.

[6] H. Huang  ‚ÄúMechanisms of dimensionality reduction and decorrelation in deep neural networks ‚Äù

Physical Review E  vol. 98  no. 6  p. 062313  2018.

[7] L. Amsaleg  O. Chelly  T. Furon  S. Girard  M. E. Houle  K.-i. Kawarabayashi  and M. Nett  ‚ÄúEs-
timating local intrinsic dimensionality ‚Äù in Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  pp. 29‚Äì38  ACM  2015.

[8] L. Amsaleg  J. Bailey  D. Barbe  S. Erfani  M. E. Houle  V. Nguyen  and M. Radovanovi¬¥c  ‚ÄúThe
vulnerability of learning to adversarial perturbation increases with intrinsic dimensionality ‚Äù in
Information Forensics and Security (WIFS)  2017 IEEE Workshop on  pp. 1‚Äì6  IEEE  2017.

[9] X. Ma  B. Li  Y. Wang  S. M. Erfani  S. N. R. Wijewickrema  M. E. Houle  G. Schoenebeck 
D. Song  and J. Bailey  ‚ÄúCharacterizing adversarial subspaces using local intrinsic dimensional-
ity ‚Äù CoRR  vol. abs/1801.02613  2018.

[10] T. Yu  H. Long  and J. E. Hopcroft  ‚ÄúCurvature-based comparison of two neural networks ‚Äù

arXiv preprint arXiv:1801.06801  2018.

[11] A. Krizhevsky  I. Sutskever  and G. E. Hinton  ‚ÄúImagenet classiÔ¨Åcation with deep convolutional
neural networks ‚Äù in Advances in neural information processing systems  pp. 1097‚Äì1105  2012.

[12] M. Raghu  J. Gilmer  J. Yosinski  and J. Sohl-Dickstein  ‚ÄúSvcca: Singular vector canonical
correlation analysis for deep learning dynamics and interpretability ‚Äù in Advances in Neural
Information Processing Systems  pp. 6078‚Äì6087  2017.

[13] A. Morcos  M. Raghu  and S. Bengio  ‚ÄúInsights on representational similarity in neural networks
with canonical correlation ‚Äù in Advances in Neural Information Processing Systems  pp. 5727‚Äì
5736  2018.

[14] D. G. Barrett  A. S. Morcos  and J. H. Macke  ‚ÄúAnalyzing biological and artiÔ¨Åcial neural
networks: challenges with opportunities for synergy? ‚Äù Current opinion in neurobiology  vol. 55 
pp. 55‚Äì64  2019.

10

[15] S. Gong  V. N. Boddeti  and A. K. Jain  ‚ÄúOn the intrinsic dimensionality of image representa-
tions ‚Äù in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pp. 3987‚Äì3996  2019.

[16] X. Ma  Y. Wang  M. E. Houle  S. Zhou  S. M. Erfani  S.-T. Xia  S. Wijewickrema  and J. Bailey 

‚ÄúDimensionality-driven learning with noisy labels ‚Äù arXiv preprint arXiv:1806.02612  2018.

[17] E. Facco  M. d‚ÄôErrico  A. Rodriguez  and A. Laio  ‚ÄúEstimating the intrinsic dimension of
datasets by a minimal neighborhood information ‚Äù ScientiÔ¨Åc reports  vol. 7  no. 1  p. 12140 
2017.

[18] E. Levina and P. J. Bickel  ‚ÄúMaximum likelihood estimation of intrinsic dimension ‚Äù in Advances

in neural information processing systems  pp. 777‚Äì784  2005.

[19] K. He  X. Zhang  S. Ren  and J. Sun  ‚ÄúDeep residual learning for image recognition ‚Äù CoRR 

vol. abs/1512.03385  2015.

[20] K. Simonyan and A. Zisserman  ‚ÄúVery deep convolutional networks for large-scale image

recognition ‚Äù arXiv preprint arXiv:1409.1556  2014.

[21] S. Vascon  Y. Parin  E. Annavini  M. D‚ÄôAndola  D. Zoccolan  and M. Pelillo  ‚ÄúCharacterization
of visual object representations in rat primary visual cortex ‚Äù in European Conference on
Computer Vision  pp. 577‚Äì586  Springer  2018.

[22] D. Granata and V. Carnevale  ‚ÄúAccurate estimation of the intrinsic dimension using graph
distances: Unraveling the geometric complexity of datasets ‚Äù ScientiÔ¨Åc reports  vol. 6  p. 31377 
2016.

[23] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei  ‚ÄúImageNet: A Large-Scale

Hierarchical Image Database ‚Äù in CVPR09  2009.

[24] A. Achille and S. Soatto  ‚ÄúEmergence of invariance and disentanglement in deep representations ‚Äù

The Journal of Machine Learning Research  vol. 19  no. 1  pp. 1947‚Äì1980  2018.

[25] C. Stringer  M. Pachitariu  N. Steinmetz  M. Carandini  and K. D. Harris  ‚ÄúHigh-dimensional

geometry of population responses in visual cortex ‚Äù bioRxiv  p. 374090  2018.

[26] S. Tafazoli  H. Safaai  G. De Franceschi  F. B. Rosselli  W. Vanzella  M. Riggi  F. Buffolo 
S. Panzeri  and D. Zoccolan  ‚ÄúEmergence of transformation-tolerant representations of visual
objects in rat lateral extrastriate cortex ‚Äù Elife  vol. 6  p. e22794  2017.

[27] J. J. DiCarlo  D. Zoccolan  and N. C. Rust  ‚ÄúHow does the brain solve visual object recognition? ‚Äù

Neuron  vol. 73  no. 3  pp. 415‚Äì434  2012.

[28] G. Matteucci  R. B. Marotti  M. Riggi  F. B. Rosselli  and D. Zoccolan  ‚ÄúNonlinear processing
of shape information in rat lateral extrastriate cortex ‚Äù Journal of Neuroscience  pp. 1938‚Äì18 
2019.

[29] J. J. DiCarlo and D. D. Cox  ‚ÄúUntangling invariant object recognition ‚Äù Trends in cognitive

sciences  vol. 11  no. 8  pp. 333‚Äì341  2007.

[30] B. A. Olshausen and D. J. Field  ‚ÄúSparse coding of sensory inputs ‚Äù Current opinion in neurobi-

ology  vol. 14  no. 4  pp. 481‚Äì487  2004.

[31] R. Basri and D. Jacobs  ‚ÄúEfÔ¨Åcient representation of low-dimensional manifolds using deep

networks ‚Äù arXiv preprint arXiv:1602.04723  2016.

[32] S. Chung  D. D. Lee  and H. Sompolinsky  ‚ÄúClassiÔ¨Åcation and geometry of general perceptual

manifolds ‚Äù Physical Review X  vol. 8  no. 3  p. 031003  2018.

[33] P. P. Brahma  D. Wu  and Y. She  ‚ÄúWhy deep learning works: A manifold disentanglement
perspective. ‚Äù IEEE Trans. Neural Netw. Learning Syst.  vol. 27  no. 10  pp. 1997‚Äì2008  2016.

[34] E. P. Simoncelli and B. A. Olshausen  ‚ÄúNatural image statistics and neural representation ‚Äù

Annual review of neuroscience  vol. 24  no. 1  pp. 1193‚Äì1216  2001.

11

[35] S. Chung  D. D. Lee  and H. Sompolinsky  ‚ÄúLinear readout of object manifolds ‚Äù Physical

Review E  vol. 93  no. 6  p. 060301  2016.

[36] R. Shwartz-Ziv and N. Tishby  ‚ÄúOpening the black box of deep neural networks via information ‚Äù

arXiv preprint arXiv:1703.00810  2017.

[37] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 

L. Antiga  and A. Lerer  ‚ÄúAutomatic differentiation in pytorch ‚Äù in NIPS-W  2017.

[38] Y. LeCun and C. Cortes  ‚ÄúMNIST handwritten digit database ‚Äù 2010.

12

,Alessio Ansuini
Alessandro Laio
Jakob Macke
Davide Zoccolan