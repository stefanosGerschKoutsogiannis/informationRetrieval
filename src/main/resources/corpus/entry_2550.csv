2019,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervisions  LfO is more practical in leveraging previously inapplicable resources (e.g.  videos)  yet more challenging due to the incomplete expert guidance. In this paper  we investigate LfO and its difference with LfD in both theoretical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and expert  if following the modeling approach of GAIL. More importantly  the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts.,Imitation Learning from Observations by
Minimizing Inverse Dynamics Disagreement

1 Beijing National Research Center for Information Science and Technology (BNRist) 

State Key Lab on Intelligent Technology and Systems 

Department of Computer Science and Technology  Tsinghua University

2 Center for Vision  Cognition  Learning and Autonomy  Department of Computer Science  UCLA

Chao Yang1∗  Xiaojian Ma12∗  Wenbing Huang1∗ 

Fuchun Sun1  Huaping Liu1  Junzhou Huang3  Chuang Gan4

3 Tencent AI Lab  4 MIT-IBM Watson AI Lab

yangchao18@mails.tsinghua.edu.cn  maxiaojian@ucla.edu

hwenbing@126.com  fcsun@tsinghua.edu.cn

Abstract

This paper studies Learning from Observations (LfO) for imitation learning with
access to state-only demonstrations. In contrast to Learning from Demonstration
(LfD) that involves both action and state supervision  LfO is more practical in
leveraging previously inapplicable resources (e.g. videos)  yet more challenging
due to the incomplete expert guidance. In this paper  we investigate LfO and its
difference with LfD in both theoretical and practical perspectives. We ﬁrst prove
that the gap between LfD and LfO actually lies in the disagreement of inverse
dynamics models between the imitator and the expert  if following the modeling
approach of GAIL [15]. More importantly  the upper bound of this gap is revealed
by a negative causal entropy which can be minimized in a model-free way. We
term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which
enhances the conventional LfO method through further bridging the gap to LfD.
Considerable empirical results on challenging benchmarks indicate that our method
attains consistent improvements over other LfO counterparts.

1

Introduction

A crucial aspect of intelligent robots is their ability to perform a task of interest by imitating
expert behaviors from raw sensory observations [5]. Towards this goal  GAIL [15] is one of the
most successful imitation learning methods  which adversarially minimizes the discrepancy of the
occupancy measure between the agent and the expert for policy optimization. However  along with
many other methods [31  3  29  30  1  23  6  9  13  2]  GAIL adopts a heavily supervised training
mechanism  which demands not only the expert’s state (e.g. observable spatial locations)  but also its
accurate action (e.g. controllable motor commands) performed at each time step.
Whereas providing expert action indeed enriches the information and hence facilitates the imitation
learning process  collecting them could be difﬁcult and sometimes infeasible for some certain practical
cases  particularly when we would like to learn skills from a large number of internet videos. Besides 
imitation learning under action guidance is not biologically reasonable [39]  as our human can imitate
skills through adjusting the action to match the demonstrators’ state  without knowing what exact
action the demonstrator has performed. To address these concerns  several methods have been
proposed [35  39  5  21  36]  including the one named GAIfO [40] that extends the idea of GAIL to

∗Denotes equal contributions. Corresponding author: Fuchun Sun.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the case with the absence of action guidance. Joining the previous denotations  this paper will deﬁne
the original problem as Learning from Demonstrations (LfD)  and the new action agnostic setting as
Learning from Observations (LfO).
Undoubtedly  conducting LfO is non-trivial. For many tasks (e.g. robotic manipulation  locomotion
and video-game playing)  the reward function depends on both action and state. It remains chal-
lenging to determine the optimal action corresponding to the best reward purely from experts’ state
observations  since there could be multiple choices of action corresponding to the same sequence of
state in a demonstration—when  for example  manipulating redundant-degree robotic hands  there
exist countless force controls of joints giving rise to the same pose change. Yet  realizing LfO is still
possible  especially if the expert and the agent share the same dynamics system (namely  the same
robot). In this condition and what this paper has assumed  the correlation between action and state
can be learned by the self-playing of the agent (see for example in [39]).
In this paper  we approach LfO by leveraging the concept of inverse dynamics disagreement mini-
mization. As its name implies  inverse dynamics disagreement is deﬁned as the discrepancy between
the inverse dynamics models of the expert and the agent. Minimizing such disagreement becomes the
task of inverse dynamics prediction  a well-known problem that has been studied in robotics [24].
Interestingly  as we will draw in this paper  the inverse dynamics disagreement is closely related
to LfD and LfO. To be more speciﬁc  we prove that the inverse dynamics disagreement actually
accounts for the optimization gap between LfD and naive LfO  if we model LfD by using GAIL [15]
and consider naive LfO as GAIfO [40]. This result is crucial  not only for it tells the quantitative
difference between LfD and naive LfO but also for it enables us to solve LfO more elegantly by
minimizing the inverse dynamics disagreement as well.
To mitigate the issue of inverse dynamics disagreement  here we propose a model-free solution
for the consideration of efﬁciency. In detail  we derive an upper bound of the gap  which turns
out to be a negative entropy of the state-action occupancy measure. Under the assumption of
deterministic system  such entropy contains a mutual information term that can be optimized with the
popularly-used tool (i.e. MINE [4]). For convenience  we term our method as the Inverse-Dynamics-
Disagreement-Minimization (IDDM) based LfO in what follows. To verify the effectiveness of our
IDDM  we perform experimental comparisons on seven challenging control tasks  ranging from
traditional control to locomotion [8]. The experimental results demonstrate that our proposed method
attains consistent improvements over other LfO counterparts.
The rest of the paper is organized as follows. In Sec. 2  we will ﬁrst review some necessary notations
and preliminaries. Then our proposed method will be detailed in Sec. 3 with theoretical analysis and
efﬁcient implementation  and the discussions with existing LfD and LfO methods will be included in
Sec. 4. Finally  experimental evaluations and ablation studies will be demonstrated in Sec. 5.

2 Preliminaries

Notations. To model the action decision procedure in our context  we consider a standard Markov
decision process (MDP) [37] as (S A  r T   µ  γ)  where S and A are the sets of feasible state and
action  respectively; r(s  a) : S × A → R denotes the reward function on state s and action a;
T (s(cid:48)|s  a) : S × A × S → [0  1] characterizes the dynamics of the environment and deﬁnes the
transition probability to next-step state s(cid:48) if the agent takes action a at current state s; µ(s) : S →
[0  1] is the distribution of initial state and γ ∈ (0  1) is the discount factor. A stationary policy
π(a|s) : S × A → [0  1] deﬁnes the probability of choosing action a at state s. A temporal sequence
of state-action pairs {(s0  a0)  (s1  a1) ···} is called a trajectory denoted by ζ.

Occupancy measure. To characterize the statistical properties of an MDP  the concept of occupancy
measure [28  38  15  17] is proposed to describe the distribution of state and action under a given
policy π. Below  we introduce its simplest form  i.e.  State Occupancy Measure.
Deﬁnition 1 (State Occupancy Measure). Given a stationary policy π  state occupancy measure
ρπ(s) : S → R denotes the discounted state appearance frequency under policy π

ρπ(s) =

γtP (st = s|π).

(1)

∞(cid:88)

t=0

2

With the use of state occupancy measure  we can deﬁne other kinds of occupancy measures under
different supports  including state-action occupancy measure  station transition occupancy measure 
and joint occupancy measure. We list their deﬁnitions in Tab. 1 for reader’s reference.

Inverse dynamics model. We present the inverse dynamics model [34  33] in Deﬁnition 2  which
infers the action inversely given state transition (s  s(cid:48)).

Table 1: Different occupancy measures for MDP

State-Action

Occupancy Measure

Denotation

Support
Deﬁnition

ρπ(s  a)
S × A

ρπ(s)π(a|s)

State Transition

Occupancy Measure

ρπ(s  s(cid:48))
S × S

(cid:82)
A ρπ(s  a)T (s(cid:48)|s  a)da

Joint

Occupancy Measure

ρπ(s  a  s(cid:48))
S × A × S

ρπ(s  a)T (s(cid:48)|s  a)

Deﬁnition 2 (Inverse Dynamics Model). Let ρπ(a|s  s(cid:48)) denotes the density function of the inverse
dynamics model under the policy π  whose relation with T and π can be shown as follows.

ρπ(a|s  s(cid:48)) :=

3 Methodology

(cid:82)
T (s(cid:48)|s  a)π(a|s)
A T (s(cid:48)|s  ¯a)π(¯a|s)d¯a

.

(2)

In this section  we ﬁrst introduce the concepts of LfD  naive LfO  and inverse dynamics disagreement.
Then  we prove that the optimization gap between LfD and naive LfO actually leads to the inverse
dynamics disagreement. As such  we enhance naive LfO by further minimizing the inverse dynamics
disagreement. We also demonstrate that such disagreement can be bounded by an entropy term and
can be minimized by a model-free method. Finally  we provide a practical implementation for our
proposed method.

3.1

Inverse Dynamics Disagreement: the Gap between LfD and LfO

LfD.
In Sec. 1  we have mentioned that GAIL and many other LfD methods [15  18  16  27] exploit
the discrepancy of the occupancy measure between the agent and expert as a reward for policy
optimization. Without loss of generality  we will consider GAIL as the representative LfD framework
and build our analysis on this description. This LfD framework requires to compute the discrepancy
over the state-action occupancy measure  leading to

DKL (ρπ(s  a)||ρE(s  a))  

min

(3)
where ρE(s  a) denotes the occupancy measure under the expert policy  and DKL(·) computes the
Kullback-Leibler (KL) divergence2. We have omitted the policy entropy term in GAIL  but our
following derivations will ﬁnd that the policy entropy term is naturally contained in the gap between
LfD and LfO.

π

In LfO  the expert action is absent  thus directly working on DKL(ρπ(s  a)||ρE(s  a))
Naive LfO.
is infeasible. An alternative objective could be minimizing the discrepancy on the state transition
occupancy measure ρπ(s  s(cid:48))  as mentioned in GAIfO [40]. The objective function in (3) becomes
(4)

DKL (ρπ(s  s(cid:48))||ρE(s  s(cid:48))) .

min

π

2 The original GAIL method applies Jensen-Shannon (JS) divergence rather than KL divergence for measure-
ment. Here  we will use KL divergence for the consistency throughout our derivations. Indeed  our method is
also compatible with JS divergence  with the details provided in the supplementary material.

3

We will refer this as naive LfO in the following context. Compared to LfD  the key challenge in
LfO comes from the absence of action information  which prevents it from applying typical action-
involved imitation learning approaches like behavior cloning [31  3  11  29  30] or apprenticeship
learning [23  1  38]. Actually  action information can be implicitly encoded in the state transition
(s  s(cid:48)). We have assumed the expert and the agent share the same dynamics system T (s(cid:48)|s  a). It is
thus possible for us to learn the action-state relation by exploring the difference between their inverse
dynamics models.
We deﬁne the inverse dynamics disagreement between the expert and the agent as follows.
Deﬁnition 3 (Inverse Dynamics Disagreement). Given expert policy πE and agent policy π  the
inverse dynamics disagreement is deﬁned as the KL divergence between the inverse dynamics models
of the expert and the agent.

Inverse Dynamics Disagreement := DKL (ρπ(a|s  s(cid:48))||ρE(a|s  s(cid:48))) .

(5)
Given a state transition (s  s(cid:48))  minimizing the inverse dynamics disagreement is learning an optimal
policy to ﬁt the expert/ground-truth action labels. This is a typical robotic task [24]  and it can be
solved by using a mixture method of combining machine learning model and control model.
Here  we contend another role of the inverse dynamics disagreement in the context of imitation
learning. Joining the denotations in (3)  (4) and Deﬁnition 3  we provide the following result.
Theorem 1. If the agent and the expert share the same dynamics system  the relation between LfD 
naive LfO  and inverse dynamics disagreement can be characterized as

DKL (ρπ(a|s  s(cid:48))||ρE(a|s  s(cid:48))) = DKL (ρπ(s  a)||ρE(s  a)) − DKL (ρπ(s  s(cid:48))||ρE(s  s(cid:48))) .

(6)
Theorem 1 states that the inverse dynamics disagreement essentially captures the optimization gap
between LfD and naive LfO. As (5) is non-negative by nature  optimizing the objective of LfD implies
minimizing the objective of LfO but not vice versa. One interesting observation is that when the
action corresponding to a given state transition is unique (or equivalently  the dynamics T (s(cid:48)|s  a) is
injective w.r.t a)  the inverse dynamics is invariant to different conducted policies  hence the inverse
dynamics disagreement between the expert and the agent reduces to zero. We summarize this by the
following corollary.
Corollary 1. If the dynamics T (s(cid:48)|s  a) is injective w.r.t a  LfD is equivalent to naive LfO.

DKL (ρπ(s  a)||ρE(s  a)) = DKL (ρπ(s  s(cid:48))||ρE(s  s(cid:48))) .

(7)
However  since most of the real world tasks are performed in rather complex environments  (5) is
usually not equal to zero and the gap between LfD and LfO should not be overlooked  which makes
minimizing the inverse dynamics disagreement become unavoidable.

3.2 Bridging the Gap with Entropy Maximization

We have shown that the inverse dynamics disagreement amounts to the optimization gap between
LfD and naive LfO. Therefore  the key to improving naive LfO mainly lies in inverse dynamics
disagreement minimization. Nevertheless  accurately computing the disagreement is difﬁcult  as it
relies on the environment dynamics T and the expert policy (see (2))  both of which are assumed to
be unknown. In this section  we try a smarter way and propose an upper bound for the gap  without the
access of the dynamics model and expert guidance. This upper bound is tractable to be minimized if
assuming the dynamics to be deterministic. We introduce the upper bound by the following theorem.
Theorem 2. Let Hπ(s  a) and HE(s  a) denote the causal entropies over the state-action occupancy
measure of the agent and expert  respectively. When DKL [ρπ(s  s(cid:48))||ρE(s  s(cid:48))] is minimized  we have
(8)
Now we take a closer look at Hπ(s  a). Following the deﬁnition in Tab. 1  the entropy of state-action
occupancy measure can be decomposed as the sum of the policy entropy and the state entropy by

DKL [ρπ(a|s  s(cid:48))||ρE(a|s  s(cid:48))] (cid:54) −Hπ(s  a) + Const.

Hπ(s  a) = Eρπ(s a) [− log ρπ(s  a)] = Eρπ(s a) [− log π(a|s)] + Eρπ(s) [− log ρπ(s)]

(9)
For the ﬁrst term  the policy entropy Hπ(a|s) can be estimated via sampling similar to previous
studies [15]. For the second term  we leverage the mutual information (MI) between s and (s(cid:48)  a) to
obtain an unbiased estimator of the entropy over the state occupancy measure  namely 

= Hπ(a|s) + Hπ(s).

4

Algorithm 1 Inverse-Dynamics-Disagreement-Minimization (IDDM)
i } where ζi = {sE
Input: State-only expert demonstrations DE = {ζ E

nator Dφ  MI estimator I  entropy weights λp  λs  maximum iterations M.
for 1 to M do

0   sE

Sample agent rollouts DA = {ζ i}  ζ i ∼ πθ and update the MI estimator I with DA.
Update the discriminator Dφ with the gradient

1   ...}  policy πθ  discrimi-

ˆEDA [∇φ log Dφ(s  s(cid:48))] + ˆEDE [∇φ log(1 − Dφ(s  s(cid:48)))] .

Update policy πθ using the following gradient (can be integrated into methods like PPO [32])

ˆEDA[∇θ log πθ(a|s)Q(s  a)] − λp∇θHπθ (a|s) − λs∇θIπθ (s; (s(cid:48)  a)) 
where Q(¯s  ¯a) = ˆEDA [log Dφ(s  s(cid:48))|s0 = ¯s  a0 = ¯a] .

end for

(cid:124)

(cid:123)(cid:122)

(cid:125)

Hπ(s) = Iπ(s; (s(cid:48)  a)) + Hπ(s|s(cid:48)  a)

= Iπ(s; (s(cid:48)  a)) 

=0

(10)
where we have Hπ(s|s(cid:48)  a) = 0 as we have assumed (s  a) → s(cid:48) is a deterministic function3. In
our implementation  the MI Iπ(s; (s(cid:48)  a)) is computed via maximizing the lower bound of KL
divergence between the product of marginals and the joint distribution following the formulation
of [25]. Speciﬁcally  we adopt MINE [4  14] which implements the score function with a neural
network to achieve a low-variance MI estimator.
Overall loss. By combining the results in Theorem 1  Theorem 2  (9) and (10)  we enhance naive
LfO by further minimizing the upper bound of its gap to LfD. The eventual objective is

Lπ = DKL(ρπ(s  s(cid:48))||ρE(s  s(cid:48))) − λpHπ(a|s) − λsIπ(s; (s(cid:48)  a)) 

(11)
where the ﬁrst term is from naive LfO  and the last two terms are to minimize the gap between LfD
and naive LfO. We also add trade-off weights λp and λs to the last two terms for more ﬂexibility.

Implementation. As our above derivations can be generalized to JS-divergence (see Sec. A.3-4
in the supplementary material)  we can utilize the GAN-like [25] method to minimize the ﬁrst term
in (11). In detail  we introduce a parameterized discriminator network Dφ and a policy network πθ
(serves as a generator) to realize the ﬁrst term in (11). The term log Dφ(s  s(cid:48)) could be interpreted as
an immediate cost since we minimize its expectation over the current occupancy measure. A similar
training method can also be found in GAIL [15]  but it relies on state-action input instead. We defer
the derivations for the gradients of the causal entropy ∇Hπ(a|s) and MI ∇Iπ(s; (s(cid:48)  a)) with respect
to the policy in Sec. A.5 of the supplementary material. Note that the objective (11) can be optimized
by any policy gradient method  like A3C [22] or PPO [32]  and we apply PPO in our experiments.
The algorithm details are summarized in Alg. 1.

4 Related Work

4.1 Learning from Demonstrations

Modern dominant approaches on LfD mainly fall into two categories: Behavior Cloning (BC) [31 
3  29  30]  which seeks the best policy that can minimize the action prediction error in demonstration 
and Inverse Reinforcement Learning (IRL) [23  1]  which infers the reward used by expert to guide
the agent policy learning procedure. A notable implementation of the latter is GAIL [15]  which
reformulates IRL as an occupancy measure matching problem [28]  and utilizes the GAN [12] method
3In this paper  the tasks in our experiments indeed reveal deterministic dynamics. The mapping (s  a) → s(cid:48)
is deterministic also underlying that (s(cid:48)  a) → s is deterministic. Referring to [20]  when s  s(cid:48)  a are continuous 
H(s|s(cid:48)  a) can be negative; but since these variables are actually quantiﬁed as ﬁnite-bit precision numbers (e.g.
stored as 32-bit discrete numbers in computer)  it is still true that conditional entropy is zero in practice.

5

along with a forward RL to minimize the discrepancy of occupancy measures between imitator and
demonstrator. There are also several follow-up works that attempt to enhance the effectiveness of
discrepancy computation [19  16  10  27]  whereas all these methods require exact action guidance at
each time step.

4.2 Learning from Observations

There have already been some researches on exploring LfO. These approaches exploit either a
complex hand-crafted reward function or an inverse dynamics model that predicts the exact action
given state transitions. Here is a summary to show how they are connected to our method.

LfO with Hand-crafted Reward and Forward RL. Recently  Peng et al. propose DeepMimic 
a method that can imitate locomotion behaviors from motion clips without action labeling. They
design a reward to encourage the agent to directly match the expert’s physical proprieties  such as
joint angles and velocities  and run a forward RL to learn the imitation policy. However  as the
hand-crafted reward function does not take expert action (or implicitly state transition) into account 
it is hard to be generalized to tasks whose reward depends on actions.

Model-Based LfO. BCO [39] is another LfO approach. The authors infer the exact action from
state transition with a learned inverse dynamics model (2). The state demonstrations augmented with
the predicted actions deliver common state-action pairs that enable imitation learning via BC [31]. At
its heart  the inverse dynamics model is trained in parallel by collecting rollouts in the environment.
However  as showed in (2)  the inverse dynamics model depends on the current policy  underlying
that an optimal inverse dynamics model would be infeasible to obtain before the optimal policy is
learned. The performance of BCO would thus be not theoretically guaranteed.

LfO with GAIL. GAIfO [40  41] is the closest work to our method. The authors follow the
formulation of GAIL [15] but replace the state-action deﬁnition (s  a) with state transition (s  s(cid:48)) 
which gives the same objective in Eq. (4) if replacing KL with JS divergence. As we have discussed
in Sec. 3.1  there is a gap between Eq. (4) and the objective of original LfD in Eq. (3)  and this gap
is induced by inverse dynamics disagreement. Unlike our method  the solution by GAIfO never
minimizes the gap and is thereby no better than ours in principal.

5 Experiments

For the experiments below  we investigate the following questions:

1. Does inverse dynamics disagreement really account for the gap between LfD and LfO?
2. With state-only guidance  can our method achieve better performance than other counterparts

that do not consider inverse dynamics disagreement minimization?

3. What are the key ingredients of our method that contribute to performance improvement?

To answer the ﬁrst question  we conduct toy experiments with the Gridworld environment [37]. We
test and contrast the performance of our method (refer to Eq. (11)) against GAIL (refer to Eq. (3))
and GAIfO (refer to Eq. (4)) on the tasks under different levels of inverse dynamics disagreement.
Regarding the second question  we evaluate our method against several baselines on six physics-
based control benchmarks [8]  ranging from low-dimension control to challenging high-dimension
continuous control. Finally  we explore the ablation analysis of two major components in our method
(the policy entropy term and the MI term) to address the last question. Due to the space limit  we
defer more detailed speciﬁcations of all the evaluated tasks into the supplementary material.

5.1 Understanding the Effect of Inverse Dynamics Disagreement

This collection of experiments is mainly to demonstrate how inverse dynamics disagreement inﬂu-
ences the LfO approaches. We ﬁrst observe that inverse dynamics disagreement will increase when
the number of possible action choices grows. This is justiﬁed in Fig. 1a  and more details about
the relation between inverse dynamics disagreement and the number of action choices are provided
in Sec. B.2 of the supplementary material. Hence  we can utilize different action scales to reﬂect

6

different levels of inverse dynamics disagreement in our experiments. Controlling the action scale in
Gridworld is straightforward. For example in Fig. 1b  agent block (in red) may take various kinds of
actions (walk  jump or others) for moving to a neighbor position towards the target (in green)  and we
can specify different numbers of action choices.
We simulate the expert demonstrations by collecting the trajectories of the policy trained by PPO [32].
Then we conduct GAIL  GAIfO  and our method  and evaluate the pre-deﬁned reward values for
the policies they learn. It should be noted that all imitation learning methods have no access to the
reward function during training. As we can see in Fig. 1c  the gap between GAIL and GAIfO is
growing as the number of action choices (equivalently the level of inverse dynamics disagreement)
increases  which is consistent with our conclusion in Theorem 1. We also ﬁnd that the rewards
of GAIL and GAIfO are the same when the number of action choice is 1 (i.e.
the dynamics is
injective)  which follows the statement in Corollary 1. Our method lies between GAIL and GAIfO 
indicating that the gap between GAIL and GAIfO can be somehow mitigated by explicitly minimizing
inverse dynamics disagreement. Note that  GAIL also encounters performance drop when inverse
dynamics disagreement becomes large. This is mainly because the imitation learning problem itself
also becomes more difﬁcult when the dynamics is complicated and beyond injective.

(a)

(b)

(c)

Figure 1: Toy examples on illustrating the effect of inverse dynamics disagreement.

5.2 Comparative Evaluations

For comparative evaluations  we carry out several LfO baselines  including DeepMimic [26] 
BCO [39]  and GAIfO [40]. In particular  we introduce a modiﬁed version of GAIfO that only
takes a single state as input to illustrate the necessity of leveraging state transition; we denote this
method as GAIfO-s. We also run GAIL [15] to provide oracle reference. All experiments are
evaluated within ﬁxed steps. On each task  we run each algorithm over ﬁve times with different
random seeds. In Fig. 2  the solid curves correspond to the mean returns  and the shaded regions
represent the variance over the ﬁve runs. The eventual results are summarized in Tab. 2  which is
averaged over 50 trials of the learned policies. Due to the space limit  we defer more details to the
supplementary material.

Table 2: Summary of quantitative results. All results correspond to the original exact reward deﬁned
in [7]. CartPole is excluded from DeepMimic because no crafting reward is available.

CartPole

-

Pendulum DoublePendulum
HalfCheetah
731.0±19.0 454.4±154.0 2292.6±1068.9 202.6±4.4

Hopper

200.0±0.0 24.9±0.8

DeepMimic
80.3±13.1
BCO
GAIfO 197.5±7.3 980.2±3.0 4240.6±4525.6
GAIfO-s∗ 200.0±0.0 952.1±23.0 1089.2±51.4
9359.7±0.2

-985.3±13.6
1266.2±1062.8 4557.2±90.0 562.5±384.1
1021.4±0.6 3955.1±22.1 -1415.0±161.1
1022.5±0.40 2896.5±53.8 -5062.3±56.9
3300.9±52.1 5699.3±51.8 2800.4±14.0
200.0±0.0 1000.0±0.0
200.0±0.0 1000.0±0.0 9174.8±1292.5 3249.9±34.0 6279.0±56.5 5508.8±791.5
200.0±0.0 1000.0±0.0
3645.7±181.8 5988.7±61.8 5746.8±117.5

Ant

Ours
GAIL
Expert

9318.8±8.5

∗GAIfO with single state only.

The results read that our method achieves comparable performances with the baselines on the easy
tasks (such as CartPole) and outperforms them by a large margin on the difﬁcult tasks (such as
Ant  Hopper). We also ﬁnd that our algorithm exhibits more stable behaviors. For example  the

7

246810Number of possible action choices0.00.10.20.30.4Disagreementstart stateend statewalk actionjump action246810Number of possible action choices20406080Averaged returnGAILGAIfOOursFigure 2: Learning curves under challenging robotic control benchmarks. For each experiment  a step
represents one interaction with the environment. Detailed plots can be found in the supplementary.
performance of BCO on Ant and Hopper will unexpectedly drop down as the training continues.
We conjecture that BCO explicitly but not accurately learns the inverse dynamics model from data 
which yet is prone to over-ﬁtting and leads to performance degradation. Conversely  our algorithm
is model-free and guarantees the training stability as well as the eventual performance  even for the
complex tasks including HalfCheetah  Ant and Hopper.
Besides  GAIfO performs better than GAIfO-s in most of the evaluated tasks. This illustrates the
importance of taking state-transition into account to reﬂect action information in LfO. Compared
with GAIfO  our method clearly attains consistent and signiﬁcant improvements on HalfCheetah
(+1744.2)  Ant (+4215.0) and Hopper (+2279.5)  thus convincingly verifying that minimizing the
optimization gap induced by inverse dynamics disagreement plays an essential role in LfO  and our
proposed approach can effectively bridge the gap. For the tasks that have relatively simple dynamics
(e.g. CartPole)  GAIfO achieves satisfying performances  which is consistent with our conclusion in
Corollary 1.
DeepMimic that relies on hand-crafted reward struggles on most of the evaluated tasks. Our proposed
method does not depend on any manually-designed reward signal  thus it becomes more self-contained
and more practical in general applications.

Figure 3: Comparative results of GAIL [15]  GAIfO [40] and our method with different number of
trajectories in demonstrations. The performance is the averaged cumulative return over 5 trajectories
and has been scaled within [0  1] (the random and the expert policies are ﬁxed to be 0 and 1  respec-
tively). We also conduct experiments with demonstrations containing state-action/state-transition
pairs with the number less than that within one complete trajectory. We use 32(cid:48)  128(cid:48) and 256(cid:48) pairs
(denoted in the beginning of the x axes) for the ﬁrst three tasks  respectively.

Finally  we compare the performances of GAIL  GAIfO and our method with different numbers
of demonstrations. The results are presented in Fig. 3. It reads that for simple tasks like CartPole
and Pendulum  there are no signiﬁcant differences for all evaluated approaches  when the number of

8

0.00.20.40.60.81.00.00.20.40.60.81.0Averaged returnExpertBCODeepMimicGAIfOGAIfO (single state)GAILOurs0.000.250.500.751.00CartPole1e3501001502000.00.51.01.5Pendulum1e30200400600800100001234DoublePendulum1e302000400060008000024Hopper1e601000200030000.00.51.01.5HalfCheetah1e702000400060000123Ant1e7200002000400060000.00.20.40.60.81.0Number of trajectories in demonstrations0.00.20.40.60.81.0Performance (Scaled)Expert (PPO)RandomGAILGAIfOOurs32'15250.00.51.0CartPole128'15500.00.51.0Pendulum256'15500.00.51.0DoublePendulum51020500.00.20.40.60.81.0Hopper51020500.00.20.40.60.81.0HalfCheetah51020500.50.00.51.0Antdemonstrations changes. While for the tasks with a higher dimension of state and action  our method
performs advantageously over GAIfO. Even compared with GAIL that involves action demonstrations 
our method still delivers comparable results. For all methods  more demonstrations facilitate better
performances especially when the tasks become more complicated (HalfCheetah and Ant).

5.3 Ablation Study

The results presented in the previous section suggest that our proposed method can outperform other
LfO approaches on several challenging tasks. Now we further perform a diverse set of analyses on
assessing the impact of the policy entropy term and the MI term in (11). As these two terms are
controlled by λp  λs  we will explore the sensitivity of our algorithm in terms of their values.

Sensitivity to Policy Entropy. We design four groups of
parameters on HalfCheetah  where λp is selected from
{0  0.0005  0.001  0.01} and λs is ﬁxed at 0.01. The ﬁnal re-
sults are plotted in Fig. 4  with the learning curves and detailed
quantitative results provided in the supplementary material. The
results suggest that we can always promote the performances by
adding policy entropy. Although different choices of λp induce
minor differences in their ﬁnal performances  they are overall
better than GAIfO that does not include the policy entropy term
in its objective function.

Figure 4: Sensitivity to the policy
entropy weight λp.

Sensitivity to Mutual Information. We conduct four groups
of experiments on HalfCheetah by ranging λs from 0.0 to 0.1
and ﬁxing λp to be 0.001. The ﬁnal results are shown in Fig. 5
(the learning curves and averaged return are also reported in
the supplementary material). It is observed that the imitation
performances could always beneﬁt from adding the MI term 
and the improvements become more signiﬁcant when the λs has
a relatively large magnitude. All of the variants of our method
consistently outperform GAIfO  thus indicating the importance
of the mutual information term in our optimization objective.
We also provide the results of performing a grid search on λs and λp in the supplementary material
to further illustrate how better performance could be potentially obtained.

Figure 5: Sensitivity to the MI
weight λs.

6 Conclusion
In this paper  our goal is to perform imitation Learning from Observations (LfO). Based on the
theoretical analysis for the difference between LfO and Learning from Demonstrations (LfD)  we
introduce inverse dynamics disagreement and demonstrate it amounts to the gap between LfD and
LfO. To minimize inverse dynamics disagreement in a principled and efﬁcient way  we realize
its upper bound as a particular negative causal entropy and optimize it via a model-free method.
Our model  dubbed as Inverse-Dynamics-Disagreement-Minimization (IDDM)  attains consistent
improvement over other LfO counterparts on various challenging benchmarks. While our paper
mainly focuses on control planning  further exploration on combining our work with representation
learning to enable imitation across different domains could be a new direction for future work.

Acknowledgments

This research was funded by National Science and Technology Major Project of the Ministry of
Science and Technology of China (No.2018AAA0102900).
It was also partially supported by
National Science Foundation of China (Grant No.91848206)  National Science Foundation of China
(NSFC) and the German Research Foundation (DFG) in project Cross Modal Learning  NSFC
61621136008/DFG TRR-169. We would like to thank Mingxuan Jing and Dr. Boqing Gong for the
insightful discussions and the anonymous reviewers for the constructive feedback.

9

350040004500500055006000Averaged returnGAIfOp=0p=0.0005p=0.001p=0.01350040004500500055006000Averaged returnGAIfOs=0s=0.001s=0.01s=0.1References
[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning.

In International conference on Machine learning (ICML)  2004.

[2] Marcin Andrychowicz  Bowen Baker  Maciek Chociej  Rafal Jozefowicz  Bob McGrew  Jakub
Pachocki  Arthur Petron  Matthias Plappert  Glenn Powell  Alex Ray  et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177  2018.

[3] Christopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In International

Conference on Machine Learning (ICML)  1997.

[4] Mohamed Ishmael Belghazi  Aristide Baratin  Sai Rajeshwar  Sherjil Ozair  Yoshua Bengio 
Aaron Courville  and Devon Hjelm. Mutual information neural estimation. In International
Conference on Machine Learning (ICML)  2018.

[5] Darrin C Bentivegna  Christopher G Atkeson  and Gordon Cheng. Learning tasks from observa-

tion and practice. Robotics and Autonomous Systems  47(2-3):163–169  2004.

[6] Mariusz Bojarski  Davide Del Testa  Daniel Dworakowski  Bernhard Firner  Beat Flepp  Prasoon
Goyal  Lawrence D Jackel  Mathew Monfort  Urs Muller  Jiakai Zhang  et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316  2016.

[7] Greg Brockman  Vicki Cheung  Ludwig Pettersson  Jonas Schneider  John Schulman  Jie Tang 

and Wojciech Zaremba. Openai gym  2016.

[8] Yan Duan  Xi Chen  Rein Houthooft  John Schulman  and Pieter Abbeel. Benchmarking
deep reinforcement learning for continuous control. In International Conference on Machine
Learning (ICML)  2016.

[9] Yan Duan  Marcin Andrychowicz  Bradly Stadie  Jonathan Ho  Jonas Schneider  Ilya Sutskever 
Pieter Abbeel  and Wojciech Zaremba. One-shot imitation learning. In Advances in neural
information processing systems (NeurIPS)  2017.

[10] Justin Fu  Katie Luo  and Sergey Levine. Learning robust rewards with adversarial inverse

reinforcement learning. International conference on Learning Representation (ICLR)  2018.

[11] Alessandro Giusti  Jérôme Guzzi  Dan C Ciresan  Fang-Lin He  Juan P Rodríguez  Flavio
Fontana  Matthias Faessler  Christian Forster  Jürgen Schmidhuber  Gianni Di Caro  et al. A
machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics
and Automation Letters (RA-L)  2016.

[12] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems (NeurIPS)  2014.

[13] Tuomas Haarnoja  Aurick Zhou  Sehoon Ha  Jie Tan  George Tucker  and Sergey Levine.
Learning to walk via deep reinforcement learning. In Robotics: Science and Systems (RSS) 
2019.

[14] R Devon Hjelm  Alex Fedorov  Samuel Lavoie-Marchildon  Karan Grewal  Phil Bachman 
Adam Trischler  and Yoshua Bengio. Learning deep representations by mutual information
estimation and maximization. In International Conference on Learning Representations (ICLR) 
2019.

[15] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in

Neural Information Processing Systems (NeurIPS)  2016.

[16] Mingxuan Jing  Xiaojian Ma  Wenbing Huang  Fuchun Sun  and Huaping Liu. Task transfer by

preference-based cost learning. In AAAI Conference on Artiﬁcial Intelligence (AAAI)  2019.

[17] Bingyi Kang  Zequn Jie  and Jiashi Feng. Policy optimization with demonstrations.

International Conference on Machine Learning (ICML)  2018.

In

[18] Beomjoon Kim and Joelle Pineau. Maximum mean discrepancy imitation learning. In Robotics:

Science and systems (RSS)  2013.

[19] Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In AAAI

Conference on Artiﬁcial Intelligence (AAAI)  2018.

[20] Rithesh Kumar  Anirudh Goyal  Aaron Courville  and Yoshua Bengio. Maximum entropy

generators for energy-based models. arXiv preprint arXiv:1901.08508  2019.

10

[21] YuXuan Liu  Abhishek Gupta  Pieter Abbeel  and Sergey Levine. Imitation from observation:
Learning to imitate behaviors from raw video via context translation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA)  2018.

[22] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lilli-
crap  Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. In International conference on machine learning (ICML)  2016.

[23] Andrew Y Ng  Stuart J Russell  et al. Algorithms for inverse reinforcement learning.

International conference on Machine learning (ICML)  2000.

In

[24] Duy Nguyen-Tuong and Jan Peters. Model learning for robot control: a survey. Cognitive

processing  12(4):319–340  2011.

[25] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems (NeurIPS)  2016.

[26] Xue Bin Peng  Pieter Abbeel  Sergey Levine  and Michiel van de Panne. Deepmimic: Example-
guided deep reinforcement learning of physics-based character skills. ACM Transactions on
Graphics (TOG)  2018.

[27] Xue Bin Peng  Angjoo Kanazawa  Sam Toyer  Pieter Abbeel  and Sergey Levine. Variational
discriminator bottleneck: Improving imitation learning  inverse RL  and GANs by constraining
information ﬂow. In International Conference on Learning Representations (ICLR)  2019.

[28] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming.

John Wiley & Sons  1994.

[29] Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In International

Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2010.

[30] Stephane Ross and J Andrew Bagnell. Agnostic system identiﬁcation for model-based rein-

forcement learning. In International conference on Machine learning (ICML)  2012.

[31] Stefan Schaal. Learning from demonstration. In Advances in neural information processing

systems (NeurIPS)  1997.

[32] John Schulman  Filip Wolski  Prafulla Dhariwal  Alec Radford  and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347  2017.

[33] Bruno Siciliano and Oussama Khatib. Springer handbook of robotics. Springer  2008.
[34] Mark W Spong and Romeo Ortega. On adaptive inverse dynamics control of rigid robots. IEEE

Transactions on Automatic Control (T-AC)  1990.

[35] Bradly C Stadie  Pieter Abbeel  and Ilya Sutskever. Third-person imitation learning.

International Conference on Learning Representations (ICLR)  2017.

In

[36] Wen Sun  Anirudh Vemula  Byron Boots  and Drew Bagnell. Provably efﬁcient imitation
learning from observation alone. In International Conference on Machine Learning (ICML) 
2019.

[37] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

1998.

[38] Umar Syed  Michael Bowling  and Robert E Schapire. Apprenticeship learning using linear

programming. In International Conference on Machine Learning (ICML)  2008.

[39] Faraz Torabi  Garrett Warnell  and Peter Stone. Behavioral cloning from observation.

International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2018.

In

[40] Faraz Torabi  Garrett Warnell  and Peter Stone. Generative adversarial imitation from observa-

tion. arXiv preprint arXiv:1807.06158  2018.

[41] Faraz Torabi  Garrett Warnell  and Peter Stone. Imitation learning from video by leveraging

proprioception. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  2019.

11

,Chao Yang
Xiaojian Ma
Wenbing Huang
Fuchun Sun
Huaping Liu
Junzhou Huang
Chuang Gan