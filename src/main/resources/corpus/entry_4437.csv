2013,Probabilistic Principal Geodesic Analysis,Principal geodesic analysis (PGA) is a generalization of principal component analysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Currently PGA is defined as a geometric fit to the data  rather than as a probabilistic model. Inspired by probabilistic PCA  we present a latent variable model for PGA that provides a probabilistic framework for factor analysis on manifolds. To compute maximum likelihood estimates of the parameters in our model  we develop a Monte Carlo Expectation Maximization algorithm  where the expectation is approximated by Hamiltonian Monte Carlo sampling of the latent variables. We demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data  as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images.,Probabilistic Principal Geodesic Analysis

Miaomiao Zhang
School of Computing
University of Utah
Salt Lake City  UT

miaomiao@sci.utah.edu

P. Thomas Fletcher
School of Computing
University of Utah
Salt Lake City  UT

fletcher@sci.utah.edu

Abstract

Principal geodesic analysis (PGA) is a generalization of principal component anal-
ysis (PCA) for dimensionality reduction of data on a Riemannian manifold. Cur-
rently PGA is deﬁned as a geometric ﬁt to the data  rather than as a probabilistic
model. Inspired by probabilistic PCA  we present a latent variable model for PGA
that provides a probabilistic framework for factor analysis on manifolds. To com-
pute maximum likelihood estimates of the parameters in our model  we develop
a Monte Carlo Expectation Maximization algorithm  where the expectation is ap-
proximated by Hamiltonian Monte Carlo sampling of the latent variables. We
demonstrate the ability of our method to recover the ground truth parameters in
simulated sphere data  as well as its effectiveness in analyzing shape variability of
a corpus callosum data set from human brain images.

1

Introduction

Principal component analysis (PCA) [12] has been widely used to analyze high-dimensional data.
Tipping and Bishop proposed probabilistic PCA (PPCA) [22]  which is a latent variable model for
PCA. A similar formulation was proposed by Roweis [18]. Their work opened up the possibility
for probabilistic interpretations for different kinds of factor analyses. For instance  Bayesian PCA
[3] extended PPCA by adding a prior on the factors  resulting in automatic selection of model di-
mensionality. Other examples of latent variable models include probabilistic canonical correlation
analysis (CCA) [1] and Gaussian process latent variable models [15]. Such latent variable models
have not  however  been extended to handle data from a Riemannian manifold.
Manifolds arise naturally as the appropriate representations for data that have smooth constraints.
For example  when analyzing directional data [16]  i.e.  vectors of unit length in Rn  the correct rep-
resentation is the sphere  Sn−1. Another important example of manifold data is in shape analysis 
where the deﬁnition of the shape of an object should not depend on its position  orientation  or scale.
Kendall [14] was the ﬁrst to formulate a mathematically precise deﬁnition of shape as equivalence
classes of all translations  rotations  and scalings of point sets. The result is a manifold represen-
tation of shape  or shape space. Linear operations violate the natural constraints of manifold data 
e.g.  a linear average of data on a sphere results in a vector that does not have unit length. As shown
recently [5]  using the kernel trick with a Gaussian kernel maps data onto a Hilbert sphere  and
utilizing Riemannian distances on this sphere rather than Euclidean distances improves clustering
and classiﬁcation performance. Other examples of manifold data include geometric transformations 
such as rotations and afﬁne transforms  symmetric positive-deﬁnite tensors [9  24]  Grassmannian
manifolds (the set of m-dimensional linear subspaces of Rn)  and Stiefel manifolds (the set of or-
thonormal m-frames in Rn) [23]. There has been some work on density estimation on Riemannian
manifolds. For example  there is a wealth of literature on parametric density estimation for direc-
tional data [16]  e.g.  spheres  projective spaces  etc. Nonparametric density estimation based on
kernel mixture models [2] was proposed for compact Riemannian manifolds. Methods for sam-
pling from manifold-valued distributions have also been proposed [4  25]. It’s important to note

1

the distinction between manifold data  where the manifold representation is known a priori  versus
manifold learning and nonlinear component analysis [15  20]  where the data lies in Euclidean space
on some unknown  lower-dimensional manifold that must be learned.
Principal geodesic analysis (PGA) [10] generalizes PCA to nonlinear manifolds. It describes the
geometric variability of manifold data by ﬁnding lower-dimensional geodesic subspaces that mini-
mize the residual sum-of-squared geodesic distances to the data. While [10] originally proposed an
approximate estimation procedure for PGA  recent contributions [19  21] have developed algorithms
for exact solutions to PGA. Related work on manifold component analysis has introduced variants of
PGA. This includes relaxing the constraint that geodesics pass through the mean of the data [11] and 
for spherical data  replacing geodesic subspaces with nested spheres of arbitrary radius [13]. All of
these methods are based on geometric  least-squares estimation procedures  i.e.  they ﬁnd subspaces
that minimize the sum-of-squared geodesic distances to the data. Much like the original formulation
of PCA  current component analysis methods on manifolds lack a probabilistic interpretation. In this
paper  we propose a latent variable model for PGA  called probabilistic PGA (PPGA). The model
deﬁnition applies to generic manifolds. However  due to the lack of an explicit formulation for the
normalizing constant  our estimation is limited to symmetric spaces  which include many common
manifolds such as Euclidean space  spheres  Kendall shape spaces  Grassman/Stiefel manifolds  and
more. Analogous to PPCA  our method recovers low-dimensional factors as maximum likelihood.

2 Riemannian Geometry Background

In this section we brieﬂy review some necessary facts about Riemannian geometry (see [6] for more
details). Recall that a Riemannian manifold is a differentiable manifold M equipped with a metric
g  which is a smoothly varying inner product on the tangent spaces of M. Given two vector ﬁelds
v  w on M  the covariant derivative ∇vw gives the change of the vector ﬁeld w in the v direction.
The covariant derivative is a generalization of the Euclidean directional derivative to the manifold
setting. Consider a curve γ : [0  1] → M and let ˙γ = dγ/dt be its velocity. Given a vector ﬁeld
dt = ∇ ˙γV . A vector ﬁeld
V (t) deﬁned along γ  we can deﬁne the covariant derivative of V to be DV
is called parallel if the covariant derivative along the curve γ is zero. A curve γ is geodesic if it
satisﬁes the equation ∇ ˙γ ˙γ = 0. In other words  geodesics are curves with zero acceleration.
Recall that for any point p ∈ M and tangent vector v ∈ TpM  the tangent space of M at p  there
is a unique geodesic curve γ  with initial conditions γ(0) = p and ˙γ(0) = v. This geodesic is only
guaranteed to exist locally. When γ is deﬁned over the interval [0  1]  the Riemannian exponential
map at p is deﬁned as Expp(v) = γ(1). In other words  the exponential map takes a position and
velocity as input and returns the point at time 1 along the geodesic with these initial conditions.
The exponential map is locally diffeomorphic onto a neighbourhood of p. Let V (p) be the largest
such neighbourhood. Then within V (p) the exponential map has an inverse  the Riemannian log
map  Logp : V (p) → TpM. For any point q ∈ V (p)  the Riemannian distance function is given by
d(p  q) = (cid:107) Logp(q)(cid:107). It will be convenient to include the point p as a parameter in the exponential
and log maps  i.e.  deﬁne Exp(p  v) = Expp(v) and Log(p  q) = Logp(q). The gradient of the
squared distance function is ∇pd(p  q)2 = −2 Log(p  q).

3 Probabilistic Principal Geodesic Analysis

Before introducing our PPGA model for manifold data  we ﬁrst review PPCA. The main idea of
PPCA is to model an n-dimensional Euclidean random variable y as

y = µ + Bx +  

(1)
where µ is the mean of y  x is a q-dimensional latent variable  with x ∼ N (0  I)  B is an n×q factor
matrix that relates x and y  and  ∼ N (0  σ2I) represents error. We will ﬁnd it convenient to model
the factors as B = W Λ  where the columns of W are mutually orthogonal  and Λ is a diagonal
matrix of scale factors. This removes the rotation ambiguity of the latent factors and makes them
analogous to the eigenvectors and eigenvalues of standard PCA (there is still of course an ambiguity
of the ordering of the factors). We now generalize this model to random variables on Riemannian
manifolds.

2

3.1 Probability Model

(2)

Following [8  17]  we use a generalization of the normal distribution for a Riemannian manifold as
our noise model. Consider a random variable y taking values on a Riemannian manifold M  deﬁned
by the probability density function (pdf)
p(y|µ  τ ) =

1

 

d(µ  y)2(cid:17)
(cid:16)− τ
d(µ  y)2(cid:17)

dy.

2

C(µ  τ )

(cid:90)

M

exp

(cid:16)− τ

2

C(µ  τ ) =

exp

We term this distribution a Riemannian normal distribution  and use the notation y ∼ NM (µ  τ−1)
to denote it. The parameter µ ∈ M acts as a location parameter on the manifold  and the parameter
τ ∈ R+ acts as a dispersion parameter  similar to the precision of a Gaussian. This distribution has
the advantages that (1) it is applicable to any Riemannian manifold  (2) it reduces to a multivariate
normal distribution (with isotropic covariance) when M = Rn  and (3) much like the Euclidean nor-
mal distribution  maximum-likelihood estimation of parameters gives rise to least-squares methods
(see [8] for details). We note that this noise model could be replaced with a different distribution 
perhaps speciﬁc to the type of manifold or application  and the inference procedure presented in the
next section could be modiﬁed accordingly.
The PPGA model for a random variable y on a smooth Riemannian manifold M is

(cid:0)Exp(µ  z)  τ−1(cid:1)   z = W Λx 

y|x ∼ NM

(3)
where x ∼ N (0  1) are again latent random variables in Rq  µ here is a base point on M  W is
a matrix with q columns of mutually orthogonal tangent vectors in TµM  Λ is a q × q diagonal
In this
matrix of scale factors for the columns of W   and τ is a scale parameter for the noise.
model  a linear combination of W Λ and the latent variables x forms a new tangent vector z ∈ TµM.
Next  the exponential map shoots the base point µ by z to generate the location parameter of a
Riemannian normal distribution  from which the data point y is drawn. Note that in Euclidean
space  the exponential map is an addition operation  Exp(µ  z) = µ + z. Thus  our model coincides
with (1)  the standard PPCA model  when M = Rn.

3.2

Inference

We develop a maximum likelihood procedure to estimate the parameters θ = (µ  W  Λ  τ ) of the
PPGA model deﬁned in (3). Given observed data yi ∈ {y1  ...  yN} on M  with associated latent
variable xi ∈ Rq  and zi = W Λxi  we formulate an expectation maximization (EM) algorithm.
Since the expectation step over the latent variables does not yield a closed-form solution  we develop
a Hamiltonian Monte Carlo (HMC) method to sample xi from the posterior p(x|y; θ)  the log of
which is given by

N(cid:89)

p(xi|yi; θ) ∝ −N log C − N(cid:88)

i=1

i=1

log

d (Exp(µ  zi)  yi)2 − (cid:107)xi(cid:107)2

2

τ
2

 

(4)

and use this in a Monte Carlo Expectation Maximization (MCEM) scheme to estimate θ. The
procedure contains two main steps:

3.2.1 E-step: HMC

For each xi  we draw a sample of size S from the posterior distribution (4) using HMC with the cur-
rent estimated parameters θk. Denote xij as the jth sample for xi  the Monte Carlo approximation
of the Q function is given by

Q(θ|θk) = Exi|yi;θk

log p(xi|yi; θk)

log p(xij|yi; θk).

(5)

(cid:34) N(cid:89)

i=1

(cid:35)

S(cid:88)

N(cid:88)

j=1

i=1

≈ 1
S

In our HMC sampling procedure  the potential energy of the Hamiltonian H(xi  m) = U (xi) +
V (m) is deﬁned as U (xi) = − log p(xi|yi; θ)  and the kinetic energy V (m) is a typical isotropic

3

dt = − ∂H

∂xi

dt = ∂H

∂m = m  and dm

Gaussian distribution on a q-dimensional auxiliary momentum variable  m. This gives us a Hamil-
= −∇xiU. Due to the fact that xi is
tonian system to integrate: dxi
a Euclidean variable  we use a standard “leap-frog” numerical integration scheme  which approxi-
mately conserves the Hamiltonian and results in high acceptance rates.
The computation of the gradient term ∇xiU (xi) requires we compute dv Exp(p  v)  i.e.  the deriva-
tive operator (Jacobian matrix) of the exponential map with respect to the initial velocity v. To derive
this  consider a variation of geodesics c(s  t) = Exp(p  su + tv)  where u ∈ TpM. The variation
c produces a “fan” of geodesics; this is illustrated for a sphere on the left side of Figure 1. Taking
the derivative of this variation results in a Jacobi ﬁeld: Jv(t) = dc/ds(0  t). Finally  this gives an
expression for the exponential map derivative as

dv Exp(p  v)u = Jv(1).

(6)

For a general manifold  computing the Jacobi ﬁeld Jv requires solving a second-order ordinary dif-
ferential equation. However  Jacobi ﬁelds can be evaluated in closed-form for the class of manifolds
known as symmetric spaces. For the sphere and Kendall shape space examples  we provide explicit
formulas for these computations in Section 4. For more details on the derivation of the Jacobi ﬁeld
equation and symmetric spaces  see for instance [6].
Now  the gradient with respect to each xi is

∇xiU = xi − τ ΛW T{dzi Exp(µ  zi)†Log(Exp(µ  zi)  yi)} 

(7)

where † represents the adjoint of a linear operator  i.e.

(cid:104)dzi Exp(µ  zi)ˆu  ˆv(cid:105) = (cid:104)ˆu  dzi Exp(µ  zi)†ˆv(cid:105).

3.2.2 M-step: Gradient Ascent

In this section  we derive the maximization step for updating the parameters θ = (µ  W  Λ  τ ) by
maximizing the HMC approximation of the Q function in (5). This turns out to be a gradient ascent
scheme for all the parameters since there are no closed-form solutions.

Gradient for τ: The gradient of the Q function with respect to τ requires evaulation of the deriva-
tive of the normalizing constant in the Riemannian normal distribution (2). When M is a symmetric
space  this constant does not depend on the mean parameter  µ  because the distribution is invariant
to isometrics (see [8] for details). Thus  the normalizing constant can be written as

(cid:90)

M

(cid:16)− τ

d(µ  y)2(cid:17)

2

dy.

C(τ ) =

exp

We can rewrite this integral in normal coordinates  which can be thought of as a polar coordinate sys-
tem in the tangent space  TµM. The radial coordinate is deﬁned as r = d(µ  y)  and the remaining
n − 1 coordinates are parametrized by a unit vector v  i.e.  a point on the unit sphere Sn−1 ⊂ TµM.
Thus we have the change-of-variables  φ(rv) = Exp(µ  rv). Now the integral for the normalizing
constant becomes

(cid:90)

(cid:90) R(v)

(cid:16)− τ

r2(cid:17)| det(dφ(rv))|dr dv 

(8)

C(τ ) =

Sn−1

0

exp

2

where R(v) is the maximum distance that φ(rv) is deﬁned. Note that this formula is only valid if
M is a complete manifold  which guarantees that normal coordinates are deﬁned everywhere except
possibly a set of measure zero on M.
The integral in (8) is difﬁcult to compute for general manifolds  due to the presence of the determi-
nant of the Jacobian of φ. However  for symmetric spaces this change-of-variables term has a simple
form. If M is a symmetric space  there exists a orthonormal basis u1  . . .   un  with u1 = v  such
that

| det(dφ(rv))| =

√

1√
κk

fk(

κkr) 

(9)

n(cid:89)

k=2

4

where κk = K(u1  uk) denotes the sectional curvature  and fk is deﬁned as

if κk > 0 
if κk < 0 
if κk = 0.

sinh(x)
x

sin(x)
(cid:16)− τ
r2(cid:17) n(cid:89)

2

k=2

fk(x) =

(cid:90) R

0

Notice that with this expression for the Jacobian determinant there is no longer a dependence on v
inside the integral in (8). Also  if M is simply connected  then R(v) = R does not depend on the
direction v  and we can write the normalizing constant as

C(τ ) = An−1

exp

−1/2
κ
k

√
fk(

κkr)dr 

where An−1 is the surface area of the n − 1 hypersphere  Sn−1. The remaining integral is one-
dimensional  and can be quickly and accurately approximated by numerical integration. While
this formula works only for simply connected symmetric spaces  other symmetric spaces could be
handled by lifting to the universal cover  which is simply connected  or by restricting the deﬁnition
of the Riemannian normal pdf in (2) to have support only up to the injectivity radius  i.e.  R =
minv R(v).
The gradient term for estimating τ is
∇τ Q =

r2(cid:17) n(cid:89)

d(Exp(µ  zij)  yi)2dr.

(cid:16)− τ

N(cid:88)

S(cid:88)

(cid:90) R

An−1

exp

−1/2
κ
k

fk(

√

κkr)dr− 1
2

r2
2

0

1

C(τ )

i=1

j=1

2

k=2

Gradient for µ: From (4) and (5)  the gradient term for updating µ is

N(cid:88)

S(cid:88)

i=1

j=1

∇µQ =

1
S

τ dµ Exp(µ  zij)†Log (Exp(µ  zij)  yi).

Here the derivative dµ Exp(µ  v) is with respect to the base point  µ. Similar to before (6) 
this derivative can be derived from a variation of geodesics: c(s  t) = Exp(Exp(µ  su)  tv(s)) 
where v(s) comes from parallel translating v along the geodesic Exp(µ  su). Again  the deriva-
tive of the exponential map is given by a Jacobi ﬁeld satisfying Jµ(t) = dc/ds(0  t)  and we have
dµ Exp(µ  v) = Jµ(1).

Gradient for Λ: For updating Λ  we take the derivative w.r.t. each ath diagonal element Λa as

τ (W axa

ij)T{dzij Exp(µ  zij)†Log(Exp(µ  zij)  yi)} 

N(cid:88)

S(cid:88)

i=1

j=1

∂Q
∂Λa =

1
S

where W a denotes the ath column of W   and xa

ij is the ath component of xij.

Gradient for W : The gradient w.r.t. W is

N(cid:88)

S(cid:88)

i=1

j=1

∇W Q =

1
S

τ dzij Exp(µ  zij)† Log(Exp(µ  zij)  yi)xT

ijΛ.

(10)

To preserve the mutual orthogonality constraint on the columns of W   we represent W as a point
on the Stiefel manifold Vq(TµM )  i.e.  the space of orthonormal q-frames in TµM. We project the
gradient in (10) onto the tangent space TW Vq(TµM )  and then update W by taking a small step
along the geodesic in the projected gradient direction. For details on the geodesic computations for
Stiefel manifolds  see [7].
The MCEM algorithm for PPGA is an iterative procedure for ﬁnding the subspace spanned by q
principal components  shown in Algorithm 1. The computation time per iteration depends on the
complexity of exponential map  log map  and Jacobi ﬁeld which may vary for different manifold.
Note the cost of the gradient ascent algorithm also linearly depends on the data size  dimensionality 
and the number of samples drawn. An advantage of MCEM is that it can run in parallel for each
data point.

5

Algorithm 1 Monte Carlo Expectation Maximization for Probabilistic Principal Geodesic Analysis

Input: Data set Y   reduced dimension q.
Initialize µ  W  Λ  σ.
repeat

Sample X according to (7) 
Update µ  W  Λ  σ by gradient ascent in Section 3.2.2.

until convergence

4 Experiments

In this section  we demonstrate the effectiveness of PPGA and our ML estimation using both simu-
lated data on the 2D sphere and a real corpus callosum data set. Before presenting the experiments
of PPGA  we brieﬂy review the necessary computations for the speciﬁc types of manifolds used 
including  the Riemannian exponential map  log map  and Jacobi ﬁelds.

4.1 Simulated Sphere Data
Sphere geometry overview: Let p be a point on an n-dimensional sphere embedded in Rn+1  and
let v be a tangent at p. The inner product between tangents at a base point p is the usual Euclidean
inner product. The exponential map is given by a 2D rotation of p by an angle given by the norm of
the tangent  i.e. 

Exp(p  v) = cos θ · p +

θ = (cid:107)v(cid:107).

sin θ

θ

· v 

(11)

The log map between two points p  q on the sphere can be computed by ﬁnding the initial velocity
of the rotation between the two points. Let πp(q) = p · (cid:104)p  q(cid:105) denote the projection of the vector q
onto p. Then 

Log(p  q) =

θ · (q − πp(q))
(cid:107)q − πp(q)(cid:107)  

θ = arccos((cid:104)p  q(cid:105)).

(12)

All sectional curvatures for Sn are equal to one. The adjoint derivatives of the exponential map are
given by

dp Exp(p  v)†w = cos((cid:107)v(cid:107))w⊥ + w(cid:62) 

dv Exp(p  v)†w =

sin((cid:107)v(cid:107))
(cid:107)v(cid:107) w⊥ + w(cid:62) 

where w⊥  w(cid:62) denote the components of w that are orthogonal and tangent to v  respectively. An
illustration of geodesics and the Jacobi ﬁelds that give rise to the exponential map derivatives is
shown in Figure 1.

Parameter estimation on the sphere: Using our generative model for PGA (3)  we forward
simulated a random sample of 100 data points on the unit sphere S2  with known parameters θ =
(µ  W  Λ  τ )  shown in Table 1. Next  we ran our maximum likelihood estimation procedure to test
whether we could recover those parameters. We initialized µ from a random uniform point on the
sphere. We initialized W as a random Gaussian matrix  to which we then applied the Gram-Schmidt
algorithm to ensure its columns were orthonormal. Figure 1 compares the ground truth principal
geodesics and MLE principal geodesic analysis using our algorithm. A good overlap between the
ﬁrst principal geodesic shows that PPGA recovers the model parameters.
One advantage that our PPGA model has over the least-squares PGA formulation is that the mean
point is estimated jointly with the principal geodesics. In the standard PGA algorithm  the mean
is estimated ﬁrst (using geodesic least-squares)  then the principal geodesics are estimated second.
This does not make a difference in the Euclidean case (principal components must pass through the
mean)  but it does in the nonlinear case. We compared our model with PGA and standard PCA (in
the Euclidean embedding space). The estimation error of principal geodesics turned to be larger in
PGA compared to our model. Furthermore  the standard PCA converges to an incorrect solution due
to its inappropriate use of a Euclidean metric on Riemannian data. A comparison of the ground truth
parameters and these methods is given in Table 1. Note that the noise precision τ is not a part of
either the PGA or PCA models.

6

Ground truth
PPGA
PGA
PCA

µ
(−0.78  0.48 −0.37)
(−0.78  0.48 −0.40)
(−0.79  0.46 −0.41)
(−0.70  0.41 −0.46)

w
(−0.59 −0.42  0.68)
(−0.59 −0.43  0.69)
(−0.59 −0.38  0.70)
(−0.62 −0.37  0.69)

τ
Λ
100
0.40
0.41
102
0.41 N/A
0.38 N/A

Table 1: Comparison between ground truth parameters for the simulated data and the MLE of PPGA 
non-probabilistic PGA  and standard PCA.

Figure 1: Left: Jacobi ﬁelds; Right: the principal geodesic of random generated data on unit sphere.
Blue dots: random generated sphere data set. Yellow line: ground truth principal geodesic. Red
line: estimated principal geodesic using PPGA.

4.2 Shape Analysis of the Corpus Callosum

Shape space geometry: A conﬁguration of k points in the 2D plane is considered as a complex
k-vector  z ∈ Ck. Removing translation  by requiring the centroid to be zero  projects this point to

the linear complex subspace V = {z ∈ Ck : (cid:80) zi = 0}  which is equivalent to the space Ck−1.

Next  points in this subspace are deemed equivalent if they are a rotation and scaling of each other 
which can be represented as multiplication by a complex number  ρeiθ  where ρ is the scaling factor
and θ is the rotation angle. The set of such equivalence classes forms the complex projective space 
CP k−2.
We think of a centered shape p ∈ V as representing the complex line Lp = {z · p : z ∈ C\{0}} 
i.e.  Lp consists of all point conﬁgurations with the same shape as p. A tangent vector at Lp ∈ V is
a complex vector  v ∈ V   such that (cid:104)p  v(cid:105) = 0. The exponential map is given by rotating (within V )
the complex line Lp by the initial velocity v  that is 

(cid:107)p(cid:107) sin θ

θ

Exp(p  v) = cos θ · p +

(13)
Likewise  the log map between two shapes p  q ∈ V is given by ﬁnding the initial velocity of the
rotation between the two complex lines Lp and Lq. Let πp(q) = p·(cid:104)p  q(cid:105)/(cid:107)p(cid:107)2 denote the projection
of the vector q onto p. Then the log map is given by
θ · (q − πp(q))
(cid:107)q − πp(q)(cid:107)  

|(cid:104)p  q(cid:105)|
(cid:107)p(cid:107)(cid:107)q(cid:107) .

Log(p  q) =

θ = arccos

(14)

· v 

θ = (cid:107)v(cid:107).

The sectional curvatures of CP k−2  κi = K(ui  v)  used in (9)  can be computed as follows. Let
√−1. The remaining u2  . . .   un can be
u1 = i · v  where we treat v as a complex vector and i =
chosen arbitrarily to be construct an orthonormal frame with v and u1 Then we have K(u1  v) = 4
and K(ui  v) = 1 for i > 1. The adjoint derivatives of the exponential map are given by

dp Exp(p  v)†w = cos((cid:107)v(cid:107))w⊥
sin((cid:107)v(cid:107))
(cid:107)v(cid:107) w⊥
dv Exp(p  v)†w =
1 +

sin(2(cid:107)v(cid:107))
2(cid:107)v(cid:107) + w(cid:62)
2  
1 = (cid:104)w  u1(cid:105)u1  u(cid:62)
where w⊥
ing orthogonal component of w  and w(cid:62) denotes the component tangent to v.

1 denotes the component of w parallel to u1  i.e.  w⊥

1 + cos(2(cid:107)v(cid:107))w⊥

2 + w(cid:62) 

2 denotes the remain-

7

pvJ(x)MShape variability of corpus callosum data: As a demonstration of PPGA on Kendall shape
space  we applied it to corpus callosum shape data derived from the OASIS database (www.
oasis-brains.org). The data consisted of magnetic resonance images (MRI) from 32 healthy
adult subjects. The corpus callosum was segmented in a midsagittal slice using the ITK SNAP
program (www.itksnap.org). An example of a segmented corpus callosum in an MRI is
shown in Figure 2. The boundaries of these segmentations were sampled with 64 points us-
ing ShapeWorks (www.sci.utah.edu/software.html). This algorithm generates a sam-
pling of a set of shape boundaries while enforcing correspondences between different point mod-
els within the population. Figure 2 displays the ﬁrst two modes of corpus callosum shape varia-
tion  generated from the as points along the estimated principal geodesics: Exp(µ  αiwi)  where
αi = −3λi −1.5λi  0  1.5λi  3λi  for i = 1  2.

Figure 2: Left: example corpus callosum segmentation from an MRI slice. Middle to right: ﬁrst and
second PGA mode of shape variation with −3  −1.5  1.5  and 3 × λ.

5 Conclusion

We presented a latent variable model of PGA on Riemannian manifolds. We developed a Monte
Carlo Expectation Maximization for maximum likelihood estimation of parameters that uses Hamil-
tonian Monte Carlo to integrate over the posterior distribution of latent variables. This work takes the
ﬁrst step to bring latent variable models to Riemannian manifolds. This opens up several possibili-
ties for new factor analyses on Riemannian manifolds  including a rigorous formulation for mixture
models of PGA and automatic dimensionality selection with a Bayesian formulation of PGA.

Acknowledgments This work was supported in part by NSF CAREER Grant 1054057.

References
[1] F. R. Bach and M. I. Jordan. A probabilistic interpretation of canonical correlation analysis.

Technical Report 608  Department of Statistics  University of California  Berkeley  2005.

[2] A. Bhattacharya and D. B. Dunson. Nonparametric bayesian density estimation on manifolds

with applications to planar shapes. Biometrika  97(4):851–865  2010.

[3] C. M. Bishop. Bayesian PCA. Advances in neural information processing systems  pages

382–388  1999.

[4] S. Byrne and M. Girolami. Geodesic Monte Carlo on embedded manifolds. arXiv preprint

arXiv:1301.6064  2013.

[5] N. Courty  T. Burger  and P. F. Marteau. Geodesic analysis on the Gaussian RKHS hypersphere.

In Machine Learning and Knowledge Discovery in Databases  pages 299–313  2012.

[6] M. do Carmo. Riemannian Geometry. Birkh¨auser  1992.
[7] A. Edelman  T. A Arias  and S. T Smith. The geometry of algorithms with orthogonality

constraints. SIAM journal on Matrix Analysis and Applications  20(2):303–353  1998.

[8] P. T. Fletcher. Geodesic regression and the theory of least squares on Riemannian manifolds.

International Journal of Computer Vision  pages 1–15  2012.

[9] P. T. Fletcher and S. Joshi. Principal geodesic analysis on symmetric spaces: statistics of
diffusion tensors. In Workshop on Computer Vision Approaches to Medical Image Analysis
(CVAMIA)  2004.

8

-3l1-1.5l101.5l13l1-3l2-1.5l201.5l23l2[10] P. T. Fletcher  C. Lu  and S. Joshi. Statistics of shape via principal geodesic analysis on Lie

groups. In Computer Vision and Pattern Recognition  pages 95–101  2003.

[11] S. Huckemann and H. Ziezold. Principal component analysis for Riemannian manifolds  with
an application to triangular shape spaces. Advances in Applied Probability  38(2):299–319 
2006.

[12] I. T. Jolliffe. Principal Component Analysis  volume 487. Springer-Verlag New York  1986.
[13] S. Jung  I. L. Dryden  and J. S. Marron. Analysis of principal nested spheres. Biometrika 

99(3):551–568  2012.

[14] D. G. Kendall. Shape manifolds  Procrustean metrics  and complex projective spaces. Bulletin

of the London Mathematical Society  16:18–121  1984.

[15] N. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional

data. Advances in neural information processing systems  16:329–336  2004.

[16] K. V. Mardia. Directional Statistics. John Wiley and Sons  1999.
[17] X. Pennec. Intrinsic statistics on Riemannian manifolds: basic tools for geometric measure-

ments. Journal of Mathematical Imaging and Vision  25(1)  2006.

[18] S. Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing

systems  pages 626–632  1998.

[19] S. Said  N. Courty  N. Le Bihan  and S. J. Sangwine. Exact principal geodesic analysis for
In Proceedings of the 15th European Signal Processing Conference  pages

data on SO(3).
1700–1705  2007.

[20] B. Sch¨olkopf  A. Smola  and K. R. M¨uller. Nonlinear component analysis as a kernel eigen-

value problem. Neural Computation  10(5):1299–1319  1998.

[21] S. Sommer  F. Lauze  S. Hauberg  and M. Nielsen. Manifold valued statistics  exact principal
In Proceedings of the European

geodesic analysis and the effect of linear approximations.
Conference on Computer Vision  pages 43–56  2010.

[22] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  61(3):611–622  1999.

[23] P. Turaga  A. Veeraraghavan  A. Srivastava  and R. Chellappa. Statistical computations on
Grassmann and Stiefel manifolds for image and video-based recognition. IEEE Trans. Pattern
Analysis and Machine Intelligence  33(11):2273–2286  2011.

[24] O. Tuzel  F. Porikli  and P. Meer. Pedestrian detection via classiﬁcation on Riemannian mani-

folds. IEEE Trans. Pattern Analysis and Machine Intelligence  30(10):1713–1727  2008.

[25] M. Zhang  N. Singh  and P. T. Fletcher. Bayesian estimation of regularization and atlas building
in diffeomorphic image registration. In Information Processing in Medical Imaging  pages 37–
48. Springer  2013.

9

,Miaomiao Zhang
Tom Fletcher