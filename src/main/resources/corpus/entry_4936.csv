2014,Self-Adaptable Templates for Feature Coding,Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy  features are extracted and encoded  followed by a pooling step. Within this processing pipeline  the common trend is to learn the feature coding templates  often referred as codebook entries  filters  or over-complete basis. Recently  an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P). In this paper  we analyze O2P as a coding-pooling scheme. We find that at testing phase  O2P automatically adapts the feature coding templates to the input features  rather than using templates learned during the training phase. From this finding  we are able to bring common concepts of coding-pooling schemes to O2P  such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification  namely Caltech101 and VOC07.,Self-Adaptable Templates for Feature Coding

Xavier Boix1 2∗ Gemma Roig1 2∗

Salomon Diether1

Luc Van Gool1

1Computer Vision Laboratory  ETH Zurich  Switzerland

2LCSL  Massachusetts Institute of Technology & Istituto Italiano di Tecnologia  Cambridge  MA

{boxavier gemmar sdiether vangool}@vision.ee.ethz.ch

{xboix gemmar}@mit.edu

Abstract

Hierarchical feed-forward networks have been successfully applied in object
recognition. At each level of the hierarchy  features are extracted and encoded 
followed by a pooling step. Within this processing pipeline  the common trend is
to learn the feature coding templates  often referred as codebook entries  ﬁlters  or
over-complete basis. Recently  an approach that apparently does not use templates
has been shown to obtain very promising results. This is the second-order pooling
(O2P) [1]. In this paper  we analyze O2P as a coding-pooling scheme. We ﬁnd
that at testing phase  O2P automatically adapts the feature coding templates to the
input features  rather than using templates learned during the training phase. From
this ﬁnding  we are able to bring common concepts of coding-pooling schemes to
O2P  such as feature quantization. This allows for signiﬁcant accuracy improve-
ments of O2P in standard benchmarks of image classiﬁcation  namely Caltech101
and VOC07.

1

Introduction

Many object recognition schemes  inspired from biological vision  are based on feed-forward hier-
archical architectures  e.g. [2  3  4]. In each level in the hierarchy  the algorithms can be usually
divided into the steps of feature coding and spatial pooling. The feature coding extracts similarities
between the set of input features and a set of templates (the so called ﬁlters  over-complete basis or
codebook)  and then  the similarity responses are transformed using some non-linearities. Finally 
the spatial pooling extracts one single vector from the set of transformed responses. The speciﬁc ar-
chitecture of the network (e.g. how many layers)  and the speciﬁc algorithms for the coding-pooling
at each layer are usually set for a recognition task and dataset  cf. [5].
Second-order Pooling (O2P) is an alternative algorithm to the aforementioned coding-pooling
scheme. O2P is based on tensor representations that were introduced in medical imaging to analyze
magnetic resonance images [6  7]. Lately  tensor representations achieved state-of-the-art in some
computer vision tasks [8  9]  and remarkable results for semantic segmentation  in which tensor rep-
resentations were adapted and named as O2P [1  10]. A surprising fact of O2P is that it is formulated
without feature coding templates [1]. This is in contrast to the common coding-pooling schemes  in
which the templates are learned during a training phase  and at testing phase  the templates remain
ﬁxed to the learned values.
Motivated by the intriguing properties of O2P  in this paper we try to re-formulate O2P as a coding-
pooling scheme. In doing so  we ﬁnd that O2P actually computes similarities to feature coding
templates as the rest of the coding-pooling schemes. Yet  what remains uncommon of O2P  is that
the templates are “recomputed” for each speciﬁc input  rather than being ﬁxed to learned values. In
O2P  the templates are self-adapted to the input  and hence  they do not require learning.

∗Both ﬁrst authors contributed equally.

1

From our formulation  we are able to bring common concepts of coding-pooling schemes to O2P 
such as feature quantization. This allows us to achieve signiﬁcant improvements of the accuracy
of O2P for image classiﬁcation. We report experiments on two challenging benchmarks for image
classiﬁcation  namely Caltech101 [11]  and VOC07 [12].

2 Preliminaries

In this Section  we revisit several coding-pooling schemes as well as O2P  and identify some com-
mon terminology in the literature. This will serve as a basis for the new formulation of O2P  that we
introduce in the following section.
The algorithms that we analyze in this section are usually part of a layer of a hierarchical network
for object recognition. The input to these algorithms is a set of feature vectors that come from the
output of the previous layer  or from the raw image. Let {xi}N be the set of input feature vectors
to the algorithm  which is the set of N feature vectors  xi ∈ RM   indexed by i ∈ {1  . . .   N}.
The output of the algorithm is a single vector  which we denote as y  and it may have a different
dimensionality than the input vectors.
In Subsection 2.1  we present the algorithms and terminology of previous template-based meth-
ods. Then  in Subsection 2.2  we review the formulation of O2P that appears in the literature  that
apparently does not use templates.

2.1 Coding-Pooling based on Evaluating Similarities to Templates

Template-based methods are build upon similarities between the input vectors and a set of templates.
Depending on the terminology of each algorithm  the templates may be denoted as ﬁlters  codebook 
or over-complete basis. From now on  we will refer to all of them as templates. We denote the set
of templates as {bk ∈ RM}P . In this paper  bk and the input feature vectors xi have the same
dimensionality  M. The set of templates is ﬁxed to learned values during the training phase. There
are many possible learning algorithms  but analyzing them is not necessary here.
The algorithms that are interesting for our purposes  start by computing a similarity measure between
the input feature vectors {xi}N and the templates {bk}P . Let Γ(xi  bk) be the similarity function 
which depends on each algorithm. We deﬁne γi as the vector that contains the similarities of xi to
the set of templates {bk}  and γ ∈ RM×P the matrix whose columns are the vectors γi  i.e.

γki = Γ(xi  bk).

(1)

Once γ is computed  the algorithms that we analyze apply some non-linear transformation to γ  and
then  the resulting responses are merged together  with the so called pooling operation. The pooling
consists on generating one single response value for each template. We denote as gk(γ) the function
that includes both the non-linear transformation and the pooling operation  where gk : RM×P → R.
We include both operations in the same function  but in the literature it is usually presented as two
separate steps. Finally  the output vector y is built using {gk(γ)}P   {bk}P and {xi}N   depending
on the algorithm.
It is also quite common to concatenate the outputs of neighboring regions to
generate the ﬁnal output of the layer.
We now show how the presented terminology is applied to some methods based on evaluating sim-
ilarities to templates  namely assignment-based methods and Fisher Vector.
In the sequel  these
algorithms will be a basis to reformulate O2P.

Assignment-based Methods The popular Bag-of-Words and some of its variants fall into this
category  e.g. [13  14  15]. These methods consist on assigning each input vector xi to a set of
templates (the so called vector quantization)  and then  building a histogram of the assignments 
which corresponds to the average pooling operation.
We now present them using our terminology. After computing the similarities to the templates  γ
(usually based on (cid:96)2 distance)  gk(γ) computes both the vector quantization and the pooling. Let
s be the number of templates to which each input vector is assigned  and let γ(cid:48)
i be the resulting
assignment vector of xi (i.e. γ(cid:48)
i has s entries
set to 1 and the rest to 0  that indicate the assignment. Finally  gk(γ) also computes the pooling for

i is the result of applying vector quantisation on xi). γ(cid:48)

2

the assignments corresponding to the template k  i.e. gk(γ) = 1
N
is the concatenation of the resulting pooling of the different templates  y = (g1(γ)  . . .   gP (γ)).

ki. The ﬁnal output vector

(cid:80)
i<N γ(cid:48)

(cid:88)

Fisher Vectors

It uses the ﬁrst and second order statistics of the similarities between the features

and the templates [16]. Fisher Vector builds two vectors for each template bk  which are

γki (bk − xi) Φ(2)

k =

Φ(1)

k =

1
Ak

where γki =

(cid:20)

i<N
1
Zk

exp

− 1
2

(xi − bk)tDk(xi − bk)

.

(cid:0)(bk − xi)2 − Ck

(cid:1)  

(cid:88)

i<N

1
Bk

γki

(cid:21)

(2)

(3)

Ak  Bk  Ck are learned constants  Zk a normalization factor and Dk is a learned constant matrix of
the model. Note that in Eq. (3)  γki is a similarity between the feature vector xi and the template bk.
The ﬁnal output vector is y = (Φ(1)
P ). For further details we refer the reader
to [16].
We use our terminology to do a very simple re-write of the terms. We deﬁne gk(γ) and bF
the super-index F to indicate that are from Fisher vectors  and different from bk) as

k (we use

P   Φ(2)

1   Φ(2)

. . .   Φ(1)

1

gk(γ) = (cid:107)(Φ(1)

k   Φ(2)

k )(cid:107)2  bF

k =

1

gk(γ)

(Φ(1)

k   Φ(2)
k ).

(4)

We can see the templates of Fisher vectors  bF
k   are obtained from computing some transformations
to the original learned template bk  which involve the input set of features {xi}. gk(γ) is the norm
k )  which gives an idea of the importance of each template in {xi}  similarly to gk(γ)
of (Φ(1)
in assignment-based methods. Note that bF
k and gk(γ) are related to only one ﬁxed template  bk.
The ﬁnal output vector becomes y = (g1(γ)bF

k   Φ(2)

1   . . .   gP (γ)bF

P ).

2.2 Second-Order Pooling

(cid:88)

1
N

O2P starts by building a correlation matrix from the set of feature (column) vectors {xi ∈ RM}N  
i.e.

i<N

K =

xixt
i 

(5)
i is the transpose vector of xi  and K ∈ RM×M is a square matrix. K is a symmetric positive
where xt
deﬁnite (SPD) matrix  and contains second-order statistics of {xi}. The set of SPD matrices form
a Riemannian manifold  and hence  the conventional operations in the Euclidean space can not be
used. Several metrics have been proposed for SPD matrices  and the most celebrated is the Log-
Euclidean metric [17]. Such metric consists of mapping the SPD matrices to the tangent space by
using the logarithm of the matrix  log(K). In the tangent space  the standard Euclidean metrics can
be used.
The logarithm of an SPD matrix can be computed in practice by applying the logarithm individually
to each of the eigenvalues of K [18]. Thus  the ﬁnal output vector for O2P can be written as

(cid:32)(cid:88)

(cid:33)

log(λk)eket
k

y = vec (log(K)) = vec

(6)
where ek are the eigenvectors of K  and λk the corresponding eigenvalues. The vec(·) operator
vectorizes log(K).
In Eq. (6)  apparently  there are no similarities to a set of templates. The absence of templates
makes O2P look quite different from template-based methods. Recently  O2P achieved state-of-the-
art results in semantic segmentation [1  10]. Both reasons  motivates us to further analyze O2P in
relation to template-based methods.

k<M

 

3

3 Self-Adaptability of the Templates

In this section  we introduce a formulation that relates O2P and template-based methods. The new
formulation is based on comparing two ﬁnal representation vectors  rather than deﬁning how the
ﬁnal vector y is built. We denote (cid:104)yr  ys(cid:105) as the inner product between yr and ys  which are the
ﬁnal representation vectors from two sets of input feature vectors  {xr
i}N   respectively 
where we use the superscripts r and s to indicate the respective representation for each set. It will
become clear during this section why we analyze (cid:104)yr  ys(cid:105) instead of y.
We divide the analysis in three subsections. In subsection 3.1  we re-write the formulation of the
template-based methods of Section 2 with the inner product (cid:104)yr  ys(cid:105). In subsection 3.2  we do the
same for O2P  and this unveils that O2P is also based on evaluating similarities to templates. In
subsection 3.3  we analyze the characteristics of the templates in O2P  which have the particularity
that are self-adapted to the input.

i}N and {xs

3.1 Re-Formulation of Template-Based Methods

(cid:88)

(cid:88)

(cid:104)yr  ys(cid:105) =

We re-write a generic formulation for the template-based methods described in Section 2 with the
inner product between two ﬁnal output vectors. The algorithms of Section 2 can be expressed as

gk(γr)gq(γs)S(br

k  bs

q) 

(7)

k<P

q<P

where γki = Γ(xi  bk) 

and S(u  v) is a similarity function between the templates that depends on each algorithm. Recall
that gk(γ) is a function that includes the non-linearities and the pooling of the similarities between
the input feature vectors and the the templates. To see how Eq. (7) arises naturally from the algo-
rithms of Section 2  we now analyze them in terms of this formulation.

Assignment-Based Methods The inner product between two ﬁnal output vectors can be written

as

(cid:104)yr  ys(cid:105) =(g1(γr)  . . .   gP (γr))t(gs

1(γs)  . . .   gs

P (γs)) =

(cid:88)

k<P

(cid:88)

(cid:88)

k<P

q<P

=

gk(γr)gk(γs) =

gk(γr)gq(γs)I(br

k = bs

q) 

(8)

where the last step introduces an outer summation  and the indicator function I(·) eliminates the
unnecessary cross terms. Comparing this last equation to Eq. (7)  we can identify that S(br
q) is
the indicator function (returns 1 when br

q  and 0 otherwise).

k  bs

k = bs

Fisher Vectors The inner product between two ﬁnal Fisher Vectors is

(cid:104)yr  ys(cid:105) =(g1(γr)brF

(cid:88)

(cid:88)

=

1   . . .   gP (γr)brF
gk(γr)gq(γs)I(br

P )t(g1(γs)bsF
k = bs

q)(cid:104)brF

q (cid:105).
k   bsF

1   . . .   gP (γs)bsF
P )

(9)

k<P

q<P

The indicator function appears for the same reason as in Assignment-Based Methods. The ﬁnal
templates for each set of input vectors  brF
k   respectively  are compared with each other with
the similarity (brF
k   bsF

k   bsF
q ) in Eq. (7) is equal to I(br

q . Thus  S(brF

q .
k )tbsF

k )tbsF

k = bs

q)(brF

3.2 O2P as Coding-Pooling based on Pattern Similarities

We now re-formulate O2P  in the same way as we did for template-based methods in the previous
subsection. This will allow relating O2P to template-based methods  and show that O2P also uses
similarities to templates.
We re-write the deﬁnition of O2P in Eq. (6) with (cid:104)yr  ys(cid:105). Using the property vec(A)tvec(B) =
tr(AtB)  where tr(·) is the trace function of a matrix  (cid:104)yr  ys(cid:105) becomes (in the supplementary

4

Method

Assignment-based

Fisher Vectors

O2P

S(br
I(br
k = bs
(cid:104)br

k  bs
q)
k = bs
q)
q)(cid:104)bsF
P (cid:105)
k   bsF
q(cid:105)2
k  bs

I(br

γki = Γ(xi  bk) templates

(cid:104)xi  bk(cid:105)
Eq. (3)
(cid:104)xi  bk(cid:105)2

ﬁxed

ﬁxed/adapted
self-adapted

(cid:80)
gk(γ)
i γ(cid:48)
log(cid:0) 1
(cid:1)
(cid:80)
k )(cid:107)2
k   Φ(2)
i γki

(cid:107)(Φ(1)

1
N

ki

N

Table 1: Summary Table of the elements of our formulation for Assignment-based methods  Fisher
Vectors and O2P.

(cid:88)

i

1
N

(cid:32)

(cid:88)

i<N

1
N

(cid:33)

γki

 

(11)

(12)

(13)

material we do the full derivation)

(cid:88)

(cid:88)

k<M

q<M

=

k  es

log(λr

q)(cid:104)er

k) log(λs

k is a square matrix  and the eigenvectors  {er

(cid:104)yr  ys(cid:105) = (cid:104)vec (log(Kr))   vec (log(Ks))(cid:105) =
q(cid:105)2 
(10)
k  es
k}M   are compared all against
k}M and {es
where eket
q(cid:105)2. Going back to the generic formulation of template-based methods in
each other with (cid:104)er
q)  can be identiﬁed in
Eq. (7)  we can see that the similarity function between the templates  S(er
O2P as (cid:104)er
q(cid:105)2. Also  note that in O2P the sums go over M  which is the number of eigenvectors 
and in Eq. (7)  go over P   which is the number of templates. Finally  gk(γ) in Eq. (7) corresponds
to log(λk) in O2P.
At this point  we have expressed O2P in a similar way as template-based methods. Yet  we still have
to ﬁnd the similarity between the input feature vectors and the templates. For that purpose  we use
the deﬁnition of eigenvalues and eigenvectors  i.e. λkek = Kek  and also that tr(eket
k) = 1 (the
eigenvectors are orthonormal). Then  we can derive the following equivalence: λk = λktr(eket
k) =
i  we ﬁnd that the eigenvalues  λk  can be written using the
tr(Keket
similarity between the input vectors  xi  and the eigenvectors  ek:

k). Replacing K by 1

i xixt

k  es

k  es

N

λk =

1
N

tr((xixt

i)(eket

k)) =

(cid:104)xi  ek(cid:105)2.

Finally  we can integrate all the above derivations in Eq. (10)  and we obtain that

(cid:104)yr  ys(cid:105) =

gk(γr)gq(γs)(cid:104)er

q(cid:105)2 

k  es

(cid:88)

(cid:88)

(cid:80)
(cid:88)

i

k<M

q<M

where gk(γ) = log(λk) = log

and γki = Γ(xi  ek) = (cid:104)xi  ek(cid:105)2.

(14)
We can see by analyzing Eq. (12) that this equation takes the same form as the general equation
of template-based methods in Eq. (7). Note that the eigenvectors take the same role as the set of
templates  i.e. bk = ek and P = M. Also  observe that S(br
q) is the square of the inner product
between eigenvectors  Γ(xi  bk) is the square of the inner product between the input vectors and the
eigenvectors  and the pooling operation is the logarithm of the average of the similarities. In Table 1
we summarize the corresponding elements of all the described methods.

k  bs

3.3 Self-Adaptative Templates

We deﬁne self-adaptative templates as templates that only depend on the input set of feature vec-
tors  and are not ﬁxed to predeﬁned values. This is the case in O2P  because the templates in O2P
correspond to the eigenvectors computed from the set of input feature vectors. The templates in
O2P are not ﬁxed to values learned during the training phase. Interestingly  the ﬁnal templates in
Fisher Vectors  bF
k are obtained by
modifying the ﬁxed learned templates  bk  with the input feature vectors.
Finally  note that in O2P the number of templates is equal to the dimensionality of the input feature
vectors. Thus  in O2P the number of templates can not be increased without changing the input
vectors’ length  M. This begs the following question: do M templates allow for sufﬁcient gener-

k   are also partially self-adapted to the input vectors. Note that bF

5

Algorithm 1: Sparse Quantization in O2P
Input: {xi}N   k
Output: y
foreach i = {1  . . .   N} do
end
K = 1
i ˆxi ˆxt
i
N
y = vec(log(K))

(cid:80)

ˆxi ← Set k highest values of xi to its vector entry: xi  and the rest to 0

alization for object recognition for any set of input vectors? We analyze this question in the next
section.

4 Application: Quantization for O2P

We observe in the experiments section that the performance of O2P degrades when the number of
vectors in the set of input features increases. It is reasonable that M templates are not sufﬁcient
when the number of different vectors in {xi}N increases  specially when they are very different
from each other. We now introduce an algorithm to increase the robustness of O2P to the variability
of the input vectors.
We quantize the input feature vectors  {xi}  before computing O2P. Quantization may discard de-
tails  and hence  reduce the variability among vectors.
In the experiments section it is reported
that this allows preventing the degradation of performance in object recognition  when the number
of input feature vectors increases. The quantization algorithm that we use is sparse quantization
(SQ) [15  19]  because SQ does not change the dimensionality of the feature vector. Also  SQ is fast
to compute  and does not increase the computational cost of O2P.

(cid:1). The

k

k  Bq

k = {0  1}q

k| is equal to(cid:0)q

Sparse Quantization for O2P For the quantization of {xi} we use SQ  which is a quantization
k be the set of k-sparse vectors  i.e. {s ∈ Rq : (cid:107)s(cid:107)0 ≤ k}.
to the set of k-sparse vectors. Let Rq
k = {s ∈ {0  1}q : (cid:107)s(cid:107)0 = k}  which is the set of binary vectors
Also  we deﬁne Bq
with k elements set to one and (q − k) set to zero. The cardinality of |Bq
quantization of a vector v ∈ Rq into a codebook {ci} is a mapping of v to the closest element in
{ci}  i.e. ˆv(cid:63) = arg minˆv∈{ci} (cid:107)ˆv − v(cid:107)2  where ˆv(cid:63) is the quantized vector v. In the case of SQ  the
codebook {ci} contains the set of k-sparse vectors. These may be any of the previously introduced
types: Rq
k. An important advantage of SQ over a general quantization is that it can be computed
much more efﬁciently. The naive way to compute a general quantization is to evaluate the nearest
neighbor of v in {ci}  which may be costly to compute for large codebooks and high-dimensional
v. In contrast  SQ can be computed by selecting the k higher values of the set {vi}  i.e. for SQ into
Rq
k  ˆvi = vi if i is one of the k-highest entries of vector v  and 0 otherwise. For SQ into Bq
k  the
dimension indexed by the k-highest are set to 1 instead of vi  and 0 otherwise. (We refer the reader
to [15  19] for a more detailed explanation on SQ).
In Algorithm 1 we depict the implementation of SQ in O2P  which highlights its simplicity. The
computational cost of SQ is negligible compared to the cost of computing O2P. We use the set of
k-sparse vectors in RM

k for SQ  which worked best in practice  as shown in the following.

5 Experiments

In this section  we analyze O2P in image classiﬁcation from dense sampled SIFT descriptors. This
setup is common in image classiﬁcation  and it allows direct comparison to previous works on O2P.
We report results on the Caltech101 [11] and VOC07 [12] datasets  using the standard evaluation
benchmarks  which are the mean average precision accuracy across all classes.

6

5.1

Implementation Details

We use the standard pipeline for image classiﬁcation. We never use ﬂipped or blurred images to
extend the training set.

Pipeline. For Caltech101  the image is re-sized to take a maximum height and width of 300
pixels  which is the standard resizing protocol for this dataset. For VOC07 the size of the images
remains the same as the original. We extract SIFT [4] from patches on a regular grid  at different
scales. In Caltech 101  we extract them at every 8 pixels and at the scales of 16  32 and 48 pixels
diameter. In VOC07  SIFT is sampled at each 4 pixels and at the scales of 12  24 and 36 pixels
diameter. O2P is computed using the SIFT descriptors as input  and using spatial pyramids. In
Caltech101  we generate the pooling regions dividing the image in 4 × 4  2 × 2 and 1 × 1 regions 
and in VOC07 in 3 × 1  2 × 2 and 1 × 1 regions. To generate the ﬁnal descriptor for the whole
image  we concatenate the descriptors for each pooled region. We apply the power normalization to
the ﬁnal feature dimensions  sign(x)|x|3/4  that was shown to work well in practice [1]. Finally  we
use a linear one-versus-rest SVM classiﬁer for each class with the parameter C of the SVM set to
1000. We use the LIBLINEAR library for the SVM[20].

Other Feature Codings. As a sanity check of our results  we replace O2P with the Bag-of-
Words [13] baseline  without changing any of the parameters. In Caltech101  we replace the average
pooling of Bag-of-Words by max-pooling (without normalization) as it performs better. The code-
book is learned by randomly picking a set of patches as codebook entries  which was shown to work
well for the encodings we are evaluating [14]. We use a codebook of 8192 entries  since with more
entries the performance does not increase signiﬁcantly  but the computational cost does.

5.2 Results on Caltech101

We use 3 random splits of 30 images per class for training and the rest for testing. In Fig. 1a  results
are shown for different spatial pyramid conﬁgurations  as well as different levels of quantization.
Note that SQ with k = 128 is not introducing any quantization  as SIFT features are 128 dimensional
vectors. Note that using SQ increases the performance more than 5% compared to when not using
SQ (k = 128)  when using only the ﬁrst level of the pyramid. For the other levels of the pyramid 
there is less improvement with SQ. This is in accordance with the observation that in smaller regions
there are less SIFT vectors  the variability is smaller  and the limited amount of templates is able to
better capture the meaningful information than in bigger regions. We can also see that for small k
of SQ  the performance degrades due to the introduction of too much quantization.
We also run experiments with Bag-of-Words with max-pooling (74.8%)  and O2P without SQ
(76.52%)  and both of them are surpassed by O2P with SQ (78.63%).
In [1]  O2P accuracy is
reported to be 79.2% with SIFT descriptor (we do not compare to their version of enriched SIFT 
since all our experiments are with normal SIFT). We inspected the code of [1]  and we found that
the difference of accuracy mainly comes from using a more drastic resizing of the image  that takes
a maximum of 100 pixels of width and height (usually in the literature it is 300 pixels). Note that re-
sizing is another way of discarding information  and hence  O2P may beneﬁt from that. We conﬁrm
this by resizing the image back to 300 pixels in [1]’s code  and the accuracy is 77.1%  similar to the
one that we report without SQ in our code. The accuracy is not exactly the same due to differences
in the SIFT parameters in [1]. Also  we tested SQ in [1]’s code with the resizing to a maximum of
100 pixels  and the accuracy increased to 79.45%  which is higher than reported in [1]  and close to
state-of-the-art results using SIFT descriptors (80.3%) [21].

5.3 Results on VOC07

In Fig. 1b  we run the same experiment as in Caltech101. Note that the impact of SQ is even more
evident than in Caltech101. In Table 2 we report the per-class accuracy  in addition to the mean
average precision reported in Fig. 1b. We follow the evaluation procedure as described in [12].
With the full pyramid  when we use SQ the accuracy increases from 18.81% to 50.97%. In con-
trast to Caltech101  O2P with SQ performance is similar to our implementation of Bag-of-Words
(51.14%). Thus  under adverse conditions for O2P  i.e. images with high variability such as in

7

(a)

(b)

Figure 1: Results for different numbers of non-zero entries of SQ. Note that SQ at k = 128 is not
introducing any quantization  since SIFT features are 128 dimensional vectors.
(a) Caltech 101
(using 30 images per class for training)  (b) VOC07.

e
l
c
y
c
i
B

e
n
a
l
p
o
r
e
A

r
o
t
i
n
o
M
V
T
3 Pyr. O2P + SQ 72 53 45 63 23 51 69 52 50 35 44 41 74 56 78 19 35 50 67 45
3 Pyr. O2P w/o SQ 34 9 12 18 6 19 40 14 26 14 9 21 28 17 55 7 7 10 16 12
2 Pyr. O2P + SQ 71 50 41 62 20 50 68 47 47 33 41 37 69 56 74 18 36 51 66 44
1 Pyr. O2P + SQ 66 41 32 58 15 37 58 38 40 27 28 30 61 43 66 20 33 37 56 36
1 Pyr. O2P w/o SQ 21 7 11 9 6 8 29 10 22 4 7 12 12 8 49 6 5 7 9 9

e
k
i
b
r
o
t
o
M

t
n
a
l

P
d
e
t
t
o
P

r
a
C

t
a
C

d
r
i

B

t
a
o
B

a
f
o
S

n
i
a
r
T

r
i
a
h
C

w
o
C

e
s
r
o
H

g
o
D

e
l
t
t
o
B

s
u
B

e
g
a
r
e
v
A
50.97
18.81
49.09
41.20
12.53

e
l
b
a
T
g
n
i
n
n
i
D

n
o
s
r
e
P

p
e
e
h
S

/

Table 2: PASCAL VOC 2007 classiﬁcation results. The average score provides the per-class aver-
age. We report results for O2P  with and without SQ  with the ﬁrst plus second plus third levels of
pyramids (3 Pyr.)  O2P with SQ with the ﬁrst plus second levels of pyramids (2 Pyr.)  and O2P with
and without SQ only with the ﬁrst level of pyramids (1 Pyr.).

VOC07 and with a high number of input vectors  we can use SQ and obtain huge improvements of
the O2P’s accuracy. The best reported results [22] in VOC07 are around 10% better than O2P with
SQ  yet we obtain more than 30% improvement from the baseline.

6 Conclusions

We found that O2P can be posed as a coding-pooling scheme based on evaluating similarities to tem-
plates. The templates of O2P self-adapt to the input  while the rest of the analyzed methods do not.
In practice  our formulation was used to improve the performance of O2P in image classiﬁcation.
We are currently analyzing self-adaptative templates in deep hierarchical networks.
Acknowledgments: We thank the ERC for support from AdG VarCity.

References
[1] J. Carreira  R. Caseiro  J. Batista  and C. Sminchisescu  “Semantic segmentation with second-

order pooling ” in ECCV  2012.

[2] K. Fukushima  “Neocognitron: A self-organizing neural network model for a mechanism of

pattern recognition unaffected by shift in position ” Biological cybernetics  1980.

[3] M. Riesenhuber and T. Poggio  “Hierarchical models of object recognition in cortex ” Nature

neuroscience  1999.

[4] D. G. Lowe  “Distinctive image features from scale-invariant keypoints ” IJCV  2004.

8

1 pyr.1+2 pyr.1+2+3 pyr.1+2+3 pyr. w/o SQSQ selected in val. set5204060801001280.550.60.650.70.750.8Sparse QuantizationMean accuracyCaltech 10176.52%78.63%75.55%65.14%5204060801001280.10.20.30.40.5Sparse QuantizationMean average precisionPASCAL VOC 200718.81%50.97%49.09%41.20%[5] J. Bergstra  D. Yamins  and D. Cox  “Making a science of model search: Hyperparameter

optimization in hundreds of dimensions for vision architectures ” in ICML  2013.

[6] D. Le Bihan  J.-F. Mangin  C. Poupon  C. A. Clark  S. Pappata  N. Molko  and H. Chabriat 
“Diffusion tensor imaging: concepts and applications ” Journal of magnetic resonance imag-
ing  2001.

[7] J. Weickert and H. Hagen  Visualization and Processing of Tensor Fields. Springer  2006.
[8] O. Tuzel  F. Porikli  and P. Meer  “Region covariance: A fast descriptor for detection and

classiﬁcation ” in ECCV  2006.

[9] P. Li and Q. Wang  “Local log-euclidean covariance matrix (L2ECM) for image representation

and its applications ” in ECCV  2012.

[10] R. Girshick  J. Donahue  T. Darrell  and J. Malik  “Rich feature hierarchies for accurate object

detection and semantic segmentation ” in CVPR  2014.

[11] L. Fei-Fei  R. Fergus  and P. Perona  “One-shot learning of object categories ” TPAMI  2006.
[12] M. Everingham  L. Van Gool  C. Williams  J. Winn  and A. Zisserman  “The PASCAL visual

object classes (VOC) challenge ” IJCV  2010.

[13] G. Csurka  C. R. Dance  L. Fan  J. Willamowski  and C. Bray  “Visual categorization with bags

of keypoints ” in Workshop on Statistical Learning in Computer Vision  ECCV  2004.

[14] A. Coates and A. Ng  “The importance of encoding versus training with sparse coding and

vector quantization ” in ICML  2011.

[15] X. Boix  G. Roig  and L. Van Gool  “Nested sparse quantization for efﬁcient feature coding ”

in ECCV  2012.

[16] J. Sanchez  F. Perronnin  T. Mensink  and J. Verbeek  “Image classiﬁcation with the ﬁsher

vector: Theory and practice ” IJCV  2013.

[17] V. Arsigny  P. Fillard  X. Pennec  and N. Ayache  “Geometric means in a novel vector space
structure on symmetric positive-deﬁnite matrices ” Journal on matrix analysis and applica-
tions  2007.

[18] R. Bhatia  Positive deﬁnite matrices. Princeton University Press  2009.
[19] X. Boix  M. Gygli  G. Roig  and L. Van Gool  “Sparse quantization for patch description ” in

CVPR  2013.

[20] R. E. Fan  K. W. Chang  C. J. Hsieh  X. R. Wang  and C. J. Lin  “LIBLINEAR: A library for

large linear classiﬁcation ” JMLR  2008.

[21] O. Duchenne  A. Joulin  and J. Ponce  “A graph-matching kernel for object categorization ” in

ICCV  2011.

[22] X. Zhou  K. Yu  T. Zhang  and T. S. Huang  “Image classiﬁcation using super-vector coding of

local image descriptors ” in ECCV  2010.

9

,Xavier Boix
Gemma Roig
Salomon Diether
Luc Gool
Pan Li
Niao He
Olgica Milenkovic