2010,Universal Consistency of Multi-Class Support Vector Classification,Steinwart was the ﬁrst to prove universal consistency of support vector machine  classiﬁcation. His proof analyzed the ‘standard’ support vector machine classiﬁer   which is restricted to binary classiﬁcation problems. In contrast  recent analysis  has resulted in the common belief that several extensions of SVM classiﬁcation to  more than two classes are inconsistent.  Countering this belief  we proof the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case.,Universal Consistency of Multi-Class

Support Vector Classiﬁcation

Dalle Molle Institute for Artiﬁcial Intelligence (IDSIA)  6928 Manno-Lugano  Switzerland

tobias@idsia.ch

Tobias Glasmachers

Abstract

Steinwart was the ﬁrst to prove universal consistency of support vector machine
classiﬁcation. His proof analyzed the ‘standard’ support vector machine classiﬁer 
which is restricted to binary classiﬁcation problems. In contrast  recent analysis
has resulted in the common belief that several extensions of SVM classiﬁcation to
more than two classes are inconsistent.
Countering this belief  we prove the universal consistency of the multi-class sup-
port vector machine by Crammer and Singer. Our proof extends Steinwart’s tech-
niques to the multi-class case.

1

Introduction

Support vector machine (SVM) as proposed in [1  8] are powerful classiﬁers  especially in the bi-
nary case of two possible classes. They can be extended to multi-class problems  that is  problems
involving more than two classes  in multiple ways which all reduce to the standard machine in the
binary case.

This is trivially the case for general techniques such as one-versus-one architectures and the one-
versus-all approach  which combine a set of binary machines to a multi-class decision maker. At
least three different ‘true’ multi-class SVM extensions have been proposed in the literature: The
canonical multi-class machine proposed by Vapnik [8] and independently by Weston and Watkins
[9]  the variant by Crammer and Singer [2]  and a conceptually different extension by Lee  Lin  and
Wahba [4].

Recently  consistency of multi-class support vector machines has been investigated based on proper-
ties of the loss function Ψ measuring empirical risk in machine training [7]. The analysis is based on
the technical property of classiﬁcation calibration (refer to [7] for details). This work is conceptually
related to Fisher consistency  in contrast to univeral statistical consistency  see [3  5]. Schematically 
Theorem 2 by Tewari and Bartlett [7] establishes the relation
SA ⇔ (SB ⇒ SC)  

(1)

for the terms

SA: The loss function Ψ is classiﬁcation calibrated.
SB: The Ψ-risk of a sequence ( ˆfn)n∈N of classiﬁers converges to the minimal possible Ψ-risk:

SC : The 0-1-risk of the same sequence ( ˆfn)n∈N of classiﬁers converges to the minimal possible

limn→∞ RΨ( ˆfn) = R∗
Ψ.
0-1-risk (Bayes risk): limn→∞ R( ˆfn) = R∗.

The classiﬁers ˆfn are assumed to result from structural risk minimization [8]  that is  the space Fn
for which we obtain ˆfn = arg min{RΨ(f )| f ∈ Fn} grows suitably with the size of the training
set such that SB holds.

1

The confusion around the consistency of multi-class machines arises from mixing the equivalence
and the implication in statement (1). Examples 1 and 2 in [7] show that the loss functions Ψ used
in the machines by Crammer and Singer [2] and by Weston and Watkins [9] are not classiﬁcation
calibrated  thus SA = false. Then it is deduced that the corresponding machines are not consistent
(SC = false)  although it can be deduced only that the implication SB ⇒ SC does not hold. This
tells us nothing about SC   even if SB can be established per construction.
We argue that the consistency of a machine is not necessarily determined by properties of its loss
function. This is because for SVMs it is necessary to provide a sequence of regularization parameters
in order to make the inﬁnite sample limit well-deﬁned. Thus  we generalize Steinwart’s universal
consistency theorem for binary SVMs (Theorem 2 in [6]) to the multi-class support vector machine
[2] proposed by Crammer and Singer:

Theorem 2. Let X ⊂ Rd be compact and k : X × X → R be a universal kernel with1
N ((X  dk)  ε) ∈ O(ε−α) for some α > 0. Suppose that we have a positive sequence (Cℓ)ℓ∈N
with ℓ · Cℓ → ∞ and Cℓ ∈ O(ℓβ−1) for some 0 < β < 1
α . Then for all Borel probability measures
P on X × Y and all ε > 0 it holds

lim
ℓ→∞

Pr∗(cid:0){T ∈ (X × Y )ℓ | R(fT k Cℓ) ≤ R∗ + ε}(cid:1) = 1 .

The corresponding notation will be introduced in sections 2 and 3. The theorem does not only estab-
lish the universal consistency of the multi-class SVM by Crammer and Singer  it also gives precise
conditions for how exactly the complexity control parameters needs to be coupled to the training set
size in order to obtain universal consistency. Moreover  the rigorous proof of this statement implies
that the common belief on the inconsistency of the popular multi-class SVM by Crammer and Singer
is wrong. This important learning machine is indeed universally consistent.

2 Multi-Class Support Vector Classiﬁcation

A multi-class classiﬁcation problem is stated by a training dataset T = (cid:0)(x1  y1)  . . .   (xℓ  yℓ)(cid:1) ∈
(X × Y )ℓ with label set Y of size |Y | = q < ∞. W.l.o.g.  the label space is represented by
Y = {1  . . .   q}. In contrast to the conceptually simpler binary case we have q > 2. The training
examples are supposed to be drawn i.i.d. from a probability distribution P on X × Y .
Let k : X × X → R be a positive deﬁnite (Mercer) kernel function  and let φ : X → H be
a corresponding feature map into a feature Hilbert space H such that hφ(x)  φ(x′)i = k(x  x′).
We call a function on X induced by k if there exists w ∈ H such that f (x) = hw  φ(x)i. Let
dk(x  x′) := kφ(x) − φ(x′)kH = pk(x  x) − 2k(x  x′) + k(x′  x′) denote the metric induced on
X by the kernel k.

Analog to Steinwart [6] we require that the input space X is a compact subset of Rd  and deﬁne the
notion of a universal kernel:

Deﬁnition 1.

(Deﬁnition 2 in [6]) A continuous positive deﬁnite kernel function k : X × X → R
on a compact subset X ⊂ Rd is called universal if the set of induced functions is dense in the
space C 0(X) of continuous functions  i.e.  for all g ∈ C 0(X) and all ε > 0 there exists an induced
function f with kg − fk∞ < ε.
Intuitively  this property makes sure that the feature space of a kernel is rich enough to achieve con-
sistency for all possible data generating distributions. For a detailed treatment of universal kernels
we refer to [6].

An SVM classiﬁer for a q-class problem is given in the form of a vector-valued function f : X → Rq
with component functions fu : X → R  u ∈ Y (sometimes restricted by the so-called sum-to-zero
constraintPu∈Y fu = 0). Each of its components takes the form fu(x) = hwu  φ(x)i + bu with
wu ∈ H and bu ∈ R. Then we turn f into a classiﬁer by feeding its result into the ‘decision’ function

κ : Rq → Y ;

(v1  . . .   vq)T 7→ minn arg max{vu | u ∈ Y }o ∈ Y .

1For f  g : R+ → R+ we deﬁne f (x) ∈ O(g(x)) iff ∃c  x0 > 0 such that f (x) ≤ c · g(x) ∀x > x0.

2

Here  the arbitrary rule for breaking ties favors the smallest class index.2 We denote the SVM
hypothesis by h = κ ◦ f : X → Y .
The multi-class SVM variant proposed by Crammer and Singer uses functions without offset terms
(bu = 0 for all u ∈ Y ). For a given training set T = (cid:0)(x1  y1)  . . .   (xℓ  yℓ)(cid:1) ∈ (X × Y )ℓ this
machine deﬁnes the function f   determined by (w1  . . .   wq) ∈ Hq  as the solution of the quadratic

program

minimize Xu∈Y

Xi=1
hwyi − wu  φ(xi)i ≥ 1 − ξi
The slack variables in the optimum can be written as

hwu  wui +

s.t.

ξi

C
ℓ ·

ℓ

∀ i ∈ {1  . . .   ℓ}  u ∈ Y \ {yi} .

ξi = max

v∈Y \{yi}n(cid:2)1 − (fyi (xi) − fv(xi))(cid:3)+o ≥(cid:2)1 − δh(xi) yi − fyi(xi) + fh(xi)(xi)(cid:3)+  

with the auxiliaury function [t]+ := max{0  t}. We denote the function induced by the solution of
this problem by f = fT k C = (hw1 ·i  . . .  hwq ·i)T .
Let s(x) := 1 − max{P (y|x)| y ∈ Y } denote the noise level  that is  the probability of error of a
Bayes optimal classiﬁer. We denote the Bayes risk by R∗ =RX s(x)dx. For a given (measurable)
hypothesis h we deﬁne its error as Eh(x) := 1 − P (h(x)|x)  and its suboptimality w.r.t. Bayes-
optimal classiﬁcation as ηh(x) := Eh(x) − s(x) = max{P (y|x)| y ∈ Y } − P (h(x)|x). We have
Eh(x) ≥ s(x) and thus ηh(x) ≥ 0 up to a zero set.

(2)

(3)

3 The Central Construction

In this section we introduce a number of deﬁnitions and constructions preparing the proofs in the
later sections. Most of the differences to the binary case are incorporated into these constructions
such that the lemmas and theorems proven later on naturally extend to the multi-class case. Let

∆ := {p ∈ Rq | pu ≥ 0 ∀ u ∈ Y and Pu∈Y pu = 1} denote the probability simplex over Y . We

introduce the covering number of the metric space (X  dk) as

N ((X  dk)  ε) := minnn (cid:12)(cid:12)(cid:12) ∃{x1  . . .   xn} ⊂ X such that X ⊂

B(xi  ε)o  
with B(x  ε) = {x′ ∈ X | dk(x  x′) < ε} being the open ball of radius ε > 0 around x ∈ X.
Next we construct a partition of a large part of the input space X into suitable subsets. In a ﬁrst step
we partition the probability simplex  then we transfer this partition to the input space  and ﬁnally
we discard small subsets of negligible impact. The resulting partition has a number of properties of
importance for the proofs of diverse lemmas in the next section.

[i=1

n

We start by deﬁning τ = ε/(q + 5)  where ε is the error bound found in Theorems 1 and 2. Thus  τ
is simply a multiple of ε  which we can think of as an arbitrarily small positive number.

We split the simplex ∆ into a partition of ‘classiﬁcation-aligned’ subsets

for y ∈ Y   on which the decision function κ decides for class y. We deﬁne the grid

∆y := κ−1({y}) =np ∈ ∆ (cid:12)(cid:12)(cid:12) py > pu for u < y and py ≥ pu for u > yo
˜Γ =n[n1τ  (n1 + 1)τ ) × ··· × [nqτ  (nq + 1)τ ) ⊂ Rq (cid:12)(cid:12)(cid:12) (n1  . . .   nq)T ∈ Zqo

of half-open cubes. Then we combine both constructions to the partition

Γ := [y∈Y n˜γ ∩ ∆y (cid:12)(cid:12)(cid:12) ˜γ ∈ ˜Γ and ˜γ ∩ ∆y 6= ∅o

2Note that any other deterministic rule for breaking ties can be realized by permuting the class indices.

3

probabiliy mass  resulting in

of ∆ into classiﬁcation-aligned subsets of side length upper bounded by τ . We have the trivial
upper bound |Γ| ≤ D := q · (1/τ + 1)q for the size of the partition. The partition Γ will serve
as an index set in a number of cases. The ﬁrst one of these is the partition X = Sγ∈Γ Xγ with
Xγ :=(cid:8)x ∈ X(cid:12)(cid:12) P (y|x) ∈ γ(cid:9).
The compactness of X ensures that the distribution P is regular. Thus  for each γ ∈ Γ there exists a
compact subset ˜Kγ ⊂ Xγ with P ( ˜Kγ) ≥ (1 − τ /2) · P (Xγ). We choose minimal partitions ˜Aγ of
A such that the diameter of each A ∈ ˜Aγ is bounded by σ = τ /(2√C). All of
each ˜Kγ =SA∈ ˜Aγ
these sets are summarized in the partition ˜A =Sγ∈Γ
˜Aγ . Now we drop all A ∈ ˜Aγ below a certain
with M := D · N ((X  dk)  σ). We summarize these sets in Kγ =SA∈Aγ

Aγ :=nA ∈ ˜Aγ (cid:12)(cid:12)(cid:12) PX (A) ≥
2Mo  
A! ≥ PX
A
Kγ
 = PX  [A∈A
 [A∈ ˜A
 − τ /2
Xγ
 − τ /2 ≥ PX
˜Kγ
[γ∈Γ
 − τ /2 − τ /2 = PX (X) − τ = 1 − τ

PX
[γ∈Γ
= PX
[γ∈Γ

The ﬁrst estimate makes use of | ˜A| ≤ M and condition (4)  while the second inequality follows
from the deﬁnition of ˜Kγ .
To simplify notation  we associate a number of quantities with the sets γ ∈ Γ and Xγ . We denote
the Bayes-optimal decision by y(Xγ) = y(γ) := κ(p) for any p ∈ γ  and for y ∈ Y we deﬁne the
lower and upper bounds

A and A :=Sγ∈Γ Aγ .

These sets cover nearly all probability mass of PX in the sense

(4)

τ

Ly(Xγ) = Ly(γ) := infnpy (cid:12)(cid:12)(cid:12) p ∈ γo

and

on the corresponding components in the probability simplex. We canonically extend these deﬁni-

tions to the above deﬁned sets Kγ   ˜Kγ   and A ∈ A  which are all subsets of exactly one of the sets
Xγ   by deﬁning y(S) := y(γ) for all non-empty subsets S ⊂ Xγ . The resulting construction has
the following properties:

Uy(Xγ) = Uy(γ) := supnpy (cid:12)(cid:12)(cid:12) p ∈ γo

Ly(γ) = 0 or Ly(γ) ≥ τ .
only on τ and q  but not on T   k  or C.3

each set Xγ as well as on each of their subsets  most importantly on each A ∈ A.

(P1) The decision function κ is constant on each set γ ∈ Γ  and thus h = κ ◦ f is constant on
(P2) For each y ∈ Y   the side length Uy(γ) − Ly(γ) of each set γ ∈ Γ is upper bounded by τ .
(P3) It follows from the construction of Γ that for each y ∈ Y and γ ∈ Γ we have either
(P4) The cardinality of the partition Γ is upper bounded by D = q · (1/τ + 1)q  which depends
(P5) The cardinality of the partition A is upper bounded by M = D · N ((X  dk)  τ /(2√C)) 
(P6) The setSA∈A A =Sγ∈Γ Kγ ⊂ X covers a probability mass (w.r.t. PX ) of at least (1−τ ).
(P7) Each A ∈ A covers a probability mass (w.r.t. PX ) of at least
(P8) Each A ∈ A has diameter less than σ = τ /(2√C)  that is  for x  x′ ∈ A we have

which is ﬁnite by Lemma 1.

τ
2M .

dk(x  x′) < σ.

With properties (P2) and (P6) it is straight-forward to obtain the inequality

1 −Xγ∈Γ
≤ R∗ ≤1 −Xγ∈Γ

Ly(γ)(γ) · PX (Xγ) − τ ≤ 1 −Xγ∈Γ
Ly(γ)(γ) · PX (Xγ) ≤ 1 −Xγ∈Γ

Uy(γ)(γ) · PX (Xγ)

Uy(γ)(γ) · PX (Xγ) + τ

(5)

3A tight bound would be in O(τ 1−q).

4

F A u

ℓ

for the risk.
Now we are in the position to deﬁne the notion of a ‘typical’ training set. For ℓ ∈ N  u ∈ Y   and
A ∈ A  we deﬁne

:=n(cid:0)(x1  y1)  . . .   (xℓ  yℓ)(cid:1) ∈ (X × Y )ℓ (cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:8)n ∈ {1  . . .   ℓ}(cid:12)(cid:12) xn ∈ A  yn = u(cid:9)(cid:12)(cid:12) ≥ ℓ · (1 − τ ) · Lu(A) · PX (A)o .

Intuitively  we ask that the number of examples of class u in A does not deviate too much from its
expectation  introducing two approximations: The multiplicative factor (1−τ )  and the lower bound
Lu(A) on the conditional probability of class u in A. We combine the properties of all these sets in
the set Fℓ :=Tu∈Y TA∈A F A u
of training sets of size ℓ  with the same lower bound on the number
of training examples in all sets A ∈ A  and for all classes u ∈ Y .

ℓ

4 Preparations

The proof of our main result follows the proofs of Theorems 1 and 2 in [6] as closely as possible. For
the sake of clarity we organize the proof such that all six lemmas in this section directly correspond
to Lemmas 1-6 in [6].

Lemma 1.

(Lemma 1 from [6]) Let k : X × X → R be a universal kernel on a compact subset X

or Rd and Φ : X → H be a feature map of k. The Φ is continuous and

dk(x  x′) := kΦ(x) − Φ(x′)k

deﬁnes a metric on X such that id : (X k·k) → (X  dk) is continuous. In particular  N ((X  dk)  ε)
is ﬁnite for all ε > 0.

Lemma 2. Let X ⊂ Rd be compact and let k : X × X → R be a universal kernel. Then  for all
ε > 0 and all pairwise disjoint and compact (or empty) subsets ˜Ku ⊂ X  u ∈ Y   there exists an

induced function

f : X →h − 1/2 · (1 + ε)  1/2 · (1 + ε)iq

such that

;

x 7→ (hw∗

1  xi  . . .  hw∗

q   xi)T  

fu(x) ∈ [1/2  1/2 · (1 + ε)]
fu(x) ∈ [−1/2 · (1 + ε) −1/2]

if x ∈ ˜Ku
if x ∈ ˜Kv for some v ∈ Y \ {u}

for all u ∈ Y .
Proof. This lemma directly corresponds to Lemma 2 in [6]  with slightly different cases. Its proof
is completely analogous.

Lemma 3. The probability of the training sets Fℓ is lower bounded by

P ℓ(Fℓ) ≥ 1 − q · M · exp(cid:18)−

1
8

(τ 6/M 2)ℓ(cid:19) .

ℓ

Proof. Let us ﬁx A ∈ A and u ∈ Y . In the case Lu(A) = 0 we trivially have P ℓ(cid:0)(X × Y )ℓ \
(cid:1) = 0. Otherwise we consider T =(cid:0)(x1  y1)  . . .   (xℓ  yℓ)(cid:1) ∈ (X × Y )ℓ and deﬁne the binary
F A u
variables zi := 1{A×{u}}(xi  yi)  where the indicator function 1S(s) is one for s ∈ S and zero
otherwise. This deﬁnition allows us to express the cardinality (cid:12)(cid:12)(cid:8)n ∈ {1  . . .   ℓ}(cid:12)(cid:12) xn ∈ A  yn =
u(cid:9)(cid:12)(cid:12) =Pℓ

inequality. The inequality  applied to the variables zi  states

i=1 zi found in the deﬁnition of F A u

in a form suitable for the application of Hoeffding’s

ℓ

P ℓ  ℓ
Xi=1

zi ≤ (1 − τ ) · E · ℓ! ≤ exp(cid:0)−2(τ E)2ℓ(cid:1)  

5

where E := E[zi] =RA×{u} dP (x  y) =RA P (u|x)dx ≥ Lu(A) · PX (A) > 0. Due to E > 0 we

can use the relation

in order to obtain Hoeffding’s formula for the case of strict inequality

ℓ

Xi=1
P ℓ  ℓ
Xi=1

zi ≤ (1 − τ ) · E · ℓ ⇒

zi < (1 − τ /2) · E · ℓ

ℓ

Xi=1

zi < (1 − τ ) · E · ℓ! ≤ exp(cid:18)−

1
2

(τ E)2ℓ(cid:19) .

obtain

Combining E ≥ Lu(A) · PX (A) andPℓ
i=1 zi < (1 − τ ) · Lu(A) · PX (A) · ℓ ⇔ T 6∈ F A u
ℓ (cid:17) = P ℓ  ℓ
zi < (1 − τ ) · Lu(A) · PX (A) · ℓ!
Xi=1

P ℓ(cid:16)(X × Y )ℓ \ F A u
≤ P ℓ  ℓ
(τ Lu(A)PX (A))2ℓ(cid:19) .
Xi=1
Properties (P3) and (P7) ensure Lu(A) ≥ τ and PX (A) ≥ τ /(2M ). Applying these to the previous
inequality results in

zi < (1 − τ ) · E · ℓ! ≤ exp(cid:18)−

(τ E)2ℓ(cid:19) ≤ exp(cid:18)−

1
2

1
2

we

ℓ

which also holds in the case Lu(A) = 0 treated earlier. Finally  we use the union bound

ℓ (cid:17) ≤ exp(cid:18)−
P ℓ(cid:16)(X × Y )ℓ \ F A u
1 − P ℓ(Fℓ) = 1 − P ℓ \u∈Y \A∈A
≤ |Y | · |A| · exp(cid:18)−

1
8

1
2

1
8

(τ 6/M 2)ℓ(cid:19)  
(τ 3/(2M ))2ℓ(cid:19) = exp(cid:18)−
ℓ (cid:17)!
ℓ ! = P ℓ [u∈Y [A∈A(cid:16)(X × Y )ℓ \ F A u

F A u

(τ 6/M 2)ℓ(cid:19) ≤ q · M · exp(cid:18)−

1
8

(τ 6/M 2)ℓ(cid:19)

and properties (P4) and (P5) to prove the assertion.

Lemma 4. The SVM solution f and the hypothesis h = f ◦ κ fulﬁll

R(f ) ≤ R∗ +ZX

ηh(x)dx .

Proof. The lemma follows directly from the deﬁnition of ηh  even with equality. We keep it here
because it is the direct counterpart to the (stronger) Lemma 4 in [6].

Lemma 5. For all training sets T ∈ Fℓ the SVM solution given by (w1  . . .   wq) and (ξ1  . . .   ξℓ)
fulﬁlls

Xu∈Y

hwu  wui +

C
ℓ

ℓ

Xi=1

ξi ≤ Xu∈Y

u  w∗

hw∗

ui + C(R∗ + 2τ )  

with (w∗

1  . . .   w∗

q ) as deﬁned in Lemma 2.

Proof. The optimality of the SVM solution for the primal problem (2) implies

hwu  wui +

C
ℓ

Xu∈Y

ℓ

Xi=1

ξi ≤ Xu∈Y

u  w∗

hw∗

ui +

C
ℓ

ξ∗
i

ℓ

Xi=1

for any feasible choice of the slack variables ξ∗
i =
1 + τ for P (y | xi) 6∈ ∆yi and zero otherwise which corresponds to a feasible solution according
to the construction of w∗
i ≤ ℓ · (R∗ + 2τ ).

i . We choose the values of these variables as ξ∗

u in Lemma 2. Then it remains to show that Pℓ

i=1 ξ∗

6

The deﬁnition of Fℓ yields

Let n+ = (cid:12)(cid:12)(cid:8)i ∈ {1  . . .   ℓ}(cid:12)(cid:12) P (y | xi) ∈ ∆yi(cid:9)(cid:12)(cid:12) denote the number of training examples correctly
classiﬁed by the Bayes rule expressed by ∆yi (or κ). Then we havePℓ
i = (1 + τ )(ℓ − n+).
PX (A)

Lu(γ) · XA∈Aγ
ℓ · (1 − τ ) · Lu(γ) · PX (A) = ℓ · (1 − τ ) ·Xu∈Y Xγ∈Γ
n+ ≥ Xu∈Y XA∈A

hLu(γ) · PX (Kγ)i = ℓ · (1 − τ ) ·Xγ∈ΓhLy(γ)(γ) · PX (Kγ)i

i=1 ξ∗

y(A)=u

y(γ)=u

= ℓ · (1 − τ ) ·Xu∈Y Xγ∈Γ
≥ ℓ · (1 − τ ) ·

y(γ)=u

where the last line is due to inequality (5). We obtain

Xγ∈ΓhLy(γ)(γ) · PX (Xγ)i − τ
Xi=1
= ℓ · [R∗ + τ + τ 2(1 − R∗)] ≤ ℓ · [R∗ + τ + τ 2] ≤ ℓ · (R∗ + 2τ )  

 ≥ ℓ · (1 − τ ) · (1 − R∗)  

ξ∗
i ≤ ℓ · (1 + τ ) · (1 − (1 − τ ) · (1 − R∗))

ℓ

which proves the claim.

Lemma 6. For all training sets T ∈ Fℓ the sum of the slack variables (ξ1  . . .   ξℓ) corresponding
to the SVM solution fulﬁlls

ℓ

Xi=1

ξi ≥ ℓ · (1 − τ )2 ·(cid:18)R∗ +ZX

ηh(x) dPX (x) − q · τ(cid:19) .

Proof. Problem (2) takes the value C in the feasible solution w1 = . . .   wq = 0 and ξ1 = ··· =
u ∈ Y . Thus  property (P8) makes sure that |fu(x) − fu(x′)| ≤ τ /2 for all x  x′ ∈ A and u ∈ Y .

ξℓ = 1. Thus  we havePu∈Y kwuk2 ≤ C in the optimum  and we deduce kwuk ≤ √C for each
Xi=1

The proof works through the following series of inequalities. The details are discussed below.

ξi

ℓ

1

yi=u

yi=u

PX (A) ·ZAh1 − δh(x) u + fh(x)(x) − fu(x) − 2 ·

yi=u(cid:2)1 − δh(xi) u + fh(xi)(xi) − fu(xi)(cid:3)+

ξi = XA∈AXu∈Y Xxi∈A
≥ XA∈AXu∈Y Xxi∈A
≥ XA∈AXu∈Y Xxi∈A
≥ XA∈AXu∈Y
= ℓ · (1 − τ ) · XA∈AZA
{z
≥ ℓ · (1 − τ ) · XA∈AZA
≥ ℓ · (1 − τ )2 · XA∈AZA(cid:16)1 − q · τ − Lh(x)(A)(cid:17) dPX (x)



+

1 − τ − δh(x) u + fh(x)(x) − fu(x)
}
(1 − τ ) · Xu∈Y \{h(x)}

Lu(A) dPX (x)

|

≥0

ℓ · (1 − τ ) · Lu(A) ·ZA(cid:2)1 − τ − δh(x) u + fh(x)(x) − fu(x)(cid:3)+ dPX (x)

dPX (x)

τ

2i+

Lu(A) dPX (x)

·Xu∈Y

7

≥ ℓ · (1 − τ )2 · XA∈AZA(cid:16)1 − q · τ − 1 + s(x) + ηh(x)(cid:17) dPX (x)
= ℓ · (1 − τ )2 ·(cid:18)R∗ +ZX

ηh(x) dPX (x) − q · τ(cid:19)

ℓ

The ﬁrst inequality follows from equation (3). The second inequality is clear from the deﬁnition of
F A u
together with |fu(x) − fu(x′)| ≤ τ /2 within each A ∈ A. For the third inequality we use
that the case u = h(x) does not contribute  and the non-negativity of fh(x)(x) − fu(x). In the next
steps we make use ofPu∈Y Lu(A) ≥ 1 − q · τ and the lower bound Lh(x)(x) ≤ P (h(x)|x) =
1 − Eh(x) = 1 − s(x) − ηh(x)  which can be deduced from properties (P1) and (P2).

5 Proof of the Main Result

Just like the lemmas  we organize our theorems analogous to the ones found in [6]. We start with a
detailed but technical auxiliaury result.

1
8

Lemmas 5 and 6 to

(τ 6/M 2)ℓ(cid:19)  

Theorem 1. Let X ⊂ Rd be compact  Y = {1  . . .   q}  and k : X × X → R a universal kernel.
Then  for all Borel probability measures P on X × Y and all ε > 0 there exists a constant C ∗ > 0
such that for all C ≥ C ∗ and all ℓ ≥ 1 we have

Pr∗(cid:16)(cid:8)T ∈ (X × Y )ℓ (cid:12)(cid:12) R(fT k C) ≤ R∗ + ε(cid:9)(cid:17) ≥ 1 − qM exp(cid:18)−

where Pr∗ is the outer probability of P ℓ  fT k C is the solution of problem (2)  M = q · (1/τ + 1)q ·
N ((X  dk)  τ /(2√C))  and τ = ε/(q + 5).
Proof. According to Lemma 3 it is sufﬁcient to show R(fT k C ) ≤ R∗ + ε for all T ∈ Fℓ.
Lemma 4 provides the estimate R(f ) ≤ R∗ +RX ηh(x) dPX (x)  such that it remains to show
that RX ηh(x) dPX (x) ≤ ε for T ∈ Fℓ. Consider w∗
u as deﬁned in Lemma 2  then we combine


Xu∈Y
(1 − τ )2 ·




R∗ +ZX

uk2 −Xu∈Y
kw∗
kwuk2
|
|
{z
}
Using a−τ ≤ (1−τ )·a for any a ∈ [0  1]  we deriveRX ηh(x) dPX (x) ≤ 1
C Pu∈Y kw∗
τ . With the choice C ∗ = 1
(q + 5) · τ = ε.
Proof of Theorem 2. Up to constants  this short proof coincides with the proof of Theorem 2 in [6].
Because of the importance of the statement and the brevity of the proof we repeat it here:

uk2+(q+4)·
uk2 and the condition C ≥ C ∗ we obtainRX ηh(x) dPX (x) ≤



ηh(x) dPX (x) − q · τ
}

{z
τ ·Pu∈Y kw∗

+ (R∗ + 2τ ) .

1
C

≤

≤1

≥0

Since ℓ · Cℓ → ∞ there exists an integer ℓ0 such that ℓ · Cℓ ≥ C ∗ for all ℓ ≥ ℓ0. Thus for ℓ ≥ ℓ0

Theorem 1 yields

Pr∗(cid:16)(cid:8)T ∈ (X × Y )ℓ (cid:12)(cid:12) R(fT k Cℓ ) ≤ R∗ + ε(cid:9)(cid:17) ≥ 1 − qMℓ exp(cid:18)−

where Mℓ = D · N ((X  dk)  τ /(2√Cℓ)). Moreover  by the assumption on the covering numbers of

ℓ )ℓ(cid:19)  

(τ 6/M 2

1
8

(X  dk) we obtain M 2

ℓ ∈ O((ℓ · Cℓ)2) and thus ℓ · M −2

ℓ → ∞.

6 Conclusion

We have proven the universal consistency of the popular multi-class SVM by Crammer and Singer.
This result disproves the common belief that this machine is in general inconsistent. The proof itself
can be understood as an extension of Steinwart’s universal consistency result for binary SVMs. Just
like there are different extensions of the binary SVM to multi-class classiﬁcation in the literature 
we strongly believe that our proof can be further generalized to cover other multi-class machines 
such as the one proposed by Weston and Watkins  which is a possible direction for future research.

8

References

[1] C. Cortes and V. Vapnik. Support-Vector Networks. Machine Learning  20(3):273–297  1995.

[2] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector

machines. Journal of Machine Learning Research  2:265–292  2002.

[3] S. Hill and A. Doucet. A Framework for Kernel-Based Multi-Category Classiﬁcation. Journal

of Artiﬁcial Intelligence Research  30:525–564  2007.

[4] Y. Lee  Y. Lin  and G. Wahba. Multicategory Support Vector Machines: Theory and Application
to the Classiﬁcation of Microarray Data and Satellite Radiance Data. Journal of the American
Statistical Association  99(465):67–82  2004.

[5] Y. Liu. Fisher Consistency of Multicategory Support Vector Machines. Journal of Machine

Learning Research  2:291–298  2007.

[6] I. Steinwart. Support Vector Machines are Universally Consistent. J. Complexity  18(3):768–

791  2002.

[7] A. Tewari and P. L. Bartlett. On the Consistency of Multiclass Classiﬁcation Methods. Journal

of Machine Learning Research  8:1007–1025  2007.

[8] V. Vapnik. Statistical Learning Theory. Wiley  New-York  1998.

[9] J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition.

In
M. Verleysen  editor  Proceedings of the Seventh European Symposium On Artiﬁcial Neural
Networks (ESANN)  pages 219–224  1999.

9

,Jianlong Chang
Jie Gu
Lingfeng Wang
GAOFENG MENG
SHIMING XIANG
Chunhong Pan