2016,beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data,During the past few years  the machine learning community has paid attention to developping new methods for learning from weakly labeled data. This field covers different settings like semi-supervised learning  learning with label proportions  multi-instance learning  noise-tolerant learning  etc. This paper presents a generic framework to deal with these weakly labeled scenarios. We introduce the beta-risk as a generalized formulation of the standard empirical risk based on surrogate margin-based loss functions. This risk allows us to express the reliability on the labels and to derive different kinds of learning algorithms. We specifically focus on SVMs and propose a soft margin beta-svm algorithm  which behaves better that the state of the art.,β-risk: a New Surrogate Risk for Learning

from Weakly Labeled Data

Valentina Zantedeschi∗

Rémi Emonet

Marc Sebban

ﬁrstname.lastname@univ-st-etienne.fr

Univ Lyon  UJM-Saint-Etienne  CNRS  Institut d Optique Graduate School 
Laboratoire Hubert Curien UMR 5516  F-42023  SAINT-ETIENNE  France

Abstract

During the past few years  the machine learning community has paid attention to
developing new methods for learning from weakly labeled data. This ﬁeld covers
different settings like semi-supervised learning  learning with label proportions 
multi-instance learning  noise-tolerant learning  etc. This paper presents a generic
framework to deal with these weakly labeled scenarios. We introduce the β-risk as
a generalized formulation of the standard empirical risk based on surrogate margin-
based loss functions. This risk allows us to express the reliability on the labels and
to derive different kinds of learning algorithms. We speciﬁcally focus on SVMs
and propose a soft margin β-SVM algorithm which behaves better that the state of
the art.

1

Introduction

The growing amount of data available nowadays allowed us to increase the conﬁdence in the models
induced by machine learning methods. On the other hand  it also caused several issues  especially in
supervised classiﬁcation  regarding the availability of labels and their reliability. Because it may be
expensive and tricky to assign a reliable and unique label to each training instance  the data at our
disposal for the application at hand are often weakly labeled. Learning from weak supervision has
received important attention over the past few years [14  12]. This research ﬁeld includes different
settings: only a fraction of the labels are known (Semi-Supervised learning [22]); we can access only
the proportions of the classes (Learning with Label Proportions [19] and Multi-Instance Learning [8]);
the labels are uncertain or noisy (Noise-Tolerant Learning [1  18  16]); different discording labels are
given to the same instance by different experts (Multi-Expert Learning [21]); labels are completely
unknown (Unsupervised Learning [11]). As a consequence of this statement of fact  the data provided
in all these situations cannot be fully exploited using supervised techniques  at the risk of drastically
reducing the performance of the learned models. To address this issue  numerous machine learning
methods have been developed to deal with each of the previous speciﬁc situations. However  all
these weakly labeled learning tasks share common features mainly relying on the conﬁdence in
the labels  opening the door to the development of generic frameworks. Unfortunately  only a few
attempts have tried to address several settings with the same approach. The most interesting one has
been presented in [14] where the authors propose WELLSVM which is dedicated to deal with three
different weakly labeled learning scenarios: semi-supervised learning  multi-instance learning and
clustering. However  WELLSVM focuses speciﬁcally on Support Vector Machines and it requires to

∗http://vzantedeschi.com/

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

derive a new optimization problem for each new task. Even though WELLSVM constitutes a step
further towards general models  it stopped in midstream constraining the learner to use SVMs.
This paper aims to bridge this gap by presenting a generic framework for learning from weakly labeled
data. Our approach is based on the derivation of the β-risk   a new surrogate empirical risk deﬁned as
a strict generalization of the standard empirical risk relying on surrogate margin-based loss functions.
The main interesting property of the β-risk comes from its ability to exploit the information given
by the weakly supervised setting and encoded as a β matrix reﬂecting the supervision on the labels.
Moreover  the instance-speciﬁc weights β let one integrate in classical methods the side information
provided by the setting. This is the peculiarity w.r.t. [18  16]: in both papers  the proposed losses are
deﬁned using class-dependent weights (ﬁxed to 1/2 for the ﬁrst paper  and dependent on the class
noise rate for the latter) while in our approach the used weights are provided for each instance  which
gives a more ﬂexible formulation. Making use of this β-risk   we design a generic algorithm devoted
to address different kinds of aforementioned weakly labeled settings. To allow a comparison with
the state of the art  we instantiate it with a learner that takes the form of an SVM algorithm. In this
context  we derive a soft margin β-SVM algorithm and show that it outperforms WELLSVM.
The remainder of this paper is organized as follows: in Section 2  we deﬁne the empirical surrogate
β-risk and show under which conditions it can be used to learn without explicitly accessing the
labels; we also show how to instantiate β according to the weakly labeled learning setting at hand; in
Section 3  we present our generic iterative algorithm for learning with weakly labeled data and in
Section 4 we exploit our new framework to derive a novel formulation of the Support Vector Machine
problem  the β-SVM ; ﬁnally  we report experiments in semi-supervised learning and learning with
label noise  conducted on classical datasets from the UCI repository [15]  in order to compare our
algorithm with the state of the art approaches.

2 From Classical Surrogate Losses and Surrogate Risks to the β-risk

In this section  we ﬁrst provide reminders about surrogate losses and then exploit the characteristics of
the popular loss functions to introduce the empirical surrogate β-risk . The β-risk formulation allows
us to tackle the problem of learning with weakly labeled data. We show under which conditions it
can be used instead of the standard empirical surrogate risk (deﬁned in a fully supervised context).
Those conditions give insight on how to design algorithms that learn from weak supervision. We
restrain our study to the context of binary classiﬁcation.

2.1 Preliminaries
In statistical learning  a common approach for choosing the optimal hypothesis h∗ from a hypothesis
class H is to select the classiﬁer that minimizes the expected risk over the joint space Z = X × Y  
where X is the feature space and Y the label space  expressed as

(cid:90)

R(cid:96)(h) =

X×Y

(cid:96)(yh(x))p(x  y)dxdy

with (cid:96) : H × Z → R+ a margin-based loss function.
Since the true distribution of the data p(x  y) is usually unknown  machine learning algorithms
typically minimize the empirical version of the risk  computed over a ﬁnite set S composed of m
instances (xi  yi) i.i.d. drawn from a distribution over X × {−1  1}:

R(cid:96)(S  h) =

1
m

(cid:96)(yih(xi)).

m(cid:88)

i=1

The most natural loss function is the so-called 0-1 loss. As this function is not convex  not differ-
entiable and has zero gradient  other loss functions are commonly employed instead. These losses 
such as the logistic loss (e.g.  for the logistic regression [6])  the exponential loss (e.g.  for boosting
techniques [10]) and the hinge loss (e.g.  for the SVM [7])  are convex and smooth relaxations of
the 0-1 loss. Theoretical studies on the characteristics and behavior of such surrogate losses can
be found in [17  2  20]. In particular  [17] showed that each commonly used surrogate loss can be

2

characterized by a permissible function φ (see below) and rewritten as Fφ(x)

Fφ(x) =

φ∗(−x) − aφ

bφ

where φ∗(x) = supa(xa − φ(a)) is the Legendre conjugate of φ (for more details  see [4])  aφ =
2 ) − aφ > 0. As presented by the authors of [13] and [17] 
−φ(0) = −φ(1) ≥ 0 and bφ = −φ( 1
a permissible function is a function f : [0  1] → R−  symmetric about − 1
2  differentiable on
]0  1[ and strictly convex. For instance  the permissible function φlog related to the logistic loss
Fφ(x) = log(1 + exp−x) is:

φlog(x) = x log(x) + (1 − x) log(1 − x)

and aφ = 0 and bφ = log(2).
As detailed in [17]  considering a surrogate loss Fφ  the empirical surrogate risk of an hypothesis
h : X → R w.r.t. S can be expressed as:

Rφ(S  h) =

1
m

(cid:0)yi ∇−1

φ (h(xi))(cid:1) =

Dφ

m(cid:88)

i=1

m(cid:88)

i=1

bφ
m

Fφ(yih(xi))

with Dφ the Bregman Divergence

Dφ(x  y) = φ(x) − φ(y) − (x − y)∇φ(y).

In order to evaluate such risk Rφ(S  h)  it is mandatory to provide the labels y for all the instances. In
addition  it is not possible to take into account eventual uncertainties on the given labels. Consequently 
Rφ is deﬁned in a totally supervised context  where the labels y are known and considered to be
true. In order to face the numerous situations where training data may be weakly labeled  we claim
that there is a need to ﬁll the gap by deﬁning a new empirical surrogate risk that can deal with such
settings. In the following section  we propose a generalization of the empirical surrogate risk  called
the empirical surrogate β−risk  which can be employed in the context of weakly labeled data instead
of the standard one under some linear conditions on the margin.

2.2 The Empirical Surrogate β-risk
Before deﬁning the empirical surrogate β-risk for any loss Fφ and hypothesis h ∈ H  let us rewrite
the deﬁnition of Rφ introducing a new set of variables named β  and that can be laid out as a 2×m
matrix.
Lemma 2.1. For any S  φ and h  and for any non-negative real coefﬁcients β-1
for each instance xi ∈ S such that β-1
rewritten as

i deﬁned
i = 1  the empirical surrogate risk Rφ(S  h) can be

i and β+1

i + β+1

Rφ(S  h) = Rφ(S  h  β)

where

Rφ(S  h  β) =

m(cid:88)

i=1

bφ
m

(cid:88)

σ∈

{-1 +1}

βσ
i Fφ(σh(xi)) +

m(cid:88)

i=1

1
m

i (−yih(xi)).
β-yi

The coefﬁcient β+1
i
in (or the probability of) the label +1 (resp. -1) assigned to xi.

(resp. β-1

i ) for an instance xi can be interpreted here as the degree of conﬁdence

3

Proof.

Rφ(S  h) =

=

=

=

bφ
m

bφ
m

bφ
m

bφ
m

i=1

m(cid:88)
m(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

(cid:0)βyi
(cid:18)
(cid:88)

i=1

σ∈

{-1 +1}

Fφ(yih(xi))

i Fφ(yih(xi)) + β-yi

i Fφ(yih(xi))(cid:1)
(cid:18)

i Fφ(yih(xi)) + β-yi
βyi

i

Fφ(−yih(xi)) − yih(xi)

βσ
i Fφ(σh(xi)) +

1
m

m(cid:88)

i=1

bφ
i (−yih(xi)).
β-yi

(cid:19)(cid:19)

(1)

(2)

(3)

i + β+1

i = 1; Eq. (2) is due to the fact that φ∗(−x) = φ∗(x) − x (see the sup-
=

Eq. (1) is because β-1
plementary material) for any permissible function φ  so that Fφ(x) = φ∗(−x)−aφ
Fφ(−x) − x
From Eq. (3)  and considering that the sample S is composed by the ﬁnite set of features X and labels
Y  we can write that

= φ∗(x)−aφ−x

bφ

bφ

bφ

.

Rφ(S  h) = Rφ(S  h  β) = Rβ

β-yi
i yih(xi)

(4)

m(cid:88)

i=1

φ(X   h) − 1
m
(cid:88)

σ∈

m(cid:88)

i=1

bφ
m

βσ
i Fφ(σh(xi))

where

Rβ
φ(X   h) =

m |β-1

0   ...  β+1

0   ...  β-1
m].

{-1 +1}
is the empirical surrogate β-risk for a matrix β = [β+1
It is worth noticing that Rφ(S  h  β) is expressed in the form of a sum of two terms: the second one
takes into account the labels of the data  while the ﬁrst one  the β-risk  focuses on the loss suffered by
h over X without explicitly needing the labels Y.
−yi
The empirical β-risk is a generalization of the empirical risk: setting βyi
i = 0)
for each instance  the second term vanishes and we retrieve the classical formulation of the empirical
risk. Additionally  as developed in Section 2.3  the introduction of β makes it possible to inject some
side-information about the labels. For this reason  we claim that the β-risk is suited to deal with
classiﬁcation in the context of weakly labeled data.
Let us now focus on the conditions allowing the empirical β-risk (i) to be a surrogate of the 0-1
loss-based empirical risk and (ii) to be sufﬁcient to learn with a weak supervision on the labels.
From (4)  we deduce:

i = 1 (and thus β

Rβ
φ(X   h) = Rφ(S  h  β) +

1
m

i yih(xi) ≥ R0/1(S  h) +
β-yi

1
m

β-yi
i yih(xi)

(5)

to force the following constraint:(cid:80)m
Unfortunately  the constraint(cid:80)m

where R0/1(S  h) the empirical risk related to the 0-1 loss and Eq. (5) is because bφFφ(x) ≥ F0/1(x)
(for any surrogate loss).
It is possible to ensure that the β-risk is both a convex upper-bound of the 0-1 loss based risk and a
φ(X   h) ≤ Rφ(S  h)) is
relaxation as tight as the traditional risk (i.e.  that we have R0/1(S  h) ≤ Rβ

i=1 β-yi
i yih(xi) = 0 still depends on the vector y of labels  which is
not always provided and most likely uncertain or inaccurate in a weakly labeled data setting. We will
show in Section 3 that this issue can be overcome by means of an iterative 2-step learning procedure 
that ﬁrst learns a classiﬁer minimizing the β-risk   possibly violating the constraint  and then learns a
new matrix β that fulﬁlls the constraint.

i yih(xi) = 0.

i=1 β-yi

m(cid:88)

i=1

m(cid:88)

i=1

4

2.3

Instantiating β for Different Weakly Supervised Settings

The β-risk can be used as the basis for handling different learning settings  including weakly labeled
learning. This can be achieved by ﬁxing the β values  choosing their initial values or putting a prior
on them. We have already seen that  fully supervised learning can be obtained by ﬁxing all β values
to 1 for the assigned class and to 0 for the opposite class. The current section provides guidance on
how β could be instantiated to handle various weakly labeled settings.
In a semi-supervised setting  as detailed in the experimental section  we propose to initialize the
β of unlabeled points to 0.5 and then to automatically reﬁne them in an iterative process. Going
further  and if we are ready to integrate spatial or topological information in the process  the β
values of each unlabeled point could be initialized using a density estimation procedure (e.g.  by
considering the label proportions of the k nearest labeled neighbors). In the context of multi-expert
learning  the experts’ votes for each instance i can simply be averaged to produce the βi values (or
their initialization  or a prior). The case of learning with label proportions is especially useful for
privacy-preserving data processing: the training points are grouped into bags and  for each bag  the
proportion of labels are given. One way of handling such supervision is to initialize  for each bag 
all the β with the same value that corresponds to the provided proportion of labels. Noise-tolerant
learning aims at learning in the presence of label noise  where labels are given but can be wrong. For
any point that can be possibly noisy  a direct approach is to use lower β values (instead of 1 in the
supervised case) and reﬁne them as in the semi-supervised setting. β can also be initialized using the
label proportion of the k nearest labeled example (as done in the experimental section). The case of
Multiple Instance Learning (MIL) is trickier: in a typical MIL setting  instances are grouped in bags
and the supervision is given as a single label per bag that is positive if the bag contains at least one
positive instance (negative bags contain only negative instances). A straightforward solution would
be to recast the MIL supervision as a “learning with label proportion” (e.g.  considering exactly one
positive instance in each bag). It is not fully satisfying and a more promising solution would be to
consider  within each bag  the set of β+1 variables and put a sparsity-inducing prior on them. This
approach would be a less-constrained version of the relaxation proposed in WellSVM [14] (where it
is supposed that there is exactly one positive instance per positive bag) and could be achieved by a l1
penalty or using a Dirichlet prior (with low α to promote sparsity).

3 An Iterative Algorithm for Weakly-labeled Learning

(cid:80)m

i=1 β-yi

As explained in Section 2  a sufﬁcient condition for guaranteeing that the β-risk is a convex
upper-bound of the 0-1 loss based risk and it is not worse than the traditional risk is to ﬁx
i yih(xi) = 0. However  the previous constraint depends on the labels. We overcome
this problem by (i) iteratively learning a classiﬁer minimizing the β-risk and most likely violating the
constraint and then (ii) learning a new matrix β that fulﬁlls it. The algorithm is generic. It can be used
in different weakly labeled settings and can be instantiated with different losses and regularizations 
as we will do in the next Section with SVMs.
As the process is iterative  let tβ be the estimation of β at iteration t. At each iteration  our algorithm
consists in two steps. We ﬁrst learn an hypothesis h for the following problem P1:

ht+1 = P1(X   tβ) = arg min

cRtβ

φ (X   h) + N (h)

h

which boils down to minimizing the N -regularized empirical surrogate β-risk over the training
sample X of size m  where N   for instance  can take the form of a L1 or a L2 norm.
Then  we ﬁnd the optimal β of the following problem P2 for the points of X :

t+1β = P2(X   ht+1) = arg min

Rβ
φ(X   ht+1)

β

m(cid:88)

i=1

s.t.

i (−yi ht+1(xi)) = 0
β-yi
i ≥ 0  β+1

i ≥ 0 ∀i = 1..m .

β-1
i + β+1

i = 1  β-1

For this step  a vector of labels is required. We choose to re-estimate it at each iteration according
to the current value of β: we affect to an instance the most probable label  i.e. the σ corresponding

5

to the biggest βσ. The matrix β has to be initialized at the beginning of the algorithm according to
the problem setting (see Section 2.3). While some stabilization criterion does not exceed a given
threshold   the two steps are repeated.

4 Soft-margin β-SVM

A major advantage of the empirical surrogate β-risk is that it can be plugged in numerous learning
settings without radically modifying the original formulations. As an example  in this section we
derive a new version of the Support Vector Machine problem  using the empirical surrogate β-risk  
that takes into account the knowledge provided for each training instance (through the matrix β).
The soft-margin β-SVM optimization problem is a direct generalization of a standard soft-margin
SVM and is deﬁned as follows:

(cid:1)

(cid:0)β-1

m(cid:88)
i ∀i = 1..m  σ ∈ {−1  1}

i ξ-1

i + β+1

i ξ+1

i

i=1

arg min

θ

(cid:107)θ(cid:107)2

2 + c

1
2

s.t. σ(θT µ(xi) + b) ≥ 1 − ξσ

i ≥ 0 ∀i = 1..m  σ ∈ {−1  1}
ξσ

where θ ∈ X(cid:48) is the vector deﬁning the margin hyperplane and b its offset  µ : X → X(cid:48) a mapping
function and c ∈ R a tuned hyper-parameter. In the rest of the paper  we will refer to K : X×X → R
as the kernel function corresponding to µ  i.e. K(xi  xj) = µ(xi)µ(xj).
The corresponding Lagrangian dual problem is given by (the complete derivation is provided in the
supplementary material):

max

α

− 1
2

m(cid:88)

(cid:88)

m(cid:88)

(cid:88)

i=1

σ∈

{-1 +1}

j=1

σ(cid:48)∈
{-1 +1}

ασ
i σασ

j σ(cid:48)K(xi  xj) +

m(cid:88)

(cid:88)

i=1

σ∈

{-1 +1}

ασ
i

s.t. 0 ≤ ασ

m(cid:88)

(cid:88)

i ∀i = 1..m  σ ∈ {−1  1}
i ≤ cβσ
i σ = 0 ∀i = 1..m  σ ∈ {−1  1}
ασ

i=1

σ∈

{-1 +1}

which is concave w.r.t. α as for the standard SVM.
The β-SVM formulation differs from the SVM one in two points: ﬁrst  the number of Lagrangian
multipliers is doubled  because we consider both positive and negative labels for each instance;
second  the upper-bounds for α are not the same for all instances but depend on the given matrix
β. Like the coefﬁcient c in the classical formulation of SVM  those upper-bounds play the role of
trade-off between under-ﬁtting and over-ﬁtting: the smaller they are  the more robust to outliers the
learner is but the less it adapts to the data. It is then logical that the upper-bound for an instance
i because it reﬂects the reliability on the label σ for that instance: if the label σ is
i depends on βσ
unlikely  its corresponding ασ
i will be constrained to be null (and its adversary will have more chance
to be selected as a support vector  as βσ
= 1). Also  those points for which no label is more
i → 0.5) will have less importance in the learning process compared to
probable than the other (βσ
those for which a label is almost certain. In order to fully exploit the advantages of our formulation  c
has to be ﬁnite and bigger than 0. As a matter of fact  when c → ∞ or c → 0  the constraints become
exactly those of the original formulation.

i + β − σ

i

5 Experimental Results

In the ﬁrst part of this section  we present some experimental results obtained by adapting the iterative
algorithm presented in Section 3 for semi-supervised learning and combining it with the previously
derived β-SVM . Note that some approaches based on SVMs have been already presented in the
literature to address the problem of semi-supervised learning. Among them  TransductiveSVM [5]

6

iteratively learns a separator with the labeled instances  classiﬁes a subset of the unlabeled instances
and adds it to the training set. On the other hand  WellSVM [14] combines the classical SVM with a
label generation strategy that allows one to learn the optimal separator  even when the training sample
is not completely labeled  by convexly relaxing the original Mixed-Integer Programming problem. In
[14]  WellSVM has been shown to be very effective and better than TransductiveSVM and the state of
the art. For this reason  we compare in this section β-SVM to WellSVM. In the second subsection  we
present some preliminary results in the noise-tolerant learning setting  showing how β-SVM behaves
when facing data with label noise.

5.1

Iterative β-SVM for semi-supervised learning

We compare our method’s performances to those of WellSVM  that has been proved  in [14]  to
performs in average better than the state of the art semi-supervised learning methods based on SVM
and the standard SVM as well. In a semi-supervised context  a set Xl of labeled instances of size ml
and a set Xu of unlabeled instances of size mu are provided. The matrix β is initialized as follows:

∀i = 1..ml and ∀σ in {−1  1}  0βσ

i = 1 if σ = yi  0 otherwise 

∀i = ml +1..mu and ∀σ in {−1  1}  0βσ

i = 0.5

and we learn an optimal separator:

ht+1 = P1(Xl ∪ Xu  tβ) = arg min

h

c1Rtβ

φ (Xl  h) + c2Rtβ

φ (Xu  h) + N (h).

Here c1 and c2 are balance constants between the labeled and unlabeled set: when the number of
unlabeled instances become greater than the number of labeled instances  we need to reduce the
importance of the unlabeled set in the learning procedure because there exists the risk that the labeled
set will be ignored. We consider the provided labels to be correct  so we keep the corresponding
lβ ﬁxed during the iterations of the algorithm and estimate uβ by optimizing P2(Xu  ht+1). The
iterative algorithm with β-SVM is implemented in Python using Cvxopt (for optimizing β-SVM )
and Cvxpy 2 with its Ecos solver [9].
For each dataset  we show in Figure 1 the accuracy of the two methods with an increasing proportion
of labeled data. The different approaches are compared on the same kernel  either the linear or the
gaussian  the one that gives higher overall accuracy. As a matter of fact  the choice of the kernel
depends on the geometry of the data  not on the learning method.
For each proportion of labeled data  we perform a 4-fold cross-validation and we show the average
accuracy over 10 iterations. Concerning the hyper-parameters of the different methods  we ﬁx c2
of β-SVM to c1
m   c1 of WellSVM to 1 as explained in [14] and all the other hyper-parameters
ml
(c1 for β-SVM and c2 for WellSVM) are tuned by cross-validation through grid search. As for
the stopping criteria  we ﬁx  of β-SVM to 10−5 + 10−3(cid:107)h(cid:107)F and  of WellSVM to 10−3 and the
maximal number of iterations to 20 for both methods. When using the gaussian kernel  the γ in
K(xi  xj) = exp(−(cid:107)xi − xj(cid:107)2
Our method performs better than WellSVM  with few exceptions  and is more efﬁcient in terms
of CPU time: for the Australian dataset  the biggest dataset in number of features and instances 
WellSVM is in average 30 times slower than our algorithm (without particular optimization efforts).

2/γ) is ﬁxed to the mean distance between instances.

5.2 Preliminary results under label-noise

We quickly tackle another setting of the weakly labeled data ﬁeld: the noise-tolerant learning  the
task of learning from data that have noisy or uncertain labels. It has been shown in [3] that SVM
learning is extremely sensitive to outliers  especially the ones lying next to the boundary. We study 
the sensitivity of β-SVM to label noise artiﬁcially introduced on the Ionosphere dataset. We consider
two initialization strategies for β: the standard on where βyi = 1 and β−yi = 0 and the 4-nn one
where βσ is set to the proportion of neighboring instances with label σ. In Figure 2  we draw the mean
accuracy over 4 repetitions w.r.t. an increasing percentage (as a proportion of the smallest dataset) of
two kinds of noise: the symmetric noise  introduced by swapping the labels of instances belonging
to different classes  and the asymmetric noise  introduced by gradually changing the labels of the

2http://cvxopt.org/ and http://www.cvxpy.org/

7

0.85

0.8

0.75

0.8

0.75

0.7

0.65

0.6

0.55

0.8

0.75

0.7

5

10

15

20

5

10

15

20

5

10

15

20

5

10

15

20

(a) Ionosphere 
gaussian kernel.

(b) Heart-statlog 

linear kernel.

(c) Liver 

linear kernel.

(d) Australian 
gaussian kernel.

0.75

0.7

0.65

0.65

0.6

0.65
0.6
0.55
0.5

5

10

15

20

5

10

15

20

5

10

15

20

(e) Pima 

linear kernel.

(f) Sonar 

linear kernel.

(g) Splice 

gaussian kernel.

WellSVM
betaSVM

Figure 1: Comparison of the mean accuracies of WellSVM and β-SVM versus the percentage of labeled data on

different UCI datasets.

0.85

0.8

0.75

0.85

0.8

0.75

standard

4-nn

10

20

30

40

50

10

20

30

40

50

(a) Symmetric Noise.

(b) Asymmetric Noise.

Figure 2: Comparison of the mean accuracy versus the percentage of noise of iterative β-SVM with different
initializations of β. The standard curve refers to the initialization of βyi = 1 and β−yi = 0 and the 4-nn to the

initialization of βσ to the proportion of neighboring instances with label σ.

instances of one class. These preliminary results are encouraging and show that locally estimating
the conditional class density to initialize the β matrix improves the robustness of our method to label
noise.

6 Conclusion

This paper focuses on the problem of learning from weakly labeled data. We introduced the β-
risk which generalizes the standard empirical risk while allowing the integration of weak supervision.
From the expression of the β-risk   we derived a generic algorithm for weakly labeled data and
specialized it in an SVM-like context. The resulting β-SVM algorithm has been applied in two
different weakly labeled settings  namely semi-supervised learning and learning with label noise 
showing the advantages of the approach.
The perspectives of this work are numerous and of two main kinds: covering new weakly labeled
settings and studying theoretical guarantees. As proposed in Section 2.3  the β-risk can be used in
various weakly labeled scenarios. This requires to use different strategies for the initialization and the
reﬁnement of β  and also to propose proper priors for these parameters. Generalizing the proposed
β-risk to a multi-class setting is a natural extension as β is already a matrix of class probabilities.
Another broad direction involves deriving robustness and convergence bounds for the algorithms
built on the β-risk .

7 Acknowledgments

We thank the reviewers for their valuable remarks. We also thank the ANR projects SOLSTICE
(ANR-13-BS02-01) and LIVES (ANR-15-CE230026-03).

8

References
[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning  2(4):343–370 

1988.

[2] S. Ben-David  D. Loker  N. Srebro  and K. Sridharan. Minimizing the misclassiﬁcation error
rate using a surrogate convex loss. In Proceedings of the 29th International Conference on
Machine Learning  ICML. icml.cc / Omnipress  2012.

[3] B. E. Boser  I. M. Guyon  and V. N. Vapnik. A training algorithm for optimal margin classiﬁers.
In Proceedings of the ﬁfth annual workshop on Computational learning theory  pages 144–152.
ACM  1992.

[4] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.
[5] L. Bruzzone  M. Chi  and M. Marconcini. A novel transductive svm for semisupervised
classiﬁcation of remote-sensing images. Geoscience and Remote Sensing  IEEE Transactions
on  44(11):3363–3373  2006.

[6] M. Collins  R. E. Schapire  and Y. Singer. Logistic regression  adaboost and bregman distances.

Machine Learning  48(1-3):253–285  2002.

[7] C. Cortes and V. Vapnik. Support-vector networks. Machine learning  20(3):273–297  1995.
[8] T. G. Dietterich  R. H. Lathrop  and T. Lozano-Pérez. Solving the multiple instance problem

with axis-parallel rectangles. Artiﬁcial intelligence  89(1):31–71  1997.

[9] A. Domahidi  E. Chu  and S. Boyd. Ecos: An socp solver for embedded systems. In Control

Conference (ECC)  2013 European  pages 3071–3076. IEEE  2013.

[10] Y. Freund  R. E. Schapire  et al. Experiments with a new boosting algorithm.

In ICML 

volume 96  pages 148–156  1996.

[11] T. Hastie  R. Tibshirani  and J. Friedman. Unsupervised learning. Springer  2009.
[12] A. Joulin and F. Bach. A convex relaxation for weakly supervised classiﬁers. arXiv preprint

arXiv:1206.6413  2012.

[13] M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms.
In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing  pages
459–468. ACM  1996.

[14] Y.-F. Li  I. W. Tsang  J. T. Kwok  and Z.-H. Zhou. Convex and scalable weakly labeled svms.

The Journal of Machine Learning Research  14(1):2151–2188  2013.

[15] M. Lichman. UCI machine learning repository  2013.
[16] N. Natarajan  I. S. Dhillon  P. K. Ravikumar  and A. Tewari. Learning with noisy labels. In

Advances in neural information processing systems  pages 1196–1204  2013.

[17] R. Nock and F. Nielsen. Bregman divergences and surrogates for learning. IEEE Transactions

on Pattern Analysis and Machine Intelligence  31(11):2048–2059  2009.

[18] G. Patrini  F. Nielsen  R. Nock  and M. Carioni. Loss factorization  weakly supervised learning

and label noise robustness. arXiv preprint arXiv:1602.02450  2016.

[19] G. Patrini  R. Nock  T. Caetano  and P. Rivera. (almost) no label no cry. In Advances in Neural

Information Processing Systems  pages 190–198  2014.

[20] L. Rosasco  E. De Vito  A. Caponnetto  M. Piana  and A. Verri. Are loss functions all the same?

Neural Computation  16(5):1063–1076  2004.

[21] V. S. Sheng  F. Provost  and P. G. Ipeirotis. Get another label? improving data quality and data
mining using multiple  noisy labelers. In Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining  pages 614–622. ACM  2008.

[22] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530  Computer Sciences 

University of Wisconsin-Madison  2005.

9

,Valentina Zantedeschi
Rémi Emonet
Marc Sebban