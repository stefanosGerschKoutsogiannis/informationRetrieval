2019,Stagewise Training Accelerates Convergence of Testing Error Over SGD,Stagewise training strategy is  widely used for learning neural networks  which runs a stochastic algorithm (e.g.  SGD) starting with a relatively large step size (aka learning rate) and geometrically decreasing the step size after a number of iterations.  It has been  observed  that the stagewise SGD has much faster convergence  than the vanilla SGD with a polynomially decaying step size in terms of both training error and testing error. {\it But how to explain this phenomenon has been largely ignored by existing studies.} This paper provides some theoretical evidence for explaining this faster convergence. In particular  we consider a stagewise training strategy for minimizing empirical risk that satisfies the  Polyak-\L ojasiewicz (PL) condition  which has been observed/proved for neural networks and also holds for a broad family of convex functions. For convex loss functions and two classes of ``nice-behaviored" non-convex objectives that are close to a convex function  we establish faster convergence of stagewise training than the vanilla SGD under the PL condition on both training error and testing error. Experiments on stagewise learning of deep residual networks exhibits that  it satisfies one type of non-convexity assumption and therefore can be explained by our theory.,Stagewise Training Accelerates Convergence of

Testing Error Over SGD

Zhuoning Yuan†  Yan Yan†  Rong Jin‡  Tianbao Yang†

†Department of Computer Science  The University of Iowa  Iowa City  IA 52242  USA

‡Machine Intelligence Technology  Alibaba Group  Bellevue  WA 98004  USA

{zhuoning-yuan  yan-yan-2  tianbao-yang}@uiowa.edu  jinrong.jr@alibaba-inc.com

Abstract

Stagewise training strategy is widely used for learning neural networks  which runs
a stochastic algorithm (e.g.  SGD) starting with a relatively large step size (aka
learning rate) and geometrically decreasing the step size after a number of iterations.
It has been observed that the stagewise SGD has much faster convergence than
the vanilla SGD with a polynomially decaying step size in terms of both training
error and testing error. But how to explain this phenomenon has been largely
ignored by existing studies. This paper provides some theoretical evidence for
explaining this faster convergence. In particular  we consider a stagewise training
strategy for minimizing empirical risk that satisﬁes the Polyak-Łojasiewicz (PL)
condition  which has been observed/proved for neural networks and also holds for
a broad family of convex functions. For convex loss functions and two classes
of “nice-behaved" non-convex objectives that are close to a convex function  we
establish faster convergence of stagewise training than the vanilla SGD under the
PL condition on both training error and testing error. Experiments on stagewise
learning of deep neural networks exhibits that it satisﬁes one type of non-convexity
assumption and therefore can be explained by our theory.

Introduction

1
In this paper  we consider learning a predictive model by using a stochastic algorithm to minimize
the expected risk via solving the following empirical risk problem:

min
w∈Ω

FS (w) :=

1
n

f (w  zi) 

(1)

n(cid:88)

i=1

where f (w  z) is a smooth loss function of the model w on the data z  Ω is a closed convex set 
and S = {z1  . . .   zn} denotes a set of n observed data points that are sampled from an underlying
distribution Pz with support on Z.
There are tremendous studies devoted to solving this empirical risk minimization (ERM) problem
in machine learning and related ﬁelds. Among all existing algorithms  stochastic gradient descent
(SGD) is probably the simplest and attracts most attention  which takes the following update:
(2)
where it ∈ {1  . . .   n} is randomly sampled  ΠΩ is the projection operator  and ηt is the step size
that is usually decreasing to 0. Convergence theories have been extensively studied for SGD with a
polynomially decaying step size (e.g.  1/t  1/
t) for an objective that satisﬁes various assumptions 
e.g.  convexity [25]  non-convexity [12]  strong convexity [15]  local strong convexity [27]  Polyak-
Łojasiewicz inequality [18]  Kurdyka-Łojasiewicz inequality [30]  etc. The list of papers about SGD
is so long that can not be exhausted here.

wt+1 = ΠΩ[wt − ηt∇f (wt  zit)] 

√

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2µ(FS (w) − min
w∈Ω

FS (w)) ≤ (cid:107)∇FS (w)(cid:107)2 

The success of deep learning is mostly driven by stochastic algorithms as simple as SGD running on
big data sets [20  17]. However  an interesting phenomenon that can be observed in practice for deep
learning is that no one is actually using the vanilla SGD with a polynomially decaying step size that is
well studied in theory for non-convex optimization [18  12  9]. Instead  a common trick used to speed
up the convergence of SGD is by using a stagewise step size strategy  i.e.  starting from a relatively
large step size and decreasing it geometrically after a number of iterations [20  17]. Not only the
convergence of training error is accelerated but also is the convergence of testing error. However 
there is still a lack of theory for explaining this phenomenon. Although a stagewise step size strategy
has been considered in some studies [16  30  18  19  7]  none of them explains the beneﬁt of stagewise
training used in practice compared with standard SGD with a decreasing step size  especially on the
convergence of testing error for non-convex problems.
Our Contributions. This paper aims to provide some theoretical evidence to show that an appropriate
stagewise training algorithm can have faster convergence than SGD with a polynomially deccaying
step size under some condition. In particular  we analyze a stagewise training algorithm under the
Polyak-Łojasiewicz condition [26]:

where µ is a constant. This property has been recently observed/proved for learning deep and shallow
neural networks [13  29  24  31  5]  and it also holds for a broad family of convex functions [30].
We will focus on the scenario that µ is a small positive value and n is large  which corresponds to
√
ill-conditioned big data problems and is indeed the case for many problems [13  5]. We compare
with two popular vanilla SGD variants with Θ(1/t) or Θ(1/
t) step size scheme for both the convex
loss and two classes of non-convex objectives that are close to a convex function. We show that the
considered stagewise training algorithm has a better dependence on µ than the vanilla SGD with
Θ(1/t) step size scheme for both the training error (under the same number of iterations) and the
testing error (under the same number of data and a less number of iterations)  while keeping the same
√
dependence on the number of data for the testing error bound. Additionally  it has faster convergence
and a smaller testing error bound than the vanilla SGD with Θ(1/
t) step size scheme for big data.
To be fair for comparison between two algorithms  we adopt a uniﬁed approach that considers
both the optimization error and the generalization error  which together with algorithm-independent
optimal empirical risk constitute the testing error. In addition  we use the same tool for analysis of
the generalization error - a key component in the testing error. The techniques for us to prove the
convergence of optimization error and testing error are simple and standard. It is of great interest to
us that simple analysis of the widely used learning strategy can possibly explain its greater success in
practice than using the standard SGD method with a polynomially decaying step size.
Besides theoretical contributions  the considered algorithm also has additional features that come with
theoretical guarantee for the considered non-convex problems and help improve the generalization
performance  including allowing for explicit algorithmic regularization at each stage  using an
averaged solution for restarting  and returning the last stagewise solution as the ﬁnal solution. It is
also notable that the widely used stagewise SGD is covered by the proposed framework. We refer to
the considered algorithm as stagewise regularized training algorithm or START.
Other closely related works. It is notable that many papers have proposed and analyzed determinis-
tic/stochastic optimization algorithms under the PL condition  e.g.  [18  23  28  2]. This list could be
long if we consider its equivalent condition in the convex case. However  none of them exhibits the
beneﬁt of stagewise learning strategy used in practice. One may also notice that linear convergence
for the optimization error was proved for a stochastic variance reduction gradient method [28]. Never-
theless  its uniform stability bound remains unclear for making a fair comparison with the considered
algorithms in this paper  and variance reduction method is not widely used for deep learning.
We also notice that some recent studies [21  32  5] have used other techniques (e.g.  data-dependent
bound  average stability  point-wise stability) to analyze the generalization error of a stochastic
algorithm. Nevertheless  we believe similar techniques can be also used for analyzing stagewise
learning algorithm  which is beyond the scope of this paper. The generalization error results in [32  5]
under PL condition are not directly comparable to ours because they have stronger assumptions (e.g. 
the global minimizer is unique for deriving uniform stability [5]  the condition number is small [32]).
Finally  it was brought to our attention when a preliminary version of this paper is done that an
independent work [11] observes a similar advantage of stagewise SGD over SGD with a polynomially

2

decaying step size lying at the better dependence on the condition number. However  they only
analyze the strongly convex quadratic case and the training error of ERM.
2 Preliminaries and Notations
Let A denote a randomized algorithm  which returns a randomized solution wS = A(S) based on
the given data set S. Denote by EA the expectation over the randomness in the algorithm and by ES
expectation over the randomness in the data set. When it is clear from the context  we will omit the
subscript S and A in the expectation notations. Let w∗
S ∈ arg minw∈Ω FS (w) denote an empirical
risk minimizer  and F (w) = Ez[f (w  z)] denote the true risk of w (also called testing error in this
paper). We use (cid:107) · (cid:107) to denote the Euclidean norm  and use [n] = {1  . . .   n}.
In order to analyze the testing error convergence of a random solution  we use the following decom-
position of testing error.

EA S [F (wS )] = ES [FS (w∗

S )] + ES EA[FS (wS ) − FS (w∗
S )]

+ EA S [F (wS ) − FS (wS)]
 

(cid:124)

(cid:123)(cid:122)

εopt

(cid:125)

(cid:124)

(cid:123)(cid:122)

εgen

(cid:125)

where εopt measures the optimization error  i.e.  the difference between empirical risk (or called
training error) of the returned solution wS and the optimal value of the empirical risk  and εgen
measures the generalization error  i.e.  the difference between the true risk of the returned solution
and the empirical risk of the returned solution. The difference EA S [F (wS )] − ES [FS (w∗
S )] is an
upper bound of the so-called excess risk bound in the literature  which is deﬁned as EA S [F (wS )]−
minw∈Ω F (w). It is notable that the ﬁrst term ES [FS (w∗
S )] in the above bound is independent
of the choice of randomized algorithms. Hence  in order to compare the performance of different
randomized algorithms  we can focus on analyzing εopt and εgen. For analyzing the generalization
error  we will leverage the uniform stability tool [4]. A randomized algorithm A is called -uniformly
stable if for all data sets S S(cid:48) ∈ Z n that differs at most one example the following holds:

εstab := sup

EA[f (A(S)  z) − f (A(S(cid:48))  z)] ≤ .

z

A well-known result is that if A is -uniformly stable  then its generalization error is bounded by  [4] 
i.e.  if A is -uniformly stable  we have εgen ≤ . In light of the above discussion  in order to compare
the convergence of testing error of different randomized algorithms  it sufﬁces to analyze their
convergence in terms of optimization error and their uniform stability. We would like to emphasize
that the PL condition is not used in our generalization error analysis by uniform stability  which
makes our comparison to the results in [14] fair.
A function f (w) is L-smooth if it is differentiable and its gradient is L-Lipchitz continuous  i.e. 
(cid:107)∇f (w) − ∇f (u)(cid:107) ≤ L(cid:107)w − u(cid:107) ∀w  u ∈ Ω. A function f (w) is G-Lipchitz continuous if
(cid:107)∇f (w)(cid:107) ≤ G ∀w ∈ Ω. We summarize the used assumptions below with some positive L  σ  G  µ
and 0.
Assumption 1. Assume that

(i) f (w  z) is L-smooth in terms of w ∈ Ω for every z ∈ Z.
(ii) f (w  z) is ﬁnite-valued and G-Lipchitz continuous in terms of w ∈ Ω for every z ∈ Z.
(iii) there exists σ such that Ei[(cid:107)∇f (w  z) − ∇FS (w)(cid:107)2] ≤ σ2 for w ∈ Ω.
(iv) FS (w) satisﬁes the PL condition for any S of size n  i.e.  there exists µ
S )) ≤ (cid:107)∇FS (w)(cid:107)2 ∀w ∈ Ω.

2µ(FS (w) − FS (w∗

(v) For an initial solution w0 ∈ Ω  there exists 0 such that FS (w0) − FS (w∗

S ) ≤ 0.

Remark 1: The second assumption is imposed for the analysis of uniform stability of a randomized
algorithm. W.o.l.g we assume |f (w  z)| ≤ 1 ∀w ∈ Ω. The third assumption is for the purpose of
analyzing optimization error. It is notable that σ2 ≤ 4G2. For simplicity  we assume the PL condition
of FS (w) holds uniformly over S. It is known that the PL condition is much weaker than strong
convexity. If FS is strongly convex  µ corresponds to the strong convexity parameter. In this paper 
we are particularly interested in the case when µ is small  i.e. the condition number L/µ is large.
Remark 2: It is worth mentioning that we do not assume the PL condition holds in the whole space
Rd. Hence  our analysis presented below can capture some cases that the PL condition only holds in
a local space Ω that contains a global minimum. For example  the recent papers by [10  29  1] shows
that the global minimum of learning a two-layer and deep overparameterized neural networks resides
in a ball centered around a random initial solution and the PL condition holds in the ball.

3

3 Review: SGD under PL Condition
In this section  we review the training error convergence and generalization error of SGD with a
decreasing step size for functions satisfying the PL condition in order to derive its testing error bound.
√
We will focus on SGD using the step size Θ(1/t) and brieﬂy mention the results corresponding to
t) at the end of this section. We would like to emphasize the results presented in this section
Θ(1/
are mainly from existing works [18  14]. The optimization error and the uniform stability of SGD
have been studied in these two papers separately. Since we are not aware of any studies that piece
them together  it is of our interest to summarize these results here for comparing with our new results
established later in this paper. Let us ﬁrst consider the optimization error convergence  which has
been analyzed in [18] and is summarized below.
Theorem 1. [18] Suppose Ω = Rd. Under Assumption 1 (i)  (iv) and Ei[(cid:107)∇f (w  zi)(cid:107)2] ≤ G2  by
setting ηt = 2t+1

2µ(t+1)2 in the update of SGD (2)  we have
E[FS (wT ) − FS (w∗
and by setting ηt = η  we have E[FS (wT ) − FS (w∗

S )] ≤ LG2
2T µ2  
S )] ≤ (1 − 2ηµ)T (FS (w0) − FS (w∗

(3)
S )) + ηLG2
4µ .

µ2

LG2 and T = LG2

Remark 3: In order to have an  optimization error  one can set T = LG2
2µ2 in the decreasing step
size setting. In the constant step size setting  one can set η = 2µ
4µ2 log(20/)  where
0 ≥ FS (w0) − FS (w∗
S) is the initial optimization error bound. [18] also mentioned a stagewise
step size strategy based on the second result above. By starting with η1 = 0µ
LG2 and running for
t1 = LG2 log 4
iterations  and restarting the second stage with η2 = η1/2 and t2 = 2t1  then after
2µ20
K = log(0/) stages  we have optimization error less than   and the total iteration complexity is
O( LG2 log 4
). We can see that the analysis of [18] cannot explain why stagewise optimization strategy
brings any improvement compared with SGD with a decreasing step size of O(1/t). No matter which
µ2 ).
step size strategy is used among the ones discussed above  the total iteration complexity is O( L
It is also interesting to know that the above convergence result does not require the convexity of
f (w  z). On the other hand  it is unclear how to directly analyze SGD with a polynomially decaying
step size for a convex loss to obtain a better convergence rate than (3).
The generalization error bound by uniform stability for both convex and non-convex losses have been
analyzed in [14]. We just need to plug the step size of SGD in Theorem 1 into their results (Theorem
3.7 and Theorem 3.8) to prove the uniform stability. For the sake of space  we summarize the uniform
stability results in Theorem 11 in the supplement. Combining the optimization error and uniform
stability  we obtain the convergence of testing error of SGD for smooth loss functions under the PL
condition. By optimizing the value of T in the bounds  we obtain the following testing error bound
dependent on n only.
If f (·  z) is convex for any z ∈ Z  with step size
Theorem 2. Suppose Assumption 1 holds.
ηt = 2t+1

4(L+2G2)µ iterations SGD returns a solution wT satisfying

2µ(t+1)2 and T = nLG2

EA S [F (wT )] ≤ ES [FS (w∗

S )] +

nµ
If f (·  z) is non-convex for any z ∈ Z  with step size ηt =
max{

} iterations SGD returns a solution wT satisfying

√
(n−1)LG
√
8µ3/4

(n−1)LG
2µe ˆG

√

nµ

 

2(L + 2G2)

(L + 2G2) log(T + 1)

+

2t+1

2µ(t+1)2 and T =

EA S [F (wT )] ≤ ES [FS (w∗

S )] + 2 min

(cid:26) √
(cid:112)(n − 1)µ5/4

2L1/2G3/2

 

(cid:112)

√

Le2 ˆGG
n − 1µ

.

(cid:27)

.

Remark 4: If the loss is convex  the excess risk bound is in the order of O( L log(nL/µ)
) by running
SGD with T = O(nL/µ) iterations. It notable that an O(1/n) excess risk bound is called the fast
µ > e2 ˆG (an interesting case 1)  the excess
rate in the literature. If the loss is non-convex and 2G/
nL/µ) iterations. When µ is
risk bound is in the order of O(

√
√
L√
nµ ) by running SGD with T = O(

√

nµ

1We can scale up L such that e2 ˆG is a small constant  which only scales up the bound by a constant factor.

4

Algorithm 1 START Algorithm: START(FS   w0  γ  K)
1: Input: w0  γ and K
2: for k = 1  . . .   K do
3:
4: wk = SGD(F γ
5: end for
6: Return: wK

2γ(cid:107)w − wk−1(cid:107)2

(w) = FS (w) + 1

  wk−1  ηk  Tk)

Let F γ

wk−1

wk−1

  w1  η  T )

Algorithm 2 SGD(F γ
w1
1: for t = 1  . . .   T do
Sample a random data zit ∈ S
2:
3: wt+1 = minw∈Ω ∇f (wt  zit)(cid:62)w +
2γ(cid:107)w − w1(cid:107)2
2η(cid:107)w − wt(cid:107)2 + 1
5: Output: (cid:98)wT = O(w1  . . .   wT +1)
4: end for

1

convergence rate for FS ((cid:98)wT ) − FS (w∗

very small  the convergence of testing error is very slow. In addition  the number of iterations is also
scaled by 1/µ for achieving a minimal excess risk bound.
√
Remark 5: Another possible choice of decreasing step size is O(1/

√
t)  which yields an O(1/

T )
S ) in the convex case [25] or for (cid:107)∇FS(wt)(cid:107)2 in the non-
√
convex case with a randomly sampled t [12]. In the latter case  it also implies a worse convergence
T )) for the optimization error FS (wt) − minw FS (w) under the PL condition 2.
√
rate of O(1/(µ
Regarding the uniform stability  the step size of O(1/
t) will also yield a worse growth rate in terms
√
of T [14]. For example  if the loss function is convex  the generalization error by uniform stability
scales as O(
nµ)  which is worse

than the above testing error bound (cid:101)O(1/(nµ)) for the big data setting µ ≥ Ω(1/n). Hence  below

√
T /n) and hence the testing error bound is in the order of O(1/

we will focus on the comparison with the theoretical results in Theorem 2.

wk−1

4 START for a Convex Function
First  let us present the algorithm that we intend to analyze in Algorithm 1. At the k-th stage  a
regularized funciton F γ
(w) is constructed that consists of the original objective FS (w) and a
2γ(cid:107)w − wk−1(cid:107)2. The reference point wk−1 is a returned solution from the
quadratic regularizer 1
previous stage  which is also used for an initial solution for the current stage. Adding the strongly
convex regularizer at each stage is not essential but could be helpful for reducing the generalization
error and is also important for one class of non-convex loss considered in next section. For each
regularized problem  SGD with a constant step size is employed for a number of iterations with an
appropriate returned solution. We will reveal the value of step size  the number of iterations and the
returned solution for each class of problems separately. Note that the widely used stagewise SGD
falls into the framework of START when γ = ∞ and O = (w1  . . .   wT +1) = wT +1.
In this section  we will analyze START algorithm for a convex function under the PL condition. We
would like to point out that similar algorithms have been proposed and analyzed in [16  30] for convex
problems. They focus on analyzing the convergence of optimization error for convex problems under
a quadratic growth condition or more general local error bound condition. In the following  we will
show that the PL condition implies a quadratic growth condition. Hence  their algorithms can be used
for optimizing FS as well enjoying a similar convergence rate in terms of optimization error. However 
there is still slight difference between the analyzed algorithm from their considered algorithms. In
2γ(cid:107)w − wk−1(cid:107)2 is absent in [16]  which corresponds to γ = ∞
particular  the regularization term 1
in our case. However  adding a small regularization (with not too large γ) can possibly help reduce
the generalization error. In addition  their initial step size is scaled by 1/µ. The initial step size of
our algorithm depends on the quality of initial solution that seems more natural and practical. A
similar regularization at each stage is also used in [30]. But their algorithm will suffer from a large
generalization error  which is due to the key difference between START and their algorithm (ASSG-r).
In particular  they use a geometrically decreasing the parameter γk starting from a relatively large
value in the order of O(1/(µ)) with a total iteration number T = O(1/(µ)). According to our
analysis of generalization error (see Theorem 4)  their algorithm has a generalization error in the
order of O(T /n) in contrast to log T /n of our algorithm.
Below  we summarize the convergence of optimization error in Theorem 3 and generalization error in
Theorem 4. We need the following lemma for the optimization error analysis.

2Note that here µ is not required for running the algorithm.

5

Lemma 1. If FS (w) satisﬁes the PL condition  then for any w ∈ Ω we have

(4)

(FS (w) − FS (w∗

S )) 

S(cid:107)2 ≤ 1
2µ
S is the closest optimal solution to w.

(cid:107)w − w∗

t denote the solution computed during the k-th stage at the t-th iteration.

where w∗
Remark 6: The above result does not require the convexity of FS. For a proof  please refer to [3  18].
Indeed  this error bound condition instead of the PL condition is enough to derive the results in
Section 4 and Section 5.
Below  we let wk
Theorem 3. (Optimization Error) Suppose Assumption 1 holds  and f (w  z) is a convex function of
w. Then by setting γ ≥ 1.5/µ and Tk = 9σ2
t+1/Tk  
where k = 0/2k  α ≤ min(1  3σ2
0L )  after K = log(0/) stages with a total iteration complexity of
O(L/(µ)) we have E[FS (wK) − FS (w∗
Remark 7: Compared to the result in Theorem 1  the convergence rate of START is faster by a factor
of O(1/µ). It is also notable that γ can be as large as ∞ in the convex case.
By showing supz EA[f (wK  z) − f (w(cid:48)
K  z)] ≤   we can show the generalization error is bounded
by   where wK is learned on a data set S and w(cid:48)
K is learned a different data set S(cid:48) that only differs
from S at most one example. Our analysis is closely following the route in [14]. The difference is
that we have to consider the difference on the reference points wk−1 of two copies of our algorithm
on two data sets S S(cid:48).
Theorem 4. (Uniform Stability) After K stages  START satisﬁes uniform stability with

2µkα   ηk = kα
S )] ≤ .

Tk+1) =(cid:80)Tk

3σ2   O(wk

1   . . .   wk

t=1 wk

(cid:40) 2γG2(cid:80)K
2G2(cid:80)K

εstab ≤

k=1(1−(

γ

ηk +γ )Tk )

n

k=1 ηkTk
n

≤ 2G2(cid:80)K

k=1 ηkTk
n

if γ < ∞
else

.

Put them Together. Finally  we have the following testing error bound of wK returned by START.
Theorem 5. (Testing Error) After K = log(0/) stages with a total number of iterations T = 18σ2
αµ .
The testing error of wK is bounded by

.

nµ

3G2 log(0/)

S )] +  +

EA S [F (wK)] ≤ E[FS (w∗
Remark 8: Let  = 1
nµ  the excess risk bound becomes O(log(nµ)/(nµ)) and the total iteration
complexity is T = O(nL). This improves the convergence of testing error of SGD stated in
Theorem 2 for the convex case when µ (cid:28) 1  which needs T = O(nL/µ) iterations and has a testing
error bound of O(L log(nL/µ)/(nµ)).
5 START for Non-Convex Functions
Next  we will establish faster convergence of START than SGD for “nice-behaved" non-convex
functions. In particular  we will consider two classes of non-convex functions that are are close to
a convex function  namely one-point weakly quasi-convex and weakly convex functions. We ﬁrst
introduce the deﬁnitions of these functions followed by some discussions.
Deﬁnition 1 (One-point Weakly Quasi-Convex). A non-convex function F is called one-point θ-
weakly quasi-convex for θ > 0 if there exists a global minimum w∗ such that
∇F (w)(cid:62)(w − w∗) ≥ θ(F (w) − F (w∗)) ∀w ∈ Ω.

(5)
Deﬁnition 2 (Weakly Convex). A non-convex function F is ρ-weakly convex for ρ > 0 if F (w) +
2(cid:107)w(cid:107)2 is convex.
ρ
It is interesting to connect one-point weakly quasi-convexity to one-point strong convexity that has
been considered for non-convex optimization  especially optimizing neural networks [24  19].
Deﬁnition 3 (One-point Strongly Convex). A non-convex function F is one-point strongly convex
with respect to a global minimum w∗ if there exists µ1 > 0 such that
∇F (w)(cid:62)(w − w∗) ≥ µ1(cid:107)w − w∗(cid:107)2.

The following lemma shows that one-point strong convexity implies both the PL condition and the
one-point weakly quasi-convexity.

6

Lemma 2. Suppose F is L-smooth and one-point strongly convex w.r.t w∗ with µ1 > 0 and
∇F (w∗) = 0  then

min{(cid:107)∇F (w)(cid:107)2 ∇F (w)(cid:62)(w − w∗)} ≥ 2µ1
L

(F (w) − F (w∗))

S. Then by setting γ ≥ 1.5/(θµ)  ηk = 2kθ
S )] ≤ . The total iteration complexity is O( 1

For “nice-behaved" one-point weakly quasi-convex function FS (w) that satisﬁes the PL condition 
we are interested in the case that θ is a constant close to or larger than 1. Note that a convex
function has θ = 1 and a strongly convex function has θ > 1. For the case of µ (cid:28) 1 in the PL
condition  this indicates that ∇F (w)(cid:62)(w− w∗) is larger than (cid:107)∇F (w)(cid:107)2  which further implies that
(cid:107)w− w∗(cid:107) ≥ (cid:107)∇F (w)(cid:107). Intuitively  this inequality (illustrated in Figure 2 in appendix) also connects
itself to the ﬂat minimum that has been observed in deep learning experiments [6]. For “nice-behaved"
weakly convex function  we are interested in the case that ρ ≤ µ/4 is close to zero. Weakly convex
functions with a small ρ have been considered in the literature of non-convex optimization [22]. In
both cases  we will establish faster convergence of optimization error and testing error of START.
Convergence of Optimization Error. The approach of optimization error analysis for the considered
non-convex functions is similar to that of convex functions. We also ﬁrst analyze the convergence
of SGD for each stage and then extend it to K stages for START. Due to limit of space  we only
summarize the ﬁnal convergence results here for the two classes of non-convex functions separately.
Theorem 6. Suppose FS (w) is one-point θ-weakly quasi-convex w.r.t w∗
S and (4) holds for the
4µkθ2 and O(wk
same w∗
Tk+1) = wk
where τ ∈ {1  . . .   Tk} is randomly sampled  after K = log(k/) stages we have E[FS (wK) −
FS (w∗
Theorem 7. Suppose Assumption 1 holds  and FS (w) is ρ-weakly convex with ρ ≤ µ/4. Then by
setting γ = 4/µ ≤ 1/ρ  ηk = kα
t+1/Tk 
t=1 wk
S )] ≤ . The
where α ≤ min(1  2σ2
total iteration complexity is O( 1
Remark 9: Several differences are noticeable between the two classes of non-convex functions:
(i) γ in the weakly quasi-convex case can be as large as ∞  in contrast it is required to be smaller
than 1/ρ in the weakly convex case; (ii) the returned solution by SGD at the end of each stage is a
randomly selected solution in the weakly quasi-convex case and is an averaged solution in the weakly
convex case. Finally  we note that the total iteration complexity for both cases is O(1/µ) under
θ ≈ 1 and ρ ≤ O(µ)  which is better than O(1/µ2) of SGD as in Theorem 1.
Generalization Error. The analysis of generalization error follows similarly to the non-convex case
in [14]. Please note that the quasi- and weak-convexity are not directly leveraged in the generalization
error analysis. The only thing that matters here is the value of step size. Hence  we present a uniﬁed
result for the two cases below.

0L )  and after K = log(0/) stages we have E[FS (wK) − FS (w∗

Tk+1) =(cid:80)Tk

4σ2 ≤ 1/L  Tk = 4σ2

µkα and O(wk

3G2   Tk = 9G2

1   . . .   wk

1   . . .   wk

θ2µ ).

αµ ).

τ

Theorem 8. Let SK−1 =(cid:80)K−1

µ ¯α ) and ηK ≤ c/(µTK)  where ¯α = θ2  c = 1.5/θ in
the one-point weakly quasi-convex case and ¯α = α  c = 1 in the weakly convex case. Then we have

k=1 Tk = O( 1

εstab ≤ SK−1
n

+

1 + µ/(Lc)

n − 1

(2G2c/µ)1/(1+Lc/µ)T
Lc/µ+1
k

Lc/µ

By putting the optimization error and generalization error together  we have the following testing
error bound.
Theorem 9. Under the same assumptions as in Theorem 6 or 7 and µ (cid:28) 1. After K = log(0/)
stages with a total number of iterations T = O( 1

¯αµ )  the testing error bound of wK is

EA S [F (wK)] ≤ E[FS (w∗

S )] +  + O(

1

).

n¯αµ

√
Remark 10: We are mostly interested in the case when θ is constant close to or larger than 1. By
1√
setting  = Θ(1/
n ¯αµ ) under the total iteration

complexity T = O((cid:112)n/(¯αµ)). This improves the testing error bound of SGD stated in Theorem 2

n¯αµ)  we have the excess risk bounded by O(

√
for the non-convex case when µ ≤ ¯α  which needs T = O(
√
error bound of O(1/(

nµ)).

n/µ) iterations and suffers a testing

7

Figure 1: From left to right: training  generalization and testing error  and verifying assumptions for
stagewise learning of ResNets.
Finally  it is worth noting that our analysis is applicable to an approximate optimal solution w∗
long as the inequality (4) and (5) hold for that particular w∗
assumptions in numerical experiments.

S as
S. This fact is helpful for us to verify the

√

6 Numerical Experiments
We focus experiments on non-convex deep learning  and include in the supplement some experimental
results of START for convex functions that satisfy the PL condition. The numerical experiments
mainly serve two purposes: (i) verifying that using different algorithmic choices in practice (e.g 
regularization  averaged solution) is consistent with the provided theory; (ii) verifying the assumptions
made for non-convex objectives in our analysis in order to support our theory.
We compare stagewise learning with different algorithmic choices against SGD using two polynomi-
ally decaying step sizes (i.e.  O(1/t) and O(1/
t)). For stagewise learning  we consider the widely
used version that corresponds to START with γ = ∞ and the returned solution at each stage being
the last solution  which is denoted as stagewise SGD (V1). We also implement other two variants of
START that solves a regularized function at each stage (corresponding to γ < ∞) and uses the last
solution or the averaged solution for the returned solution at each stage. We refer to these variants as
stagewise SGD (V2) and (V3)  respectively.
We conduct experiments on two datasets CIFAR-10  -100 using different neural network structures 
including residual nets and convolutional neural nets without skip connection. Two residual nets
namely ResNet20 and ResNet56 [17] are used for CIFAR-10 and CIFAR-100. For each network
structure  we use two types of activation functions  namely RELU and ELU (α = 1) [8]. ELU is
smooth that is consistent with our assumption. Although RELU is non-smooth  we would like to show
that the provided theory can also explain the good performance of stagewise SGD. For stagewise
SGD on CIFAR datasets  we use a similar stagewise step size strategy as in [17]  i.e.  the step size
is decreased by 10 times at 40k  60k iterations. For all algorithms  we select the best initial step
size from 10−3 ∼ 103 and the best regularization parameter 1/γ of stagewise SGD (V2  V3) from
0.0001 ∼ 0.1 by cross-validation based on performance on a validation data. Due to limit of space 
we only report the results for using ResNet56 and ELU and no weight decay  and results for other
settings are included in the supplement.
The training error  generalization error and testing error are shown in Figure 1. We can see that SGD
with a decreasing step size converges slowly  especially SGD with a step size proportional to 1/t. It is
because that the initial step size of SGD (c/t) is selected as a small value less than 1. We observe that
using a large step size cannot lead to convergence. In terms of different algorithmic choices of START 
we can see that using an explicit regularization as in V2  V3 can help reduce the generalization error
that is consistent with theory  but also slows down the training a little. Using an averaged solution as
the returned solution in V3 can further reduce the generalization error but also further slow downs the
training. Overall  stagwise SGD (V2) achieves the best tradeoff in training error convergence and
generalization error  which leads to the best testing error.
Finally  we verify the assumptions about the non-convexity made in Section 5. To this end  on a
selected wt we compute the value of θ  i.e.  the ratio of ∇FS (wt)(cid:62)(wt − w∗
S ) to FS (wt)− FS (w∗
S )

8

02040608010041907 94380.00.20.40.60.81.0%7 337747#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'02040608010041907 94380.000.050.100.150.200.25 -897 3077
9089077#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'02040608010041907 94380.00.20.40.60.81.0%08937747#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'05010015020041907 94380.00.51.01.52.02.53.03.50.112910.043150.012760.01331̄μ=0.00108#0809
&#909 30
23
0
; 2:02040608010041907 94380.00.20.40.60.81.0%7 337747#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'02040608010041907 94380.00.10.20.30.40.5 -897 3077
9089077#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'02040608010041907 94380.20.40.60.81.0%08937747#0809
&#$.9$.86799<$9 080$'$9 080$'$9 080$'05010015020041907 94380.00.51.01.52.02.53.03.50.045350.024510.006560.00710̄μ=0.00055#0809
&#909 30
23
0
; 2:S ) to 2(cid:107)wt − w∗

as in (5)  and the value of µ  i.e.  the ratio of FS (wt) − FS (w∗
S(cid:107)2 as in (4). For w∗
S 
we use the solution found by stagewise SGD (V1) after a large number of iterations (200k)  which
gives a small objective value close to zero. We select 200 points during the process of training by
stagewise SGD (V1) across all stages  and plot the curves for the values of θ and µ averaged over 5
trials in the most right panel of Figure 1. We can clearly see that our assumptions about µ (cid:28) 1 and
one-point weakly quasi-convexity with θ > 1 are satisﬁed. Hence  the provided theory for stagewise
learning is applicable. We also compute the minimum eigen-value of the Hessian on several selected
solutions by the Lanczos method to verify the assumption about weak convexity. The Hessian-vector
product is approximated by the ﬁnite-difference using gradients. The negative value of minimum
eigen-value (i.e.  ρ) is marked as (cid:5) in the same ﬁgure of θ  µ. We can see that the assumption about
ρ ≤ O(µ) seems not to hold for learning deep neural networks.

7 Conclusion

In this paper  we have analyzed the convergence of training error and testing error of a stagewise
regularized training algorithm for solving empirical risk minimization under the Polyak-Łojasiewicz
condition. We give the ﬁrst theory to justify why the widely used stagewise step size scheme gives
faster convergence than a polynomially decreasing step size. Our numerical experiments on deep
learning verify that one class of non-convexity assumption holds and hence the provided theory of
faster convergence applies. In particular  our generalization error bound analysis is based on the
nice-behaved properties of non-convex functions by using uniform stability  which is designed for
general non-convex problems. In the future  we consider to improve the generalization error bound
for the speciﬁc conditions.

References
[1] Z. Allen-Zhu  Y. Li  and Z. Song. A convergence theory for deep learning via over-

parameterization. CoRR  abs/1811.03962  2018.

[2] R. Bassily  M. Belkin  and S. Ma. On exponential convergence of sgd in non-convex over-

parametrized learning. CoRR  abs/1811.02564  2018.

[3] J. Bolte  T. P. Nguyen  J. Peypouquet  and B. Suter. From error bounds to the complexity of

ﬁrst-order descent methods for convex functions. CoRR  abs/1510.08234  2015.

[4] O. Bousquet and A. Elisseeff. Stability and generalization. J. Mach. Learn. Res.  2:499–526 

Mar. 2002.

[5] Z. Charles and D. Papailiopoulos. Stability and generalization of learning algorithms that
converge to global optima. In Proceedings of the 35th International Conference on Machine
Learning (ICML)  pages 745–754  2018.

[6] P. Chaudhari  A. Choromanska  S. Soatto  Y. LeCun  C. Baldassi  C. Borgs  J. T. Chayes 
L. Sagun  and R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. CoRR 
abs/1611.01838  2016.

[7] Z. Chen  Z. Yuan  J. Yi  B. Zhou  E. Chen  and T. Yang. Universal stagewise learning for
non-convex problems with convergence on averaged solutions. In International Conference on
Learning Representations  2019.

[8] D. Clevert  T. Unterthiner  and S. Hochreiter. Fast and accurate deep network learning by

exponential linear units (elus). CoRR  abs/1511.07289  2015.

[9] D. Davis and D. Drusvyatskiy. Stochastic subgradient method converges at the rate o(k−1/4)

on weakly convex functions. CoRR  /abs/1802.02988  2018.

[10] S. S. Du  X. Zhai  B. Poczos  and A. Singh. Gradient descent provably optimizes over-
parameterized neural networks. In International Conference on Learning Representations 
2019.

9

[11] R. Ge  S. M. Kakade  R. Kidambi  and P. Netrapalli. Rethinking learning rate schedules for
stochastic optimization. In Submitted to International Conference on Learning Representations 
2019. under review.

[12] S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex stochastic

programming. SIAM Journal on Optimization  23(4):2341–2368  2013.

[13] M. Hardt and T. Ma. Identity matters in deep learning. CoRR  abs/1611.04231  2016.

[14] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient
descent. In Proceedings of the 33nd International Conference on Machine Learning (ICML) 
pages 1225–1234  2016.

[15] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[16] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on
Learning Theory (COLT)  pages 421–436  2011.

[17] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR 

pages 770–778. IEEE Computer Society  2016.

[18] H. Karimi  J. Nutini  and M. W. Schmidt. Linear convergence of gradient and proximal-
gradient methods under the polyak-łojasiewicz condition. In Machine Learning and Knowledge
Discovery in Databases - European Conference (ECML-PKDD)  pages 795–811  2016.

[19] B. Kleinberg  Y. Li  and Y. Yuan. An alternative view: When does SGD escape local minima?
In Proceedings of the 35th International Conference on Machine Learning  pages 2698–2707 
2018.

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in Neural Information Processing Systems (NIPS)  pages 1106–
1114  2012.

[21] I. Kuzborskij and C. H. Lampert. Data-dependent stability of stochastic gradient descent. In
Proceedings of the 35nd International Conference on Machine Learning (ICML)  volume 80 of
JMLR Workshop and Conference Proceedings  pages 2820–2829. JMLR.org  2018.

[22] G. Lan and Y. Yang. Accelerated stochastic algorithms for nonconvex ﬁnite-sum and multi-block

optimization. CoRR  abs/1805.05411  2018.

[23] L. Lei  C. Ju  J. Chen  and M. I. Jordan. Non-convex ﬁnite-sum optimization via SCSG methods.

In Advances in Neural Information Processing Systems 30 (NIPS)  pages 2345–2355  2017.

[24] Y. Li and Y. Yuan. Convergence analysis of two-layer neural networks with relu activation. In

Advances in Neural Information Processing Systems 30 (NIPS  pages 597–607  2017.

[25] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19:1574–1609  2009.

[26] B. T. Polyak. Gradient methods for minimizing functionals. Zh. Vychisl. Mat. Mat. Fiz. 

3:4:864?878  1963.

[27] C. Qu  H. Xu  and C. Ong. Fast rate analysis of some stochastic optimization algorithms. In M. F.
Balcan and K. Q. Weinberger  editors  Proceedings of The 33rd International Conference on
Machine Learning  volume 48 of Proceedings of Machine Learning Research  pages 662–670 
New York  New York  USA  20–22 Jun 2016. PMLR.

[28] S. J. Reddi  A. Hefny  S. Sra  B. Poczos  and A. Smola. Stochastic variance reduction for
nonconvex optimization. In Proceedings of The 33rd International Conference on Machine
Learning (ICML)  volume 48  pages 314–323  2016.

[29] B. Xie  Y. Liang  and L. Song. Diversity leads to generalization in neural networks. CoRR 

abs/1611.03131  2016.

10

[30] Y. Xu  Q. Lin  and T. Yang. Stochastic convex optimization: Faster local growth implies faster
global convergence. In Proceedings of the 34th International Conference on Machine Learning
(ICML)  pages 3821 – 3830  2017.

[31] Y. Zhou and Y. Liang. Characterization of gradient dominance and regularity conditions for

neural networks. CoRR  abs/1710.06910  2017.

[32] Y. Zhou  Y. Liang  and H. Zhang. Generalization error bounds with probabilistic guarantee for

SGD in nonconvex optimization. CoRR  abs/1802.06903  2018.

11

,Tian Gao
Qiang Ji
Mikhail Yurochkin
XuanLong Nguyen
nikolaos Vasiloglou
Zhuoning Yuan
Yan Yan
Rong Jin
Tianbao Yang