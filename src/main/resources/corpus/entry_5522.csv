2019,The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic,The emergence of XNOR networks seek to reduce the model size and computational cost of neural networks for their deployment on specialized hardware requiring real-time processes with limited hardware resources. In XNOR networks  both weights and activations are binary  bringing great benefits to specialized hardware by replacing expensive multiplications with simple XNOR operations. Although XNOR convolutional and fully-connected neural networks have been successfully developed during the past few years  there is no XNOR network implementing commonly-used variants of recurrent neural networks such as long short-term memories (LSTMs). The main computational core of LSTMs involves vector-matrix multiplications followed by a set of non-linear functions and element-wise multiplications to obtain the gate activations and state vectors  respectively. Several previous attempts on quantization of LSTMs only focused on quantization of the vector-matrix multiplications in LSTMs while retaining the element-wise multiplications in full precision. In this paper  we propose a method that converts all the multiplications in LSTMs to XNOR operations using stochastic computing. To this end  we introduce a weighted finite-state machine and its synthesis method to approximate the non-linear functions used in LSTMs on stochastic bit streams. Experimental results show that the proposed XNOR LSTMs reduce the computational complexity of their quantized counterparts by a factor of 86x without any sacrifice on latency while achieving a better accuracy across various temporal tasks.,The Synthesis of XNOR Recurrent Neural Networks

with Stochastic Logic

Department of Electrical and Computer Engineering  McGill University  Montreal  Canada

{arash.ardakani  zhengyun.ji  amir.ardakani}@mail.mcgill.ca

Arash Ardakani  Zhengyun Ji  Amir Ardakani  Warren J. Gross

warren.gross@mcgill.ca

Abstract

The emergence of XNOR networks seek to reduce the model size and compu-
tational cost of neural networks for their deployment on specialized hardware
requiring real-time processes with limited hardware resources. In XNOR networks 
both weights and activations are binary  bringing great beneﬁts to specialized
hardware by replacing expensive multiplications with simple XNOR operations.
Although XNOR convolutional and fully-connected neural networks have been
successfully developed during the past few years  there is no XNOR network
implementing commonly-used variants of recurrent neural networks such as long
short-term memories (LSTMs). The main computational core of LSTMs involves
vector-matrix multiplications followed by a set of non-linear functions and element-
wise multiplications to obtain the gate activations and state vectors  respectively.
Several previous attempts on quantization of LSTMs only focused on quantization
of the vector-matrix multiplications in LSTMs while retaining the element-wise
multiplications in full precision. In this paper  we propose a method that converts
all the multiplications in LSTMs to XNOR operations using stochastic computing.
To this end  we introduce a weighted ﬁnite-state machine and its synthesis method
to approximate the non-linear functions used in LSTMs on stochastic bit streams.
Experimental results show that the proposed XNOR LSTMs reduce the compu-
tational complexity of their quantized counterparts by a factor of 86× without
any sacriﬁce on latency while achieving a better accuracy across various temporal
tasks.

1 Introduction

Recurrent neural networks (RNNs) have exhibited state-of-the-art performance across different
temporal tasks that require processing variable-length sequences such as image captioning [1]  speech
recognition [2] and natural language processing [3]. Despite the remarkable success of RNNs on a
wide range of complex sequential problems  they suffer from the exploding gradient problem that
occurs when learning long-term dependencies [4  5]. Therefore  various RNN architectures such
as long short-term memories (LSTMs) [6] and gated recurrent units (GRUs) [7] have emerged to
mitigate the exploding gradient problem. Due to the prevalent use of LSTMs in both academia and
industry  we mainly focus on the LSTM architecture in this work. The recurrent transition in LSTM is
performed in two stages: the ﬁrst stage performing gate computations and the second one performing
state computations. The gate computations are described as

ft = σ (Wf hht−1 + Wf xxt + bf )   it = σ(Wihht−1 + Wixxt + bi) 
ot = σ(Wohht−1 + Woxxt + bo)  gt = tanh(Wghht−1 + Wgxxt + bg) 

(1)
where {Wf h  Wih  Woh  Wgh} ∈ Rdh×dh  {Wf x  Wix  Wox  Wgx} ∈ Rdx×dh and {bf   bi  bo 
bg} ∈ Rdh denote the recurrent weights and bias. The input vector x ∈ Rdx denotes input temporal

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

features whereas the hidden state h ∈ Rdh retains the temporal state of the network. The logistic
sigmoid and hyperbolic tangent functions are denoted as σ and tanh  respectively. The updates of
the LSTM parameters are regulated through a set of gates: ft  it  ot and gt. The state computations
are then performed as

ct = ft ⊗ ct−1 + it ⊗ gt  ht = ot ⊗ tanh(ct) 

(2)

where the parameter c ∈ Rdh is the cell state. The operator ⊗ denotes the Hadamard product.
The ﬁrst computational stage of LSTM is structurally similar to a fully-connected layer as it only
involves several vector-matrix multiplications. Therefore  LSTMs are memory intensive similar to
fully-connected layers [8]. LSTMs are also computationally intensive due to their recursive nature
[9]. These limitations make LSTM models difﬁcult to deploy on specialized hardware requiring
real-time processes with inferior hardware resources and power budget. Several techniques have
been introduced in literature to alleviate the computational complexity and memory footprint of
neural networks such as low-rank approximation [10]  weight/activation pruning [11  12  13  14] and
quantization [15  16  17]. Among these solutions  quantization methods speciﬁcally binarization
methods bring signiﬁcant beneﬁts to dedicated hardware since they reduce the required memory
footprint and implementation cost by constrainting both weights and activations to only two values
(i.e.  -1 or 1) and replacing multiplications with simple XNOR operations  respectively [17]. As a
result  several attempts were reported in literature to binarize LSTM models during the past few years
[18  19  20]. However  all the existing methods only focused on the gate computations of LSTMs
by binarizing either the weights or both the weights and the hidden vector h while retaining the
state computations in full precision (FP). Although the recurrent computations of LSTM models
are dominated by the gate computations  using full-precision multipliers are inevitable for the state
computations when designing dedicated hardware for LSTM models  making the existing binarized
LSTM models unsuitable for embedded systems with limited hardware resources and tight power
budget. It is worth mentioning that a full-precision multiplier requires 200× more Xilinx FPGA slices
than an XNOR gate [17]. Therefore  an XNOR-LSTM model that can perform the multiplications of
both the gate and the state computations using XNOR operations is missing in literature.
In this paper  we ﬁrst extend an existing LSTM model with binary weights to binarize the hidden
state vector h. In this way  the multiplications of the gate computations can be performed using
XNOR operations. We then propose a method to binarize the state computations using stochastic
computing (SC) [21]. More precisely  we show that the binarized weights and the hidden state vector
h can be represented as stochastic bit streams  allowing us to perform the gate computations using
stochastic logic and to implement the non-linear activation functions (i.e.  the sigmoid and hyperbolic
tangent functions) using ﬁnite-state machines (FSMs). We also introduce a new FSM topology and
its synthesis method to accurately approximate the nonlinear functions of LSTM. We show that the
proposed FSM outputs a binary stream that its expected value is an approximation to the nonlinear
activation functions of LSTMs. Ultimately  we use the binary streams generated by the FSMs to
replace the full-precision multipliers required for the state computations with XNOR gates  forming
an XNOR-LSTM model.

2 Related Work
In the binarization process  the full-precision weight matrix W ∈ RdI×dJ is estimated using a binary
weight matrix Wb ∈ {−1  1}dI×dJ and a scaling factor α ∈ R+ such that W ≈ αWb. In [15] 
the sign function was used as the transformation function to obtain the binary weight matrix (i.e. 
Wb = sign(W)) while using a ﬁxed scaling factor for all the weights. Lin et al. [22] introduced
a ternarization method to reduce the accuracy loss of the binarization process by clamping values
hesitating to be either 1 or -1 to zero. Some methods [23  24] were then proposed to improve upon
the ternarization method by learning the scaling factor α. Zhou et al. [25] proposed a method that
quantizes weights  activations and gradients of neural networks using different bitwidths. Rastegari
et al. [16] and Lin et al. [17] proposed binary neural networks (BNNs) in which both weights and
activations of convolutional neural networks (CNNs) are represented in binary. Despite the great
performance of the aforementioned works in quantization of CNNs  they fail to work well on RNNs
[26]. As a result  recent studies mainly attempted to quantize RNNs in particular LSTMs.
Hou et al. [19] introduced the loss-aware binarization method (LAB) that uses the proximal Newton
algorithm to minimize the loss w.r.t the binarizied weights. The LAB method was further extended

2

a : 1 0 0 0 1 0 0 0 (2/8)

b : 0 0 1 0 1 1 0 1 (4/8)

y : 0 0 0 0 1 0 0 0 (1/8)

a : 0 0 0 0 1 0 1 0 (−4/8)

b : 1 0 1 0 1 0 0 1 (0/8)

(a)

(b)

y : 0 1 0 1 1 1 0 0 (0/8)

Figure 1: Stochastic multiplications using bit-wise operations in (a) unipolar and (b) bipolar formats.

in [27] to support different bitwidths for the weights. Both of these methods were tested on LSTM
models performing character-level language modeling experiments. Xu et al. [18] presented the
alternating multi-bit quantization (AMQ) method that uses a binary search tree to obtain optimal
quantization coefﬁcients for LSTM models. Wang et al. [26] proposed a ternary RNN  called
HitNet  which exploits a hybrid of different quantization methods to quantize the weights and the
hidden state vector based on their statistical characteristics. Both HitNet and alternating multi-
bit quantization method were tested on RNNs performing word-language modeling experiments.
Recently  Ardakani et al. [20] leveraged batch normalization in both the input-to-hidden and the
hidden-to-hidden transformations of LSTMs to binarize/ternarize the recurrent weights. This method
was tested on various sequential tasks  such as sequence classiﬁcation  language modeling  and
reading comprehension. While all the aforementioned approaches successfully managed to quantize
the weights and the hidden state vector (i.e.  the gate computations) of LSTM models  the state
computations were retained in full precision. More precisely  no attempt was reported to binarize
both the gate and state computations of LSTMs. Motivated by this observation  we propose the
ﬁrst XNOR-LSTM model in literature  performing all the recurrent multiplications with XNOR
operations.

3 Preliminaries

3.1 Stochastic Computing

Stochastic computing is a well-known technique to obtain ultra low-cost hardware implementations
for various applications [28]. In SC  continuous values are represented as sequences of random bits 
allowing complex computations to be computed by simple bit-wise operations on the bit streams.
More precisely  the statistics of the bits determine the information content of the stream. For example 
a real number a ∈ [0  1] is represented as the sequence a ∈ {0  1}l in SC’s unipolar format such that
(3)
where E[a] and l denote the expected value of the Bernoulli random vector a and the length of the
sequence  respectively. Another well-known SC’s representation format is the bipolar format where
a ∈ [−1  1] is represented as

E[a] = a 

(4)
To represent any real number using these two formats  we need to scale it down to ﬁt within the
appropriate interval (i.e.  either [0  1] or [−1  1]). It is worth mentioning that the stochastic stream a
is generated using a linear feedback shift register (LFSR) and a comparator in custom hardware [28] 
referred to as stochastic number generator (SNG).

E[a] = (a + 1)/2.

3.1.1 Multiplication and Addition in SC
Multiplication of two stochastic streams of a and b in the unipolar format is performed as

y = a · b 

(5)
where “·” denotes the bit-wise AND operation  E[y] = E[a] × E[b] if and only if the input stochastic
streams (i.e  a and b) are independent. However  this multiplication in the bipolar format is computed
using an XNOR gate as

(6)
where “(cid:12)” denotes the bit-wise XNOR operation. Similarly  if the input sequences are independent 
we have

y = a (cid:12) b 

2 × E[y] − 1 = (2 × E[a] − 1) × (2 × E[b] − 1).

(7)

3

yj = 0

yj = 1

aj = 1

aj = 1

aj = 1

aj = 1

aj = 1

aj = 0

C0

C1

Cn/2−1

Cn/2

Cn−2

Cn−1

aj = 1

aj = 0

aj = 0

aj = 0

aj = 0

aj = 0

Figure 2: State transition diagram of the FSM implementing tanh where aj and yj denote the jth entry
of the input stream a ∈ {0  1}l and the output stream y ∈ 0  1l for j ∈ {1  2  . . .   l}  respectively.

Figure 1 shows an example of a multiplication in stochastic domain using both the unipolar and
bipolar formats. Since stochastic numbers are represented as probabilities falling into the interval
of [0  1] in the unipolar format  additions in SC are performed using the scaled adder that ﬁts the
result of the addition into the [0  1] interval [28]. Additions of two stochastic streams of a and b is
computed by

(8)
where the signal c is a stochastic stream with a probability of 0.5 (i.e.  E[c] = 0.5). The scaled
adder is implemented using a multiplexer in which the stream c is used as its selector signal. The
aforementioned discussion on the stochastic addition also holds true for the bipolar format.

y = a · c + b · (1 − c) 

3.1.2 FSM-Based Functions in SC

In SC  non-linear functions such as the hyperbolic tangent  sigmoid and exponentiation functions can
be performed on stochastic bit streams using FSMs [29]. An FSM in SC can be viewed as a saturating
counter that does not increment beyond its maximum value or decrement below its minimum value.
For example  the FSM-based transfer function “Stanh” that approximates the hyperbolic tangent
function is constructed such that

(cid:16) na

(cid:17) ≈ 2 × E[Stanh(n  a)] − 1 

tanh

2

(9)

where n denotes the number of states in the FSM. Figure 2 illustrates the state transition of the
FSM-based transfer function approximating the hyperbolic tangent function when using a set of
states C0 → Cn−1. Since the sigmoid function is obtained from the hyperbolic tangent functions  the
transfer function Stanh is also used to approximate the sigmoid function  that is 

σ(na) =

1 + tanh

2

2

≈ E[Stanh(n  a)].

(10)

(cid:16) na

(cid:17)

m(cid:88)

j=1

4

Integral Stochastic Computing

3.2
In integral stochastic computing (ISC)  a real value s ∈ [0  m] in the unipolar format (or s ∈ [−m  m]
in the bipolar format) is represented as a sequence of integer numbers [30]. In this way  each element
of the sequence s ∈ {0  1  . . .   m}l in the unipolar format (or s ∈ {−m −m + 1  . . .   m}l in the
bipolar format) is represented using the two’s complement format  where l denotes the length of the
stochastic stream. The integral stochastic stream s is obtained by the element-wise additions of m
binary stochastic streams as follows

s =

aj 

(11)

where the expected value of each binary stochastic stream  denoted as aj  is equal to s/m. With this
deﬁnition  we have

E[y] =

E[aj] =

s
m

= s.

(12)

m(cid:88)

j=1

m(cid:88)

j=1

For instance  the element-wise addition of two binary stochastic streams  {0  1  1  1  1  0  1  1} and
{0  1  1  1  0  1  1  1}  each representing the real value of 0.75  results in the integral stochastic stream

of {0  2  2  2  1  1  2  2} representing the real value of 1.5 for m = 2 and l = 8. We hereafter refer to
the integral stochastic number generator function as ISNG.
Additions in ISC are performed using the conventional binary-radix adders  retaining all the input
information as opposed to the scaled adders that decrease the precision of the output streams [30].
Multiplications are also implemented using the binary-radix multiplier in ISC. The main advantage of
ISC lies in its FSM-based functions that take integral stochastic streams and output binary stochastic
streams  allowing the rest of computations to be performed with simple bit-wise operations in binary
SC. The approximate transfer function of hyperbolic tangent and sigmoid  which is referred to as
IStanh  is deﬁned as

(cid:16) ns

(cid:17) ≈ 2 × E[IStanh(n × m  s)] − 1 

(13)

σ(ns) ≈ E[IStanh(n × m  s)].

(14)
The IStanh outputs zero when the state counter is less than n × m/2  otherwise it outputs one.
Considering kj as an entry of the state counter vector k ∈ {0  1  . . .   m}l and yj as an entry of the
IStanh’s output vector y ∈ {0  1}l   we have

tanh

2

(cid:26)0  kj < (n × m/2)

yj =

1  otherwise

(15)
where j ∈ {1  . . .   l}. As opposed to the FSM-based functions in binary SC in which the state counter
is incremented or decremented only by 1  the state counter of the FSM-based functions in ISC is
increased or decreased according to the integer input value. In fact  the maximum possible transition
at each time slot is equal to m in ISC. Moreover  the FSM-based functions in ISC require m times
more states than the ones in SC. Despite the complexity of the FSM-based functions in ISC  they are
more accurate than their counterparts in SC [30].

 

4 Synthesis of XNOR RNNs

4.1 Binarization of the Hidden State

In [20]  the recurrent weights of LSTMs and GRUs were binarized using batch normalization in
both the input-to-hidden and hidden-to-hidden transformations. More speciﬁcally  the recurrent
computations of gate ft is performed as

ft = σ

BN(Wb

f hht−1; φf h  0) + BN(Wb

f xxt; φf x  0) + bf

 

(16)

(cid:16)

(cid:17)

where Wb
as follows

f h and Wb

f x are the binarized weights obtained by sampling from the Bernoulli distribution

Wb = 2 × Bernoulli(P (W = 1)) − 1.
(cid:112)V(u) + 
BN also denotes the batch normalization transfer function such that
BN(u; φ  γ) = γ + φ ⊗ u − E(u)

 

(17)

(18)

(19)

where u is the unnormalized vector and V(u) denotes its variance. The model parameters γ and φ
determine the mean and variance of the normalized vector. The rest of the gate computations (i.e. 
it  ot and gt) are binarized in a similar fashion. So far  we have reviewed the method introduced in
[20] to binarize the recurrent weights. We now extend this method to also binarize the hidden state
vector h. To this end  we use the sign function. However  the derivative of the sign function is zero
during backpropagation  making the gradients of the loss w.r.t the parameters before the quantization
function to be zero [17]. To address this issue  we estimate the derivative of the sign function as

(cid:26)1 

∂ sign(h)

∂h

≈

|h| < 1

0  otherwise  

similar to [17]. In this way  the gradient’s information are preserved. Training LSTMs with this
method allows us to perform the matrix-vector multiplications of the gate computations using XNOR
operations. We use the extended LSTM (ELSTM) with binary weights and the state hidden vector as
our baseline for the rest of this paper.

5

4.2 Stochastic Representation of Gate Computations

Let us only consider the recurrent computations for a single neuron of a baseline’s gate as

dh(cid:88)

dx(cid:88)

y = αh

h (cid:12) hj + αx
wj

x × xj + b 
wi

(20)

j=1

j=1

where wh  wx  h and x are the element entries of the hidden-to-hidden weight vector wh ∈ {−1  1}dh 
the input-to-hidden weight vector wx ∈ {−1  1}dx  the hidden vector h ∈ {−1  1}dh and the input
vector x ∈ Rdx  respectively. The bias is denoted as b ∈ R. The parameters αh ∈ R and αx ∈ R
denote the scaling factors dictated by the binarization process. Note that the batch normalization
processes are considered in the parameters αh  αx and b in Eq. (20). In most of the temporal
tasks  the input vector x is one-hot encoded  replacing the vector-vector multiplication of wxx with a
simple indexing operation implemented by a lookup table. As such  let us merge this vector-vector
multiplication into the bias as follows

dh(cid:88)

j=1

y = αh

h (cid:12) hj + b.
wj

(21)

Considering the linear property of the expected value operator  we can rewrite Eq. (21) as follows

αhdh

h (cid:12) hj
wj
dh

+ b = αhdhE[wh (cid:12) h] + b = E[αhdh(wh (cid:12) h) + b] = E[y].

(22)

dh(cid:88)

j=1

y =

So far  we have represented the output y ∈ R as a sequence of real numbers (i.e.  y ∈ Rdh) where
each entry of the vector y is either αhdh + b or −αhdh + b. Passing the vector y into the ISNG
function generates the integral stochastic stream yISC such that

y = E[y] = E[ISNG(y)] = E[yISC].

(23)
Note that the integer range of the integral stream is equal to (cid:100)|αhdh| + |b|(cid:101). For instance 
considering αh = 0.2  dh = 10  b = 0.5 and wh (cid:12) h = {1 −1  1  1  1 −1  1 −1 −1  1} 
yISC = {3 −2  2  3  2 −1  3 −1 −2  2} is an integral stochastic representation of y =
{2.5 −1.5  2.5  2.5  2.5 −1.5  2.5 −1.5 −1.5  2.5}  resulting in y = 0.9. To guarantee the stochas-
ticity of the sequence yISC  we can permute the reading addresses of the memories storing the
weights and the hidden state vector h. Note that Eq. (20) with the input vector x that is not one-hot
encoded can also be represented as a stochastic stream by equalizing vector lengths of dx and dh.
Assuming that dh > dx and dx is a multiple of dh  this can simply obtained by repeating the input
vector (i.e.  x) dh/dx times as the mean of the repeated vector remains unchanged. Of course dh is a
design parameter and can take any arbitrary value.

4.3 Weighted FSM-Based Function

So far  we have shown that the output of each neuron can be represented as an integral stochastic
stream  allowing us to perform the nonlinear functions using the FSM-based IStanh function. How-
ever  our experiments show that the IStanh function fails to resemble the hyperbolic tangent and
sigmoid functions (see Figures 3(a) and 3(b)). We attribute this problem to the even distribution of
positive and negative integer elements in the vector yISC for both positive and negative values of y.
More precisely  the vector yISC contains almost the same number of positive and negative integer
entries since the expected value (i.e  the mean) of the vector wh (cid:12) h is a small number. However 
integral stochastic streams representing positive and negative real values are more likely to have more
positive and negative entries  respectively. To address this issue  we propose a weighted FSM-based
function  referred to as WIStanh  in which each state is associated with a weight. In the weighted
FSM-based function  we use the same FSM that is used in the IStanh function. However  the output
is determined by sampling from the weights associated to the states as follows

yFSM
j = Bernoulli(

wkj + 1

2

) 

6

(24)

M
S
F
y

1

0.5

0

−0.5

−1

M
S
F
y

1

0.8

0.6

0.4

0.2

0

2 × IStanh(128 y) − 1

tanh(y)

M
S
F
y

1

0.5

0

−0.5

−1

IStanh(128 y)

σ(y)

M
S
F
y

1

0.8

0.6

0.4

0.2

0

2 × WIStanh(128 y) − 1

tanh(y)

WIStanh(128 y)

σ(y)

5

0

−5
Input stream y
(a)

−10 −5

0

5

10

15

Input stream y
(b)

−6 −4 −2

0

2

4

6

8

Input stream y
(c)

−10

−5

0

5

10

Input stream y

(d)

Figure 3: The IStanh function approximating (a) tanh and (b) sigmoid functions. The WIStanh
function approximating (c) tanh and (d) sigmoid functions. The results were obtained by measuring
the output of a single neuron for 12K input samples taken from the test set of the Penn Treebank
dataset when performing character-level language modeling.

are entries of the weight vector w ∈ Rnm  the state counter vector k ∈
where wkj   kj and yFSM
{0  1  . . .   m × n − 1}dh and the WIStanh’s output vector yFSM ∈ {0  1}dh for j ∈ {1  . . .   dh}. To
obtain the weights approximating the FSM as the tanh function  we use linear regression such that

j

m×n−1(cid:88)

tanh(y) =

pCq × wq 

(25)

q=0

where pCq denotes the probability of the occurrence of the state Cq (i.e.  the qth state in the state
set of C0 → Cn×m−1). The sigmoid function can also be obtained in a similar fashion. Note that
we constraint the weight values to lie into the interval of [−1  1]. Figures 3(c) and 3(d) show the
tanh and sigmoid functions implemented using the proposed WIStanh function where the FSM was
trained on the Penn Treebank dataset [31] when performing the character-language modeling task.
The early states of the trained FSM mainly contains values near to −1 in the bipolar format (or
zero in the unipolar format) whereas the weight values of the latter states are close 1 (see Figure 4) 
complying with the state values of the conventional integral stochastic FSMs. Note that we ﬁne tune
the weights of our baseline model (i.e.  ELSTM) with the proposed stochastic functions to comply
with the approximation error.

4.4 XNOR LSTM

Let us rewrite the gate computations of LSTMs using the proposed stochastic representation as

Fs
t = WIStanh(ISNG(Wb
Is
t = WIStanh(ISNG(Wb
Os
t = WIStanh(ISNG(Wb
t = WIStanh(ISNG(Wb
Gs
t ∈ {0  1}dh×dh  Is
t ∈ {0  1}dh×dh  Os

f xxt + bf )) 
ixxt + bi)) 
oxxt + bo)) 
gxxt + bg)) 

t−1 + Wb
t−1 + Wb
t−1 + Wb
t−1 + Wb

f hhb
ihhb
ohhb
ghhb
t ∈ {0  1}dh×dh and Gs

(26)
t ∈ {0  1}dh×dh denote the
where Fs
stochastic representation of the gate vectors (i.e.  ft  it  ot and gt) in which each entry of the vectors
is represented as a binary stochastic stream generated by the WIStanh function. More precisely  the
expected value of the gate matrices Fs
t over their second dimension is equal to the
gate vectors ft  it  ot and gt  respectively. This stochastic representation of the gates allows us to
perform the Hadamard products of the state computations using XNOR operations. More precisely 
we can formulate the state computations as

t and Gs

t   Os

t   Is

t (cid:12) WIStanh(Cs

t (cid:12) SNG(ct−1) + Is

t (cid:12) Gs

t   ht = S2B(Os

t = Fs
Cs
(27)
t ∈ {0  1  . . .   m(cid:48)}dh×dh is an integral stochastic representation of the cell state vector
where Cs
ct. The SNG function generates a binary stochastic stream in the bipolar format (see Section 3.1) 
allowing us to replace the Hadamard products with XNOR operations. The S2B and IS2B functions
convert binary and integral stochastic streams into a real number. In other words  these two functions
ﬁnd the expected value (i.e.  the mean) of the stochastic streams by accumulating the stream entries
and dividing the accumulated value by the stream length l = dh. Of course setting the stream length

t ))  ct = IS2B(Cs
t ) 

7

s
t
n
e
i
c
ﬁ
f
e
o
C
e
t
a
t

S

1

0.5

0

−0.5

−1

Hyperbolic tangent

Sigmoid

0

20

60

40
80
No. States

100

120

Figure 4: The weight values assigned for each state of the WIStanh to implement tanh and σ
functions.

w
ht−1

b

+

r
e
t
s
i
g
e
r

σ
/
h
n
a
t

g

i
c
f

×
×

α
×

(a)

+

h ×

n
a
t

ht

o

(b)

w
ht−1

α

b

G
N
S
I

h
n
a
t

S
I
W

(c)

gs
is
f s
cs

+

h
n
a
t

S
I
W

os

(d)

+

r
e
t
s
i
g
e
r

ht

Figure 5: The main computational core of (a) the gate computations and (b) the state computations of
the conventional binarized LSTM. The main computational core of (c) the gate computations and (d)
the state computations in the proposed XNOR LSTM.

dh to a number of power of two replaces the division with a simple shift operation. With our stochastic
representation  the accumulations of the vector-matrix multiplication at the gate computations are
now shifted to the end of the state computations (see Figure 5). Moreover  both the gate and state
computations now involve several vector-vector products  performed using XNOR operators. As
opposed to stochastic computing systems requiring long latency to generate the stochastic streams 
the stochastic bit streams of the proposed XNOR LSTM are already generated by the binarization of
the gate computations. Therefore  the computational latency of the proposed XNOR LSTM is either
the same or even less than of the conventional quantized LSTMs since using simple operators allows
us to run the XNOR LSTM at higher frequencies.
5 Experimental Results
In this section  we evaluate the performance of the proposed XNOR LSTM across different temporal
tasks including character-level/word-level language modeling and quation answering (QA). Note
that the length of all stochastic streams (i.e.  the parameter l) in our proposed method is equal to the
size of LSTMs (i.e.  the parameter dh). For the character-level and word-level language modeling 
we conduct our experiments on Penn Treebank (PTB) [31] corpus. For the character-level language
modeling (CLLM) experiment  we use an LSTM layer of size 1 000 on a sequence length of 100 when
performing PTB. We set the training parameters similar to [31]. The performance of CLLM models
are evaluated as bits per character (BPC). For the word-level language modeling (WLLM) task  we
train one layer of LSTM with 300 units on a sequence length of 35 while applying the dropout rate of
0.5. The performance of WLLM models are measured in terms of perplexity per word (PPW). For
the QA task  we perform our experiment on the CNN corpus [32]. We also adopt the LSTM-based
Attentive Reader architecture and its the training parameters  introduced in [32]. We measure the
performance of the QA task as a error rate (ER). Note that lower BPC  PPW and ER values show
a better performance. For a fair comparison with prior works  our XNOR-LSTM model for each
task contains the same number of parameters as of their previous counterparts. Table 1 summarizes
the performance of our XNOR-LSTM models. We consider a typical semi-parallel architecture of
LSTMs  in which each neuron is implemented using a multiply-and-accumulate (MAC) unit  to
obtain the implementation cost reported in Table 1. Depending on the precision used for the gate and
state computations  we replace the multiplier inside the MAC unit with a simpler logic and report the
implementation cost in terms of XNOR counts. In fact  we approximate the cost of a ternary/2-bit

8

Table 1: Performance of the proposed XNOR-LSTM models vs their quantized counterparts.

Baseline

LAB

AMQ

HitNet

(ICLR’17 [19])

(ICLR’18 [18])

(NeurIPS’18 [26])

Precision of Gate Computations
Precision of State Computations

CLLM

WLLM

QA

Accuracy (BPC)
Size (MByte)
Cost (XNOR count)
Cost (No. clock cycles)
Accuracy (PPW)
Size (KByte)
Cost (XNOR count)
Cost (No. clock cycles)
Accuracy (ER)
Size (MByte)
Cost (XNOR count)
Cost (No. clock cycles)

FP
FP
1.39
16.8

1 400 000

1 000
91.5
2 880
420 000

300
40.19
7 471

1 433 600

256

Binary

FP
1.56
0.525
604 000
1 000
NA
NA
NA
NA
NA
NA
NA
NA

2 bits
FP
NA
NA
NA
NA
95.8
180

Ternary

FP
NA
NA
NA
NA
110.3
180

ELSTM XNOR
(ours)
(ours)
Binary
Binary
Binary
1.52
0.525
7 000
1 000
95.5
90

FP
1.47
0.525
604 000
1 000
93.5
90

182 400

182 400

181 200

300
NA
NA
NA
NA

300
NA
NA
NA
NA

300
40.4
233

618 496

256

2 100
300
43.8
233
7 168
256

multiplication as two XNOR gates and the cost of a full-precision multiplication as 200 XNOR
gates [17]. The experimental results show that our XNOR-LSTM models outperform the previous
quantized LSTMs in terms of accuracy performance while requiring 86× fewer XNOR gates to
perform the recurrent computations. While all the LSTM models in Table 1 require the same number
of clock cycles to perform the recurrent computations  the inference time of our XNOR LSTMs is
less than of other quantized works when running at higher frequencies due to their simpler operators.
As a ﬁnal note  the small gap between the XNOR and ELSTM models shows the approximation error
caused by the use of stochastic computing.
6 Discussion
In Section 5  we only considered the implementation cost of our method in terms of XNOR operations
since our main focus was to replace the costly multipliers with simple XNOR gates while the rest of
the computing elements (i.e.  the adders and look-up tables) almost remains the same (see Figure
5). Note that since SNG and ISNG can be easily implemented with magnetic tunnel junction (MTJ)
devices which come almost at no cost compared to CMOS technologies [33]  we excluded them from
the implementation cost in Table 1. However  even if we include these units in our cost model  our
stochastic-based implementation is still superior to its conventional binary-radix counterpart. To this
end  we have implemented both the non-stochastic binarized method (e.g.  [26]) and our proposed
method on a Xilinx Virtex-7 FPGA device where each architecture contains 300 neurons. The
implementation of our proposed method requires 66K FPGA slices while yielding the throughput of
3.2 TOPS @ 934 MHz whereas the implementation of the non-stochastic binarized method requires
1.1M FPGA slices while yielding the throughput of 1.8 TOPS @ 515 MHz. Therefore  our proposed
method outperforms its binarized counterpart by factors of 16.7× and 1.8× in terms of area and
throughput  respectively  while considering all the required logic such as SNG  ISNG and look-up
tables. Note that the number of occupied slices denotes the area size of the implemented design.
Also  the implementation of our proposed method runs at a higher frequency since its critical path is
shorter than the conventional method due to the simpler hardware of XNOR gates versus multipliers.
Therefore  this work is the ﬁrst successful application of SC to the best of our knowledge where the
SC-based implementation outperforms its conventional binary-radix counterpart in terms of both the
computational latency and the area.
7 Conclusion
In this paper  we presented a method to synthesize XNOR LSTMs. To this end  we ﬁrst represented
the gate computations of LSTMs with binary weights and binary hidden state vector h in stochastic
computing domain  allowing to replace the non-linear activation functions with stochastic FSM-based
functions. We then proposed a new FSM-based function and its synthesis method to approximate the
hyperbolic tangent and sigmoid functions. In this way  the gate activation values are represented as
stochastic binary streams  allowing to perform the multiplications of the state computations using
simple XNOR gates. To the best of our knowledge  this paper is the ﬁrst to perform the multiplications
of both the gate and state computations with XNOR operations.

9

References
[1] A. Karpathy and L. Fei-Fei  “Deep Visual-Semantic Alignments for Generating Image Descriptions ”
IEEE Trans. Pattern Anal. Mach. Intell.  vol. 39  no. 4  pp. 664–676  Apr. 2017. [Online]. Available:
https://doi.org/10.1109/TPAMI.2016.2598339

[2] A. Graves and J. Schmidhuber  “Framewise Phoneme Classiﬁcation with Bidirectional LSTM Networks ”
in Proceedings. 2005 IEEE International Joint Conference on Neural Networks  2005.  vol. 4  July 2005 
pp. 2047–2052 vol. 4.

[3] T. Mikolov  M. Karaﬁát  L. Burget 

Network Based Language Model.”
//dblp.uni-trier.de/db/conf/interspeech/interspeech2010.html#MikolovKBCK10

J. Cernocký  and S. Khudanpur  “Recurrent Neural
http:

ISCA  2010  pp. 1045–1048. [Online]. Available:

[4] Y. Bengio  P. Simard  and P. Frasconi  “Learning Long-term Dependencies with Gradient Descent
is Difﬁcult ” Trans. Neur. Netw.  vol. 5  no. 2  pp. 157–166  Mar. 1994. [Online]. Available:
http://dx.doi.org/10.1109/72.279181

[5] R. Pascanu  T. Mikolov  and Y. Bengio  “On the Difﬁculty of Training Recurrent Neural Networks ”
the 30th International Conference on International Conference on Machine
JMLR.org  2013  pp. III–1310–III–1318. [Online]. Available:

in Proceedings of
Learning - Volume 28  ser. ICML’13.
http://dl.acm.org/citation.cfm?id=3042817.3043083

[6] “Long Short-Term Memory ” Neural computation  vol. 9  no. 8  pp. 1735–1780  1997.
[7] K. Cho  B. van Merriënboer  Ç. Gülçehre  D. Bahdanau  F. Bougares  H. Schwenk  and Y. Bengio 
“Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation ” in
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Doha  Qatar: Association for Computational Linguistics  Oct. 2014  pp. 1724–1734. [Online]. Available:
http://www.aclweb.org/anthology/D14-1179

[8] S. Han  X. Liu  H. Mao  J. Pu  A. Pedram  M. A. Horowitz  and W. J. Dally  “EIE: Efﬁcient Inference
Engine on Compressed Deep Neural Network ” in 2016 ACM/IEEE 43rd Annual International Symposium
on Computer Architecture (ISCA)  June 2016  pp. 243–254.

[9] A. Ardakani  Z. Ji  and W. J. Gross  “Learning to Skip Ineffectual Recurrent Computations in LSTMs ” in

2019 Design  Automation Test in Europe Conference Exhibition (DATE)  March 2019  pp. 1427–1432.

[10] T. N. Sainath  B. Kingsbury  V. Sindhwani  E. Arisoy  and B. Ramabhadran  “Low-Rank Matrix
Factorization for Deep Neural Network Training with High-Dimensional Output Targets.” IEEE  2013  pp.
6655–6659. [Online]. Available: http://dblp.uni-trier.de/db/conf/icassp/icassp2013.html#SainathKSAR13
[11] B. Liu  M. Wang  H. Foroosh  M. Tappen  and M. Penksy  “Sparse Convolutional Neural Networks ” in

2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June 2015  pp. 806–814.

[12] S. Han  J. Pool  J. Tran  and W. J. Dally  “Learning both Weights and Connections for Efﬁcient Neural

Networks ” CoRR  vol. abs/1506.02626  2015. [Online]. Available: http://arxiv.org/abs/1506.02626

[13] W. Wen  C. Wu  Y. Wang  Y. Chen  and H. Li  “Learning Structured Sparsity in Deep Neural
Networks ” in Proceedings of the 30th International Conference on Neural Information Processing
Systems  ser. NIPS’16. USA: Curran Associates Inc.  2016  pp. 2082–2090. [Online]. Available:
http://dl.acm.org/citation.cfm?id=3157096.3157329

[14] A. Ardakani  C. Condo  and W. J. Gross  “Sparsely-Connected Neural Networks: Towards Efﬁcient VLSI
Implementation of Deep Neural Networks ” 5th International Conference on Learning Representations
(ICLR)  2016. [Online]. Available: https://arxiv.org/abs/1611.01427

[15] M. Courbariaux  Y. Bengio  and J. David  “BinaryConnect: Training Deep Neural Networks with binary

weights during propagations ” CoRR  vol. abs/1511.00363  2015.

[16] M. Rastegari  V. Ordonez  J. Redmon  and A. Farhadi  “XNOR-Net: ImageNet Classiﬁcation Using
Binary Convolutional Neural Networks ” CoRR  vol. abs/1603.05279  2016. [Online]. Available:
http://dblp.uni-trier.de/db/journals/corr/corr1603.html#RastegariORF16

[17] I. Hubara  M. Courbariaux  D. Soudry  R. El-Yaniv  and Y. Bengio  “Binarized Neural Networks ” in
Advances in Neural Information Processing Systems 29  D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  Eds. Curran Associates  Inc.  2016  pp. 4107–4115. [Online]. Available:
http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf

[18] C. Xu  J. Yao  Z. Lin  W. Ou  Y. Cao  Z. Wang  and H. Zha  “Alternating Multi-bit Quantization for
Recurrent Neural Networks ” in International Conference on Learning Representations  2018. [Online].
Available: https://openreview.net/forum?id=S19dR9x0b

[19] L. Hou  Q. Yao  and J. T. Kwok  “Loss-aware Binarization of Deep Networks ” CoRR  vol. abs/1611.01600 

2016. [Online]. Available: http://arxiv.org/abs/1611.01600

10

[20] A. Ardakani  Z. Ji  S. C. Smithson  B. H. Meyer  and W. J. Gross  “Learning Recurrent Binary/Ternary
Weights ” in International Conference on Learning Representations  2019. [Online]. Available:
https://openreview.net/forum?id=HkNGYjR9FX

[21] B. R. Gaines  Stochastic Computing Systems. Boston  MA: Springer US  1969  pp. 37–172. [Online].

Available: https://doi.org/10.1007/978-1-4899-5841-9_2

[22] Z. Lin  M. Courbariaux  R. Memisevic  and Y. Bengio  “Neural Networks with Few Multiplications ”

CoRR  vol. abs/1510.03009  2015.

[23] C. Zhu  S. Han  H. Mao  and W. J. Dally  “Trained Ternary Quantization ” CoRR  vol. abs/1612.01064 

2016. [Online]. Available: http://arxiv.org/abs/1612.01064

[24] F. Li and B. Liu  “Ternary Weight Networks ” CoRR  vol. abs/1605.04711  2016. [Online]. Available:

http://arxiv.org/abs/1605.04711

[25] S. Zhou  Z. Ni  X. Zhou  H. Wen  Y. Wu  and Y. Zou  “DoReFa-Net: Training Low Bitwidth Convolutional
Neural Networks with Low Bitwidth Gradients ” CoRR  vol. abs/1606.06160  2016. [Online]. Available:
http://arxiv.org/abs/1606.06160

[26] P. Wang  X. Xie  L. Deng  G. Li  D. Wang  and Y. Xie  “HitNet: Hybrid Ternary Recurrent Neural Network ”
in Advances in Neural Information Processing Systems 31. Curran Associates  Inc.  2018  pp. 604–614.
[Online]. Available: http://papers.nips.cc/paper/7341-hitnet-hybrid-ternary-recurrent-neural-network.pdf
[27] L. Hou and J. T. Kwok  “Loss-aware Weight Quantization of Deep Networks ” in International Conference

on Learning Representations  2018. [Online]. Available: https://openreview.net/forum?id=BkrSv0lA-

[28] A. Alaghi and J. P. Hayes  “Survey of Stochastic Computing ” ACM Trans. Embed. Comput. Syst.  vol. 12 

no. 2s  pp. 92:1–92:19  May 2013. [Online]. Available: http://doi.acm.org/10.1145/2465787.2465794

[29] B. D. Brown and H. C. Card  “Stochastic Neural Computation. I. Computational Elements ” IEEE Transac-

tions on Computers  vol. 50  no. 9  pp. 891–905  Sep. 2001.

[30] A. Ardakani  F. Leduc-Primeau  N. Onizawa  T. Hanyu  and W. J. Gross  “VLSI Implementation of Deep
Neural Network Using Integral Stochastic Computing ” IEEE Transactions on Very Large Scale Integration
(VLSI) Systems  vol. 25  no. 10  pp. 2688–2699  Oct 2017.

[31] M. P. Marcus  M. A. Marcinkiewicz  and B. Santorini  “Building a Large Annotated Corpus of English:
The Penn Treebank ” Comput. Linguist.  vol. 19  no. 2  pp. 313–330  June 1993. [Online]. Available:
http://dl.acm.org/citation.cfm?id=972470.972475

[32] K. M. Hermann  T. Kocisky  E. Grefenstette  L. Espeholt  W. Kay  M. Suleyman  and P. Blunsom  “Teaching
Machines to Read and Comprehend ” in Advances in Neural Information Processing Systems 28. Curran
Associates  Inc.  2015  pp. 1693–1701.

[33] N. Onizawa  D. Katagiri  W. J. Gross  and T. Hanyu  “Analog-to-stochastic converter using magnetic tunnel
junction devices for vision chips ” IEEE Transactions on Nanotechnology  vol. 15  no. 5  pp. 705–714  Sep.
2016.

11

,Arash Ardakani
Zhengyun Ji
Amir Ardakani
Warren Gross