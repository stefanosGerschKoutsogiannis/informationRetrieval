2007,Robust Regression with Twinned Gaussian Processes,We propose a Gaussian process (GP) framework for robust inference in which a GP prior on the mixing weights of a two-component noise model augments the standard process over latent function values. This approach is a generalization of the mixture likelihood used in traditional robust GP regression  and a specialization of the GP mixture models suggested by Tresp (2000) and Rasmussen and Ghahramani (2002). The value of this restriction is in its tractable expectation propagation updates  which allow for faster inference and model selection  and better convergence than the standard mixture. An additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions  and to recover with the predictive distribution information about the outlier distribution via the gating process. The model has asymptotic complexity equal to that of conventional robust methods  but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions  for which they fail altogether. We show further how our approach can be used without adjustment for more smoothly heteroscedastic data  and suggest how it could be extended to more general noise models. We also address similarities with the work of Goldberg et al. (1998)  and the more recent contributions of Tresp  and Rasmussen and Ghahramani.,Robust Regression with Twinned Gaussian Processes

Andrew Naish-Guzman & Sean Holden

Cambridge  CB3 0FD. United Kingdom
{agpn2 sbh11}@cl.cam.ac.uk

Computer Laboratory

University of Cambridge

Abstract

We propose a Gaussian process (GP) framework for robust inference in which a
GP prior on the mixing weights of a two-component noise model augments the
standard process over latent function values. This approach is a generalization of
the mixture likelihood used in traditional robust GP regression  and a specializa-
tion of the GP mixture models suggested by Tresp [1] and Rasmussen and Ghahra-
mani [2]. The value of this restriction is in its tractable expectation propagation
updates  which allow for faster inference and model selection  and better conver-
gence than the standard mixture. An additional beneﬁt over the latter method lies
in our ability to incorporate knowledge of the noise domain to inﬂuence predic-
tions  and to recover with the predictive distribution information about the outlier
distribution via the gating process. The model has asymptotic complexity equal
to that of conventional robust methods  but yields more conﬁdent predictions on
benchmark problems than classical heavy-tailed models and exhibits improved
stability for data with clustered corruptions  for which they fail altogether. We
show further how our approach can be used without adjustment for more smoothly
heteroscedastic data  and suggest how it could be extended to more general noise
models. We also address similarities with the work of Goldberg et al. [3].

1 Introduction

Regression data are often modelled as noisy observations of an underlying process. The simplest
assumption is that all noise is independent and identically distributed (i.i.d.) zero-mean Gaussian 
such that a typical set of samples appears as a cloud around the latent function. The Bayesian frame-
work of Gaussian processes [4] is well-suited to these conditions  for which all computations remain
tractable (see ﬁgure 1a). Furthermore  the Gaussian noise model enjoys the theoretical justiﬁcation
of the central limit theorem  which states that the sum of sufﬁciently many i.i.d. random variables of
ﬁnite variance will be distributed normally. However  only rarely can perturbations affecting data in
the real world be argued to have originated in the addition of many i.i.d. sources. The random com-
ponent in the signal may be caused by human or measurement error  or it may be the manifestation
of systematic variation invisible to a simpliﬁed model. In any case  if ever there is the possibility of
encountering small quantities of highly implausible data  we require robustness  i.e. a model whose
predictions are not greatly affected by outliers.

Such demands render the standard GP inappropriate: the light tails of the Gaussian distribution
cannot explain large non-Gaussian deviations  which either skew the mean interpolant away from
the majority of the data  or force us to infer an unreasonably large (global) noise variance (see
ﬁgure 1b). Robust methods use a heavy-tailed likelihood to allow the interpolant effectively to
favour smoothness and ignore such erroneous data. Figure 1c shows how this can be achieved using
a two-component noise model

p(yn|fn) = (1 − ǫ)N(cid:0)yn ; fn   σ2

R(cid:1) + ǫN(cid:0)yn ; fn   σ2
O(cid:1)  

1

(1)

(a)

(b)

(c)

(d)

Figure 1: Black dots show noisy samples from the sinc function. In panels (a) and (b)  the be-
haviour of a GP with a Gaussian noise assumption is illustrated; the shaded region shows 95%
conﬁdence intervals. The presence of a single outlier is highly inﬂuential in this model  but the
heavy-tailed likelihood (1) in panel (c) is more resilient. Unfortunately  even this model fails for
the cluster of outliers in panel (d). Here  grey lines show ten repeated runs of the EP inference
algorithm  while the black line and shaded region are their averaged mean and conﬁdence intervals
respectively—grossly at odds with those of the latent generative model.

in which observations yn are Gaussian corruptions of fn  being drawn with probability ǫ from a
large variance outlier distribution (σ2
R). Inference in this model is tractable  but impractical
for all but the smallest problems due to the exponential explosion of terms in products of (1).

O ≫ σ2

In this paper  we address the more fundamental GP assumption of i.i.d. noise. Our research is mo-
tivated by observing how the predictive distribution suffers for heavy-tailed models when outliers
appear in bursts: ﬁgure 1d replicates ﬁgure 1c  but introduces an additional three outliers. All param-
eters were taken from the optimal solution to (c)  but even without the challenge of hyperparameter
optimization there is now considerable uncertainty in the posterior since the competing interpreta-
tions of the cluster as signal or noise have similar posterior mass. Viewed another way  the tails of
the effective log likelihood of four clustered observations have approximately one-quarter the weight
of a single outlier  so the magnitude of the posterior peak associated with the robust solution is com-
parably reduced. One simple remedy is to make the tails of the likelihood heavier. However  since
the noise model is global  this has ramiﬁcations across the entire data space  potentially causing
underﬁtting elsewhere when real data are relegated to the tails. We can establish an optimal choice
for the parameters by gradient ascent on the marginal likelihood  but it is entirely possible that no
single setting will be universally satisfactory.

The model introduced in this paper  which we call the twinned Gaussian process (TGP)  generalizes
the noise model (1) by using a GP gating function to choose between the “real” and “outlier dis-
tributions”: in regions of conﬁdence  the tails can be made very light  encouraging the interpolant
to hug the data points tightly; more dubious observations can be treated appropriately by broaden-
ing the noise distribution in their vicinity. Our model is also a specialization of the GP mixtures
proposed by Tresp [1] and Rasmussen and Ghahramani [2]; indeed  the latter automatically infers
the correct number of components to use. One may therefore wonder what can possibly be gained
by restricting ourselves to a comparatively simple architecture. The answer is in the computational
overhead required for the different approaches  since these more general models require inference
by Monte Carlo methods. We argue that the two-component mixture is often a sensible distribution
for modelling real data  with a natural interpretation and the heavy tails required for robustness;
its weaknesses are exposed primarily when the noise distribution is not homoscedastic. The TGP
largely solves this problem  and allows inference by an efﬁcient expectation propagation (EP) [5]
procedure (rather than resorting to more heavy duty Monte Carlo methods). Hence  provided a two-
component mixture is likely to reﬂect adequately the noise on our data  the TGP will give similar
results to the generalized mixtures mentioned above  but at a fraction of the cost.

Goldberg et al. [3] suggest an approach to input-dependent noise in the spirit of the TGP  in which
the log variance on observations is itself modelled as a GP (the logarithm since noise variance is
a non-negative property).
Inference is again analytically intractable  so Gibbs sampling is used
to generate noise vectors from the posterior distribution by alternately ﬁtting the signal process
and ﬁtting the noise process. A further stage of Gibbs sampling is required at each test point to
estimate the predictive variance  making testing rather slow. Model selection is even slower  and the
Metropolis-Hastings algorithm is suggested for updating hyperparameters.

2

2 Twinned Gaussian processes

Given a domain X and covariance function K(·  ·) ∈ X × X → R  a Gaussian process (GP) over
the space of real-valued functions of X speciﬁes the joint distribution at any ﬁnite set X ⊂ X :

p(f |X) = N (f ; 0   Kf )  

where the f = {fn}N
n=1 are (latent) values associated with each xn ∈ X  and Kf is the Gram
matrix  the evaluation of the covariance function at all pairs (xi  xj). We apply Bayes’ rule to obtain
the posterior distribution over the f  given the observed X and y  which with the assumption of
i.i.d. Gaussian corrupted observations is also normally distributed. Predictions at X⋆ are made by
marginalizing over f in the (Gaussian) joint p(f   f⋆|X  y  X⋆). See [6] for a thorough introduction.
Robust GP regression is achieved by using a leptokurtic likelihood distribution  i.e. one whose tails
have more mass than the Gaussian. Common choices are the Laplace (or double exponential) distri-
bution  Student’s t distribution  and the mixture model (1). In product with the prior  a heavy-tailed
likelihood over an outlying observation does not exert the strong pull on the posterior witnessed
with a light-tailed noise model. Kuss [7] describes how inference can be performed for all these
likelihoods  and establishes that in many cases their performance is broadly comparable. Since it
bears closest resemblance to the twinned GP  we are particularly interested in the mixture; however 
in section 4  we include results for the Laplace model: it is the heaviest-tailed log concave distri-
bution  which guarantees a unimodal posterior and allows more reliable EP convergence. In any
case  all such methods make a global assumption about the noise distribution  and it is where this is
inappropriate that our model is most beneﬁcial.

The graphical model for the TGP is shown in ﬁgure 2b. We augment the standard process over f
with another GP over a set of variables u; this acts as a gating function  probabilistically dividing
the domain between the real and outlier components of the noise model

p(yn|fn) = σ(un)N(cid:0)yn ; fn   σ2

N (z ; 0   1) dz.

.

R(cid:1) + σ(−un)N(cid:0)yn ; fn   σ2
O(cid:1)  

where σ(un)

=Z un

−∞

(2)

In the TGP likelihood  we therefore mix two forms of Gaussian corruption  one strongly peaked at
the observation  the other a broader distribution which provides the heavy tails  in proportion deter-
mined by u(x). This makes intuitive sense; crucially to us  it retains the advantage of tractability
with respect to EP updates. The two priors may have quite different covariance structure  reﬂect-
ing our different beliefs about correlations in the signal and in the noise domain. In addition  we
accommodate prior beliefs about the prevalence of outliers with a non-zero mean process on u 

p(u|X) = N (u ; mu   Ku)

p(f |X) = N (f ; 0   Kf ) .

Our model can be understood as lying between two extremes: observe that we recover the heavy-
tailed (mixture of Gaussians) GP by forcing absolute correlation in u and adjusting the mean of
the u-process to mu = σ−1(1 − e); conversely  if we remove all correlations in u  we return to a
standard mixture model where independently we must decide to which component an input belongs.

3 Inference

We begin with a very brief account of EP; for more details  see [5  8]. Suppose we have an intractable
distribution over f whose unnormalized form factorizes into a product of terms  such as a dense
Gaussian prior t0(f   u) and a series of independent likelihoods {tn(yn|fn  un)}N
n=1. EP constructs
the approximate posterior as a product of scaled site functions ˜tn. For computational tractability 
these sites are usually chosen from an exponential family with natural parameters θ  since in this
case their product retains the same functional form as its components. The Gaussian (µ  Σ) has a
natural parameterization (b  Π) = (Σ−1µ  − 1
2 Σ−1). If the prior is of this form  its site function is
exact:

p(f   u|y) =

1
Z

t0(f   u)

N

Yn=1

tn(yn|fn  un) ≈ q(f ; θ) = t0(f   u)

zn˜tn(fn  un; θn) 

(3)

N

Yn=1

3

x1

x2

x3

xN

x1

x2

x3

xN

f1

f2

f3

y1

y2

y3

(a)

fN

yN

u1

u2

u3

uN

f1

f2

f3

fN

y1

y2

yN

y3

(b)

Figure 2: In panel (a) we show a graphical model for the Gaussian process. The data ordinates are x 
observations y  and the GP is over the latent f. The bold black lines indicate a fully-connected set.
Panel (b) shows a graphical model for the twinned Gaussian process (TGP)  in which an auxiliary
set of hidden variables u describes the noisiness of the data.

where Z is the marginal likelihood and zn are the scale parameters. Ideally  we would choose θ
at the global minimum of some divergence measure d(pkq)  but the necessary optimization is usu-

ally intractable. EP is an iterative procedure that ﬁnds a minimizer of KL(cid:0)p(f   u|y)kq(f   u; θ)(cid:1)

on a pointwise basis: at each iteration  we select a new site n  and from the product of the cav-
ity distribution formed by the current marginal with the omission of that site  and the true likeli-
hood term tn  we obtain the so-called tilted distribution qn(fn  un; θ\n). A simpler optimization
minθn
ment matching between the two distributions  with scale zn chosen to match the zeroth-order mo-
ments. After each site update  the moments at the remaining sites are liable to change  and several
iterations may be required before convergence.

KL(cid:0)qn(fn  un; θ\n)kq(fn  un; θ)(cid:1) then ﬁts only the parameters θn: this is equivalent to mo-

The priors over u and f are independent  but we expect correlations in the posterior after condi-
tioning on observations. To understand this  consider a single observation (xn  yn); in principle  it
admits two explanations corresponding to its classiﬁcation as either “outlier” or as “real” data: in
general terms  either un > 0 and fn ≈ yn  or un < 0 and fn respects the global structure of the
signal. A diagram to assist the visualization of the behaviour of the posterior is provided in ﬁgure 3.

Now  recall that the prior over u and f is

X! = N(cid:18)(cid:20) u

f (cid:21) ; (cid:20) mu

0 (cid:21)   (cid:20)Ku

0 Kf(cid:21)(cid:19)

0

f (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
p (cid:20) u

and the likelihood factorizes into a product of terms (2); our site approximations ˜tn are therefore
Gaussian in (fn  un). Of importance for EP are the moments of the tilted distribution which we
seek to match. These are most easily obtained by differentiation of the zeroth moments ZR and ZO
of each component. We ﬁnd

σ(u)N(cid:0)y ; f   σ2

R(cid:1) N(cid:18)(cid:20) u
ZR =ZZf u
writing the inner Gaussian as N(cid:18)(cid:20) zn

N(cid:18)(cid:20) z

y (cid:21) ; µ   (cid:20)1

0
0 σ2

R(cid:21) + Σ(cid:19) dz;

C BR(cid:21)(cid:19)  Z R = N (y ; µf   BR) σ(q) 

0

f (cid:21) ; µ   Σ(cid:19) dudf =Z ∞
µf (cid:21)   (cid:20)A C
yn (cid:21) ; (cid:20) µu
qA − C 2

µu + C
BR

q =

BR

(y − µf )

.

where

The integral for the outlier component is similar; ZO = N (y ; µf   BO) σ(−q). With partial deriva-
tives ∂ log Z
T we are equipped for EP; algorithmic details appear in Seeger’s note [8]. For
efﬁciency  we make rank-two updates of the full approximate covariance on (f   u) during the EP
loop  and refresh the posterior at the end of each cycle to avoid loss of precision.

and ∂ 2 log Z

∂µµ

∂µ

4

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

prior
likelihood
posterior
EP

-5

0

5

10

f

-5

0

5

10

f

replacements

p
g
o
l

p
g
o
l

p
g
o
l

p
g
o
l

prior
likelihood
posterior
EP

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

u

u

u

u

-5

0

5

10

f

-5

0

5

10

f

-5

0

5

10

f

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

10

5

0

-5

-10

u

u

u

u

-5

0

5

10

f

-5

0

5

10

-5

0

5

10

f

f

Figure 3: Using the twinned Gaussian process provides a natural resilience against clustered noisy
data. The left-hand column illustrates the behaviour of a ﬁxed heavy-tailed likelihood for one 
two  four and ﬁve repeated observations at f = 5. (Outliers in real data are not necessarily so
tightly packed  but the symmetry of this approximation allows us to treat them as a single unit: by
“posterior”  for example  we mean the a posteriori belief in all the observations’ (identical) latent
f .) The context is provided by the prior  which gives 95% conﬁdence to data around f = 0 ± 2. The
top-left box illustrates how the inﬂuence of isolated outliers is mitigated by the standard mixture.
However  a repeated observation (box two on the left) causes the EP solution to collapse onto the
spike at the data (the log scale is deceptive:
the second peak contributes only about 8% of the
posterior mass). The twinned GP better preserves the marginal distribution of f by maintaining a
joint distribution over both f and u: in the second and third columns respectively are contours of
the true log joint (we use a broad zero-mean prior on u) and that inferred by EP  together with the
marginal posterior over f . Only with a ﬁfth observation—ﬁnal box—is the context of f essentially
overruled by the TGP approximation. The thick bar in the central column marks the cross-section
corresponding to the unnormalized posterior from column one.

5

3.1 Predictions

If the outlier component describes nuisance noise that should be eliminated  we require at test in-
puts x⋆ only the marginal distribution p(f⋆|x⋆  X  y)  obtained by marginalizing over u in the full
(approximate) posterior

N(cid:18)(cid:20) u

f (cid:21) ; (cid:20) ˆµu
p(f⋆|x⋆  X  y) =Z p(f⋆|x⋆  f )p(f |X  y)df

ˆµf (cid:21)   (cid:20) ˆΣuu

ˆΣf u

ˆΣuf

ˆΣff(cid:21)(cid:19) :

≈ N(cid:16)f⋆ ; kT

f ⋆K−1

f ˆµf   kf

⋆⋆ − kT

f ⋆K−1

f kf ⋆ + kT

f ⋆K−1

ˆΣff K−1

f

f kf ⋆(cid:17) .

The noise process may itself be of interest  in which case we need to marginalize over both u⋆ and
f⋆ in

f (cid:21)!N(cid:18)(cid:20) u
(cid:20) u

f (cid:21) ; ˆµ   ˆΣ(cid:19) du⋆df⋆dudf .

p(y⋆|x⋆  X  y) =ZZ p y⋆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x⋆ (cid:20) u
≈ZZZZ p y⋆(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x⋆ (cid:20) u⋆

f (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
X  y! dudf
f (cid:21)!p (cid:20) u
f⋆ (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
f⋆ (cid:21)!p (cid:20) u⋆

This distribution is no longer Gaussian  but its moments may be recovered easily by the same method
used to obtain moments of the tilted distribution.

EP provides in addition to the approximate moments of the posterior distribution an estimate of the
marginal likelihood and its derivatives with respect to kernel hyperparameters. Again  we refer the
interested reader to the algorithm presented in [8]  adding here only that our implementation uses
log noise values on (σ2

O) to allow for their unconstrained optimization.

R  σ2

3.2 Complexity

The EP loop is dominated by the rank-two updates of the covariance. Each such update is

O(cid:0)(2N )2(cid:1)  making every N iterations O(4N 3). The posterior refresh is O(8N 3) since it re-

quires the inverse of a 2N × 2N positive semi-deﬁnite matrix  most efﬁciently achieved through
Cholesky factorization (this Cholesky factor can be retained for use in calculating the approximate
log marginal likelihood). The total number of loops required for convergence of EP is typically in-
dependent of N   and can be upper bounded by a small constant  say 10  making the entire inference
process O(8N 3) = O(N 3). Thus  our algorithm has the same limiting time complexity as i.i.d. ro-
bust regression by EP  which admittedly masks the larger coefﬁcient that appears in approximating
both u and f simultaneously. Additionally  the body of the EP loop is slightly slower  since the pre-
cision matrix in a standard GP can be obtained with a single division  whereas our model requires
the inversion of a 2 × 2 matrix.

4 Experiments

We identify two general noise characteristics for which our model may be suitable. The ﬁrst is
when the outlying observations can appear in clusters: we saw in ﬁgure 1d how these occurrences
affect the standard mixture model. In fact the problem is quite severe  since the multimodality of the
posterior impedes the convergence of EP  while the possibility of conﬂicting gradient information at
the optima hampers procedures for evidence maximization. In ﬁgure 4 we illustrate how the TGP
succeeds where the mixture and Laplace models fail; note how the mean process on u falls sharply
in the contaminated regions. This is a stable solution  and hyperparameters can be ﬁt reliably.

A data set which exhibits the superior predictive modelling of the TGP in a domain where robust
methods can also expect to perform well is provided by Kuss [7] in a variation on a set of Friedman
[9]. The samples are drawn from a function of ten-dimensional vectors x which depend only on the
ﬁrst ﬁve components:

f (x) = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5.

6

-10

0

10

-10

0

10

-10

(a) Mixture noise

(b) Laplace noise

0

(c) TGP

10

Figure 4: The corruptions are i.i.d. around x = −10  and highly correlated near x = 0.

We generated ten sets of 90 training examples and 10000 test examples by sampling x uniformly
in [0  1]10  and adding to the training data noise N (0  1). In our ﬁrst experiment  we replicated the
procedure of [7]: ten training points were added at random with outputs sampled from N (15  9) (a
value likely to lie in the same range as f ). The results appear as Friedman (1) in ﬁgure 5. Observe
that the r.m.s. error for the robust methods is similar  but the TGP is able to ﬁt the variance far more
accurately. In a second experiment  the training set was augmented with two Gaussian clusters each
of ﬁve noisy observations. The cluster centres were drawn uniformly in [0  1]10  with variance ﬁxed
at 10−3. Output values were then drawn from N (0  1) for all ten points  to give highly correlated
values distant from the underlying function (Friedman (2)). Now the TGP excels where the other
methods offer no improvement on the standard GP; it also yields very conﬁdent predictions (cf.
Friedman (1))  because once the outliers have been accounted for there are fewer corrupted regions;
furthermore  estimates of where the data are corrupted can be recovered by considering the process
on u. In both experiments  the training data were renormalized to zero mean and unit variance  and
throughout  we used the anisotropic squared exponential for the f process (implementing so-called
relevance determination)  and an isotropic version for u. The approximate marginal likelihood was
maximized on three to ﬁve randomly initialized models; we chose for testing the most favoured.

The second domain of application is when the noise on the data is believed a priori to be a function of
the input (i.e. heteroscedastic). The twinned GP can simulate this changing variance by modulating
the u process  allocating varying weight to the two components. By way of example  the behaviour
for the one-dimensional motorcycle set [10] is shown in ﬁg. 5c. However  since the input-dependent
noise is not modelled directly  there are two notable dangers associated with this approach: ﬁrst 
the predictive variance saturates when all weight has been apportioned to one or other component;
second  the “outlier” component can dominate the variance estimates of the mixture. This is partic-
ularly problematic when variance on the data ranges over several orders of magnitude  such that the
“outlier” width must be comparably broader than that of the “real” component. In such cases  only
with extreme values of u can the smallest errors be predicted  but in consequence the process tends
to sweep precipitately through the region of sensitivity where variance predictions can be made ac-
curately. To circumvent these problems we might employ the warped GP [11] to rescale the process
on u in a supervised manner  but we do not explore these ideas further here.

0.5

0

0.4

0.2

GP Lap MixTGP

GP Lap MixTGP
neg. log probability

test error
(a) Friedman (1)

0.6

0.4

0.2

0

-1

GP Lap MixTGP

GP Lap MixTGP
neg. log probability

test error
(b) Friedman (2)

(c) Motorcycle

Figure 5: Results for the Friedman data  and the predictions of the TGP on the motorcycle set.

7

5 Extensions

With prior knowledge of the nature of corruptions affecting the signal  we can seek to model the
noise distribution more accurately  for example by introducing a compound likelihood for the outlier

component pO(yn|fn) = Pj αjN(cid:0)yn ; µj(fn)   σ2

weight of outlier corruptions to be constant across the entire domain. A richer alternative is provided
by extending the single u-process on noise to a series u(1)  u(2)  . . .   u(ν) of noise processes  and
broadening the likelihood function appropriately. For example  with ν = 2  we may write

j(cid:1)  Pj αj = 1. This constrains the relative

p(yn|fn  u(1)

n   u(2)

n ) = σ(u(1)

n )N(cid:0)yn ; fn   σ2

n )σ(u(2)

σ(−u(1)

R(cid:1) +
n )N(cid:0)yn ; fn   σ2

σ(−u(1)

O1(cid:1) +

n )σ(−u(2)

In the former case  the preceding analysis applies with small changes: each component of the outlier
distribution contributes moments independently. The second model introduces signiﬁcant compu-
tational difﬁculty: ﬁrstly  we must maintain a posterior distribution over f and all ν us  yielding
space requirements O(N (ν + 1)) and time complexity O(N 3(ν + 1)3). More importantly  the req-
uisite moments needed in the EP loop are now intractable  although an inner EP loop can be used
to approximate them  since the product of σs behaves in essence like the standard model for GP
classiﬁcation. We omit details  and defer experiments with such a model to future work.

n )N(cid:0)yn ; f0   σ2
O2(cid:1) .

(4)

6 Conclusions

We have presented a method for robust GP regression that improves upon classical approaches by
allowing the noise variance to vary in the input space. We found improved convergence on problems
which upset the standard mixture model  and have shown how predictive certainty can be improved
by adopting the TGP even for problems which do not. The model also allows an arbitrary process
on u  such that specialized prior knowledge could be used to drive the inference over f to respecting
regions which may otherwise be considered erroneous. A generalization of our ideas appears as
the mixture of GPs [1]  and the inﬁnite mixture [2]  but both involve a slow inference procedure.
When faster solutions are required for robust inference  and a two-component mixture is an adequate
model for the task  we believe the TGP is a very attractive option.

References

[1] Volker Tresp. Mixtures of Gaussian processes. In Advances in Neural Information Processing Systems 

pages 654–660  2000.

[2] Carl Edward Rasmussen and Zoubin Ghahramani.

Inﬁnite mixtures of gaussian process experts.

In

Advances in Neural Information Processing Systems  2002.

[3] Paul Goldberg  Christopher Williams  and Christopher Bishop. Regression with input-dependent noise:
a Gaussian process treatment. In Advances in Neural Information Processing Systems. MIT Press  1998.
[4] Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances

in Neural Information Processing Systems 18. MIT Press  2005.

[5] Thomas Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  Massachusetts

Institute of Technology  2001.

[6] Carl Rasmussen and Christopher Williams. Gaussian processes for machine learning. MIT Press  2006.
[7] Malte Kuss. Gaussian process models for robust regression  classiﬁcation and reinforcement learning.

PhD thesis  Technische Universit¨at Darmstadt  2006.

[8] Matthias Seeger.

Expectation propagation for exponential

families  2005.

http://www.cs.berkeley.edu/˜mseeger/papers/epexpfam.ps.gz.

Available from

[9] J. H. Friedman. Multivariate adaptive regression splines. Annals of Statistics  19(1):1–67  1991.
[10] B.W. Silverman. Some aspects of the spline smoothing approach to non-parametric regression curve

ﬁtting. Journal of the Royal Statistical Society B  47:1–52  1985.

[11] Edward Snelson  Carl Edward Rasmussen  and Zoubin Ghahramani. Warped Gaussian processes.

Advances in Neural Information Processing Systems 16  2003.

In

8

,Odalric-Ambrym Maillard
Timothy Mann
Shie Mannor