2011,Robust Lasso with missing and grossly corrupted observations,This paper studies the problem of accurately recovering a sparse vector $\beta^{\star}$ from highly corrupted linear measurements $y = X \beta^{\star} + e^{\star} + w$ where $e^{\star}$ is a sparse error vector whose nonzero entries may be unbounded and $w$ is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both $\beta^{\star}$ and $e^{\star}$. Our first result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix $X$. Our second set of results applies to a general class of Gaussian design matrix $X$ with i.i.d rows $\oper N(0  \Sigma)$  for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both $\beta^{\star}$ and $e^{\star}$ from only $\Omega(k \log p \log n)$ observations  even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal.,Robust Lasso with missing and grossly corrupted

observations

Nam H. Nguyen

Johns Hopkins University

nam@jhu.edu

Nasser M. Nasrabadi
U.S. Army Research Lab

nasser.m.nasrabadi.civ@mail.mil

Trac D. Tran

Johns Hopkins University

trac@jhu.edu

Abstract

This paper studies the problem of accurately recovering a sparse vector β(cid:63) from
highly corrupted linear measurements y = Xβ(cid:63) + e(cid:63) + w where e(cid:63) is a sparse
error vector whose nonzero entries may be unbounded and w is a bounded noise.
We propose a so-called extended Lasso optimization which takes into consider-
ation sparse prior information of both β(cid:63) and e(cid:63). Our ﬁrst result shows that the
extended Lasso can faithfully recover both the regression and the corruption vec-
tors. Our analysis is relied on a notion of extended restricted eigenvalue for the
design matrix X. Our second set of results applies to a general class of Gaus-
sian design matrix X with i.i.d rows N (0  Σ)  for which we provide a surprising
phenomenon: the extended Lasso can recover exact signed supports of both β(cid:63)
and e(cid:63) from only Ω(k log p log n) observations  even the fraction of corruption is
arbitrarily close to one. Our analysis also shows that this amount of observations
required to achieve exact signed support is optimal.

1

Introduction

One of the central problems in statistics is the linear regression in which the goal is to accurately
estimate a regression vector β(cid:63) ∈ Rp from the noisy observations

y = Xβ(cid:63) + w 

(1)
where X ∈ Rn×p is the measurement or design matrix  and w ∈ Rn is the stochastic observation
vector noise. A particular situation recently attracted much attention from research community
concerns with the model in which the number of regression variables p is larger than the number
of observations n (p ≥ n). In such circumstances  without imposing some additional assumptions
for this model  it is well known that the problem is ill-posed  and thus the linear regression is not
consistent. Accordingly  there have been various lines of work on high dimensional inference based
on imposing different types of structure constraints such as sparsity and group sparsity [15] [5] [21].
Among them  the most popular model focused on sparsity assumption of the regression vector. To
estimate β  a standard method  namely Lasso [15]  was proposed to use l1-penalty as a surrogate
function to enforce sparsity constraint.

where λ is the positive regularization parameter and l1-norm (cid:107)β(cid:107)1 is deﬁned by (cid:107)β(cid:107)1 =(cid:80)p

(2)
i=1 |βi|.
During the past few years  there has been numerous studies to understand the (cid:96)1-regularization for
sparse regression models [23] [11] [10] [17] [4] [2] [22]. These works are mainly characterized by

min

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1  

1
2

β

1

the type of the loss functions considered. For instance  some authors [4] seek to obtain a regression

estimate (cid:98)β that delivers small prediction error while other authors [2] [11] [22] seek to produce a
regressor with minimal parameter estimation error  which is measured by the (cid:96)2-norm of ((cid:98)β − β(cid:63)).

Another line of work [23] [17] considers the variable selection in which the goal is to obtain an
estimate that correctly identiﬁes the support of the true regression vector. To achieve low prediction
or parameter estimation loss  it is now well known that it is both sufﬁcient and necessary to impose
certain lower bounds on the smallest singular values of the design matrix [10] [2]  while a notion of
small mutual incoherence for the design matrix [4] [23] [17] is required to achieve accurate variable
selection.
We notice that all the previous work relies on the assumption that the observation noise has bounded
energy. Without this assumption  it is very likely that the estimated regressor is either not reliable
or unable to identify the correct support. With this observation in mind  in this paper  we extend the
linear model (1) by considering the noise with unbounded energy. It is clear that if all the entries
of y is corrupted by large error  then it is impossible to faithfully recover the regression vector β(cid:63).
However  in many practical applications such as face and acoustic recognition  only a portion of the
observation vector is contaminated by gross error. Formally  we have the mathematical model

y = Xβ(cid:63) + e(cid:63) + w 

(3)
where e(cid:63) ∈ Rn is the sparse error whose locations of nonzero entries are unknown and magnitudes
can be arbitrarily large and w is another noise vector with bounded entries. In this paper  we assume
that w has a multivariate Gaussian N (0  σ2In×n) distribution. This model also includes as a par-
ticular case the missing data problem in which all the entries of y is not fully observed  but some
are missing. This problem is particularly important in computer vision and biology applications.
If some entries of y are missing  the nonzero entries of e(cid:63) whose locations are associated with the
missing entries of the observation vector y have the same values as entries of y but with inverse
signs.
The problems of recovering the data under gross error has gained increasing attentions recently with
many interesting practical applications [18] [6] [7] as well as theoretical consideration [9] [13] [8].
Another recent line of research on recovering the data from grossly corrupted measurements has
been also studied in the context of robust principal component analysis (RPCA) [3] [20] [1]. Let us
consider some examples to illustrate:

• Face recognition. The model (3) has been originally proposed by Wright et al. [19] in
the context of face recognition. In this problem  a face test sample y is assumed to be
represented as a linear combination of training faces in the dictionary X  y = Xβ where β
is the coefﬁcient vector used for classiﬁcation. However  it is often the case that the face is
occluded by unwanted objects such as glasses  hats etc. These occlusions  which occupy a
portion of the test face  can be considered as the sparse error e(cid:63) in the model (3).

• Subspace clustering. One of the important problem on high dimensional analysis is to
cluster the data points into multiple subspaces. A recent work of Elhamifar and Vidal [6]
showed that this problem can be solved by expressing each data point as a sparse linear
combination of all other data points. Coefﬁcient vectors recovered from solving the Lasso
problems are then employed for clustering. If the data points are represented as a matrix X 
then we wish to ﬁnd a sparse coefﬁcient matrix B such that X = XB and diag(B) = 0.
When the data is missing or contaminated with outliers  [6] formulates the problem as
X = XB + E and minimize a sum of two (cid:96)1-norms with respect to both B and E.
• Sensor network. In this model  sensors collect measurements of a signal β(cid:63) independently
by simply projecting β(cid:63) onto row vectors of a sensing matrix X  yi = (cid:104)Xi  β(cid:63)(cid:105). The
measurements yi are then sent to the center hub for analysis. However  it is highly likely
that some sensors might fail to send the measurements correctly and sometimes report
totally irrelevant measurements. Therefore  it is more accurate to employ the observation
model (3) than model (1).

It is worth noticing that in the aforementioned applications  e(cid:63) plays the role as the sparse (unde-
sired) error. However  in many other applications  e(cid:63) can contain meaningful information  and thus
necessary to be recovered. An example of this kind is signal separation  in which β(cid:63) and e(cid:63) are two
distinct signal components (video or audio). Furthermore  in applications such as classiﬁcation and

2

clustering  the assumption that the test sample y is a linear combination of a few training samples in
the dictionary (design matrix) X might be violated. This sparse component e(cid:63) can thus be seen as
the compensation for linear regression model mismatch.
Given the observation model (1) and the sparsity assumptions on both regression vector β(cid:63) and error
e(cid:63)  we propose the following convex minimization to estimate the unknown parameter β(cid:63) as well as
the error e(cid:63).

min
β e

1
2

(cid:107)y − Xβ − e(cid:107)2

2 + λβ (cid:107)β(cid:107)1 + λe (cid:107)e(cid:107)1  

(4)

where λβ and λe are positive regularization parameters. This optimization  we call extended Lasso 
can be seen as a generalization of the Lasso program. Indeed  by setting λe = 0  (4) returns to
the standard Lasso. The additional regularization associated with e encourages sparsity on the error
where parameter λe controls the sparsity level. In this paper  we focus on the following questions:
what are necessary and sufﬁcient conditions for the ambient dimension p  the number of observations
n  the sparsity index k of the regression β(cid:63) and the fraction of corruption so that (i) the extended
Lasso is able (or unable) to recover the exact support sets of both β(cid:63) and e(cid:63)? (ii) the extended Lasso
is able to recover β(cid:63) and e(cid:63) with small prediction error and parameter error? We are particularly
interested in understanding the asymptotic situation where the the fraction of error is arbitrarily close
to 100%.
Previous work. The problem of recovering the estimation vector β(cid:63) and error e(cid:63) has originally
proposed and analyzed by Wright and Ma [18]. In the absence of the stochastic noise w in the
observation model (3)  the authors proposed to estimate (β(cid:63)  e(cid:63)) by solving the linear program

(cid:107)β(cid:107)1 + (cid:107)e(cid:107)1

min
β e

s.t.

y = Xβ + e.

(5)

The result of [18] is asymptotic in nature. They showed that for a class of Gaussian design matrix
with i.i.d entries  the optimization (5) can recover (β(cid:63)  e(cid:63)) precisely with high probability even when
the fraction of corruption is arbitrarily close to one. However  the result holds under rather stringent
conditions. In particularly  they require the number of observations n grow proportionally with the
ambient dimension p  and the sparsity index k is a very small portion of n. These conditions is
of course far from the optimal bound in compressed sensing (CS) and statistics literature (recall
k ≤ O(n/ log p) is sufﬁcient in conventional analysis [17]).
Another line of work has also focused on the optimization (5). In both papers of Laska et al. [7] and
Li et al. [9]  the authors establish that for Gaussian design matrix X  if n ≥ C(k + s) log p where s
is the sparsity level of e(cid:63)  then the recovery is exact. This follows from the fact that the combination
matrix [X  I] obeys the restricted isometry property  a well-known property used to guarantee exact
recovery of sparse vectors via (cid:96)1-minimization. These results  however  do not allow the fraction of
corruption close to one.
Among the previous work  the most closely related to the current paper are recent results by Li [8]
and Nguyen et al. [13] in which a positive regularization parameter λ is employed to control the
√
sparsity of e(cid:63). Using different methods  both sets of authors show that as λ is deterministically se-
lected to be 1/
log p and X is a sub-orthogonal matrix  then the solution of following optimization
is exact even a constant fraction of observation is corrupted. Moreover  [8] establishes a similar
result with Gaussian design matrix in which the number of observations is only an order of k log p -
an amount that is known to be optimal in CS and statistics.

(cid:107)β(cid:107)1 + λ(cid:107)e(cid:107)1

min
β e

s.t.

y = Xβ + e.

(6)

Our contribution. This paper considers a general setting in which the observations are contaminated
by both sparse and dense errors. We allow the corruptions to linearly grow with the number of
observations and have arbitrarily large magnitudes. We establish a general scaling of the quadruplet
(n  p  k  s) such that the extended Lasso stably recovers both the regression and corruption vectors.
Of particular interest to us are the following equations:

(a) First  under what scalings of (n  p  k  s) does the extended Lasso obtain the unique solution

with small estimation error.

(b) Second  under what scalings of (n  p  k) does the extended Lasso obtain the exact signed

support recovery even almost all the observations are corrupted?

3

(c) Third  under what scalings of (n  p  k  s) does no solution of the extended Lasso specify

the correct signed support?

  e(cid:63)T

To answer for the ﬁrst question  we introduce a notion of extended restricted eigenvalue for a matrix
[X  I] where I is an identity matrix. We show that this property satisﬁes for a general class of
random Gaussian design matrix. The answers to the last two questions requires stricter conditions
for the design matrix. In particular  for random Gaussian design matrix with i.i.d rows N (0  Σ)  we
rely on two standard assumptions: invertibility and mutual incoherence.
If we denote Z = [X  I] where I is an identity matrix and β = [β(cid:63)T
]T   then the observation
vector y is reformulated as y = Zβ + w  which is the same as standard Lasso model. However 
previous results [2] [17] applying to random Gaussian design matrix are irrelevant to this setting
since the Z no longer behave like a Gaussian matrix. To establish theoretical analysis  we need
more study on the interaction between the Gaussian and identity matrices. By exploiting the fact
that the matrix Z consists of two component where one component has special structure  our analysis
reveals an interesting phenomenon: extended Lasso can accurately recover both the regressor β(cid:63) and
corruption e(cid:63) even when the fraction of corruption is up to 100%. We measure the recoverability of
these variables under two criterions: parameter accuracy and feature selection accuracy. Moreover 
our analysis can be extended to the situation in which the identity matrix can be replaced by a tight
frame D as well as extended to other models such as group Lasso or matrix Lasso with sparse error.
Notation We summarize here some standard notation used throughout the paper. We reserve T
and S as the sparse support of β(cid:63) and e(cid:63)  respectively. Given and design matrix X ∈ Rn×p and
subsets S and T   we use XST to denote the |S| × |T| submatrix obtained by extracting those rows
indexed by S and columns indexed by T . We use the notation C1  C2  c1  c2  etc.  to refer to positive
constants  whose value may change from line to line. Given two functions f and g  the notation
f (n) = O(g(n)) means that there exists a constant c < +∞ such that f (n) ≤ cg(n); the notation
f (n) = Ω(g(n)) means that f (n) ≥ cg(n) and the notation f (n) = Θ(g(n)) means that f (n) =
(g(n)) and f (n) = Ω(g(n)). The symbol f (n) = o(g(n)) means that f (n)/g(n) → 0.

2 Main results

In this section  we provide precise statements of the main results of this paper. In the ﬁrst sub-
section  we establish the parameter estimation and provide a deterministic result which bases on the
notion of extended restricted eigenvalue. We further show that the random Gaussian design matrix
satisﬁes this property with high probability. The next sub-section considers the feature estimation.
We establish conditions for the design matrix such that the solution of the extended Lasso has the
exact signed supports.

2.1 Parameter estimation

As in conventional Lasso  to obtain a low parameter estimation bound  it is necessary to impose
In this paper  we introduce a notion of extended restricted
conditions on the design matrix X.
eigenvalue (extended RE) condition. Let C be a restricted set  we say that the matrix X satisﬁes the
extended RE assumption over the set C if there exists some κl > 0 such that
for all (h  f ) ∈ C 

(cid:107)Xh + f(cid:107)2 ≥ κl((cid:107)h(cid:107)2 + (cid:107)f(cid:107)2)

(7)

where the restricted set C of interest is deﬁned with λn := λe/λβ as follow

C := {(h  f ) ∈ Rp × Rn | (cid:107)hT c(cid:107)1 + λn (cid:107)fSc(cid:107)1 ≤ 3(cid:107)hT(cid:107)1 + 3λn (cid:107)fS(cid:107)1}.

(8)

This assumption is a natural extension of the restricted eigenvalue condition and restricted strong
convexity considered in [2] [14] and [12]. In the absent of a vector f in the equation (7) and in the
set C  this condition returns to the restricted eigenvalue deﬁned in [2]. As explained at more length
in [2] and [16]  restricted eigenvalue is among the weakest assumption on the design matrix such
that the solution of the Lasso is consistent.
With this assumption at hand  we now state the ﬁrst theorem

4

Theorem 1. Consider the optimal solution ((cid:98)β (cid:98)e) to the optimization problem (4) with regularization

parameters chosen as

λβ ≥ 2
γ

(9)
where γ ∈ (0  1]. Assuming that the design matrix X obeys the extended RE  then the error set

= γ

 

(h  f ) = ((cid:98)β − β(cid:63) (cid:98)e − e(cid:63)) is bounded by

λe
λβ

(cid:107)X∗w(cid:107)∞ and λn :=
(cid:16)

(cid:107)h(cid:107)2 + (cid:107)f(cid:107)2 ≤ 3κ−2

λβ

l

√

k + λe

s

.

(cid:107)w(cid:107)∞
(cid:107)X∗w(cid:107)∞
(cid:17)

√

(10)

γ

(cid:112)σ2 log p and λe ≥ 4(cid:112)σ2 log n.

that with high probability  (cid:107)X∗w(cid:107)∞ ≤ 2(cid:112)σ2 log p and (cid:107)w(cid:107)∞ ≤ 2(cid:112)σ2 log n. Thus  it is sufﬁcient

There are several interesting observations from this theorem
1) The error bound naturally split into two components related to the sparsity indices of β(cid:63) and e(cid:63).
In addition  the error bound contains three quantity: the sparsity indices  regularization parameters
and the extended RE constant. If the terms related to the corruption e(cid:63) are omitted  then we obtain
similar parameter estimation bound as the standard Lasso [2] [12].
2) The choice of regularization parameters λβ and λe can make explicitly: assuming w is a Gaussian
random vector whose entries are N (0  σ2) and the design matrix has unit-normed columns  it is clear
to select λβ ≥ 4
3) At the ﬁrst glance  the parameter γ does not seem to have any meaningful interpretation and the
γ = 1 seems to be the best selection due to the smallest estimation error it can produce. However 
this parameter actually control the sparsity level of the regression vector with respect to the fraction
of corruption. This relation is made via the restricted set C.
In the following lemma  we show that the extended RE condition actually exists for a large class of
random Gaussian design matrix whose rows are i.i.d zero mean with covariance Σ. Before stating the
lemma  let us deﬁne some quantities operating on the covariance matrix Σ: Cmin := λmin(Σ) is the
smallest eigenvalue of Σ  Cmax := λmax(Σ) is the biggest eigenvalue of Σ and ξ(Σ) := maxi Σii
is the maximal entry on the diagonal of the matrix Σ.
Lemma 1. Consider the random Gaussian design matrix whose rows are i.i.d N (0  Σ) and assume
n2Cmaxξ(Σ) = Θ(1). Select

(cid:115)

γ(cid:112)ξ(Σ)n

log n
log p

 

λn :=

(11)
then with probability greater than 1 − c1 exp(−c2n)  the matrix X satisﬁes the extended RE with
√
parameter κl = 1
for some
4
small constants C1  C2.

  provided that n ≥ C ξ(Σ)

k log p and s ≤ min

γ2 log n   C2n

(cid:110)

(cid:111)

Cmin

C1

n

2

dent with the Gaussian stochastic noise w  we can easily show that (cid:107)X∗w(cid:107)∞ ≤ 2(cid:112)ξ(Σ)nδ2 log p

We would like to make some remarks:
1) The choice of parameter λn is nothing special here. When design matrix is Gaussian and indepen-
with probability at least 1− 2 exp(− log p). Therefore  the selection of λn follows from Theorem 1.
2) The proof of this lemma  shown in the Appendix  boils down to control two terms

• Restricted eigenvalue with X.
2 + (cid:107)f(cid:107)2

(cid:107)Xh(cid:107)2

2 ≥ κr((cid:107)h(cid:107)2

2 + (cid:107)f(cid:107)2
2)

for all

(h  f ) ∈ C.

• Mutual incoherence. Column space of the matrix X is incoherent with the column space

of the identity matrix. That is  there exists some κm > 0 such that

|(cid:104)Xh  f(cid:105)| ≤ κm((cid:107)h(cid:107)2 + (cid:107)f(cid:107)2)2

for all

(h  f ) ∈ C.

If the incoherence between these two column spaces is sufﬁciently small such that 4κm < κr  then
we can conclude that (cid:107)Xh + f(cid:107)2
2 ≥ (κr − 2κm)((cid:107)h(cid:107)2 + (cid:107)f(cid:107)2)2. The small mutual incoherence

5

property is especially important since it provides how the regression separates away from the sparse
error.
3) To simplify our result  we consider a special case of the uniform Gaussian design  in which
n Ip×p. In this situation  Cmin = Cmax = ξ(Σ) = 1/n. We have the following result which is
Σ = 1
a corollary of Theorem 1 and Lemma 1
Corollary 1 (Standard Gaussian design). Let X be a standard Gaussian design matrix. Consider

the optimal solution ((cid:98)β (cid:98)e) to the optimization problem (4) with regularization parameters chosen as

λβ ≥ 4
γ

σ2 log p and λe ≥ 4

(12)
γ2 log n   C2n} for some small
where γ ∈ (0  1]. Also assuming that n ≥ Ck log p and s ≤ min{C1
constants C1  C2. Then with probability greater than 1 − c1 exp(−c2n)  the error set (h  f ) =
(cid:19)

((cid:98)β − β(cid:63) (cid:98)e − e(cid:63)) is bounded by

σ2 log n 

n

(cid:112)

(cid:112)

(cid:112)

(cid:107)h(cid:107)2 + (cid:107)f(cid:107)2 ≤ 384

σ2k log p +

σ2s log n

 

(13)

(cid:18) 1

(cid:112)

γ

√

Corollary 1 reveals an interesting phenomenon: by setting γ = 1/
log n  even when the fraction
of corruption is linearly proportional with the number of samples n  the extended Lasso (4) is still
capable to recover both coefﬁcient vector β(cid:63) and corruption (missing) vector e(cid:63) within a bounded
error (13). Without the dense noise w in the observation model (3) (σ = 0)  the extended Lasso
recovers the exact solution. This result is impossible to achieve with standard Lasso. Furthermore  if
we know in prior that the number of corrupted observations is an order of O(n/ log p)  then selecting
γ = 1 instead of 1/ log n will minimize the estimation error (see equation (13)) of Theorem 1.

2.2 Feature selection with random Gaussian design

In many applications  the feature selection criteria is more preferred [17] [23]. Feature selection
refers to the property that the recovered parameter has the same signed support as the true regressor.
In general  good feature selection implies good parameter estimation but the reverse direction does
not usually hold. In this part  we investigate conditions for the design matrix and the scaling of
(n  p  k  s) such as both regression and sparse error vectors obtain this criteria.
Consider the linear model (3) where X is the Gaussian random design matrix whose rows are i.i.d
zero mean with covariance matrix Σ. It has been well known in the Lasso that in order to obtain
feature selection accuracy  the covariance matrix Σ must obey two properties: invertibility and small
mutual coherence restricted on the set T . The ﬁrst property guarantees that (4) is strictly convex 
leading to the unique solution of the convex program  while the second property requires the sepa-
ration between two components of Σ  one related to the set T and the other to the set T c must be
sufﬁciently small.

1. Invertibility. To guarantee uniqueness  we require ΣT T to be invertible. Particularly  let
Cmin = λmin(ΣT T )  we require Cmin > 0.
2. Mutual incoherence. For some γ ∈ (0  1) 

(cid:13)(cid:13)Σ∗
T cT (ΣT T )−1(cid:13)(cid:13)∞ ≤ 1

(1 − γ)

(14)
where (cid:107)·(cid:107)∞ refers to (cid:96)∞/(cid:96)∞ operator norm. It is worth noting that in the standard Lasso
the factor 1
2 is omitted. Our condition is tighter than condition used to establish feature
estimation in the Lasso by a constant factor. In fact  the quantity 1/2 is nothing special
here and we can set any value close to one with a compensation that the number of samples
n will increase. Thus  we put 1/2 for the simplicity of the proof.

2

Toward the end  we will also elaborate three other quantities operating on the restricted co-
variance matrix ΣT T : Cmax  which is deﬁned as the maximum eigenvalue of ΣT T : Cmax :=
λmax(ΣT T ); D−
T T and ΣT T :
D−

max  which are denoted as (cid:96)∞-norm of matrices Σ−1

max := (cid:107)ΣT T(cid:107)∞.

max :=(cid:13)(cid:13)(ΣT T )−1(cid:13)(cid:13)∞ and D+

max and D+

6

ΣT c|T := ΣT cT c − ΣT cT Σ−1

Our result also involves in two other quantities operating on the conditional covariance matrix of
(XT c|XT ) deﬁned as

T T ΣT T c.

(15)
2 mini(cid:54)=j[(ΣT c|T )ii + (ΣT c|T )jj −

We then deﬁne ρu(ΣT c|T ) = maxi(ΣT c|T )ii and ρl(ΣT c|T ) = 1
2(ΣT c|T )ij]. Toward the end  we denote a shorthand ρu and ρl.
We establish the following result for Gaussian random design whose covariance matrix Σ obeys the
two assumptions.
Theorem 2. (Achievability) Given the linear model (3) with random Gaussian design and the co-
variance matrix Σ satisfy invertibility and incoherence properties for any γ ∈ (0  1)  suppose we
solve the extended Lasso (4) with regularization parameters obeying

max{ρu  D+

max}nσ2 log p

and

λe = 8

σ2 log n.

(16)

(cid:112)

(cid:113)

λβ =

4
γ

1

32γ2 log n   the sequence (n  p  k  s) and regularization parameters λβ  λe satisfying

Also  let η =
s ≤ ηn

n ≥ max

(cid:26)

(cid:27)

k log(p − k) log n

 
(17)
i | > fβ(λβ) and

and

(18)

C1

1

(1 − η)

ρu
Cmin

k log(p − k)  C2

η

(1 − η)2

max{ρu  D+

max}

Cmin

where C1 and C2 are numerical constants. In addition  suppose that mini∈T |β(cid:63)
mini∈S |e(cid:63)

(cid:114)
i | > fe(λβ  λe) where

fβ := c1

λβ
n − s

(cid:13)(cid:13)(cid:13)Σ

−1/2
T T

(cid:13)(cid:13)(cid:13)2
(cid:114)

k log(p − k)

n
√
s + s

√

(cid:115)

∞ + 20
k log(p − k)

σ2 log k

Cmin(n − s)

(cid:13)(cid:13)(cid:13)Σ

−1/2
T T

(cid:13)(cid:13)(cid:13)2

fe := c2(Cmax(k

(19)
Then the following properties holds with probability greater than 1−c exp(−c(cid:48) max{log n  log pk})

1. The solution pair ((cid:98)β (cid:98)e) of the extended Lasso (4) is unique and has exact signed support.

∞ + c3λe.

n

k))1/2 λβ
n − s

(cid:13)(cid:13)(cid:13)(cid:98)β − β(cid:63)(cid:13)(cid:13)(cid:13)∞

≤ fβ(λβ) and (cid:107)(cid:98)e − e(cid:63)(cid:107)∞ ≤ fe(λβ).

2. (cid:96)∞-norm bounds:

There are several interesting observations from the theorem
1) The ﬁrst and important observation is that extended Lasso is robust to arbitrarily large and sparse
error observation. In that sense  the extended Lasso can be viewed as a generalization of the Lasso.
Under the same invertibility and mutual incoherence assumptions on the covariance matrix Σ as
the standard Lasso  the extended Lasso program can recover both the regression vector and error
with exact signed supports even when almost all the observations are contaminated by arbitrarily
large error with unknown support. What we sacriﬁce for the corruption robustness is an additional
log factor to the number of samples. We notice that when the error fraction is O(n/ log n)  only
O(k log(p − k)) samples are sufﬁcient to recover the exact signed supports of both regression and
sparse error vectors.
2) We consider the special case with Gaussian random design in which the covariance matrix Σ =
In this case  entries of X is i.i.d N (0  1/n) and we have quantities Cmin = Cmax =
n Ip×p.
1
max = D−
max = ρu = ρl = 1. In addition  the invertibility and mutual incoherence properties
D+
are automatically satisﬁed. The theorem implies that when the number of errors s is close to n 
log n = Ω(k log(p −
the number of samples n needed to recover exact signed supports satisﬁes
k)). Furthermore  Theorem 2 guarantees consistency in element-wise (cid:96)∞-norm of the estimated
regression at the rate

(cid:18)(cid:112)σ2 log p

(cid:113) k log(p−k)

(cid:13)(cid:13)(cid:13)(cid:98)β − β(cid:63)(cid:13)(cid:13)(cid:13)∞ = O

(cid:19)

γ2n

n

.

√

As γ is chosen to be 1/
of O(σ

√

log p)  which is known to be the same as that of the standard Lasso.

32 log n (equivalent to establish s close to n)  the (cid:96)∞ error rate is an order

7

3) Corollary 1  though interesting  is not able to guarantee stable recovery when the fraction of
corruption converges to one. We show in Theorem 2 that this fraction can come arbitrarily close
to one by sacriﬁcing a factor of log n for the number of samples. Theorem 2 also implies that
there is a signiﬁcant difference between recovery to obtain small parameter estimation error versus
recovery to obtain correct variable selection. When the amount of corrupted observations is linearly
proportional with n  recovering the exact signed supports require an increase from Ω(k log p) (in
Corollary 1) to Ω(k log p log n) samples (in Theorem 2). This behavior is captured similarly by the
standard Lasso  as pointed out in [17]  Corollary 2.
Our next theorem show that the number of samples needed to recover accurate signed support is
optimal. That is  whenever the rescaled sample size satisﬁes (20)  then for whatever regularization
parameters λβ and λe are selected  no solution of the extended Lasso correctly identiﬁes the signed
supports with high probability.
Theorem 3. (Inachievability) Given the linear model (3) with random Gaussian design and the
covariance matrix Σ satisfy invertibility and incoherence properties for any γ ∈ (0  1). Let η =
32γ2 log(n−s) and the sequence (n  p  k  s) satisﬁes s ≥ ηn and

1

C3

(cid:32)

(cid:112)σ2 log n

λe

(cid:33)−1  

n ≤ min

1

(1 − η)

ρu
Cmin

k log(p − k)  C4

η

(1 − η)2

min{ρl  D+
Cmax

max}

k log(p − k) log(1 − η)n

1 +

(20)
where C3 and C4 are some small universal constants. Then with probability tending to one  no
solution pair of the extended Lasso (5) has the correct signed support.

3

Illustrative simulations

In this section  we provide some simulations to illustrate the possibility of the extended Lasso in
recovering the exact regression signed support when a signiﬁcant fraction of observations is cor-
rupted by large error. Simulations are performed for a range of parameters (n  p  k  s) where the
design matrix X is uniform Gaussian random whose rows are i.i.d N (0  Ip×p). For each ﬁxed set of
(n  p  k  s)  we generate sparse vectors β(cid:63) and e(cid:63) where locations of nonzero entries are uniformly
random and magnitudes are Gaussian distributed.
In our experiments  we consider varying problem sizes p = {128  256  512} and three types of
regression sparsity indices: sublinear sparsity (k = 0.2p/ log(0.2p))  linear sparsity (k = 0.1p) and
fractional power sparsity (k = 0.5p0.75). In all cases  we ﬁxed the error support size s = n/2.
This means half of the observations is corrupted. By this selection  Theorem 2 suggests that number
of samples n ≥ 2Ck log(p − k) log n to guarantee exact signed support recovery. We choose
log n = 4θk log(p − k) where parameter θ is the rescaled sample size. This parameter control the
success/failure of the extended Lasso.

In the algorithm  we select λβ = 2(cid:112)σ2 log p log n and λe = 2(cid:112)σ2 log n as suggested by Theorem

2  where the noise level σ = 0.1 is ﬁxed. The algorithm reports a success if the solution pair has
the same signed support as (β(cid:63)  e(cid:63)). In Fig. 1  each point on the curve represents the average of 100
trials.
As demonstrated by simulations  our extended Lasso is cable to recover the exact signed support
of both β(cid:63) and e(cid:63) even 50% of the observations are contaminated. Furthermore  up to unknown
log n ≤ 2k log(p−
constants  our theorem 2 and 3 match with simulation results. As the sample size n
k)  the probability of success starts going to zero  implying the failure of the extended Lasso.

n

Acknowledgments

We acknowledge support from the Army Research Ofﬁce (ARO) under Grant 60291-MA and Na-
tional Science Foundation (NSF) under Grant CCF-1117545.

References
[1] A. Agarwal  S. Negahban  and M. Wainwright. Noisy matrix decomposition via convex relaxation: Opti-
mal rates in high dimensions. Proc. 28th Inter. Conf. Mach. Learn. (ICML-11)  pages 1129–1136  2011.

8

Figure 1: Probability of success in recovering the signed supports

[2] P. Bickel  Y. Ritov  and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of

statistics  37(4):1705–1732  2009.

[3] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Submitted for publica-

tion  2009.

[4] E. J. Cand`es and Y. Plan. Near-ideal model selection by l1 minimization. Annals of Statistics  37:2145–

2177  2009.

[5] E. J. Cand`es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals

of statistics  35(6):2313–2351  2007.

[6] E. Elhamifar and R. Vidal. Sparse subspace clustering. IEEE Conference on Computer Vision and Pattern

Recognition (CVPR)  pages 2790–2797  2009.

[7] J. N. Laska  M. A. Davenport  and R. G. Baraniuk. Exact signal recovery from sparsely corrupted mea-
surements through the pursuit of justice. In Asilomar conference on Signals  Systems and Computers 
pages 1556–1560  2009.

[8] X. Li. Compressed sensing and matrix completion with constant proportion of corruptions. Preprint 

2011.

[9] Z. Li  F. Wu  and J. Wright. On the systematic measurement matrix for compressed sensing in the presence

of gross error. In Data compression conference (DCC)  pages 356–365  2010.

[10] N. Meinshausen and P. Buehlmann. High dimensional graphs and variable selection with the lasso. Annals

of statistics  34(3):1436–1462  2008.

[11] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Annals of statistics  37(1):2246–2270  2009.

[12] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Preprint  2010.

[13] N. H. Nguyen and Trac. D. Tran. Exact recoverability from dense corrupted observations via l1 mini-

mization. preprint  2010.

[14] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue properties for correlated gaussian designs.

Journal of Machine Learning Research  11:2241–2259  2010.

[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B  58(1):267–288  1996.

[16] S. A. van de Geer and P. Buehlmann. On the conditions used to prove oracle results for the lasso. Elec-

tronic Journal of Statistics  3(1360-1392)  2009.

[17] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1 -constrained

quadratic programming ( lasso ). IEEE Trans. Information Theory  55(5):2183–2202  2009.

[18] J. Wright and Y. Ma. Dense error correction via l1 minimization.

Theory  56(7):3540–3560  2010.

IEEE Transaction on Information

[19] J. Wright  A. Y. Yang  A. Ganesh  S. S. Sastry  and Y. Ma. Robust face recognition via sparse representa-

tion. IEEE Transaction on Pattern Analysis and Machine Intelligence  31(2):210–227  2009.

[20] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. Ad. Neural Infor. Proc. Sys. (NIPS) 

pages 2496–2504  2010.

[21] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the

Royal Statistical Society: Series B  68(1):49–67  2006.

[22] T. Zhang. Some sharp performance bounds for least squares regression with l1 regularization. Annals of

statistics  37(5):2109–2144  2009.

[23] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research 

7:2541–2563  2006.

9

00.20.40.60.8100.20.40.60.81Rescaled sample size θProbability of successSublinear sparsity  p=128p=256p=51200.20.40.60.8100.20.40.60.81Rescaled sample size θProbability of successLinear sparsity  p=128p=256p=51200.20.40.60.8100.20.40.60.81Rescaled sample size θProbability of successFractional power sparsity  p=128p=256p=512,Soumyadeep Chatterjee
Sheng Chen
Arindam Banerjee
Ariel Procaccia
Nisarg Shah