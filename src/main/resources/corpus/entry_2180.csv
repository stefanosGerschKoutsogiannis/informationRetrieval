2019,Fully Parameterized Quantile Function for Distributional Reinforcement Learning,Distributional Reinforcement Learning (RL) differs from traditional RL in that  rather than the expectation of total returns  it estimates distributions and has achieved state-of-the-art performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function  leaving the other side uniformly fixed as in C51  QR-DQN or randomly sampled as in IQN. In this paper  we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e.  the x-axis) and the value axis (i.e.  y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly outperforms existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents.,Fully Parameterized Quantile Function for

Distributional Reinforcement Learning

Derek Yang∗
UC San Diego

dyang1206@gmail.com

Li Zhao

Microsoft Research

lizo@microsoft.com

Zichuan Lin

Tsinghua University

linzc16@mails.tsinghua.edu.cn

Tao Qin

Microsoft Research

taoqin@microsoft.com

Jiang Bian

Microsoft Research

jiang.bian@microsoft.com

Tieyan Liu

Microsoft Research

tyliu@microsoft.com

Abstract

Distributional Reinforcement Learning (RL) differs from traditional RL in that 
rather than the expectation of total returns  it estimates distributions and has
achieved state-of-the-art performance on Atari Games. The key challenge in
practical distributional RL algorithms lies in how to parameterize estimated dis-
tributions so as to better approximate the true continuous distribution. Existing
distributional RL algorithms parameterize either the probability side or the return
value side of the distribution function  leaving the other side uniformly ﬁxed as in
C51  QR-DQN or randomly sampled as in IQN. In this paper  we propose fully
parameterized quantile function that parameterizes both the quantile fraction axis
(i.e.  the x-axis) and the value axis (i.e.  y-axis) for distributional RL. Our algo-
rithm contains a fraction proposal network that generates a discrete set of quantile
fractions and a quantile value network that gives corresponding quantile values.
The two networks are jointly trained to ﬁnd the best approximation of the true
distribution. Experiments on 55 Atari Games show that our algorithm signiﬁcantly
outperforms existing distributional RL algorithms and creates a new record for the
Atari Learning Environment for non-distributed agents.

1

Introduction

Distributional reinforcement learning [Jaquette et al.  1973  Sobel  1982  White  1988  Morimura
et al.  2010  Bellemare et al.  2017] differs from value-based reinforcement learning in that  instead
of focusing only on the expectation of the return  distributional reinforcement learning also takes
the intrinsic randomness of returns within the framework into consideration [Bellemare et al.  2017 
Dabney et al.  2018b a  Rowland et al.  2018]. The randomness comes from both the environment
itself and agent’s policy. Distributional RL algorithms characterize the total return as random variable
and estimate the distribution of such random variable  while traditional Q-learning algorithms estimate
only the mean (i.e.  traditional value function) of such random variable.
The main challenge of distributional RL algorithm is how to parameterize and approximate the
distribution. In Categorical DQN [Bellemare et al.  2017](C51)  the possible returns are limited to
a discrete set of ﬁxed values  and the probability of each value is learned through interacting with
environments. C51 out-performs all previous variants of DQN on a set of 57 Atari 2600 games in the
Arcade Learning Environment (ALE) [Bellemare et al.  2013]. Another approach for distributional
reinforcement learning is to estimate the quantile values instead. Dabney et al. [2018b] proposed QR-

∗Contributed during internship at Microsoft Research.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

DQN to compute the return quantiles on ﬁxed  uniform quantile fractions using quantile regression
and minimize the quantile Huber loss [Huber  1964] between the Bellman updated distribution and
current return distribution. Unlike C51  QR-DQN has no restrictions or bound for value and achieves
signiﬁcant improvements over C51. However  both C51 and QR-DQN approximate the distribution
function or quantile function on ﬁxed locations  either value or probability. Dabney et al. [2018a]
propose learning the quantile values for sampled quantile fractions rather than ﬁxed ones with an
implicit quantile value network (IQN) that maps from quantile fractions to quantile values. With
sufﬁcient network capacity and inﬁnite number of quantiles  IQN is able to approximate the full
quantile function.
However  it is impossible to have inﬁnite quantiles in practice. With limited number of quantile
fractions  efﬁciency and effectiveness of the samples must be reconsidered. The sampling method
in IQN mainly helps training the implicit quantile value network rather than approximating the full
quantile function  and thus there is no guarantee in that sampled probabilities would provide better
quantile function approximation than ﬁxed probabilities.
In this work  we extend the method in Dabney et al. [2018b] and Dabney et al. [2018a] and propose
to fully parameterize the quantile function. By fully parameterization  we mean that unlike QR-DQN
and IQN where quantile fractions are ﬁxed or sampled and only the corresponding quantile values
are parameterized  both quantile fractions and corresponding quantile values in our algorithm are
parameterized. In addition to a quantile value network similar to IQN that maps quantile fractions
to corresponding quantile values  we propose a fraction proposal network that generates quantile
fractions for each state-action pair. The fraction proposal network is trained so that as the true
distribution is approximated  the 1-Wasserstein distance between the approximated distribution and
the true distribution is minimized. Given the proposed fractions generated by the fraction proposal
network  we can learn the quantile value network by quantile regression. With self-adjusting fractions 
we can approximate the true distribution better than with ﬁxed or sampled fractions.
We begin with related works and backgrounds of distributional RL in Section 2. We describe
our algorithm in Section 3 and provide experiment results of our algorithm on the ALE environ-
ment [Bellemare et al.  2013] in Section 4. At last  we discuss the future extension of our work  and
conclude our work in Section 5.

2 Background and Related Work

We consider the standard reinforcement learning setting where agent-environment interactions are
modeled as a Markov Decision Process (X  A  R  P  γ) [Puterman  1994]  where X and A denote
state space and action space  P denotes the transition probability given state and action  R denotes
state and action dependent reward function and γ ∈ (0  1) denotes the reward discount factor.

For a policy π  deﬁne the discounted return sum a random variable by Z π(x  a) =(cid:80)∞

t=0 γtR(xt  at) 
where x0 = x  a0 = a  xt ∼ P (·|xt−1  at−1) and at ∼ π(·|xt). The objective in reinforcement
learning can be summarized as ﬁnding the optimal π∗ that maximizes the expectation of Z π  the
action-value function Qπ(x  a) = E[Z π(x  a)]. The most common approach is to ﬁnd the unique
ﬁxed point of the Bellman optimality operator T [Bellman  1957]:

Q∗(x  a) = T Q∗(x  a) := E[R(x  a)] + γEP max

a(cid:48) Q∗ (x(cid:48)  a(cid:48)) .

To update Q  which is approximated by a neural network in most deep reinforcement learning
studies  Q-learning [Watkins  1989] iteratively trains the network by minimizing the squared temporal
difference (TD) error deﬁned by

(cid:20)

(cid:21)2

δ2
t =

rt + γ max

a(cid:48)∈A Q (xt+1  a(cid:48)) − Q (xt  at)

along the trajectory observed while the agent interacts with the environment following -greedy
policy. DQN [Mnih et al.  2015] uses a convolutional neural network to represent Q and achieves
human-level play on the Atari-57 benchmark.

2

2.1 Distributional RL

Instead of a scalar Qπ(x  a)  distributional RL looks into the intrinsic randomness of Z π by studying
its distribution. The distributional Bellman operator for policy evaluation is

Z π(x  a) D= R(x  a) + γZ π (X(cid:48)  A(cid:48))  

where X(cid:48) ∼ P (·|x  a) and A(cid:48) ∼ π(·|X(cid:48))  A D= B denotes that random variable A and B follow the
same distribution.
Both theory and algorithms have been established for distributional RL. In theory  the distribu-
tional Bellman operator for policy evaluation is proved to be a contraction in the p-Wasserstein
distance [Bellemare et al.  2017]. Bellemare et al. [2017] shows that C51 outperforms value-based
RL  in addition Hessel et al. [2018] combined C51 with enhancements such as prioritized experience
replay [Schaul et al.  2016]  n-step updates [Sutton  1988]  and the dueling architecture [Wang et al. 
2016]  leading to the Rainbow agent  current state-of-the-art in Atari-57 for non-distributed agents 
while the distributed algorithm proposed by Kapturowski et al. [2018] achieves state-of-the-art per-
formance for all agents. From an algorithmic perspective  it is impossible to represent the full space
of probability distributions with a ﬁnite collection of parameters. Therefore the parameterization of
quantile functions is usually the most crucial part in a general distributional RL algorithm. In C51 
the true distribution is projected to a categorical distribution [Bellemare et al.  2017] with ﬁxed values
for parameterization. QR-DQN ﬁxes probabilities instead of values  and parameterizes the quantile
values [Dabney et al.  2018a] while IQN randomly samples the probabilities [Dabney et al.  2018a].
We will introduce QR-DQN and IQN in Section 2.2  and extend from their work to ours.

2.2 Quantile Regression for Distributional RL

In contrast to C51 which estimates probabilities for N ﬁxed locations in return  QR-DQN [Dabney
et al.  2018b] estimates the respected quantile values for N ﬁxed  uniform probabilities. In QR-DQN 
the distribution of the random return is approximated by a uniform mixture of N Diracs 

N(cid:88)

i=1

Zθ(x  a) :=

1
N

δθi(x a) 

with each θi assigned a quantile value trained with quantile regression.
Based on QR-DQN  Dabney et al. [2018a] propose using probabilities sampled from a base distribu-
tion  e.g. τ ∈ U ([0  1])  rather than ﬁxed probabilities. They further learn the quantile function that
maps from embeddings of sampled probabilities to the corresponding quantiles  called implicit quan-
tile value network (IQN). At the time of this writing  IQN achieves the state-or-the-art performance
on Atari-57 benchmark  human-normalized mean and median of all agents that does not combine
distributed RL  prioritized replay [Schaul et al.  2016] and n-step update.
Dabney et al. [2018a] claimed that with enough network capacity  IQN is able to approximate to
the full quantile function with inﬁnite number of quantile fractions. However  in practice one needs
to use a ﬁnite number of quantile fractions to estimate action values for decision making  e.g. 32
randomly sampled quantile fractions as in Dabney et al. [2018a]. With limited fractions  a natural
question arises that  how to best utilize those fractions to ﬁnd the closest approximation of the true
distribution?

3 Our Algorithm

We propose Fully parameterized Quantile Function (FQF) for Distributional RL. Our algorithm
consists of two networks  the fraction proposal network that generates a set of quantile fractions
for each state-action pair  and the quantile value network that maps probabilities to quantile values.
We ﬁrst describe the fully parameterized quantile function in Section 3.1  with variables on both
probability axis and value axis. Then  we show how to train the fraction proposal network in Section
3.2  and how to train the quantile value network with quantile regression in Section 3.3. Finally  we
present our algorithm and describe the implementation details in Section 3.4.

3

3.1 Fully Parameterized Quantile Function

In FQF  we estimate N adjustable quantile values for N adjustable quantile fractions to approximate
the quantile function. The distribution of the return is approximated by a weighted mixture of N
Diracs given by

Zθ τ (x  a) :=

(1)
where δz denotes a Dirac at z ∈ R  τ1  ...τN−1 represent the N-1 adjustable fractions satisfying
τi−1 < τi  with τ0 = 0 and τN = 1 to simplify notation. Denote quantile function [Müller  1997]
F −1
Z the inverse function of cumulative distribution function FZ(z) = P r(Z < z). By deﬁnition we
have

(τi+1 − τi)δθi(x a) 

N−1(cid:88)

i=0

Z (p) := inf {z ∈ R : p ≤ FZ(z)}
F −1

where p is what we refer to as quantile fraction.
Based on the distribution in Eq.(1)  denote Πθ τ the projection operator that projects quantile function
onto a staircase function supported by θ and τ  the projected quantile function is given by

−1 θ τ
Z

F

(ω) = Πθ τ F −1

Z (ω) = θ0 +

N−1(cid:88)
(θi+1 − θi)Hτi+1 (ω) 

i=0

where H is the Heaviside step function and Hτ (ω) is the short for H(ω − τ ). Figure 1 gives an
example of such projection. For each state-action pair (x  a)  we ﬁrst generate the set of fractions τ
using the fraction proposal network  and then obtain the quantiles values θ corresponding to τ using
the quantile value network.
To measure the distortion between approximated quantile function and the true quantile function  we
use the 1-Wasserstein metric given by

(cid:90) τi+1

N−1(cid:88)

i=0

τi

(cid:12)(cid:12)F −1

Z (ω) − θi

(cid:12)(cid:12) dω.

W1(Z  θ  τ ) =

(2)

Unlike KL divergence used in C51 which considers only the probabilities of the outcomes  the
p-Wasseretein metric takes both the probability and the distance between outcomes into consideration.
Figure 1 illustrates the concept of how different approximations could affect W1 error  and shows an
example of ΠW1. However  note that in practice Eq.(2) can not be obtained without bias.

(a)

(b)

Figure 1: Two approximations of the same quantile function using different set of τ with N = 6  the
area of the shaded region is equal to the 1-Wasserstein error. (a) Finely-adjusted τ with minimized
W1 error. (b) Randomly chosen τ with larger W1 error.

4

3.2 Training fraction proposal Network

To achieve minimal 1-Wasserstein error  we start from ﬁxing τ and ﬁnding the optimal corresponding
quantile values θ. In QR-DQN  Dabney et al. [2018a] gives an explicit form of θ to achieve the goal.
We extend it to our setting:
Lemma 1. [Dabney et al.  2018a] For any τ1  ...τN−1 ∈ [0  1] satisfying τi−1 < τi for i  with τ1 = 0
and τN = 1  and cumulative distribution function F with inverse F −1  the set of θ minimizing Eq.(2)
is given by

θi = F −1
Z (

τi + τi+1

2

)

(3)

We can now substitute θi in Eq.(2) with equation Eq.(3) and ﬁnd the optimal condition for τ to
minimize W1(Z  τ ). For simplicity  we denote ˆτi = τi+τi+1
.
Proposition 1. For any continuous quantile function F −1
Wasserstein loss of F −1

Z that is non-decreasing  deﬁne the 1-

by

2

Z and F

−1 τ
Z

(cid:90) τi+1

N−1(cid:88)

i=0

τi

(cid:12)(cid:12)F −1

Z (ˆτi)(cid:12)(cid:12) dω.

Z (ω) − F −1

W1(Z  τ ) =

(4)

(5)

= 0.

∂W1
∂τi

is given by

∂W1
∂τi

= 2F −1

Z (τi) − F −1

Z (ˆτi) − F −1

Z (ˆτi−1) 

∀i ∈ (0  N ).
Further more  ∀τi−1  τi+1 ∈ [0  1]  τi−1 < τi+1  ∃τi ∈ (τi−1  τi+1) s.t. ∂W1

∂τi

Proof of proposition 1 is given in the appendix. While computing W1 without bias is usually
impractical  equation 5 provides us with a way to minimize W1 without computing it. Let w1 be
the parameters of the fraction proposal network P   for an arbitrary quantile function F −1
Z   we can
minimize W1 by iteratively applying gradients descent to w1 according to Eq.(5) and convergence is
guaranteed. As the true quantile function F −1
Z is unknown to us in practice  we use the quantile value
network F −1
The expected return  also known as action-value based on FQF is then given by

with parameters w2 for current state and action as true quantile function.

Z w2

N−1(cid:88)

Q(x  a) =

(τi+1 − τi)F −1

Z w2

(ˆτi) 

where τ0 = 0 and τN = 1.

3.3 Training quantile value network

i=0

With the properly chosen probabilities  we combine quantile regression and distributional Bellman
update on the optimized probabilities to train the quantile function. Consider Z a random variable
denoting the action-value at (xt  at) and Z(cid:48) the action-value random variable at (xt+1  at+1)  the
weighted temporal difference (TD) error for two probabilities ˆτi and ˆτj is deﬁned by

ij = rt + γF −1
δt
Z(cid:48) w1

(ˆτi) − F −1

Z w1

(ˆτj)

(6)

Quantile regression is used in QR-DQN and IQN to stochastically adjust the quantile estimates so as
to minimize the Wasserstein distance to a target distribution. We follow QR-DQN and IQN where
quantile value networks are trained by minimizing the Huber quantile regression loss [Huber  1964] 
with threshold κ 

ρκ

τ (δij) = |τ − I{δij < 0}| Lκ (δij)
Lκ (δij) =

(cid:26) 1
κ(cid:0)|δij| − 1
2 κ(cid:1)  

  with
κ
if |δij| ≤ κ
otherwise

2 δ2
ij 

5

The loss of the quantile value network is then given by

N−1(cid:88)

N−1(cid:88)

ρκ
ˆτj

(δt

ij)

(7)

L(xt  at  rt  xt+1) =

1
N

i=0

j=0

Z and its Bellman target share the same proposed quantile fractions ˆτ to reduce compu-

Note that F −1
tation.
We perform joint gradient update for w1 and w2  as illustrated in Algorithm 1.

Algorithm 1: FQF update
Parameter :N  κ
Input: x  a  r  x(cid:48)  γ ∈ [0  1)
// Compute proposed fractions for x  a
τ ← Pw1(x);
// Compute proposed fractions for x(cid:48)  a(cid:48)
for a(cid:48) ∈ A do

i )F −1
Z(cid:48) w2

(ˆτi)a;

(ˆτi) − F −1

Z w2

(ˆτj)

τ(cid:48) ← Pw1(x(cid:48));

Q(s(cid:48)  a(cid:48)) ←(cid:80)N−1

end
// Compute greedy action
i+1 − τ(cid:48)

i=0 (τ(cid:48)
Q(s(cid:48)  a(cid:48));

a(cid:48)

a∗ ← argmax
// Compute L
for 0 ≤ i ≤ N − 1 do
for 0 ≤ j ≤ N − 1 do
δij ← r + γF −1
Z(cid:48) w2

N

i=0

(cid:80)N−1

(cid:80)N−1

end
end
L = 1
// Compute ∂W1
∂τi
∂W1
∂τi
Update w1 with ∂W1
∂τi
Output: Q

= 2F −1

Z w2

3.4

Implementation Details

(δij);

j=0 ρκ
ˆτj
for i ∈ [1  N − 1]
(τi) − F −1

(ˆτi) − F −1

Z w2

; Update w2 with ∇L;

Z w2

(ˆτi−1);

Our fraction proposal network is represented by one fully-connected MLP layer. It takes the state
embedding of original IQN as input and generates fraction proposal. Recall that in Proposition 1 
we require τi−1 < τi and τ0 = 0  τN = 1. While it is feasible to have τ0 = 0  τN = 1 ﬁxed and sort
the output of τw1  the sort operation would make the network hard to train. A more reasonable and
practical way would be to let the neural network automatically have the output sorted using cumulated
softmax. Let q ∈ RN denote the output of a softmax layer  we have qi ∈ (0  1)  i ∈ [0  N − 1] and
j=0 qj  i ∈ [0  N ]  then straightforwardly we have τi < τj for ∀i < j
and τ0 = 0  τN = 1 in our fraction proposal network. Note that as W1 is not computed  we can’t
directly perform gradient descent for the fraction proposal network. Instead  we use the grad_ys
argument in the tensorﬂow operator tf.gradients to assign ∂W1
to the optimizer. In addition  one
∂τi
i=0 qi log qi to prevent the distribution

i=0 qi = 1. Let τi =(cid:80)i−1
(cid:80)N−1
can use entropy of q as a regularization term H(q) = −(cid:80)N−1

from degenerating into a deterministic one.
We borrow the idea of implicit representations from IQN to our quantile value network. To be speciﬁc 
we compute the embedding of τ  denoted by φ(τ )  with

(cid:33)

φj(τ ) := ReLU

cos(iπτ )wij + bj

 

(cid:32)n−1(cid:88)

i=0

6

Figure 2: Performance comparison with IQN. Each training curve is averaged by 3 seeds. The
training curves are smoothed with a moving average of 10 to improve readability.

(ψ(x) (cid:12) φ(τ )).

where wij and bj are network parameters. We then compute the element-wise (Hadamard) product of
state feature ψ(x) and embedding φ(τ ). Let (cid:12) denote element-wise product  the quantile values are
given by F −1
In IQN  after the set of τ is sampled from a uniform distribution  instead of using differences
between τ as probabilities of the quantiles  the mean of the quantile values is used to compute
) with τ0 = 0  τN = 1
Z (τi) are equal  we use the former one to consist with our projection operation.

action-value Q. While in expectation  Q =(cid:80)N−1

Z (τ ) ≈ F −1
(cid:80)N
i=1 F −1

i=0 (τi+1 − τi)F −1

Z ( τi+τi+1

2

Z w2

and Q = 1
N

4 Experiments

We test our algorithm on the Atari games from Arcade Learning Environment (ALE) Bellemare
et al. [2013]. We select the most relative algorithm to ours  IQN [Dabney et al.  2018a]  as baseline 
and compare FQF with QR-DQN [Dabney et al.  2018b]  C51 [Bellemare et al.  2017]  prioritized
experience replay [Schaul et al.  2016] and Rainbow [Hessel et al.  2018]  the current state-of-art
that combines the advantages of several RL algorithms including distributional RL. The baseline
algorithm is implemented by Castro et al. [2018] in the Dopamine framework  with slightly lower
performance than reported in IQN. We implement FQF based on the Dopamine framework. Unfortu-
nately  we fail to test our algorithm on Surround and Defender as Surround is not supported by the
Dopamine framework and scores of Defender is unreliable in Dopamine. Following the common
practice [Van Hasselt et al.  2016]  we use the 30-noop evaluation settings to align with previous
works. Results of FQF and IQN using sticky action for evaluation proposed by Machado et al. [2018]
are also provided in the appendix. In all  the algorithms are tested on 55 Atari games.
Our hyper-parameter setting is aligned with IQN for fair comparison. The number of τ for FQF is 32.
The weights of the fraction proposal network are initialized so that initial probabilities are uniform as
in QR-DQN  also the learning rates are relatively small compared with the quantile value network to
keep the probabilities relatively stable while training. We run all agents with 200 million frames. At
the training stage  we use -greedy with  = 0.01. For each evaluation stage  we test the agent for

7

0255075100125150175200Epoch01000200030004000500060007000ReturnBerzerkIQNFQF0255075100125150175200Epoch010000200003000040000500006000070000ReturnGopherIQNFQF0255075100125150175200Epoch02000400060008000100001200014000ReturnKangarooIQNFQF0255075100125150175200Epoch050000100000150000200000250000ReturnChopperCommandIQNFQF0255075100125150175200Epoch20004000600080001000012000ReturnCentipedeIQNFQF0255075100125150175200Epoch0100200300400500600ReturnBreakoutIQNFQF0255075100125150175200Epoch0500100015002000ReturnAmidarIQNFQF0255075100125150175200Epoch010000200003000040000500006000070000ReturnKungFuMasterIQNFQF0255075100125150175200Epoch201001020ReturnDoubleDunkIQNFQF0.125 million frames with  = 0.001. For each algorithm we run 3 random seeds. All experiments
are performed on NVIDIA Tesla V100 16GB graphics cards.

Mean Median
221%
580%
701%

79%
DQN
124%
PRIOR.
C51
178%
RAINBOW 1213% 227%
902%
QR-DQN
193%
IQN
1112% 218%
1426% 272%
FQF

>Human
24
39
40
42
41
39
44

>DQN
0
48
50
52
54
54
54

Table 1: Mean and median scores across 55 Atari 2600 games  measured as percentages of human
baseline. Scores are averages over 3 seeds.

Table 1 compares the mean and median human normalized scores across 55 Atari games with up
to 30 random no-op starts  and the full score table is provided in the Appendix. It shows that FQF
outperforms all existing distributional RL algorithms  including Rainbow [Hessel et al.  2018] that
combines C51 with prioritized replay  and n-step updates. We also set a new record on the number of
games where non-distributed RL agent performs better than human.
Figure 2 shows the training curves of several Atari games. Even on games where FQF and IQN
have similar performance such as Centipede   FQF is generally much faster thanks to self-adjusting
fractions.
However  one side effect of the full parameterization in FQF is that the training speed is decreased.
With same settings  FQF is roughly 20% slower than IQN due to the additional fraction proposal
network. As the number of τ increases  FQF slows down signiﬁcantly while IQN’s training speed is
not sensitive to the number of τ samples.

5 Discussion and Conclusions

Based on previous works of distributional RL  we propose a more general complete approximation
of the return distribution. Compared with previous distributional RL algorithms  FQF focuses not
only on learning the target  e.g. probabilities for C51  quantile values for QR-DQN and IQN  but
also which target to learn  i.e quantile fraction. This allows FQF to learn a better approximation of
the true distribution under restrictions of network capacity. Experiment result shows that FQF does
achieve signiﬁcant improvement.
There are some open questions we are yet unable to address in this paper. We will have some
discussions here. First  does the 1-Wasserstein error converge to its minimal value when the quantile
function is not ﬁxed? We cannot guarantee convergence of the fraction proposal network in deep
neural networks where we involve quantile regression and Bellman update. Second  though we
empirically believe so  does the contraction mapping result for ﬁxed probabilities given by Dabney
et al. [2018b] also apply on self-adjusting probabilities? Third  while FQF does provide potentially
better distribution approximation with same amount of fractions  how will a better approximated
distribution affect agent’s policy and how will it affect the training process? More generally  how
important is quantile fraction selection during training?
As for future work  we believe that studying the trained quantile fractions will provide intriguing
results. Such as how sensitive are the quantile fractions to state and action  and that how the
quantile fractions will evolve in a single run. Also  the combination of distributional RL and
DDPG in D4PG [Barth-Maron et al.  2018] showed that distributional RL can also be extended to
continuous control settings. Extending our algorithm to continuous settings is another interesting
topic. Furthermore  in our algorithm we adopted the concept of selecting the best target to learn. Can
this intuition be applied to areas other than RL?
Finally  we also noticed that most of the games we fail to reach human-level performance involves
complex rules that requires exploration based policies  such as Montezuma Revenge and Venture.
Integrating distributional RL will be another potential direction as in [Tang and Agrawal  2018]. In

8

general  we believe that our algorithm can be viewed as a natural extension of existing distributional
RL algorithms  and that distributional RL may integrate greatly with other algorithms to reach higher
performance.

References
Gabriel Barth-Maron  Matthew W Hoffman  David Budden  Will Dabney  Dan Horgan  Alistair
Muldal  Nicolas Heess  and Timothy Lillicrap. Distributed distributional deterministic policy
gradients. International Conference on Learning Representations  2018.

Marc G Bellemare  Yavar Naddaf  Joel Veness  and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research  47:
253–279  2013.

Marc G Bellemare  Will Dabney  and Rémi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 
pages 449–458. JMLR. org  2017.

Richard Bellman. Dynamic Programming. Princeton University Press  Princeton  NJ  USA  1 edition 

1957.

Pablo Samuel Castro  Subhodeep Moitra  Carles Gelada  Saurabh Kumar  and Marc G. Bellemare.
Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http:
//arxiv.org/abs/1812.06110.

Will Dabney  Georg Ostrovski  David Silver  and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In International Conference on Machine Learning  pages
1104–1113  2018a.

Will Dabney  Mark Rowland  Marc G Bellemare  and Rémi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
2018b.

Matteo Hessel  Joseph Modayil  Hado Van Hasselt  Tom Schaul  Georg Ostrovski  Will Dabney  Dan
Horgan  Bilal Piot  Mohammad Azar  and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence  2018.

Peter J. Huber. Robust estimation of a location parameter. Annals of Mathematical Statistics  35(1):

73–101  March 1964. ISSN 0003-4851. doi: 10.1214/aoms/1177703732.

Stratton C Jaquette et al. Markov decision processes with a new optimality criterion: Discrete time.

The Annals of Statistics  1(3):496–505  1973.

Steven Kapturowski  Georg Ostrovski  John Quan  Remi Munos  and Will Dabney. Recurrent

experience replay in distributed reinforcement learning. 2018.

Marlos C Machado  Marc G Bellemare  Erik Talvitie  Joel Veness  Matthew Hausknecht  and Michael
Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for
general agents. Journal of Artiﬁcial Intelligence Research  61:523–562  2018.

Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G Bellemare 
Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al. Human-level control
through deep reinforcement learning. Nature  518(7540):529  2015.

Tetsuro Morimura  Masashi Sugiyama  Hisashi Kashima  Hirotaka Hachiya  and Toshiyuki Tanaka.
Nonparametric return distribution approximation for reinforcement learning. In Proceedings of the
27th International Conference on Machine Learning (ICML-10)  pages 799–806  2010.

Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in

Applied Probability  29(2):429–443  1997.

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons  Inc.  New York  NY  USA  1st edition  1994. ISBN 0471619779.

9

Mark Rowland  Marc Bellemare  Will Dabney  Remi Munos  and Yee Whye Teh. An analysis
of categorical distributional reinforcement learning. In International Conference on Artiﬁcial
Intelligence and Statistics  pages 29–37  2018.

Tom Schaul  John Quan  Ioannis Antonoglou  and David Silver. Prioritized experience replay.

International Conference on Learning Representations  abs/1511.05952  2016.

Matthew J Sobel. The variance of discounted markov decision processes. Journal of Applied

Probability  19(4):794–802  1982.

Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning  3

(1):9–44  1988.

Yunhao Tang and Shipra Agrawal. Exploration by distributional reinforcement learning. In Proceed-
ings of the 27th International Joint Conference on Artiﬁcial Intelligence  pages 2710–2716. AAAI
Press  2018.

Hado Van Hasselt  Arthur Guez  and David Silver. Deep reinforcement learning with double q-

learning. In Thirtieth AAAI Conference on Artiﬁcial Intelligence  2016.

Ziyu Wang  Tom Schaul  Matteo Hessel  Hado Hasselt  Marc Lanctot  and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International Conference on Machine
Learning  pages 1995–2003  2016.

Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.

DJ White. Mean  variance  and probabilistic criteria in ﬁnite markov decision processes: a review.

Journal of Optimization Theory and Applications  56(1):1–29  1988.

10

,Shailee Jain
Alexander Huth
Derek Yang
Li Zhao
Zichuan Lin
Tao Qin
Jiang Bian
Tie-Yan Liu