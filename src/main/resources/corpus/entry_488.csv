2017,Alternating minimization for dictionary learning with random initialization,We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize vector samples $y^{1} y^{2} \ldots  y^{n}$ into an appropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*} \ldots x^{n*}$. Our algorithm is a simple alternating minimization procedure that switches between $\ell_1$ minimization and gradient descent in alternate steps. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However  in contrast to previous theoretical analyses for this problem  we replace a condition on the operator norm (that is  the largest magnitude singular value) of the true underlying dictionary $A^*$ with a condition on the matrix infinity norm (that is  the largest magnitude term). This not only allows us to get convergence rates for the error of the estimated dictionary measured in the matrix infinity norm  but also ensures that a random initialization will provably converge to the global optimum. Our guarantees are under a reasonable generative model that allows for dictionaries with growing operator norms  and can handle an arbitrary level of overcompleteness  while having sparsity that is information theoretically optimal. We also establish upper bounds on the sample complexity of our algorithm.,Alternating minimization for dictionary learning with

random initialization

Niladri S. Chatterji

UC Berkeley

niladri.chatterji@berkeley.edu

Peter L. Bartlett

UC Berkeley

peter@berkeley.edu

Abstract

We present theoretical guarantees for an alternating minimization algorithm for
the dictionary learning/sparse coding problem. The dictionary learning problem
is to factorize vector samples y1  y2  . . .   yn into an appropriate basis (dictionary)
A⇤ and sparse vectors x1⇤  . . .   xn⇤. Our algorithm is a simple alternating mini-
mization procedure that switches between `1 minimization and gradient descent in
alternate steps. Dictionary learning and speciﬁcally alternating minimization algo-
rithms for dictionary learning are well studied both theoretically and empirically.
However  in contrast to previous theoretical analyses for this problem  we replace
a condition on the operator norm (that is  the largest magnitude singular value)
of the true underlying dictionary A⇤ with a condition on the matrix inﬁnity norm
(that is  the largest magnitude term). This not only allows us to get convergence
rates for the error of the estimated dictionary measured in the matrix inﬁnity norm 
but also ensures that a random initialization will provably converge to the global
optimum. Our guarantees are under a reasonable generative model that allows
for dictionaries with growing operator norms  and can handle an arbitrary level of
overcompleteness  while having sparsity that is information theoretically optimal.
We also establish upper bounds on the sample complexity of our algorithm.

yi = A⇤xi⇤

Introduction

1
In the problem of sparse coding/dictionary learning  given i.i.d. samples y1  y2  . . .   yn 2 Rd
produced from the generative model
(1)
for i 2{ 1  2  . . .   n}  the goal is to recover a ﬁxed dictionary A⇤ 2 Rd⇥r and s-sparse vectors
xi⇤ 2 Rr. (An s-sparse vector has no more than s non-zero entries.) In many problems of interest 
the dictionary is often overcomplete  that is  r  d. This is believed to add ﬂexibility in modeling
and robustness. This model was ﬁrst proposed in neuroscience as an energy minimization heuristic
that reproduces features of the V1 region of the visual cortex [28; 22]. It has also been an extremely
successful approach to identifying low dimensional structure in high dimensional data; it is used
extensively to ﬁnd features in images  speech and video (see  for example  references in [13]).
Most formulations of dictionary learning tend to yield non-convex optimization problems. For
example  note that if either xi⇤ or A⇤ were known  given yi  this would just be a (matrix/sparse)
regression problem. However  estimating both xi⇤ and A⇤ simultaneously leads to both computational
as well as statistical complications. The heuristic of alternating minimization works well empirically
for dictionary learning. At each step  ﬁrst an estimate of the dictionary is held ﬁxed while the sparse
coefﬁcients are estimated; next  using these sparse coefﬁcients the dictionary is updated. Note that in
each step the sub-problem has a convex formulation  and there is a range of efﬁcient algorithms that
can be used. This heuristic has been very successful empirically  and there has also been signiﬁcant
recent theoretical progress in understanding its performance  which we discuss next.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 Related Work
A recent line of work theoretically analyzes local linear convergence rates for alternating minimization
procedures applied to dictionary learning [1; 4]. Arora et al. [4] present a neurally plausible algorithm

that recovers the dictionary exactly for sparsity up to s = O(pd/(µ log(d)))  where µ/pd is the

level of incoherence in the dictionary (which is a measure of the similarity of the columns; see
Assumption A1 below). Agarwal et al. [1] analyze a least squares/`1 minimization scheme and show
that it can tolerate sparsity up to s = O(d1/6). Both of these establish local linear convergence
guarantees for the maximum column-wise distance. Exact recovery guarantees require a singular-
value decomposition (SVD) or clustering based procedure to initialize their dictionary estimates (see
also the previous work [5; 2]).
For the undercomplete case (when r  d)  Sun et al. [33] provide a Riemannian trust region method
that can tolerate sparsity s = O(d)  while earlier work by Spielman et al. [32] provides an algorithm
that works in this setting for sparsity O(pd).

Local and global optima of non-convex formulations for the problem have also been extensively
studied in [36; 17; 18]  among others. Apart from alternating minimization  other approaches (without
theoretical convergence guarantees) for dictionary learning include K-SVD [3] and MOD [14]. There
is also a nice formulation by Barak et al. [7]  based on the sum-of-squares hierarchy. Recently 
Hazan and Ma [20] provide guarantees for improper dictionary learning  where instead of learning
a dictionary  they learn a comparable encoding via convex relaxations. Our work also adds to the
recent literature on analyzing alternating minimization algorithms [21; 26; 27; 19; 6].

1.2 Contributions
Our main contribution is to present new conditions under which alternating minimization for dictio-
nary learning converges at a linear rate to the global optimum. We impose a condition on the matrix
inﬁnity norm (largest magnitude entry) of the underlying dictionary. This allows dictionaries with
operator norm growing with dimension (d  r). The error rates are measured in the matrix inﬁnity
norm  which is sharper than the previous error rates in maximum column-wise error.
We also identify conditions under which a trivial random initialization of the dictionary works  as
opposed to the more complex SVD and clustering procedures required in previous work. This is
possible as our radius of convergence  again measured in the matrix inﬁnity norm  is larger than that
of previous results  which required the initial estimate to be close column-wise. Our results hold
for a rather arbitrary level of overcompleteness  r = O(poly(d)). We establish convergence results
for sparsity level s = O(pd)  which is information theoretically optimal for incoherent dictionaries
and improves the previously best known results in the overcomplete setting by a logarithmic factor.
Our algorithm is simple  involving an `1-minimization step followed by a gradient update for the
dictionary.
A key step in our proofs is an analysis of a robust sparse estimator—{`1 ` 2 ` 1}-MU Selector—
under ﬁxed (worst case) corruption in the dictionary. We prove that this estimator is minimax optimal
in this setting  which might be of independent interest.

1.3 Organization
In Section 2  we present our alternating minimization algorithm and discuss the sparse regression
estimator. In Section 3  we list the assumptions under which our algorithm converges and state the
main convergence result. Finally  in Section 4  we prove convergence of our algorithm. We defer
technical lemmas  analysis of the sparse regression estimator  and minimax analysis to the appendix.

Notation
For a vector v 2 Rd  vi denotes the ith component of the vector  kvkp denotes the `p norm  supp(v)
denotes the support of a vector v  that is  the set of non-zero entries of the vector  sgn(v) denotes
the sign of the vector v  that is  a vector with sgn(v)i = 1 if vi > 0  sgn(v)i = 1 if vi < 0 and
sgn(v)i = 0 if vi = 0. For a matrix W   Wi denotes the ith column  Wij is the element in the ith
row and jth column  kWkop denotes the operator norm  and kWk1 denotes the maximum of the
magnitudes of the elements of W . For a set J  we denote its cardinality by |J|. Throughout the paper 

2

Algorithm 1: Alternating Minimization for Dictionary Learning

Input

:Step size ⌘  samples {yk}n
{⌧ (t)}T

t=1  initial radius R(0) and parameters {(t)}T

k=1  initial estimate A(0)  number of steps T   thresholds
t=1.

t=1 {(t)}T

t=1 and {⌫(t)}T

for k = 1  2  . . .   n do

1 for t = 1  2  . . .   T do
2
3
4

wk (t) = M U S(t) (t) ⌫(t)(yk  A(t1)  R(t1))
for l = 1  2  3 . . .   r do

(xk (t) is the sparse estimate)

I⇣|wk (t)

l

| >⌧ (t)⌘  

xk (t)
l

= wk (t)

l

end

end
for i = 1  2  . . .   d do

5

6
7
8
9

10

for j = 1  2  . . .   r do
k=1hPr
nPn
 ⌘

ij = A(t1)
A(t)

end

ij

p=1⇣A(t1)

ip

xk (t)
p

xk (t)
j  yk

i xk (t)

j

⌘i

11
12
13
14 end

end
R(t) = 7

8 R(t1).

we use C multiple times to denote global constants that are independent of the problem parameters
and dimension. We denote the indicator function by I(·).
2 Algorithm

Given an initial estimate of the dictionary A(0) we alternate between an `1 minimization procedure
(speciﬁcally the {`1 ` 2 ` 1}-MU Selector—M U S  ⌫ in the algorithm—followed by a thresholding
step) and a gradient step  under sample `2 loss  to update the dictionary. We analyze this algorithm
and demand linear convergence at a rate of 7/8; convergence analysis for other rates follows in the
same vein with altered constants. In subsequent sections  we also establish conditions under which
the initial estimate for the dictionary A(0) can be chosen randomly. Below we state the permitted
range for the various parameters in the algorithm above.

1. Step size: We need to set the step size in the range 3r/4s < ⌘ < r/s.
2. Threshold: At each step set the threshold at ⌧ (t) = 16R(t1)M (R(t1)(s + 1) + s/pd).
3. Tuning parameters: We need to pick (t) and ⌫(t) such that the assumption (D5) is satisﬁed.

A choice that is suitable that satisﬁes this assumption is

32✓s3/2⇣R(t1)⌘2

+

s3/2R(t1)

d1/2

We need to set (t) as speciﬁed by Theorem 16 

(t) = ps⇣R(t1)⌘2

 ⌫(t)  3 
ps◆  (t)  3.

6

128s⇣R(t1)⌘2
◆✓4 +
+r s

R(t1).

d

2.1 Sparse Regression Estimator

Our proof of convergence for Algorithm 1 also goes through with a different choices of robust sparse
regression estimators  however  we can establish the tightest guarantees when the {`1 ` 2 ` 1}-MU
Selector is used in the sparse regression step. The {`1 ` 2 ` 1}-MU Selector [8] was established as
a modiﬁcation of the Dantzig selector to handle uncertainty in the dictionary. There is a beautiful
line of work that precedes this that includes [30; 31; 9]. There are also modiﬁed non-convex LASSO

3

programs that have been studied [23] and Orthogonal Matching Pursuit algorithms under in-variable
errors [11]. However these estimators require the error in the dictionary to be stochastic and zero mean
which makes them less suitable in this setting. Also note that standard `1 minimization estimators
like the LASSO and Dantzig selector are highly unstable under errors in the dictionary and would
lead to much worse guarantees in terms of radius of convergence (as studied in [1]). We establish the
error guarantees for a robust sparse estimator {`1 ` 2 ` 1}-MU Selector under ﬁxed corruption in
the dictionary. We also establish that this estimator is minimax optimal when the error in the sparse
estimate is measured in inﬁnity norm kˆ✓  ✓⇤k1 and the dictionary is corrupted.
The {`1 ` 2 ` 1}-MU Selector
Deﬁne the estimator ˆ✓ such that (ˆ✓  ˆt  ˆu) 2 Rr ⇥ R+ ⇥ R+ is the solution to the convex minimization
problem

min

✓ t u(k✓k1 + t + ⌫u

1
d

✓ 2 Rr 

A>y  A✓1  t + R2u k✓k2  t k✓k1  u) (2)

where     and ⌫ are tuning parameters that are chosen appropriately. R is an upper bound on the
error in our dictionary measured in matrix inﬁnity norm. Henceforth the ﬁrst coordinate (ˆ✓) of this
estimator is called M U S  ⌫ (y  A  R)  where the ﬁrst argument is the sample  the second is the
matrix  and the third is the value of the upper bound on the error of the dictionary measured in inﬁnity
norm. We will see that under our assumptions we will be able to establish an upper bound on the

error on the estimator  kˆ✓  ✓⇤k1  16RM⇣R(s + 1) + s/pd⌘  where |✓⇤j| M 8j. We deﬁne a
threshold at each step ⌧ = 16RM (R(s + 1) + s/pd). The thresholded estimate ˜✓ is deﬁned as

˜✓i = ˆ✓iI[|ˆ✓i| >⌧ ]

8i 2{ 1  2  . . .   r}.

(3)
Our assumptions will ensure that we have the guarantee sgn(˜✓) = sgn(✓⇤). This will be crucial in
our proof of convergence. The analysis of this estimator is presented in Appendix B.
To identify the signs of the sparse covariates correctly using this class of thresholded estimators  we
would like in the ﬁrst step to use an estimator ˆ✓ that is optimal  as this would lead to tighter control
over the radius of convergence. This makes the choice of {`1 ` 2 ` 1}-MU Selector natural  as we
will show it is minimax optimal under certain settings.

Theorem 1 (informal). Deﬁne the sets of matrices A = {B 2 Rd⇥rkBik2  1  8i 2{ 1  . . .   r}}
and W = {P 2 Rd⇥rkPk1  R} with R = O(1/ps). Then there exists an A⇤ 2A and W 2W

with A   A⇤ + W such that

✓⇤ k ˆT  ✓⇤k1  CRL s1 

sup

log(s)

log(r)!  

inf
ˆT

(4)

where the inf ˆT is over all measurable estimators ˆT with input (A⇤✓⇤  A  R)  and the sup is over
s-sparse vectors ✓⇤ with 2-norm L > 0.
Remark 2. Note that when R = O(1/ps) and s = O(pd)  this lower bound matches the upper
bound we have for Theorem 16 (up to logarithmic factors) and hence the {`1 ` 2 ` 1}-MU Selector
is minimax optimal.

The proof of this theorem follows by Fano’s method and is relegated to Appendix C.

2.2 Gradient Update for the dictionary
We note that the update to the dictionary at each step in Algorithm 1 is as follows

ij = A(t1)
A(t)

ij

n

 ⌘ 1
|

nXk=1" rXp=1⇣A(t1)

ip

xk (t)
j  yk

i xk (t)

j

xk (t)
p

 ˆg(t)

ij

{z

 

⌘#!
}

4

for i 2{ 1  . . .   d}  j 2{ 1  . . .   r} and t 2{ 1  . . .   T}. If we consider the loss function at time step t
built using the vector samples y1  . . .   yn and sparse estimates x1 (t)  . . .   xn (t) 

Ln(A) =

1
2n

nXk=1yk  Axk (t)

2

2

 

A 2 Rd⇥r 

we can identify the update to the dictionary ˆg(t) as the gradient of this loss function evaluated at
A(t1) 

ˆg(t) =

3 Main Results and Assumptions

@Ln(A)

@A A(t1)

.

In this section we state our convergence result and state the assumptions under which our results are
valid.

3.1 Assumptions
Assumptions on A⇤
(A1) Incoherence: We assume the the true underlying dictionary is µ/pd-incoherent

|hA⇤i   A⇤ji| 

µ
pd

8 i  j 2{ 1  . . .   r} such that  i 6= j.

This is a standard assumption in the sparse regression literature when support recovery is of
interest. It was introduced in [15; 34] in signal processing and independently in [38; 25] in
statistics. We can also establish guarantees under the strictly weaker `1-sensitivity condition
(cf. [16]) used in analyzing sparse estimators under in-variable uncertainty in [9; 31]. The
{`1 ` 2 ` 1}-MU selector that we use for our sparse recovery step also works with this more
general assumption  however for ease of exposition we assume A⇤ to be µ/pd-incoherent.

(A2) Normalized Columns: We assume that all the columns of A⇤ are normalized to 1 

kA⇤ik2 = 1 8 i 2{ 1  . . .   r}.

i=1 are invariant when we scale the columns of A⇤ or under
Note that the samples {yi}n
permutations of its columns. Thus we restrict ourselves to dictionaries with normalized
columns and label the entire equivalence class of dictionaries with permuted columns and
varying signs as A⇤.

(A3) Bounded max-norm: We assume that A⇤ is bounded in matrix inﬁnity norm

kA⇤k1 

Cb
s

.

This is in contrast with previous work that imposes conditions on the operator norm of A⇤
[4; 1; 5]. Our assumptions help provide guarantees under alternate assumptions and it also
allows the operator norm to grow with dimension  whereas earlier work requires A⇤ to be such

that kA⇤kop  C⇣pr/d⌘. In general the inﬁnity norm and operator norm balls are hard to

compare. However  one situation where a comparison is possible is if we assume the entries
of the dictionary to be drawn iid from a Gaussian distribution N (0  2). Then by standard
concentration theorems  for the operator norm condition to be satisﬁed we would need the
variance (2) of the distribution to scale as O(1/d) while  for the inﬁnity norm condition to be
satisﬁed we need the variance to be ˜O(1/s2). This means that modulo constants the variance
can be much larger for the inﬁnity norm condition to be satisﬁed than for the operator norm
condition.

Assumption on the initial estimate and initialization
(B1) We require an initial estimate for the dictionary A(0) such that 

kA(0)  A⇤k1 

CR
s

.

5

with 2Cb = CR; where CR = 1/2000M 2. Assuming 2Cb = CR allows a fast random
initialization  where we draw each entry of the initial estimate from the uniform distribution
(on the interval (Cb/2s  Cb/2s)). This allows us to circumvent the computationally heavy
SVD/clustering step required in previous work to get an initial dictionary estimate [4; 1; 5].
Note that we start with a random initialization and not with A(0) = 0  as this causes our sparse
estimator to fail (columns of A need to be non-zero).

Assumptions on x⇤
Next we assume a generative model on the s-sparse covariates x⇤. Here are the assumptions we make
about the (unknown) distribution of x⇤.

(C1) Conditional Independence: We assume that distribution of non-zero entries of x⇤ is condi-

tionally independent and identically distributed. That is  x⇤i ?? x⇤j|x⇤i   x⇤j 6= 0.

(C2) Sparsity Level:We assume that the level of sparsity s is bounded
2  s  min(2pd  Cbpd  Cpd/µ) 

where C is an appropriate global constant such that A⇤ satisﬁes assumption (D3)  see Re-
mark 15. For incoherent dictionaries  this upper bound is tight up to constant factors for sparse
recovery to be feasible [12; 18].

(C3) Boundedness: Conditioned on the event that i is in the subset of non-zero entries  we have

m | x⇤i| M 

with m  32R(0)M (R(0)(s + 1) + s/pd) and M > 1. This is needed for the thresholded
sparse estimator to correctly predict the sign of the true covariate (sgn(x) = sgn(x⇤)). We can
also relax the boundedness assumption: it sufﬁces for the x⇤i to have sub-Gaussian distributions.
(C4) Probability of support: The probability of i being in the support of x⇤ is uniform over all

i 2{ 1  2  . . .   r}. This translates to
Pr(x⇤i 6= 0) =
Pr(x⇤i   x⇤j 6= 0) =

s
r
s(s  1)
r(r  1)

8 i 2{ 1  . . .   r} 

8 i 6= j 2{ 1  . . .   r}.

(C5) Mean and variance of variables in the support: We assume that the non-zero random

variables in the support of x⇤ are centered and are normalized

E(x⇤i|x⇤i 6= 0) = 0 

E(x⇤2
i

|x⇤i 6= 0) = 1.

We note that these assumptions (A1)  (A2) and (C1) - (C5) are similar to those made in [4; 1].
Agarwal et al. [1] require the matrices to satisfy the restricted isometry property  which is strictly
weaker than µ/pd-incoherence  however they can tolerate a much lower level of sparsity (d1/6).

3.2 Main Result

Theorem 3. Suppose that
the s-sparse sam-
ples x⇤ satisfy the assumptions stated in Section 3.1 and we are given an estimate A(0)
such that kA(0)  A⇤k1  R(0)  CR/s.
samples
s(R(t1))2 log(dr/)) then Algorithm 1 with parameters
in every iteration with n(t) =⌦(
t=1 ⌘ ) chosen as speciﬁed in Section 3.1 after T iterations
({⌧ (t)}T
returns a dictionary A(T ) such that 

true dictionary A⇤ and the distribution of
If we are given {n(t)}T

t=1 {(t)}T

t=1 {(t)}T

t=1 {⌫(t)}T

t=1 i.i.d.

r

kA(T )  A⇤k1 ✓ 7
8◆T

R(0) 

with probability 1  .

6

ˆgij =

=

1
n

1
n

k xm

i xm

nXm=1" rXk=1aikxm
j #
j   ym
nXm=1" rXk=1⇣aikxm
j #
k  a⇤ikxm⇤k ⌘xm
= E" rXk=1⇣aikxk  a⇤ikx⇤k⌘xj#
nXm=1" rXk=1⇣aikxm
+" 1
| {z }

= gij + ˆgij  gij

k  a⇤ikxm⇤k ⌘xm

 ✏n

n

 

j #  E" rXk=1⇣aikxk  a⇤ikx⇤k⌘xj##

4 Proof of Convergence

In this section we will prove the main convergence results stated as Theorem 3. To prove this result
we will analyze the gradient update to the dictionary at each step. We will decompose this gradient
update (which is a random variable) into a ﬁrst term which is its expected value and a second term
which is its deviation from expectation. We will prove a deterministic convergence result by working
with the expected value of the gradient and then appeal to standard concentration arguments to control
the deviation of the gradient from its expected value with high probability.
By Lemma 8  Algorithm 1 is guaranteed to estimate the sign pattern correctly at every round of the
algorithm  sgn(x) = sgn(x⇤) (see proof in Appendix A.1).
To un-clutter notation let  A⇤ij = a⇤ij  A(t)
ij = aij  A(t+1)
ij. The kth coordinate of the mth
covariate is written as xm⇤k . Similarly let xm
k be the kth coordinate of the estimate of the mth
covariate at step t. Finally let R(t) = R  n(t) = n and ˆgij be the (i  j)th element of the gradient with
n (n(t)) samples at step t. Unwrapping the expression for ˆgij we get 

= a0

ij

where gij denotes (i  j)th element of the expected value (inﬁnite samples) of the gradient. The second
term ✏n is the deviation of the gradient from its expected value. By Theorem 10 we can control the
deviation of the sample gradient from its mean via an application of McDiarmid’s inequality. With
this notation in place we are now ready to prove Theorem 3.
Proof [Proof of Theorem 3] First we analyze the structure of the expected value of the gradient.
Step 1: Unwrapping the expected value of the gradient we ﬁnd it decomposes into three terms

s

aikxkxj  a⇤ikx⇤kxj35
r E⇥(xj  x⇤j )xj|x⇤j 6= 0⇤
}

j  a⇤ijx⇤j xj + E24Xk6=j
gij = Eaijx2
j|x⇤j 6= 0⇤
r E⇥x2
= (aij  a⇤ij)
|
{z
}
|
The ﬁrst term gc
ij points in the correct direction and will be useful in converging to the right answer.
The other terms could be in a bad direction and we will control their magnitude with Lemma 5 such
3r R. The proof of Lemma 5 is the main technical challenge in the convergence
that |⌅1| + |⌅2| s
analysis to control the error in the gradient. Its proof is deferred to the appendix.
Step 2: Given this bound  we analyze the gradient update 

aikxkxj  a⇤ikx⇤kxj35
{z
}

+ E24Xk6=j
|

{z

+ a⇤ij

 ⌅1

 ⌅2

 gc

s

ij

.

a0
ij = aij  ⌘ˆgij

= aij  ⌘(gij + ✏n)

= aij  ⌘⇥gc

ij + (⌅ 1 +⌅ 2) + ✏n⇤ .

7

So if we look at the distance to the optimum a⇤ij we have the relation 

(i)

(ii)

s

s

s

|a0
ij  a⇤ij|

s

r E⇥x2

Taking absolute values  we get

a0
ij  a⇤ij = aij  a⇤ij  ⌘(aij  a⇤ij)

r E⇥x2
r E⇥x2
r⇢E⇥x2

⇣1  ⌘
 ⇣1  ⌘
✓1  ⌘

j|x⇤j 6= 0⇤  ⌘ {(⌅1 +⌅ 2) + ✏n} .
j|x⇤j 6= 0⇤⌘|aij  a⇤ij| + ⌘ {|⌅1| + |⌅2| + |✏n|}
j|x⇤j 6= 0⇤⌘|aij  a⇤ij| + ⌘⇣ s
R⌘ + ⌘|✏n|
3◆ R + ⌘|✏n| 
j|x⇤j 6= 0⇤ 
j|x⇤j 6= 0⇤. We would expect that as R
j |x⇤j 6= 0⇤ = 1. By invoking Lemma 6 we can bound
j|x⇤j 6= 0⇤ 
Coupled with Lemma 6  Inequality (i) follows from ⌘  r
4s . We also
have by Theorem 10 that ⌘|✏n| R/8 with probability 1  . So if we unroll the bound for t steps
we have 

Lemma 5. Next we give an upper and lower bound on E⇥x2
gets smaller this variance term approaches E⇥x⇤2
r⇢E⇥x2

provided the ﬁrst term is at non-negative. Here  (i) follows by triangle inequality and (ii) is by

j|x⇤j 6= 0⇤  4
✓1  ⌘

3. We want the ﬁrst term to contract at a rate 3/4; it sufﬁces to

3  E⇥x2

this term to be 2
have

(i)

0

s

3r

1

.

1

3
4



3◆ (ii)
s while (ii) follows from ⌘  3r
R(t1) ✓ 7
8◆t

R(t1) =

7
8

R(0).

|a(t)
ij  a⇤ij|

3
4

R(t1) + ⌘|✏n|

3
4

R(t1) +

1
8

We also have ⌘|✏n| R/8  R(0)/8 with probability at least 1   for all t 2{ 1  . . .   T}; thus we
are guaranteed to remain in our initial ball of radius R(0) with high probability  completing the proof.

5 Conclusion

An interesting question would be to further explore and analyze the range of algorithms for which
alternating minimization works and identifying the conditions under which they provably converge.
There also seem to be many open questions for improper dictionary learning and developing provably
faster algorithms there. Going beyond sparsity pd still remains challenging  and as noted in previous
work alternating minimization also appears to break down experimentally and new algorithms are
required in this regime. Also all theoretical work on analyzing alternating minimization for dictionary
learning seems to rely on identifying the signs of the samples (x⇤) correctly at every step. It would
be an interesting theoretical question to analyze if this is a necessary condition or if an alternate
proof strategy and consequently a bigger radius of convergence are possible. Lastly  it is not known
what the optimal sample complexity for this problem is and lower bounds there could be useful in
designing more sample efﬁcient algorithms.

Acknowledgments

We gratefully acknowledge the support of the NSF through grant IIS-1619362  and of the Australian
Research Council through an Australian Laureate Fellowship (FL110100281) and through the ARC
Centre of Excellence for Mathematical and Statistical Frontiers. Thanks also to the Simons Institute
for the Theory of Computing Spring 2017 Program on Foundations of Machine Learning. The authors
would like to thank Sahand Negahban for pointing out an error in the µ-incoherence assumption in an
earlier version.

8

References
[1] Agarwal  A.  A. Anandkumar  P. Jain  P. Netrapalli  and R. Tandon (2014). Learning sparsely

used overcomplete dictionaries. In COLT  pp. 123–137.

[2] Agarwal  A.  A. Anandkumar  and P. Netrapalli (2013). A clustering approach to learn sparsely-

used overcomplete dictionaries. arXiv preprint arXiv:1309.1952.

[3] Aharon  M.  M. Elad  and A. Bruckstein (2006). K-SVD: An algorithm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on signal processing 54(11) 
4311–4322.

[4] Arora  S.  R. Ge  T. Ma  and A. Moitra (2015). Simple  efﬁcient  and neural algorithms for sparse

coding. In COLT  pp. 113–149.

[5] Arora  S.  R. Ge  and A. Moitra (2013). New algorithms for learning incoherent and overcomplete

dictionaries.

[6] Balakrishnan  S.  M. J. Wainwright  B. Yu  et al. (2017). Statistical guarantees for the EM

algorithm: From population to sample-based analysis. The Annals of Statistics 45(1)  77–120.

[7] Barak  B.  J. A. Kelner  and D. Steurer (2015). Dictionary learning and tensor decomposition via
the sum-of-squares method. In Proceedings of the Forty-Seventh Annual ACM on Symposium on
Theory of Computing  pp. 143–151. ACM.

[8] Belloni  A.  M. Rosenbaum  and A. B. Tsybakov (2014). An {`1 ` 2 ` 1}-Regularization

Approach to High-Dimensional Errors-in-variables Models. arXiv preprint arXiv:1412.7216.

[9] Belloni  A.  M. Rosenbaum  and A. B. Tsybakov (2016). Linear and conic programming
estimators in high dimensional errors-in-variables models. Journal of the Royal Statistical Society:
Series B (Statistical Methodology).

[10] Boucheron  S.  G. Lugosi  and P. Massart (2013). Concentration inequalities: A nonasymptotic

theory of independence. Oxford university press.

[11] Chen  Y. and C. Caramanis (2013). Noisy and Missing Data Regression: Distribution-Oblivious

Support Recovery.

[12] Donoho  D. L. and X. Huo (2001). Uncertainty principles and ideal atomic decomposition.

IEEE Transactions on Information Theory 47(7)  2845–2862.

[13] Elad  M. and M. Aharon (2006). Image denoising via sparse and redundant representations over

learned dictionaries. IEEE Transactions on Image processing 15(12)  3736–3745.

[14] Engan  K.  S. O. Aase  and J. H. Husoy (1999). Method of optimal directions for frame
design. In Acoustics  Speech  and Signal Processing  1999. Proceedings.  1999 IEEE International
Conference on  Volume 5  pp. 2443–2446. IEEE.

[15] Fuchs  J.-J. (2004). Recovery of exact sparse representations in the presence of noise. In
Acoustics  Speech  and Signal Processing  2004. Proceedings.(ICASSP’04). IEEE International
Conference on  Volume 2  pp. ii–533. IEEE.

[16] Gautier  E. and A. Tsybakov (2011). High-dimensional instrumental variables regression and

conﬁdence sets. arXiv preprint arXiv:1105.2454.

[17] Gribonval  R.  R. Jenatton  and F. Bach (2015). Sparse and spurious: dictionary learning with

noise and outliers. IEEE Transactions on Information Theory 61(11)  6298–6319.

[18] Gribonval  R. and M. Nielsen (2003). Sparse representations in unions of bases.

transactions on Information theory 49(12)  3320–3325.

IEEE

[19] Hardt  M. (2014). Understanding alternating minimization for matrix completion. In Foun-
dations of Computer Science (FOCS)  2014 IEEE 55th Annual Symposium on  pp. 651–660.
IEEE.

9

[20] Hazan  E. and T. Ma (2016). A Non-generative Framework and Convex Relaxations for
Unsupervised Learning. In Advances in Neural Information Processing Systems  pp. 3306–3314.
[21] Jain  P.  P. Netrapalli  and S. Sanghavi (2013). Low-rank matrix completion using alternating
minimization. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing 
pp. 665–674. ACM.

[22] Lewicki  M. S. and T. J. Sejnowski (2000). Learning overcomplete representations. Neural

computation 12(2)  337–365.

[23] Loh  P.-L. and M. J. Wainwright (2011). High-dimensional regression with noisy and missing
data: Provable guarantees with non-convexity. In Advances in Neural Information Processing
Systems  pp. 2726–2734.

[24] McDiarmid  C. (1989). On the method of bounded differences. Surveys in combinatorics 141(1) 

148–188.

[25] Meinshausen  N. and P. Bühlmann (2006). High-dimensional graphs and variable selection with

the Lasso. The annals of statistics  1436–1462.

[26] Netrapalli  P.  P. Jain  and S. Sanghavi (2013). Phase retrieval using alternating minimization.

In Advances in Neural Information Processing Systems  pp. 2796–2804.

[27] Netrapalli  P.  U. Niranjan  S. Sanghavi  A. Anandkumar  and P. Jain (2014). Non-convex robust

PCA. In Advances in Neural Information Processing Systems  pp. 1107–1115.

[28] Olshausen  B. A. and D. J. Field (1997). Sparse coding with an overcomplete basis set: A

strategy employed by V1? Vision research 37(23)  3311–3325.

[29] Rigollet  P. and A. Tsybakov (2011). Exponential screening and optimal rates of sparse

estimation. The Annals of Statistics  731–771.

[30] Rosenbaum  M.  A. B. Tsybakov  et al. (2010). Sparse recovery under matrix uncertainty. The

Annals of Statistics 38(5)  2620–2651.

[31] Rosenbaum  M.  A. B. Tsybakov  et al. (2013). Improved matrix uncertainty selector. In From
Probability to Statistics and Back: High-Dimensional Models and Processes–A Festschrift in
Honor of Jon A. Wellner  pp. 276–290. Institute of Mathematical Statistics.

[32] Spielman  D. A.  H. Wang  and J. Wright (2012). Exact Recovery of Sparsely-Used Dictionaries.

In COLT  pp. 37–1.

[33] Sun  J.  Q. Qu  and J. Wright (2017). Complete dictionary recovery over the sphere I: Overview

and the geometric picture. IEEE Transactions on Information Theory 63(2)  853–884.

[34] Tropp  J. A. (2006). Just relax: Convex programming methods for identifying sparse signals in

noise. IEEE transactions on information theory 52(3)  1030–1051.

[35] Tsybakov  A. B. (2009). Introduction to nonparametric estimation. Revised and extended from

the 2004 French original. Translated by Vladimir Zaiats.

[36] Wu  S. and B. Yu (2015). Local identiﬁability of `1-minimization dictionary learning: a

sufﬁcient and almost necessary condition. arXiv preprint arXiv:1505.04363.

[37] Yu  B. (1997). Assouad  Fano  and Le Cam. In Festschrift for Lucien Le Cam  pp. 423–435.

Springer.

[38] Zhao  P. and B. Yu (2006). On model selection consistency of Lasso. Journal of Machine

learning research 7(Nov)  2541–2563.

10

,Ping Li
Gennady Samorodnitsk
John Hopcroft
Niladri Chatterji
Peter Bartlett