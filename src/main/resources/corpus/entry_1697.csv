2015,Efficient Compressive Phase Retrieval with Constrained Sensing Vectors,We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.In recent years  various methods are proposed for compressive phase retrieval  but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements  there is a standard framework for recovering a sparse matrix  and a standard framework for recovering a low-rank matrix. However  a general  efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.Deviating from the models with generic measurements  in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace  then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is $\mathsf{O}(k\ \log\frac{d}{k})$  where $k$ and $d$ denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.,Efﬁcient Compressive Phase Retrieval

with Constrained Sensing Vectors

Sohail Bahmani 

Justin Romberg

School of Electrical and Computer Engineering.

Georgia Institute of Technology

Atlanta  GA 30332

{sohail.bahmani jrom}@ece.gatech.edu

Abstract

We propose a robust and efﬁcient approach to the problem of compressive phase
retrieval in which the goal is to reconstruct a sparse vector from the magnitude
of a number of its linear measurements. The proposed framework relies on con-
strained sensing vectors and a two-stage reconstruction method that consists of
two standard convex programs that are solved sequentially.
In recent years  various methods are proposed for compressive phase retrieval  but
they have suboptimal sample complexity or lack robustness guarantees. The main
obstacle has been that there is no straightforward convex relaxations for the type
of structure in the target. Given a set of underdetermined measurements  there is a
standard framework for recovering a sparse matrix  and a standard framework for
recovering a low-rank matrix. However  a general  efﬁcient method for recovering
a jointly sparse and low-rank matrix has remained elusive.
Deviating from the models with generic measurements  in this paper we show that
if the sensing vectors are chosen at random from an incoherent subspace  then the
low-rank and sparse structures of the target signal can be effectively decoupled.
We show that a recovery algorithm that consists of a low-rank recovery stage fol-
lowed by a sparse recovery stage will produce an accurate estimate of the target
when the number of measurements is O(k log d
k )  where k and d denote the spar-
sity level and the dimension of the input signal. We also evaluate the algorithm
through numerical simulation.

1

Introduction

1.1 Problem setting

The problem of Compressive Phase Retrieval (CPR) is generally stated as the problem of estimating
a k-sparse vector x(cid:63) ∈ Rd from noisy measurements of the form

(1)
for i = 1  2  . . .   n  where ai is the sensing vector and zi denotes the additive noise. In this paper 
we study the CPR problem with speciﬁc sensing vectors ai of the form

yi = |(cid:104)ai  x(cid:63)(cid:105)|2 + zi

(2)
where Ψ ∈ Rm×d and wi ∈ Rm are known. In words  the measurement vectors live in a ﬁxed
low-dimensional subspace (i.e  the row space of Ψ). These types of measurements can be applied in
imaging systems that have control over how the scene is illuminated; examples include systems that
use structured illumination with a spatial light modulator or a scattering medium [1  2].

ai = Ψ Twi 

1

By a standard lifting of the signal x(cid:63) to X (cid:63) = x(cid:63)x(cid:63)T  the quadratic measurements (1) can be
expressed as

yi =(cid:10)aiaT
i   X (cid:63)(cid:11) + zi =
i   B(cid:11)(cid:3)n
W :B (cid:55)→(cid:2)(cid:10)wiwT

i=1

With the linear operator W and A deﬁned as

(cid:68)

and

Ψ TwiwT

i Ψ   X (cid:63)(cid:69)
A : X (cid:55)→ W(cid:16)

+ zi.

(3)

Ψ XΨ T(cid:17)

 

we can write the measurements compactly as

y = A (X (cid:63)) + z.

Our goal is to estimate the sparse  rank-one  and positive semideﬁnite matrix X (cid:63) from the measure-
ments (3)  which also solves the CPR problem and provides an estimate for the sparse signal x(cid:63) up
to the inevitable global phase ambiguity.

Assumptions We make the following assumptions throughout the paper.

A1. The vectors wi are independent and have the standard Gaussian distribution on Rm: wi ∼
A2. The matrix Ψ is a restricted isometry matrix for 2k-sparse vectors and for a constant δ2k ∈

N (0  I) .

[0  1]. Namely  it obeys

(1 − δ2k)(cid:107)x(cid:107)2

2 ≤ (cid:107)Ψ x(cid:107)2

2 ≤ (1 + δ2k)(cid:107)x(cid:107)2
2  

(4)

for all 2k-sparse vectors x ∈ Rd.

A3. The noise vector z is bounded as (cid:107)z(cid:107)2 ≤ ε.

As will be seen in Theorem 1 and its proof below  the Gaussian distribution imposed by the assump-
tion A1 will be used merely to guarantee successful estimation of a rank-one matrix through trace
norm minimization. However  other distributions (e.g.  uniform distribution on the unit sphere) can
also be used to obtain similar guarantees. Furthermore  the restricted isometry condition imposed
by the assumption A2 is not critical and can be replaced by weaker assumptions. However  the guar-
antees obtained under these weaker assumptions usually require more intricate derivations  provide
weaker noise robustness  and often do not hold uniformly for all potential target signals. Therefore 
to keep the exposition simple and straightforward we assume (4) which is known to hold (with high
probability) for various ensembles of random matrices (e.g.  Gaussian  Rademacher  partial Fourier 
etc). Because in many scenarios we have the ﬂexibility of selecting Ψ  the assumption (4) is realistic
as well.

Notation Let us ﬁrst set the notation used throughout the paper. Matrices and vectors are denoted
by bold capital and small letters  respectively. The set of positive integers less than or equal to
n is denoted by [n]. The notation f = O (g) is used when f = cg for some absolute constant
c > 0. For any matrix M  the Frobenius norm  the nuclear norm  the entrywise (cid:96)1-norm  and the
largest entrywise absolute value of the entries are denoted by (cid:107)M(cid:107)F   (cid:107)M(cid:107)∗  (cid:107)M(cid:107)1  and (cid:107)M(cid:107)∞ 
respectively. To indicate that a matrix M is positive semideﬁnite we write M (cid:60) 0.

1.2 Contributions

The main challenge in the CPR problem in its general formulation is to design an accurate estimator
that has optimal sample complexity and computationally tractable. In this paper we address this
challenge in the special setting where the sensing vectors can be factored as (2). Namely  we propose
an algorithm that

• provably produces an accurate estimate of the lifted target X (cid:63) from only n = O(cid:0)k log d

(cid:1)

k

measurements  and

• can be computed in polynomial time through efﬁcient convex optimization methods.

2

1.3 Related work

Several papers including [3  4  5  6  7] have already studied the application of convex programming
for (non-sparse) phase retrieval (PR) in various settings and have established estimation accuracy
through different mathematical techniques. These phase retrieval methods attain nearly optimal
sample complexities that scales with the dimension of the target signal up to a constant factor [4  5  6]
or at most a logarithmic factor [3]. However  to the best of our knowledge  the exiting methods for
CPR either lack accuracy and robustness guarantees or have suboptimal sample complexities.
The problem of recovering a sparse signal from the magnitude of its subsampled Fourier transforms
is cast in [8] as an (cid:96)1-minimization with non-convex constraints. While [8] shows that a sufﬁcient
number of measurements would grow quadratically in k (i.e.  the sparsity of the signal)  the numer-
ical simulations suggest that the non-convex method successfully estimates the sparse signal with
only about k log d
k measurements. Another non-convex approach to CPR is considered in [9] which
poses the problem as ﬁnding a k-sparse vector that minimizes the residual error that takes a quartic
form. A local search algorithm called GESPAR [10] is then applied to (approximate) the solution
to the formulated sparsity-constrained optimization. This approach is shown to be effective through
simulations  but it also lacks global convergence or statistical accuracy guarantees. An alternating
minimization method for both PR and CPR is studied in [11]. This method is appealing in large
scale problems because of computationally inexpensive iterations. More importantly  [11] proposes
a speciﬁc initialization using which the alternating minimization method is shown to converge lin-
early in noise-free PR and CPR. However  the number of measurements required to establish this
convergence is effectively quadratic in k.
In [12] and [13] the (cid:96)1-regularized form of the trace
minimization

trace (X) + λ(cid:107)X(cid:107)1

argmin
X(cid:60)0
subject to A (X) = y

(5)

is proposed for the CPR problem. The guarantees of [13] are based on the restricted isometry prop-
erty of the sensing operator X (cid:55)→ [(cid:104)aia∗
i=1 for sparse matrices. In [12]  however  the anal-
ysis is based on construction of a dual certiﬁcate through an adaptation of the golﬁng scheme [14].
Assuming standard Gaussian sensing vectors ai and with appropriate choice of the regularization

parameter λ  it is shown in [12] that (5) solves the CPR when n = O(cid:0)k2 log d(cid:1). Furthermore  this

i   X(cid:105)]n

method fails to recover the target sparse and rank-one matrix if n is dominated by k2. Estimation
of simultaneously structured matrices through convex relaxations similar to (5) is also studied in
[15] where it is shown that these methods do not attain optimal sample complexity. More recently 
assuming that the sparse target has a Bernoulli-Gaussian distribution  a generalized approximate
message passing framework is proposed in [16] to solve the CPR problem. Performance of this
method is evaluated through numerical simulations for standard Gaussian sensing matrices which

show the empirical phase transition for successful estimation occurs at n = O(cid:0)k log d

the algorithms can have a signiﬁcantly lower runtime compared to some of the competing algo-
rithms including GESPAR [10] and CPRL [13]. The PhaseCode algorithm is proposed in [17] to
solve the CPR problem with sensing vectors designed using sparse graphs and techniques adapted
from coding theory. Although PhaseCode is shown to achieve the optimal sample complexity  it
lacks robustness guarantees.
While preparing the ﬁnal version of the current paper  we became aware of [18] which has indepen-
dently proposed an approach similar to ours to address the CPR problem.

(cid:1) and also

k

2 Main Results

2.1 Algorithm

We propose a two-stage algorithm outlined in Algorithm 1. Each stage of the algorithm is a convex
program for which various efﬁcient numerical solvers exists. In the ﬁrst stage we solve (6) to obtain

a low-rank matrix (cid:98)B which is an estimator of the matrix

B(cid:63) = Ψ X (cid:63)Ψ T.

3

Then (cid:98)B is used in the second stage of the algorithm as the measurements for a sparse estimation

expressed by (7). The constraint of (7) depends on an absolute constant C > 0 that should be
sufﬁciently large.

Algorithm 1:
input : the measurements y  the operator W  and the matrix Ψ

output: the estimate(cid:99)X
1 Low-rank estimation stage: (cid:98)B ∈ argmin

2 Sparse estimation stage:(cid:99)X ∈ argmin

X

subject to

B(cid:60)0
subject to

trace (B)
(cid:107)W (B) − y(cid:107)2 ≤ ε

(cid:107)X(cid:107)1

(cid:13)(cid:13)(cid:13)Ψ XΨ T − (cid:98)B

(cid:13)(cid:13)(cid:13)F

≤ Cε√
n

(6)

(7)

at most k nonzero rows and columns) and rank-one. In fact  since we have not imposed the posi-

Post-processing. The result of the low-rank estimation stage (6) is generally not rank-one. Simi-

larly  the sparse estimation stage does not necessarily produce a(cid:99)X that is k × k-sparse (i.e.  it has
tive semideﬁniteness constraint (i.e.  X (cid:60) 0) in (7)  the estimate(cid:99)X is not even guaranteed to be
PSD matrices. The simple but important observation is that projecting(cid:99)X onto the desired sets at

positive semideﬁnite (PSD). However  we can enforce the rank-one or the sparsity structure in post-
processing steps simply by projecting the produced estimate on the set of rank-one or k × k-sparse

most doubles the estimation error. This fact is shown by Lemma 2 in Section 4 in a general setting.

Alternatives. There are alternative convex relaxations for the low-rank estimation and the sparse
estimation stages of Algorithm (1). For example  (6) can be replaced by its regularized least squares
analog

(cid:98)B ∈ argmin

B(cid:60)0

(cid:107)W (B) − y(cid:107)2

2 + λ(cid:107)B(cid:107)∗  

1
2

for an appropriate choice of the regularization parameter λ. Similarly  instead of (7) we can use
an (cid:96)1-regularized least squares. Furthermore  to perform the low-rank estimation and the sparse
estimation we can use non-convex greedy type algorithms that typically have lower computational
costs. For example  the low-rank estimation stage can be performed via the Wirtinger ﬂow method
proposed in [19]. Furthermore  various greedy compressive sensing algorithms such as the Iterative
Hard Thresholding [20] and CoSaMP [21] can be used to solve the desired sparse estimation. To
guarantee the accuracy of these compressive sensing algorithms  however  we might need to adjust
the assumption A2 to have the restricted isometry property for ck-sparse vectors with c being some
small positive integer.

2.2 Accuracy guarantees

The following theorem shows that any solution of the proposed algorithm is an accurate estimator
of X (cid:63).
Theorem 1. Suppose that the assumptions A1  A2  and A3 hold with a sufﬁciently small constant
δ2k. Then  there exist positive absolute constants C1  C2  and C3 such that if

(8)

then any estimate(cid:99)X of the Algorithm 1 obeys
(cid:13)(cid:13)(cid:13)(cid:99)X − X (cid:63)(cid:13)(cid:13)(cid:13)F

n ≥ C1m 

≤ C2ε√
n

 

4

for all rank-one and k × k-sparse matrices X (cid:63) (cid:60) 0 with probability exceeding 1 − e−C3n.
The proof of Theorem 1 is straightforward and is provided in Section 4. The main idea is ﬁrst to
show the low-rank estimation stage produces an accurate estimate of B(cid:63). Because this stage can
be viewed as a standard phase retrieval through lifting  we can simply use accuracy guarantees that
are already established in the literature (e.g.  [3  6  5]). In particular  we use [5  Theorem 2] which
established an error bound that holds uniformly for all valid B(cid:63). Thus we can ensure that X (cid:63) is
feasible in the sparse estimation stage. Then the accuracy of the sparse estimation stage can also be
established by a simple adaptation of the analyses based on the restricted isometry property such as
[22].
The dependence of n (i.e.  the number of measurements) and k (i.e.  the sparsity of the signal) is
not explicit in Theorem 1. This dependence is absorbed in m which must be sufﬁciently large for
Assumption A2 to hold. Considering a Gaussian matrix Ψ  the following corollary gives a concrete
example where the dependence of non k through m is exposed.
Corollary 1. Suppose that the assumptions of Theorem 1 including (8) hold. Furthermore  suppose

that Ψ is a Gaussian matrix with iid N(cid:0)0  1
for some absolute constant c1 > 0. Then any estimate(cid:99)X produced by Algorithm 1 obeys

(cid:1) entries and
(cid:13)(cid:13)(cid:13)(cid:99)X − X (cid:63)(cid:13)(cid:13)(cid:13)F
Proof. It is well-known that if Ψ has iid N(cid:0)0  1

for all rank-one and k× k-sparse matrices X (cid:63) (cid:60) 0 with probability exceeding 1− 3e−c2m for some
constant c2 > 0.

(cid:1) and we have (9) then (4) holds with high prob-

m ≥ c1k log

≤ C2ε√
n

ability. For example  using a standard covering argument and a union bound [23] shows that if
(9) holds for a sufﬁciently large constant c1 > 0 then we have (4) for a sufﬁciently small con-
stant δ2k with probability exceeding 1 − 2e−cm for some constant c > 0 that depends only
on δ2k. Therefore  Theorem 1 yields the desired result which holds with probability exceeding
1 − 2e−cm − e−C3n ≥ 1 − 3e−c2m for some constant c2 > 0 depending only on δ2k.

(9)

d
k

 

 

m

m

3 Numerical Experiments

(cid:107)(cid:98)X−X (cid:63)(cid:107)F

independently from N (0  1). The noise vector z is also Gaussian with independent N(cid:0)0  10−4(cid:1). The

We evaluated the performance of Algorithm 1 through some numerical simulations. The low-rank
estimation stage and the sparse estimation stage are implemented using the TFOCS package [24].
We considered the target k-sparse signal x(cid:63) to be in R256 (i.e.  d = 256). The support set of
of the target signal is selected uniformly at random and the entry values on this support are drawn
operator W and the matrix Ψ are drawn from some Gaussian ensembles as described in Corollary
1. We measured the relative error
of achieved by the compared methods over 100 trials
with sparsity level (i.e.  k) varying in the set {2  4  6  . . .   20}.
In the ﬁrst experiment  for each value of k  the pair (m  n) that determines the size W and Ψ are
selected from {(8k  24k)   (8k  32k)   (12k  36k)   (12k  48k)   (16k  48k)}. Figure 1 illustrates the
0.9 quantiles of the relative error versus k for the mentioned choices of m.
In the second experiment we compared the performance of Algorithm 1 to the convex optimization
methods that do not exploit the structure of the sensing vectors. The setup for this experiment is the
n = 3m  where (cid:100)r(cid:101) denotes the smallest integer greater than r. Figure 2 illustrates the 0.9 quantiles
of the measured relative errors for Algorithm 1  the semideﬁnite program (5) for λ = 0 and λ = 0.2 
and the (cid:96)1-minimization

same as in the ﬁrst experiment except for the size of W and Ψ; we chose m =(cid:6)2k(cid:0)1 + log d

(cid:1)(cid:7) and

(cid:107)X (cid:63)(cid:107)F

k

(cid:107)X(cid:107)1

argmin
subject to A (X) = y 

X

5

Figure 1: The empirical 0.9 quantile of the relative estimation error vs. sparsity for various choices
of m and n with d = 256.

and different trace- and/or (cid:96)1- minimization methods with d = 256  m = (cid:6)2k(cid:0)1 + log d

Figure 2: The empirical 0.9 quantile of the relative estimation error vs. sparsity for Algorithm 1

(cid:1)(cid:7)  and

k

n = 3m.

which are denoted by 2-stage  SDP  SDP+(cid:96)1  and (cid:96)1  respectively. The SDP-based method did not
perform signiﬁcantly different for other values of λ in our complementary simulations. The relative
error for each trial is also overlaid in Figure 2 visualize its empirical distribution. The empirical
performance of the algorithms are in agreement with the theoretical results. Namely in a regime

where n = O (m) = O(cid:0)k log d
(cid:1)  Algorithm 1 can produce accurate estimates whereas while the
explained intuitively by the fact that the (cid:96)1-minimization succeeds with n = O(cid:0)k2(cid:1) measurements
which for small values of k can be sufﬁciently close to the considered n = 3(cid:6)2k(cid:0)1 + log d
(cid:1)(cid:7)

other approaches fail in this regime. The SDP and SDP+(cid:96)1 show nearly identical performance. The
(cid:96)1-minimization  however  competes with Algorithm 1 for small values of k. This observation can be

k

k

measurements.

6

4 Proofs

Proof of Theorem 1. Clearly  B(cid:63) = Ψ X (cid:63)Ψ T is feasible in 6 because of A3. Therefore  we can

show that any solution (cid:98)B of (6) accurately estimates B(cid:63) using existing results on nuclear-norm

minimization. In particular  we can invoke [5  Theorem 2 and Section 4.3] which guarantees that for
some positive absolute constants C1  C(cid:48)

2  and C3 if (8) holds then

(cid:13)(cid:13)(cid:13)(cid:98)B − B(cid:63)(cid:13)(cid:13)(cid:13)F

≤ C(cid:48)
2ε√
n

 

holds for all valid B(cid:63)   thereby for all valid X (cid:63)  with probability exceeding 1 − e−C3n. Therefore 
with C = C(cid:48)
2  the target matrix X (cid:63) would be feasible in (7). Now  it sufﬁces to show that the
sparse estimation stage can produce an accurate estimate of X (cid:63). Recall that by A2  the matrix Ψ
is restricted isometry for 2k-sparse vectors. Let X be a matrix that is 2k × 2k-sparse  i.e.  a matrix
whose entries except for some 2k × 2k submatrix are all zeros. Applying (4) to the columns of X
and adding the inequalities yield

Because the columns of X TΨ T are also 2k-sparse we can repeat the same argument and obtain

(1 − δ2k)(cid:107)X(cid:107)2

(cid:13)(cid:13)(cid:13)X TΨ T(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)X TΨ T(cid:13)(cid:13)(cid:13)F

F

(1 − δ2k)

F ≤ (cid:107)Ψ X(cid:107)2

≤(cid:13)(cid:13)(cid:13)Ψ X TΨ T(cid:13)(cid:13)(cid:13)2
F ≤(cid:13)(cid:13)(cid:13)Ψ XΨ T(cid:13)(cid:13)(cid:13)2

= (cid:107)Ψ X(cid:107)F and

F ≤ (1 + δ2k)(cid:107)X(cid:107)2
F .

≤ (1 + δ2k)

(cid:13)(cid:13)(cid:13)Ψ X TΨ T(cid:13)(cid:13)(cid:13)F

F

=

(cid:13)(cid:13)(cid:13)X TΨ T(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Ψ XΨ T(cid:13)(cid:13)(cid:13)F

.

(10)

(11)

F
  the inequalities (10)

Using the facts that
and (11) imply that

(1 − δ2k)2 (cid:107)X(cid:107)2

(12)
The proof proceeds with an adaptation of the arguments used to prove accuracy of (cid:96)1-minimization
Furthermore  let S0 ⊆ [d] × [d] denote the support set of the k × k-sparse target X (cid:63). Deﬁne E0 to
be a d × d matrix that is identical to E over the index set S0 and zero elsewhere. By optimality of

in compressive sensing based on the restricted isometry property (see  e.g.  [22]). Let E =(cid:99)X−X (cid:63).
(cid:99)X and feasibility of X (cid:63) in (7) we have

F

F

.

≤ (1 + δ2k)2 (cid:107)X(cid:107)2

= (cid:107)X (cid:63) + E − E0 + E0(cid:107)1 ≥ (cid:107)X (cid:63) + E − E0(cid:107)1 − (cid:107)E0(cid:107)1

(cid:107)X (cid:63)(cid:107)1 ≥(cid:13)(cid:13)(cid:13)(cid:99)X

(cid:13)(cid:13)(cid:13)1

where the last line follows from the fact that X (cid:63) and E − E0 have disjoint supports. Thus  we have
(13)
Now consider a decomposition of E − E0 as the sum

(cid:107)E − E0(cid:107)1 ≤ (cid:107)E0(cid:107)1 ≤ k (cid:107)E0(cid:107)F .

= (cid:107)X (cid:63)(cid:107)1 + (cid:107)E − E0(cid:107)1 − (cid:107)E0(cid:107)1  
J(cid:88)

E − E0 =

j=2

j=1

Ej

Ej 

J(cid:88)

(cid:107)Ej(cid:107)F ≤ 1

≤ J(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) J(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)Ψ (E0 + E1) Ψ T(cid:13)(cid:13)(cid:13)2

(14)
such that for j ≥ 0 the d × d matrices Ej have disjoint support sets of size k × k except perhaps for
the last few matrices that might have smaller supports. More importantly  the partitioning matrices
Ej are chosen to have a decreasing Frobenius norm (i.e.  (cid:107)Ej(cid:107)F ≥ (cid:107)Ej+1(cid:107)F ) for j ≥ 1. We have
(cid:107)E − E0(cid:107)1 ≤ (cid:107)E0(cid:107)F ≤ (cid:107)E0 + E1(cid:107)F   (15)
the fact that (cid:107)Ej(cid:107)∞ ≤
where the chain of inequalities follow from the triangle inequality 
k2 (cid:107)Ej−1(cid:107)1 by construction  the fact that the matrices Ej have disjoint support and satisfy (14) 
 Ψ T
(cid:43)
1
the bound (13)  and the fact that E0 and E1 are orthogonal. Furthermore  we have
(cid:12)(cid:12)(cid:12)(cid:68)
Ψ EiΨ T  Ψ EjΨ T(cid:69)(cid:12)(cid:12)(cid:12)  
J(cid:88)
1(cid:88)

(cid:42)
≤(cid:13)(cid:13)(cid:13)Ψ (E0 + E1) Ψ T(cid:13)(cid:13)(cid:13)F

E − J(cid:88)
(cid:13)(cid:13)(cid:13)Ψ EΨ T(cid:13)(cid:13)(cid:13)F

(cid:107)Ej−1(cid:107)1 ≤ 1

Ψ (E0 + E1) Ψ T  Ψ

Ej

j=2

j=2

j=2

+

=

F

k

k

i=0

j=2

(16)

7

1(cid:88)
J(cid:88)
(cid:107)Ei(cid:107)F (cid:107)Ej(cid:107)F
1(cid:88)
J(cid:88)

j=2

i=0

i=0

j=2

(cid:107)Ei(cid:107)F (cid:107)Ej(cid:107)F
(cid:19)

(1 + δ2k)(cid:107)E0 + E1(cid:107)F + 2δ2k

≤ 2Cε√
n
≤ 2Cε√
n

(1 + δ2k)(cid:107)E0 + E1(cid:107)F + 2δ2k ((cid:107)E0(cid:107)F + (cid:107)E1(cid:107)F )(cid:107)E0 + E1(cid:107)F

≤ (cid:107)E0 + E1(cid:107)F

(1 + δ2k) + 2

2δ2k (cid:107)E0 + E1(cid:107)F

√

(cid:18) 2Cε√

n

≤(cid:13)(cid:13)(cid:13)Ψ(cid:99)XΨ T − (cid:98)B

2Cε√
i ∈ {0  1} and j ≥ 2 we have

where the ﬁrst term is obtained by the Cauchy-Schwarz inequality and the summation is obtained by

≤
n . Furthermore  Lemma 1 below which is adapted from [22  Lemma 2.1] guarantees that for

the triangle inequality. Because E =(cid:99)X − X (cid:63) by deﬁnition  the triangle inequality and the fact that
(cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)Ψ X (cid:63)Ψ T − (cid:98)B
(cid:13)(cid:13)(cid:13)Ψ EΨ T(cid:13)(cid:13)(cid:13)F
X (cid:63) and(cid:99)X are feasible in (7) imply that
Ψ EiΨ T  Ψ EjΨ T(cid:69)(cid:12)(cid:12)(cid:12) ≤ 2δ2k (cid:107)Ei(cid:107)F (cid:107)Ej(cid:107)F . Therefore  we obtain
(cid:12)(cid:12)(cid:12)(cid:68)
F ≤(cid:13)(cid:13)(cid:13)Ψ (E0 + E1) Ψ T(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)Ψ (E0 + E1) Ψ T(cid:13)(cid:13)(cid:13)F

(1 − δ2k)2 (cid:107)E0 + E1(cid:107)2

(cid:13)(cid:13)(cid:13)F

≤ 2Cε√
n

+ 2δ2k

+

F

(cid:17) ≈ 0.216  then we have γ := (1 − δ2k)2 − 2
bound in (12)  the bound (15)  and the fact that (cid:107)E0(cid:107)F + (cid:107)E1(cid:107)F ≤ √
√

where the chain of inequalities follow from the lower bound in (12)  the bound (16)  the upper
2(cid:107)E0 + E1(cid:107)F . If δ2k <
2δ2k > 0 and thus

1 −(cid:112)

(cid:16)

1 +

1 +

√

√

2

2

(cid:107)E0 + E1(cid:107)F ≤ 2C (1 + δ2k) ε

√

.

γ

n

Adding the above inequality to (13) and applying the triangle then yields the desired result.
Lemma 1. Let Ψ be a matrix obeying (4). Then for any pair of k × k-sparse matrices X and X(cid:48)
with disjoint supports we have

(cid:12)(cid:12)(cid:12)(cid:68)
Ψ XΨ T  Ψ X(cid:48)Ψ T(cid:69)(cid:12)(cid:12)(cid:12) ≤ 2δ2k (cid:107)X(cid:107)F
(cid:18)(cid:13)(cid:13)(cid:13)Ψ(cid:0)X + X(cid:48)(cid:1) Ψ T(cid:13)(cid:13)(cid:13)2

(cid:13)(cid:13)X(cid:48)(cid:13)(cid:13)F .
−(cid:13)(cid:13)(cid:13)Ψ(cid:0)X − X(cid:48)(cid:1) Ψ T(cid:13)(cid:13)(cid:13)2

that X and X(cid:48) have unit Frobenius norm.

= 1
4

(cid:19)

Proof. Suppose

(cid:68)
Ψ XΨ T  Ψ X(cid:48)Ψ T(cid:69)

Using the

identity
and the fact that

F

F

2

≤(cid:68)

(1 − δ2k)2 − (1 + δ2k)2

Ψ XΨ T  Ψ X(cid:48)Ψ T(cid:69) ≤ (1 + δ2k)2 − (1 − δ2k)2

X and X(cid:48) have disjoint supports  it follows from (12) that
−2δ2k =
The general result follows immediately as the desired inequality is homogeneous in the Frobenius
norms of X and X(cid:48).
Lemma 2 (Projected estimator). Let S be a closed nonempty subset of a normed vector space

(V (cid:107)·(cid:107)). Suppose that for v(cid:63) ∈ S we have an estimator(cid:98)v ∈ V  not necessarily in S  that obeys
(cid:107)(cid:98)v − v(cid:63)(cid:107) ≤ . If(cid:101)v denotes a projection of(cid:98)v onto S  then we have (cid:107)(cid:101)v − v(cid:63)(cid:107) ≤ 2.
Proof. By deﬁnition(cid:101)v ∈ argminv∈S (cid:107)v −(cid:98)v(cid:107) . Therefore  because v(cid:63) ∈ S we have

(cid:107)(cid:101)v − v(cid:63)(cid:107) ≤ (cid:107)(cid:98)v − v(cid:63)(cid:107) + (cid:107)(cid:101)v −(cid:98)v(cid:107) ≤ 2(cid:107)(cid:98)v − v(cid:63)(cid:107) ≤ 2.

= 2δ2k.

2

Acknowledgements

This work was supported by ONR grant N00014-11-1-0459  and NSF grants CCF-1415498 and
CCF-1422540.

8

References
[1] Jacopo Bertolotti  Elbert G. van Putten  Christian Blum  Ad Lagendijk  Willem L. Vos  and Allard P.
Mosk. Non-invasive imaging through opaque scattering layers. Nature  491(7423):232–234  Nov. 2012.
[2] Antoine Liutkus  David Martina  Sébastien Popoff  Gilles Chardon  Ori Katz  Geoffroy Lerosey  Sylvain
Gigan  Laurent Daudet  and Igor Carron. Imaging with nature: Compressive imaging using a multiply
scattering medium. Scientiﬁc Reports  volume 4  article no. 5552  Jul. 2014.

[3] Emmanuel J. Candès  Thomas Strohmer  and Vladislav Voroninski. PhaseLift: Exact and stable signal
recovery from magnitude measurements via convex programming. Communications on Pure and Applied
Mathematics  66(8):1241–1274  2013.

[4] Emmanuel J. Candès and Xiaodong Li. Solving quadratic equations via PhaseLift when there are about

as many equations as unknowns. Foundations of Computational Mathematics  14(5):1017–1026  2014.

[5] R. Kueng  H. Rauhut  and U. Terstiege. Low rank matrix recovery from rank one measurements. Applied

and Computational Harmonic Analysis  2015. In press. Preprint arXiv:1410.6913 [cs.IT].

[6] Joel A. Tropp. Convex recovery of a structured signal from independent random linear measurements.

Preprint arXiv:1405.1102 [cs.IT]  2014.

[7] Irène Waldspurger  Alexandre d’Aspremont  and Stéphane Mallat. Phase recovery  MaxCut and complex

semideﬁnite programming. Mathematical Programming  149(1-2):47–81  2015.

[8] Matthew L. Moravec  Justin K. Romberg  and Richard G. Baraniuk. Compressive phase retrieval.

Proceedings of SPIE Wavelets XII  volume 6701  pages 670120 1–11  2007.

In

[9] Yoav Shechtman  Yonina C. Eldar  Alexander Szameit  and Mordechai Segev. Sparsity based sub-
wavelength imaging with partially incoherent light via quadratic compressed sensing. Optics Express 
19(16):14807–14822  Aug. 2011.

[10] Yoav Shechtman  Amir Beck  and Yonina C. Eldar. GESPAR: Efﬁcient phase retrieval of sparse signals.

Signal Processing  IEEE Transactions on  62(4):928–938  Feb. 2014.

[11] Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase retrieval using alternating minimization. In

Advances in Neural Information Processing Systems 26 (NIPS 2013)  pages 2796–2804  2013.

[12] Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via convex

programming. SIAM Journal on Mathematical Analysis  45(5):3019–3033  2013.

[13] Henrik Ohlsson  Allen Yang  Roy Dong  and Shankar Sastry. CPRL–an extension of compressive sensing
to the phase retrieval problem. In Advances in Neural Information Processing Systems 25 (NIPS 2012) 
pages 1367–1375  2012.

[14] David Gross. Recovering low-rank matrices from few coefﬁcients in any basis. Information Theory  IEEE

Transactions on  57(3):1548–1566  Mar. 2011.

[15] Samet Oymak  Amin Jalali  Maryam Fazel  Yonina Eldar  and Babak Hassibi. Simultaneously structured
Information Theory  IEEE Transactions on 

models with application to sparse and low-rank matrices.
61(5):2886–2908  2015.

[16] P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message passing.

Signal Processing  IEEE Transactions on  63(4):1043–1055  February 2015.

[17] Ramtin Pedarsani  Kangwook Lee  and Kannan Ramchandran. Phasecode: Fast and efﬁcient compressive
phase retrieval based on sparse-graph codes. In Communication  Control  and Computing (Allerton)  52nd
Annual Allerton Conference on  pages 842–849  Sep. 2014. Extended preprint arXiv:1408.0034
[cs.IT].

[18] Mark Iwen  Aditya Viswanathan  and Yang Wang. Robust sparse phase retrieval made easy. Applied and

Computational Harmonic Analysis  2015. In press. Preprint arXiv:1410.5295 [math.NA].

[19] Emmanuel J. Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger ﬂow: Theory

and algorithms. Information Theory  IEEE Transactions on  61(4):1985–2007  Apr. 2015.

[20] Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied

and Computational Harmonic Analysis  27(3):265–274  2009.

[21] Deanna Needell and Joel A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate

samples. Applied and Computational Harmonic Analysis  26(3):301–321  2009.

[22] Emmanuel J. Candès. The restricted isometry property and its implications for compressed sensing.

Comptes Rendus Mathematique  346(9-10):589–592  2008.

[23] Richard Baraniuk  Mark Davenport  Ronald DeVore  and Michael Wakin. A simple proof of the restricted

isometry property for random matrices. Constructive Approximation  28(3):253–263  2008.

[24] Stephen R. Becker  Emmanuel J. Candès  and Michael C. Grant. Templates for convex cone problems
with applications to sparse signal recovery. Mathematical Programming Computation  3(3):165–218 
2011.

9

,Sohail Bahmani
Justin Romberg