2019,Learning Deterministic Weighted Automata with Queries and Counterexamples,We present an algorithm for reconstruction of a probabilistic deterministic finite automaton (PDFA) from a given black-box language model  such as a recurrent neural network (RNN). 
The algorithm is a variant of the exact-learning algorithm L*  adapted to work in a probabilistic setting under noise.
The key insight of the adaptation is the use of conditional probabilities when making observations on the model  and the introduction of a variation tolerance when comparing observations. 
When applied to RNNs  our algorithm returns models with better or equal word error rate (WER) and normalised distributed cumulative gain (NDCG) than achieved by n-gram or weighted finite automata (WFA) approximations of the same networks. The PDFAs capture a richer class of languages than n-grams  and are guaranteed to be stochastic and deterministic -- unlike the WFAs.,Learning Deterministic Weighted Automata

with Queries and Counterexamples

Gail Weiss
Technion

sgailw@cs.technion.ac.il

Yoav Goldberg
Bar Ilan University
Allen Institute for AI
yogo@cs.biu.ac.il

Eran Yahav

Technion

yahave@cs.technion.ac.il

Abstract

We present an algorithm for extraction of a probabilistic deterministic ﬁnite au-
tomaton (PDFA) from a given black-box language model  such as a recurrent
neural network (RNN). The algorithm is a variant of the exact-learning algorithm
L⇤  adapted to a probabilistic setting with noise. The key insight is the use of
conditional probabilities for observations  and the introduction of a local tolerance
when comparing them. When applied to RNNs  our algorithm often achieves better
word error rate (WER) and normalised distributed cumulative gain (NDCG) than
that achieved by spectral extraction of weighted ﬁnite automata (WFA) from the
same networks. PDFAs are substantially more expressive than n-grams  and are
guaranteed to be stochastic and deterministic – unlike spectrally extracted WFAs.

1

Introduction

We address the problem of learning a probabilistic deterministic ﬁnite automaton (PDFA) from a
trained recurrent neural network (RNN) [17]. RNNs  and in particular their gated variants GRU [13 
14] and LSTM [21]  are well known to be very powerful for sequence modelling  but are not
interpretable. PDFAs  which explicitly list their states  transitions  and weights  are more interpretable
than RNNs [20]  while still being analogous to them in behaviour: both emit a single next-token
distribution from each state  and have deterministic state transitions given a state and token. They are
also much faster to use than RNNs  as their sequence processing does not require matrix operations.
We present an algorithm for reconstructing a PDFA from any given black-box distribution over
sequences  such as an RNN trained with a language modelling objective (LM-RNN). The algorithm
is applicable for reconstruction of any weighted deterministic ﬁnite automaton (WDFA)  and is
guaranteed to return a PDFA when the target is stochastic – as an LM-RNN is.
Weighted Finite Automata (WFA) A WFA is a weighted non-deterministic ﬁnite automaton  capable
of encoding language models but also other  non-stochastic weighted functions. Ayache et al. [2]
and Okudono et al. [24] show how to apply spectral learning [5] to an LM-RNN to learn a weighted
ﬁnite automaton (WFA) approximating its behaviour.
Probabilistic Deterministic Finite Automata (PDFAs) are a weighted variant of DFAs where each
state deﬁnes a categorical next-token distribution. Processing a sequence in a PDFA is simple: input
tokens are processed one by one  getting the next state and probability for each token by table lookup.
WFAs are non-deterministic and so not immediately analogous to RNNs. They are also slower to use
than PDFAs  as processing each token in an input sequence requires a matrix multiplication. Finally 
spectral learning algorithms are not guaranteed to return stochastic hypotheses even when the target
is stochastic – though this can remedied by using quadratic weighted automata [3] and normalising
their weights. For these reasons we prefer PDFAs over WFAs for RNN approximation. Formally:

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Problem Deﬁnition Given an LM-RNN R  ﬁnd a PDFA W approximating R  such that for any preﬁx
p its next-token distributions in W and in R have low total variation distance between them.
Existing works on PDFA reconstruction assume a sample based paradigm: the target cannot be
queried explicitly for a sequence’s probability or conditional probabilities [15  9  6]. As such  these
methods cannot take full advantage of the information available from an LM-RNN1. Meanwhile 
most work on the extraction of ﬁnite automata from RNNs has focused on “binary” deterministic
ﬁnite automata (DFAs) [25  12  38  39  23]  which cannot fully express the behaviour of an LM-RNN.
Our Approach Following the successful application of L⇤ [1] to RNNs for DFA extraction [39]  we
develop an adaptation of L⇤ for the weighted case. The adaptation returns a PDFA when applied to a
stochastic target such as an LM-RNN. It interacts with an oracle using two types of queries:

1. Membership Queries: requests to give the target probability of the last token in a sequence.
2. Equivalence Queries: requests to accept or reject a hypothesis PDFA  returning a counterex-
ample — a sequence for which the hypothesis automaton and the target language diverge
beyond the tolerance on the next token distribution — if rejecting.

The algorithm alternates between ﬁlling an observation table with observations of the target behaviour 
and presenting minimal PDFAs consistent with that table to the oracle for equivalence checking. This
continues until an automaton is accepted. The use of conditional properties in the observation table
prevents the observations from vanishing to 0 on low probabilities. To the best of our knowledge  this
is the ﬁrst work on learning PDFAs from RNNs.
A key insight of our adaptation is the use of an additive variation tolerance t2[0  1] when comparing
rows in the table. In this framework  two probability vectors are considered t-equal if their proba-
bilities for each event are within t of each other. Using this tolerance enables us to extract a much
smaller PDFA than the original target  while still making locally similar predictions to it on any given
sequence. This is necessary because RNN states are real valued vectors  making the potential number
of reachable states in an LM-RNN unbounded. The tolerance is non-transitive  making construction
of PDFAs from the table more challenging than in L⇤. Our algorithm suggests a way to address this.
Even with this tolerance  reaching equivalence may take a long time for large target PDFAs  and
so we design our algorithm to allow anytime stopping of the extraction. The method allows the
extraction to be limited while still maintaining certain guarantees on the reconstructed PDFA.
Note. While this paper only discusses RNNs  the algorithm itself is actually agnostic to the underlying
structure of the target  and can be applied to any language model. In particular it may be applied to
transformers [35  16]. However  in this case the analogy to PDFAs breaks down.

Contributions The main contributions of this paper are:

1. An algorithm for reconstructing a WDFA from any given weighted target  and in particular

a PDFA if the target is stochastic.

2. A method for anytime extraction termination while still maintaining correctness guarantees.
3. An implementation of the algorithm 2 and an evaluation over extraction from LM-RNNs 

including a comparison to other LM reconstruction techniques.

2 Related Work

In Weiss et al [39]  we presented a method for applying Angluin’s exact learning algorithm L⇤[1]
to RNNs  successfully extracting deterministic ﬁnite automata (DFAs) from given binary-classiﬁer
RNNs. This work expands on this by adapting L⇤to extract PDFAs from LM-RNNs. To apply exact
learning to RNNs  one must implement equivalence queries: requests to accept or reject a hypothesis.
Okudono et al. [24] show how to adapt the equivalence query presented in [39] to the weighted case.
There exist many methods for PDFA learning  originally for acyclic PDFAs [31  29  10]  and later for
PDFAs in general [15  9  33  26  11  6]. These methods split and merge states in the learned PDFAs

1It is possible to adapt these methods to an active learning setting  in which they may query an oracle for
exact probabilities. However  this raises other questions: on which sufﬁxes are preﬁxes compared? How does
one pool the probabilities of two preﬁxes when merging them? We leave such an adaptation to future work.

2Available at www.github.com/tech-srl/weighted_lstar

2

according to sample-based estimations of their conditional distributions. Unfortunately  they require
very large sample sets to succeed (e.g.  [15] requires ~13m samples for a PDFA with |Q| |⌃| = 2).
Distributions over ⌃⇤ can also be represented by WFAs  though these are non-deterministic. These
can be learned using spectral algorithms  which use SVD decomposition and |⌃| + 1 matrices of
observations from the target to build a WFA [4  5  8  22]. Spectral algorithms have recently been
applied to RNNs to extract WFAs representing their behaviour [2  24  28]  we compare to [2] in this
work. The choice of observations used is also a focus of research in this ﬁeld [27].
For more on language modelling  see the reviews of Goodman [19] or Rosenfeld [30]  or the Sequence
Prediction Challenge (SPiCe) [7] and Probabilistic Automaton Challenge (PAutomaC) [36].

3 Background

Sequences and Notations For a ﬁnite alphabet ⌃  the set of ﬁnite sequences over ⌃ is denoted by
⌃⇤  and the empty sequence by ". For any ⌃ and stopping symbol $ /2 ⌃  we denote ⌃$   ⌃ [{ $} 
and ⌃+$   ⌃⇤·⌃$ – the set of s 2 ⌃$ \ {"} where the stopping symbol may only appear at the end.
For a sequence w 2 ⌃⇤  its length is denoted |w|  its concatenation after another sequence u is denoted
u·w  its i-th element is denoted wi  and its preﬁx of length k | w| is denoted w:k = w1·...·wk. We
use the shorthand w1   w|w| and w:1   w:|w|1. A set of sequences S ✓ ⌃⇤ is said to be preﬁx
closed if for every w 2 S and k | w|  wk 2 S. Sufﬁx closedness is deﬁned analogously.
For any ﬁnite alphabet ⌃ and set of sequences S ✓ ⌃⇤  we assume some internal ordering of the set’s
elements s1  s2  ... to allow discussion of vectors of observations over those elements.
Probabilistic Deterministic Finite Automata (PDFAs) are tuples A = hQ  ⌃  Q  qi  Wi such that
Q is a ﬁnite set of states  qi 2 Q is the initial state  ⌃ is the ﬁnite input alphabet  Q : Q ⇥ ⌃ ! Q
is the transition function and W : Q ⇥ ⌃$ ! [0  1] is the transition weight function  satisfying
P2⌃$
The recurrent application of Q to a sequence is denoted by ˆ : Q⇥⌃⇤ ! Q  and deﬁned: ˆ(q  ")   q
and ˆ(q  w·a)   Q(ˆ(q  w)  a) for every q 2 Q  a 2 ⌃  w 2 ⌃⇤. We abuse notation to denote:
ˆ(w)   ˆ(qi  w) for every w 2 ⌃⇤. If for every q 2 Q there exists a series of non-zero transitions
reaching a state q with W (q  $) > 0  then A deﬁnes a distribution PA over ⌃⇤ as follows: for every
w 2 ⌃⇤  PA(w) = W (ˆ(w)  $) ·Qi|w| W (ˆ(w:i1)  wi).
Language Models (LMs) Given a ﬁnite alphabet ⌃  a language model M over ⌃ is a model deﬁning
a distribution PM over ⌃⇤. For any w 2 ⌃⇤  S ⇢ ⌃+$  and  2 ⌃  P = PM induces the following:

W (q  ) = 1 for every q 2 Q.

• Preﬁx Probability: P p(w)  Pv2⌃⇤ P (w·v).
• Last Token Probability: if P p(w) > 0  then P l(w·)   P p(w·)
• Last Token Probabilities Vector: if P p(w) > 0  P l
• Next Token Distribution: P n(w) :⌃ $ ! [0  1]  deﬁned: P n(w)() = P l(w·).

P p(w) and P l(w·$)   P (w)
P p(w).
S(w)   (P l(w·s1)  ...  P l(w·s|S|)).

Variation Tolerance Given two categorical distributions p and q  their total variation distance is
deﬁned (p  q)   kp  qk1  i.e.  the largest difference in probabilities that they assign to the same
event. Our algorithm tolerates some variation distance between next-token probabilities  as follows:
Two event probabilities p1  p2 are called t-equal and denoted p1 ⇡t p2 if |p1 p2| t. Similarly  two
vectors of probabilities v1  v2 2 [0  1]n are called t-equal and denoted v1 ⇡t v2 if kv1 v2k1  t 
(|v1i  v2i|)  t. For any distribution P over ⌃⇤  S ⇢ ⌃+$  and p1  p2 2 ⌃⇤  we
i.e. if max
i2[n]
S(p2)  or simply p1 ⇡(S t) p2 if P is clear from context. For
denote p1 ⇡(P S t) p2 if P l
any two language models A  B over ⌃⇤ and w 2 ⌃+$  we say that A  B are t-consistent on w if
P l
A(u) ⇡t P l
Oracles and Observation Tables Given an oracle O  an observation table for O is a sequence indexed
matrix OP S of observations taken from it  with the rows indexed by preﬁxes P and the columns

B(u) for every preﬁx u 6= " of w. We call t the variation tolerance.

S(p1) ⇡t P l

3

by sufﬁxes S. The observations are OP S(p  s) = O(p·s) for every p 2 P   s 2 S. For any p 2 ⌃⇤
we denote OS(p)   (O(p·s1)  ... O(p·s2))  and for every p 2 P the p-th row in OP S is denoted
OP S(p)   OS(p). In this work we use an oracle for the last-token probabilities of the target 
O(w) = P l(w) for every w 2 ⌃+$  and maintain S ✓ ⌃+$.
Recurrent Neural Networks (RNNs) An RNN is a recursive parametrised function ht = f (xt  ht1)
with initial state h0  such that ht 2 Rn is the state after time t and xt 2 X is the input at time t. A
language model RNN (LM-RNN) over an alphabet X =⌃ is an RNN coupled with a prediction
function g : h 7! d  where d 2 [0  1]|⌃$| is a vector representation of a next-token distribution.
RNNs differ from PDFAs only in that their number of reachable states (and so number of different
next-token distributions for sequences) may be unbounded.

4 Learning PDFAs with Queries and Counterexamples

In this section we describe the details of our algorithm. We explain why a direct application of L⇤ to
PDFAs will not work  and then present our non-trivial adaptation. Our adaptation does not rely on
the target being stochastic  and can in fact be applied to reconstruct any WDFA from an oracle.
Direct application of L⇤ does not work for LM-RNNs: L⇤ is a polynomial-time algorithm for
learning a deterministic ﬁnite automaton (DFA) from an oracle. It can be adapted to work with
oracles giving any ﬁnite number of classiﬁcations to sequences  and can be naively adapted to a
probabilistic target P with ﬁnite possible next-token distributions {P n(w)|w 2 ⌃⇤} by treating each
next-token distribution as a sequence classiﬁcation. However  this will not work for reconstruction
from RNNs. This is because the set of reachable states in a given RNN is unbounded  and so also
the set of next-token distributions. Thus  in order to practically adapt L⇤ to extract PDFAs from
LM-RNNs  we must reduce the number of classes L⇤ deals with.
Variation Tolerance Our algorithm reduces the number of classes it considers by allowing an additive
variation tolerance t 2 [0  1]  and considering t-equality (as presented in Section 3) as opposed to
actual equality when comparing probabilities. In introducing this tolerance we must handle the fact
that it may be non-transitive: there may exist a  b  c 2 [0  1] such that a ⇡t b  b ⇡t c  but a 6⇡t c. 3
To avoid potentially grouping together all predictions on long sequences  which are likely to have
very low probabilities  our algorithm observes only local probabilities. In particular  the algorithm
uses an oracle that gives the last-token probability for every non-empty input sequence.

4.1 The Algorithm

The algorithm loops over three main steps: (1) expanding an observation table OP S until it is closed
and consistent  (2) constructing a hypothesis automaton  and (3) making an equivalence query about
the hypothesis. The loop repeats as long as the oracle returns counterexamples for the hypotheses. In
our setting  counterexamples are sequences w 2 ⌃⇤ after which the hypothesis and the target have
next-token distributions that are not t-equal. They are handled by adding all of their preﬁxes to P .
Our algorithm expects last token probabilities from the oracle  i.e.: O(w) = P l
T (w) where PT is the
T (")  which is undeﬁned. To observe the entirety of
target distribution. The oracle is not queried on P l
every preﬁx’s next-token distribution  OP S is initiated with P = {"}  S =⌃ $.
Step 1: Expanding the observation table OP S is expanded as in L⇤ [1]  but with the deﬁnition
of row equality relaxed. Precisely  it is expanded until:

1. Closedness For every p1 2 P and  2 ⌃  there exists some p2 2 P such that p1· ⇡S t p2.
2. Consistency For every p1  p2 2 P such that p1 ⇡S t p2  for every  2 ⌃  p1· ⇡S t p2·.
The table expansion is managed by a queue L initiated to P   from which preﬁxes p are processed
one at a time as follows: If p /2 P   and there is no p0 2 P s.t. p ⇡(t S) p0  then p is added to P . If
p 2 P already  then it is checked for inconsistency  i.e. whether there exist p0  s.t. p ⇡(t S) p0 but
3We could deﬁne a variation tolerance by quantisation of the distribution space  which would be transitive.

However this may be unnecessarily aggressive at the edges of the intervals.

4

p· 6⇡(t S) p0·. In this case a separating sufﬁx ˜s  P l
T (p0··˜s) is added to S  such that
now p 6⇡t S p0  and the expansion restarts. Finally  if p 2 P then L is updated with p·⌃.
As in L⇤  checking closedness and consistency can be done in arbitrary order. However  if the
algorithm may be terminated before OP S is closed and consistent  it is better to process L in order of
preﬁx probability (see section 4.2).

T (p··˜s) 6⇡t P l

Step 2: PDFA construction Intuitively  we would like to group equivalent rows of the observation
table to form the states of the PDFA  and map transitions between these groups according to the table’s
observations. The challenge in the variation-tolerating setting is that t-equality is not transitive.
Formally  let C be a partitioning (clustering) of P   and for each p 2 P let c(p) 2 C be the partition
(cluster) containing p. C should satisfy:
1. Determinism For every c 2 C  p1  p2 2 c   2 ⌃: p1·  p2· 2 P =) c(p1·) = c(p2·).
2. t-equality (Cliques) For every c 2 C and p1  p2 2 c  p1 ⇡(t S) p2.
For c 2 C   2 ⌃  we denote Cc  = {c(p·)|p 2 c  p· 2 P} the next-clusters reached from c with
  and kc    |Cc |. Note that C satisﬁes determinism iff kc   1 for every c 2 C   2 ⌃. Note
also that the constraints are always satisﬁable by the clustering C = {{p}}p2P
We present a 4-step algorithm to solve these constraints while trying to avoid excessive partitions: 4
1. Initialisation: The preﬁxes p 2 P are partitioned into some initial clustering C according to
2. Determinism I: C is reﬁned until it satisﬁes determinism: clusters c 2 C with tokens  for
3. Cliques: Each cluster is reﬁned into cliques (with respect to t-equality).
4. Determinism II: C is again reﬁned until it satisﬁes determinism  as in (2).

which kc  > 1 are split by next-cluster equivalence into kc  new clusters.

the t-equality of their rows  OS(p).

Note that reﬁning a partitioning into cliques may break determinism  but reﬁning into a deterministic
partitioning will not break cliques. In addition  when only allowed to reﬁne clusters (and not merge
them)  all determinism reﬁnements are necessary. Hence the order of the last 3 stages.
Once the clustering C is found  a PDFA A = hC  ⌃  Q  c(")  Wi is constructed from it. Where
possible  Q is deﬁned directly by C: for every p· 2 P   Q(c(p)  )   c(p·). For c   for which
kc  = 0  Q(c  ) is set as the best cluster match for p·  where p = argmaxp2c P p
T (p). This is
chosen according to the heuristics presented in Section 4.2. The weights W are deﬁned as follows:
for every c 2 C   2 ⌃$  W (c  )   Pp2c P p
Step 3: Answering Equivalence Queries We sample the target LM-RNN and hypothesis PDFA
A a ﬁnite number of times  testing every preﬁx of each sample to see if it is a counterexample. If none
is found  we accept A. Though simple  we ﬁnd this method to be sufﬁciently effective in practice. A
more sophisticated approach is presented in [24].

T (p)·P l
Pp2c P p
T (p)

T (p·)

.

4.2 Practical Considerations

We present some methods and heuristics that allow a more effective application of the algorithm to
large (with respect to |⌃|  |Q|) or poorly learned grammars.
Anytime Stopping In case the algorithm runs for too long  we allow termination before OP S is
closed and consistent  which may be imposed by size or time limits on the table expansion. If |S|
reaches its limit  the table expansion continues but stops checking consistency. If the time or |P|
limits are reached  the algorithm stops  constructing and accepting a PDFA from the table as is.
The construction is unchanged up to the fact that some of the transitions may not have a deﬁned
destination  for these we use a “best cluster match” as described in section 4.2. This does not harm
the guarantees on t-consistency between OP S and the returned PDFA discussed in Section 5.
Order of Expansion As some preﬁxes will not be added to P under anytime stopping  the order
in which rows are checked for closedness and consistency matters. We sort L by preﬁx weight.

4We describe our implementation of these stages in appendix C.

5

Moreover  if a preﬁx p1 being considered is found inconsistent w.r.t. some p2 2 P   2 ⌃$  then
all such pairs p2  are considered and the separating sufﬁx ˜s 2 ·S  O(p1·˜s) 6⇡t O(p2·˜s) with the
highest minimum conditional probability maxp2mini=1 2

) is added to S.

P p
T (pi·˜s)
P p
T (pi

Best Cluster Match Given a preﬁx p /2 P and set of clusters C  we seek a best ﬁt c 2 C for p. First
we ﬁlter C for the following qualities until one is non-empty  in order of preference: (1) c0 = c [{ p}
is a clique w.r.t. t-equality. (2) There exists some p0 2 c such that p0 ⇡(t S) p  and c is not a clique.
(3) There exists some p0 2 c such that p0 ⇡(t S) p. If no clusters satisfy these qualities  we remain
with C. From the resulting group C0 of potential matches  the best match could be the cluster c
minimising ||OS(p0)  OS(p)||1  p0 2 c. In practice  we choose from C0 arbitrarily for efﬁciency.
Sufﬁx and Preﬁx Thresholds Occasionally when checking the consistency of two rows p1 ⇡t p2 
a separating sufﬁx ·s 2 ⌃·S will be found that is actually very unlikely to be seen after p1 or p2. In
this case it is unproductive to add ·s to S. Moreover – especially as RNNs are unlikely to perfectly
learn a probability of 0 for some event – it is possible that going through ·s will reach a large number
of ‘junk’ states. Similarly when considering a preﬁx p  if P l
T (p) is very low then it is possible that it
is the failed encoding of probability 0  and that all states reachable through p are not useful.
We introduce thresholds "S and "P for both sufﬁxes and preﬁxes. When a potential separating sufﬁx
˜s is found from preﬁxes p1 and p2  it is added to S only if mini=1 2P p(pi·˜s)/P p(pi)  "S. Similarly 
potential new rows p /2 P are only added to P if P l(p)  "P .
Finding Close Rows We maintain P in a KD-tree T indexed by row entries OP S(p)  with one
level for every column s 2 S. When considering of a preﬁx p·  we use T to get the subset of all
potentially t-equal preﬁxes. T ’s levels are split into equal-length intervals  we ﬁnd 2t to work well.

Choosing the Variation Tolerance
In our initial experiments (on SPiCes 0-3)  we used t = 1/|⌃|.
The intuition was that given no data  the fairest distribution over |⌃| is the uniform distribution  and
so this may also be a reasonable threshold for a signiﬁcant difference between two probabilities.
In practice  we found that t = 0.1 often strongly differentiates states even in models with larger
alphabets – except for SPiCe 1  where t = 0.1 quickly accepted a model of size 1. A reasonable
strategy for choosing t is to begin with a large one  and reduce it if equivalence is reached too quickly.

5 Guarantees

We note some guarantees on the extracted model’s qualities and relation to its target model. Formal
statements and full proofs for each of the guarantees listed here are given in appendix A.

Model Qualities The model is guaranteed to be deterministic by construction. Moreover  if the
target is stochastic  then the returned model is guaranteed to be stochastic as well.

Reaching Equivalence
If the algorithm terminates successfully (i.e.  having passed an equivalence
query)  then the returned model is t-consistent with the target on every sequence w 2 ⌃⇤  by
deﬁnition of the query. In practice we have no true oracle and only approximate equivalence queries
by sampling the models  and so can only attain a probable guarantee of their relative t-consistency.

t-Consistency and Progress No matter when the algorithm is stopped  the returned model is
always t-consistent with its target on every p 2 P·⌃$  where P is the set of preﬁxes in the table OP S.
Moreover  as long as the algorithm is running  the preﬁx set P is always increased within a ﬁnite
number of operations. This means that the algorithm maintains a growing set of preﬁxes on which
any PDFA it returns is guaranteed to be t-consistent with the target. In particular  this means that if
equivalence is not reached  at least the algorithm’s model of the target improves for as long as it runs.

6 Experimental Evaluation

We apply our algorithm to 2-layer LSTMs trained on grammars from the SPiCe competition [7] 
adaptations of the Tomita grammars [34] to PDFAs  and small PDFAs representing languages with

6

unbounded history. The LSTMs have input dimensions 2-60 and hidden dimensions 20-100. The
LSTMs and their training methods are fully described in Appendix E.
Compared Methods We compare our algorithm to the sample-based method ALERGIA [9]  the
spectral algorithm used in [2]  and n-grams. An n-gram is a PDFA whose states are a sliding
window of length n  1 over the input sequence  with transition function 1·...·n  7! 2·...n·.
The probability of a token  from state s 2 ⌃n1 is the MLE estimate N (s·)
N (s)   where N (w) is the
number of times the sequence w appears as a subsequence in the samples. For ALERGIA  we use the
PDFA/DFA inference toolkit FLEXFRINGE [37].
Target Languages We train 10 RNNs on a subset of the SPiCe grammars  covering languages
generated by HMMs  and languages from the NLP  software  and biology domains. We train 7 RNNs
on PDFA adaptations of the 7 Tomita languages [34]  made from the minimal DFA for each language
by giving each of its states a next-token distribution as a function of whether it is accepting or not.
We give a full description of the Tomita adaptations and extraction results in appendix D. As we show
in (6.1)  the n-gram models prove to be very strong competitors on the SPiCe languages. To this end 
we consider three additional languages that need to track information for an unbounded history  and
thus cannot be captured by any n-gram model. We call these UHLs (unbounded history languages).
UHLs 1 and 2 are PDFAs that cycle through 9 and 5 states with different next token probabilities.
UHL 3 is a weighted adaptation of the 5th Tomita grammar  changing its next-token distribution
according to the parity of the seen 0s and 1s. The UHLs are drawn in appendix D.
Extraction Parameters Most of the extraction parameters differ between the RNNs  and are described
in the results tables (1  2). For our algorithm  we always limited the equivalence query to 500
samples. For the spectral algorithm  we made WFAs for all ranks k 2 [50]  k = 50m  m 2 [10] 
k = 100m  m 2 [10]  and k = rank(H). For the n-grams we used all n 2 [6]. For these two  we
always show the best results for NDCG and WER. For ALERGIA in the FLEXFRINGE toolkit  we
use the parameters symbol_count=50 and state_count=N  with N given in the tables.
Evaluation Measures We evaluate the extracted models against their target RNNs on word error rate
(WER) and on normalised discounted cumulative gain (NDCG)  which was the scoring function
for the SPiCe challenge. In particular the SPiCe challenge evaluated models on N DCG5  and we
evaluate the models extracted from the SPiCe RNNs on this as well. For the UHLs  we use N DCG2
as they have smaller alphabets. We do not use probabilistic measures such as perplexity  as the
spectral algorithm is not guaranteed to return probabilistic automata.

1. Word error rate (WER): The WER of model A against B on a set of predictions is the fraction

of next-token predictions (most likely next token) that are different in A and B.
2. Normalised discounted cumulative gain (NDCG): The NDCG of A against B on a set of
sequences {w} scores A’s ranking of the top k most likely tokens after each sequence w 
a1  ...  ak  in comparison to the actual most likely tokens given by B  b1  ...  bk. Formally:

N DCGk(a1  ...  ak) = Pn2[k]

P l
B (w·bn)
log2(n+1)

P l
B (w·an)

log2(n+1)/Pn2[k]

For NDCG we sample the RNN repeatedly  taking all the preﬁxes of each sample until we have 2000
preﬁxes. We then compute the NDCG for each preﬁx and take the average. For WER  we take 2000
full samples from the RNN  and return the fraction of errors over all of the next-token predictions in
those samples. An ideal WER and NDCG is 0 and 1  we note this with # " in the tables.
6.1 Results and Discussion
Tables 1 and 2 show the results of extraction from the SPiCe and UHL RNNs  respectively. In them 
we list our algorithm as WL⇤(Weighted L⇤). For the WFAs and n-grams  which are generated with
several values of k (rank) and n  we show the best scores for each metric. We list the size of the best
model for each metric. We do not report the extraction times separately  as they are very similar: the
majority of time in these algorithms is spent generating the samples or Hankel matrices.
For PDFAs and WFAs the size columns present the number of states  for the WFAs this is equal to
the rank k with which they were reconstructed. For n-grams the size is the number of table entries in
the model  and the chosen value of n is listed in brackets. In the SPiCe languages  our algorithm did
not reach equivalence  and used between 1 and 6 counterexamples for every language before being

7

WER Size

NDCG Size

1118 (n=6)

1118 (n=6)

4988
k=200

66
152
1

k=11

7
962
k=5

11
675
k=8

8
4999
k=250

42
5000
k=32

26
4996
k=27

8
4992
k=44

44
4987
k=41

13
4999
k=100

8421 (n=4)

421 (n=3)

1111 (n=4)

1111 (n=4)

1111 (n=4)

11110 (n=5)

186601 (n=6)

61851 (n=5)

127817 (n=5)

127817 (n=5)

133026 (n=5)

133026 (n=5)

44533 (n=6)

44533 (n=6)

153688 (n=5)

153688 (n=5)

4988
k=150

66
152
1

k=12

7
962
k=7

11
675
k=6

8
4999
k=450

42
5000
k=17

26
4996
k=50

8
4992
k=44

44
4987
k=42

13
4999
k=100

Model
Language (|⌃| ` )
SPiCe 0 (4  1.15) WL⇤

Spectral
N-Gram
ALERGIA

SPiCe 1 (20  2.77) WL⇤†
WL⇤
Spectral
N-Gram
ALERGIA

SPiCe 2 (10  2.13) WL⇤‡
Spectral
N-Gram
ALERGIA

SPiCe 4 (33  1.73) WL⇤

SPiCe 3 (10  2.15) WL⇤‡
Spectral
N-Gram
ALERGIA ‡‡
Spectral
N-Gram
ALERGIA ‡‡
Spectral
N-Gram
ALERGIA

SPiCe 6 (60  1.66) WL⇤

SPiCe 7 (20  1.8) WL⇤

SPiCe 9 (11  1.15) WL⇤

SPiCe 10 (20  2.1) WL⇤

SPiCe 14 (27  0.89) WL⇤

Spectral
N-Gram
ALERGIA

Spectral
N-Gram
ALERGIA

Spectral
N-Gram
ALERGIA

Spectral††
N-Gram
ALERGIA ‡‡

WER# NDCG"
0.987
0.084
0.996
0.053
0.991
0.096
0.961
0.353
0.093
0.971
0.891
0.376
0.909
0.319
0.897
0.337
0.892
0.376
0.08
0.972
0.893
0.263
0.894
0.278
0.844
0.419
0.928
0.327
0.466
0.843
0.847
0.46
0.79
0.679
0.829
0.301
0.727
0.453
0.099
0.968
0.646
0.639
0.644
0.593
0.535
0.705
0.888
0.285
0.687
0.538
0.642
0.626
0.472
0.801
0.812
0.441
0.569
0.735
0.503
0.721
0.877
0.303
0.961
0.123
0.739
0.501
0.593
0.651
0.845
0.4
0.845
0.348
0.51
0.81
0.716
0.442
0.653
0.531
0.079
0.977
0.611
0.641

Time (h)
0.3
0.3
0.8
2.9
0.4
0.1
2.9
0.8
1.2
0.8
1.6
0.8
1.2
1.0
1.2
0.8
1.2
0.7
1.2
0.8
4.4
2.5
6.1
0.8
1.9
0.5
2.4
0.7
1.4
0.5
1.9
1.0
1.1
0.9
1.7
0.8
2.0
0.8
2.4
0.7
1.2

46158 (n=5)
Table 1: SPiCe results. Each language is listed with its alphabet size |⌃| and RNN test
loss `. The n-grams and sample-based PDFAs were created from 5 000 000 samples  and
shared samples. FLEXFRINGE was run with state_count=5000. Our algorithm was run with
t=0.1 " P  " S=0.01 |P|5000 and |S|100  and spectral with |P| |S|=1000  with some excep-
tions: †:t=0.05 " S " P =0.0  ‡:"S=0  ††:|P| |S|=750  ‡‡:state_count=10  000.

125572 (n=6)

19

19

stopped – with the exception of SPiCe1 with t = 0.1  which reached equivalence on a single state.
The UHLs and Tomitas used 0-2 counterexamples each before reaching equivalence.
The SPiCe results show a strong advantage to our algorithm in most of the small synthetic languages
(1-3)  with the spectral extraction taking a slight lead on SPiCe 0. However  in the remaining
SPiCe languages  the n-gram strongly outperforms all other methods. Nevertheless  n-gram models
are inherently restricted to languages that can be captured with bounded histories  and the UHLs
demonstrate cases where this property does not hold. Indeed  all the algorithms outperform the
n-grams on these languages (Table 2).
Our algorithm succeeds in perfectly reconstructing the target PDFA structure for each of the UHL
languages  and giving it transition weights within the given variation tolerance (when extracting from
the RNN and not directly from the original target  the weights can only be as good as the RNN has
learned). The sample-based PDFA learning method  ALERGIA  achieved good WER and NDCG

8

Language (|⌃| ` ) Model
UHL 1 (2  0.72) WL⇤
Spectral
N-Gram
ALERGIA

UHL 2 (5  1.32) WL⇤

UHL 3 (2  0.86) WL⇤

Spectral
N-Gram
ALERGIA

Spectral
N-Gram
ALERGIA

WER# NDCG"
1.0
1.0
0.966
0.999
1.0
1.0
0.94
0.979
1.0
1.0
0.991
0.999

0.0
0.0
0.129
0.004
0.0
0.002
0.12
0.023
0.0
0.0
0.189
0.02

Time (s) WER Size

NDCG Size

15
56
259
278
73
126
269
329
55
71
268
319

9

56
5

25
4

47

k=80

63 (n=6)

k=150
63 (n=6)

k=49

k=47

3859 (n=6)

3859 (n=6)

k=44

63 (n=6)

k=17

63 (n=6)

9

56
5

25
4

47

Table 2: UHL results. Each language is listed with its alphabet size |⌃| and RNN test
loss `. The n-grams and sample-based PDFAs were created from 500 000 samples  and
shared samples. FLEXFRINGE was run with state_count = 50 . Our algorithm was run with
t=0.1 " P  " S=0.01 |P|5000 and |S|100  and spectral with |P| |S|=250.

scores but did not manage to reconstruct the original PDFA structure. This may be improved by
taking a larger sample size  though it comes at the cost of efﬁciency.
Tomita Grammars The full results for the Tomita extractions are given in Appendix D. All of the
methods reconstruct them with perfect or near-perfect WER and NDCG  except for n-gram which
sometimes fails. For each of the Tomita RNNs  our algorithm extracted and accepted a PDFA with
identical structure to the original target in approximately 1 minute (the majority of this time was
spent on sampling the RNN and hypothesis before accepting the equivalence query). These PDFAs
had transition weights within the variation tolerance of the corresponding target transition weights.

On the effectiveness of n-grams The n-gram models prove to be a very strong competitors for
many of the languages. Indeed  n-gram models are very effective for learning in cases where the
underlying languages have strong local properties  or can be well approximated using local properties 
which is rather common (see e.g.  Sharan et al. [32]). However  there are many languages  including
ones that can be modeled with PDFAs  for which the locality property does not hold  as demonstrated
by the UHL experiments.
As n-grams are merely tables of observed samples  they are very quick to create. However  their
simplicity also works against them: the table grows exponentially in n and polynomially in |⌃|. In
the future  we hope that our algorithm can serve as a base for creating reasonably sized ﬁnite state
machines that will be competitive on real world tasks.

7 Conclusions

We present a novel technique for learning a distribution over sequences from a trained LM-RNN. The
technique allows for some variation between the predictions of the RNN’s internal states while still
merging them  enabling extraction of a PDFA with fewer states than in the target RNN. It can also be
terminated before completing  while still maintaining guarantees of local similarity to the target. The
technique does not make assumptions about the target model’s representation  and can be applied to
any language model – including LM-RNNs and transformers. It also does not require a probabilistic
target  and can be directly applied to recreate any WDFA.
When applied to stochastic models such as LM-RNNs  the algorithm returns PDFAs  which are a
desirable model for LM-RNN extraction because they are deterministic and therefore faster and more
interpretable than WFAs. We apply it to RNNs trained on data taken from small PDFAs and HMMs 
evaluating the extracted PDFAs against their target LM-RNNs and comparing to extracted WFAs and
n-grams. When the LM-RNN has been trained on a small target PDFA  the algorithm successfully
reconstructs a PDFA that has identical structure to the target  and local probabilities within tolerance
of the target. For simple languages  our method is generally the strongest of all those considered.
However for natural languages n-grams maintain a strong advantage. Improving our method to be
competitive on naturally occuring languages as well is an interesting direction for future work.

9

Acknowledgments

The authors wish to thank Rémi Eyraud for his helpful discussions and comments  and Chris Ham-
merschmidt for his assistance in obtaining the results with FLEXFRINGE . The research leading to the
results presented in this paper is supported by the Israeli Science Foundation (grant No.1319/16)  and
the European Research Council (ERC) under the European Union’s Seventh Framework Programme
(FP7-2007-2013)  under grant agreement no. 802774 (iEXTRACT).

References
[1] Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput.  75(2):87–

106  1987.

[2] S. Ayache  R. Eyraud  and N. Goudian. Explaining Black Boxes on Sequential Data using

Weighted Automata. ArXiv e-prints  October 2018.

[3] Raphael Bailly. Quadratic weighted automata:spectral algorithm and likelihood maximization.
In Proceedings of the Asian Conference on Machine Learning  volume 20 of Proceedings of
Machine Learning Research  pages 147–163. PMLR  2011.

[4] Raphael Bailly  François Denis  and Liva Ralaivola. Grammatical inference as a principal
component analysis problem. In Proceedings of the 26th Annual International Conference on
Machine Learning  pages 33–40. ACM  2009.

[5] Borja Balle  Xavier Carreras  Franco M. Luque  and Ariadna Quattoni. Spectral learning of
weighted automata - A forward-backward perspective. Machine Learning  96(1-2):33–63  2014.

[6] Borja Balle  Jorge Castro  and Ricard Gavaldà. Learning probabilistic automata: A study in

state distinguishability. Theor. Comput. Sci.  473:46–60  2013.

[7] Borja Balle  Rémi Eyraud  Franco M. Luque  Ariadna Quattoni  and Sicco Verwer. Results
of the sequence prediction challenge (spice): a competition on learning the next symbol in a
sequence. In Proceedings of the 13th International Conference on Grammatical Inference ICGI 
2016.

[8] Borja Balle and Mehryar Mohri. Learning weighted automata. In Algebraic Informatics - 6th
International Conference  CAI 2015  Stuttgart  Germany  September 1-4  2015. Proceedings 
pages 1–21  2015.

[9] Rafael C. Carrasco and José Oncina. Learning stochastic regular grammars by means of a state
merging method. In Rafael C. Carrasco and José Oncina  editors  Grammatical Inference and
Applications  pages 139–152  Berlin  Heidelberg  1994. Springer Berlin Heidelberg.

[10] Rafael C. Carrasco and José Oncina. Learning deterministic regular grammars from stochastic

samples in polynomial time. ITA  33(1):1–20  1999.

[11] Jorge Castro and Ricard Gavaldà. Towards feasible pac-learning of probabilistic deterministic
ﬁnite automata. In Grammatical Inference: Algorithms and Applications  9th International
Colloquium  ICGI 2008  Saint-Malo  France  September 22-24  2008  Proceedings  pages
163–174  2008.

[12] Adelmo Luis Cechin  Denise Regina Pechmann Simon  and Klaus Stertz. State automata
extraction from recurrent neural nets using k-means and fuzzy clustering. In Proceedings of
the XXIII International Conference of the Chilean Computer Science Society  SCCC ’03  pages
73–78  Washington  DC  USA  2003. IEEE Computer Society.

[13] KyungHyun Cho  Bart van Merrienboer  Dzmitry Bahdanau  and Yoshua Bengio. On the
properties of neural machine translation: Encoder-decoder approaches. CoRR  abs/1409.1259 
2014.

[14] Junyoung Chung  Çaglar Gülçehre  KyungHyun Cho  and Yoshua Bengio. Empirical evaluation

of gated recurrent neural networks on sequence modeling. CoRR  abs/1412.3555  2014.

10

[15] Alexander Clark and Franck Thollard. Pac-learnability of probabilistic deterministic ﬁnite state

automata. Journal of Machine Learning Research  5:473–497  2004.

[16] Jacob Devlin  Ming-Wei Chang  Kenton Lee  and Kristina Toutanova. BERT: pre-training of

deep bidirectional transformers for language understanding. CoRR  abs/1810.04805  2018.

[17] Jeffrey L. Elman. Finding structure in time. Cognitive Science  14(2):179–211  1990.

[18] Martin Ester  Hans-Peter Kriegel  Jörg Sander  and Xiaowei Xu. A density-based algorithm for
discovering clusters a density-based algorithm for discovering clusters in large spatial databases
with noise. In Proceedings of the Second International Conference on Knowledge Discovery
and Data Mining  KDD’96  pages 226–231. AAAI Press  1996.

[19] Joshua T Goodman. A bit of progress in language modeling. Computer Speech & Language 

15(4):403–434  2001.

[20] Christian Albert Hammerschmidt  Sicco Verwer  Qin Lin  and Radu State. Interpreting Finite

Automata for Sequential Data. arXiv e-prints  page arXiv:1611.07100  Nov 2016.

[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation 

9(8):1735–1780  1997.

[22] Daniel J. Hsu  Sham M. Kakade  and Tong Zhang. A spectral algorithm for learning hidden

markov models. CoRR  abs/0811.4413  2008.

[23] Franz Mayr and Sergio Yovine. Regular inference on artiﬁcial neural networks. In Machine
Learning and Knowledge Extraction - Second IFIP TC 5  TC 8/WG 8.4  8.9  TC 12/WG 12.9
International Cross-Domain Conference  CD-MAKE 2018  Hamburg  Germany  August 27-30 
2018  Proceedings  pages 350–369  2018.

[24] Takamasa Okudono  Masaki Waga  Taro Sekiyama  and Ichiro Hasuo. Weighted automata

extraction from recurrent neural networks via regression on state spaces  2019.

[25] Christian W. Omlin and C. Lee Giles. Extraction of rules from discrete-time recurrent neural

networks. Neural Networks  9(1):41–52  1996.

[26] Nick Palmer and Paul W. Goldberg. Pac-learnability of probabilistic deterministic ﬁnite state

automata in terms of variation distance. Theor. Comput. Sci.  387(1):18–31  2007.

[27] Ariadna Quattoni  Xavier Carreras  and Matthias Gallé. A maximum matching algorithm for

basis selection in spectral learning. CoRR  abs/1706.02857  2017.

[28] Guillaume Rabusseau  Tianyu Li  and Doina Precup. Connecting weighted automata and
recurrent neural networks through spectral learning. In Proceedings of Machine Learning
Research  pages 1630–1639  2019.

[29] Dana Ron  Yoram Singer  and Naftali Tishby. On the learnability and usage of acyclic proba-

bilistic ﬁnite automata. J. Comput. Syst. Sci.  56(2):133–152  1998.

[30] Ronald Rosenfeld. Two decades of statistical language modeling: Where do we go from here?

Proceedings of the IEEE  88(8):1270–1278  2000.

[31] H. Rulot and E. Vidal. An efﬁcient algorithm for the inference of circuit-free automata. In
Gabriel Ferraté  Theo Pavlidis  Alberto Sanfeliu  and Horst Bunke  editors  Syntactic and
Structural Pattern Recognition  pages 173–184. Springer-Verlag New York  Inc.  New York 
NY  USA  1988.

[32] Vatsal Sharan  Sham M. Kakade  Percy Liang  and Gregory Valiant. Prediction with a short

memory. CoRR  abs/1612.02526  2016.

[33] Franck Thollard  Pierre Dupont  and Colin de la Higuera. Probabilistic DFA inference using
kullback-leibler divergence and minimality. In Proceedings of the Seventeenth International
Conference on Machine Learning (ICML 2000)  Stanford University  Stanford  CA  USA  June
29 - July 2  2000  pages 975–982  2000.

11

[34] M. Tomita. Dynamic construction of ﬁnite automata from examples using hill-climbing. In
Proceedings of the Fourth Annual Conference of the Cognitive Science Society  pages 105–108 
Ann Arbor  Michigan  1982.

[35] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N. Gomez 
Lukasz Kaiser  and Illia Polosukhin. Attention is all you need. CoRR  abs/1706.03762  2017.
[36] Sicco Verwer  Rémi Eyraud  and Colin de la Higuera. Pautomac: a probabilistic automata and

hidden markov models learning competition. Machine Learning  96(1):129–154  Jul 2014.

[37] Sicco Verwer and Christian Hammerschmidt. ﬂexfringe: A passive automaton learning package.

pages 638–642  09 2017.

[38] Qinglong Wang  Kaixuan Zhang  Alexander G. Ororbia II  Xinyu Xing  Xue Liu  and
C. Lee Giles. An empirical evaluation of recurrent neural network rule extraction. CoRR 
abs/1709.10380  2017.

[39] Gail Weiss  Yoav Goldberg  and Eran Yahav. Extracting automata from recurrent neural networks
using queries and counterexamples. In Proceedings of the 35th International Conference on
Machine Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018 
pages 5244–5253  2018.

12

,Gail Weiss
Yoav Goldberg
Eran Yahav