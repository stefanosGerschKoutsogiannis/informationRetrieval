2018,Improving Explorability in Variational Inference with Annealed Variational Objectives,Despite the advances in the representational capacity of approximate distributions for variational inference  the optimization process can still limit the density that is ultimately learned.
We demonstrate the drawbacks of biasing the true posterior to be unimodal  and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods.
Inspired by Annealed Importance Sampling  the proposed method facilitates learning by incorporating energy tempering into the optimization objective.
In our experiments  we demonstrate our method's robustness to deterministic warm up  and the benefits of encouraging exploration in the latent space.,Improving Explorability in Variational Inference with

Annealed Variational Objectives

Chin-Wei Huang† ? 1 Shawn Tan† 2 Alexandre Lacoste? 3 Aaron Courville†k 4

1chin-wei.huang@umontreal.ca  2jing.shan.shawn.tan@umontreal.ca

3allac@elementai.com  4aaron.courville@umontreal.ca

†MILA  University of Montreal

?Element AI

kCIFAR Fellow

Abstract

Despite the advances in the representational capacity of approximate distributions
for variational inference  the optimization process can still limit the density that is
ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be
unimodal  and introduce Annealed Variational Objectives (AVO) into the training
of hierarchical variational methods. Inspired by Annealed Importance Sampling 
the proposed method facilitates learning by incorporating energy tempering into
the optimization objective. In our experiments  we demonstrate our method’s
robustness to deterministic warm up  and the beneﬁts of encouraging exploration
in the latent space.

1

Introduction

Variational Inference (VI) has played an important role in Bayesian model uncertainty calibration and
in unsupervised representation learning. It is different from Markov Chain Monte Carlo (MCMC)
methods  which rely on the Markov chain to reach an equilibrium; in VI one can easily draw i.i.d.
samples from the variational distribution  and enjoy lower variance in inference. On the other hand
VI is subject to bias on account of the introduction of the approximating variational distribution.
As pointed out by Turner and Sahani (2011)  variational approximations tend not to propagate
uncertainty well. This inaccuracy and overconﬁdence in inference can result in bias in statistics of
certain features of the unobserved variable  such as marginal likelihood of data or the predictive
posterior in the context of a Bayesian model. We argue that this is especially true in the amortized VI
setups (Kingma and Welling  2014; Rezende et al.  2014)  where one seeks to learn the representations
of the data in an efﬁcient manner. We note that this sacriﬁces the chance of exploring different
conﬁgurations of representation in inference  and can bias and hurt the training of the model.
This bias is believed to be caused by the variational family that is used  such as factorial Gaussian for
computational tractability  which limits the expressiveness of the approximate posterior. In principle 
this can be alleviated by having a more rich family of distributions  but in practice  it remains a
challenging issue for optimization. To see this  we write the training objective of VI as the KL
divergence  also known as the variational free energy F  between the proposal q and the target
distribution f:

F(q) = Eq [log q(z)  log f (z)] = DKL(q||f ).

Due to the KL divergence  q gets penalized for allocating probability mass in regions where f
has low density. This behaviour of the objective can result in a cascade of consequences. The
variational distribution becomes biased towards being excessively conﬁdent  which  in turn  can
inhibit the variational approximation from escaping poor local minima  even when it has sufﬁcient
representational capacity to accurately represent the posterior. This usually happens when the target

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(posterior) distribution has a complicated structure  such as multimodality  where the variational
distribution might get stuck ﬁtting only a subset of modes.
In what follows  we discuss two annealing techniques that are designed with two diverging goals in
mind. On the one hand  alpha-annealing is used to encourage exploration of signiﬁcant modes and is
designed for learning a good inference model. On the other  beta-annealing facilitates the learning of
a good generative model by reducing noise and regularization during training.

Alpha-annealing: The optimization problem of inference is due to the non-convex nature of the
objective  and can be mitigated via energy tempering (Katahira et al.  2008; Abrol et al.  2014; Mandt
et al.  2016):

Eq [log q(z)  ↵ log f (z)]

(1)
where ↵ = 1
T and T is analogous to the temperature in statistical mechanics. The temperature
is usually initially set to be high  and gradually annealed to 1  i.e. ↵ goes from a small value
to 1. The intuition is that when ↵ is small the energy landscape is smoothed out  as rzf (z)↵ =
↵f (z)↵1rzf (z) = ↵f (z)↵rz log f (z) goes to zero everywhere when ↵ ! 0 if log f is continuous
and has bounded gradient.
However  alpha-annealing might not be ideal in practice. Tempering schemes are typically suitable
for one time inference  e.g. in the case of inferring the posterior distribution of a Bayesian model 
but it can be time consuming for latent variable models or hierarchical Bayesian models where
multiple inferences are required for maximizing the marginal likelihood. Examples include deep
latent variable models (Kingma and Welling  2014; Rezende et al.  2014)  ﬁltering and smoothing
in deep state space models (Chung et al.  2015; Fraccaro et al.  2016; Karl et al.  2016; Shabanian
et al.  2017)  and hierarchical Bayes with inference network (Edwards and Storkey  2017). Doing
energy tempering in these setups would do harm to the training due to the excess noise injected to the
gradient estimate of the model.

Beta-annealing: Deterministic warm-up (Raiko et al.  2007) is applied to improve training of
a generative model  which is of even greater importance especially in the case of hierarchical
models (Sønderby et al.  2016) and latent variable models with ﬂexible likelihood (Gulrajani et al. 
2017; Bowman et al.  2016). Let the joint likelihood of observed data x and latent variable z be
p(x  z) = p(x|z)p(z)  which is equal to the true posterior f (z) = p(z|x) up to a normalizing constant
(marginal p(x)). The annealed variational objective (negative Evidence Lower Bound  or ELBO) is
(2)
where  is annealed from 0 to 1. The rationale behind this annealing is that the ﬁrst term in the
parenthesis over-regularizes the model by forcing the approximate posterior q(z) to be like the prior
p(z)  and thus by reducing this prior contrastive coefﬁcient early on during the training we allow
the decoder to make better use of the latent code to represent the underlying structure of the data.
However  the entropy term has less importance when the coefﬁcient is smaller  leading it to be more
a deterministic autoencoder and biasing the encoding distribution to be more sharp and unimodal.
This approach is clearly in conﬂict with the principle of energy tempering  where one allows the
approximate posterior to “explore” in the energy landscape for signiﬁcant modes.

Eq[(log q(z)  log p(z))  log p(x|z)]

This work aims to clarify the implications of doing these two kinds of annealing  in terms of the
inductive bias the learning objective and algorithm has. We ﬁrst review a few techniques in the
VI and MCMC literature to tackle the expressiveness problem (i.e. limited expressiveness of the
approximate posterior) and the optimization problem (i.e. the mode seeking problem) in inference.
We focus on the latter. These are summarized in Section 3. We introduce Annealed Variational
Objectives (AVO) in Section 4  which aims to satisfy the criteria the alpha and beta annealing schemes
seek to achieve separately. Finally  in Section 5 we demonstrate the biasing effect of VI with beta
annealing  and demonstrate the relative robustness and effectiveness of the proposed method.

2 Related work

Naturally  recent works in VI have been focused on reducing representational bias  especially in the
setting of amortized VI  known as the Variational Auto-Encoders (VAE) (Kingma and Welling  2014;

2

Rezende et al.  2014)  by (1) ﬁnding more expressive families of variational distributions without
losing the computational tractability. Explicit density methods include Rezende and Mohamed (2015);
Ranganath et al. (2016); Kingma et al. (2016); Tomczak and Welling (2016  2017); Berg et al. (2018);
Huang et al. (2018). Other methods include implicit density methods (Huszár  2017; Mescheder
et al.  2017; Shi et al.  2018). A second line of research focuses on (2) reducing the amortization error
introduced by the use of a conditional network (Cremer et al.  2018; Krishnan et al.  2017; Marino
et al.  2018; Kim et al.  2018).
In terms of non-parametric methods  the Importance Weighted Auto-Encoder (IWAE) developped by
Burda et al. (2016) uses several samples for evaluating the loss to reduce the variational gap  which
can be expensive in scenarios where the decoder is much more complex. Nowozin (2018) further
reduces the bias in inference via Jackknife Variational Inference. However  Rainforth et al. (2018)
notices that the signal-to-noise ratio of the encoder’s gradient vanishes with increasing number of
importance samples (reduced bias)  rendering the encoder an ineffective representation model in
the limit case. Salimans et al. (2015) combines MCMC with VI but the inference process requires
multiple passes through the decoder as well.
Our method is orthogonal to all these  assuming we have a rich family of approximate posterior at
hand  and smooths out the objective landscape using annealed objectives speciﬁcally for hierarichical
VI methods. We also note that it is possible to consider alternative losses to induce different
optimization behavior  such as in Li and Turner (2016); Dieng et al. (2017).

3 Background
In what follows  we consider a latent variable model with a joint probability p✓(x  z) = p(x|z)p(z) 
where x and z are observed and real-valued unobserved variables  and ✓ denotes the set of parameters
to be learned. Due to the non-conjugate prior-likelihood pair or the non-linearity of the conditioning 
exact inference is often intractable. Direct maximization of the marginal likelihood is impossible
because the marginalization involves integration: log p(x) = logR p(x  z) d z. Thus  training is

usually conducted by maximizing the expected complete data log likelihood (ECLL) over an auxiliary
distribution q:

max

✓

Eq(z)[log p✓(x  z)].

When using the conditional q(z|x) we emphasize the use of a recognition network in amortized VI
(Kingma and Welling  2014; Rezende et al.  2014). When the exact posterior p(z|x) is tractable 
one could choose to use q(z) = p(z|x)  which leads to the well known Expectation-Maximization
(EM) algorithm. When exact inference is not possible  we need to approximate the true posterior 
usually by sampling from a Markov chain in MCMC or from a variational distribution in VI. The
approximation induces bias in learning the model ✓  as

Eq[log p✓(x  z)] = log p✓(x) + Eq[log p✓(z|x)].

That is to say  maximizing ECLL increases the marginal likelihood of the data while biasing the
true posterior to be more like the auxiliary distribution. The second effect vanishes when q(z)
approximates p(z|x) better.
In this paper we focus on the case of VI  where learning of the auxiliary distribution  i.e. variational
distribution  is through maximizing the ELBO  which is equivalent to minimizing DKL(q(z)||p(z|x)).
Due to the zero-forcing property of the KL  q tends to be unimodal and more concentrated. We
emphasize that a highly ﬂexible parametric form of q can potentially alleviate this problem but does
not address the issue of ﬁnding the optimal parameters. Before going into technical details of choice
of q and loss function  we now discuss a few implications and effects doing approximate inference
has in practice:

1. At initialization  the true posterior is likely to be multi-modal. Having a unimodal q helps
to regularize the latent space  biasing the true posterior towards being unimodal as well.
Depending on the type of data being modeled  this may be a good property to have. A
unimodal approximate posterior also means less noise when sampling from q. This facilitates
learning by having lower variance gradient estimates.

2. In cases where the true posterior has to be multi-modal  biasing the posterior to be unimodal
inhibits the model from learning the true generative process of the data. Allowing sufﬁcient

3

uncertainty in the approximate posterior encourages the learning process to explore the
spurious modes in the true posterior.

3. Beta-annealing facilitates point 1 by lowering the penalty of the prior contrastive term 
allowing the q(z) estimate to be sharp (per point 1). Alpha-annealing encourages exploration
(point 2) by lowering the penalty of the cross-entropy term  increasing the importance of the
entropy term  and therefore the tendency to explore the latent space (per point 2).

3.1 Assumption of the variational family
Recent works have shown promising results in having more expressive parametric form of the
variational distribution. This allows the variational family to cover a wider range of distributions 
ideally including the true posterior that we seek to approximate. For instance  the Hierarchical
Variational Inference (HVI) methods (Ranganath et al.  2016) are a generic family of methods that
subsume discrete mixture proposals (e.g. mixture of Gaussian)  auxiliary variable methods (Agakov
and Barber  2004; Maaløe et al.  2016)  and normalizing ﬂows (Rezende and Mohamed  2015) as
subfamilies.

In HVI  we use a latent variable model q(zT ) = R q(zT   zt<T ) d zt<T as approximate posterior 
with the index t < T indicating the latent variable 1. Thus the entropy term  Eq(zT )[log q(zT )] is
intractable  and needs to be approximated. One can lower bound this term by introducing a reverse
network r(zt<T|zT ):

 Eq(zT )[log q(zT )]   Eq(zT ) [log q(zT ) + DKL(q(zt<T|zT )||r(zt<T|zT ))]

The variational lower bound then becomes:

=  Eq(zT  zt<T ) [log q(zT|zt<T )q(zt<T )  log r(zt<T|zT )] .
= Eq(zT  zt<T )log

q(zT|zt<T )q(zt<T ) .

p(x  zT )r(zt<T|zT )

.

L(x)

(3)

Since q(zT ) can be seen as an inﬁnite mixture model  in principle it has the capability to represent
any posterior distribution given enough capacity in the conditional.
In the special case where
q(zT|zt<T ) is deterministic and invertible  we can choose r to be its inverse function. The KL term
would vanish  and the entropy can be computed recursively via the change-of-variable formula:
@zt1|1. Universal approximation results of certain parameterizations of invertible
q(zt) = q(zt1)| @zt
functions have also been recently shown in Huang et al. (2018).
Expressive as these methods can be  they may be susceptible to bad local minima when ﬁtting a target
distribution with well separated modes. Furthermore  we demonstrate in Section 5.2 and 5.3 (and
also Appendix D) that they are not robust to beta-annealing.

3.2 Loss function tempering: annealed importance sampling
As explained in the introduction  the purpose of doing alpha-annealing is to let the variational
distribution be more exploratory early on during training. Annealed importance sampling (AIS) is
an MCMC method that encapsulates this concept (Neal  2001). In AIS  we consider an extended
state space with z0  z1  ...  zT   where z0 is sampled from a simple distribution (such as the prior
distribution p(z))  and the subsequent particles are sampled from the transition operators qt(zt|zt1).
We design a set of intermediate target densities as ˜ft = ˜f ↵t
  for t 2 [0  T ]  where ˜ft / ft  fT
T
is the true posterior we want to approximate  and f0 is the initial distribution z0 is sampled from.
These (Markov chain) transitions are constructed to leave the intermediate targets ft(z) invariant 
and the importance weight is calculated as

˜f 1↵t
0

wj =

˜f1(z1)
˜f0(z1)

˜f2(z2)
˜f1(z2)

...

˜fT (zT )
˜fT1(zT )

.

A downside of AIS is that it requires constructing a long sequence of transitions in order for the
intermediate targets to be close enough and thus for the estimate to be accurate.

1 Here we consider sequence of latent variables zt of the same dimension as zT   but in principle the

dimensionality can vary.

4

4 Annealed variational objectives

Inspired by AIS and alpha-annealing  we propose to integrate energy tempering into the optimization
objective of the variational distribution. As in AIS  we consider an extended state space with random
variables z0  ...  zT   and we take the marginal qT (zT ) as approximate posterior. We construct T
forward transition operators and T backward operators  and assign an intermediate target ˜ft to each
transition pair  which is deﬁned as an interpolation between the true (unnormalized) posterior and the
initial distribution: ˜ft = ˜f ↵t
  where 0 = ↵0 <↵ 1 < ... <↵ T = 1 2. Different from AIS  we
T
propose to learn the parametric transition operators. More formally  we deﬁne:

˜f 1↵t
0

qt(zt  zt1) = qt1(zt1)qt(zt|zt1)  and qt(zt) =Zz0 ... zt1
qt0(zt0|zt01) d zt0.
Set q0(z0) = f0(zt) 3  which is easy to sample from. We deﬁne the backward transitions in the
reverse ordering  i.e. rt(zt1  zt) = rt(zt1|zt). We consider maximizing the following objective 
which we refer to as the Annealed Variational Objectives (AVO):

qt(zt|zt1)

t1Yt0=0

(4)

max

qt(zt|zt1) rt(zt1|zt)

Eqt(zt|zt1)qt1(zt1)"log

˜ft(zt)rt(zt1|zt)

qt(zt|zt1)qt1(zt1)#

for t 2 [1  T ]  by taking the partial derivative with respect to the parameters t of the t’th transition
(ﬁxing the intermediate marginal qt1(zt1)) 4.
Intuitively  the goal of each forward transition is to stochastically transform the samples drawn from
the previous step into the intermediate target distribution assigned to it. Since each intermediate
target only differs from the previous one slightly  each transition only needs to correct the marginal
samples from the previous step incrementally. Also  in the case of amortized VI  we set f0 to be
the prior distribution of the VAE  so each intermediate target has a more widely distributed density
function  which facilitates exploration of the transition operators.

4.1 Analysis on the optimality of transition operators
The following proposition shows that if each forward transition and backward transition are locally
optimal  the result is globally consistent  which means the marginals r⇤t (zt) and q⇤t (zt) constructed
by locally optimal transitions match the intermediate target ft(zt). The corollary below shows that
the optimal transitions in Equation 4 bridge the variational gap.
Proposition 1. Let q⇤t (zt|zt1) and r⇤t (zt1|zt) be the functional solutions to Equation 4. Then the
resulting marginal density functions q⇤t (zt) and r⇤t (zt) are equal to ft(zt).
corollary 1. Take fT (zT ) as the true posterior distribution p(zT|x)  or simply choose ˜fT =
p(x|zT )p(zT ). The variational lower bound in Equation 3 is exact with respect to the marginal
likelihood p(x) if optimal transitions q⇤t and r⇤t are used.
The above results demonstrate that optimizing the transitions according to Equation 4 is consistent
with the variational Bayes objective when all of them are optimal with respect to the objectives they
are assigned to. The proof can be found in Appendix A.

4.2 Speciﬁcation of the transitions
When ↵t1 is close enough to ↵t  assuming the marginal qt1(zt1) approximates ft1 accurately 
the transition qt(zt|zt1) only needs to modestly reﬁne the samples from qt1(zt1). In light of this 
we design the following stochastic reﬁnement operator for both qt(zt|zt1) and rt(zt1|zt):
t (zt))

t (zt)  r

rt(zt1|zt) = N (zt1|µr

where the mean µ and standard deviation  of the conditional normal distribution is deﬁned as follows

qt(zt|zt1) = N (zt|µq

t (zt1)  q

t (zt1));

µ(z) = g(z)  m(z) + (1  g(z))  z;

(z) = act(W · h(z) + b)

2 In the experiments  we use a linear annealing schedule for all models trained with AVO.
3 We learn the initial distribution q0(z0|x) in the case of amortized VI.
4 In practice  this is achieved by disconnecting the gradient.

5

(a) Data

(b) IWAE

(c) VAE

(d) HVI-AVO

Figure 1: Learning the noise process. (a) the true data distribution. (b-d) the distributions learned by the
speciﬁed methods.

(a) p(z|X = [0.1 1]>)

(b) p(z|X = [0 1]>)

Figure 2: Learned posteriors given different locations where inference is ambiguous.

(c) p(z|X = [0.1 1]>)

m(z) = Wm · h(z) + bm;

g(z) = actg(Wg · h(z) + bg);

h(z) = acth(Wh · z + bh).

The activation functions act  actg and acth are chosen to be softplus  sigmoid and ReLU (Nair and
Hinton  2010) (or ELU (Clevert et al.  2016) in the case of amortized VI). In the case of amortized VI 
we replace the dot product with the conditional weight normalization (Salimans and Kingma  2016)
operator proposed in Krueger et al. (2017); Huang et al. (2018).

4.3 Loss calibrated AVO

Since the correctness of the marginal qT (zT ) trained with AVO depends on the optimality of each
transition operator  when used for amortized VI each update will not necessarily improve the marginal
to be a better approximate posterior. Hence  in amortized VI  we consider the following loss calibrated
version of AVO 5:

max

qt(zt|zt1) rt(zt1|zt)

a Eqt(zt|zt1)qt1(zt1)"log

˜ft(zt)rt(zt1|zt)

q(zt|zt1)q(zt1)# + (1  a)L(x)

for all t  where a 2 [0  1] is a hyperparameter that determines the weight of AVO used in training. In
practice naive implementation of this objective can be computationally expensive  so we select the
loss function stochastically (with probability a maximize AVO  otherwise maximize ELBO) in the
amortized VI experiments  which we found helps to make progress in improving the model p✓(x  z).

5 Experiments

Our ﬁrst experiment shows that having a unimodal approximate posterior is not universally benign.
The second and third experiments analyze the effect of the optimization process has on the learned
(marginal) density of qT   qualitatively and quantitatively. We do this in an unamortized setup  in an
attempt to suppress the confounding effect of suboptimal amortization. Finally  we experiment with
real data and demonstrate that HVI can be improved with our proposed method.

5 Note that in amortized VI ˜ft  rt and qt all depend on the input x  but we omit the notation for simplicity.

6

Figure 3: Learned marginals qt(zt) at different layers (T = 10). First row: true targets f1...f10. Second row:
HVI-ELBO. Third row: HVI-AVO.

5.1 Biased noise model

To demonstrate the biasing effect of the approximate posterior in amortized VI  we utilize a toy
example in the form of the following data distribution:

z ⇠N (0  1) ✏
x1 = sin (⇡ tanh (⌘z)) + ✏1 

1 ✏ 2 ⇠N (0 " ) 

x2 = cos (⇡ tanh (⌘z)) + ✏2 

(5)
(6)

where ⌘ = 0.75. The result is the data distribution depicted in Figure 1a. The data distribution is
constructed so that x = [0 1]> has a bimodal true posterior with a mode at the tail ends of the
Gaussian. This is analogous to some real data distribution in which two data points close in the input
space can come from two well separated modes in latent space (e.g. someone wearing glasses vs. not
wearing glasses).
We train three different models which differ only in the way their approximate posterior is parame-
terized: (1) q(z|x) = p(z)  trained using the IWAE loss and 500 samples  (2) A VAE model with a
Gaussian approximate posterior q(z|x) = N (z|x)  and ﬁnally (3) A VAE model with an AVO loss.
We use a decoder exactly as in Equation 6 ﬁxed for the mean of the conditional Gaussian likelihood 
with standard deviation of ✏1 and ✏2 parameterized as an MLP conditioned on z to be learned. The
densities learned by each model is depicted in Figure 1. We estimate the posterior of the learned
generative model in each case  and compare them in Figure 2. At the point x = [0 1]>  the true
posterior is a bimodal distribution as discussed before. Shifting slightly to the left or right in x-space
towards the higher density regions gives a sharper prediction in the latent space at their respective
ends. In the unambiguous regions of the data  the posterior is unimodal.
We observe that the VAE encoder predicts the posterior to be centered around z = 0. From Figure 2
we can clearly see the zero-forcing effect mentioned before  where encoder matches one of the modes
of the true posterior. As a consequence of that behavior  the variance of the decoder’s prediction at
that point is high  resulting in the plot seen in Figure 1c. The IWAE model on the other hand  matches
the distribution better. With 500 samples per data point in this case  we are effectively training the
generative model with a non-parametric posterior that can perfectly ﬁt the true posterior. The price of
doing so  however  is that we require many samples in order to train the model.
For a simple task such as this one  this approach works  but is meant to demonstrate the beneﬁts of
having sufﬁcient uncertainty in the proposal distribution. The proposed AVO method (T = 10) also
approximates the distribution well  but without incurring the same computational cost as IWAE.

5.2 Toy energy ﬁtting

In this experiment  we take a four-mode mixture of Gaussian as a true target energy  and compare HVI
trained with ELBO and HVI trained with AVO (with T = 10). In Figure 3 we see that HVI-ELBO
overﬁts two modes out of four  whereas HVI-AVO captures all four modes. Also  ﬁrst few layers of
HVI-ELBO do not contribute much to the resulting marginal qT (zT )  where each layer of HVI-AVO
has to ﬁt the assigned target.

7

Figure 4: Robustness of HVI to beta-annealing. “n” denotes normal ELBO and “a” denotes AVO. x-axis: the
percentage of total training time it takes to anneal  linearly back to 1; y-axis: estimated Eq [E(x)  log q(x)]
(negative KL) shifted by a log normalizing constant (the higher the better).

Table 1: Amortized VI with VAE. L⇤ is the ELBO  with tr  va and te representing the training  validation
and test set respectively. log p(D⇤) is the estimated log-likelihood on the dataset. L indicates the number of
leap-frog steps used in Salimans et al. (2015).

(a) MNIST

T = 5
a = 0
84.51
92.37
87.63
92.48
87.62
7.97
4.86

T = 5
a = 0.2
86.15
90.00
86.06
90.01
86.06
3.86
3.95

T = 0
a = 0
84.69
92.37
87.49
92.40
87.50
7.71
4.90

 log p(Dva)
 log p(Dte)

L tr
L va
L te
gen gap
var gap

T = 5
a = 0.5 L = 4 L = 8
87.02
90.22
86.12
90.26
86.12
3.24
4.14

89.82
86.40

88.3
85.51

-
-
-

-

-
-
-

-

3.42

2.79

(b) OMNIGLOT

T = 0
a = 0
106.67
114.68
108.41
115.25
108.55
8.58
6.70

T = 5
a = 0
111.32
110.78
106.32
113.19
106.25
1.87
6.94

T = 5
a = 0.5
108.94
109.47
105.18
111.94
105.04
3.00
6.90

5.3 Quantitative analysis on robustness to beta annealing
We perform the experiments in Section 5.2 again  on six different energy functions  E (summarized
in Appendix B) with different linear schedules of beta-annealing. We run 10 trials for each energy
function  and do 2000 stochastic updates with a batch size of 64 and a learning rate of 0.001. We
evaluate the resulting marginal qT (zT ) according to its negative KL divergence from the true energy
function. Since the entropy of qT (zT ) is intractable  we estimate it using importance sampling; we
elaborate this in Appendix C. We see that HVI-AVO constantly outperforms HVI-ELBO. Even when
most of the time performance of HVI-ELBO degrades along with prolonged beta-annealing schedule 
HVI-AVO’s performance is relatively invariant. In Appendix D  we visualize the learned marginals
qt(zt) under beta annealing using both stochastic and deterministic transitions.

5.4 Amortized inference

We train VAEs using a standard VAE (with a Gaussian approximate posterior)  HVI  and HVI with
AVO on the Binarized MNIST dataset from Larochelle and Murray (2011)  and the Omniglot dataset
as used in Burda et al. (2016). In these experiments  we used an MLP for both the encoder and
decoder  with gating activation as in Tomczak and Welling (2016). Both the encoder and decoder
have 2 hidden layers  each with 300 hidden units. For MNIST  we used a dimension of 40 for the
latent space  and 200 dimensions in Omniglot. We used hyperparameter search to determine batch
size  learning rate  and the beta-annealing schedule.
Table 1a lists the results from our experiments on MNIST  alongside results from Salimans et al. (2015)
combining MCMC in HVI for comparison (see Appendix E). Table 1b lists the results for Omniglot.
We ﬁnd that HVI’s approximate posterior trained with AVO tends to have smaller variational gap
(estimated by difference between test likelihood and ELBO)  and also better test likelihood. It is also
worth noting that in the MNIST case  smaller variational gap translates into smaller generalization
gap and also better test likelihood  while the same is not true in the OMNIGLOT example  which
corroborates our ﬁnding in Section 5.1 that true posterior can be biased towards approximate posterior 
resulting in a smaller variatinal gap (8.58) but a worse density model (6.70).

8

6 Conclusion

We ﬁnd that despite the representational capacity of the chosen family of approximate distributions in
VI  the density that can be represented is still limited by the optimization process. We resolve this by
incorporating annealed objectives into the training of hierarchical variational methods. Experimentally 
we demonstrate (1) our method’s robustness to deterministic warm-up  (2) the beneﬁts of encouraging
exploration and (3) the downside of biasing the true posterior to be unimodal. Our method is
orthogonal to ﬁnding a more rich family of variational distributions  and sheds light on an important
optimization issue that has thus far been neglected in the amortized VI literature.

7 Acknowledgements

We would like to thank Massimo Cassia and Aristide Baratin for their useful feedback and discussion.

References
Abrol  F.  Mandt  S.  Ranganath  R.  and Blei  D. (2014). Deterministic annealing for stochastic

variational inference. stat  1050:7.

Agakov  F. V. and Barber  D. (2004). An auxiliary variational method.

Processing.

In Neural Information

Berg  R. v. d.  Hasenclever  L.  Tomczak  J. M.  and Welling  M. (2018). Sylvester normalizing ﬂows

for variational inference. arXiv preprint arXiv:1803.05649.

Bowman  S. R.  Vilnis  L.  Vinyals  O.  Dai  A. M.  Jozefowicz  R.  and Bengio  S. (2016). Generating
sentences from a continuous space. In SIGNLL Conference on Computational Natural Language
Learning (CONLL).

Burda  Y.  Grosse  R.  and Salakhutdinov  R. (2016).

International Conference on Learning Representations.

Importance weighted autoencoders.

In

Chung  J.  Kastner  K.  Dinh  L.  Goel  K.  Courville  A. C.  and Bengio  Y. (2015). A recurrent latent

variable model for sequential data. In Advances in neural information processing systems.

Clevert  D.-A.  Unterthiner  T.  and Hochreiter  S. (2016). Fast and accurate deep network learning by

exponential linear units (elus). International Conference on Learning Representations.

Cremer  C.  Li  X.  and Duvenaud  D. (2018). Inference suboptimality in variational autoencoders. In

International Conference on Machine Learning.

Dieng  A. B.  Tran  D.  Ranganath  R.  Paisley  J.  and Blei  D. (2017). Variational inference via 

upper bound minimization. In Advances in Neural Information Processing Systems.

Edwards  H. and Storkey  A. (2017). Towards a neural statistician. In International Conference on

Learning Representations.

Fraccaro  M.  Sønderby  S. K.  Paquet  U.  and Winther  O. (2016). Sequential neural models with

stochastic layers. In Advances in neural information processing systems  pages 2199–2207.

Gulrajani  I.  Kumar  K.  Ahmed  F.  Taiga  A. A.  Visin  F.  Vazquez  D.  and Courville  A. (2017).
Pixelvae: A latent variable model for natural images. In International Conference on Learning
Representations.

Huang  C.-W.  Krueger  D.  Lacoste  A.  and Courville  A. (2018). Neural autoregressive ﬂows. In

International Conference on Machine Learning.

Huszár  F. (2017). Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235.

Karl  M.  Soelch  M.  Bayer  J.  and van der Smagt  P. (2016). Deep variational bayes ﬁlters:
Unsupervised learning of state space models from raw data. In International Conference on
Learning Representations.

9

Katahira  K.  Watanabe  K.  and Okada  M. (2008). Deterministic annealing variant of variational
bayes method. In Journal of Physics: Conference Series  volume 95  page 012015. IOP Publishing.

Kim  Y.  Wiseman  S.  Miller  A. C.  Sontag  D.  and Rush  A. M. (2018). Semi-amortized variational

autoencoders. In International Conference on Machine Learning.

Kingma  D. P.  Salimans  T.  Jozefowicz  R.  Chen  X.  Sutskever  I.  and Welling  M. (2016).
Improved variational inference with inverse autoregressive ﬂow. In Advances in Neural Information
Processing Systems.

Kingma  D. P. and Welling  M. (2014). Auto-encoding variational bayes. In International Conference

on Learning Representations.

Krishnan  R. G.  Liang  D.  and Hoffman  M. (2017). On the challenges of learning with inference

networks on sparse  high-dimensional data. arXiv preprint arXiv:1710.06085.

Krueger  D.  Huang  C.-W.  Islam  R.  Turner  R.  Lacoste  A.  and Courville  A. (2017). Bayesian

hypernetworks. arXiv preprint arXiv:1710.04759.

Larochelle  H. and Murray  I. (2011). The neural autoregressive distribution estimator. In International

Conference on Artiﬁcial Intelligence and Statistics.

Li  Y. and Turner  R. E. (2016). Rényi divergence variational inference. In Advances in Neural

Information Processing Systems.

Maaløe  L.  Sønderby  C. K.  Sønderby  S. K.  and Winther  O. (2016). Auxiliary deep generative

models. In International Conference on Machine Learning.

Mandt  S.  McInerney  J.  Abrol  F.  Ranganath  R.  and Blei  D. (2016). Variational tempering. In

International Conference on Artiﬁcial Intelligence and Statistics.

Marino  J.  Yue  Y.  and Mandt  S. (2018). Iterative amortized inference. In International Conference

on Machine Learning.

Mescheder  L.  Nowozin  S.  and Geiger  A. (2017). Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In International Conference on
Machine Learning.

Nair  V. and Hinton  G. E. (2010). Rectiﬁed linear units improve restricted boltzmann machines. In

International Conference on Machine Learning.

Neal  R. M. (2001). Annealed importance sampling. Statistics and computing  11(2).

Nowozin  S. (2018). Debiasing evidence approximations: On importance-weighted autoencoders and

jackknife variational inference. In International Conference on Learning Representations.

Raiko  T.  Valpola  H.  Harva  M.  and Karhunen  J. (2007). Building blocks for variational bayesian

learning of latent variable models. Journal of Machine Learning Research.

Rainforth  T.  Kosiorek  A. R.  Le  T. A.  Maddison  C. J.  Igl  M.  Wood  F.  and Teh  Y. W. (2018).
Tighter variational bounds are not necessarily better. In International Conference on Machine
Learning.

Ranganath  R.  Tran  D.  and Blei  D. (2016). Hierarchical variational models. In International

Conference on Machine Learning.

Rezende  D. J. and Mohamed  S. (2015). Variational inference with normalizing ﬂows. In Interna-

tional Conference on Machine Learning.

Rezende  D. J.  Mohamed  S.  and Wierstra  D. (2014). Stochastic backpropagation and approximate

inference in deep generative models. In International Conference on Machine Learning.

Salimans  T.  Kingma  D.  and Welling  M. (2015). Markov chain monte carlo and variational

inference: Bridging the gap. In International Conference on Machine Learning.

10

Salimans  T. and Kingma  D. P. (2016). Weight normalization: A simple reparameterization to
In Advances in Neural Information Processing

accelerate training of deep neural networks.
Systems.

Shabanian  S.  Arpit  D.  Trischler  A.  and Bengio  Y. (2017). Variational bi-lstms. arXiv preprint

arXiv:1711.05717.

Shi  J.  Sun  S.  and Zhu  J. (2018). Kernel implicit variational inference. In International Conference

on Learning Representations.

Sønderby  C. K.  Raiko  T.  Maaløe  L.  Sønderby  S. K.  and Winther  O. (2016). Ladder variational

autoencoders. In Advances in neural information processing systems  pages 3738–3746.

Tomczak  J. M. and Welling  M. (2016). Improving variational auto-encoders using householder ﬂow.

arXiv preprint arXiv:1611.09630.

Tomczak  J. M. and Welling  M. (2017). Improving variational auto-encoders using convex combina-

tion linear inverse autoregressive ﬂow. In Benelearn.

Turner  R. E. and Sahani  M. (2011). Two problems with variational expectation maximisation for

time-series models. Bayesian Time series models  1(3.1):3–1.

11

,Chin-Wei Huang
Shawn Tan
Alexandre Lacoste
Aaron Courville