2019,Singleshot : a scalable Tucker tensor decomposition,This paper introduces a new approach for the scalable Tucker decomposition
problem. Given a tensor X   the method proposed allows to infer the latent factors
by processing one subtensor drawn from X at a time. The key principle of our
approach is based on the recursive computations of gradient and on cyclic update of factors involving only one single step of gradient descent. We further improve the
computational efficiency of this algorithm by proposing an inexact gradient version.
These two algorithms are backed with theoretical guarantees of convergence and
convergence rate under mild conditions. The scalabilty of the proposed approaches
which can be easily extended to handle some common constraints encountered in
tensor decomposition (e.g non-negativity)  is proven via numerical experiments on
both synthetic and real data sets.,Singleshot : a scalable Tucker tensor decomposition

Abraham Traoré
LITIS EA4108

University of Rouen Normandy

abraham.traore@etu.univ-rouen.fr

Maxime Bérar
LITIS EA4108

University of Rouen Normandy
maxime.berar@univ-rouen.fr

Alain Rakotomamonjy

LITIS EA4108

University of Rouen Normandy

Criteo AI Lab  Criteo Paris

alain.rakoto@insa-rouen.fr

Abstract

This paper introduces a new approach for the scalable Tucker decomposition
problem. Given a tensor X   the algorithm proposed  named Singleshot  allows to
perform the inference task by processing one subtensor drawn from X at a time.
The key principle of our approach is based on the recursive computations of the
gradient and on cyclic update of the latent factors involving only one single step of
gradient descent. We further improve the computational efﬁciency of Singleshot by
proposing an inexact gradient version named Singleshotinexact. The two algorithms
are backed with theoretical guarantees of convergence and convergence rates under
mild conditions. The scalabilty of the proposed approaches  which can be easily
extended to handle some common constraints encountered in tensor decomposition
(e.g non-negativity)  is proven via numerical experiments on both synthetic and
real data sets.

1

Introduction

The recovery of information-rich and task-relevant variables hidden behind observation data (com-
monly referred to as latent variables) is a fundamental task that has been extensively studied in
machine learning. In many applications  the dataset we are dealing with naturally presents different
modes (or dimensions) and thus  can be naturally represented by multidimensional arrays (also called
tensors). The recent interest for efﬁcient techniques to deal with such datasets is motivated by the
fact that the methodologies that matricize the data and then apply matrix factorization give a ﬂattened
view of data and often cause a loss of the internal structure information. Hence  to mitigate the
extent of this loss  it is more favorable to process a multimodal data set in its own domain  i.e. tensor
domain  to obtain a multiple perspective view of data rather than a ﬂattened one.
Tensors represent generalization of matrices and the related decomposition techniques are promising
tools for exploratory analysis of multidimensional data in diverse disciplines including signal process-
ing [11]  social networks analysis [28]  etc. The two most common decompositions used for tensor
analysis are the Tucker decomposition [43] and the Canonical Polyadic Decomposition also named
CPD[16  6]. These decompositions are used to infer multilinear relationships from multidimensional
datasets as they allow to extract hidden (latent) components and investigate the relationships among
them.
In this paper  we focus on the Tucker decomposition motivated by the fact that this decomposition
and its variants have been successfully used in many real applications [24  19]. Our technical goal

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

is to develop a scalable Tucker decomposition technique in a static setting (the tensor is ﬁxed).
Such an objective is relevant in a situation where it is not possible to load in memory the tensor of
interest or when the decomposition process may result in memory overﬂow generated by intermediate
computations [20  31].

1.1 Related work and main limitations

Divide-and-conquer type methods (i.e. which divide the data set into sub-parts) have already been
proposed for the scalable Tucker decomposition problem  with the goal of efﬁciently decomposing
a large ﬁxed tensor (static setting). There are mainly three trends for these methods: distributed
methods  sequential processing of small subsets of entries drawn from the tensor or the computation of
the tensor-matrix product in a piecemeal fashion by adaptively selecting the order of the computations.
A variant of the Tucker-ALS has been proposed in [31] and it solves each alternate step of the Tucker
decomposition by processing on-the-ﬂy intermediate data  reducing then the memory footprint of the
algorithm. Several other approaches following the same principles are given in [5  9  4  33] while
others consider some sampling strategies [29  36  14  39  48  18  47  35  27  10  25] or distributed
approaches [49  7  34]. One major limitation related to these algorithms is their lack of genericness
(i.e. they cannot be extended to incorporate some constraints such as non-negativity).
Another set of techniques for large-scale Tucker decomposition in a static setting focuses on designing
both deterministic and randomized algorithms in order to alleviate the computational burden of the
decomposition. An approach proposed by [4] performs an alternate minimization and reduces the
scale of the intermediate problems via the incorporation of sketching operators. In the same ﬂavor 
one can reduce the computational burden of the standard method HOSVD through randomization
and by estimating the orthonormal basis via the so-called range ﬁnder algorithm [51]. This class
of approaches encompasses other methods that can be either random [8  30  13  37  42  46] or
deterministic [40  2  38  3  50  17  26  32]. The main limitation of these methods essentially stems
from the fact that they use the whole data set at once (instead of dividing it)  which makes them
non-applicable when the tensor does not ﬁt the available memory.
From a theoretical point of view  among all these works  some algorithms are backed up with
convergence results [4] or have quality of approximation guarantees materialized by a recovery bound
[1]. However  there is still a lack of convergence rate analysis for the scalable Tucker problem.

1.2 Main contributions

In contrast to the works described above  our contributions are the following ones:
• We propose a new approach for the scalable Tucker decomposition problem  denoted as Singleshot
leveraging on coordinate gradient descent [41] and sequential processing of data chunks amenable
to constrained optimization.
• In order to improve the computational efﬁciency of Singleshot  we introduce an inexact gradient
variant  denoted as Singleshotinexact. This inexact approach can be further extended so as to make
it able to decompose a tensor growing in every mode and in an online fashion.

• From a theoretical standpoint  we establish for Singleshot an ergodic convergence rate of O(cid:16) 1√

(cid:17)

(K: maximum number of iterations) to a stationary point and for Singleshotinexact  we establish a
convergence rate of O( 1
• We provide experimental analyses showing that our approaches are able to decompose bigger
tensors than competitors without compromising efﬁciency. From a streaming tensor decomposition
point of view  our Singleshot extension is competitive with its competitor.

k ) (k being the iteration number) to a minimizer.

K

2 Notations & Deﬁnitions
A N−order tensor is denoted by a boldface Euler script letter X ∈ RI1×···×IN . The matrices are
denoted by bold capital letters (e.g. A). The identity matrix is denoted by Id. The jth row of a
matrix A ∈ RJ×L is denoted by Aj : and the transpose of a matrix A by A(cid:62).
Matricization is the process of reordering all the elements of a tensor into a matrix. The mode-
matrix X(n) ∈ RIn×((cid:81)
n matricization of a tensor [X ](n) arranges the mode-n ﬁbers to be the columns of the resulting
m(cid:54)=n Im). The mode-n product of a tensor G ∈ RJ1×···×JN with a matrix

2

in

in is obtained by reshaping the ith

n row of X(n)  i.e. the tensor X n

A ∈ RIn×Jn denoted by G ×n A yields a tensor of the same order B ∈ RJ1×···Jn−1×In×Jn+1···×JN
whose mode-n matricized form is deﬁned by: B(n) = AG(n). For a tensor X ∈ RI1×...×IN   its
n subtensor with respect to the mode n is denoted by X n
∈ RI1×···×In−1×1×In+1×···×IN . This
ith

subtensor is a N-order tensor deﬁned via the mapping between its n-mode matricization(cid:2)X n
matrices(cid:8)A(m) ∈ RIm×Jm(cid:9)

n row of X(n)  with the target
the ith
N = {n  ..  N}:
shape (I1  ..  In−1  1  In+1  ..  IN ). The set of integers from n to N is denoted by In
if n = 1  the set is simply denoted by IN . The set of integers from 1 to N with n excluded is
denoted by IN(cid:54)=n = {1  ..  n − 1  n + 1  ..  N}. Let us deﬁne the tensor G ∈ RJ1×..×JN and N
. The product of G with the matrices A(m)  m ∈ IN denoted by
G ×1 A(1) ×2 ... ×N A(N ) will be alternatively expressed by:
G ×m
m∈IN
The Frobenius norm of a tensor X ∈ RI1×···×IN   denoted by (cid:107)X(cid:107)F is deﬁned by:
(cid:107)X(cid:107)F =
2 . The same deﬁnition holds for matrices.

n∈IN
A(m) ×n A(n) ×q
q∈In+1

(cid:16)(cid:80)
1≤in≤In 1≤n≤N X 2

A(m) = G ×m
m∈In−1

A(q) = G ×m
m∈IN(cid:54)=n

(cid:3)(n) and

A(m) ×n A(n).

(cid:17) 1

i1 ···  iN

in

N

3 Piecewise tensor decomposition: Singleshot

3.1 Tucker decomposition and problem statement
Given a tensor X ∈ RI1×...×IN   the Tucker decomposition aims at the following approximation:

X ≈ G ×m
m∈IN

A(m) G ∈ RJ1×...×JN   A(m) ∈ RIm×Jm

The tensor G is generally named the core tensor and the matrices(cid:8)A(m)(cid:9)
A natural way to tackle this problem is to infer the latent factors G and(cid:8)A(m)(cid:9)

the loading matrices.
With orthogonality constraints on the loading matrices  this decomposition can be seen as the
multidimensional version of the singular value decomposition [23].
in such a
way that the discrepancy is low. Thus  the decomposition of X is usually obtained by solving the
following optimization problem:

m∈IN

m∈IN

(cid:26)

(cid:16)G  A(1) ···   A(N )(cid:17) (cid:44) 1

min

G A(1) ···  A(N )

f

(cid:27)

(cid:107)X − G ×m∈IN A(m)(cid:107)2

F

2

(1)

Our goal in this work is to solve the above problem  for large tensors  while addressing two potential
issues : the processing of a tensor that does not ﬁt into the available memory and avoiding memory
overﬂow problem generated by intermediate operations during the decomposition process [21].
For this objective  we leverage on a reformulation of the problem (1) in terms of subtensors drawn
from X with respect to one mode (which we suppose to be predeﬁned)  the ﬁnal objective being to
set up a divide-and-conquer type approach for the inference task. Let’s consider a ﬁxed integer n (in
the sequel  n will be referred to as the splitting mode). Indeed  the objective function can be rewritten
in the following form (see supplementary  property 2):

(cid:16)G  A(1) ···   A(N )(cid:17)

f

=

In(cid:88)

in=1

(cid:107)X n

in

1
2

− G ×m
m∈IN(cid:54)=n

A(m) ×n A(n)

in :(cid:107)2

F

(2)

More generally  the function f given by (1) can be expressed in terms of subtensors drawn with
respect to every mode (see supplementary material  property 3). For simplicity concerns  we only
address the case of subtensors drawn with respect to one mode and the general case can be derived
following the same principle (see supplementary material  section 5).

3.2 Singleshot

Since the problem (1) does not admit any analytic solution  we propose a numerical resolution
based on coordinate gradient descent [41]. The underlying idea is based on a cyclic update over
each of the variables G  A(1)  ..  A(N ) while ﬁxing the others at their last updated values and each

3

k = 0

1≤m≤N

X tensor of interest  n splitting mode 

G (cid:8)A(m)(cid:9)

Algorithm 1 Singleshot
Inputs:
Output:
Initialization:
1: while a predeﬁned stopping criterion is not met do
2:
3:
4:
Compute optimal step ηp
5:
k
k+1 ← A(p)
A(p)
kDp
6:
end for
7:
8: end while

Compute optimal step ηG
Gk+1 ← Gk − ηG
k DG
k
for p from 1 to N do
k − ηp

k

k

(cid:110)

A(m)

0

(cid:111)

1≤m≤N

initial loading matrices 

with DG

k given by (4)

with Dp

k given by (5) (6)

k

k   ...  A(N )

(cid:16)Gk+1  A(1)

(cid:16)Gk  A(1)
update being performed via a single iteration of gradient descent. More formally  given at iteration
k  Gk  A(1)
the value of the latent factors  the derivatives DG
k of f with respect
k  ···   A(N )
and
to the core tensor and the pth loading matrix respectively evaluated at
(cid:19)
(cid:111)p−1
(cid:110)
(cid:111)N

(cid:18)
k+1 ···   A(p−1)

k+1   A(p)

are given by:

k .  A(N )

k and Dp

(cid:111)N

(cid:19)

(cid:18)

(cid:110)

(cid:110)

(cid:17)

(cid:17)

k

k

DG
k = ∂Gf

Gk 

A(m)

k

 

1

Dp

k = ∂A(p)f

Gk+1 

A(m)
k+1 

1

 

A(q)
k

p

(3)

The resulting cyclic update algorithm  named Singleshot  is summarized in Algorithm 1. A naive
implementation of the gradient computation would result in memory overﬂow problem. In what
follows  we show that the derivatives DG
k  1 ≤ p ≤ N given by the equation (3) can be
computed by processing a single subtensor at a time  making Algorithm 1 amenable to sequential
processing of subtensors. Discussions on how the step sizes are obtained will be provide in Section 4.
Derivative with respect to G. The derivative with respect to the core tensor is given by (details in
Property 7 of supplementary material):

k and Dp

In(cid:88)

in=1

DG
k =

(cid:124)
Rin ×m
m∈IN(cid:54)=n
sequence(cid:8)(DG
k )j(cid:9)

It is straightforward to see that DG

(cid:16)

A(m)

k

(cid:18)(cid:16)

(cid:17)(cid:62) ×n
(cid:123)(cid:122)
deﬁned as(cid:0)DG

θin

(cid:17)

A(n)

k

in :

(cid:19)(cid:62)
(cid:125)
 Rin = −X n
(cid:1)j−1

+ θj  with(cid:0)DG

in

(4)
k (given by the equation (4)) is the last term of the recursive

(cid:1)0 being the null tensor.

(cid:1)j

=(cid:0)DG

k

k

1≤j≤In
j . This is the key of our approach since it allows the computation of DG

An important observation is that the additive term θj (given by the equation (4)) depends only on one
single subtensor X n
k through
the sequential processing of a single subtensor X n
Derivatives with respect to A(p)  p (cid:54)= n (n being the splitting mode). For those derivatives  we
can exactly follow the same reasoning  given in detail in Property 9 of the Supplementary material 
and obtain for p < n (the case p > n yields a similar formula):

j at a time.

k

+Gk ×m
m∈IN(cid:54)=n

k ×n
A(m)

A(n)

k

(cid:16)

(cid:17)

in :

In(cid:88)

(cid:16)−(cid:0)Xn

in

(cid:1)(p)

Dp

k =

(cid:17)(cid:16)

(cid:17)(cid:62)

+ A(p)

k B(p)
in

B(p)
in

(5)

in=1

The matrices (Xn
subtensor X n
in

)(p) and B(p)
in

in and the tensor Bin is deﬁned by:

represent respectively the mode-p matricized forms of the ith
n

Bin = Gk+1 ×m
m∈Ip−1

k+1 ×p Id ×q
A(m)
q∈Ip+1
N(cid:54)=n

k ×n (A(n)
A(q)

k )in :

4

with Id ∈ RJp×Jp being the identity matrix. With a similar reasoning as for the derivative with
respect to the core  it is straightforward to see that Dp
k can be computed by processing a single
subtensor at a time.

Derivative with respect to A(n) (n being the splitting mode). The derivative with respect to the
matrix A(n) can be computed via the row-wise stacking of independent terms  that are  the derivatives
j : depends only on X n
with respect to the rows A(n)
j .
Indeed  let’s consider 1 ≤ j ≤ In. In the expression of the objective function f given by the equation
j : ×q
(2)  the only term that depends on A(n)
F   thus
j :
q∈In+1

j : and the derivative of f with respect to A(n)
A(m) ×n A(n)

is (cid:107)X n

A(q)(cid:107)2

N

j − G ×m
m∈In−1
j : depends only on X n

j and is given by (see property 8 in the

j : B(n)(cid:17)

(Xn

j )(n) − A(n)

B(n)(cid:62)

(6)

the derivative of f with respect to A(n)
supplementary material):

(cid:19)

= −(cid:16)

G 

∂A(n)

A(m)(cid:111)N
(cid:110)

(cid:18)
j )(n) ∈ R1×(cid:81)
j and B with B = G ×p
p∈In−1

f

j :

1

The tensors (Xn
the tensors X n

k(cid:54)=n Ik and B(n) respectively represent the mode-n matricized form of
A(q)  Id ∈ RJn×Jn: identity matrix.

A(p) ×n Id ×q
q∈In+1

N

Remark 1. . For one-mode subtensors  it is relevant to choose n such that In is the largest dimension
since this yields the smallest subtensors. We stress that all entries of the tensor X have been
entirely processed when running Algorithm 1 and our key action is the sequential processing of
subtensors X n
. In addition  if one subtensor does not ﬁt in the available memory  the recursion 
as shown in section 5 of the supplementary material  can still be applied to subtensors of the form
X θ1 ... θN   θm ⊂ {1  2  ..  Im} with (X θ1 .. θN )i1 .. iN = X i1 .. iN   (i1  ..  iN ) ∈ θ1 × ... × θN   ×
referring to the Cartesian product.

in

in

3.3 Singleshotinexact
While all of the subtensors X n
  1 ≤ in ≤ In are considered in the Singleshot algorithm for the
computation of the gradient  in Singleshotinexact  we propose to use only a subset of them for the
sake of reducing computational time. The principle is to use for the gradients computation only
Bk < In subtensors. Let’s consider the set SET k (of cardinality Bk) composed of the integers
representing the indexes of the subtensors that are going to be used at iteration k. The numerical
resolution scheme is identical to the one described by Algorithm 1 except for the deﬁnition of DG
and Dp

k and (cid:98)Dp

k

k  p (cid:54)= n deﬁned by:
(cid:17)

A(m)(cid:62)

k

×n

(cid:18)(cid:16)
(cid:17)(cid:16)

A(n)

k

(cid:19)(cid:62)
(cid:17)(cid:62)

in :

+ A(p)B(p)
in

B(p)
in

(cid:1)(p)

(7)

(8)

k =

in∈SET k

(cid:98)DG
(cid:98)Dp

k which are respectively replaced by (cid:98)DG
Rin ×m
(cid:16)−(cid:0)Xn
m∈IN(cid:54)=n

(cid:88)
(cid:88)
worth to highlight that the derivative (cid:98)Dn
(cid:40)
(cid:41)
A(m) ×n A(n)
j : (cid:107)2

in∈SET k

k =

in

F

For the theoretical convergence  the descent steps are deﬁned as ηG

  1 ≤ p ≤ N. It is
k (n being the mode with respect to which the subtensors are
drawn) is sparse: Singleshotinexact amounts to minimize f deﬁned by (2) by dropping the terms
(cid:107)X n
j − G ×m
  j (cid:54)∈ SET k are
m∈IN(cid:54)=n
all equal to zero.

with j (cid:54)∈ SET k  thus  the rows

(cid:16)(cid:98)Dn

and ηp
k
Bk

(cid:17)

k
Bk

j :

k

3.4 Discussions

First  we discuss the space complexity needed by our algorithms supposing that the subtensors
are drawn with respect to one mode. Let’s denote by n the splitting mode. For Singleshot and
Singleshot-inexact  at the same time  we only need to have in memory the tensor X n
j of size

5

m∈IN(cid:54)=n

m∈IN(cid:54)=n

m∈IN(cid:54)=n

Im +(cid:80)

Im = I1..In−1In+1..IN   the matrices(cid:8)A(m)(cid:9)

(cid:81)
the gradient. Thus  the complexity in space is(cid:81)
being the space complexity of the previous gradient iterate: for the core update  AT =(cid:81)

  A(n)
in : and the previous iterate of
m(cid:54)=n ImJm + Jn + AT with AT
Jm
and for a matrix A(m)  AT = ImJm. If the recursion used for the derivatives computation is applied
to subtensors of the form Xθ1 ···  θN   the space complexity is smaller than these complexities.
Another variant of Singleshotinexact can be derived to address an interesting problem that has received
little attention so far [4]  that is the decomposition of a tensor streaming in every mode with a single
pass constraint (i.e. each chunk of data is processed only once) named Singleshotonline. This is
enabled by the inexact gradient computation which uses only subtensors that are needed. In the
streaming context  the gradient is computed based only on the available subtensor.
Positivity constraints is one of the most encountered constraints in tensor computation and we can
simply handle those constraints via the so-called projected gradient descent [45]. This operation does
not alter the space complexity with respect to the unconstrained case  since no addition storage is
required but increases the complexity in time. For more details  see the section 3 in the supplementary
material for the algorithmic details for the proposed variants.

m∈IN

4 Theoretical result

f

min

G A(1) .. A(N )

Let’s consider the minimization problem (1):

(cid:16)G  A(1)  ..  A(N )(cid:17)
native notations of f (G  A(1) ···   A(N )) given by: f (G (cid:8)A(m)(cid:9)N

By denoting the block-wise derivative by ∂xf  the derivative of f  denoted ∇f and deﬁned by
(∂Gf  ∂A(1)f..∂A(N ) f )  is an element of RJ1×..×JN ×RI1×J1×...×RIN×JN endowed with the norm
(cid:107)·(cid:107)∗ deﬁned as the sum of the Frobenius norms. Besides  let’s consider  for writing simplicity  the alter-
p+1).
For the theoretical guarantees which details have been reported in the supplementary material  we
consider the following assumptions:
Assumption 1. Uniform boundedness. The nth subtensors are bounded: (cid:107)X n
Assumption 2. Boundedness of factors. We consider the domain G ∈ Dg  A(m) ∈ Dm with:

1 )  f (G (cid:8)A(m)(cid:9)p

1  (cid:8)A(q)(cid:9)N

(cid:107)F ≤ ρ.

in

Dg = {(cid:107)Ga(cid:107)F ≤ α}   Dm =

a (cid:107)F ≤ α

(cid:110)(cid:107)A(m)

(cid:111)

4.1 Convergence result of Singleshot
For the convergence analysis  we consider the following deﬁnitions of the descent steps ηG
the (k + 1)th iteration:

k and ηp

k at

ηG
k = arg min
  δ2√
K

η∈[ δ1√

K

(η − δ1√
K

)φg(η)  ηp

k = arg min
  δ2√
K

η∈[ δ1√

K

(η − δ1√
K

)φp(η)

(9)

(cid:18)

φp (η) =f

(cid:110)

]

(cid:18)
(cid:111)p−1

1

φg(η) = f

Gk − ηDG
k  

A(m)

− f

Gk 

A(m)

k

(cid:110)

(cid:19)
(cid:111)N
(cid:111)N

1

k

(cid:110)

(cid:18)

(cid:19)

]

(cid:110)
(cid:18)

(cid:19)

(cid:111)N
(cid:110)

1

Gk+1 

A(m)
k+1

  A(p)

k − ηDp
k 

A(q)
k

− f

Gk+1 

A(m)
k+1

p+1

(cid:111)p−1

1

 

(cid:110)

A(q)
k

(cid:19)

(cid:111)N

p

and δ2 > δ1 > 0 being user-deﬁned parameters. The motivation of the problems given by the
equation (9) is to ensure a decreasing of the objective function after each update. Also note that  the
minimization problems related to ηG
k are well deﬁned since all the factors involved in their
deﬁnitions are known at the computation stage of Gk+1 and A(p)
k+1 and correspond to the minimization
of a continuous function on a compact set.

k and ηp

6

Along with Assumption 1 and Assumption 2  as well as the deﬁnitions given by (9)  we assume that:

δ1√
K

< ηG

k ≤ δ2√

K

and

δ1√
K

< ηp

k ≤ δ2√

K

(10)

This simply amounts to consider that the solutions of the minimization problems deﬁned by the
equation (9) are not attained at the lower bound of the interval. Under this framework  we establish 
as in standard non-convex settings [12]  an ergodic convergence rate. Precisely  we prove that:

k=0

1
K

(cid:110)

(cid:110)

Gk 

Gk 

A(m)

A(m)

(cid:107)∇f

with (cid:107)∇f

∃K0 ≥ 1 ∀K ≥ K0 

(cid:19)
(cid:111)N
2 +(cid:80)N

(cid:18)
(cid:111)N
(cid:16)
supremun of f (cid:107)∂A(p) f (G (cid:8)A(m)(cid:9))(cid:107)F  (cid:107)∂Gf (G (cid:8)A(m)(cid:9))(cid:107)F on the compact set Dg × D1.. × DN
the rate O(cid:16) 1√

)(cid:107)F  
  Γ  Γp  Γg ≥ 0 being respectively the

This result proves that Singleshot converges ergodically to a point where the gradient is equal to 0 at

(cid:107)2∗ ≤ (N + 1)∆√
(cid:110)

p=1 (cid:107)∂A(p)f (Gk 

(cid:107)∗ = (cid:107)∂Gf (Gk 

p=1(1 + 2Γ + α2N Γ2

2Γ + α2N Γ2

∆ = δ2
δ2
1

1
pδ2
2)

1
gδ2

A(m)

A(m)

(cid:17)

(11)

K

.

k

k

k

1

K

(cid:110)
(cid:111)N

k

(cid:19)
(cid:111)N
)(cid:107)F +(cid:80)N
(cid:17)

1

K−1(cid:88)

(cid:18)

4.2 Convergence result for Singleshotinexact
Let us consider that (cid:96)j(A(N )) (cid:44) 1

2(cid:107)X n

k+1)j : ×q
q∈In+1
N−1
k for A(N ) is deﬁned by the following minimization problem:

j − Gk+1 ×m
m∈In−1

k+1 ×n (A(n)
A(m)

k+1 ×N A(N )(cid:107)2
A(q)

F

and that the step ηN

(cid:18)
η − 1
(cid:111)N
(cid:110)
4K γ

(cid:19)
(cid:19)

A(m)

k

1

φ(η)

+ λf

(cid:18)

Gk+1 

(cid:110)

A(m)
k+1

(cid:111)N−1

1

(12)

(cid:19)

  ˜φ(η)

φ(η) = f

(cid:18)

(cid:110)
Gk+1 
k − η

(cid:111)N−1
(cid:80)

1

A(m)
k+1

ηN
k = arg min
η∈[ 1
4Kγ   1
− f

  A(N )

(cid:19)

(cid:18)

Kγ ]
Gk 

k

k

Bk

j∈SET k

2. Non-vanishing gradient with respect to A(N ) ∂A(N ) f

∂A(N )(cid:96)j  ∂A(N ) (cid:96)j being the derivative of (cid:96)j evaluated at A(N )

and ˜φ(η) = A(N )
.
The parameters λ > 0  γ > 1 represent user-deﬁned parameters. In addition to Assumption 1 and
Assumption 2  we consider the following three additional assumptions:
k ≤ 1
1. Descent step related to the update of A(N ). We assume that
(cid:111)N−1
that the solution of problem (12) is not attained at the lower bound of the interval.
(cid:54)= 0.
A(m)
k+1
∂A(N )(cid:96)j (cid:54)= 0 and the set

(cid:18)
(cid:110)
This condition ensures the existence of a set SET k such that(cid:80)
3. Choice of the number of subtensors Bk. We suppose that In ×(cid:113) 1
(cid:19)
(cid:111)N

considered for the problem (12) is one of such sets.

This condition In > 2 ensures that In

Gk+1 
j∈SET k

≤ Bk and In > 2.

α2N   which means

(cid:113) 1

4Kγ < ηN

2 + 1
In

2 + 1
In

  A(N )

< In.

(cid:19)

(cid:18)

(cid:110)

k

1

1

With these assumptions at hand  the sequence ∆k = f

− fmin veriﬁes:

(cid:18)

A(m)

k

1

Gk 

(cid:19)

∀k > k0 = 1 +

1

log

1

log(1 + λ)

log(1 + λ)

  ∆k ≤ ∆1 + ζ(λ  ρ  α  In)

k − k0

(13)

with log being the logarithmic function  fmin representing the minimizer of f  a continuous function
deﬁned on the compact set Dg × D1 × .... × DN and ζa function of λ  ρ  α  In. The parameter k0 is
generated by
well-deﬁned since λ > 0. This result ensures that the sequence

(cid:110)Gk  A(1)
Singleshotinexact converges to the set of minimizers of f at the rate O(cid:0) 1
(cid:1)

k   .  A(N )

(cid:111)

k

Remark 2. The problems deﬁned by the equations (9) and (12)  which solutions are global and can
be solved by simple heuristics (e.g. Golden section)  are not in contradiction with our approach since
they can be solved by processing a single subtensor at a time due to the expression of f given by (2).

k

7

Figure 1: Approximation error and running time for the unconstrained decomposition algorithms.
From left to right: ﬁrst and second ﬁgures represent Movielens  third and fourth represent Enron. As
M grows   missing markers for a given algorithm means that it ran out of memory. The core G rank
is (5  5  5).

Figure 2: Approximation error and running time for the non-negative decomposition algorithms.
From left to right: ﬁrst and second ﬁgures represent Movielens  third and fourth represent Enron. As
M grows  missing markers  for a given algorithm means that it ran out of memory. The core G rank
is (5  5  5).

5 Numerical experiments

Our goal here is to illustrate that for small tensors  our algorithms Singleshot and Singleshotinexact
and their positive variants  are comparable to some state-the-art decomposition methods. Then as the
tensor size grows  we show that they are the only ones that are scalable. The competitors we have
considered include SVD-based iterative algorithm [44](denoted GreedyHOOI )  a very recent alternate
minimization approach based on sketching [4] (named Tensorsketch) and randomization-based
methods [51] (Algorithm 2 in [51] named Scalrandomtucker and Algorithm 1 in [51] with positivity
constraints named posScalrandomtucker). Other materials related to the numerical experiments are
provided in the section 4 of the supplementary material. For the tensor computation  we have used
the TensorLy tool [22].
The experiments are performed on the Movielens dataset [15]  from which we construct a 3-order
tensor whose modes represent timesteps  movies and users and on the Enron email dataset  from which
a 3-order tensor is constructed  the ﬁrst and second modes representing the sender and recipients of
the emails and the third one denoting the most frequent words used in the miscellaneous emails. For
Movielens  we set up a recommender system for which we report a mean average precision (MAP)
obtained over a test set (averaged over ﬁve 50 − 50 train-test random splits) and for Enron  we report
an error (AE) on a test set (with the same size as the training one) for a regression problem. As our
goal is to analyze the scalability of the different methods as the tensor to decompose grows  we have
arbitrarily set the size of the Movielens and Enron tensors to M × M × 200 and M × M × 610  M
being a user-deﬁned parameter. Experiments have been run on MacOs with 32 Gb of memory.
Another important objective is to prove the robustness of our approach with respect to the assumptions
and the deﬁnitions related to the descent steps laid out in the section 4  which is of primary importance
since the minimization problems deﬁning these steps can be time-consuming in practice for large
tensors. This motivates the fact that for our algorithms  the descent steps are ﬁxed in advance. For
Singleshot  the steps are ﬁxed to 10−6. For Singleshot-inexact  the steps are respectively ﬁxed to
10−6 and 10−8 for Enron and Movielens. Regarding the computing of the inexact gradient for
Singleshotinexact  the elements in SET k are generated randomly without replacement with the same
cardinality for any k. The number of slices is chosen according to the third assumption in section 4.2.
For the charts  the unconstrained versions of Singleshot and Singleshotinexact will be followed by
the term "unconstrained" and "positive" for the positive variantes.

8

5001000150020002500300035004000Dimension M0123456Mean Average Precision (MAP)GreedyHOOIScalablerandomizedtuckerTensorsketchSingleshotinexactunconstrainedSingleshotunconstrained5001000150020002500300035004000Dimension M101102103104Running time (s)GreedyHOOIScalablerandomizedtuckerTensorsketchSingleshotinexactunconstrainedSingleshotunconstrained150020002500300035004000Dimension M0100020003000400050006000700080009000Approximation error (AE)GreedyHOOIScalablerandomtuckerTensorsketchSingleshotinexactunconstrainedSingleshotunconstrained1000150020002500300035004000Dimension M101102103104Running time (s)GreedyHOOIScalablerandomtuckerTensorsketchSingleshotinexactunconstrainedSingleshotunconstrained5001000150020002500300035004000Dimension M0123456Mean Average PrecisionposScalablerandomtuckerSingleshotinexactpositiveSingleshotpositive5001000150020002500300035004000Dimension M101102103104Running time (s)posScalablerandomtuckerSingleshotinexactpositiveSingleshotpositive150020002500300035004000Dimension M30004000500060007000Approximation errorposScalablerandomizedtuckerSingleshotinexactpositiveSingleshotpositive150020002500300035004000Dimension M102103104Running time (s)posScalablerandomizedtuckerSingleshotinexactpositiveSingleshotpositiveFigure 3: Comparing Online version of Tensorsketch and Singleshot with positive constraints on the
Enron dataset. (left) Approximation error. (right) running time.

Figure 1 presents the results we obtain for these two datasets. At ﬁrst  we can note that performance 
in terms of MAP or AE  are rather equivalent for the different methods. Regarding the running
time  the Scalrandomtucker is the best performing algorithm being an order of magnitude more
efﬁcient than other approaches. However  all competing methods struggle to decompose tensors with
dimension M = 4000 and M ≥ 2800 respectively for Enron and Movielens due to memory error.
Instead  our Singleshot methods are still able to decompose those tensors  although the running time
is large. As expected  Singleshotinexact is more computationally efﬁcient than Singleshot.
Figure 2 displays the approximation error and the running time for Singleshot and singleshotinexact
with positivity constraints and a randomized decomposition approach with non-negativity constraints
denoted here as PosScalrandomtucker for Enron and Movielens. Quality of the decomposition is in
favor of Singleshotpositive for both Movielens and Enron. In addition  when the tensor size is small
enough  PosScalrandomtucker is very computationally efﬁcient  being one order of magnitude faster
than our Singleshot approaches on Enron. However  PosScalrandomtucker is not able to decompose
very large tensors and ran out of memory contrarily to Singleshot.
For illustrating the online capability of our algorithm  we have considered a tensor of size 20000 ×
2000 × 50 constructed from Enron which is artiﬁcially divided into slices drawn with respect to
the ﬁrst and the second modes. The core rank is (R  R  R). We compare the online variant of our
approach associated to positivity constraints named Singleshotonlinepositive to the online version
of Tensorsketch [4] denoted Tensorsketchonline. Figure 3 shows running time for both algorithms.
While of equivalent performance  our method is faster as our proposed update schemes  based on one
single step of gradient descent  are more computationally efﬁcient than a full alternate minimization.

Remark 3. Other assessments are provided in the supplementary material: comparisons with other
recent divide-and-conquer type approaches are provided  the non-nullity of the gradient with respect
to A(n) is numerically shown  and ﬁnally  we demonstrated the expected behavior of Singleshotinexact 
i.e. “the higher the number of subtensors in the gradient approximation  the better performance we
get”.

6 Conclusion

Singleshotinexact  a convergence rate of O(cid:0) 1

K

k

We have introduced two new algorithms named Singleshot and Singleshotinexact for scalable Tucker
decomposition with convergence rates guarantees: for Singleshot  we have established a convergence
rate to the set of minimizers of O( 1√
) (K being the maximum number of iterations) and for

(cid:1) (k being the iteration number). Besides  we have

proposed a new approach for a problem that has drawn little attention so far  that is  the Tucker
decomposition under the single pass constraint (with no need to resort to the past data) of a tensor
streaming with respect to every mode. In future works  we aim at applying the principle of Singleshot
to other decomposition problems different from Tucker.

9

23456Rank R920092509300935094009450950095509600Approximation Error (AE)TensorsketchonlineSingleshotonline positive23456Rank R1000120014001600180020002200240026002800Running time (s)TensorsketchonlineSingleshotonline positiveAcknowledgments

This work was supported by grants from the Normandie Projet GRR-DAISI  European funding
FEDER DAISI and LEAUDS ANR-18-CE23-0020 Project of the French National Research Agency
(ANR).

References
[1] Woody Austin  Grey Ballard  and Tamara G. Kolda. Parallel tensor compression for large-scale
scientiﬁc data. 2016 IEEE International Parallel and Distributed Processing Symposium  2016.

[2] Grey Ballard  Alicia Klin  and Tamara G. Kolda. Tuckermpi: A parallel c++/mpi software

package for large-scale data compression via the tucker tensor decomposition. arxiv  2019.

[3] Muthu Manikandan Baskaran  Benoît Meister  Nicolas Vasilache  and Richard Lethin. Efﬁcient
and scalable computations with sparse tensors. 2012 IEEE Conference on High Performance
Extreme Computing  pages 1–6  2012.

[4] Stephen Becker and Osman Asif Malik. Low-rank tucker decomposition of large tensors using
tensorsketch. Advances in Neural Information Processing Systems  pages 10117–10127  2018.

[5] Cesar F. Caiafa and Andrzej Cichocki. Generalizing the column-row matrix decomposition to
multiway arrays. In Linear Algebra and its Applications  volume 433  pages 557–573  2010.

[6] Raymond B. Cattell. Parallel proportional proﬁles” and other principles for determining the

choice of factors by rotation. Psychometrika  9(4):267–283  1944.

[7] Venkatesan T. Chakaravarthy  Jee W. Choi  Douglas J. Joseph  Prakash Murali  Shivmaran S.
Pandian  Yogish Sabharwal  and Dheeraj Sreedhar. On optimizing distributed tucker decomposi-
tion for sparse tensors. In Proceedings of the 2018 International Conference on Supercomputing 
pages 374–384  2018.

[8] Maolin Che and Yimin Wei. Randomized algorithms for the approximations of tucker and the

tensor train decompositions. Advances in Computational Mathematics  pages 1–34  2018.

[9] Dongjin Choi  Jun-Gi Jang  and Uksong Kang. Fast  accurate  and scalable method for sparse

coupled matrix-tensor factorization. CoRR  2017.

[10] Dongjin Choi and Lee Sael. Snect: Scalable network constrained tucker decomposition for

integrative multi-platform data analysis. CoRR  2017.

[11] Andrzej Cichocki  Rafal Zdunek  Anh Huy Phan  and Shun-ichi Amari. Nonnegative Matrix
and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind
Source Separation. Wiley Publishing  2009.

[12] Alistarh Dan and al. The convergence of sparsiﬁed gradient methods. Advances in Neural

Information Processing Systems  pages 5977–5987  2018.

[13] Petros Drineas and Michael W. Mahoney. A randomized algorithm for a tensor-based general-
ization of the singular value decomposition. Linear Algebra and its Applications  420:553–571 
2007.

[14] Dóra Erdös and Pauli Miettinen. Walk ’n’ merge: A scalable algorithm for boolean tensor
factorization. 2013 IEEE 13th International Conference on Data Mining  pages 1037–1042 
2013.

[15] F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. TiiS 

5:19:1–19:19  2015.

[16] F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. J. Math.Phys. 

6(1):164–189  1927.

[17] Inah Jeon  Evangelos E. Papalexakis  Uksong Kang  and Christos Faloutsos. Haten2: Billion-
scale tensor decompositions. 2015 IEEE 31st International Conference on Data Engineering 
pages 1047–1058  2015.

10

[18] Oguz Kaya and Bora Uçar. High performance parallel algorithms for the tucker decomposition
of sparse tensors. 2016 45th International Conference on Parallel Processing (ICPP)  pages
103–112  2016.

[19] Tamara Kolda and Brett Bader. The tophits model for higher-order web link analysis. Workshop

on Link Analysis  Counterterrorism and Security  7  2006.

[20] Tamara G. Kolda and Jimeng Sun. Scalable tensor decompositions for multi-aspect data mining.
In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining  pages
363–372  2008.

[21] Tamara G. Kolda and Jimeng Sun. Scalable tensor decompositions for multi-aspect data mining.
In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining  ICDM ’08 
pages 363–372  2008.

[22] Jean Kossaiﬁ  Yannis Panagakis  and Maja Pantic. Tensorly: Tensor learning in python. arXiv 

2018.

[23] Lieven Lathauwer and al. A multilinear singular value decomposition. SIAM J. Matrix Anal.

Appl.  21(4):1253–1278  2000.

[24] Lieven De Lathauwer and Joos Vandewalle. Dimensionality reduction in higher-order signal
processing and rank-(r1  r2 ... rn) reduction in multilinear algebra. Linear Algebra and its
Applications  391:31–55  2004.

[25] Dongha Lee  Jaehyung Lee  and Hwanjo Yu. Fast tucker factorization for large-scale tensor
completion. 2018 IEEE International Conference on Data Mining (ICDM)  pages 1098–1103 
2018.

[26] Xiaoshan Li  Hua Zhou  and Lexin Li. Tucker tensor regression and neuroimaging analysis.

Statistics in Biosciences  04 2013.

[27] Xinsheng Li  K. Selçuk Candan  and Maria Luisa Sapino. M2td: Multi-task tensor decom-
position for sparse ensemble simulations. 2018 IEEE 34th International Conference on Data
Engineering (ICDE)  pages 1144–1155  2018.

[28] Ching-Yung Lin  Nan Cao  Shi Xia Liu  Spiros Papadimitriou  J Sun  and Xifeng Yan. Smallblue:
Social network analysis for expertise search and collective intelligence. ICDE  pages 1483 –
1486  2009.

[29] Michael W. Mahoney  Mauro Maggioni  and Petros Drineas. Tensor-cur decompositions for

tensor-based data. In SIGKDD  pages 327–336  2006.

[30] Carmeliza Navasca and Deonnia N. Pompey. Random projections for low multilinear rank
tensors. In Visualization and Processing of Higher Order Descriptors for Multi-Valued Data 
pages 93–106  2015.

[31] Jinoh Oh  Kijung Shin  Evangelos E. Papalexakis  Christos Faloutsos  and Hwanjo Yu. S-hot:
Scalable high-order tucker decomposition. In Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining  pages 761–770  2017.

[32] Sejoon Oh  Namyong Park  Lee Sael  and Uksong Kang. Scalable tucker factorization for
sparse tensors - algorithms and discoveries. 2018 IEEE 34th International Conference on Data
Engineering (ICDE)  pages 1120–1131  2018.

[33] Moonjeong Park  Jun-Gi Jang  and Sael Lee. Vest: Very sparse tucker factorization of large-scale

tensors. 04 2019.

[34] Namyong Park  Sejoon Oh  and U Kang. Fast and scalable method for distributed boolean

tensor factorization. In The VLDB Journal  page 1–26  2019.

[35] Ioakeim Perros  Robert Chen  Richard Vuduc  and J Sun. Sparse hierarchical tucker factorization

and its application to healthcare. pages 943–948  2015.

11

[36] Kijung Shin  Lee Sael  and U Kang. Fully scalable methods for distributed tensor factorization.

IEEE Trans. on Knowl. and Data Eng.  29(1):100–113  January 2017.

[37] Nicholas D. Sidiropoulos  Evangelos E. Papalexakis  and Christos Faloutsos. Parallel randomly
compressed cubes ( paracomp ) : A scalable distributed architecture for big tensor decomposition.
2014.

[38] Shaden Smith and George Karypis. Accelerating the tucker decomposition with compressed

sparse tensors. In Euro-Par  2017.

[39] Jimeng Sun  Spiros Papadimitriou  Ching-Yung Lin  Nan Cao  Mengchen Liu  and Weihong
Qian. Multivis: Content-based social network exploration through multi-way visual analysis.
In SDM  2009.

[40] Jimeng Sun  Dacheng Tao  Spiros Papadimitriou  Philip S. Yu  and Christos Faloutsos. Incre-

mental tensor analysis: Theory and applications. TKDD  2:11:1–11:37  2008.

[41] Paul Tseng and Sangwoon Yun. A coordinate gradient descent method for nonsmooth separable

minimization. In Mathematical Programming  volume 117  page 387–423  2007.

[42] Charalampos E. Tsourakakis. Mach: Fast randomized tensor decompositions. In SDM  2009.

[43] L. R. Tucker. Implications of factor analysis of three-way matrices for measurement of change.
C.W. Harris (Ed.)  Problems in Measuring Change  University of Wisconsin Press  pages
122–137  1963.

[44] Yangyang Xu. On the convergence of higher-order orthogonal iteration. Linear and Multilinear

Algebra  pages 2247–2265  2017.

[45] Rafal Zdunek and al. Fast nonnegative matrix factorization algorithms using projected gradient

approaches for large-scale problems. Intell. Neuroscience  2008:3:1–3:13  2008.

[46] Qibin Zhao  Liqing Zhang  and Andrzej Cichocki. Bayesian sparse tucker models for dimension

reduction and tensor completion. CoRR  2015.

[47] Shandian Zhe  Yuan Qi  Youngja Park  Zenglin Xu  Ian Molloy  and Suresh Chari. Dintucker:

Scaling up gaussian process models on large multidimensional arrays. In AAAI  2016.

[48] Shandian Zhe  Zenglin Xu  Xinqi Chu  Yuan Qi  and Youngja Park. Scalable nonparametric

multiway data analysis. In AISTATS  2015.

[49] Shandian Zhe  Kai Zhang  Pengyuan Wang  Kuang-chih Lee  Zenglin Xu  Yuan Qi  and Zoubin
Gharamani. Distributed ﬂexible nonlinear tensor factorization. In Proceedings of the 30th
International Conference on Neural Information Processing Systems  NIPS’16  pages 928–936 
2016.

[50] Guoxu Zhou and al. Efﬁcient nonnegative tucker decompositions: Algorithms and uniqueness.

IEEE Transactions on Image Processing  24(12):4990–5003  2015.

[51] Guoxu Zhou  Andrzej Cichocki  and Shengli Xie. Decomposition of big tensors with low

multilinear rank. CoRR  2014.

12

,Abraham Traore
Maxime Berar
Alain Rakotomamonjy