2019,Memory-oriented Decoder for Light Field Salient Object Detection,Light field data have been demonstrated in favor of many tasks in computer vision  but existing works about light field saliency detection still rely on hand-crafted features. In this paper  we present a deep-learning-based method where a novel memory-oriented decoder is tailored for light field saliency detection. Our goal is to deeply explore and comprehensively exploit internal correlation of focal slices for accurate prediction by designing feature fusion and integration mechanisms. The success of our method is demonstrated by achieving the state of the art on three datasets. We present this problem in a way that is accessible to members of the community and provide a large-scale light field dataset that facilitates comparisons across algorithms. The code and dataset will be made publicly available.,Memory-oriented Decoder for Light Field

Salient Object Detection

Miao Zhang∗

Jingjing Li∗

Wei Ji∗

Yongri Piao†

Huchuan Lu

Dalian University of Technology  China

miaozhang@dlut.edu.cn  {lijingjing  jiwei521}@mail.dlut.edu.cn 

{yrpiao  lhchuan}@dlut.edu.cn

Abstract

Light ﬁeld data have been demonstrated in favor of many tasks in computer vision 
but existing works about light ﬁeld saliency detection still rely on hand-crafted
features. In this paper  we present a deep-learning-based method where a novel
memory-oriented decoder is tailored for light ﬁeld saliency detection. Our goal is
to deeply explore and comprehensively exploit internal correlation of focal slices
for accurate prediction by designing feature fusion and integration mechanisms.
The success of our method is demonstrated by achieving the state of the art on
three datasets. We present this problem in a way that is accessible to members
of the community and provide a large-scale light ﬁeld dataset that facilitates
comparisons across algorithms. The code and dataset are made publicly available
at https://github.com/OIPLab-DUT/MoLF.

1

Introduction

Salient object detection (SOD) is the ability to identify the most visually distinctive objects despite
substantial appearance similarity in a scene. This fundamental task has attracted lots of interest due
to its importance in various applications  such as visual tracking [20  47]  object recognition [43  10] 
image segmentation [33]  image retrieval [44]  and robot navigation [9].
Existing methods can be categorized into 2D (RGB)  3D (RGB-D) and 4D (light ﬁeld) saliency
detection based on the input data types. 2D methods [15  23  8  18  21  36  27  63] have achieved great
success and long been dominant in the ﬁeld of saliency detection. However  2D saliency detection
methods may suffer from false positives when it comes to challenging scenes shown in Fig. 1. The
reasons are twofold: First  traditional 2D methods underlie many prior knowledges in which violations
highly pose a risk under complex scenes; Second  2D deep-learning-based methods are subject to the
features extracted from limited RGB data not containing as much special information from RGB-D
data or light ﬁeld data. 3D saliency detection has also attracted a lot of attention because depth maps
providing scene layout can improve the saliency accuracy to some extent. However  mediocre-quality
depth maps heavily jeopardize the accuracy of saliency detection.
The light ﬁeld provides images of the scene from an array of viewpoints which spread over the extent
of the lens aperture. These different views can be used to produce a stack of focal slices  containing
abundant spatial parallax information as well as accurate depth information about the objects in the
scene. Furthermore  focusness is one of the strongest information  allowing a human observer to
instantly understand the order in which objects are arranged along the depth in a scene [24  59  29].
Light ﬁeld data have been demonstrated in favor of many applications in computer vision  such as
depth estimation [16  48  64]  super resolution [67  55]  and material recognition [51]. Due to the

∗denotes equal contributions.
†Prof.Piao is the corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Left: some challenging scenes  e.g.  similar foreground and background  complex back-
ground  transparent objects  and low intensity environment. Right: the light ﬁeld data. (a)-(d) are
four focal slices that focus at different depth levels. The green box with red dot represents different
focus positions. From our observation  they are beneﬁcial for efﬁcient foreground and background
separation. (e) shows our model’s saliency results. ‘GT’ means ground truths.

unique property of light ﬁeld  it has shown promising prospects in saliency detection [24  28  58  56 
59  29]. However  deep-learning-based light ﬁeld methods have been missing from contemporary
studies in saliency detection. We have strong reasons to believe introducing the CNN framework for
light ﬁeld saliency detection is an important aspect  as do 2D and 3D methods in SOD.
In order to incorporate the CNN framework and light ﬁeld for accurate SOD  there are three key
issues needed to be considered. First  how do we solve the deﬁciency of training data? Second  how
do we effectively and properly fuse light ﬁeld features generated from different focal slices? Third 
how do we comprehensively integrate multi-level features?
In this paper  we leverage the ideas from light ﬁeld to confront these challenges. To better adapt our
network to fuse features from focal slices  we may neither want to ignore more contribution of the
corresponding focal slices where the salient object happens to be in focus  nor destroy the spatial
correlation between different focal slices. Therefore  we propose a novel memory-oriented spatial
fusion module (Mo-SFM) to resemble the memory mechanism of how human fuse information to
understand a scene by going through all pieces of information and emphasizing the most relevant ones.
On the other hand  integration of fused features is used for higher cognitive processing. Therefore 
we propose a sophisticated multi-level integration mechanism in a top-down manner where high-level
features are used to guide low-level feature selection  namely memory-oriented feature integration
module (Mo-FIM). The previous information referred to as memory is used in our channel attention to
update the current light ﬁeld feature  so that important and unnecessary features can be distinguishable.
In summary  our main contributions are as follows:

• We introduce a large-scale light ﬁled saliency dataset with 1462 samples  each of which con-
tains an all-focus image  a focal stack with 12 focal slices  a depth map  and a corresponding
ground truth  genuinely hoping that this could pave the way for light ﬁeld SOD and enable
more advanced research and development.
• We propose a novel memory-oriented decoder tailored for light ﬁeld SOD. Feature fusion
mechanism in Mo-SFM and feature integration mechanism in Mo-FIM enable more accurate
prediction. This work is  to the best of our knowledge  the ﬁrst exploitation of using the
unique focal slices in light ﬁeld data for deep-learning-based saliency detection.
• Extensive experiments on three light ﬁeld datasets show that our method achieves consis-

tently superior performance over 25 state-of-the-art 2D  3D and 4D approaches.

2 Related Work

Salient Object Detection. Early works [23  8  18  19  40  68  32  30  41  49] for saliency detection
mainly rely on hand-crafted features and prior knowledges  such as color-contrast and background
prior. Recently  with the utilization of CNNs  2D SOD has achieved appealing performance. Li
et al. [27] adopt a CNN to extract multi-scale features to predict saliency for each super-pixel. Wang
et al. [50] propose two CNNs to integrate local super-pixel estimation and global search for SOD.
Zhao et al. [63] utilize two independent CNNs to extract both global and local contexts. Lee et al. [26]
combine low-level distant map with high-level semantic features of deep CNNs for SOD. These

2

0393	0420	1029RGB(a)(b)(c)(d)GT(e)Complex Scenariosmethods achieve better performance but suffer from time-consuming computation and injure the
spatial information of the input images. Afterwords  Liu and Han [35] ﬁrst generate a coarse saliency
map and then reﬁne its details step by step. Hou et al. [21] introduce short connections into multiple
side-outputs based on HED [54] architecture. Zhang et al. [60] integrate multi-level features in
multiple resolutions and combine them for accurate prediction. Luo et al. [37] propose a simpliﬁed
CNN to combine both local and global information and design a loss to penalize boundary errors.
Zhang et al. [62] and Liu et al. [36] introduce attention mechanism to guide feature integration. Deng
et al. [11] design a residual reﬁnement block to learn the complementary saliency information of the
intermediate prediction. Li et al. [31] transfer contour knowledge to saliency detection without using
any manual saliency masks. Detailed surveys about 2D SOD can be found in [3  2  4  52].
In 3D SOD  depth images with afﬂuent spatial information can act as complementary cues for
saliency detection [38  39  14  25  42  5]. Peng et al. [39] regard the depth data as one channel of
input and feed it into a multi-stage saliency detection model. Ju et al. [25] and Feng et al. [14] present
saliency methods based on anisotropic center-surround difference or local background enclosure.
Zhu et al. [66] propose a center-dark channel prior for RGB-D SOD. Qu et al. [42] use hand-crafted
features to train a CNN and achieve better performance than tradition methods. In [17  7]  two-stream
models are used to process the RGB image and depth map separately and cross-modal features are
combined to jointly make prediction. Due to limited training sets  they are trained in a stage-wise
manner. Chen et al. [5] design a progressive fusion network to fuse cross-modal multi-level features
to predict saliency maps. Chen et al. [6] propose a three-stream network to extract RGB-D features
and use attention mechanism to adaptively select complement. Zhu et al. [65] use large-scale RGB
datasets to pre-train a prior model and employ depth-induced features to enhance the network.
Previous works in light ﬁeld SOD have shown promising prospects  especially for some complex
scenarios. Li et al. [29  28] report a saliency detection approach on the light ﬁeld data and propose
the ﬁrst light ﬁeld saliency dataset-LFSD. Zhang et al. [58] propose saliency method based on depth
contrast and focusness-based background priors  and show the effectiveness and superiority of light
ﬁeld properties. Li et al. [56] introduce a weighted sparse coding structure for handling heterogenous
types of input data. Zhang et al. [59] integrate multiple visual cues from light ﬁeld images to detect
salient regions. However  deep-learning-based light ﬁeld methods are still in the infancy  and many
issues have yet to be explored.

3 Light Field Dataset

To remedy the data deﬁciency problem  we introduce a large-scale light ﬁeld saliency dataset with
1462 selected high-quality samples captured by Lytro Illum camera. We decode the light ﬁeld format
ﬁle using Lytro Desktop. Each light ﬁeld consists of an all-focus image  a focal stack with 12 focal
slices focusing at different depths  a depth image  and a corresponding manually labeled ground truth.
The focal stack resembles human perception using eyes  i.e.  the eyes can dynamically refocus at
different focal slices to determine saliency [29]. Fig. 1 shows samples of light ﬁelds in our proposed
dataset. From our observation  they are beneﬁcial for efﬁcient foreground and background separation.
During annotation  three volunteers are asked to draw a rectangle to the most attractive objects.
Then  we collect 1462 samples by choosing the images with consensus. We manually label the
salient objects from the all-focus image using a commonly used segmentation tool. By supplying
the easy-to-understand dataset  we hope to promote the research and make the SOD problem more
accessible to those familiar with this ﬁeld. The proposed light ﬁeld saliency dataset provides the
unique focal slices that can be used to support the training needs of deep neural networks.
This dataset consists of 900 indoor and 562 outdoor scenes captured in the surrounding environments
of our daily life  e.g.  ofﬁces  supermarkets  campuses  streets and so on. Besides  this dataset contains
many challenging scenes as shown in Fig. 1  e.g.  similar foreground and background(108)  complex
background(31)  transparent objects(28)  multiple objects(95)  and low-intensity environments(9).

4 The Proposed Network

4.1 The Overall Architecture

We adopt the widely utilized VGG-19 net [46] as the backbone architecture  drop the last pooling
layer and fully connected layers  and reserve ﬁve convolutional blocks to better ﬁt for our task  as

3

Figure 2: The overall architecture of our proposed network  which contains an encoder and a
memory-oriented decoder.

shown in Fig. 2. In the encoder  RGB image is fed into a stream to generate raw RGB features
while all focal slices are fed into another stream to generate light ﬁeld features with abundant spacial
information. For simplicity  we just illustrate one single encoder  which represents the two streams
simultaneously. As suggested in [5]  the Conv1_2 block (i.e.  Block1) might be too shallow to make
reliable prediction. We hereby perform our decoder on deeper layers (i.e.  Block2-Block5). More
speciﬁcally  given the RGB image I0 and the focal slices {Ii}12
i=1 with size H × W   we denote the
outputs of the last four blocks as {f i
i=0  where i = 0 represents features generated
in the RGB stream  i = 1 · · · 12 represents the indexes of focal slices and m = 2  3  4  5 represents
the last four convolution blocks.

m  m = 2  3  4  5}12

4.2 The Memory-oriented Spatial Fusion Module (Mo-SFM)

With the raw RGB and light ﬁeld features generated from the encoder  we aim at fusing all available
information to address the challenging problem of light ﬁeld SOD. A straightforward solution is to
simply concatenate light ﬁeld features produced by different focal slices. However  two drawbacks
emerge in this approach. First  it ignores the relative contributions of different focal slices to the
ﬁnal results. Focal slices represent images focused at different depths in a scene as shown in Fig. 1.
Intuitively  different focal slices have different weights regarding the salient objects. Second  direct
concatenation operation seriously damages the spatial correlation of those focal slices. A more proper
and effective fusion strategy should be considered. Hence  we propose a novel memory-oriented
spatial fusion module (Mo-SFM) to address this problem. In this module  we introduce an attention
mechanism shown in Fig. 2 to emphasize the useful features and suppress the unnecessary ones from
focused and blurred information. This procedure can be deﬁned as:

Attm = δ(Wm ∗ AvgP ooling(D[f 0

m; f 1

m;··· ; f 12

m ]) + bm) 

(1)

(cid:101)f i

m (cid:12) Atti

m  i = 0  1 ···   12 

m = f i

(2)
where D[ · ; ··· ; · ] means concatenation operation. ∗  Wm and bm represent convolution operator
and convolution parameters in m-th layers. AvgP ooling(·) means global average pooling operation
and δ(·) means softmax function. Attm ∈ R1×1×N means the channel-wise attention map in m-th
Then those weighted light ﬁeld features {(cid:101)f i
layers. (cid:12) denotes feature-wise multiplication.
m}12

i=0 are regarded as a sequence of inputs corresponding
to the consecutive time steps. They are fed into a ConvLSTM [45] structure to gradually reﬁne their

4

Mo-FIMCellMo-FIMCellMo-FIMCellMo-FIMCellEncoderThe illustration of Mo-SFM ConvLSTMCellSCIMThe illustration of Mo-FIM Mo-FIMCellConvLSTMCellSCIMMo-FIMCellPoolingSoftmax(cid:2)Channel	AttentionSCIMMo-SFM -× -× -Mo-SFM -× -× -Mo-SFM -× -× -Mo-SFM -× -× -Block 2/01×/01×/01Block 3 -× -×02 Block 430×30×2/0Block 5/ ×/ ×2/0Block 102 ×02 × -Memory-orientedDecoderPredictionRGBFocal StackSupervision456476486496:;<=ℎ;<=:;ℎ;(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)ℱ@ABCDEElement-wisemultiplicationElement-wiseadditionFeature-wisemultiplicationUp-Sampleℱ@A⊕⊗⨀⊗⊕I9I8I9I7I8I9I5I7I8I9ICIC<=ℎ;<=ICConvConvIC~CellCellCellCellGPMSupervision(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)~4C64CIC⨀PoolingConcatConv(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)4C6KLLCSoftmaxGPMConcatenateDilated_rate= 7 Dilated_rate= 5 Dilated_rate= 3 Dilated_rate= 1 64(cid:17)64(cid:17)(64(cid:17)5)4CIC(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:1)(cid:11)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:1)(cid:2)(cid:2)(cid:1)(cid:2)64Figure 3: Visual comparisons in ablation studies. (a) means using RGB image only. (b) means using
light ﬁeld data (concatenation without weighting). (c) means concatenation with weighting. (d)
means ConvLSTM fusion with weighting. (e) represents (d)+GPM (i.e.  full Mo-SFM). (f) means our
whole network without the SCIM. (g) means the ﬁnal model.

spatial information for accurately identifying the salient objects. This procedure can be deﬁned as:

m + Whi ∗ Ht−1 + Wci◦Ct−1 + bi) 
m + Whf ∗ Ht−1 + Wcf◦Ct−1 + bf ) 

it = σ(Wxi ∗ (cid:101)f i
ft = σ(Wxf ∗ (cid:101)f i
Ct = ft◦Ct−1 + it◦ tanh(Wxc ∗ (cid:101)f i
ot = σ(Wxo ∗ (cid:101)f i

Ht = ot◦ tanh(Ct) 

m + Whc ∗ Ht−1 + bc) 

(3)

m + Who ∗ Ht−1 + Wco◦Ct + bo) 

where ◦ denotes the Hadamard product and σ(·) is sigmoid function. A memory cell Ct stores the
earlier information. All W∗ and b∗ are model parameters to be learned. All the gates it  ft  ot 
memory cell Ct  and hidden state Ht are 3D tensors. In this way  after 13 steps  four fused light ﬁeld
features {f2  f3  f4  f5} are effectively generated: fm = H13. The unique property of the light ﬁeld
data makes it spontaneously suitable to use ConvLSTM for feature fusion. The ConvLSTM is also
beneﬁcial for making better use of the spatial correlation between multiple focal slices thanks to its
powerful gate and memory mechanism. By now  our model enhances the average MAE performance
by nearly 14.7% points on our proposed dataset and LFSD dataset (b vs d in Tab. 1).
Furthermore  to capture global contextual information at different scales  we further extend a global
perception module (GPM) on the top of fm. The GPM can be deﬁned as:

Fm = Conv1×1(D(fm;(cid:113)r∈RS (Convd(fm; θm; r))))  m = 2  3  4  5 

(4)
where D[ · ; ··· ; · ] denotes concatenation operation. (cid:113)r∈RS (OP) means operation  OP is
performed several times using different dilation rates r in rates_set (denoted as RS) and all results
are returned. θm is parameters to be learned in m-th layer. {Fm}5
m=2 are the ﬁnal fused light ﬁeld
features in multiple layers. At the end  we add several intermediate supervisions on Fm in each layer
to facilitate network convergence and encourage explicit fusion of those light ﬁeld features.

4.3 The Memory-oriented Feature Integration Module (Mo-FIM)

Efﬁcient integration of hierarchical
deep features is signiﬁcant for pixel-
wise prediction tasks  e.g.  salient
object detection [60  5]  semantic
segmentation [34]. We propose a
new memory-oriented module  which
from a novel perspective  utilizes the
memory mechanism to effectively in-
tegrate multi-level light ﬁeld features
in a top-down manner. Speciﬁcally  as
each channel of a feature map is con-
sidered as a ‘feature detector’ [53  57] 
we design a scene context integration

Table 1: Quantitative results of the ablation analysis for our
network. The meaning of indexes has been explained in the
caption of Fig. 3.

indexes Modules

(a)
(b)
(c)
(d)
(e)
(f)
(g)

RGB
LF(w/o weighting)
LF(with weighting)
+SFM(w/o GPM)
+SFM(with GPM)
+FIM(w/o SCIM)
+FIM(with SCIM)

Ours

Fβ ↑ M AE ↓
0.144
0.643
0.074
0.805
0.069
0.819
0.821
0.062
0.059
0.825
0.054
0.838
0.843
0.052

LFSD

Fβ ↑ M AE ↓
0.194
0.607
0.121
0.781
0.116
0.789
0.797
0.105
0.099
0.807
0.092
0.814
0.819
0.089

5

HFUT(cid:1)0060DUT(cid:1)0141	0202Image(a)(b)(c)(d)(e)(f)GT(g)module (SCIM) shown in Fig. 2  which utilizes memory information from toper layers to learn a
channel attention map and updates the current light ﬁeld feature by focusing on important channels
and suppressing unnecessary ones. Then  the ConvLSTM progressively integrates the high-level
memories and the current elaborately reﬁned input. That is to say  the high-level features with
abundant semantic information are gradually summarized as memory and then being used to guide
the selection of low-level spatial details for precise saliency prediction.
More speciﬁcally  in the SCIM shown in Fig. 2  Ht−1 represents the previous scene understanding
(i.e.  hidden state of ConvLSTM in t − 1 time step) and Fm means the fused light ﬁeld feature in
mth layer. The SCIM can de deﬁned as:

(cid:101)Fm = δ(AvgP ooling(W1 ∗ Ht−1 ⊕ W2 ∗ Fm)) ⊗ Fm 

(5)
where ⊕ and ⊗ denote element-wise addition and multiplication  respectively. Then the updated

feature (cid:101)Fm is fed into a ConvLSTM cell to further summarize spatial information from the historical
memory and current input (cid:101)Fm. We use the output of Block5 as the initial state of ConvLSTM and
SCIM  i.e.  H0 = F5. After 4 steps (corresponding to (cid:101)F5  (cid:101)F4  (cid:101)F3  (cid:101)F2  respectively)  the output of the

be deﬁned as: Fm =(cid:80)5

ConvLSTM is followed by a transition convolutional layer and an up-sample operation to get the
ﬁnal saliency map S. The calculation procedure is similar to Equ. 3 by replacing the inputs.
However  the top-down structure may cause
high-level features diluted as they are trans-
mitted to the lower layers. To address this
problem  inspired by DenseNet [22]  we link
the features in low and high levels in the way
shown in Fig. 2  to alleviate gradient vanish-
ing and meanwhile encourage feature reuse.
The ﬁnal light ﬁeld features to be used can
r=m Fr  m is set to
2  3  4  5  successively. Besides  in order to
guarantee each time step of the ConvLSTM
can explicitly learn the most important in-
formation for accurately identifying salient
objects  we add intermediate supervisions
on all internal outputs of the ConvLSTM.
Generally speaking  those intermediate supervisions can act as instruction to guide the SCIM and
ConvLSTM to accurately ﬁlter the non-salient areas and retain salient areas. Intermediate results are
illustrated in Fig. 4. Full details about codes will be made publicly available.

Figure 4: Visual results of the intermediate supervi-
sions. In such a complex scene  our model can gradu-
ally optimize the saliency maps and produce a precise
prediction.

5 Experiments

5.1 Datasets

To evaluate the performance of our proposed network  we conduct experiments on our proposed
dataset and the only two public light ﬁeld saliency datasets: LFSD [29] and HFUT [59].
Ours: This dataset consists of 1462 light ﬁeld samples. We randomly select 1000 samples for training
and the remaining 462 samples for testing. More details can be found in Sec. 3.
LFSD: This dataset contains 100 light ﬁelds captured by Lytro camera. This dataset is proposed by
Li et al.  in [29]  which pioneered the use of light ﬁeld for solving challenging problems in SOD.
HFUT: HFUT consists of 255 samples captured by Lytro camera. It is a challenging dataset  with
the real-life scenarios at various distances  sensors noises  lighting conditions  and so on.
All samples in LFSD and HFUT are used for testing to evaluate the generalization abilities of saliency
models. To avoid overﬁtting  we augment the training set by ﬂipping  cropping and rotating.

5.2 Experiments Setup

Evaluation Metrics. We adopt ﬁve metrics for comprehensive evaluation  including Precision-
Recall (PR) curve  F-measure [1]  Mean Absolute Error (MAE)  S-measure [12] and E-measure [13].

6

Mo-FIMCellMo-FIMCellMo-FIMCellMo-FIMCellMo-SFMMo-SFMMo-SFMMo-SFMDecoderImageGT(a)(b)(c)(d)(e)(f)(g)!"#$(cid:3)(cid:2)(cid:1)Figure 5: Illustration of the baseline network. Using RGB or light ﬁeld data as input correspond to
(a) and (b) in Fig. 3 and Tab. 1  respectively. In term of light ﬁeld input  here  we use ‘concatenation
without weighting’ strategy to fuse light ﬁeld features from different focal slices in each Conv-Block.
For fairness  the intermediate supervisions are same as our proposed network.

Figure 6: The PR curves of our proposed method and other CNNs-based methods. Obviously  ours is
consistently outstanding over other approaches.
They are universally-agreed and standard for evaluating a SOD model and well explained in many
literatures. Due to limited space  we will not show the detailed description.
Implementation Details. Our network is implemented on Pytorch framework and trained with a
GTX 2080 Ti GPU. All training and test images are uniformly resized to 256 × 256. Our network
is trained in an end-to-end manner  in which the momentum  weight decay and learning rate are set
to 0.9  0.0005  1e-10  respectively. During the training phrase  we use softmax entropy loss  and
the network is trained by standard SGD and converges after 40 epochs with batch size of 1. The
two backbone networks of the RGB and focal stack streams are all initialized with corresponding
pre-trained VGG-19 net [46]. Other parameters are initialized with Gaussian kernels.

5.3 Ablation Studies

The Effectiveness of Light Field Data. Tab. 1 (a) and (b) show the detection results of our baseline
network illustrated in Fig. 5 with RGB data and with light ﬁeld data  respectively. Numerical results
measured by F-measure and MAE demonstrate that the network using light ﬁeld data outperforms
the one only using RGB data. Fig. 3 (a) and (b) show the visual comparisons of two aforementioned
networks  respectively. This also indicates that light ﬁeld data improve prediction performance under
challenging circumstances. Moreover  we conduct an experiment by repeating the RGB input-frame
12 times  in such a way that the model architecture is identical to the 4D version but the input data is
only 2D. The quantitative results in term of F-measure and MAE are 0.819 / 0.089 (focal slices) and
0.740 / 0.140 (RGB) respectively. This further conﬁrms the effectiveness of the focusness information
and our spatial fusion module.
The Effectiveness of Mo-SFM. To give evidence for the effectiveness of the Mo-SFM  we compare
the baseline network with it adding the Mo-SFM. Signiﬁcant improvement can be visually observed
between them shown in Fig. 3 (b) and (e). Numerically  our Mo-SFM reduces the MAE performances
by nearly 19.2% on two datasets. To conduct further investigation  we provide internal inspection on
the Mo-SFM. The gradual improvements  as we add our feature weighting mechanism  ConvLSTM
integrator and the GPM into the Mo-SFM shown in Fig. 3 (c)  (d) and (e)  are consistent with our
assertion that different contributions and spatial correlation of different focal slices are beneﬁcial to
SOD. Also  GPM is proved to be able to adaptively detect objects of different scales. Quantitative
results in Tab. 1 also numerically show the accumulative accuracy gains from the three components.
The Effectiveness of Mo-FIM. The Mo-FIM is proposed for higher cognitive processing. Fig. 3
(g) visually shows the inﬂuence of the addition of the Mo-FIM. We observe that considerable gains
(reduce the MAE by 11.8% and 10.1% shown in Tab. 1) are achieved. This result is logical since
high-level features are gradually summarized as memory and then being used to guide the selection
of low-level spatial details by using the Mo-FIM. Results in Fig. 3 show that removing the SCIM
from the Mo-FIM may lead to false positives. This suggests that the SCIM effectively updates the
original input according to memory-oriented scene understanding and may greatly bias the results.

7

Decoder(cid:7)(cid:10)(cid:11)(cid:8)(cid:9)(cid:1)(cid:2)(cid:7)(cid:10)(cid:11)(cid:8)(cid:9)(cid:1)(cid:3)(cid:7)(cid:10)(cid:11)(cid:8)(cid:9)(cid:1)(cid:4)(cid:7)(cid:10)(cid:11)(cid:8)(cid:9)(cid:1)(cid:5)(cid:7)(cid:10)(cid:11)(cid:8)(cid:9)(cid:1)(cid:6)InputOutput⊕⊕⊕⊕1×1  64  Conv2×  Up-Sample#$%#$%#$%#$%#$%Skip-connectionSkip-connectionSkip-connectionRecall00.20.40.60.81Precision0.10.20.30.40.50.60.70.80.9OursAmuletC2SCTMFDFDHSDSSMMCINLDFPAGRNPCAPDNetPiCANetR³NetTANetUCFRecall00.20.40.60.81Precision0.20.30.40.50.60.70.80.91OursAmuletC2SCTMFDFDHSDSSMMCINLDFPAGRNPCAPDNetPiCANetR³NetTANetUCFRecall00.20.40.60.81Precision0.20.30.40.50.60.70.80.91OursAmuletC2SCTMFDFDHSDSSMMCINLDFPAGRNPCAPDNetPiCANetR³NetTANetUCFRecallRecallRecallPrecisionPrecisionPrecisionOursHFUTLFSDTable 2: Quantitative comparisons on the light ﬁeld datasets. The best three results are shown in
boldface  red  and green fonts respectively. ∗ means non-deep-learning. - means no available results.

Types Methods

4D

3D

2D

Years
-
Ours
LFS∗ [29]
TPAMI’17
MCA∗ [59]
TOMM’17
WSC∗ [56]
CVPR’15
DILF∗ [58]
IJCAI’15
TIP’19
TANet [6]
PR’19
MMCI [7]
CVPR’18
PCA [5]
arXiv’18
PDNet [65]
TCyb’17
CTMF [17]
TIP’17
DF [42]
CDCP∗ [66]
ICCVW’17
ACSD∗ [25]
ICIP’15
NLPR∗ [39]
ECCV’14
PiCANet [36] CVPR’18
CVPR’18
PAGRN [62]
ECCV’18
C2S [31]
R3Net [11]
IJCAI’18
ICCV’17
Amulet [60]
ICCV’17
UCF [61]
CVPR’17
NLDF [37]
DSS [21]
CVPR’17
DHS [35]
CVPR’16
MST∗ [49]
CVPR’16
BSCA∗ [41]
CVPR’15
DSR∗ [30]
ICCV’13

Es ↑
0.923
0.728

-
-

0.805
0.861
0.853
0.857
0.864
0.881
0.838
0.795
0.629
0.768
0.892
0.878
0.874
0.833
0.882
0.850
0.862
0.827
0.872
0.785
0.811
0.799

Sα ↑
0.887
0.563

-
-

0.705
0.803
0.785
0.800
0.803
0.823
0.716
0.690
0.385
0.564
0.829
0.822
0.844
0.819
0.847
0.837
0.786
0.764
0.841
0.686
0.720
0.678

Ours

Fβ ↑ MAE↓ Es ↑
0.785
0.843
0.484
0.650
0.714

0.052
0.240

HFUT [59]
Sα ↑
0.742
0.559
0.652

-
-

0.641
0.771
0.750
0.762
0.763
0.790
0.733
0.639
0.151
0.659
0.821
0.828
0.791
0.783
0.805
0.769
0.778
0.728
0.801
0.629
0.690
0.645

-
-

0.168
0.096
0.116
0.100
0.111
0.100
0.151
0.159
0.321
0.177
0.083
0.084
0.084
0.113
0.083
0.107
0.103
0.128
0.090
0.157
0.180
0.164

-

0.701
0.761
0.748
0.757
0.758
0.747
0.701
0.696
0.665
0.706
0.762
0.758
0.762
0.697
0.737
0.729
0.761
0.759
0.720
0.693
0.693
0.695

-

0.669
0.711
0.711
0.730
0.741
0.723
0.641
0.653
0.559
0.579
0.719
0.704
0.736
0.720
0.739
0.736
0.685
0.699
0.642
0.641
0.651
0.655

0.095
0.222
0.139

-

-

Fβ ↑ MAE↓ Es ↑
0.886
0.627
0.416
0.771
0.841
0.558
0.794
0.810
0.849
0.848
0.846
0.849
0.856
0.816
0.739
0.803
0.744
0.780
0.805
0.820
0.838
0.821
0.776
0.810
0.749
0.836
0.754
0.777
0.736

0.148
0.111
0.116
0.104
0.112
0.119
0.156
0.159
0.201
0.148
0.115
0.116
0.112
0.151
0.118
0.144
0.107
0.138
0.129
0.156
0.193
0.153

0.529
0.605
0.608
0.619
0.608
0.596
0.531
0.528
0.421
0.567
0.600
0.619
0.618
0.606
0.615
0.596
0.583
0.606
0.542
0.529
0.530
0.518

LFSD [29]
Sα ↑
0.830
0.680
0.749
0.706
0.755
0.803
0.799
0.807
0.786
0.801
0.751
0.659
0.731
0.553
0.729
0.727
0.806
0.789
0.773
0.762
0.745
0.677
0.770
0.659
0.718
0.633

Fβ ↑ MAE↓
0.089
0.819
0.740
0.208
0.150
0.815
0.156
0.706
0.168
0.728
0.112
0.804
0.128
0.796
0.801
0.112
0.116
0.780
0.119
0.791
0.162
0.756
0.201
0.642
0.185
0.764
0.216
0.712
0.671
0.158
0.147
0.725
0.113
0.749
0.128
0.781
0.135
0.757
0.169
0.710
0.138
0.748
0.644
0.190
0.133
0.761
0.191
0.631
0.203
0.688
0.631
0.208

The Limitations of Our Approach. In this paper  we present a deep-learning-based light ﬁeld
saliency detection method for deeply exploring and comprehensively exploiting internal correlation
of focal slices. We demonstrate the success of our method by achieving the state-of-the-art on three
datasets. We see this work as opening two potential directions for future study. The ﬁrst is building a
big and versatile dataset for training and validating different models. We present one dataset-training
our model and testing other 2D  3D and 4D models-but one could also be bigger for improving
generalization ability of all the models training on it. The other direction is developing a more
computation-efﬁcient and memory-efﬁcient method as the focal stack is employed in the training
process. We present the ﬁrst deep-learning-based method for light ﬁeld saliency detection  but there
are other lightweight models that could potentially beneﬁt from the light ﬁeld data.

5.4 Comparisons with State-of-the-arts

We compare results from our method and other 25 2D  3D and 4D ones  containing both deep-learning-
based methods and non-deep learning ones(remarked with ∗). There are 4 4D light ﬁeld methods:
LFS∗ [29]  MCA∗ [59]  WSC∗ [56]  DILF∗ [58]; 9 3D RGB-D methods: TANet [6]  MMCI [7] 
PCA [5]  PDNet [65]  CTMF [17]  DF [42]  CDCP∗ [66]  ACSD∗ [25]  NLPR∗ [39]; and 12 top-
ranking RGB methods: PiCANet [36]  PAGR [62]  C2S [31]  R3Net [11]  Amulet [60]  UCF [61] 
NLDF [37]  DSS [21]  DHS [35]  MST∗ [49]  BSCA∗ [41]  DSR∗ [30]. For fair comparisons  the
results from competing methods are generated by authorized codes or directly provided by authors.
Quantitative Evaluation. Quantitative results are shown in Tab. 2. The proposed model consistently
achieves the highest scores on all datasets across four evaluation metrics. An important observation
should be noted: compared to the latest CNNs-based RGB SOD methods with large-quantity training
sets  our method also achieves signiﬁcant advantages with a relatively small training set. This
indicates that light ﬁeld data are signiﬁcant and promising. Fig. 6 shows that the PR curves of our
method outperform those top-ranking approaches.
Qualitative Evaluation. Fig. 7 shows some selected representative samples of results comparing our
method with those of the current state-of-the-art methods. Our method is able to handle a wide rage
of challenging scenes  including shown in Fig. 7  small objects (1st row)  similar foreground and
background (2nd  4th and 9th rows)  clutter background (3rd-5th and 8th rows)  and other difﬁcult
scenes (6th and 7th rows). In those complex cases  we can see that our predicted results can be
positively inﬂuenced by the light ﬁeld data and our proposed network where the light ﬁeld features
from different focal slices are effectively fused and the multi-level global semantic information and
local detail cues are sufﬁciently integrated.

8

Figure 7: Visual comparisons of our method with top-ranking CNNs-based methods in some chal-
lenging cases. Obviously  our model can generate precise salient results even in those complex scenes 
which indicates that our method takes full advantages of light ﬁelds for accurate saliency prediction.
6 Conclusion

In this paper  we develop a novel memory-oriented decoder tailored for light ﬁeld saliency detection.
Our Mo-SFM resembles the memory mechanism of how human fuse information and effectively
excavates the various contributions and spatial correlations of different focal slices. The Mo-FIM
also sufﬁciently integrates multi-level features by leveraging high-level memory to guide low-level
selection. Additionally  we introduce a large-scale light ﬁeld saliency dataset to pave the way for
future studies. Experiments show that our method achieves superior performance over 25 methods
including 2D  3D and 4D ones  especially in complex scenarios.

Acknowledgements

This work was supported by the National Natural Science Foundation of China (61605022 and
61976035) and the Fundamental Research Funds for the Central Universities (DUT19JC58). The
authors are grateful to the reviewers for their suggestions in improving the quality of the paper.

References
[1] R. Achanta  S. S. Hemami  F. J. Estrada  and S. Süsstrunk. Frequency-tuned salient region detection. In

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 1597–1604  2009.

[2] A. Borji  M. Cheng  H. Jiang  and J. Li. Salient object detection: A survey. arXiv preprint arXiv:1411.5878 

2014.

[3] A. Borji  M.-M. Cheng  H. Jiang  and J. Li. Salient object detection: A benchmark. IEEE Transactions on

Image Processing (TIP)  24(12):5706–5722  2015.

[4] A. Borji  D. N. Sihite  and L. Itti. Salient object detection: a benchmark. In European Conference on

Computer Vision (ECCV)  pages 414–429  2012.

[5] H. Chen and Y. Li. Progressively complementarity-aware fusion network for rgb-d salient object detection.

In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3051–3060  2018.

[6] H. Chen and Y. Li. Three-stream attention-aware network for rgb-d salient object detection.

IEEE

Transactions on Image Processing (TIP)  28(6):2825–2835  2019.

[7] H. Chen  Y. Li  and D. Su. Multi-modal fusion network with multi-scale multi-path and cross-modal

interactions for rgb-d salient object detection. Pattern Recognition  86:376–385  2019.

[8] M.-M. Cheng  G.-X. Zhang  N. J. Mitra  X. Huang  and S.-M. Hu. Global contrast based salient region
detection. In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  volume 37  pages
409–416  2011.

[9] C. Craye  D. Filliat  and J.-F. Goudou. Environment exploration for object-based visual saliency learning.

In IEEE International Conference on Robotics and Automation (ICRA)  pages 2303–2309  2016.

[10] J. Dai  Y. Li  K. He  and J. Sun. R-fcn: object detection via region-based fully convolutional networks.

International Conference on Neural Information Processing Systems (NIPS)  pages 379–387  2016.

[11] Z. Deng  X. Hu  L. Zhu  X. Xu  J. Qin  G. Han  and P.-A. Heng. R3net: Recurrent residual reﬁnement
network for saliency detection. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages
684–690  2018.

9

OursTANetPCAMMCIPAGRNR3NetC2SImageGTPiCANet0070 	0082 0090 	0126 0148 0160 0202 0272 0432 0881 1563 1604LFSD:	0012 0024PDNetAmuletDSSUCFCTMFNLDF[12] D.-P. Fan  M.-M. Cheng  Y. Liu  T. Li  and A. Borji. Structure-measure: A new way to evaluate foreground

maps. In International Conference on Computer Vision (ICCV)  pages 4558–4567  2017.

[13] D.-P. Fan  C. Gong  Y. Cao  B. Ren  M.-M. Cheng  and A. Borji. Enhanced-alignment measure for binary
foreground map evaluation. In International Joint Conference on Artiﬁcial Intelligence (IJCAI)  pages
698–704  2018.

[14] D. Feng  N. Barnes  S. You  and C. McCarthy. Local background enclosure for rgb-d salient object
detection. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2343–2350  2016.
[15] D. Gao  V. Mahadevan  and N. Vasconcelos. The discriminant centersurround hypothesis for bottom-up

saliency. In International Conference on Neural Information Processing Systems (NIPS)  2007.

[16] X. Guo  Z. Chen  S. Li  Y. Yang  and J. Yu. Deep depth inference using binocular and monocular cues.

arXiv preprint arXiv:1711.10729  2017.

[17] J. Han  H. Chen  N. Liu  C. Yan  and X. Li. Cnns-based rgb-d saliency detection via cross-view transfer

and multiview fusion. IEEE Transactions on Systems  Man  and Cybernetics  48(11):3171–3183  2018.
[18] J. Harel  C. Koch  and P. Perona. Graph-based visual saliency. In International Conference on Neural

Information Processing Systems (NIPS)  pages 545–552  2006.

[19] B. Hariharan  P. A. Arbeláez  R. B. Girshick  and J. Malik. Hypercolumns for object segmentation and
ﬁne-grained localization. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages
447–456  2015.

[20] S. Hong  T. You  S. Kwak  and B. Han. Online tracking by learning discriminative saliency map with
convolutional neural network. International Conference on Machine Learning (ICML)  pages 597–606 
2015.

[21] Q. Hou  M.-M. Cheng  X. Hu  A. Borji  Z. Tu  and P. H. S. Torr. Deeply supervised salient object detection
with short connections. In Conference on Computer Vision and Pattern Recognition (CVPR)  volume 41 
pages 815–828  2017.

[22] G. Huang  Z. Liu  L. van der Maaten  and K. Q. Weinberger. Densely connected convolutional networks.

In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2261–2269  2017.

[23] L. Itti  C. Koch  and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE

Transactions on Pattern Analysis and Machine Intelligence (TPAMI)  20(11):1254–1259  1998.

[24] P. Jiang  H. Ling  J. Yu  and J. Peng. Salient region detection by ufo: Uniqueness  focusness and objectness.

In International Conference on Computer Vision (ICCV)  pages 1976–1983  2013.

[25] R. Ju  L. Ge  W. Geng  T. Ren  and G. Wu. Depth saliency based on anisotropic center-surround difference.

In International Conference on Image Processing (ICIP)  pages 1115–1119  2014.

[26] G. Lee  Y.-W. Tai  and J. Kim. Deep saliency with encoded low level distance map and high level features.

In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 660–668  2016.

[27] G. Li and Y. Yu. Visual saliency based on multiscale deep features. In Conference on Computer Vision and

Pattern Recognition (CVPR)  pages 5455–5463  2015.

[28] N. Li  J. Ye  Y. Ji  H. Ling  and J. Yu. Saliency detection on light ﬁeld. In Conference on Computer Vision

and Pattern Recognition (CVPR)  pages 2806–2813  2014.

[29] N. Li  J. Ye  Y. Ji  H. Ling  and J. Yu. Saliency detection on light ﬁeld. IEEE Transactions on Pattern

Analysis and Machine Intelligence (TPAMI)  39(8):1605–1616  2017.

[30] X. Li  H. Lu  L. Zhang  X. Ruan  and M.-H. Yang. Saliency detection via dense and sparse reconstruction.

In International Conference on Computer Vision (ICCV)  pages 2976–2983  2013.

[31] X. Li  F. Yang  H. Cheng  W. Liu  and D. Shen. Contour knowledge transfer for salient object detection. In

European Conference on Computer Vision (ECCV)  pages 370–385  2018.

[32] X. Li  L. Zhao  L. Wei  M.-H. Yang  F. Wu  Y. Zhuang  H. Ling  and J. Wang. Deepsaliency: Multi-task
deep neural network model for salient object detection. IEEE Transactions on Image Processing (TIP) 
25(8):3919–3930  2016.

[33] Y. Li  X. Hou  C. Koch  J. M. Rehg  and A. L. Yuille. The secrets of salient object segmentation. In

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 280–287  2014.

[34] G. Lin  A. Milan  C. Shen  and I. D. Reid. Reﬁnenet: Multi-path reﬁnement networks for high-resolution
In Conference on Computer Vision and Pattern Recognition (CVPR)  pages

semantic segmentation.
5168–5177  2017.

[35] N. Liu and J. Han. Dhsnet: Deep hierarchical saliency network for salient object detection. In Conference

on Computer Vision and Pattern Recognition (CVPR)  pages 678–686  2016.

[36] N. Liu  J. Han  and M.-H. Yang. Picanet: Learning pixel-wise contextual attention for saliency detection.

In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3089–3098  2018.

[37] Z. Luo  A. K. Mishra  A. Achkar  J. A. Eichel  S. Li  and P.-M. Jodoin. Non-local deep features for salient
object detection. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 6593–6601 
2017.

[38] Y. Niu  Y. Geng  X. Li  and F. Liu. Leveraging stereopsis for saliency analysis. In Conference on Computer

Vision and Pattern Recognition (CVPR)  pages 454–461  2012.

[39] H. Peng  B. Li  W. Xiong  W. Hu  and R. Ji. Rgbd salient object detection: A benchmark and algorithms.

In European Conference on Computer Vision (ECCV)  pages 92–109  2014.

[40] F. Perazzi  P. Krähenbühl  Y. Pritch  and A. Hornung. Saliency ﬁlters: Contrast based ﬁltering for salient
region detection. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 733–740 
2012.

10

[41] Y. Qin  H. Lu  Y. Xu  and H. Wang. Saliency detection via cellular automata. In Conference on Computer

Vision and Pattern Recognition (CVPR)  pages 110–119  2015.

[42] L. Qu  S. He  J. Zhang  J. Tian  Y. Tang  and Q. Yang. Rgbd salient object detection via deep fusion. IEEE

Transactions on Image Processing (TIP)  26(5):2274–2285  2017.

[43] S. Ren  K. He  R. B. Girshick  and J. Sun. Faster r-cnn: towards real-time object detection with region
proposal networks. In International Conference on Neural Information Processing Systems (NIPS)  volume
2015  pages 91–99  2015.

[44] L. Shao and M. Brady. Speciﬁc object retrieval based on salient regions. Pattern Recognition  39(10):1932–

1948  2006.

[45] X. Shi  Z. Chen  H. Wang  D. Y. Yeung  W. K. Wong  and W. Woo. Convolutional lstm network: a
machine learning approach for precipitation nowcasting. International Conference on Neural Information
Processing Systems (NIPS)  pages 802–810  2015.

[46] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

International Conference on Learning Representations (ICLR)  2015.

[47] A. W. M. Smeulders  D. M. Chu  R. Cucchiara  S. Calderara  A. Dehghan  and M. Shah. Visual tracking:
An experimental survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 
36(7):1442–1468  2014.

[48] G. Song and K. M. Lee. Depth estimation network for dual defocused images with different depth-of-ﬁeld.

In International Conference on Image Processing (ICIP)  pages 1563–1567  2018.

[49] W.-C. Tu  S. He  Q. Yang  and S.-Y. Chien. Real-time salient object detection with a minimum spanning

tree. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2334–2342  2016.

[50] L. Wang  H. Lu  X. Ruan  and M.-H. Yang. Deep networks for saliency detection via local estimation and
global search. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3183–3192 
2015.

[51] T.-C. Wang  J.-Y. Zhu  E. Hiroaki  M. Chandraker  A. A. Efros  and R. Ramamoorthi. A 4d light-ﬁeld
dataset and cnn architectures for material recognition. European Conference on Computer Vision (ECCV) 
pages 121–138  2016.

[52] W. Wang  Q. Lai  H. Fu  J. Shen  and H. Ling. Salient object detection in the deep learning era: An in-depth

survey. arXiv preprint arXiv:1904.09146  2019.

[53] S. Woo  J. Park  J.-Y. Lee  and I. S. Kweon. Cbam: Convolutional block attention module. European

Conference on Computer Vision (ECCV)  pages 3–19  2018.

[54] S. Xie and Z. Tu. Holistically-nested edge detection. International Journal of Computer Vision (IJCV) 

125(1-3):3–18  2015.

[55] H. W. F. Yeung  J. Hou  X. Chen  J. Chen  Z. Chen  and Y. Y. Chung. Light ﬁeld spatial super-resolution
using deep efﬁcient spatial-angular separable convolution. IEEE Transactions on Image Processing (TIP) 
28(5):2319–2330  2019.

[56] N. yi Li  B. Sun  and J. Yu. A weighted sparse coding framework for saliency detection. In Conference on

Computer Vision and Pattern Recognition (CVPR)  pages 5216–5223  2015.

[57] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. European Conference

on Computer Vision (ECCV)  pages 818–833  2014.

[58] J. Zhang  M. Wang  J. Gao  Y. Wang  X. Zhang  and X. Wu. Saliency detection with a deeper investigation

of light ﬁeld. In International Conference on Artiﬁcial Intelligence (IJCAI)  pages 2212–2218  2015.

[59] J. Zhang  M. Wang  L. Lin  X. Yang  J. Gao  and Y. Rui. Saliency detection on light ﬁeld: A multi-cue
approach. ACM Transactions on Multimedia Computing  Communications  and Applications (TOMM) 
13(3):32  2017.

[60] P. Zhang  D. Wang  H. Lu  H. Wang  and X. Ruan. Amulet: Aggregating multi-level convolutional features
for salient object detection. In International Conference on Computer Vision (ICCV)  pages 202–211 
2017.

[61] P. Zhang  D. Wang  H. Lu  H. Wang  and B. Yin. Learning uncertain convolutional features for accurate

saliency detection. In International Conference on Computer Vision (ICCV)  pages 212–221  2017.

[62] X. Zhang  T. Wang  J. Qi  H. Lu  and G. Wang. Progressive attention guided recurrent network for salient
object detection. In Conference on Computer Vision and Pattern Recognition (CVPR)  pages 714–722 
2018.

[63] R. Zhao  W. Ouyang  H. Li  and X. Wang. Saliency detection by multi-context deep learning. In Conference

on Computer Vision and Pattern Recognition (CVPR)  pages 1265–1274  2015.

[64] W. Zhou  L. Liang  H. Zhang  A. Lumsdaine  and L. Lin. Scale and orientation aware epi-patch learning for
light ﬁeld depth estimation. In International Conference on Pattern Recognition (ICPR)  pages 2362–2367 
2018.

[65] C. Zhu  X. Cai  K. Huang  T. H. Li  and G. Li. Pdnet: Prior-model guided depth-enhanced network for

salient object detection. arXiv preprint arXiv:1803.08636  2018.

[66] C. Zhu  G. Li  W. Wang  and R. Wang. An innovative salient object detection using center-dark channel

prior. In International Conference on Computer Vision Workshops (ICCVW)  pages 1509–1515  2017.

[67] H. Zhu  M. Guo  H. Li  Q. Wang  and A. Robles-Kelly. Breaking the spatio-angular trade-off for light ﬁeld

super-resolution via lstm modelling on epipolar plane images. arXiv preprint arXiv:1902.05672  2019.

[68] W. Zhu  S. Liang  Y. Wei  and J. Sun. Saliency optimization from robust background detection. In

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2814–2821  2014.

11

,Jinzhuo Wang
Wenmin Wang
xiongtao Chen
Ronggang Wang
Wen Gao
Miao Zhang
Jingjing Li
JI WEI
Yongri Piao
Huchuan Lu