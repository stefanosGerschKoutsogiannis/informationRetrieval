2013,Polar Operators for Structured Sparse Estimation,Structured sparse estimation has become an important technique in many areas of data analysis. Unfortunately  these estimators normally create computational difficulties that entail sophisticated algorithms. Our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently. With such an operator  a simple conditional gradient method can then be developed that  when combined with smoothing and local optimization  significantly reduces training time vs. the state of the art. We also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.,Polar Operators for Structured Sparse Estimation

Xinhua Zhang

Machine Learning Research Group
National ICT Australia and ANU
xinhua.zhang@anu.edu.au

Yaoliang Yu and Dale Schuurmans

Department of Computing Science  University of Alberta

Edmonton  Alberta T6G 2E8  Canada

{yaoliang dale}@cs.ualberta.ca

Abstract

Structured sparse estimation has become an important technique in many areas of
data analysis. Unfortunately  these estimators normally create computational dif-
ﬁculties that entail sophisticated algorithms. Our ﬁrst contribution is to uncover a
rich class of structured sparse regularizers whose polar operator can be evaluated
efﬁciently. With such an operator  a simple conditional gradient method can then
be developed that  when combined with smoothing and local optimization  signif-
icantly reduces training time vs. the state of the art. We also demonstrate a new
reduction of polar to proximal maps that enables more efﬁcient latent fused lasso.

Introduction

1
Sparsity is an important concept in high-dimensional statistics [1] and signal processing [2] that has
led to important application successes by reducing model complexity and improving interpretability
of the results. Standard computational strategies such as greedy feature selection [3] and generic
convex optimization [4–7] can be used to implement simple sparse estimators. However  sophis-
ticated notions of structured sparsity have been recently developed that can encode combinatorial
patterns over variable subsets [8]. Although combinatorial structure greatly enhances modeling ca-
pability  it also creates computational challenges that require sophisticated optimization approaches.
For example  current structured sparse estimators often adopt an accelerated proximal gradient
(APG) strategy [9  10]  which has a low per-step complexity and enjoys an optimal convergence
rate among black-box ﬁrst-order procedures [10]. Unfortunately  APG must also compute a proxi-
mal update (PU) of the nonsmooth regularizer during each iteration. Not only does the PU require a
highly nontrivial computation for structured regularizers [4]—e.g.  requiring tailored network ﬂow
algorithms in existing cases [5  11  12]—it yields dense intermediate iterates. Recently  [6] has
demonstrated a class of regularizers where the corresponding PUs can be computed by a sequence
of submodular function minimizations  but such an approach remains expensive.
Instead  in this paper  we demonstrate that an alternative approach can be more effective for many
structured regularizers. We base our development on the generalized conditional gradient (GCG)
algorithm [13  14]  which also demonstrates promise for sparse model optimization. Although GCG
possesses a slower convergence rate than APG  it demonstrates competitive performance if its up-
dates are interleaved with local optimization [14–16]. Moreover  GCG produces sparse intermediate
iterates  which allows additional sparsity control. Importantly  unlike APG  GCG requires comput-
ing the polar of the regularizer  instead of the PU  in each step. This difference allows important
new approaches for characterizing and evaluating structured sparse regularizers.
Our ﬁrst main contribution is to characterize a rich class of structured sparse regularizers that allow
efﬁcient computation of their polar operator. In particular  motivated by [6]  we consider a family
of structured sparse regularizers induced by a cost function on variable subsets. By introducing a
“lifting” construction  we show how these regularizers can be expressed as linear functions  which
after some reformulation  allows efﬁcient evaluation by a simple linear program (LP). Important
examples covered include overlapping group lasso [5] and path regularization in directed acyclic
graphs [12]. By exploiting additional structure in these cases  the LP can be reduced to a piecewise

1

linear objective over a simple domain  allowing further reduction in computation time via smoothing
√
[17]. For example  for the overlapping group lasso with n groups where each variable belongs to at
most r groups  the cost of evaluating the polar operator can be reduced from O(rn3) to O(rn
n/)
for a desired accuracy of . Encouraged by the superior performance of GCG in these cases  we
then provide a simple reduction of the polar operator to the PU. This reduction makes it possible to
extend GCG to cases where the PU is easy to compute. To illustrate the usefulness of this reduction
we provide an efﬁcient new algorithm for solving the fused latent lasso [18].

2 Structured Sparse Models

Consider the standard regularized risk minimization framework

min
w∈Rn

f (w) + λ Ω(w) 

(1)

where f is the empirical risk  assumed to be convex with a Lipschitz continuous gradient  and Ω is
a convex  positively homogeneous regularizer  i.e. a gauge [19  §4]. Let 2[n] denote the power set of
[n] := {1  . . .   n}  and let R+ := R+ ∪ {∞}. Recently  [6] has established a principled method for
deriving regularizers from a subset cost function F : 2[n] → R+ based on deﬁning the gauge:

ΩF (w) = inf{γ ≥ 0 : w∈ γ conv(SF )}  where SF =(cid:8)wA : (cid:107)wA(cid:107)p

˜p = 1/F (A) ∅ (cid:54)= A ⊆ [n](cid:9). (2)

p = 1  (cid:107) · (cid:107)p
Here γ is a scalar  conv(SF ) denotes the convex hull of the set SF   ˜p  p ≥ 1 with 1
throughout is the usual (cid:96)p-norm  and wA denotes a duplicate of w with all coordinates not in A set
to 0. Note that we have tacitly assumed F (A) = 0 iff A = ∅ in (2). The gauge ΩF deﬁned in (2)
is also known as the atomic norm with the set of atoms SF [20]. It will be useful to recall that the
polar of a gauge Ω is deﬁned by [19  §15]:

Ω◦(g) := supw{(cid:104)g  w(cid:105) : Ω(w) ≤ 1}.

(3)
In particular  the polar of a norm is its dual norm. (Recall that any norm is also a gauge.) For the
speciﬁc gauge ΩF deﬁned in (2)  its polar is simply the support function of SF [19  Theorem 13.2]:
(4)

(cid:107)gA(cid:107)p /[F (A)]1/p.

˜p + 1

(cid:104)g  w(cid:105) = max
∅(cid:54)=A⊆[n]

Ω◦
F (g) = max
w∈SF

(The ﬁrst equality uses the deﬁnition of support function  and the second follows from (2).) By vary-
ing ˜p and F   one can generate a class of sparsity inducing regularizers that includes most current
proposals [6]. For instance  if F (A) = 1 whenever |A| (the cardinality of A) is 1  and F (A) = ∞
for |A| > 1  then Ω◦
F is the (cid:96)∞ norm and ΩF is the usual (cid:96)1 norm. More importantly  one can encode
structural information through the cost function F   which selects and establishes preferences over
the set of atoms SF . As pointed out in [6]  when F is submodular  (4) can be evaluated by a se-
cant method with submodular minimizations ([21  §8.4]  see also Appendix B). However  as we will
show  it is possible to do signiﬁcantly better by completely avoiding submodular optimization. Be-
fore presenting our main results  we ﬁrst review the state of the art for solving (1)  and demonstrate
how the performance of current methods can hinge on efﬁcient computation of (4).

2.1 Optimization Algorithms

(cid:107)w − wk(cid:107)2

A standard approach for minimizing (1) is the accelerated proximal gradient (APG) algorithm [9 
10]  where each iteration involves solving the proximal update (PU): wk+1 = arg minw (cid:104)dk  w(cid:105) +
2 + λΩF (w)  for some step size sk and descent direction dk. Although it can be
) iterations [9  10]  each update can be quite

1
2sk
shown that APG ﬁnds an  accurate solution in O(1/
difﬁcult to compute when ΩF encodes combinatorial structure  as noted in the introduction.
An alternative approach to solving (1) is the generalized conditional gradient (GCG) method [13 
14]  which has recently received renewed attention. Unlike APG  GCG only requires the polar
operator of the regularizer ΩF to be computed in each iteration  given by the argument of (4):
P◦
F (g) = arg max
w∈SF

(cid:104)gC  w(cid:105) for C = arg max
∅(cid:54)=A⊆[n]

(cid:104)g  w(cid:105) = F (C)

p /F (A). (5)

−1
p arg max
w:(cid:107)w(cid:107) ˜p=1

(cid:107)gA(cid:107)p

√

Algorithm 1 outlines a GCG procedure for solving (1) that only requires the evaluation of P◦
F in
each iteration without needing the full PU to be computed. The algorithm is quite simple: Line 3

2

Algorithm 1 Generalized conditional gradient (GCG) for optimizing (1).
1: Initialize w0 ← 0  s0 ← 0  (cid:96)0 ← 0.
2: for k = 0  1  . . . do
3:
4:
5:

Polar operator: vk ←P◦
2-D Conic search: (α  β) := arg minα≥0 β≥0 f (αwk + βvk) + λ(αsk + β).
Local re-optimization: {ui}k
i F (Ai)

} f ((cid:80)
i ui  (cid:96)i ← ui for i ≤ k  sk+1 ←(cid:80)

6: wk+1 ←(cid:80)

i ui) + λ(cid:80)

1 := arg min{ui=ui
Ai

p(cid:107)ui(cid:107) ˜p.

i F (Ai)

1

F (gk)  Ak ← C(gk)  where gk =−∇f (wk) and C is deﬁned in (5).

where the {ui} are initialized by ui = α(cid:96)i for i < k and ui = βvi for i = k.

1

p(cid:107)ui(cid:107) ˜p

7: end for

evaluates the polar operator  which provides a descent direction vk; Line 4 ﬁnds the optimal step
sizes for combining the current iterate wk with the direction vk; and Line 5 locally improves the
objective (1) by maintaining the same support patterns but re-optimizing the parameters. It has been
shown that GCG can ﬁnd an  accurate solution to (1) in O(1/) steps  provided only that the polar
(5) is computed to  accuracy [14]. Although GCG has a slower theoretical convergence rate than
APG  the introduction of local optimization (Line 5) often yields faster convergence in practice [14–
16]. Importantly  Line 5 does not increase the sparsity of the intermediate iterates. Our main goal
in this paper therefore is to extend this GCG approach to structured sparse models by developing
efﬁcient algorithms for computing the polar operator for the structured regularizers deﬁned in (2).

3 Polar Operators for Atomic Norms

Let 1 denote the vector of all 1s with length determined by context. Our ﬁrst main contribution is
to develop a general class of atomic norm regularizers whose polar operator (5) can be computed
efﬁciently. To begin  consider the case of a (partially) linear function F where there exists a c ∈
Rn such that F (A) = (cid:104)c  1A(cid:105) for all A ∈ dom F (note that the domain need not be a lattice).
A few useful regularizers can be generated by linear functions: for example  the (cid:96)1 norm can be
derived from F (A) = (cid:104)1  1A(cid:105) for |A| = 1  which is linear. Unfortunately  linearity is too restrictive
to capture most structured regularizers of interest  therefore we will need to expand the space of
functions F we consider. To do so  we introduce the more general class of marginalized linear
functions: we say that F is marginalized linear if there exists a nonnegative linear function M on an
extended domain 2[n+l] such that its marginalization to 2[n] is exactly F :
∀ A ⊆ [n].

F (A) =

M (B) 

min

(6)

B:A⊆B⊆[n+l]

Essentially  such a function F is “lifted” to a larger domain where it becomes linear. The key
question is whether the polar Ω◦
To develop an efﬁcient procedure for computing the polar Ω◦
computing the polar Ω◦
can be expressed as M (B) = (cid:104)b  1B(cid:105) for B ∈ dom M ⊆ 2[n+l] (b ∈ Rn+l
domain of M need not be the whole space in general  we make use of the specialized polytope:

F   ﬁrst consider the simpler case of
M for a nonnegative linear function M. Note that by linearity the function M
+ ). Since the effective

F can be efﬁciently evaluated for such functions.

P := conv{1B : B ∈ dom M} ⊆ [0  1]n+l.

(7)
Note P may have exponentially many faces. From the deﬁnition (4) one can then re-express the
polar Ω◦
M as:
Ω◦
M (g) =

where ˜gi = |gi|p ∀i 

(cid:107)gB(cid:107)p /M (B)1/p =

(cid:19)1/p

(cid:18)

max

(8)

(cid:104)˜g  w(cid:105)
(cid:104)b  w(cid:105)

max
0(cid:54)=w∈P

∅(cid:54)=B∈dom M

where we have used the fact that the linear-fractional objective must attain its maximum at vertices of
P ; that is  at 1B for some B ∈ dom M. Although the linear-fractional program (8) can be reduced to
a sequence of LPs using the classical method of [22]  a single LP sufﬁces for our purposes. Indeed 
let us ﬁrst remove the constraint w (cid:54)= 0 by considering the alternative polytope:

Q := P ∩ {w ∈ Rn+l : (cid:104)1  w(cid:105) ≥ 1}.

(9)
As shown in Appendix A  all vertices of Q are scalar multiples of the nonzero vertices of P . Since
the objective in (8) is scale invariant  we can restrict the constraints to w ∈ Q. Then  by applying
transformations ˜w = w/(cid:104)b  w(cid:105)  σ = 1/(cid:104)b  w(cid:105)  problem (8) can be equivalently re-expressed by:
(10)

(cid:104)˜g  ˜w(cid:105)   subject to ˜w ∈ σQ  (cid:104)b  ˜w(cid:105) = 1.

max
˜w σ>0

3

(cid:107)gA(cid:107)p
p
F (A)

Of course  whether this LP can be solved efﬁciently depends on the structure of Q (and of P indeed).
Finally  we note that the same formulation allows the polar to be efﬁciently computed for a marginal-
ized linear function F via a simple reduction: Consider any g ∈ Rn and let [g; 0] ∈ Rn+l denote g
padded by l zeros. Then Ω◦

F (g) = Ω◦

M ([g; 0]) for all g ∈ Rn because
(cid:107)gA(cid:107)p
(cid:107)gA(cid:107)p
p
M (B)

= max
∅(cid:54)=A⊆B

p

(cid:107)[g; 0]B(cid:107)p
M (B)

p

= max

∅(cid:54)=A⊆[n]

minB:A⊆B⊆[n+l] M (B)

. (11)
max
∅(cid:54)=A⊆[n]
To see the last equality  ﬁxing B the optimal A is attained at A = B ∩ [n]. If B ∩ [n] is empty  then
(cid:107)[g; 0]B(cid:107) = 0 and the corresponding B cannot be the maximizer of the last term  unless Ω◦
F (g) = 0
in which case it is easy to see Ω◦
Although we have kept our development general so far  the idea is clear: once an appropriate “lifting”
has been found so that the polytope Q in (9) can be compactly represented  the polar (5) can be
reformulated as the LP (10)  for which efﬁcient implementations can be sought. We now demonstrate
this new methodology for the two important structured regularizers: group sparsity and path coding.

= max
B:∅(cid:54)=B⊆[n+l]

M ([g; 0]) = 0.

3.1 Group Sparsity
For a general formulation of group sparsity  let G ⊆ 2[n] be a set of variable groups (subsets) that
possibly overlap [3  6  7]. Here we use i ∈ [n] to index variables and G ∈ G to index groups.
Consider the cost function over variable groups Fg : 2[n] → R+ deﬁned by:

Fg(A) =

cG I(A ∩ G (cid:54)= ∅) 

(12)

(cid:88)

G∈G

where cG is a nonnegative cost and I is an indicator such that I(·) = 1 if its argument is true  and
0 otherwise. The value Fg(A) provides a weighted count of how many groups overlap with A.
Unfortunately  Fg is not linear  so we need to re-express it to recover an efﬁcient polar operator. To
do so  augment the domain by adding l = |G| variables such that each new variable G corresponds
to a group G. Then deﬁne a weight vector b ∈ Rn+l
+ such that bi = 0 for i ≤ n and bG = cG for
n < G ≤ n + l. Finally  consider the linear cost function Mg : 2[n+l] → R+ deﬁned by:
Mg(B) = (cid:104)b  1B(cid:105) if i ∈ B ⇒ G ∈ B  ∀ i ∈ G ∈ G; Mg(B) = ∞ otherwise.

(13)
The constraint ensures that if a variable i ≤ n appears in the set B  then every variable G corre-
sponding to a group G that contains i must also appear in B. By construction  Mg is a nonnegative
linear function. It is also easy to verify that Fg satisﬁes (6) with respect to Mg.
To compute the corresponding polar  observe that the effective domain of Mg is a lattice  hence (4)
can be solved by combinatorial methods. However  we can do better by exploiting problem structure
in the LP. For example  observe that the polytope (7) can now be compactly represented as:

Pg = {w ∈ Rn+l : 0 ≤ w ≤ 1  wi ≤ wG ∀ i ∈ G ∈ G}.

(14)
Indeed  it is easy to verify that the integral vectors in Pg are precisely {1B : B ∈ dom Mg}.
Moreover  the linear constraint in (14) is totally unimodular (TUM) since it is the incidence matrix
of a bipartite graph (variables and groups)  hence Pg is the convex hull of its integral vectors [23].
Using the fact that the scalar σ in (10) admits a closed form solution σ = (cid:104)1  ˜w(cid:105) in this case  the LP
(10) can be reduced to:

max

˜w

˜gi min

G:i∈G∈G ˜wG  subject to ˜w ≥ 0 

bG ˜wG = 1.

(15)

(cid:88)

G∈G

(cid:88)

i∈[n]

Note only { ˜wG} appear in the problem as implicitly ˜wi = minG:i∈G ˜wG  ∀ i ∈ [n]. This is now
just a piecewise linear objective over a (reweighted) simplex. Since projecting to a simplex can
be performed in linear time  the smoothing method of [17] can be used to obtain a very efﬁcient
implementation. We illustrate a particular case where each variable i ∈ [n] belongs to at most r > 1
groups. (Appendix D considers when the groups form a directed acyclic graph.)

G:i∈G r−n˜gi ˜wG/ satisﬁes: (i) the gradient of h is(cid:0) n

Proposition 1 Let h( ˜w) denote the negated objective of (15). Then for any  > 0  h( ˜w) :=
(ii) h( ˜w) − h( ˜w) ∈ (−  0] for all ˜w  and (iii) the gradient of h can be computed in O(nr) time.

 (cid:107)˜g(cid:107)2∞ log r(cid:1)-Lipschitz 

(cid:80)
i∈[n] log(cid:80)

n log r



4

(The proof is given in Appendix C.) With this construction  APG can be run on h to achieve a 2
accurate solution to (15) within O( 1
n log r).

Note that this is signiﬁcantly cheaper than the O(n2(l + n)r) worst case complexity of [11  Al-
gorithm 2]. More importantly  we gain explicit control of the trade-off between accuracy  and
computational cost. A detailed comparison to related approaches is given in Appendix B.1 and E.

n log r) steps [17]  using a total time cost of O( nr


√

√

3.2 Path Coding
Another interesting regularizer  recently investigated by [12]  is determined by path costs in a di-
rected acyclic graph (DAG) deﬁned over the set of variables i ∈ [n]. For convenience  we add two
nodes  a source s and a sink t  with dummy edges (s  i) and (i  t) for all i ∈ [n]. An (s  t)-path (or
simply path) is then given by a sequence (s  i1)  (i1  i2)  . . .   (ik−1  ik)  (ik  t) with k ≥ 1. A non-
negative cost is associated with each edge including (s  i) and (i  t)  so the cost of a path is the sum
of its edge costs. A regularizer can then be deﬁned by (2) applied to the cost function Fp : 2[n]→R+
(16)

(cid:26)cost of the path if the nodes in A form an (s  t)-path (unique for DAG)

Fp(A) =

.

if such a path does not exist

∞

Note Fp is not submodular. Although Fp is not linear  a similar “lifting” construction can be used to
show that it is marginalized linear  hence it supports efﬁcient computation of the polar. To explain
the construction  let V := [n] ∪ {s  t} be the node set including s and t  E be the edge set including
(s  i) and (i  t)  T = V ∪ E  and let b ∈ R|T|
+ be the concatenation of zeros for node costs and the
given edge costs. Let m := |E| be the number of edges. It is then easy to verify that Fp satisﬁes (6)
with respect to the linear cost function Mp : 2T → R+ deﬁned by:
(17)
To efﬁciently compute the resulting polar  we consider the form (8) using ˜gi = |gi|p ∀i as before:
wki  ∀i ∈ [n]. (18)
Ω◦
Mp(g) =
Here the constraints form the well-known ﬂow polytope whose vertices are exactly all the paths in a
DAG. Similar to (15)  the normalized LP (10) can be simpliﬁed by solving for the scalar σ to obtain:
˜wki  ∀i ∈ [n]. (19)

Mp(B) = (cid:104)b  1B(cid:105) if B represents a path; ∞ otherwise.

(cid:32) (cid:88)

  s.t. (cid:104)b  ˜w(cid:105) = 1 

(cid:104)˜g  w(cid:105)
(cid:104)b  w(cid:105)  

0(cid:54)=w∈[0 1]|T |

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

s.t. wi =

k:(k i)∈E

j:(i j)∈E

(cid:33)

wij =

˜wij +

˜wki

max

max
˜w≥0

˜gi

i∈[n]

j:(i j)∈E

k:(k i)∈E

˜wij =

j:(i j)∈E

k:(k i)∈E

Due to the extra constraints  the LP (19) is more complicated than (15) obtained for group spar-
sity. Nevertheless  after some reformulation (essentially dualization)  (19) can still be converted to
a simple piecewise linear objective  hence it is amenable to smoothing; see Appendix F for details.
To ﬁnd a 2 accurate solution  the cutting plane method takes O( mn
2 ) computations to optimize the
nonsmooth piecewise linear objective  while APG needs O( 1
n) steps to optimize the smoothed

objective  using a total time cost of O( m
n). This too is faster than the O(nm) worst case com-

plexity of [12  Appendix D.5] in the regime where n is large and the desired accuracy  is moderate.

√

√

4 Generalizing Beyond Atomic Norms

(cid:88)

Although we ﬁnd the above approach to be effective  many useful regularizers are not expressed in
form of an atomic norm (2)  which makes evaluation of the polar a challenge and thus creates difﬁ-
culty in applying Algorithm 1. For example  another important class of structured sparse regularizers
is given by an alternative  composite gauge construction:

i

Ωs(w) =

κi(w)  where κi is a closed gauge that can be different for different i.

(20)
i wi = g}  where each
The polar for such a regularizer is given by Ω◦
wi is an independent vector and κ◦
i corresponds to the polar of κi (proof given in Appendix H).
Unfortunately  a polar in this form does not appear to be easy to compute. However  for some
regularizers in the form (20) the following proximal objective can indeed be computed efﬁciently:

s(g) = inf{maxi κ◦

i (wi) :(cid:80)

1

2(cid:107)g − θ(cid:107)2

ProxΩ(g) = minθ
2 + Ω(θ).
The key observation is that computing Ω◦ can be efﬁciently reduced to just computing ProxΩ.
Proposition 2 For any closed gauge Ω  its polar Ω◦ can be equivalently expressed by:

ArgProxΩ(g) = arg minθ

2 + Ω(θ) 

1

2(cid:107)g − θ(cid:107)2

Ω◦(g) = inf{ ζ ≥ 0 : ProxζΩ(g) = 1

2 }.
2(cid:107)g(cid:107)2

(21)

(22)

5

(The proof is included in Appendix I.) Since the left hand side of the inner constraint is decreasing in
ζ  one can efﬁciently compute the polar Ω◦ by a simple root ﬁnding search in ζ. Thus  regularizers in
the form of (20) can still be accommodated in an efﬁcient GCG method in the form of Algorithm 1.

4.1 Latent Fused Lasso

(cid:17)

 

i

(cid:16)

(cid:88)

W U∈U f (W U  X) + Ωp(W )  where Ωp(W ) =
min

To demonstrate the usefulness of this reduction we consider the recently proposed latent fused lasso
model [18]  where for given data X ∈ Rm×n one seeks a dictionary matrix W ∈ Rm×t and
coefﬁcient matrix U ∈ Rt×n that allow X to be accurately reconstructed from a dictionary that has
desired structure. In particular  for a reconstruction loss f  the problem is speciﬁed by:
λ1 (cid:107)W:i(cid:107)p + λ2 (cid:107)W:i(cid:107)TV

such that (cid:107) · (cid:107)TV is given by (cid:107)w(cid:107)TV = (cid:80)m−1

(23)
j=1 |wj+1 − wj| and (cid:107) · (cid:107)p is the usual (cid:96)p-norm. The
fused lasso [24] corresponds to p = 1. Note that U is constrained to be in a compact set U to avoid
degeneracy. To ease notation  we assume w.l.o.g. λ1 = λ2 = 1.
The main motivation for this regularizer arises from biostatistics  where one wishes to identify DNA
copy number variations simultaneously for a group of related samples [18]. In this case the total
variation norm (cid:107) · (cid:107)TV encourages the dictionary to vary smoothly from entry to entry while the (cid:96)p
norm shrinks the dictionary so that few latent features are selected. Conveniently  Ωp decomposes
along the columns of W   so one can apply the reduction in Proposition 2 to compute its polar assum-
ing ProxΩp can be efﬁciently computed. Solving ProxΩp appears non-trivial due to the composition
of two overlapping norms  however [25] showed that for p = 1 the polar can be solved efﬁciently
by computing Prox for each of the two norms successively. Here we extend this results by proving
in Appendix J that the same fact holds for any (cid:96)p norm.
Proposition 3 For any 1 ≤ p ≤ ∞  ArgProx(cid:107)·(cid:107)TV+(cid:107)·(cid:107)p (w) = ArgProx(cid:107)·(cid:107)p
Since Prox(cid:107)·(cid:107)p is easy to compute  the only remaining problem is to develop an efﬁcient algorithm
for computing Prox(cid:107)·(cid:107)TV. Although [26] has recently proposed an approximate iterative method  we
provide an algorithm in Appendix K that is able to efﬁciently compute the exact solution. Therefore 
by combining this result with Propositions 2 and 3 we are able to efﬁciently compute the polar Ω◦
p
and hence apply Algorithm 1 to solving (23) with respect to W .

(cid:0)ArgProx(cid:107)·(cid:107)TV

(w)(cid:1).

5 Experiments

To investigate the effectiveness of these computational schemes we considered three applications:
group lasso  path coding  and latent fused lasso. All algorithms were implemented in Matlab unless
otherwise noted.

5.1 Group Lasso: CUR-like Matrix Factorization

1

minW

i (cid:107)Wi:(cid:107)∞ +(cid:80)

2 (cid:107)X−XW X(cid:107)2 + λ(cid:0)(cid:80)

Our ﬁrst experiment considered an example of group lasso that is inspired by CUR matrix factor-
ization [27]. Given a data matrix X ∈ Rn×d  the goal is to compute an approximate factorization
X ≈ CU R  such that C contains a subset of c columns from X and R contains a subset of r rows
from X. Mairal et al. [11  §5.3] proposed a convex relaxation of this problem:
j (cid:107)W:j(cid:107)∞

(24)
Conveniently  the regularizer ﬁts the development of Section 3.1  with p = 1 and the groups deﬁned
to be the rows and columns of W . To evaluate different methods  we used four gene-expression data
sets [28]: SRBCT  Brain Tumor 2  9 Tumor  and Leukemia2  of sizes 83 × 2308  50 × 10367 
60× 5762  and 72× 11225  respectively. The data matrices were ﬁrst centered columnwise and then
rescaled to have unit Frobenius norm.
Algorithms. We compared three algorithms: GCG (Algorithm 1) with our polar operator which we
call GCG TUM  GCG with the polar operator of [11  Algorithm 2] (GCG Secant)  and APG (see
Section 2.1). The PU in APG uses the routine mexProximalGraph from the SPAMS package [29].
The polar operator of GCG Secant was implemented with a mex wrapper of a max-ﬂow package
[30]  while GCG TUM used L-BFGS to ﬁnd an optimal solution {w∗
G} for the smoothed version of

(cid:1).

6

(a) SRBCT

(b) Brain Tumor 2

(a) Obj vs CPU time (λ = 10−2)

(c) 9 Tumor

(d) Leukemia2

Figure 1: Convex CUR matrix factorization results.

(b) Obj vs CPU time (λ = 10−3)
Figure 2: Path coding results.
(15) given in Proposition 1  with smoothing parameter  set to 10−3. To recover an integral solution
it sufﬁces to ﬁnd an optimal solution to (15) that has the form wG = c for some groups and wG = 0
G} and set the wG of the smallest k
for the remainder (such a solution must exist). So we sorted {w∗
groups to 0  and wG for the remaining groups set to a common value that satisﬁes the constraint. The
best k can be recovered from {0  1  . . .  |G| − 1} in O(nr) time. See more details in Appendix G.
Both GCG methods relinquish local optimization (step 5) in Algorithm 1  but use a totally corrective
variant of step 4  which allows efﬁcient optimization by L-BFGS-B via pre-computing XP◦
(gk)X.
Results. For simplicity  we tested three values for λ: 10−3  10−4  and 10−5  which led to increas-
ingly dense solutions. Due to space limitations we only show in Figure 1 the results for λ = 10−4
which gives moderately sparse solutions. On these data sets  GCG TUM proves to be an order of
magnitude faster than GCG Secant in computing the polar. As [11] observes  network ﬂow based
algorithms often ﬁnd solutions in practice far more quickly than their theoretical bounds. Thanks
to the efﬁciency of totally corrective update  almost all computations taken by GCG Secant were
devoted to the polar operator. Therefore the acceleration proffered by GCG TUM in computing the
polar leads to a reduction of overall optimization time by at least 50%. Finally  APG is always even
slower than GCG Secant by an order of magnitude  with PU taking up the most computation.

Fg

5.2 Path Coding
Following [12  §4.3]  we consider a logistic regression problem where one is given training examples
xi ∈ Rn with corresponding labels yi ∈ {−1  1}. For this problem  we formulate (1) with a path
coding regularizer ΩFp and the empirical risk:

f (w) =(cid:80)

log(1 + exp(−yi (cid:104)w  xi(cid:105))) 

i

1
ni

(25)
where ni is the number of examples that share the same label as yi. We used the breast cancer data
set for this experiment  which consists of 8141 genes and 295 tumors [31]. The gene network is
adopted from [32]. Similar to [12  §4.3]  we removed all isolated genes (nodes) to which no edge is
incident  randomly oriented the raw edges  and removed cycles to form a DAG using the function
mexRemoveCyclesGraph in SPAMS. This resulted in 34864 edges and n = 7910 nodes.
Algorithms. We again considered three methods: APG  GCG with our polar operator (GCG TUM) 
and GCG with the polar operator from [12  Algorithm 1]  which we label as GCG Secant. The PU
in APG uses the routine mexProximalPathCoding from SPAMS  which solves a quadratic network
ﬂow problem. It turns out the time cost for a single call of the PU was enough for GCG TUM and

7

1011021031040.050.10.150.20.25CPU time (seconds)Objective function value1011021031040.030.040.050.060.070.08CPU time (seconds)Objective function value1011021031040.040.050.060.070.08CPU time (seconds)Objective function value1011021030.050.060.070.080.090.1CPU time (seconds)Objective function value10−110010111.11.21.3CPU time (seconds)Objective function value10−11001011020.20.40.60.81CPU time (seconds)Objective function valueGCG Secant to converge to a ﬁnal solution  and so the APG result is not included in our plots. We
implemented the polar operator for GCG Secant based on Matlab’s built-in shortest path routine
graphshortestpath (C++ wrapped by mex). For GCG TUM  we used cutting plane to solve a vari-
ant of the dual of (19) (see Appendix F)  which is much simipler than smoothing in implementation 
but exhibits similar efﬁciency in practice. An integral solution can also be naturally recovered in the
course of computing the objective. Again  both GCG methods only used totally corrective updates.
Results. Figure 2 shows the result for path coding  with the regularization coefﬁcient λ set to 10−2
and 10−3 so that the solution is moderately sparse. Again it is clear that GCG TUM is an order of
magnitude faster than GCG Secant.

we use the model ˜Wij = (cid:80)Sj

5.3 Latent Fused Lasso
Finally  we compared GCG and APG on the latent fused lasso problem (23). Two algorithms were
tested as the PU in APG: our proposed method and the algorithm in [26]  which we label as APG-
Liu. The synthetic data is generated by following [18]. For each basis (column) of the dictionary 
s=1 csI(is ≤ i ≤ is + ls)  where Sj ∈ {3  5  8  10} speciﬁes the
number of consecutive blocks in the j-th basis  cs ∈ {±1 ±2 ±3 ±4 ±5}  is ∈ {1  . . .   m − 10}
and ls ∈ {5  10  15  20}  which are the magnitude  starting position  and length of the s-th block 
respectively. Note that we choose cs  is  ls randomly (and independently for each block s) from
their respective sets. The coefﬁcient matrix ˜U are sampled from the Gaussian distribution N (0  1)
(independently for each entry) and normalized to have unit (cid:96)2 norm for each row. Finally  we
generate the observation matrix X = ˜W ˜U + ε  with added (zero mean and unit variance) Gaussian
noise ε. We set the dimension m = 300  the number of samples n = 200  and the number of bases
(latent dimension) ˜t = 10.
F   but the algo-
Since the noise is Gaussian  we choose the squared loss f (W U  X) = 1
rithm is applicable to any other smooth loss as well. To avoid degeneracy  we constrained each row
of U to have unit (cid:96)2 norm. Finally  to pick an appropriate dictionary size  we tried t ∈ {5  10  20} 
which corresponds to under-  perfect- and over-estimation  respectively. The regularization con-
stants λ1  λ2 in Ωp were chosen from {0.01  0.1  1  10  100}.

2(cid:107)X − W U(cid:107)2

Note that problem (23) is not jointly convex in W and
U  so we followed the same strategy as [18]; that is 
we alternatively optimized W and U keeping the other
ﬁxed. For each subproblem  we ran both APG and
GCG to compare their performance. For space limita-
tions  we only report the running time for the setting
λ1 = λ2 = 0.1  t = 20 and p ∈ {1  2}.
In these
experiments we observed that the polar typically only
requires 5 to 6 calls to Prox. As can be seen from Fig-
ure 3  GCG is signiﬁcantly faster than APG and APG-
Liu in reducing the objective. This is due to the greedy
nature of GCG  which yields very sparse iterates  and
when interleaved with local search achieves fast con-
vergence.

Figure 3: Latent fused lasso.

6 Conclusion
We have identiﬁed and investigated a new class of structured sparse regularizers whose polar can
be reformulated as a linear program with totally unimodular constraints. By leveraging smoothing
techniques  we are able to compute the corresponding polars with signiﬁcantly better efﬁciency than
previous approaches. When plugged into the GCG algorithm  one can observe signiﬁcant reductions
in run time for both group lasso and path coding regularization. We have further developed a generic
scheme for converting an efﬁcient proximal solver to an efﬁcient method for computing the polar
operator. This reduction allowed us to develop a fast new method for latent fused lasso. For future
work  we plan to study more general subset cost functions and investigate new structured regularizers
amenable to our approach. It will also be interesting to extend GCG to handle nonsmooth losses.

8

0204060801005.25.45.65.866.26.4x 104CPU time (sec)Loss + Reg APG  p=1GCG  p=1APG(cid:239)Liu  p=1APG  p=2GCG  p=2APG(cid:239)Liu p=2References
[1] P. B¨uhlmann and S. van de Geer. Statistics for High-Dimensional Data. Springer  2011.
[2] Y. Eldar and G. Kutyniok  editors. Compressed Sensing: Theory and Applications. Cambridge  2012.
[3] J. Huang  T. Zhang  and D. Metaxas. Learning with structured sparsity. JMLR  12:3371–3412  2011.
[4] S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In ICML 

2010.

[5] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse coding.

JMLR  12:2297–2334  2011.

[6] G. Obozinski and F. Bach. Convex relaxation for combinatorial penalties. Technical Report HAL

00694765  2012.

[7] P. Zhao  G. Rocha  and B. Yu. The composite absolute penalties family for grouped and hierarchical

variable selection. Annals of Statistics  37(6A):3468–3497  2009.

[8] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Optimization with sparsity-inducing penalties. Foun-

dations and Trends in Machine Learning  4(1):1–106  2012.

[9] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[10] Y. Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming  140:

125–161  2013.

[11] J. Mairal  R. Jenatton  G. Obozinski  and F. Bach. Convex and network ﬂow optimization for structured

sparsity. JMLR  12:2681–2720  2011.

[12] J. Mairal and B. Yu. Supervised feature selection in graphs with path coding penalties and network ﬂows.

JMLR  14:2449–2485  2013.

[13] M. Dudik  Z. Harchaoui  and J. Malick. Lifted coordinate descent for learning with trace-norm regular-

izations. In AISTATS  2012.

[14] X. Zhang  Y. Yu  and D. Schuurmans. Accelerated training for matrix-norm regularization: A boosting

approach. In NIPS  2012.

[15] S. Laue. A hybrid algorithm for convex semideﬁnite optimization. In ICML  2012.
[16] B. Mishra  G. Meyer  F. Bach  and R. Sepulchre. Low-rank optimization with trace norm penalty. Tech-

nical report  2011. http://arxiv.org/abs/1112.2318.

[17] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming  103(1):127–

152  2005.

[18] G. Nowak  T. Hastie  J. R. Pollack  and R. Tibshirani. A fused lasso latent feature model for analyzing

multi-sample aCGH data. Biostatistics  12(4):776–791  2011.

[19] R. T. Rockafellar. Convex Analysis. Princeton University Press  1970.
[20] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear inverse

problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[21] F. Bach. Convex analysis and optimization with submodular functions: a tutorial. Technical Report HAL

00527714  2010.

[22] W. Dinkelbach. On nonlinear fractional programming. Management Science  13(7)  1967.
[23] A. Schrijver. Theory of Linear and Integer Programming. John Wiley & Sons  1st edition  1986.
[24] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the fused lasso.

Journal of the Royal Statistical Society: Series B  67:91–108  2005.

[25] J. Friedman  T. Hastie  H. H¨oﬂing  and R. Tibshirani. Pathwise coordinate optimization. The Annals of

Applied Statistics  1(2):302–332  2007.

[26] J. Liu  L. Yuan  and J. Ye. An efﬁcient algorithm for a class of fused lasso problems. In Conference on

Knowledge Discovery and Data Mining  2010.

[27] M. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the

National Academy of Sciences  106(3):697–702  2009.

[28] URL http://www.gems-system.or.
[29] URL http://spams-devel.gforge.inria.fr.
[30] URL http://drwn.anu.edu.au/index.html.
[31] M. Van De Vijver et al. A gene-expression signature as a predictor of survival in breast cancer. The New

England Journal of Medicine  347(25):1999–2009  2002.

[32] H. Chuang  E. Lee  Y. Liu  D. Lee  and T. Ideker. Network-based classiﬁcation of breast cancer metastasis.

Molecular Systems Biology  3(140)  2007.

9

,Xinhua Zhang
Yao-Liang Yu
Dale Schuurmans
Jun-Yan Zhu
Zhoutong Zhang
Chengkai Zhang
Jiajun Wu
Antonio Torralba
Josh Tenenbaum
Bill Freeman