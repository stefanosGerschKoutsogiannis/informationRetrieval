2013,Convex Tensor Decomposition via Structured Schatten Norm Regularization,We propose a new class of structured Schatten norms for tensors that includes two recently  proposed norms (overlapped'' and "latent'') for convex-optimization-based  tensor decomposition. Based on the properties of the structured Schatten norms  we mathematically analyze the performance of "latent'' approach for tensor decomposition  which was empirically found to perform better than the "overlapped'' approach in some settings. We show theoretically that this is indeed the case. In particular  when the unknown true tensor is low-rank in a specific mode  this approach performs as well as knowing the mode with the smallest rank. Along the way  we show a novel duality result for structures Schatten norms  which is also interesting in the general context of structured sparsity. We confirm through  numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.  ",Convex Tensor Decomposition via Structured

Schatten Norm Regularization

Ryota Tomioka

Toyota Technological Institute at Chicago

Chicago  IL 60637

tomioka@ttic.edu

Taiji Suzuki

Department of Mathematical

and Computing Sciences

Tokyo Institute of Technology

Tokyo 152-8552  Japan

s-taiji@is.titech.ac.jp

Abstract

We study a new class of structured Schatten norms for tensors that includes two
recently proposed norms (“overlapped” and “latent”) for convex-optimization-
based tensor decomposition. We analyze the performance of “latent” approach
for tensor decomposition  which was empirically found to perform better than the
“overlapped” approach in some settings. We show theoretically that this is indeed
the case. In particular  when the unknown true tensor is low-rank in a speciﬁc
unknown mode  this approach performs as well as knowing the mode with the
smallest rank. Along the way  we show a novel duality result for structured Schat-
ten norms  which is also interesting in the general context of structured sparsity.
We conﬁrm through numerical simulations that our theory can precisely predict
the scaling behaviour of the mean squared error.

1 Introduction

Decomposition of tensors [10  14] (or multi-way arrays) into low-rank components arises naturally
in many real world data analysis problems. For example  in neuroimaging  spatio-temporal patterns
of neural activities that are related to certain experimental conditions or subjects can be found by
computing the tensor decomposition of the data tensor  which can be of size channels (cid:2) time-
points (cid:2) subjects (cid:2) conditions [18]. More generally  any multivariate spatio-temporal data (e.g. 
environmental monitoring) can be regarded as a tensor. If some of the observations are missing  low-
rank modeling enables the imputation of missing values. Tensor modelling may also be valuable for
collaborative ﬁltering with temporal or contextual dimension.
Conventionally  tensor decomposition has been tackled through non-convex optimization problems 
using alternate least squares or higher-order orthogonal iteration [6]. Compared to its empirical
success  little has been theoretically understood about the performance of tensor decomposition
algorithms. De Lathauwer et al. [5] showed an approximation bound for a truncated higher-order
SVD (also known as the Tucker decomposition). Nevertheless the generalization performance of
these approaches has been widely open. Moreover  the model selection problem can be highly
challenging  especially for the Tucker model [5  27]  because we need to specify the rank rk for each
mode (here a mode refers to one dimensionality of a tensor); that is  we have K hyper-parameters
to choose for a K-way tensor  which is challenging even for K = 3.
Recently a convex-optimization-based approach for tensor decomposition has been proposed by
several authors [9  15  23  25]  and its performance has been analyzed in [26].

1

Figure 1: Estimation of a low-rank 50(cid:2)50(cid:2)20 tensor of rank r (cid:2) r (cid:2) 3 from noisy measurements.
The noise standard deviation is (cid:27) = 0:1. The estimation errors of two convex optimization based
methods are plotted against the rank r of the ﬁrst two modes. The solid lines show the error at the
ﬁxed regularization constant (cid:21)  which is 0.89 for the overlapped approach and 3.79 for the latent
approach (see also Figure 2). The dashed lines show the minimum error over candidates of the
regularization constant (cid:21) from 0.1 to 100. In the inset  the errors of the two approaches are plotted
against the regularization constant (cid:21) for rank r = 40 (marked with gray dashed vertical line in
the outset). The two values (0.89 and 3.79) are marked with vertical dashed lines. Note that both
approaches need no knowledge of the true rank; the rank is automatically learned.

The basic idea behind their convex approach  which we call overlapped approach  is to unfold1 a
tensor into matrices along different modes and penalize the unfolded matrices to be simultaneously
low-rank based on the Schatten 1-norm  which is also known as the trace norm and nuclear norm [7 
22  24]. This approach does not require the rank of the decomposition to be speciﬁed beforehand 
and due to the low-rank inducing property of the Schatten 1-norm  the rank of the decomposition is
automatically determined.
However  it has been noticed that the above overlapped approach has a limitation that it performs
poorly for a tensor that is only low-rank in a certain mode. The authors of [25] proposed an alter-
native approach  which we call latent approach  that decomposes a given tensor into a mixture of
tensors that each are low-rank in a speciﬁc mode. Figure 1 demonstrates that the latent approach
is preferable to the overlapped approach when the underlying tensor is almost full rank in all but
one mode. However  so far no theoretical analysis has been presented to support such an empirical
success.
In this paper  we rigorously study the performance of the latent approach and show that the mean
squared error of the latent approach scales no greater than the minimum mode-k rank of the underly-
ing true tensor  which clearly explains why the latent approach performs better than the overlapped
approach in Figure 1.
Along the way  we show a novel duality between the two types of norms employed in the above
two approaches  namely the overlapped Schatten norm and the latent Schatten norm. This result
is closely related and generalize the results in structured sparsity literature [2  13  17  21]. In fact 
the (plain) overlapped group lasso constrains the weights to be simultaneously group sparse over
overlapping groups. The latent group lasso predicts with a mixture of group sparse weights [see
also 1  3  12]. These approaches clearly correspond to the two variations of tensor decomposition
algorithms we discussed above.
Finally we empirically compare the overlapped approach and latent approach and show that even
when the unknown tensor is simultaneously low-rank  which is a favorable situation for the over-
lapped approach  the latent approach performs better in many cases. Thus we provide both theoreti-
cal and empirical evidence that for noisy tensor decomposition  the latent approach is preferable to
the overlapped approach. Our result is complementary to the previous study [25  26]  which mainly
focused on the noise-less tensor completion setting.

1For a K-way tensor  there are K ways to unfold a tensor into a matrix. See Section 2.

2

01020304050601015202530Rank of the first two modesEstimation error ||W−W*||Fsize=[50 50 20]  Overlapped Schatten 1−normLatent Schatten 1−norm1001011020204060Regularization constant λ||W−W*||Frank=[40 40 3]This paper is structured as follows. In Section 2  we provide basic deﬁnitions of the two variations of
structured Schatten norms  namely the overlapped/latent Schatten norms  and discuss their proper-
ties  especially the duality between them. Section 3 presents our main theoretical contributions; we
establish the consistency of the latent approach  and we analyze the denoising performance of the
latent approach. In Section 4  we empirically conﬁrm the scaling predicted by our theory. Finally 
Section 5 concludes the paper. Most of the proofs are presented in the supplementary material.

2 Structured Schatten norms for tensors

K

In this section  we deﬁne the overlapped Schatten norm and the latent Schatten norm and discuss
their basic properties.
∏
First we need some basic deﬁnitions.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Let W 2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)nK be a K-way tensor. We denote the total number of entries in W by N =
√⟨W;W⟩. Each dimensionality of a tensor is called a mode. The mode k unfolding W (k) 2
vec(X );
k=1 nk. The dot product between two tensors W and X is deﬁned as ⟨W;X⟩ = vec(W)
i.e.  the dot product as vectors in RN . The Frobenius norm of a tensor is deﬁned as
F =
Rnk(cid:2)N=nk is a matrix that is obtained by concatenating the mode-k ﬁbers along columns; here a
mode-k ﬁber is an nk dimensional vector obtained by ﬁxing all the indices but the kth index of W.
The mode-k rank rk of W is the rank of the mode-k unfolding W (k). We say that a tensor W has
multilinear rank (r1; : : : ; rK) if the mode-k rank is rk for k = 1; : : : ; K [14]. The mode k folding
is the inverse of the unfolding operation.

⊤

2.1 Overlapped Schatten norms

The low-rank inducing norm studied in [9  15  23  25]  which we call overlapped Schatten 1-norm 
can be written as follows:

In this paper  we consider the following more general overlapped Sp=q-norm  which includes the
Schatten 1-norm as the special case (p; q) = (1; 1). The overlapped Sp=q-norm is written as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∑K

S1=1 =

k=1

∥W (k)∥S1 :
)1=q
∥W (k)∥q
)1=p

Sp

(∑K
(∑r

∥W∥Sp =

(cid:27)p
j (W )

j=1

(1)

(3)

where 1 (cid:20) p; q (cid:20) 1; here

Sp=q =

k=1

;

(2)

is the Schatten p-norm for matrices  where (cid:27)j(W ) is the jth largest singular value of W .
When used as a regularizer  the overlapped Schatten 1-norm penalizes all modes of W to be jointly
low-rank. It is related to the overlapped group regularization [see 13  16] in a sense that the same
object W appears repeatedly in the norm.
The following inequality relates the overlapped Schatten 1-norm with the Frobenius norm  which

was a key step in the analysis of [26]:(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:20) K∑

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

F ;

p
rk

S1=1

k=1

where rk is the mode-k rank of W.
Now we are interested in the dual norm of the overlapped Sp=q-norm  because deriving the dual
norm is a key step in solving the minimization problem that involves the norm (2) [see 16]  as
well as computing various complexity measures  such as  Rademacher complexity [8] and Gaussian
(cid:3)-norm as
width [4]. It turns out that the dual norm of the overlapped Sp=q-norm is the latent Sp(cid:3) =q
shown in the following lemma (proof is presented in Appendix A).

3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)X(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Lemma 1. The dual norm of the overlapped Sp=q-norm is the latent Sp(cid:3) =q
1=p

= 1  which is deﬁned as follows:

= 1 and 1=q + 1=q

(cid:3)

(cid:3)

(∑K

)1=q

(cid:3)-norm  where 1=p +

(cid:3)

:

(4)

Sp(cid:3) =q(cid:3) =

inf

(X (1)+(cid:1)(cid:1)(cid:1)+X (K))=X

∥X (k)

(k)

∥q
(cid:3)
Sp(cid:3)

k=1

Here the inﬁmum is taken over the K-tuple of tensors X (1); : : : ;X (K) that sums to X .
In the supplementary material  we show a slightly more general version of the above lemma that
naturally generalizes the duality between overlapped/latent group sparsity norms [1  12  17  21]; see
Section A. Note that when the groups have no overlap  the overlapped/latent group sparsity norms
become identical  and the duality is the ordinary duality between the group Sp=q-norms and the
group Sp(cid:3)=q

(cid:3)-norms.

2.2 Latent Schatten norms

The latent approach for tensor decomposition [25] solves the following minimization problem

minimize
W (1);:::;W (K)

L(W (1) + (cid:1)(cid:1)(cid:1) + W (K)) + (cid:21)

∥W (k)

(k)

∥S1 ;

(5)

K∑

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

S1=1 =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)X(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where L is a loss function  (cid:21) is a regularization constant  and W (k)
(k) is the mode-k unfolding of
W (k). Intuitively speaking  the latent approach for tensor decomposition predicts with a mixture of
K tensors that each are regularized to be low-rank in a speciﬁc mode.
Now  since the loss term in the minimization problem (5) only depends on the sum of the tensors
W (1); : : : ;W (K)  minimization problem (5) is equivalent to the following minimization problem

minimizeW

L(W) + (cid:21)

S1=1:

In other words  we have identiﬁed the structured Schatten norm employed in the latent approach as
the latent S1=1-norm (or latent Schatten 1-norm for short)  which can be written as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
K∑

∥W (k)

(k)

∥S1 :

(6)

According to Lemma 1  the dual norm of the latent S1=1-norm is the overlapped S1=1-norm

k=1

inf

(W (1)+(cid:1)(cid:1)(cid:1)+W (K))=W

S1=1 = max

∥X (k)∥S1 ;

(7)

where ∥ (cid:1) ∥S1 is the spectral norm.
The following lemma is similar to inequality (3) and is a key in our analysis (proof is presented in
Appendix B).
Lemma 2.

k

(

)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

F ;

(cid:20)

S1=1

p
rk

min

k

where rk is the mode-k rank of W.
Compared to inequality (3)  the latent Schatten 1-norm is bounded by the minimal square root of the
ranks instead of the sum. This is the fundamental reason why the latent approach performs betters
than the overlapped approach as in Figure 1.

3 Main theoretical results

In this section  combining the duality we presented in the previous section with the techniques
from Agarwal et al. [1]  we study the generalization performance of the latent approach for tensor
decomposition in the context of recovering an unknown tensor W(cid:3) from noisy measurements. This
is the setting of the experiment in Figure 1. We ﬁrst prove a generic consistency statement that does
not take the low-rank-ness of the truth into account. Next we show that a tighter bound that takes the
low-rank-ness into account can be obtained with some incoherence assumption. Finally  we discuss
the difference between overlapped approach and latent approach and provide an explanation for the
empirically observed superior performance of the latent approach in Figure 1.

4

3.1 Consistency
Let W(cid:3) be the underlying true tensor and the noisy version Y is obtained as follows:

Y = W(cid:3)

+ E;

where E 2 Rn1(cid:2)(cid:1)(cid:1)(cid:1)(cid:2)nK is the noise tensor.
A consistency statement can be obtained as follows (proof is presented in Appendix C):

Theorem 1. Assume that the regularization constant (cid:21) satisﬁes (cid:21) (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(

)
S1=1 (overlapped S1=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Y (cid:0) W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

norm of the noise)  then the estimator deﬁned by ^W = argminW
satisﬁes the inequality

1
2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (cid:0) W(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

√
(cid:20) 2(cid:21)

(8)
In particular when the noise goes to zero E ! 0  the right hand side of inequality (8) shrinks to zero.

min

nk:

F

k

F + (cid:21)

;

S1=1

3.2 Deterministic bound
The consistency statement in the previous section only deals with the sum ^W =
^W (k) and
the statement does not take into account the low-rank-ness of the truth. In this section  we establish
a tighter statement that bounds the errors of individual terms ^W (k).
To this end  we need some additional assumptions. First  we assume that the unknown tensor W(cid:3) is
a mixture of K tensors that each are low-rank in a certain mode and we have a noisy observation Y
as follows:

K
k=1

Y = W(cid:3)

+ E =

W(cid:3)(k) + E;

(9)
(k)) is the mode-k rank of the kth component W(cid:3)(k); note that this does not

where (cid:22)rk = rank(W (k)
equal the mode-k rank rk of W(cid:3) in general.
Second  we assume that the spectral norm of the mode-k unfolding of the lth component is bounded
by a constant (cid:11) for all k ̸= l as follows:

k=1

∑K

∑

∥W

(cid:3)(l)
(k)

∥S1 (cid:20) (cid:11) (8l ̸= k; k; l = 1; : : : ; K):

(10)

)

;

8l ̸= k

(

Note that such an additional incoherence assumption has also been used in [1  3  11].
We employ the following optimization problem to recover the unknown tensor W(cid:3):
^W = argminW

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Y (cid:0) W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)W(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

W (k); ∥W (l)

s:t: W =

K∑

∥S1 (cid:20) (cid:11);

F + (cid:21)

S1=1

(k)

1
2

k=1

(11)
where (cid:21) > 0 is a regularization constant. Notice that we have introduced additional spectral norm
constraints to control the correlation between the components; see also [1].
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Our deterministic performance bound can be stated as follows (proof is presented in Appendix D):
Theorem 2. Let ^W (k) be an optimal decomposition of ^W induced by the latent Schatten 1-norm (6).
S1=1 + (cid:11)(K (cid:0) 1). Then there is
Assume that the regularization constant (cid:21) satisﬁes (cid:21) (cid:21) 2
a universal constant c such that  any solution ^W of the minimization problem (11) satisﬁes the
∑K

following deterministic bound:∑K

(cid:20) c(cid:21)2

rk:

k=1

(12)

Moreover  the overall error can be bounded in terms of the multilinear rank of W(cid:3) as follows:

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

F

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (k) (cid:0) W(cid:3)(k)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (cid:0) W(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(cid:20) c(cid:21)2 min

k

F

5

rk:

(13)

K
k=1

=
W(cid:3)(k) to replace the sum over the ranks with the minimal mode-k rank. This is possible
′ ̸= k  is allowed for

∑
Note that in order to get inequality (13)  we exploit the arbitrariness of the decomposition W(cid:3)
because a singleton decomposition  i.e.  W(cid:3)(k) = W(cid:3) and W(cid:3)(k
any k.
Comparing two inequalities (8) and (13)  we see that there are two regimes. When the noise is small 
≪ mink nk  (13) is tighter.
(8) is tighter. On the other hand  when the noise is larger and/or mink rk

) = 0 for k

′

3.3 Gaussian noise
When the elements of the noise tensor E are Gaussian  we obtain the following theorem.
Theorem 3. Assume that the elements of the noise tensor E are independent zero-mean Gaussian
random variables with variance (cid:27)2. In addition  assume without loss of generality that the dimen-
sionalities of W(cid:3) are sorted in the descending order  i.e.  n1 (cid:21) (cid:1)(cid:1)(cid:1) (cid:21) nK. Then there is a universal
constant c such that  with probability at least 1 (cid:0) (cid:14)  any solution of the minimization problem (11)
with regularization constant (cid:21) = 2(cid:27)(

√
∑
2 log(K=(cid:14))) + (cid:11)(K (cid:0) 1) satisﬁes

p
n1 +

√
K∑
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (k) (cid:0) W(cid:3)(k)
(√

N=nK +

k=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

F

((

√

1
N

)

(cid:20) cF (cid:27)2

)√

K
k=1 (cid:22)rk
nK

)2

;

(14)

where F =
on the dimensionalities and the constant (cid:11) in (10).

1 +

n1nK

+

N

2 log(K=(cid:14)) + (cid:11)(K(cid:0)1)

2(cid:27)

nK
N

is a factor that mildly depends

Note that the theoretically optimal choice of regularization constant (cid:21) is independent of the ranks of
the truth W(cid:3) or its factors in (9)  which are unknown in practice.
Again we can obtain a bound corresponding to the minimum rank singleton decomposition as in
inequality (13) as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (cid:0) W(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

1
N

(cid:20) cF (cid:27)2 mink rk

nK

;

(15)

F

where F is the same factor as in Theorem 3.

3.4 Comparison with the overlapped approach

Inequality (15) explains the superior performance of the latent approach for tensor decomposition in
Figure 1. The inequality obtained in [26] for the overlapped approach that uses overlapped Schatten
1-norm (1) can be stated as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (cid:0) W(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

1
N

(cid:20) c
′

(cid:27)2

1
K

F

(

)2(

K∑

√

k=1

1
nk

)2

K∑

k=1

1
K

p
rk

:

(16)

Comparing inequalities (15) and (16)  we notice that the complexity of the overlapped approach
depends on the average (square root) of the mode-k ranks r1; : : : ; rK  whereas that of the latent
approach only grows linearly against the minimum mode-k rank. Interestingly  the latent approach
performs as if it knows the mode with the minimum rank  although such information is not given.
Recently  Mu et al. [19] proved a lower bound of the number of measurements for solving linear
inverse problem via the overlapped approach. Although the setting is different  the lower bound
depends on the minimum mode-k rank  which agrees with the complexity of the latent approach.

4 Numerical results

In this section  we numerically conﬁrm the theoretically obtained scaling behavior.
The goal of this experiment is to recover the true low rank tensor W(cid:3) from a noisy observation Y.
We randomly generated the true low rank tensors W(cid:3) of size 50 (cid:2) 50 (cid:2) 20 or 80 (cid:2) 80 (cid:2) 40 with
various mode-k ranks (r1; r2; r3). A low-rank tensor is generated by ﬁrst randomly drawing the

6

∑

(
∑

1
K

K
k=1 (cid:22)rk
nK

;

TR complexity =

LR complexity =

√

)2(

∑

K
k=1

1
nk

1
K

K
k=1

)2

;

p
rk

(17)

(18)

√

Figure 2: Performance of the overlapped approach and latent approach for tensor decomposition are
shown against their theoretically predicted complexity measures (see Eqs. (17) and (18)). The right
panel shows the improvement of the latent approach from the overlapped approach against the ratio
of their complexity measures.

(cid:2) r2

(cid:2) r3 core tensor from the standard normal distribution and multiplying an orthogonal factor
r1
matrix drawn uniformly to its each mode. The observation tensor Y is obtained by adding Gaussian
noise with standard deviation (cid:27) = 0:1. There is no missing entries in this experiment.
For each observation Y  we computed tensor decompositions using the overlapped approach and the
latent approach (11). For the optimization  we used the algorithms2 based on alternating direction
method of multipliers described in Tomioka et al. [25]. We computed the solutions for 20 candidate
regularization constants ranging from 0.1 to 100 and report the results for three representative values
for each method.
We measured the quality of the solutions obtained by the two approaches by the mean squared error
F =N. In order to make our theoretical predictions more concrete  we deﬁne
(MSE)
the quantities in the right hand side of the bounds (16) and (14) as Tucker rank (TR) complexity and
Latent rank (LR) complexity  respectively  as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ^W (cid:0) W(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

where without loss of generality we assume n1 (cid:21) (cid:1)(cid:1)(cid:1) (cid:21) nK. We have ignored terms like
nk=N
because they are negligible for nk (cid:25) 50 and N (cid:25) 50; 000. The TR complexity is equivalent to the
normalized rank in [26]. Note that the TR complexity (17) is deﬁned in terms of the multilinear rank
(r1; : : : ; rK) of the truth W(cid:3)  whereas the LR complexity (18) is deﬁned in terms of the ranks of the
latent factors (r1; : : : ; rK) in (9). In order to ﬁnd a decomposition that minimizes the right hand side
of (18)  we ran the latent approach to the true tensor W(cid:3) without noise  and took the minimum of
the sum of ranks found by the run and mink rk  i.e.  the minimal mode-k rank (because a singleton
solution is also allowed). The whole procedure is repeated 10 times and averaged.
Figure 2 shows the results of the experiment. The left panel shows the MSE of the overlapped
approach against the TR complexity (17). The middle panel shows the MSE of the latent approach
against the LR complexity (18). The right panel shows the improvement (i.e.  MSE of the overlap
approach over that of the latent approach) against the ratio of the respective complexity measures.
First  from the left panel  we can conﬁrm that as predicted by [26]  the MSE of the overlapped
approach scales linearly against the TR complexity (17) for each value of the regularization constant.
From the central panel  we can clearly see that the MSE of the latent approach scales linearly against
the LR complexity (18) as predicted by Theorem 3. The series with △ ((cid:21) = 3:79 for 50 (cid:2) 50 (cid:2) 20 

2The solver is available online: https://github.com/ryotat/tensor.

7

00.20.40.60.8100.0050.010.015Tucker rank complexityMean squared error (overlap)Overlapped approach  size=[50 50 20] λ=0.43size=[50 50 20] λ=0.89size=[50 50 20] λ=3.79size=[80 80 40] λ=0.62size=[80 80 40] λ=1.27size=[80 80 40] λ=5.4600.20.40.60.8100.0050.010.015Latent rank complexityMean squared error (latent)Latent approach  size=[50 50 20] λ=0.89size=[50 50 20] λ=3.79size=[50 50 20] λ=11.29size=[80 80 40] λ=1.27size=[80 80 40] λ=5.46size=[80 80 40] λ=16.240123400.511.522.533.544.555.5TR complexity/LR complexityMSE (overlap) / MSE (latent)Comparison(cid:21) = 5:46 for 80(cid:2) 80(cid:2) 40) is mostly below other series  which means that the optimal choice of the
regularization constant is independent of the rank of the true tensor and only depends on the size;
this agrees with the condition on (cid:21) in Theorem 3. Since the blue series and red series with the same
markers lie on top of each other (especially the series with △ for which the optimal regularization
constant is chosen)  we can see that our theory predicts not only the scaling against the latent ranks
but also that against the size of the tensor correctly. Note that the regularization constants are scaled
by roughly 1.6 to account for the difference in the dimensionality.
The right panel reveals that in many cases the latent approach performs better than the overlapped
approach  i.e.  MSE (overlap)/ MSE (latent) greater than one. Moreover  we can see that the success
of the latent approach relative to the overlapped approach is correlated with high TR complexity
to LR complexity ratio. Indeed  we found that an optimal decomposition of the true tensor W(cid:3)
was typically a singleton decomposition corresponding to the smallest tucker rank (see Section 3.2).
Note that the two approaches perform almost identically when they are under-regularized (crosses).
The improvements here are milder than that in Figure 1. This is because most of the randomly
generated low-rank tensors were simultaneously low-rank to some degree. It is encouraging that the
latent approach perform at least as well as the overlapped approach in such situations as well.

5 Conclusion

In this paper  we have presented a framework for structured Schatten norms. The current framework
includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the
context of convex-optimization-based tensor decomposition [9  15  23  25]  and connects these stud-
ies to the broader studies on structured sparsity [2  13  17  21]. Moreover  we have shown a duality
that holds between the two types of norms.
Furthermore  we have rigorously studied the performance of the latent approach for tensor decom-
position. We have shown the consistency of the latent Schatten 1-norm minimization. Next  we have
analyzed the denoising performance of the latent approach and shown that the error of the latent ap-
proach is upper bounded by the minimal mode-k rank  which contrasts sharply against the average
(square root) dependency of the overlapped approach analyzed in [26]. This explains the empirically
observed superior performance of the latent approach compared to the overlapped approach. The
most difﬁcult case for the overlapped approach is when the unknown tensor is only low-rank in one
mode as in Figure 1.
We have also conﬁrmed through numerical simulations that our analysis precisely predicts the scal-
ing of the mean squared error as a function of the dimensionalities and the sum of ranks of the factors
of the unknown tensor  which is dominated by the minimal mode-k rank. Unlike mode-k ranks  the
ranks of the factors are not easy to compute. However  note that the theoretically optimal choice of
the regularization constant does not depend on these quantities.
Thus  we have theoretically and empirically shown that for noisy tensor decomposition  the latent
approach is more likely to perform better than the overlapped approach. Analyzing the performance
of the latent approach for tensor completion would be an important future work.
The structured Schatten norms proposed in this paper include norms for tensors that are not em-
ployed in practice yet. Therefore  it would be interesting to explore various extensions  such as 
using the overlapped S1=1-norm instead of the S1=1-norm or a non-sparse tensor decomposition.
Acknowledgment: This work was carried out while both authors were at The University of Tokyo.
This work was partially supported by JSPS KAKENHI 25870192 and 25730013  and the Aihara
Project  the FIRST program from JSPS  initiated by CSTP.

References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Noisy matrix decomposition via convex relaxation:

Optimal rates in high dimensions. The Annals of Statistics  40(2):1171–1197  2012.

[2] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing norms. In

Optimization for Machine Learning. MIT Press  2011.

8

[3] E. J. Candes  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Technical report 

arXiv:0912.3599  2009.

[4] V. Chandrasekaran  B. Recht  P. Parrilo  and A. Willsky. The convex geometry of linear inverse problems 

prepint. Technical report  arXiv:1012.0621v2  2010.

[5] L. De Lathauwer  B. De Moor  and J. Vandewalle. A multilinear singular value decomposition. SIAM J.

Matrix Anal. Appl.  21(4):1253–1278  2000.

[6] L. De Lathauwer  B. De Moor  and J. Vandewalle. On the best rank-1 and rank-(R1; R2; : : : ; RN ) ap-

proximation of higher-order tensors. SIAM J. Matrix Anal. Appl.  21(4):1324–1342  2000.

[7] M. Fazel  H. Hindi  and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order

System Approximation. In Proc. of the American Control Conference  2001.

[8] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. Technical

report  arXiv:1102.3923  2011.

[9] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex opti-

mization. Inverse Problems  27:025010  2011.

[10] F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. J. Math. Phys.  6(1):

164–189  1927.

[11] D. Hsu  S. M. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions. Information

Theory  IEEE Transactions on  57(11):7221–7234  2011.

[12] A. Jalali  P. Ravikumar  S. Sanghavi  and C. Ruan. A dirty model for multi-task learning. In Advances in

NIPS 23  pages 964–972. 2010.

[13] R. Jenatton  J. Audibert  and F. Bach. Structured variable selection with sparsity-inducing norms. J.

Mach. Learn. Res.  12:2777–2824  2011.

[14] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500 

2009.

[15] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.

In Prof. ICCV  2009.

[16] J. Mairal  R. Jenatton  G. Obozinski  and F. Bach. Convex and network ﬂow optimization for structured

sparsity. J. Mach. Learn. Res.  12:2681–2720  2011.

[17] A. Maurer and M. Pontil. Structured sparsity and generalization. Technical report  arXiv:1108.3476 

2011.

[18] M. Mørup. Applications of tensor (multiway array) factorizations and decompositions in data mining.

Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery  1(1):24–40  2011.

[19] C. Mu  B. Huang  J. Wright  and D. Goldfarb. Square deal: Lower bounds and improved relaxations for

tensor recovery. arXiv preprint arXiv:1307.5870  2013.

[20] S. Negahban  P. Ravikumar  M. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional
analysis of m-estimators with decomposable regularizers. In Advances in NIPS 22  pages 1348–1356.
2009.

[21] G. Obozinski  L. Jacob  and J.-P. Vert. Group lasso with overlaps:

Technical report  arXiv:1110.0413  2011.

the latent group lasso approach.

[22] B. Recht  M. Fazel  and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM Review  52(3):471–501  2010.

[23] M. Signoretto  L. De Lathauwer  and J. Suykens. Nuclear norms for tensors and their use for convex

multilinear estimation. Technical Report 10-186  ESAT-SISTA  K.U.Leuven  2010.

[24] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. In Proc. of the 18th Annual Conference

on Learning Theory (COLT)  pages 545–560. Springer  2005.

[25] R. Tomioka  K. Hayashi  and H. Kashima. Estimation of low-rank tensors via convex optimization.

Technical report  arXiv:1010.0789  2011.

[26] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor decompo-

sition. In Advances in NIPS 24  pages 972–980. 2011.

[27] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):279–311 

1966.

[28] R. Vershynin.

Introduction to the non-asymptotic analysis of random matrices. Technical report 

arXiv:1011.3027  2010.

9

,Ryota Tomioka
Taiji Suzuki
Jimei Yang
Scott Reed
Ming-Hsuan Yang
Honglak Lee