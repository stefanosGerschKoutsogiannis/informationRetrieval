2019,Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning,Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However  slow convergence and sample inefficiency remain challenging problems in RL  especially when handling continuous and high-dimensional state spaces. To tackle this problem  we propose a general acceleration method for model-free  off-policy deep RL algorithms by drawing the idea underlying regularized Anderson acceleration (RAA)  which is an effective approach to accelerating the solving of fixed point problems with perturbations. Specifically  we first explain how policy iteration can be applied directly with Anderson acceleration. Then we extend RAA to the case of deep RL by introducing a regularization term to control the impact of perturbation induced by function approximation errors. We further propose two strategies  i.e.  progressive update and adaptive restart  to enhance the performance. The effectiveness of our method is evaluated on a variety of benchmark tasks  including Atari 2600 and MuJoCo. Experimental results show that our approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms.,Regularized Anderson Acceleration for Off-Policy

Deep Reinforcement Learning

Wenjie Shi  Shiji Song  Hui Wu  Ya-Chu Hsu  Cheng Wu  Gao Huang‚àó

Department of Automation  Tsinghua University  Beijing  China

Beijing National Research Center for Information Science and Technology (BNRist)

{shiwj16  wuhui14  xuyz17}@mails.tsinghua.edu.cn

{shijis  wuc  gaohuang}@tsinghua.edu.cn

Abstract

Model-free deep reinforcement learning (RL) algorithms have been widely used
for a range of complex control tasks. However  slow convergence and sample
inefÔ¨Åciency remain challenging problems in RL  especially when handling con-
tinuous and high-dimensional state spaces. To tackle this problem  we propose
a general acceleration method for model-free  off-policy deep RL algorithms by
drawing the idea underlying regularized Anderson acceleration (RAA)  which is an
effective approach to accelerating the solving of Ô¨Åxed point problems with pertur-
bations. SpeciÔ¨Åcally  we Ô¨Årst explain how policy iteration can be applied directly
with Anderson acceleration. Then we extend RAA to the case of deep RL by
introducing a regularization term to control the impact of perturbation induced by
function approximation errors. We further propose two strategies  i.e.  progressive
update and adaptive restart  to enhance the performance. The effectiveness of our
method is evaluated on a variety of benchmark tasks  including Atari 2600 and
MuJoCo. Experimental results show that our approach substantially improves both
the learning speed and Ô¨Ånal performance of state-of-the-art deep RL algorithms.
The code and models are available at: https://github.com/shiwj16/raa-drl.

Introduction

1
Reinforcement learning (RL) is a principled mathematical framework for experience-based au-
tonomous learning of policies. In recent years  model-free deep RL algorithms have been applied
in a variety of challenging domains  from game playing [1  2] to robot navigation [3  4]. However 
sample inefÔ¨Åciency  i.e.  the required number of interactions with the environment is impractically
high  remains a major limitation of current RL algorithms for problems with continuous and high-
dimensional state spaces. For example  many RL approaches on tasks with low-dimensional state
spaces and fairly benign dynamics may even require thousands of trials to learn. Sample inefÔ¨Åciency
makes learning in real physical systems impractical and severely prohibits the applicability of RL
approaches in more challenging scenarios.
A promising way to improve the sample efÔ¨Åciency of RL is to learn models of the underlying system
dynamics. However  learning models of the underlying transition dynamics is difÔ¨Åcult and inevitably
leads to modelling errors. Alternatively  off-policy algorithms such as deep Q-learning (DQN) [1]
and its variants [5  6]  deep deterministic policy gradient (DDPG) [7]  soft actor-critic (SAC) [8  9]
and off-policy hierarchical RL [10]  which instead aim to reuse past experience  are commonly used
to alleviate the sample inefÔ¨Åciency problem. Unfortunately  off-policy algorithms are typically based
on policy iteration or value iteration  which repeatedly apply the Bellman operator of interest and
generally require an inÔ¨Ånite number of iterations to converge exactly to the optima. Moreover  the
Bellman iteration constructs a contraction mapping which converges asymptotically to the optimal

‚àóCorresponding auther.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

value function [11]. Iterating this mapping essentially results in a Ô¨Åxed-point problem [12] and thus
may be unacceptably slow to converge. These issues are further exacerbated when nonlinear function
approximator such as neural network is utilized or the tasks have continuous state and action spaces.
This paper explores how to accelerate the convergence or improve the sample efÔ¨Åciency for model-
free  off-policy deep RL. We make the observation that RL is closely linked to Ô¨Åxed-point iteration:
the optimal policy can be found by solving a Ô¨Åxed-point problem of associated Bellman operator.
Therefore  we attempt to embrace the idea underlying Anderson acceleration (also known as Anderson
mixing  Pulay mixing) [13  14]  which is a method capable of speeding up the computation of Ô¨Åxed-
point iterations. While the classic Ô¨Åxed-point iteration repeatedly applies the operator to the last
estimate  Anderson acceleration searches for the optimal point that has minimal residual within the
subspace spanned by several previous estimates  and then applies the operator to this optimal estimate.
Prior work [15] has successfully applied Anderson acceleration to value iteration and preliminary
experiments show a signiÔ¨Åcant speed up of convergence. However  existing application is only
feasible on simple tasks with low-dimensional  discrete state and action spaces. Besides  as far as we
know  Anderson acceleration has never been applied to deep RL due to some long-standing issues
including biases induced by sampling a minibatch and function approximation errors.
In this paper  Anderson acceleration is Ô¨Årst applied to policy iteration under a tabular setting. Then 
we propose a practical acceleration method for model-free  off-policy deep RL algorithms based
on regularized Anderson acceleration (RAA) [16]  which is a general paradigm with a Tikhonov
regularization term to control the impact of perturbations. The structure of perturbations could be the
noise injected from the outside and high-order error terms induced by a nonlinear Ô¨Åxed-point iteration
function. In the context of deep RL  function approximation errors are major perturbation source
for RAA. We present two bounds to characterize how the regularization term controls the impact of
function approximation errors. Two strategies  i.e.  progressive update and adaptive restart  are further
proposed to enhance the performance. Moreover  our acceleration method can be implemented readily
to deep RL algorithms including Dueling-DQN [5] and twin delayed DDPG (TD3) [17] to solve very
complex  high-dimensional tasks  such as Atari 2600 and MuJoCo [18] benchmarks. Finally  the
empirical results show that our approach exhibits a substantial improvement in both learning speed
and Ô¨Ånal performance over vanilla deep RL algorithms.

2 Related Work
Prior works have made a number of efforts to improve the sample efÔ¨Åciency and speed up the
convergence of deep RL from different respects  such as variance reduction [19  20]  model-based RL
[21  22  23]  guided exploration [24  25]  etc. One of the most widely used techniques is off-policy
RL  which combines temporal difference [26] and experience replay [27  28] so as to make use of
all the previous samples before each update to the policy parameters. Though introducing biases
by using previous samples  off-policy RL alleviates the high variance in estimation of Q-value and
policy gradient [29]. Consequently  fast convergence is rendered when under Ô¨Åne parameter-tuning.
As one kernel technique of off-policy RL  temporal difference is derived from the Bellman itera-
tion which can be regarded as a Ô¨Åxed-point problem [12]. Our work focuses on speeding up the
convergence of off-policy RL via speeding up the convergence of the eseential Ô¨Åxed-point problem 
and replying on a technique namely Anderson acceleration. This method is exploited by prior work
[13  30] to accelerate the Ô¨Åxed-point iteration by computing the new iteration as linear combination
of previous evaluations. In the linear case  the convergence rate of Anderson acceleration has been
elaborately analyzed and proved to be equal to or better than Ô¨Åxed-point iteration in [14]. For nonlin-
ear Ô¨Åxed-point iteration  regularized Anderson acceleration is proposed by [16] to constrain the norm
of coefÔ¨Åcient vector and reduce the impact of perturbations. Recent works [15  31] have applied the
Anderson acceleration to value iteration and deep neural network  and preliminary experiments show
that a signiÔ¨Åcant speedup of convergence is achieved. However  there is still no research showing its
acceleration effect on deep RL for complex high-dimensional problems  as far as we know.

3 Preliminaries
Under RL paradigm  the interaction between an agent and the environment is described as a Markov
Decision Process (MDP). SpeciÔ¨Åcally  at a discrete timestamp t  the agent takes an action at in a state
st and transits to a subsequent state st+1 while obtaining a reward rt = r(st  at) from the environ-
ment. The transition between states satisÔ¨Åes the Markov property  i.e.  P (st+1|st  at  . . .   s0  a0) =
P (ss+t|st  at). Usually  the RL algorithm aims to search a policy œÄ(a|s) that maximizes the expected

2

state-action pair (s  a): QœÄ(s  a) = E [(cid:80)‚àû

sum of discounted future rewards. Q-value function describes the expected return starting from a
t=0 Œ≥trt+1|s0 = s  a0 = a]  where the policy œÄ(a|s) is a
function or conditional distribution mapping the state space S to the action space A.

3.1 Off-policy reinforcement learning
Most off-policy RL algorithms are derived from policy iteration  which alternates between policy
evaluation and policy improvement to monotonically improve the policy and the value function until
convergence. For complex environments with unknown dynamics and continuous spaces  policy
iteration is generally combined with function approximation  and parameterized Q-value function (or
critic) and policy function are learned from sampled interactions with environment. Since critic is
represented as parameterized function instead of look-up table  the policy evaluation is replaced with
an optimization problem which minimizes the squared temporal difference error  the discrepancy
between the outputs of critics after and before applying the Bellman operator

L(Œ∏) = E(cid:2)((T QŒ∏(cid:48))(s  a) ‚àí QŒ∏(s  a))2(cid:3)  

(1)
where typically the Bellman operator is applied to a separate target value network QŒ∏(cid:48) whose
parameter is periodically replaced or softly updated with copy of current Q-network weight.
In off-policy RL Ô¨Åeld  prior works have proposed a number of modiÔ¨Åcations on the Bellman operator
to alleviate the overestimation or function approximation error problems and thus achieved signiÔ¨Åcant
improvement. Similar to policy improvement  DQN replaces the current policy with a greedy policy
for the next state in the Bellman operator

(2)
As the state-of-the-art actor-critic algorithm for continuous control  TD3 [17] proposes a clipped
double Q-learning variant and a target policy smoothing regularization to modify the Bellman operator 
which alleviates overestimation and overÔ¨Åtting problems 

(3)
where QŒ∏j (s  a)(j = 1  2) denote two critics with decoupled parameters Œ∏j. The added noise
 ‚àº clip(N (0  œÉ) ‚àíc  c) is clipped by the positive constant c.

QŒ∏(cid:48)

j=1 2

(cid:2)r(st  at) + Œ≥ min

(T QŒ∏(cid:48))(st  at) = Est+1 rt

a

j

QŒ∏(cid:48)(st+1  a)(cid:3).
(st+1  œÄœÜ(cid:48)(st+1) + )(cid:3) 

(cid:2)r(st  at) + Œ≥ max

(T QŒ∏(cid:48))(st  at) = Est+1 rt

3.2 Anderson acceleration for value iteration
Most RL algorithms are derived from a fundamental framework named policy iteration which consists
of two phases  i.e. policy evaluation and policy improvement. The policy evaluation estimates the
Q-value function induced by current policy by iterating a Bellman operator from an initial estimate.
Following the policy evaluation  the policy improvement acquires a better policy from a greedy
strategy  The policy iteration alternates two phases to update the Q-value and the policy respectively
until convergence. As a special variant of policy iteration  value iteration merges policy evaluation
and policy improvement into one iteration

Es(cid:48) r [r + Œ≥Vk(s(cid:48))]  ‚àÄs ‚àà S 

Vk+1(s) ‚Üê (T Vk)(s) = max

a

(4)
and iterates it until convergence from a initial V0  where the Bellman operation is only repeatedly
applied to the last estimate. Anderson acceleration is a widely used technique to speed up the
convergence of Ô¨Åxed-point iterations and has been successfully applied to speed up value iteration
[15] by linearly combining previous m (m > 1) value estimates 
i T Vk‚àím+i 
Œ±k

(5)
where the coefÔ¨Åcient vector Œ±k ‚àà Rm is determined by minimizing the norm of total Bellman
residuals of these estimates 

Vk+1 ‚Üê m(cid:88)

i=1

Œ±i(T Vk‚àím+i ‚àí Vk‚àím+i)

Œ±i = 1.

(6)

For the (cid:96)2-norm  the minimum can be analytically solved by using the Karush-Kuhn-Tucker condi-
tions. Corresponding coefÔ¨Åcient vector is given by
(‚àÜT

(7)
where ‚àÜk = [Œ¥k‚àím+1  . . . Œ¥k] ‚àà R|S|√óm is a Bellman residuals matrix with Œ¥i = T Vi ‚àí Vi ‚àà R|S| 
and 1 ‚àà Rm denotes the vector with all components equal to one [15].

k ‚àÜk)‚àí11
k ‚àÜk)‚àí11

1T (‚àÜT

Œ±k =

 

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

Œ±k = argmin
Œ±‚ààRm

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)   s.t.

m(cid:88)

i=1

3

4 Regularized Anderson Acceleration for Deep Reinforcement Learning
Our regularized Anderson acceleration (RAA) method for deep RL can be derived starting from a
direct implementation of Anderson acceleration to the classic policy iteration algorithm. We will
Ô¨Årst present this derivation to show that the resulting algorithm converges faster to the optimal policy
than the vanilla form. Then  a regularized variant is proposed for a more general case with function
approximation. Based on this theory  a progressive and practical acceleration method with adaptive
restart is presented for off-policy deep RL algorithms.

4.1 Anderson acceleration for policy iteration
As described above  Anderson acceleration can be directly applied to value iteration. However 
policy iteration is more fundamental and suitable to scale to deep RL  compared to value iteration.
Unfortunately  the implementation of Anderson acceleration is complicated when considering policy
iteration  because there is no explicit Ô¨Åxed-point mapping between the policies in any two consecutive
steps  which make it impossible to straightforwardly apply Anderson acceleration to the policy œÄ.
Due to the one-to-one mapping between policies and Q-value functions  policy iteration can be
accelerated by applying Anderson acceleration to the policy improvement  which establishes a
mapping from the current Q-value estimate to the next policy. In this section  our derivation is based
on a tabular setting  to enable theoretical analysis. SpeciÔ¨Åcally  for the prototype policy iteration 
suppose that estimates have been computed up to iteration k  and that in addition to the current
estimate QœÄk  the m ‚àí 1 previous estimates QœÄk‚àí1  ...  QœÄk‚àím+1 are also known. Then  a linear
combination of estimates QœÄi with coefÔ¨Åcients Œ±i

2 reads

Due to this equality constraint  we deÔ¨Åne combined Bellman operator Tc as follows

i=1

i=1

(8)

(9)

m(cid:88)

Qk

Œ± =

Œ±iQœÄk‚àím+i with

Œ±i = 1.

m(cid:88)

i=1

TcQk

Œ± =

Œ±iT QœÄk‚àím+i.

m(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)   s.t.

m(cid:88)

i=1

Then  one searches a coefÔ¨Åcient vector Œ±k that minimizes the following objective function J deÔ¨Åned
as the combined Bellman residuals among the entire state-action space S √ó A 

Œ±k = argmin
Œ±‚ààRm

J(Œ±) = argmin
Œ±‚ààRm

Œ±i(T QœÄk‚àím+i ‚àí QœÄk‚àím+i)

Œ±i = 1.

(10)

In this paper  we will consider the (cid:96)2-norm  although a different norm may also be feasible (for
example (cid:96)1 and (cid:96)‚àû  in which case the optimization problem becomes a linear program). The solution
to this optimization problem is identical to (7) except that ‚àÜk = [Œ¥k‚àím+1  ...  Œ¥k] ‚àà R|S√óA|√óm with
Œ¥i = T QœÄi ‚àíQœÄi ‚àà R|S√óA|. Detailed derivation can be found in Appendix A.1 of the supplementary
material. Then  the new policy improvement steps are given by

œÄk+1(s) = argmax

a

Qk

Œ±(s  a) = argmax

a

i QœÄk‚àím+1(s  a) ‚àÄs ‚àà S.
Œ±k

(11)

Meanwhile  Q-value estimate QœÄk+1 can be obtained by iteratively applying the following policy
evaluation operator by starting from some initial function Q0 

(cid:2)r + Œ≥Ea(cid:48)‚àºœÄk+1[Qi‚àí1(s(cid:48)  a(cid:48))](cid:3)  ‚àÄ(s  a) ‚àà (S A).

(12)
In fact  the effect of acceleration can be explained intuitively. The linear combination Qk
Œ± is a better
estimate of Q-value than the last one QœÄk in terms of combined Bellman residuals. Accordingly  the
policy is improved from a better policy baseline corresponding to the better estimate of Q-value.

Qi(s  a) ‚Üê Es(cid:48) r

m(cid:88)

i=1

4.2 Regularized variant with function approximation

For RL control tasks with continuous state and action spaces  or high-dimensional state space  we
generally consider the case in which Q-value function is approximated by a parameterized function
approximator. If the approximation is sufÔ¨Åciently good  it might be appropriate to use it in place of
QœÄ in (8)-(12). However  there are several key challenges when implementing Anderson acceleration
with function approximation.

2Notice that we don‚Äôt impose a positivity condition on the coefÔ¨Åcients.

4

First  notice that the Bellman residuals in (10) are calculated among the entire state-action space.
Unfortunately  sweeping entire state-action space is intractable for continuous RL  and a Ô¨Åne grained
discretization will lead to the curse of dimensionality. A feasible alternative to avoid this issue is

to use a sampled Bellman residuals matrix (cid:101)‚àÜk instead. To alleviate the bias induced by sampling a

minibatch  we adopt a large sample size NA speciÔ¨Åcally for Anderson acceleration.
Second  function approximation errors are unavoidable and lead to biased solution of Anderson
acceleration. The intricacies of this issue will be exacerbated by deep models. Therefore  function
approximation errors will induce severe perturbation when implementing Anderson acceleration to
policy iteration with function approximation. In addition to the perturbation  the solution (7) contains
the inverse of a squared Bellman residuals matrix  which may suffer from ill-conditioning when the
squared Bellman residuals matrix is rank-deÔ¨Åcient  and this is a major source of numerical instability
in vanilla Anderson acceleration. In other words  even if the perturbation is small  its impact on the
solution can be arbitrarily large.
Under the above observations  we scale the idea underlying RAA to the policy iteration with function

approximation in this section. Then  the coefÔ¨Åcient vector (10) is now adjusted to(cid:101)Œ±k that minimizes
(cid:101)Œ±k = argmin

the perturbed objective function added with a Tikhonov regularization term 

Œ±i(T QœÄk‚àím+i ‚àí QœÄk‚àím+i + ek‚àím+i)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) + Œª(cid:107)Œ±(cid:107)2   s.t.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

m(cid:88)

i=1

Œ±‚ààRm

Œ±i = 1 

(13)

where ek‚àím+i represents the perturbation induced by function approximation errors. The solution to
this regularized optimization problem can be obtained analytically similar to (10) 

where Œª is a positive scalar representing the scale of regularization. (cid:101)‚àÜk = [(cid:101)Œ¥k‚àím+1  ... (cid:101)Œ¥k] ‚àà RNA√óm
is the sampled Bellman residuals matrix with(cid:101)Œ¥i = T QœÄi ‚àí QœÄi + ei ‚àà RNA.

 

(14)

In fact  the regularization term controls the norm of coefÔ¨Åcient vector produced by RAA and reduces
the impact of perturbation induced by function approximation errors  as shown analytically by the
following proposition.
Proposition 1. Consider two identical policy iterations I1 and I2 with function approximation. I2
is implemented with regularized Anderson acceleration and takes into account approximation errors 
vectors of I1 and I2 respectively. Then  we have the following bounds

whereas I1 is only implemented with vanilla Anderson acceleration. Let Œ±k and(cid:101)Œ±k be the coefÔ¨Åcient

(cid:101)Œ±k =

((cid:101)‚àÜT
k(cid:101)‚àÜk + ŒªI)‚àí11
k(cid:101)‚àÜk + ŒªI)‚àí11
1T ((cid:101)‚àÜT

(cid:115)

(cid:107)(cid:101)Œ±k(cid:107) ‚â§

Œª + (cid:107)(cid:101)‚àÜk(cid:107)2

 

mŒª

(cid:107)(cid:101)Œ±k ‚àí Œ±k(cid:107) ‚â§ (cid:107)(cid:101)‚àÜT

k(cid:101)‚àÜk ‚àí ‚àÜT

Œª

k ‚àÜk(cid:107) + Œª

(cid:107)Œ±k(cid:107).

(15)

Proof. See Appendix A.2 of the supplementary material.

From the above bounds  we can observe that regularization allows a better control of the impact of

function approximation errors  but also causes an inevitable gap between(cid:101)Œ±k and Œ±k. Qualitatively 
overlarge Œª leads to very small norm of coefÔ¨Åcient vector (cid:101)Œ±k  which means the coefÔ¨Åcients for

large regularization scale Œª means less impact of function approximation errors. On the other hand 

previous estimates is nearly identical. However  according to (10)  equal coefÔ¨Åcients are probably far
away from the optima Œ±k and thus result in great performance loss of Anderson acceleration.

4.3 Implementation on off-policy deep reinforcement learning
As discussed in last section  it is impossible to directly use policy iteration in very large continuous
domains. To that end  most off-policy deep RL algorithms apply the mechanism underlying policy
iteration to learn approximations to both the Q-value function and the policy. Instead of iterating
policy evaluation and policy improvement to convergence  these off-policy algorithms alternate
between optimizing two networks with stochastic gradient descent. For example  actor-critic method
is a well-known implementation of this mechanism. In this section  we show that RAA for policy
iteration can be readily extended to existing off-policy deep RL algorithms for both discrete and
continuous control tasks  with only a few modiÔ¨Åcations to the update of critic.

5

Algorithm 1: RAA-Dueling-DQN Algorithm

Initialize a critic network QŒ∏ with random parameters Œ∏;
Initialize m target networks Œ∏i ‚Üê Œ∏ (i = 1  ...  m) and replay buffer D;
Initialize restart checking period Tr and maximum training steps K;
Set k = 0  c1 = 1  ‚àÜmin = inf  ‚àÜTr = 0;
while k < K do

Receive initial observation state s0;
for t = 1 to T do

Set k = k + 1  and mk = min(ck  m);
With probability Œµ select a random action at  otherwise select at = argmaxa QŒ∏(st  a);
Execute at  receive rt and st+1  store transition (st  at  rt  st+1) into D;
Sample minibatch of transitions (s  a  r  s(cid:48)) from D;

Perform Anderson acceleration steps (13)-(14) and obtain(cid:101)Œ±k  ‚àÜTr = ‚àÜTr + (cid:107)(cid:101)Œ¥k(cid:107)2

2;
Update the critic by minimizing the loss function (18) with yt equal to the RHS of (17);
Update target networks every M steps: Œ∏i ‚Üê Œ∏i+1 (i = 1  ...  m ‚àí 1) and Œ∏m ‚Üê Œ∏;
ck+1 = ck + 1;
if k mod Tr = 0 then

‚àÜmin = min(‚àÜmin  ‚àÜTr);
if ‚àÜTr > ‚àÜmin then

‚àÜmin = inf  and ck+1 = 1;

4.3.1 Regularized Anderson acceleration for actor-critic
Consider a parameterized Q-value function QŒ∏(st  at) and a tractable policy œÄœÜ(at|st)  the parameters
of these networks are Œ∏ and œÜ. In the following  we Ô¨Årst give the main results of RAA for actor-critic.
Then  RAA is combined with Dueling-DQN and TD3 respectively.
Under the paradigm of off-policy deep RL (actor-critic)  RAA variant of policy iteration (11)-(12)
degrades into the following Bellman equation

(cid:35)

QŒ∏(st  at) = Est+1 rt

rt + Œ≥

QŒ∏i(st+1  at+1)

 

(16)

i=1

i=1

at+1

(cid:35)

rt + Œ≥

m(cid:88)

m(cid:88)

QŒ∏(st  at) = Œ≤

QŒ∏i(st+1  at+1)

 

(17)

where Œ∏i is the parameters of target network before i update steps. Furthermore  to mitigate the
instability resulting from drastic update step of Anderson acceleration  the following progressive
Bellman equation (or progressive update) with RAA is used practically 

(cid:101)Œ±iQŒ∏i(st  at) + (1 ‚àí Œ≤)Est+1 rt

(cid:101)Œ±i max
LQ(Œ∏) = E(st at)‚ààD(cid:2)(QŒ∏(st  at) ‚àí yt)2(cid:3)  

where Œ≤ is a small positive coefÔ¨Åcient.
Generally  the loss function of critic is then formulated as the following squared consistency error of
Bellman equation 
(18)
where D is the distribution of previously sampled transitions  or a replay buffer. The target value of
Q-value function or critic is represented by yt.
RAA-Dueling-DQN. Different from vanilla Dueling-DQN algorithm using general Bellman equa-
tion  we instead use progressive Bellman equation with RAA (17) to update the critic. That is  yt is
the RHS of (17) for RAA-Dueling-DQN.
RAA-TD3. For the case of TD3 where an actor and two critics are learned for deterministic policy
and Q-value function respectively  the implementation of RAA is more complicated. SpeciÔ¨Åcally 
two critics QŒ∏j (j = 1  2) are simultaneously trained with clipped double Q-learning. Then  the target
values yj t(j = 1  2) for RAA-TD3 are given by

(cid:35)
(cid:101)Œ±i(cid:98)QŒ∏i (st+1  œÄœÜ(cid:48)(st+1) + )

m(cid:88)

i=1

 

(19)

m(cid:88)

(cid:101)Œ±i(cid:98)QŒ∏i (st  at) + (1 ‚àí Œ≤)Est+1 rt

yj t = Œ≤

where (cid:98)QŒ∏i(st  at) = minj=1 2 QŒ∏i

i=1

(st  at).

j

(cid:34)

m(cid:88)

i=1

at+1

(cid:101)Œ±i max
(cid:34)

(cid:34)

rt + Œ≥

6

(a) Breakout

(b) Enduro

(c) Qbert

(d) SpaceInvaders

(e) Ant-v2

(f) Hopper-v2

(g) Walker2d-v2

(h) HalfCheetah-v2

Figure 1: Learning Curves of Dueling-DQN  TD3 and their RAA variants on discrete and continuous control
tasks. The solid curves correspond to the mean and the shaded region to the standard deviation over several trials.
Curves are smoothed uniformly for visual clarity.

4.3.2 Adaptive restart
The idea of restarting an algorithm is well known in the numerical analysis literature. Vanilla
Anderson acceleration has shown substantial improvements by incorporating with periodic restarts
[30]  where one periodically starts the acceleration scheme anew by only using information from the
most recent iteration. In this section  to alleviate the problem that deep RL is notoriously prone to be
trapped in local optimum  we propose an adaptive restart strategy for our RAA method.
Among the training steps of actor-critic with RAA  periodic restart checking steps are enforced to
clear the memory immediately before the iteration completely crashes. More explicitly  the iteration
is restarted whenever the average squared residual of current period exceeds the average squared
residual of last period. Complete description of RAA-Dueling-DQN is summarized in Algorithm 1.
And RAA-TD3 is given in Appendix B of the supplementary material.

5 Experiments
In this section  we present our experimental results and discuss their implications. We Ô¨Årst give a
detailed description of the environments (Atari 2600 and MuJoCo) used to evaluate our methods.
Then  we report results on both discrete and continuous control tasks. Finally  we provide an ablative
analysis for the proposed methodology. All default hyperparameters used in these experiments are
listed in Appendix C of the supplementary material.

5.1 Experimental setup
Atari 2600. For discrete control tasks  we perform experiments in the Arcade Learning Environment.
We select four games (Breakout  Enduro  Qbert and SpaceInvaders) varying in their difÔ¨Åculty of
convergence. The agent receives 84 √ó 84 √ó 4 stacked grayscale images as inputs  as described in [1].
MuJoCo. For continuous control tasks  we conduct experiments in environments built on the
MuJoCo physics engine. We select a number of control tasks to evaluate the performance of the
proposed methodology and the baseline methods. In each task  the agent takes a vector of physical
states as input  and generates an action to manipulate the robots in the environment.

5.2 Comparative evaluation
To evaluate our RAA variant method  we select Dueling-DQN and TD3 as the baselines for discrete
and continuous control tasks  respectively. Please note that we do not select DDPG as the baseline
for continuous control tasks  as DDPG shows bad performance in difÔ¨Åcult control tasks such as
robotic manipulation. Figure 1 shows the total average return of evaluation rollouts during training
for Dueling-DQN  TD3 and their RAA variants. We train Ô¨Åve and seven different instances of each
algorithm for Atari 2600 and MuJoCo  respectively. Besides  each baseline and corresponding RAA

7

0.00.51.01.52.02.53.0%20890580050100150200250;07 0709:73#
:03
":03
"0.00.20.40.60.81.01.21.4%208905800100200300400500600700#
:03
":03
"0.00.20.40.60.81.01.21.4%2089058005001000150020002500#
:03
":03
"0.00.20.40.60.81.01.21.4%20890580100200300400500600700#
:03
":03
"0.00.20.40.60.81.0%20890580010002000300040005000;07 0709:73#
%%0.00.20.40.60.81.0%208905800500100015002000250030003500#
%%0.00.20.40.60.81.0%20890580010002000300040005000#
%%0.00.20.40.60.81.0%208905800200040006000800010000#
%%variant are trained with same random seeds set and evaluated every 10000 environment steps  where
each evaluation reports the average return over ten different rollouts.
The results in Figure 1 show that  overall  RAA variants outperform to corresponding baseline on
most tasks with a large margin such as HalfCheetah-v2 and perform comparably to them on the
easier tasks such as Enduro in terms of learning speed  which indicate that RAA is a feasible method
to make existing off-policy RL algorithms more sample efÔ¨Åcient. In addition to the direct beneÔ¨Åt
of acceleration mentioned above  we also observe that our RAA variants demonstrate superior or
comparable Ô¨Ånal performance to the baseline methods in all tasks. In fact  RAA-Dueling-DQN can
be seen as a weighted variant of Average-DQN [32]  which can effectively reduce the variance of
approximation error in the target values and thus shows improved performance. In summary  our
approach brings an improvement in both the learning speed and Ô¨Ånal performance.

5.3 Ablation studies
The results in the previous section suggest that our RAA method can improve the sample efÔ¨Åciency
of existing off-policy RL algorithms. In this section  we further examine how sensitive our approach
is to the scaling of regularization. We also perform ablation studies to understand the contribution of
each individual component: progressive update and adaptive restart. Additionally  we analyze the
impact of different number of previous estimates m and compare the behavior of our proposed RAA
method over different learning rates.
Regularization scale. Our approach is sensitive to the scaling of regularization Œª  because it control
the norm of the coefÔ¨Åcient vector and reduces the impact of approximation error. According to the
conclusions of Proposition 1  larger regularization magnitude implies less impact of approximation
error  but overlarge regularization will make the coefÔ¨Åcients nearly identical and thus result in
substantial degradation of acceleration performance. Figure 2 shows how learning performance
changes on discrete control tasks when the regularization scale is varied  and consistent conclusion
as above can be drawn from Figure 2. For continuous control tasks  it is difÔ¨Åcult to obtain same
conclusion due to the dominant effect of bias induced by sampling a minibatch relative to function
approximation errors. Additional learning curves on continuous control tasks can be found in
Appendix D of the supplementary material.

(a) Breakout

(b) Enduro

(c) Qbert

(d) SpaceInvaders

Figure 2: Sensitivity of RAA-Dueling-DQN to the scaling of regularization on discrete control tasks.

Progressive update and adaptive restart. This experiment compares our proposed approach with:
(i) RAA without using progressive update (no progressive); (ii) RAA without adding adaptive restart
(no restart); (iii) RAA without using progressive update and adding adaptive restart (no progressive
and no restart). Figure 3 shows comparative learning curves on continuous control tasks. Although the
signiÔ¨Åcance of each component varies task to task  we see that using progressive update is essential
for reducing the variance on all four tasks  consistent conclusion can also be drawn from Figure
1. Moreover  adding adaptive restart marginally improves the performance. Additional results on
discrete control tasks can be found in Appendix D of the supplementary material.

(a) Ant-v2
Figure 3: Ablation analysis of RAA-TD3 (blue) over progressive update and adaptive restart.

(c) Walker2d-v2

(b) Hopper-v2

(d) HalfCheetah-v2

8

0.00.20.40.60.81.01.21.4%208905800255075100125150175200;07 0709:730.00.20.40.60.81.01.21.4%2089058001002003004005006007000.00.20.40.60.81.01.21.4%208905800500100015002000250030000.00.20.40.60.81.01.21.4%208905802003004005006007000.00.20.40.60.81.0%2089058010002000300040005000;07 0709:730.00.20.40.60.81.0%2089058005001000150020002500300035000.00.20.40.60.81.0%208905800100020003000400050000.00.20.40.60.81.0%208905800200040006000800010000120005745480/; 7 39347089 79345747088;0345747088;0 3/347089 79The number of previous estimates m.
In our experi-
ments  the number of previous estimates m is set to 5. In
fact  there is a tradeoff between performance and compu-
tational cost. Fig.4 shows the results of RAA-TD3 using
different m(m = 1  3  5  7  9) on Walker2d task. Overall 
we can conclude that larger m leads to faster convergence
and better Ô¨Ånal performance  but the improvement be-
comes small when m exceeds a threshold. In practice  we
suggest to take into account available computing resource
and sample efÔ¨Åciency when applying our proposed RAA
method to other works.

Learning rate. To compare the behavior of our pro-
posed RAA method over different learning rates (lr)  we
perform additional experiments on Walker2d task  and the
results of TD3 and our RAA-TD3 are shown in Fig.5.
Overall  the improvement of our method is consistent
across all learning rates  though the performance of both
TD3 and our RAA-TD3 is bad under the setting with
non-optimal learning rates  and the improvement is more
signiÔ¨Åcant when the learning rate is smaller. Moreover 
consistent improvement of performance means that our
proposed RAA method is effective and robust.

6 Conclusion

Figure 4: Learning Curves of RAA-TD3 on
Walker2d-v2 with different m.

Figure 5:
Walker2d-v2 with different learning rates.

Performance comparison on

In this paper  we presented a general acceleration method for existing deep reinforcement learning
(RL) algorithms. The main idea is drawn from regularized Anderson acceleration (RAA)  which is
an effective approach to speeding up the solving of Ô¨Åxed point problems with perturbations. Our
theoretical results explain that vanilla Anderson acceleration can be directly applied to policy iteration
under a tabular setting. Furthermore  RAA is extended to model-free deep RL by introducing an
additional regularization term. Two rigorous bounds about coefÔ¨Åcient vector demonstrate that the
regularization term controls the norm of the coefÔ¨Åcient vector produced by RAA and reduces the
impact of perturbation induced by function approximation errors. Moreover  we veriÔ¨Åed that the
proposed method can signiÔ¨Åcantly accelerate off-policy deep RL algorithms such as Dueling-DQN
and TD3. The ablation studies show that progressive update and adaptive restart strategies can
enhance the performance. For future work  how to combine Anderson acceleration or its variants
with on-policy deep RL is an exciting avenue.

Acknowledgments

Gao Huang is supported in part by Beijing Academy of ArtiÔ¨Åcial Intelligence (BAAI) under
grant BAAI2019QN0106 and Tencent AI Lab Rhino-Bird Focused Research Program under grant
JR201914. This research is supported by the National Science Foundation of China (NSFC) under
grant 41427806.

References
[1] V. Mnih  K. Kavukcuoglu  D. Silver  A. A. Rusu  J. Veness  M. G. Bellemare  A. Graves  M. Riedmiller 
A. K. Fidjeland  G. Ostrovski et al.  ‚ÄúHuman-level control through deep reinforcement learning ‚Äù Nature 
vol. 518  no. 7540  p. 529  2015.

[2] D. Silver  A. Huang  C. J. Maddison  A. Guez  L. Sifre  G. Van Den Driessche  J. Schrittwieser 
I. Antonoglou  V. Panneershelvam  M. Lanctot et al.  ‚ÄúMastering the game of go with deep neural
networks and tree search ‚Äù Nature  vol. 529  no. 7587  p. 484  2016.

[3] W. Shi  S. Song  C. Wu  and C. P. Chen  ‚ÄúMulti pseudo q-learning-based deterministic policy gradient
for tracking control of autonomous underwater vehicles ‚Äù IEEE Transactions on Neural Networks and
Learning Systems  pp. 3534‚Äì3546  2018.

9

0.00.20.40.60.81.0%20890580010002000300040005000;07 0709:73#
%m#
%m#
%m#
%m#
%m0.00.20.40.60.81.0%20890580010002000300040005000;07 0709:73%lr#
%lr%lr#
%lr%lr

#
%lr

[4] P. Mirowski  R. Pascanu  F. Viola  H. Soyer  A. J. Ballard  A. Banino  M. Denil  R. Goroshin  L. Sifre 
K. Kavukcuoglu et al.  ‚ÄúLearning to navigate in complex environments ‚Äù in Proceedings of the International
Conference on Learning Representations  2017.

[5] Z. Wang  T. Schaul  M. Hessel  H. Van Hasselt  M. Lanctot  and N. De Freitas  ‚ÄúDueling network
architectures for deep reinforcement learning ‚Äù in Proceedings of the 33rd International Conference on
Machine Learning  vol. 48. PMLR  2016  pp. 1995‚Äì2003.

[6] H. Van Hasselt  A. Guez  and D. Silver  ‚ÄúDeep reinforcement learning with double q-learning ‚Äù in Thirtieth

AAAI Conference on ArtiÔ¨Åcial Intelligence  2016.

[7] T. P. Lillicrap  J. J. Hunt  A. Pritzel  N. Heess  T. Erez  Y. Tassa  D. Silver  and D. Wierstra  ‚ÄúContinuous
control with deep reinforcement learning ‚Äù in Proceedings of the International Conference on Learning
Representations  2016.

[8] T. Haarnoja  A. Zhou  P. Abbeel  and S. Levine  ‚ÄúSoft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor ‚Äù in Proceedings of the 35th International Conference on
Machine Learning  vol. 80. PMLR  2018  pp. 1861‚Äì1870.

[9] W. Shi  S. Song  and C. Wu  ‚ÄúSoft policy gradient method for maximum entropy deep reinforcement
learning ‚Äù in Proceedings of the International Joint Conference on ArtiÔ¨Åcial Intelligence  2019  pp. 3425‚Äì
3431.

[10] O. Nachum  S. S. Gu  H. Lee  and S. Levine  ‚ÄúData-efÔ¨Åcient hierarchical reinforcement learning ‚Äù in

Advances in Neural Information Processing Systems  2018  pp. 3303‚Äì3313.

[11] D. P. Bertsekas and J. N. Tsitsiklis  Neuro-dynamic programming. Athena ScientiÔ¨Åc Belmont  MA  1996 

vol. 5.

[12] A. Granas and J. Dugundji  Fixed point theory. Springer Science & Business Media  2013.
[13] H. F. Walker and P. Ni  ‚ÄúAnderson acceleration for Ô¨Åxed-point iterations ‚Äù SIAM Journal on Numerical

Analysis  vol. 49  no. 4  pp. 1715‚Äì1735  2011.

[14] A. Toth and C. Kelley  ‚ÄúConvergence analysis for anderson acceleration ‚Äù SIAM Journal on Numerical

Analysis  vol. 53  no. 2  pp. 805‚Äì819  2015.

[15] M. Geist and B. Scherrer  ‚ÄúAnderson acceleration for reinforcement learning ‚Äù arXiv preprint arX-

iv:1809.09501  2018.

[16] D. Scieur  A. d‚ÄôAspremont  and F. Bach  ‚ÄúRegularized nonlinear acceleration ‚Äù in Advances In Neural

Information Processing Systems  2016  pp. 712‚Äì720.

[17] S. Fujimoto  H. van Hoof  and D. Meger  ‚ÄúAddressing function approximation error in actor-critic methods ‚Äù
in Proceedings of the 35th International Conference on Machine Learning  vol. 80. PMLR  2018  pp.
1587‚Äì1596.

[18] E. Todorov  T. Erez  and Y. Tassa  ‚ÄúMujoco: A physics engine for model-based control ‚Äù in 2012 IEEE/RSJ

International Conference on Intelligent Robots and Systems  2012  pp. 5026‚Äì5033.

[19] E. Greensmith  P. L. Bartlett  and J. Baxter  ‚ÄúVariance reduction techniques for gradient estimates in
reinforcement learning ‚Äù Journal of Machine Learning Research  vol. 5  no. Nov  pp. 1471‚Äì1530  2004.
[20] J. Schulman  P. Moritz  S. Levine  M. Jordan  and P. Abbeel  ‚ÄúHigh-dimensional continuous control
using generalized advantage estimation ‚Äù in Proceedings of the International Conference on Learning
Representations  2016.

[21] M. Deisenroth and C. E. Rasmussen  ‚ÄúPilco: A model-based and data-efÔ¨Åcient approach to policy search ‚Äù

in Proceedings of the 28th International Conference on Machine Learning  2011  pp. 465‚Äì472.

[22] G. Williams  N. Wagener  B. Goldfain  P. Drews  J. M. Rehg  B. Boots  and E. A. Theodorou  ‚ÄúInformation
theoretic mpc for model-based reinforcement learning ‚Äù in 2017 IEEE International Conference on Robotics
and Automation  2017  pp. 1714‚Äì1721.

[23] J. Buckman  D. Hafner  G. Tucker  E. Brevdo  and H. Lee  ‚ÄúSample-efÔ¨Åcient reinforcement learning with
stochastic ensemble value expansion ‚Äù in Advances in Neural Information Processing Systems  2018  pp.
8224‚Äì8234.

[24] S. Levine and P. Abbeel  ‚ÄúLearning neural network policies with guided policy search under unknown

dynamics ‚Äù in Advances in Neural Information Processing Systems  2014  pp. 1071‚Äì1079.

[25] Y. Chebotar  M. Kalakrishnan  A. Yahya  A. Li  S. Schaal  and S. Levine  ‚ÄúPath integral guided policy

search ‚Äù in 2017 IEEE International Conference on Robotics and Automation  2017  pp. 3381‚Äì3388.

[26] R. S. Sutton  ‚ÄúLearning to predict by the methods of temporal differences ‚Äù Machine learning  vol. 3  no. 1 

pp. 9‚Äì44  1988.

[27] L.-J. Lin  ‚ÄúReinforcement learning for robots using neural networks ‚Äù Carnegie-Mellon Univ Pittsburgh PA

School of Computer Science  Tech. Rep.  1993.

10

[28] Z. Wang  V. Bapst  N. Heess  V. Mnih  R. Munos  K. Kavukcuoglu  and N. de Freitas  ‚ÄúSample efÔ¨Å-
cient actor-critic with experience replay ‚Äù in Proceedings of the International Conference on Learning
Representations  2017.

[29] S. Gu  T. Lillicrap  Z. Ghahramani  R. E. Turner  and S. Levine  ‚ÄúQ-prop: Sample-efÔ¨Åcient policy gradient
with an off-policy critic ‚Äù in Proceedings of the International Conference on Learning Representations 
2017.

[30] N. C. Henderson and R. Varadhan  ‚ÄúDamped anderson acceleration with restarts and monotonicity control
for accelerating em and em-like algorithms ‚Äù Journal of Computational and Graphical Statistics  pp. 1‚Äì42 
2019.

[31] G. Xie  Y. Wang  S. Zhou  and Z. Zhang  ‚ÄúInterpolatron: Interpolation or extrapolation schemes to

accelerate optimization for deep neural networks ‚Äù arXiv preprint arXiv:1805.06753  2018.

[32] O. Anschel  N. Baram  and N. Shimkin  ‚ÄúAveraged-dqn: Variance reduction and stabilization for deep
reinforcement learning ‚Äù in Proceedings of the 34th International Conference on Machine Learning  vol. 70 
2017  pp. 176‚Äì185.

11

,Wenjie Shi
Shiji Song
Hui Wu
Ya-Chu Hsu
Cheng Wu
Gao Huang