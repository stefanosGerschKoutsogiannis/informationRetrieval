2019,Escaping from saddle points on Riemannian manifolds,We consider minimizing a nonconvex  smooth function $f$ on a Riemannian manifold $\mathcal{M}$. We show that a perturbed version of the gradient descent algorithm converges to a second-order stationary point for this problem (and hence is able to escape saddle points on the manifold). While the unconstrained problem is well-studied  our result is the first to prove such a rate for  nonconvex  manifold-constrained problems.
The rate of convergence depends as $1/\epsilon^2$ on the accuracy $\epsilon$  which matches a rate known only for unconstrained smooth minimization. The convergence rate also has a  polynomial dependence on the parameters denoting the curvature of the manifold and the smoothness of the function.,Escaping from saddle points on Riemannian

manifolds

Yue Sun

University of Washington

Seattle  WA 98105
yuesun@uw.edu

Nicolas Flammarion

EPFL

Lausanne  Switzerland

nicolas.flammarion@epfl.ch

Maryam Fazel

University of Washington

Seattle  WA 98105
mfazel@uw.edu

Abstract

We consider minimizing a nonconvex  smooth function f on a Riemannian man-
ifold M. We show that a perturbed version of Riemannian gradient descent
algorithm converges to a second-order stationary point (and hence is able to escape
saddle points on the manifold). The rate of convergence depends as 1/2 on the
accuracy   which matches a rate known only for unconstrained smooth minimiza-
tion. The convergence rate depends polylogarithmically on the manifold dimension
d  hence is almost dimension-free. The rate also has a polynomial dependence
on the parameters describing the curvature of the manifold and the smoothness of
the function. While the unconstrained problem (Euclidean setting) is well-studied 
our result is the ﬁrst to prove such a rate for nonconvex  manifold-constrained
problems.

Introduction

1
We consider minimizing a non-convex smooth function on a smooth manifold M 

minimizex∈M f (x) 

(1)
where M is a d-dimensional smooth manifold1  and f is twice differentiable  with a Hessian that
is ρ-Lipschitz (assumptions are formalized in section 4). This framework includes a wide range of
fundamental problems (often non-convex)  such as PCA (Edelman et al.  1998)  dictionary learning
(Sun et al.  2017)  low rank matrix completion (Boumal & Absil  2011)  and tensor factorization
(Ishteva et al.  2011). Finding the global minimum to Eq. (1) is in general NP-hard; our goal is to
ﬁnd an approximate second order stationary point with ﬁrst order optimization methods. We are
interested in ﬁrst-order methods because they are extremely prevalent in machine learning  partly
because computing Hessians is often too costly. It is then important to understand how ﬁrst-order
methods fare when applied to nonconvex problems  and there has been a wave of recent interest on
this topic since (Ge et al.  2015)  as reviewed below.
In the Euclidean space  it is known that with random initialization  gradient descent avoids saddle
points asymptotically (Pemantle  1990; Lee et al.  2016). Lee et al. (2017) (section 5.5) show that this
is also true on smooth manifolds  although the result is expressed in terms of nonstandard manifold
smoothness measures. Also  importantly  this line of work does not give quantitative rates for the
algorithm’s behaviour near saddle points.

1Here d is the dimension of the manifold itself; we do not consider M as a submanifold of a higher

dimensional space. For instance  if M is a 2-dimensional sphere embedded in R3  its dimension is d = 2.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Du et al. (2017) show gradient descent can be exponentially slow in the presence of saddle points. To
alleviate this phenomenon  it is shown that for a β-gradient Lipschitz  ρ-Hessian Lipschitz function 
et al.  2017a) converges to ( −√
cubic regularization (Carmon & Duchi  2017) and perturbed gradient descent (Ge et al.  2015; Jin
ρ) local minimum 2 in polynomial time  and momentum based
method accelerates (Jin et al.  2017b). Much less is known about inequality constraints: Nouiehed
et al. (2018) and Mokhtari et al. (2018) discuss second order convergence for general inequality-
constrained problems  where they need an NP-hard subproblem (checking the co-positivity of a
matrix) to admit a polynomial time approximation algorithm. However such an approximation exists
only under very restrictive assumptions.
An orthogonal line of work is optimization on Riemannian manifolds. Absil et al. (2009) provide
comprehensive background  showing how algorithms such as gradient descent  Newton and trust
region methods can be implemented on Riemannian manifolds  together with asymptotic convergence
guarantees to ﬁrst order stationary points. Zhang & Sra (2016) provide global convergence guarantees
for ﬁrst order methods when optimizing geodesically convex functions. Bonnabel (2013) obtains the
ﬁrst asymptotic convergence result for stochastic gradient descent in this setting  which is further
extended by Tripuraneni et al. (2018); Zhang et al. (2016); Khuzani & Li (2017). If the problem is
non-convex  or the Riemannian Hessian is not positive deﬁnite  one can use second order methods
to escape from saddle points. Boumal et al. (2016a) shows that Riemannian trust region method
converges to a second order stationary point in polynomial time (see  also  Kasai & Mishra  2018; Hu
et al.  2018; Zhang & Zhang  2018). But this method requires a Hessian oracle  whose complexity is
d times more than computing gradient. In Euclidean space  trust region subproblem can be sometimes
solved via a Hessian-vector product oracle  whose complexity is about the same as computing
gradients. Agarwal et al. (2018) discuss its implementation on Riemannian manifolds  but not clear
about the complexity and sensitivity of Hessian vector product oracle on manifold.
The study of the convergence of gradient descent for non-convex Riemannian problems is previously
done only in the Euclidean space by modeling the manifold with equality constraints. Ge et al. (2015 
Appendix B) prove that stochastic projected gradient descent methods converge to second order
stationary points in polynomial time (here the analysis is not geometric  and depends on the algebraic
representation of the equality constraints). Sun & Fazel (2018) proves perturbed projected gradient
descent converges with a comparable rate to the unconstrained setting (Jin et al.  2017a) (polylog in
dimension). The paper applies projections from the ambient Euclidean space to the manifold and
analyzes the iterations under the Euclidean metric. This approach loses the geometric perspective
enabled by Riemannian optimization  and cannot explain convergence rates in terms of inherent
quantities such as the sectional curvature of the manifold.
Contributions. We provide convergence guarantees for perturbed ﬁrst order Riemannian optimization
methods to seond-order stationary points (local minima). We prove that as long as the function is
appropriately smooth and the manifold has bounded sectional curvature  a perturbed Riemannian
gradient descent algorithm escapes (an approximate) saddle points with a rate of 1/2  a polylog
dependence on the dimension of the manifold (hence almost dimension-free)  and a polynomial
dependence on the smoothness and curvature parameters. This is the ﬁrst result showing such a rate
for Riemannian optimization  and the ﬁrst to relate the rate to geometric parameters of the manifold.
Despite analogies with the unconstrained (Euclidean) analysis and with the Riemannian optimization
literature  the technical challenge in our proof goes beyond combining two lines of work: we need to
analyze the interaction between the ﬁrst-order method and the second order structure of the manifold
to obtain second-order convergence guarantees that depend on the manifold curvature. Unlike in
Euclidean space  the curvature affects the Taylor approximation of gradient steps. On the other hand 
unlike in the local rate analysis in ﬁrst-order Riemannian optimization  our second-order analysis
requires more reﬁned properties of the manifold structure (whereas in prior work  ﬁrst order oracle
makes enough progress for a local convergence rate proof  see Lemma 1)  and second order algorithms
such as (Boumal et al.  2016a) use second order oracles (Hessian evaluation). See section 4 for further
discussion.

2deﬁned as x satisfying (cid:107)∇f (x)(cid:107) ≤   λmin∇2f (x) ≥ −√

ρ

2

2 Notation and Background
We consider a complete3  smooth  d dimensional Riemannian manifold (M  g)  equipped with a
Riemannian metric g  and we denote by TxM its tangent space at x ∈ M (which is a vector space
of dimension d). We also denote by Bx(r) = {v ∈ TxM (cid:107)v(cid:107) ≤ r} the ball of radius r in TxM
centered at 0. At any point x ∈ M  the metric g induces a natural inner product on the tangent space
denoted by (cid:104)· ·(cid:105) : TxM × TxM → R. We also consider the Levi-Civita connection ∇ (Absil et al. 
2009  Theorem 5.3.1). The Riemannian curvature tensor is denoted by R(x)[u  v] where x ∈ M 
u  v ∈ TxM and is deﬁned in terms of the connection ∇ (Absil et al.  2009  Theorem 5.3.1). The
sectional curvature K(x)[u  v] for x ∈ M and u  v ∈ TxM is then deﬁned in Lee (1997  Prop. 8.8).

K(x)[u  v] =

(cid:104)R(x)[u  v]u  v(cid:105)
(cid:104)u  u(cid:105)(cid:104)v  v(cid:105) − (cid:104)u  v(cid:105)2   x ∈ M  u  v ∈ TxM.

x (y)  which satisﬁes d(x  y) = (cid:107)Exp−1

x (y)(cid:107). Parallel translation Γy

Denote the distance (induced by the Riemannian metric) between two points in M by d(x  y). A
geodesic γ : R → M is a constant speed curve whose length is equal to d(x  y)  so it is the shortest
path on manifold linking x and y. γx→y denotes the geodesic from x to y (thus γx→y(0) = x and
γx→y(1) = y).
The exponential map Expx(v) maps v ∈ TxM to y ∈ M such that there exists a geodesic γ with
dt γ(0) = v. The injectivity radius at point x ∈ M is the maximal radius
γ(0) = x  γ(1) = y and d
r for which the exponential map is a diffeomorphism on Bx(r) ⊂ TxM. The injectivity radius of
the manifold  denoted by I  is the inﬁmum of the injectivity radii at all points. Since the manifold
is complete  we have I > 0. When x  y ∈ M satisﬁes d(x  y) ≤ I  the exponential map admits
an inverse Exp−1
x denotes a the
xv ∈ TyM along γx→y such that the vector stays constant by
map which transports v ∈ TxM to Γy
satisfying a zero-acceleration condition (Lee  1997  equation (4.13)).
For a smooth function f : M → R  gradf (x) ∈ TxM denotes the Riemannian gradient of f at
x ∈ M which satisﬁes d
dt f (γ(t)) = (cid:104)γ(cid:48)(t)  gradf (x)(cid:105) (see Absil et al.  2009  Sec 3.5.1 and (3.31)).
The Hessian of f is deﬁned jointly with the Riemannian structure of the manifold. The (directional)
Hessian is H(x)[ξx] := ∇ξx gradf  and we use H(x)[u  v] := (cid:104)u  H(x)[v](cid:105) as a shorthand. We call
x ∈ M an ( −√
ρ. We refer the
interested reader to Do Carmo (2016) and Lee (1997) which provide a thorough review on these
important concepts of Riemannian geometry.
3 Perturbed Riemannian gradient algorithm
Our main Algorithm 1 runs as follows:

ρ) saddle point when (cid:107)∇f (x)(cid:107) ≤  and λmin(H(x)) ≤ −√

1. Check the norm of the gradient: If it is large  do one step of Riemannian gradient descent 

consequently the function value decreases.

2. If the norm of gradient is small  it’s either an approximate saddle point or a local minimum.
Perturb the variable by adding an appropriate level of noise in its tangent space  map it back
to the manifold and run a few iterations.
(a) If the function value decreases  iterates are escaping from the approximate saddle point

(and the algorithm continues)

(b) If the function value does not decrease  then it is an approximate local minimum (the

algorithm terminates).

Algorithm 1 relies on the manifold’s exponential map  and is useful for cases where this map is
easy to compute4. We refer readers to Lee (1997  pp. 81-86) for the exponential map of sphere and
hyperbolic manifolds  and Absil et al. (2009  Example 5.4.2  5.4.3) for the Stiefel and Grassmann
manifolds. If the exponential map is not computable  the algorithm can use a retraction5 instead 

3Since our results are local  completeness is not necessary and our results can be easily generalized  with

extra assumptions on the injectivity radius.

4Numerous interesting manifolds have closed-form exponential maps: the Grassmannian manifold  the Stiefel
manifold  the Minkowski space  the hyperbolic space  SE(n)  SO(n)...(see  Miolane et al. (2018); Boumal et al.
(2014) and their open-source packages and Bécigneul & Ganea (2019  Sec 5) for an example in NLP).
5A retraction is a ﬁrst-order approximation of the exponential map which is often easier to compute.

3

Algorithm 1 Perturbed Riemannian gradient algorithm
Require: Initial point x0 ∈ M  parameters β  ρ  K  I  accuracy   probability of success δ (parame-

ters deﬁned in Assumptions 1  2  3 and assumption of Theorem 1).
Set constants: ˆc ≥ 4  C := C(K  β  ρ) (deﬁned in Lemma 2 and proof of Lemma 8)
  χ = 3 max{log( dβ(f (x0)−f∗)

cmax ≤ 1

and

√

√

56ˆc2   r =

cmax
χ2

(cid:113) 3

√

ρ   gthres =

cmax
χ2

ˆc2δ

)  4}.
  tthres = χ
cmax

ρ  tnoise = −tthres−1.
β√

tnoise ← t  ˜xt ← xt  xt ← Expxt(ξt)  ξt uniformly sampled from Bxt(r) ⊂ TxM.

Set threshold values: fthres = cmax
χ3
Set stepsize: η = cmax
β .
while 1 do

if (cid:107)gradf (xt)(cid:107) ≤ gthres and t − tnoise > tthres then
end if
if t − tnoise = tthres and f (xt) − f (˜xtnoise) > −fthres then
end if
xt+1+ ← Expxt(− min{η 
t ← t + 1.

(cid:107)gradf (xt)(cid:107)}gradf (xt)).

output ˜xtnoise

I

end while

however our current analysis only covers the case of the exponential map. In Figure 1  we illustrate a
function with saddle point on sphere  and plot the trajectory of Algorithm 1 when it is initialized at a
saddle point.

1−x2

2 +4x2

Figure 1: Function f with saddle point on a sphere. f (x) = x2
3. We plot the contour of this
function on unit sphere. Algorithm 1 initializes at x0 = [1  0  0] (a saddle point)  perturbs it towards
x1 and runs Riemannian gradient descent  and terminates at x∗ = [0 −1  0] (a local minimum). We
amplify the ﬁrst iteration to make saddle perturbation visible.
4 Main theorem: escape rate for perturbed Riemannian gradient descent
We now turn to our main results  beginning with our assumptions and a statement of our main theorem.
We then develop a brief proof sketch.
Our main result involves two conditions on function f and one on the curvature of the manifold M.
Assumption 1 (Lipschitz gradient). There is a ﬁnite constant β such that

(cid:107)gradf (y) − Γy

xgradf (x)(cid:107) ≤ βd(x  y)

for all x  y ∈ M.

Assumption 2 (Lipschitz Hessian). There is a ﬁnite constant ρ such that

(cid:107)H(y) − Γy

xH(x)Γx

y(cid:107)2 ≤ ρd(x  y)

for all x  y ∈ M.

Assumption 3 (Bounded sectional curvature). There is a ﬁnite constant K such that

|K(x)[u  v]| ≤ K for all x ∈ M and u  v ∈ TxM

K is an intrinsic parameter of the manifold capturing the curvature. We list a few examples here: (i)
A sphere of radius R has a constant sectional curvature K = 1/R2 (Lee  1997  Theorem 1.9). If

4

property of manifold. If the manifold is a sphere(cid:80)d+1

the radius is bigger  K is smaller which means the sphere is less curved; (ii) A hyper-bolic space
R of radius R has K = −1/R2 (Lee  1997  Theorem 1.9); (iii) For sectional curvature of the
H n
Stiefel and the Grasmann manifolds  we refer readers to Rapcsák (2008  Section 5) and Wong (1968) 
respectively.
Note that the constant K is not directly related to the RLICQ parameter R deﬁned by Ge et al. (2015)
which ﬁrst requires describing the manifold by equality constraints. Different representations of the
same manifold could lead to different curvature bounds  while sectional curvature is an intrinsic
i = R2  then K = 1/R2  but more generally
there is no simple connection. The smoothness parameters we assume are natural compared to some
quantity from complicated compositions Lee et al. (2017) (Section 5.5) or pullback (Zhang & Zhang 
2018). With these assumptions  the main result of this paper is the following:
Theorem 1. Under Assumptions 1 2 3  let C(K  β  ρ) be a function deﬁned in Lemma 2  ˆρ =
max{ρ  C(K  β  ρ)}  if  satisﬁes that

(cid:18) Iˆρ
(cid:18) dβ√
(2)
where c2(K)  c3(K) are deﬁned in Lemma 4  then with probability 1 − δ  perturbed Riemannian
gradient descent with step size cmax/β converges to a ( −√
(cid:18) βd(f (x0) − f (x∗))

log

(cid:18) dβ√
(cid:19)(cid:33)

56 max{c2(K)  c3(K)}ηβ

β(f (x0) − f (x∗))

ˆρ)-stationary point of f in

(cid:19)(cid:19)2(cid:41)

√
12ˆc

ηβ

 ≤ min

(cid:19)

 

i=1 x2

log

ˆρδ

(cid:40)

ˆρδ

ˆρ

(cid:32)

O

log4

2

2δ

iterations.
Proof roadmap. For a function satisfying smoothness condition (Assumption 1 and 2)  we use a
local upper bound of the objective based on the third-order Taylor expansion (see supplementary
material Section A for a review) 

f (u) ≤ f (x) + (cid:104)gradf (x)  Exp−1

x (u)(cid:105) +

H(x)[Exp−1

x (u)  Exp−1

x (u)] +

(cid:107)Exp−1

x (u)(cid:107)3.

ρ
6

1
2

1

I

When the norm of the gradient is large (not near a saddle)  the following lemma guarantees the
decrease of the objective function in one iteration.
Lemma 1. (Boumal et al.  2018) Under Assumption 1  by choosing ¯η = min{η 
(cid:107)gradf (u)(cid:107)} =
O(1/β)  the Riemannian gradient descent algorithm is monotonically descending  f (u+)≤ f (u)−
2 ¯η(cid:107)gradf (u)(cid:107)2.
Thus our main challenge in proving the main theorem is the Riemannian gradient behaviour at an
approximate saddle point:
1. Similar to the Euclidean case studied by Jin et al. (2017a)  we need to bound the “thickness” of the
“stuck region” where the perturbation fails. We still use a pair of hypothetical auxiliary sequences and
study the “coupling” sequences. When two perturbations couple in the thinnest direction of the stuck
region  their distance grows and one of them escapes from saddle point.
2. However our iterates are evolving on a manifold rather than a Euclidean space  so our strategy is to
map the iterates back to an appropriate ﬁxed tangent space where we can use the Euclidean analysis.
This is done using the inverse of the exponential map and various parallel transports.
3. Several key challenges arise in doing this. Unlike Jin et al. (2017a)  the structure of the manifold
interacts with the local approximation of the objective function in a complicated way. On the other
hand  unlike recent work on Riemannian optimization by Boumal et al. (2016a)  we do not have
access to a second order oracle and we need to understand how the sectional curvature and the
injectivity radius (which both capture intrinsic manifold properties) affect the behavior of the ﬁrst
order iterates.
4. Our main contribution is to carefully investigate how the various approximation errors arising
from (a) the linearization of the iteration couplings and (b) their mappings to a common tangent
space can be handled on manifolds with bounded sectional curvature. We address these challenges in
a sequence of lemmas (Lemmas 3 through 6) we combine to linearize the coupling iterations in a
common tangent space and precisely control the approximation error. This result is formally stated in
the following lemma.

5

(a)

(b)

xy step in TxM  and map to manifold. Expz(Γz

Figure 2: (a) Eq. (5). First map w and w+ to TuM and Tu+M  and transport the two vectors to
TxM  and get their relation. (b) Lemma 3 bounds the difference of two steps starting from x: (1)
take y + a step in TxM and map it to manifold  and (2) take a step in TxM  map to manifold  call it
z  and take Γz
δ ). Let us consider x be a ( −√
Lemma 2. Deﬁne γ =
ˆρ)
saddle point  and deﬁne u+ = Expu(−ηgradf (u)) and w+ = Expw(−ηgradf (w)). Under
Assumptions 1  2  3  if all pairwise distances between u  w  u+  w+  x are less than 12S   then for
some explicit constant C(K  ρ  β) depending only on K  ρ  β  there is
x (u+) − (I − ηH(x))(Exp−1
(cid:107)Exp−1
≤ C(K  ρ  β)d(u  w) (d(u  w) + d(u  x) + d(w  x)) .

xy) is close to Expx(y + a).

x (w+) − Exp−1

γ   and S =

x (w) − Exp−1

x (u))(cid:107)

ˆρ  κ = β

−1( dκ

ˆρ log

√

√

ηβ γ

(3)

The proof of this lemma includes novel contributions by strengthen known result (Lemmas 3) and
also combining known inequalities in novel ways (Lemmas 4 to 6) that allow us to control all the
approximation errors and arrive at the tight rate of escape for the algorithm.
5 Proof of Lemma 2
Lemma 2 controls the error of the linear approximation of the iterates when mapped in TxM. In this
section  we assume that all points are within a region of diameter R := 12S ≤ I (inequality follows
from Eq. (2) )  i.e.  the distance of any two points in the following lemmas are less than R. The proof
of Lemma 2 is based on the sequence of following lemmas.
Lemma 3. Let x ∈ M and y  a ∈ TxM. Let us denote by z = Expx(a) then under Assumption 3
(4)

xy)) ≤ c1(K) min{(cid:107)a(cid:107) (cid:107)y(cid:107)}((cid:107)a(cid:107) + (cid:107)y(cid:107))2.

d(Expx(y + a)  Expz(Γz

This lemma tightens the result of Karcher (1977  C2.3)  which only shows an upper-bound
O((cid:107)a(cid:107)((cid:107)a(cid:107) + (cid:107)y(cid:107))2). We prove the upper-bound O((cid:107)y(cid:107)((cid:107)a(cid:107) + (cid:107)y(cid:107))2) in the supplement. We
also need the following lemma showing that both the exponential map and its inverse are Lipschitz.
Lemma 4. Let x  y  z ∈ M  and the distance of each two points is no bigger than R. Then under
assumption 3

(1 + c2(K)R2)−1d(y  z) ≤ (cid:107)Exp−1

x (y) − Exp−1

x (z)(cid:107) ≤ (1 + c3(K)R2)d(y  z).

Intuitively this lemma relates the norm of the difference of two vectors of TxM to the distance
between the corresponding points on the manifold M and follows from bounds on the Hessian of the
square-distance function (Sakai  1996  Ex. 4 p. 154). The upper-bound is directly proven by Karcher
(1977  Proof of Cor. 1.6)  and we prove the lower-bound via Lemma 3 in the supplement.
The following contraction result is fairly classical and is proven using the Rauch comparison theorem
from differential geometry (Cheeger & Ebin  2008).
Lemma 5. (Mangoubi et al.  2018  Lemma 1) Under Assumption 3  for x  y ∈ M and w ∈ TxM 

d(Expx(w)  Expy(Γy

xw)) ≤ c4(K)d(x  y).

Finally we need the following corollary of the Ambrose-Singer theorem (Ambrose & Singer  1953).
Lemma 6. (Karcher  1977  Section 6) Under Assumption 3  for x  y  z ∈ M and w ∈ TxM 

(cid:107)Γz

xw − Γz

xw(cid:107) ≤ c5(K)d(x  y)d(y  z)(cid:107)w(cid:107).

yΓy

6

Lemma 3 through 6 are mainly proven in the literature  and we make up the missing part in Supple-
mentary material Section B. Then we prove Lemma 2 in Supplementary material Section B.
The spirit of the proof is to linearize the manifold using the exponential map and its inverse 
and to carefully bounds the various error terms caused by the approximation. Let us denote by
θ = d(u  w) + d(u  x) + d(w  x).
1. We ﬁrst show using twice Lemma 3 and Lemma 5 that
d(Expu(Exp−1
2. We use Lemma 4 to linearize this iteration in TuM as

wgradf (w))  Expu(−ηgradf (u) + Γu

(w+))) = O(θd(u  w)).

u (w) − ηΓu

Exp−1

u+

u+

u (w) + η[gradf (u) − Γu

wgradf (w)](cid:107) = O(θd(u  w)).

(cid:107)Γu

(w+) − Exp−1
3. Using the Hessian Lipschitzness

Exp−1

u+

u+

(cid:107)Γu

u+

Exp−1

u+

(w+)) − Exp−1

u (w) + ηH(u)Exp−1

u (w)(cid:107) = O(θd(u  w)).

3. We use Lemma 6 to map to TxM and the Hessian Lipschitzness to compare H(u) to H(x). This
is an important intermediate result (see Lemma 1 in Supplementary material Section B).
u (w)(cid:107) = O(θd(u  w)).

u (w) + ηH(x)Γx

(w+) − Γx

uExp−1

uExp−1

Exp−1

(cid:107)Γx

(5)

u+

u+

4. We use Lemma 3 and 4 to approximate two iteration updates in TxM.

(cid:107)Exp−1

x (w) − (Exp−1

x (u) + Γx

uExp−1

u (w))(cid:107) ≤ O(θd(u  w)).

(6)

And same for the u+  w+ pair replacing u  w.
5. Combining Eq. (5) and Eq. (6) together  we obtain

(cid:107)Exp−1

x (w+) − Exp−1

x (u+) − (I − ηH(x))(Exp−1

x (w) − Exp−1

x (u))(cid:107) ≤ O(θd(u  w)).

x (ut) and Exp−1

x (·) to map them to the same tangent space at x.

Now note that  the iterations u  u+  w  w+ of the algorithm are both on the manifold. We use
Exp−1
Therefore we have linearized the two coupled trajectories Exp−1
x (wt) in a common
tangent space  and we can modify the Euclidean escaping saddle analysis thanks to the error bound
we proved in Lemma 2.
6 Proof of main theorem
In this section we suppose all assumptions in Section 4 hold. The proof strategy is to show with
high probability that the function value decreases of F in T iterations at an approximate saddle
point. Lemma 7 suggests that  if after a perturbation and T steps  the iterate is Ω(S ) far from the
approximate saddle point  then the function value decreases. If the iterates do not move far  the
perturbation falls in a stuck region. Lemma 8 uses a coupling strategy  and suggests that the width of
the stuck region is small in the negative eigenvector direction of the Riemannian Hessian.
Deﬁne

F = ηβ

γ3
ˆρ2 log

−3(

dκ
δ

γ2
ˆρ

−2(

log

dκ
δ

)  T =

log( dκ
δ )

.

ηγ

At an approximate saddle point ˜x  let y be in the neighborhood of ˜x where d(y  ˜x) ≤ I  denote

)  G =(cid:112)ηβ

˜fy(x) := f (y) + (cid:104)gradf (y)  Exp−1

y (˜x)(cid:105) +

Γy
˜xH(˜x)Γ˜x

y[Exp−1

y (˜x)  Exp−1

y (˜x)].

1
2

Let (cid:107)gradf (˜x)(cid:107) ≤ G and λmin(H(˜x)) ≤ −γ. We consider two iterate sequences  u0  u1  ... and
w0  w1  ... where u0  w0 are two perturbations at ˜x.
Lemma 7. Assume Assumptions 1  2  3 and Eq. (2) hold. There exists a constant cmax  ∀ˆc > 3  δ ∈
e ]  for any u0 with d(˜x  u0) ≤ 2S /(κ log( dκ
(0  dκ

(cid:110)

(cid:110)
t| ˜fu0(ut) − f (u0) ≤ −3F(cid:111)

δ ))  κ = β/γ.

  ˆcT(cid:111)

 

T = min

inf
t

then ∀η ≤ cmax/β  we have ∀0 < t < T   d(u0  ut) ≤ 3(ˆcS ).

7

Lemma 8. Assume Assumptions 1  2  3 and Eq. (2) hold. Take two points u0 and w0 which are
perturbed from an approximate saddle point  where d(˜x  u0) ≤ 2S /(κ log( dκ
˜x (w0) −
Exp−1
d)  1]  and the algorithm
runs two sequences {ut} and {wt} starting from u0 and w0. Denote

˜x (u0) = µre1  e1 is the smallest eigenvector6 of H(˜x)  µ ∈ [δ/(2

δ ))  Exp−1

√

(cid:110)

(cid:110)
t| ˜fw0(wt) − f (w0) ≤ −3F(cid:111)

  ˆcT(cid:111)

 

T = min

inf
t

then ∀η ≤ cmax/l  if ∀0 < t < T   d(˜x  ut) ≤ 3(ˆcS )  we have T < ˆcT .
We prove Lemma 7 and 8 in supplementary material Section C. We also prove  in the same section 
the main theorem using the coupling strategy of Jin et al. (2017a). but with the additional difﬁculty of
taking into consideration the effect of the Riemannian geometry (Lemma 2) and the injectivity radius.

7 Examples
kPCA. We consider the kPCA problem  where we want to ﬁnd the k ≤ n principal eigenvectors of
a symmetric matrix H ∈ Rn×n  as an example (Tripuraneni et al.  2018). This corresponds to

min

X∈Rn×k

− 1
2

tr(X T HX)

subject to X T X = I 

which is an optimization problem on the Grassmann manifold deﬁned by the constraint X T X = I.
If the eigenvalues of H are distinct  we denote by v1 ... vn the eigenvectors of H  corresponding to
eigenvalues with decreasing order. Let V ∗ = [v1  ...  vk] be the matrix with columns composed of the
top k eigenvectors of H  then the local minimizers of the objective function are V ∗G for all unitary
matrices G ∈ Rk×k. Denote also by V = [vi1   ...  vik ] the matrix with columns composed of k
distinct eigenvectors  then the ﬁrst order stationary points of the objective function (with Riemannian
gradient being 0) are V G for all unitary matrices G ∈ Rk×k. In our numerical experiment  we choose
H to be a diagonal matrix H = diag(0  1  2  3  4) and let k = 3. The Euclidean basis (ei) are an
eigenbasis of H and the ﬁrst order stationary points of the objective function are [ei1   ei2   ei3]G with
distinct basis and G being unitary. The local minimizers are [e3  e4  e5]G. We start the iteration at
X0 = [e2  e3  e4] and see in Fig. 3 the algorithm converges to a local minimum.
Burer-Monteiro approach for certain low rank problems. Following Boumal et al. (2016b)  we
consider  for A ∈ Sd×d and r(r + 1)/2 ≤ d  the problem

trace(AX)  s.t. diag(X) = 1  X (cid:23) 0  rank(X) ≤ r.

min
X∈Sd×d

We factorize X by Y Y T with an overparametrized Y ∈ Rd×p and p(p + 1)/2 ≥ d. Then any local
minimum of

min

Y ∈Rd×p

trace(AY Y T )  s.t. diag(Y Y T ) = 1 

is a global minimum where Y Y T = X∗ (Boumal et al.  2016b). Let f (Y ) = 1
2 trace(AY Y T ). In
the experiment  we take A ∈ R100×20 being a sparse matrix that only the upper left 5 × 5 block
is random and other entries are 0. Let the initial point Y0 ∈ R100×20  such that (Y0)i j = 1 for
5j − 4 ≤ i ≤ 5j and (Y0)i j = 0 otherwise. Then Y0 is a saddle point. We see in Fig. 3 the algorithm
converges to the global optimum.

Summary We have shown that for the constrained optimization problem of minimizing f (x)
subject to a manifold constraint as long as the function and the manifold are appropriately smooth  a
perturbed Riemannian gradient descent algorithm will escape saddle points with a rate of order 1/2
in the accuracy   polylog in manifold dimension d  and depends polynomially on the curvature and
smoothness parameters.
A natural extension of our result is to consider other variants of gradient descent  such as the heavy
ball method  Nesterov’s acceleration  and the stochastic setting. The question is whether these
algorithms with appropriate modiﬁcation (with manifold constraints) would have a fast convergence
to second-order stationary point (not just ﬁrst-order stationary as studied in recent literature)  and
whether it is possible to show the relationship between convergence rate and smoothness of manifold.

6“smallest eigenvector” means the eigenvector corresponding to the smallest eigenvalue.

8

(a)

(b)

Figure 3: (a) kPCA problem. We start from an approximate saddle point  and it converges to
a local minimum (which is also global minimum). (b) Burer-Monteiro approach Plot f (Y ) =
2 trace(AY Y T ) versus iterations. We start from the saddle point  and it converges to a local
1
minimum (which is also global minimum).

References
Absil  P.-A.  Mahony  R.  and Sepulchre  R. Optimization Algorithms on Matrix Manifolds. Princeton

University Press  2009.

Agarwal  N.  Boumal  N.  Bullins  B.  and Cartis  C. Adaptive regularization with cubics on manifolds

with a ﬁrst-order analysis. arXiv preprint arXiv:1806.00065  2018.

Ambrose  W. and Singer  I. M. A theorem on holonomy. Transactions of the American Mathematical

Society  75(3):428–443  1953.

Bécigneul  G. and Ganea  O.-E. Riemannian adaptive optimization methods.

Conference on Learning Representations  2019.

In International

Bonnabel  S. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic

Control  58(9):2217–2229  2013.

Boumal  N. and Absil  P.-a. Rtrmc: A riemannian trust-region method for low-rank matrix completion.

In Advances in neural information processing systems  pp. 406–414  2011.

Boumal  N.  Mishra  B.  Absil  P.-A.  and Sepulchre  R. Manopt  a Matlab toolbox for optimization

on manifolds. JMLR  2014.

Boumal  N.  Absil  P.-A.  and Cartis  C. Global rates of convergence for nonconvex optimization on

manifolds. IMA Journal of Numerical Analysis  2016a.

Boumal  N.  Voroninski  V.  and Bandeira  A. The non-convex burer-monteiro approach works
on smooth semideﬁnite programs. In Advances in Neural Information Processing Systems  pp.
2757–2765  2016b.

Boumal  N.  Absil  P.-A.  and Cartis  C. Global rates of convergence for nonconvex optimization on
manifolds. IMA Journal of Numerical Analysis  pp. drx080  2018. doi: 10.1093/imanum/drx080.
URL http://dx.doi.org/10.1093/imanum/drx080.

Carmon  Y. and Duchi  J. C. Gradient descent efﬁciently ﬁnds the cubic-regularized non-convex

newton step. arXiv preprint arXiv:1612.00547  2017.

Cheeger  J. and Ebin  D. G. Comparison Theorems in Riemannian Geometry. AMS Chelsea

Publishing  Providence  RI  2008.

Do Carmo  M. P. Differential Geometry of Curves and Surfaces. Courier Dover Publications  2016.

Du  S. S.  Jin  C.  Lee  J. D.  Jordan  M. I.  Singh  A.  and Poczos  B. Gradient descent can take
exponential time to escape saddle points. In Advances in Neural Information Processing Systems 
pp. 1067–1077  2017.

Edelman  A.  Arias  T. A.  and Smith  S. T. The geometry of algorithms with orthogonality constraints.

SIAM journal on Matrix Analysis and Applications  20(2):303–353  1998.

9

0510152025303540Iterations33.544.5function value051015202530354045Iterations0123456Function valueGe  R.  Huang  F.  Jin  C.  and Yuan  Y. Escaping from saddle points – online stochastic gradient for

tensor decomposition. In Conference on Learning Theory  pp. 797–842  2015.

Hu  J.  Milzarek  A.  Wen  Z.  and Yuan  Y. Adaptive quadratically regularized Newton method for

Riemannian optimization. SIAM J. Matrix Anal. Appl.  39(3):1181–1207  2018.

Ishteva  M.  Absil  P.-A.  Van Huffel  S.  and De Lathauwer  L. Best low multilinear rank approxi-
mation of higher-order tensors  based on the riemannian trust-region scheme. SIAM Journal on
Matrix Analysis and Applications  32(1):115–135  2011.

Jin  C.  Ge  R.  Netrapalli  P.  Kakade  S.  and Jordan  M. I. How to escape saddle points efﬁciently.

In ICML  2017a.

Jin  C.  Netrapalli  P.  and Jordan  M. I. Accelerated gradient descent escapes saddle points faster

than gradient descent. arXiv preprint arXiv:1711.10456  2017b.

Karcher  H. Riemannian center of mass and molliﬁer smoothing. Communications on pure and

applied mathematics  30(5):509–541  1977.

Kasai  H. and Mishra  B. Inexact trust-region algorithms on riemannian manifolds. In Advances in

Neural Information Processing Systems 31  pp. 4254–4265. 2018.

Khuzani  M. B. and Li  N. Stochastic primal-dual method on riemannian manifolds with bounded

sectional curvature. arXiv preprint arXiv:1703.08167  2017.

Lee  J. D.  Simchowitz  M.  Jordan  M. I.  and Recht  B. Gradient descent only converges to

minimizers. Conference on Learning Theory  pp. 1246–1257  2016.

Lee  J. D.  Panageas  I.  Piliouras  G.  Simchowitz  M.  Jordan  M. I.  and Recht  B. First-order

methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406  2017.

Lee  J. M. Riemannian manifolds : an introduction to curvature. Graduate texts in mathematics ;

176. Springer  New York  1997. ISBN 9780387227269.

Mangoubi  O.  Smith  A.  et al. Rapid mixing of geodesic walks on manifolds with positive curvature.

The Annals of Applied Probability  28(4):2501–2543  2018.

Miolane  N.  Mathe  J.  Donnat  C.  Jorda  M.  and Pennec  X. geomstats: a python package for

riemannian geometry in machine learning. arXiv preprint arXiv:1805.08308  2018.

Mokhtari  A.  Ozdaglar  A.  and Jadbabaie  A. Escaping saddle points in constrained optimization.

arXiv preprint arXiv:1809.02162  2018.

Nouiehed  M.  Lee  J. D.  and Razaviyayn  M. Convergence to second-order stationarity for con-

strained non-convex optimization. arXiv preprint arXiv:1810.02024  2018.

Pemantle  R. Nonconvergence to unstable points in urn models and stochastic approximations. The

Annals of Probability  pp. 698–712  1990.

Rapcsák  T. Sectional curvatures in nonlinear optimization. Journal of Global Optimization  40(1-3):

375–388  2008.

Sakai  T. Riemannian Geometry  volume 149 of Translations of Mathematical Monographs. American

Mathematical Society  1996.

Sun  J.  Qu  Q.  and Wright  J. Complete dictionary recovery over the sphere ii: Recovery by
riemannian trust-region method. IEEE Transactions on Information Theory  63(2):885–914  2017.

Sun  Y. and Fazel  M. Escaping saddle points efﬁciently in equality-constrained optimization
problems. In Workshop on Modern Trends in Nonconvex Optimization for Machine Learning 
International Conference on Machine Learning  2018.

Tripuraneni  N.  Flammarion  N.  Bach  F.  and Jordan  M. I. Averaging Stochastic Gradient Descent

on Riemannian Manifolds. arXiv preprint arXiv:1802.09128  2018.

10

Wong  Y.-c. Sectional curvatures of Grassmann manifolds. Proc. Nat. Acad. Sci. U.S.A.  60:75–79 

1968.

Zhang  H. and Sra  S. First-order methods for geodesically convex optimization. arXiv:1602.06053 

2016. Preprint.

Zhang  H.  Reddi  S. J.  and Sra  S. Riemannian svrg: fast stochastic optimization on riemannian

manifolds. In Advances in Neural Information Processing Systems  pp. 4592–4600  2016.

Zhang  J. and Zhang  S. A cubic regularized newton’s method over riemannian manifolds. arXiv

preprint arXiv:1805.05565  2018.

11

,Yue Sun
Nicolas Flammarion
Maryam Fazel