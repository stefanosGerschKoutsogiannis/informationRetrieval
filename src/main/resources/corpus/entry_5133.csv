2009,Sparse and Locally Constant Gaussian Graphical Models,Locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes  motion trajectories  2D and 3D images). Although these datasets are typically under-sampled and high-dimensional  they often need to be represented with low-complexity statistical models  which are comprised of only the important probabilistic dependencies in the datasets. Most methods attempt to reduce model complexity by enforcing structure sparseness. However  sparseness cannot describe inherent regularities in the structure. Hence  in this paper we first propose a new class of Gaussian graphical models which  together with sparseness  imposes local constancy through ${\ell}_1$-norm penalization. Second  we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions. Through synthetic experiments  we evaluate the closeness of the recovered models to the ground truth. We also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it can capture useful structures such as the rotation and shrinking of a beating heart  motion correlations between body parts during walking and functional interactions of brain regions. Our method outperforms the state-of-the-art structure learning techniques for Gaussian graphical models both for small and large datasets.,Sparse and Locally Constant Gaussian Graphical

Models

Jean Honorio 

Luis Ortiz 

Dimitris Samaras

{jhonorio leortiz samaras}@cs.sunysb.edu

Department of Computer Science

Stony Brook University
Stony Brook  NY 11794

Nikos Paragios
Laboratoire MAS
Ecole Centrale Paris

Chatenay-Malabry  France

Rita Goldstein

Medical Department

Brookhaven National Laboratory

Upton  NY 11973

nikos.paragios@ecp.fr

rgoldstein@bnl.gov

Abstract

Locality information is crucial in datasets where each variable corresponds to a
measurement in a manifold (silhouettes  motion trajectories  2D and 3D images).
Although these datasets are typically under-sampled and high-dimensional  they
often need to be represented with low-complexity statistical models  which are
comprised of only the important probabilistic dependencies in the datasets. Most
methods attempt to reduce model complexity by enforcing structure sparseness.
However  sparseness cannot describe inherent regularities in the structure. Hence 
in this paper we ﬁrst propose a new class of Gaussian graphical models which 
together with sparseness  imposes local constancy through (cid:96)1-norm penalization.
Second  we propose an efﬁcient algorithm which decomposes the strictly convex
maximum likelihood estimation into a sequence of problems with closed form
solutions. Through synthetic experiments  we evaluate the closeness of the recov-
ered models to the ground truth. We also test the generalization performance of
our method in a wide range of complex real-world datasets and demonstrate that
it captures useful structures such as the rotation and shrinking of a beating heart 
motion correlations between body parts during walking and functional interactions
of brain regions. Our method outperforms the state-of-the-art structure learning
techniques for Gaussian graphical models both for small and large datasets.

1 Introduction
Structure learning aims to discover the topology of a probabilistic network of variables such that
this network represents accurately a given dataset while maintaining low complexity. Accuracy
of representation is measured by the likelihood that the model explains the observed data  while
complexity of a graphical model is measured by its number of parameters. Structure learning faces
several challenges: the number of possible structures is super-exponential in the number of variables
while the required sample size might be even exponential. Therefore  ﬁnding good regularization
techniques is very important in order to avoid over-ﬁtting and to achieve a better generalization
performance. In this paper  we propose local constancy as a prior for learning Gaussian graphical
models  which is natural for spatial datasets such as those encountered in computer vision [1  2  3].
For Gaussian graphical models  the number of parameters  the number of edges in the structure
and the number of non-zero elements in the inverse covariance or precision matrix are equivalent

1

measures of complexity. Therefore  several techniques focus on enforcing sparsity of the preci-
sion matrix. An approximation method proposed in [4] relied on a sequence of sparse regressions.
Maximum likelihood estimation with an (cid:96)1-norm penalty for encouraging sparseness is proposed in
[5  6  7]. The difference among those methods is the optimization technique: a sequence of box-
constrained quadratic programs in [5]  solution of the dual problem by sparse regression in [6] or
an approximation via standard determinant maximization with linear inequality constraints in [7]. It
has been shown theoretically and experimentally  that only the covariance selection [5] as well as
graphical lasso [6] converge to the maximum likelihood estimator.
In datasets which are a collection of measurements for variables with some spatial arrangement 
one can deﬁne a local neighborhood for each variable or manifold. Such variables correspond to
points in silhouettes  pixels in 2D images or voxels in 3D images. Silhouettes deﬁne a natural
one-dimensional neighborhood in which each point has two neighbors on each side of the closed
contour. Similarly  one can deﬁne a four-pixel neighborhood for 2D images as well as six-pixel
neighborhood for 3D images. However  there is little research on spatial regularization for structure
learning. Some methods assume a one-dimensional spatial neighborhood (e.g. silhouettes) and that
variables far apart are only weakly correlated [8]  interaction between a priori known groups of
variables as in [9]  or block structures as in [10] in the context of Bayesian networks.
Our contribution in this paper is two-fold. First  we propose local constancy  which encourages
ﬁnding connectivities between two close or distant clusters of variables  instead of between isolated
variables. It does not heavily constrain the set of possible structures  since it only imposes restric-
tions of spatial closeness for each cluster independently  but not between clusters. We impose an
(cid:96)1-norm penalty for differences of spatially neighboring variables  which allows obtaining locally
constant models that preserve sparseness  unlike (cid:96)2-norm penalties. Our model is strictly convex
and therefore has a global minimum. Positive deﬁniteness of the estimated precision matrix is also
guaranteed  since this is a necessary condition for the deﬁnition of a multivariate normal distribution.
Second  since optimization methods for structure learning on Gaussian graphical models [5  6  4  7]
are unable to handle local constancy constraints  we propose an efﬁcient algorithm by maximizing
with respect to one row and column of the precision matrix at a time. By taking directions involving
either one variable or two spatially neighboring variables  the problem reduces to minimization of a
piecewise quadratic function  which can be performed in closed form.
We initially test the ability of our method to recover the ground truth structure from data  of a
complex synthetic model which includes locally and not locally constant interactions as well as
independent variables. Our method outperforms the state-of-the-art structure learning techniques
[5  6  4] for datasets with both small and large number of samples. We further show that our method
has better generalization performance on real-world datasets. We demonstrate the ability of our
method to discover useful structures from datasets with a diverse nature of probabilistic relationships
and spatial neighborhoods: manually labeled silhouettes in a walking sequence  cardiac magnetic
resonance images (MRI) and functional brain MRI.
Section 2 introduces Gaussian graphical models as well as techniques for learning such structures
from data. Section 3 presents our sparse and locally constant Gaussian graphical models. Section 4
describes our structure learning algorithm. Experimental results on synthetic and real-world datasets
are shown and explained in Section 5. Main contributions and results are summarized in Section 6.

2 Background

In this paper  we use the notation in Table 1. For convenience  we deﬁne two new operators: the
zero structure operator and the diagonal excluded product.
A Gaussian graphical model [11] is a graph in which all random variables are continuous and jointly
Gaussian. This model corresponds to the multivariate normal distribution for N variables x ∈ RN
with mean vector µ ∈ RN and a covariance matrix Σ ∈ RN×N   or equivalently x ∼ N (µ  Σ)
where Σ (cid:194) 0. Conditional independence in a Gaussian graphical model is simply reﬂected in the
zero entries of the precision matrix Ω = Σ−1 [11]. Let Ω = {ωn1n2}  two variables xn1 and
xn2 are conditionally independent if and only if ωn1n2 = 0. The precision matrix representation
is preferred because it allows detecting cases in which two seemingly correlated variables  actually
depend on a third confounding variable.

2

(cid:80)

n |cn|
(cid:80)

Notation
(cid:107)c(cid:107)1
(cid:107)c(cid:107)∞
|c|
diag(c) ∈ RN×N matrix with elements of c ∈ RN on its diagonal
(cid:107)A(cid:107)1
(cid:104)A  B(cid:105)
A ◦ B ∈ RM×N
J(A) ∈ RM×N
A (cid:174) B ∈ RM×N
A (cid:194) 0
diag(A) ∈ RN×N matrix with diagonal elements of A ∈ RN×N only
vec(A) ∈ RM N

Description
(cid:96)1-norm of c ∈ RN   i.e.
(cid:96)∞-norm of c ∈ RN   i.e. maxn |cn|
entrywise absolute value of c ∈ RN   i.e. (|c1| |c2|  . . .  |cN|)T
(cid:96)1-norm of A ∈ RM×N   i.e.
scalar product of A  B ∈ RM×N   i.e.
Hadamard or entrywise product of A  B ∈ RM×N   i.e. (A ◦ B)mn = amnbmn
zero structure operator of A ∈ RM×N   by using the Iverson bracket jmn(A) =
[amn = 0]
diagonal excluded product of A ∈ RM×N and B ∈ RN×N   i.e. A (cid:174) B =
J(A) ◦ (AB). It has the property that no diagonal entry of B is used in A (cid:174) B
A ∈ RN×N is symmetric and positive deﬁnite
vector containing all elements of A ∈ RM×N

(cid:80)
mn |amn|

mn amnbmn

Table 1: Notation used in this paper.

The concept of robust estimation by performing covariance selection was ﬁrst introduced in [12]
where the number of parameters to be estimated is reduced by setting some elements of the precision
matrix Ω to zero. Since ﬁnding the most sparse precision matrix which ﬁts a dataset is a NP-hard
problem [5]  in order to overcome it  several (cid:96)1-regularization methods have been proposed for
learning Gaussian graphical models from data.

Covariance selection [5] starts with a dense sample covariance matrix (cid:98)Σ and ﬁts a sparse preci-

sion matrix Ω by solving a maximum likelihood estimation problem with a (cid:96)1-norm penalty which
encourages sparseness of the precision matrix or conditional independence among variables:

(1)

(cid:179)

max
Ω(cid:194)0

log det Ω − (cid:104)(cid:98)Σ  Ω(cid:105) − ρ(cid:107)Ω(cid:107)1

(cid:180)

for some ρ > 0. Covariance selection computes small perturbations on the sample covariance
matrix such that it generates a sparse precision matrix  which results in a box-constrained quadratic
programming. This method has moderate run time.
The Meinshausen-B¨uhlmann approximation [4] obtains the conditional dependencies by performing
a sparse linear regression for each variable  by using lasso regression [13]. This method is very fast
but does not yield good estimates for lightly regularized models  as noted in [6]. The constrained
optimization version of eq.(1) is solved in [7] by applying a standard determinant maximization
with linear inequality constraints  which requires iterative linearization of (cid:107)Ω(cid:107)1. This technique in
general does not yield the maximum likelihood estimator  as noted in [14]. The graphical lasso
technique [6] solves the dual form of eq.(1)  which results in a lasso regression problem. This
method has run times comparable to [4] without sacriﬁcing accuracy in the maximum likelihood
estimator.
Structure learning through (cid:96)1-regularization has been also proposed for different types of graphical
models: Markov random ﬁelds (MRFs) by a clique selection heuristic and approximate inference
[15]; Bayesian networks on binary variables by logistic regression [16]; Conditional random ﬁelds
by pseudo-likelihood and block regularization in order to penalize all parameters of an edge simulta-
neously [17]; and Ising models  i.e. MRFs on binary variables with pairwise interactions  by logistic
regression [18] which is similar in spirit to [4].
There is little work on spatial regularization for structure learning. Adaptive banding on the
Cholesky factors of the precision matrix has been proposed in [8]. Instead of using the traditional
lasso penalty  a nested lasso penalty is enforced. Entries at the right end of each row are promoted to
zero faster than entries close to the diagonal. The main drawback of this technique is the assumption
that the more far apart two variables are the more likely they are to be independent. Grouping of
entries in the precision matrix into disjoint subsets has been proposed in [9]. Such subsets can model
for instance dependencies between different groups of variables in the case of block structures. Al-
though such a formulation allows for more general settings  its main disadvantage is the need for an
a priori segmentation of the entries in the precision matrix.

3

Related approaches have been proposed for Bayesian networks. In [10] it is assumed that variables
belong to unknown classes and probabilities of having edges among different classes were enforced
to account for structure regularity  thus producing block structures only.

3 Sparse and Locally Constant Gaussian Graphical Models

First  we describe our local constancy assumption and its use to model the spatial coherence of
dependence/independence relationships. Local constancy is deﬁned as follows: if variable xn1 is
dependent (or independent) of variable xn2  then a spatial neighbor xn(cid:48)
1 of xn1 is more likely to
be dependent (or independent) of xn2. This encourages ﬁnding connectivities between two close or
distant clusters of variables  instead of between isolated variables. Note that local constancy imposes
restrictions of spatial closeness for each cluster independently  but not between clusters.
In this paper  we impose constraints on the difference of entries in the precision matrix Ω ∈ RN×N
sample covariance matrix and D ∈ RM×N be the discrete derivative operator on the manifold 
where M ∈ O(N) is the number of spatial neighborhood relationships. For instance  in a 2D image 
M is the number of pixel pairs that are spatial neighbors on the manifold. More speciﬁcally  if pixel
n1 and pixel n2 are spatial neighbors  we include a row m in D such that dmn1 = 1  dmn2 = −1 and
dmn3 = 0 for n3 /∈ {n1  n2}. The following penalized maximum likelihood estimation is proposed:

for N variables  which correspond to spatially neighboring variables. Let (cid:98)Σ ∈ RN×N be the dense

(cid:179)

max
Ω(cid:194)0

log det Ω − (cid:104)(cid:98)Σ  Ω(cid:105) − ρ(cid:107)Ω(cid:107)1 − τ(cid:107)D (cid:174) Ω(cid:107)1

(cid:180)

(2)

for some ρ  τ > 0. The ﬁrst two terms model the quality of the ﬁt of the estimated multivariate
normal distribution to the dataset. The third term ρ(cid:107)Ω(cid:107)1 encourages sparseness while the fourth
term τ(cid:107)D (cid:174) Ω(cid:107)1 encourages local constancy in the precision matrix by penalizing the differences
of spatially neighboring variables.
In conjunction with the (cid:96)1-norm penalty for sparseness  we introduce an (cid:96)1-norm penalty for local
constancy. As discussed further in [19]  (cid:96)1-norm penalties lead to locally constant models which
preserve sparseness  where as (cid:96)2-norm penalties of differences fail to do so.
The use of the diagonal excluded product for penalizing differences instead of the regular product of
matrices  is crucial. The regular product of matrices would penalize the difference between the di-
agonal and off-diagonal entries of the precision matrix  and potentially destroy positive deﬁniteness
of the solution for strongly regularized models.
Even though the choice of the linear operator in eq.(2) does not affect the positive deﬁniteness
properties of the estimated precision matrix or the optimization algorithm  in the following Section
4  we discuss positive deﬁniteness properties and develop an optimization algorithm for the speciﬁc
case of the discrete derivative operator D.

4 Coordinate-Direction Descent Algorithm

Positive deﬁniteness of the precision matrix is a necessary condition for the deﬁnition of a multivari-
ate normal distribution. Furthermore  strict convexity is a very desirable property in optimization 
since it ensures the existence of a unique global minimum. Notice that the penalized maximum like-
lihood estimation problem in eq.(2) is strictly convex due to the convexity properties of log det Ω on
the space of symmetric positive deﬁnite matrices [20]. Maximization can be performed with respect
to one row and column of the precision matrix Ω at a time. Without loss of generality  we use the
last row and column in our derivation  since permutation of rows and columns is always possible.
Also  note that rows in D can be freely permuted without affecting the objective function. Let:

(cid:184)

(cid:183)

(cid:184)

(cid:183)

(cid:184)

(cid:183)

  (cid:98)Σ =

Ω =

W y
yT z

u
S
uT v

  D =

D1 0M−L
D2

d3

(3)

where W  S ∈ RN−1×N−1  y  u ∈ RN−1  d3 ∈ RL is a vector with all entries different than zero 
which requires a permutation of rows in D  D1 ∈ RM−L×N−1 and D2 ∈ RL×N−1.

4

In term of the variables y  z and the constant matrix W  the penalized maximum likelihood estima-
tion problem in eq.(2) can be reformulated as:

(cid:161)

max
Ω(cid:194)0

(cid:162)

(4)

(5)

log(z − yTW−1y) − 2uTy − (v + ρ)z − 2ρ(cid:107)y(cid:107)1 − τ (cid:107)Ay − b(cid:107)1

where (cid:107)Ay − b(cid:107)1 can be written in an extended form:

(cid:107)Ay − b(cid:107)1 = (cid:107)D1y(cid:107)1 +

(cid:176)(cid:176)vec(J(D2) ◦ (d3yT + D2W))

(cid:176)(cid:176)

1

(cid:176)(cid:176)vec(J(D2) ◦ (d3yT + D2W))
(cid:176)(cid:176)

Intuitively  the term (cid:107)D1y(cid:107)1 penalizes differences across different rows of Ω which affect only val-
ues in y  while the term
1 penalizes differences across different
columns of Ω which affect values of y as well as W.
It can be shown that the precision matrix Ω is positive deﬁnite since its Schur complement z −
yTW−1y is positive. By maximizing eq.(4) with respect to z  we get:

z − yTW−1y =

1

v + ρ

(6)

and since v > 0 and ρ > 0  this implies that the Schur complement in eq.(6) is positive.
Maximization with respect to one variable at a time leads to a strictly convex  non-smooth  piecewise
quadratic function. By replacing the optimal value for z given by eq.(6) into the objective function
in eq.(4)  we get:

1

2 yT(v + ρ)W−1y + uTy + ρ(cid:107)y(cid:107)1 + τ

2 (cid:107)Ay − b(cid:107)1

(7)

(cid:161)

min

y∈RN−1

(cid:162)

Since the objective function in eq.(7) is non-smooth  its derivative is not continuous and therefore
methods such as gradient descent cannot be applied. Although coordinate descent methods [5  6]
are suitable when only sparseness is enforced  they are not when local constancy is encouraged. As
shown in [21]  when penalizing an (cid:96)1-norm of differences  a coordinate descent algorithm can get
stuck at sharp corners of the non-smooth optimization function; the resulting coordinates are station-
ary only under single-coordinate moves but not under diagonal moves involving two coordinates at
a time.
For a discrete derivative operator D used in the penalized maximum likelihood estimation problem
in eq.(2)  it sufﬁces to take directions involving either one variable g = (0  . . .   0  1  0  . . .   0)T or
two spatially neighboring variables g = (0  . . .   0  1  0  . . .   0  1  0  . . .   0)T such that 1s appear in
the position corresponding to the two neighbor variables. Finally  assuming an initial value y0 and
a direction g  the objective function in eq.(7) can be reduced to ﬁnd t in y(t) = y0 + tg such that it
minimizes:

(cid:161)

1

(cid:183)

(cid:184)

m rm|t − sm|(cid:162)
(cid:80)
(cid:183)

mint∈R
p = (v + ρ)gTW−1g   q = ((v + ρ)W−1y0 + u)Tg
r =
−diag(Ag)−1(Ay0 − b)

2 pt2 + qt +
ρ|g|
2|Ag|

−diag(g)−1(y0)

  s =

τ

(cid:184)

(8)

For simplicity of notation  we assume that r  s ∈ RM use only non-zero entries of g and Ag on
its deﬁnition in eq.(8). We sort and remove duplicate values in s  and propagate changes to r by
adding the entries corresponding to the duplicate values in s. Note that these apparent modiﬁcations
do not change the objective function  but they simplify its optimization. The resulting minimization
problem in eq.(8) is convex  non-smooth and piecewise quadratic. Furthermore  since the objec-
tive function is quadratic on each interval [−∞; s1]  [s1; s2]  . . .   [sM−1; sM ]  [sM ; +∞]  it admits
a closed form solution.
The coordinate-direction descent algorithm is presented in detail in Table 2. A careful implemen-
tation of the algorithm allows obtaining a time complexity of O(KN 3) for K iterations and N
variables  in which W−1  W−1y and Ay are updated at each iteration. In our experiments  the

5

Coordinate-direction descent algorithm

τ and a discrete derivative operator D  ﬁnd the precision matrix Ω (cid:194) 0 that maximizes:

1. Given a dense sample covariance matrix (cid:98)Σ  sparseness parameter ρ  local constancy parameter
log det Ω − (cid:104)(cid:98)Σ  Ω(cid:105) − ρ(cid:107)Ω(cid:107)1 − τ(cid:107)D (cid:174) Ω(cid:107)1
2. Initialize Ω = diag((cid:98)Σ)−1
(a) Split Ω into W  y  z and (cid:98)Σ into S  u  v as described in eq.(3)

3. For each iteration 1  . . . K and each variable 1  . . .   N

(b) Update W−1 by using the Sherman-Woodbury-Morrison formula (Note that when iterat-

ing from one variable to the next one  only one row and column change on matrix W)

(c) Transform local constancy regularization term from D into A and b as described in eq.(5)
(d) Compute W−1y and Ay
(e) For each direction g involving either one variable or two spatially neighboring variables

i. Find t that minimizes eq.(8) in closed form
ii. Update y ← y + tg
iii. Update W−1y ← W−1y + tW−1g
iv. Update Ay ← Ay + tAg
v+ρ + yTW−1y

(f) Update z ← 1

Table 2: Coordinate-direction descent algorithm for learning sparse and locally constant Gaussian graphical
models.

Figure 1: (a) Ground truth model on an open contour manifold. Spatial neighbors are connected with black
dashed lines. Positive interactions are shown in blue  negative interactions in red. The model contains two
locally constant interactions between (x1  x2) and (x6  x7)  and between (x4  x5) and (x8  x9)  a not locally
constant interaction between x1 and x4  and an independent variable x3; (b) colored precision matrix of the
ground truth  red for negative entries  blue for positive entries; learnt structure from (c) small and (d) large
datasets. Note that for large datasets all connections are correctly recovered.

algorithm converges quickly in usually K = 10 iterations. The polynomial dependency on the num-
ber of variables of O(N 3) is expected since we cannot produce an algorithm faster than computing
the inverse of the sample covariance in the case of an inﬁnite sample.
Finally  in the spirit of [5]  a method for reducing the size of the original problem is presented.
Given a P -dimensional spatial neighborhood or manifold (e.g. P = 1 for silhouettes  P = 2 for
a four-pixel neighborhood on 2D images  P = 3 for a six-pixel neighborhood on 3D images)  the
objective function in eq.(7) has the maximizer y = 0 for variables on which (cid:107)u(cid:107)∞ ≤ ρ− P τ. Since
this condition does not depend on speciﬁc entries in the iterative estimation of the precision matrix 
this property can be used to reduce the size of the problem in advance by removing such variables.

5 Experimental Results
Convergence to Ground Truth. We begin with a small synthetic example to test the ability of
the method for recovering the ground truth structure from data  in a complex scenario in which
our method has to deal with both locally and not locally constant interactions as well as independent
variables. The ground truth Gaussian graphical model is shown in Figure 1 and it contains 9 variables
arranged in an open contour manifold.
In order to measure the closeness of the recovered models to the ground truth  we measure the
Kullback-Leibler divergence  average precision (one minus the fraction of falsely included edges) 
average recall (one minus the fraction of falsely excluded edges) as well as the Frobenius norm be-
tween the recovered model and the ground truth. For comparison purposes  we picked two of the

6

0.45-0.350.4ݔ1 ݔ2 ݔ3 ݔ4 ݔ5 ݔ6 ݔ7 ݔ8 ݔ9 (a)(b)(c)(d)Figure 2: Kullback-Leibler divergence with respect to the best method  average precision  recall and Frobenius
norm between the recovered model and the ground truth. Our method (SLCGGM) outperforms the fully con-
nected model (Full)  Meinshausen-B¨uhlmann approximation (MB-or  MB-and)  covariance selection (CovSel) 
graphical lasso (GLasso) for small datasets (in blue solid line) and for large datasets (in red dashed line). The
fully independent model (Indep) resulted in relative divergences of 2.49 for small and 113.84 for large datasets.

state-of-the-art structure learning techniques: covariance selection [5] and graphical lasso [6]  since
it has been shown theoretically and experimentally that they both converge to the maximum likeli-
hood estimator. We also test the Meinshausen-B¨uhlmann approximation [4]. The fully connected as
well as fully independent model are also included as baseline methods.
Two different scenarios are tested: small datasets of four samples  and large datasets of 400 samples.
Under each scenario  50 datasets are randomly generated from the ground truth Gaussian graphical
model. It can be concluded from Figure 2 that our method outperforms the state-of-the-art structure
learning techniques both for small and large datasets. This is due to the fact that the ground truth
data contains locally constant interactions  and our method imposes a prior for local constancy.
Although this is a complex scenario which also contains not locally constant interactions as well as
an independent variable  our method can recover a more plausible model when compared to other
methods. Note that even though other methods may exhibit a higher recall for small datasets  our
method consistently recovers a better probability distribution.
A visual comparison of the ground truth versus the best recovered model by our method from small
and large datasets is shown in Figure 1. The image shows the precision matrix in which red squares
represent negative entries  while blue squares represent positive entries. There is very little differ-
ence between the ground truth and the recovered model from large datasets. Although the model
is not fully recovered from small datasets  our technique performs better than the Meinshausen-
B¨uhlmann approximation  covariance selection and graphical lasso in Figure 2.

Real-World Datasets.
In the following experiments  we demonstrate the ability of our method to
discover useful structures from real-world datasets. Datasets with a diverse nature of probabilistic
relationships are included in our experiments: from cardiac MRI [22]  our method recovers global
deformation in the form of rotation and shrinking; from a walking sequence1  our method ﬁnds the
long range interactions between different parts; and from functional brain MRI [23]  our method
recovers functional interactions between different regions and discover differences in processing
monetary rewards between cocaine addicted subjects versus healthy control subjects. Each dataset
is also diverse in the type of spatial neighborhood: one-dimensional for silhouettes in a walking
sequence  two-dimensional for cardiac MRI and three-dimensional for functional brain MRI.

Generalization. Cross-validation was performed in order to measure the generalization perfor-
mance of our method in estimating the underlying distribution. Each dataset was randomly split
into ﬁve sets. On each round  four sets were used for training and the remaining set was used for
measuring the log-likelihood. Table 3 shows that our method consistently outperforms techniques
that encourage sparsity only. This is strong evidence that datasets that are measured over a spa-
tial manifold are locally constant  as well as that our method is a good regularization technique
that avoids over-ﬁtting and allows for better generalization. Another interesting fact is that for the
brain MRI dataset  which is high dimensional and contains a small number of samples  the model
that assumes full independence performed better than the Meinshausen-B¨uhlmann approximation 
covariance selection and graphical lasso. Similar observations has been already made in [24  25]
where it was found that assuming independence often performs better than learning dependencies
among variables.

1Human Identiﬁcation at a Distance dataset http://www.cc.gatech.edu/cpl/projects/hid/

7

012345Kullback−Leibler divergenceFullMB−orMB−andCovSelGLassoSLCGGM0.20.40.60.811.2PrecisionFullMB−orMB−andCovSelGLassoSLCGGM0.40.60.81RecallMB−orMB−andCovSelGLassoSLCGGM01234567Frobenius normIndepFullMB−orMB−andCovSelGLassoSLCGGMFigure 3: Real-world datasets: cardiac MRI displacement (a) at full contraction and (b) at full expansion  (c)
2D spatial manifold and (d) learnt structure  which captures contraction and expansion (in red)  and similar dis-
placements between neighbor pixels (in blue); (e) silhouette manifold and (f) learnt structure from a manually
labeled walking sequence  showing similar displacements from each independent leg (in blue) and opposite dis-
placements between both legs as well as between hands and feet (in red); and structures learnt from functional
brain MRI in a monetary reward task for (g) drug addicted subjects with more connections in the cerebellum
(in yellow) versus (h) control subjects with more connections in the prefrontal cortex (in green).

Method

Synthetic

Indep
MB-and
MB-or
CovSel
GLasso
SLCGGM

-6428.23
-5595.87*
-5595.13*
-5626.32
-5625.79
-5623.52

Cardiac
MRI
-5150.58
-5620.45
-4135.98*
-5044.41
-5041.52
-4017.56

Walking
Sequence
-12957.72
-12542.15
-11317.24
-12051.51
-12035.50
-10718.62

Brain MRI
Drug-addicted
-324724.24
-418605.02
-398725.04
-409402.60
-413176.45
-297318.61

Brain MRI
Control
-302729.54
-317034.67
-298186.66
-300829.98
-305307.25
-278678.35

Table 3: Cross-validated log-likelihood on the testing set. Our method (SLCGGM) outperforms the
Meinshausen-B¨uhlmann approximation (MB-and  MB-or)  covariance selection (CovSel)  graphical lasso
(GLasso) and the fully independent model (Indep). Values marked with an asterisk are not statistically sig-
niﬁcantly different from our method.

6 Conclusions and Future Work

In this paper  we proposed local constancy for Gaussian graphical models  which encourages ﬁnding
probabilistic connectivities between two close or distant clusters of variables  instead of between
isolated variables. We introduced an (cid:96)1-norm penalty for local constancy into a strictly convex
maximum likelihood estimation. Furthermore  we proposed an efﬁcient optimization algorithm and
proved that our method guarantees positive deﬁniteness of the estimated precision matrix. We tested
the ability of our method to recover the ground truth structure from data  in a complex scenario with
locally and not locally constant interactions as well as independent variables. We also tested the
generalization performance of our method in a wide range of complex real-world datasets with a
diverse nature of probabilistic relationships as well as neighborhood type.
There are several ways of extending this research. Methods for selecting regularization parameters
for sparseness and local constancy need to be further investigated. Although the positive deﬁnite-
ness properties of the precision matrix as well as the optimization algorithm still hold when including
operators such as the Laplacian for encouraging smoothness  beneﬁts of such a regularization ap-
proach need to be analyzed. In practice  our technique converges in a small number of iterations  but
a more precise analysis of the rate of convergence needs to be performed. Finally  model selection
consistency when the number of samples grows to inﬁnity needs to be proved.

Acknowledgments

This work was supported in part by NIDA Grant 1 R01 DA020949-01 and NSF Grant CNS-0721701

8

(a)(b)(c)(d)(e)(f)References
[1] D. Crandall  P. Felzenszwalb  and D. Huttenlocher. Spatial priors for part-based recognition using statis-

tical models. IEEE Conf. Computer Vision and Pattern Recognition  2005.

[2] P. Felzenszwalb and D. Huttenlocher. Pictorial structures for object recognition. International Journal of

Computer Vision  2005.

[3] L. Gu  E. Xing  and T. Kanade. Learning GMRF structures for spatial priors.

Vision and Pattern Recognition  2007.

IEEE Conf. Computer

[4] N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with the lasso. The

Annals of Statistics  2006.

[5] O. Banerjee  L. El Ghaoui  A. d’Aspremont  and G. Natsoulis. Convex optimization techniques for ﬁtting

sparse Gaussian graphical models. International Conference on Machine Learning  2006.

[6] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.

Biostatistics  2007.

[7] M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika  2007.
[8] E. Levina  A. Rothman  and J. Zhu. Sparse estimation of large covariance matrices via a nested lasso

penalty. The Annals of Applied Statistics  2008.

[9] J. Duchi  S. Gould  and D. Koller. Projected subgradient methods for learning sparse Gaussians. Uncer-

tainty in Artiﬁcial Intelligence  2008.

[10] V. Mansinghka  C. Kemp  J. Tenenbaum  and T. Grifﬁths. Structured priors for structure learning. Uncer-

tainty in Artiﬁcial Intelligence  2006.

[11] S. Lauritzen. Graphical Models. Oxford Press  1996.
[12] A. Dempster. Covariance selection. Biometrics  1972.
[13] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

1996.

[14] O. Banerjee  L. El Ghaoui  and A. d’Aspremont. Model selection through sparse maximum likelihood

estimation for multivariate Gaussian or binary data. Journal of Machine Learning Research  2008.

[15] S. Lee  V. Ganapathi  and D. Koller. Efﬁcient structure learning of Markov networks using (cid:96)1-

regularization. Advances in Neural Information Processing Systems  2006.

[16] M. Schmidt  A. Niculescu-Mizil  and K. Murphy. Learning graphical model structure using (cid:96)1-

regularization paths. AAAI Conf. Artiﬁcial Intelligence  2007.

[17] M. Schmidt  K. Murphy  G. Fung  and R. Rosales. Structure learning in random ﬁelds for heart motion

abnormality detection. IEEE Conf. Computer Vision and Pattern Recognition  2008.

[18] M. Wainwright  P. Ravikumar  and J. Lafferty. High dimensional graphical model selection using (cid:96)1-

regularized logistic regression. Advances in Neural Information Processing Systems  2006.

[19] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the fused lasso.

Journal of the Royal Statistical Society  2005.

[20] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  2006.
[21] J. Friedman  T. Hastie  H. H¨oﬂing  and R. Tibshirani. Pathwise coordinate optimization. The Annals of

Applied Statistics  2007.

[22] J. Deux  A. Rahmouni  and J. Garot. Cardiac magnetic resonance and 64-slice cardiac CT of lipomatous

metaplasia of chronic myocardial infarction. European Heart Journal  2008.

[23] R. Goldstein  D. Tomasi  N. Alia-Klein  L. Zhang  F. Telang  and N. Volkow. The effect of practice on a

sustained attention task in cocaine abusers. NeuroImage  2007.

[24] P. Domingos and M. Pazzani. On the optimality of the simple Bayesian classiﬁer under zero-one loss.

Machine Learning  1997.

[25] N. Friedman  D. Geiger  and M. Goldszmidt. Bayesian network classiﬁers. Machine Learning  1997.

9

,Liwei Wu
Shuqing Li
Cho-Jui Hsieh
James Sharpnack