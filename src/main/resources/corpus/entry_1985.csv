2018,Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making,We draw attention to an important  yet largely overlooked aspect of evaluating fairness for automated decision making systems---namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics  and is justified by the Rawlsian conception of fairness behind a veil of ignorance. The convex formulation of our welfare-based measures of fairness allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy  (b) group discrimination  and (c) Dwork et al's notion of individual fairness. Furthermore and perhaps most importantly  our work provides both heuristic justification and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level inequality.,Fairness Behind a Veil of Ignorance:

A Welfare Analysis for Automated Decision Making

Hoda Heidari
ETH Zürich

hheidari@inf.ethz.ch

Claudio Ferrari

ETH Zürich

ferraric@ethz.ch

Krishna P. Gummadi

MPI-SWS

gummadi@mpi-sws.org

Andreas Krause

ETH Zürich

krausea@ethz.ch

Abstract

We draw attention to an important  yet largely overlooked aspect of evaluating
fairness for automated decision making systems—namely risk and welfare con-
siderations. Our proposed family of measures corresponds to the long-established
formulations of cardinal social welfare in economics  and is justiﬁed by the Rawl-
sian conception of fairness behind a veil of ignorance. The convex formulation
of our welfare-based measures of fairness allows us to integrate them as a con-
straint into any convex loss minimization pipeline. Our empirical analysis reveals
interesting trade-offs between our proposal and (a) prediction accuracy  (b) group
discrimination  and (c) Dwork et al.’s notion of individual fairness. Furthermore
and perhaps most importantly  our work provides both heuristic justiﬁcation and
empirical evidence suggesting that a lower-bound on our measures often leads to
bounded inequality in algorithmic outcomes; hence presenting the ﬁrst computa-
tionally feasible mechanism for bounding individual-level inequality.

1

Introduction

Traditionally  data-driven decision making systems have been designed with the sole purpose of
maximizing some system-wide measure of performance  such as accuracy or revenue. Today  these
systems are increasingly employed to make consequential decisions for human subjects—examples
include employment [Miller  2015]  credit lending [Petrasic et al.  2017]  policing [Rudin  2013] 
and criminal justice [Barry-Jester et al.  2015]. Decisions made in this fashion have long-lasting
impact on people’s lives and—absent a careful ethical analysis—may affect certain individuals or
social groups negatively [Sweeney  2013; Angwin et al.  2016; Levin  2016]. This realization has
recently spawned an active area of research into quantifying and guaranteeing fairness for machine
learning [Dwork et al.  2012; Kleinberg et al.  2017; Hardt et al.  2016].
Virtually all existing formulations of algorithmic fairness focus on guaranteeing equality of some
notion of beneﬁt across different individuals or socially salient groups. For instance  demographic
parity [Kamiran and Calders  2009; Kamishima et al.  2011; Feldman et al.  2015] seeks to equalize
the percentage of people receiving a particular outcome across different groups. Equality of op-
portunity [Hardt et al.  2016] requires the equality of false positive/false negative rates. Individual
fairness [Dwork et al.  2012] demands that people who are equal with respect to the task at hand
receive equal outcomes. In essence  the debate so far has mostly revolved around identifying the right
notion of beneﬁt and a tractable mathematical formulation for equalizing it.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Predictive model A assigns the same beneﬁt of 0.8 to everyone; model C assigns the
same beneﬁt of 1 to everyone; model B results in beneﬁts (0.5  0.6  0.8  0.9  1.2)  and model D 
(0.78  0.9  0.92  1.1  1.3). Our proposed measures prefer A to B  C to D  and D to A.

The view of fairness as some form of equality is indeed an important perspective in the moral
evaluation of algorithmic decision making systems—decision subjects often compare their outcomes
with other similarly situated individuals  and these interpersonal comparisons play a key role in
shaping their judgment of the system. We argue  however  that equality is not the only factor at
play: we draw attention to two important  yet largely overlooked aspects of evaluating fairness of
automated decision making systems—namely risk and welfare1 considerations. The importance of
these factors is perhaps best illustrated via a simple example.
Example 1 Suppose we have four decision making models A  B  C  D each resulting in a different
beneﬁt distribution across 5 groups/individuals i1  i2  i3  i4  i5 (we will precisely deﬁne in Section 2
how beneﬁts are computed  but for the time being and as a concrete example  suppose beneﬁts are
equivalent to salary predictions made through different regression models). Figure 1 illustrates the
setting. Suppose a decision maker is tasked with determining which one of these alternatives is
ethically more desirable. From an inequality minimizing perspective  A is clearly more desirable
than B: note that both A  B result in the same total beneﬁt of 4  and A distributes it equally across i1 
...  i5. With a similar reasoning  C is preferred to D. Notice  however  that by focusing on equality
alone  A would be deemed more desirable than D  but there is an issue with this conclusion: almost
everyone—expect for i1 who sees a negligible drop of less than 2% in their beneﬁt—is signiﬁcantly
better off under D compared to A.2 In other words  even though D results in unequal beneﬁts and it
does not Pareto-dominate A  collectively it results in higher welfare and lower risk  and therefore 
both intuitively and from a rational point of view  it should be considered more desirable. With a
similar reasoning  the decision maker should conclude C is more desirable than A  even though both
provide beneﬁts equally to all individuals.

In light of this example and inspired by the long line of research on distributive justice in economics  in
this paper we propose a natural family of measures for evaluating algorithmic fairness corresponding
to the well-studied notions of cardinal social welfare in economics [Harsanyi  1953  1955]. Our
proposed measures indeed prefer A to B  C to D  and D to A.
The interpretation of social welfare as a measure of fairness is justiﬁed by the concept of veil of
ignorance (see [Freeman  2016] for the philosophical background). Rawls [2009] proposes “veil
of ignorance” as the ideal condition/mental state under which a policy maker can select the fairest
among a number of political alternatives. He suggests that the policy maker performs the following
thought experiment: imagine him/herself as an individual who knows nothing about the particular
position they will be born in within the society  and is tasked with selecting the most just among a set
of alternatives. According to the utilitarian doctrine in this hypothetical original/ex-ante position if
the individual is rational  they would aim to minimize risk and insure against unlucky events in which
they turn out to assume the position of a low-beneﬁt individual. Note that decision making behind a
1We deﬁne welfare precisely in Sec. 2  but for now it can be taken as the sum of beneﬁts across all subjects.
2In political philosophy  this problem is sometimes referred to as the “leveling down objection to equality”.

2

veil of ignorance is a purely imaginary condition: the decision maker can never in actuality be in
this position  nonetheless  the thought experiment is useful in detaching him/her from the needs and
wishes of a particular person/group  and consequently making a fair judgment. Our main conceptual
contribution is to measure fairness in the context of algorithmic decision making by evaluating it
from behind a veil of ignorance: our proposal is for the ML expert wishing to train a fair decision
making model (e.g. to decide whether salary predictions are to be made using a neural network or a
decision tree) to perform the aforementioned thought experiment: He/she should evaluate fairness
of each alternative by taking the perspective of the algorithmic decision making subjects—but not
any particular one of them: he/she must imagine themselves in a hypothetical setting where they
know they will be born as one of the subjects  but don’t know in advance which one. We consider the
alternative he/she deems best behind this veil of ignorance to be the fairest.
To formalize the above  our core idea consists of comparing the expected utility a randomly chosen 
risk-averse subject of algorithmic decision making receives under different predictive models. In
the example above  if one is to choose between models A  D without knowing which one of the 5
individuals they will be  then the risk associated with alternative D is much less than that of A—under
A the individual is going to receive a (relatively low) beneﬁt of 0.8 with certainty  whereas under D
with high probability (i.e. 4/5) they obtain a (relatively large) beneﬁt of 0.9 or more  and with low
probability (1/5) they receive a beneﬁt of 0.78  roughly the same as the level of beneﬁt they would
attain under A. Such considerations of risk is precisely what our proposal seeks to quantify. We
remark that in comparing two beneﬁt distributions of the same mean (e.g. A  B or C  D in our earlier
example)  our risk-averse measures always prefer the more equal one (A is preferred to B and C
is preferred to D). See Proposition 2 for the formal statement. Thus  our measures are inherently
equality preferring. However  the key advantage of our measures of social welfare over those focusing
on inequality manifests when  as we saw in the above example  comparing two beneﬁt distributions
of different means. In such conditions  inequality based measures are insufﬁcient and may result in
misleading conclusions  while risk-averse measures of social welfare are better suited to identify the
fairest alternative. When comparing two beneﬁt distributions of the same mean  social welfare and
inequality would always yield identical conclusions.
Furthermore and from a computational perspective  our welfare-based measures of fairness are more
convenient to work with due to their convex formulation. This allows us to integrate them as a
constraint into any convex loss minimization pipeline  and solve the resulting problem efﬁciently
and exactly. Our empirical analysis reveals interesting trade-offs between our proposal and (a)
prediction accuracy  (b) group discrimination  and (c) Dwork et al.’s notion of individual fairness.
In particular  we show how loss in accuracy increases with the degree of risk aversion  ↵  and as
the lower bound on social welfare  ⌧  becomes more demanding. We observe that the difference
between false positive/negative rates across different social groups consistently decreases with ⌧. The
impact of our constraints on demographic parity and Dwork et al.’s notion of individual fairness is
slightly more nuanced and depends on the type of learning task at hand (regression vs. classiﬁcation).
Last but not least  we provide empirical evidence suggesting that a lower bound on social welfare
often leads to bounded inequality in algorithmic outcomes; hence presenting the ﬁrst computationally
feasible mechanism for bounding individual-level inequality.

1.1 Related Work

Much of the existing work on algorithmic fairness has been devoted to the study of discrimination
(also called statistical- or group-level fairness). Statistical notions require that given a classiﬁer  a
certain fairness metric is equal across all protected groups (see e.g. [Kleinberg et al.  2017; Zafar et
al.  2017b a]). Statistical notions of fairness fail to guarantee fairness at the individual level. Dwork et
al. [2012] ﬁrst formalized the notion of individual fairness for classiﬁcation learning tasks  requiring
that two individuals who are similar with respect to the task at hand receive similar classiﬁcation
outcomes. The formulation relies on the existence of a suitable similarity metric between individuals 
and as pointed out by Speicher et al.  it does not take into account the variation in social desirability of
various outcomes and people’s merit for different decisions. Speicher et al. [2018] recently proposed a
new measure for quantifying individual unfairness utilizing income inequality indices from economics
and applying them to algorithmic beneﬁt distributions. Both existing formulations of individual-level
fairness focus solely on the inter-personal comparisons of algorithmic outcomes/beneﬁts across
individuals and do not account for risk and welfare considerations. Furthermore  we are not aware of
computationally efﬁcient mechanisms for bounding either of these notions.

3

We consider our family of measures to belong to the individual category: our welfare-based measures
do not require knowledge of individuals’ membership in protected groups  and compose the individual
level utilities through summation. Note that Dwork et al. [2012] propose a stronger notion of
individual fairness—one that requires a certain (minimum) condition to hold for every individual.
As we will see shortly  a limiting case of our proposal (the limit of ↵ = 1) provides a similar
guarantee in terms of beneﬁts. While our main focus in this work is on individual-level fairness  our
proposal can be readily extended to measure and constraint group-level unfairness.
Zafar et al. [2017c] recently proposed two preference-based notions of fairness at the group-level 
called preferred treatment and preferred impact. A group-conditional classiﬁer satisﬁes preferred
treatment if no group collectively prefers another group’s classiﬁer to their own (in terms of average
misclassiﬁcation rate). This deﬁnition is based on the notion of envy-freeness [Varian  1974] in
economics and applies to group-conditional classiﬁers only. A classiﬁer satisﬁes preferred impact
if it Pareto-dominates an existing impact parity classiﬁer (i.e. every group is better off using the
former classiﬁer compared to the latter). Pareto-dominance (to be deﬁned precisely in Section 2)
leads to a partial ordering among alternatives and usually in practice  does not have much bite (recall 
for instance  the comparison between models A  D in our earlier example). Similar to [Zafar et al. 
2017c]  our work can be thought of as a preference-based notions of fairness  but unlike their proposal
our measures lead to a total ordering among all alternatives  and can be utilized to measure both
individual and group-level (un)fairness.
Further discussion of related work can be found in Appendix A.

2 Our Proposed Family of Measures

We consider the standard supervised learning setting: A learning algorithm receives the training
data set D = {(xi  yi)}n
i=1 consisting of n instances  where xi 2X speciﬁes the feature vector for
individual i and yi 2Y   the ground truth label for him/her. The training data is sampled i.i.d. from
a distribution P on X⇥Y . Unless speciﬁed otherwise  we assume X✓ Rk  where k denotes the
number of features. To avoid introducing extra notation for an intercept  we assume feature vectors
are in homogeneous form  i.e. the kth feature value is 1 for every instance. The goal of a learning
algorithm is to use the training data to ﬁt a model (or hypothesis) h : X!Y that accurately predicts
the label for new instances. Let H be the hypothesis class consisting of all the models the learning
algorithm can choose from. A learning algorithm receives D as the input; then utilizes the data to
select a model h 2H that minimizes some notion of loss  LD(h). When h is clear from the context 
we use ˆyi to refer to h(xi).
We assume there exists a beneﬁt function b : Y⇥Y! R that quantiﬁes the beneﬁt an individual
with ground truth label y receives  if the model predicts label ˆy for them.3 The beneﬁt function
is meant to capture the signed discrepancy between an individual’s predicted outcome and their
true/deserved outcome. Throughout  for simplicity we assume higher values of ˆy correspond to more
desirable outcomes (e.g. loan or salary amount). With this assumption in place  a beneﬁt function
must assign a high value to an individual if their predicted label is greater (better) than their deserved
label  and a low value if an individual receives a predicted label less (worse) than their deserved
label. The following are a few examples of beneﬁt functions that satisfy this: b(y  ˆy) = ˆy  y;
b(y  ˆy) = log1 + eˆyy; b(y  ˆy) = ˆy/y.
In order to maintain the convexity of our fairness constraints  throughout this work  we will focus
on beneﬁt functions that are positive and linear in ˆy. In general (e.g. when the prediction task
is regression or multi-class classiﬁcation) this limits the beneﬁt landscape that can be expressed 
but in the important special case of binary classiﬁcation  the following Proposition establishes that
this restriction is without loss of generality4. That is  we can attach an arbitrary combination of
beneﬁt values to the four possible (y  ˆy)-pairs (i.e. false positives  false negatives  true positives  true
negative).

3Our formulation allows the beneﬁt function to depend on x and other available information about the
individual. As long the formulation is linear in the predicted label ˆy  our approach remains computationally
efﬁcient. For simplicity and ease of interpretation  however  we focus on beneﬁt functions that depend on y and
ˆy  only.

4All proofs can be found in Appendix B.

4

Proposition 1 For y  ˆy 2{ 0  1}  let ¯by ˆy 2 R be arbitrary constants specifying the beneﬁt an
individual with ground truth label y receives when their predicted label is ˆy. There exists a linear
beneﬁt function of form cy ˆy + dy such that for all y  ˆy 2{ 0  1}  b(y  ˆy) = ¯by ˆy.
In order for ¯b’s in the above proposition to reﬂect the signed discrepancy between y and ˆy  it must
hold that ¯b1 0 < ¯b0 0  ¯b1 1 < ¯b0 1. Given a model h  we can compute its corresponding beneﬁt
proﬁle b = (b1 ···   bn) where bi denotes individual i’s beneﬁt: bi = b(yi  ˆyi). A beneﬁt proﬁle b
Pareto-dominates b0 (or in short b ⌫ b0)  if for all i = 1 ···   n  bi  b0i.
Following the economic models of risk attitude  we assume the existence of a utility function
u : R ! R  where u(b) represent the utility derived from algorithmic beneﬁt b. We will focus on
Constant Relative Risk Aversion (CRRA) utility functions. In particular  we take u(b) = b↵ where
↵ = 1 corresponds to risk-neutral  ↵> 1 corresponds to risk-seeking  and 0  ↵< 1 corresponds
to risk-averse preferences. Our main focus in this work is on values of 0 <↵< 1: the larger
one’s initial beneﬁt is  the smaller the added utility he/she derives from an increase in his/her beneﬁt.
While in principle our model can allow for different risk parameters for different individuals (↵i for
individual i)  for simplicity throughout we assume all individuals have the same risk parameter. Our
measures assess the fairness of a decision making model via the expected utility a randomly chosen 
risk-averse individual receives as the result of being subject to decision making through that model.
Formally  our measure is deﬁned as follows: UP (h) = E(xi yi)⇠P [u (b(yi  h(xi))]. We estimate this
nPn
expectation by UD(h) = 1
Connection to Cardinal Welfare Our proposed family of measures corresponds to a particular
subset of cardinal social welfare functions. At a high level  a cardinal social welfare function is meant
to rank different distributions of welfare across individuals  as more or less desirable in terms of
distributive justice [Moulin  2004]. More precisely  let W be a welfare function deﬁned over beneﬁt
vectors  such that given any two beneﬁt vectors b and b0  b is considered more desirable than b0 if
and only if W(b) W (b0). The rich body of work on welfare economics offers several axioms to
characterize the set of all welfare functions that pertain to collective rationality or fairness. Any such
function  W  must satisfy the following axioms [Sen  1977; Roberts  1980]:

i=1 u(b(yi  h(xi))).

1. Monotonicity: If b0  b  then W(b0) > W(b). That is  if everyone is better off under b0 
then W should strictly prefer it to b.
2. Symmetry: W(b1  . . .   bn) = Wb(1) ···   b(n). That is  W does not depend on the
identity of the individuals  but only their beneﬁt levels.
3. Independence of unconcerned agents: W should be independent of individuals whose
beneﬁts remain at the same level. Formally  let (b|ia) be a beneﬁt vector that is identical to
b  expect for the ith component which has been replaced by a. The property requires that
for all b  b0  a  c  W(b|ia) W (b0|ia)  W (b|ic) W (b0|ic).

It has been shown that every continuous5 social welfare function W with properties 1–3 is additive
and can be represented asPn
i=1 w(bi). According to the Debreu-Gorman theorem [Debreu  1959;
Gorman  1968]  if in addition to 1–3  W satisﬁes:

4. Independence of common scale: For any c > 0  W(b) W (b0)  W (cb) W (cb0).
The simultaneous rescaling of every individual beneﬁt  should not affect the relative order
of b  b0.

then it belongs to the following one-parameter family: W↵(b1  . . .   bn) =Pn
i=1 w↵(bi)  where (a)
for ↵> 0  w↵(b) = b↵; (b) for ↵ = 0  w↵(b) = ln(b); and (c) for ↵< 0  w↵(b) = b↵. Note that
the limiting case of ↵ ! 1 is equivalent to the leximin ordering (or Rawlsian max-min welfare).
Our focus in this work is on 0 <↵< 1. In this setting  our measures exhibit aversion to pure
inequality. More precisely  they satisfy the following important property:

5. Pigou-Dalton transfer principle [Pigou  1912; Dalton  1920]: Transferring beneﬁt from
a high-beneﬁt to a low-beneﬁt individual must increase social welfare  that is  for any 1 
i < j  n and 0 << b(j)b(i)
  W(b(1) ···   b(i) +  ···   b(j)   ···   b(n)) > W(b).
5That is  for every vector b  the set of vectors weakly better than b (i.e. {b0 : b0 ⌫ b}) and the set of vectors

2

weakly worse than b (i.e. {b0 : b0  b}) are closed sets.

5

2.1 Our In-processing Method to Guarantee Fairness

To guarantee fairness  we propose minimizing loss subject to a lower bound on our measure:

min
h2H
s.t.

LD(h)
UD(h)  ⌧

where the parameter ⌧ speciﬁes a lower bound that must be picked carefully to achieve the right
tradeoff between accuracy and fairness. As a concrete example  when the learning task is linear
regression  b(y  ˆy) = ˆy  y + 1  and the degree of risk aversion in ↵  this optimization amounts to:

min
✓2H

s.t.

nXi=1
nXi=1

(✓.xi  yi)2

(✓.xi  yi + 1)↵  ⌧n

(1)

Note that both the objective function and the constraint in (1) are convex in ✓  therefore  the
optimization can be solved efﬁciently and exactly.

Connection to Inequality Measures Speicher et al. [2018] recently proposed quantifying
individual-level unfairness utilizing a particular inequality index  called generalized entropy. This
measure satisﬁes four important axioms: symmetry  population invariance  0-normalization6  and
the Pigou–Dalton transfer principle. Our measures satisfy all the aforementioned axioms  except
for 0-normalization. Additionally and in contrast with measures of inequality—where the goal
is to capture interpersonal comparison of beneﬁts—our measure is monotone and independent of
unconcerned agents. The latter two are the fundamental properties that set our proposal apart from
measures of inequality.
Despite these fundamental differences  we will shortly observe in Section 3 that lower-bounding our
measures often in practice leads to low inequality. Proposition 2 provides a heuristic explanation
for this: Imposing a lower bound on social welfare is equivalent to imposing an upper bound on
inequality if we restrict attention to the region where beneﬁt vectors are all of the same mean. More
precisely  for a ﬁxed mean beneﬁt value  our proposed measure of fairness results in the same total
ordering as the Atkinson’s index [Atkinson  1970]. The index is deﬁned as follows:

A(b1  . . .   bn) =8<:
nPn

µ⇣ 1
nPn
1  1
µ (Qn
1  1

i=1 b1
i=1 bi)1/n

i

⌘1/(1)

for 0   6= 1
for  = 1 

i

nPn

where µ = 1
i=1 bi is the mean beneﬁt. Atkinson’s inequality index is a welfare-based measure
of inequality: The measure compares the actual average beneﬁt individuals receive under beneﬁt
distribution b (i.e. µ) with its Equally Distributed Equivalent (EDE)—the level of beneﬁt that if
i=1 b1
obtained by every individual  would result in the same level of welfare as that of b (i.e. 1
).
It is easy to verify that for 0 <↵< 1  the generalized entropy and Atkinson index result in the same
total ordering among beneﬁt distributions (see Proposition 3 in Appendix B). Furthermore  for a
ﬁxed mean beneﬁt µ  our measure results in the same indifference curves and total ordering as the
Atkinson index with  = 1  ↵.
Proposition 2 Consider two beneﬁt vectors b  b0  0 with equal means (µ = µ0). For 0 <↵< 1 
A1↵(b)  A1↵(b0) if and only if W↵(b) W ↵(b0).
Tradeoffs Among Different Notions of Fairness We end this section by establishing the existence
of multilateral tradeoffs among social welfare  accuracy  individual  and statistical notions of fairness.
We illustrate this by ﬁnding the predictive model that optimizes each of these quantities. In Table 1
we compare these optimal predictors in two different cases: 1) In the realizable case  we assume
the existence of a hypothesis h⇤ 2H such that y = h⇤(x)  i.e.  h⇤ achieves perfect prediction
accuracy. 2) In the unrealizable case  we assume the existence of a hypothesis h⇤ 2H   such
that h⇤(x) = E[y|x])  i.e.  h⇤ is the Bayes Optimal Predictor. We use the following notations:

6

Table 1: Optimal predictions with respect to different fairness notions.
Regression

Classiﬁcation

Realizable
ˆy ⌘ ymax
ˆy = h⇤(x)
ˆy ⌘ c
ˆy ⌘ c

ˆy ⌘ ymin or ˆy = h⇤(x)
ˆy ⌘ ymax or ˆy = h⇤(x)

Unrealizable
ˆy ⌘ ymax
ˆy ⌘ ymax
ˆy ⌘ c
ˆy ⌘ c
ˆy ⌘ ymin
ˆy ⌘ ymax

Social welfare
Atkinson index

Dwork et al.’s notion

Mean difference

Positive residual diff.
Negative residual diff.

Realizable
ˆy ⌘ 1
ˆy = h⇤(x)
ˆy ⌘ 0 or 1
ˆy ⌘ 0 or 1

ˆy ⌘ 0 or ˆy = h⇤(x)
ˆy ⌘ 1 or ˆy = h⇤(x)

Unrealizable

ˆy ⌘ 1
ˆy ⌘ 1
ˆy ⌘ 1 or 0
ˆy ⌘ 1 or 0
ˆy ⌘ 0
ˆy ⌘ 1

ymax = maxh2H x2X h(x) and ymin = minh2H x2X h(x). The precise deﬁnition of each notion in
Table 1 can be found in Appendix C.
As illustrated in Table 1  there is no unique predictors that simultaneously optimizes social welfare 
accuracy  individual  and statistical notions of fairness. Take the unrealizable classiﬁcation as an
example. Optimizing for accuracy requires the predictions to follow the Bayes optimal classiﬁer.
A lower bound on social welfare requires the model to predict the desirable outcome (i.e. 1) for a
large fraction of the population. To guarantee low positive residual difference  all individuals must be
predicted to belong to the negative class. In the next Section  we will investigate these tradeoffs in
more detail and through experiments on two real-world datasets.

3 Experiments

In this section  we empirically illustrate our proposal  and investigate the tradeoff between our family
of measures and accuracy  as well as existing deﬁnitions of group discrimination and individual
fairness. We ran our experiments on a classiﬁcation data set (Propublica’s COMPAS dataset [Larson
et al.  2016])  as well as a regression dataset (Crime and Communities data set [Lichman  2013]).7
For regression  we deﬁned the beneﬁt function as follows: b(y  ˆy) = ˆy  y + 1. On the Crime data
set this results in beneﬁt levels between 0 and 2. For classiﬁcation  we deﬁned the beneﬁt function as
follows: b(y  ˆy) = cy ˆy + dy where y 2 {1  1}  c1 = 0.5  d1 = 0.5  and c1 = 0.25  d1 = 1.25.
This results in beneﬁt levels 0 for false negatives  1 for true positives and true negatives  and 1.5 for
false positives.

Welfare as a Measure of Fairness Our proposed family of measures is relative by design: It allows
for meaningful comparison among different unfair alternatives. Furthermore  there is no unique
value of our measures that always correspond to perfect fairness. This is in contrast with previously
proposed  absolute notions of fairness which characterize the condition of perfect fairness—as
opposed to measuring the degree of unfairness of various unfair alternatives. We start our empirical
analysis by illustrating that our proposed measures can compare and rank different predictive models.
We trained the following models on the COMPAS dataset: a multi-layered perceptron  fully connected
with one hidden layer with 100 units (NN)  the AdaBoost classiﬁer (Ada)  Logistic Regression (LR) 
a decision tree classiﬁer (Tree)  a nearest neighbor classiﬁer (KNN). Figure 2 illustrates how these
learning models compare with one another according to accuracy  Atkinson index  and social welfare.
All values were computed using 20-fold cross validation. The conﬁdence intervals are formed
assuming samples come from Student’s t distribution. As shown in Figure 2  the rankings obtained
from Atkinson index and social welfare are identical. Note that this is consistent with Proposition 2.
Given the fact that all models result in similar mean beneﬁts  we expect the rankings to be consistent.

Impact on Model Parameters Next  we study the impact of changing ⌧ on the trained model
parameters (see Figure 3a). We observe that as ⌧ increases  the intercept continually rises to guarantee
high levels of beneﬁt and social welfare. On the COMPAS dataset  we notice an interesting trend for
the binary feature sex (0 is female  1 is male); initially being male has a negative weight and thus
a negative impact on the classiﬁcation outcome  but as ⌧ is increased  the sign changes to positive
to ensure men also get high beneﬁts. The trade-offs between our proposed measure and prediction

60-normalization requires the inequality index to be 0 if and only if the distribution is perfectly equal/uniform.
7A more detailed description of the data sets and our preprocessing steps can be found in Appendix C.

7

Figure 2: Comparison of different learning models according to accuracy  social welfare (↵ = 0.8)
and Atkinson index ( = 0.2). The mean beneﬁts are 0.97 for LogReg  0.96 for NN  0.96 for
AdaBoost  0.89 for KNN  and 0.89 for Tree. Note that for Atkinson measure  smaller values
correspond to fairer outcomes  where as for social welfare larger values reﬂect greater fairness.

(a)

(b)

(c)

Figure 3: (a) Changes in weights—✓ in linear and logistic regression—as the function of ⌧. Note the
continuous rise of the intercept with ⌧. (b) Atkinson index as a function of the threshold ⌧. Note the
consistent decline in inequality as ⌧ increases. (c) Average violation of Dwork et al.’s constraints as a
function of ⌧. Trends are different for regression and classiﬁcation.

accuracy can be found in Figure 5 in Appendix C. As one may expect  imposing more restrictive
fairness constraints (larger ⌧ and smaller ↵)  results in higher loss of accuracy.
Next  we will empirically investigate the tradeoff between our family of measures and existing
deﬁnitions of group discrimination and individual fairness. Note that since our proposed family of
measures is relative  we believe it is more suitable to focus on tradeoffs as opposed to impossibility
results. (Existing impossibility results (e.g. [Kleinberg et al.  2017]) establish that a number of
absolute notions of fairness cannot hold simultaneously.)

Trade-offs with Individual Notions Figures 3b  3c illustrate the impact of bounding our measure
on existing individual measures of fairness. As expected  we observe that higher values of ⌧ (i.e.
social welfare) consistently result in lower inequality. Note that for classiﬁcation  ⌧ cannot be
arbitrarily large (due to the infeasibility of achieving arbitrarily large social welfare levels). Also as
expected  smaller ↵ values (i.e. higher degrees of risk aversion) lead to a faster drop in inequality.
The impact of our mechanism on the average violation of Dwork et al.’s constraints is slightly more
nuanced: as ⌧ increases  initially the average violation of Dwork et al.’s pairwise constraints go
down. For classiﬁcation  the decline continues until the measure reaches 0—which is what we expect
the measure to amount to once almost every individual receives the positive label. For regression
in contrast  the initial decline is followed by a phase in which the measure quickly climbs back up

8

(a)

(b)

(c)

Figure 4: Group discrimination as a function of ⌧ for different values of ↵. (a) Negative residual
difference is decreasing with ⌧ and approaches 0. (b) Positive residual difference monotonically
approaches a certain asymptote. (c) Note the striking similarity of patterns for the average violation
of Dwork et al.’s constraints and mean difference.

to its initial (high) value. The reason is for larger values of ⌧  the high level of social welfare is
achieved mainly by means of adding a large intercept to the unconstrained model’s predictions (see
Figure 3a). Due to its translation invariance property  the addition of an intercept cannot limit the
average violation of Dwork et al.’s constraints.

Trade-offs with Statistical Notions Next  we illustrate the impact of bounding our measure on
statistical measures of fairness. For the Crime and Communities dataset  we assumed a neighborhood
belongs to the protected group if and only if the majority of its residents are non-Caucasian  that is 
the percentage of African-American  Hispanic  and Asian residents of the neighborhood combined 
is above 50%. For the COMPAS dataset we took race as the sensitive feature. Figure 4a shows the
impact of ⌧ and ↵ on false negative rate difference and its continuous counterpart  negative residual
difference. As expected  both quantities decrease with ⌧ until they reach 0—when everyone receives
a label at least as large as their ground truth. The trends are similar for false positive rate difference
and its continuous counterpart  positive residual difference (Figure 4b). Note that in contrast to
classiﬁcation  on our regression data set  even though positive residual difference decreases with ⌧  it
never reaches 0. Figure 4c shows the impact of ⌧ and ↵ on demographic parity and its continuous
counterpart  means difference. Note the striking similarity between this plot and Figure 3c. Again
here for large values of ⌧  guaranteeing high social welfare requires adding a large intercept to the
unconstrained model’s prediction. See Proposition 4 in Appendix B  where we formally prove this
point for the special case of linear predictors. The addition of intercept in this fashion  cannot put an
upper-bound on a translation-invariant measure like mean difference.

4 Summary and Future Directions

Our work makes an important connection between the growing literature on fairness for machine
learning  and the long-established formulations of cardinal social welfare in economics. Thanks to
their convexity  our measures can be bounded as part of any convex loss minimization program. We
provided evidence suggesting that constraining our measures often leads to bounded inequality in
algorithmic outcomes. Our focus in this work was on a normative theory of how rational individuals
should compare different algorithmic alternatives. We plan to extend our framework to descriptive
behavioural theories  such as prospect theory [Kahneman and Tversky  2013]  to explore the human
perception of fairness and contrast it with normative prescriptions.

9

Acknowledgments

H. Heidari and A. Krause acknowledge support from CTI grant no. 27248.1 PFES-ES. Krishna P.
Gummadi was supported in part by a European Research Council (ERC) Advanced Grant “Founda-
tions for Fair Social Computing” (No. 789373).

References
Yoram Amiel and Frank A. Cowell. Inequality  welfare and monotonicity. In Inequality  Welfare and

Poverty: Theory and Measurement  pages 35–46. Emerald Group Publishing Limited  2003.
Julia Angwin  Jeff Larson  Surya Mattu  and Lauren Kirchner. Machine bias. Propublica  2016.
Anthony B. Atkinson. On the measurement of inequality. Journal of Economic Theory  2(3):244–263 

1970.

Anna Barry-Jester  Ben Casselman  and Dana Goldstein. The new science of sentencing. The

Marshall Project  August 2015.

Toon Calders  Asim Karim  Faisal Kamiran  Wasif Ali  and Xiangliang Zhang. Controlling attribute
effect in linear regression. In Proceedings of the International Conference on Data Mining  pages
71–80. IEEE  2013.

Fredrik Carlsson  Dinky Daruvala  and Olof Johansson-Stenman. Are people inequality-averse  or

just risk-averse? Economica  72(287):375–396  2005.

Sam Corbett-Davies  Emma Pierson  Avi Feller  Sharad Goel  and Aziz Huq. Algorithmic decision
making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining  pages 797–806. ACM  2017.

Frank A. Cowell and Erik Schokkaert. Risk perceptions and distributional judgments. European

Economic Review  45(4-6):941–952  2001.

Camilo Dagum. On the relationship between income inequality measures and social welfare functions.

Journal of Econometrics  43(1-2):91–102  1990.

Hugh Dalton. The measurement of the inequality of incomes. The Economic Journal  30(119):348–

361  1920.

Gerard Debreu. Topological methods in cardinal utility theory. Technical report  Cowles Foundation

for Research in Economics  Yale University  1959.

Cynthia Dwork  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Richard Zemel. Fairness through
awareness. In Proceedings of the Innovations in Theoretical Computer Science Conference  pages
214–226. ACM  2012.

Michael Feldman  Sorelle A. Friedler  John Moeller  Carlos Scheidegger  and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In Proceedings of the International Conference
on Knowledge Discovery and Data Mining  pages 259–268. ACM  2015.

Samuel Freeman. Original position. In Edward N. Zalta  editor  The Stanford Encyclopedia of

Philosophy. Metaphysics Research Lab  Stanford University  winter 2016 edition  2016.

William M. Gorman. The structure of utility functions. The Review of Economic Studies  35(4):367–

390  1968.

Moritz Hardt  Eric Price  and Nati Srebro. Equality of opportunity in supervised learning.
Proceedings of Advances in Neural Information Processing Systems  pages 3315–3323  2016.

In

John C. Harsanyi. Cardinal utility in welfare economics and in the theory of risk-taking. Journal of

Political Economy  61(5):434–435  1953.

John C. Harsanyi. Cardinal welfare  individualistic ethics  and interpersonal comparisons of utility.

Journal of political economy  63(4):309–321  1955.

10

Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In
Handbook of the Fundamentals of Financial Decision Making: Part I  pages 99–127. World
Scientiﬁc  2013.

Faisal Kamiran and Toon Calders. Classifying without discriminating. In Proceedings of the 2nd

International Conference on Computer  Control and Communication  pages 1–6. IEEE  2009.

Toshihiro Kamishima  Shotaro Akaho  and Jun Sakuma. Fairness-aware learning through regulariza-
tion approach. In Proceedings of the International Conference on Data Mining Workshops  pages
643–650. IEEE  2011.

Jon Kleinberg  Sendhil Mullainathan  and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. In In proceedings of the 8th Innovations in Theoretical Computer Science
Conference  2017.

Jeff Larson  Surya Mattu  Lauren Kirchner  and Julia Angwin. Data and analysis for ‘How we analyzed
the COMPAS recidivism algorithm’. https://github.com/propublica/compas-analysis 
2016.

Sam Levin. A beauty contest was judged by AI and the robots didn’t like dark skin. The Guardian 

2016.

M. Lichman. UCI machine learning repository: Communities and crime data set. http://archive.

ics.uci.edu/ml/datasets/Communities+and+Crime  2013.

Clair Miller. Can an algorithm hire better than a human? The New York Times  June 25 2015.

Retrieved 4/28/2016.

Hervé Moulin. Fair division and collective welfare. MIT press  2004.
Kevin Petrasic  Benjamin Saul  James Greig  and Matthew Bornfreund. Algorithms and bias: What

lenders need to know. White & Case  2017.

Arthur Cecil Pigou. Wealth and welfare. Macmillan and Company  limited  1912.
John Rawls. A theory of justice. Harvard university press  2009.
Kevin W. S. Roberts. Interpersonal comparability and social choice theory. The Review of Economic

Studies  pages 421–439  1980.

Cynthia Rudin. Predictive policing using machine learning to detect patterns of crime. Wired

Magazine  August 2013. Retrieved 4/28/2016.

Joseph Schwartz and Christopher Winship. The welfare approach to measuring inequality. Sociologi-

cal methodology  11:1–36  1980.

Amartya Sen. On weights and measures:

informational constraints in social welfare analysis.

Econometrica: Journal of the Econometric Society  pages 1539–1572  1977.

Till Speicher  Hoda Heidari  Nina Grgic-Hlaca  Krishna P. Gummadi  Adish Singla  Adrian Weller 
and Muhammad Bilal Zafar. A uniﬁed approach to quantifying algorithmic unfairness: Measuring
individual and group unfairness via inequality indices.
In Proceedings of the International
Conference on Knowledge Discovery and Data Mining  2018.

Latanya Sweeney. Discrimination in online ad delivery. Queue  11(3):10  2013.
Hal R. Varian. Equity  envy  and efﬁciency. Journal of economic theory  9(1):63–91  1974.
Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  and Krishna P Gummadi. Fairness
beyond disparate treatment & disparate impact: Learning classiﬁcation without disparate mistreat-
ment. In Proceedings of the 26th International Conference on World Wide Web  pages 1171–1180 
2017.

Muhammad Bilal Zafar  Isabel Valera  Manuel Gomez Rodriguez  and Krishna P. Gummadi. Fairness
constraints: Mechanisms for fair classiﬁcation. In Proceedings of the 20th International Conference
on Artiﬁcial Intelligence and Statistics  2017.

11

Muhammad Bilal Zafar  Isabel Valera  Manuel Rodriguez  Krishna Gummadi  and Adrian Weller.
From parity to preference-based notions of fairness in classiﬁcation. In Proceedings of Advances
in Neural Information Processing Systems  pages 228–238  2017.

12

,Jian Zhang
Alex Schwing
Raquel Urtasun
Gang Niu
Marthinus Christoffel du Plessis
Tomoya Sakai
Masashi Sugiyama
Hoda Heidari
Claudio Ferrari
Krishna Gummadi
Andreas Krause