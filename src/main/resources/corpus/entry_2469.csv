2011,High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity,Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner  many  applications involve noisy and/or missing data  possibly involving dependencies. We study these issues in the context of high-dimensional  sparse linear regression  and propose novel estimators for the cases of noisy  missing  and/or dependent data. Many standard approaches to noisy or missing data  such as those using the EM algorithm  lead to optimization problems that are inherently non-convex  and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs  we are able to both analyze the statistical error associated with any global optimum  and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side  we provide non-asymptotic bounds that hold with high probability for the cases of noisy  missing  and/or dependent data. On the computational side  we prove that under the same types of conditions required for statistical consistency  the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations  showing agreement with the predicted scalings.,High-dimensional regression with noisy and missing data:

Provable guarantees with non-convexity

Po-Ling Loh

Department of Statistics

University of California  Berkeley

Berkeley  CA 94720

ploh@berkeley.edu

Martin J. Wainwright

Departments of Statistics and EECS
University of California  Berkeley

Berkeley  CA 94720

wainwrig@stat.berkeley.edu

Abstract

Although the standard formulations of prediction problems involve fully-observed and noise-
less data drawn in an i.i.d. manner  many applications involve noisy and/or missing data 
possibly involving dependencies. We study these issues in the context of high-dimensional
sparse linear regression  and propose novel estimators for the cases of noisy  missing  and/or
dependent data. Many standard approaches to noisy or missing data  such as those using the
EM algorithm  lead to optimization problems that are inherently non-convex  and it is difﬁcult
to establish theoretical guarantees on practical algorithms. While our approach also involves
optimizing non-convex programs  we are able to both analyze the statistical error associated
with any global optimum  and prove that a simple projected gradient descent algorithm will
converge in polynomial time to a small neighborhood of the set of global minimizers. On
the statistical side  we provide non-asymptotic bounds that hold with high probability for the
cases of noisy  missing  and/or dependent data. On the computational side  we prove that
under the same types of conditions required for statistical consistency  the projected gradient
descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate
these theoretical predictions with simulations  showing agreement with the predicted scalings.

1

Introduction

In standard formulations of prediction problems  it is assumed that the covariates are fully-observed and sam-
pled independently from some underlying distribution. However  these assumptions are not realistic for many
applications  in which covariates may be observed only partially  observed with corruption  or exhibit dependen-
cies. Consider the problem of modeling the voting behavior of politicians: in this setting  votes may be missing
due to abstentions  and temporally dependent due to collusion or “tit-for-tat” behavior. Similarly  surveys often
suffer from the missing data problem  since users fail to respond to all questions. Sensor network data also tends
to be both noisy due to measurement error  and partially missing due to failures or drop-outs of sensors.
There are a variety of methods for dealing with noisy and/or missing data  including various heuristic meth-
ods  as well as likelihood-based methods involving the expectation-maximization (EM) algorithm (e.g.  see the
book [1] and references therein). A challenge in this context is the possible non-convexity of associated opti-
mization problems. For instance  in applications of EM  problems in which the negative likelihood is a convex
function often become non-convex with missing or noisy data. Consequently  although the EM algorithm will
converge to a local minimum  it is difﬁcult to guarantee that the local optimum is close to a global minimum.
In this paper  we study these issues in the context of high-dimensional sparse linear regression  in the case
when the predictors or covariates are noisy  missing  and/or dependent. Our main contribution is to develop and
study some simple methods for handling these issues  and to prove theoretical results about both the associated
statistical error and the optimization error. Like EM-based approaches  our estimators are based on solving
optimization problems that may be non-convex; however  despite this non-convexity  we are still able to prove
that a simple form of projected gradient descent will produce an output that is “sufﬁciently close”—meaning as

1

small as the statistical error—to any global optimum. As a second result  we bound the size of this statistical
error  showing that it has the same scaling as the minimax rates for the classical cases of perfectly observed and
independently sampled covariates. In this way  we obtain estimators for noisy  missing  and/or dependent data
with guarantees similar to the usual fully-observed and independent case. The resulting estimators allow us to
solve the problem of high-dimensional Gaussian graphical model selection with missing data.
There is a large body of work on the problem of corrupted covariates or errors-in-variables for regression
problems (see the papers and books [2  3  4  5] and references therein). Much of the earlier theoretical work
is classical in nature  where the sample size n diverges with the dimension p held ﬁxed. Most relevant to this
paper is more recent work that has examined issues of corrupted and/or missing data in the context of high-
dimensional sparse linear models  allowing for n (cid:28) p. St¨adler and B¨uhlmann [6] developed an EM-based
method for sparse inverse covariance matrix estimation in the missing data regime  and used this result to
derive an algorithm for sparse linear regression with missing data. As mentioned above  however  it is difﬁcult
to guarantee that EM will converge to a point close to a global optimum of the likelihood  in contrast to the
methods studied here. Rosenbaum and Tsybakov [7] studied the sparse linear model when the covariates are
corrupted by noise  and proposed a modiﬁed form of the Dantzig selector  involving a convex program. This
convexity produces a computationally attractive method  but the statistical error bounds that they establish scale
proportionally with the size of the additive perturbation  hence are often weaker than the bounds that can be
proved using our methods.
The remainder of this paper is organized as follows. We begin in Section 2 with background and a precise
description of the problem. We then introduce the class of estimators we will consider and the form of the
projected gradient descent algorithm. Section 3 is devoted to a description of our main results  including a pair
of general theorems on the statistical and optimization error  and then a series of corollaries applying our results
to the cases of noisy  missing  and dependent data. In Section 4  we demonstrate simulations to conﬁrm that our
methods work in practice. For detailed proofs  we refer the reader to the technical report [8].
Notation. For a matrix M  we write kMkmax := maxi j |mij| to be the elementwise ‘∞-norm of M. Further-
more  |||M|||1 denotes the induced ‘1-operator norm (maximum absolute column sum) of M  and |||M|||op is the
induced ‘2-operator norm (spectral norm) of M. We write κ(M) := λmax(M )

λmin(M )   the condition number of M.

2 Background and problem set-up

In this section  we provide a formal description of the problem and motivate the class of estimators studied in
the paper. We then describe a class of projected gradient descent algorithms to be used in the sequel.

2.1 Observation model and high-dimensional framework
Suppose we observe a response variable yi ∈ R that is linked to a covariate vector xi ∈ Rp via the linear model
(1)
Here  the regression vector β∗ ∈ Rp is unknown  and i ∈ R is observation noise  independent of xi. Rather than
directly observing each xi ∈ Rp  we observe a vector zi ∈ Rp linked to xi via some conditional distribution:

yi = hxi  β∗i + i 

for i = 1  2  . . .   n.

zi ∼ Q(· | xi) 

for i = 1  2  . . .   n.

(2)

This setup allows us to model various types of disturbances to the covariates  including

(a) Additive noise: We observe zi = xi + wi  where wi ∈ Rp is a random vector independent of xi  say
zero-mean with known covariance matrix Σw.
(b) Missing data: For a fraction ρ ∈ [0  1)  we observe a random vector zi ∈ Rp such that independently
for each component j  we observe zij = xij with probability 1 − ρ  and zij = ∗ with probability ρ.
This model can also be generalized to allow for different missing probabilities for each covariate.

Our ﬁrst set of results is deterministic  depending on speciﬁc instantiations of the observed variables
{(yi  zi)}n
i=1. However  we are also interested in proving results that hold with high probability when the
xi’s and zi’s are drawn at random from some distribution. We develop results for both the i.i.d. setting and the
case of dependent covariates  where the xi’s are generated according to a stationary vector autoregressive (VAR)
process. Furthermore  we work within a high-dimensional framework where n (cid:28) p  and assume β∗ has at most
k non-zero parameters  where the sparsity k is also allowed to increase to inﬁnity with the sample size n. We
assume the scaling kβ∗k2 = O(1)  which is reasonable in order to have a non-diverging signal-to-noise ratio.

2

2.2 M-estimators for noisy and missing covariates
We begin by examining a simple deterministic problem. Let Cov(X) = Σx (cid:31) 0  and consider the program

As long as the constraint radius R is at least kβ∗k1  the unique solution to this convex program isbβ = β∗. This
Σxβ∗  denoted bybΓ andbγ  respectively  and consider the modiﬁed program and its regularized version:

idealization suggests various estimators based on the plug-in principle. We form unbiased estimates of Σx and

kβk1≤R

(3)

bβ ∈ arg min

2 βT Σxβ − hΣxβ∗  βi(cid:9).
(cid:8)1
(cid:8)1
2 βTbΓβ − hbγ  βi(cid:9) 
bβ ∈ arg min
(cid:8)1
bβ ∈ arg min
2 βTbΓβ − hbγ  βi + λnkβk1
bΓLas :=
X T X and bγLas :=

kβk1≤R

X T y 

β∈Rp

1
n

1
n

(cid:9) 

where λn > 0 is the regularization parameter. The Lasso [9  10] is a special case of these programs  where

(4)

(5)

(6)

and we have introduced the shorthand y = (y1  . . .   yn)T ∈ Rn  and X ∈ Rn×p  with xT
i as its ith row. In
this paper  we focus on more general instantiations of the programs (4) and (5)  involving different choices of

the pair (bΓ bγ) that are adapted to the cases of noisy and/or missing data. Note that the matrixbΓLas is positive
natural choice of the matrixbΓ is not positive semideﬁnite  hence the loss functions appearing in the problems (4)

semideﬁnite  so the Lasso program is convex. In sharp contrast  for the cases of noisy or missing data  the most

and (5) are non-convex. It is generally impossible to provide a polynomial-time algorithm that converges to a
(near) global optimum of a non-convex problem. Remarkably  we prove that a simple projected gradient descent
algorithm still converges with high probability to a vector close to any global optimum in our setting.
Let us illustrate these ideas with some examples:
Example 1 (Additive noise). Suppose we observe the n × p matrix Z = X + W   where W is a random
matrix independent of X  with rows wi drawn i.i.d. from a zero-mean distribution with known covariance Σw.
Consider the pair

(7)
which correspond to unbiased estimators of Σx and Σxβ∗  respectively. Note that when Σw = 0 (corresponding
not positive semideﬁnite in the high-dimensional regime (n (cid:28) p) of interest. Indeed  since the matrix 1

to the noiseless case)  the estimators reduce to the standard Lasso. However  when Σw 6= 0  the matrixbΓadd is
has rank at most n  the subtracted matrix Σw may causebΓadd to have a large number of negative eigenvalues.

n Z T Z
Example 2 (Missing data). Suppose each entry of X is missing independently with probability ρ ∈ [0  1)  and
we observe the matrix Z ∈ Rn×p with entries

Z T y 

bΓadd :=

1
n

Z T Z − Σw and bγadd :=

1
n

(cid:26)Xij with probability 1 − ρ 
− ρ diag(cid:0)eZ TeZ
and bγmis :=

otherwise.

(cid:1)

0

Zij =

bΓmis := eZ TeZ

eZ T y 

1
n

Given the observed matrix Z ∈ Rn×p  consider an estimator of the general form (4)  based on the choices

where eZij = Zij/(1− ρ). It is easy to see that the pair (bΓmis bγmis) reduces to the pair (bΓLas bγLas) for the standard
in equation (8) has rank at most n  so the subtracted diagonal matrix may cause the matrixbΓmis to have a
Lasso when ρ = 0  corresponding to no missing data. In the more interesting case when ρ ∈ (0  1)  the matrix
eZT eZ
large number of negative eigenvalues when n (cid:28) p  and the associated quadratic function is not convex.

(8)

n

n

n

2.3 Restricted eigenvalue conditions

Given an estimatebβ  there are various ways to assess its closeness to β∗. We focus on the ‘2-norm kbβ−β∗k2  as
well as the closely related ‘1-norm kbβ − β∗k1. When the covariate matrix X is fully observed (so that the Lasso
can be applied)  it is well understood that a sufﬁcient condition for ‘2-recovery is that the matrixbΓLas = 1

n X T X
satisfy a restricted eigenvalue (RE) condition (e.g.  [11  12  13]). In this paper  we use the following condition:

3

Deﬁnition 1 (Lower-RE condition). The matrixbΓ satisﬁes a lower restricted eigenvalue condition with curva-

ture α‘ > 0 and tolerance τ‘(n  p) > 0 if

θTbΓθ ≥ α‘ kθk2

2 − τ‘(n  p)kθk2

1

for all θ ∈ Rp.

has low ‘2-error for any vector β∗ supported on any subset of size at most k . 1

It can be shown that when the Lasso matrixbΓLas = 1
that for various random choices of the design matrix X  the Lasso matrixbΓLas will satisfy such an RE condition
Deﬁnition 2 (Upper-RE condition). The matrix bΓ satisﬁes an upper restricted eigenvalue condition with

n X T X satisﬁes this RE condition (9)  the Lasso estimate
τ‘(n p). Moreover  it is known

with high probability (e.g.  [14]). We also make use of the analogous upper restricted eigenvalue condition:

(9)

smoothness αu > 0 and tolerance τu(n  p) > 0 if

θTbΓθ ≤ αukθk2

2 + τu(n  p)kθk2

1

for all θ ∈ Rp.

(10)

In recent work on high-dimensional projected gradient descent  Agarwal et al. [15] use a more general form of
bounds (9) and (10)  called the restricted strong convexity (RSC) and restricted smoothness (RSM) conditions.

2.4 Projected gradient descent

In addition to proving results about the global minima of programs (4) and (5)  we are also interested in
polynomial-time procedures for approximating such optima. We show that the simple projected gradient descent
algorithm can be used to solve the program (4). The algorithm generates a sequence of iterates βt according to

βt+1 = Π(cid:0)βt − 1

(bΓβt −bγ)(cid:1) 

η

(11)

where η > 0 is a stepsize parameter  and Π denotes the ‘2-projection onto the ‘1-ball of radius R. This
projection can be computed rapidly in O(p) time  for instance using a procedure due to Duchi et al. [16]. Our
analysis shows that under a reasonable set of conditions  the iterates for the family of programs (4) converges to
a point extremely close to any global optimum in both ‘1-norm and ‘2-norm  even for the non-convex program.

3 Main results and consequences

We provide theoretical guarantees for both the constrained estimator (4) and the regularized variant

bβ ∈ arg min

kβk1≤b0

√
k

(cid:8)1
2 βTbΓβ − hbγ  βi + λnkβk1

(cid:9) 

for a constant b0 ≥ kβ∗k2  which is a hybrid between the constrained (4) and regularized (5) programs.

(12)

3.1 Statistical error

In controlling the statistical error  we assume that the matrixbΓ satisﬁes a lower-RE condition with curvature
α‘ and tolerance τ‘(n  p)  as previously deﬁned (9). In addition  recall that the matrixbΓ and vectorbγ serve

as surrogates to the deterministic quantities Σx ∈ Rp×p and Σxβ∗ ∈ Rp  respectively. We assume there is a
function ϕ(Q  σ)  depending on the standard deviation σ of the observation noise vector  from equation (1)
and the conditional distribution Q from equation (2)  such that the following deviation conditions are satisﬁed:

rlog p
The following result applies to any global optimumbβ of the program (12) with λn ≥ 4 ϕ(Q  σ)
Theorem 1 (Statistical error). Suppose the surrogates (bΓ bγ) satisfy the deviation bounds (13)  and the matrix
bΓ satisﬁes the lower-RE condition (9) with parameters (α‘  τ‘) such that

k(bΓ − Σx)β∗k∞ ≤ ϕ(Q  σ)

kbγ − Σxβ∗k∞ ≤ ϕ(Q  σ)

rlog p

n :

.

q log p

(13)

and

n

n

k τ‘(n  p) ≤ min(cid:8) α‘

√

√

ϕ(Q  σ)

2 b0

 

k

128

rlog p

(cid:9).

n

(14)

4

Then for any vector β∗ with sparsity at most k  there is a universal positive constant c0 such that any global

optimumbβ satisﬁes the bounds

√

kbβ − β∗k2 ≤ c0
kbβ − β∗k1 ≤ 8 c0 k

α‘

k

α‘

max(cid:8)ϕ(Q  σ)
max(cid:8)ϕ(Q  σ)

rlog p
rlog p

n

  λn

  λn

n

(cid:9) 
(cid:9).

and

(15a)

(15b)

The same bounds (without λn) also apply to the constrained program (4) with radius choice R = kβ∗k1.

Remarks: Note that for the standard Lasso pair (bΓLas bγLas)  bounds of the form (15) for sub-Gaussian noise

are well-known from past work (e.g.  [12  17  18  19]). The novelty of Theorem 1 is in allowing for general
pairs of such surrogates  which can lead to non-convexity in the underlying M-estimator.

3.2 Optimization error

Although Theorem 1 provides guarantees that hold uniformly for any choice of global minimizer  it does not
provide any guidance on how to approximate such a global minimizer using a polynomial-time algorithm.

Nonetheless  we are able to show that for the family of programs (4)  under reasonable conditions onbΓ sat-
is active. Suppose that the surrogate matrixbΓ satisﬁes the lower-RE (9) and upper-RE (10) conditions with
constants (c1  c2) such that for any global optimumbβ  the gradient descent iterates {βt}∞

isﬁed in various settings  a simple projected gradient method will converge geometrically fast to a very good
approximation of any global optimum.
Theorem 2 (Optimization error). Consider the program (4) with any choice of radius R for which the constraint
τu  τl (cid:16) log p
n   and that we apply projected gradient descent (11) with constant stepsize η = 2αu. Then as
long as n % k log p  there is a contraction coefﬁcient γ ∈ (0  1) independent of (n  p  k) and universal positive
t=0 satisfy the bound

2 ≤ γtkβ0 −bβk2

kβt −bβk2
kβt −bβk1 ≤ 2

2 + c1

log p

n

√

k kβt −bβk2 + 2

√

In addition  we have the ‘1-bound

2

kbβ − β∗k2
1 + c2kbβ − β∗k2
k kbβ − β∗k2 + 2kbβ − β∗k1
n ) and O(cid:0)k

for all t = 0  1  2  . . ..

(16)

for all t = 0  1  2  . . ..

(17)

in polynomial-time  and any global optimum bβ of the program (4)  which may be difﬁcult to compute. Since

Note that the bound (16) controls the ‘2-distance between the iterate βt at time t  which is easily computed
γ ∈ (0  1)  the ﬁrst term in the bound vanishes as t increases. Together with Theorem 1  equations (16) and (17)
imply that the ‘2- and ‘1-optimization error are bounded as O( k log p

(cid:1)  respectively.

q log p

n

3.3 Some consequences

Both Theorems 1 and 2 are deterministic results; applying them to speciﬁc models requires additional work to
establish the stated conditions. We turn to the statements of some consequences of these theorems for different
cases of noisy  missing  and dependent data. A zero-mean random variable Z is sub-Gaussian with parameter
σ > 0 if E(eλZ) ≤ exp(λ2σ2/2) for all λ ∈ R. We say that a random matrix X ∈ Rn×p is sub-Gaussian
i ∈ Rp is sampled independently from a zero-mean distribution with
with parameters (Σ  σ2) if each row xT
covariance Σ  and for any unit vector u ∈ Rp  the random variable uT xi is sub-Gaussian with parameter at
most σ.
We begin with the case of i.i.d. samples with additive noise  as described in Example 1.
Corollary 1. Suppose we observe Z = X + W   where the random matrices X  W ∈ Rn×p are sub-
Gaussian with parameters (Σx  σ2
w)  respectively  and the sample size is lower-bounded as

  1(cid:9)k log p. Then for the M-estimator based on the surrogates (bΓadd bγadd)  the results of

n % max(cid:8)(cid:0) σ2

x) and (Σw  σ2

(cid:1)2

λmin(Σx)

x+σ2

w

Theorems 1 and 2 hold with parameters

α‘ =

1
2 λmin(Σx) and ϕ(Q  σ) = c0

with probability at least 1 − c1 exp(−c2 log p).

5

(cid:8)σ2

pσ2

(cid:9) 

x + σ2

w + σ

x + σ2

w

For i.i.d. samples with missing data  we have the following:
Corollary 2. Suppose X ∈ Rn×p is a sub-Gaussian matrix with parameters (Σx  σ2
data matrix with parameter ρ.
(1−ρ)4
probability at least 1 − c1 exp(−c2 log p) for α‘ = 1
ϕ(Q  σ) = c0

min(Σx)   1(cid:1)k log p  then Theorems 1 and 2 hold with
(cid:0)σ + σx

If n % max(cid:0)

x)  and Z is the missing

(cid:1).

σ4
x

λ2

1

2 λmin(Σx) and
σx
1 − ρ

1 − ρ

xi+1 = Axi + vi 

Now consider the case where the rows of X are drawn from a vector autoregressive (VAR) process according to
(18)
where vi ∈ Rp is a zero-mean noise vector with covariance matrix Σv  and A ∈ Rp×p is a driving matrix with
spectral norm |||A|||2 < 1. We assume the rows of X are drawn from a Gaussian distribution with covariance
Σx  such that Σx = AΣxAT + Σv  so the process is stationary. Corollary 3 corresponds to the case of additive
noise for a Gaussian VAR process. A similar result can be derived in the missing data setting.
Corollary 3. Suppose the rows of X are drawn according to a Gaussian VAR process with driving matrix A.

Suppose the additive noise matrix W is i.i.d. with Gaussian rows. If n % max(cid:0)

min(Σx)   1(cid:1)k log p  with

for i = 1  2  . . .   n − 1 

λ2

ζ4

ζ 2 = |||Σw|||op +

2|||Σx|||op
1 − |||A|||op

 

then Theorems 1 and 2 hold with probability at least 1 − c1 exp(−c2 log p) for α‘ = 1

2 λmin(Σx) and

ϕ(Q  σ) = c0(σζ + ζ 2).

3.4 Application to graphical model inverse covariance estimation

The problem of inverse covariance estimation for a Gaussian graphical model is closely related to the Lasso.
Meinshausen and B¨uhlmann [20] prescribed a way to recover the support of the precision matrix Θ when each
column of Θ is k-sparse  via linear regression and the Lasso. More recently  Yuan [21] proposed a method for

estimating Θ using linear regression and the Dantzig selector  and obtained error bounds on |||bΘ − Θ|||1 when

the columns of Θ are bounded in ‘1. Both of these results assume the rows of X are observed noiselessly and
independently.
Suppose we are given a matrix X ∈ Rn×p of samples from a multivariate Gaussian distribution  where each row
is distributed according to N(0  Σ). We assume the rows of X are either i.i.d. or sampled from a Gaussian VAR
process (18). Based on the modiﬁed Lasso  we devise a method to estimate Θ based on a corrupted observation
matrix Z. Let X j denote the jth column of X  and let X−j denote the matrix X with jth column removed. By
standard results on Gaussian graphical models  there exists a vector θj ∈ Rp−1 such that

X j = X−jθj + j 

(19)
where j is a vector of i.i.d. Gaussians and j ⊥⊥ X−j. Deﬁning aj := −(Σjj − Σj −jθj)−1  we have

Θj −j = ajθj. Our algorithm estimatesbθj andbaj for each j and combines the estimates to obtainbΘj −j =bajbθj.
pair (bΓ(j) bγ(j)) = (bΣ−j −j  1

In the additive noise case  we observe Z = X + W . The equations (19) yield Z j = X−jθj + (j + W j).
Note that δj = j + W j is a vector of i.i.d. Gaussians  and since X ⊥⊥ W   we have δj ⊥⊥ X−j. Hence  our
results on covariates with additive noise produce an estimate of θj by solving the program (4) or (12) with the
n Z T Z − Σw. When Z is a missing-data version of X 
using the modiﬁed Lasso program (4) or (12) with the estimators (bΓ(j) bγ(j))  to obtain estimatesbθj.
(1) Perform p linear regressions of the variables Z j upon the remaining variables Z−j 
(2) Estimate the scalars aj usingbaj := −(bΣjj −bΣj −jbθj)−1. SeteΘj −j =bajbθj andeΘjj = −baj.
|||Θ −eΘ|||1  where Sp is the set of symmetric p × p matrices.
(3) Construct the matrixbΘ = arg min

we similarly estimate the vectors θj with suitable corrections. We arrive at the following algorithm:
Algorithm 3.1.

n Z−jT Z j)  wherebΣ = 1

Θ∈Sp

Note that the minimization in step (3) is a linear program  so is easily solved with standard methods. We have:

6

Corollary 4. Suppose the columns of the matrix Θ are k-sparse  and suppose the condition number κ(Θ) is
nonzero and ﬁnite. Suppose the deviation conditions

k(bΓ(j) − Σ−j −j)θjk∞ ≤ ϕ(Q  σ)

rlog p

kbγ(j) − Σ−j −jθjk∞ ≤ ϕ(Q  σ)

hold for all j  and suppose we have the following additional deviation condition onbΣ:
Finally  suppose the lower-RE condition holds uniformly over the matrices bΓ(j) with the scaling (14). Then

rlog p

under the estimation procedure of Algorithm 3.1  there exists a universal constant c0 such that

(20)

(21)

n

n

.

n

and

rlog p
kbΣ − Σkmax ≤ cϕ(Q  σ)
(cid:0) ϕ(Q  σ)

λmin(Σ)

λmin(Σ)

|||bΘ − Θ|||op ≤ c0κ2(Σ)

rlog p

(cid:1)k

.

n

+ ϕ(Q  σ)

α‘

4 Simulations

In this section  we provide simulation results to conﬁrm that the scalings predicted by our theory are sharp.
In Figure 1  we plot the results of simulations under the additive noise model described in Example 1  using
Σx = I and Σw = σ2
wI with σw = 0.2. Panel (a) provides plots of ‘2-error versus the sample size n  for
p ∈ {128  256  512}. For all three choices of dimensions  the error decreases to zero as the sample size n in-
creases  showing consistency of the method. If we plot the ‘2-error versus the rescaled sample size n/(k log p) 
as depicted in panel (b)  the curves roughly align for different values of p  agreeing with Theorem 1. Panel (c)
shows analogous curves for VAR data with additive noise  using a driving matrix A with |||A|||op = 0.2.

(a)

Figure 1. Plots of the error kbβ − β∗k2 after running projected gradient descent on the non-convex objective  with
sparsity k ≈ √
rescaled sample size
by Theorem 1  the curves align for different values of p in the rescaled plot.

p. Plot (a) is an error plot for i.i.d. data with additive noise  and plot (b) shows ‘2-error versus the
k log p . Plot (c) depicts a similar (rescaled) plot for VAR data with additive noise. As predicted

n

(b)

(c)

We also veriﬁed the results of Theorem 2 empirically. Figure 2 shows the results of applying projected gradient
descent to solve the optimization problem (4) in the cases of additive noise and missing data. We ﬁrst applied

projected gradient to obtain an initial estimate bβ  then reapplied projected gradient descent 10 times  tracking
the optimization error kβt −bβk2 (in blue) and statistical error kβt − β∗k2 (in red). As predicted by Theorem 2 

the iterates exhibit geometric convergence to roughly the same ﬁxed point  regardless of starting point.
Finally  we simulated the inverse covariance matrix estimation algorithm on three types of graphical models:

(a) Chain-structured graphs. In this case  all nodes of are arranged in a line. The diagonal entries of Θ

equal 1  and entries corresponding to links in the chain equal 0.1. Then Θ is rescaled so |||Θ|||op = 1.
In this case  all nodes are connected to a central node  which has degree
k ≈ 0.1p. All other nodes have degree 1. The diagonal entries of Θ are set equal to 1  and all entries
corresponding to edges in the graph are set equal to 0.1. Then Θ is rescaled so |||Θ|||op = 1.

(b) Star-structured graphs.

(c) Erd¨os-Renyi graphs. As in Rothman et al. [22]  we ﬁrst generate a matrix B with diagonal entries 0 
and all other entries independently equal to 0.5 with probability k/p  and 0 otherwise. Then δ is chosen
so Θ = B + δI has condition number p  and Θ is rescaled so |||Θ|||op = 1.

7

0500100015002000250030000.080.10.120.140.160.180.20.220.240.260.28nl2 norm errorAdditive noise p=128p=256p=51224681012141618200.080.10.120.140.160.180.20.220.240.260.28n/(k log p)l2 norm errorAdditive noise p=128p=256p=512024681012141618200.050.10.150.20.250.30.350.40.450.50.55n/(k log p)l2 norm errorAdditive noise with autoregressive data p=128p=256p=512(a)

Figure 2. Plots of the optimization error log(kβt − bβk2) and statistical error log(kβt − β∗k2) versus iteration

number t  generated by running projected gradient descent on the non-convex objective. As predicted by Theorem 2 
the optimization error decreases geometrically.

(b)

|||bΘ−Θ|||op plotted against the sample size n for a chain-structured graph  with panel (a) showing the

After generating the matrix X of n i.i.d. samples from the appropriate graphical model  with covariance matrix
Σx = Θ−1  we generated the corrupted matrix Z = X + W with Σw = (0.2)2I. Figure 3 shows the rescaled
‘2-error 1√
original plot and panel (b) plotting against the rescaled sample size. We obtained qualitatively similar results
for the star and Erd¨os-Renyi graphs  in the presence of missing and/or dependent data.

k

(a) ‘2 error plot for chain graph  additive noise

k

Figure 3. (a) Plots of the rescaled error 1√
Gaussian graphical model with additive noise. As predicted by Theorems 1 and 2  all curves align when the rescaled
error is plotted against the ratio

k log p   as shown in (b). Each point represents the average over 50 trials.

n

|||bΘ−Θ|||op after running projected gradient descent for a chain-structured

(b) rescaled plot

5 Discussion

In this paper  we formulated an ‘1-constrained minimization problem for sparse linear regression on corrupted
data. The source of corruption may be additive noise or missing data  and although the resulting objective is
not generally convex  we showed that projected gradient descent is guaranteed to converge to a point within
statistical precision of the optimum. In addition  we established ‘1- and ‘2-error bounds that hold with high
probability when the data are drawn i.i.d. from a sub-Gaussian distribution  or drawn from a Gaussian VAR
process. Finally  we used our results on linear regression to perform sparse inverse covariance estimation for a
Gaussian graphical model  based on corrupted data. The bounds we obtain for the spectral norm of the error are
of the same order as existing bounds for inverse covariance matrix estimation with uncorrupted  i.i.d. data.

Acknowledgments

PL acknowledges support from a Hertz Foundation Fellowship and an NDSEG Fellowship; MJW and PL were
also partially supported by grants NSF-DMS-0907632 and AFOSR-09NL184. The authors thank Alekh Agar-
wal  Sahand Negahban  and John Duchi for discussions and guidance.

8

020406080100(cid:239)3.5(cid:239)3(cid:239)2.5(cid:239)2(cid:239)1.5(cid:239)1(cid:239)0.500.5Iteration countlog(||(cid:96)t (cid:239) (cid:96)||2)Log error plot: additive noise case Stat errorOpt error020406080100(cid:239)3.5(cid:239)3(cid:239)2.5(cid:239)2(cid:239)1.5(cid:239)1(cid:239)0.500.5Iteration countlog(||(cid:96)t (cid:239) (cid:96)||2)Log error plot: missing data case Stat errorOpt error010020030040050060070000.10.20.30.40.50.60.7n1/sqrt(k) * l2 operator norm errorChain graph p=64p=128p=25610203040506000.10.20.30.40.50.60.7n/(k log p)1/sqrt(k) * l2 operator norm errorChain graph p=64p=128p=256References
[1] R. Little and D. B. Rubin. Statistical analysis with missing data. Wiley  New York  1987.
[2] J. T. Hwang. Multiplicative errors-in-variables models with applications to recent data released by the
U.S. Department of Energy. Journal of the American Statistical Association  81(395):pp. 680–688  1986.
[3] R. J. Carroll  D. Ruppert  and L. A. Stefanski. Measurement Error in Nonlinear Models. Chapman and

Hall  1995.

[4] S. J. Iturria  R. J. Carroll  and D. Firth. Polynomial regression and estimating functions in the presence of
multiplicative measurement error. Journal of the Royal Statistical Society Series B - Statistical Methodol-
ogy  61:547–561  1999.

[5] Q. Xu and J. You. Covariate selection for linear errors-in-variables regression models. Communications

in Statistics - Theory and Methods  36(2):375–386  2007.

[6] N. St¨adler and P. B¨uhlmann. Missing values: Sparse inverse covariance estimation and an extension to

sparse regression. Statistics and Computing  pages 1–17  2010.

[7] M. Rosenbaum and A. B. Tsybakov. Sparse recovery under matrix uncertainty. Annals of Statistics 

38:2620–2651  2010.

[8] P. Loh and M.J. Wainwright. High-dimensional regression with noisy and missing data: Provable
guarantees with non-convexity. Technical report  UC Berkeley  September 2011. Available at http:
//arxiv.org/abs/1109.3714.

[9] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[10] S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on

Scientiﬁc Computing  20(1):33–61  1998.

[11] S. van de Geer. The deterministic Lasso. In Proceedings of Joint Statistical Meeting  2007.
[12] P. J. Bickel  Y. Ritov  and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of

Statistics  37(4):1705–1732  2009.

[13] S. van de Geer and P. Buhlmann. On the conditions used to prove oracle results for the Lasso. Electronic

Journal of Statistics  3:1360–1392  2009.

[14] G. Raskutti  M. J. Wainwright  and B. Yu. Restricted eigenvalue properties for correlated Gaussian designs.

Journal of Machine Learning Research  11:2241–2259  2010.

[15] A. Agarwal  S. Negahban  and M.J. Wainwright. Fast global convergence of gradient methods for high-
dimensional statistical recovery. Technical report  UC Berkeley  April 2011. Available at http://
arxiv.org/abs/1104.4824.

[16] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the ‘1-ball for learning

in high dimensions. In International Conference on Machine Learning  pages 272–279  2008.

[17] C. H. Zhang and J. Huang. The sparsity and bias of the Lasso selection in high-dimensional linear regres-

sion. Annals of Statistics  36(4):1567–1594  2008.

[18] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Annals of Statistics  37(1):246–270  2009.

[19] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis of regu-

larized M-estimators. In Advances in Neural Information Processing Systems  2009.

[20] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selection with the Lasso. Annals

of Statistics  34:1436–1462  2006.

[21] M. Yuan. High-dimensional inverse covariance matrix estimation via linear programming. Journal of

Machine Learning Research  99:2261–2286  August 2010.

[22] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation.

Electronic Journal of Statistics  2:494–515  2008.

9

,Chengxu Zhuang
Jonas Kubilius
Mitra Hartmann
Daniel Yamins