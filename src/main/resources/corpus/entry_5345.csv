2019,Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections,Computing optimal transport (OT) between measures in high dimensions is doomed by the curse of dimensionality. A popular approach to avoid this curse is to project input measures on lower-dimensional subspaces (1D lines in the case of sliced Wasserstein distances)  solve the OT problem between these reduced measures  and settle for the Wasserstein distance between these reductions  rather than that between the original measures. This approach is however difficult to extend to the case in which one wants to compute an OT map (a Monge map) between the original measures. Since computations are carried out on lower-dimensional projections  classical map estimation techniques can only produce maps operating in these reduced dimensions. We propose in this work two methods to extrapolate  from an transport map that is optimal on a subspace  one that is nearly optimal in the entire space. We prove that the best optimal transport plan that takes such "subspace detours" is a generalization of the Knothe-Rosenblatt transport. We show that these plans can be explicitly formulated when comparing Gaussian measures (between which the Wasserstein distance is commonly referred to as the Bures or Fréchet distance). We provide an algorithm to select optimal subspaces given pairs of Gaussian measures  and study scenarios in which that mediating subspace can be selected using prior information. We consider applications to semantic mediation between elliptic word embeddings and domain adaptation with Gaussian mixture models.,Subspace Detours: Building Transport Plans that are

Optimal on Subspace Projections

Boris Muzellec
CREST  ENSAE

boris.muzellec@ensae.fr

Marco Cuturi

Google Brain and CREST  ENSAE

cuturi@google.com

Abstract

Computing optimal transport (OT) between measures in high dimensions is doomed
by the curse of dimensionality. A popular approach to avoid this curse is to project
input measures on lower-dimensional subspaces (1D lines in the case of sliced
Wasserstein distances)  solve the OT problem between these reduced measures 
and settle for the Wasserstein distance between these reductions  rather than that
between the original measures. This approach is however difﬁcult to extend to
the case in which one wants to compute an OT map (a Monge map) between
the original measures. Since computations are carried out on lower-dimensional
projections  classical map estimation techniques can only produce maps operating
in these reduced dimensions. We propose in this work two methods to extrapolate 
from an transport map that is optimal on a subspace  one that is nearly optimal
in the entire space. We prove that the best optimal transport plan that takes such
“subspace detours” is a generalization of the Knothe-Rosenblatt transport. We show
that these plans can be explicitly formulated when comparing Gaussian measures
(between which the Wasserstein distance is commonly referred to as the Bures or
Fréchet distance). We provide an algorithm to select optimal subspaces given pairs
of Gaussian measures  and study scenarios in which that mediating subspace can be
selected using prior information. We consider applications to semantic mediation
between elliptic word embeddings and domain adaptation with Gaussian mixture
models.

1

Introduction

Minimizing the transport cost between two probability distributions [32] results in two useful
quantities: the minimum cost itself  often cast as a loss or a metric (the Wasserstein distance)  and
the minimizing solution  a function known as the Monge [20] map that pushes forward the ﬁrst
measure onto the second with least expected cost. While the former has long attracted the attention
of the machine learning community  the latter is playing an increasingly important role in data
sciences. Indeed  important problems such as domain adaptation [8]  generative modelling [17  2  16] 
reconstruction of cell trajectories in biology [28] and auto-encoders [19  30] among others can be
recast as the problem of ﬁnding a map  preferably optimal  which transforms a reference distribution
into another. However  accurately estimating an OT map from data samples is a difﬁcult problem 
plagued by the well documented instability of OT in high-dimensional spaces [11  13] and its high
computational cost.
Optimal Transport on Subspaces. Several approaches  both in theory and in practice  aim at
bridging this gap. Theory [33] supports the idea that sample complexity can be improved when
the measures are supported on lower-dimensional manifolds of high-dimensional spaces. Practical
insights [9] supported by theory [15] advocate using regularizations to improve both computational
and sample complexity. Some regularity in OT maps can also be encoded by looking at speciﬁc

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

families of maps [29  23]. Another trend relies on lower-dimensional projections of measures before
computing OT. In particular  sliced Wasserstein (SW) distances [4] leverage the simplicity of OT
between 1D measures to deﬁne distances and barycentres  by averaging the optimal transport between
projections onto several random directions. This approach has been applied to alleviate training
complexity in the GAN/VAE literature [10  34] and was generalized very recently in [22] who
considered projections on k-dimensional subspaces that are adversarially selected. However  these
subspace approaches only carry out half of the goal of OT: by design  they do result in more robust
measures of OT costs  but they can only provide maps in subspaces that are optimal (or nearly so)
between the projected measures  not transportation maps in the original  high-dimensional space in
which the original measures live. For instance  the closest thing to a map one can obtain from using
several SW univariate projections is an average of several permutations  which is not a map but a
transport plan or coupling [26][25  p.6].
Our approach. Whereas the approaches cited above focus on OT maps and plans in projection
subspaces only  we consider here plans and maps on the original space that are constrained to be
optimal when projected on a given subspace E. This results in the deﬁnition of a class of transportation
plans that ﬁguratively need to make an optimal “detour” in E. We propose two constructions to
recover such maps corresponding respectively (i) to the independent product between conditioned
measures  and (ii) to the optimal conditioned map.
Paper Structure. After recalling background material on OT in section 2  we introduce in section 3
the class of subspace-optimal plans that satisfy projection constraints on a given subspace E. We
characterize the degrees of freedom of E-optimal plans using their disintegrations on E and introduce
two extremal instances: Monge-Independent plans  which assume independence of the conditionals 
and Monge-Knothe maps  in which the conditionals are optimally coupled. We give closed forms
for the transport between Gaussian distributions in section 4  respectively as a degenerate Gaussian
distribution  and a linear map with block-triangular matrix representation. We provide guidelines
and a minimizing algorithm for selecting a subspace E when it is not prescribed a priori in section 5.
Finally  in section 6 we showcase the behavior of MK and MI transports on (noisy) synthetic data 
show how using a mediating subspace can be applied to selecting meanings for polysemous elliptical
word embeddings  and experiment using MK maps with the minimizing algorithm on a domain
adaptation task with Gaussian mixture models.
Notations. For E a linear subspace of Rd  E? is its orthogonal complement  VE 2 Rd⇥k (resp.
VE? 2 Rd⇥dk) the matrix of orthonormal basis vectors of E (resp E?). pE : x ! V>Ex is the
orthogonal projection operator onto E. P2(Rd) is the space of probability distributions over Rd with
ﬁnite second moments. B(Rd) is the Borel algebra over Rd. * denotes the weak convergence of
measures. ⌦ is the product of measures  and is used in measure disintegration by abuse of notation.
2 Optimal Transport: Plans  Maps and Disintegration of Measure
Kantorovitch Plans. For two probability measures µ  ⌫ 2P 2(Rd)  we refer to the set of couplings

⇧(µ  ⌫) def={ 2P (Rd ⇥ Rd) : 8A  B 2B (Rd)  (A ⇥ Rd) = µ(A)  (Rd ⇥ B) = ⌫(B)}

as the set of transportation plans between µ  ⌫. The 2-Wasserstein distance between µ and ⌫ is
deﬁned as

W 2

2 (µ  ⌫) def= min

2⇧(µ ⌫)

E(X Y )⇠⇥kX  Y k2⇤ .

Conveniently  transportation problems with quadratic cost can be reduced to transportation between
centered measures. Indeed  let mµ (resp. m⌫) denote ﬁrst moment of µ (resp. ⌫). Then  8 2
⇧(µ  ⌫)  E(X Y )⇠[kX  Y k2] = kmµ m⌫k2 +E(X Y )⇠[k(X  mµ) (Y  m⌫)k2]. Therefore 
in the following all probability measures are assumed to be centered  unless stated otherwise.
Monge Maps. For a Borel-measurable map T   the push-forward of µ by T is deﬁned as the measure
T]µ satisfying for all A 2B (Rd)  T]µ(A) = µ(T 1(A)). A map such that T]µ = ⌫ is called a
transportation map from µ to ⌫. When a transportation map exists  the Wasserstein distance can be
written in the form of the Monge problem

W 2

2 (µ  ⌫) = min

T :T]µ=⌫

EX⇠µ[kX  T (X)k2].

2

When it exists  the optimal transportation map T ? in the Monge problem is called the Monge map
from µ to ⌫. It is then related to the optimal transportation plan ? by the relation ? = (Id  T ?)]µ.
When µ and ⌫ are absolutely continuous (a.c.)  a Monge map always exists ([27]  Theorem 1.22).
Global Maps or Plans that are Locally Optimal. Considering the projection operator on E  pE 
we write µE = (pE)]µ for the marginal distribution of µ on E. Suppose that we are given a Monge
map S between the two projected measures µE and ⌫E. One of the contributions of this paper is to
propose extensions of this map S as a transportation plan  (resp. a new map T ) whose projection
E = (pE  pE)] on that subspace E coincides with the optimal transportation plan (IdE  S)]µE
(resp. pE  T = S  pE). Formally  the transports introduced in section 3 only require that S be a
transport map from µE to ⌫E  but optimality is required in the closed forms given in section 4 for
Gaussian distributions. In either case  this constraint implies that  is built “assuming that” it is equal
to (IdE  S)]µE on E. This is rigorously deﬁned using the notion of measure disintegration.
Disintegration of Measures. The disintegration of µ on a subspace E is the collection of measures
(µxE )xE2E supported on the ﬁbers {xE}⇥ E? such that any test function  can be integrated
against µ asRRd dµ =RERE? (y)dµxE (y) dµE(xE). In particular  if X ⇠ µ  then the law of
X given xE is µxE. By abuse of the measure product notation ⌦  measure disintegration is denoted
as µ = µxE ⌦ µE. A more general description of disintegration can be found in [1]  Ch. 5.5.
3 Lifting Transport from Subspace to Full Space

Given two distributions µ  ⌫ 2P 2(Rd)  it is often easier to compute a Monge map S between their
marginals µE ⌫ E on a k-dimensional subspace E rather than in the whole space Rd. When k = 1 
this fact is at the heart of sliced wasserstein approaches [4]  which have recently sparked interest
in the GAN/VAE literature [10  34]. However  when k < d  there is in general no straightforward
way of extending S to a transportation map or plan between µ and ⌫. In this section  we prove the
existence of such extensions and characterize them.
Subspace-Optimal Plans. A transportation plan between µE and ⌫E is a coupling living in P(E⇥E).
In general  it cannot be cast directly as a transportation plan between µ and ⌫ taking values in
P(Rd ⇥ Rd). However  the existence of such a “lifted” plan is given by the following result  which is
used in OT theory to prove that Wp is a metric:
Lemma 1 (The Gluing Lemma  [32]). Let µ1  µ2  µ3 2P (Rd). If 12 is a coupling of (µ1  µ2) and
23 is a coupling of (µ2  µ3)  then one can construct a triple of random variables (Z1  Z2  Z3) such
that (Z1  Z2) ⇠ 12 and (Z2  Z3) ⇠ 23.
By extension of the lemma  if we deﬁne (i) a coupling between µ and µE  (ii) a coupling between
⌫ and ⌫E  and (iii) the optimal coupling between µE and ⌫E  (Id  S)]µE (where S stands for the
Monge map from µE to ⌫E)  we get the existence of four random variables (with laws µ  µE ⌫ and
⌫E) which follow the desired joint laws. However  the lemma does not imply the uniqueness of those
random variables  nor does it give a closed form for the corresponding coupling between µ and ⌫.
Deﬁnition 1 (Subspace-Optimal Plans). Let µ  ⌫ 2P 2(Rd) and E be a k-dimensional subspace of
Rd. Let S be a Monge map from µE to ⌫E. We deﬁne the set of E-optimal plans between µ and ⌫ as
⇧E(µ  ⌫) def={ 2 ⇧(µ  ⌫) : E = (IdE  S)]µE}.
Degrees of freedom in ⇧E(µ  ⌫). When k < d  there can be inﬁnitely many E-optimal plans.
However  we can further characterize the degrees of freedom available to deﬁne plans in ⇧E(µ  ⌫).
Indeed  let  2 ⇧E(µ  ⌫). Then  disintegrating  on E ⇥ E  we get  = (xE  yE ) ⌦ E  i.e. plans
in ⇧E(µ  ⌫) only differ on their disintegrations on E ⇥ E. Further  since E stems from a transport
(Monge) map S  it is supported on the graph of S on E  G(S) = {(xE  S(xE)) : xE 2 E}⇢ E ⇥ E.
This implies that  puts zero mass when yE 6= S(xE) and thus that  is fully characterized by
(xE  S(xE ))  xE 2 E  i.e. by the couplings between µxE and ⌫S(xE ) for xE 2 E. This is illustrated
in Figure 1. Two such couplings are presented: the ﬁrst  MI (Deﬁnition 2) corresponds to independent
couplings between the conditionals  while the second (MK  Deﬁnition 3) corresponds to optimal
couplings between the conditionals.
Deﬁnition 2 (Monge-Independent Plans). ⇡MI

def= (µxE ⌦ ⌫S(xE )) ⌦ (IdE  S)]µE.

3

Monge-Independent transport only re-
quires that there exists a Monge map
S between µE and ⌫E (and not on the
whole space)  but extends S as a trans-
portation plan and not a map. Since
it couples disintegrations with the in-
dependent law  it is particularly suited
to settings where all the information
is contained in E  as shown in section
6.
When there exists a Monge map be-
tween disintegrations µxE to ⌫S(xE )
for all xE 2 E (e.g. when µ and ⌫
are a.c.)  it is possible to extend S as
a transportation map between µ and
⌫ using those maps. Indeed  for all
xE 2 E  let ˆT (xE;·) : E? ! E?
denote the Monge map from µxE to ⌫S(xE ). The Monge-Knothe transport corresponds to the E-
optimal plan with optimal couplings between the disintegrations:
Deﬁnition 3 (Monge-Knothe Transport). TMK(xE  xE?) def= (S(xE)  ˆT (xE; xE?)) 2 E  E?.
The proof that TMK deﬁnes a transport map from µ to ⌫ is a direct adaptation of the proof for the
Knothe-Rosenblatt transport ([27]  Section 2.3). When it is not possible to deﬁne a Monge map
between the disintegrations  one can still consider the optimal couplings ⇡OT(µxE  ⌫ S(xE )) and deﬁne
⇡MK = ⇡OT(µxE  ⌫ S(xE ))⌦ (IdE  S)]µE  which we still call Monge-Knothe plan by abuse. In either
case  ⇡MK is the E-optimal plan with lowest global cost:
Proposition 1. The Monge-Knothe plan is optimal in ⇧E(µ  ⌫)  namely

Figure 1: A d = 2  k = 1 illustration. Any  2 ⇧E(µ  ⌫)
being supported on G(S) ⇥ (E?)2  all the mass from x
is transported on the ﬁber {S(xE)}⇥ E?. Different ’s
in ⇧E(µ  ⌫) correspond to different couplings between the
ﬁbers {xE}⇥ E? and {S(xE)}⇥ E?.

⇡MK 2 arg min
2⇧E (µ ⌫)

E(X Y )⇠[kX  Y k2].

Proof. E-optimal plans only differ in the couplings they induce between µxE and ⌫S(xE ) for xE 2 E.
Since ⇡MK corresponds to the case when these couplings are optimal  disintegrating  over E ⇥ E in
RRd⇥Rd kx  yk2d(x  y) shows that  = ⇡MK has the lowest cost. ⌅
Relation with the Knothe-Rosenblatt (KR) transport. These deﬁnitions are related to the KR
transport ([27]  section 2.3)  which consists in deﬁning a transport map between two a.c. measures
by recursively (i) computing the Monge map T1 between the ﬁrst two one-dimensional marginals of
µ and ⌫ and (ii) repeating the process between the disintegrated measures µx1 and ⌫T1(x1). MI and
MK marginalize on the k  1 dimensional subspace E  and respectively deﬁne the transport between
disintegrations µxE and ⌫S(xE ) as the product measure and the optimal transport instead of recursing.
MK as a limit of optimal transport with re-weighted quadratic costs. Similarly to KR [5]  MK
transport maps can intuitively be obtained as the limit of optimal transport maps  when the costs on
E? become negligible compared to the costs on E.
Proposition 2. Let Rd = EE?  (VE VE?) an orthonormal basis of EE? and µ  ⌫ 2P 2(Rd)
be two a.c. probability measures. Deﬁne

def= VEV>E + "VE?V>E? 
Let T" be the optimal transport map for the cost d2

8"> 0  P"

P"(x  y) def= (x  y)>P"(x  y).
d2

P". Then T" ! TMK in L2(µ).

Proof in the supplementary material.
MI as a limit of the discrete case. When µ and ⌫ are a.c.  for n 2 N let µn ⌫ n denote the uniform
distribution over n i.i.d. samples from µ and ⌫ respectively  and let ⇡n be an optimal transportation
plan between (pE)]µn and (pE)]⌫n given by a Monge map (which is possible assuming uniform
weights and non-overlapping projections). We have that µn *µ and ⌫n *µ . From [27]  Th
1.50  1.51  we have that ⇡n 2P 2(E ⇥ E) converges weakly  up to subsequences  to a coupling
⇡ 2P 2(E ⇥ E) that is optimal for µE and ⌫E. On the other hand  up to points having the same

4

projections  the discrete plans ⇡n can also be seen as plans in P(Rd ⇥ Rd). A natural question is
then whether the sequence ⇡n 2P (Rd ⇥ Rd) has a limit in P(Rd ⇥ Rd).
Proposition 3. Let µ  ⌫ 2P 2(Rd) be a.c. and compactly supported  µn ⌫ n  n  0 be uniform
distributions over n i.i.d. samples  and ⇡n 2 ⇧E(µn ⌫ n)  n  0. Then ⇡n * ⇡MI(µ  ⌫).
Proof in the supplementary material. We conjecture that under additional assumptions  the compact-
ness hypothesis can be relaxed. In particular  we empirically observe convergence for Gaussians.

4 Explicit Formulas for Subspace Detours in the Bures Metric

Multivariate Gaussian measures are a speciﬁc case of continuous distributions for which Wasserstein
distances and Monge maps are available in closed form. We ﬁrst recall basic facts about optimal
transport between Gaussian measures  and then show that the E-optimal transports MI and MK
introduced in section 3 are also in closed form. For two Gaussians µ  ⌫  one has W 2
2 (µ  ⌫) =
kmµ  m⌫k2 + B2(var µ  var ⌫) where B is the Bures metric [3] between PSD matrices [14]:
B2(A  B) def= TrA + TrB  2Tr(A1/2BA1/2)1/2. The Monge map from a centered Gaussian
distribution µ with covariance matrix A to one ⌫ with covariance matrix B is linear and is represented
by the matrix TAB def= A1/2(A1/2BA1/2)1/2A1/2. For any linear transport map  T]µ has
covariance TAT>  and the transportation cost from µ to ⌫ is EX⇠µ[kX  TXk2] = TrA +
TrB  Tr(TA + AT>). In the following  µ (resp. ⌫) will denote the centered Gaussian distribution
EE? AE? ⌘ when A is represented in an
with covariance matrix A (resp. B). We write A =⇣ AE AEE?
orthonormal basis (VE VE?) of E  E?.
Monge-Independent Transport for Gaussians. The MI transport between Gaussian measures is
given by a degenerate Gaussian  i.e. a measure with Gaussian density over the image of its covariance
matrix ⌃ (we refer to the supplementary material for the proof).
Proposition 4 (Monge-Independent (MI) Transport for Gaussians). Let

A>

C def=VEAE + VE?A>EE? TAE BEVE> + (BE)1BEE?V>E? and ⌃ def= A C
C> B.

Then ⇡MI(µ  ⌫) = N (02d  ⌃) 2P (Rd ⇥ Rd).
Knothe-Rosenblatt and Monge-Knothe for Gaus-
sians. Before giving the closed-form MK map for
Gaussian measures  we derive the KR map ([27] 
section 2.3) with successive marginalization1 on
x1  x2  ...  xd. When d = 2 and the basis is orthonor-
mal for E  E?  those two notions coincide.
Proposition 5 (Knothe-Rosenblatt (KR) Transport
between Gaussians). Let LA (resp. LB) be the
Cholesky factor of A (resp. B). The KR transport
from µ to ⌫ is a linear map whose matrix is given by
KR = LB(LA)1. Its cost is the squared Frobe-
TAB
nius distance between the Cholesky factors LA and
LB:

EX⇠µ[kX  T AB

KR Xk2] = kLA  LBk2.

Proof. The KR transport with successive marginaliza-
tion on x1  x2  ...  xd between two a.c. distributions
has a lower triangular Jacobian with positive entries
on the diagonal. Further  since the one-dimensional
disintegrations of Gaussians are Gaussians them-
selves  and since Monge maps between Gaussians

Figure 2: MI transport from a 2D Gaussian
(red) to a 1D Gaussian (blue)  projected on
the x-axis. The two 1D distributions repre-
sent the projections of both Gaussians on the
x-axis  the blue one being already originally
supported on the x-axis. The oblique hyper-
plane is the support of ⇡MI  onto which its
density is represented.

1Note that compared to [27]  this is the reversed marginalization order  which is why the KR map here has

lower triangular Jacobian.

5

are linear  the KR transport between two centered Gaussians is a linear map  hence its matrix
representation equals its Jacobian and is lower triangular.
Let T = LB(LA)1. We have TAT> = LBL1
A LAL>AL>A L>B = LBL>B = B  i.e. T]µ = ⌫.
Further  since TLA is the Cholesky factor for B  and since A is supposed non-singular  by unicity of
the Cholesky decomposition T is the only lower triangular matrix satisfying T]µ = ⌫. Hence  it is
the KR transport map from µ to ⌫.
Finally  we have that EX⇠µ[kX  TKRXk2] = Tr(A + B  (A(TKR)> + TKRA)) = Tr(LAL>A +
LBL>B  (LAL>B + LBL>A)) = kLA  LBk2 ⌅
Corollary 1. The (square root) cost of the Knothe-Rosenblatt transport (EX⇠µ[kX  TKRXk2])1/2
between centered gaussians deﬁnes a distance (i.e. it satisﬁes all three metric axioms).
Proof. This comes from the fact thatEX⇠µ[kX  TKRXk2]1/2 = kLA  LBk. ⌅

As can be expected from the fact that MK can be seen as a generalization of KR  the MK transportation
map is linear and has a block-triangular structure. The next proposition shows that the MK transport
map can be expressed as a function of the Schur complements A/AE
E AEE?
of A w.r.t. AE  and B w.r.t. BE  which are the covariance matrices of µ (resp. ⌫) conditioned on E.

def= AE?  A>EE?A1

(a) Usual Monge Interpolation of Gaussians

(b) Monge-Knothe Interpolation through E

TAE BE

⇥B>EE?(TAE BE )1  T(A/AE )(B/BE )A>EE?⇤ (AE)1 T(A/AE )(B/BE )◆ .

Figure 3: (a) Wasserstein-Bures geodesic and (b) Monge-Knothe interpolation through E = {(x  y) :
x = y} from µ0 to µ1  at times t = 0  0.25  0.5  0.75  1.
Proposition 6 (Monge-Knothe (MK) Transport for Gaussians). Let A  B be represented in an
orthonormal basis for E  E?. The MK transport map on E between µ and ⌫ is given by
TMK =✓
0k⇥(dk)
Proof. As can be seen from the structure of the MK transport map in Deﬁnition 3  TMK has a lower
block-triangular Jacobian (with block sizes k and d  k)  with PSD matrices on the diagonal (corre-
sponding to the Jacobians of the Monge maps (i) between marginals and (ii) between conditionals).
Further  since µ and ⌫ are Gaussian  their disintegrations are Gaussian as well. Hence  all Monge
maps from the disintegrations of µ to that of ⌫ are linear  and therefore the matrix representing T is
equal to its Jacobian. One can check that the map T in the proposition veriﬁes TAT> = B and is of
the right form. One can also check that it is the unique such matrix  hence it is the MK transport map.
⌅

5 Selecting the Supporting Subspace

Algorithm 1 MK Subspace Selection
Input: A  B 2 PSD  k 2 [[1  d]] ⌘

Both MI and MK transports are highly dependent on the
chosen subspace E. Depending on applications  E can
either be prescribed (e.g. if one has access to a transport
map between the marginals in a given subspace) or has to
be selected. In the latter case  we give guidelines on how
prior knowledge can be used  and alternatively propose an
algorithm for minimizing the MK distance.
Subspace Selection Using Prior Knowledge. When
prior knowledge is available  one can choose a mediating
subspace E to enforce speciﬁc criteria when comparing
two distributions. Indeed  if the directions in E are known to correspond to given properties of the
data  then MK or MI transport privileges those properties when matching distributions over those not
encoded by E. In particular  if one has access to features X from a reference dataset  one can use

Output: E = Span{v1  ..  vk}

V Polar(AB)
while not converged do

L MK(V>AV  V>BV; k)
V V  ⌘rVL
V Polar(V)

end while

6

principal component analysis (PCA) and select the ﬁrst k principal directions to compare datasets X1
and X2. MK and MI then allow comparing X1 and X2 using the most signiﬁcant features from the
reference X with higher priority. In section 6  we experiment this method on word embeddings.
Minimal Monge-Knothe Subspace. Alternatively  in the absence of prior knowledge  it is natural
to aim at ﬁnding the subspace which minimizes MK. Unfortunately  optimization on the Grassmann
manifold is quite hard in general  which makes direct optimization of MK w.r.t. E impractical.
Optimizing with respect to an orthonormal matrix V of basis vectors of Rd is a more practical
parameterization  which allows to perform projected gradient descent (Algorithm 1). The projection
step consists in computing a polar decomposition  as the projection of a matrix V onto the set of
unitary matrices is the unitary matrix in the polar decomposition of V. The proposed initialization is
V = Polar(AB)  as this is the optimal solution when A  B are co-diagonalizable. Note that since
the function being minimized is non-convex  Algorithm 1 is only guaranteed to converge to a local
minimum. In section 6  experimental evaluation of Algorithm 1 is carried out on noise-contaminated
synthetic data (Figure 6) and on a domain adaptation task with Gaussian mixture models on the Ofﬁce
Home dataset [31] with inception features (Figure 7).

6 Experiments

Color Transfer. Given a source and a target
image  the goal of color transfer is to map the
color palette of the source image (represented
by its RGB histogram) into that of the target
image. A natural toolbox for such a task is
Figure 4: OT color transfer between gray projections.
optimal transport  see e.g. [4  12  24]. First  a
k-means quantization of both images is computed. Then  the colors of the pixels within each source
cluster are modiﬁed according to the optimal transport map between both color distributions. In
Figure 5  we illustrate discrete MK transport maps for color transfer. In this setting  we project
images on the 1D space of grayscale images  relying on the 1D OT sorting-based algorithm (Figure
4). Then  we solve small 2D OT problems on the corresponding disintegrations. We compare this
approach with classic full OT maps and a sliced OT approach (with 100 random projections). As
can be seen in Figure 5  MK results are visually very similar to that of full OT  with a x50 speedup
allowed by the fast 1D OT sorting-based algorithm that is comparable to sliced OT.

(a) Gray Source

(b) Gray OT

(c) Gray Target

(a) Source

(b) Full OT (2.67s) (c) Gray MK (0.052s)

(d) Sliced (0.057s)

(e) Target

Figure 5: Color transfer  after quantization using 3000 k-means clusters  with corresponding runtimes.

Synthetic Data. We test the behavior of MK and MI in a noisy environment  where the signal is
supported in a subspace of small dimension. We represent the signal using two normalized PSD
matrices A  B 2 Rd1⇥d1 and sample noise ⌃1  ⌃2 2 Rd2⇥d2  d2  d1 from a Wishart distribution
with parameter I. We then build the noisy covariance A" = ( A 0
0 0 ) + "⌃1 2 Rd2⇥d2 (and likewise
B") for different noise levels " and compute MI and MK distances along the ﬁrst k directions 
k = 1  ...  d2. As can be seen in Figure 6  both MI and MK curves exhibit a local minimum or
an “elbow” when k = d1  i.e. when E corresponds to the subspace where the signal is located.
However  important differences in the behaviors of MI and MK can be noticed. Indeed  MI has a
steep decreasing curve from 1 to d1 and then a slower decreasing curve. This is explained by the fact
that MI transport computes the OT map along the k directions of E only  and treats the conditionals
as being independent. Therefore  if k  d1  all the signal has been ﬁtted and for increasing values of
k MI starts ﬁtting the noise as well. On the other hand  MK transport computes the optimal transport
on both E and the corresponding (d2  k)-dimensional conditionals. Therefore  if k 6= d1  either or
both maps ﬁt a mixture of signal and noise. Local maxima correspond to cases where the signal is the
most contaminated by noise  and minima k = d1  k = d2 to cases where either the marginals or the
conditionals are unaffected by noise. Using Algorithm 1 instead of the principle directions allows
to ﬁnd better subspaces than the ﬁrst k directions when k  d1  and then behaves similarly (up to
the gradient being stuck in local minima and thus being occasionally less competitive). Overall  the

7

differences in behavior of MI and MK show that MI is more adapted to noisy environments  and MK
to applications where all directions are meaningful  but where one wishes to prioritize ﬁtting on a
subset of those directions  as shown in the next experiment.

(a) Monge-Independent

(b) Monge-Knothe

(c) Bures

Figure 6: (a)-(b): Difference between (a) MI and Bures and (b) MK and Bures metrics for different
noise levels " and subspace dimensions k. (c): Corresponding Bures values. For each ✏  100 different
noise matrices are sampled. Points show mean values  and shaded areas the 25%-75% and 10%-90%
percentiles. Top row: d1 = 4  d2 = 8. Bottom row: d1 = 4  d2 = 16.
Semantic Mediation. We experiment using reference features for comparing distributions with
elliptical word embeddings [21]  which represent each word from a given corpus using a mean vector
and a covariance matrix. For a given embedding  we expect the principal directions of its covariance
matrix to be linked to its semantic content. Therefore  the comparison of two words w1  w2 based on
the principal eigenvectors of a context word c should be impacted by the semantic relations of w1
and w2 with respect to c  e.g. if w1 is polysemous and c is related to a speciﬁc meaning. To test this
intuition  we compute the nearest neighbors of a given word w according to the MK distance with E
taken as the subspace spanned by the principal directions of two different contexts c1 and c2. We
exclude means and compute MK based on covariances only  and look at the symmetric difference of
the returned sets of words (i.e. words in KNN(w|c1) but not in KNN(w|c2)  and inversely). Table 1
shows that speciﬁc contexts affect the nearest neighbors of ambiguous words.

Table 1: Symmetric differences of the 20-NN sets of w given c1 minus w given c2 using MK.
Embeddings are 12 ⇥ 12 pretrained normalized covariance matrices from [21]. E is spanned by the 4
principal directions of the contexts. Words are printed in increasing distance order.

Word

instrument

windows

fox

oboe
monitor

Context 1 Context 2
monitor
oboe
pc
door
media

door
pc

hedgehog

media

hedgehog

Difference

cathode  monitor  sampler  rca  watts  instrumentation  telescope  synthesizer  ambient
tuned  trombone  guitar  harmonic  octave  baritone  clarinet  saxophone  virtuoso
netscape  installer  doubleclick  burner  installs  adapter  router  cpus
screwed  recessed  rails  ceilings  tiling  upvc  proﬁled  roofs
Penny  quiz  Whitman  outraged  Tinker  ads  Keating  Palin  show
panther  reintroduced  kangaroo  Harriet  fair  hedgehog  bush  paw  bunny

MK Domain Adaptation with Gaussian Mixture Models. Given a source dataset of labeled data 
domain adaptation (DA) aims at ﬁnding labels for a target dataset by transfering knowledge from the
source. Such a problem has been successfully tackled using OT-based techniques [8]. We illustrate
using MK Gaussian maps on a domain adaptation task where both source and target distributions
are modeled by a Gaussian mixture model (GMM). We use the Ofﬁce Home dataset [31]  which
comprises 15000 images from 65 different classes across 4 domains: Art  Clipart  Product and
Real World. For each image  we consider 2048-dimensional features taken from the coding layer
of an inception model  as with Fréchet inception distances [18]. For each source/target pair  we
represent the source as a GMM by ﬁtting one Gaussian per source class and deﬁning mixture weights
proportional to class frequencies  and we ﬁt a GMM with the same number of components on the
target. Since label information is not available for the target dataset  data from different classes may
be assigned to the same component. We then compute pairwise MK distances between all source and
target components  and solve for the discrete OT plan P using those distances as costs and mixture
weights as marginals (as in [6] with Bures distances). Finally  we map the source distribution on
MK 

the target by computing the P -barycentric projection of the component-wise MK mapsPij PijT ij

8

and assign target labels using 1-NN prediction over the mapped source data. The same procedure
is applied using Bures distances between the projections on E. We use Algorithm 1 between the
empirical covariance matrices of the source and target datasets to select the supporting subspace E 
for different values of the supporting dimension k (Figure 7).

Figure 7: Domain Adaptation: 1-NN accuracy scores on the Ofﬁce Home dataset v.s. dimension k.
We compare the k-dimensional projected Bures maps with the E-MK maps and the 2048-D Bures
baseline. E is selected using Algorithm 1 between the source and target covariance matrices for
k = 32  64  128  256  512  1024. Rows: sources  Columns: targets.

Several facts can be observed from Figure 7. First  using the full 2048-dimensional Bures maps
is regularly sub-optimal compared to Bures (resp. MK) maps on a lower-dimensional subspace 
even though this is dependent on the source/target combination. This shows the interest of not
using all available features equally in transport problems. Secondly  when E is chosen using the
minimizing algorithm 1  in most cases MK maps yield equivalent or better classiﬁcation accuracy
that the corresponding Bures maps on the projections  even though they have the same projections on
E. However  as can be expected  this does not hold for an arbitrary choice of E (not shown in the
ﬁgure). Due to the relative simplicity of this DA method (which models the domains as GMMs)  we
do not aim at comparing with state-of-the-art OT DA methods [8  7] (which compute transportation
plans between the discrete distributions directly). The goal is rather to illustrate how MK maps can
be used to compute maps which put higher priority on the most meaningful feature dimensions. Note
also that the mapping between source and target distributions used here is piecewise linear  and is
therefore more regular.
Conclusion and Future Work. We have proposed in this paper a new class of transport plans and
maps that are built using optimality constraints on a subspace  but deﬁned over the whole space. We
have presented two particular instances  MI and MK  with different properties  and derived closed
formulations for Gaussian distributions. Future work includes exploring other applications of OT to
machine learning relying on low-dimensional projections  from which subspace-optimal transport
could be used to recover full-dimensional plans or maps.

9

References
[1] Luigi Ambrosio  Nicola Gigli  and Giuseppe Savare. Gradient Flows in Metric Spaces and in

the Space of Probability Measures. 01 2005.

[2] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial
networks. In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th International
Conference on Machine Learning  volume 70 of Proceedings of Machine Learning Research 
pages 214–223  International Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[3] Rajendra Bhatia  Tanvi Jain  and Yongdo Lim. On the Bures-Wasserstein distance between

positive deﬁnite matrices. Expositiones Mathematicae  2018.

[4] Nicolas Bonneel  Julien Rabin  Gabriel Peyré  and Hanspeter Pﬁster. Sliced and Radon Wasser-
stein Barycenters of Measures. Journal of Mathematical Imaging and Vision  1(51):22–45 
2015.

[5] Guillaume Carlier  Alfred Galichon  and Filippo Santambrogio. From knothe’s transport to

brenier’s map and a continuation method for optimal transport. SIAM J. Math. An.  2009.

[6] Y. Chen  T. T. Georgiou  and A. Tannenbaum. Optimal transport for gaussian mixture models.

IEEE Access  7:6269–6278  2019.

[7] Nicolas Courty  Rémi Flamary  Amaury Habrard  and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In I. Guyon  U. V. Luxburg  S. Bengio  H. Wallach 
R. Fergus  S. Vishwanathan  and R. Garnett  editors  Advances in Neural Information Processing
Systems 30  pages 3730–3739. Curran Associates  Inc.  2017.

[8] Nicolas Courty  Remi Flamary  Alain Rakotomamonjy  and Devis Tuia. Optimal transport for
domain adaptation. In NIPS  Workshop on Optimal Transport and Machine Learning  Montréal 
Canada  December 2014.

[9] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances

in Neural Information Processing Systems  pages 2292–2300  2013.

[10] Ishan Deshpande  Ziyu Zhang  and Alexander G. Schwing. Generative modeling using the sliced
wasserstein distance. In 2018 IEEE Conference on Computer Vision and Pattern Recognition 
CVPR 2018  Salt Lake City  UT  USA  June 18-22  2018  pages 3483–3491  2018.

[11] RM Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical

Statistics  40(1):40–50  1969.

[12] Sira Ferradans  Nicolas Papadakis  Gabriel Peyré  and Jean-François Aujol. Regularized discrete

optimal transport. SIAM Journal on Imaging Sciences  7(3):1853–1882  2014.

[13] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the

empirical measure. Probability Theory and Related Fields  162(3-4):707–738  2015.

[14] Matthias Gelbrich. On a formula for the l2 Wasserstein metric between measures on Euclidean

and Hilbert spaces. Mathematische Nachrichten  147(1):185–203  1990.

[15] Aude Genevay  Lénaïc Chizat  Francis Bach  Marco Cuturi  and Gabriel Peyré. Sample
complexity of sinkhorn divergences. In Kamalika Chaudhuri and Masashi Sugiyama  editors 
Proceedings of Machine Learning Research  volume 89 of Proceedings of Machine Learning
Research  pages 1574–1583. PMLR  16–18 Apr 2019.

[16] Aude Genevay  Gabriel Peyre  and Marco Cuturi. Learning generative models with sinkhorn
divergences. In Amos Storkey and Fernando Perez-Cruz  editors  Proceedings of the Twenty-
First International Conference on Artiﬁcial Intelligence and Statistics  volume 84 of Proceedings
of Machine Learning Research  pages 1608–1617  Playa Blanca  Lanzarote  Canary Islands 
09–11 Apr 2018. PMLR.

10

[17] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani 
M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors  Advances in Neural
Information Processing Systems 27  pages 2672–2680. Curran Associates  Inc.  2014.

[18] Martin Heusel  Hubert Ramsauer  Thomas Unterthiner  Bernhard Nessler  and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon 
U. V. Luxburg  S. Bengio  H. Wallach  R. Fergus  S. Vishwanathan  and R. Garnett  editors 
Advances in Neural Information Processing Systems 30  pages 6626–6637. Curran Associates 
Inc.  2017.

[19] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations  ICLR 2014  Banff  AB  Canada  April 14-16  2014 
Conference Track Proceedings  2014.

[20] Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Histoire de l’Académie

Royale des Sciences  pages 666–704  1781.

[21] Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the wasserstein space
of elliptical distributions. In Advances in Neural Information Processing Systems 31  pages
10237–10248. Curran Associates  Inc.  2018.

[22] François-Pierre Paty and Marco Cuturi. Subspace robust Wasserstein distances. In Kamalika
Chaudhuri and Ruslan Salakhutdinov  editors  Proceedings of the 36th International Conference
on Machine Learning  volume 97 of Proceedings of Machine Learning Research  pages 5072–
5081  Long Beach  California  USA  09–15 Jun 2019. PMLR.

[23] François-Pierre Paty  Alexandre d’Aspremont  and Marco Cuturi. Regularity as regulariza-
tion: Smooth and strongly convex brenier potentials in optimal transport. arXiv preprint
arXiv:1905.10812  2019.

[24] Julien Rabin  Sira Ferradans  and Nicolas Papadakis. Adaptive color transfer with relaxed
optimal transport. In 2014 IEEE International Conference on Image Processing (ICIP)  pages
4852–4856. IEEE  2014.

[25] Julien Rabin  Gabriel Peyré  Julie Delon  and Marc Bernot. Wasserstein barycenter and its
application to texture mixing. In International Conference on Scale Space and Variational
Methods in Computer Vision  pages 435–446. Springer  2011.

[26] Mark Rowland  Jiri Hron  Yunhao Tang  Krzysztof Choromanski  Tamas Sarlos  and Adrian
Weller. Orthogonal estimation of wasserstein distances. In Kamalika Chaudhuri and Masashi
Sugiyama  editors  Proceedings of Machine Learning Research  volume 89 of Proceedings of
Machine Learning Research  pages 186–195. PMLR  16–18 Apr 2019.

[27] Filippo Santambrogio. Optimal transport for applied mathematicians  2015.
[28] Geoffrey Schiebinger  Jian Shu  Marcin Tabaka  Brian Cleary  Vidya Subramanian  Aryeh
Solomon  Joshua Gould  Siyan Liu  Stacie Lin  Peter Berube  et al. Optimal-transport analysis
of single-cell gene expression identiﬁes developmental trajectories in reprogramming. Cell 
176(4):928–943  2019.

[29] Vivien Seguy  Bharath Bhushan Damodaran  Rémi Flamary  Nicolas Courty  Antoine Rolet  and
Mathieu Blondel. Large-scale optimal transport and mapping estimation. In Proceedings of the
International Conference in Learning Representations  2018.

[30] Ilya Tolstikhin  Olivier Bousquet  Sylvain Gelly  and Bernhard Scholkopf. Wasserstein auto-

encoders. In International Conference on Learning Representations  ICLR  2018.

[31] Hemanth Venkateswara  Jose Eusebio  Shayok Chakraborty  and Sethuraman Panchanathan.
Deep hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer
Vision and Pattern Recognition (CVPR)  2017.

[32] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business

Media  2008.

11

[33] Jonathan Weed and Francis Bach. Sharp asymptotic and ﬁnite-sample rates of convergence of

empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087  2017.

[34] Jiqing Wu  Zhiwu Huang  Dinesh Acharya  Wen Li  Janine Thoma  Danda Pani Paudel  and
Luc Van Gool. Sliced wasserstein generative models. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2019.

12

,Boris Muzellec
Marco Cuturi