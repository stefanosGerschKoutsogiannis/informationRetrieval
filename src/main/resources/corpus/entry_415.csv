2009,Adaptive Regularization of Weight Vectors,We present AROW  a new online learning algorithm that combines several properties of successful : large margin training  confidence weighting  and the capacity to handle non-separable data. AROW performs adaptive regularization of the prediction function upon seeing each new instance  allowing it to perform especially well in the presence of label noise.  We derive a mistake bound  similar in form to the second order perceptron bound  which does not assume separability. We also relate our algorithm to recent confidence-weighted online learning techniques and empirically show that AROW achieves state-of-the-art performance and notable robustness in the case of non-separable data.,Adaptive Regularization of Weight Vectors

Koby Crammer
Department of

Electrical Enginering

The Technion

Haifa  32000 Israel

koby@ee.technion.ac.il

Alex Kulesza

Department of Computer
and Information Science
University of Pennsylvania

Philadelphia  PA 19104
kulesza@cis.upenn.edu

Mark Dredze

Human Language Tech.

Center of Excellence

Johns Hopkins University

Baltimore  MD 21211
mdredze@cs.jhu.edu

Abstract

We present AROW  a new online learning algorithm that combines sev-
eral useful properties: large margin training  conﬁdence weighting  and the
capacity to handle non-separable data. AROW performs adaptive regular-
ization of the prediction function upon seeing each new instance  allowing
it to perform especially well in the presence of label noise. We derive
a mistake bound  similar in form to the second order perceptron bound 
that does not assume separability. We also relate our algorithm to recent
conﬁdence-weighted online learning techniques and show empirically that
AROW achieves state-of-the-art performance and notable robustness in the
case of non-separable data.

1 Introduction

Online learning algorithms are fast  simple  make few statistical assumptions  and perform
well in a wide variety of settings. Recent work has shown that parameter conﬁdence in-
formation can be eﬀectively used to guide online learning [2]. Conﬁdence weighted (CW)
learning  for example  maintains a Gaussian distribution over linear classiﬁer hypotheses
and uses it to control the direction and scale of parameter updates [6]. In addition to for-
mal guarantees in the mistake-bound model [11]  CW learning has achieved state-of-the-art
performance on many tasks. However  the strict update criterion used by CW learning is
very aggressive and can over-ﬁt [5]. Approximate solutions can be used to regularize the
update and improve results; however  current analyses of CW learning still assume that the
data are separable. It is not immediately clear how to relax this assumption.
In this paper we present a new online learning algorithm for binary classiﬁcation that com-
bines several attractive properties:
large margin training  conﬁdence weighting  and the
capacity to handle non-separable data. The key to our approach is the adaptive regular-
ization of the prediction function upon seeing each new instance  so we call this algorithm
Adaptive Regularization of Weights (AROW). Because it adjusts its regularization for each
example  AROW is robust to sudden changes in the classiﬁcation function due to label
noise. We derive a mistake bound  similar in form to the second order perceptron bound 
that does not assume separability. We also provide empirical results demonstrating that
AROW is competitive with state-of-the-art methods and improves upon them signiﬁcantly
in the presence of label noise.

2 Conﬁdence Weighted Online Learning of Linear Classiﬁers
Online algorithms operate in rounds. In round t the algorithm receives an instance xt ∈ Rd
and applies its current prediction rule to make a prediction ˆyt ∈ Y. It then receives the true

1

label yt ∈ Y and suﬀers a loss ‘(yt  ˆyt). For binary classiﬁcation we have Y = {−1  +1} and
use the zero-one loss ‘01(yt  ˆyt) = 0 if yt = ˆyt and 1 otherwise. Finally  the algorithm updates
its prediction rule using (xt  yt) and proceeds to the next round. In this work we consider
linear prediction rules parameterized by a weight vector w: ˆy = hw(x) = sign(w · x).
Recently Dredze  Crammer and Pereira [6  5] proposed an algorithmic framework for on-
line learning of binary classiﬁcation tasks called conﬁdence weighted (CW) learning. CW
learning captures the notion of conﬁdence in a linear classiﬁer by maintaining a Gaussian
distribution over the weights with mean µ ∈ Rd and covariance matrix Σ ∈ Rd×d. The
values µp and Σp p  respectively  encode the learner’s knowledge of and conﬁdence in the
weight for feature p: the smaller Σp p  the more conﬁdence the learner has in the mean
weight value µp. Covariance terms Σp q capture interactions between weights.
Conceptually  to classify an instance x  a CW classiﬁer draws a parameter vector w ∼
N (µ  Σ) and predicts the label according to sign(w · x). In practice  however  it can be
easier to simply use the average weight vector E [w] = µ to make predictions. This is similar
to the approach taken by Bayes point machines [9]  where a single weight vector is used to
approximate a distribution. Furthermore  for binary classiﬁcation  the prediction given by
the mean weight vector turns out to be Bayes optimal.
CW classiﬁers are trained according to a passive-aggressive rule [3] that adjusts the dis-
tribution at each round to ensure that the probability of a correct prediction is at least
η ∈ (0.5  1]. This yields the update constraint Pr [yt (w · xt) ≥ 0] ≥ η . Subject to this
constraint  the algorithm makes the smallest possible change to the hypothesis weight dis-
tribution as measured using the KL divergence. This implies the following optimization
problem for each round t:

(µt  Σt) = min

DKL

µ Σ

(cid:0)N (µ  Σ) kN(cid:0)µt−1  Σt−1

(cid:1)(cid:1)

s.t. Prw∼N (µ Σ) [yt (w · xt) ≥ 0] ≥ η

Conﬁdence-weighted algorithms have been shown to perform well in practice [5  6]  but they
suﬀer from several problems. First  the update is quite aggressive  forcing the probability
of predicting each example correctly to be at least η > 1/2 regardless of the cost to the
objective. This may cause severe over-ﬁtting when labels are noisy; indeed  current analyses
of the CW algorithm [5] assume that the data are linearly separable. Second  they are
designed for classiﬁcation  and it is not clear how to extend them to alternative settings
such as regression. This is in part because the constraint is written in discrete terms where
the prediction is either correct or not.
We deal with both of these issues  coping more eﬀectively with label noise and generalizing
the advantages of CW learning in an extensible way.

3 Adaptive Regularization Of Weights

explicitly as yt (µ · xt) ≥ φpx>

We identify two important properties of the CW update rule that contribute to its good
performance but also make it sensitive to label noise. First  the mean parameters µ are
guaranteed to correctly classify the current training example with margin following each
update. This is because the probability constraint Pr [yt (w · xt) ≥ 0] ≥ η can be written
t Σxt  where φ > 0 is a positive constant related to η.
This aggressiveness yields rapid learning  but given an incorrectly labeled example  it can
also force the learner to make a drastic and incorrect change to its parameters. Second 
conﬁdence  as measured by the inverse eigenvalues of Σ  increases monotonically with every
update. While it is intuitive that our conﬁdence should grow as we see more data  this
also means that even incorrectly labeled examples causing wild parameter swings result in
artiﬁcially increased conﬁdence.
In order to maintain the positives but reduce the negatives of these two properties  we
isolate and soften them. As in CW learning  we maintain a Gaussian distribution over
weight vectors with mean µ and covariance Σ; however  we recast the above characteristics
of the CW constraint as regularizers  minimizing the following unconstrained objective on

2

each round:

(cid:0)N (µ  Σ) kN(cid:0)µt−1  Σt−1

(cid:1)(cid:1) + λ1‘h2 (yt  µ · xt) + λ2x>

C (µ  Σ) = DKL

t Σxt  

(1)
where ‘h2 (yt  µ · xt) = (max{0  1 − yt(µ · xt)})2 is the squared-hinge loss suﬀered using the
weight vector µ to predict the output for input xt when the true output is yt. λ1  λ2 ≥ 0 are
two tradeoﬀ hyperparameters. For simplicity and compactness of notation  in the following
we will assume that λ1 = λ2 = 1/(2r) for some r > 0.
The objective balances three desires. First  the parameters should not change radically on
each round  since the current parameters contain information about previous examples (ﬁrst
term). Second  the new mean parameters should predict the current example with low loss
(second term). Finally  as we see more examples  our conﬁdence in the parameters should
generally grow (third term).
Note that this objective is not simply the dualization of the CW constraint  but a new
formulation inspired by the properties discussed above. Since the loss term depends on µ
only via the inner-product µ· xt  we are able to prove a representer theorem (Sec. 4). While
we use the squared-hinge loss for classiﬁcation  diﬀerent loss functions  as long as they are
convex and diﬀerentiable in µ  yield algorithms for diﬀerent settings.1
To solve the optimization in (1)  we begin by writing the KL explicitly:

(cid:18)det Σt−1

(cid:19)

det Σ

Tr(cid:0)Σ−1

t−1Σ(cid:1) +

(cid:0)µt−1 − µ(cid:1)>

1
2

+

1
2

Σ−1
t−1

(cid:0)µt−1 − µ(cid:1) − d

2

C (µ  Σ) =

1
2

log

‘h2 (yt  µ · xt) +

1
2r

1
2r

x>
t Σxt

+

(2)
We can decompose the result into two terms: C1(µ)  depending only on µ  and C2(Σ)  de-
pending only on Σ. The updates to µ and Σ can therefore be performed independently.
The squared-hinge loss yields a conservative (or passive) update for µ in which the mean
parameters change only when the margin is too small  and we follow CW learning by en-
forcing a correspondingly conservative update for the conﬁdence parameter Σ  updating it
only when µ changes. This results in fewer updates and is easier to analyze. Our update
thus proceeds in two stages.

1. Update the mean parameters:
2. If µt 6= µt−1  update the conﬁdence parameters:

µt = arg min
µ
Σt = arg min
Σ

C1 (µ)
C2 (Σ)

(3)

(4)

We now develop the update equations for (3) and (4) explicitly  starting with the former.
Taking the derivative of C (µ  Σ) with respect to µ and setting it to zero  we get

(cid:20) d

dz

(cid:21)

µt = µt−1 − 1
2r

‘h2 (yt  z)|z=µt·xt

Σt−1xt  

(5)

assuming Σt−1 is non-singular. Substituting the derivative of the squared-hinge loss in (5)
and assuming 1 − yt (µt · xt) ≥ 0  we get
µt = µt−1 + yt
r

(1 − yt (µt · xt)) Σt−1xt .

(6)

We solve for µt by taking the dot product of each side of the equality with xt and substituting
back in (6) to obtain the rule

µt = µt−1 +

Σt−1ytxt .

(7)

It can be easily veriﬁed that (7) satisﬁes our assumption that 1 − yt (µt · xt) ≥ 0.

1It can be shown that the well known recursive least squares (RLS) regression algorithm [7] is a

special case of AROW with the squared loss.

3

max(cid:0)0  1 − ytx>

x>
t Σt−1xt + r

t µt−1

(cid:1)

Input parameters
Initialize µ0 = 0   Σ0 = I 
For t = 1  . . .   T

r

• Receive a training example xt ∈ Rd
• Compute margin and conﬁdence mt = µt−1 · xt
vt = x>
• Receive true label yt  and suﬀer loss ‘t = 1 if sign (mt) 6= yt
• If mtyt < 1  update using eqs. (7) & (9):

t Σt−1xt

µt = µt−1 + αtΣt−1ytxt

βt =

1

x>
t Σt−1xt + r

Output: Weight vector µT and conﬁdence ΣT .

Σt = Σt−1 − βtΣt−1xtx

>
t Σt−1

αt = max

0  1 − ytx

>
t µt−1

βt

”

“

Figure 1: The AROW algorithm for online binary classiﬁcation.

6= µt−1  that is  if 1 >
t µt−1. In this case  we compute the update of the conﬁdence parameters by setting

The update for the conﬁdence parameters is made only if µt
ytx>
the derivative of C (µ  Σ) with respect to Σ to zero:

Σ−1
t = Σ−1

t−1 + xtx>

t

r

(8)

Using the Woodbury identity we can also rewrite the update for Σ in non-inverted form:

Σt = Σt−1 − Σt−1xtx>

r + x>

t Σt−1
t Σt−1xt

(9)

Note that it follows directly from (8) and (9) that the eigenvalues of the conﬁdence pa-
rameters are monotonically decreasing: Σt (cid:22) Σt−1; Σ−1
t−1 . Pseudocode for AROW
appears in Fig. 1.

t (cid:23) Σ−1

4 Analysis

We ﬁrst show that AROW can be kernelized by stating the following representer theorem.

Lemma 1 (Representer Theorem) Assume that Σ0 = I and µ0 = 0. The mean param-
eters µt and conﬁdence parameters Σt produced by updating via (7) and (9) can be written
as linear combinations of the input vectors (resp. outer products of the input vectors with
themselves) with coeﬃcients depending only on inner-products of input vectors.

Proof sketch: By induction. The base case follows from the deﬁnitions of µ0 and Σ0 
and the induction step follows algebraically from the update rules (7) and (9).
We now prove a mistake bound for AROW. Denote by M (M = |M|) the set of example
indices for which the algorithm makes a mistake  yt
P
set of example indices for which there is an update but not a mistake  0 < yt (µt · xt) ≤ 1.
Other examples do not aﬀect the behavior of the algorithm and can be ignored. Let XM =
t∈M xix>

(cid:1) ≤ 0  and by U (U = |U|) the

(cid:0)µt−1 · xt

i and XA = XM + XU.

t∈U xix>

Theorem 2 For any reference weight vector u ∈ Rd  the number of mistakes made by
AROW (Fig. 1) is upper bounded by

i   XU =P
q

gt − U  

(10)

s

(cid:18)

(cid:18)

log

det

(cid:19)(cid:19)

+ U + X

t∈M∪U

I +

1
r

XA

M ≤

r kuk2 + u>XAu

where gt = max(cid:0)0  1 − ytu>xt

(cid:1).

The proof depends on two lemmas; we omit the proof of the ﬁrst for lack of space.

4

Lemma 3 Let ‘t = max(cid:0)0  1 − ytµ>

t−1xt

t Σt−1xt. Then  for every t ∈ M∪U 

(cid:1) and χt = x>
t−1µt−1 + ytu>xt
t−1µt−1 + χt + r − ‘2
t r
r (χt + r)
≤ log(cid:0)det(cid:0)Σ−1
(cid:1)(cid:1) .

r

T +1

u>Σ−1
t µt = u>Σ−1
t Σ−1
t−1Σ−1
µ>
t µt = µ>
X

χtr

r (χt + r)

t

Lemma 4 Let T be the number of rounds. Then

t

t Σt−1

t = x>

x>
t Σtx>

Proof: We compute the following quantity:

(cid:0)Σt−1 − βtΣt−1xtx>

(cid:1) xt = χt − χ2
(cid:1)
t = 1 − det(cid:0)Σ−1
Using Lemma D.1 from [2] we have that
(cid:1) .
det(cid:0)Σ−1
t Σtx>
x>
 det(cid:0)Σ−1
(cid:1)!
(cid:1)
≤ −X
det(cid:0)Σ−1

 
1 − det(cid:0)Σ−1
det(cid:0)Σ−1

=X

Combining  we get

(cid:1)!
(cid:1)

X

r (χt + r)

t−1

t−1

t−1

χtr

log

1
r

t

t

t

t

t

t

t

= χtr
χt + r

.

χt + r

(11)

≤ log(cid:0)det(cid:0)Σ−1

T +1

(cid:1)(cid:1) .

We now prove Theorem 2.
Proof: We iterate the ﬁrst equality of Lemma 3 to get

ytu>xt

r

1 − gt

r

= M + U

r

− 1
r

gt .

(12)

X

t∈M∪U

We iterate the second equality to get

≥ X
= X

t∈M∪U

u>Σ−1

T Σ−1
µ>

t∈M∪U

T µT = X
T µT = X
(cid:0)xt · µt−1

t∈M∪U

X

t∈M∪U

1 − ‘2
t
χt + r

Using Lemma 4 we have that the ﬁrst term of (13) is upper bounded by 1
For the second term in (13) we consider two cases. First  if a mistake occurred on example
t ≤ 0. Second  if an the algorithm
t  then we have that yt
made an update (but no mistake) on example t  then 0 < yt
thus 1 − ‘2

t ≤ 1. We therefore have

+ X

t∈M∪U

(cid:0)xt · µt−1
=X

1

χt + r

χt + r − ‘2
t r
r (χt + r)

χt

r (χt + r)

t∈M∪U

1

0

t∈U

t∈M

χt + r

χt + r

(cid:1) ≤ 0 and ‘t ≥ 1  so 1− ‘2
≤ X
T µT ≤q
gt ≤q
q

+X
q
log(cid:0)det(cid:0)Σ−1
(cid:1)(cid:1) + U + X

s1
log(cid:0)det(cid:0)Σ−1

u>Σ−1
T u

u>Σ−1
T u

T µT  

t∈U

r

T

T

(cid:1)(cid:1) +X

t∈U

gt − U .

t∈M∪U

1 − ‘2
t
χt + r

.

(13)

(cid:1)(cid:1).
r log(cid:0)det(cid:0)Σ−1
(cid:1) ≤ 1 and ‘t ≥ 0 

T

.

(14)

1

χt + r

.

(15)

Combining and plugging into the Cauchy-Schwarz inequality
T Σ−1
µ>

u>Σ−1

we get

M + U

r

− 1
r

X
q

t∈M∪U

M ≤√

r

u>Σ−1
T u

Rearranging the terms and using the fact that χt ≥ 0 yields

5

By deﬁnition 

Σ−1
T = I +

xix>

i = I +

XA  

1
r

so substituting and simplifying completes the proof:

t∈M∪U

X
(cid:18)
(cid:18)

1
r

s
(cid:18)

(cid:19)
s

u>

I +

1
r

XA

u

log

det

r kuk2 + u>XAu

log

det

s

(cid:18)

r

M ≤√
q

=

(cid:18)

I +

1
r

XA

(cid:19)(cid:19)

I +

1
r

XA

(cid:19)(cid:19)
+ U + X
+ U + X

t∈M∪U

t∈M∪U

gt − U

gt − U .

A few comments are in order. First  the two square-root terms of the bound depend on r
in opposite ways: the ﬁrst is monotonically increasing  while the second is monotonically
decreasing. One could expect to optimize the bound by minimizing over r. However  the
bound also depends on r indirectly via other quantities (e.g. XA)  so there is no direct way
to do so. Second  if all the updates are associated with errors  that is  U = ∅  then the bound
reduces to the bound of the second-order perceptron [2]. In general  however  the bounds
are not comparable since each depends on the actual runtime behavior of its algorithm.

5 Empirical Evaluation

We evaluate AROW on both synthetic and real data  including several popular datasets
for document classiﬁcation and optical character recognition (OCR). We compare with
three baselines: Passive-Aggressive (PA)  Second Order Perceptron (SOP)2 and Conﬁdence-
Weighted (CW) learning3.
Our synthetic data are as in [5]  but we invert the labels on 10% of the training examples.
(Note that evaluation is still done against the true labels.) Fig. 2(a) shows the online learning
curves for both full and diagonalized versions of the algorithms on these noisy data. AROW
improves over all competitors  and the full version outperforms the diagonal version. Note
that CW-full performs worse than CW-diagonal  as has been observed previously for noisy
data.
We selected a variety of document classiﬁcation datasets popular in the NLP community 
summarized as follows. Amazon: Product reviews to be classiﬁed into domains (e.g. 
books or music) [6]. We created binary datasets by taking all pairs of the six domains (15
datasets). Feature extraction follows [1] (bigram counts). 20 Newsgroups: Approximately
20 000 newsgroup messages partitioned across 20 diﬀerent newsgroups4. We binarized the
corpus following [6] and used binary bag-of-words features (3 datasets). Each dataset has
between 1850 and 1971 instances. Reuters (RCV1-v2/LYRL2004): Over 800 000 man-
ually categorized newswire stories. We created binary classiﬁcation tasks using pairs of
labels following [6] (3 datasets). Details on document preparation and feature extraction
are given by [10]. Sentiment: Product reviews to be classiﬁed as positive or negative. We
used each Amazon product review domain as a sentiment classiﬁcation task (6 datasets).
Spam: We selected three task A users from the ECML/PKDD Challenge5  using bag-of-
words to classify each email as spam or ham (3 datasets). For OCR data we binarized two
well known digit recognition datasets  MNIST6 and USPS  into 45 all-pairs problems. We
also created ten one vs. all datasets from the MNIST data (100 datasets total).
Each result for the text datasets was averaged over 10-fold cross-validation. The OCR
experiments used the standard split into training and test sets. Hyperparameters (including

2

For the real world (high dimensional) datasets  we must drop cross-feature conﬁdence terms by projecting
onto the set of diagonal matrices  following the approach of [6]. While this may reduce performance  we make the
same approximation for all evaluated algorithms.

3

4

5

6

We use the “variance” version developed in [6].

http://people.csail.mit.edu/jrennie/20Newsgroups/

http://ecmlpkdd2006.org/challenge.html

http://yann.lecun.com/exdb/mnist/index.html

6

(a) synthetic data

(b) MNIST data

Figure 2: Learning curves for AROW (full/diagonal) and baseline methods. (a) 5k synthetic
training examples and 10k test examples (10% noise  100 runs). (b) MNIST 3 vs. 5 binary
classiﬁcation task for diﬀerent amounts of label noise (left: 0 noise  right: 10%).

r for AROW) and the number of online iterations (up to 10) were optimized using a single
randomized run. We used 2000 instances from each dataset unless otherwise noted above.
In order to observe each algorithm’s ability to handle non-separable data  we performed each
experiment using various levels of artiﬁcal label noise  generated by independently ﬂipping
each binary label with ﬁxed probability.

5.1 Results and Discussion

experimental

are

Algorithm
AROW
CW
PA
SOP

0.2
1.25
2.42
2.33
4.00

0.3
1.25
2.76
2.08
3.91

0.0
1.51
1.63
2.95
3.91

0.05
1.44
1.87
2.83
3.87

Noise level
0.15
0.1
1.38
1.42
2.08
1.95
2.61
2.78
3.89
3.89

Table 1: Mean rank (out of 4  over all datasets) at diﬀer-
ent noise levels. A rank of 1 indicates that an algorithm
outperformed all the others.

Our
results
are summarized in Table 1.
AROW outperforms the base-
lines at all noise levels  but
does especially well as noise
increases.
More detailed
results for AROW and CW 
the overall best performing
baseline 
compared in
Fig. 3. AROW and CW are
comparable when there is no
added noise  with AROW
winning the majority of the time. As label noise increases (moving across the rows in
Fig. 3) AROW holds up remarkably well. In almost every high noise evaluation  AROW
improves over CW (as well as the other baselines  not shown). Fig. 2(b) shows the total
number of mistakes (w.r.t. noise-free labels) made by each algorithm during training on the
MNIST dataset for 0% and 10% noise. Though absolute performance suﬀers with noise 
the gap between AROW and the baselines increases.
To help interpret the results  we classify the algorithms evaluated here according to four
characteristics: the use of large margin updates  conﬁdence weighting  a design that acco-
modates non-separable data  and adaptive per-instance margin (Table 2). While all of these
properties can be desirable in diﬀerent situations  we would like to understand how they
interact and achieve high performance while avoiding sensitivity to noise.
Based on the results in Ta-
ble 1  it is clear that the com-
bination of conﬁdence informa-
tion and large margin learning
is powerful when label noise is
low. CW easily outperforms
the other baselines in such situ-
ations  as it has been shown to
do in previous work. However 
as noise increases  the separa-
bility assumption inherent in CW appears to reduce its performance considerably.

Large
Algorithm Margin
PA
SOP
CW
AROW

Yes
No
Yes
Yes

Table 2: Online algorithm properties overview.

Conf-
idence

Non-

Adaptive
Separable Margin

No
Yes
Yes
Yes

Yes
Yes
No
Yes

No
No
Yes
No

7

5001000150020002500300035004000450050002004006008001000120014001600InstancesMistakes PerceptronPASOPAROW−fullAROW−diagCW−fullCW−diag0200040006000800010000Instances0100200300400500600700800MistakesPACWAROWSOP0200040006000800010000Instances0500100015002000MistakesPACWAROWSOPFigure 3: Accuracy on text (top) and OCR (bottom) binary classiﬁcation. Plots compare
performance between AROW and CW  the best performing baseline (Table 1). Markers
above the line indicate superior AROW performance and below the line superior CW per-
formance. Label noise increases from left to right: 0%  10% and 30%. AROW improves
relative to CW as noise increases.

AROW  by combining the large margin and conﬁdence weighting of CW with a soft update
rule that accomodates non-separable data  matches CW’s performance in general while
avoiding degradation under noise. AROW lacks the adaptive margin of CW  suggesting
that this characteristic is not crucial to achieving strong performance. However  we leave
open for future work the possibility that an algorithm with all four properties might have
unique advantages.

6 Related and Future Work

AROW is most similar to the second order perceptron [2]. The SOP performs the same type
of update as AROW  but only when it makes an error. AROW  on the other hand  updates
even when its prediction is correct if there is insuﬃcient margin. Conﬁdence weighted (CW)
[6  5] algorithms  by which AROW was inspired  update the mean and conﬁdence parameters
simultaneously  while AROW makes a decoupled update and softens the hard constraint of
CW. The AROW algorithm can be seen as a variant of the PA-II algorithm from [3] where
the regularization is modiﬁed according to the data.
Hazan [8] describes a framework for gradient descent algorithms with logarithmic regret in
which a quantity similar to Σt plays an important role. Our algorithm diﬀers in several
ways. First  Hazan [8] considers gradient algorithms  while we derive and analyze algo-
rithms that directly solve an optimization problem. Second  we bound the loss directly  not
the cumulative sum of regularization and loss. Third  the gradient algorithms perform a
projection after making an update (not before) since the norm of the weight vector is kept
bounded.
Ongoing work includes the development and analysis of AROW style algorithms for other
settings  including a multi-class version following the recent extension of CW to multi-class
problems [4]. Our mistake bound can be extended to this case. Applying the ideas behind
AROW to regression problems turns out to yield the well known recursive least squares
(RLS) algorithm  for which AROW oﬀers new bounds (omitted). Finally  while we used the
conﬁdence term x>
t Σxt in (1)  we can replace this term with any diﬀerentiable  monotoni-
cally increasing function f(x>

t Σxt). This generalization may yield additional algorithms.

8

0.750.800.850.900.951.00CW0.750.800.850.900.951.00AROW20newsamazonreuterssentimentspam0.50.60.70.80.91.0CW0.50.60.70.80.91.0AROW20newsamazonreuterssentimentspam0.50.60.70.80.91.0CW0.50.60.70.80.91.0AROW20newsamazonreuterssentimentspam0.900.920.940.960.981.00CW0.900.920.940.960.981.00AROWUSPS 1 vs. AllUSPS All PairsMNIST 1 vs. All0.50.60.70.80.91.0CW0.50.60.70.80.91.0AROWUSPS 1 vs. AllUSPS All PairsMNIST 1 vs. All0.50.60.70.80.91.0CW0.50.60.70.80.91.0AROWUSPS 1 vs. AllUSPS All PairsMNIST 1 vs. AllReferences

[1] John Blitzer  Mark Dredze  and Fernando Pereira. Biographies  bollywood  boom-boxes

and blenders: Domain adaptation for sentiment classiﬁcation. In ACL  2007.

[2] Nicol´o Cesa-Bianchi  Alex Conconi  and Claudio Gentile. A second-order perceptron

algorithm. Siam J. of Comm.  34  2005.

[3] Koby Crammer  Ofer Dekel  Joseph Keshet  Shai Shalev-Shwartz  and Yoram Singer.
Online passive-aggressive algorithms. Journal of Machine Learning Research  7:551–
585  2006.

[4] Koby Crammer  Mark Dredze  and Alex Kulesza. Multi-class conﬁdence weighted

algorithms. In Empirical Methods in Natural Language Processing (EMNLP)  2009.

[5] Koby Crammer  Mark Dredze  and Fernando Pereira. Exact convex conﬁdence-weighted

learning. In Neural Information Processing Systems (NIPS)  2008.

[6] Mark Dredze  Koby Crammer  and Fernando Pereira. Conﬁdence-weighted linear clas-

siﬁcation. In International Conference on Machine Learning  2008.

[7] Simon Haykin. Adaptive Filter Theory. 1996.
[8] Elad Hazan. Eﬃcient algorithms for online convex optimization and their applications.

PhD thesis  Princeton University  2006.

[9] Ralf Herbrich  Thore Graepel  and Colin Campbell. Bayes point machines. Journal of

Machine Learning Research (JMLR)  1:245–279  2001.

[10] David D. Lewis  Yiming Yang  Tony G. Rose  and Fan Li. Rcv1: A new benchmark

collection for text categorization research. JMLR  5:361–397  2004.

[11] Nick Littlestone. Learning when irrelevant attributes abound: A new linear-threshold

algorithm. Machine Learning  2:285–318  1988.

9

,Assaf Glazer
Omer Weissbrod
Michael Lindenbaum
Shaul Markovitch
Heiko Strathmann
Dino Sejdinovic
Samuel Livingstone
Zoltan Szabo
Arthur Gretton
Albert Berahas
Jorge Nocedal
Martin Takac