2018,Watch Your Step: Learning Node Embeddings via Graph Attention,Graph embedding methods represent nodes in a continuous vector space 
preserving different types of relational information from the graph.
There are many hyper-parameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph.
In this paper  we replace previously fixed hyper-parameters with trainable ones that we automatically learn via backpropagation. 
In particular  we propose a novel attention model on the power series of the transition matrix  which guides the random walk to optimize an upstream objective.
Unlike previous approaches to attention models  the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk)  and are not used by the model for inference.
We experiment on link prediction tasks  as we aim to produce embeddings that best-preserve the graph structure  generalizing to unseen information. 
We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social  collaboration  and biological networks  where we observe that our graph attention model can reduce the error by up to 20\%-40\%.
We show that our automatically-learned attention parameters can vary significantly per graph  and correspond to the optimal choice of hyper-parameter if we manually tune existing methods.,Watch Your Step:

Learning Node Embeddings via Graph Attention

Sami Abu-El-Haija∗

Information Sciences Institute 
University of Southern California

haija@isi.edu

Bryan Perozzi

Google AI

New York City  NY
bperozzi@acm.org

Rami Al-Rfou

Google AI

Mountain View  CA
rmyeid@google.com

Alex Alemi
Google AI

Mountain View  CA
alemi@google.com

Abstract

Graph embedding methods represent nodes in a continuous vector space  preserving
different types of relational information from the graph. There are many hyper-
parameters to these methods (e.g. the length of a random walk) which have to be
manually tuned for every graph. In this paper  we replace previously ﬁxed hyper-
parameters with trainable ones that we automatically learn via backpropagation. In
particular  we propose a novel attention model on the power series of the transition
matrix  which guides the random walk to optimize an upstream objective. Unlike
previous approaches to attention models  the method that we propose utilizes
attention parameters exclusively on the data itself (e.g. on the random walk)  and
are not used by the model for inference. We experiment on link prediction tasks  as
we aim to produce embeddings that best-preserve the graph structure  generalizing
to unseen information. We improve state-of-the-art results on a comprehensive
suite of real-world graph datasets including social  collaboration  and biological
networks  where we observe that our graph attention model can reduce the error
by up to 20%-40%. We show that our automatically-learned attention parameters
can vary signiﬁcantly per graph  and correspond to the optimal choice of hyper-
parameter if we manually tune existing methods.

1

Introduction

Unsupervised graph embedding methods seek to learn representations that encode the graph structure.
These embeddings have demonstrated outstanding performance on a number of tasks including node
classiﬁcation [29  15]  knowledge-base completion [24]  semi-supervised learning [37]  and link
prediction [2]. In general  as introduced by Perozzi et al [29]  these methods operate in two discrete
steps: First  they sample pair-wise relationships from the graph through random walks and counting
node co-occurances. Second  they train an embedding model e.g. using Skipgram of word2vec [25] 
to learn representations that encode pairwise node similarities.
While such methods have demonstrated positive results on a number of tasks  their performance
can signiﬁcantly vary based on the setting of their hyper-parameters. For example  [29] observed
that the quality of learned representations is dependent on the length of the random walk (C). In
practice  DeepWalk [29] and many of its extensions [e.g. 15] use word2vec implementations [25].

∗Work done while at Google AI.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Accordingly  it has been revealed by [21] that the hyper-parameter C  refered to as training window
length in word2vec [25]  actually controls more than a ﬁxed length of the random walk. Instead 
it parameterizes a function  we term the context distribution and denote Q  which controls the
probability of sampling a node-pair when visited within a speciﬁc distance 2. Implicitly  the choices
of C and Q  create a weight mass on every node’s neighborhood. In general  the weight is higher
on nearby nodes  but the speciﬁc form of the mass function is determined by the aforementioned
hyper-parameters. In this work  we aim to replace these hyper-parameters with trainable parameters 
so that they can be automatically learned for each graph. To do so  we pose graph embedding as
end-to-end learning  where the (discrete) two steps of random walk co-occurance sampling  followed
by representation learning  are joint using a closed-form expectation over the graph adjacency matrix.
Our inspiration comes from the successful application of attention models in domains such as Natural
Language Processing (NLP) [e.g. 4  38]  image recognition [26]  and detecting rare events in videos
[31]. To the best of our knowledge  the approach we propose is signiﬁcantly different from the
standard application of attention models. Instead of using attention parameters to guide the model
where to look when making a prediction  we use attention parameters to guide our learning algorithm
to focus on parts of the data that are most helpful for optimizing an upstream objective.
We show the mathematical equivalence between the context distribution and the co-efﬁcients of
power series of the transition matrix. This allows us to learn the context distribution by learning an
attention model on the power series. The attention parameters “guide” the random walk  by allowing
it to focus more on short- or long-term dependencies  as best suited for the graph  while optimizing
an upstream objective. To the best of our knowledge  this work is the ﬁrst application of attention
methods to graph embedding.
Speciﬁcally  our contributions are the following:

1. We propose an extendible family of graph attention models that can learn arbitrary (e.g.

non-monotonic) context distributions.

2. We show that the optimal choice of context distribution hyper-parameters for competing
methods  found by manual tuning  agrees with our automatically-found attention parameters.
3. We evaluate on a number of challenging link prediction tasks comprised of real world
datasets  including social  collaboration  and biological networks. Experiments show we
substantially improve on our baselines  reducing link-prediction error by 20%-40%.

2 Preliminaries

2.1 Graph Embeddings
Given an unweighted graph G = (V  E)  its (sparse) adjacency matrix A ∈ {0  1}|V |×|V | can be
constructed according to Avu = 1[(v  u) ∈ E]  where the indicator function 1[.] evaluates to 1 iff its
boolean argument is true. In general  graph embedding methods minimize an objective:

L(f (A)  g(Y));

min
Y

(1)
where Y ∈ R|V |×d is a d-dimensional node embedding dictionary; f : R|V |×|V | → R|V |×|V | is a
transformation of the adjacency matrix; g : R|V |×d → R|V |×|V | is a pairwise edge function; and
L : R|V |×|V | × R|V |×|V | → R is a loss function.
Many popular embedding methods can be viewed in this light. For instance  a stochastic3 version of
Singular Value Decomposition (SVD) is an embedding method  and can be cast into our framework by
setting f (A) = A; decomposing Y into two halves  the left and right representations4 as Y = [L|R]

2To clarify  as noted by Levy et al [21] – studying the implementation of word2vec reveals that rather than
using C as constant and assuming all nodes visited within distance C are related  a desired context distance ci is
sampled from uniform (ci ∼ U{1  C}) for each node pair i in training. If the node pair i was visited more than
ci-steps apart  it is not used for training. Many DeepWalk-style methods inherited this context distribution  as
they internally utilize standard word2vec implementations.

3Orthonormality Constraints are not shown.
4Also known in NLP [25] as the “input” and “output” embedding representations.

2

with L  R ∈ R|V |× d
setting L to the Frobenius norm of the error  yielding:

2 then setting g to their outer product g(Y) = g([L|R]) = L × R(cid:62); and ﬁnally

||A − L × R(cid:62)||F

min
L R

2.2 Learning Embeddings via Random Walks

Introduced by [29]  this family of methods [incl. 15  19  30  10] induce random walks along E by
starting from a random node v0 ∈ sample(V )  and repeatedly sampling an edge to transition to
next node as vi+1 := sample(N [vi])  where N [vi] are the outgoing edges from vi. The transition
sequences v0 → v1 → v2 → . . . (i.e. random walks) can then be passed to word2vec algorithm 
which learns embeddings by stochastically taking every node along the sequence vi  and the embed-
ding representation of this anchor node vi is brought closer to the embeddings of its next neighbors 
{vi+1  vi+2  . . .   vi+c}  the context nodes. In practice  the context window size c is sampled from a
distribution e.g. uniform U{1  C} as explained in [21]. For further information on graph embedding
methods see [9].
Let D ∈ R|V |×|V | be the co-occurrence matrix from random walks  with each entry Dvu containing
the number of times nodes v and u are co-visited within context distance c ∼ U{1  C}  in all
simulated random walks. Embedding methods utilizing random walks  can also be viewed using
the framework of Eq. (1). For example  to get Node2vec [15]  we can set f (A) = D  set the edge
function to the embeddings outer product g(Y) = Y × Y(cid:62)  and set the loss function to negative log
likelihood of softmax  yielding:

log Z − (cid:88)

v u∈V

  

Dvu(Y (cid:62)

v Yu)

v u exp(Y (cid:62)

v Yu) can be estimated with negative sampling [25  15].

(2)

where partition function Z =(cid:80)

min
Y

2.2.1 Graph Likelihood

(cid:89)

v u∈V

A recently-proposed objective for learning embeddings is the graph likelihood [2]:

σ(g(Y)v u)Dvu(1 − σ(g(Y)v u))1[(v u) /∈E] 

(3)

where g(Y)v u is the output of the model evaluated at edge (v  u)  given node embeddings Y; the
activation function σ(.) is the logistic; Maximizing the graph likelihood pushes the model score
g(Y)v u towards 1 if value Dvu is large and pushes it towards 0 if (v  u) /∈ E.
In our work  we minimize the negative log of Equation 3  written in our matrix notation as:

min
Y

||−D ◦ log (σ(g(Y))) − 1[A = 0] ◦ log (1 − σ(g(Y)))||1  

(4)
which we minimize w.r.t node embeddings Y ∈ R|V |×d   where ◦ is the Hadamard product; and the
L1-norm ||.||1 of a matrix is the sum of its entries. The entries of this matrix are positive because
0 < σ(.) < 1. Matrix D ∈ R|V |×|V | can be created similar to the one described in [2]  by counting
node co-occurrences in simulated random walks.

2.3 Attention Models

We mention attention models that are most similar to ours [e.g. 26  31  35]  where an attention
function is employed to suggest positions within the input example that the classiﬁcation function
should pay attention to  when making inference. This function is used during the training phase in
the forward pass and in the testing phase for prediction. The attention function and the classiﬁer are
jointly trained on an upstream objective e.g. cross entropy. In our case  the attention mechanism
is only guides the learning procedure  and not used by the model for inference. Our mechanism
suggests parts of the data to focus on  during training  as explained next.

3

3 Our Method
Following our general framework (Eq 1)  we set g(Y) = g ([L | R]) = L× R(cid:62) and f (A) = E[D] 
the expectation on co-occurrence matrix produced from simulated random walk. Using this closed
form  we extend the the Negative Log Graph Likelihood (NLGL) loss (Eq. 4) to include attention
parameters on the random walk sampling.

3.1 Expectation on the co-occurance matrix: E[D]

Rather than obtaining D by simulation of random walks and sampling co-occurances  we formulate
an expectation of this sampling  as E[D]. In general. this allows us to tune sampling parameters
living inside of the random walk procedure including number of steps C.
Let T be the transition matrix for a graph  which can be calculated by normalizing the rows of A to
sum to one. This can be written as:

T = diag(A × 1n)−1 × A.

(5)
Given an initial probability distribution p(0) ∈ R|V | of a random surfer  it is possible to ﬁnd the
distribution of the surfer after one step conditioned on p(0) as p(1) = p(0)(cid:62)T and after k steps as
p(k) = p(0)(cid:62)
(T )k  where (T )k multiplies matrix T with itself k-times. We are interested in an
analytical expression for E[D]  the expectation over co-occurrence matrix produced by simulated
random walks. A closed form expression for this matrix will allow us to perform end-to-end learning.
In practice  random walk methods based on DeepWalk [29] do not use C as a hard limit; instead 
given walk sequence (v1  v2  . . . )  they sample ci ∼ U{1  C} separately for each anchor node vi and
potential context nodes  and only keep context nodes that are within ci-steps of vi. In expectation then 
nodes vi+1  vi+2  vi+3  . . .   will appear as context for anchor node vi  respectively with probabilities
1  1 − 1

C   . . . . We can write an expectation on D ∈ R|V |×|V |:

C   1 − 2

E(cid:2)DDEEPWALK; C(cid:3) =

C(cid:88)

k=1

Pr(c ≥ k) ˜P(0) (T )k  

(cid:21)

(T )k .

(6)
which is parametrized by the (discrete) walk length C; where Pr(c ≥ k) indicates the probability of
node with distance k from anchor to be selected; and ˜P(0) ∈ R|V |×|V | is a diagonal matrix (the initial
positions matrix)  with ˜P(0)
C for
j=k P (c = j)  and re-write the expectation as:
1 − k − 1

all k = {1  2  . . .   C}  we can expand Pr(c ≥ k) =(cid:80)C
(cid:20)
C(cid:88)
E(cid:2)DDEEPWALK; C(cid:3) = ˜P(0)
(cid:3)  but we note that the coefﬁcient decreases with k.

Eq. (7) is derived  step-by-step  in the Appendix. We are not concerned by the exact deﬁnition of the

vv set to the number of walks starting at node v. Since Pr(c = k) = 1

Instead of keeping C a hyper-parameter  we want to analytically optimize it on an upstream objective.
Further  we are interested to learn the co-efﬁcients to (T )k instead of hand-engineering a formula.
As an aside  running the GloVe embedding algorithm [28] over the random walk sequences  in expec-

scalar coefﬁcient (cid:2)1 − k−1
tation  is equivalent to factorizing the co-occurance matrix: E(cid:2)DGloVe; C(cid:3) = ˜P(0)(cid:80)C
3.2 Learning the Context Distribution
as Q = (Q1  Q2 ···   QC) with Qk ≥ 0 and(cid:80)
We want to learn the co-efﬁcients to (T )k. Let the context distribution Q be a C-dimensional vector
k Qk = 1. We assign co-efﬁcient Qk to (T )k.
C(cid:88)

Formally  our expectation on D is parameterized with  and is differentiable w.r.t.  Q:

(cid:3) (T )k .

(cid:2) 1

k

(7)

k=1

k=1

C

C

E [D; Q1  Q2  . . . QC] = ˜P(0)

Qk (T )k = ˜P(0) E
k∼Q

[(T )k] 

special cases of Equation 8  with Q ﬁxed apriori as Qk =(cid:2)1 − k−1

k=1

(cid:3) or Qk ∝ 1

Training embeddings over random walk sequences  using word2vec or GloVe  respectively  are

k .

C

(8)

4

Dataset
wiki-vote
ego-Facebook
ca-AstroPh
ca-HepTh
PPI [33]

|V |
7  066
4  039
17  903
8  638
3  852

|E|
103  663
88  234
197  031
24  827
20  881

nodes
users
users

researchers
researchers

proteins

edges
votes

friendship

co-authorship
co-authorship

chemical interaction

(a) Datasets used in our experiments: wiki-vote is
directed but all others are undirected graphs.
Figure 1: In 1a we present statistics of our datasets. In 1b  we motivate our work by showing the
necessity of setting the parameter C for node2vec (d=128  each point is the average of 7 runs).

(b) Test ROC-AUC as a function of C using node2vec.

(a) Learned Attention weights Q (log scale).

(b) Q with varying the regularization β (linear scale).
Figure 2: (a) shows learned attention weights Q  which agree with grid-search of node2vec (Figure
1b). (b) shows how varying β affects the learned Q. Note that distributions can quickly tail off to zero
(ego-Facebook and PPI)  while other graphs (wiki-vote) contain information across distant nodes.

3.3 Graph Attention Models

To learn Q automatically  we propose an attention model which guides the random surfer on “where
to attend to” as a function of distance from the source node. Speciﬁcally  we deﬁne a Graph Attention
Model as a process which models a node’s context distribution Q as the output of softmax:

and(cid:80)

(Q1  Q2  Q3  . . . ) = softmax((q1  q2  q3  . . . )) 

(9)
where the variables qk are trained via backpropagation  jointly while learning node embeddings. Our
hypothesis is as follows. If we don’t impose a speciﬁc formula on Q = (Q1  Q2  . . . QC)  other than
(regularized) softmax  then we can use very large values of C and allow every graph to learn its own
form of Q with its preferred sparsity and own decay form. Should the graph structure require a small
C  then the optimization would discover a left-skewed Q with all of probability mass on {Q1  Q2}
k>2 Qk ≈ 0. However  if according to the objective  a graph is more accurately encoded by
making longer walks  then they can learn to use a large C (e.g. using uniform or even right-skewed Q
distribution)  focusing more attention on longer distance connections in the random walk.
To this end  we propose to train softmax attention model on the inﬁnite power series of the transition
matrix. We deﬁne an expectation on our proposed random walk matrix Dsoftmax[∞] as5:

Dsoftmax[∞]; q1  q2  q3  . . .

= ˜P(0)

lim
C→∞

softmax(q1  q2  q3  . . . )k (T )k  

(10)

where q1  q2  . . . are jointly trained with the embeddings to minimize our objective.

3.4 Training Objective

min
L R q

β||q||2

The ﬁnal training objective for the Softmax attention mechanism  coming from the NLGL Eq. (4) 

2 +(cid:12)(cid:12)(cid:12)(cid:12)−E[D; q] ◦ log(cid:0)σ(L × R(cid:62))(cid:1) − 1[A = 0] ◦ log(cid:0)1 − σ(L × R(cid:62))(cid:1)(cid:12)(cid:12)(cid:12)(cid:12)1

(11)
is minimized w.r.t attention parameter vector q = (q1  q2  . . . ) and node embeddings L  R ∈ R|V |× d
2 .
Hyper-parameter β ∈ R applies L2 regularization on the attention parameters. We emphasize that
our attention parameters q live within the expectation over data D  and are not part of the model
k Qk = 1  through the softmax
activation  prevents E[Dsoftmax] from collapsing into a trivial solution (zero matrix).

(L  R) and are therefore not required for inference. The constraint(cid:80)

5We do not actually unroll the summation in Eq. (10) an inﬁnite number of times. Our experiments show

that unrolling it 10 or 20 times is sufﬁcient to obtain state-of-the-art results.

5

E(cid:104)

(cid:105)

C(cid:88)

k=1

12345678910C0.99000.99050.99100.99150.99200.9925ROC-AUCfacebook12345678910C0.700.750.800.85ppi12345678910C0.600.610.620.630.64wiki-voteego-Facebookca-HepThca-AstroPhPPIwiki-vote103102101100Attention Probability MasssoftmaxQ1Q2Q3Q4Q512345678910Q0.00.51.0Attention Probability Massego-Facebook12345678910QPPI12345678910Qwiki-vote=0.3=0.5=0.7Dataset dim Eigen

Methods Use: A
Maps SVD DNGR n2v

64

61.3 86.0 59.8
wiki-vote 128 62.2 80.8 55.4
ego-Facebook 64
96.4 96.7 98.1
128 95.4 94.5 98.4
64
82.4 91.1 93.9
ca-AstroPh 128 82.9 92.4 96.8
ca-HepTh 64
80.2 79.3 86.8
128 81.2 78.0 89.7
64
70.7 75.4 76.7
PPI 128 73.7 71.2 76.9

D
n2v
C = 5
63.6
64.6
99.0
99.2
96.9
97.5
91.8
92.0
70.6
74.4

Asym
Proj
91.7
91.7
97.4
97.3
95.7
95.7
90.3
90.3
82.4
83.9

E[D]

(ours)

Graph Attention
93.8 ± 0.13
93.8 ± 0.05
99.4 ± 0.10
99.5 ± 0.03
97.9 ± 0.21
98.1 ± 0.49
93.6 ± 0.06
93.9 ± 0.05
89.8 ± 1.05
91.0 ± 0.28

Error

Reduction

25.2%
25.2%
33.3%
28.6%
19.2%
24.0%
22.0%
23.8%
43.5%
44.2%

C = 2
64.4
63.7
99.1
99.3
97.4
97.7
90.6
90.1
79.7
81.8

Table 1: Results on Link Prediction Datasets. Shown is the ROC-AUC. Each row shows results for
one dataset results on one dataset when training embedding with We bold the highest accuracy per
dataset-dimension pair  including when the highest accuracy intersects with the mean ± standard
deviation. We use the train:test splits of [2]  hosted on http://sami.haija.org/graph/splits

3.5 Computational Complexity
The naive computation of (T )k requires k matrix multiplications and so is O(|V |3k). However  as
most real-world adjacency matrices have an inherent low rank structure  a number of fast approxima-
tions to computing the random walk transition matrix raised to a power k have been proposed [e.g.
34]. Alternatively SVD can decompose T as T = UΛV(cid:62) and then the kth power can be calculated
by raising the diagonal matrix of singular values to k as (T )k = U(Λ)kV(cid:62) since V(cid:62)U = I. Further-
more  the SVD can be approximated in time linear to the number of non-zero entries [16]. Therefore 
we can approximate (T )k in O(|E|). In this work  we compute (T )k without approximations. Our
algorithm runs in seconds over the given datasets (at least 10X faster than node2vec [15]  DVNE [? ] 
DNGR [8]). We leave stochastic and approximation versions of our method as future work.

3.6 Extensions

As presented  our proposed method can learn the weights of the context distribution Q. However 
we brieﬂy note that such a model can be trivially extended to learn the weight of any other type of
pair-wise node similarity (e.g. Personalized PageRank  Adamic-Adar  etc). In order to do this  we
can extend the deﬁnition of the context Q with an additional dimension Qk+1 for the new type of
similarity  and an additional element in the softmax qk+1 to learn a joint importance function.

4 Experiments

4.1 Link Prediction Experiments

We evaluate the quality of embeddings produced when random walks are augmented with attention 
through experiments on link prediction [23]. Link prediction is a challenging task  with many real
world applications in information retrieval  recommendation systems and social networks. As such 
it has been used to study the properties of graph embeddings [29  15]. Such an intrinsic evaluation
emphasizes the structure-preserving properties of embedding.
Our experimental setup is designed to determine how well the embeddings produced by a method
captures the topology of the graph. We measure this in the manner of [15]: remove a fraction (=50%)
of graph edges  learn embeddings from the remaining edges  and measure how well the embeddings
can recover those edges which have been removed. More formally  we split the graph edges E into
two partitions of equal size Etrain and Etest such that the training graph is connected. We also sample
non existent edges ((u  v) /∈ E) to make E−
train) for training and model
selection  and use (Etest  E−

test. We use (Etrain  E−

test) to compute evaluation metrics.

train and E−

6

Dataset
Cora
Citeseer

n2v
C = 5
63.1
45.6

Graph Attention

(ours)
67.9
51.5

(a) node2vec  Cora

(b) Graph Attention (ours)  Cora

(c) Classiﬁcation accuracy

Figure 3: Node Classiﬁcation. Fig. (a)/(b): t-SNE visualization of node embeddings for Cora dataset.
We note that both methods are unsupervised  and we have colored the learned representations by
node labels. Fig. (c) However  quantitatively  our embeddings achieves better separation.

Training: We train our models using TensorFlow  with PercentDelta optimizer [1]. For the results
Table 1  we use β = 0.5  C = 10  and ˜P(0) = diag(80)  which corresponds to 80 walks per node.
We analyze our model’s sensitivity in Section 4.2. To ensure repeatability of results  we have released
our model and instructions6.
Datasets: Table 1a describes the datasets used in our experiments. Datasets available from SNAP
https://snap.stanford.edu/data.
Baselines: We evaluate against many baselines. For all methods  we calculate g(Y) ∈ R|V |×|V | 
and extract entries from g(Y) corresponding to positive and negative test edges  then use them to
compute ROC AUC. We compare against following baselines. We mark symmetric models with
†. Their counterparts  asymmetric models including ours  can learn g(Y)vu (cid:54)= g(Y)uv  which we
expect to perform relatively better on the directed graph wiki-vote.
– †EigenMaps [5]. Minimizes Euclidean distance of adjacent nodes of A.
– SVD. Singular value decomposition of A. Inference is through the function g(Y ) = Ud×(Λ)d×Vd 
where (Ud  Λd Vd) is a low-rank SVD decomposiiton corresponding to the d largest singular values.
– †DNGR [8]. Non-linear (i.e. deep) embedding of nodes  using an auto-encoder on A. We use
author’s code to learn the deep embeddings Y and use for inference g(Y) = YYT .
– †n2v: node2vec [15] is a popular baseline. It simulates random walks and uses word2vec to
learn node embeddings. Minimizes objective in Eq. (2). For Table 1  we use author’s code to learn
embeddings Y then use g(Y) = YY(cid:62). We run with C = 2 and C = 5.7
– AsymProj [2]. Learns edges as asymmetric projections in a deep embedding space  trained by
maximizing the graph likelihood (Eq. 3).
Results: Our results  summarized in Table 1  show that our proposed methods substantially out-
perform all baseline methods. Speciﬁcally  we see that the error is reduced by up to 45% over
baseline methods which have ﬁxed context deﬁnitions. This shows that by parameterizing the context
distribution and allowing each graph to learn its own distribution  we can better preserve the graph
structure (and thereby better predict missing edges).
Discussion: Figure 2a shows how the learned attention weights Q vary across datasets. Each dataset
learns its own attention form  and the highest weights generally correspond to the highest weights
when doing a grid search over C for node2vec (as in Figure 1b).
The hyper-parameter C determines the highest power of the transition matrix  and hence the maximum
context size available to the attention model. We suggest using large values for C  since the attention
weights can effectively use a subset of the transition matrix powers. For example  if a network needs
only 2 hops to be accurately represented  then it is possible for the softmax attention model to learn
Q3  Q4 ··· ≈ 0. Figure 2b shows how varying the regularization term β allows the softmax attention
model to “attend to” only what each dataset requires. We observe that for most graphs  the majority
of the mass gets assigned to Q1  Q2. This shows that shorter walks are more beneﬁcial for most
graphs. However  on wiki-vote  better embeddings are produced by paying attention to longer walks 
as its softmax Q is uniform-like  with a slight right-skew.

6Available at http://sami.haija.org/graph/context
7We sweep C in Figure 1b  showing that there are no good default for C that works best across datasets.

7

Figure 4: Sensitivity Analysis of softmax attention model. Our method is robust to choices of both β
and C. We note that it consistently outperforms even an optimally set node2vec.

4.2 Sensitivity Analysis

So far  we have removed two hyper-parameters  the maximum window size C  and the form of the
context distribution U. In exchange  we have introduced other hyper-parameters – speciﬁcally walk
length (also C) and a regularization term β for the softmax attention model. Nonetheless  we show
that our method is robust to various choices of these two. Figures 2a and 2b both show that the
softmax attention weights drop to almost zero if the graph can be preserved using shorter walks 
which is not possible with ﬁxed-form distributions (e.g. U).
Figure 4 examines this relationship in more detail for d = 128 dimensional embeddings  sweeping
our hyper-parameters C and β  and comparing results to the best and worst node2vec embeddings for
C ∈ [1  10]. (Note that node2vec lines are horizontal  as they do not depend on β.) We observe that
all the accuracy metrics are within 1% to 2%  when varying these hyper-parameters  and are all still
well-above our baseline (which sample from a ﬁxed-form context distribution).

4.3 Node Classiﬁcation Experiments

We conduct node classiﬁcation experiments  on two citation datasets  Cora and Citeseer  with the
following statistics: Cora contains (2  708 nodes  5  429 edges and K = 7 classes); and Citeseer
contains (3  327 nodes  4  732 edges and K = 6 classes). We learn embeddings from only the
graph structure (nodes and edges)  without observing node features nor labels during training.
Figure 3 shows t-SNE visualization of the Cora dataset  comparing our method with node2vec

[15]. For classiﬁcation  we follow the data splits of [37]. We predict labels (cid:101)L ∈ R|V |×K as:
(cid:101)L = exp (αg(Y)) × Ltrain   where Ltrain ∈ {0  1}|V |×K contains rows of ones corresponding to

nodes in training set and zeros elsewhere. The scalar α ∈ R is manually tuned on the validation set.
The classiﬁcation results  summarized in Table 3c  show that our model learns a better unsupervised
representation than previous methods  that can then be used for supervised tasks. We do not compare
against other semi-supervised methods that utilize node features during training and inference [incl.
37  20]  as our method is unsupervised.
Our classiﬁcation prediciton function contains one scalar parameter α. It can be thought of a “smooth”
k-nearest-neighbors  as it takes a weighted average of known labels  where the weights are exponential
of the dot-product similarity. Such a simple function should introduce no model bias.

5 Related Work

The ﬁeld of learning on graphs has attracted much attention lately. Here we summarize two broad
classes of algorithms  and point the reader to recent reviews [10  6  18  14] for more context.
The ﬁrst class of algorithms are semi-supervised and concerned with predicting labels over a graph 
its edges  and/or its nodes. Typically  these algorithms process a graph (nodes and edges) as well as
per-node features. These include recent graph convolution methods [e.g. 27  7  3  17] with spectral
variants [12  7]  diffusion methods [e.g. 11  13]  including ones trained until ﬁxed-point convergence
[32  22] and semi-supervised node classiﬁcation [37] with low-rank approximation of convolution
[12  20]. We differ from these methods as (1) our algorithm is unsupervised (trained exclusively from
the graph structure itself) without utilizing labels during training  and (2) we explicitly model the
relationship between all node pairs.

8

0.30.50.70.90.9650.9700.9750.9800.985ROC-AUCca-AstroPh0.30.50.70.90.900.910.920.930.94ca-HepTh0.30.50.70.90.700.750.800.850.90PPI0.30.50.70.90.600.650.700.750.800.850.900.95wiki-votesoftmax[C=5]softmax[C=10]softmax[C=20]softmax[C=30]node2vec[best C]node2vec[worst C]The second class of algorithms consist of unsupervised graph embedding methods. Their primary
goal is to preserve the graph structure  to create task independent representations. They explicitly
model the relationship of all node pairs (e.g. as dot product of node embeddings). Some methods
directly use the adjacency matrix [8  36]  and others incorporate higher order structure (e.g. from
simulated random walks) [29  15  2]. Our work falls under this class of algorithms  where inference
is a scoring function V × V → R  trained to score positive edges higher than negative ones. Unlike
existing methods  we do not specify a ﬁxed context distribution apriori  whereas we push gradients
through the random walk to those parameters  which we jointly train while learning the embeddings.

6 Conclusion

In this paper  we propose an attention mechanism for learning the context distribution used in graph
embedding methods. We derive the closed-form expectation of the DeepWalk [29] co-occurrence
statistics  showing an equivalence between the context distribution hyper-parameters  and the co-
efﬁcients of the power series of the graph transition matrix. Then  we propose to replace the context
hyper-parameters with trainable models  that we learn jointly with the embeddings on an objective
that preserves the graph structure (the Negative Log Graph Likelihood  NLGL). Speciﬁcally  we
propose Graph Attention Models  using a softmax to learn a free-form contexts distribution with a
parameter for each type of context similarity (e.g. distance in a random walk).
We show signiﬁcant improvements on link prediction and node classiﬁcation over state-of-the-
art baselines (that use a ﬁxed-form context distribution)  reducing error on link prediction and
classiﬁcation  respectively by up to 40% and 10%. In addition to improved performance (by learning
distributions of arbitrary forms)  our method can obviate the manual grid search over hyper-parameters:
walk length and form of context distribution  which can drastically ﬂuctuate the quality of the learned
embeddings and are different for every graph. On the datasets we consider  we show that our method
is robust to its hyper-parameters  as described in Section 4.2. Our visualizations of converged attention
weights convey to us that some graphs (e.g. voting graphs) can be better preserved by using longer
walks  while other graphs (e.g. protein-protein interaction graphs) contain more information in short
dependencies and require shorter walks.
We believe that our contribution in replacing these sampling hyperparameters with a learnable
context distribution is general and can be applied to many domains and modeling techniques in graph
representation learning.

References
[1] S. Abu-El-Haija. Proportionate gradient updates with percentdelta. In arXiv  2017.

[2] S. Abu-El-Haija  B. Perozzi  and R. Al-Rfou. Learning edge representations via low-rank asymmetric
projections. In ACM International Conference on Information and Knowledge Management (CIKM)  2017.

[3] J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural Information

Processing Systems (NIPS)  2016.

[4] D. Bahdanau  K. Cho  and Y. Bengio. Neural machine translation by jointly learning to align and translate.

In International Conference on Learning Representations (ICLR)  2015.

[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. In

Neural Computation  2003.

[6] M. M. Bronstein  J. Bruna  Y. LeCun  A. Szlam  and P. Vandergheynst. Geometric deep learning: going

beyond euclidean data. In IEEE Signal Processing Magazine  2017.

[7] J. Bruna  W. Zaremba  A. Szlam  and Y. LeCun. Spectral networks and deep locally connected networks

on graphs. In International Conference on Learning Representations  2013.

[8] S. Cao  W. Lu  and Q. Xu. Deep neural networks for learning graph representations. In Association for the

Advancement of Artiﬁcial Intelligence  2016.

[9] H. Chen  B. Perozzi  R. Al-Rfou  and S. Skiena. A tutorial on network embeddings. arXiv preprint

arXiv:1808.02590  2018.

9

[10] H. Chen  B. Perozzi  Y. Hu  and S. Skiena. Harp: hierarchical representation learning for networks. The

32nd AAAI Conference on Artiﬁcial Intelligence  2018.

[11] H. Dai  B. Dai  and L. Song. Discriminative embeddings of latent variable models for structured data. In

International Conference on Machine Learning  2016.

[12] M. Defferrard  X. Bresson  and P. Vandergheynst. Convolutional neural networks on graphs with fast

localized spectral ﬁltering. In Advances in Neural Information Processing Systems (NIPS)  2016.

[13] D. Duvenaud  D. Maclaurin  J. Iparraguirre  R. Bombarell  T. Hirzel  A. Aspuru-Guzik  and R. Adams.
Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in Neural Information
Processing Systems (NIPS)  2015.

[14] P. Goyal and E. Ferrara. Graph embedding techniques  applications  and performance: A survey. In

Knowledge-Based Systems  2018.

[15] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In International Conference

on Knowledge Discovery and Data Mining  2016.

[16] N. Halko  P. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for

constructing approximate matrix decompositions. In SIAM Review  2011.

[17] W. Hamilton  R. Ying  and J. Leskovec. Inductive representation learning on large graphs. In NIPS  2017.

[18] W. L. Hamilton  R. Ying  and J. Leskovec. Representation learning on graphs: Methods and applications.

In IEEE Data Engineering Bulletin  2017.

[19] G. J  S. Ganguly  M. Gupta  V. Varma  and V. Pudi. Author2vec: Learning author representations by
combining content and link information. In International Conference Companion on World Wide Web
(WWW)  WWW ’16 Companion  2016.

[20] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.

International Conference on Learning Representations (ICLR)  2017.

In

[21] O. Levy  Y. Goldberg  and I. Dagan. Improving distributional similarity with lessons learned from word

embeddings. In TACL  2015.

[22] Y. Li  D. Tarlow  M. Brockschmidt  and R. Zemel. Gated graph sequence neural networks. In International

Conference on Learning Representations  2016.

[23] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. In Journal of American

Society for Information Science and Technology  2007.

[24] Y. Luo  Q. Wang  B. Wang  and L. Guo. Context-dependent knowledge graph embedding. In Conference

on Emperical Methods in Natural Language Processing (EMNLP)  2015.

[25] T. Mikolov  I. Sutskever  K. Chen  G. Corrado  and J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in Neural Information Processing Systems NIPS. 2013.

[26] V. Mnih  N. Heess  A. Graves  and k. kavukcuoglu. Recurrent models of visual attention. In Advances in

Neural Information Processing Systems (NIPS). 2014.

[27] M. Niepert  M. Ahmed  and K. Kutzkov. Learning convolutional neural networks for graphs. In Interna-

tional Conference on Machine Learning (ICML)  2016.

[28] J. Pennington  R. Socher  and C. D. Manning. Glove: Global vectors for word representation. In Conference

on Empirical Methods in Natural Language Processing  EMNLP  2014.

[29] B. Perozzi  R. Al-Rfou  and S. Skiena. Deepwalk: Online learning of social representations. In Knowledge

Discovery and Data Mining (KDD)  2014.

[30] B. Perozzi  V. Kulkarni  H. Chen  and S. Skiena. Don’t walk  skip!: Online learning of multi-scale network

embeddings. In Advances in Social Networks Analysis and Mining (ASONAM)  2017.

[31] V. Ramanathan  J. Huang  S. Abu-El-Haija  A. Gorban  K. Murphy  and L. Fei-Fei. Detecting events
and key actors in multi-person videos. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  2016.

[32] F. Scarselli  M. Gori  A. Tsoi  M. Hagenbuchner  and G. Monfardini. The graph neural network model. In

IEEE Trans. on Neural Networks  2009.

10

[33] C. Stark  B. Breitkreutz  T. Reguly  L. Boucher  A. Breitkreutz  and M. Tyers. Biogrid: A general repository

for interaction datasets. In Nucleic Acids Research  2006.

[34] H. Tong  C. Faloutsos  and J.-Y. Pan. Fast random walk with restart and its applications. In International

Conference on Data Mining (ICDM). IEEE  2006.

[35] P. Veliˇckovi´c  G. Cucurull  A. Casanova  A. Romero  P. Liò  and Y. Bengio. Graph attention networks. In

International Conference on Learning Representations (ICLR)  2018.

[36] D. Wang  P. Cui  and W. Zhu. Structural deep network embedding. In International Conference on

Knowledge Discovery and Data Mining  2016.

[37] Z. Yang  W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. In

International Conference on Machine Learning (ICML)  2016.

[38] Z. Yang  D. Yang  C. Dyer  X. He  A. Smola  and E. Hovy. Hierarchical attention networks for document
In Conference of the North American Chapter of the Association for Computational

classiﬁcation.
Linguistics (NAACL)  2016.

11

,Sami Abu-El-Haija
Bryan Perozzi
Rami Al-Rfou
Alexander Alemi
Wasim Huleihel
Arya Mazumdar
Muriel Medard
Soumyabrata Pal