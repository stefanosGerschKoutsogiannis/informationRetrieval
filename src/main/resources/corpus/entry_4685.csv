2018,Removing the Feature Correlation Effect of Multiplicative Noise,Multiplicative noise  including dropout  is widely used to regularize deep neural networks (DNNs)  and is shown to be effective in a wide range of architectures and tasks. From an information perspective  we consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways  which leads to the observation that multiplicative noise tends to increase the correlation between features  so as to increase the signal-to-noise ratio of information pathways. However  high feature correlation is undesirable  as it increases redundancy in representations. In this work  we propose non-correlating multiplicative noise (NCMN)  which exploits batch normalization to remove the correlation effect in a simple yet effective way. We show that NCMN significantly improves the performance of standard multiplicative noise on image classification tasks  providing a better alternative to dropout for batch-normalized networks. Additionally  we present a unified view of NCMN and shake-shake regularization  which explains the performance gain of the latter.,Removing the Feature Correlation Effect of

Multiplicative Noise

Zijun Zhang

University of Calgary

zijun.zhang@ucalgary.ca

yining.zhang1@ucalgary.ca

Yining Zhang

University of Calgary

Zongpeng Li

Wuhan University

zongpeng@whu.edu.cn

Abstract

Multiplicative noise  including dropout  is widely used to regularize deep neural
networks (DNNs)  and is shown to be effective in a wide range of architectures
and tasks. From an information perspective  we consider injecting multiplicative
noise into a DNN as training the network to solve the task with noisy information
pathways  which leads to the observation that multiplicative noise tends to increase
the correlation between features  so as to increase the signal-to-noise ratio of
information pathways. However  high feature correlation is undesirable  as it
increases redundancy in representations. In this work  we propose non-correlating
multiplicative noise (NCMN)  which exploits batch normalization to remove the
correlation effect in a simple yet effective way. We show that NCMN signiﬁcantly
improves the performance of standard multiplicative noise on image classiﬁcation
tasks  providing a better alternative to dropout for batch-normalized networks.
Additionally  we present a uniﬁed view of NCMN and shake-shake regularization 
which explains the performance gain of the latter.

1

Introduction

State-of-the-art deep neural networks are often over-parameterized to deliver more expressive power.
For instance  a typical convolutional neural network (CNN) for image classiﬁcation can consist of
tens to hundreds of layers  and millions to tens of millions of learnable parameters [1  2]. To combat
overﬁtting  a variety of regularization techniques have been developed. Examples include dropout [3] 
DropConnect [4]  and the recently proposed shake-shake regularization [5]. Among them  dropout is
arguably the most popular  due to its simplicity and effectiveness in a wide range of architectures and
tasks  e.g.  convolutional neural networks (CNNs) for image recognition [6]  and recurrent neural
networks (RNNs) for natural language processing (NLP) [7].
Nonetheless  we observe a side effect of dropout that has long been ignored. That is  it tends to increase
the correlation between the features it is applied to  which reduces the efﬁciency of representations.
It is also known that decorrelated features can lead to better generalization [8  9  10  11]. Thus  this
side effect may counteract  to some extent  the regularization effect of dropout.
In this work  we demonstrate the feature correlation effect of dropout  as well as other types of
multiplicative noise  through analysis and experiments. Our analysis is based on a simple assumption
that  in order to reduce the interference of noise  the training process will try to maximize the
signal-to-noise ratio (SNR) of representations. We show that the tendency of increasing the SNR
will increase feature correlation as a result. To remove the correlation effect  it is possible to resort to
feature decorrelation techniques. However  existing techniques penalize high correlation explicitly;
they either introduce a substantial computational overhead [10]  or yield marginal improvements
[11]. Moreover  these techniques require extra hyperparameters to control the strength of the penalty 
which further hinders their practical application.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

We propose a simple yet effective approach to solve this problem. Speciﬁcally  we ﬁrst decompose
noisy features into the sum of two components  a signal component and a noise component  and then
truncate the gradient through the latter  i.e.  treat it as a constant. However  naively modifying the
gradient would encourage the magnitude of features to grow in order to increase the SNR  causing
optimization difﬁculties. We solve this problem by combining the aforementioned technique with
batch normalization [12]  which effectively counteracts the tendency of increasing feature magnitude.
The resulting method  non-correlating multiplicative noise (NCMN)  is able to reduce the correlation
between features  reaching a level even lower than that without multiplicative noise. More importantly 
it signiﬁcantly improves the performance of standard multiplicative noise on image classiﬁcation
tasks.
As another contribution of this work  we further investigate the connection between NCMN and
shake-shake regularization. Despite its impressive performance  how shake-shake works remains
elusive. We show that the noise produced by shake-shake has a similar form to that of a NCMN
variant  and both NCMN and shake-shake achieve superior generalization performance by avoiding
feature correlation.
The rest of this paper is organized as follows. In Section 2  we deﬁne a general form of multiplicative
noise  and identify the feature correlation effect. In Section 3  we ﬁrst propose NCMN  which we show
through analysis is able to remove the feature correlation effect; then we develop multiple variants
of NCMN  and provide a uniﬁed view of NCMN and shake-shake regularization. In Section 4  we
provide empirical evidence of our analysis  and evaluate the performance of the proposed methods.1

2 Motivation

2.1 Multiplicative Noise

˜xi = uixi ∀i ∈ Hl.

Let xi be the activation of hidden unit i  we consider multiplicative noise  ui  which is applied to the
activations of layer l as
(1)
Here  Hl represents the set of hidden units in layer l. For simplicity  we restrict our analysis to
fully-connected layers without batch normalization  and will later extend it to batch-normalized layers
and convolutional layers. When used for regularization purpose  multiplicative noise is typically
applied at training time  and removed at test time. Consequently  the noise should satisfy E [ui] = 1 
such that E [˜xi] = xi.
The noise mask  ui  can be sampled from various distributions  as exempliﬁed by Bernoulli  Gaussian 
and uniform distributions. We take dropout and DropConnect as examples. For dropout  let mi be
the dropout mask sampled from a Bernoulli distribution  Bern (p)  then the equivalent multiplicative
noise is given by ui = mi/p. DropConnect is slightly different from dropout  in that the dropout
mask is independently sampled for each weight instead of each activation. Thus  we denote the
dropout mask by mij  where j ∈ Hl+1  then we have uij = mij/p and

˜xij = uijxi ∀i ∈ Hl  j ∈ Hl+1.

(2)
Comparing Eq. (2) with Eq. (1)  we observe that applying multiplicative noise to weights is equivalent
to applying it to activations  except that the noise mask is independently sampled for each hidden unit
in the upper layer. Therefore  without loss of generality  we consider multiplicative noise of the form
in Eq. (1) in the following discussion.
Compared to other types of noise  such as additive isotropic noise  multiplicative noise can adapt the
scale of noise to the scale of features  which may contribute to its empirical success.

2.2 The Feature Correlation Effect

As a regularization technique  dropout  and other multiplicative noise  improve generalization by
preventing feature co-adaptation [3]. From an information perspective  injecting noise into a neural
network can be seen as training the model to solve the task with noisy information pathways. To
better solve the task  a simple and natural strategy that can be learned is to increase the signal-to-noise
ratio (SNR) of the information pathways.

1Code is available at https://github.com/zj10/NCMN.

2

Concretely  the noisy activations of layer l is aggregated by a weighted sum to form the pre-activations
(without biases) of layer l + 1 as

zj =

wij ˜xi ∀j ∈ Hl+1 

(3)

(cid:88)

i∈Hl

(cid:88)

j

(cid:3)(cid:1)2(cid:105)

E(cid:104)(cid:0)zs
j − E(cid:2)zs
E(cid:104)(cid:0)zn
(cid:1)2(cid:105)

j

j

where wij is the weight between unit i in layer l and unit j in layer l + 1. Although we cannot
increase the SNR of ˜xi due to the multiplicative nature of the noise  it is possible to increase the SNR
of zj instead. In the following  we omit the range of summation when it is over Hl.
We now focus on the pre-activation  zj  of an arbitrary unit in layer l + 1  and deﬁne its signal and
noise components  respectively  as

wijxi  and zn
j =
where vi = ui − 1  such that E [vi] = 0  and zj = zs
increasing the SNR of information pathways by the following implicit objective function:

j + zn

wijvixi 

j . Then we can model the tendency of

zs
j =

(4)

i

i

(cid:88)

maximize

SNR (zj) =

.

(5)

Here  the expectations are taken with respect to both xi and vi  ∀i ∈ Hl. Note that in Eq. (5)  we
input samples. Let σ2 = Var [vi]  ∀i ∈ Hl  we have
(cid:80)
E(cid:2)(cid:80)

(cid:3)  from the signal  since it does not capture the variation of
2 E(cid:104)(cid:80)

subtract the constant component  E(cid:2)zs
1 +
2 E(cid:104)(cid:80)

Thus  the objective function in Eq. (5) is equivalent to the following:

i (wijxi)2(cid:3)
(cid:105)
E(cid:2)zs
(cid:3)2
i (wijxi)2(cid:3) .
E(cid:2)(cid:80)

(cid:80)
E(cid:2)(cid:80)
i (wijxi)2(cid:3)

(cid:105) − E(cid:2)zs

i (wijxi) (wi(cid:48)jxi(cid:48) )

i (wijxi) (wi(cid:48)jxi(cid:48) )

 .

SNR (zj) =

maximize

(cid:3)2

1
σ2

i(cid:48)(cid:54)=i

i(cid:48)(cid:54)=i

(6)

(7)

−

j

j

Intuitively  the ﬁrst term in Eq. (7) tries to maximize the correlation between each pair of wijxi
and wi(cid:48)jxi  where i (cid:54)= i(cid:48). Although it is not the same as the commonly used Pearson correlation
coefﬁcient  it can be regarded as a generalization of the latter to multiple variables. Since wij can
be either positive or negative  maximizing the correlations between wijxi’s essentially increases the
magnitudes of the correlations between xi’s  and hence causing the feature correlation effect. The

second term in Eq. (7) penalizes non-zero values of E(cid:2)zs
(cid:3)  and does not affect feature correlations.
numerator and denominator of Eq. (5) will be divided by the same factor (cid:112)Var [zj]. Therefore 

For batch-normalized layers  if batch normalization is applied to zj (as a common practice)  the

Eq. (5) remains the same  and the analysis still holds.
For convolutional layers  one should consider Hl+1 as the set of convolutional kernels or feature maps
in layer l + 1  and Hl as the set of input activations at each spatial location. Accordingly  the inputs
at different spatial locations are considered as different input samples. Since adjacent activations in
the same feature map are often highly correlated  sharing the same noise mask across different spatial
locations is shown to be more effective [13]. In this setting  the feature correlation effect tends to be
more prominent between activations in different feature maps than that in the same feature map.

j

3 Methods

3.1 Non-Correlating Multiplicative Noise

A high correlation between features increases redundancy in neural representations  and can thus
reduce the expressive power of neural networks. To remove this effect  one can directly penalize
the correlation between features as part of the objective function [10]  which  however  introduces
a substantial computational overhead. Alternatively  one can penalize the correlation between the
weight vectors (or convolutional kernels) of different hidden units [11]  which is less computationally

3

(cid:1)2(cid:105)

expensive  but yields only marginal improvements. Moreover  both approaches require manually-
tuned penalty strength. A more desirable approach is to simply avoid feature correlation in the ﬁrst
place  rather than counteracting it with other regularization techniques.

From Eq. (5) we observe that  if we consider the denominator E(cid:104)(cid:0)zn
(cid:3)(cid:1)2(cid:105)

gradient during training  the objective function is equivalent to

maximize E(cid:104)(cid:0)zs

j − E(cid:2)zs

(8)
j   the tendency of increasing
Eq. (8) implies that if we ignore the gradient of the noise component  zn
j   instead of increasing
the SNR of zj will attempt to increase the variance of the signal component  zs
the feature correlation of the lower layer. However  we ﬁnd in practice that such modiﬁcation to the
gradient causes optimization difﬁculties  preventing the training process from converging. Fortunately 
by using batch normalization  the remedy for this problem is surprisingly simple.
Concretely  we apply batch normalization to zj as

as a constant  i.e.  ignore its

j

.

j

ˆzj = BN(cid:0)zj; zs

j

(cid:1) =

zj − E(cid:2)zs
(cid:3)
(cid:113)
(cid:3) .
Var(cid:2)zs

j

j

(9)

(10)

(cid:3) .

We neglect the small difference between the true mean/variance  and the sample mean/variance  and
adopt the same notation for simplicity. Note that in Eq. (9)  zj is normalized using the statistics of zs
j  
which is slightly different from standard batch normalization. We now consider the SNR of the new
pre-activation  ˆzj. The signal and noise components of ˆzj are respectively

j

j

zs

ˆzs
j =

(cid:3)
(cid:3)   and ˆzn

j − E(cid:2)zs
(cid:113)
Var(cid:2)zs
(cid:1) = BN(cid:0)zs
j + AsConst(cid:0)ˆzn
maximize E(cid:104)(cid:0)ˆzs

zn

j =

j = ˆzj − ˆzs

j(cid:113)
Var(cid:2)zs
(cid:1) + AsConst(cid:0)BN(cid:0)zj; zs

j

j

j

j

j

j

.

j

j

(11)

/∂zs

(cid:1)2(cid:105)

(cid:1)(cid:1)  

(cid:48)
j = ˆzs
ˆz

(cid:1)2(cid:105)

(cid:1) is

equivalent to

(cid:1) − BN(cid:0)zs

For clarity  we deﬁne an identity function  AsConst (·)  meaning that the argument of the function is
considered as a constant during the backpropagation phase  or in other words  its gradient is set to
zero by the function. We then substitute ˆzj with

such that the noise component is considered as a constant. Therefore  maximizing SNR(cid:0)ˆz(cid:48)
thus we have ∂ E(cid:104)(cid:0)ˆzs

(12)
j   and
j (m) = 0 ∀m ∈ B  where B denotes a set of mini-batch samples 
and zs
j corresponding to sample m. Therefore  we can now remove the
feature correlation effect of multiplicative noise without causing optimization difﬁculties. We refer to
this approach as non-correlating multiplicative noise (NCMN).
We also note that the non-standard batch normalization used in Eq. (9) is not necessary in practice.
To take advantage of existing optimized implementations of batch normalization  we can modify
Eq. (11) as follows:

Due to the use of batch normalization  Eq. (12) is a constant with respect to each sample of zs

j (m) denotes the value of zs

(cid:1) + AsConst(cid:0)BN (zj) − BN(cid:0)zs

(13)
In this case  to keep the forward pass consistent between training and testing  the running mean and
variance should be calculated based on zj  rather than zs
j .
Interestingly  Eq. (11) and Eq. (13) can be seen as adding a noise component to batch-normalized zs
j  
where the noise is generated in a special way and passed through the AsConst (·) function. However 
the analysis in this section does not depend on the particular choice of the noise  as long as it is
considered as a constant. This observation leads to a uniﬁed view of multiple variants of NCMN and
shake-shake regularization  as discussed in the following section.

j = BN(cid:0)zs

(cid:1)(cid:1) .

(cid:48)
ˆz

j

j

3.2 A Uniﬁed View of NCMN and Shake-shake Regularization

the lower layer activations. Assuming the independence of vi’s  we have E(cid:2)ˆzn

In Eq. (11)  the noise component  ˆzn

j   is generated indirectly from the multiplicative noise applied to

Var(cid:2)ˆzn

j

(cid:3) =

Var(cid:2)zs

σ2

j

(cid:3)(cid:88)

i

(cid:3) = 0  and
  

j

(cid:0)zs

j

(cid:1)2 − 2

(cid:88)

(cid:88)

i(cid:48)(cid:54)=i

i

Var(cid:2)zs

σ2

j

(cid:3)

4

(wijxi)2 =

(wijxi) (wi(cid:48)jxi(cid:48) )

(14)

j + AsConst(cid:0)vj ˆzs

(cid:1) .

(cid:48)
j = ˆzs
ˆz

which implies that ˆzn
j   if we neglect
the correlation term in Eq. (14)  and only the mean and variance of the noise are of interest. We can
further simplify the approximation by applying multiplicative noise directly to ˆzs

j can be approximated by a noise multiplied by zs

j but applied to ˆzs

j as

j

(15)
The advantage of this variant is that  while Eq. (11) and Eq. (13) require an extra forward pass for
each layer  Eq. (15) introduces no computational overhead  and is straightforward to implement. A
similar idea was explored for fast dropout training [14].
To indicate the number of layers involved in noise generation  we refer to this variant (Eq. (15)) as
NCMN-0  and the original form (Eq. (13)) as NCMN-1. We next generalize NCMN from NCMN-1
to NCMN-2  and demonstrate its connection to shake-shake regularization [5]. For convenience  we
deﬁne the following two functions to indicate the pre-activations and activations of a batch-normalized
layer:

(cid:18)(cid:16)
where xl = [xi]  Wl+1 = [wij]  i ∈ Hl  j ∈ Hl+1  and

Ψl+1(cid:16)
xl(cid:17)
xl(cid:17)
Φl+1(cid:16)

xl(cid:17)T
γl+1 (cid:12) Ψl+1(cid:16)
xl(cid:17)

= BN

(cid:16)

= φ

 

(cid:19)
+ βl+1(cid:17)

(16)

(17)

 

Wl+1

where γl+1 and βl+1 denote  respectively  the scaling and shifting vectors of batch normalization 
(cid:12) denotes element-wise multiplication  and φ (·) denotes the activation function. Let Ψl
i (·) be an
element of Ψl (·)  the noise component of NCMN-1 is then given by

(cid:16)

ul (cid:12) xl(cid:17) − Ψl+1

ˆzn
j = Ψl+1

j

where ul = [ui]   i ∈ Hl  and j ∈ Hl+1.
We then deﬁne a natural generalization from NCMN-1 to NCMN-2 as

Φl+1(cid:16)
(cid:16)
xl(cid:17)(cid:17)

ˆzs
k = Ψl+2

k

  and ˆzn

k = Ψl+2

k

j

 

(cid:16)

xl(cid:17)
ul+1 (cid:12) Φl+1(cid:16)

(cid:16)

ul (cid:12) xl(cid:17)(cid:17) − ˆzs

k 

(18)

(19)

k )  

(cid:48)
k = ˆzs
ˆz

k + AsConst (ˆzn

and
(20)
where k ∈ Hl+2. Different from NCMN-0 and NCMN-1  which can be applied to every layer 
NCMN-2 can only be applied once every two layers. For residual networks (ResNets) in particular 
NCMN-2 should be aligned with residual blocks  such that γl+2 ˆz(cid:48)
the residual of a residual block  depending on which variant is used [15].
Interestingly  we can formulate shake-shake regularization in a similar way to NCMN-2. Shake-shake
regularization is a regularization technique developed speciﬁcally for ResNets with two residual
branches  as opposed to only one of standard ResNets. It works by averaging the outputs of two
residual branches with random weights. For the forward pass at training time  one of the weights 
α1  is sampled uniformly from [0  1]  while the other one is set to α2 = 1 − α1. For the backward
pass  the weights are either sampled in the same way as the forward pass  or set to the same value 
2 = 1/2. We ﬁrst consider the latter case. Let p ∈ {1  2} denote the index of residual
i.e  α(cid:48)
branches  then we have the signal component as

k + βl+2 or φ(cid:0)γl+2 ˆz(cid:48)

k + βl+2(cid:1) is

1 = α(cid:48)

ˆzs
k = (ˆz1 k + ˆz2 k) /2  where ˆzp k = Ψl+2
p k

Φl+1

p

 

(21)

and the noise component as
(22)
where v = α1 − 1/2 is uniformly sampled from [−1/2  1/2]. Accordingly  the noisy pre-activation
is also given by Eq. (20).
Similar to Eq. (5)  we can deﬁne the SNR of ˆz(cid:48)

k = v (ˆz1 k − ˆz2 k)  
ˆzn

(cid:34)

k as

E(cid:2)(ˆz1 k − ˆz2 k)2(cid:3)(cid:35)

E(cid:2)(ˆzs
k)2(cid:3)
(cid:1) =
E(cid:2)(ˆzn
k )2(cid:3) = 3
k) will encourage large E [ˆz1 k ˆz2 k] and small E(cid:104)

4 E [ˆz1 k ˆz2 k]

1 +

.

Apparently  maximizing SNR (ˆz(cid:48)
branch. However  if we keep the gradient of the noise component  or  equivalently  let α(cid:48)
α(cid:48)
2 = α2  then maximizing SNR (ˆz(cid:48)

k) does not affect the correlation between features from the same
1 = α1 and
 

(ˆz1 k − ˆz2 k)2(cid:105)

SNR(cid:0)ˆz

(cid:48)
k

(23)

5

(cid:16)

(cid:16)
xl(cid:17)(cid:17)

1 = α(cid:48)

leading to highly correlated branches. On the other hand  if we consider the noise component as a
constant  then only large E [ˆz1 k ˆz2 k] will be encouraged  which results in much weaker correlation 
and hence the better generalization performance observed in practice. For a similar reason to that
of NCMN  the batch normalization layers before Eq. (20) are crucial for avoiding optimization
difﬁculties.
It is worth noting that setting α(cid:48)
2 = 1/2 for the backward pass leads to exactly the same
gradient for ˆz1 k and ˆz2 k  which can also increase the correlation between the two branches. Thus 
sampling α(cid:48)
1 randomly further breaks the symmetry between the two branches  which may explain its
slightly better performance on large networks.
While the noise components of NCMN-2 and shake-shake are generated in different ways  they are
injected into the network in the same way (Eq. (20)). Therefore  we expect similar regularization
effects from NCMN-2 and shake-shake in practice. However  while adjusting the regularization
strength is difﬁcult for shake-shake  it can be easily done for NCMN by tuning the variance of
multiplicative noise. Moreover  NCMN is not restricted to ResNets with multiple residual branches 
and can be applied to various architectures.

4 Experiments

In our preliminary experiments we found that different choices of noise distributions  including
Bernoulli  Gaussian  and uniform distributions  lead to similar performance  which is consistent with
the experimental results for Gaussian dropout [3]. We use uniform noise with variable standard
deviation in the following experiments. For fair comparison  in contrast to previous work [5  16]  we
tune the hyperparameters (e.g.  learning rate  L2 weight decay  noise standard deviation) separately
for different types of noise  as well as for different datasets. We use ND-Adam [17] for optimization 
which is a variant of Adam [18] that has similar generalization performance to SGD  but is easier to
tune due to its decoupled learning rate and weight decay hyperparameters. See the supplementary
material for hyperparameter settings  and practical guidelines for tuning them.
We ﬁrst empirically verify the feature correlation effect of multiplicative noise  and the decorrelation
effect of NCMN. To avoid possible interactions with skip connections  we use plain CNNs rather
than more sophisticated architectures for this purpose. Due to the lack of skip connections  we use a
relatively shallow network in case of optimization difﬁculties. Speciﬁcally  we construct a CNN by
removing the skip connections from a wide residual network (WRN) [16]  WRN-16-10  which is a
16-layer ResNet with 10 times more ﬁlters per layer than the original. Accordingly  we refer to the
modiﬁed network as CNN-16-10. We train the network with different types of noise on the CIFAR-10
and CIFAR-100 datasets [19]. For each convolutional layer except the ﬁrst one  we calculate the
correlation between each pair of feature maps after batch normalization  and take the average of their
absolute values. The results are grouped by the size of feature maps  and are shown in Fig. 1. In
addition  the corresponding test error rates (average of 3 or more runs) are shown in Table 1.

(a) Results on CIFAR-10.

(b) Results on CIFAR-100.

Figure 1: Feature correlations of CNN-16-10 networks trained with different types of noise. None
refers to the baseline without noise injection  and MN refers to standard multiplicative noise. The
error bars represent the standard deviation across different layers in a single run  which varies little
across different runs.

6

0 9:702 5800 9:70.4770 943430


0 9:702 5800 9:70.4770 943430


Compared to the baseline  standard multiplicative noise exhibits slightly higher feature correlations
for both CIFAR-10 and CIFAR-100. By modifying the gradient as Eq. (13)  which results in NCMN-
1  the feature correlations are signiﬁcantly reduced as predicted by our analysis. Surprisingly  for
all sizes of feature maps  the feature correlations of NCMN-1 reach a level even lower than that
of the baseline. This intriguing result may indicate that the regularization effect of multiplicative
noise strongly encourages decorrelation between features  which  however  is counteracted by the
feature correlation effect. As a result  NCMN-1 signiﬁcantly improves the performance of standard
multiplicative noise  as shown in Table 1. As discussed in Section 3.2  NCMN-0 can be considered
as an approximation to NCMN-1  which is consistent with the fact that the feature correlations
and test errors of NCMN-0 are both close to that of NCMN-1. On the other hand  since NCMN-2
applies noise more sparsely (once every two layers)  it exhibits higher correlations and slightly worse
generalization performance than NCMN-0 and NCMN-1.

Table 1: CIFAR-10/100 error rates (%) of
CNN-16-10 networks trained with different
types of noise.
Noise type
None
MN
NCMN-0
NCMN-1
NCMN-2

CIFAR-100
19.22± 0.05
18.08± 0.03
17.37±0.05
17.55± 0.06
18.16± 0.04

CIFAR-10
4.05± 0.05
3.76± 0.00
3.51± 0.07
3.41±0.07
3.44± 0.03

Table 2: CIFAR-10/100 error rates (%) of
WRN-22-7.5 networks trained with different
types of noise.
Noise type
None
MN
NCMN-0
NCMN-1
NCMN-2

CIFAR-100
19.29± 0.07
18.60± 0.03
17.05± 0.08
17.09± 0.10
16.70±0.13

CIFAR-10
3.68± 0.02
3.59± 0.06
3.34± 0.02
3.02± 0.06
3.00±0.05

Next  we extend our experiments to ResNets  in order to investigate possible interactions between
skip connections and NCMN. Speciﬁcally  we test different types of noise on a WRN-22-7.5 network 
which has comparable performance to WRN-28-10  but is much faster to train. Since WRN is based
on the full pre-activation variant of ResNets [15]  NCMN-1  NCMN-2  and shake-shake require an
extra batch normalization layer after each residual branch. The feature correlation is calculated for
the output of each residual block  instead of for each layer.
As shown in Fig. 2  NCMN continues to lower the correlation between features  which is especially
prominent in the second and third stages of the network. In addition  as shown in Table 2  we
obtain even more performance gains from NCMN compared to the CNN-16-10 network. However  a
notable difference is that  while NCMN-1 works well with both architectures  NCMN-0 and NCMN-2
perform slightly better  respectively with plain CNNs and ResNets  which may result from the
interaction between the skip connections of ResNets and NCMN.

(a) Results on CIFAR-10.

(b) Results on CIFAR-100.

Figure 2: Feature correlations of WRN-22-7.5 networks trained with different types of noise.

To compare the performance of NCMN with shake-shake regularization  we train a WRN-22-5.4
network with two residual branches  which has comparable number of parameters to WRN-22-7.5.
We refer to this network as WRN-22-5.4×2. The averaging weights of residual branches are randomly
sampled for each input image  and are independently sampled for the forward and backward passes.
The training curves corresponding to different types of noise are shown in Fig. 3. On both CIFAR-

7

0 9:702 5800 9:70.4770 943430


0 9:702 5800 9:70.4770 943430


10 and CIFAR-100  NCMN and shake-shake show stronger regularization effect than standard
multiplicative noise  as indicated by the difference between training (dashed lines) and testing (solid
lines) accuracies. However  we make the observation that  the regularization strength of shake-shake
is stronger at the early stage of training  but diminishes rapidly afterwards  while that of NCMN is
more consistent throughout the training process. See Table 3 for detailed results.
As shown in Table 3  we provide additional results demonstrating the performance of NCMN on
models of different sizes. Interestingly  NCMN is able to signiﬁcantly improve the performance
of both small and large models. It is worth noting that  apart from the difference in architecture
and the number of parameters  the number of training epochs can also notably affect generalization
performance [20]. Compared to the results of shake-shake regularization  a WRN-28-10 network
trained with NCMN is able to achieve comparable performance in 9 times less epochs. For practical
uses  NCMN-0 is simple  fast  and can be applied to any batch-normalized neural networks  while
NCMN-2 yields better generalization performance on ResNets.

(a) Results on CIFAR-10.

(b) Results on CIFAR-100.

Figure 3: Training curves of WRN-22-7.5 networks trained with different types of noise  and that of
a WRN-22-5.4×2 network trained with shake-shake regularization.

Table 3: More results on CIFAR-10/100 for comparison.

Model
DenseNet-BC (250  24) [21]
ResNeXt-26 (2×96d) [5]
ResNeXt-29 (8×64d) [5]
WRN-28-10 [16]
DenseNet-BC (40  48)
CNN-16-3
CNN-16-10
WRN-22-2
WRN-22-7.5
WRN-22-5.4×2
WRN-28-10

Params Epochs
15.3M 300
26.2M 1800
34.4M 1800
36.5M 200
300
3.9M
1.6M
200
17.1M 200
1.1M
200
15.1M 200
15.5M 200
36.5M 200

Noise type

CIFAR-10 CIFAR-100

None

Shake/None
Shake/None
Dropout/None
NCMN-0/None
NCMN-0/None
NCMN-1/None
NCMN-0/None
NCMN-2/None

Shake/None

NCMN-2/None

3.62

2.86/3.58

—

3.89/4.00
3.51/4.07
4.47/5.10
3.41/4.05
4.56/5.19
3.00/3.68
3.51/4.04
2.78/3.70

17.60

—

15.85/16.34
18.85/19.25
17.68/19.92
21.92/24.97
17.55/19.22
23.54/25.90
16.70/19.29
17.77/19.71
15.86/18.42

We also experimented with language models based on long short-term memories (LSTMs) [22].
Intriguingly  we found that the hidden states of LSTMs had a consistently low level of correlation (less
than 0.1 on average)  even in the presence of dropout. Consequently  we did not observe signiﬁcant
improvement by replacing dropout with NCMN.

5 Conclusion

In this work  we analyzed multiplicative noise from an information perspective. Our analysis suggests
a side effect of dropout and other types of multiplicative noise  which increases the correlation between
features  and consequently degrades generalization performance. The same theoretical framework
also provides a principled explanation for the performance gain of shake-shake regularization.

8

54.877477 90430


$ 0
8 054.877477 90430


$ 0
8 0Furthermore  we proposed a simple modiﬁcation to the gradient of noise components  which  when
combined with batch normalization  is able to effectively remove the feature correlation effect. The
resulting method  NCMN  outperforms standard multiplicative noise by a large margin  proving it to
be a better alternative for batch-normalized networks.
While we combine batch normalization with NCMN to counteract the tendency of increasing feature
magnitude  an interesting future work would be to investigate if other normalization schemes  such as
layer normalization [23] and group normalization [24]  can serve the same purpose.

Acknowledgments

This work was supported by NSFC 61628209  Hubei Science Foundation 2016CFA030 
2017AAA125  and Wuhan Science & Tech Program 2018010401011288.

References
[1] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov 
Dumitru Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions.
In IEEE Conference on Computer Vision and Pattern Recognition  pages 1–9  2015.

[2] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition  pages 770–778 
2016.

[3] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research  15(1):1929–1958  2014.

[4] Li Wan  Matthew Zeiler  Sixin Zhang  Yann Le Cun  and Rob Fergus. Regularization of
neural networks using dropconnect. In International Conference on Machine Learning  pages
1058–1066  2013.

[5] Xavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In Workshop of

International Conference on Learning Representations  2017.

[6] Alex Krizhevsky  Ilya Sutskever  and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems  pages
1097–1105  2012.

[7] Stanislau Semeniuta  Aliaksei Severyn  and Erhardt Barth. Recurrent dropout without memory

loss. arXiv preprint arXiv:1603.05118  2016.

[8] Yoshua Bengio and James S Bergstra. Slow  decorrelated features for pretraining complex
cell-like networks. In Advances in neural information processing systems  pages 99–107  2009.
[9] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on

Learning Representations  2016.

[10] Michael Cogswell  Faruk Ahmed  Ross Girshick  Larry Zitnick  and Dhruv Batra. Reducing
overﬁtting in deep networks by decorrelating representations. In International Conference on
Learning Representations  2016.

[11] Pau Rodríguez  Jordi Gonzalez  Guillem Cucurull  Josep M Gonfaus  and Xavier Roca. Regu-
larizing cnns with locally constrained decorrelations. In International Conference on Learning
Representations  2017.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning  pages
448–456  2015.

[13] Jonathan Tompson  Ross Goroshin  Arjun Jain  Yann LeCun  and Christoph Bregler. Efﬁcient
object localization using convolutional networks. In IEEE Conference on Computer Vision and
Pattern Recognition  pages 648–656  2015.

[14] Sida Wang and Christopher Manning. Fast dropout training. In International Conference on

Machine Learning  pages 118–126  2013.

9

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Identity mappings in deep residual

networks. In European Conference on Computer Vision  pages 630–645. Springer  2016.

[16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks.

arXiv:1605.07146  2016.

arXiv preprint

[17] Zijun Zhang  Lin Ma  Zongpeng Li  and Chuan Wu. Normalized direction-preserving adam.

arXiv preprint arXiv:1709.04546  2017.

[18] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International

Conference on Learning Representations  2015.

[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

Technical report  University of Toronto  2009.

[20] Elad Hoffer  Itay Hubara  and Daniel Soudry. Train longer  generalize better: closing the
generalization gap in large batch training of neural networks. In Advances in Neural Information
Processing Systems  pages 1729–1739  2017.

[21] Gao Huang  Zhuang Liu  Kilian Q Weinberger  and Laurens van der Maaten. Densely connected
convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition 
volume 1  page 3  2017.

[22] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation  9(8):

1735–1780  1997.

[23] Jimmy Lei Ba  Jamie Ryan Kiros  and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450  2016.

[24] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision.

Springer  2018.

10

,Deepti Pachauri
Risi Kondor
Gautam Sargur
Vikas Singh
Sylvestre-Alvise Rebuffi
Hakan Bilen
Andrea Vedaldi
Zijun Zhang
Yining Zhang
Zongpeng Li