2013,Buy-in-Bulk Active Learning,In many practical applications of active learning  it is more cost-effective to request labels in large batches  rather than one-at-a-time. This is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch. In this work  we study the label complexity of active learning algorithms that request labels in a given number of batches  as well as the tradeoff between the total number of queries and the number of rounds allowed. We additionally study the total cost sufficient for learning  for an abstract notion of the cost of requesting the labels of a given number of examples at once. In particular  we find that for sublinear cost functions  it is often desirable to request labels in large batches (i.e.  buying in bulk); although this may increase the total number of labels requested  it reduces the total cost required for learning.,Buy-in-Bulk Active Learning

Liu Yang

Machine Learning Department 

Carnegie Mellon University

Jaime Carbonell

Language Technologies Institute 

Carnegie Mellon University

liuy@cs.cmu.edu

jgc@cs.cmu.edu

Abstract

In many practical applications of active learning  it is more cost-effective to re-
quest labels in large batches  rather than one-at-a-time. This is because the cost of
labeling a large batch of examples at once is often sublinear in the number of ex-
amples in the batch. In this work  we study the label complexity of active learning
algorithms that request labels in a given number of batches  as well as the tradeoff
between the total number of queries and the number of rounds allowed. We ad-
ditionally study the total cost sufﬁcient for learning  for an abstract notion of the
cost of requesting the labels of a given number of examples at once. In particular 
we ﬁnd that for sublinear cost functions  it is often desirable to request labels in
large batches (i.e.  buying in bulk); although this may increase the total number of
labels requested  it reduces the total cost required for learning.

1

Introduction

In many practical applications of active learning  the cost to acquire a large batch of labels at once is
signiﬁcantly less than the cost of the same number of sequential rounds of individual label requests.
This is true for both practical reasons (overhead time for start-up  reserving equipment in discrete
time-blocks  multiple labelers working in parallel  etc.) and for computational reasons (e.g.  time
to update the learner’s hypothesis and select the next examples may be large). Consider making
one vs multiple hematological diagnostic tests on an out-patient. There are ﬁxed up-front costs:
bringing the patient in for testing  drawing and storing the blood  entring the information in the
hospital record system  etc. And there are variable costs  per speciﬁc test. Consider a microarray
assay for gene expression data. There is a ﬁxed cost in setting up and running the microarray  but
virtually no incremental cost as to the number of samples  just a constraint on the max allowed.
Either of the above conditions are often the case in scientiﬁc experiments (e.g.  [1])  As a different
example  consider calling a focused group of experts to address questions w.r.t new product design
or introduction. There is a ﬁxed cost in forming the group (determine membership  contract  travel 
etc.)  and a incremental per-question cost. The common abstraction in such real-world versions
of “oracles” is that learning can buy-in-bulk to advantage because oracles charge either per batch
(answering a batch of questions for the same cost as answering a single question up to a batch
maximum)  or the cost per batch is axp + b  where b is the set-up cost  x is the number of queries 
and p = 1 or p < 1 (for the case where practice yields efﬁciency).
Often we have other tradeoffs  such as delay vs testing cost. For instance in a medical diagnosis case 
the most cost-effective way to minimize diagnostic tests is purely sequential active learning  where
each test may rule out a set of hypotheses (diagnoses) and informs the next test to perform. But
a patient suffering from a serious disease may worsen while sequential tests are being conducted.
Hence batch testing makes sense if the batch can be tested in parallel. In general one can convert
delay into a second cost factor and optimize for batch size that minimizes a combination of total
delay and the sum of the costs for the individual tests. Parallelizing means more tests would be
needed  since we lack the beneﬁt of earlier tests to rule out future ones. In order to perform this

1

batch-size optimization we also need to estimate the number of redundant tests incurred by turning
a sequence into a shorter sequence of batches.

For the reasons cited above  it can be very useful in practice to generalize active learning to active-
batch learning  with buy-in-bulk discounts. This paper developes a theoretical framework exploring
the bounds and sample compelxity of active buy-in-bulk machine learning  and analyzing the trade-
off that can be achieved between the number of batches and the total number of queries required for
accurate learning.

In another example  if we have many labelers (virtually unlimited) operating in parallel  but must
pay for each query  and the amount of time to get back the answer to each query is considered
independent with some distribution  it may often be the case that the expected amount of time
needed to get back the answers to m queries is sublinear in m  so that if the “cost” is a function
of both the payment amounts and the time  it might sometimes be less costly to submit multiple
queries to be labeled in parallel. In scenarios such as those mentioned above  a batch mode active
learning strategy is desirable  rather than a method that selects instances to be labeled one-at-a-time.

There have recently been several attempts to construct heuristic approaches to the batch mode active
learning problem (e.g.  [2]). However  theoretical analysis has been largely lacking. In contrast 
there has recently been signiﬁcant progress in understanding the advantages of fully-sequential ac-
tive learning (e.g.  [3  4  5  6  7]). In the present work  we are interested in extending the techniques
used for the fully-sequential active learning model  studying natural analogues of them for the batch-
model active learning model.

Formally  we are interested in two quantities: the sample complexity and the total cost. The sample
complexity refers to the number of label requests used by the algorithm. We expect batch-mode
active learning methods to use more label requests than their fully-sequential cousins. On the other
hand  if the cost to obtain a batch of labels is sublinear in the size of the batch  then we may
sometimes expect the total cost used by a batch-mode learning method to be signiﬁcantly less than
the analogous fully-sequential algorithms  which request labels individually.

2 Deﬁnitions and Notation

As in the usual statistical learning problem  there is a standard Borel space X   called the instance
space  and a set C of measurable classiﬁers h : X → {−1  +1}  called the concept space. Through-
out  we suppose that the VC dimension of C  denoted d below  is ﬁnite.
In the learning problem  there is an unobservable distribution DXY over X × {−1  +1}. Based on
t=1 denote an inﬁnite sequence of independent DXY -distributed
this quantity  we let Z = {(Xt  Yt)}∞
random variables. We also denote by Zt = {(X1  Y1)  (X2  Y2)  . . .   (Xt  Yt)} the ﬁrst t such
labeled examples. Additionally denote by DX the marginal distribution of DXY over X . For a
classiﬁer h : X → {−1  +1}  denote er(h) = P(X Y )∼DXY (h(X) 6= Y )  the error rate of h.
Additionally  for m ∈ N and Q ∈ (X × {−1  +1})m  let er(h; Q) = 1
I[h(x) 6= y] 
the empirical error rate of h. In the special case that Q = Zm  abbreviate erm(h) = er(h; Q).
For r > 0  deﬁne B(h  r) = {g ∈ C : DX (x : h(x) 6= g(x)) ≤ r}. For any H ⊆ C  deﬁne
DIS(H) = {x ∈ X : ∃h  g ∈ H s.t. h(x) 6= g(x)}. We also denote by η(x) = P (Y = +1|X = x) 
where (X  Y ) ∼ DXY   and let h∗(x) = sign(η(x) − 1/2) denote the Bayes optimal classiﬁer.
In the active learning protocol  the algorithm has direct access to the Xt sequence  but must request
to observe each label Yt  sequentially. The algorithm asks up to a speciﬁed number of label requests
n (the budget)  and then halts and returns a classiﬁer. We are particularly interested in determining 
for a given algorithm  how large this number of label requests needs to be in order to guarantee
small error rate with high probability  a value known as the label complexity. In the present work 
we are also interested in the cost expended by the algorithm. Speciﬁcally  in this context  there
is a cost function c : N → (0 ∞)  and to request the labels {Yi1   Yi2   . . .   Yim} of m examples
{Xi1   Xi2   . . .   Xim} at once requires the algorithm to pay c(m); we are then interested in the sum
of these costs  over all batches of label requests made by the algorithm. Depending on the form
of the cost function  minimizing the cost of learning may actually require the algorithm to request
labels in batches  which we expect would actually increase the total number of label requests.

|Q|P(x y)∈Q

2

To help quantify the label complexity and cost complexity  we make use of the following deﬁnition 
due to [6  7].
Deﬁnition 2.1. [6  7] Deﬁne the disagreement coefﬁcient of h∗ as
DX (DIS(B(h∗  r)))

.

θ(ǫ) = sup
r>ǫ

r

3 Buy-in-Bulk Active Learning in the Realizable Case: k-batch CAL

We begin our anlaysis with the simplest case: namely  the realizable case  with a ﬁxed prespeciﬁed
number of batches. We are then interested in quantifying the label complexity for such a scenario.
Formally  in this section we suppose h∗ ∈ C and er(h∗) = 0. This is refered to as the realizable
case. We ﬁrst review a well-known method for active learning in the realizable case  refered to as
CAL after its discoverers Cohn  Atlas  and Ladner [8].

Algorithm: CAL(n)
1. t ← 0  m ← 0  Q ← ∅
2. While t < n
3. m ← m + 1
4.

If max

y∈{−1 +1}

min
h∈C

er(h;Q ∪ {(Xm  y)}) = 0

Request Ym  let Q ← Q ∪ {(Xm  Ym)}  t ← t + 1

5.
6. Return ˆh = argminh∈C er(h;Q)

The label complexity of CAL is known to be O (θ(ǫ)(d log(θ(ǫ)) + log(log(1/ǫ)/δ)) log(1/ǫ)) [7].
That is  some n of this size sufﬁces to guarantee that  with probability 1 − δ  the returned classiﬁer
ˆh has er(ˆh) ≤ ǫ.
One particularly simple way to modify this algorithm to make it batch-based is to simply divide up
the budget into equal batch sizes. This yields the following method  which we refer to as k-batch
CAL  where k ∈ {1  . . .   n}.

Algorithm: k-batch CAL(n)
1. Let Q ← {}  b ← 2  V ← C
2. For m = 1  2  . . .
3.
4.
5.
6.
7.
8.
9.
10.

If Xm ∈ DIS(V )
Q ← Q ∪ {Xm}
If |Q| = ⌊n/k⌋
Request the labels of examples in Q
Let L be the corresponding labeled examples
V ← {h ∈ V : er(h; L) = 0}
b ← b + 1 and Q ← ∅
If b > k  Return any ˆh ∈ V

We expect the label complexity of k-batch CAL to somehow interpolate between passive learning
(at k = 1) and the label complexity of CAL (at k = n). Indeed  the following theorem bounds the
label complexity of k-batch CAL by a function that exhibits this interpolation behavior with respect
to the known upper bounds for these two cases.
Theorem 3.1. In the realizable case  for some

λ(ǫ  δ) = O(cid:16)kǫ−1/kθ(ǫ)1−1/k(d log(1/ǫ) + log(1/δ))(cid:17)  

for any n ≥ λ(ǫ  δ)  with probability at least 1 − δ  running k-batch CAL with budget n produces a
classiﬁer ˆh with er(ˆh) ≤ ǫ.
Proof. Let M = ⌊n/k⌋. Deﬁne V0 = C and i0M = 0. Generally  for b ≥ 1  let ib1  ib2  . . .   ibM
denote the indices i of the ﬁrst M points Xi ∈ DIS(Vb−1) for which i > i(b−1)M   and let Vb = {h ∈

3

Vb−1 : ∀j ≤ M  h(Xibj ) = h∗(Xibj )}. These correspond to the version space at the conclusion of
batch b in the k-batch CAL algorithm.
Note that Xib1  . . .   XibM are conditionally iid given Vb−1  with distribution of X given X ∈
DIS(Vb−1). Thus  the PAC bound of [9] implies that  for some constant c ∈ (0 ∞)  with prob-
ability ≥ 1 − δ/k 

Vb ⊆ B(cid:18)h∗  c

d log(M/d) + log(k/δ)

M

P (DIS(Vb−1))(cid:19) .

By a union bound  the above holds for all b ≤ k with probability ≥ 1 − δ; suppose this is the
case. Since P (DIS(Vb−1)) ≤ θ(ǫ) max{ǫ  maxh∈Vb−1er(h)}  and any b with maxh∈Vb−1 er(h) ≤ ǫ
would also have maxh∈Vb er(h) ≤ ǫ  we have

max
h∈Vb

er(h) ≤ max(cid:26)ǫ  c

d log(M/d) + log(k/δ)

M

θ(ǫ) max
h∈Vb−1

M

max
h∈Vk

Noting that P (DIS(V0)) ≤ 1 implies V1 ⊆ B(cid:16)h∗  c d log(M/d)+log(k/δ)
(cid:19)k

er(h) ≤ max(ǫ (cid:18)c
For some constant c′ > 0  any M ≥ c′ θ(ǫ)
(cid:0)d log 1
Since M = ⌊n/k⌋  it sufﬁces to have n ≥ k(cid:18)1 + c′ θ(ǫ)

d log(M/d) + log(k/δ)

k
ǫ1/k

k
ǫ1/k

M

k−1

er(h))(cid:27) .
(cid:17)  by induction we have
θ(ǫ)k−1) .

ǫ + log(k/δ)(cid:1) makes the right hand side ≤ ǫ.

k−1

(cid:0)d log 1

ǫ + log(k/δ)(cid:1)(cid:19).

Theorem 3.1 has the property that  when the disagreement coefﬁcient is small  the stated bound
on the total number of label requests sufﬁcient for learning is a decreasing function of k. This
makes sense  since θ(ǫ) small would imply that fully-sequential active learning is much better than
passive learning. Small values of k correspond to more passive-like behavior  while larger values of
k take fuller advantage of the sequential nature of active learning. In particular  when k = 1  we
recover a well-known label complexity bound for passive learning by empirical risk minimization
[10]. In contrast  when k = log(1/ǫ)  the ǫ−1/k factor is e (constant)  and the rest of the bound is at
most O(θ(ǫ)(d log(1/ǫ) + log(1/δ)) log(1/ǫ))  which is (up to a log factor) a well-known bound on
the label complexity of CAL for active learning [7] (a slight reﬁnement of the proof would in fact
recover the exact bound of [7] for this case); for k larger than log(1/ǫ)  the label complexity can only
improve; for instance  consider that upon reaching a given data point Xm in the data stream  if V is
the version space in k-batch CAL (for some k)  and V ′ is the version space in 2k-batch CAL  then
we have V ′ ⊆ V (supposing n is a multiple of 2k)  so that Xm ∈ DIS(V ′) only if Xm ∈ DIS(V ).
Note that even k = 2 can sometimes provide signiﬁcant reductions in label complexity over passive
learning: for instance  by a factor proportional to 1/√ǫ in the case that θ(ǫ) is bounded by a ﬁnite
constant.

4 Batch Mode Active Learning with Tsybakov noise

The above analysis was for the realizable case. While this provides a particularly clean and simple
analysis  it is not sufﬁciently broad to cover many realistic learning applications. To move beyond
the realizable case  we need to allow the labels to be noisy  so that er(h∗) > 0. One popular noise
model in the statistical learning theory literature is Tsybakov noise  which is deﬁned as follows.
Deﬁnition 4.1. [11] The distribution DXY satisﬁes Tsybakov noise if h∗ ∈ C  and for some c > 0
and α ∈ [0  1] 
equivalently  ∀h  P (h(x) 6= h∗(x)) ≤ c2(er(h) − er(h∗))α  where c1 and c2 are constants.
Supposing DXY satisﬁes Tsybakov noise  we deﬁne a quantity
Em = c3(cid:18) d log(m/d) + log(km/δ)

∀t > 0  P(|η(x) − 1/2| < t) < c1t

(cid:19)

1−α  

m

2−α

.

α

1

4

based on a standard generalization bound for passive learning [12]. Speciﬁcally  [12] have shown
that  for any V ⊆ C  with probability at least 1 − δ/(4km2) 

h g∈V |(er(h) − er(g)) − (erm(h) − erm(g))| < Em.
sup

(1)

Consider the following modiﬁcation of k-batch CAL  designed to be robust to Tsybakov noise. We
refer to this method as k-batch Robust CAL  where k ∈ {1  . . .   n}.

Algorithm: k-batch Robust CAL(n)
1. Let Q ← {}  b ← 1  V ← C  m1 ← 0
2. For m = 1  2  . . .
3.
4.
5.
6.
7.
8.
9.
10.
11.

If Xm ∈ DIS(V )
Q ← Q ∪ {Xm}
If |Q| = ⌊n/k⌋
Request the labels of examples in Q
Let L be the corresponding labeled examples
V ← {h ∈ V : (er(h; L) − ming∈V er(g; L)) ⌊n/k⌋
b ← b + 1 and Q ← ∅
mb ← m
If b > k  Return any ˆh ∈ V

m−mb ≤ Em−mb}

Theorem 4.2. Under the Tsybakov noise condition  letting β = α

λ(ǫ  δ) = O k(cid:18) 1
ǫ(cid:19)

2−α

¯β

(c2θ(c2ǫα))1− βk−1

¯β (cid:18)d log(cid:18) d

i=0 βi  for some

2−α   and ¯β =Pk−1
ǫ(cid:19) + log(cid:18) kd

δǫ(cid:19)(cid:19)

¯β

1+β ¯β−βk

! 

M

(1)

that ∀m > 0 

(applied under
total probability)

for any n ≥ λ(ǫ  δ)  with probability at least 1 − δ  running k-batch Robust CAL with budget n
produces a classiﬁer ˆh with er(ˆh) − er(h∗) ≤ ǫ.
Proof. Let M = ⌊n/k⌋. Deﬁne i0M = 0 and V0 = C. Generally  for b ≥ 1 
let
ib1  ib2  . . .   ibM denote the indices i of the ﬁrst M points Xi ∈ DIS(Vb−1) for which i >
i(b−1)M   and let Qb = {(Xib1   Yib1 )  . . .   (XibM   YibM )} and Vb = {h ∈ Vb−1 : (er(h; Qb) −
ibM −i(b−1)M ≤ EibM −i(b−1)M}. These correspond to the set V at the conclu-
ming∈Vb−1 er(g; Qb))
sion of batch b in the k-batch Robust CAL algorithm.
For b ∈ {1  . . .   k} 
the conditional distribution given Vb−1  com-
bined with the law of
letting Zb m =
implies
{(Xi(b−1)M +1  Yi(b−1)M +1)  ...  (Xi(b−1)M +m  Yi(b−1)M +m)}  with probability at least 1−δ/(4km2) 
if h∗ ∈ Vb−1  then er(h∗; Zb m) − ming∈Vb−1 er(g; Zb m) < Em  and every h ∈ Vb−1 with
er(h; Zb m) − ming∈Vb−1 er(g; Zb m) ≤ Em has er(h) − er(h∗) < 2Em. By a union bound 
In particular  this means it
this holds for all m ∈ N  with probability at least 1 − δ/(2k).
holds for m = ibM − i(b−1)M . But note that for this value of m  any h  g ∈ Vb−1 have
er(h; Zb m) − er(g; Zb m) = (er(h; Qb) − er(g; Qb)) M
m (since for every (x  y) ∈ Zb m \ Qb  either
both h and g make a mistake  or neither do). Thus if h∗ ∈ Vb−1  we have h∗ ∈ Vb as well  and
furthermore suph∈Vb er(h) − er(h∗) < 2EibM −i(b−1)M . By induction (over b) and a union bound 
these are satisﬁed for all b ∈ {1  . . .   k} with probability at least 1 − δ/2. For the remainder of the
proof  we suppose this 1 − δ/2 probability event occurs.
Next  we focus on lower bounding ibM − i(b−1)M   again by induction. As a base case  we clearly
have i1M − i0M ≥ M. Now suppose some b ∈ {2  . . .   k} has i(b−1)M − i(b−2)M ≥ Tb−1
for some Tb−1. Then  by the above  we have suph∈Vb−1 er(h) − er(h∗) < 2ETb−1. By the Tsy-
bakov noise condition  this implies Vb−1 ⊆ B(cid:0)h∗  c2(cid:0)2ETb−1(cid:1)α(cid:1)  so that if suph∈Vb−1 er(h) −
er(h∗) > ǫ  P (DIS(Vb−1)) ≤ θ(c2ǫα)c2(cid:0)2ETb−1(cid:1)α. Now note that the conditional distribution
of ibM − i(b−1)M given Vb−1 is a negative binomial random variable with parameters M and
1 − P (DIS(Vb−1)) (that is  a sum of M Geometric(P (DIS(Vb−1))) random variables). A Cher-
noff bound (applied under the conditional distribution given Vb−1) implies that P (ibM − i(b−1)M <

5

M/(2P (DIS(Vb−1)))|Vb−1) < e−M/6. Thus  for Vb−1 as above  with probability at least 1−e−M/6 
2θ(c2ǫα)c2(2ETb−1 )α . Thus  we can deﬁne Tb as in the right hand side  which thereby
ibM−i(b−1)M ≥
deﬁnes a recurrence. By induction  with probability at least 1 − ke−M/6 > 1 − δ/2 

M

ikM − i(k−1)M ≥ M

¯β(cid:18)

1

4c2θ(c2ǫα)(cid:19)

¯β−βk−1

2(d log(M ) + log(kM/δ))(cid:19)β( ¯β−βk−1)

1

(cid:18)

.

By a union bound  with probability 1−δ  this occurs simultaneously with the above suph∈Vk er(h)−
er(h∗) < 2EikM −i(k−1)M bound. Combining these two results yields

er(h) − er(h∗) = O   (c2θ(c2ǫα)) ¯β−βk−1

sup
h∈Vk
Setting this to ǫ and solving for n  we ﬁnd that it sufﬁces to have

!

M ¯β

1

2−α

(d log(M ) + log(kM/δ))

1+β( ¯β−βk−1)

2−α

!.

ǫ(cid:19)
M ≥ c4(cid:18) 1

2−α

¯β

(c2θ(c2ǫα))1− βk−1

¯β (cid:18)d log(cid:18) d

ǫ(cid:19) + log(cid:18) kd

δǫ(cid:19)(cid:19)

1+β ¯β−βk

¯β

 

for some constant c4 ∈ [1 ∞)  which then implies the stated result.
Note: the threshold Em in k-batch Robust CAL has a direct dependence on the parameters of the
Tsybakov noise condition. We have expressed the algorithm in this way only to simplify the pre-
sentation. In practice  such information is not often available. However  we can replace Em with a
data-dependent local Rademacher complexity bound ˆEm  as in [7]  which also satisﬁes (1)  and satis-
ﬁes (with high probability) ˆEm ≤ c′Em  for some constant c′ ∈ [1 ∞) (see [13]). This modiﬁcation
would therefore provide essentially the same guarantee stated above (up to constant factors)  with-
out having any direct dependence on the noise parameters  and the analysis gets only slightly more
involved to account for the conﬁdences in the concentration inequalities for these ˆEm estimators.
A similar result can also be obtained for batch-based variants of other noise-robust disagreement-
based active learning algorithms from the literature (e.g.  a variant of A2 [5] that uses updates based
on quantities related to these ˆEm estimators  in place of the traditional upper-bound/lower-bound
construction  would also sufﬁce).

When k = 1  Theorem 4.2 matches the best results for passive learning (up to log factors)  which
are known to be minimax optimal (again  up to log factors). If we let k become large (while still
considered as a constant)  our result converges to the known results for one-at-a-time active learning
with RobustCAL (again  up to log factors) [7  14]. Although those results are not always minimax
optimal  they do represent the state-of-the-art in the general analysis of active learning  and they are
really the best we could hope for from basing our algorithm on RobustCAL.

5 Buy-in-Bulk Solutions to Cost-Adaptive Active Learning

The above sections discussed scenarios in which we have a ﬁxed number k of batches  and we
simply bounded the label complexity achievable within that constraint by considering a variant of
CAL that uses k equal-sized batches. In this section  we take a slightly different approach to the
problem  by going back to one of the motivations for using batch-based active learning in the ﬁrst
place: namely  sublinear costs for answering batches of queries at a time. If the cost of answering
m queries at once is sublinear in m  then batch-based algorithms arise naturally from the problem
of optimizing the total cost required for learning.
Formally  in this section  we suppose we are given a cost function c : (0 ∞) → (0 ∞)  which is
nondecreasing  satisﬁes c(αx) ≤ αc(x) (for x  α ∈ [1 ∞))   and further satisﬁes the condition that
for every q ∈ N  ∃q′ ∈ N such that 2c(q) ≤ c(q′) ≤ 4c(q)  which typically amounts to a kind of
smoothness assumption. For instance  c(q) = √q would satisfy these conditions (as would many
other smooth increasing concave functions); the latter assumption can be generalized to allow other
constants  though we only study this case below for simplicity.

To understand the total cost required for learning in this model  we consider the following cost-
adaptive modiﬁcation of the CAL algorithm.

6

Algorithm: Cost-Adaptive CAL(C)
1. Q ← ∅  R ← DIS(C)  V ← C  t ← 0
2. Repeat
3.
q ← 1
4. Do until P (DIS(V )) ≤ P (R)/2
5.
6.
7.
8.
9.
10.
11. R ← DIS(V )

Let q′ > q be minimal such that c(q′ − q) ≥ 2c(q)
If c(q′ − q) + t > C  Return any ˆh ∈ V
Request the labels of the next q′ − q examples in DIS(V )
Update V by removing those classiﬁers inconsistent with these labels
Let t ← t + c(q′ − q)
q ← q′

Note that the total cost expended by this method never exceeds the budget argument C. We have the
following result on how large of a budget C is sufﬁcient for this method to succeed.
Theorem 5.1. In the realizable case  for some

λ(ǫ  δ) = O(cid:16)c(cid:16)θ(ǫ) (d log(θ(ǫ)) + log(log(1/ǫ)/δ))(cid:17) log(1/ǫ)(cid:17)  

for any C ≥ λ(ǫ  δ)  with probability at least 1 − δ  Cost-Adaptive CAL(C) returns a classiﬁer ˆh
with er(ˆh) ≤ ǫ.
Proof. Supposing an unlimited budget (C = ∞)  let us determine how much cost the algorithm
incurs prior to having suph∈V er(h) ≤ ǫ; this cost would then be a sufﬁcient size for C to guarantee
this occurs. First  note that h∗ ∈ V is maintained as an invariant throughout the algorithm. Also 
note that if q is ever at least as large as O(θ(ǫ)(d log(θ(ǫ)) + log(1/δ′)))  then as in the analysis for
CAL [7]  we can conclude (via the PAC bound of [9]) that with probability at least 1 − δ′ 

sup
h∈V

P (h(X) 6= h∗(X)|X ∈ R) ≤ 1/(2θ(ǫ)) 

so that

sup
h∈V

er(h) = sup
h∈V

P (h(X) 6= h∗(X)|X ∈ R)P (R) ≤ P (R)/(2θ(ǫ)).

We know R = DIS(V ′) for the set V ′ which was the value of the variable V at the time this R was
obtained. Supposing suph∈V ′ er(h) > ǫ  we know (by the deﬁnition of θ(ǫ)) that

Therefore 

P (R) ≤ P (cid:18)DIS(cid:18)B(cid:18)h∗  sup

h∈V ′

er(h)(cid:19)(cid:19)(cid:19) ≤ θ(ǫ) sup

h∈V ′

er(h).

sup
h∈V

er(h) ≤

1
2

sup
h∈V ′

er(h).

this implies the condition in Step 4 will be satisﬁed if this happens while
In particular 
this condition can be satisﬁed at most ⌈log2(1/ǫ)⌉ times while
suph∈V er(h) > ǫ. But
suph∈V er(h) > ǫ (since suph∈V er(h) ≤ P (DIS(V ))). So with probability at least 1 −
δ′⌈log2(1/ǫ)⌉  as long as suph∈V er(h) > ǫ  we always have c(q) ≤ 4c(O(θ(ǫ)(d log(θ(ǫ)) +
this is
log(1/δ′)))) ≤ O(c(θ(ǫ)(d log(θ(ǫ)) + log(1/δ′)))). Letting δ′ = δ/⌈log2(1/ǫ)⌉ 
1 − δ. So for each round of the outer loop while suph∈V er(h) > ǫ  by summing the geomet-
ric series of cost values c(q′ − q) in the inner loop  we ﬁnd the total cost incurred is at most
O(c(θ(ǫ)(d log(θ(ǫ)) + log(log(1/ǫ)/δ)))). Again  there are at most ⌈log2(1/ǫ)⌉ rounds of the
outer loop while suph∈V er(h) > ǫ  so that the total cost incurred before we have suph∈V er(h) ≤ ǫ
is at most O(c(θ(ǫ)(d log(θ(ǫ)) + log(log(1/ǫ)/δ))) log(1/ǫ)).

Comparing this result to the known label complexity of CAL  which is (from [7])

O (θ(ǫ) (d log(θ(ǫ)) + log(log(1/ǫ)/δ)) log(1/ǫ))  

we see that the major factor  namely the O (θ(ǫ) (d log(θ(ǫ)) + log(log(1/ǫ)/δ))) factor  is now
inside the argument to the cost function c(·). In particular  when this cost function is sublinear  we

7

expect this bound to be signiﬁcantly smaller than the cost required by the original fully-sequential
CAL algorithm  which uses batches of size 1  so that there is a signiﬁcant advantage to using this
batch-mode active learning algorithm.

Again  this result is formulated for the realizable case for simplicity  but can easily be extended to
the Tsybakov noise model as in the previous section. In particular  by reasoning quite similar to
that above  a cost-adaptive variant of the Robust CAL algorithm of [14] achieves error rate er(ˆh) −
er(h∗) ≤ ǫ with probability at least 1 − δ using a total cost

O(cid:16)c(cid:16)θ(c2ǫα)c2

2ǫ2α−2dpolylog (1/(ǫδ))(cid:17) log (1/ǫ)(cid:17) .

We omit the technical details for brevity. However  the idea is similar to that above  except
that the update to the set V is now as in k-batch Robust CAL (with an appropriate modiﬁca-
tion to the δ-related logarithmic factor in Em)  rather than simply those classiﬁers making no
mistakes. The proof then follows analogous to that of Theorem 5.1  the only major change be-
ing that now we bound the number of unlabeled examples processed in the inner loop before
suph∈V P (h(X) 6= h∗(X)) ≤ P (R)/(2θ); letting V ′ be the previous version space (the one for
which R = DIS(V ′))  we have P (R) ≤ θc2(suph∈V ′ er(h) − er(h∗))α  so that it sufﬁces to have
suph∈V P (h(X) 6= h∗(X)) ≤ (c2/2)(suph∈V ′ er(h) − er(h∗))α  and for this it sufﬁces to have
suph∈V er(h)− er(h∗) ≤ 2−1/α suph∈V ′ er(h)− er(h∗); by inverting Em  we ﬁnd that it sufﬁces to
have a number of samples ˜O(cid:0)(2−1/α suph∈V ′ er(h) − er(h∗))α−2d(cid:1). Since the number of label re-
quests among m samples in the inner loop is roughly ˜O(mP (R)) ≤ ˜O(mθc2(suph∈V ′ er(h) −
er(h∗))α)  the batch size needed to make suph∈V P (h(X) 6= h∗(X)) ≤ P (R)/(2θ) is at
most ˜O(cid:0)θc222/α(suph∈V ′ er(h) − er(h∗))2α−2d(cid:1). When suph∈V ′ er(h) − er(h∗) > ǫ  this is
˜O(cid:0)θc222/αǫ2α−2d(cid:1). If suph∈V P (h(X) 6= h∗(X)) ≤ P (R)/(2θ) is ever satisﬁed  then by the
same reasoning as above  the update condition in Step 4 would be satisﬁed. Again  this update can
be satisﬁed at most log(1/ǫ) times before achieving suph∈V er(h) − er(h∗) ≤ ǫ.
6 Conclusions

We have seen that the analysis of active learning can be adapted to the setting in which labels are
requested in batches. We studied this in two related models of learning. In the ﬁrst case  we supposed
the number k of batches is speciﬁed  and we analyzed the number of label requests used by an
algorithm that requested labels in k equal-sized batches. As a function of k  this label complexity
became closer to that of the analogous results for fully-sequential active learning for larger values of
k  and closer to the label complexity of passive learning for smaller values of k  as one would expect.
Our second model was based on a notion of the cost to request the labels of a batch of a given size.
We studied an active learning algorithm designed for this setting  and found that the total cost used
by this algorithm may often be signiﬁcantly smaller than that used by the analogous fully-sequential
active learning methods  particularly when the cost function is sublinear.

There are many active learning algorithms in the literature that can be described (or analyzed) in
terms of batches of label requests. For instance  this is the case for the margin-based active learning
strategy explored by [15]. Here we have only studied variants of CAL (and its noise-robust gen-
eralization). However  one could also apply this style of analysis to other methods  to investigate
analogous questions of how the label complexities of such methods degrade as the batch sizes in-
crease  or how such methods might be modiﬁed to account for a sublinear cost function  and what
results one might obtain on the total cost of learning with these modiﬁed methods. This could
potentially be a fruitful future direction for the study of batch mode active learning.

The tradeoff between the total number of queries and the number of rounds examined in this paper is
natural to study. Similar tradeoffs have been studied in other contexts. In any two-party communica-
tion task  there are three measures of complexity that are typically used: communication complexity
(the total number of bits exchanged)  round complexity (the number of rounds of communication) 
and time complexity. The classic work [16] considered the problem of the tradeoffs between com-
munication complexity and rounds of communication. [17] studies the tradeoffs among all three of
communication complexity  round complexity  and time complexity. Interested readers may wish
to go beyond the present and to study the tradeoffs among all the three measures of complexity for
batch mode active learning.

8

References

[1] V. S. Sheng and C. X. Ling. Feature value acquisition in testing: a sequential batch test

algorithm. In Proceedings of the 23rd international conference on Machine learning  2006.

[2] S. Chakraborty  V. Balasubramanian  and S. Panchanathan. An optimization based framework
for dynamic batch mode active learning. In Advances in Neural Information Processing  2010.
[3] S. Dasgupta  A. Kalai  and C. Monteleoni. Analysis of perceptron-based active learning. Jour-

nal of Machine Learning Research  10:281–299  2009.

[4] S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural

Information Processing Systems 18  2005.

[5] M. F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. In Proc. of the 23rd

International Conference on Machine Learning  2006.

[6] S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of

the 24th International Conference on Machine Learning  2007.

[7] S. Hanneke. Rates of convergence in active learning. The Annals of Statistics  39(1):333–361 

2011.

[8] D. Cohn  L. Atlas  and R. Ladner. Improving generalization with active learning. Machine

Learning  15(2):201–221  1994.

[9] V. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer-Verlag  New York 

1982.

[10] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  1999.

[11] E. Mammen and A.B. Tsybakov. Smooth discrimination analysis. The Annals of Statistics 

27:1808–1829  1999.

[12] P. Massart and ´E. N´ed´elec. Risk bounds for statistical learning. The Annals of Statistics 

34(5):2326–2366  2006.

[13] V. Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.

The Annals of Statistics  34(6):2593–2656  2006.

[14] S. Hanneke. Activized learning: Transforming passive to active with improved label complex-

ity. Journal of Machine Learning Research  13(5):1469–1587  2012.

[15] M.-F. Balcan  A. Broder  and T. Zhang. Margin based active learning. In Proceedings of the

20th Conference on Learning Theory  2007.

[16] C. H. Papadimitriou and M. Sipser. Communication complexity. Journal of Computer and

System Sciences  28(2):260269  1984.

[17] P. Harsha  Y. Ishai  Joe Kilian  Kobbi Nissim  and Srinivasan Venkatesh. Communication
versus computation. In The 31st International Colloquium on Automata  Languages and Pro-
gramming  pages 745–756  2004.

9

,Liu Yang
Jaime Carbonell
Huishuai Zhang
Yi Zhou
Yingbin Liang