2015,Convergence Rates of Active Learning for Maximum Likelihood Estimation,An active learner is given a class of models  a large set of unlabeled examples  and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning  but most of this work has focused on the PAC or the agnostic PAC model. In this paper  we shift our attention to a more general setting -- maximum likelihood estimation. Provided certain conditions hold on the model class  we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general  and cover the widely popular class of Generalized Linear Models  which in turn  include models for binary and multi-class classification  regression  and conditional random fields. We provide an upper bound on the label requirement of our algorithm  and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case  just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation.  On the empirical side  the recent work in (Gu et al. 2012) and (Gu et al. 2014) (on active linear and logistic regression) shows the promise of this approach.,Convergence Rates of Active Learning
for Maximum Likelihood Estimation

Kamalika Chaudhuri ⇤

Sham M. Kakade †

Praneeth Netrapalli ‡

Sujay Sanghavi §

Abstract

An active learner is given a class of models  a large set of unlabeled examples  and
the ability to interactively query labels of a subset of these examples; the goal of
the learner is to learn a model in the class that ﬁts the data well.
Previous theoretical work has rigorously characterized label complexity of active
learning  but most of this work has focused on the PAC or the agnostic PAC model.
In this paper  we shift our attention to a more general setting – maximum likeli-
hood estimation. Provided certain conditions hold on the model class  we provide
a two-stage active learning algorithm for this problem. The conditions we re-
quire are fairly general  and cover the widely popular class of Generalized Linear
Models  which in turn  include models for binary and multi-class classiﬁcation 
regression  and conditional random ﬁelds.
We provide an upper bound on the label requirement of our algorithm  and a lower
bound that matches it up to lower order terms. Our analysis shows that unlike
binary classiﬁcation in the realizable case  just a single extra round of interaction is
sufﬁcient to achieve near-optimal performance in maximum likelihood estimation.
On the empirical side  the recent work in [12] and [13] (on active linear and
logistic regression) shows the promise of this approach.

Introduction

1
In active learning  we are given a sample space X   a label space Y  a class of models that map X to
Y  and a large set U of unlabelled samples. The goal of the learner is to learn a model in the class
with small target error while interactively querying the labels of as few of the unlabelled samples as
possible.
Most theoretical work on active learning has focussed on the PAC or the agnostic PAC model  where
the goal is to learn binary classiﬁers that belong to a particular hypothesis class [2  14  10  7  3  4  22] 
and there has been only a handful of exceptions [19  9  20]. In this paper  we shift our attention to
a more general setting – maximum likelihood estimation (MLE)  where Pr(Y |X) is described by a
model ✓ belonging to a model class ⇥. We show that when data is generated by a model in this class 
we can do active learning provided the model class ⇥ has the following simple property: the Fisher
information matrix for any model ✓ 2 ⇥ at any (x  y) depends only on x and ✓. This condition is
satisﬁed in a number of widely applicable model classes  such as Linear Regression and Generalized
Linear Models (GLMs)  which in turn includes models for Multiclass Classiﬁcation and Conditional
Random Fields. Consequently  we can provide active learning algorithms for maximum likelihood
estimation in all these model classes.
The standard solution to active MLE estimation in the statistics literature is to select samples for
label query by optimizing a class of summary statistics of the asymptotic covariance matrix of the

⇤Dept. of CS  University of California at San Diego. Email: kamalika@cs.ucsd.edu
†Dept. of CS and of Statistics  University of Washington. Email: sham@cs.washington.edu
‡Microsoft Research New England. Email:praneeth@microsoft.com
§Dept. of ECE  The University of Texas at Austin. Email:sanghavi@mail.utexas.edu

1

estimator [6]. The literature  however  does not provide any guidance towards which summary statis-
tic should be used  or any analysis of the solution quality when a ﬁnite number of labels or samples
are available. There has also been some recent work in the machine learning community [12  13  19]
on this problem; but these works focus on simple special cases (such as linear regression [19  12] or
logistic regression [13])  and only [19] involves a consistency and ﬁnite sample analysis.
In this work  we consider the problem in its full generality  with the goal of minimizing the expected
log-likelihood error over the unlabelled data. We provide a two-stage active learning algorithm for
this problem. In the ﬁrst stage  our algorithm queries the labels of a small number of random samples
from the data distribution in order to construct a crude estimate ✓1 of the optimal parameter ✓⇤. In
the second stage  we select a set of samples for label query by optimizing a summary statistic of the
covariance matrix of the estimator at ✓1; however  unlike the experimental design work  our choice
of statistic is directly motivated by our goal of minimizing the expected log-likelihood error  which
guides us towards the right objective.
We provide a ﬁnite sample analysis of our algorithm when some regularity conditions hold and
when the negative log likelihood function is convex. Our analysis is still fairly general  and applies
to Generalized Linear Models  for example. We match our upper bound with a corresponding lower
bound  which shows that the convergence rate of our algorithm is optimal (except for lower order
terms); the ﬁnite sample convergence rate of any algorithm that uses (perhaps multiple rounds of)
sample selection and maximum likelihood estimation is either the same or higher than that of our
algorithm. This implies that unlike what is observed in learning binary classiﬁers  a single round of
interaction is sufﬁcient to achieve near-optimal log likelihood error for ML estimation.

1.1 Related Work

Previous theoretical work on active learning has focussed on learning a classiﬁer belonging to a
hypothesis class H in the PAC model. Both the realizable and non-realizable cases have been con-
sidered. In the realizable case  a line of work [7  18] has looked at a generalization of binary search;
while their algorithms enjoy low label complexity  this style of algorithms is inconsistent in the
presence of noise. The two main styles of algorithms for the non-realizable case are disagreement-
based active learning [2  10  4]  and margin or conﬁdence-based active learning [3  22]. While active
learning in the realizable case has been shown to achieve an exponential improvement in label com-
plexity over passive learning [2  7  14]  in the agnostic case  the gains are more modest (sometimes
a constant factor) [14  10  8]. Moreover  lower bounds [15] show that the label requirement of any
agnostic active learning algorithm is always at least ⌦(⌫2/✏2)  where ⌫ is the error of the best hy-
pothesis in the class  and ✏ is the target error. In contrast  our setting is much more general than
binary classiﬁcation  and includes regression  multi-class classiﬁcation and certain kinds of condi-
tional random ﬁelds that are not covered by previous work.
[19] provides an active learning algorithm for linear regression problem under model mismatch.
Their algorithm attempts to learn the location of the mismatch by ﬁtting increasingly reﬁned par-
titions of the domain  and then uses this information to reweight the examples. If the partition is
highly reﬁned  then the computational complexity of the resulting algorithm may be exponential
in the dimension of the data domain. In contrast  our algorithm applies to a more general setting 
and while we do not address model mismatch  our algorithm has polynomial time complexity. [1]
provides an active learning algorithm for Generalized Linear Models in an online selective sampling
setting; however  unlike ours  their input is a stream of unlabelled examples  and at each step  they
need to decide whether the label of the current example should be queried.
Our work is also related to the classical statistical work on optimal experiment design  which mostly
considers maximum likelihood estimation [6]. For uni-variate estimation  they suggest selecting
samples to maximize the Fisher information which corresponds to minimizing the variance of the
regression coefﬁcient. When ✓ is multi-variate  the Fisher information is a matrix; in this case  there
are multiple notions of optimal design which correspond to maximizing different parameters of the
Fisher information matrix. For example  D-optimality maximizes the determinant  and A-optimality
maximizes the trace of the Fisher information. In contrast with this work  we directly optimize
the expected log-likelihood over the unlabelled data which guides us to the appropriate objective
function; moreover  we provide consistency and ﬁnite sample guarantees.

2

Finally  on the empirical side  [13] and [12] derive algorithms similar to ours for logistic and linear
regression based on projected gradient descent. Notably  these works provide promising empirical
evidence for this approach to active learning; however  no consistency guarantees or convergence
rates are provided (the rates presented in these works are not stated in terms of the sample size). In
contrast  our algorithm applies more generally  and we provide consistency guarantees and conver-
gence rates. Moreover  unlike [13]  our logistic regression algorithm uses a single extra round of
interaction  and our results illustrate that a single round is sufﬁcient to achieve a convergence rate
that is optimal except for lower order terms.

2 The Model

We begin with some notation. We are given a pool U = {x1  . . .   xn} of n unlabelled examples
drawn from some instance space X   and the ability to interactively query labels belonging to a label
space Y of m of these examples. In addition  we are given a family of models M = {p(y|x  ✓) ✓ 2
⇥} parameterized by ✓ 2 ⇥ ✓ Rd. We assume that there exists an unknown parameter ✓⇤ 2 ⇥ such
that querying the label of an xi 2 U generates a yi drawn from the distribution p(y|xi ✓ ⇤). We also
abuse notation and use U to denote the uniform distribution over the examples in U.
We consider the ﬁxed-design (or transductive) setting  where our goal is to minimize the error on
the ﬁxed set of points U. For any x 2X   y 2Y and ✓ 2 ⇥  we deﬁne the negative log-likelihood
function L(y|x  ✓) as:
Our goal is to ﬁnd a ˆ✓ to minimize LU (ˆ✓)  where

L(y|x  ✓) =  log p(y|x  ✓)

LU (✓) = EX⇠U Y ⇠p(Y |X ✓⇤)[L(Y |X  ✓)]

by interactively querying labels for a subset of U of size m  where we allow label queries with
replacement i.e.  the label of an example may be queried multiple times.
An additional quantity of interest to us is the Fisher information matrix  or the Hessian of the nega-
tive log-likelihood L(y|x  ✓) function  which determines the convergence rate. For our active learn-
ing procedure to work correctly  we require the following condition.
Condition 1. For any x 2X   y 2Y   ✓ 2 ⇥  the Fisher information @2L(y|x ✓)
x and ✓ (and does not depend on y.)

is a function of only

@✓2

Condition 1 is satisﬁed by a number of models of practical interest; examples include linear re-
gression and generalized linear models. Section 5.1 provides a brief derivation of Condition 1 for
generalized linear models.
For any x  y and ✓  we use I(x  ✓) to denote the Hessian @2L(y|x ✓)
; observe that by Assumption 1 
this is just a function of x and ✓. Let  be any distribution over the unlabelled samples in U; for any
✓ 2 ⇥  we use:

@✓2

I(✓) = EX⇠[I(X  ✓)]

3 Algorithm

The main idea behind our algorithm is to sample xi from a well-designed distribution  over U 
query the labels of these samples and perform ML estimation over them. To ensure good perfor-
mance   should be chosen carefully  and our choice of  is motivated by Lemma 1. Suppose
the labels yi are generated according to: yi ⇠ p(y|xi ✓ ⇤). Lemma 1 states that the expected log-
likelihood error of the ML estimate with respect to m samples from  in this case is essentially
TrI(✓⇤)1IU (✓⇤) /m.
This suggests selecting  as the distribution ⇤ that minimizes TrI⇤(✓⇤)1IU (✓⇤). Unfortu-
1-2). In the second stage  we calculate a distribution 1 which minimizes TrI1(✓1)1IU (✓1) and

nately  we cannot do this as ✓⇤ is unknown. We resolve this problem through a two stage algorithm;
in the ﬁrst stage  we use a small number m1 of samples to construct a coarse estimate ✓1 of ✓⇤ (Steps

draw samples from (a slight modiﬁcation of) this distribution for a ﬁner estimation of ✓⇤ (Steps 3-5).

3

Algorithm 1 ActiveSetSelect
Input: Samples xi  for i = 1 ···   n
1: Draw m1 samples u.a.r from U  and query their labels to get S1.
2: Use S1 to solve the MLE problem:

✓1 = argmin✓2⇥ X(xi yi)2S1

L(yi|xi ✓ )

3: Solve the following SDP (refer Lemma 3):

a⇤ = argminaTrS1IU (✓1)

s.t. ( S =Pi aiI(xi ✓ 1)
Pi ai = m2

0  ai  1

4: Draw m2 examples using probability = ↵1 + (1 ↵)U where the distribution 1 = a⇤i
5: Use S2 to solve the MLE problem:

. Query their labels to get S2.

↵ = 1  m1/6

m2

2

and

✓2 = argmin✓2⇥ X(xi yi)2S2

L(yi|xi ✓ )

Output: ✓2

The distribution 1 is modiﬁed slightly to ¯ (in Step 4) to ensure that I¯(✓⇤) is well conditioned
with respect to IU (✓⇤).
The algorithm is formally presented in Algorithm 1.
Finally  note that Steps 1-2 are necessary because IU and I are functions of ✓. In certain special
cases such as linear regression  IU and I are independent of ✓.
In those cases  Steps 1-2 are
unnecessary  and we may skip directly to Step 3.

4 Performance Guarantees

The following regularity conditions are essentially a quantiﬁed version of the standard Local Asymp-
totic Normality (LAN) conditions for studying maximum likelihood estimation (see [5  21]).
Assumption 1. (Regularity conditions for LAN)

1. Smoothness: The ﬁrst three derivatives of L(y|x  ✓) exist in all interior points of ⇥ ✓ Rd.
2. Compactness: ⇥ is compact and ✓⇤ is an interior point of ⇥.
3. Strong Convexity: IU (✓⇤) = 1

i=1 I (xi ✓ ⇤) is positive deﬁnite with smallest singular

value min > 0.

nPn

4. Lipschitz continuity: There exists a neighborhood B of ✓⇤ and a constant L3 such that for

all x 2 U  I(x  ✓) is L3-Lipschitz in this neighborhood.

for every ✓  ✓0 2 B.

5. Concentration at ✓⇤: For any x 2 U and y  we have (with probability one) 

IU (✓⇤)1/2 (I (x  ✓)  I (x  ✓0)) IU (✓⇤)1/22  L3 k✓  ✓0kIU (✓⇤)  
krL(y|x  ✓⇤)kIU (✓⇤)1  L1  and IU (✓⇤)1/2I (x  ✓⇤) IU (✓⇤)1/22  L2.

6. Boundedness: max(x y) sup✓2⇥ |L(x  y|✓)| R.

In addition to the above  we need one extra condition which is essentially a pointwise self concor-
dance. This condition is satisﬁed by a vast class of models  including the generalized linear models.

4

Assumption 2. Point-wise self concordance:

L4 k✓  ✓⇤k2 I (x  ✓⇤)  I (x  ✓)  I (x  ✓⇤)  L4 k✓  ✓⇤k2 I (x  ✓⇤) .

Deﬁnition 1. [Optimal Sampling Distribution ⇤] We deﬁne the optimal sampling distribution ⇤

over the points in U as the distribution ⇤ = (⇤1   . . .   ⇤n) for which ⇤i  0 Pi ⇤i = 1  and
TrI⇤(✓⇤)1IU (✓⇤) is as small as possible.

Deﬁnition 1 is motivated by Lemma 1  which indicates that under some mild regularity conditions  a
ML estimate calculated on samples drawn from ⇤ will provide the best convergence rates (including
the right constant factor) for the expected log-likelihood error.
We now present the main result of our paper. The proof of the following theorem and all the sup-
porting lemmas will be presented in Appendix A.
Theorem 1.
Let 


hold.
>
O✓max✓L2 log2 d  L2
1⇣L2
Then with
probability  1    the expected log likelihood error of the estimate ✓2 of Algorithm 1 is bounded
as:

conditions
of
diameter(⇥)
Tr(IU (✓⇤)1)

in Assumptions
used
step

regularity
number

2
and
be m1

the
3 + 1

Suppose
10 

samples

  2L2

1
(1)

and

the

in

min⌘ log2 d 
  1◆4

2

4

 Tr⇣IU (✓⇤)1⌘◆◆.
(1 +e✏m2)Tr⇣I⇤(✓⇤)1IU (✓⇤)⌘ 1

in Deﬁnition

1

m2

+

R
m2
2

 

(1)

E [LU (✓2)]  LU (✓⇤) ✓1 +

2

is

the

optimal

sampling

where ⇤

O✓L1L3 + pL2 plog dm2

=
for any sampling distribution  satisfying
I(✓⇤) ⌫ cIU (✓⇤) and label constraint of m2  we have the following lower bound on the
expected log likelihood error for ML estimate:

and e✏m2

distribution

Moreover 

◆.

m1/6

EhLU (b✓)i  LU (✓⇤)  (1  ✏m2) Tr⇣I(✓⇤)1IU (✓⇤)⌘ 1
= e✏m2

where ✏m2
Remark 1. (Restricting to Maximum Likelihood Estimation) Our restriction to maximum likelihood
estimators is minor  as this is close to minimax optimal (see [16]). Minor improvements with certain
kinds of estimators  such as the James-Stein estimator  are possible.

L2
1
cm2
2

 

(2)

m2 

c2m1/3

def

.

2

4.1 Discussions

Several remarks about Theorem 1 are in order.
The high probability bound in Theorem 1 is with respect to the samples drawn in S1; provided these
samples are representative (which happens with probability  1  )  the output ✓2 of Algorithm 1
will satisfy (1). Additionally  Theorem 1 assumes that the labels are sampled with replacement; in
other words  we can query the label of a point xi multiple times. Removing this assumption is an
avenue for future work.

Second  the highest order term in both (1) and (2) is Tr⇣I⇤(✓⇤)1IU (✓⇤)⌘ /m. The terms involving
✏m2 ande✏m2 are lower order as both ✏m2 ande✏m2 are o(1). Moreover  if  = !(1)  then the term

involving  in (1) is of a lower order as well. Observe that  also measures the tradeoff between m1
and m2  and as long as  = o(pm2)  m1 is also of a lower order than m2. Thus  provided  is !(1)
and o(pm2)  the convergence rate of our algorithm is optimal except for lower order terms.
Finally  the lower bound (2) applies to distributions  for which I(✓⇤)  cIU (✓⇤)  where c occurs
in the lower order terms of the bound. This constraint is not very restrictive  and does not affect
the asymptotic rate. Observe that IU (✓⇤) is full rank. If I(✓⇤) is not full rank  then the expected
log likelihood error of the ML estimate with respect to  will not be consistent  and thus such a 
will never achieve the optimal rate. If I(✓⇤) is full rank  then there always exists a c for which
I(✓⇤)  cIU (✓⇤). Thus (2) essentially states that for distributions  where I(✓⇤) is close to
being rank-deﬁcient  the asymptotic convergence rate of O(TrI(✓⇤)1IU (✓⇤) /m2) is achieved
at larger values of m2.

5

4.2 Proof Outline

Our main result relies on the following three steps.

4.2.1 Bounding the Log-likelihood Error
First  we characterize the log likelihood error (wrt U) of the empirical risk minimizer (ERM) esti-

1
m2

the ERM estimate using the distribution :

b✓ = argmin✓2⇥

mate obtained using a sampling distribution . Concretely  let  be a distribution on U. Letb✓ be

where Xi ⇠  and Yi ⇠ p(y|Xi ✓ ⇤). The core of our analysis is Lemma 1  which shows a precise

m2Xi=1
estimate of the log likelihood error EhLU⇣b✓⌘  LU (✓⇤)i.
tribution on U andb✓ be the ERM estimate (3) using m2 labeled examples. Suppose further that
I(✓⇤) ⌫ cIU (✓⇤) for some constant c < 1. Then  for any p  2 and m2 large enough such that
⌘ < 1  we have:
= O⇣ 1
✏m2

Lemma 1. Suppose L satisﬁes the regularity conditions in Assumptions 1 and 2. Let  be a dis-

L(Yi|Xi ✓ ) 

(3)

def

 EhLU⇣b✓⌘  LU (✓⇤)i  (1 + ✏m2)

⌧ 2
m2

+

R
mp
2

 

m2

c2L1L3 + pL2q p log dm2
= Tr⇣I(✓⇤)1IU (✓⇤)⌘.

(1  ✏m2)

⌧ 2
m2 

cmp/2

L2
1

2

where ⌧ 2 def

4.2.2 Approximating ✓⇤
Lemma 1 motivates sampling from the optimal sampling distribution ⇤ that minimizes

Tr⇣I⇤(✓⇤)1IU (✓⇤)⌘. However  this quantity depends on ✓⇤  which we do not know. To re-

solve this issue  our algorithm ﬁrst queries the labels of a small fraction of points (m1) and solves a
ML estimation problem to obtain a coarse estimate ✓1 of ✓⇤.
How close should ✓1 be to ✓⇤? Our analysis indicates that it is sufﬁcient for ✓1 to be close enough that
for any x  I(x  ✓1) is a constant factor spectral approximation to I(x  ✓⇤); the number of samples
needed to achieve this is analyzed in Lemma 2.
Lemma 2. Suppose L satisﬁes the regularity conditions in Assumptions 1 and 2. If the number of
samples used in the ﬁrst step

m1 > O0@max0@L2 log2 d  L2
1✓L2

then  we have:

3 +

1

min◆ log2 d 

diameter(⇥)

Tr⇣IU (✓⇤)1⌘  

2L2
4



Tr⇣IU (✓⇤)1⌘1A1A  

1




I (x  ✓⇤)  I (x  ✓1)  I (x  ✓⇤) 

1


I (x  ✓⇤) 8 x 2 X

with probability greater than 1  .
4.2.3 Computing 1
Third  we are left with the task of obtaining a distribution 1 that minimizes the log likelihood error.
We now pose this optimization problem as an SDP.
From Lemmas 1 and 2  it is clear that we should aim to obtain a sampling distribution = ( ai
m2

[n]) minimizing Tr⇣I(✓1)1IU (✓1)⌘. Let IU (✓1) =Pj jvjvj> be the singular value decompo-
sition (svd) of IU (✓1). Since Tr⇣I(✓1)1IU (✓1)⌘ = Pd

j=1 jvj>I(✓1)1vj  this is equivalent

: i 2

6

to solving:

jcj

min
a c

s.t.8><>:
dXj=1
Among the above constraints  the constraint vj>S1vj  cj seems problematic. However  Schur
S  ⌫ 0   S ⌫ 0 and vj>S1vj  cj. In our case 
complement formula tells us that: cj
we know that S ⌫ 0  since it is a sum of positive semi deﬁnite matrices. The above argument proves
the following lemma.
Lemma 3. The following two optimization programs are equivalent:

S =Pi aiI(xi ✓ 1)
vj>S1vj  cj
Pi ai = m2.

ai 2 [0  1]

vj>

(4)

vj

mina
s.t.

TrS1IU (✓1)
S =Pi aiI(xi ✓ 1)
Pi ai = m2.

ai 2 [0  1]

mina c

s.t.

⌘

where IU (✓1) =Pj jvjvj> denotes the svd of IU (✓1).

5

Illustrative Examples

vj>

j=1 jcj

Pd
S =Pi aiI(xi ✓ 1)
 cj
S  ⌫ 0
Pi ai = m2 

vj
ai 2 [0  1]

We next present some examples that illustrate Theorem 1. We begin by showing that Condition 1 is
satisﬁed by the popular class of Generalized Linear Models.
5.1 Derivations for Generalized Linear Models

A generalized linear model is speciﬁed by three parameters – a linear model  a sufﬁcient statis-
tic  and a member of the exponential family. Let ⌘ be a linear model: ⌘ = ✓>X. Then  in a
Generalized Linear Model (GLM)  Y is drawn from an exponential family distribution with pa-
rameter ⌘. Speciﬁcally  p(Y = y|⌘) = e⌘>t(y)A(⌘)  where t(·) is the sufﬁcient statistic and
A(·) is the log-partition function. From properties of the exponential family  the log-likelihood
If we take ⌘ = ✓>x  and take the derivative with
is written as log p(y|⌘) = ⌘>t(y)  A(⌘).
respect to ✓  we have: @ log p(y|✓ x)
= xt(y)  xA0(✓>x). Taking derivatives again gives us
@2 log p(y|✓ x)

= xx>A00(✓>x)  which is independent of y.

@✓2

@✓

5.2 Speciﬁc Examples

We next present three illustrative examples of problems that our algorithm may be applied to.
Linear Regression. Our ﬁrst example is linear regression. In this case  x 2 Rd and Y 2 R are
generated according to the distribution: Y = ✓>⇤ X + ⌘  where ⌘ is a noise variable drawn from
In this case  the negative loglikelihood function is: L(y|x  ✓) = (y  ✓>x)2  and the
N (0  1).
corresponding Fisher information matrix I(x  ✓) is given as: I(x  ✓) = xx>. Observe that in this
(very special) case  the Fisher information matrix does not depend on ✓; as a result we can eliminate
nPi xixi> is the
the ﬁrst two steps of the algorithm  and proceed directly to step 3.
covariance matrix of U  then Theorem 1 tells us that we need to query labels from a distribution ⇤

If ⌃= 1

We illustrate the advantages of active learning through a simple example. Suppose U is the unla-
belled distribution:

with covariance matrix ⇤ such that Tr⇤1⌃ is minimized.
w.p. 1  d1
d2  
d2 for j 2{ 2 ···   d}  

xi =⇢ e1

ej w.p. 1

where ej is the standard unit vector in the jth direction. The covariance matrix ⌃ of U is a diagonal
matrix with ⌃11 = 1  d1
d2 for j  2. For passive learning over U  we query labels

d2 and ⌃jj = 1

7

of examples drawn from U which gives us a convergence rate of Tr(⌃1⌃)
= d
active learning chooses to sample examples from the distribution ⇤ such that

m

m. On the other hand 

1

2d  

ej w.p. ⇠ 1

2d for j 2{ 2 ···   d}  

d2  + (d  1) · 2d · 1

d2. This has a diagonal covariance matrix
2d for j  2  and convergence rate of Tr(⇤1⌃)
m  which does not grow with d!

xi =⇢ e1 w.p. ⇠ 1  d1
where ⇠ indicates that the probabilities hold upto O 1
2d and ⇤jj ⇠ 1
⇤ such that ⇤11 ⇠ 1  d1
d2⌘  4
m⇣ 2d
d+1 ·1  d1
Logistic Regression. Our second example is logistic regression for binary classiﬁcation. In this
case  x 2 Rd  Y 2 {1  1} and the negative log-likelihood function is: L(y|x  ✓) = log(1 +
ey✓>x)  and the corresponding Fisher information I(x  ✓) is given as: I(x  ✓) = e✓>x
(1+e✓>x)2 · xx>.
For illustration  suppose k✓⇤k2 and kxk2 are bounded by a constant and the covariance matrix ⌃ is
sandwiched between two multiples of identity in the PSD ordering i.e.  c
d I for some
constants c and C. Then the regularity assumptions 1 and 2 are satisﬁed for constant values of
L1  L2  L3 and L4. In this case  Theorem 1 states that choosing m1 to be !⇣Tr⇣IU (✓⇤)1⌘⌘ =

d I  ⌃  C

! (d) gives us the optimal convergence rate of (1 + o(1))

Tr(I⇤ (✓⇤)1IU (✓⇤))

⇠

m

m2

.

Fii =

Multinomial Logistic Regression. Our third example is multinomial logistic regression for multi-
class classiﬁcation. In this case  Y 2 1  . . .   K  x 2 Rd  and the parameter matrix ✓ 2 R(K1)⇥d.
The negative log-likelihood function is written as: L(y|x  ✓) = ✓>y x + log(1 +PK1
k=1 e✓>k x) 
if y 6= K  and L(y = k|x  ✓) = log(1 +PK1
k=1 e✓>k x) otherwise. The corresponding Fisher
information matrix is a (K  1)d ⇥ (K  1)d matrix  which is obtained as follows. Let F be the
(K  1) ⇥ (K  1) matrix with:
e✓>i x(1 +Pk6=i e✓>k x)
(1 +Pk e✓>k x)2

(1 +Pk e✓>k x)2

a constant and the covariance matrix ⌃ satisﬁes c

Similar to the example in the logistic regression case  suppose✓⇤y2 and kxk2 are bounded by
Since F ⇤ = diag (p⇤i )  p⇤p⇤>  where p⇤i = P (y = i|x  ✓⇤)  the boundedness of✓⇤y2 and kxk2
implies thatecI  F ⇤  eCI for some constantsec and eC (depending on K). This means that
d I  I(x  ✓⇤)  CeC
cec

d I and so the regularity assumptions 1 and 2 are satisﬁed with L1  L2  L3 and
L4 being constants. Theorem 1 again tells us that using !(d) samples in the ﬁrst step gives us the
optimal convergence rate of maximum likelihood error.

d I for some constants c and C.

Then  I(x  ✓) = F ⌦ xx>.

d I  ⌃  C

  Fij = 

e✓>i x+✓>j x

6 Conclusion

In this paper  we provide an active learning algorithm for maximum likelihood estimation which
provably achieves the optimal convergence rate (upto lower order terms) and uses only two rounds
of interaction. Our algorithm applies in a very general setting  which includes Generalized Linear
Models.
There are several avenues of future work. Our algorithm involves solving an SDP which is computa-
tionally expensive; an open question is whether there is a more efﬁcient  perhaps greedy  algorithm
that achieves the same rate. A second open question is whether it is possible to remove the with
replacement sampling assumption. A ﬁnal question is what happens if IU (✓⇤) has a high condition
number. In this case  our algorithm will require a large number of samples in the ﬁrst stage; an open
question is whether we can use a more sophisticated procedure in the ﬁrst stage to reduce the label
requirement.
Acknowledgements. KC thanks NSF under IIS 1162581 for research support.

8

References
[1] A. Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Pro-
ceedings of the 30th International Conference on Machine Learning  ICML 2013  Atlanta 
GA  USA  16-21 June 2013  pages 1220–1228  2013.

[2] M.-F. Balcan  A. Beygelzimer  and J. Langford. Agnostic active learning. J. Comput. Syst.

Sci.  75(1):78–89  2009.

[3] M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-

concave distributions. In COLT  2013.

[4] A. Beygelzimer  D. Hsu  J. Langford  and T. Zhang. Agnostic active learning without con-

straints. In NIPS  2010.

[5] L. Cam and G. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer Series in

Statistics. Springer New York  2000.

[6] J. Cornell. Experiments with Mixtures: Designs  Models  and the Analysis of Mixture Data

(third ed.). Wiley  2002.

[7] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS  2005.
[8] S. Dasgupta. Two faces of active learning. Theor. Comput. Sci.  412(19)  2011.
[9] S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In ICML  2008.
[10] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In

NIPS  2007.

[11] R. Frostig  R. Ge  S. M. Kakade  and A. Sidford. Competing with the empirical risk minimizer

in a single pass. arXiv preprint arXiv:1412.6606  2014.

[12] Q. Gu  T. Zhang  C. Ding  and J. Han. Selective labeling via error bound minimization. In In
Proc. of Advances in Neural Information Processing Systems (NIPS) 25  Lake Tahoe  Nevada 
United States  2012.

[13] Q. Gu  T. Zhang  and J. Han. Batch-mode active learning via error bound minimization. In

30th Conference on Uncertainty in Artiﬁcial Intelligence (UAI)  2014.

[14] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML  2007.
[15] M. K¨a¨ari¨ainen. Active learning in the non-realizable case. In ALT  2006.
[16] L. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer  1986.
[17] E. L. Lehmann and G. Casella. Theory of point estimation  volume 31. Springer Science &

Business Media  1998.

[18] R. D. Nowak. The geometry of generalized binary search. IEEE Transactions on Information

Theory  57(12):7893–7906  2011.

[19] S. Sabato and R. Munos. Active regression through stratiﬁcation. In NIPS  2014.
[20] R. Urner  S. Wulff  and S. Ben-David. Plal: Cluster-based active learning. In COLT  2013.
[21] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic

Mathematics. Cambridge University Press  2000.

[22] C. Zhang and K. Chaudhuri. Beyond disagreement-based agnostic active learning. In Proc. of

Neural Information Processing Systems  2014.

9

,Haim Cohen
Koby Crammer
Kamalika Chaudhuri
Sham Kakade
Praneeth Netrapalli
Sujay Sanghavi