2011,Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning,We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which  from a Bayesian perspective  is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights  thus inducing relation between tasks. This model unifies several sparse linear models  such as generalized linear models  sparse factor analysis and matrix factorization with missing values  so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression  multi-class classification  image processing applications and collaborative filtering.,Spike and Slab Variational Inference for Multi-Task

and Multiple Kernel Learning

Michalis K. Titsias

University of Manchester
mtitsias@gmail.com

Miguel L´azaro-Gredilla

Univ. de Cantabria & Univ. Carlos III de Madrid

miguel@tsc.uc3m.es

Abstract

We introduce a variational Bayesian inference algorithm which can be widely
applied to sparse linear models. The algorithm is based on the spike and slab prior
which  from a Bayesian perspective  is the golden standard for sparse inference.
We apply the method to a general multi-task and multiple kernel learning model
in which a common set of Gaussian process functions is linearly combined with
task-speciﬁc sparse weights  thus inducing relation between tasks. This model
uniﬁes several sparse linear models  such as generalized linear models  sparse
factor analysis and matrix factorization with missing values  so that the variational
algorithm can be applied to all these cases. We demonstrate our approach in multi-
output Gaussian process regression  multi-class classiﬁcation  image processing
applications and collaborative ﬁltering.

1

Introduction

Sparse inference has found numerous applications in statistics and machine learning [1  2  3]. It is a
generic idea that can be combined with popular models  such as linear regression  factor analysis and
more recently multi-task and multiple kernel learning models. In the regularization theory literature
sparse inference is tackled via (cid:96)1 regularization [2]  which requires expensive cross-validation for
model selection. From a Bayesian perspective  the spike and slab prior [1  4  5]  also called two-
groups prior [6]  is the golden standard for sparse linear models. However  the discrete nature of
the prior makes Bayesian inference a very challenging problem. Speciﬁcally  for M linear weights 
inference under a spike and slab prior distribution on those weights requires a combinatorial search
over 2M possible models. The problems found when working with the spike and slab prior led
several researchers to consider soft-sparse or shrinkage priors such as the Laplace and other related
scale mixtures of normals [3  7  8  9  10]. However  such priors are not ideal since they assign zero
probability mass to events associated with weights having zero value.
In this paper  we introduce a simple and efﬁcient variational inference algorithm based on the spike
and slab prior which can be widely applied to sparse linear models. The novel characteristic of this
algorithm is that the variational distribution over sparse weights has a factorial nature  i.e.  it can be
written as a mixture of 2M components where M is the number of weights. Unlike the standard
mean ﬁeld approximation which uses a unimodal variational distribution  our variational algorithm
can more precisely match the combinational nature of the posterior distribution over the weights.
We will show that the proposed variational approach is more accurate and robust to unfavorable
initializations than the standard mean ﬁeld variational approximation.
We apply the variational method to a general multi-task and multiple kernel learning model that
expresses the correlation between tasks by letting them share a common set of Gaussian process
latent functions. Each task is modeled by linearly combining these latent functions with task-
speciﬁc weights which are given a spike and slab prior distribution. This model is a spike and
slab Bayesian reformulation of previous Gaussian process-based single-task multiple kernel learning

1

methods [11  12  13] and multi-task Gaussian processes (GPs) [14  15  16  17]. Further  this model
uniﬁes several sparse linear models  such as generalized linear models  factor analysis  probabilistic
PCA and matrix factorization with missing values. In the experiments  we apply the variational in-
ference algorithms to all the above models and present results in multi-output regression  multi-class
classiﬁcation  image denoising  image inpainting and collaborative ﬁltering.

2 Spike and slab multi-task and multiple kernel learning

Section 2.1 discusses the spike and slab multi-task and multiple kernel learning (MTMKL) model
that linearly combines Gaussian process latent functions. Spike and slab factor analysis and proba-
bilistic PCA is discussed in Section 2.2  while missing values are dealt with in Section 2.3.

(1a)

(1b)

∀n q
∀q
∀q m
∀m.

2.1 The model
Let D = {X  Y}  with X ∈ RN×D and Y ∈ RN×Q  be a dataset such that the n-th row of X is
an input vector xn and the n-th row of Y is the set of Q corresponding tasks or outputs. We use
yq to refer to the q-th column of Y and ynq to the (n  q) entry. Outputs Y are then assumed to be
generated according to the following hierarchical Bayesian model:

ynq ∼ N (ynq|fq(xn)  σ2
q ) 

M(cid:88)

m=1

wqmφm(x) = w(cid:62)

fq(x) =
wqm ∼ πN (wqm|0  σ2
φm(x) ∼ GP(µm(x)  km(xi  xj)) 

q φ(x) 

w) + (1 − π)δ0(wqm) 

(1c)
(1d)
Here  each µm(x) is a mean function  km(xi  xj) a covariance function  wq = [wq1  . . .   wqM ](cid:62) 
φ(x) = [φ1(x)  . . .   φM (x)](cid:62) and δ0(wqm) denotes the Dirac delta function centered at zero. Since
each of the Q tasks is a linear combination of the same set of latent functions {φm(x)}M
m=1 (where
typically M < Q )  correlation is induced in the outputs. Sharing a common set of features means
that “knowledge transfer” between tasks can occur and latent functions are inferred more accurately 
since data belonging to all tasks are used.
Several linear models can be expressed as special cases of the above. For instance  a generalized
linear model is obtained when the GPs are Dirac delta measures (with zero covariance functions)
that deterministically assign each φm(x) to its mean function µm(x). However  the model in (1) has
a number of additional features not present in standard linear models. Firstly  the basis functions are
no longer deterministic  but they are instead drawn from different GPs  so an extra layer of ﬂexibility
is added to the model. Thus  a posterior distribution over the basis functions of the generalized linear
model can be inferred from data. Secondly  a truly sparse prior  the spike and slab prior (1c)  is
placed over the weights of the model. Speciﬁcally  with probability 1−π  each wqm is zero  and with
probability π  it is drawn from a Gaussian. This contrasts with previous approaches [3  7  8  9  13]
in which soft-sparse priors that assign zero probability mass to the weights being exactly zero were
used. Hyperparameters π and σ2
w are learnable in order to determine the amount of sparsity and
the discrepancy of nonzero weights  respectively. Thirdly  the number of basis functions M can be
inferred from data  since the sparse prior on the weights allows basis functions to be “switched off”
as necessary by setting the corresponding weights to zero.
Further  the model in (1) can be considered as a spike and slab Bayesian reformulation of multi-
task [14  15] and multiple kernel learning previous methods [11  12] that learn the weights using
maximum likelihood. By assuming the weights wq are given  each output function yq(x) is a GP
with covariance function

M(cid:88)

Cov[(yq(xi)  yq(xj)] =

w2

qmkm(xi  xj) 

which clearly consists of a conic combination of kernel functions. Therefore  the proposed model
can be reinterpreted as multiple kernel learning in which the weights of each kernel are assigned
spike and slab priors in a full Bayesian formulation.

m=1

2

2.2 Sparse factor and principal component analysis
An interesting case arises when µm(x) = 0 and km(xi  xj) = δij ∀m  where δij is the Kronecker
delta. This says that each latent function is drawn from a white process so that it consists of indepen-
dent values each following the standard normal distribution. We ﬁrst deﬁne matrices Φ ∈ RN×M
and W ∈ RQ×M   whose elements are  respectively  φnm = φm(xn) and wqm. Then  the model in
(1) reduces to

Y = ΦW(cid:62) + ξ 

w) + (1 − π)δ0(wqm) 

wqm ∼ πN (wqm|0  σ2
φnm ∼ N (φnm|0  1) 
ξnq ∼ N (ξnq|0  σ2
q ) 

(2a)
(2b)
(2c)
(2d)
where ξ is an N × Q noise matrix with entries ξnq. The resulting model thus corresponds to sparse
factor analysis or sparse probabilistic PCA (when the noise is homoscedastic  i.e.  σ2
q is constant for
all q). Observe that the sparse spike and slab prior is placed on the factor loadings W.

∀q m
∀n m
∀n q 

2.3 Missing values
The method can easily handle missing values and thus be applied to problems involving matrix
completion and collaborative ﬁltering. More precisely  in the presence of missing values we have
a binary matrix Z ∈ RN×Q that indicates the observed elements in Y. Using Z the likelihood in
q )  ∀n q s.t. [Z]nq = 1. In the experiments we
(1a) is modiﬁed according to ynq ∼ N (ynq|fq(xn)  σ2
consider missing values in applications such as image inpainting and collaborative ﬁltering.

3 Efﬁcient variational inference

that follows the probability distribution in eq. (1c). This allows to reparameterize wqm according to

The presence of the Dirac delta mass function makes the application of variational approximate
inference algorithms in spike and slab Bayesian models troublesome. However  there exists a sim-
ple reparameterization of the spike and slab prior that is more amenable to approximate inference
w) and a Bernoulli

methods. Speciﬁcally  assume a Gaussian random variable (cid:101)wqm ∼ N ((cid:101)wqm|0  σ2
random variable sqm ∼ πsqm (1 − π)1−sqm. The product sqm(cid:101)wqm forms a new random variable
wqm = sqm(cid:101)wqm and assign the above prior distributions on sqm and (cid:101)wqm. Thus  the reparameter-
sqm(cid:101)wqm. After the above reparameterization  a standard mean ﬁeld variational method that uses the
factorized variational distribution over(cid:102)W = {(cid:101)wq}Q
q=1 takes the form q((cid:102)W  S) =
(cid:81)Q
q=1 q((cid:101)wq  sq)  where

(3)
Notice that the presence of wqm in the likelihood function in (1a) is now replaced by the product

p((cid:101)wqm  sqm) = N (wqm|0  σ2

ized spike and slab prior takes the form:

w)πsqm (1 − π)1−sqm 

q=1 and S = {sq}Q

∀q m.

q((cid:101)wq  sq) = q((cid:101)wq)q(sq) = N ((cid:101)wq|µwq   Σwq )

M(cid:89)

qm (1 − γqm)1−sqm
γsqm

(4)

m=1

and where (µwq   Σwq   γq) are variational parameters. Such an approach has extensively used in [18]
and also considered in [19]. However  the above variational distribution leads to a very inefﬁcient
approximation. This is because (4) is a unimodal distribution  and therefore has limited capacity
when approximating the factorial true posterior distribution which can have exponentially many
modes. To analyze the nature of the true posterior distribution  we consider the following two
properties derived by assuming for simplicity a single output (Q = 1) so index q is dropped.

Property 1: The true marginal posterior p((cid:101)w|Y) can be written as a mixture distribution having 2M
components. This is an obvious fact since p((cid:101)w|Y) =(cid:80)
s p((cid:101)w|s  Y)p(s|Y)  where the summation
The second property characterizes the nature of each conditional p((cid:101)w|s  Y) in the above sum.
Property 2: Assume the conditional distribution p((cid:101)w|s  Y). We can write s = s1 ∪ s0  where

involves all 2M possible values of the binary vector s.

s1 denotes the elements in s with value one and s0 the elements with value zero. Using the

3

correspondence between s and (cid:101)w  we have (cid:101)w = (cid:101)w1 ∪ (cid:101)w0. Then  p((cid:101)w|s  Y) factorizes as
p((cid:101)w|s  Y) = p((cid:101)w1|Y)N ((cid:101)w0|0  σ2
wI|(cid:101)w0|)  which says that the posterior over (cid:101)w0 given s0 = 0
is equal to the prior over (cid:101)w0. This property is obvious because (cid:101)w0 and s0 appear in the likelihood
as an elementwise product (cid:101)w0 ◦ s0  thus when s0 = 0  (cid:101)w0 becomes disconnected from the data.
p((cid:101)w|Y)  which is a mixture with 2M components  with a single Gaussian distribution. Next we

The standard variational distribution in (4) ignores these properties and approximates the marginal

present an alternative variational approximation that takes into account the above properties.

3.1 The proposed variational method

In the reparameterized spike and slab prior  each pair of variables {(cid:101)wqm  sqm} is strongly correlated
mation must treat each pair {(cid:101)wqm  sqm} as a unit so that {(cid:101)wqm  sqm} are placed in the same factor

since their product is the underlying variable that interacts with the data. Thus  a sensible approxi-

of the variational distribution. The simplest factorization that achieves this is:

M(cid:89)

q((cid:101)wq  sq) =

q((cid:101)wqm  sqm).

(5)

m=1

a mixture of 2M components is obtained. Therefore  Property 1 is satisﬁed by (5). In turns out
that Property 2 is also satisﬁed. This can be shown by taking the stationary condition for the factor

This variational distribution yields a marginal q((cid:101)wq) which has 2M components. This can be seen by
writing q((cid:101)wq) =(cid:81)M
m=1 [q((cid:101)wqm  sqm = 1) + q((cid:101)wqm  sqm = 0)] and then by multiplying the terms
q((cid:101)wqm  sqm) when maximizing the variational lower bound (on the true marginal likelihood):
w)πsqm (1 − π)1−sqm
q((cid:101)wqm sqm)q(Θ)
where Θ are the remaining random variables in the model (i.e.  excluding {(cid:101)wqm  sqm}) and q(Θ)
their variational distribution. The stationary condition for q((cid:101)wqm  sqm) is

p(Y (cid:101)wqm  sqm  Θ)p(Θ)N ((cid:101)wqm|0  σ2
q((cid:101)wqm  sqm)q(Θ)

(cid:28)

(cid:29)

(6)

log

 

q((cid:101)wqm  sqm) =

Z e(cid:104)log p(Y (cid:101)wqm sqm Θ)(cid:105)q(Θ)N ((cid:101)wqm|0  σ2

1

w)πsqm (1 − π)1−sqm 

that does not depend on {(cid:101)wqm  sqm}.

where Z is a normalizing constant

have q((cid:101)wqm|sqm = 0) ∝ q((cid:101)wqm  sqm = 0) = C
Therefore  we
e(cid:104)log p(Y (cid:101)wqm sqm=0 Θ)(cid:105)q(Θ) is a constant that does not depend on (cid:101)wqm. From the last expression
w)(1 − π)  where C =
we obtain q((cid:101)wqm|sqm = 0) = N ((cid:101)wqm|0  σ2
slab probability models as long as the weights(cid:101)w and binary variables s interact inside the likelihood
function according to (cid:101)w ◦ s.

The above remarks regarding variational distribution (5) are general and can hold for many spike and

w) which implies that Property 2 is satisﬁed.

Z N ((cid:101)wqm|0  σ2

(7)

3.2 Application to the multi-task and multiple kernel learning model

Here  we brieﬂy discuss the variational method applied to the multi-task and multiple kernel model
described in Section 2.1 and refer to supplementary material for variational EM update equations.
The explicit form of the joint probability density function on the training data of model (1) is

p(Y (cid:102)W  S  Φ) = N (Y|Φ((cid:102)W◦S)(cid:62)  Σ)
N (φm|µm  Km) 
where {(cid:102)W  S  Φ} is the whole set of random variables that need to be marginalized out to compute

w)πsqm(1 − π)sqm(cid:3) M(cid:89)

(cid:2)N ((cid:101)wqm|0  σ2

the marginal likelihood. The marginal likelihood is analytically intractable  so we lower bound it
using the following variational distribution

(cid:89)

m=1

q m

q((cid:102)W  S  Φ) =

Q(cid:89)

M(cid:89)

M(cid:89)

q((cid:101)wqm  sqm)

q=1

m=1

m=1

4

q(φm).

(8)

w  π {θm}M

The stationary conditions of the lower bound result in analytical updates for all factors above. More

precisely  q(φm) is an N-dimensional Gaussian distribution and each factor q((cid:101)wqm  sqm) leads to
a marginal q((cid:101)wqm) which is a mixture of two Gaussians where one component is q((cid:101)wqm|sqm =
0) = N ((cid:101)wqm|0  σ2
w)  as shown in the previous section. The optimization proceeds using an EM
algorithm that at the E-step updates the factors in (8) and at the M-step updates hyperparameters
m=1} where θm parameterize kernel matrix Km. There is  however  one
{{σq}Q
surprise in these updates. The GP hyperparameters θm are strongly dependent on the factor q(φm)
of the corresponding GP latent vector  so updating θm by keeping ﬁxed the factor q(φm) exhibits
slow convergence. This problem is efﬁciently resolved by applying a Marginalized Variational step
[20] which jointly updates the pair (q(φm)  θm). This more advanced update together with all
remaining updates of the EM algorithm are discussed in detail in the supplementary material.
4 Assessing the accuracy of the approximation
In this section we compare the proposed variational inference method  in the following called
paired mean ﬁeld (PMF)  against the standard mean ﬁeld (MF) approximation. For simplicity 
we consider a single-output linear regression problem where the data are generated according to:

y = ((cid:101)w ◦ s)T x + ξ. Moreover  to remove the effect of hyperparameter learning from the com-
accuracy when approximating the true posterior mean value for the parameter vector wtr = E[(cid:101)w◦ s]

w) are ﬁxed to known values. The objective of the comparison is to measure the

where the expectation is under the true posterior distribution. wtr is obtained by running a very
long run of Gibbs sampling. PMF and MF provide alternative approximations wPMF and wMF  and
absolute errors between these approximations and wtr are used to measure accuracy. Since initial-
ization is crucial for variational non-convex algorithms  the accuracy of PMF and MF is averaged
over many random initializations of their respective variational distributions.

q=1  σ2

parison  (σ2  π  σ2

soft-error

soft-bound

extreme-error

extreme-bound

MF
PMF

0.917 [0.002 1.930]
0.208 [0.002 0.454]

-628.9 [-554.6 -793.5]
-560.7 [-557.8  -564.1]

1.880 [0.965  2.561]
0.204 [0.002  0.454]

-895.0 [-618.9 -1483.3]
-560.6 [-557.8  -564.0]

m=1 |wtr

m − wappr

for soft and extreme initializations. Average values for the variational lower bound are also shown.

Table 1: Comparison of MF and PMF in Boston-housing data in terms of approximating the ground-truth.
m |) together with 95% conﬁdence intervals (given by percentiles) are shown

Average errors ((cid:80)13
exactly the same principle as PMF. This Gibbs sampler iteratively samples the pair ((cid:101)wm  sm) from
the conditional p((cid:101)wm  sm|(cid:101)w\m  s\m  y) and has been observed to mix much faster than the standard
Gibbs sampler that samples (cid:101)w and s separately. More details about the paired Gibbs sampler are

For the purpose of the comparison we also derived an efﬁcient paired Gibbs sampler that follows

given in the supplementary material.
We considered the Boston-housing dataset which consists of 456 training examples and 13 inputs.
Hyperparameters were ﬁxed to values (σ2 = 0.1×var(y)  π = 0.25  σ2
w = 1) where var(y) denotes
the variance of the data. We performed two types of experiments each repeated 300 times. Each
repetition of the ﬁrst type uses a soft random initialization of each q(sm = 1) = γm from the range
(0  1). The second type uses an extreme random initialization so that each γm is initialized to either
0 or 1. For each run PMF and MF are initialized to the same variational parameters.
Table 1 reports average absolute errors and also average values of the variational lower bounds.
Clearly  PMF is more accurate than MF  achieves signiﬁcantly higher values for the lower bound
and exhibits smaller variance under different initializations. Further  for the more difﬁcult case
of extreme initializations the performance of MF becomes worse  while the performance of PMF
remains unchanged. This shows that optimization in PMF  although non-convex  is very robust to
unfavorable initializations. Similar experiments in other datasets have conﬁrmed the above remarks.

5 Experiments
Toy multi-output regression dataset. To illustrate the capabilities of the proposed model  we ﬁrst
apply it to a toy multi-output dataset with missing observations. Toy data is generated as follows:

5

Ten random latent functions are generated by sampling i.i.d. from zero-mean GPs with the following
non-stationary covariance function

(cid:16)−x2

i − x2
20

j

(cid:17)

k(xi  xj) = exp

(4 cos(0.5(xi − xj)) + cos(2(xi − xj))) 

at 201 evenly spaced points in the interval x ∈ [−10  10]. Ten tasks are then generated by adding
Gaussian noise with standard deviation 0.2 to those random latent functions  and two additional tasks
consist only of Gaussian noise with standard deviations 0.1 and 0.4. Finally  for each of the 12 tasks 
we artiﬁcially simulate missing data by removing 41 contiguous observations  as shown in Figure
1. Missing data are not available to any learning algorithm  and will be used to test performance
only. Note that the above covariance function is rank-4  so ten out of the twelve tasks will be related 
though we do not know how  or which ones.
All tasks are then learned using both independent GPs with squared exponential (SE) covariance
function kSE(xi  xj) = exp(−(xi − xj)2/(2(cid:96))) and the proposed MTMKL with M = 7 latent
functions  each of them also using the SE prior. Hyperparameter (cid:96)  as well as noise levels are
learned independently for each latent function. Figure 1 shows the inferred posterior means.

Figure 1: Twelve related tasks and predictions according to independent GPs (blue  continuous line) and
MTMKL (red  dashed line). Missing data for each task is represented using green circles.

The mean square error (MSE) between predictions and missing observations for each task are dis-
played in Table 2. MTMKL is able to infer how tasks are related and then exploit that information
to make much better predictions. After learning  only 4 out of the 7 available latent functions re-
main active  while the other ones are pruned by setting the corresponding weights to zero. This is in
correspondence with the generating covariance function  which only had 4 eigenfunctions  showing
how model order selection is automatic.

Method \ Task #
12
Independent GPs 6.51 11.70 7.52 2.49 1.53 18.25 0.41 7.43 2.73 1.81 19.93 93.80
2.83
MTMKL

2.09 0.41 1.96 1.90 1.57

4.57 7.71 1.94 1.98

1.20

9

10

2

3

4

5

1.97

6

7

8

11

1

Table 2: MSE performance of independent GPs vs. MTMKL on the missing observations for each task.

6

−10−8−6−4−20246810−2−1.5−1−0.500.511.522.5−10−8−6−4−20246810−2.5−2−1.5−1−0.500.511.52−10−8−6−4−20246810−1.5−1−0.500.511.5−10−8−6−4−20246810−2−101234−10−8−6−4−20246810−4−3−2−1012−10−8−6−4−20246810−3−2−10123−10−8−6−4−20246810−0.4−0.3−0.2−0.100.10.20.3−10−8−6−4−20246810−2−1.5−1−0.500.511.522.5−10−8−6−4−20246810−3−2−10123456−10−8−6−4−20246810−1.5−1−0.500.511.522.5−10−8−6−4−20246810−2.5−2−1.5−1−0.500.511.522.5−10−8−6−4−20246810−3−2.5−2−1.5−1−0.500.511.5Inferred noise standard deviations for the noise-only tasks are 0.10 and 0.45  and the average for the
remaining tasks is 0.22  which agrees well with the stated actual values.
The ﬂowers dataset. Though the proposed model has been designed as a tool for regression  it
can also be used approximately to solve classiﬁcation problems by using output values to identify
class membership. In this section we will apply it to the challenging ﬂower identiﬁcation problem
posed in [21]. There are 2040 instances of ﬂowers for training and 6149 for testing  mainly acquired
from the web  with varying scales  resolutions  etc.  which are labeled into 102 categories. In [21] 
four relevant features are identiﬁed: Color  histogram of gradient orientations and the scale invariant
feature transform  sampled on both the foreground region and its boundary. More information is
available at http://www.robots.ox.ac.uk/˜vgg/data/flowers/.
For this type of dataset  state of the art performance has been achieved using a weighted linear
combination of kernels (one per feature) in a support vector machine (SVM) classiﬁer. A different
set of weights is learned for each class. In [22] it is shown that these weights can be learned by
solving a convex optimization problem. I.e.  the standard approach to tackle the ﬂower classiﬁcation
problem would correspond to solving 102 independent binary classiﬁcation problems  each using a
linear combination of 4 kernels. We take a different approach: Since all the 102 binary classiﬁcation
tasks are related  we learn all of them at once as a multi-task multiple-kernel problem  hoping that
knowledge transfer between them will enhance performance.
For each training instance  we set the corresponding output to +1 for the desired task  whereas the
output for the remaining tasks is set to -1. Then we consider both using 10 and 13 latent functions
per feature (i.e.  M = 40 and M = 52). We measure performance in terms of the recognition
rate (RR)  which is the average of break-even points (where precision equals recall) for each class;
average area under the curve (AUC); and the multi-class accuracy (MA) which is the rate of correctly
classiﬁed instances. As baseline  recall that a random classiﬁer would yield a RR and AUC of 0.5
and a MA of 1/102 = 0.0098. Results are reported in Table 3.

Latent function # AUC on test set RR on test set MA on test set

Method
MTMKL
M = 40
MTMKL
M = 52
MKL from [21] M = 408
MKL from [13] M = 408

0.944
0.952

-

0.957

0.889
0.893
0.728

-

0.329
0.400

-
-

Table 3: Performance of the different multiple kernel learning algorithms on the ﬂowers dataset.

MTMKL signiﬁcantly outperforms the state-of-the-art method in [21]  yielding a performance in
line with [13]  due to its ability to share information across tasks.
Image denoising and dictionary learning. Here we illustrate denoising on the 256 × 256 “house”
image used in [19]. Three noise levels (standard deviations 15  25 and 50) are considered. Follow-
ing [19]  we partition the noisy image in 62 001 overlapping 8 × 8 blocks and regard each block
as a different task. MTMKL is then run using M = 64 “latent blocks”  also known as “dictionary
elements” (bigger dictionaries do not result in signiﬁcant performance increase). For the covariance
of the latent functions  we consider two possible choices: Either a white covariance function (as
in [19]) or an exponential covariance of the form kEXP(xi  xj) = e− |xi−xj|
  where x are the pixel
coordinates within each block. The ﬁrst option is equivalent to placing an independent standard nor-
mal prior on each pixel of the dictionary. The second one  on the other hand  introduces correlation
between neighboring pixels in the dictionary. Results are shown in Table 4. The exponential co-
variance clearly enhances performance and produces a more structured dictionary  as can be seen in
Figure 3.(a). The Peak-to-Signal Ratio (PSNR) obtained using the proposed approach is comparable
to the state-of-the-art results obtained in [19].
Image inpainting and dictionary learning. We now address the inpainting problem in color im-
ages. Following [19]  we consider a color image in which a random 80% of the RGB components
are missing. Using an analogous partitioning scheme as in the previous section we obtain 148 836
blocks of size 8× 8× 3  each of which is regarded as a different task. A dictionary size of M = 100
and a white covariance function (which is used in [19]) are selected. Note that we do not apply any
other preprocessing to data or any speciﬁc initialization as it is done in [19]. The PSNR of the image

(cid:96)

7

PSNR (dB)
Noise std Noisy image White
33.98
30.98
26.14

σ = 15
σ = 25
σ = 50

24.66
20.22
14.20

Expon.
34.29
31.88
28.08

Figure 2: Noisy “house” image with σ = 25 and re-
stored version using Exponential cov. function.

Table 4: PSNR for noisy and restored image using
several noise levels and covariance functions.

after it is restored using MTMKL is 28.94 dB  see Figure 3.(b). This result is similar to the results
reported in [19] and close to the state-of-the-art result of 29.65 dB achieved in [23].

(a) House: Dict. for white and Exponential

(b) Castle: Missing values  restored and original

Figure 3: Dictionaries inferred from noisy (σ = 25) “house” image; and “castle” inpainting results.

Collaborative ﬁltering. Finally  we performed an experiment on the 10M MovieLens data
set that consists of 10 million ratings for 71 567 users and 10 681 ﬁlms  with ratings ranging
{1  0.5  2  . . .   4.5  5}. We followed the setup in [24] and used the ra and rb partitions provided
with the database  that split the data into a training and testing  so that they are 10 ratings per user
in the test set. We applied the sparse factor analysis model (i.e. sparse PCA but with heteroscedastic
noise for the columns of the observation matrix Y which corresponds to ﬁlms) with M = 20 latent
dimensions. The RMSE for the ra partition was 0.88 for the rb partition was 0.85 so one average
0.865. This result is slightly better than 0.8740 RMSE reported in [24] using GP-LVM.
6 Discussion
In this work we have proposed a spike and slab multi-task and multiple kernel learning model. A
novel variational algorithm to perform inference in this model has been derived. The key contri-
bution in this regard that explains the good performance of the algorithm is the choice of a joint
distribution over ˜wqm and sqm in the variational posterior  as opposed to the usual independence
assumption. This has the effect of using exponentially many modes to approximate the posterior 
thus rendering it more accurate and much more robust to poor initializations of the variational pa-
rameters. The relevance and wide applicability of the proposed model has been illustrated by using
it on very diverse tasks: multi-output regression  multi-class classiﬁcation  image denoising  image
inpainting and collaborative ﬁltering. Prior structure beliefs were introduced in image dictionaries 
which is also a novel contribution to the best of our knowledge. Finally an interesting topic for future
research is to optimize the variational distribution proposed here with alternative approximate infer-
ence frameworks such as belief propagation or expectation propagation. This could allow to extend
current methodologies within such frameworks that assume unimodal approximations [25  26].
Acknowledgments
We thank the reviewers for insightful comments. MKT was supported by EPSRC Grant No
EP/F005687/1 “Gaussian Processes for Systems Identiﬁcation with Applications in Systems Bi-
ology”. MLG gratefully acknowledges funding from CAM project CCG10-UC3M/TIC-5511 and
CONSOLIDER-INGENIO 2010 CSD2008-00010 (COMONSENS).

8

References
[1] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal of the Ameri-

can Statistical Association  83(404):1023–1032  1988.

[2] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58:267–288  1994.

[3] M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning

Research  1:211–244  2001.

[4] E.I. George and R.E. Mcculloch. Variable selection via Gibbs sampling. Journal of the American Statis-

tical Association  88(423):881–889  1993.

[5] M. West. Bayesian factor regression models in the ”large p  small n” paradigm. In Bayesian Statistics 

pages 723–732. Oxford University Press  2003.

[6] B. Efron. Microarrays  empirical Bayes and the two-groups model. Statistical Science  23:1–22  2008.
[7] C. Archambeau and F. Bach. Sparse probabilistic projections. In D. Koller  D. Schuurmans  Y. Bengio 

and L. Bottou  editors  Advances in Neural Information Processing Systems 21  pages 73–80. 2009.

[8] F. Caron and A. Doucet. Sparse Bayesian nonparametric regression. In In 25th International Conference

on Machine Learning (ICML). ACM  2008.

[9] Matthias W. Seeger and Hannes Nickisch. Compressed sensing and Bayesian experimental design. In

ICML  pages 912–919  2008.

[10] C.M. Carvalho  N.G. Polson  and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika 

97:465–480  2010.

[11] T. Damoulas and M.A. Girolami. Probabilistic multi-class multi-kernel learning: on protein fold recogni-

tion and remote homology detection. Bioinformatics  24:1264–1270  2008.

[12] M. Christoudias  R. Urtasun  and T. Darrell. Bayesian localized multiple kernel learning. Technical report 

EECS Department  University of California  Berkeley  Jul 2009.

[13] C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 23 workshop on New Directions

in Multiple Kernel Learning. 2010.

[14] Y.W. Teh  M. Seeger  and M.I. Jordan. Semiparametric latent factor models.
International Workshop on Artiﬁcial Intelligence and Statistics  volume 10  2005.

In Proceedings of the

[15] E.V. Bonilla  K.M.A. Chai  and C.K.I. Williams. Multi-task Gaussian process prediction. In Advances

Neural Information Processing Systems 20  2008.

[16] P Boyle and M. Frean. Dependent Gaussian processes. In Advances in Neural Information Processing

Systems 17  pages 217–224. MIT Press  2005.

[17] M. Alvarez and N.D. Lawrence. Sparse convolved Gaussian processes for multi-output regression. In

Advances in Neural Information Processing Systems 20  pages 57–64  2008.

[18] R. Yoshida and M. West. Bayesian learning in sparse graphical factor models via variational mean-ﬁeld

annealing. Journal of Machine Learning Research  11:1771–1798  2010.

[19] M. Zhou  H. Chen  J. Paisley  L. Ren  G. Sapiro  and L. Carin. Non-parametric Bayesian dictionary
learning for sparse image represent ations. In Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams 
and A. Culotta  editors  Advances in Neural Information Processing Systems 22  pages 2295–2303. 2009.
In 28th
International Conference on Machine Learning (ICML-11)  pages 841–848  New York  NY  USA  June
2011. ACM.

[20] M. L´azaro-Gredilla and M. Titsias. Variational heteroscedastic Gaussian process regression.

[21] M.E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In

Proceedings of the Indian Conference on Computer Vision  Graphics and Image Processing  Dec 2008.

[22] M. Varma and D. Ray. Learning the discriminative power invariance trade-off. In International Confer-

ence on Computer Vision. 2007.

[23] J. Mairal  M. Elad  and G. Sapiro. Sparse representation for color image restoration. IEEE Trans. Image

Processing  17  2008.

[24] N.D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian processes. In Proceedings

of the 26th Annual International Conference on Machine Learning  pages 601–608  2009.

[25] K. Sharp and M. Rattray. Dense message passing for sparse principal component analysis.

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  pages 725–732  2010.

In 13th

[26] J.M. Hern´andez-Lobato  D. Hern´andez-Lobato  and A. Su´arez. Network-based sparse Bayesian classiﬁ-

cation. Pattern Recognition  44(4):886–900  2011.

9

,Christopher Choy
JunYoung Gwak
Silvio Savarese
Manmohan Chandraker
Gregory Plumb
Denali Molitor
Ameet Talwalkar