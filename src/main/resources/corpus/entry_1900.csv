2015,Exactness of Approximate MAP Inference in Continuous MRFs,Computing the MAP assignment in graphical models is generally intractable.  As a result  for discrete graphical models  the MAP problem is often approximated using linear programming relaxations.  Much research has focused on characterizing when these LP relaxations are tight  and while they are relatively well-understood in the discrete case  only a few results are known for their continuous analog.  In this work  we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight.  We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models.  We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.,Exactness of Approximate MAP Inference in

Continuous MRFs

Department of Computer Science

University of Texas at Dallas

Nicholas Ruozzi

Richardson  TX 75080

Abstract

Computing the MAP assignment in graphical models is generally intractable. As a
result  for discrete graphical models  the MAP problem is often approximated us-
ing linear programming relaxations. Much research has focused on characterizing
when these LP relaxations are tight  and while they are relatively well-understood
in the discrete case  only a few results are known for their continuous analog.
In this work  we use graph covers to provide necessary and sufﬁcient conditions
for continuous MAP relaxations to be tight. We use this characterization to give
simple proofs that the relaxation is tight for log-concave decomposable and log-
supermodular decomposable models. We conclude by exploring the relationship
between these two seemingly distinct classes of functions and providing speciﬁc
conditions under which the MAP relaxation can and cannot be tight.

1

Introduction

Graphical models are a popular modeling tool for both discrete and continuous distributions. We are
commonly interested in one of two inference tasks in graphical models: ﬁnding the most probable
assignment (a.k.a.  MAP inference) and computing marginal distributions. These problems are NP-
hard in general  and a variety of approximate inference schemes are used in practice.
In this work  we will focus on approximate MAP inference. For discrete state spaces  linear pro-
gramming relaxations of the MAP problem (speciﬁcally  the MAP LP) are quite common [1; 2; 3].
These relaxations replace global marginalization constraints with a collection of local marginaliza-
tion constraints. Wald and Globerson [4] refer to these as local consistency relaxations (LCRs). The
advantage of LCRs is that they are often much easier to specify and to optimize over (e.g.  by using
a message-passing algorithm such as loopy belief propagation (LBP)). However  the analogous re-
laxations for continuous state spaces may not be compactly speciﬁed and can lead to an unbounded
number of constraints (except in certain special cases). To overcome this problem  further relax-
ations have been proposed [5; 4]. By construction  each of these further relaxations can only be tight
if the initial LCR was tight. As a result  there are compelling theoretical and algorithmic reasons to
investigate when LCRs are tight.
Among the most well-studied continuous models are the Gaussian graphical models. For this class
of models  it is known that the continuous MAP relaxation is tight when the corresponding inverse
covariance matrix is positive deﬁnite and scaled diagonally dominant (a special case of the so-called
log-concave decomposable models)[4; 6; 7]. In addition  LBP is known to converge to the correct
solution for Gaussian graphical models and log-concave decomposable models that satisfy a scaled
diagonal dominance condition [8; 9]. While much of the prior work in this domain has focused on
log-concave graphical models  in this work  we provide a general necessary and sufﬁcient condition
for the continuous MAP relaxation to be tight. This condition mirrors the known results for the
discrete case and is based on the notion of graph covers: the MAP LP is tight if and only if the

1

optimal solution to the MAP problem is an upper bound on the MAP solution over any graph cover 
appropriately scaled. This characterization will allow us to understand when the MAP relaxation is
tight for more general models.
Apart from this characterization theorem  the primary goal of this work is to move towards a uni-
form treatment of the discrete and continuous cases; they are not as different as they may initially
appear. To this end  we explore the relationship between log-concave decomposable models and log-
supermodular decomposable models (introduced here in the continuous case). Log-supermodular
models provide an example of continuous graphical models for which the MAP relaxation is tight 
but the objective function is not necessarily log-concave. These two concepts have analogs in dis-
crete state spaces. In particular  log-concave decomposability is related to log-concave closures of
discrete functions and log-supermodular decomposability is a known condition which guarantees
that the MAP LP is exact in the discrete setting. We prove a number of results that highlight the
similarities and differences between these two concepts as well as a general condition under which
the MAP relaxation corresponding to a pairwise twice continuously differentiable model cannot be
tight.

2 Prerequisites
Let f : X n → R≥0 be a non-negative function where X is the set of possible assignments of each
variable. A function f factors with respect to a hypergraph G = (V A)  if there exist potential
functions fi : X → R≥0 for each i ∈ V and fα : X |α| → R≥0 for each α ∈ A such that

f (x1  . . .   xn) =

fi(xi)

fα(xα).

(cid:89)

i∈V

(cid:89)

α∈A

The hypergraph G together with the potential functions fi∈V and fα∈A deﬁne a graphical model.
In general  this MAP inference task is NP-hard 
We are interested computing supx∈X n f G(x).
but in practice  local message-passing algorithms based on approximations from statistical physics 
such as LBP  produce reasonable estimates in many settings. Much effort has been invested into
understanding when LBP solves the MAP problem. In this section  we brieﬂy review approximate
MAP inference in the discrete setting (i.e.  when X is a ﬁnite set). For simplicity and consistency  we
will focus on log-linear models as in [4]. Given a vector of sufﬁcient statistics φi(xi) ∈ Rk for each
i ∈ V and xi ∈ X and a parameter vector θi ∈ Rk  we will assume that fi(xi) = exp ((cid:104)θi  φi(xi)(cid:105)) .
Similarly  given a vector of sufﬁcient statistics φα(xα) for each α ∈ A and xα ∈ X |α| and a
parameter vector θα  we will assume that fα(xα) = exp ((cid:104)θα  φα(xα)(cid:105)) . We will write φ(x) to
represent the concatenation of the individual sufﬁcient statistics and θ to represent the concatenation
of the parameters. The objective function can then be expressed as f G(x) = exp ((cid:104)θ  φ(x)(cid:105)) .

2.1 The MAP LP relaxation

The MAP problem can be formulated in terms of mean parameters [10].

sup
x∈X n

log f (x) = sup
µ∈M

(cid:104)θ  µ(cid:105)

M = {µ ∈ Rm : ∃τ ∈ ∆ s.t. Eτ [φ(x)] = µ}

where ∆ is the space of all densities over X n and M is the set of all realizable mean parameters.
In general  M is a difﬁcult object to compactly describe and to optimize over. As a result  one
typically constructs convex outerbounds on M that are more manageable. In the case that X is ﬁnite 
one such outerbound is given by the MAP LP. For each i ∈ V and k ∈ X   deﬁne φi(xi)k (cid:44) 1{xi=k}.
Similarly  for each α ∈ A and k ∈ X |α|  deﬁne φα(xα)k (cid:44) 1{xα=k}. With this choice of sufﬁcient
statistics  M is equivalent to the set of all marginal distributions over the individual variables and
elements of A that arise from some joint probability distribution. The MAP LP is obtained by
replacing M with a relaxation that only enforces local consistency constraints.

(cid:80)
The set of constraints  ML  is known as the local marginal polytope. The approximate MAP prob-
lem is then to compute maxµ∈ML(cid:104)θ  µ(cid:105).

for all α ∈ A  i ∈ α  xi ∈ X

xα\{i} µα(xα) = µi(xi) 

for all i ∈ V

µi(xi) = 1 

xi

ML =

µ ≥ 0 :

(cid:80)

(cid:40)

(cid:41)

2

1  2  3

1  4

2  3  4

1  2  3

1  4

2  3  4

2  3  4

1  4

1  2  3

1

2

3

4

2

3

1

4

4

1

3

2

(a) A hypergraph graph  G.

(b) One possible 2-cover of G.

Figure 1: An example of a graph cover of a factor graph. The nodes in the cover are labeled for the
node that they copy in the base graph.

2.2 Graph covers

In this work  we are interested in understanding when this relaxation is tight (i.e.  when does
supµ∈ML(cid:104)θ  µ(cid:105) = supx∈X n log f (x)). For discrete MRFs  the MAP LP is known to be tight in
a variety of different settings [11; 12; 13; 14]. Two different theoretical tools are often used to inves-
tigate the tightness of the MAP LP: duality and graph covers. Duality has been particularly useful in
the design of convergent and correct message-passing schemes that solve the MAP LP [1; 15; 2; 16].
Graph covers provide a theoretical framework for understanding when and why message-passing al-
gorithms such as belief propagation fail to solve the MAP problem [17; 18; 3].
Deﬁnition 2.1. A graph H covers a graph G = (V  E) if there exists a graph homomorphism
h : H → G such that for all vertices i ∈ G and all j ∈ h−1(i)  h maps the neighborhood ∂j of j in
H bijectively to the neighborhood ∂i of i in G.

If a graph H covers a graph G  then H looks locally the same as G. In particular  local message-
passing algorithms such as LBP have difﬁculty distinguishing a graph and its covers. If h(j) = i 
then we say that j ∈ H is a copy of i ∈ G. Further  H is said to be an M-cover of G if every vertex
of G has exactly M copies in H.
This deﬁnition can be easily extended to hypergraphs. Each hypergraph G can be represented in
factor graph form: create a node in the factor graph for each vertex (called variable nodes) and each
hyperedge (called factor nodes) of G. Each factor node is connected via an edge in the factor graph
to the variable nodes on which the corresponding hyperedge depends. For an example of a 2-cover 
see Figure 1.
To any M-cover H = (V H  AH ) of G given by the homomorphism h  we can associate a collection
the potential at node i ∈ V H is equal to fh(i)  the potential at node h(i) ∈ G 
of potentials:
and for each β ∈ AH  we associate the potential fh(β). In this way  we can construct a function
f H : X M|V | → R≥0 such that f H factorizes over H. We will say that the graphical model H is an
M-cover of the graphical model G whenever H is an M-cover of G and f H is chosen as described
is the mth
above. It will be convenient in the sequel to write f H (xH ) = f H (x1  . . .   xM ) where xm
copy of variable i ∈ V .
i
There is a direct correspondence between µ ∈ ML and assignments on graph covers. This corre-
spondence is the basis of the following theorem.
Theorem 2.2 (Ruozzi and Tatikonda 3).

sup
µ∈ML

(cid:104)θ  µ(cid:105) = sup

M

sup

H∈CM (G)

sup
xH

1
M

log f H (xH )

where CM (G) is the set of all M-covers of G.

Theorem 2.2 claims that the optimal value of the MAP LP is equal to the supremum over all MAP
assignments over all graph covers  appropriately scaled. In particular  the proof of this result shows
that  under mild conditions  there exists an M-cover H of G and an assignment xH such that
M log f H (xH ) = supµ∈ML(cid:104)θ  µ(cid:105).
1

3 Continuous MRFs

In this section  we will describe how to extend the previous results from discrete to continuous MRFs
(i.e.  X = R) using graph covers. The relaxation that we consider here is the appropriate extension

3

of the MAP LP where each of the sums are replaced by integrals [4].

µ :

(cid:82) τα(xα)dxα\i = τi(xi) 

∃ densities τi  τα s.t.
µi = Eτi [φi] 
µα = Eτα [φα] 

ML =

for all α ∈ A  i ∈ α  xi ∈ X
for all i ∈ V
for all α ∈ A



Our goal is to understand under what conditions this continuous relaxation is tight. Wald and Glober-
son [4] have approached this problem by introducing a further relaxation of ML which they call the
weak local consistency relaxation (weak LCR). They provide conditions under which the weak LCR
(and hence the above relaxation) is tight. In particular  they show that weak LCR is tight for the class
of log-concave decomposable models. In this work  we take a different approach. We ﬁrst prove
the analog of Theorem 2.2 in the continuous case and then we show that the known conditions that
guarantee tightness of the continuous relaxation are simple consequences of this general theorem.
Theorem 3.1.

(cid:104)θ  µ(cid:105) = sup

sup
µ∈ML

sup

H∈CM (G)

sup
xH

M

1
M

log f H (xH )

where CM (G) is the set of all M-covers of G.

The proof of Theorem 3.1 is conceptually straightforward  albeit technical  and can be found in
Appendix A. The proof approximates the expectations in ML as expectations with respect to sim-
ple functions  applies the known results for ﬁnite spaces  and takes the appropriate limit. Like its
discrete counterpart  Theorem 3.1 provides necessary and sufﬁcient conditions for the continuous
relaxation to be tight. In particular  for the relaxation to be tight  the optimal solution on any M-
cover  appropriately scaled  cannot exceed the value of the optimal solution of the MAP problem
over G.

3.1 Tightness of the MAP relaxation

Theorem 3.1 provides necessary and sufﬁcient conditions for the tightness of the continuous re-
laxation. However  checking that the maximum value attained on any M-cover is bounded by the
maximum value over the base graph to the M  in and of itself  appears to be a daunting task. In
this section  we describe two families of graphical models for which this condition is easy to ver-
ify: the log-concave decomposable functions and the log-supermodular decomposable functions.
Log-concave decomposability has been studied before  particularly in the case of Gaussian graphi-
cal models. Log-supermodularity with respect to graphical models  however  appears to have been
primarily studied in the discrete case.

3.1.1 Log-concave decomposability
A function f : Rn → R≥0 is log-concave if f (x)λf (y)1−λ ≤ f (λx + (1 − λ)y) for all x  y ∈ Rn
and all λ ∈ [0  1]. If f can be written as a product of log-concave potentials over a hypergraph G 
we say that f is log-concave decomposable over G.
Theorem 3.2. If f is log-concave decomposable  then supx log f (x) = supµ∈ML(cid:104)θ  µ(cid:105).
Proof. By log-concave decomposability  for any M-cover H of G 

f H (x1  . . .   xM ) ≤ f G

(cid:18) x1 + ··· + xM

(cid:19)M

 

M

which we obtain by applying the deﬁnition of log-concavity separately to each of the M copies of
the potential functions for each node and factor of G. As a result  supx1 ... xM f H (x1  . . .   xM ) ≤
supx f G(x)M . The proof of the theorem then follows by applying Theorem 3.1.

Wald and Globerson [4] provide a different proof of Theorem 3.2 by exploiting duality and the weak
LCR.

4

f H (x1  . . .   xM ) ≤ M(cid:89)
(cid:81)M

m=1

f G(zm(x1  . . .   xM )).

3.1.2 Log-supermodular decomposability

Log-supermodular functions have played an important role in the study of discrete graphical models 
and log-supermodularity arises in a number of classical correlations inequalities (e.g.  the FKG
inequality). For log-supermodular decomposable models  the MAP LP is tight and the MAP problem
can be solved exactly in polynomial time [19; 20]. In the continuous case  log-supermodularity is
deﬁned analogously to the discrete case. That is  f : Rn → R≥0 is log-supermodular if f (x)f (y) ≤
f (x ∧ y)f (x ∨ y) for all x  y ∈ Rn  where x ∨ y is the componentwise maximum of the vectors
x and y and x ∧ y is the componentwise minimum. Continuous log-supermodular functions are
sometimes said to be multivariate totally positive of order two [21]. We will say that a graphical
model is log-supermodular decomposable if f can be factorized as a product of log-supermodular
potentials.
For any collection of vectors x1  . . .   xk ∈ Rn  let zi(x1  . . .   xk) be the vector whose jth compo-
nent is the ith largest element of x1
Theorem 3.3. If f is log-supermodular decomposable  then supx log f (x) = supµ∈ML(cid:104)θ  µ(cid:105).
Proof. By log-supermodular decomposability  for any M-cover H of G 

j for each j ∈ {1  . . .   n}.

j   . . .   xk

Again  this follows by repeatedly applying the deﬁnition of log-supermodularity separately to
each of the M copies of the potential functions for each node and factor of G. As a result 
supx1 ... xM f H (x1  . . .   xM ) ≤ supx1 ... xM
m=1 f G(xm). The proof of the theorem then fol-
lows by applying Theorem 3.1.

4 Log-supermodular decomposability vs. log-concave decomposability

As discussed above  log-concave decomposable and log-supermodular decomposable models are
both examples of continuous graphical models for which the MAP relaxation is tight. These two
classes are not equivalent: twice continuously differentiable functions are supermodular if and only
if all off diagonal elements of the Hessian matrix are non-negative. Contrast this with twice con-
tinuously differentiable concave functions where the Hessian matrix must be negative semideﬁnite.
In particular  this means that log-supermodular functions can be multimodel. In this section  we
explore the relationship between log-supermodularity and log-concavity.

4.1 Gaussian MRFs

We begin with the case of Gaussian graphical models  i.e.  pairwise graphical models given by
exp (−Aijxixj)

f (x) ∝ =(cid:0)−1/2xT Ax + bT x(cid:1) =

Aiix2

exp

i + bixi

(cid:19) (cid:89)

(i j)∈E

(cid:89)

i∈V

(cid:18)

− 1
2

for some symmetric positive deﬁnite matrix A ∈ Rn×n and vector b ∈ Rn. Here  f factors over the
graph G corresponding to the non-zero entries of the matrix A.
Gaussian graphical models are a relatively well-studied class of continuous graphical models.
In fact  sufﬁcient conditions for the convergence and correctness of Gaussian belief propagation
(GaBP) are known for these models. Speciﬁcally  GaBP converges to the optimal solution if the
positive deﬁnite matrix A is walk-summable  scaled diagonally dominant  or log-concave decom-
posable [22; 7; 8; 9]. These conditions are known to be equivalent [23; 6].
Deﬁnition 4.1. Γ ∈ Rn×n is scaled diagonally dominant if ∃w ∈ Rn  w > 0 such that |Γii|wi >

(cid:80)
j(cid:54)=i |Γij|wj.

In addition  the following theorem provides a characterization of scaled diagonal dominance (and
hence log-concave decomposability) in terms of graph covers for these models.
Theorem 4.2 (Ruozzi and Tatikonda 6). Let A be a symmetric positive deﬁnite matrix. The following
are equivalent.

5

1. A is scaled diagonally dominant.

2. All covers of A are positive deﬁnite.

3. All 2-covers of A are positive deﬁnite.

The proof of this theorem constructs a speciﬁc 2-cover whose covariance matrix has negative eigen-
values whenever the matrix A is positive deﬁnite but not scaled diagonally dominant. The joint
distribution corresponding to this 2-cover is not bounded from above  so the optimal value of the
MAP relaxation is +∞ as per Theorem 3.1.
For Gaussian graphical models  log-concave decomposability and log-supermodular decomposabil-
ity are related: every positive deﬁnite  log-supermodular decomposable model is log-concave de-
composable  and every positive deﬁnite  log-concave decomposable model is a signed version of
some positive deﬁnite  log-supermodular decomposable Gaussian graphical model. This follows
from the following simple lemma.
Lemma 4.3. A symmetric positive deﬁnite matrix A is scaled diagonally dominant if and only if the
matrix B such that Bii = Aii for all i and Bij = −|Aij| for all i (cid:54)= j is positive deﬁnite.
If A is positive deﬁnite and scaled diagonally dominant  then the model is log-concave decompos-
able. In contrast  the model would be log-supermodular decomposable if all of the off-diagonal ele-
ments of A were negative  independent of the diagonal. In particular  the diagonal could have both
positive and negative elements  meaning that f (x) could be either log-concave  log-convex  or nei-
ther. As log-convex quadratic forms do not correspond to normalizable Gaussian graphical models 
the log-convex case appears to be less interesting as the MAP problem is unbounded from above.
However  the situation is entirely different for constrained (over some convex set) log-quadratic
maximization. As an example  consider a box constrained log-quadratic maximization problem
where the matrix A has all negative off-diagonal entries. Such a model is always log-supermodular
decomposable. Hence  the MAP relaxation is tight  but the model is not necessarily log-concave.

4.2 Pairwise twice differentiable MRFs

All of the results from the previous section can be extended to general twice continuously differen-
tiable functions over pairwise graphical models (i.e.  |α| = 2 for all α ∈ A). In this section  unless
otherwise speciﬁed  assume that all models are pairwise.
Theorem 4.4. If log f (x) is strictly concave and twice continuously differentiable  the following are
equivalent.

1. ∇2(log f )(x) is scaled diagonally dominant for all x.
2. ∇2(log f H )(xH ) is negative deﬁnite for every graph cover H of G and every xH.
3. ∇2(log f H )(xH ) is negative deﬁnite for every 2-cover H of G and every xH.

The equivalence of 1-3 in Theorem 4.4 follows from Theorem 4.2.
Corollary 4.5. If ∇2(log f )(x) is scaled diagonally dominant for all x  then the continuous MAP
relaxation is tight.
Corollary 4.6. If f is log-concave decomposable over a pairwise graphical model and strictly log-
concave  then ∇2(log f )(x) is scaled diagonally dominant for all x.
Whether or not log-concave decomposability is equivalent to the other conditions listed in the state-
ment of Theorem 4.4 remains an open question (though we conjecture that this is the case). Similar
ideas can be extended to general twice continuously differentiable functions.
Theorem 4.7. Suppose log f (x) is twice continuously differentiable with a maximum at x∗. Let
Bij = |∇2(log f )(x∗)ij| for all i (cid:54)= j and Bii = ∇2(log f )(x∗)ii. If f admits a pairwise factoriza-
tion over G and B has both positive and negative eigenvalues  then the continuous MAP relaxation
is not tight.

Proof. If B has both positive and negative eigenvalues  then there exists a 2-cover H of G such that
∇2(log f H )(x∗  x∗) has both positive and negative eigenvalues. As a result  the lift of x∗ to the

6

2-cover f H is a saddle point. Consequently  f H (x∗  x∗) < supxH f H (xH ). By Theorem 3.1  the
continuous MAP relaxation cannot be tight.

If ∇2(log f ) is positive deﬁnite but not scaled diagonally
This negative result is quite general.
dominant at any global optimum  then the MAP relaxation is not tight. In particular  this means that
all log-supermodular decomposable functions that meet the conditions of the theorem must be s.d.d.
at their optima.
Algorithmically  Moallemi and Van Roy [9] argued that belief propagation converges for models
that are log-concave decomposable and scaled diagonally dominant. It is unknown whether or not a
similar convergence argument applies to log-supermodular decomposable functions.

4.3 Concave closures

Many of the tightness results in the discrete case can be seen as a speciﬁc case of the continuous
results described above. Again  suppose that X ⊂ R is a ﬁnite set.
Deﬁnition 4.8. The concave closure of a function g : X n → R ∪ {−∞} at x ∈ Rn is given by

λ(y)g(y) :(cid:80)

y λ(y) = 1 (cid:80)

y λ(y)y = x  λ(y) ≥ 0

g(x) = sup

(cid:88)

y∈X n



Equivalently  the concave closure of a function is the smallest concave function such that g(x) ≤
g(x) for all x. A function and its concave closure must necessarily have the same maximum. Com-
puting the concave (or convex) closure of a function is NP-hard in general  but it can be efﬁciently
computed for certain special classes of discrete functions. In particular  when X = {0  1} and log f
is supermodular  then its concave closure can be computed in polynomial time as it is equal to the
Lov´asz extension of log f. The Lov´asz extension has a number of interesting properties. Most
notably  it is linear (the Lov´asz extension of a sum of functions is equal to sum of the Lov´asz ex-
tensions). Deﬁne the log-concave closure of f to be ˆf (x) = exp(log f (x)). As a result  if f is
log-supermodular decomposable  then ˆf is log-concave decomposable.

(cid:81)
α∈A ˆfα  then supx∈X n f (x) =(cid:80)

This theorem is a direct consequence of Theorem 3.2. For example  the tightness results of Bayati
et al. [11] and Sanghavi et al. [14] (and indeed many others) can be seen as a special case of this
theorem. Even when |X| is not ﬁnite  the concave closure can be similarly deﬁned  and the the-
orem holds in this case as well. Given the characterization in the discrete case  this suggests that
there could be a  possibly deep  connection between log-concave closures and log-supermodular
decomposability.

Theorem 4.9. If ˆf =(cid:81)

i∈V

ˆfi

(cid:104)θ  µ(cid:105).

µ∈ML

5 Discussion

We have demonstrated that the same necessary and sufﬁcient condition based on graph covers for
the tightness of the MAP LP in the discrete case translates seamlessly to the continuous case. This
characterization allowed us to provide simple proofs of the tightness of the MAP relaxation for log-
concave decomposable and log-supermodular decomposable models. While the proof of Theorem
3.1 is nontrivial  it provides a powerful tool to reason about the tightness of MAP relaxations. We
also explored the intricate relationship between log-concave and log-supermodular decomposablity
in both the discrete and continuous cases which provided intuition about when the MAP relaxation
can or cannot be tight for pairwise graphical models.

A Proof of Theorem 3.1

The proof of this theorem proceeds in two parts. First  we will argue that

sup
µ∈ML

(cid:104)θ  µ(cid:105) ≥ sup

M

sup

H∈CM (G)

sup
xH

1
M

log f H (xH ).

To see this  ﬁx an M-cover  H  of G via the homomorphism h and consider any assignment xH.
Construct the mean parameters µ(cid:48) ∈ ML as follows.

7

(cid:88)
(cid:88)

j∈V (H):h(j)=i

β∈A(H):h(β)=α

δ(xH

j − xi)
β − xα)

δ(xH

τi(xi) =

τα(xα) =

1
M
1
M

(cid:90)
(cid:90)

µ(cid:48)
i =

µ(cid:48)
α =

τi(xi)φi(xi)dxi

τα(xα)φα(xα)dxα

Here  δ(·) is the Dirac delta function1. This implies that

1
M

log f H (xH ) = (cid:104)θ  µ(cid:48)(cid:105) ≤ sup
µ∈ML

(cid:104)θ  µ(cid:105).

(cid:26) 2s(cid:82)

For the other direction  ﬁx some µ(cid:48) ∈ ML such that µ(cid:48) is generated by the vector of densities τ.
We will prove the result for locally consistent probability distributions with bounded support. The
result for arbitrary τ will then follow by constructing sequences of these distributions that converge
(in measure) to τ. For simplicity  we will assume that each potential function is strictly positive2.
Consider the space [−t  t]|V | for some positive integer t. We will consider local probability dis-
tributions that are supported on subsets of this space. That is  supp(τi) ⊆ [−t  t] for each i and
supp(τα) ⊆ [−t  t]|α| for each α. For a ﬁxed positive integer s  divide the interval [−t  t] into 2s+1t
intervals of size 1/2s and let Sk denote the kth interval. This partitioning divides [−t  t]|V | into dis-
joint cubes of volume 1/2s|V |. The distribution τ can be approximated as a sequence of distributions
τ 1  τ 2  . . . as follows. Deﬁne a vector of approximate densities τ s by setting

Sk

τi(xi)dxi 

if x(cid:48)

i ∈ Sk
0  otherwise

α ∈(cid:81)

if x(cid:48)

kj :j∈α Skj

(cid:40)

i) (cid:44)

2|α|s(cid:82)(cid:81)
i (x(cid:48)
τ s
(cid:82)
α(xα)φα(xα)dxα → µ(cid:48)

[−t t] τ s

α(x(cid:48)
τ s

(cid:82)

τα(xα)dxα 

0  otherwise

kj :j∈α Skj
i (xi)φi(xi)dxi → µ(cid:48)

α) (cid:44)
We have τ s → τ 
[−t t]|α| τ s
The continuous MAP relaxation for local probability distributions of this form can be expressed in
terms of discrete variables over X = {1  . . .   2s+1t}. To see this  deﬁne µs
τ s
i (xi)dxi
α(xα)dxα for each zα ∈ {1  . . .   2s+1t}|α|. The
for each zi ∈ {1  . . .   2s+1t} and µs
τ s
(cid:88)
(cid:88)
corresponding MAP LP objective  evaluated at µs  is then

α(zα) =(cid:82)

i (zi) =(cid:82)

α for each α ∈ A(G).

for each i

(cid:88)

(cid:88)

V (G) 

(cid:90)

(cid:90)

and

∈

Szα

Szi

i

2|α|s log fα(xα)dxα.

(1)

µs

i (zi)

2s log fi(xi)dxi +

µs

α(zα)

This MAP LP objective corresponds to a discrete graphical model that factors over the hypergraph
G  with potential functions corresponding to the above integrals over the partitions indexed by the
vector z.

i∈V

zi

gs(z) ∝ (cid:89)
(cid:89)

i∈V (G)

=

i∈V (G)

exp

exp

Szi

(cid:32)(cid:90)
(cid:18)(cid:90)

Szi

Sz

α∈A

zα

(cid:33) (cid:89)
(cid:19) (cid:89)

α∈A(G)

(cid:32)(cid:90)

Szα

Szα

(cid:18)(cid:90)

exp

Sz

α∈A(G)

2s log fi(xi)dxi

exp

2|α|s log fα(xα)dxα

2|V (G)|s log fi(xi)dx

2|V (G)|s log fα(xα)dx

(cid:33)

(cid:19)

Every assignment selects a single cube indexed by z. The value of the objective is calculated by
averaging log f over the cube indexed by z. As a result  maxz gs(z) ≤ supx f (x) and for any
M-cover H of G  maxz1:M gH s(z1  . . .   zM ) ≤ supx1:m f H (x1  . . .   xM ). As this upper bound
holds for any ﬁxed s  it must also hold for any vector of distributions that can be written as a limit of
such distributions. Now  by applying Theorem 2.2 for the discrete case  (cid:104)θ  µ(cid:48)(cid:105) = lims→∞(cid:104)θ  µs(cid:105) ≤
M log f H (xH ) as desired. To ﬁnish the proof  observe that any Riemann
supM supH∈CM (G) supxH
integrable density can be arbitrarily well approximated by densities of this form as t → ∞.

1

1In order to make this precise  we would need to use Lebesgue integration or take a sequence of probability

distributions over the space RM|V | that arbitrarily well-approximate the desired assignment xH.
the support of the corresponding potential function (i.e.  supp(τi) ⊆ supp(fi)) for the integrals to exist.

2The same argument will apply in the general case  but each of the local distributions must be contained in

8

References
[1] A. Globerson and T. S. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP
LP-relaxations. In Proc. 21st Neural Information Processing Systems (NIPS)  Vancouver  B.C.  Canada 
2007.

[2] T. Werner. A linear programming approach to max-sum problem: A review. Pattern Analysis and Machine

Intelligence  IEEE Transactions on  29(7):1165–1179  2007.

[3] N. Ruozzi and S. Tatikonda. Message-passing algorithms: Reparameterizations and splittings.

Transactions on Information Theory  59(9):5860–5881  Sept. 2013.

IEEE

[4] Y. Wald and A. Globerson. Tightness results for local consistency relaxations in continuous MRFs. In

Proc. 30th Uncertainty in Artiﬁcal Intelligence (UAI)  Quebec City  Quebec  Canada  2014.

[5] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seven-

teenth conference on Uncertainty in Artiﬁcial Intelligence (UAI)  pages 362–369  2001.

[6] N. Ruozzi and S. Tatikonda. Message-passing algorithms for quadratic minimization. Journal of Machine

Learning Research  14:2287–2314  2013.

[7] D. M. Malioutov  J. K. Johnson  and A. S. Willsky. Walk-sums and belief propagation in Gaussian

graphical models. Journal of Machine Learning Research  7:2031–2064  2006.

[8] C. C. Moallemi and B. Van Roy. Convergence of min-sum message passing for quadratic optimization.

Information Theory  IEEE Transactions on  55(5):2413 –2423  May 2009.

[9] C. C. Moallemi and B. Van Roy. Convergence of min-sum message-passing for convex optimization.

Information Theory  IEEE Transactions on  56(4):2041 –2050  April 2010.

[10] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends R(cid:13) in Machine Learning  1(1-2):1–305  2008.

[11] M. Bayati  C. Borgs  J. Chayes  and R. Zecchina. Belief propagation for weighted b-matchings on arbi-
trary graphs and its relation to linear programs with integer solutions. SIAM Journal on Discrete Mathe-
matics  25(2):989–1011  2011.

[12] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? In Computer

VisionECCV 2002  pages 65–81. Springer  2002.

[13] S. Sanghavi  D. M. Malioutov  and A. S. Willsky. Belief propagation and LP relaxation for weighted
matching in general graphs. Information Theory  IEEE Transactions on  57(4):2203 –2212  April 2011.
[14] S. Sanghavi  D. Shah  and A. S. Willsky. Message passing for maximum weight independent set. Infor-

mation Theory  IEEE Transactions on  55(11):4822–4834  Nov. 2009.

[15] M. J. Wainwright  T. S. Jaakkola  and A. S. Willsky. MAP estimation via agreement on (hyper)trees:
Information Theory  IEEE Transactions on  51(11):3697–

Message-passing and linear programming.
3717  Nov. 2005.

[16] David Sontag  Talya Meltzer  Amir Globerson  Yair Weiss  and Tommi Jaakkola. Tightening LP relax-
ations for MAP using message-passing. In 24th Conference in Uncertainty in Artiﬁcial Intelligence  pages
503–510. AUAI Press  2008.

[17] P. O. Vontobel. Counting in graph covers: A combinatorial characterization of the Bethe entropy function.

Information Theory  IEEE Transactions on  Jan. 2013.

[18] P. O. Vontobel and R. Koetter. Graph-cover decoding and ﬁnite-length analysis of message-passing itera-

tive decoding of LDPC codes. CoRR  abs/cs/0512078  2005.

[19] S. Iwata  L. Fleischer  and S. Fujishige. A strongly polynomial-time algorithm for minimizing submodular

functions. Journal of The ACM  1999.

[20] A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time.

Journal of Combinatorial Theory  Series B  80(2):346 – 355  2000.

[21] S. Karlin and Y. Rinott. Classes of orderings of measures and related correlation inequalities. i. multivari-

ate totally positive distributions. Journal of Multivariate Analysis  10(4):467 – 498  1980.

[22] Y. Weiss and W. T. Freeman. Correctness of belief propagation in Gaussian graphical models of arbitrary

topology. Neural Comput.  13(10):2173–2200  Oct. 2001.

[23] D. M. Malioutov. Approximate inference in Gaussian graphical models. Ph.D. thesis  EECS  MIT  2008.

9

,Nicholas Ruozzi
Arulkumar Subramaniam
Moitreya Chatterjee
Anurag Mittal