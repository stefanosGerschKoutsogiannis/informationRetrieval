2015,Learning with Incremental Iterative Regularization,Within a statistical learning setting   we propose and study an iterative regularization algorithm for least squares defined by  an incremental gradient method.   In particular  we show that  if all other parameters are fixed a priori  the number of passes over the data (epochs) acts as a regularization parameter  and  prove strong universal consistency  i.e.  almost sure convergence of the risk  as well as  sharp finite sample bounds for the iterates. Our  results are a step towards understanding the effect of multiple epochs in  stochastic gradient techniques in machine learning and rely  on  integrating  statistical and optimizationresults.,Learning with Incremental Iterative Regularization

Lorenzo Rosasco

DIBRIS  Univ. Genova  ITALY

LCSL  IIT & MIT  USA
lrosasco@mit.edu

Silvia Villa

LCSL  IIT & MIT  USA

Silvia.Villa@iit.it

Abstract

Within a statistical learning setting  we propose and study an iterative regulariza-
tion algorithm for least squares deﬁned by an incremental gradient method. In
particular  we show that  if all other parameters are ﬁxed a priori  the number of
passes over the data (epochs) acts as a regularization parameter  and prove strong
universal consistency  i.e. almost sure convergence of the risk  as well as sharp
ﬁnite sample bounds for the iterates. Our results are a step towards understanding
the effect of multiple epochs in stochastic gradient techniques in machine learning
and rely on integrating statistical and optimization results.

1 Introduction

Machine learning applications often require efﬁcient statistical procedures to process potentially
massive amount of high dimensional data. Motivated by such applications  the broad objective of
our study is designing learning procedures with optimal statistical properties  and  at the same time 
computational complexities proportional to the generalization properties allowed by the data  rather
than their raw amount [6]. We focus on iterative regularization as a viable approach towards this
goal. The key observation behind these techniques is that iterative optimization schemes applied to
scattered  noisy data exhibit a self-regularizing property  in the sense that early termination (early-
stop) of the iterative process has a regularizing effect [21  24]. Indeed  iterative regularization algo-
rithms are classical in inverse problems [15]  and have been recently considered in machine learning
[36  34  3  5  9  26]  where they have been proved to achieve optimal learning bounds  matching
those of variational regularization schemes such as Tikhonov [8  31].
In this paper  we consider an iterative regularization algorithm for the square loss  based on a recur-
sive procedure processing one training set point at each iteration. Methods of the latter form  often
broadly referred to as online learning algorithms  have become standard in the processing of large
data-sets  because of their low iteration cost and good practical performance. Theoretical studies
for this class of algorithms have been developed within different frameworks. In composite and
stochastic optimization [19  20  29]  in online learning  a.k.a. sequential prediction [11]  and ﬁnally 
in statistical learning [10]. The latter is the setting of interest in this paper  where we aim at devel-
oping an analysis keeping into account simultaneously both statistical and computational aspects.
To place our contribution in context  it is useful to emphasize the role of regularization and different
ways in which it can be incorporated in online learning algorithms. The key idea of regularization
is that controlling the complexity of a solution can help avoiding overﬁtting and ensure stability and
generalization [33]. Classically  regularization is achieved penalizing the objective function with
some suitable functional  or minimizing the risk on a restricted space of possible solutions [33].
Model selection is then performed to determine the amount of regularization suitable for the data
at hand. More recently  there has been an interest in alternative  possibly more efﬁcient  ways to
incorporate regularization. We mention in particular [1  35  32] where there is no explicit regular-
ization by penalization  and the step-size of an iterative procedure is shown to act as a regularization
parameter. Here  for each ﬁxed step-size  each data point is processed once  but multiple passes are
typically needed to perform model selection (that is  to pick the best step-size). We also mention

1

[22] where an interesting adaptive approach is proposed  which seemingly avoid model selection
under certain assumptions.
In this paper  we consider a different regularization strategy  widely used in practice. Namely  we
consider no explicit penalization  ﬁx the step size a priori  and analyze the effect of the number of
passes over the data  which becomes the only free parameter to avoid overﬁtting  i.e. regularize.
The associated regularization strategy  that we dub incremental iterative regularization  is hence
based on early stopping. The latter is a well known ”trick”  for example in training large neural
networks [18]  and is known to perform very well in practice [16]. Interestingly  early stopping
with the square loss has been shown to be related to boosting [7]  see also [2  17  36]. Our goal
here is to provide a theoretical understanding of the generalization property of the above heuristic
for incremental/online techniques. Towards this end  we analyze the behavior of both the excess
risk and the iterates themselves. For the latter we obtain sharp ﬁnite sample bounds matching those
for Tikhonov regularization in the same setting. Universal consistency and ﬁnite sample bounds for
the excess risk can then be easily derived  albeit possibly suboptimal. Our results are developed
in a capacity independent setting [12  30]  that is under no conditions on the covering or entropy
numbers [30].
In this sense our analysis is worst case and dimension free. To the best of our
knowledge the analysis in the paper is the ﬁrst theoretical study of regularization by early stopping
in incremental/online algorithms  and thus a ﬁrst step towards understanding the effect of multiple
passes of stochastic gradient for risk minimization.
The rest of the paper is organized as follows. In Section 2 we describe the setting and the main
assumptions  and in Section 3 we state the main results  discuss them and provide the main elements
of the proof  which is deferred to the supplementary material. In Section 4 we present some experi-
mental results on real and synthetic datasets.
Notation We denote by R+ = [0  +1[   R++ = ]0  +1[   and N⇤ = N \{0}. Given a normed
space B and linear operators (Ai)1im  Ai : B!B for every i  their composition Am ··· A1
will be denoted asQm
i=j Ai = I  where I is the identity
of B. The operator norm will be denoted by k·k and the Hilbert-Schmidt norm by k·k HS. Also  if
j > m  we setPm

i=1 Ai. By convention  if j > m  we setQm

i=j Ai = 0.

2 Setting and Assumptions

We ﬁrst describe the setting we consider  and then introduce and discuss the main assumptions that
will hold throughout the paper. We build on ideas proposed in [13  27] and further developed in a
series of follow up works [8  3  28  9]. Unlike these papers where a reproducing kernel Hilbert space
(RKHS) setting is considered  here we consider a formulation within an abstract Hilbert space. As
discussed in the Appendix A  results in a RKHS can be recovered as a special case. The formula-
tion we consider is close to the setting of functional regression [25] and reduces to standard linear
regression if H is ﬁnite dimensional  see Appendix A.
Let H be a separable Hilbert space with inner product and norm denoted by h· ·iH and k·kH. Let
(X  Y ) be a pair of random variables on a probability space (⌦  S  P)  with values in H and R 
respectively. Denote by ⇢ the distribution of (X  Y )  by ⇢X the marginal measure on H  and by
⇢(·|x) the conditional measure on R given x 2H . Considering the square loss function  the problem
under study is the minimizazion of the risk 

inf

w2HE(w) 

E(w) =ZH⇥R

(hw  xiH  y)2d⇢(x  y)  

(1)

provided the distribution ⇢ is ﬁxed but known only through a training set z =
{(x1  y1)  . . .   (xn  yn)}  that is a realization of n 2 N⇤ independent identical copies of (X  Y ).
In the following  we measure the quality of an approximate solution ˆw 2H (an estimator) consid-
ering the excess risk
(2)
If the set of solutions of Problem (1) is non empty  that is O = argminH E6 = ?  we also consider
(3)

  where w† = argmin

E( ˆw)  inf

 ˆw  w†H

H E.

2

w2O kwkH.

H   ⇢X-almost surely.

More precisely we are interested in deriving almost sure convergence results and ﬁnite sample
bounds on the above error measures. This requires making some assumptions that we discuss next.
We make throughout the following basic assumption.
Assumption 1. There exist M 2 ]0  +1[ and  2 ]0  +1[ such that |y| M⇢ -almost surely  and
kxk2
The above assumption is fairly standard. The boundness assumption on the output is satisﬁed in
classiﬁcation  see Appendix A  and can be easily relaxed  see e.g. [8]. The boundness assumption
on the input can also be relaxed  but the resulting analysis is more involved. We omit these develop-
ments for the sake of clarity. It is well known that (see e.g. [14])  under Assumption 1  the risk is a
convex and continuous functional on L2(H ⇢ X)  the space of square-integrable functions with norm
⇢ =RH⇥R |f (x)|2d⇢X(x). The minimizer of the risk on L2(H ⇢ X) is the regression function
kfk2
f⇢(x) = R yd⇢(y|x) for ⇢X-almost every x 2H . By considering Problem (1) we are restricting
the search for a solution to linear functions. Note that  since H is in general inﬁnite dimensional 
the minimum in (1) might not be achieved. Indeed  bounds on the error measures in (2) and (3)
depend on if  and how well  the regression function can be linearly approximated. The following
assumption quantiﬁes in a precise way such a requirement.
Assumption 2. Consider the space L⇢ = {f : H! R |9 w 2H with f (x) = hw  xi ⇢X- a.s.} 
and let L⇢ be its closure in L2(H ⇢ X). Moreover  consider the operator
L : L2(H ⇢ X) ! L2(H ⇢ X)  Lf (x) =ZH hx  x0i f (x0)d⇢(x0) 
Deﬁne g⇢ = argming2L⇢ kf⇢  gk⇢. Let r 2 [0  +1[  and assume that
g⇢ = Lrg.

8f 2 L2(H ⇢ X).

(4)

(5)

(9g 2 L2(H ⇢ X))

such that

The above assumption is standard in the context of RKHS [8]. Since its statement is somewhat
technical  and we provide a formulation in a Hilbert space with respect to the usual RKHS setting 
we further comment on its interpretation. We begin noting that L⇢ is the space of linear functions
indexed by H and is a proper subspace of L2(H ⇢ X) – if Assumption 1 holds. Moreover  under
the same assumption  it is easy to see that the operator L is linear  self-adjoint  positive deﬁnite and
trace class  hence compact  so that its fractional power in (4) is well deﬁned. Most importantly  the
following equality  which is analogous to Mercer’s theorem [30]  can be shown fairly easily:

L⇢ = L1/2L2(H ⇢ X) .

(6)
This last observation allows to provide an interpretation of Condition (5). Indeed  given (6)  for
r = 1/2  Condition (5) states that g⇢ belongs to L⇢  rather than its closure. In this case  Problem 1
has at least one solution  and the set O in (3) is not empty. Vice versa  if O6 = ? then g⇢ 2L ⇢ 
and w† is well-deﬁned. If r > 1/2 the condition is stronger than for r = 1/2  for the subspaces of
Lr(L2(H ⇢ X)) are nested subspaces of L2(H ⇢ X) for increasing r1.
2.1

Iterative Incremental Regularized Learning

The learning algorithm we consider is deﬁned by the following iteration.

Let ˆw0 2H and  2 R++. Consider the sequence ( ˆwt)t2N generated through the following
procedure: given t 2 N  deﬁne
(7)

ˆwt+1 = ˆun
t  

where ˆun

t is obtained at the end of one cycle  namely as the last step of the recursion

ˆu0
t = ˆwt;

t = ˆui1
ˆui

t 


n

(hˆui1

t

  xiiH  yi)xi 

i = 1  . . .   n.

(8)

1If r < 1/2 then the regression function does not have a best linear approximation since g⇢ /2L ⇢  and in
particular  for r = 0 we are making no assumption. Intuitively  for 0 < r < 1/2  the condition quantiﬁes how
far g⇢ is from L⇢  that is to be well approximated by a linear function.

3

Each cycle  called an epoch  corresponds to one pass over data. The above iteration can be seen as
the incremental gradient method [4  19] for the minimization of the empirical risk corresponding to
z  that is the functional 

ˆE(w) =

1
n

nXi=1

(hw  xiiH  yi)2.

(9)

(see also Section B.2). Indeed  there is a vast literature on how the iterations (7)  (8) can be used to
minimize the empirical risk [4  19]. Unlike these studies in this paper we are interested in how the
iterations (7)  (8) can be used to approximately minimize the risk E. The key idea is that while ˆwt is
close to a minimizer of the empirical risk when t is sufﬁciently large  a good approximate solution
of Problem (1) can be found by terminating the iterations earlier (early stopping). The analysis in
the next few sections grounds theoretically this latter intuition.
Remark 1 (Representer theorem). Let H be a RKHS of functions from X to Y deﬁned by a kernel
K : X⇥X! R. Let ˆw0 = 0  then the iteration after t epochs of the algorithm in (7)-(8) can
be written as ˆwt(·) = Pn
k=1(↵t)kKxk  for suitable coefﬁcients ↵t = ((↵t)1  . . .   (↵t)n) 2 Rn 
updated as follows:
t)k =((ci1

)j  yi⌘   k = i

j=1 K(xi  xj)(ci1

↵t+1 = cn
t

)k 
)k 

c0
t = ↵t 



n⇣Pn

(ci

t

(ci1

t

t

k 6= i

3 Early stopping for incremental iterative regularization

In this section  we present and discuss the main results of the paper  together with a sketch of the
proof. The complete proofs can be found in Appendix B. We ﬁrst present convergence results and
then ﬁnite sample bounds for the quantities in (2) and (3).

Theorem 1. In the setting of Section 2  let Assumption 1 hold. Let  2⇤0  1⇤. Then the following

hold:

(i) If we choose a stopping rule t⇤ : N⇤ ! N⇤ such that
lim

lim

t⇤(n) = +1 and

n!+1

n!+1

t⇤(n)3 log n

n

= 0

then

n!+1E( ˆwt⇤(n))  inf
lim

w2HE(w) = 0 P-almost surely.

(10)

(11)

(ii) Suppose additionally that the set O of minimizers of (1) is nonempty and let w† be deﬁned
as in (3). If we choose a stopping rule t⇤ : N⇤ ! N⇤ satisfying the conditions in (10) then
(12)

k ˆwt⇤(n)  w†kH ! 0 P-almost surely.

The above result shows that for an a priori ﬁxed step-sized  consistency is achieved computing a
suitable number t⇤(n) of iterations of algorithm (7)-(8) given n points. The number of required
iterations tends to inﬁnity as the number of available training points increases. Condition (10) can
be interpreted as an early stopping rule  since it requires the number of epochs not to grow too fast.
In particular  this excludes the choice t⇤(n) = 1 for all n 2 N⇤  namely considering only one pass
over the data. In the following remark we show that  if we let  = (n) to depend on the length of
one epoch  convergence is recovered also for one pass.
Remark 2 (Recovering Stochastic Gradient descent). Algorithm in (7)-(8) for t = 1 is a stochastic
gradient descent (one pass over a sequence of i.i.d. data) with stepsize /n. Choosing (n) =
1n↵  with ↵< 1/5 in Algorithm (7)-(8)  we can derive almost sure convergence of E( ˆw1)infH E
as n ! +1 relying on a similar proof to that of Theorem 1.
To derive ﬁnite sample bounds further assumptions are needed. Indeed  we will see that the behavior
of the bias of the estimator depends on the smoothness Assumption 2. We are in position to state
our main result  giving a ﬁnite sample bound.

4

Theorem 2 (Finite sample bounds in H). In the setting of Section 2  let  2⇤0  1⇤ for every t 2 N.
Suppose that Assumption 2 is satisﬁed for some r 2 ]1/2  +1[. Then the set O of minimizers of (1)
is nonempty  and w† in (3) is well deﬁned. Moreover  the following hold:
(i) There exists c 2 ]0  +1[ such that  for every t 2 N⇤  with probability greater than 1   
k ˆwt  w†kH 
(ii) For the stopping rule t⇤ : N⇤ ! N⇤ : t⇤(n) =⌃n
k ˆwt⇤(n)  w†kH 2432 log

2r+1⌥  with probability greater than 1   
kgk⇢35 n

2⌘ t +✓r  1
 ◆r 1
2+✓ r  1

 M1/2+ 2M 21+ 3kgk⇢r 3

⇣M1/2 + 2M 21 + 3kgk⇢r 3

 ◆r 1

32 log 16
pn

2r. (13)

kgk⇢t

r 1
2r+1 .

2

(14)

16

2

1

2

2

2

1

The dependence on  suggests that a big   which corresponds to a small   helps in decreasing the
sample error  but increases the approximation error. Next we present the result for the excess risk.
We consider only the attainable case  that is the case r > 1/2 in Assumption 2. The case r  1/2
is deferred to Appendix A  since both the proof and the statement are conceptually similar to the
attainable case.
Theorem 3 (Finite sample bounds for the risk – attainable case). In the setting of Section 2  let

Assumptions 1 holds  and let  2⇤0  1⇤. Let Assumption 2 be satisﬁed for some r 2 ]1/2  +1].
Then the following hold:
(i) For every t 2 N⇤  with probability greater than 1   

E( ˆwt)  inf

H E

232 log(16/)2

n

(ii) For the stopping rule t⇤ : N⇤ ! N⇤ : t⇤(n) =⌃n

E( ˆwt⇤(n))  inf

H E "8✓32 log

hM + 2M 21/2 + 3rkgk⇢i2

t2 + 2✓ r
t◆2r

1

⇢

(15)

kgk2
2(1+r)⌥  with probability greater than 1   
⇢# nr/(r+1)
kgk2

+ 2✓ r
◆2r

16

 ◆2⇣M + 2M 21/2 + 3rkgk⇢⌘2

(16)

Equations (13) and (15) arise from a form of bias-variance (sample-approximation) decomposition
of the error. Choosing the number of epochs that optimize the bounds in (13) and (15) we derive
a priori stopping rules and corresponding bounds (14) and (16). Again  these results conﬁrm that
the number of epochs acts as a regularization parameter and the best choices following from equa-
tions (13) and (15) suggest multiple passes over the data to be beneﬁcial. In both cases  the stopping
rule depends on the smoothness parameter r which is typically unknown  and hold-out cross vali-
dation is often used in practice. Following [9]  it is possible to show that this procedure allows to
adaptively achieve the same convergence rate as in (16).

3.1 Discussion

In Theorem 2  the obtained bound can be compared to known lower bounds  as well as to pre-
vious results for least squares algorithms obtained under Assumption 2. Minimax lower bounds
and individual lower bounds [8  31]  suggest that  for r > 1/2  O(n(r1/2)/(2r+1)) is the optimal
capacity-independent bound for the H norm2. In this sense  Theorem 2 provides sharp bounds on
the iterates. Bounds can be improved only under stronger assumptions  e.g. on the covering num-
bers or on the eigenvalues of L [30]. This question is left for future work. The lower bounds for
the excess risk [8  31] are of the form O(n2r/(2r+1)) and in this case the results in Theorems 3
and 7 are not sharp. Our results can be contrasted with online learning algorithms that use step-size

2In a recent manuscript  it has been proved that this is indeed the minimax lower bound (G. Blanchard 

personal communication)

5

as regularization parameter. Optimal capacity independent bounds are obtained in [35]  see also
[32] and indeed such results can be further improved considering capacity assumptions  see [1] and
references therein. Interestingly  our results can also be contrasted with non incremental iterative
regularization approaches [36  34  3  5  9  26]. Our results show that incremental iterative regular-
ization  with distribution independent step-size  behaves as a batch gradient descent  at least in terms
of iterates convergence. Proving advantages of incremental regularization over the batch one is an
interesting future research direction. Finally  we note that optimal capacity independent and depen-
dent bounds are known for several least squares algorithms  including Tikhonov regularization  see
e.g. [31]  and spectral ﬁltering methods [3  9]. These algorithms are essentially equivalent from a
statistical perspective but different from a computational perspective.

3.2 Elements of the proof

The proofs of the main results are based on a suitable decomposition of the error to be estimated as
the sum of two quantities that can be interpreted as a sample and an approximation error  respec-
tively. Bounds on these two terms are then provided. The main technical contribution of the paper is
the sample error bound. The difﬁculty in proving this result is due to multiple passes over the data 
which induce statistical dependencies in the iterates.

Error decomposition. We consider an auxiliary iteration (wt)t2N which is the expectation of the
iterations (7) and (8)  starting from w0 2H with step-size  2 R++. More explicitly  the considered
iteration generates wt+1 according to

wt+1 = un
t  

(17)

(18)

where un

t is given by

u0
t = wt;

t = ui1
ui

t 



nZH⇥Rhui1

t

  xiH  y x d⇢ (x  y) .

If we let S : H! L2(H ⇢ X) be the linear map w 7! hw ·iH  which is bounded by p under
Assumption 1  then it is well-known that [13]

(8t 2 N) E( ˆwt)  inf

H E = kS ˆwt  g⇢k2
 2k ˆwt  wtk2

⇢  2kS ˆwt  Swtk2
H + 2(E(wt)  inf

⇢ + 2kSwt  g⇢k2
H E).

⇢

(19)

In this paper  we refer to the gap between the empirical and the expected iterates k ˆwt  wtkH as the
sample error  and to A(t    n) = E(wt)  infH E as the approximation error. Similarly  if w† (as
deﬁned in (3)) exists  using the triangle inequality  we obtain

k ˆwt  w†kH  k ˆwt  wtkH + kwt  w†kH.

(20)

Proof main steps.
In the setting of Section 2  we summarize the key steps to derive a general
bound for the sample error (the proof of the behavior of the approximation error is more standard).
The bound on the sample error is derived through many technical lemmas and uses concentration
inequalities applied to martingales (the crucial point is the inequality in STEP 5 below). Its complete
derivation is reported in Appendix B.2. We introduce the additional linear operators: T : H!
H : T = S⇤S  and  for every x 2X   Sx : H! R : Sxw = hw  xi  and Tx : H!H : Tx = SxS⇤x.
Moreover  set ˆT =Pn
i=1 Txi/n. We are now ready to state the main steps of the proof.
Sample error bound (STEP 1 to 5)
STEP 1 (see Proposition 1): Find equivalent formulations for the sequences ˆwt and wt:

ˆwt+1 = (I   ˆT ) ˆwt + ✓ 1
wt+1 = (I  T ) wt + S⇤g⇢ + 2(Awt  b) 

S⇤xj yj◆ + 2⇣ ˆA ˆwt  ˆb⌘

nXj=1

n

6

where

ˆA =

A =

1
n2

1
n2

nXk=2" nYi=k+1⇣I 
nXk=2" nYi=k+1⇣I 


n


n

Txi⌘# Txk
T⌘# T
k1Xj=1

k1Xj=1

T 

Txj   ˆb =

b =

1
n2

1
n2

nXk=2" nYi=k+1⇣I 
nXk=2" nYi=k+1⇣I 


n


n

Txi⌘# Txk
T⌘# T
k1Xj=1

S⇤xj yj.

k1Xj=1

S⇤g⇢.

STEP 2 (see Lemma 5): Use the formulation obtained in STEP 1 to derive the following recursive
inequality 

i=1

⇣k

( ˆw0  w0) + 

t1Xk=0⇣I   ˆT +  ˆA⌘tk+1

ˆwt  wt =⇣I   ˆT + 2 ˆA⌘t
ˆS⇤xiyi  S⇤g⇢ + (b  ˆb).
with ⇣k = (T  ˆT )wk + ( ˆA  A)wk + 1
nPn
STEP 3 (see Lemmas 6 and 7): Initialize ˆw0 = w0 = 0  prove that kI   ˆT +  ˆAk  1  and derive
from STEP 2 that 
kwkkH + t⇣ 1
k ˆwt  wtkH  kT  ˆTk + k ˆA  Ak t1Xk=0
nXi=1
kwtkH ⇢max{r1/2  (t)1/2r}kgk⇢

STEP 4 (see Lemma 8): Let Assumption 2 hold for some r 2 R+ and g 2 L2(H ⇢ X). Prove that

ˆS⇤xiyi  S⇤g⇢ + kb  ˆbk⌘.

if r 2 [0  1/2[ 
if r 2 [1/2  +1[

STEP 5 (see Lemma 9 and Proposition 2: Prove that with probability greater than 1   the
following inequalities hold:

r1/2kgk⇢

(8t 2H )

n

322
3pn
16
3pn

log

log

4


2


 

 

k ˆA  AkHS 

 ˆT  THS 

32M 2
3pn

log

4


 

kˆb  bkH 
nXi=1
1
n



S⇤xiyi  S⇤g⇢H 

16pM
3pn

log

2


.

STEP 6 (approximation error bound  see Theorem 6): Prove that  if Assumption 2 holds for
some r 2 ]0  +1[  then E(wt)  infH E r/t2rkgk2
⇢. Moreover  if Assumption 2 holds with
r = 1/2  then kwt  w†kH ! 0  and if Assumption 2 holds for some r 2 ]1/2  +1[  then
kwt  w†kH  r1/2

STEP 7: Plug the sample and approximation error bounds obtained in STEP 1-5 and STEP 6 in
(19) and (20)  respectively.

t r1/2kgk⇢.

4 Experiments

Synthetic data. We consider a scalar linear regression problem with random design. The input
points (xi)1in are uniformly distributed in [0  1] and the output points are obtained as yi =
hw⇤  (xi)i + Ni  where Ni is a Gaussian noise with zero mean and standard deviation 1 and =
('k)1kd is a dictionary of functions whose k-th element is 'k(x) = cos((k1)x)+sin((k1)x).
In Figure 1  we plot the test error for d = 5 (with n = 80 in (a) and 800 in (b)). The plots show
that the number of the epochs acts as a regularization parameter  and that early stopping is beneﬁcial
to achieve a better test error. Moreover  according to the theory  the experiments suggest that the
number of performed epochs increases if the number of available training points increases.
Real data. We tested the kernelized version of our algorithm (see Remark 1 and Appendix A)
on the cpuSmall3  Adult and Breast Cancer Wisconsin (Diagnostic)4 real-world

3Available at http://www.cs.toronto.edu/˜delve/data/comp-activ/desc.html
4Adult and Breast Cancer Wisconsin (Diagnostic)  UCI repository  2013.

7

1.2

1 

0.8

r
o
r
r
e
 
t
s
e
T

r
o
r
r
e
 
t
s
e
T

2

1.5

1
0

0

2000

4000

Iterations
(a)

6000

8000

1

2

Iterations
(b)

3

4
×105

Figure 1: Test error as a function of the number of iterations. In (a)  n = 80  and total number of
iterations of IIR is 8000  corresponding to 100 epochs. In (b)  n = 800 and the total number of
epochs is 400. The best test error is obtained for 9 epochs in (a) and for 31 epochs in (b).

datasets. We considered a subset of Adult  with n = 1600. The results are shown in Figure 2. A
comparison of the test errors obtained with the kernelized version of the method proposed in this
paper (Kernel Incremental Iterative Regularization (KIIR))  Kernel Iterative Regularization (KIR) 
that is the kernelized version of gradient descent  and Kernel Ridge Regression (KRR) is reported in
Table 1. The results show that the test error of KIIR is comparable to that of KIR and KRR.

0.1

0.08

0.06

0.04

0.02

r
o
r
r

E

0

0

Validation Error
Training Error

1

2

3

Iterations

4
×10 6

Figure 2: Training (orange) and validation (blue) classiﬁcation errors obtained by KIIR on the
Breast Cancer dataset as a function of the number of iterations. The test error increases after a
certain number of iterations  while the training error is “decreasing” with the number of iterations.

Table 1: Test error comparison on real datasets. Median values over 5 trials.

Dataset

cpuSmall

Adult

Breast Cancer

ntr
5243
1600
400

d
12
123
30

Error Measure

RMSE

Class. Err.
Class. Err.

KIIR
5.9125
0.167
0.0118

KRR
3.6841
0.164
0.0118

KIR
5.4665
0.154
0.0237

Acknowledgments
This material is based upon work supported by CBMM  funded by NSF STC award CCF-1231216.
and by the MIUR FIRB project RBFR12M3AC. S. Villa is member of GNAMPA of the Istituto
Nazionale di Alta Matematica (INdAM).

References
[1] F. Bach and A. Dieuleveut.

arXiv:1408.0361  2014.

Non-parametric stochastic approximation with large step sizes.

[2] P. Bartlett and M. Traskin. Adaboost is consistent. J. Mach. Learn. Res.  8:2347–2368  2007.
[3] F. Bauer  S. Pereverzev  and L. Rosasco. On regularization algorithms in learning theory. J. Complexity 

23(1):52–72  2007.

[4] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM J. Optim. 

7(4):913–926  1997.

[5] G. Blanchard and N. Kr¨amer. Optimal learning rates for kernel conjugate gradient regression. In Advances

in Neural Inf. Proc. Systems (NIPS)  pages 226–234  2010.

8

[6] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Suvrit Sra  Sebastian Nowozin  and

Stephen J. Wright  editors  Optimization for Machine Learning  pages 351–368. MIT Press  2011.

[7] P. Buhlmann and B. Yu. Boosting with the l2 loss: Regression and classiﬁcation. J. Amer. Stat. Assoc. 

98:324–339  2003.

[8] A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Found. Comput.

Math.  2006.

[9] A. Caponnetto and Y. Yao. Cross-validation based adaptation for regularization operators in learning

theory. Anal. Appl.  08:161–183  2010.

[10] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

IEEE Trans. Information Theory  50(9):2050–2057  2004.

[11] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
[12] F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University

Press  2007.

[13] E. De Vito  L. Rosasco  A. Caponnetto  U. De Giovannini  and F. Odone. Learning from examples as an

inverse problem. J.Mach. Learn. Res.  6:883–904  2005.

[14] E. De Vito  L. Rosasco  A. Caponnetto  M. Piana  and A. Verri. Some properties of regularized kernel

methods. Journal of Machine Learning Research  5:1363–1390  2004.

[15] H. W. Engl  M. Hanke  and A. Neubauer. Regularization of inverse problems. Kluwer  1996.
[16] P.-S. Huang  H. Avron  T. Sainath  V. Sindhwani  and B. Ramabhadran. Kernel methods match deep

neural networks on timit. In IEEE ICASSP  2014.

[17] W. Jiang. Process consistency for adaboost. Ann. Stat.  32:13–29  2004.
[18] Y. LeCun  L. Bottou  G. Orr  and K. Muller. Efﬁcient backprop. In G. Orr and Muller K.  editors  Neural

Networks: Tricks of the trade. Springer  1998.

[19] A. Nedic and D. P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM

Journal on Optimization  12(1):109–138  2001.

[20] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM J. Optim.  19(4):1574–1609  2008.

[21] A. Nemirovskii. The regularization properties of adjoint gradient method in ill-posed problems. USSR

Computational Mathematics and Mathematical Physics  26(2):7–16  1986.

[22] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning.

NIPS Proceedings  2014.

[23] I. Pinelis. Optimum bounds for the distributions of martingales in Banach spaces. Ann. Probab. 

22(4):1679–1706  1994.

[24] B. Polyak. Introduction to Optimization. Optimization Software  New York  1987.
[25] J. Ramsay and B. Silverman. Functional Data Analysis. Springer-Verlag  New York  2005.
[26] G. Raskutti  M. Wainwright  and B. Yu. Early stopping for non-parametric regression: An optimal data-

dependent stopping rule. In in 49th Annual Allerton Conference  pages 1318–1325. IEEE  2011.

[27] S. Smale and D. Zhou. Shannon sampling II: Connections to learning theory. Appl. Comput. Harmon.

Anal.  19(3):285–302  November 2005.

[28] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations.

Constr. Approx.  26(2):153–172  2007.

[29] N. Srebro  K. Sridharan  and A. Tewari. Optimistic rates for learning with a smooth loss. arXiv:1009.3896 

2012.

[30] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[31] I. Steinwart  D. R. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In COLT 

2009.

[32] P. Tarr`es and Y. Yao. Online learning as stochastic approximation of regularization paths: optimality and

almost-sure convergence. IEEE Trans. Inform. Theory  60(9):5716–5735  2014.

[33] V. Vapnik. Statistical learning theory. Wiley  New York  1998.
[34] Y. Yao  L. Rosasco  and A. Caponnetto. On early stopping in gradient descent learning. Constr. Approx. 

26:289–315  2007.

[35] Y. Ying and M. Pontil. Online gradient descent learning algorithms. Found. Comput. Math.  8:561–596 

2008.

[36] T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics 

pages 1538–1579  2005.

9

,Lorenzo Rosasco
Silvia Villa
Shichen Liu
Mingsheng Long
Jianmin Wang
Michael Jordan