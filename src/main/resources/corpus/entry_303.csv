2019,A unified theory for the origin of grid cells through the lens of pattern formation,Grid cells in the brain fire in strikingly regular hexagonal patterns across space. There are currently two seemingly unrelated frameworks for understanding these patterns. Mechanistic models account for hexagonal firing fields as the result of pattern-forming dynamics in a recurrent neural network with hand-tuned center-surround connectivity. Normative models specify a neural architecture  a learning rule  and a navigational task  and observe that grid-like firing fields emerge due to the constraints of solving this task. Here we provide an analytic theory that unifies the two perspectives by casting the learning dynamics of neural networks trained on navigational tasks as a pattern forming dynamical system. This theory provides insight into the optimal solutions of diverse formulations of the normative task  and shows that symmetries in the representation of space correctly predict the structure of learned firing fields in trained neural networks. Further  our theory proves that a nonnegativity constraint on firing rates induces a symmetry-breaking mechanism which favors hexagonal firing fields.  We extend this theory to the case of learning multiple grid maps and demonstrate that optimal solutions consist of a hierarchy of maps with increasing length scales. These results unify previous accounts of grid cell firing and provide a novel framework for predicting the learned representations of recurrent neural networks.,A uniﬁed theory for the origin of grid cells through

the lens of pattern formation

Ben Sorscher*1  Gabriel C. Mel*2  Surya Ganguli1  Samuel A. Ocko1

1Department of Applied Physics  Stanford University
2Neurosciences PhD Program  Stanford University

Abstract

Grid cells in the brain ﬁre in strikingly regular hexagonal patterns across space.
There are currently two seemingly unrelated frameworks for understanding these
patterns. Mechanistic models account for hexagonal ﬁring ﬁelds as the result of
pattern-forming dynamics in a recurrent neural network with hand-tuned center-
surround connectivity. Normative models specify a neural architecture  a learning
rule  and a navigational task  and observe that grid-like ﬁring ﬁelds emerge due to
the constraints of solving this task. Here we provide an analytic theory that uniﬁes
the two perspectives by casting the learning dynamics of neural networks trained
on navigational tasks as a pattern forming dynamical system. This theory pro-
vides insight into the optimal solutions of diverse formulations of the normative
task  and shows that symmetries in the representation of space correctly predict
the structure of learned ﬁring ﬁelds in trained neural networks. Further  our theory
proves that a nonnegativity constraint on ﬁring rates induces a symmetry-breaking
mechanism which favors hexagonal ﬁring ﬁelds. We extend this theory to the case
of learning multiple grid maps and demonstrate that optimal solutions consist of a
hierarchy of maps with increasing length scales. These results unify previous ac-
counts of grid cell ﬁring and provide a novel framework for predicting the learned
representations of recurrent neural networks.

1

Introduction

How does the brain construct an internal map of space? One such map is generated by grid cells in
the medial entorhinal cortex (MEC)  which exhibit regular hexagonal spatial ﬁring ﬁelds  forming a
periodic  low-dimensional representation of space [1]. Grid cells are clustered into discrete modules
sharing a periodicity and an orientation  but varying randomly in phase [1  2]. A complementary
map is generated by place cells in the adjacent hippocampus  which exhibit localized spatial ﬁring
ﬁelds  forming a sparser representation of space [3].
Early mechanistic models of grid cells corresponded to recurrent neural networks (RNNs) with
hand-tuned connectivity designed speciﬁcally to reproduce hexagonal grid cell ﬁring patterns [4  5 
6]. Such continuous attractor models can robustly integrate and store 2D positional information via
path integration [7]. Recent enhancements to such attractor networks that incorporate plastic inputs
from landmark cells can explain why grid cells deform in irregular environments [8]  and when
they either phase shift or remap in altered virtual reality environments [9]. However  none of these
recurrent network models show that grid-like ﬁring patterns are required to solve navigational tasks.
Thus they cannot demonstrate that hexagonal ﬁring patterns naturally arise as the optimal solution
to any computational problem  precisely because the hexagonal patterns are simply assumed in the
ﬁrst place by hand-tuning the recurrent connectivity.
More recent normative models have shown that neural networks trained on tasks that involve encod-
ing a representation of spatial position learn grid-like responses in their hidden units. For example 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

[10] found that the weights of a one layer neural network  trained via Oja’s rule on simulated place
cell inputs  learned grid cells with square grid ﬁring patterns. When a non-negativity constraint
was imposed  these grids became hexagonal. In [11]  the eigenvectors of the graph Laplacian for
a navigational task were shown to be square-like grids. [12] showed that learning basis functions
for local transition operators also yields square grids. [13  14] trained recurrent neural networks on
a path integration task  and observed that the hidden units developed grid-like patterns. [13] found
square grids in square environments and hexagonal grids in triangular environments. [14] claimed
to ﬁnd hexagonal grids  though their resulting patterns had substantial heterogeneity. While these
normative models hint at the intriguing hypothesis that grid like representations in the brain may
arise as an inevitable consequence of solving a spatial navigation task   these models alone do not
offer any theoretical clarity on when and why grid cell patterns emerge from such navigational so-
lutions  and if they do  why they are sometimes square  sometimes hexagonal  or sometimes highly
heterogeneous 1
Here we provide such theoretical clarity by forging an analytic link between the learning dynamics
of neural networks trained on navigational tasks  to a central  unifying pattern forming dynamical
system. Our theory correctly predicts the structure and hierarchy of grid patterns learned in diverse
neural architectures  and proves that nonnegativity is just one of a family of interpretable representa-
tional constraints that promotes hexagonal grids. Furthermore  this theory uniﬁes both mechanistic
and normative models  by proving that the learning dynamics induced by optimizing a normative
position encoding objective is equivalent to the mechanistic pattern forming dynamics implemented
by a population of recurrently connected neurons.

2 Optimally encoding position yields diverse grid-like patterns

To study the space of solutions achieved by the normative models outlined above  we train a variety
of neural network architectures on navigational tasks  reproducing the work of [10  13  14] (Fig.
1A). We simulate an animal moving in a square box  2.2m on a side  and record the activity of
nP simulated place cells tiled randomly and uniformly throughout the environment (Fig. 1B). We
collect the place cell activations at nx locations as the animal explores its environment in a matrix
P 2 Rnx⇥nP . We then train the following network architectures on the following tasks:

1. 1-layer NN. Following [10]  we train a single layer neural network to perform unsuper-
vised Hebbian learning on place cell activations P as inputs. Hidden unit representations
are made orthogonal by a generalized Hebbian algorithm similar to Gram-Schmidt orthog-
onalization (see [10] for details). This learning procedure is equivalent to performing PCA
on place cell inputs.

2. RNN. We train an RNN to encode position by path integrating velocity inputs. At each time
step  the network receives the animal’s 2-dimensional velocity ~v(t) as input. The velocity
signal is integrated by the network’s nG recurrently connected units  and the network’s
current position representation is linearly read out into a layer of estimated place cells.
This approach is identical to that used in [13]  except that our RNNs are trained to encode
position by encoding in its outputs a place cell representation rather than a 2D vector of
Cartesian (x  y) position.

3. LSTM. We train a signiﬁcantly more complex LSTM architecture on the same path inte-
gration task as in 2  reproducing the work of [14]. The ”grid cells” in this architecture are
not recurrently connected  but reside in a dense layer immediately following the LSTM.
The grid cells are also subject to dropout at a rate of 0.5. We train both with and without
the additional objective of integrating head direction inputs  and obtain qualitatively similar
results.

Remarkably  in each case the networks learn qualitatively similar grid-like representations (Fig. 1C-
E). We observe that the structure of the grid patterns depends sensitively on the shape of the place
cell tuning curves. We ﬁrst train with Gaussian place cell tuning curves of size 400cm2  and ﬁnd
that each network develops regular square grid patterns (Fig. 1C)  like those in [13]. We next train
with a center-surround place cell tuning curve  like that used in [10]  and ﬁnd that each network

1Though several works have shown that if grid cells do obey a lattice-like structure  hexagonal lattices are

better than other lattice types for decoding position under noise [15] and are more economical [16].

2

GPWP/RNN/LSTMCDEBARNNLSTMUnconstrainedNonnegative1-layer NNPlace cell centersSimulated trajectoryUnconstrainedFigure1:Neuralnetworkstrainedonnormativetasksdevelopgrid-likeﬁringﬁelds.(A)Aschematicofthepositionencodingobjective.Dependingonthetask Gmayreceiveexternalinputsdirectlyfromplacecells(asin[10]) orfromanRNNoranLSTMthatinturnonlyreceivesvelocityinputs(asin[13 14]).Inthelattercase therecurrentnetmustgeneratedhiddenrepresentationsGthatconvertvelocityinputstoaplacecodePthroughasetoftrainedread-outweightsW.(B)Left:simulatedanimaltrajectory.Right:theplacecellcenters(dots)ofthedesiredplacecodePin(A)uniformlyandisotropicallytiletheenvironment.Bluetoredindicatelowtohighﬁringrateswhentheanimalisatthelocationontheleft.(C-E)Fromlefttoright wetrainasinglelayerneuralnetwork anRNN andanLSTMonplacecelloutputs reproducingtheresultsof[10 13 14].C)Whentheplacecellreceptiveﬁeld(left)isbroad allnetworkslearnsquaregrids.D)Whentheplacecellreceptiveﬁeldisadifferenceofgaussians allnetworkslearnamorphous quasi-periodicpatterns.E)Whenanonnegativityconstraintisimposedonhiddenunitactivations allnetworksnowlearnregularhexagonalgrids.developsamorphous quasi-periodicpatterns(Fig.1D).Thesepatternshavegrid-likefeatures andoccasionallyappearhexagonal muchlikethepatternsfoundin[14].Wealsonotethatweobtainedsimilaramorphouspatternswhenwetrainedwiththetuningcurvesusedin[14] whichareextremelysharplypeaked(1cm2) sothattheplacecellcodeisapproximatelyone-hot(Fig.6).Anon-negativityconstraintinduceshexagonalgrids.[10]observedthatimposinganon-negativityconstraintontheoutputoftheir1-layerneuralnetworkchangedthelearnedgridpatternsfromsquaretohexagonal.However sincesuchfeedforwardnetworksonlyconvertplacecellinputstooutputslearnedinanunsupervisedmanner itisaprioriunclearhownon-negativitymightimpacttheinternalrepresentationsofrecurrentneuralnetworkstrainedtoconvertvelocityinputstoplacecelloutputs.Toinvestigatewhetherthesameconstraintwouldalterthestructureofthepatternsob-servedintheotherarchitecturesandinmorecomplexnavigationaltasks weretrainallarchitectureswhileimposingnon-negativityontheactivationsofhiddenunitsthatultimatelygiverisetogridcells.Weﬁndthatthisconstraintconsistentlyyieldsregularhexagonalgridsineacharchitecture(Fig.1E).3ABFDEHIGCFigure2:Patternformationtheorypredictsstructureoflearnedrepresentations.A B)Visualizationof˜⌃and˜⌃⇤.D-E)Thetopsubspaceof˜⌃⇤isadegeneratering.Absentanyotherconstraint patternformingdynamicswillyieldarbitrarycombinationsofFouriermodesonthisring.B)Whentheringisclosetozero only90combinationsofmodesareavailableduetodiscretizationeffects yieldingsquarelattices.C)Whentheringislarger manydegeneratemodesareavailable.E)ThesoftenednonnegativityconstraintofEq.6inducesathree-bodyinteractionbetweentripletsofspatialfrequencieswhichaddtozero.Withinthetopsubspaceof˜⌃⇤ theseformanequilateraltriangle yieldingahexagonallattice.F)Thesumofthreeplanewavesat60interferestoformahexagonallatticeifallwavesareinphase.G-I)Numericalsimulationofpatternformingdynamics(Eq.5).G)When˜⌃ispeakednearzero patternformingdynamicsyieldsquaregrids.H)When˜⌃ispeakedfarfromzero patternformingdynamicsyieldquasi-periodiclatticescomprisedofmanymodes.I)Asoftenednonnegativityconstraintinducesaregularhexagonallattice.InsetsinG)-I)2DFouriertransformsofthelearnedmaps:4peaks widelydistributedactivity and6hexagonallydistributedpeaks respectively.Thiscollectionofresultsraisesfundamentalscientiﬁcquestions.Whydothesediversearchitectures acrossdiversetasks(bothnavigationandautoencoding) allconvergetoagrid-likesolution andwhatgovernsthelatticestructureofthissolution?Weaddressthisquestionbynotingthatalloptimizationproblemsinnetworks1-3containwithinthemacommonsub-problem whichwecallthepositionencodingobjective:selectinghiddenresponsesGandlinearlycombiningthemwithreadoutweightsWinordertopredictplacecellresponsesP(Fig.1A).Wefurthershowthatduetothetranslation-invarianceofplacecellresponses thelearningdynamicsofthispositionencodingobjectivecanbeformulatedasapatternformingdynamicalsystem allowingustounderstandthenatureandstructureoftheresultantgrid-likesolutionsandtheirdependenceonvariousparameters.3PatternformationtheorypredictsstructureoflearnedrepresentationsThecommonpositionencodingsub-problemidentiﬁedintheprevioussectioncanbemathemati-callyformulatedasminimizingthefollowingobjectivefunctionE(G W)=kPˆPk2F whereˆP=GW.(1)HereP2Rnx⇥nPrepresentstrueplacecellactivations wherePx iistheactivationofplacecelliatspatialindexx.G2Rnx⇥nGrepresentshiddenlayeractivations(whichwilllearngrid-likerepresentations) whereGx jistheactivationofhiddenunitjatspatialindexx.W2RnG⇥nPrepresentslinearreadoutweights whereWjiisthecontributionofgridcelljtoplacecelli.ˆP2Rnx⇥nPrepresentsthepredictionsoftheplacecellencodingsystem.Forsimplicity weconsideranL2penaltyonencodingerrors.BecauseweareultimatelyinterestedinthehiddenunitactivationsG wereplaceWwithitsoptimumvalueforﬁxedG(seeApp.B.1fordetails):argminWE(G W)=(GTG)1GTP.(2)4The objective E is unchanged by any transformation of the form G ! GZ  W ! Z1W . In partic-
ular  we can simplify our objective by choosing Z so that G’s columns are orthonormal. Enforcing
this constraint via Lagrange multipliers  we obtain the following Lagrangian

L = Tr⇥GT ⌃G  (GT G  I)⇤  

(3)
where ⌃= P P T is the nx⇥nx correlation matrix of place cell outputs. Note that assuming the place
cell receptive ﬁelds uniformly and isoptropically cover space  ⌃ will  in the limit of large numbers
of place cells  be approximately translation invariant (i.e. Toeplitz) and circularly symmetric. This
Lagrangian is optimized when G’s columns span the top nG eigenvectors of ⌃ (Eckart-Young-
Mirsky  see App. B.2)  and is invariant to a unitary transformation G ! GU. Moreover  since ⌃
is a Toeplitz matrix  the eigenvectors of ⌃ are approximately plane waves. Thus the optimization
in (3) yields arbitrary linear combinations of different plane wave eigenmodes of ⌃ corresponding
to the nG largest eigenvalues. However  this multiplicity of solutions is a special feature due to the
lack of any further constraints  like non-negativity. As we’ll see below  once a nonlinear constraint 
like non-negativity  is added  this multiplicity of solutions disappears  and the optimization favors a
single type of map corresponding to hexagonal grid cells.

3.1 Single-cell dynamics

To build intuition  we begin by studying the case of a single encoding cell g 2 Rnx and difference
of Gaussian place cell tuning. The Lagrangian for this cell is given by

Gradient ascent on this objective function at ﬁxed  yields the dynamics

L = gT ⌃g + (1  gT g).

d
dt

g = g +⌃ g.

(4)

(5)

This is a pattern forming dynamics in which the ﬁring ﬁelds at two positions gx and gx0 mutually
excite (inhibit) each other if the spatial autocorrelation ⌃xx0 of the desired place cell code at the
two positions x and x0 is positive (negative). Under this dynamics  patterns corresponding to the
eigenmodes of largest eigenvalue of ⌃ grow the fastest  with an exponential growth-rate given by
the corresponding eigenvalue. In actuality  to solve the constrained optimization problem in (4) we
run a projected gradient ascent algorithm in which we iteratively project g to the constraint surface
gT g = 1. Such a dynamics converges to a linear combination of degenerate eigenmodes of ⌃ all of
whom share the same maximal eigenvalue. Since ⌃ is translation invariant and circularly symmetric 
these corresponding eigenmodes are linear combinations of plane waves whose wave-vectors lie on
a ring in Fourier space whose radius k⇤ is determined by the top eigenvalue-eigenvector pair of ⌃.
In Fig. 2 we plot the eigenvalue associated to each plane wave as a function of its wave-vector
for a Gaussian (A) and difference of Gaussian (B) place cell tuning curve. For Gaussian tuning 
optimal wave-vectors  corresponding to the largest eigenvalues of ⌃  lie close to the origin  while
for difference of Gaussians tuning with covariance ⌃⇤  the optimal wave-vectors  corresponding to
the largest eigenvalues of ⌃⇤  are concentrated on a ring of radius k⇤ in Fourier-space  far from the
origin.
Consistent with this analysis and the results of Fig. 1  numerical simulations of pattern forming
dynamics corresponding to optimizing (4) yield quasi-periodic patterns like those in (Fig. 2G H).
In simulations  the ﬁnite box size discretizes Fourier space onto a lattice. Thus  numerical solutions
will consist of discrete combinations of plane waves with wave-vectors of radius k⇤. The lowest
nonzero Fourier modes occur at 0 and 90 on the Fourier lattice. Therefore  when ⌃’s spectrum
is peaked near the origin as in the case of Gaussian place cell tuning (Fig. 2C)  solutions will
be dominated by square grids like those found in [10  13]. Rings further from the origin may
occasionally intersect six lattice points  “accidentally” yielding hexagonal grids like those observed
in [14]. However  as the difference of Gaussian case shows  in general  optimal patterns can contain
any mixture of wavevectors from a ring (Fig 2D)  giving rise to amorphous patterns (Fig 2H; inset
shows Fourier power is distributed over whole ring). Indeed  the encoding objective considered
above does not prefer one type of lattice to another. As we will see  adding a nonnegativity constraint
to our objective breaks this degeneracy  and reliably picks out hexagonal solutions.

5

3.2 A nonnegativity constraint favors hexagonal grids

We have seen empirically that a nonnegativity constraint tends to produce hexagonal grids (Fig.
1E). To understand this effect  we add a softened nonnegativity constraint to our objective function
as follows

L = gT ⌃g + (1  gT g) + (g) 

(6)

where (g) penalizes negative activites in the map g. It will be convenient to write gx as g(~x) 
treating g as a scalar ﬁeld deﬁned for all points in space. Our objective then takes the form

L[g(~x)] =ZZ~x  ~x0

g(~x)⌃(~x ~x0)g(~x0) + ✓1 Z~x

g2(~x)◆ +Z~x

(g(~x)).

(7)

We can approximate the negativity penalty by Taylor expanding about 0: (g) ⇡ 0 + 1g + 2g2 +
3g3. Our Lagrangian then has a straightforward form in Fourier space

˜L[˜g(~k)] ⇡Z~k |˜g(~k)|2 ˜⌃(~k) + ˜✓1 Z~k |˜g(~k)|2◆

+ [0 + 1˜g(~0) + 2Z~k |˜g(~k)|2 + 3ZZZ~k ~k0 ~k00

˜g(~k)˜g(~k0)˜g(~k00)(~k + ~k0 + ~k00)].

(8)

0  1  and 2 will not qualitatively change the structure of the solutions. 0 simply shifts the
optimal value of L  but not its argmax; 1 controls the amount of the constant mode in the maps 
and does not affect their qualitative shape; and 2 can be absorbed into ˜ [17]. Critically  however 
the cubic term 3 introduces an interaction between wavevector triplets ~k  ~k0  ~k00 whenever the three
sum to zero (Fig. 2E).
In the limit of weak 3  the maps will be affected in two separate ways. First  weak 3 will pull the
maps slightly outside of the linear span of the optimal plane-waves  or eigenmodes of ⌃ of largest
eigenvalue. As 3 ! 0  this effect shrinks and effectively disappears  so that we can assume the
optimal maps are still constrained to be linear combinations of plane waves  with wave-vectors on
the same ring in Fourier space. The second  stronger effect is due to the fact that no matter how small
3 is made  it will break L’s symmetry  effectively forcing it to choose one solution from the set of
previously degenerate optima. Therefore  in the limit of small 3  we can determine the optimal
maps by considering which wavevector mixture on the ring of radius k⇤ maximizes the nonlinear
term

Lint =ZZZ~k ~k0 ~k00

˜g(~k)˜g(~k0)˜g(~k00)(~k + ~k0 + ~k00).

(9)

Subject to the normalization constraint R |˜g(~k)|2 = 1  this term is maximized when ˜g(~k) =
1p6P3
i=1 (~k  ~ki) + c.c.2  where ~k1 + ~k2 + ~k3 = 0. The only combination of ~k1  ~k2  ~k3 on
the ring of radius |k⇤| that sums to zero is an equilateral triangle (Fig. 2E). Therefore  rather than
arbitrary linear combinations of plane waves as in Eq. 1  the optimal solutions consist of three plane
waves with equal amplitude and wavevectors that lie on an equilateral triangle.

g(~x) =

1
p6

(ei~k1·~x+1 + ei~k2·~x+2 + ei~k3·~x+3 + c.c.).

(10)

The interaction Lint is maxizimed when 1 + 2 + 3 = 0  in which case the three plane waves
interfere to form a regular hexagonal lattice (Fig. 2F).

2Note that c.c. is shorthand for complex conjugate. For any real solution g(~x) to Eq. 6  ˜g(~k) = ˜g†(~k).

Therefore  for each wavevector ~k we must also include its negative  ~k.

6

We can optimize the above Lagrangian using the same pattern forming dynamics as in Eq. 5  under
the nonnegativity constraint  deﬁned above3. When we perform numerical simulations of this
dynamics  we ﬁnd regular hexagonal grids (Fig. 2I). Taking the 2D Fourier transform of the resulting
pattern reveals that the nonnegativity constraint has picked out three wavevectors oriented at 60
relative to one another (and their negatives) from the optimal ring of solutions (Fig. 2I  inset).

3.3 Hexagonal grids and g ! g symmetry breaking
We see from the above argument that the rectiﬁcation nonlinearity is but one of a large class of
nonlinearities which will favor hexagonal grids. A generic nonlinearity with a non-trivial cubic term
in its Taylor expansion will break the g ! g symmetry  and introduce a three-body interaction
which picks out hexagonal lattices. While nonnegativity is a speciﬁc nonlinearity motivated by
biological considerations  a broad class of nonlinearities will achieve the same effect (numerical
simulations in Appendix A1).

4 Multiple cells & nonnegativity yields hierarchies of hexagonal grids

We now return to the full Lagrangian with multiple grid cells (Eq. 3) 

L = Tr GT ⌃G +Xij

ij(I  GT G)ij + (G).

(11)

Recall that the solution space of the unperturbed objective with  = 0 corresponds to maps G whose
wavevectors fall on a series of concentric rings in Fourier space  ˜⌃top (Fig. 3B). By symmetry of L 
any unitary mixture of such a set of maps  G ! GU  will perform equally well. The nonlinearity 
then breaks U-symmetry and selects for speciﬁc mixtures of wavevectors. As before  the cubic term
3 induces a three-body interaction which promotes triplets of wavevectors that sum to zero.
If the number of maps to be learned nG is small enough that the top subspace rings ˜⌃top form a rela-
tively thin annulus in Fourier space  then all wavevector triplets that sum to 0 will be approximately
equilateral  giving rise to regular hexagonal grid maps. Once the number of maps to be learned
is large  the top subspace rings will form a thick annulus  inside of which many different triplet
wavevector arrangements - not just equilateral triangles - will sum to 0. Despite this  a signiﬁcant
fraction of maps learned in simulations are still hexagonal. As a ﬁrst step toward understanding
these results  we analyze a few possible non-equilateral triplets and show that the optimum has a
dominant equilateral component.
One simple non-equilateral arrangement is any triplet of the form (~k  ~k 2~k)  corresponding to a
stripe pattern with its ﬁrst overtone. In Appendix B.3  we prove that such an arrangement contributes
at most Lcoupled
=
2⇥2⇥3!/63/2 ⇡ 1.63. Another possibility is a hybrid map consisting of a mixture of two equilateral
triangles  one twice as big as the other. We prove that the optimal mixture puts most weight on either
the big or the small triangle  making the optimal solutions relatively pure-tone hexagonal maps.
Empirically  we can optimize the unperturbed multiple grid cell Lagrangian in Eq. 3 using the
pattern forming dynamics

= 3/2 to the Lagrangian  whereas an equilateral triangle contributes Ldecoupled

int

int

d
dt

G = G +⌃ G

(12)

and enforcing orthonormality of G. Introducing the nonnegativity constraint  we obtain dominantly
hexagonal maps across multiple spatial scales (Fig. 3D).
Historically  the roughly constant ratio of grid scale in neighboring MEC modules has led to interest
in geometric hierarchies - both which kinds of encoding objectives favor them  and which pattern
forming dynamics produce them. During simulations  we ﬁnd that lattice discretization effects can
sometimes create the illusion of a geometric hierarchy (3E). Roughly speaking  if ˜⌃ is peaked at the
origin  as the number of encoding maps is increased  wavevector rings are ﬁlled up one by one. Due
3In App. B.4 we prove that the dynamics satisﬁes KKT optimality conditions for the Lagrangian in Eq. 6.

7

Figure 3: A) Visualization of ˜⌃. B) Top subspace of ⌃ when multiple grid cells are available. Absent
any other constraint  there is a full rotational degree of freedom within this space. C) A cubic term
in the nonlinearity induces a three-body interaction between triplets of spatial frequencies which
add to zero. D) Results of multi-grid pattern forming dynamics with nonnegativity constraint show
regular hexagonal grids across multiple spatial scales. E) Left: distribution of grid scales for pattern
forming dynamics with Gaussian place cell tuning curve. This distribution can create the illusion of
geometric hierarchy  but is due to discretization restricting the lowest frequency modes to the lattice.
Right: (top) First ﬁve available spatial frequencies and (bottom): their corresponding wavevectors.

Three of the ﬁrst four lattice spacings are separated by a ratio of p2 ⇡ 1.4.

to the geometry of the lattice points in wave-vector space  the ﬁve wave-vectors of smallest length
(i.e. smallest spatial frequency) will have relative length ratios given by (1 p2 p2
).
These correspond exactly to the peaks we observe in 3E. Because these particular scale relationships
are strongly dependent on discretization effects and boundary conditions - both of which arise from
an ad hoc modelling decision to use a square  periodic environment - it is not clear that this effect in
the model is a potentially likely explanation for the apparent geometric hierarchy of maps in MEC.

 p5 p2

2

3

5 Unifying normative and mechanistic models of grid cells

This theory establishes a remarkable equivalence between optimizing the position encoding objec-
tive  and the recurrent neural network dynamics of continuous attractor models. To see this  consider
a closely related single-cell Lagrangian ¯L obtained by adding a constraint on the sum of the cell ac-
tivations:

¯L = gT ⌃g  gT g + µ1T g

(13)

Empirically  we ﬁnd that that this sum constraint minimally affects the structure of the optimal
patterns. We can optimize this objective for nonnegative g by stepping along ¯L’s gradient and
rectifying the result. In the limit of small step size  this becomes

d
dt

g =⇢g +⌃ g + µ
g + ⇥⌃g + µ⇤

8

g > 0
g = 0

(14)

ABDECGrid scale (cm)Probability density1.41.41.46090120150210180240where  is the rectifying nonlinearity. Interestingly  this is almost exactly the dynamics proposed in
continuous attractor models of grid cells [4  5  7  6]  with appropriate choice of time constant ⌧ and
scaling of the recurrent weights J and feedforward drive b:

⌧

d
dt

g = g + [Jg + b].

(15)

We prove in App. B.4 and B.5 that the dynamics of Eq. 14 and Eq. 15 have identical ﬁxed points
which satisfy KKT optimality conditions for the constrained position encoding objective. In this uni-
ﬁcation of normative and mechanistic models  the spatial autocorrelation structure ⌃ of the desired
place cell code in a normative position encoding model corresponds to the recurrent connectivity J
of the mechanistic RNN model. While the normative model learns grid cell ﬁring ﬁelds as a func-
tion of space by minimizing an encoding objective  the mechanistic RNN model generates stable
periodic bump patterns on a sheet of neurons by choosing translation invariant connectivity between
neurons on the sheet. Our theory predicts that if the neural nonlinearity ReLU in a mechanistic
model breaks ﬁring rate inversion symmetry g ! g  then periodic patterns on the neural sheet
should be hexagonal. Historically  a rectifying nonlinearity has indeed been used in mechanistic
models  and hexagonal grids have emerged. Consistent with our theory  other nonlinearities that
preserve the ﬁring rate inversion symmetry yield square grids (see Appendix A1).
This unifying connection between normative and mechanistic models yields an intuitive insight:
continuous attractor dynamics not only reproduce the patterns of activity observed in the MEC; they
are equivalent to optimizing the position encoding objective. That is  the patterns formed in the
continuous attractor model are also optimal for linearly generating place cell-like activations.

6 Discussion

Our unifying theory and numerical results suggest that much of the diversity and structure of so-
lutions obtained across many different normative models of MEC can be explained by the learning
dynamics of a simple linear  place cell encoding objective. This is intriguing given that the archi-
tectures and navigational tasks employed in [13  14] are considerably more sophisticated. Further
studies could explore how changing this common subproblem changes the solutions found by RNNs
trained to navigate. Moreover  our theory predicts why hexagonal grids should emerge in RNNs
trained to path integrate  but it does not explain how RNNs trained via backprop learn to stabilize
and update these patterns in order to path integrate. Future studies could reverse engineer these
trained networks to determine whether their inner workings coincide with the simple mechanistic
models we describe in Section 5 and have been proposed for the past 20 years.
Furthermore  our theory made simplifying assumptions about uniform  isotropic coverage of place
cell representations  yielding highly regular  stable grid cell patterns by solving the position encoding
objective. Our theoretical framework enables us to explore quantitatively how grid cell solutions
change when the environment is deformed  rewards or obstacles are incorporated  or place cells are
lesioned. Recent experiments have characterized the MECs response to each of these scenarios [18 
19  20  21]  but no uniﬁed theory has been put forth to explain the results. These questions could
potentially be addressed by drawing from a rich literature of how patterns respond to defects in
either spatial statistics or neural connectivity. Such defects could play a role in accounting for the
heterogeneity of grid cells [22]. Another interesting approach is to incorporate landmark inputs in
trained networks  in addition to velocity inputs. Such landmark inputs are known to correct drift in
the grid cell system [23] and can successfully account for various deformations in grid cell ﬁring
patterns due to environmental manipulations [8  9].
Finally  a growing body of work has explored experimentally the hypothesis that MEC encodes
continuous variables other than position  such as sound pitch [24] or abstract quantities like the
width and height of a bird [25]. While we have referred to a “position” encoding objective and
“path” integration  we note that our theory actually holds for generic continuous variables. That
is  we would expect networks trained to keep track of sound pitch and volume to behave the same
way. Perhaps  intriguingly  grid like structure may be relevant for neural processing in even more
abstract domains of semantic cognition [25  26]. Overall  the unifying pattern formation framework
we have identiﬁed  that spans both normative and mechanistic models  affords a powerful conceptual
tool to address many questions about the origins  structure  variability and robustness of grid-like
representations in the brain.

9

Acknowledgments
S.G. thanks the Simons  and James S. McDonnell foundations  and NSF Career 1845166 for funding.
B.S. thanks the Stanford Graduate Fellowship for ﬁnancial support.

References

[1] Torkel Hafting et al. “Microstructure of a spatial map in the entorhinal cortex”. In: Nature
436.7052 (Aug. 2005)  pp. 801–806. ISSN: 0028-0836. DOI: 10.1038/nature03721. URL:
http://www.nature.com/articles/nature03721.

[2] Vegard Heimly Brun et al. “Progressive increase in grid scale from dorsal to ventral medial

entorhinal cortex”. In: Hippocampus 18.12 (2008)  pp. 1200–1212.

[3] B L Mcnaughton  C A Barnes  and J O’keefe. The Contributions of Position  Direction  and
Velocity to Single Unit Activity in the Hippocampus of Freely-moving Rats. Tech. rep. 1983 
pp. 41–49. URL: https : / / link . springer . com / content / pdf / 10 . 1007 % 7B % 5C %
%7D2FBF00237147.pdf.

[4] William E Skaggs et al. An Information-Theoretic Approach to Deciphering the Hip-
pocampal Code. Tech. rep. URL: https : / / pdfs . semanticscholar . org / 079a /
c9a229f99400777bb433c96417d549761bb8 . pdf ? %7B % 5C _ %7Dga = 2 . 54977259 .
1986563027.1540487071-102463582.1538966432.

[5] Kechen Zhang. Representation of Spatial Orientation by the Intrinsic Dynamics of the Head-
Direction Cell Ensemble: A Theory. Tech. rep. 6. 1996  pp. 2112–2126. URL: http://www.
jneurosci.org/content/jneuro/16/6/2112.full.pdf.

[6] Mark C Fuhs and David S Touretzky. “A Spin Glass Model of Path Integration in Rat Medial
Entorhinal Cortex”. In: (2006). DOI: 10.1523/JNEUROSCI.4353- 05.2006. URL: http:
//www.jneurosci.org/content/jneuro/26/16/4266.full.pdf.

[7] Y Burak and I R Fiete. “Accurate Path Integration in Continuous Attractor Network Models
of Grid Cells”. In: PLoS Comput Biol 5.2 (2009)  p. 1000291. DOI: 10 . 1371 / journal .
pcbi.1000291. URL: www.ploscompbiol.org.

[8] Samuel A. Ocko et al. “Emergent elasticity in the neural code for space”. In: Proceedings
of the National Academy of Sciences of the United States of America 115.50 (Dec. 2018) 
E11798–E11806. ISSN: 10916490. DOI: 10.1073/pnas.1805959115.

[9] Malcolm G. Campbell et al. “Principles governing the integration of landmark and self-
motion cues in entorhinal cortical codes for navigation”. In: Nature Neuroscience 21.8 (Aug.
2018)  pp. 1096–1106. ISSN: 1097-6256. DOI: 10 . 1038 / s41593 - 018 - 0189 - y. URL:
http://www.nature.com/articles/s41593-018-0189-y.

[10] Yedidyah Dordek et al. “Extracting grid cell characteristics from place cell inputs using non-
negative principal component analysis”. In: eLife 5 (Mar. 2016). ISSN: 2050-084X. DOI: 10.
7554/eLife.10094. URL: https://elifesciences.org/articles/10094.

[11] Kimberly L Stachenfeld  Matthew M Botvinick  and Samuel J Gershman. “The hippocampus
as a predictive map”. In: Nature Neuroscience 20.11 (Nov. 2017)  pp. 1643–1653. ISSN: 1097-
6256. DOI: 10.1038/nn.4650. URL: http://www.nature.com/articles/nn.4650.
James C R Whittington et al. Generalisation of structural knowledge in the hippocampal-
entorhinal system. Tech. rep.

[12]

[13] Christopher J Cueva and Xue-Xin Wei. Emergence of Grid-like Representations by Train-
ing Recurrent Neural Networks to Perform Spatial Localization. Tech. rep. arXiv: 1803 .
07770v1. URL: https://arxiv.org/pdf/1803.07770.pdf.

[14] Andrea Banino et al. “Vector-based navigation using grid-like representations in artiﬁcial
agents”. In: Nature 557.7705 (2018)  pp. 429–433. ISSN: 0028-0836. DOI: 10 . 1038 /
s41586-018-0102-6.

[15] Alexander Mathis  Martin B. Stemmier  and Andreas V.M. Herz. “Probable nature of higher-
dimensional symmetries underlying mammalian grid-cell activity patterns”. In: eLife 2015.4
(2015)  pp. 1–29. ISSN: 2050084X. DOI: 10.7554/eLife.05979. arXiv: 1411.2136.

10

[16] Xue-Xin Wei  Jason Prentice  and Vijay Balasubramanian. “A principle of economy predicts
the functional architecture of grid cells”. In: Elife 4 (2015)  e08362. DOI: 10.7554/eLife.
08362.

[17] M C Cross and P C Hohenberg. “Pattern formation outside of equilibrium”. In: Rev. Mod.
Phys. 65.3 (July 1993)  pp. 851–1112. DOI: 10.1103/RevModPhys.65.851. URL: https:
//link.aps.org/doi/10.1103/RevModPhys.65.851.

[18] William N Butler  Kiah Hardcastle  and Lisa M Giocomo. “Remembered reward locations
restructure entorhinal spatial maps.” In: Science (New York  N.Y.) 363.6434 (Mar. 2019) 
pp. 1447–1452. ISSN: 1095-9203. DOI: 10.1126/science.aav5297. URL: http://www.
ncbi.nlm.nih.gov/pubmed/30923222%20http://www.pubmedcentral.nih.gov/
articlerender.fcgi?artid=PMC6516752.
Julija Krupic et al. “Grid cell symmetry is shaped by environmental geometry”. In: Nature
518.7538 (Feb. 2015)  pp. 232–235. ISSN: 0028-0836. DOI: 10.1038/nature14153. URL:
http://www.nature.com/articles/nature14153.

[19]

[20] Revekka Ismakov et al. “Grid Cells Encode Local Positional Information”. In: Curr Biol

[21]

27.15 (2017)  2337–2343.e3. ISSN: 0960-9822. DOI: 10.1016/j.cub.2017.06.034.
Jena B Hales et al. “Medial Entorhinal Cortex Lesions Only Partially Disrupt Hippocampal
Place Cells and Hippocampus-Dependent Place Memory”. In: CellReports 9 (2014)  pp. 893–
901. DOI: 10.1016/j.celrep.2014.10.009. URL: http://dx.doi.org/10.1016/j.
celrep.2014.10.009.

[22] Kiah Hardcastle et al. “A Multiplexed  Heterogeneous  and Adaptive Code for Navigation in
Medial Entorhinal Cortex”. In: (2017). DOI: 10 . 1016 / j . neuron . 2017 . 03 . 025. URL:
http://dx.doi.org/10.1016/j.neuron.2017.03.025.

[23] Kiah Hardcastle  Surya Ganguli  and Lisa M. Giocomo. “Environmental Boundaries as an
Error Correction Mechanism for Grid Cells”. In: Neuron 86.3 (May 2015)  pp. 827–839.
ISSN: 10974199. DOI: 10.1016/j.neuron.2015.03.039.

[24] Dmitriy Aronov  Rhino Nevers  and David W. Tank. “Mapping of a non-spatial dimension
by the hippocampal-entorhinal circuit”. In: Nature 543.7647 (Mar. 2017)  pp. 719–722. ISSN:
14764687. DOI: 10.1038/nature21692.

[25] Alexandra O. Constantinescu  Jill X. O’Reilly  and Timothy E.J. Behrens. “Organizing con-
ceptual knowledge in humans with a gridlike code”. In: Science 352.6292 (June 2016) 
pp. 1464–1468. ISSN: 10959203. DOI: 10.1126/science.aaf0941.

[26] Andrew M. Saxe  James L. McClelland  and Surya Ganguli. “A mathematical theory of se-
mantic development in deep neural networks”. In: Proceedings of the National Academy
of Sciences of the United States of America 166.23 (June 2019)  pp. 11537–11546. ISSN:
10916490. DOI: 10.1073/pnas.1820226116. arXiv: 1810.10531.

[27] Uˇgur M. Erdem and Michael Hasselmo. “A goal-directed spatial navigation model using
forward trajectory planning based on grid cells”. In: European Journal of Neuroscience 35.6
(Mar. 2012)  pp. 916–931. ISSN: 0953816X. DOI: 10.1111/j.1460-9568.2012.08015.x.

11

,Ben Sorscher
Gabriel Mel
Surya Ganguli
Samuel Ocko