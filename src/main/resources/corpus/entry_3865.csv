2008,Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks,Offline handwriting recognition---the transcription of images of handwritten text---is an interesting task  in that it combines computer vision with sequence learning. In most systems the two elements are handled separately  with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems  it does not require any alphabet specific preprocessing  and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition  where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.,Ofﬂine Handwriting Recognition with

Multidimensional Recurrent Neural Networks

Alex Graves

TU Munich  Germany
graves@in.tum.de

J¨urgen Schmidhuber

IDSIA  Switzerland and TU Munich  Germany

juergen@idsia.ch

Abstract

Ofﬂine handwriting recognition—the automatic transcription of images of hand-
written text—is a challenging task that combines computer vision with sequence
learning. In most systems the two elements are handled separately  with sophisti-
cated preprocessing techniques used to extract the image features and sequential
models such as HMMs used to provide the transcriptions. By combining two re-
cent innovations in neural networks—multidimensional recurrent neural networks
and connectionist temporal classiﬁcation—this paper introduces a globally trained
ofﬂine handwriting recogniser that takes raw pixel data as input. Unlike competing
systems  it does not require any alphabet speciﬁc preprocessing  and can therefore
be used unchanged for any language. Evidence of its generality and power is pro-
vided by data from a recent international Arabic recognition competition  where it
outperformed all entries (91.4% accuracy compared to 87.2% for the competition
winner) despite the fact that neither author understands a word of Arabic.

1 Introduction

Ofﬂine handwriting recognition is generally observed to be harder than online handwriting recogni-
tion [14]. In the online case  features can be extracted from both the pen trajectory and the resulting
image  whereas in the ofﬂine case only the image is available. Nonetheless  the standard recognition
process is essentially the same: a sequence of features are extracted from the data  then matched to a
sequence of labels (usually characters or sub-character strokes) using either a hidden Markov model
(HMM) [9] or an HMM-neural network hybrid [10].
The main drawback of this approach is that the input features must meet the stringent independence
assumptions imposed by HMMs (these assumptions are somewhat relaxed in the case of hybrid
systems  but long-range input dependencies are still problematic). In practice this means the features
must be redesigned for every alphabet  and  to a lesser extent  for every language. For example it
would be impossible to use the same system to recognise both English and Arabic.
Following our recent success in transcribing raw online handwriting data with recurrent net-
works [6]  we wanted to build an ofﬂine recognition system that would work on raw pixels. As well
as being alphabet-independent  such a system would have the advantage of being globally trainable 
with the image features optimised along with the classiﬁer.
The online case was relatively straightforward  since the input data formed a 1D sequence that could
be fed directly to a recurrent network. The long short-term memory (LSTM) network architec-
ture [8  3] was chosen for its ability to access long-range context  and the connectionist temporal
classiﬁcation [5] output layer allowed the network to transcribe the data with no prior segmentation.
The ofﬂine case  however  is more challenging  since the input is no longer one-dimensional. A
naive approach would be to present the images to the network one vertical line at a time  thereby
transforming them into 1D sequences. However such a system would be unable to handle distor-

1

Figure 1: Two dimensional MDRNN. The thick lines show connections to the current point (i  j).
The connections within the hidden layer plane are recurrent. The dashed lines show the scanning
strips along which previous points were visited  starting at the top left corner.

tions along the vertical axis; for example the same image shifted up by one pixel would appear
completely different. A more ﬂexible solution is offered by multidimensional recurrent neural net-
works (MDRNNs) [7]. MDRNNs  which are a special case of directed acyclic graph networks [1] 
generalise standard RNNs by providing recurrent connections along all spatio-temporal dimensions
present in the data. These connections make MDRNNs robust to local distortions along any com-
bination of input dimensions (e.g. image rotations and shears  which mix vertical and horizontal
displacements) and allow them to model multidimensional context in a ﬂexible way. We use multi-
dimensional LSTM because it is able to access long-range context.
The problem remains  though  of how to transform two-dimensional images into one-dimensional
label sequences. Our solution is to pass the data through a hierarchy of MDRNN layers  with
blocks of activations gathered together after each level. The heights of the blocks are chosen to
incrementally collapse the 2D images onto 1D sequences  which can then be labelled by the output
layer. Such hierarchical structures are common in computer vision [15]  because they allow complex
features to be built up in stages. In particular our multilayered structure is similar to that used by
convolution networks [11]  although it should be noted that because convolution networks are not
recurrent  they cannot be used for cursive handwriting recognition without presegmented inputs.
The method is described in detail in Section 2  experimental results are given in Section 3  and
conclusions and directions for future work are given in Section 4.

2 Method

The three components of our recognition system are: (1) multidimensional recurrent neural net-
works  and multidimensional LSTM in particular; (2) the connectionist temporal classiﬁcation out-
put layer; and (3) the hierarchical structure. In what follows we describe each component in turn 
then show how they ﬁt together to form a complete system. For a more detailed description of (1)
and (2) we refer the reader to [4]

2.1 Multidimensional Recurrent Neural Networks

The basic idea of multidimensional recurrent neural networks (MDRNNs) [7] is to replace the single
recurrent connection found in standard recurrent networks with as many connections as there are
spatio-temporal dimensions in the data. These connections allow the network to create a ﬂexible
internal representation of surrounding context  which is robust to localised distortions.
An MDRNN hidden layer scans through the input in 1D strips  storing its activations in a buffer. The
strips are ordered in such a way that at every point the layer has already visited the points one step
back along every dimension. The hidden activations at these previous points are fed to the current
point through recurrent connections  along with the input. The 2D case is illustrated in Fig. 1.
One such layer is sufﬁcient to give the network access to all context against the direction of scan-
ning from the current point (e.g. to the top and left of (i  j) in Fig. 1). However we usually want
surrounding context in all directions. The same problem exists in 1D networks  where it is often
useful to have information about the future as well as the past. The canonical 1D solution is bidi-

2

rectional recurrent networks [16]  where two separate hidden layers scan through the input forwards
and backwards. The generalisation of bidirectional networks to n dimensions requires 2n hidden
layers  starting in every corner of the n dimensional hypercube and scanning in opposite directions.
For example  a 2D network has four layers  one starting in the top left and scanning down and right 
one starting in the bottom left and scanning up and right  etc. All the hidden layers are connected to
a single output layer  which therefore receives information about all surrounding context.
The error gradient of an MDRNN can be calculated with an n-dimensional extension of backprop-
agation through time. As in the 1D case  the data is processed in the reverse order of the forward
pass  with each hidden layer receiving both the output derivatives and its own n ‘future’ derivatives
at every timestep.
Let ap
j be respectively the input and activation of unit j at point p = (p1  . . .   pn) in an n-
d = (p1  . . .   pd − 1  . . .   pn)
dimensional input sequence x with dimensions (D1  . . .   Dn). Let p−
and p+
ij be respectively the weight of the feedforward
connection from unit i to unit j and the recurrent connection from i to j along dimension d. Let θh
be the activation function of hidden unit h  and for some unit j and some differentiable objective
function O let δp
. Then the forward and backward equations for an n-dimensional MDRNN
with I input units  K output units  and H hidden summation units are as follows:

d = (p1  . . .   pd + 1  . . .   pn). Let wij and wd

j = ∂O
∂ap
j

j and bp

Forward Pass

IX

ap
h =

xp
i wih +

i=1

h = θh(ap
bp
h)

nX

HX

d=1:
pd>0

ˆh=1

−
d

p
ˆh

b

wd
ˆhh

Backward Pass

δp
h = θ

(cid:48)
h(ap
h)

0B@ KX

k=1

nX

HX

+
d

p
δ
ˆh

wd
hˆh

δp
k whk +

d=1:

pd<Dd−1

ˆh=1

1CA

2.1.1 Multidimensional LSTM

Long Short-Term Memory (LSTM) [8  3] is an RNN architecture designed for data with long-range
interdependencies. An LSTM layer consists of recurrently connected ‘memory cells’  whose activa-
tions are controlled by three multiplicative gate units: the input gate  forget gate and output gate. The
gates allows the cells to store and retrieve information over time  giving them access to long-range
context.
The standard formulation of LSTM is explicitly one-dimensional  since each cell contains a single
recurrent connection  whose activation is controlled by a single forget gate. However we can extend
this to n dimensions by using instead n recurrent connections (one for each of the cell’s previous
states along every dimension) with n forget gates.
Consider an MDLSTM memory cell in a hidden layer of H cells  connected to I input units and K
output units. The subscripts c  ι  φ and ω refer to the cell  input gate  forget gate and output gate
respectively. bp
c is
the state of cell c at p. f1 is the activation function of the gates  and f2 and f3 are respectively the
cell input and output activation functions. The sufﬁx φ  d denotes the forget gate corresponding to
recurrent connection d. The input gate ι is connected to previous cell c along all dimensions with
the same weight (wcι) whereas the forget gates are connected to cell c with a separate weight wc(φ d)
for each dimension d. Then the forward and backward equations are as follows:
Forward Pass

h is the output of cell h in the hidden layer at point p in the input sequence  and sp

Input Gate: bp

ι = f1

xp
i wiι +

−
p
c +
d

wcιs

p

−
h wd
d

hι

b

Forget Gate: bp

φ d = f1

!1CA
(

−
h wd(cid:48)
d(cid:48)
p

h(φ d) +

−
d

p
c

wc(φ d)s
0 otherwise

1CCA

if pd > 0

0B@ IX
0BB@ IX

i=1

i=1

 

nX

HX

h=1

d=1:
pd>0

xp
i wi(φ d) +

HX

h=1

b

nX

d(cid:48)=1:
pd(cid:48) >0

3

IX

i=1

Cell: ap

c =

xp
i wic +

State: sp

c = bp

ι f2(ap

c ) +

HX

h=1

b

p

−
h wd
d

hc

nX
0B@ IX

d=1:
pd>0

i=1

nX

HX

b

h=1

d=1:
pd>0

p

−
h wd
d

hω + wcωsp
c

1CA

nX

d=1:
pd>0

−
p
c bp
d

φ d

s

Output Gate: bp

ω = f1

xp
i wiω +

Cell Output: bp

c = bp

ωf3(sp
c )

Backward Pass

Cell Output: p
c

def
=

∂O
∂bp
c

=

KX

k=1

δp
k wck +

Output Gate: δp

ω = f

(cid:48)
1(ap

ω)p

c f3(sp
c )

State: p
s

def
=

∂O
∂sp
c

= bp
ωf

(cid:48)
3(sp

c )p

c + δp

ωwcω +

p

+
d

h wd
δ

ch

nX

h=1

HX
„
(

d=1:

pd<Dd−1

nX

+
p

s b
d
pd<Dd−1

d=1:

Cell: δp

c = bp
ι f

(cid:48)
2(ap

c )p
s

Forget Gate: δp

φ d =

Input Gate: δp

ι = f

(cid:48)
1(ap

ι )f2(ap

c )p
s

−
d

f(cid:48)
p
1(ap
c
0 otherwise

φ d)s

s if pd > 0
p

«

+
p
φ dwc(φ d)
d

+
p
φ d + δ
d

+
d

p
ι wcι + δ

2.2 Connectionist Temporal Classiﬁcation

Connectionist temporal classiﬁcation (CTC) [5] is an output layer designed for sequence labelling
with RNNs. Unlike other neural network output layers it does not require pre-segmented training
data  or postprocessing to transform its outputs into transcriptions. Instead  it trains the network to
directly estimate the conditional probabilities of the possible labellings given the input sequences.
A CTC output layer contains one more unit than there are elements in the alphabet L of labels for the
task. The output activations are normalised at each timestep with the softmax activation function [2].
The ﬁrst |L| outputs estimate the probabilities of observing the corresponding labels at that time  and
the extra output estimates the probability of observing a ‘blank’  or no label. The combined output
sequence estimates the joint probability of all possible alignments of the input sequence with all
sequences of labels and blanks. The probability of a particular labelling can then be estimated by
summing over the probabilities of all the alignments that correspond to it.
More precisely  for a length T input sequence x  the CTC outputs deﬁne a probability distribution
over the set L(cid:48)T of length T sequences over the alphabet L(cid:48) = L ∪ {blank}. To distinguish them
from labellings  we refer to the elements of L(cid:48)T as paths. Since the probabilities of the labels at
each timestep are conditionally independent given x  the conditional probability of a path π ∈ L(cid:48)T

is given by p(π|x) =(cid:81)T
L≤T is the sum of the probabilities of all paths corresponding to it: p(l|x) = (cid:80)

Paths are mapped onto labellings l ∈ L≤T by an operator B that removes ﬁrst the repeated labels 
then the blanks. So for example  both B(a −  a  b −) and B(−  a  a − −  a  b  b) yield the labelling
(a  a  b). Since the paths are mutually exclusive  the conditional probability of some labelling l ∈
π∈B−1(l) p(π|x).
Although a naive calculation of this sum is unfeasible  it can be efﬁciently evaluated with a dynamic
programming algorithm  similar to the forward-backward algorithm for HMMs.
To allow for blanks in the output paths  for each labelling l ∈ L≤T consider a modiﬁed labelling
l(cid:48) ∈ L(cid:48)≤T   with blanks added to the beginning and the end and inserted between every pair of labels.
The length |l(cid:48)| of l(cid:48) is therefore 2|l| + 1.
For a labelling l  deﬁne the forward variable αt(s) as the summed probability of all path beginnings
reaching index s of l(cid:48) at time t  and the backward variables βt(s) as the summed probability of all
path endings that would complete the labelling l if the path beginning had reached s at time t. Both

k is the activation of output unit k at time t.

. where yt

t=1 yt
πt

4

p(l|x) =(cid:80)|l(cid:48)|
labelling all of S: O = −(cid:80)

s=1 αt(s)βt(s).

the forward and backward variables are calculated recursively [5]. The label sequence probability
is given by the sum of the products of the forward and backward variables at any timestep  i.e.

Let S be a training set  consisting of pairs of input and target sequences (x  z)  where |z| ≤ |x|.
Then the objective function O for CTC is the negative log probability of the network correctly
(x z)∈S ln p(z|x). The network can be trained with gradient descent by
ﬁrst differentiating O with respect to the outputs  then using backpropagation through time to ﬁnd
the derivatives with respect to the weights.
Note that the same label (or blank) may be repeated several times for a single labelling l. We deﬁne
the set of positions where label k occurs as lab(l  k) = {s :
s = k}  which may be empty.
l(cid:48)
Setting l = z and differentiating O with respect to the network outputs  we obtain:

− ∂O
∂at
k

= − ∂ ln p(z|x)

∂at
k

= yt

k − 1

p(z|x)

(cid:88)

αt(s)βt(s) 

s∈lab(z k)

k and yt

k are respectively the input and output of CTC unit k at time t for some (x  z) ∈ S.
where at
Once the network is trained  we can label some unknown input sequence x by choosing the labelling
l∗ with the highest conditional probability  i.e. l∗ = arg maxl p(l|x). In cases where a dictionary
is used  the labelling can be constrained to yield only sequences of complete words by using the
CTC token passing algorithm [6]. For the experiments in this paper  the labellings were further
constrained to give single word sequences only  and the ten most probable words were recorded.

2.3 Network Hierarchy

Many computer vision systems use a hierarchical approach to feature extraction  with the features
at each level used as input to the next level [15]. This allows complex visual properties to be built
up in stages. Typically  such systems use subsampling  with the feature resolution decreased at each
stage. They also generally have more features at the higher levels. The basic idea is to progress from
a small number of simple local features to a large number of complex global features.
We created a hierarchical structure by repeatedly composing MDLSTM layers with feedforward
layers. The basic procedure is as follows: (1) the image is divided into small pixel blocks  each of
which is presented as a single input to the ﬁrst set of MDLSTM layers (e.g. a 4x3 block is reduced
to a length 12 vector). If the image does not divide exactly into blocks  it is padded with zeros.
(2) the four MDLSTM layers scan through the pixel blocks in all directions. (3) the activations of
the MDLSTM layers are collected into blocks. (4) these blocks are given as input to a feedforward
layer. Note that all the layers have a 2D array of activations: e.g. a 10 unit feedforward layer with
input from a 5x5 array of MDLSTM blocks has a total of 250 activations.
The above process is repeated as many times as required  with the activations of the feedforward
layer taking the place of the original image. The purpose of the blocks is twofold: to collect local
contextual information  and to reduce the area of the activation arrays. In particular  we want to
reduce the vertical dimension  since the CTC output layer requires a 1D sequence as input. Note
that the blocks themselves do not reduce the overall amount of data; that is done by the layers that
process them  which are therefore analogous to the subsampling steps in other approaches (although
with trainable weights rather than a ﬁxed subsampling function).
For most tasks we ﬁnd that a hierarchy of three MDLSTM/feedforward stages gives the best results.
We use the standard ‘inverted pyramid’ structure  with small layers at the bottom and large layers at
the top. As well as allowing for more features at higher levels  this leads to efﬁcient networks  since
most of the weights are concentrated in the upper layers  which have a smaller input area.
In general we cannot assume that the input images are of ﬁxed size. Therefore it is difﬁcult to choose
block heights that ensure that the ﬁnal activation array will always be one-dimensional  as required
by CTC. A simple solution is to collapse the ﬁnal array by summing over all the inputs in each
vertical line  i.e. the input at time t to CTC unit k is given by at
is the
uncollapsed input to unit k at point (x  y) in the ﬁnal array.

k =(cid:80)

  where a(x y)

k

x a(x t)

k

5

Figure 2: The complete recognition system. First the input image is collected into boxes 3 pixels
wide and 4 pixels high which are then scanned by four MDLSTM layers. The activations of the cells
in each layer are displayed separately  and the arrows in the corners indicates the scanning direction.
Next the MDLSTM activations are gathered into 4 x 3 boxes and fed to a feedforward layer of tanh
summation units. This process is repeated two more times  until the ﬁnal MDLSTM activations are
collapsed to a 1D sequence and transcribed by the CTC layer. In this case all characters are correctly
labelled except the second last one  and the correct town name is chosen from the dictionary.

3 Experiments

To see how our method compared to the state of the art  we applied it to data from the ICDAR
2007 Arabic handwriting recognition competition [12]. Although we were too late to enter the
competition itself  the organisers kindly agreed to evaluate our system according to the competition
criteria. We did not receive the test data at any point  and all evaluations were carried out by them.
The goal of the competition was to identify the postcodes of Tunisian town and village names. The
names are presented individually  so it is an isolated word recognition task. However we would
like to point out that our system is equally applicable to unconstrained handwriting  and has been
successfully applied to complete lines of English text.

3.1 Data

The competition was based on the IFN/ENIT database of handwritten Arabic words [13]. The
publically available data consists of 32 492 images of handwritten Tunisian town names  of which
we used 30 000 for training  and 2 492 for validation. The images were extracted from artiﬁcial

6

Table 1: Results on the ICDAR 2007 Arabic handwriting recognition contest. All scores are
percentages of correctly identiﬁed postcodes. The systems are ordered by the ‘top 1’ results on test
set ‘f’. The best score in each column is shown in bold.

SYSTEM

CACI-3
CACI-2
CEDAR
MITRE
UOB-ENST-1
PARIS V
ICRA
UOB-ENST-2
UOB-ENST-4
UOB-ENST-3
SIEMENS-1
MIE
SIEMENS-2
Ours

top 1
14.28
15.79
59.01
61.70
79.10
80.18
81.47
81.65
81.81
81.93
82.77
83.34
87.22
91.43

SET f
top 5
29.88
21.34
78.76
81.61
87.69
91.09
90.07
90.81
88.71
91.20
92.37
91.67
94.05
96.12

top 10
37.91
22.33
83.70
85.69
90.21
92.98
92.15
92.35
90.40
92.76
93.92
93.48
95.42
96.75

top 1
10.68
14.24
41.32
49.91
64.97
64.38
72.22
69.61
70.57
69.93
68.09
68.40
73.94
78.83

SET s
top 5
21.74
19.39
61.98
70.50
78.39
78.12
82.84
83.79
79.85
84.11
81.70
80.93
85.44
88.00

top 10
30.20
20.53
69.87
76.48
82.20
82.13
86.27
85.89
83.34
87.03
85.19
83.73
88.18
91.05

forms ﬁlled in by over 400 Tunisian people. The forms were designed to simulate writing on a
letter  and contained no lines or boxes to constrain the writing style.
Each image was supplied with a ground truth transcription for the individual characters1. There were
120 distinct characters in total. A list of 937 town names and postcodes was provided. Many of the
town names had transcription variants  giving a total of 1 518 entries in the complete dictionary.
The test data (which is not published) was divided into sets ‘f’ and ‘s’. The main competition results
were based on set ‘f’. Set ‘s’ contains data collected in the United Arab Emirates using the same
forms; its purpose was to test the robustness of the recognisers to regional writing variations. The
systems were allowed to choose up to 10 postcodes for each image  in order of preference. The test
set performance using the top 1  top 5  and top 10 answers was recorded by the organisers.

3.2 Network Parameters

The structure shown in Figure 2 was used  with each layer fully connected to the next layer in the
hierarchy  all MDLSTM layers connected to themselves  and all units connected to a bias weight.
There were 159 369 weights in total. This may sound like a lot  but as mentioned in Section 2.3  the
‘inverted pyramid’ structure greatly reduces the actual number of weight operations. In effect the
higher up networks (where the vast majority of the weights are concentrated) are processing much
smaller images than those lower down. The squashing function for the gates was the logistic sigmoid
f1(x) = 1/(1 + e−x)  while tanh was used for f2 and f3. Each pass through the training set took
about an hour on a desktop computer  and the network converged after 85 passes.
The complete system was trained with online gradient descent  using a learning rate of 10−4 and
a momentum of 0.9. The character error rate was evaluated on the validation set after every pass
through the training set  and training was stopped after 50 evaluations with no improvement. The
weights giving the lowest error rate on the validation set were passed to the competition organisers
for assessment on the test sets.

3.3 Results

Table 1 clearly shows that our system outperformed all entries in the 2007 ICDAR Arabic recogni-
tion contest. The other systems  most of which are based on hidden Markov models  are identiﬁed
by the names of the groups that submitted them (see [12] for more information).

1At ﬁrst we forgot that Arabic reads right to left and presented the transcriptions backwards. The system

performed surprisingly well  with a character error rate of 17.8%  compared to 10.7% for the correct targets.

7

4 Conclusions and Future Work

We have combined multidimensional LSTM with connectionist temporal classiﬁcation and a hierar-
chical layer structure to create a powerful ofﬂine handwriting recogniser. The system is very general 
and has been successfully applied to English as well as Arabic. Indeed  since the dimensionality of
the networks can be changed to match that of the data  it could in principle be used for almost any
supervised sequence labelling task.

Acknowledgements

We would like to thank Haikal El Abed for giving us access to the ICDAR competition data  and
for persisting in the face of technical despair to install and evaluate our software. This work was
supported by the excellence cluster “Cognition for Technical Systems” (CoTeSys) from the German
Research Foundation (DFG).

References
[1] P. Baldi and G. Pollastri. The principled design of large-scale recursive neural network architectures–dag-

rnns and the protein structure prediction problem. J. Mach. Learn. Res.  4:575–602  2003.

[2] J. S. Bridle. Probabilistic interpretation of feedforward classiﬁcation network outputs  with relationships
to statistical pattern recognition. In F. Fogleman-Soulie and J.Herault  editors  Neurocomputing: Algo-
rithms  Architectures and Applications  pages 227–236. Springer-Verlag  1990.

[3] F. Gers  N. Schraudolph  and J. Schmidhuber. Learning precise timing with LSTM recurrent networks.

Journal of Machine Learning Research  3:115–143  2002.

[4] A. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. PhD thesis.
[5] A. Graves  S. Fern´andez  F. Gomez  and J. Schmidhuber. Connectionist temporal classiﬁcation: La-
belling unsegmented sequence data with recurrent neural networks. In Proceedings of the International
Conference on Machine Learning  ICML 2006  Pittsburgh  USA  2006.

[6] A. Graves  S. Fern´andez  M. Liwicki  H. Bunke  and J. Schmidhuber. Unconstrained online handwriting
In J. Platt  D. Koller  Y. Singer  and S. Roweis  editors 

recognition with recurrent neural networks.
Advances in Neural Information Processing Systems 20. MIT Press  Cambridge  MA  2008.

[7] A. Graves  S. Fern´andez  and J. Schmidhuber. Multidimensional recurrent neural networks.

In Pro-
ceedings of the 2007 International Conference on Artiﬁcial Neural Networks  Porto  Portugal  September
2007.

[8] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation  9(8):1735–1780 

1997.

[9] J. Hu  S. G. Lim  and M. K. Brown. Writer independent on-line handwriting recognition using an HMM

approach. Pattern Recognition  33:133–147  2000.

[10] S. Jaeger  S. Manke  J. Reichert  and A. Waibel. On-line handwriting recognition: the NPen++ recognizer.

International Journal on Document Analysis and Recognition  3:169–180  2001.

[11] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  November 1998.

[12] V. Margner and H. E. Abed. Arabic handwriting recognition competition. In ICDAR ’07: Proceedings of
the Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2  pages
1274–1278  Washington  DC  USA  2007. IEEE Computer Society.

[13] M. Pechwitz  S. S. Maddouri  V. Mrgner  N. Ellouze  and H. Amiri. IFN/ENIT-database of handwritten
In 7th Colloque International Francophone sur l’Ecrit et le Document (CIFED 2002) 

arabic words.
Hammamet  Tunis  2002.

[14] R. Plamondon and S. N. Srihari. On-line and off-line handwriting recognition: a comprehensive survey.

IEEE Transactions on Pattern Analysis and Machine Intelligence  2000.

[15] M. Reisenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience 

2(11):1019–1025  1999.

[16] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal

Processing  45:2673–2681  November 1997.

8

,Kareem Amin
Afshin Rostamizadeh
Umar Syed
Janne Korhonen
Pekka Parviainen
Ashish Kumar
Saurabh Gupta
David Fouhey
Sergey Levine
Jitendra Malik