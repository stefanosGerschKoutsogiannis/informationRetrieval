2013,Learning Multiple Models via Regularized Weighting,We consider the general problem of Multiple Model Learning (MML) from data  from the statistical and algorithmic perspectives; this problem includes clustering  multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd's algorithm  for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.   We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important  case of clustering that our approach has a non-trivial breakdown point  i.e.  is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.,Learning Multiple Models via Regularized Weighting

Daniel Vainsencher

Shie Mannor

Department of Electrical Engineering

Department of Electrical Engineering

Technion  Haifa  Israel

Technion  Haifa  Israel

danielv@tx.technion.ac.il

shie@ee.technion.ac.il

Huan Xu

Mechanical Engineering Department

National University of Singapore  Singapore

mpexuh@nus.edu.sg

Abstract

We consider the general problem of Multiple Model Learning (MML) from data 
from the statistical and algorithmic perspectives; this problem includes clustering 
multiple regression and subspace clustering as special cases. A common approach
to solving new MML problems is to generalize Lloyd’s algorithm for clustering
(or Expectation-Maximization for soft clustering). However this approach is un-
fortunately sensitive to outliers and large noise: a single exceptional point may
take over one of the models.
We propose a different general formulation that seeks for each model a distribu-
tion over data points; the weights are regularized to be sufﬁciently spread out.
This enhances robustness by making assumptions on class balance. We further
provide generalization bounds and explain how the new iterations may be com-
puted efﬁciently. We demonstrate the robustness beneﬁts of our approach with
some experimental results and prove for the important case of clustering that our
approach has a non-trivial breakdown point  i.e.  is guaranteed to be robust to a
ﬁxed percentage of adversarial unbounded outliers.

1

Introduction

The standard approach to learning models from data assumes that the data were generated by a
certain model  and the goal of learning is to recover this generative model. For example  in linear
regression  an unknown linear functional  which we want to recover  is believed to have generated
covariate-response pairs. Similarly  in principal component analysis  a random variable in some
unknown low-dimensional subspace generated the observed data  and the goal is to recover this
low-dimensional subspace. Yet  in practice  it is common to encounter data that were generated by a
mixture of several models rather than a single one  and the goal is to learn a number of models such
that any given data can be explained by at least one of the learned models. It is also common for the
data to contain outliers: data-points that are not well explained by any of the models to be learned 
possibly inserted by external processes.

We brieﬂy explain our approach (presented in detail in the next section). At its center is the problem
of assigning data points to models  with the main consideration that every model be consistent with
many of the data points. Thus we seek for each model a distribution of weights over the data
points  and encourage even weights by regularizing these distributions (hence our approach is called
Regularized Weighting; abbreviated as RW). A data point that is inconsistent with all available
models will receive lower weight and even sometimes be ignored. The value of ignoring difﬁcult
points is illustrated by contrast with the common approach  which we consider next.

1

The arguably most widely applied approach for multiple model learning is the minimum loss ap-
proach  also known as Lloyd’s algorithm [1] in clustering  where the goal is to ﬁnd a set of models 
associate each data point to one model (in so called “soft” variations  one or more models)  such
that the sum of losses over data points is minimal. Notice that in this approach  every data point
must be explained by some model. This leaves the minimum loss approach vulnerable to outliers
and corruptions: If one data point goes to inﬁnity  so must at least one model.

Our remedy to this is relaxing the requirement that each data point must be explained.
Indeed 
as we show later  the RW formulation is provably robust in the case of clustering  in the sense of
having non-zero breakdown point [2]. Moreover  we also establish other desirable properties  both
computational and statistical  of the proposed method. Our main contributions are:

1. A new formulation of the sub-task of associating data points to models as a convex op-
timization problem for setting weights. This problem favors broadly based models  and
may ignore difﬁcult data points entirely. We formalize such properties of optimal solutions
through analysis of a strongly dual problem. The remaining results are characteristics of
this approach.

2. Outlier robustness. We show that the breakdown point of the proposed method is bounded
away from zero for the clustering case. The breakdown point is a concept from robust
statistics:
it is the fraction of adversarial outliers that an algorithm can sustain without
having its output arbitrarily changed.

3. Robustness to fat tailed noise. We show  empirically on a synthetic and real world datasets 

that our formulation is more resistant to fat tailed additive noise.

4. Generalization. Ignoring some of the data  in general  may lead to overﬁtting. We show that
when the parameter α (deﬁned in Section 2) is appropriately set  this essentially does not
occur. We prove this through uniform convergence bounds resilient to the lack of efﬁcient
algorithms to ﬁnd near-optimal solutions in multiple model learning.

5. Computational complexity. As almost every method to tackle the multiple model learning
problem  we use alternating optimization of the models and the association (weights)  i.e. 
we iteratively optimize one of them while ﬁxing the other. Our formulation for optimizing
the association requires solving a quadratic problem in kn variables  where k is the number
of models and n is the number of points. Compared to O(kn) steps for some formulations 
this seems expensive. We show how to take advantage of the special problem structure and
repetition in the alternating optimization subproblems to reduce this cost.

1.1 Relation to previous work

Learning multiple models is by no means a new problem. Indeed  special examples of multi-model
learning have been studied  including k-means clustering [3  4  5] (and many other variants thereof) 
Gaussian mixture models (and extensions) [6  7] and subspace segmentation problem [8  9  10]; see
Section 2 for details. Fewer studies attempt to cross problem type boundaries. A general treatment
of the sample complexity of problems that can be interpreted as learning a code book (which en-
compasses some types of multiple model learning) is [11]. Slightly closer to our approach is [12] 
whose formulation generalizes a common approach to different model types and permits for prob-
lem speciﬁc regularization  giving both generalization results and algorithmic iteration complexity
results. A probabilistic and generic algorithmic approach to learning multiple models is Expectation
Maximization [13].

Algorithms for dealing with outliers and multiple models together have been proposed in the context
of clustering [14]. Reference [15] provides an example of an algorithm for outlier resistance in
learning a single subspace  and partly inspires the current work. In contrast  we abstract almost
completely over the class of models  allowing both algorithms and analysis to be easily reused to
address new classes.

2 Formulation

In this section we show how multi-model learning problems can be formed from simple estimation
problem (where we seek to explain weighted data points by a single model)  and imposing a par-

2

ticular joint loss. We contrast the joint loss proposed here to a common one through the weights
assigned by each and their effects on robustness.
We refer throughout to n data points from X by (xi)n
by k models from M denoted (mj)k
distributions (wj)k

i=1 = X ∈ X n  which we seek to explain
j=1 = M ∈ Mk. A data set may be weighted by a set of k

j=1 = W ∈ (△n)k where △n ⊂ Rn is the simplex.

Deﬁnition 1. A base weighted learning problem is a tuple (X   M  ℓ  A)  where ℓ : X × M → R+
is a non-negative convex function  which we call a base loss function and A : △n × X n → M
deﬁnes an efﬁcient algorithm for choosing a model. Given the weight w and data X  A obtains
wiℓ (xi  m) (the weighted empirical loss need not be minimal 

allowing for regularization which we do not discuss further).

low weighted empirical loss Pn

i=1

We will often denote the losses of a model m over X as a vector l = (ℓ(xi  m))n
i=1. In the context
of a set of models M   we similarly associate the loss vector lj and the weight vector wj with the
model mj ; this allows us to use the terse notation w⊤
j

lj for the weighted loss of model j.

Given a base weighted learning problem  one may pose a multi-model learning problem

Example 1. The multi-model learning problem covers many examples  here we list a few:

• In k-means clustering  the goal is to partition the training samples into k subsets  where
each subset of samples is “close” to their mean. In our terminology  a multi-model learning

problem where the base learning problem is (cid:16)Rd  Rd  (x  m) 7→ kx − mk2

ﬁnds the weighted mean of the data. The weights allow us to compute each cluster center
according to the relevant subset of points.

2   A(cid:17) where A

• In subspace clustering  also known as subspace segmentation  the objective is to group
the training samples into subsets  such that each subset can be well approximated by a
low-dimensional afﬁne subspace. This is a multi-model learning problem where the corre-
sponding single-model learning problem is PCA.

• Regression clustering [16] extends the standard linear regression problem in that the train-
ing samples cannot be explained by one linear function. Instead  multiple linear function
are sought  so that the training samples can be split into groups  and each group can be
approximated by one linear function.

• Gaussian Mixture Model considers the case where data points are generated by a mixture
of a ﬁnite number of Gaussian distributions  and seeks to estimate the mean and variance
of each of these distribution  and simultaneously to group the data points according to the
distribution that generates it. This is a multi-model learning problem where the respective
single model learning problem is estimating the mean and variance of a distribution.

The most common way to tackle the multiple model learning problem is the minimum loss approach 
i.e  to minimize the following joint loss

L (X  M ) =

1

n Xx∈X

min
m∈M

ℓ (x  m) .

(2.1)

In terms of weighted base learning problems  each model gives equal weight to all points for which
it is the best (lowest loss) model. For example  when M = X = Rn with ℓ(x  m) = kx − mk2
2
the squared Euclidean distance loss yields k means clustering. In this context  alternating between
choosing for each x its loss minimizing model  and adjusting each model to minimized the squared
Euclidean loss  yields Lloyd’s algorithm (and its generalizations for other problems).

The minimum loss approach requires that every point is assigned to a model  this can potentially
cause problems in the presence of outliers. For example  consider the clustering case where the data
contain a single outlier point xi. Let xi tend to inﬁnity; there will always be some mj that is closest
to xi  and is therefore (at equilibrium) the average of xi and some other data points. Then mj will
tend to inﬁnity also. We call this phenomenon mode I of sensitivity to outliers; it is common also

3

to such simple estimators as the mean. Mode II of sensitivity is more particular: as mj follows xi
to inﬁnity  it stops being the closest to any other points  until the model is associated only to the
outlier and thus matches it perfectly. Thus under Eq. (2.1) outliers tend to take over models. Mode
II of sensitivity is not clustering speciﬁc  and Fig. 2.1 provides an example in multiple regression.
Neither mode is avoided by spreading a point’s weight among models as in mixture models [6].

To overcome both modes of sensitivity  we propose a different joint loss  in which the hard constraint
is only that for each model we produce a distribution over data points. A penalty term discourages
the concentration of a model on few points and thus mode II sensitivity. Deweighting difﬁcult points
helps mitigate mode I. For clustering this robustness is formalized in Theorem 2.

Robust and Lloyds association methods  quadratic regression.

−0.5

−1.0

−1.5

−2.0

−2.5

−3.0

−3.5

Data
Minimum loss 0.20 correct on 34 points
Minimum loss 0.20 correct on 4 points
Robust joint loss 0.20 correct on 29 points
Robust joint loss 0.20 correct on 37 points

0.0

0.2

0.4

0.6

0.8

Figure 2.1: Data is a mixture of two quadratics  with positive fat tailed noise. Under a minimum loss
approach an off-the-chart high-noise point sufﬁces to prevent the top broken line from being close
to many other data points. Our approach is free to better model the bulk of data. We used a robust
(mean absolute deviation) criterion to choose among the results of multiple restarts for each model.

Deﬁnition 2. Let u ∈ △n be the uniform distribution. Given k weight vectors  we denote their aver-
wj   and just v when W is clear from context. The Regularized Weighting

multiple model learning loss is a function Lα : X n × Mk × (△n)k → R deﬁned as

age v (W ) = k−1Pk

j=1

Lα (X  M  W ) = α ku − v (W )k2

2 + k−1

k

Xj=1

l⊤
j

wj

which in particular deﬁnes the weight setting subproblem:

Lα (X  M ) = min

W ∈(△n)k

Lα (X  M  W ) .

(2.2)

(2.3)

As its name suggests  our formulation regularizes distributions of weight over data points; speciﬁ-
cally  wj are controlled by forcing their average v to be close to the uniform distribution u. Our goal
is for each model to represent many data points  so weights should not be concentrated. We avoid
this by penalizing squared Euclidean distance from uniformity  which emphasizes points receiving
weight much higher than the natural n−1  and essentially ignores small variations around n−1. The
effect is later formalized in Lemma 1  but to illustrate we next calculate the penalties for two stylized
cases. This will also produce the ﬁrst of several hints about the appropriate range of values for α.
In the following examples  we will consider a set of γnk−1 data points  recalling that nk−1 is the
natural number of points per model. To avoid letting a few high loss outliers skew our models (mode
I of sensitivity)  we prefer instead to give them zero weight. Take γ ≪ k/2  then the cost of ignoring
some γnk−1 points in all models is at most αn−1 · 2γk−1 ≪ αn−1. In contrast  basing a model

4

Clustering in 1D w. varied class size. α/n = 0.335

model 1 weighs 21 points
model 2 weighs 39 points

1.6

1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0

k
/
n

·

i
 
j

w

:

l

d
e
a
c
s

 
l

e
d
o
m
y
b
d
e
n
g
s
s
a

i

t

i

h
g
e
W

−0.2

−0.6

−0.4

−0.2

0.0

Location

0.2

0.4

0.6

Figure 2.2: For each location (horizontal) of a data point  the vertical locations of corresponding
markers gives the weights assigned by each model. The left cluster is half as populated as the right 
thus must give weights about twice as large. Within each model  weights are afﬁne in the loss
(see Section 2.1)  causing the concave parabolas. The gap allowed between the maximal weights
of different models allows a point from the right cluster to be adopted by the left model  lowering
overall penalty at a cost to weighted losses.

on very few points (mode II of sensitivity) should be avoided. If the jth model is ﬁt to only γnk−1
points for γ ≪ 1  the penalty from those points will be at least (approximately) αn−1 · γ−1k−1.
We can make the ﬁrst situation cheap and the second expensive (per model) in comparison to the
empirical weighted loss term by choosing

αn−1 ≈ k−1

k

Xj=1

w⊤
j

lj.

(2.4)

On the ﬂip side  highly unbalanced classes in the data can be challenging to our approach. Consider
the case where a model has low loss for fewer than n/(2k) points: spreading its weight only over
them can incur very high costs due to the regularization term  which might be lowered by including
some higher-loss points that are indeed better explained by another model (see Figure 2.2 on page
5 for an illustration). This challenge might be solved by explicitly and separately estimating the
relative frequencies of the classes  and penalizing deviations from the estimates rather than from
equal frequencies  as is done in mixture models [6]; this is left for future study.

2.1 Two properties of Regularized Weighting

Two properties of our formulation result from an analysis (in Appendix A for lack of space) of a
dual problem of the weight setting problem (2.3). These provide the basis for later theory by relating
v  losses and α. The ﬁrst illustrates the uniform control of v:
Lemma 1. Let all losses be in [0  B]  then in an optimal solution to (2.3)  we have

kv − uk∞ ≤ B/ (2α) .

This strengthens the conclusion of (2.4): if outliers are present and αn−1 > 2B where B bounds
losses on all points including outliers  weights will be almost uniform (enabling mode I of sensi-

5

tivity). On the positive side  this lemma plays an important role in the generalization and iteration
complexity results presented in the sequel. A more detailed view of vi for individual points is
provided by the second property.

By PC we denote the orthogonal projection mapping into a convex set C.
Lemma 2. For an optimal solution to (2.3)  there exists t ∈ Rk such that:

v = P△n (cid:18)u − min

j

(lj − tj) / (2α)(cid:19)  

where minj should be read as operating element-wise  and in particular wj i > 0 implies that j
minimizes the ith element.

This establishes that average weight (when positive) is afﬁne in the loss; the concave parabolas
visible in Figure 2.2 on page 5 are an example. We also learn the role of α in solutions is determining
the coefﬁcient in the afﬁne relation. Distinct t allow for different densities of points around different
models. One observation from this lemma is that if a particular model j gives weight to some point i 
then every point with lower loss ℓ (xi′   mj) under that model will receive at least that much weight.
This property plays a key role in the proof of robustness to outliers in clustering.

2.2 An alternating optimization algorithm

The RW multiple model learning loss  like other MML losses  is not convex. However the weight
setting problem (2.3) is convex when we ﬁx the models  and an efﬁcient procedure A is assumed
for solving a weighted base learning problem for a model  supporting an alternating optimization
approach  as in Algorithm 1; see Section 5 for further discussion.

Data: X
Result: The model-set M
M ← initialM odels (X);
repeat

M ′ ← M ;
W ← arg minW ′ Lα (X  M  W ′);
mj ← A (wj  X)

(∀j ∈ [k]) ;

until L (X  M ′) − L (X  M ) < ε;

Algorithm 1: Alternating optimization for Regularized Weighting

3 Breakdown point in clustering

Our formulation allows a few difﬁcult outliers to be ignored if the right models are found; does this
happen in practice? Figure 2.1 on page 4 provides a positive example in regression clustering  and
a more substantial empirical evaluation on subspace clustering is in Appendix B. In the particular
case of clustering with the squared Euclidean loss  robustness beneﬁts can be proved.

We use “breakdown point” – the standard robustness measure in the literature of robust statistics [2] 
see also [17  18] and many others – to quantify the robustness property of the proposed formula-
tion. The breakdown point of an estimator is the smallest fraction of bad observations that can
cause the estimator to take arbitrarily aberrant values  i.e.  the smallest fraction of outliers needed to
completely break an estimator.

For the case of clustering with the squared Euclidean distance base loss  the min-loss approach
corresponds to k-means clustering which is not robust in this sense; its breakdown point is 0. The
non robustness of k-means has led to the development of many formulations of robust clustering 
see a review by [14]. In contrast  we show that our joint loss yields an estimator that has a non-zero
breakdown point  and is hence robust.

In general  a squared loss clustering formulation that assigns equal weight to different data points
cannot be robust – as one data point tends to inﬁnity so must at least one model. This applies to
our model if α is allowed to tend to inﬁnity. On the other hand if α is too low  it becomes possible

6

for each model to assign all of its weight to a single point  which may well be an outlier tending
to inﬁnity. Thus  it is well expected that the robustness result below requires α to belong to a data
dependent range.

Theorem 2. Let X = M be a Euclidean space in which we perform clustering with the loss
ℓ (xi  mj) = kmj − xik2 and k centers. Denote by R the radius of any ball containing the inliers 
and η < k−2/22 the proportion of outliers allowed to be outside the ball. Denote also by r a radius
such that there exists M ′ = {m′
k} such that each inlier is within a distance r of some
model m′
j and each mj approximates (i.e.  within a distance r) at least n/(2k) inliers; this always
holds for some r ≤ R.

1  · · ·   m′

For any α ∈ n(cid:2)r2  13R2(cid:3) let (M  W ) be minimizers of Lα (X  M  W ).
kmj − xik2 ≤ 6R for every model mj and inlier xi.

Then we have

Theorem 2 shows that when the number of outliers is not too high  then the learned model  regardless
of the magnitude of the outliers  is close to the inliers and hence cannot be arbitrarily bad.
In
particular  the theorem implies a non-zero breakdown point for any α > nr2; taking too high an
α merely forces a larger but still ﬁnite R. If the inliers are amenable to balanced clustering so that
r ≪ R  the regime of non-zero breakdown is extended to smaller α.

The proof follows three steps. First  due to the regularization term  for any model  the total weight
on the few outliers is at most 1/3. Second  an optimal model must thus be at least twice as close to
the weighted average of its inlier as it is to the weighted average of its outliers. This step depends
critically on squared Euclidean loss being used. Lastly  this gap in distances cannot be large in
absolute terms  due to Lemma 2; an outlier that is much farther from the model than the inliers must
receive weight zero. For the proof see Appendix C of the supplementary material.

4 Regularized Weighting formulation sample complexity

An important consideration in learning algorithms is controlling overﬁtting  in which a model is
found that is appropriate for some data  rather than for the source that generates the data. The
current formulation seems to be particularly vulnerable since it allows data to be ignored  in contrast
to most generalization bounds that assume equal weight is given to all data.

Our loss Lα(X  M ) differs from common losses in allowing data points to be differently weighted.
Thus  to obtain the sample complexity of our formulation we need to bound the difference that a
single sample can make to the loss. For a common empirical average loss this is bounded by Bn−1
where B is the maximal value of the non-negative loss on a single data point  and in our case by
B kvk∞  because if X  X ′ differ only on the ith element  then:

|Lα (X ′  M  W ) − Lα (X  M  W )| = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

k

k−1

Xj=1(cid:0)wj i(cid:0)lj i − l′

j i(cid:1)(cid:1)

≤ Bk−1

k

Xj=1

wj i ≤ Bvi.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Whenever W is optimal with respect to either X or X ′  Lemma 1 provides the necessary bound
on kvk∞. Along with covering numbers as deﬁned next and standard arguments (found in the
supplementary material)  this bound on differences provides us with the desired generalization result.
Deﬁnition 3 (Covering numbers for multiple models). We shall endow Mk with the metric

d∞ (M  M ′) = max

j∈[k](cid:13)(cid:13)ℓ (·  mj) − ℓ(cid:0)·  m′

j(cid:1)(cid:13)(cid:13)∞

and deﬁne its covering number Nε(cid:0)Mk(cid:1) as the minimal cardinality of a set Mk
SM ∈Mk

B(M  ε).

ε

ε such that Mk ⊆

The bound depends on an upper bound on base losses denoted B; this should be viewed as ﬁxing
a scale for the losses and is standard where losses are not naturally bounded (e.g.  classical bounds
on SVM kernel regression [19] use bounded kernels). Thus  we have the following generalization
result  whose proof can be found in Appendix D of the supplementary material.

7

Theorem 3. Let the base losses be bounded in the interval [0  B]  let Mk have covering num-

bers Nε(cid:0)Mk(cid:1) ≤ (C/ε)dk and let γ = nB/ (2α). Then we have with probability at least
1 − expndk log(cid:0) 2C

τ (cid:1) − 2nτ 2

B 2(1+γ)2o:

∀M ∈ Mk

|Lα (X  M ) − EX ′∼Dn Lα (X ′  M )| ≤ 3τ.

5 The weight assignment optimization step

As is typical in multi-model learning  simultaneously optimizing the model and the association of
the data (in our formulation  the weight) is computationally hard [20]  thus Algorithm 1 alternates
between optimizing the weight with the model ﬁxed  and optimizing the model with the weights
ﬁxed. Thus we show how to efﬁciently solve a sequence of weight setting problems  minimizing
Lα(X  Mi  W ) over W   where Mi typically converge.

We propose to solve each instance of weight setting using gradient methods  and in particular FISTA
[21]. This has two advantages compared to Interior Point methods: First  the use of memory for
gradient methods depends only linearly with respect to the dimension  which is O(kn) in problem
(2.3)  allowing scaling to large data sets. Second  gradient methods have “warm start” properties:
the number of iterations required is proportional to the distance between the initial and optimal
solutions  which is useful both due to bounds on kv − uk∞ and when Mi converge.
Theorem 4. Given data and models (X  M ) there exists an algorithm that ﬁnds a weight matrix W

such that Lα(X  M  W ) − Lα(X  M ) ≤ ε using O(cid:16)pkα/ε(cid:17) iterations  each costing O(kn) time
and memory. If α ≥ Bn/4 then O(cid:16)kpαn−1/ε(cid:17) iterations sufﬁce.

The ﬁrst bound might suggest that typical settings of α ∝ n requires iterations to increase with the
number of points n; the second bounds shows this is not always necessary.

This result can be realized by applying the algorithm FISTA  with a starting point wj = u  with
2αk−2 as a bound on the Lipschitz constant for the gradient. For the ﬁrst bound we estimate the
distance from u by the radius of the product of k simplices; for the second we use Lemma 1 in Ap-
pendix E.

6 Conclusion

In this paper  we proposed and analyzed  from a general perspective  a new formulation for learning
multiple models that explain well much of the data. This is based on associating to each model a
regularized weight distribution over the data it explains well. A main advantage of the new formula-
tion is its robustness to fat tailed noise and outliers: we demonstrated this empirically for regression
clustering and subspace clustering tasks  and proved that for the important case of clustering  the
proposed method has a non-trivial breakdown point  which is in sharp contrast to standard meth-
ods such as k-means. We further provided generalization bounds and explained an optimization
procedure to solve the formulation in scale.

Our main motivation comes from the fast growing attention to analyzing data using multiple models 
under the names of k-means clustering  subspace segmentation  and Gaussian mixture models  to
list a few. While all these learning schemes share common properties  they are largely studied
separately  partly because these problems come from different sub-ﬁelds of machine learning. We
believe general methods with desirable properties such as generalization and robustness will supply
ready tools for new applications using other model types.

Acknowledgments

H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier Two
grant R-265-000-443-112 and NUS startup grant R-265-000-384-133. This research was funded (in
part) by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).

8

References

[1] S. Lloyd. Least squares quantization in PCM.

Information Theory  IEEE Transactions on 

28(2):129–137  1982.

[2] P. J. Huber. Robust Statistics. John Wiley & Sons  New York  1981.

[3] J.A. Hartigan and M.A. Wong. Algorithm AS 136: A k-means clustering algorithm. Journal

of the Royal Statistical Society. Series C (Applied Statistics)  28(1):100–108  1979.

[4] R. Ostrovsky  Y. Rabani  L.J. Schulman  and C. Swamy. The effectiveness of Lloyd-type
methods for the k-means problem. In Foundations of Computer Science  2006. FOCS’06. 47th
Annual IEEE Symposium on  pages 165–176. IEEE  2006.

[5] P. Hansen  E. Ngai  B.K. Cheung  and N. Mladenovic. Analysis of global k-means  an incre-
mental heuristic for minimum sum-of-squares clustering. Journal of classiﬁcation  22(2):287–
310  2005.

[6] G. J. McLachlan and K. E. Basford. Mixture Models: Inference and Applications to Clustering.

Marcel Dekker  New York  1998.

[7] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In FOCS
2010: Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science 
pages 103–112. IEEE Computer Society  2010.

[8] G. Chen and M. Maggioni. Multiscale geometric and spectral analysis of plane arrangements.
In Computer Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on  pages 2825–
2832. IEEE  2011.

[9] Yaoliang Yu and Dale Schuurmans. Rank/norm regularization with closed-form solutions:
Application to subspace clustering. In Fabio Gagliardi Cozman and Avi Pfeffer  editors  UAI 
pages 778–785. AUAI Press  2011.

[10] M. Soltanolkotabi and E.J. Cand`es. A geometric analysis of subspace clustering with outliers.

Arxiv preprint arXiv:1112.4258  2011.

[11] A. Maurer and M. Pontil. k-dimensional coding schemes in hilbert spaces. Information Theory 

IEEE Transactions on  56(11):5839–5846  2010.

[12] A.J. Smola  S. Mika  B. Sch¨olkopf  and R.C. Williamson. Regularized principal manifolds.

The Journal of Machine Learning Research  1:179–209  2001.

[13] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the EM

algorithm. Journal of the Royal Statistical Society. Series B  39(1):1–38  1977.

[14] R.N. Dav´e and R. Krishnapuram. Robust clustering methods: a uniﬁed view. Fuzzy Systems 

IEEE Transactions on  5(2):270–293  1997.

[15] Huan Xu  Constantine Caramanis  and Shie Mannor. Outlier-robust PCA: The high-

dimensional case. IEEE transactions on information theory  59(1):546–572  2013.

[16] B. Zhang. Regression clustering. In Data Mining  2003. ICDM 2003. Third IEEE International

Conference on  pages 451–458. IEEE  2003.

[17] P. J. Rousseeuw and A. M. Leroy. Robust Regression and Outlier Detection. John Wiley &

Sons  New York  1987.

[18] R. A. Maronna  R. D. Martin  and V. J. Yohai. Robust Statistics: Theory and Methods. John

Wiley & Sons  New York  2006.

[19] Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. The Journal of Machine

Learning Research  2:499–526  2002.

[20] M. Mahajan  P. Nimbhorkar  and K. Varadarajan. The planar k-means problem is np-hard.

WALCOM: Algorithms and Computation  pages 274–285  2009.

[21] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences  2(1):183–202  2009.

[22] Roberto Tron and Ren´e Vidal. A benchmark for the comparison of 3-d motion segmentation

algorithms. In CVPR. IEEE Computer Society  2007.

[23] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient projections onto the l1-
ball for learning in high dimensions. In Proceedings of the 25th international conference on
Machine learning  pages 272–279  2008.

9

,Daniel Vainsencher
Shie Mannor
Huan Xu