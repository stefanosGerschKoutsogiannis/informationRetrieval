2017,Parametric Simplex Method for Sparse Learning,High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper  we investiage a broad class of sparse learning approaches formulated as linear programs parametrized by a {\em regularization factor}  and solve them by the parametric simplex method (PSM). PSM offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations  and the solution sparsity significantly reduces the computational cost per iteration. Particularly  we demonstrate the superiority of PSM over various sparse learning approaches  including Dantzig selector for sparse linear regression  sparse support vector machine for sparse linear classification  and sparse differential network estimation. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.,Parametric Simplex Method for Sparse Learning

Haotian Pang‡ Robert Vanderbei‡ Han Liu?‡

Tuo Zhao⇧

‡Princeton University ?Tencent AI Lab ‡Northwestern University ⇧Georgia Tech⇤

Abstract

High dimensional sparse learning has imposed a great computational challenge to
large scale data analysis. In this paper  we are interested in a broad class of sparse
learning approaches formulated as linear programs parametrized by a regularization
factor  and solve them by the parametric simplex method (PSM). Our parametric
simplex method offers signiﬁcant advantages over other competing methods: (1)
PSM naturally obtains the complete solution path for all values of the regularization
parameter; (2) PSM provides a high precision dual certiﬁcate stopping criterion;
(3) PSM yields sparse solutions through very few iterations  and the solution
sparsity signiﬁcantly reduces the computational cost per iteration. Particularly 
we demonstrate the superiority of PSM over various sparse learning approaches 
including Dantzig selector for sparse linear regression  LAD-Lasso for sparse robust
linear regression  CLIME for sparse precision matrix estimation  sparse differential
network estimation  and sparse Linear Programming Discriminant (LPD) analysis.
We then provide sufﬁcient conditions under which PSM always outputs sparse
solutions such that its computational performance can be signiﬁcantly boosted.
Thorough numerical experiments are provided to demonstrate the outstanding
performance of the PSM method.

Introduction

1
A broad class of sparse learning approaches can be formulated as high dimensional optimization
problems. A well known example is Dantzig Selector  which minimizes a sparsity-inducing `1 norm
with an `1 norm constraint. Speciﬁcally  let X 2 Rn⇥d be a design matrix  y 2 Rn be a response
vector  and ✓ 2 Rd be the model parameter. Dantzig Selector can be formulated as the solution to the
following convex program 
(1.1)

k✓k1

s.t. kX>(y  X✓ )k1  .

where k·k 1 and k·k 1 denote the `1 and `1 norms respectively  and > 0 is a regularization factor.
Candes and Tao (2007) suggest to rewrite (1.1) as a linear program solved by linear program solvers.
Dantzig Selector motivates many other sparse learning approaches  which also apply a regularization
factor to tune the desired solution. Many of them can be written as a linear program in the following
generic form with either equality constraints:

b✓ = argmin

✓

or inequality constraints:

max

x

(c + ¯c)>x s.t. Ax = b + ¯b  x  0 

max

x

(c + ¯c)>x s.t. Ax  b + ¯b  x  0.

(1.2)

(1.3)

Existing literature usually suggests the popular interior point method (IPM) to solve (1.2) and (1.3).
The interior point method is famous for solving linear programs in polynomial time. Speciﬁcally 
the interior point method uses the log barrier to handle the constraints  and rewrites (1.2) or (1.3)

⇤Correspondence to Tuo Zhao: tuo.zhao@isye.gatech.edu.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

as a unconstrained program  which is further solved by the Newton’s method. Since the log barrier
requires the Newton’s method to only iterate within the interior of the feasible region  IPM cannot
yield exact sparse iterates  and cannot take advantage of sparsity to boost the computation.
An alternative approach is the simplex method. From a geometric perspective  the classical simplex
method iterates over the vertices of a polytope. Algebraically  the algorithm involves moving from
one partition of the basic and nonbasic variables to another. Each partition deviates from the previous
in that one basic variable gets swapped with one nonbasic variable in a process called pivoting.
Different variants of the simplex method are deﬁned by different rules of pivoting. The simplex
method has been shown to work well in practice  even though its worst-case iteration complexity has
been shown to scale exponentially with the problem scale in existing literature.
More recently  some researchers propose to use alternating direction methods of multipliers (ADMM)
to directly solve (1.1) without reparametrization as a linear program. Although these methods enjoy
O(1/T ) convergence rates based on variational inequality criteria  where T is the number of iterations.
ADMM can be viewed as an exterior point method  and always gives infeasible solutions within
ﬁnite number of iterations. We often observe that after ADMM takes a large number of iterations 
the solutions still suffer from signiﬁcant feasibility violation. These methods work well only for
moderate scale problems (e.g.  d < 1000). For larger d  ADMM becomes less competitive.
These methods  though popular  are usually designed for solving (1.2) and (1.3) for one single
regularization factor. This is not satisfactory  since an appropriate choice of  is usually unknown.
Thus  one usually expects an algorithm to obtain multiple solutions tuned over a reasonable range of
values for . For each value of   we need to solve a linear program from scratch  and it is therefore
often very inefﬁcient for high dimensional problems.
To overcome the above drawbacks  we propose to solve both (1.2) and (1.3) by a variant of the
parametric simplex method (PSM) in a principled manner (Murty  1983; Vanderbei  1995). Speciﬁ-
cally  the parametric simplex method parametrizes (1.2) and (1.3) using the unknown regularization
factor as a “parameter”. This eventually yields a piecewise linear solution path for a sequence of
regularization factors. Such a varying parameter scheme is also called homotopy optimization in
existing literature. PSM relies some special rules to iteratively choose the pair of variables to swap 
which algebraically calculates the solution path during each pivoting. PSM terminates at a value of
parameter  where we have successfully solved the full solution path to the original problem. Although
in the worst-case scenario  PSM can take an exponential number of pivots to ﬁnd an optimal solution
path. Our empirical results suggest that the number of iterations is roughly linear in the number of
nonzero variables for large regularization factors with sparse optima. This means that the desired
sparse solutions can often be found using very few pivots.
Several optimization methods for solving (1.1) are closely related to PSM. However  there is a lack
of generic design in these methods. Their methods  for example  the simplex method proposed in
Yao and Lee (2014) can be viewed as a special example of our proposed PSM  where the perturbation
is only considered on the right-hand-side of the inequalities in the constraints. DASSO algorithm
computes the entire coefﬁcient path of Dantzig selector by a simplex-like algorithm. Zhu et al. (2004)
propose a similar algorithm which takes advantage of the piece-wise linearity of the problem and
computes the whole solution path on `1-SVM. These methods can be considered as similar algorithms
derived from PSM but only applied to special cases  where the entire solution path is computed but
an accurate dual certiﬁcate stopping criterion is not provided.
Notations: We denote all zero and all one vectors by 1 and 0 respectively. Given a vector a =

(a1  ...  ad)> 2 Rd  we deﬁne the number of nonzero entries kak0 = Pj 1(aj 6= 0)  kak1 =
Pj |aj|  kak2
j  and kak1 = maxj |aj|. When comparing vectors  “” and “” mean
component-wise comparison. Given a matrix A 2 Rd⇥d with entries ajk  we use |||A||| to denote
entry-wise norms and kAk to denote matrix norms. Accordingly |||A|||0 =Pj k 1(ajk 6= 0)  |||A|||1 =
Pj k |ajk|  |||A|||1 = maxj k |ajk|  kAk1 = maxkPj |ajk|  kAk1 = maxjPk |ajk|  kAk2 =
maxkak21 kAak2  and kAk2
jk. We denote A\i\j as the submatrix of A with i-th row
and j-th column removed. We denote Ai\j as the i-th row of A with its j-th entry removed and
A\ij as the j-th column of A with its i-th entry removed. For any subset G of {1  2  . . .   d}  we let
AG denote the submatrix of A 2 Rp⇥d consisting of the corresponding columns of A. The notation
A  0 means all of A’s entries are nonnegative. Similarly  for a vector a 2 Rd  we let aG denote the
subvector of a associated with the indices in G. Finally  Id denotes the d-dimensional identity matrix

2 = Pj a2

F =Pj k a2

2

and ei denotes vector that has a one in its i-th entry and zero elsewhere. In a large matrix  we leave a
submatrix blank when all of its entries are zeros.
2 Background
Many sparse learning approaches are formulated as convex programs in a generic form:

✓ k✓k1
min

✓ L(✓) + k✓k1 
min

s.t. krL(✓)k1   

(2.1)
where L(✓) is a convex loss function  and > 0 is a regularization factor controlling bias and
variance. Moreover  if L(✓) is smooth  we can also consider an alternative formulation:
(2.2)
where rL(✓) is the gradient of L(✓)  and > 0 is a regularization factor. As will be shown later 
both (2.2) and (2.1) are naturally suited for our algorithm  when L(✓) is piecewise linear or quadratic
respectively. Our algorithm yields a piecewise-linear solution path as a function of  by varying 
from large to small.
Before we proceed with our proposed algorithm  we ﬁrst introduce the sparse learning problems of
our interests  including sparse linear regression  sparse linear classiﬁcation  and undirected graph
estimation. Due to space limit  we only present three examples  and defer the others to the appendix.
Dantzig Selector: The ﬁrst problem is sparse linear regression. Let y 2 Rn be a response vector
and X 2 Rn⇥d be the design matrix. We consider a linear model y = X✓⇤ + ✏  where ✓⇤ 2 Rd is the
unknown regression coefﬁcient vector  and ✏ is the observational noise vector. Here we are interested
in a high dimensional regime: d is much larger than n  i.e.  d  n  and many entries in ✓⇤ are zero 
i.e.  k✓⇤k0 = s⇤ ⌧ n. To get a sparse estimator of ✓0  machine learning researchers and statisticians
have proposed numerous approaches including Lasso (Tibshirani  1996)  Dantzig Selector (Candes
and Tao  2007) and LAD-Lasso (Wang et al.  2007).
The Dantzig selector is formulated as the solution to the following convex program:

subject to kX>(y  X✓ )k1  .
(2.3)
j = ✓j · 1(✓j < 0)  we rewrite (2.3) as a
✓◆ ✓1 + X>y

(2.4)

1  X>y◆  ✓ + ✓   0.
x =✓✓+
✓◆ .

¯b = 1 

¯c = 0 

j = ✓j · 1(✓j > 0) and ✓+
X>X X>X ◆✓✓+
s.t.✓ X>X X>X
b =✓ X>y
X>y◆  

c = 1 

By complementary slackness  we can guarantee that the optimal ✓+
complementary to each other. Note that (2.4) ﬁts into our parametric linear program as (1.3) with

j ’s and ✓j ’s are nonnegative and

✓ k✓k1
min
By setting ✓ = ✓+  ✓ with ✓+
linear program:

1>(✓+ + ✓)

min
✓+ ✓

A =✓ X>X X>X
X>X X>X ◆  

Sparse Support Vector Machine: The second problem is Sparse SVM (Support Vector Machine) 
which is a model-free discriminative modeling approach (Zhu et al.  2004). Given n independent
and identically distributed samples (x1  y1)  ...  (xn  yn)  where xi 2 Rd is the feature vector and
yi 2{ 1 1} is the binary label. Similar to sparse linear regression  we are interested in the high
dimensional regime. To obtain a sparse SVM classiﬁer  we solve the following convex program:

nXi=1

min
✓0 ✓

[1  yi(✓0 + ✓>xi)]+ s.t. k✓k1   

(2.5)

i  ti . Notice [1  yi(✓0 + ✓>xi)]+ can be represented by t+

where ✓0 2 R and ✓ 2 Rd. Given a new sample z 2 Rd  Sparse SVM classiﬁer predicts its label
by sign(✓0 + ✓>z). Let ti = 1  yi(✓0 + ✓>xi) for i = 1  ...  n. Then ti can be expressed as
ti = t+
i . We split ✓ and ✓0 into
0 + ✓0 and add slack variable w
positive and negative parts as well: ✓ = ✓+  ✓ and ✓0 = ✓+
to the constraint so that the constraint becomes equality: ✓+ + ✓ + w = 1  w  0. Now we
cast the problem into the equality parametric simplex form (1.2). We identify each component
✓0 w> 2 R(n+1)⇥(2n+3d+2)  x 
of (1.2) as the following: x = t+ t ✓+ ✓ ✓+
c = 1> 0> 0> 0> 0
1>◆ 2
1> 0> 2 Rn+1  ¯b =0> 1> 2 Rn+1  and A =✓In In Z Z y y
R(n+1)⇥(2n+3d+2)  where Z = (y1x1  . . .   ynxn)> 2 Rn⇥d.

0 0>> 2 R2n+3d+2 

¯c = 0 2 R2n+3d+2 
1> 1>

b =

0 

0

3

Differential Graph Estimation: The third problem is the differential graph estimation  which
aims to identify the difference between two undirected graphs (Zhao et al.  2013; Danaher et al. 
2013). The related applications in biological and medical research can be found in existing literature
(Hudson et al.  2009; Bandyopadhyaya et al.  2010; Ideker and Krogan  2012). Speciﬁcally  given n1
i.i.d. samples x1  ...  xn from Nd(µ0
X) and n2 i.i.d. samples y1  ...  yn from Nd(µ0
Y ) We
are interested in estimating the difference of the precision matrices of two distributions:

X  ⌃0

Y   ⌃0

We deﬁne the empirical covariance matrices as: SX = 1
1
propose to estimate 0 by solving the following problem:

j=1(yj  ¯y)(yj  ¯x)>  where ¯x = 1

n2Pn2

0 = (⌃ 0

Y )1.

X)1  (⌃0
n1Pn1
j=1(xj  ¯x)(xj  ¯x)> andSY =
n2Pn
n1Pn
j=1 yj. Then Zhao et al. (2013)
s.t. |||SXSY  SX + SY |||1   

j=1 xj and ¯y = 1

(2.6)

 ||||||1
min

where SX and SY are the empirical covariance matrices. As can be seen  (2.6) is essentially a special
example of a more general parametric linear program as follows 

D |||D|||1
min

s.t. |||XDZ  Y |||1   

(2.7)

where D 2 Rd1⇥d2  X 2 Rm1⇥d1  Z 2 Rd2⇥m2 and Y 2 Rm1⇥m2 are given data matrices.
Instead of directly solving (2.7)  we consider a reparametrization by introducing an axillary variable
C = XD. Similar to CLIME  we decompose D = D+ + D  and eventually rewrite (2.7) as
1>(D+ + D)1 s.t. |||CZ  Y |||1    X(D+  D) = C  D+  D  0 

min
D+ D

(2.8)

1A

Im1m2
z11Im1

Let vec(D+)  vec(D)  vec(C) and vec(Y ) be the vectors obtained by stacking the columns of
matrices D+  D C and Y   respectively. We write (2.8) as a parametric linear program 

Im1m2

X 0 X 0 Im1d2
Z0
Z0

A =0@
1CA 2 Rm1d2⇥d1d2  Z0 = 0B@

X

...

...

X

...

zd21Im1

vec(D)

z1m2Im1

x =⇥vec(D+)

Rm1m2⇥m1d2  where zij denotes the (i  j) entry of matrix Z;

···
...
···  zd2m2Im1

vec(C) w⇤> 2 R2d1d2+m1d2+2m1m2 

1CA 2
with X 0 = 0B@
where w 2 R2m1m2 is nonnegative slack variable vector used to make the inequality become
an equality. Moreover  we also have b = ⇥0> vec(Y ) vec(Y )⇤> 2 Rm1d2+2m1m2  where
the ﬁrst m1d2 components of vector b are 0 and the rest components are from matrix Y ; ¯b =
0> 1> 1>> 2 Rm1d2+2m1m2  where the ﬁrst m1d2 components of vector ¯b are 0 and the rest
2m1m2 components are 1; c =1> 1> 0> 0>> 2 R2d1d2+m1d2+2m1m2  where the ﬁrst
2d1d2 components of vector c are 1 and the rest m1d2 + 2m1m2 components are 0.
3 Homotopy Parametric Simplex Method
We ﬁrst brieﬂy review the primal simplex method for linear programming  and then derive the
proposed algorithm.
Preliminaries: We consider a standard linear program as follows 

x

max

c>x s.t. Ax = b 

x  0 x 2 Rn 

(3.1)
where A 2 Rm⇥n  b 2 Rm and c 2 Rn are given. Without loss of generality  we assume that m  n
and matrix A has full row rank m. Throughout our analysis  we assume that an optimal solution
exists (it needs not be unique). The primal simplex method starts from a basic feasible solution (to
be deﬁned shortly—but geometrically can be thought of as any vertex of the feasible polytope) and
proceeds step-by-step (vertex-by-vertex) to the optimal solution. Various techniques exist to ﬁnd the
ﬁrst feasible solution  which is often referred to the Phase I method. See Vanderbei (1995); Murty
(1983); Dantzig (1951).

4

Algebraically  a basic solution corresponds to a partition of the indices {1  . . .   n} into m basic
indices denoted B and n  m non-basic indices denoted N . Note that not all partitions are allowed—
the submatrix of A consisting of the columns of A associated with the basic indices  denoted AB 
must be invertible. The submatrix of A corresponding to the nonbasic indices is denoted AN .
Suppressing the fact that the columns have been rearranged  we can write A = [AN   AB]. If
we rearrange the rows of x and c in the same way  we can introduce a corresponding partition of

these vectors: x =xNxB   c =cNcB . From the commutative property of addition  we rewrite the

constraint as AN xN + ABxB = b. Since the matrix AB is assumed to be invertible  we can express
xB in terms of xN as follows:
(3.2)
B AN xN  
where we have written x⇤
B b. This rearrangement of the equality constraints
B
is called a dictionary because the basic variables are deﬁned as functions of the nonbasic variables.
Denoting the objective c>x by ⇣  then we also can write:

B  A1
as an abbreviation for A1

xB = x⇤

(3.3)

N )>xN  

⇣ = c>x = c>
B = A1
B b and z⇤

B xB + c>
N = (A1

N xN = ⇣⇤  (z⇤
B AN )>cB  cN .

B A1

B b  x⇤

xB = x⇤
B

where ⇣⇤ = c>
We call equations (3.2) and (3.3) the primal dictionary associated with the current basis B. Cor-
responding to each dictionary  there is a basic solution (also called a dictionary solution) ob-
tained by setting the nonbasic variables to zero and reading off values of the basic variables:
. This particular “solution” satisﬁes the equality constraints of the problem by
xN = 0 
construction. To be a feasible solution  one only needs to check that the values of the basic variables
are nonnegative. Therefore  we say that a basic solution is a basic feasible solution if x⇤
The dual of (3.1) is given by
y b>y
max

In this case  we separate variable z into basic and nonbasic parts as before: [z] = zNzB. The

z  0 z 2 Rn  y 2 Rm.

s.t. A>y  z = c 

B  0.

(3.4)

corresponding dual dictionary is given by:
N + (A1

zN = z⇤

(3.5)
N =

B AN )>zB  ⇠ = ⇣⇤  (x⇤
B A1

B)>zB 
B b  x⇤

B = A1

B b and z⇤

B AN )>cB  cN .

where ⇠ denotes the objective function in the (3.4)  ⇣⇤ = c>
(A1
For each dictionary  we set xN and zB to 0 (complementarity) and read off the solutions to xB and
zN according to (3.2) and (3.5). Next  we remove one basic index and replacing it with a nonbasic
index  and then get an updated dictionary. The simplex method produces a sequence of steps to
adjacent bases such that the value of the objective function is always increasing at each step. Primal
feasibility requires that xB  0  so while we update the dictionary  primal feasibility must always be
satisﬁed. This process will stop when zN  0 (dual feasibility)  since it satisﬁes primal feasibility 
dual feasibility and complementarity (i.e.  the optimality condition).
Parametric Simplex Method: We derive the parametric simplex method used to ﬁnd the full
solution path while solving the parametric linear programming problem only once. A few variants of
the simplex method are proposed with different rules for choosing the pair of variables to swap at each
iteration. Here we describe the rule used by the parametric simplex method: we add some positive
perturbations (¯b and ¯c) times a positive parameter  to both objective function and the right hand side
of the primal problem. The purpose of doing this is to guarantee the primal and dual feasibility when
 is large. Since the problem is already primal feasible and dual feasible  there is no phase I stage
required for the parametric simplex method. Furthermore  if the i-th entry of b or the j-th entry of c
has already satisﬁed the feasibility condition (bi  0 or cj  0)  then the corresponding perturbation
¯bi or ¯cj to that entry is allowed to be 0. With these perturbations  (3.1) becomes:
(3.6)

(c + ¯c)>x s.t. Ax = b + ¯b 

max

x

x  0 x 2 Rn.

We separate the perturbation vectors into basic and nonbasic parts as well and write down the the
dictionary with perturbations corresponding to (3.2) (3.3)  and (3.5) as:

xB = (x⇤

B + ¯xB)  A1

B AN xN  ⇣ = ⇣⇤  (z⇤

N + ¯zN )>xN  

(3.7)

5

zN = (z⇤
B b  z⇤

N + ¯zN ) + (A1
N = (A1

B AN )>zB  ⇠ = ⇣⇤  (x⇤

B + ¯xB)>zB 

(3.8)

B = A1

¯b and ¯zN = (A1
B AN )>cB  cN   ¯xB = A1
where x⇤
B
When  is large  the dictionary will be both primal and dual feasible (x⇤
N +¯zN 
0). The corresponding primal solution is simple: xB = x⇤
B + ¯xB and xN = 0. This solution is
valid until  hits a lower bound which breaks the feasibility. The smallest value of  without break
any feasibility is given by

B +¯xB  0 and z⇤

B AN )>¯cB  ¯cN .

⇤ = min{ : z⇤

N + ¯zN  0 and x⇤

In other words  the dictionary and its corresponding solution xB = x⇤
for the value of  2 [⇤  max]  where

B + ¯xB  0}.

(3.9)
B + ¯xB and xN = 0 is optimal

⇤ = max ✓maxj2N   ¯zNj >0 
max = min ✓minj2N   ¯zNj <0 

z⇤
Nj
¯zNj

z⇤
Nj
¯zNj

  maxi2B ¯xBi >0 

  mini2B ¯xBi <0 

x⇤
Bi

¯xBi◆  
¯xBi◆ .

x⇤
Bi

(3.10)

(3.11)

x⇤j = t 

¯x⇤j = ¯t 

B  txB 

z⇤i = s 
z⇤
N z⇤

¯xB ¯xB  ¯txB 

Note that although initially the perturbations are nonnegative  as the dictionary gets updated  the
perturbation does not necessarily maintain nonnegativity. For each dictionary  there is a corresponding
interval of  given by (3.10) and (3.11). We have characterized the optimal solution for this interval 
and these together give us the solution path of the original parametric linear programming problem.
Next  we show how the dictionary gets updated as the leaving variable and entering variable swap.
We expect that after swapping the entering variable j and leaving variable i  the new solution in the
dictionary (3.7) and (3.8) would slightly change to:
x⇤
B x⇤
¯zN ¯zN  ¯szN  

¯z⇤i = ¯s 
N  szN  

N + ¯zN = 0 or an i 2B for which x⇤

where t and ¯t are the primal step length for the primal basic variables and perturbations  s and ¯s are
the dual step length for the dual nonbasic variables and perturbations  xB and zN are the primal
and dual step directions  respectively. We explain how to ﬁnd these values in details now.
There is either a j 2N for which z⇤
B + ¯xB = 0 in (3.9). If
it corresponds to a nonbasic index j  then we do one step of the primal simplex. In this case  we
declare j as the entering variable  then we need to ﬁnd the primal step direction xB. After the
entering variable j has been selected  xN changes from 0 to tej  where t is the primal step length.
Then according to (3.7)  we have that xB = (x⇤
B AN tej. The step direction xB
is given by xB = A1
B AN ej. We next select the leaving variable. In order to maintain primal
feasibility  we need to keep xB  0  therefore  the leaving variable i is selected such that i 2B
achieves the maximal value of xi
. It only remains to show how zN changes. Since i is the
leaving variable  according to (3.8)  we have zN = (A1
B AN )>ei. After we know the entering
variables  the primal and dual step directions  the primal and dual step lengths can be found as
t = x⇤i
xi
If  on the other hand  the constraint in (3.9) corresponds to a basic index i  we declare i as the leaving
variable  then similar calculation can be made based on the dual simplex method (apply the primal
simplex method to the dual problem). Since it is very similar to the primal simplex method  we omit
the detailed description.
The algorithm will terminate whenever ⇤  0. The corresponding solution is optimal since our
dictionary always satisﬁes primal feasibility  dual feasibility and complementary slackness condition.
The only concern during the entire process of the parametric simplex method is that  does not equal
to zero  so as long as  can be set to be zero  we have the optimal solution to the original problem.
We summarize the parametric simplex method in Algorithm 1:
The following theorem shows that the updated basic and nonbasic partition gives the optimal solution.
Theorem 3.1. For a given dictionary with parameter  in the form of (3.7) and (3.8)  let B be a basic
index set and N be an nonbasic index set. Assume this dictionary is optimal for  2 [⇤  max] 
where ⇤ and max are given by (3.10) and (3.11)  respectively. The updated dictionary with basic
index set B⇤ and nonbasic index set N ⇤ is still optimal at  = ⇤.

B + ¯xB)  A1

s = z⇤j
zj

¯s = ¯zj
zj

¯t = ¯xi
xi

x⇤i +⇤ ¯xi

.

 

 

 

6

Write down the dictionary as in (3.7) and (3.8);
Find ⇤ given by (3.10);
while ⇤ > 0 do

B AN ej;

if the constraint in (3.10) corresponds to an index j 2N then

else if the constraint in (3.10) corresponds to an index i 2B then

Declare xj as the entering variable;
Compute primal step direction. xB = A1
Select leaving variable. Need to ﬁnd i 2B that achieves the maximal value of xi
x⇤i +⇤ ¯xi
Compute dual step direction. It is given by zN = (A1
Declare zi as the leaving variable;
Compute dual step direction. zN = (A1
Select entering variable. Need to ﬁnd j 2N that achieves the maximal value of zj
z⇤j +⇤ ¯zj
Compute primal step direction. It is given by xB = A1

Compute the dual and primal step lengths for both variables and perturbations:

B AN )>ei;

B AN )>ei;

B AN ej;

;

;

t =

x⇤i
xi

 

¯t =

¯xi
xi

 

s =

z⇤j
zj

 

¯s =

¯zj
zj

.

Update the primal and dual solutions:

x⇤j = t 

z⇤i = s 
N z⇤
z⇤
Update the basic and nonbasic index sets B := B \ {i}\{ j} and N := N \ {j}\{ i}. Write
down the new dictionary and compute ⇤ given by (3.10);

¯xj = ¯t 
¯xB ¯xB  ¯txB

¯zi = ¯s 
N  szN  

¯zN ¯zN  ¯szN .

B  txB 

x⇤
B x⇤

end
Set the nonbasic variables as 0s and read the values of the basic variables.
Algorithm 1: The parametric simplex method

During each iteration  there is an optimal solution corresponding to  2 [⇤  max]. Notice each of
these ’s range is determined by a partition between basic and nonbasic variables  and the number
of the partition into basic and nonbasic variables is ﬁnite. Thus after ﬁnite steps  we must ﬁnd the
optimal solution corresponding to all  values.
Theory: We present our theoretical analysis on solving Dantzig selector using PSM. Speciﬁcally 
given X 2 Rn⇥d  y 2 Rn  we consider a linear model y = X✓⇤ + ✏  where ✓⇤ is the unknown sparse
regression coefﬁcient vector with k✓⇤k0 = s⇤  and ✏ ⇠ N (0  2In). We show that PSM always
maintains a pair of sparse primal and dual solutions. Therefore  the computation cost within each
iteration of PSM can be signiﬁcantly reduced. Before we proceed with our main result  we introduce
two assumptions. The ﬁrst assumption requires the regularization factor to be sufﬁciently large.
Assumption 3.2. Suppose that PSM solves (2.3) for a regularization sequence {K}N
smallest regularization factor N satisﬁes

K=0. The

N = Cr log d

n  4kX>✏k1 for some generic constant C.

Existing literature has extensively studied Assumption 3.2 for high dimensional statistical theories.
Such an assumption enforces all regularization parameters to be sufﬁciently large in order to eliminate
irrelevant coordinates along the regularization path. Note that Assumption 3.2 is deterministic
for any given N. Existing literature has veriﬁed that for sparse linear regression models  given
✏ ⇠ N (0  2In)  Assumption 3.2 holds with overwhelming probability.
Before we present the second assumption  we deﬁne the largest and smallest s-sparse eigenvalues of
n1X>X respectively as follows.
Deﬁnition 3.3. Given an integer s  1  we deﬁne

⇢+(s) = sup

kk0s

T X>X

nkk2

2

and ⇢(s) = inf

kk0s

7

T X>X

.

nkk2

2

Assumption 3.4 guarantees that n1X>X satisﬁes the sparse eigenvalue conditions as long as the

es  100s⇤ ⇢ +(s⇤ +es) < +1  and e⇢(s⇤ +es) > 0 

Assumption 3.4. Given k✓⇤k0  s⇤  there exists an integeres such that
where  is deﬁned as  = ⇢+(s⇤ +es)/e⇢(s⇤ +es).
number of active irrelevant blocks never exceeds e2s along the solution path. That is closely related to

the restricted isometry property (RIP) and restricted eigenvalue (RE) conditions  which have been
extensively studied in existing literature.
We then characterize the sparsity of the primal and dual solutions within each iteration.
Theorem 3.5 (Primal and Dual Sparsity). Suppose that Assumptions 3.2 and 3.4 hold. We consider
an alterantive formulation to the Dantzig selector 

subject to  rjL(✓)  0  rjL(✓)  0.

(3.12)

✓

k✓k1

b✓0 = argmin
1   ... bµ0
d  b0
d+1  ... b0

kX>
S

XS(X>

S XS)1k1  1  ⇣ 

2d]> denote the optimal dual variables to (3.12). For any 0   

Letbµ0 = [bµ0
we have kbµ0k0 + kb0k0  s⇤ +es. Moreover  given design matrix satisfying
where ⇣> 0 is a generic constant  S = {j | ✓⇤j 6= 0} and S = {j | ✓⇤j = 0}  we have kb✓0k0  s⇤.

The proof of Theorem 3.5 is provided in Appendix B. Theorem 3.5 shows that within each iteration 
both primal and dual variables are sparse  i.e.  the number of nonzero entries are far smaller than d.
Therefore  the computation cost within each iteration of PSM can be signiﬁcantly reduced by a factor
of O(d/s⇤). This partially justiﬁes the superior performance of PSM in sparse learning.
4 Numerical Experiments
In this section  we present some numerical experiments and give some insights about how the
parametric simplex method solves different linear programming problems. We verify the following
assertions: (1) The parametric simplex method requires very few iterations to identify the nonzero
component if the original problem is sparse. (2) The parametric simplex method is able to ﬁnd the
full solution path with high precision by solving the problem only once in an efﬁcient and scalable
manner. (3) The parametric simplex method maintains the feasibility of the problem up to machine
precision along the solution path.

2

r
o

1

 

t
c
e
V
e
s
n
o
p
s
e
R
e
h

 

t
 
f

o
 
s
e
i
r
t

 

n
E
o
r
e
z
n
o
N

0

1
−

2
−

3
−

●

True Value
Estimated Path

●
●
●

●

●

●

●

●

h

t

a
P
e
h

 

t
 

g
n
o
a

l

 

a
d
b
m
a
L

 
f

o

 
s
e
u
a
V

l

0
0
5

0
0
4

0
0
3

0
0
2

0
0
1

0

y
t
i
l
i

i

b
s
a
e
n

f

I

Flare
PSM

0
0
4

0
0
3

0
0
2

0
0
1

0

0

5

10
Iteration

15

(a) Solution Path

0

5

10

15

0

100

200

300

400

500

Iteration

Iteration

(b) Parameter Path (Rescaled by n)

(c) Feasibility Violation

Figure 1: Dantzig selector method: (a) The solution path of the parametric simplex method; (b) The parameter
path of the parametric simplex method; (c) Feasibility violation along the solution path.
Solution path of Dantzig selector: We start with a simple example that illustrates how the recov-
ered solution path of the Dantzig selector model changes as the parametric simplex method iterates.
We adopt the example used in Candes and Tao (2007). The design matrix X has n = 100 rows and
d = 250 columns. The entries of X are generated from an array of independent Gaussian random
variables that are then Gaussianized so that each column has a given norm. We randomly select s = 8
entries from the response vector ✓0  and set them as ✓0
i = si(1 + ai)  where si = 1 or 1  with
probability 1/2 and ai ⇠N (0  1). The other entries of ✓0 are set to zero. We form y = X✓ 0 + ✏ 
where ✏i ⇠N (0  )  with  = 1. We stop the parametric simplex method when   nplog d/n.
The solution path of the result is shown in Figure 1(a). We see that our method correctly identiﬁes all
nonzero entries of ✓ in less than 10 iterations. Some small overestimations occur in a few iterations
after all nonzero entries have been identiﬁed. We also show how the parameter  evolves as the
parametric simplex method iterates in Figure 1(b). As we see   decreases sharply to less than 5

8

after all nonzero components have been identiﬁed. This reconciles with the theorem we developed.
The algorithm itself only requires a very small number of iterations to correctly identify the nonzero
entries of ✓. In our example  each iteration in the parametric simplex method identiﬁes one or two
non-sparse entries in ✓.
Feasibility of Dantzig Selector: Another advantage of the parametric simplex method is that the
solution is always feasible along the path while other estimating methods usually generate infeasible
solutions along the path. We compare our algorithm with “ﬂare” (Li et al.  2015) which uses the
Alternating Direction Method of Multipliers (ADMM) using the same example described above. We
compute the values of kX>X✓ i  X>yk1  i along the solution path  where ✓i is the i-th basic
solution (with corresponding i) obtained while the parametric simplex method is iterating. Without
any doubts  we always obtain 0s during each iteration. We plug the same list of i into “ﬂare” and
compute the solution path for this list as well. As shown in Table 1  the parametric simplex method is
always feasible along the path since it is solving each iteration up to machine precision; while the
solution path of the ADMM is almost always breaking the feasibility by a large amount  especially in
the ﬁrst few iterations which correspond to large  values. Each experiment is repeated for 100 times.

Table 1: Average feasibility violation with standard errors along the solution path

Maximum violation Minimum Violation

ADMM
PSM

498(122)

0(0)

143(73.2)

0(0)

Performance Benchmark of Dantzig Selector:
In this part  we compare the timing performance
of our algorithm with R package “ﬂare”. We ﬁx the sample size n to be 200 and vary the data
dimension d from 100 to 5000. Again  each entries of X is independent Gaussian and Gaussianized
such that the column has uniform norm. We randomly select 2% entries from vector ✓ to be nonzero
and each entry is chosen as ⇠N (0  1). We compute y = X✓ + ✏  with ✏i ⇠N (0  1) and try to
recover vector ✓  given X and y. Our method stops when  is less than 2plog d/n  such that
the full solution path for all the values of  up to this value is computed by the parametric simplex
method. In “ﬂare”  we estimate ✓ when  is equal to the value in the Dantzig selector model. This
means “ﬂare” has much less computation task than the parametric simplex method. As we can see in
Table 2  our method has a much better performance than “ﬂare” in terms of speed. We compare and
present the timing performance of the two algorithms in seconds and each experiment is repeated for
100 times. In practice  only very few iterations is required when the response vector ✓ is sparse.

Table 2: Average timing performance (in seconds) with standard errors in the parentheses on Dantzig selector

500

Flare
19.5(2.72)
PSM 2.40(0.220)

1000

44.4(2.54)
29.7(1.39)

2000

142(11.5)
47.5(2.27)

5000

1500(231)
649(89.8)

y =⌦ 0

x + D.

Performance Benchmark of Differential Network: We now apply this optimization method to
the Differential Network model. We need the difference between two inverse covariance matrices
to be sparse. We generate ⌃0
x = U>⇤U  where ⇤ 2 Rd⇥d is a diagonal matrix and its entries are
i.i.d. and uniform on [1  2]  and U 2 Rd⇥d is a random matrix with i.i.d. entries from N (0  1). Let
D1 2 Rd⇥d be a random sparse symmetric matrix with a certain sparsity level. Each entry of D1
is i.i.d. and from N (0  1). We set D = D1 + 2|min(D1)|Id in order to guarantee the positive
deﬁniteness of D  where min(D1) is the smallest eigenvalue of D1. Finally  we let ⌦0
x)1
and ⌦0
We then generate data of sample size n = 100. The corresponding sample covariance matrices SX
and SY are also computed based on the data. We are not able to ﬁnd other software which can
efﬁciently solve this problem  so we only list the timing performance of our algorithm as dimension
d varies from 25 to 200 in Table 3. We stop our algorithm whenever the solution achieved the desired
sparsity level. When d = 25  50 and 100  the sparsity level of D1 is set to be 0.02 and when d = 150
and 200  the sparsity level of D1 is set to be 0.002. Each experiment is repeated for 100 times.
Table 3: Average timing performance (in seconds) and iteration numbers with standard errors in the parentheses
on differential network

x = (⌃ 0

Timing

Iteration Number

0.0185(0.00689)

15.5(7.00)

0.376(0.124)
55.3(18.8)

25

50

100

6.81(2.38)
164(58.2)

150

200

13.41(1.26)
85.8(16.7)

46.88(7.24)
140(26.2)

9

References
BANDYOPADHYAYA  S.  MEHTA  M.  KUO  D.  SUNG  M.-K.  CHUANG  R.  JAEHNIG  E. J. 
BODENMILLER  B.  LICON  K.  COPELAND  W.  SHALES  M.  FIEDLER  D.  DUTKOWSKI 
J.  GUÉNOLÉ  A.  ATTIKUM  H. V.  SHOKAT  K. M.  KOLODNER  R. D.  HUH  W.-K. 
AEBERSOLD  R.  KEOGH  M.-C. and KROGAN  N. J. (2010). Rewiring of genetic networks in
response to dna damage. Science Signaling 330 1385–1389.

BÜHLMANN  P. and VAN DE GEER  S. (2011). Statistics for high-dimensional data: methods 

theory and applications. Springer Science &amp; Business Media.

CAI  T. and LIU  W. (2011). A direct estimation approach to sparse linear discriminant analysis.

Journal of the American Statistical Association 106 1566–1578.

CAI  T.  LIU  W. and LUO  X. (2011). A constrained l1 minimization approach to sparse precision

matrix estimation. Journal of the American Statistical Association 106 594–607.

CANDES  E. and TAO  T. (2007). The dantzig selector: Statistical estimation when p is much larger

than n. The Annals of Statistics 35 2313–2351.

DANAHER  P.  WANG  P. and WITTEN  D. M. (2013). The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical Society Series B 7
373–397.

DANTZIG  G. (1951). Linear Programming and Extensions. Princeton University Press.
DEMPSTER  A. (1972). Covariance selection. Biometrics 28 157–175.
GAI  Y.  ZHU  L. and LIN  L. (2013). Model selection consistency of dantzig selector. Statistica

Sinica 615–634.

HUDSON  N. J.  REVERTER  A. and DALRYMPLE  B. P. (2009). A differential wiring analysis of
expression data correctly identiﬁes the gene containing the causal mutation. PLoS Computational
Biology. 5.

IDEKER  T. and KROGAN  N. (2012). Differential network biology. Molecular Systems Biology 5

565.

LI  X.  ZHAO  T.  YUAN  X. and LIU  H. (2015). The ﬂare package for hign dimensional linear
regression and precision matrix estimation in r. Journal of Machine Learning Research 16 553–557.

MURTY  K. (1983). Linear Programming. Wiley  New York  NY.
TIBSHIRANI  R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society 101 267–288.

VANDERBEI  R. (1995). Linear Programming  Foundations and Extensions. Kluwer.
WANG  H.  LI  G. and JIANG  G. (2007). Robust regression shrinkage and consistent variable

selection through the lad-lasso. Journal of Business & Economic Statistics 25 347–355.

YAO  Y. and LEE  Y. (2014). Another look at linear programming for feature selection via methods

of regularization. Statistics and Computing 24 885–905.

ZHAO  S. D.  CAI  T. and LI  H. (2013). Direct estimation of differential networks. Biometrika 58

253–268.

ZHOU  H. and HASTIE  T. (2005). Regularization and variable selection via the elastic net. Journal

of the Royal Statistical Society 67 301–320.

ZHU  J.  ROSSET  S.  HASTIE  T. and TIBSHIRANI  R. (2004). 1-norm support vector machines.

Advances in Neural Information Processing Systems 16.

10

,Haotian Pang
Han Liu
Robert Vanderbei
Tuo Zhao
David Rolnick
Arun Ahuja
Jonathan Schwarz
Timothy Lillicrap
Gregory Wayne