2019,Deep Signature Transforms,The signature is an infinite graded sequence of statistics known to characterise a stream of data up to a negligible equivalence class. It is a transform which has previously been treated as a fixed feature transformation  on top of which a model may be built. We propose a novel approach which combines the advantages of the signature transform with modern deep learning frameworks. By learning an augmentation of the stream prior to the signature transform  the terms of the signature may be selected in a data-dependent way. More generally  we describe how the signature transform may be used as a layer anywhere within a neural network. In this context it may be interpreted as a pooling operation. We present the results of empirical experiments to back up the theoretical justification. Code available at \texttt{github.com/patrick-kidger/Deep-Signature-Transforms}.,DeepSignatureTransformsPatricBonnier1 ∗PatrickKidger1 2 ∗ImanolPerezArribas1 2 ∗CristopherSalvi1 2 ∗TerryLyons1 21MathematicalInstitute UniversityofOxford2TheAlanTuringInstitute BritishLibrary{bonnier kidger perez salvi tlyons}@maths.ox.ac.ukAbstractThesignatureisaninﬁnitegradedsequenceofstatisticsknowntocharacteriseastreamofdatauptoanegligibleequivalenceclass.Itisatransformwhichhaspreviouslybeentreatedasaﬁxedfeaturetransformation ontopofwhichamodelmaybebuilt.Weproposeanovelapproachwhichcombinestheadvantagesofthesignaturetransformwithmoderndeeplearningframeworks.Bylearninganaugmentationofthestreampriortothesignaturetransform thetermsofthesignaturemaybeselectedinadata-dependentway.Moregenerally wedescribehowthesignaturetransformmaybeusedasalayeranywherewithinaneuralnetwork.Inthiscontextitmaybeinterpretedasapoolingoperation.Wepresenttheresultsofempiricalexperimentstobackupthetheoreticaljustiﬁcation.Codeavailableatgithub.com/patrick-kidger/Deep-Signature-Transforms.1Introduction1.1Whatisthesignaturetransform?Whendataisorderedsequentiallythenitcomeswithanaturalpath-likestructure:thedatamaybethoughtofasadiscretisationofapathX:[0 1]→V whereVissomeBanachspace.InpracticeweshallalwaystakeV=Rdforsomed∈N.ForexamplethechangingairpressureataparticularlocationmaybethoughtofasapathinR;themotionofapenonpapermaybethoughtofasapathinR2;thechangeswithinﬁnancialmarketsmaybethoughtofasapathinRd withdpotentiallyverylarge.Givenapath wemaydeﬁneitssignature whichisacollectionofstatisticsofthepath.Themapfromapathtoitssignatureiscalledthesignaturetransform.Deﬁnition1.1.Letx=(x1 ... xn) wherexi∈Rd.Letf=(f1 ... fd):[0 1]→Rdbecontinuous suchthatf(i−1n−1)=xi andlinearontheintervalsinbetween.Thenthesignatureofxisdeﬁnedasthecollectionofiteratedintegrals2Sig(x)=Z···Z0<t1<···<tk<1kYj=1dfijdt(tj)dt1···dtk1≤i1 ... ik≤dk≥0.∗Equalcontribution.2Forclarityherewehaveusedmorewidely-understoodnotation.Thedeﬁnitionofthesignaturetransformisusuallywritteninanequivalentbutalternatemannerusingthenotationofstochasticcalculus;seeDeﬁnitionA.1inAppendixA.33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.Weshalloftenusethetermsignaturetorefertobothapath’ssignatureandthesignaturetransform.Othertextssometimesusethetermpathsignatureinasimilarmanner.Wereferthereaderto[1]foraprimerontheuseofthesignatureinmachinelearning.AbriefoverviewofitskeypropertiesmaybefoundinAppendixA alongwithassociatedreferences.Inshort thesignatureofapathdeterminesthepathessentiallyuniquely anddoessoinanefﬁcient computableway.Furthermore thesignatureisrichenoughthateverycontinuousfunctionofthepathmaybeapproximatedarbitrarilywellbyalinearfunctionofitssignature;itmaybethoughtofasa‘universalnonlinearity’.Takentogetherthesepropertiesmakethesignatureanattractivetoolformachinelearning.Themostsimplewaytousethesignatureisasfeaturetransformation asitmayoftenbesimplertolearnafunctionofthesignaturethanoftheoriginalpath.OriginallyintroducedandstudiedbyChenin[2 3 4] thesignaturehasseenuseinﬁnance[5 6 7 8 9] roughpaththeory[10 11]andmachinelearning[12 13 14 15 16 17 18 19 20].1.2ComparisontotheFouriertransformThesignaturetransformismostcloselyanalogoustotheFouriertransform.ThefundamentaldifferencebetweenthesignaturetransformandclassicalsignaltransformssuchasFouriertransformsandwaveletsisthatthelatterareusedtomodelacurveasalinearcombinationinafunctionalbasis.Thesignaturedoesnottrytomodelorparameterisethecurveitself butinsteadprovidesabasisforfunctionsonthespaceofcurves.Forexample regularlyseeingthesequence:phonecall trade pricemovementinthestreamofofﬁcedatamonitoringatradermightbeanindicationofinsidertrading.Suchoccurrencesarestraightforwardtodetectbyviaalinearregressioncomposedwiththesignaturetransform.ModellingthissignalusingFourierseriesorwaveletswouldbemuchmoreexpensive:linearityofthesetransformsimplythateachchannelmustberesolvedaccuratelyenoughtoseetheorderofevents.Fromasignalprocessingperspective thesignaturecanbethoughtofasaﬁlterwhichisinvarianttoresamplingoftheinputsignal.(SeePropositionA.7inAppendixA).1.3UseofthesignaturetransforminmachinelearningThesignatureisaninﬁnitesequence soinpracticesomeﬁnitecollectionoftermsmustbeselected.Sincethemagnitudeofthetermsexhibitfactorialdecay seePropositionA.5inAppendixA itisusual[21]tosimplychoosetheﬁrstNtermsofthissequence whichwilltypicallybethelargestterms.TheseﬁrstNtermsarecalledthesignatureofdepthNorthetruncatedsignatureofdepthN andthecorrespondingtransformisdenotedSigN.Butifthefunctiontobelearneddependednontriviallyonthehigherdegreeterms thencrucialinformationhasnonethelessbeenlost.Thismayberemedied.Applyapointwiseaugmentationtotheoriginalstreamofdatabeforetakingthesignature.ThentheﬁrstNtermsofthesignaturemaybetterencodethenecessaryinformation[19 20].Explicitly letΦ:Rd→Rebeﬁxed;onecouldensurethatinformationisnotlostbytakingΦ(x)=(x ϕ(x))forsomeϕ.Thenratherthantakingthesignatureofx=(x1 ... xn) wherexi∈Rd insteadtakethesignatureofΦ(x)=(Φ(x1) ... Φ(xn)).Inthiswayonemaycapturehigherorderinformationfromthestreaminthelowerdegreetermsofthesignature.1.4OurworkButhowshouldthisaugmentationΦbechosen?Previousworkhasﬁxeditarbitrarily orexperimentedwithseveraloptionsbeforechoosingone[19 20].Observethatineachcasethemapx7→SigN(Φ(x))isstillultimatelyjustafeaturetransformationontopofwhichamodelisbuilt.OurmoregeneralapproachistoallowtheselectionofΦtobedata-dependent byhavingitbelearned;inparticularitmaybeaneuralnetwork.Furthermorethereisnoreasonitshouldnecessarilyoperatepointwise nor(sinceitisnowlearned)needitbeoftheform(x ϕ(x)).Inthiswaywemayenjoythebeneﬁtsofusingsignatureswhileavoidingtheirmainlimitation.Butthismeansthatthesignaturetransformisessentiallyoperatingasalayerwithinaneuralnetwork.Itconsumesatensorofshape(b d n)–correspondingtoabatchofsizebofpathsinRdthathave2beensampledntimes–andreturnsatensorofshape(b (dN+1−1)/(d−1)) whereNisthenumberoftermsusedinthetruncatedsignature.3Thesignatureisbeingusedasapoolingoperation.Thereisnoreasontostophere.Ifthesignaturelayerworkswelloncethenitisnaturaltoseektouseitagain.Theobviousproblemisthatthesignaturetransformconsumesastreamofdataandreturnsstatisticswhichhavenoobviousstream-likequalities.Thesolutionistolifttheinputstreamtoastreamofstreams;forexample thestreamofdata(x1 ... xn)maybeliftedtothe‘expandingwindows’of(x2 ... xn) wherexi=(x1 ... xi).Nowapplythesignaturetoeachstreamtoobtainastreamofsignatures(SigN(x2) ... SigN(xn)) whichisessentiallyastreaminEuclideanspace.Andnowthisnewstreammaybeaugmentedviaaneuralnetworkandtheprocessrepeatedagain asmanytimesaswewish.Inthiswaythesignaturetransformhasbeenelevatedfromaone-timefeaturetransformationtoaﬁrst-classlayerwithinaneuralnetwork.Thuswemayreapthebeneﬁtsofboththesignaturetransform withitsstrongcorpusofmathematicaltheory andthebeneﬁtsofneuralnetworks withtheirgreatempiricalsuccess.Naturallyallofthisimpliestheneedforanefﬁcientimplementationofthesignaturetransform.Suchconcernshavemotivatedthecreationofthespin-offSignatoryproject[22].Theremainderofthepaperislaidoutasfollows.InSection2webrieﬂydiscusssomerelatedwork inSection3wedetailthespeciﬁcsofembeddingthesignatureasalayerwithinaneuralnetwork.Sections4coversexperiments;wedemonstratepositiveresultsforgenerative supervised andreinforcementlearningproblems.Section5istheconclusion.AppendixAprovidesanexpositionofthetheoreticalpropertiesofthesignature andAppendixBspeciﬁesimplementationdetails.2RelatedWorkThesignaturetransformisroughlyanalogoustotheuseofwaveletsorFouriertransforms andtherearealsorelatedmodelsbasedaroundthese forexample[23 24 25 26].Wedonotknowofadetailedcomparisonbetweentheuseofthesevarioustransformationsinthecontextofmachinelearning.Somerelatedworkusingsignatureshasalreadybeendiscussedintheprevioussection.Weexpandontheirproposedmodelshere.Deﬁnition2.1.GivenasetV thespaceofstreamsofdatainVisdeﬁnedasS(V)={x=(x1 ... xn):xi∈V n∈N}.Givenx=(x1 ... xn)∈S(V) theintegerniscalledthelengthofx.TwosimplemodelsutilisingthesignaturelayerareshowninFigure1.Inprincipletheuniversalnonlinearitypropertyofsignatures(seePropositionA.6inAppendixA)guaranteesthatthemodelshowninFigure1a isrichenoughtolearnanycontinuousfunction.(Withtheneuralnetworktakentobeasinglelinearlayerandtheinputstreamassumedtoalreadybetime-augmented.)Inpractice ofcourse thesignaturemustbetruncated.Furthermore itisnotclearhowtoappropriatelychoosethetruncationhyperparameterN.Thusamorepracticalapproachistoremovetherestrictionthattheneuralnetworkmustbelinear andlearnanonlinearfunctioninstead.Thisapproachhasbeenappliedsuccessfullyinvarioustasks[5 12 13 14 15 16 17 18].3As(dN+1−1)/(d−1)=PNk=0dkisthenumberofscalarvaluesinasignaturewithNterms.xInputstreamSignaturetransformSigNNeuralnetworkfθOutputσ(a)Neural-signaturemodel.Trainableparameters:θ.xInputstreamFeaturemapΦSignaturetransformSigNNeuralnetworkfθOutputσ(b)Neural-signature-augmentmodel.Trainableparameters:θ.Figure1:Twosimplearchitectureswithasignaturelayer.3AnalternatemodelisshowninFigure1b.Following[19 20] apointwisetransformationcouldbeappliedtothestreambeforetakingthesignaturetransform.Thatis applyingthefeaturemapΦ:Rd→Retothed-dimensionalstreamofdata(x1 ... xn)∈S(Rd)yields(Φ(x1) ... Φ(xn))∈S(Re);thesignatureofΦ(x)maythenpotentiallycapturepropertiesofthestreamofdatathatwillyieldmoreeffectivemodels.3ThesignaturetransformasalayerinaneuralnetworkHowever thereisnotalwaysaclearcandidateforthefeaturemapΦandagoodchoiceislikelytobedata-dependent.ThusweproposetomakeΦlearnablebytakingΦ=Φθtobeaneuralnetworkwithtrainableparametersθ.Inthiscase weagainobtaintheneuralnetworkshowninFigure1b exceptthatΦisnowalsolearnable.Thesignaturehasnowbecomealayerwithinaneuralnetwork.Itconsumesatensorofshape(b d n)–correspondingtoabatchofsizebofpathsinRdthathavebeensampledntimes–andreturnsatensorofshape(b (dN+1−1)/(d−1)) whereNisthenumberoftermsusedinthetruncatedsignature.Despitebeingformedofintegrals thesignatureisinfactstraightforwardandefﬁcienttocomputeexactly seeSectionA.3inAppendixA.Morethanthat thecomputationmayinfactbedescribedintermsofstandardtensoroperations.Assuchitmaybebackpropagatedthroughwithoutdifﬁculty.3.1Stream-preservingneuralnetworksLetx=(x1 ... xn)∈S(Rd).WhateverthechoiceofΦθ itmustpreservethestream-likenatureofthedataifwearetotakeasignatureafterwards.ThesimplestwayofdoingthisistohaveΦθmapRd→Re sothatitoperatespointwise.ThisdeﬁnesΦ(x)byΦ(x)=(Φθ(x1) ... Φθ(xn))∈S(Re).(1)Anotherwaytopreservethestream-likenatureistosweepaonedimensionalconvolutionalongthestream;moregenerallyonecouldsweepawholefeedforwardnetworkalongthestream.Forsomem∈NandΦθ:Rd×m→RethisdeﬁnesΦ(x)byΦ(x)=(Φθ(x1 ... xm) ... Φθ(xn−m+1 ... xn))∈S(Re).(2)Moregenerallystillthenetworkcouldberecurrent byhavingmemory.LetΦ0=0 ﬁxm∈N anddeﬁneΦk=Φθ(xk ... xk+m;Φk−1)fork=1 ... n−m+1.ThendeﬁneΦ(x)byΦ(x)=(Φ1 ... Φn−m+1)∈S(Re).(3)3.2Stream-likedataItisworthtakingamomenttothinkwhatisreallymeantby‘stream-likenature’.Thesignaturetransformisdeﬁnedonpaths;itisappliedtoastreamofdatainS(Rd)byﬁrstinterpolatingthedataintoapathandthentakingthesignature.Thedataistreatedasadiscretisationorsetofobservationsofsomeunderlyingpath.Notethatthereisnothingwrongwiththepathitselfhavingadiscretestructuretoit;forexampleasentence.Inprincipleonecouldreshapeatensorofshape(b nd)withnostream-likenatureintooneofshape(b d n) andthentakethesignature.Howeveritisnotclearwhatthismeansmathematically.Thereisnounderlyingpath.Thesignatureisatthispointanessentiallyarbitrarytransformation withoutthemathematicalguaranteesnormallyassociatedwithit.3.3Stream-preservingsignatures usingliftsWewouldliketoapplythesignaturelayermultipletimes.Howeverapplyingthesignaturetransformconsumesthestream-likenatureofthedata whichpreventsthis.Thesolutionistoconstructastreamofsignaturesinthefollowingway:givenastreamx=(x1 ... xn)∈S(Rd) letxk=(x1 ... xk)fork=2 ... n andapplythesignaturetoeachxktoobtainthestream(SigN(x2) ... SigN(xn))∈S(R(dN+1−1)/(d−1)).(4)4Theshorteststreamitismeaningfultotakethesignatureofisoflengthtwo whichiswhythereisnocorrespondingSigN(x1)term.Inthiswaythestream-likenatureofthedataispreservedthroughthesignaturetransform.Thisnotionmaybegeneralised:let‘=(‘1 ‘2 ... ‘v):S(Rd)→S(S(Re)) whichwerefertoasaliftintothespaceofstreamsofstreams(andvwilllikelydependonthelengthoftheinputto‘).Thenapplythesignaturestream-wisetodeﬁneSigN(‘(x))bySigN(‘(x))=(cid:0)SigN(‘1(x)) ... SigN(‘v(x))(cid:1)∈S(R(eN+1−1)/(e−1)).(5)Intheexampleofequation(4) ‘isgivenby‘(x)=(x2 ... xn).(6)Otherplausiblechoicesfor‘aretocutupxintomultiplepieces forexample‘(x)=((x1 x2) (x3 x4) ... (x2bn/2c−1 x2bn/2c)) (7)ortotakeaslidingwindow‘(x)=((x1 x2 x3) (x2 x3 x4) ... (xn−2 xn−1 xn)).(8)3.4MultiplesignaturelayersByinsertinglifts thesignaturetransformmaybecomposedasmanytimesasdesired.Thatis supposewewishtolearnamapfromS(Rd)toX whereXissomeset.(Whichmaybeﬁniteforaclassiﬁcationproblemorinﬁniteforaregressionproblem.)Letci di ei Ni∈Nbesuchthatd1=danddi+1=(cNi+1i−1)/(ci−1) fori=1 ... k.LetΦθii:Rdi×mi→Rei ‘i:S(Rei)→S(S(Rci)) fθk+1:S(R(cNk+1k−1)/(ck−1))→X whereΦθiiand‘iaredeﬁnedinthemannerofequations(1)–(3)and(6)–(8) andθ1 ... θk+1aresometrainableparameters.Thendeﬁningcompositionsinthemannerofequations(1)–(5) letσ=(cid:16)fθk+1◦SigNk◦‘k◦Φθkk◦···◦Φθ22◦SigN1◦‘1◦Φθ11(cid:17)(x).Thisdeﬁnesthedeepsignaturemodel summarisedinFigure2.AnimportantspecialcaseiswhenV=S(Re) sothattheﬁnalnetworkfθk+1isstream-preserving.Thentheoverallmodelx7→σisalsostream-preserving.SeeforexampleSection4.1.Notethatinprincipleitisacceptabletotakethetriviallifttoasequenceofasingleelement ‘(x)=(x).(9)Takingthesignatureofthiswillthenessentiallyremovethestream-likenature however soitissuitableonlyfortheﬁnalliftofadeepsignaturemodel.WeobserveinparticularthatthisiswhatisdoneinthemodelsdescribedinFigure1 whichweidentifyasspecialcasesofthedeepsignaturemodel lackingalsoanylearnedtransformationbeforethesignature.Itiseasytoseethatthedeepsignaturemodelexhibitstheuniversalapproximationproperty.Thisfactfollowsfromtheuniversalapproximationtheoremforneuralnetworks[27]andfromtheuniversalnonlinearitypropertyofsignatures(seePropositionA.6inAppendixA).xInputstreamNeuralnetworkΦθ11Lift‘1SignaturetransformSigN1NeuralnetworkΦθ22...Lift‘kSignaturetransformSigNkNeuralnetworkfθk+1OutputσFigure2:Deepsignaturemodel.Trainableparameters:θ1 ... θk+1.53.5ImplementationWhenusingthesignaturetransformasafeaturetransformation thenitsufﬁcestojustpre-processandsavetheentiredatasetbeforetraining.Howeverwhenthesignaturetransformisplacedwithinaneuralnetworkthenthesignaturetransformmustbeevaluatedandbackpropagatedthroughforeachstepoftraining;thisismuchmorecomputationallyintensive.Thishasmotivatedthecreationoftheseparatespin-offSignatoryproject[22] toefﬁcientlyperformandbackpropagatethroughthesignaturetransform.3.6InvertingthetruncatedsignatureHowwelldoesatruncatedsignatureencodetheoriginalstreamofdata?Asimpleexperimentistoattempttorecovertheoriginalstreamofdatagivenitstruncatedsignature.Weremarkthatﬁndingamathematicaldescriptionofthisinversionisachallengingtask[28 29 30].Fixastreamofdatax=(x1 ... xn)∈S(Rd).AssumethatthetruncatedsignatureSigN(x)andthenumberofstepsn∈Nareknown.NowapplygradientdescenttominimiseL(y;x)=(cid:13)(cid:13)SigN(y)−SigN(x)(cid:13)(cid:13)22fory=(y1 ... yn)∈S(Rd).Figure3showsfourhandwrittendigitsfromthePenDigitsdataset[31].Thesolidbluepathistheoriginalpathx whilstthedashedorangepathisthereconstructedpathyminimisingL(y;x).TruncatedsignaturesoforderN=12wereusedforthistask.Weseethatthetruncatedsignatureshavemanagedtoencodetheinputpathsxalmostperfectly.Figure3:Originalpath(blue)andpathreconstructedfromitssignature(dashedorange)forfourhandwrittendigitsinthePenDigitsdataset[31].4Numericalexperiments4.1AgenerativemodelforastochasticprocessGenerativemodelsaretypicallytrainedtolearntotransformrandomnoisetoatargetdistribution.OnecommonapproachareGenerativeAdversarialNetworks[32].AnalternativeapproachistodeﬁneadistanceonthespaceofdistributionsbyembeddingthemintoaReproducingKernelHilbertSpace.Thediscriminatoristhenaﬁxedtwo-sampletestbasedonakernelmaximummeandiscrepancy.ThisisknownasaGenerativeMomentMatchingNetwork[33 34 35].Withthisframeworkweproposeadeepsignaturemodeltogeneratesequentialdata.Thediscriminatorisasin[19 20].ThenaturalchoiceforrandomnoiseisBrownianmotionBt.Deﬁnethekernelk:S(Rd)×S(Rd)→Rbyk(x y)=(cid:0)SigN(λxx) SigN(λyy)(cid:1) whereλx∈RisacertainnormalisingconstantwhichguaranteesthatkisthekernelofaReproducingKernelHilbertSpace and(· ·)denotesthedotproduct.Givennsamples{x(i)}ni=1⊆S(Rd)fromthegeneratorandmsamples{y(i)}mi=1⊆S(Rd)fromthetargetdistribution deﬁnethelossTbyT(cid:16){x(i)}ni=1 {y(i)}mi=1(cid:17)=1n2Xi jk(x(i) x(j))−2nmXi jk(x(i) y(j))+1m2Xi jk(y(i) y(j)).Lettheinputtothenetworkbetime-augmentedBrownianmotionB=((t1 Bt1) ... (tn Btn))∈S(R2).6BStepsChannelsΦθ1StepsChannels‘StepsChannelsStepsofstepsSigNStepsSignaturetermsfθ2StepsChannelsxStepsChannelsSigMStepsSignaturetermsGeneratorDiscriminatorTwo-sampletestyStepsChannelsSigMStepsSignaturetermsFigure4:Generativemodelarchitecture.Trainableparameters:θ1 θ2.Thereisanimplicitbatchdimensionthroughout.Giventwostream-preservingneuralnetworksΦθ1andfθ2 andalift‘ thenthegenerativemodelisdeﬁnedbyx=(fθ2◦SigN◦‘◦Φθ1)(B).TheoverallmodelisshowninFigure4.Inanicetwist boththegeneratorandthediscriminatorinvolvethesignature.Figure5:Generatedpathsalongsidetheoriginalpaths.Observehowthegenerativepartisaparticularcaseofthedeepsignaturemodel andthatfurthermorethewholegenerator-discriminatorpairisalsoaparticularcaseofthedeepsignaturemodel withthetrivialliftofequation(9)beforethesecondsignaturelayer.Weappliedtheproposedmodeltoadatasetof1024realisationsofanOrnstein–Uhlenbeckprocess[36].Thelosswasminimisedat6.6×10−4 whichimpliesthatthegeneratedpathsarestatisticallyalmostindistinguishablefromtherealOrnstein–Uhlenbeckprocess.Figure5showsthegeneratedpathsalongsidetheoriginalones.FurtherimplementationdetailsareinAppendixB.4.2SupervisedlearningwithfractionalBrownianmotionFractionalBrownianmotion[37]isaGaussianprocessBH:[0 ∞)→RthatgeneralisesBrownianmotion.Itisself-similarandexhibitsfractal-likebehaviour.FractionalBrownianmotiondependsuponaparameterH∈(0 1) knownastheHurstparameter.LowerHurstparametersresultinnoticeablyrougherpaths.ThecaseofH=1/2correspondstousualBrownianmotion.FractionalBrownianmotionhasbeensuccessfullyusedtomodelphenomenaindiverseﬁelds.Forexample empiricalevidencefromﬁnancialmarkets[38]suggeststhatlog-volatilityiswellmodelledbyfractionalBrownianmotionwithHurstparameterH≈0.1.EstimatingtheHurstparameterofafractionalBrownianmotionpathisconsideredanontrivialtaskbecauseofthepaths’non-stationarityandlongrangedependencies[39].Wetrainavarietyofmodelstoperformthisestimation.Thatis tolearnthemapxH7→H wherexH=((t0 BHt0) ... (tn BHtn))∈S(R2)forsomerealisationofBH.7Table1:Finaltestmeansquarederror(MSE)forthedifferentmodels averagedover3trainingruns orderedfromlargesttosmallest.TestMSEMeanVariance#ParamsRescaledRange7.2×10−23.7×10−3N/ALSTM4.3×10−28.0×10−312961Feedforward2.8×10−23.0×10−310209Neural-Sig1.1×10−28.2×10−410097GRU3.3×10−31.3×10−39729RNN1.7×10−34.9×10−410091DeepSigNet2.1×10−48.7×10−59261DeeperSigNet1.6×10−42.1×10−59686Figure6:PerformanceatestimatingtheHurstparameterforvariousmodels withandwithoutsignatures foraparticular(typical)trainingrun.TheresultsareshowninFigure6andTable1.AlsoshowninTable1aretheresultsoftherescaledrangemethod[40] whichisamathematicallyderivedmethodratherthanalearnedmethod.RNN GRUandLSTMmodelsprovidebaselinesinthecontextofrecurrentneuralnetworks.ThesimpleNeural-SigmodeloutlinedpreviouslyinFigure1aprovidesabaselinefromthecontextofsignatures.DeepSigNetandDeeperSigNetarebothdeepsignaturemodelsoftheformgivenbyFigure2.DeepSigNethasasinglelargeNeural-Lift-Signatureblock whilstDeeperSigNethasthreesmallerones.Weobservethattraditionalsignaturebasedmodelsperformslightlyworsethantraditionalrecurrentmodels butthatdeepsignaturemodelsoutperformallothermodelsbyatleastanorderofmagnitude.FurtherimplementationdetailsarefoundinAppendixB.4.3Non-MarkoviandeepreinforcementlearningFinallyweshowhowtheseideasmaybeextended bydemonstratingamodelthataddsaresidualconnectiontothedeepsignaturemodel;itmayalsobeinterpretedasusingsignaturesasthememoryofarecurrentneuralnetwork.Asanexample weapplythisarchitecturetotackleanon-Markovianreinforcementlearningproblem.Thismeansthattheoptimalactiondependsnotjustonthecurrentstateoftheenvironment butuponthehistoryofpaststates sothattheagentmustmaintainamemory.LetΦθ1:Rd→Reandfθ2:Rd+(eN+1−1)/(e−1)→{actions}befunctionsdependingonlearnableparametersθ1 θ2.Giveninputxi∈Rdattimei letyi=Φθ1(xi) σi=σi−1⊗SigN((yi−1 yi)) ai=fθ2(xi σi) whereaiistheactionproposedbythenetworkattimei andyiandσiarethememoryattimei and⊗denotesthetensorproductasinA.13inAppendixA.8ThemodelissummarisedinFigure7asarecurrentneuralnetworkwithsignature-basedmemory.Notethatyiispreservedinmemoryonlytocomputethesignatureatthenexttimestep astheshortestpathitismeaningfultocomputethesignatureofisoflengthtwo.However notethatbyPropositionA.15inAppendixA σi=SigN(Φθ1(x1) ... Φθ1(xi))∈R(eN+1−1)/(e−1).xiΦθ1yiSigNyi−1⊗σiσi−1fθ2aiFigure7:Agentarchitectureasarecurrentnetwork.Trainableparameters:θ1 θ2.Furthermorethexi yi σiandaimaybecollectedintostreams(xi)i∈S(Rd) (yi)i∈S(Re) (σi)i∈S(R(eN+1−1)/(e−1)) (ai)i∈S({actions}).Inthiswaywemayinterpretthismodelasageneralisationofdeepsignaturemodel:ithasasingleNeural-Lift-Signatureblock withaskipconnectionacrossthewholeblock.TheneuralcomponentisgivenbytheneuralnetworkΦθ1 whichisstream-preservingasitoperatespointwise inthemannerofequation(1).Theliftisthe‘expandingwindow’liftgivenbyequation(6).Finallyfθ2isanotherneuralnetwork whichisagainpointwiseandthusstream-preserving.ThisinterpretationofthemodelisdemonstratedinFigure8.Wetestthismodelonanon-MarkovianmodiﬁcationtotheclassicalMountainCarproblem[41] inwhichtheagentreceivesonlypartialinformation:itisonlygiventhecar’sposition andnotitsvelocity.Weﬁndthatitiscapableoflearninghowtosolvetheproblemwithinasetnumberofepisodes whilstacomparableRNNarchitecturefailstodoso.ThereinforcementlearningtechniqueusedwasDeepQLearning[42]withthespeciﬁedmodelsperformingfunctionapproximationonQ.Bothmodelswerechosentohavecomparablenumbersofparameters.FurtherimplementationdetailscanbefoundinAppendixB.(xi)iΦθ1(yi)i‘SigN(σi)ifθ2(ai)iFigure8:Agentarchitectureasaresidualnetwork.Trainableparameters:θ1 θ2.Thelift‘isthe‘expandingwindow’liftofequation(6).5ConclusionThereisastrongcorpusoftheorymotivatingtheuseofthesignaturetransformasatooltounderstandstreamsofdata.Meanwhileneuralnetworkshaveenjoyedgreatempiricalsuccess.Itisthusdesirabletobringthemtogether;inthispaperwehavedescribedhowthismaybedoneinageneralfashion andhaveprovidedexamplesofhowthisprinciplemaybeusedinavarietyofdomains.Therearetwokeycontributions.First wediscussstream-preservingneuralnetworks whicharewhatallowforusingsignaturetransformsdeeperwithinanetwork ratherthanasjustafeaturetransformation.Second wediscusslifts whichiswhatallowsfortheuseofmultiplesignaturetransforms.Inthiswaywehavesigniﬁcantlyextendedtheuseofthesignaturetransforminmachinelearning:ratherthanlimitingitsusagetodatapreprocessing wedemonstratehowthesignaturetransform asauniveralnonlinearity maybeusedasapoolinglayerwithinaneuralnetwork.9AcknowledgementsPBwassupportedbytheEPSRCgrantEP/R513295/1.PKwassupportedbytheEPSRCgrantEP/L015811/1.PK IPA CS TLweresupportedbytheAlanTuringInstituteundertheEPSRCgrantEP/N510129/1.References[1]I.ChevyrevandA.Kormilitzin “Aprimeronthesignaturemethodinmachinelearning ”arXivpreprintarXiv:1603.03788 2016.[2]K.T.Chen “Iteratedintegralsandexponentialhomomorphisms ”Proc.LondonMath.Soc 4 502–512 1954.[3]K.T.Chen “Integrationofpaths geometricinvariantsandageneralizedBaker-Hausdorffformula ”Ann.ofMath.(2) 65:163–178 1957.[4]K.T.Chen “Integrationofpaths-afaithfulrepresentationofpathsbynon-commutativeformalpowerseries ”Trans.Amer.Math.Soc.89(1958) 395–407 1958.[5]T.Lyons H.Ni andH.Oberhauser “Afeaturesetforstreamsandanapplicationtohigh-frequencyﬁnancialtickdata ”ICBDC 2014.[6]L.G.Gyurk´o T.Lyons M.Kontkowski andJ.Field “Extractinginformationfromthesignatureofaﬁnancialdatastream ”arXivpreprintarXiv:1307.7244 2014.[7]T.Lyons S.Nejad andI.P.Arribas “Nonparametricpricingandhedgingofexoticderivatives ”arXivpreprintarXiv:1905.00711 2019.[8]T.Lyons S.Nejad andI.P.Arribas “Model-freepricingandhedgingindiscretetimeusingroughpathsignatures ”arXivpreprintarXiv:1905.01720 2019.[9]J.Kalsi T.Lyons andI.P.Arribas “Optimalexecutionwithroughpathsignatures ”arXivpreprintarXiv:1905.00728 2019.[10]T.J.Lyons “Differentialequationsdrivenbyroughsignals ”RevistaMatem´aticaIberoamericana vol.14 no.2 pp.215–310 1998.[11]P.K.FrizandN.B.Victoir “Multidimensionalstochasticprocessesasroughpaths:theoryandapplications ”CambridgeUniversityPress 2010.[12]W.Yang L.Jin andM.Liu “Chinesecharacter-levelwriteridentiﬁcationusingpathsignaturefeature DropStrokeanddeepCNN ”in201513thInternationalConferenceonDocumentAnalysisandRecognition(ICDAR) pp.546–550 IEEE 2015.[13]Z.Xie Z.Sun L.Jin H.Ni andT.Lyons “Learningspatial-semanticcontextwithfullyconvolutionalrecurrentnetworkforonlinehandwrittenChinesetextrecognition ”IEEEtransactionsonpatternanalysisandmachineintelligence vol.40 no.8 pp.1903–1917 2018.[14]W.Yang L.Jin D.Tao Z.Xie andZ.Feng “DropSample:Anewtrainingmethodtoenhancedeepconvolutionalneuralnetworksforlarge-scaleunconstrainedhandwrittenChinesecharacterrecognition ”PatternRecognition vol.58 pp.190–203 2016.[15]W.Yang L.Jin andM.Liu “Deepwriterid:Anend-to-endonlinetext-independentwriteridentiﬁcationsystem ”IEEEIntelligentSystems vol.31 no.2 pp.45–53 2016.[16]C.Li X.Zhang andL.Jin “LPSNet:anovellogpathsignaturefeaturebasedhandgesturerecognitionframework ”inProceedingsoftheIEEEInternationalConferenceonComputerVision pp.631–639 2017.[17]W.Yang T.Lyons H.Ni C.Schmid L.Jin andJ.Chang “Leveragingthepathsignatureforskeleton-basedhumanactionrecognition ”arXivpreprintarXiv:1707.03993 2017.[18]W.Yang L.Jin H.Ni andT.Lyons “Rotation-freeonlinehandwrittencharacterrecognitionusingdyadicpathsignaturefeatures hangingnormalization anddeepneuralnetwork ”in201623rdInternationalConferenceonPatternRecognition(ICPR) pp.4083–4088 IEEE 2016.[19]F.J.Kir´alyandH.Oberhauser “Kernelsforsequentiallyordereddata ”JournalofMachineLearningResearch 2019.[20]I.ChevyrevandH.Oberhauser “Signaturemomentstocharacterizelawsofstochasticprocesses ”arXivpreprintarXiv:1810.10971 2018.[21]T.Lyons “Roughpaths signaturesandthemodellingoffunctionsonstreams ”arXivpreprintarXiv:1405.4537 2014.10[22]P.Kidger “Signatory:differentiablecomputationsofthesignatureandlogsignaturetransforms onbothCPUandGPU ”2019.https://github.com/patrick-kidger/signatory.[23]A.Silvescu “Fourierneuralnetworks ”ProceedingsoftheInternationalJointConferenceOnNeuralNetworks IEEE 1999.[24]L.Mingo L.Aslanyan J.Castellanos M.Diaz andV.Riazanov “Fourierneuralnetworks:anapproachwithsinusoidalactivationfunctions ”Int.J.Inf.TheoryAppl. 11 2004.[25]M.GashlerandS.Ashmore “Modelingtimeseriesdatawithdeepfourierneuralnetworks ”Neurocomputing 2016.[26]Q.ZhangandA.Benveniste “Waveletnetworks ”IEEETrans.NeuralNetw. 1992.[27]A.Pinkus “ApproximationtheoryoftheMLPmodelinneuralnetworks ”ActaNumer. vol.8 pp.143–195 1999.[28]T.J.LyonsandW.Xu “Invertingthesignatureofapath ”JournaloftheEuropeanMathematicalSociety vol.20 no.7 pp.1655–1687 2018.[29]J.Chang N.Dufﬁeld H.Ni W.Xu etal. “Signatureinversionformonotonepaths ”ElectronicCommunicationsinProbability vol.22 2017.[30]J.Chang Effectivealgorithmsforinvertingthesignatureofapath.PhDthesis UniversityofOxford 2018.[31]D.DuaandC.Graff “UCIMachineLearningRepository ”2017.[32]I.Goodfellow J.Pouget-Abadie M.Mirza B.Xu D.Warde-Farley S.Ozair A.Courville andY.Bengio “Generativeadversarialnets ”inAdvancesinneuralinformationprocessingsystems pp.2672–2680 2014.[33]Y.Li K.Swersky andR.Zemel “Generativemomentmatchingnetworks ”ICML 2015.[34]G.K.Dziugaite D.M.Roy andZ.Ghahramani “Traininggenerativeneuralnetworksviamaximummeandiscrepancyoptimization ”UAI 2015.[35]A.Gretton K.M.Borgwardt M.Rasch B.Scholkopf andA.J.Smola “Akernelmethodforthetwo-sampleproblem ”AdvancesinNeuralInformationProcessingSystems 2007.[36]G.E.UhlenbeckandL.S.Ornstein “OnthetheoryoftheBrownianmotion ”Physicalreview vol.36 no.5 p.823 1930.[37]Y.Mishura StochasticcalculusforfractionalBrownianmotionandrelatedprocesses vol.1929.SpringerScience&BusinessMedia 2008.[38]J.Gatheral T.Jaisson andM.Rosenbaum “VolatilityisRough ”QuantitativeFinance 18:6 933-949 2018.[39]L.Lacasa B.Luque J.Luque andJ.C.Nuno “Thevisibilitygraph:AnewmethodforestimatingtheHurstexponentoffractionalBrownianmotion ”EPL(EurophysicsLetters) vol.86 no.3 p.30001 2009.[40]H.Hurst “TheLong-TermStorageCapacityofReservoirs ”TransactionsoftheAmericanSocietyofCivilEngineers 1951.[41]G.Brockman V.Cheung L.Pettersson J.Schneider J.Schulman J.Tang andW.Zaremba “Openaigym ”arXivpreprintarXiv:1606.01540 2016.[42]V.Mnih K.Kavukcuoglu D.Silver A.A.Rusu J.Veness M.G.Bellemare A.Graves M.Riedmiller A.K.Fidjeland G.Ostrovski etal. “Human-levelcontrolthroughdeepreinforcementlearning ”Nature vol.518 no.7540 p.529 2015.[43]B.M.HamblyandT.J.Lyons “Uniquenessforthesignatureofapathofboundedvariationandthereducedpathgroup ”AnnalsofMathematics vol.171 no.1 pp.109–167 2010.[44]I.PerezArribas “Derivativespricingusingsignaturepayoffs ”arXivpreprintarXiv:1809.09466 2018.[45]D.KingmaandJ.Ba “Adam:Amethodforstochasticoptimization ”ICLR 2015.[46]A.Paszke S.Gross S.Chintala G.Chanan E.Yang Z.DeVito Z.Lin A.Desmaison L.Antiga andA.Lerer “AutomaticdifferentiationinPyTorch ”2017.[47]J.ReizensteinandB.Graham “Theiisignaturelibrary:efﬁcientcalculationofiterated-integralsignaturesandlogsignatures ”arXivpreprintarXiv:1802.08252 2018.[48]P.Embrechts Selfsimilarprocesses vol.21.PrincetonUniversityPress 2009.[49]C.J.C.H.Watkins “Learningfromdelayedrewards ”1989.11,Patrick Kidger
Patric Bonnier
Imanol Perez Arribas
Cristopher Salvi
Terry Lyons