2008,Empirical performance maximization for linear rank statistics,The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications  ranging from anomaly detection in signal processing to information retrieval  through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC  the local AUC  the p-norm push  the DCG and others  can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process.,Empirical performance maximization

for linear rank statistics

St´ephan Cl´emenc¸on

Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141

stephan.clemencon@telecom-paristech.fr

Nicolas Vayatis

ENS Cachan & UniverSud - CMLA UMR CNRS 8536

vayatis@cmla.ens-cachan.fr

Abstract

The ROC curve is known to be the golden standard for measuring performance of
a test/scoring statistic regarding its capacity of discrimination between two popu-
lations in a wide variety of applications  ranging from anomaly detection in signal
processing to information retrieval  through medical diagnosis. Most practical
performance measures used in scoring applications such as the AUC  the local
AUC  the p-norm push  the DCG and others  can be seen as summaries of the
ROC curve. This paper highlights the fact that many of these empirical criteria
can be expressed as (conditional) linear rank statistics. We investigate the proper-
ties of empirical maximizers of such performance criteria and provide preliminary
results for the concentration properties of a novel class of random variables that
we will call a linear rank process.

1 Introduction

In the context of ranking  several performance measures may be considered. Even in the simplest
framework of bipartite ranking  where a binary label is available  there is not one and only natural cri-
terion  but many possible options. The ROC curve provides a complete description of performance
but its functional nature renders direct optimization strategies rather complex. Empirical risk mini-
mization strategies are thus based on summaries of the ROC curve  which take the form of empirical
risk functionals where the averages involved are no longer taken over i.i.d. sequences. The most
popular choice is the so-called AUC criterion (see [AGH+05] or [CLV08] for instance)  but when
top-ranked instances are more important  various choices can be considered: the Discounted Cumu-
lative Gain or DCG [CZ06]  the p-norm push (see [Rud06])  or the local AUC (refer to [CV07]).
The present paper starts from the simple observation that all these summary criteria have a common
feature: conditioned upon the labels  they all belong to the class of linear rank statistics. Such statis-
tics have been extensively studied in the mathematical statistics literature because of their optimality
properties in hypothesis testing  see [HS67]. Now  in the statistical learning view  with the impor-
tance of excess risk bounds  the theory of rank tests needs to be revisited and new problems come
up. The arguments required to deal with risk functionals based on linear rank statistics have been
sketched in [CV07] in a special case. The empirical AUC  known as the Wilcoxon-Mann-Whitney
statistic  is also a U-statistic and this particular dependence structure was extensively exploited in
[CLV08]. In the present paper  we describe the generic structure of linear rank statistics as an or-
thogonal decomposition after projection onto the space of sums of i.i.d. random variables (Section
2). This projection method is the key to all statistical results related to maximizers of such criteria:
consistency  (fast) rates of convergence or model selection. We relate linear rank statistics to perfor-
mance measures relevant for the ranking problem by showing that the target of ranking algorithms

1

correspond to optimal ordering rules in that sense (Section 3). Eventually  we provide some pre-
liminary results in Section 4 for empirical maximizers of performance criteria based on linear rank
statistics with smooth score-generating functions.

2 Criteria based on linear rank statistics

Along the paper  we shall consider the standard binary classiﬁcation model. Take a random pair
(X  Y ) ∈ X ×{−1  +1}  where X is an observation vector in a high dimensional space X ⊂ Rd and
Y is a binary label  and denote by P the distribution of (X  Y ). The dependence structure between X
and Y can be described by conditional distributions. We can consider two descriptions: either P =
(µ  η) where µ is the marginal distribution of X and η is the posterior distribution deﬁned by η(x) =
P{Y = 1 | X = x} for all x ∈ Rd  or else P = (p  G  H) with p = P{Y = 1} being the proportion
of positive instances  G = L(X | Y = +1) the conditional distribution of positive instances and
H = L(X | Y = −1) the conditional distribution of negative instances. A sample of size n of i.i.d.
realizations of this statistical model can be represented as a set of pairs {(Xi  Yi)}1≤i≤n  where
(Xi  Yi) is a copy of (X  Y )  but also as a set {X +
i ) = G 
L(X−
i ) = H  and k + m = n. In this setup  the integers k and m are random  drawn as binomials
of size n and respective parameters p and 1 − p.

m } where L(X +

1   . . .   X−

1   . . .   X +

k   X−

2.1 Motivation

Most of the statistical learning theory has been developed for empirical risk minimizers (ERM) of
sums of i.i.d. random variables. Mathematical results were elaborated with the use of empirical
processes techniques and particularly concentration inequalities for such processes (see [BBL05]
for an overview). This was made possible by the standard assumption that  in a batch setup  for
the usual prediction problems (classiﬁcation  regression or density estimation)  the sample data
{(Xi  Yi)}i=1 ... n are i.i.d. random variables. Another reason is that the error probability in these
problems involves only ”ﬁrst-order” events  depending only on (X1  Y1). In classiﬁcation  for in-
stance  most theoretical developments were focused on the error probability P{Y1 (cid:54)= g(X1)} of a
classiﬁer g : X → {−1  +1}  which is hardly considered in practice because the two populations are
rarely symmetric in terms of proportions or costs. For prediction tasks such as ranking or scoring 
more involved statistics need to be considered  such as the Area Under the ROC curve (AUC)  the
local AUC  the Discounted Cumulative Gain (DCG)  the p-norm push  etc. For instance  the AUC 
a very popular performance measure in various scoring applications  such as medical diagnosis or
credit-risk screening  can be seen as a probability of an ”event of order two”  i.e. depending on
(X1  Y1)  (X2  Y2). In information retrieval  the DCG is the reference measure and it seems to have
a rather complicated statistical structure. The ﬁrst theoretical studies either attempt to get back to
sums of i.i.d. random variables by artiﬁcially reducing the information available (see [AGH+05] 
[Rud06]) or adopt a plug-in strategy ([CZ06]). Our approach is to i) avoid plug-in in order to under-
stand the intimate nature of the learning problem  ii) keep all the information available and provide
the analysis of the full statistic. We shall see that this approach requires the development of new
tools for handling the concentration properties of rank processes  namely collections of rank statis-
tics indexed by classes of functions  which have never been studied before.

2.2 Empirical performance of scoring rules

The learning task on which we focus here is known as the bipartite ranking problem. The goal of
ranking is to order the instances Xi by means of a real-valued scoring function s : X → R   given the
binary labels Yi. We denote by S the set of all scoring functions. It is natural to assume that a good
scoring rule s would assign higher ranks to the positive instances (those for which Yi = +1) than to
the negative ones. The rank of the observation Xi induced by the scoring function s is expressed as
I{s(Xj )≤s(Xi)} and ranges from 1 to n. In the present paper  we consider a

Rank(s(Xi)) =(cid:80)n

particular class of simple (conditional) linear rank statistics inspired from the Wilcoxon statistic.

j=1

2

n(cid:88)

(cid:99)Wn(s) =

I{Yi=+1}φ

(cid:18)Rank(s(Xi))

(cid:19)

  ∀s ∈ S.

Deﬁnition. 1 Let φ : [0  1] → [0  1] be a nondecreasing function. We deﬁne the ”empirical W-
ranking performance measure” as the empirical risk functional

The function φ is called the ”score-generating function” of the ”rank process” {(cid:99)Wn(s)}s∈S.

n + 1

i=1

We refer to the book by Serﬂing [Ser80] for properties and asymptotic theory of rank statistics.
We point out that our deﬁnition does not match exactly with the standard deﬁnition of linear rank
statistics. Indeed  in our case  coefﬁcients of the ranks in the sum are random because they involve

the variables Yi. We will call statistics(cid:99)Wn(s) conditional linear rank statistics.

It is a very natural idea to consider ranking criteria based on ranks. Observe indeed that the perfor-
mance of a given scoring function s is invariant by increasing transforms of the latter  when evaluated
through the empirical W -ranking performance measure. For speciﬁc choices of the score-generating
function φ  we recover the main examples mentioned in the introduction and many relevant criteria
can be accurately approximated by statistics of this form:

related to the empirical version of the AUC (see [CLV08]).

• φ(u) = u - this choice leads to the celebrated Wilcoxon-Mann-Whitney statistic which is
• φ(u) = u · I{u≥u0}  for some u0 ∈ (0  1) - such a score-generating function corresponds
to the local AUC criterion  introduced recently in [CV07]. Such a criterion is of interest
when one wants to focus on the highest ranks.
• φ(u) = up - this is another choice which puts emphasis on high ranks but in a smoother
way than the previous one. This is related to the p-norm push approach taken in [Rud06].
However  we point out that the criterion studied in the latter work relies on a different
deﬁnition of the rank of an observation. Namely  the rank of positive instances among
negative instances (and not in the pooled sample) is used. This choice permits to use
independence which makes the technical part much simpler  at the price of increasing the
variance of the criterion.
• φ(u) = φn(u) = c ((n + 1) u)·I{u≥k/(n+1)} - this corresponds to the DCG criterion in the
bipartite setup  one of the ”gold standard quality measure” in information retrieval  when
grades are binary (namely I{Yi=+1}). The c(i)’s denote the discount factors  c(i) measur-
ing the importance of rank i. The integer k denotes the number of top-ranked instances to
take into account. Notice that  with our indexation  top positions correspond to the largest
ranks and the sequence {ci} should be chosen to be increasing.

2.3 Uniform approximation of linear rank statistics

This subsection describes the main result of the present analysis  which shall serve as the essential
tool for deriving statistical properties of maximizers of empirical W -ranking performance mea-
sures. For a given scoring function s  we denote by Gs  respectively Hs  the conditional cumulative
distribution function of s(X) given Y = +1  respectively Y = −1. With these notations  the uncon-
ditional cdf of s(X) is then Fs = pGs + (1− p)Hs. For averages of non-i.i.d. random variables  the
underlying statistical structure can be revealed by orthogonal projections onto the space of sums of
i.i.d. random variables in many situations. This projection argument was the key for the study of em-
pirical AUC maximization  which involved U-processes  see [CLV08]. In the case of U-statistics 
this orthogonal decomposition is known as the Hoeffding decomposition and the remainder may be
expressed as a degenerate U-statistic  see [Hoe48]. For rank statistics  a similar though less accurate
decomposition can be considered. We refer to [Haj68] for a systematic use of the projection method
for investigating the asymptotic properties of general statistics.

integrable statistic. The r.v. (cid:98)T =(cid:80)n

Lemma. 2 ([Haj68]) Let Z1  . . .   Zn be independent r.v.’s and T = T (Z1  . . .   Zn) be a square
E[T | Zi] − (n − 1)E[Z] is called the H´ajek projection of

T . It satisﬁes

i=1

E[(cid:98)T ] = E[T ] and E[((cid:98)T − T )2] = E[(T − E[T ])2] − E[((cid:98)T − E[(cid:98)T ])2].

3

From the perspective of ERM in statistical learning theory  through the projection method  well-
known concentration results for standard empirical processes may carry over to more complex col-
lections of r.v. such as rank processes  as shown by the next approximation result.

on [0  1]. We set Φs(x) = φ(Fs(s(x))) + p(cid:82) +∞
where (cid:98)Vn(s) =(cid:80)n

Proposition. 3 Consider a score-generating function φ which is twice continuously differentiable
s(x) φ(cid:48)(Fs(u))dGs(u) for all x ∈ X . Let S0 ⊂ S be a
VC major class of functions. Then  we have: ∀s ∈ S0 
I{Yi=+1}Φs(Xi) and (cid:98)Rn(s) = OP(1) as n → ∞ uniformly over s ∈ S .

(cid:99)Wn(s) = (cid:98)Vn(s) + (cid:98)Rn(s) 

i=1

The notation OP(1) means bounded in probability and the integrals are represented in the sense of
the Lebesgue-Stieltjes integral. Details of the proof can be found in the Appendix.

Remark 1 (ON THE COMPLEXITY ASSUMPTION.) On the terminology of major sets and major
classes  we refer to [Dud99]. In the Proposition 3’s proof  we need to control the complexity of
subsets of the form {x ∈ X : s(x) ≤ t}. The stipulated complexity assumption garantees that this
collection of sets indexed by (s  t) ∈ S0 × R forms a VC class.
Remark 2 (ON THE SMOOTHNESS ASSUMPTION.) We point out that it is also possible to deal with
discontinuous score-generating functions as seen in [CV07]. In this case  the lack of smoothness of φ
has to be compensated by smoothness assumptions on the underlying conditional distributions. An-

the score-generating function ψ would be a smooth approximation of φ. Owing to space limitations 
here we only handle the smooth case.

other approach would consist of approximating(cid:99)Wn(s) by the empirical W-ranking criterion where
it as a function of the sampling cdf. Denoting by (cid:98)Fs(x) = n−1(cid:80)n
i )(cid:1)(cid:19)
(cid:0)s(X +
counterpart of Fs(x)  we have:(cid:99)Wn(s) =

An essential hint to the study of the asymptotic behavior of a linear rank statistic consists in rewriting
I{s(Xi)≤x} the empirical

(cid:18) n
n + 1(cid:98)Fs

i=1

.

k(cid:88)

i=1

φ

which may easily shown to converge to E[φ(Fs(s(X)) | Y = +1] as n → ∞  see [CS58].
Deﬁnition. 4 For a given score-generating function φ  we will call the functional

a ”W-ranking performance measure”.

Wφ(s) = E[φ(Fs(s(X)) | Y = +1]  

The following result is a consequence of Proposition 3 and its proof can be found in the Appendix.
Proposition. 5 Let S0 ⊂ S be a VC major class of functions with VC dimension V and φ be a
score-generating function of class C1. Then  as n → ∞  we have with probability one:

|(cid:99)Wn(s) − kWφ(s)| → 0.

1
n

sup
s∈S0

3 Optimality
We introduce the class S∗ of scoring functions obtained as strictly increasing transformations of the
regression function η:

S∗ = { s∗ = T ◦ η | T : [0  1] → R strictly increasing }.

The class S∗ contains the optimal scoring rules for the bipartite ranking problem. The next para-
graphs motivate the use of W -ranking performance measures as optimization criteria for this prob-
lem.

4

3.1 ROC curves

A classical tool for measuring the performance of a scoring rule s is the so-called ROC curve

ROC(s  .) : α ∈ [0  1] (cid:55)→ 1 − Gs ◦ H−1

s (1 − α) 

s (x) = inf{t ∈ R | Hs(t) ≥ x}.

where H−1
In the case where s = η  we will denote
ROC(η  α) = ROC∗(α)  for any α ∈ [0  1]. The set of points (α  β) ∈ [0  1]2 which can be
achieved as (α  ROC(s  α)) for some scoring function s is called the ROC space.
It is a well-known fact that the regression function provides an optimal scoring function for the ROC
curve. This fact relies on a simple application of Neyman-Pearson’s lemma. We refer to [CLV08]
for the details. Using the fact that  for a given scoring function  the ROC curve is invariant by
increasing transformations of the scoring function  we get the following result:
Lemma. 6 For any scoring function s and any α ∈ [0  1]  we have:

∀s∗ ∈ S∗   ROC(s  α) ≤ ROC(s∗  α) .= ROC∗(α) .

The next result states that the set of optimal scoring functions coincides with the set of maximizers
of the Wφ-ranking performance  provided that the score-generating function φ is strictly increasing.

Proposition. 7 Assume that the score-generating function φ is strictly increasing. Then  we have:

Moreover W ∗

φ

.= Wφ(η) = Wφ(s∗) for any s∗ ∈ S∗.

∀s ∈ S   Wφ(s) ≤ Wφ(η) .

Remark 3 (ON PLUG-IN RANKING RULES) Theoretically  a possible approach to ranking is the
plug-in method ([DGL96])  which consists of using an estimate ˆη of the regression function as a
scoring function. As shown by the subsequent bound  when φ is differentiable with a bounded
derivative  when ˆη is close to η in the L1-sense  it leads to a nearly optimal ordering in terms of
W-ranking criterion:

φ − Wφ ((cid:98)η) ≤ (1 − p)||φ(cid:48)||∞E[|(cid:98)η(X) − η(X)|].

W ∗

However  one faces difﬁculties with the plug-in approach when dealing with high-dimensional data 
see [GKKW02])  which provides the motivation for exploring algorithms based on W-ranking per-
formance maximization.

3.2 Connection to hypothesis testing

From the angle embraced in this paper  the ranking problem is tightly related to hypothesis testing.
Denote by X + and X− two r.v. distributed as G and H respectively. As a ﬁrst go  we can reformu-
late the ranking problem as the one of ﬁnding a scoring function s such that s(X−) is stochastically
smaller than s(X +)  which means  for example  that: ∀t ∈ R  P{s(X−) ≥ t} ≤ P{s(X +) ≥ t}.
It is easy to see that the latter statement means that the ROC curve of s dominates the ﬁrst diagonal
of the ROC space. We point out the fact that the ﬁrst diagonal corresponds to nondiscriminating
scoring functions s0 such that Hs0 = Gs0. However  searching for a scoring function s fulﬁlling
this property is generally not sufﬁcient in practice. Heuristically  one would like to pick an s in order
to be as far as possible from the case where ”Gs = Hs”. This requires to specify a certain measure
of dissimilarity between distributions. In this respect  various criteria may be considered such as the
L1-Mallows metric (see the next remark). Indeed  assuming temporarily that s is ﬁxed and consid-
ering the problem of testing similarity vs. dissimilarity between two distributions Hs and Gs based
on two independent samples s(X +
m)  it is well-known that
nonparametric tests based on linear rank statistics have optimality properties. We refer to Chapter
9 in [Ser80] for an overview of rank procedures for testing homogeneity  which may yield relevant
criteria in the ranking context.

1 )  . . .   s(X−

k ) and s(X−

1 )  . . .   s(X +

criterion: AUC(s) =(cid:82) 1

Remark 4 (CONNECTION BETWEEN AUC AND THE L1-MALLOWS METRIC ) Consider the AUC
α=0 ROC(s  α)dα. It is well-known that this criterion may be interpreted as

5

the ”rate of concording pairs”: AUC(s) = P{s(X) < s(X(cid:48)) | Y = −1  Y (cid:48) = +1} where (X  Y )
and (X(cid:48)  Y (cid:48)) denote independent copies. Furthermore  it may be easily shown that

(cid:90) ∞

−∞

AUC(s) =

1
2

+

{Hs(t) − Gs(t)}dF (t) 

where the cdf F may be taken as any linear convex combination of Hs and Gs. Provided that Hs is
stochastically smaller than Gs and that F (dt) is the uniform distribution over (0  1) (this is always
possible  even if it means replacing s by F ◦ s  which leaves the ordering untouched)  the second
term may be identiﬁed as the L1-Mallows distance between Hs and Gs  a well-known probability
metric widely considered in the statistical literature (also known as the L1-Wasserstein metric).

4 A generalization error bound

We now provide a bound on the generalization ability of scoring rules based on empirical maximiza-
tion of W-ranking performance criteria.

Theorem. 8 Set the empirical W -ranking performance maximizer ˆsn = arg maxs∈S(cid:99)Wn(s). Un-

der the same assumptions as in Proposition 3 and assuming in addition that the class of functions
Φs induced by S0 is also a VC major class of functions  we have  for any δ > 0  and with probability
1 − δ:

(cid:114)

(cid:114)log(1/δ)

 

n

φ − Wφ(ˆsn) ≤ c1
W ∗

for some positive constants c1  c2.

V
n

+ c2

The proof is a straightforward consequence from Proposition 3 and it can be found in the Appendix.

5 Conclusion

In this paper  we considered a general class of performance measures for ranking/scoring which can
be described as conditional linear rank statistics. Our overall setup encompasses in particular known
criteria used in medical diagnosis and information retrieval. We have described the statistical nature
√
of such statistics  proved that they ar compatible with optimal scoring functions in the bipartite setup 
and provided a preliminary generalization bound with a
n-rate of convergence. By doing so  we
provided the very results on a class of linear rank processes. Further work is needed to identify a
variance control assumption in order to derive fast rates of convergence and to obtain consistency
under weaker complexity assumptions. Moreover  it is not clear how to formulate convex surrogates
for such functionals yet.

Appendix - Proofs

Proof of Proposition 5

By virtue of the ﬁnite increment theorem  we have:

|(cid:99)Wn(s) − kWφ(s)| ≤ k||φ(cid:48)||∞

sup
s∈S0

1

n + 1

+ sup

(s t)∈S0×R

(cid:32)

(cid:33)
|(cid:98)Fs(t) − Fs(t)|

and the desired result immediately follows from the application of the VC inequality  see Remark 1.

Proof of Proposition 3
Since φ is of class C2  a Taylor expansion at the second order immediately yields:

k(cid:88)

(cid:99)Wn(s) =

i ))) + (cid:98)Bn(s) + (cid:98)Rn(s) 

φ(Fs(s(X +

i=1

6

with

k(cid:88)
(cid:98)Bn(s) =
|(cid:98)Rn(s)| ≤ k(cid:88)

i=1

i=1

(cid:18)Rank(s(X +
(cid:18)Rank(s(X +

n + 1

n + 1

i ))

− Fs(s(X +
i ))

φ(cid:48)(Fs(s(X +

i )))

i ))

− Fs(s(X +
i ))

(cid:19)
(cid:19)2 ||φ(cid:48)(cid:48)||∞.
(cid:19)

E

E

i=1

i=1

(cid:21)

 

i=1

j=1

(cid:19)

n + 1

n + 1

(I) =

(II) =

k(cid:88)

I{Yi=+1}

I{Yi=+1}

This projection may be splitted into two terms:

i (Xi  Yi)] < ∞ for all i ∈ {1  . . .   n}:
i≤n fi(Xi  Yi) such that E[f 2
i ))

Following in the footsteps of [Haj68]  we ﬁrst compute the projection of (cid:98)Bn(s) onto the space Σ of
r.v.’s of the form(cid:80)
PΣ((cid:98)Bn(s)) =
n(cid:88)
n(cid:88)

(cid:21)
have E[Rank(s(Xi)) | s(Xi)] = n(cid:98)Fs(s(Xi)) and  by assumption  sup(s t)∈S×R |(cid:98)Fs(t) − Fs(t)| =

(cid:20)(cid:18)Rank(s(X +
n(cid:88)
(cid:18) 1
(cid:88)

The ﬁrst term is easily handled and may be seen as negligible (it is of order OP(n−1/2))  since we

E[Rank(s(Xi)) | s(Xi)] − Fs(s(Xi))

(cid:20)(cid:18)Rank(s(X +

φ(cid:48)(Fs(s(Xi))) | s(Xj)  Yj

φ(cid:48)(Fs(s(Xi))) | Xj  Yj

− Fs(s(X +
i ))

− Fs(s(X +
i ))

φ(cid:48)(Fs(s(Xi))) 

I{Yi=+1}

OP(n−1/2) (see Remark 1). Up to an additive term of order OP(1) uniformly over s ∈ S  the second
term may be rewritten as

E(cid:2)I{s(Xj )≤s(Xi)}φ(cid:48)(Fs(s(Xi))) | s(Xj)  Yj
(cid:90) ∞

n(cid:88)
n(cid:88)
φ(cid:48)(Fs(u))dGs(u).
I{Yi=+1}(cid:82) ∞
s(Xi) φ(cid:48)(Fs(u))dGs(u)/(n + 1) ≤ supu∈[0 1] φ(cid:48)(t) and k/(n + 1) ∼ p  we get
k(cid:88)

φ(cid:48)(Fs(u))dGs(u) − 1
n + 1

that  uniformly over s ∈ S0:

As(cid:80)n

(cid:90) ∞

I{Yi=+1}

(cid:88)

n(cid:88)

(II) =

n + 1

n + 1

n + 1

(cid:19)

i ))

1

k

j=1

s(Xj )

(cid:3)

j(cid:54)=i

j(cid:54)=i

s(Xi)

=

i=1

i=1

i=1

.

The term (cid:98)Rn(s) is negligible  since  up to the multiplicative constant ||φ(cid:48)(cid:48)||∞  it is bounded by

i=1

φ(Fs(s(X +

i ))) + PΣ((cid:98)Bn(s))) = (cid:98)Vn(s) + OP(1) as n → ∞.
2 .

2Fs(s(Xi)) +(cid:88)
2

{I{s(Xk)≤s(Xi)} − Fs(s(Xi))}

| s(Xi)

k(cid:54)=i

i=1

E

k(cid:54)=i

 =(cid:88)

{I{s(Xk)≤s(Xi)} − Fs(s(Xi))}

As Fs is bounded by 1  it sufﬁces to observe that for all i:


(cid:88)
Eventually  one needs to evaluate the accuracy of the approximation yield by the projection (cid:98)Bn(s)−
{PΣ((cid:98)Bn(s)) − (n − 1)E[(cid:98)Bn(s)]}. Write  for all s ∈ S0 
(cid:18) 1

Bounding the variance of the binomial r.v. E[I{s(Xk) ≤ s(Xi)} − Fs(s(Xi))})2 | s(Xi)] by 1/4 
one ﬁnally gets that ˆRn(s) is of order OP(1) uniformly over s ∈ S0.

E(cid:2)(I{s(Xk)≤s(Xi)} − Fs(s(Xi))})2 | s(Xi)(cid:3) .

(cid:19)

k(cid:54)=i

n(cid:88)

(cid:98)Bn(s) = n(cid:98)Un(s) +

φ(cid:48)(Fs(s(Xi))) 

− Fs(s(Xi))

I{Yi=+1}

n(cid:88)

E

1

(n + 1)2

i=1

n + 1

7

Hence  we have

i(cid:54)=j qs((Xi  Yi)  (Xj  Yj))/(n(n + 1)) is a U-statistic with kernel:
qs((x  y)  (x(cid:48)  y(cid:48))) = I{y=+1} · I{s(x(cid:48))≤s(x)} · φ(cid:48)(Fs(s(x))).

where Un(s) =(cid:80)
n−1(cid:16)(cid:98)Bn(s) − {PΣ((cid:98)Bn(s)) − (n − 1)E[(cid:98)Bn(s)]}(cid:17)
= (cid:98)Un(s) − {PΣ((cid:98)Un(s)) − (n − 1)E[(cid:98)Un(s)]}
(cid:98)Un(s). Now  given that sups∈S0 ||qs||∞ < ∞  it follows from Theorem 11 in [CLV08] for instance 

which actually corresponds to the degenerate part of the Hoeffding decomposition of the U-statistic

combined with the basic symmetrization device of the kernel qs  that

|(cid:98)Un(s) − {PΣ((cid:98)Un(s)) − (n − 1)E[(cid:98)Un(s)]}| = OP(n−1) as n → ∞ 

sup
s∈S0

which concludes the proof.

Proof of Proposition 7
Using the decomposition Fs = pGs + (1 − p)Hs  we are led to the following expression:

pWφ(s) =

φ(u) du − (1 − p)E[φ(Fs(s(X))) | Y = −1].

(cid:90) 1

0
Then  using a change of variable:

E[φ(Fs(s(X))) | Y = −1] =

(cid:90) 1

0

φ(p(1 − ROC(s  α)) + (1 − p)(1 − α)) dα .

It is now easy to conclude since φ is increasing (by assumption) and because of the optimality of
elements of S∗ in the sense of Lemma 6.

Proof of Theorem 8

Observe that  by virtue of Proposition 3 

φ − Wφ(ˆsn) ≤ 2 sup
W ∗
s∈S0

|(cid:99)Wn(s)/k − Wφ(s)| ≤ 2

|(cid:98)Vn(s) − kWφ(s)| + OP(n−1) 

sup
s∈S0

k

and the desired bound derives from the VC inequality applied to the sup term  noticing that it follows
from our assumptions that {(x  y) (cid:55)→ I{y=+1}Φs(x)}s∈S0 is a VC class of functions.

[CLV08]

[BBL05]

References
[AGH+05] S. Agarwal  T. Graepel  R. Herbrich  S. Har-Peled  and D. Roth. Generalization bounds
for the area under the ROC curve. Journal of Machine Learning Research  6:393–425 
2005.
S. Boucheron  O. Bousquet  and G. Lugosi. Theory of Classiﬁcation: A Survey of
Some Recent Advances. ESAIM: Probability and Statistics  9:323–375  2005.
S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical risk minimization of
U-statistics. The Annals of Statistics  36(2):844–874  2008.
J. Chernoff and Savage. Asymptotic normality and efﬁciency of certain non parametric
test statistics. Ann. Math. Stat.  29:972–994  1958.
S. Cl´emenc¸on and N. Vayatis. Ranking the best instances. Journal of Machine Learn-
ing Research  8:2671–2699  2007.
D. Cossock and T. Zhang. Subset ranking using regression. In H.U. Simon and G. Lu-
gosi  editors  Proceedings of COLT 2006  volume 4005 of Lecture Notes in Computer
Science  pages 605–619  2006.
L. Devroye  L. Gy¨orﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition.
Springer  1996.

[DGL96]

[CV07]

[CZ06]

[CS58]

8

[Dud99]
[GKKW02] L. Gy¨orﬁ  M. K¨ohler  A. Krzyzak  and H. Walk. A Distribution-Free Theory of Non-

R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press  1999.

[Haj68]

[Hoe48]

[HS67]
[Rud06]

[Ser80]

parametric Regression. Springer  2002.
J. Hajek. Asymptotic normality of simple linear rank statistics under alternatives. Ann.
Math. Stat.  39:325–346  1968.
W. Hoeffding. A class of statistics with asymptotically normal distribution. Ann. Math.
Stat.  19:293–325  1948.
J. H´ajek and Z. Sid´ak. Theory of Rank Tests. Academic Press  1967.
C. Rudin. Ranking with a P-Norm Push.
In H.U. Simon and G. Lugosi  editors 
Proceedings of COLT 2006  volume 4005 of Lecture Notes in Computer Science  pages
589–604  2006.
R.J. Serﬂing. Approximation theorems of mathematical statistics. John Wiley & Sons 
1980.

9

,Subhaneil Lahiri
Surya Ganguli