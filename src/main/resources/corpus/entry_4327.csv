2017,Polynomial time algorithms for dual volume sampling,We study dual volume sampling  a method for selecting k columns from an n*m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume spanned by the rows of the induced submatrix. This method was proposed by Avron and Boutsidis (2013)  who showed it to be a promising method for column subset selection and its multiple applications. However  its wider adoption has been hampered by the lack of polynomial time sampling algorithms. We remove this hindrance by developing an exact (randomized) polynomial time sampling algorithm as well as its derandomization. Thereafter  we study dual volume sampling via the theory of real stable polynomials and prove that its distribution satisfies the “Strong Rayleigh” property. This result has numerous consequences  including a provably fast-mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners. This sampler is closely related to classical algorithms for popular experimental design methods that are to date lacking theoretical analysis but are known to empirically work well.,Polynomial time algorithms for dual volume sampling

Chengtao Li

MIT

ctli@mit.edu

Stefanie Jegelka

MIT

stefje@csail.mit.edu

Suvrit Sra

MIT

suvrit@mit.edu

Abstract

We study dual volume sampling  a method for selecting k columns from an n ⇥ m
short and wide matrix (n  k  m) such that the probability of selection is propor-
tional to the volume spanned by the rows of the induced submatrix. This method
was proposed by Avron and Boutsidis (2013)  who showed it to be a promising
method for column subset selection and its multiple applications. However  its
wider adoption has been hampered by the lack of polynomial time sampling algo-
rithms. We remove this hindrance by developing an exact (randomized) polynomial
time sampling algorithm as well as its derandomization. Thereafter  we study
dual volume sampling via the theory of real stable polynomials and prove that its
distribution satisﬁes the “Strong Rayleigh” property. This result has numerous
consequences  including a provably fast-mixing Markov chain sampler that makes
dual volume sampling much more attractive to practitioners. This sampler is closely
related to classical algorithms for popular experimental design methods that are to
date lacking theoretical analysis but are known to empirically work well.

Introduction

1
A variety of applications share the core task of selecting a subset of columns from a short  wide
matrix A with n rows and m > n columns. The criteria for selecting these columns typically aim at
preserving information about the span of A while generating a well-conditioned submatrix. Classical
and recent examples include experimental design  where we select observations or experiments [38];
preconditioning for solving linear systems and constructing low-stretch spanning trees (here A
is a version of the node-edge incidence matrix and we select edges in a graph) [6  4]; matrix
approximation [11  13  24]; feature selection in k-means clustering [10  12]; sensor selection [25]
and graph signal processing [14  41].
In this work  we study a randomized approach that holds promise for all of these applications. This
approach relies on sampling columns of A according to a probability distribution deﬁned over its
submatrices: the probability of selecting a set S of k columns from A  with n  k  m  is

P (S; A) / det(ASA>S ) 

(1.1)
where AS is the submatrix consisting of the selected columns. This distribution is reminiscent of
volume sampling  where k < n columns are selected with probability proportional to the determinant
det(A>S AS) of a k ⇥ k matrix  i.e.  the squared volume of the parallelepiped spanned by the selected
columns. (Volume sampling does not apply to k > n as the involved determinants vanish.) In contrast 
P (S; A) uses the determinant of an n ⇥ n matrix and uses the volume spanned by the rows formed
by the selected columns. Hence we refer to P (S; A)-sampling as dual volume sampling (DVS).

Contributions. Despite the ostensible similarity between volume sampling and DVS  and despite
the many practical implications of DVS outlined below  efﬁcient algorithms for DVS are not known
and were raised as open questions in [6]. In this work  we make two key contributions:

– We develop polynomial-time randomized sampling algorithms and their derandomization for

DVS. Surprisingly  our proofs require only elementary (but involved) matrix manipulations.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

– We establish that P (S; A) is a Strongly Rayleigh measure [8]  a remarkable property that
captures a speciﬁc form of negative dependence. Our proof relies on the theory of real stable
polynomials  and the ensuing result implies a provably fast-mixing  practical MCMC sampler.
Moreover  this result implies concentration properties for dual volume sampling.

In parallel with our work  [16] also proposed a polynomial time sampling algorithm that works
efﬁciently in practice. Our work goes on to further uncover the hitherto unknown “Strong Rayleigh”
property of DVS  which has important consequences  including those noted above.
1.1 Connections and implications.
The selection of k  n columns from a short and wide matrix has many applications. Our algorithms
for DVS hence have several implications and connections; we note a few below.
Experimental design. The theory of optimal experiment design explores several criteria for selecting
the set of columns (experiments) S. Popular choices are
J(AS) = kA†SkF = k(ASA>S )1kF (A-optimal design)  
S 2 argminS✓{1 ... m}J(AS)  with
J(AS) = kA†Sk2 (E-optimal design)   J(AS) =  log det(ASA>S ) (D-optimal design).
(1.2)
Here  A† denotes the Moore-Penrose pseudoinverse of A  and the minimization ranges over all S
such that AS has full row rank n. A-optimal design  for instance  is statistically optimal for linear
regression [38].
Finding an optimal solution for these design problems is NP-hard; and most discrete algorithms use
local search [33]. Avron and Boutsidis [6  Theorem 3.1] show that dual volume sampling yields an
approximation guarantee for both A- and E-optimal design: if S is sampled from P (S; A)  then

EhkA†Sk2
Fi 

m  n + 1
k  n + 1 kA†k2

F ; EhkA†Sk2

2i ✓1 +

n(m  k)

k  n + 1◆kA†k2

2.

(1.3)

Avron and Boutsidis [6] provide a polynomial time sampling algorithm only for the case k = n. Our
algorithms achieve the bound (1.3) in expectation  and the derandomization in Section 2.3 achieves
the bound deterministically. Wang et al. [43] recently (in parallel) achieved approximation bounds
for A-optimality via a different algorithm combining convex relaxation and a greedy method. Other
methods include leverage score sampling [30] and predictive length sampling [45].
Low-stretch spanning trees and applications. Objectives 1.2 also arise in the construction of
low-stretch spanning trees  which have important applications in graph sparsiﬁcation  preconditioning
and solving symmetric diagonally dominant (SDD) linear systems [40]  among others [18]. In the
node-edge incidence matrix ⇧ 2 Rn⇥m of an undirected graph G with n nodes and m edges  the
column corresponding to edge (u  v) ispw(u  v)(eu  ev). Let ⇧= U ⌃Y be the SVD of ⇧ with
Y 2 Rn1⇥m. The stretch of a spanning tree T in G is then given by StT (G) = kY 1
F [6]. In
T k2
those applications  we hence search for a set of edges with low stretch.
Network controllability. The problem of sampling k  n columns in a matrix also arises in
network controllability. For example  Zhao et al. [44] consider selecting control nodes S (under
certain constraints) over time in complex networks to control a linear time-invariant network. After
transforming the problem into a column subset selection problem from a short and wide controllability
matrix  the objective becomes essentially an E-optimal design problem  for which the authors use
greedy heuristics.
Notation. From a matrix A 2 Rn⇥m with m  n columns  we sample a set S ✓ [m] of k columns
(n  k  m)  where [m] := {1  2  . . .   m}. We denote the singular values of A by {i(A)}n
i=1 
in decreasing order. We will assume A has full row rank r(A) = n  so n(A) > 0. We also
assume that r(AS) = r(A) = n for every S ✓ [m] where |S| n. By ek(A)  we denote the k-th
elementary symmetric polynomial of A  i.e.  the k-th coefﬁcient of the characteristic polynomial
det(I  A) =PN

2 Polynomial-time Dual Volume Sampling
We describe in this section our method to sample from the distribution P (S; A). Our ﬁrst method
relies on the key insight that  as we show  the marginal probabilities for DVS can be computed in
polynomial time. To demonstrate this  we begin with the partition function and then derive marginals.

j=0(1)jej(A)Nj.

2

2.1 Marginals
The partition function has a conveniently simple closed form  which follows from the Cauchy-Binet
formula and was also derived in [6].
Lemma 1 (Partition Function [6]). For A 2 Rn⇥m with r(A) = n and n | S| = k  m  we have

ZA :=X|S|=k S✓[m]

det(ASA>S ) =✓m  n

k  n◆ det(AA>).

Next  we will need the marginal probability P (T ✓ S; A) = PS:T✓S P (S; A) that a given set
T ✓ [m] is a subset of the random set S. In the following theorem  the set Tc = [m] \ T denotes the
(set) complement of T   and Q? denotes the orthogonal complement of Q.
Theorem 2 (Marginals). Let T ✓ [m]  |T| k  and "> 0. Let AT = Q⌃V > be the singular value
decomposition of AT where Q 2 Rn⇥r(AT )  and Q? 2 Rn⇥(nr(AT )). Further deﬁne the matrices

B = (Q?)>ATc 2 R(nr(AT ))⇥(m|T|) 
. . .

1 (AT )+"

0
1p2
...

1p2
0
...

C =2664
P (T ✓ S; A) = hQr(AT )

. . .
...

3775 Q>ATc 2 Rr(AT )⇥(m|T|).
j (B)i ⇥ 
i (AT )i ⇥hQr(B)

j=1 2

ZA

.

Let QBdiag(2

i (B))Q>B be the eigenvalue decomposition of B>B where QB 2 R|Tc|⇥r(B). More-
over  let W > =⇥ITc; C>⇤ and = ek|T|r(B)(W ((Q?B)>Q?B)W >). Then the marginal probabil-

ity of T in DVS is

2 (AT )+"

i=1 2

We prove Theorem 2 via a perturbation argument that connects DVS to volume sampling. Speciﬁcally 
observe that for ✏> 0 and |S| n it holds that

det(ASA>S + "In) = "nk det(A>S AS + "Ik) = "nk det  ASp"(Im)S> ASp"(Im)S! . (2.1)

Carefully letting ✏ ! 0 bridges volumes with “dual” volumes. The technical remainder of the proof
further relates this equality to singular values  and exploits properties of characteristic polynomials. A
similar argument yields an alternative proof of Lemma 1. We show the proofs in detail in Appendix A
and B respectively.
Complexity. The numerator of P (T ✓ S; A) in Theorem 2 requires O(mn2) time to compute the
ﬁrst term  O(mn2) to compute the second and O(m3) to compute the third. The denominator takes
O(mn2) time  amounting in a total time of O(m3) to compute the marginal probability.
2.2 Sampling
The marginal probabilities derived above directly yield a polynomial-time exact DVS algorithm.
Instead of k-sets  we sample ordered k-tuples !S = (s1  . . .   sk) 2 [m]k. We denote the k-tuple
variant of the DVS distribution by !P (·; A):
!P ((sj = ij)k
!P (sj = ij|s1 = i1  . . .   sj1 = ij1; A).
Sampling !S is now straightforward. At the jth step we sample sj via !P (sj = ij|s1 =
i1  . . .   sj1 = ij1; A); these probabilities are easily obtained from the marginals in Theorem 2.
Corollary 3. Let T = {i1  . . .   it1}  and P (T ✓ S; A) as in Theorem 2. Then 
P (T [{ i}✓ S; A)
As a result  it is possible to draw an exact dual volume sample in time O(km4).
The full proof may be found in the appendix. The running time claim follows since the sampling
algorithm invokes O(mk) computations of marginal probabilities  each costing O(m3) time.

!P (st = i; A|s1 = i1  . . .   st1 = it1) =

P ({i1  . . .   ik}; A) =Yk

(k  t + 1) P (T ✓ S; A)

j=1; A) =

1
k!

j=1

.

3

Remark A potentially more efﬁcient approximate algorithm could be derived by noting the rela-
tions between volume sampling and DVS. Speciﬁcally  we add a small perturbation to DVS as in
Equation 2.1 to transform it into a volume sampling problem  and apply random projection for more
efﬁcient volume sampling as in [17]. Please refer to Appendix C for more details.

F | s1 = i1  . . .   st1 = it1i = Pn

2.3 Derandomization
Next  we derandomize the above sampling algorithm to deterministically select a subset that satisﬁes
the bound (1.3) for the Frobenius norm  thereby answering another question in [6]. The key insight
for derandomization is that conditional expectations can be computed in polynomial time  given the
marginals in Theorem 2:
Corollary 4. Let (i1  . . .   it1) 2 [m]t1 be such that the marginal distribution satisﬁes !P (s1 =
i1  . . .   st1 = it1; A) > 0. The conditional expectation can be expressed as
EhkA†Sk2
j=1 P 0({i1  . . .   it1}✓ S|S ⇠ P (S; A[n]\{j}))
where P 0 are the unnormalized marginal distributions  and it can be computed in O(nm3) time.
We show the full derivation in Appendix D.
Corollary 4 enables a greedy derandomization procedure. Starting with the empty tuple !S 0 = ;  in
F | (s1  . . .   si) = !S i1 j] and append
the ith iteration  we greedily select j⇤ 2 argmaxj E[kA†S[jk2
it to our selection: !S i = !S i1  j. The ﬁnal set is the non-ordered version Sk of !S k. Theorem 5
shows that this greedy procedure succeeds  and implies a deterministic version of the bound (1.3).
Theorem 5. The greedy derandomization selects a column set S satisfying
n(m  n + 1)
k  n + 1 kA†k2

P 0({i1  . . .   it1}✓ S|S ⇠ P (S; A))

m  n + 1
k  n + 1 kA†k2

kA†Sk2

F 

 

F ;

kA†Sk2

2 

2.

In the proof  we construct a greedy algorithm. In each iteration  the algorithm computes  for each
column that has not yet been selected  the expectation conditioned on this column being included
in the current set. Then it chooses the element with the lowest conditional expectation to actually
be added to the current set. This greedy inclusion of elements will only decrease the conditional
expectation  thus retaining the bound in Theorem 5. The detailed proof is deferred to Appendix E.
Complexity. Each iteration of the greedy selection requires O(nm3) to compute O(m) conditional
expectations. Thus  the total running time for k iterations is O(knm4). The approximation bound for
the spectral norm is slightly worse than that in (1.3)  but is of the same order if k = O(n).
3 Strong Rayleigh Property and Fast Markov Chain Sampling

Next  we investigate DVS more deeply and discover that it possesses a remarkable structural property 
namely  the Strongly Rayleigh (SR) [8] property. This property has proved remarkably fruitful
in a variety of recent contexts  including recent progress in approximation algorithms [23]  fast
sampling [2  27]  graph sparsiﬁcation [22  39]  extensions to the Kadison-Singer problem [1]  and
certain concentration of measure results [37]  among others.
For DVS  the SR property has two major consequences: it leads to a fast mixing practical MCMC
sampler  and it implies results on concentration of measure.

Strongly Rayleigh measures. SR measures were introduced in the landmark paper of Borcea
et al. [8]  who develop a rich theory of negatively associated measures. In particular  we say that a

probability measure µ : 2[n] ! R+ is negatively associated ifR F dµR Gdµ R F Gdµ for F  G

increasing functions on 2[n] with disjoint support. This property reﬂects a “repelling” nature of µ  a
property that occurs more broadly across probability  combinatorics  physics  and other ﬁelds—see
[36  8  42] and references therein. The negative association property turns out to be quite subtle in
general; the class of SR measures captures a strong notion of negative association and provides a
framework for analyzing such measures.

4

Speciﬁcally  SR measures are deﬁned via their connection to real stable polynomials [36  8  42]. A
multivariate polynomial f 2 C[z] where z 2 Cm is called real stable if all its coefﬁcients are real and
f (z) 6= 0 whenever Im(zi) > 0 for 1  i  m. A measure is called an SR measure if its multivariate
generating polynomial fµ(z) := PS✓[n] µ(S)Qi2S zi is real stable. Notable examples of SR
measures are Determinantal Point Processes [31  29  9  26]  balanced matroids [19  37]  Bernoullis
conditioned on their sum  among others. It is known (see [8  pg. 523]) that the class of SR measures
is exponentially larger than the class of determinantal measures.

3.1 Strong Rayleigh Property of DVS
Theorem 6 establishes the SR property for DVS and is the main result of this section. Here and in the

following  we use the notation zS =Qi2S zi.
Theorem 6. Let A 2 Rn⇥m and n  k  m. Then the multiafﬁne polynomial
det(ASA>S )Yi2S

zi = X|S|=k S✓[m]

p(z) := X|S|=k S✓[m]

is real stable. Consequently  P (S; A) is an SR measure.

det(ASA>S )zS 

(3.1)

The proof of Theorem 6 relies on key properties of real stable polynomials and SR measures
established in [8]. Essentially  the proof demonstrates that the generating polynomial of P (Sc; A)
can be obtained by applying a few carefully chosen stability preserving operations to a polynomial
that we know to be real stable. Stability  although easily destroyed  is closed under several operations
noted in the important proposition below.
Proposition 7 (Prop. 2.1 [8]). Let f : Cm ! C be a stable polynomial. The following properties pre-
serve stability: (i) Substitution: f (µ  z2  . . .   zm) for µ 2 R; (ii) Differentiation: @Sf (z1  . . .   zm)
for any S ✓ [m]; (iii) Diagonalization: f (z  z  z3 . . .   zm) is stable  and hence f (z  z  . . .   z); and
(iv) Inversion: z1 ··· znf (z1
In addition  we need the following two propositions for proving Theorem 6.
Proposition 8 (Prop. 2.4 [7]). Let B be Hermitian  z 2 Cm and Ai (1  i  m) be Hermitian
semideﬁnite matrices. Then  the following polynomial is stable:

n ).
1   . . .   z1

f (z) := det(B +Xi

ziAi).

(3.2)

i=1) be a diagonal matrix. Using the Cauchy-Binet identity we have

Proposition 9. For n | S| m and L := A>A  we have det(ASA>S ) = en(LS S).
Proof. Let Y = Diag([yi]m
det((AY ): T ) det((A>)T :) =X|T|=n T✓[m]
det(AY A>) =X|T|=n T✓[m]
Thus  when Y = IS  the (diagonal) indicator matrix for S  we obtain AY A> = ASA>S . Conse-
quently  in the summation above only terms with T ✓ S survive  yielding
det(A>T AT ) = X|T|=n T✓S
det(ASA>S ) = X|T|=n T✓S

det(LT T ) = en(LS S).

det(A>T AT )yT .

We are now ready to sketch the proof of Theorem 6.

Proof. (Theorem 6). Notationally  it is more convenient to prove that the “complement” polynomial

pc(z) :=P|S|=k S✓[m] det(ASA>S )zSc is stable; subsequently  an application of Prop. 7-(iv) yields

stability of (3.1). Using matrix notation W = Diag(w1  . . .   wm)  Z = Diag(z1  . . .   zm)  our
starting stable polynomial (this stability follows from Prop. 8) is

which can be expanded as

h(z  w) =XS✓[m]

h(z  w) := det(L + W + Z)  w 2 Cm  z 2 Cm 

det(WS + LS)zSc =XS✓[m]⇣XT✓S

wS\T det(LT T )⌘ zSc.

5

(3.3)

Thus  h(z  w) is real stable in 2m variables  indexed below by S and R where R := S\T . Instead of
the form above  We can sum over S  R ✓ [m] but then have to constrain the support to the case when

Next  we truncate polynomial (3.3) at degree (mk)+(kn) = mn by restricting |Sc[R| = mn.
By [8  Corollary 4.18] this truncation preserves stability  whence

Sc \ T = ; and Sc \ R = ;. In other words  we may write (using Iverson-bracketsJ·K)
h(z  w) = XS R✓[m]JSc \ R = ; ^ Sc \ T = ;K det(LT T )zScwR.
H(z  w) := XS R✓[m]
|Sc[R|=mnJSc \ R = ;K det(LS\R S\R)zScwR 
)) = XS R✓[m]
|Sc[R|=mnJSc \ R = ;K det(LS\R S\R)zScy|R|
}
det(LT T )⌘y|S||T|zSc = XS✓[m]
= XS✓[m]⇣X|T|=n T✓S

is also stable. Using Prop. 7-(iii)  setting w1 = . . . = wm = y retains stability; thus

g(z  y) : = H(z  (y  y  . . .   y

en(LS S)y|S|nzSc 

|

m times

{z

is also stable. Next  differentiating g(z  y)  k n times with respect to y and evaluating at 0 preserves
stability (Prop. 7-(ii) and (i)). In doing so  only terms corresponding to |S| = k survive  resulting in
@kn
det(ASA>S )zSc 

= (k  n)! X|S|=k S✓[m]

en(LS S)zSc = (k  n)! X|S|=k S✓[m]

which is just pc(z) (up to a constant); here  the last equality follows from Prop. 9. This establishes
stability of pc(z) and hence of p(z). Since p(z) is in addition multiafﬁne  it is the generating
polynomial of an SR measure  completing the proof.

@y kn g(z  y)y=0

Implications: MCMC

3.2
The SR property of P (S; A) established in Theorem 6 implies a fast mixing Markov chain for
sampling S. The states for the Markov chain are all sets of cardinality k. The chain starts with a
randomly-initialized active set S  and in each iteration we swap an element sin 2 S with an element
sout /2 S with a speciﬁc probability determined by the probability of the current and proposed set.
The stationary distribution of this chain is the one induced by DVS  by a simple detailed-balance
argument. The chain is shown in Algorithm 1.

Algorithm 1 Markov Chain for Dual Volume Sampling

Input: A 2 Rn⇥m the matrix of interest  k the target cardinality  T the number of steps
Output: S ⇠ P (S; A)
Initialize S ✓ [m] such that |S| = k and det(ASA>S ) > 0
for i = 1 to T do

draw b 2{ 0  1} uniformly
if b = 1 then
Pick sin 2 S and sout 2 [m]\S uniformly randomly
q(sin  sout  S) minn1  det(AS[{sout}\{sin}A>S[{sout}\{sin}
S S [{ sout}\{sin} with probability q(sin  sout  S)

)/ det(ASA>S )o

end if
end for

The convergence of the markov chain is measured via its mixing time: The mixing time of the chain
indicates the number of iterations t that we must perform (starting from S0) before we can consider
St as an approximately valid sample from P (S; A). Formally  if S0(t) is the total variation distance
between the distribution of St and P (S; A) after t steps  then

⌧S0(") := min{t : S0(t0)  "  8t0  t}

6

is the mixing time to sample from a distribution "-close to P (S; A) in terms of total variation distance.
We say that the chain mixes fast if ⌧S0 is polynomial in the problem size.
The fast mixing result for Algorithm 1 is a corollary of Theorem 6 combined with a recent result
of [3] on fast-mixing Markov chains for homogeneous SR measures. Theorem 10 states this precisely.
Theorem 10 (Mixing time). The mixing time of Markov chain shown in Algorithm 1 is given by

⌧S0(")  2k(m  k)(log P (S0; A)1 + log "1).

Proof. Since P (S; A) is k-homogeneous SR by Theorem 6  the chain constructed for sampling S
following that in [3] mixes in ⌧S0(")  2k(m  k)(log P (S0; A)1 + log "1) time.
Implementation. To implement Algorithm 1 we need to compute the transition probabilities
q(sin  sout  S). Let T = S\{sin} and assume r(AT ) = n. By the matrix determinant lemma we have
the acceptance ratio

det(AS[{sout}\{sin}A>S[{sout}\{sin}

det(ASA>S )

)

=

(1 + A>
(1 + A>

{sout}
{sin}

(AT A>T )1A{sout})
(AT A>T )1A{sin})

.

Thus  the transition probabilities can be computed in O(n2k) time. Moreover  one can further
accelerate this algorithm by using the quadrature techniques of [28] to compute lower and upper
bounds on this acceptance ratio to determine early acceptance or rejection of the proposed move.

Initialization. A remaining question is initialization.
Since the mixing time involves
log P (S0; A)1  we need to start with S0 such that P (S0; A) is sufﬁciently bounded away from 0.
We show in Appendix F that by a simple greedy algorithm  we are able to initialize S such that

log P (S; A)1  log(2nk!m
eO(k3n2m)  which is linear in the size of data set m and is efﬁcient when k is not too large.

k) = O(k log m)  and the resulting running time for Algorithm 1 is

3.3 Further implications and connections
Concentration. Pemantle and Peres [37] show concentration results for strong Rayleigh measures.
As a corollary of our Theorem 6 together with their results  we directly obtain tail bounds for DVS.

Algorithms for experimental design. Widely used  classical algorithms for ﬁnding an approximate
optimal design include Fedorov’s exchange algorithm [20  21] (a greedy local search) and simulated
annealing [34]. Both methods start with a random initial set S  and greedily or randomly exchange
a column i 2 S with a column j /2 S. Apart from very expensive running times  they are known
to work well in practice [35  43]. Yet so far there is no theoretical analysis  or a principled way of
determining when to stop the greedy search.
Curiously  our MCMC sampler is essentially a randomized version of Fedorov’s exchange method.
The two methods can be connected by a uniﬁed  simulated annealing view  where we deﬁne
P (S; A) / exp{log det(ASA>S )/} with temperature parameter . Driving  to zero essen-
tially recovers Fedorov’s method  while our results imply fast mixing for  = 1  together with
approximation guarantees. Through this lens  simulated annealing may be viewed as initializing
Fedorov’s method with the fast-mixing sampler. In practice  we observe that letting < 1 improves
the approximation results  which opens interesting questions for future work.

4 Experiments

We report selection performance of DVS on real regression data (CompAct  CompAct(s)  Abalone
and Bank32NH1) for experimental design. We use 4 000 samples from each dataset for estima-
tion. We compare against various baselines  including uniform sampling (Unif)  leverage score
sampling (Lev) [30]  predictive length sampling (PL) [45]  the sampling (Smpl)/greedy (Greedy)
selection methods in [43] and Fedorov’s exchange algorithm [20]. We initialize the MCMC sampler
with Kmeans++ [5] for DVS and run for 10 000 iterations  which empirically yields selections that are

1http://www.dcc.fc.up.pt/?ltorgo/Regression/DataSets.html

7

sufﬁciently good. We measure performances via (1) the prediction error ky  X ˆ↵k  and 2) running
times. Figure 1 shows the results for these three measures with sample sizes k varying from 60 to
200. Further experiments (including for the interpolation < 1)  may be found in the appendix.

r
o
r
r

E

0.4

0.35

0.3

0.25

0.2

0.15

Prediction Error

Unif
Lev
PL
Smpl
Greedy
DVS
Fedorov

s
d
n
o
c
e
S

100

150

k

200

100

80

60

40

20

0

Running Time

r
o
r
r

E

100

150

k

200

0.28

0.26

0.24

0.22

0.2

0.18

Time-Error Trade-off

0

10

20
30
Seconds

40

50

Figure 1: Results on the CompAct(s) dataset. Results are the median of 10 runs  except Greedy and
Fedorov. Note that Unif  Lev  PL and DVS use less than 1 second to ﬁnish experiments.

In terms of prediction error  DVS performs well and is comparable with Lev. Its strength compared
to the greedy and relaxation methods (Smpl  Greedy  Fedorov) is running time  leading to good
time-error tradeoffs. These tradeoffs are illustrated in Figure 1 for k = 120.
In other experiments (shown in Appendix G) we observed that in some cases  the optimization and
greedy methods (Smpl  Greedy  Fedorov) yield better results than sampling  however with much
higher running times. Hence  given time-error tradeoffs  DVS may be an interesting alternative in
situations where time is a very limited resource and results are needed quickly.
5 Conclusion
In this paper  we study the problem of DVS and develop an exact (randomized) polynomial time
sampling algorithm as well as its derandomization. We further study dual volume sampling via
the theory of real-stable polynomials and prove that its distribution satisﬁes the “Strong Rayleigh”
property. This result has remarkable consequences  especially because it implies a provably fast-
mixing Markov chain sampler that makes dual volume sampling much more attractive to practitioners.
Finally  we observe connections to classical  computationally more expensive experimental design
methods (Fedorov’s method and SA); together with our results here  these could be a ﬁrst step towards
a better theoretical understanding of those methods.

Acknowledgement
This research was supported by NSF CAREER award 1553284  NSF grant IIS-1409802  DARPA
grant N66001-17-1-4039  DARPA FunLoL grant (W911NF-16-1-0551) and a Siebel Scholar Fellow-
ship. The views  opinions  and/or ﬁndings contained in this article are those of the author and should
not be interpreted as representing the ofﬁcial views or policies  either expressed or implied  of the
Defense Advanced Research Projects Agency or the Department of Defense.

References
[1] N. Anari and S. O. Gharan. The Kadison-Singer problem for strongly Rayleigh measures and

applications to asymmetric TSP. arXiv:1412.1143  2014.

[2] N. Anari and S. O. Gharan. Effective-resistance-reducing ﬂows and asymmetric TSP. In IEEE

Symposium on Foundations of Computer Science (FOCS)  2015.

[3] N. Anari  S. O. Gharan  and A. Rezaei. Monte Carlo Markov chain algorithms for sampling
strongly Rayleigh distributions and determinantal point processes. In COLT  pages 23–26  2016.
[4] M. Arioli and I. S. Duff. Preconditioning of linear least-squares problems by identifying basic

variables. SIAM J. Sci. Comput.  2015.

[5] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings

of the eighteenth annual ACM-SIAM symposium on Discrete algorithms  2007.

8

[6] H. Avron and C. Boutsidis. Faster subset selection for matrices and applications. SIAM Journal

on Matrix Analysis and Applications  34(4):1464–1499  2013.

[7] J. Borcea and P. Brändén. Applications of stable polynomials to mixed determinants: Johnson’s
conjectures  unimodality  and symmetrized Fischer products. Duke Mathematical Journal 
pages 205–223  2008.

[8] J. Borcea  P. Brändén  and T. Liggett. Negative dependence and the geometry of polynomials.

Journal of the American Mathematical Society  22:521–567  2009.

[9] A. Borodin. Determinantal point processes. arXiv:0911.1153  2009.
[10] C. Boutsidis and M. Magdon-Ismail. Deterministic feature selection for k-means clustering.

IEEE Transactions on Information Theory  pages 6099–6110  2013.

[11] C. Boutsidis  M. W. Mahoney  and P. Drineas. An improved approximation algorithm for the

column subset selection problem. In SODA  pages 968–977  2009.

[12] C. Boutsidis  A. Zouzias  M. W. Mahoney  and P. Drineas. Stochastic dimensionality reduction

for k-means clustering. arXiv preprint arXiv:1110.2897  2011.

[13] C. Boutsidis  P. Drineas  and M. Magdon-Ismail. Near-optimal column-based matrix recon-

struction. SIAM Journal on Computing  pages 687–717  2014.

[14] S. Chen  R. Varma  A. Sandryhaila  and J. Kovaˇcevi´c. Discrete signal processing on graphs:

Sampling theory. IEEE Transactions on Signal Processing  63(24):6510–6523  2015.

[15] A. Çivril and M. Magdon-Ismail. On selecting a maximum volume sub-matrix of a matrix and

related problems. Theoretical Computer Science  pages 4801–4811  2009.

[16] M. Derezinski and M. K. Warmuth. Unbiased estimates for linear regression via volume

sampling. Advances in Neural Information Processing Systems (NIPS)  2017.

[17] A. Deshpande and L. Rademacher. Efﬁcient volume sampling for row/column subset selection.
In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages
329–338. IEEE  2010.

[18] M. Elkin  Y. Emek  D. A. Spielman  and S.-H. Teng. Lower-stretch spanning trees. SIAM

Journal on Computing  2008.

[19] T. Feder and M. Mihail. Balanced matroids. In Symposium on Theory of Computing (STOC) 

pages 26–38  1992.

[20] V. Fedorov. Theory of optimal experiments. Preprint 7 lsm  Moscow State University  1969.
[21] V. Fedorov. Theory of optimal experiments. Academic Press  1972.
[22] A. Frieze  N. Goyal  L. Rademacher  and S. Vempala. Expanders via random spanning trees.

SIAM Journal on Computing  43(2):497–513  2014.

[23] S. O. Gharan  A. Saberi  and M. Singh. A randomized rounding approach to the traveling
salesman problem. In IEEE Symposium on Foundations of Computer Science (FOCS)  pages
550–559  2011.

[24] N. Halko  P.-G. Martinsson  and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review  53(2):217–288 
2011.

[25] S. Joshi and S. Boyd. Sensor selection via convex optimization. IEEE Transactions on Signal

Processing  pages 451–462  2009.

[26] A. Kulesza and B. Taskar. Determinantal Point Processes for machine learning  volume 5.

Foundations and Trends in Machine Learning  2012.

[27] C. Li  S. Jegelka  and S. Sra. Fast mixing markov chains for strongly Rayleigh measures  DPPs 
and constrained sampling. In Advances in Neural Information Processing Systems (NIPS)  2016.
[28] C. Li  S. Sra  and S. Jegelka. Gaussian quadrature for matrix inverse forms with applications.

In ICML  pages 1766–1775  2016.

[29] R. Lyons. Determinantal probability measures. Publications Mathématiques de l’Institut des

Hautes Études Scientiﬁques  98(1):167–212  2003.

9

[30] P. Ma  M. Mahoney  and B. Yu. A statistical perspective on algorithmic leveraging. In Journal

of Machine Learning Research (JMLR)  2015.

[31] O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied

Probability  7(1)  1975.

[32] A. Magen and A. Zouzias. Near optimal dimensionality reductions that preserve volumes. In
Approximation  Randomization and Combinatorial Optimization. Algorithms and Techniques 
pages 523–534. Springer  2008.

[33] A. J. Miller and N.-K. Nguyen. A fedorov exchange algorithm for d-optimal design. Journal of

the royal statistical society  1994.

[34] M. D. Morris and T. J. Mitchell. Exploratory designs for computational experiments. Journal

of Statistical Planning and Inference  43:381–402  1995.

[35] N.-K. Nguyen and A. J. Miller. A review of some exchange algorithms for constructng discrete

optimal designs. Computational Statistics and Data Analysis  14:489–498  1992.

[36] R. Pemantle. Towards a theory of negative dependence. Journal of Mathematical Physics  41:

1371–1390  2000.

[37] R. Pemantle and Y. Peres. Concentration of Lipschitz functionals of determinantal and other

strong Rayleigh measures. Combinatorics  Probability and Computing  23:140–160  2014.

[38] F. Pukelsheim. Optimal design of experiments. SIAM  2006.
[39] D. Spielman and N. Srivastava. Graph sparsiﬁcation by effective resistances. SIAM J. Comput. 

40(6):1913–1926  2011.

[40] D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning  graph

sparsiﬁcation  and solving linear systems. In STOC  2004.

[41] M. Tsitsvero  S. Barbarossa  and P. D. Lorenzo. Signals on graphs: Uncertainty principle and

sampling. IEEE Transactions on Signal Processing  64(18):4845–4860  2016.

[42] D. Wagner. Multivariate stable polynomials: theory and applications. Bulletin of the American

Mathematical Society  48(1):53–84  2011.

[43] Y. Wang  A. W. Yu  and A. Singh. On Computationally Tractable Selection of Experiments in

Regression Models. ArXiv e-prints  2016.

[44] Y. Zhao  F. Pasqualetti  and J. Cortés. Scheduling of control nodes for improved network
controllability. In 2016 IEEE 55th Conference on Decision and Control (CDC)  pages 1859–
1864  2016.

[45] R. Zhu  P. Ma  M. W. Mahoney  and B. Yu. Optimal subsampling approaches for large sample

linear regression. arXiv preprint arXiv:1509.05111  2015.

10

,Chengtao Li
Stefanie Jegelka
Suvrit Sra
Jeffrey Pennington
Pratik Worah