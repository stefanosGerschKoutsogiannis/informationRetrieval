2016,Verification Based Solution for Structured MAB Problems,We consider the problem of finding the best arm in a stochastic Mutli-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations  additional structure is known in advance  causing the task of verifying the optimality of a candidate to be  easier than discovering the best arm. Our results are focused on the scenario where the failure probability $\delta$ must be very low; we essentially show that in this high confidence regime  identifying the best arm is as easy as the task of verification.  We demonstrate the effectiveness of our framework by applying it  and improving the state-of-the art results in the problems of: Linear bandits  Dueling bandits with the Condorcet assumption  Copeland dueling bandits  Unimodal bandits and Graphical bandits.,Veriﬁcation Based Solution for Structured MAB

Problems

Zohar Karnin
Yahoo Research

New York  NY 10036
zkarnin@ymail.com

Abstract

We consider the problem of ﬁnding the best arm in a stochastic Multi-armed
Bandit (MAB) game and propose a general framework based on veriﬁcation that
applies to multiple well-motivated generalizations of the classic MAB problem. In
these generalizations  additional structure is known in advance  causing the task of
verifying the optimality of a candidate to be easier than discovering the best arm.
Our results are focused on the scenario where the failure probability  must be very
low; we essentially show that in this high conﬁdence regime  identifying the best
arm is as easy as the task of veriﬁcation. We demonstrate the effectiveness of our
framework by applying it  and matching or improving the state-of-the art results in
the problems of: Linear bandits  Dueling bandits with the Condorcet assumption 
Copeland dueling bandits  Unimodal bandits and Graphical bandits.

1

Introduction

The Multi-Armed Bandit (MAB) game is one where in each round the player chooses an action 
also referred to as an arm  from a pre-determined set. The player then gains a reward associated
with the chosen arm and observes the reward while rewards associated with the other arms are not
revealed. In the stochastic setting  each arm x has a ﬁxed associated value µ(x) throughout all rounds 
and the reward associated with the arm is a random variable  independent of the history  with an
expected value of µ(x). In this paper we focus on the pure exploration task [9] in the stochastic
setting where our objective is to identify the arm maximizing µ(x) with sufﬁciently high probability 
while minimizing the required number of rounds  otherwise known as the query complexity. This
task  as opposed to the classic task of maximizing the sum of accumulated rewards is motivated
by numerous scenarios where exploration (i.e. trying multiple options) is only possible in an initial
testing phase  and not throughout the running time of the game.
As an example consider a company testing several variations of a (physical) product  and then once
realizing the best one  moving to a production phase where the product is massively produced and
shipped to numerous vendors. It is very natural to require that the identiﬁed option is the best one
with very high probability  as a mistake can be very costly. Generally speaking  the vast majority of
uses-cases of a pure exploration requires the error probability  to be very small  so much so that
even a logarithmic dependence over  is non-negligible. Another example to demonstrate this is that
of explore-then-exploit type algorithms. There are many examples of papers providing a solution to a
regret based MAB problem where the ﬁrst phase consists of identifying the best arm with probability
at least 1  1/T   and then using it in the remainder of the rounds. Here   = 1/T is often assumed to
be the only non-constant.
We do not focus on the classic MAB problem but rather on several extensions of it for settings where
we are given as input some underlying structural properties of the reward function µ. We elaborate
on the formal deﬁnitions and different scenarios in Section 2. Another extension we consider is that

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

of Dueling Bandits where  informally  we do not query a single arm but rather a pair  and rather than
observing the reward of the arms we observe a hint as to the difference between their associated µ
values. Each extension we discuss is motivated by different scenarios which we elaborate on in the
upcoming sections. In all of the cases mentioned  we focus on the regime of high conﬁdence meaning
where the failure probability  is very small.
Notice that due to the additional structure (that does not exist in the classic case)  verifying a candidate
arm is indeed the best arm can be a much easier task  at least conceptually  compared to that of
discovering which arm is the best. This observation leads us to the following design: Explore the
arms and obtain a candidate arm that is the best arm w.p. 1   for some constant   then verify it
is indeed the best with conﬁdence 1  . If the exploration procedure happened to be correct  the
query complexity of the problem will be composed of a sum of two quantities. One is that of the
exploration algorithm that is completely independent of   and the other is dependent of  but is
the query complexity of the easier veriﬁcation task. The query complexity is either dominated by
that of the veriﬁcation task  or by that of the original task with a constant failure probability. Either
way  for small values of  the savings are potentially huge. As it turns out  as discussed in Section 3 
a careful combination of an exploration and veriﬁcation algorithm can achieve an expected query
complexity of Hexplore + Hverify where Hexplore is the exploration query complexity  independent of  
and Hverify is the query complexity of the veriﬁcation procedure with conﬁdence 1  . Below  we
design exploration and veriﬁcation algorithms for the problems of: Dueling bandits §4  Linear bandits
§5  Unimodal graphical bandits §6 and Graphical bandits1 . In the corresponding sections we provide
short reviews of each MAB problem  and analyze their exploration and veriﬁcation algorithms. Our
results improve upon the state-of-the-art results in each of these mentioned problem (See Table 1 for
a detailed comparison).
Related Works: We are aware of one attempt to capture multiple (stochastic) bandit problems
in a single frameworks  given in [20]. The focus there is mostly on problems where the observed
random variables do not necessarily reﬂect the reward  such as the dueling bandit problem  rather
than methods to exploit structure between the arms. For example  in the case of the dueling bandit
problem with the Condorcet assumption their algorithm does not take advantage of the structural
properties and the corresponding query complexity is larger than that obtained here (see Section 4.1).
We review the previous literature of each speciﬁc problem in the corresponding sections.

2 Formulation of Bandit Problems

The pure exploration Multi-Armed Bandit (MAB) problem  in the stochastic setting  can be generally
formalized as follows. Our input consists of a set K of arms  where each arm x is associated with
some reward µ(x). In each round t we play an arm xt and observe the outcome of a random variable
whose expected value is µ(xt). Other non-stochastic settings exist yet they are outside the scope of
our paper; see [4] for a survey on bandit problems  including the stochastic and non-stochastic settings.
The objective in the best arm identiﬁcation problem is to identify the arm2 x⇤ = arg max µ(x) while
minimizing the expected number of queries to the reward values of the arms. Other than the classic
MAB problem  where K is a ﬁnite set and µ is an arbitrary function there exist other frameworks
where some structure is assumed regarding the behavior of µ over the arms of K. An example for a
common framework matching this formulation  that we will analyze in detail in Section 5  is that
of the linear MAB. Here  K is a compact subset of Rd  and the reward function µ is assumed to be
linear. Unlike the classic MAB case  an algorithm can take advantage of the structure of µ and obtain
a performance that is independent of the size of K. Yet another example  discuss in Section 6  is that
of unimodal bandits  where we are given a graph whose vertices are the arms  and it is guaranteed
that the best arm is the unique arm having a maximal value among its neighbors in the graph.
The above general framework captures many variants of the MAB problem  yet does not capture
the Dueling Multi Armed Bandit (DMAB) problem. Here  the input as before consists of a set of
arms denoted by K yet we are not allowed to play a single arm in a round but rather a pair x  y 2K .
The general deﬁnition of the observation from playing the pair x  y is a random variable whose

1Do to space restrictions we defer the section of Graphical bandits [7] to the extended version.
2This objective is naturally extended in the PAC setting where we are interested in an arm that is approximately
the best arm. For simplicity we restrict our focus to the best arm identiﬁcation problem. We note that our general
framework of exploration and veriﬁcation can be easily expanded to handle the PAC setting as well.

2

expected value is P (x  y) where P : K⇥K! R. The original motivating example for the DMAB
[22] problem is that of information retrieval  where a query to a pair of arms is a presentation of the
interleaved results of two ranking algorithms. The output is the 0 or 1  depending on the choice of the
user  i.e. whether she chose a result from one or ranker or the other. The µ score here can be thought
of a quality score for a ranker  deﬁned according to the P scores. We elaborate on the motivation
for the MAB problem and the exact deﬁnition of the best arm in Section 4. In an extended version
of this paper we discuss the problem of graphical bandits that is in some sense a generalization of
the dueling bandit problem. There  we are not allowed to query any pair but rather pairs from some
predeﬁned set E ✓K⇥K .
3 Boosting the Exploration Process with a Veriﬁcation Policy

In what follows we present results for different variants of the MAB problem. We discuss two types
of problems. The ﬁrst is the well known pure exploration problem. Our input is the MAB instance 
including the set of arms and possible structural information  and a conﬁdence parameter . The
objective is to ﬁnd the best arm w.p. at least 1   while using a minimal number of queries. We
often discuss variants of the exploration problem where in addition to ﬁnding the best arm  we wish
to obtain some additional information about the problem such as an estimate of the gaps of the reward
value of suboptimal arms from the optimal one  the identity of important arm pairs  etc. We refer
to this additional information as an advice vector ✓  and our objective is to minimize queries while
obtaining a sufﬁciently accurate advice vector and the true optimal arm with probability at least
1  . For each MAB problem we describe an algorithm referred to as FindBestArm with a query
complexity of3 Hexplore · log(1/) that obtains an advice vector ✓ that is sufﬁciently accurate4 w.p. at
least 1  .
Deﬁnition 1. Let FindBestArm be an algorithm that given the MAB problem and conﬁdence param-
eter > 0 has the following guarantees. (1) with probability at least 1   it outputs a correct best
arm and advice vector ✓. (2) its expected query complexity is Hexplore · log(1/)  where Hexplore is
some instance speciﬁc complexity (that is not required to be known).

The second type of problem is that of veriﬁcation. Here we are given as input not only the MAB
problem and conﬁdence parameter   but an advice vector ✓  including the identity of a candidate
optimal arm.
Deﬁnition 2. Let VerifyBestArm be an algorithm that given the MAB problem  conﬁdence parameter
> 0 and an advice vector ✓ including a proposed identity of the best arm  has the following
guarantees. (1) if the candidate optimal arm is not the actual optimal arm  the output is ‘fail’ w.p. at
least 1  . (2) if the advice vector is sufﬁciently accurate  and in particular the candidate is indeed
the optimal arm  we should output ‘success’ w.p. at least 1  . (3) if the advice vector is sufﬁciently
accurate the expected query complexity is Hverify log(1/). Otherwise  it is Hexplore log(1/).
It is very common that Hverify ⌧ Hexplore as it is clearly an easier problem to simply verify the
identity of the optimal arm rather than discover it. Our main result is thus somewhat surprising as
it essentially shows that in the regime of high conﬁdence  the best arm identiﬁcation problem is as
easy as verifying the identity of a candidate. Speciﬁcally we provide a complexity that is additive in
Hexplore and log(1/) rather than multiplicative. The formal result is as follows.

Algorithm 1 Explore-Verify Framework
Input: Best arm identiﬁcation problem  Oracle access to FindBestArm and VerifyBestArm with

failure probability tuning  failure probability parameter   parameter .
for all r = 1 . . . do

Call FindBestArm with failure probability   denote by ✓ its output.
Call VerifyBestArm with advice vector ✓  that includes a candidate best arm ˆx  and failure
probability /2r2. If succeeded  return ˆx. Else  continue to the next iteration

end for

3The general form of such algorithms is in fact H1 log(1/) + H0. For simplicity we state our results for

the form H log(1/); the general statements are an easy modiﬁcation.

4The exact deﬁnition of sufﬁciently accurate is given per problem instance.

3

Theorem 3. Assume that algorithm 1 is given oracle access to FindBestArm and VerifyBestArm
with the above mentioned guarantees  and a conﬁdence parameter < 1/3. For any < 1/3  the
algorithm identiﬁes the best arm with probability 1   while using an expected number of at most

O (Hexplore log(1/) + (Hverify +  · Hexplore) log(1/))

The following provides the guarantees for two suggested values of . The ﬁrst may not be known to
us but can very often be estimated beforehand. The second depends only on  hence is always known
in advance.
Corollary 4. By setting  = min{1/3  Hverify/Hexplore}  algorithm 1 has an expected number of at
most

O(Hexplore log(Hexplore/Hverify) + Hverify log(1/))

queries. By setting  = min{1/3  1/ log(1/)}  algorithm 1 has an expected query complexity of at
most

O(Hexplore log(log(1/)) + Hverify log(1/))

Notice that by setting  to min{1/3  1/ log(1/)}  for any practical use-case  the dependence on 
in the left summand is nonexistent. In particular  this default value for  provides a multiplicative
saving of either Hexplore/Hverify  i.e. the ratio between the exploration and veriﬁcation problem  or
log(log(1/)). Since log(1/) is rarely a negligible term  and as we will see in what follows  neither is
Hexplore/Hverify  the savings are signiﬁcant  hence the effectiveness of our result.

log(1/)

Proof of Theorem 3. In the analysis we often discuss the output of the sub-procedures in round r > 1 
even if the algorithm terminated before round r. We note that these values are well-deﬁned random
variables regardless of the fact that we may not reach the round. To prove the correctness of the

algorithm notice that sinceP1r=1 r2  2 we have with probability at least 1   that all runs of
VerifyBestArm do not err. Since we halt only when VerifyBestArm outputs ‘success’ our algorithm
indeed outputs the best arm w.p. at least 1  
We proceed to analyze the expected query complexity  and start with a simple observation. Let
QCsingle(r) denote the expected query complexity in round r  and let Yr be the indicator variable to
whether the algorithm reached round r. Since Yr is independent of the procedures running in round r
and in particular of the number of queries required by them  we have that the total expected query
complexity is

E" 1Xr=1

YrQCsingle(r)# =
1Xr=1
E⇥QCsingle(r)⇤  Hexplore log(1/)+

E [Yr] · E⇥QCsingle(r)⇤

Hence  we proceed to analyze E⇥QCsingle(r)⇤ and E[Yr] separately. For E⇥QCsingle(r)⇤ we have

((1  ) Hverify + Hexplore) log✓ 2r2
Hexplore log(1/) + (Hexplore + Hverify) log✓ 2r2
 ◆

 ◆ 

To explain the ﬁrst inequality  the ﬁrst summand is the complexity of FindBestArm . The second
summand is that of VerifyBestArm   that is decomposed to the complexity in the scenario where
FindBestArm succeeded vs. the scenario where it failed. To compute E[Yr]  we notice that Yr is an
indicator function hence E[Yr] = Pr[Yr = 1]. In order for Yr to take the value of 1 we must have that
for all rounds r0 < r either VerifyBestArm or FindBestArm have failed. Since the failure or success of
the algorithms at different rounds are independent we have


Pr[Yr = 1]  Yr0<r✓ +

2(r0)2◆  21r .

The last inequality is since     1/3. We get that the expected number of queries required by the
algorithm is at most

2 ·

1Xr=1

2r✓Hexplore log(1/) + (Hexplore + Hverify) log✓ 2r2

 ◆◆ =

4

cite

existing solution

our solution

✓K1+✏ ·Px6=x⇤ min y:
Px6=x⇤ miny pxy <0 p2

pxy <0

p2

xy◆ + Px6=x⇤

y6=x

xy log(1/)

d log(K/)

2

min

Px6=x⇤ (
Px2(x⇤) 2

x)2+
x log(1/)

KD log(K/) log2(K)

2

min

p2

xy9=;

pxy0 <0
xy log(1/)

p2
xy   min y0 :

min8<:
Px6=x⇤ miny pxy <0 p2
d log⇣Kd/2
min⌘
Px6=x⇤ (x)2+
Px2(x⇤) 2

+
⇢⇤(Y ⇤) log (1/)

2

min

x log(1/)

KD log3(K)

2

min

+ KD log(1/)

2

min

improvement

ratio

+

 K✏ for
large 

up to d

for small 
can be ⌦(K)

in typical
settings
(large )
log2(K)

MAB
task

Dueling

[16]

Bandits

(Condorcet)

Linear
Bandits
Unimodal
Bandits

(line graph)
(line graph)
Graphical
Bandits

[19]

[6]

[7]

2 ·

1Xr=1

2 ·

1Xr=1

Table 1: Comparison between the results obtained by our techniques and the state-of-the-art results in several bandit problem. K represents
the total number of arms   the failure probability; in the case of linear bandits  d is the dimension of the space in which the arms lie. The
deﬁnitions the rest of the problem speciﬁc quantities are given in the corresponding sections. The ratio between the solutions  for a typical case
is given in the last column.

2r (Hexplore log(1/) + (Hexplore + Hverify) log(1/)) +

2r log(2r2) (Hexplore + Hverify) = O (Hexplore log(1/) + (Hexplore + Hverify) log(1/))

In the following sections we provide algorithms for several bandit problems using the framework
of Theorem 3. In Table 1 we provide a comparison between the state-of-the-art results prior to this
paper and the results here.
4 Application to Dueling Bandits

The dueling bandit problem  introduced in [22]  arises naturally in domains where feedback is
more reliable when given as a pairwise preference (e.g.  when it is provided by a human) and
specifying real-valued feedback instead would be arbitrary or inefﬁcient. Examples include ranker
evaluation [14  23  12] in information retrieval  ad placement and recommender systems. As with
other preference learning problems [10]  feedback consists of a pairwise preference between a
selected pair of arms  instead of scalar reward for a single selected arm  as in the K-armed bandit
problem.
The formulation of the problem is the following. Given a set of arms K  a query is to a pair x  y 2K
and its output is a r.v. in {1  1} with an expected reward of Pij. It is assumed that P is anti-
symmetric meaning5 P (x  y) = P (y  x) and the µ values are determined by those of P . One
common assumption regarding P is the existence of a Condorcet winner  meaning there exist some
x⇤ 2K for which P (x⇤  y)  0 for all y 2K . In this case  x⇤ is deﬁned as the best arm and the
reward associated with arm y is typically P (x⇤  y). A more general framework can be considered
where a Condorcet winner is not assumed to exist. In the absence of a Condorcet winner there is no
clear answer as to which arm is the best; several approaches are discussed in [20]  [5]  and recently in
[8  3]  that use some of the notions proposed by social choice theorists  such as the Copeland score or
the Borda score to measure the quality of each arm  or game theoretic concepts to determine the best
worst-case strategy over arms; we do not elaborate on all of them as they are outside the scope of this
paper. In Appendix B.2 we discuss one solution based on the Copeland score  where µ(x) is deﬁned
as the number of arm y 6= x where P (x  y) > 0.
A general framework capturing both the MAB and DMAB scenarios is that of partial monitoring
games introduced by [18]. In this framework  when playing an arm K one obtains a reward µ(x) yet
observes a different function h(x). Some connection between h and µ is known in advance and based
on it  one can design a strategy to discover the best arm or minimize regret. As we do not present
results regarding this framework we do not elaborate on it any further  but rather mention that our
results  in terms of query complexity  cannot be matched by the existing results there.

5It is actually common to deﬁne the output of P as a number in [0  1] and have P (x  y) = 1  P (y  x)  but

both deﬁnitions are equivalent up to a linear shift of P .

5

4.1 Dueling Bandits with the Condorcet Assumption
The Condorcet assumption in the Dueling bandit setting asserts the existence of an arm x⇤ that beats
all other arms. In this section we discuss a solution for ﬁnding this arm under the assumption of its
existence. Recall that the observable input consists of a set of arms K of size K. There is assumed
to exist some matrix P mapping each pair of arms x  y 2K to a number pxy 2 [1  1]; the matrix
P has a zero diagonal  meaning pxx = 0 and is anti-symmetric pxy = pyx. A query to the pair
(x  y) gives an observation to a random Bernoulli variable with expected value (1 + pxy)/2 and is
considered as an outcome of a match between x  y. As we assume the existence of a Condorcet
winner  there exists some x⇤ 2K with px⇤y > 0 for all y 6= x.
The Condorcet dueling bandit problem  as stated here and without any additional assumptions was
tackled in several papers [20  26  16]. The best guarantees to date are given by [16] that provide an
asymptotically optimal regret bound for the problem  for the regime of a very large time horizon. This
result can be transformed into a best-arm identiﬁcation algorithm  and the corresponding guarantee is
listed in Table 1. Loosely speaking  the result shows that it sufﬁces to query each pair sufﬁciently
many times to separate the corresponding Px y from 0.5 with constant probability  and additionally
only K pairs must be queried sufﬁciently many times in order to separate the corresponding Px y from
0.5 with probability 1  . We note that other improvements exist that achieve a better constant term
(the additive term independent of ) [25  24] or an overall improved result via imposing additional
assumptions about P such as an induced total order  stochastic triangle inequality etc. [22  23  1].
These types of results however fall outside the scope of our paper.
In Appendix B.1 we provide an exploration and veriﬁcation algorithm for the problem. The explo-
ration algorithm queries all pairs until ﬁnding  for each suboptimal arm x  an arm y with pxy < 0;
the exploration algorithm provides as output not only the identity of the optimal arm  but for each
sub-optimal arm x  the identity of an arm y(x) that (approximately) maximizes pyx meaning it beats
x by the largest gap. The veriﬁcation procedure is now straightforward. Given the above advice the
algorithm makes sure that for each allegedly sub-optimal x  the arm y(x) indeed beats it meaning
p(yx) > 0. We obtain the following formal result.
Theorem 5. Algorithm 1  along with the exploration and veriﬁcation algorithms given in Ap-
pendix B.1  ﬁnds the Condorcet winner w.p. at least 1   while using an expected amount of at
most

˜O0@Xy6=x⇤

queries  where x⇤ is the Condorcet winner.

p2

x⇤y + Xx6=x⇤Xy6=x

min⇢p2

xy   min

y0 pxy0 <0

p2

xy1A + O0@Xx6=x⇤

min

y pxy<0

p2
xy ln(K/p2

xy)1A

5 Application to Linear Bandits

The linear bandit problem was originally introduced in [2]. It captures multiple problems where there
is linear structure among the available options. Its pure exploration variant (as opposed to the regret
setting) was recently discussed in [19]. Recall that in the linear bandit problem the set of arms K is
a subset of Rd. The reward function associated with an arm x is a random variable with expected
value µ(x) = w>x  for some unknown w 2 Rd. For simplicity we assume that all vectors w  and
those of K lie inside the Euclidean unit ball  and that the noise is sub-gaussian with variance 1 (hence
concentration bounds such as Hoeffding’s inequality can be applied).
The results of [19] offer two approaches. The ﬁrst is a static strategy that guarantees  for failure
probability   a query complexity of d log(K/)
with x⇤ being the best arm  x = w>(x⇤  x) for
x 6= x⇤ and min = minx6=x⇤ x. The second is adaptive and provides better bounds in a speciﬁc
case where the majority of the hardship of the problem is in separating the best arm from the second
best arm.
The algorithms are based on tools from the area of Optimal design of experiments where the high level
idea is the following: Consider our set of vectors (arms) K and an additional set of vecotrs Y . We are
interested in querying a sequence of t arms from K that will minimize the maximum variance of the
estimation of w>y  where the maximum is taken over all y 2 Y . Recall that via the Azuma-Hoeffding
inequality  one can show that by querying a set of points x1  . . .   xt and solving the Ordinary Least

2

min

6

Squares (OLS) problem  one obtains an unbiased estimator of w and the corresponding variance to a
point y is

⇢x1 ... xt(y) = y> tXi=1

xix>i !1

y

Hence  our formal problem statement is to obtain a sequence x1  . . .   xt that minimizes ⇢x1 ... xt(Y )
deﬁned as ⇢x1 ... xt(Y ) = maxy2Y ⇢x1 ... xt(y). Tools from the area of Optimal design of experi-
ments (see e.g. [21]) provide ways to obtain such sequences that achieve a multiplicative approxima-
tion of 1 + d(d + 1)/t of the optimal sequence. In particular it is shown that as t tends to inﬁnity  t
times the ⇢ value of the optimal sequence of length t tends to

⇢⇤(Y ) = min
p

max
y2Y

y> Xx2K

pxxx>!1

y

with p restricted to being a distribution over K. We elaborate on these in the extended version of the
paper.
[19] propose two and analyze two different choices of the set Y . The ﬁrst is the set Y = K; querying
points of K in order to minimize ⇢x1... xt(K) leads to a best arm identiﬁcation algorithm with a query
complexity of d log(K/)/2
min for failure probability . We use essentially the same approach for
the exploration procedure (given in the extended version)  and with the same (asymptotic) query
complexity we do not only obtain a candidate best arm ˆx but also approximations of the different x
for all x 6= x⇤. These are required for the veriﬁcation procedure.
The second interesting set Y is the set Y =n x⇤x

x |x 2K   x 6= x⇤o. Clearly this set is not known to

us in advance  but it helps in [19] to deﬁne a notion of the ‘true’ complexity of the problem. Indeed 
one cannot discover the best arm without verifying that it is superior to the others  and the set Y
provides the best strategy to do so. The authors show that6

max

y2Y kyk2  ⇢⇤(Y )  4d/2

min

and bring examples where each of the inequalities are tight. Notice that the multiplicative gap between
the bounding expressions can be huge (at least linear in the dimension d)  hence an algorithm with a
query complexity depending on ⇢⇤(Y ) as opposed to d/2
min can potentially be much better than the
above mentioned algorithm. The bound on ⇢⇤(Y ) proves in particular that indeed querying w.r.t. Y is a
better strategy than querying w.r.t. K. This immediately translates into a veriﬁcation procedure. Given
the advice from our exploration procedure  we have access to a candidate best arm  and approximate 
values. Hence  we construct this set Y and query according to it. We show that given a correct advice 
the query complexity for failure probability  is at most O (⇢⇤(Y ⇤) log(K⇢⇤(Y ⇤)/)). Combining
the exploration and veriﬁcation algorithms  we get the following result.
Theorem 6. Algorithm 1  along with the exploration and veriﬁcation algorithms described above
(we give a the formal version only in the extended version of the paper)  ﬁnds the best arm w.p. at
least 1   while using an expected query complexity of

O d logKd/2
min

2

min

+ ⇢⇤(Y ⇤) log (1/)!

6 Application to Unimodal Bandits
The unimodal bandit problem consists of a MAB problem given unimodality information. We focus
on a graphical variant deﬁned as follows: There exist some graph G whose vertex set is the set of
arm K and an arbitrary edge set E. For every sub-optimal arm x there exist some neighbor y in the
graph such that µ(x) < µ(y). In other words  the best arm x⇤ is the unique arm having a superior
reward compared to its immediate neighbors. The graphical unimodal bandit problem was introduced
by7 [13].

6Under the assumption that all vectors in K lie in the Euclidean unit sphere
7Other variants of the unimodal bandit problem exist  e.g. one where the arms are the scalars in the intervals
[0  1] yet we do not deal with them in this paper  as we focus on pure best arm identiﬁcation problems and in
that scenario the regret setting is more common  and only a PAC algorithm is possible  translating to a T 2/3
rather than pT regret algorithm

7

Due to space constraints we limit the discussion here to a speciﬁc type of unimodal bandits in
which the underlying graph is line. The motivation here comes from a scenario where the point
set K represents an ✏-net over the [0  1] interval and the µ values come from some unimodal one-
dimensional function. We discuss the more general graph scenario only in the extended version of
the paper. To review the existing results we introduce some notations. For an arm x let (x) denote
the set of its neighbors in the graph. For a suboptimal arm x we let 
x = maxy2(x) µ(y)  µ(x)
be the gap between the reward of x and its neighbors and let x = µ(x⇤)  µ(x) be its gap from the
best arm x⇤. We denote by 
x and min be the minimal value of x.
Notice that in reasonable scenarios  for a typical arm x we have 
x ⌧ x since many arms are far
from being optimal but have a close value to those of their two neighbors.
The state-of-the-art results to date  as far as we are aware  for the problem at hand is by [6]  where
a method OSUB is proposed achieving an expected query complexity of (up to logarithmic terms
independent of )8

min the minimal value of 

O0@Xx6=x⇤

(

x)2 + Xx2(x⇤)

2

x log(1/)1A

They show that the summand with the logarithmic dependence over  is optimal. In the context of a
line graph we provide an algorithm whose exploration is a simple naive application of a best arm
identiﬁcation algorithm that ignores the structure of the problem  e.g. Exponential Gap-Elimination
by [15]. The veriﬁcation algorithm requires only the identity of the candidate best arm as advice. It
simply applies a best arm identiﬁcation algorithm over the candidate arm and its neighborhood. The
following provides our formal results.
Theorem 7. Algorithm 1  along with the exploration of Exponential Gap-Elimination and the
veriﬁcation algorithm of Exponential Gap-Elimination  applied to the neighborhood of the candidate
best arm  ﬁnds the best arm w.p. at least 1   while using an expected query complexity of

O0@Xx6=x⇤

2

x log (K/min) + Xx2(x⇤)

2

x log (1/)1A

The improvement w.r.t. the results of [6] is in the constant term independent of . The replacement
of 
x with x leads to a signiﬁcant improvement in many reasonable submodular functions. For
example  if the arms for an ✏-net over the [0  1] interval  and the function is O(1)-Lipchitz then

x)2 =⌦( ✏3) whilePx6=x⇤(x)2 can potentially be O(✏2). Perhaps for this reason 

experiments in [6] showed that often  performing UCB on an ✏-net is superior to other algorithms.
7 Conclusions

Px6=x⇤(

We presented a general framework for improving the performance of best-arm identiﬁcation problems 
for the regime of high conﬁdence. Our framework is based on the fact that in MAB problems with
structure  it is often easier to design an algorithm for verifying a candidate arm is the best one  rather
than discovering the identity of the best arm. We demonstrated the effectiveness of our framework by
improving the state-of-the-art results in several MAB problems.

References
[1] Nir Ailon  Zohar Karnin  and Thorsten Joachims. Reducing dueling bandits to cardinal bandits.

In
Proceedings of the 31st International Conference on Machine Learning (ICML-14)  pages 856–864  2014.

[2] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine

Learning Research  3:397–422  2003.

[3] Akshay Balsubramani  Zohar Karnin  Robert Schapire  and Masrour Zoghi. Instance-dependent regret
bounds for dueling bandits. In Proceedings of The 29th Conference on Learning Theory  COLT 2016 
2016.

8The result of [6] is in fact tighter in the sense that it takes advantage of the variance of the estimators by
using conﬁdence bounds based on KL-divergence. In the case of uniform variance however  the stated results
here are accurate. More importantly  the KL-divergence type techniques can be applied here to obtain the same
type of guarantees  at the expense of a slightly more technical analysis. For this reason we present the results for
the case of uniform variance.

8

[4] Sébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Machine Learning  5(1):1–122  2012.

[5] Róbert Busa-Fekete  Balázs Szörényi  and Eyke Hüllermeier. Pac rank elicitation through adaptive sampling
of stochastic pairwise preferences. In Proceedings of the Twenty-Eighth AAAI Conference on Artiﬁcial
Intelligence  AAAI  2014.

[6] Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.
In Proceedings of the 31st International Conference on Machine Learning (ICML-14)  pages 521–529 
2014.

[7] Dotan Di Castro  Claudio Gentile  and Shie Mannor. Bandits with an edge. CoRR  abs/1109.2296  2011.

[8] Miroslav Dudík  Katja Hofmann  Robert E. Schapire  Aleksandrs Slivkins  and Masrour Zoghi. Contextual

dueling bandits. In Grünwald et al. [11]  pages 563–587.

[9] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Action elimination and stopping conditions for the
multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research 
7:1079–1105  2006.

[10] J. Fürnkranz and E. Hüllermeier  editors. Preference Learning. Springer-Verlag  2010.

[11] Peter Grünwald  Elad Hazan  and Satyen Kale  editors. Proceedings of The 28th Conference on Learning

Theory  COLT 2015  Paris  France  July 3-6  2015  volume 40 of JMLR Proceedings. JMLR.org  2015.

[12] K. Hofmann  S. Whiteson  and M. de Rijke. Balancing exploration and exploitation in listwise and pairwise

online learning to rank for information retrieval. Information Retrieval  16(1):63–90  2013.

[13] Y Yu Jia and Shie Mannor. Unimodal bandits. In Proceedings of the 28th International Conference on

Machine Learning (ICML-11)  pages 41–48  2011.

[14] T. Joachims. Optimizing search engines using clickthrough data. In KDD  2002.

[15] Zohar Karnin  Tomer Koren  and Oren Somekh. Almost optimal exploration in multi-armed bandits. In
Proceedings of the 30th International Conference on Machine Learning (ICML-13)  pages 1238–1246 
2013.

[16] Junpei Komiyama  Junya Honda  Hisashi Kashima  and Hiroshi Nakagawa. Regret lower bound and

optimal algorithm in dueling bandit problem. In Grünwald et al. [11]  pages 1141–1154.

[17] Junpei Komiyama  Junya Honda  and Hiroshi Nakagawa. Copeland dueling bandit problem: Regret lower

bound  optimal algorithm  and computationally efﬁcient algorithm  2016.

[18] A. Piccolboni and C. Schindelhauer. Discrete prediction games with arbitrary feedback and loss. In

Computational Learning Theory  pages 208–223  2001.

[19] Marta Soare  Alessandro Lazaric  and Rémi Munos. Best-arm identiﬁcation in linear bandits. In Advances

in Neural Information Processing Systems  pages 828–836  2014.

[20] Tanguy Urvoy  Fabrice Clerot  Raphael Féraud  and Sami Naamane. Generic exploration and k-armed
voting bandits. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) 
pages 91–99  2013.

[21] Kai Yu  Jinbo Bi  and Volker Tresp. Active learning via transductive experimental design. In Proceedings

of the 23rd international conference on Machine learning  pages 1081–1088. ACM  2006.

[22] Y. Yue  J. Broder  R. Kleinberg  and T. Joachims. The K-armed dueling bandits problem. Journal of

Computer and System Sciences  78(5):1538–1556  September 2012.

[23] Y. Yue and T. Joachims. Beat the mean bandit. In ICML  2011.

[24] Masrour Zoghi  Zohar Karnin  Shimon Whiteson  and Maarten de Rijke. Copeland dueling bandits. In

Advances in Neural Information Processing Systems  pages 307–315  2015.

[25] Masrour Zoghi  Shimon Whiteson  and Maarten de Rijke. Mergerucb: A method for large-scale online
ranker evaluation. In Proceedings of the Eighth ACM International Conference on Web Search and Data
Mining  pages 17–26. ACM  2015.

[26] Masrour Zoghi  Shimon Whiteson  Remi Munos  and Maarten D Rijke. Relative upper conﬁdence bound
for the k-armed dueling bandit problem. In Proceedings of the 31st International Conference on Machine
Learning (ICML-14)  pages 10–18  2014.

9

,Zohar Karnin