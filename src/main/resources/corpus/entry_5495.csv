2019,The Parameterized Complexity of Cascading Portfolio Scheduling,Cascading portfolio scheduling is a static algorithm selection strategy which uses a sample of test instances to compute an optimal ordering (a cascading schedule) of a portfolio of available algorithms.  The algorithms are then applied to each future instance according to this cascading schedule  until some algorithm in the schedule succeeds. Cascading algorithm scheduling has proven to be effective in several applications  including QBF solving and the generation of ImageNet classification models.

It is known that the computation of an optimal cascading schedule in
the offline phase is NP-hard. In this paper we study the parameterized complexity of this problem and establish its fixed-parameter tractability by utilizing structural properties of the success relation between algorithms and test instances.

Our findings are significant as they reveal that in spite of the intractability of the problem in its general form  one can indeed exploit sparseness or density of the success relation to obtain non-trivial runtime guarantees for finding an optimal cascading schedule.,The Parameterized Complexity of
Cascading Portfolio Scheduling

Eduard Eiben
Royal Holloway

University of London

Department of CS

UK

Robert Ganian

TU Wien

Algorithms and

Complexity Group

Austria

Iyad Kanj

DePaul University

School of Computing

Chicago

USA

Stefan Szeider

TU Wien

Algorithms and

Complexity Group

Austria

Abstract

Cascading portfolio scheduling is a static algorithm selection strategy which uses a
sample of test instances to compute an optimal ordering (a cascading schedule) of
a portfolio of available algorithms. The algorithms are then applied to each future
instance according to this cascading schedule  until some algorithm in the schedule
succeeds. Cascading scheduling has proven to be effective in several applications 
including QBF solving and generation of ImageNet classiﬁcation models.
It is known that the computation of an optimal cascading schedule in the ofﬂine
phase is NP-hard. In this paper we study the parameterized complexity of this
problem and establish its ﬁxed-parameter tractability by utilizing structural prop-
erties of the success relation between algorithms and test instances. Our ﬁndings
are signiﬁcant as they reveal that in spite of the intractability of the problem in its
general form  one can indeed exploit sparseness or density of the success relation
to obtain non-trivial runtime guarantees for ﬁnding an optimal cascading schedule.

Introduction

1
When dealing with hard computational problems  one often has access to a portfolio of different
algorithms that can be applied to solve the given problem  with each of the algorithms having
complementary strengths. There are various ways of how this performance complementarity can be
exploited. Algorithm selection  a line of research initiated by Rice [19]  studies various approaches
one can use to select algorithms from the portfolio. Algorithm selection has proven to be an extremely
powerful tool with many success stories in Propositional Satisﬁability  Constraint Satisfaction 
Planning  QBF Solving  Machine Learning and other domains [12  13  14  20]. A common approach
to algorithm selection is per-instance-based algorithm selection  where an algorithm is chosen for
each instance independently  based on some features of the instance (see  e.g.  [15  10]). However 
sometimes information about the individual instances is not available or difﬁcult to use. Then  one
can instead make use of information about the distribution of the set of instances  e.g.  in terms of
a representative sample of instances which can be used as a training set. In such cases  one can
compute in an ofﬂine phase a suitable linear ordering of the algorithms  optimizing the ordering for
the training set of instances. This ordering is then applied uniformly to any given problem instance in
an online fashion—in particular  if the ﬁrst algorithm in our ordering fails to solve a given instance
(due to timeout  memory overﬂow  or due to not reaching a desired accuracy)  then the second
algorithm is called  and this continues until we solve the instance. Such a static algorithm selection 
“cascading portfolio scheduling”  is simpler to implement than per-instance selection methods and
can be very effective [22]. One prominent recent application of cascading portfolio scheduling lies in
state-of-the-art ImageNet classiﬁcation models  where it resulted in a signiﬁcant speedup by reducing
the number of ﬂoating-point operations [23]. Cascading portfolio scheduling is also related to online
portfolio scheduling [11  16].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In this paper we address the fundamental problem of ﬁnding an optimal cascading schedule for a
given portfolio A of algorithms with respect to a given training set T of instances. In particular 
for the problem CASCADING PORTFOLIO SCHEDULING (or CPS for short) that we consider  we
are given m algorithms  n test instances  and a cost mapping cost  where cost(α  t) denotes the cost
of running algorithm α on test instance t  and a success relation S where (α  t) ∈ S means that
algorithm α succeeds on test instance t. As the cost mapping and the success relation are deﬁned
independently  this setting is very general and entails different scenarios.
Scenario 1 Each algorithm is run until a globally set timeout C is reached. If the algorithm α solves
test instance t in time c ≤ C then cost(α  t) = c and (α  t) ∈ S; otherwise we have cost(α  t) = C
and (α  t) /∈ S.
Scenario 2 Algorithm α solves a test instance t in time c and outputs an accuracy estimate r for its
solution. r is then compared with a globally set accuracy threshold R. If r ≥ R then (α  t) ∈ S 
otherwise (α  t) /∈ S; in any case cost(α  t) = c. Such a strategy has been used for prediction
model generation [23].

Scenario 3 All the algorithms are ﬁrst run with a short timeout and if the test instance has not been
solved after this  algorithms are run again without a timeout (a similar strategy has been used for
QBF solving [18]). Such a strategy can be instantiated to our setting by adding two copies of each
algorithm to the portfolio  one with a short timeout and one without a timeout.

Contribution. We establish the ﬁxed-parameter tractability1 of computing an optimal cascading
schedule by utilizing structural properties of the success relation. We look at the success relation in
terms of a Boolean matrix  the evaluation matrix  where each row corresponds to a test instance and
each column corresponds to an algorithm. A cell contains the entry 1 iff the corresponding algorithm
succeeds on the corresponding test. We show that if this matrix is either very sparse or very dense 
then the computation of an optimal schedule is tractable. More speciﬁcally  we establish the following
results  which we describe by writing CPS[parm] for CASCADING PORTFOLIO SCHEDULING
parameterized by parameter parm.
First we consider the algorithm failure degree which is the largest number of tests a single algorithm
fails on  and the test failure degree which is the largest number of algorithms that fail on a single
test (these two parameters can also be seen as the largest number of 0’s that appear in a row and the
largest number of 0’s that appear in a column of the matrix  respectively).
(1) CPS[algorithm failure degree] and CPS[test failure degree] are ﬁxed-parameter tractable (Theo-

rems 4 and 5).

It is natural to consider also the dual parameters algorithm success degree and test success degree.
However  it follows from known results that CPS is already NP-hard if both of these parameters
are bounded by a constant (Proposition 6). Hence  our results exhibit a certain asymmetry between
failure and success degrees.
We then consider more sophisticated parameters that capture the sparsity or density of the evaluation
matrix. The failure cover number is the smallest number of rows and columns in the evaluation
matrix needed to cover all the 0’s in the matrix; similarly  the success cover number is the smallest
number of rows and columns needed to cover all the 1’s. In fact  both parameters can be computed in
polynomial time using bipartite vertex cover algorithms [7].
(2) CPS[failure cover number] and CPS[success cover number] are ﬁxed-parameter tractable

(Corollary 8 and Theorem 16).

These results are signiﬁcant as they indicate that CASCADING PORTFOLIO SCHEDULING can be
solved efﬁciently as long as the evaluation matrix is sufﬁciently sparse or dense. Our result for
CPS[failure cover number] in fact also shows ﬁxed-parameter tractability of the problem for an even
more general parameter than success cover number: the treewidth [21] of the bipartite graph between
the algorithms and tests  where edges join success pairs. This is our most technical contribution and
reveals how a fundamental graphical parameter [see  e.g.  8] can be utilized for algorithm scheduling.
Another natural variant of the problem  CPSopt[length]  arises by adding an upper bound (cid:96) on the
length  i.e.  cardinality  of the computed schedule  and asking for a schedule of length ≤ (cid:96) of minimum
cost. We obtain a complexity classiﬁcation of the problem under this parameterization as well.

1Fixed-parameter tractability is a relaxation of polynomial tractability; deﬁnitions are provided in Section 2.

2

(3) CPS[length] can be solved in polynomial time for each ﬁxed bound (cid:96)  but is not ﬁxed-parameter

tractable parameterized by (cid:96) subject to established complexity assumptions.

An overview of our results is provided in Table 1.

Parameter
Algorithm failure degree
Test failure degree
Algorithm and test success degree
Failure cover number and failure treewidth
Success cover number
Length

Complexity
FPT
FPT
NP-hard (for constant parameters)
FPT
FPT
in XP and W[2]-hard

Reference
Proposition 4
Proposition 5
Proposition 6
Theorem 7
Theorem 16
Proposition 3

Table 1: An overview of the complexity results presented in this paper.

2 Preliminaries
Problem Deﬁnition. An instance of the CASCADING PORTFOLIO SCHEDULING problem is a
tuple (A  T  cost  S) comprising:
• a set A of m algorithms 
• a set T of n tests 
• a cost mapping cost : (A × T ) → N  and
• a success relation S ⊆ A × T .
Let τ be a totally ordered subset of A; we call such a set a schedule. The length of a schedule is
its cardinality. We say that τ is valid if for each test t there exists an algorithm α ∈ τ such that
(α  t) ∈ S. Throughout the paper  we will assume that there exists a valid schedule for our considered
instances—or  equivalently  that each test is solved by at least one algorithm.

The processing cost of a test t for a valid schedule τ = (α1  . . .   αq) is deﬁned as(cid:80)j

i=1 cost(αi  t) 
where j is the ﬁrst algorithm in τ such that (αj  t) ∈ S. The cost of a valid schedule τ  denoted
cost(τ )  is the sum of the processing costs of all tests in T for τ. The aim in CASCADING PORTFOLIO
SCHEDULING is to ﬁnd a valid schedule τ of minimum cost.
Parameterized Complexity.
In parameterized algorithmics [6  4  3  9] the complexity of a problem
is studied not only with respect to the input size n but also a parameter k ∈ N. The most favorable
complexity class in this setting is FPT (ﬁxed-parameter tractable) which contains all problems that
can be solved by an algorithm running in time f (k) · nO(1)  where f is a computable function.
Algorithms running in this time are called ﬁxed-parameter algorithms. We will also make use of the
complexity classes W[2] and XP  where W[2] ⊆ XP. Problems complete for W[2] are are widely
believed to not be in FPT. The class XP contains problems that are solvable in time O(nf (k)) 
where f is a computable function; in other words  problems in XP are polynomial-time solvable
when the parameter is bounded by a constant. To obtain our lower bound results  we will need the
notion of a parameterized reduction  referred to as FPT-reduction  which is in many ways analogous
to the standard polynomial-time reductions; the distinction is that a parameterized reduction runs in
time f (k) · nO(1) for some computable function f  and provides upper bounds on the parameter size
in the resulting instance [4  3  6  17].
We write O∗(f (k)) to denote a function of the form f (k) · nO(1)  where n is the input length and k
is the parameter.
Problem Parameters. CASCADING PORTFOLIO SCHEDULING is known to be NP-hard [23]  and
our aim in this paper will be to circumvent this by obtaining parameters that exploit the ﬁne-grained
structure in relevant problem instances. We note that we explicitly aim for results which allow for
arbitrary cost mappings  since these are expected to consist of large (and often disorderly) numbers in
real-life settings. Instead  we will consider parameters that restrict structural properties of the “binary”
success relation. To visualize this success relation  it will be useful to view an instance I as an m × n
matrix MI where MI[i  j] = 1 if (αi  tj) ∈ S (i.e. if the j-th test succeeds on the i-th algorithm  for
some ﬁxed ordering of algorithms and tests)  and MI[i  j] = 0 otherwise.

3

Figure 1: An instance with 4 algorithms and 5 tests in the setting where (exact) algorithms are executed
with a global timeout of 7  as discussed in Scenario 1. On the left is the matrix MI representing the
success relation. The failure covering number is 3  as witnessed by the highlighted two rows and one
column. The matrix CI on the right represents the cost relation  with CI[i  j] = cost[αi  tj]. The
instance I depicted here has a single solution  notably (α1  α3).

The two most natural parameters to consider are m and n  and these correspond to the number of rows
and columns in MI  respectively. Unfortunately  these two parameters are also fairly restrictive—it
is unlikely that instances of interest will have a very small number of algorithms or test instances.
Another option would be to use the maximum number of times an algorithm (or test) can fail (or
succeed) as a parameter. In particular  the algorithm success (or failure) degree is the maximum
number of 1’s (or 0’s  respectively) occurring in any row in MI. Similarly  we let the test success (or
failure) degree be the maximum number of 1’s (or 0’s  respectively) occurring in any column in MI.
Instances where these parameters are small correspond to cases where “almost everything” either
fails or succeeds.
A more advanced parameter that can be extracted from MI is the covering number  which intuitively
captures the minimum number of rows and columns that are needed to “cover” all successes (or
failures) in the matrix. More formally  we say that an entry MI[i  j] is covered by row i and by
column j. Then the success (or failure) covering number is the minimum value of r + c such that
there exist r rows and c columns in MI with the property that each occurrence of 1 (or 0  respectively)
in MI is covered by one of these rows or columns. Intuitively  an instance has success covering
number s if there exist r algorithms and s − r tests such that these have a non-empty intersection
with every relation in S—see Figure 1 for an example. We note that the covering number has been
used as a structural parameter of matrices  notably in previous work on the MATRIX COMPLETION
problem [7]  and that it is possible to compute r algorithms and c tests achieving a minimum covering
number in polynomial time [7  Proposition 1]. We will denote the success covering number by covs
and the failure covering number by covf .
3 Results for Basic Parameters
In this section we consider the CASCADING PORTFOLIO SCHEDULING problem parameterized by
the number of algorithms (i.e.  by m = |A|)  by the number of tests (i.e.  by n = |T|)  and by the
length of the computed schedule.
We begin mapping the complexity of our problem with two initial propositions. Note that both
propositions can also be obtained as corollaries of the more general Theorem 16  presented later. Still 
we consider it useful to present a short sketch of proof of Proposition 1  since it nicely introduces the
combinatorial techniques that will later be extended in the proof of Theorem 1.
Proposition 1. CPS[number of algorithms] is in FPT.
Proof Sketch. We reduce the problem to that of ﬁnding a minimum-weight path in a directed acyclic
graph (DAG) D. We construct D as follows. We create a single source vertex s  and a single
destination vertex z in D. We deﬁne L0 = {s}  Lm+1 = {z}  and apart from z  D contains m layers 
L0  . . .   Lm  of vertices  where layer Li  for i ∈ {0  . . .   m}  contains a vertex for each subset of A of
cardinality i  with vertex s corresponding to the empty set. We connect each vertex that corresponds
to a subset of A which is a valid portfolio to z. For each vertex u in layer Li  i ∈ {0  . . .   m − 1} 
corresponding to a subset Su ⊂ A  and each vertex v ∈ Li+1 corresponding to a subset Sv ⊆ A 
where Sv = Su ∪ {α}  for α ∈ A  we add an edge (u  v) if there exists a test t ∈ T such that (1)
(α  t) ∈ S and (2) there does not exist β ∈ Su such that (β  t) ∈ S; in such case the weight of (u  v) 
wt(u  v)  is deﬁned as follows. Let Tα ⊆ T be the set of tests that cannot be solved by any algorithm
cost(α  t). Informally speaking  the weight of (u  v) is the additional
cost incurred by appending algorithm α to any (partial) portfolio consisting of the algorithms in Su.
This completes the construction of D.

in Su. Then wt(u  v) =(cid:80)

t∈Tα

4

11101001010101011101t1t2t3t4t5α1α2α3α415273773757176725374MICIIt is not difﬁcult to show that an optimal portfolio for A corresponds to a minimum-weight path from
s to z  which can be computed in time O∗(2m).
Proposition 2. CPS[number of tests] is in FPT.

To formally capture the parameterization of the problem by the length (cid:96) of the computed schedule 
we need to slightly adjust its formal deﬁnition. Let CPSval[length] and CPSopt[length] denote the
variants of CASCADING PORTFOLIO SCHEDULING where for each problem instance we are also
given an integer (cid:96) > 0 and only schedules up to length (cid:96) are considered ((cid:96) being the parameter).
CPSval[length] is the decision problem that asks whether there exists a valid schedule of length ≤ (cid:96) 
and CPSopt[length] asks to compute a valid schedule of length ≤ (cid:96) of smallest cost or decide that no
valid schedule of length ≤ (cid:96) exists. Both problems are parameterized by the length (cid:96).
Proposition 3. CPSopt[length] is in XP  but is unlikely to be in FPT since already CPSval[length] is
W[2]-complete.
Proof Sketch. Membership of CPSopt[length] in XP is easy: We enumerate every ordered selection
of at most (cid:96) algorithms from A (there are at most O((cid:96)!m(cid:96)) many) and if valid  we compute its cost 
and keep track of a valid selection (if any) of minimum cost over all enumerations.
To prove the W[2]-hardness of CPSval[length]  we give an FPT-reduction from the W[2]-complete
problem SET COVER [4]. The membership of CPSval[length] in W[2] follows from a straightforward
reduction to SET COVER  which is omitted.
Given an instance ((U F)  k) of SET COVER  where U is a ground set of elements  F is a family of
subsets for U  and k ∈ N is the parameter  we create an instance of CASCADING PORTFOLIO SCHE-
DULING as follows. We set T = U  and for each F ∈ F  we create an algorithm αF ∈ A and add
(αF   t) to S  for every t ∈ F . Finally  we set (cid:96) = k. The function cost can be deﬁned arbitrarily. The
above reduction is clearly a (polynomial-time) FPT-reduction  and it is straightforward to verify that
((U F)  k) is a yes-instance of SET COVER if and only if the constructed instance of CASCADING
PORTFOLIO SCHEDULING has a valid portfolio of size at most (cid:96).
We remark that the above construction can also be used to show that the problem variants arising in
Scenarios 1-3 described in the introduction remain W[2]-complete.
4 Results for Degree Parameters
This section presents a classiﬁcation of the complexity of CASCADING PORTFOLIO SCHEDULING
parameterized by the considered (success and failure) degree parameters.
Proposition 4. CPS[algorithm failure degree] is in FPT.
f the algorithm failure degree  and let I = (A  T  cost  S) be an instance of
Proof. Denote by degA
CASCADING PORTFOLIO SCHEDULING. Consider an algorithm which loops over each algorithm
α ∈ A and proceeds under the assumption that α is the ﬁrst algorithm in an optimal valid portfolio.
For each such α  the number of tests in T that cannot be evaluated by α is at most degA
f . Removing
α from A and the subset of tests {t | (α  t) ∈ S} from T results in an instance I− of CASCADING
PORTFOLIO SCHEDULING with at most degA
f tests  which  by Proposition 2  can be solved in time
O∗((degA
f ) to obtain an optimal solution for I−. Preﬁxing α to the optimal solution obtained
for I− (assuming a solution exists) results in an optimal solution Sα for I under the constraint
that algorithm α is the ﬁrst algorithm. Enumeration every algorithm α ∈ A as the ﬁrst algorithm 
computing Sα  and keeping track of the solution of minimum cost over all enumerations  results in
an optimal solution for I. The running time of the above algorithm is O∗((degA
Proposition 5. CPS[test failure degree] is in FPT.
f the test failure degree  and let I = (A  T  cost  S) be an instance of CASCA-
Proof. Denote by degT
DING PORTFOLIO SCHEDULING. Consider an algorithm which (1) loops over each algorithm α ∈ A
and proceeds under the assumption that α is the last algorithm in an optimal valid portfolio τ  and
then (2) loops over every test t in our instance and proceed under the assumption that t is a test that
is solved only by α in τ. For each such choice of t and α  it follows that the algorithms preceding
α in τ do not solve t  and hence there are at most degT
f many such algorithms. Therefore  we can
check the validity and compute the cost of every possible ordered selection of a subset from these
algorithms that precede α in τ. After we ﬁnish looping over all choices of α and t  we output a valid
portfolio of minimum cost.

f )degA
f ).

f )degA

5

f )!).

There are |A| choices for a last algorithm α and |T| choices for a desired test t. For each ﬁxed α
and t  there are at most O∗((degT
f )!) many ordered selections of a subset of algorithms preceding α
in τ. It follows that the problem can be solved in time O∗((degT
Proposition 6. CPS[algorithm success degree]  CPS[test success degree]  and even CPS[algorithm
success degree + test success degree] are NP-hard already if the algorithm success degree is at most 3
and test success degree is at most 2.
Proof. We reduce from the problem 3-MIN SUM VERTEX COVER  where we are given a graph
H = (V  E) with maximum degree 3  and the task is to ﬁnd a bijection σ : V → {1  . . . .V } that
e∈E fσ(e)  where fσ(e) = minv∈e σ(v). Feige et al. [5] showed that there exists
 > 0 such that it is NP-hard to approximate 3-MIN SUM VERTEX COVER within a ratio better
than 1 + . Given an instance of this problem  we construct an instance of (A  T  cost  S) of
CASCADING PORTFOLIO SCHEDULING by letting A = V   adding for each edge e ∈ E a test te
to T   setting S = { (α  te) ∈ A × T : α ∈ e}  and setting cost(α  t) = 1 for all α ∈ A and t ∈ T . It
e∈E fσ(e) are exactly those that give an ordering τ
of A of minimal cost. It remains to observe that the the algorithm success degree is 3 and the test
success degree is 2.
5 Results for Cover Numbers
In this section we show that CPS[failure cover number] and CPS[success cover number] are both
ﬁxed-parameter tractable.

minimizes(cid:80)
is easy to verify that bijections σ that minimize(cid:80)

5.1 Using the Failure Cover Number

The ﬁrst of the two results follows from an even more general result  the ﬁxed-parameter tractability
of CPS[failure treewidth]  where as the parameter we take the treewidth of the failure graph GI
deﬁned as follows.
The failure graph GI is a bipartite graph whose vertices consist of A ∪ T and where there is an edge
between α ∈ A and t ∈ T iff t fails on A. We note that the algorithm (or test) failure degree naturally
corresponds to the maximum degree in the respective bipartition of GI  and that the failure covering
number is actually the size of a minimum vertex cover in GI.
Treewidth [21  8  1] is a well-established graph parameter that measures the “tree-likeness” of
instances. Aside from treewidth  we will also need the notion of balanced separators in graphs. We
introduce these technical notions below.
Treewidth and Separators. Let G = (V  E) be a graph. A tree decomposition of G is a pair
Xi∈V = V   and T is a rooted tree whose
node set is V  such that:
1. For every edge {u  v} ∈ E  there is an Xi ∈ V  such that {u  v} ⊆ Xi; and
2. for all Xi  Xj  Xk ∈ V  if the node Xj lies on the path between the nodes Xi and Xk in the tree

(V T ) where V is a collection of subsets of V such that(cid:83)

T   then Xi ∩ Xk ⊆ Xj.

The width of the tree decomposition (V T ) is deﬁned to be max{|Xi| | Xi ∈ V} − 1. The treewidth
of the graph G  denoted tw(G)  is the minimum width over all tree decompositions of G.
A pair of vertex subsets (A  B) is a separation in graph G if A ∪ B = V (G) and there is no edge
between A \ B and B \ A. The separator of this separation is A ∩ B  and the order of separation
(A  B) is equal to |A ∩ B|. We say that a separation (A  B) of G is an α-balanced separation if
|A \ B| ≤ α|V (G)| and |B \ A| ≤ α|V (G)|.
Proof Strategy. Our main aim in this section will be to prove the following theorem:
Theorem 7. CPS[failure treewidth] is in FPT.

It is easy to see that failure treewidth is at most the failure cover number plus 1 (consider  e.g.  a tree
decomposition of the failure graph consisting of a sequence of bags  each containing the algorithms
and tests forming the cover and one additional test or algorithm). Hence  once we establish Theorem 7
we obtain the following as an immediate corollary:
Corollary 8. CPS[failure cover number] is in FPT.

6

We ﬁrst provide below a high-level overview of the proof of Theorem 7.
We solve the problem using dynamic programming on a tree decomposition of GI  by utilizing
the upper bound on the solution length derived in the ﬁrst step. The running time is O∗(4tw(GI ) ·
tw(GI)tw(GI )). To make the dynamic programming approach work  for a current bag in the tree
decomposition  and for each test in the bag  we remember whether the test is solved by an algorithm
in the future or by an algorithm in the past. Moreover  we remember which tests are solved by the
same algorithm. We also remember speciﬁcally which algorithm is the “ﬁrst” from the future and
which is the “ﬁrst” from the past. Finally  we remember the relative positions of the algorithms in
the bag  the ﬁrst algorithm from the future  the ﬁrst algorithm from the past  and the algorithms that
solve the tests in the bag. Note that we do not remember which algorithms solve tests in the bag  only
their relative position and whether they are in the past or future.
We now turn to giving a more detailed proof for Theorem 7.

Lemma 9. A minimum cost schedule for CASCADING PORTFOLIO SCHEDULING can be computed
in time O∗(4tw · twtw).

Proof Sketch. As with virtually all ﬁxed-parameter algorithms parameterized by treewidth  we use
leaf-to-root dynamic programming along a tree decomposition (in this case of the failure graph
GI)—see for instance the numerous examples presented in the literature [4  3]. However  due to
the speciﬁc nature of our problem  the records dynamically computed by the program are far from
standard. This can already be seen by considering the size of our records: while most such dynamic
programming algorithms only store records that have size bounded by a function of the treewidth  in
our case the records will also have a polynomial dependence on m.
As a starting point  we will use the known algorithm of Bodlaender et al. [2] to compute a tree-
decomposition of width at most 5 · tw(GI). We proceed by formalizing the used records. Let Xi be
a bag in the tree decomposition. A conﬁguration w.r.t. Xi is a tuple (αpast  αfuture  σ  δ)  where
• αpast is an algorithm that has been forgotten in a descendant of Xi 
• αfuture is an algorithm that has not been introduced yet in Xi 
• σ : Xi ∪ {αpast  αfuture} → [|Xi| + 2]  and
• δ : T ∩ Xi → {“past”  “future”}.
Note that there are at most 2|Xi| · (|Xi| + 2)|Xi|+2 · m2 = O∗(2tw · twtw) conﬁgurations. The
interpretation of the conﬁguration is that σ tells us the relative positions in the ﬁnal schedule of the
algorithms in Xi  αpast  αfuture  and for each test in Xi the algorithm that ﬁnally solves the test t. The
function δ  for a test t  tells us whether the algorithm that is the ﬁrst in the schedule that solves t was
already introduced (“past”) or will be introduced (“future”). The entry αpast represents the speciﬁc
algorithm that is ﬁrst in the schedule among all algorithms that have been already forgotten in the
descendant  and αfuture that among the ones that have not been introduced yet.
We say that a conﬁguration C = (αpast  αfuture  σ  δ) w.r.t. Xi is admissible if
• for all algorithms α1  α2 ∈ A ∩ (Xi ∪ {αpast  αfuture})  it holds that σ(α1) (cid:54)= σ(α2);
• for all t ∈ T ∩ Xi if σ(t) = j  then for every j(cid:48) < j: if there is α ∈ A ∩ (Xi ∪ {αpast  αfuture})
• for all t ∈ T ∩ Xi if δ(t) = “past”  then either σ(αpast) ≤ σ(t) or there is α ∈ A ∩ Xi such that
• for all t ∈ T ∩ Xi if δ(t) = “future”  then σ(αfuture) ≤ σ(t);
• for all j(cid:48)  j ∈ [|Xi| + 2] such that j(cid:48) < j  if σ−1(j(cid:48)) = ∅  then σ−1(j) = ∅; and
• if σ(α) = σ(t) for some α ∈ A ∩ (Xi ∪ {αpast}) and t ∈ T ∩ Xi  then δ(t) = “past” and α solve

such that σ(α) = j(cid:48) then α does not solve t;

σ(α) = σ(t);

t.

Note that if we take any valid schedule  we can project it w.r.t. a bag Xi and obtain a conﬁguration
(αpast  αfuture  σ  δ). Such a conﬁguration will always be admissible and so we can restrict our attention
to admissible conﬁgurations only. To simplify the notation we let Γi[C] = ∞ if C is not an admissible
conﬁguration w.r.t. Xi.
Now for each Xi  we will compute a table Γi that contains an entry for each admissible conﬁguration
C such that Γi[C] ∈ N is the best cost  w.r.t. conﬁguration C  of the already introduced tests restricted
to the already introduced algorithms and the algorithm αfuture.

7

Clearly  the minimum cost schedule of the instance gives rise to some admissible conﬁguration C
w.r.t. the root node Xr of the tree decomposition. Hence Γr[C] contains the minimum cost of a
schedule. To complete the proof  it sufﬁces to show how to update the records when traversing the
tree-decomposition in dynamic fashion. Below  we list the sequence of claims (along with some
exemplary proofs) used to this end.
Claim 10. If Xi is a leaf node  then Γi can be computed in O(|Γi|) time.
Proof of Claim. Note that Xi = ∅ and that none of the algorithms has been introduced in any leaf
node. The only admissible conﬁguration looks like (∅  α {(α  0)} ∅)  where α ∈ A. Moreover 
since no tests or algorithms were introduced at that point  the cost of all of these conﬁgurations is
zero.

k be a conﬁguration (αpast  αfuture  σ1

k  δp) such that σ1

k(t) = k and let C 2

k be a conﬁguration (αpast  αfuture  σ2

k(t) = k. Note that σ2

k(x) = σ(x) for all x ∈ (Xi∪{αpast  αfuture})\{t} such that σ(x) < k  σ2

Claim 11. If Xi is an introduce node for a test with the only child Xj  then Γi can be computed in
O(|Γi|) time.
Claim 12. If Xi is an introduce node for an algorithm with the only child Xj  then Γi can be
computed in O(|Γi|) time.
Claim 13. If Xi is a forget node  which forgets a test t  with the only child Xj  then Γi can be
computed in O((cid:96)|Γi|) time.
Proof of Claim. Let C = (αpast  αfuture  σ  δ) be an admissible conﬁguration w.r.t. Xi. Forgetting
a test does not change the costs of introduced tests w.r.t. introduced algorithms. Hence  we only
need to ﬁnd a conﬁguration w.r.t. Xj of the lowest cost that after removing t from δ results in
C. Let δp be a function we get from δ by adding δp(t) = “past” and let δf be a function we
get from δ by adding δf (t) = “future”. First let Cf be a conﬁguration (αpast  αfuture  σf   δf ) such
that σf (x) = σ(x) for all x ∈ (Xi ∪ {αpast  αfuture}) \ {t} and σf (t) = σ(αfuture). Now  for
k ∈ [|Xi| + 2] and let C 1
k(x) = σ(x) for all
x ∈ (Xi ∪ {αpast  αfuture}) \ {t} and σ1
k  δp)
such that σ2
k(x) = σ(x)+1
for all x ∈ (Xi ∪ {αpast  αfuture}) \ {t} such that σ(x) ≥ k  and σ1
k would be
also shifted to σ after removing the entry for t.
We let Γi[C] be minimum among Cf and mink∈[|Xi|+2] (cid:96)∈{1 2} Γj[C (cid:96)
k].
Claim 14. If Xi is a forget node  which forgets an algorithm α  with the only child Xj  then Γi can
be computed in O(((cid:96) + m)|Γi|) time.
Proof of Claim. Let C = (αpast  αfuture  σ  δ) be an admissible conﬁguration w.r.t. Xi. Clearly  when
we forget an algorithm  the cost of schedule given by σ w.r.t. already introduced algorithms and tests
does not change. Hence  we just need to choose the best conﬁguration of Xj that can result in C.
We distinguish two cases depending on whether αpast = α or not.
First  if αpast = α  then for an already forgotten algorithm α(cid:48)  k ∈ [|Xi| + 2] such that σ(αpast) ≥ k 
and (cid:96) ∈ {0  1} let us denote by Cα(cid:48) k (cid:96) the conﬁguration (α(cid:48)  αfuture  σ(cid:96)
α(cid:48) k(α(cid:48)) = k 
α(cid:48) k  δ) such that σ(cid:96)
for all x ∈ Xi ∪ {αpast  αfuture} σ(cid:96)
α(cid:48) k(x) = σ(x) if σ(x) < k and σ(cid:96)
α(cid:48) k(x) = σ(x) + (cid:96) otherwise.
α(cid:48) k to be admissible  σ−1(k) contains at least one test and no algorithm. In
Note that in order for σ0
this case we let Γi[C] = minα(cid:48) k (cid:96) Γj[Cα(cid:48) k (cid:96)].
If αpast (cid:54)= α  then for k ∈ [|Xi| + 2] such that σ(αpast) < k  and (cid:96) ∈ {0  1} let us denote by Ck (cid:96) the
conﬁguration (αpast  αfuture  σ(cid:96)
k(x) = σ(x)
k to be admissible 
if σ(x) < k and σ(cid:96)
σ−1(k) contains at least one test and no algorithm. In this case we let Γi[C] = mink (cid:96) Γj[Ck (cid:96)].
Claim 15. If Xi is a join node with children Xj1 and Xj2  then Γi can be computed from Γj1 and
Γj2 in O(2(cid:96)m|Γi|) time.
To conclude  the last four claims show that it is possible to dynamically compute our records from the
leaves of a nice tree decomposition to its root; once the records are known for the root  the algorithm
has all the information it needs to output with the solution.

k(x) = σ(x) + (cid:96) otherwise. Note that again in order for σ0

k(α) = k  for all x ∈ Xi∪{αpast  αfuture} σ(cid:96)

k  δ) such that σ(cid:96)

It follows that CPS[failure treewidth] is ﬁxed-parameter tractable  hence establishing Theorem 7.

8

5.2 Using the Success Cover Number

The aim of this section is to establish the ﬁxed-parameter tractability of CPS[success cover number] 
which can be viewed as a dual result to Corollary 8. The techniques used to obtain this result are
entirely different from those used in the previous subsection; in particular  the proof is based on a
signiﬁcant extension of the ideas introduced in the proof of Proposition 1.
Theorem 16. CPS[success cover number] is in FPT.
Proof Sketch. Let I be an instance of CPS[covs]. Our ﬁrst step is to compute a witness for the
success cover number covs  i.e.  a set of algorithms A(cid:48) and tests T (cid:48) such that |A(cid:48) ∪ T (cid:48)| = covs and
each pair in S has a non-empty intersection with A(cid:48) ∪ T (cid:48); as discussed in Subsection 2  this can be
done in polynomial time [7  Proposition 1]. Let V = 2A(cid:48)∪T (cid:48)
be the set of all subsets of covs. We will
construct a directed arc-weighted graph D with vertex set V ∪ {x}  and with the property that each
shortest path from ∅ to x precisely corresponds to a minimum-cost schedule for the input instance I.
Intuitively  reaching a vertex v in D which corresponds to a certain set of algorithms A0 and tests T0
means that the schedule currently contains the algorithms in A0 plus an optimal choice of algorithms
which can process the remaining tests in T0; information about the ordering inside the schedule is not
encoded by the vertex v itself  but rather by the path from ∅ to v.
In order to implement this idea  we will add the following arcs to D. To simplify the description  let
A∗ be an arbitrary subset of A(cid:48) and T ∗ be an arbitrary subset of T (cid:48). First of all  for each A∗ such that
for every test t ∈ T \ T (cid:48) there is some α ∈ A∗ satisfying (α  t) ∈ S  we add the arc (A∗ ∪ T (cid:48)  x)
and assign it a weight of 0. This is done to indicate that A∗ ∪ T (cid:48) corresponds to a valid schedule.
Second  for each A∗ that is a proper subset of A(cid:48)  α0 ∈ A(cid:48) \ A∗  and T ∗  we add the arc e from
A∗ ∪ T ∗ to A∗ ∪ {α0} ∪ T ∗ ∪ T0  where T0 contains every test t0 ∈ T (cid:48) such that (α0  t0) ∈ S. In
order to compute the weight of this arc e  we ﬁrst compute the set Te of all tests outside of T ∗ where
α0 will be queried (assuming α0 is added to the schedule at this point); formally  t ∈ Te if t (cid:54)∈ T ∗
and for each α(cid:48) ∈ A∗ it holds that (α(cid:48)  t) (cid:54)∈ S. For clarity  observe that T0 ⊆ Te. Now  we set the

weight of e to(cid:80)

cost(α0  t).

t(cid:54)∈T (cid:48):∀α∈A∗:(α t)(cid:54)∈S cost(αλ  t)(cid:1).

(cid:0)(cid:80)
t∈(T (cid:48)\T ∗) cost(αλ  t)(cid:1) +(cid:0)(cid:80)

t∈Te
To add our third and ﬁnal set of edges  we ﬁrst pre-compute for each Tλ ⊆ T (cid:48) \ T ∗ an algorithm
αλ ∈ A \ A(cid:48) such that:
1. for each tλ (cid:54)∈ T ∗  (αλ  tλ) ∈ S iff tλ ∈ Tλ (i.e.  αλ successfully solves exactly Tλ)  and
2. among all possible algorithms satisfying the above condition  αλ achieves the minimum
cost for all as-of-yet-unprocessed tests. Formally  αλ minimizes the term price(αλ) =
Now  we add an arc e from each A∗ ∪ T ∗ to each A∗ ∪ T ∗ ∪ Tλ  where Tλ is deﬁned as above and
associated with the test αλ. The weight of e is precisely the value price(αλ).
s + 1 many vertices  a shortest path P from ∅ to x in D can be
Note that since the graph D has 2cov
computed in time 2O(covs). Moreover  it is easy to verify that D can be constructed from an instance
I in time at most 2O(covs) · |I|2. At this point  it remains to verify that a shortest ∅-x path P in D
can be used to obtain a solution for I.
6 Conclusion
We studied the parameterized complexity of the CASCADING PORTFOLIO SCHEDULING problem
under various parameters. We identiﬁed several settings where the NP-hardness of the problem can
be circumvented via exact ﬁxed-parameter algorithms  including cases where (i) the algorithms have
a small failure degree  (ii) the tests have a small failure degree  (iii) the evaluation matrix has a small
failure cover  and (iv) the evaluation matrix has a small success cover. The ﬁrst three cases can be
seen as settings in which most algorithms succeed on most of the tests  whereas case (iv) can be seen
as a setting where most algorithms fail.
We have complemented our algorithmic results with hardness results which allowed us to draw a
detailed complexity landscape of the problem. We would like to point out that all our hardness results
hold even when all costs are unit costs. This ﬁnding is signiﬁcant  as it reveals that the complexity of
the problem mainly depends on the success relation and not on the cost mapping.
For future work  it would be interesting to extend our study to the more complex setting where up to
p algorithms from the portfolio can be run in parallel. Here  the number p could be seen as a natural
additional parameter.

9

Acknowledgments

Robert Ganian acknowledges the support by the Austrian Science Fund (FWF)  Project P 31336  and
is also afﬁliated with FI MUNI  Brno  Czech Republic. Stefan Szeider acknowledges the support by
the Austrian Science Fund (FWF)  Project P 32441.
References
[1] H. L. Bodlaender. Discovering treewidth. In Proceedings of the 31st Conference on Current
Trends in Theory and Practice of Computer Science (SOFSEM’05)  volume 3381 of Lecture
Notes in Computer Science  pages 1–16. Springer Verlag  2005.

[2] Hans L. Bodlaender  Pål Grønås Drange  Markus S. Dregi  Fedor V. Fomin  Daniel Lokshtanov 
and Michal Pilipczuk. A ckn 5-approximation algorithm for treewidth. SIAM J. Comput. 
45(2):317–378  2016.

[3] M. Cygan  F. Fomin  L. Kowalik  D. Lokshtanov  D. Marx  M. Pilipczuk  M. Pilipczuk  and

S. Saurabh. Parameterized Algorithms. Springer  2015.

[4] Rodney G. Downey and Michael R. Fellows. Fundamentals of Parameterized Complexity. Texts

in Computer Science. Springer  2013.

[5] Uriel Feige  László Lovász  and Prasad Tetali. Approximating min sum set cover. Algorithmica 

40(4):219–234  2004.

[6] Jörg Flum and Martin Grohe. Parameterized Complexity Theory  volume XIV of Texts in

Theoretical Computer Science. An EATCS Series. Springer Verlag  Berlin  2006.

[7] Robert Ganian  Iyad Kanj  Sebastian Ordyniak  and Stefan Szeider. Parameterized algorithms
In Proceeding of ICML  the Thirty-ﬁfth International
for the matrix completion problem.
Conference on Machine Learning  Stockholm  July 10–15  2018  pages 1642–1651. JMLR.org 
2018. ISSN: 1938-7228.

[8] Georg Gottlob  Reinhard Pichler  and Fang Wei. Bounded treewidth as a key to tractability of

knowledge representation and reasoning. Artiﬁcial Intelligence  174(1):105–132  2010.

[9] Georg Gottlob and Stefan Szeider. Fixed-parameter algorithms for artiﬁcial intelligence 
constraint satisfaction  and database problems. The Computer Journal  51(3):303–325  2006.
Survey paper.

[10] Holger H. Hoos  Tomáš Peitl  Friedrich Slivovsky  and Stefan Szeider. Portfolio-based algorithm
In John N. Hooker  editor  Proceedings of CP 2018  the 24rd
selection for circuit QBFs.
International Conference on Principles and Practice of Constraint Programming  volume
11008 of Lecture Notes in Computer Science  pages 195–209. Springer Verlag  2018.

[11] Shinji Ito  Daisuke Hatano  Hanna Sumita  Akihiro Yabe  Takuro Fukunaga  Naonori Kakimura 
and Ken-ichi Kawarabayashi. Regret bounds for online portfolio selection with a cardinality
constraint. In Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal 
Canada.  pages 10611–10620  2018.

[12] Pascal Kerschke  Holger H. Hoos  Frank Neumann  and Heike Trautmann. Automated algorithm

selection: Survey and perspectives. Evolutionary Computation  pages 1–47  2018.

[13] Lars Kotthoff. Algorithm selection for combinatorial search problems: A survey. AI Magazine 

35(3):48–60  2014.

[14] Marius Lindauer  Holger Hoos  Frank Hutter  and Kevin Leyton-Brown. Selection and conﬁgu-
ration of parallel portfolios. In Handbook of Parallel Constraint Reasoning.  pages 583–615.
2018.

[15] Marius Lindauer  Frank Hutter  Holger H. Hoos  and Torsten Schaub. Autofolio: An automati-
cally conﬁgured algorithm selector (extended abstract). In Carles Sierra  editor  Proceedings
of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence  IJCAI 2017  Mel-
bourne  Australia  August 19-25  2017  pages 5025–5029. ijcai.org  2017.

10

[16] Haipeng Luo  Chen-Yu Wei  and Kai Zheng. Efﬁcient online portfolio with logarithmic regret.
In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada. 
pages 8245–8255  2018.

[17] Rolf Niedermeier. Invitation to Fixed-Parameter Algorithms. Oxford Lecture Series in Mathe-

matics and its Applications. Oxford University Press  Oxford  2006.

[18] Luca Pulina and Armando Tacchella. A self-adaptive multi-engine solver for quantiﬁed boolean

formulas. Constraints  14(1):80–116  2009.

[19] John R. Rice. The algorithm selection problem. Advances in Computers  15:65–118  1976.

[20] Mattia Rizzini  Chris Fawcett  Mauro Vallati  Alfonso Emilio Gerevini  and Holger H. Hoos.
Static and dynamic portfolio methods for optimal planning: An empirical analysis. International
Journal on Artiﬁcial Intelligence Tools  26(1):1–27  2017.

[21] Neil Robertson and Paul D. Seymour. Graph minors. III. planar tree-width. J. Comb. Theory 

Ser. B  36(1):49–64  1984.

[22] Olivier Roussel. Description of ppfolio 2012. In et al. A. Balint  editor  Proceedings of SAT

Challenge 2012  page 47. University of Helsinki  2012.

[23] Matthew Streeter. Approximation algorithms for cascading prediction models. In Jennifer G.
Dy and Andreas Krause  editors  Proceedings of the 35th International Conference on Machine
Learning  ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  volume 80 of
JMLR Workshop and Conference Proceedings  pages 4759–4767. JMLR.org  2018.

11

,Eduard Eiben
Robert Ganian
Iyad Kanj
Stefan Szeider