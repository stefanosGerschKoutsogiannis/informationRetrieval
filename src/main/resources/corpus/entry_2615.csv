2009,Indian Buffet Processes with Power-law Behavior,The Indian buffet process (IBP) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models.  In this paper we propose a three-parameter generalization of the IBP exhibiting power-law behavior.  We achieve this by generalizing the beta process (the de Finetti measure of the IBP) to the \emph{stable-beta process} and deriving the IBP corresponding to it.  We find interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties).  We show that our power-law IBP is a good model for word occurrences in documents with improved performance over the normal IBP.,Indian Buffet Processes with Power-law Behavior

Yee Whye Teh and Dilan G¨or¨ur

17 Queen Square  London WC1N 3AR  United Kingdom

Gatsby Computational Neuroscience Unit  UCL
{ywteh dilan}@gatsby.ucl.ac.uk

Abstract

The Indian buffet process (IBP) is an exchangeable distribution over binary ma-
trices used in Bayesian nonparametric featural models. In this paper we propose
a three-parameter generalization of the IBP exhibiting power-law behavior. We
achieve this by generalizing the beta process (the de Finetti measure of the IBP) to
the stable-beta process and deriving the IBP corresponding to it. We ﬁnd interest-
ing relationships between the stable-beta process and the Pitman-Yor process (an-
other stochastic process used in Bayesian nonparametric models with interesting
power-law properties). We derive a stick-breaking construction for the stable-beta
process  and ﬁnd that our power-law IBP is a good model for word occurrences in
document corpora.

1 Introduction

The Indian buffet process (IBP) is an inﬁnitely exchangeable distribution over binary matrices with
a ﬁnite number of rows and an unbounded number of columns [1  2]. It has been proposed as a
suitable prior for Bayesian nonparametric featural models  where each object (row) is modeled with
a potentially unbounded number of features (columns). Applications of the IBP include Bayesian
nonparametric models for ICA [3]  choice modeling [4]  similarity judgements modeling [5]  dyadic
data modeling [6] and causal inference [7].
In this paper we propose a three-parameter generalization of the IBP with power-law behavior. Using
the usual analogy of customers entering an Indian buffet restaurant and sequentially choosing dishes
from an inﬁnitely long buffet counter  our generalization with parameters α > 0  c > −σ and
σ ∈ [0  1) is simply as follows:

• Customer 1 tries Poisson(α) dishes.
• Subsequently  customer n + 1:

– tries dish k with probability mk−σ
– tries Poisson(α Γ(1+c)Γ(n+c+σ)

Γ(n+1+c)Γ(c+σ)) new dishes.

n+c   for each dish that has previously been tried;

where mk is the number of previous customers who tried dish k. The dishes and the customers
correspond to the columns and the rows of the binary matrix respectively  with an entry of the matrix
being one if the corresponding customer tried the dish (and zero otherwise). The mass parameter α
controls the total number of dishes tried by the customers  the concentration parameter c controls
the number of customers that will try each dish  and the stability exponent σ controls the power-law
behavior of the process. When σ = 0 the process does not exhibit power-law behavior and reduces
to the usual two-parameter IBP [2].
Many naturally occurring phenomena exhibit power-law behavior  and it has been argued that using
models that can capture this behavior can improve learning [8]. Recent examples where this has led
to signiﬁcant improvements include unsupervised morphology learning [8]  language modeling [9]

1

and image segmentation [10]. These examples are all based on the Pitman-Yor process [11  12  13] 
a generalization of the Dirichlet process [14] with power-law properties. Our generalization of the
IBP extends the ability to model power-law behavior to featural models  and we expect it to lead to
a wealth of novel applications not previously well handled by the IBP.
The approach we take in this paper is to ﬁrst deﬁne the underlying de Finetti measure  then to derive
the conditional distributions of Bernoulli process observations with the de Finetti measure integrated
out. This automatically ensures that the resulting power-law IBP is inﬁnitely exchangeable. We call
the de Finetti measure of the power-law IBP the stable-beta process. It is a novel generalization of
the beta process [15] (which is the de Finetti measure of the normal two-parameter IBP [16]) with
characteristics reminiscent of the stable process [17  11] (in turn related to the Pitman-Yor process).
We will see that the stable-beta process has a number of properties similar to the Pitman-Yor process.
In the following section we ﬁrst give a brief description of completely random measures  a class of
random measures which includes the stable-beta and the beta processes. In Section 3 we introduce
the stable-beta process  a three parameter generalization of the beta process and derive the power-
law IBP based on the stable-beta process. Based on the proposed model  in Section 4 we construct
a model of word occurrences in a document corpus. We conclude with a discussion in Section 5.

2 Completely Random Measures

In this section we give a brief description of completely random measures [18]. Let Θ be a measure
space with Ω its σ-algebra. A random variable whose values are measures on (Θ  Ω) is referred
to as a random measure. A completely random measure (CRM) µ over (Θ  Ω) is a random mea-
sure such that µ(A)⊥⊥µ(B) for all disjoint measurable subsets A  B ∈ Ω. That is  the (random)
masses assigned to disjoint subsets are independent. An important implication of this property is
that the whole distribution over µ is determined (with usually satisﬁed technical assumptions) once
the distributions of µ(A) are given for all A ∈ Ω.
CRMs can always be decomposed into a sum of three independent parts: a (non-random) measure 
an atomic measure with ﬁxed atoms but random masses  and an atomic measure with random atoms
and masses. CRMs in this paper will only contain the second and third components. In this case we
can write µ in the form 

N(cid:88)

k=1

M(cid:88)

l=1

µ =

ukδφk +

vlδψl  

(1)

where uk  vl > 0 are the random masses  φk ∈ Θ are the ﬁxed atoms  ψl ∈ Θ are the random atoms 
and N  M ∈ N∪{∞}. To describe µ fully it is sufﬁcient to specify N and {φk}  and to describe the
joint distribution over the random variables {uk} {vl} {ψl} and M. Each uk has to be independent
from everything else and has some distribution Fk. The random atoms and their weights {vl  ψl}
are jointly drawn from a 2D Poisson process over (0 ∞] × Θ with some nonatomic rate measure
Λ called the L´evy measure. The rate measure Λ has to satisfy a number of technical properties; see
(0 ∞] Λ(du × dθ) = M∗ < ∞ then the number of random atoms M in µ
is Poisson distributed with mean M∗  otherwise there are an inﬁnite number of random atoms. If µ
is described by Λ and {φk  Fk}N

[18  19] for details. If(cid:82)

k=1 as above  we write 

(cid:82)

Θ

µ ∼ CRM(Λ {φk  Fk}N

k=1).

(2)

3 The Stable-beta Process

In this section we introduce a novel CRM called the stable-beta process (SBP). It has no ﬁxed atoms
while its L´evy measure is deﬁned over (0  1) × Θ:

Γ(1 + c)

Λ0(du × dθ) = α

(3)
where the parameters are: a mass parameter α > 0  a concentration parameter c > −σ  a stability
exponent 0 ≤ σ < 1  and a smooth base distribution H. The mass parameter controls the overall
mass of the process and the base distribution gives the distribution over the random atom locations.

Γ(1 − σ)Γ(c + σ) u−σ−1(1 − u)c+σ−1duH(dθ)

2

The mean of the SBP can be shown to be E[µ(A)] = αH(A) for each A ∈ Ω  while var(µ(A)) =
α 1−σ
1+c H(A). Thus the concentration parameter and the stability exponent both affect the variability
of the SBP around its mean. The stability exponent also governs the power-law behavior of the SBP.
When σ = 0 the SBP does not have power-law behavior and reduces to a normal two-parameter beta
process [15  16]. When c = 1 − σ the stable-beta process describes the random atoms with masses
< 1 in a stable process [17  11]. The SBP is so named as it can be seen as a generalization of both
the stable and the beta processes. Both the concentration parameter and the stability exponent can
be generalized to functions over Θ though we will not deal with this generalization here.

3.1 Posterior Stable-beta Process

Consider the following hierarchical model:
µ ∼ CRM(Λ0 {}) 
Zi|µ ∼ BernoulliP(µ)

iid  for i = 1  . . .   n.

(4)
The random measure µ is a SBP with no ﬁxed atoms and with L´evy measure (3)  while Zi ∼
BernoulliP(µ) is a Bernoulli process with mean µ [16]. This is also a CRM: in a small neighborhood
dθ around θ ∈ Θ it has a probability µ(dθ) of having a unit mass atom in dθ; otherwise it does not
have an atom in dθ. If µ has an atom at θ the probability of Zi having an atom at θ as well is µ({θ}).
If µ has a smooth component  say µ0  Zi will have random atoms drawn from a Poisson process
with rate measure µ0. In typical applications to featural models the atoms in Zi give the features
associated with data item i  while the weights of the atoms in µ give the prior probabilities of the
corresponding features occurring in a data item.
We are interested in both the posterior of µ given Z1  . . .   Zn  as well as the conditional distribu-
tion of Zn+1|Z1  . . .   Zn with µ marginalized out. Let θ∗
K be the K unique atoms among
Z1  . . .   Zn with atom θ∗
k occurring mk times. Theorem 3.3 of [20] shows that the posterior of µ
given Z1  . . .   Zn is still a CRM  but now including ﬁxed atoms given by θ∗
K. Its updated
L´evy measure and the distribution of the mass at each ﬁxed atom θ∗
k  Fnk}K

µ|Z1  . . .   Zn ∼ CRM(Λn {θ∗

k are 
k=1) 

1  . . .   θ∗

1  . . .   θ∗

(5)

(6a)

(6b)

where

Λn(du × dθ) =α

Fnk(du) =

Γ(1 + c)

Γ(1 − σ)Γ(c + σ) u−σ−1(1 − u)n+c+σ−1duH(dθ) 
Γ(mk − σ)Γ(n − mk + c + σ) umk−σ−1(1 − u)n−mk+c+σ−1du.

Γ(n + c)

Intuitively  the posterior is obtained as follows. Firstly  the posterior of µ must be a CRM since
both the prior of µ and the likelihood of each Zi|µ factorize over disjoint subsets of Θ. Secondly 
µ must have ﬁxed atoms at each θ∗
k since otherwise the probability that there will be atoms among
Z1  . . .   Zn at precisely θ∗
k is zero. The posterior mass at θ∗
k is obtained by multiplying a Bernoulli
“likelihood” umk(1 − u)n−mk (since there are mk occurrences of the atom θ∗
k among Z1  . . .   Zn)
to the “prior” Λ0(du×dθ∗
k) in (3) and normalizing  giving us (6b). Finally  outside of these K atoms
there are no other atoms among Z1  . . .   Zn. We can think of this as n observations of 0 among n
iid Bernoulli variables  so a “likelihood” of (1 − u)n is multiplied into Λ0 (without normalization) 
giving the updated L´evy measure in (6a).
Let us inspect the distributions (6) of the ﬁxed and random atoms in the posterior µ in turn. The
k has a distribution Fnk which is simply a beta distribution with parameters (mk −
random mass at θ∗
σ  n − mk + c + σ). This differs from the usual beta process in the subtraction of σ from mk and
addition of σ to n − mk + c. This is reminiscent of the Pitman-Yor generalization to the Dirichlet
process [11  12  13]  where a discount parameter is subtracted from the number of customers seated
around each table  and added to the chance of sitting at a new table. On the other hand  the L´evy
measure of the random atoms of µ is still a L´evy measure corresponding to an SBP with updated
parameters

Γ(1 + c)Γ(n + c + σ)
Γ(n + 1 + c)Γ(c + σ)  

α(cid:48) ← α
c(cid:48) ← c + n 

σ(cid:48) ← σ
H(cid:48) ← H.

(7)

3

Note that the update depends only on n  not on Z1  . . .   Zn. In summary  the posterior of µ is simply
an independent sum of an SBP with updated parameters and of ﬁxed atoms with beta distributed
masses. Observe that the posterior µ is not itself a SBP. In other words  the SBP is not conjugate
to Bernoulli process observations. This is different from the beta process and again reminiscent
of Pitman-Yor processes  where the posterior is also a sum of a Pitman-Yor process with updated
parameters and ﬁxed atoms with random masses  but not a Pitman-Yor process [11]. Fortunately 
the non-conjugacy of the SBP does not preclude efﬁcient inference. In the next subsections we de-
scribe an Indian buffet process and a stick-breaking construction corresponding to the SBP. Efﬁcient
inference techniques based on both representations for the beta process can be straightforwardly
generalized to the SBP [1  16  21].

3.2 The Stable-beta Indian Buffet Process

We can derive an Indian buffet process (IBP) corresponding to the SBP by deriving  for each n 
the distribution of Zn+1 conditioned on Z1  . . .   Zn  with µ marginalized out. This derivation is
straightforward and follows closely that for the beta process [16]. For each of the atoms θ∗
k the
posterior of µ(θ∗

k) given Z1  . . .   Zn is beta distributed with mean mk−σ
p(Zn+1(θ∗

k) = 1|Z1  . . .   Zn) = E[µ(θ∗

k)|Z1  . . .   Zn] = mk − σ

n+c . Thus

(8)

n + c

Metaphorically speaking  customer n + 1 tries dish k with probability mk−σ
atoms. Let θ ∈ Θ\{θ∗

K}. In a small neighborhood dθ around θ  we have:

1  . . .   θ∗

n+c . Now for the random

(cid:90) 1

0

uΛn(du × dθ)

(cid:90) 1

p(Zn+1(dθ) = 1|Z1  . . .   Zn) = E[µ(dθ)|Z1  . . .   Zn] =
Γ(1 − σ)Γ(c + σ) u−1−σ(1 − u)n+c+σ−1duH(dθ)
Γ(1 + c)

(cid:90) 1

Γ(1 + c)

uα

u−σ(1 − u)n+c+σ−1du

0

=

=α

Γ(1 − σ)Γ(c + σ) H(dθ)
Γ(1 + c)Γ(n + c + σ)
Γ(n + 1 + c)Γ(c + σ) H(dθ)

0

(cid:18)

n(cid:88)

(cid:19) K(cid:89)

=α

(9)
K}
Since Zn+1 is completely random and H is smooth  the above shows that on Θ\{θ∗
1  . . .   θ∗
Zn+1 is simply a Poisson process with rate measure α Γ(1+c)Γ(n+c+σ)
Γ(n+1+c)Γ(c+σ) H. In particular  it will have
Poisson(α Γ(1+c)Γ(n+c+σ)
Γ(n+1+c)Γ(c+σ)) new atoms  each independently and identically distributed according to
H. In the IBP metaphor  this corresponds to customer n+1 trying new dishes  with each dish associ-
ated with a new draw from H. The resulting Indian buffet process is as described in the introduction.
It is automatically inﬁnitely exchangeable since it was derived from the conditional distributions of
the hierarchical model (4).
Multiplying the conditional probabilities of each Zn given previous ones together  we get the joint
probability of Z1  . . .   Zn with µ marginalized out:

p(Z1  . . .   Zn) = exp

−α

Γ(1+c)Γ(i−1+c+σ)

Γ(i+c)Γ(c+σ)

Γ(mk−σ)Γ(n−mk+c+σ)Γ(1+c)

Γ(1−σ)Γ(c+σ)Γ(n+c)

αh(θ∗

k)  (10)

i=1

k=1

where there are K atoms (dishes) θ∗
K among Z1  . . .   Zn with atom k appearing mk times 
and h is the density of H. (10) is to be contrasted with (4) in [1]. The Kh! terms in [1] are absent
as we have to distinguish among these Kh dishes in assigning each of them a distinct atom (this
also contributes the h(θ∗
k) terms). The fact that (10) is invariant to permuting the ordering among
Z1  . . .   Zn also indicates the inﬁnite exchangeability of the stable-beta IBP.

1  . . .   θ∗

3.3 Stick-breaking constructions

In this section we describe stick-breaking constructions for the SBP generalizing those for the beta
process. The ﬁrst is based on the size-biased ordering of atoms induced by the IBP [16]  while

4

the second is based on the inverse L´evy measure method [22]  and produces a sequence of random
atoms of strictly decreasing masses [21].
The size-biased construction is straightforward: we use the IBP to generate the atoms (dishes) in the
SBP; each time a dish is newly generated the atom is drawn from H and its mass from Fnk. This
leads to the following procedure:

for n = 1  2  . . .:
for k = 1  . . .   Jn:

Jn ∼ Poisson(α Γ(1+c)Γ(n−1+c+σ)
vnk ∼ Beta(1 − σ  n − 1 + c + σ) 

Γ(n+c)Γ(c+σ)

) 

ψnk ∼ H 

(11)

∞(cid:88)

Jn(cid:88)

n=1

k=1

µ =

vnkδψnk .

Γ(1+c)

The inverse L´evy measure is a general method of generating from a Poisson process with non-
uniform rate measure.
It essentially transforms the Poisson process into one with uniform rate 
generates a sample  and transforms the sample back. This method is more involved for the
SBP because the inverse transform has no analytically tractable form. The L´evy measure Λ0 of
the SBP factorizes into a product Λ0(du× dθ) = L(du)H(dθ) of a σ-ﬁnite measure L(du) =
Γ(1−σ)Γ(c+σ) u−σ−1(1−u)c+σ−1du over (0  1) and a probability measure H over Θ. This implies
α
that we can generate a sample {vl  ψl}∞
l=1 of the random atoms of µ and their masses by ﬁrst sam-
l=1 ∼ PoissonP(L) from a Poisson process on (0  1) with rate measure L  and
pling the masses {vl}∞
associating each vl with an iid draw ψl ∼ H [19]. Now consider the mapping T : (0  1) → (0 ∞)
(cid:90) 1
given by

(cid:90) 1

T (u) =

L(du) =

u

u

α

Γ(1 + c)

Γ(1 − σ)Γ(c + σ) u−σ−1(1 − u)c+σ−1du.

(12)

l=1 ∼ PoissonP(L) if and only if {T (vl)}∞

T is bijective and monotonically decreasing. The Mapping Theorem for Poisson processes [19]
shows that {vl}∞
l=1 ∼ PoissonP(L) where L is
l=1 ∼ PoissonP(L) can be easily drawn by letting
Lebesgue measure on (0 ∞). A sample {tl}∞
i=1 ei for all l. Transforming back with vl = T −1(tl) 
l=1 ∼ PoissonP(L). As t1  t2  . . . is an increasing sequence and T is decreasing 
we have {vl}∞
v1  v2  . . . is a decreasing sequence of masses. Deriving the density of vl given vl−1  we get:

el ∼ Exponential(1) and setting tl = (cid:80)l
p(vl|vl−1) =(cid:12)(cid:12) dtl

(cid:12)(cid:12)p(tl|tl−1) = α

(1−vl)c+σ−1 exp

Γ(1−σ)Γ(c+σ) v−σ−1

(cid:90) vl−1

(cid:110)−

. (13)

L(du)

(cid:111)

Γ(1+c)

l

dvl

vl

In general these densities do not simplify and we have to resort to solving for T −1(tl) numerically.
There are two cases for which they do simplify. For c = 1  σ = 0  the density function reduces to
p(vl|vl−1) = αvα−1
l−1  leading to the stick-breaking construction of the single parameter IBP
[21]. In the stable process case when c = 1 − σ and σ (cid:54)= 0  the density of vl simpliﬁes to:

/vα

l

p(vl | vl−1) = α Γ(2−σ)

Γ(1−σ)Γ(1) v−σ−1
exp

= α(1 − σ)v−σ−1

l

l

Doing a change of values to yl = v−σ

l

  we get:
p(yl|yl−1) = α 1−σ

σ exp

(cid:111)
α Γ(2−σ)
Γ(1−σ)Γ(1) u−σ−1du

× exp

vl

(cid:110) −(cid:82) vl−1
(cid:110) − α(1−σ)
(cid:110) − α 1−σ

l − v−σ
(v−σ
l−1)

(cid:111)
(cid:111)
σ (yl − yl−1)

σ

.

.

(14)

(15)

That is  each yl is exponentially distributed with rate α 1−σ
and offset by yl−1. For general values
of the parameters we do not have an analytic stick breaking form. However note that the weights
generated using this method are still going to be strictly decreasing.

σ

3.4 Power-law Properties

The SBP has a number of appealing power-law properties. In this section we shall assume σ > 0
since the case σ = 0 reduces the SBP to the usual beta process with less interesting power-law
properties. Derivations are given in the appendix.

5

Figure 1: Power-law properties of the stable-beta Indian buffet process.

Firstly  the total number of dishes tried by n customers is O(nσ). The left panel of Figure 1 shows
this for varying σ. Secondly  the number of customers trying each dish follows a Zipf’s law [23].
This is shown in the right panel of Figure 1  which plots the number of dishes Km versus the
number of customers m trying each dish (that is  Km is the number of dishes k for which mk = m).
Asymptotically we can show that the proportion of dishes tried by m customers is O(m−1−σ). Note
that these power-laws are similar to those observed for Pitman-Yor processes. One aspect of the
SBP which is not power-law is the number of dishes each customer tries. This is simply Poisson(α)
distributed. It seems difﬁcult obtain power-law behavior in this aspect within a CRM framework 
because of the fundamental role played by the Poisson process.

4 Word Occurrence Models with Stable-beta Processes

In this section we use the SBP as a model for word occurrences in document corpora. Let n be
the number of documents in a corpus. Let Zi({θ}) = 1 if word type θ occurs in document i and
0 otherwise  and let µ({θ}) be the occurrence probability of word type θ among the documents
in the corpus. We use the hierarchical model (4) with a SBP prior1 on µ and with each document
modeled as a conditionally independent Bernoulli process draw. The joint distribution over the word
occurrences Z1  . . .   Zn  with µ integrated out  is given by the IBP joint probability (10).
We applied the word occurrence model to the 20newsgroups dataset. Following [16]  we modeled
the training documents in each of the 20 newsgroups as a separate corpus with a separate SBP. We
use the popularity of each word type across all 20 newsgroups as the base distribution2: for each
word type θ let nθ be the number of documents containing θ and let H({θ}) ∝ nθ.
In the ﬁrst experiment we compared the SBP to the beta process by ﬁtting the parameters α  c and
σ of both models to each newsgroup by maximum likelihood (in beta process case σ is ﬁxed at
0) . We expect the SBP to perform better as it is better able to capture the power-law statistics of
the document corpora (see Figure 2). The ML values of the parameters across classes did not vary
much  taking values α = 142.6 ± 40.0  c = 4.1 ± 0.9 and σ = 0.47 ± 0.1. In comparison  the
parameters values obtained by the beta process are α = 147.3 ± 41.4 and c = 25.9 ± 8.4. Note that
the estimated values for c are signiﬁcantly larger than for the SBP to allow the beta process to model
the fact that many words occur in a small number of documents (a consequence of the power-law

1Words are discrete objects. To get a smooth base distribution we imagine appending each word type with

a U [0  1] variate. This does not affect the modelling that follows.

2The appropriate technique  as proposed by [16]  would be to use a hierarchical SBP to tie the word occur-
rence probabilities across the newsgroups. However due to difﬁculties dealing with atomic base distributions
we cannot deﬁne a hierarchical SBP easily (see discussion).

6

100102104106100101102103104105number of customersmean number of dishes tried!=1  c=1  "=0.8"=0.5"=0.2"=0100102104100101102103104number of customers trying each dishnumber of dishes!=1  c=1  "=0.5Figure 2: Power-law properties of the 20newsgroups dataset. The faint dashed lines are the distribu-
tions of words in the documents in each class  the solid curve is the mean of these lines. The dashed
lines are the means of the word distributions generated by the ML parameters for the beta process
(pink) and the SBP (green).

Table 1: Classiﬁcation performance of SBP and beta process (BP). The jth column (denoted 1:j)
shows the cumulative rank j classiﬁcation accuracy of the test documents. The three numbers after
the models are the percentages of training  validation and test sets respectively.
1:4

assigned to classes:

1:5

1

1:2

1:3

BP - 20/20/60
SBP - 20/20/60
BP - 60/20/20
SBP - 60/20/20

78.7(±0.5)
79.9(±0.5)
85.5(±0.6)
85.5(±0.4)

87.4(±0.2)
87.6(±0.1)
91.6(±0.3)
91.9(±0.4)

91.3(±0.2)
91.5(±0.2)
94.2(±0.3)
94.4(±0.2)

95.1(±0.2)
93.7(±0.2)
95.6(±0.4)
95.6(±0.3)

96.2(±0.2)
95.1(±0.2)
96.6(±0.3)
96.6(±0.3)

statistics of word occurrences; see Figure 2). We also plotted the characteristics of data simulated
from the models using the estimated ML parameters. The SBP has a much better ﬁt than the beta
process to the power-law properties of the corpora.
In the second experiment we tested the two models on categorizing test documents into one of the
20 newsgroups. Since this is a discriminative task  we optimized the parameters in both models to
maximize the cumulative ranked classiﬁcation performance. The rank j classiﬁcation performance
is deﬁned to be the percentage of documents where the true label is among the top j predicted classes
(as determined by the IBP conditional probabilities of the documents under each of the 20 newsgroup
classes). As the cost function is not differentiable  we did a grid search over the parameter space 
using 20 values of α  c and σ each  and found the parameters maximizing the objective function on
a validation set separate from the test set. To see the effect of sample size on model performance we
tried splitting the documents in each newsgroup into 20% training  20% validation and 60% test sets 
and into 60% training  20% validation and 20% test sets. We repeated the experiment ﬁve times with
different random splits of the dataset. The ranked classiﬁcation rates are shown in Table 1. Figure 3
shows that the SBP model has generally higher classiﬁcation performances than the beta process.

5 Discussion

We have introduced a novel stochastic process called the stable-beta process. The stable-beta process
is a generalization of the beta process  and can be used in nonparametric Bayesian featural models
with an unbounded number of features. As opposed to the beta process  the stable-beta process has
a number of appealing power-law properties. We developed both an Indian buffet process and a
stick-breaking construction for the stable-beta process and applied it to modeling word occurrences
in document corpora. We expect the stable-beta process to ﬁnd uses modeling a range of natural
phenomena with power-law properties.

7

1002003004005002000400060008000100001200014000number of documentscumulative number of words  BPSBPDATA100101102100101102103number of documents per wordnumber of words  BPSBPDATAFigure 3: Differences between the classiﬁcation rates of the SBP and the beta process. The perfor-
mance of the SBP was consistently higher than that of the beta process for each of the ﬁve runs.

We derived the stable-beta process as a completely random measure with L´evy measure (3).
It
would be interesting and illuminating to try to derive it as an inﬁnite limit of ﬁnite models  however
we were not able to do so in our initial attempts. A related question is whether there is a natural
deﬁnition of the stable-beta process for non-smooth base distributions. Until this is resolved in the
positive  we are not able to deﬁne hierarchical stable-beta processes generalizing the hierarchical
beta processes [16].
Another avenue of research we are currently pursuing is in deriving better stick-breaking construc-
tions for the stable-beta process. The current construction requires inverting the integral (12)  which
is expensive as it requires an iterative method which evaluates the integral numerically within each
iteration.

Acknowledgement

We thank the Gatsby Charitable Foundation for funding  Romain Thibaux  Peter Latham and Tom
Grifﬁths for interesting discussions  and the anonymous reviewers for help and feedback.

A Derivation of Power-law Properties
We will make large n and K assumptions here  and make use of Stirling’s approximation Γ(n+1) ≈
√
2πn(n/e)n  which is accurate in the larger n regime. The expected number of dishes is 

Γ(n+1+c)Γ(c+σ) ∈ O
α Γ(1+c)Γ(n+c+σ)

i=1

i=1

2π(i+c+σ−1)((i+c+σ−1)/e)i+c+σ−1

2π(i+c)((i+c)/e)i+c

= O

K!Qn

= O(nσ). (16)

e−σ+1(1 + σ−1

e−σ+1eσ−1iσ−1

We are interested in the joint distribution of the statistics (K1  . . .   Kn)  where Km is the number
of dishes tried by exactly m customers and where there are a total of n customers in the restaurant.
As there are
(K1  . . .   Kn)  we have (ignoring constant terms and collecting terms in (10) with mk = m) 

i+c )i+c(i + c + σ − 1)σ−1
(cid:0)
(cid:81)n
K!Qn
m=1 Km as well  we see that (K1  . . .   Kn) is multinomial with the prob-
ability of a dish having m customers being proportional to the term in large parentheses. For large
m (and even larger n)  this probability simpliﬁes to 

(cid:1)Km conﬁgurations of the IBP with the same statistics
(cid:16)

p(K1  . . .   Kn|n) ∝

Conditioning on K =(cid:80)n

Γ(m−σ)Γ(n−m+c+σ)Γ(1+c)

Γ(1−σ)Γ(c+σ)Γ(n+c)

(cid:17)Km

m=1 Km!

m=1

m=1 Km!

m=1

(cid:81)n

.

(17)

n!

m!(n−m)!

n!

m!(n−m)!

(cid:19)

= O(cid:0)m−1−σ(cid:1) .

(18)

(cid:18)√

O( Γ(m−σ)

Γ(m+1) ) = O

n(cid:88)

E[K] =

(cid:32) n(cid:88)

=O

i=1

(cid:32) n(cid:88)

√

√

(cid:33)

(cid:32) n(cid:88)

i=1

(cid:33)
(cid:33)

2π(m−1−σ)((m−1−σ)/e)m−1−σ

√

2πm(m/e)m

8

12345−20246x 10−3SBP−BPclass orderReferences
[1] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process.

In Advances in Neural Information Processing Systems  volume 18  2006.

[2] Z. Ghahramani  T. L. Grifﬁths  and P. Sollich. Bayesian nonparametric latent feature models

(with discussion and rejoinder). In Bayesian Statistics  volume 8  2007.

[3] D. Knowles and Z. Ghahramani. Inﬁnite sparse factor analysis and inﬁnite independent com-
ponents analysis. In International Conference on Independent Component Analysis and Signal
Separation  volume 7 of Lecture Notes in Computer Science. Springer  2007.

[4] D. G¨or¨ur  F. J¨akel  and C. E. Rasmussen. A choice model with inﬁnitely many latent features.

In Proceedings of the International Conference on Machine Learning  volume 23  2006.

[5] D. J. Navarro and T. L. Grifﬁths. Latent features in similarity judgment: A nonparametric

Bayesian approach. Neural Computation  in press 2008.

[6] E. Meeds  Z. Ghahramani  R. M. Neal  and S. T. Roweis. Modeling dyadic data with binary

latent factors. In Advances in Neural Information Processing Systems  volume 19  2007.

[7] F. Wood  T. L. Grifﬁths  and Z. Ghahramani. A non-parametric Bayesian method for inferring
In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence 

hidden causes.
volume 22  2006.

[8] S. Goldwater  T.L. Grifﬁths  and M. Johnson. Interpolating between types and tokens by es-
timating power-law generators. In Advances in Neural Information Processing Systems  vol-
ume 18  2006.

[9] Y. W. Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguistics  pages 985–992  2006.

[10] E. Sudderth and M. I. Jordan. Shared segmentation of natural scenes using dependent Pitman-

Yor processes. In Advances in Neural Information Processing Systems  volume 21  2009.

[11] M. Perman  J. Pitman  and M. Yor. Size-biased sampling of Poisson point processes and

excursions. Probability Theory and Related Fields  92(1):21–39  1992.

[12] J. Pitman and M. Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable

subordinator. Annals of Probability  25:855–900  1997.

[13] H. Ishwaran and L. F. James. Gibbs sampling methods for stick-breaking priors. Journal of

the American Statistical Association  96(453):161–173  2001.

[14] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics 

1(2):209–230  1973.

[15] N. L. Hjort. Nonparametric Bayes estimators based on beta processes in models for life history

data. Annals of Statistics  18(3):1259–1294  1990.

[16] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In
Proceedings of the International Workshop on Artiﬁcial Intelligence and Statistics  volume 11 
pages 564–571  2007.

[17] M. Perman. Random Discrete Distributions Derived from Subordinators. PhD thesis  Depart-

ment of Statistics  University of California at Berkeley  1990.

[18] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics  21(1):59–78 

1967.

[19] J. F. C. Kingman. Poisson Processes. Oxford University Press  1993.
[20] Y. Kim. Nonparametric Bayesian estimators for counting processes. Annals of Statistics 

27(2):562–588  1999.

[21] Y. W. Teh  D. G¨or¨ur  and Z. Ghahramani. Stick-breaking construction for the Indian buffet pro-
cess. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics 
volume 11  2007.

[22] R. L. Wolpert and K. Ickstadt. Simulations of l´evy random ﬁelds. In Practical Nonparametric

and Semiparametric Bayesian Statistics  pages 227–242. Springer-Verlag  1998.

[23] G. Zipf. Selective Studies and the Principle of Relative Frequency in Language. Harvard

University Press  Cambridge  MA  1932.

9

,Anshumali Shrivastava
Ping Li