2015,Convolutional spike-triggered covariance analysis for neural subunit models,Subunit models provide a powerful yet parsimonious description of  neural spike responses to complex stimuli. They can be expressed by  a cascade of two linear-nonlinear (LN) stages  with the first linear  stage defined by convolution with one or more filters.  Recent  interest in such models has surged due to their biological  plausibility and accuracy for characterizing early sensory  responses. However  fitting subunit models poses a difficult  computational challenge due to the expense of evaluating the  log-likelihood and the ubiquity of local optima.  Here we address  this problem by forging a theoretical connection between  spike-triggered covariance analysis and nonlinear subunit models.  Specifically  we show that a ''convolutional'' decomposition of the  spike-triggered average (STA) and covariance (STC) provides an  asymptotically efficient estimator for the subunit model under  certain technical conditions. We also prove the identifiability of  such convolutional decomposition under mild assumptions.  Our  moment-based methods outperform highly regularized versions of the  GQM on neural data from macaque primary visual cortex  and achieves  nearly the same prediction performance as the full  maximum-likelihood estimator  yet with substantially lower cost.,Convolutional Spike-triggered Covariance Analysis

for Neural Subunit Models

Anqi Wu1

Il Memming Park2

Jonathan W. Pillow1

1 Princeton Neuroscience Institute  Princeton University

2 Department of Neurobiology and Behavior  Stony Brook University

{anqiw  pillow}@princeton.edu
memming.park@stonybrook.edu

Abstract

Subunit models provide a powerful yet parsimonious description of neural re-
sponses to complex stimuli. They are deﬁned by a cascade of two linear-nonlinear
(LN) stages  with the ﬁrst stage deﬁned by a linear convolution with one or more
ﬁlters and common point nonlinearity  and the second by pooling weights and an
output nonlinearity. Recent interest in such models has surged due to their biologi-
cal plausibility and accuracy for characterizing early sensory responses. However 
ﬁtting poses a difﬁcult computational challenge due to the expense of evaluating
the log-likelihood and the ubiquity of local optima. Here we address this problem
by providing a theoretical connection between spike-triggered covariance analy-
sis and nonlinear subunit models. Speciﬁcally  we show that a “convolutional”
decomposition of a spike-triggered average (STA) and covariance (STC) matrix
provides an asymptotically efﬁcient estimator for class of quadratic subunit mod-
els. We establish theoretical conditions for identiﬁability of the subunit and pool-
ing weights  and show that our estimator performs well even in cases of model
mismatch. Finally  we analyze neural data from macaque primary visual cortex
and show that our moment-based estimator outperforms a highly regularized gen-
eralized quadratic model (GQM)  and achieves nearly the same prediction perfor-
mance as the full maximum-likelihood estimator  yet at substantially lower cost.

Introduction

1
A central problem in systems neuroscience is to build ﬂexible and accurate models of the sensory
encoding process. Neurons are often characterized as responding to a small number of features in the
high-dimensional space of natural stimuli. This motivates the idea of using dimensionality reduction
methods to identify the features that affect the neural response [1–9]. However  many neurons in
the early visual pathway pool signals from a small population of upstream neurons  each of which
integrates and nolinearly transforms the light from a small region of visual space. For such neurons 
stimulus selectivity is often not accurately described with a small number of ﬁlters [10]. A more
accurate description can be obtained by assuming that such neurons pool inputs from an earlier stage
of shifted  identical nonlinear “subunits” [11–13].
Recent interest in subunit models has surged due to their biological plausibility and accuracy for
characterizing early sensory responses. In the visual system  linear pooling of shifted rectiﬁed linear
ﬁlters was ﬁrst proposed to describe sensory processing in the cat retina [14  15]  and more recent
work has proposed similar models for responses in other early sensory areas [16–18]. Moreover 
recent research in machine learning and computer vision has focused on hierarchical stacks of such
subunit models  often referred to as Convolutional Neural Networks (CNN) [19–21].
The subunit models we consider here describe neural responses in terms of an LN-LN cascade  that
is  a cascade of two linear-nonlinear (LN) processing stages  each of which involves linear projection
and a nonlinear transformation. The ﬁrst LN stage is convolutional  meaning it is formed from one or

1

W4

1st LN stage

2nd LN stage

W3

W1 W2

W5 W6

W7

stimulus

subunit
filter

pooling
weights

subunit
nonliearity

output
nonlinearity

more banks of identical  spatially shifted subunit
ﬁlters  with outputs transformed by a shared sub-
unit nonlinearity. The second LN stage consists
of a set of weights for linearly pooling the non-
linear subunits  an output nonlinearity for mapping
the output into the neuron’s response range  and
ﬁnally  an noise source for capturing the stochas-
ticity of neural responses (typically assumed to be
Gaussian  Bernoulli or Poisson). Vintch et al pro-
posed one variant of this type of subunit model  and
showed that it could account parsimoniously for the
multi-dimensional input-output properties revealed
by spike-triggered analysis of V1 responses [12  13].
However  ﬁtting such models remains a challeng-
ing problem. Simple LN models with Gaussian or
Poisson noise can be ﬁt very efﬁciently with spike-
triggered-moment based estimators [6–8]  but there
is no equivalent theory for LN-LN or subunit mod-
els. This paper aims to ﬁll that gap. We show that a
convolutional decomposition of the spike-triggered
average (STA) and covariance (STC) provides an
asymptotically efﬁcient estimator for a Poisson sub-
unit model under certain technical conditions:
the
stimulus is Gaussian  the subunit nonlinearity is well
described by a second-order polynomial  and the ﬁnal nonlinearity is exponential. In this case  the
subunit model represents a special case of a canonical Poisson generalized quadratic model (GQM) 
which allows us to apply the expected log-likelihood trick [7  8] to reduce the log-likelihood to a
form involving only the moments of the spike-triggered stimulus distribution. Estimating the subunit
model from these moments  an approach we refer to as convolutional STC  has ﬁxed computational
cost that does not scale with the dataset size after a single pass through the data to compute sufﬁcient
statistics. We also establish theoretical conditions under which the model parameters are identiﬁ-
able. Finally  we show that convolutional STC is robust to modest degrees of model mismatch  and
is nearly as accurate as the full maximum likelihood estimator when applied to neural data from V1
simple and complex cells.
2 Subunit Model
We begin with a general deﬁnition of the Poisson convolutional subunit model (Fig. 1). The model
is speciﬁed by:

Figure 1: Schematic of subunit LN-LNP cas-
cade model. For simplicity  we show only 1
subunit type.

Poisson
spiking 

response

subunit outputs:
spike rate:

spike count:

smi = f (km · xi)

 = g⇣Xm Xi

y| ⇠ Poiss() 

wmi smi⌘

(1)
(2)

(3)

where km is the ﬁlter for the m’th type of subunit  xi is the vectorized stimulus segment in the i’th
position of the shifted ﬁlter during convolution  and f is the nonlinearity governing subunit outputs.
For the second stage  wmi is a linear pooling weight from the m’th subunit at position i  and g is the
neuron’s output nonlinearity. Spike count y is conditionally Poisson with rate .
Fitting subunit models with arbitrary g and f poses signiﬁcant computational challenges. However 
if we set g to exponential and f takes the form of second-order polynomial  the model reduces to

where

 = exp⇣ 1
= exp⇣
C[w k] =Xm

2Xwmi (km · xi)2+Xwmi (km · xi) + a⌘
+ a⌘

2 x>C[w k]x + b>[w k]x

1

K>m diag(wm)Km 

b[w k] =Xm

K>mwm 

2

(4)

(5)

(6)

and Km is a Toeplitz matrix consisting of shifted copies of km satisfying Kmx =
[x1  x2  x3  . . .]>km.
In essence  these restrictions on the two nonlinearities reduce the subunit model to a (canonical-
form) Poisson generalized quadratic model (GQM) [7  8  22]  that is  a model in which the Poisson
spike rate takes the form of an exponentiated quadratic function of the stimulus. We will pursue
the implications of this mapping below. We assume that k is a spatial ﬁlter vector without time
expansion. If we have a spatio-temporal stimulus-response  k should be a spatial-temporal ﬁlter  but
the subunit convolution (across ﬁlter position i) involves only the spatial dimension(s). From (eqs. 4
and 5) it can be seen that the subunit model contains fewer parameters than a full GLM  making it a
more parsimonious description for neurons with multi-dimensional stimulus selectivity.
3 Estimators for Subunit Model
With the above deﬁnitions and formulations  we now present three estimators for the model pa-
rameters {w  k}. To simplify the notation  we omit the subscript in C[w k] and b[w k]  but their
dependence on the model parameters is assumed throughout.
Maximum Log-Likelihood Estimator
The maximum log-likelihood estimator (MLE) has excellent asymptotic properties  though it comes
with the high computational cost. The log-likelihood function can be written:

(7)

(8)

(9)

LMLE(✓) = Xi

i

yi log i Xi
2 x>i Cxi + b>xi + a) X exp( 1

= X yi( 1
= Tr[C⇤] + b>µ + ansp "Xi

exp( 1

2 x>i Cxi + b>xi + a)

2 x>i Cxi + b>xi + a)#

where µ =Pi yixi is the spike-triggered average (STA) and ⇤= Pi yixix>i
covariance (STC) and nsp =Pi yi is the total number of spikes. We denote the MLE as ✓MLE.
Moment-based Estimator with Expected Log-Likelihood Fitting
If the stimuli are drawn from x ⇠N (0  )  a zero-mean Gaussian with covariance   then the
expression in square brackets divided by N in (eq. 9) will converge to its expectation  given by

is the spike-triggered

(10)
Substituting this expectation into (9) yields a quantity called expected log-likelihood  with the ob-
jective function as 

2 x>i Cxi + b>xi + a)⇤ = |I  C| 1
E⇥exp( 1
LELL(✓) = Tr[C⇤] + b>µ + ansp  N|I  C| 1

(11)
where N is the number of time bins. We refer to ✓MELE = arg max✓ LELL(✓) as the MELE (maxi-
mum expected log-likelihood estimator) [7  8  22].
Moment-based Estimator with Least Squares Fitting
Maximizing (11) w.r.t {C  b  a} yields analytical expected maximum likelihood estimates [7]:

2 exp 1
2 exp 1

2 b>(1  C)1b + a
2 b>(1  C)1b + a

Cmele = 1  ⇤1  bmele =⇤ 1µ  amele = log( nsp

(12)
With these analytical estimates  it is straightforward and to optimize w and k by directly minimizing
squared error:

2 µ>1⇤1µ

N |⇤1|

2 )  1

1

LLS(✓) = ||Cmele  K> diag(w)K||2

(13)
which corresponds to an optimal “convolutional” decomposition of the moment-based estimates.
This formulation shows that the eigenvectors of Cmele are spanned by shifted copies of k. We
denote this estimate ✓LS.
All three estimators  ✓MLE  ✓MELE and ✓LS should provide consistent estimates for the subunit model
parameters due to consistency of ML and MELE estimates. However  the moment-based estimates

2 + ||bmele  K>w||2

2

3

Identiﬁability

(MELE and LS) are computationally much simpler  and scale much better to large datasets  due
to the fact that they depend on the data only via the spike-triggered moments. In fact their only
dependence on the dataset size is the cost of computing the STA and STC in one pass through the
data. As for efﬁciency  ✓LS has the drawback of being sensitive to noise in the Cmele estimate  which
has far more free parameters than in the two vectors w and k (for a 1-subunit model). Therefore 
accurate estimation of Cmele should be a precondition for good performance of ✓LS  and we expect
✓MELE to perform better for small datasets.
4
The equality C = C[w k] = K> diag(w)K is a core assumption to bridge the theoretical con-
nection between a subunit model and the spike-triggered moments (STA & STC). In case we care
about recovering the underlying biological structure  we maybe interested to know when the solution
is unique and naively interpretable. Here we address the identiﬁability of the convolution decom-
position of C for k and w estimation. Speciﬁcally  we brieﬂy study the uniqueness of the form
C = K> diag(w)K for a single subunit and multiple subunits respectively. We provide the proof
for the single subunit case in the main text  and the proof for multiple subunits sharing the same
pooling weight w in the supplement.
Note that failure of identiﬁability only indicates that there are possible symmetries in the solution
space so that there are multiple equivalent optima  which is a question of theoretical interest  but it
holds no implications for practical performance.
4.1
Identiﬁability for Single Subunit Model
We will frequently make use of frequency domain representation. Let B 2 Rd⇥d denote the discrete
Fourier transform (DFT) matrix with j-th column is 

(14)

d (j1)  e 2⇡

d 2(j1)  e 2⇡

d 3(j1)  . . .   e 2⇡

bj =h1  e 2⇡

We assume that k and w have full support in the frequency domain.

d (d1)(j1)i> .
Letek be a d-dimensional vector resulting from a discrete Fourier transform  that is ek = Bkk where
Bk is a d ⇥ dk DFT matrix  and similarly ew 2 Rd be a Fourier representation of w.
Assumption 1. No element inek or ew is zero.
Theorem. Suppose Assumption 1 holds  the convolution decomposition C = K> diag(w)K is
uniquely identiﬁable up to shift and scale  where C 2 Rd⇥d and d = dk + dw  1.
Proof. We ﬁx k (and thusek) to be a unit vector to deal with the obvious scale invariance. First note
where B 2 Rd⇥d is the DFT matrix and (·)H denotes conjugate transpose operation. Thus 
Note thatfW := Bw diag(w)BH

C = BH diag(ek)H Bw diag(w)BH

that we can rewrite the convolution operator K using DFT matrices as 

K = BH diag(Bkk)Bw

w is a circulant matrix 

fW := circulant(ew) =0BBBB@

w diag(ek)B
ew2
ew3
ew4
ew3
...
...
ew1
ewd
ew1
ew2
eC = BCBH = diag(ek)H fW diag(ek) =fW  (ekekH)>

ewd
ew1
...
ewd2
ewd1

ew1
ew2
...
ewd1
ewd

···
···
...
···
···

1CCCCA

(15)

(16)

(17)

(19)

Hence  we can rewrite (16) in the frequency domain as 

(18)
Since B is invertible  the uniqueness of the original C decomposition is equivalent to the uniqueness

of eC decomposition. The newly deﬁned decomposition is
eC = fW  (ekekH)>.

4

(20)

i=1 ribibH

Suppose there are two distinct decompositions {fW  ek} and {eV  eg}  where both {k ek} and {g eg}
are unit vectors  such that eC =fW  (ekekH)> = eV  (egegH)>. Since bothfW and eV have no zero 
deﬁne the element-wise ratio R := (fW ./eV )> 2 Rd⇥d  then we have
Note that rank(R ekekH) = rank(egegH) = 1.
We can express R as R = Pd
vector a and b  (aaH)  (bbH) = (a  b)(a  b)H  we get
dXi=1
dXi=1

R is also a circulant matrix which can be diagonalized by DFT [23]: R = B diag (r1  . . .   rd)BH.
i . Using the identity for Hadamard product that for any

R ekekH =egegH

By Lemma 1 (in the appendix)  {b1 ek  b2 ek  . . .   bd ek} is a linearly independent set.
Therefore  to satisfy the rank constraint rank(R ekekH) = 1  ri can be non-zero at most a single i.
Without loss of generality  let ri 6= 0 and all other r· to be zero  then we have 
Because bi  ek and eg are unit vectors  ri = 1. By recognizing that⇣diag(bi)ek⌘ is the Fourier
transform of i  1 positions shifted k  denoted as ki1  we have  ki1(ki1)> = gg>. Therefore 
i ) eV =fW thus  vi1 = w. that is  v must also
g = ki1. Moreover  from (20) and (22)  (bibH
be a shifted version of w.
If restricting k and g to be unit vectors  then any solution v and g would satisfy w = vi1 and
g = ki1. Therefore  the two decompositions are identical up to scale and shift.

i ) ekekH =egegH =) ri diag(bi)ekekH diag(bi)H =egegH

ri(bi ek)(bi ek)H

i )  (ekekH) =

R ekekH =

ri(bibH

ri(bibH

(21)

(22)

Identiﬁability for Multiple Subunits Model

4.2
Multiple subunits model (with m > 1 subunits) is far more complicated to analyze due to large
degree of hidden invariances. In this study  we only provide the analysis under a speciﬁc condition
when all subunits share a common pooling weight w.
Assumption 2. All models share a common w.
We make a few additional assumptions. We would like to consider a tight parameterization where
no combination of subunits can take over another subunit’s task.
Assumption 3. K := [k1  k2  k3  . . .   km] spans an m-dimensional subspace where ki is the sub-
unit ﬁlter for i-th subunit and K 2 Rdk⇥m. In addition  K has orthogonal columns.
We denote K with p positions shifted along the column as Kp := [kp
3  . . .   kp
that trivially  m  dk < dk + dw  1 < d since dw > 1.
To allow arbitrary scale corresponding to each unit vector ki  we introduce coefﬁcient ↵i to the i-th
subunit  thus extending (19) to 

m]. Also  note

2  kp

1  kp

(23)

C =

mXi=1fW  (↵iekiekH

i )> =fW  mXi=1

i !>
↵iekiekH

=fW  (eKAeKH)>

where A 2 Rm⇥m is a diagonal matrix of ↵i and eK 2 Rd⇥m is the DFT of K.
Assumption 4. @⌦ 2 Rm⇥m such that Ki⌦= P Ki  8i  where P 2 Rdk⇥dk is the permutation
matrix from Ki to Kj by shifting rows  namely Kj = P Ki  8i  j  and ⌦ is a linear projection
coefﬁcient matrix satisfying Kj = Ki⌦.
Assumption 5. A has all positive or all negative values on the diagonal.
Given these assumptions  we establish the proposition for multiple subunits model.

uniquely identiﬁable up to shift and scale.

Proposition. Under Assumptions (1-5)  the convolutional decomposition C =fW  (eKAeKH)> is

The proof for the proposition and illustrations of Assumption 4-5 are in the supplement.

5

0.4

0.2

0

 0

10

20

30

a)

c)

0.4

0

-0.4
0

10

20

30

true parameters
MELE
smoothMELE

exponential

output nonlinearity

smoothLS
smoothMELE
smoothMLE

b)

7

3.5

)
c
e
s
(
 
e
m

i
t
 
n
u
r

0
103

104

sample size

105

soft-rectifier

c
i
t
a
r
d
a
u
q

1.13

0.64

0.14

0.88

E
S
M

0.45

i

d
o
m
g
i
s

y
t
i
r
a
e
n

i
l

n
o
n
 
t
i
n
u
b
u
s

0.03
103

104

sample size

105

0.76

0.56

0.37

0.74

0.39

0.05

0.7

0.38

0.07

0.7

0.4

0.09

0.6

0.39

0.17

0.71

0.43

0.14

smoothLS

smoothMELE

smoothMLE

Figure 2: a) True parameters and MELE and smoothMELE estimations. b) Speed performance for
smoothLS  smoothMELE and smoothMLE. The slightly decreasing running time along with a larger
size is resulted from a more and more fully supported subspace  which makes optimization require
fewer iterations. c) Accuracy performance for all combinations of subunit and output nonlinearities
for smoothLS  smoothMELE and smoothMLE. Top left is the subunit model matching the data;
others are model mismatch.

5 Experiments
5.1 Initialization
All three estimators are non-convex and contain many local optima  thus the selection of model
initialization would affect the optimization substantially. Similar to [12] using ‘convolutional STC’
for initialization  we also use a simple moment based method with some assumptions. For simplicity 
we assume all subunit models sharing the same w with different scaling factors as in eq. (23). Our
initializer is generated from a shallow bilinear regression. Firstly  initialize w with a wide Gaussian

and A contain information about ki’s and ↵’s respectively  hypothesizing that k’s are orthogonal to
each other and ↵’s are all positive (Assumptions 3 and 5). Based on the ki’s and ↵i’s we estimated
from the rough Gaussian proﬁle of w  now we ﬁx those and re-estimate w with the same element-

proﬁle  then estimate eKAeKH from element-wise division of Cmele by fW . Secondly  use SVD
to decompose eKAeKH into an orthogonal base set eK and a positive diagonal matrix A  where eK
wise division forfW . This bilinear iterative procedure proceeds only a few times in order to avoid

overﬁtting to Cmele which is a coarse estimate of C.
5.2 Smoothing prior
Neural receptive ﬁelds are generally smooth  thus a prior smoothing out high frequency ﬂuctuations
would improve the performance of estimators  unless the data likelihood provides sufﬁcient evidence
for jaggedness. We apply automatic smoothness determination (ASD [24]) to both w and k  each
with an associated balancing hyper parameter w and k. Assuming w ⇠N (0  Cw) with

Cw = exp✓⇢w  kk2
w ◆

22

(24)

where  is the vector of differences between neighboring locations in w. ⇢w and 2
w are variance
and length scale of Cw that belong to the hyper parameter set. k also has the same ASD prior with
hyper parameters ⇢k and 2
k. For multiple subunits  each wi and ki would have its own ASD prior.

6

low−rank  smooth  
expected GQM

low−rank  
smooth GQM
smoothLS(#1)

smoothLS(#2)

smoothMELE(#1)

smoothMELE(#2)

smoothMLE(#1)

smoothMLE(#2)

t
fi
-
f
o
-
s
s
e
n
d
o
o
g

)
k
p
s
/
s
t
a
n
(

0

−1

−2

−3

 

performance

0

−0.06

−0.12

−0.18

 

4
10

4
10

training size

)
c
e
s
(
 
e
m

i

i
t
 
g
n
n
n
u
r

250

200

150

100

50

 

 

5
10

5
10

speed

 

 

4

10

training size

5
10

Figure 3: Goodness-of-model ﬁts from various estimators and their running speeds (without GQM
comparisons). Black curves are regularized GQM (with and without expected log-likelihood trick);
blue is smooth LS; green is smooth MELE; red is smooth MLE. All the subunit estimators have
results for 1 subunit and 2 subunits. The inset ﬁgure in performance is the enlarged view for large
goodness-of-ﬁt values. The right ﬁgure is the speed result showing that MLE-based methods require
exponentially increasing running time when increasing the training size  but our moment-based ones
have quite consistent speed.

Fig. 2a shows the true w and k and the estimations from MELE and smoothMELE (MELE with
smoothing prior). From now on  we use smoothing prior by default.
5.3 Simulations
To illustrate the performance of our moment-based estimators  we generated Gaussian stimuli from
an LNP neuron with exponentiated-quadratic nonlinearity and 1 subunit model with 8-element ﬁlter
k and 33-element pooling weights w. Mean ﬁring rate is 0.91 spk/s. In our estimation  each time
bin stimulus with 40 dimensions is treated as one sample to generate spike response. Fig. 2 b and c
show the speed and accuracy performance of three estimators LS  MELE and MLE (with smoothing
prior). LS and MELE are comparable with baseline MLE in terms of accuracy but are exponentially
faster.
Although LNP with exponential nonlinearity has been widely adapted in neuroscience for its sim-
plicity  the actual nonlinearity of neural systems is often sub-exponential  such as soft-rectiﬁer non-
linearity. But exponential is favored as a convenient approximation of soft-rectiﬁer within a small
regime around the origin. Also generally  LNP neuron leans towards sigmoid subunit nonlinearity
rather than quadratic. Quadratic could well approximate a sigmoid within a small nonlinear regime
before the linear regime of the sigmoid. Therefore  in order to check the generalization performance
of LS and MELE on mismatch models  we stimulated data from a neuron with sigmoid subunit non-
linearity or soft-rectiﬁer output nonlinearity as shown in Fig. 2c. All the full MLEs formulated with
no model mismatch provide a baseline for inspecting the performance of the ELL methods. Despite
the model-mismatch  our estimators (LS and MELE) are on par with MLE when the subunit nonlin-
earity is quadratic  but the performance is notably worse for the sigmoid nonlinearity. Even so  in
real applications  we will explore ﬁts with different subunit nonlinearities using full MLE  where the
exponential and quadratic assumption is thus primarily useful for a reasonable and extremely fast
initializer. Moreover  the running time for moment-based estimators is always exponentially faster.
5.4 Application to neural data
In order to show the predictive performance more comprehensively in real neural dataset  we applied
LS  MELE and MLE estimators to data from a population of 57 V1 simple and complex cells (data
published in [11]). The stimulus consisted of oriented binary white noise (“ﬂickering bar”) aligned
with the cell’s preferred orientation. The size of receptive ﬁeld was chosen to be # of bars d ⇥10
time bins  yielding a 10d-dimensional stimulus space. The time bin size is 10 ms and the number of
bars (d) is 16 in our experiment.
We compared moment-based estimators and MLE with smoothed low-rank expected GQM and
smoothed low-rank GQM [7  8]. Models are trained on stimuli with size varying from 6.25 ⇥ 103
to 105 and tested on 5 ⇥ 104 samples. Each subunit ﬁlter has a length of 5. All hyper parameters
are chosen by cross validation. Fig. 3 shows that GQM is weakly better than LS but its running
time is far more than LS (data not shown). Both MELE and MLE (but not LS) outﬁght GQM and

7

a)

subunit #1

subunit #1
0

-0.1

-0.2
0

10

20

subunit #2

subunit #2

0.2

0.1

0
0

10

20

b)

STA

excitatory STC filters

suppressive STC filters

V1
responses

subunit
model

Figure 4: Estimating visual receptive ﬁelds from a complex cell (544l029.p21). a) k and w by
ﬁtting smoothMELE(#2). Subunit #1 is suppressive (negative w) and #2 is excitatory (positive
w). Form the y-axis we can tell from w that both imply that middle subunits contribute more than
the ends. b) Qualitative analysis. Each image corresponds to a normalized 24 dimensions spatial
pixels (horizontal) by 10 time bins (vertical) ﬁlter. Top row: STA/STC from true data; Bottom row:
simulated response from 2-subunit MELE model given true stimuli and applied the same subspace
analysis.

expected GQM with both 1 subunit and 2 subunits. Especially the improvement is the greatest with
1 subunit  which results from the average over all simple and complex cells. Generally  the more
“complex” the cell is  the higher probability that multiple subunits would ﬁt better. Outstandingly 
MELE outperforms others with best goodness-of-ﬁt and ﬂat speed curve. The goodness-of-ﬁt is
deﬁned to be the log-likelihood on the test set divided by spike count.
For qualitative analysis  we ran smoothMELE(#2) for a complex cell and learned the optimal sub-
unit ﬁlters and pooling weights (Fig. 4a)  and then simulated V1 response by ﬁtting 2-subunit MELE
generative model given the optimal parameters. STA/STC analysis is applied to both neural data and
simulated V1 response data. The quality of the ﬁlters trained on 105 stimuli are qualitatively close to
that obtained by STA/STC (Fig. 4b). Subunit models can recover STA  the ﬁrst six excitatory STC
ﬁlters and the last four suppressive ones but with a considerably parsimonious parameter space.
6 Conclusion
We proposed an asymptotically efﬁcient estimator for quadratic convolutional subunit models  which
forges an important theoretical link between spike-triggered covariance analysis and nonlinear sub-
unit models. We have shown that the proposed method works well even when the assumptions
about model speciﬁcation (nonlinearity and input distribution) were violated. Our approach reduces
the difﬁculty of ﬁtting subunit models because computational cost does not depend on dataset size
(beyond the cost of a single pass through the data to compute the spike-triggered moments). We
also proved conditions for identiﬁability of the convolutional decomposition  which reveals that for
most cases the parameters are indeed identiﬁable. We applied our estimators to the neural data from
macaque primary visual cortex  and showed that they outperform a highly regularized form of the
GQM and achieve similar performance to the subunit model MLE at substantially lower computa-
tional cost.
Acknowledgments
This work was supported by the Sloan Foundation (JP)  McKnight Foundation (JP)  Simons Global
Brain Award (JP)  NSF CAREER Award IIS-1150186 (JP)  and a grant from the NIH (NIMH grant
MH099611 JP). We thank N. Rust and T. Movshon for V1 data.

8

References
[1] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-senstivive neuron
in the blowﬂy visual system: coding and information transmission in short spike sequences. Proc. R. Soc.
Lond. B  234:379–414  1988.

[2] J. Touryan  B. Lau  and Y. Dan. Isolation of relevant visual features from random stimuli for cortical

complex cells. Journal of Neuroscience  22:10811–10818  2002.

[3] B. Aguera y Arcas and A. L. Fairhall. What causes a neuron to spike? Neural Computation  15(8):1789–

1807  2003.

[4] Tatyana Sharpee  Nicole C. Rust  and William Bialek. Analyzing neural responses to natural signals:

maximally informative dimensions. Neural Comput  16(2):223–250  Feb 2004.

[5] O. Schwartz  J. W. Pillow  N. C. Rust  and E. P. Simoncelli. Spike-triggered neural characterization.

Journal of Vision  6(4):484–507  7 2006.

[6] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic
generalization of spike-triggered average and covariance analysis. Journal of Vision  6(4):414–428  4
2006.

[7] Il Memming Park and Jonathan W. Pillow. Bayesian spike-triggered covariance analysis. In J. Shawe-
Taylor  R.S. Zemel  P. Bartlett  F.C.N. Pereira  and K.Q. Weinberger  editors  Advances in Neural Infor-
mation Processing Systems 24  pages 1692–1700  2011.

[8] Il M. Park  Evan W. Archer  Nicholas Priebe  and Jonathan W. Pillow. Spectral methods for neural char-
acterization using generalized quadratic models. In Advances in Neural Information Processing Systems
26  pages 2454–2462  2013.

[9] Ross S. Williamson  Maneesh Sahani  and Jonathan W. Pillow. The equivalence of information-theoretic
and likelihood-based methods for neural dimensionality reduction. PLoS Comput Biol  11(4):e1004141 
04 2015.

[10] Kanaka Rajan  Olivier Marre  and Gaˇsper Tkaˇcik. Learning quadratic receptive ﬁelds from neural re-

sponses to natural stimuli. Neural Computation  25(7):1661–1692  2013/06/19 2013.

[11] Nicole C. Rust  Odelia Schwartz  J. Anthony Movshon  and Eero P. Simoncelli. Spatiotemporal elements

of macaque v1 receptive ﬁelds. Neuron  46(6):945–956  Jun 2005.

[12] B Vintch  A Zaharia  J A Movshon  and E P Simoncelli. Efﬁcient and direct estimation of a neural
subunit model for sensory coding. In Adv. Neural Information Processing Systems (NIPS*12)  volume 25 
Cambridge  MA  2012. MIT Press. To be presented at Neural Information Processing Systems 25  Dec
2012.

[13] Brett Vintch  Andrew Zaharia  J Movshon  and Eero P Simoncelli. A convolutional subunit model for

neuronal responses in macaque v1. J. Neursoci  page in press  2015.

[14] HB Barlow and W Ro Levick. The mechanism of directionally selective units in rabbit’s retina. The

Journal of physiology  178(3):477  1965.

[15] S. Hochstein and R. Shapley. Linear and nonlinear spatial subunits in y cat retinal ganglion cells. J.

Physiol.  262:265–284  1976.

[16] Jonathan B Demb  Kareem Zaghloul  Loren Haarsma  and Peter Sterling. Bipolar cells contribute to
nonlinear spatial summation in the brisk-transient (y) ganglion cell in mammalian retina. The Journal of
neuroscience  21(19):7447–7454  2001.

[17] Joanna D Crook  Beth B Peterson  Orin S Packer  Farrel R Robinson  John B Troy  and Dennis M Dacey.
Y-cell receptive ﬁeld and collicular projection of parasol ganglion cells in macaque monkey retina. The
Journal of neuroscience  28(44):11277–11291  2008.

[18] PX Joris  CE Schreiner  and A Rees. Neural processing of amplitude-modulated sounds. Physiological

reviews  84(2):541–577  2004.

[19] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern

recognition unaffected by shift in position. Biological cybernetics  36(4):193–202  1980.

[20] T. Serre  L. Wolf  S. Bileschi  M. Riesenhuber  and T. Poggio. Robust object recognition with cortex-like

mechanisms. Pattern Analysis and Machine Intelligence  IEEE Transactions on  29(3):411–426  2007.

[21] Yann LeCun  L´eon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[22] AlexandroD. Ramirez and Liam Paninski. Fast inference in generalized linear models via expected log-

likelihoods. Journal of Computational Neuroscience  pages 1–20  2013.
[23] Philip J Davis. Circulant matrices. American Mathematical Soc.  1979.
[24] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions.

NIPS  15  2003.

9

,Anqi Wu
Il Memming Park
Jonathan Pillow
Xuesong Niu
Hu Han
Shiguang Shan
Xilin Chen