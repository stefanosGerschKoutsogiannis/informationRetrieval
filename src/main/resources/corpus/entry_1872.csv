2010,Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs,We present simple and computationally efficient nonparametric estimators of R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an unknown  absolutely continuous distribution over $\R^d$. The estimators are calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time  we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence  the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.,Estimation of R´enyi Entropy and Mutual Information

Based on Generalized Nearest-Neighbor Graphs

D´avid P´al

Department of Computing Science

University of Alberta
Edmonton  AB  Canada

dpal@cs.ualberta.ca

Barnab´as P´oczos

School of Computer Science
Carnegie Mellon University

Pittsburgh  PA  USA

poczos@ualberta.ca

Csaba Szepesv´ari

Department of Computing Science

University of Alberta
Edmonton  AB  Canada

szepesva@ualberta.ca

Abstract

We present simple and computationally efﬁcient nonparametric estimators of
R´enyi entropy and mutual information based on an i.i.d. sample drawn from an
unknown  absolutely continuous distribution over Rd. The estimators are cal-
culated as the sum of p-th powers of the Euclidean lengths of the edges of the
‘generalized nearest-neighbor’ graph of the sample and the empirical copula of
the sample respectively. For the ﬁrst time  we prove the almost sure consistency
of these estimators and upper bounds on their rates of convergence  the latter of
which under the assumption that the density underlying the sample is Lipschitz
continuous. Experiments demonstrate their usefulness in independent subspace
analysis.

1

Introduction

We consider the nonparametric problem of estimating R´enyi α-entropy and mutual information (MI)
based on a ﬁnite sample drawn from an unknown  absolutely continuous distribution over Rd. There
are many applications that make use of such estimators  of which we list a few to give the reader
a taste: Entropy estimators can be used for goodness-of-ﬁt testing (Vasicek  1976; Goria et al. 
2005)  parameter estimation in semi-parametric models (Wolsztynski et al.  2005)  studying fractal
random walks (Alemany and Zanette  1994)  and texture classiﬁcation (Hero et al.  2002b a). Mu-
tual information estimators have been used in feature selection (Peng and Ding  2005)  clustering
(Aghagolzadeh et al.  2007)  causality detection (Hlav´ackova-Schindler et al.  2007)  optimal exper-
imental design (Lewi et al.  2007; P´oczos and L˝orincz  2009)  fMRI data processing (Chai et al. 
2009)  prediction of protein structures (Adami  2004)  or boosting and facial expression recogni-
tion (Shan et al.  2005). Both entropy estimators and mutual information estimators have been used
for independent component and subspace analysis (Learned-Miller and Fisher  2003; P´oczos and
L˝orincz  2005; Hulle  2008; Szab´o et al.  2007)  and image registration (Kybic  2006; Hero et al. 
2002b a). For further applications  see Leonenko et al. (2008); Wang et al. (2009a).
In a na¨ıve approach to R´enyi entropy and mutual information estimation  one could use the so called
“plug-in” estimates. These are based on the obvious idea that since entropy and mutual information
are determined solely by the density f (and its marginals)  it sufﬁces to ﬁrst estimate the density
using one’s favorite density estimate which is then “plugged-in” into the formulas deﬁning entropy

1

and mutual information. The density is  however  a nuisance parameter which we do not want to
estimate. Density estimators have tunable parameters and we may need cross validation to achieve
good performance.
The entropy estimation algorithm considered here is direct—it does not build on density estimators.
It is based on k-nearest-neighbor (NN) graphs with a ﬁxed k. A variant of these estimators  where
each sample point is connected to its k-th nearest neighbor only  were recently studied by Goria
et al. (2005) for Shannon entropy estimation (i.e.
the special case α = 1) and Leonenko et al.
(2008) for R´enyi α-entropy estimation. They proved the weak consistency of their estimators under
certain conditions. However  their proofs contain some errors  and it is not obvious how to ﬁx them.
Namely  Leonenko et al. (2008) apply the generalized Helly-Bray theorem  while Goria et al. (2005)
apply the inverse Fatou lemma under conditions when these theorems do not hold. This latter error
originates from the article of Kozachenko and Leonenko (1987)  and this mistake can also be found
in Wang et al. (2009b).
The ﬁrst main contribution of this paper is to give a correct proof of consistency of these estimators.
Employing a very different proof techniques than the papers mentioned above  we show that these
estimators are  in fact  strongly consistent provided that the unknown density f has bounded support
and α ∈ (0  1). At the same time  we allow for more general nearest-neighbor graphs  wherein
as opposed to connecting each point only to its k-th nearest neighbor  we allow each point to be
connected to an arbitrary subset of its k nearest neighbors. Besides adding generality  our numer-
ical experiments seem to suggest that connecting each sample point to all its k nearest neighbors
improves the rate of convergence of the estimator.
The second major contribution of our paper is that we prove a ﬁnite-sample high-probability bound
on the error (i.e. the rate of convergence) of our estimator provided that f is Lipschitz. According
to the best of our knowledge  this is the very ﬁrst result that gives a rate for the estimation of R´enyi
entropy. The closest to our result in this respect is the work by Tsybakov and van der Meulen
(1996) who proved the root-n consistency of an estimator of the Shannon entropy and only in one
dimension.
The third contribution is a strongly consistent estimator of R´enyi mutual information that is based on
NN graphs and the empirical copula transformation (Dedecker et al.  2007). This result is proved for
d ≥ 3 1 and α ∈ (1/2  1). This builds upon and extends the previous work of P´oczos et al. (2010)
where instead of NN graphs  the minimum spanning tree (MST) and the shortest tour through the
sample (i.e. the traveling salesman problem  TSP) were used  but it was only conjectured that NN
graphs can be applied as well.
There are several advantages of using k-NN graph over MST and TSP (besides the obvious concep-
tual simplicity of k-NN): On a serial computer the k-NN graph can be computed somewhat faster
than MST and much faster than the TSP tour. Furthermore  in contrast to MST and TSP  computa-
tion of k-NN can be easily parallelized. Secondly  for different values of α  MST and TSP need to
be recomputed since the distance between two points is the p-th power of their Euclidean distance
where p = d(1 − α). However  the k-NN graph does not change for different values of p  since
p-th power is a monotone transformation  and hence the estimates for multiple values of α can be
calculated without the extra penalty incurred by the recomputation of the graph. This can be advan-
tageous e.g. in intrinsic dimension estimators of manifolds (Costa and Hero  2003)  where p is a free
parameter  and thus one can calculate the estimates efﬁciently for a few different parameter values.
The fourth major contribution is a proof of a ﬁnite-sample high-probability error bound (i.e. the rate
of convergence) for our mutual information estimator which holds under the assumption that the
copula of f is Lipschitz. According to the best of our knowledge  this is the ﬁrst result that gives a
rate for the estimation of R´enyi mutual information.
The toolkit for proving our results derives from the deep literature of Euclidean functionals  see 
(Steele  1997; Yukich  1998). In particular  our strong consistency result uses a theorem due to Red-
mond and Yukich (1996) that essentially states that any quasi-additive power-weighted Euclidean
functional can be used as a strongly consistent estimator of R´enyi entropy (see also Hero and Michel
1999). We also make use of a result due to Koo and Lee (2007)  who proved a rate of convergence
result that holds under more stringent conditions. Thus  the main thrust of the present work is show-

1Our result for R´enyi entropy estimation holds for d = 1 and d = 2  too.

2

ing that these conditions hold for p-power weighted nearest-neighbor graphs. Curiously enough  up
to now  no one has shown this  except for the case when p = 1  which is studied in Section 8.3 of
(Yukich  1998). However  the condition p = 1 gives results only for α = 1 − 1/d.
Unfortunately  the space limitations do not allow us to present any of our proofs  so we relegate them
into the extended version of this paper (P´al et al.  2010). We instead try to give a clear explanation
of R´enyi entropy and mutual information estimation problems  the estimation algorithms and the
statements of our converge results.
In the ﬁrst experiment  we compare the
Additionally  we report on two numerical experiments.
empirical rates of convergence of our estimators with our theoretical results and plug-in estimates.
Empirically  the NN methods are the clear winner. The second experiment is an illustrative applica-
tion of mutual information estimation to an Independent Subspace Analysis (ISA) task.
The paper is organized as follows: In the next section  we formally deﬁne R´enyi entropy and R´enyi
mutual information and the problem of their estimation. Section 3 explains the ‘generalized nearest
neighbor’ graphs. This graph is then used in Section 4 to deﬁne our R´enyi entropy estimator. In
the same section  we state a theorem containing our convergence results for this estimator (strong
consistency and rates). In Section 5  we explain the copula transformation  which connects R´enyi
entropy with R´enyi mutual information. The copula transformation together with the R´enyi entropy
estimator from Section 4 is used to build an estimator of R´enyi mutual information. We conclude
this section with a theorem stating the convergence properties of the estimator (strong consistency
and rates). Section 6 contains the numerical experiments. We conclude the paper by a detailed
discussion of further related work in Section 7  and a list of open problems and directions for future
research in Section 8.

2 The Formal Deﬁnition of the Problem

information of d real-valued random variables2 X =
R´enyi entropy and R´enyi mutual
(X 1  X 2  . . .   X d) with joint density f : Rd → R and marginal densities fi : R → R  1 ≤ i ≤ d 
are deﬁned for any real parameter α assuming the underlying integrals exist. For α (cid:54)= 1  R´enyi
entropy and R´enyi mutual information are deﬁned respectively as3

log

f α(x1  x2  . . .   xd) d(x1  x2  . . .   xd)  

(1)

f α(x1  x2  . . .   xd)

fi(xi)

d(x1  x2  . . .   xd).

(2)

(cid:90)
(cid:90)

Rd

Hα(X) = Hα(f ) =

1

1 − α

Iα(X) = Iα(f ) =

1

α − 1

log

Rd

(cid:33)1−α

(cid:32) d(cid:89)

i=1

For α = 1 they are deﬁned by the limits H1 = limα→1 Hα and I1 = limα→1 Iα. In fact  Shannon
(differential) entropy and the Shannon mutual information are just special cases of R´enyi entropy
and R´enyi mutual information with α = 1.
The goal of this paper is to present estimators of R´enyi entropy (1) and R´enyi information (2) and
study their convergence properties. To be more explicit  we consider the problem where we are
given i.i.d. random variables X1:n = (X1  X2  . . .   Xn) where each Xj = (X 1
j ) has
density f : Rd → R and marginal densities fi : R → R and our task is to construct an estimate

(cid:98)Hα(X1:n) of Hα(f ) and an estimate(cid:98)Iα(X1:n) of Iα(f ) using the sample X1:n.

j   X 2

j   . . .   X d

3 Generalized Nearest-Neighbor Graphs

The basic tool to deﬁne our estimators is the generalized nearest-neighbor graph and more speciﬁ-
cally the sum of the p-th powers of Euclidean lengths of its edges.
Formally  let V be a ﬁnite set of points in an Euclidean space Rd and let S be a ﬁnite non-empty
set of positive integers; we denote by k the maximum element of S. We deﬁne the generalized

2We use superscript for indexing dimension coordinates.
3The base of the logarithms in the deﬁnition is not important; any base strictly bigger than 1 is allowed.
Similarly as with Shannon entropy and mutual information  one traditionally uses either base 2 or e. In this
paper  for deﬁnitiveness  we stick to base e.

3

nearest-neighbor graph N NS(V ) as a directed graph on V . The edge set of N NS(V ) contains
for each i ∈ S an edge from each vertex x ∈ V to its i-th nearest neighbor. That is  if we sort
V \{x} = {y1  y2  . . .   y|V |−1} according to the Euclidean distance to x (breaking ties arbitrarily):
(cid:107)x − y1(cid:107) ≤ (cid:107)x − y2(cid:107) ≤ ··· ≤ (cid:107)x − y|V |−1(cid:107) then yi is the i-th nearest-neighbor of x and for each
i ∈ S there is an edge from x to yi in the graph.
For p ≥ 0 let us denote by Lp(V ) the sum of the p-th powers of Euclidean lengths of its edges.
Formally 

(cid:88)

Lp(V ) =

(x y)∈E(N NS (V ))

(cid:107)x − y(cid:107)p  

(3)

where E(N NS(V )) denotes the edge set of N NS(V ). We intentionally hide the dependence on S
in the notation Lp(V ). For the rest of the paper  the reader should think of S as a ﬁxed but otherwise
arbitrary ﬁnite non-empty set of integers  say  S = {1  3  4}.
The following is a basic result about Lp. The proof can be found in P´al et al. (2010).
Theorem 1 (Constant γ). Let X1:n = (X1  X2  . . .   Xn) be an i.i.d. sample from the uniform
distribution over the d-dimensional unit cube [0  1]d. For any p ≥ 0 and any ﬁnite non-empty set S
of positive integers there exists a constant γ > 0 such that

lim
n→∞

Lp(X1:n)
n1−p/d

= γ

a.s.

(4)

The value of γ depends on d  p  S and  except for special cases  an analytical formula for its value is
not known. This causes a minor problem since the constant γ appears in our estimators. A simple
and effective way to deal with this problem is to generate a large i.i.d. sample X1:n from the uniform
distribution over [0  1]d and estimate γ by the empirical value of Lp(X1:n)/n1−p/d.

4 An Estimator of R´enyi Entropy

(cid:98)Hα(X1:n) =

1

We are now ready to present an estimator of R´enyi entropy based on the generalized nearest-neighbor
graph. Suppose we are given an i.i.d. sample X1:n = (X1  X2  . . .   Xn) from a distribution µ over
Rd with density f. We estimate entropy Hα(f ) for α ∈ (0  1) by

Lp(X1:n)
γn1−p/d

where p = d(1 − α) 

log

1 − α

(5)
and Lp(·) is the sum of p-th powers of Euclidean lengths of edges of the nearest-neighbor graph
N NS(·) for some ﬁnite non-empty S ⊂ N+ as deﬁned by equation (3). The constant γ is the same
as in Theorem 1.

It states that (cid:98)Hα is strongly
The following theorem is our main result about the estimator (cid:98)Hα.
Theorem 2 (Consistency and Rate for (cid:98)Hα). Let α ∈ (0  1). Let µ be an absolutely continuous

consistent and gives upper bounds on the rate of convergence. The proof of theorem is in P´al et al.
(2010).

distribution over Rd with bounded support and let f be its density. If X1:n = (X1  X2  . . .   Xn) is
an i.i.d. sample from µ then

lim

n→∞ (cid:98)Hα(X1:n) = Hα(f )
O
(cid:16)
(cid:16)

d(2d−p) (log(1/δ))1/2−p/(2d)(cid:17)
d(d+1) (log(1/δ))1/2−p/(2d)(cid:17)

− d−p
− d−p

a.s.

O

 

n

n

(6)

(7)

 

if 0 < p < d − 1 ;
if d − 1 ≤ p < d .

Moreover  if f is Lipschitz then for any δ > 0 with probability at least 1 − δ 

(cid:12)(cid:12)(cid:12)(cid:98)Hα(X1:n) − Hα(f )
(cid:12)(cid:12)(cid:12) ≤

5 Copulas and Estimator of Mutual Information

Estimating mutual information is slightly more complicated than estimating entropy. We start with a
basic property of mutual information which we call rescaling. It states that if h1  h2  . . .   hd : R →
R are arbitrary strictly increasing functions  then

Iα(h1(X 1)  h2(X 2)  . . .   hd(X d)) = Iα(X 1  X 2  . . .   X d) .

(8)

4

A particularly clever choice is hj = Fj for all 1 ≤ j ≤ d  where Fj is the cumulative distribution
function (c.d.f.) of X j. With this choice  the marginal distribution of hj(X j) is the uniform distri-
bution over [0  1] assuming that Fj  the c.d.f. of X j  is continuous. Looking at the deﬁnition of Hα
and Iα we see that
Iα(X 1  X 2  . . .   X d) = Iα(F1(X 1)  F2(X 2)  . . .   Fd(X d)) = −Hα(F1(X 1)  F2(X 2)  . . .   Fd(X d)) .
In other words  calculation of mutual information can be reduced to the calculation of entropy pro-
vided that marginal c.d.f.’s F1  F2  . . .   Fd are known. The problem is  of course  that these are not

known and need to be estimated from the sample. We will use empirical c.d.f.’s ((cid:98)F1 (cid:98)F2  . . .  (cid:98)Fd)

as their estimates. Given an i.i.d. sample X1:n = (X1  X2  . . .   Xn) from distribution µ and with
density f  the empirical c.d.f’s are deﬁned as

(cid:98)Fj(x) =

Introduce the compact notation F : Rd → [0  1]d (cid:98)F : Rd → [0  1]d 

|{i : 1 ≤ i ≤ n  x ≤ X j
i }|

1
n

for x ∈ R  1 ≤ j ≤ d .

F(x1  x2  . . .   xd) = (F1(x1)  F2(x2)  . . .   Fd(xd))

(cid:98)F(x1  x2  . . .   xd) = ((cid:98)F1(x1) (cid:98)F2(x2)  . . .  (cid:98)Fd(xd))

for (x1  x2  . . .   xd) ∈ Rd ;
for (x1  x2  . . .   xd) ∈ Rd .

(9)
(10)

1
n

i =

1   X j

(cid:98)Z j

rank(X j

i  {X j

2   . . .   X j

where rank(x  A) is the number of element of A less than or equal to x. Also  observe

respectively. The joint distribution of F(X) = (F1(X 1)  F2(X 2)  . . .   Fd(X d)) is called the copula

Let us call the maps F  (cid:98)F the copula transformation  and the empirical copula transformation 
of µ  and the sample ((cid:98)Z1 (cid:98)Z2  . . .  (cid:98)Zn) = ((cid:98)F(X1) (cid:98)F(X2)  . . .  (cid:98)F(Xn)) is called the empirical
copula (Dedecker et al.  2007). Note that j-th coordinate of(cid:98)Zi equals
n})  
that the random variables (cid:98)Z1 (cid:98)Z2  . . .  (cid:98)Zn are not even independent! Nonetheless  the empiri-
cal copula ((cid:98)Z1 (cid:98)Z2  . . .  (cid:98)Zn) is a good approximation of an i.i.d.
(cid:98)Iα(X1:n) = −(cid:98)Hα((cid:98)Z1 (cid:98)Z2  . . .  (cid:98)Zn) 

sample (Z1  Z2  . . .   Zn) =
(F(X1)  F(X2)  . . .   F(Xn)) from the copula of µ. Hence  we estimate the R´enyi mutual infor-
mation Iα by

where (cid:98)Hα is deﬁned by (5). The following theorem is our main result about the estimator (cid:98)Iα. It
states that(cid:98)Iα is strongly consistent and gives upper bounds on the rate of convergence. The proof of
Theorem 3 (Consistency and Rate for (cid:98)Iα). Let d ≥ 3 and α = 1 − p/d ∈ (1/2  1). Let µ be an

absolutely continuous distribution over Rd with density f. If X1:n = (X1  X2  . . .   Xn) is an i.i.d.
sample from µ then

this theorem can be found in P´al et al. (2010).

(11)

Moreover  if the density of the copula of µ is Lipschitz  then for any δ > 0 with probability at least
1 − δ 

lim

a.s.

n→∞(cid:98)Iα(X1:n) = Iα(f )
(cid:16)
(cid:16)
(cid:16)

− d−p
− d−p
− d−p

max{n
max{n
max{n

d(2d−p)   n−p/2+p/d}(log(1/δ))1/2(cid:17)
d(2d−p)   n−1/2+p/d}(log(1/δ))1/2(cid:17)
d(d+1)   n−1/2+p/d}(log(1/δ))1/2(cid:17)

 



O

O

O

 

 

if 0 < p ≤ 1 ;
if 1 ≤ p ≤ d − 1 ;
if d − 1 ≤ p < d .

(cid:12)(cid:12)(cid:12)(cid:98)Iα(X1:n) − Iα(f )
(cid:12)(cid:12)(cid:12) ≤

6 Experiments

In this section we show two numerical experiments to support our theoretical results about the con-
vergence rates  and to demonstrate the applicability of the proposed R´enyi mutual information esti-

mator (cid:98)Iα.

5

6.1 The Rate of Convergence

In our ﬁrst experiment (Fig. 1)  we demonstrate that the derived rate is indeed an upper bound on

the convergence rate. Figure 1a-1c show the estimation error of (cid:98)Iα as a function of the sample

size. Here  the underlying distribution was a 3D uniform  a 3D Gaussian  and a 20D Gaussian with
randomly chosen nontrivial covariance matrices  respectively. In these experiments α was set to 0.7.
For the estimation we used S = {3} (kth) and S = {1  2  3} (knn) sets. Our results also indicate that
these estimators achieve better performances than the histogram based plug-in estimators (hist). The
number and the sizes of the bins were determined with the rule of Scott (1979). The histogram based
estimator is not shown in the 20D case  as in this large dimension it is not applicable in practice. The
ﬁgures are based on averaging 25 independent runs  and they also show the theoretical upper bound
(Theoretical) on the rate derived in Theorem 3. It can be seen that the theoretical rates are rather
conservative. We think that this is because the theory allows for quite irregular densities  while the
densities considered in this experiment are very nice.

(a) 3D uniform

(b) 3D Gaussian

(c) 20D Gaussian

Figure 1: Error of the estimated R´enyi informations in the number of samples.

6.2 Application to Independent Subspace Analysis

An important application of dependence estimators is the Independent Subspace Analysis problem
(Cardoso  1998). This problem is a generalization of the Independent Component Analysis (ICA) 
where we assume the independent sources are multidimensional vector valued random variables.
The formal description of the problem is as follows. We have S = (S1; . . . ; Sm) ∈ Rdm  m inde-
pendent d-dimensional sources  i.e. Si ∈ Rd  and I(S1  . . .   Sm) = 0.4 In the ISA statistical model
we assume that S is hidden  and only n i.i.d. samples from X = AS are available for observation 
where A ∈ Rq×dm is an unknown invertible matrix with full rank and q ≥ dm. Based on n i.i.d.
observation of X  our task is to estimate the hidden sources Si and the mixing matrix A. Let the
estimation of S be denoted by Y = (Y1; . . . ; Ym) ∈ Rdm  where Y = WX. The goal of ISA is
to calculate argminWI(Y1  . . .   Ym)  where W ∈ Rdm×q is a matrix with full rank. Following the
ideas of Cardoso (1998)  this ISA problem can be solved by ﬁrst preprocessing the observed quan-
tities X by a traditional ICA algorithm which provides us WICA estimated separation matrix5  and
then simply grouping the estimated ICA components into ISA subspaces by maximizing the sum of
the MI in the estimated subspaces  that is we have to ﬁnd a permutation matrix P ∈ {0  1}dm×dm
which solves

m(cid:88)

max

I(Y j

1   Y j

2   . . .   Y j

d ) .

(12)

where Y = PWICAX. We used the proposed copula based information estimation  (cid:98)Iα with

α = 0.99 to approximate the Shannon mutual information  and we chose S = {1  2  3}. Our
experiment shows that this ISA algorithm using the proposed MI estimator can indeed provide good

j=1

P

4Here we need the generalization of MI to multidimensional quantities  but that is obvious by simply re-

placing the 1D marginals by d-dimensional ones.

5for simplicity we used the FastICA algorithm in our experiments (Hyv¨arinen et al.  2001)

6

10210310−210−1100101 kthknnhistTheoretical10210310−210−1100101 kthknnhistTheoretical102103104100101 kthknnTheoreticalestimation of the ISA subspaces. We used a standard ISA benchmark dataset from Szab´o et al.
(2007); we generated 2 000 i.i.d. sample points on 3D geometric wireframe distributions from 6
different sources independently from each other. These sampled points can be seen in Fig. 2a  and
they represent the sources  S. Then we mixed these sources by a randomly chosen invertible matrix
A ∈ R18×18. The six 3-dimensional projections of X = AS observed quantities are shown in
Fig. 2b. Our task was to estimate the original sources S using the sample of the observed quantity
X only. By estimating the MI in (12)  we could recover the original subspaces as it can be seen
in Fig. 2c. The successful subspace separation is shown in the form of Hinton diagrams as well 
which is the product of the estimated ISA separation matrix W = PWICA and A. It is a block
permutation matrix if and only if the subspace separation is perfect (Fig. 2d).

(a) Original

(b) Mixed

(c) Estimated

(d) Hinton

Figure 2: ISA experiment for six 3-dimensional sources.

7 Further Related Works

As it was pointed out earlier  in this paper we heavily built on the results known from the theory of
Euclidean functionals (Steele  1997; Redmond and Yukich  1996; Koo and Lee  2007). However 
now we can be more precise about earlier work concerning nearest-neighbor based Euclidean func-
tionals: The closest to our work is Section 8.3 of Yukich (1998)  where the case of N NS graph
based p-power weighted Euclidean functionals with S = {1  2  . . .   k} and p = 1 was investigated.
Nearest-neighbor graphs have ﬁrst been proposed for Shannon entropy estimation by Kozachenko
and Leonenko (1987).
In particular  in the mentioned work only the case of N NS graphs with
S = {1} was considered. More recently  Goria et al. (2005) generalized this approach to S = {k}
and proved the resulting estimator’s weak consistency under some conditions on the density. The
estimator in this paper has a form quite similar to that of ours:

˜H1 = log(n − 1) − ψ(k) + log

(cid:18) 2πd/2

(cid:19)

dΓ(d/2)

n(cid:88)

i=1

+

d
n

log (cid:107)ei(cid:107) .

Here ψ stands for the digamma function  and ei is the directed edge pointing from Xi to its kth
nearest-neighbor. Comparing this with (5)  unsurprisingly  we ﬁnd that the main difference is the
use of the logarithm function instead of | · |p and the different normalization. As mentioned before 
Leonenko et al. (2008) proposed an estimator that uses the N NS graph with S = {k} for the purpose
of estimating the R´enyi entropy. Their estimator takes the form

˜Hα =

1

1 − α

log

(cid:32)

n − 1
n

(cid:33)

 

n(cid:88)
(cid:105)1/(1−α)

(cid:107)ei(cid:107)d(1−α)
(n − 1)α

i=1

k

d C 1−α
V 1−α
(cid:104)

Γ(k)

Γ(k+1−α)

and Vd = πd/2Γ(d/2 + 1)
where Γ stands for the Gamma function  Ck =
is the volume of the d-dimensional unit ball  and again ei is the directed edge in the N NS graph
starting from node Xi and pointing to the k-th nearest node. Comparing this estimator with (5) 
it is apparent that it is (essentially) a special case of our N NS based estimator. From the results
of Leonenko et al. (2008) it is obvious that the constant γ in (5) can be found in analytical form
when S = {k}. However  we kindly warn the reader again that the proofs of these last three cited
articles (Kozachenko and Leonenko  1987; Goria et al.  2005; Leonenko et al.  2008) contain a few
errors  just like the Wang et al. (2009b) paper for KL divergence estimation from two samples.
Kraskov et al. (2004) also proposed a k-nearest-neighbors based estimator for the Shannon mutual
information estimation  but the theoretical properties of their estimator are unknown.

7

8 Conclusions and Open Problems

We have studied R´enyi entropy and mutual information estimators based on N NS graphs. The
estimators were shown to be strongly consistent. In addition  we derived upper bounds on their
convergence rate under some technical conditions. Several open problems remain unanswered:
An important open problem is to understand how the choice of the set S ⊂ N+ affects our estimators.
Perhaps  there exists a way to choose S as a function of the sample size n (and d  p) which strikes
the optimal balance between the bias and the variance of our estimators.
Our method can be used for estimation of Shannon entropy and mutual information by simply using
α close to 1. The open problem is to come up with a way of choosing α  approaching 1  as a
function of the sample size n (and d  p) such that the resulting estimator is consistent and converges
as rapidly as possible. An alternative is to use the logarithm function in place of the power function.
However  the theory would need to be changed signiﬁcantly to show that the resulting estimator
remains strongly consistent.

In the proof of consistency of our mutual information estimator (cid:98)Iα we used Kiefer-Dvoretzky-

Wolfowitz theorem to handle the effect of the inaccuracy of the empirical copula transformation
(see P´al et al. (2010) for details). Our particular use of the theorem seems to restrict α to the interval
(1/2  1) and the dimension to values larger than 2. Is there a better way to estimate the error caused
by the empirical copula transformation and prove consistency of the estimator for a larger range of
α’s and d = 1  2?
Finally  it is an important open problem to prove bounds on converge rates for densities that have
higher order smoothness (i.e. β-H¨older smooth densities). A related open problem  in the context of
of theory of Euclidean functionals  is stated in Koo and Lee (2007).

Acknowledgements

This work was supported in part by AICML  AITF (formerly iCore and AIF)  NSERC  the PAS-
CAL2 Network of Excellence under EC grant no. 216886 and by the Department of Energy under
grant number DESC0002607. Cs. Szepesv´ari is on leave from SZTAKI  Hungary.

References
C. Adami. Information theory in molecular biology. Physics of Life Reviews  1:3–22  2004.
M. Aghagolzadeh  H. Soltanian-Zadeh  B. Araabi  and A. Aghagolzadeh. A hierarchical clustering based on

mutual information maximization. In in IEEE ICIP  pages 277–280  2007.

P. A. Alemany and D. H. Zanette. Fractal random walks from a variational formalism for Tsallis entropies.

Phys. Rev. E  49(2):R956–R958  Feb 1994.

J. Cardoso. Multidimensional independent component analysis. Proc. ICASSP’98  Seattle  WA.  1998.
B. Chai  D. B. Walther  D. M. Beck  and L. Fei-Fei. Exploring functional connectivity of the human brain using

multivariate information analysis. In NIPS  2009.

J. A. Costa and A. O. Hero. Entropic graphs for manifold learning. In IEEE Asilomar Conf. on Signals  Systems 

and Computers  2003.

J. Dedecker  P. Doukhan  G. Lang  J.R. Leon  S. Louhichi  and C Prieur. Weak Dependence: With Examples

and Applications  volume 190 of Lecture notes in Statistics. Springer  2007.

M. N. Goria  N. N. Leonenko  V. V. Mergel  and P. L. Novi Inverardi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics  17:
277–297  2005.

A. O. Hero and O. J. Michel. Asymptotic theory of greedy approximations to minimal k-point random graphs.

IEEE Trans. on Information Theory  45(6):1921–1938  1999.

A. O. Hero  B. Ma  O. Michel  and J. Gorman. Alpha-divergence for classiﬁcation  indexing and retrieval 

2002a. Communications and Signal Processing Laboratory Technical Report CSPL-328.

A. O. Hero  B. Ma  O. Michel  and J. Gorman. Applications of entropic spanning graphs.

Processing Magazine  19(5):85–95  2002b.

IEEE Signal

8

K. Hlav´ackova-Schindler  M. Paluˆsb  M. Vejmelkab  and J. Bhattacharya. Causality detection based on

information-theoretic approaches in time series analysis. Physics Reports  441:1–46  2007.

M. M. Van Hulle. Constrained subspace ICA based on mutual information optimization directly. Neural

Computation  20:964–973  2008.

A. Hyv¨arinen  J. Karhunen  and E. Oja. Independent Component Analysis. John Wiley  New York  2001.
Y. Koo and S. Lee. Rates of convergence of means of Euclidean functionals. Journal of Theoretical Probability 

20(4):821–841  2007.

L. F. Kozachenko and N. N. Leonenko. A statistical estimate for the entropy of a random vector. Problems of

Information Transmission  23:9–16  1987.

A. Kraskov  H. St¨ogbauer  and P. Grassberger. Estimating mutual information. Phys. Rev. E  69:066138  2004.
J. Kybic. Incremental updating of nearest neighbor-based high-dimensional entropy estimation. In Proc. Acous-

tics  Speech and Signal Processing  2006.

E. Learned-Miller and J. W. Fisher. ICA using spacings estimates of entropy. Journal of Machine Learning

Research  4:1271–1295  2003.

N. Leonenko  L. Pronzato  and V. Savani. A class of R´enyi information estimators for multidimensional densi-

ties. Annals of Statistics  36(5):2153–2182  2008.

J. Lewi  R. Butera  and L. Paninski. Real-time adaptive information-theoretic optimization of neurophysiology

experiments. In Advances in Neural Information Processing Systems  volume 19  2007.

D. P´al  Cs. Szepesv´ari  and B. P´oczos. Estimation of R´enyi entropy and mutual information based on general-

ized nearest-neighbor graphs  2010. http://arxiv.org/abs/1003.1954.

H. Peng and C. Ding. Feature selection based on mutual information: Criteria of max-dependency  max-

relevance  and min-redundancy. IEEE Trans On Pattern Analysis and Machine Intelligence  27  2005.

B. P´oczos and A. L˝orincz.

673–680  2005.

Independent subspace analysis using geodesic spanning trees.

In ICML  pages

B. P´oczos and A. L˝orincz. Identiﬁcation of recurrent neural networks by Bayesian interrogation techniques.

Journal of Machine Learning Research  10:515–554  2009.

B. P´oczos  S. Kirshner  and Cs. Szepesv´ari. REGO: Rank-based estimation of R´enyi information using Eu-

clidean graph optimization. In AISTATS 2010  2010.

C. Redmond and J. E. Yukich. Asymptotics for Euclidean functionals with power-weighted edges. Stochastic

processes and their applications  61(2):289–304  1996.

D. W. Scott. On optimal and data-based histograms. Biometrika  66:605–610  1979.
C. Shan  S. Gong  and P. W. Mcowan. Conditional mutual information based boosting for facial expression

recognition. In British Machine Vision Conference (BMVC)  2005.

J. M. Steele. Probability Theory and Combinatorial Optimization. Society for Industrial and Applied Mathe-

matics  1997.

Z. Szab´o  B. P´oczos  and A. L˝orincz. Undercomplete blind subspace deconvolution. Journal of Machine

Learning Research  8:1063–1095  2007.

A. B. Tsybakov and E. C. van der Meulen. Root-n consistent estimators of entropy for densities with unbounded

support. Scandinavian Journal of Statistics  23:75–83  1996.

O. Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical Society  Series B 

38:54–59  1976.

Q. Wang  S. R. Kulkarni  and S. Verd´u. Universal estimation of information measures for analog sources.

Foundations and Trends in Communications and Information Theory  5(3):265–352  2009a.

Q. Wang  S. R. Kulkarni  and S. Verd´u. Divergence estimation for multidimensional densities via k-nearest-

neighbor distances. IEEE Transactions on Information Theory  55(5):2392–2405  2009b.

E. Wolsztynski  E. Thierry  and L. Pronzato. Minimum-entropy estimation in semi-parametric models. Signal

Process.  85(5):937–949  2005.

J. E. Yukich. Probability Theory of Classical Euclidean Optimization Problems. Springer  1998.

9

,Jifeng Dai
Yi Li
Kaiming He
Jian Sun