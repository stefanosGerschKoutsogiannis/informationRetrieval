2011,Sequence learning with hidden units in spiking neural networks,We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons significantly improves the storing capacity of the network. Furthermore  we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike  potentiation is induced and otherwise depression is elicited.,Sequence learning with hidden units

in spiking neural networks

Johanni Brea  Walter Senn and Jean-Pascal Pﬁster

Department of Physiology

University of Bern

B¨uhlplatz 5

CH-3012 Bern  Switzerland

{brea  senn  pfister}@pyl.unibe.ch

Abstract

We consider a statistical framework in which recurrent networks of spiking neu-
rons learn to generate spatio-temporal spike patterns. Given biologically realistic
stochastic neuronal dynamics we derive a tractable learning rule for the synaptic
weights towards hidden and visible neurons that leads to optimal recall of the train-
ing sequences. We show that learning synaptic weights towards hidden neurons
signiﬁcantly improves the storing capacity of the network. Furthermore  we de-
rive an approximate online learning rule and show that our learning rule is consis-
tent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly
precedes a postynaptic spike  potentiation is induced and otherwise depression is
elicited.

1 Introduction

Learning to produce temporal sequences is a general problem that the brain needs to solve. Move-
ments  songs or speech  all require the generation of speciﬁc spatio-temporal patterns of neural
activity that have to be learned. Early attempts to model sequence learning used a simple asym-
metric Hebbian learning rule [10  20  6] and succeeded to store sequences of random patterns  but
perform poorly as soon as there are temporal correlations between the patterns [3].

Later work on pattern storage or sequence learning recognized the need for matching the storage
rule with the recall dynamics [2  18  12] and derived the optimal storage rule for a given recall
dynamics [2  18] or an optimal recall dynamics for a given storage rule [12]  but didn’t consider
hidden neurons and therefore restricted the class of possible patterns to be learned. Other studies
[14] included a reservoir of hidden neurons but assumed weights towards the hidden neurons to be
ﬁxed. Finally  Boltzmann machines [1] - which learn to produce a given distribution of patterns with
visible and hidden neurons - applied to sequence learning [9  22  21] are trained with Contrastive
Divergence [8] and either an approximation that neglects the inﬂuence of the future or use a non-
local and non-causal learning rule.

Here we start by deﬁning a stochastic neuronal dynamics - that can be arbitrarily complicated (e.g.
with non-Markovian dependencies). This stochastic dynamics deﬁnes the overall probability dis-
tribution which is parametrized by the synaptic weights. The goal of learning is to adapt synaptic
weights such that the model distribution approximates as good as possible the target distribution
of temporal sequences. This can be seen as the extension of the maximum likelihood approach
of Barber [2] where we add stochastic hidden neurons with plastic weights. In order to learn the
weights  we implement a variant of the Expectation-Maximization (EM) algorithm [5] where we
use importance sampling in the expectation step in a way that makes the sampling procedure easy.

1

A

B

ht−1

ht

vt−1

vt

stochastic
hidden
neurons

stochastic
visible
neurons

ht−1

ht

vt−1

vt

Figure 1: Graphical representation of the conditional dependencies of the joint distribution over
visible and hidden sequences. A Graphical model used for the derivation of the learning rule in
section 2 and the example in section 4. B Markovian model used in the example with binary neurons
in section 3.

The resulting learning rule is local (but modulated by a global factor)  causal and biologically rele-
vant in the sense that it shares important features with Spike-Timing Dependent Plasticity (STDP).
We also derive an online version of the learning rule and show numerically that it performs almost
equally well as the exact batch learning rule.

2 Learning a distribution of sequences

Let us consider temporal sequences v = {vt i|t = 0 . . . T  i = 1 . . . Nv} of Nv visible neurons
over the interval [0  T ]. We will use the notation vt = {vt i|i = 1 . . . Nv} and vt1:t2 = {vt i|t =
t1 . . . t2  i = 1 . . . Nv} to denote parts of the sequence. Note that v = v0:T denotes the whole
sequence. Those visible sequences v are drawn i.i.d. from a target distribution P ∗(v) that must
be learned by a model which consists of Nv visible neurons and Nh hidden neurons. The model

distribution over those visible sequences is denoted by Pθ(v) =Ph Pθ(v  h) where θ denotes the

model parameters  h = {ht i|t = 0 . . . T  i = 1 . . . Nh} the hidden temporal sequence and Pθ(v  h)
the joint distribution over the visible and the hidden sequences. The natural way to quantify the
mismatch between the target distribution P ∗(v) and the model distribution Pθ(v) is given by the
Kullback-Leibler divergence:

If the joint model distribution Pθ(v  h) is differentiable with respect to the model parameters θ  then
the sequence learning problem can be phrased as gradient descent on the KL divergence in Eq. (1):

P ∗(v) log

P ∗(v)
Pθ(v)

.

(1)

DKL(P ∗(v)||Pθ(v)) =Xv
∆θ = η(cid:28) ∂ log Pθ(v  h)

∂θ

(cid:29)Pθ (h|v)P ∗(v)

 

(2)

∂

∂θPh Pθ(v  h) =

where η is the learning rate and we used the fact that ∂

∂θ log Pθ(v) = 1

Pθ (v)

Ph Pθ(h|v) ∂

∂θ log Pθ(v  h). Eq. (2) can be seen as a variant of the EM algorithm [5  16  3] where
the expectation h·iPθ (h|v)P ∗(v) corresponds to the E step and the gradient of log Pθ(v  h) is related
to the M step1.
Instead of calculating analytically the true expectation in Eq. (2)  it is possible to approximate it
by sampling the visible sequences v from the target distribution P ∗(v) and the hidden sequences
from the posterior distribution Pθ(h|v) given the visible ones. Note that the posterior distribution
Pθ(h|v) could be hard to sample from. Indeed  at a time t the posterior distribution over ht does not
only depend on the past visible activity but also on the future visible activity  since it is conditioned
on the whole visible activity v0:T from time step 0 to T . This renders a true challenge for on-
line algorithms. In the case of Hidden Markov Model training  the forward-backward algorithm

1Strictly speaking the M step of the EM algorithm directly calculates the solution θnew for which

∂

∂θ log Pθ(v  h) = 0 whereas in Eq. (2) there is only one step done in the direction of the gradient.

2

[4  19] combines information from the past (by forward ﬁltering) and from the future (by backward
smoothing) to calculate Pθ(h|v).
If the statistical model does not have the Markovian property  the problem of calculating Pθ(h|v)
(or sampling from it) becomes even harder. Here  we propose an alternative solution that does not
require to sample from Pθ(h|v) and does not require the Markovian assumption (see [11  17] for
other approaches on sampling Pθ(h|v)). We exploit that in all neuronal network models of interest 
neuronal ﬁring at any time point is conditionally independent given the past activity of the network.
Using the chain rule this means that we can write the joint distribution Pθ(v  h) (see Fig. 1A) as

|

Pθ(v  h) = Pθ(v0)

TYt=1

NvYi=1

Pθ(vt i|v0:t−1  h0:t−1)!
}
{z

Rθ (v|h)

 Pθ(h0)
|

TYt=1

NhYi=1

Pθ(ht i|v0:t−1  h0:t−1)!
}
{z

Qθ (h|v)

 

(3)
where Rθ(v|h) is easy to calculate (see below) and Qθ(h|v) is easy to sample from. The sampling
can be accomplished by clamping the visible neurons to a target sequence v and let the hidden
dynamics run  i.e. at time t  ht is sampled from Pθ(ht|v0:t−1h0:t−1). 2 From Eq. (3)  the posterior
distribution Pθ(h|v) can be written as

Pθ(h|v) =

Rθ(v|h)Qθ(h|v)

Pθ(v)

 

(4)

where the marginal distribution over the visible sequences v can be also expressed as Pθ(v) =
hRθ(v|h)iQθ (h|v). As a consequence  by using Eq. (4)  the learning rule in Eq. (2) can be rewritten
as

∆θ =Xv h

P ∗(v)Pθ(h|v)

∂ log Pθ(v  h)

∂θ

P ∗(v)Qθ(h|v)

Rθ(v|h)
Pθ(v)

∂ log Pθ(v  h)

∂θ

= η*

Rθ(v|h)

∂ log Pθ(v  h)

hRθ(v|h′)iQθ (h′|v)

∂θ

+Qθ (h|v)P ∗(v)

.

(5)

=Xv h

Instead of calculating the true expectation  Eq. (5) can be evaluated by using N samples (see algo-
rithm 1) where the factor γθ(v  h) := Rθ(v|h)/ hRθ(v|h′)iQθ (h′|v) acts as the importance weight
[15]. Note that in the absence of hidden neurons  this factor γθ(v  h) is equal to one and the maxi-
mum likelihood learning rule [2  18] is recovered.

2Note that for other conditional dependencies it might be reasonable to split Pθ(h|v) differently. For
example in models with the structure of Hidden Markov Models one could make use of the fact that
θ(ht+1|ht)
Pθ(h|v) = QT −1
θ(ht+1|v0:t) Pθ(ht|v0:t) and take the product of ﬁltering
distributions Qθ(h|v) = QT −1
t=0 Pθ(ht|v0:t) to sample from and use the importance weights Rθ(v  h) =
θ (ht+1|ht)
QT −1
θ (ht+1|v0:t) . Following the reasoning in the main text one ﬁnds an alternative to the forward-backward
t=0
algorithm [4  19] that might be interesting to investigate further.

t=0 Pθ(ht|v0:t  ht+1) = QT −1

t=0

P

P

P

P

Algorithm 1 Sequence learning (batch mode)

Set an initial θ
while θ not converged do

v ∼ P ∗(v)
α(v) = 0  Pθ(v) = 0
for i = 1 . . . N do

h ∼ Qθ(h|v)
α(v) ← α(v) + Rθ(v|h) ∂ log Pθ (v h)
∂θ
Pθ(v) ← Pθ(v) + N −1Rθ(v|h)

end for
θ ← θ + η α(v)
Pθ (v)

end while
return θ

3

A

B

C

G

r
e
b
m
u
n

t
i
n
u

10

20

30

r
e
b
m
u
n

t
i
n
u

20

40

60

10 20 30
time step

10 20 30
time step

10 20 30
time step

D

E

F

10 20 30
time step

10 20 30
time step

10 20 30
time step

e
c
n
a
m

r
o
f
r
e
p

1.
0.9
0.8
0.7
0.6
0.5

0

7500

15 000

learning step

H

I

J

r
e
b
m
u
n

t
i
n
u

10
20
30

10 20 30
tim e step

10
20
30
40

10 20 30

10 20 30

time step

time step

Figure 2: Learning a non-Markovian sequence of temporally correlated and linearly dependent states
with different learning rules. A The target distribution contained only this training pattern for 30
visible neurons and 30 time steps. B-F  H-J Overlay of 20 recalls after learning with 15 000 training
pattern presentations  B with only visible neurons and a simple asymmetric Hebb rule (see main
text) C only visible neurons and learning rule Eq. (5) D static weights towards 30 hidden neurons
(Reservoir Computing) E learning rule Eq. (5)  F online approximation Eq. (14). G Learning curves
for the training pattern in A for only visible neurons (black line)  static weights towards hidden (blue
line)  online learning approximation (purple line) exact learning rule (red line). The performance
was measured in one minus average Hamming distance per neuron per time step (see main text).
H A training pattern that exhibits a gap of 5 time-steps. I Recall with a network of 30 visible and
10 hidden neurons without learning the weights towards hidden neurons. J Recall after training the
same network with learning rule Eq. (5).

3 Binary neurons

In order to illustrate the learning rule given by Eq. (5)  let us consider sequences of binary pat-
terns. Let x denote the activity of the visible and hidden neurons  i.e. x = (v  h). Since the
individual neurons are binary xt i ∈ {−1  1}  their distribution is given by Pθ(xt i|x0:t−1) =
(ρt iδt)(1+xt i)/2(1 − ρt iδt)(1−xt i)/2  where the ﬁring rate ρt i of neuron i at time t is given by
a monotonically increasing (and non-linear) function g of its membrane potential ut i  i.e.

ρt i = g(ut i) with ut i =Xj

wij xt−1 j .

(6)

Note that these assumptions lead to Markovian neuronal dynamics i.e. Pθ(xt i|x0:t−1) =
Pθ(xt i|xt−1) (see Fig. 1B). Further calculations will be slightly simpliﬁed  if we assume that the
non-linear function g is constraint by the following differential equation dg(u)/du = βg(u)(1 −
g(u)δt). Note that in the limit of δt → 0  this function is an exponential  i.e. g(u) = g0 exp(βu) and

for ﬁnite δt  it is a sigmoidal and takes the form g(u) = δt−1(cid:0)1 +(cid:0)(g0δt)−1 − 1(cid:1) exp(−βu)(cid:1)−1 

where we constrained the solutions such that g(0) = g0 in order to be consistent with the case where
δt → 0.
For the distribution over the initial conditions Pθ(v0) and Pθ(h0) we choose delta distributions such
that v0 is equal to the ﬁrst state of the training sequence and h0 is an arbitrary but ﬁxed vector of
binary values. If we assume that the weights wij are the only adaptable parameters in this model 

4

A

e
c
n
a
m

r
o
f
r
e
p

1.0

0.9

0.8

0.7

0.6

0.5

20

40

80
number of hidden units

60

B

1.0

0.9

0.8

0.7

0.6

0.5

e
c
n
a
m
r
o
f
r
e
p

100

20

60

40
80
sequence length

100

Figure 3: Adding trainable hidden neurons leads to much better recall performance than having static
hidden neurons or no hidden neurons at all. A Comparison of the performance after 20000 learning
cycles between static (blue curve) and dynamic weights (red curve) towards hidden neurons for a
network with 30 visible and different numbers of hidden neurons in a training task with a uncorre-
lated random pattern of length 60 time steps. For B we generated random  uncorrelated sequences of
different length and compared the performance after 20000 learning cycles for only visible neurons
(black curve)  static weights towards hidden (blue curve) and dynamic weights towards hidden (red
curve).

we have

∂ log Pw(xt i|x0:t−1)

∂wij

=

1

2(cid:18)(1 + xt i)

g′(ut i)
g(ut i)

− (1 − xt i)

With the above assumption on g(u) and Eq. (3) and (6) we ﬁnd

g′(ut i)δt

1 − g(ut i)δt(cid:19) ∂ut i

∂wij

.

(7)

∂ log Pw(x)

∂wij

=

β
2

TXt=1

(xt i − hxt iiPθ (xt i|xt−1))xt−1 j  

(8)

where hxt iiPθ (xt i|xt−1) = g(ut i)δt − (1 − g(ut i)δt) and the indices i and j run over all visible
and hidden neurons. The factor Rw(v|h) can be expressed as

Rw(v|h) = exp  1

2

TXt=0

NvXi=1

(1 + vt i) log(ρt iδt) + (1 − vt i) log(1 − ρt iδt)! .

(9)

t }T

t=0 v∗

t+1 iv∗

ﬁcult pattern to learn with a simple asymmetric Hebb rule ∆wij ∝PT
by one minus the Hamming distance per visible neuron and time step 1−(T Nv)−1Pt i |vt i−v∗

Let us now consider a simple case (Fig. 2) where the distribution over sequences is a delta distribu-
tion P ∗(v) = δ(v − v∗) around a single pattern v∗ (Fig. 2A) which is made of a set of temporally
t=0  i.e. a non-Markovian pattern  thus making it a dif-
correlated and linearly dependent states {v∗
t j (Fig. 2B) or only
visible neurons (Fig. 2C)  which are both Markovian learning rules. The performance was measured
t i|/2
between target pattern and recall pattern averaged over 100 runs. Adding hidden neurons without
learning the weights towards hidden neurons is similar to the idea used in the framework of Reser-
voir Computing (for a review see [13]): the visible states feed a ﬁxed reservoir of neurons that
returns a non-linear transformation of the input. Only the readout from hidden to visible neurons
and in our case the recurrent connections in the visible layer are trained. To assure a sensible distri-
bution of weights towards hidden units  we used the weights that were obtained after learning with
Eq. (5) and reshufﬂed them. Obviously  without training the reservoir the performance is always
worse compared to a system with an equal number of hidden neurons but dynamic weights (Fig. 2E
and 2F). With only a few hidden neurons our rule is also capable to learn patterns where the visi-
ble neurons are silent during a few time-steps. The training pattern in Fig. 2H exhibits a gap of 5
time steps. After learning the weights towards 10 hidden neurons with learning rule Eq. (5) recall
performance is nearly perfect (see Fig. 2J). With only visible neurons (not shown in Fig. 2) or static
weights towards hidden neurons the time gap was not learned (see Fig. 2I).

5


s
t
i
n
u
y
r
a
r
t
i
b
r
a


w
D

0

-40 -20

0

20
tpost -tpre ms

40

Figure 4: The learning rule Eq. (11) is compatible with Spike-Timing Dependent Plasticity (STDP):
the weight gets potentiated if a presynaptic spike is followed by a postsynaptic spike and depressed
otherwise. The time course of the postsynaptic potential and the refractory kernel is given in the
text.

In Fig. 3 we used again delta target distributions P ∗(v) = δ(v − v∗) with random uncorrelated
patterns v∗ of different length. Each model was trained with 20000 pattern presentations. For a
pattern of length 2Nv = 60 only Nv/2 = 15 trainable hidden neurons are sufﬁcient to reach perfect
recall (see Fig. 3A). This is in clear contrast to the case of static hidden weights. Again the static
weights were obtained by reshufﬂing those that we obtained after learning with Eq. (5). Fig. 3B
compares the capacity of our learning rule with Nh = Nv = 30 hidden neurons to the case of
no hidden neuron or static weights towards hidden neurons. Without learning the weights towards
hidden neurons the performance drops to almost chance level for sequences of 45 or more time
steps  whereas with our learning rule this decrease of performance occurs only at sequences of 100
or more time steps.

4 Limit to Continuous Time

Starting from the neurons in the last section we show that in the limit to continuous time we can
implement the sequence learning task with stochastic spiking neurons [7].

First note that the state of a neuron at time t in the model described in the previous section is

fully deﬁned by ut i := Pj wij xt−1 j (see Eq. (6)) and its spiking activity xt i. The weighted
sumPj wij xt−1 j is the response of neuron i to the spikes of its presynaptic neurons and its own

spikes. The terms in this sum depend on the previous time step only. In a more realistic model the
postsynaptic neuron feels the inﬂuence of presynaptic spikes through a perturbation of the membrane
potential on the order of a few milliseconds  which in the limit to continuous time clearly cannot be
modeled by a one-time step response. For a more realistic model we replace ut i in Eq. (6) by

ut i =

κsxt−s i

ǫsxt−s j

 

(10)

where xt−s i ∈ {0  1}. The kernel ǫ models the time-course of the response to a presynaptic spike
and κ the refractoriness. Our model holds for any choices of ǫ and κ  including for example a hard
refractory period where the neuron is forced not to spike.
In order to take the limit δt → 0 in Eq. (9) we note that we can scale Rw(v|h) without changing
the learning rule Eq. (5)  since there only the ratio Rθ(v|h)/ hRθ(v|h′)iQθ(h′|v) enters. We use the

scaling Rw(v|h) → eRw(v|h) := (g0δt)−Sv Rw(v|h)  where Sv denotes the total number of spikes
in the visible sequence v  i.e. Sv = PT

i=1 vt i. Note that for (0  1)-units the expectation in
Eq. (8) becomes hxt iiPθ (xt i|xt−1) = g(ut i)δt = ρt iδt . Now we take the limit δt → 0 in Eq. (8)

t=0PNv

6

∞Xs=1
|

=:xκ
t i

{z

}

+Xj6=i

wij

∞Xs=1
|

=:xǫ

t j

{z

}

and (9) and ﬁnd

∂ log Pw(x)

0

∂wij

=Z T
eRw(v|h) = exp Z T

0

dt β(xi(t) − ρi(t))xǫ

j (t)

βvi(t)ui(t) − ρi(t)!  

dt

NvXi=1

(11)

(12)

where the training pattern runs from time 0 to T   xi(t) = Pt(f )

) is the sum of delta
spikes of neuron i at times t(f )
i (t)) is the convolution
of presynaptic spike trains with the response kernel ǫ(t). With neuron i’s response to past spiking
j (t) and the escape rate function ρi(t) = g0 exp (βui(t)) we
activity ui(t) = xκ
recovered the deﬁning equations of a simpliﬁed stochastic spike response model [7].

j (t) =R ds ǫ(s)xj (t − s) (and similarly xκ

δ(t − t(f )

  xǫ

i

i

i

i (t) +Pj6=i wij xǫ

In Fig. 4 we display the weight change after forcing two neurons to ﬁre with a ﬁxed time lag. For the
ﬁgure we used the kernels ǫs ∝ exp(−s/τm)−exp(−s/τs) and κs ∝ − exp(−s/τm) with τm = 10
ms and τs = 2 ms. Our learning rule is consistent with STDP in the sense that a presynaptic spike
followed by a postsynaptic spike leads to potentiation and to depression otherwise. Note that this
result was also found in [18].

5 Approximate online version

Without hidden neurons the learning rule found by using Eq. (11) is straightforward to implement
in an online way where the parameters are updated at every moment in time according to ˙wij ∝
j (t) instead of waiting with the update until a training batch ﬁnished. Finding
(xi(t) − ρi(t))xǫ
an online version of the learning algorithm for networks with hidden neurons turns out to be a
challenge  since we need to know the whole sequences v and h in order to evaluate the importance
factor Rθ(v|h)/hRθ(v|h′)iQθ (h′|v). Here we propose to use in each time step an approximation
of the importance factor based on the network dynamics during the preceding period of typical
sequence length and multiply it by the low-pass ﬁltered change of parameters. We write this section
with xi(t) ∈ {0  1}  but similar expressions are easily found for xi(t) ∈ {−1  1}.

Algorithm 2 Sequence learning (online mode)

Set an initial wij  eij  a  ¯r  t
while wij not converged do
if t mod N T == 0 then

v ∼ P ∗(v)

end if
s = t mod T
if s < τ then

end if
x(s) = (v(s)  h(s))
eij ← (1 − δt
a ← (1 − δt
¯r ← (1 − δt
wij ← wij + η exp(a)
t ← t + δt

T )a +PNv

¯r

eij

N T )¯r + exp(a)

end while
return wij

h(s) ∼ P (h(s)) else h(s) ∼ Pw(h(s)|past spiking activity)

T )eij + β(xi(s) − ρi(s))xǫ

j (s)
i=1 βvi(s)ui(s) − ρi(s)

In Eq. (13a) and (13b) we summarize how to use low-pass ﬁlters to approximate the integrals in
Eq. (11) and Eq. (12). The time constant of the low-pass ﬁlter is chosen to match the sequence length
T . To ﬁnd an online estimate of hRθ(v  h′)iQθ (h′|v) we assume that a training pattern v ∼ P ∗(v)
is presented a few times in a row and after time N T   with N ∈ N  N ≫ 1  a new training pattern
is picked from the training distribution. Under this assumption we can replace the average over

7

hidden sequences by a low-pass ﬁlter of r with time constant N T   see Eq. (13c). At the beginning
of each pattern presentation - i.e. during the time interval [0  τ )  with τ on the order of the kernel
time constant τm - the hidden activity h(s) is drawn from a given distribution P (h(s)).

˙eij(t) = −

˙a(t) = −

1
T

1
T

a(t) +

NvXi=1

eij(t) + β(xi(t) − ρi(t))xǫ

j (t)

eij(T ) ≈

∂ log Pw(x)

∂wij

N T ˙¯r(t) = −¯r(t) + r(t) 

βvi(t)ui(t) − ρi(t)

exp(a(T )) ≈ Rw(v|h)

r(t) := exp(a(t))

¯r(N T ) ≈ hRθ(v  h′)iQθ (h′|v)

Finally we learn the model parameters in each time step according to

˙wij (t) = η

r(t)
¯r(t)

eij(t) .

(13a)

(13b)

(13c)

(14)

This online algorithm is certainly a rough approximation of the batch algorithm. Nevertheless  when
applied to the challenging example (Fig. 2A) in section 3  the performance of the online rule is close
to the one of the batch rule (Fig. 2F  G).

6 Discussion

Learning long and temporally correlated sequences with neural networks is a difﬁcult task. In this
paper we suggested a statistical model with hidden neurons and derived a learning rule that leads to
optimal recall of the learned sequences given the neuronal dynamics. The learning rule is derived by
minimizing the Kullback-Leibler divergence from training distribution to model distribution with a
variant of the EM-algorithm  where we use importance sampling to draw hidden sequences given the
visible training sequence. Choosing an appropriate distribution in the importance sampling step we
are able to circumvent inference which usually makes the training of non-Markovian models hard.
The resulting learning algorithm consists of a local term modulated by a global factor. We showed
that it is ready to be implemented with biologically realistic neurons and that an approximate online
version exists.

Our approach follows the ideas outlined in [2]  where sequence learning was considered with visible
neurons. Here we extended this model by adding stochastic hidden neurons that help to perform well
with sequences of linearly depend states - including non-Markovian sequences - or long sequences.
As in [18] we look at the limit of continuous time and ﬁnd that the learning rule is consistent with
Spike-Timing Dependent Plasticity. In contrast to Reservoir Computing [13] we train the weights
towards hidden neurons which clearly helps to improve performance. Our learning rule does not
need a “wake” and a “sleep” phase as we know it from Boltzmann machines [1  8].

Viewed in a different light our learning algorithm has a nice interpretation: as in reinforcement
learning  the hidden neurons explore different sequences  where each trial leads to a global reward
signal that modulates the weight change. However  in contrast to common reinforcement learning
the reward is not provided by an external teacher but depends solely on the internal dynamics and
the visible neurons do not explore but are clamped to the training sequence.

To make our model even more biologically relevant  future work should aim for a biological im-
plementation of the global importance factor that depends on the spike timing and the membrane
potential of all the visible neurons (see Eq. (9)). It would also be interesting to study online ap-
proximations of the learning algorithm in more detail or its application to models with the Hidden
Markov structure.

Acknowledgments

The authors thank Robert Urbanczik for helpful discussions. This work was supported by the Swiss
National Science Foundation (SNF)  grant 31-133094  and a grant from the Swiss SystemsX.ch
initiative (Neurochoice  evaluated by the SNF).

8

References

[1] D. Ackley and G. E. Hinton. A learning algorithm for boltzmann machines. Cognitive Science  9(1):147–

169  1985.

[2] D. Barber. Learning in spiking neural assemblies. Advances in Neural Information Processing Systems 

15  2003.

[3] D. Barber. Bayesian Reasoning and Machine Learning. Cambridge University Press  2011. In press.
[4] L. Baum  T. Petrie  G. Soules  and N. Weiss. A maximization technique occurring in the statistical analysis
of probabilistic functions of Markov chains. The Annals of Mathematical Statistics  41(1):164–171  1970.
[5] A. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.

Journal of the Royal Statistical Society. Series B (Methodological)  39(1):1–38  1977.

[6] A. D¨uring  A. Coolen  and D. Sherrington. Phase diagram and storage capacity of sequence processing

neural networks. Journal of Physics A: Mathematical and General  31:8607  1998.

[7] W. Gerstner and W. M. Kistler. Spiking neuron models: single neurons  populations  plasticity. Cambridge

University Press  2002.

[8] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation 

14(8):1771–800  2002.

[9] G. E. Hinton and A. Brown. Spiking boltzmann machines. Advances in Neural Information Processing

Systems  12  2000.

[10] J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.

Proceedings of the National Academy of Sciences of the United States of America  79(8):2554  1982.

[11] P. Latham and J. W. Pillow. Neural characterization in partially observed populations of spiking neurons.

Advances in Neural Information Processing Systems  20:1161–1168  2008.

[12] M. Lengyel  J. Kwag  O. Paulsen  and P. Dayan. Matching storage and recall: hippocampal spike timing-

dependent plasticity and phase response curves. Nature Neuroscience  8(12):1677–83  2005.

[13] M. Lukoˇseviˇcius and H. Jaeger. Reservoir computing approaches to recurrent neural network training.

Computer Science Review  3(3):127–149  2009.

[14] W. Maass  T. Natschl¨ager  and H. Markram. Real-time computing without stable states: a new framework

for neural computation based on perturbations. Neural Computation  14(11):2531–60  2002.

[15] D. J. C. MacKay. Information Theory  Inference & Learning Algorithms. Cambridge University Press 

2002.

[16] G. McLachlan and T. Krishnan. The EM Algorithm and Extensions. John Wiley and Sons  1997.
[17] Y. Mishchenko and L. Paninski. Efﬁcient methods for sampling spike trains in networks of coupled

neurons. The Annals of Applied Statistics  5(3):1893–1919  2011.

[18] J.-P. Pﬁster  T. Toyoizumi  D. Barber  and W. Gerstner. Optimal spike-timing-dependent plasticity for

precise action potential ﬁring in supervised learning. Neural Computation  18(6):1318–1348  2006.

[19] L. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Pro-

ceedings of the IEEE  77(2):257–86  1989.

[20] H. Sompolinsky and I. Kanter. Temporal association in asymmetric neural networks. Physical Review

Letters  57(22):2861–64  1986.

[21] I. Sutskever  G. E. Hinton  and G. Taylor. The Recurrent Temporal Restricted Boltzmann Machine.

Advances in Neural Information Processing Systems  21:1601–08  2009.

[22] G. Taylor  G. E. Hinton  and S. Roweis. Modeling human motion using binary latent variables. Advances

in Neural Information Processing Systems  19:1345–52  2007.

9

,Elias Bareinboim
Andrew Forney
Judea Pearl
Artem Sokolov
Julia Kreutzer
Stefan Riezler