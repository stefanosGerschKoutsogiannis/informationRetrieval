2016,Large-Scale Price Optimization via Network Flow,This paper deals with price optimization  which is to find the best pricing strategy that maximizes revenue or profit  on the basis of demand forecasting models. Though recent advances in regression technologies have made it possible to reveal price-demand relationship of a number of multiple products  most existing price optimization methods  such as mixed integer programming formulation  cannot handle tens or hundreds of products because of their high computational costs. To cope with this problem  this paper proposes a novel approach based on network flow algorithms. We reveal a connection between supermodularity of the revenue and cross elasticity of demand. On the basis of this connection  we propose an efficient algorithm that employs network flow algorithms. The proposed algorithm can handle hundreds or thousands of products  and returns an exact optimal solution under an assumption regarding cross elasticity of demand. Even in case in which the assumption does not hold  the proposed algorithm can efficiently find approximate solutions as good as can other state-of-the-art methods  as empirical results show.,Large-Scale Price Optimization via Network Flow

Shinji Ito

NEC Corporation

s-ito@me.jp.nec.com

Ryohei Fujimaki
NEC Corporation

rfujimaki@nec-labs.com

Abstract

This paper deals with price optimization  which is to ﬁnd the best pricing strategy
that maximizes revenue or proﬁt  on the basis of demand forecasting models.
Though recent advances in regression technologies have made it possible to reveal
price-demand relationship of a large number of products  most existing price
optimization methods  such as mixed integer programming formulation  cannot
handle tens or hundreds of products because of their high computational costs. To
cope with this problem  this paper proposes a novel approach based on network
ﬂow algorithms. We reveal a connection between supermodularity of the revenue
and cross elasticity of demand. On the basis of this connection  we propose an
efﬁcient algorithm that employs network ﬂow algorithms. The proposed algorithm
can handle hundreds or thousands of products  and returns an exact optimal solution
under an assumption regarding cross elasticity of demand. Even if the assumption
does not hold  the proposed algorithm can efﬁciently ﬁnd approximate solutions as
good as other state-of-the-art methods  as empirical results show.

1

Introduction

Price optimization is a central research topic with respect to revenue management in marketing
science [10  16  18]. The goal is to ﬁnd the best price strategy (a set of prices for multiple products)
that maximizes revenue or proﬁt. There is a lot of literature regarding price optimization [1  5  10 
13  17  18  20]  and signiﬁcant success has been achieved in industries such as online retail [7] 
fast-fashion [5]  hotels [13  14]  and airlines [16]. One key component in price optimization is
demand modeling  which reveals relationships between price and demand. Though traditional studies
have focused more on a single price-demand relationship  such as price elasticity of demand [13  14]
and the law of diminishing marginal utility [16]  multi-product relationships such as cross price
elasticity of demand [15] have recently received increased attention [5  17]. Recent advances in
regression technologies (non-linear  sparse  etc.) make demand modeling over tens or even hundreds
of products possible  and data oriented demand modeling has become more and more important.
Given demand models of multiple products  the role of optimization is to ﬁnd the best price strategy.
Most existing studies for multi-product price optimization employ mixed-integer programming [5  13 
14] due to the discrete nature of individual prices  but their methods cannot be applied to large scale
problems with tens or hundreds of products since their computational costs exponentially increases
over increasing numbers of products. Though restricting demand models might make optimization
problems tractable [5  7]  such approaches cannot capture complicated price-demand relationships
and often result in poor performance. Ito and Fujimaki [9] have recently proposed a prescriptive
price optimization framework to efﬁciently solve multi-product price optimization with non-linear
demand models. In this prescriptive price optimization  the problem is transformed into a sort of
binary quadratic programming problem  and they have proposed an efﬁcient relaxation method
based on semi-deﬁnite programming (SDP). Although their approach has signiﬁcantly improved
computational efﬁciency over that of mixed-integer approaches  the computational complexity of
their SDP formulation requires O(M 6) in theory  where M is the number of products  and it is not

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

sufﬁciently scalable for large scale problems with hundreds of products  as our empirical evaluation
show in Section 5.
The goal of this paper is to develop an efﬁcient algorithm for large scale multi-product price optimiza-
tion problems that can handle hundreds of products as well as ﬂexible demand models. Our main
technical contributions are two-fold. First  we reveal the connection between submodularity of the
revenue and cross elasticity of demand. More speciﬁcally  we show that the gross proﬁt function of
the prescriptive price optimization is supermodular (i.e.  the maximization of the gross proﬁt function
is equivalent to the submodular minimization) under the assumption regarding cross elasticity of
demand that there are no pairs of complementary goods (we refer to this property as a substitute-goods
property).1 On the basis of the submodularity  we propose a practical  efﬁcient algorithm that employs
network ﬂow algorithms for minimum cut problems and returns exact solutions for problems with the
substitute-goods property. Further  even in cases in which the property does not hold  it can efﬁciently
ﬁnd approximate solutions by iteratively improving submodular lower bounds. Our empirical results
show that the proposed algorithm can successfully handle hundreds of products and derive solutions
as good as other state-of-the-art methods  while its computational cost is much cheaper  regardless of
whether the substitute-goods property holds or not.

2 Literature review

Our price optimization problems are reduced to binary quadratic problems such as (4). It is well
known that submodular binary quadratic programming problems can be reduced to minimum cut
problems [12]  and hence it can be solved by maximum ﬂow algorithms. Also for unconstrained non-
submodular binary quadratic programming problems  there is a lot of literature regarding optimization
algorithm using minimum cut  especially in the context of Markov random ﬁelds inference or energy
minimization in computer vision [2  3  4  8  11  22]. Above all  QPBO method [2  11] and its
extensions such as QPBOI method [19] are known to be state-of-the-art methods in terms of scalability
and theoretical properties. These QPBO/QPBOI and our method are similar in that they all employ
network ﬂow algorithms and derive not only partial/approximate solutions but also lower bounds of
the exact optimal (minimum) value. Our methods  however  differs from QPBO and its extensions in
network structures  accuracy and scalability  as is shown in Section 5.

3 Price optimization and submodularity in cross elasticity of demand
Suppose we have M products and a product index is denoted by i ∈ {1  . . .   M}. In prescriptive price
optimization [9]  for a price strategy p = [p1  . . .   pM ](cid:62)  where pi is the price of the i-th product 
and for external variables r = [r1  . . .   rD](cid:62) such as weather  temperature and days of the week  the
sales quantity (demand) for the i-th product is modeled by the following regression formula:

git(rt) 

fij(pj) +

qi(p  r) =

(1)
where fii expresses the effect of price elasticity of demand  fij (i (cid:54)= j) reﬂects the effect of cross
elasticity  and git represent how the t-th external variable affect the sales quantity. Note that fij for
all (i  j) can be arbitrary functions  and Eq. (1) covers various regression (demand) models  such
as linear regression  additive models [21]  linear regression models with univariate basis functions 
etc. This paper assumes that the regression models are given using existing methods and focuses its
discussion on optimization.
Given qi(p) for all i and a cost vector c = [c1  . . .   cM ](cid:62)  and ﬁxed external variables r  the gross
proﬁt can be represented as

(cid:96)(p) =

(pi − ci)qi(p) =

(pi − ci)

fij(pj) +

git(rt)

(2)

i=1

i=1

j=1

t=1

1"Complementary goods" and "substitute goods" are terms in economics. A good example of complementary
goods might be wine and cheese  i.e.  if we discount wine  the sales of cheese will increase. An example of
substitute goods might be products of different brands in the same product category. If we discount one product 
sales of the other products will decrease.

2

D(cid:88)

 .

M(cid:88)

j=1

M(cid:88)

D(cid:88)

t=1

 M(cid:88)

M(cid:88)

The goal of price optimization is to ﬁnd p maximizing (cid:96)(p). In practice  pm is often chosen from
the ﬁnite set Pi = {Pi1  . . .   PiK} ⊆ R of K price candidates  where PiK might be a list price and
Pik (k < K) might be discounted prices such as 10%-off  5%-off  3%-off. Then  the problem of
maximizing the gross proﬁt can be formulated as the following combinatorial optimization problem:
(3)

subject to pi ∈ Pi.

Maximize

(cid:96)(p)

It is trivial to show that (3) is NP-hard in general.
Let us formally deﬁne the "substitute-goods property" as follows.
Deﬁnition 1 (Substitute-Goods Property). The demand model deﬁned by (1) of the i-th product is
said to satisfy the substitute-goods property if fij is monotone non-decreasing for all j (cid:54)= i.
The concept of substitute-goods property is practical and important because retailers often deal with
substitute goods. Suppose the situation that a retailer decides a price strategy of different brand in the
same products category. For example  supermarkets sell milk of different brands and car dealerships
sell various types of cars. These products are usually substitute goods. This kind of cross elasticity
effect is one of advanced topics in revenue management and is practically important [13  14  17].
Our key observation is the connection between the substitute-goods property in marketing science
and the supermodularity of the gross proﬁt function  which is formally described in the following
proposition.
Proposition 2. The gross proﬁt function (cid:96) : P1 ×···×PM → R is supermodular2 if demand models
deﬁned by (1) for all products satisﬁes the substitute-goods property.

The above proposition implies that  under the assumption of the substitute-goods property  problem
(3) can be solved precisely using submodular minimization algorithms  where time complexity is a
polynomial in M and K. This fact  however  does not necessarily imply that there exists a practical 
efﬁcient algorithm for problem (3). Indeed  general submodular minimization algorithms are slow in
practice even though their time complexities are polynomial. Further  actual models do not always
satisfy the substitute-goods property. We propose solutions to these problems in the next section.

4 Network ﬂow-based algorithm for revenue maximization

4.1 Binary quadratic programming formulation

This section shows that problem (3) can be reduced to the following binary quadratic programming
problem (notations are explained in the latter part of this section):

Minimize
subject to

x(cid:62)Ax + b(cid:62)x
x = [x1  . . .   xn](cid:62) ∈ {0  1}n 
xu ≤ xv

((u  v) ∈ C) 

(4)

Each variable pi takes Pik if and only if the binary vector xi = [xi1  . . .   xi K−1](cid:62) ∈ {0  1}(K−1)
satisﬁes:

](cid:62) (k = 1  . . .   K).

(5)

xi = ck := [1  . . .   1

(cid:124) (cid:123)(cid:122) (cid:125)

k−1

(cid:124) (cid:123)(cid:122) (cid:125)

  0  . . .   0

K−k

M ](cid:62) ∈ {0  1}(K−1)M and redeﬁne the indices of the entries of x as
Also we deﬁne x = [x(cid:62)
x = [x1  x2  . . .   x(K−1)M ]  i.e. xi k = xi(K−1)+k for notational simplicity.
Deﬁning (cid:96)ij : Pi × Pj → R by (cid:96)ij(pi  pj) = (pi − ci)fij(pj) for i (cid:54)= j and (cid:96)i : Pi → R by

1   . . .   x(cid:62)

(cid:96)i(pi) = (pi − ci)(fii(pi) +(cid:80)D

t=1 git(rt))  we can express (cid:96) as

(cid:88)

1≤i j≤M i(cid:54)=j

M(cid:88)

i=1

(cid:96)(p) =

(cid:96)ij(pi  pj) +

(cid:96)i(pi).

(6)

2We say that a function f : D1 × ··· × Dn → R (Dj ⊆ R) is submodular if f (x) + f (y) ≤ f (x ∨ y) +
f (x ∧ y) for all x  y  where x ∨ y and x ∧ y denote the coordinate-wise maximum and minimum  respectively.
We say a function f is supermodular if −f is submodular.

3

Algorithm 1 s-t cut for price optimization with the substitute-goods property
Input: Problem instance (A  b  C) of (4)  where all entries of A are non-positive.
Output: An optimal solution x∗ to (4).
1: Construct a weighted directed graph G = (V  E  w) satisfying (9).
2: Add edges C with weight ∞ to G  i.e.  set E ← E ∪ C and w(u  v) ← ∞ for all (u  v) ∈ C.
3: Compute a minimum s-t cut U∗ of G  deﬁne x∗ by (10) and return x∗.

Using xi  we can construct matrices Aij ∈ R(K−1)×(K−1) for which it holds that

(cid:96)ij(pi  pj) = −x(cid:62)

i Aijxj + const.

Indeed  matrices Aij = [aij

uv]1≤u v≤K−1 ∈ R(K−1)×(K−1) deﬁned by

uv = −(cid:96)ij(Pi u+1  Pj v+1) + (cid:96)ij(Pi u  Pj v+1) + (cid:96)ij(Pi u+1  Pj v) − (cid:96)ij(Pi u  Pj v)
aij

satisfy (7). In a similar way  we can construct bi ∈ RK−1 such that (cid:96)i(pi) = −b(cid:62)
i xi + const.
Accordingly  the objective function (cid:96) of problem (3) satisﬁes (cid:96)(p) = −(x(cid:62)Ax + b(cid:62)x) + const 
where we deﬁne A = [Aij]1≤i j≤M ∈ R(K−1)M×(K−1)M and b = [bi]1≤i≤M ∈ R(K−1)M . The
conditions xi ∈ {c1  . . .   cK}
((u  v) ∈ C) 
where we deﬁne C := {((K − 1)(i − 1) + k + 1  (K − 1)(i − 1) + k) | 1 ≤ i ≤ M  1 ≤ k ≤
K − 2}. Consequently  problem (3) is reduced to problem (4). Although [9] also gives another
BQP formulation for the problem (3) and relaxes it to a semi-deﬁnite programming problem  our
construction of the BQP problem can be solved much more efﬁciently  as is explained in the next
section.

(i = 1  . . .   M ) can be expressed as xu ≤ xv

(7)

(8)

4.2 Minimum cut for problems with substitute goods property

As is easily seen from (8)  if the problem satisﬁes the substitute-goods property  matrix A has only
non-positive entries. It is well known that unconstrained binary quadratic programming problems
such as (4) with non-positive A ∈ Rn×n and C = ∅ can be efﬁciently solved3 by algorithms
for minimum cut [6]. Indeed  we can construct a positive weighted directed graph  G = (V =
{s  t  1  2  . . .   n}  E ⊆ V × V  w : E → R>0 ∪ {∞})4 for which

x(cid:62)Ax + b(cid:62)x = cG({s} ∪ {u | xu = 1}) + const

(9)
holds for all x ∈ {0  1}n  where cG is the cut function of graph G5. Hence  once we can compute a
minimum s-t cut U that is a vertex set U ⊆ V minimizing cG(U ) subject to s ∈ U and t /∈ U  we
can construct an optimal solution x = [x1  . . .   xn](cid:62) to the problem (4) by setting

(cid:26) 1

0

(u ∈ U )
(u /∈ U )

xu =

(10)
For constrained problems such as (4) with C (cid:54)= ∅  the constraint xu ≤ xv is equivalent to xu =
1 =⇒ xv = 1. This condition can be  in the minimum cut problem  expressed as u ∈ U =⇒ v ∈ U.
By adding a directed edge (u  v) with weight ∞  we can forbid the minimum cut to violate the
constraints. In fact  if both u ∈ U and v /∈ U hold  the value of the cut function is ∞  and hence such
a U cannot be a minimum cut. We summarize this in Algorithm 1.

(u = 1  . . .   n).

4.3 Submodular relaxation for problems without the substitute-goods property

For problems without the substitute-goods property  we ﬁrst decompose the matrix A into A+ and
A− so that A+ + A− = A  where A+ = [a+

uv] ∈ Rn×n are given by

(cid:26) auv

0

a+
uv =

(cid:26) 0

uv] and A− = [a−
a−
uv =

auv

(auv ≥ 0)
(auv < 0)

 

(auv ≥ 0)
(auv < 0)

(u  v ∈ N ).

(11)

3The computational cost of the minimum cut depends on the choice of algorithms. For example  if we use

Dinic’s method  the time complexity is O(n3 log n) = O((KM )3 log(KM )).

4 s  t are auxiliary vertices different from 1  . . .   n corresponding to source  sink in maximum ﬂow problems.
5 For details about the construction of G  see  e.g.  [4  12].

4

This leads to a decomposition of the objective function of Problem (4) into supermodular and
submodular terms:

x(cid:62)Ax + b(cid:62)x = x(cid:62)A+x + x(cid:62)A−x + b(cid:62)x 

(12)
where x(cid:62)A+x is supermodular and x(cid:62)A−x + b(cid:62)x is submodular. Our approach is to replace the
supermodular term x(cid:62)A+x by a linear function to construct a submodular function approximating
x(cid:62)Ax+b(cid:62)x  that can be minimized by Algorithm 1. Similar approaches can be found in the literature 
e.g. [8  22]  but ours has a signiﬁcant point of difference; our method constructs approximate functions
bounding objectives from below  which provides information about the degree of accuracy.
Consider an afﬁne function h(x) such that h(x) ≤ x(cid:62)A+x for all x ∈ {0  1}n. Such an h can be
constructed as follows. Since

γuv(xu + xv − 1) ≤ xuxv

(xu  xv ∈ {0  1})

(13)

holds for all γuv ∈ [0  1]  an arbitrary matrix Γ ∈ [0  1]n×n satisﬁes

x(cid:62)A+x ≥ x(cid:62)(A+ ◦ Γ)1 + 1(cid:62)(A+ ◦ Γ)x − 1(cid:62)(A+ ◦ Γ)1 =: hΓ(x) 

where A+ ◦ Γ denotes the Hadamard product  i.e.  (A+ ◦ Γ)uv = a+
the optimal value of the following problem 

(14)
uv · γuv. From inequality (14) 

Minimize
subject to

x(cid:62)A−x + b(cid:62)x + hΓ(x)
x = [x1  . . .   xn](cid:62) ∈ {0  1}n 
xu ≤ xv
((u  v) ∈ C) 

(15)

is a lower bound for that of problem (4). Since A− has non-positive entries and b(cid:62)x + hΓ(x) is
afﬁne  we can solve (15) using Algorithm 1 to obtain an approximate solution for (4) and a lower
bound for the optimal value of (4).

4.4 Proximal gradient method with sequential submodular relaxation
An essential problem in submodular relaxation is how to choose Γ ∈ [0  1]n×n and to optimize x
given Γ. Let ψ(Γ) denote the optimal value of (15)  i.e.  deﬁne ψ(Γ) by ψ(Γ) = minx∈R x(cid:62)A−x +
b(cid:62)x + hΓ(x)  where R is the feasible region of (15). Then  for simultaneous optimization of x and
Γ  we consider the following problem:

Maximize ψ(Γ)

subject to Γ ∈ [0  1]n×n 

which can be rewritten as follows:6

Minimize − ψ(Γ) + Ω(Γ)

subject to Γ ∈ Rn×n 

where we deﬁne Ω : Rn×n → R ∪ {∞} by

(cid:26) 0

∞

Ω(Γ) =

(Γ ∈ [0  1]n×n)
(Γ /∈ [0  1]n×n)

.

(16)

(17)

(18)

Then  −ψ(Γ) is convex and (17) can be solved using a proximal gradient method.
Let Γt ∈ Rn×n denote the solution on the t-th step. Let xt be the optimal solution of (15) with
Γ = Γt  i.e. 

(19)
The partial derivative of −hΓ(x) w.r.t. Γ at (Γt  xt)  denoted by St  is then a subgradient of −ψ(Γ)
at Γt  which can be computed as follows:

{x(cid:62)A−x + b(cid:62)x + hΓt(x)}.

xt ∈ arg min
x∈R

St = A+ ◦ (11(cid:62) − xt1(cid:62) − 1x(cid:62)
t )

(20)

6Problem (16) can be also solved using the ellipsoid method  which guarantees polynomial time-complexity
in the input size. However  it is known that the order of its polynomial is large and that the performance of
the algorithm can be poor in practice  especially for large size problems. To try to achieve more practical
performance  this paper proposes a proximal gradient algorithm.

5

Algorithm 2 An iterative relaxation algorithm for (4)
Input: Problem instance (A  b  C) of (4).
Output: An approximate solution ˆx to (4) satisfying (25)  a lower bound ψ of optimal value of (4).
1: Set Γ1 = 11(cid:62)/2  t = 1  min_value = ∞  ψ = −∞.
2: while Not converged do
3:

Compute xt satisfying (19) by using Algorithm 1  and compute
valuet = x(cid:62)
if valuet < max_value then

t Axt + b(cid:62)xt  ψt = x(cid:62)

t A−xt + b(cid:62)xt + hΓt(xt)  ψ = max{ψ  ψt}

4:
5:

Update value and ˆx by

min_value = valuet 

ˆx = xt.

(24)

end if
Compute Γt+1 by (22) and (23).

6:
7:
8: end while
9: Return ˆx  min_value and ψ.

By using St and a decreasing sequence {ηt} of positive real numbers  we can express the update
scheme for the proximal gradient method as follows:
{St · Γ +

(cid:107)Γ − Γt(cid:107)2 + Ω(Γ)} 

(21)

Γt+1 ∈ arg min
Γ∈Rn×n
We can compute Γt+1 satisfying (21) by

1
2ηt

where Proj[0 1](X) is deﬁned by

Γt+1 = Proj[0 1]n×n (Γt − ηtSt) 

(cid:40) 0

(Proj[0 1](X))uv =

1
(X)uv

((X)uv < 0)
((X)uv > 1)
(otherwise)

.

(22)

(23)

The proposed algorithm can be summarized as Algorithm 2.
The choice of {ηt} has a major impact on the rate of the convergence of the algorithm. From a
√
√
t)  it is guaranteed
convergence analysis of the proximal gradient method  when we set ηt = Θ(1/
that ψt converge to the optimal value ψ∗ of (16) and |ψt − ψ∗| = O(1/
t). Because ψ(Γ) is
non-smooth and not strongly concave  there is no better guarantee of convergence rate  to the best of
our knowledge. In practice  however  we can observe the convergence in ∼ 10 steps iteration.

Initialization of Γ

4.5
Let ˜xΓ denote an optimal solution to (15). We employ Γ1 = 1/211(cid:62) for the initialization of
Γ because (xu + xu − 1)/2 is the tightest lower bound of xuxv in the max-norm sense  i.e. 
h(xu  xv) = (xu + xv − 1)/2 is the unique minimizer of maxxu xv∈{0 1}{|xuxv − h(xu  xv)|} 
subject to the constraints that h(xu  xv) is afﬁne and bounded from above by xuxv. In this case  ˜xΓ
is an approximate solution satisfying the following performance guarantee.
Proposition 3. If Γ = 11(cid:62)/2  then ˜xΓ satisﬁes
Γ A˜xΓ + b(cid:62) ˜xΓ ≤ x(cid:62)
˜x(cid:62)

∗ Ax∗ + b(cid:62)x∗ +

(25)

1(cid:62)A+1 

1
2

where x∗ is an optimal solution to (4).

5 Experiments

5.1 Simulations

This section investigates behavior of Algorithm 2 on the basis of the simulation model used in [9] 
and we compare the proposed method with state-of-the-art methods: the SDP relaxation method [9]

6

Table 1: Ranges of parameters in regression models.
(i) is supermodular  (ii) is supermodular + submodular 
and (iii) is submodular.

Table 2: Results on real retail data. (a) is
computational time  (b) is estimated gross
proﬁt  (c) is upper bound.

βij (i (cid:54)= j)
[0  2]
[−25  25]
[−2  0]

(i)
(ii)
(iii)

βii
[−2M −M ]
[−2M  0]
[M − 3  M − 1]

αi
[M  3M ]
[M  3M ]
[1  3]

(a)
(b)
(c)

actual

1403700

-

-

proposed

36[s]

1883252
1897393

QPBO
964[s]
1245568
1894555

and the QPBO and QBPOI methods [11]. We use SDPA 7.3.8 to solve SDP problems7 and use the
implementation of QPBO and QPBOI written by Kolmogolov.8 QPBO methods computes partial
labeling  i.e.  there might remain unlabeled variables  and we set unlabeled variables to 0 in our
experiments. For computing a minimum s-t cut  we use Dinic’s algorithm [6]. All experiments were
conducted in a machine equipped with Intel(R) Xeon(R) CPU E5-2699 v3 @ 2.30GHz  768GB RAM.
We limited all processes to a single CPU core.

regression model qi = αi +(cid:80)M

Revenue simulation model [9] The sales quantity qi of the i-th product was generated from the
j=1 βijpj  where {αi} and {βij} were generated by uniform distribu-
tions. We considered three types of uniform distributions to investigate the effect of submodularity 
as shown in Table 1  which correspond to three different situations: (i) all pairs of products are
substitute goods  i.e.  the gross proﬁt function is supermodular  (ii) half pairs are substitute goods
and the others are complementary goods  i.e.  the gross proﬁt function contains submodular terms
and supermodular terms  and (iii) all pairs are complementary goods  i.e.  the gross proﬁt function is
submodular. Price candidates Pi and cost ci for each product are ﬁxed to Pi = {0.6  0.7  . . .   1.0}
and ci = 0  respectively.

Scalability and accuracy comparison We evaluated four methods in terms of computational
time (sec) and optimization accuracy (i.e. optimal values calculated by four methods). In addition
to calculating approximate optimal solutions and values  all four algorithms derive upper bounds of
exact optimal value  which provide information about how accurate the calculated solution.9 Fig. 1
shows the results with M = 30  60  . . .   300 for situations (i) (ii) and (iii). The plotted values are
arithmetic means of 5 random problem instances. We can observe that proposed  QPBO and QPBOI
methods derived exact solutions in the case (i)  which can be conﬁrmed from the computed upper
bounds coinciding with the values of objective function. For situations (ii) and (iii)  on the other
hand  the upper bound and the objective value did not coincide and the solutions by QPBO were
worse than the others. The solutions by QPBOI and SDPrelax are as good as the proposed methods 
but their computational costs are signiﬁcantly higher especially for the situations (ii) and (iii). For
all situations  the proposed method successfully derived solutions as good as the best of the four
methods did  and its computational cost was the lowest.

5.2 Real-world retail data

Data and settings We applied the proposed method to actual retail data from a middle-size su-
permarket located in Tokyo [23].10 We selected 50 regularly-sold beer products. The data range is
approximately three years from 2012/01 to 2014/12  and we used the ﬁrst 35 months (1065 samples)
for training regression models and simulated the best price strategy for the next 20 days. Therefore 
the problem here was to determine 1000 prices (50 products × 20 days).
For forecasting the sales quantity q(d)
of the i-product on the d-th day  we use prices features
{p(d(cid:48))
}1≤j≤50 d−19≤d(cid:48)≤d of 50 products for the 20 days before the d-th day. In addition to these
1000 linear price features  we employed “day of the week" and “month" features (both binary)  as
well as temperature forecasting features (continuous)  as external features. The price candidates

j

i

7http://sdpa.sourceforge.net/
8http://pub.ist.ac.at/~vnk/software.html
9 For example  the coincidence of the upper bound and the calculated optimal value implies that the algorithm

computed the exact optimal solution.

10The Data has been provided by KSP-SP Co.  LTD  http://www.ksp-sp.com.

7

(i) supermodular

(ii) supermodular + submodular

(iii) submodular

Figure 1: Comparisons of proposed  QPBO  QPBOI  and SDPrelax methods on revenue simulation
data. The horizontal axis represents the number M of products. The vertical axes represent computa-
tional time (top) and optimal values of four methods (3) (bottom). For the bottom  circle markers
with dashed line represent the computed upper bounds of the optimal values  and optimal values and
upper bounds are normalized so that upper bounds with the proposed method are equal to 1.

{P (d)
ik }5
k=1 were generated by splitting equally the range [Pi1  Pi5]  where Pi1 and Pi5 are the highest
and lowest prices of the i-th product in the historical data. We assumed that the cost c(d)
i was
0.3Pi5 (30% of the list prices). Our objective was to obtain a price strategy for 50-products over
the 20 days  from the 1066-th to 1085-th  which involves 1000-dimensional variables  in order
to maximize the sum of the gross proﬁt for the 20 days. We estimated parameters in regression
models  using the ridge regression method. The estimated model contained 310293 pairs with the
substitute-goods property and 189207 pairs with complementary goods property.
The results are summarized in Table 2  where “actual” means the gross proﬁt computed on the basis
of the historical data regarding sales quantities and prices over the 20 days  from the 1066-th to
1085-th  and costs c(d)
i = 0.3Pi5. Thus  the target is to ﬁnd a strategy that expectedly achieves better
gross proﬁt than “actual”. We have omitted results for QPBOI and SDPrelax here because they did
not terminate after running over 8 hours. We observe that the proposed method successfully derived
a price strategy over 1000 products  which can be expected to increase gross proﬁt signiﬁcantly
in spite of its cheap computational cost  in contrast to QPBO  which failed with more expensive
computation. Although Table 2 shows results using a single CPU core for fair comparison  the
algorithm can be easily parallelized that can ﬁnish optimization in a few seconds. This makes it
possible to dynamically change prices in real time or enables price managers to ﬂexibly explore a
better price strategy (changing a price range  target products  domain constraints  etc.)

6 Conclusion

In this paper we dealt with price optimization based on large-scale demand forecasting models. We
have shown that the gross proﬁt function is supermodular under the assumption of the substitute-goods
property. On the basis of this supermodularity  we have proposed an efﬁcient algorithm that employs
network ﬂow algorithms and that returns exact solutions for problems with the substitute-goods
property. Even in case in which the property does not hold  the proposed algorithm can efﬁciently
ﬁnd approximate solutions. Our empirical results have shown that the proposed algorithm can handle
hundreds/thousands products with much cheaper computational cost than other existing methods.

References
[1] G. Bitran and R. Caldentey. An overview of pricing models for revenue management. Manufacturing &

Service Operations Management  5(3):203–229  2003.

8

050100150200250300M: number of products050100150200250computational time [s]proposedQPBOQPBOISDPrelax050100150200250300M: number of products050100150200250computational time [s]proposedQPBOQPBOISDPrelax050100150200250300M: number of products050100150200250computational time [s]proposedQPBOQPBOISDPrelax050100150200250300M: number of products0.60.70.80.91.01.11.2value of objective functionproposedQPBOQPBOISDPrelax050100150200250300M: number of products0.700.750.800.850.900.951.001.051.10value of objective functionproposedQPBOQPBOISDPrelax050100150200250300M: number of products0.00.20.40.60.81.01.2value of objective functionproposedQPBOQPBOISDPrelax[2] E. Boros and P. L. Hammer. Pseudo-boolean optimization. Discrete applied mathematics  123(1):155–225 

2002.

[3] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy
minimization in vision. Pattern Analysis and Machine Intelligence  IEEE Transactions on  26(9):1124–
1137  2004.

[4] Y. Boykov  O. Veksler  and R. Zabih. Fast approximate energy minimization via graph cuts. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  23(11):1222–1239  2001.

[5] F. Caro and J. Gallien. Clearance pricing optimization for a fast-fashion retailer. Operations Research 

60(6):1404–1422  2012.

[6] T. H. Cormen. Introduction to algorithms. MIT press  2009.
[7] K. J. Ferreira  B. H. A. Lee  and D. Simchi-Levi. Analytics for an online retailer: Demand forecasting and

price optimization. Manufacturing & Service Operations Management  pages 69–88  2015.

[8] L. Gorelick  Y. Boykov  O. Veksler  I. Ayed  and A. Delong. Submodularization for binary pairwise
energies. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
1154–1161  2014.

[9] S. Ito and R. Fujimaki. Optimization beyond prediction: Prescriptive price optimization. ArXiv e-prints 

http://arxiv.org/abs/1605.05422  2016.

[10] R. Klein. Revenue Management. Springer  2008.
[11] V. Kolmogorov and C. Rother. Minimizing nonsubmodular functions with graph cuts-a review. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  29(7):1274–1279  2007.

[12] V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? Pattern Analysis

and Machine Intelligence  IEEE Transactions on  26(2):147–159  2004.

[13] D. Koushik  J. A. Higbie  and C. Eister. Retail price optimization at intercontinental hotels group. Interfaces 

42(1):45–57  2012.

[14] S. Lee. Study of demand models and price optimization performance. PhD thesis  Georgia Institute of

Technology  2011.

[15] A. Marshall. Principles of Economics. Library of Economics and Liberty  1920.
[16] J. I. McGill and G. J. Van Ryzin. Revenue management: Research overview and prospects. Transportation

science  33(2):233–256  1999.

[17] M. Natter  T. Reutterer  and A. Mild. Dynamic pricing support systems for diy retailers - a case study from

austria. Marketing Intelligence Review  1:17–23  2009.

[18] R. L. Phillips. Pricing and Revenue Optimization. Stanford University Press  2005.
[19] C. Rother  V. Kolmogorov  V. Lempitsky  and M. Szummer. Optimizing binary mrfs via extended roof
duality. In Computer Vision and Pattern Recognition  2007. CVPR’07. IEEE Conference on  pages 1–8.
IEEE  2007.

[20] P. Rusmevichientong  B. Van Roy  and P. W. Glynn. A nonparametric approach to multiproduct pricing.

Operations Research  54(1):82–98  2006.

[21] C. J. Stone. Additive regression and other nonparametric models. The annals of Statistics  pages 689–705 

1985.

[22] M. Tang  I. B. Ayed  and Y. Boykov. Pseudo-bound optimization for binary energies. In Computer

Vision–ECCV 2014  pages 691–707. Springer  2014.

[23] J. Wang  R. Fujimaki  and Y. Motohashi. Trading interpretability for accuracy: Oblique treed sparse

additive models. In KDD  pages 1245–1254  2015.

9

,Andrew McDonald
Massimiliano Pontil
Dimitris Stamos
Shinji Ito
Ryohei Fujimaki
Antonio Ginart
Melody Guan
Gregory Valiant
James Zou