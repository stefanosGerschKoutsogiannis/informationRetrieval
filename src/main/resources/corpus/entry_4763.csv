2013,Robust Low Rank Kernel Embeddings of Multivariate Distributions,Kernel embedding of distributions has led to many recent advances in machine learning. However  latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting. Furthermore  no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified. In this paper  we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspecification. We also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation.,Robust Low Rank Kernel Embeddings of

Multivariate Distributions

Le Song  Bo Dai

College of Computing  Georgia Institute of Technology
lsong@cc.gatech.edu  bodai@gatech.edu

Abstract

Kernel embedding of distributions has led to many recent advances in machine
learning. However  latent and low rank structures prevalent in real world distri-
butions have rarely been taken into account in this setting. Furthermore  no prior
work in kernel embedding literature has addressed the issue of robust embedding
when the latent and low rank information are misspeciﬁed.
In this paper  we
propose a hierarchical low rank decomposition of kernels embeddings which can
exploit such low rank structures in data while being robust to model misspeci-
ﬁcation. We also illustrate with empirical evidence that the estimated low rank
embeddings lead to improved performance in density estimation.

Introduction

1
Many applications of machine learning  ranging from computer vision to computational biology 
require the analysis of large volumes of high-dimensional continuous-valued measurements. Com-
plex statistical features are commonplace  including multi-modality  skewness  and rich dependency
structures. Kernel embedding of distributions is an effective framework to address challenging prob-
lems in this regime [1  2]. Its key idea is to implicitly map distributions into potentially inﬁnite di-
mensional feature spaces using kernels  such that subsequent comparison and manipulation of these
distributions can be achieved via feature space operations (e.g.  inner product  distance  projection
and spectral analysis). This new framework has led to many recent advances in machine learning
such as kernel independence test [3] and kernel belief propagation [4].
However  algorithms designed with kernel embeddings have rarely taken into account latent and
low rank structures prevalent in high dimensional data arising from various applications such as
gene expression analysis. While these information have been extensively exploited in other learning
contexts such as graphical models and collaborative ﬁltering  their use in kernel embeddings re-
mains scarce and challenging. Intuitively  these intrinsically low dimensional structures of the data
should reduce the effect number of parameters in kernel embeddings  and allow us to obtain a better
estimator when facing with high dimensional problems.
As a demonstration of the above intuition  we illustrate the behavior of low rank kernel embeddings
(which we will explain later in more details) when applied to density estimation (Figure 1). 100 data
points are sampled i.i.d. from a mixture of 2 spherical Gaussians  where the latent variable is the
cluster indicator. The ﬁtted density based on an ordinary kernel density estimator has quite different
contours from the ground truth (Figure 1(b))  while those provided by low rank embeddings appear
to be much closer to the ground truth ((Figure 1(c)). Essentially  the low rank approximation step
endows kernel embeddings with an additional mechanism to smooth the estimator which can be
beneﬁcial when the number of data points is small and there are clusters in the data. In our later
more systematic experiments  we show that low rank embeddings can lead to density estimators
which can signiﬁcantly improve over alternative approaches in terms of held-out likelihood.
While there are a handful of exceptions [5  6] in the kernel embedding literature which have ex-
ploited latent and low rank information  these algorithms are not robust in the sense that  when such
information are misspeciﬁcation  no performance guarantee can be provided and these algorithms
can fail drastically. The hierarchical low rank kernel embeddings we proposed in this paper can be

1

−
3

−
3

−
2

−
1

0

1

2

3

−
3

−
3

−
2

−
1

0

1

2

3

−
3

−
3

−
2

−
1

0

1

2

3

−
2

−
1

0

1

2

3

−
2

−
1

0

1

2

3

−
2

−
1

0

1

2

3

(a) Ground truth

(b) Ordinary KDE

(c) Low Rank KDE

Figure 1: We draw 100 samples from a mixture of 2 spherical Gaussians with equal mixing weights.
(a) the contour plot for the ground truth density  (b) for ordinary kernel density estimator (KDE)  (c)
for low rank KDE. We used cross-validation to ﬁnd the best kernel bandwidth for both the KDE and
low rank KDE. The latter produces a density which is visibly closer to the ground truth  and in term
of the integrated square error  it is smaller than the KDE (0.0092 vs. 0.012).
considered as a kernel generalization of the discrete valued tree-structured latent variable models
studied in [7]. The objective of the current paper is to address previous limitations of kernel embed-
dings as applied to graphical models and make them more practically useful. Furthermore  we will
provide both theoretical and empirical support to the new approach.
Another key contribution of the paper is a novel view of kernel embedding of multivariate distri-
butions as inﬁnite dimensional higher order tensors  and the low rank structure of these tensors in
the presence of latent variables. This novel view allows us to introduce modern multi-linear alge-
bra and tensor decomposition tools to address challenging problems in the interface between kernel
methods and latent variable models. We believe our work will play a synergistic role in bridging to-
gether largely separate areas in machine learning research  including kernel methods  latent variable
models  and tensor data analysis.
In the remainder of the paper  we will ﬁrst present the tensor view of kernel embeddings of mul-
tivariate distributions and its low rank structure in the presence of latent variables. Then we will
present our algorithm for hierarchical low rank decomposition of kernel embeddings by making use
of a sequence of nested kernel singular value decompositions. Last  we will provide both theoretical
and empirical support to our proposed approach.
2 Kernel Embeddings of Distributions
We will focus on continuous domains  and denote X a random variable with domain ⌦ and density
p(X). The instantiations of X are denoted by lower case character  x. A reproducing kernel Hilbert
space (RKHS) F on ⌦ with a kernel k(x  x0) is a Hilbert space of functions f :⌦ 7! R with inner
product h· ·iF
. Its element k(x ·) satisﬁes the reproducing property: hf (·)  k(x ·)iF = f (x)  and
consequently  hk(x ·)  k(x0 ·)iF = k(x  x0)  meaning that we can view the evaluation of a function
f at any point x 2 ⌦ as an inner product. Alternatively  k(x ·) can be viewed as an implicit feature
map (x) where k(x  x0) = h(x)  (x0)iF
. For simplicity of notation  we assumes that the domain
of all variables are the same and the same kernel function is applied to all variables.
A kernel embedding represents a density by its expected features  i.e.  µX := EX [(X)] =
R⌦ (x)p(x)dx  or a point in a potentially inﬁnite-dimensional and implicit feature space of a k-
ernel [8  1  2]. The embedding µX has the property that the expectation of any RKHS func-
tion f 2F can be evaluated as an inner product in F  hµX  fiF := EX[f (X)]. Kernel
embeddings can be readily generalized to joint density of d variables  X1  . . .   Xd  using d-
th order tensor product feature space F d  In this feature space  the feature map is deﬁned as
i=1(xi) := (x1) ⌦ (x2) ⌦ . . . ⌦ (xd)  and the inner product in this space satisﬁes
⌦d
⌦⌦d
i=1 k(xi  x0i). Then we can embed a
i=1(xi) ⌦d
joint density p(X1  . . .   Xd) into a tensor product feature space F d by

i=1 h(xi)  (x0i)iF

i=1(x0i)↵F d = Qd
CX1:d := EX1:d⇥⌦d

i=1(Xi)⇤ =Z⌦d⌦d

= Qd
i=1(xi) p(x1  . . .   xd)

dYi=1

dxi 

(1)

where we used X1:d to denote the set of variables {X1  . . .   Xd}.

2

The kernel embeddings can also be generalized to conditional densities p(X|z) [9]

µX|z := EX|z[(X)] =Z⌦

(x) p(x|z) dx

(2)
Given this embedding  the conditional expectation of a function f 2F can be computed as
EX|z[f (X)] = ⌦f  µX|z↵F
. Unlike the ordinary embeddings  an embedding of conditional dis-
tribution is not a single element in the RKHS  but will instead sweep out a family of points in the
RKHS  each indexed by a ﬁxed value z of the conditioning variable Z. It is only by ﬁxing Z to a
particular value z  that we will be able to obtain a single RKHS element  µX|z 2F . In other words 
conditional embedding is an operator  denoted as CX|Z  which can take as input an z and output an
embedding  i.e.  µX|z = CX|Z(z). Likewise  kernel embedding of conditional distributions can
also be generalized to joint distribution of d variables.
We will represent an observation from a discrete variable Z taking r possible value using the stan-
dard basis in Rr (or one-of-r representation). That is when z takes the i-th value  the i-th dimension
of vector z is 1 and other dimensions 0. For instance  when r = 3  Z can take three possible val-
ue (1  0  0)>  (0  1  0)> and (0  0  1)>. In this case  we let (Z) = Z and use the linear kernel
k(Z  Z0) = Z>Z. Then  the conditional embedding operator reduces to a separate embedding µX|z
for each conditional density p(X|z). Conceptually  we can concatenate these µX|z for different val-
ue of z in columns CX|Z := (µX|z=(1 0 0)>  µX|z=(0 1 0)>  µX|z=(0 0 1)>). The operation CX|Z(z)
essentially picks up the corresponding embedding (or column).
3 Kernel Embeddings as Inﬁnite Dimensional Higher Order Tensors
The above kernel embedding CX1:d can also be viewed as a multi-linear operator (tensor) of order
d mapping from F⇥ . . . ⇥F to R. (For generic introduction to tensor and tensor notation  please
see [10]). The operator is linear in each argument (mode) when ﬁxing other arguments. Furthermore 
the application of the operator to a set of elements {fi 2F} d
i=1 can be deﬁned using the inner
product from the tensor product feature space  i.e. 

h(Xi)  fiiF#  

DCX1:d eCX1:dE•

=

CX1:d •1 f1 •2 . . . •d fd :=⌦CX1:d  ⌦d
of CX1:d as kCX1:dk2

i=1fd↵F d = EX1:d" dYi=1
where •i means applying fi to the i-th argument of CX1:d. Furthermore  we can deﬁne the gener-
• =P1i1=1 ···P1id=1(CX1:d •1 ei1 •2 . . . •d eid)2
alized Frobenius norm k·k•
using an orthonormal basis {ei}1i=1 ⇢F . We can also deﬁne the inner product for the space of such
operator that kCX1:dk• < 1 as
1Xi1=1

1Xid=1
(CX1:d •1 ei1 •2 . . . •d eid)(eCX1:d •1 ei1 •2 . . . •d eid).
i=1(Xi)⇤  the above inner product reduces to EX1:d[eCX1:d •1
When CX1:d has the form of EX1:d⇥⌦d
(X1) •2 . . . •d (Xd)].
In this paper  the ordering of the tensor modes is not essential so we simply label them using the
corresponding random variables. We can reshape a higher order tensor into a lower order tensor by
partitioning its modes into several disjoint groups. For instance  let I1 = {X1  . . .   Xs} be the set
of modes corresponding to the ﬁrst s variables and I2 = {Xs+1  . . .   Xd}. Similarly to the Matlab
function  we can obtain a 2nd order tensor by
(5)
In the reverse direction  we can also reshape a lower order tensor into a higher order one by further
partitioning certain mode of the tensor. For instance  we can partition I1 into I 01 = {X1  . . .   Xt}
and I 001 = {Xt+1  . . .   Xs}  and turn CI1;I2 into a 3rd order tensor by
(6)
Note that given a orthonormal basis {ei}1i=1 2F   we can readily obtain an orthonormal basis
for  e.g.  F t  as {ei1 ⌦ . . . ⌦ eit}1i1 ... it=1  and hence deﬁne the generalized Frobenius norm for
CI1;I2 and CI 01;I 001 ;I2. This also implies that the generalized Frobenius norms are the same for all
these reshaped tensors  i.e.  kCX1:dk• = kCI1;I2k•

CI 01;I 001 ;I2 = reshape (CI1;I2  I 01  I 001   I2) : F t ⇥F st ⇥F ds 7! R.

CI1;I2 = reshape (CX1:d  I1  I2) : F s ⇥F ds 7! R.

(3)

(4)

···

=CI 01;I 001 ;I2•

.

3

Z

X1

X3

Z1

Z2

X1
X2
(a) X1 ? X2|Z

X2
X4
(b) X1:2 ? X3:4|Z1:2

Z1

X1

Z2

X2

Zd

Xd

(c) Caterpillar tree (hidden Markov model)

Figure 2: Three latent variable model with different tree topologies

The 2nd order tensor CI1;I2 can also be viewed as the cross-covariance operator between two sets of
variables in I1 and I2. In this case  we can essentially use notation and operations for matrices. For
instance  we can perform singular value decomposition of CI1;I2 =P1i=1 si(ui⌦vi) where si 2 R
are ordered in nonincreasing manner  {ui}1i=1 ⇢F s and {vi}1i=1 ⇢F ds are singular vectors. The
In this case  we will also deﬁne
rank of CI1;I2 is the smallest r such that si = 0 for i  r.
Ur = (u1  u2  . . .   ur)  Vr = (v1  v2  . . .   vr) and Sr = diag (s1  s2  . . .   sr)  and denote the low
rank approximation as CI1;I2 = UrSrV>r . Finally  a 1st order tensor reshape (CX1:d {X1:d}  ;) 
is simply a vector where we we will use vector notation.

4 Low Rank Kernel Embeddings Induced by Latent Variables
In the presence of latent variables  the kernel embedding CX1:d will be low rank. For example 
the two observed variables X1 and X2 in the example in Figure 1 is conditional independent giv-
en the latent cluster indicator variable Z. That is the joint density factorizes as p(X1  X2) =
Pz p(z)p(X1|z)p(X2|z) (see Figure 2(a) for the graphical model). Throughout the paper  we as-
sume that z is discrete and takes r possible values. Then the embedding CX1X2 of p(X1  X2) has a
rank at most r. Let z be represented as the standard basis in Rr. Then
CX1X2 = EZ⇥EX1|Z[(X1)]Z ⌦EX2|Z[(X2)]Z⇤ = CX1|Z EZ [Z ⌦ Z]CX2|Z>
(7)
where EZ [Z ⌦ Z] is an r ⇥ r matrix  and hence restricting the rank of CX1X2 to be at most r.
In our second example  four observed variables are connected via two latent variables Z1 and Z2
each taking r possible values. The conditional independence structure implies that the density of
p(X1  X2  X3  X4) factorizes as Pz1 z2
p(X1|z1)p(X2|z1)p(z1  z2)p(X3|z2)p(X4|z2) (see Fig-
ure 2(b) for the graphical model). Reshaping its kernel embedding CX1:4  we obtain CX1:2;X3:4 =
reshape (CX1:4 {X1:2}  {X3:4}) which factorizes as

EX1:2|Z1[(X1) ⌦ (X2)] EZ1Z2[Z1 ⌦ Z2] EX3:4|Z2[(X3) ⌦ (X4)]>

(8)
where EZ1Z2[Z1 ⌦ Z2] is an r ⇥ r matrix. Hence the intrinsic “rank” of the reshaped embedding is
only r  although the original kernel embedding CX1:4 is a 4th order tensor with inﬁnite dimensions.
In general  for a latent variable model p(X1  . . .   Xd) where the conditional independence structure
is a tree T   various reshapings of its kernel embedding CX1:d according to edges in the tree will be
low rank. More speciﬁcally  each edge in the latent tree corresponds to a pair of latent variables
(Zs  Zt) (or an observed and a hidden variable (Xs  Zt)) which induces a partition of the observed
variables into two groups  I1 and I2. One can imagine splitting the latent tree into two subtrees by
cutting the edge. One group of variables reside in the ﬁrst subtree  and the other group in the second
subtree. If we reshape the tensor according to this partitioning  then

Theorem 1 Assume that all observed variables are leaves in the latent tree structure  and all latent
variables take r possible values  then rank(CI1;I2)  r.
Proof Due to the conditional independence structure induced by the latent tree  p(X1  . . .   Xd) =
PzsPzt
p(I1|zs)p(zs  zt)p(I2|zt). Then its embedding can be written as
CI1;I2 = CI1|Zs EZsZt[Zs ⌦ Zt]CI2|Zt>  
(9)
where CI1|Zs and CI2|Zt are the conditional embedding operators for p(I1|zs) and p(I2|zt) re-
spectively. Since EZsZt[Zs ⌦ Zt] is a r ⇥ r matrix  rank(CI1;I2)  r.
Theorem 1 implies that  given a latent tree model  we obtain a collection of low rank reshapings
{CI1;I2} of the kernel embedding CX1:d  each corresponding to an edge (Zs  Zt) of the tree. We

4

will denote by H(T   r) the class of kernel embeddings CX1:d whose various reshapings according to
the latent tree T have rank at most r.1 We will also use CX1:d 2H (T   r) to indicator such a relation.
In practice  the latent tree model assumption may be misspeciﬁed for a joint density p(X1  . . .   Xd) 
and consequently the various reshapings of its kernel embedding CX1:d are only approximately low
rank. In this case  we will instead impose a (potentially misspeciﬁed) latent structure T and a ﬁxed
rank r on the data and obtain an approximate low rank decomposition of the kernel embedding. The
goal is to obtain a low rank embedding eCX1:d 2H (T   r)  while at the same time insure keCX1:d 
CX1:dk• is small. In the following  we will present such a decomposition algorithm.
5 Low Rank Decomposition of Kernel Embeddings
For simplicity of exposition  we will focus on the case where the latent tree structure T has a cater-
pillar shape (Figure 2(c)). This decomposition can be viewed as a kernel generalization of the hier-
archical tensor decomposition in [11  12  7]. The decomposition proceeds by reshaping the kernel
embedding CX1:d according to the ﬁrst edge (Z1  Z2)  resulting in A1 := CX1;X2:d. Then we perform
a rank r approximation for it  resulting in A1 ⇡U rSrV>r . This leads to the ﬁrst intermediate tensor
G1 = Ur  and we reshape SrV>r and recursively decompose it. We note that Algorithm 1 contains
only pseudo codes  and not implementable in practice since the kernel embedding to decompose can
have inﬁnite dimensions. We will design a practical kernel algorithm in the next section.

Algorithm 1 Low Rank Decomposition of Kernel Embeddings
In: A kernel embedding CX1:d  the caterpillar tree T and desired rank r
Out: A low rank embedding eCX1:d 2H (T   r) as intermediate tensors {G1  . . .  Gd}
1: A1 = reshape(CX1:d  {X1}   {X2:d}) according to tree T .
2: A1 ⇡U rSrV>r   approximate A1 using its r leading singular vectors.
3: G1 = Ur  and B1 = SrV>r . G1 can be viewed as a model with two variables  X1 and Z1; and
B1 as a new caterpillar tree model T1 with variable X1 removed from T .
4: for j = 2  . . .   d  1 do
5: Aj = reshape(Bj1  {Zj1  Xj}   {Xj+1:d}) according to tree Tj1.
6: Aj ⇡U rSrV>r   approximate Aj using its r leading singular vectors.
7:

Gj = reshape(Ur  {Zj1}   {Xj}   {Zj})  and Bj = SrV>r . Gj can be viewed as a model
with three variables  Xj  Zj and Zj1; and Bj as a new caterpillar tree model Tj with variable
Zj1 and Xj removed from Tj1.

8: end for
9: Gd = Bd1
Once we ﬁnish the decomposition  we obtain the low rank representation of the kernel embedding
as a set of intermediate tensors {G1  . . .  Gd}. In particular  we can think of G1 as a second order
tensor with dimension 1 ⇥ r  Gd as a second order tensor with dimension r ⇥ 1  and Gj for
2 6 j 6 d  1 as a third order tensor with dimension r ⇥ 1 ⇥ r. Then we can apply the low
rank kernel embedding eCX1:d to a set of elements {fi 2F} d
i=1 as follows eCX1:d •1 f1 •2 . . . •d fd =
(G1 •1 f1)> (G2 •2 f2) . . . (Gd1 •2 fd1)(Gd •2 fd). Based on the above decomposition  one can
obtain a low rank density estimate byep(X1  . . .   Xd) = eCX1:d •1 (X1) •2 . . . •d (Xd). We can
also compute the difference betweeneCX1:d and the operator CX1:d by using the generalized Frobenius
norm keCX1:d C X1:dk•.
In practice  we are only provided with a ﬁnite number of samples(xi
i=1 draw i.i.d. from
p(X1  . . .   Xd)  and we want to obtain an empirical low rank decomposition of the kernel embed-
ding. In this case  we will perform a low rank decomposition of the empirical kernel embedding
j). Although the empirical kernel embedding still has inﬁnite dimen-
i=1⌦d
nPn
¯CX1:d = 1
sions  we will show that we can carry out the decomposition using just the kernel matrices. Let us
denote the kernel matrix for each dimension of the data by Kj where j 2{ 1  . . .   d}. The (i  i0)-th
entry in Kj can be computed as Kii0
j ). Alternatively  one can think of implicitly forming
1One can readily generalize this notation to decompositions where different reshapings have different ranks.

6 Kernel Algorithm

j = k(xi

j  xi0

j=1(xi

1  . . .   xi

d) n

5

j0).

j )  . . .   (xn

j0=j+1(xn

Furthermore  we denote the tensor feature matrix formed from dimension j + 1 to d of the data as

the feature matrix j =(x1
 j =⌦d

j0=j+1(x1

j0)  . . .  ⌦d

j )  and the corresponding kernel matrix is Kj = >j j.
j0). The corresponding kernel matrix Lj = >j j with
j =Qd

j0=j+1 k(xi

j0  xi0

n2 1 >1 1>1 1 =  1   1

the (i  i0)-th entry in Lj deﬁned as Lii0
Step 1-3 in Algorithm 1. The key building block of the algorithm is a kernel singular value de-
composition (Algorithm 2)  which we will explain in more details using the example in step 2 of
n 1 >1 .
Algorithm 1. Using the implicitly deﬁned feature matrix  A1 can be expressed as A1 = 1
For the low rank approximation  A1 ⇡U rSrV>r   using singular value decomposition  the lead-
ing r singular vector Ur = (u1  . . .   ur) will lie in the span of 1  i.e.  Ur = 1(1  . . .   r)
where  2 Rn. Then we can transform the singular value decomposition problem for an inﬁnite
dimensional matrix to a generalized eigenvalue problem involving kernel matrices  A1A1>u =
n2 K1L1K1 = K 1. Let the Cholesky decom-
u   1
position of K1 be R>R  then the generalized eigenvalue decomposition problem can be solved by
redeﬁning e = R  and solving an ordinary eigenvalue problem
The resulting singular vectors satisfy u>l ul0 = >l >1 1l0 = >l Kl0 = e>l el0 = ll0. Then we
can obtain B1 := SrV>r = U>r A1 by projecting the column of A1 using the singular vectors Ur 
(11)
where  2 Rr can be treated as the reduced r-dimensional feature representation for each feature
mapped data point (xi
1). Then we have the ﬁrst intermediate tensor G1 = Ur = 1(1  . . .   r) =:
1(✓1  . . .   ✓n)>  where ✓ 2 Rr. Then the kernel singular value decomposition can be carried out
recursively on the reshaped tensor B1.
ne2 >2   where
Step 5-7 in Algorithm 1. When j = 2  we ﬁrst reshape B1 = SrV>r to obtain A2 = 1
e2 = (1⌦(x1
before  and obtain Ur = e2(1  . . .   r) =: e2(✓1  . . .   ✓n)>. Then we have the second operator
G2 =Pn

n2 RL1R>e = e  and obtain  = R†e.

2) ⌦ ✓i. Last  we deﬁne B2 := SrV>r = U>r A2 as
(1  . . .   r)>(  K2) >2 =:

2 )). Then we can carry out similar singular value decomposition as

(1  . . .   r)>K1 >1 =: (1  . . .   n) >1

i=1 i ⌦ (xi
1
n

(1  . . .   r)>>1 1 >1 =

2)  . . .   n⌦(xn

(1  . . .   n) >2  

B1 =

1
n

(10)

B2 =

(1  . . .   r)>e>2e2 >2 =

1
n

and carry out the recursive decomposition further.

(12)

1
n

1
n

1

The result of the algorithm is an empirical low rank kernel embedding  bCX1:d  represented as a col-
lection of intermediate tensors {G1  . . .  Gd}. The overall algorithm is summarized in Algorithm 3.
More details about the derivation can be found in Appendix A.
The application of the set of intermediate tensor {G1  . . .  Gd} to a set of elements {fi 2F} can be
expressed as kernel operations. For instance  we can obtain a density estimate bybp(x1  . . .   xd) =
bCX1:d •1 (x1) •2 . . . •d (xd) = Pz1 ... zd
g1(x1  z1)g2(z1  x2  z2) . . . gd(zd1  xd) where (see

Appendix A for more details)

(z>1 ✓i)k(xi

1  x1)

(13)

g1(x1  z1) = G1 •1 (x1) •2 z1 =Xn
gj(zj1  xj  zj) = Gj •1 zj1 •2 (xj) •3 zj =Xn
gd(zd1  xd) = Gd •1 zd1 • xd =Xn

i=1

i=1

d  xd)

(z>d1i)k(xi

(15)
In the above formulas  each term is a weighted combination of kernel functions  and the weighting
is determined by the kernel singular value decomposition and the values of the latent variable {zj}.
7 Performance Guarantees
As we mentioned in the introduction  the imposed latent structure used in the low rank decompo-
sition of kernel embeddings may be misspeciﬁed  and the decomposition of empirical embeddings
may suffer from sampling error. In this section  we provide ﬁnite guarantee for Algorithm 3 even
when the latent structures are misspeciﬁed. More speciﬁcally  we will bound  in terms of the gen-

(z>j1i)k(xi

j  xj)(z>j ✓i)

(14)

i=1

6

Algorithm 2 KernelSVD(K  L  r)
Out: A collection of vectors (✓1  . . .   ✓n)
1: Perform Cholesky decomposition K = R>R
2: Solve eigen decomposition 1

Compute matrix Kj with Kii0

j = k(xi

j  xi0

1  . . .   xi

i=1  desired rank r  a query point (x1  . . .   xd)

we observed that the difference can be decomposed into two terms

j ); furthermore  if j < d  then Lj = Lj+1  Kj+1

n2 RLR>e = e  and keep the leading r eigen vectors

(e1  . . .  er)
3: Compute 1 = R†e1  . . .   r = R†er  and reorgnaize (✓1  . . .   ✓n)> = (1  . . .   r)
Algorithm 3 Kernel Low Rank Decomposition of Empirical Embedding ¯CX1:d
In: A sample(xi
d) n
Out: A low rank embedding bCX1:d 2H (T   r) as intermediate operators {G1  . . .  Gd}
1: Ld = 11>
2: for j = d  d  1  . . .   1 do
3:
4: end for
5: (✓1  . . .   ✓n) = KernelSVD(K1  L1  r)
6: G1 = 1(✓1  . . .   ✓n)>  and compute (1  . . .   n) = (✓1  . . .   ✓n)K1
7: for j = 2  . . .   d  1 do
8:
i=1 i ⌦ (xi
9:
10: end for
11: Gd = (1  . . .   n)>d
eralized Frobenius norm kCX1:d  bCX1:dk•  the difference between the true kernel embeddings and
the low rank kernel embeddings estimated from a set of n i.i.d. samples(xi
+keCX1:d bCX1:dk•
}
|

= ( 1  . . .   n)>(1  . . .   n)  and compute (✓1  . . .   ✓n) = KernelSVD(Ki    Li  r)
Gj =Pn

kCX1:d bCX1:dk• 6 kCX1:d eCX1:dk•
}

where the ﬁrst term is due to the fact that the latent structures may be misspeciﬁed  while the second
term is due to estimation from ﬁnite number of data points. We will bound these two sources of
error separately (the proof is deferred to Appendix B)
Theorem 2 Suppose each reshaping CI1;I2 of CX1:d according to an edge in the latent tree struc-

j) ⌦ ✓i  and compute (1  . . .   n) = (✓1  . . .   ✓n)Ki

ture has a rank r approximation UrSrV>r with errorCI1;I2 U rSrV>r • 6 ✏. Then the low rank
decomposition eCX1:d from Algorithm 1 satisﬁes kCX1:d eCX1:dk• 6 pd  1 ✏.

Although previous work [5  6] have also used hierarchical decomposition for kernel embeddings 
their decompositions make the strong assumption that the latent tree models are correctly speciﬁed.
When the models are misspeciﬁed  these algorithms have no guarantees whatsoever  and may fail
drastically as we show in later experiments. In contrast  the decomposition we proposed here are
robust in the sense that even when the latent tree structure is misspeciﬁed  we can still provide
the approximation guarantee for the algorithm. Furthermore  when the latent tree structures are
correctly speciﬁed and the rank r is also correct  then CI1;I2 has rank r and hence ✏ = 0 and our
decomposition algorithm does not incur any modeling error.
Next  we provide bound for the the estimation error. The estimation error arises from decomposing
the empirical estimate ¯CX1:d of the kernel embedding  and the error can accumulate as we combine
intermediate tensors {G1  . . .  Gd} to form the ﬁnal low rank kernel embedding. More speciﬁcally 
we have the following bound (the proof is deferred to Appendix C)
Theorem 3 Suppose the r-th singular value of each reshaping CI1;I2 of CX1:d according to an
edge in the latent tree structure is lower bounded by   then with probability at least 1   keCX1:d 
bCX1:dk•  (1+)d2
d2pn   with some constant c associated with the

CX1:d  ¯CX1:d• 6 (1+)d2c

kernel and the probability .

d2

1  . . .   xi

i=1. First

|

{z

d) n

(16)

E1: model error

E2: estimation error

{z

7

From the above theorem  we can see that the smaller the r-th singular value  the more difﬁcult it is
to estimate the low rank kernel embedding. Although in the bound the error grows exponential in
1/d2  in our experiments  we did not observe such exponential degradation of performance even
in relatively high dimensional datasets.
8 Experiments
Besides the synthetic dataset we showed in Figure 1 where low rank kernel embedding can lead to
signiﬁcant improvement in term of estimating the density  we also experimented with real world
datasets from UCI data repository. We take 11 datasets with varying dimensions and number of data
points  and the attributes of the datasets are continuous-valued. We whiten the data and compare
low rank kernel embeddings (Low Rank) obtained from Algorithm 3 to 3 other alternatives for
continuous density estimation  namely  mixture of Gaussian with full covariance matrix  ordinary
kernel density estimator (KDE) and the kernel spectral algorithm for latent trees (Spectral) [6]. We
use Gaussian kernel k(x  x0) = 1p2⇡s
exp(kx  x0k2/(2s2)) for KDE  Spectral and our method
(Low rank). We split each dataset into 10 subsets  and use nested cross-validation based on held-
out likelihood to choose hyperparameters: the kernel parameter s for KDE  Spectral and Low rank
({23  22  21  1  2  4  8} times the median pairwise distance)  the rank parameter r for Spectral
and Low rank (range from 2 to 30)  and the number of components in the Gaussian mixture (range
from 2 to # Sample
). For both Spectral and Low rank  we use a caterpillar tree in Figure 2(c) as the
structure for the latent variable model.
From Table 1  we can see that low rank kernel embeddings provide the best or comparable held-out
negative log-likelihood across the datasets we experimented with. In some datasets  low rank kernel
embeddings can lead to drastic improvement over the alternatives. For instance  in dataset “sonar”
and “yeast”  the improvement is dramatic. The Spectral approach performs even worse sometimes.
This makes sense  since the caterpillar tree supplied to the algorithm may be far away from the
reality and Spectral is not robust to model misspeciﬁcation. Meanwhile  the Spectral algorithm also
caused numerical problem in practical.
In contrast  our method Low Rank uses the same latent
structure  but achieved much more robust results.

30

Table 1: Negative log-likelihood on held-out data (the lower the better).

Method

KDE

Low rank
Spectral
15.88±0.11
18.32±0.64
33.50 ±2.17
7.57±0.14
8.36±0.17
25.01±0.66
30.57 ± 0.15 28.40 ± 11.64 22.89 ± 0.26
21.50 ± 2.39 16.95 ± 0.13
18.23 ±0.18
35.84 ± 1.00
54.91±1.35
43.53 ± 1.25
10.38 ± 0.19 31.42 ± 2.40 10.07 ± 0.11
33.20 ± 0.70 28.19 ± 0.37
30.65 ± 0.66
89.26 ± 2.75 57.96 ± 2.67
96.17 ± 0.27
48.66 ± 2.56 40.78 ± 0.86
49.48 ± 0.64
19.25 ± 0.58 18.67 ± 0.17
19.56 ± 0.56
137.15 ± 1.80 76.58 ± 2.24 72.67 ±4.05

Data Set
# Sample Dim. Gaussian mixture
australian
690
bupa
345
german
1000
heart
270
ionosphere 351
pima
768
parkinsons 195
sonar
208
198
wpbc
178
wine
yeast
208

17.97±0.26
8.17±0.30
31.14 ± 0.41
17.72 ±0.23
47.60 ±1.77
11.78 ± 0.04
30.13± 0.24
107.06 ± 1.36
50.75 ± 1.11
19.59 ± 0.14
146.11 ± 5.36

14
6
24
13
34
8
22
60
33
13
79

9 Discussion and Conclusion
In this paper  we presented a robust kernel embedding algorithm which can make use of the low
rank structure of the data  and provided both theoretical and empirical support for it. However  there
are still a number of issues which deserve further research. First  the algorithm requires a sequence
of kernel singular decompositions which can be computationally intensive for high dimensional and
large datasets. Developing efﬁcient algorithms yet with theoretical guarantees will be interesting fu-
ture research. Second  the statistical analysis could be sharpened. For the moment  the analysis does
not seem to suggest that the obtained estimator by our algorithm is better than ordinary KDE. Third 
it will be interesting empirical work to explore other applications for low rank kernel embeddings 
such as kernel two-sample tests  kernel independence tests and kernel belief propagation.

8

References
[1] A. J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert space embedding for dis-
tributions. In Proceedings of the International Conference on Algorithmic Learning Theory 
volume 4754  pages 13–31. Springer  2007.

[2] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Injective Hilbert
space embeddings of probability measures. In Proc. Annual Conf. Computational Learning
Theory  pages 111–122  2008.

[3] A. Gretton  K. Fukumizu  C.-H. Teo  L. Song  B. Sch¨olkopf  and A. J. Smola. A kernel
statistical test of independence. In Advances in Neural Information Processing Systems 20 
pages 585–592  Cambridge  MA  2008. MIT Press.

[4] L. Song  A. Gretton  D. Bickson  Y. Low  and C. Guestrin. Kernel belief propagation. In Proc.
Intl. Conference on Artiﬁcial Intelligence and Statistics  volume 10 of JMLR workshop and
conference proceedings  2011.

[5] L. Song  B. Boots  S. Siddiqi  G. Gordon  and A. J. Smola. Hilbert space embeddings of hidden

markov models. In International Conference on Machine Learning  2010.

[6] L. Song  A. Parikh  and E.P. Xing. Kernel embeddings of latent tree graphical models.

Advances in Neural Information Processing Systems  volume 25  2011.

In

[7] L. Song  M. Ishteva  H. Park  A. Parikh  and E. Xing. Hierarchical tensor decomposition of
latent tree graphical models. In International Conference on Machine Learning (ICML)  2013.
[8] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Kluwer  2004.

[9] L. Song  J. Huang  A. J. Smola  and K. Fukumizu. Hilbert space embeddings of conditional

distributions. In Proceedings of the International Conference on Machine Learning  2009.

[10] Tamara. G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review 

51(3):455–500  2009.

[11] L. Grasedyck. Hierarchical singular value decomposition of tensors. SIAM Journal on Matrix

Analysis and Applications  31(4):2029–2054  2010.

[12] I Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing  33(5):2295–

2317  2011.

[13] L. Rosasco  M. Belkin  and E.D. Vito. On learning with integral operators. Journal of Machine

Learning Research  11:905–934  2010.

9

,Le Song
Bo Dai
Piotr Indyk
Ilya Razenshteyn
Tal Wagner
Elliot Crowley
Gavin Gray
Amos Storkey
Takuya Hiraoka
Takahisa Imagawa
Tatsuya Mori
Takashi Onishi
Yoshimasa Tsuruoka