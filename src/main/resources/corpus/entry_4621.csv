2016,Refined Lower Bounds for Adversarial Bandits,We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses  are close to tight. Besides this we prove two impossibility results. First  the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second  the regret cannot scale with the effective range of the losses. In contrast  both results are possible in the full-information setting.,Reﬁned Lower Bounds for Adversarial Bandits

Sébastien Gerchinovitz

Institut de Mathématiques de Toulouse

Université Toulouse 3 Paul Sabatier

Toulouse  31062  France

Tor Lattimore

Department of Computing Science

University of Alberta
Edmonton  Canada

sebastien.gerchinovitz@math.univ-toulouse.fr

tor.lattimore@gmail.com

Abstract

We provide new lower bounds on the regret that must be suffered by adversarial
bandit algorithms. The new results show that recent upper bounds that either (a)
hold with high-probability or (b) depend on the total loss of the best arm or (c)
depend on the quadratic variation of the losses  are close to tight. Besides this we
prove two impossibility results. First  the existence of a single arm that is optimal
in every round cannot improve the regret in the worst case. Second  the regret
cannot scale with the effective range of the losses. In contrast  both results are
possible in the full-information setting.

1

Introduction

T(cid:88)

t=1

T(cid:88)

We consider the standard K-armed adversarial bandit problem  which is a game played over T
rounds between a learner and an adversary. In every round t ∈ {1  . . .   T} the learner chooses a
probability distribution pt = (pi t)1(cid:54)i(cid:54)K over {1  . . .   K}. The adversary then chooses a loss vector
(cid:96)t = ((cid:96)i t)1(cid:54)i(cid:54)K ∈ [0  1]K  which may depend on pt. Finally the learner samples an action from pt
denoted by It ∈ {1  . . .   K} and observes her own loss (cid:96)It t. The learner would like to minimise her
regret  which is the difference between cumulative loss suffered and the loss suffered by the optimal
action in hindsight:

RT ((cid:96)1:T ) =

(cid:96)It t − min
1(cid:54)i(cid:54)K

(cid:96)i t  

t=1

Exp3  which satisﬁes E[RT ((cid:96)1:T )] = O((cid:112)KT log(K))) where the expectation is taken over the

where (cid:96)1:T ∈ [0  1]T K is the sequence of losses chosen by the adversary. A famous strategy is called

randomness in the algorithm and the choices of the adversary [Auer et al.  2002]. There is also a
√
lower bound showing that for every learner there is an adversary for which the expected regret is
E[RT ((cid:96)1:T )] = Ω(
√
KT ) [Auer et al.  1995]. If the losses are chosen ahead of time  then the adver-
sary is called oblivious  and in this case there exists a learner for which E[RT ((cid:96)1:T )] = O(
KT )
[Audibert and Bubeck  2009]. One might think that this is the end of the story  but it is not so. While
the worst-case expected regret is one quantity of interest  there are many situations where a reﬁned
regret guarantee is more informative. Recent research on adversarial bandits has primarily focussed
on these issues  especially the questions of obtaining regret guarantees that hold with high probability
as well as stronger guarantees when the losses are “nice” in some sense. While there are now a wide
range of strategies with upper bounds that depend on various quantities  the literature is missing lower
bounds for many cases  some of which we now provide.
We focus on three classes of lower bound  which are described in detail below. The ﬁrst addresses the
optimal regret achievable with high probability  where we show there is little room for improvement
over existing strategies. Our other results concern lower bounds that depend on some kind of regularity
in the losses (“nice” data). Speciﬁcally we prove lower bounds that replace T in the regret bound
with the loss of the best action (called ﬁrst-order bounds) and also with the quadratic variation of the
losses (called second-order bounds).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

(1)

(2)

High-probability bounds Existing strategies Exp3.P [Auer et al.  2002] and Exp3-IX [Neu  2015a]
are tuned with a conﬁdence parameter δ ∈ (0  1) and satisfy  for all (cid:96)1:T ∈ [0  1]KT  

P(cid:16)
(cid:32)

(cid:17) (cid:54) δ
RT ((cid:96)1:T ) (cid:62) c(cid:112)KT log(K/δ)
(cid:32)(cid:112)log(K) +

√
RT ((cid:96)1:T ) (cid:62) c

KT

for some universal constant c > 0. An alternative tuning of Exp-IX or Exp3.P [Bubeck and Cesa-
Bianchi  2012] leads to a single algorithm for which  for all (cid:96)1:T ∈ [0  1]KT  
log(1/δ)

(cid:33)(cid:33)

∀δ ∈ (0  1)

P

(cid:112)log(K)

(cid:54) δ .

The difference is that in (1) the algorithm depends on δ while in (2) it does not. The cost of not
knowing δ is that the log(1/δ) moves outside the square root. In Section 2 we prove two lower
bounds showing that there is little room for improvement in either (1) or (2).
First-order bounds An improvement over the worst-case regret bound of O(
T K) is the
so-called improvement for small losses. Speciﬁcally  there exist strategies (eg.  FPL-TRIX by Neu
[2015b] with earlier results by Stoltz [2005]  Allenberg et al. [2006]  Rakhlin and Sridharan [2013])
such that for all (cid:96)1:T ∈ [0  1]KT

√

E[RT ((cid:96)1:T )] (cid:54) O

L∗
T K log(K) + K log(KT )

  with L∗

T = min
1(cid:54)i(cid:54)K

(cid:96)i t  

(3)

(cid:18)(cid:113)

(cid:19)

T(cid:88)

t=1

√
where the expectation is with respect to the internal randomisation of the algorithm (the losses are
ﬁxed). This result improves on the O(
(cid:54) T is always guaranteed and
sometimes L∗
T is much smaller than T . In order to evaluate the optimality of this bound  we ﬁrst
rewrite it in terms of the small-loss balls Bα T deﬁned for all α ∈ [0  1] and T (cid:62) 1 by

KT ) bounds since L∗

T

Bα T (cid:44)

(cid:27)

(cid:96)1:T ∈ [0  1]KT :

(cid:26)
E[RT ((cid:96)1:T )] (cid:54) O(cid:16)(cid:112)αT K log(K) + K log(KT )

L∗
T
T

(cid:54) α

.

(4)

(cid:17)

.

Corollary 1. The ﬁrst-order regret bound (3) of Neu [2015b] is equivalent to:

∀α ∈ [0  1] 

sup

(cid:96)1:T ∈Bα T

√
The proof is straightforward. Our main contribution in Section 3 is a lower bound of the order of
αT K for all α ∈ Ω(log(T )/T ). This minimax lower bound shows that we cannot hope for a better
bound than (3) (up to log factors) if we only know the value of L∗
T .

Second-order bounds Another type of improved regret bound was derived by Hazan and Kale
[2011b] and involves a second-order quantity called the quadratic variation:

(cid:80)T

QT =

(cid:107)(cid:96)t − µT(cid:107)2

2

(cid:54) T K
4

 

(5)

where µT = 1
t=1 (cid:96)t is the mean of all loss vectors. (In other words  QT /T is the sum of the
T
empirical variances of all the K arms). Hazan and Kale [2011b] addressed the general online linear
optimisation setting. In the particular case of adversarial K-armed bandits with an oblivious adversary
(as is the case here)  they showed that there exists an efﬁcient algorithm such that for some absolute
constant c > 0 and for all T (cid:62) 2

K 2(cid:112)QT log T + K 1.5 log2 T + K 2.5 log T

(6)
As before we can rewrite the regret bound (6) in terms of the small-variation balls Vα T deﬁned for
all α ∈ [0  1/4] and T (cid:62) 1 by

.

∀(cid:96)1:T ∈ [0  1]KT  

E[RT ((cid:96)1:T )] (cid:54) c

(cid:16)

(cid:17)

Vα T (cid:44)

(cid:96)1:T ∈ [0  1]KT :

QT
T K

(cid:54) α

.

Corollary 2. The second-order regret bound (6) of Hazan and Kale [2011b] is equivalent to:
∀α ∈ [0  1/4] 

K 2(cid:112)αT K log T + K 3/2 log2 T + K 5/2 log T

E[RT ((cid:96)1:T )] (cid:54) c

sup

(cid:16)

(7)

(cid:17)

.

(cid:26)

(cid:27)

(cid:96)1:T ∈Vα T

T(cid:88)

t=1

2

The proof is straightforward because the losses are deterministic and ﬁxed in advance by an oblivious
adversary. In Section 4 we provide a lower bound of order
αT K that holds whenever α =
Ω(log(T )/T ). This minimax lower bound shows that we cannot hope for a bound better than (7) by
log T if we only know the value of QT . Closing the gap is left as an open

more than a factor of K 2√

√

question.

Two impossibility results in the bandit setting We also show in Section 4 that  in contrast to
the full-information setting  regret bounds involving the cumulative variance of the algorithm as in
[Cesa-Bianchi et al.  2007] cannot be obtained in the bandit setting. More precisely  we prove that
two consequences that hold true in the full-information case  namely: (i) a regret bound proportional
to the effective range of the losses and (ii) a bounded regret if one arm performs best at all rounds 
must fail in the worst case for every bandit algorithm.

Additional notation and key tools Before the theorems we develop some additional notation and
describe the generic ideas in the proofs. For 1 (cid:54) i (cid:54) K let Ni(t) be the number of times action i
has been chosen after round t. All our lower bounds are derived by analysing the regret incurred
by strategies when facing randomised adversaries that choose the losses for all actions from the
same joint distribution in every round (sometimes independently for each action and sometimes not).
Ber(α) denotes the Bernoulli distribution with parameter α ∈ [0  1]. If P and Q are measures on the
same probability space  then KL(P  Q) is the KL-divergence between them. For a < b we deﬁne
clip[a b](x) = min{b  max{a  x}} and for x  y ∈ R we let x ∨ y = max{x  y}. Our main tools
throughout the analysis are the following information-theoretic lemmas. The ﬁrst bounds the KL
divergence between the laws of the observed losses/actions for two distributions on the losses.
Lemma 1. Fix a randomised bandit algorithm and two probability distributions Q1 and Q2
on [0  1]K. Assume the loss vectors (cid:96)1  . . .   (cid:96)T ∈ [0  1]K are drawn i.i.d. from either Q1 or Q2 
and denote by Qj the joint probability distribution on all sources of randomness when Qj is used
(formally  Qj = Pint ⊗ (Q⊗T
)  where Pint is the probability distribution used by the algorithm for
its internal randomisation). Let t (cid:62) 1. Denote by ht = (Is  (cid:96)Is s)1(cid:54)s(cid:54)t−1 the history available
at the beginning of round t  by Q(ht It)
the law of (ht  It) under Qj  and by Qj i the ith marginal
distribution of Qj. Then 

j

j

(cid:2)Ni(t − 1)(cid:3) KL(cid:0)Q1 i  Q2 i

(cid:1) .

EQ1

(cid:16)Q(ht It)

1

KL

(cid:17)

K(cid:88)

i=1

  Q(ht It)

2

=

Results of roughly this form are well known and the proof follows immediately from the chain rule
for the relative entropy and the independence of the loss vectors across time (see [Auer et al.  2002]
or the supplementary material). One difference is that the losses need not be independent across the
arms  which we heavily exploit in our proofs by using correlated losses. The second key lemma is an
alternative to Pinsker’s inequality that proves useful when the Kullback-Leibler divergence is larger
than 2. It has previously been used for bandit lower bounds (in the stochastic setting) by Bubeck et al.
[2013].
Lemma 2 (Lemma 2.6 in Tsybakov 2008). Let P and Q be two probability distributions on the
same measurable space. Then  for every measurable subset A (whose complement we denote by Ac) 

exp(cid:0)− KL(P  Q)(cid:1) .

P (A) + Q(Ac) (cid:62) 1
2

2 Zero-Order High Probability Lower Bounds

shows that no strategy can enjoy smaller regret than Ω((cid:112)KT log(1/δ)) with probability at least

We prove two new high-probability lower bounds on the regret of any bandit algorithm. The ﬁrst
1 − δ. Upper bounds of this form have been shown for various algorithms including Exp3.P [Auer
et al.  2002] and Exp3-IX [Neu  2015a]. Although this result is not very surprising  we are not aware
of any existing work on this problem and the proof is less straightforward than one might expect.
An added beneﬁt of our result is that the loss sequences producing large regret have two special
properties. First  the optimal arm is the same in every round and second the range of the losses in

each round is O((cid:112)K log(1/δ)/T ). These properties will be useful in subsequent analysis.

√
In the second lower bound we show that any algorithm for which E[RT ((cid:96)1:T )] = O(
necessarily suffer a high probability regret of at least Ω(

KT ) must
KT log(1/δ)) for some sequence (cid:96)1:T .

√

3

The important difference relative to the previous result is that strategies with log(1/δ) appearing
inside the square root depend on a speciﬁc value of δ  which must be known in advance.
Theorem 1. Suppose K (cid:62) 2 and δ ∈ (0  1/4) and T (cid:62) 32(K − 1) log(2/δ)  then there exists a
sequence of losses (cid:96)1:T ∈ [0  1]KT such that

(cid:112)(K − 1)T log(1/(4δ))

(cid:19)

(cid:62) δ/2  

P

RT ((cid:96)1:T ) (cid:62) 1
27

where the probability is taken with respect to the randomness in the algorithm. Furthermore (cid:96)1:T
can be chosen in such a way that there exists an i such that for all t it holds that (cid:96)i t = minj (cid:96)j t and

maxj k{(cid:96)j t − (cid:96)k t} (cid:54)(cid:112)(K − 1) log(1/(4δ))/T /(4
for any (cid:96)1:T ∈ [0  1]KT it holds that E[RT ((cid:96)1:T )] (cid:54) C(cid:112)(K − 1)T . Let δ ∈ (0  1/4) satisfy
(cid:112)(K − 1)/T log(1/(4δ)) (cid:54) C and T (cid:62) 32 log(2/δ). Then there exists (cid:96)1:T ∈ [0  1]KT for which

Theorem 2. Suppose K (cid:62) 2  T (cid:62) 1  and there exists a strategy and constant C > 0 such that

log 2).

√

(cid:18)

(cid:32)

(cid:112)(K − 1)T log(1/(4δ))

(cid:33)

203C

(cid:62) δ/2  

P

RT ((cid:96)1:T ) (cid:62)

where the probability is taken with respect to the randomness in the algorithm.
Corollary 3.
∈ [0  1]KT and δ ∈ (0  1) the regret
that

If p ∈ (0  1) and C > 0 

then there does not exist a strategy such
is bounded by

for all T   K 

RT ((cid:96)1:T ) (cid:62) C(cid:112)(K − 1)T logp(1/δ)

(cid:17) (cid:54) δ.

P(cid:16)

(cid:96)1:T

The corollary follows easily by integrating the assumed high-probability bound and applying Theo-
rem 2 for sufﬁciently large T and small δ. The proof may be found in the supplementary material.

Proof of Theorems 1 and 2 Both proofs rely on a carefully selected choice of correlated stochastic
losses described below. Let Z1  Z2  . . .   ZT be a sequence of i.i.d. Gaussian random variables with
mean 1/2 and variance σ2 = 1/(32 log(2)). Let ∆ ∈ [0  1/30] be a constant that will be chosen
differently in each proof and deﬁne K random loss sequences (cid:96)1

1:T   . . .   (cid:96)K

1:T where

clip[0 1](Zt − ∆)

clip[0 1](Zt − 2∆)
clip[0 1](Zt)

(cid:96)j
i t =

if i = 1
if i = j (cid:54)= 1
otherwise .

(cid:0)RT ((cid:96)i

For 1 (cid:54) j (cid:54) K let Qj be the measure on (cid:96)1:T ∈ [0  1]KT and I1  . . .   IT when (cid:96)i t = (cid:96)j
i t for all
1 (cid:54) i (cid:54) K and 1 (cid:54) t (cid:54) T . Informally  Qj is the measure on the sequence of loss vectors and actions
when the learner interacts with the losses sampled from the jth environment deﬁned above.
Lemma 3. Let δ ∈ (0  1) and suppose ∆ (cid:54) 1/30 and T (cid:62) 32 log(2/δ).
Then
1:T )] (cid:62) 7∆EQi[T − Ni(T )]/8.
Qi
The proof follows by substituting the deﬁnition of the losses and applying Azuma’s inequality to
show that clipping does not occur too often. See the supplementary material for details.

1:T ) (cid:62) ∆T /4(cid:1) (cid:62) Qi (Ni(T ) (cid:54) T /2)− δ/2 and EQi[RT ((cid:96)i

(cid:112)σ2(K − 1) log(1/(4δ))/(2T ) (cid:54) 1/30. By the pigeonhole principle there exists an i > 1 for

Proof of Theorem 1. First we choose the value of ∆ that determines the gaps in the losses by ∆ =
which EQ1 [Ni(T )] (cid:54) T /(K − 1). Therefore by Lemmas 2 and 1  and the fact that the KL divergence
between clipped Gaussian distributions is always smaller than without clipping 

Q1 (N1(T ) (cid:54) T /2) + Qi (N1(T ) > T /2) (cid:62) 1
2

−EQ1[Ni(T )](2∆)2

(cid:18)
1:T ) (cid:62) T ∆/4(cid:1) (cid:62) max{Q1 (N1(T ) (cid:54) T /2)   Qi (Ni(T ) (cid:54) T /2)} − δ/2

− 2T ∆2
σ2(K − 1)

(cid:0)RT ((cid:96)k

  Q(hT  IT )

(cid:62) 1
2

(cid:19)

(cid:18)

= 2δ .

2σ2

exp

exp

exp

i

(cid:62) 1
2
But by Lemma 3

Qk

max
k∈{1 i}

(cid:16)− KL

(cid:16)Q(hT  IT )
(cid:19)

1

(cid:17)(cid:17)

(cid:62) 1
2

(Q1 (N1(T ) (cid:54) T /2) + Qi (N1(T ) > T /2)) − δ/2 (cid:62) δ/2 .

4

The result is completed by substituting the value of σ2 = 1/(32 log(2)) and by noting that

32

(cid:115)

log

4δ

Qi

(cid:32)

RT ((cid:96)i

= Qi

1:T ) (cid:62)

(cid:18) 1

σ2T (K − 1)

Therefore there exists an i ∈ {1  . . .   K} such that

(cid:19)(cid:33)
maxj k{(cid:96)j t − (cid:96)k t} (cid:54) 2∆ (cid:54)(cid:112)(K − 1) log(1/(4δ))/T /(4
(cid:18) 1
(cid:35)

Proof of Theorem 2. By the assumption on δ we have ∆ = 7σ2
16C
for all i > 1 that

C(cid:112)(K − 1)T (cid:62) EQ1[RT ((cid:96)1

EQ1[Ni(T )] >

(cid:34) K(cid:88)

σ2
2∆2 log

Ni(T )

>

√

1:T )] (cid:62) 7∆
8

EQ1

.

4δ

1:T ) (cid:62) T ∆/4(cid:1) (cid:62) δ/2 .
(cid:0)RT ((cid:96)i
(cid:113) K−1
log(cid:0) 1
(cid:19)

log 2) Qi-almost surely.

4δ

T

(cid:1) (cid:54) 1/30. Suppose

(8)

7σ2(K − 1)

16∆

log

1
4δ

= C(cid:112)(K − 1)T  
1:T ) (cid:62) T ∆/4(cid:1) (cid:62) δ/2 .

(cid:0)RT ((cid:96)k

Then by the assumption in the theorem statement and the second part of Lemma 3 we have

which is a contradiction. Therefore there exists an i > 1 for which Eq. (8) does not hold. Then by the
same argument as the previous proof it follows that

(cid:18)

i=2

(cid:112)(K − 1)T log

(cid:19)

Qk

max
k∈{1 i}

RT ((cid:96)k

1:T ) (cid:62) 7σ2
4 · 16C

1
4δ

= max
k∈{1 i}

Qk

The result is completed by substituting the value of σ2 = 1/(32 log(2)).
Remark 1. It is possible to derive similar high-probability regret bounds with non-correlated losses.
However the correlation makes the results cleaner (we do not need an additional concentration
argument to locate the optimal arm) and it is key to derive Corollaries 4 and 5 in Section 4.

3 First-Order Lower Bound

small-loss balls Bα T deﬁned in (4). Theorem 3 below provides a new lower bound of order(cid:112)L∗

First-order upper bounds provide improvement over minimax bounds when the loss of the optimal
action is small. Recall from Corollary 1 that ﬁrst-order bounds can be rewritten in terms of the
T K 
which matches the best existing upper bounds up to logarithmic factors. As is standard for minimax
results this does not imply a lower bound on every loss sequence (cid:96)1:T . Instead it shows that we cannot
hope for a better bound if we only know the value of L∗
T .
Theorem 3. Let K (cid:62) 2  T (cid:62) K ∨ 118  and α ∈ [(c log(32T ) ∨ (K/2))/T  1/2]  where c = 64/9.
Then for any randomised bandit algorithm sup(cid:96)1:T ∈Bα T
αT K/27  where the
expectation is taken with respect to the internal randomisation of the algorithm.

E[RT ((cid:96)1:T )] (cid:62) √

√

Our proof is inspired by that of Auer et al. [2002  Theorem 5.1]. The key difference is that we take
Bernoulli distributions with parameter close to α instead of 1/2. This way the best cumulative loss
L∗
T is ensured to be concentrated around αT   and the regret lower bound
can be seen to involve the variance α(1 − α)T of the binomial distribution with parameters α and T .
First we state the stochastic construction of the losses and prove a general lemma that allows us
to prove Theorem 3 and will also be useful in Section 4 to a derive a lower bound in terms of the
quadratic variation. Let ε ∈ [0  1 − α] be ﬁxed and deﬁne K probability distributions (Qj)K
j=1 on
[0  1]KT such that under Qj the following hold:

αT K ≈(cid:112)α(1 − α)T K

α if i = j.

• All random losses (cid:96)i t for 1 (cid:54) i (cid:54) K and 1 (cid:54) t (cid:54) T are independent.
• (cid:96)i t is sampled from a Bernoulli distribution with parameter α + ε if i (cid:54)= j  or with parameter
Lemma 4. Let α ∈ (0  1)  K (cid:62) 2  and T (cid:62) K/(4(1−α)). Consider the probability distributions Qj
Qj. Then for

on [0  1]KT deﬁned above with ε = (1/2)(cid:112)α(1 − α)K/T   and set ¯Q = 1
any randomised bandit algorithm E[RT ((cid:96)1:T )] (cid:62)(cid:112)α(1 − α)T K/8  where the expectation is with

respect to both the internal randomisation of the algorithm and the random loss sequence (cid:96)1:T which
is drawn from ¯Q.

(cid:80)K

j=1

K

5

The assumption T (cid:62) K/(4(1 − α)) above ensures that ε (cid:54) 1 − α  so that the Qj are well deﬁned.
Proof of Lemma 4. We lower bound the regret by the pseudo-regret for each distribution Qj:

EQj

(cid:35)

T(cid:88)

(cid:34) T(cid:88)
(cid:96)It t − min
T(cid:88)
1(cid:54)i(cid:54)K

(cid:34) T(cid:88)
(cid:2)α + ε − ε1{It=j}(cid:3) − T α = T ε

(cid:62) EQj

EQj

(cid:96)i t

t=1

t=1

t=1

=

(cid:35)
(cid:32)

(cid:96)It t

(cid:34) T(cid:88)

t=1

(cid:35)
(cid:33)

(cid:96)i t

− min
1(cid:54)i(cid:54)K

EQj

T(cid:88)

1 − 1
T

Qj(It = j)

t=1

(9)
where the ﬁrst equality follows because EQj [(cid:96)It t] = EQj [EQj [(cid:96)It t|(cid:96)1:t−1  It]] = EQj [α + ε −
ε1{It=j}] since under Qj  the conditional distribution of (cid:96)t given ((cid:96)1:t−1  It) is simply ⊗K
i=1B(α +
ε − ε1{i=j}). To bound (9) from below  note that by Pinsker’s inequality we have for all t ∈
{1  . . .   T} and j ∈ {1  . . .   K}  Qj(It = j) (cid:54) Q0(It = j) + (KL(QIt
j )/2)1/2  where Q0 =
Ber(α + ε)⊗KT is the joint probability distribution that makes all the (cid:96)i t i.i.d. Ber(α + ε)  and
QIt
0 and QIt
j denote the laws of It under Q0 and Qj respectively. Plugging the last inequality above
into (9)  averaging over j = 1  . . .   K and using the concavity of the square root yields

0   QIt

t=1

 

(cid:34) T(cid:88)

t=1

E ¯Q

(cid:96)It t − min
1(cid:54)i(cid:54)K

T(cid:88)

t=1

(cid:118)(cid:117)(cid:117)(cid:116) 1

2T

−

1 − 1
(cid:17)

K

(cid:96)i t

(cid:62) T ε

(cid:35)
(cid:80)K
(cid:16)Q(ht It)
(cid:2)Nj(t − 1)(cid:3)

K

0

= EQ0

j

  Q(ht It)
ε2

α(1 − α)

(cid:17) (cid:54) KL

(cid:54) EQ0

where we recall that ¯Q = 1
KL(QIt
From Lemma 1

0   QIt

Qj. The rest of the proof is devoted to upper-bounding
j ). Denote by ht = (Is  (cid:96)Is s)1(cid:54)s(cid:54)t−1 the history available at the beginning of round t.

j=1

(cid:2)Nj(t − 1)(cid:3) KL(cid:0)B(α + ε) B(α)(cid:1)

(cid:16)QIt

KL

0   QIt

j

T(cid:88)

1
K

K(cid:88)

KL(cid:0)QIt

0   QIt

j

t=1

j=1

(cid:1)  

(10)

Part 2: Next we prove that

Qj(L∗

T > T α) (cid:54) 1
32T

.

6

 

(11)

where the last inequality follows by upper bounding the KL divergence by the χ2 divergence (see the
supplementary material). Averaging (11) over j ∈ {1  . . .   K} and t ∈ {1  . . .   T} and noting that

(cid:80)T
t=1(t − 1) (cid:54) T 2/2 we get
K(cid:88)
Plugging the above inequality into (10) and using the deﬁnition of ε = (1/2)(cid:112)α(1 − α)K/T yields

(t − 1)ε2
Kα(1 − α)

2Kα(1 − α)

0   QIt

1
K

T ε2

(cid:54)

j=1

.

j

KL(cid:0)QIt
T(cid:88)

T(cid:88)
(cid:18)

t=1

(cid:1) (cid:54) 1
(cid:35)

T

(cid:62) T ε

(cid:19)

(cid:112)α(1 − α)T K .

1 − 1
K

− 1
4

(cid:62) 1
8

E ¯Q

(cid:96)It t − min
1(cid:54)i(cid:54)K

(cid:96)i t

t=1

1
T

T(cid:88)
(cid:34) T(cid:88)

t=1

t=1

Proof of Theorem 3. We show that there exists a loss sequence (cid:96)1:T ∈ [0  1]KT such that L∗
(cid:54) αT
√
and E[RT ((cid:96)1:T )] (cid:62) (1/27)
αT K. Lemma 4 above provides such kind of lower bound  but without
the guarantee on L∗
T . For this purpose we will use Lemma 4 with a smaller value of α (namely  α/2)
and combine it with Bernstein’s inequality to prove that L∗
Part 1: Applying Lemma 4 with α/2 (note that T (cid:62) K (cid:62) K/(4(1 − α/2)) by assumption on T )
and noting that maxj EQj [RT ((cid:96)1:T )] (cid:62) E ¯Q[RT ((cid:96)1:T )] we get that for some j ∈ {1  . . .   K} the

probability distribution Qj deﬁned with ε = (1/2)(cid:112)(α/2)(1 − α/2)K/T satisﬁes

(cid:54) T α with high probability.

T

T

(cid:114) α

(cid:16)

(cid:17)

1 − α
2

T K (cid:62) 1
32

2

√

EQj [RT ((cid:96)1:T )] (cid:62) 1
8

since α (cid:54) 1/2 by assumption.

6αT K

(12)

(13)

t=1 (cid:96)j t. Second  note that under Qj  the (cid:96)j t  t (cid:62) 1  are i.i.d.
To this end  ﬁrst note that L∗
Ber(α/2). We can thus use Bernstein’s inequality: applying Theorem 2.10 (and a remark on p.38)
of Boucheron et al. [2013] with Xt = (cid:96)j t − α/2 (cid:54) 1 = b  with v = T (α/2)(1 − α/2)  and with
c = b/3 = 1/3)  we get that  for all δ ∈ (0  1)  with Qj-probability at least 1 − δ 

T

(cid:16)

(cid:17)

2T

α
2

1 − α
2

log

1
δ

+

1
3

log

1
δ

(cid:54)(cid:80)T

(cid:114)
(cid:19)(cid:114)

+

(cid:96)j t (cid:54) T α
2

(cid:18)

L∗

T

(cid:54) T(cid:88)

t=1

(cid:54) T α
2

1
3

1
δ

(cid:54) T α
2

T α
2

T

+

1 +

T α log

(14)
where the second last inequality is true whenever T α (cid:62) log(1/δ) and that last is true whenever
T α (cid:62) (8/3)2 log(1/δ) = c log(1/δ). By assumption on α  these two conditions are satisﬁed for
δ = 1/(32T )  which concludes the proof of (13).
Conclusion: We show by contradiction that there exists a loss sequence (cid:96)1:T ∈ [0  1]KT such that
L∗

(cid:54) αT and

= T α  

+

E[RT ((cid:96)1:T )] (cid:62) 1
64

6αT K  

(15)
where the expectation is with respect to the internal randomisation of the algorithm. Imagine for
a second that (15) were false for every loss sequence (cid:96)1:T ∈ [0  1]KT satisfying L∗
(cid:54) αT . Then
we would have 1{L∗
6αT K almost surely (since the internal
source of randomness of the bandit algorithm is independent of (cid:96)1:T ). Therefore by the tower rule for
the ﬁrst expectation on the r.h.s. below  we would get
+ EQj

√
(cid:54)αT}EQj [RT ((cid:96)1:T )|(cid:96)1:T ] (cid:54) (1/64)
(cid:104)
(cid:104)

RT ((cid:96)1:T )1{L∗

(cid:105)

(cid:105)

T

T

√

EQj [RT ((cid:96)1:T )] = EQj
√
(cid:54) 1
64

(cid:54)αT}
RT ((cid:96)1:T )1{L∗
6αT K + T · Qj(L∗

T

T > T α) (cid:54) 1
64

6αT K (16)
√
6αT K since α (cid:62) K/(2T ) >
where (16) follows from (13) and by noting that 1/32 < (1/64)
4/(6T ) (cid:62) 4/(6T K). Comparing (16) and (12) we get a contradiction  which proves that there exists
a loss sequence (cid:96)1:T ∈ [0  1]KT satisfying both L∗
(cid:54) αT and (15). We conclude the proof by
6/64 (cid:62) 1/27. Finally  the condition T (cid:62) K ∨ 118 is sufﬁcient to make the interval
noting that

(cid:2)(c log(32T ) ∨ (K/2))/T  1

(cid:3) non empty.

6αT K +

√

<

T

2

√

T >αT}
1
32

√

1
32

4 Second-Order Lower Bounds

We start by giving a lower bound on the regret in terms of the quadratic variation that is close to
existing upper bounds except in the dependence on the number of arms. Afterwards we prove that
bandit strategies cannot adapt to losses that lie in a small range or the existence of an action that is
always optimal.

Lower bound in terms of quadratic variation We prove a lower bound of Ω(
αT K) over any
small-variation ball Vα T (as deﬁned by (7)) for all α = Ω(log(T )/T ). This minimax lower bound

matches the upper bound of Corollary 2 up to a multiplicative factor of K 2(cid:112)log(T ). Closing this

gap is left as an open question  but we conjecture that the upper bound is loose (see also the COLT
open problem by Hazan and Kale [2011a]).
Theorem 4. Let K (cid:62) 2  T (cid:62) (32K) ∨ 601  and α ∈ [(2c1 log(T ) ∨ 8K)/T  1/4] 
√
where c1 = (4/9)2(3
Then for any randomised bandit algorithm 
αT K/25  where the expectation is taken with respect to the internal
sup(cid:96)1:T ∈Vα T
randomisation of the algorithm.

E[RT ((cid:96)1:T )] (cid:62) √

5 + 1)2 (cid:54) 12.

√

The proof is very similar to that of Theorem 3; it also follows from Lemma 4 and Bernstein’s
inequality. It is postponed to the supplementary material.

Impossibility results
In the full-information setting (where the entire loss vector is observed after
each round) Cesa-Bianchi et al. [2007  Theorem 6] designed a carefully tuned exponential weighting
algorithm for which the regret depends on the variation of the algorithm and the range of the losses:
(17)

E[RT ((cid:96)1:T )] (cid:54) 4(cid:112)VT log K + 4ET log K + 6ET  

∀(cid:96)1:T ∈ RKT  

7

VT =(cid:80)T

where the expectation is taken with respect to the internal randomisation of the algorithm
and ET = max1(cid:54)t(cid:54)T max1(cid:54)i j(cid:54)K |(cid:96)i t − (cid:96)j t| denotes the effective range of the losses and
t=1 VarIt∼pt((cid:96)It t) denotes the cumulative variance of the algorithm (in each round t the
expert’s action It is drawn at random from the weight vector pt). The bound in (17) is not closed-form
because VT depends on the algorithm  but has several interesting consequences:

1. If for all t the losses (cid:96)i t lie in an unknown interval [at  at + ρ] with a small width ρ > 0  then

VarIt∼pt((cid:96)It t) (cid:54) ρ2/4  so that VT (cid:54) T ρ2/4. Hence

E[RT ((cid:96)1:T )] (cid:54) 2ρ(cid:112)T log K + 4ρ log K + 6ρ .

Therefore  though the algorithm by Cesa-Bianchi et al. [2007  Section 4.2] does not use the prior
knowledge of at or ρ  it is able to incur a regret that scales linearly in the effective range ρ.

2. If all the losses (cid:96)i t are nonnegative  then by Corollary 3 of [Cesa-Bianchi et al.  2007] the

second-order bound (17) implies the ﬁrst-order bound

(cid:115)

(cid:18)

(cid:19)

MT − L∗

T
T

E[RT ((cid:96)1:T )] (cid:54) 4

L∗

T

log K + 39MT max{1  log K}  

(18)

where MT = max1(cid:54)t(cid:54)T max1(cid:54)i(cid:54)K (cid:96)i t .

3. If there exists an arm i∗ that is optimal at every round t (i.e.  (cid:96)i∗ t = mini (cid:96)i t for all t (cid:62) 1)  then
any translation-invariant algorithm with regret guarantees as in (18) above suffers a bounded
regret. This is the case for the fully automatic algorithm of Cesa-Bianchi et al. [2007  Theorem 6]
mentioned above. Then by the translation invariance of the algorithm all losses (cid:96)i t appearing in
the regret bound can be replaced with the translated losses (cid:96)i t − (cid:96)i∗ t (cid:62) 0  so that a bound of
the same form as (18) implies a regret bound of O(log K).
4. Assume that the loss vectors (cid:96)t are i.i.d. with a unique optimal arm in expectation (i.e.  there
exists i∗ such that E[(cid:96)i∗ 1] < E[(cid:96)i 1] for all i (cid:54)= i∗). Then using the Hoeffding-Azuma inequality
we can show that the algorithm of Cesa-Bianchi et al. [2007  Section 4.2] has with high
probability a bounded cumulative variance VT   and therefore (by (17)) incurs a bounded regret 
in the same spirit as in de Rooij et al. [2014]  Gaillard et al. [2014].

√

domised bandit algorithm  sup(cid:96)1 ... (cid:96)T ∈Cρ

We already know that point 2 has a counterpart in the bandit setting. If one is prepared to ignore
logarithmic terms  then point 4 also has an analogue in the bandit setting due to the existence
of logarithmic regret guarantees for stochastic bandits [Lai and Robbins  1985]. The following
corollaries show that in the bandit setting it is not possible to design algorithms to exploit the range
of the losses or the existence of an arm that is always optimal. We use Theorem 1 as a general tool
but the bounds can be improved to
T K/30 by analysing the expected regret directly (similar to
Lemma 4).

Corollary 5. Let K (cid:62) 2 and T (cid:62) 32(K − 1) log(14). Then  for any randomised bandit algorithm 
there is a loss sequence (cid:96)1:T ∈ [0  1]KT such that there exists an arm i∗ that is optimal at every

Corollary 4. Let K (cid:62) 2  T (cid:62) 32(K − 1) log(14) and ρ (cid:62) 0.22(cid:112)(K − 1)/T . Then for any ran-
E[RT ((cid:96)1:T )] (cid:62)(cid:112)T (K − 1)/504  where the expectation
is with respect to the randomness in the algorithm  and Cρ (cid:44)(cid:8)x ∈ [0  1]K : maxi j |xi − xj| (cid:54) ρ(cid:9).
round t (i.e.  (cid:96)i∗ t = mini (cid:96)i t for all t (cid:62) 1)  but E[RT ((cid:96)1:T )] (cid:62) (cid:112)T (K − 1)/504  where the
there exists an (cid:96)1:T such that P{RT ((cid:96)1:T ) (cid:62)(cid:112)(K − 1)T log(1/(4 · 0.15)/27} (cid:62) 0.15/2  which
implies (since RT ((cid:96)1:T ) (cid:62) 0 here) that E[RT ((cid:96)1:T )] (cid:62)(cid:112)(K − 1)T /504. Finally note that (cid:96)1:T ∈
Cρ since ρ (cid:62)(cid:112)(K − 1) log(1/(4δ))/T /(4

Proof of Corollaries 4 and 5. Both results follow from Theorem 1 by choosing δ = 0.15. Therefore

log 2) and there exists an i such that (cid:96)i t (cid:54) (cid:96)j t for all

expectation is with respect to the randomness in the algorithm.

√

j and t.

Acknowledgments

The authors would like to thank Aurélien Garivier and Émilie Kaufmann for insightful discus-
sions. This work was partially supported by the CIMI (Centre International de Mathématiques et
d’Informatique) Excellence program. The authors acknowledge the support of the French Agence
Nationale de la Recherche (ANR)  under grants ANR-13-BS01-0005 (project SPADRO) and ANR-
13-CORD-0020 (project ALICIA).

8

References
C. Allenberg  P. Auer  L. Györﬁ  and G. Ottucsák. Hannan consistency in on-line learning in case of
unbounded losses under partial monitoring. In Proceedings of ALT’2006  pages 229–243. Springer 
2006.

J. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings

of Conference on Learning Theory (COLT)  pages 217–226  2009.

P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. Gambling in a rigged casino: The adversarial
multi-armed bandit problem. In Foundations of Computer Science  1995. Proceedings.  36th
Annual Symposium on  pages 322–331. IEEE  1995.

P. Auer  N. Cesa-Bianchi  Y. Freund  and R.E. Schapire. The nonstochastic multi-armed bandit

problem. SIAM J. Comput.  32(1):48–77  2002.

S. Boucheron  G. Lugosi  and P. Massart. Concentration inequalities: a nonasymptotic theory of

independence. Oxford University Press  2013.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit

problems. Foundations and Trends in Machine Learning  5(1):1–122  2012.

S. Bubeck  V. Perchet  and P. Rigollet. Bounded regret in stochastic multi-armed bandits.

Proceedings of The 26th Conference on Learning Theory  pages 122–134  2013.

In

N. Cesa-Bianchi  Y. Mansour  and G. Stoltz. Improved second-order bounds for prediction with

expert advice. Mach. Learn.  66(2/3):321–352  2007.

S. de Rooij  T. van Erven  P. D. Grünwald  and W. M. Koolen. Follow the leader if you can  hedge if

you must. J. Mach. Learn. Res.  15(Apr):1281–1316  2014.

P. Gaillard  G. Stoltz  and T. van Erven. A second-order bound with excess losses. In Proceedings of

the 27th Conference on Learning Theory (COLT’14)  2014.

E. Hazan and S. Kale. A simple multi-armed bandit algorithm with optimal variation-bounded regret.

In Proceedings of the 24th Conference on Learning Theory  pages 817–820  2011a.

E. Hazan and S. Kale. Better algorithms for benign bandits. J. Mach. Learn. Res.  12(Apr):1287–1311 

2011b.

T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Adv. in Appl. Math.  6:

4–22  1985.

G. Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. In

Advances in Neural Information Processing Systems 28 (NIPS 2015)  2015a.

G. Neu. First-order regret bounds for combinatorial semi-bandits. In Proceedings of The 28th

Conference on Learning Theory  pages 1360–1375  2015b.

A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Proceedings of the 26th

Conference on Learning Theory  pages 993–1019  2013.

G. Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD

thesis  Paris-Sud XI University  2005.

A. Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media  2008.

9

,Sébastien Gerchinovitz
Tor Lattimore
Haw-Shiuan Chang
Erik Learned-Miller
Andrew McCallum
Junnan Li
Yongkang Wong
Qi Zhao
Mohan Kankanhalli