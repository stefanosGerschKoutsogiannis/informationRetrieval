2009,Learning with Compressible Priors,We describe probability distributions  dubbed compressible priors  whose independent and identically distributed (iid) realizations result in compressible signals. A signal is compressible when sorted magnitudes of its coefficients exhibit a power-law decay so that the signal can be well-approximated by a sparse signal. Since compressible signals live close to sparse signals  their intrinsic information can be stably embedded via simple non-adaptive linear projections into a much lower dimensional space whose dimension grows logarithmically with the ambient signal dimension. By using order statistics  we show that N-sample iid realizations of generalized Pareto  Student’s t  log-normal  Frechet  and log-logistic distributions are compressible  i.e.  they have a constant expected decay rate  which is independent of N. In contrast  we show that generalized Gaussian distribution with shape parameter q is compressible only in restricted cases since the expected decay rate of its N-sample iid realizations decreases with N as 1/[q log(N/q)]. We use compressible priors as a scaffold to build new iterative sparse signal recovery algorithms based on Bayesian inference arguments. We show how tuning of these algorithms explicitly depends on the parameters of the compressible prior of the signal  and how to learn the parameters of the signal’s compressible prior on the fly during recovery.,Learning with Compressible Priors

Volkan Cevher
Rice University

volkan@rice.edu

Abstract

We describe a set of probability distributions  dubbed compressible priors  whose
independent and identically distributed (iid) realizations result in p-compressible
signals. A signal x ∈ RN is called p-compressible with magnitude R if its sorted
coefﬁcients exhibit a power-law decay as |x|(i) (cid:46) R · i−d  where the decay rate d
is equal to 1/p. p-compressible signals live close to K-sparse signals (K (cid:28) N)
in the (cid:96)r-norm (r > p) since their best K-sparse approximation error decreases

with O(cid:0)R · K 1/r−1/p(cid:1). We show that the membership of generalized Pareto  Stu-

dent’s t  log-normal  Fr´echet  and log-logistic distributions to the set of compress-
ible priors depends only on the distribution parameters and is independent of N.
In contrast  we demonstrate that the membership of the generalized Gaussian dis-
tribution (GGD) depends both on the signal dimension and the GGD parameters:
the expected decay rate of N-sample iid realizations from the GGD with the shape
parameter q is given by 1/ [q log (N/q)]. As stylized examples  we show via ex-
periments that the wavelet coefﬁcients of natural images are 1.67-compressible
whereas their pixel gradients are 0.95 log (N/0.95)-compressible  on the average.
We also leverage the connections between compressible priors and sparse signals
to develop new iterative re-weighted sparse signal recovery algorithms that out-
perform the standard (cid:96)1-norm minimization. Finally  we describe how to learn the
hyperparameters of compressible priors in underdetermined regression problems
by exploiting the geometry of their order statistics during signal recovery.

Introduction

1
Many problems in signal processing  machine learning  and communications can be cast as a linear
regression problem where an unknown signal x ∈ RN is related to its observations y ∈ RM via

y = Φx + n.

(1)
In (1)  the observation matrix Φ ∈ RM×N is a non-adaptive measurement matrix with random
entries in compressive sensing (CS)  an over-complete dictionary of features in sparse Bayesian
learning (SBL)  or a code matrix in communications [1  2]. The vector n ∈ RM usually accounts
for physical noise with partially or fully known distribution  or it models bounded perturbations in
the measurement matrix or the signal.
Because of its theoretical and practical interest  we focus on the instances of (1) where there are
more unknowns than equations  i.e.  M < N. Hence  determining x from y in (1) is ill-posed: ∀v ∈
kernel (Φ)  x + v deﬁnes a solution space that produces the same observations y. Prior information
is therefore necessary to distinguish the true x among the inﬁnitely many possible solutions. For
instance  CS and SBL frameworks assume that the signal x belongs to the set of sparse signals. By
sparse  we mean that at most K out of the N signal coefﬁcients are nonzero where K (cid:28) N. CS
and SLB algorithms then regularize the solution space by signal priors that promote sparseness and
they have been extremely successful in practice in a number of applications even if M (cid:28) N [1–3].
Unfortunately  prior information by itself is not sufﬁcient to recover x from noisy y. Two more
key ingredients are required: (i) the observation matrix Φ must stably embed (or encode) the set of
signals x into the space of y  and (ii) a tractable decoding algorithm must exist to map y back to
x. By stable embedding  we mean that Φ is bi-Lipschitz where the encoding x → Φx is one to
one and the inverse mapping ∆ = {∆ (Φx) → x} is smooth. The bi-Lipschitz property of Φ is
crucial to ensure the stability in decoding x by controlling the amount by which perturbations of the

1

observations are ampliﬁed [1  4]. Tractable decoding is important for practical reasons as we have
limited time and resources  and it can clearly restrict the class of usable signal priors.
In this paper  we describe compressible prior distributions whose independent and identically dis-
tributed (iid) realizations result in compressible signals. A signal is compressible when sorted mag-
nitudes of its coefﬁcients exhibit a power-law decay. For certain decay rates  compressible signals
live close to the sparse signals  i.e.  they can be well-approximated by sparse signals. It is well-
known that the set of K-sparse signals has stable and tractable encoder-decoder pairs (Φ  ∆) for M
as small as O(K log (N/K)) [1  5]. Hence  an N-dimensional compressible signal with the proper
decay rate inherits the encoder-decoder pairs of its K-sparse approximation for a given approxima-
tion error  and can be stably embedded into dimensions logarithmic in N.
Compressible priors analytically summarize the set of compressible signals and shed new light on
underdetermined linear regression problems by building upon the literature on sparse signal recov-
ery. Our main results are summarized as follows:
1) By using order statistics  we show that the compressibility of the iid realizations of generalized
Pareto  Student’s t  Fr´echet  and log-logistics distributions is independent of the signals’ dimension.
These distributions are natural members of compressible priors: they truly support logarithmic di-
mensionality reduction and have important parameter learning guarantees from ﬁnite sample sizes.
We demonstrate that probabilistic models for the wavelet coefﬁcients of natural images must also be
a natural member of compressible priors.
2) We point out a common misconception about the generalized Gaussian distribution (GGD): GGD
generates signals that lose their compressibility as N grows. For instance  special cases of the GGD
distribution  e.g.  Laplacian distribution  are commonly used as sparsity promoting priors in CS and
SBL problems where M is assumed to grow logarithmically with N [1–3  6]. We show that signals
generated from Laplacian distribution can only be stably embedded into lower dimensions that grow
proportional to N. Hence  we identify an inconsistency between the decoding algorithms motivated
by the GGD distribution and their sparse solutions.
3) We use compressible priors as a scaffold to build new decoding algorithms based on Bayesian
inference arguments. The objective of these algorithms is to approximate the signal realization from
a compressible prior as opposed to pragmatically producing sparse solutions. Some of these new
algorithms are variants of the popular iterative re-weighting schemes [3 6–8]. We show how the tun-
ing of these algorithms explicitly depends on the compressible prior parameters  and how to learn
the parameters of the signal’s compressible prior on the ﬂy while recovering the signal.
The paper is organized as follows. Section 2 provides the necessary background on sparse signal
recovery. Section 3 mathematically describes the compressible signals and ties them with the order
statistics of distributions to introduce compressible priors. Section 4 deﬁnes compressible priors 
identiﬁes common misconceptions about the GGD distribution  and examines natural images as in-
stances of compressible priors. Section 5 derives new decoding algorithms for underdetermined
linear regression problems. Section 6 describes an algorithm for learning the parameters of com-
pressible priors. Section 7 provides simulations results and is followed by our conclusions.
2 Background on Sparse Signals
Any signal x ∈ RN can be represented in terms of N coefﬁcients αN×1 in a basis ΨN×N via
x = Ψα. Signal x has a sparse representation if only K (cid:28) N entries of α are nonzero. To account
for sparse signals in an appropriate basis  (1) should be modiﬁed as y = Φx + n = ΦΨα + n.
Let ΣK denote the set of all K-sparse signals. When Φ in (1) satisﬁes the so-called restricted
isometry property (RIP)  it can be shown that ΦΨ deﬁnes a bi-Lipschitz embedding of ΣK into
RM [1 4 5]. Moreover  RIP implies the recovery of K-sparse signals to within a given error bound 
and the best attainable lower bounds for M are related to the Gelfand width of ΣK  which is log-
arithmic in the signal dimension  i.e.  M = O(K log (N/K)) [5]. Without loss of generality  we
restrict our attention in the sequel to canonically sparse signals and assume that Ψ = I (the N × N
identity matrix) so that x = α.
With the sparsity prior and RIP assumptions  inverse maps can be obtained by solving the following
convex problems:

(2)

∆1(y) = arg min(cid:107)x
∆2(y) = arg min(cid:107)x
∆3(y) = arg min(cid:107)x

(cid:48)(cid:107)1 s.t. y = Φx
(cid:48)
 
(cid:48)(cid:107)2 ≤  
(cid:48)(cid:107)1 s.t. (cid:107)y − Φx
(cid:48)(cid:107)1 + τ(cid:107)y − Φx
(cid:48)(cid:107)2
2 

2

where  and τ are constants  and (cid:107)x(cid:107)r (cid:44) ((cid:80)

i |xi|r)1/r. The decoders ∆i (i = 1  2) are known as
basis pursuit (BP) and basis pursuit denoising (BPDN)  respectively; and  ∆3 is a scalarization of
BPDN [1  9]. They also have the following deterministic worst-case guarantee when Φ has RIP:

(cid:107)x − xK(cid:107)1

√
K

+ C2(cid:107)n(cid:107)2 

(cid:107)x − ∆(y)(cid:107)2 ≤ C1

(3)
where C1 2 are constants  xK is the best K-term approximation  i.e.  xK = arg min(cid:107)x(cid:48)(cid:107)0≤K (cid:107)x −
x(cid:48)(cid:107)r for r ≥ 1  and (cid:107)x(cid:107)0 is a pseudo-norm that counts the number of nonzeros of x [1  4  5].
Note that the error guarantee (3) is adaptive to each given signal x because of the deﬁnition of xK.
Moreover  the guarantee does not assume that the signal is sparse.
3 Compressible Signals  Order Statistics and Quantile Approximations
We deﬁne a signal x as p-compressible if it lives close to the shell of the weak-(cid:96)p ball of radius
R (sw(cid:96)p(R)–pronounced as swell p). Deﬁning ¯xi = |xi|  we arrange the signal coefﬁcients xi in
decreasing order of magnitude as

Then  when x ∈ sw(cid:96)p(R)  the i-th ordered entry ¯x(i) in (4) obeys

¯x(1) ≥ ¯x(2) ≥ . . . ≥ ¯x(N ).

(4)

¯x(i) (cid:46) R · i

−1/p 

(5)
where (cid:46) means “less than or approximately equal to.” We deliberately substitute (cid:46) for ≤ in the
p-compressibility deﬁnition of [1] to reduce the ambiguity of multiple feasible R and p values. In
Section 6  we describe a geometric approach to learn R and p so that R · i−1/p ≈ ¯x(i).
Signals in sw(cid:96)p(R) can be well-approximated by sparse signals as the best K-term approximation
error decays rapidly to zero as

(cid:107)x − xK(cid:107)r (cid:46) (r/p − 1)

−1/r RK 1/r−1/p  when p < r.

(6)
Given M  a good rule of thumb is to set K = M/[C log(N/M)] (C ≈ 4 or 5) and use (6) to predict
the approximation error for the decoders ∆i in Section 2. Since the decoding guarantees are bounded
by the best K-term approximation error in (cid:96)1 (i.e.  r = 1; cf. (3))  we will restrict our attention to
x ∈ sw(cid:96)p where p < 1. Including p = 1 adds a logarithmic error factor to the approximation errors 
which is not severe; however  it is not considered in this paper to avoid a messy discussion.
Suppose now the individual entries xi of the signal x are random variables (RV) drawn iid with
respect to a probability density function (pdf) f(x)  i.e.  xi ∼ f(x) for i = 1  . . .   N. Then  ¯x(i)’s
in (4) are also RV’s and are known as the order statistics (OS) of yet another pdf ¯f(¯x)  which can
be related to f(x) in a straightforward manner: ¯f(¯x) = f(¯x) + f(−¯x). Note that even though the
RV’s xi (hence  ¯xi) are iid  the RV’s ¯x(i) are statistically dependent.
The concept of OS enables us to create a link between signals summarized by pdf’s and their com-
pressibility  which is a deterministic property after the signals are realized. The key to establish-
ing this link turns out to be the parameterized form of the quantile function of the pdf ¯f(¯x). Let
¯f(v)dv be the cumulative distribution function (CDF) and u = ¯F (¯x). The quantile
function ¯F (cid:63)(u) of ¯f(¯x) is then given by the inverse of its CDF: ¯F (cid:63)(u) = ¯F −1(u). We will refer to
¯F (cid:63)(u) as the magnitude quantile function (MQF) of f(x).
A well-known quantile approximation to the expected OS of a pdf is given by [10]:

¯F (¯x) = (cid:82) ¯x

0

(7)
where E[·] is the expected value. Moreover  we have the following moment matching approximation

E[¯x(i)] = ¯F (cid:63)

 

(cid:18)

N + 1

1 − i

(cid:19)
(cid:0)1 − i
(cid:1)
N(cid:2)f(cid:0)E[¯x(i)](cid:1)(cid:3)2

i
N

N

(cid:33)

(cid:32)

¯x(i) ∼ N

E[¯x(i)] 

 

(8)

which can be used to quantify how much the actual realizations ¯x(i) deviate from E[¯x(i)]. For
instance  these deviations for i > K can be used to bound the statistical variations of the best K-
term approximation error. In practice  the deviations are relatively small for compressible priors.
In Sections 4–6  we will use the quantile approximation in (7) as our basis to motivate the set of
compressible priors  derive recovery algorithms for x  and learn the parameters of compressible
priors during recovery.

3

Table 1: Example distributions and the sw(cid:96)p(R) parameters of their iid realizations
Distribution

R

p

(cid:16)

(cid:17)−(q+1)

pdf
|x|
λ

1 +

q
2λ

(cid:18)

(cid:19)−(q+1)/2
Γ((q+1)/2)
√
2πλΓ(q/2)
(q/λ) (x/λ)−(q+1) e−(x/λ)−q

1 + x2
λ2

(q/λ)(x/λ)q−1
[1+(x/λ)q ]2
q

2λΓ(1/q) e−(|x|/λ)q
(q/λ) (x/λ)q−1 e−(x/λ)q
λΓ(q) (x/λ)q−1 e−x/λ
e−(q log(x/λ))2/2
q√

1

2πx

Generalized Pareto

Student’s t

Fr´echet

Log-Logistic

Generalized Gaussian

Weibull
Gamma

Log-Normal

(cid:104) 2Γ((q+1)/2)

λN 1/q

(cid:105)1/q

√

πqΓ(q/2)

λN 1/q
λN 1/q

λN 1/q

λ max {1  Γ (1 + 1/q)} log1/q (N/q)

λ max {1  Γ (1 + 1/q)q} log (qN )

λ log1/q N

√

λe

2 log N /q

q

q

q

q

q log (N/q)

q log N
log (qN )
√

2 log N q

¯F (cid:63)

N + 1

(cid:19)

(cid:18)

1 − i

(cid:46) R(N  θ) · i

4 Compressible Priors
A compressible prior f(x; θ) in (cid:96)r is a pdf with parameters θ whose MQF satisﬁes
−1/p(N θ)  where R > 0 and p < r.

(9)
Table 4 lists example pdf’s  parameterized by θ = (q  λ) (cid:31) 0  and the sw(cid:96)p(R) parameters of their
N-sample iid realizations. In this paper  we ﬁx r = 1 (cf. Section 3); hence  the example pdf’s are
compressible priors whenever p < 1. In (9)  we make it explicit that the sw(cid:96)p(R) parameters can
depend on the parameters θ of the speciﬁc compressible prior as well as the signal dimension N.
The dependence of the parameter p on N is of particular interest since it has important implications
in signal recovery as well as parameter learning from ﬁnite sample sizes  as discussed below.
We deﬁne natural p-compressible priors as the set Np of compressible priors such that p = p(θ) < 1
is independent of N  ∀f(x; θ) ∈ Np. It is possible to prove that we can capture most of the (cid:96)1-
energy in an N-sample iid realization from a natural p-compressible prior by using a constant K 
i.e.  (cid:107)x − xK(cid:107)1 ≤ (cid:107)x(cid:107)1 for any desired 0 <  (cid:28) 1 by choosing K = (cid:100)(p/) p
1−p(cid:101). Hence 
N-sample iid signal realizations from the compressible priors in Np can be truly embedded into
dimensions M that grow logarithmically with N with tractable decoding guarantees due to (3). Np
members include the generalized Pareto (GPD)  Fr´echet (FD)  and log-logistic distributions (LLD).
It then only comes as a surprise that generalized Gaussian distribution (GGD) is not a natural p-
compressible prior since its iid realizations lose their compressibility as N grows (cf. Table 4). While
it is common practice to use a GGD prior with q ≤ 1 for sparse signal recovery  we have no recov-
ery guarantees for signals generated from GGD when M grows logarithmically with N in (1).1 In
fact  to be p-compressible  the shape parameter of a GGD prior should satisfy q = NeW−1(−p/N ) 
where W−1(·) is the Lambert W -function with the alternate branch. As a result  the learned GGD
parameters from dimensionality-reduced data will in general depend on the dimension and may not
generalize to other dimensions. Along with GGD  Table 4 shows how Weibull  gamma  and log-
normal distributions are dimension-restricted in their membership to the set of compressible priors.
Wavelet coefﬁcients of natural images provide a stylized example to demonstrate why we should
care about the dimensional independence of the parameter p.2 As a brief background  we ﬁrst note
that research in natural image modeling to date has had two distinct approaches  with one focus-
ing on deterministic explanations and the other pursuing probabilistic models [12]. Deterministic
approaches operate under the assumption that the natural images belong to Besov spaces  having a
bounded number of derivatives between edges. Unsurprisingly  wavelet thresholding is proven near-
optimal for representing and denoising Besov space images. As the simplest example  the magnitude
sorted discrete wavelet coefﬁcients ¯w(i) of a Besov q-image should satisfy ¯w(i) = R · i−1/q. The
probabilistic approaches  on the other hand  exploit the power-law decay of the power spectra of im-
ages and ﬁt various pdf’s  such as GGD and the Gaussian scale mixtures  to the histograms of wavelet

1To illustrate the issues with the compressibility of GGD  consider the Laplacian distribution (LD: GGD
with q = 1)  which is the conventional convex prior for promoting sparsity. Via order statistics  it is possible
to show that ¯x(i) ≈ λ log N
for xi ∼ GGD(1  λ). Without loss of generality  let us judiciously pick λ =
1/ log N so that R = 1. Then  we have (cid:107)x(cid:107)1 ≈ N − 1 and (cid:107)x − xK(cid:107)1 ≈ N − K log (N/K) − K. When
we only have K terms to capture (1 − ) of the (cid:96)1 energy ( (cid:28) 1) in the signal x  we need K ≈ (1 − √
)N.
2Here  we assume that the reader is familiar with the discrete wavelet transform and its properties [11].

i

4

coefﬁcients while trying to simultaneously capture the dependencies observed in the marginal and
joint distributions of natural image wavelet coefﬁcients. Probabilistic approaches are quite important
in image compression because optimal compressors quantize the wavelet coefﬁcients according to
the estimated distributions  dictating the image compression limits via Shannon’s coding theorem.
We conjecture that probabilistic models that summarize the wavelet coefﬁcients of natural images
belong to the set of natural (non-iid) p-compressible priors. We base our claim on two observations:
1) Due to the multiscale nature of the wavelet transform  the decay proﬁle of the magnitude sorted
wavelet coefﬁcients are scale-invariant  i.e.  preserved at different resolutions  where lower resolu-
tions inherit the highest resolution. Hence  probabilistic models that explain the wavelet transform of
any signals should exhibit this decay proﬁle inheritance property. 2) The magnitude sorted wavelet
coefﬁcients of natural images exhibit a constant decay rate  as expected of Besov space images.
Section 7.2 demonstrates the ideas using natural images from the Berkeley natural images database.
5 Signal Decoding Algorithms
Convex problems to recover sparse or compressible signals in (2) are usually motivated by Bayesian
inference. In a similar fashion  we formalize two new decoding algorithms below by assuming prior
distributions on the signal x and the noise n  and then asking inference questions given y in (1).
5.1 Fixed point continuation for a non-iid compressible prior
The multivariate Lomax distribution (MLD) provides an elementary example of a non-iid compress-

ible prior. The pdf of the distribution is given by MLD(x; q  λ) ∝ (cid:16)

1 +(cid:80)N

(cid:0)1 +(cid:80)n

|xi|(cid:17)−q−N [13].
|xi|(cid:1)−1). In the sequel  we assume λi = λ ∀i  for which

For MLD  the marginal distribution of the signal coefﬁcients is GPD  i.e.  xi ∼ GPD(x; q  λi).
Moreover  given n-realizations x1:n of MLD (n ≤ N)  the joint marginal distribution of xn+1:N is
MLD(xn+1:N ; q + k  λn+1:N
it can be proved that MLD is compressible with p = 1 [14]. For now  we will only demonstrate
this property via simulations in Section 7.1. With the MLD prior on x  we focus on only two op-
timization problems below  one based on BP and the other based on maximum a posteriori (MAP)
estimation. Other convex formulations  such as BPDN (∆2 in (2)) and LASSO [15]  trivially follow.
1) BP Decoder: When there is no noise  the observations are given by y = Φx  which has inﬁnitely
many solutions  as discussed in Section 1. In this case  we can exploit the MLD likelihood function
to regularize the solution space. For instance  when we ask for the solution that maximizes the MLD
likelihood given y  it is easy to see that we obtain the BP decoder formulation  i.e.  ∆1(y) in (2).
2) MAP Decoder: Suppose that the noise coefﬁcients (ni’s in (1)) are iid Gaussian with zero mean
and variance σ2  ni ∼ N (n; 0  σ2). Although many inference questions are possible  here we seek
the mode of the posterior distribution to obtain a point estimate  also known as the MAP estimate.
be derived using the Bayes rule as(cid:98)xMAP = arg maxx(cid:48) f (y|x(cid:48))f (x(cid:48))  which is explicitly given by
Since we have f (y|x) = N (y − Φx; 0  σ2I M×M ) and f (x) =MLD(x; q  λ)  the MAP estimate can

i=1 λ

i=1 λ

−1
i

−1
i

(cid:98)xMAP = arg min

x(cid:48) (cid:107)y − Φx

(cid:48)(cid:107)2

2 + 2σ2(q + N ) log(cid:0)1 + λ

−1(cid:107)x

(cid:48)(cid:107)1

(cid:1) .

(10)

iterative decoder below  indexed by k  where(cid:98)x{k} is the k-th iteration estimate ((cid:98)x{0} = 0):

Unfortunately  we stumble upon a non-convex problem in (10) during our quest for the MAP es-
timate. We circumvent the non-convexity in (10) using a majorization-minimization idea where
we iteratively obtain a tractable upperbound on the log-term in (10) using the following inequality:
log u ≤ log v + u/v − 1. After some straightforward calculus  we obtain the
∀u  v ∈ (0 ∞) 
(cid:98)x{k} = arg min

(cid:48)(cid:107)1  where νk =

(cid:48)(cid:107)2
2 + νk(cid:107)x

(11)

x(cid:48) (cid:107)y − Φx

2σ2(q + N )

λ + (cid:107)(cid:98)x{k−1}(cid:107)1

.

The decoding approach in (11) can be viewed as a continuation (or a homotopy) algorithm where a
ﬁxed point is obtained at each iteration  similar to [16]. This decoding scheme has provable  linear

convergence guarantees when (cid:107)(cid:98)x{k}(cid:107)1 is strictly increasing (cid:20) (equivalently  νk (cid:21)) [16].

Iterative (cid:96)s-decoding for iid scale mixtures of GGD

5.2
We consider a generalization of GPD and the Student’s t distribution  which we will denote as
the generalized Gaussian gamma scale mixture distribution (SMD  in short)  whose pdf is given
−(q+1)/s. The additional parameter s of SMD modulates its
by SMD(x; q  λ  s) ∝ (1 + |x|s /λs)
It can be proved that SMD is p-compressible with p = q [14]. SMD  for
OS near the origin.
instance  arises through the following interaction of the gamma distribution and GGD: x = a−1/sb 
a ∼ Gamma(a; q/s  λ−s)  and b ∼ GGD(b; s  1). Given a  the distribution of x is a scaled GGD:

5

f(x|a) ∼ GGD(x; s  a−1). Marginalizing a from f(x|a)  we reach the SMD as the true underlying
distribution of x. SMD arise in multiple contexts  such as the SLB framework that exploit Student’s
t (i.e.  s = 2) for learning problems [2]  and the Laplacian and Gaussian scale mixtures (i.e.  s = 1
and 2  respectively) that model natural images [17  18].
Due to lack of space  we only focus on noiseless observations in (1). We assume that x is an N-
sample iid realization from SMD(x; q  λ  s) with known parameters (q  λ  s) (cid:31) 0 and choose a solu-

tion(cid:98)x that maximizes the SMD likehood to ﬁnd the true vector x among the kernel of Φ:

(cid:88)

i

−s |xi|s(cid:1)   s.t. y = Φx

log(cid:0)1 + λ
; where wi {k} =(cid:0)λs +(cid:12)(cid:12)xi {k}(cid:12)(cid:12)s(cid:1)−1 .

.

(cid:48)

(12)

(13)

(cid:98)x = max
(cid:88)
(cid:98)x{k} = min

x(cid:48)

i

x(cid:48) SMD(x; q  λ  s) = min
x(cid:48)

wi {k} |xi|s   s.t. y = Φx

(cid:48)

The majorization-minimization trick in Section 5.1 also circumvents the non-convexity in (12):

The decoding scheme in (13) is well-known as the iterative re-weighted (cid:96)s algorithms [7  19–21].
6 Parameter Learning for Compressible Distributions
While deriving decoding algorithms in Section 5  we assumed that the signal coefﬁcients xi are
generated from a compressible prior f(x; θ) and that θ is known. We now relax the latter assumption
and discuss how to simultaneously estimate x and learn the parameters θ.
When we visualize the joint estimation of x and θ from y in (1) as a graphical model  we imme-
diately realize that x creates a Markov blanket for θ. Hence  to determine θ  we have to estimate
the signal coefﬁcients. When Φ has the stable embedding property  we know that the decoding al-
gorithms can obtain x with approximation guarantees  such as (3). Then  given x  we can choose
an estimator for θ via standard Bayesian inference arguments. Unfortunately  this argument leads to
one important road block: estimation of the signal x without knowing the prior parameters θ.
A n¨aive approach to overcoming this road block is to split the optimization space and alternate
on x and θ while optimizing the Bayesian objective. Unfortunately  there is one important and
unrecognized bug in this argument: the estimated signal values are in general not iid  hence we
would be minimizing the wrong Bayesian objective to determine θ. To see this  we ﬁrst note that the

approximation of the signal xK and some other coefﬁcients that explain the small tail energy. We
then recall from Section (3) that the coefﬁcients of xK are statistically dependent. Hence  at least

recovered signals(cid:98)x in general consist of M (cid:28) N non-zero coefﬁcients that mimic the best K-term
partially  the signiﬁcant coefﬁcients of(cid:98)x are also dependent. One way to overcome this dependency
in ﬁtting the sw(cid:96)p(R) parameters via the auxiliary signal estimates(cid:98)x{k} during iterative recovery.
log(cid:12)(cid:12)(cid:98)xi {k}(cid:12)(cid:12) = log R(N  θ) −

issue is to treat the recovered signals as if they are drawn iid from a censored GPD. However  the
optimization becomes complicated and the approach does not provide any additional guarantees.
As an alternative  we propose to exploit geometry and use the consensus among the coefﬁcients

To do this  we employ Fischler and Bolles’ probabilistic random sampling consensus (RANSAC)
algorithm [22] to ﬁt a line  whose y-intercept is log R(N  θ) and whose slope is 1/p(N  θ):

1

p(N  θ)

log i  for i = 1  . . .   K; where K = M/[C log(N/M )] 

(14)
where C ≈ 4  5 as discussed in Section. 3. RANSAC provides excellent results with high probability
even if the data contains signiﬁcant outliers. Because of its probabilistic nature  it is computationally
efﬁcient. The RANSAC algorithm requires a threshold to gate the observations and count how much
a proposed solution is supported by the observations [22]. We determine this threshold by bounding
the tail probability that the OS of a compressible prior will be out of bounds. For the pseudo-code
and further details of the RANSAC algorithm  cf. [22].
7 Experiments
7.1 Order Statistics
To demonstrate the sw(cid:96)p(R) decay proﬁle of p-compressible priors  we generated iid realizations
of GGD with q = 1 (LD) and GPD with q = 1  and (non-iid) realizations of MLD with q = 1 of
varying signal dimensions N = 10j  where j = 2  3  4  5. We sorted the magnitudes of the signal
coefﬁcients  normalized them by their corresponding value of R. We then plotted the results on a
log-log scale in Fig. 1. At http://dsp.rice.edu/randcs  we provide a MATLAB routine (randcs.m) so
that it is easy to repeat the same experiment for the rest of the distributions in Table 4.

6

(a) LD (iid)

(b) GPD (iid)

(c) MLD

Figure 1: Numerical illustration of the sw(cid:96)p(R) decay proﬁle of three different pdfs.

To live in sw(cid:96)p(1) with 0 < p ≤ 1  the slope of the resulting curve must be less than or equal to −1.
Figure 1(a) illustrates that the iid LD slope is much greater than −1 and moreover logarithmically
grows with N. In contrast  Fig. 1(b) shows that iid GPD with q = 1 exhibits the constant slope of
−1 that is independent of N. MLD with q = 1 also delivers such a slope (Fig. 1(c)). The latter two
distributions thus produce compressible signal realizations  while the Laplacian does not.
7.2 Natural Images
We investigate the images from the Berkeley natural images database in the context of p-
compressible priors. We randomly sample 100 image patches of varying sizes N = 2j × 2j
(j = 3  . . .   8)  take their wavelet transforms (scaling ﬁlter: daub2)  and plot the average of their
magnitude ordered wavelet coefﬁcients in Figs. 2(a) and (b) (solid lines). Figure 2(c) also illustrates
the OS of the pixel gradients  which are of particular interest in many applications.
Along with the wavelet coefﬁcients  Fig. 2(a) superposes the expected OS of GPD with q = 1.67

and λ = 10 (dashed line)  given by ¯x(i){GPD(q  λ)} = λ(cid:2)(N + 1)1/qi−1/q − 1(cid:3) (i = 1  . . .   N).

Although wavelet coefﬁcients of natural images do not follow an iid distribution  they exhibit a
constant decay rate  which can be well-approximated by an iid GPD distribution. This apparent
constant decay rate is well-explained by the decay proﬁle inheritance of the wavelet transform across
different resolutions and supports the Besov space assumption used in the deterministic approaches.
The GPD rate of q = 1.67 implies a disappointing O(K−0.1) approximation rate in the (cid:96)2-norm
vs. the theoretical O(K−0.5) rate [23]. Moreover  we lose all the guarantees in the (cid:96)1-norm.

(a) wavelet coefﬁcients

(b) wavelet coefﬁcients

(c) pixel gradients

Figure 2: Approximation of the order statistics and histograms of natural images with GPD and GGD.

In contrast  Fig. 2(b) demonstrates the GGD histogram ﬁts to the wavelet coefﬁcients  where the
GGD exponent q ∈ [0.5  1] depends on the particular dimension and decreases as N increases. The
histogram matching is common practice in the existing probabilistic approaches (e.g.  [18]) to de-
termine pdf’s that explain the statistics of natural images. Typically  least square error metrics or
Kullback-Liebler (KL) divergence measures are used. Although the GGD ﬁt via histogram matching
in Fig. 2(b) deceptively appears to ﬁt a small number of coefﬁcients  we emphasize the log-log scale
of the plots and mention that there is a signiﬁcant number of coefﬁcients in the narrow space where
the GGD distribution is a good ﬁt. Unfortunately  these approaches approximate the wavelet coefﬁ-

7

02345−10−5−4−3−2−10ordered index [power of 10]normalized values slope = −1average of 100 realizations02345−10−5−4−3−2−10ordered index [power of 10]normalized values slope = −1average of 100 realizations02345−10−5−4−3−2−10ordered index [power of 10]normalized values slope = −1average of 100 realizations012345−5012345ordered index [power of 10]coefficient amplitudes average of 100 imagesGPD(q=1/0.6;λ=10)012345−5012345ordered index [power of 10]coefficient amplitudes average of 100 imageshistogram fit − GGD012345−5012345ordered index [power of 10]coefficient amplitudes average of 100 imagesGGD(q=0.95;λ=25)(a)

(b)

(c)

Figure 3: Improvements afforded by re-weighted (cid:96)1-decoding (a) with known parameters θ and (b) with
learning. (c) The learned sw(cid:96)p exponent of the GPD distribution with q = 0.4 via the RANSAC algorithm.

Iterative (cid:96)1 Decoding

cients of natural images that have almost no approximation power of the overall image. Moreover 
the learned GGD distribution is dimension dependent  assigns lower probability to the large coefﬁ-
cients that explain the image well  and predicts a mismatched OS of natural images (cf.Fig. 2(b)).
Figure 2(c) compares the magnitude ordered pixel gradients of the images (solid lines) with the
expected OS of GGD (dashed line). From the ﬁgure  it appears that the natural image pixel gradients
lose their compressibility as the image dimensions grow  similar to the GGD  Weibull  gamma  and
log-normal distributions. In the ﬁgure  the GGD parameters are given (q  λ) = (0.95  25).
7.3
We repeat the compressible signal recovery experiment in Section 3.2 of [7] to demonstrate the
performance of our iterative (cid:96)s decoder with s = 1 in (13). We ﬁrst randomly sample a signal
x ∈ RN (N = 256) where the signal coefﬁcients are iid from the GPD distribution with q = 0.4
and λ = (N + 1)−1/q so that the E[¯x(1)] ≈ 1. We set M = 128 and draw a random M × N matrix
with iid Gaussian entries to obtain y = Φx. We then decode signals via (13) where maximum
iterations is set to 5  with the knowledge of the signal parameters and with learning. During the
learning phase  we use log(2) as the threshold for the RANSAC algorithm. We set the maximum
iteration count of RANSAC to 500.
The results of a Monte Carlo run with 100 independent realizations are illustrated in Fig. 3.
In
Figs. 3(a) and (b)  the plots summarize the average improvement over the standard decoder ∆1(y)

via the histograms of (cid:107)x − (cid:98)x{4}(cid:107)2/(cid:107)x − ∆1(y)(cid:107)2  which have mean and standard deviation

(0.7062  0.1380) when we know the parameters of the GPD (a) and (0.7101  0.1364) when we learn
the parameters of the GPD via RANSAC (b). The learned sw(cid:96)p exponent is summarized by the his-
togram in Fig. 3(c)  which has mean and standard deviation (0.3757  0.0539). Hence  we conclude
that the our alternative learning approach via the RANSAC algorithm is competitive with knowing
the actual prior parameters that generated the signal. Moreover  the computational time of learning
is insigniﬁcant compared to time required by the state-of-the art linear SPGL algorithm [24].
8 Conclusions3
Compressible priors create a connection between probabilistic and deterministic models for signal
compressibility. The bridge between these seemingly two different modeling frameworks turns out
to be the concept of order statistics. We demonstrated that when the p-parameter of a compressible
prior is independent of the ambient dimension N  it is possible to have truly logarithmic embedding
of its iid signal realizations. Moreover  the learned parameters of such compressible priors are di-
mension agnostic. In contrast  we showed that when the p-parameter depends on N  we have many
restrictions in signal embedding and recovery as well as in parameter learning. We illustrated that
wavelet coefﬁcients of natural images can be well approximated by the generalized Pareto prior 
which in turn predicts a disappointing approximation rate for image coding with the n¨aive sparse
model and for CS image recovery from measurements that grow only logarithmically with the im-
age dimension. We motivated many of the existing sparse signal recovery algorithm as instances of
a corresponding compressible prior and discussed parameter learning for these priors from dimen-
sionality reduced data. We hope that the iid compressibility view taken in this paper will pave the
way for a better understanding of probabilistic non-iid and structured compressibility models.

3We thank R. G. Baraniuk  M. Wakin  M. Davies  J. Haupt  and J. P. Slavinksy for useful discussions.

Supported by ONR N00014-08-1-1112  DARPA N66001-08-1-2065  ARO W911NF-09-1-0383 grants.

8

0.20.40.60.811.20.20.40.60.811.200.20.40.60.81References
[1] E. J. Cand`es. Compressive sampling.

volume 3  pages 1433–1452  Madrid  Spain  2006.

In Proc. International Congress of Mathematicians 

[2] M.E. Tipping. Sparse bayesian learning and the relevance vector machine. The Journal of

Machine Learning Research  1:211–244  2001.

[3] D. P. Wipf and B. D. Rao. Sparse Bayesian learning for basis selection. IEEE Transactions on

Signal Processing  52(8):2153–2164  2004.

[4] T. Blumensath and M.E. Davies. Sampling theorems for signals from the union of linear

subspaces. IEEE Trans. Info. Theory  2009.

[5] A. Cohen  W. Dahmen  and R. DeVore. Compressed sensing and best k-term approximation.

American Mathematical Society  22(1):211–231  2009.

[6] I. F. Gorodnitsky  J. S. George  and B. D. Rao. Neuromagnetic source imaging with FO-
CUSS: a recursive weighted minimum norm algorithm. Electroenceph. and Clin. Neurophys. 
95(4):231–251  1995.

[7] E. J. Cand`es  M. B. Wakin  and S. P. Boyd. Enhancing sparsity by reweighted (cid:96)1 minimization.

Journal of Fourier Analysis and Applications  14(5):877–905  2008.

[8] D. P. Wipf and S. Nagarajan. Iterative reweighted (cid:96)1 and (cid:96)2 methods for ﬁnding sparse solu-

tions. In SPARS09  Rennes  France  2009.

[9] S. S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM

review  pages 129–159  2001.

[10] H.A. David and H.N. Nagaraja. Order Statistics. Wiley-Interscience  2004.
[11] S. Mallat. A Wavelet Tour of Signal Processing. Academic Press  1999.
[12] H. Choi and R. G. Baraniuk. Wavelet statistical models and Besov spaces. Lecture Notes in

Statistics  pages 9–30  2003.

[13] T. K. Nayak. Multivariate Lomax distribution: properties and usefulness in reliability theory.

Journal of Applied Probability  pages 170–177  1987.

[14] V. Cevher. Compressible priors. IEEE Trans. on Information Theory  in preparation  2010.
[15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society  pages 267–288  1996.

[16] E. T. Hale  W. Yin  and Y. Zhang. Fixed-point continuation for (cid:96)1-minimization: Methodology

and convergence. SIAM Journal on Optimization  19:1107  2008.

[17] P. J. Garrigues. Sparse Coding Models of Natural Images: Algorithms for Efﬁcient Inference
and Learning of Higher-Order Structure. PhD thesis  EECS Department  University of Cali-
fornia  Berkeley  May 2009.

[18] M. J. Wainwright and E. P. Simoncelli. Scale mixtures of Gaussians and the statistics of natural

images. In NIPS  2000.

[19] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In NIPS  vol-

ume 20  2008.

[20] I. Daubechies  R. DeVore  M. Fornasier  and S. Gunturk. Iteratively re-weighted least squares

minimization for sparse recovery. Commun. Pure Appl. Math  2009.

[21] R. Chartrand and W. Yin.

Iteratively reweighted algorithms for compressive sensing.

ICASSP  pages 3869–3872  2008.

In

[22] M.A. Fischler and R.C. Bolles. Random sample consensus: a paradigm for model ﬁtting
with applications to image analysis and automated cartography. Communications of the ACM 
24(6):381–395  1981.

[23] E. J. Candes and D. L. Donoho. Curvelets and curvilinear integrals. Journal of Approximation

Theory  113(1):59–90  2001.

[24] E. van den Berg and M. P. Friedlander. Probing the Pareto frontier for basis pursuit solutions.

SIAM Journal on Scientiﬁc Computing  31(2):890–912  2008.

9

,Ashok Cutkosky
Róbert Busa-Fekete