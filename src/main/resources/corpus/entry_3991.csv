2017,On the Power of Truncated SVD for General High-rank Matrix Estimation Problems,We show that given an estimate $\widehat{\mat A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $\mat A$ in spectral norm (i.e.  $\|\widehat{\mat A}-\mat A\|_2 \leq \delta$)  the simple truncated Singular Value Decomposition of $\widehat{\mat A}$ produces a multiplicative approximation of $\mat A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems: 1.High-rank matrix completion: we show that it is possible to recover a {general high-rank matrix} $\mat A$ up to $(1+\varepsilon)$ relative error in Frobenius norm from partial observations  with sample complexity independent of the spectral gap of $\mat A$. 2.High-rank matrix denoising: we design algorithms that recovers a matrix $\mat A$ with relative error in Frobenius norm from its noise-perturbed observations  without assuming $\mat A$ is exactly low-rank. 3.Low-dimensional estimation of high-dimensional covariance: given $N$ i.i.d.~samples of dimension $n$ from $\mathcal N_n(\mat 0 \mat A)$  we show that it is possible to estimate the covariance matrix $\mat A$ with relative error in Frobenius norm with $N\approx n$ improving over classical covariance estimation results which requires $N\approx n^2$.,On the Power of Truncated SVD for General

High-rank Matrix Estimation Problems

Simon S. Du

Carnegie Mellon University

ssdu@cs.cmu.edu

Yining Wang

Carnegie Mellon University
yiningwa@cs.cmu.edu

Aarti Singh

Carnegie Mellon University
aartisingh@cmu.edu

Abstract

We show that given an estimate �A that is close to a general high-rank positive semi-
deﬁnite (PSD) matrix A in spectral norm (i.e.  ��A−A�2 ≤ δ)  the simple truncated
Singular Value Decomposition of �A produces a multiplicative approximation of A

in Frobenius norm. This observation leads to many interesting results on general
high-rank matrix estimation problems:
1. High-rank matrix completion: we show that it is possible to recover a general
high-rank matrix A up to (1 + ε) relative error in Frobenius norm from partial
observations  with sample complexity independent of the spectral gap of A.

2. High-rank matrix denoising: we design an algorithm that recovers a matrix A
with error in Frobenius norm from its noise-perturbed observations  without
assuming A is exactly low-rank.

3. Low-dimensional approximation of high-dimensional covariance: given N
i.i.d. samples of dimension n from Nn(0  A)  we show that it is possible to
approximate the covariance matrix A with relative error in Frobenius norm with
N ≈ n  improving over classical covariance estimation results which requires
N ≈ n2.

1

Introduction

Let A be an unknown general high-rank n × n PSD data matrix that one wishes to estimate. In many
machine learning applications  though A is unknown  it is relatively easy to obtain a crude estimate
�A that is close to A in spectral norm (i.e.  ��A − A�2 ≤ δ). For example  in matrix completion

a simple procedure that ﬁlls all unobserved entries with 0 and re-scales observed entries produces
an estimate that is consistent in spectral norm (assuming the matrix satisﬁes a spikeness condition 
standard assumption in matrix completion literature). In matrix de-noising  an observation that is
corrupted by Gaussian noise is close to the underlying signal  because Gaussian noise is isotropic and
has small spectral norm. In covariance estimation  the sample covariance in low-dimensional settings
is close to the population covariance in spectral norm under mild conditions [Bunea and Xiao  2015].
However  in most such applications it is not sufﬁcient to settle for a spectral norm approximation. For
example  in recommendation systems (an application of matrix completion) the zero-ﬁlled re-scaled
rating matrix is close to the ground truth in spectral norm  but it is an absurd estimator because
most of the estimated ratings are zero. It is hence mandatory to require a more stringent measure of

which ensures that (on average) the estimate is close to the ground truth in an element-wise sense. A

performance. One commonly used measure is the Frobenius norm of the estimation error ��A − A�F  
spectral norm approximation �A is in general not a good estimate under Frobenius norm  because in
high-rank scenarios ��A − A�F can be √n times larger than ��A − A�2.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

In this paper  we show that in many cases a powerful multiplicative low-rank approximation in
Frobenius norm can be obtained by applying a simple truncated SVD procedure on a crude  easy-
to-ﬁnd spectral norm approximate. In particular  given the spectral norm approximation condition

in Frobenius and spectral norm. To our knowledge  the best existing result under the assumption

��A − A�2 ≤ δ  the top-k SVD of �Ak of �A multiplicatively approximates A in Frobenius norm; that
is  ��Ak − A�F ≤ C(k  δ  σk+1(A))�A − Ak�F   where Ak is the best rank-k approximation of A
��A − A�2 ≤ δ is due to Achlioptas and McSherry [2007]  who showed that ��Ak − A�F ≤
�A − Ak�F + √kδ + 2k1/4�δ�Ak�F   which depends on �Ak�F and is not multiplicative in
�A − Ak�F .
Below we summarize applications in several matrix estimation problems.

High-rank matrix completion Matrix completion is the problem of (approximately) recovering a
data matrix from very few observed entries. It has wide applications in machine learning  especially
in online recommendation systems. Most existing work on matrix completion assumes the data matrix
is exactly low-rank [Candes and Recht  2012  Sun and Luo  2016  Jain et al.  2013]. Candes and Plan
[2010]  Keshavan et al. [2010] studied the problem of recovering a low-rank matrix corrupted by
stochastic noise; Chen et al. [2016] considered sparse column corruption. All of the aforementioned
work assumes that the ground-truth data matrix is exactly low-rank  which is rarely true in practice.
Negahban and Wainwright [2012] derived minimax rates of estimation error when the spectrum
of the data matrix lies in an �q ball. Zhang et al. [2015]  Koltchinskii et al. [2011] derived oracle
inequalities for general matrix completion; however their error bound has an additional O(√n)
multiplicative factor. These results also require solving computationally expensive nuclear-norm
penalized optimization problems whereas our method only requires solving a single truncated
singular value decomposition. Chatterjee et al. [2015] also used the truncated SVD estimator for
matrix completion. However  his bound depends on the nuclear norm of the underlying matrix
which may be √n times larger than our result. Hardt and Wootters [2014] used a “soft-deﬂation”
technique to remove condition number dependency in the sample complexity; however  their error
bound for general high-rank matrix completion is additive and depends on the “consecutive” spectral
gap (σk(A) − σk+1(A))  which can be small in practical settings [Balcan et al.  2016  Anderson
et al.  2015]. Eriksson et al. [2012] considered high-rank matrix completion with additional union-of-
subspace structures.
In this paper  we show that if the n × n data matrix A satisﬁes µ0-spikeness condition  1 then for
any � ∈ (0  1)  the truncated SVD of zero-ﬁlled matrix �Ak satisﬁes ��Ak − A�F ≤ (1 + O(�))�A −
Ak�F if the sample complexity is lower bounded by Ω( n max{�−4 k2}µ2
)  which can be
0 max{�−4  k2}γk(A)2 · nrs(A) log n)  where γk(A) = σ1(A)/σk+1(A)
further simpliﬁed to Ω(µ2
is the kth-order condition number and rs(A) = �A�2
2 ≤ rank(A) is the stable rank of
A. Compared to existing work  our error bound is multiplicative  gap-free  and the estimator is
computationally efﬁcient. 2

F /�A�2

σk+1(A)2

0�A�2

F log n

Gaussian noise matrix with zero mean and ν2/n variance on each entry. By simple concentration

High-rank matrix de-noising Let �A = A + E be a noisy observation of A  where E is a PSD
results we have ��A − A�2 = ν with high probability; however  �A is in general not a good estimator
of A in Frobenius norm when A is high-rank. Speciﬁcally  ��A − A�F can be as large as √nν.
Applying our main result  we show that if ν < σk+1(A) for some k � n  then the top-k SVD �Ak of
�A satisﬁes ��Ak − A�F ≤ (1 + O(�ν/σk+1(A)))�A − Ak�F + √kν. This suggests a form of
bias-variance decomposition as larger rank threshold k induces smaller bias �A − Ak�F but larger
variance kν2. Our results generalize existing work on matrix de-noising [Donoho and Gavish  2014 
Donoho et al.  2013  Gavish and Donoho  2014]  which focus primarily on exact low-rank A.

1n�A�max ≤ µ0�A�F ; see also Deﬁnition 2.1.
2We remark that our relative-error analysis does not  however  apply to exact rank-k matrix where σk+1 = 0.
This is because for exact rank-k matrix a bound of the form (1 + O(�))�A − Ak�F requires exact recovery of
A  which truncated SVD cannot achieve. On the other hand  in the case of σk+1 = 0 a weaker additive-error
bound is always applicable  as we show in Theorem 2.3.

2

Low-rank estimation of high-dimensional covariance The (Gaussian) covariance estimation
problem asks to estimate an n × n PSD covariance matrix A  either in spectral or Frobenius
norm  from N i.i.d. samples X1 ···   XN ∼ N (0  A). The high-dimensional regime of covariance
estimation  in which N ≈ n or even N � n  has attracted enormous interest in the mathematical
statistics literature [Cai et al.  2010  Cai and Zhou  2012  Cai et al.  2013  2016]. While most existing
work focus on sparse or banded covariance matrices  the setting where A has certain low-rank
structure has seen rising interest recently [Bunea and Xiao  2015  Kneip and Sarda  2011]. In
particular  Bunea and Xiao [2015] shows that if n = O(N β) for some β ≥ 0 then the sample
covariance estimator �A = 1

N�N
N �  
��A − A�F = OP��A�2re(A)� log N

i=1 XiX�i satisﬁes

(1)

paper we

where re(A) = tr(A)/�A�2 ≤ rank(A) is the effective rank of A. For high-rank matrices where
re(A) ≈ n  Eq. (1) requires N = Ω(n2 log n) to approximate A consistently in Frobenius norm.
In
re(A) max{�−4 k2}γk(A)2 log N
≤ c for some small universal constant c > 0  then ��Ak − A�F admits
a relative Frobenius-norm error bound (1+O(�))�A−Ak�F with high probability. Our result allows
reasonable approximation of A in Frobenius norm under the regime of N = Ω(npoly(k) log n)
if γk = O (poly (k))  which is signiﬁcantly more ﬂexible than N = Ω(n2 log n)  though the
dependency of � is worse than [Bunea and Xiao  2015]. The error bound is also agnostic in nature 
making no assumption on the actual or effective rank of A.

reduced-rank

show that 

�Ak

consider

a

this

N

estimator

and

if

1 + ··· + σ2

Notations For an n×n PSD matrix A  denote A = UΣU� as its eigenvalue decomposition  where
U is an orthogonal matrix and Σ = diag(σ1 ···   σn) is a diagonal matrix  with eigenvalues sorted in
descending order σ1 ≥ σ2 ≥ ··· ≥ σn ≥ 0. The spectral norm and Frobenius norm of A are deﬁned
as �A�2 = σ1 and �A�F =�σ2
n  respectively. Suppose u1 ···   un are eigenvectors as-
sociated with σ1 ···   σn. Deﬁne Ak =�k
i=1 σiuiu�i = UkΣkU�k   An−k =�n
i=k+1 σiuiu�i =
Un−kΣn−kU�n−k and Am1:m2 =�m2
i=m1+1 σiuiu�i = Um1:m2 Σm1:m2 U�m1:m2. For a tall matrix
U ∈ Rn×k  we use U = Range(U) to denote the linear subspace spanned by the columns of U. For
two linear subspaces U and V  we write W = U⊕V if U∩V = {0} and W = {u+v : u ∈ U  v ∈ V}.
For a sequence of random variables {Xn}∞n=1 and real-valued function f : N → R  we say
Xn = OP(f (n)) if for any � > 0  there exists N ∈ N and C > 0 such that Pr[|Xn| ≥ C ·|f (n)|] ≤ �
for all n ≥ N.
2 Multiplicative Frobenius-norm Approximation and Applications

We ﬁrst state our main result  which shows that truncated SVD on a weak estimator with small
approximation error in spectral norm leads to a strong estimator with multiplicative Frobenius-norm
error bound. We remark that truncated SVD in general has time complexity

O�min�n2k  nnz��A� + npoly (k)��  

where nnz(�A) is the number of non-zero entries in �A  and the time complexity is at most linear in
matrix sizes when k is small. We refer readers to [Allen-Zhu and Li  2016] for details.
Theorem 2.1. Suppose A is an n × n PSD matrix with eigenvalues σ1(A) ≥ ··· ≥ σn(A) ≥ 0 
and a symmetric matrix �A satisﬁes ��A − A�2 ≤ δ = �2σk+1(A) for some � ∈ (0  1/4]. Let Ak and
�Ak be the best rank-k approximations of A and �A. Then
Remark 2.1. Note when � = O(1/√k) we obtain an (1 + O (�)) error bound.
Remark 2.2. This theorem only studies PSD matrices. Using similar arguments in the proof  we
believe similar results for general asymmetric matrices can be obtained as well.

��Ak − A�F ≤ (1 + 32�)�A − Ak�F + 102√2k�2�A − Ak�2.

(2)

3

(3)

Achlioptas and McSherry [2007]  who showed that

To our knowledge  the best existing bound for ��Ak − A�F assuming ��A − A�2 ≤ δ is due to

��Ak − A�F ≤ �A − Ak�F + �(�A − A)k�F + 2��(�A − A)k�F�Ak�F
≤ �A − Ak�F + √kδ�A − Ak�2 + 2k1/4√δ��Ak�F .

Compared to Theorem 2.1  Eq. (3) is not relative because the third term 2k1/4��Ak�F depends on
the k largest eigenvalues of A  which could be much larger than the remainder term �A − Ak�F . In
contrast  Theorem 2.1  together with Remark 2.1  shows that ��Ak − A�F could be upper bounded
by a small factor multiplied with the remainder term �A − Ak�F .
We also provide a gap-dependent version.
Theorem 2.2. Suppose A is an n × n PSD matrix with eigenvalues σ1(A) ≥ ··· ≥ σn(A) ≥ 0 
and a symmetric matrix �A satisﬁes ��A − A�2 ≤ δ = � (σk(A) − σk+1(A)) for some � ∈ (0  1/4].
Let Ak and �Ak be the best rank-k approximations of A and �A. Then
If A is an exact rank-k matrix  Theorem 2.2 implies that truncated SVD gives an �√2kσk error
approximation in Frobenius norm  which has been established by many previous works [Yi et al. 
2016  Tu et al.  2015  Wang et al.  2016].
Before we proceed to the applications and proof of Theorem 2.1  we ﬁrst list several examples of A
with classical distribution of eigenvalues and discuss how Theorem 2.1 could be applied to obatin
good Frobenius-norm approximations of A. We begin with the case where eigenvalues of A have a
polynomial decay rate (i.e.  power law). Such matrices are ubiquitous in practice [Liu et al.  2015].
Corollary 2.1 (Power-law spectral decay). Suppose � ˆA − A�2 ≤ δ for some δ ∈ (0  1/2] and
σj(A) = j−β for some β > 1/2. Set k = �min{C1δ−1/β  n} − 1�. If k ≥ 1 then

��Ak − A�F ≤ �A − Ak�F + 102√2k� (σk(A) − σk+1(A)) .

(4)

where C1  C�1 > 0 are constants that only depend on β.

��Ak − A�F ≤ C�1 · max�δ

2β−1
2β   n− 2β−1

2β �  

We remark that the assumption σj(A) = j−β implies that the eigenvalues lie in an �q ball for
j=1 σj(A)q = O(1). The error bound in Corollary 2.1 matches the minimax
rate (derived by Negahban and Wainwright [2012]) for matrix completion when the spectrum is

q = 1/β; that is �n
constrained in an �q ball  by replacing δ with�n/N where N is the number of observed entries.
Next  we consider the case where eigenvalues satisfy a faster decay rate.
Corollary 2.2 (Exponential spectral decay). Suppose � ˆA − A�2 ≤ δ for some δ ∈ (0  e−16) and
σj(A) = exp{−cj} for some c > 0. Set k = �min{c−1 log(1/δ) − c−1 log log(1/δ)  n} − 1�. If
k ≥ 1 then

��Ak − A�F ≤ C�2 · max�δ�log(1/δ)3  n1/2 exp(−cn)�  

where C�2 > 0 is a constant that only depends on c.

Both corollaries are proved in the appendix. The error bounds in both Corollaries 2.1 and 2.2 are

signiﬁcantly better than the trivial estimate �A  which satisﬁes ��A − A�F ≤ n1/2δ. We also remark

that the bound in Corollary 2.1 cannot be obtained by a direct application of the weaker bound Eq. (3) 
which yields a δ
We next state results that are consequences of Theorem 2.1 in several matrix estimation problems.

2β−1 bound.

β

2.1 High-rank Matrix Completion
Suppose A is a high-rank n × n PSD matrix that satisﬁes µ0-spikeness condition deﬁned as follows:

4

Deﬁnition 2.1 (Spikeness condition). An n × n PSD matrix A satisﬁes µ0-spikeness condition if
n�A�max ≤ µ0�A�F   where �A�max = max1≤i j≤n |Aij| is the max-norm of A.
Spikeness condition makes uniform sampling of matrix entries powerful in matrix completion
problems. If A is exactly low rank  the spikeness condition is implied by an upper bound on
max1≤i≤n �e�i Uk�2  which is the standard incoherence assumption on the top-k space of A [Candes
and Recht  2012]. For general high-rank A  the spikeness condition is implied by a more restrictive
incoherence condition that imposes an upper bound on max1≤i≤n �e�i Un−k�2 and �An−k�max 
which are assumptions adopted in [Hardt and Wootters  2014].
Suppose �A is a symmetric re-scaled zero-ﬁlled matrix of observed entries. That is 

(5)

[�A]ij =� Aij/p  with probability p;

with probability 1 − p;

0 

Here p ∈ (0  1) is a parameter that controls the probability of observing a particular entry in A 
corresponding to a sample complexity of O(n2p). Note that both �A and A are symmetric so we only
specify the upper triangle of �A. By a simple application of matrix Bernstein inequality [Mackey
et al.  2014]  one can show �A is close to A in spectral norm when A satisﬁes µ0-spikeness. Here we
cite a lemma from [Hardt  2014] to formally establish this observation:
Lemma 2.1 (Corollary of [Hardt  2014]  Lemma A.3). Under the model of Eq. (5) and µ0-spikeness
condition of A  for t ∈ (0  1) it holds with probability at least 1 − t that

∀1 ≤ i ≤ j ≤ n.

��A − A�2 ≤ Omax

� µ2

0�A�2
F log(n/t)
np

 

µ0�A�F log(n/t)

np

 .



if

F log(n/t)

0�A�2

F log(n/t)

nσk+1(A)2 � .

0 max{�−4  k2}�A�2
nσk+1(A)2

Let �Ak be the best rank-k approximation of �A in Frobenius/spectral norm. Applying Theorem 2.1
and 2.2 we obatin the following result:
Theorem 2.3. Fix t ∈ (0  1). Then with probability 1 − t we have
p = Ω� µ2
��Ak − A�F ≤ O(√k) · �A − Ak�F
Furthermore  for ﬁxed � ∈ (0  1/4]  with probability 1 − t we have
if p = Ω� µ2
��Ak − A�F ≤ (1 + O(�))�A − Ak�F
��Ak − A�F ≤ �A − Ak�F + � (σk (A) − σk+1 (A)) if p = Ω� µ2

�
n�2 (σk(A) − σk+1(A))2� .
As a remark  because µ0 ≥ 1 and �A�F /σk+1(A) ≥ √k always hold  the sample complexity is
For example  if A has stable rank rs(A) = r then ��Ak − A�F has an O(√k) multiplicative error

lower bounded by Ω(nk log n)  the typical sample complexity in noiseless matrix completion. In
the case of high rank A  the results in Theorem 2.3 are the strongest when A has small stable rank
2 and the top-k condition number γk(A) = σ1(A)/σk+1(A) is not too large.
rs(A) = �A�2
bound with sample complexity Ω(µ2
0γk(A)2 · nr log n); or an (1 + O(�)) relative error bound with
sample complexity Ω(µ2
0 max{�−4  k2}γk(A)2 · nr log n). Finally  when σk+1(A) is very small
and the “gap” σk(A) − σk+1(A) is large  a weaker additive-error bound is applicable with sample
complexity independent of σk+1(A)−1.
Comparing with previous works  if‘ the gap (1 − σk+1/σk) is of order �  then sample complexity
of[Hardt  2014] Theorem 1.2 and [Hardt and Wootters  2014] Theorem 1 scale with 1/�7. Our result
improves their results to the scaling of 1/�4 with a much simpler algorithm (truncated SVD).

F /�A�2

0k�A�2

F log(n/t)

5

We refer the readers to [Gavish and Donoho  2014] for a list of references that shows the ubiquitous
application of matrix de-noising in scientiﬁc ﬁelds.

2.2 High-rank matrix de-noising
Let A be an n× n PSD signal matrix and E a symmetric random Gaussian matrix with zero mean and
i.i.d.∼ N (0  ν2/n) for 1 ≤ i ≤ j ≤ n and Eij = Eji. Deﬁne �A = A + E.
ν2/n variance. That is  Eij
The matrix de-noising problem is then to recover the signal matrix A from noisy observations �A.
It is well-known by concentration results of Gaussian random matrices  that ��A − A�2 = �E�2 =
OP(ν). Let �Ak be the best rank-k approximation of �A in Frobenius/spectral norm. Applying
Theorem 2.1  we immediately have the following result:
Theorem 2.4. There exists an absolute constant c > 0 such that  if ν < c · σk+1(A) for some
1 ≤ k < n  then with probability at least 0.8 we have that

��Ak − A�F ≤�1 + O�� ν

σk+1(A)���A − Ak�F + O(√kν).

(6)

Eq. (6) can be understood from a classical bias-variance tradeoff perspective:
the ﬁrst (1 +
O(�ν/σk+1(A)))�A − Ak�F acts as a bias term  which decreases as we increase cut-off rank k 
corresponding to a more complicated model; on the other hand  the second O(√kν) term acts as the
(square root of) variance  which does not depend on the signal A and increases with k.

2000])  its statistical power in high-dimensional regimes where n and N are comparable are highly

2.3 Low-rank estimation of high-dimensional covariance
Suppose A is an n × n PSD matrix and X1 ···   XN are i.i.d. samples drawn from the multivariate
Gaussian distribution Nn(0  A). The question is to estimate A from samples X1 ···   XN . A
common estimator is the sample covariance �A = 1
i=1 XiX�i . While in low-dimensional
regimes (i.e.  n ﬁxed and N → ∞) the asymptotic efﬁciency of �A is obvious (cf. [Van der Vaart 
non-trivial. Below we cite results by Bunea and Xiao [2015] for estimation error ��A− A�ξ  ξ = 2/F
when n is not too large compared to N:
Lemma 2.2 (Bunea and Xiao [2015]). Suppose n = O(N β) for some β ≥ 0 and let re(A) =
tr(A)/�A�2 denote the effective rank of the covariance A. Then the sample covariance �A =
N�N

i=1 XiX�i satisﬁes

N�N

1

(7)

and

N �
��A − A�F = OP��A�2re(A)� log N
��A − A�2 = OP��A�2 max�� re(A) log(N n)

N

 

re(A) log(N n)

N

�� .

(8)

Let �Ak be the best rank-k approximation of �A in Frobenius/spectral norm. Applying Theorem 2.1
and 2.2 together with Eq. (8)  we immediately arrive at the following theorem.
Theorem 2.5. Fix � ∈ (0  1/4] and 1 ≤ k < n. Recall that re(A) = tr(A)/�A�2 and γk(A) =
σ1(A)/σk+1(A). There exists a universal constant c > 0 such that  if

re(A) max{�−4  k2}γk(A)2 log(N )

N

≤ c

then with probability at least 0.8 

and if

��Ak − A�F ≤ (1 + O(�))�A − Ak�F
N �2 (σk (A) − σk+1 (A))2 ≤ c

re(A)k�A�2

2 log(N )

6

then with probability at least 0.8 

��Ak − A�F ≤ �A − Ak�F + � (σk (A) − σk+1 (A)) .

Theorem 2.5 shows that it is possible to obtain a reasonable Frobenius-norm approximation of �A

by truncated SVD in the asymptotic regime of N = Ω(re(A)poly(k) log N )  which is much more
ﬂexible than Eq. (7) that requires N = Ω(re(A)2 log N ).

3 Proof Sketch of Theorem 2.1

In this section we give a proof sketch of Theorem 2.1. The proof of Theorem 2.2 is similar and less
challenging so we defer it to appendix. We defer proofs of technical lemmas to Section A.

Because both �Ak and Ak are low-rank  ��Ak − Ak�F is upper bounded by an O(√k) factor of
��Ak − Ak�2. From the condition that ��A − A�2 ≤ δ  a straightforward approach to upper
bound ��Ak − Ak�2 is to consider the decomposition ��Ak − Ak�2 ≤ ��A − A�2 + 2�UkU�k −
�Uk�U�k �2��Ak�2  where UkU�k and �Uk�U�k are projection operators onto the top-k eigenspaces
of A and �A  respectively. Such a naive approach  however  has two major disadvantages. First 
the upper bound depends on ��Ak�2  which is additive and may be much larger than ��A − A�2.
Perhaps more importantly  the quantity �UkU�k − �Uk�U�k �2 depends on the “consecutive” sepctral
gap (σk(A) − σk+1(A))  which could be very small for large matrices.
The key idea in the proof of Theorem 2.1 is to ﬁnd an “envelope” m1 ≤ k ≤ m2 in the spectrum of
A surrounding k  such that the eigenvalues within the envelope are relatively close. Deﬁne

m1 = argmax0≤j≤k{σj(A) ≥ (1 + 2�)σk+1(A)};
m2 = argmaxk≤j≤n{σj(A) ≥ σk(A) − 2�σk+1(A)} 

W = argmaxdim(W)=k−m1 W∈Um1 :m2

σk−m1�W��Uk� .

Now deﬁne

inequality we can obtain the following result.

where we let σ0 (A) = ∞ for convenience. Let Um  �Um be basis of the top m-dimensional linear
subspaces of A and �A  respectively. Also denote Un−m and �Un−m as basis of the orthogonal
complement of Um and �Um. By asymmetric Davis-Kahan inequality (Lemma C.1) and Wely’s
Lemma 3.1. If ��A − A�2 ≤ �2σk+1(A) for � ∈ (0  1) then ��U�n−kUm1�2 ��U�k Un−m2�2 ≤ �.
Let Um1:m2 be the linear subspace of A associated with eigenvalues σm1+1(A) ···   σm2 (A).
Intuitively  we choose a (k − m1)-dimensional linear subspace in Um1:m2 that is “most aligned” with
the top-k subspace �Uk of �A. Formally  deﬁne
W is then a d × (k − m1) matrix with orthonormal columns that corresponds to a basis of W. W is
carefully constructed so that it is closely aligned with �Uk  yet still lies in Uk. In particular  Lemma
3.2 shows that sin ∠(W �Uk) = ��U�n−kW�2 is upper bounded by �.
Lemma 3.2. If ��A − A�2 ≤ �2σk+1(A) for � ∈ (0  1) then ��U�n−kW�2 ≤ �.
�A = Am1 + WW�AWW�.
We use �A as the “reference matrix" because we can decompose ��Ak − A�F as
�Ak and �A have rank at most k. The following lemma bounds the ﬁrst term.
Lemma 3.3. If ��A− A�2 ≤ �2σk+1(A)2 for � ∈ (0  1/4] then �A−�A�F ≤ (1 + 32�)�A− Ak�F .

��Ak − A�F ≤ �A − �A�F + ��Ak − �A�F ≤ �A − �A�F + √2k��Ak − �A�2

(9)
and bound each term on the right hand side separately. Here the last inequality holds because both

7

v = �Uk�U�k v + �U�U��Un−k�U�n−kv
= �U�U�v + �Uk�U�k �U⊥�U�

⊥v.

(10)
(11)

The proof of this lemma relies Pythagorean theorem and Poincaré separation theorem. Let Um1:m2
be the (m2 − m1)-dimensional linear subspace such that Um2 = Um1 ⊕ Um1:m2. Deﬁne Am1:m2 =
Um1:m2 Σm1:m2 U�m1:m2  where Σm1:m2 = diag(σm1+1(A) ···   σm2 (A)) and Um1:m2 is an or-
thonormal basis associated with Um1:m2. Applying Pythagorean theorem (Lemma C.2)  we can
decompose

F + �Am1:m2�2

�A − �A�2
Applying Poincaré separation theorem (Lemma C.3) where X = Σm1:m2 and P = U�m1:m2W  we
have �W�Am1:m2W�2
j=m1+m2−k+1 σj(A)2. With some
routine algebra we can prove Lemma 3.3.
To bound the second term of Eq. (9) we use the following lemma.

j=m2−k+1 σj(Am1:m2 )2 = �m2

F = �A − Am2�2
F ≥ �m2−m1

F − �WW�Am1:m2WW��2
F .

Lemma 3.4. If ��A − A�2 ≤ �2σk+1(A) for � ∈ (0  1/4] then ��Ak − �A�2 ≤ 102�2�A − Ak�2.
The proof of Lemma 3.4 relies on the low-rankness of �Ak and �A. Recall the deﬁnition that
�U = Range(�A) and �U⊥ = Null(�A). Consider �v�2 = 1 such that v�(�Ak − �A)v = ��Ak − �A�2.
Because v maximizes v�(�Ak−�A)v over all unit-length vectors  it must lie in the range of��Ak − �A�
that v = v1 + v2 where v1 ∈ Range(�Ak) = �Uk and v2 ∈ Range(�A) = �U. Subsequently  we have

because otherwise the component outside the range will not contribute. Therefore  we can choose v

that

Consider the following decomposition:

���v�(�Ak − �A)v��� ≤���v�(�A − A)v��� +���v�(�Ak − �A)v��� +���v�(A − �A)v��� .

The ﬁrst term |v�(�A − A)v| is trivially upper bounded by ��A − A�2 ≤ �2σk+1(A). The second
and the third term can be bounded by Wely’s inequality (Lemma C.4) and basic properties of �A

(Lemma A.3). See Section A for details.

4 Discussion

We mention two potential directions to further extend results of this paper.

4.1 Model selection for general high-rank matrices

The validity of Theorem 2.1 depends on the condition ��A − A�2 ≤ �2σk+1(A)  which could be

hard to verify if σk+1(A) is unknown and difﬁcult to estimate. Furthermore  for general high-rank
matrices  the model selection problem of determining an appropriate (or even optimal) cut-off rank k
requires knowledge of the distribution of the entire spectrum of an unknown data matrix  which is
even more challenging to obtain.
One potential approach is to impose a parametric pattern of decay of the eigenvalues (e.g.  polynomial
and exponential decay)  and to estimate a small set of parameters (e.g.  degree of polynomial) from

analysis  similar to the examples in Corollaries 2.1 and 2.2. Another possibility is to use repeated
sampling techniques such as boostrap in a stochastic problem (e.g.  matrix de-noising) to estimate the

the noisy observations �A. Afterwards  the optimal cut-off rank k could be determined by a theoretical
“bias” term �A − Ak�F for different k  as the variance term √kν is known or easy to estimate.
4.2 Minimax rates for polynomial spectral decay

Consider the class of PSD matrices whose eigenvalues follow a polynomial (power-law) decay:
Θ(β  n) = {A ∈ Rn×n : A � 0  σj(A) = j−β}. We are interested in the following minimax rates
for completing or de-noising matrices in Θ(β  n):

8

Question 1 (Completion of Θ(β  n)). Fix n ∈ N  p ∈ (0  1) and deﬁne N = pn2. For M ∈ Θ(β  n) 
let �Aij = Mij with probability p and �Aij = 0 with probability 1 − p. Also let Λ(µ0  n) = {M ∈
Rn×n : n�M�max ≤ µ0�M�F} be the class of all non-spiky matrices. Determine
E��M − M�2
Question 2 (De-noising of Θ(β  n)). Fix n ∈ N  ν > 0 and let �A = M + ν/√nZ  where Z is a

symmetric matrices with i.i.d. standard Normal random variables on its upper triangle. Determine

R1(µ0  β  n  N ) := inf

M∈Θ(β n)∩Λ(µ0 n)

�A�→�M

sup

F .

R2(ν  β  n) := inf

sup

M∈Θ(β n)

�A�→�M

F .

E��M − M�2

Compared to existing settings on matrix completion and de-noising  we believe Θ(β  n) is a more
natural matrix class which allows for general high-rank matrices  but also imposes sufﬁcient spectral
decay conditions so that spectrum truncation algorithms result in signiﬁcant beneﬁts. Based on
Corollary 2.1 and its matching lower bounds for a larger �p class [Negahban and Wainwright  2012] 
we make the following conjecture:
Conjecture 4.1. For β > 1/2 and ν not too small  we conjecture that

R1(µ0  β  n  N ) � C(µ0) ·� n

N� 2β−1

2β

where C(µ0) > 0 is a constant that depends only on µ0.

and

R2(ν  β  n) ��ν2� 2β−1

2β

 

5 Acknowledgements

S.S.D. was supported by ARPA-E Terra program. Y.W. and A.S. were supported by the NSF CAREER
grant IIS-1252412.

References
Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix approximations.

Journal of the ACM  54(2):9  2007.

Zeyuan Allen-Zhu and Yuanzhi Li. Even faster svd decomposition yet without agonizing pain. In

Advances in Neural Information Processing Systems  pages 974–982  2016.

David Anderson  Simon Du  Michael Mahoney  Christopher Melgaard  Kunming Wu  and Ming Gu.
Spectral gap error bounds for improving cur matrix decomposition and the nyström method. In
Artiﬁcial Intelligence and Statistics  pages 19–27  2015.

Maria Florina Balcan  Simon S Du  Yining Wang  and Adams Wei Yu. An improved gap-dependency
analysis of the noisy power method. In 29th Annual Conference on Learning Theory  pages
284–309  2016.

Florentina Bunea and Luo Xiao. On the sample covariance matrix estimator of reduced effective rank

population matrices  with applications to fpca. Bernoulli  21(2):1200–1230  2015.

T Tony Cai and Harrison H Zhou. Optimal rates of convergence for sparse covariance matrix

estimation. The Annals of Statistics  40(5):2389–2420  2012.

T Tony Cai  Cun-Hui Zhang  and Harrison H Zhou. Optimal rates of convergence for covariance

matrix estimation. The Annals of Statistics  38(4):2118–2144  2010.

T Tony Cai  Zhao Ren  and Harrison H Zhou. Optimal rates of convergence for estimating toeplitz

covariance matrices. Probability Theory and Related Fields  156(1-2):101–143  2013.

T Tony Cai  Zhao Ren  and Harrison H Zhou. Estimating structured high-dimensional covariance and
precision matrices: Optimal rates and adaptive estimation. Electronic Journal of Statistics  10(1):
1–59  2016.

9

Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. Commu-

nications of the ACM  55(6):111–119  2012.

Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):

925–936  2010.

Sourav Chatterjee et al. Matrix estimation by universal singular value thresholding. The Annals of

Statistics  43(1):177–214  2015.

Yudong Chen  Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Matrix completion with
column manipulation: Near-optimal sample-robustness-rank tradeoffs. IEEE Transactions on
Information Theory  62(1):503–526  2016.

David Donoho and Matan Gavish. Minimax risk of matrix denoising by singular value thresholding.

The Annals of Statistics  42(6):2413–2440  2014.

David L Donoho  Matan Gavish  and Andrea Montanari. The phase transition of matrix recovery
from gaussian measurements matches the minimax mse of matrix denoising. Proceedings of the
National Academy of Sciences  110(21):8405–8410  2013.

Brian Eriksson  Laura Balzano  and Robert D Nowak. High-rank matrix completion. In AISTATS 

pages 373–381  2012.

Matan Gavish and David L Donoho. The optimal hard threshold for singular values is 4/√3. IEEE

Transactions on Information Theory  60(8):5040–5053  2014.

Moritz Hardt. Understanding alternating minimization for matrix completion. In Foundations of
Computer Science (FOCS)  2014 IEEE 55th Annual Symposium on  pages 651–660. IEEE  2014.

Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. In COLT 

pages 638–678  2014.

Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion using alternating
minimization. In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing 
pages 665–674. ACM  2013.

Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from a few

entries. Information Theory  IEEE Transactions on  56(6):2980–2998  2010.

Alois Kneip and Pascal Sarda. Factor models and variable selection in high-dimensional regression

analysis. The Annals of Statistics  pages 2410–2447  2011.

Vladimir Koltchinskii  Karim Lounici  and Alexandre B Tsybakov. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics  pages 2302–2329 
2011.

Ziqi Liu  Yu-Xiang Wang  and Alexander Smola. Fast differentially private matrix factorization. In
Proceedings of the 9th ACM Conference on Recommender Systems  pages 171–178. ACM  2015.

Lester Mackey  Michael I Jordan  Richard Y Chen  Brendan Farrell  and Joel A Tropp. Matrix
concentration inequalities via the method of exchangeable pairs. The Annals of Probability  42(3):
906–945  2014.

Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix
completion: Optimal bounds with noise. The Journal of Machine Learning Research  13(1):
1665–1697  2012.

Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. IEEE

Transactions on Information Theory  62(11):6535–6579  2016.

Stephen Tu  Ross Boczar  Max Simchowitz  Mahdi Soltanolkotabi  and Benjamin Recht. Low-rank
solutions of linear matrix equations via procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.

Aad W Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press  2000.

10

Lingxiao Wang  Xiao Zhang  and Quanquan Gu. A uniﬁed computational and statistical framework

for nonconvex low-rank matrix estimation. arXiv preprint arXiv:1610.05275  2016.

Xinyang Yi  Dohyung Park  Yudong Chen  and Constantine Caramanis. Fast algorithms for robust pca
via gradient descent. In Advances in Neural Information Processing Systems  pages 4152–4160 
2016.

Lijun Zhang  Tianbao Yang  Rong Jin  and Zhi-Hua Zhou. Analysis of nuclear norm regularization

for full-rank matrix completion. arXiv preprint arXiv:1504.06817  2015.

11

,Simon Du
Yining Wang
Aarti Singh