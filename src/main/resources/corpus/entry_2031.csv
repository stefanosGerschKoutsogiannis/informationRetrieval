2008,Fast Rates for Regularized Objectives,We show that the empirical minimizer of a stochastic strongly convex objective  where the stochastic component is linear  converges to the population minimizer with rate $O(1/n)$. The result applies  in particular  to the SVM objective. Thus  we get a rate of $O(1/n)$ on the convergence of the SVM objective to its infinite data limit. We demonstrate how this is essential for obtaining tight oracle inequalities for SVMs. The results extend also to strong convexity with respect to other $\ellnorm_p$ norms  and so also to objectives regularized using other norms.,Fast Rates for Regularized Objectives

Karthik Sridharan  Nathan Srebro  Shai Shalev-Shwartz

Toyota Technological Institute — Chicago

Abstract

We study convergence properties of empirical minimization of a stochastic
strongly convex objective  where the stochastic component is linear. We show
that the value attained by the empirical minimizer converges to the optimal value
with rate 1/n. The result applies  in particular  to the SVM objective. Thus  we
obtain a rate of 1/n on the convergence of the SVM objective (with ﬁxed regular-
ization parameter) to its inﬁnite data limit. We demonstrate how this is essential
for obtaining certain type of oracle inequalities for SVMs. The results extend
also to approximate minimization as well as to strong convexity with respect to an
arbitrary norm  and so also to objectives regularized using other (cid:96)p norms.

1 Introduction

We consider the problem of (approximately) minimizing a stochastic objective

F (w) = Eθ [f(w; θ)]

(1)
where the optimization is with respect to w ∈ W  based on an i.i.d. sample θ1  . . .   θn. We focus
on problems where f(w; θ) has a generalized linear form:

f(w; θ) = (cid:96)((cid:104)w  φ(θ)(cid:105)  θ) + r(w) .

(2)
The relevant special case is regularized linear prediction  where θ = (x  y)  (cid:96)((cid:104)w  φ(x)(cid:105)  y) is the
loss of predicting (cid:104)w  φ(x)(cid:105) when the true target is y  and r(w) is a regularizer.
It is well known that when the domain W and the mapping φ(·) are bounded  and the function
(cid:96)(z; θ) is Lipschitz continuous in z  the empirical averages

converge uniformly to their expectations F (w) with rate(cid:112)1/n. This justiﬁes using the empirical

ˆF (w) = ˆE [f(w; θ)] = 1

f(w; θi)

(3)

i=1

n(cid:88)

n

ˆw = arg min
w∈W

ˆF (w) 

F (w(cid:63)) = min
w∈W

F (w)

and we can then establish convergence of F ( ˆw) to the population optimum

minimizer

with a rate of(cid:112)1/n.

(4)

(5)

Recently  Hazan et al [1] studied an online analogue to this problem  and established that if
f(w; θ) is strongly convex in w  the average online regret diminishes with a much faster rate 
namely (log n)/n. The function f(w; θ) becomes strongly convex when  for example  we have
r(w) = λ
In this paper we present an analogous “fast rate” for empirical minimization of a strongly convex
stochastic objective. In fact  we do not need to assume that we perform the empirical minimization

2 (cid:107)w(cid:107)2 as in SVMs and other regularized learning settings.

1

exactly: we provide uniform (over all w ∈ W) guarantees on the population sub-optimality F (w)−
F (w(cid:63)) in terms of the empirical sub-optimality ˆF (w)− ˆF ( ˆw) with a rate of 1/n. This is a stronger
type of result than what can be obtained with an online-to-batch conversion  as it applies to any
possible solution w  and not only to some speciﬁc algorithmically deﬁned solution. For example  it
can be used to analyze the performance of approximate minimizers obtained through approximate
optimization techniques. Speciﬁcally  consider f(w; θ) as in (2)  where (cid:96)(z; θ) is convex and L-
Lipschitz in z  the norm of φ(θ) is bounded by B  and r is λ-strongly convex. We show that for any
a > 0 and δ > 0  with probability at least 1 − δ  for all w (of arbitrary magnitude):

(cid:18)
(1 + 1/a) L2B2(log(1/δ))

(cid:19)

λn

F (w) − F (w(cid:63)) ≤ (1 + a)( ˆF (w) − ˆF ( ˆw)) + O

.

(6)

We emphasize that here and throughout the paper the big-O notation hides only ﬁxed numeric con-
stants.
It might not be surprising that requiring strong convexity yields a rate of 1/n. Indeed  the connection
between strong convexity  variance bounds  and rates of 1/n  is well known. However  it is interest-
ing to note the generality of the result here  and the simplicity of the conditions. In particular  we do
not require any “low noise” conditions  nor that the loss function is strongly convex (it need only be
weakly convex).
In particular  (6) applies  under no additional conditions  to the SVM objective. We therefore obtain
convergence with a rate of 1/n for the SVM objective. This 1/n rate on the SVM objective is
always valid  and does not depend on any low-noise conditions or on speciﬁc properties of the
√
kernel function. Such a “fast” rate might seem surprising at a ﬁrst glance to the reader familiar with
√
the 1/
n rate on the expected loss of the SVM optimum. There is no contradiction here—what we
establish is that although the loss might converge at a rate of 1/
n  the SVM objective (regularized
loss) always converges at a rate of 1/n.
√
In fact  in Section 3 we see how a rate of 1/n on the objective corresponds to a rate of 1/
n on the
loss. Speciﬁcally  we perform an oracle analysis of the optimum of the SVM objective (rather than
of empirical minimization subject to a norm constraint  as in other oracle analyses of regularized
linear learning)  based on the existence of some (unknown) low-norm  low-error predictor w.
Strong convexity is a concept that depends on a choice of norm. We state our results in a general
form  for any choice of norm (cid:107)·(cid:107). Strong convexity of r(w) must hold with respect to the chosen
norm (cid:107)·(cid:107)  and the data φ(θ) must be bounded with respect to the dual norm (cid:107)·(cid:107)∗  i.e. we must
have (cid:107)φ(θ)(cid:107)∗ ≤ B. This allows us to apply our results also to more general forms of regularizers 
p  for p < 1 ≤ 2 (see Corollary 2). However 
including squared (cid:96)p norm regularizers  r(w) = λ
the reader may choose to read the paper always thinking of the norm (cid:107)w(cid:107)  and so also its dual norm
(cid:107)w(cid:107)∗  as the standard (cid:96)2-norm.

2 (cid:107)w(cid:107)2

2 Main Result
We consider a generalized linear function f : W × Θ → R  that can be written as in (2)  deﬁned
over a closed convex subset W of a Banach space equipped with norm (cid:107)·(cid:107).
Lipschitz continuity and boundedness We require that the mapping φ(·) is bounded by B 
i.e. (cid:107)φ(θ)(cid:107)∗ ≤ B  and that the function (cid:96)(z; θ) is L-Lipschitz in z ∈ R for every θ.
Strong Convexity We require that F (w) is λ-strongly convex w.r.t. the norm (cid:107)w(cid:107). That is  for all
w1  w2 ∈ W and α ∈ [0  1] we have:

F (αw1 + (1 − α)w2) ≤ αF (w1) + (1 − α)F (w2) − λ

2 α(1 − α)(cid:107)w1 − w2(cid:107)2 .

Recalling that w(cid:63) = arg minw F (w)  this ensures (see for example [2  Lemma 13]):

(7)
We require only that the expectation F (w) = E [f(w; θ)] is strongly convex. Of course  requiring
that f(w; θ) is λ-strongly convex for all θ (with respect to w) is enough to ensure the condition.

F (w) ≥ F (w(cid:63)) + λ

2 (cid:107)w − w(cid:63)(cid:107)2

2

In particular  for a generalized linear function of the form (2) it is enough to require that (cid:96)(z; y) is
convex in z and that r(w) is λ-strongly convex (w.r.t. the norm (cid:107)w(cid:107)).
We now provide a faster convergence rate using the above conditions.
Theorem 1. Let W be a closed convex subset of a Banach space with norm (cid:107)·(cid:107) and dual norm (cid:107)·(cid:107)∗
and consider f(w; θ) = (cid:96)((cid:104)w  φ(θ)(cid:105); θ) + r(w) satisfying the Lipschitz continuity  boundedness 
and strong convexity requirements with parameters B  L  and λ. Let w(cid:63)  ˆw  F (w) and ˆF (w) be as
deﬁned in (1)-(5). Then  for any δ > 0 and any a > 0  with probability at least 1 − δ over a sample
of size n  we have that for all w ∈ W:

(where [x]+ = max(x  0))

F (w) − F (w(cid:63)) ≤ (1 + a)[ ˆF (w) − ˆF (w(cid:63))]+ +
≤ (1 + a)( ˆF (w) − ˆF ( ˆw)) +

8 (1 + 1

a)L2B2(32 + log(1/δ))

λn

8 (1 + 1

a)L2B2(32 + log(1/δ))

λn

.

p  which are (p−1)λ-
It is particularly interesting to consider regularizers of the form r(w) = λ
strongly convex w.r.t. the corresponding (cid:96)p-norm [2]. Applying Theorem 1 to this case yields the
following bound:
Corollary 2. Consider an (cid:96)p norm and its dual (cid:96)q  with 1 < p ≤ 2  1
p = 1  and the objective
f(w; θ) = (cid:96)((cid:104)w  φ(θ)(cid:105); θ) + λ
p  where (cid:107)φ(θ)(cid:107)q ≤ B and (cid:96)(z; y) is convex and L-Lipschitz
in z. The domain is the entire Banach space W = (cid:96)p. Then  for any δ > 0 and any a > 0 
with probability at least 1 − δ over a sample of size n  we have that for all w ∈ W = (cid:96)p (of any
magnitude):

2 (cid:107)w(cid:107)2

2 (cid:107)w(cid:107)2

q + 1

F (w) − F (w(cid:63)) ≤ (1 + a)( ˆF (w) − ˆF ( ˆw)) + O

(cid:18)(1 + 1

(cid:19)

.

a)L2B2 log(1/δ)
(p − 1)λn

Corollary 2 allows us to analyze the rate of convergence of the regularized risk for (cid:96)p-regularized
linear learning. That is  training by minimizing the empirical average of:

p

(cid:107)w(cid:107)2

f(w; x  y) = (cid:96)((cid:104)w  x(cid:105)  y) + λ
2

(8)
where (cid:96)(z  y) is some convex loss function and (cid:107)x(cid:107)q ≤ B. For example  in SVMs we use the (cid:96)2
norm  and so bound (cid:107)x(cid:107)2 ≤ B  and the hinge loss (cid:96)(z  y) = [1 − yz]+  which is 1-Lipschitz. What
we obtain is a bound on how quickly we can minimize the expectation F (w) = E [(cid:96)((cid:104)w  x(cid:105)  y)] +
2 (cid:107)w(cid:107)2
p  i.e. the regularized empirical loss  or in other words  how quickly do we converge to the
λ
inﬁnite-data optimum of the objective.
We see  then  that the SVM objective converges to its optimum value at a fast rate of 1/n  without
any special assumptions. This still doesn’t mean that the expected loss L( ˆw) = E [(cid:96)((cid:104) ˆw  x(cid:105)  y)]
converges at this rate. This behavior is empirically demonstrated on the left plot of Figure 1. For
each data set size we plot the excess expected loss L( ˆw) − L(w(cid:63)) and the sub-optimality of the
2 (cid:107) ˆw(cid:107)2). Although the
regularized expected loss F ( ˆw) − F (w(cid:63)) (recall that F ( ˆw) = L( ˆw) + λ
regularized expected loss converges to its inﬁnite data limit  i.e. to the population minimizer  with

rate roughly 1/n  the expected loss L( ˆw) converges at a slower rate of roughly(cid:112)1/n.

Studying the convergence rate of the SVM objective allows us to better understand and appreci-
ate analysis of computational optimization approaches for this objective  as well as obtain oracle
inequalities on the generalization loss of ˆw  as we do in the following Section.
Before moving on  we brieﬂy provide an example of applying Theorem 1 with respect to the (cid:96)1-
norm. The bound in Corollary 2 diverges when p → 1 and the Corollary is not applicable for
(cid:96)1 regularization. This is because (cid:107)w(cid:107)2
1 is not strongly convex w.r.t. the (cid:96)1-norm. An example of a
regularizer that is strongly convex with respect to the (cid:96)1 norm is the (unnormalized) entropy regu-
w-strongly convex w.r.t. (cid:107)w(cid:107)1  as
long as (cid:107)w(cid:107)1 ≤ Bw (see [2])  yielding:
i=1 |wi| log(|wi|)  where
(cid:107)φ(θ)(cid:107)∞ ≤ B and (cid:96)(z; y) is convex and L-Lipschitz in z. Take the domain to be the (cid:96)1 ball

larizer [3]: r(w) =(cid:80)d
Corollary 3. Consider a function f(w; θ) = (cid:96)((cid:104)w  φ(θ)(cid:105); θ) + (cid:80)d

i=1 |wi| log(|wi|). This regularizer is 1/B2

3

minw L(wo)  relative to the overall optimal wo = arg minw L(w)  with λn =(cid:112)300/n. Both plots are on a

Figure 1: Left: Excess expected loss L( ˆw) − L(w(cid:63)) and sub-optimality of the regularized expected loss
F ( ˆw) − F (w(cid:63)) as a function of training set size  for a ﬁxed λ = 0.8. Right: Excess expected loss L( ˆwλ) −
logarithmic scale and refer to a synthetic example with x uniform over [−1.5  1.5]300  and y = sign x1 when
|x1| > 1 but uniform otherwise.

W = {w ∈ Rd : (cid:107)w(cid:107)1 ≤ Bw}. Then  for any δ > 0 and any a > 0  with probability at least 1 − δ
over a sample of size n  we have that for all w ∈ W:

(cid:18)(1 + 1

(cid:19)

.

a)L2B2B2

w log(1/δ)

λn

F (w) − F (w(cid:63)) ≤ (1 + a)( ˆF (w) − ˆF ( ˆw)) + O

3 Oracle Inequalities for SVMs

In this Section we apply the results from previous Section to obtain an oracle inequality on the
expected loss L(w) = E [(cid:96)((cid:104)w  x(cid:105)  y)] of an approximate minimizer of the SVM training objective
ˆFλ(w) = ˆE [fλ(w)] where

(cid:107)w(cid:107)2  

fλ(w; x  y) = (cid:96)((cid:104)w  x(cid:105)  y) + λ
2

(9)
and (cid:96)(z  y) is the hinge-loss  or any other 1-Lipschitz loss function. As before we denote B =
supx (cid:107)x(cid:107) (all norms in this Section are (cid:96)2 norms).
We assume  as an oracle assumption  that there exists a good predictor wo with low norm (cid:107)wo(cid:107)
and which attains low expected loss L(wo). Consider an optimization algorithm for ˆFλ(w) that is
guaranteed to ﬁnd ˜w such that ˆFλ( ˜w) ≤ min ˆFλ(w) + opt. Using the results of Section 2  we can
translate this approximate optimality of the empirical objective to an approximate optimality of the
expected objective Fλ(w) = E [fλ(w)]. Speciﬁcally  applying Corollary 2 with a = 1 we have that
with probability at least 1 − δ:

(cid:19)

(cid:18) B2 log(1/δ)
(cid:18) B2 log(1/δ)
(cid:19)

λn

.

λn

Fλ( ˜w) − Fλ(w(cid:63)) ≤ 2opt + O

Optimizing to within opt = O( B2

λn ) is then enough to ensure

Fλ( ˜w) − Fλ(w(cid:63)) = O

.

(10)

(11)

In order to translate this to a bound on the expected loss L( ˜w) we consider the following decompo-
sition:

L( ˜w) = L(wo) + (Fλ( ˜w) − Fλ(w(cid:63))) + (Fλ(w(cid:63)) − Fλ(wo)) + λ

2 (cid:107)wo(cid:107)2 − λ

2 (cid:107) ˜w(cid:107)2

(cid:18) B2 log(1/δ)

(cid:19)

λn

≤ L(wo) + O

+ 0 + λ

2 (cid:107)wo(cid:107)2

4

(12)

10310410−210−1n  Suboptimality of ObjectiveExcess Expected Loss10310410−210−1nwhere we used the bound (11) to bound the second term  the optimality of w(cid:63) to ensure the third
term is non-positive  and we also dropped the last  non-positive  term.
This might seem like a rate of 1/n on the generalization error  but we need to choose λ so as to
balance the second and third terms. The optimal choice for λ is

λ(n) = c

 

(13)

B(cid:112)log(1/δ)
(cid:107)wo(cid:107)√

n

for some constant c. We can now formally state our oracle inequality  which is obtained by substi-
tuting (13) into (12):
Corollary 4. Consider an SVM-type objective as in (9). For any wo and any δ > 0  with probability
at least 1−δ over a sample of size n  we have that for all ˜w s.t. ˆFλ(n)( ˜w) ≤ min ˆFλ(n)(w)+O( B2
λn ) 
where λ(n) chosen as in (13)  the following holds:

(cid:115)

L( ˜w) ≤ L(wo) + O

B2 (cid:107)wo(cid:107)2 log(1/δ)

n



Corollary 4 is demonstrated empirically on the right plot of Figure 1.
The way we set λ(n) in Corollary 4 depends on (cid:107)wo(cid:107). However  using

λ(n) = B(cid:112)log(1/δ)

√
n

(14)

we obtain:
Corollary 5. Consider an SVM-type objective as in (9) with λ(n) set as in (14). For any δ > 0 
with probability at least 1 − δ over a sample of size n  we have that for all ˜w s.t. ˆFλ(n)( ˜w) ≤
min ˆFλ(n)(w) + O( B2

λn )  the following holds:

L(wo) + O

(cid:115)

L( ˜w) ≤ inf

wo

B2((cid:107)wo(cid:107)4 + 1) log(1/δ)

n



The price we pay here is that the bound of Corollary 5 is larger by a factor of (cid:107)wo(cid:107) relative to the

bound of Corollary 4. Nevertheless  this bound allows us to converge with a rate of(cid:112)1/n to the

expected loss of any ﬁxed predictor.
It is interesting to repeat the analysis of this Section using the more standard result:

(cid:32)(cid:114)

(cid:33)

Fλ(w) − Fλ(w(cid:63)) ≤ ˆFλ(w) − ˆFλ(w(cid:63)) + O

for (cid:107)w(cid:107) ≤ Bw where we ignore the dependence on δ. Setting Bw =(cid:112)2/λ  as this is a bound on

the norm of both the empirical and population optimums  and using (15) instead of Corollary 2 in
our analysis yields the oracle inequality:

wB2
B2
n

(15)

(cid:32)

(cid:33)1/3

L( ˜w) ≤ L(wo) + O

B2 (cid:107)wo(cid:107)2 log(1/δ)

n

(16)

The oracle analysis studied here is very simple—our oracle assumption involves only a single
predictor wo  and we make no assumptions about the kernel or the noise. We note that a more
√
sophisticated analysis has been carried out by Steinwart et al [4]  who showed that rates faster than
1/
n are possible under certain conditions on noise and complexity of kernel class. In Steinwart’s
et al analyses the estimation rates (i.e. rates for expected regularized risk) are given in terms of
2(cid:107)w(cid:63)(cid:107)2 + L(w(cid:63)) − L∗ where L∗ is the Bayes risk. In our re-
the approximation error quantity λ
sult we consider the estimation rate for regularized objective independent of the approximation error.

5

4 Proof of Main Result

(cid:110)

Gr =

To prove Theorem 1 we use techniques of reweighing and peeling following Bartlett et al [5].
For each w  we deﬁne gw(θ) = f(w; θ) − f(w(cid:63); θ)  and so our goal is to bound the expectation of
gw in terms of its empirical average. We denote by G = {gw|w ∈ W}.
Since our desired bound is not exactly uniform  and we would like to pay different attention to func-
tions depending on their expected sub-optimality  we will instead consider the following reweighted
class. For any r > 0 deﬁne

4k(w) : w ∈ W  k(w) = min{k(cid:48) ∈ Z+ : E [gw] ≤ r4k(cid:48)}(cid:111)

(17)
w ∈ Gr is just a scaled version of
where Z+ is the set of non-negative integers. In other words  gr
gw ∈ G and the scaling factor ensures that E [gr
We will begin by bounding the variation between expected and empirical average values of gr ∈ Gr.
This is typically done in terms of the complexity of the class Gr. However  we will instead use the
complexity of a slightly different class of functions  which ignores the non-random (i.e. non-data-
dependent) regularization terms r(w). Deﬁne:

4k(w) : w ∈ W  k(w) = min{k(cid:48) ∈ Z+ : E [gw] ≤ r4k(cid:48)}(cid:111)

w = hw
hr

w] ≤ r.

w = gw
gr

Hr =

(cid:110)

(18)

where

hw(θ) = gw(θ) − (r(w) − r(w(cid:63))) = (cid:96)((cid:104)w  φ(θ)(cid:105); θ) − (cid:96)((cid:104)w∗  φ(θ)(cid:105); θ).

w(θ) is the data dependent component of gr
w] = E [hr

(19)
w  dropping the (scaled) regularization terms.
That is  hr
w]− ˆE [hr
With this deﬁnition we have E [gr
w] (the regularization terms on the left
hand side cancel out)  and so it is enough to bound the deviation of the empirical means in Hr. This
can be done in terms of the Rademacher Complexity of the class  R(Hr) [6  Theorem 5]: For any
(cid:32)
δ > 0  with probability at least 1 − δ 

w]− ˆE [gr

(cid:33)(cid:113) log 1/δ

2n .

(20)

j=0

6

E [hr] − ˆE [hr] ≤ 2R(Hr) +

sup
hr∈Hr

|hr(θ)|

sup

hr∈Hr θ

We will now proceed to bounding the two terms on the right hand side:
Lemma 6.

sup

|hr(θ)| ≤ LB(cid:112)2r / λ

hr∈Hr θ

Proof. From the deﬁnition of hr
bound (cid:107)φ(θ)(cid:107)∗ ≤ B  we have for all w  θ:

w given in (18)–(19)  the Lipschitz continuity of (cid:96)(·; θ)  and the

w(θ)| ≤ |hw(θ)|
|hr

4k(w) ≤ LB (cid:107)w − w(cid:63)(cid:107) /4k(w)

(cid:107)w − w(cid:63)(cid:107) ≤(cid:113) 2

(21)
We now use the strong convexity of F (w)  and in particular eq. (7)  as well as the deﬁnitions of gw
and k(w)  and ﬁnally note that 4k(w) ≥ 1  to get:
λ(F (w) − F (w(cid:63))) =
Substituting (22) in (21) yields the desired bound.
Lemma 7. R(Hr) ≤ 2L B
Proof. We will use the following generic bound on the Rademacher complexity of linear function-
als [7  Theorem 1]: for any t(w) which is λ-strongly convex (w.r.t a norm with dual norm (cid:107)·(cid:107)∗) 

λ4k(w)r ≤(cid:113) 2

E [gw] ≤(cid:113) 2

(cid:113) 2r

(cid:113) 2

λ16k(w)r

(22)

λn

λ

R({φ (cid:55)→ (cid:104)w  φ(cid:105) | t(w) ≤ a}) ≤ (sup(cid:107)φ(cid:107)∗)

(23)
For each a > 0  deﬁne H(a) = {hw : w ∈ W  E [gw] ≤ a}. First note that E [gw] = F (w) −
F (w(cid:63)) is λ-strongly convex. Using (23) and the Lipschitz composition property we therefore have
R(H(a)) ≤ LB

λn .

(cid:113) 2a

(cid:113) 2a
j=04−jH(r4j)(cid:1) ≤
R(Hr) = R(cid:0)∪∞

λn. Now:

∞(cid:88)

4−jR(H(4rj)) ≤ LB

4−j/2 = 2LB

(cid:113) 2r

λn

(cid:113) 2r

∞(cid:88)

λn

j=0

We now proceed to bounding E [gw] = F (w)− F (w(cid:63)) and thus proving Theorem 1. For any r > 0 
with probability at least 1 − δ we have:

E [gw] − ˆE [gw] = 4k(w)(E [gr

(cid:113) 1

√
λn(4

w] − ˆE [gr

2+(cid:112)log(1/δ)) ≤ 2LB

w]) = 4k(w)(E [hr

(cid:113) 32+log(1/δ)

where D = LB
6 and 7 into (20). We now consider two possible cases: k(w) = 0 and k(w) > 0.
The case k(w) = 0 corresponds to functions with an expected value close to optimal: E [gw] ≤ r 
i.e. F (w) ≤ F (w(cid:63)) + r. In this case (24) becomes:

is obtained by substituting Lemmas

λn

w] − ˆE [hr

w]) ≤ 4k(w)√

rD

(24)

E [gw] ≤ ˆE [gw] +

√
rD

(25)

We now turn to functions for which k(w) > 0  i.e. with expected values further away from optimal.
In this case  the deﬁnition of k(w) ensures 4k(w)−1r < E [gw] and substituting this into (24) we
have E [gw] − ˆE [gw] ≤ 4

√
E [gw]

r

rD. Rearranging terms yields:
E [gw] ≤

ˆE [gw]

1
1−4D/

√
r

(26)
r ≥ 1)  we always
√

1
1−4D/

(27)

Combining the two cases (25) and (26) (and requiring r ≥ (4D)2 so that
have:

(cid:104)ˆE [gw]

(cid:105)

√
rD

+

+

E [gw] ≤

1
1−4D/

√
r

Setting r = (1 + 1

a)2(4D)2 yields the bound in Theorem 1.

5 Comparison with Previous “Fast Rate” Guarantees

√
Rates faster than 1/
n for estimation have been previously explored under various conditions 
where strong convexity has played a signiﬁcant role. Lee et al [8] showed faster rates for
squared loss  exploiting the strong convexity of this loss function  but only under ﬁnite pseudo-
dimensionality assumption  which do not hold in SVM-like settings. Bousquet [9] provided similar
√
guarantees when the spectrum of the kernel matrix (covariance of the data) is exponentially decay-
ing. Tsybakov [10] introduced a margin condition under which rates faster than 1/
n are shown
possible. It is also possible to ensure rates of 1/n by relying on low noise conditions [9  11]  but
here we make no such assumption.
Most methods for deriving fast rates ﬁrst bound the variance of the functions in the class by some
√
monotone function of their expectations. Then  using methods as in Bartlett et al [5]  one can
get bounds that have a localized complexity term and additional terms of order faster than 1/
n.
However  it is important to note that the localized complexity term typically dominates the rate and
still needs to be controlled. For example  Bartlett et al [12] show that strict convexity of the loss
function implies a variance bound  and provide a general result that can enable obtaining faster rates
√
as long as the complexity term is low. For instance  for classes with ﬁnite VC dimension V   the
resulting rate is n−(V +2)/(2V +2)  which indeed is better than 1/
n but is not quite 1/n. Thus we
see that even for a strictly convex loss function  such as the squared loss  additional conditions are
necessary in order to obtain “fast” rates.
In this work we show that strong convexity not only implies a variance bound but in fact can be used
to bound the localized complexity. An important distinction is that we require strong convexity of
the function F (w) with respect to the norm (cid:107)w(cid:107). This is rather different than requiring the loss
function z (cid:55)→ (cid:96)(z  y) be strongly convex on the reals. In particular  the loss of a linear predictor 
w (cid:55)→ (cid:96)((cid:104)w  x(cid:105)  y) can never be strongly convex in a multi-dimensional space  even if (cid:96) is strongly
convex  since it is ﬂat in directions orthogonal to x.
As mentioned  f(w; x  y) = (cid:96)((cid:104)w  x(cid:105)  y) can never be strongly convex in a high-dimensional space.
However  we actually only require the strong convexity of the expected loss F (w).
If the loss
function (cid:96)(z  y) is λ-strongly convex in z  and the eigenvalues of the covariance of x are bounded
away from zero  strong convexity of F (w) can be ensured.
In particular  F (w) would be cλ-
strongly-convex  where c is the minimal eigenvalue of the COV[x]. This enables us to use Theorem

7

1 to obtain rates of 1/n on the expected loss itself. However  we cannot expect the eigenvalues to be
bounded away from zero in very high dimensional spaces  limiting the applicability of the result of
low-dimensional spaces were  as discussed above  other results also apply.
An interesting observation about our proof technique is that the only concentration inequality we
invoked was McDiarmid’s Inequality (in [6  Theorem 5] to obtain (20)—a bound on the deviations
in terms of the Rademacher complexity). This was possible because we could make a localization
argument for the (cid:96)∞ norm of the functions in our function class in terms of their expectation.

6 Summary

We believe this is the ﬁrst demonstration that  without any additional requirements  the SVM objec-
tive converges to its inﬁnite data limit with a rate of O(1/n). This improves the previous results that
considered the SVM objective only under special additional conditions. The results extends also to
other regularized objectives.
Although the quantity that is ultimately of interest to us is the expected loss  and not the regularized
expected loss  it is still important to understand the statistical behavior of the regularized expected
loss. This is the quantity that we actually optimize  track  and often provide bounds on (e.g. in ap-
proximate or stochastic optimization approaches). A better understanding of its behavior can allow
us to both theoretically explore the behavior of regularized learning methods  to better understand
empirical behavior observed in practice  and to appreciate guarantees of stochastic optimization ap-
proaches for such regularized objectives. As we saw in Section 3  deriving such fast rates is also
essential for obtaining simple and general oracle inequalities  that also helps us guide our choice of
regularization parameters.

References
[1] E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex optimiza-

tion. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory  2006.

[2] S. Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis  The Hebrew

University  2007.

[3] T. Zhang. Covering number bounds of certain regularized linear function classes. J. Mach. Learn. Res. 

2:527–550  2002.

[4] I. Steinwart  D. Hush  and C. Scovel. A new concentration result for regularized risk minimizers. High-

dimensional Probability IV  in IMS Lecture Notes  51:260–275  2006.

[5] P. L. Bartlett  O. Bousquet  and S. Mendelson. Localized rademacher complexities. In COLT ’02: Pro-
ceedings of the 15th Annual Conference on Computational Learning Theory  pages 44–58  London  UK 
2002. Springer-Verlag.

[6] O. Bousquet  S. Boucheron  and G. Lugosi. Introduction to statistical learning theory. In O. Bousquet 
U.v. Luxburg  and G. R¨atsch  editors  Advanced Lectures in Machine Learning  pages 169–207. Springer 
2004.

[7] S. M. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk bounds  margin

bounds  and regularization. In NIPS  2008.

[8] W. S. Lee  P. L. Bartlett  and R. C. Williamson. The importance of convexity in learning with squared

loss. In Computational Learing Theory  pages 140–146  1996.

[9] O. Bousquet. Concentration Inequalities and Empirical Processes Theory Applied to the Analysis of

Learning Algorithms. PhD thesis  Ecole Polytechnique  2002.

[10] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Statistics  32:135–166 

2004.

[11] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels. ANNALS OF

STATISTICS  35:575  2007.

[12] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  classiﬁcation  and risk bounds. Journal of the

American Statistical Association  101:138–156  March 2006.

8

,Rein Houthooft
Xi Chen
Yan Duan
John Schulman
Filip De Turck
Pieter Abbeel