2019,First-order methods almost always avoid saddle points: The case of vanishing step-sizes,In a series of papers [Lee et al 2016]  [Panageas and Piliouras 2017]  [Lee et al 2019]  it was established that some of the most commonly used first order methods almost surely (under random initializations) and with step-size being small enough  avoid strict saddle points  as long as the objective function $f$ is $C^2$ and has Lipschitz gradient.  The key observation was that first order methods can be studied from a dynamical systems perspective  in which instantiations of Center-Stable manifold theorem allow for a global analysis. The results of the aforementioned papers were limited to the case where the step-size $\alpha$ is constant  i.e.  does not depend on time (and typically bounded from the inverse of the Lipschitz constant of the gradient of $f$). It remains an open question whether or not the results still hold when the step-size is time dependent and vanishes with time.

In this paper  we resolve this question on the affirmative for gradient descent  mirror descent  manifold descent and proximal point. The main technical challenge is that the induced (from each first order method) dynamical system is time non-homogeneous and the stable manifold theorem is not applicable in its classic form. By exploiting the dynamical systems structure of the aforementioned first order methods  we are able to prove a stable manifold theorem that is applicable to time non-homogeneous dynamical systems and generalize the results in [Lee et al 2019] for time dependent step-sizes.,First-order methods almost always avoid saddle

points: The case of vanishing step-sizes

Ioannis Panageas

SUTD

Singapore

Georgios Piliouras

SUTD

Singapore

ioannis@sutd.edu.sg

georgios@sutd.edu.sg

Xiao Wang

SUTD

Singapore

xiao_wang@sutd.edu.sg

Abstract

In a series of papers [17  22  16]  it was established that some of the most commonly
used ﬁrst order methods almost surely (under random initializations) and with step-
size being small enough  avoid strict saddle points  as long as the objective function
f is C 2 and has Lipschitz gradient. The key observation was that ﬁrst order methods
can be studied from a dynamical systems perspective  in which instantiations of
Center-Stable manifold theorem allow for a global analysis. The results of the
aforementioned papers were limited to the case where the step-size α is constant 
i.e.  does not depend on time (and bounded from the inverse of the Lipschitz
constant of the gradient of f). It remains an open question whether or not the
results still hold when the step-size is time dependent and vanishes with time.
In this paper  we resolve this question on the afﬁrmative for gradient descent  mirror
descent  manifold descent and proximal point. The main technical challenge is
that the induced (from each ﬁrst order method) dynamical system is time non-
homogeneous and the stable manifold theorem is not applicable in its classic form.
By exploiting the dynamical systems structure of the aforementioned ﬁrst order
methods  we are able to prove a stable manifold theorem that is applicable to
time non-homogeneous dynamical systems and generalize the results in [16] for
vanishing step-sizes.

1

Introduction

Non-convex optimization has been studied extensively the last years and has been one of the main
focuses of Machine Learning community. The reason behind the interest of ML community is that in
many applications of interest  one has to deal with the optimization of a non-convex landscape. One
of the key obstacles of non-convex optimization is the existence of numerous saddle points (which
can outnumber the local minima [10  24  6]). Avoiding them is a fundamental challenge for ML [14].
Recent progress [11  16] has shown that under mild regularity assumptions on the objective function 
ﬁrst-order methods such as gradient descent can provably avoid the so-called strict saddle points1.

1These are saddle points where the Hessian of the objective admits at least one direction of negative curvature.
Such property has been shown to hold in a wide range of objective functions  see [11  29  28  13  12  3] and
references therein.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In particular  a uniﬁed theoretical framework is established in [16] to analyze the asymptotic behavior
of ﬁrst-order optimization algorithms such as gradient descent  mirror descent  proximal point  coor-
dinate descent and manifold descent. It is shown that under random initialization  the aforementioned
methods avoid strict saddle points almost surely. The proof exploits a powerful theorem from the
dynamical systems literature  the so-called Stable-manifold theorem (see supplementary material for
a statement of this theorem). For example  given a C 2 (twice continuously differentiable function) f
with L-Lipschitz gradient  gradient descent method

xk+1 = g(xk) := xk − α∇f (xk)

k or

1√
k

avoids strict saddle points almost surely  under the assumption that the stepsize is constant and
L. The crux of the proof in [16] is the Stable-manifold theorem for the time-homogeneous2
0 < α < 1
dynamical system xk+1 = g(xk). The Stable-manifold theorem implies that the dynamical system
g avoids its unstable ﬁxed points and with the fact that the unstable ﬁxed points of the dynamical
system g coincide with the strict saddles of f the claim follows. Results of similar ﬂavor can be
shown for Expectation Maximization algorithm [19]  Multiplicative Weights Update [18  23] and for
min-max optimization [9].
In many applications/algorithms  however  the stepsize is adaptive or vanishing/diminishing (meaning
limk αk = 0  e.g.  αk = 1
). Such applications include stochastic gradient descent (see
[27] for analysis of SGD for convex functions)  urn models and stochastic approximation [25] 
gradient descent [4]  online learning algorithms like multiplicative weights update [1  15] (which
is an instantiation of Mirror Descent with entropic regularizer). It is also important to note that the
choice of the stepsize is really crucial in the aforementioned applications as changing the stepsize can
change the convergence properties (transition from convergence to oscillations/chaos [20  21  8  5]) 
the rate of convergence [20] as well as the system efﬁciency [7].
The proof in [16] does not carry over when the stepsize depends on time  because the Stable-manifold
theorem is not applicable. Hence  whether the results of [16] hold for vanishing step-sizes remains
unresovled. This was stated explicitly as an open question in [16]. Our work resolves this question in
the afﬁrmative. Our main result is stated below informally.
Theorem 1.1 (Informal). Gradient Descent  Mirror Descent  Proximal point and Manifold descent 

(cid:1) avoid the set of strict saddle points (isolated and non-

with vanishing step-size αk of order Ω(cid:0) 1

isolated) almost surely under random initialization.

k

Organization of the paper. The paper is organized as follows: In Section 2 we give important
deﬁnitions for the rest of the paper  in Section 3 we provide intuition and technical overview of
our results  in Section 4 we show a new Stable-manifold theorem that is applicable to a class of
time non-homogeneous dynamical systems and ﬁnally in Section 5 we show how this new manifold
theorem can be applied to Gradient descent  Mirror Descent  Proximal point and Manifold Descent.
Due to space constraints  most of the proofs can be found in the supplementary material.

Notation. Throughout this paper  we denote N the set of nonnegative integers and R the set of
real numbers  (cid:107)·(cid:107) the Euclidean norm  bolded x the vector  B(x  δ) the open ball centering at x
with radius δ  g(k  x) the update rule for optimization algorithms indexed by k ∈ N  ˜g(m  n  x) the
composition g(m  ...  g(n + 1  g(n  x))...) for m ≥ n  ∇f the gradient of f : Rd → R and ∇2f (x)
the Hessian of f at x  Dxg(k  x) the differential with respect to variable x 

2 Preliminaries

In this section we provide all necessary deﬁnitions that will be needed for the rest of the paper.
Deﬁnition 2.1 (Time (non)-homogeneous). We call a dynamical system xk+1 = g(xk) as time
homogeneous since g does not on step k. Furthermore  we call a dynamical system xk+1 = g(k  xk)
time non-homogeneous as g depends on k.
Deﬁnition 2.2 (Critical point). Given a C 2 (twice continuously differentiable) function f : X → R
where X is an open  convex subset of Rd  the following deﬁnitions are provided for completeness.

2This means that g does not depend on time. In the dynamical systems/differential equations literature such

systems are called "autonomous" whereas time-dependent systems are called "non-autonomous".

2

1. A point x∗ is a critical point of f if ∇f (x∗) = 0.
2. A critical point is a local minimum if there is a neighborhood U around x∗ such that

f (x∗) ≤ f (x) for all x ∈ U  and a local maximum if f (x∗) ≥ f (x).

3. A critical point is a saddle point if for all neighborhoods U around x∗  there are x  y ∈ U

such that f (x) ≤ f (x∗) ≤ f (y).

4. A critical point x∗ is isolated if there is a neighborhood U around x∗  and x∗ is the only

critical point in U.

This paper will focus on saddle points that have directions of strictly negative curvature  that is the
concept of strict saddle.
Deﬁnition 2.3 (Strict Saddle). A critical point x∗ of f is a strict saddle if λmin(∇2f (x∗)) < 0
(minimum eigenvalue of the Hessian computed at the critical point is negative).
Let X ∗ be the set of strict saddle points of function f and we follow the Deﬁnition 2 of [16] for the
global stable set of X ∗.
Deﬁnition 2.4 (Global Stable Set and ﬁxed points). Given a dynamical system (e.g.  gradient descent
xk+1 = xk − αk∇f (xk))
(1)
the global stable set W s(X ∗) of X ∗ is the set of initial conditions where the sequence xk converges
to a strict saddle. This is deﬁned as:

xk+1 = g(k  xk) 

W s(X ∗) = {x0 :

k→∞ xk ∈ X ∗}.

lim

Moreover  z is called a ﬁxed point of the system (1) if z = g(k  z) for all natural numbers k.
Deﬁnition 2.5 (Manifold). A C k-differentiable  d-dimensional manifold is a topological space M 
together with a collection of charts {(Uα  φα)}  where each φα is a C k-diffeomorphism from an
open subset Uα ⊂ M to Rd. The charts are compatible in the sense that  whenever Uα ∩ Uβ (cid:54)= ∅ 
the transition map φα ◦ φ−1

β : φβ(Uβ ∩ Uα) → Rd is of C k.

3

Intuition and Overview

to saddle points  even for time varying/vanishing step-sizes αk of order Ω(cid:0) 1

In this section we will illustrate why gradient descent and related ﬁrst-order methods do not converge

(cid:1).

k

3.1

Intuition

2 xT Ax where A = diag(λ1  ...  λd) is a d × d  non-
Consider the case of a quadratic  f (x) = 1
singular  diagonal matrix with at least a negative eigenvalue. Let λ1  ...  λj be the positive eigenvalues
of A (the ﬁrst j) and λj+1  ...  λd be the non-positive ones. It is clear that x∗ = 0 is the unique
critical point of function f and the Hessian ∇2f is A everywhere (and hence at the critical point).
Moreover  it is clear that x∗ is a strict saddle point (not a local minimum).
Gradient descent with step-size αk (it holds that αk ≥ 0 for all k and limk→∞ αk = 0) has the
following form:

xk+1 = xk + αkAxk = (I − αkA)xk.
Assuming that x0 is the starting point  then it holds that xk+1 =
conclude that

We examine when it is true that limk→∞ xk = x∗. It is clear that(cid:81)∞

xk+1 = diag

t=0

(1 − λ1αt)  ... 

(1 − λnαt)

and has the same convergence properties as

(cid:32) k(cid:89)

k(cid:89)

t=0

e−λ(cid:80)∞

t=0 αt.

3

(cid:16)(cid:81)k
(cid:17)
t=0(I − αk−tA)
(cid:33)

x0.

t=0(1− λαt) = e

x0. We

(2)

(cid:80)∞
t=0 ln(1−λαt) 

(3)

For λ > 0  the term (3) converges to zero if and only if(cid:80)∞
stepsize αk) and for λ < 0 it holds that the term (3) diverges for αt to be Ω(cid:0) 1
being Ω(cid:0) 1

(cid:1) we conclude that limk→∞ xk = 0 whenever the initial point x0 satisﬁes xi

t=0 αt = +∞ which is true if αt is Ω(cid:0) 1
(cid:1).
(cid:1). Therefore  for αk

Moreover  for λ = 0 it holds that the term (3) remains a constant (independently of the choice of

0 = 0 (i-th
coordinate of x0) for λi ≥ 0.
Hence  if x0 ∈ Es := span(e1  . . .   ej)3  then xt converges to the saddle point x∗ and if x0 has a
component outside Es then gradient descent diverges. For the example above  the global stable set of
x∗ is the subspace Es which is of measure zero since Es is not full dimensional.

(cid:1)). In the case where αk is a sequence of stepsizes that converges

Remark 3.1 (αk of order O(cid:0) 1

k

t

t

to zero with a rate
t=0 αk converges
and hence in our example above we conclude that limk→∞ xk exists  i.e.  xk converges but not
necessarily to a critical point.

k1+ for any  > 0 (example 1

k2   1

1

2k etc)  then it holds that(cid:80)∞

k1+

3.2 Technical Overview

The stability of non-homogeneous (i.e. non-autonomous) systems  at least for the case of continuous-
time systems  has been the subject of intensive investigation ([2] and references therein). Although
some work on discrete-time systems exists [26]  this area is less developed and as far as we know
no explicit connections to optimization applications have been made before. Moreover  as far as
gradient descent  mirror descent  etc are concerned  the corresponding dynamical system that needs
to be analyzed is more complicated when the objective function is not quadratic and the analysis of
previous subsection does not apply.
Suppose we are given a function f that is C 2  and 0 is a saddle point of f. The Taylor expansion of
the gradient descent in a neighborhood of 0 is as follows:

xk+1 = (I − αk∇2f (0))xk + η(k  xk) 

(4)

where η(k  0) = 0 and η(k  x) is of order o((cid:107)x(cid:107)) around 0 for all naturals k.
Due to the error term η(k  xk)  the approach for quadratic functions does not imply the existence
of the stable manifold. Inspired by the proof of Stable-manifold theorem for time homogeneous
ODEs  we prove a Stable-manifold theorem for discrete time non-homogeneous dynamical system
(4). In words  we prove the existence of a manifold W s that is not of full dimension (it has the
same dimension as Es  where Es denotes the subspace that is spanned by the eigenvectors with
corresponding positive eigenvalues of matrix ∇2f (0)).
To show this  we derive the expression of (2) for the general function f to be:

k(cid:88)

i=0

A (k  i + 1) η (i  xi)  

xk+1 = A (k  0) x0 +

where A (m  n) =(cid:0)I − αm∇2f (0)(cid:1) ...(cid:0)I − αn∇2f (0)(cid:1) for m ≥ n  and A (m  n) = I if m < n.

Next  we generate a sequence {xk}k∈N from (5) with an initial point x0 = (x+
0 ∈ Es
0 ∈ Eu. If this sequence converges to 0  the equation (5) induces an operator T on the space
and x−
of sequences converging to 0  and the sequence {xk}k∈N is the ﬁxed point of T . This is so called
the Lyapunov-Perron method (see supplementary material for some brief overview of the method).
By Banach ﬁxed point theorem (see supplementary material for the statement of the theorem)  it can
be proved that the sequence {xk}k∈N (as the ﬁxed point of T ) exists and is unique. Furthermore 
this implies that there is a unique x−
0 corresponding to x+
0   i.e. there exists a well deﬁned function
ϕ : Es → Eu such that x−
0 = ϕ(x+
0 ).

0 )  where x+

0   x−

(5)

4 Stable Manifold Theorem for Time Non-homogeneous Dynamical Systems

We start this section by showing the main technical result of this paper. This is a new stable manifold
theorem that works for time non-homogeneous dynamical systems and is used to prove our main result
(Theorem 1.1) for Gradient Descent  Mirror Descent  Proximal Point and Manifold Descent. The

3{e1  ...  ed} denote the classic orthogonal basis of Rd.

4

proof of this theorem exploits the structure of the aforementioned ﬁrst-order methods as dynamical
systems.
Theorem 4.1 (A new stable manifold theorem). Let H be a d × d real diagonal matrix with at least
one negative eigenvalue  i.e. H = diag{λ1  ...  λd} with λ1 ≥ λ2 ≥ ...λs > 0 ≥ λs+1 ≥ ... ≥ λd
and assume λd < 0. Let η(k  x) be a continuously differentiable function such that η(k  0) = 0 and
for each  > 0  there exists a neighborhood of 0 in which it holds

Let {αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

(cid:107)η (k  x) − η (k  y)(cid:107) ≤ αk(cid:107)x − y(cid:107)   for all naturals k.

(cid:1) that converges to zero. We deﬁne

(6)

k

the time non-homogeneous dynamical system

xk+1 = g(k  xk)  where g(k  x) = (I − αkH)x + η(k  x).

(7)
Suppose that E = Es ⊕ Eu  where Es is the span of the eigenvectors corresponding to negative
eigenvalues of H  and Eu is the span of the eigenvectors corresponding to nonnegative eigenvalues
of H. Then there exists a neighborhood U of 0 and a C 1-manifold V (0) in U that is tangent to Es
k=0 ˜g−1(k  0  U ) ⊂ V (0).
We can generalize Theorem 4.1 to the case where matrix H is diagonalizable and for any ﬁxed point
x∗ (instead of 0  using a shifting argument). The statement is given below.
Corollary 4.2. Let {αk}k∈N be a sequence of positive real numbers that converges to zero. Addi-

at 0  such that for all x0 ∈ V (0)  limk→∞ g(k  xk) = 0. Moreover (cid:84)∞
(cid:1). Let g(k  x) : Rd → Rd be C 1 maps for all k ∈ N and
tionally  αk ∈ Ω(cid:0) 1

k

(8)
be a time non-homogeneous dynamical system. Assume x∗ is a ﬁxed point  i.e. g(k  x∗) = x∗ for all
k ∈ N. Suppose the Taylor expansion of g(k  x) at x∗ in some neighborhood of x∗ 

xk+1 = g(k  xk)

g(k  x) = g(k  x∗) + Dxg(k  x∗)(x − x∗) + θ(k  x)  satisﬁes

(9)

(10)

1. Dxg(k  x∗) = I − αkG  G real diagonalizable with at least one negative eigenvalue;
2. For each  > 0  there exists an open neighborhood centering at x∗ of radius δ > 0  denoted

as B(x∗  δ)  such that

(cid:107)θ(k  u1) − θ(k  u2)(cid:107) ≤ αk(cid:107)u1 − u2(cid:107)

for all k ∈ N and all u1  u2 ∈ B(x∗  δ).

one  such that for x0 ∈ W (x∗)  limk→∞ g(k  x∗) = x∗. Moreover (cid:84)∞

There exists a open neighborhood U of x∗ and a C 1-manifold W (x∗) in U  with codimension at least
k=0 ˜g−1(k  0  U ) ⊂ W (x∗).
Proof. Since G is diagonalizable  there exists invertible matrix Q such that G = Q−1HQ  hence
QGQ−1 = H  where H = diag{λ1  ...  λd} (i.e.  H is a diagonal matrix with entries λ1  ...  λd).
Consider the map z = ϕ(x) = Q(x − x∗). ϕ induces a new dynamical system in terms of z as
follows:

Q−1zk+1 = (I − αkG)Q−1zk + θ(k  Q−1zk + x∗).

Multiplying by Q from the left on both sides  we have

zk+1 = Q(I − αkG)Q−1zk + Qθ(k  Q−1zk + x∗) = (I − αkH)zk + ˆθ(k  zk) 

(11)
where ˆθ(k  zk) = Qθ(k  Q−1zk +x∗). Denote q(k  z) = (I −αkH)z+ ˆθ(k  z) the update rule given
by equation (11). In order to apply Theorem 4.1  we next verify that ˆθ(k ·) satisﬁes the condition (6)
in Theorem 4.1 for all k ∈ N. It is essentially to verify that given any  > 0  there exists a δ(cid:48) > 0 
such that

(cid:13)(cid:13)(cid:13) =(cid:13)(cid:13)Qθ(k  Q−1w1 + x∗) − Qθ(k  Q−1w2 + x∗)(cid:13)(cid:13) ≤ αk(cid:107)w1 − w2(cid:107)

(cid:13)(cid:13)(cid:13)ˆθ(k  w1) − ˆθ(k  w2)

(12)
for all w1  w2 ∈ B(0  δ(cid:48)). Let’s elaborate it. According to (10) of condition 2  for any given  > 0 
(cid:107)Q(cid:107)(cid:107)Q−1(cid:107) )  such that
and then

(cid:107)Q(cid:107)(cid:107)Q−1(cid:107) is also a small positive number  there exists a δ > 0 (w.r.t.
(cid:107)Q(cid:107) (cid:107)Q−1(cid:107) (cid:107)u1 − u2(cid:107)

(cid:107)θ(k  u1) − θ(k  u2)(cid:107) ≤ αk







5

for all u1  u2 ∈ B(x∗  δ). Denote V = Q(B(x∗  δ) − x∗)  i.e.

V = {w ∈ Rd : w = Q(u − x∗) for some u ∈ B(x∗  δ)} 

and it is easy to see that 0 ∈ V . Since Q(u − x∗) is a diffeomorphism (composition of a translation
and a linear isomorphism) from the open ball B(x∗  δ) to Rd  V is an open neighborhood (not
necessarily a ball) of 0. Therefore  there exists an open ball at 0 with radius δ(cid:48)  denoted as B(0  δ(cid:48)) 
such that B(0  δ(cid:48)) ⊂ V . Next we show that B(0  δ(cid:48)) satisﬁes the inequality (12). By the deﬁnition of
V   for any w1  w2 ∈ B(0  δ(cid:48)) ⊂ V   there exist u1  u2 ∈ B(x∗  δ)  such that

(13)
and the inverse transformation is given by u1 = Q−1w1 + x∗  u2 = Q−1w2 + x∗. Plugging to
inequality (12)  we have

w1 = Q(u1 − x∗)  w2 = Q(u1 − x∗) 

(cid:13)(cid:13)(cid:13)ˆθ(k  w1) − ˆθ(k  w2)

(cid:13)(cid:13)(cid:13) =(cid:13)(cid:13)Qθ(k  Q−1w1 + x∗) − Qθ(k  Q−1w2 + x∗)(cid:13)(cid:13)

= (cid:107)Qθ(k  u1) − Qθ(k  u2)(cid:107)
≤ (cid:107)Q(cid:107) (cid:107)θ(k  u1) − θ(k  u2)(cid:107) ≤ (cid:107)Q(cid:107) αk
= (cid:107)Q(cid:107) αk
≤ (cid:107)Q(cid:107) αk

(cid:13)(cid:13)(Q−1w1 + x∗) − (Q−1w2 + x∗)(cid:13)(cid:13)
(cid:13)(cid:13)Q−1(cid:13)(cid:13) (cid:107)w1 − w2(cid:107) = αk(cid:107)w1 − w2(cid:107) .

(cid:107)u1 − u2(cid:107)
(cid:107)Q(cid:107) (cid:107)Q−1(cid:107)

(cid:107)Q(cid:107) (cid:107)Q−1(cid:107)
(cid:107)Q(cid:107) (cid:107)Q−1(cid:107)





Thus the veriﬁcation is complete. So as a consequence of Theorem 4.1  there exists a C 1-manifold
V (0) such that for all z0 ∈ V (0)  limk→∞ ˜q(k  0  z0) = 0. For the neighborhood ϕ−1(B(0  δ(cid:48))) of
x∗  denote W (x∗) the local stable set of dynamical system given by g(k  x)  i.e. 
k→∞ ˜g(k  0  x0) = x∗}.

W (x∗) = {x0 ∈ ϕ−1(B(0  δ)) :

lim

We claim that W (x∗) ⊂ ϕ−1(V (0)) and the proof is as follows:
Suppose x0 ∈ W (x∗)  then the sequence {xk}k∈N generated by xk+1 = g(k  xk) with initial
condition x0 converges to x∗. The map ϕ induces a sequence {zk}k∈N  where z0 = ϕ(x0) and

zk+1 = ϕ(xk+1) = ϕ (g(k  xk))

= Q (x∗ + (I − αkG)(xk − x∗) + θ(k  xk) − x∗)
(since xk = ϕ−1(zk) = Q−1zk + x∗)
= Q(I − αkG)Q−1zk + Qθ(k  Q−1zk + x∗) = (I − αkH)zk + ˆθ(k  zk).

(14)
(15)
(16)
(17)
Since zk = ϕ(xk)  and xk → x∗  we have that zk → 0. This implies sequence zk generated
by zk+1 = q(k  zk) with initial condition z0 converges to 0  meaning that z0 = ϕ(x0) ∈ V (x∗).
Therefore W (x∗) ⊂ ϕ−1(V (0)). Let U = ϕ−1(B(0  δ)) and the proof is complete.

We conclude this section by the following corollary which can be proved using standard arguments
about separability of Rd (every open cover has a countable subcover). We denote W s(A∗) the set of
initial conditions so that the given dynamical system g converges to a ﬁxed point x∗ such that matrix
Dxg(k  x∗) has an eigenvalue with absolute value greater than one for all k.
Corollary 4.3. Let g(k  x) : Rd → Rd be the mappings deﬁned in Theorem 4.2. Then W s(A∗) has
Lebesgue measure zero.

5 Applications

In this section  we apply Theorem 4.1 (or its corollary 4.2) to the four of the most commonly used
ﬁrst-order methods and we prove that each one of them avoids strict saddle points even with vanishing

stepsize αk of order Ω(cid:0) 1

(cid:1).

k

6

5.1 Gradient Descent
Let f (x) : Rd → R be a real-valued C 2 function  and g(k  x) = x − αk∇f (x) be the update rule of
gradient descent  where {αk}k∈N is a sequence of positive real numbers. Then

xk+1 = xk − αk∇f (xk)

(18)

is a time non-homogeneous dynamical system.
Theorem 5.1. Let xk+1 = g(k  xk) be the gradient descent algorithm deﬁned by equation 18  and

{αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

(cid:1) that converges to zero. Then the

stable set of strict saddle points has Lebesgue measure zero.
Proof. We need to verify that the Taylor expansion of g(k  x) at x∗ satisﬁes the conditions of
Corollary 4.2. Condition 1 is obvious since the Hessian ∇2f (x∗) is diagonalizable and has at least
one negative eigenvalue. It sufﬁce to verify condition 2. Consider the Taylor expansion of g(k  x) in
a neighborhood U of x∗:

k

g(k  x) = g(k  x∗) + Dxg(k  x∗)(x − x∗) + θ(k  x)
= x∗ + (I − αk∇2f (x∗))(x − x∗) + θ(k  x)
= x − αk∇2f (x∗)(x − x∗) + θ(k  x).

(cid:90) 1

θ(k  x) − θ(k  y) =

So we can write θ(k  x) = g(k  x) − x + αk∇2f (x∗)(x − x∗)  and then the differential of θ(k  x)
with respect to x is Dxθ(k  x) = Dx(g(k  x) − x) + αk∇2f (x∗) = −αk∇2f (x) + αk∇2f (x∗).
From the Fundamental Theorem of Calculus and chain rule for multivariable functions  we have
Dzθ(k  z)|z=tx+(1−t)y · (x − y)dt.

By the assumption of f being C 2  we have that ∇2f (x) is continuous everywhere. And then for any
x ∈ B(x∗). And this implies that (cid:107)Dxθ(k  x)(cid:107) ≤ αk for all x ∈ B(x∗). Since tx+(1−t)y ∈ B(x∗)

given  > 0  there exists a open ball B(x∗) centering at x∗  such that(cid:13)(cid:13)∇2f (x) − ∇2f (x∗)(cid:13)(cid:13) for all
(cid:13)(cid:13) ≤ αk for all t ∈ [0  1]. By Cauchy-Schwarz
if x  y ∈ B(x∗)  we have that(cid:13)(cid:13)Dzθ(k  z)|z=tx+(1−t)y

θ(k  tx + (1 − t)y)dt =

(cid:90) 1

d
dt

0

0

inequality  we have

(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90) 1
(cid:18)(cid:90) 1

0

0

(cid:107)θ(k  x) − θ(k  y)(cid:107) =

≤

Dzθ(k  z)|z=tx+(1−t)y · (x − y)dt

(cid:13)(cid:13)Dzθ(k  z)|z=tx+(1−t)y

(cid:19)
(cid:13)(cid:13) dt

· (cid:107)x − y(cid:107) = αk(cid:107)x − y(cid:107)  

the veriﬁcation completes. By Corollary 4.2 and Corollary 4.3  we conclude that the stable set of
strict saddle points has Lebesgue measure zero.

5.2 Mirror Descent
We consider mirror descent algorithm in this section. Let D be a convex open subset of Rd  and
M = D ∩ A for some afﬁne space A. Given a function f : M → R and a mirror map Φ  the mirror
descent algorithm with vanishing step-size is deﬁned as

xk+1 = g(k  xk) := h(∇Φ(xk) − αk∇f (xk)) 

(19)

where h(x) = argmaxz∈M(cid:104)z  x(cid:105) − Φ(z).
Deﬁnition 5.2 (Mirror Map). We say that Φ is a mirror map if it satisﬁes the following properties.

• Φ : D → R is C 2 and strictly convex.
• The gradient of Φ is surjective onto Rd  that is ∇Φ(D) = Rd.
• ∇RΦ diverges on the relative boundary of M  that is limx→∂M ||∇RΦ(x)|| = ∞.

{αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

stable set of strict saddle points has Lebesgue measure zero.

k

(cid:1) that converges to zero. Then the

Theorem 5.3. Let xk+1 = g(k  xk) be the mirror descent algorithm deﬁned by equation (19)  and

7

5.3 Proximal Point

The proximal point algorithm is given by the iteration

xk+1 = g(k  xk) := arg min
z

{αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

stable set of strict saddle points has Lebesgue measure zero.

f (z) +

k

Theorem 5.4. Let xk+1 = g(k  xk) be the proximal point algorithm deﬁned by equation (20)  and

(cid:107)xk − z(cid:107)2 .

1
2αk

(20)

(cid:1) that converges to zero. Then the

5.4 Manifold Gradient Descent
Let M be a submanifold of Rd  and TxM be the tangent space of M at x. PM and PTxM be the
orthogonal projector onto M and TxM respectively. Assume that f : M → R is extendable to
neighborhood of M and let ¯f be a smooth extension of f to Rd. Suppose that the Riemannian metric
on M is induced by Euclidean metric of Rd  then the Riemannian gradient ∇Rf (x) is the projection
of the gradient of f (x) on Rd  i.e. ∇Rf (x) = PTxM∇f (x). Then the manifold gradient descent
algorithm is:
(21)
Theorem 5.5. Let xk+1 = g(k  xk) be the manifold gradient descent deﬁned by equation (21)  and

{αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

(cid:1) that converges to zero. Then the

xk+1 = g(k  xk) := PM (xk − αkPTxk M∇f (xk)).

stable set of strict saddle points has measure zero.
For the case when M is not a submanifold of Rd  the manifold gradient descent algorithm depends
on the Riemannian metric R deﬁned intrinsically  i.e.  R is not induced by any ambient metric.
Given f : M → R  the Riemannian gradient ∇Rf is deﬁned to be the unique vector ﬁeld such that
R(∇Rf  X) = ∂X f for all vector ﬁeld X on M. In local coordinate systems x(p) = (x1  ...  xd) 
p ∈ M  the Riemannian gradient is written as ∇Rf (x) =

(cid:16)
where(cid:0)Rij(cid:1) is the inverse of the metric matrix at the point x and Rij ∂f
(cid:0)Rij(cid:1) · ∇f (xk).
{αk}k∈N be a sequence of positive real numbers of order Ω(cid:0) 1

(22)
Theorem 5.6. Let xk+1 = g(k  xk) be the manifold gradient descent deﬁned by equation (22)  and

(cid:1) that converges to zero. Then the

(cid:17)
=(cid:0)Rij(cid:1) · ∇f (x) 
= (cid:80)

Einstein convention. Then the update rule (in a local coordinate system) is

xk+1 = g(k  xk) := xk − αk

  ...  Rdj ∂f
∂xj

R1j ∂f
∂xj

j Rij ∂f
∂xj

as the

∂xj

k

k

stable set of strict saddle points has measure zero.

6 Conclusion

the stepsize αk converges to zero with order Ω(cid:0) 1

(cid:1)  then gradient descent  mirror descent  proximal

In this paper  we generalize the results of [16] for the case of vanishing stepsizes. We showed that if

point and manifold descent still avoid strict saddles. We believe that this is an important result
that was missing from the literature since in practice vanishing or adaptive stepsizes are commonly
used. Our main result boils down to the proof of a Stable-manifold theorem 4.1 that works for time
non-homogeneous dynamical systems and might be of independent interest. We leave as an open
question the case of Block Coordinate Descent (as it also appears in [16]).

k

7 Acknowledgements

Ioannis Panageas acknowledges SRG ISTD 2018 136 and NRF fellowship for AI. Georgios Piliouras
and Xiao Wang acknowledge MOE AcRF Tier 2 Grant 2016-T2-1-170  grant PIE-SGP-AI-2018-01
and NRF 2018 Fellowship NRF-NRFF2018-07. We thank Tony Roberts for pointers to the literature
of stability of non-autonomous dynamical systems.

8

Figure 1: Steps of Gradient Descent for x2 − y2. (0  0) is a strict saddle. Stepsizes 1√
green) avoid (0  0) (blue faster than green). Stepsize 1

k (blue 
  1
k4 (red) converges to a non-critical point.

k

References
[1] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta algorithm

and applications. In Theory of Computing  2012.

[2] Luis Barreira  Claudia Valls  and Claudia Valls. Stability of nonautonomous differential equa-

tions  volume 1926. Springer  2008.

[3] Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro. Global optimality of local search
for low rank matrix recovery. In Advances in Neural Information Processing Systems  pages
3873–3881  2016.

[4] Sébastien Bubeck. Theory of convex optimization for machine learning. CoRR  abs/1405.4980 

2014.

[5] Vaggos Chatziafratis  Sai Ganesh Nagarajan  Ioannis Panageas  and Xiao Wang. Depth-width

trade-offs for relu networks via sharkovsky’s theorem. In Arxiv  2019.

[6] Anna Choromanska  Mikael Henaff  Michael Mathieu  Gérard Ben Arous  and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics  pages 192–204 
2015.

[7] Thiparat Chotibut  Fryderyk Falniowski  Micha? Misiurewicz  and Georgios Piliouras. The
route to chaos in routing games: Population increase drives period-doubling instability  chaos &
inefﬁciency with price of anarchy equal to one  2019.

[8] Thiparat Chotibut  Fryderyk Falniowski  Michał Misiurewicz  and Georgios Piliouras. Family
of chaotic maps from game theory  2018. Manuscript available at https://arxiv.org/abs/
1807.06831.

[9] Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent
in min-max optimization. In Advances in Neural Information Processing Systems 31  pages
9256–9266  2018.

[10] Yann N Dauphin  Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  Surya Ganguli  and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in Neural Information Processing Systems  pages 2933–2941 
2014.

9

[11] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points; online stochastic
gradient for tensor decomposition. In Conference on Learning Theory  pages 797–842  2015.

[12] Rong Ge  Chi Jin  and Yi Zheng. No spurious local minima in nonconvex low rank problems:

A uniﬁed geometric analysis. arXiv preprint arXiv:1704.00708  2017.

[13] Rong Ge  Jason D Lee  and Tengyu Ma. Matrix completion has no spurious local minimum. In

Advances in Neural Information Processing Systems  pages 2973–2981  2016.

[14] Michael I. Jordan. Dynamical  symplectic and stochastic perspectives on gradient-based

optimization. 2018.

[15] Robert Kleinberg  Georgios Piliouras  and Éva Tardos. Multiplicative updates outperform
generic no-regret learning in congestion games. In ACM Symposium on Theory of Computing
(STOC)  2009.

[16] J. D. Lee  I. Panageas  G. Piliouras  M. Simchowitz  M. I. Jordan  and B. Recht. First-order

methods almost always avoid saddle points. In Mathematical Programming  2019.

[17] Jason D Lee  Max Simchowitz  Michael I Jordan  and Benjamin Recht. Gradient descent only

converges to minimizers. In Conference on Learning Theory  pages 1246–1257  2016.

[18] Ruta Mehta  Ioannis Panageas  and Georgios Piliouras. Natural selection as an inhibitor of
genetic diversity: Multiplicative weights updates algorithm and a conjecture of haploid genetics
[working paper abstract]. In Proceedings of the 2015 Conference on Innovations in Theoretical
Computer Science  ITCS  page 73  2015.

[19] Sai Ganesh Nagarajan and Ioannis Panageas. On the analysis of EM for truncated mixtures of

two gaussians. CoRR  abs/1902.06958  2019.

[20] Kamil Nar and Shankar Sastry. Step size matters in deep learning. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018  NeurIPS 2018  3-8 December 2018  Montréal  Canada.  pages 3440–3448  2018.

[21] G. Palaiopanos  I. Panageas  and G. Piliouras. Multiplicative weights update with constant

step-size in congestion games: Convergence  limit cycles and chaos. In NIPS  2017.

[22] Ioannis Panageas and Georgios Piliouras. Gradient descent only converges to minimizers:
Non-isolated critical points and invariant regions. In Innovations of Theoretical Computer
Science (ITCS)  2017.

[23] Ioannis Panageas  Georgios Piliouras  and Xiao Wang. Multiplicative weights updates as a
distributed constrained optimization algorithm: Convergence to second-order stationary points
almost always. In Proceedings of the 36th International Conference on Machine Learning 
ICML  pages 4961–4969  2019.

[24] Razvan Pascanu  Yann N Dauphin  Surya Ganguli  and Yoshua Bengio. On the saddle point

problem for non-convex optimization. arXiv:1405.4604  2014.

[25] Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approxima-

tions. The Annals of Probability  pages 698–712  1990.

[26] Christian Pötzsche and Martin Rasmussen. Computation of nonautonomous invariant and

inertial manifolds. Numerische Mathematik  112(3):449  2009.

[27] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to

Algorithms. Cambridge University Press  2014.

[28] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere i: Overview

and the geometric picture. IEEE Transactions on Information Theory  63(2):853–884  2017.

[29] Ju Sun  Qing Qu  and John Wright. Complete dictionary recovery over the sphere ii: Recovery
by riemannian trust-region method. IEEE Transactions on Information Theory  63(2):885–914 
2017.

10

,Yingyezhe Jin
Wenrui Zhang
Peng Li
Ioannis Panageas
Georgios Piliouras
Xiao Wang