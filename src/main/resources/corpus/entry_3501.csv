2014,Model-based Reinforcement Learning and the Eluder Dimension,We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that  if the MDP can be parameterized within some known function class  we can obtain regret bounds that scale with the dimensionality  rather than cardinality  of the system. We characterize this dependence explicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed  $d_K$ is the Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover  we present a simple and computationally efficient algorithm \emph{posterior sampling for reinforcement learning} (PSRL) that satisfies these bounds.,Model-based Reinforcement Learning

and the Eluder Dimension

Ian Osband

Stanford University

iosband@stanford.edu

Benjamin Van Roy
Stanford University
bvr@stanford.edu

Abstract

We consider the problem of learning to optimize an unknown Markov deci-
sion process (MDP). We show that  if the MDP can be parameterized within
some known function class  we can obtain regret bounds that scale with the
dimensionality  rather than cardinality  of the system. We characterize this
dependence explicitly as ˜O(ÔdKdET) where T is time elapsed  dK is the
Kolmogorov dimension and dE is the eluder dimension. These represent
the ﬁrst uniﬁed regret bounds for model-based reinforcement learning and
provide state of the art guarantees in several important settings. More-
over  we present a simple and computationally ecient algorithm posterior
sampling for reinforcement learning (PSRL) that satisﬁes these bounds.

1 Introduction
We consider the reinforcement learning (RL) problem of optimizing rewards in an unknown
Markov decision process (MDP) [1].
In this setting an agent makes sequential decisions
within its enironment to maximize its cumulative rewards through time. We model the
environment as an MDP  however  unlike the standard MDP planning problem the agent
is unsure of the underlying reward and transition functions. Through exploring poorly-
understood policies  an agent may improve its understanding of its environment but it may
improve its short term rewards by exploiting its existing knowledge [2  3].
The focus of the literature in this area has been to develop algorithms whose performance
will be close to optimal in some sense. There are numerous criteria for statistical and
computational eciency that might be considered. Some of the most common include PAC
(Probably Approximately Correct) [4]  MB (Mistake Bound) [5]  KWIK (Knows What It
Knows) [6] and regret [7]. We will focus our attention upon regret  or the shortfall in the
agent’s expected rewards compared to that of the optimal policy. We believe this is a natural
criteria for performance during learning  although these concepts are closely linked. A good
overview of various eciency guarantees is given in section 3 of Li et al. [6].
Broadly  algorithms for RL can be separated as either model-based  which build a generative
model of the environment  or model-free which do not. Algorithms of both type have been
developed to provide PAC-MDP bounds polynomial in the number of states S and actions
A [8  9  10]. However  model-free approaches can struggle to plan ecient exploration. The
only near-optimal regret bounds to time T of ˜O(SÔAT) have only been attained by model-
based algorithms [7  11]. But even these bounds grow with the cardinality of the state and
action spaces  which may be extremely large or even inﬁnite. Worse still  there is a lower
bound (ÔSAT) for the expected regret in an arbitrary MDP [7].
In special cases  where the reward or transition function is known to belong to a certain
functional family  existing algorithms can exploit the structure to move beyond this “‘tabula
rasa” (where nothing is assumed beyond S and A) lower bound. The most widely-studied

1

parameterization is the degenerate MDP with no transitions  the mutli-armed bandit [12 
13  14]. Another common assumption is that the transition function is linear in states and
actions. Papers here establigh regret bounds ˜O(ÔT) for linear quadratic control [16]  but
with constants that grow exponentially with dimension. Later works remove this exponential
dependence  but only under signiﬁcant sparsity assumptions [17]. The most general previous
analysis considers rewards and transitions that are –-H¨older in a d-dimensional space to
establish regret bounds ˜O(T (2d+–)/(2d+2–)) [18]. However  the proposed algorithm UCCRL
is not computationally tractable and the bounds approach linearity in many settings.
In this paper we analyse the simple and intuitive algorithm posterior sampling for reinforce-
ment learning (PSRL) [20  21  11]. PSRL was initially introduced as a heuristic method [21] 
but has since been shown to satisfy state of the art regret bounds in ﬁnite MDPs [11] and
also exploit the structure of factored MDPs [15]. We show that this same algorithm satisﬁes
general regret bounds that depends upon the dimensionality  rather than the cardinality  of
the underlying reward and transition function classes. To characterize the complexity of this
learning problem we extend the deﬁnition of the eluder dimension  previously introduced for
bandits [19]  to capture the complexity of the reinforcement learning problem. Our results
provide a uniﬁed analysis of model-based reinforcement learning in general and provide new
state of the art bounds in several important problem settings.

2 Problem formulation
We consider the problem of learning to optimize a random ﬁnite horizon MDP M =
(S A  RM   P M  · ﬂ ) in repeated ﬁnite episodes of interaction. S is the state space  A is
the action space  RM(s  a) is the reward distribution over R and P M(·|s  a) is the transition
distribution over S when selecting action a in state s  · is the time horizon  and ﬂ the initial
state distribution. All random variables we will consider are on a probability space (  F  P).
A policy µ is a function mapping each state s œS and i = 1  . . .  · to an action a œA . For
each MDP M and policy µ  we deﬁne a value function V :

V M

µ i(s) := EM µ# ·ÿj=i

rM(sj  aj)---si = s$

(1)

µ i(s) = maxµÕ V M

where rM(s  a) := E[r|r ≥ RM(s  a)] and the subscripts of the expectation operator indicate
that aj = µ(sj  j)  and sj+1 ≥ P M(·|sj  aj) for j = i  . . .   ·. A policy µ is said to be optimal
µÕ i(s) for all s œS and i = 1  . . .  · . We will associate with
for MDP M if V M
each MDP M a policy µM that is optimal for M.
We require that the state space S is a subset of Rd for some ﬁnite d with a Î·Î 2-norm
induced by an inner product. These result actually extend to general Hilbert spaces  but we
will not deal with that in this paper. This allows us to decompose the transition function
as a mean value in S plus additive noise sÕ ≥ P M(·|s  a) =∆ sÕ = pM(s  a) + ‘P . At
ﬁrst this may seem to exclude discrete MDPs with S states from our analysis. However 
we can represent the discrete state as a probability vector st œS = [0  1]S µ RS with a
single active component equal to 1 and 0 otherwise. In fact  the notational convention that
S™ Rd should not impose a great restriction for most practical settings.
For any distribution  over S  we deﬁne the one step future value function U to be the
expected value of the optimal policy with the next state distributed according to .
(2)
One natural regularity condition for learning is that the future values of similar distributions
should be similar. We examine this idea through the Lipschitz constant on the means of
these state distributions. We write E() := E[s|s ≥ ] œS for the mean of a distribution
i with respect to the Î·Î 2-norm of the mean:
 and express the Lipschitz continuity for U M
(3)
We deﬁne KM(D) := maxi KM
i (D) to be a global Lipschitz contant for the future value
function with state distributions from D. Where appropriate  we will condense our notation

i (D)ÎE() ≠E (˜)Î2 for all   ˜ œD

i () := EM µM#V M

µM  i+1(s)--s ≥ $.

i () ≠ U M
|U M

i (˜)|Æ KM

U M

2

to write KM := KM(D(M)) where D(M) := {P M(·|s  a)|s œS   a œA} is the set of all
possible one-step state distributions under the MDP M.
The reinforcement learning agent interacts with the MDP over episodes that begin at times
tk = (k ≠ 1)· + 1  k = 1  2  . . .. Let Ht = (s1  a1  r1  . . .   st≠1  at≠1  rt≠1) denote the history
of observations made prior to time t. A reinforcement learning algorithm is a deterministic
sequence {ﬁk|k = 1  2  . . .} of functions  each mapping Htk to a probability distribution
ﬁk(Htk) over policies which the agent will employ during the kth episode. We deﬁne the
regret incurred by a reinforcement learning algorithm ﬁ up to time T to be

Regret(T ﬁ  M ú) :=

k :=⁄sœS

ﬂ(s)1V Mú

k 

ÁT /· Ëÿk=1
µk 12 (s)
µú 1 ≠ V Mú

where k denotes regret over the kth episode  deﬁned with respect to the MDP Mú by

with µú = µMú and µk ≥ ﬁk(Htk). Note that regret is not deterministic since it can
depend on the random MDP Mú  the algorithm’s internal random sampling and  through
the history Htk  on previous random transitions and random rewards. We will assess and
compare algorithm performance in terms of regret and its expectation.

3 Main results
We now review the algorithm PSRL  an adaptation of Thompson sampling [20] to rein-
forcement learning. PSRL was ﬁrst proposed by Strens [21] and later was shown to satisfy
ecient regret bounds in ﬁnite MDPs [11]. The algorithm begins with a prior distribution
over MDPs. At the start of episode k  PSRL samples an MDP Mk from the posterior. PSRL
then follows the policy µk = µMk which is optimal for this sampled MDP during episode k.

Algorithm 1
Posterior Sampling for Reinforcement Learning (PSRL)
1: Input: Prior distribution „ for Mú  t=1
2: for episodes k = 1  2  .. do
sample Mk ≥ „(·|Ht)
3:
4:
compute µk = µMk
for timesteps j = 1  .. · do
5:
6:
7:
8:
9:
10: end for

apply at ≥ µk(st  j)
observe rt and st+1
advance t = t + 1

end for

To state our results we ﬁrst introduce some notation. For any set X and Y™ Rd for d ﬁnite
let P C ‡
be the family the distributions from X to Y with mean Î·Î 2-bounded in [0  C] and
X  Y
additive ‡-sub-Gaussian noise. We let N(F –  Î·Î 2) be the –-covering number of F with
respect to the Î·Î 2-norm and write nF = log(8N(F  1/T 2 Î·Î 2)T) for brevity. Finally we
write dE(F) = dimE(F  T ≠1) for the eluder dimension of F at precision T ≠1  a notion of
dimension specialized to sequential measurements described in Section 4.
Our main result  Theorem 1  bounds the expected regret of PSRL at any time T.
Theorem 1 (Expected regret for PSRL in parameterized MDPs).
for
Fix a state space S  action space A  function families R™P CR ‡R
any CR  CP  ‡ R ‡ P > 0. Let Mú be an MDP with state space S  action space A  rewards
Rú œR and transitions P ú œP . If „ is the distribution of Mú and Kú = KMú is a global
Lipschitz constant for the future value function as per (3) then:

S◊A  R and P™P CP  ‡P
S◊A  S

E[Regret(T ﬁ P S  Mú)] Æ#CR + CP$ + ˜D(R) + +E[Kú]31 + 1

T ≠ 14 ˜D(P)

(4)

3

Where for F equal to either R or P we will use the shorthand:
˜D(F) := 1 + ·CF dE(F) + 8ÒdE(F)(4CF +2‡2

log(32T 3)) + 82‡2

Theorem 1 is a general result that applies to almost all RL settings of interest. In particular 
we note that any bounded function is sub-Gaussian. To clarify the assymptotics if this bound
we use another classical measure of dimensionality.
Deﬁnition 1. The Kolmogorov dimension of a function class F is given by:

F nF dE(F)T.

F

dimK(F) := lim sup
–¿0

log(N(F –  Î·Î 2))

log(1/–)

.

(5)

Using Deﬁnition 1 in Theorem 1 we can obtain our Corollary.
Corollary 1 (Assymptotic regret bounds for PSRL in parameterized MDPs).
Under the assumptions of Theorem 1 and writing dK(F) := dimK(F):
E[Regret(T ﬁ P S  Mú)] = ˜O1 ‡RdK(R)dE(R)T + E[Kú]‡PdK(P)dE(P)T 2
Where ˜O(·) ignores terms logarithmic in T.
In Section 4 we provide bounds on the eluder dimension of several function classes. These
lead to explicit regret bounds in a number of important domains such as discrete MDPs 
linear-quadratic control and even generalized linear systems. In all of these cases the eluder
dimension scales comparably with more traditional notions of dimensionality. For clarity 
we present bounds in the case of linear-quadratic control.
Corollary 2 (Assymptotic regret bounds for PSRL in bounded linear quadratic systems).
Let Mú be an n-dimensional linear-quadratic system with ‡-sub-Gaussian noise. If the state
is Î·Î 2-bounded by C and „ is the distribution of Mú  then:

E[Regret(T ﬁ P S  Mú)] = ˜O1‡C⁄1n2ÔT 2 .
(6)
Here ⁄1 is the largest eigenvalue of the matrix Q given as the solution of the Ricatti equations
for the unconstrained optimal value function V (s) = ≠sT Qs [22].
Proof. We simply apply the results of for eluder dimension in Section 4 to Corollary 1 and
upper bound the Lipschitz constant of the constrained LQR by 2C⁄1  see Appendix D.

Algorithms based upon posterior sampling are intimately linked to those based upon opti-
mism [14]. In Appendix E we outline an optimistic variant that would attain similar regret
bounds but with high probility in a frequentist sense. Unfortunately this algorithm remains
computationally intractable even when presented with an approximate MDP planner. Fur-
ther  we believe that PSRL will generally be more statistically ecient than an optimistic
variant with similar regret bounds since the algorithm is not aected by loose analysis [11].

4 Eluder dimension
To quantify the complexity of learning in a potentially inﬁnite MDP  we extend the existing
notion of eluder dimension for real-valued functions [19] to vector-valued functions. For any
we deﬁne the set of mean functions F = E[G] := {f|f = E[G] for G œG} . If
G™P C ‡
X  Y
we consider sequential observations yi ≥ Gú(xi) we can equivalently write them as yi =
fú(xi)+ ‘i for some fú(xi) = E[y|y ≥ Gú(xi)] and ‘i zero mean noise. Intuitively  the eluder
dimension of F is the length d of the longest possible sequence x1  ..  xd such that for all i 
knowing the function values of f(x1)  ..  f(xi) will not reveal f(xi+1).
Deﬁnition 2 ((F ‘ ) ≠ dependence).
We will say that x œX is (F ‘ )-dependent on {x1  ...  xn}™X

≈∆ ’f  ˜f œF  

nÿi=1 Îf(xi) ≠ ˜f(xi)Î2

2 Æ ‘2 =∆ Îf(x) ≠ ˜f(x)Î2 Æ ‘.

x œX is (‘ F)-independent of {x1  ..  xn} i it does not satisfy the deﬁnition for dependence.

4

Deﬁnition 3 (Eluder Dimension).
The eluder dimension dimE(F ‘ ) is the length of the longest possible sequence of elements
in X such that for some ‘Õ Ø ‘ every element is (F ‘ Õ)-independent of its predecessors.
Traditional notions from supervised learning  such as the VC dimension  are not sucient to
characterize the complexity of reinforcement learning. In fact  a family learnable in constant
time for supervised learning may require arbitrarily long to learn to control well [19]. The
eluder dimension mirrors the linear dimension for vector spaces  which is the length of the
longest sequence such that each element is linearly independent of its predecessors  but
allows for nonlinear and approximate dependencies. We overload our notation for G™P C ‡
X  Y
and write dimE(G ‘ ) := dimE(E[G] ‘ )  which should be clear from the context.
4.1 Eluder dimension for speciﬁc function classes
Theorem 1 gives regret bounds in terms of the eluder dimension  which is well-deﬁned for
any F ‘ . However  for any given F ‘ actually calculating the eluder dimension may take
some additional analysis. We now provide bounds on the eluder dimension for some common
function classes in a similar approach to earlier work for real-valued functions [14]. These
proofs are available in Appendix C.
Proposition 1 (Eluder dimension for ﬁnite X).
A counting argument shows that for |X| = X ﬁnite  any ‘> 0 and any function class F:

dimE(F ‘ ) Æ X

5

e

e

e

dimE(F ‘ ) Æ p(4n ≠ 1)

This bound is tight in the case of independent measurements.
Proposition 2 (Eluder dimension for linear functions).
Let F = {f |f(x) = ◊„(x) for ◊ œ Rn◊p „ œ Rp Î◊Î2 Æ C◊ Î„Î2 Æ C„} then ’X:
‘ 42B (4n ≠ 1)D + 1 = ˜O(np)
Proposition 3 (Eluder dimension for quadratic functions).
Let F = {f |f(x) = „(x)T ◊„(x) for ◊ œ Rp◊p „ œ Rp Î◊Î2 Æ C◊ Î„Î2 Æ C„} then ’X:
B2Rb (4p ≠ 1)TV + 1 = ˜O(p2).

e ≠ 1 logCA1 +32C„C◊
e ≠ 1 logSUQa1 +A2pC2

dimE(F ‘ ) Æ p(4p ≠ 1)

Proposition 4 (Eluder dimension for generalized linear functions).
Let g(·) be a component-wise independent function on Rn with derivative in each component
If F =
bounded œ [h  h] with h > 0. Deﬁne r = h
h > 1 to be the condition number.
{f |f(x) = g(◊„(x)) for ◊ œ Rn◊p „ œ Rp Î◊Î2 Æ C◊ Î„Î2 Æ C„} then for any X:
22464+1 = ˜O(r2np)
dimE(F ‘ ) Æ p!r2(4n ≠ 2) + 1"

e ≠ 13log5!r2(4n ≠ 2) + 1"31 +12C◊C„

„C◊
‘

5 Conﬁdence sets
We now follow the standard argument that relates the regret of an optimistic or pos-
terior sampling algorithm to the construction of conﬁdence sets [7  11]. We will use
the eluder dimension build conﬁdence sets for the reward and transition which contain
the true functions with high probability and then bound the regret of our algorithm by
the maximum deviation within the conﬁdence sets. For observations from fú œF we
will center the sets around the least squares estimate ˆf LS
œ arg minfœF L2 t(f) where
L2 t(f) :=qt≠1
2 is the cumulative squared prediciton error. The conﬁdence
t Î2 Et Æ Ô—t} where —t controls the growth
sets are deﬁned Ft = Ft(—t) := {f œF|Î f ≠ ˆf LS
2 Et :=qt≠1
of the conﬁdence set and the empirical 2-norm is deﬁned ÎgÎ2

i=1 Îf(xt) ≠ ytÎ2

i=1 Îg(xi)Î2
2.

t

‘

For F™P C ‡
X  Y

  we deﬁne the distinguished control parameter:

—út (F ” – ) := 8‡2 log(N(F –  Î·Î 2)/”) + 2–t18C +8‡2 log(4t2/”))2

This leads to conﬁdence sets which contain the true function with high probability.
Proposition 5 (Conﬁdence sets with high probability).
For all ”> 0 and –> 0 and the conﬁdence sets Ft = Ft(—út (F ” – )) for all t œ N then:

(7)

PAfú œ

Œ‹t=1FtB Ø 1 ≠ 2”

Proof. We combine standard martingale concentrations with a discretization scheme. The
argument is essentially the same as Proposition 6 in [14]  but extends statements about R
to vector-valued functions. A full derivation is available in the Appendix A.

5.1 Bounding the sum of set widths
We now bound the deviation from fú by the maximum deviation within the conﬁdence set.
Deﬁnition 4 (Set widths).
For any set of functions F we deﬁne the width of the set at x to be the maximum L2 deviation
between any two members of F evaluated at x.
wF(x) := sup
f  fœF

Îf(x) ≠ f(x)Î2

We can bound for the number of large widths in terms of the eluder dimension.
Lemma 1 (Bounding the number of large widths).

If {—t > 0--t œ N} is a nondecreasing sequence with Ft = Ft(—t) then
‘2 + ·4 dimE(F ‘ )

1{wFtk (xtk+i) >‘ }Æ 34—T

mÿk=1

·ÿi=1

Proof. This result follows from proposition 8 in [14] but with a small adjustment to account
for episodes. A full proof is given in Appendix B.

We now use Lemma 1 to control the cumulative deviation through time.
Proposition 6 (Bounding the sum of widths).

If {—t > 0--t œ N} is nondecreasing with Ft = Ft(—t) and ÎfÎ2 Æ C for all f œF then:

wFtk (xtk+i) Æ 1 + ·C dimE(F  T ≠1) + 4—T dimE(F  T ≠1)T

mÿk=1

·ÿi=1

(8)

.

mÿk=1

Proof. Once again we follow the analysis of Russo [14] and strealine notation by letting wt =
wFtk (xtk+i) abd d = dimE(F  T ≠1). Reordering the sequence (w1  ..  wT ) æ (wi1  ..  wiT )
such that wi1 Ø .. Ø wiT we have that:
wFtk (xtk+i) =

Tÿi=1
·ÿi=1
By the reordering we know that wit >‘ means that qm
t≠·d . So that if wit > T ≠1 then wit Æ min{C Ò 4—T d
From Lemma 1  ‘ ÆÒ 4—T d
Tÿi=1
1{wit Ø T ≠1}Æ ·Cd +

1{wit Ø T ≠1}
k=1q·
i=1 1{wFtk (xtk+i) >‘ }Ø t.
t≠·d }. Therefore 
0 Ú d
t ≠ ·d Æ ·Cd +2—T⁄ T
dt Æ ·Cd +4—T dT

Tÿt=·d+1Ú 4—T d

wit Æ 1 +

Tÿt=1

wit

wit

t

6

6 Analysis
We will now show reproduce the decomposition of expected regret in terms of the Bellman
error [11]. From here  we will apply the conﬁdence set results from Section 5 to obtain
our regret bounds. We streamline our discussion of P M   RM   V M
µ by simply
writing ú in place of Mú or µú and k in place of Mk or µk where appropriate; for example
V úk i := V Mú
˜µk i.
The ﬁrst step in our ananlysis breaks down the regret by adding and subtracting the imagined
optimal reward of µk under the MDP Mk.

ú 1 ≠ V úk 1" (s0) =!V ú

k 1" (s0) +!V k
(9)
ú 1 ≠ V k
Here s0 is a distinguished initial state  but moving to general ﬂ(s) poses no real challenge.
Algorithms based upon optimism bound (V úú 1 ≠ V k
k 1) Æ 0 with high probability. For PSRL
we use Lemma 2 and the tower property to see that this is zero in expectation.
Lemma 2 (Posterior sampling).
If „ is the distribution of Mú then  for any ‡(Htk)-measurable function g 

k 1 ≠ V úk 1" (s0)

k =!V ú

and T M

µ i  U M
i

E[g(Mú)|Htk] = E[g(Mk)|Htk]

(10)
µ   which for any MDP M = (S A  RM   P M  · ﬂ ) 

We introduce the Bellman operator T M
stationary policy µ : SæA and value function V : Sæ R  is deﬁned by
P M(sÕ|s  µ(s))V (sÕ).

µ V (s) := rM(s  µ(s)) +⁄sÕœS
T M

This returns the expected value of state s where we follow the policy µ under the laws of M 
for one time step. The following lemma gives a concise form for the dynamic programming
paradigm in terms of the Bellman operator.
Lemma 3 (Dynamic programming equation).
For any MDP M = (S A  RM   P M  · ﬂ ) and policy µ : S◊{ 1  . . .  · }æA   the value
functions V M
µ
(11)

satisfy

µ ·+1 := 0.

for i = 1 . . .·   with V M
Through repeated application of the dynamic programming operator and taking expectation
of martingale dierences we can mirror earlier analysis [11] to equate expected regret with
the cumulative Bellman error:

µ i = T M
V M

µ(· i)V M

µ i+1

E[k] =

(T k
k i ≠T úk i)V k

k i+1(stk+i)

(12)

·ÿi=1

6.1 Lipschitz continuity
Ecient regret bounds for MDPs with an inﬁnite number of states and actions require some
regularity assumption. One natural notion is that nearby states might have similar optimal
values  or that the optimal value function function might be Lipschitz. Unfortunately  any
discontinuous reward function will usually lead to discontious values functions so that this
assumption is violated in many settings of interest. However  we only require that the
future value is Lipschitz in the sense of equation (3). This will will be satisﬁed whenever the
underlying value function is Lipschitz  but is a strictly weaker requirement since the system
noise helps to smooth future values.
Since P has ‡P -sub-Gaussian noise we write st+1 = pM(st  at) + ‘P
in the natural way. We
now use equation (12) to reduce regret to a sum of set widths. To reduce clutter and more
closely follow the notation of Section 4 we will write xk i = (stk+i  atk+i).

t

E[k] Æ EC ·ÿi=1)rk(xk i) ≠ rú(xk i) + U k

i (P ú(xk i))*D
Æ EC ·ÿi=1)|rk(xk i) ≠ rú(xk i)| + KkÎpk(xk i) ≠ pú(xk i)Î2*D

i (P k(xk i)) ≠ U k

(13)

7

Æ

E[Kú]

mÿk=1

·ÿi=1

Where Kk is a global Lipschitz constant for the future value function of Mk as per (3).
We now use the results from Sections 4 and 5 to form the corresponding conﬁdence sets
Rk := Rtk(—ú(R ” – )) and Pk := Ptk(—ú(P ” – )) for the reward and transition functions
respectively. Let A = {Rú  Rk œR k ’k} and B = {P ú  Pk œP k ’k} and condition upon
these events to give:

E[Regret(T ﬁ P S  Mú)] Æ EC mÿk=1

·ÿi=1)|rk(xk i) ≠ rú(xk i)| + KkÎpk(xk i) ≠ pú(xk i)Î2*D
·ÿi=1)wRk(xk i) + E[Kk|A  B]wPk(xk i) + 8”(CR + CP)* (14)
The posterior sampling lemma ensures that E[Kk] = E[Kú] so that E[Kk|A  B] Æ E[Kú]
1≠8” by a union bound on {Ac ﬁ Bc}. We ﬁx ” = 1/8T to see that:
wRk(xk i) + E[Kú]11 + 1
T ≠ 12 mÿk=1
·ÿi=1
wPt(xk i)
E[Regret(T ﬁ P S  Mú)] Æ (CR + CP) +
We now use equation (7) together with Proposition 6 to obtain our regret bounds. For ease
of notation we will write dE(R) = dimE(R  T ≠1) and dE(P) = dimE(P  T ≠1).
E[Regret(T ﬁ P S  Mú)] Æ 2 + (CR + CP) + ·(CRdE(R) + CP dE(P)) +

P(A B) Æ

mÿk=1

F

(16)

4Ò—úT (R  1/8T –)dE(R)T + 4Ò—úT (P  1/8T –)dE(P)T(15)
We let – = 1/T 2 and write nF = log(8N(F  1/T 2 Î·Î 2)T) for R and P to complete our
proof of Theorem 1:

E[Regret(T ﬁ P S  Mú)] Æ#CR + CP$ + ˜D(R) + E[Kú]31 + 1

T ≠ 14 ˜D(P)
Where ˜D(F) is shorthand for 1 + ·CF dE(F) + 8ÒdE(F)(4CF +2‡2
log(32T 3)) +
82‡2
F nF dE(F)T. The ﬁrst term [CR + CP] bounds the contribution from missed con-
ﬁdence sets. The cost of learning the reward function Rú is bounded by ˜D(R). In most
problems the remaining contribution bounding transitions and lost future value will be
dominant. Corollary 1 follows from the Deﬁnition 1 together with nR and nP.
7 Conclusion
We present a new analysis of posterior sampling for reinforcement learning that leads to
a general regret bound in terms of the dimensionality  rather than the cardinality  of the
underlying MDP. These are the ﬁrst regret bounds for reinforcement learning in such a
general setting and provide new state of the art guarantees when specialized to several
important problem settings. That said  there are a few clear shortcomings which we do not
address in the paper. First  we assume that it is possible to draw samples from the posterior
distribution exactly and in some cases this may require extensive computational eort.
Second  we wonder whether it is possible to extend our analysis to learning in MDPs without
episodic resets. Finally  there is a fundamental hurdle to model-based reinforcement learning
that planning for the optimal policy even in a known MDP may be intractable. We assume
access to an approximate MDP planner  but this will generally require lengthy computations.
We would like to examine whether similar bounds are attainable in model-free learning
[23]  which may obviate complicated MDP planning  and examine the computational and
statistical eciency tradeos between these methods.

Acknowledgments
Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work
was supported in part by Award CMMI-0968707 from the National Science Foundation.

8

References
[1] Apostolos Burnetas and Michael Katehakis. Optimal adaptive policies for Markov decision

processes. Mathematics of Operations Research  22(1):222–255  1997.

[2] Tze Leung Lai and Herbert Robbins. Asymptotically ecient adaptive allocation rules. Ad-

vances in applied mathematics  6(1):4–22  1985.

[3] Leslie Pack Kaelbling  Michael L Littman  and Andrew W Moore. Reinforcement learning: A

survey. arXiv preprint cs/9605103  1996.

[4] Leslie G Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142 

1984.

[5] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold

algorithm. Machine learning  2(4):285–318  1988.

[6] Lihong Li  Michael L Littman  Thomas J Walsh  and Alexander L Strehl. Knows what it

knows: a framework for self-aware learning. Machine learning  82(3):399–443  2011.

[7] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. The Journal of Machine Learning Research  99:1563–1600  2010.

[8] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine Learning  49(2-3):209–232  2002.

[9] Ronen Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. The Journal of Machine Learning Research  3:213–231 
2003.

[10] Alexander Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael Littman. Pac model-
free reinforcement learning. In Proceedings of the 23rd international conference on Machine
learning  pages 881–888. ACM  2006.

[11] Ian Osband  Daniel Russo  and Benjamin Van Roy. (More) Ecient Reinforcement Learning

via Posterior Sampling. Advances in Neural Information Processing Systems  2013.

[12] Peter Auer. Using conﬁdence bounds for exploitation-exploration trade-os. The Journal of

Machine Learning Research  3:397–422  2003.

[13] S´ebastien Bubeck  R´emi Munos  Gilles Stoltz  and Csaba Szepesv´ari. X-armed bandits. Journal

of Machine Learning Research  12:1587â1627  2011.

[14] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. CoRR 

abs/1301.2609  2013.

[15] Ian Osband and Benjamin Van Roy. Near-optimal regret bounds for reinforcement learning in

factored MDPs. arXiv preprint arXiv:1403.3741  2014.

[16] Yassin Abbasi-Yadkori  D´avid P´al  and Csaba Szepesv´ari.

Improved algorithms for linear

stochastic bandits. Advances in Neural Information Processing Systems  24  2011.

[17] Morteza Ibrahimi  Adel Javanmard  and Benjamin Van Roy. Ecient reinforcement learning

for high dimensional linear quadratic systems. In NIPS  pages 2645–2653  2012.

[18] Ronald Ortner  Daniil Ryabko  et al. Online regret bounds for undiscounted continuous rein-

forcement learning. In NIPS  pages 1772–1780  2012.

[19] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of opti-
mistic exploration. In Advances in Neural Information Processing Systems  pages 2256–2264 
2013.

[20] William Thompson. On the likelihood that one unknown probability exceeds another in view

of the evidence of two samples. Biometrika  25(3/4):285–294  1933.

[21] Malcom Strens. A Bayesian framework for reinforcement learning. In Proceedings of the 17th

International Conference on Machine Learning  pages 943–950  2000.

[22] Dimitri Bertsekas. Dynamic programming and optimal control  volume 1. Athena Scientiﬁc

Belmont  MA  1995.

[23] Benjamin Van Roy and Zheng Wen. Generalization and exploration via randomized value

functions. arXiv preprint arXiv:1402.0635  2014.

9

,Ian Osband
Benjamin Van Roy
Jian Li
Yong Liu
Rong Yin
Hua Zhang
Lizhong Ding
Weiping Wang