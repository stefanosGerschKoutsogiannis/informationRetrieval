2012,Classification Calibration Dimension for General Multiclass Losses,We study consistency properties of surrogate loss functions for general multiclass classification problems  defined by a general loss matrix. We extend the notion of classification calibration  which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems)  to the general multiclass setting  and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting. We then introduce the notion of \emph{classification calibration dimension} of a multiclass loss matrix  which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity  and use these results to analyze various loss matrices. In particular  as one application  we provide a different route from the recent result of Duchi et al.\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.,Classiﬁcation Calibration Dimension for

General Multiclass Losses

Harish G. Ramaswamy

Shivani Agarwal

Department of Computer Science and Automation
Indian Institute of Science  Bangalore 560012  India

{harish gurup shivani}@csa.iisc.ernet.in

Abstract

We study consistency properties of surrogate loss functions for general multiclass
classiﬁcation problems  deﬁned by a general loss matrix. We extend the notion
of classiﬁcation calibration  which has been studied for binary and multiclass 0-1
classiﬁcation problems (and for certain other speciﬁc learning problems)  to the
general multiclass setting  and derive necessary and sufﬁcient conditions for a
surrogate loss to be classiﬁcation calibrated with respect to a loss matrix in this
setting. We then introduce the notion of classiﬁcation calibration dimension of a
multiclass loss matrix  which measures the smallest ‘size’ of a prediction space
for which it is possible to design a convex surrogate that is classiﬁcation cali-
brated with respect to the loss matrix. We derive both upper and lower bounds on
this quantity  and use these results to analyze various loss matrices. In particular 
as one application  we provide a different route from the recent result of Duchi
et al. (2010) for analyzing the difﬁculty of designing ‘low-dimensional’ convex
surrogates that are consistent with respect to pairwise subset ranking losses. We
anticipate the classiﬁcation calibration dimension may prove to be a useful tool in
the study and design of surrogate losses for general multiclass learning problems.

Introduction

1
There has been signiﬁcant interest and progress in recent years in understanding consistency of
learning methods for various ﬁnite-output learning problems  such as binary classiﬁcation  multi-
class 0-1 classiﬁcation  and various forms of ranking and multi-label prediction problems [1–15].
Such ﬁnite-output problems can all be viewed as instances of a general multiclass learning problem 
whose structure is deﬁned by a loss function  or equivalently  by a loss matrix. While the studies
above have contributed to the understanding of learning problems corresponding to certain forms
of loss matrices  a framework for analyzing consistency properties for a general multiclass learning
problem  deﬁned by a general loss matrix  has remained elusive.
In this paper  we analyze consistency of surrogate losses for general multiclass learning problems 
building on the results of [3  5–7] and others. We start in Section 2 with some background and
examples that will be used as running examples to illustrate concepts throughout the paper  and for-
malize the notion of classiﬁcation calibration with respect to a general loss matrix. In Section 3  we
derive both necessary and sufﬁcient conditions for classiﬁcation calibration with respect to general
multiclass losses; these are both of independent interest and useful in our later results. Section 4 in-
troduces the notion of classiﬁcation calibration dimension of a loss matrix  a fundamental quantity
that measures the smallest ‘size’ of a prediction space for which it is possible to design a convex sur-
rogate that is classiﬁcation calibrated with respect to the loss matrix. We derive both upper and lower
bounds on this quantity  and use these results to analyze various loss matrices. As one application 
in Section 5  we provide a different route from the recent result of Duchi et al. [10] for analyzing
the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to
certain pairwise subset ranking losses. We conclude in Section 6 with some future directions.

1

2 Preliminaries  Examples  and Background

+

Setup. We are given training examples (X1  Y1)  . . .   (Xm  Ym) drawn i.i.d. from a distribution D
on X ×Y  where X is an instance space and Y = [n] = {1  . . .   n} is a ﬁnite set of class labels. We
are also given a ﬁnite set T = [k] = {1  . . .   k} of target labels in which predictions are to be made 
and a loss function � : Y × T →[0 ∞)  where �(y  t) denotes the loss incurred on predicting t ∈ T
when the label is y ∈ Y. In many common learning problems  T = Y  but in general  these could
be different (e.g. when there is an‘abstain’ option available to a classiﬁer  in which case k = n + 1).
We will ﬁnd it convenient to represent the loss function � as a loss matrix L ∈ Rn×k
(here R+ =
[0 ∞))  and for each y ∈ [n]  t ∈ [k]  will denote by �yt the (y  t)-th element of L  �yt = (L)yt =
�(y  t)  and by �t the t-th column of L  �t = (�1t  . . .   �nt)� ∈ Rn. Some examples follow:
Example 1 (0-1 loss). Here Y = T = [n]  and the loss incurred is 1 if the predicted label t is
different from the actual class label y  and 0 otherwise: �0-1(y  t) = 1(t �= y)   where 1(·) is 1 if the
argument is true and 0 otherwise. The loss matrix L0-1 for n = 3 is shown in Figure 1(a).
Example 2 (Ordinal regression loss). Here Y = T = [n]  and predictions t farther away from the
actual class label y are penalized more heavily  e.g. using absolute distance: �ord(y  t) = |t − y| .
The loss matrix Lord for n = 3 is shown in Figure 1(b).
Example 3 (Hamming loss). Here Y = T = [2r] for some r ∈ N  and the loss incurred on
predicting t when the actual class label is y is the number of bit-positions in which the r-bit binary
representations of t − 1 and y − 1 differ: �Ham(y  t) =�r
i=1 1((t − 1)i �= (y − 1)i)   where for any
z ∈ {0  . . .   2r − 1}  zi ∈ {0  1} denotes the i-th bit in the r-bit binary representation of z. The loss
matrix LHam for r = 2 is shown in Figure 1(c). This loss is used in sequence labeling tasks [16].
Example 4 (‘Abstain’ loss). Here Y = [n] and T = [n+1]  where t = n+1 denotes ‘abstain’. One
possible loss function in this setting assigns a loss of 1 to incorrect predictions in [n]  0 to correct
2 1(t = n + 1) . The loss
predictions  and 1
matrix L(?) for n = 3 is shown in Figure 1(d).
The goal in the above setting is to learn from the training examples a function h : X→[k] with low
expected loss on a new example drawn from D  which we will refer to as the �-risk of h:

2 for abstaining: �(?)(y  t) = 1(t �= y) 1(t ∈ [n]) + 1

D[h] �= E(X Y )∼D�(Y  h(X)) = EX
er�

(1)
where py(x) = P(Y = y | X = x) under D  and p(x) = (p1(x)  . . .   pn(x))� ∈ Rn denotes the
conditional probability vector at x. In particular  the goal is to learn a function with �-risk close to
the optimal �-risk  deﬁned as

py(X)�(y  h(X)) = EX p(X)��h(X)  

n�y=1

er� ∗D

�= inf

h:X→[k]

er�

D[h] = inf

EX p(X)��h(X) = EX min
t∈[k]

h:X→[k]

p(X)��t .

(2)

Minimizing the discrete �-risk directly is typically difﬁcult computationally; consequently  one usu-

py(X)ψ(y  f (X)) .

D[f ] �= E(X Y )∼Dψ(Y  f (X)) = EX
erψ

ally employs a surrogate loss function ψ : Y × �T →R+ operating on a surrogate target space
�T ⊆ Rd for some appropriate d ∈ N 1 and minimizes (approximately  based on the training sample)
the ψ-risk instead  deﬁned for a (vector) function f : X→�T as
n�y=1
The learned function f : X→�T is then used to make predictions in [k] via some transformation pred :
�T →[k]: the prediction on a new instance x ∈ X is given by pred(f (x))  and the �-risk incurred is
er�
D[pred◦ f ]. As an example  several algorithms for multiclass classiﬁcation with respect to 0-1 loss
learn a function of the form f : X→Rn and predict according to pred(f (x)) = argmaxt∈[n]ft(x).
Below we will ﬁnd it useful to represent the surrogate loss function ψ via n real-valued functions
ψy : �T →R+ deﬁned as ψy(ˆt) = ψ(y  ˆt) for y ∈ [n]  or equivalently  as a vector-valued function
ψ : �T →Rn
where for any A ⊆ Rn  conv(A) denotes the convex hull of A.
1Equivalently  one can deﬁne ψ : Y × Rd→ ¯R+  where ¯R+ = R+ ∪ {∞} and ψ(y  ˆt) = ∞ ∀ˆt /∈ �T .

Rψ �=�ψ(ˆt) : ˆt ∈ �T�

+ deﬁned as ψ(ˆt) = (ψ1(ˆt)  . . .   ψn(ˆt))�. We will also deﬁne the sets

and Sψ �= conv(Rψ)  

(3)

(4)

2

1
1

0 �

� 0 1

1 0
1 1
(a)

2
1

0 �

� 0 1

1 0
2 1
(b)

0
1
1
2



1
1
2
0
0
2
1
1
(c)

2
1
1
0



0
1
1



1
1
1
0
1
0
(d)

1
2
1
2
1
2



Figure 1: Loss matrices corresponding to Examples 1-4: (a) L0-1 for n = 3; (b) Lord for n = 3; (c)
LHam for r = 2 (n = 4); (d) L(?) for n = 3.

erψ

D[f ] = inf

�= inf
f :X→�T

EX p(X)�ψ(f (X)) = EX inf
z∈Rψ

Under suitable conditions  algorithms that approximately minimize the ψ-risk based on a training
sample are known to be consistent with respect to the ψ-risk  i.e. to converge (in probability) to the
optimal ψ-risk  deﬁned as
erψ ∗D

p(X)�z .
(5)
This raises the natural question of whether  for a given loss �  there are surrogate losses ψ for which
consistency with respect to the ψ-risk also guarantees consistency with respect to the �-risk  i.e.
guarantees convergence (in probability) to the optimal �-risk (deﬁned in Eq. (2)). This question has
been studied in detail for the 0-1 loss  and for square losses of the form �(y  t) = ay1(t �= y)  which
can be analyzed similarly to the 0-1 loss [6  7]. In this paper  we consider this question for general
multiclass losses � : [n] × [k]→R+  including rectangular losses with k �= n. The only assumption
we make on � is that for each t ∈ [k]  ∃p ∈ Δn such that argmint�∈[k]p��t� = {t} (otherwise the
label t never needs to be predicted and can simply be ignored).2

p(X)�z = EX inf
z∈Sψ

f :X→�T

Deﬁnitions and Results. We will need the following deﬁnitions and basic results  generalizing
those of [5–7]. The notion of classiﬁcation calibration will be central to our study; as Theorem 3
below shows  classiﬁcation calibration of a surrogate loss ψ w.r.t. � corresponds to the property that
consistency w.r.t. ψ-risk implies consistency w.r.t. �-risk. Proofs of these results are straightforward
generalizations of those in [6  7] and are omitted.

p�ψ(ˆt) .

ˆt∈�T

∀p ∈ P :

inf

∀p ∈ P :

p�ψ(ˆt) > inf

ˆt∈�T :pred(ˆt) /∈argmintp��t

Deﬁnition 1 (Classiﬁcation calibration). A surrogate loss function ψ : [n] × �T →R+ is said to be
classiﬁcation calibrated with respect to a loss function � : [n]× [k]→R+ over P ⊆ Δn if there exists
a function pred : �T →[k] such that
Lemma 2. Let � : [n] × [k]→R+ and ψ : [n] × �T →R+. Then ψ is classiﬁcation calibrated with
respect to � over P ⊆ Δn iff there exists a function pred� : Sψ→[k] such that
p�z .
Theorem 3. Let � : [n] × [k]→R+ and ψ : [n] × �T →R+. Then ψ is classiﬁcation calibrated with
respect to � over Δn iff ∃ a function pred : �T →[k] such that for all distributions D on X × [n] and
all sequences of random (vector) functions fm : X→�T (depending on (X1  Y1)  . . .   (Xm  Ym)) 3
Deﬁnition 4 (Positive normals). Let ψ : [n] × �T →R+. For each point z ∈ Sψ  the set of positive
Deﬁnition 5 (Trigger probabilities). Let � : [n] × [k]→R+. For each t ∈ [k]  the set of trigger
probabilities of t with respect to � is deﬁned as

NSψ (z) �=�p ∈ Δn : p�(z − z�) ≤ 0 ∀z� ∈ Sψ� .

D[pred ◦ fm] P−→ er� ∗D .
er�

D[fm] P−→ erψ ∗D
erψ

normals at z is deﬁned as4

p�z > inf
z∈Sψ

z∈Sψ:pred�(z) /∈argmintp��t

inf

implies

3

t

Examples of trigger probability sets for various losses are shown in Figure 2.

�=�p ∈ Δn : p�(�t − �t� ) ≤ 0 ∀t� ∈ [k]� = �p ∈ Δn : t ∈ argmint�∈[k]p��t�� .

Q�
2Here Δn denotes the probability simplex in Rn  Δn = {p ∈ Rn : pi ≥ 0 ∀ i ∈ [n] �n
3Here P−→ denotes convergence in probability.
4The set of positive normals is non-empty only at points z in the boundary of Sψ.

i=1 pi = 1}.

Q0-1
Q0-1
Q0-1

1 = {p ∈ Δ3 : p1 ≥ max(p2  p3)}
2 = {p ∈ Δ3 : p2 ≥ max(p1  p3)}
3 = {p ∈ Δ3 : p3 ≥ max(p1  p2)}

Qord
1 = {p ∈ Δ3 : p1 ≥ 1
2}
Qord
2 = {p ∈ Δ3 : p1 ≤ 1
2   p3 ≤ 1
2}
Qord
3 = {p ∈ Δ3 : p3 ≥ 1
2}

(a)

(b)

Q(?)
1 = {p ∈ Δ3 : p1 ≥ 1
2}
Q(?)
2 = {p ∈ Δ3 : p2 ≥ 1
2}
Q(?)
3 = {p ∈ Δ3 : p3 ≥ 1
2}
Q(?)
4 = {p ∈ Δ3 : max(p1  p2  p3) ≤ 1
2}

(c)

Figure 2: Trigger probability sets for (a) 0-1 loss �0-1; (b) ordinal regression loss �ord; and (c) ‘ab-
stain’ loss �(?); all for n = 3  for which the probability simplex can be visualized easily. Calculations
of these sets can be found in the appendix. We note that such sets have also been studied in [17 18].

3 Necessary and Sufﬁcient Conditions for Classiﬁcation Calibration

We start by giving a necessary condition for classiﬁcation calibration of a surrogate loss ψ with
respect to any multiclass loss � over Δn  which requires the positive normals of all points z ∈ Sψ to
be ‘well-behaved’ w.r.t. � and generalizes the ‘admissibility’ condition used for 0-1 loss in [7]. All
proofs not included in the main text can be found in the appendix.
Theorem 6. Let ψ : [n] × �T →R+ be classiﬁcation calibrated with respect to � : [n] × [k]→R+
over Δn. Then for all z ∈ Sψ  there exists some t ∈ [k] such that NSψ (z) ⊆ Q�
t.
We note that  as in [7]  it is possible to give a necessary and sufﬁcient condition for classiﬁcation
calibration in terms of a similar property holding for positive normals associated with projections of
Sψ in lower dimensions. Instead  below we give a different sufﬁcient condition that will be helpful
in showing classiﬁcation calibration of certain surrogates. In particular  we show that for a surrogate
loss ψ to be classiﬁcation calibrated with respect to � over Δn  it is sufﬁcient for the above property
of positive normals to hold only at a ﬁnite number of points in Rψ  as long as their positive normal
sets jointly cover Δn:
Theorem 7. Let � : [n]×[k]→R+ and ψ : [n]×�T →R+. Suppose there exist r ∈ N and z1  . . .   zr ∈
Rψ such that�r
ψ is classiﬁcation calibrated with respect to � over Δn.
Computation of NSψ (z). The conditions in the above results both involve the sets of positive
normals NSψ (z) at various points z ∈ Sψ. Thus in order to use the above results to show that a
surrogate ψ is (or is not) classiﬁcation calibrated with respect to a loss �  one needs to be able to
compute or characterize the sets NSψ (z). Here we give a method for computing these sets for certain
surrogate losses ψ and points z ∈ Sψ.
Lemma 8. Let �T ⊆ Rd be a convex set and let ψ : �T →Rn
+ be convex.5 Let z = ψ(ˆt) for some
ˆt ∈ �T such that for each y ∈ [n]  the subdifferential of ψy at ˆt can be written as ∂ψy(ˆt) =
sy ∈ Rd.6 Let s =�n
conv({wy
1  . . .   wy
sn� ∈ Rd×s ;
1 . . . wn
where byj is 1 if the j-th column of A came from {wy
NSψ (z) =�p ∈ Δn : p = Bq for some q ∈ Null(A) ∩ Δs�  

j=1 NSψ (zj) = Δn and for each j ∈ [r]  ∃t ∈ [k] such that NSψ (zj) ⊆ Q�

where Null(A) ⊆ Rs denotes the null space of the matrix A.
5A vector function is convex if all its component functions are convex.
6Recall that the subdifferential of a convex function φ : Rd→ ¯R+ at a point u0 ∈ Rd is deﬁned as
∂φ(u0) =�w ∈ Rd : φ(u) − φ(u0) ≥ w�(u − u0) ∀u ∈ Rd� and is a convex set in Rd (e.g. see [19]).

y=1 sy  and let
B = [byj] ∈ Rn×s  

sy}) for some sy ∈ N and wy
s2 . . . . . . wn
1 . . . w1

1 . . . w2

s1 w2

1  . . .   wy

sy} and 0 otherwise. Then

t. Then

1  . . .   wy

A =�w1

4

We give an example illustrating the use of Theorem 7 and Lemma 8 to show classiﬁcation calibration
of a certain surrogate loss with respect to the ordinal regression loss �ord deﬁned in Example 2:
Example 5 (Classiﬁcation calibrated surrogate for ordinal regression loss). Consider the ordinal

deﬁned as (see Figure 3)

ψ(y  ˆt) = |ˆt − y|

regression loss �ord deﬁned in Example 2 for n = 3. Let �T = R  and let ψ : {1  2  3} × R→R+ be
Thus Rψ =�ψ(ˆt) =�|ˆt − 1| |ˆt − 2| |ˆt − 3|�� : ˆt ∈ R�. We will show there are 3 points in Rψ
satisfying the conditions of Theorem 7. Speciﬁcally  consider ˆt1 = 1  ˆt2 = 2  and ˆt3 = 3  giving
z1 = ψ(ˆt1) = (0  1  2)�  z2 = ψ(ˆt2) = (1  0  1)�  and z3 = ψ(ˆt3) = (2  1  0)� in Rψ. Observe
that �T here is a convex set and ψ : �T →R3 is a convex function. Moreover  for ˆt1 = 1  we have

∀y ∈ {1  2  3}  ˆt ∈ R .

(6)

∂ψ1(1) = [−1  1] = conv({+1 −1}) ;
∂ψ2(1) = {−1} = conv({−1}) ;
∂ψ3(1) = {−1} = conv({−1}) .

0
0

0
0

1
0
0

0
1
0

This gives

1 � .

Therefore  we can use Lemma 8 to compute NSψ (z1). Here
s = 4  and
A = [ +1 −1 −1 −1 ] ; B =� 1
NSψ (z1) = �p ∈ Δ3 : p = (q1 + q2  q3  q4) for some q ∈ Δ4  q1 − q2 − q3 − q4 = 0�

= �p ∈ Δ3 : p = (q1 + q2  q3  q4) for some q ∈ Δ4  q1 = 1
2�
= �p ∈ Δ3 : p1 ≥ 1
2�
= Qord
1 .
A similar procedure yields NSψ (z2) = Qord
ψ is classiﬁcation calibrated with respect to �ord over Δ3.
We note that in general  computational procedures such as Fourier-Motzkin elimination [20] can be
helpful in computing NSψ (z) via Lemma 8.
4 Classiﬁcation Calibration Dimension

3 . Thus  by Theorem 7  we get that

2 and NSψ (z3) = Qord

Figure 3: The surrogate ψ

We now turn to the study of a fundamental quantity associated with the property of classiﬁcation
calibration with respect to a general multiclass loss �. Speciﬁcally  in the above example  we saw
that to develop a classiﬁcation calibrated surrogate loss w.r.t. the ordinal regression loss for n = 3 

it was sufﬁcient to consider a surrogate target space �T = R  with dimension d = 1; in addition  this
yielded a convex surrogate ψ : R→R3
+ which can be used in developing computationally efﬁcient
algorithms. In fact the same surrogate target space with d = 1 can be used to develop a similar
convex  classiﬁcation calibrated surrogate loss w.r.t. the ordinal regression loss for any n ∈ N.
However not all losses � have such ‘low-dimensional’ surrogates. This raises the natural question
of what is the smallest dimension d that supports a convex classiﬁcation calibrated surrogate for a
given multiclass loss �  and leads us to the following deﬁnition:
Deﬁnition 9 (Classiﬁcation calibration dimension). Let � : [n] × [k]→R+. Deﬁne the classiﬁcation
calibration dimension (CC dimension) of � as

CCdim(�) �= min�d ∈ N : ∃ a convex set �T ⊆ Rd and a convex surrogate ψ : �T →Rn

that is classiﬁcation calibrated w.r.t. � over Δn�  

if the above set is non-empty  and CCdim(�) = ∞ otherwise.
From the above discussion  CCdim(�ord) = 1 for all n. In the following  we will be interested in
developing an understanding of the CC dimension for general losses �  and in particular in deriving
upper and lower bounds on this.

+

5

4.1 Upper Bounds on the Classiﬁcation Calibration Dimension

We start with a simple result that establishes that the CC dimension of any multiclass loss � is ﬁnite 
and in fact is strictly smaller than the number of class labels n.

Lemma 10. Let � : [n] × [k]→R+. Let �T =�ˆt ∈ Rn−1
ψy : �T →R+ be given by

:�n−1
ψy(ˆt) = 1(y �= n) (ˆty − 1)2 + �j∈[n−1] j�=y

j=1

+

ˆtj

2 .

ˆtj ≤ 1�  and for each y ∈ [n]  let

In particular  since ψ is convex 

Then ψ is classiﬁcation calibrated with respect to � over Δn.
CCdim(�) ≤ n − 1.
It may appear surprising that the convex surrogate ψ in the above lemma is classiﬁcation calibrated
with respect to all multiclass losses � on n classes. However this makes intuitive sense  since in
principle  for any multiclass problem  if one can estimate the conditional probabilities of the n
classes accurately (which requires estimating n−1 real-valued functions on X )  then one can predict
a target label that minimizes the expected loss according to these probabilities. Minimizing the above
surrogate effectively corresponds to such class probability estimation. Indeed  the above lemma can
be shown to hold for any surrogate that is a strictly proper composite multiclass loss [21].
In practice  when the number of class labels n is large (such as in a sequence labeling task  where n
is exponential in the length of the input sequence)  the above result is not very helpful; in such cases 
it is of interest to develop algorithms operating on a surrogate target space in a lower-dimensional
space. Next we give a different upper bound on the CC dimension that depends on the loss �  and
for certain losses  can be signiﬁcantly tighter than the general bound above.
Theorem 11. Let � : [n] × [k]→R+. Then CCdim(�) ≤ rank(L)  the rank of the loss matrix L.
Proof. Let rank(L) = d. We will construct a convex classiﬁcation calibrated surrogate loss ψ for �
with surrogate target space �T ⊆ Rd.
Let �t1   . . .   �td be linearly independent columns of L. Let {e1  . . .   ed} denote the standard basis
in Rd. We can deﬁne a linear function ˜ψ : Rd→Rn by

˜ψ(ej) = �tj ∀j ∈ [d] .

+ as
ψ(ˆt) = ˜ψ(ˆt) ;

we note that the resulting vectors are always in Rn

Then for each z in the column space of L  there exists a unique vector u ∈ Rd such that ˜ψ(u) = z.
In particular  there exist unique vectors u1  . . .   uk ∈ Rd such that for each t ∈ [k]  ˜ψ(ut) = �t.
Let �T = conv({u1  . . .   uk})  and deﬁne ψ : �T →Rn
t=1 αtut for
α ∈ Δk  ψ(ˆt) =�k
+ ∀t ∈ [k]. The function ψ is clearly convex. To show ψ is
classiﬁcation calibrated w.r.t. � over Δn  we will use Theorem 7. Speciﬁcally  consider the k points
zt = ψ(ut) = �t ∈ Rψ for t ∈ [k]. By deﬁnition of ψ  we have Sψ = conv({�1  . . .   �k}); from the
deﬁnitions of positive normals and trigger probabilities  it then follows that NSψ (zt) = NSψ (�t) =
t for all t ∈ [k]. Thus by Theorem 7  ψ is classiﬁcation calibrated w.r.t. � over Δn.
Q�
Example 6 (CC dimension of Hamming loss). Consider the Hamming loss �Ham deﬁned in Example
3  for n = 2r. For each i ∈ [r]  deﬁne σi ∈ Rn as

+  since by deﬁnition  for any ˆt =�k

t=1 αt�t  and �t ∈ Rn

if (y − 1)i  the i-th bit in the r-bit binary representation of (y − 1)  is 1
otherwise.

σiy =�+1

−1

Then the loss matrix LHam satisﬁes

LHam =

r
2

ee� −

1
2

σiσi�  

r�i=1

where e is the n × 1 all ones vector. Thus rank(LHam) ≤ r + 1  giving us CCdim(�Ham) ≤ r + 1.
For r ≥ 3  this is a signiﬁcantly tighter upper bound than the bound of 2r − 1 given by Lemma 10.

6

We note that the upper bound of Theorem 11 need not always be tight: for example  for the ordinal
regression loss  for which we already know CCdim(�ord) = 1  the theorem actually gives an upper
bound of n  which is even weaker than that implied by Lemma 10.

4.2 Lower Bound on the Classiﬁcation Calibration Dimension

In this section we give a lower bound on the CC dimension of a loss function � and illustrate it by
using it to calculate the CC dimension of the 0-1 loss. Section 5 we will explore consequences of
the lower bound for classiﬁcation calibrated surrogates for certain types of ranking losses. We will
need the following deﬁnition:
Deﬁnition 12. The feasible subspace dimension of a convex set C at p ∈ C  denoted by µC(p)  is
deﬁned as the dimension of the subspace FC(p) ∩ (−FC(p))  where FC(p) is the cone of feasible
directions of C at p.7
The following gives a lower bound on the CC dimension of a loss � in terms of the feasible subspace
dimension of the trigger probability sets Q�
Theorem 13. Let � : [n]× [k]→R+. Then for all p ∈ relint(Δn) and t ∈ arg mint� p��t� (i.e. such
that p ∈ Q�

t at certain points p ∈ Q�
t:

t): 8

CCdim(�) ≥ n − µQ�

t

(p) − 1 .

The proof requires extensions of the deﬁnition of positive normals and the necessary condition of
Theorem 6 to sequences of points in Sψ and is quite technical. In the appendix  we provide a proof
in the special case when p ∈ relint(Δn) is such that inf z∈Sψ p�z is achieved in Sψ  which does not
require these extensions. Full proof details will be provided in a longer version of the paper. Both
the proof of the lower bound and its applications make use of the following lemma  which gives a
method to calculate the feasible subspace dimension for certain convex sets C and points p ∈ C:
Lemma 14. Let C = �u ∈ Rn : A1u ≤ b1  A2u ≤ b2  A3u = b3�. Let p ∈ C be such that
A3��  the dimension of the null space of�A1
A3�.
A1p = b1  A2p < b2. Then µC(p) = nullity��A1

The above lower bound allows us to calculate precisely the CC dimension of the 0-1 loss:

Example 7 (CC dimension of 0-1 loss). Consider the 0-1 loss �0-1 deﬁned in Example 1. Take
for all t ∈ [k] = [n] (see Figure 2); in particular 
p = ( 1
we have p ∈ Q0-1

n   . . .   1

n )� ∈ relint(Δn). Then p ∈ Q0-1
t
1 . Now Q0-1
1 can be written as
Q0-1

1

= �q ∈ Δn : q1 ≥ qy ∀y ∈ {2  . . .   n}�
= �q ∈ Rn :�−en−1 In−1�q ≤ 0 −q ≤ 0  e�n q = 1}  

where en−1  en denote the (n − 1) × 1 and n × 1 all ones vectors  respectively  and In−1 denotes
the (n− 1)× (n− 1) identity matrix. Moreover  we have�−en−1 In−1�p = 0  −p < 0. Therefore 
by Lemma 14  we have

Thus by Theorem 13  we get CCdim(�0-1) ≥ n − 1. Combined with the upper bound of Lemma 10 
this gives CCdim(�0-1) = n − 1.

FA(p) = {v ∈ Rn : ∃�0 > 0 such that p + �v ∈ C ∀� ∈ (0  �0)}.

7For a set C ⊆ Rn and point p ∈ C  the cone of feasible directions of C at p is deﬁned as
8Here relint(Δn) denotes the relative interior of Δn: relint(Δn) = {p ∈ Δn : py > 0 ∀y ∈ [n]}.

7

(p) = nullity��−en−1 In−1

e�n

µ

Q0-1

1

�� = nullity



−1 1 0 . . . 0
−1 0 1 . . . 0

...

−1 0 0 . . . 1
1 1 1 . . . 1





= 0 .

5 Application to Pairwise Subset Ranking

We consider an application of the above framework to analyzing certain types of subset ranking
problems  where each instance x ∈ X consists of a query together with a set of r documents (for
simplicity  r ∈ N here is ﬁxed)  and the goal is to learn a predictor which given such an instance will
return a ranking (permutation) of the r documents [8]. Duchi et al. [10] showed recently that for
certain pairwise subset ranking losses �  ﬁnding a predictor that minimizes the �-risk is an NP-hard
problem. They also showed that several common pairwise convex surrogate losses that operate on

respect to such losses �  even under some low-noise conditions on the distribution  and proposed

�T = Rr (and are used to learn scores for the r documents) fail to be classiﬁcation calibrated with
an alternative convex surrogate  also operating on �T = Rr  that is classiﬁcation calibrated under

certain conditions on the distribution (i.e. over a strict subset of the associated probability simplex).
Here we provide an alternative route to analyzing the difﬁculty of obtaining consistent surrogates for
such pairwise subset ranking problems using the classiﬁcation calibration dimension. Speciﬁcally 
we will show that even for a simple setting of such problems  the classiﬁcation calibration dimension

of the underlying loss � is greater than r  and therefore no convex surrogate operating on �T ⊆ Rr
can be classiﬁcation calibrated w.r.t. such a loss over the full probability simplex.
Formally  we will identify the set of class labels Y with a set G of ‘preference graphs’  which are
simply directed acyclic graphs (DAGs) over r vertices; for each directed edge (i  j) in a preference
graph g ∈ G associated with an instance x ∈ X   the i-th document in the document set in x is
preferred over the j-th document. Here we will consider a simple setting where each preference
graph has exactly one edge  so that |Y| = |G| = r(r − 1); in this setting  we can associate each
g ∈ G with the edge (i  j) it contains  which we will write as g(i j). The target labels consist of
permutations over r objects  so that T = Sr with |T | = r!. Consider now the following simple
pairwise loss �pair : Y × T →R+:

(7)

(8)

1

�pair(g(i j)  σ) = 1�σ(i) > σ(j)� .
σ ∀σ ∈ T .

σ = 1

2 for all σ ∈ T .

σ1 − �pair

1

r(r−1)   . . .  
σ − �pair

σ� ) = 0 ∀σ  σ� ∈ T   and so p ∈ Qpair

r(r−1) )� ∈ relint(Δr(r−1))  and observe that p��pair

σ1   deﬁned by
σt ) ≤ 0 for t = 2  . . .   r! and
σ1 satisﬁes

Let p = (
Thus p�(�pair
Let (σ1  . . .   σr!) be any ﬁxed ordering of the permutations in T   and consider Qpair
the intersection of r! − 1 half-spaces of the form q�(�pair
the simplex constraints q ∈ Δr(r−1). Moreover  from the above observation  p ∈ Qpair
p�(�pair

σ1 − �pair
σt ) = 0 ∀t = 2  . . .   r!. Therefore  by Lemma 14  we get
σ1 − �pair

σr! )  e���  
where e is the r(r − 1) × 1 all ones vector. It is not hard to see that the set {�pair
: σ ∈ T } spans a
2 −1�.
dimensional space  and hence the nullity of the above matrix is at most r(r−1)−� r(r−1)
r(r−1)
Thus by Theorem 13  we get that CCdim(�pair) ≥ r(r − 1) −� r(r−1)
2 + 1� − 1 = r(r−1)
2 − 2 . In
particular  for r ≥ 5  this gives CCdim(�pair) > r  and therefore establishes that no convex surrogate
ψ operating on a surrogate target space �T ⊆ Rr can be classiﬁcation calibrated with respect to �pair
on the full probability simplex Δr(r−1).

(p) = nullity��(�pair

σ1 − �pair

σ2 )  . . .   (�pair

µ

Qpair

σ1

2

σ

6 Conclusion

We developed a framework for analyzing consistency for general multiclass learning problems de-
ﬁned by a general loss matrix  introduced the notion of classiﬁcation calibration dimension of a
multiclass loss  and used this to analyze consistency properties of surrogate losses for various gen-
eral multiclass problems. An interesting direction would be to develop a generic procedure for
designing consistent convex surrogates operating on a ‘minimal’ surrogate target space according to
the classiﬁcation calibration dimension of the loss matrix. It would also be of interest to extend the
results here to account for noise conditions as in [9  10].

8

Acknowledgments

We would like to thank the anonymous reviewers for helpful comments. HG thanks Microsoft
Research India for a travel grant. This research is supported in part by a Ramanujan Fellowship to
SA from DST and an Indo-US Joint Center Award from the Indo-US Science & Technology Forum.

References
[1] G´abor Lugosi and Nicolas Vayatis. On the bayes-risk consistency of regularized boosting

methods. Annals of Statistics  32(1):30–55  2004.

[2] Wenxin Jiang. Process consistency for AdaBoost. Annals of Statistics  32(1):13–29  2004.
[3] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. Annals of Statistics  32(1):56–134  2004.

[4] Ingo Steinwart. Consistency of support vector machines and other regularized kernel classi-

ﬁers. IEEE Transactions on Information Theory  51(1):128–142  2005.

[5] Peter Bartlett  Michael Jordan  and Jon McAuliffe. Convexity  classiﬁcation and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[6] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

[7] Ambuj Tewari and Peter Bartlett. On the consistency of multiclass classiﬁcation methods.

Journal of Machine Learning Research  8:1007–1025  2007.

[8] David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE

Transactions on Information Theory  54(11):5140–5154  2008.

[9] Fen Xia  Tie-Yan Liu  Jue Wang  Wensheng Zhang  and Hang Li. Listwise approach to learning

to rank: Theory and algorithm. In International Conference on Machine Learning  2008.

[10] John Duchi  Lester Mackey  and Michael Jordan. On the consistency of ranking algorithms. In

International Conference on Machine Learning  2010.

[11] Pradeep Ravikumar  Ambuj Tewari  and Eunho Yang. On NDCG consistency of listwise rank-
ing methods. In International Conference on Artiﬁcial Intelligence and Statistics(AISTATS) 
volume 15. JMLR: W&CP  2011.

[12] David Buffoni  Cl´ement Calauz`enes  Patrick Gallinari  and Nicolas Usunier. Learning scoring
functions with order-preserving losses and standardized supervision. In International Confer-
ence on Machine Learning  2011.

[13] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In Conference on

Learning Theory  2011.

[14] Wojciech Kotlowski  Krzysztof Dembczynski  and Eyke Huellermeier. Bipartite ranking
through minimization of univariate loss. In International Conference on Machine Learning 
2011.

[15] Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approx-

imation  26:225–287  2007.

[16] Ben Taskar  Carlos Guestrin  and Daphne Koller. Max-margin Markov networks. In Neural

Information Processing Systems  2003.

[17] Deirdre O’Brien  Maya Gupta  and Robert Gray. Cost-sensitive multi-class classiﬁcation from

probability estimates. In International Conference on Machine Learning  2008.

[18] Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions.

In ACM Conference on Electronic Commerce  2009.

[19] Dimitri Bertsekas  Angelia Nedic  and Asuman Ozdaglar. Convex Analysis and Optimization.

Athena Scientiﬁc  2003.

[20] Jean Gallier. Notes on convex sets  polytopes  polyhedra  combinatorial topology  Voronoi
diagrams and Delaunay triangulations. Technical report  Department of Computer and Infor-
mation Science  University of Pennsylvania  2009.

[21] Elodie Vernet  Robert C. Williamson  and Mark D. Reid. Composite multiclass losses.

Neural Information Processing Systems  2011.

In

9

,Supasorn Suwajanakorn
Noah Snavely
Jonathan Tompson
Mohammad Norouzi