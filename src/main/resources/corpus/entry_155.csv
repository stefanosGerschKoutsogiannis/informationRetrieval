2015,A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks,To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction  modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently  we derived such an algorithm  termed similarity matching  from a Multidimensional Scaling (MDS) objective function. However  in the existing algorithm  the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here  we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which  in the offline setting  are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn  the output eigenvalues are computed as i) soft-thresholded  ii) hard-thresholded  iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting  we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably  in the last two networks  neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.,A Normative Theory of Adaptive Dimensionality

Reduction in Neural Networks

Cengiz Pehlevan

Simons Center for Data Analysis

Simons Foundation
New York  NY 10010

Dmitri B. Chklovskii

Simons Center for Data Analysis

Simons Foundation
New York  NY 10010

cpehlevan@simonsfoundation.org

dchklovskii@simonsfoundation.org

Abstract

To make sense of the world our brains must analyze high-dimensional datasets
streamed by our sensory organs. Because such analysis begins with dimension-
ality reduction  modeling early sensory processing requires biologically plausible
online dimensionality reduction algorithms. Recently  we derived such an algo-
rithm  termed similarity matching  from a Multidimensional Scaling (MDS) ob-
jective function. However  in the existing algorithm  the number of output dimen-
sions is set a priori by the number of output neurons and cannot be changed. Be-
cause the number of informative dimensions in sensory inputs is variable there is a
need for adaptive dimensionality reduction. Here  we derive biologically plausible
dimensionality reduction algorithms which adapt the number of output dimensions
to the eigenspectrum of the input covariance matrix. We formulate three objective
functions which  in the ofﬂine setting  are optimized by the projections of the input
dataset onto its principal subspace scaled by the eigenvalues of the output covari-
ance matrix. In turn  the output eigenvalues are computed as i) soft-thresholded 
ii) hard-thresholded  iii) equalized thresholded eigenvalues of the input covari-
ance matrix.
In the online setting  we derive the three corresponding adaptive
algorithms and map them onto the dynamics of neuronal activity in networks with
biologically plausible local learning rules. Remarkably  in the last two networks 
neurons are divided into two classes which we identify with principal neurons and
interneurons in biological circuits.

1

Introduction

Our brains analyze high-dimensional datasets streamed by our sensory organs with efﬁciency and
speed rivaling modern computers. At the early stage of such analysis  the dimensionality of sensory
inputs is drastically reduced as evidenced by anatomical measurements. Human retina  for example 
conveys signals from ≈125 million photoreceptors to the rest of the brain via ≈1 million ganglion
cells [1] suggesting a hundred-fold dimensionality reduction. Therefore  biologically plausible di-
mensionality reduction algorithms may offer a model of early sensory processing.
In a seminal work [2] Oja proposed that a single neuron may compute the ﬁrst principal component
of activity in upstream neurons. At each time point  Oja’s neuron projects a vector composed of ﬁr-
ing rates of upstream neurons onto the vector of synaptic weights by summing up currents generated
by its synapses. In turn  synaptic weights are adjusted according to a Hebbian rule depending on the
activities of only the postsynaptic and corresponding presynaptic neurons [2].
Following Oja’s work  many multineuron circuits were proposed to extract multiple principal com-
ponents of the input  for a review see [3]. However  most multineuron algorithms did not meet the
same level of rigor and biological plausibility as the single-neuron algorithm [2  4] which can be
derived using a normative approach  from a principled objective function [5]  and contains only lo-

1

cal Hebbian learning rules. Algorithms derived from principled objective functions either did not
posess local learning rules [6  4  7  8] or had other biologically implausible features [9]. In other
algorithms  local rules were chosen heuristically rather than derived from a principled objective
function [10  11  12  9  3  13  14  15  16].
There is a notable exception to the above observation but it has other shortcomings. The two-
layer circuit with reciprocal synapses [17  18  19] can be derived from the minimization of the
representation error. However  the activity of principal neurons in the circuit is a dummy variable
without its own dynamics. Therefore  such principal neurons do not integrate their input in time 
contradicting existing experimental observations.
Other normative approaches use an information theoretical objective to compare theoretical lim-
its with experimentally measured information in single neurons or populations [20  21  22] or to
calculate optimal synaptic weights in a postulated neural network [23  22].
Recently  a novel approach to the problem has been proposed [24]. Starting with the Multidimen-
sional Scaling (MDS) strain cost function [25  26] we derived an algorithm which maps onto a
neuronal circuit with local learning rules. However  [24] had major limitations  which are shared by
vairous other multineuron algorithms:
1. The number of output dimensions was determined by the ﬁxed number of output neurons pre-
cluding adaptation to the varying number of informative components. A better solution would be
to let the network decide  depending on the input statistics  how many dimensions to represent
[14  15]. The dimensionality of neural activity in such a network would be usually less than the
maximum set by the number of neurons.

2. Because output neurons were coupled by anti-Hebbian synapses which are most naturally imple-
mented by inhibitory synapses  if these neurons were to have excitatory outputs  as suggested by
cortical anatomy  they would violate Dale’s law (i.e. each neuron uses only one fast neurotrans-
mitter). Here  following [10]  by anti-Hebbian we mean synaptic weights that get more negative
with correlated activity of pre- and postsynaptic neurons.

3. The output had a wide dynamic range which is difﬁcult to implement using biological neurons
with a limited range. A better solution [27  13] is to equalize the output variance across neurons.
In this paper  we advance the normative approach of [24] by proposing three new objective func-
tions which allow us to overcome the above limitations. We optimize these objective functions by
proceeding as follows. In Section 2  we formulate and solve three optimization problems of the
form:

Oﬄine setting : Y∗ = arg min

L (X  Y) .

Y

(1)

Here  the input to the network  X = [x1  . . .   xT ] is an n × T matrix with T centered input data
samples in Rn as its columns and the output of the network  Y = [y1  . . .   yT ] is a k×T matrix with
corresponding outputs in Rk as its columns. We assume T >> k and T >> n. Such optimization
problems are posed in the so-called ofﬂine setting where outputs are computed after seeing all data.
Whereas the optimization problems in the ofﬂine setting admit closed-form solution  such setting
is ill-suited for modeling neural computation on the mechanistic level and must be replaced by the
online setting.
Indeed  neurons compute an output  yT   for each data sample presentation  xT  
before the next data sample is presented and past outputs cannot be altered. In such online setting 
optimization is performed at every time step  T   on the objective which is a function of all inputs
and outputs up to time T . Moreover  an online algorithm (also known as streaming) is not capable
of storing all previous inputs and outputs and must rely on a smaller number of state variables.
In Section 3  we formulate three corresponding online optimization problems with respect to yT  
while keeping all the previous outputs ﬁxed:

Online setting : yT ← arg min

yT

L (X  Y) .

(2)

Then we derive algorithms solving these problems online and map their steps onto the dynamics of
neuronal activity and local learning rules for synaptic weights in three neural networks.
We show that the solutions of the optimization problems and the corresponding online algorithms
remove the limitations outlined above by performing the following computational tasks:

2

Figure 1:
Input-output
functions of the three
solutions
ofﬂine
and
im-
neural
network
the
plementations of
corresponding
online
algorithms. A-C. Input-
output
of
covariance eigenvalues.
A.
Soft-thresholding.
B. Hard-thresholding.
C. Equalization after
D-F.
thresholding.
Corresponding network
architectures.

functions

1. Soft-thresholding the eigenvalues of the input covariance matrix  Figure 1A: eigenvalues below
the threshold are set to zero and the rest are shrunk by the threshold magnitude. Thus  the num-
ber of output dimensions is chosen adaptively. This algorithm maps onto a single-layer neural
network with the same architecture as in [24]  Figure 1D  but with modiﬁed learning rules.

2. Hard-thresholding of input eigenvalues  Figure 1B: eigenvalues below the threshold vanish as
before  but eigenvalues above the threshold remain unchanged. The steps of such algorithm map
onto the dynamics of neuronal activity in a network which  in addition to principal neurons  has a
layer of interneurons reciprocally connected with principal neurons and each other  Figure 1E.

3. Equalization of non-zero eigenvalues  Figure 1C. The corresponding network’s architecture  Fig-
ure 1F  lacks reciprocal connections among interneurons. As before  the number of above-
threshold eigenvalues is chosen adaptively and cannot exceed the number of principal neurons. If
the two are equal  this network whitens the output.

In Section 4  we demonstrate that the online algorithms perform well on a synthetic dataset and  in
Discussion  we compare our neural circuits with biological observations.

2 Dimensionality reduction in the ofﬂine setting

In this Section  we introduce and solve  in the ofﬂine setting  three novel optimization problems
whose solutions reduce the dimensionality of the input. We state our results in three Theorems
which are proved in the Supplementary Material.

2.1 Soft-thresholding of covariance eigenvalues

We consider the following optimization problem in the ofﬂine setting:

where α ≥ 0 and IT is the T ×T identity matrix. To gain intuition behind this choice of the objective
function let us expand the squared norm and keep only the Y-dependent terms:

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y − αT IT

arg min

Y

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y(cid:13)(cid:13)2

F + 2αT Tr(cid:0)Y(cid:62)Y(cid:1)  

where the ﬁrst term matches the similarity of input and output[24] and the second term is a nuclear
norm of Y(cid:62)Y known to be a convex relaxation of the matrix rank used for low-rank matrix modeling
[28]. Thus  objective function (3) enforces low-rank similarity matching.
We show that the optimal output Y is a projection of the input data  X  onto its principal subspace.
The subspace dimensionality is set by m  the number of eigenvalues of the data covariance matrix 
C = 1

t   that are greater than or equal to the parameter α.

(cid:80)T
t=1 xtx(cid:62)

T XX(cid:62) = 1

T

3

min
Y

(cid:13)(cid:13)2

F  

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y − αT IT
(cid:13)(cid:13)2

F = arg min

Y

(3)

(4)

x1xn. . .y1ykanti-Hebbian synapsesADx2HebbianBECFx1xn. . .y1ykx2ααααβinput eig.x1xn. . .x2output eig.input eig.input eig.output eig.output eig.PrincipalInter-neuronsz1zly1ykz1zlTheorem 1. Suppose an eigen-decomposition of X(cid:62)X = VX ΛX VX(cid:62)

1   . . .   λX
T

(cid:1) with λX

diag(cid:0)λX
are optima of (3)  where STk(ΛX   αT ) = diag(cid:0)ST(cid:0)λX

ciding with those of T C. Then 

1 ≥ . . . ≥ λX

Y∗ = Uk STk(ΛX   αT )1/2 VX

  where ΛX =
T . Note that ΛX has at most n nonzero eigenvalues coin-

 

k

(cid:62)

1   αT(cid:1)   . . .   ST(cid:0)λX
k   αT(cid:1)(cid:1)  ST is the soft-
(cid:3) and Uk is any k × k orthogonal matrix  i.e.

k consists of the columns of VX corresponding

(5)

k > αT and

thresholding function  ST(a  b) = max(a− b  0)  VX
to the top k eigenvalues  i.e. VX
1   . . .   vX
k
Uk ∈ O(k). The form (5) uniquely deﬁnes all optima of (3)  except when k < m  λX
λX
k = λX

k+1.

k = (cid:2)vX

2.2 Hard-thresholding of covariance eigenvalues

Consider the following minimax problem in the ofﬂine setting:

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y(cid:13)(cid:13)2

(cid:13)(cid:13)Y(cid:62)Y − Z(cid:62)Z − αT IT

(cid:13)(cid:13)2

Z

F  

max

min
Y

F −

(cid:1) with λX

(6)
where α ≥ 0 and we introduced an internal variable Z  which is an l × T matrix Z = [z1  . . .   zT ]
with zt ∈ Rl. The intuition behind this objective function is again based on similarity matching but
diag(cid:0)λX
rank regularization is applied indirectly via the internal variable  Z.
Theorem 2. Suppose an eigen-decomposition of X(cid:62)X = VX ΛX VX(cid:62)
  where ΛX =
are optima of (6)  where HTk(ΛX   αT ) = diag(cid:0)HT(cid:0)λX
b  STl min(k m)(ΛX   αT ) = diag(cid:0)ST(cid:0)λX
1   αT(cid:1)   . . .   ST
(cid:2)vX

T ≥ 0. Assume l ≥ min(k  m). Then 
1   . . .   λX
1 ≥ . . . ≥ λX
1   αT(cid:1)   . . .   HT(cid:0)λX
k   αT(cid:1)(cid:1)  HT(a  b) =
T
(cid:62)
Y∗ = Uk HTk(ΛX   αT )1/2 VX
(cid:17)
(cid:16)
(cid:1) VX
(cid:124)
(cid:3) and Up ∈ O(p). The form (7) uniquely deﬁnes all optima (6) except when either

aΘ(a − b) with Θ() being the step function: Θ(a − b) = 1 if a ≥ b and Θ(a − b) = 0 if a <
p =

Z∗ = Ul STl min(k m)(ΛX   αT )1/2 VX

  0  . . .   0
l−min(k m)

λX
min(k m)  αT

1   . . .   vX
p

(cid:123)(cid:122)

(cid:125)

(7)

(cid:62)

1) α is an eigenvalue of C or 2) k < m and λX

k

 

 

l

k = λX

k+1.

2.3 Equalizing thresholded covariance eigenvalues

Z

max

min
Y

1   . . .   λX
T

1 ≥ . . . ≥ λX

Consider the following minimax problem in the ofﬂine setting:

−X(cid:62)XY(cid:62)Y + Y(cid:62)YZ(cid:62)Z + αT Y(cid:62)Y − βT Z(cid:62)Z(cid:1)  

Tr(cid:0)
(cid:1) with λX
(cid:112)βT Θk(ΛX   αT )1/2 VX

(8)
where α ≥ 0 and β > 0. This objective function follows from (6) after dropping the quartic Z term.
Theorem 3. Suppose an eigen-decomposition of X(cid:62)X is X(cid:62)X = VX ΛX VX(cid:62)
  where ΛX =
Z∗ = Ul Σl×T OΛY ∗ VX(cid:62)

T ≥ 0. Assume l ≥ min(k  m). Then 

diag(cid:0)λX
are optima of (8)  where Θk(ΛX   αT ) = diag(cid:0)Θ(cid:0)λX
(cid:2)vX

k − αT(cid:1)(cid:1)  Σl×T is an
(cid:3)  and Up ∈ O(p). The form (9) uniquely deﬁnes all optima of (8) except when either

l × T rectangular diagonal matrix with top min(k  m) diagonals are set to arbitrary nonnegative
constants and the rest are zero  OΛY ∗ is a block-diagonal orthogonal matrix that has two blocks:
the top block is min(k  m) dimensional and the bottom block is T − min(k  m) dimensional  Vp =
1) α is an eigenvalue of C or 2) k < m and λX
Remark 1. If k = m  then Y is full-rank and 1
equalizing variance across all channels.

k = λX
T YY(cid:62) = βIk  implying that the output is whitened 

1 − αT(cid:1)   . . .   Θ(cid:0)λX

Y∗ = Uk

1   . . .   vX
p

k+1.

(9)

(cid:62)

k

 

 

3 Online dimensionality reduction using Hebbian/anti-Hebbian neural nets

In this Section  we formulate online versions of the dimensionality reduction optimization problems
presented in the previous Section  derive corresponding online algorithms and map them onto the dy-
namics of neural networks with biologically plausible local learning rules. The order of subsections
corresponds to that in the previous Section.

4

where η is the weight parameter  and WY X
output covariances 

T

T yT

(13)
are normalized input-output and output-

3.1 Online soft-thresholding of eigenvalues

Consider the following optimization problem in the online setting:

(cid:13)(cid:13)2

F .

(10)

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y − αT IT
(cid:32)T−1(cid:88)
(cid:33)

yT ← arg min

yT

(cid:32)T−1(cid:88)

(cid:33)

t

T

xty(cid:62)

yT + 2y(cid:62)

By keeping only the terms that depend on yT we get the following objective for (2):
L = −4x(cid:62)

yT − 2(cid:107)xT(cid:107)2(cid:107)yT(cid:107)2 + (cid:107)yT(cid:107)4. (11)
In the large-T limit  the last two terms can be dropped since the ﬁrst two terms grow linearly with T
and dominate. The remaining cost is a positive deﬁnite quadratic form in yT and the optimization
problem is convex. At its minimum  the following equality holds:
ytx(cid:62)

t + αT Im

yty(cid:62)

(cid:33)

(12)

xT .

t=1

t=1

T

t

While a closed-form analytical solution via matrix inversion exists for yT   we are interested in
biologically plausible algorithms. Instead  we use a weighted Jacobi iteration where yT is updated
according to:

t=1

yT =

(cid:33)

yty(cid:62)

t + αT Im

(cid:32)T−1(cid:88)
(cid:32)T−1(cid:88)
yT ← (1 − η) yT + η(cid:0)WY X
T−1(cid:80)

and WY Y

t=1

T

T xT − WY Y
T−1(cid:80)

yt iyt j

(cid:1)  

W Y X

T ik =

t=1

αT +

yt ixt k

T−1(cid:80)

t=1

 

y2
t i

W Y Y

T i j(cid:54)=i =

t=1

αT +

W Y Y

T ii = 0.

(14)

 

y2
t i

T−1(cid:80)

t=1

T

T

and WY Y

Iteration (13) can be implemented by the dynamics of neuronal activity in a single-layer network 
represent the weights of feedforward (xt → yt) and lateral
Figure 1D. Then  WY X
(yt → yt) synaptic connections  respectively. Remarkably  synaptic weights appear in the online
solution despite their absence in the optimization problem formulation (3). Previously  nonnormal-
ized covariances have been used as state variables in an online dictionary learning algorithm [29].
To formulate a fully online algorithm  we rewrite (14) in a recursive form. This requires introducing
a scalar variable DY
T i =
t i. Then  at each data sample presentation  T   after the output yT converges to a steady
y2

T i representing cumulative activity of a neuron i up to time T − 1  DY

T−1(cid:80)

αT +
state  the following updates are performed:

t=1

DY
T +1 i ← DY
T +1 ij ← W Y X
W Y X
T +1 i j(cid:54)=i ← W Y Y
W Y Y

T i 

T i + α + y2

T ij +(cid:0)yT ixT j −
T ij +(cid:0)yT iyT j −

(cid:0)α + y2
(cid:0)α + y2

T i

T i

(cid:1) W Y X
(cid:1) W Y Y

T ij

(cid:1) /DY
(cid:1) /DY

T ij

T +1 i 
T +1 i.

(15)

Hence  we arrive at a neural network algorithm that solves the optimization problem (10) for stream-
ing data by alternating between two phases. After a data sample is presented at time T   in the ﬁrst
phase of the algorithm (13)  neuron activities are updated until convergence to a ﬁxed point. In the
second phase of the algorithm  synaptic weights are updated for feedforward connections according
to a local Hebbian rule (15) and for lateral connections according to a local anti-Hebbian rule (due
to the (−) sign in equation (13)). Interestingly  in the α = 0 limit  these updates have the same
form as the single-neuron Oja rule [24  2]  except that the learning rate is not a free parameter but is
determined by the cumulative neuronal activity 1/DY

T +1 i [4  5].

3.2 Online hard-thresholding of eigenvalues

Consider the following minimax problem in the online setting  where we assume α > 0:
F .

arg max

{yT   zT} ← arg min

yT

F −

(cid:13)(cid:13)Y(cid:62)Y − Z(cid:62)Z − αT IT

(cid:13)(cid:13)X(cid:62)X − Y(cid:62)Y(cid:13)(cid:13)2

(cid:13)(cid:13)2

zT

(16)

By keeping only those terms that depend on yT or zT and considering the large-T limit  we get the

5

following objective:

L = 2αT (cid:107)yT(cid:107)2 − 4x(cid:62)

T

(cid:32)T−1(cid:88)

(cid:33)

xty(cid:62)

t

t=1

(cid:32)T−1(cid:88)

(cid:33)

(cid:32)T−1(cid:88)

(cid:33)

zT .

ytz(cid:62)

t

zT + 4y(cid:62)

T

ztz(cid:62)

t + αT Ik

t=1

yT − 2z(cid:62)

T

t=1

(17)
Note that this objective is strongly convex in yT and strongly concave in zT . The solution of this
minimax problem is the saddle-point of the objective function  which is found by setting the gradient
of the objective with respect to {yT   zT} to zero [30]:
αT yT =

(cid:32)T−1(cid:88)

(cid:32)T−1(cid:88)

ytx(cid:62)

zty(cid:62)

ytz(cid:62)

(cid:33)

(cid:33)

(cid:33)

yT .

t

t

xT −

t=1

yT ← (1 − η) yT + η(cid:0)WY X

To obtain a neurally plausible algorithm  we solve these equations by a weighted Jacobi iteration:
T zT

T zT

T yT − WZZ

Here  similarly to (14)  WT are normalized covariances that can be updated recursively:

t

t=1

t=1

t=1

zT  

zT =

(cid:33)

ztz(cid:62)

(cid:1)  

t + αT Ik

T xT − WY Z

(cid:32)T−1(cid:88)
(cid:32)T−1(cid:88)
zT ← (1 − η) zT + η(cid:0)WZY
(cid:1) /DY
T ij +(cid:0)yT ixT j − αW Y X
(cid:1) /DY
T ij +(cid:0)yT izT j − αW Y Z
(cid:1) W ZY
(cid:0)α + z2
T ij +(cid:0)zT iyT j −
T ij +(cid:0)zT izT j −
(cid:0)α + z2
(cid:1) W ZZ

(cid:1) /DZ
(cid:1) /DZ

T +1 i
T +1 i  W ZZ

DZ
T +1 i ← DZ

T i + α + z2
T i

T i + α 

T +1 i

T +1 i

T ij

T ij

T ij

T i

DY
T +1 i ← DY
W Y X
T +1 ij ← W Y X
W Y Z
T +1 ij ← W Y Z
W ZY
T +1 i j ← W ZY
T +1 i j(cid:54)=i ← W ZZ
W ZZ

(18)

(cid:1) .

(19)

(20)
Equations (19) and (20) deﬁne an online algorithm that can be naturally implemented by a neural
network with two populations of neurons: principal and interneurons  Figure 1E. Again  after each
data sample presentation  T   the algorithm proceeds in two phases. First  (19) is iterated until
convergence by the dynamics of neuronal activities. Second  synaptic weights are updated according
to local  anti-Hebbian (for synapses from interneurons) and Hebbian (for all other synapses) rules.

T ii = 0.

T ij

T i

3.3 Online thresholding and equalization of eigenvalues

Consider the following minimax problem in the online setting  where we assume α > 0 and β > 0:
(21)

arg max

{yT   zT} ← arg min

yT

zT

By keeping only those terms that depend on yT or zT and considering the large-T limit  we get the
following objective:

Tr(cid:2)
−X(cid:62)XY(cid:62)Y + Y(cid:62)YZ(cid:62)Z + αTY(cid:62)Y − βTZ(cid:62)Z(cid:3) .
(cid:32)T−1(cid:88)

ytz(cid:62)

t

zT .

(22)

This objective is strongly convex in yT and strongly concave in zT and its saddle point is given by:

xty(cid:62)

(cid:33)
(cid:32)T−1(cid:88)

t

t=1

T

yT − βT (cid:107)zT(cid:107)2 + 2y(cid:62)
(cid:33)
(cid:1)  

βT zT =

zT  

t

ytz(cid:62)

(cid:33)
(cid:33)

(cid:32)T−1(cid:88)
(cid:32)T−1(cid:88)

t=1

t=1

T

(cid:32)T−1(cid:88)

L = αT (cid:107)yT(cid:107)2 − 2x(cid:62)
(cid:33)
yT ← (1 − η) yT + η(cid:0)WY X

αT yT =

ytx(cid:62)

t=1

t

t=1

xT −

To obtain a neurally plausible algorithm  we solve these equations by a weighted Jacobi iteration:

As before  WT are normalized covariances which can be updated recursively:

T xT − WY Z

T zT

zT ← (1 − η) zT + ηWZY

T yT  

(24)

zty(cid:62)

t

yT .

(23)

DY
T +1 i ← DY
W Y X
T +1 ij ← W Y X
T +1 ij ← W Y Z
W Y Z
W ZY
T +1 i j ← W ZY

T i + α 

DZ
T +1 i ← DZ

T ij +(cid:0)yT ixT j − αW Y X
(cid:1) /DY
(cid:1) /DY
T ij +(cid:0)yT izT j − αW Y Z
(cid:1) /DZ
T ij +(cid:0)zT iyT j − βW ZY

T i + β

T ij

T ij

T ij

T +1 i

T +1 i
T +1 i.

(25)
Equations (24) and (25) deﬁne an online algorithm that can be naturally implemented by a neural
network with principal neurons and interneurons. As beofre  after each data sample presentation at

6

Figure 2: Performance of the three neural networks: soft-thresholding (A)  hard-thresholding (B) 
equalization after thresholding (C). Top: eigenvalue error  bottom: subspace error as a function
of data presentations. Solid lines - means and shades - stds over 10 runs. Red - principal  blue -
inter-neurons. Dashed lines - best-ﬁt power laws. For metric deﬁnitions see text.

time T   the algorithm  ﬁrst  iterates (24) by the dynamics of neuronal activities until convergence
and  second  updates synaptic weights according to local anti-Hebbian (for synapses from interneu-
rons) and Hebbian (25) (for all other synapses) rules.
While an algorithm similar to (24)  (25)  but with predetermined learning rates  was previously given
in [15  14]  it has not been derived from an optimization problem. Plumbley’s convergence analysis
of his algorithm [14] suggests that at the ﬁxed point of synaptic updates  the interneuron activity is
also a projection onto the principal subspace. This result is a special case of our ofﬂine solution  (9) 
supported by the online numerical simulations (next Section).

4 Numerical simulations

0 i and 1/DZ

Here  we evaluate the performance of the three online algorithms on a synthetic dataset  which is
generated by an n = 64 dimensional colored Gaussian process with a speciﬁed covariance matrix.
In this covariance matrix  the eigenvalues  λ1..4 = {5  4  3  2} and the remaining λ5..60 are chosen
uniformly from the interval [0  0.5]. Correlations are introduced in the covariance matrix by gen-
erating random orthonormal eigenvectors. For all three algorithms  we choose α = 1 and  for the
equalizing algorithm  we choose β = 1. In all simulated networks  the number of principal neurons 
k = 20  and  for the hard-thresholding and the equalizing algorithms  the number of interneurons 
l = 5. Synaptic weight matrices were initialized randomly  and synaptic update learning rates 
0 i were initialized to 0.1. Network dynamics is run with a weight η = 0.1 until the
1/DY
relative change in yT and zT in one cycle is < 10−5.
To quantify the performance of these algorithms  we use two different metrics. The ﬁrst metric 
eigenvalue error  measures the deviation of output covariance eigenvalues from their optimal ofﬂine
values given in Theorems 1  2 and 3. The eigenvalue error at time T is calculated by summing
T ZZ(cid:62)  and their optimal ofﬂine values
squared differences between the eigenvalues of 1
at time T . The second metric  subspace error  quantiﬁes the deviation of the learned subspace from
the true principal subspace. To form such metric  at each T   we calculate the linear transforma-
tion that maps inputs  xT   to outputs  yT = FY X
T xT   at the ﬁxed points of
the neural dynamics stages ((13)  (19)  (24)) of the three algorithms. Exact expressions for these
matrices for all algorithms are given in the Supplementary Material. Then  at each T   the deviation
  where Fm T is an n × m matrix whose columns are the top m
m T is the projection matrix to the subspace spanned by these
m T is an n×m matrix whose columns are the principal eigenvectors of the input

m T − UX
right singular vectors of FT   Fm T F(cid:62)
singular vectors  UX
covariance matrix C at time T   UX

is(cid:13)(cid:13)Fm T F(cid:62)

m T is the projection matrix to the principal subspace.

T xT and zT = FZX

(cid:13)(cid:13)2

F

m T UX (cid:62)

m T

T YY(cid:62) or 1

m T UX (cid:62)

7

ABC1TSubspace ErrorEigenvalue ErrorT -1.50T -1.56Subspace ErrorEigenvalue ErrorSubspace ErrorEigenvalue Error10310210110-110-210-31010210310410110-110-210-310-41T10102103104∝∝T -1.53T -1.4310110-110-210-31T10102103104∝∝T -1.33T -1.80∝∝1T10310210110-110-210-3101021031041T1010210310410310210110-110-2T -1.48∝1T1010210310410110-110-210-3T -1.41T -1.38∝∝Further numerical simulations comparing the performance of the soft-thresholding algorithm with
α = 0 with other neural principal subspace algorithms can be found in [24].

5 Discussion and conclusions

We developed a normative approach for dimensionality reduction by formulating three novel opti-
mization problems  the solutions of which project the input onto its principal subspace  and rescale
the data by i) soft-thresholding  ii) hard-thresholding  iii) equalization after thresholding of the input
eigenvalues. Remarkably we found that these optimization problems can be solved online using
biologically plausible neural circuits. The dimensionality of neural activity is the number of either
input covariance eigenvalues above the threshold  m  (if m < k) or output neurons  k (if k ≤ m).
The former case is ubiquitous in the analysis of experimental recordings  for a review see [31].
Interestingly  the division of neurons into two populations  principal and interneurons  in the last
two models has natural parallels in biological neural networks. In biology  principal neurons and
interneurons usually are excitatory and inhibitory respectively. However  we cannot make such an
assignment in our theory  because the signs of neural activities  xT and yT   and  hence  the signs of
synaptic weights  W  are unconstrained. Previously  interneurons were included into neural circuits
[32]  [33] outside of the normative approach.
Similarity matching in the ofﬂine setting has been used to analyze experimentally recorded neu-
ron activity lending support to our proposal. Semantically similar stimuli result in similar neural
activity patterns in human (fMRI) and monkey (electrophysiology) IT cortices [34  35]. In addi-
tion  [36] computed similarities among visual stimuli by matching them with the similarity among
corresponding retinal activity patterns (using an information theoretic metric).
We see several possible extensions to the algorithms presented here: 1) Our online objective func-
tions may be optimized by alternative algorithms  such as gradient descent  which map onto different
circuit architectures and learning rules. Interestingly  gradient descent-ascent on convex-concave ob-
jectives has been previously related to the dynamics of principal and interneurons [37]. 2) Inputs
coming from a non-stationary distribution (with time-varying covariance matrix) can be processed
by algorithms derived from the objective functions where contributions from older data points are
“forgotten”  or “discounted”. Such discounting results in higher learning rates in the corresponding
online algorithms  even at large T   giving them the ability to respond to variations in data statistics
[24  4]. Hence  the output dimensionality can track the number of input dimensions whose eigen-
values exceed the threshold. 3) In general  the output of our algorithms is not decorrelated. Such
decorrelation can be achieved by including a correlation-penalizing term in our objective functions
[38]. 4) Choosing the threshold parameter α requires an a priori knowledge of input statistics. A
better solution  to be presented elsewhere  would be to let the network adjust such threshold adap-
tively  e.g. by ﬁltering out all the eigenmodes with power below the mean eigenmode power. 5)
Here  we focused on dimensionality reduction using only spatial  as opposed to the spatio-temporal 
correlation structure.
We thank L. Greengard  A. Sengupta  A. Grinshpan  S. Wright  A. Barnett and E. Pnevmatikakis.

References
[1] David H Hubel. Eye  brain  and vision. Scientiﬁc American Library/Scientiﬁc American Books  1995.
[2] E Oja. Simpliﬁed neuron model as a principal component analyzer. J Math Biol  15(3):267–273  1982.
[3] KI Diamantaras and SY Kung. Principal component neural networks: theory and applications. John

Wiley & Sons  Inc.  1996.

[4] B Yang. Projection approximation subspace tracking. IEEE Trans. Signal Process.  43(1):95–107  1995.
[5] T Hu  ZJ Towﬁc  C Pehlevan  A Genkin  and DB Chklovskii. A neuron as a signal processing device. In

Asilomar Conference on Signals  Systems and Computers  pages 362–366. IEEE  2013.

[6] E Oja. Principal components  minor components  and linear neural networks. Neural Networks  5(6):927–

935  1992.

[7] R Arora  A Cotter  K Livescu  and N Srebro. Stochastic optimization for pca and pls. In Allerton Conf.

on Communication  Control  and Computing  pages 861–868. IEEE  2012.

[8] J Goes  T Zhang  R Arora  and G Lerman. Robust stochastic principal component analysis. In Proc. 17th

Int. Conf. on Artiﬁcial Intelligence and Statistics  pages 266–274  2014.

8

[9] Todd K Leen. Dynamics of learning in recurrent feature-discovery networks. NIPS  3  1990.
[10] P F¨oldiak. Adaptive network for optimal linear feature extraction. In Int. Joint Conf. on Neural Networks 

pages 401–405. IEEE  1989.

[11] TD Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural

networks  2(6):459–473  1989.

[12] J Rubner and P Tavan. A self-organizing network for principal-component analysis. EPL  10:693  1989.
[13] MD Plumbley. A hebbian/anti-hebbian network which optimizes information capacity by orthonormaliz-

ing the principal subspace. In Proc. 3rd Int. Conf. on Artiﬁcial Neural Networks  pages 86–90  1993.

[14] MD Plumbley. A subspace network that determines its own output dimension. Tech. Rep.  1994.
[15] MD Plumbley. Information processing in negative feedback neural networks. Network-Comp Neural 

7(2):301–305  1996.

[16] P Vertechi  W Brendel  and CK Machens. Unsupervised learning of an efﬁcient short-term memory

network. In NIPS  pages 3653–3661  2014.

[17] BA Olshausen and DJ Field. Sparse coding with an overcomplete basis set: A strategy employed by v1?

Vision Res  37(23):3311–3325  1997.

[18] AA Koulakov and D Rinberg. Sparse incomplete representations: a potential role of olfactory granule

cells. Neuron  72(1):124–136  2011.

[19] S Druckmann  T Hu  and DB Chklovskii. A mechanistic model of early sensory processing based on

subtracting sparse representations. In NIPS  pages 1979–1987  2012.

[20] AL Fairhall  GD Lewen  W Bialek  and RRR van Steveninck. Efﬁciency and ambiguity in an adaptive

neural code. Nature  412(6849):787–792  2001.

[21] SE Palmer  O Marre  MJ Berry  and W Bialek. Predictive information in a sensory population. PNAS 

112(22):6908–6913  2015.

[22] E Doi  JL Gauthier  GD Field  J Shlens  et al. Efﬁcient coding of spatial information in the primate retina.

J Neurosci  32(46):16256–16264  2012.

[23] R Linsker. Self-organization in a perceptual network. Computer  21(3):105–117  1988.
[24] C Pehlevan  T Hu  and DB Chklovskii. A hebbian/anti-hebbian neural network for linear subspace learn-
ing: A derivation from multidimensional scaling of streaming data. Neural Comput  27:1461–1495  2015.
[25] G Young and AS Householder. Discussion of a set of points in terms of their mutual distances. Psychome-

trika  3(1):19–22  1938.

[26] WS Torgerson. Multidimensional scaling: I. theory and method. Psychometrika  17(4):401–419  1952.
[27] HG Barrow and JML Budd. Automatic gain control by a basic neural circuit. Artiﬁcial Neural Networks 

2:433–436  1992.

[28] EJ Cand`es and B Recht. Exact matrix completion via convex optimization. Found Comput Math 

9(6):717–772  2009.

[29] J Mairal  F Bach  J Ponce  and G Sapiro. Online learning for matrix factorization and sparse coding.

JMLR  11:19–60  2010.

[30] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2004.
[31] P Gao and S Ganguli. On simplicity and complexity in the brave new world of large-scale neuroscience.

Curr Opin Neurobiol  32:148–155  2015.

[32] M Zhu and CJ Rozell. Modeling inhibitory interneurons in efﬁcient sensory coding models. PLoS Comput

Biol  11(7):e1004353  2015.

[33] PD King  J Zylberberg  and MR DeWeese. Inhibitory interneurons decorrelate excitatory cells to drive

sparse code formation in a spiking model of v1. J Neurosci  33(13):5475–5485  2013.

[34] N Kriegeskorte  M Mur  DA Ruff  R Kiani  et al. Matching categorical object representations in inferior

temporal cortex of man and monkey. Neuron  60(6):1126–1141  2008.

[35] R Kiani  H Esteky  K Mirpour  and K Tanaka. Object category structure in response patterns of neuronal

population in monkey inferior temporal cortex. J Neurophysiol  97(6):4296–4309  2007.

[36] G Tkaˇcik  E Granot-Atedgi  R Segev  and E Schneidman. Retinal metric: a stimulus distance measure

derived from population neural responses. PRL  110(5):058104  2013.

[37] HS Seung  TJ Richardson  JC Lagarias  and JJ Hopﬁeld. Minimax and hamiltonian dynamics of

excitatory-inhibitory networks. NIPS  10:329–335  1998.

[38] C Pehlevan and DB Chklovskii. Optimization theory of hebbian/anti-hebbian networks for pca and

whitening. In Allerton Conf. on Communication  Control  and Computing  2015.

9

,Aditya Bhaskara
Silvio Lattanzi
Vahab Mirrokni
Cengiz Pehlevan
Dmitri Chklovskii
Marc Vuffray
Sidhant Misra
Andrey Lokhov
Michael Chertkov