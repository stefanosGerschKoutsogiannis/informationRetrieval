2018,Modelling and unsupervised learning of symmetric deformable object categories,We propose a new approach to model and learn  without manual supervision  the symmetries of natural objects  such as faces or flowers  given only images as input. It is well known that objects that have a symmetric structure do not usually result in symmetric images due to articulation and perspective effects. This is often tackled by seeking the intrinsic symmetries of the underlying 3D shape  which is very difficult to do when the latter cannot be recovered reliably from data. We show that  if only raw images are given  it is possible to look instead for symmetries in the space of object deformations. We can then learn symmetries from an unstructured collection of images of the object as an extension of the recently-introduced object frame representation  modified so that object symmetries reduce to the obvious symmetry groups in the normalized space. We also show that our formulation provides an explanation of the ambiguities that arise in recovering the pose of symmetric objects from their shape or images and we provide a way of discounting such ambiguities in learning.,Modelling and unsupervised learning of
symmetric deformable object categories

James Thewlis1

Hakan Bilen2

Andrea Vedaldi1

1 Visual Geometry Group

University of Oxford

{jdt vedaldi}@robots.ox.ac.uk

2 School of Informatics
University of Edinburgh

hbilen@ed.ac.uk

Abstract

We propose a new approach to model and learn  without manual supervision  the
symmetries of natural objects  such as faces or ﬂowers  given only images as input.
It is well known that objects that have a symmetric structure do not usually result in
symmetric images due to articulation and perspective effects. This is often tackled
by seeking the intrinsic symmetries of the underlying 3D shape  which is very
difﬁcult to do when the latter cannot be recovered reliably from data. We show that 
if only raw images are given  it is possible to look instead for symmetries in the
space of object deformations. We can then learn symmetries from an unstructured
collection of images of the object as an extension of the recently-introduced object
frame representation  modiﬁed so that object symmetries reduce to the obvious
symmetry groups in the normalized space. We also show that our formulation
provides an explanation of the ambiguities that arise in recovering the pose of
symmetric objects from their shape or images and we provide a way of discounting
such ambiguities in learning.

1

Introduction

Most natural objects are symmetric: mammals have a bilateral symmetry  a glass is rotationally
symmetric  many ﬂowers have a radial symmetry  etc. While such symmetries are easy to understand
for a human  it remains surprisingly challenging to develop algorithms that can reliably detect the
symmetries of visual object in images. The key difﬁculty is that objects that are structurally symmetric
do not generally result in symmetric images; in fact  the latter occurs only when the object is imaged
under special viewpoints and  for deformable objects  with a special poses (Leonardo’s Vitruvian
Man illustrates this point).
The standard approach to characterizing symmetries in objects is to look not at their images  but at
their 3D shape; if the latter is available  then symmetries can be recovered by analysing the intrinsic
geometry of the shape. However  often only images of the objects are available  and reconstructing an
accurate 3D shape from them can be very challenging  especially if the object is deformable.
In this paper  we thus seek a new approach to learn without supervision and from raw images alone
the symmetries of deformable object categories. This may sound difﬁcult since even characterising
the basic geometry of natural objects without external supervision remains largely an open problem.
Nevertheless  we show that it is possible to extend the method of [38]  which was recently introduced
to learn the “topology” of object categories  to do exactly this.
There are three key enabling factors in our approach. First  we do not consider symmetries of a single
object or 3D shape in isolation; instead  we seek symmetries shared by all the instances of the objects
in a given category  imaged under different viewing conditions and deformations. Second  rather than
considering the common concept of intrinsic symmetries  we propose to look at symmetries not of
3D shapes  but of the space of their deformations (section 4). Third  we show that the normalized
object frame of [38] can be learned in such a way that the deformation symmetries are represented by
32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: Symmetric object frame for human (left) and cat (right) faces (test set). Our method
learns a viewpoint and identity invariant geometric embedding which captures the symmetry of
natural objects (in this case bilateral) without manual supervision. Top: input images with the axis of
symmetry superimposed (shown in green). Middle: dense embedding mapped to colours. Bottom:
image pixels mapped to 3D representation space with the reﬂection plane (green).

the obvious symmetry groups in the object frame. The latter also result in a constraint that can be
easily added to the self-supervised formulation of [38] to learn symmetries in practice (section 3).
We start by deriving our formulation for the special case of bilateral symmetries (section 3). Then 
we propose a theory of symmetric deformation spaces (section 4) that generalises the method to other
symmetry groups. An important step in this generalization is to characterise the ambiguities that
symmetries induce in recovering the pose of an object from an image of it  or from its 3D shape 
which may not occur with bilateral symmetries.
The resulting approach is the ﬁrst that  to our knowledge  can learn the symmetries of object categories
given only raw images as input  without manual annotations. For demonstration  we show that this
approach can learn the bilateral symmetry in human and pet faces (ﬁg. 1) as well as in synthetic 3D
objects (section 6). To assess the method  we look at how well the resulting representation can detect
pairs of symmetric object landmarks (e.g. left and right eyes) even when the object does not appear
symmetric.
We also investigate the problem of symmetry-induced ambiguities in learning the geometry of natural
objects. For objects such as animals that have a bilateral symmetry  it is generally possible to uniquely
identify their left and right sides and thus recover their pose uniquely. On the other hand  for objects
such as ﬂowers that may have a radial symmetry  it is generally impossible to say which way is
“up”  creating an ambiguity in pose recovery. Our framework clariﬁes why and when this occurs and
suggests how to modify the learning formulation to mitigate the effect of such ambiguities (sections 4
and 6.2).

2 Related work

Cross-instance object matching. Our method is also related to the techniques that ﬁnd dense
correspondences between different object instances by matching their SIFT features [25]  establishing
region correspondences [14  15] and matching the internal representations of neural networks [24].
In addition  dense correspondences have been generalized between image pairs to arbitrary number
of multiple images by Learned-Miller [20]. More recently  RSA [32]  Collection Flow [18] and
Mobahi et al. [28] show that a collection of images can be projected into a lower dimensional
subspace before performing a joint alignment among the projected images. Novotny et al. [30] train
a neural network with image labels that learns to automatically discover semantically meaningful
parts across animals.
Unsupervised learning of object structure. Supervised visual object characterization [6  11  21 
8  10] is a well established problem in computer vision and successfully applied to facial landmark
detection and human body pose estimation. Unsupervised methods include Spatial Transformer
Networks [16] that learn to transform images to improve image classiﬁcation  WarpNet [17] and geo-
metric matching networks [34] that learn to match object pairs by estimating relative transformations
between them. In contrast to ours  these methods do not learn a canonical object geometry and only
provide relative mapping from one object to another. More related to ours  Thewlis et al. [39  38]
propose to characterize object structure via detecting landmarks [39] or dense labels [38] that are

2

Figure 2: Left: an object category consisting of two poses π  π(cid:48) with bilateral symmetry. Middle:
the non-rigid deformation t = π(cid:48) ◦ π−1 transporting one pose into the other. Right: construction of
t = mπm−1π−1 by applying the reﬂection operator m both in Euclidean space and in representation
space S2. This also shows that the symmetric pose π(cid:48) = mπm−1 is the “conjugate” of π.
consistent with object deformations and viewpoint changes. In fact  our method builds on [38] and
also learns a dense geometric embedding for objects  however  by using a different supervision
principle  symmetry.
Symmetry. Computational symmetry [22] has a long history in sciences and played an essential
role in several important discoveries including the theory of relativity [29]  the double helix structure
of DNA [42]. Symmetry is shown to help grouping [19] and recognition [41] in human perception.
There is a vast body of computer vision literature dedicated to ﬁnding symmetries in images [26] 
two dimensional [1] and three dimensional shapes [37]. Other axes of variations among symmetry
detection methods are whether we seek transformations to map the whole [33] or part of an object [12]
to itself; whether distances are measured in the extrinsic Euclidean space [1] or with respect to
an intrinsic metric of the surface [33]. In addition to symmetry detection  symmetry is also used
as prior information to improve object localization [4]  text spotting [47]  pose estimation [44] and
3D reconstruction [35]. Symmetry constraints been used to ﬁnd objects in 3D point clouds [9  40].
Symmetrization [27] can be used to warp meshes to a symmetric pose. Symmetry cues can be used
in segmentation [3  5]. [2] learns representations that respect a group structure learned from data
symmetries.

3 Self-supervised learning of bilateral symmetries

In this section  we extend the approach of [38] to learn the bilateral symmetry of an object category.
Object frame. The key idea of [38] is to study 3D objects not via 3D reconstruction  which is
challenging  but by characterizing the correspondences between different 3D shapes of the object  up
to pose or intra-class variations.
In this model  an object category is a space Π of homeomorphisms π : S2 → R3 that embed
the sphere S2 into R3. Each possible shape of the object is obtained as the (mathematical) image
S = π[S2] under a corresponding function π ∈ Π  which we therefore call a pose of the object
(different poses may result in the same shape). The correspondences between a pair of shapes
S = π[S2] and S(cid:48) = π(cid:48)[S2] is then given by π(cid:48) ◦ π−1  which is a bijective deformation of S into S(cid:48).
Next  we study how poses relate to images of the object. A (color) image is a function x : Ω → R3
mapping pixels u ∈ Ω to colors xu. Suppose that x is the image of the object under pose π; then  a
point z ∈ S2 on the sphere projects to a point πz ∈ R3 on the object surface S and the latter projects
to a pixel u = Proj(πz) ∈ Ω  where Proj is the camera projection operator.
The idea of [38] is to learn a function ψu(x) that “reverses” this process and  given a pixel u in image
x  recovers the corresponding point z on the sphere (so that ∀u : u = Proj (πψu(x))). The intuition
is that z identiﬁes a certain object landmark (e.g. the corner of the left eye in a face) and that the
function ψu(x) recovers which landmark lands at a certain pixel u.
The way the function ψu(x) is learned is by considering pairs of images x and x(cid:48) = tx related by a
known 2D deformation t : Ω → Ω (where the warped image tx is given by (tx)u = xt−1u). In this
manner  pixels u and u(cid:48) = tu are images of the same object landmark and therefore must project
on the same sphere point. In formulas  and ignoring visibility effects and other complications  the
learned function must satisfy the invariance constraint:

(1)
In practice  triplets (x  x(cid:48)  t) are obtained by randomly sampling 2D warps t  assuming that the latter
approximate warps that could arise form an actual pose change π(cid:48) ◦ π−1. In this manner  knowledge
of t is automatic and the method can be used in an unsupervised setting.

∀u ∈ Ω : ψu(x) = ψtu(tx)

3

Symmetric object frame. So far the object frame has been used to learn correspondences between
different object poses; here  we show that it can be used to establish auto-correspondences in order to
model object symmetries as well.
Consider in particular an object that has a bilateral symmetry. This symmetry is generated by a
reﬂection operator  say the function m : R3 → R3 that ﬂips the ﬁrst axis:

m :

R3 → R3 

(2)

(cid:34)p1

(cid:35)

p2
p3

(cid:34)−p1

(cid:35)

.

p2
p3

(cid:55)→

If S is a shape of a bilaterally-symmetric object  no matter how we align S to the symmetry plane  in
general m[S] (cid:54)= S due to object deformations. However  we can expect m[S] to still be a valid shape
for the object. Consider the example of ﬁg. 2 of a person with his/her right hand raised; if we apply
m to this shape  we obtain the shape of a person with the left hand raised  which is valid.
However  reasoning about shapes is insufﬁcient to apply the object frame model; we require instead
to work with correspondences  encoded by poses. Unfortunately  even though m[S] is a valid shape 
m is not a valid correspondence as it ﬂips the left and right sides of a person  which is not a “physical”
deformation (why this is important will be clearer later; intuitively it is the reason why we can tell
our left hand from the right by looking).
Our key intuition is that we can learn the pose representation in such a way that the correct corre-
spondences are trivially expressible there. Namely  assume that m applied to the sphere amounts
to swapping each left landmark of the object with its corresponding right counterpart. The correct
deformation t that maps the “right arm raised” pose to the “left arm raised” pose can now be found
by applying m ﬁrst in the normalized object frame (to swap left and right sides while leaving the
shape unchanged) and then again in 3D space (undoing the swap while actually deforming the shape).
This two-step process is visualised in ﬁg. 2 right.
This derivation is captured by a simple change to constraint (1)  encoding equivariance rather than
invariance w.r.t. the warp m:

∀u ∈ Ω : mψu(x) = ψmu(mx)

(3)
We will show that this simple variant of eq. (1) can be used to learn a representation of the bilateral
symmetry of the object category.
Learning formulation. We follow [38] and learn the model ψu(x) by considering a dataset of
images x of a certain object category  modelling the function ψu(x) by a convolutional neural
network  and formulating learning as a Siamese conﬁguration  combining constraints (3) and (1)
into a single loss. To avoid learning the trivial solution where ψu(x) is the constant function  the
constraints are extended to capture not just invariance/equivariance but also distinctiveness (namely 
equalities (3) and (1) should not hold if u is replaced with a different pixel v in the left-hand side).
Following [38]  this is captured probabilistically by the loss:
p(v|u) =

(4)
The probability p(v|u) represents the model’s belief that pixel u in image x matches pixel v in image
mtx based on the learned embedding function; the latter is relaxed to span R3 rather than only S2 to
allow the length of the embedding vectors to encode the belief strength (as shorter vectors results in
ﬂatter distributions p(v|u)). For unsupervised training  warps t ∼ T are randomly sampled from a
ﬁxed distribution T as in [38]  whereas m is set to be either the identity or the reﬂection along the
ﬁrst axis with 50% probability.

2 p(v|u) dvdu 

(cid:82) exp(cid:104)mψu(x)  ψw(mtx)(cid:105) dw

exp(cid:104)mψu(x)  ψv(mtx)(cid:105)

L(x  m  t) =

(cid:107)v − mtu(cid:107)γ

(cid:90)

Ω

4 Theory

In the previous section  we have given a formulation for learning the bilateral symmetry of an object
category  relying mostly on an intuitive derivation. In this section  we develop the underlying theory
in a more rigorous manner (proofs can be found in the supplementary material)  while clarifying
three important points: how to model symmetries other than the bilateral one  why symmetries such
as radial result in ambiguities in establishing correspondences and why this is usually not the case for
the bilateral symmetry  and what can be done to handle such ambiguities in the learning formulation
when they arise.

4

Figure 3: Left: a set Π = {π0  . . .   π3} of four poses with rotational symmetry group H = {hk  k =
0  1  2  3} where h is a rotation by π/2. Note that none of the shapes is symmetric; rather  the object 
which stays “upright”  can deform in four symmetric ways. The shape of the object is then sufﬁcient
to recover the pose uniquely. Middle: closure of the pose space Π by rotations G = H. Now pose
can be recovered from shapes only up to the symmetry group H. Right: an equilateral triangle is
represented by a pose π0 invariant to conjugation by 60 degrees rotations (which are the “ordinary”
extrinsic symmetries of this object).
Symmetric pose spaces. A symmetry of a shape S ⊂ R3 is often deﬁned as an isometry1 h :
R3 → R3 that leaves the set invariant  i.e. h[S] = S. This deﬁnition is not very useful when dealing
with symmetric but deformable objects  as it works only for special poses (cf. the Vitruvian Man);
we require instead a deﬁnition of symmetry that is not pose dependent. A common approach is to
deﬁne intrinsic symmetries [33] as maps h : S → S that preserve the geodesic distance dS deﬁned on
the surface of the object (i.e. ∀p  q ∈ S : dS(hp  hq) = dS(p  q)). This works because the geodesic
distance captures the intrinsic geometry of the shape  which is pose invariant (but elastic shape
deformations are still a problem); however  using this deﬁnition requires to accurately reconstruct the
3D shape of objects from images  which is very challenging.
In order to sidestep this difﬁculty  we propose to study the symmetry not of the 3D shapes of objects 
but rather of the space of their deformations. As discussed in section 3  such deformations are
captured as a whole by the pose space Π. We deﬁne the symmetries of the pose space Π as the subset
of linear isometries that leave Π unchanged via conjugation:

H(Π) = {h ∈ O(3) : ∀π ∈ Π : hπh−1 ∈ Π ∧ h−1πh ∈ Π}.

For example  in ﬁg. 2 we have obtained the “left hand raised” pose π(cid:48) from the “right hand raised”
pose via conjugation π(cid:48) = mπm−1 via the reﬂection m (note that m = m−1).
Lemma 1. The set H(Π) is a subgroup of O(3).

The symmetry group H(Π) partitions Π in equivalence classes of symmetric poses: two poses π and
π(cid:48) are symmetric  denoted π ∼H(Π) π(cid:48)  if  and only if  π(cid:48) = hπh−1 for an h ∈ H(Π). In fact:
Lemma 2. π ∼H(Π) π(cid:48) is an equivalence relation on the space of poses Π.

Figure 3 shows an example of an object Π that has four rotationally-symmetric poses H(Π) =
{hkπ0h−k  k = 0  1  2  3} where h is a clockwise rotation of 90 degrees.
In the example of ﬁg. 3  the object is pinned at the origin of R3
Motion-induced ambiguities.
and cannot rotate (it can only be “upright”); in order to allow it to move around  we can extend the
pose space to Π(cid:48) = GΠ by applying further transformations to the poses. For example  choosing
G = SE(3) to be the Euclidean group allows the object to move rigidly; ﬁg. 3-middle shows an
example in which G = H(Π) is the same group of four rotations as before  so the object is still
pinned at the origin but not necessarily upright.
Motions are important because they induce ambiguities in pose recover. We formalise this concept
next. First  we note that  if G contains H(Π)  extending Π by G preserves all the symmetries:
Lemma 3. If H(Π) ⊂ G  then H(Π) ⊂ H(GΠ).
Second  consider being given a shape S (intended as a subset of R3) and being tasked with recovering
the pose π ∈ Π that generates S = π[S2]. Motions makes this recovery ambiguous:
Lemma 4. Let the pose space Π be closed under a transformation group G  in the sense that
GΠ = Π. Then  if pose π ∈ Π is a solution of the equation S = π[S2] and if h ∈ H(Π) ∩ G  then
πh−1 is another pose that solves the same equation.

1I.e. ∀p  q ∈ R3 : d(hp  hq) = d(p  q).

5

Lemma 4 does not necessarily provide a complete characterization of all the ambiguities in identifying
pose π from shape S; rather  it captures the ambiguities arising from the symmetry of the object and
its ability to move around in a certain manner. Nevertheless  it is possible for speciﬁc poses to result
in further ambiguities (e.g. consider a pose that deforms an object into a sphere).
In order to use the lemma to characterise ambiguities in pose recovery  given a pose space Π one
must still ﬁnd the space of possible motions G. We can take the latter to be the maximal subgroup
G∗ ⊂ SE(3) of rigid motions under which Π is closed2

4.1 Bilateral symmetry

Bilateral symmetries are generated by the reﬂection operator m of eq. (2): a pose space Π has bilateral
symmetry if H(Π) = {1  m}  which induces pairs of symmetric poses π(cid:48) = mπm−1 as in ﬁg. 2.
Even if poses Π are closed under rigid motions (i.e. G∗Π = Π where G∗ = SE(3))  in this case there
is generally no ambiguity in recovering the object pose from its shape S. The reason is that in lemma 4
one has G∗ ∩ H(Π) = {1} due to the fact that all transformations in G∗ are orientation-preserving
whereas m is not. This explains why it is possible to still distinguish left from right sides in most
bilaterally-symmetric objects despite symmetries and motions. However  this is not the case for other
types of symmetries such as radial.
Symmetry plane. Note that  given a pair of symmetric poses (π  π(cid:48))  π(cid:48) = mπm−1  the corre-
p (cid:55)→
spondences between the underlying 3D shapes are given by the map mπ :
(mπm−1π−1)(p). For example  in ﬁg. 2 this map sends the raised left hand of a person to the lowered
left hand in the symmetric pose. Of particular interest are the points where mπ coincides with m as
they are on the “plane of symmetry”. In fact  let p = π(z); then:

S → m[S] 

mπ(p) = m(p) ⇒ mπm−1π−1(p) = m(p) ⇒ m−1(z) = z ⇒ z =

(cid:34) 0

(cid:35)

.

(5)

z2
z3

4.2 Extrinsic symmetries

Our formulation captures the standard notion of extrinsic (standard) symmetries as well. If H(S) =
{h ∈ O(3) : h[S] = S} are the extrinsic symmetries of a geometric shape S (say regular pyramid) 
we can parametrize S using a single pose Π = {π0} that: (i) generates the shape (S = π0[S2]) and
(ii) has the same symmetries as the latter (H(Π) = H(S)).
In this case  the pose π0 is self-conjugate  in the sense that π0 = hπ0h−1 for all h ∈ H(Π).
Furthermore  given S it is obviously possible to recover the pose uniquely (since there is only
one element in Π); however  as before ambiguities arise by augmenting poses via rigid motions
G = SE(3). In this case  due to lemma 4  if gπ0 is a possible pose of S  so must be gπ0h−1. We
can rewrite the latter as (gh−1)(hπ0h−1) = (gh−1)π0  which shows that the ambiguous poses are
obtained via selected rigid motions gh−1 of the reference pose π0.

5 Learning with ambiguities

In section 3 we have explained how the learning formulation of [38] can be extended in order to
learn objects with a bilateral symmetry. The latter is an example where symmetries do not induce an
ambiguities in the recovery of the object’s pose (the reason is given in section 4.1). Now we consider
the case in which symmetries induce a genuine ambiguity in pose recovery.
Recall that ambiguities arise from a non-empty intersection of object symmetries H(Π) and object
motions G∗ (section 4). A typical example may be an object with a ﬁnite rotational symmetry group
(ﬁg. 3). In this case  it is not possible to recover the object pose uniquely from an image  which in
turn suggests that ψu(x) cannot be learned using the formulation of section 3.

2Being maximal means that G∗Π = G∗ ∧ GΠ = G ⇒ G ⊂ G∗. The maximal group can be constructed as
G∗ = (cid:104)G ⊂ SE(3) : GΠ = Π(cid:105)   where ⊂ denotes a subgroup and (cid:104)·(cid:105) the generated subgroup. This deﬁnition is
well posed: the generated group G∗ contains all the other subgroups G so it is maximal; furthermore G∗Π = Π
because  for any pose π ∈ Π and ﬁnite combination of other group elements  gn1

1 . . . gnk

k π ∈ Π.

6

Method

Eyes Mouth
23.29 15.27
[38] & plane est. 5.17 5.38
3.21 3.47

[38]

Ours

(a) Pixel error when using the re-
ﬂected descriptor from the left eye
or left mouth corner to locate its
counterpart on the right side of
the face  across 200 images from
CelebA (MAFL test subset)

(b) Visualisation of ﬁg. 4a.
+: ground truth. ◦ •: [38] with
no learned symmetry. ◦ •: [38]
with mirroring around the plane
estimated using annotations. ◦ •:
Our method. Where ◦ • is eye 
mouth respectively

(c) Difference between us (left)
and [38] (right). We learn an axis
aligned frame symmetric around
a plane (green)  [38] has arbitrary
rotation and no guaranteed sym-
metry plane. But we can estimate
a plane using annotations (cyan).

Figure 4: Comparing object frames

Figure 5: Bilateral symmetry of animal faces. The discovered plane of symmetry is shown in green.
Top: Inputs  Middle: Colour mapping  Bottom: Embedding (sphere) space

We propose to address this problem by relaxing loss (4) in order to discount the ambiguity as follows:

(cid:90)

Ω

LH(Π)(x  t) = min
h∈H(Π)

(cid:107)v − tu(cid:107)γ

2 ph(v|u) dvdu 

ph(v|u) =

(cid:82) exp(cid:104)hψu(x)  ψw(tx)(cid:105) dw

exp(cid:104)hψu(x)  ψv(tx)(cid:105)

(6)

This loss allows ψu(x) to estimate the embedding vector z ∈ S2 (or z ∈ R3) up to an unknown
transformation h.

6 Experiments

We now validate empirically our formulation. To ensure that we have a fair comparison to [38] 
who introduced learning formulation (4) which our approach extends  we use the same network
architecture and hyperparameter values (e.g. γ = 0.5 in eq. (4)). We show that our extension
successfully recovers the symmetric structure of bilateral objects (section 6.1) as well as allowing to
manage ambiguities arising from symmetries in learning such structures (section 6.2).

6.1 Learning objects with bilateral symmetry

In this section  we apply the learning formulation (4) to objects with a bilateral symmetry. Due to the
structure imposed on the embedding function by eq. (3)  we expect the symmetry plane of the object
to be mapped to the plane z1 = 0 in the embedding space (section 4.1). Once the model is learned 
this locus can be projected back to an image for visualisation and qualitative assessment. We also test
quantitatively the accuracy of the learned geometric embedding in localising object landmarks and
their symmetric counterparts.
Faces. We evaluate the proposed formulation on faces of humans and animals  which have limited
out-of-plane rotations. For humans we use the CelebA [23] face dataset  with over 200K images. We
use an identical setup to [38  39]  training on 162K images and employing the MAFL [46] subset
of 1000 images as a validation set. For cats we use the Cat Head dataset [45]  with 8609 training
images. We also combine multiple animals in the same training set  with Animal Faces dataset [36]
(20 animal classes  about 100 images per class). We exclude birds and elephants since these images
have a signiﬁcantly different appearance  and add additional cat  dog and human faces [45  31  23]
(but keep roughly the same distribution of animal classes per batch as the original dataset).

7

QueryTargetIn all cases  we do not use any manual annotation; instead  we use learning formulation (4) using the
same synthetic transformations t ∼ T as [38]. Additionally  with 50% probability we also apply a
left-to-right ﬂip m to both the image and the embedding space  as prescribed by eq. (4).
Results (ﬁgs. 1 and 5) show that our method  like [38]  learns a geometric embedding of the object
invariant to viewpoint and intra-category changes. In addition  our new formulation localises the
intrinsic bilateral symmetry plane in the face images and maps it to a plane of reﬂection in the
embedding space. We note that images are embedded symmetrically with respect to the plane (shown
in green in ﬁg. 1  bottom row). The plane can also be projected back to the image and  as predicted
by eq. (5)  corresponds to our intuitive notion of symmetry plane in faces (ﬁg. 1  top row). Importantly 
symmetry here is a statistical concept that applies to the category as a whole; speciﬁc face instances
need not be nor appear symmetric — the latter in particular means that faces need not be imaged
fronto-parallel for the method to capture their symmetry.
To evaluate the learned symmetry quantitatively we use manual annotations (eyes  mouth corners) to
verify if the representation can transport landmarks to their symmetric counterparts. In particular  we
take landmarks on the left side of the face (e.g. left eye)  use m (eq. (3)) to mirror their embedding
vectors  backproject those to the image  and compare the resulting positions to the ground-truth
symmetric landmark locations (e.g. right eye). We report the measured pixel error in ﬁg. 4a. As a
baseline  we replace our embedding function with the one from [38] which results in much higher
error. This is however expected as the mapping m has no particular meaning in this embedding
space; for a fairer comparison  we then explicitly estimate an ad-hoc plane of symmetry deﬁned by
the nose  mean of the eyes  and mean of the mouth corners  using 200 training images. This still
gives higher error than our method  showing that enforcing symmetry during training leads to a better
representation of symmetric objects.
In terms of the accuracy of the geometric embedding as such  we evaluate
simply matching annotations between different images and obtain similar
error to the embedding of [38] (ours 2.60  theirs 2.59 pixel error on 200
pairs of faces  and both 1.63 error for when the second image is a warped
version of the ﬁrst). Hence representing symmetries does not harm geometric
accuracy.
We also examine the inﬂuence of the synthetic warp intensity  in ﬁg. 6 we
train for 5 epochs scaling the original control point parameters by a factor 
indicating we are around the sweet spot and unnatural excessive warping is
harmful.
Synthetic 3D car model. A challenging problem is capturing bilateral
symmetry across out-of-plane rotations. We use a 3D car  animated with
random motion [13] for 30K frames. The heading follows a random walk 
eventually rotating 360◦ out of plane. Translation  pitch and roll are sinusoidal. The back of the car is
red to easily distinguish from the front. We use consecutive frames for training  with the ground truth
optical ﬂow used for t and image size 75 × 75. The loss ignores pixels with ﬂow smaller than 0.001 
preventing confusion with the solid background. Figure 8 depicts examples from this dataset. Unlike
CelebA  the cars are rendered from signiﬁcantly different views  but our method can successfully
localize the bilateral axis accurately.
Synthetic
robot arm model.
We trained our model on videos of
a left-right pair of robotics arms 
extending the setup of [38] to a
system of two arms.
Figure 7 shows the discovered sym-
metry by joining corresponding
points in a few video frames. Note that symmetries are learned automatically from raw videos
and ground truth optical ﬂow alone. Note also that none of the images is symmetric in the trivial
left-right ﬂip sense due to the object deformations.

Figure 7: Symmetry in a pair of toy robotics arms

Figure 6: Varying
warp intensity

8

012345Warp factor34567Symmetry Error (px)EyesMouthFigure 8: Bilateral symmetry on synthetic car images  Top: Input images with the axis of symmetry
superimposed (shown in green)  Bottom: Image pixels mapped to 3D with the reﬂection plane (green)

Figure 9: Rotational symmetry on protein. Top: Frames  found center of symmetry red. Middle:
Colorized object frame  a different colouring is assigned to each leg despite ambiguity. Bottom:
Embedding in 3D  it learns to be symmetric around an axis (red). Last column: Without relaxed loss.
6.2 Rotational symmetry

We create an example based on 3-fold rotational symmetry in nature  the Clathrin protein [43]. We
use the protein mesh3 and animate it as a soft body in a physics engine [13  7]  generating 200
400-frame sequences. For each we vary the camera rotation  lighting  mesh smoothing and position.
The protein is anchored at its centre. We vary the gravity vector to produce varied motion.
We train using the relaxed loss in eq. (6)  where H(Π) corresponds to rotating our sphere 0◦  120◦ or
240◦. The mapping then need only be learned up to this rotational ambiguity. As shown in ﬁg. 9  this
maps the protein images onto a canonical position which has rotational symmetry around the chosen
axis  whereas without the relaxed loss the object frame is not aligned and symmetrical.
We also show results for rotational symmetry in real images  using ﬂower class Stapelia from
ImageNet in ﬁg. 10 which has 5-fold rotational symmetry.

7 Conclusions

In this paper we have developed a new model of the symmetries of deformable object categories.
The main advantage of this approach is that it is ﬂexible and robust enough that it supports learning
symmetric objects in an unsupervised manner  from raw images  despite variable viewpoint  defor-
mations  and intra-class variations. We have also characterised ambiguities in pose recovery caused
by symmetries and developed a learning formulation that can handle them. Our contributions have
been validated empirically  showing that we can learn to represent symmetries robustly on a variety
of object categories  while retaining the accuracy of the learned geometric embedding compared to
previous approaches.

3https://www.rcsb.org/structure/3LVG

Figure 10: Rotational symmetry on Stapelia ﬂower. Superimposed in green  projection into the
image of a set of half-planes 72◦ apart in the sphere space. In red  predicted axis of rotational
symmetry.

9

Acknowledgments: This work acknowledges the support of the AIMS CDT (EPSRC EP/L015897/1) and ERC
638009-IDIU. We thank Almut Sophia Koepke for feedback and corrections.

References

[1] Helmut Alt  Kurt Mehlhorn  Hubert Wagener  and Emo Welzl. Congruence  similarity  and
symmetries of geometric objects. Discrete & Computational Geometry  3(3):237–256  1988.

[2] Fabio Anselmi  Georgios Evangelopoulos  Lorenzo Rosasco  and Tomaso Poggio. Symmetry-

adapted representation learning. Pattern Recognition  86:201–208  2019.

[3] Shai Bagon  Oren Boiman  and Michal Irani. What is a good image segment? a uniﬁed approach

to segment extraction. In Proc. ECCV  pages 30–44. Springer  2008.

[4] Hakan Bilen  Marco Pedersoli  and Tinne Tuytelaars. Weakly supervised object detection with

posterior regularization. In Proceedings BMVC 2014  pages 1–12  2014.

[5] Oren Boiman and Michal Irani. Similarity by composition. In Proc. NeurIPS  pages 177–184 

2007.

[6] T F Cootes  C J Taylor  D H Cooper  and J Graham. Active shape models: their training and

application. CVIU  1995.

[7] Erwin Coumans. Bullet physics engine. Open Source Software: http://bulletphysics. org  2010.

[8] Navneet Dalal and Bill Triggs. Histograms of Oriented Gradients for Human Detection. In

Proc. CVPR  2005.

[9] Aleksandrs Ecins  Cornelia Fermüller  and Yiannis Aloimonos. Cluttered scene segmentation
using the symmetry constraint. In Robotics and Automation (ICRA)  2016 IEEE International
Conference on  pages 2271–2278. IEEE  2016.

[10] Pedro F. Felzenszwalb  Ross B. Girshick  David McAllester  and Deva Ramanan. Object

Detection with Discriminatively Trained Part Based Models. PAMI  2010.

[11] Rob Fergus  Pietro Perona  and Andrew Zisserman. Object class recognition by unsupervised

scale-invariant learning. In Proc. CVPR  2003.

[12] Ran Gal and Daniel Cohen-Or. Salient geometric features for partial shape matching and

similarity. ACM Transactions on Graphics (TOG)  25(1):130–150  2006.

[13] Mike Goslin and Mark R Mine. The Panda3D graphics engine. Computer  37(10):112–114 

2004.

[14] Bumsub Ham  Minsu Cho  Cordelia Schmid  and Jean Ponce. Proposal ﬂow. In Proc. CVPR 

pages 3475–3484  2016.

[15] Kai Han  Rafael S Rezende  Bumsub Ham  Kwan-Yee K Wong  Minsu Cho  Cordelia Schmid 

and Jean Ponce. Scnet: Learning semantic correspondence. In Proc. ICCV  2017.

[16] Max Jaderberg  Karen Simonyan  Andrew Zisserman  and Koray Kavukcuoglu. Spatial Trans-

former Networks. In Proc. NeurIPS  2015.

[17] A. Kanazawa  D. W. Jacobs  and M. Chandraker. WarpNet: Weakly supervised matching for

single-view reconstruction. In Proc. CVPR  2016.

[18] Ira Kemelmacher-Shlizerman and Steven M. Seitz. Collection ﬂow. In Proc. CVPR  2012.

[19] Kurt Koffka. Principles of Gestalt psychology  volume 44. Routledge  2013.

[20] Erik G Learned-Miller. Data driven image models through continuous joint alignment. IEEE

Transactions on Pattern Analysis and Machine Intelligence  2006.

10

[21] Bastian Leibe  Ales Leonardis  and Bernt Schiele. Combined object categorization and segmen-
tation with an implicit shape model. In Workshop on statistical learning in computer vision 
ECCV  2004.

[22] Yanxi Liu  Hagit Hel-Or  Craig S Kaplan  Luc Van Gool  et al. Computational symmetry in
computer vision and computer graphics. Foundations and Trends R(cid:13) in Computer Graphics and
Vision  5(1–2):1–195  2010.

[23] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proc. ICCV  2015.

[24] Jonathan L Long  Ning Zhang  and Trevor Darrell. Do convnets learn correspondence? In

Advances in Neural Information Processing Systems  pages 1601–1609  2014.

[25] David G Lowe. Distinctive image features from scale-invariant keypoints. International journal

of computer vision  60(2):91–110  2004.

[26] Giovanni Marola. On the detection of the axes of symmetry of symmetric and almost symmetric

planar images. PAMI  11(1):104–108  1989.

[27] Niloy J Mitra  Leonidas J Guibas  and Mark Pauly. Symmetrization. In ACM Transactions on

Graphics (TOG)  volume 26  page 63. ACM  2007.

[28] Hossein Mobahi  Ce Liu  and William T. Freeman. A Compositional Model for Low-

Dimensional Image Set Representation. Proc. CVPR  2014.

[29] Gregory L Naber. The geometry of Minkowski spacetime: An introduction to the mathematics

of the special theory of relativity  volume 92. Springer Science & Business Media  2012.

[30] D. Novotny  D. Larlus  and A. Vedaldi. Learning 3d object categories by looking around them.

In Proc. ICCV  2017.

[31] O. M. Parkhi  A. Vedaldi  A. Zisserman  and C. V. Jawahar. Cats and dogs. In Proc. CVPR 

2012.

[32] Yigang Peng  Arvind Ganesh  John Wright  Wenli Xu  and Yi Ma. Rasl: Robust alignment by
sparse and low-rank decomposition for linearly correlated images. PAMI  34(11):2233–2246 
2012.

[33] Dan Raviv  Alexander M Bronstein  Michael M Bronstein  and Ron Kimmel. Full and partial

symmetries of non-rigid shapes. IJCV  89(1):18–39  2010.

[34] I. Rocco  R. Arandjelovi´c  and J. Sivic. Convolutional neural network architecture for geometric

matching. In Proc. CVPR  2017.

[35] Ilan Shimshoni  Yael Moses  and Michael Lindenbaum. Shape reconstruction of 3d bilaterally

symmetric surfaces. IJCV  39(2):97–110  2000.

[36] Zhangzhang Si and Song-Chun Zhu. Learning hybrid image templates (hit) by information

projection. PAMI.

[37] Changming Sun and Jamie Sherrah. 3d symmetry detection using the extended gaussian image.

PAMI  19(2):164–168  1997.

[38] J. Thewlis  H. Bilen  and A. Vedaldi. Unsupervised learning of object frames by dense

equivariant image labelling. In Proc. NeurIPS  2017.

[39] J. Thewlis  H. Bilen  and A. Vedaldi. Unsupervised learning of object landmarks by factorized

spatial embeddings. In Proc. ICCV  2017.

[40] Sebastian Thrun and Ben Wegbreit. Shape from symmetry. In Proc. ICCV  pages 1824–1831 

2005.

[41] Thomas Vetter and Tomaso Poggio. Linear object classes and image synthesis from a single
example image. IEEE Transactions on Pattern Analysis and Machine Intelligence  19(7):733–
742  1997.

11

[42] James D Watson  Francis HC Crick  et al. Molecular structure of nucleic acids. Nature 

171(4356):737–738  1953.

[43] Jeremy D Wilbur  Peter K Hwang  Joel A Ybe  Michael Lane  Benjamin D Sellers  Matthew P
Jacobson  Robert J Fletterick  and Frances M Brodsky. Conformation switching of clathrin light
chain regulates clathrin lattice assembly. Developmental cell  18(5):854–861  2010.

[44] Heng Yang and Ioannis Patras. Mirror  mirror on the wall  tell me  is the error small? In Proc.

CVPR  pages 4685–4693  2015.

[45] Weiwei Zhang  Jian Sun  and Xiaoou Tang. Cat head detection - How to effectively exploit

shape and texture features. In Proc. ECCV  2008.

[46] Zhanpeng Zhang  Ping Luo  Chen Change Loy  and Xiaoou Tang. Learning Deep Representation

for Face Alignment with Auxiliary Attributes. PAMI  2016.

[47] Zheng Zhang  Wei Shen  Cong Yao  and Xiang Bai. Symmetry-based text line detection in

natural scenes. In Proc. CVPR  pages 2558–2567  2015.

12

,James Thewlis
Hakan Bilen
Andrea Vedaldi
Jinjin Tian
Aaditya Ramdas