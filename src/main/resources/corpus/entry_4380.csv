2011,Prediction strategies without loss,Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say `predict 0' or `predict 1'  and our payoff is $+1$ if the prediction is correct and $-1$ otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far.  In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting $0$ or always predicting $1$. For a sequence of length $T$ our algorithm has  regret $14\epsilon T $ and loss   $2\sqrt{T}e^{-\epsilon^2 T} $ in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors.    Our techniques extend to the general setting of $N$ experts  where the related problem of trading off regret to the best expert for regret to the 'special' expert has been studied by Even-Dar et al. (COLT'07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff   improving upon the results of Even-Dar et al (COLT'07) and settling the main question left open in their paper.     The strong loss bounds of the algorithm have some surprising consequences. First  we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to $k$-shifting optima  i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover  for {\em any window of size $n$} the regret of our algorithm to any expert never exceeds $O(\sqrt{n(\log N+\log T)})$  where $N$ is the number of experts and $T$ is the time horizon  while maintaining the essentially zero loss property.,Prediction strategies without loss

Michael Kapralov
Stanford University

Stanford  CA

kapralov@stanford.edu

Rina Panigrahy

Microsoft Research Silicon Valley

Mountain View  CA

rina@microsoft.com

Abstract

Consider a sequence of bits where we are trying to predict the next bit from the
previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’  and our
payoff is +1 if the prediction is correct and −1 otherwise. We will say that at
each point in time the loss of an algorithm is the number of wrong predictions
minus the number of right predictions so far. In this paper we are interested in
algorithms that have essentially zero (expected) loss over any string at any point
in time and yet have small regret with respect to always predicting 0 or always
√
predicting 1. For a sequence of length T our algorithm has regret 14T and loss
T e−2T in expectation for all strings. We show that the tradeoff between loss
2
and regret is optimal up to constant factors.
Our techniques extend to the general setting of N experts  where the related prob-
lem of trading off regret to the best expert for regret to the ’special’ expert has
been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with
respect to the special expert and optimal loss/regret tradeoff  improving upon the
results of Even-Dar et al and settling the main question left open in their paper.
The strong loss bounds of the algorithm have some surprising consequences.
First  we obtain a parameter free algorithm for the experts problem that has op-
timal regret bounds with respect to k-shifting optima  i.e. bounds with respect
to the optimum that is allowed to change arms multiple times. Moreover  for
any window of size n the regret of our algorithm to any expert never exceeds

O((cid:112)n(log N + log T ))  where N is the number of experts and T is the time

horizon  while maintaining the essentially zero loss property.

1

Introduction

Consider a gambler who is trying to predict the next bit in a sequence of bits. One could think of
the bits as indications of whether a stock price goes up or down on a given day  where we assume
that the stock always goes up or down by 1 (this is  of course  a very simpliﬁed model of the stock
market). If the gambler predicts 1 (i.e. that the stock will go up)  she buys one stock to sell it the next
day  and short sells one stock if her prediction is 0. We will also allow the gambler to bet fractionally
by letting him specify a conﬁdence c where 0 ≤ c ≤ 1 in his prediction. If the prediction is right
the gambler gets a payoff of c otherwise −c. While the gambler is tempted to make predictions with
the prospect of making money  there is also the risk of ending up with a loss. Is there a way to never
end up with a loss? Clearly there is the strategy of never predicting (by setting conﬁdence 0) all
the time that never has a loss but also never has a positive payoff. However  if the sequence is very
imbalanced and has many more 0’s than 1’s then this never predict strategy has a high regret with
respect to the strategy that predicts the majority bit. Thus  one is interested in a strategy that has a
small regret with respect to predicting the majority bit and incurs no loss at the same time.
√
Our main result is that while one cannot always avoid a loss and still have a small regret  this is
possible if we allow for an exponentially small loss. More precisely  we show that for any  > 1/
T

1

T   where T is

√
there exists an algorithm that achieves regret at most 14T and loss at most 2e−2T
the time horizon. Thus  the loss is exponentially small in the length of the sequence.
The bit prediction problem can be cast as the experts problem with two experts: S+  that always
predicts 1 and S− that always predicts 0. This problem has been studied extensively  and very
√
efﬁcient algorithms are known. The weighted majority algorithm of [12] is known to give optimal
regret guarantees. However  it can be seen that weighted majority may result in a loss of Ω(
T ).
The best known result on bounding loss is the work of Even-Dar et al. [7] on the problem of trading
√
off regret to the best expert for regret to the average expert  which is equivalent to our problem.
√
Stated as a result on bounding loss  they were able to obtain a constant loss and regret O(
T log T ).
Their work left the question open as to whether it is possible to even get a regret of O(
T log T )
and constant loss. In this paper we give an optimal regret/loss tradeoff  in particular showing that
this regret can be achieved even with subconstant loss.
Our results extend to the general setting of prediction with expert advice when there are multiple
experts.
In this problem the decision maker iteratively chooses among N available alternatives
without knowledge of their payoffs  and gets payoff based on the chosen alternative. The payoffs
of all alternatives are revealed after the decision is made. This process is repeated over T rounds 
and the goal of the decision maker is to maximize her cumulative payoff over all time steps t =
1  . . .   T . This problem and its variations has been studied extensively  and efﬁcient algorithms
have been obtained (e.g. [5  12  6  2  1]). The most widely used measure of performance of an
online decision making algorithm is regret  which is deﬁned as the difference between the payoff
of the best ﬁxed alternative and the payoff of the algorithm. The well-known weighted majority
algorithm of [12] obtains regret O(
T log N ) even when no assumptions are made on the process
generating the payoff. Regret to the best ﬁxed alternative in hindsight is a very natural notion when
√
the payoffs are sampled from an unknown distribution  and in fact such scenarios show that the
bound of O(
√
Even-Dar et al. [7] gave an algorithm that has constant regret to any ﬁxed distribution on the experts
T log N (log T + log log N )) with respect to all other experts1. We ob-
at the expense of regret O(

tain an optimal tradeoff between the two  getting an algorithm with regret O((cid:112)T (log N + log T ))

T log N ) on regret achieved by the weighted majority algorithm is optimal.

√

(cid:17)

(cid:16) x

to the best and O((N T )−Ω(1)) to the average as a special case. We also note  similarly to [7] that
our regret/loss tradeoff cannot be obtained by using standard regret minimization algorithms with
a prior that is concentrated on the ’special’ expert  since the prior would have to put a signiﬁcant
weight on the ’special’ expert  resulting in Ω(T ) regret to the best expert.
The extension to the case of N experts uses the idea of improving one expert’s predictions by that of
another. The strong loss bounds of our algorithm allow us to achieve lossless boosting  i.e. we use
available expert to continuously improve upon the performance of the base expert whenever possible
while essentially never hurting its performance. When comparing two experts  we track the differ-
ence in the payoffs discounted geometrically over time and apply a transform g(x) on this difference
to obtain a weighting that is applied to give a linear combination of the two experts with a higher
weight being applied on the expert with a higher discounted payoff. The shape of g(x) is given
ex2/(16T )  capped at ±1. The weighted majority algorithm on the other hand uses
by erf
a transform with the shape of the tanh( x√
) function and ignores geometric discounting (see Fig-
ure 1).
An important property of our algorithm is that it does not need a high imbalance between the number
of ones and the number of zeros in the whole sequence to have a gain: it is sufﬁcient for the imbal-
ance to be large enough in at least one contiguous time window2  the size of which is a parameter
of the algorithm. This property allows us to easily obtain optimal adaptive regret bounds  i.e. we

show that the payoff of our algorithm in any geometric window of size n is at most O((cid:112)n log(N T ))
T log N (log T +log log N )) regret to the best and D-Prod  yielding O((cid:112)T / log N log T ) regret

1In fact  [7] provide several algorithms  of which the most relevant for comparison are Phased Agression 

yielding O(
to the best. For the bit prediction problem one would set N = 2 and use the uniform distribution over the
‘predict 0’ and ‘predict 1’ strategy as the special distribution. Our algorithm improves on both of them  yielding
an optimal tradeoff.

√
T

4

2More precisely  we use an inﬁnite window with geometrically decreasing weighting  so that most of the

weight is contained in the window of size O(n)  where n is a parameter of the algorithm.

√

T

2

worse than the payoff of the strategy that is best in that window (see Theorem 11). In the full version
of the paper ([11]) we also obtain bounds against the class of strategies that are allowed to change
experts multiple times while maintaining the essentially zero loss property. We note that even though
similar bounds (without the essentially zero loss property) have been obtained before ([3  9  14] and 
more recently  [10])  our approach is very different and arguably simpler.
In the full version of the paper  we also show how our algorithm yields regret bounds that depend on
the lp norm of the costs  regret bounds dependent on Kolmogorov complexity as well as applications
of our framework to multi-armed bandits with partial information and online convex optimization.

1.1 Related work

The question of what can be achieved if one would like to have a signiﬁcantly better guarantee
with respect to a ﬁxed arm or a distribution on arms was asked before in [7] as we discussed in
the introduction. Tradeoffs between regret and loss were also examined in [13]  where the author
studied the set of values of a  b for which an algorithm can have payoff aOP T + b log N  where
OP T is the payoff of the best arm and a  b are constants. The problem of bit prediction was also
considered in [8]  where several loss functions are considered. None of them  however  corresponds
to our setting  making the results incomparable.
In recent work on the NormalHedge algorithm[4] the authors use a potential function which is very
similar to our function g(x) (see (2) below)  getting strong regret guarantees to the -quantile of best
experts. However  the use of the function g(x) seems to be quite different from ours  as is the focus
of the paper [4].

1.2 Preliminaries

We start by deﬁning the bit prediction problem formally. Let bt  t = 1  . . .   T be an adversarial
It will be convenient to adopt the convention that bt ∈ {−1  +1} instead of
sequence of bits.
bt ∈ {0  1} since it simpliﬁes the formula for the payoff. In fact  in what follows we will only
assume that −1 ≤ bt ≤ 1  allowing bt to be real numbers. At each time step t = 1  . . .   T the
algorithm is required to output a conﬁdence level ft ∈ [−1  1]  and then the value of bt is revealed
to it. The payoff of the algorithm by time t(cid:48) is

t(cid:48)(cid:88)

t=1

(1)
For example  if bt ∈ {−1  +1}  then this setup is analogous to a prediction process in which a player
observes a sequence of bits and at each point in time predicts that the value of the next bit will be
sign(ft) with conﬁdence |ft|. Predicting ft ≡ 0 amounts to not playing the game  and incurs no
loss  while not bringing any proﬁt. We deﬁne the loss of the algorithm on a string b as

At(cid:48) =

ftbt.

loss = min{−At  0} 

i.e. the absolute value of the smallest negative payoff over all time steps.
It is easy to see that any algorithm that has a positive expected payoff on some sequence necessar-
ily loses on another sequence. Thus  we are concerned with ﬁnding a prediction strategy that has
exponentially small loss bounds but also has low regret against a number of given prediction strate-
gies. In the simplest setting we would like to design an algorithm that has low regret against two
basic strategies: S+  which always predicts +1 and S−  which always predicts −1. Note that the
maximum of the payoffs of S+ and S− is always equal to
strategy  which predicts with conﬁdence 0  by S0. In what follows we will use the notation AT for
the cumulative payoff of the algorithm by time T as deﬁned above.
As we will show in section 3  our techniques extend easily to give an algorithm that has low regret
with respect to the best of any N bit prediction strategies and exponentially small loss. Our tech-
niques work for the general experts problem  where loss corresponds to regret with respect to the
’special’ expert S0  and hence we give the proof in this setting. This provides the connection to the
work of [7].
In section 2 we give an algorithm for the case of two prediction strategies S+ and S−  and in section 3
we extend it to the general experts problem  additionally giving the claimed adaptive regret bounds.

(cid:12)(cid:12)(cid:12). We denote the base random

(cid:12)(cid:12)(cid:12)(cid:80)T

t=1 bt

3

2 Main algorithm

The main result of this section is

Theorem 1 For any  ≥(cid:113) 1

AT ≥ max



T there exists an algorithm A for which
√

 − 2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − 14T  0
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88)
T   we get a regret bound of(cid:112)T log(1/Z).

T e−2T  

bj

j=1

√

i.e. the algorithm has at most 14T regret against S+ and S− as well as a exponentially small loss.
By setting  so that the loss bound is 2Z

the algorithm maintains a discounted deviation xt = (cid:80)t−1

We note that the algorithm is a strict generalization of weighted majority  which can be seen by
letting Z = Θ(1) (this property will also hold for the generalization to N experts in section 3).
Our algorithm will have the following form. For a chosen discount factor ρ = 1 − 1/n  0 ≤ ρ ≤ 1
j=1 ρt−1−jbj at each time t = 1  . . .   T .
The value of the prediction at time t is then given by g(xt) for a function g(·) to be deﬁned (note
that xt depends only on bt(cid:48) for t(cid:48) < t  so this is an online algorithm). The function g as well as the
discount factor ρ depend on the desired bound on expected loss and regret against S+ and S−. In
particular  we will set ρ = 1− 1
T for our main result on regret/loss tradeoff  and will use the freedom
to choose different values of ρ to obtain adaptive regret guarantees in section 3. The algorithm is
given by

Algorithm 1: Bounded loss prediction
1: x1 ← 0
2: for t = 1 to T do
3:
4:
5: end for

Predict sign(g(xt)) with conﬁdence |g(xt)|.
Set xt+1 ← ρxt + bt.

We start with an informal sketch of the proof  which will be made precise in Lemma 2 and Lemma 3
below. The proof is based on a potential function argument.
In particular  we will choose the
conﬁdence function g(x) so that

Φt =

g(s)ds.

(cid:90) xt

0

(cid:90) x

is a potential function  which serves as a repository for guarding our loss (we will chose g(x) to be
an odd function  and hence will always have Φt ≥ 0). In particular  we will choose g(x) so that the
change of Φt lower bounds the payoff of the algorithm. If we let Φt = G(xt) (assuming for sake of
clarity that xt > 0)  where

G(x) =

g(s)ds 

we have
Φt+1 − Φt = G(xt+1) − G(xt) ≈ G(cid:48)(x)∆x + G(cid:48)(cid:48)(x)∆x2/2 ≈ g(x) [(ρ − 1)x + bt] + g(cid:48)(x)/2.

0

Since the payoff of the algorithm at time step t is g(xt)bt  we have

∆Φt − g(xt)bt = −g(xt)(1 − ρ)xt + g(cid:48)(xt)/2 

so the condition becomes

−g(xt)(1 − ρ)xt + g(cid:48)(xt)/2 ≤ Z 

where Z is the desired bound on per step loss of the algorithm. Solving this equation yields a
function of the form

g(x) = (2Z

ex2/(16T ) 

(cid:19)

(cid:18) x

√
4

T

√

T ) · erf

4

(cid:82) x
0 e−s2

π

where erf(x) = 2√

ds is the error function (see Figure 1 for the shape of g(x)).

We now make this proof sketch precise. For t = 1  . . .   T deﬁne Φt =(cid:82) xt

0 g(x)dx. The function
g(x) will be chosen to be a continuous odd function that is equal to 1 for x > U and to −1 when
x < −U  for some 0 < U < T . Thus  we will have that |xt| − U ≤ Φt ≤ |xt|. Intuitively  Φt
captures the imbalance between the number of −1’s and +1’s in the sequence up to time t.
We will use the following parameters. We always have ρ = 1 − 1/n for some n > 1 and use the
notation ¯ρ = 1 − ρ. We will later choose n = T to prove Theorem 1  but we will use different value
of n for the adaptive regret guarantees in section 3.
We now prove that if the function g(x) approximately satisﬁes a certain differential equation  then
Φt deﬁned as above is a potential function. The statement of Lemma 2 involves a function h(x) that
will be chosen as a step function that is 1 when x ∈ [−U  U ] and 0 otherwise.
Lemma 2 Suppose that the function g(x) used in Algorithm 1 satisﬁes

(¯ρx + 1)2 ·

1
2

max

s∈[ρx−1 ρx+1]

|g(cid:48)(s)| ≤ ¯ρxg(x)h(x) + Z(cid:48)

for a function h(x)  0 ≤ h(x) ≤ 1 ∀x  for some Z(cid:48) > 0. Then the payoff of the algorithm is at least

T(cid:88)

t=1

¯ρxtg(xt)(1 − h(x)) + ΦT +1 − Z(cid:48)T

as long as |bt| ≤ 1 for all t.
Proof: We will show that at each t

i.e.

T(cid:88)

t=1

T(cid:88)

t=1

Φt+1 − Φt ≤ btg(xt) + Z(cid:48) − ¯ρxtg(xt)(1 − h(xt)) 

btg(xt) ≥ −Z(cid:48)T +

¯ρxtg(xt)(1 − h(xt)) + ΦT +1 − Φ1 

thus implying the claim of the lemma since Φ1 = 0.
We consider the case xt > 0. The case xt < 0 is analogous. In the following derivation we will
write [A  B] to denote [min{A  B}  max{A  B}].
0 ≤ bt ≤ 1: We have xt+1 = ρxt + bt = xt − ¯ρxt + bt  and the expected payoff of the algorithm

is g(xt)bt. Then

Φt+1 − Φt =

(cid:90) xt− ¯ρxt+bt
(cid:20)

xt

g(s)ds

≤ g(xt)(bt − ¯ρxt) +

(¯ρxt + bt)2 ·

1
2

max

s∈[xt xt− ¯ρxt+bt]

|g(cid:48)(s)|

−g(xt)¯ρxt +

≤ g(xt)bt +
(¯ρxt + 1)2 ·
≤ g(xt)bt + (−1 + h(xt))¯ρxtg(xt) + Z(cid:48).

1
2

max

s∈[xt xt− ¯ρxt+bt]

|g(cid:48)(s)|

(cid:21)

−1 ≤ bt ≤ 0: This case is analogous.
We now deﬁne g(x) to satisfy the requirement of Lemma 2. For any Z  L > 0 and let

(cid:18)|x|

(cid:19)

(cid:27)

One can show that one has g(x) = 1 for |x| ≥ U for some U ≤ 7L(cid:112)log(1/Z). A plot of the

g(x) = sign(xt) · min

Z · erf

x2
16L2   1

(2)

4L

e

.

function g(x) is given in Figure 1.
We choose

h(x) =

|x| < U
o.w.

.

(3)

(cid:26)

(cid:26) 1 

0

The following lemma shows that the function g(x) satisﬁes the properties stated in Lemma 2:

5

tanh(cid:0) x

U

(cid:1)

+1

−U

g(x)

−1

+U

x

Figure 1: The shape of the conﬁdence function g(x) (solid line) and the tanh(x) function used by
weighted majority (dotted line).

Lemma 3 Let L > 0 be such that ¯ρ = 1/n ≥ 1/L2. Then for n ≥ 80 log(1/Z) the function g(x)
deﬁned by (2) satisﬁes

(¯ρx + 1)2 ·

1
2

max

|g(cid:48)(s)| ≤ ¯ρxg(x)h(x)/2 + 2¯ρLZ 

s∈[ρx−1 ρx+1]
where h(x) is the step function deﬁned above.
Proof: The intuition behind the Lemma is very simple. Note that s ∈ [ρx − 1  ρx + 1] is not much
πL Z. Since ¯ρ ≥ 1/L2 
further than 1 away from x  so g(cid:48)(s) is very close to g(cid:48)(x) = ( x
we have g(cid:48)(x) ≤ ¯ρxg(x)/2 + 1√
We can now lower bound the payoff of Algorithm 1. We will use the notation

π ¯ρLZ. We defer the details of the proof to the full version.

2L2 )g(x) + 1√

(cid:26)

|x|+

 =

0 

|x| −  

|x| < 
|x| > 

Theorem 4 Let n be the window size parameter of Algorithm 1. Then one has

¯ρ|xt|+

U + |xT +1|+

√
U − 2ZT /

n.

AT ≥ T(cid:88)

t=1

T(cid:88)

t=1

Proof: By Lemma 3 we have that the function g(x) satisﬁes the conditions of Lemma 2  and so
from the bounds stated in Lemma 2 the payoff of the algorithm is at least

¯ρ|xt|+

√
U + ΦT +1 − 2ZT /

n.

Theorem 5

AT ≥ max

U   which gives the

By deﬁnition of Φt  since |g(x)| = 1 for |x| ≥ U  one has ΦT +1 ≥ |xT +1|+
desired statement.
Now  setting n = T   we obtain



(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) T(cid:88)

j=1

bj

√

 − 2Z
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) − 14(cid:112)T log(1/Z)  0
T(cid:88)

bt(1 − ρT−t) +

T .

Proof: In light of Theorem 4 it remains to bound(cid:80)T
T(cid:88)
T−1(cid:88)
Thus  since U ≤ 2(cid:112)T log(1/Z)  and we chose ρ = 1 − 1/n = 1 − 1/T   we get the result by

t=1 ¯ρxt + xT +1. We have

ρt−jbj + xT +1 =

ρT−tbt =

xt + xT +1 = ¯ρ

T−1(cid:88)

T(cid:88)

t(cid:88)

(4)

t=1

j=1

bt.

t=1

t=1

t=1

t=1

¯ρ

combining Theorem 4 and equation (4).
Proof of Theorem 1: Follows by setting log(1/Z) = 2T .
Our loss/regret tradeoff is optimal up to constant factors (proof deferred to the full version):

6

Theorem 6 Any algorithm that has regret O((cid:112)T log(1/Z)) incurs loss Ω(Z

sequence of bits bt  t = 1  . . .   T .

√

T ) on at least one

Note that if Z = o(1/T )  then the payoff of the algorithm is positive whenever the absolute value of
the deviation xt is larger than  say 8

n log T in at least one window of size n.

√

3 Combining strategies (lossless boosting)

In the previous section we derived an algorithm for the bit prediction problem with low regret to
the S+ and S− strategies and exponentially small loss. We now show how our techniques yield an
algorithm that has low regret to the best of N bit prediction strategies S1  . . .   SN and exponentially
small loss. However  since the proof works for the general experts problem  where loss corresponds
to regret to a ’special’ expert S0  we state it in the general experts setting. In what follows we will
refer to regret to S0 as loss. We will also prove optimal bounds on regret that hold in every window
of length n at the end of the section. We start by proving

strategy S0 ﬁxed a priori. These bounds are optimal up to constant factors.

Theorem 7 For any Z < 1/e there exists an algorithm for combining N strategies that has regret
T ) with respect to any

O((cid:112)T log(N/Z)) against the best of N strategies and loss at most O(ZN
wjt on the set of experts j = 1  . . .   N such that wjt depends only on bt(cid:48)  t(cid:48) < t and(cid:80)N

We ﬁrst ﬁx notation. A prediction strategy S given a bit string bt  produces a sequence of weights
j=1 wjt =
1  wjt ≥ 0 for all t. Thus  using strategy S amounts to using expert j with probability wj t at time
t  for all t = 1  . . .   T . For two strategies S1  S2 we write αtS1 + (1 − αt)S2 to denote the strategy
whose weights are a convex combination of weights of S1 and S2 given by coefﬁcients αt ∈ [0  1].
For a strategy S we denote its payoff at time t by st.
We start with the case of two strategies S1  S2. Our algorithm will consider S1 as the base strategy
(corresponding to the null strategy S0 in the previous section) and will use S2 to improve on S1
whenever possible  without introducing signiﬁcant loss over S1 in the process. We deﬁne

√

(cid:26) g( 1

¯g(x) =

2 x)  x > 0
o.w 
0

Algorithm 1 on this sequence. In particular  we set xt = (cid:80)t−1

i.e. we are using a one-sided version of g(x). It is easy to see that ¯g(x) satisﬁes the conditions of
Lemma 2 with h(x) as deﬁned in (3). The intuition behind the algorithm is that since the difference
in payoff obtained by using S2 instead of S1 is given by (s2 t − s1 t)  it is sufﬁcient to emulate
j=1 ρt−1−j(s2 j − s1 j) and predict
¯g(xt) (note that since |s1 t − s2 t| ≤ 2  we need to use g( 1
2 x) in the deﬁnition of ¯g to scale the
payoffs). Predicting 0 corresponds to using S1  predicting 1 corresponds to using S2 and fractional
values correspond to a convex combination of S1 and S2.
Formally  the algorithm COMBINE(S1  S2  ρ) takes the following form:

Algorithm 2: COMBINE(S1  S2  ρ)
1: Input: strategies S1  S2
2: Output: strategy S∗
3: x1 ← 0
4: for t = 1 to T do
Set S∗
5:
Set xt+1 ← ρxt + (s2 t − s1 t).
6:
7: end for
8: return S∗
Note that COMBINE(S1  S2  ρ) is an online algorithm  since S∗

t ← S1 t(1 − ¯g(xt)) + S2 t¯g(xt).

t only depends on s1 t(cid:48)  s2 t(cid:48)  t(cid:48) < t.

7

Lemma 8 There exists an algorithm that given two strategies S1 and S2 gets payoff at least

T(cid:88)

(cid:40) T(cid:88)

s1 t + max

(s2 t − s1 t) − O

(cid:16)(cid:112)T log(1/Z)
(cid:17)

  0

(cid:41)

− O(Z

√

T ).

t=1

t=1

PROOF OUTLINE: Use Algorithm 2 with ρ = 1 − 1/T . This amounts to applying Algorithm 1 to
the sequence (s2 t − s1 t)  so the guarantees follow by Theorem 4.
We emphasize the property that Algorithm 2 combines two strategies S1 and S2  improving on the
performance of S1 using S2 whenever possible  essentially without introducing any loss with respect
to S1. Thus  this amounts to lossless boosting of one strategy’s performance using another. Thus 
we have
Proof of Theorem 7: Use Algorithm 2 repeatedly to combine N strategies S1  . . .   SN by initializ-
ing S0 ← S0 and setting Sj ← COMBINE(Sj−1  Sj  1− 1/T )  j = 1  . . .   N  where S0 is the null
strategy. The regret and loss guarantees follow by Lemma 8.

Corollary 9 Setting Z = (N T )−1−γ for γ > 0  we get regret O((cid:112)γT (log N + log T )) to the

best of N strategies and loss at most O((N T )−γ) wrt strategy S0 ﬁxed a priori. These bounds are
optimal and improve on the work on [7].
So far we have used ρ = 1 − 1/T for all results. One can obtain optimal adaptive guaran-
tees by performing boosting over a range of decay parameters ρ.
In particular  choose ρj =
1 − nj  where nj  j = 1  . . .   W are powers of two between 80 log(N T ) and T . Then let
Algorithm 3: Boosting over different time scales
1: S0 W ← S0
2: for j = W downto 1 do
3:
4:
5:
6:
7: end for
8: S∗ ← S0 0
9: return S∗

Sk j ← COMBINE(Sk−1 j  Sk  1 − 1/nj)

end for
S0 j−1 ← SN j

for k = 1 to N do

We note that it is important that the outer loop in Algorithm 3 goes from large windows down to
small windows. Finally  we show another adaptive regret property of Algorithm 3. First  for a
sequence wt  t = 1  . . .   T of real numbers and for a parameter ρ = 1 − 1/n ∈ (0  1) deﬁne

t(cid:88)

j=1

˜wρ

t =

ρt−jwj.

ρ ≤ c(cid:112)n log(1/Z)

We will need the following deﬁnition:

Deﬁnition 10 A sequence wj is Z-uniform at scale ρ = 1 − 1/n if one has(cid:102)wt

for all 1 ≤ t ≤ T   for some constant c > 0.
Note that if the input sequence is iid Ber(±1  1/2)  then it is Z-uniform at any scale with probability
at least 1 − Z for any Z > 0. We now prove that the difference between the payoff of our algorithm
and the payoff of any expert is Z-uniform  i.e. does not exceed the standard deviation of a uniformly
random variable in any sufﬁciently large window  when the loss is bounded by Z. More precisely 
Theorem 11 The sequences sj t − s∗
t are Z-uniform for any 1 ≤ j ≤ N at any scale ρ ≥ 1 −
1/(80 log(1/Z)) when Z = o((N T )−2). Moreover  the loss of the algorithm with respect to the
base strategy is at most 2ZN

√

T .

The proof of Theorem 11 is given in the full version.

8

References
[1] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. COLT 

2009.

[2] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. Schapire. The nonstochastic multi-armed bandit

problem. SIAM J. Comput.  32:48–77  2002.

[3] A. Blum and Y. Mansour. From external to internal regret. Journal of Machine Learning

Research  pages 1307–1324  2007.

[4] K. Chaudhuri  Y. Freund  and D. Hsu. A parameter free hedging algorithm. NIPS  2009.
[5] T. Cover. Behaviour of sequential predictors of binary sequences. Transactions of the Fourth
Prague Conference on Information Theory  Statistical Decision Functions  Random Processes 
1965.

[6] T. Cover. Universal portfolios. Mathematical Finance  1991.
[7] E. Even-Dar  M. Kearns  Y. Mansour  and J. Wortman. Regret to the best vs. regret to the

average. Machine Learning  72:21–37  2008.

[8] Y. Freund. Predicting a binary sequence almost as well as the optimal biased coin. COLT 

1996.

[9] Y. Freund  R. E. Schapire  Y. Singer  and M. K. Warmuth. Using and combining predictors

that specialize. STOC  pages 334–343  1997.

[10] E. Hazan and C. Seshadhri. Efﬁcient learning algorithms for changing environments (full ver-
sion available at http://eccc.hpi-web.de/eccc-reports/2007/tr07-088/index.html). ICML  pages
393–400  2009.

[11] M. Kapralov and R. Panigrahy. Prediction without loss in multi-armed bandit problems.

http://arxiv.org/abs/1008.3672  2010.

[12] N. Littlestone and M.K. Warmuth. The weighted majority algorithm. FOCS  1989.
[13] V. Vovk. A game of prediction with expert advice. Journal of Computer and System Sciences 

1998.

[14] V. Vovk. Derandomizing stochastic prediction strategies. Machine Learning  pages 247–282 

1999.

9

,Long Jin
Justin Lazarow
Zhuowen Tu