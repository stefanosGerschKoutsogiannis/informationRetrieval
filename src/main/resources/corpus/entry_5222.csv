2017,Sticking the Landing: Simple  Lower-Variance Gradient Estimators for Variational Inference,We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically  we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically  and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.,Sticking the Landing: Simple  Lower-Variance
Gradient Estimators for Variational Inference

Geoffrey Roeder
University of Toronto

roeder@cs.toronto.edu

Yuhuai Wu

University of Toronto

ywu@cs.toronto.edu

David Duvenaud

University of Toronto

duvenaud@cs.toronto.edu

Abstract

We propose a simple and general variant of the standard reparameterized gradient
estimator for the variational evidence lower bound. Speciﬁcally  we remove a part
of the total derivative with respect to the variational parameters that corresponds to
the score function. Removing this term produces an unbiased gradient estimator
whose variance approaches zero as the approximate posterior approaches the exact
posterior. We analyze the behavior of this gradient estimator theoretically and
empirically  and generalize it to more complex variational distributions such as
mixtures and importance-weighted posteriors.

1

Introduction

)

e
u
r
t

φ
(cid:107)

t
i
n
i
φ
(
L
K

Recent advances in variational inference have begun to
make approximate inference practical in large-scale latent
variable models. One of the main recent advances has
been the development of variational autoencoders along
with the reparameterization trick [Kingma and Welling 
2013  Rezende et al.  2014]. The reparameterization
trick is applicable to most continuous latent-variable mod-
els  and usually provides lower-variance gradient esti-
mates than the more general REINFORCE gradient es-
timator [Williams  1992].
Intuitively  the reparameterization trick provides more in-
formative gradients by exposing the dependence of sam-
pled latent variables z on variational parameters φ. In
contrast  the REINFORCE gradient estimate only de-
pends on the relationship between the density function
log qφ(z|x  φ) and its parameters.
Surprisingly  even the reparameterized gradient estimate
contains the score function—a special case of the REIN-
FORCE gradient estimator. We show that this term can
easily be removed  and that doing so gives even lower-variance gradient estimates in many circum-
stances. In particular  as the variational posterior approaches the true posterior  this gradient estimator
approaches zero variance faster  making stochastic gradient-based optimization converge and "stick"
to the true variational parameters  as seen in ﬁgure 1.

Figure 1: Fitting a 100-dimensional varia-
tional posterior to another Gaussian  using
standard gradient versus our proposed path
derivative gradient estimator.

Iterations

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

40060080010001200Optimizationusing:PathDerivativeTotalDerivative1.1 Contributions

that has zero variance when the variational approximation is exact.

• We present a novel unbiased estimator for the variational evidence lower bound (ELBO)
• We provide a simple and general implementation of this trick in terms of a single change to
• We generalize our gradient estimator to mixture and importance-weighted lower bounds 
and discuss extensions to ﬂow-based approximate posteriors. This change takes a single
function call using automatic differentiation packages.

the computation graph operated on by standard automatic differentiation packages.

• We demonstrate the efﬁcacy of this trick through experimental results on MNIST and

Omniglot datasets using variational and importance-weighted autoencoders.

1.2 Background

Making predictions or computing expectations using latent variable models requires approximating
the posterior distribution p(z|x). Calculating these quantities in turn amounts to using Bayes’ rule:
p(z|x) = p(x|z)p(z)/p(x).
Variational inference approximates p(z|x) with a tractable distribution qφ(z|x) parameterized by φ
that is close in KL-divergence to the exact posterior. Minimizing the KL-divergence is equivalent to
maximizing the evidence lower bound (ELBO):

L(φ) = Ez∼q[log p(x  z) − log qφ(z| x)]

(ELBO)
An unbiased approximation of the gradient of the ELBO allows stochastic gradient descent to scalably
learn parametric models. Stochastic gradients of the ELBO can be formed from the REINFORCE-
style gradient  which applies to any continuous or discrete model  or a reparameterized gradient 
which requires the latent variables to be modeled as continuous. Our variance reduction trick applies
to the reparameterized gradient of the evidence lower bound.

2 Estimators of the variational lower bound

L(φ) = Ez∼q[log p(x|z) + log p(z) − log qφ(z|x)]

= Ez∼q[log p(x|z) + log p(z))] + H[qφ]
= Ez∼q[log p(x|z)] − KL(qφ(z|x)||p(z))

In this section  we analyze the gradient of the ELBO with respect to the variational parameters to
show a source of variance that depends on the complexity of the approximate distribution.
When the joint distribution p(x  z) can be evaluated by p(x|z) and p(z) separately  the ELBO can be
written in the following three equivalent forms:
(1)
(2)
(3)
Which ELBO estimator is best? When p(z) and qφ(z|x) are multivariate Gaussians  using equation
(3) is appealing because it analytically integrates out terms that would otherwise have to be estimated
by Monte Carlo. Intuitively  we might expect that using exact integrals wherever possible will give
lower-variance estimators by reducing the number of terms to be estimated by Monte Carlo methods.
Surprisingly  even when analytic forms of the entropy or KL divergence are available  sometimes it is
better to use (1) because it will have lower variance. Speciﬁcally  this occurs when qφ(z|x) = p(z|x) 
i.e. the variational approximation is exact. Then  the variance of the full Monte Carlo estimator ˆLM C
is exactly zero. Its value is a constant  independent of z iid
∼ qφ(z|x). This follows from the assumption
qφ(z|x) = p(z|x):

ˆLM C(φ) = log p(x  z) − log qφ(z|x) = log p(z|x) + log p(x) − log p(z|x) = log p(x) 

(4)
This suggests that using equation (1) should be preferred when we believe that qφ(z|x) ≈ p(z|x).
Another reason to prefer the ELBO estimator given by equation (1) is that it is the most generally
applicable  requiring a closed form only for qφ(z|x). This makes it suitable for highly ﬂexible
approximate distributions such as normalizing ﬂows [Jimenez Rezende and Mohamed  2015]  Real
NVP [Dinh et al.  2016]  or Inverse Autoregressive Flows [Kingma et al.  2016].

2

Estimators of the lower bound gradient What about estimating the gradient of the evidence
lower bound? Perhaps surprisingly  the variance of the gradient of the fully Monte Carlo estimator (1)
with respect to the variational parameters is not zero  even when the variational parameters exactly
capture the true posterior  i.e.  qφ(z|x) = p(z|x).
This phenomenon can be understood by decomposing the gradient of the evidence lower bound.
Using the reparameterization trick  we can express a sample z from a parametric distribution qφ(z)
as a deterministic function of a random variable  with some ﬁxed distribution and the parameters φ
of qφ  i.e.  z = t(  φ). For example  if qφ is a diagonal Gaussian  then for  ∼ N (0  I)  z = µ + σ
is a sample from qφ.
Under such a parameterization of z  we can decompose the total derivative (TD) of the integrand of
estimator (1) w.r.t. the trainable parameters φ as

ˆ∇TD(  φ) = ∇φ [log p(x|z) + log p(z) − log qφ(z|x)]
(cid:125)
= ∇φ [log p(z|x) + log p(x) − log qφ(z|x)]
= ∇z [log p(z|x) − log qφ(z|x)]∇φt(  φ)

(cid:123)(cid:122)

(cid:124)

(cid:124)

path derivative

(cid:123)(cid:122)

(cid:125)

 
−∇φ log qφ(z|x)

score function

(5)
(6)

(7)

The reparameterized gradient estimator w.r.t. φ
decomposes into two parts. We call these the
path derivative and score function components.
The path derivative measures dependence on
φ only through the sample z. The score func-
tion measures the dependence on log qφ directly 
without considering how the sample z changes
as a function of φ.
When qφ(z|x) = p(z|x) for all z  the path
derivative component of equation (7) is iden-
tically zero for all z. However  the score func-
tion component is not necessarily zero for any
z in some ﬁnite sample  meaning that the total
derivative gradient estimator (7) will have non-
zero variance even when q matches the exact
posterior everywhere.
This variance is induced by the Monte Carlo
sampling procedure itself. Figure 3 depicts
this phenomenon through the loss surface of
log p(x  z)− log qφ(z|x) for a Mixture of Gaus-
sians approximate and true posterior.

Figure 2: The evidence lower bound is a function of the
sampled latent variables z and the variational parameters
φ. As the variational distribution approaches the true
posterior  the gradient with respect to the sampled z
(blue) vanishes.

Path derivative of the ELBO Could we remove the high-variance score function term from the
gradient estimate? For stochastic gradient descent to converge  we require that our gradient estimate
is unbiased. By construction  the gradient estimate given by equation (7) is unbiased. Fortunately  the
problematic score function term has expectation zero. If we simply remove that term  we maintain an
unbiased estimator of the true gradient:

ˆ∇PD(  φ) = ∇z [log p(z|x) − log qφ(z|x)]∇φt(  φ) − (((((((
∇φ log qφ(z|x).

(8)
This estimator  which we call the path derivative gradient estimator due to its dependence on the
gradient ﬂow only through the path variables z to update φ  is equivalent to the standard gradient
estimate with the score function term removed. The path derivative estimator has the desirable
property that as qφ(z|x) approaches p(z|x)  the variance of this estimator goes to zero.
When to prefer the path derivative estimator Does eliminating the score function term from the
gradient yield lower variance in all cases? It might seem that its removal can only have a variance
reduction effect on the gradient estimator. Interestingly  the variance of the path derivative gradient
estimator may actually be higher in some cases. This will be true when the score function is positively
correlated with the remaining terms in the total derivative estimator. In this case  the score function
acts as a control variate: a zero-expectation term added to an estimator in order to reduce variance.

3

VariationalParameters(φ)qφ(z|x)=p(z|x)LatentVariable(z)logp(x z)−logqφ(z|x)logp(x z)−logqφ(z|x)SurfaceAlongTrajectorythroughTrueφELBOAlg. 2 Path Derivative ELBO Gradient
Input: Variational parameters φt  Data x
t ∼ p()
def ˆLt(φ):

← stop_gradient(φ)

zt ← sample_q(φ  t)
return log p(x  zt) - log q(zt|x  φ)

zt ← sample_q(φ  t)
φ(cid:48)
return log p(x  zt) - log q(zt|x  φ(cid:48))
return ∇φ ˆLt(φt)
Control variates are usually scaled by an adaptive constant c∗  which modiﬁes the magnitude and
preceding discussion  we have shown that (cid:98)c∗ = 1 is optimal when the variational approximation is
direction of the control variate to optimally reduce variance  as in Ranganath et al. [2014]. In the

return ∇φ ˆLt(φt)

Alg. 1 Standard ELBO Gradient
Input: Variational parameters φt  Data x
t ∼ p()
def ˆLt(φ):

exact  since that choice yields analytically zero variance. When the variational approximation is not
exact  an estimate of c∗ based on the current minibatch will change sign and magnitude depending on
the positive or negative correlation of the score function with the path derivative.
Optimal scale estimation procedures is particularly important when the variance of an estimator is
so large that convergence is unlikely. However  in the present case of reparameterized gradients 
where the variance is already low  estimating a scaling constant introduces another source of variance.
Indeed  we can only recover the true optimal scale when the variational approximation is exact in the
regime of inﬁnite samples during Monte Carlo integration.
Moreover  the score function must be independently estimated in order to scale it. Estimating the
gradient of the score function independent of automatic reverse-mode differentiation can be a chal-
lenging engineering task for many ﬂexible approximate posterior distributions such as Normalizing
Flows [Jimenez Rezende and Mohamed  2015]  Real NVP [Dinh et al.  2016]  or IAF [Kingma et al. 
2016].
By contrast  in section 6 we show improved performance on the MNIST and Omniglot density
estimation benchmarks by approximating the optimal scale with 1 throughout optimization. This
technique is easy to implement using existing automatic differentiation software packages. However 
if estimating the score function independently is computationally feasible  and a practitioner has
evidence that the variance induced by Monte Carlo integration will reduce the overall variance away
from the optimum point  we recommend establishing an annealling schedule for the optimal scaling
constant that converges to 1.

3

Implementation Details

In this section  we introduce algorithms 1 and 2 in relation to reverse-mode automatic differentiation 
and discuss how to implement the new gradient estimator in Theano  Autograd  Torch or Tensorﬂow
Bergstra et al. [2010]  Maclaurin et al. [2015]  Collobert et al. [2002]  Abadi et al. [2015].
Algorithm 1 shows the standard reparameterized gradient for the ELBO. We require three function
deﬁnitions: q_sample to generate a reparameterized sample from the variational approximation 
and functions that implement log p(x  z) and log q(z|x  φ). Once the loss ˆLt is deﬁned  we can
leverage automatic differentiation to return the standard gradient evaluated at φt. This yields equation
(7).
Algorithm 2 shows the path derivative gradient for the ELBO. The only difference from al-
gorithm 1 is the application of the stop_gradient function to the variational parameters
inside ˆLt. Table 1 indicates the names of stop_gradient in popular software packages.

Theano:
T.gradient.disconnected_grad
Autograd:
autograd.core.getval
TensorFlow:
tf.stop_gradient
Torch:
torch-autograd.util.get_value
Table 1: Functions that implement stop_gradient

4

i=1  Data x
t}K

t}K

Alg. 3 Path Derivative Mixture ELBO Gradient
Input: Params πt = {πj
j=1  φt = {φi
t ∼ p()
φ(cid:48)
t  π(cid:48)
t ← stop_gradient(φt  πt)
def ˆLc
t(φ):
zc
t ← sample_q(φ  t)
return log p(x  zc
ˆLc
t (φc
c=1 πc
t

t ) - log(cid:80)K
t )(cid:1)

(cid:0)(cid:80)K

c=1 π(cid:48)c

t q(zc

t|x  φ(cid:48)

t)

Alg. 4 IWAE ELBO Gradient
Input: Params φt  Data x

1  2  . . .   K ∼ p()
φ(cid:48)
t ← stop_gradient(φt)
def wi(φ  i):
zi ← sample_q(φ  i)
return p(x zi)
q(zi|x φ(cid:48)
t)

i=1 wi(φ  i)(cid:1)
(cid:80)K

return ∇φ log(cid:0) 1

k

return ∇φ π
This simple modiﬁcation to algorithm 1 generates a copy of the parameter variable that is treated as a
constant with respect to the computation graph generated for automatic differentiation. The copied
variational parameters are used to evaluate variational the density log qφ at z.
Recall that the variational parameters φ are used both to generate z through some deterministic
function of an independent random variable   and to evaluate the density of z through log qφ. By
blocking the gradient through variational parameters in the density function  we eliminate the score
function term that appears in equation (7). Per-iteration updates to the variational parameters φ rely
on the z channel only  e.g.  the path derivative component of the gradient of the loss function ˆLt.
This yields the gradient estimator corresponding to equation (8).

4 Extensions to Richer Variational Families

Mixture Distributions
In this section  we dis-
cuss extensions of the path derivative gradient
estimator to richer variational approximations to
the true posterior.
Using a mixture distribution as an approximate
posterior in an otherwise differentiable estima-
tor introduces a problematic  non-differentiable
random variable π ∼ Cat(α). We solve this by
integrating out the discrete mixture choice from
both the ELBO and the mixture distribution. In
this section  we show that such a gradient es-
timator is unbiased  and introduce an extended
algorithm to handle mixture variational families.
For any mixture of K base distributions qφ(z|x) 
a mixture variational family can be deﬁned by
c=1 πc qφc(z|x)  where φM =
{π1  ...  πk  φ1  ...  φk} are variational parame-
ters  e.g.  the weights and distributional param-
eters for each component. Then  the mixture
ELBO LM is given by:
K(cid:88)

qφM (z|x) =(cid:80)K

(cid:20)

x
i
r
t
a

M

e
c
n
a
i
r
a
v
o
C

f
o
m
r
o
N
e
c
a
r
T

Variational Parameters φinit → φtrue

Figure 3: Fitting a mixture of 5 Gaussians as a varia-
tional approximation to a posterior that is also a mixture
of 5 Gaussians. Path derivative and score function gra-
dient components were measured 1000 times. The path
derivative goes to 0 as the variational approximation
becomes exact  along an arbitrarily chosen path

πcEzc∼qφc

c=1

log p(x  zc) − log

(cid:18) K(cid:88)

k=1

(cid:19)(cid:21)

πkqφk (zc|x)

 

where the outer sum integrates over the choice of mixture component for each sample from qφM  
and the inner sum evaluates the density. Applying the new gradient estimator to the mixture ELBO
involves applying it to each qφk (zc|x) in the inner marginalization.
Algorithm 3 implements the gradient estimator of (8) in the context of a continuous mixture distribu-
tion. Like algorithm 2  the new gradient estimator of 3 differs from the vanilla gradient estimator only
in the application of stop_gradient to the variational parameters. This eliminates the gradient
of the score function from the gradient of any mixture distribution.

5

0.0e+002.0e+054.0e+056.0e+058.0e+05TotalDerivativeEstimatorPathDerivativeEstimatorTruePosteriorVariationalApproximationImportance-Weighted Autoencoder We also explore the effect of our new gradient estimator on
the IWAE bound Burda et al. [2015]  deﬁned as

ˆLK = Ez1 ... zK∼q(z|x)

with gradient

(9)

(10)

(11)

(cid:20)

log

K(cid:88)

i=1

(cid:18) 1
(cid:20) K(cid:88)

k

i=1

p(x  zi)
q(zi|x)

˜wi∇φ log wi

(cid:19)(cid:21)

(cid:21)

(cid:21)

∇φ ˆLK = Ez1 ... zK∼q(z|x)

where wi := p(x  zi)/q(zi|x) and ˜wi := wi/(cid:80)k

i=1 wi. Since ∇φ log wi is the same gradient as
the Monte Carlo estimator of the ELBO (equation (7))  we can again apply our trick to get a new
estimator.
However  it is not obvious whether this new gradient estimator is unbiased. In the unmodiﬁed IWAE
bound  when q = p  the gradient with respect to the variational parameters reduces to:

(cid:20)

k(cid:88)

i=1

Ez1 ... zk∼q(z|x)

−

˜wi∇φ log qφ(zi|x)

.

Each sample zi is used to evaluate both ˜wi and the partial derivative term. Hence  we cannot
simply appeal to the linearity of expectation to show that this gradient is 0. Nevertheless  a natural
extension of the variance reduction technique in equation (8) is to apply our variance reduction to
each importance-weighted gradient sample. See algorithm 4 for how to implement the path derivative
estimator in this form.
We present empirical validation of the idea in our experimental results section  which shows markedly
improved results using our gradient estimator. We observe a strong improvement in many cases 
supporting our conjecture that the gradient estimator is unbiased as in the mixture and multi-sample
ELBO cases.

Flow Distributions Flow-based approximate posteriors such as Kingma et al. [2016]  Dinh et al.
[2016]  Jimenez Rezende and Mohamed [2015] are a powerful and ﬂexible framework for ﬁtting
approximate posterior distributions in variational inference. Flow-based variational inference samples
an initial z0 from a simple base distribution with known density  then learns a chain of invertible 
k=1 log(cid:12)(cid:12) det ∂fk
(cid:80)K
parameterized maps fk(zk−1) that warp z0 into zK = fK ◦ fK−1 ◦ ... ◦ f1(z0). The endpoint
zK represents a sample from a more ﬂexible distribution with density log qK(zK) = log q0(z0) −

(cid:12)(cid:12).

∂zk−1

We expect our gradient estimator to improve the performance of ﬂow-based stochastic variational
inference. However  due to the chain composition used to learn zK  we cannot straightforwardly apply
our trick as described in algorithm 2. This is because each intermediate zj  1 ≤ j ≤ K contributes
to the path derivative component in equation (8). The log-Jacobian terms used in the evaluation of
log q(zk)  however  require this gradient information to calculate the correct estimator. By applying
stop_gradient to the variational parameters used to generate each intermediate zi and passing
only the endpoint zK to a log density function  we would lose necessary gradient information at
each intermediate step needed for the gradient estimator to be correct. At time of writing  the
requisite software engineering to track and expose intermediate steps during backpropagation is not
implemented in the packages listed in Table 1  and so we leave this to future work.

5 Related Work

Our modiﬁcation of the standard reparameterized gradient estimate can be interpreted as adding a
control variate  and in fact Ranganath et al. [2014] investigated the use of the score function as a
control variate in the context of non-reparameterized variational inference. The variance-reduction
effect we use to motivate our general gradient estimator has been noted in the special cases of
Gaussian distributions with sparse precision matrices and Gaussian copula inference in Tan and
Nott [2017] and Han et al. [2016] respectively. In particular  Tan and Nott [2017] observes that by

6

MNIST

Omniglot

VAE

IWAE

VAE

IWAE

stochastic layers
1

2

k
1
5
50
1
5
50

Total
86.76
86.47
86.35
85.33
85.01
84.78

Path
86.40
86.33
86.48
84.77
84.68
84.33

Total
86.76
85.54
84.78
85.33
83.89
82.90

Path
86.40
85.20
84.45
84.77
83.57
83.16

Total
108.11
107.62
107.80
107.58
106.31
106.30

Path
107.39
107.40
107.42
105.22
104.87
105.70

Total
108.11
106.12
104.67
107.56
104.79
103.38

Path
107.39
105.42
104.16
105.22
103.59
102.86

Table 2: Results on variational (VAE) and importance-weighted (IWAE) autoencoders using the total
derivative estimator  equation (7)  versus the path derivative estimator  equation (8) (ours).

eliminating certain terms from a gradient estimator for Gaussian families parameterized by sparse
precision matrices  multiple lower-variance unbiased gradient estimators may be derived.
Our work is a generalization to any continuous variational family. This provides a framework for
easily implementing the technique in existing software packages that provide automatic differentiation.
By expressing the general technique in terms of automatic differentiation  we eliminate the need
for case-by-case analysis of the gradient of the variational lower bound as in Tan and Nott [2017]
and Han et al. [2016].
An innovation by Ruiz et al. [2016] introduces the generalized reparameterization gradient (GRG)
which uniﬁes the REINFORCE-style and reparameterization gradients. GRG employs a weaker
form of reparameterization that requires only the ﬁrst moment to have no dependence on the latent
variables  as opposed to complete independence as in Kingma and Welling [2013]. GRG improves on
the variance of the score-function gradient estimator in BBVI without the use of Rao-Blackwellization
as in Ranganath et al. [2014]. A term in their estimator also behaves like a control variate.
The present study  in contrast  develops a simple drop-in variance reduction technique through an
analysis of the functional form of the reparameterized evidence lower bound gradient. Our technique
is developed outside of the framework of GRG but can strongly improve the performance of existing
algorithms  as demonstrated in section 6. Our technique can be applied alongside GRG.
In the python toolkit Edward [Tran et al.  2016]  efforts are ongoing to develop algorithms that
implement stochastic variational inference in general as a black-box method. In cases where an
analytic form of the entropy or KL-divergence is known  the score function term can be avoided
using Edward. This is equivalent to using equations (2) or (3) respectively to estimate the ELBO. As
of release 1.2.4 of Edward  the total derivative gradient estimator corresponding to (7) is used for
reparameterized stochastic variational inference.

6 Experiments

Experimental Setup Because we follow the experimental setup of Burda et al. [2015]  we review
it brieﬂy here. Both benchmark datasets are composed of 28 × 28 binarized images. The MNIST
dataset was split into 60  000 training and 10  000 test examples. The Omniglot dataset was split
into 24  345 training and 8070 test examples. Each model used Xavier initialization [Glorot and
Bengio  2010] and trained using Adam with parameters β1 = 0.9  β2 = 0.999  and  = 1e−4 with
20 observations per minibatch [Kingma and Ba  2015]. We compared against both architectures
reported in Burda et al. [2015]. The ﬁrst has one stochastic layer with 50 hidden units  encoded using
two fully-connected layers of 200 neurons each  using a tanh nonlinearity throughout. The second
architecture is two stochastic layers: the ﬁrst stochastic layer encodes the observations  with two
fully-connected layers of 200 hidden units each  into 100 dimensional outputs. The output is used as
the parameters of diagonal Gaussian. The second layer takes samples from this Gaussian and passes
them through two fully-connected layers of 100 hidden units each into 50 dimensions.
See table 2 for NLL scores estimated as the mean of equation (9) with k=5000 on the test set. We can
see that the path derivative gradient estimator improves over the original gradient estimator in all but
two cases.

7

Benchmark Datasets We evaluate our path derivative estimator using two benchmark datasets:
MNIST  a dataset of handwritten digits [LeCun et al.  1998]  and Omniglot  a dataset of handwritten
characters from many different alphabets [Lake  2014]. To underscore both the easy implementation
of this technique and the improvement it offers over existing approaches  we have empirically
evaluated our new gradient estimator by a simple modiﬁcation of existing code1 [Burda et al.  2015].

Omniglot Results For a two-stochastic-layer VAE using the multi-sample ELBO with gradient
corresponding to equation (8) improves over the results in Burda et al. [2015] by 2.36  1.44  and 0.6
nats for k={1  5  50} respectively. For a one-stochastic-layer VAE  the improvements are more modest:
0.72  0.22  and 0.38 nats lower for k={1  5  50} respectively. A VAE with a deep recognition network
appears to beneﬁt more from our path derivative estimator than one with a shallow recognition
network. For comparison  a VAE using the path derivative estimator with k=5 samples performs only
0.08 nats worse than an IWAE using the total derivative gradient estimator (7) and 5 samples. By
contrast  using the total derivative (vanilla) estimator for both models  IWAE otherwise outperforms
VAE for k=5 samples by 1.52 nats.
By increasing the accuracy of the ELBO gradient estimator  we may also increase the risk of
overﬁtting. Burda et al. [2015] report that they didn’t notice any signiﬁcant problems with overﬁtting 
as the training log likelihood was usually 2 nats lower than the test log likelihood. With our gradient
estimator  we observe only 0.77 nats worse performance for a VAE with k=50 compared to k=5 in
the two-layer experiments. IWAE using equation (8) markedly outperforms IWAE using equation
(7) on Omniglot. For a 2-layer IWAE  we observe an improvement of 2.34  1.2  and 0.52 nats
for k={1  5  50} respectively. For a 1-layer IWAE  the improvements are 0.72  0.7  and 0.51 for
k={1  5  50} respectively. Just as in the VAE Omniglot results  a deeper recognition network for an
IWAE model beneﬁts more from the improved gradient estimator than a shallow recognition network.

MNIST Results For all but one experiment  a VAE with our path derivative estimator outperforms a
vanilla VAE on MNIST data. For k=50 with one stochastic layer  our gradient estimator underperforms
a vanilla VAE by 0.13 nats. Interestingly  the training NLL for this run is 86.11  only 0.37 nats
different than the test NLL. The similar magnitude of the two numbers suggests that training for
longer than Burda et al. [2015] would improve the performance of our gradient estimator. We
hypothesize that the worse performance using the path derivative estimator is a consequence of
ﬁne-tuning towards the characteristics of the total derivative estimator.
For a two-stochastic-layer VAE on MNIST  the improvements are 0.56  0.33 and 0.45 for k={1  5  50}
respectively. In a one-stochastic-layer VAE on MNIST  the improvements are 0.36 and 0.14 for
k={1  5} respectively.
The improvements on IWAE are of a similar magnitude. For k=50 in a two-layer path-derivative
IWAE  we perform 0.26 nats worse than with a vanilla IWAE. The training loss for the k=50 run is
82.74  only 0.42 nats different. As in the other failure case  this suggests we have room to improve
these results by ﬁne-tuning over our method. For a two stochastic layer IWAE  the improvements are
0.66 and 0.22 for k=1 and 5 respectively. In a one stochastic layer IWAE  the improvements are 0.36 
0.34  and 0.33 for k={1  5  50} respectively.

7 Conclusions and Future Work

We demonstrated that even when the reparameterization trick is applicable  further reductions in
gradient variance are possible. We presented our variance reduction method in a general way by
expressing it as a modiﬁcation of the computation graph used for automatic differentiation. The
gain from using our method grows with the complexity of the approximate posterior  making it
complementary to the development of non-Gaussian posterior families.
Although the proposed method is speciﬁc to variational inference  we suspect that similar unbiased
but high-variance terms might exist in other stochastic optimization settings  such as in reinforcement
learning  or gradient-based Markov Chain Monte Carlo.

1See https://github.com/geoffroeder/iwae

8

References
Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S.
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow  Andrew
Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser  Manjunath
Kudlur  Josh Levenberg  Dan Mané  Rajat Monga  Sherry Moore  Derek Murray  Chris Olah  Mike
Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal Talwar  Paul Tucker  Vincent
Vanhoucke  Vijay Vasudevan  Fernanda Viégas  Oriol Vinyals  Pete Warden  Martin Wattenberg 
Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems  2015. URL http://tensorflow.org/. Software available from
tensorﬂow.org.

James Bergstra  Olivier Breuleux  Frédéric Bastien  Pascal Lamblin  Razvan Pascanu  Guillaume
Desjardins  Joseph Turian  David Warde-Farley  and Yoshua Bengio. Theano: A cpu and gpu math
compiler in python. In Proc. 9th Python in Science Conf  pages 1–7  2010.

Yuri Burda  Roger Grosse  and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv

preprint arXiv:1509.00519  2015.

Ronan Collobert  Samy Bengio  and Johnny Mariéthoz. Torch: a modular machine learning software

library. Technical report  Idiap  2002.

Laurent Dinh  Jascha Sohl-Dickstein  and Samy Bengio. Density estimation using real nvp. arXiv

preprint arXiv:1605.08803  2016.

Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In Aistats  volume 9  pages 249–256  2010.

Shaobo Han  Xuejun Liao  David B Dunson  and Lawrence Carin. Variational gaussian copula
inference. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence and
Statistics  volume 51  pages 829–838  2016.

Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In The

32nd International Conference on Machine Learning  2015.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of the

3rd international conference on learning representations  2015.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes.

arXiv:1312.6114  2013.

arXiv preprint

Diederik P. Kingma  Tim Salimans  and Max Welling. Improving variational inference with inverse

autoregressive ﬂow. Advances in Neural Information Processing Systems 29  2016.

Brenden M Lake. Towards more human-like concept learning in machines: Compositionality 

causality  and learning-to-learn. PhD thesis  Massachusetts Institute of Technology  2014.

Yann LeCun  Corinna Cortes  and Christopher JC Burges. The mnist dataset of handwritten digits.

URL http://yann. lecun. com/exdb/mnist  1998.

Dougal Maclaurin  David Duvenaud  Matthew Johnson  and Ryan P. Adams. Autograd: Reverse-
mode differentiation of native Python  2015. URL http://github.com/HIPS/autograd.

Rajesh Ranganath  Sean Gerrish  and David M Blei. Black box variational inference. In AISTATS 

pages 814–822  2014.

Danilo J Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In Proceedings of the 31st International Conference on
Machine Learning (ICML-14)  pages 1278–1286  2014.

Francisco JR Ruiz  Michalis K Titsias  and David M Blei. The generalized reparameterization

gradient. arXiv preprint arXiv:1610.02287  2016.

Linda SL Tan and David J Nott. Gaussian variational approximation with sparse precision matrices.

Statistics and Computing  pages 1–17  2017.

9

Dustin Tran  Alp Kucukelbir  Adji B. Dieng  Maja Rudolph  Dawen Liang  and David M.
Blei. Edward: A library for probabilistic modeling  inference  and criticism. arXiv preprint
arXiv:1610.09787  2016.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning  8(3-4):229–256  1992.

10

,Geoffrey Roeder
Yuhuai Wu
David Duvenaud