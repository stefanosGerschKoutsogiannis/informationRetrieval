2013,Matrix Completion From any Given Set of Observations,In the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries.  This problem comes up in many application areas  and has received a great deal of attention in the context of the netflix prize.  A central approach to this problem is to output a matrix of lowest  possible complexity (e.g. rank or trace norm) that agrees with the partially  specified matrix.  The performance of this approach under the assumption that the revealed entries are sampled randomly   has received considerable attention. In practice  often the set of revealed entries is not chosen at random and these results do not apply. We are therefore left with no guarantees on the performance of the algorithm we are using.  We present a means to obtain performance guarantees with respect to any set of initial observations.  The first  step remains the same: find a matrix of lowest possible complexity that agrees with the partially specified matrix.   We give a new way to interpret the output of this algorithm by next finding a probability distribution over  the non-revealed entries with respect to which a bound on the generalization error can be proven.  The more  complex the set of revealed entries according to a certain measure  the better the bound on the generalization  error.,Matrix Completion From any Given Set of

Observations

Nanyang Technological University and

Centre for Quantum Technologies

Troy Lee

troyjlee@gmail.com

Adi Shraibman

Department of Computer Science
Tel Aviv-Yaffo Academic College
adi.shribman@gmail.com

Abstract

In the matrix completion problem the aim is to recover an unknown real matrix
from a subset of its entries. This problem comes up in many application areas 
and has received a great deal of attention in the context of the netﬂix prize.
A central approach to this problem is to output a matrix of lowest possible
complexity (e.g.
rank or trace norm) that agrees with the partially speciﬁed
matrix. The performance of this approach under the assumption that the re-
vealed entries are sampled randomly has received considerable attention (e.g.
[1  2  3  4  5  6  7  8]). In practice  often the set of revealed entries is not chosen
at random and these results do not apply. We are therefore left with no guarantees
on the performance of the algorithm we are using.
We present a means to obtain performance guarantees with respect to any set of
initial observations. The ﬁrst step remains the same: ﬁnd a matrix of lowest possi-
ble complexity that agrees with the partially speciﬁed matrix. We give a new way
to interpret the output of this algorithm by next ﬁnding a probability distribution
over the non-revealed entries with respect to which a bound on the generalization
error can be proven. The more complex the set of revealed entries according to a
certain measure  the better the bound on the generalization error.

1

Introduction

In the matrix completion problem we observe a subset of the entries of a target matrix Y   and our aim
is to retrieve the rest of the matrix. Obviously some restriction on the target matrix Y is unavoidable
as otherwise it is impossible to retrieve even one missing entry; usually  it is assumed that Y is
generated in a way so as to have low complexity according to a measure such as matrix rank.
A common scheme for the matrix completion problem is to select a matrix X that minimizes some
combination of the complexity of X and the distance between X and Y on the observed part. In
particular  one can demand that X agrees with Y on the observed initial sample (i.e. the distance
between X and Y on the observed part is zero). This general algorithm is described in Figure 1  and
we refer to it as Alg1. It outputs a matrix with minimal complexity that agrees with Y on the initial
sample S. The complexity measure can be rank  or a norm to serve as an efﬁciently computable
proxy for the rank such as the trace norm or γ2 norm. When we wish to mention which complexity
measure is used we write it explicitly  e.g. Alg1(γ2). Our framework is suitable using any norm
satisfying few simple conditions described in the sequel.
The performance of Alg1 under the assumption that the initial subset is picked at random is well
understood [1  2  3  4  5  6  7  8]. This line of research can be divided into two parts. One line
of research [5  6  4] studies conditions under which Alg1(Tr) retrieves the matrix exactly 1. They

1There are other papers studying exact matrix completion  e.g. [7].

1

deﬁne what they call an incoherence property which quantiﬁes how spread the singular vectors of Y
are. The exact deﬁnition of the incoherence property varies in different results. It is then proved that
if there are enough samples relative to the rank of Y and its incoherence property  then Alg1(Tr)
retrieves the matrix Y exactly with high probability  assuming the samples are chosen uniformly at
random. Note that in this line of research the trace norm is used as the complexity measure in the
algorithm. It is not clear how to prove similar results with the γ2 norm.
Candes and Recht [5] observed that it is impossible to reconstruct a matrix that has only one entry
equal to 1 and zeros everywhere else  unless most of its entries are observed. Thus  exact matrix
completion must assume some special property of the target matrix Y . In a second line of research 
general results are proved regarding the performance of Alg1. These results are weaker in that they
do not prove exact recovery  but rather bounds on the distance between the output matrix X and
Y . But these results apply for every matrix Y   they can be generalized for non-uniform probability
distributions  and also apply when the complexity measure is the γ2 norm. These results take the
following form:
Theorem 1 ([2]) Let Y be an n × n real matrix  and P a probability distribution on pairs (i  j) ∈
[n]2. Choose a sample S of |S| > n log n entries according to P . Then  with probability at least
1 − 2−n/2 over the sample selection  the following holds:

Pij|Xij − Yij| ≤ cγ2(X)

(cid:114) n

|S| .

(cid:88)

i j

Where X is the output of the algorithm with sample S  and c is a universal constant.

In practice  the assumption that the sample is random is not always valid. Sometimes the subset we
see reﬂects our partial knowledge which is not random at all. What can we say about the output
of the algorithm in this case? The analysis of random samples does not help us here  because these
proofs do not reveal the structure that makes generalization possible. In order to answer this question
we need to understand what properties of a sample enable generalization.
A ﬁrst step in this direction was taken in [9] where the initial subset was chosen deterministically
as the set of edges of a good expander (more generally  a good sparsiﬁer). Deterministic guarantees
were proved for the algorithm in this case  that resemble the guarantees proved for random sampling.
For example:

Theorem 2 [9] Let S be the set of edges of a d-regular graph with second eigenvalue 2 bound λ.
For every n × n real matrix Y   if X is the output of Alg1 with initial subset S  then

(cid:88)

1
n2

(Xij − Yij)2 ≤ cγ2(Y )2 λ
d

 

i j
where c is a small universal constant.

√

d) can be constructed in linear time using e.g.

Recall that d-regular graphs with λ = O(
well-known LPS Ramanujan graphs [10].
This theorem was also generalized to bound the error with respect to any probability distribution.
Instead of expanders  sparsiﬁers were used to select the entries to observe for this result.
Theorem 3 [9] Let P be a probability distribution on pairs (i  j) ∈ [n]2  and d > 1. There is an
efﬁciently constructible set S ⊂ [n]2 of size at most dn  such that for every n × n real target matrix
Y   if X is the output of our algorithm with initial subset S  then

the

(cid:88)

i j

Pij(Xij − Yij)2 ≤ cγ2(Y )2 1√
d

.

The results in [9] still do not answer the practical question of how to reconstruct a matrix from an
arbitrary sample. In this paper we continue the work started in [9]  and give a simple and general
answer to this second question.
We extend the results of [9] in several ways:

2The eigenvalues are eigenvalues of the adjacency matrix of the graph.

2

1. We upper bound the generalization error of Alg1 given any set of initial observations. This

bound depends on properties of the set of observed entries.

2. We show there is a probability distribution outside of the observed entries such that the
generalization error under this distribution is bounded in terms of the complexity of the
observed entries  under a certain complexity measure.

3. The results hold not only for γ2 but also for the trace norm  and in fact any norm satisfying

a few basic properties.

2 Preliminaries

Here we introduce some of the matrix notation and norms that we will be using. For matrices A  B
of the same size  let A ◦ B denote the Hadamard or entrywise product of A and B. For a m-by-n
matrix A with m ≥ n let σ1(A) ≥ ··· ≥ σn(A) denote the singular values of A. The trace norm 
denoted (cid:107)A(cid:107)tr  is the (cid:96)1 norm of the vector of singular values  and the Frobenius norm  denoted
(cid:107)A(cid:107)F   is the (cid:96)2 norm of the vector of singular values.
As the rank of a matrix is equal to the number of non-zero singular values  it follows from the
Cauchy-Schwarz inequality that

(cid:107)A(cid:107)2
(cid:107)A(cid:107)2

tr

F

≤ rk(A) .

(1)

This inequality motivates the use of the trace norm as a proxy for rank in rank minimization prob-
lems. A problem with the bound of (1) as a complexity measure is that it is not monotone—the
bound can be larger on a submatrix of A than on A itself. As taking the Hadamard product of a
matrix with a rank one matrix does not increase its rank  a way to ﬁx this problem is to consider
instead:

(cid:107)A ◦ vuT(cid:107)2
(cid:107)A ◦ vuT(cid:107)2

tr

F

max
u v

(cid:107)u(cid:107)=(cid:107)v(cid:107)=1

≤ rk(A) .

(cid:107)A ◦ vuT(cid:107)2

tr ≤ rk(A) .

max
u v

(cid:107)u(cid:107)=(cid:107)v(cid:107)=1

When A is a sign matrix  this bound simpliﬁes nicely—for then  (cid:107)A ◦ vuT(cid:107)F = (cid:107)u(cid:107)(cid:107)v(cid:107) = 1  and
we are left with

This motivates the deﬁnition of the γ2 norm.
Deﬁnition 4 Let A be a n-by-n matrix. Then

γ2(A) = max
u v

(cid:107)u(cid:107)=(cid:107)v(cid:107)=1

(cid:107)A ◦ vuT(cid:107)tr .

We will also make use of the dual norms of the trace and γ2 norms. Recall that in general for a norm
Φ(A) the dual norm Φ∗ is deﬁned as

Φ∗(A) = max

B

(cid:104)A  B(cid:105)
Φ(B)

Notice that this means that

(cid:104)A  B(cid:105) ≤ Φ∗(A)Φ(B) .

(2)
The dual of the trace norm is (cid:107) · (cid:107) the operator norm from (cid:96)2 to (cid:96)2  also known as the spectral norm.
The dual of the γ2 norm looks as follows.
Deﬁnition 5

(cid:1)

(cid:0)(cid:107)X(cid:107)2

F + (cid:107)Y (cid:107)2

1
2
(cid:107)X(cid:107)F(cid:107)Y (cid:107)F  

F

γ∗
2 (A) = min
X Y

X T Y =A

= min
X Y

X T Y =A

where the min is taken over X  Y with orthogonal columns.

3

Finally  we will make use of the approximate γ2 norm. This is the minimum of the γ2 norm over all
matrices which approximate the target matrix in some sense. The particular version we will need is
denoted γ0 ∞
Deﬁnition 6 Let S ∈ {0  1}m×n be a boolean matrix. Let ¯S denote the complement of S  that is
¯S = J − S where J is the all ones matrix. Then

and is deﬁned as follows.

2

γ0 ∞

2

(S) = min

T

{γ2(T ) : T ◦ S ≥ S  T ◦ ¯S = 0}

2

In words  γ0 ∞
(S) is the minimum γ2 norm of a matrix T which is 0 whenever S is zero  and at
least 1 whenever S is 1. This can be thought of as a “one-sided error” version of the more familiar
γ∞
2 norm of a sign matrix  which is the minimum γ2 norm of a matrix which agrees in sign with the
target matrix and has all entries of magnitude at least 1. The γ∞
2 bound is also known to be equal to
the margin complexity [11].

3 The algorithm
Let S ⊂ [m] × [n] be a subset of entries  representing our partial knowledge. We can always run
Alg1 and get an output matrix X. What we need in order to make intelligent use of X is a way to
measure the distance between X and Y . Our ﬁrst observation is that although Y is not known  it
is possible to bound the distance between X and Y . This result is stated in the following theorem
which generalizes Theorems (2) and (4) of [9] 3:
Theorem 7 Fix a set of entries S ⊂ [m] × [n]. Let P be a probability distribution on pairs (i  j) ∈
[m] × [n]  such that there exists a real matrix Q satisfying

1. Qij = 0 when (i  j) (cid:54)∈ S.
2. γ∗

2 (P − Q) ≤ Λ

Then for every m × n real target matrix Y   if X is the output of our algorithm with initial subset S 
it holds that

(cid:88)

Pij(Xij − Yij)2 ≤ 4Λγ2(Y )2 .

i j

2 (P − Q) determines  at least to some extent  the expected distance between

Theorem 7 says that γ∗
X and Y with respect to P .
This gives us a way to measure the quality of the output of Alg1 for any set S of initial observations.
Namely  we can do the following:

1. Choose a probability distribution P on the entries of the matrix.
2. Find a real matrix Q such that Qij = 0 when (i  j) (cid:54)∈ S  and γ∗
3. Output the minimal value Λ.

2 (P − Q) is minimal.

We then know  using Theorem 7  that the expected square distance between X and Y can be bounded
in terms of Λ and the complexity of Y .
Obviously  the choice of P makes a big difference. For example if the set of initial observations is
contained in a submatrix we cannot expect X to be close to Y outside this submatrix. In such cases
it makes sense to restrict P to the submatrix containing S.
One approach to ﬁnd a distribution for which we can expect to be close on the unseen entries is to
optimize over probability distributions P such that Theorem 7 gives the best bound. Since γ∗
2 can
be expressed as the optimum of semideﬁnite program  we can ﬁnd in polynomial time a probability
2 (P − Q) is minimizd. Thus  instead of
distribution P and a weight function Q on S such that γ∗
trying different parameters  we can ﬁnd a probability distribution for which we can prove optimal

3Here we state the result for γ2. See Section 4 for the corresponding result for the trace norm as well.

4

1.
2.

Input: a subset S ⊂ [n]2 and the value of Y on S.
Output: a matrix X of smallest possible CC(X) under the condition that
Xij = Yij for all (i  j) ∈ S.

Figure 1: Algorithm Alg1(CC)

guarantees using Theorem 7. The second algorithm we suggest does exactly that. We refer to this
algorithm as Alg2  or Alg2(CC) if we wish to state the complexity measure that is used.
For Alg2(γ2)  we do the following: Minimize γ∗
that:

2 (P − Q) over all m × n matrices Q and P such

1. Qij = 0 for (i  j) (cid:54)∈ S.
2. Pij = 0 for (i  j) ∈ S.

3. (cid:80)

i j Pij = 1.

Globally  our algorithm for matrix completion therefore works in two phases. We ﬁrst use Alg1 to
get an output matrix X  and then use Alg2 in order to ﬁnd optimal guarantees regarding the distance
between X and Y . The generalization error bounds for this algorithm are proved in Section 4.

3.1 Using a general norm

In our description of Alg2 above we have used the norm γ2. The same idea works for any norm Φ
satisfying the property Φ(A ◦ A) ≤ Φ(A)2. Moreover  if the dual norm can be computed efﬁciently
via a linear or semideﬁnite program  then the optimal distribution P for the bound can be found
efﬁciently as well.
For example for the trace norm the algorithm becomes: Given the sample S run Alg1((cid:107) · (cid:107)tr) and
get an output matrix X. The second part of the algorithm is: Minimize (cid:107)P − Q(cid:107) over all m × n
matrices Q and P such that:

1. Qij = 0 for (i  j) (cid:54)∈ S.
2. Pij = 0 for (i  j) ∈ S.

3. (cid:80)

i j Pij = 1.

Denote by Λ the optimal value of the above program  and by P the optimal probability distribution.

Then analogously to Theorem 7  we have(cid:88)

Pij(Xij − Yij)2 ≤ 4Λ(cid:107)Y (cid:107)2

tr .

i j

Both of these results will follow from a more general theorem which we show in the next section.

4 Generalization bounds

Here we show a more general theorem which will imply Theorem 7.
Theorem 8 Let Φ be a norm and Φ∗ its dual norm. Suppose that Φ(A◦ A) ≤ Φ(A)2 for any matrix
A.
Fix a set of indices S ⊂ [m] × [n]. Let P be a probability distribution on pairs (i  j) ∈ [m] × [n] 
such that there exists a real matrix Q satisfying

1. Qij = 0 when (i  j) (cid:54)∈ S.
2. Φ∗(P − Q) ≤ Λ

5

Then for every m × n real target matrix Y   if X is the output of algorithm Alg1(Φ) with initial
subset S  it holds that

(cid:88)

Pij(Xij − Yij)2 ≤ 4Φ(Y )2Λ.

i j

Proof Let R be the matrix where Rij = (Xij − Yij)2. By assumption Φ∗(P − Q) ≤ Λ thus by (2)

(cid:104)P − Q  R(cid:105) ≤ ΛΦ(R) .

Now let us focus on Φ(R). As R = (X − Y ) ◦ (X − Y ) by the assumption on Φ we have

Φ(R) ≤ Φ(X − Y )2 ≤ (Φ(X) + Φ(Y ))2 .

Now by deﬁnition of Alg1(Φ) we have Φ(X) ≤ Φ(Y )  thus Φ(R) ≤ 4Φ(Y )2. Also  by deﬁnition
of the algorithm Rij = 0 for (i  j) ∈ S  and Qij equals zero outside of S  which implies that

(cid:80)
i j QijRij = 0. We conclude that(cid:88)

Pij(Xij − Yij)2 ≤ 4ΛΦ(Y )2.

i j

Both the trace norm and γ2 norm satisfy the condition of the theorem as they are multiplicative under
tensor product.

5 Analyzing the error bound

We now look more closely at the minimal value of the parameter Λ from Theorem 7. The optimal
value of Λ depends only on the set of observed indices S. For a set of indices S ⊂ [m] × [n] let ¯S
be its complement.
Given samples S we want to ﬁnd P  Q so as to minimize γ∗
distribution over ¯S and Q has support in S. We can express this as a semideﬁnite program

2 (P − Q) such that P is a probability

Λ =minimize

α P Q

1
2

Tr(α)

subject to α − ( ˆP − ˆQ) (cid:23) 0

P ≥ 0
(cid:104)P  ¯S(cid:105) = 1
(cid:104)Q  S(cid:105) = Q.

(cid:20) 0

ˆP =

(cid:21)

P
0

Here

P T
is the “bipartite” version of P   and similarly for ˆQ.
Taking the dual of this program we ﬁnd

1/Λ =minimize

γ2(A)
subject to A ≥ ¯S

A

A ◦ ¯S = A

In words  this says that that 1
in S and at least 1 on all entries in ¯S. Thus Λ = 1/γ0 ∞
more complex the set of unobserved entries ¯S according to the measure γ0 ∞
of Λ. Note that in particular  if we consider the sign matrix ¯S−S then γ0 ∞
is lower bounded by the margin complexity of S − ¯S.

Λ is equal to the minimum γ2 norm of a matrix that is zero on all entries
( ¯S) (recall Deﬁnition 6). This says that the
  the smaller the value
2 ( ¯S−S)−1)/2

( ¯S) ≥ (γ∞

2

2

2

6

References
[1] N. Srebro  J. D. M. Rennie  and T. S. Jaakola. Maximum-margin matrix factorization.

Neural Information Processing Systems  2005.

In

[2] N. Srebro and A. Shraibman. Rank  trace-norm and max-norm. In 18th Annual Conference on

Computational Learning Theory (COLT)  pages 545–560  2005.

[3] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction.

Technical report  arXiv  2011.

[4] E. J. Candes and T. Tao. The power of convex relaxation: near-optimal matrix completion.

IEEE Transactions on Information Theory  56(5):2053–2080  2010.

[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational Mathematics  9(6):717–772  2009.

[6] B. Recht. A simpler approach to matrix completion. Technical report  arXiv  2009.
[7] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. Journal of

Machine Learning Research  11:2057–2078  2010.

[8] V. Koltchinskii  A. B. Tsybakov  and K. Lounici. Nuclear norm penalization and optimal rates

for noisy low rank matrix completion. Technical report  arXiv  2010.

[9] E. Heiman  G. Schechtman  and A. Shraibman. Deterministic algorithms for matrix comple-

tion. Random Structures and Algorithms  2013.

[10] A. Lubotzky  R. Phillips  and P. Sarnak. Ramanujan graphs. Combinatorica  8:261–277  1988.
[11] N. Linial  S. Mendelson  G. Schechtman  and A. Shraibman. Complexity measures of sign

matrices. Combinatorica  27(4):439–463  2007.

7

,Troy Lee
Adi Shraibman
Xu Chen
Xiuyuan Cheng
Stephane Mallat
Benjamin Cowley
Ryan Williamson
Katerina Clemens
Matthew Smith
Byron Yu