2015,Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions,We propose a kernel-based method for finding matching between instances across different domains  such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features  e.g.  a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty  the proposed method embeds all the features of different domains in a shared latent space  and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically  we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments  we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles  between documents and tags  and between images and tags.,Cross-Domain Matching for Bag-of-Words Data
via Kernel Embeddings of Latent Distributions

Nara Institute of Science and Technology

NTT Communication Science Laboratories

Yuya Yoshikawa(cid:3)

Nara  630-0192  Japan

Tomoharu Iwata

Kyoto  619-0237  Japan

yoshikawa.yuya.yl9@is.naist.jp

iwata.tomoharu@lab.ntt.co.jp

Hiroshi Sawada

NTT Service Evolution Laboratories

Kanagawa  239-0847  Japan

Takeshi Yamada

NTT Communication Science Laboratories

Kyoto  619-0237  Japan

sawada.hiroshi@lab.ntt.co.jp

yamada.tak@lab.ntt.co.jp

Abstract

We propose a kernel-based method for ﬁnding matching between instances across
different domains  such as multilingual documents and images with annotations.
Each instance is assumed to be represented as a multiset of features  e.g.  a bag-of-
words representation for documents. The major difﬁculty in ﬁnding cross-domain
relationships is that the similarity between instances in different domains cannot
be directly measured. To overcome this difﬁculty  the proposed method embeds
all the features of different domains in a shared latent space  and regards each
instance as a distribution of its own features in the shared latent space. To repre-
sent the distributions efﬁciently and nonparametrically  we employ the framework
of the kernel embeddings of distributions. The embedding is estimated so as to
minimize the difference between distributions of paired instances while keeping
unpaired instances apart. In our experiments  we show that the proposed method
can achieve high performance on ﬁnding correspondence between multi-lingual
Wikipedia articles  between documents and tags  and between images and tags.

1 Introduction
The discovery of matched instances in different domains is an important task  which appears in nat-
ural language processing  information retrieval and data mining tasks such as ﬁnding the alignment
of cross-lingual sentences [1]  attaching tags to images [2] or text documents [3]  and matching user
identiﬁcations in different databases [4].
When given an instance in a source domain  our goal is to ﬁnd the instance in a target domain that
is the most closely related to the given instance. In this paper  we focus on a supervised setting 
where correspondence information between some instances in different domains is given. To ﬁnd
matching in a single domain  e.g.  ﬁnd documents relevant to an input document  a similarity (or
distance) measure between instances can be used. On the other hand  when trying to ﬁnd matching
between instances in different domains  we cannot directly measure the distances since they con-
sist of different types of features. For example  when matching documents in different languages 
since the documents have different vocabularies we cannot directly measure the similarities between
documents across different languages without dictionaries.

(cid:3)

The author moved to Software Technology and Artiﬁcial Intelligence Research Laboratory (STAIR Lab)

at Chiba Institute of Technology  Japan.

1

Figure 1: An example of the proposed method
used on a multilingual document matching
task. Correspondences between instances in
source (English) and target (Japanese) do-
mains are observed. The proposed method as-
sumes that each feature (vocabulary term) has
a latent vector in a shared latent space  and
each instance is represented as a distribution
of the latent vectors of the features associated
with the instance. Then  the distribution is
mapped as an element in a reproducing kernel
Hilbert space (RKHS) based on the kernel em-
beddings of distributions. The latent vectors
are estimated so that the paired instances are
embedded closer together in the RKHS.

One solution is to map instances in both the source and target domains into a shared latent space.
One such method is canonical correspondence analysis (CCA) [5]  which maps instances into a la-
tent space by linear projection to maximize the correlation between paired instances in the latent
space. However  in practice  CCA cannot solve non-linear relationship problems due to its linearity.
To ﬁnd non-linear correspondence  kernel CCA [6] can be used. It has been reported that kernel
CCA performs well as regards document/sentence alignment between different languages [7  8] 
when searching for images from text queries [9] and when matching 2D-3D face images [10]. Note
that the performance of kernel CCA depends on how appropriately we deﬁne the kernel function
for measuring the similarity between instances within a domain. Many kernels  such as linear  poly-
nomial and Gaussian kernels  cannot consider the occurrence of different but semantically similar
words in two instances because these kernels use the inner-product between the feature vectors rep-
resenting the instances. For example  words  ‘PC’ and ‘Computer’  are different but indicate the
same meaning. Nevertheless  the kernel value between instances consisting only of ‘PC’ and con-
sisting only of ‘Computer’ is equal to zero with linear and polynomial kernels. Even if a Gaussian
kernel is used  the kernel value is determined only by the vector length of the instances.
In this paper  we propose a kernel-based cross-domain matching method that can overcome the
problem of kernel CCA. Figure 1 shows an example of the proposed method. The proposed method
assumes that each feature in source and target domains is associated with a latent vector in a shared
latent space. Since all the features are mapped into the latent space  the proposed method can mea-
sure the similarity between features in different domains. Then  each instance is represented as a
distribution of the latent vectors of features that are contained in the instance. To represent the dis-
tributions efﬁciently and nonparametrically  we employ the framework of the kernel embeddings of
distributions  which measures the difference between distributions in a reproducing kernel Hilbert
space (RKHS) without the need to deﬁne parametric distributions. The latent vectors are estimated
by minimizing the differences between the distributions of paired instances while keeping unpaired
instances apart. The proposed method can discover unseen matching in test data by using the dis-
tributions of the estimated latent vectors. We will explain matching between two domains below 
however  the proposed method can be straightforwardly extended to matching between three and
more domains by regarding one of the domains as a pivot domain.
In our experiments  we demonstrate the effectiveness of our proposed method in tasks that involve
ﬁnding the correspondence between multi-lingual Wikipedia articles  between documents and tags 
and between images and tags  by comparison with existing linear and non-linear matching methods.

2 Related Work
As described above  canonical correlation analysis (CCA) and kernel CCA have been successfully
used for ﬁnding various types of cross-domain matching. When we want to match cross-domain
instances represented by bag-of-words such as documents  bilingual topic models [1  11] can also
be used. The difference between the proposed method and these methods is that since the proposed
method represents each instance as a set of latent vectors of its own features  the proposed method
can learn a more complex representation of the instance than these existing methods that represent

2

each instance as a single latent vector. Another difference is that the proposed method employs a
discriminative approach  while kernel CCA and bilingual topic models employ generative ones.
To model cross-domain data  deep learning and neural network approaches have been recently pro-
posed [12  13]. Unlike such approaches  the proposed method performs non-linear matching without
deciding the number of layers of the networks  which largely affects their performances.
A key technique of the proposed method is the kernel embeddings of distributions [14]  which can
represent a distribution as an element in an RKHS  while preserving the moment information of
the distribution such as the mean  covariance and higher-order moments without density estima-
tion. The kernel embeddings of distributions have been successfully used for a statistical test of the
independence of two sample sets [15]  discriminative learning on distribution data [16]  anomaly
detection for group data [17]  density estimation [18] and a three variable interaction test [19]. Most
previous studies about the kernel embeddings of distributions consider cases where the distributions
are unobserved but the samples generated from the distributions are observed. Additionally  each
of the samples is represented as a dense vector. With the proposed method  the kernel embedding
technique cannot be used to represent the observed multisets of features such as bag-of-words for
documents  since each of the features is represented as a one-hot vector whose dimensions are zero
except for the dimension indicating that the feature has one. In this study  we beneﬁt from the kernel
embeddings of distributions by representing each feature as a dense vector in a shared latent space.
The proposed method is inspired by the use of the kernel embeddings of distributions in bag-of-
words data classiﬁcation [20] and regression [21]. Their methods can be applied to single domain
data  and the latent vectors of features are used to measure the similarity between the features in a
domain. Unlike these methods  the proposed method is used for the cross-domain matching of two
different types of domain data  and the latent vectors are used for measuring the similarity between
the features in different domains.
3 Kernel Embeddings of Distributions
In this section  we introduce the framework of the kernel embeddings of distributions. The kernel
embeddings of distributions are used to embed any probability distribution P on space X into a re-
producing kernel Hilbert space (RKHS) Hk speciﬁed by kernel k  and the distribution is represented
(P) in the RKHS. More precisely  when given distribution P  the kernel embedding
as element m
of the distribution m

(P) is deﬁned as follows:

(cid:3)

(cid:3)

∫

(cid:3)

(P) := Ex(cid:24)P[k((cid:1); x)] =

k((cid:1); x)dP 2 Hk;

m

(1)
(P) pre-
where kernel k is referred to as embedding kernel. It is known that kernel embedding m
serves the properties of probability distribution P such as the mean  covariance and higher-order
∑
moments by using characteristic kernels (e.g.  Gaussian RBF kernel) [22].
When a set of samples X = fxlgn
empirical distribution ^P = 1
empirical kernel embedding m(X) is given by

l=1 is drawn from the distribution  by interpreting sample set X as
l=1 (cid:14)xl ((cid:1))  where (cid:14)x((cid:1)) is the Dirac delta function at point x 2 X  

X

(cid:3)

n

n

l=1

1
n

m(X) =

k((cid:1); xl);
which can be approximated with an error rate of jjm(X)(cid:0)m
(cid:3)
2 ) [14]. Unlike ker-
nel density estimation  the error rate of the kernel embeddings is independent of the dimensionality
of the given distribution.
3.1 Measuring Difference between Distributions
By using the kernel embedding representation Eq. (2)  we can measure the difference between two
distributions. Given two sets of samples X = fxlgn
′
l′=1 where xl and yl′ belong
to the same space  we can obtain their kernel embedding representations m(X) and m(Y). Then 
the difference between m(X) and m(Y) is given by

l=1 and Y = fyl′gn

(P)jjHk = Op(n

(2)

(cid:0) 1

(3)
Intuitively  it reﬂects the difference in the moment information of the distributions. The difference
is equivalent to the square of maximum mean discrepancy (MMD)  which is used for a statistical test

:

D(X; Y) = jjm(X) (cid:0) m(Y)jj2Hk

n∑

3

of independence of two distributions [15]. The difference can be calculated by expanding Eq. (3) as
follows:

jjm(X) (cid:0) m(Y)jj2Hk

(cid:0) 2⟨m(X); m(Y)⟩Hk ;
n∑
where  ⟨(cid:1);(cid:1)⟩Hk is an inner-product in the RKHS. In particular  ⟨m(X); m(Y)⟩Hk is given by

= ⟨m(X); m(X)⟩Hk + ⟨m(Y); m(Y)⟩Hk
⟨

′∑

′∑

n∑

n

n

⟩
k((cid:1); yl′ )

(4)

(5)

⟨m(X); m(Y)⟩Hk =

1
n

l=1

k((cid:1); xl);

1
n′

l′=1

=

1
nn′

Hk

l=1

l′=1

k(xl; yl′):

i ; dt

i)gN

⟨m(X); m(X)⟩Hk and ⟨m(Y); m(Y)⟩Hk can also be calculated by Eq. (5).
4 Proposed Method
Suppose that we are given a training set consisting of N instance pairs O = f(ds
i=1  where ds
i
is the ith instance in a source domain and dt
i is the ith instance in a target domain. These instances
i are represented as multisets of features included in source feature set F s and target feature
i and dt
ds
set F t  respectively. This means that these instances are represented as bag-of-words (BoW). The
goal of our task is to determine the unseen relationship between instances across source and target
domains in test data. The number of instances in the source domain may be different to that in the
target domain.
4.1 Kernel Embeddings of Distributions in a Shared Latent Space
As described in Section 1  the difﬁculty as regards ﬁnding cross-domain instance matching is that the
similarity between instances across source and target domains cannot be directly measured. We have
also stated that although we can ﬁnd a latent space that can measure the similarity by using kernel
CCA  standard kernel functions  e.g.  a Gaussian kernel  cannot reﬂect the co-occurrence of different
but related features in a kernel calculation between instances. To overcome them  we propose a new
data representation for ﬁnding cross-domain instance matching. The proposed method assumes that
each feature in a source feature set  f 2 F s  has a q-dimensional latent vector xf 2 Rq in a
shared space. Likewise  each feature in target feature set  g 2 F t  has a q-dimensional latent vector
yg 2 Rq in the shared space. Since all the features in the source and target domains are mapped into
a common shared space  the proposed method can capture the relationship between features both
in each domain and across different domains. We deﬁne the sets of latent vectors in the source and
target domains as X = fxfgf2F s and Y = fyggg2F t  respectively.
The proposed method assumes that each instance is represented by a distribution (or multiset) of
the latent vectors of the features that are contained in the instance. The ith instance in the source
domain ds
and the jth instance in the target
domain dt
. Note that Xi and Yj lie in the
same latent space.
In Section 3  we introduced the kernel embedding representation of a distribution and described how
to measure the difference between two distributions when samples generated from the distribution
are observed. In the proposed method  we employ the kernel embeddings of distributions to repre-
sent the distributions of the latent vectors for the instances. The kernel embedding representations
for the ith source and the jth target domain instances are given by
1jdt

i is represented by a set of latent vectors Xi = fxfgf2ds
j is represented by a set of latent vectors Yj = fyggg2dt

k((cid:1); xf );

k((cid:1); yg):

∑

∑

m(Yj) =

m(Xi) =

1jds

(6)

j

j

i

j

i

f2ds

i

j

g2dt

j

Then  the difference between the distributions of the latent vectors are measured by using Eq. (3) 
that is  the difference between the ith source and the jth target domain instances is given by

D(Xi; Yj) = jjm(Xi) (cid:0) m(Yj)jj2Hk

:

(7)

4.2 Model
The proposed method assumes that paired instances have similar distributions of latent vectors and
unpaired instances have different distributions. In accordance with the assumption  we deﬁne the
likelihood of the relationship between the ith source domain instance and the jth target domain
instance as follows:

jds
i ; X; Y; (cid:18)) =

p(dt
j

exp ((cid:0)D(Xi; Yj))
j′=1 exp ((cid:0)D(Xi; Yj′ ))

N

;

(8)

∑

4

2

jjyjj2

2

x2X exp

y2Y exp

′ ̸= jgN

((cid:0) (cid:26)

((cid:0) (cid:26)

where  (cid:18) is a set of hyper-parameters for the embedding kernel used in Eq. (6). Eq. (8) is in fact
the conditional probability with which the jth target domain instance is chosen given the ith source
domain instance. This formulation is more efﬁcient than we consider a bidirectional matching.
Intuitively  when distribution Xi is more similar to Yj than other distributions fYj′ j j
j′=1 
)
precision parameter (cid:26) > 0 for X and Y  that is  p(Xj(cid:26)) / ∏
the probability has a higher value.
∏
We deﬁne the posterior distribution of latent vectors X and Y. By placing Gaussian priors with
; p(Yj(cid:26)) /

)
; the posterior distribution is given by
p(X; YjO; (cid:2)) =
(9)
∫∫
i=1 is a training set of N instance pairs  (cid:2) = f(cid:18); (cid:26)g is a set of hyper-
i)gN
p(X; Y;O; (cid:2))dXdY is a marginal probability  which is constant with

jds
i ; X; Y; (cid:18));

where  O = f(ds
i ; dt
parameters and Z =
respect to X and Y.
4.3 Learning
We estimate latent vectors X and Y by maximizing the posterior probability of the latent vectors
given by Eq. (9). Instead of Eq. (9)  we consider the following negative logarithm of the posterior
probability 
L(X; Y) =

exp ((cid:0)D(Xi; Yj))

p(Xj(cid:26))p(Yj(cid:26))

1A ;

N∑

∑

N∑

N∏

jjxjj2

2

p(dt
i

1
Z

i=1

jjyjj2

2

9=; +

2

(cid:26)
2

i=1

j=1

2 +

y2Y

(10)
and minimize it with respect to the latent vectors. Here  maximizing Eq. (9) is equivalent to min-
imizing Eq. (10). To minimize Eq. (10) with respect to X and Y  we perform a gradient-based
optimization. The gradient of Eq. (10) with respect to each xf 2 X is given by

8<:D(Xi; Yi) + log
∑

=

i:f2ds

i

8<: @D(Xi; Yi)

@xf

x2X

jjxjj2

0@∑
9=; + (cid:26)xf

@L(X; Y)

@xf

where 

(cid:0) 1
ci

N∑
N∑

j=1

eij

ci =

j=1

@D(Xi; Yj)

@xf

exp ((cid:0)D(Xi; Yj)) ;

∑

∑

(11)

(12)

eij = exp ((cid:0)D(Xi; Yj)) ;
∑

∑

and the gradient of the difference between distributions Xi and Yj with respect to xf is given by

@D(Xi; Yj)

@xf

=

1jds
j2

i

@k(xl; xl′ )

(cid:0)

2jds
jjdt

i

j

j

l2ds

i

l′2ds

i

@xf

@k(xl; yg)

:

(13)

l2ds

i

g2dt

i

@xf

@xf

When the distribution Xi does not include the latent vector xf   the gradient consistently becomes a
zero vector. @k(xl;xl′ )
is the gradient of an embedding kernel. This depends on the choice of kernel.
When the embedding kernel is a Gaussian kernel  the gradient is calculated as with Eq. (15) in [21].
Similarly  The gradient of Eq. (10) with respect to each yg 2 Y is given by
@D(Xi; Yj)

@L(X; Y)

∑

N∑

8<: @D(Xi; Yi)

@yg

9=; + (cid:26)yg;

(cid:0) 1
ci

eij

j:g2dt

j

@yg

@yg

=

i=1

(14)

where  the gradient of the difference between distributions Xi and Yj with respect to yg can be
calculated as with Eq. (13)
Learning is performed by alternately updating X using Eq. (11) and updating Y using Eq. (14) until
the improvement in the negative log likelihood Eq. (10) converges.
4.4 Matching
After the estimation of the latent vectors X and Y  the proposed method can reveal the matching
between test instances. The matching is found by ﬁrst measuring the difference between a given
source domain instance and target domain instances using Eq. (7)  and then searching for the instance
pair with the smallest difference.

5

)

jjxf (cid:0) ygjj2

2

2

(cid:0)2; 10

((cid:0) (cid:13)

(cid:0)1g and (cid:13) 2 f10

5 Experiments
In this section  we report our experimental results for three different types of cross-domain datasets:
multi-lingual Wikipedia  document-tag and image-tag datasets.
Setup of proposed method. Throughout these experiments  we used a Gaussian kernel with param-
eter (cid:13) (cid:21) 0: k(xf ; yg) = exp
as an embedding kernel. The hyper-parameters of
the proposed method are the dimensionality of a shared latent space q  a regularizer parameter for
latent vectors (cid:26) and a Gaussian embedding kernel parameter (cid:13). After we train the proposed method
with various hyper-parameters q 2 f8; 10; 12g  (cid:26) 2 f0; 10
(cid:0)1; 100;(cid:1)(cid:1)(cid:1) ; 103g 
we chose the optimal hyper-parameters by using validation data. When training the proposed
method  we initialized latent vectors X and Y by applying principle component analysis (PCA)
to a matrix concatenating two feature-frequency matrices in the source and target domains. Then 
we employed the L-BFGS method [23] with gradients given by Eqs. (11) (14) to learn the latent
vectors.
Comparison methods. We compared the proposed method with the k-nearest neighbor method
(KNN)  canonical correspondence analysis (CCA)  kernel CCA (KCCA)  bilingual latent Dirichlet
allocation (BLDA)  and kernel CCA with the kernel embeddings of distributions (KED-KCCA). For
a test instance in the source domain  our KNN searches for the nearest neighbor source instances in
the training data  and outputs a target instance in the test data  which is located close to the target
instances that are paired with the searched for source instances. CCA and KCCA ﬁrst learn the
projection of instances into a shared latent space using training data  and then they ﬁnd matching
between instances by projecting the test instances into the shared latent space. KCCA used a Gaus-
sian kernel for measuring the similarity between instances and chose the optimal Gaussian kernel
parameter and regularizer parameter by using validation data. With BLDA  we ﬁrst learned the same
model as [1  11] and found matching between instances in the test data by obtaining the topic dis-
tributions of these instances from the learned model. KED-KCCA uses the kernel embeddings of
distributions described in Section 3 for obtaining the kernel values between the instances. The vec-
tor representations of features were obtained by applying singular value decomposition (SVD) for
instance-feature frequency matrices. Here  we set the dimensionality of the vector representations to
100. Then  KED-KCCA learns kernel CCA with the kernel values as with the above KCCA. With
CCA  KCCA  BLDA and KED-KCCA  we chose the optimal latent dimensionality (or number of
topics) within f10; 20;(cid:1)(cid:1)(cid:1) ; 100g by using validation data.
Evaluation method. Throughout the experiments  we quantitatively evaluated the matching perfor-
mance by using the precision with which the true target instance is included in a set of R candidate
instances  S(R)  found by each method. More formally  the precision is given by

Nte∑

i=1

Precision@R =

1
Nte

(cid:14) (ti 2 Si(R)) ;

(15)

where  Nte is the number of test instances in the target domain  ti is the ith true target instance 
Si(R) is R candidate instances of the ith source instance and (cid:14)((cid:1)) is the binary function that returns
1 if the argument is true  and 0 otherwise.

5.1 Matching between Bilingual Documents
With a multi-lingual Wikipedia document dataset  we examine whether the proposed method can
ﬁnd the correct matching between documents written in different languages. The dataset includes
34 024 Wikipedia documents for each of six languages: German (de)  English (en)  Finnish (ﬁ) 
French (fr)  Italian (it) and Japanese (ja)  and documents with the same content are aligned across
the languages. From the dataset  we create 6C2 = 15 bilingual document pairs. We regard the
ﬁrst component of the pair as a source domain and the other as a target domain. For each of the
bilingual document pairs  we randomly create 10 evaluation sets that consist of 1 000 document
pairs as training data  100 document pairs as validation data and 100 document pairs as test data.
Here  each document is represented as a bag-of-words without stopwords and low frequency words.
Figure 2 shows the matching precision for each of the bilingual pairs of the Wikipedia dataset.
With all the bilingual pairs  the proposed method achieves signiﬁcantly higher precision than the
other methods with a wide range of R. Table 1 shows examples of predicted matching with the
Japanese-English Wikipedia dataset. Compared with KCCA  which is the second best method  the

6

Figure 2: Precision of matching prediction and its standard deviation on multi-lingual Wikipedia
datasets.

Table 1: Top ﬁve English documents matched by the proposed method and KCCA given ﬁve
Japanese documents in the Wikipedia dataset. Titles in bold typeface indicate correct matching.

(a) Japanese Input title: SD (cid:935)(cid:660)(cid:965) (SD card)
Intel  SD card  Libavcodec  MPlayer  Freeware
Proposed
BBC World News  SD card  Morocco  Phoenix  24 Hours of Le Mans
KCCA

(b) Japanese Input title: (cid:2936)(cid:5739)(cid:2481) (Anthrax)
Proposed
KCCA

Psittacosis  Anthrax  Dehydration  Isopoda  Cataract
Dehydration  Psittacosis  Cataract  Hypergeometric distribution  Long Island Iced Tea

(c) Japanese Input title: (cid:965)(cid:959)(cid:979)(cid:997)(cid:660)(cid:1966)(cid:1356) (Doppler effect)
Proposed
KCCA

LU deconmposition  Redshift  Doppler effect  Phenylalanine  Dehydration
Long Island Iced Tea  Opportunity cost  Cataract  Hypergeometric distribution  Intel

(d) Japanese Input title: (cid:989)(cid:937)(cid:947)(cid:943)(cid:3977)(cid:3943) (Mexican cuisine)
Proposed Mexican cuisine  Long Island Iced Tea  Phoenix  Baldr  China Radio International
Taoism  Chariot  Anthrax  Digital Millennium Copyright Act  Alexis de Tocqueville

KCCA

(e) Japanese Input title: (cid:977)(cid:998)(cid:660)(cid:930)(cid:931)(cid:926) (Freeware)
Proposed
KCCA

BBC World News  Opportunity cost  Freeware  NFS  Intel
Digital Millennium Copyright Act  China Radio International  Hypergeometric distribution  Taoism  Chariot

proposed method can ﬁnd both the correct document and many related documents. For example 
in Table 1(a)  the correct document title is “SD card”. The proposed method outputs the SD card’s
document and documents related to computer technology such as “Intel” and “MPlayer”. This is
because the proposed method can capture the relationship between words and reﬂect the difference
between documents across different domains by learning the latent vectors of the words.

5.2 Matching between Documents and Tags  and between Images and Tags
We performed experiments matching documents and tailgates  and matching images and tailgates
with the datasets used in [3]. When matching documents and tailgates  we use datasets obtained
from two social bookmarking sites  delicious1 and hatena2  and patent dataset. The
delicious and the hatena datasets include pairs consisting of a web page and a tag list la-
beled by users  and the patent dataset includes pairs consisting of a patent description and a tag list
representing the category of the patent. Each web page and each patent description are represented

1https://delicious.com/
2http://b.hatena.ne.jp/

7

Figure 3: Precision of matching prediction and its standard deviation on delicious  hatena 
patent and flickr datasets.

Figure 4: Two examples of input tag lists and the top ﬁve images matched by the proposed method
on the flickr dataset.

as a bag-of-words as with the experiments using the Wikipedia dataset  and the tag list is represented
as a set of tags. With the matching of images and tag lists  we use the flickr dataset  which con-
sists of pairs of images and tag lists. Each image is represented as a bag-of-visual-words  which
is obtained by ﬁrst extracting features using SIFT  and then applying K-means clustering with 200
components to the SIFT features. For all the datasets  the numbers of training  test and validation
pairs are 1 000  100 and 100  respectively.
Figure 3 shows the precision of the matching prediction of the proposed and comparison methods
for the delicious  hatena  patent and flickr datasets. The precision of the comparison
methods with these datasets was much the same as the precision of random prediction. Nevertheless 
the proposed method achieved very high precision particularly for the delicious  hatena and
patent datasets. Figure 4 shows examples of input tag lists and the top ﬁve images matched by
the proposed method with the flickr dataset. In the examples  the proposed method found the
correct images and similar related images from given tag lists.

6 Conclusion

We have proposed a novel kernel-based method for addressing cross-domain instance matching tasks
with bag-of-words data. The proposed method represents each feature in all the domains as a latent
vector in a shared latent space to capture the relationship between features. Each instance is rep-
resented by a distribution of the latent vectors of features associated with the instance  which can
be regarded as samples from the unknown distribution corresponding to the instance. To calculate
difference between the distributions efﬁciently and nonparametrically  we employ the framework of
kernel embeddings of distributions  and we learn the latent vectors so as to minimize the difference
between the distributions of paired instances in a reproducing kernel Hilbert space. Experiments
on various types of cross-domain datasets conﬁrmed that the proposed method signiﬁcantly outper-
forms the existing methods for cross-domain matching.
Acknowledgments. This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867).

8

References
[1] T Zhang  K Liu  and J Zhao. Cross Lingual Entity Linking with Bilingual Topic Model. In Proceedings

of the Twenty-Third International Joint Conference on Artiﬁcial Intelligence  2013.

[2] Yunchao Gong  Qifa Ke  Michael Isard  and Svetlana Lazebnik. A Multi-View Embedding Space
International Journal of Computer Vision 

for Modeling Internet Images  Tags  and Their Semantics.
106(2):210–233  oct 2013.

[3] Tomoharu Iwata  T. Yamada  and N. Ueda. Modeling Social Annotation Data with Content Relevance

using a Topic Model. In Advances in Neural Information Processing Systems. Citeseer  2009.

[4] Bin Li  Qiang Yang  and Xiangyang Xue. Transfer Learning for Collaborative Filtering via a Rating-
In Proceedings of the 26th Annual International Conference on Machine

Matrix Generative Model.
Learning  2009.

[5] H. Hotelling. Relations Between Two Sets of Variants. Biometrika  28:321–377  1936.
[6] S Akaho. A Kernel Method for Canonical Correlation Analysis. In Proceedings of International Meeting

on Psychometric Society  number 4  2001.

[7] Alexei Vinokourov  John Shawe-Taylor  and Nello Cristianini. Inferring a Semantic Representation of
Text via Cross-Language Correlation Analysis. In Advances in Neural Information Processing Systems 
2003.

[8] Yaoyong Li and John Shawe-Taylor. Using KCCA for Japanese-English Cross-Language Information
Retrieval and Document Classiﬁcation. Journal of Intelligent Information Systems  27(2):117–133  sep
2006.

[9] Nikhil Rasiwasia  Jose Costa Pereira  Emanuele Coviello  Gabriel Doyle  Gert R.G. Lanckriet  Roger
Levy  and Nuno Vasconcelos. A New Approach to Cross-Modal Multimedia Retrieval. In Proceedings of
the International Conference on Multimedia  2010.

[10] Patrik Kamencay  Robert Hudec  Miroslav Benco  and Martina Zachariasov. 2D-3D Face Recognition
Method Based on a Modiﬁed CCA-PCA Algorithm. International Journal of Advanced Robotic Systems 
2014.

[11] Tomoharu Iwata  Shinji Watanabe  and Hiroshi Sawada. Fashion Coordinates Recommender System
Using Photographs from Fashion Magazines. In Proceedings of the Twenty-Second International Joint
Conference on Artiﬁcial Intelligence. AAAI Press  jul 2011.

[12] Jiquan Ngiam  Aditya Khosla  Mingyu Kim  Juhan Nam  Honglak Lee  and Andrew Y Ng. Multimodal
Deep Learning. In Proceedings of The 28th International Conference on Machine Learning  pages 689–
696  2011.

[13] Galen Andrew  Raman Arora  Jeff Bilmes  and Karen Livescu. Deep Canonical Correlation Analysis. In

Proceedings of The 30th International Conference on Machine Learning  pages 1247–1255  2013.

[14] Alex Smola  Arthur Gretton  Le Song  and Bernhard Sch¨olkopf. A Hilbert Space Embedding for Distri-

butions. In Algorithmic Learning Theory. 2007.

[15] A. Gretton  K. Fukumizu  C.H. Teo  L. Song  B. Sch¨olkopf  and A.J. Smola. A Kernel Statistical Test of

Independence. In Advances in Neural Information Processing Systems  2008.

[16] Krikamol Muandet  Kenji Fukumizu  Francesco Dinuzzo  and Bernhard Sch¨olkopf. Learning from Dis-
tributions via Support Measure Machines. In Advances in Neural Information Processing Systems  2012.
[17] Krikamol Muandet and Bernhard Sch¨olkopf. One-Class Support Measure Machines for Group Anomaly
Detection. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence  2013.
[18] M Dudik  S J Phillips  and R E Schapire. Maximum Entropy Density Estimation with Generalized Regu-
larization and an Application to Species Distribution Modeling. Journal of Machine Learning Research 
8:1217–1260  2007.

[19] Dino Sejdinovic  Arthur Gretton  and Wicher Bergsma. A Kernel Test for Three-Variable Interactions. In

Advances in Neural Information Processing Systems  2013.

[20] Yuya Yoshikawa  Tomoharu Iwata  and Hiroshi Sawada. Latent Support Measure Machines for Bag-of-

Words Data Classiﬁcation. In Advances in Neural Information Processing Systems  2014.

[21] Yuya Yoshikawa  Tomoharu Iwata  and Hiroshi Sawada. Non-linear Regression for Bag-of-Words Data
via Gaussian Process Latent Variable Set Model. In Proceedings of the 29th AAAI Conference on Artiﬁcial
Intelligence  2015.

[22] Bharath K. Sriperumbudur  Arthur Gretton  Kenji Fukumizu  Bernhard Sch¨olkopf  and Gert R. G. Lanck-
riet. Hilbert Space Embeddings and Metrics on Probability Measures. The Journal of Machine Learning
Research  11:1517–1561  2010.

[23] Dong C. Liu and Jorge Nocedal. On the Limited Memory BFGS Method for Large Scale Optimization.

Mathematical Programming  45(1-3):503–528  aug 1989.

9

,Yuya Yoshikawa
Tomoharu Iwata
Hiroshi Sawada
Takeshi Yamada
Ding Liu
Bihan Wen
Yuchen Fan
Chen Change Loy
Thomas Huang