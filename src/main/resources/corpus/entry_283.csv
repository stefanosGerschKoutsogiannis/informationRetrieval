2016,A primal-dual method for conic constrained distributed optimization problems,We consider cooperative multi-agent consensus optimization problems over an undirected network of agents  where only those agents connected by an edge can directly communicate. The objective is to minimize the sum of agent-specific composite convex functions over agent-specific private conic constraint sets; hence  the optimal consensus decision should lie in the intersection of these private sets. We provide convergence rates in sub-optimality  infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.,A primal-dual method for conic constrained

distributed optimization problems

Necdet Serhat Aybat

Erfan Yazdandoost Hamedani

Department of Industrial Engineering

Department of Industrial Engineering

Penn State University

University Park  PA 16802

nsa10@psu.edu

Penn State University

University Park  PA 16802

evy5047@psu.edu

Abstract

We consider cooperative multi-agent consensus optimization problems over an
undirected network of agents  where only those agents connected by an edge
can directly communicate. The objective is to minimize the sum of agent-
speciﬁc composite convex functions over agent-speciﬁc private conic constraint
sets; hence  the optimal consensus decision should lie in the intersection of these
private sets. We provide convergence rates in sub-optimality  infeasibility and
consensus violation; examine the effect of underlying network topology on the
convergence rates of the proposed decentralized algorithms; and show how to ex-
tend these methods to handle time-varying communication networks.

1

Introduction

Let G = (N  E) denote a connected undirected graph of N computing nodes  where N  
{1  . . .   N} and E ⊆ N × N denotes the set of edges – without loss of generality assume that
(i  j) ∈ E implies i < j. Suppose nodes i and j can exchange information only if (i  j) ∈ E   and
each node i ∈ N has a private (local) cost function Φi : Rn → R ∪ {+∞} such that
where ρi : Rn → R ∪ {+∞} is a possibly non-smooth convex function  and fi : Rn → R is a
smooth convex function. We assume that fi is differentiable on an open set containing dom ρi with
a Lipschitz continuous gradient ∇fi  of which Lipschitz constant is Li; and the prox map of ρi 

Φi(x)   ρi(x) + fi(x) 

(1)

(2)

proxρi

(x)   argmin

y∈Rn (cid:8)ρi(y) + 1

2 ky − xk2(cid:9)  

is efﬁciently computable for i ∈ N   where k.k denotes the Euclidean norm. Let Ni   {j ∈ N :
(i  j) ∈ E or (j  i) ∈ E} denote the set of neighboring nodes of i ∈ N   and di   |Ni| is the degree
of node i ∈ N . Consider the following minimization problem:
s.t. Aix − bi ∈ Ki 

∀i ∈ N  

Φi(x)

(3)

where Ai ∈ Rmi×n  bi ∈ Rmi and Ki ⊆ Rmi is a closed  convex cone. Suppose that projections
onto Ki can be computed efﬁciently  while the projection onto the preimage A−1
(Ki+bi) is assumed
to be impractical  e.g.  when Ki is the positive semideﬁnite cone  projection to preimage requires
solving an SDP. Our objective is to solve (3) in a decentralized fashion using the computing nodes
N and exchanging information only along the edges E. In Section 2 and Section 3  we consider (3)
when the topology of the connectivity graph is static and time-varying  respectively.

i

min

x∈Rn Xi∈N

This computational setting  i.e.  decentralized consensus optimization  appears as a generic model
for various applications in signal processing  e.g.  [1  2]  machine learning  e.g.  [3  4  5] and sta-
tistical inference  e.g.  [6]. Clearly  (3) can also be solved in a “centralized” fashion by commu-
nicating all the private functions Φi to a central node  and solving the overall problem at this

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

node. However  such an approach can be very expensive both from communication and com-
putation perspectives when compared to the distributed algorithms which are far more scalable
to increasing problem data and network sizes.

In particular  suppose (Ai  bi) ∈ Rm×(n+1) and
Φi(x) = λkxk1 + kAix − bik2 for some given λ > 0 for i ∈ N such that m ≪ n and N ≫ 1.
Hence  (3) is a very large scale LASSO problem with distributed data. To solve (3) in a centralized
fashion  the data {(Ai  bi) : i ∈ N} needs to be communicated to the central node. This can be
prohibitively expensive  and may also violate privacy constraints – in case some node i does not
want to reveal the details of its private data. Furthermore  it requires that the central node has large
enough memory to be able to accommodate all the data. On the other hand  at the expense of slower
convergence  one can completely do away with a central node  and seek for consensus among all
the nodes on an optimal decision using “local” decisions communicated by the neighboring nodes.
From computational perspective  for certain cases  computing partial gradients locally can be more
computationally efﬁcient when compared to computing the entire gradient at a central node. With
these considerations in mind  we propose decentralized algorithms that can compute solutions to (3)
using only local computations without explicitly requiring the nodes to communicate the functions
{Φi : i ∈ N}; thereby  circumventing all privacy  communication and memory issues. Examples
of constrained machine learning problems that ﬁt into our framework include multiple kernel learn-
ing [7]  and primal linear support vector machine (SVM) problems. In the numerical section we
implemented the proposed algorithms on the primal SVM problem.

1.1 Previous Work

There has been active research [8  9  10  11  12] on solving convex-concave saddle point problems
minx maxy L(x  y). In [9] primal-dual proximal algorithms are proposed for convex-concave prob-
lems with known saddle-point structure minx maxy Ls(x  y)   Φ(x) + hT x  yi − h(y)  where Φ
and h are convex functions  and T is a linear map. These algorithms converge with rate O(1/k) for
the primal-dual gap  and they can be modiﬁed to yield a convergence rate of O(1/k2) when either
Φ or h is strongly convex  and O(1/ek) linear rate  when both Φ and h are strongly convex. More

recently  in [11] Chambolle and Pock extend their previous work in [9]  using simpler proofs  to
handle composite convex primal functions  i.e.  sum of smooth and (possibly) nonsmooth functions 
and to deal with proximity operators based on Bregman distance functions.
Consider minx∈Rn{Pi∈N
Φi(x) : x ∈ ∩i∈NXi} over G = (N  E). Although the uncon-
strained consensus optimization  i.e.  Xi = Rn  is well studied – see [13  14] and the references
therein  the constrained case is still an immature  and recently developing area of active research
[13  14  15  16  17  18  19]. Other than few exceptions  e.g.  [15  16  17]  the methods in liter-
ature require that each node compute a projection on the privately known set Xi in addition to
consensus and (sub)gradient steps  e.g.  [18  19]. Moreover  among those few exceptions that do not
use projections onto Xi when ΠXi is not easy to compute  only [15  16] can handle agent-speciﬁc
constraints without assuming global knowledge of the constraints by all agents. However  no rate
results in terms of suboptimality  local infeasibility  and consensus violation exist for the primal-
dual distributed methods in [15  16] when implemented for the agent-speciﬁc conic constraint sets
Xi = {x : Aix − bi ∈ Ki} studied in this paper. In [15]  a consensus-based distributed primal-
dual perturbation (PDP) algorithm using a square summable but not summable step-size sequence
is proposed. The objective is to minimize a composition of a global network function (smooth) with
the summation of local objective functions (smooth)  subject to local compact sets and inequality
constraints on the summation of agent speciﬁc constrained functions. They showed that the local
primal-dual iterate sequence converges to a global optimal primal-dual solution; however  no rate
result was provided. The proposed PDP method can also handle non-smooth constraints with sim-
ilar convergence guarantees. Finally  while we were preparing this paper  we became aware of a
very recent work [16] related to ours. The authors proposed a distributed algorithm on time-varying
communication network for solving saddle-point problems subject to consensus constraints. The
algorithm can also be applied to solve consensus optimization problems with inequality constraints
that can be written as summation of local convex functions of local and global variables. Under
some assumptions  it is shown that using a carefully selected decreasing step-size sequence  the

ergodic average of primal-dual sequence converges with O(1/√k) rate in terms of saddle-point

evaluation error; however  when applied to constrained optimization problems  no rate in terms of
either suboptimality or infeasibility is provided.

2

Contribution. We propose primal-dual algorithms for distributed optimization subject to agent
speciﬁc conic constraints. By assuming composite convex structure on the primal functions  we
show that our proposed algorithms converge with O(1/k) rate where k is the number of consensus
iterations. To the best of our knowledge  this is the best rate result for our setting. Indeed  ǫ-optimal
and ǫ-feasible solution can be computed within O(1/ǫ) consensus iterations for the static topology 
and within O(1/ǫ1+1/p) consensus iterations for the dynamic topology for any rational p ≥ 1 
although O(1) constant gets larger for large p. Moreover  these methods are fully distributed  i.e. 
the agents are not required to know any global parameter depending on the entire network topology 
e.g.  the second smallest eigenvalue of the Laplacian; instead  we only assume that agents know who
their neighbors are. Due to limited space  we put all the technical proofs to the appendix.

1.2 Preliminary

Let X and Y be ﬁnite-dimensional vector spaces.
proposed a primal-dual algorithm (PDA) for the following convex-concave saddle-point problem:

In a recent paper  Chambolle and Pock [11]

min
x∈X

y∈Y L(x  y)   Φ(x) + hT x  yi − h(y)  where Φ(x)   ρ(x) + f (x) 
max

(4)

ρ and h are possibly non-smooth convex functions  f is a convex function and has a Lipschitz
continuous gradient deﬁned on dom ρ with constant L  and T is a linear map. Brieﬂy  given x0  y0
and algorithm parameters νx  νy > 0  PDA consists of two proximal-gradient steps:

xk+1 ← argmin
yk+1 ← argmin

x

y

ρ(x) + f (xk) +D∇f (xk)  x − xkE +DT x  ykE +
h(y) −DT (2xk+1 − xk)  yE +

Dy(y  yk) 

1
νy

1
νx

Dx(x  xk)

(5a)

(5b)

where Dx and Dy are Bregman distance functions corresponding to some continuously differen-
tiable strongly convex ψx and ψy such that dom ψx ⊃ dom ρ and dom ψy ⊃ dom h. In particu-
lar  Dx(x  ¯x)   ψx(x) − ψx(¯x) − h∇ψx(¯x)  x − ¯xi  and Dy is deﬁned similarly. In [11]  a simple
proof for the ergodic convergence is provided for (5); indeed  it is shown that  when the convexity
modulus for ψx and ψy is 1  if τ  κ > 0 are chosen such that ( 1

max(T )  then

νx − L) 1

νy ≥ σ2

(6)

L(¯xK   y) − L(x  ¯yK ) ≤

K (cid:18) 1
for all x  y ∈ X × Y  where ¯xK   1

1

νx

K PK

Dx(x  x0) +

1
νy

Dy(y  y0) −(cid:10)T (x − x0)  y − y0(cid:11)(cid:19)  

k=1 xk and ¯yK   1

K PK

k=1 yk.

First  we deﬁne the notation used throughout the paper. Next  in Theorem 1.1  we discuss a special
case of (4)  which will help us prove the main results of this paper  and also allow us to develop
decentralized algorithms for the consensus optimization problem in (3). The proposed algorithms in
this paper can distribute the computation over the nodes such that each node’s computation is based
on the local topology of G and the private information only available to that node.
Notation. Throughout the paper  k.k denotes the Euclidean norm. Given a convex set S  let σS (.)
denote its support function  i.e.  σS (θ)   supw∈S hθ  wi  let IS(·) denote the indicator function of
S  i.e.  IS(w) = 0 for w ∈ S and equal to +∞ otherwise  and let PS (w)   argmin{kv − wk :
v ∈ S} denote the projection onto S. For a closed convex set S  we deﬁne the distance function
as dS (w)   kPS (w) − wk. Given a convex cone K ∈ Rm  let K∗ denote its dual cone  i.e. 
K∗   {θ ∈ Rm : hθ  wi ≥ 0 ∀w ∈ K}  and K◦   −K∗ denote the polar cone of K. Note that for
a given cone K ∈ Rm  σK(θ) = 0 for θ ∈ K◦ and equal to +∞ if θ 6∈ K◦  i.e.  σK(θ) = IK◦ (θ)
for all θ ∈ Rm. Cone K is called proper if it is closed  convex  pointed  and it has a nonempty
interior. Given a convex function g : Rn → R ∪ {+∞}  its convex conjugate is deﬁned as g∗(w)  
supθ∈Rn hw  θi − g(θ). ⊗ denotes the Kronecker product  and In is the n × n identity matrix.
Deﬁnition 1. Let X   Πi∈N Rn and X ∋ x = [xi]i∈N ; Y   Πi∈N Rmi × Rm0   Y ∋ y =
[θ⊤λ⊤]⊤ and θ = [θi]i∈N ∈ Rm  where m   Pi∈N
mi  and Π denotes the Cartesian product.
Given parameters γ > 0  κi  τi > 0 for i ∈ N   let Dγ   1
Imi ]i∈N ) 
2 λ⊤Dγ λ
and Dτ   diag([ 1
τi
leads to the following Bregman distance functions: Dx(x  ¯x) = 1
  and Dy(y  ¯y) =
1
1
2 for Q ≻ 0.
  where the Q-norm is deﬁned as kzkQ

2 x⊤Dτ x and ψy(y)   1
2 kx − ¯xk2

In]i∈N ). Deﬁning ψx(x)   1

γ Im0   Dκ   diag([ 1
κi

Dτ
  (z⊤Qz)

2 θ⊤Dκθ + 1

2
Dκ

2
Dγ

+ 1

2 (cid:13)(cid:13)θ − ¯θ(cid:13)(cid:13)

2 (cid:13)(cid:13)λ − ¯λ(cid:13)(cid:13)

3

Φi(xi)  and h(y)   h0(λ) + Pi∈N

Theorem 1.1. Let X   Y  and Bregman functions Dx  Dy be deﬁned as in Deﬁnition 1. Suppose
Φ(x)   Pi∈N
hi(θi)  where {Φi}i∈N are composite convex
functions deﬁned as in (1)  and {hi}i∈N are closed convex with simple prox-maps. Given A0 ∈
Rm0×n|N| and {Ai}i∈N such that Ai ∈ Rmi×n  let T = [A⊤ A⊤0 ]⊤  where A   diag([Ai]i∈N ) ∈
Rm×n|N| is a block-diagonal matrix. Given the initial point (x0  y0)  the PDA iterate sequence
{xk  yk}k≥1  generated according to (5a) and (5b) when νx = νy = 1  satisﬁes (6) for all K ≥ 1
if ¯Q(A  A0)   
τi − Li)In]i∈N ). Moreover  if a

saddle point exists for (4)  and ¯Q(A  A0) ≻ 0  then {xk  yk}k≥1 converges to a saddle point of (4);
hence  {¯xk  ¯yk}k≥1 converges to the same point.


 (cid:23) 0  where ¯Dτ   diag([( 1

¯Dτ −A⊤ −A⊤
−A Dκ
0
−A0
0
Dγ

0

Although the proof of Theorem 1.1 follows from the lines of [11]  we provide the proof in the
appendix for the sake of completeness as it will be used repeatedly to derive our results.

Next we discuss how (5) can be implemented to compute an ǫ-optimal solution to (3) in a distributed
way using only O(1/ǫ) communications over the communication graph G while respecting node-
speciﬁc privacy requirements. Later  in Section 3  we consider the scenario where the topology of
the connectivity graph is time-varying  and propose a distributed algorithm that requires O(1/ǫ1+ 1
p )
communications for any p ≥ 1. Finally  in Section 4 we test the proposed algorithms for solving the
primal SVM problem in a decentralized manner. These results are shown under Assumption 1.1.
Assumption 1.1. The duality gap for (3) is zero  and a primal-dual solution to (3) exists.
A sufﬁcient condition for this is the existence of a Slater point  i.e.  there exists ¯x ∈ relint(dom Φ)
such that Ai ¯x − bi ∈ int(Ki) for i ∈ N   where dom Φ = ∩i∈N dom Φi.
2 Static Network Topology

Let xi ∈ Rn denote the local decision vector of node i ∈ N . By taking advantage of the fact that G

is connected  we can reformulate (3) as the following distributed consensus optimization problem:

min

xi∈Rn  i∈N(Xi∈N

Φi(xi) | xi = xj : λij  ∀(i  j) ∈ E  Aixi − bi ∈ Ki : θi  ∀i ∈ N)  

(7)

where λij ∈ Rn and θi ∈ Rmi are the corresponding dual variables. Let x = [xi]i∈N ∈ Rn|N|. The
consensus constraints xi = xj for (i  j) ∈ E can be formulated as M x = 0  where M ∈ Rn|E|×n|N|
is a block matrix such that M = H ⊗ In where H is the oriented edge-node incidence matrix  i.e. 
the entry H(i j) l  corresponding to edge (i  j) ∈ E and l ∈ N   is equal to 1 if l = i  −1 if l = j 
and 0 otherwise. Note that M TM = H TH ⊗ In = Ω ⊗ In  where Ω ∈ R|N|×|N| denotes the graph
Laplacian of G  i.e.  Ωii = di  Ωij = −1 if (i  j) ∈ E or (j  i) ∈ E  and equal to 0 otherwise.
For any closed convex set S  we have σ∗
i ∈ N   one can obtain the following saddle point problem corresponding to (7) 

S (·) = IS(·); therefore  using the fact that σ∗
Ki
y L(x  y)   Xi∈N(cid:18)Φi(xi) + hθi  Aixi − bii − σKi (θi)(cid:19) + hλ  M xi 
where y = [θ⊤ λ⊤]⊤ for λ = [λij](i j)∈E ∈ Rn|E|  θ = [θi]i∈N ∈ Rm  and m   Pi∈N
Next  we study the distributed implementation of PDA in (5a)-(5b) to solve (8). Let Φ(x)  
σKi (θi) + hbi  θii. Deﬁne the block-diagonal matrix A  
Pi∈N
diag([Ai]i∈N ) ∈ Rm×n|N| and T = [A⊤M⊤]⊤. Therefore  given the initial iterates x0  θ0  λ0
and parameters γ > 0  τi  κi > 0 for i ∈ N   choosing Dx and Dy as deﬁned in Deﬁnition 1  and
setting νx = νy = 1  PDA iterations in (5a)-(5b) take the following form:
i )  xii + hAixi − bi  θk

Φi(xi)  and h(y)   Pi∈N

xk+1 ← argmin

1
2τi kxi − xk

= IKi for

i i +

mi.

max

min

(9a)

(8)

x

x

hλk  M xi +Xi∈Nhρi(xi) + h∇f (xk
σKi (θi) − hAi(2xk+1
i − xk
n − hM (2xk+1 − xk)  λi +

1

1
2κi kθi − θk

i k2 

i ) − bi  θii +
2γ kλ − λkk2o = λk + γM (2xk+1 − xk).

i ∈ N

i k2i 

θk+1
i ← argmin
λk+1 ← argmin

θi

λ

(9b)

(9c)

4

Since Ki is a cone  proxκiσKi

(.); hence  θk+1

i

can be written in closed form as

i

(.) = PK◦
i(cid:16)θk

θk+1
i = PK◦

i + κi(cid:16)Ai(2xk+1

i − xk

i ) − bi(cid:17)(cid:17) 

i ∈ N .

Using recursion in (9c)  we can write λk+1 as a partial summation of primal iterates {xℓ}k
λk = λ0 + γ Pk−1
ℓ=0 M (2xℓ+1 − xℓ). Let λ0 ← γM x0  s0 ← x0  and sk   xk + Pk
k ≥ 1; hence  λk = γM sk. Using the fact that M⊤M = Ω ⊗ In  we obtain
hM x  λki = γ hx  (Ω ⊗ In)ski = γPi∈Nhxi  Pj∈Ni

Thus  PDA iterations given in (9) for the static graph G can be computed in a decentralized way  via
the node-speciﬁc computations as in Algorithm DPDA-S displayed in Fig. 1 below.

ℓ=0  i.e. 
ℓ=1 xℓ for

(sk
i − sk

j )i.

i ← x0
i  

Algorithm DPDA-S ( x0  θ0  γ {τi  κi}i∈N )
Initialization: s0
Step k: (k ≥ 0)
1. xk+1
2. sk+1
3. θk+1

i ∈ N
i − τi(cid:16)∇fi(xk
i ← proxτiρi(cid:16)xk
i +Pk+1
i ← xk+1
i ∈ N
i(cid:16)θk
i + κi Ai(2xk+1
i − xk
i ← PK◦
Figure 1: Distributed Primal Dual Algorithm for Static G (DPDA-S)

i + γPj∈Ni
i ∈ N

i ) − bi(cid:1)(cid:17) 

j )(cid:17)(cid:17) 

(sk
i − sk

i ) + A⊤

ℓ=1 xℓ
i  

i θk

i ∈ N

The convergence rate for DPDA-S  given in (6)  follows from Theorem 1.1 with the help of following

technical lemma which provides a sufﬁcient condition for ¯Q(A  A0) ≻ 0.
Lemma 2.1. Given {τi  κi}i∈N and γ such that γ > 0  and τi  κi > 0 for i ∈ N   let A0 = M and
A   diag([Ai]i∈N ). Then ¯Q   ¯Q(A  A0) (cid:23) 0 if {τi  κi}i∈N and γ are chosen such that

(cid:18) 1
τi − Li − 2γdi(cid:19) 1

κi ≥ σ2

max(Ai) 

∀ i ∈ N  

(10)

and ¯Q ≻ 0 if (10) holds with strict inequality  where ¯Q(A  A0) is deﬁned in Theorem 1.1.
Remark 2.1. Choosing τi = (ci + Li + 2γdi)−1  κi = ci/σ2

max(Ai) for any ci > 0 satisﬁes (10).

Next  we quantify the suboptimality and infeasibility of the DPDA-S iterate sequence.

Theorem 2.2. Suppose Assumption 1.1 holds. Let {xk  θk  λk}k≥0 be the sequence generated by
Algorithm DPDA-S  displayed in Fig. 1  initialized from an arbitrary x0 and θ0 = 0. Let step-sizes
{τi  κi}i∈N and γ be chosen satisfying (10) with strict inequality. Then {xk  θk  λk}k≥0 converges
to {x∗  θ∗  λ∗}  a saddle point of (8) such that x∗ = 1 ⊗ x∗ and (x∗  θ∗) is a primal-dual optimal
solution to (3); moreover  the following error bounds hold for all K ≥ 1:

kλ∗kkM ¯xKk +Xi∈N
γkλ∗k2 − γ

where Θ1   2

kθ∗

i k dKi (Ai ¯xK

i − bi) ≤ Θ1/K 

2 (cid:13)(cid:13)M x0(cid:13)(cid:13)

2

+Pi∈N h 1

2τikx∗i − x0

ik2 + 4

|Φ(¯xK ) − Φ(x∗)| ≤ Θ1/K 
κikθ∗i k2i  and ¯xK   1

K PK

k=1 xk.

3 Dynamic Network Topology

In this section we develop a distributed primal-dual algorithm for solving (3) when the com-
munication network topology is time-varying. We assume a compact domain  i.e.  let Di  
maxxi x′

i∈dom ρi kx − x′k and B   maxi∈N Di < ∞. Let C be the set of consensus decisions:

C   {x ∈ Rn|N | : xi = ¯x  ∀i ∈ N for some ¯x ∈ Rn s.t. k¯xk ≤ B} 

then one can reformulate (3) in a decentralized way as follows:

min

x

max

y L(x  y)   Xi∈N(cid:16)Φi(xi) + hθi  Aixi − bii − σKi (θi)(cid:17) + hλ  xi − σC (λ) 

where y = [θ⊤λ⊤]⊤ such that λ ∈ Rn|N|  θ = [θi]i∈N ∈ Rm  and m   Pi∈N

mi.

(11)

5

Next  we consider the implementation of PDA in (5) to solve (11). Let Φ(x)   Pi∈N
Φi(xi)  and
h(y)   σC(λ) +Pi∈N
σKi (θi) +hbi  θii. Deﬁne the block-diagonal matrix A   diag([Ai]i∈N ) ∈
Rm×n|N| and T = [A⊤ In|N|]⊤. Therefore  given the initial iterates x0  θ0  λ0 and parameters γ >
0  τi  κi > 0 for i ∈ N   choosing Dx and Dy as deﬁned in Deﬁnition 1  and setting νx = νy = 1 
PDA iterations given in (5) take the following form: Starting from µ0 = λ0  compute for i ∈ N

x

xk+1
i ← argmin
θk+1
i ← argmin
λk+1 ← argmin

θi

µ

i )  xii + hAixi − bi  θk
i − xk
i ) − bi  θii +

ρi(xi) + h∇f (xk
σKi (θi) − hAi(2xk+1
σC (µ) − h2xk+1 − xk  µi +

1
2γ kµ − µkk2
2 

i i + hxi  µk

1
2τi kxi − xk
i i +
1
2κi kθi − θk
i k2
2 
µk+1 ← λk+1.

i k2
2 

Using extended Moreau decomposition for proximal operators  λk+1 can be written as

λk+1 = argmin

σC (µ) +

µ

1
2γ kµ − (µk + γ(2xk+1 − xk))k2 = proxγσC

(µk + γ(2xk+1 − xk))

= µk + γ(2xk+1 − xk) − γ PC(cid:16) 1

γ

µk + 2xk+1 − xk(cid:17).

Let 1 ∈ R|N| be the vector all ones  B0   {x ∈ Rn : kxk ≤ B}. Note PB0 (x) = x min{1  B
For any x = [xi]i∈N ∈ Rn|N|  PC(x) can be computed as

kxk}.

PC (x) = 1 ⊗ p(x)  where p(x)   argmin

kξ − xik2 = argmin

ξ∈B0 kξ −

xik2.

(14)

ξ∈B0 Xi∈N

1

|N| Xi∈N

(12a)

(12b)

(12c)

(13)

(15)

Let B   {x : kxik ≤ B  i ∈ N} = Πi∈NB0. Hence  we can write PC(x) = PB ((W ⊗ In)x)
where W   1
|N|

11⊤ ∈ R|N|×|N|. Equivalently 

PC (x) = PB (1 ⊗ ˜p(x))   where

˜p(x)   1

|N | Pi∈N xi.

Although x-step and θ-step of the PDA implementation in (12) can be computed locally at each
node  computing λk+1 requires communication among the nodes. Indeed  evaluating the average
operator ˜p(.) is not a simple operation in a decentralized computational setting which only allows
for communication among neighbors. In order to overcome this issue  we will approximate ˜p(.)
operator using multi-consensus steps  and analyze the resulting iterations as an inexact primal-dual
algorithm. In [20]  this idea has been exploited within a distributed primal algorithm for uncon-
strained consensus optimization problems. We deﬁne the consensus step as one time exchanging
local variables among neighboring nodes – the details of this operation will be discussed shortly.

i   and V t

Since the connectivity network is dynamic  let Gt = (N  E t) be the connectivity network at the time
t-th consensus step is realized for t ∈ Z+. We adopt the information exchange model in [21].
Assumption 3.1. Let V t ∈ R|N|×|N| be the weight matrix corresponding to Gt = (N  E t) at the
time of t-th consensus step and N t
  {j ∈ N : (i  j) ∈ E t or (j  i) ∈ E t}. Suppose for all
t ∈ Z+: (i) V t is doubly stochastic; (ii) there exists ζ ∈ (0  1) such that for i ∈ N   V t
ij ≥ ζ if
i ; (iii) G∞ = (N  E∞) is connected where E∞   {(i  j) ∈ N × N :
j ∈ N t
(i  j) ∈ E t for inﬁnitely many t ∈ Z+}  and there exists Z ∋ T ◦ > 1 such that if (i  j) ∈ E∞  then
(i  j) ∈ E t ∪ E t+1 ∪ ... ∪ E t+T ◦−1 for all t ≥ 1.
[21] Let Assumption 3.1 holds  and W t s = V tV t−1...V s+1 for t ≥ s + 1. Given
s ≥ 0 the entries of W t s converges to 1
N as t → ∞ with a geometric rate  i.e.  for all i  j ∈ N   one
N (cid:12)(cid:12) ≤ Γαt−s  where Γ   2(1+ζ− ¯T )/(1−ζ ¯T )  α   (1−ζ ¯T )1/ ¯T   and ¯T   (N −1)T ◦.
has (cid:12)(cid:12)W t s

ij = 0 if j /∈ N t

Consider the k-th iteration of PDA as shown in (12). Instead of computing λk+1 exactly according
to (13)  we propose to approximate λk+1 with the help of Lemma 3.1 and set µk+1 to this approx-
imation. In particular  let tk be the total number of consensus steps done before k-th iteration of
PDA  and let qk ≥ 1 be the number of consensus steps within iteration k. For x = [xi]i∈N   deﬁne

ij − 1

Lemma 3.1.

i

(16)

to approximate PC(x) in (13). Note that Rk(·) can be computed in a distributed fashion requiring
qk communications with the neighbors for each node. Indeed 

Rk(x)   PB (W tk +qk  tk ⊗ In) x(cid:1)
i (x)   PB0(cid:16)Xj∈N

i (x)]i∈N such that Rk

Rk(x) = [Rk

W tk +qk  tk

ij

xj(cid:17).

(17)

6

Moreover  the approximation error  Rk(x) − PC(x)  for any x can be bounded as in (18) due to
non-expansivity of PB and using Lemma 3.1. From (15)  we get for all i ∈ N  
N Xj∈N
xj(cid:1)k
√N Γαqk kxk .

i (x) − PB0 ˜p(x)(cid:1)k = kPB0 Xj∈N
kRk

W tk +qk  tk

(18)

ij

xj(cid:1) − PB0 1
− 1
N(cid:1)xjk ≤

≤ kXj∈N W tk +qk  tk

ij

Thus  (15) implies that kRk(x) − PC(x)k ≤ N Γαqk kxk. Next  to obtain an inexact variant of

(12)  we replace the exact computation in (12c) with the inexact iteration rule:

µk+1 ← µk + γ(2xk+1 − xk) − γRk(cid:16) 1

γ µk + 2xk+1 − xk(cid:17) .

Thus  PDA iterations given in (12) can be computed inexactly  but in decentralized way for dynamic
connectivity  via the node-speciﬁc computations as in Algorithm DPDA-D displayed in Fig. 2 below.

(19)

Algorithm DPDA-D ( x0  θ0  γ {τi  κi}i∈N  {qk}k≥0 )
Initialization: µ0
Step k: (k ≥ 0)
1. xk+1

i ← 0 

i ) + A⊤

i θk

i + µk

i ∈ N
i − τi(cid:16)∇fi(xk
i + κi Ai(2xk+1
i − xk
V tk +ℓ
ij

rj 

tk +ℓ
i

∪{i}

i ← proxτiρi(cid:16)xk
i ← PK◦

2. θk+1
3. For ℓ = 1  . . .   qk
4.

i(cid:16)θk
ri ←Pj∈N

5. End For
6. µk+1

i ← µk

i + γ(2xk+1

i − xk

i ) − bi(cid:1)(cid:17) 
i ∈ N
i ) − γPB0 ri(cid:1) 

i ∈ N

i(cid:17)(cid:17) 
i ∈ N

ri ← 1

γ µk

i + 2xk+1

i − xk

i

i ∈ N

Figure 2: Distributed Primal Dual Algorithm for Dynamic Gt (DPDA-D)

Next  we deﬁne the proximal error sequence {ek}k≥1 as in (20)  which will be used later for ana-

lyzing the convergence of Algorithm DPDA-D displayed in Fig. 2.

ek+1   PC(cid:16) 1

γ µk + 2xk+1 − xk(cid:17) − Rk(cid:16) 1

γ µk + 2xk+1 − xk(cid:17) ;

hence  µk = λk + γek for k ≥ 1 when (12c) is replaced with (19). In the rest  we assume µ0 = 0.
The following observation will also be useful to prove error bounds for DPDA-D iterate sequence.
For each i ∈ N   the deﬁnition of Rk
i (x) ∈ B0 for all x; hence  from (19) 
i − xk

i in (17) implies that Rk
i )k + γkRk
γ µk + 2xk+1 − xk(cid:1)k ≤ kµk
i 1
Thus  we trivially get the following bound on (cid:13)(cid:13)µk(cid:13)(cid:13):
kµkk ≤ 4γ√N B k.

i k + 4γB.

i + γ(2xk+1

k ≤ kµk

kµk+1

i

Moreover  for any µ and λ we have that

σC (µ) = sup

x∈C hλ  xi + hµ − λ  xi ≤ σC (λ) + √N B kµ − λk.

(20)

(21)

(22)

Theorem 3.2. Suppose Assumption 1.1 holds. Starting from µ0 = 0  θ0 = 0  and an arbitrary
x0  let {xk  θk  µk}k≥0 be the iterate sequence generated using Algorithm DPDA-D  displayed in
p√k consensus steps at the k-th iteration for all k ≥ 1 for some rational p ≥ 1.
Fig. 2  using qk =
Let primal-dual step-sizes {τi  κi}i∈N and γ be chosen such that the following holds:

(cid:16) 1
τi − Li − γ(cid:17) 1

κi

> σ2

max(Ai) 

∀ i ∈ N .

(23)

Then {xk  θk  µk}k≥0 converges to {x∗  θ∗  λ∗}  a saddle point of (11) such that x∗ = 1 ⊗ x∗
and (x∗  θ∗) is a primal-dual optimal solution to (3). Moreover  the following bounds hold for all
K ≥ 1:
kλ∗k d ˜C (¯xK ) +Xi∈N
K PK

κikθ∗i k2i 
k=1 αqkh2γk2 + (cid:16)γ + kλ∗k√N B(cid:17) ki. Moreover  supK∈Z+ Θ3(K) < ∞;

γkλ∗k+(cid:13)(cid:13)x0 − x∗(cid:13)(cid:13)(cid:1)+Pi∈N h 1

where ¯xK   1
and Θ3(K)   8N 2B2Γ PK
hence  1

k=1 xk  Θ2   2kλ∗k! 1

|Φ(¯xK ) − Φ(x∗)| ≤

i k dKi (Ai ¯xK

τikx∗i −x0

i − bi) ≤

ik2 + 4

Θ2 + Θ3(K)

Θ2 + Θ3(K)

kθ∗

K

K

 

 

K Θ3(K) = O( 1
K ).

7

Remark 3.1. Note that the suboptimality  infeasibility and consensus violation at the K-th iteration
is O(Θ3(K)/K)  where Θ3(K) denotes the error accumulation due to approximation errors  and
Θ3(K) can be bounded above for all K ≥ 1 as Θ3(K) ≤ RPK
k=1 αqk k2 for some constant
p√k for k ≥ 1  then the
p√kk2 < ∞ for any p ≥ 1  if one chooses qk =
R > 0. Since P∞k=1 α
total number of communications per node until the end of K-th iteration can be bounded above by
PK
k=1 qk = O(K 1+1/p). For large p  qk grow slowly  it makes the method more practical at the cost
of longer convergence time due to increase in O(1) constant. Note that qk = (log(k))2 also works
and it grows very slowly. We assume agents know qk as a function of k at the beginning  hence 
synchronicity can be achieved by simply counting local communications with each neighbor.

4 Numerical Section

We tested DPDA-S and DPDA-D on a primal linear SVM problem where the data is distributed
among the computing nodes in N . For the static case  communication network G = (N  E) is a
connected graph that is generated by randomly adding edges to a spanning tree  generated uniformly
at random  until a desired algebraic connectivity is achieved. For the dynamic case  for each consen-
sus round t ≥ 1  Gt is generated as in the static case  and V t   I − 1
c Ωt  where Ωt is the Laplacian
of Gt  and the constant c > dt
max. We ran DPDA-S and DPDA-D on the line and complete graphs as
well to see the topology effect – for the dynamic case when the topology is line  each Gt is a random
line graph. Let S   {1  2  ..  s} and D   {(xℓ  yℓ) ∈ Rn × {−1  +1} : ℓ ∈ S} be a set of feature
vector and label pairs. Suppose S is partitioned into Stest and Strain  i.e.  the index sets for the
test and training data; let {Si}i∈N be a partition of Strain among the nodes N . Let w = [wi]i∈N  
b = [bi]i∈N   and ξ ∈ R|Strain| such that wi ∈ Rn and bi ∈ R for i ∈ N .
w b ξn 1
o
Similar to [3]  {xℓ}ℓ∈S is generated from two-dimensional multivariate Gaussian distribution with
covariance matrix Σ = [1  0; 0  2] and with mean vector either m1 = [−1 −1]T or m2 = [1  1]T
with equal probability. The experiment was performed for C = 2  |N| = 10  s = 900 such
that |Stest| = 600  |Si| = 30 for i ∈ N   i.e.  |Strain| = 300  and qk = √k. We ran DPDA-S
and DPDA-D on line  random  and complete graphs  where the random graph is generated such
that the algebraic connectivity is approximately 4. Relative suboptimality and relative consensus

Consider the following distributed SVM problem:
yℓ(wT

kwik2 + C |N|Xi∈N Xℓ∈Si

ℓ ∈ Si  i ∈ N  

2 Xi∈N

(i  j) ∈ E

min

ξℓ :

i xℓ + bi) ≥ 1 − ξℓ 

wi = wj 

bi = bj

ξℓ ≥ 0 

violation  i.e.  max(i j)∈E k[w⊤i bi]⊤ − [w⊤j bj]⊤k/(cid:13)(cid:13)(cid:13)

  and absolute feasibility violation are
plotted against iteration counter in Fig. 3  where [w∗⊤b∗] denotes the optimal solution to the central
problem. As expected  the convergence is slower when the connectivity of the graph is weaker.
Furthermore  visual comparison between DPDA-S  local SVMs (for two nodes) and centralized
SVM for the same training and test data sets is given in Fig. 4 and Fig. 5 in the appendix.

[w∗⊤b∗](cid:13)(cid:13)(cid:13)

Figure 3: Static (top) and Dynamic (bottom) network topologies: line  random  and complete graphs

8

References

[1] Qing Ling and Zhi Tian. Decentralized sparse signal recovery for compressive sleeping wireless sensor

networks. Signal Processing  IEEE Transactions on  58(7):3816–3827  2010.

[2] Ioannis D Schizas  Alejandro Ribeiro  and Georgios B Giannakis. Consensus in ad hoc WSNs with noisy
links - Part I: Distributed estimation of deterministic signals. Signal Processing  IEEE Transactions on 
56(1):350–364  2008.

[3] Pedro A Forero  Alfonso Cano  and Georgios B Giannakis. Consensus-based distributed support vector

machines. The Journal of Machine Learning Research  11:1663–1707  2010.

[4] Ryan McDonald  Keith Hall  and Gideon Mann. Distributed training strategies for the structured percep-
tron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics  pages 456–464. Association for Computational Linguis-
tics  2010.

[5] F. Yan  S. Sundaram  S. Vishwanathan  and Y. Qi. Distributed autonomous online learning: Regrets
and intrinsic privacy-preserving properties. Knowledge and Data Engineering  IEEE Transactions on 
25(11):2483–2493  2013.

[6] Gonzalo Mateos  Juan Andr´es Bazerque  and Georgios B Giannakis. Distributed sparse linear regression.

Signal Processing  IEEE Transactions on  58(10):5262–5276  2010.

[7] Francis R Bach  Gert RG Lanckriet  and Michael I Jordan. Multiple kernel learning  conic duality  and the
smo algorithm. In Proceedings of the twenty-ﬁrst international conference on Machine learning  page 6.
ACM  2004.

[8] Angelia Nedi´c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of opti-

mization theory and applications  142(1):205–228  2009.

[9] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems with

applications to imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.

[10] Bingsheng He and Xiaoming Yuan. Convergence analysis of primal-dual algorithms for a saddle-point

problem: from contraction perspective. SIAM Journal on Imaging Sciences  5(1):119–149  2012.

[11] Antonin Chambolle and Thomas Pock. On the ergodic convergence rates of a ﬁrst-order primal–dual

algorithm. Mathematical Programming  159(1):253–287  2016.

[12] Yunmei Chen  Guanghui Lan  and Yuyuan Ouyang. Optimal primal-dual methods for a class of saddle

point problems. SIAM Journal on Optimization  24(4):1779–1814  2014.

[13] A. Nedic and A. Ozdaglar. Convex Optimization in Signal Processing and Communications  chapter

Cooperative Distributed Multi-agent Optimization  pages 340–385. Cambridge University Press  2010.

[14] A. Nedi´c. Distributed optimization. In Encyclopedia of Systems and Control  pages 1–12. Springer  2014.

[15] Tsung-Hui Chang  Angelia Nedic  and Anna Scaglione.

by consensus-based primal-dual perturbation method.
59(6):1524–1538  2014.

Distributed constrained optimization
Automatic Control  IEEE Transactions on 

[16] David Mateos-N´u˜nez and Jorge Cort´es. Distributed subgradient methods for saddle-point problems. In

2015 54th IEEE Conference on Decision and Control (CDC)  pages 5462–5467  Dec 2015.

[17] Deming Yuan  Shengyuan Xu  and Huanyu Zhao. Distributed primal–dual subgradient method for multi-
agent optimization via consensus algorithms. Systems  Man  and Cybernetics  Part B: Cybernetics  IEEE
Transactions on  41(6):1715–1724  2011.

[18] Angelia Nedi´c  Asuman Ozdaglar  and Pablo A Parrilo. Constrained consensus and optimization in multi-

agent networks. Automatic Control  IEEE Transactions on  55(4):922–938  2010.

[19] Kunal Srivastava  Angelia Nedi´c  and Duˇsan M Stipanovi´c. Distributed constrained optimization over
In Decision and Control (CDC)  2010 49th IEEE Conference on  pages 1945–1950.

noisy networks.
IEEE  2010.

[20] Albert I Chen and Asuman Ozdaglar. A fast distributed proximal-gradient method. In Communication 
Control  and Computing (Allerton)  2012 50th Annual Allerton Conference on  pages 601–608. IEEE 
2012.

[21] Angelia Nedi´c and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.

Automatic Control  IEEE Transactions on  54(1):48–61  2009.

[22] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press  2015.

[23] H. Robbins and D. Siegmund. Optimizing methods in statistics (Proc. Sympos.  Ohio State Univ.  Colum-
bus  Ohio  1971)  chapter A convergence theorem for non negative almost supermartingales and some
applications  pages 233 – 257. New York: Academic Press  1971.

9

,Necdet Serhat Aybat
Erfan Yazdandoost Hamedani
Supratik Paul
Vitaly Kurin
Shimon Whiteson