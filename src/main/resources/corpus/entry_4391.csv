2018,Preference Based Adaptation for Learning Objectives,In many real-world learning tasks  it is hard to directly optimize the true performance measures  meanwhile choosing the right surrogate objectives is also difficult. Under this situation  it is desirable to incorporate an optimization of objective process into the learning loop based on weak modeling of the relationship between the true measure and the objective. In this work  we discuss the task of objective adaptation  in which the learner iteratively adapts the learning objective to the underlying true objective based on the preference feedback from an oracle. We show that when the objective can be linearly parameterized  this preference based learning problem can be solved by utilizing the dueling bandit model. A novel sampling based algorithm DL^2M is proposed to learn the optimal parameter  which enjoys strong theoretical guarantees and efficient empirical performance. To avoid learning a hypothesis from scratch after each objective function update  a boosting based hypothesis adaptation approach is proposed to efficiently adapt any pre-learned element hypothesis to the current objective. We apply the overall approach to multi-label learning  and show that the proposed approach achieves significant performance under various multi-label performance measures.,Preference Based Adaptation for Learning Objectives

Yao-Xiang Ding

Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology 

Nanjing University  Nanjing  210023  China
{dingyx  zhouzh}@lamda.nju.edu.cn

Abstract

In many real-world learning tasks  it is hard to directly optimize the true perfor-
mance measures  meanwhile choosing the right surrogate objectives is also difﬁ-
cult. Under this situation  it is desirable to incorporate an optimization of objec-
tive process into the learning loop based on weak modeling of the relationship
between the true measure and the objective. In this work  we discuss the task of
objective adaptation  in which the learner iteratively adapts the learning objective
to the underlying true objective based on the preference feedback from an oracle.
We show that when the objective can be linearly parameterized  this preference
based learning problem can be solved by utilizing the dueling bandit model. A
novel sampling based algorithm DL2M is proposed to learn the optimal parameter 
which enjoys strong theoretical guarantees and efﬁcient empirical performance.
To avoid learning a hypothesis from scratch after each objective function update 
a boosting based hypothesis adaptation approach is proposed to efﬁciently adapt
any pre-learned element hypotheses to the current objective. We apply the overall
approach to multi-label learning  and show that the proposed approach achieves
signiﬁcant performance under various multi-label performance measures.

1

Introduction

Machine learning approaches have already been applied on many real-world tasks  in which the tar-
get is usually to optimize some task-speciﬁc performance measures. For complex problems  the per-
formance measures are usually hard to be optimized directly  such as the click-through-rate in online
advertisement and the proﬁt gain in recommendation system design. Instead of directly optimizing
these complex measures  surrogate objectives with better mathematical properties are designed to
simplify optimization. It is obvious that whether the objective is correctly designed essentially af-
fects the application performance. However  it also requires delicate knowledge on the relationship
between the true measure and the objective  which is sometimes difﬁcult and challenging to acquire.
Under this situation  it is more desirable to learn both objective and hypothesis simultaneously.
Based on this motivation  we consider the novel scenario of learning with objective adaptation from
preference feedback. Under this scenario  in each iteration of the objective adaptation process  the
learner maintains a pair of objective functions  as well as the corresponding learned hypotheses 
obtained from the latest two iterations. An oracle then provides a preference over the pair of hy-
potheses to the learner  according to the true task performance measure. Based on this preference
information  the learner updates both the objective function and the corresponding hypothesis. In
special  this formulation even allows us to model complex scenerios when the true performance mea-
sure is not quantiﬁed  such as subjective human preference. It is expected that the objective function
converges to the optimal one so that the learned hypothesis optimizes the true performance measure.
In this work  we focus on the following linear parameterized objective function class. Denote the
objective by Lw; w 2 W  in which W is the parameter space  and w = [w1 w2 (cid:1)(cid:1)(cid:1) wK] is a K

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

dimensional real-valued vector. We assume that Lw can be represented as

K∑

Lw =

wili + w0(cid:3); w0 2 f0; 1g;

(1)

i=1

in which l1; l2; : : : ; lK are K convex element objectives  and w0 is an additional indicator of (cid:3).
When w0 = 0  Lw is a linear combination of the K element objectives. When w0 = 1  (cid:3) can be
utilized to represent an additional convex regularization term. It is easy to see that this linear for-
mulation covers a broad class of commonly used objectives in different learning tasks. By choosing
different w  we are allowed to consider different trade-offs among element objectives. The target of
objective adaptation is then to learn the optimal w(cid:3) which corresponds to the best trade-off leading
to the optimal task performance measure. To ensure the problem of learning a hypothesis under any
choice of w is solvable  we restrict 8wi (cid:21) 0. When w0 = 0  the scale of w does not matter  thus we
restrict W to be the non-negative part of the K-dimensional unit sphere  i.e. ∥w∥2 = 1;8wi (cid:21) 0.
the scale of w is meaningful  we restrict W to be the K-dimensional ball
When w0 = 1  i.e.
∥w (cid:0) R1K∥2 (cid:20) R  in which 1K is the K-dimensional full-one vector and R is the radius. By this
way  Lw is kept convex over all w 2 W.
There are two main challenges under the above objective adaptation scenario. One is to learn the
objective function based on preference feedback from the oracle  which requires proper modeling
of the preference feedback. Another is how hypothesis learning can be done efﬁciently without
learning from scratch when the objective is updated. In this work  we take the ﬁrst step towards
the above two challenges. First  we naturally formulate the objective adaptation process into the
dueling bandit model [Yue et al.  2012]  in which w is treated as the bandit arm and the oracle
preference is treated as the reward. A novel sampling based algorithm DL2M   which stands for
Dueling bandit Learning for Logit Model  is proposed for learning the optimal weight w(cid:3)  which
enjoys ~O(K 3=2
T ) regret bound and efﬁcient empirical performance. Second  by assuming to
learn K element hypotheses f i beforehand  which correspond to one-hot weights wi; i 2 [K] with
only one non-zero wi  a novel gradient boosting based approach named Adapt-Boost is proposed
for adapting the element hypotheses to the hypothesis hw corresponding to any w. We apply the
proposed objective adaptation approach to multi-label learning  and the experimental results show
that our approach achieves signiﬁcant performance for various multi-label performance measures.

p

2 Related Work

Some similarities exist between the objective adaptation scenerio and multi-objective optimization
(MOO) [Deb  2014]. Under both scenerios  multiple element objectives are considered  and the
trade-offs among them should be properly dealt with. While in MOO  the target is to ﬁgure out
the Pareto solutions reﬂecting different trade-offs instead of a single optimal solution deﬁned by the
oracle’s preference. In fact  for our objective adaptation problem  it is also possible to utilize evolu-
tionary algorithms instead of the proposed DL2M algorithm. While evolutionary algorithms are usu-
ally heuristic and theoretical guarantees are lacking. In [Agarwal et al.  2014]  the multi-objective
decision making problem is considered. The target of the learner is to optimize all objectives by
observing the actions provided by a mentor. There is a signiﬁcant difference between their setting
and ours since we focus on general learning tasks instead of decision making.
The proposed DL2M algorithm belongs to the family of continuous dueling bandit algorithms. In
p
[Yue and Joachims  2009]  an online bandit gradient descent algorithm [Flaxman et al.  2005] was
proposed  which achieves O(
In [Kumagai 
2017]  they showed that when the value function is strongly convex and smooth  their stochastic
mirror descent algorithm achieves near optimal ~O(K
T ) regret bound. Similar to DL2M   both
the above two algorithms follow from the online convex optimization framework [Zinkevich  2003] 
while DL2M assumes the underlying value function follows from a linear model. The major advan-
tage of DL2M lies on the reduction of the total number of arms needed to be sampled during learning.
For the above two algorithms  two arms are needed to be sampled for comparison in one iteration.
While DL2M samples only one arm in one iteration t  and compares it with the arm sampled on t(cid:0)1.
Thus the total number of arms needed is halved for DL2M   comparing to the above two algorithms.
For objective adaptation  choosing an arm incurs the cost of learning the corresponding hypothesis 
thus it is important to reduce the total number of arms sampled.

KT 3=4) regret bound for convex value functions.

p

2

Our boosting based hypothesis adaptation procedure is motivated from multi-task learning [Evge-
niou and Pontil  2004; Chapelle et al.  2010]. By regarding each element objective as a single task 
the hypothesis adaptation procedure can be decomposed into the adaptation of the element hypoth-
esis for each element objective  and the adaptation of a global hypothesis for the weighted total
objective. On the other hand  since the target is to optimize the single total objective  directly uti-
lizing multi-task learning approaches is invalid to our problem. The hypothesis adaptation task is
also considered in [Li et al.  2013]  in which an efﬁcient adaptation approach is proposed under
the assumption that the auxiliary hypothesis is a linear model. On the other hand  the linear model
assumption also restricts the capacity of adaptation. To address this issue  our approach utilizes a
gradient boosting based learner  which can use any weak hypothesis for adaptation  thus the opti-
mization procedure can be more ﬂexible and efﬁcient.

3 Dueling Bandit Learning for Objective Adaptation

In this section  a dueling bandit algorithm DL2M is proposed to learn the optimal weight vector w(cid:3)
from preference feedback to solve the objective adaptation task. For convenience of optimization 
we assume the arm space for DL2M is the full K-dimensional unit sphere W : ∥w∥2 = 1. How to
apply DL2M on W deﬁned in Section 1 is discussed in Remark 3 below. To model the preference of
the oracle  we assume a total order ⪯ exists on W. For w(cid:3) 2 W  we have w ⪯ w(cid:3);8w 2 W.
Whenever the oracle is given an ordered pair (w; w
)  the oracle gives the feedback of r = 1
′ ⪯ w  and r = (cid:0)1 otherwise. To precisely model the partial order and how the oracle
if w
provides the preference information  we assume that each arm can be evaluated by a value function
′ ⪯ w  and the preference feedback is generated by the
v(w)  such that v(w
′
probabilistic model considering the gap between v(w) and v(w
)) 
in which (cid:22)(x) is a strictly increasing link function. In this paper  the logistic probability model
(cid:22)(x) = 1=(1 + exp((cid:0)x)) is utilized  which is the common choice in related researches. The
In each
generation of preferences is also assumed to be independent of other parts of learning.
′
t) is submitted to the oracle for feedback. The
iteration t out of the total T iterations  a pair (wt; w
target is to minimize the total (pseudo) regret

): Pr(r = 1) = (cid:22)(v(w) (cid:0) v(w

) (cid:20) v(w)   w

′

′

′

t=1

∆T =

′
t)):

(cid:22)(v(w(cid:3)) (cid:0) v(wt)) + (cid:22)(v(w(cid:3)) (cid:0) v(w
∑
∑
′
t to be wt(cid:0)1  then we can only consider the summation over wt. Furthermore 
If further restricts w
by observing that (cid:22)(v(w(cid:3))(cid:0)v(wt)) achieves minimum 1=2 when wt = w(cid:3)  we can reformulate the
t=1 (cid:22)(v(w(cid:3)) (cid:0) v(wt)) (cid:0) 1=2.
t=1 (cid:22)(v(w(cid:3)) (cid:0) v(wt)) (cid:0) (cid:22)(v(w(cid:3)) (cid:0) v(w(cid:3))) =
regret as ∆T =
∑
From the above deﬁnition  the tasks of regret minimization and optimal weight vector estimation
(cid:0)x)(cid:0)1=2 has the same convergence rate as f (x) = x
coincide. By L’Hopital’s rule  f (x) = 1=(1+e
when x ! 0. Thus we can only consider ∆T =
t=1 v(w(cid:3)) (cid:0) v(wt). In this work  we adopt the
commonly used linear value function vLIN(w) = wT (cid:18)(cid:3)  in which (cid:18)(cid:3) is an underlying optimal
T∑
evaluation vector. This leads to the classical linear regret formulation

T

T

T

∆LIN

T =

t=1

wT(cid:3) (cid:18)(cid:3) (cid:0) wT

t (cid:18)(cid:3);

(2)

T∑

which indicates that the objective is to maximize the linear value function. Since ∥w∥2 = 1  wT (cid:18)(cid:3)
is the projection of (cid:18)(cid:3) onto w  and achieves the maximum when the directions of w and (cid:18)(cid:3) coincide.
Thus the direction of (cid:18)(cid:3) can be interpreted as the direction of the optimal weight vector kept in the
oracle’s mind. To simplify optimization  we assume ∥(cid:18)(cid:3)∥2 (cid:20) 1 without loss of generality.
From the deﬁnition of regret  it is crucial to estimate (cid:18)(cid:3) accurately. Thus we consider the procedure
of estimating (cid:18)(cid:3) in each iteration ﬁrst. Motivated by the logit one-bit bandit algorithm proposed in
[Zhang et al.  2016]  in each iteration t  we can utilize the online version of the maximum likelihood
estimator  i.e. to minimize the loss function

ft((cid:18)) = log

1 + exp

t (cid:18) (cid:0) wT

t(cid:0)1(cid:18))

(

( (cid:0) rt(wT

3

))

;

Algorithm 1 Dueling bandit Learning for Logit Model (DL2M )
1: Input Initialization (cid:18)1 = 0; Z1 = (cid:21)I; w0  number of iterations T .
2: for t = 1 to T do
3:
4:
5:

Sample (cid:17)t (cid:24) N (0; I K).
Choose (cid:20)t according to Theorem 1.
Compute ~(cid:18)t as

~(cid:18)t (cid:18)t + (cid:20)tZ

(cid:0)1=2
t

(cid:17)t:

6:

Compute wt as

wt arg max
∥w∥2=1

wT ~(cid:18)t:

Submit wt and wt(cid:0)1 and get rt.
Compute (cid:18)t+1 and Zt+1 as Equation 3 and 4.

7:
8:
9: end for

(5)

(6)

which satisﬁes the exponentially concave property. As a result  the optimal update can be approxi-
mated by the analogy of the online Newton step [Hazan et al.  2007]:

∥(cid:18) (cid:0) (cid:18)t∥2

Zt+1

+ ((cid:18) (cid:0) (cid:18)t)T∇ft((cid:18)t);

(cid:18)t+1 = min
∥(cid:18)∥2(cid:20)1

2

(3)

(4)

)

in which

Zt+1 = Zt +

(cid:12)
2

(wt (cid:0) wt(cid:0)1)(wt (cid:0) wt(cid:0)1)T ; Z1 = (cid:21)I;

2(e+1). Next  we consider how to choose wt in each round for better exploration. Different
and (cid:12) = 1
from the UCB based exploration strategy implemented in [Zhang et al.  2016]  we extend the linear
Thompson sampling technique proposed in [Abeille and Lazaric  2017] to our dueling bandit setting 
leading to the DL2M algorithm  which is illustrated in Algorithm 1. We provide regret guarantee for
the proposed algorithm  whose proof will be presented in a longer version of the paper.
Theorem 1. Assume that (cid:20)t in Algorithm 1 is set according to (cid:20)t =

√

(cid:13)t( (cid:14)

4T )  where

8
(cid:12)

32
3

2
(cid:12)

det(Zt+1)
det(Z1)

:

+

) log

+

log

√

(cid:13)t+1((cid:14)) = (cid:21) + 16 + (

(7)
After running DL2M for T rounds  then for 8(cid:14) > 0  the following result holds with probability at
)
T∑
least 1 (cid:0) (cid:14):
(

[Abbasi-Yadkori et al.  2011]  we have log(det(Zt+1)= det(Z1)) (cid:20)

wT(cid:3) (cid:18)(cid:3) (cid:0) wT
)

det(Zt+1)
det(Z1)

t (cid:18)(cid:3) (cid:20)

√

)KT log

(cid:14)
4T

+ 128

8KT

(cid:13)T (

log

:

1
(cid:21)

391

log

1
(cid:12)

4
(cid:14)

t=1

(cid:14)

2⌈2 log t⌉t2
√

(

p

(

(cid:14)

By Lemma 10 of
K log

1 + (cid:12)t
2(cid:21)K

. Thus Theorem 1 provides ~O(K 3=2

T ) regret guarantee for DL2M .

√

Remark 1 The well-known doubling trick [Shalev-Shwartz  2012] can be utilized to make (cid:20)t in-
In practice  since (cid:20)t determines the step size
dependent of the total number of iterations T .
of exploration  it is desirable to further make it ﬁne-tunable.
In all the experiments  we set
log(det(Zt)= det(Z1)))  in which c is a hyperparameter. The min operator is
(cid:20)t = min(c=2; c
introduced to control the largest step size.
Remark 2 Theorem 1 provides the guarantee of total regret. Our objective adaptation approach can
be utilized in many real-world tasks  in which the learned is already in application during the learning
stage  and the preference feedback is generated from its true effectiveness. The total regret guarantee
is natural under this situation. Meanwhile  it is also important to consider another kind of tasks  in
which only the ﬁnal estimation accuracy matters. Under this situation  it is better to consider simple
regret instead of total regret since it is a pure exploration problem. This is a particularly interesting

4

and challenging task since we assume the continuous arm space. The experimental results in Section
5 show that DL2M is efﬁcient in ﬁnding the best arms  and we leave designing the optimal pure
exploration algorithm for continuous dueling bandits a future work to investigate.
Remark 3 In the above discussion  we assume that the arm space for DL2M is W0 : ∥w∥2 = 1.
For objective learning  the parameter domains introduced in Section 1 are different. We discuss how
DL2M can be applied.
When w0 = 0  the domain of w is W : ∥w∥2 = 1;8wi (cid:21) 0  which is the nonnegative part of W0.
To apply DL2M   it is necessary to restrict each dimension of (cid:18)t; ~(cid:18)t to be nonnegative. For (cid:18)t  we
can simply change the domain of the update of (cid:18) as

(cid:18)t+1 =

min

∥(cid:18)∥2(cid:20)1;8(cid:18)i(cid:21)0;i2[K]

∥(cid:18) (cid:0) (cid:18)t∥2

Zt+1

2

+ ((cid:18) (cid:0) (cid:18)t)T∇ft((cid:18)t);

t+1

 max(0; ~(cid:18)i

in which (cid:18)i is the i-th entry of (cid:18). Since the domain remains convex  the efﬁciency of optimization
t+1);8i 2 [K] operation to limit
is unaffected. For ~(cid:18)t+1  we can simply take a ~(cid:18)i
its value. Though this operation may affect the theoretical guarantee for w near the boundary  we
observe that the performance is not affected in experiments.
When w0 = 1  the domain of w is W : ∥w (cid:0) R1K∥2 (cid:20) R. The main idea is to establish a
topologically identical mapping from the arm space to W  then we can perform DL2M in the arm
space  then map the result to W. First  it is easy to establish a bijective mapping g1 from the
K dimensional ball W1 : ∥w∥2 (cid:20) 1 to W with constant shifting and scaling. Second  another
simple bijective mapping g2 exists to map a point in half of the K + 1-dimensional sphere  i.e.
W2 : ∥w∥2 = 1; wK+1 (cid:21) 0  to a point in W1  by simply setting wK+1 = 0 (just imagine the
mapping from the upper half of the three-dimensionl sphere onto a two-dimensional circle). Thus
we can simply utilize the composite mapping g1(g2) to map an arm in W2 to a parameter in W. To
apply DL2M on W2  we can update (cid:18) by

∥(cid:18) (cid:0) (cid:18)t∥2

Zt+1

2

+ ((cid:18) (cid:0) (cid:18)t)T∇ft((cid:18)t);

∥(cid:18)∥2(cid:20)1;(cid:18)K+1(cid:21)0

(cid:18)t+1 =
min
 max(0; ~(cid:18)K+1

and perform ~(cid:18)K+1
t+1

t+1 ) to restrict both (cid:18)t+1; ~(cid:18)t+1  which is similar to w0 = 0.

4 Boosting Based Hypothesis Adaptation

After each objective adaptation step  w is updated  then a new Lw is obtained. To avoid learning
the corresponding hypothesis F w from scratch  a hypothesis adaptation procedure is considered.
Recall the formulation of objective function deﬁned in Equation 1. Assume that before the whole
objective adaptation process  we have learned K element hypotheses f i under (regularized) element
objectives li + w0(cid:3); i 2 [K]. To obtain F w corresponding to Lw  we can linearly combine f i
together with a newly learned auxiliary hypothesis ϕw  i.e. make F w =
i=1 (cid:11)if i + ϕw. As a
result  the learning problem is transformed into

∑

K

)

)

(( K∑

i=1

min

(cid:11)i;i2[K];ϕw

Lw

(cid:11)if i

+ ϕw

:

(8)

Under the above formulation  the learning target is to decide the weight (cid:11)i for each f i  together
with the auxiliary hypothesis ϕw. Intuitively  there should be a close relationship between (cid:11)i and
wi  which is the weight for li in Lw. When wi is large  the corresponding li has a large impact to the
global Lw. Since f i is learned under li + w0(cid:3)  then (cid:11)i should also be large to make the contribution
of f i in F w more signiﬁcant. For the similar reason  if wi is small then (cid:11)i should follow. As a result 
to solve Equation 8 properly  establishing a close relationship between (cid:11)i and wi is a necessary task.
Based on this motivation  a boosting based hypothesis adaptation approach named Adapt-Boost is
proposed. Assume that the learning procedure runs for N iterations. Under Adapt-Boost  one weak
hypothesis hj is learned in each iteration j. Denote by H = [h1 h2 (cid:1)(cid:1)(cid:1) hN ]T the vector of all
′;i = wi; i 2 [K]  Adapt-Boost solves the following
learned weak hypotheses and set w

′;0 = 1; w

5

Algorithm 2 Adapt-Boost
1: Input: Loss parameter w  loss function Lw  element hypotheses f i; i 2 [K]  f 0 (cid:17) 0  number

′;0 1; F0 f 0.

of iterations N  number of element losses K  step size ϵ.
′;i wi; i 2 [K]; w
Calculate current residual (cid:0)∇Lw(Fj(cid:0)1).
for i = 0 to K do
Fit residual (cid:0)∇Lw(Fj(cid:0)1) with f i + hi
end for
Choose the optimal update i

2: w
3: for j = 1 to N do
4:
5:
6:
7:
8:

j to obtain a weak hypothesis hi
j.

(cid:3) as
= arg maxi

(cid:3)

i

(cid:0)w

′;i[∇Lw(Fj(cid:0)1)](f i + hi
j):

9:

Update the current hypothesis as

Fj = Fj(cid:0)1 + (w

′;i
(cid:3)

ϵ)(f i

(cid:3)

+ hi

(cid:3)
j ):

10: end for
11: Output The learned hypothesis FN .

l1-regularized problem:

min

(cid:12)i;i2[K][f0g;H

Lw

(( K∑

)

(cid:12)k;T (1N f k + H)

k=1

+ (cid:12)0;T H

K∑

s:t:

i=0

)
(∑

;

1
w′;i

∥(cid:12)i∥1 (cid:20) (cid:22);
)

(9)

K
k=1 (cid:12)k;T (1N f k + H)

in which (cid:12)i; i 2 [K][f0g are N-dimensional weight vectors and 1N is the N-dimensional full-one
vector. Comparing to Equation 8  F w is further restricted as
+ (cid:12)0;T H 
and the auxiliary hypothesis ϕw is decomposed into K local (cid:12)k;T H corresponding to f k  together
with a global (cid:12)0;T H. For each f k  the weight (cid:11)k  which represents the importance of f k in learning
F w  is substituted by the weight vector (cid:12)k. Thus controlling the magnitude of (cid:11)k is equivalent to
′;i-weighted l1-
controlling the norm of (cid:12)k. This target is realized by introducing the sum of 1=w
norm constraints on (cid:12)i; i 2 [K] [ f0g in Equation 9 with a hyperparameter (cid:22) controlling the global
′;k; k 2 [K]  we are able to
sparsity. Meanwhile  by controlling the local sparsity of each (cid:12)k using w
relate the importance of f k in F w with objective weights w.
The key advantage to employ Equation 9 is that this sparsity-constrained problem can be solved by
the ϵ-boost algorithm [Rosset et al.  2004]  which will be brieﬂy introduced below. To simplify nota-
tion  we use (cid:12) to denote the vector which is the concatenation of all (cid:12)i; i 2 [K] [ f0g. Temporarily 
we also assume that the weak hypotheses H are ﬁxed  and only (cid:12) needs to be optimized. Instead of
explicitly setting the sparsity level (cid:22)  we decompose the sparsity constraint over all steps. In each
iteration  a small increment ∆(cid:12) is added on (cid:12)  and an ϵ-sparsity constraint is applied on ∆(cid:12)  leading
to the following inside-iteration optimization problem:

Lw((cid:12) + ∆(cid:12));

s:t:

min
∆(cid:12)

∥∆(cid:12)i∥1 (cid:20) ϵ;

1
w′;i

(10)

K∑

i=0

in which ∆(cid:12)i is the part of ∆(cid:12) added on (cid:12)i. The objective function can be approximated as

Lw((cid:12) + ∆(cid:12)) (cid:25) Lw((cid:12)) + [∇Lw((cid:12))]T ∆(cid:12)

∑

(11)
by Taylor expansion. Thus we turn to minimize [∇Lw((cid:12))]T ∆(cid:12). Since the sparsity constraints are
gradually added by ϵ over the learning process  and Lw is convex  the optimal solution for Equation
j; i 2 [K] [ f0g; j 2 [N ] be the
10 always satisﬁes
(N i + j)-th dimension of ∇Lw((cid:12)) and ∆(cid:12). It is easy to see that the optimal ∆(cid:12) in Equation 11
′;i[∇Lw((cid:12))]i
(cid:3)
(cid:3)
is a vector of all zeros except for [∆(cid:12)]i
j.
j(cid:3) = w
Furthermore  we can explicity write the (N i + j)-th component of ∇Lw((cid:12)) as @Lw=@(cid:12)i
j =
j) = [∇Lw(F w)](f i + hj)  in which hj is the j-th weak hypothesis in
[∇Lw(F w)](@F w=@(cid:12)i

w′;i∥∆(cid:12)i∥1 = ϵ. Let [∇Lw((cid:12))]i

= arg mini;j w

ϵ such that i

j; [∆(cid:12)]i

(cid:3)

; j

K
i=0

1

′;i
(cid:3)

6

(cid:0)w

j ﬁt for the residual (cid:0)∇Lw(F w)  and then we can choose the optimal update f i

H  and an additional f 0 (cid:17) 0 is introduced to simplify notation. Now let us take choosing weak hy-
potheses H into consideration. Based on the above discussion  in the j-th iteration  our target is to
′;i[∇Lw(F w)](f i + hj). This formulation inspires us to utilize gradient boosting.
solve maxi;hj
j is chosen for each i; i 2 [K] [ f0g to
To obtain the optimal update  a candidate weak hypothesis hi
(cid:3)
let f i + hi
+ hi
j
′;i
(cid:3)
which optimally ﬁts the residual weighted by w
ϵ
according to the previous discussion. The whole process of Adapt-Boost is illustrated in Algorithm
2. It can be seen that Adapt-Boost utilizes a boosting based process to gradually add the element
and weak hypotheses into the learned hypothesis instead of explicitly setting their weights. Thanks
to the ﬂexability of choosing the weak learners and the efﬁciency of gradient boosting  we are able
to solve complex hypothesis adaptation problems with low cost.

′;i. The optimal step size for the update is w

(cid:3)

(a) K = 10; c = 0:1

(b) K = 10; c = 0:05

(c) K = 10; c = 0:01

(d) K = 100; c = 0:1

(e) K = 100; c = 0:05

(f) K = 100; c = 0:01

Figure 1: Instantaneous regret of DL2M .

7

50100150200t00.20.40.60.81Instantaneous Regret50100150200t00.20.40.60.81Instantaneous Regret50010001500200025003000t00.20.40.60.81Instantaneous Regret12345t10400.20.40.60.81Instantaneous Regret12345t10400.20.40.60.81Instantaneous Regret12345t10400.20.40.60.81Instantaneous Regret5 Experiments

5.1 Testing DL2M on Synthetic Data

)

( (cid:0) (cid:26)rt(wt (cid:0) wt(cid:0)1)T (cid:18)(cid:3)

We present experimental results on synthetic data to verify the effectiveness of DL2M . In each
experiment  a K dimensional point is uniformly sampled from the unit ball as (cid:18)(cid:3). Once the learner
submits the pair of arms (wt; wt(cid:0)1)  a preference feedback rt 2 f(cid:0)1; 1g is randomly generated
according to Pr(rt = (cid:6)1j(wt; wt(cid:0)1)) = 1=(1 + exp
)  in which (cid:26) is the
parameter controlling the randomness of the preferences. In all experiments  we use (cid:26) = 100 to
ensure the preferences are relatively consistent. We also set (cid:21) = 1 in all the experiments. The per-
formance is measured by the change of instantaneous regret wT(cid:3) (cid:18)(cid:3) (cid:0) wT
t (cid:18)(cid:3) over time. We compare
the results among different c and K  which are illustrated in Figure 1. It can be observed that when
the parameter c is properly set and K is not large  DL2M achieves very efﬁcient performance  which
can quickly converge in limited number of iterations. As the dimension gets larger  the performance
degenerates accordingly. To verify the efﬁciency of Thompson sampling in dueling and one-bit
bandit problems  it is interesting to compare the above results with those reported in [Zhang et al. 
2016]  which utilizes UCB based exploration. It can be seen that Thompson sampling can achieve
more efﬁcient performance in practice as in many other bandit problems.

5.2 Multi-Label Performance Measure Adaptation

The multi-label classiﬁcation task is utilized to evaluate the effectiveness of DL2M and Adapt-Boost 
both separately and jointly.
It is well-known that for multi-label learning  various performance
measures exist  and the choice of the performance measure will largely affect the evaluation of
the learned classiﬁer. In [Wu and Zhou  2017]  two notions of multi-label margin  i.e. label-wise
and instance-wise margins are proposed to characterize different multi-label performance measures.
According to their work  one speciﬁc multi-label performance measure tends to be biased towards
one of the two margins  such that optimizing the corresponding margin will also make the measure
optimized. Based on this ﬁnding  the stochastic gradient descent (SGD) based LIMO algorithm is
proposed to jointly optimize the both margins  in order to achieve good performance on different
measures simultaneously. The high-level formulation of LIMO’s objective is

LLIM O = (cid:3) + w1Llabel + w2Linst;

(12)
in which Llabel; Linst are two margin loss terms for maximizing the two margins  and (cid:3) is a reg-
ularization term. For LLIM O  the weights w1; w2 control the relative importance of the two loss
terms  thus different choices of the weights can signiﬁcantly affect the performance. We will show
that by utilizing DL2M and Adapt-Boost  we can automatically ﬁnd the proper weights between the
two margin losses in LIMO’s objective  and efﬁciently adapt to different performance measures.
The experiments are conducted on six benchmark multi-label datasets 1: emotions  CAL500  enron 
Corel5k  medical and bibtex. On each dataset  four multi-label performance measures are adopted
for evaluation  i.e. ranking loss  coverage  average precision and one error. Three LIMO based
comparison methods are adopted as baselines: (i) LIMO-label  which optimizes LLIM O with w1 =
1; w2 = 0  (ii) LIMO-inst  which optimizes LLIM O with w1 = 0; w2 = 1  (iii) LIMO  which
optimizes LLIM O with w1 = 1; w2 = 1 and corresponds to the recommended parameter in the
original paper. To evaluate DL2M and Adapt-Boost both separately and jointly  three adaptation
based approaches are tested: (i) ADAPT-hypo  which optimizes LLIM O with w1 = 1; w2 = 1
using Adapt-Boost  (ii) ADAPT-obj  which utilizes DL2M with SGD training  (iii) ADAPT-both 
which utilizes both DL2M and Adapt-Boost. To implement DL2M   each dataset is randomly split
into training  validation and testing set  with ratio of size 3:1:1. During the learning process  the
preference feedback is generated by testing the learned hypothesis on the validation set  and DL2M
is utilized to update the objective for 20 iterations  with c = 0:05; (cid:21) = 1. For Adapt-Boost  to
evaluate its efﬁciency  we only use half number of training iterations than standard LIMO training.
Furthermore  to make Adapt-Boost compatible to LIMO training  the SGD updates are utilized as
the weaker learners for adaptation.
The experimental results are illustrated in Table 1  and the average ranks in all experiments are
illustrated in Table 2. It can be seen that DL2M based method ADAPT-obj achieve better perfor-
mance than LIMO  which assigns ﬁxed weights to the two margin losses. This phenomenon reveals

1http://mulan.sourceforge.net/datasets-mlc.html

8

that DL2M can automatically identify the best trade-off among different element objectives. Further-
more  though running with much fewer training iterations  Adapt-Boost based method ADAPT-hypo
achieves even better performance than LIMO  which is based on standard SGD training. This veriﬁes
the efﬁciency of Adapt-Boost. ADAPT-both  which utilizes both two adaptation methods  achieves
superior performance. It shows that by utilizing DL2M and Adapt-Boost  we can effectively solve
the objective and hypothesis adaptation problem better and faster.

Dataset

emotions

CAL500

enron

Corel5k

medical

bibtex

Algorithm
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both
LIMO-inst
LIMO-label

LIMO

ADAPT-hypo
ADAPT-obj
ADAPT-both

ranking loss #
:420 (cid:6) :051(6)
:349 (cid:6) :028(5)
:299 (cid:6) :023(4)
:279 (cid:6) :026(3)
:268 (cid:6) :033(2)
:254 (cid:6) :020(1)
:522 (cid:6) :026(6)
:182 (cid:6) :005(2)
:182 (cid:6) :004(2)
:182 (cid:6) :005(2)
:182 (cid:6) :005(2)
:181 (cid:6) :004(1)
:229 (cid:6) :010(6)
:087 (cid:6) :009(3)
:089 (cid:6) :009(5)
:087 (cid:6) :009(3)
:086 (cid:6) :009(2)
:085 (cid:6) :008(1)
:302 (cid:6) :006(6)
:121 (cid:6) :005(3)
:130 (cid:6) :004(5)
:121 (cid:6) :005(3)
:118 (cid:6) :005(2)
:114 (cid:6) :005(1)
:019 (cid:6) :005(2)
:028 (cid:6) :004(6)
:020 (cid:6) :005(4)
:021 (cid:6) :004(5)
:019 (cid:6) :004(2)
:018 (cid:6) :004(1)
:120 (cid:6) :003(6)
:072 (cid:6) :003(5)
:060 (cid:6) :002(4)
:056 (cid:6) :002(1)
:057 (cid:6) :003(3)
:056 (cid:6) :002(1)

coverage #

2:950 (cid:6) :134(6)
2:745 (cid:6) :174(5)
2:483 (cid:6) :070(4)
2:331 (cid:6) :090(2)
2:377 (cid:6) :144(3)
2:298 (cid:6) :200(1)
162:950 (cid:6) 2:417(6)
131:439 (cid:6) 1:764(5)
131:020 (cid:6) 1:697(2)
131:297 (cid:6) 1:899(4)
131:088 (cid:6) 1:849(3)
131:008 (cid:6) 2:072(1)
25:166 (cid:6) :957(6)
12:362 (cid:6) :612(5)
12:199 (cid:6) :625(4)
12:060 (cid:6) :648(2)
12:049 (cid:6) :624(1)
12:066 (cid:6) :577(3)
188:785 (cid:6) 3:122(6)
106:920 (cid:6) 2:457(5)
104:465 (cid:6) 2:149(4)
101:668 (cid:6) 2:141(3)
100:478 (cid:6) 3:376(2)
98:880 (cid:6) 2:989(1)
1:781 (cid:6) :337(5)
2:326 (cid:6) :489(6)
1:563 (cid:6) :249(3)
1:621 (cid:6) :246(4)
1:499 (cid:6) :340(2)
1:447 (cid:6) :288(1)
32:751 (cid:6) 1:144(6)
20:460 (cid:6) :515(5)
17:648 (cid:6) :596(4)
16:708 (cid:6) :430(1)
17:119 (cid:6) :832(2)
17:128 (cid:6) :590(3)

avg. precision "
:603 (cid:6) :028(6)
:619 (cid:6) :025(5)
:648 (cid:6) :028(4)
:671 (cid:6) :032(3)
:673 (cid:6) :033(2)
:678 (cid:6) :028(1)
:153 (cid:6) :010(6)
:496 (cid:6) :006(5)
:498 (cid:6) :006(1)
:497 (cid:6) :007(2)
:497 (cid:6) :007(2)
:497 (cid:6) :008(2)
:504 (cid:6) :017(6)
:672 (cid:6) :014(4)
:670 (cid:6) :014(5)
:680 (cid:6) :013(2)
:675 (cid:6) :012(3)
:683 (cid:6) :016(1)
:101 (cid:6) :006(6)
:281 (cid:6) :005(1)
:222 (cid:6) :006(5)
:252 (cid:6) :006(3)
:247 (cid:6) :010(4)
:280 (cid:6) :007(2)
:857 (cid:6) :020(5)
:830 (cid:6) :020(6)
:869 (cid:6) :026(2)
:863 (cid:6) :023(4)
:874 (cid:6) :021(1)
:866 (cid:6) :025(3)
:488 (cid:6) :007(6)
:526 (cid:6) :007(5)
:567 (cid:6) :007(4)
:579 (cid:6) :007(2)
:575 (cid:6) :006(3)
:581 (cid:6) :006(1)

one-error #

:500 (cid:6) :047(6)
:509 (cid:6) :064(5)
:498 (cid:6) :057(4)
:481 (cid:6) :048(3)
:478 (cid:6) :062(2)
:465 (cid:6) :058(1)
:971 (cid:6) :019(6)
:099 (cid:6) :026(2)
:131 (cid:6) :053(5)
:128 (cid:6) :036(4)
:098 (cid:6) :028(1)
:107 (cid:6) :024(3)
:350 (cid:6) :043(6)
:235 (cid:6) :024(1)
:246 (cid:6) :029(3)
:246 (cid:6) :022(3)
:251 (cid:6) :022(5)
:242 (cid:6) :027(2)
:893 (cid:6) :007(6)
:718 (cid:6) :017(1)
:793 (cid:6) :012(5)
:762 (cid:6) :014(3)
:772 (cid:6) :013(4)
:719 (cid:6) :016(2)
:192 (cid:6) :034(5)
:216 (cid:6) :037(6)
:163 (cid:6) :028(1)
:181 (cid:6) :031(4)
:171 (cid:6) :034(2)
:176 (cid:6) :036(3)
:486 (cid:6) :018(6)
:440 (cid:6) :018(5)
:395 (cid:6) :014(4)
:384 (cid:6) :013(2)
:391 (cid:6) :013(3)
:383 (cid:6) :013(1)

Table 1: Experimental results for the adaptation based and LIMO based methods. For each measure 
“#” indicates “the smaller the better” and “"” indicates “the larger the better”. The results are shown
in mean(cid:6)std(rank) format calculated from ten repeated experiments. The rank is calculated from
the mean. The smaller the rank  the better the performance. The ﬁrst-ranked results are bolded.

Algorithm LIMO-inst
avg. rank

LIMO-label

5:71
Table 2: The average performance rank in all experiments.

4:21

2:83

LIMO ADAPT-hypo ADAPT-obj ADAPT-both
3:67

2:42

1:58

6 Conclusion and Future Work

In this work  the preference based objective adaptation task is studied. The DL2M algorithm is
proposed under this setting  which can efﬁciently solve the objective adaptation problem based on
the dueling bandit model. For better hypothesis adaptation  the Adapt-Boost method is proposed in
order to adapt the pre-learned element classiﬁers to the new objective with low cost.
To further investigate the objective adaptation problem  it is possible to relax the linear combinaiton
formulation of the objective function adopted in this work. We are also interested in applying the
proposed approaches in other real-world problems  especially the tasks in which human expert feed-
back can be utilized. Furthermore  it is also interesting to investigate Adapt-Boost on problems with
larger scale  as well as to study its theoretical guarantees.

9

Acknowledgement

This research is supported by National Key R&D Program of China (2018YFB1004300)  NSFC
(61751306) and Collaborative Innovation Center of Novel Software Technology and Industrializa-
tion. Yao-Xiang Ding is supported by the Outstanding PhD Candidate Program of Nanjing Univer-
sity. The Authors would like to thank the anonymous reviewers for constructive suggestions  as well
as Lijun Zhang  Ming Pang  Xi-Zhu Wu and Yichi Xiao for helpful discussions.

References
Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In NIPS  pages 2312–2320  2011.

Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In AISTATS  pages

176–184  2017.

Alekh Agarwal  Ashwinkumar Badanidiyuru  Miroslav Dudik  Robert E Schapire  and Aleksandrs
Slivkins. Robust multi-objective learning with mentor feedback. In COLT  pages 726–741  2014.

Olivier Chapelle  Pannagadatta Shivaswamy  Srinivas Vadrevu  Kilian Weinberger  Ya Zhang  and
Belle Tseng. Multi-task learning for boosting with application to web search ranking. In KDD 
pages 1189–1198  2010.

Kalyanmoy Deb. Multi-objective optimization. In Search methodologies  pages 403–449. Springer 

2014.

Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In KDD  pages

109–117  2004.

Abraham D Flaxman  Adam Tauman Kalai  and H Brendan McMahan. Online convex optimization

in the bandit setting: gradient descent without a gradient. In SODA  pages 385–394  2005.

Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  69(2-3):169–192  2007.

Wataru Kumagai. Regret analysis for continuous dueling bandit. In NIPS  pages 1488–1497  2017.

Nan Li  Ivor W Tsang  and Zhi-Hua Zhou. Efﬁcient optimization of performance measures by

classiﬁer adaptation. IEEE TPAMI  35(6):1370–1382  2013.

Saharon Rosset  Ji Zhu  and Trevor Hastie. Boosting as a regularized path to a maximum margin

classiﬁer. JMLR  5(Aug):941–973  2004.

Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R⃝

in Machine Learning  4(2):107–194  2012.

Xi-Zhu Wu and Zhi-Hua Zhou. A uniﬁed view of multi-label performance measures.

pages 3780–3788  2017.

In ICML 

Yisong Yue and Thorsten Joachims.

Interactively optimizing information retrieval systems as a

dueling bandits problem. In ICML  pages 1201–1208  2009.

Yisong Yue  Josef Broder  Robert Kleinberg  and Thorsten Joachims. The k-armed dueling bandits

problem. JCSS  78(5):1538–1556  2012.

Lijun Zhang  Tianbao Yang  Rong Jin  Yichi Xiao  and Zhi-hua Zhou. Online stochastic linear

optimization under one-bit feedback. In ICML  pages 392–401  2016.

Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In

ICML  pages 928–936  2003.

10

,Yao-Xiang Ding
Zhi-Hua Zhou