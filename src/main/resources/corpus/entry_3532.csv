2019,MonoForest framework for tree ensemble analysis,In this work  we introduce a new decision tree ensemble representation framework: instead of using a graph model we transform each tree into a well-known polynomial form. We apply the new representation to three tasks: theoretical analysis  model reduction  and interpretation. The polynomial form of a tree ensemble allows a straightforward interpretation of the original model. In our experiments  it shows comparable results with state-of-the-art interpretation techniques. Another application of the framework is the ensemble-wise pruning: we can drop monomials from the polynomial  based on train data statistics. This way we reduce the model size up to 3 times without loss of its quality. It is possible to show the equivalence of tree shape classes that share the same polynomial. This fact gives us the ability to train a model in one tree's shape and exploit it in another  which is easier for computation or interpretation. We formulate a problem statement for optimal tree ensemble translation from one form to another and build a greedy solution to this problem.,MonoForest framework for tree ensemble analysis

Igor Kuralenok

Yandex / JetBrains Research
solar@yandex-team.ru

Vasily Ershov

Yandex

noxoomo@yandex-team.ru

Yandex / Saint Petersburg campus of National Research University Higher School of Economics

Igor Labutin

Labutin.IgorL@gmail.com

Abstract

In this work  we introduce a new decision tree ensemble representation framework:
instead of using a graph model we transform each tree into a well-known polyno-
mial form. We apply the new representation to three tasks: theoretical analysis 
model reduction  and interpretation. The polynomial form of a tree ensemble
allows a straightforward interpretation of the original model. In our experiments 
it shows comparable results with state-of-the-art interpretation techniques. An-
other application of the framework is the ensemble-wise pruning: we can drop
monomials from the polynomial  based on train data statistics. This way we reduce
the model size up to 3 times without loss of its quality. It is possible to show the
equivalence of tree shape classes that share the same polynomial. This fact gives
us the ability to train a model in one tree’s shape and exploit it in another  which
is easier for computation or interpretation. We formulate a problem statement for
optimal tree ensemble translation from one form to another and build a greedy
solution to this problem.

1

Introduction

Industry and science combined efforts in the ﬁeld of machine learning give us powerful techniques
and tools to solve different kinds of supervised learning tasks. Just a few lines of code could train
a model that solves classiﬁcation  regression  ranking  and other problems. Modern techniques 
like deep neural networks He et al. (11) learn complex models  but for many practical applications
we need to understand why and how the model makes a prediction. This knowledge allows us to
improve quality  protect from adversarial attacks  make a model resistant to data corruptions and
so on. Recent efforts in deep learning models interpretation allow us to understand the decision for
particular examples Ribeiro et al. (26); Shrikumar et al. (27); Štrumbelj  Kononenko (31); Koh  Liang
(15); Lundberg  Lee (19)  but understanding the logic behind a complex model is still a challenging
task.
Unlike neural networks  decision trees are supposed to be easy to understand  which is true in case of
shallow trees  but becomes a complicated task if we start using ensembles or increase the depth of a
tree. Ensemble methods  especially gradient boosted decision trees  show state-of-the-art results on
structured and categorical data Prokhorenkova et al. (25). For well-engineered input features decision
tree ensembles signiﬁcantly outperform deep networks: in competitions held by Kaggle ensemble
models built by GBDT libraries often outperform their rivals.
One way to work with the interpretation problem is to make a clearer representation of a model or an
optimization setup. An example of such perspective change is (4): the results of this paper allow us
to use known facts and intuitions from differential equations for neural networks analysis.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Our work proposes a new framework for tree ensemble representation  learned by techniques like
Random Forest (2) or Gradient Boosted Decision Tree Friedman (9). We call it MonoForest as sum
of trees is converted to forest of monomials. The proposed framework can be used for both theoretical
tree ensemble analysis and practical enhancements of existing methods. The main contributions of
this paper are:

• an algorithm for tree ensemble conversion to polynomial form;
• a proof of uniqueness of the polynomial form for all shapes of trees that have the same

values;

oblivious trees;

• an algorithm for conversion of polynomial ensemble representation to sum of symmetric

• an ensemble-wise pruning algorithm and its experimental study;
• a method for global feature attribution  based on polynomial form of tree ensemble.

The rest of the paper is organized as follows: section 3 introduces the new framework; the next three
sections study applications of the proposed framework: theoretical analysis is in section 4  model
reduction is in section 5  and interpretation is in section 6. Finally  section 7 summarizes future work
and presents our conclusions.

ensemble  e.g. H(x) =(cid:80)T

2 Notation
At ﬁrst  let us introduce a notation that will be used throughout the paper. I{∗} denotes the indicator
function  i.e.  I{condition} is equal to one if the condition is true and zero otherwise. The vector
of input features will be denoted by x ∈ Rn. We assume that the decision tree ensemble is ﬁxed 
therefore there are a ﬁxed number of minimal possible split rules C. For the rules we will use the
following notations: c(x) ∈ {0  1} as a right split indicator function  i.e. c(x) = I{xi(c) > b(c)} 
where i(c) is an index of feature to split  and b(c) is a condition used; associated left split indicator 
I{xi(c) ≤ b(c)}  is equal to 1− c(x). We can group conditions by feature: Ck = {c ∈ C : i(c) = k}
and order them by condition value: cki  ckj ∈ Ck  b(cki) < b(ckj ≤ bk) ⇔ i < j with maximal
condition bk for each feature. H is used for the entire ensemble  and ht for the components of the
t=1 ht(x); for the ﬁxed decision tree we use d to denote depth of the tree.
By 2C we denote the set of all possible products of right split indicator functions; elements of this set
we call monomials and denote by M.
Popular boosting libraries have different growing policies for decision tree induction. All these
policies use a greedy algorithm to search for a tree  but in a slightly different manner. As a result 
libraries generate trees of different shape:
XGBoost Chen  Guestrin (5)  by default  use a level-wise policy. Trees are built level by level  on
each iteration each leaf is independently split by condition with the best gain. Optionally  the tree is
prunned after training. This policy generates ensemble with balanced trees of ﬁxed depth.
LightGBM Ke et al. (13) uses a loss-guide policy. This policy on each step of tree induction splits
only a single leaf  one with the best gain. As a result  trees are usually highly imbalanced and deep.
CatBoost (25) uses a level-wise policy  similar to one used in XGBoost  but with one more restriction:
CatBoost searches for just one single condition to split all leaves simultaneously. Thus  the result is 
effectively  a decision table. Such trees are also called oblivious trees (16).
Throughout this paper we will refer to trees  generated by XGBoost and LightGBM as non-symmetric
trees  and to trees  generated by CatBoost  as symmetric.

3 Tree ensemble as polynomial

The core idea of MonoForest is quite simple: we transfer a single tree to algebraic form and then
minimize the entire ensemble  using properties of a tree representation. Decision tree splits the
features space into non-intersecting areas (leaves l) where a single constant or vector is used to

2

Figure 1: Core idea of MonoForest framework

(cid:88)

l∈leaves

(cid:89)

(cid:88)

M∈2C

(cid:89)

(cid:89)

c∈M

specify leaf property. It is possible to express this statement in algebraic form:

h(x) =

wlI{x ∈ l}

(1)

Each leaf indicator function is a product of indicators induced by splits along the path from root to
terminal node:

I{x ∈ l} =

c(x)

c∈right splits

c∈left splits

(1 − c(x))

(2)

Thus  replacing leaf indicator from equation (1) with equation (2) and expanding brackets  we will
obtain tree in polynomial form:

ht(x) =

wMt

c(x)

(3)

(cid:81)

Figure 1 illustrates the idea of the transformation. The atomic parts of the representation
(wM
c∈M c(x)) we refer to as monomials through the rest of this paper. The proposed repre-
sentation of the tree has three valuable properties:

1. the values wM t are deﬁned by points  satisfying all conditions of the monomial;
2. conditions in a single monomial are based on different features;
3. it is possible to reorder conditions inside monomials.

The ﬁrst property seems to be obvious  but it has interesting consequences: the set of points that
deﬁne the value wM reduces when the number of conditions increases and the value itself becomes
noisy; for all trees in the ensemble values wMt are dependant on the exact same set of points and
consequently depend on each other. The level of dependency grows with the number of conditions in
M 1. In extreme case  conditions from M can split a point from the others and all wMt are deﬁned by
this single point.
The second property comes from the fact that stronger conditions devours weaker ones: if two
conditions of the same monomial are based on the same feature  their borders are always ordered
and we can remove the weakest from the monomial. For example  assume leaf is a product of
two indicator functions I{x0 > 0}I{x0 ≤ 1}. This is equal to I{x0 > 0}(1 − I{x0 > 1}) =
I{x0 > 0} − I{x0 > 0}I{x0 > 1} = I{x0 > 0} − I{x0 > 1}. I{x0 > 1} is a stronger condition
in the second monomial  thus I{x0 > 0} could be removed. This property limits the number of

(cid:1)(maxk |Ck|)i  which is signiﬁcantly smaller then the naive estimate
(cid:1). Please note  that despite this fact  for simpliﬁcation of notation we are using 2C as set of
(cid:0)|C|

possible monomials to(cid:80)d
(cid:80)d

possible monomials.
The third property allows optimizing model application time. For example  if there are two conditions
that are met in the result model together and one condition c1 is much more computationally difﬁcult

(cid:0)n

i

i=1

i

i=1

1MonoForest decomposition is similar to n-way ANOVA decomposition  where dependence for factors
x  y  z is decomposed to main effects (x  y  z) and their iterations (xy  yz  xyz). In ANOVA x  y  z are
categorical factors  while in MonoForest it is right split indicator functions.

3

(e.g. taken from a database) than the other c2  we can skip its calculation completely for examples x
such that c1(x) = 0.
Summation over elements of ensemble gives us the ﬁnal form of a decision function:

H(x) =

wMt

c(x) =

wM

c(x)

(4)

(cid:32) T(cid:88)

(cid:88)

M∈2C

t=1

(cid:33) (cid:89)

c∈M

(cid:88)

M∈2C

(cid:89)

c∈M

This transformation alone is able to reduce the number of monomials comparing the leaves count in
the original model due to the grouping of monomials from different trees.
To the best of our knowledge  this form was not studied before in the literature. The polynomial form
is easier to analyze  interpret and it provides a way to work with different tree shapes in an uniﬁed
manner.

Implementation and conversion complexity Polynomial conversion could be done in a straight-
forward recursive way: we could extract leaves from the tree one-by-one. Each leaf produces at most
2d monomials  where d is the depth of this leaf. This conversion could be done in O(|L|2d)  where
|L| denotes the number of leaves in the decision tree and d is maximum depth.

4 Theoretical analysis

Decision tree equivalence  normalization  and minimal representation are known problems in statistics
Lavalle  Fishburn (17) and computer science Zantema  Bodlaender (30). The proposed framework
gives one more way to view these problems: we made a decomposition of complex decision function
onto atomic decision factors with a certain degree of features interaction. In this section we use this
property to set up a task of tree shape change in ensemble. We represent a tree ensemble as a sum of
trees of ﬁxed shape and minimize the length of this sum. To be able to declare the existence of such
transfer we need to deﬁne equivalence class of tree ensembles with the following theorem:
Theorem 1. Two tree ensembles H and H(cid:48) are deﬁned on Rn2  have the same value for all possible
points ∀x ∈ Rn  H(x) = H(cid:48)(x)  iff: 1) their set of conditions C and C(cid:48) are equal  2) they have
equal polynomial representation ∀M ∈ 2C  wM = w(cid:48)

M

j = x(cid:48)(cid:48)

i(c) = b(c) +  and  x(cid:48)

Proof. The reverse part of the proof is obvious. To prove the direct ﬁrst proposition suppose  without loss of
generality  that there is a condition c ∈ C  c /∈ C(cid:48). We can take x(cid:48)  x(cid:48)(cid:48) ∈ X such that H(x(cid:48)) (cid:54)= H(x(cid:48)(cid:48))  x(cid:48)
i(c) =
j   j (cid:54)= i(c) because C is a minimal conditions set for ensemble H  otherwise
b(c)  x(cid:48)(cid:48)
there is no such pair of points in X that are split by c. Thus c can be excluded from C. We can ﬁnd  such that
C(cid:48) won’t be able to split x(cid:48) from x(cid:48)(cid:48) because it does not contains c  consequently H(cid:48)(x(cid:48)) = H(cid:48)(x(cid:48)(cid:48)). This is
contradiction of the initial statement.
Conditions sets are equal and we need to show that ∀M  wM = w(cid:48)
∅ and we
need to prove the induction step by size of M and going from lowest to highest condition values for each of
feature in set. For each features combination CM = {i(c)|c ∈ M} we start from the lowest conditions set
Mi = M0 : c ∈ M0 ⇒ b(c) = ci(c)0 and proceed to highest ¯M : c ∈ ¯M ⇒ b(c) = bi(c) rising single
condition at a time. For this monomial Mi choose x(cid:48)  x(cid:48)(cid:48) ∈ Rn such that:

M . It is clear that w∅ = w(cid:48)

• for all coordinates outside CM they both have value of minimal bound for this feature: x(cid:48)
• for all coordinates from c ∈ Mi: x(cid:48)

k = b(c) + 

k = x(cid:48)(cid:48)

k = x(cid:48)(cid:48)

k = ck0 

0 = H(x(cid:48)) − H(x(cid:48)(cid:48)) − (H(cid:48)(x(cid:48)) − H(cid:48)(x(cid:48)(cid:48))) =(cid:80)i

For these points the difference between values of ensembles comes only from monomials from features set CM :
Mj   because all monomials of lesser length are
supposed to be equal in H and H(cid:48)  monomial of length q with higher conditions (j > i) are zero by construction
of x(cid:48) and x(cid:48)(cid:48). At ﬁrst step there is the only element left in the right part of the equation: 0 = wM0 − w(cid:48)
M0 and
wM0 = w(cid:48)
Mi.

M0  constructing pair of x at each step of induction it is easy to show  that all wMi = w(cid:48)

j=0 wMj − w(cid:48)

2To save the space we formulated the theorem in Rn task space  this limitation can be easily eliminated  but

the proof will be longer.

4

Figure 2: Oblivious tree conversion to polynomial form

The direct implication of the theorem is that ensembles of trees that are able to generate the same
set of monomials (e.g. of the same depth) are equivalent. Theoretically  this allows us to claim
equivalence of the decisions set used by libraries with different shapes of atomic trees. Polynomial
form determines the tree ensemble  but it is possible to convert the polynomial form back to a tree
ensemble of a different form. This task could be formalized in the following way:

(cid:32)
H(x) − T(cid:88)

(cid:33)2 + λT

(5)

{h∗

t}T
t=1 = arg min
T ht∈H

Ex∼X

ht(x)

t=1

where H is the set of trees of a certain shape and λ—regularization parameter. This set optimization
is clearly NP-hard: there is a set cover problem under the shell3. On the other hand  the target
of the optimization is submodular  so it is possible to ﬁnd an approximately optimal solution in
polynomial time Bach (1). Unfortunately  set optimization algorithms often become impractical
because of their computational difﬁculty  and we decided to start from a simple greedy algorithm 
leaving the mathematically sound version for future work. We have ﬁxed the tree shape to symmetric
oblivious trees. This type of trees is trivial in computations and it is possible to speed up decision
function computation if we could convert tree ensembles of arbitrary shapes to them. From a practical
perspective  the resolution of this task allows us to train easier for training but heavier model and
then  transfer the result solution to some lightweight form  easier for exploitation.
The polynomial form of the ensemble gives the idea of a greedy algorithm for such optimization:
at each step  we eliminate monomial with the greatest number of features in it. Using the fact that
oblivious trees have a single monomial of maximum length (see Figure 2)  the task becomes much
easier. The resulting algorithm is presented in Algorithm 1.
There is no universal strategy for tree induction: optimal one changes with dataset at hand. On the
other hand  symmetric trees outperform other tree shapes: they are coded by a decision table  thus
evaluation for one tree requires several bit-wise operations and one look-up in the table and takes
>10x less time for the same number of trees Dorogush et al. (6).
For our experiment we have used model built by LightGBM for Higgs dataset4  transferred it to an
ensemble of symmetric trees by Algorithm 1 and then compared model execution times of the original
model and transformed version. The time we need to apply the original model with fast CatBoost
calculator for non-symmetric trees was 2.57 seconds; the transformed ensemble was applied in 1.57
seconds5. This gives us 40% speedup free of charge.

5 Model reduction

Decision tree pruning has been studied for decades: critical values  error complexity and reduced
error methods (24)  more recent bottom-up methods (14)  minimum description length based methods
(23) and others. Ensemble-wise pruning is less studied. Several works were done on the pruning of
trained ensemble. (21)  (22) trained a random forest ensemble  later using boosting to prune it. On
each boosting iteration  tree search spaces were restricted to only those that were part of a random

3We need to cover all monomials by polynomials of the ﬁxed shape.
4LightGBM provides the best quality model for this dataset
5For experiment we have used dual-socket server with Intel Xeon CPU E5-2650 and 256GB of RAM.

CatBoost version was equal to 0.14.2

5

Result: Symmetric tree ensemble H(x) =(cid:80)T

Data: Monomials M1 . . . MN   monomial weights W1  . . . WN

t=1 H[t](x)
def SymmetricTree(M: monomial features  W : weight):

return oblivious tree  generated by monomial M with weights W ;

def IsSubset(M : monomial features   h : tree):

return True if features from M are subset of feature of tree h;

def AddMonomialToTree(M : monomial features   W : weight  h : tree):

Add monomial M with weight W to tree h ;

H = [];
T = 0;
for i ∈ 1 . . . N do

if ∃t ∈ 0 . . . T : IsSubset(Mi  H[t]) = True then

/* Tree has the same split conditions
AddMonomialToTree(Mi  Wi  H[t]);

H[T ] = SymmetricTree(Mi  Wi);
T = T + 1;

else

Return(cid:80)T

end

t=1 H[t] ;

Algorithm 1: Greedy ensemble composition algorithm.

*/

M

(cid:0)w2

c∈M c(x)(cid:1). This approach is
(cid:81)

forest. Kappa pruning Margineantu  Dietterich (20)  and modiﬁcation of this technique (29)  were
proposed to prune AdaBoost ensembles. Those heuristic techniques rely on the assumption that
gradient boosting builds an ensemble of weak classiﬁers. Greedy strategies are then used to select the
most diverse sample of them. To the best of our knowledge  those methods are not used in practice
today. In practice  pruning is done on a per-tree basis  and early stopping strategies are used to select
the optimal ensemble size Chen  Guestrin (5); Prokhorenkova et al. (25).
In this part we use the ﬁrst property of the polynomial representation: the monomial coefﬁcients
are determined by the same set of points in all trees of the ensemble. We will deﬁne a quality
measure of the resulting monomials  based on point statistics and remove the least valuable from the
model. To measure monomial quality we use η(M ) = Ex∼X
closely related to feature importance  proposed by Breiman et al. (3). They estimated a squared risk
improvement from region partitioning; our measure is an estimated squared risk improvement over
setting the speciﬁed monomial weight to zero. This way we get a simple ensemble-wise pruning
strategy while it is deﬁnitely possible to use more sophisticated methods like sparse re-weighting
monomials.
The introduced monomial quality measure leads to a straightforward pruning scheme: select some
threshold α and for each monomial M with η(M ) < α set weight to zero. This threshold could be
selected by heuristics like ‘Elbow method‘  based on learn statistics only  or α could be estimated
using the validation set. The former approach requires human judgment; thus its quality is hard to
estimate. The latter one could be applied automatically; thus we could estimate its performance in a
fair way.
We use several publicly-available binary classiﬁcation datasets6 to evaluate the quality of the automatic
approach. The experimental setup is the following: we split the data into train/validate/test groups 
the parameters were tuned on train/validation pair  then these parameters were used on train+validate
joined dataset to get the ﬁnal model. The result model is evaluated on the test part of a data. It
is important to note that we tuned the optimal gradient step  number of trees in the ensemble 
regularization parameters of a single trees  etc. This way we get an ensemble with the minimal
number of leaves using state-of-the-art techniques.
Obtained ROC AUC values are presented in the second (original model) and third (pruned model)
columns of the Table 1. The last column contains the polynomial model reduction ratio. The
experimental results allow us to claim that the tree ensemble can be signiﬁcantly reduced without
loss of model quality in a variety of practical tasks. The features used there have numerical as well as
categorical nature.

6Information about datasets is available in supplementary materials.

6

Trained ensemble(AUC)

Pruned ensemble(AUC) Model reduction (Ratio)

DataSet
Adult
Amazon

KDD Internet
KDD Upselling

Epsilon

92.76%
82.51%
95.71%
85.72%
95.76%

92.75%
82.51%
95.74%
85.72%
95.76%

2.58
2.6
2.07
3.78
1.11

Table 1: Quality and size of original and pruned models.

6

Interpretation

Feature attribution methods are designed to answer  why and how each feature inﬂuences models
predictions. These methods could be global  describing feature inﬂuence on average  or local 
explaining how the model deals with one sample.
Global feature attribution methods are well developed. Classical approaches are still widely used
today. Breiman et al. (3) gain  or total reduction of loss contributed by all splits  provides a way
to estimate the relative contribution of each input feature to the response. Partial dependency plots
Hastie et al. (10) are used to summarize dependence of response on the input feature. Resampling
strategies are used to design alternatives to Breiman et al. (3) feature importance measures. Brieman’s
‘Variable Importance‘ Breiman (2) for random forests and its model-agnostic version ‘Model Reliance‘
Fisher et al. (8) are the best known examples. Other sampling strategies lead to a big variety of others
Díaz-Uriarte  Andrés de (7); Ishwaran (12); Strobl et al. (28).
Local feature attribution methods deal with importance measures for each sample in the dataset.
Work by Lundberg  Lee (19) has recently shown that  under certain conditions  there is a single
unique solution for additive feature attribution methods with three desirable properties (local accuracy 
missingness  and consistency) — SHAP values. This work was adapted to decision trees in (18).
All these techniques could be applied combined with MonoForest framework. However  our repre-
sentation allows us to interpret a tree ensemble model as a linear function. As shown in Section 3  it
is possible to present a tree ensemble as polynomial:

(cid:88)

M∈2C

I(cid:8)xi(c) > b(c)(cid:9)

(cid:89)

c∈M

H(x) =

wM

Due to the second property of the representation  for each feature k monomial M either contains
a single condition c such that i(c) = k  or there is no dependency on this feature. Let us denote
C−k = {c ∈ C : i(c) (cid:54)= k}  cki—i-th ordered border condition on feature k: i(c) = k  i > j ⇔
b(cki) > b(ckj). We can redistribute monomials the following way:

(cid:88)

M∈2C−k

(cid:89)

c∈M

(cid:88)

i

 (cid:88)

M∈2C−k

H(x) =

wM

c(x) +

I {xk > bki}

wM∪{cki}

c(x)

(7)

This formula is linearly dependant on conditions I {xk > bki} and we can use expected linear
coefﬁcients to evaluate the inﬂuence of each condition cki. To get a single value for entire feature we
sum all dependant condition values:

(cid:88)

i

ν(k) =

Ex∼X

I {xk > bki} (cid:88)

M∈2C−k

(cid:89)

c∈M

wM∪{cki}

c(x)

(8)

The aggregation to the ﬁnal feature score is straightforward and has its limitations. For example
if the monomial values are spread around zero throughout the data points it will have next to zero
expectation despite the possible big inﬂuence of the feature to particular points. The naive nature of
the proposed score does not interfere with its good behavior in real-world examples.
There is no established way to compare feature attribution methods. To demonstrate the quality of
the proposed approach we built a decision tree ensemble binary one-vs-rest classiﬁer for each class
of the MNIST dataset using CatBoost and analyzed each classiﬁer using three methods: MonoForest 
SHAP  and permutation based Model Reliance  proposed by Fisher et al. (8). The model accuracy

7

(6)



(cid:89)

c∈M



Figure 3: Feature importance visualisations on MNIST data.

for each class was > 98%. The task of the MNIST dataset is to identify a single handwritten digit.
The features are the levels of grey of each pixel in the input image. We have calculated importance
values for all features and put them on the picture of the same size as input images. SHAP is known
as a local attribution method  but the authors suggested a way to use it as a global attribution too.
The results for the ﬁrst four classes are presented in the Figure 3. The MonoForest results allow
splitting positive and negative inﬂuence of features in comparison to SHAP global attribution and
Model Reliance.

7 Conclusions and Future work

In this work  we have introduced a new framework for decision tree ensemble representation. The
proposed framework demonstrated good results on two popular tasks: model interpretation and
reduction. The new representation has interesting mathematical properties of its components  and
they allow us to simplify algorithms relevant for decision tree ensembles. We show that  using
primitive ﬁltering techniques for linear models  it is possible to signiﬁcantly simplify the original
model without loss of its quality after applying other pruning strategies.
Another important task for MonoForest application is model interpretation. We used a straightforward
approach to polynomial models analysis and were able to get promising results in comparison with
state-of-the-art techniques  such as SHAP and VI  though a comparison of interpretation methods is
not well-established ﬁeld yet.
The proposed representation allows us to introduce a class of equivalence in tree ensembles and claim
that the decision space of the popular GBDT libraries is equivalent. We have set up a problem of
optimal ensemble conversion and provided a greedy algorithm to solve this problem. It is important
to note that this way we can split tasks of ensemble training and performance optimization of the
result decision function. As an example of such conversion  we used easy-to-train LightGBM trees
and then converted them to a more effective form of oblivious trees used by CatBoost. The algorithm
can be improved by set optimization techniques to achieve even better results.
The MonoForest framework has shown its applicability to white-box analysis of decision trees
ensembles. This work shows that even ‘naive‘ methods provide good results  thus promising much
more when combined with more sophisticated techniques. There are several directions for future
explorations:

• framework applications expansion to other tasks;
• improvement of ensemble tree pruning algorithm: LASSO and other regularization based
• greedy performance optimization of the model application: if we want to build ordered by
model value list  it is possible to skip some computationally demanding features valuation

techniques look promising;

8

References
[1] Bach Francis R. Learning with Submodular Functions: A Convex Optimization Perspective //

CoRR. 2011. abs/1111.6453.

[2] Breiman Leo. Random Forests // Mach. Learn. X 2001. 45  1. 5–32.

[3] Breiman Leo  Friedman J. H.  Olshen R. A.  Stone C. J. Classiﬁcation and Regression Trees.

1984.

[4] Chen Tian Qi  Rubanova Yulia  Bettencourt Jesse  Duvenaud David K. Neural Ordinary

Differential Equations // NeurIPS. 2018. 6572–6583.

[5] Chen Tianqi  Guestrin Carlos. Xgboost: A scalable tree boosting system // Proceedings of
the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
2016. 785–794.

[6] Dorogush Anna Veronika  Ershov Vasily  Gulin Andrey. CatBoost: gradient boosting with

categorical features support // CoRR. 2018. abs/1810.11363.

[7] Díaz-Uriarte Ramón  Andrés Sara Alvarez de. Gene selection and classiﬁcation of microarray

data using random forest. // BMC Bioinformatics. 2006. 7. 3.

[8] Fisher Aaron  Rudin Cynthia  Dominici Francesca. All Models are Wrong  but Many are
Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models
Simultaneously. 2018.

[9] Friedman Jerome H. Greedy function approximation: a gradient boosting machine // Annals of

statistics. 2001. 1189–1232.

[10] Hastie Trevor  Tibshirani Robert  Friedman Jerome H. The elements of statistical learning: data

mining  inference  and prediction  2nd Edition. 2009. (Springer series in statistics).

[11] He Kaiming  Zhang Xiangyu  Ren Shaoqing  Sun Jian. Delving Deep into Rectiﬁers: Surpassing
Human-Level Performance on ImageNet Classiﬁcation // Proceedings of the 2015 IEEE
International Conference on Computer Vision (ICCV). Washington  DC  USA: IEEE Computer
Society  2015. 1026–1034. (ICCV ’15).

[12] Ishwaran Hemant. Variable importance in binary regression trees and forests // Electronic

Journal of Statistics. 1 2007. 1. 519–537.

[13] Ke Guolin  Meng Qi  Finley Thomas  Wang Taifeng  Chen Wei  Ma Weidong  Ye Qiwei  Liu
Tie-Yan. LightGBM: A Highly Efﬁcient Gradient Boosting Decision Tree // Advances in Neural
Information Processing Systems 30. 2017. 3146–3154.

[14] Kearns Michael J.  Mansour Yishay. A Fast  Bottom-Up Decision Tree Pruning Algorithm
with Near-Optimal Generalization // Proceedings of the Fifteenth International Conference
on Machine Learning. San Francisco  CA  USA: Morgan Kaufmann Publishers Inc.  1998.
269–277. (ICML ’98).

[15] Koh Pang Wei  Liang Percy. Understanding Black-box Predictions via Inﬂuence Functions
// Proceedings of the 34th International Conference on Machine Learning. 70. International
Convention Centre  Sydney  Australia: PMLR  06–11 Aug 2017. 1885–1894. (Proceedings of
Machine Learning Research).

[16] Kohavi Ron  Li Chia-Hsin. Oblivious Decision Trees Graphs and Top Down Pruning //
Proceedings of the 14th International Joint Conference on Artiﬁcial Intelligence - Volume 2.
San Francisco  CA  USA: Morgan Kaufmann Publishers Inc.  1995. 1071–1077. (IJCAI’95).

[17] Lavalle Irving H.  Fishburn Peter C. Equivalent decision trees and their associated strategy sets

// Theory and Decision. Jul 1987. 23  1. 37–63.

[18] Lundberg Scott M.  Erion Gabriel G.  Lee Su-In. Consistent Individualized Feature Attribution

for Tree Ensembles // CoRR. 2018. abs/1802.03888.

9

[19] Lundberg Scott M  Lee Su-In. A Uniﬁed Approach to Interpreting Model Predictions // Advances

in Neural Information Processing Systems 30. 2017. 4765–4774.

[20] Margineantu Dragos D.  Dietterich Thomas G. Pruning Adaptive Boosting // Proceedings
of the Fourteenth International Conference on Machine Learning. San Francisco  CA  USA:
Morgan Kaufmann Publishers Inc.  1997. 211–218. (ICML ’97).

[21] Martínez-Muñoz G.  Hernández-Lobato D.  Suárez A. An Analysis of Ensemble Pruning
Techniques Based on Ordered Aggregation // IEEE Transactions on Pattern Analysis and
Machine Intelligence. Feb 2009. 31  2. 245–259.

[22] Martínez-Muñoz Gonzalo  Suárez Alberto. Using boosting to prune bagging ensembles //

Pattern Recognition Letters. 2007. 28  1. 156 – 165.

[23] Mehta Manish  Rissanen Jorma  Agrawal Rakesh. MDL-based Decision Tree Pruning //
Proceedings of the First International Conference on Knowledge Discovery and Data Mining.
1995. 216–221. (KDD’95).

[24] Mingers John. An Empirical Comparison of Pruning Methods for Decision Tree Induction //

Machine Learning. Nov 1989. 4  2. 227–243.

[25] Prokhorenkova Liudmila  Gusev Gleb  Vorobev Aleksandr  Dorogush Anna Veronika  Gulin An-
drey. CatBoost: unbiased boosting with categorical features // Advances in Neural Information
Processing Systems 31. 2018. 6638–6648.

[26] Ribeiro Marco Túlio  Singh Sameer  Guestrin Carlos. Anchors: High-Precision Model-Agnostic
Explanations // Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
(AAAI-18)  the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18)  and the 8th
AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18)  New Orleans 
Louisiana  USA  February 2-7  2018. 2018. 1527–1535.

[27] Shrikumar Avanti  Greenside Peyton  Kundaje Anshul. Learning Important Features Through

Propagating Activation Differences // CoRR. 2017. abs/1704.02685.

[28] Strobl Carolin  Boulesteix Anne-Laure  Kneib Thomas  Augustin Thomas  Zeileis Achim. Condi-

tional variable importance for random forests // BMC Bioinformatics. 2008. 9.

[29] Tamon Christino  Xiang Jie. On the Boosting Pruning Problem // Machine Learning: ECML

2000. Berlin  Heidelberg: Springer Berlin Heidelberg  2000. 404–412.

[30] Zantema Hans  Bodlaender Hans. Finding Small Equivalent Decision Trees is Hard // Interna-

tional Journal of Foundations of Computer Science. 1999. 11. 343–354.

[31] Štrumbelj Erik  Kononenko Igor. Explaining Prediction Models and Individual Predictions with

Feature Contributions // Knowl. Inf. Syst. XII 2014. 41  3. 647–665.

10

,Igor Kuralenok
Vasilii Ershov
Igor Labutin